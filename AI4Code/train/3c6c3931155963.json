{"cell_type":{"9d09d599":"code","1f59417b":"code","9752e846":"code","9eac60c4":"code","7e2659c5":"code","d59573b5":"code","68a8322e":"code","46d3fd4f":"code","0b8f9a2b":"code","b32a6bc0":"code","53143c6f":"code","d0a5e161":"code","0b438cb5":"code","2811fa9f":"code","86cf616d":"code","908b22b7":"code","2e21bf27":"code","437d6735":"code","5dba7804":"code","867356db":"code","da5c0b5d":"code","eb6003af":"code","9e555009":"code","0101e8fb":"code","a1c81b51":"code","9896bde9":"code","bb6080b1":"code","7582094b":"code","020bde2c":"code","7eb69e0f":"markdown","e0c6d0f7":"markdown","7f1c957e":"markdown","ebef574b":"markdown","620cab39":"markdown","70c2e11b":"markdown","6b4dea42":"markdown","ead5b758":"markdown","0d98c048":"markdown","dd9fc16a":"markdown","1a20d45e":"markdown","d55220e7":"markdown","f19963c2":"markdown","300d7e20":"markdown","8d4544ed":"markdown","6d6cb9f7":"markdown","d41d5a00":"markdown","932143cb":"markdown","48ae8737":"markdown","f29e81f4":"markdown","a2a42b7e":"markdown","e26b1b80":"markdown","fcdb459f":"markdown","b5850c86":"markdown","78760534":"markdown"},"source":{"9d09d599":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom random import randint\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score,accuracy_score, precision_score, recall_score, roc_auc_score \n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1f59417b":"data = pd.read_csv(\"\/kaggle\/input\/company-bankruptcy-prediction\/data.csv\")\ndata.columns = [i.title().strip() for i in list(data.columns)]\n\nrow = data.shape[0]\ncol = data.shape[1]\nprint(\"The number of rows within the dataset are {} and the number of columns is {}\".format(row,col))","9752e846":"data.isnull().sum().sort_values(ascending=False).head()","9eac60c4":"colors = ['Accent', 'Accent_r', 'Blues', 'Blues_r', 'BrBG', 'BrBG_r', 'BuGn', 'BuGn_r', 'BuPu', 'BuPu_r', 'CMRmap',\n          'CMRmap_r', 'Dark2', 'Dark2_r', 'GnBu', 'GnBu_r', 'Greens', 'Greens_r', 'Greys', 'Greys_r', 'OrRd', 'OrRd_r', \n          'Oranges', 'Oranges_r', 'PRGn', 'PRGn_r', 'Paired', 'Paired_r', 'Pastel1', 'Pastel1_r', 'Pastel2', 'Pastel2_r', \n          'PiYG', 'PiYG_r', 'PuBu', 'PuBuGn', 'PuBuGn_r', 'PuBu_r', 'PuOr', 'PuOr_r', 'PuRd', 'PuRd_r', 'Purples', 'Purples_r', \n          'RdBu', 'RdBu_r', 'RdGy', 'RdGy_r', 'RdPu', 'RdPu_r', 'RdYlBu', 'RdYlBu_r', 'RdYlGn', 'RdYlGn_r', 'Reds', 'Reds_r', \n          'Set1', 'Set1_r', 'Set2', 'Set2_r', 'Set3', 'Set3_r', 'Spectral', 'Spectral_r', 'Wistia', 'Wistia_r', 'YlGn', 'YlGnBu', \n          'YlGnBu_r', 'YlGn_r', 'YlOrBr', 'YlOrBr_r', 'YlOrRd', 'YlOrRd_r', 'afmhot', 'afmhot_r', 'autumn', 'autumn_r', 'binary', \n          'binary_r', 'bone', 'bone_r', 'brg', 'brg_r', 'bwr', 'bwr_r', 'cividis', 'cividis_r', 'cool', 'cool_r', 'coolwarm', \n          'coolwarm_r', 'copper', 'copper_r', 'crest', 'crest_r', 'cubehelix', 'cubehelix_r', 'flag', 'flag_r', 'flare', \n          'flare_r', 'gist_earth', 'gist_earth_r', 'gist_gray', 'gist_gray_r', 'gist_heat', 'gist_heat_r', 'gist_ncar', \n          'gist_ncar_r', 'gist_rainbow', 'gist_rainbow_r', 'gist_stern', 'gist_stern_r', 'gist_yarg', 'gist_yarg_r', 'gnuplot', \n          'gnuplot2', 'gnuplot2_r', 'gnuplot_r', 'gray', 'gray_r', 'hot', 'hot_r', 'hsv', 'hsv_r', 'icefire', 'icefire_r', \n          'inferno', 'inferno_r', 'jet', 'jet_r', 'magma', 'magma_r', 'mako', 'mako_r', 'nipy_spectral', 'nipy_spectral_r', \n          'ocean', 'ocean_r', 'pink', 'pink_r', 'plasma', 'plasma_r', 'prism', 'prism_r', 'rainbow', 'rainbow_r', 'rocket', \n          'rocket_r', 'seismic', 'seismic_r', 'spring', 'spring_r', 'summer', 'summer_r', 'tab10', 'tab10_r', 'tab20', \n          'tab20_r', 'tab20b', 'tab20b_r', 'tab20c', 'tab20c_r', 'terrain', 'terrain_r', 'turbo', 'turbo_r', 'twilight', \n          'twilight_r', 'twilight_shifted', 'twilight_shifted_r', 'viridis', 'viridis_r', 'vlag', 'vlag_r', 'winter', 'winter_r']","7e2659c5":"value = randint(0, len(colors)-1)\n\nsns.countplot('Bankrupt?',data=data,palette = colors[value])","d59573b5":"numeric_features = data.dtypes[data.dtypes != 'int64'].index\ncategorical_features = data.dtypes[data.dtypes == 'int64'].index\n\ndata[categorical_features].columns.tolist()","68a8322e":"value = randint(0, len(colors)-1)\n\nprint(data['Liability-Assets Flag'].value_counts())\nsns.countplot('Liability-Assets Flag',data=data,palette = colors[value])","46d3fd4f":"value = randint(0, len(colors)-1)\n\nprint(data[['Liability-Assets Flag','Bankrupt?']].value_counts())\nsns.countplot(x = 'Liability-Assets Flag',hue = 'Bankrupt?',data = data,palette = colors[value])","0b8f9a2b":"value = randint(0, len(colors)-1)\n\nprint(data['Net Income Flag'].value_counts())\nsns.countplot('Net Income Flag',data=data,palette = colors[value])","b32a6bc0":"value = randint(0, len(colors)-1)\n\nprint(data[['Net Income Flag','Bankrupt?']].value_counts())\nsns.countplot(x = 'Net Income Flag',hue = 'Bankrupt?',data = data,palette = colors[value])","53143c6f":"positive_corr = data[numeric_features].corrwith(data[\"Bankrupt?\"]).sort_values(ascending=False)[:6].index.tolist()\nnegative_corr = data[numeric_features].corrwith(data[\"Bankrupt?\"]).sort_values()[:6].index.tolist()\n\npositive_corr = data[positive_corr + [\"Bankrupt?\"]].copy()\nnegative_corr = data[negative_corr + [\"Bankrupt?\"]].copy()","d0a5e161":"def corrbargraph(x_value, y_value):\n    \n    plt.figure(figsize=(15,8))\n    value = randint(0, len(colors)-1)\n\n    for i in range(1,7):\n        plt.subplot(2,3,i)  \n        sns.barplot(x = x_value, y = y_value[i-1],data = data,palette = colors[value])\n\n    plt.tight_layout(pad=0.5)","0b438cb5":"x_value = positive_corr.columns.tolist()[-1]\ny_value = positive_corr.columns.tolist()[:-1]\n\ncorrbargraph(x_value, y_value)","2811fa9f":"x_value = negative_corr.columns.tolist()[-1]\ny_value = negative_corr.columns.tolist()[:-1]\n\ncorrbargraph(x_value, y_value)","86cf616d":"plt.figure(figsize=(10,3))\n\nplt.suptitle(\"Correlation Between Positive Attributes\")\n\nplt.subplot(1,2,1)\nplt.xlabel(\"Debt Ratio\")\nplt.ylabel(\"Current Liability To Assets Ratio\")\nplt.scatter(data[\"Debt Ratio %\"],data[\"Current Liability To Assets\"], marker='v',color = 'red')\n\nplt.subplot(1,2,2)\nplt.xlabel(\"Borrowing Dependency\")\nplt.ylabel(\"Liability To Equity Ratio\")\nplt.scatter(data[\"Borrowing Dependency\"],data[\"Liability To Equity\"], marker='v',color = 'red')\n\nplt.tight_layout(pad=0.8)","908b22b7":"plt.figure(figsize=(10,3))\n\nplt.suptitle(\"Correlation Between Negative Attributes\")\n\nplt.subplot(1,2,1)\nplt.xlabel(\"ROA (A)\")\nplt.ylabel(\"ROA (B)\")\nsns.scatterplot(data=data, x='Roa(A) Before Interest And % After Tax', y='Roa(B) Before Interest And Depreciation After Tax',color = 'red')\n\nplt.subplot(1,2,2)\nplt.xlabel(\"ROA (B)\")\nplt.ylabel(\"ROA (C)\")\nsns.scatterplot(data=data, x='Roa(B) Before Interest And Depreciation After Tax', y='Roa(C) Before Interest And Depreciation Before Interest',color = 'red')\n\nplt.tight_layout(pad=0.8)","2e21bf27":"relation = positive_corr.columns.tolist()[:-1] + negative_corr.columns.tolist()[:-1]\nplt.figure(figsize=(8,7))\nsns.heatmap(data[relation].corr(),annot=True)","437d6735":"numeric_features = data.dtypes[data.dtypes != 'int64'].index\ndata[numeric_features] = data[numeric_features].apply(lambda x: (x - x.mean()) \/ (x.std()))\n\ndata[numeric_features] = data[numeric_features].fillna(0)","5dba7804":"Models = pd.DataFrame(columns=['Algorithm','Model Score','Precision','Recall','F1 score','ROC-AUC score'])\n\ndef taining_without_feature_selection(Parameters, Model, Dataframe, Modelname):\n    \n    data = Dataframe.copy()\n    \n    X = data.drop('Bankrupt?', axis=1)\n    y = data['Bankrupt?']\n    \n    #Traditional split of the dataset 80% - 20%\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    x_train, x_test, y_train, y_test = x_train.values, x_test.values, y_train.values, y_test.values\n\n    #Proportional split of 80% data with respect to the class of the target feature ie. [1,0]\n    sf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n    for train_index, test_index in sf.split(x_train, y_train):\n        sf_x_train, sf_x_test = X.iloc[train_index], X.iloc[test_index]\n        sf_y_train, sf_y_test = y.iloc[train_index], y.iloc[test_index]\n\n    sf_x_train, sf_x_test, sf_y_train, sf_y_test = sf_x_train.values, sf_x_test.values, sf_y_train.values, sf_y_test.values\n    \n    model_parameter_sm = Parameters\n\n    rand_model = RandomizedSearchCV(Model, model_parameter_sm, n_iter=4)\n     \n    #Identifying the best parameters through RandomizedSearchCV()\n    for train, test in sf.split(sf_x_train, sf_y_train):\n        pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_model) \n        fitting_model = pipeline.fit(sf_x_train[train], sf_y_train[train])\n        best_model = rand_model.best_estimator_\n\n    #Evaluation with against 20% unseen testing data\n    print()\n    print(\"Evaluation Of Models\")  \n\n    sm = SMOTE(sampling_strategy='minority', random_state=42)\n    Xsm_train, ysm_train = sm.fit_resample(sf_x_train, sf_y_train)\n    \n    print()\n    print(\"Random Model Evaluation\")  \n    \n    final_model_sm = rand_model.best_estimator_\n    final_model_sm.fit(Xsm_train, ysm_train)\n\n    prediction = final_model_sm.predict(x_test)\n\n    print(classification_report(y_test, prediction))\n    \n    model = {}\n\n    model['Algorithm'] = Modelname\n    model['Model Score'] = str(round((accuracy_score(y_test, prediction)*100),2)) + \"%\"\n    model['Precision'] = round(precision_score(y_test, prediction),2)\n    model['Recall'] = round(recall_score(y_test, prediction),2)\n    model['F1 score'] = round(f1_score(y_test, prediction),2)\n    model['ROC-AUC score'] = round(roc_auc_score(y_test, prediction),2)\n    \n    return model","867356db":"print(\"K Nearest Neighbour\")\nTrainedModel = taining_without_feature_selection({\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}, KNeighborsClassifier(), data,\"K Nearest Neighbour\")\nModels = Models.append(TrainedModel,ignore_index=True)","da5c0b5d":"print(\"Logistic Regression\")\nTrainedModel = taining_without_feature_selection({\"penalty\": ['l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}, LogisticRegression(solver='liblinear'), data, \"Logistic Regression\")\nModels = Models.append(TrainedModel,ignore_index=True)","eb6003af":"print(\"DecisionTree Classifier\")\nTrainedModel = taining_without_feature_selection({\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)),\"min_samples_leaf\": list(range(5,7,1))}, DecisionTreeClassifier(), data, \"DecisionTree Classifier\")\nModels = Models.append(TrainedModel,ignore_index=True)","9e555009":"print(\"Random Forest Classifier\")\nTrainedModel = taining_without_feature_selection({\"max_depth\": [3, 5, 10, None],\"n_estimators\": [100, 200, 300, 400, 500]},  RandomForestClassifier(), data, \"Random Forest Classifier\")\nModels = Models.append(TrainedModel,ignore_index=True)","0101e8fb":"print(\"Support Vector Classifier\")\nTrainedModel = taining_without_feature_selection({'C': [1,10,20],'kernel': ['rbf','linear']},  SVC(), data, \"Support Vector Classifier\")\nModels = Models.append(TrainedModel,ignore_index=True)","a1c81b51":"Models.sort_values('F1 score',ascending=False)","9896bde9":"Models = pd.DataFrame(columns=['Algorithm','Model Score','Precision','Recall','F1 score','ROC-AUC score'])\n\ndef taining_with_feature_selection(Parameters, Model, Dataframe, Modelname):\n    \n    data = Dataframe.copy()\n    \n    X = data.drop('Bankrupt?', axis=1)\n    y = data['Bankrupt?']\n    \n    '''\n    Feature Selection Process: \n    class sklearn.feature_selection.SelectKBest(score_func=<function>, k=<number of features>\n        score_func - Scoring measure\n        k - Total features to be returned \n    '''\n    \n    fs = SelectKBest(score_func=f_classif, k=int((data.shape[1]*85)\/100))\n\n    X = fs.fit_transform(X, y)\n    \n    X = pd.DataFrame(X)\n    y = pd.DataFrame(y)\n    \n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    x_train, x_test, y_train, y_test = x_train.values, x_test.values, y_train.values, y_test.values\n\n    sf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n    for train_index, test_index in sf.split(x_train, y_train):\n        sf_x_train, sf_x_test = X.iloc[train_index], X.iloc[test_index]\n        sf_y_train, sf_y_test = y.iloc[train_index], y.iloc[test_index]\n\n    sf_x_train, sf_x_test, sf_y_train, sf_y_test = sf_x_train.values, sf_x_test.values, sf_y_train.values, sf_y_test.values\n    \n    model_parameter_sm = Parameters\n\n    rand_model = RandomizedSearchCV(Model, model_parameter_sm, n_iter=4)\n\n    for train, test in sf.split(sf_x_train, sf_y_train):\n        pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_model) \n        fitting_model = pipeline.fit(sf_x_train[train], sf_y_train[train])\n        best_model = rand_model.best_estimator_\n\n    print()\n    print(\"Evaluation Of Models\")  \n\n    sm = SMOTE(sampling_strategy='minority', random_state=42)\n    Xsm_train, ysm_train = sm.fit_resample(sf_x_train, sf_y_train)\n    \n    print()\n    print(\"Random Model Evaluation\")  \n    \n    final_model_sm = rand_model.best_estimator_\n    final_model_sm.fit(Xsm_train, ysm_train)\n\n    prediction = final_model_sm.predict(x_test)\n\n    print(classification_report(y_test, prediction))\n    \n    model = {}\n\n    model['Algorithm'] = Modelname\n    model['Model Score'] = str(round((accuracy_score(y_test, prediction)*100),2)) + \"%\"\n    model['Precision'] = round(precision_score(y_test, prediction),2)\n    model['Recall'] = round(recall_score(y_test, prediction),2)\n    model['F1 score'] = round(f1_score(y_test, prediction),2)\n    model['ROC-AUC score'] = round(roc_auc_score(y_test, prediction),2)\n    \n    return model","bb6080b1":"print(\"Random Forest Classifier\")\nTrainedModel = taining_with_feature_selection({\"max_depth\": [3, 5, 10, None],\"n_estimators\": [100, 200, 300, 400, 500]},  RandomForestClassifier(), data, \"Random Forest Classifier\")\nModels = Models.append(TrainedModel,ignore_index=True)","7582094b":"print(\"K Nearest Neighbour\")\nTrainedModel = taining_with_feature_selection({\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}, KNeighborsClassifier(), data,\"K Nearest Neighbour\")\nModels = Models.append(TrainedModel,ignore_index=True)","020bde2c":"Models.sort_values('F1 score',ascending=False)","7eb69e0f":"There is a positive relation between attributes that have a low correlation with the target attribute.","e0c6d0f7":"A small portion of organizations suffers bankruptcy, although possessing more assets than their liabilities.","7f1c957e":"We see that three attributes - \"Debt Ratio %, Current Liability To Assets, Current Liability To Current Assets\" are commonly high in bankrupt organizations.","ebef574b":"# Summary of Analysis","620cab39":"A company faces bankruptcy when they are unable to pay off their debts. The Taiwan Economic Journal for the years 1999 to 2009 has listed the details of company bankruptcy based on the business regulations of the Taiwan Stock Exchange. The Taiwan Stock Exchange was established in 1961 and began operating as a stock exchange on 9 February 1962. It is a financial institution located in Taipei, Taiwan. It has over 900 listed companies. The data includes a majority of numerical attributes that help understand the possibility of bankruptcy.  \n\nThis notebook aims at analyzing the possibility of whether an organization would face bankruptcy. The main focus of this notebook is to demonstrate concepts listed as follows:<br>\n- **Dealing with imbalanced datasets the right way** <br>(A detailed look on all the do's and don't has been demonstrated in \"[Credit Fraud || Dealing with Imbalanced Datasets](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets)\" notebook by \"[Janio Martinez Bachmann](https:\/\/www.kaggle.com\/janiobachmann)\". This notebook provides complete clarity for dealing with imbalanced datasets)<br>\n    - An imbalanced dataset is a situation when there is an unequal distribution of classes in the dataset. We can solve this by either upsampling - introducing minority class at power with majority class or downsampling - reducing majority class to match the size of minority class<br><br>\n    \n- **Finding the best attributes to work with through feature selection**\n    - We are now dealing with over 50 attributes in this dataset, let us just play around and demonstrate how we can programmatically pick the best features, and analyze how it affects our model.\n\n<br>\nKindly provide an upvote if this notebook was useful. Also, I would greatly appreciate any feedback or suggestions for improvement. Thank-you :)\n\n","70c2e11b":"Since there are no missing values, we can jump into analyzing the data","6b4dea42":"# Bankruptcy Analysis","ead5b758":"The \"Liability-Assets\" flag denotes the status of an organization, where if the total liability exceeds total assets, the flagged value will be 1, else the value is 0. A majority number of times, organizations\/company's assets are more than their liabilities.","0d98c048":"So, as we can see narrowing the data features may not always provide better results it seems! But there is always a possibility that the results may appear few points better than that of the entire dataset features.","dd9fc16a":"# Data Loading and Cleaning","1a20d45e":"- The number of organizations that have gone bankrupt in 10 years between 1999 \u2013 2000 is few.\n- Several companies possess many assets, which is always a good sign for an organization.\n- An organization cannot guarantee not being bankrupt, although owning several assets.\n- The organizations in the dataset are running into losses for the past two years as their net income poses to be negative.\n- Very few of the organizations that have had negative income in the past two years suffer from bankruptcy.\n- It is observed that \u201cDebt Ratio %, Current Liability To Assets, Current Liability To Current Assets\" attributes are a few of the attributes that have a high correlation with the target attribute.\n- An increase in the values of the attributes \u201cDebt Ratio %, Current Liability To Assets, Current Liability To Current Assets\u201d causes an organization to suffer heavy losses, thus resulting in bankruptcy.\n- An increase in the values of the attributes that have a negative correlation with the target attribute helps an organization avoid bankruptcy.\n- There seems to be a relation between attributes that have a high correlation with the target attribute and a low correlation with the target attribute.\n- We observed several correlations among the top 12 attributes, one of which being \u201cNet Worth\/Assets and Debt Ratio %\u201d that is negatively correlated with one another.\n","d55220e7":"The \"Net Income\" flag denotes the status of an organization's income in the last two years, where if the net income is negative for the past two years, the flagged value will be 1, else the value is 0. We observe that all the records have been exhibiting a loss for the past two years.","f19963c2":"For the sake of simplicity, we analyze the six top positively and negatively correlated attributes.","300d7e20":"# Data Modeling","8d4544ed":"There is a positive relation between attributes that have a high correlation with the target attribute.","6d6cb9f7":"The records are observed to be highly imbalanced. Thus it is necessary to consider balancing the dataset through \"Upsampling or Downsampling\" techniques.\n<br><br>Through data.info(), we observed that we have a majority of \"float64\" data. The categorical data is distinguished as binary 1 and 0, thus stored as \"int64\". We separate the numeric and categoric data to analyze our dataset.","d41d5a00":"These attributes show us that the more the assets and earing of a company, the less likely is the organization to be bankrupt.\n<br>Let us check the relation of top six positive and negative correlation attributes among eachother","932143cb":"![image.png](attachment:5ba8d27f-beb0-4037-adbc-36989f40479b.png)","48ae8737":"The numeric attributes have been normalized.<br><br>\n\nOur dataset is highly imbalanced. Thus before training the model, we need to deal with this data. Let us lay down some steps we must follow when we come across an imbalanced dataset.\n- Split the dataset into training and testing sets (80% - 20%). We preserve the 20% testing set for the final evaluation. \n- Through \"Stratified K Fold Cross-Validation\" we will now distribute the 80% training set into further training and testing splits.\n- Since we are dealing with over 50 features, we use \"Randomized Search Cross-Validation\" as this technique proves to perform better with many features.","f29e81f4":"Below is the list of a variety of color palette that can be used while creating visuals","a2a42b7e":"We see that among all the models \"**Random Forest Classifier and K Nearest Neighbour**\" have the highest **F-1 Score**. Thus, we may use these models to train our data. <br><br>At the very beginning of this notebook, we spoke about feature selection.<br><br>\nSince my knowledge in finance is limited, why not let machine learning help me? :P<br><br>\n\"SelectKBest\"  is used to select features that add the most value to the target variable","e26b1b80":"There are only three categorical data columns, we will first explore these columns ","fcdb459f":"A total correlation of the top 12 attributes are given above","b5850c86":"Many organizations that have suffered losses for the past two years have stabilized their business, thus avoiding bankruptcy.","78760534":"# Data Visualization"}}