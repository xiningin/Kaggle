{"cell_type":{"e1cd687f":"code","639b9bec":"code","7c5032cb":"code","3875c947":"code","428bee6d":"code","2a55ffd1":"code","db68e105":"code","c2ed8de7":"code","42ea8d8a":"code","aa71c62b":"code","e404997d":"code","77a7efd0":"code","ed5d2b33":"code","68683ebe":"code","9aef5b9b":"code","292dc232":"code","451b28ed":"code","52100b9c":"code","a261f133":"code","a7ead6d9":"code","2ced57fd":"code","0141d867":"code","a72b11be":"code","10057573":"code","19adafd4":"code","05ae37e3":"code","b1d036ec":"code","0154008e":"code","89054bcd":"code","5b4739d5":"code","07cde2d8":"code","edb379e2":"code","99189680":"code","7443b67f":"code","ea7e9066":"code","0b0eab19":"code","83d78a60":"code","0249833e":"code","689456df":"code","fff196c3":"code","88eec14f":"code","f829c4bc":"code","fd53d5a8":"code","1750e3c7":"code","4963abc3":"code","11cd09ea":"code","dc97bbef":"code","a310eb4d":"code","5a0fde63":"code","93052074":"code","a3398b6a":"code","d56e6ece":"code","f0e75eea":"code","aea40c83":"code","44cd10f0":"code","0734aa4e":"code","fc612c0a":"code","38cf3496":"code","125cbbbc":"code","90bfb133":"code","e82bd769":"code","7eb59cc6":"code","96f970a3":"code","87a8f165":"code","5eaea6c5":"code","5f205d12":"code","1df0cb5c":"code","a852cc7c":"code","ce7759cb":"code","d95f41c1":"code","847e6f8d":"code","415c2fc7":"code","e9f23302":"code","158f0066":"code","3d450837":"code","8d5ee724":"code","88845243":"code","e45ffb2b":"code","c4ffc4ca":"code","2cc11ad3":"code","09656876":"code","425bfa8f":"code","ab9c0bb3":"code","a1af881b":"markdown","024ca300":"markdown","c00dd350":"markdown","d67d051a":"markdown","67f3adcc":"markdown","a52a7abe":"markdown","31f14f68":"markdown","516c32c3":"markdown","7afc7a51":"markdown","328aec15":"markdown","fd2b55b3":"markdown","4a489951":"markdown","32707f17":"markdown","06fca5bc":"markdown","accfee64":"markdown","a1ae6129":"markdown","1c39d295":"markdown","4bce1c94":"markdown","62d03f76":"markdown","6e32db09":"markdown","45f6aa22":"markdown","b0433d31":"markdown","8fdcbc39":"markdown","11ec5c6a":"markdown","23cb2ec6":"markdown","4a6a082a":"markdown","30c9ed57":"markdown","c62c50ec":"markdown","a2f21579":"markdown","7ac53f5f":"markdown","b5aa218f":"markdown","300cf393":"markdown","3650a93e":"markdown","6e5f42ed":"markdown","1acb8ba9":"markdown","58f219d0":"markdown","11b707d5":"markdown","e492695e":"markdown","bf4b0dd2":"markdown","7a4a1c88":"markdown","054bf737":"markdown","75fda271":"markdown"},"source":{"e1cd687f":"#Imports\nimport numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\n# style.use('default')\n\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n# sns.set(font_scale=1.5)\n# %config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n# set display max rows and columns\npd.set_option('display.max_rows',1500)\npd.set_option('display.max_columns',85)","639b9bec":"house_train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',sep=',') # load the data\nhouse_test=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',sep=',') #load the data","7c5032cb":"y=house_train.SalePrice #set the target","3875c947":"X = pd.concat([house_train,house_test],join='inner',ignore_index=True) #Merge the data for easier cleaning","428bee6d":"X.head(2) #Get the head ","2a55ffd1":"X.shape #Get The Shape of Data","db68e105":"X.columns #Get The Data Columns","c2ed8de7":"X.info() #Get the info of data","42ea8d8a":"X.describe() #Give basic stat discription of data","aa71c62b":"X.isnull().sum().sort_values(ascending=False)#Get the null values","e404997d":"# Doing a function for heatmap of missing variables to be called many times in this notebook\ndef missing_heat_map(DataFrame):#Plot a heat map of the missing varibles\n    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (18, 6))\n    sns.heatmap(DataFrame.isnull(), yticklabels=False, ax = ax, cbar=False, cmap='viridis')\n    ax.set_title('Train & Test Concatenated DataSet')\n    plt.show()\nmissing_heat_map(X)","77a7efd0":"#Specify the features that contain NaN to replace it by the string NA\nfeatures_with_na=['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu',\\\n                  'GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\n\nfor i in features_with_na:\n    X[i]=X[i].map(lambda x: 'NA' if pd.isnull(x) else x) # Convert to NA insted of null (Just like the data discription)","ed5d2b33":"# Call the heatmap function again to check what is still missing\nmissing_heat_map(X)","68683ebe":"X.GarageYrBlt.isnull().sum() # Get the sum of of null values","9aef5b9b":"#Grabbing all the features of Garage to try and understand why the year is missing values\nGara_temp=X[['GarageYrBlt','GarageFinish','GarageType','GarageCars','GarageArea','GarageQual','GarageCond','YearBuilt']]\nGara_temp=Gara_temp[pd.isnull(Gara_temp['GarageYrBlt'])]\nGara_temp.shape\nGara_temp.head()","292dc232":"X['GarageYrBlt']=X['GarageYrBlt'].fillna(X['YearBuilt']) #Replace the null values of garage with year built\nX.GarageYrBlt.isnull().sum()# Check the sum of null values ","451b28ed":"X.LotFrontage.isnull().sum() #Check null values","52100b9c":"X.LotFrontage=X.LotFrontage.fillna(X.LotFrontage.median()) #Replace the null values with the median of LotFrontage\nX.LotFrontage.isnull().sum() #Check the null values after change \n## Change after to mean and look at the difference","a261f133":"X.KitchenQual.isnull().sum() #Getting the null values","a7ead6d9":"X.MasVnrType.value_counts() #Getting the values in MasVnrType","2ced57fd":"X.KitchenQual=X.KitchenQual.fillna(X.KitchenQual.mode()[0]) #Filling the missing value with the mode\n# X.KitchenQual.mode()[0]","0141d867":"X.KitchenQual.isnull().sum() #Getting the null values","a72b11be":"X.KitchenQual.value_counts() #Getting the values of KitchenQual","10057573":"X.MasVnrType.isnull().sum() #Getting the sum of null values","19adafd4":"X.MasVnrType.value_counts() #Getting values of MasVnrType","05ae37e3":"# We are going to take the mode for the missing items. Mode is 'None'\nX.MasVnrType=X.MasVnrType.fillna(X.MasVnrType.mode()[0])\n#X.MasVnrType=X.MasVnrType.fillna('None')","b1d036ec":"X.MasVnrType.isnull().sum() #Getting the null value","0154008e":"X.MasVnrType.value_counts() #Getting the values of MasVnrType","89054bcd":"X.MasVnrArea.isnull().sum() #Getting the null values ","5b4739d5":"X.MasVnrArea=X.MasVnrArea.fillna(X.MasVnrArea.mean()) #Replacing the null values with the mean","07cde2d8":"X.MasVnrArea.isnull().sum()#Getting the null values","edb379e2":"X.MSZoning.isnull().sum() #Getting the null values","99189680":"X.MSZoning.value_counts() #Getting the values of MSZoning","7443b67f":"X.MSZoning = X.MSZoning.fillna(X.MSZoning.mode()[0]) #replacing the null values with the mode\n#X.MSZoning=X.MSZoning.fillna('RL')","ea7e9066":"X.MSZoning.isnull().sum()#Getting the null values","0b0eab19":"X.MSZoning.value_counts()#Getting the values of MSZoning","83d78a60":"#these columns don't have effective numbers of null values therefore we applied this function to filter a fill the null values\n#Choosing the desired columns\nrest_of_columns = ['BsmtHalfBath','Functional','Utilities','KitchenQual','Exterior1st','Exterior2nd',\\\n                  'GarageCars','GarageArea','BsmtFinSF1','Electrical','SaleType','TotalBsmtSF','BsmtUnfSF',\\\n                  'BsmtFinSF2','BsmtFullBath']\ndef column_null_cleaner(list_of_columns):\n    for i in list_of_columns: #Iterate over the selected columns\n        \n        if X[i].dtype == 'float64': #If the value of the column is a float replace the null values with the median\n            #print(X[i].dtype)\n            #print(X[i].median())\n            X[i]=X[i].fillna(X[i].median())\n            \n        elif X[i].dtype == 'object': #If the values of the column is object replace the null values with mode\n            #print(i)\n            #print(X[i].dtype)\n            #print((X[i].mode()[0]))\n            X[i]=X[i].fillna(X[i].mode()[0])\n\n            \n                  \n        elif X[i].dtype == 'int64': #If the values of the column is int then replace the null values with median\n            #print(X[i].dtype)\n            #print((X[i].median()))\n            X[i]=X[i].fillna(X[i].median())\n\ncolumn_null_cleaner(rest_of_columns)","0249833e":"#X.BsmtHalfBath = X.BsmtHalfBath.fillna(X.BsmtHalfBath.mode()[0])\n#X.Functional  = X.Functional.fillna('Typ')\n#X.Utilities  = X.Utilities.fillna('AllPub')\n#X.KitchenQual = X.KitchenQual.fillna('TA')\n#X.Exterior1st = X.Exterior1st.fillna('VinylSd')\n#X.Exterior2nd = X.Exterior2nd.fillna('VinylSd')\n#X.GarageCars = X.GarageCars.fillna(X.GarageCars.mode()[0])\n#X.GarageArea = X.GarageArea.fillna(X['GarageArea'].median())\n#X.BsmtFinSF1 = X.BsmtFinSF1.fillna(X['BsmtFinSF1'].median())\n#X.Electrical = X.Electrical.fillna('SBrkr')\n#X.SaleType = X.SaleType.fillna('WD')\n#X.TotalBsmtSF = X.TotalBsmtSF.fillna(X['TotalBsmtSF'].median())\n#X.BsmtUnfSF = X.BsmtUnfSF.fillna(X['BsmtUnfSF'].median())\n#X.BsmtFinSF2 = X.BsmtFinSF2.fillna(X.BsmtFinSF2.mode()[0])\n#X.BsmtFullBath = X.BsmtFullBath.fillna(X.BsmtFullBath.mode()[0])","689456df":"histo_grams = X.hist(bins=20, figsize=(20, 15)) #Histogram of columns","fff196c3":"X.isnull().sum().sort_values(ascending=False).head(35) #Check the null values in all columns","88eec14f":"missing_heat_map(X)","f829c4bc":"# This function will check for outliers and plot the distibutions of data. The input takes the column as string an the dataframe\ndef no_outlier(Data_column,data_set): \n    import math #Import math\n    X = data_set[Data_column] #Set the target column to a new varible\n    no_outlier = [] #Make a list of the features values without outliers\n    confidence = [] #Make a list to get the 90% confidance intervals\n    \n    q1 = float(X.describe()['25%']) #Get the first quartile (Using the describe function)\n    q3 = float(X.describe()['75%']) #Get the third quartile (Using the describe function)\n    iqr = (q3 - q1)*1.5 #Calculate the IQR\n    std = float(X.describe()['std']) #Get the standered deviation\n    mean = float(X.describe()['mean']) #Get the mean\n    lower_limit = mean-(1.645*(std\/math.sqrt(len(X)))) #Compute the upper limit of the 90% interval\n    higher_limit = mean+(1.645*(std\/math.sqrt(len(X)))) #Compute the upper limit of the 90% interval\n    \n    for total in X: #Loop over the target\n        if lower_limit < total < higher_limit: #Get the points within the 90% interval\n            confidence.append(total) #Append it to the list\n        \n        if (q1 - iqr) < (total) < (q3 + iqr): #Get points without the outliers \n            no_outlier.append(total) # Append it to the no_outlier list\n        else:\n            pass\n    print('Tukeys method number of outliers is {}'.format((len(X)-len(sorted(no_outlier)))))\n    print('90% confidence interval has {} values between {} and {}'.format(len(sorted(confidence)),round(lower_limit),round(higher_limit)))\n    \n    #Plot representaions\n    fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(12, 12))\n    sns.distplot(X, ax=ax[0,0])\n    sns.distplot(no_outlier,color='red', ax=ax[0,1])\n    sns.boxplot(X,notch=True,orient='v',ax=ax[1,0])\n    sns.boxplot(no_outlier,notch=True,orient='v',color='red',ax=ax[1,1])\n    \n    fig.suptitle('{}'.format(Data_column), fontsize=24)\n    ax[0,0].set_title('Distribution of {}'.format(Data_column), fontsize=12)\n    ax[0,1].set_title('Distribution of {} after removing outliers'.format(Data_column), fontsize=10)\n    ax[1,0].set_title('Boxplot of {}'.format(Data_column), fontsize=10)\n    ax[1,1].set_title('Boxplot of {} after removing outliers'.format(Data_column), fontsize=10)\n    ","fd53d5a8":"no_outlier('LotFrontage',X)\n","1750e3c7":"no_outlier('MasVnrArea',X) #Applying the function to understand the distribution","4963abc3":"house_train.dtypes","11cd09ea":"#Converting MSSubClass, OverallQual,OverallCond to objects (because they are classifications represented as floats)\nX.MSSubClass=X.MSSubClass.map(lambda x : str(x))\nX.OverallQual=X.OverallQual.map(lambda x : str(x))\nX.OverallCond=X.OverallCond.map(lambda x : str(x))","dc97bbef":"X.shape","a310eb4d":"# Adding the Building Age when it was sold. This can be done by subtracting YrSold-YearBuilt and add month sold\nX['BuildingAge']=X.YrSold-X.YearBuilt \n","5a0fde63":"# Adding Age Since Remodel \nX['RemodelAge']=X.YrSold-X.YearRemodAdd \nX[['RemodelAge']].head(5)\n","93052074":"X.shape","a3398b6a":"z = pd.get_dummies(X,drop_first=True) #Converting the data into dummies to apply to models\nz.head()","d56e6ece":"z.shape","f0e75eea":"#Split based on index\n\nX_train=z.iloc[0:1460,:] #Spliting the train \nX_test=z.iloc[1460:,:] #Spliting the test\n\ny_train=y","aea40c83":"# Drop the ID column from the X_train and X_test\nX_train.drop('Id',axis=1,inplace=True) \nX_test.drop('Id',axis=1,inplace=True)","44cd10f0":"X_train_corr=X_train.copy()\nX_train_corr['SalesPrice']=y #Set the target to the Salesprice in X_train_corr","0734aa4e":"X_train_corr.head()","fc612c0a":"#Grab a list of 20 best positive features based on pairwise correlation\nbest_feature_corr=X_train_corr.corr()['SalesPrice'].sort_values(ascending=False).index[1:20].tolist()\nprint('list of 20 best positive features based on pairwise correlation:\\n',best_feature_corr)","38cf3496":"'SalesPrice' in best_feature_corr","125cbbbc":"best_feature_corr.append('SalesPrice')","90bfb133":"'SalesPrice' in best_feature_corr","e82bd769":"#Make a heat map to visulize the corrilations\nplt.figure(figsize=(18,8))\ncorr=X_train_corr[best_feature_corr].corr()\nsns.heatmap(corr,annot=True, vmin=0, vmax=1, cmap = 'rainbow')\n\n","7eb59cc6":"X_train_corr[['TotalBsmtSF','1stFlrSF']].head(10) #Investigate 'TotalBsmtSF' and '1stFlrSF'","96f970a3":"X_train_corr[['GarageArea','GarageCars']] #Investigate 'GarageArea' and 'GarageCars'\ngrouped_cars = X_train_corr.groupby(['GarageCars'])\ngrouped_cars['GarageArea'].mean()","87a8f165":"# Investigate the Correlation with of the features to SalesPrice\n\ncmap=sns.diverging_palette(5, 250, as_cmap=True)\ncorr = X_train_corr.corr()[['SalesPrice']].head(15)\ndef magnify():\n    \n    return [dict(selector=\"th\",\n                 props=[(\"font-size\", \"7pt\")]),\n            dict(selector=\"td\",\n                 props=[('padding', \"0em 0em\")]),\n            dict(selector=\"th:hover\",\n                 props=[(\"font-size\", \"12pt\")]),\n            dict(selector=\"tr:hover td:hover\",\n                 props=[('max-width', '200px'),\n                        ('font-size', '12pt')])\n]\ncorr.style.background_gradient(cmap, axis=1)\\\n    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n    .set_caption(\"Hover to magify\")\\\n    .set_precision(2)\\\n    .set_table_styles(magnify())","5eaea6c5":"correlation = X_train_corr.corr(method='pearson')\ncolumns_larg = correlation.nlargest(15,'SalesPrice')[['SalesPrice']].head(15)\ncolumns_larg #Grabbing the largest corrilations (Positive)","5f205d12":"correlation = X_train_corr.corr(method='pearson')\ncolumns_smal = correlation.nsmallest(15,'SalesPrice')[['SalesPrice']].head(15)\ncolumns_smal #Grabbing the largest corrilations (Neagitive)","1df0cb5c":"#Making scatter plots for viewing corrilations\nsns.set()\ncols = ['SalesPrice', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(X_train_corr[cols], size = 2.5)\nplt.show();","a852cc7c":"#Positive corr(red) , Negitive Corr(blue) \nfig, ax = plt.subplots(ncols=2, nrows=3, figsize=(12, 12)) #Intoduce a figure that includes 4 graphs\n\n#plot the graph and set to axs\nsns.regplot('GrLivArea', 'SalesPrice', data=X_train_corr, fit_reg=False,color='red', ax=ax[0,0]) \nsns.regplot('GarageArea', 'SalesPrice', data=X_train_corr, fit_reg=False,color='red', ax=ax[0,1])\nsns.regplot('YearBuilt', 'SalesPrice', data=X_train_corr, fit_reg=False,color='red', ax=ax[1,0])\nsns.regplot('BuildingAge', 'SalesPrice', data=X_train_corr, fit_reg=False,color='blue', ax=ax[1,1])\nsns.regplot('YearRemodAdd', 'SalesPrice', data=X_train_corr, fit_reg=False,color='red', ax=ax[2,0])\nsns.regplot('RemodelAge', 'SalesPrice', data=X_train_corr, fit_reg=False,color='blue', ax=ax[2,1])\n\n\n\nplt.show()","ce7759cb":"scaler = StandardScaler() #Call the StandardScaler\nscaler.fit(X_train) #Fit x_train to the scaler","d95f41c1":"import joblib # import joblib \n\n# Saving the transformation, a good ML practice\njoblib.dump(scaler, 'scaling_transformation.pkl')\nprint('transformation saved as scaling_transformation.pkl')","847e6f8d":"# Loading saved transformation \nscaler = joblib.load('scaling_transformation.pkl') \nprint('Saved transformation in loaded.')","415c2fc7":"# transforming features \nX_train_ss = pd.DataFrame(scaler.transform(X_train),columns=X_train.columns)\nX_test_ss = pd.DataFrame(scaler.transform(X_test),columns=X_test.columns)\nprint(\"scaled features are in 'X_train_ss' and 'X_test_ss'\")\n\n# X_train_ss = scaler.transform(X_train)\n# X_test_ss = scaler.transform(X_test)","e9f23302":"(len(X_train_ss),len(X_test_ss)) #Compairing the shape of test and train","158f0066":"#Make month map to try and understand the data\nmonth_map = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \n             7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'} \nX_train_corr['Month_sold'] = X_train_corr['MoSold'].map(month_map)","3d450837":"fig = plt.figure(figsize=(14,8)) #introduce a figure and set the figure size\nax = fig.gca() # create the axis\nsns.barplot(x=\"Month_sold\", y=\"SalesPrice\",hue='YrSold',data=X_train_corr,ax=ax,ci=None) #plot the graph\nax.set_title('Month_sold\/SalesPrice')# Set the title","8d5ee724":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import LinearRegression,RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor","88845243":"lm = LinearRegression()\nlm.fit(X_train_ss,y_train)\n\nprint('Training Score :' , lm.score(X_train_ss,y_train))","e45ffb2b":"ridge_cv = RidgeCV(cv=5)\nridge_cv.fit(X_train_ss, y_train)\n\nprint('Training Score :' , ridge_cv.score(X_train_ss , y_train))","c4ffc4ca":"lasso_alphas = np.arange(1,200, 0.05)\nlasso_cv = LassoCV(cv=5,alphas=lasso_alphas)\nlasso_cv.fit(X_train_ss, y_train)\n\nprint('Training Score :' , lasso_cv.score(X_train_ss , y_train))","2cc11ad3":"elastic_cv = ElasticNetCV(cv=5)\nelastic_cv.fit(X_train_ss, y_train)\n\nprint('Training Score :' , elastic_cv.score(X_train_ss , y_train))","09656876":"forest_cv = RandomForestRegressor(n_jobs=-1)\nforest_cv.fit(X_train_ss,y_train)\n\nprint('Train Score :' , forest_cv.score(X_train_ss , y_train))","425bfa8f":"param_grid = {'n_neighbors' : [3,4,5,6,7,8,9,10,15]}\n\ngrid_knn = GridSearchCV(KNeighborsRegressor(), param_grid, cv=5, refit=True, verbose=1)\ngrid_knn.fit(X_train_ss, y_train)\nprint('Training Score :' , grid_knn.score(X_train_ss , y_train))","ab9c0bb3":"y_test=forest_cv.predict(X_test_ss)\nSub1 = [x for x in range (1461,2920)]\nSubmission1 = {'Id':Sub1,\n               'SalePrice':y_test}\ndf_submission = pd.DataFrame(Submission1)\ndf_submission.to_csv('submission',index=False)","a1af881b":"### Doing Correlation","024ca300":"When we speak about home most people have an idea of what they want in a place called home.There are many features that contribute to the value of a house. In order to estimate the value of a home we will try and analyze the top features that has an effect on the house value.\nWe are aiming to create a prediction model that will analyze the features and give an overall prediction of the house value.This will give us the ability to predict a house value with only the features.\n\n**This solution will contribute to solving many problems:**\n- Any home buyer can have a value approximation of their dream house.\n- The Real-estate agent will have an expected value of a house given it's features.\n- Real-estate investors can have an idea about the most desired features that contibute to the properties value.\n\nThe data obtained has 79 explanatory variables describing (almost) every aspect of residential homes. The data was gathered in Ames, Iowa.We will analize the data,clean it,visulize it then fit it in a model to get a prediction of the expected `SalePrice` of the house.","c00dd350":"##### Cleaning the rest of data","d67d051a":"##### Cleaning KitchenQual","67f3adcc":"### Imports","a52a7abe":"## Fit the model","31f14f68":"#### Ridge CV","516c32c3":"### Data Exploration","7afc7a51":"### Rescale the variables","328aec15":"## Checking Missing Values and Do Cleaning","fd2b55b3":"**From the heat map we concluded that the top corrilated features with `SalesPrice` are ,`GrlivArea`,`GarageCars`,`GarageArea`,`TotalBsmtSF`**","4a489951":"#### Elastic","32707f17":"**A lot of the data has null values which need to be dealt with**\n**After inspecting the data and the Data Discription document we relized that there are many features that have NA values which is intepreted by python as `Nan(Null)`. We are going to convert them back to `string:NA`**\n\n|Feature|Feature|Feature|\n|---|---|---|\n|Alley|BsmtQual|BsmtCond|\n|BsmtExposure|BsmtFinType1|BsmtFinType2|\n|FireplaceQu|GarageType|GarageFinish|\n|GarageQual|GarageCond|PoolQC|\n|Fence|MiscFeature||\n","06fca5bc":"#### KNN Regressor","accfee64":"### The best prediction","a1ae6129":"**MasVnrType is catagorical data which means that the removel of outliers is unnessary**","1c39d295":"##### Cleaning GarageYrBlt","4bce1c94":"### After doing all the models above, we got the best result with Random Forest (0.9708189495134608). We are submitting this result to the kaggle site.","62d03f76":"### Train-Test Split Data back","6e32db09":"## Exploratory Data Analysis EDA","45f6aa22":"##### Cleaning MSZoning","b0433d31":"**Analyzing the relationship between `GarageArea`,`GarageCars` It seems that one car takes around 300 square meters The number of cars that fit into the garage is a consequence of the garage area. Therefore we don't need both features since the both convay the same thing.**","8fdcbc39":"#### Lasso","11ec5c6a":"### Load Data","23cb2ec6":"#### Linear Regression","4a6a082a":"**After analyzing the outlier distribution we relized that we can't use Tukey's method for outline detection because after applying it on the 'LotFrontage' column we got 275 outliers which is alot of data being lost. Therefore we will keep the distribution of data the way it is.**","30c9ed57":"### Checking and Fixing Data Types","c62c50ec":"### Checking Outliers","a2f21579":"**After checking `TotalBsmtSF` and `1stFlrSF` we concluded that these features are extremly similar. The only difference is wether the house has a basment or not**","7ac53f5f":"## Problem Statement","b5aa218f":"##### Cleaning MasVnrType","300cf393":"### Creating Dummy ","3650a93e":"##### Cleaning MasVnrArea","6e5f42ed":"**Done with cleaning data**","1acb8ba9":"**`GarageYrBlt` with NaN value actually means that a garage was never built as shown in the above table.`GarageYrBlt` will not affect the sale price but to make the data heatmap clean and to make sure that we did not miss any missing values, we are going to fill the empty data with the YearBuilt Value.**","58f219d0":"#### Random Forest","11b707d5":"### Team Name on Kaggle: HBE\n**Members:**\nBader Abanmi, Husain Al-Amer , Ebrahim Balghunaim\n\n<span style=\"color:blue\"> **Link of Kaggle:** <\/span>   \nhttps:\/\/www.kaggle.com\/hma2022\/kernelbb47aaf6e5?scriptVersionId=22926066","e492695e":"##### Cleaning LotFrontage","bf4b0dd2":"### Merge Data\n- **We need to concat the two datasets to make data cleaning on all of the two datasets.**","7a4a1c88":"##### Data Engineering","054bf737":"**After analyzing the outlier distribution we relized that we can't use Tukey's method for outline detection because after applying it on the 'MasVnrArea' column we got 202 outliers which is alot of data being lost. Therefore we will keep the distribution of data the way it is.**","75fda271":"# House Prices: Advanced Regression Techniques (Project2)"}}