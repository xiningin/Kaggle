{"cell_type":{"e1f163ba":"code","d99c19ef":"code","713229cf":"code","0893358c":"code","43d45c5d":"code","82058de2":"code","860b7302":"code","b2c99532":"code","38743c6b":"code","77e92a21":"code","7f74115f":"code","5d2f67bf":"code","56492d9d":"code","24ee522a":"code","3bce98f4":"code","35a07e16":"code","63650423":"code","7e87e9c5":"code","41e98d95":"code","ab6b7414":"code","a72b9657":"code","a4e3de6e":"code","e86f06c2":"code","40a44be7":"code","ad0c59ea":"code","0fb61c02":"code","774ea6c3":"code","643d3703":"code","5c640467":"code","fe9090c9":"code","39448101":"code","000116fe":"code","71ecbce1":"markdown","0dfa0187":"markdown","90fdeb9e":"markdown","14f5feea":"markdown","9e41c7da":"markdown","42502231":"markdown"},"source":{"e1f163ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d99c19ef":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n#Now keras libraries\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\nfrom tensorflow.keras.models import Model\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nimport seaborn as sns \nplt.style.use('ggplot')","713229cf":"fake_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')","0893358c":"fake_df.head()","43d45c5d":"real_df.head()","82058de2":"# Check null values \nreal_df.isnull().sum()","860b7302":"fake_df.isnull().sum()","b2c99532":"#Check Data type\nreal_df.subject.unique()","38743c6b":"fake_df.subject.unique()","77e92a21":"fake_df.info()","7f74115f":"real_df.info()","5d2f67bf":"fake_df.drop(['date', 'subject'], axis=1, inplace=True)\nreal_df.drop(['date', 'subject'], axis=1, inplace=True)","56492d9d":"fake_df['class'] = 0 \nreal_df['class'] = 1","24ee522a":"plt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df), color='red')\nplt.bar('Real News', len(real_df), color='green')\nplt.title('Distribution of Fake News and Real News', size=15)\nplt.xlabel('News Type', size=15)\nplt.ylabel('# OF News Articles', size=15)","3bce98f4":"print('Difference in news articles:',len(fake_df)-len(real_df))","35a07e16":"news_df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)\nnews_df","63650423":"news_df['text'] = news_df['title'] + news_df['text']\nnews_df.drop('title', axis=1, inplace=True)","7e87e9c5":"features = news_df['text']\ntargets = news_df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.20, random_state=18)","41e98d95":"def normalize(data):\n    normalized = []\n    for i in data:\n        i = i.lower()\n        # get rid of urls\n        i = re.sub('https?:\/\/\\S+|www\\.\\S+', '', i)\n        # get rid of non words and extra spaces\n        i = re.sub('\\\\W', ' ', i)\n        i = re.sub('\\n', '', i)\n        i = re.sub(' +', ' ', i)\n        i = re.sub('^ ', '', i)\n        i = re.sub(' $', '', i)\n        normalized.append(i)\n    return normalized\n\nX_train = normalize(X_train)\nX_test = normalize(X_test)","ab6b7414":"max_vocab = 10000\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(X_train)","a72b9657":"# tokenize the text into vectors \nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","a4e3de6e":"#Build RNN modle\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_vocab, 32),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","e86f06c2":"X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)","40a44be7":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])","ad0c59ea":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Loss', size=20)\nplt.legend(prop={'size': 20})\nplt.show()\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Accuracy', size=20)\nplt.legend(prop={'size': 20})\nplt.ylim((0.5,1))\nplt.show()","0fb61c02":"model.evaluate(X_test, y_test)","774ea6c3":"pred = model.predict(X_test)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0) ","643d3703":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))","5c640467":"matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(16, 9))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=20)\nax.set_ylabel('True Labels', size=20)\nax.set_title('Confusion Matrix', size=20) \nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15)","fe9090c9":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)","39448101":"word_index = list(tokenizer.word_index.keys())\nword_index = word_index[:max_vocab-1]","000116fe":"import io\n\nout_v = io.open('fakenews_vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('fakenews_meta.tsv', 'w', encoding='utf-8')\n\nfor num, word in enumerate(word_index):\n  vec = weights[num+1] # skip 0, it's padding.\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\nout_v.close()\nout_m.close()","71ecbce1":"# Pippline\n* Import \n* Data EDA\n* Train an LSTM Model\n* Evaluate trained model performance","0dfa0187":"# Train an LSTM Model\n","90fdeb9e":"# Data EDA An Feature Engineering","14f5feea":"# Evaluate trained model performance","9e41c7da":"* NLP Engineering","42502231":"# Import "}}