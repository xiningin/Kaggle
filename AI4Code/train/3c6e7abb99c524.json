{"cell_type":{"0b00306d":"code","9f888de1":"markdown"},"source":{"0b00306d":"#Import the word_tokenize and sentence_bleu methods from NLTK.\nfrom nltk import word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu\n\n#Enter the machine-translated sentence (hypothesis) and the human reference translation (reference).\nhypothesis = input(\"\\nPlease enter a machine-translated sentence and confirm with Enter: \")\nreference = input(\"\\nPlease enter the corresponding human reference translation and confirm with Enter: \")\n\n#Tokenize the two sentences and store the individual tokes in two lists (one for the hypothesis tokens and one for the reference tokens).\nhypothesis = word_tokenize(hypothesis)\nreference = word_tokenize(reference)\n\n#Calculate and print BLEU-1, using 1-grams as highest-order n-grams \n#Reference is placed in [square brackets] because you can score the machine-translated sentence against multiple references (applies to all BLEU score calculations listed here).\n#The weights are set so that calculation is based solely on 1-gram precision.\nBLEU_1 = sentence_bleu([reference], hypothesis, weights=(1, 0, 0, 0))\nprint(f\"\\nThe score for BLEU-1 is: {BLEU_1}\")\n\n#Calculate and print BLEU-2, using 2-grams as highest-order n-grams.\n#The weights are set so that calculation is based on 1-gram and 2-gram precisions.\nBLEU_2 = sentence_bleu([reference], hypothesis, weights=(0.5, 0.5, 0, 0))\nprint(f\"\\nThe score for BLEU-2 is: {BLEU_2}\")\n\n#Calculate and print BLEU-3, using 3-grams as highest-order n-grams.\n#The weights are set so that calculation is based on 1-gram, 2-gram and 3-gram precisions.\nBLEU_3 = sentence_bleu([reference], hypothesis, weights=(0.33, 0.33, 0.33, 0))\nprint(f\"\\nThe score for BLEU-3 is: {BLEU_3}\")\n\n#Calculate and print BLEU-4, using 4-grams as highest-order n-grams.\n#The weights are set so that calculation is based on 1-gram, 2-gram, 3-gram and 4-gram precisions.\nBLEU_4 = sentence_bleu([reference], hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\nprint(f\"\\nThe score for BLEU-4 is: {BLEU_4}\")\n\n#Calculate and print BLEU-5, using 5-grams as highest-order n-grams.\n#The weights are set so that calculation is based on 1-gram, 2-gram, 3-gram, 4-gram and 5-gram precisions.\nBLEU_5 = sentence_bleu([reference], hypothesis, weights=(0.2, 0.2, 0.2, 0.2, 0.2))\nprint(f\"\\nThe score for BLEU-5 is: {BLEU_5}\")","9f888de1":"# NLTK BLEU Score Calculator\nThis is a very basic notebook for quick-and-dirty sentence-based BLEU score calculation using the [Natural Language Toolkit (NLTK)](https:\/\/www.nltk.org\/) library. Remember that BLEU is a corpus-based metric which is not intended to be used for scoring individual sentences but rather for scoring complete texts. However, for didactic purposes, it it easier to illustrate the BLEU score algorithm when focusing on a single sentence pair.<p>\nWhen you run the code below, you are asked to provide a machine-translated sentence (hypothesis) and the corresponding human translation (reference). Then, BLEU-1 to BLEU-5 are calculated and printed for this sentence pair. If you observe that a higher-order BLEU score receives a higher value than the lower-order scores, this value is no longer reliable (the first time this happens, you also see the following warning: \"BLEU scores might be undesirable; use SmoothingFunction().\"). For example, if BLEU-4 has a higher score than BLEU-3, this means that no 4-gram precisions could be identified in the sentence pair provided (which means that actually this BLEU score should be 0). The technical details of this phenomenon do not have to concern us here (however, if you're curious, see [this discussion](https:\/\/github.com\/nltk\/nltk\/issues\/1838)). Just remember that if this phenomenon occurs, don't trust the corresponding score.<br><p>\nNOTE: Before running the code below, set the Editor Theme to 'Dark' (View \u2192 Editor Theme \u2192 Dark). Otherwise, you won't see the text prompting you to enter the sentences. "}}