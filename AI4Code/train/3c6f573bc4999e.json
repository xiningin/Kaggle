{"cell_type":{"ede9bc52":"code","0b08103f":"code","46bde7dd":"code","ef5d661e":"code","49bdfb02":"code","57936725":"code","fcf791a1":"code","ab9d2dac":"code","8044eb18":"code","36c04b60":"code","d57b1e90":"code","59f7be45":"code","26fce899":"code","2b206118":"code","6ca18221":"code","46572476":"code","a53d8495":"code","95562799":"code","a2527e7b":"code","44b50649":"code","36ec15c0":"code","d0443dd3":"code","26ffc44f":"code","73d0bfd7":"code","5deae9cc":"code","5d65847c":"code","1fe4a888":"code","b6bb7510":"code","3daffadb":"code","9a9776a2":"code","19f5cf66":"code","8d5ff01c":"code","6b5b4c27":"code","8de2ce0f":"code","63c26f01":"code","7d0fa176":"code","f73d9c99":"code","7021a89e":"code","0be6d74c":"code","ad8c2f39":"code","10e4c9e7":"code","103c76e0":"code","7239dcb5":"code","b567a91e":"code","14d0ce9e":"code","6cb23210":"code","ae4654c0":"code","f2672f3f":"code","1fafac37":"code","e828deab":"code","d09b9a10":"code","cb54c87f":"code","809d6edc":"code","0f166480":"code","9c30fa40":"code","5fa9ab95":"code","e5509e73":"code","734b2aaf":"code","8e7f3462":"code","a2f362e6":"code","c9350220":"code","bd3ac805":"code","d830ee4d":"code","9ca4d0d3":"markdown","67096ae6":"markdown","5c35aaae":"markdown","3c187b30":"markdown","925ac155":"markdown","91f9620d":"markdown","78a61c9b":"markdown","3a48a1f7":"markdown","6fd0a9b9":"markdown","01bbfeaf":"markdown","c066a0c5":"markdown","a7379f26":"markdown"},"source":{"ede9bc52":"import riiideducation\n# import dask.dataframe as dd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nenv = riiideducation.make_env()","0b08103f":"train = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv',\n                   usecols=[1, 2, 3, 4, 5, 7, 8, 9],\n                   dtype={'timestamp': 'int64',\n                          'user_id': 'int32',\n                          'content_id': 'int16',\n                          'content_type_id': 'int8',\n                          'task_container_id': 'int16',\n                          'answered_correctly':'int8',\n                          'prior_question_elapsed_time': 'float32',\n                          'prior_question_had_explanation': 'boolean'}\n                   )","46bde7dd":"#reading in question df\nquestions_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/questions.csv',                         \n                            usecols=[0, 3],\n                            dtype={'question_id': 'int16',\n                              'part': 'int8'}\n                          )","ef5d661e":"#removing True or 1 for content_type_id\n\ntrain = train[train.content_type_id == False].sort_values('timestamp').reset_index(drop = True)","49bdfb02":"train[(train.task_container_id == 9999)].tail()","57936725":"train[(train.content_type_id == False)].task_container_id.nunique()","fcf791a1":"#saving value to fillna\nelapsed_mean = train.prior_question_elapsed_time.mean()\n","ab9d2dac":"group1 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['count'])\ngroup1.columns = ['avg_questions']\ngroup2 = train.loc[(train.content_type_id == False), ['task_container_id', 'user_id']].groupby(['task_container_id']).agg(['nunique'])\ngroup2.columns = ['avg_questions']\ngroup3 = group1 \/ group2","8044eb18":"group3['avg_questions_seen'] = group3.avg_questions.cumsum()","36c04b60":"group3.iloc[0].avg_questions_seen","d57b1e90":"results_u_final = train.loc[train.content_type_id == False, ['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_final.columns = ['answered_correctly_user']\n\nresults_u2_final = train.loc[train.content_type_id == False, ['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_final.columns = ['explanation_mean_user']","59f7be45":"results_u2_final.explanation_mean_user.describe()","26fce899":"train = pd.merge(train, questions_df, left_on = 'content_id', right_on = 'question_id', how = 'left')","2b206118":"results_q_final = train.loc[train.content_type_id == False, ['question_id','answered_correctly']].groupby(['question_id']).agg(['mean'])\nresults_q_final.columns = ['quest_pct']","6ca18221":"results_q2_final = train.loc[train.content_type_id == False, ['question_id','part']].groupby(['question_id']).agg(['count'])\nresults_q2_final.columns = ['count']","46572476":"question2 = pd.merge(questions_df, results_q_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","a53d8495":"question2 = pd.merge(question2, results_q2_final, left_on = 'question_id', right_on = 'question_id', how = 'left')","95562799":"question2.quest_pct = round(question2.quest_pct,5)","a2527e7b":"display(question2.head(), question2.tail())","44b50649":"train.head()","36ec15c0":"len(train)","d0443dd3":"len(train)","26ffc44f":"train.answered_correctly.mean()","73d0bfd7":"prior_mean_user = results_u2_final.explanation_mean_user.mean()","5deae9cc":"train.loc[(train.timestamp == 0)].answered_correctly.mean()","5d65847c":"train.loc[(train.timestamp != 0)].answered_correctly.mean()","1fe4a888":"train.drop(['timestamp', 'content_type_id', 'question_id', 'part'], axis=1, inplace=True)","b6bb7510":"len(train)","3daffadb":"validation = train.groupby('user_id').tail(5)\ntrain = train[~train.index.isin(validation.index)]\nlen(train) + len(validation)","9a9776a2":"validation.answered_correctly.mean()","19f5cf66":"train.answered_correctly.mean()","8d5ff01c":"results_u_val = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_val.columns = ['answered_correctly_user']\n\nresults_u2_val = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_val.columns = ['explanation_mean_user']","6b5b4c27":"X = train.groupby('user_id').tail(18)\ntrain = train[~train.index.isin(X.index)]\nlen(X) + len(train) + len(validation)","8de2ce0f":"X.answered_correctly.mean()","63c26f01":"train.answered_correctly.mean()","7d0fa176":"results_u_X = train[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean'])\nresults_u_X.columns = ['answered_correctly_user']\n\nresults_u2_X = train[['user_id','prior_question_had_explanation']].groupby(['user_id']).agg(['mean'])\nresults_u2_X.columns = ['explanation_mean_user']","f73d9c99":"#clearing memory\ndel(train)","7021a89e":"X = pd.merge(X, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nX = pd.merge(X, results_u_X, on=['user_id'], how=\"left\")\nX = pd.merge(X, results_u2_X, on=['user_id'], how=\"left\")","0be6d74c":"validation = pd.merge(validation, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\nvalidation = pd.merge(validation, results_u_val, on=['user_id'], how=\"left\")\nvalidation = pd.merge(validation, results_u2_val, on=['user_id'], how=\"left\")","ad8c2f39":"from sklearn.preprocessing import LabelEncoder\n\nlb_make = LabelEncoder()\n\nX.prior_question_had_explanation.fillna(False, inplace = True)\nvalidation.prior_question_had_explanation.fillna(False, inplace = True)\n\nvalidation[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(validation[\"prior_question_had_explanation\"])\nX[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(X[\"prior_question_had_explanation\"])","10e4c9e7":"#reading in question df\n#question2 = pd.read_csv('\/kaggle\/input\/question2\/question2.csv)","103c76e0":"content_mean = question2.quest_pct.mean()\n\nquestion2.quest_pct.mean()\n#there are a lot of high percentage questions, should use median instead?","7239dcb5":"#filling questions with no info with a new value\nquestion2.quest_pct = question2.quest_pct.mask((question2['count'] < 3), .65)\n\n\n#filling very hard new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct < .2) & (question2['count'] < 21), .2)\n\n#filling very easy new questions with a more reasonable value\nquestion2.quest_pct = question2.quest_pct.mask((question2.quest_pct > .95) & (question2['count'] < 21), .95)","b567a91e":"X = pd.merge(X, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nvalidation = pd.merge(validation, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\nX.part = X.part - 1\nvalidation.part = validation.part - 1","14d0ce9e":"X.head()","6cb23210":"y = X['answered_correctly']\nX = X.drop(['answered_correctly'], axis=1)\nX.head()\n\ny_val = validation['answered_correctly']\nX_val = validation.drop(['answered_correctly'], axis=1)","ae4654c0":"X = X[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']]\nX_val = X_val[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n       'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']]","f2672f3f":"\n# Filling with 0.5 for simplicity; there could likely be a better value\nX['answered_correctly_user'].fillna(0.65,  inplace=True)\nX['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX['quest_pct'].fillna(content_mean, inplace=True)\n\nX['part'].fillna(4, inplace = True)\nX['avg_questions_seen'].fillna(1, inplace = True)\nX['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX['prior_question_had_explanation_enc'].fillna(0, inplace = True)\n","1fafac37":"X_val['answered_correctly_user'].fillna(0.65,  inplace=True)\nX_val['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\nX_val['quest_pct'].fillna(content_mean,  inplace=True)\n\nX_val['part'].fillna(4, inplace = True)\nX['avg_questions_seen'].fillna(1, inplace = True)\nX_val['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\nX_val['prior_question_had_explanation_enc'].fillna(0, inplace = True)","e828deab":"import lightgbm as lgb\n\ny_true = np.array(y_val)\nlgb_train = lgb.Dataset(X, y, categorical_feature = ['part', 'prior_question_had_explanation_enc'],free_raw_data=False)\nlgb_eval = lgb.Dataset(X_val, y_val, categorical_feature = ['part', 'prior_question_had_explanation_enc'], reference=lgb_train, free_raw_data=False)","d09b9a10":"def objective(trial):    \n    params = {\n            'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n            'boosting_type': 'gbdt',\n            'max_bin': trial.suggest_int('max_bin', 700, 900),\n            'objective': 'binary',\n            'metric': 'auc',\n            'max_depth': trial.suggest_int('max_depth', 4, 16),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 16),\n            'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n            'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n            'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n            'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 1.0),\n            'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 1.0),\n            'early_stopping_rounds': 10\n            }\n    model = lgb.train(params, lgb_train, valid_sets=[lgb_train, lgb_eval], verbose_eval=1000, num_boost_round=1300)\n    val_pred = model.predict(X_val)\n    score = roc_auc_score(y_true, val_pred)\n    print(f\"AUC = {score}\")\n    return score","cb54c87f":"import optuna","809d6edc":"# Bayesian optimization\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=5)","0f166480":"print('Number of finished trials: {}'.format(len(study.trials)))\n\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('  Value: {}'.format(trial.value))\n\nprint('  Params: ')\nfor key, value in trial.params.items():\n    print('    {}: {}'.format(key, value))","9c30fa40":"# plot history\nfrom optuna.visualization import plot_optimization_history\nplot_optimization_history(study)","5fa9ab95":"model = lgb.train(trial.params, lgb_train, valid_sets=[lgb_train, lgb_eval], verbose_eval=1000,num_boost_round=1300)","e5509e73":"y_pred = model.predict(X_val)\nroc_auc_score(y_true, y_pred)","734b2aaf":"import matplotlib.pyplot as plt\nimport seaborn as sns","8e7f3462":"#displaying the most important features by split\nlgb.plot_importance(model)\nplt.show()","a2f362e6":"#displaying the most important features by gain\nlgb.plot_importance(model, importance_type = 'gain')\nplt.show()","c9350220":"iter_test = env.iter_test()","bd3ac805":"for (test_df, sample_prediction_df) in iter_test:\n    test_df['task_container_id'] = test_df.task_container_id.mask(test_df.task_container_id > 9999, 9999)\n    test_df = pd.merge(test_df, group3, left_on=['task_container_id'], right_index= True, how=\"left\")\n    test_df = pd.merge(test_df, question2, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    test_df = pd.merge(test_df, results_u_final, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_u2_final, on=['user_id'],  how=\"left\")\n    test_df['answered_correctly_user'].fillna(0.65,  inplace=True)\n    test_df['explanation_mean_user'].fillna(prior_mean_user,  inplace=True)\n    test_df['quest_pct'].fillna(content_mean,  inplace=True)\n    test_df['part'] = test_df.part - 1\n\n    test_df['part'].fillna(4, inplace = True)\n    test_df['avg_questions_seen'].fillna(1, inplace = True)\n    test_df['prior_question_elapsed_time'].fillna(elapsed_mean, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    \n    test_df['answered_correctly'] =  model.predict(test_df[['answered_correctly_user', 'explanation_mean_user', 'quest_pct', 'avg_questions_seen',\n                                                            'prior_question_elapsed_time','prior_question_had_explanation_enc', 'part']])\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","d830ee4d":"#students don't appear in every task container ID what can I do about this, can't always follow sequentially?","9ca4d0d3":"## Reading Data and Importing Libraries ##","67096ae6":"## Modeling ##","5c35aaae":"## Examining Feature Importance ##","3c187b30":"You can try more trials than 5...","925ac155":"## Creating Validation Set (Most Recent Answers by User) ##","91f9620d":"Affirmatives (True) for content_type_id are only for those with a different type of content (lectures). These are not real questions.","78a61c9b":"## Making Predictions for New Data ##","3a48a1f7":"## Extracting Training Data ##","6fd0a9b9":"## Merging Data ##","01bbfeaf":"## Data Exploration ##","c066a0c5":"Does it make sense to use last questions as validation? Why is the rate of correct answers so low?\nI am convinced there is a better way to match the test data.","a7379f26":"This is a small modification and combination of these two notebooks\n1. https:\/\/www.kaggle.com\/takamotoki\/lgbm-iii-part2 \n2. https:\/\/www.kaggle.com\/code1110\/riiid-lgb-hyperparameter-tuning\nHope that it will help you guys..."}}