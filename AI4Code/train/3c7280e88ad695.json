{"cell_type":{"2c7f0833":"code","4e938c62":"code","434e563c":"code","e609709c":"code","01472abf":"code","87360c7c":"code","dad8e169":"code","d5eed177":"code","30cd22b6":"code","3f9f30cb":"code","1b2eeab5":"code","c044fe9a":"code","b9669da1":"code","174b934b":"code","7206e081":"code","228f4dd4":"code","aab8fd6d":"code","3e6f0d3e":"code","1843f7b2":"code","b57f18d4":"code","9037f767":"code","f50492b6":"code","8f0cbe29":"code","bcb0a886":"code","8225fcfa":"code","ca64adfb":"code","a7de2ffa":"code","d95e7a12":"code","c7ddd3a7":"code","9022aeb8":"code","76391146":"code","2ef4003c":"code","2e7caccb":"code","bb24f3d4":"code","38d71feb":"code","28fa1dc3":"code","4d6c5dd2":"code","d765bf8b":"code","a0af0498":"code","351b9b19":"code","25ad21b5":"code","28665051":"code","59297a88":"code","147d0eec":"code","b9831a9e":"code","fe0c626a":"code","5cd3ec21":"code","a274c354":"code","b12cb476":"code","e0b852d4":"code","60005fec":"markdown","840c2a56":"markdown","a2b54572":"markdown","b19d189e":"markdown","a4f55d65":"markdown","83b96014":"markdown","7c53e1b7":"markdown","9e132509":"markdown","d4c0c741":"markdown","c1834f45":"markdown","afa9a84a":"markdown","ec225a60":"markdown","08cd79cb":"markdown","b87d5116":"markdown","d0bf7ad2":"markdown","99ce183d":"markdown","615e8150":"markdown"},"source":{"2c7f0833":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.options.mode.chained_assignment = None  # default='warn'","4e938c62":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","434e563c":"train.head()","e609709c":"test.head()","01472abf":"train.info()","87360c7c":"import scipy.stats as stats\nfrom scipy.stats import norm","dad8e169":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(train['SalePrice'])\n\nfig, axes = plt.subplots(1,2,figsize = (20,6))\nsns.distplot(train['SalePrice'], kde = True, hist=True, fit = norm, ax = axes[0])\nsns.distplot(np.log(train.iloc[:,-1].values), kde=True,hist=True,ax=axes[1], fit = norm)\naxes[0].set_title('SalePrice distribution vs Normal Distribution', fontsize = 13)\naxes[1].set_title('SalePrice After Apply Log Transform', fontsize = 13)\naxes[0].set_xlabel(\"House's sale Price in $\", fontsize = 12)\naxes[1].set_xlabel(\"House's sale Price in $\", fontsize = 12)\naxes[0].legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n               loc='best')\nplt.show()","d5eed177":"corrmatrix = train.corr()\nplt.figure(figsize = (10,6))\ncolumnss = corrmatrix.nlargest(10, \"SalePrice\")[\"SalePrice\"].index\ncm = np.corrcoef(train[columnss].values.T)\nsns.set(font_scale = 1.1)\nhm = sns.heatmap(cm, cbar = True, annot = True, square = True, cmap = \"RdPu\" ,  fmt = \".2f\", annot_kws = {\"size\": 10},\n                 yticklabels = columnss.values, xticklabels = columnss.values)\nplt.show()","30cd22b6":"# OverallQuall\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=train, x = 'OverallQual', y='SalePrice', ax = ax[0])\nsns.violinplot(data=train, x = 'OverallQual', y='SalePrice', ax = ax[1])\nsns.boxplot(data=train, x = 'OverallQual', y='SalePrice', ax = ax[2])\nplt.show()","3f9f30cb":"# GrLivArea\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.regplot(x = train[\"GrLivArea\"], y = train[\"SalePrice\"],ax=ax[0])\nsns.boxplot(train[\"GrLivArea\"], ax=ax[1])\nsns.distplot(train[\"GrLivArea\"],ax=ax[2])\nplt.show()","1b2eeab5":"# GarageCars\nfigure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=train, x = 'GarageCars', y='SalePrice', ax = ax[0])\nsns.violinplot(data=train, x = 'GarageCars', y='SalePrice', ax = ax[1])\nsns.boxplot(data=train, x = 'GarageCars', y='SalePrice', ax = ax[2])\nplt.show()","c044fe9a":"Y_train = train['SalePrice']\nX_train = train.drop(columns=['Id','SalePrice'],axis=1)\nX_test = test.drop(columns=['Id'],axis=1)\n\ntrain_test = pd.concat([X_train,X_test],axis=0).reset_index(drop=True)\n\nnumerical_feat = train_test.select_dtypes(include=[np.number])\ncategorical_feat = train_test.select_dtypes(exclude=[np.number])","b9669da1":"from scipy import stats\n# calculate zscore\nz = np.abs(stats.zscore(numerical_feat))\nr_outlier,c_outlier = np.where(z>3)\n\n# change the value using its mean\nfor i in range(c_outlier.shape[0]):\n    numerical_feat.iloc[r_outlier[i],c_outlier[i]] = numerical_feat.iloc[:,c_outlier[i]].mean()","174b934b":"# listing nan value to the dataframe\n\nnum_nan = pd.DataFrame(numerical_feat.isna().sum(),columns=['NaN_count'])\nnum_nan['Variable'] = num_nan.index\nnum_nan['Percentage'] = (num_nan['NaN_count']\/1460)*100      # calculate percentage NaN_count based on length of train data\n\ncat_nan = pd.DataFrame(categorical_feat.isna().sum(),columns=['NaN_count'])\ncat_nan['Variable'] = cat_nan.index\ncat_nan['Percentage'] = (cat_nan['NaN_count']\/1460)*100      # calculate percentage NaN_count based on length of train data\n\ndf_nan = pd.concat([num_nan,cat_nan],axis=0)\ndf_nan = df_nan.sort_values(by = ['NaN_count'])\ndf_nan['Decision'] = np.where(df_nan['Percentage'] > 20, 'Discard', 'Keep')\ndf_nan","7206e081":"idx_del = np.where(df_nan['Decision'] == 'Discard')[0]\nidx_del\nfor i in range(len(idx_del)):\n    if train_test[df_nan.index[idx_del[i]]].dtype == 'float64':\n        numerical_feat = numerical_feat.drop(columns=[df_nan.index[idx_del[i]]],axis=1)\n    else:\n        categorical_feat = categorical_feat.drop(columns=[df_nan.index[idx_del[i]]],axis=1)","228f4dd4":"# fill the categorical feature\ncategorical_feat['Functional'] = categorical_feat['Functional'].fillna('Typ')\ncategorical_feat['Electrical'] = categorical_feat['Electrical'].fillna('SBrkr')\ncategorical_feat['KitchenQual'] = categorical_feat['KitchenQual'].fillna('TA')\ncategorical_feat['Exterior1st'] = categorical_feat['Exterior1st'].fillna(categorical_feat['Exterior1st'].mode()[0])\ncategorical_feat['Exterior2nd'] = categorical_feat['Exterior2nd'].fillna(categorical_feat['Exterior2nd'].mode()[0])\ncategorical_feat['SaleType'] = categorical_feat['SaleType'].fillna(categorical_feat['SaleType'].mode()[0])\ncategorical_feat['MSZoning'] = categorical_feat['MSZoning'].fillna(categorical_feat['MSZoning'].mode()[0])\ncategorical_feat['Utilities'] = categorical_feat['Utilities'].fillna(categorical_feat['Utilities'].mode()[0])\ncategorical_feat['MasVnrType'] = categorical_feat['MasVnrType'].fillna(categorical_feat['MasVnrType'].mode()[0])\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    categorical_feat[col] = categorical_feat[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    categorical_feat[col] = categorical_feat[col].fillna('None')","aab8fd6d":"import numpy as np\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=3)\nnumerical_feat = pd.DataFrame(imputer.fit_transform(numerical_feat),columns=numerical_feat.columns)","3e6f0d3e":"print('missing value numerical: ',numerical_feat.isna().sum().sum())\nprint('missing value categorical: ',numerical_feat.isna().sum().sum())","1843f7b2":"from scipy.stats import skew\nimport statsmodels.api as sm","b57f18d4":"# one hot encoding for categorical data\n\ncategorical_feat = pd.get_dummies(categorical_feat)","9037f767":"# transform log (for normalization) in numerical data\n\nskewed_features = numerical_feat.apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index\n\n# Normalize skewed features using log_transformation\n    \nfor i in skew_index:\n    numerical_feat[i] = np.log1p(numerical_feat[i])","f50492b6":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(Y_train, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(Y_train, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","8f0cbe29":"# SalePrice after transformation\n\ntarget_log = np.log1p(Y_train)\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","bcb0a886":"import xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error","8225fcfa":"train_test = pd.concat([numerical_feat, categorical_feat],axis=1)\n\ntrain = train_test[0:1460]\ntest = train_test[1460:]\n\nprint('length train: ',len(train))\nprint('length test: ',len(test))","ca64adfb":"# Creation of the RMSE metric:\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","a7de2ffa":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\nkey = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n       'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n       'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.']","d95e7a12":"# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())","c7ddd3a7":"# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())","9022aeb8":"# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())","76391146":"# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())","2ef4003c":"# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())","2e7caccb":"# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())","bb24f3d4":"# XGB Regressor\n\nxgb = xgb.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())","38d71feb":"# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())","28fa1dc3":"# Cat Boost Regressor\n\ncatb = CatBoostRegressor(verbose = 0)\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())","4d6c5dd2":"# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(verbose = 0),\n                                          LinearRegression(),\n                                          BayesianRidge(),\n                                          GradientBoostingRegressor()),\n                              meta_regressor = CatBoostRegressor(verbose = 0),\n                              use_features_in_secondary = True)\n\nscore_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())","d765bf8b":"final_cv_score = pd.DataFrame(key, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","a0af0498":"final_cv_score","351b9b19":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=45)\nplt.show()","25ad21b5":"# Train-Test split the data\nX_train,X_val,y_train,y_val = train_test_split(train,target_log,test_size = 0.1,random_state=42)\n\n# Cat Boost Regressor\ncat = CatBoostRegressor()\ncat_model = cat.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = 0)","28665051":"# Stacked Regressor\nstack = StackingRegressor(regressors=(CatBoostRegressor(verbose = 0),\n                                          LinearRegression(),\n                                          BayesianRidge(),\n                                          GradientBoostingRegressor()),\n                              meta_regressor = CatBoostRegressor(verbose = 0))\nstack_model = stack.fit(X_train,y_train);","59297a88":"cat_pred = cat_model.predict(X_val)\ncat_score = rmse(y_val, cat_pred)\ncat_score","147d0eec":"stack_pred = stack.predict(X_val)\nstack_score = rmse(y_val, stack_pred)\nstack_score","b9831a9e":"# Preforming a Random Grid Search to find the best combination of parameters\n\ngrid = {'iterations': [1000,6000],\n        'learning_rate': [0.05, 0.005, 0.0005],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 9]}\n\nfinal_model = CatBoostRegressor(verbose = 0)\nrandomized_search_result = final_model.randomized_search(grid,\n                                                   X = X_train,\n                                                   y= y_train,\n                                                   verbose = False,\n                                                   plot=True)","fe0c626a":"randomized_search_result['params']","5cd3ec21":"# Final Cat-Boost Regressor\nparams = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = False)\n\ncatf_pred = cat_model_f.predict(X_val)\ncatf_score = rmse(y_val, catf_pred)","a274c354":"catf_score","b12cb476":"# Test CSV Submission\ntest_pred = cat_f.predict(test)\nsubmission = pd.DataFrame(pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")['Id'], columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()","e0b852d4":"# Saving the results in a csv file\nsubmission.to_csv(\"result.csv\", index = False, header = True)","60005fec":"## Predict the Test Set and Submit","840c2a56":"## List and Fill the Missing Value","a2b54572":"From the graph, we can say that the data is not normally distributed. So, before we do modelling must transform data to normally distributed.\n\nThen, check the first 10 correlation coefficient from our data:","b19d189e":"## Hyperparameter Optimization","a4f55d65":"# House Price - Advanced Regression Techniques","83b96014":"Let's check the distribution of our target. Normally distributed make us easier to get the best accuracy.","7c53e1b7":"Transform the target data with logaritma, so we can get normal distribution","9e132509":"## Modelling","d4c0c741":"## Feature Engineering\n\nIn this section we will do several task:\n- One hot encoding for categorical data\n- Normalizing the numerical data using log transform (based on the skewness)\n- Normalizing the output or target variable","c1834f45":"## Concatenate Train Test\n\nThis process to combine our train and test so we can fill the NaN value from both of them simultaneously.","afa9a84a":"Check the Ouutlier using zscore and change the value using its mean","ec225a60":"## Load Data & Information","08cd79cb":"Drop columns where missing value > 20%","b87d5116":"Let's look the ditribution of 3 highest correlation from our data","d0bf7ad2":"## Exploratory Data Analysis\n\nBefore working with any kind of data it is important to understand them. A crucial step to this aim is the ***Exploratory data analysis (EDA)***: a combination of visualizations and statistical analysis (uni, bi, and multivariate) that helps us to better understand the data we are working with and to gain insight into their relationships. So, let's explore our target variable and how the other features influence it.","99ce183d":"fill the numerical data using KNN imputer","615e8150":"fill missing values"}}