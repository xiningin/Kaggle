{"cell_type":{"100106a9":"code","1940945e":"code","aca4bd32":"code","73e73c7d":"code","ff9f0d1c":"code","d3cf433e":"code","860ba6e5":"code","9c2110bb":"code","a1c1694b":"code","d2a8775b":"code","755ef3ce":"code","c5a042bc":"code","ab77bc30":"code","02611203":"code","0b4fffea":"code","80492d4d":"code","00367736":"code","de4812a1":"code","89dc2004":"code","fd9ad5b6":"code","184e0ef1":"code","4da37e73":"markdown","cafddc00":"markdown","79800a02":"markdown","e7b4222a":"markdown","3f9c7c07":"markdown","fa133256":"markdown","4fbf86d2":"markdown","46e78b85":"markdown","bce2b533":"markdown","5c4574ed":"markdown","2c32a662":"markdown","3ec19d06":"markdown","e5bc1433":"markdown","562f81b4":"markdown","f694a88e":"markdown","ecfa2e35":"markdown","af6ef0c6":"markdown","46f41d6a":"markdown","04faf48f":"markdown","9b35b4a2":"markdown","102f12c5":"markdown","bdd4cb98":"markdown","124e46ab":"markdown","f5a72985":"markdown","88d1dd20":"markdown"},"source":{"100106a9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd \n\n#Read in data\ndf = pd.read_csv('\/kaggle\/input\/the-boston-houseprice-data\/boston.csv')\nprint(df.shape)\nprint('m: Number of training examples: ', df.shape[0])\nprint('n: Number of independent Variables: ', df.shape[1])\nprint('Target variable: MEDV')\n","1940945e":"df.head()","aca4bd32":"sns.distplot(df['MEDV'])","73e73c7d":"\nsns.lmplot(x='CRIM',y='MEDV',data=df,aspect=2,height=6)\nplt.xlabel('Crime rate')\nplt.ylabel('Median value of owner-occupied home')\nplt.title('Crime rate vs owner-home value')","ff9f0d1c":"sns.lmplot(x='NOX', y='MEDV', data=df, aspect=2)","d3cf433e":"X = df.drop('MEDV', axis=1)\ny = df['MEDV']","860ba6e5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","9c2110bb":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\n\ndef cross_val(model):\n    pred = cross_val_score(model, X, y, cv=10)\n    return pred.mean()\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')\n    \ndef evaluate(true, predicted):\n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    return mae, mse, rmse, r2_square","a1c1694b":"####### Rescaling variables #######\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('std_scalar', StandardScaler())\n])\n\nX_train = pipeline.fit_transform(X_train)\nX_test = pipeline.transform(X_test)\n\n\n","d2a8775b":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train,y_train)","755ef3ce":"coeff_df = pd.DataFrame(lin_reg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","c5a042bc":"#Plotting our prediction\npred = lin_reg.predict(X_test)\nplt.scatter(y_test, pred)\nplt.show()\n","ab77bc30":"#Prediction on test\/train sets\ntest_pred = lin_reg.predict(X_test)\ntrain_pred = lin_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","02611203":"results_df = pd.DataFrame(data=[[\"Linear Regression\", *evaluate(y_test, test_pred)]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square'])\nresults_df","0b4fffea":"from sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=2)\n\nX_train_2_d = poly_reg.fit_transform(X_train)\nX_test_2_d = poly_reg.transform(X_test)\n\nlin_reg = LinearRegression(normalize=True)\nlin_reg.fit(X_train_2_d,y_train)\n\ntest_pred = lin_reg.predict(X_test_2_d)\ntrain_pred = lin_reg.predict(X_train_2_d)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","80492d4d":"results_df_2 = pd.DataFrame(data=[[\"Polynomial\", *evaluate(y_test, test_pred)]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square'])\n\nresults_df = results_df.append(results_df_2, ignore_index=True)\nresults_df","00367736":"from sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","de4812a1":"results_df_3 = pd.DataFrame(data=[[\"Random Forest\", *evaluate(y_test, test_pred)]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square'])\n\nresults_df = results_df.append(results_df_3, ignore_index=True)\nresults_df","89dc2004":"from sklearn.svm import SVR\n\nsvm_reg = SVR(kernel='rbf', C=1000000, epsilon=0.001)\nsvm_reg.fit(X_train, y_train)\n\ntest_pred = svm_reg.predict(X_test)\ntrain_pred = svm_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","fd9ad5b6":"results_df_4 = pd.DataFrame(data=[[\"Support Vector Machine\", *evaluate(y_test, test_pred)]], \n                          columns=['Model', 'MAE', 'MSE', 'RMSE', 'R2 Square'])\n\nresults_df = results_df.append(results_df_4, ignore_index=True)\nresults_df","184e0ef1":"results_df.set_index('Model', inplace=True)\nresults_df['R2 Square'].plot(kind='barh', figsize=(12, 8)) #barh stands for bar histogram ","4da37e73":"# 1. Linear Regression Algorithm","cafddc00":"# Training Linear Regression Model\nFirst, we will need to split our data into X and Y\n<br> X is a matrix containing all columns(features) except 'MEDV' <\/br>\n<br> Y is a column vector containing only 'MEDV' <\/br>\n","79800a02":"## Train test split\nWe will split our data into test set and training set.","e7b4222a":"## Import Library","3f9c7c07":"# Explore Data Analysis","fa133256":"## Question 1: Does price increases as CRIME rate decreases ?","4fbf86d2":"# Preparing data for linear regression\n* ****Linear Assumption****. Linear regression assumes that the relationship between your input and output is linear. It does not support anything else. This may be obvious, but it is good to remember when you have a lot of attributes. You may need to transform data to make the relationship linear (e.g. log transform for an exponential relationship).\n* ****Remove Noise**** Linear regression assumes that your input and output variables are not noisy. Consider using data cleaning operations that let you better expose and clarify the signal in your data. This is most important for the output variable and you want to remove outliers in the output variable (y) if possible.\n* ****Remove Collinearity****. Linear regression will over-fit your data when you have highly correlated input variables. Consider calculating pairwise correlations for your input data and removing the most correlated.\n* ****Gaussian Distributions****. Linear regression will make more reliable predictions if your input and output variables have a Gaussian distribution. You may get some benefit using transforms (e.g. log or BoxCox) on you variables to make their distribution more Gaussian looking.\n* ****Rescale Inputs:**** Linear regression will often make more reliable predictions if you rescale input variables using standardization or normalization.","46e78b85":"#### Vectorized Form of hypothesis function\n-> Vectorized implementation makes our code run faster \n$$\\mathbf{ h_\\theta{(x)} = X\\theta}$$.","bce2b533":"## Question 2: Does NOX has effect on price of home?","5c4574ed":"# 4. Support Vector Machine Algorithm","2c32a662":"Our R2 Square and MSE are excellent on training set, but it is terrible in test set. This is example of overfitting","3ec19d06":"### Multi-variate Linear Regression and Matrix dimension\n h(theta) = regression line to predict future values\n <br> theta = parameters <\/br>\n <br> <\/br>\n<br> $$\\mathbf{ h_\\theta(x_{i}) = \\theta_0+\\theta_1 CRIM + \\theta_2 ZN + \\theta_3 INDUS + \\theta_4 CHAS + \\theta_5 NOX + \\theta_6 RM + ... }$$ <\/br>\n\n<br> $$\\mathbf{X} = \\left( \\begin{smallmatrix} x_{11} & x_{12} &.&.&.&.& x_{1n}\\\\\n                                x_{21} & x_{22} &.&.&.&.& x_{2n}\\\\\n                                x_{31} & x_{32} &.&.&.&.& x_{3n}\\\\\n                                .&.&.&. &.&.&.& \\\\\n                                .&.&.&. &.&.&.& \\\\\n                                x_{m1} & x_{m2} &.&.&.&.&. x_{mn}\\\\\n                                \\end{smallmatrix} \\right)_{(m,n)}$$ <\/br>\n                                \n$$\\theta = \\left (\\begin{matrix} \\theta_0 \\\\ \\theta_1 \\\\ .\\\\.\\\\ \\theta_j\\\\.\\\\.\\\\ \\theta_n \\end {matrix}\\right)_{(n+1,1)} \n\\mathbf{ y } = \\left (\\begin{matrix} y_1\\\\ y_2\\\\. \\\\. \\\\ y_i \\\\. \\\\. \\\\ y_m \\end{matrix} \\right)_{(m,1)}$$\n","e5bc1433":"It seems like as crime rate goes to nearly 0, price of home increases.\n<br> Safety of neighborhood increases price of owned-home. <\/br>","562f81b4":"****This is example of overfitting. Polynomial regression tries so hard to fit data in training set, it leads to overfitting or not fitting\nunseen data well****","f694a88e":"As Nitric oxide concentration increases, prices of home deceases.\n<br> Since excessive nitric oxide is harmful to human body, it is unlikely to see people to move in a home with nearby high NOX. Therefore, price of home decreases.<\/br>","ecfa2e35":"****OverFitting****: training MSE lower than testing MSE\n<br>****UnderFitting****: very high MSE for testing MSE <\/br>","af6ef0c6":"#### Model Evaluation","46f41d6a":"## ****Data****\n* CRIM: capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town \n* CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) \n* NOX: nitric oxides concentration (parts per 10 million) [parts\/10M]\n* RM: average number of rooms per dwelling \n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to five Boston employment centres\n*  RAD: index of accessibility to radial highways \n*  TAX: full-value property-tax rate per $10,000 [$\/10k] \n*  PTRATIO: pupil-teacher ratio by town \n*  B: The result of the equation B=1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n*  LSTAT: % lower status of the population \n* MEDV: Median value of owner-occupied homes in $1000's [k$]","04faf48f":"# 3. Random Forest Algorithm","9b35b4a2":"# Summary","102f12c5":"# Comparing Models","bdd4cb98":"### Random Forest outperforms all other regression.\n##### Comparing Linear Regression to Random Forest\n<br> Random Forest performs better because it does not make the assumption of linear regression <\/br>\n<br> CHAS features is a categorical variables with 0 and 1. Random Forest performs better than linear regression on such dataset.<\/br>","124e46ab":"## Linear Regression\nFor this prediction, we will try out the following regression algorithm\n\n1. Linear Regression\n2. Polynomial Regression\n3. Random Forest Regression\n4. Support Vector Machine\n\nFirst part of notebook deals with EDA \n<br> Second part of notebook deals with preprocessing\/training <\/br> ","f5a72985":"****Note:**** coefficient are the values that multiply predict values","88d1dd20":"# 2. Polynomial Regression"}}