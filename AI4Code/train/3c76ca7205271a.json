{"cell_type":{"c828ed02":"code","fd7a722f":"code","e595304b":"code","e7a61d87":"code","09388658":"code","d9e9c36c":"code","a5e96f8b":"code","97f9a133":"code","6f65d6d2":"code","f5195383":"code","7c026401":"code","9e89c78a":"code","f71c23f8":"code","302a888c":"code","0fa9428d":"code","8b5eb314":"code","467e53bf":"code","88d54127":"code","c336c621":"code","1fc20c89":"code","06765c5f":"code","42593e33":"code","c1204d93":"code","59dc4862":"code","43a49f91":"code","2ba55706":"code","a2744eef":"code","6337de38":"code","d9680adf":"code","80e10b6d":"markdown","3cf5cd4b":"markdown","00379029":"markdown","bb12c7f6":"markdown","22fd3b7d":"markdown","e941de55":"markdown","30f02170":"markdown","aca64efd":"markdown","bdf0e6f9":"markdown","ab118fd6":"markdown","5c7a5b91":"markdown","1b953db7":"markdown","6130abea":"markdown"},"source":{"c828ed02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fd7a722f":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold, KFold\nwarnings.filterwarnings('ignore')","e595304b":"#load the training file\ntrain_data = pd.read_csv('..\/input\/train.csv')","e7a61d87":"train_data.head(10)","09388658":"value_count = train_data['target'].value_counts()\nprint(f'THE VALUE COUNTS OF THE TARGET VARIABLE : \\n{value_count}')","d9e9c36c":"sns.set(style = 'darkgrid')\nplt.figure(figsize = (12,10))\nsns.countplot(y = train_data['target'])","a5e96f8b":"#load in the testing file\ntest_data = pd.read_csv('..\/input\/test.csv')","97f9a133":"test_data.head(10)","6f65d6d2":"#spearated the dataset into input features and labels\nX = train_data.drop(['target', 'ID_code'], axis = 1)\ny = train_data['target']","f5195383":"import keras\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras import optimizers\n\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau","7c026401":"kernel_init = 'normal'\ndef SimpleFFNN(input_dim, activation, classes):\n    model = Sequential()\n\n    model.add(Dense(512, kernel_initializer = kernel_init, input_dim = input_dim))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dense(512, kernel_initializer = kernel_init, input_dim = input_dim))\n    model.add(BatchNormalization())\n    model.add(Activation(activation))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(256, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(256, kernel_initializer = kernel_init)) \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(128, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dense(128, kernel_initializer = kernel_init))    \n    model.add(Activation(activation))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n\n    model.add(Dense(classes, kernel_initializer = kernel_init))    \n    model.add(Activation('sigmoid'))\n    \n    return model","9e89c78a":"#we will also flatten our output label\ny_flatten = y.ravel()\nprint(f\"THE SIZE OF THE OTUPUT LABELS : {y_flatten.shape}\")","f71c23f8":"#let's scale the original training data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X)\n\nX_scaled = sc.transform(X)","302a888c":"#scaling the testing data\ntest = test_data.drop('ID_code', axis = 1)\ntest_scaled = sc.transform(test)","0fa9428d":"input_dim = X_scaled.shape[1]\nactivation = 'relu'\nclasses = 1\n\nhistory = dict() #dictionery to store the history of individual models for later visualization\nprediction_scores = dict() #dictionery to store the predicted scores of individual models on the test dataset\n\n#here we will be training the same model for a total of 10 times and will be considering the mean of the output values for predictions\nfor i in np.arange(0, 5):\n    optim = optimizers.Adam(lr = 0.001)\n    ensemble_model = SimpleFFNN(input_dim = input_dim, activation = activation, classes = classes)\n    ensemble_model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])\n    print('TRAINING MODEL NO : {}'.format(i))\n    H = ensemble_model.fit(X_scaled, y_flatten,\n                           batch_size = 128,\n                           epochs = 200,\n                           verbose = 1)\n    history[i] = H\n    \n    ensemble_model.save('MODEL_{}.model'.format(i))\n    \n    predictions = ensemble_model.predict(test_scaled, verbose = 1, batch_size = 128)\n    prediction_scores[i] = predictions","8b5eb314":"#we will considering all the features except 'ID_code' and 'target'\nfeatures = [value for value in train_data.columns if value not in ['ID_code', 'target']]","467e53bf":"#defining the parameters\nparam = {\n        'bagging_freq': 5,\n        'bagging_fraction': 0.38,\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.045,\n        'learning_rate': 0.0095,\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 80,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 13,\n        'num_threads': 8,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }","88d54127":"folds = StratifiedKFold(n_splits = 12, shuffle = False, random_state = 101)\ntrain_mat = np.zeros(len(train_data))\npredictions = np.zeros(len(test_data))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_data.values, y.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train_data.iloc[trn_idx][features], label = y.iloc[trn_idx])\n    val_data = lgb.Dataset(train_data.iloc[val_idx][features], label = y.iloc[val_idx])\n\n    num_round = 500000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval = 1000, early_stopping_rounds = 500)\n    train_mat[val_idx] = clf.predict(train_data.iloc[val_idx][features], num_iteration = clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis = 0)\n    \n    predictions += clf.predict(test_data[features], num_iteration = clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(y, train_mat)))","c336c621":"## Catboost : https:\/\/www.kaggle.com\/wakamezake\/starter-code-catboost-baseline\nfrom catboost import Pool, CatBoostClassifier\nmodel = CatBoostClassifier(loss_function = \"Logloss\", eval_metric = \"AUC\")\nkf = KFold(n_splits = 5, random_state = 42, shuffle = True)\n\ny_valid_pred = 0 * y\ny_test_pred = 0\n\nfor idx, (train_index, valid_index) in enumerate(kf.split(train_data)):\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    X_train, X_valid = train_data[features].iloc[train_index,:], train_data[features].iloc[valid_index,:]\n    _train = Pool(X_train, label = y_train)\n    _valid = Pool(X_valid, label = y_valid)\n    print( \"\\nFold \", idx)\n    fit_model = model.fit(_train,\n                          eval_set = _valid,\n                          use_best_model = True,\n                          verbose = 1\n                         )\n    pred = fit_model.predict_proba(X_valid)[:,1]\n    print( \"  auc = \", roc_auc_score(y_valid, pred) )\n    y_valid_pred.iloc[valid_index] = pred\n    y_test_pred += fit_model.predict_proba(test_data[features])[:,1]\ny_test_pred \/= 5","1fc20c89":"#submission for LGBM\ndf_lgbm = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf_lgbm[\"target\"] = predictions\ndf_lgbm.to_csv(\"lgbm_submission.csv\", index = False)","06765c5f":"#submission for CAT\ndf_cat = pd.DataFrame({\"ID_code\": test_data[\"ID_code\"].values})\ndf_cat[\"target\"] = y_test_pred\ndf_cat.to_csv(\"cat_submission.csv\", index = False)","42593e33":"#making predictions\nprediction = np.hstack([p.reshape(-1,1) for p in prediction_scores.values()]) #taking the scores of all the trained models\npredictions_ensemble = np.mean(prediction, axis = 1)\n\nprint(predictions_ensemble.shape)","c1204d93":"#submission for ENSEMBLE\ndf_ensemble = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf_ensemble[\"target\"] = predictions_ensemble\ndf_ensemble.to_csv(\"ensemble_submission.csv\", index = False)","59dc4862":"##submission of combined model\ndf_total = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf_total[\"target\"] = 0.4*df_lgbm[\"target\"] + 0.4*df_cat[\"target\"] + 0.2*df_ensemble['target']\ndf_total.to_csv(\"lgbm_cat_ensemble_submission.csv\", index = False)","43a49f91":"df1 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf1[\"target\"] = 0.5*df_lgbm[\"target\"] + 0.5*df_cat[\"target\"]\ndf1.to_csv(\"lgbm_cat_submission.csv\", index = False)","2ba55706":"df2 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf2[\"target\"] = 0.5*df_lgbm[\"target\"] + 0.5*df_ensemble[\"target\"]\ndf2.to_csv(\"lgbm_ensemble_submission.csv\", index = False)","a2744eef":"df3 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf3[\"target\"] = 0.5*df_cat[\"target\"] + 0.5*df_ensemble[\"target\"]\ndf3.to_csv(\"cat_ensemble_submission.csv\", index = False)","6337de38":"df4 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf4[\"target\"] = 0.2*df_cat[\"target\"] + 0.4*df_ensemble[\"target\"] + 0.4*df_lgbm[\"target\"]\ndf4.to_csv(\"0.2cat_0.4ensemble_0.4lgbm_submission.csv\", index = False)","d9680adf":"df5 = pd.DataFrame({\"ID_code\" : test_data[\"ID_code\"].values})\ndf5[\"target\"] = 0.4*df_cat[\"target\"] + 0.4*df_ensemble[\"target\"] + 0.2*df_lgbm[\"target\"]\ndf5.to_csv(\"0.4cat_0.4ensemble_0.2lgbm_submission.csv\", index = False)","80e10b6d":"## CATBOOST","3cf5cd4b":"2. CATBOOST","00379029":"7. CATBOOST AND ENSEMBLE","bb12c7f6":"## CREATING SUBMISSION FILE","22fd3b7d":"## ENSEMBLE NEURAL NETWORK","e941de55":"6. LGBM AND ENSEMBLE","30f02170":"8. LGBM, CATBOOST AND ENSEMBLE","aca64efd":"5. CATBOOST AND LGBM","bdf0e6f9":"1. LGBM ","ab118fd6":"3. ENSEMBLE","5c7a5b91":"9. LGBM, CATBOOST AND ENSEMBLE","1b953db7":"## LightGBM","6130abea":"4. COMBINED MODEL"}}