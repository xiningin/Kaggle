{"cell_type":{"0ddb04a2":"code","0bc82d4b":"code","a125c006":"code","11fcd5e8":"code","53989b17":"code","852c0c12":"code","9d0ae444":"code","a2b46306":"code","79c3d6eb":"code","215455e6":"code","bdd0b2d2":"code","cbc49f52":"code","f219f120":"code","080cd28a":"code","4dfe8946":"code","e9acd9ef":"code","6aa854cc":"code","85eafa0d":"markdown","3a959056":"markdown","64c5fe60":"markdown","7d768bae":"markdown","b62b6a1e":"markdown","d6508e0b":"markdown","8d1df2bc":"markdown","4a9a71cc":"markdown","7e7a0a08":"markdown","6ae0e0f7":"markdown","67a94486":"markdown","2a4e324f":"markdown"},"source":{"0ddb04a2":"######################\n##installing efficientne net models\n######################\n!\/opt\/conda\/bin\/python3.7 -m pip install --upgrade pip\n! pip install -q efficientnet","0bc82d4b":"########################\n##importing necessary libraries\n#########################\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport PIL\nimport efficientnet.tfkeras as efn\nfrom kaggle_datasets import KaggleDatasets","a125c006":"AUTO = tf.data.experimental.AUTOTUNE\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","11fcd5e8":"GCS_DS_PATH = KaggleDatasets().get_gcs_path()\nTRAIN_PATH = GCS_DS_PATH + \"\/train_images\/\"\n\ntrain_df = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n\nlabel2id = dict(zip(range(train_df.label_group.nunique()),train_df.label_group.unique()))\nid2label = dict(zip(train_df.label_group.unique(),range(train_df.label_group.nunique())))\ntrain_df[\"label_group\"] = train_df[\"label_group\"].map(id2label)\ntrain_df.index = train_df[\"image\"]\n\ntrain_df.head()","53989b17":"'''h_ls,w_ls = [],[]\nfor i,img in enumerate(train_df.image):\n    im = PIL.Image.open(os.path.join(TRAIN_PATH,img))\n    h_ls.append(im.height)\n    w_ls.append(im.width)\n    \ntrain_df[\"height\"] = h_ls\ntrain_df[\"width\"]  = w_ls\nnum_labels = train_df.label_group.nunique()\nnum_ids = train_df.label_group.nunique()\nnum_imgs = train_df.image.nunique()\nnum_phashs = train_df.image_phash.nunique()\nnum_titles = train_df.title.nunique()\n\n\nplt.figure(figsize=(18,5))\nplt.subplot(1,3,1)\nplt.title(\"Histogram of Image Height\",fontweight =\"bold\")\nplt.xlabel(\"Image Height\")\nplt.ylabel(\"Number of Images\")\nplt.bar(np.array(train_df.height.value_counts().index),train_df.height.value_counts().values)\nplt.xlim(80,1100)\nplt.ylim(0,2000)\n\n\nplt.subplot(1,3,2)\nplt.title(\"Histogram of Image Width\",fontweight =\"bold\")\nplt.xlabel(\"Image width\")\nplt.ylabel(\"Number of Images\")\nplt.bar(np.array(train_df.width.value_counts().index),train_df.width.value_counts().values)\nplt.xlim(80,1100)\nplt.ylim(0,2000)\n\nplt.subplot(1,3,3)\nplt.bar([\"Label\",\"Posting Id\",\"Image\",\"Image_Phash\",\"Title\"],[num_labels,num_ids,num_imgs,num_phashs,num_titles])\nplt.title(\"Unique values\",fontweight =\"bold\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Num Unique values\")\nplt.show()'''\nprint(\"Time Taking\")","852c0c12":"#------------------\n##Here I have done grouping of training id's based on labels.\n#------------------\n\ntrain_df[\"posting_label\"] = np.arange(len(train_df))\ntmp = train_df.groupby('label_group').posting_id.agg('unique').to_dict()\ntrain_df['posting_ids'] = train_df.label_group.map(tmp)\ntrain_df['posting_ids'] = train_df['posting_ids'].apply(lambda x: list(x))\n\ntmp = train_df.groupby('label_group').posting_label.agg('unique').to_dict()\ntrain_df['posting_labels'] = train_df.label_group.map(tmp)\ntrain_df['posting_labels'] = train_df['posting_labels'].apply(lambda x: list(x))\ntrain_df = train_df.drop_duplicates(subset=['image'])\ntrain_df.head()","9d0ae444":"##############################\n###Initializing necessary constants\n##############################\n\nNUM_CLASSES =  train_df['label_group'].nunique()\nBATCH_SIZE = BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nHEIGHT,WIDTH = 512,512\nCHANNELS = 3\nSPLIT = int(0.8*len(train_df))\nAUTO = tf.data.experimental.AUTOTUNE\nSTEPS_PER_EPOCH  = SPLIT\/\/BATCH_SIZE\nVALID_STEPS = (len(train_df)-SPLIT)\/\/BATCH_SIZE\nSEED = 143","a2b46306":"##############################\n###Display Samples\n##############################\n\ntrain_df.index = train_df[\"posting_id\"]\n\ndef filepath_to_arr(filepath):\n    img = tf.keras.preprocessing.image.load_img(filepath,target_size= (HEIGHT,WIDTH))\n    arr = tf.keras.preprocessing.image.img_to_array(img)\/255.\n    return arr\n\ndef display_img(training_ids):\n    num_imgs = len(training_ids)\n \n    plt.figure(figsize = (5*num_imgs,10))\n    for i,_id in enumerate(training_ids):\n        plt.subplot(1,num_imgs+1,i+1)\n        \n        filepath = os.path.join(\"..\/input\/shopee-product-matching\/train_images\",train_df.loc[_id][\"image\"])\n        plt.title(\"Image : \"+str(i+1))\n        arr = filepath_to_arr(filepath)\n        plt.imshow(arr)\n        plt.axis(\"off\")\n    plt.show()\n\nfor j in range(5):\n    display_img(train_df.iloc[j][\"posting_ids\"])\n    \n\ntrain_df.index = train_df[\"image\"]","79c3d6eb":"#------------------\n##processing image\n#------------------\ndef process_img(filepath,label):\n    image = tf.io.read_file(filepath)\n    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n    image = tf.image.convert_image_dtype(image, tf.float32) \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label\n\n\n#-----------------------------------\n##adding augmentations to image data\n#-----------------------------------\ndef data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) \n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) \n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) \n        \n    \n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    \n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n    \n    image = tf.image.resize(image, [HEIGHT,WIDTH])\n    return image,label","215455e6":"#------------------------\n##preparing data pipeline\n#------------------------\n\nfiles_ls = tf.io.gfile.glob(TRAIN_PATH + '*.jpg')\n#labels = np.array(train_df.label_group)\nlabels = np.zeros((len(files_ls),))\n\nfor i,file in enumerate(files_ls):\n    file = file.split(\"\/\")[-1]\n    labels[i] = train_df.loc[file][\"label_group\"]\n    #tf.one_hot(train_df.loc[file][\"label_group\"] ,depth = NUM_CLASSES, dtype = tf.int32)\n    \n    \ndataset = tf.data.Dataset.from_tensor_slices((files_ls,labels))\ndataset = dataset.map(process_img,num_parallel_calls=AUTO)\nds = dataset.map(data_augment,num_parallel_calls=AUTO)\n\ntrain_ds = ds.take(SPLIT)\nval_ds = ds.skip(SPLIT)\n\ntrain_ds = train_ds.cache().repeat().shuffle(2048).batch(BATCH_SIZE).prefetch(AUTO)\nval_ds = val_ds.cache().repeat().batch(BATCH_SIZE).prefetch(AUTO)\nprint(\"Data Pipeline\")","bdd0b2d2":"class NormalizeLayer(tf.keras.layers.Layer):\n    def __init__(self,**kwargs):\n        super(NormalizeLayer, self).__init__()\n\n    def call(self, inputs, mask=None):\n        return tf.math.l2_normalize(inputs, axis=-1)\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config}\n    \nnorm_layer = NormalizeLayer()\n\ndef create_model():\n    pretrained = efn.EfficientNetB4(include_top=False, weights='noisy-student',input_shape=[HEIGHT,WIDTH, 3])\n            \n    x = pretrained.output\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    outputs = norm_layer (x)\n        \n    model = tf.keras.Model(pretrained.input, outputs)\n    return model\n\nmodel = create_model()\n#model.summary()","cbc49f52":"############\n#Custom Triplet Loss\n############\n\ndef pairwise_distances(embeddings):\n    dot_product = tf.linalg.matmul(embeddings, tf.transpose(embeddings))\n    square_norm = tf.linalg.diag_part(dot_product)\n    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n    distances = tf.math.maximum(distances, 0.0)\n\n    mask = tf.cast(tf.equal(distances, 0.0),tf.float32)\n    distances = distances + mask * 1e-16\n    distances = tf.math.sqrt(distances)\n    distances = distances * (1.0 - mask)\n\n    return distances\n\ndef get_anchor_positive_triplet_mask(labels):\n    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n    indices_not_equal = tf.math.logical_not(indices_equal)\n\n    labels_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n    mask = tf.math.logical_and(indices_not_equal, labels_equal)\n\n    return mask\n\ndef get_anchor_negative_triplet_mask(labels):\n    labels_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n    mask = tf.math.logical_not(labels_equal)\n\n    return mask\n\ndef get_triplet_mask(labels):\n    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n    indices_not_equal = tf.math.logical_not(indices_equal)\n    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n\n    distinct_indices = tf.math.logical_and(tf.math.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n\n\n    label_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n    i_equal_j = tf.expand_dims(label_equal, 2)\n    i_equal_k = tf.expand_dims(label_equal, 1)\n\n    valid_labels = tf.math.logical_and(i_equal_j, tf.logical_not(i_equal_k))\n\n    mask = tf.math.logical_and(distinct_indices, valid_labels)\n\n    return mask\n\n\nclass TripletLossFn(tf.keras.losses.Loss):\n    def __init__(self,margin=1.0,**kwargs):\n        super().__init__(**kwargs)\n        self.margin = margin\n  \n    def call(self,y_true,y_pred):\n\n        labels = tf.convert_to_tensor(y_true)\n        labels = tf.squeeze(labels,axis=-1)\n        embeddings = tf.convert_to_tensor(y_pred)\n\n        pairwise_dist = pairwise_distances(embeddings)\n\n        mask_anchor_positive = get_anchor_positive_triplet_mask(labels)\n        mask_anchor_positive = tf.cast(mask_anchor_positive,tf.float32)\n\n        anchor_positive_dist = tf.math.multiply(mask_anchor_positive, pairwise_dist)\n\n        hardest_positive_dist = tf.math.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n\n\n        mask_anchor_negative = get_anchor_negative_triplet_mask(labels)\n        mask_anchor_negative = tf.cast(mask_anchor_negative,tf.float32)\n\n        max_anchor_negative_dist = tf.math.reduce_max(pairwise_dist, axis=1, keepdims=True)\n        anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n\n\n        hardest_negative_dist = tf.math.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n    \n\n        triplet_loss = tf.math.maximum(hardest_positive_dist - hardest_negative_dist + self.margin, 0.0)\n\n        triplet_loss = tf.math.reduce_mean(triplet_loss)\n\n        return triplet_loss\n    \n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config,\"margin\":self.margin}","f219f120":"import tensorflow_addons as tfa\n\ndef compile_model(model, lr=0.0001):\n    \n    optimizer = tf.keras.optimizers.Adam(lr=lr)\n    \n    #loss = tf.keras.losses.SparseCategoricalCrossentropy()\n    loss = TripletLossFn(0.7) \n    metrics = [\n       tf.keras.metrics.SparseCategoricalAccuracy(name='acc')\n    ]\n\n    model.compile(optimizer=optimizer, loss=loss)\n\n    return model","080cd28a":"def create_callbacks():\n    \n    cpk_path = '.\/best_model.h5'\n    \n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        mode='min',\n        factor=0.1,\n        patience=3,\n        verbose=0\n    )\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        filepath=cpk_path,\n        monitor='val_loss',\n        mode='min',\n        save_best_only=True,\n        verbose=1,\n    )\n\n    earlystop = tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        mode='min',\n        patience=10, \n        verbose=1\n    )\n    \n    callbacks = [checkpoint, reducelr, earlystop]         \n    \n    return callbacks","4dfe8946":"EPOCHS= 50\nVERBOSE =1\n\ntf.keras.backend.clear_session()\n\nwith strategy.scope():\n    \n    model = create_model()\n    model = compile_model(model, lr=0.0001)\n   \n    callbacks = create_callbacks()\n    \n    history = model.fit(train_ds, \n                        epochs=EPOCHS,\n                        callbacks=callbacks,\n                        validation_data = val_ds,\n                        verbose=VERBOSE,\n                        steps_per_epoch = STEPS_PER_EPOCH,\n                        validation_steps=VALID_STEPS\n                       )","e9acd9ef":"'''acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(history.history['val_loss']))\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()'''\nprint(\"Plotting the History\")","6aa854cc":"#------------------------------------------\n##Taking last but one layer for embeddings\n#------------------------------------------\n\nwith tf.device('\/device:CPU:0'):\n    dataset = dataset.batch(BATCH_SIZE)\n    embed_model = tf.keras.models.load_model(\".\/best_model.h5\",\n                                       custom_objects={'TripletLossFn': TripletLossFn,'NormalizeLayer':NormalizeLayer})\n    #embed_model = tf.keras.Model(model.layers[0].input,model.layers[-2].output)\n    embeddings = embed_model.predict(dataset)\n    np.save(\".\/embeddings.npy\",embeddings)","85eafa0d":"# History Plotting","3a959056":"# Creating Model","64c5fe60":"![Shopee](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fe\/Shopee.svg\/1200px-Shopee.svg.png)\n\n## **Description of competition**\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective. In this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products.\n\n## **About Shopee**\n\n### Website : [Shopee](https:\/\/shopee.com\/)\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast     online shopping experience tailored to their region. The company also provides strong payment and logistical support      along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.","7d768bae":"# Data Pipeline","b62b6a1e":"# Checking for TPU","d6508e0b":"# Callbacks","8d1df2bc":"### Hope this notebook is helpful. If you have any doubts or suggestions feel free to comment here. \n\n## An upvote will be very much encouraging for me. \n\n# Happy kaggling\u2764","4a9a71cc":"# Compiling Model","7e7a0a08":"# Introduction","6ae0e0f7":"# Training","67a94486":"# Data Visualization","2a4e324f":"# Embeddings of training images"}}