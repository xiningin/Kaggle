{"cell_type":{"31843a9c":"code","b692f0d2":"code","f66807a2":"code","7998cc4f":"code","27575486":"code","139afae6":"code","2ae82828":"code","c2dc8cd3":"code","e7bb45cc":"code","183ab372":"code","5eba85d1":"code","df924319":"code","bfacfa69":"code","1829e7e2":"code","234a3fbc":"code","77b9e948":"code","42cc40ef":"code","85842430":"code","4f880ed3":"code","acd19635":"code","e4922e35":"code","b05e4290":"code","b6564e76":"code","c1f9ffb9":"code","51d351dc":"code","b866e755":"code","9bab0372":"code","865cf0da":"markdown","a718ec0d":"markdown","4cecd5e1":"markdown","425097aa":"markdown","61842120":"markdown","5bc59988":"markdown","5f1657da":"markdown","0287c96b":"markdown","0929ec2c":"markdown","54ca61f3":"markdown","ebc98a2d":"markdown","6d5f8d89":"markdown","01b86604":"markdown","8b3c901e":"markdown","58c2abca":"markdown","f69600dc":"markdown","7266b61b":"markdown","ab4a0d7e":"markdown","cdc73282":"markdown","70cb7f84":"markdown","3cafcd5c":"markdown","34219045":"markdown"},"source":{"31843a9c":"import pandas as pd\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b692f0d2":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ndata.tail()","f66807a2":"data.info()","7998cc4f":"data['ticket_type'] = data.Ticket.apply(lambda x : x.replace('.','').split(' ')[0] if len(x.split(' ')) > 1 else '')\ntest['ticket_type'] = test.Ticket.apply(lambda x : x.replace('.','').split(' ')[0] if len(x.split(' ')) > 1 else '')\n\ndata['cabin_type'] = data.Cabin.apply(lambda x : str(x)[0] if pd.notnull(x) else 'X')\ntest['cabin_type'] = test.Cabin.apply(lambda x : str(x)[0] if pd.notnull(x) else 'X')","27575486":"from sklearn.model_selection import train_test_split, StratifiedKFold\nX_train, X_test, y_train, y_test = train_test_split(data.drop(columns='Survived'), \n                                                    data.Survived, test_size=0.2, \n                                                    stratify=data[['Survived', 'Sex', 'Pclass']]\n                                                   )\n\nX_train['Survived'] = y_train","139afae6":"sns.distplot(data.Age.dropna(), bins=20)","2ae82828":"sns.distplot(data.Fare.dropna(), bins=100)","c2dc8cd3":"data.Fare.dropna().quantile(.75)","e7bb45cc":"sns.barplot(x='Embarked', y='Survived', data=data.groupby('Embarked')['Survived'].mean().reset_index())","183ab372":"sns.barplot(x='cabin_type', y='Survived', data=data.groupby('cabin_type')['Survived'].mean().reset_index())","5eba85d1":"ax, fig = plt.subplots(figsize=(18,5))\nsns.barplot(x='ticket_type', y='Survived', data=data.groupby('ticket_type')['Survived'].mean().reset_index())","df924319":"sns.boxplot(x='Survived', y='Fare', data=data)","bfacfa69":"sns.barplot(x='Pclass', y='Survived', data=data.groupby('Pclass')['Survived'].mean().reset_index())","1829e7e2":"def feature_engineering(df):\n    df = df.merge(X_train.groupby('cabin_type')['Survived'].mean().reset_index().rename(columns={'Survived':'cabin_pct'}), on='cabin_type', how='left')\n    df['cabin_pct'] = df.cabin_pct > .7\n#     df = df.merge(X_train.groupby('Embarked')['Survived'].mean().reset_index().rename(columns={'Survived':'embarked_pct'}), on='Embarked', how='left')\n    df['cherbourg'] = df.Embarked.apply(lambda e : e == 'C')\n#     df = df.merge(X_train.groupby('Pclass')['Survived'].mean().reset_index().rename(columns={'Survived':'pclass_pct'}), on='Pclass', how='left')\n    df = df.merge(X_train.groupby('ticket_type')['Survived'].mean().reset_index().rename(columns={'Survived':'ticket_pct'}), on='ticket_type', how='left')\n    df['ticket_pct'] = df.ticket_pct > .6\n    df['title'] = df.Name.apply(lambda x : x.split('.')[0].split(',')[1].strip())\n    df['sex'] = df.Sex.apply(lambda x : 1 if x == 'male' else 0)\n    df['child'] = df.Age.apply(lambda x : x < 13)\n#     df['teenager'] = df.Age.apply(lambda x : (x > 12) & (x < 20))\n#     df['young_adult'] = df.Age.apply(lambda x : (x > 19) & (x < 36))\n#     df['adult'] = df.Age.apply(lambda x : (x > 35) & (x < 60))\n    df['elder'] = df.Age.apply(lambda x : x > 59)\n    df['mother'] = df.Parch.apply(lambda x : x > 0) & df.title.apply(lambda x : x == 'Mrs')\n    df['single'] = df.SibSp.apply(lambda x : x == 0) & df.Parch.apply(lambda x : x == 0)\n    df['fare>30'] = df.Fare.apply(lambda x : x > 30)\n    df['fare<10'] = df.Fare.apply(lambda x : x < 10)\n    df['first_class'] = df.Pclass == 1\n    df['big_fam'] = (df.Parch + df.SibSp).apply(lambda x : x > 3)\n      \n    return df\n\nX_train = feature_engineering(X_train)\nX_test = feature_engineering(X_test)\ntest = feature_engineering(test)","234a3fbc":"X_train['surname'] = X_train.Name.apply(lambda name : name.split(',')[0])\nX_train = X_train.merge(X_train.groupby('surname')['Survived'].mean().reset_index().rename(columns={'Survived':'surname_pct'}), on='surname', how='left')\n\nX_test['surname'] = X_test.Name.apply(lambda name : name.split(',')[0])\nX_test = X_test.merge(X_train.groupby('surname')['Survived'].mean().reset_index().rename(columns={'Survived':'surname_pct'}), on='surname', how='left')\n\ntest['surname'] = test.Name.apply(lambda name : name.split(',')[0])\ntest = test.merge(X_train.groupby('surname')['Survived'].mean().reset_index().rename(columns={'Survived':'surname_pct'}), on='surname', how='left')","77b9e948":"X_train = X_train.drop(columns=['PassengerId', 'Age', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', \n                                'Cabin', 'Embarked', 'title', 'surname', 'cabin_type', 'ticket_type']).fillna(X_train.mean()).astype('float64')\n\nX_test = X_test.drop(columns=['PassengerId', 'Age', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', \n                                'Cabin', 'Embarked', 'title', 'surname', 'cabin_type', 'ticket_type']).fillna(X_train.mean()).astype('float64')\n\ntest_num = test.drop(columns=['PassengerId', 'Age', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', \n                                'Cabin', 'Embarked', 'title', 'surname', 'cabin_type', 'ticket_type']).fillna(X_train.mean()).astype('float64')\n\n","42cc40ef":"X_train.head()","85842430":"import numpy as np\n\ncorr = X_train.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","4f880ed3":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest\n\nmodel = Pipeline(steps=[\n                        ('scaler', StandardScaler()),\n                        ('norm', Normalizer()),\n                        ('poly', PolynomialFeatures(3)),\n                        ('PCA', PCA(6)),\n                        ('minmax', MinMaxScaler()),\n#                         ('logreg', RidgeClassifier())\n                        ('rfc', RandomForestClassifier(1000, max_depth=5)),\n#                         ('xgb', xgb.XGBRFClassifier(1000, depth=2)),\n#                         ('svm', SVC(kernel='rbf', gamma='scale'))\n                       ])\n\nmodel.fit(X_train.drop(columns=['Survived']), y_train)\ny_pred = model.predict(X_test)\nmodel.score(X_test, y_test)","acd19635":"precision_score(y_pred, y_test)","e4922e35":"recall_score(y_pred, y_test)","b05e4290":"y_pred = model.predict(test_num)\nprint(y_pred)","b6564e76":"sum(y_pred) \/ len(y_pred)","c1f9ffb9":"submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","51d351dc":"submission['Survived'] = y_pred","b866e755":"submission.head(3)","9bab0372":"submission.to_csv(\"submission.csv\", index=False)","865cf0da":"### Pipeline\n\nPipeline used to predict survival. It contains:\n\n1. A StandardScaler and a Normalizer (to normalize all data)\n2. Polynomial feature generator (to capture possibly revelant non-linear relations)\n3. PCA (to reduce dimensionality while keeping the more relevant components)\n4. MinMaxScaler (to keep data between 0 and 1 before entering ML model)\n5. Model (to do the cool Machine Learning stuff)","a718ec0d":"Survival by generated cabin_type feature","4cecd5e1":"### Feature Engineering\n\nThe following function creates most of the used features in my final model, and it uses a few of the EDA discoveries above. The features not inserted in this functions have a special behavior that makes it necessary for them to be created later (so far only one feature - *\"surname_pct\"*).","425097aa":"Correlation matrix of the final features","61842120":"Survival by Embarked feature","5bc59988":"A view of the final training matrix","5f1657da":"**Processing strings**\n\nHere I'm taking only a piece of the original strings in order to later (maybe) use them as inputs","0287c96b":"Survival by generated ticket_type feature","0929ec2c":"Finding what a high Fare value would be using quantile.","54ca61f3":"Survival by Pclass feature","ebc98a2d":"**Training and validation**\n\nUsing tran_test_split from scikit-learn to create the traning and validation sets","6d5f8d89":"Age distribuition","01b86604":"Fare distribuition","8b3c901e":"### Conclusion\n\nI was able to surpass 80% on evaluation data a few times when using the final pipeline, I've tried a few different classifiers, but used Random Forests in the end. When submitting the result to Kaggle, the accuracy would go below 80%, which I believe can be solved\/tackled with a little more feature engineering.\n\nI only spent a couple days on this challenge, so I hope I will come back to it in the near future to make some improvements.","58c2abca":"Dropping useless columns (non-numerical and\/or already processed to create other features)","f69600dc":"**Import datasets**","7266b61b":"## Predicting survivals for the famous 1912 Titanic Disaster\nIn this notebook, I'm showing the step-by-step process that I've taken while trying to predict survivals on Kaggle's Titanic competition.","ab4a0d7e":"### Exploratory data analysis\n\nIn the following plots, I'm trying to make some sense of the data in order to think of new features to create","cdc73282":"**Precision**","70cb7f84":"**Recall**","3cafcd5c":"Boxplot of Survival (categorical) x Fare (continuous)","34219045":"#### Predicting test.csv and saving submission\nPrediction for Kaggle unlabeled *test.csv*"}}