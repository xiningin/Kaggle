{"cell_type":{"710a7dd8":"code","f22679d2":"code","64bf6382":"code","ca225914":"code","9a77742b":"code","dc2a68fc":"code","c033167c":"markdown","545118de":"markdown","c977c4f6":"markdown","10a46937":"markdown"},"source":{"710a7dd8":"!pip install pycocotools>=2.0.2 > \/dev\/null\n!pip install timm>=0.3.2 > \/dev\/null\n!pip install omegaconf>=2.0 > \/dev\/null\n!pip install ensemble-boxes > \/dev\/null\n!pip install effdet > \/dev\/null","f22679d2":"import sys\nimport torch\nimport os\nimport warnings\nimport time\nimport cv2\nimport random\nimport gc\nimport numba\nimport re\nimport ast\n\nimport pandas as pd\nimport numpy as np\n\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom collections import Counter\nfrom glob import glob\n\nfrom ensemble_boxes import weighted_boxes_fusion\nfrom albumentations.pytorch.transforms import ToTensorV2\nimport albumentations as A\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np\n\nfrom numba import jit\nfrom typing import List, Union, Tuple\nfrom scipy.optimize import linear_sum_assignment\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold, train_test_split\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.utils.data.dataloader import default_collate\n\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\nfrom effdet.efficientdet import HeadNet\nfrom effdet import create_model, unwrap_bench, create_loader, create_dataset, create_evaluator, create_model_from_config\nfrom effdet.data import resolve_input_config, SkipSubset\nfrom effdet.anchors import Anchors, AnchorLabeler\nfrom timm.models import resume_checkpoint, load_checkpoint\nfrom timm.optim import create_optimizer\nfrom timm.scheduler import create_scheduler\n\n# Paths\nTRAIN_ROOT_PATH = '..\/input\/swimming-pool-512x512\/CANNES_TILES_512x512_PNG\/CANNES_TILES_512x512_PNG'\nTRAIN_LABELS_PATH = '..\/input\/k\/alexj21\/swimmingpool-eda-csv\/swimming_pools_labels_512x512.csv'\n\n# Models\nNFOLDS = 5\nMODELS_PATH = []\nMODELS_PATH.append('..\/input\/k\/alexj21\/efficientdet-swimming-pool-detection-custom\/training_job\/fold-0-best-checkpoint-039epoch.bin')\nMODELS_PATH.append('..\/input\/k\/alexj21\/efficientdet-swimming-pool-detection-custom\/training_job\/fold-1-best-checkpoint-033epoch.bin')\nMODELS_PATH.append('..\/input\/k\/alexj21\/efficientdet-swimming-pool-detection-custom\/training_job\/fold-2-best-checkpoint-030epoch.bin')\nMODELS_PATH.append('..\/input\/k\/alexj21\/efficientdet-swimming-pool-detection-custom\/training_job\/fold-3-best-checkpoint-039epoch.bin')\nMODELS_PATH.append('..\/input\/k\/alexj21\/efficientdet-swimming-pool-detection-custom\/training_job\/fold-4-best-checkpoint-031epoch.bin')\n\nSEED = 42\nIMG_SIZE = 512\nPREDS_TH = 0.3\n\nlabel2color = [[255, 0, 0]]\nviz_labels =  [\"pool\"]\n\n# Load data\ndf_annotations = pd.read_csv(TRAIN_LABELS_PATH)\ndf_annotations['image_path'] = df_annotations['image_id'].map(lambda x:os.path.join(TRAIN_ROOT_PATH, str(x)))\n\n# Filter wrong annotations (or too small pools)\ndf_annotations = df_annotations.drop(df_annotations[(df_annotations['xmin'] == 0) & (df_annotations['xmax'] == 0)].index)\ndf_annotations = df_annotations.drop(df_annotations[(df_annotations['ymin'] == 0) & (df_annotations['ymax'] == 0)].index)\n\n# Keep only pools\ndf_annotations = df_annotations[df_annotations['class'] == 'pool']\ndf_annotations.reset_index(drop=True, inplace=True)\ndf_annotations['class'] = 1\ndf_annotations['xmin'] = df_annotations['xmin'] - 1\ndf_annotations['ymin'] = df_annotations['ymin'] - 1\ndf_annotations.head(5)","64bf6382":"image_paths = df_annotations['image_path'].unique()\nprint(\"Number of Images :\",len(image_paths))\nanno_count = df_annotations.shape[0]\nprint(\"Number of Annotations:\", anno_count)","ca225914":"def calc_stats(gt_boxes, pred_boxes, th=0.5):\n    cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n    for i, box1 in enumerate(gt_boxes):\n        for j, box2 in enumerate(pred_boxes):\n            iou_score = calculate_iou(box1, box2)\n            \n            if iou_score < th:\n                continue\n            else:\n                cost_matix[i,j]=0\n\n    row_ind, col_ind = linear_sum_assignment(cost_matix)\n    fn = len(gt_boxes) - row_ind.shape[0]\n    fp = len(pred_boxes) - col_ind.shape[0]\n    tp=0\n    for i, j in zip(row_ind, col_ind):\n        if cost_matix[i,j]==0:\n            tp+=1\n        else:\n            fp+=1\n            fn+=1\n    return tp, fp, fn\n\n@jit(nopython=True)\ndef calculate_iou(gt, pr, form='pascal_voc') -> float:\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt\/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        gt = gt.copy()\n        pr = pr.copy()\n\n        gt[2] = gt[0] + gt[2]\n        gt[3] = gt[1] + gt[3]\n        pr[2] = pr[0] + pr[2]\n        pr[3] = pr[1] + pr[3]\n\n    # Calculate overlap area\n    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n    \n    if dx < 0:\n        return 0.0\n    \n    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n            overlap_area\n    )\n\n    return overlap_area \/ union_area\n\n\ndef show_result(sample_id, preds, gt_boxes):\n    sample = cv2.imread(f'{TRAIN_ROOT_PATH}\/{sample_id}', cv2.IMREAD_COLOR)\n    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n\n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    for pred_box in preds:\n        cv2.rectangle(\n            sample,\n            (pred_box[0], pred_box[1]),\n            (pred_box[2], pred_box[3]),\n            (220, 0, 0), 2\n        )\n\n    for gt_box in gt_boxes:    \n        cv2.rectangle(\n            sample,\n            (gt_box[0], gt_box[1]),\n            (gt_box[2], gt_box[3]),\n            (0, 0, 220), 2\n        )\n\n    ax.set_axis_off()\n    ax.imshow(sample)\n    ax.set_title(\"RED: Predicted | BLUE - Ground-truth\")","9a77742b":"# Seed\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Plots\ndef plot_img(img, size=(18, 18), is_rgb=True, title=\"\", cmap=None):\n    plt.figure(figsize=size)\n    plt.imshow(img)\n    plt.suptitle(title)\n    plt.show()\n\ndef plot_imgs(imgs, cols=2, size=10, is_rgb=True, title=\"\", cmap=None, img_size=None):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size), constrained_layout=True)\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.axis('off')\n        plt.imshow(img)\n    plt.suptitle(title)\n    return fig\n\ndef draw_bbox_small(image, box, label, color):   \n    alpha = 0\n    alpha_text = 0.4\n    thickness = 1\n    font_size = 0.4\n    overlay_bbox = image.copy()\n    overlay_text = image.copy()\n    output = image.copy()\n\n    text_width, text_height = cv2.getTextSize(label.upper(), cv2.FONT_HERSHEY_SIMPLEX, font_size, thickness)[0]\n    cv2.rectangle(overlay_bbox, (box[0], box[1]), (box[2], box[3]),\n                color, -1)\n    cv2.addWeighted(overlay_bbox, alpha, output, 1 - alpha, 0, output)\n    cv2.rectangle(overlay_text, (box[0], box[1]-7-text_height), (box[0]+text_width+2, box[1]),\n                (0, 0, 0), -1)\n    cv2.addWeighted(overlay_text, alpha_text, output, 1 - alpha_text, 0, output)\n    cv2.rectangle(output, (box[0], box[1]), (box[2], box[3]),\n                    color, thickness)\n    cv2.putText(output, label.upper(), (box[0], box[1]-5),\n            cv2.FONT_HERSHEY_SIMPLEX, font_size, (255, 255, 255), thickness, cv2.LINE_AA)\n    return output\n        \n# Dataset class\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, marking, image_ids, transforms=None, test=False):\n        super().__init__()\n        self.image_ids = image_ids\n        self.marking = marking\n        self.transforms = transforms\n        self.test = test\n        \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        \n        image, boxes, labels = self.load_image_and_boxes(index)\n        \n        ## To prevent ValueError: y_max is less than or equal to y_min for bbox from albumentations bbox_utils\n        labels = np.array(labels, dtype=np.int).reshape(len(labels), 1)\n        combined = np.hstack((boxes.astype(np.int), labels))\n        combined = combined[np.logical_and(combined[:,2] > combined[:,0],\n                                                          combined[:,3] > combined[:,1])]\n        boxes = combined[:, :4]\n        labels = combined[:, 4].tolist()\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = torch.tensor(labels)\n        target['image_id'] = torch.tensor([index])\n        if self.transforms:\n            for i in range(10):\n                sample = self.transforms(**{\n                    'image': image,\n                    'bboxes': target['boxes'],\n                    'labels': labels\n                })\n                if len(sample['bboxes']) > 0:\n                    image = sample['image']\n                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n                    target['boxes'][:,[0,1,2,3]] = target['boxes'][:,[1,0,3,2]]  ## ymin, xmin, ymax, xmax\n                    break\n            \n            ## Handling case where no valid bboxes are present\n            if len(target['boxes'])==0 or i==9:\n                return None\n            else:\n                ## Handling case where augmentation and tensor conversion yields no valid annotations\n                try:\n                    assert torch.is_tensor(image), f\"Invalid image type:{type(image)}\"\n                    assert torch.is_tensor(target['boxes']), f\"Invalid target type:{type(target['boxes'])}\"\n                except Exception as E:\n                    print(\"Image skipped:\", E)\n                    return None      \n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def load_image_and_boxes(self, index):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{TRAIN_ROOT_PATH}\/{image_id}', cv2.IMREAD_COLOR).copy()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        records = self.marking[self.marking['image_id'] == image_id]\n        boxes = records[['xmin', 'ymin', 'xmax', 'ymax']].values\n        labels = records['class'].tolist()\n        resize_transform = A.Compose([A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0)], \n                                    p=1.0, \n                                    bbox_params=A.BboxParams(\n                                        format='pascal_voc',\n                                        min_area=0.1, \n                                        min_visibility=0.1,\n                                        label_fields=['labels'])\n                                    )\n\n        resized = resize_transform(**{\n                'image': image,\n                'bboxes': boxes,\n                'labels': labels\n            })\n\n        resized_bboxes = np.vstack((list(bx) for bx in resized['bboxes']))\n        return resized['image'], resized_bboxes, resized['labels']\n    \ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n            ToTensorV2(p=1.0),\n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='pascal_voc',\n            min_area=0, \n            min_visibility=0,\n            label_fields=['labels']\n        )\n    )\n\n\ndef collate_fn(batch):\n    batch = list(filter(lambda x: x is not None, batch))\n    \n    return tuple(zip(*batch))\n            \nseed_everything(SEED)","dc2a68fc":"warnings.filterwarnings(\"ignore\")\n\nmodels = [None] * NFOLDS\npreds = [[] for j in range(NFOLDS)]\n\nprecision = [None] * NFOLDS\nrecall = [None] * NFOLDS\nf1_score = [None] * NFOLDS\n\nfor fold in range(NFOLDS):\n    # Validation ids\n    val_ids = df_annotations[df_annotations['fold'] == fold].image_id.unique()\n\n    # Creation validation dataset\n    validation_dataset = DatasetRetriever(\n                            image_ids=val_ids,\n                            marking=df_annotations,\n                            transforms=get_valid_transforms(),\n                            test=True,\n                            )\n\n    # Create validation loader\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=4,\n        num_workers=1,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    # Load and configure model\n    base_config = get_efficientdet_config('tf_efficientdet_d1')\n    base_config.image_size = (IMG_SIZE, IMG_SIZE)\n    \n    models[fold] = create_model_from_config(base_config, \n                                            bench_task='predict', \n                                            num_classes=1,\n                                            pretrained=True)\n\n    checkpoint = torch.load(MODELS_PATH[fold])\n    models[fold].model.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n    models[fold].eval()\n    models[fold].cuda()\n    \n    # Loop through validation images\n    print('Compute preds for fold', fold)\n    ftp, ffp, ffn = [], [], []\n    for images, targets, image_ids in tqdm(val_loader, total=len(val_loader)):\n        with torch.no_grad():\n            images = torch.stack(images)\n            images = images.cuda().float()\n\n            target_res = {}\n            boxes = [target['boxes'].cuda().float() for target in targets]\n            labels = [target['labels'].cuda().float() for target in targets]\n            target_res['bbox'] = boxes\n            target_res['cls'] = labels \n            target_res[\"img_scale\"] = torch.tensor([1.0] * 8,\n                                                   dtype=torch.float).cuda()\n            target_res[\"img_size\"] = torch.tensor([images[0].shape[-2:]] * 8,\n                                                  dtype=torch.float).cuda()\n\n            det = models[fold](images, target_res)\n\n            for i in range(images.shape[0]):\n                boxes = det[i].detach().cpu().numpy()[:,:4].astype(int)    \n                scores = det[i].detach().cpu().numpy()[:,4]\n                gt_boxes = (targets[i]['boxes'][:,[1,0,3,2]]).cpu().numpy().astype(int)\n\n                indexes = np.where(scores>0.3)\n                pred_boxes = boxes[indexes]\n                scores = scores[indexes]\n\n                tp, fp, fn = calc_stats(gt_boxes, pred_boxes, th=0.3)\n                ftp.append(tp)\n                ffp.append(fp)\n                ffn.append(fn)\n\n    tp = np.sum(ftp)\n    fp = np.sum(ffp)\n    fn = np.sum(ffn)\n    precision[fold] = tp \/ (tp + fp + 1e-6)\n    recall[fold] =  tp \/ (tp + fn +1e-6)\n    f1_score[fold] = 2*(precision[fold] *recall[fold])\/(precision[fold] +recall[fold]+1e-6)\n    \n    print(f'TP: {tp}, FP: {fp}, FN: {fn}, PRECISION: {precision[fold] :.4f}, RECALL: {recall[fold]:.4f}, F1 SCORE: {f1_score[fold]:.4f} \\n \\n')\n    \nprint(f'OOF -  PRECISION: {np.mean(precision) :.4f}, RECALL: {np.mean(recall):.4f}, F1 SCORE: {np.mean(f1_score):.4f}')","c033167c":"# Functions","545118de":"# Iterate through images","c977c4f6":" <span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Online installations<\/span>","10a46937":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 2.0em; font-weight: 300;\">Import Packages<\/span>"}}