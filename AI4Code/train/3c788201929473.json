{"cell_type":{"1d3e2667":"code","432491b4":"code","7b5b5948":"code","59824114":"code","7977b039":"code","688991a6":"code","9242f714":"code","a12ca63a":"code","c0f9fd03":"code","8a19e909":"code","7a48fb8f":"code","f712c807":"code","f5d93359":"code","e3d540ca":"code","80bc2ebe":"code","fc8f96a9":"code","fd39b6d4":"code","c9f8db11":"code","144ea7c5":"code","78374f0b":"code","054bfbe1":"code","381e8ea8":"code","7da04b5a":"code","2b9a4a1f":"code","0bde2b2b":"code","f5c50156":"code","cf1868c9":"code","a22b9fe2":"code","2b2bb876":"code","b3577558":"code","0d39f067":"code","855cb6e9":"code","cbc6a55f":"code","98299bab":"code","c8acef1f":"markdown","478b7886":"markdown","7867ca99":"markdown","83386776":"markdown","f672a089":"markdown","c181891c":"markdown","cf8e11a3":"markdown"},"source":{"1d3e2667":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","432491b4":"# to prevent unnecessary warnings\nimport warnings\nwarnings.simplefilter(action='ignore')","7b5b5948":"# importing useful libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\n#import helper modules\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","59824114":"#reading the data set with pandas\nstroke_df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\n\nstroke_df.head() #get the first 5 rows of the dataset","7977b039":"stroke_df.info()","688991a6":"stroke_df.describe() #numerically describing the characteristics of the dataset","9242f714":"stroke_df.isnull().sum() #checking the null values of each column of the dataset","a12ca63a":"#we have only one column with missing value which is bmi, we can fill the missing values with mode\n\nstroke_df['bmi']= stroke_df['bmi'].fillna(stroke_df['bmi'].mode().iloc[0])","c0f9fd03":"stroke_df.isnull().sum().sum() #checking the total number of null values in the dataset","8a19e909":"#checking the value counts for the label column\n\nprint(stroke_df['stroke'].value_counts())\n\n#plotting the values\nsns.countplot(stroke_df['stroke'])\n\n# we can see there is a huge gap in the data for those patient with stroke and those without stroke \n# which can cause our model to be biased in prediction or behave poorly if used directly without any change","7a48fb8f":"#converting the categorical columns to that with numeric value\n\nstroke_df['gender'] = stroke_df['gender'].astype('category').cat.codes\nstroke_df['ever_married'] = stroke_df['ever_married'].astype('category').cat.codes\nstroke_df['work_type'] = stroke_df['work_type'].astype('category').cat.codes\nstroke_df['Residence_type'] = stroke_df['Residence_type'].astype('category').cat.codes\nstroke_df['smoking_status'] = stroke_df['smoking_status'].astype('category').cat.codes","f712c807":"stroke_df.info() #information on the characteristics of the datasset","f5d93359":"plt.figure(figsize = (25,10)) #set figure size for the plot generated\n\nsns.heatmap(stroke_df.corr(), annot= True)#visualization of the numerical correlation of each feature of the dataset","e3d540ca":"sns.pairplot(stroke_df, kind = 'scatter', diag_kind= 'kde',hue = 'stroke')\n#for distribution and relationship of each feature","80bc2ebe":"#from the correlation plot, the age and ever married column seems highly correlated\n#well drop the ever married column and also id column\n\ns_df = stroke_df.drop(columns= ['ever_married'], axis = 1)","fc8f96a9":"#splitting the data into train and test set\n\nX = s_df.drop('stroke', axis= 1) # all columns except the target column\ny = s_df['stroke'] #the target column\n\n\n#splitting the data set into train and test sample\n# using 30% of the dataset as the test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state= 42)\n\nprint(X_train.shape)\nprint(X_test.shape)\n","fd39b6d4":"# dealing with the imabalnce dataset with imblearn library (SMOTE)\n\nfrom imblearn.over_sampling import SMOTE\n#SMOTE is an oversampling technique that generates synthetic samples\n#from the dataset which increases the predictive power for minority classes.\n\nsmote = SMOTE() \n\n# call the smote module only on the training sample\nX_smote, y_smote = smote.fit_resample(X_train, y_train)\n\nprint(X_smote.shape)\n\nsns.countplot(y_smote) #plotting to see the data distribution of the target after using SMOTE\n","c9f8db11":"testing = X_test['id'] #taking ID column for the purpose of submission","144ea7c5":"# drop the id column from both the train and test set\nX_smote = X_smote.drop(columns =['id'])# could have been .drop('id', axis = 1) if columns= wasn't set\nX_test = X_test.drop(columns =['id'])","78374f0b":"#scaling the data with min max\n\n# import module\nfrom sklearn.preprocessing import MinMaxScaler\n  \n# scale features\nscaler = MinMaxScaler() # minimum-maximum scaler module\nX_smote = scaler.fit_transform(X_smote) #call both fit and transform on the resampled training data\n\nX_test = scaler.transform(X_test) #call just transfrom on the test data","054bfbe1":"from sklearn.linear_model import LogisticRegression #import the Logistic Regression from library\n\nlog = LogisticRegression()\nlog.fit(X_smote,y_smote) #fit the model on the train data\n\n\n#use the model to evaluate the performance on the test set\npred = log.predict(X_test)","381e8ea8":"# import the metrics used for evaluation from sklearn library\nfrom sklearn.metrics import (f1_score, roc_auc_score,accuracy_score,\n                             precision_recall_curve, auc, roc_curve, recall_score)\n\nclf_log = classification_report(y_test, pred) #get classification report for performance of the logistic model\nprint(clf_log)","7da04b5a":"#confusion matrix for the prediction\ncm = confusion_matrix(y_test, pred)\ncm","2b9a4a1f":"#import from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import  VotingClassifier \n\n\nrdf = RandomForestClassifier(random_state = 42) #base random forest model\n\ndt = DecisionTreeClassifier(random_state = 42) #base decision tree model\n\nvoting = VotingClassifier(estimators = [('tree',dt), #build the voting model with decision tree and random forest \n                                        #as the two base sub model\n          ('rdf', rdf)],\n         voting = 'soft')\n          \n          \nvoting.fit(X_smote, y_smote) #fit the classifier on the resampled training data","0bde2b2b":"dtc, rd = voting.estimators_ #get the estimators for the two sub model\n\n#check performance of the voting classifier and the ones for the individuals\n\n#print the score for the individual model\nprint(voting.score(X_test, y_test))\nprint(dtc.score(X_test, y_test))\nprint(rd.score(X_test, y_test))","f5c50156":"#get the performance of the voting model on the prediction \n#as compared to the y_test values as classification report\nclf_v = classification_report(y_test, voting.predict(X_test))\nprint(clf_v)","cf1868c9":"#confusion matrix for the prediction\ncv = confusion_matrix(y_test, voting.predict(X_test))\ncv","a22b9fe2":"#get the performance of the random forest model on the prediction \n#as compared to the y_test values as classification report\n\nclf_r = classification_report(y_test, rd.predict(X_test))\nprint(clf_r) #print the report","2b2bb876":"#confusion matrix for the prediction\ncr = confusion_matrix(y_test, rd.predict(X_test))\ncr","b3577558":"xg = xgb.XGBClassifier() # generate the model\nxg.fit(X_smote, y_smote) #fit the model on the resampled training data\n\n\n#use the model to evaluate the performance on the test set\nxgpred = xg.predict(X_test)\n","0d39f067":"clf_x = classification_report(y_test, xgpred) #get the performance of the xgboost model on the \n#prediction as compared to the y_test values as classification report \nprint(clf_x)","855cb6e9":"#confusion matrix for the predictions using the xgboost model\ncx = confusion_matrix(y_test, xgpred)\ncx","cbc6a55f":"xgprd = xg.predict_proba(X_test)[:,1]\n\n#Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\nfpr_log, tpr_log, _ = roc_curve(y_test, xgprd)\nroc_auc_log = auc(fpr_log, tpr_log)\n\n#plot the AUC_ROC area\nsns.set_style(\"white\")\nplt.figure(figsize=(10, 7)) #to set the size of the figure generated\nplt.plot(fpr_log, tpr_log, color='darkorange',\n         label='ROC curve (area = %0.2f)' % roc_auc_log)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n\nplt.xlim([0.0, 1.0])#value range and limit on x axis\nplt.ylim([0.0, 1.05]) #value range and limit on y axis\n\nplt.xlabel('False Positive Rate',fontsize=18,labelpad =10) #Label for x axis\nplt.ylabel('True Positive Rate',fontsize=18) #Label for y axis\n\nplt.title('Receiver Operating Characteristic',fontsize=22).set_position([.5, 1.02]) #Plot title\nplt.legend(loc=\"lower right\",fontsize=13)\nplt.show()","98299bab":"submission = pd.DataFrame({'Id': testing, 'Stroke': xgpred}) #form a dataframe with only the id and predictions column\nsubmission.to_csv('submission.csv', index=False) #make the dataframe into a csv file  \nsubmission = pd.read_csv('submission.csv')\nsubmission","c8acef1f":"### Preprocessing","478b7886":"### Voting Classifier","7867ca99":"**Logistic Regression model**","83386776":"### Xgboost model","f672a089":"## **Exploratory Data Analysis (EDA)**","c181891c":"## **Building Models**","cf8e11a3":"**SMOTE - Synthetic Minority Over-sampling Technique**"}}