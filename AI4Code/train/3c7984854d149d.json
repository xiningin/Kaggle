{"cell_type":{"d4ff3b46":"code","f043a75b":"code","72c4be98":"code","c8bbd8c3":"code","8af187c3":"code","94a11107":"code","75faf5cc":"code","8fb3357a":"code","dc844e14":"code","099eadd5":"code","7629d78e":"code","8f1b5ad2":"code","92c81947":"code","0f9d8e3f":"code","79cdf11b":"code","98276d23":"code","59553749":"code","24c654d9":"code","f3da7527":"code","44dab13d":"code","8055bff9":"code","27624c73":"code","242bf008":"markdown","1bb1c8b0":"markdown","bcef086a":"markdown","a5e1faef":"markdown","3c3c515a":"markdown","2d09e782":"markdown","dd2fb703":"markdown","005020fa":"markdown","c2a9c527":"markdown","61882e2c":"markdown","422b0cde":"markdown","705b6b11":"markdown","572f5fdc":"markdown","126ac395":"markdown","cbb30059":"markdown","c54911dc":"markdown","5d39fa08":"markdown","a4809773":"markdown","8e8a4cb9":"markdown","e88da271":"markdown","674e8d05":"markdown","ca31eeae":"markdown"},"source":{"d4ff3b46":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, f1_score, accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f043a75b":"cancer = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ncancer = cancer.drop('id', axis=1)\ncancer.sample(5)","72c4be98":"def EDA(df):\n        \n    print('\\033[1m' + 'Shape of the data :' + '\\033[0m')\n    print(df.shape, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'All columns from the dataframe :' + '\\033[0m')\n    print(df.columns, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Datatpes and Missing values:' + '\\033[0m')\n    print(df.info(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Summary statistics for the data' + '\\033[0m')\n    print(df.describe(include='all'), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Outliers in the data :' + '\\033[0m')\n    Q1 = df.quantile(0.25)\n    Q3 = df.quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = (df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))\n    print(outliers.sum(), \n          '\\n------------------------------------------------------------------------------------\\n')\n        \n    print('\\033[1m' + 'Memory used by the data :' + '\\033[0m')\n    print(df.memory_usage(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Number of duplicate values :' + '\\033[0m')\n    print(df.duplicated().sum())\n          \nEDA(cancer)","c8bbd8c3":"# Dropping Unnamed column\ncancer = cancer.loc[:, ~cancer.columns.str.contains('^Unnamed')]\n\n# Encoding target variable\ncancer.diagnosis = cancer.diagnosis.astype('category')\ncancer.diagnosis = cancer.diagnosis.cat.codes\ncancer.diagnosis.value_counts()","8af187c3":"cancer_mean = cancer.loc[:, 'radius_mean':'fractal_dimension_mean']\ncancer_mean['diagnosis'] = cancer['diagnosis']","94a11107":"# Plotly's Scatterplot matrix\n\ndimensions = []\nfor col in cancer_mean:\n    dimensions.append(dict(label = col, values = cancer_mean[col]))\n    \nfig = go.Figure(data = go.Splom(\n                dimensions = dimensions[:-2],\n                showupperhalf=False,\n                diagonal_visible=False,\n                marker=dict(\n                    color='rgba(135, 206, 250, 0.5)',\n                    size=5,\n                    line=dict(\n                        color='MediumPurple',\n                        width=0.5))\n                ))\n\nfig.update_layout(\n    title='Pairplot for mean attributes of the dataset',\n    width=1100,\n    height=1500,\n)\n\nfig.show()","75faf5cc":"# Correlation matrix\n\nplt.figure(figsize = (20, 12), dpi = 150)\n\ncorr = cancer.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr,\n            mask = mask,\n            cmap = 'BuPu',\n            annot = True,\n            linewidths = 0.5,\n            fmt = \".2f\")\n\nplt.title('Correlation Matrix',\n          fontsize = 20,\n          weight = 'semibold',\n          color = '#de573c')\nplt.show()","8fb3357a":"def subplot_titles(cols):\n    '''\n    Creates titles for the subplot's subplot_titles parameter.\n    '''\n    titles = []\n    for i in cols:\n        titles.append(i+' : Distribution')\n        titles.append(i+' : Violin plot')\n        titles.append(i+' by Diagnosis')\n    \n    return titles\n\ndef subplot(cols, row = 0, col = 3):\n    '''\n    Takes a dataframe as an input and returns distribution plots for each variable.\n    '''\n    row = len(cols)\n    fig = make_subplots(rows=row, cols=3, subplot_titles = subplot_titles(cols))\n    \n    for i in range(row):\n        fig.add_trace(go.Histogram(x = cancer[ cols[i] ],\n                      opacity = 0.7),\n                      row=i+1, col=1)\n\n        fig.add_trace(go.Violin(y = cancer[cols[i]],\n                     box_visible=True),\n                     row=i+1, col=2)\n\n        fig.add_trace(go.Box(\n                     y = cancer[ cols[i] ][cancer.diagnosis == 0],\n                     marker_color = '#6ce366',\n                     name = 'Benign'\n                     ),row=i+1, col=3)\n\n        fig.add_trace(go.Box(\n                     y = cancer[ cols[i] ][cancer.diagnosis == 1],\n                     marker_color = '#de5147',\n                     name = 'Malignant'\n                     ),row=i+1, col=3)\n    \n    for i in range(row):\n        fig.update_xaxes(title_text = cols[i], row=i+1)\n    \n    fig.update_yaxes(title_text=\"Count\")\n    fig.update_layout(height= 450*row, width=1100,\n                  title = 'Summary of mean tumor attributes (For Diagnois : Green=Benign, Red=Malignant)',\n                  showlegend = False,\n                  plot_bgcolor=\"#f7f1cb\"\n                  )    \n    \n    fig.show()\n    \n    ","dc844e14":" x = subplot(cancer.drop('diagnosis', axis=1).columns)","099eadd5":"def outlier(df):\n        df_ = df.copy()\n        df = df.drop('diagnosis', axis=1)\n\n\n        q1 = df.quantile(0.25)\n        q3 = df.quantile(0.75)\n\n        iqr = q3 - q1\n\n        lower_limit = q1 -(1.5 * iqr) \n        upper_limit = q3 +(1.5 * iqr)\n\n\n        for col in df.columns:\n            for i in range(0,len(df[col])):\n                if df[col][i] < lower_limit[col]:            \n                    df[col][i] = lower_limit[col]\n\n                if df[col][i] > upper_limit[col]:            \n                    df[col][i] = upper_limit[col]    \n\n\n        for col in df.columns:\n            df_[col] = df[col]\n\n        return(df_)\n\ncancer = outlier(cancer)","7629d78e":"X = cancer.drop('diagnosis', axis=1)\ny = cancer.diagnosis","8f1b5ad2":"def VIF(df):\n    vif = pd.DataFrame()\n    vif['Predictor'] = df.columns\n    vif['VIF'] = [variance_inflation_factor(df.values, col) for col in range(len(df.columns))]\n    return vif\n\nvif_df = VIF(X).sort_values('VIF', ascending = False, ignore_index = True)\nprint(vif_df.head(8))\n\n# Removing features with VIF > 10,000\n\nhigh_vif_features = list(vif_df.Predictor.iloc[:2])\nvif_features = X.drop(high_vif_features, axis=1)","92c81947":"# Splitting data for training and testing\nX_train, X_test, y_train, y_test = train_test_split(vif_features, y, test_size = 0.2, random_state = 39)","0f9d8e3f":"# Logistic regression with VIF features , BaggingClassifier and hyperparameter tuning\n\nsteps = [('scaler', StandardScaler()),\n         ('log_reg', LogisticRegression())]\npipeline = Pipeline(steps)\n\nparameters = dict(log_reg__solver = ['newton-cg', 'lbfgs', 'liblinear'],\n                  log_reg__penalty =  ['l2'],\n                  log_reg__C = [100, 10, 1.0, 0.1, 0.01])\n\n\ncv = GridSearchCV(pipeline,\n                  param_grid = parameters,\n                  cv = 5,\n                  scoring = 'accuracy',\n                  n_jobs = -1,\n                  error_score = 0.0)\n\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\nlog_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(log_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","79cdf11b":"# KNN with VIF features and hyperparameter tuning\n\nsteps = [('scaler', StandardScaler()),\n         ('knn', BaggingClassifier(KNeighborsClassifier()))]\npipeline = Pipeline(steps)\n\nparameters = dict(knn__base_estimator__metric = ['euclidean', 'manhattan', 'minkowski'],\n                  knn__base_estimator__weights =  ['uniform', 'distance'],\n                  knn__base_estimator__n_neighbors = range(2,15),\n                  knn__bootstrap = [True, False],\n                  knn__bootstrap_features = [True, False],\n                  knn__n_estimators = [5])\n\n\ncv = GridSearchCV(pipeline,\n                  param_grid = parameters,\n                  cv = 5,\n                  scoring = 'accuracy',\n                  n_jobs = -1,\n                  )\n\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\nknn_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(knn_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","98276d23":"# SVC with VIF features and hyperparameter tuning\n\nsteps = [('scaler', StandardScaler()),\n         ('svc', SVC())]\npipeline = Pipeline(steps)\n\nparameters = dict(svc__kernel = ['poly', 'rbf', 'sigmoid'],\n                  svc__gamma =  [0.0001, 0.001, 0.01, 0.1],\n                  svc__C = [0.01, 0.05, 0.5, 0.1, 1, 10, 15, 20])\n\n\ncv = GridSearchCV(pipeline,\n                  param_grid = parameters,\n                  cv = 5,\n                  scoring = 'accuracy',\n                  n_jobs = -1)\n\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\nsvc_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(svc_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","59553749":"# Random Forest Classifier with VIF features and hyperparameter tuning\n\nsteps = [('scaler', StandardScaler()),\n         ('rf', RandomForestClassifier(random_state = 0))]\npipeline = Pipeline(steps)\n\nparameters = dict(rf__n_estimators = [10,100],\n                  rf__max_features = ['sqrt', 'log2'],\n)\n\n\ncv = GridSearchCV(pipeline,\n                  param_grid = parameters,\n                  cv = 5,\n                  scoring = 'accuracy',\n                  n_jobs = -1)\n\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\nrf_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(rf_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","24c654d9":"# Ridge Classifier with VIF features and hyperparameter tuning\n\nsteps = [('scaler', StandardScaler()),\n         ('ridge', RidgeClassifier())]\npipeline = Pipeline(steps)\n\nparameters = dict(ridge__alpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n\n\ncv = GridSearchCV(pipeline,\n                  param_grid = parameters,\n                  cv = 5,\n                  scoring = 'accuracy',\n                  n_jobs = -1,\n                  error_score = 0.0)\n\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\nridge_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(ridge_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","f3da7527":"# Gradient Boosting Classifier  with VIF features  and hyperparameter tuning\n\nsteps = [('scaler', StandardScaler()),\n         ('gbc', GradientBoostingClassifier())]\npipeline = Pipeline(steps)\n\nparameters = dict(gbc__n_estimators = [10,100,200],\n                  gbc__loss = ['deviance', 'exponential'],\n                  gbc__learning_rate = [0.001, 0.1, 1, 10]\n)\n\n\ncv = GridSearchCV(pipeline,\n                  param_grid = parameters,\n                  cv = 5,\n                  scoring = 'accuracy',\n                  n_jobs = -1,\n                  error_score = 0.0\n                  )\n\ncv.fit(X_train, y_train)\ny_pred = cv.predict(X_test)\ngb_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(gb_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","44dab13d":"# Extreme gradient Boosting classifier with VIF features\n\nxgb = XGBClassifier(max_depth = 5,\n                        min_child_weight = 1,\n                        gamma = 0.3,\n                        subsample = 0.8,\n                        colsample_bytree = 0.8,\n                        learning_rate = 0.1,\n                        reg_alpha=0.05,\n                        disable_default_eval_metric = True)\n\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_test)\n\nxgb_accuracy = accuracy_score(y_pred, y_test) * 100\n\nprint('\\033[1m' +'Best parameters : '+ '\\033[0m', cv.best_params_)\nprint('\\033[1m' +'Accuracy : {:.2f}%'.format(xgb_accuracy) + '\\033[0m')\nprint('\\033[1m' +'Classification report : '+ '\\033[0m\\n', classification_report(y_test, y_pred))\n\ncm = confusion_matrix(y_pred, y_test)\nprint('\\033[1m' +'Confusion Matrix : '+ '\\033[0m')\nsns.heatmap(cm, cmap = 'OrRd',annot = True, fmt='d')\nplt.show()","8055bff9":"# Accuracies of models\n\nresults = {'Model' :['Logistic Regression', 'KNN', 'SVC', 'Random Forest', 'Rigde Classifier', 'Gradient Boosting', 'XGBoost'],\n           'Accuracy' : [log_accuracy, knn_accuracy, svc_accuracy, rf_accuracy, ridge_accuracy, gb_accuracy, xgb_accuracy]}\n\nresults = pd.DataFrame(results).sort_values('Accuracy', ignore_index=True, ascending=False)\nresults.Accuracy = results.Accuracy.round(2)\nresults","27624c73":"fig = px.line(results,\n            x = results.Model,\n            y = results.Accuracy,\n            text=results.Accuracy,\n        )\nfig.update_traces(textposition = 'top right')\nfig.update_layout(title = 'Model vs Accuracy',\n                  plot_bgcolor = '#f9faed')\n\nfig.show()","242bf008":"We saw in our correlation matrix, many of our predictor variables were higly correlated.  \nTo avoid multicollinearity, we must deal with such columns.\n\nWe can use several techniques (Chi-sq test, Random Forest Importance, Forest Feature Selection, Exhaustive Feature Selection, fisher score just to name a few)  \nbut here, I'll use Variance inflation factor.","1bb1c8b0":"# Visualization","bcef086a":"# XGBoost","a5e1faef":"# Data Preprocessing","3c3c515a":"# Feature Selection using VIF","2d09e782":"# Support Vector Classifier","dd2fb703":"# ","005020fa":"# RESULTS","c2a9c527":"* Thanks for sitting through my notebook.\n* Any feedback will be valuable.","61882e2c":"### Separating features and target","422b0cde":"# Ridge Classifier","705b6b11":"# EDA","572f5fdc":"# Gradient Boosting","126ac395":"# Random Forest","cbb30059":"## Dealing with outliers","c54911dc":"# KNearestNeighbor","5d39fa08":"<img src= \"https:\/\/static-01.hindawi.com\/styles\/hindawi_wide\/s3\/2019-11\/Cancer_Awareness-2019_blog_v1.0_noText.jpg?itok=CR034IE-\" alt =\"Titanic\" style='width: 1080px;'>","a4809773":"# Breast Cancer Prediction\n\n### Disclaimer :  \n\n* Acccuracies and other metrics may change with random states of your splitting algorithms and models.  \n* To evaluate metrics representative of your model, run your model a few times with different random states and different splitting proportion and get their mean.\n\n\nTo know more about the data : https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data","8e8a4cb9":"Hypertuning parameters of XGBoost is out of scope of this notebook as it can take hours or even days to reach the optimal parameters.  \nBut in case you want to do so, you can try using something similar to the code below.\n\n\n\nparameters = dict(xgb__max_depth = range(3,10,2),  \n                  xgb__min_child_weight = range(1,6,2),  \n                  xgb__gamma = [i\/10.0 for i in range(0,5)],  \n                  xgb__subsample = [i\/10.0 for i in range(6,10)],  \n                  xgb__colsample_bytree = [i\/10.0 for i in range(6,10)],  \n                  xgb__reg_alpha = [1e-5, 1e-2, 0.1, 1, 100],  \n                  xgb__learning_rate = [0.001, 0.1, 1, 10])  \n\n\n","e88da271":"#### We see with hyperparamter tuning, three of our models got an out of sample accuracy of over 99%.","674e8d05":"We observe there's a strong positive correlation among a few variables.","ca31eeae":"# Logistic Regression"}}