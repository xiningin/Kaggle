{"cell_type":{"aa9dbcca":"code","5ec73559":"code","1cf50850":"code","2afc6c2a":"code","efe67aa9":"code","ce06a517":"code","9c8adb20":"code","49517060":"code","bcb4d943":"code","10749cca":"markdown","391a61ff":"markdown","a27cb711":"markdown","4cc45e12":"markdown","1eabfd70":"markdown","c3845047":"markdown","e7d51afc":"markdown","8c3a40c1":"markdown","a3533c06":"markdown"},"source":{"aa9dbcca":"import numpy as np\nfrom sklearn import preprocessing\nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nsns.set()","5ec73559":"lInterval = math.floor(29.7 - 1.96 * (14.5\/26.7))\nhInterval = math.ceil(29.7 + 1.96 * (14.5\/26.7))\nprint('---Train data---')\nprint('low','high')\nprint(lInterval,hInterval)\n\n# For test data\nlInterval = math.floor(30.3 - 1.96 * (14.2\/18.2))\nhInterval = math.ceil(30.3 + 1.96 * (14.2\/18.2))\nprint('---Test data---')\nprint('low','high')\nprint(lInterval,hInterval)","1cf50850":"raw_data = pd.read_csv('..\/input\/titanic-preprocessed-data\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic-preprocessed-data\/test.csv')\n\nraw_data = raw_data.drop(['PassengerId','Name','Ticket','Cabin','Fare'],axis=1)\nraw_data['Sex'] = raw_data['Sex'].map({'male':0,'female':1})\nraw_data['Embarked'] = raw_data['Embarked'].map({'C':0,'Q':1,'S':2})\nraw_data['Embarked'].fillna(1,inplace = True)\n\ntest_data = test_data.drop(['Name','Ticket','Cabin','Fare'],axis=1)\ntest_data['Sex'] = test_data['Sex'].map({'male':0,'female':1})\ntest_data['Embarked'] = test_data['Embarked'].map({'C':0,'Q':1,'S':2})\n\n\ndf = pd.DataFrame(raw_data)\ndf.to_csv('train_data.csv', header=False, index=False)\ntrain_data = np.loadtxt('.\/train_data.csv',delimiter=',')\ndf = pd.DataFrame(test_data)\ndf.to_csv('test_data.csv', header=False, index=False)\ntest_data = np.loadtxt('.\/test_data.csv',delimiter=',')\n\nraw_data","2afc6c2a":"inputs = train_data[:,1:]\ntargets = train_data[:,0]\n\n# Total samples\nsamples = inputs.shape[0]\n\n# Split count\ntrain_samples_count = int(0.8 * samples)\nvalidation_samples_count = samples - train_samples_count\n\n# Creating train set\ntrain_inputs = inputs[:train_samples_count]\ntrain_targets = targets[:train_samples_count]\n\n# Creating validation set\nvalidation_inputs = inputs[train_samples_count:]\nvalidation_targets = targets[train_samples_count:]\n\n# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\nprint(\"---Train---\")\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) \/ train_samples_count)\nprint(\"---Validation---\")\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) \/ validation_samples_count)","efe67aa9":"np.savez('.\/Titanic_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('.\/Titanic_data_validation', inputs=validation_inputs, targets=validation_targets)","ce06a517":"# Train set\nnpz = np.load('Titanic_data_train.npz')\ntrain_inputs, train_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# Validation set\nnpz = np.load('Titanic_data_validation.npz')\nvalidation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)","9c8adb20":"input_size = 6\noutput_size = 2\n\n# Configuring the NN values. Hidden layers = 2\nhidden_layer_size = 60\nbatch_size = 100\nmax_epochs = 100\n\n# Early stopping mechanism with 5 patience level. Which means the model will continue learing until the error has been minimized \n# and cannot minimize further. The model will stop learning after 5 (patience level) such instances\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=5)\n        \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(hidden_layer_size,activation='relu'),  \n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),     \n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\nmodel.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(train_inputs, \n          train_targets, \n          batch_size=batch_size, \n          epochs=max_epochs, \n          callbacks=[early_stopping], \n          validation_data=(validation_inputs, validation_targets), \n          verbose = 2 \n          )  ","49517060":"test_data_inputs = test_data[:,1:]\ntest_data_output = test_data[:,0]\ntest_data_inputs\npredictions = model.predict(test_data_inputs)\nsurvival = np.argmax(predictions, axis = 1)\n#test_data_output[1] = survival\n\ndata = {'PassengerId':test_data_output,\n       'Survived':survival}\noutput = pd.DataFrame(data)\noutput.to_csv('.\/predictions.csv')\nprint(output)","bcb4d943":"## Accuacy = 78% on Test set","10749cca":"## Split Training data","391a61ff":"# Create *.npz for model","a27cb711":"# PreProcessing","4cc45e12":"# Titanic Survival Prediction","1eabfd70":"## Load data","c3845047":"# Load npz file","e7d51afc":"# Model","8c3a40c1":"# Imports","a3533c06":"## Excel Pre-Processing\nI have used the z-score distribution to find the confidence interval of Age. As there are 177 records with missing age values, the missing values have been populated using random number generated between the below interval.\n\nWe cannot exclude 177 records as they are almost 20% of the current data samples. Hard-coded values are used in the below formula as the values are the ones before performing excel pre-processing."}}