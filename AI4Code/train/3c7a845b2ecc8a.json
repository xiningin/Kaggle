{"cell_type":{"e71dba5f":"code","66fc04d0":"code","909f0754":"code","2c991939":"code","b8125faa":"code","1f26d499":"code","8a7a253c":"code","e4e6020d":"code","6ca10b1c":"code","a47b42eb":"code","78038f81":"code","968073a9":"code","217bba5d":"code","8d4c5301":"code","e7968ed8":"code","4fe8be80":"code","20c1d1aa":"code","8787e199":"code","5843c802":"code","c2aa0b1e":"code","a2bf1873":"code","83d2a90e":"code","d2022822":"code","271d44bd":"code","75e1fb06":"code","a0dd4f26":"code","2296b848":"code","5f60f220":"code","eccfd638":"markdown","2d1b0fe1":"markdown","615a006c":"markdown","b01ff638":"markdown","67ebd739":"markdown","5b4dcc19":"markdown","40550156":"markdown","ab71de8d":"markdown","f4fed2b4":"markdown","aed76d43":"markdown","30666a63":"markdown","94a76ba4":"markdown","b1519dbb":"markdown","c9dc498a":"markdown","0b317bea":"markdown","49a57ef8":"markdown"},"source":{"e71dba5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","66fc04d0":"#importing dependencies\nimport numpy as np  #python library for scientific computing\nimport pandas as pd #python library for data analysis and dataframes","909f0754":"data = pd.read_csv('..\/input\/ex1data2.txt', header=None)\ndata.head()","2c991939":"data.columns =(['Size','Bedroom','Price'])\ndata.head()","b8125faa":"data.drop('Bedroom', axis=1, inplace=True)\ndata.head()","1f26d499":"data = data.sample(frac=1)\ndata.head()","8a7a253c":"# necessary dependencies for plotting\nimport matplotlib.pyplot as plt #python library for plot and graphs\n%matplotlib inline","e4e6020d":"plt.plot(data.Size, data.Price, 'r.')\nplt.show()","6ca10b1c":"# another way to test the correlation\ndata.corr()","a47b42eb":"class LinearModel():\n    \n    def __init__(self, features, target):\n        self.X = features\n        self.y = target\n    \n    def GradDesc(self, parameters, learningRate, cost): # Successive Approximation\n        self.a = learningRate # step\n        self.c = cost # loss or error rate. How far off are we? Sum of distances.\n        self.p = parameters\n        return self.a, self.Cost(self.c), self.p\n    \n    def Cost(self,c): # loss or error rate. How far off are we? Sum of distances.\n        if c =='RMSE': # Reduced average distance\n            return self.y\n        elif c == 'MSE': #Positive average distance\n            return self.X\n            \n            \nX=1\ny=0\na = LinearModel(5,4)\nprint(a.GradDesc(2,0.01,'MSE'))\nprint(a.Cost('RMSE'))\n\n# list, array, \n","78038f81":"\n# Vector - List\/Array\nv = [\n    12,\n    1,\n    34,\n    23\n]\n\n#      3\n# n = v\n#      2\n\n# # Matrix - List of Lists, Nested Arrays\n# m = [\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#     ]\n\n# # Tensor 3 or more dimensions (shape) - List of Lists of Lists, deeply nested array\n# t = [\n#     [\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#     ],\n# [\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#     ],\n# [\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#         [12,1,34,23],\n#     ]\n# ]","968073a9":"## given a matrix A (3x2) and a matrix B (1x2)\nA = np.array([[1,2],\n              [1,3],\n              [1,4]])\n\nB = np.array([[2],[3]])\n\nprint('A =')\nprint(A,'\\nsize =',A.shape) # Size\/Dimensions - Height & Width\nprint('\\nB =')\nprint(B,'\\nsize =',B.shape)","217bba5d":"# let's try it\nH = A.dot(B)\nprint(H)","8d4c5301":"X = np.array(data.drop('Price',axis=1))\ny = np.array(data.Price)\nm = len(data)\n\nprint(X.shape)\nprint(y.shape)\nprint(m)","e7968ed8":"y = y.reshape((m,1))\nprint(y.shape)","4fe8be80":"def normscaler(Z, normal=False, scale='max'): \n    Zn = np.zeros(Z.shape)\n    for col in range(Zn.shape[1]):\n        std = Z[:,col].std()\n        clm = Z[:,col]\n        mn = Z[:,col].mean()\n        mx = Z[:,col].max()\n        nrm = 0\n        sclr = 1\n        if normal:\n            nrm = mn\n        if scale =='max':\n            sclr = mx\n        elif scale == 'std':\n            sclr = std\n        Zn[:,col] = (clm-nrm)\/sclr\n        \n    return Zn\n    \nXn = normscaler(X, normal=True, scale='std')\nyn = normscaler(y, normal=True, scale='std')","20c1d1aa":"plt.plot(Xn, yn, 'r.')\nplt.show()","8787e199":"# parameter initialization\ntheta = np.array([0.9,-1])","5843c802":"lineX = np.linspace(Xn.min(), Xn.max(), 100)\nliney = [theta[0] + theta[1]*xx for xx in lineX]\n\nplt.plot(Xn,yn,'r.', label='Training data')\nplt.plot(lineX,liney,'b--', label='Current hypothesis')\nplt.legend()\nplt.show()","c2aa0b1e":"def cost_function(X, y, theta, deriv=False):\n    z = np.ones((len(X),1))\n    X = np.append(z, X, axis=1)\n    \n    if deriv:\n        loss     = X.dot(theta)-y\n        gradient = X.T.dot(loss)\/len(X)\n        return gradient, loss\n        \n    else:\n        h = X.dot(theta)\n        j = (h-y.flatten())\n        J = j.dot(j)\/2\/(len(X))\n        return J\n    \ncost_function(Xn, yn, theta)","a2bf1873":"def GradDescent(features, target, param, learnRate=0.01, multiple=1, batch=len(X), log=False):\n\n    iterations = batch*len(features)\n    epochs     = iterations*multiple\n    y          = target.flatten()\n    t          = param\n    b          = batch\n    a          = learnRate # step or alpha\n    \n    theta_history  = np.zeros((param.shape[0],epochs)).T\n    cost_history   = [0]*epochs\n    \n    for ix in range(epochs):\n        \n        i    = epochs%len(X)\n        cost = cost_function(features[i:i+b], y[i:i+b], t)\n\n        cost_history[ix]   = cost\n        theta_history[ix]  = t\n\n        g, l = cost_function(features[i:i+b], y[i:i+b], t, deriv=True)\n        t    = t-a*g\n        \n        if log:\n            if ix%250==0:\n                print(\"iteration :\", ix+1)\n                #print(\"\\tloss     = \", l)\n                print(\"\\tgradient = \", g)\n                print(\"\\trate     = \", a*g)\n                print(\"\\ttheta    = \", t)\n                print(\"\\tcost     = \", cost)\n            \n    return cost_history, theta_history\n\n# hyperparameters\nalpha = 0.001 # step or learning rate\nmul = 100\nbat = 80\nch, th = GradDescent(Xn,yn,theta,alpha,mul,bat,log=False) # FIT FUNCTION","83d2a90e":"lineX = np.linspace(Xn.min(), Xn.max(), 100)\nliney = [th[-1,0] + th[-1,1]*xx for xx in lineX]\n\nplt.plot(Xn,yn,'r.', label='Training data')\nplt.plot(lineX,liney,'b--', label='Current hypothesis')\nplt.legend()\nplt.show()","d2022822":"plt.plot(ch,'g--')\nplt.show()","271d44bd":"plt.plot(th[:,0],'r-.')\nplt.plot(th[:,1],'b-.')\nplt.show()","75e1fb06":"#Grid over which we will calculate J\ntheta0_vals = np.linspace(-2, 2, 100)\ntheta1_vals = np.linspace(-2, 3, 100)\n\n#initialize J_vals to a matrix of 0's\nJ_vals = np.zeros((theta0_vals.size, theta1_vals.size))\n\n#Fill out J_vals\nfor t1, element in enumerate(theta0_vals):\n    for t2, element2 in enumerate(theta1_vals):\n        thetaT = np.zeros(shape=(2, 1))\n        thetaT[0][0] = element\n        thetaT[1][0] = element2\n        J_vals[t1, t2] = cost_function(Xn, yn, thetaT.flatten())\n\n#Contour plot\nJ_vals = J_vals.T","a0dd4f26":"A, B = np.meshgrid(theta0_vals, theta1_vals)\nC = J_vals\n\ncp = plt.contourf(A, B, C)\nplt.colorbar(cp)\nplt.plot(th.T[0],th.T[1],'r--')\nplt.show()","2296b848":"#Animation\nimport matplotlib.animation as animation\n\n#Set the plot up,\nfig = plt.figure(figsize=(12,5))\n\nplt.subplot(121)\nplt.plot(Xn,yn,'ro', label='Training data')\nplt.title('Housing Price Prediction')\nplt.axis([Xn.min()-Xn.std(),Xn.max()+Xn.std(),yn.min()-yn.std(),yn.max()+yn.std()])\nplt.grid(axis='both')\nplt.xlabel(\"Size of house in ft^2 (X1) \")\nplt.ylabel(\"Price in $1000s (Y)\")\nplt.legend(loc='lower right')\n\nline, = plt.plot([], [], 'b-', label='Current Hypothesis')\nannotation = plt.text(-2, 3,'',fontsize=20,color='green')\nannotation.set_animated(True)\n\nplt.subplot(122)\ncp = plt.contourf(A, B, C)\nplt.colorbar(cp)\nplt.title('Filled Contours Plot')\nplt.xlabel('theta 0')\nplt.ylabel('theta 1')\ntrack, = plt.plot([], [], 'r-')\npoint, = plt.plot([], [], 'ro')\n\nplt.tight_layout()\nplt.close()\n\n#Generate the animation data,\ndef init():\n    line.set_data([], [])\n    track.set_data([], [])\n    point.set_data([], [])\n    annotation.set_text('')\n    return line, track, point, annotation\n\n# animation function.  This is called sequentially\ndef animate(i):\n    fit1_X = np.linspace(Xn.min()-Xn.std(), Xn.max()+Xn.std(), 1000)\n    fit1_y = th[i][0] + th[i][1]*fit1_X\n    \n    fit2_X = th.T[0][:i]\n    fit2_y = th.T[1][:i]\n    \n    track.set_data(fit2_X, fit2_y)\n    line.set_data(fit1_X, fit1_y)\n    point.set_data(th.T[0][i], th.T[1][i])\n    \n    annotation.set_text('Cost = %.4f' %(ch[i]))\n    return line, track, point, annotation\n\nanim = animation.FuncAnimation(fig, animate, init_func=init,\n                               frames=800, interval=0, blit=True)\n\nanim.save('animation.gif', writer='imagemagick', fps = 30)","5f60f220":"#Display the animation...\nimport io\nimport base64\nfrom IPython.display import HTML\n\nfilename = 'animation.gif'\n\nvideo = io.open(filename, 'r+b').read()\nencoded = base64.b64encode(video)\nHTML(data='''<img src=\"data:image\/gif;base64,{0}\" type=\"gif\" \/>'''.format(encoded.decode('ascii')))","eccfd638":"Suppose A is our feature matrix <b><i>X<\/i><\/b> and B as our parameter matrix <b><i>theta<\/i><\/b>, that is,\n$$X = [\\ 1\\ 2\\ ] \\ \\ \\ \\theta = [\\ 2\\ 3\\ ]$$ \n$$[\\ 1\\ 3\\ ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\n$$[\\ 1\\ 4\\ ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\nRemember that we have our linear model\n$$h(x) = \\theta_0 x_0 + \\theta_1 x_1$$\nWe know that\n$$X_0 = [\\ 1\\ ] \\ \\ \\ X_1 = [\\ 2\\ ] \\ \\ \\ \\theta^T = [\\ 2\\ ] $$ \n$$\\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 1\\ ]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 3\\ ]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 3\\ ]$$\n$$[\\ 1\\ ] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ [\\ 4\\ ]\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $$\nthen we can actually use matrix dot product to do the multiplication and addition at the same time\n(and faster)\n$$H=[\\ \\theta_0 X_0^0+\\theta_1 X_1^0\\ ]=[\\ \\theta_0+\\theta_1X_1^0\\ ]=[\\ 2+3(2)\\ ]=[\\ \\ 8\\ \\ ]\\ \\ \\ \\ \\ \\ \\ \\ $$\n$$[\\ \\theta_0 X_0^1+\\theta_1 X_1^1\\ ]\\ \\ \\ \\ \\ [\\ \\theta_0+\\theta_1 X_1^1\\ ]\\ \\ \\ \\ \\ [\\ 2+3(3)\\ ]\\ \\ \\ \\ \\ [\\ 11\\ ]$$\n$$[\\ \\theta_0 X_0^2+\\theta_1 X_1^2\\ ]\\ \\ \\ \\ \\ [\\ \\theta_0+\\theta_1 X_1^2\\ ]\\ \\ \\ \\ \\ [\\ 2+3(4)\\ ]\\ \\ \\ \\ \\ [\\ 14\\ ]$$\n\n$$ can\\ be\\ as\\ simple\\ as $$\n\n$$ H = X \\ dot \\ \\theta $$\n<i>Yes, that is the power of <b>Matrices<\/b>!<\/i>","2d1b0fe1":"## Supervised Learning\n\nIn supervised learning, the training data you feed to the algorithm includes the desired solutions,\ncalled <i>labels<\/i>\n\n* A typical supervised learning task is <b>classification<\/b>. The spam filter is a good example of this: \nit is trained with many examples emails along with their <i>class<\/i> (spam or ham), and it must learn how to\nclassify new emails.\n* Another typical task is to predict a <i>target<\/i> numeric value, such as the price of a car, given a set\nof <i>features<\/i> (mileage, age, brand, etc.) called <i>predictors<\/i>. This sort of task is called <b>regression<\/b>.\nTo train the system, you need to give it many examples of cars, including both their predictors and\ntheir labels (i.e., their prices)\n   \n<i>Let's try <b>Regression!<\/b><i>","615a006c":"## Featuring Scaling & Normalization\n\nLet's go back to our data and store it as <b>X<\/b> (features) and <b>y<\/b> (target) matrices first\nalso <b>m<\/b> (number of sample data also called 'training samples')","b01ff638":"From the plot results we could see that there is a <b>high correlation<\/b> between Housing <b>Area<\/b>\nand Housing <b>Price<\/b> (obviously) and therefore we could use a <b>line<\/b> (linear model) to fit this data.","67ebd739":"# Machine Learning #1: Linear Regression in the beginning\n\nThis tutorial series is for absolute beginners in machine learning algorithms, for those who want to review\/practice the fundamentals of machine learning and how to build them from scratch.","5b4dcc19":"Now that looks much <b>simpler<\/b>!\nLet's <b>plot<\/b> our data and draw some <b>insights<\/b> of how a <b>linear model<\/b> could fit.","40550156":"Wow! Worked like a charm ;)","ab71de8d":"The data itself does not contain feature names or labels, let's set that up first.\nAccording to the source the first column is the <b>size<\/b> of the house in sq.ft. followed by \nthe no. of <b>bedrooms<\/b> and lastly the <b>price<\/b>.","f4fed2b4":"## What is Machine Learning?\n\n\"Machine Learning is the science (and art) of programming computers so they can <i>learn from data <\/i>\"\n- Aurelion Geron, 2017","aed76d43":"## Linear Regression: Univariate\n\nLet's start with a very simple task of <i>linear regression<\/i> using a sample dataset called Portland \nHousing Prices, wherein we are given some <i>features<\/i> of a house (i.e. area, no. of rooms, etc) and\npredict the <i>target<\/i> price.\n\nTo make things much simpler. Let us use only one <i>feature<\/i> or in this case one variable, also known as \n<b>univariate linear regression<\/b>. That is we are only gonna use the 'Area' of a given house to train a\nlinear model\n\n<i>Let's <b>get<\/b> the data and <b>examine<\/b> it!<\/i>","30666a63":"## Matrix Math\n\nAs it turns, using Matrices and Vectors is actually very convenient in these type of problems\n(talking about the obvious)\nTo demonstrate that let's have an example:","94a76ba4":"## Type of Machine Learning Systems\n\n<br>reference 1: [Hands-On Machine Learning with Scikit-Learn and Tensorflow](http:\/\/shop.oreilly.com\/product\/0636920052289.do)\n\nreference 2: [Machine Learning, Stanford University by Andrew Ng](https:\/\/www.coursera.org\/learn\/machine-learning)\n\n<br>There are a lot of different types of Machine Learning Systems and usually it is best to classify them in broad categories based on:\n\n    * Whether or not they are trained with human supervision (supervised, unsupervised, \n        semisupervised, and Reinforcement Learning  \n    \n    * Whether or not they can learn incrementally on the fly (online versus batch learning)\n    \n    * Whether they work by simply comparing new data points to know data points or instead \n        detect patterns in the training data and build a predictive model, much like scientists\n        do (instance-based versus model-based learning)\n\n<i> Let's look at the very <b> first criteria <\/b> a bit more closely... <\/i>","b1519dbb":"## Supervised\/Unsupervised Learning\n\nMachine Learning systems are usually classified according to the amount and type of supervision they\nget during training. There are four major categories: supervised learning, unsupervised learning,\nsemisupervised learning, and Reinforcement Learning.\n\n<i> Let us tackle <b> Supervised Learning <\/b> for now <i>\n\n","c9dc498a":"Looking at the <b>y<\/b> variable, it is shaped as a flattened array <i>(47, )<\/i>\nLet's reshape it to a matrix of form <i>(47, 1)<\/i>","0b317bea":"## Linear Model\n\nThe idea of linear regression is to fit a line to a set of points.\nSo let's use the line function given by:\n$$f(x) = y = mx + b$$\nwhere <b>m<\/b> is the slope and <b>b<\/b> is our <b>y<\/b> intercept, or for a more general form (multiple variables)\n$$h(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$$\nsuch that for a single variable where <b>n = 1<\/b>, \n$$h(x) = \\theta_0 + \\theta_1 x_1$$\n$$ when \\space x_0 = 1 $$\nwhere theta is our <b>parameters<\/b> (slope and intercept) and h(x) is our <b>hypothesis<\/b> or predicted value\n","49a57ef8":"Let us remove the 'Bedroom' feature since we are doing <b>univariate linear regression<\/b>"}}