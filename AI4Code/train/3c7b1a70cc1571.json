{"cell_type":{"87275b4d":"code","121a121d":"code","931dfc76":"code","98891a3f":"code","a2e99849":"code","0434f99c":"code","e445dbe4":"code","ace60745":"code","a11333b8":"code","61dc8a00":"code","6f1a8d1e":"code","0b64bdb7":"code","4313fa50":"code","2fe7884e":"code","0d1e2bfa":"code","2197b02a":"code","06cdb482":"code","fe513383":"code","305bcfa9":"code","a5a922f0":"code","4990d633":"code","97334180":"code","93a5a6a0":"code","a4e06169":"code","dad9f0a2":"code","3c94e1ef":"code","b6b07401":"code","49bc92bf":"code","2c51e73f":"code","e459b5fe":"markdown","f58c9627":"markdown","45684d05":"markdown","a22bf5e0":"markdown","c60ba643":"markdown","61187082":"markdown","5d53c1e0":"markdown","91586557":"markdown","75dd9f12":"markdown","f4a3a36c":"markdown","fa714f20":"markdown","aa926c98":"markdown","dffcfa2f":"markdown","9b7707b3":"markdown","14884711":"markdown","82ae443a":"markdown","9d59cd6f":"markdown","a2bba199":"markdown","6db2f558":"markdown","27c4378c":"markdown","8475edaa":"markdown","ec31bc15":"markdown","93d6c1a6":"markdown","cdb6387d":"markdown","d24440f0":"markdown","ff823f1c":"markdown","c6df1bd5":"markdown","a51169b7":"markdown","ad224bf0":"markdown","10583bd9":"markdown","86cf35ec":"markdown"},"source":{"87275b4d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nimport collections\nimport scipy.stats as stats\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","121a121d":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","931dfc76":"df.head()","98891a3f":"df.dtypes","a2e99849":"sns.factorplot(x='Class' , kind='count' , data=df , palette=['r','g'])","0434f99c":"sns.distplot(df['Amount'],fit=stats.norm)","e445dbe4":"from sklearn.preprocessing import StandardScaler\ndf['normAmount'] = StandardScaler().fit_transform(df[['Amount']])\ndf = df.drop(['Amount'],axis=1)","ace60745":"sns.distplot(df['Time'],fit=stats.norm)","a11333b8":"df['normTime'] = StandardScaler().fit_transform(df[['Time']])\ndf = df.drop(['Time'],axis=1)","61dc8a00":"df.head()","6f1a8d1e":"df.Class.value_counts()","0b64bdb7":"# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n# Shuffle dataframe rows\nndf = normal_distributed_df.sample(frac=1, random_state=42)\n\nndf.head()","4313fa50":"sns.factorplot(x='Class' , kind='count' , data=ndf , palette=['r','g'])","2fe7884e":"f, (g1, g2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire (Unbalanced) DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=g1)\ng1.set_title(\"UNbalanced Correlation Matrix \\n (not required)\", fontsize=14)\n\n# Balanced DataFramw\nsub_sample_corr = ndf.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=g2)\ng2.set_title('SubSample Correlation Matrix \\n (required)', fontsize=14)\nplt.show()","0d1e2bfa":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nsns.boxplot(x=\"Class\", y=\"V17\", data=ndf, palette='Accent', ax=axes[0])\naxes[0].set_title('V17 vs Class')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=ndf, palette='Accent', ax=axes[1])\naxes[1].set_title('V14 vs Class')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=ndf, palette='Accent', ax=axes[2])\naxes[2].set_title('V12 vs Class')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=ndf, palette='Accent', ax=axes[3])\naxes[3].set_title('V10 vs Class')\n\nplt.show()","2197b02a":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=ndf, palette='Accent', ax=axes[0])\naxes[0].set_title('V11 vs Class')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=ndf, palette='Accent', ax=axes[1])\naxes[1].set_title('V4 vs Class')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=ndf, palette='Accent', ax=axes[2])\naxes[2].set_title('V2 vs Class')\n\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=ndf, palette='Accent', ax=axes[3])\naxes[3].set_title('V19 vs Class')\n\nplt.show()","06cdb482":"# for scaled dataframe\nX_ndf = ndf.drop('Class', axis=1)\ny_ndf = ndf['Class']\nX_train_ndf, X_test_ndf, y_train_ndf, y_test_ndf = train_test_split(X_ndf, y_ndf, test_size=0.2, random_state=42)\n\n# for unbalanced dataframe\nX = df.drop('Class', axis=1)\ny = df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","fe513383":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","305bcfa9":"X_train_ndf = X_train_ndf.values\nX_test_ndf = X_test_ndf.values\ny_train_ndf = y_train_ndf.values\ny_test_ndf = y_test_ndf.values","a5a922f0":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","4990d633":"from sklearn.model_selection import cross_val_score\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train_ndf, y_train_ndf)\n    training_score = cross_val_score(classifier, X_train_ndf, y_train_ndf, cv=10)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","97334180":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train_ndf, y_train_ndf)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train_ndf, y_train_ndf)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\n# Overfitting Case\nsvc_score = cross_val_score(svc, X_train_ndf, y_train_ndf, cv=10)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train_ndf, y_train_ndf, cv=10)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","93a5a6a0":"y_pred_ndf=grid_tree.predict(X_test_ndf)\nfrom sklearn import metrics\nmetrics.accuracy_score(y_pred_ndf, y_test_ndf)","a4e06169":"tree_clf","dad9f0a2":"grid_tree.fit(X_train,y_train)","3c94e1ef":"y_pred=grid_tree.predict(X_test)","b6b07401":"from sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\ny_pred = grid_tree.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\nroc_auc","49bc92bf":"from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,f1_score\ncm = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred, normalize = True)\nprecision = precision_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)","2c51e73f":"print(\"\\nconfusion_matrix{}\\n\".format(cm))\nprint(\"accuracy_score{}\\n\".format(accuracy))\nprint(\"precision_score{}\\n\".format(precision))\nprint(\"f1_score{}\\n\".format(f1))","e459b5fe":"### Converting these values to numpy arrayys","f58c9627":"As we can see in the plot above the data is completly balanced.","45684d05":"## Moving on to Classifiers:","a22bf5e0":"We can see that all the data is in the numerical format and is continuous except for the **class** which is the dependent variable.","c60ba643":"## Importing the required libraries","61187082":"#### Negative Correlation :","5d53c1e0":"## Thank You","91586557":"**Heatmap or the Correlation Matrix** is a very essential part of understanding data. It tells us about presence of negative and positive correlation among the variables and how significant this correlation is.\n#### Inferences from above plot are:\n  *  V17, V14, V12 and V10 are **negatively correlated**. Notice how the lower these values are, the more likely the end result will be a fraud transaction.\n  * V2, V4, V11, and V19 are **positively correlated**. Notice how the higher these values are, the more likely the end result will be a fraud transaction.","75dd9f12":"## Using Grid Search CV","f4a3a36c":"### Let's have look at the data:","fa714f20":"## Plot ROC Curve","aa926c98":"** Clearly the data is too unbalaced** . This can create example of accuracy paradox in which our accuracy is high but we'll be predicting all the 1's incorrectly.\n#### To avoid this problem in this case we do the following things:\n    We will change the performance metrics to **Confusion Matrix, F1 Score, Kappa, Roc curves**\n    We will also try resampling and feature engineering.","dffcfa2f":"**Time variable is also not normalized**. So we have to do the same thing for Time variable.","9b7707b3":"## Fitting it in whole data:","14884711":"Clearly the **Amount** variable is not normalized, so we'll normalize it frist.","82ae443a":"## Impoting the dataset:","9d59cd6f":"## K-Fold Cross Validation","a2bba199":"## Resampling:\n    We will resample the data in train data will be split into the 50:50 ratio. We have to use the undersampling techbique.","6db2f558":"### Let's verify this with the help of BoxPlots","27c4378c":"### Classifiers taken into account:","8475edaa":"#### We get very impressive results.","ec31bc15":"## Making Predictions:","93d6c1a6":"### We have a quite good (almost 90%) area under the curve.","cdb6387d":"Therefore we have to split the data into 492 equally sized sample.","d24440f0":"### Splitting the data:","ff823f1c":"### These are my results. Suggestions are always welcomed. Please Upvote\/Star if you like my work.","c6df1bd5":"#### These are the best parameters.","a51169b7":"## Analyzing the performance metrics:","ad224bf0":"## Let's see Correlation Matrix","10583bd9":"## Introduction:\n    This kernel\/noterbook contains the complete Exploratory Data Analysis, Feature Engeering, and Various Machine Learning\n    techniques to detect if a transaction is fraud or not.\n    This will also the contain various statistical aspects of dataset like skewness.etc.","86cf35ec":"### Now we have a completly balanced data. Let's verify it."}}