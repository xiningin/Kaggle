{"cell_type":{"932c2952":"code","0531f671":"code","83f535f5":"code","431fec2b":"code","0c1881f0":"code","56cccf95":"code","1e23a3b9":"code","c04ec2ac":"code","8cc1d47d":"code","fc9a4e6d":"code","f44dd433":"code","37fc6326":"code","7ef7006e":"code","78e37318":"code","0b8c8e69":"code","b6a0786a":"code","34f79b45":"code","3393e6e5":"code","ea06abde":"code","c52f2d20":"code","11538b74":"code","81838358":"code","851d81ae":"code","e6eee648":"code","2b28003f":"code","49d39acb":"code","e2534d45":"code","04fdf7a6":"code","35bd3c11":"code","e947a720":"code","3c633a3a":"code","c9812a38":"code","1e63ba1c":"code","c63e7c26":"code","6ef418d2":"code","885580bf":"code","39b9be41":"code","490bc97a":"code","ea0c5731":"code","2e5c3d79":"code","d79bd1e2":"code","2a1b4121":"code","83f998bc":"code","85e7a115":"code","a1581780":"code","0e56fbe6":"code","fa644255":"code","968b1a01":"code","aaaf5264":"code","9d6fe3a6":"code","bbc58f09":"code","129b5a97":"code","7606c7ce":"code","041afec6":"code","f0847f00":"code","75109b5c":"code","4d52df99":"code","f20bff28":"code","19cad298":"code","6978b92f":"code","ac3b3e45":"code","5c4a42ae":"code","9a0c9e4c":"code","396af7c3":"code","9e025eb5":"code","719b9008":"code","49812c43":"code","2ebf070f":"code","1b8d705c":"code","76a75210":"code","95d5ae3a":"code","4d6b810f":"code","9a204520":"code","b6f0c2b7":"code","c9fed149":"code","8a623a4c":"code","3e30c106":"code","ded129c2":"code","4a907456":"code","fc8fdcca":"code","59c30553":"code","31d81d14":"code","c4c76b25":"code","ec62d19b":"code","abf1d4a0":"code","636eb88d":"code","14c30189":"code","8494a6fd":"code","0e10ac20":"code","855edd41":"code","d0ff4106":"code","ef5acb65":"code","db64c7eb":"code","41d2fa7c":"code","6d97703a":"code","6e525a98":"code","7e809bd2":"code","516e3b63":"code","98e2831e":"code","070cb69f":"code","40f4b5ed":"code","e1d35f33":"code","a661f18f":"code","1dfa2add":"code","188d73cc":"code","72523396":"code","aae11925":"code","5ad16899":"code","804f30e3":"code","5c138833":"code","0fdb1ecc":"code","087d4658":"code","7dd896db":"code","85a29916":"code","d2175ff5":"code","d46f6fa1":"code","cbba4535":"code","8ff55fad":"code","7f307611":"code","f1a8bbeb":"code","c653c2d5":"code","1337d16c":"code","206098b3":"code","bd89022d":"code","69dce530":"code","6c2a308b":"code","b2eaf14d":"code","797927d5":"code","a7ee3b04":"code","5b6b9164":"code","98a74b96":"code","f42fcfee":"code","edef9a25":"code","dd1a56ac":"code","ccad8c6a":"code","f2988fb0":"code","18d704ea":"code","88e558c4":"code","31781f04":"code","d42c4bda":"code","c7f26099":"code","63f8b60f":"code","284f8aa0":"code","2a073f7f":"code","13522671":"code","862ec22f":"code","f76e068c":"code","cbfcab76":"code","2a252512":"code","6a383215":"code","5764caf4":"code","08810244":"code","b612445d":"code","316829f2":"code","4054e2fc":"code","3603a734":"code","98e726cb":"code","b32cd2aa":"code","0fbfce50":"code","a3de8eb5":"code","46fd0eb8":"code","54c832be":"code","545c52fb":"code","94a41cd6":"code","caf833d9":"code","d6543538":"code","7123db57":"code","4e5b767a":"code","c869d1af":"code","a2c64a2b":"code","1c31bf91":"code","a8762969":"code","2e7d0c74":"code","faf80524":"code","36978614":"code","e84ff0fa":"code","2e203d3d":"code","71703cd6":"code","a4d8bd55":"code","49f622e1":"code","f39d5d47":"code","e124df14":"code","df17e618":"markdown","932f9b8b":"markdown","8782d965":"markdown","b4354563":"markdown","04b082b5":"markdown","9aa7147e":"markdown","5e57bee0":"markdown","7fe87a64":"markdown","2551d7ab":"markdown","d976fe2a":"markdown","6ec972f3":"markdown","78f433f4":"markdown","dbf31917":"markdown","e41b0e8e":"markdown","81789cb2":"markdown","b4ddf56a":"markdown","9c38f1f4":"markdown","8784c5e5":"markdown","64002673":"markdown","14e9f172":"markdown","22d0831b":"markdown","2fd5497a":"markdown","36e1a4bf":"markdown","d0027999":"markdown","f7dc1832":"markdown","3e595f43":"markdown","693938df":"markdown","4a8e4206":"markdown","39fe7ab5":"markdown","ae7c4d94":"markdown"},"source":{"932c2952":"# Run this cell and select the kaggle.json file downloaded\n# from the Kaggle account settings page.\nfrom google.colab import files\nfiles.upload()","0531f671":"# Let's make sure the kaggle.json file is present.\n!ls -lha kaggle.json","83f535f5":"# Next, install the Kaggle API client.\n!pip install -q kaggle","431fec2b":"def submit_kaggle(submission_path, competition, message=\"submission\"):\n    !kaggle competitions submit -c {competition} -f {submission_path} -m \"{message}\"","0c1881f0":"# The Kaggle API client expects this file to be in ~\/.kaggle,\n# so move it there.\n!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\/\n\n# This permissions change avoids a warning on Kaggle tool startup.\n!chmod 600 ~\/.kaggle\/kaggle.json","56cccf95":"# List available datasets.\n!kaggle datasets list","1e23a3b9":"# Copy the stackoverflow data set locally.\n#!kaggle datasets download -d stackoverflow\/stack-overflow-2018-developer-survey\n!kaggle competitions download -c ashrae-energy-prediction -p \/content\/ashrae-energy-prediction","c04ec2ac":"ls \/content\/ashrae-energy-prediction","8cc1d47d":"DATA = \"\/content\/ashrae-energy-prediction\/\"\n","fc9a4e6d":"#!cd \/content\/ashrae-energy-prediction\n#!unzip *.zip","f44dd433":"!unzip \/content\/ashrae-energy-prediction\/sample_submission.csv.zip -d {DATA}\n!unzip \/content\/ashrae-energy-prediction\/test.csv.zip -d {DATA}\n!unzip \/content\/ashrae-energy-prediction\/train.csv.zip -d {DATA}\n!unzip \/content\/ashrae-energy-prediction\/weather_test.csv.zip -d {DATA}\n!unzip \/content\/ashrae-energy-prediction\/weather_train.csv.zip -d {DATA}","37fc6326":"ls \/content\/ashrae-energy-prediction","7ef7006e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc, math\n\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nfrom datetime import datetime","78e37318":"sns.set(rc={'figure.figsize':(11,8)})\nsns.set(style=\"whitegrid\")","0b8c8e69":"#DATA = \"\/content\/\"\n","b6a0786a":"ls","34f79b45":"%%time\nmetadata_df = pd.read_csv(f\"{DATA}building_metadata.csv\")\ntrain_df = pd.read_csv(f\"{DATA}train.csv\", parse_dates=['timestamp'])\ntest_df = pd.read_csv(f'{DATA}test.csv', parse_dates=['timestamp'])\nweather_train_df = pd.read_csv(f'{DATA}weather_train.csv', parse_dates=['timestamp'])\nweather_test_df = pd.read_csv(f'{DATA}weather_test.csv', parse_dates=['timestamp'])","3393e6e5":"pd.Series(np.log1p(train_df['meter_reading'])).hist()","ea06abde":"train_df.head()","c52f2d20":"metadata_df.head()","11538b74":"weather_train_df.shape","81838358":"weather_train_df.head()","851d81ae":"test_df.head()","e6eee648":"weather = pd.concat([weather_train_df,weather_test_df],ignore_index=True)\nweather_key = ['site_id', 'timestamp']\n\ntemp_skeleton = weather[weather_key + ['air_temperature']].drop_duplicates(subset=weather_key).sort_values(by=weather_key).copy()\n\n# calculate ranks of hourly temperatures within date\/site_id chunks\ntemp_skeleton['temp_rank'] = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.date])['air_temperature'].rank('average')\n\n# create a dataframe of site_ids (0-16) x mean hour rank of temperature within day (0-23)\ndf_2d = temp_skeleton.groupby(['site_id', temp_skeleton.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)\n\n# Subtract the columnID of temperature peak by 14, getting the timestamp alignment gap.\nsite_ids_offsets = pd.Series(df_2d.values.argmax(axis=1) - 14)\nsite_ids_offsets.index.name = 'site_id'\n\ndef timestamp_align(df):\n    df['offset'] = df.site_id.map(site_ids_offsets)\n    df['timestamp_aligned'] = (df.timestamp - pd.to_timedelta(df.offset, unit='H'))\n    df['timestamp'] = df['timestamp_aligned']\n    del df['timestamp_aligned']\n    return df","2b28003f":"weather_train_df.tail()","49d39acb":"weather_train_df = timestamp_align(weather_train_df)\nweather_test_df = timestamp_align(weather_test_df)","e2534d45":"del weather \ndel df_2d\ndel temp_skeleton\ndel site_ids_offsets","04fdf7a6":"weather_train_df.tail()","35bd3c11":"def add_lag_feature(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_std = rolled.std().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]","e947a720":"add_lag_feature(weather_train_df, window=72)\nadd_lag_feature(weather_test_df, window=72)","3c633a3a":"weather_train_df.columns","c9812a38":"weather_train_df.isna().sum()","1e63ba1c":"weather_test_df.isna().sum()","c63e7c26":"weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))","6ef418d2":"weather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))","885580bf":"weather_train_df.isna().sum()","39b9be41":"weather_test_df.isna().sum()","490bc97a":"# Since loss metric is RMSLE\ntrain_df['meter_reading'] = np.log1p(train_df['meter_reading'])","ea0c5731":"weather_train_df.head()","2e5c3d79":"## Function to reduce the memory usage\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","d79bd1e2":"le = LabelEncoder()\nmetadata_df['primary_use'] = le.fit_transform(metadata_df['primary_use'])","2a1b4121":"metadata_df = reduce_mem_usage(metadata_df)\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)\nweather_train_df = reduce_mem_usage(weather_train_df)\nweather_test_df = reduce_mem_usage(weather_test_df)","83f998bc":"print (f'Training data shape: {train_df.shape}')\nprint (f'Weather training shape: {weather_train_df.shape}')\nprint (f'Weather training shape: {weather_test_df.shape}')\nprint (f'Weather testing shape: {metadata_df.shape}')\nprint (f'Test data shape: {test_df.shape}')","85e7a115":"train_df.head()","a1581780":"weather_train_df.head()","0e56fbe6":"metadata_df.head()","fa644255":"test_df.head()","968b1a01":"train_df.head()","aaaf5264":"%%time\nfull_train_df = train_df.merge(metadata_df, on='building_id', how='left')\nfull_train_df = full_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')","9d6fe3a6":"full_train_df = full_train_df.loc[~(full_train_df['air_temperature'].isnull() & full_train_df['cloud_coverage'].isnull() & full_train_df['dew_temperature'].isnull() & full_train_df['precip_depth_1_hr'].isnull() & full_train_df['sea_level_pressure'].isnull() & full_train_df['wind_direction'].isnull() & full_train_df['wind_speed'].isnull() & full_train_df['offset'].isnull())]\n#full_train_df.loc[(full_train_df['air_temperature'].isnull() & full_train_df['cloud_coverage'].isnull() & full_train_df['dew_temperature'].isnull() & full_train_df['precip_depth_1_hr'].isnull() & full_train_df['sea_level_pressure'].isnull() & full_train_df['wind_direction'].isnull() & full_train_df['wind_speed'].isnull() & full_train_df['offset'].isnull())] = -1","bbc58f09":"full_train_df.shape","129b5a97":"# Delete unnecessary dataframes to decrease memory usage\ndel train_df\ndel weather_train_df\ngc.collect()","7606c7ce":"%%time\nfull_test_df = test_df.merge(metadata_df, on='building_id', how='left')\nfull_test_df = full_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')","041afec6":"full_test_df.shape","f0847f00":"# Delete unnecessary dataframes to decrease memory usage\ndel metadata_df\ndel weather_test_df\ndel test_df\ngc.collect()","75109b5c":"ax = sns.barplot(pd.unique(full_train_df['primary_use']), full_train_df['primary_use'].value_counts())\nax.set(xlabel='Primary Usage', ylabel='# of records', title='Primary Usage vs. # of records')\nax.set_xticklabels(ax.get_xticklabels(), rotation=50, ha=\"right\")\nplt.show()","4d52df99":"meter_types = {0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}\nax = sns.barplot(np.vectorize(meter_types.get)(pd.unique(full_train_df['meter'])), full_train_df['meter'].value_counts())\nax.set(xlabel='Meter Type', ylabel='# of records', title='Meter type vs. # of records')\nplt.show()","f20bff28":"# Average meter reading\nprint (f'Average meter reading: {full_train_df.meter_reading.mean()} kWh')","19cad298":"ax = sns.barplot(np.vectorize(meter_types.get)(full_train_df.groupby(['meter'])['meter_reading'].mean().keys()), full_train_df.groupby(['meter'])['meter_reading'].mean())\nax.set(xlabel='Meter Type', ylabel='Meter reading', title='Meter type vs. Meter Reading')\nplt.show()","6978b92f":"fig, ax = plt.subplots(1,1,figsize=(14, 6))\nax.set(xlabel='Year Built', ylabel='# Of Buildings', title='Buildings built in each year')\nfull_train_df['year_built'].value_counts(dropna=False).sort_index().plot(ax=ax)\nfull_test_df['year_built'].value_counts(dropna=False).sort_index().plot(ax=ax)\nax.legend(['Train', 'Test']);","ac3b3e45":"fig, ax = plt.subplots(1,1,figsize=(15, 7))\nfull_train_df.groupby(['building_id'])['square_feet'].mean().plot(ax=ax)\nax.set(xlabel='Building ID', ylabel='Area in Square Feet', title='Square Feet area of buildings')\nplt.show()","5c4a42ae":"pd.DataFrame(full_train_df.isna().sum().sort_values(ascending=False), columns=['NaN Count'])","9a0c9e4c":"def mean_without_overflow_fast(col):\n    col \/= len(col)\n    return col.mean() * len(col)","396af7c3":"missing_values = (100-full_train_df.count() \/ len(full_train_df) * 100).sort_values(ascending=False)","9e025eb5":"%%time\nmissing_features = full_train_df.loc[:, missing_values > 0.0]\nmissing_features = missing_features.apply(mean_without_overflow_fast)","719b9008":"# Both train and test are interpolated with mean of train\nfor key in full_train_df.loc[:, missing_values > 0.0].keys():\n    if key == 'year_built' or key == 'floor_count':\n        full_train_df[key].fillna(math.floor(missing_features[key]), inplace=True)\n        full_test_df[key].fillna(math.floor(missing_features[key]), inplace=True)\n    else:\n        full_train_df[key].fillna(missing_features[key], inplace=True)\n        full_test_df[key].fillna(missing_features[key], inplace=True)","49812c43":"full_train_df.tail()","2ebf070f":"full_test_df.tail()","1b8d705c":"full_train_df.isna().sum().sum(), full_test_df.isna().sum().sum()","76a75210":"full_train_df['timestamp'].dtype","95d5ae3a":"full_train_df[\"timestamp\"] = pd.to_datetime(full_train_df[\"timestamp\"])\nfull_test_df[\"timestamp\"] = pd.to_datetime(full_test_df[\"timestamp\"])","4d6b810f":"def transform(df):\n    df['hour'] = np.uint8(df['timestamp'].dt.hour)\n    df['day'] = np.uint8(df['timestamp'].dt.day)\n    df['weekday'] = np.uint8(df['timestamp'].dt.weekday)\n    df['month'] = np.uint8(df['timestamp'].dt.month)\n    df['year'] = np.uint8(df['timestamp'].dt.year-1900)\n    \n    df['square_feet'] = np.log(df['square_feet'])\n    \n    return df","9a204520":"full_train_df = transform(full_train_df)\nfull_test_df = transform(full_test_df)","b6f0c2b7":"dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')\nus_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())\nfull_train_df['is_holiday'] = (full_train_df['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)\nfull_test_df['is_holiday'] = (full_test_df['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)","c9fed149":"# Assuming 5 days a week for all the given buildings\nfull_train_df.loc[(full_train_df['weekday'] == 5) | (full_train_df['weekday'] == 6) , 'is_holiday'] = 1\nfull_test_df.loc[(full_test_df['weekday']) == 5 | (full_test_df['weekday'] == 6) , 'is_holiday'] = 1","8a623a4c":"full_train_df.shape","3e30c106":"full_train_df = full_train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","ded129c2":"full_train_df.shape","4a907456":"full_test_df = full_test_df.drop(['timestamp'], axis=1)\nfull_train_df = full_train_df.drop(['timestamp'], axis=1)\nprint (f'Shape of training dataset: {full_train_df.shape}')\nprint (f'Shape of testing dataset: {full_test_df.shape}')","fc8fdcca":"full_train_df.tail()","59c30553":"## Reducing memory\nfull_train_df = reduce_mem_usage(full_train_df)\nfull_test_df = reduce_mem_usage(full_test_df)\ngc.collect()","31d81d14":"# def degToCompass(num):\n#     val=int((num\/22.5)+.5)\n#     arr=[i for i in range(0,16)]\n#     return arr[(val % 16)]","c4c76b25":"# full_train_df['wind_direction'] = full_train_df['wind_direction'].apply(degToCompass)","ec62d19b":"# beaufort = [(0, 0, 0.3), (1, 0.3, 1.6), (2, 1.6, 3.4), (3, 3.4, 5.5), (4, 5.5, 8), (5, 8, 10.8), (6, 10.8, 13.9), \n#           (7, 13.9, 17.2), (8, 17.2, 20.8), (9, 20.8, 24.5), (10, 24.5, 28.5), (11, 28.5, 33), (12, 33, 200)]\n\n# for item in beaufort:\n#     full_train_df.loc[(full_train_df['wind_speed']>=item[1]) & (full_train_df['wind_speed']<item[2]), 'beaufort_scale'] = item[0]","abf1d4a0":"# le = LabelEncoder()\n# full_train_df['primary_use'] = le.fit_transform(full_train_df['primary_use'])\n\ncategoricals = ['site_id', 'building_id', 'primary_use', 'hour', 'weekday', 'meter',  'wind_direction', 'is_holiday']\n# drop_cols = ['sea_level_pressure', 'wind_speed']\nnumericals = ['square_feet', 'year_built', 'air_temperature', 'cloud_coverage',\n              'dew_temperature', 'precip_depth_1_hr', 'floor_count', 'air_temperature_mean_lag72',\n       'cloud_coverage_mean_lag72', 'dew_temperature_mean_lag72',\n       'precip_depth_1_hr_mean_lag72']\n\nfeat_cols = categoricals + numericals","636eb88d":"full_train_df[numericals].describe()","14c30189":"full_train_df.tail()","8494a6fd":"full_train_df = reduce_mem_usage(full_train_df)\ngc.collect()","0e10ac20":"cat_card = full_train_df[categoricals].nunique().apply(lambda x: min(x, 50)).to_dict()\ncat_card","855edd41":"target = full_train_df[\"meter_reading\"]\nfull_train_df.to_pickle('full_train_df.pkl')\n#del full_train_df[\"meter_reading\"]","d0ff4106":"# full_train_df.drop(drop_cols, axis=1)\n# gc.collect()","ef5acb65":"# Save the testing dataset to freeup the RAM. We'll read after training\nfull_test_df.to_pickle('full_test_df.pkl')\ndel full_test_df\ngc.collect()","db64c7eb":"import pickle","41d2fa7c":"with open('full_train_df.pkl', 'rb') as f:\n    full_train_df = pickle.load(f)","6d97703a":"full_train_df.columns","6e525a98":"from fastai.tabular import *","7e809bd2":"from fastai.basic_train import *","516e3b63":"ls","98e2831e":"#learner = learn.load(f'model1.bin')","070cb69f":"full_train_df_sample = full_train_df.sample(frac=0.1)","40f4b5ed":"full_train_df_sample.shape","e1d35f33":"procs = [FillMissing, Categorify] # Took out Normalize to address NaN prob when getting mean","a661f18f":"valid_idx = range(len(full_train_df_sample)- int(full_train_df_sample.shape[0] * 0.1), len(full_train_df_sample))","1dfa2add":"#folds = 2\n#seed = 666\n\n#kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)","188d73cc":"#index_splits = next(kf.split(full_train_df, full_train_df['building_id']))","72523396":"dep_var = \"meter_reading\"\ncat_names = categoricals\npath = DATA","aae11925":"#full_train_df[dep_var] = target","5ad16899":"#del target","804f30e3":"full_train_df_sample[dep_var]","5c138833":"# Not including test set first since takes too much space\ndata = TabularDataBunch.from_df(path, full_train_df_sample, dep_var, valid_idx=valid_idx, procs=procs, cat_names=cat_names) #, test_df=full_test_df)\nprint(data.train_ds.cont_names)  # `cont_names` defaults to: set(df)-set(cat_names)-{dep_var}","0fdb1ecc":"(cat_x,cont_x),y = next(iter(data.train_dl))\nfor o in (cat_x, cont_x, y): print(to_np(o[:5]))","087d4658":"learn = tabular_learner(data, layers=[200,100], emb_szs=cat_card, metrics=rmse)\n","7dd896db":"learn.fit_one_cycle(1, 1e-2)","85a29916":"pred_batch1 = learn.pred_batch()\npred_batch1[: 5]","d2175ff5":"learn.save('model_sample')","d46f6fa1":"learn.export('model_sample_export')","cbba4535":"# Checking if predictions will be similar after loading\nlearn1 = tabular_learner(data, layers=[200,100], emb_szs=cat_card, metrics=rmse)","8ff55fad":"learn1.load('model_sample')","7f307611":"ls \/content\/ashrae-energy-prediction\/","f1a8bbeb":"pred_batch2 = learn1.pred_batch()\npred_batch2[: 5]","c653c2d5":"learn2.data.train_ds","1337d16c":"tabList_sample = TabularList.from_df(full_train_df.head(10), cat_names=cat_names, procs=procs)\nlearn2 = load_learner(DATA, 'model_sample_export', test=tabList_sample)","206098b3":"pred_batch3 = learn2.pred_batch(ds_type=DatasetType.Test)\npred_batch3[: 5]","bd89022d":"full_test_df = pd.read_pickle('full_test_df.pkl')","69dce530":"nan_cols = full_test_df.columns.values[full_test_df.isna().sum().nonzero()[0]].tolist()","6c2a308b":"for col_ in nan_cols:\n    median = full_test_df[col_].median()\n    full_test_df[col_] = full_test_df[col_].fillna(median)\n","b2eaf14d":"# Need to input data as tabList when learner is loaded\n# Note that the learner is loaded without the datasets\ntabList = TabularList.from_df(full_test_df, cat_names=cat_names, procs=procs)\nlearn2 = load_learner(DATA, 'model_sample_export', test=tabList)","797927d5":"del tabList\ndel full_test_df","a7ee3b04":"pred_test = learn2.get_preds(ds_type=DatasetType.Test)","5b6b9164":"len(pred_test[0]), full_test_df.shape","98a74b96":"pred_test_np = pred_test[0].numpy()","f42fcfee":"from google.colab import drive\ndrive.mount('\/content\/gdrive')","edef9a25":"res = pred_test_np","dd1a56ac":"submission = pd.read_csv(f'{DATA}sample_submission.csv')\nsubmission.shape","ccad8c6a":"submission = pd.read_csv(f'{DATA}sample_submission.csv')\n# Remember, we predicted the log consumption, so we have to get the exponential to convert it back!\nsubmission['meter_reading'] = np.expm1(res)\nsubmission.loc[submission['meter_reading']<0, 'meter_reading'] = 0\nsubmission.to_csv('submission_fastai_20191103.csv', index=False)\nsubmission","f2988fb0":"submission.head()","18d704ea":"!cp submission_fastai_20191103.csv \/content\/gdrive\/'My Drive'\/ashrae\/submissions\/submission_fastai_20191103.csv","88e558c4":"ls \/content\/gdrive\/'My Drive'\/ashrae\/submissions\/","31781f04":"!kaggle competitions submit -c ashrae-energy-prediction -f submission_fastai_20191103.csv -m \"Hello fastai tabular FCNN (corrected w\/ exponential)\"","d42c4bda":"del pred_test\ndel learn2\ndel learn","c7f26099":"from fastai.tabular import *","63f8b60f":"from fastai.basic_train import *","284f8aa0":"ls","2a073f7f":"#learner = learn.load(f'model1.bin')","13522671":"full_train_df_sample = full_train_df.sample(frac=1)","862ec22f":"full_train_df_sample.shape","f76e068c":"del full_train_df","cbfcab76":"procs = [FillMissing, Categorify] # Took out Normalize to address NaN prob when getting mean","2a252512":"#valid_idx = range(len(full_train_df_sample)- int(full_train_df_sample.shape[0] * 0.1), len(full_train_df_sample))\nvalid_idx = np.random.permutation(len(full_train_df_sample))[: int(full_train_df_sample.shape[0] * 0.05)]","6a383215":"valid_idx","5764caf4":"#folds = 2\n#seed = 666\n\n#kf = StratifiedKFold(n_splits=folds, shuffle=False, random_state=seed)","08810244":"#index_splits = next(kf.split(full_train_df, full_train_df['building_id']))","b612445d":"dep_var = \"meter_reading\"\ncat_names = categoricals\npath = DATA","316829f2":"#full_train_df[dep_var] = target","4054e2fc":"#del target","3603a734":"full_train_df_sample[dep_var]","98e726cb":"# Not including test set first since takes too much space\ndata = TabularDataBunch.from_df(path, full_train_df_sample, dep_var, valid_idx=valid_idx, procs=procs, cat_names=cat_names, bs=128) #, test_df=full_test_df)\nprint(data.train_ds.cont_names)  # `cont_names` defaults to: set(df)-set(cat_names)-{dep_var}","b32cd2aa":"del full_train_df_sample","0fbfce50":"(cat_x,cont_x),y = next(iter(data.train_dl))\nfor o in (cat_x, cont_x, y): print(to_np(o[:5]))","a3de8eb5":"learn = tabular_learner(data, layers=[200,100], emb_szs=cat_card, metrics=rmse)\n","46fd0eb8":"learn.fit_one_cycle(1, 1e-2)","54c832be":"pred_batch1 = learn.pred_batch()\npred_batch1[: 5]","545c52fb":"learn.export('model_all_export')","94a41cd6":"# Checking if predictions will be similar after loading\n#learn1 = tabular_learner(data, layers=[200,100], emb_szs=cat_card, metrics=rmse)","caf833d9":"#learn1.load('model_sample')","d6543538":"#ls \/content\/ashrae-energy-prediction\/","7123db57":"#pred_batch2 = learn1.pred_batch()\n#pred_batch2[: 5]","4e5b767a":"#learn2.data.train_ds","c869d1af":"#tabList_sample = TabularList.from_df(full_train_df.head(10), cat_names=cat_names, procs=procs)\n#learn2 = load_learner(DATA, 'model_sample_export', test=tabList_sample)","a2c64a2b":"#pred_batch3 = learn2.pred_batch(ds_type=DatasetType.Test)\n#pred_batch3[: 5]","1c31bf91":"full_test_df = pd.read_pickle('full_test_df.pkl')","a8762969":"nan_cols = full_test_df.columns.values[full_test_df.isna().sum().nonzero()[0]].tolist()","2e7d0c74":"for col_ in nan_cols:\n    median = full_test_df[col_].median()\n    full_test_df[col_] = full_test_df[col_].fillna(median)\n","faf80524":"## Need to input data as tabList when learner is loaded\n## Note that the learner is loaded without the datasets\ntabList = TabularList.from_df(full_test_df, cat_names=cat_names, procs=procs)\nlearn2 = load_learner(DATA, 'model_all_export', test=tabList)","36978614":"del tabList\ndel full_test_df","e84ff0fa":"pred_test = learn2.get_preds(ds_type=DatasetType.Test)","2e203d3d":"len(pred_test[0])#, full_test_df.shape","71703cd6":"pred_test_np = pred_test[0].numpy()","a4d8bd55":"from google.colab import drive\ndrive.mount('\/content\/gdrive')","49f622e1":"res = pred_test_np","f39d5d47":"submission = pd.read_csv(f'{DATA}sample_submission.csv')\nsubmission.shape","e124df14":"submission = pd.read_csv(f'{DATA}sample_submission.csv')\nsubmission['meter_reading'] = np.expm1(res)\nsubmission.loc[submission['meter_reading'] < 0, 'meter_reading'] = 0\nsubmission.to_csv('submission_fastai_full_20191103.csv', index=False)\nsubmission","df17e618":"### Distribution of meter readings for each meter type","932f9b8b":"## Feature Engineering\nThe joined dataframe (full_train_df) now has 20,216,100 rows, and 16 features in training dataset.","8782d965":"### Fill NaNs in weather data by interpolation","b4354563":"Now let's change the data types of necessary feature columns based on the range of the data values. This will lower the data usage. But **how**? Let's see. For example the datatype of feature `building_id` is `int64` but based on the range of this feature, it can be accomodated in lower range i.e. `int16`. So this will decrease the memory usage.","04b082b5":"## Downloading from Kaggle with CLI","9aa7147e":"#### Reducing the memory usage\nLet's delete unnecessary dataframes from memory to lower the memory usage","5e57bee0":"#### Reducing the memory usage\nLet's delete unnecessary dataframes from memory to lower the memory usage","7fe87a64":"So all the missing values for training and testing data is now filled with the mean of corresponding feature columns.","2551d7ab":"# fastai tabular (sample)","d976fe2a":"### Adding few more features","6ec972f3":"### Distribution of meter types","78f433f4":"### Average meter reading for training dataset","dbf31917":"Now let's change the data types of necessary feature columns based on the range of the data values. This will lower the data usage. But **how**? Let's see. For example the datatype of feature `building_id` is `int64` but based on the range of this feature, it can be accomodated in lower range i.e. `int16`. So this will decrease the memory usage.","e41b0e8e":"### Distribution of square feet area of buildings","81789cb2":"First let's expand timestamp to multiple components","b4ddf56a":"Missing data can be filled in many ways. Here are few techniques to fill missing values: \n\n* Ignore the data row\n* Back-fill or forward-fill to propagate next or previous values respectively\n* Replace with some constant value outside fixed value range-999,-1 etc.\n* Replace with mean, median value\n\nFor now, we will go with last method. So let's fill all the missing data with it's average(mean) values of corresponding columns.","9c38f1f4":"Drop all NaN rows which are generated by timestamp alignment","8784c5e5":"## Exploratory Data Analysis\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data.","64002673":"##### Set the size and styles of graphs","14e9f172":"### Analysing missing data\nFirst let's count and fill missing data in training datasets","22d0831b":"# fastai tabular (all)","2fd5497a":"#### Get insights of shapes and first few data rows of all the files","36e1a4bf":"### Distribition of primary usage of buildings","d0027999":"#### Merge necessary files\nAs mentioned previously, to get a single dataframe for training and a single data frame for testing with all the feature included, we need to join the tables\/files which are related by foreign keys. Let's first merge\/join training data.","f7dc1832":"### Removing weired data on site_id 0\nAs you can see above, this data looks weired until May 20. It is reported in this discussion by @barnwellguy that All electricity meter is 0 until May 20 for site_id == 0. Let's remove these data from training data.","3e595f43":"# This kernel shows how to make a baseline submission using the fastai library using Google Colab\n# You can find this on google colab on this link\nhttps:\/\/colab.research.google.com\/drive\/1Y08AaPFCIwwHEAvaazN6JtNHsUWd8Ugq\n\n## NOTE: This solution yielded me a score of 1.22 on the public leaderboard","693938df":"### Distribution of buildings built in each year for both training and test datasets","4a8e4206":"### Read the dataset\nData is given in different CSV files which we will need to merge afterwards. \n\n`train.csv` only contains the ID of the building and meter related information including our target variable to be predicted (`meter_reading`). This `building_id` is foreign key in `building_metadata.csv`. All the information related to the buildings are given in this file. \n\n\nSame goes for `weather_train.csv` and `building_metadata.csv` files with common column (foreign key) `site_id`. So all three files are related and we will have to join these tables later","39fe7ab5":"#### Let's do the same for test data","ae7c4d94":"### Align timestamps\nTimestap data is not in their local time. As energy consumptions are related to the local time, an alighment is nescessary before using timestamp. \n\nThe credit goes to [this kernel](https:\/\/www.kaggle.com\/nz0722\/aligned-timestamp-lgbm-by-meter-type) for the idea. Refer it for more details and explanation about below code."}}