{"cell_type":{"585fcded":"code","dfbc9905":"code","3052a3e0":"code","03188969":"code","333301c5":"code","bb523545":"code","68d9b1ac":"code","57168fcf":"code","186c8a4b":"code","e358ba93":"code","193b4ccf":"code","667cd664":"code","efc403d9":"code","dfc49327":"code","b165fc74":"code","feea4fd5":"code","4e3d54f0":"code","061d6874":"code","d386619f":"code","9c4c5123":"code","08444e1d":"code","6f0273ae":"code","b4c09019":"code","1c08a6eb":"code","dfe50f9c":"code","c9d17db1":"code","8792f7f3":"code","cf5d83c4":"code","69d16e6d":"code","19722517":"code","e11dbb20":"code","0f0ceeed":"code","e7043cbd":"code","170f1e9d":"code","dda06c0c":"code","94c4572a":"code","cd21a4cc":"code","3a9c6ca8":"code","13be308a":"code","994e7e40":"code","60c5cd47":"code","fa7cdab2":"code","3cc2a75b":"code","2996d078":"code","c83cdc1f":"code","ab2c4014":"code","9c20ec4e":"code","2f99b5e4":"code","37d4fcee":"code","25a1b019":"code","50a02487":"code","620960de":"code","01c1f979":"code","bd9d6cdb":"code","fa092d5e":"code","de6fa4a7":"code","5c545ad2":"code","5881623f":"code","a309de0f":"code","91c30948":"code","28bbce29":"markdown","95cc461c":"markdown","94859cc3":"markdown","023f7000":"markdown","04c31026":"markdown","d85a5609":"markdown","db220797":"markdown","7e995db6":"markdown","df009983":"markdown","13b1ab41":"markdown","227d03d5":"markdown","785e816b":"markdown","6f759081":"markdown","28919c9f":"markdown"},"source":{"585fcded":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dfbc9905":"#\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Test.csv\n#\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Sample Submission.csv\n#\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Train.csv","3052a3e0":"df=pd.read_csv(\"\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Train.csv\")","03188969":"df1=pd.read_csv(\"\/kaggle\/input\/the-great-indian-hiring-hackathon\/Participants_Data_TGIH\/Test.csv\")","333301c5":"df1.info()","bb523545":"df.drop_duplicates(inplace=True)","68d9b1ac":"df['first_2_CID']=df['CustomerID'].apply(lambda x: int(str(x)[:2]))   \n# extract the first two number of the customerid\ndf1['first_2_CID']=df1['CustomerID'].apply(lambda x: int(str(x)[:2])) \n#feature extrzction","57168fcf":"df.head() #there are no null values","186c8a4b":"df.info()","e358ba93":"df[\"InvoiceDate\"]=pd.to_datetime(df[\"InvoiceDate\"])\ndf1[\"InvoiceDate\"]=pd.to_datetime(df1[\"InvoiceDate\"]) #change to date type","193b4ccf":"df[\"InvoiceDateO\"]=df[\"InvoiceDate\"].apply(lambda x: x.toordinal())\ndf1[\"InvoiceDateO\"]=df1[\"InvoiceDate\"].apply(lambda x: x.toordinal()) #change date to ordinal data","667cd664":"df.InvoiceNo.nunique()","efc403d9":"#df=df.sort_values(\"StockCode\")","dfc49327":"#df[df[\"InvoiceDate\"].dt.month==12]","b165fc74":"df.info() #all data type is now correct","feea4fd5":"#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)","4e3d54f0":"#df1[df1['Country']==35]","061d6874":"for i in df[['InvoiceNo', 'StockCode', 'Description', 'Quantity',\n       'UnitPrice', 'CustomerID', 'Country','InvoiceDateO']].columns:\n    sns.boxplot(df[i])\n    plt.show() #check for outliers \n#looks like quantity and unitprice has some outliers","d386619f":"df[df[\"UnitPrice\"]>5000]","9c4c5123":"#df[df['InvoiceDateO']==734298]","08444e1d":"#df.drop(labels=[239556],inplace=True)","6f0273ae":"df.drop(df[df[\"UnitPrice\"]>10000].index,inplace=True) \n#df.reset_index(drop=True,inplace=True)","b4c09019":"#df[df[\"UnitPrice\"]==0]","1c08a6eb":"#df1[df1['Quantity']<0] \n#quantity cannot be negative it could be a grabage or the quantity is by mistake negative","dfe50f9c":"df['Quantity']=df['Quantity'].abs()\ndf1['Quantity']=df1['Quantity'].abs()\n#quantity has negative which is invalid and also they are not garbage value \n#converting them to positive","c9d17db1":"#df.drop(df[df[\"Quantity\"]>10000].index,inplace=True) #10k","8792f7f3":"df.info()","cf5d83c4":"df1.describe()","69d16e6d":"df1.info()","19722517":"df.corr() ","e11dbb20":"from scipy.stats import spearmanr, pearsonr\nspearmanr(df['UnitPrice'], df['Quantity']) ","0f0ceeed":"plt.figure(figsize=(17,7))\nsns.scatterplot(x=\"Country\",y=\"UnitPrice\",data=df)  ","e7043cbd":"plt.figure(figsize=(17,7))\nsns.scatterplot(x=\"CustomerID\",y=\"UnitPrice\",data=df) ","170f1e9d":"plt.figure(figsize=(17,7))\nsns.scatterplot(x=\"StockCode\",y=\"UnitPrice\",data=df)","dda06c0c":"df1.columns","94c4572a":"X_train=df[['InvoiceDateO','Quantity','StockCode', 'Description','Country','first_2_CID']]\ny_train=df1[['InvoiceDateO','Quantity','StockCode', 'Description',  'Country', 'first_2_CID']]\nX_test=df['UnitPrice'].values","cd21a4cc":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nX_train = sc.fit_transform(X_train)\ny_train = sc.fit_transform(y_train)\n#X_test = sc.transform(X_test)","3a9c6ca8":"from sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.model_selection import cross_val_score\nextra1=ExtraTreesRegressor(max_depth=200,\n  n_estimators=200)\nextra1.fit(X_train,X_test)\nnp.mean(cross_val_score(extra1,X_train,X_test,scoring = 'neg_mean_squared_error', cv= 3))#36.9 #7.5569158 #.3212 #7.55609","13be308a":"results = extra1.predict(y_train)","994e7e40":"df1['UnitPrice']=results","60c5cd47":"df1['UnitPrice'].to_csv(\"Submission_Final.csv\",header=True,index=False)","fa7cdab2":"#from tpot import TPOTRegressor\n\n\n#tpot_classifier = TPOTRegressor(generations= 5, population_size= 24, offspring_size= 12,\n#                                 verbosity= 2, early_stop= 12,\n#                                 config_dict={'sklearn.ensemble.ExtraTreesRegressor': param}, \n#                                 cv = 2)\n#tpot_classifier.fit(X_train,X_test)","3cc2a75b":"#results=tpot_classifier.predict(y_train)","2996d078":"#tpot_classifier.fitted_pipeline_","c83cdc1f":"#tpot.score(X_train, X_test)","ab2c4014":"#tpot.export(path_to_pipeline)\n#pickle.dump(tpot.fitted_pipeline_, open(file_name, \u2018wb\u2019))\n#import pickle\n#tpot.score(X_train, X_test)\n#pickle.dump(tpot.fitted_pipeline_, open(\"tpot.pkl\", \"wb\"))\n#tpot.export(\"tpot.py\")\n#tpot.export()","9c20ec4e":"from sklearn.neighbors import KNeighborsRegressor\nclassifier = KNeighborsRegressor(n_neighbors = 1, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, X_test)\ny_pred=classifier.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))#36.9","2f99b5e4":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import cross_val_score\nmodel = CatBoostRegressor(\nmax_depth=10,\n        n_estimators=100,\n        learning_rate=1)\nmodel.fit( X_train,X_test, use_best_model=True, silent=True )\ny_pred=model.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))#36.9","37d4fcee":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nRegressor = DecisionTreeRegressor(criterion='mse')\nRegressor.fit(X_train, X_test)\nnp.mean(cross_val_score(Regressor,X_train,X_test, scoring = 'neg_mean_squared_error', cv= 3)) #145.88","25a1b019":"y_pred=Regressor.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nmean_squared_error(X_test,y_pred)","50a02487":"from sklearn.ensemble import RandomForestRegressor\nrf =  RandomForestRegressor(300)\nrf.fit(X_train, X_test)\n#np.mean(cross_val_score(rf,X_train,X_test,scoring = 'neg_mean_squared_error', cv= 3)) #124\ny_pred=rf.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))#36.9","620960de":"from sklearn.linear_model import LinearRegression\nclf = LinearRegression().fit(X_train, X_test)\ny_pred=rf.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))#36.9","01c1f979":"from sklearn.linear_model import LinearRegression, Lasso, Ridge\nalpha = []\nerror = []\n\nfor i in range(1,1000):\n    alpha.append(i\/1)\n    lml = Lasso(alpha=(i\/1))\n    error.append(np.mean(cross_val_score(lml,X_train,X_test, scoring = 'neg_mean_absolute_error', cv= 3)))\n    \nplt.plot(alpha,error)","bd9d6cdb":"err = tuple(zip(alpha,error))\ndf_err = pd.DataFrame(err, columns = ['alpha','error'])\ndf_err[df_err.error == max(df_err.error)]","fa092d5e":"lm_l = Lasso(alpha=581.0)\nlm_l.fit(X_train,X_test)\ny_pred=lm_l.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))","de6fa4a7":"from xgboost import XGBRegressor","5c545ad2":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nmodel = XGBRegressor(  \n        max_depth=100,\n        n_estimators=200,\n        learning_rate=1.5,bootstrap=False\n        )\nmodel.fit(X_train, X_test)\n#np.mean(cross_val_score(model,X_train,X_test,scoring = 'neg_mean_squared_error', cv= 3)) #103\n#max_depth=6, min_child_weight=3, nthread=1,objective='reg:squarederror', subsample=0.8\ny_pred=model.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred)) #6.4 #5.769 #33.5295 #57.106 #7.556915879158611","5881623f":"from sklearn.ensemble import AdaBoostRegressor\nclassifier = AdaBoostRegressor( \n        n_estimators=100,\n        learning_rate=1)\nclassifier.fit(X_train, X_test)\n#np.mean(cross_val_score(classifier,X_train,X_test,scoring = 'neg_mean_squared_error', cv= 3))\ny_pred=classifier.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))","a309de0f":"from sklearn.ensemble import GradientBoostingRegressor\ngb_clf = GradientBoostingRegressor(\n        max_depth=20,\n        n_estimators=100,\n        learning_rate=1)\ngb_clf.fit(X_train, X_test)\n#np.mean(cross_val_score(gb_clf,X_train,X_test,scoring = 'neg_mean_squared_error', cv= 3))\ny_pred=gb_clf.predict(X_train)\nfrom sklearn.metrics import mean_squared_error\nnp.sqrt(mean_squared_error(X_test,y_pred))","91c30948":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n#d_train = lgb.Dataset(X_train, label=X_test)\n#X_train, X_test, y_train, y_test = train_test_split( X_train, X_test, test_size=0.2, random_state=42)\nhyper_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': ['l2', 'auc'],\n    'learning_rate': 0.5,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 10,\n    'verbose': 0,\n    \"max_depth\": 8,\n    \"num_leaves\": 128,  \n    \"max_bin\": 512,\n    \"num_iterations\": 100,\n    \"n_estimators\": 100\n}\ngbm = lgb.LGBMRegressor(**hyper_params)\ngbm.fit(X_train, X_test,\n        eval_set=[(X_train, X_test)],\n        eval_metric='l1',\n        early_stopping_rounds=100)\nnp.mean(cross_val_score(gbm,X_train,X_test,scoring = 'neg_mean_squared_error', cv= 3)) ","28bbce29":"# Data Cleaning","95cc461c":" ### Compared all algorithm and got extra trees regressor with high accuracy as they use all their data in making trees                   ","94859cc3":"****3700 or 3800 has also high unit price with low unitprice****","023f7000":"unit price and quantity has high non linear correlation after checking with all the other variables ([Spearman-RankCorrelation](http:\/\/en.wikipedia.org\/wiki\/Spearman%27s_rank_correlation_coefficient))","04c31026":"# Data Processing","d85a5609":"****Low customer id has also high unit price with low prices****","db220797":"### Used genetic algorithm for optimization for extra tree","7e995db6":"# Appendix","df009983":"# EDA","13b1ab41":"linear correlation is very less w.r.t unitprice with every variable so there could be a \nnon linear correlation ","227d03d5":"### Using this value because the data is randomly selected in for training and testing so outliers can be used for trees based algorithm you will know more which algorithm is suitable at the bottom","785e816b":"****In country 35 ,14 and 30 there are comparatively high unit price****","6f759081":"### You can see the other algorithm used in the appendix section","28919c9f":"# ML Algorithms"}}