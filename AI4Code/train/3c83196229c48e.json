{"cell_type":{"fbcae304":"code","b843b36b":"code","87bea4ad":"code","025169e5":"code","01d66c85":"code","7e147d50":"code","0e2487a2":"code","d170efa6":"code","0d1b033b":"code","0e1333dd":"code","a4336840":"code","f3cab1b1":"code","45e31303":"code","14986630":"code","b7e1d9eb":"code","33507dfd":"code","4aa925d4":"code","b194df40":"code","e19021ed":"code","cc411b0a":"code","7ce8350e":"code","e7195924":"code","ff995aa3":"code","2658dd71":"code","30002382":"code","51789441":"code","1a552e07":"code","d92357a4":"code","8a9fec19":"code","d56a9cdb":"code","95cad93b":"code","a52e1d90":"code","03bdfdce":"code","76279827":"code","bca0833c":"code","d51b3542":"code","0007f43e":"code","eda0bc5b":"code","792b180e":"code","a1921b30":"code","9e3f4f5b":"code","443b2ee5":"code","c740a8be":"code","99f3668f":"code","4a0177ad":"code","f2ebf59c":"code","e7f8878d":"code","7f2fe94e":"code","bb6c3bda":"code","8704b57c":"code","05c8c9e4":"code","c34214f2":"code","140f4728":"code","17c3975d":"code","505ac1d2":"code","b809e076":"code","e21ac81f":"code","fa390312":"code","6056872b":"code","49923d27":"code","ffc89510":"code","1290eb02":"code","e5d74328":"code","8ee96cc6":"code","48212992":"code","2abe2620":"code","bce1a1b6":"code","e7f24616":"code","cfca100e":"code","b3c07acc":"code","48aca1c3":"code","599ac551":"code","087f8516":"code","30757ec6":"code","80fe014c":"code","8d970bdc":"code","46dabcfe":"code","554baab4":"code","0a1a7b40":"code","f7f8d239":"code","3555927e":"code","d442414b":"code","d90e6c06":"code","67ffbf14":"code","7933c482":"code","13408a6e":"code","f3e940fa":"code","b044ffa1":"code","576e7bdb":"code","895e541b":"code","d91ec1d0":"code","2e6c28cb":"code","f64143cc":"code","8ffdb468":"code","2315eeb4":"code","ec67ce9b":"code","711fdd8f":"code","de6a1416":"code","9c250edf":"code","6d0d254d":"code","696f87c6":"code","79c991b0":"code","c58bf548":"code","1eb18f13":"code","2c406ce3":"code","7616eb9b":"code","d98caac6":"code","c55aac7d":"code","24472dee":"code","c5dae322":"code","78aef93c":"code","3df1520b":"code","f7d1aa9a":"code","8b4486bd":"code","f38738a7":"code","c7de0cd8":"code","fbb22925":"code","b9924bd4":"code","e806c5c0":"code","88982f5b":"code","2de0e0b0":"code","a0bba492":"code","2a23c6df":"code","8eae6245":"code","17d29c37":"code","7f8523d4":"code","ab08303d":"code","8391e1ee":"code","cf519d14":"code","9338863f":"code","279abd3e":"code","9204afd4":"code","c9ad2a6f":"code","e9d10460":"markdown","c73b46ba":"markdown","5f021378":"markdown"},"source":{"fbcae304":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b843b36b":"import nltk \nfrom nltk.tokenize import sent_tokenize, word_tokenize, WhitespaceTokenizer\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.probability import FreqDist\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk import pos_tag\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","87bea4ad":"# nltk.corpus.wordnet.fileids()","025169e5":"# lemma = WordNetLemmatizer()\n# pstem = PorterStemmer()\n# lemma.lemmatize('goodness'), pstem.stem('goodness')","01d66c85":"# for word in nltk.corpus.wordnet.words():\n#     print(word)","7e147d50":"df_train=pd.read_csv('..\/input\/train.csv')\ndf_test=pd.read_csv('..\/input\/test.csv')\ndf_subm=pd.read_csv('..\/input\/sample_submission.csv')","0e2487a2":"df_train.info()","d170efa6":"df_train.sample(5)","0d1b033b":"df_train.shape, df_test.shape","0e1333dd":"# for i in range(len(df_train.columns)):\n#     if i in [0,2,3,6,7,8,9]:\n#         pass\n#     else:\n#         print(df_train.iloc[:,i].value_counts())","a4336840":"df_train.dtypes","f3cab1b1":"df_train.isnull().sum()","45e31303":"col = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6']\nfor c in col:\n    df_train[c].fillna(df_train[c].dropna().median(), inplace=True)\n    df_test[c].fillna(df_train[c].dropna().median(), inplace=True)\n\ndf_train['advice_to_mgmt'].fillna('', inplace=True)\ndf_test['advice_to_mgmt'].fillna('', inplace=True)\n\ndf_train.dropna(subset=['negatives','summary'], inplace=True)\ndf_test.dropna(subset=['negatives','summary'], inplace=True)","14986630":"df_train.shape, df_test.shape","b7e1d9eb":"df_train.isnull().sum()","33507dfd":"drop_col = ['ID', 'location', 'date']\ndf_train.drop(columns=drop_col, inplace=True)\ndf_test.drop(columns=drop_col, inplace=True)","4aa925d4":"df_train.shape, df_test.shape","b194df40":"df_train.sample(5)","e19021ed":"df_train['Place'].shape","cc411b0a":"OEncoder = OrdinalEncoder()\nEncoded = OEncoder.fit_transform(df_train[['Place', 'status']])\nEncoded_test = OEncoder.transform(df_test[['Place', 'status']])","7ce8350e":"Encoded","e7195924":"Encoded.shape, df_train.shape","ff995aa3":"def Create_ENC(df, Enc):\n#   Create empty arrays with random elements with dimensions of the encoded column\n    Place_enc = np.empty((len(Enc),))  \n    Status_enc = np.empty((len(Enc),))\n    for i in range(len(Enc)):\n        Place_enc[i] = Enc[i][0]\n        Status_enc[i] = Enc[i][1]\n    df['place_enc'] = Place_enc\n    df['status_enc'] = Status_enc","2658dd71":"Create_ENC(df_train, Encoded)\nCreate_ENC(df_test, Encoded_test)","30002382":"df_test.isnull().sum()","51789441":"df_train.sample(5)","1a552e07":"df_train.groupby('overall').Place.count()","d92357a4":"df_train.groupby('Place').overall.count()","8a9fec19":"# df_train.groupby('job_title').overall.count()\n# no information from this","d56a9cdb":"def Review_len(df):\n    df['len_pos'] = df['positives'].str.len()\n    df['len_neg'] = df['negatives'].str.len()\n    df['len_sum'] = df['summary'].str.len()","95cad93b":"Review_len(df_train)\nReview_len(df_test)","a52e1d90":"df_train.sample(10)","03bdfdce":"df_train.dtypes","76279827":"def ChangeToInt(df,col):\n    df[col]=df[col].astype('int')","bca0833c":"label='overall'\nChangeToInt(df_train,label)","d51b3542":"def show_wordcloud(data, title = None):\n    V_wordcloud = WordCloud(\n        background_color = 'white',\n        max_words = 200,\n        max_font_size = 40, \n        scale = 3,\n        random_state = 7\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize = (20, 20))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize = 20)\n        fig.subplots_adjust(top = 2.3)\n\n    plt.imshow(V_wordcloud)\n    plt.show()","0007f43e":"# print positive wordcloud\nshow_wordcloud(df_train[\"positives\"])","eda0bc5b":"# print negatives wordcloud\nshow_wordcloud(df_train[\"negatives\"])","792b180e":"# print summary wordcloud\nshow_wordcloud(df_train[\"summary\"])","a1921b30":"# def Reviews(df):\n#     df['Reviews']=df['positives']+' '+df['negatives']+' '+df['summary']\n# #     +' '+df['advice_to_mgmt']","9e3f4f5b":"# Reviews(df_train)\n# Reviews(df_test)","443b2ee5":"# df_train.Reviews[0]","c740a8be":"def get_wordnet_pos(pos_tag):\n    if pos_tag.startswith('J'):\n        return wordnet.ADJ\n    elif pos_tag.startswith('V'):\n        return wordnet.VERB\n    elif pos_tag.startswith('N'):\n        return wordnet.NOUN\n    elif pos_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","99f3668f":"def clean_text(text):\n    # lower text\n    text = text.lower()\n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # remove stop words\n    stop = stopwords.words('english')\n    text = [x for x in text if x not in stop]\n    # remove empty and less than 3 length tokens\n    text = [t for t in text if len(t) >= 3]\n    # pos tag text\n    pos_tags = pos_tag(text)\n    # lemmatize text\n    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n    # remove words with less than 3 letters\n    text = [t for t in text if len(t) >= 3]\n    # join all\n    text = \" \".join(text)\n    return(text)","4a0177ad":"# # clean text data\n# df_train[\"Clean_reviews\"] = df_train[\"Reviews\"].apply(lambda x: clean_text(x))\n# df_test[\"Clean_reviews\"] = df_test[\"Reviews\"].apply(lambda x: clean_text(x))","f2ebf59c":"# df_train.drop(columns='Reviews', inplace=True)\n# df_test.drop(columns='Reviews', inplace=True)","e7f8878d":"df_train[\"Clean_positives\"] = df_train[\"positives\"].apply(lambda x: clean_text(x))\ndf_train[\"Clean_negatives\"] = df_train[\"negatives\"].apply(lambda x: clean_text(x))","7f2fe94e":"df_test[\"Clean_positives\"] = df_test[\"positives\"].apply(lambda x: clean_text(x))\ndf_test[\"Clean_negatives\"] = df_test[\"negatives\"].apply(lambda x: clean_text(x))","bb6c3bda":"df_train[\"Clean_summary\"] = df_test[\"summary\"].apply(lambda x: clean_text(x))\ndf_test[\"Clean_summary\"] = df_test[\"summary\"].apply(lambda x: clean_text(x))","8704b57c":"df_train.shape, df_test.shape","05c8c9e4":"df_train.isnull().sum()","c34214f2":"df_test.sample(2)","140f4728":"df_train[\"Clean_reviews\"] = df_train[\"Clean_positives\"]+' '+df_train[\"Clean_negatives\"]+' '+df_train[\"Clean_summary\"]\ndf_test[\"Clean_reviews\"] = df_test[\"Clean_positives\"]+' '+df_test[\"Clean_negatives\"]+' '+df_test[\"Clean_summary\"]","17c3975d":"df_train.head(2)","505ac1d2":"# scores = ['score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'score_6']\n# for col in scores:\n#     print(df_train[col].value_counts())\n# score_6 column doesn't have uniform values. Ignore this column in analysis","b809e076":"# df_train.isnull().sum()  \n# 1115\n# df_train[df_train['Clean_reviews'].isnull() == True]","e21ac81f":"# df_train.drop(columns=['num_words_pos', 'num_words_neg'], inplace=True)\n# df_test.drop(columns=['num_words_pos', 'num_words_neg'], inplace=True)","fa390312":"df_train.shape, df_test.shape","6056872b":"df_train.isnull().sum()","49923d27":"df_train.dropna(inplace=True)\ndf_test.dropna(inplace=True)","ffc89510":"df_train.shape, df_test.shape","1290eb02":"len(df_train.loc[0,'Clean_positives'].split())","e5d74328":"def num_words(df):\n    df['num_words_pos'] = df['positives'].apply(lambda x: len(x.split()))\n    df['num_words_neg'] = df['negatives'].apply(lambda x: len(x.split()))\n    df['num_words_sum'] = df['summary'].apply(lambda x: len(x.split()))\n#     df['num_words_neg'] = len(df['negatives'].str.split())","8ee96cc6":"num_words(df_train)\nnum_words(df_test)","48212992":"df_train.columns","2abe2620":"# df_test.isnull().sum()","bce1a1b6":"# token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n# CVec = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\nCVec = CountVectorizer(stop_words='english', min_df=3)\nfeat_col=['place_enc', 'status_enc', 'len_pos', 'len_neg', 'num_words_pos', 'num_words_neg', 'num_words_sum', \n          'len_sum', 'score_1', 'score_2', 'score_3', 'score_4', 'score_5', 'Clean_reviews']\nX=df_train[feat_col]\nY=df_train['overall']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=7)","e7f24616":"X_train_vect=CVec.fit_transform(X_train['Clean_reviews'])\nX_test_vect=CVec.transform(X_test['Clean_reviews'])","cfca100e":"X_train_vect.shape, X_test_vect.shape","b3c07acc":"len(CVec.get_feature_names())","48aca1c3":"reviews_train = pd.DataFrame(X_train_vect.todense(), columns=CVec.get_feature_names())\nreviews_test = pd.DataFrame(X_test_vect.todense(), columns=CVec.get_feature_names())","599ac551":"reviews_train.shape, reviews_test.shape","087f8516":"y_train.shape, y_test.shape","30757ec6":"X_train.drop(columns='Clean_reviews', inplace=True)\nX_test.drop(columns='Clean_reviews', inplace=True)","80fe014c":"X_train.shape , X_test.shape","8d970bdc":"# X_train_withvect = pd.concat([X_train, reviews_train], axis=1)\n# X_test_withvect = pd.concat([X_test, reviews_test], axis=1)\n\n# creating a new df is causing RAM to be exhausted.","46dabcfe":"X_train.reset_index(drop=True, inplace=True)\n# X_train.drop(columns='index', inplace=True)\nX_test.reset_index(drop=True, inplace=True)\n# X_test.drop(columns='index', inplace=True)","554baab4":"X_train.head()\n","0a1a7b40":"reviews_train.head()","f7f8d239":"# X_train_ = pd.concat([X_train, reviews_train], axis=1)\n# X_test_ = pd.concat([X_test, reviews_test], axis=1)","3555927e":"# X_train_.shape, X_test_.shape","d442414b":"clean_text('aa'), clean_text('abused'), clean_text('abusive')","d90e6c06":"# X_train_.isnull().sum()","67ffbf14":"# X_train_.dropna(inplace=True)\n# X_test_.dropna(inplace=True)","7933c482":"X_train.columns","13408a6e":"dummy_cols=['place_enc','status_enc']\ntrain_dummies=pd.get_dummies(data=X_train, columns=dummy_cols)\ntest_dummies=pd.get_dummies(data=X_test, columns=dummy_cols)","f3e940fa":"t_dummy_cols = [ col for col in X_train.columns.tolist() if col not in dummy_cols]","b044ffa1":"train_dummies.drop(columns=t_dummy_cols, inplace=True)\ntest_dummies.drop(columns=t_dummy_cols, inplace=True)","576e7bdb":"train_dummies.dtypes","895e541b":"train_dummies.shape, test_dummies.shape","d91ec1d0":"train_cat = pd.concat([train_dummies, reviews_train], axis=1)\ntest_cat = pd.concat([test_dummies, reviews_test], axis=1)","2e6c28cb":"train_cat.shape, test_cat.shape","f64143cc":"train_cat.head()","8ffdb468":"X_train.drop(columns=['place_enc', 'status_enc'], inplace=True)\nX_test.drop(columns=['place_enc', 'status_enc'], inplace=True)","2315eeb4":"X_train.shape, X_test.shape","ec67ce9b":"X_train.head()","711fdd8f":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","de6a1416":"X_train.columns[:10]","9c250edf":"# col_to_scale = ['len_pos', 'len_neg', 'num_words_pos', 'num_words_neg', 'num_words_sum', 'len_sum']\ncol_to_scale = X_train.columns.tolist()\nscaled_tr_values = scaler.fit_transform(X_train[col_to_scale])\nscaled_ts_values = scaler.transform(X_test[col_to_scale])","6d0d254d":"X_train.loc[:,col_to_scale] = scaled_tr_values\nX_test.loc[:,col_to_scale] = scaled_ts_values","696f87c6":"X_test.head()","79c991b0":"model_XGB = XGBClassifier(random_state=5, n_jobs=-1)\nmodel_XGB.fit(train_cat, y_train)","c58bf548":"# pred_XGB = model_XGB.predict(test_cat)\n# print('XGB_Train_Accuracy: ', model_XGB.score(train_cat, y_train))\n# print('XGB_Test_Accuracy: ', accuracy_score(y_test, pred_XGB))\n# print('XGB_F1_score: ', f1_score(y_test, pred_XGB, average='weighted'))","1eb18f13":"pred_test_XGB = model_XGB.predict_proba(test_cat)","2c406ce3":"pred_test_XGB","7616eb9b":"pb_col=['pb1', 'pb2', 'pb3', 'pb4', 'pb5']","d98caac6":"prob_sc = pd.DataFrame(data=pred_test_XGB, columns=pb_col)","c55aac7d":"prob_sc.shape","24472dee":"X_test.shape","c5dae322":"# from random import sample\ndata_tr = pd.concat([prob_sc, X_test], axis = 1)\n# index_trn = sample(list(data_tr.index),round(len(data_tr)*0.8))","78aef93c":"y_test.reset_index(drop=True, inplace=True)","3df1520b":"Xf_train, Xf_test, yf_train, yf_test = train_test_split(data_tr, y_test, random_state=4)","f7d1aa9a":"Xf_train.shape, yf_train.shape","8b4486bd":"logi1 = LogisticRegression('l2',1,.01,.05,1,solver='liblinear',max_iter=500)\nlogi1.fit(Xf_train,yf_train)\nprediction_logi1 = logi1.predict(Xf_test)","f38738a7":"print('Train_Accuracy: ', logi1.score(Xf_train,yf_train))\nprint('Test_Accuracy: ', accuracy_score(yf_test, prediction_logi1))\nprint('F1_score: ', f1_score(yf_test, prediction_logi1, average='weighted'))","c7de0cd8":"# model_NB = MultinomialNB()\n# model_NB.fit(train_cat, y_train)\n\n# prediction_NB = model_NB.predict_proba(test_cat)","fbb22925":"# # print('AUC: ', roc_auc_score(y_test, prediction))\n# print('NB_Train_Accuracy: ', model_NB.score(X_train_, y_train))\n# print('NB_Test_Accuracy: ', accuracy_score(y_test, prediction_NB))\n# print('NB_F1_score: ', f1_score(y_test, prediction_NB, average='weighted'))","b9924bd4":"# X_prob_NB = model_NB.predict_proba(X_test_)","e806c5c0":"# X_prob_NB[:5]","88982f5b":"# y_test[:10]","2de0e0b0":"# prediction_NB[:10]","a0bba492":"# model_DTC = DecisionTreeClassifier(random_state=7)\n# model_DTC.fit(X_train_, y_train)\n\n# prediction_DTC = model_DTC.predict(X_test_)","2a23c6df":"# print('DTC_Train_Accuracy: ', model_DTC.score(X_train_, y_train))\n# print('DTC_Test_Accuracy: ', accuracy_score(y_test, prediction_DTC))\n# print('DTC_F1_score: ', f1_score(y_test, prediction_DTC, average='weighted'))","8eae6245":"# # # num = [5 , 7 , 9, 11]\n# # num = [5]\n# # for k in num:\n# k=5\n# model_KNN = KNeighborsClassifier(n_neighbors=k)\n# model_KNN.fit(X_train_, y_train)\n\n# prediction_KNN = model_KNN.predict(X_test_)","17d29c37":"# print('Num_Neighbors: ', k)\n# print('KNN_Train_Accuracy: ', model_KNN.score(X_train_, y_train))\n# print('KNN_Test_Accuracy: ', accuracy_score(y_test, prediction_KNN))\n# print('KNN_F1_score: ', f1_score(y_test, prediction_KNN, average='weighted'))","7f8523d4":"# model_RDF = RandomForestClassifier(n_estimators=300, random_state=5, n_jobs=-1)\n# model_RDF.fit(X_train_, y_train)\n\n# prediction_RDF = model_RDF.predict(X_test_)","ab08303d":"# # print('AUC: ', roc_auc_score(y_test, prediction))\n# print('RDF_Accuracy: ', accuracy_score(y_test, prediction_RDF))\n# print('RDF_F1_score: ', f1_score(y_test, prediction_RDF, average='weighted'))","8391e1ee":"# model_SGD = SGDClassifier(random_state=5, n_jobs=-1)\n# model_SGD.fit(X_train_, y_train)\n\n# prediction_SGD = model_SGD.predict(X_test_)","cf519d14":"# # print('AUC: ', roc_auc_score(y_test, prediction))\n# print('SGD_Train_Accuracy: ', model_SGD.score(X_train_, y_train))\n# print('SGD_Test_Accuracy: ', accuracy_score(y_test, prediction_SGD))\n# print('SGD_F1_score: ', f1_score(y_test, prediction_SGD, average='weighted'))","9338863f":"# model_LGR = LogisticRegression(random_state=5, n_jobs=-1)\n# model_LGR.fit(X_train_, y_train)\n\n# prediction_LGR = model_LGR.predict(X_test_)","279abd3e":"# print('LGR_Train_Accuracy: ', model_LGR.score(X_train_, y_train))\n# print('LGR_Test_Accuracy: ', accuracy_score(y_test, prediction_LGR))\n# print('LGR_F1_score: ', f1_score(y_test, prediction_LGR, average='weighted'))","9204afd4":"# model_ET = ExtraTreeClassifier(random_state=5)\n# model_ET.fit(X_train_, y_train)\n\n# prediction_ET = model_ET.predict(X_test_)","c9ad2a6f":"# print('ET_Train_Accuracy: ', model_ET.score(X_train_, y_train))\n# print('ET_Test_Accuracy: ', accuracy_score(y_test, prediction_ET))\n# print('ET_F1_score: ', f1_score(y_test, prediction_ET, average='weighted'))","e9d10460":"> Reference: https:\/\/towardsdatascience.com\/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e","c73b46ba":"> K = 5 is it. with the least worst F1_score of 0.3013  (during initial testing of the data) using positives, negatives, summary, advice_to_management","5f021378":"MultinomialNB Model: Initial version commits(Not recent)\n> Positives + Negatives:\nAccuracy:  0.36494518557654204\nF1_score:  0.3254225708516727\n> Positives + Negatives + Summary:\nAccuracy:  0.36402060493990224\nF1_score:  0.32921876951906376\n> Positives + Negatives + Summary + advice_to_mgmt:\nAccuracy:  0.36309602430326243\nF1_score:  0.3289207289982767"}}