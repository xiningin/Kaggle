{"cell_type":{"a8b63d36":"code","c1441823":"code","52facd2b":"code","897f1b62":"code","112b9bf7":"code","e31d161c":"code","1810cfeb":"code","e924721f":"code","861b44cc":"code","43a64e05":"code","30b3369e":"code","37157c48":"code","196b68f6":"code","a0e17bed":"code","177f6c26":"code","97527c78":"code","911feb8d":"code","5437be08":"code","9dbba331":"code","f21b020f":"code","8cdc5ec3":"code","629dea1a":"code","aac8c0bb":"code","5c41a9c6":"code","28422bad":"code","9e76c602":"code","10903088":"code","bc0bbdb8":"code","96af07b7":"code","f1d47ed0":"markdown","08f94fc7":"markdown","d16c7e4b":"markdown","acce93af":"markdown","d4da5039":"markdown","f1e182b0":"markdown","f5bfa98f":"markdown","4e3fe8d0":"markdown","61542d5a":"markdown","cff133ab":"markdown","f825bb24":"markdown","18f93b68":"markdown","7a092d58":"markdown","1ffd1ddd":"markdown","28b5bcc1":"markdown","527b6fb7":"markdown","29e543fe":"markdown","bd9f96d7":"markdown","8b80fd5c":"markdown","61bbe460":"markdown","6b850fcd":"markdown","55100303":"markdown","57bf181c":"markdown","9834470b":"markdown","6e71ee4a":"markdown","655b22bc":"markdown","3f7764c7":"markdown","36f63b1c":"markdown","fd429ca7":"markdown"},"source":{"a8b63d36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1441823":"df = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\") \n\ndf.head()","52facd2b":"df.info()","897f1b62":"df.isnull().sum()","112b9bf7":"#Visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nprint(df.quality.value_counts()) #shows that classifications as quality 3 and 8 might be unclassifiable since only a few samples for training exist\ndf.quality.value_counts().plot(kind='bar')","e31d161c":"#Boxplots for every feature and the according quality values from 3 to 8\n\nfeatures = df.drop(columns=['quality'])\n    \nf, ax = plt.subplots(4, 3, figsize=(22, 16))\nfor i, var in enumerate(features):\n    sns.boxplot(x='quality', y=var, data=df, ax=ax.flatten()[i])","1810cfeb":"#Boxplot values for different features broken down to the different quality classes\nfor feature in features:\n    print(feature)\n\n    for i in range(3,9):     \n        df_i = df.loc[df['quality'] == i]\n\n        features_perqual = df_i.drop(columns=['quality'])\n      \n  \n        upper_quartile = np.percentile(features_perqual[feature], 75)\n        lower_quartile = np.percentile(features_perqual[feature], 25)\n        #print(upper_quartile)\n        \n        iqr = upper_quartile - lower_quartile\n        upper_whisker = features_perqual[feature][features_perqual[feature]<=upper_quartile+1.5*iqr].max()\n        lower_whisker = features_perqual[feature][features_perqual[feature]>=lower_quartile-1.5*iqr].min()\n        #print('Quality:', i)\n        print('Quality:', i, 'Upper whisker\/Lower whisker:', upper_whisker,'\/', lower_whisker)","e924721f":"#Boxplot values for all different features across all different quality classes\nfor feature in features:\n    \n    upper_quartile = np.percentile(features[feature], 75)\n    lower_quartile = np.percentile(features[feature], 25)\n        #print(upper_quartile)\n        \n    iqr = upper_quartile - lower_quartile\n    upper_whisker = features[feature][features[feature]<=upper_quartile+1.5*iqr].max()\n    lower_whisker = features[feature][features[feature]>=lower_quartile-1.5*iqr].min()\n    \n    print(feature)\n    print('Upper whisker\/Lower whisker:', upper_whisker,'\/', lower_whisker)","861b44cc":"#Create the pairplot\n\nsns.set_palette(\"bright\")\npp = sns.pairplot(df, hue='quality', height=1.8, aspect=1.8, diag_kind = 'kde', palette=\"bright\")\n    \nfig = pp.fig \nfig.subplots_adjust(top=0.93, wspace=0.3)\nt = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)","43a64e05":"#With calculating the Pearson correlation, linear correlation between the features are identified\nlin_corr = features.corr()\n\nf, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(lin_corr, annot=True, linewidths=.5, ax = ax)","30b3369e":"#Checking the Spearman correlation whether there are any correlations\n\nspear_corr = features.corr(method='spearman')\n\nf, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(spear_corr, annot=True, linewidths=.5, ax = ax)","37157c48":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import GridSearchCV\n\n#all feature columns must be modelled\nfeatures=df.columns[1:11]\n\n#hyperparameters to be decided\nclf=IsolationForest(n_estimators=50, max_samples='auto', contamination = 0.05, \\\n                        max_features=1.0, bootstrap=False, n_jobs=-1, verbose=0)\nclf.fit(df[features])\n\npred = clf.predict(df[features])\ndf['anomaly']=pred\noutliers=df.loc[df['anomaly']==-1]\noutlier_index=list(outliers.index)\n\n#Find the number of anomalies and normal points here points classified -1 are anomalous\nprint(df['anomaly'].value_counts())","196b68f6":"#Pairplot marking the outliers\npp = sns.pairplot(df, hue='anomaly', height=1.8, aspect=1.8, diag_kind = 'kde', palette=\"bright\")\n           \nfig = pp.fig \nfig.subplots_adjust(top=0.93, wspace=0.3)\nt = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)","a0e17bed":"#Remove all identified outliers from the dataset \n\ndf_preprocessed = df[df.anomaly != -1]\n\ndf_preprocessed = df_preprocessed.drop(columns=['anomaly'])\n\nprint(df_preprocessed.quality.value_counts())","177f6c26":"from sklearn.model_selection import train_test_split\n\n#X Dataset, y dataset\n\nX = df_preprocessed.drop(columns=['quality'])\ny = df_preprocessed['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","97527c78":"#Scaling the X Data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","911feb8d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# 4. Model training\n\n#Perform Grid Search for hyperparameter tuning\nparam_grid = {'n_estimators': [1,5,10,50,100,200,500],\n              'max_depth': [2, 4, 6, 8]}\n\nRFC_qual = RandomForestClassifier()\n\ngrid_clf = GridSearchCV(RFC_qual, param_grid)\ngrid_clf.fit(X_train, y_train)\n\nprint(grid_clf.best_estimator_.n_estimators)\nprint(grid_clf.best_estimator_.max_depth)","5437be08":"y_pred = grid_clf.predict(X_test)","9dbba331":"from sklearn import metrics\n\nprint('RFC Accuracy =', metrics.accuracy_score(y_test, y_pred))","f21b020f":"from sklearn.metrics import confusion_matrix\n\n#Confusion matrix creation\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicted'])","8cdc5ec3":"#Feature importance matrix \n\nfrom sklearn.inspection import permutation_importance\nfrom pandas import DataFrame\n\n(pd.Series(grid_clf.best_estimator_.feature_importances_, index=X.columns).nlargest(11).plot(kind='barh'))","629dea1a":"#Create a new dataframe that contains column bin instead of quality\n#Create train\/test data that can be distinguished from train\/test on single quality level\n\n# Creating  categories for poor quality as 0 (0, 1, 2, 3), medium quality as 1 (4, 5, 6) and high quality as 2 (7, 8, 9, 10)\n\ndf_preprocessed.loc[(df_preprocessed['quality']<= 3) , 'quality'] = 0\ndf_preprocessed.loc[(df_preprocessed['quality']>= 4) & (df_preprocessed['quality']< 7), 'quality'] = 1\ndf_preprocessed.loc[(df_preprocessed['quality']>= 7) , 'quality'] = 2","aac8c0bb":"print(df_preprocessed.quality.value_counts())","5c41a9c6":"#X Dataset, y dataset\n\nX_cat = df_preprocessed.drop(columns=['quality'])\ny_cat = df_preprocessed['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X_cat, y_cat, test_size=0.2, random_state=42)","28422bad":"#Scaling the X Data,\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","9e76c602":"# 4. Model training\n\nRFC = RandomForestClassifier(n_jobs=-1, max_features= 'sqrt' ,n_estimators=50)\n\n#Perform Grid Search for hyperparameter tuning\nparam_grid = {'n_estimators': [1,5,10,50,100,200,500],\n              'max_depth': [2, 4, 6, 8],\n              'max_features': ['auto', 'sqrt', 'log2']}\n\ngrid_clf = GridSearchCV(RFC, param_grid)\ngrid_clf.fit(X_train, y_train)\n\nprint(grid_clf.best_estimator_.n_estimators)\nprint(grid_clf.best_estimator_.max_depth)","10903088":"from sklearn.metrics import classification_report\n\n# 5. Model testing\ny_pred = grid_clf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nprint('RFC Accuracy =', metrics.accuracy_score(y_test, y_pred))","bc0bbdb8":"#Confusion Matrix creation\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames = ['Actual'], colnames =['Predicted'])","96af07b7":"#Feature importance matrix \n\nfrom sklearn.inspection import permutation_importance\nfrom pandas import DataFrame\n\n(pd.Series(grid_clf.best_estimator_.feature_importances_, index=X.columns).nlargest(11).plot(kind='barh'))","f1d47ed0":"For the parameters, we use trial and error by checking the pairwaise plot each time we change the parameters. Since we do not have a classification of outlier or not, viewing the pairwise plot and identify whether the anomaly detection has coloured the outliers in blue has to be sufficient. By using this methods we need to estimate specifically whether it is worth it to upgrade the **contamination rate** in order to identify more outliers trading a false positive marking inliers as outliers. We tried different contamination rates with the result that the predictions that excluded less than 5% of the instances got worse. Another way to overcome the problem of not knowing the contamination rate of outliers is an **Extended Isolation Forest**. To learn more about Extended Isoltation Forest read https:\/\/towardsdatascience.com\/outlier-detection-with-extended-isolation-forest-1e248a3fe97b","08f94fc7":"As estimated before, the prediction for qualities 3, 4 and 8 are poor since they are not even there. We cannot leave the results with an accuracy of around 60 something %.","d16c7e4b":"As a first visualization method the boxplot diagramm is a handy method to look at the distribution of the data. Since we know that the data lacks instances with qualities 3,4, and 8, we look at the distribution of the feature data for every quality (3 to 8) to make sure not to misqualify data points as outliers only because they are out of range due to the underrepresented classes. \n\nSecondly we can get a first idea which properties might contribute to outstanding quality (8) (which need to be proven later):\n\n- relatively small volatile acidity\n- high amount of citric acid\n- low density\n- high amount of sulphates\n- tend to have a higher alcohol content\n\nFor the red wines classified with poor quality it is the other way around.","acce93af":"Comparing the values for the overall values for the whiskers per feature with the values for the whiskers per feature and quality shows that the interquartile ranges mostly lie randomly within the ranges of the feature per quality. \nSince we have multiple features, treating outliers by only deleting data points that do not fall in the inter quartile range seems to be wrong. Because it would imply that the features do not interact with each other. That is an assumption we cannot make.","d4da5039":"### 7. Conclusion\n\nWe loaded the data and had a quick look. As a result we can state:\n- There are instances of red wine with 11 features of chemical values and identified quality with classes 3,4,5,6,7 and 8.\n- There is no missing data.\n- Some classes are underrepresented.\n\nWe visualized the data and found out:\n- No features are significantly correlated, so we kept every features.\n- The boxplots for every class show that points outside the interquartile range are not related to the imbalancy in the data. \n\nWe deleted outliers by trial and error comparing the amount of classified outliers with the outliers in the pairplot\nWe standardized our training and test data and trained a random forest model. We used random forest because we are dealing with classification and experience showed high performance adressing classification. \nFor optimizing the parameters, we used Gridsearch searching widely used parameters. \n\nThe evaluation of the predicted results showed a lack of performance predicting the underrepresented quality classification right. \nTherefore, we categorized the different classes into bad, medium and good quality and retrained the model using the modified categories. \nAs a different approach we oversampled the underrepresented classes with SMOTE and retrained the model using the initial classes. \nIn the end we combined binning with oversampling. When oversampling the data, we always made sure, only the training data is oversampled, test data is kept in its initial state.\n\nBinning the data without any oversampling data treatment worked out best regarding the accuracy score of ~88-90%. But those results need to be looked at critically since they will never classify a wine with poor quality and also when classifying excellent quality the performance is lacking. \nAny oversampling approach failed to improve the single classification or the binned classification better. \n\nThe feature importance confirmed what we could already guessed from the boxplots. Both models (the binned and the single classification) are highlgy influenced by:\n\n- alcohol \n- volatile acidity and \n- sulphates \n","f1e182b0":"The null test shows that there is no null value in the data which means further missing data treatment ist not required.","f5bfa98f":"#### 3.2 Data distribution","4e3fe8d0":"A quick overview tells us that the ingested data contains 11 features that lead to a classification of the redwine quality (12th column). We have 1599 instances that we can train and test the model on. ","61542d5a":"The heatmap gives us a first guess which features could be higher correlated. It shows an increased linear correlation for following pairs:\n1. \"free sulfur dioxide\" and \"total sulfur dioxide\"\n2. \"fixed acidity\" and \"citric acid\" \n\nI consider features being highly correlated if the correlation is > 0.7 https:\/\/www.andrews.edu\/~calkins\/math\/edrm611\/edrm05.htm.\nBy looking at the specific linear correlation values non of the identified pairs have a correlation above 0.7. \nTo ensure that the features do not correlate at all, we check the Spearman correlation which gives us also information about correlations that are non-linear. ","cff133ab":"For a multivariate Outlier Detection we choose Isolation Forest. The  To learn more about **Isolation Forest** read https:\/\/towardsdatascience.com\/outlier-detection-with-isolation-forest-3d190448d45e.\nAbove all we use Isolation Forest because do not exactly know the distribution of the data as well as it is a multidimensional feature space. \nOn top of that, Isolation Forests are easy to optimize and it is fairly robust. Why we do not use DBSCAN instead is a matter of effort. DBSCAN requires preprocessing steps whereas Isolation Forest does not. ","f825bb24":"Comparing the Outliers identified by Isolation Forest with the boxplots shows that not in ervery case the feature value of the anormal cases lie within the 1st and 4th quantiles of the boxplots. \nThat shows, eliminating outliers only by looking at the boxplots could lead to wrong conclusions. \n\nWe chose to remove the outliers from the dataset. What proved to be sufficient here, can be seen as cracking a nut with a sledgehammer. Other treatment could be considered marking the outliers as missing values and impute them by using the **Miss Forest** algorithm. ","18f93b68":"#### 3.7 Standardization\n\nThe last preprocessing step will be a standardization of the X data. Why standardization can optimize your model you can read here https:\/\/towardsdatascience.com\/how-and-why-to-standardize-your-data-996926c2c832. In our case it is obvious when you see the values for the different physiochemical units. Where \"total sulfur dioxide\" varies from 0 - 300, chlorides only vary from 0 - 0.6. This could lead to a bias in our model. ","7a092d58":"To learn more about caling read this https:\/\/towardsdatascience.com\/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02","1ffd1ddd":"#### 3.3 Correlations","28b5bcc1":"Counting the different classifications shows a slight imbalance in the data since data classifid as 3, 4 and 8 are underpresented compared to wine with qualities 5, 6 and 7. \nFrom the information about the data in Kaggle, we know that the rating was given from 1 to 10. Quality values 1,2,9 and 10 are not presented in the data set which means the algorithm to be chosen will never classify any new introduced wine with these values. How and if we treat the imbabalanced dataset will be decided after one iteration showing in the model evaluation how the imbalanced data influences the predictions. ","527b6fb7":"### 3. Data Preprocessing\n\n#### 3.1 Data Visualization\n","29e543fe":"As we can see, the free and total sulfur dioxide  are non-linear correlated with 0.79. \n\nNevertheless, this dataset only contains 11 features. If the amount of features were higher, we could consider eliminating some correlated due to the high computational costs. Meanwhile a correlation underneath 0.95 with only 11 features in total in the set is not enough to consider elimination here. ","bd9f96d7":"### 2. Data Validation\n\n#### 2.1 Overview ","8b80fd5c":"#### 2.2 Missing data detection","61bbe460":"### 5. Model testing & Evaluation","6b850fcd":"As a result of the preprocessing steps above we receive a dataframe that \n- contains less outliers \n- we approved has no drastic correlations across the features\n\nAlthough it is very unlikely to receive good predictions for the qualities 3,4 and 8 or any predictions for the qualities 0, 1, 2, 9, 10 which are not listed at all, the data is now brought in a suitable form for training \n","55100303":"### 4. Model training","57bf181c":"# Red Wine quality prediction\n## Structure\n### 1. Data Ingestion\n### 2. Data Validation\n### 3. Data Preprocessing\n### 4. Model Training\n### 5. Model Validation\n### 6. Iterations\n### 7. Conclusion","9834470b":"#### 3.4 Outlier treatment","6e71ee4a":"The poor performance for the qualities 3, 4 and 8 due to the few training samples, we have predicted before.\nIn order to eliminate that fact, I will split the data in three groups: Bad quality and good quality. ","655b22bc":"### 1. Data Ingestion","3f7764c7":"#### 3.6 Train-Test Split","36f63b1c":"We can interpret from the pairplot that there are not many outliers compared to the number of instances because the clouds of points seem to be very dense for the majority of features, only a few points lie outside. \nAdditionally, the quality does not seem to have an impact whether the points are within the cloud or outside. All features seem to follow more or less a gaussian distribution. These findings will help us to choose an appropiate outlier detection technique later on.","fd429ca7":"### 6. Iterations\n#### 6.1 Bin creation"}}