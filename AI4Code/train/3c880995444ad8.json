{"cell_type":{"4092ea95":"code","cb07abd6":"code","ca50f7c4":"code","5b6ee301":"code","72b98459":"code","950cae68":"code","42008c90":"code","56a3cec0":"code","0f67dcda":"code","b373e1a4":"code","af52586c":"code","62337c5e":"code","17b27c78":"code","464d9d28":"markdown","cb8e9782":"markdown","7299ca43":"markdown","3691c9b1":"markdown","98307350":"markdown"},"source":{"4092ea95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport sys\nimport os\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","cb07abd6":"# Read the data\ndf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nnum_columns = df.select_dtypes(include=np.number)\ncat_columns = df.select_dtypes(exclude=np.number)\n\n\nprint(f'We have {len(df.columns)} columns, {len(num_columns.columns)} of which are numerical.')","ca50f7c4":"df.SalePrice.describe()","5b6ee301":"sns.distplot(df.SalePrice)","72b98459":"#features_to_analyse = ['MSZoning', 'ExterQual']\nfeatures_to_analyse = cat_columns\nfor f in features_to_analyse:\n    sns.boxplot(f, 'SalePrice', data=df)\n    plt.show()","950cae68":"X_test_full = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntest_ID = X_test_full['Id']\nX_test_full.drop(['Id'], axis=1, inplace=True)\n\n# Separate target from predictors\ny = df.SalePrice              \nX = df.drop(['SalePrice'], axis=1)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# One-hot encode the data\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)","42008c90":"print(X_train.shape)\nprint(y_train.shape)\n\nprint(X_valid.shape)\nprint(y_valid.shape)\n\nprint(X_test.shape)","56a3cec0":"# model tuning\n\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport time\n\n# A parameter grid for XGBoost\nparams = {\n    'n_estimators':[500],\n    'min_child_weight':[4,5], \n    'gamma':[i\/10.0 for i in range(3,6)],  \n    'subsample':[i\/10.0 for i in range(6,11)],\n    'colsample_bytree':[i\/10.0 for i in range(6,11)], \n    'max_depth': [2,3,4,6,7],\n    'objective': ['reg:squarederror', 'reg:tweedie'],\n    'booster': ['gbtree', 'gblinear'],\n    'eval_metric': ['rmse'],\n    'eta': [i\/10.0 for i in range(3,6)],\n}\n\nreg = XGBRegressor(nthread=-1)\n\n# run randomized search\nn_iter_search = 100\nrandom_search = RandomizedSearchCV(reg, param_distributions=params,\n                                   n_iter=n_iter_search, cv=5, iid=False, scoring='neg_mean_squared_error')\n\nstart = time.time()\nrandom_search.fit(X_train, y_train)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time.time() - start), n_iter_search))","0f67dcda":"best_regressor = random_search.best_estimator_","b373e1a4":"from sklearn.metrics import mean_absolute_error\n\n# Get predictions\ny_pred = best_regressor.predict(X_valid)","af52586c":"# Calculate MAE\nrmse_pred = mean_absolute_error(y_valid, y_pred) \n\nprint(\"Root Mean Absolute Error:\" , np.sqrt(rmse_pred))","62337c5e":"# Get predictions\ny_pred_test = best_regressor.predict(X_test)","17b27c78":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = y_pred_test\nsub.to_csv('submission.csv',index=False)","464d9d28":"We can see that most of our features are directly related to the actual sale price, although some of them are clearly reduntant, providing very similar information on the property.","cb8e9782":"## Initial analysis of the data\n\nSince we know there are both numeric and categorical columns we will start by splitting those.","7299ca43":"### Let's start by analysing the target variable","3691c9b1":"We can see that most houses sell for values from 100 000 to 200 000 dollars. \nLets analyse how this value is affected by some of our other features, namely categorical.","98307350":"<h1>Needs more feature engineering here<\/h1>"}}