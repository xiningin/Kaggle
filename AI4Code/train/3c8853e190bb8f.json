{"cell_type":{"3c678d65":"code","0ee903c2":"code","6fdf7a1e":"code","f37066e2":"code","ce99e192":"code","7a1f5050":"code","0cbef341":"code","c3be1055":"code","5ce2208e":"code","d38c5a70":"code","5f74ed29":"code","1b9e2291":"code","b69eda4a":"code","e8d6fc28":"code","c85c56a6":"code","4a1fe431":"code","3cfb892b":"code","12e60b71":"code","cd3540a2":"code","8e358779":"code","e9f3d57e":"code","1ebdeec3":"markdown","e6feb27a":"markdown","24d0794d":"markdown","2d107d3e":"markdown","c609fedd":"markdown","5184ac27":"markdown","c08ed734":"markdown","8112d529":"markdown","44b3aec1":"markdown","2310e3a9":"markdown"},"source":{"3c678d65":"from IPython.display import Image\nImage(\"..\/input\/huggingface-image\/Capture.JPG\")","0ee903c2":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\nimport tensorflow as tf ","6fdf7a1e":"df_train=pd.read_csv('\/kaggle\/input\/stumbleupon\/train.tsv',sep='\\t')\n\ndf_test=pd.read_csv('\/kaggle\/input\/stumbleupon\/test.tsv',sep='\\t',usecols=['urlid','boilerplate'])\n","f37066e2":"df_train.head()\n","ce99e192":"df_train.columns","7a1f5050":"df_train['alchemy_category'].value_counts()","0cbef341":"plt.figure(figsize=(15,10))\nsns.countplot(x=df_train['alchemy_category'],hue=df_train['label']);\nplt.xlabel('Category');\nplt.xticks(rotation=90);","c3be1055":"sns.countplot(x=df_train['label'])\n# This is a balanced dataset ","5ce2208e":"df_train['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_train['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_train['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_train['boilerplate']=df_train['boilerplate'].str.lower()\n\n\n#Cleaning the test dataframe \n\ndf_test['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_test['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_test['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_test['boilerplate']=df_test['boilerplate'].str.lower()","d38c5a70":"from transformers import AutoTokenizer, TFAutoModel\n\n\n#Downloading the tokenizer and the Albert model for fine tuning\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\nbert=TFAutoModel.from_pretrained('bert-base-uncased')","5f74ed29":"#ADD all the variable for the Transformer model \n\nSEQ_length=512\n\n#Lets create the X and Y matrix from the Df train set \n\nXids=np.zeros((df_train.shape[0],SEQ_length))\nXmask=np.zeros((df_train.shape[0],SEQ_length))\ny=np.zeros((df_train.shape[0],1))\n\n#Preparing the test dataframe\n\nXids_test=np.zeros((df_test.shape[0],SEQ_length))\nXmask_test=np.zeros((df_test.shape[0],SEQ_length))\n","1b9e2291":"for i,sequence in enumerate(df_train['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids[i,:],Xmask[i,:],y[i,0]=tokens['input_ids'],tokens['attention_mask'],df_train.loc[i,'label']\n    \n\nfor i,sequence in enumerate(df_test['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids_test[i,:],Xmask_test[i,:]=tokens['input_ids'],tokens['attention_mask']","b69eda4a":"#Check if the GPU is avalaible\ntf.config.get_visible_devices()","e8d6fc28":"dataset=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask},labels\n\ndataset=dataset.map(map_func)\ndataset=dataset.shuffle(100000).batch(32).prefetch(1000)\n\nDS_size=len(list(dataset))\n\ntrain=dataset.take(round(DS_size*0.85))\nval=dataset.skip(round(DS_size*0.85))","c85c56a6":"#Preparing the test dataset\n\ndataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n\ndef map_func(input_ids,mask):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ndataset_test=dataset_test.map(map_func)\ndataset_test=dataset_test.batch(32).prefetch(1000)","4a1fe431":"input_ids=tf.keras.layers.Input(shape=(SEQ_length,),name='input_ids',dtype='int32')\ninput_mask=tf.keras.layers.Input(shape=(SEQ_length,),name='attention_mask',dtype='int32')\n\nembedding=bert(input_ids,attention_mask=input_mask)[0]\n#x=tf.keras.layers.GlobalMaxPool1D()(embedding)\nx=tf.keras.layers.GlobalAveragePooling1D()(embedding)\nx=tf.keras.layers.BatchNormalization()(x)\nx=tf.keras.layers.Dense(128,activation='relu')(x)\nx=tf.keras.layers.Dropout(0.3)(x)\nx=tf.keras.layers.Dense(64,activation='relu')(x)\noutput=tf.keras.layers.Dense(1,activation='sigmoid')(x)\n\n\nmodel=tf.keras.Model(inputs=[input_ids,input_mask],outputs=output)\n\nmodel.layers[2].trainable=False","3cfb892b":"model.summary()","12e60b71":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer='adam',metrics=[tf.keras.metrics.AUC()])","cd3540a2":"history=model.fit(train,validation_data=val,epochs=20)","8e358779":"predictions=model.predict(dataset_test)","e9f3d57e":"df_test['label']=predictions\n\ndf_test.to_csv('submission_avgpool_dp.csv',columns=['urlid','label'],index=False)","1ebdeec3":"# Model Download from Hugging Face ","e6feb27a":"## Cleaning the boilerplate text\n\nLets remove the title and url word from each description . We will also lower case the words as we are planing to used a uncased version of Transformer model","24d0794d":"# Problem Introduction\nStumbleUpon is a user-curated web content discovery engine that recommends relevant, high quality pages and media to its users, based on their interests. While some pages we recommend, such as news articles or seasonal recipes, are only relevant for a short period of time, others maintain a timeless quality and can be recommended to users long after they are discovered. In other words, pages can either be classified as \"ephemeral\" or \"evergreen\". The ratings we get from our community give us strong signals that a page may no longer be relevant - but what if we could make this distinction ahead of time? A high quality prediction of \"ephemeral\" or \"evergreen\" would greatly improve a recommendation system like ours.\n\nMany people know evergreen content when they see it, but can an algorithm make the same determination without human intuition? Your mission is to build a classifier which will evaluate a large set of URLs and label them as either evergreen or ephemeral. Can you out-class(ify) StumbleUpon? As an added incentive to the prize, a strong performance in this competition may lead to a career-launching internship at one of the best places to work in San Francisco.\n\n# Data<br>\n\nThere are two components to the data provided for this challenge:\n\nThe first component is two files: train.tsv and test.tsv. Each is a tab-delimited text file containing the fields outlined below for 10,566 urls total. Fields for which no data is available are indicated with a question mark.\n\ntrain.tsv  is the training set and contains 7,395 urls. Binary evergreen labels (either evergreen (1) or non-evergreen (0)) are provided for this set.\ntest.tsv is the test\/evaluation set and contains 3,171 urls.\n\n# Approach \n\nMy aim of taking this past competition is to try out the effect of the new advances in NLP technology over this dataset. <br> This can allow me to gain experience to in that technology . I am planing to tackle this problem using Transformer architecture (BERT) from Hugging face library and TensorFlow 2.0 . Combining this two would allow me to build a powerful state of the art model for the classification task. \n\n# Pre-processing \n\nI am just going to focus on the NLP part of the data and see if I can beat the 7 year old competition winner ( or atleast come close) . So i am going to neglect the rest of the feature columns. Also with respect to pre-processing I have done minimum pre-processing ( lower-casing and removing the title and url words )from the text as I expect the Hugging face tokenizer to take care of the rest. Also this article being some kind of news feed it wont contain as many slangs and non-proper english phrases. \n\n# Model \n\nPre-traine BERT model from Hugging Face Library \nTake out embedding from the BERT model \nFeed the embedding into a feed forward Neural network model. \nEvaluate on ROC-AUC curve. ","2d107d3e":"Decode the test data and see if urlid and text matches ","c609fedd":"Alchemy catergory does have a role in determining the label for the article \n\nWe see that business, Recreation and health are more likley to be evergreen <br>\n\nWhere as sports computer_internet and arts and entertainment are more like to be non-evergreen. <br>","5184ac27":"# Prediction ","c08ed734":"# Build the model ","8112d529":"# Load the data ","44b3aec1":"# Useful Links \n\nhttps:\/\/towardsdatascience.com\/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a\n\nhttp:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n\nhttps:\/\/www.youtube.com\/watch?v=GYDFBfx8Ts8&ab_channel=JamesBriggs\n\nhttps:\/\/medium.com\/tensorflow\/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a","2310e3a9":"# Data Explorations \n\nThe dataset containes 27 columns and the end goal is predicting if the article is evergreen or non-evergreen. <br>\n"}}