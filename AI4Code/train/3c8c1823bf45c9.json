{"cell_type":{"ae725b53":"code","30435417":"code","b17a3dd2":"code","506e9889":"code","3c876ffb":"code","91c052a8":"code","1a9f4756":"code","35c56a53":"code","d3d04147":"code","630e5f12":"code","079abe82":"code","61735ee6":"code","8b79a71f":"code","c45a917a":"markdown","ca3e9321":"markdown","774d566d":"markdown","4f20ffd4":"markdown"},"source":{"ae725b53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30435417":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom wordcloud import WordCloud, STOPWORDS\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import accuracy_score as acs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","b17a3dd2":"data = pd.read_csv(\"\/kaggle\/input\/spam-ham-dataset\/SMSSpamCollection1.csv\",encoding = 'latin-1')\ndata = data.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\ndata.columns = [\"label\", \"body_text\"]\ndata.head()","506e9889":"data['label'].value_counts().plot(kind = 'pie', explode = [0, 0.15], figsize = (8, 8), autopct = '%1.1f%%', shadow = True)\nplt.xlabel(\"Spam vs Ham\",size=15)\nplt.ylabel(\" \")\nplt.legend([\"ham\", \"spam\"])\nplt.show()","3c876ffb":"#Extracting spam and ham words from messages\nspam_messages = data[data[\"label\"] == \"spam\"][\"body_text\"]\nham_messages = data[data[\"label\"] == \"ham\"][\"body_text\"]\n\nspam_words = []\nham_words = []\n\ndef extractSpamWords(spamMessages):\n    global spam_words\n    words = [word.lower() for word in word_tokenize(spamMessages) if word.lower() \n             not in stopwords.words(\"english\") and word.lower().isalpha()]\n    spam_words = spam_words + words\n    \ndef extractHamWords(hamMessages):\n    global ham_words\n    words = [word.lower() for word in word_tokenize(hamMessages) if word.lower() \n             not in stopwords.words(\"english\") and word.lower().isalpha()]\n    ham_words = ham_words + words\n    \nspam_messages.apply(extractSpamWords)\nham_messages.apply(extractHamWords)","91c052a8":"stopwords = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer() #Stemming\n\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndata['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\ndata['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\ndata.head()\n\n# body_len shows the length of words excluding whitespaces in a message body.\n# punct% shows the percentage of punctuation marks in a message body.","1a9f4756":"plt.figure(figsize=(20,12))\n\n#Spam Word cloud\nplt.subplot(2,2,2, facecolor='k')\nspam_wordcloud = WordCloud(width=600, height=400,collocations=False,\n                           random_state=1,background_color=\"red\").generate(\" \".join(spam_words))\nplt.imshow(spam_wordcloud)\nplt.title(\"Spam Word Cloud\",size=20)\nplt.axis(\"off\")\n\n#Ham word cloud\nplt.subplot(2,2,1, facecolor='k')\nham_wordcloud = WordCloud(width=600, height=400,collocations=False,\n                          random_state=1,background_color=\"green\").generate(\" \".join(ham_words))\nplt.imshow(ham_wordcloud)\nplt.title(\"Ham Word Cloud\",size=20)\nplt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","35c56a53":"# Top 10 spam words\nspam_words = np.array(spam_words)\nprint(\"Top 10 Spam words are :\\n\")\ndisplay(pd.Series(spam_words).value_counts().head(n = 10))\n\n# Top 10 Ham words\nham_words = np.array(ham_words)\nprint(\"Top 10 Ham words are :\\n\")\ndisplay(pd.Series(ham_words).value_counts().head(n = 10))","d3d04147":"data[\"messageLength\"] = data[\"body_text\"].apply(len)\nf, ax = plt.subplots(1, 2, figsize = (20, 6))\n\nsns.distplot(data[data[\"label\"] == \"spam\"][\"messageLength\"], bins = 20, ax = ax[0],color=\"red\")\nax[0].set_xlabel(\"Spam Message Length\",fontSize=20)\nax[0].grid()\n\nsns.distplot(data[data[\"label\"] == \"ham\"][\"messageLength\"], bins = 20, ax = ax[1],color=\"green\")\nax[1].set_xlabel(\"Ham Message Length\",fontSize=20)\nax[1].grid()\nplt.show()","630e5f12":"X=data[['body_text', 'body_len', 'punct%']]\nY=data['label']\n\n#Split data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=42)","079abe82":"tfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\ntfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n\nX_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)\n\nX_train_vect.head()","61735ee6":"rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\nrf_model = rf.fit(X_train_vect, y_train)\n\ny_pred = rf_model.predict(X_test_vect)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label=\"spam\", average='binary')\nprint('Precision: {} \\nRecall: {} \\nF1-Score: {} \\nAccuracy: {}'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), round(acs(y_test,y_pred), 3)*100) +\"% \\n\")","8b79a71f":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nprint(\"The confusion matrix is:\\n {}\".format(df_cm))\n\n#Plot\nplt.figure(figsize=(10,7))\nsns.heatmap(df_cm, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\",fontsize=15)\nplt.xlabel(\"Predicted Label\",fontsize=15)\nplt.ylabel(\"True Label\",fontsize=15)\nplt.show()","c45a917a":"__Testing the model__","ca3e9321":"__Vectorize Text__","774d566d":"**SMS Spam Collection dataset**","4f20ffd4":"**Preprocessing**"}}