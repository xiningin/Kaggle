{"cell_type":{"c855d4b7":"code","3e25e679":"code","9f6a13d0":"code","a223d68c":"code","49ff0025":"code","d4f00857":"code","8a39dc48":"code","7bb7c7c9":"code","54dfdeb1":"code","d2694097":"code","f5b24de0":"code","959d86d3":"code","a04bb4a5":"code","1bcadf87":"code","fa05ee1b":"code","b362c970":"code","5920e837":"code","a79b4932":"code","3c86e3e6":"code","8b9b1ca8":"code","e5154859":"code","789165b4":"code","d2d88d7b":"code","b4f307de":"code","888639c1":"code","7294a89c":"code","390ce118":"code","675031a7":"code","af5b90a1":"code","73979a44":"code","e9c81307":"code","2dbba059":"code","ffaab569":"code","2280a46e":"code","2d4b1eac":"code","4f6ac35d":"code","2d520b96":"code","c1c8069d":"code","3048cf1e":"code","cfc1e38e":"markdown","63cb9a70":"markdown","3bfcbbc2":"markdown","9c88bb02":"markdown","2b1c6d6d":"markdown","d372ec1e":"markdown","83c47e21":"markdown","bfb0f9a2":"markdown","c8372890":"markdown","982f31a9":"markdown","c1ece183":"markdown","c086af88":"markdown","105880ef":"markdown","77c5e7f2":"markdown"},"source":{"c855d4b7":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport warnings\ntoy = True\nfrom kaggle.competitions import twosigmanews\nfrom itertools import chain\nwarnings.filterwarnings(\"ignore\")\n\nenv = twosigmanews.make_env()\n(marketdata, news) = env.get_training_data()\nprint('Done!')","3e25e679":"if toy:\n    marketdata = marketdata.tail(100_000)\n    news = news.tail(300_000)\nelse:\n    marketdata = marketdata.tail(3_000_000)\n    news = news.tail(6_000_000)","9f6a13d0":"print(\"Marketdata (Rows, Columns): \",marketdata.shape,\"News (Rows, Columns): \",news.shape)","a223d68c":"marketdata.info()","49ff0025":"news.info()","d4f00857":"marketdata.head()","8a39dc48":"news.head()","7bb7c7c9":"resumen_marketdata= marketdata.describe()\nresumen_marketdata = resumen_marketdata.transpose()\nresumen_marketdata","54dfdeb1":"resumen_news= news.describe()\nresumen_news = resumen_news.transpose()\nresumen_news","d2694097":"# Load Visualization Libraries\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\nimport plotly.figure_factory as ff\ninit_notebook_mode(connected=True) ","f5b24de0":"# Number of missing in each column\nnulls_marketdata = pd.DataFrame(marketdata.isnull().sum()).rename(columns = {0: 'total'})\nnulls_news = pd.DataFrame(news.isnull().sum()).rename(columns = {0: 'total'})\n\nprint(\"Missing Values. Marketdata: \",nulls_marketdata['total'].sum(),\" News: \",nulls_news['total'].sum())","959d86d3":"nulls_marketdata['percentage'] = nulls_marketdata['total'] \/ len(marketdata)\ncolumns_marketdata = nulls_marketdata[nulls_marketdata['percentage']>0].sort_values('percentage', ascending = False).head(10).reset_index().rename(index=str, columns={\"index\": \"name\"})","a04bb4a5":"labels_marketdata = list(columns_marketdata['name'])\nvalues_marketdata = list(columns_marketdata['percentage'])\n\n\nfig = {'data': \n       [{'type':'pie',\n                 'labels':labels_marketdata,\n                 'domain': {\"x\": [0, 1],\"y\":[0,.9]},\n                 'name': 'Marketdata',\n                 'hoverinfo':'label+percent+name',\n                 'values': values_marketdata\n        }],\n        'layout':\n        {\n          'title':'Missing Values',\n           'annotations': [\n            {\n                'font': {\n                    'size': 18\n                },\n                'showarrow': False,\n                'text': 'Marketdata',\n                'x': 0.5,\n                'y': 1\n            }]\n        }\n      }\n               \n\niplot(fig)","1bcadf87":"marketdata_AAPL = marketdata[marketdata['assetCode']=='AAPL.O']","fa05ee1b":"trace_high = go.Scatter(\n                x=marketdata_AAPL['time'],\n                y=marketdata_AAPL['returnsClosePrevRaw1'],\n                name = \"Open\",\n                line = dict(color = '#17BECF'),\n                opacity = 0.8)\n\ntrace_low = go.Scatter(\n                x=marketdata_AAPL['time'],\n                y=marketdata_AAPL['close'],\n                name = \"Close\",\n                line = dict(color = '#7F7F7F'),\n                opacity = 0.8)\n\ndata = [trace_high,trace_low]\n\nlayout = dict(\n    title = \"Apple Inc.\",\n    xaxis = dict(\n        range = ['2007-02-01','2016-12-30'])\n)\n\nfig = dict(data=data, layout=layout)\niplot(fig)","b362c970":"print(\"Unknown names: \", marketdata[marketdata[\"assetName\"]=='Unknown'].size)","5920e837":"assetCode_Unknown = marketdata[marketdata['assetName'] == 'Unknown'].groupby('assetCode').size().reset_index('assetCode')\nprint(\"Asset Codes without names: \",assetCode_Unknown.shape[0])","a79b4932":"assetCode_Unknown.head()","3c86e3e6":"news[['assetCodes','assetName']].head()","8b9b1ca8":"marketdata['time'].dt.time.describe()","e5154859":"news['time'].dt.time.describe()","789165b4":"news['time'] = (news['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\nmarketdata['time'] = marketdata['time'].dt.floor('1D')","d2d88d7b":"news['assetCodes'] = news['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")    \nassetCodes_expanded = list(chain(*news['assetCodes']))\nassetCodes_index = news.index.repeat( news['assetCodes'].apply(len) )\nassert len(assetCodes_index) == len(assetCodes_expanded)\nassetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n","b4f307de":"assetCodes.head()","888639c1":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","7294a89c":"news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())","390ce118":"def getx(news, marketdata, le=None)\n    news['time'] = (news['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n    marketdata['time'] = marketdata['time'].dt.floor('1D')\n    \n    news_train_df_expanded = pd.merge(assetCodes, news[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n    # Join with train\n    market_train_df = marketdata.join(news_train_df_aggregated, on=['time', 'assetCode'])\n    # Free memory\n    del news_train_df_aggregated\n    \n    # Free memory\n    del news_train_df_expanded\n    \n    x= market_train_df\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    \n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n    \n    def label_encode(series, min_count):\n        vc = series.value_counts()\n        le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n    le_assetCode = label_encode(x['assetCode'], min_count=10)\n    le_assetName = label_encode(x['assetName'], min_count=5)\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n    return (x,le_assetCode,le_assetName)","675031a7":"  # Save universe data for latter use\n    universe = marketdata['universe']\n    time = marketdata['time']","af5b90a1":"n_train = int(x.shape[0] * 0.8)\n\nx_train, y_train = x.iloc[:n_train], y.iloc[:n_train]\nx_valid, y_valid = x.iloc[n_train:], y.iloc[n_train:]","73979a44":"# For valid data, keep only those with universe > 0. This will help calculate the metric\nu_valid = (universe.iloc[n_train:] > 0)\nt_valid = time.iloc[n_train:]\n\nx_valid = x_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\ndel u_valid","e9c81307":"import lightgbm as lgb\n# Creat lgb datasets\ntrain_cols = x.columns.tolist()\ncategorical_cols = [] # ['assetCode', 'assetName', 'dayofweek', 'month']\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(x_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\ndvalid = lgb.Dataset(x_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)","2dbba059":"dvalid.params = {\n    'extra_time': t_valid.factorize()[0]\n}","ffaab569":"lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 127,\n    max_depth = -1,\n#     min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 10,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)\n\ndef sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    \n#    assert len(labels) == len(df_time)\n\n    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    \n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() \/ x_t_sum.std()\n\n    return 'sigma_score', score, True\n\nevals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n              early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\n\n\ndf_result = pd.DataFrame(evals_result['valid'])","2280a46e":"num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(lgb_params)\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')","2d4b1eac":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()","4f6ac35d":"dtrain_full = lgb.Dataset(x, y, feature_name=train_cols, categorical_feature=categorical_cols)\n\nmodel = lgb.train(lgb_params, dtrain, num_boost_round=num_boost_round)","2d520b96":"def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n    x, _ = get_x(market_obs_df, news_obs_df, le)\n    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)","c1c8069d":"days = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n    env.predict(predictions_template_df)\nprint('Done!')","3048cf1e":"env.write_submission_file()","cfc1e38e":"## 1.4 Unknown Assets and Code Assets in News Data","63cb9a70":"On the other hand, we have these two columns in News data: 'assetCodes' and 'assetName'","3bfcbbc2":"# 2. Trainning Model\n## 2.1 Yields","9c88bb02":"Oops! We have many entries with different times for one day in News dataset.","2b1c6d6d":"Several assetCodes are used for the same assetName. Is it possible that some of these codes with unknown names in the Market Dataset, in fact, belong to the same company name?  \nNow, what is the best way to join both dataframe?","d372ec1e":"## 1.3 Time Series [Asset]","83c47e21":"In marketdata we have one entry per day. Whit time 22:00","bfb0f9a2":"## 1.2 Missing values","c8372890":"![](https:\/\/www.reaktor.com\/wp-content\/uploads\/2017\/11\/Data-science-using-visualizationBlog2000x756-1-2800x0-c-default.jpg)","982f31a9":"We set the same time for day in News and Marketdata. ","c1ece183":"Now we need to expand the assetCodes by assetName","c086af88":"# 1. Data exploration\n## 1.1 First steps","105880ef":"# Two Sigma: Using News to Predict Stock Movements\n\nThe data includes a subset of US-listed instruments. The set of included instruments changes daily and is determined based on the amount traded and the availability of information. This means that there may be instruments that enter and leave this subset of data. There may therefore be gaps in the data provided, and this does not necessarily imply that that data does not exist (those rows are likely not included due to the selection criteria).\n\nThe marketdata contains a variety of returns calculated over different timespans. All of the returns in this set of marketdata have these properties:\n\n* Returns are always calculated either open-to-open (from the opening time of one trading day to the open of another) or close-to-close (from the closing time of one trading day to the close of another).\n* Returns are either raw, meaning that the data is not adjusted against any benchmark, or market-residualized (Mktres), meaning that the movement of the market as a whole has been accounted for, leaving only movements inherent to the instrument.\n* Returns can be calculated over any arbitrary interval. Provided here are 1 day and 10 day horizons.\n* Returns are tagged with 'Prev' if they are backwards looking in time, or 'Next' if forwards looking.\n\nThe news data contains information at both the news article level and asset level (in other words, the table is intentionally not normalized).","77c5e7f2":"In the market data set, there are a lot of unnamed assetCodes. If we are thinking of merging this data set with News, we will have to take this into account:"}}