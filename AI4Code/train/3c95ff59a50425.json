{"cell_type":{"0325dda0":"code","df7d6562":"code","3b417943":"code","4492b397":"code","b237ca6f":"code","1bf98e29":"code","77190fa2":"code","03a521b7":"code","f24840a5":"code","fda17614":"code","abe3661b":"code","419f5a6e":"code","6bdab2e0":"code","e6b7da48":"code","7df8707f":"code","d8f4c7d0":"code","93c7a605":"code","b9b2264a":"code","40b012fd":"code","66c63216":"code","44d757a0":"code","f95108d8":"code","a62b3384":"code","af1922b2":"code","14bf3e35":"code","2ecea812":"code","b64b8cfa":"code","d17d478a":"code","efa074cf":"code","c08e2937":"code","530279dd":"code","cc0abb96":"code","ce3492d2":"code","7d670ff0":"code","c739d4a4":"code","276ca27c":"code","00ce929e":"code","1b45bf6c":"code","7f0ada4e":"code","274bccb8":"code","3745ab65":"code","81747f4d":"code","2ae1317b":"code","6baeb4b8":"code","04eacd08":"code","f7db4b3c":"code","ce273b80":"code","978300d9":"code","f24873a1":"code","b158bd0e":"code","a44b6a9b":"code","de0368cf":"code","1cb66dfb":"code","c3d1d7ba":"code","569c9a59":"code","18f86303":"code","472e8e0b":"code","4018aea0":"code","b37b3f26":"code","94cb6a54":"code","a9063ad1":"code","d1700214":"code","666841b8":"code","25a99829":"code","70a08669":"code","d2b6c9c8":"code","0155465c":"code","cc97027d":"code","232798ab":"code","b458049f":"code","e46d066c":"code","2fde756b":"code","658ca94f":"code","b30e8902":"code","bf67b66a":"code","ff052162":"code","c56d6da9":"code","24ad29e6":"code","d049611d":"code","0557a3d5":"code","d728ba5c":"code","8864f528":"code","fd17de6b":"code","643d0f89":"markdown","872b4648":"markdown","7fd6d8bd":"markdown","e115ae98":"markdown","e9190e50":"markdown","cf3c7de0":"markdown","c324bfe5":"markdown","4f27ad90":"markdown","dc0d2deb":"markdown","510cb81f":"markdown","a4eaf0ae":"markdown","f8a47ee2":"markdown","412adc4e":"markdown","9ac2728b":"markdown","85879d28":"markdown","edabe0cc":"markdown","346e1b2e":"markdown","aa6ae1de":"markdown","1dabaf7d":"markdown","bafbf050":"markdown","15f01590":"markdown","ff1551a8":"markdown","59e5a6ed":"markdown","bbeaf2b5":"markdown","d8739b3e":"markdown","97200d3c":"markdown","fbdc9f3f":"markdown","d9b8a537":"markdown","6c9d0e02":"markdown","3be98b72":"markdown","962cea7e":"markdown","accf9b48":"markdown","c94371b4":"markdown","a46c90dd":"markdown","b249b9e1":"markdown","743516ab":"markdown","589d2e49":"markdown","c9a4bc78":"markdown","c24e23a2":"markdown","8617017e":"markdown","4b199a1c":"markdown","f766192e":"markdown","6e94a488":"markdown","ee20d3b5":"markdown","e10c51c8":"markdown","d519a544":"markdown","fd71dc07":"markdown","b7721a5d":"markdown","20cbd68f":"markdown","0617b174":"markdown","4a5029b1":"markdown","cae2dcf6":"markdown","896e7bd8":"markdown","d94ae5ca":"markdown","b982276f":"markdown","c8613f17":"markdown","bbfe630b":"markdown","2dcbfe12":"markdown","e5d08565":"markdown","596d87e5":"markdown","e5b311ce":"markdown","ab48e134":"markdown","9be68957":"markdown","6946486f":"markdown","4f237425":"markdown","bcbfcc36":"markdown","5abfba10":"markdown","4dd8f4e7":"markdown","95229d2e":"markdown","b0acd678":"markdown","287a1611":"markdown","6e7a8ef2":"markdown","6b916553":"markdown","81c48c1a":"markdown","9f5b200f":"markdown","90a11082":"markdown","a21922ef":"markdown","6198c5c3":"markdown","d9ac667d":"markdown","405f2aaf":"markdown"},"source":{"0325dda0":"# read the data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom mpl_toolkits import mplot3d\nfrom matplotlib import cm\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n# Pearson Residual Test\nimport scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\n# Machine Learning Model\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import SGDClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve,auc\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.naive_bayes import MultinomialNB\n\n# import Tableau\nfrom IPython.display import IFrame\n\n\nfile_path1 = \"..\/input\/titanic\/train.csv\"\nfile_path2= \"..\/input\/titanic\/test.csv\"\ndata = pd.read_csv(file_path1)\ntest_data=pd.read_csv(file_path2)\ndata.describe()","df7d6562":"%%HTML\n<div class='tableauPlaceholder' id='viz1592943474846' style='position: relative'>\n<noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ta&#47;Task3_15929207496170&#47;Dashboard2&#47;1_rss.png' style='border: none' \/>\n<\/a><\/noscript><object class='tableauViz'  style='display:none;'>\n<param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/>\n<param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/>\n<param name='name' value='Task3_15929207496170&#47;Dashboard2' \/>\n<param name='tabs' value='no' \/><param name='toolbar' value='yes' \/>\n<param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ta&#47;Task3_15929207496170&#47;Dashboard2&#47;1.png' \/>\n<param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/>\n<param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/>\n<param name='display_count' value='yes' \/><param name='language' value='en' \/>\n<param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1592943474846');                    var vizElement = divElement.getElementsByTagName('object')[0];                    if ( divElement.offsetWidth > 800 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else if ( divElement.offsetWidth > 500 ) { vizElement.style.width='1000px';vizElement.style.height='827px';} else { vizElement.style.width='100%';vizElement.style.height='2127px';}                     var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","3b417943":"# show the first five line of data\ndata.head()","4492b397":"# show the data struture and type\ndata.info()","b237ca6f":"test_data.info()","1bf98e29":"data.Pclass.value_counts()\n#s1=data.groupby('Pclass').apply(lambda df: df.loc[df.Survived==0].Survived.value_counts())\n#s2=data.groupby('Pclass').Survived.count()\ns1=data.groupby('Pclass').apply(lambda df: df.Survived.value_counts()\/len(df)) \ns2=data.groupby('Pclass').apply(lambda df: df.Survived.value_counts()) \npd.concat([s2,s1],axis=1,keys=['Survived\/Death count','Survived\/Death Rate'])","77190fa2":"data.Sex.value_counts()","03a521b7":"data.Cabin.value_counts()\ntest_data.Cabin.value_counts()","f24840a5":"data.Cabin.str[0].value_counts()\ntest_data.Cabin.str[0].value_counts()","fda17614":"s3=pd.Series(data.Embarked.value_counts())\n\npd.concat([s3,s3\/889],axis=1,keys=['count(Embarked)','portion'])","abe3661b":"data.PassengerId.value_counts()","419f5a6e":"copy1=data.copy()\ncopy2=test_data.copy()","6bdab2e0":"label_encoder=LabelEncoder()\ncopy1['new_Sex']=label_encoder.fit_transform(copy1['Sex'])\ncopy2['new_Sex']=label_encoder.transform(copy2['Sex'])","e6b7da48":"copy1['new_Cabin']=data['Cabin'].str[0]\ns1=pd.Series(copy1.new_Cabin.value_counts())\ns2=s1\/91\npd.concat([s1,s2],axis=1,keys=['count(new_cabin)','portion'])\n# Copy the test dataset and add a new column","7df8707f":"\ncopy2['new_Cabin']=test_data['Cabin'].str[0]\ns3=pd.Series(copy2.new_Cabin.value_counts())\npd.concat([s3,s3\/76],axis=1,keys=['count(new_Cabin)','portion'])","d8f4c7d0":"copy1['new_Cabin']=copy1['new_Cabin'].fillna(\"Z\")\ncopy2['new_Cabin']=copy2.new_Cabin.fillna(\"Z\")\n\nlabel_encoder=LabelEncoder()\ncopy1['new_Cabin']=label_encoder.fit_transform(copy1['new_Cabin'])\ncopy2['new_Cabin']=label_encoder.transform(copy2['new_Cabin'])\n","93c7a605":"copy1['new_Embarked'] = copy1['Embarked'].fillna(\"N\")\n\nlabel_encoder=LabelEncoder()\ncopy1['new_Embarked']=label_encoder.fit_transform(copy1['new_Embarked'])\n\n","b9b2264a":"sns.pairplot(data,hue='Pclass', diag_kws={'bw':0.1}, palette=\"husl\")","40b012fd":"# the correlation matrix\nfeatures=['PassengerId','Survived','Pclass','new_Sex','Age','SibSp','Parch','Fare','new_Cabin','new_Embarked']\ncorr=copy1[features].corr() \n# mask the upper triangle\nsns.set(style=\"white\")\nplt.figure(figsize=(11,7))\nmask=np.triu(np.ones_like(corr,dtype=np.bool))\n# colour\ncmap=sns.diverging_palette(240,10,n=9)\n# annot to display the value\nsns.heatmap(corr,annot=True,mask=mask,cmap='RdYlBu',linewidths=0.6)","66c63216":"s1=copy1.loc[copy1.Sex=='female'].Age.describe()\ns2=copy1.loc[copy1.Sex=='male'].Age.describe()\npd.concat([s1,s2],axis=1,keys=['Age|Sex=female','Age|Sex=male'])","44d757a0":"s1=copy1.groupby('Sex').apply(lambda df: df.Survived.value_counts())\ns2=copy1.groupby('Sex').apply(lambda df: df.Survived.value_counts()\/len(df))\ns3=pd.concat([s1,s2],axis=1,keys=['count(Survival\/Death)','Survival\/Death rate'])\ns3","f95108d8":"#fig, ax = plt.subplots(figsize=(12,5),ncols=2)\n#d1=copy1.loc[copy1.Sex=='male']\n#d2=copy1.loc[copy1.Sex=='female'].Survived.value_counts()\n\n#d1=copy1.loc[copy1.Sex=='female']\n#f=['Survived']\n#X=d1[f].Survived.value_counts()\n\nsns.set(style=\"whitegrid\")\nax1=sns.barplot(y=s3.index, x =s3['count(Survival\/Death)'],linewidth=2.5,facecolor=(1,1,1,0),errcolor=\"1\", edgecolor=\".1\")\nylabels = ['(female, survived)','(female, Dead)', '(male, Dead)', '(male, survived)']\nax1.set_yticklabels(ylabels)\ni=0\nlist1=s3['Survival\/Death rate']\nfor p in ax1.patches:\n    label = list1[i]*100\n    i=i+1\n    plt.text(-36+p.get_width(), p.get_y()+0.55*p.get_height(),\n             str('{:1.2f}'.format(label))+'%',\n             ha='center', va='center')\n","a62b3384":"sns.set(style=\"darkgrid\")\nf, (ax1,ax2) = plt.subplots(figsize=(18,7),ncols=2)\ns1= copy1.loc[copy1.Sex=='female']\nax1 = sns.distplot(a=s1.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='azure',label=\"Age\",ax=ax1)\ns2= s1.loc[(s1.Survived==1)]\nax1 = sns.distplot(a=s2.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='cyan',ax=ax1)\n#ax1.set_title(\"Histogram of Survival, female\")\nax1.legend(['total count', 'Survived count'])\nax1.set_title(\"Histogram of Survival, female\")\n\n\ns3= copy1.loc[copy1.Sex=='male']\nax2 = sns.distplot(a=s3.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='lavender',label=\"Age\",ax=ax2)\ns4= s1.loc[(s1.Survived==1)]\nax2 = sns.distplot(a=s4.Age,bins=34, kde=False, \n                  hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0},color='orchid',ax=ax2)\n#ax1.set_title(\"Histogram of Survival, female\")\nax2.legend(['total count', 'Survived count'])\nax2.set_title(\"Histogram of Survival, male\")","af1922b2":"copy3=copy1.loc[copy1.Age.notnull()]","14bf3e35":"X=[0.0,1.0,2.0,3.0,4.0,5.0,6.0]\nY=[0,1,2,3,4,5]\ndef f(x,y):\n    a=copy3.loc[(copy3.Parch==x)&(copy3.SibSp==y)].Age.mean()\n    return a\nX,Y=np.meshgrid(X,Y)\n#Z=f(X,Y)\nZ=np.zeros((6, 7))\nfor i in range(6):\n    for j in range(7):\n        Z[i][j]=f(X[i][j],Y[i][j])\na= copy3.loc[(copy3.SibSp>=4)].Age.mean()\nfor i in range(2):\n    for j in range(7):\n        Z[i+4][j]=a\n\nZ[0][6]=Z[0][5]\nZ[2][4]=Z[2][5]=Z[2][6]=Z[2][3]\nZ[3][3]=Z[3][4]=Z[3][5]=Z[3][6]=Z[3][2]\n","2ecea812":"#Z = np.cos(X ** 2 + Y ** 2)\nfig= plt.figure(figsize=(10,6))\n#ax = plt.axes(projection='3d')\nax = fig.add_subplot(111, projection='3d')\nsurf=ax.plot_surface(X, Y, Z,cmap='viridis', edgecolor='none')\nfig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\nax.set_title('Average Age for SibSp and Parch')\nax.set_xlim(0, 6);\nax.set_ylim(5, 0);\nax.set_xlabel('Parch')\nax.set_ylabel('SibSp')\nax.set_zlabel('Age.mean()');\n#plt.show()\nplt.show()","b64b8cfa":"\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot()\n\ncset = ax.contourf(X, Y, Z)\nplt.colorbar(cset)\n\nax.set_title('contour plot');\nplt.show()","d17d478a":"copy3=copy1[['Age','Fare','Survived','SibSp','Parch']]\ncopy3['family_size']=copy3['SibSp']+copy3['Parch']\ndef f (p):\n    if p<= 18: return '0~18'\n    if p>18 and p<35: return '18~35'\n    if p>=35 and p<55: return '35~55'\n    if p>=55: return '55~80'\ncopy3['age_group']=copy3.Age.map(lambda p: f(p))","efa074cf":"\nsns.set(style=\"darkgrid\")\n#sns.set(style=\"darkgrid\")\nsns.catplot(x='family_size',col='age_group', col_wrap=2,data=copy3,hue=\"Survived\", kind=\"count\",height=8, aspect=.8,palette=['aqua','pink'])\n#sns.catplot(x='Age',y='SibSp',data=copy3,height=8, aspect=.7)","c08e2937":"fig = plt.figure()\nfig, ax = plt.subplots(figsize=(11,6))\nax = fig.add_subplot(111, projection='3d')\nX=copy1['SibSp']+copy1['Parch']\nY=copy1.Age\nZ=copy1.Fare\ng=ax.scatter(X, Y, Z,  c= copy1.Survived,marker='o',cmap='cool')\nplt.colorbar(g)\nax.set_zlim(0, 250);\nax.set_xlabel('Family size')\nax.set_ylabel('Age')\nax.set_zlabel('Fare');\nplt.show()","530279dd":"s1=copy1.loc[copy1.Pclass==1].Fare.describe()\ns2=copy1.loc[copy1.Pclass==2].Fare.describe()\ns3=copy1.loc[copy1.Pclass==3].Fare.describe()\npd.concat([s1,s2,s3],axis=1,keys=['Fare|Pclass=1','Fare|Pclass=2','Fare|Pclass=3'])","cc0abb96":"sns.set(style=\"darkgrid\")\nfig, ax1 = plt.subplots(figsize=(16,5))\nax=sns.kdeplot(data=copy1.loc[copy1.Pclass==1]['Fare'],shade=True,color='red')\nax.set_title(\"Fare distribution|Pclass\")\nax=sns.kdeplot(data=copy1.loc[copy1.Pclass==2]['Fare'],shade=True,color='blue')\nax=sns.kdeplot(data=copy1.loc[copy1.Pclass==3]['Fare'],shade=True,color='purple')\nax.legend([\"Fare|Pclass=1\",\"Fare|Pclass=2\",\"Fare|Pclass=3\"])","ce3492d2":"fig, ax1 = plt.subplots(figsize=(12,15))\nsns.swarmplot(x=copy1.Survived,y=copy1.Fare,hue=copy1.Pclass, palette='cool')","7d670ff0":"sns.catplot(x=\"Survived\",y=\"Fare\",hue=\"Pclass\",data=copy1,kind='violin',height=8, aspect=2, palette=['violet','turquoise','tomato'])","c739d4a4":"sns.set(style=\"darkgrid\")\nsns.catplot(x=\"Survived\",y=\"Fare\",col=\"Pclass\",data=copy1 ,hue=\"Sex\",height=8, aspect=.7, palette=['violet','turquoise'])","276ca27c":"g = sns.FacetGrid(copy1,height=5, col=\"Pclass\", row=\"Sex\", margin_titles=True, hue = \"Survived\" )\ng = g.map(sns.distplot, \"Age\",kde=False,bins=15,hist_kws={\"rwidth\":1,'edgecolor':'black', 'alpha':1.0}).add_legend();","00ce929e":"s0=copy1.Embarked.value_counts()\ns1=copy1.loc[copy1.Pclass==1].Embarked.value_counts()\ns2=copy1.loc[copy1.Pclass==2].Embarked.value_counts()\ns3=copy1.loc[copy1.Pclass==3].Embarked.value_counts()\npd.concat([s0,s1\/s0*100,s2\/s0*100,s3\/s0*100],axis=1,keys=['Total','Embarked|Pclass=1 (%)','Embarked|Pclass=2 (%)','Embarked|Pclass=3 (%)'])","1b45bf6c":"plt.figure(figsize=(12,12))\nsns.boxplot(x=copy1.Embarked,y=copy1.Fare,hue=copy1.Pclass,palette='cool')","7f0ada4e":"plt.figure(figsize=(12,12))\nsns.swarmplot(x=copy1.Embarked,hue=copy1.Survived,y=copy1.Fare,palette='spring')","274bccb8":"sns.set(style=\"darkgrid\")\nsns.catplot(x=\"Survived\",col=\"Pclass\",data=copy1 ,hue=\"Embarked\",kind='count',height=8, aspect=.7, palette=['gold','orangered','brown'])","3745ab65":"copy1.loc[copy1.Embarked.isnull()]","81747f4d":"copy1.loc[(copy1.Pclass==1) & (copy1.Sex=='female')].Embarked.value_counts()","2ae1317b":"s1=copy1.loc[(copy1.Pclass==1) & (copy1.Sex=='female')]\ns2=s1.loc[s1.Embarked=='S'].Fare.describe()\ns3=s1.loc[s1.Embarked=='C'].Fare.describe()\npd.concat([s2,s3],axis=1,keys=['Fare|Embarked=S', 'Fare|Embarked=C'])","6baeb4b8":"copy1['Embarked']=copy1.Embarked.fillna('S')","04eacd08":"label_encoder=LabelEncoder()\ncopy1['new_Embarked']=label_encoder.fit_transform(copy1['Embarked'])\ncopy2['new_Embarked']=label_encoder.transform(copy2['Embarked'])","f7db4b3c":"# make a temporary column to store the first character\nprint('Null Pertcentage of Cabin in tranining dataset : ',str(len(copy1.loc[copy1.Cabin.isnull()])\/len(copy1.Cabin)*100),'%')\nprint('Null Pertcentage of Cabin in testing dataset : ',str(len(copy2.loc[copy1.Cabin.isnull()])\/len(copy2.Cabin)*100),'%')","ce273b80":"copy1=copy1.drop(['Cabin','new_Cabin'],axis=1)\ncopy2=copy2.drop(['Cabin','new_Cabin'],axis=1)","978300d9":"\n\n# training dataset\nfeature=['Pclass','SibSp','Parch']\ncopy3=copy1.loc[copy1.Age.notnull()]\nx_train=copy3[feature]\n#convert y_train to integer\ny_train=copy3.Age.astype(int)\n\n# prediction\ncopy4=copy1.loc[copy1.Age.isnull()]\nx_test=copy4[feature]\n\nlog=LogisticRegression()\nlog.fit(x_train,y_train)\ny_pred = log.predict(x_test)\n","f24873a1":"# assign the new values to Age column\ncopy1.loc[copy1.Age.isnull(), \"Age\"] = y_pred","b158bd0e":"# Testing datset\nx_test=copy2.loc[copy2.Age.isnull()][feature]\ny_pred=log.predict(x_test)\ncopy2.loc[copy2.Age.isnull(), \"Age\"] = y_pred","a44b6a9b":"copy2.loc[copy2.Fare.isnull()]","de0368cf":"# training dataset\nfeature=['Pclass','SibSp','Parch','new_Sex','new_Embarked']\ncopy3=copy2.loc[copy2.Fare.notnull()]\nx_train=copy3[feature]\n#convert y_train to integer\ny_train=copy3.Fare\n\n# prediction\ncopy4=copy2.loc[copy2.Fare.isnull()]\nx_test=copy4[feature]\n\nforest_model=RandomForestRegressor(random_state=1)\nforest_model.fit(x_train,y_train)\ny_pred = forest_model.predict(x_test)\n\n","1cb66dfb":"copy2.loc[copy2.Fare.isnull(), \"Fare\"] = y_pred","c3d1d7ba":"#[i.split('.')[1] for i in data.Name]\n#for i in range(len(data.Name)):\n    #title = data.Name[i].split('.')[0]\n    #title = title.split(',')[1]\ncopy1['Title']=[n.split('.')[0] for n in copy1.Name]\ncopy1['Title'] = [t.split(',')[1] for t in copy1.Title]\n\ncopy2['Title']=[n.split('.')[0] for n in copy2.Name]\ncopy2['Title'] = [t.split(',')[1] for t in copy2.Title]\n\n","569c9a59":"pd.concat([copy1,copy2]).Title.value_counts()\n#copy1.Title.value_counts()","18f86303":"def transform(i):\n    if 'Mr' in i: return 'Mr'\n    if 'Mrs'in i: return 'Mrs'\n    if 'Miss'in i: return 'Miss'\n    if 'Master' in i: return 'Master'\n    else: return 'NA'\n    \ncopy1['Title']=[transform(i) for i in copy1.Title]\ncopy2['Title']=[transform(i) for i in copy2.Title]\nlabel_encoder=LabelEncoder()\nlabel_encoder.fit_transform(pd.concat([copy1,copy2])['Title'])\ncopy1['new_Title']=label_encoder.transform(copy1['Title'])\ncopy2['new_Title']=label_encoder.transform(copy2['Title'])","472e8e0b":"copy1['Ticket_length']=[len(i) for i in copy1.Ticket]\ncopy2['Ticket_length']=[len(i) for i in copy2.Ticket]","4018aea0":"copy1['Famsize']=copy1['SibSp']+copy1['Parch']\ncopy2['Famsize']=copy2['SibSp']+copy2['Parch']","b37b3f26":"# the correlation matrix\ncorr=copy1.corr() \n# mask the upper triangle\nsns.set(style=\"white\")\nplt.figure(figsize=(11,7))\nmask=np.triu(np.ones_like(corr,dtype=np.bool))\n# colour\ncmap=sns.diverging_palette(240,10,n=9)\n# annot to display the value\nsns.heatmap(corr,annot=True,mask=mask,cmap='jet',linewidths=0.6)","94cb6a54":"# scipy.stats to find the p-value and of each factor\n# compare p-value with alpha=0.05 to find out the significance level\n# select the columns for test\n\nfeatures= ['PassengerId', 'Pclass','Age','SibSp','Parch','Fare','Embarked','new_Sex','new_Embarked','new_Title','Ticket_length','Famsize']\n# drop the missing value row to have a accurate estimation?\nfor feature in features:\n    table = pd.crosstab(copy1[feature], copy1['Survived'], margins=False)\n    stat, p, dof, expected = stats.chi2_contingency(table)\n    print(\"The p-value of\", feature,\"is: \",p)\n    \n    ","a9063ad1":"copy1.info()","d1700214":"\n\ncopy1 = copy1.drop(['PassengerId','Name','Sex','Embarked','Ticket','Title'],axis=1)","666841b8":"copy2=copy2.drop(['Name','Sex','Embarked','Ticket','Title'],axis=1)","25a99829":"def age(i):\n    if i<=12: return 0\n    elif i<=26: return 1\n    elif i<=40: return 2\n    elif i<=55: return 3\n    else : return 4\ncopy1['Age']=[age(i) for i in copy1.Age]\ndef alone(i):\n    if i > 0:return 1\n    else: return 0\ncopy1['isAlone']=[alone(i) for i in copy1.Famsize]\n\ndef fare(i):\n    # mostly class 3 and some of them are class 2\n    if i<=13: return 1\n    # mostly class 2 and some of them are class 1\n    elif i<=26: return 2\n    # mostly class 1 and some of them are class 2\n    elif i<=74: return 3\n    # else absolutely class 1\n    else: return 4\ncopy1['Fare']=[fare(i) for i in copy1.Fare]\n\ndef com(i):\n    if i.new_Sex==0 and i.Pclass==1: return 0\n    if i.new_Sex==0 and i.Pclass==2: return 1\n    if i.new_Sex==0 and i.Pclass==3: return 2\n    if i.new_Sex==1 and i.Pclass==1: return 3\n    if i.new_Sex==1 and i.Pclass==2: return 4\n    if i.new_Sex==1 and i.Pclass==3: return 5\ncopy1['SP']=copy1.apply(com,axis='columns')\ncopy1['SF']=copy1['new_Sex']*copy1['Fare']\n\ndef com2(i):\n    if i.new_Sex==0 and i.isAlone==0: return 3\n    if i.new_Sex==0 and i.isAlone==1: return 2\n    if i.new_Sex==1 and i.isAlone==0: return 1\n    if i.new_Sex==1 and i.isAlone==1: return 0\n    \ncopy1['sex_alone']=copy1.apply(com2,axis='columns')\n\n\ncopy2['Age']=[age(i) for i in copy2.Age]\ncopy2['isAlone']=[alone(i) for i in copy2.Famsize]\ncopy2['Fare']=[fare(i) for i in copy2.Fare]\ncopy2['SP']=copy2.apply(com,axis='columns')\ncopy2['SF']=copy2['new_Sex']*copy2['Fare']\ncopy2['sex_alone']=copy2.apply(com2,axis='columns')\n\n\ncopy1=copy1.drop(['SibSp','Parch','Famsize','Age'],axis=1)\ncopy2=copy2.drop(['SibSp','Parch','Famsize','Age'],axis=1)\n\ncopy1=copy1.drop(['Ticket_length'],axis=1)\ncopy2=copy2.drop(['Ticket_length'],axis=1)\n","70a08669":"# the correlation matrix\ncorr=copy1.corr() \n# mask the upper triangle\nsns.set(style=\"white\")\nplt.figure(figsize=(11,7))\nmask=np.triu(np.ones_like(corr,dtype=np.bool))\n# colour\ncmap=sns.diverging_palette(240,10,n=9)\n# annot to display the value\nsns.heatmap(corr,annot=True,mask=mask,cmap='jet',linewidths=0.6)","d2b6c9c8":"from sklearn.preprocessing import OneHotEncoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(copy1[['new_Embarked','new_Title','SP','SF','sex_alone']]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(copy2[['new_Embarked','new_Title','SP','SF','sex_alone']]))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = copy1.index\nOH_cols_valid.index = copy2.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = copy1.drop(['new_Embarked','new_Title','SP','SF','sex_alone'], axis=1)\nnum_X_valid = copy2.drop(['new_Embarked','new_Title','SP','SF','sex_alone'], axis=1)\n\n# Add one-hot encoded columns to numerical features\ncopy1 = pd.concat([num_X_train, OH_cols_train], axis=1)\ncopy2 = pd.concat([num_X_valid, OH_cols_valid], axis=1)","0155465c":"\ny=copy1['Survived']\ncopy3=copy1.drop(['Survived'],axis=1)\nX_train,X_valid,y_train,y_valid = train_test_split(copy3,y,train_size=0.8,test_size=0.2,random_state=0)\n\nmae=[]\naccuracy=[]\nprediction=[]","cc97027d":"l1=LogisticRegression(max_iter=150)\nl1.fit(X_train,y_train)\ny_pred=l1.predict(X_valid)\nprediction.append(y_pred)\nmae.append(mean_absolute_error(y_valid, y_pred))\naccuracy.append(accuracy_score(y_valid, y_pred))","232798ab":"min=100\na=0\ny=0\nl2=RandomForestClassifier(n_estimators=40,random_state=1)\nprint('The mean absolute error of')\nfor i in [60,80,100,120,140,160,180]:\n    l=RandomForestClassifier(n_estimators=i,random_state=1)\n    l.fit(X_train,y_train)\n    y_pred=l.predict(X_valid)\n    if mean_absolute_error(y_valid, y_pred)<min:\n        min= mean_absolute_error(y_valid, y_pred)\n        a=accuracy_score(y_valid, y_pred)\n        y=y_pred\n        l2=l\n    print('n_estimators =',i,': ', mean_absolute_error(y_valid, y_pred))\nprediction.append(y)    \nmae.append(min)\naccuracy.append(a)","b458049f":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint\n\ndef best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\ny=copy1['Survived']\n\nknn_model = KNeighborsClassifier()\n\nparams = {\n    \"n_neighbors\": randint(5, 30),\n    \"p\":randint(1, 4),\n    \"leaf_size\":randint(20, 100),\n    \"weights\":['uniform','distance']\n    \n}\n\nsearch = RandomizedSearchCV(knn_model, param_distributions=params, random_state=42, n_iter=200, cv=7, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train, y_train)\n\nbest_scores(search.cv_results_, 1)\n\n# After finding the best parameter\n# Build the model to predict X_valid and check its performance\nl3=KNeighborsClassifier(leaf_size=87,n_neighbors=10,p=1,weights='distance')\nl3.fit(X_train,y_train)\ny_pred=l3.predict(X_valid)\nmae.append(mean_absolute_error(y_valid, y_pred))\naccuracy.append(accuracy_score(y_valid, y_pred))\nprediction.append(y_pred)","e46d066c":"\n\"\"\"min=100\nl3=KNeighborsClassifier()\nprint('The mean absolute error of')\nfor i in [5,10,15,20,25,30]:\n    l=KNeighborsClassifier(n_neighbors=i)\n    l.fit(X_train,y_train)\n    y_pred=l.predict(X_valid)\n    if mean_absolute_error(y_valid, y_pred)<min:\n        min=mean_absolute_error(y_valid, y_pred)\n        a=accuracy_score(y_valid, y_pred)\n        y=y_pred\n        l3=l\n    print('n_neighbors =',i,': ',mean_absolute_error(y_valid, y_pred))\n\nprediction.append(y)\nmae.append(min)\naccuracy.append(a)\"\"\"","2fde756b":"\n\nNB=[GaussianNB(),BernoulliNB(),CategoricalNB(alpha=2),MultinomialNB()]\nname=['Gaussian NB','Bernoulli NB','Categorical NB','Multinomial NB']\nprint('The mean absolute error of')\nmin=100\nl4=NB[0]\nfor i in range(len(NB)):\n    model=NB[i]\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_valid)\n    if mean_absolute_error(y_valid, y_pred)<min:\n        l4=model\n        a=accuracy_score(y_valid, y_pred)\n        y=y_pred\n        min=mean_absolute_error(y_valid, y_pred)\n    print(name[i],': ',mean_absolute_error(y_valid, y_pred))\nprediction.append(y)\nmae.append(min)\naccuracy.append(a)","658ca94f":"l5 = SGDClassifier(alpha=0.0001,epsilon=0.06, loss='log',penalty=\"l2\", max_iter=3,random_state=0)\nl5.fit(X_train, y_train)\ny_pred=l5.predict(X_valid)\nprediction.append(y_pred)\nmae.append(mean_absolute_error(y_valid, y_pred))\naccuracy.append(accuracy_score(y_valid, y_pred))\n#print(mean_absolute_error(y_valid, y_pred))","b30e8902":"\ndef best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\ny=copy1['Survived']\n\nxgb_model = XGBClassifier(objective='binary:logistic',criterion='mae',eval_metric=\"auc\")\n\nparams = {\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), \n    \"max_depth\": randint(2, 10), \n    \"n_estimators\": randint(100, 150),\n    \"subsample\": uniform(0.6, 0.4),\n    \"eta\":uniform(0.01, 0.3),\n    \"max_leaf_nodes\":randint(30, 500),\n    \"colsample_bytree\":uniform(0,1)\n}\n\nsearch = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=7, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train, y_train)\n\nbest_scores(search.cv_results_, 1)\n\nl6=XGBClassifier(colsample_bytree=0.157,eta=0.058,gamma=0.167,learning_rate=0.1016,max_depth=6,max_leaf_nodes=199,n_estimators=135,subsample=0.847,objective='binary:logistic',criterion='mae',eval_metric=\"auc\")\nl6.fit(X_train,y_train)\ny_pred=l6.predict(X_valid)\n\nprediction.append(y_pred)\nmae.append(mean_absolute_error(y_valid, y_pred))\naccuracy.append(accuracy_score(y_valid, y_pred))\n","bf67b66a":"\"\"\"l6=XGBClassifier()\nmin=100\nprint('The mean absolute error of')\nfor i in [1,2,3,4,5,6]:\n    l=XGBClassifier(max_depth=i)\n    l.fit(X_train,y_train)\n    y_pred=l.predict(X_valid)\n    if mean_absolute_error(y_valid, y_pred)<min:\n        min=mean_absolute_error(y_valid, y_pred)\n        a=accuracy_score(y_valid, y_pred)\n        y=y_pred\n        l6=l\n    print('max_depth=',i,': ',mean_absolute_error(y_valid, y_pred) )\nprediction.append(y_pred)\nmae.append(min)\naccuracy.append(a)\"\"\"\n    \n  ","ff052162":"\n\"\"\"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\n\ny=copy1['Survived']\n# evaluate the model\nl7 = LGBMClassifier()\n#n_scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=1)\nl7.fit(X_train,y_train)\ny_pred=l7.predict(X_valid)\nmae.append(mean_absolute_error(y_valid, y_pred))\naccuracy.append(accuracy_score(y_valid, y_pred))\"\"\"\n","c56d6da9":"from lightgbm import LGBMClassifier\ndef best_scores(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\ny=copy1['Survived']\n\nlgb_model = LGBMClassifier()\n\nparams = {\n    \"learning_rate\": uniform(0.03, 0.3), \n    \"max_depth\": randint(2, 6), \n    \"n_estimators\": randint(100, 150),\n    \"subsample\": uniform(0.6, 0.4),\n    \"num_leaves\":randint(20,40),\n    \"n_splits\":randint(5,30)\n}\n\nsearch = RandomizedSearchCV(lgb_model, param_distributions=params, random_state=42, n_iter=200, cv=7, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train, y_train)\n\nbest_scores(search.cv_results_, 1)\nl7= LGBMClassifier(learning_rate=0.116,max_depth=2,n_estimators=110,n_splits=8,num_leaves=32,subsample=0.813)\nl7.fit(X_train,y_train)\ny_pred=l7.predict(X_valid)\n\nprediction.append(y_pred)\nmae.append(mean_absolute_error(y_valid, y_pred))\naccuracy.append(accuracy_score(y_valid, y_pred))\n\n","24ad29e6":"\ns1=pd.Series(mae,index=['Logistic_Regression', 'Random_Forest_Classifier','K_Nearest_Neighbors_Classifier','Categorical_NB','SGD_Classifier','XGB_Classifier','LGBMclassfier'])\ns2=pd.Series(accuracy,index=['Logistic_Regression', 'Random_Forest_Classifier','K_Nearest_Neighbors_Classifier','Categorical_NB','SGD_Classifier','XGB_Classifier','LGBMclassfier'])\npd.concat([s1,s2],axis=1,keys=['Mean Absolute Error','Accuracy Score'])","d049611d":"s1=pd.Series(['a','c'], index=['Y=1','Y=0'])\ns2=pd.Series(['b','d'], index=['Y=1','Y=0'])\npd.concat([s1,s2],axis=1,keys=['$\\hat{Y}$=1', '$\\hat{Y}$=0'])","0557a3d5":"sns.set(style=\"darkgrid\")\nfpr = dict()\ntpr = dict()\nthresholds=dict()\nroc_auc = dict()\n\nfpr[0], tpr[0], thresholds[0] = roc_curve(y_valid, l1.predict_proba(X_valid)[:,1])\nfpr[1], tpr[1], thresholds[1] = roc_curve(y_valid, l2.predict_proba(X_valid)[:,1])\nfpr[2], tpr[2], thresholds[2] = roc_curve(y_valid, l3.predict_proba(X_valid)[:,1])\nfpr[3], tpr[3], thresholds[3] = roc_curve(y_valid, l4.predict_proba(X_valid)[:,1])\nfpr[4], tpr[4], thresholds[4] = roc_curve(y_valid, l5.predict_proba(X_valid)[:,1])\nfpr[5], tpr[5], thresholds[5] = roc_curve(y_valid, l6.predict_proba(X_valid)[:,1])\nfpr[6], tpr[6], thresholds[6] = roc_curve(y_valid, l7.predict_proba(X_valid)[:,1])\n#roc_auc[0] = auc(fpr[0], tpr[0])\nfor i in range(7):\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nplt.figure(figsize=(10,10))\ncolors = ['aqua', 'darkorange', 'cornflowerblue','deeppink','chartreuse','yellow','green']\nplt.plot(fpr[0], tpr[0], label='Logistic Regression (area = %0.2f)' % roc_auc[0],color=colors[0])\nplt.plot(fpr[1], tpr[1], label='Random Forest Classifier (area = %0.2f)' % roc_auc[1],color=colors[1])\nplt.plot(fpr[2], tpr[2], label='K Nearest Neighbors Classifier (area = %0.2f)' % roc_auc[2],color=colors[2])\nplt.plot(fpr[3], tpr[3], label='Categorical NB (area = %0.2f)' % roc_auc[3],color=colors[3])\nplt.plot(fpr[4], tpr[4], label='SGD Classifier (area = %0.2f)' % roc_auc[4],color=colors[4])\nplt.plot(fpr[5], tpr[5], label='XGB Classifier (area = %0.2f)' % roc_auc[5],color=colors[5])\nplt.plot(fpr[6], tpr[6], label='LGBM Classifier (area = %0.2f)' % roc_auc[6],color=colors[6])\nplt.plot([0, 1], [0, 1], linestyle='--',label='Base',color='black')\nplt.legend()\nplt.show()","d728ba5c":"file_path3 = \"..\/input\/titanic\/gender_submission.csv\"\nsample = pd.read_csv(file_path3)\nsample.info()","8864f528":"\ns1=copy2['PassengerId']\ncopy2=copy2.drop(['PassengerId'],axis=1)\nknn_pred=l3.predict(copy2)\nxgb_pred=l6.predict(copy2)\nrf_pred=l2.predict(copy2)\nsum_pred=knn_pred+xgb_pred+rf_pred\n\ndef mean(i):\n    if i\/3>0.5: return 1\n    else: return 0\ncopy2['Survived']=[mean(i) for i in sum_pred]\ns2=pd.concat([s1,copy2['Survived']],axis=1)","fd17de6b":"submission=pd.concat([s1,copy2['Survived']],axis=1)\nsubmission.to_csv(\"titanic_submission8.csv\", index=False)","643d0f89":"<a id=\"9\"><\/a> \n## 3.1 Features Generation\n### Name\n* The name of passengers also contain title\n* Extract the title from name and treat it as categorical variable","872b4648":"* Convert 'Title' column into category variables\n* There are 18 different kinds of titles in dataset and train dataset only contains 17 of them\n* Fit the Label-Encoding on concatation of train dataset and test datset","7fd6d8bd":"* There's only 2 missing values in Embarked\n* They share the same Sex,Pclass and Fare","e115ae98":"* Apply the pairs plot","e9190e50":"* Refill the missing values for new_Embarked in both training and testing dataset","cf3c7de0":"### Cabin\n* Oragnize the Cabin data by extracting their first character (A, B, C, D, E, F, G, T)\n* Treat them as categorical variable","c324bfe5":"* Demonstrate all sorts of title:\n* However, some of the feature contains only 1 value that will cause problem: For example, there's only 4 'Col' in the whole dataset. If all the 'col' in training set are dead then the estimating 'col' survival rate will be 0, the prediction in testing set will absolutely be dead. This kind of prediction is probably inaccurate.","4f27ad90":"### Sex, Pclass, Fare and Survived","dc0d2deb":" <a id=\"1\"><\/a> \n # 1. Import and Read Data","510cb81f":"### Embarked\n* Embarked also contains missing value\n* Regard all the missing value as Embarked='N'\n* Transfer Embarked into integer by applying label encoder","a4eaf0ae":"### Pclass,Age,Sex vs Survived","f8a47ee2":"* The correlation Heatmap shows that Gender is highly correlated to 'Survived'\n* Calculate the survival rate for men and women","412adc4e":"### Ticket\n* Create a feature Ticket's string length","9ac2728b":"* Age variable can be dropped as the correlation value is small","85879d28":"### Pclass, Fare vs. Survived\n* First, consider Pclass and Fare which are negatively correlated\n* Fare distribution given a certain Pclass","edabe0cc":"-------\n<a id=\"4\"><\/a>\n# 2. Data Visualization and Missing Values\n<a id=\"5\"><\/a> \n## 2.1 Categorical Encoding\nApply the categorical encoding method to the categorical variable and visualize their relation\n","346e1b2e":"* Age and Family size vs Survive\n* Divide all the people into several Age group: 0~18, 18~35, 35~55, 55~80","aa6ae1de":"-----\n<a id=\"11\"><\/a> \n# 4.Modelling\nApply several prediction model and find the best of them","1dabaf7d":"* **Cabin**: \n\nThe cabin number is a character followed by a number and there are 147 different cabin number in train dataset and 76 different numbe in test dataset. Now we group these Cabin number by their first character.\n\n\n","bafbf050":"----\n<a id=\"15\"><\/a> \n## 2.2 Overview of Relation\nVisualize the relation between feature by correlation Heatmap and pairs plot","15f01590":"\n* Acccoring to the above distribution plot and table, we can roughly say that Fare|Pclass=1> Fare|Pclass=2 > Fare|Pclass=3 on average.\n* The Fare|Pclass=1 also has a larger standard error than any other Pclass groups\n* It implies that as Pclass get upper, the ticket price will be more expensive on average","ff1551a8":"### Fare\n* There's a missing entry in Fare column of testing dataset\n* Fare is highly correlated to Pclass, new_Embarked,new_Sex, SibSP and Parch\n* Apply the random forest regressor to make a prediction","59e5a6ed":"# Titanic_EDA,Visualization, ML","bbeaf2b5":"Now we can get all the types of Cabin are starting with these character:\n>         A \n>         B  \n>         C \n>         D  \n>         E \n>         F  \n>         G \n>         T  \n\n* Embarked (Port of Embarkation):\n>         S \n>         C  \n>         Q \n\n------------\n","d8739b3e":"### Numeric Variables\n* PassengerId (Uniquely define a passenger)","97200d3c":"There's 891 person in train dataset and each person has a unique PassengerId. However, PassengerId is a numeric value and there's a pretty small correlation (-0.05) between 'Survived' and 'PassengerId'. We may not consider this feature in the prediction modelling.\n\n\n* Age\n* SibSp (# of siblings \/ spouses aboard the Titanic)\n* Parch (# of parents \/ children aboard the Titanic)\n* Fare (the ticket price)\n\n-------\n### Text Variable\n* Name\n* Ticket\n----------","fbdc9f3f":"* However,some of the machine learning model will treat the categorical features as numeric features, such as XGBoost","d9b8a537":"<a id=\"13\"><\/a> \n## 4.2 Model Comparison\n* Mean absolute error and accuracy score of each model","6c9d0e02":"* Obviously, survival rate of female is much higher than male\n* Consider Sex vs. Age vs. Survival","3be98b72":"<a id=\"12\"><\/a> \n## 4.1 Building Machine Learning Models\n* Divide the trainging dataset into 2 groups \n* One group for training and Another group for model testing say valid dataset","962cea7e":"### Age, Family Size, Fare and Survive","accf9b48":"* The p-value of PassengerId is larger than 0.05.\n* PassengerId does not have a siginificant level of 95% and can be dropped\n* Also the object types feature [Name,Sex,Ticket, Embarked,Title] should be dropped\n* Famsize is the sum of SibSp and Parch. They have high relevancy.\n* Obviously, Famsize has a much higher siginificant level than SibSp and Parch, so SibSp and Parch should be abandoned","c94371b4":"* new_Cabin='C' has the highest portion in both dataset\n* Treat them as categorical variable by applying Label_Encoding\n* However, 'new_Cabin' contains missing value (null)\n* Regard all the missing value as new_Cabin='Z' for now and leave this problem to next part","a46c90dd":"### Embarked, Pclass and Fare vs. Survived\n* Embarked and Survived","b249b9e1":"------\n<a id=\"7\"><\/a> \n## 2.4  Missing Values\nThere are totally 891 entries in the train dataset and 418 entries in the test dataset. However, the variable: Age, Cabin and Embarked in train dataset contain null values and the variables:  Age, Fare,Cabin in test dataset contain null values. Find the best method to deal with missing value problem.","743516ab":"* The format of submission:","589d2e49":"* Visualize Age, Fare,Family size and Survived","c9a4bc78":"------\n<a id=\"6\"><\/a> \n## 2.3 Visualization of Features\nSome of the features are highly correlated with survival rate or with each other. Data visualization can help us find out the pattern behind them.\n\n### Gender, Age and Survived\n* Firstly, Consider the Age structure of each Gender \n*  We can discover that the Age structure of male and female are similar","c24e23a2":"### Random Forest Classfier\n* Find the best number of n_estimators","8617017e":"----\n<a id=\"14\"><\/a> \n# 5.Data Submisson","4b199a1c":"* 3D surface plot and contour plot to visualize the relation among SibSp, Parch and Age\n* Z represents the average Age for certain pairs of (SibSp,Parch) value","f766192e":"### Roc Curve\n* Let the sample size be: a+b+c+d\n* Let Y be the real value and $\\hat{Y}$ is the prediction value","6e94a488":"### Naive Bayes","ee20d3b5":"* The portion of blue area is much larger than purple\n* At any range of age, the survival rate of female is much higher than male\n\n### Parch and SibSp vs. Age\n* SibSp is highly correlated with SibSp\n* There's negative correlation between Parch and SibSp vs. Age\n* Group the dataset by pairs of (x=Parch,y=SibSP) value and compare each group's average age\n* Ignore the 'Age' missing value in this part","e10c51c8":"* Some of the features have almost no correlation with Survived (eg. PassengerId)\n* The correlation magnitude of [PassengerId, Age, SibSp, Parch, Ticket_length, Famsize] and Survived are less then 0.1\n* Apply the Pearson Residual Test to have a better insight\n-----\n#### Pearson's Residual Test\n* For a given factor, The null hypothesis of a feature is that 'The prediction of Survived will not consider this factor'.\n* Use the scipy.stats library to calculate the p-value and compare it with alpha=0.05. If the p-value between the factor and the response is larger than alpha, then this factor does not have a significant level of 95%. The null hypothesis will not be rejected. However, if the p-value is less than alpha, the factor will be rejected.","d519a544":"### Age\n* Age is highly correlated to Pclass, SibSp and Parch\n* We can apply a prediction for the missing values based on these features\n* Gonna choose LogisticRegression in this part","fd71dc07":"* Fare of Purple dots are higher than blue dots on average\n* Higher Fare price tends to stay alive","b7721a5d":"This is my first notebook in kaggle, so please **upvote** if you like it and leave a comment if you have any advice. I will appreciate that. There's totally 5 sections in this notebook. By Tableau and seaborn, we can do the EDA. By statistical analysis on the features and building machine learning models, we can make a prediction of whether a person survive or not.","20cbd68f":"Then we can calculate sensitivity and Specificity:\n* **Sensitivity**: $P(\\hat{Y}=1|Y=1) = \\frac{a}{a+b}$\n* **Specificity**: $P(\\hat{Y}=0|Y=0) = \\frac{d}{c+d}$\n* The ROC curve is plotting **sensitivity** in y-axis and **1-specificity** in x-axis. \n* AUC: Concordance index c is the area under ROC curve. The bigger the c, the better the model.","0617b174":"* Find the best XGBM model by hypeparameter method\n* Also apply the cross validation with cv=7","4a5029b1":"<a id=\"2\"><\/a> \n## 1.1 Import Data and Package","cae2dcf6":"* XGB_Classfier has the smallest mean absolute error and the largest accuracy score","896e7bd8":"There are totally 11 variables and we can divide them into 3 types: categorical variable, numeric variables and text variable:\n### 1.2.1 Categorical Variable\n\n* **Pclass **:\n>         1=1st  \n>         2=2st             \n>         3=3st \n","d94ae5ca":"### SibSp & Parch\n* SibSp and Parch are both illustrating number of family members\n* Set a new feature Famsize = SibSp + Parch","b982276f":"### Embarked, Fare and Pclass\n* Embarked and Pclass","c8613f17":"<a id=\"10\"><\/a> \n## 3.2 Statistical Analysis and Feature Selection\n* Draw the Correlation Heatmap based on the features in hand","bbfe630b":"* Higher Fare price and upper class tends to live ","2dcbfe12":"* As the mean of Fare are 99 and 115 for Embarked='S' and Embarked='C' respectively\n* 99 is much closer to 80 than 115\n* Assign Embarked='S' to these 2 missing values","e5d08565":"### K Nearest  Neighbor Classifier\n* Find the best hyperparameter by RandomizedSearchCV\n* Implement Cross-validation with cv=7 to train the model","596d87e5":"* We will then choose n_estimators=100","e5b311ce":"* Apply the correlation Heatmap","ab48e134":"-----\n<a id=\"8\"><\/a> \n# 3. Feature Engineering and Statistical Analysis\nIn this part, we will create some features by applying feature engineering. Then select the useful features among them by statistical analysis to prepare for model prediction.","9be68957":"* Categorical Naive Bayes has the best performance","6946486f":"* People from Pclass=1 and Sex= female are more likely to from Embarked=S or C\n* Let's see the Fare description for (Pclass=1 & Sex='female')","4f237425":"![image.png](attachment:image.png)","bcbfcc36":"* Some of the features are highly correlated with Survived\n* Apply the pairs plot on those features","5abfba10":"-----\n## Table of Content\n*   [1. Import Data and Data Structure](#1)  \n    *    [ 1.1 Import Data and Package](#2)    \n    *    [ 1.2 Overview of Data Structure (Tableau)](#3)  \n    \n    \n*   [2. Data Visualization and Missing Values](#4)  \n    *       [2.1 Categorical Encoding](#5)\n    *     [2.2 Overview of Relation](#15)\n    *       [2.3 Visualization of Features](#6)\n    *    [ 2.4 Missing Values](#7)     \n    \n    \n*   [3. Feature Engineering and Statistical Analysis](#8)  \n    * [3.1 Features Generation](#9)\n    * [3.2 Statistical Analysis and Feature Selection ](#10)    \n    \n\n*   [4. Modelling](#11)\n    * [4.1 Building Machine Learning Models (KNN, XGBoostClassifier, LGBMClassfier etc.)](#12)\n    * [4.2 Model Comparison ](#13)  \n    \n\n*   [5. Data Submission](#14)    ","4dd8f4e7":"<a id=\"3\"><\/a> \n## 1.2 Overview of Data structure \n> Find out the data structure and type","95229d2e":"* Prediction on test dataset\n* What we gonna do is take the mean value of KNN, LGBM and XGB models' prediction","b0acd678":"The above table shows the Survival rate for each Pclass which is an decreasing trend: 0.629 > 0.472 > 0.242 as 'Pclass' value increasing. Also Pclass=3 has the highest death population. Pclass will be an important feature for Survived prediction.\n* **Sex**:\n>         Male \n>         Female  \n\nMost of the passengers are male and female passenger was less than 50%","287a1611":"* Larger SibSp and Parch value implies lower average Age\n* Young people are more likely to be companied by their family member\n","6e7a8ef2":"* XGB Classifier, Random Forest Classifier and K Nearest Neighbors Classifier have the largest AUC value that are the best performance among all the models\n* Also KNN Classifier have the smallest mean absolute error and the largest accuracy score. We gonna take the average result of these 3 models\n","6b916553":"\n### Sex\n* Convert Sex into categorical variable by applying Label-Encoding","81c48c1a":"### Embarked\n* According to the correlation heatmap, we know that Embarked is highly correlated to Sex, Fare and Pclass\n* We gonna find out the missing values based on these 2 features","9f5b200f":"* 77% of the Cabin data are null for both dataset\n* Cabin column should be abandoned","90a11082":"### Logistic Regression","a21922ef":"\n### Cabin\n* Cabin is highly correlated to Pclass, Fare, Age and Sex\n","6198c5c3":"* Embarked S and C have higher survival rate than Embarked Q\n","d9ac667d":"### XGBClassifier","405f2aaf":"### Stochastic Gradient Descent Classifier\n* It's equivilant to linear SVM"}}