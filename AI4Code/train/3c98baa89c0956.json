{"cell_type":{"d8407b84":"code","04562324":"code","69734b5e":"code","738d05f8":"code","d80b582f":"code","d8d29ba3":"code","e8cb503b":"code","9a03c64a":"markdown","f32ac457":"markdown","f0048db3":"markdown","da18ef1d":"markdown"},"source":{"d8407b84":"# Plot Modified MSE \nimport numpy as np \nimport plotly.graph_objects as go\nlmbda = 2\nx = np.outer(np.linspace(-0.5, 0.5, 30), np.ones(30))\ny = x.copy().T \nz=np.where(x*y>=0, (y-x)**2, ((y-x)**2) - (x*y)*lmbda )\n\ntrace = go.Surface(x = x, y = y, z =z )\ndata = [trace]\nlayout = go.Layout(title = '3D Surface plot')\nfig = go.Figure(data = data)\nfig.update_layout(title='MSE Modified 1 Objective function values', autosize=False,\n                  width=500, height=500,\n                  margin=dict(l=80, r=70, b=85, t=110),\nscene = dict(xaxis_title='Predicted resp',yaxis_title='True resp',zaxis_title='Value of Objective Function'))\nfig.show()\n","04562324":"# Plot Modified MAE\nlmbda = 2\nx = np.outer(np.linspace(-0.5, 0.5, 30), np.ones(30))\ny = x.copy().T # transpose\nz=np.where(x*y>=0, abs(y-x), abs(y-x) - (x*y)*lmbda )\n\n\ntrace = go.Surface(x = x, y = y, z =z )\ndata = [trace]\nlayout = go.Layout(title = '3D Surface plot')\nfig = go.Figure(data = data)\nfig.update_layout(title='MAE Modified 1 Objective function values', autosize=False,\n                  width=500, height=500,\n                  margin=dict(l=80, r=70, b=85, t=110),\nscene = dict(xaxis_title='Predicted resp',yaxis_title='True resp',zaxis_title='Value of Objective Function'))\nfig.show()\n","69734b5e":"%%time\nimport datatable as dt\nimport numpy as np\nimport pandas as pd \nimport os\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport janestreet\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\nfrom sklearn.model_selection import train_test_split\n\n\nimport lightgbm as lgb\n\n\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","738d05f8":"%%time\n# Load data\ntrain_data = dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\nfeature = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/features.csv\")\ntest_example = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/example_test.csv\")\nsample_sub = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/example_sample_submission.csv\")\n\n\nALL_FEATURES = [\"feature_\" + str(i) for i in range(0,130) ]\nKEPT_FEATURES = ALL_FEATURES\nCAT_FEATURES = [\"feature_0\"]\nLABEL_COLUMNS = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\nDATE_COLUMNS = [\"date\", \"ts_id\"]\n# \"weight\" is only remeaning column\n\n\n# Derive \n#train_data = train_data.loc[train_data.weight !=0,]\ntrain_data['action'] = 0\ntrain_data.loc[train_data['resp']>0.0,'action'] = 1\nfeatures = [f\"feature_{x}\" for x in range(130)]\n\n# Filter first 85 days \ntrain_data = train_data.loc[train_data.date>85,]\n\n# Calculate daily Signal to Noise ratio to use as weight\ndaily_resp_mean = train_data.groupby(\"date\").resp.mean()[train_data.date].values\ndaily_resp_std = train_data.groupby(\"date\").resp.std()[train_data.date].values\n\n# 4 variants of daily SNR -- logged versions are quite normally distributed.\ndaily_SNR_abs = abs(daily_resp_mean) \/ daily_resp_std ## This is not as useful as the logged one\ndaily_SNR_abs_logged = -1*np.log(daily_SNR_abs)\n\ndaily_SNR_squared = np.square(daily_resp_mean) \/ np.square(daily_resp_std) ## This is not as useful as the logged one\ndaily_SNR_squared_logged = -1*np.log(daily_SNR_squared)\n\ndel daily_resp_mean, daily_resp_std\n\n\ntarget = 'resp'\n\ny = train_data[target].values\ndate = train_data[\"date\"].values\nweight = train_data[\"weight\"].values\nresp = train_data[\"resp\"].values\ntrain_data = train_data[features].values\n\n","d80b582f":"# Objective function implementation\ndef mse_modified_1(y_pred, y_true):\n    # Set hyperparameter lambda\n    lmbda = 0.15\n    \n    # This weight is SNR\n    weight = y_true.get_weight()\n    \n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float64\")\n    \n    signs_matching = (y_true * y_pred) >= 0\n    \n    grad = np.where(signs_matching,  weight *(-2 * residual), weight * (-2 * residual - y_true*lmbda))\n    hess = np.where(signs_matching, weight * 2 , weight * 2 )\n    return grad, hess\n\n\ndef Eval_mse_modified_1(y_pred, y_true):\n    lmbda = 0.15\n    \n    weight = y_true.get_weight()\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    \n    signs_matching = (y_true * y_pred) >= 0\n    \n    mse_action_value = np.where(signs_matching,  residual**2, residual**2 - y_true*y_pred*lmbda)\n    \n    mse_action_value = weight* mse_action_value \n    \n    return \"MSE_Modified_1\", np.mean(mse_action_value), False\n\n\n\n\n\n\n\ndef mse_modified_2(y_pred, y_true):\n    lmbda = 0.2\n    \n    # Weights are Daily signal to noise ratio\n    weight = y_true.get_weight()\n    \n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float64\")\n    \n    signs_matching = (y_true * y_pred) >= 0\n    \n    \n    grad = np.where(signs_matching, -2 * residual,  -2 * residual + 2*y_pred*(y_true**2)*lmbda)\n    hess = np.where(signs_matching, 2 , 2 + 2*(y_true**2)*lmbda )\n    \n    grad = weight * grad\n    hess = weight * hess\n    return grad, hess\n\n\ndef Eval_mse_modified_2(y_pred, y_true):\n    lmbda = 0.2\n    \n    # Weights are signal to noise ratio\n    weight = y_true.get_weight()\n    y_true = y_true.get_label()\n    residual = (y_true - y_pred).astype(\"float\")\n    \n    signs_matching = (y_true * y_pred) >= 0\n    \n    value = np.where(signs_matching,  residual**2, residual**2 + (y_true**2)*(y_pred**2)*lmbda)\n    \n    value = weight* value \n    \n    return \"MSE_Modified_2\", np.mean(value), False\n\n                    \n       \n\n                    \n# THIS WILL NOT WORK AS SEEN HESSIAN IS ZERO AND THE FUNCTION IS NOT DIFFERENTIABLE AT CRITICAL\n# Usually MAE is approximated by ARC COSH function at critical point to make it differentiable.\n\n# def mae_modified_1(y_pred, y_true):\n#     lmbda = 2\n#     #weight = y_true.get_weight()\n#     y_true = y_true.get_label().astype(\"float64\")\n#     signs_matching = (y_true * y_pred) >= 0\n#     is_pred_bigger = y_pred > y_true\n    \n    \n#     grad = np.where(is_pred_bigger, 1, -1)\n#     grad[~signs_matching] = grad[~signs_matching] - y_true[~signs_matching]*lmbda\n#     #grad = weight * grad\n    \n#     hess = np.where(signs_matching, 0, 0)\n#     return grad, hess\n\n# def Eval_mae_modified_1(y_pred, y_true):\n#     lmbda = 2\n#     #weight = y_true.get_weight()\n#     y_true = y_true.get_label()\n#     residual = (y_true - y_pred).astype(\"float\")\n    \n#     signs_matching = (y_true * y_pred) >= 0\n#     is_pred_bigger = y_pred > y_true\n    \n#     mae_action_value = np.where(is_pred_bigger, -residual, residual)\n#     mae_action_value[~signs_matching] += y_true[~signs_matching]*y_pred[~signs_matching]*lmbda\n    \n#     #mae_action_value = weight * mae_action_value\n    \n#     return(\"MAE_Modified_1\", np.mean(mae_action_value), False)\n\n\n\n\n# Taken from Yurin's notebook \n# https:\/\/www.kaggle.com\/gogo827jz\/jane-street-super-fast-utility-score-function\ndef utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\n\n","d8d29ba3":"split_from = train_data.shape[0] - train_data.shape[0]\/\/6\ntrain = train_data[:split_from,]\ny_train = y[:split_from]\n\nvalid = train_data[split_from:,]\ny_valid = y[split_from:]\n\ntrain_daily_SNR = daily_SNR_abs_logged[:split_from]\nvalid_daily_SNR = daily_SNR_abs_logged[split_from:]\n\n\n\nparams = {\n        'boosting_type': 'gbdt',\n        'objective': 'custom',\n        'n_jobs': -1,\n        'seed': 0,\n        \"num_leaves\": 32,\n        'learning_rate': 0.01,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 10,\n        'colsample_bytree': 0.9,\n        \"num_boost_round\": 2500,\n        \"early_stopping_rounds\": 50,\n        \"min_data_in_leaf\": 20}\n\n\n\nlgb_train = lgb.Dataset(train,y_train, weight=train_daily_SNR)\nlgb_valid = lgb.Dataset(valid,y_valid, weight=valid_daily_SNR)\n\nmodel = lgb.train(\n    params,\n    lgb_train,\n    valid_sets = [lgb_valid],\n    verbose_eval = 50,\n    fobj=mse_modified_1,\n    feval=Eval_mse_modified_1\n)\n\nresp_preds = model.predict(valid)\naction_preds = (resp_preds > 0).astype(int)\nval_utility_score = utility_score_bincount(date[split_from:], weight[split_from:], resp[split_from:], action_preds)\n\nprint(f\"Validation Utility Score: {val_utility_score:.2f}\")","e8cb503b":"for (test_df, sample_prediction_df) in tqdm(iter_test):\n    predictions = model.predict(test_df[features].values[:,])\n    predictions = predictions > 0\n    sample_prediction_df.action = predictions.astype(int)\n    env.predict(sample_prediction_df)","9a03c64a":"# Proxy Metrics for Utility Score to Use in Regression \n# &\n# Custom Objective Function Implementations for LGBM\n\n\n## Why regression might be more appropriate?\nIn this notebook, I will create proxy metrics to turn classification task into a regression task. I wanted to try and see the effect of structuring this problem as a regression problem instead of classification because \"resp\" includes more information compared to a binary label as \"action\". \n\n\n## How those Custom Objective Functions are related with Evaluation Metric i.e Utility Score?\n\nI started with adding variations of penalty terms to MSE and MAE if predicted \"resp\" and true \"resp\" doesn't have the same sign. This modification is for assigning specific importance for the match between predicted \"action\" and the true \"action\" on top of the penalty due to residuals between predicted and actual \"resp\". This property would allow those new metrics to be used as a proxy to Utility score in a regression setting.\n\n\n## Modified MSE Derivation\n\n$$Modified MSE = f(\\hat{y}) = \\sum_{j}(y_j - \\hat{y}_j)^2 + \\left\\{\n\\begin{array}{ll}\n      0  &, y . \\hat{y}_j > 0 ; \\\\\n      - \\lambda . y_j . \\hat{y}_j   &, y_j . \\hat{y}_j \\leq 0 \\\\\n\\end{array} \n\\right.$$\n\n$y_j$ :  True \"resp\" value for row j.\n\n$\\hat{y}_j$ :  Predicted \"resp\" value for row j.\n\n$ \\lambda $ : Parameter to control amount of penalty for non matching action predictions. Needs to be tuned.\n\n**Piecewise part of the metric represents the extra penalty for non matching true and predicted action. If predicted and true \"resp\" doesn't have the same label, their multiplication will be smaller than 0 and piecewise function will add extra penalty of $y . \\hat{y}$**\n\n\n### Gradient and Hessian \nI will be implementing objective functions of the different variants of Modified MSE type hybrid metrics in the code below but before starting that part, I wanted to calculate the gradient and hessian for this example -- as gradient and hessian is required to be calculated for objective funcitons in LGBM. \n\n$$ Gradient  Modified MSE =  \\frac{\\partial f(\\hat{y})}{\\partial \\hat{y}} = \\sum_{j} -2(y_j - \\hat{y}_j) + \\left\\{\n\\begin{array}{ll}\n      0  &, y . \\hat{y}_j > 0 \\\\\n      - \\lambda . y_j   &, y_j . \\hat{y}_j \\leq 0 \\\\\n\\end{array} \n\\right.$$\n\n$$ Hessian  Modified MSE = \\frac{\\partial^2 f(\\hat{y})}{\\partial \\hat{y}^2} = 2 $$\n","f32ac457":"## Weighted Training with Daily Signal-to-Noise Ratio -- Added at Version 3\n\nWe know that low signal to noise ratio is a problem in this dataset. Hence, I used an alternative definition of SNR to calculate it on daily resp as follows;\n[Here is the link to wiki for the formula](https:\/\/en.wikipedia.org\/wiki\/Signal-to-noise_ratio)\n\n$$ SNR = \\frac{\\mu}{\\sigma}\n             \\\\ or \\\\\nSNR = \\frac{\\mu^2}{\\sigma^2} $$\n\nI will be assigning daily SNR values of \"resp\" as training weights so the model will assign higher importance for the days with higher SNR.\n\nI will derive some variants of those formulas as first formula can end up with negative weights and second formula has a log-normal distribution which made me experiment with log transformed version of it as well.\n\nI derived 4 versions of SNR and results on time series splitted validation set is as follows;\n\n\nsecond formula to assign high importance for negative \"resp\" with high absolute value.\nFirst formula might be considered perhaps after taking absolute value of \"resp\" first.","f0048db3":"\nThere are 2 requirement for an objective function which are ;\n\n- It should be differentiable to first and second level wrt. $\\hat{y}_j$ as those derivatives will be gradient and hessian to be used in the underlying optimisation algorithm to minimize objective function.\n\n- Being differentiable implies another condition which is being continous.\n\nWe can quickly have a look at the shape of those objective functions using a 3-d plot. I will set $\\lambda = 2$ to be able to visualise in 3-d","da18ef1d":"Given that resp is ranged between -0.5 and 0.5 in the training set, I restricted those axis on $|resp_{pred}|<0.5$ and $|resp_{true}|<0.5$.\n"}}