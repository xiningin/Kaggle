{"cell_type":{"68f58364":"code","d317f7dc":"code","c2010a76":"code","cba8c4b2":"code","2dbb9f48":"code","b36b709f":"code","1b72a20b":"code","dec7e79e":"code","bf555df9":"code","11d0cf9e":"code","81cec9da":"code","acfc1df4":"code","ae4407db":"code","b890daed":"code","6db1407c":"code","502b00f1":"code","ae64fd2d":"code","28f9f1d5":"code","de190af0":"code","a256e1a9":"code","6e10d042":"code","28519f0e":"code","3000bea8":"code","730d185f":"code","50365d8b":"code","4a5210e0":"code","c0bf3f8c":"code","5208f294":"code","4df4422e":"code","7a87ce3f":"code","6eb57bf0":"code","578fd9df":"code","edd03b44":"code","628155e7":"code","480cccb8":"markdown","b4a94f29":"markdown","900f4269":"markdown","5d19f479":"markdown","cfc9c739":"markdown","95416e73":"markdown","7474f51a":"markdown","dae56b4a":"markdown","b128cded":"markdown","274cdada":"markdown","b66efbd4":"markdown","dd52b1e9":"markdown","d21aaf9f":"markdown","8e9658cc":"markdown","85f8a1a8":"markdown","4bf50417":"markdown","263566e4":"markdown","e8122f0d":"markdown","a19de3b9":"markdown","49f861dd":"markdown","65238aef":"markdown","e758684a":"markdown","15897d27":"markdown","63554be0":"markdown","5d2338cd":"markdown","7a2fa13b":"markdown","bb896721":"markdown","b66af7d8":"markdown","0ab59f69":"markdown"},"source":{"68f58364":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d317f7dc":"import graphviz, IPython\nimport matplotlib.lines as lines\nfrom matplotlib.ticker import FuncFormatter\nfrom sklearn.tree import export_graphviz\n\ndef draw_tree(tree, df):\n    s = export_graphviz(tree, out_file=None, feature_names=df.columns, filled=True)\n    return graphviz.Source(s)","c2010a76":"from sklearn import metrics as metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\ndef metricas(y_train,y_pred_train,y_test,y_pred_test):\n    valores=y.value_counts().index.to_list()\n    \n    # Matriz de confusion: Train\n    cm_train=metrics.confusion_matrix(y_train,y_pred_train,labels=valores)\n    df_cm=pd.DataFrame(cm_train,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusi\u00f3n: Train')\n    plt.xlabel('Predicci\u00f3n')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    # Matriz de confusion: Test\n    cm_test=metrics.confusion_matrix(y_test,y_pred_test,labels=valores)\n    df_cm=pd.DataFrame(cm_test,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusi\u00f3n: Test')\n    plt.xlabel('Predicci\u00f3n')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    accuracy_train=metrics.accuracy_score(y_train,y_pred_train)\n    accuracy_test=metrics.accuracy_score(y_test,y_pred_test)\n    precision_train=metrics.precision_score(y_train,y_pred_train,average='micro')\n    precision_test=metrics.precision_score(y_test,y_pred_test,average='micro')\n    recall_train=metrics.recall_score(y_train,y_pred_train,average='micro')\n    recall_test=metrics.recall_score(y_test,y_pred_test,average='micro')\n    f_score=f1_score(y_test,y_pred_test,average='micro')\n    \n    train = (accuracy_train*100, precision_train*100, recall_train*100)\n    test = (accuracy_test*100, precision_test*100, recall_test*100)\n\n    ind = np.arange(3)  # the x locations for the groups\n    ind_n = np.arange(4)  # the x locations for the groups\n    width = 0.3       # the width of the bars\n    \n    fig = plt.figure(figsize = (8,5))\n    ax = fig.add_subplot(111)\n    \n    rects1 = ax.bar(ind, train, width, color='r')\n    rects2 = ax.bar(ind+width, test, width, color='g')\n    rects3 = ax.bar(3, f_score*100, width, color='b')\n    \n    ax.set_ylabel('Scores')\n    ax.set_xticks(ind_n + width\/2)\n    ax.set_xticklabels( ('Accuracy', 'Precisi\u00f3n', 'Recall', 'F1 Score') )\n    ax.legend( (rects1[0], rects2[0]), ('Train', 'Test') )\n    \n    def autolabel(rects):\n        for rect in rects:\n            h = rect.get_height()\n            ax.text(rect.get_x()+rect.get_width()\/2., 1.00*h, '%.3f'%round(h,3),\n                    ha='center', va='bottom')\n\n    autolabel(rects1)\n    autolabel(rects2)\n    autolabel(rects3)\n    plt.title('Puntajes')\n    plt.ylim(0,120)\n    plt.show()\n    \n    return ","cba8c4b2":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\ndef plot_dt(X, y, modelo):\n    # Parameters\n    n_classes = 3\n    plot_colors = \"byr\"\n    plot_step = 0.02\n    \n    # Load data\n    iris = load_iris()\n    \n    # Train\n    clf = modelo\n    \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n    \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.jet)\n    \n    # Plot the training points\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                    cmap=plt.cm.jet, edgecolor='black', s=15)\n        \n    \n    plt.title(\"\u00c1rbol de desici\u00f3n\")\n    plt.legend(loc='upper right', borderpad=0, handletextpad=0)\n    plt.axis(\"tight\")\n    \n    plt.figure()\n    plt.show()\n    \n    return","2dbb9f48":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","b36b709f":"df = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndf.head()","1b72a20b":"df.isnull().sum()","dec7e79e":"df.columns.to_list()","bf555df9":"df_clean = df.drop('Id', axis=1).copy()\ndf_clean.head(2)","11d0cf9e":"variables_numericas = df_clean.drop('Species', axis=1).columns.to_list()","81cec9da":"filas=len(df_clean.columns.to_list())\nc=1\nfig=plt.figure(figsize=(25,7*filas))\n    \nfor i,j in enumerate(variables_numericas):\n    plt.subplot(filas,2, c)\n    sns.distplot(df_clean[j])\n    c = c + 1\n    \n    plt.subplot(filas,2, c)\n    ax1=sns.boxplot(x=df_clean[j],palette=\"Blues\",linewidth=1)\n    c = c + 1\n\nplt.show()","acfc1df4":"datos_x = df_clean.Species.value_counts().index.to_list()\ndatos_y = df_clean.Species.value_counts().to_list()\nsuma = df_clean.Species.value_counts().sum()\nprint(df_clean.Species.value_counts())","ae4407db":"list_color=['blue','green','red']\nplt.figure(figsize=(12,6))\ngraph = plt.bar(datos_x, datos_y, color=list_color)\nplt.title('Distribuci\u00f3n de datos del Target')\n\ni = 0\n\nfor p in graph:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    plt.text(x+width\/2,\n             y+height*1.05,\n             str(height)+\" -> \"+str(round(height\/suma,2))+'%',\n             ha='center')\n    i+=1\n\nplt.ylim(0,70)\nplt.show()","b890daed":"fig, axs = plt.subplots(1, 4, figsize=(20,6))\n\nfor i, variable in enumerate(variables_numericas):\n    sns.boxplot(ax=axs[i],data=df_clean, x='Species', y=variable)\nplt.show()","6db1407c":"sns.pairplot(df_clean, hue='Species')\nplt.show()","502b00f1":"import scipy.stats as stats\n\nranking = pd.DataFrame({'Variable':[],\n                        'p-value':[]})\n\nfor fila, variable in enumerate(variables_numericas):\n    print('#'*20)\n    prueba_anova = df_clean[['Species',variable]].copy()\n    grouped_anova = prueba_anova.groupby(['Species'])\n    anova_results_1 = stats.f_oneway(grouped_anova.get_group('Iris-setosa')[variable], grouped_anova.get_group('Iris-versicolor')[variable], grouped_anova.get_group('Iris-virginica')[variable])\n\n    print(variable)\n    print(anova_results_1.pvalue)\n    \n    ranking.loc[fila, 'Variable'] = variable\n    ranking.loc[fila, 'p-value'] = anova_results_1.pvalue\n\n    if anova_results_1.pvalue < 0.05:\n        print('The variables are associated (H0 is rejected)')\n    else:\n        print('The variables are not associated (H0 is not rejected)')\n\nranking = ranking.sort_values(['p-value'])","ae64fd2d":"ranking","28f9f1d5":"sns.heatmap(df_clean.drop('Species', axis=1).corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","de190af0":"ranking.sort_values(['p-value'])","a256e1a9":"fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\naxs[0].scatter(data=df_clean, x='PetalLengthCm',y='PetalWidthCm')\naxs[0].set(xlabel='PetalLengthCm', ylabel='PetalWidthCm')\n\naxs[1].scatter(data=df_clean, x='PetalLengthCm',y='SepalLengthCm')\naxs[1].set(xlabel='PetalLengthCm', ylabel='SepalLengthCm')\n\nplt.show()","6e10d042":"sns.heatmap(df_clean.drop(['PetalWidthCm','SepalLengthCm','Species'], axis=1).corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","28519f0e":"sns.scatterplot(data=df_clean, x='PetalLengthCm', y='SepalWidthCm', hue='Species')\nplt.show()","3000bea8":"df_final = df_clean[['PetalLengthCm','SepalWidthCm','Species']].copy()\ndf_final.head(2)","730d185f":"from sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\ndf_final['Species'] = enc.fit_transform(df_final['Species'])\ndf_final.head(3)","50365d8b":"X = df_final.drop('Species', axis=1)\ny = df_final['Species']","4a5210e0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)","c0bf3f8c":"from sklearn.tree import DecisionTreeClassifier\narbolDecision = DecisionTreeClassifier(max_depth=4)\narbolDecision.fit(X_train, y_train)\ny_pred_train = arbolDecision.predict(X_train)\ny_pred_test = arbolDecision.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","5208f294":"plot_dt(X.to_numpy(), y, arbolDecision)","4df4422e":"draw_tree(arbolDecision, X)","7a87ce3f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators':[55,56,57,58,59,60],\n              'max_depth':[1,2,3,4,5,6] ,\n              'n_jobs':[-1],\n              'max_features':[1]}\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(),\n             param_grid=parameters)","6eb57bf0":"grid_search.fit(X_train,y_train)","578fd9df":"best_param= grid_search.best_params_\nbest_param","edd03b44":"best_model = grid_search.best_estimator_","628155e7":"y_pred_train=best_model.predict(X_train)\ny_pred_test= best_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","480cccb8":"## Balanced Target","b4a94f29":"## 1.1. Missing Values","900f4269":"## 1.3. Outliers","5d19f479":"### Filter Method -> ANOVA","cfc9c739":"# 2. EDA","95416e73":"We can see that the target is correctly balanced.","7474f51a":"# Conclusions","dae56b4a":"## 4.2. Machine Learning algorithms","b128cded":"# 1. Data Cleaning","274cdada":"We can see that some variables are highly correlated, this results in the model taking redundant information to predict.","b66efbd4":"We do not have very extreme values, nor do they indicate that it is erroneous data, therefore, outliers will not be treated in this case.","dd52b1e9":"## 3.1. Feature Selection","d21aaf9f":"# 4. Training and Validation","8e9658cc":"# Loading Data","85f8a1a8":"Thus we obtain two variables that are associated with the target, and that do not have much correlation with each other.","4bf50417":"### Correlation Matrix","263566e4":"## 3.2. Feature Transformation","e8122f0d":"# Pre-functions","a19de3b9":"### GridSearchCV with Random Forest","49f861dd":"We do not have null data","65238aef":"We have the variable \"Id\" that does not provide any information, therefore it will be eliminated.","e758684a":"# 3. Feature Engineering","15897d27":"Thus we obtain the ranking of the most important variables for the target.","63554be0":"We have two powerful algorithms, but which one should we choose? Although both give us powerful scores, with the RandomForest being a complex algorithm, we lose interpretability, but with the decision tree we obtain interpretability, that is, we can know how the result was reached.","5d2338cd":"## 1.2. Irrelevant Data","7a2fa13b":"### Decision Tree","bb896721":"## 4.1. Train Test Split","b66af7d8":"We can see that all the variables pass the ANOVA test, this means that all the variables are associated with the target.","0ab59f69":"As \"PetalLengthCm\" is the most important variable for the target and it is associated a lot with \"PetalWidthCm\" and \"SepalLengthCm\", we will eliminate the two variables mentioned above, they are already highly correlated to \"PetalLengthCm\" and they are the ones that have the least association with the target. ."}}