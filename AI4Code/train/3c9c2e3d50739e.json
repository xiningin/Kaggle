{"cell_type":{"801dbddb":"code","489c05a9":"code","4c7ccf88":"code","96c3857f":"code","21c8b036":"code","77c8cba7":"code","a8b27209":"code","0b72d01c":"code","a7aec69d":"code","0d1af89b":"code","d243cb82":"code","a4d7f5e2":"code","9ddbba14":"code","d24937db":"code","92387b33":"code","fe1175ad":"code","10ad280f":"code","fd9ddf9a":"code","daf7a790":"code","3ab0f21b":"code","a1a88529":"code","e84adeb3":"code","23e5c765":"markdown","d47d3e8e":"markdown","c59c3359":"markdown","af8469bc":"markdown","602764a4":"markdown","2113f4e8":"markdown","657f054f":"markdown","dcb5d2c6":"markdown","3daaec39":"markdown","3005a750":"markdown","9bcd9a72":"markdown"},"source":{"801dbddb":"import re\nfrom bs4 import BeautifulSoup\nimport os\nimport random\nimport joblib\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold, cross_val_score\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\n\nimport matplotlib.pyplot as plt","489c05a9":"TEST_DATA_PATH = '..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv'\nVALID_DATA_PATH = '..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv'\nTRAIN_DATA_PATH = '..\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv'","4c7ccf88":"SEED = 10","96c3857f":"def set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\n\ndef set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format\n    \n    \ndef text_cleaning(text: str) -> str:\n    \"\"\"Function cleans text removing special characters,\n    extra spaces, embedded URL links, HTML tags and emojis.\n    Code source: https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer\n    :param text: Original text\n    :return: Preprocessed text\n    \"\"\"\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+')  # website links\n    text = template.sub(r'', text)\n\n    soup = BeautifulSoup(text, 'lxml')  # HTML tags\n    only_text = soup.get_text()\n    text = only_text\n\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text)  # special characters\n    text = re.sub(' +', ' ', text)  # extra spaces\n    text = text.strip()  # spaces at the beginning and at the end of string\n\n    return text","21c8b036":"set_seed(SEED)\nset_display()","77c8cba7":"# Extract classified text samples and clean the texts.\ndata_train = pd.read_csv(TRAIN_DATA_PATH)\ndata_train['comment_text'] = data_train['comment_text'].apply(text_cleaning)\ndata_train.head()","a8b27209":"data_train.describe()","0b72d01c":"categories = data_train.loc[:, 'toxic':'identity_hate'].sum()\nplt.title('Category Frequency')\nplt.bar(categories.index, categories.values)\nplt.show()","a7aec69d":"scores = data_train.loc[:, 'toxic':'identity_hate'].sum(axis=1).value_counts()\nplt.bar(scores.index, scores.values)\nplt.title('Scores Distribution: Simple Sum')\nplt.show()","0d1af89b":"# Multiplication factors for categories.\ncat_mtpl = {'toxic': 1, 'severe_toxic': 1.75, 'obscene': 0.95,\n            'threat': 2, 'insult': 1.6, 'identity_hate': 1.95}\n\nfor category in cat_mtpl:\n    data_train[category] = data_train[category] * cat_mtpl[category]\n\ndata_train['score'] = data_train.loc[:, 'toxic':'identity_hate'].sum(axis=1)","d243cb82":"plt.hist(data_train['score'])\nplt.title('Scores Distribution: Adjusted Sum')\nplt.show()","a4d7f5e2":"n_samples_toxic = len(data_train[data_train['score'] != 0])\nn_samples_normal = len(data_train) - n_samples_toxic\n\nidx_to_drop = data_train[data_train['score'] == 0].index[n_samples_toxic\/\/5:]\ndata_train = data_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic\/\/5}.')\nprint(f'Total number of training samples: {len(data_train)}')","9ddbba14":"print(f'Mean toxicity score: {data_train[\"score\"].mean()}\\n'\n      f'Standard deviation: {data_train[\"score\"].std()}')","d24937db":"# Candidate models\nkridge = make_pipeline(\n    TfidfVectorizer(decode_error='ignore', stop_words='english', \n                    max_features=20_000, ngram_range=(1, 2), min_df=2),\n    KernelRidge(alpha=1.0)\n)\n\nkridge2 = make_pipeline(\n    TfidfVectorizer(decode_error='ignore', stop_words='english', \n                    max_features=20_000, ngram_range=(1, 2), min_df=2),\n    KernelRidge(alpha=0.9)\n)\n\nrandforest = make_pipeline(\n    TfidfVectorizer(decode_error='ignore', stop_words='english', \n                    max_features=20_000, ngram_range=(1, 2), min_df=2),\n    RandomForestRegressor(n_jobs=-1)\n)\n\nrandforest2 = make_pipeline(\n    TfidfVectorizer(decode_error='ignore', stop_words='english', \n                    max_features=20_000, ngram_range=(1, 2), min_df=2),\n    RandomForestRegressor(n_estimators=150, n_jobs=-1)\n)","92387b33":"models = [\n    ('KernelRidge', kridge),\n    ('KernelRidge2', kridge2),\n    ('RandomForest', randforest),\n    ('RandomForest2', randforest2)\n]","fe1175ad":"# New data for validation: text pairs.\ndata_valid = pd.read_csv(VALID_DATA_PATH)\n\n# Clean the texts\ndata_valid['less_toxic'] = data_valid['less_toxic'].apply(text_cleaning)\ndata_valid['more_toxic'] = data_valid['more_toxic'].apply(text_cleaning)\n\ndata_valid.head()","10ad280f":"# Train each model on all available samples from previous competition.\nfor name, model in models:\n    print('-' * 50)\n    model.fit(data_train['comment_text'], data_train['score'])\n    print(f'{name} model completed training.')\n\n    # Estimate toxicity score for text pairs.\n    data_valid[f'less_toxic_score_{name}'] = model.predict(data_valid['less_toxic'])\n    data_valid[f'more_toxic_score_{name}'] = model.predict(data_valid['more_toxic'])\n    print(f'{name} model completed prediction.')\n\n    # Compare scores for all text pairs.\n    data_valid[f'result_{name}'] = \\\n        data_valid[f'more_toxic_score_{name}'] > data_valid[f'less_toxic_score_{name}']\n\n    # Ratio of correctly scored text pairs.\n    print('Correct predictions:', data_valid[f'result_{name}'].sum() \/ len(data_valid))\n\n    joblib.dump(model, f'{name}.joblib')","fd9ddf9a":"# Check the accuracy of averaged scores from the best models.\ncolumns = [f'less_toxic_score_{name}' for name, _ in models]\ndata_valid['less_toxic_score'] = data_valid[columns].mean(axis=1)\n\ncolumns = [f'more_toxic_score_{name}' for name, _ in models]\ndata_valid['more_toxic_score'] = data_valid[columns].mean(axis=1)\n\ndata_valid[f'result'] = data_valid[f'more_toxic_score'] > data_valid[f'less_toxic_score']\nprint('Correct averaged predictions:', data_valid[f'result'].sum() \/ len(data_valid))","daf7a790":"# New data for text scoring.\ndata_test = pd.read_csv(TEST_DATA_PATH)\ndata_test['text'] = data_test['text'].apply(text_cleaning)\ndata_test.head()","3ab0f21b":"# Get prediction from the best models.\nfor name, model in models:\n    data_test[f'score_{name}'] = model.predict(data_test['text'])\n    print(f'{name} model completed prediction.')","a1a88529":"# Average the result.\ncolumns = [f'score_{name}' for name, _ in models]\ndata_test['score'] = data_test[columns].mean(axis=1)","e84adeb3":"data_test[['comment_id', 'score']].to_csv('submission.csv', index=False)\ndata_test[['comment_id', 'score']].head()","23e5c765":"## Prediction on the test set","d47d3e8e":"## Functions","c59c3359":"In the original DataFrame each category contains binary values (0 or 1). We will change the original values using multiplicative factors observing the following (disputable) common sense rules:\n- Normal non-offensive text samples would have a score of 0.\n- \"toxic\" category with a score of 1 would be used as a benchmark to score other offensive categories.\n- Samples marked as \"severe_toxic\" would have higher toxicity score than those marked as \"toxic\".\n- Obscene language along would have slightly lower score than \"toxic\" samples.\n- Insults would be scored in between \"toxic\" and \"severe_toxic\" closer to the upper bound.\n- Samples containing threats would have the highest toxicity score.\n- Identity hate would be scored marginally lower than threats.\n- If a sample is marked as belonging to several offensive classes total score would be calculated as a sum of values in all individual categories.","af8469bc":"Data set is imbalanced: number of neutral text samples is much larger than the number of toxic samples. We limit number of samples with 0 toxicity score to 1\/5 of the total number of texts labeled as toxic.","602764a4":"## Define the models","2113f4e8":"In the previous competition the task was to perform multi-class classification. Text sample could be labeled with one or several categories or not labeled with any. Non-toxic comments represent the majority of text samples, while toxic comments are a minority class and extremely toxic comments are more rare than plain toxic.\n\nIn this competition we have to score texts based on the level of toxicity. To get a toxicity score from the previous data we can use two approaches:\n- Simply sum up all values in each row of the DataFrame. The toxicity score will vary between 0 and 6. However some unequally toxic samples could have the same score.\n- Adjust the values in the DataFrame according to extremety of the category (for example, \"toxic\" and \"severe toxic\" should have different score) and then sum up per row values.","657f054f":"By the rules of this competition we can use public data sets on Kaggle. However for some reason we cannot submit notebooks that directly use the data from the previous competition of 4 years ago where text samples were labeled with 6 classes. In this version we use an exact copy of this competition data set created and published by [Julian Peller](https:\/\/www.kaggle.com\/julian3833) [here](https:\/\/www.kaggle.com\/julian3833\/jigsaw-toxic-comment-classification-challenge).","dcb5d2c6":"## Data processing","3daaec39":"<p style=\"text-align:center;font-size:20px;color:blue\"> If you've made it to the end of this notebook and found something useful,<\/p>\n<p style=\"text-align:center;font-size:20px;color:blue\"> please, upvote! \ud83d\ude09<\/p>","3005a750":"## Train and validate the models","9bcd9a72":"# Average Toxicity Score\nThree regression models **RandomForestRegressor, GradientBoostingRegressor and KernelRidge** are trained based on the data from the previous competition of 4 years ago, where the task was **multi-label classification** of offensive and neutral comments.\n\n**Toxicity score** is estimated based on the **adjusted sum of all values in all categories**. Final prediction is the average prediction of best regressors selected using validation data. \n\nText preprocessing is done by **TfidfVectorizer** with the maximum number of **features limited to 20,000** after applying text cleaning function to remove special characters."}}