{"cell_type":{"30f0a5b9":"code","6ea5a26c":"code","a7f77594":"code","5ad19839":"code","67964ab3":"code","2874de5b":"code","f22f0e95":"code","d3b3179f":"code","5412ccce":"code","370a18b4":"code","04aabf52":"code","489075d2":"code","7ce924f5":"code","6c1435a9":"code","aacf36bc":"code","b98e1669":"code","90a92d5d":"code","1922d939":"code","15b5ee40":"code","f2911da1":"code","34442274":"code","adf385ce":"code","0f85fee8":"code","541c4770":"code","b9e8b540":"code","4f92efd2":"code","3686dad7":"code","18e2ecb6":"code","bacb24ee":"code","6491d64d":"code","edf06ab2":"code","643ea5ef":"code","0f618410":"code","5e23e400":"code","b8a401a5":"code","d78fcc9b":"code","2801cb7e":"markdown","8fbc9b4d":"markdown","e72a3ef0":"markdown","f43c58d1":"markdown","b3bd7b4e":"markdown","84f7fafd":"markdown","815c441b":"markdown","e760f1cf":"markdown","d776cb69":"markdown","cac9ddb2":"markdown","f11f9502":"markdown","08bc5106":"markdown","955ad535":"markdown","c46a1a78":"markdown","172741b9":"markdown","e66491a8":"markdown","35b9a499":"markdown","e79f0b4f":"markdown","f97b8199":"markdown","1e9214e9":"markdown","71692ff8":"markdown","beb52d89":"markdown","59450282":"markdown","a9752907":"markdown"},"source":{"30f0a5b9":"# !pip install transformers\n# !python -m pip install --upgrade pandas\n# # !pip install fairseq torchviz","6ea5a26c":"import pandas as pd\nimport os\nimport re","a7f77594":"data_path = '\/kaggle\/input\/tweet-sentiment-extraction'\nconfig_path = '\/kaggle\/input\/submission'","5ad19839":"train_dataframe = pd.read_csv(os.path.join(data_path, 'train.csv'))\ntest_dataframe = pd.read_csv(os.path.join(data_path, 'test.csv'))","67964ab3":"train_dataframe.head(5)","2874de5b":"test_dataframe.head(5)","f22f0e95":"import torch\nfrom transformers.tokenization_roberta import RobertaTokenizerFast\nimport re\nimport numpy as np\n\nrob_distil_tozer = RobertaTokenizerFast(vocab_file=os.path.join(config_path, 'roberta-base-vocab.json'),\n                                        merges_file=os.path.join(config_path, 'roberta-base-merges.txt'),\n                                        add_prefix_space=True)","d3b3179f":"def robdist_tokenize(tweet, tozer = None, offset_map=True):\n  if not isinstance(tweet, str):\n    raise TypeError\n  if tozer is not None and not isinstance(tozer, RobertaTokenizerFast):\n    raise TypeError\n\n  temp = tozer.encode_plus(tweet, return_offsets_mapping=offset_map, add_special_tokens=True) if tozer is not None else rob_distil_tozer.encode_plus(tweet, return_offsets_mapping=offset_map, add_special_tokens=True)\n\n  return {'input_ids': torch.tensor(temp['input_ids']),\n          'att_mask': torch.tensor(temp['attention_mask']),\n          'offsets': temp['offset_mapping']}\n\n\ndef robdist_detokenize(token_ids, tozer = None, skip_special_tok=True):\n  if not isinstance(token_ids, torch.LongTensor) and not isinstance(token_ids, torch.cuda.LongTensor):\n    raise TypeError('First argument is not type tensor.long')\n  \n  if tozer is not None and isinstance(tozer, RobertaTokenizerFast):\n    raise TypeError('Second argument is not type TokenizerFast')\n\n  return tozer.decode(token_ids.tolist(), skip_special_tokens=skip_special_tok)[1:] if tozer is not None else rob_distil_tozer.decode(token_ids.tolist(), skip_special_tokens=skip_special_tok)[1:]","5412ccce":"def offset_paddings(offset: list, maxtweetlen, prefix_space=3):\n  try:\n    assert offset[0] == offset[-1]\n  except AssertionError:\n    raise AssertionError(\"Offsets already padded or None type at tails!\")\n\n  offset[0] = (offset[1][0], offset[1][0])\n\n  while offset[-1] is None or offset[-1] == (0, 0):\n    del offset[-1]\n\n  offset = offset + [(offset[-1][-1],offset[-1][-1]) for i in range(maxtweetlen - len(offset) - prefix_space)]\n  offset = [(0,0) for i in range(prefix_space)] + offset\n\n  return torch.tensor(offset)","370a18b4":"def sentiment_padding(tweet, sent_st, max_tweet_len):\n    tweet_dump = \" \" + str(' '.join(tweet.split()))\n    sent_st_dump = \" \" + str(' '.join(sent_st.split()))\n    st_tensor = torch.zeros(max_tweet_len, dtype=torch.long)\n    st_len = len(sent_st_dump)\n    idx_start = None\n    idx_end = None\n    \n    \n    for idx in (idx for idx, letter in enumerate(tweet_dump[1:]) if letter==sent_st_dump[1]):\n        if tweet_dump[idx + 1:idx + st_len] == sent_st_dump[1:]:\n            idx_start = idx + 1\n            idx_end = idx + st_len ### INCLUSIVE GATING!!!\n            st_tensor[idx_start: idx_end + 1] = st_tensor[idx_start: idx_end + 1] + 1\n            break\n        \n    # sanity check\n    try:\n        assert (idx_start != None and idx_end != None)\n        assert (st_len != 0)\n    except AssertionError:\n        print('sentiment_padding : Either improper selected sentiment support or sentiment support doesnt have length!')\n        return -1\n    \n    return [idx_start], [idx_end], st_tensor","04aabf52":"def data_preprocess(df, max_tweet_len, datasets='test', token_gap=0, debug_mode=False, offset_return=True):\n    \n    try:\n        assert (type(df) == pd.DataFrame)\n    except AssertionError:\n        raise AssertionError(\"Input dataframe is not type pandas.DataFrame!\")\n\n\n    # Sentiment map\n    dict_sentiment = {'positive': robdist_tokenize('positive')['input_ids'],\n                      'negative': robdist_tokenize('negative')['input_ids'],\n                      'neutral': robdist_tokenize('neutral')['input_ids']}\n    enc_sentiment_len = len(dict_sentiment['neutral'])\n    \n    # Init data tensor, generic\n    zero_temp = robdist_tokenize(\" \" + str(' '.join(str(df.iloc[0]['text']).split())))\n    df_BPE = torch.cat((dict_sentiment[df.iloc[0]['sentiment']], zero_temp['input_ids']), dim=-1)\n    df_att = torch.from_numpy(np.asarray([1]*enc_sentiment_len + [0]*token_gap + [1]*len(zero_temp['input_ids']) + [0]*(max_tweet_len - enc_sentiment_len - token_gap - len(zero_temp['input_ids']))))\n    df_tok = torch.from_numpy(np.asarray([0]*(enc_sentiment_len + token_gap) + [1]*len(zero_temp['input_ids']) + [0]*(max_tweet_len - enc_sentiment_len - token_gap - len(zero_temp['input_ids']))))\n    df_textid = robdist_tokenize(str(df.iloc[0]['textID']))['input_ids']\n    df_offsets = offset_paddings(zero_temp['offsets'], max_tweet_len) # max_tweet_len x 2 ; start and end - 1\n\n    if df_BPE.shape[0] < max_tweet_len:\n        df_BPE = torch.cat((df_BPE, torch.zeros(max_tweet_len - df_BPE.shape[0], dtype=torch.long)), -1)\n    else:\n        df_BPE = df_BPE[:max_tweet_len]\n    \n    if df_textid.shape[0] < max_tweet_len:\n        df_textid = torch.cat((df_textid, torch.zeros(max_tweet_len - df_textid.shape[0], dtype=torch.long)), -1)\n    else:\n        df_textid = df_textid[:max_tweet_len]\n\n    ##### Additional init for training set ####\n    if datasets=='train':\n        init_search = True\n        targetidx_1, targetidx_2, _ = sentiment_padding(' '.join(df.iloc[0]['text'].split()), ' '.join(df.iloc[0]['selected_text'].split()), max_tweet_len)\n        for i in range(df_offsets.shape[0]):\n            if init_search and df_offsets[i, 0] <= targetidx_1[0] < df_offsets[i, 1]:\n                init_search = False\n                targetidx_1 = [i]\n            if df_offsets[i, 0] <= targetidx_2[0] <= df_offsets[i, 1]:\n                targetidx_2 = [i]\n                break\n\n    #### Converting to higher Dim Tensor for concatenating ####\n    df_att = df_att.unsqueeze(dim=-1)\n    df_tok = df_tok.unsqueeze(dim=-1)\n    df_BPE = df_BPE.unsqueeze(dim=-1)\n    df_textid = df_textid.unsqueeze(dim=-1)\n    df_offsets = df_offsets.unsqueeze(dim=0)\n      \n    \n    # Tokenize data text\n    for idx in range(1, len(df)):\n        tempstring = str(' '.join(str(df.iloc[idx]['text']).split()))\n        if len(tempstring) < 1:\n            continue\n        ins_ids = robdist_tokenize(\" \" + str(' '.join(str(df.iloc[idx]['text']).split())))\n        text_unique_ids = robdist_tokenize(str(df.iloc[idx]['textID']))['input_ids']\n        temp_token = torch.cat((dict_sentiment[df.iloc[idx]['sentiment']], ins_ids['input_ids']), dim=-1)\n        temp_att = torch.from_numpy(np.asarray([1]*enc_sentiment_len + [0]*token_gap + [1]*len(ins_ids['input_ids'])+ [0]*(max_tweet_len - enc_sentiment_len - token_gap - len(ins_ids['input_ids']))))\n        temp_tok = torch.from_numpy(np.asarray([0]*(enc_sentiment_len + token_gap) + [1]*len(ins_ids['input_ids'])+ [0]*(max_tweet_len - enc_sentiment_len - token_gap - len(ins_ids['input_ids']))))\n        \n        if temp_token.shape[0] < max_tweet_len:\n            temp_token = torch.cat((temp_token, torch.zeros(max_tweet_len - temp_token.shape[0], dtype=torch.long)), dim=0)\n        else:\n            temp_token = temp_token[:max_tweet_len]\n\n        if text_unique_ids.shape[0] < max_tweet_len:\n            text_unique_ids = torch.cat((text_unique_ids, torch.zeros(max_tweet_len - text_unique_ids.shape[0], dtype=torch.long)), dim=0)\n        else:\n            text_unique_ids = text_unique_ids[:max_tweet_len]\n        \n        df_BPE = torch.cat((df_BPE, temp_token.unsqueeze(dim=-1)), dim=-1)\n        df_textid = torch.cat((df_textid, text_unique_ids.unsqueeze(dim=-1)), dim=-1)\n        df_att = torch.cat((df_att, temp_att.unsqueeze(dim=-1)), dim=-1)\n        df_tok = torch.cat((df_tok, temp_tok.unsqueeze(dim=-1)), dim=-1)\n        df_offsets = torch.cat((df_offsets, offset_paddings(ins_ids['offsets'], max_tweet_len).unsqueeze(dim=0)), dim=0)\n        \n        ##### Separator for training set ####\n        if datasets=='train':\n            init_search = True\n            temp_idx1, temp_idx2, _ = sentiment_padding(' '.join(str(df.iloc[idx]['text']).split()), ' '.join(str(df.iloc[idx]['selected_text']).split()), max_tweet_len)\n            for i in range(df_offsets.shape[1]):\n                if init_search and df_offsets[idx, i, 0] <= temp_idx1[0] < df_offsets[idx, i, 1]:\n                    init_search = False\n                    temp_idx1 = [i]\n                if df_offsets[idx, i, 0] <= temp_idx2[0] <= df_offsets[idx, i, 1]:\n                    temp_idx2 = [i]\n                    break\n            targetidx_1 = targetidx_1 + temp_idx1\n            targetidx_2 = targetidx_2 + temp_idx2\n            \n\n    if datasets=='test':\n        return {\n            'BPEencoded': df_BPE.T,\n            'textID': df_textid.T,\n            'att_mask': df_att.T,\n            'tok_typeids': df_tok.T,\n            'offsets': df_offsets\n        }\n    elif datasets=='train':\n        return {\n            'BPEencoded': df_BPE.T,\n            'textID': df_textid.T,\n            'target_idx': torch.from_numpy(np.stack((np.asarray(targetidx_1), np.asarray(targetidx_2)), axis=0)).T,\n            'att_mask': df_att.T,\n            'tok_typeids': df_tok.T,\n            'offsets': df_offsets\n        }\n    else:\n        return -1","489075d2":"import pandas as pd\nimport os\nimport torch\nimport re\nimport numpy as np\nimport transformers\n\ndata_path = '\/kaggle\/input\/tweet-sentiment-extraction'\nconfig_path = '\/kaggle\/input\/submission'\nprep_out_path = '\/kaggle\/working'\ntemporary_tweet = '\/kaggle\/input\/roberta-openai'","7ce924f5":"def stackup_ids(input_ids, mask_ids, tokentype_ids, textID_ids, offsets, bertmodel='roberta'):\n    try:\n        assert (input_ids.shape == mask_ids.shape == tokentype_ids.shape)\n        assert (input_ids.shape[0] ==  textID_ids.shape[0])\n    except AssertionError:\n        raise AssertionError('stackup_ids: arguments do not have matched shape!')\n    \n    token_type_modifier = tokentype_ids*int(0) if bertmodel=='roberta' else tokentype_ids\n    \n    cat_out = torch.cat((input_ids.unsqueeze(dim=-1),\n                         mask_ids.long().unsqueeze(dim=-1),\n                         token_type_modifier.long().unsqueeze(dim=-1),\n                         textID_ids.unsqueeze(dim=-1),\n                         offsets.long()), dim=-1) # casting manual nparray to long for compatibility purpose\n\n    return cat_out","6c1435a9":"offnum = np.random.randint(0,27450,(1,))[0]\ngetnum = 10 # len(train_dataframe) - offnum - 1\ntestdatacheck = data_preprocess(train_dataframe.loc[offnum:offnum + getnum], 192, datasets='train')\nprint(str(train_dataframe.loc[offnum:offnum + getnum]['text']))\nprint()\nprint(testdatacheck['textID'].shape)\nprint()\nprint(testdatacheck['target_idx'])\nprint()\nprint(testdatacheck['target_idx'].shape)\nprint()\nprint(train_dataframe.loc[offnum:offnum + getnum]['textID'])\nprint()\nprint(train_dataframe.loc[offnum:offnum + getnum]['text'])\nprint()\nprint(train_dataframe.loc[offnum:offnum + getnum]['selected_text'])\nprint()\nprint('\\n'.join([robdist_detokenize(testdatacheck['textID'][i,:]) for i in range(len(testdatacheck['textID']))]))\nprint()\nprint(testdatacheck['offsets'].shape)\ntestdatastack = stackup_ids(testdatacheck['BPEencoded'],\n                            testdatacheck['att_mask'],\n                            testdatacheck['tok_typeids'],\n                            testdatacheck['textID'],\n                            testdatacheck['offsets'])\nprint()\nprint(testdatastack.shape)\nprint()\nrandidx = np.random.randint(0,getnum + 1,(1,))[0]\ntextIDdecode = testdatastack[randidx,:,-3].squeeze(dim=0)\nprint(testdatastack[randidx, :40, :])\nprint()\nprint(testdatacheck['target_idx'][randidx, :])\nprint()\n# sentiment support\nprint(str(\" \" + robdist_detokenize(testdatastack[randidx, 3:, 0]))[testdatastack[randidx, testdatacheck['target_idx'][randidx, 0], -2]:testdatastack[randidx, testdatacheck['target_idx'][randidx, 1], -1]])\nprint()\nprint(robdist_detokenize(textIDdecode))\nprint()\nprint(train_dataframe.loc[offnum + randidx:offnum + randidx]['textID'].item())\nprint()\nprint('textID matched!' if train_dataframe.loc[offnum + randidx:offnum + randidx]['textID'].item() == robdist_detokenize(textIDdecode) else 'textID do not match')\n\ndel testdatacheck, testdatastack, randidx, textIDdecode, offnum, getnum","aacf36bc":"from torch.utils.data import TensorDataset, DataLoader\nimport pickle\nimport torch.optim as optim\nimport torch.nn as nn\n\n# GLOBAL HYPERPARAMETER #1 for data\nMAX_ORIG_TWEET_LEN = 192 # Changing this requires deleting previous pre-processed dataset and ensure it is value is more than max len of entire datasets 'text', it will throw error in padding and data preprocessing\nBATCH_SIZE = 32\ndev = 'cuda' if torch.cuda.is_available() else 'cpu'\nfold_run = 1\/0.05\n\n\ntry:\n    train_preprocess, test_preprocess = pickle.load(open(os.path.join(temporary_tweet, 'tweet_preprocess.p'), mode='rb'))\n    print(\"Pre-processed tweet dataset loaded!\")\nexcept FileNotFoundError:\n    print(\"Initializing\/Repeating preprocess of data!\")\n    train_preprocess = data_preprocess(train_dataframe, MAX_ORIG_TWEET_LEN, datasets='train')\n    test_preprocess = data_preprocess(test_dataframe, MAX_ORIG_TWEET_LEN, datasets='test')\n    pickle.dump((train_preprocess, test_preprocess), open(os.path.join(prep_out_path, 'tweet_preprocess.p'), 'wb'))\n    print('Preprocessing completed and saved the data!')\n\n# Validation set slicing   \nval_ratio = float(1\/fold_run)\nval_slice_idx = int((1-val_ratio)*train_preprocess['BPEencoded'].shape[0])\n\ntrain_x_ids = stackup_ids(train_preprocess['BPEencoded'][0:val_slice_idx, :],\n                          train_preprocess['att_mask'][0:val_slice_idx, :].long(),\n                          train_preprocess['tok_typeids'][0:val_slice_idx, :].long(),\n                          train_preprocess['textID'][0:val_slice_idx, :],\n                          train_preprocess['offsets'][0:val_slice_idx, :].long())\n\ntrain_y = train_preprocess['target_idx'][0:val_slice_idx, :]\n\nval_x_ids = stackup_ids(train_preprocess['BPEencoded'][val_slice_idx:, :],\n                        train_preprocess['att_mask'][val_slice_idx:, :].long(),\n                        train_preprocess['tok_typeids'][val_slice_idx:, :].long(),\n                        train_preprocess['textID'][val_slice_idx:, :],\n                        train_preprocess['offsets'][val_slice_idx:, :].long())\nval_y = train_preprocess['target_idx'][val_slice_idx:, :]\n\n# Train data loader\ntrain_x_ids_dataset = TensorDataset(train_x_ids, train_y)\ntrain_loader = DataLoader(train_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n\n# Validation data loader\nval_x_ids_dataset = TensorDataset(val_x_ids, val_y)\nval_loader = DataLoader(val_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)","b98e1669":"print(len(train_x_ids_dataset))\nprint(len(val_x_ids_dataset))","90a92d5d":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef char_jaccard_seq(str1, str2):\n    jaccscore = 0.0\n    a = list(''.join(str1.lower().split()))\n    b = list(''.join(str2.lower().split()))\n    minlen = len(a) if len(a) < len(b) else len(b)\n    \n    for idx in range(minlen):\n        if a[idx] == b[idx]:\n            jaccscore += 1\n    \n    return (jaccscore*2)\/(len(a) + len(b))","1922d939":"stopper= 1\n\ndevv = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nfor i, batch in enumerate(train_loader):\n  if stopper < 1:\n    break\n  batchx, batchy = batch\n  batchx, batchy = batchx.to(devv), batchy.to(devv)\n  stopper -= 1\n\ntextidd = robdist_detokenize(batchx[0,:,3])\nprint(train_dataframe.loc[train_dataframe['textID'] == textidd]['text'].item())\nprint(robdist_detokenize(batchx[0,:,0]))\nprint(batchy[0,:])\nprint('---------------------------------------------------------------------------------------------------------------')\ntrain1 = train_dataframe.loc[train_dataframe['textID'] == textidd]['selected_text'].item()\ntrain2 = str(\" \" + ' '.join(train_dataframe.loc[train_dataframe['textID'] == textidd]['text'].item().split()))[batchx[0,:,-2:][batchy[0,0], 0]:batchx[0,:,-2:][batchy[0,1], 1]]\nprint(train1)\nprint(train2)\nprint(f'Jaccard {jaccard(train1,train2)}')\nprint('---------------------------------------------------------------------------------------------------------------')\nprint(batchx[0,:40,-2:].squeeze(dim=0).T)\nprint(batchx[0,:50,:])\n\ndel devv, textidd, train1, train2","15b5ee40":"# Packages and path\nfrom transformers.modeling_roberta import RobertaModel\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.modeling_bert import BertPreTrainedModel\nimport torch.nn as nn\n\nroberta_path = config_path\nmodel_in = '\/kaggle\/input\/submission\/model_colab_24.pt'","f2911da1":"# see_state = torch.load(model_in) if dev == 'cuda' else torch.load(model_in, map_location='cpu')\n# for i,e in see_state.items():\n#     if i == 'model':\n#         pass\n#     else:\n#         print(f'{i} : {e}')\n# del see_state","34442274":"def weights_init(mods):\n  if type(mods) == nn.Linear:\n    torch.nn.init.normal_(mods.weight, mean= 0.0, std=0.1)\n\n\nclass roberta_mlp_net(BertPreTrainedModel):\n\n    def __init__(self, robpath, net_config, maxtweetlen):\n      super(roberta_mlp_net, self).__init__(net_config)\n      net_config.output_hidden_states = True\n      self.roberta = RobertaModel.from_pretrained(pretrained_model_name_or_path=robpath, config=net_config)\n      self.drop_out1 = nn.Dropout(p=0.1)\n      self.laynorm = nn.LayerNorm((768*2,))\n      self.densenet = nn.Linear(768*2, 2)\n      self.densenet.apply(weights_init)\n\n    def todevice(self):\n      device = 'cuda' if torch.cuda.is_available() else 'cpu'\n      self.to(device)\n      print(f'Processing device: {device}')\n    \n    def forward(self, in_ids, att_ids, tok_ids):\n      _ , _, out = self.roberta(input_ids=in_ids, attention_mask=att_ids, token_type_ids=tok_ids) # last hidden, pooled bert cls, (embed output + all hidden states) #12 bertlayers for roberta openai\n      out = torch.cat((out[-1], out[-2]), dim=-1) #last and second last hidden output\n      out = self.drop_out1(out)\n      out = self.laynorm(out)\n      out = self.densenet(out)\n\n      return out","adf385ce":"# GLOBAL HYPERPARAMETER #2 for initial training\nlearn_rate = 80e-7\ncriterion_mod = nn.CrossEntropyLoss()\nglobal_threshold_valid = np.Inf\nn_epoch = 30\nprint_every = 100\n\nmodel_config = PretrainedConfig.from_json_file(os.path.join(roberta_path, 'roberta-base-openai-detector-config.json'))\ntestmodel = roberta_mlp_net(robpath=roberta_path, net_config=model_config, maxtweetlen=MAX_ORIG_TWEET_LEN)\noptimizer_mod = optim.AdamW(testmodel.parameters(), lr=learn_rate)\noptimizer_mod.param_groups[0]['amsgrad'] = False\n\nif dev == 'cuda':\n    testmodel.load_state_dict(torch.load(model_in)['model'])\nelse:\n    testmodel.load_state_dict(torch.load(model_in, map_location='cpu')['model'])","0f85fee8":"print(testmodel)\nprint(model_config)","541c4770":"def len_mismatch_penalty(pred_start, pred_end, tar_start, tar_end, coeff_mult=0.1):\n    _, start_cand = pred_start.squeeze(dim=-1).topk(1, dim=-1)\n    _, end_cand = pred_end.squeeze(dim=-1).topk(1, dim=-1)\n    start_cand = start_cand.squeeze(dim=-1)\n    end_cand = end_cand.squeeze(dim=-1)\n    diff_cand = abs(end_cand - start_cand) - (tar_end - tar_start)\n    result = torch.matmul(diff_cand.float(), diff_cand.float().T)\n    return result*coeff_mult","b9e8b540":"def train_func(fold, model, dev, optimizer, trainloader, validloader, tweetlen, criterion, epochs, glob_th, verbose_every=10, first_train=True):\n  valid_loss_min = np.Inf\n  init_train = True\n  early_stop = 2\n  prev_val_loss = np.Inf\n\n  model.todevice()\n  for epoch in range(1, epochs + 1):\n    train_loss = 0.0\n    valid_loss = 0.0\n\n    if 3 < epoch < 5:\n      optimizer.param_groups[0]['lr'] = 0.8*optimizer.param_groups[0]['lr']\n    elif 5 < epoch < 10:\n      optimizer.param_groups[0]['lr'] = 0.8*optimizer.param_groups[0]['lr']\n    elif epoch > 10:\n      optimizer.param_groups[0]['lr'] = 0.8*optimizer.param_groups[0]['lr']\n\n    ##### Training #####\n    model.train()\n    print(f'Fold {fold} : Training of epoch #{epoch}...')\n    for batch_idx, (trainx, trainy) in enumerate(trainloader):\n      model.zero_grad()\n      trainx, trainy = trainx.to(dev), trainy.to(dev)\n      output = model(trainx[:, :, 0], trainx[:, :, 1], trainx[:, :, 2])\n      loss = (criterion(output[:, :, 0], trainy[:, 0]) + criterion(output[:, :, 1], trainy[:, 1]))\n      loss.backward()\n      optimizer.step()\n      train_loss += loss.item()\n\n      if batch_idx % verbose_every == 0 and batch_idx != 0:\n        print(f'Training loss at epoch # {epoch}: {train_loss\/(verbose_every)}')\n        train_loss = 0.0\n    \n    if init_train and not first_train:\n      valid_loss_min = glob_th\n      init_train = False\n\n    #### Validation ####\n    model.eval()\n    print(f'Validation of epoch #{epoch}...')\n    for batch_idx, (valx, valy) in enumerate(validloader):\n      valx, valy = valx.to(dev), valy.to(dev)\n      with torch.no_grad():\n        val_output = model(valx[:, :, 0], valx[:, :, 1], valx[:, :, 2])\n        vloss = criterion(val_output[:, :, 0], valy[:, 0]) + criterion(val_output[:, :, 1], valy[:, 1])\n        valid_loss += vloss.item()\n    \n    valid_loss = valid_loss\/len(validloader)\n    print('Validation loss : {:.6f}'.format(valid_loss))\n    print('Validation loss min : {:.6f}'.format(valid_loss_min))\n        \n    if valid_loss < valid_loss_min:\n      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n      valid_loss_min = valid_loss\n      glob_th = valid_loss\n      checkpoint_model = {'architecture': save_architecture,\n                          'learnrate': learn_rate,\n                          'validmin': valid_loss_min,\n                          'model': model.state_dict()}\n      torch.save(checkpoint_model, os.path.join(model_out))\n      early_stop = 2\n    else:\n      if prev_val_loss < valid_loss:\n        early_stop -= 1\n      else:\n        early_stop = 2\n    \n    if early_stop < 0:\n      break\n    prev_val_loss = valid_loss\n  \n  checkpoint_model = {'architecture': save_architecture,\n                      'learnrate': learn_rate,\n                      'validmin': glob_th,\n                      'model': model.state_dict()}\n  torch.save(checkpoint_model, os.path.join(model_last_out))\n\n  print('Training completed!')\n  return model, glob_th","4f92efd2":"testmodel, global_threshold_valid = train_func(fold=0,\n                                               model=testmodel,\n                                               dev=dev,\n                                               optimizer=optimizer_mod,\n                                               trainloader=train_loader,\n                                               validloader=val_loader,\n                                               tweetlen=MAX_ORIG_TWEET_LEN,\n                                               criterion=criterion_mod,\n                                               epochs=n_epoch,\n                                               glob_th=global_threshold_valid,\n                                               verbose_every=print_every,\n                                               first_train=False)","3686dad7":"# # Continued run\n\n# restartidx = 2\n\n# global_threshold_valid = 1.743169\n\n# val_ratio = float(1\/fold_run)\n# val_slice_idx = int((1-val_ratio)*train_preprocess['BPEencoded'].shape[0])\n# init_slice = val_slice_idx - int((restartidx + 1)*val_ratio*train_preprocess['BPEencoded'].shape[0])\n# end_slice =  val_slice_idx - int(restartidx*val_ratio*train_preprocess['BPEencoded'].shape[0])\n\n\n# train_x_ids = stackup_ids(torch.cat((train_preprocess['BPEencoded'][0:init_slice, :],train_preprocess['BPEencoded'][end_slice:, :]), dim=0),\n#                           torch.cat((train_preprocess['att_mask'][0:init_slice, :],train_preprocess['att_mask'][end_slice:, :]), dim=0),\n#                           torch.cat((train_preprocess['tok_typeids'][0:init_slice, :],train_preprocess['tok_typeids'][end_slice:, :]), dim=0),\n#                           torch.cat((train_preprocess['textID'][0:init_slice, :],train_preprocess['textID'][end_slice:, :]), dim=0),\n#                           torch.cat((train_preprocess['offsets'][0:init_slice, :], train_preprocess['offsets'][end_slice:, :]), dim=0))\n\n# train_y = torch.cat((train_preprocess['target_idx'][0:init_slice, :],train_preprocess['target_idx'][end_slice:, :]), dim=0)\n\n# val_x_ids = stackup_ids(train_preprocess['BPEencoded'][init_slice:end_slice, :],\n#                         train_preprocess['att_mask'][init_slice:end_slice, :],\n#                         train_preprocess['tok_typeids'][init_slice:end_slice, :],\n#                         train_preprocess['textID'][init_slice:end_slice, :],\n#                         train_preprocess['offsets'][init_slice:end_slice, :])\n\n# val_y = train_preprocess['target_idx'][init_slice:end_slice, :]\n\n# # Train data loader\n# train_x_ids_dataset = TensorDataset(train_x_ids, train_y)\n# train_loader = DataLoader(train_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n\n# # Validation data loader\n# val_x_ids_dataset = TensorDataset(val_x_ids, val_y)\n# val_loader = DataLoader(val_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)","18e2ecb6":"# for i in range(0, fold_run - 1):\n#     # Model reinstantiation for next fold\n#     del model_config, testmodel, criterion_mod, optimizer_mod\n#     model_config = PretrainedConfig.from_json_file(os.path.join(roberta_path, 'roberta-base-openai-detector-config.json'))\n#     testmodel = roberta_mlp_net(robpath=roberta_path, net_config=model_config, maxtweetlen=MAX_ORIG_TWEET_LEN)\n#     # testmodel.load_state_dict(torch.load(model_out)['model'])\n#     # testmodel.load_state_dict(torch.load(model_in, map_location='cpu')['model'])\n#     criterion_mod = nn.CrossEntropyLoss()\n#     optimizer_mod = optim.AdamW(testmodel.parameters(), lr=learn_rate)\n#     testmodel, global_threshold_valid = train_func(i, testmodel, dev, optimizer_mod, train_loader, val_loader, MAX_ORIG_TWEET_LEN, criterion_mod, n_epoch, global_threshold_valid, print_every, first_train=False)\n    \n#     val_ratio = float(1\/fold_run)\n#     val_slice_idx = int((1-val_ratio)*train_preprocess['BPEencoded'].shape[0])\n#     init_slice = val_slice_idx - int((i + 1)*val_ratio*train_preprocess['BPEencoded'].shape[0])\n#     end_slice =  val_slice_idx - int(i*val_ratio*train_preprocess['BPEencoded'].shape[0])\n    \n#     if i < 3:\n#         train_x_ids = stackup_ids(torch.cat((train_preprocess['BPEencoded'][0:init_slice, :],train_preprocess['BPEencoded'][end_slice:, :]), dim=0),\n#                                   torch.cat((train_preprocess['att_mask'][0:init_slice, :],train_preprocess['att_mask'][end_slice:, :]), dim=0),\n#                                   torch.cat((train_preprocess['tok_typeids'][0:init_slice, :],train_preprocess['tok_typeids'][end_slice:, :]), dim=0),\n#                                   torch.cat((train_preprocess['textID'][0:init_slice, :],train_preprocess['textID'][end_slice:, :]), dim=0),\n#                                   torch.cat((train_preprocess['offsets'][0:init_slice, :], train_preprocess['offsets'][end_slice:, :]), dim=0))\n\n#         train_y = torch.cat((train_preprocess['target_idx'][0:init_slice, :],train_preprocess['target_idx'][end_slice:, :]), dim=0)\n\n#         val_x_ids = stackup_ids(train_preprocess['BPEencoded'][init_slice:end_slice, :],\n#                                 train_preprocess['att_mask'][init_slice:end_slice, :],\n#                                 train_preprocess['tok_typeids'][init_slice:end_slice, :],\n#                                 train_preprocess['textID'][init_slice:end_slice, :],\n#                                 train_preprocess['offsets'][init_slice:end_slice, :])\n        \n#         val_y = train_preprocess['target_idx'][init_slice:end_slice, :]\n\n#         # Train data loader\n#         train_x_ids_dataset = TensorDataset(train_x_ids, train_y)\n#         train_loader = DataLoader(train_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n        \n#         # Validation data loader\n#         val_x_ids_dataset = TensorDataset(val_x_ids, val_y)\n#         val_loader = DataLoader(val_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n#     else:\n#         train_x_ids = stackup_ids(train_preprocess['BPEencoded'][end_slice:, :],\n#                                   train_preprocess['att_mask'][end_slice:, :],\n#                                   train_preprocess['tok_typeids'][end_slice:, :],\n#                                   train_preprocess['textID'][end_slice:, :],\n#                                   train_preprocess['offsets'][end_slice:, :])\n\n#         train_y = torch.cat((train_preprocess['target_idx'][end_slice:, :],train_preprocess['target_idx'][end_slice:, :]), dim=0)\n\n#         val_x_ids = stackup_ids(train_preprocess['BPEencoded'][0:end_slice, :],\n#                                 train_preprocess['att_mask'][0:end_slice, :],\n#                                 train_preprocess['tok_typeids'][0:end_slice, :],\n#                                 train_preprocess['textID'][0:end_slice, :],\n#                                 train_preprocess['offsets'][0:end_slice, :])\n        \n#         val_y = train_preprocess['target_idx'][0:end_slice, :]\n\n#         # Train data loader\n#         train_x_ids_dataset = TensorDataset(train_x_ids, train_y)\n#         train_loader = DataLoader(train_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n        \n#         # Validation data loader\n#         val_x_ids_dataset = TensorDataset(val_x_ids, val_y)\n#         val_loader = DataLoader(val_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n        \n        \n#         # Reinstantiate model for last fold\n#         del model_config, testmodel, criterion_mod, optimizer_mod\n#         model_config = PretrainedConfig.from_json_file(os.path.join(roberta_path, 'roberta-base-openai-detector-config.json'))\n#         testmodel = roberta_mlp_net(robpath=roberta_path, net_config=model_config, maxtweetlen=MAX_ORIG_TWEET_LEN)\n#         criterion_mod = nn.CrossEntropyLoss()\n#         optimizer_mod = optim.AdamW(testmodel.parameters(), lr=learn_rate)\n        \n#         testmodel, global_threshold_valid = train_func(i + 1, testmodel, dev, optimizer_mod, train_loader, val_loader, MAX_ORIG_TWEET_LEN, criterion_mod, n_epoch, global_threshold_valid, print_every, first_train=False)","bacb24ee":"def pp(filtered_output, real_tweet):\n    filtered_output = ' '.join(filtered_output.split())\n    if len(real_tweet.split()) < 2:\n        filtered_output = real_tweet\n    else:\n        if len(filtered_output.split()) == 1:\n            if filtered_output.endswith(\"..\"):\n                if real_tweet.startswith(\" \"):\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n                else:\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '.', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\.)\\1{2,}', '..', filtered_output)\n                return filtered_output\n            if filtered_output.endswith('!!'):\n                if real_tweet.startswith(\" \"):\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n                else:\n                    st = real_tweet.find(filtered_output)\n                    fl = real_tweet.find(\"  \")\n                    if fl != -1 and fl < st:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!', filtered_output)\n                    else:\n                        filtered_output = re.sub(r'(\\!)\\1{2,}', '!!', filtered_output)\n                return filtered_output\n\n        if real_tweet.startswith(\" \"):\n            filtered_output = filtered_output.strip()\n            text_annotetor = ' '.join(real_tweet.split())\n            start = text_annotetor.find(filtered_output)\n            end = start + len(filtered_output)\n            start -= 0\n            end += 2\n            flag = real_tweet.find(\"  \")\n            if flag < start:\n                filtered_output = real_tweet[start:end]\n\n        if \"  \" in real_tweet and not real_tweet.startswith(\" \"):\n            filtered_output = filtered_output.strip()\n            text_annotetor = re.sub(\" {2,}\", \" \", real_tweet)\n            start = text_annotetor.find(filtered_output)\n            end = start + len(filtered_output)\n            start -= 0\n            end += 2\n            flag = real_tweet.find(\"  \")\n            if flag < start:\n                filtered_output = real_tweet[start:end]\n    return filtered_output","6491d64d":"def eval_jacc(model, dev, decoder, testloader, criterion, dataframe, enable_log=True, is_test=False, every_batch=10):\n  model.eval()\n  model.to(dev)\n  instance_count = 0\n  test_loss = 0.0\n  jacc_score_overall = 0.0\n  pred_seltext_global = []\n  batch_textid_global = []\n  batch_text_global = []\n  sentiment_global = []\n    \n  for batchidx, batch in enumerate(testloader):\n    pred_seltext = []\n    select_text = []\n    if batchidx % every_batch == every_batch - 1:\n      print(f'Dumping eval of {batchidx + 1}-th batch...')\n    if not is_test:\n        testx, testy = batch\n        testx, testy = testx.to(dev), testy.to(dev)\n    else:\n        testx = batch[0]\n        testx = testx.to(dev)\n    \n    test_offsets = testx[:,:, -2:] # Getting last two columns of x tensors, offset positions of each words\n\n    \n    batch_textid_tensor = testx[:, :, 3].to('cpu').long() # Roberta-encoded text ID, column 3 of x tensors\n    batch_textid = [decoder(batch_textid_tensor[i,:]) for i in range(batch_textid_tensor.shape[0])]\n    batch_textid_global = batch_textid_global + batch_textid\n    sentiment_text = [decoder(testx[i, :3, 0].to('cpu').long()) for i in range(batch_textid_tensor.shape[0])]\n    sentiment_global = sentiment_global + sentiment_text\n    \n           \n    batch_text = [str(\" \" + ' '.join(str(dataframe.loc[dataframe['textID'] == batch_textid[i]]['text'].item()).split())) for i in range(len(batch_textid))]\n    batch_text_global = batch_text_global + batch_text\n\n    if not is_test:\n      batch_gnd_seltext = [str(dataframe.loc[dataframe['textID'] == batch_textid[i]]['selected_text'].item()) for i in range(len(batch_textid))] # very raw, no split and prefix addition\n      for idx, item in enumerate(batch_text):\n\n        sel_idstart = test_offsets[idx, testy[idx, 0].item(), 0].item()\n        sel_idend = test_offsets[idx, testy[idx, 1].item(), 1].item()\n        select_text = select_text + [batch_gnd_seltext[idx]]\n\n\n    with torch.no_grad():\n        testoutput = model(testx[:, :, 0], testx[:, :, 1], testx[:, :, 2])\n    \n\n    # Prediction\n    _, pred_starts = testoutput[:, :, 0].squeeze(dim=-1).topk(1, dim=-1) # batch_size indices\n    _, pred_ends = testoutput[:, :, 1].squeeze(dim=-1).topk(1, dim=-1) # batch_size indices\n    \n    for idx, item in enumerate(batch_text):\n      instance_count += 1\n      idstart = test_offsets[idx, pred_starts[idx].item(), 0].item()\n      idend = test_offsets[idx, pred_ends[idx].item(), 1].item()\n\n      if len(item[idstart:idend]) == 0:\n          pred_seltext.append(pp(item,item))\n      else:\n          pred_seltext.append(pp(item[idstart:idend],item))\n\n      if enable_log:\n            print('--------------------------------------------------------------------------------')\n            print('startidx\\t\\t\\t: {0}'.format(pred_starts[idx].item()))\n            print('Sentiment\\t\\t\\t: {0}'.format(sentiment_text[idx]))\n            print('Text\\t\\t\\t\\t: {0}'.format(item))\n            print('Predicted sentiment text\\t: {0}'.format(pred_seltext[-1]))\n            print('Predicted offset tensor\\t\\t: [{0}, {1}]'.format(pred_starts[idx].item(),pred_ends[idx].item()))\n            print('Offset tensors\\t\\t\\t: {0}'.format(test_offsets[idx, :50, :].T))\n      if not is_test and enable_log:\n            print('Actual Offset tensors\\t\\t: {0}'.format(testy[idx, :]))\n            print('Actual sentiment text\\t\\t: {0}'.format(batch_gnd_seltext[idx]))\n            print('Jaccard score\\t\\t\\t: {0}'.format(jaccard(pred_seltext[-1], batch_gnd_seltext[idx])))\n            print('--------------------------------------------------------------------------------')\n    \n    \n      \n    \n    pred_seltext_global = pred_seltext_global + pred_seltext\n    if not is_test:\n      jacc_score = 0.0\n      for idx, seltext in enumerate(pred_seltext):\n        jacc_score += jaccard(seltext,select_text[idx])\n      \n      jacc_score_overall += jacc_score\n      if batchidx % every_batch == every_batch - 1:\n        print(f'Jaccard avg score of {batchidx + 1}-th batch: {jacc_score\/len(pred_seltext)}')\n    \n\n  if not is_test:\n    return {'pred': pred_seltext_global,\n            'textID': batch_textid_global,\n            'text': batch_text_global,\n            'predstart': pred_starts,\n            'predend': pred_ends,\n            'jaccard': jacc_score_overall\/instance_count,\n            'sentiment' : sentiment_global}\n  else:\n    return {'pred': pred_seltext_global,\n            'textID': batch_textid_global,\n            'sentiment' : sentiment_global}","edf06ab2":"small_val_loader = ()\n\nfor i, val in enumerate(val_loader):\n    if i < 1:\n        small_val_loader = small_val_loader + (val,)\n\nprint(str(len(small_val_loader)) + \" batch(es) selected!\")","643ea5ef":"eval_result = eval_jacc(testmodel, dev, robdist_detokenize, small_val_loader, criterion_mod, train_dataframe, enable_log=True, is_test=False, every_batch=50)","0f618410":"eval_result['jaccard']","5e23e400":"test_x_ids = stackup_ids(test_preprocess['BPEencoded'],\n                         test_preprocess['att_mask'],\n                         test_preprocess['tok_typeids'],\n                         test_preprocess['textID'],\n                         test_preprocess['offsets'])\n\n# Validation data loader\ntest_x_ids_dataset = TensorDataset(test_x_ids)\ntest_loader = DataLoader(test_x_ids_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=1)\n","b8a401a5":"testeval_result = eval_jacc(testmodel, dev, robdist_detokenize, test_loader, criterion_mod, test_dataframe, enable_log=False, is_test=True, every_batch=10)\n\n# for manual QC check\ntestoutdataframe = pd.DataFrame(data={'textID': testeval_result['textID'],\n                                      'selected_text': testeval_result['pred'],\n                                      'sentiment': testeval_result['sentiment']})\ntestoutdataframe.to_csv(path_or_buf=os.path.join('\/kaggle\/working', 'testdataset_out.csv'), index=False)\n\n\n# for Kaggle submission\nsubmission = pd.DataFrame(data={'textID': testeval_result['textID'],\n                                'selected_text': testeval_result['pred']})\nsubmission.to_csv(path_or_buf=os.path.join('\/kaggle\/working', 'submission.csv'), index=False)","d78fcc9b":"testoutdataframe.head(50)","2801cb7e":"### Data Tokenization ###","8fbc9b4d":"> Post-processing function that add noise in the prediction to fit 'retarded' targets. Credit to this guy here : [m.y.](https:\/\/www.kaggle.com\/futureboykid)\n\n> But actually not using it for submission, this is just to check if this post processing is a magic, probably not","e72a3ef0":"> Checking jaccard score","f43c58d1":"> Checking stackupids and data_preprocess function:\n>> Required: train_dataframe, data_preprocess, stackupids, sentiment_padding","b3bd7b4e":"> Stackup ids out: torch.tensor: (batch_size, sequence length, 6)\n>> Mapping of 6 elements in 3-rd dimension in positional manner:\n>> - sentence_ids\n>> - mask_ids\n>> - token_type_ids (segment_embeddding ids)\n>> - textID_ids\n>> - Beginning of each word idx in offset tensor\n>> - End of each word idx + 1 in offset tensor","84f7fafd":"> Data Loader sanity check:\n>> Required: train_dataframe, dataloader","815c441b":"## Data Loader Creation ##\n> Making Loader for both training and validation set\n> ### *Checkpoint* ###","e760f1cf":"> Selected sentiment from testloader (preview)","d776cb69":"> Eval function and inferring the _\"humanely selected sentiment tweet\"_","cac9ddb2":"> To run a small amount of val loader for code preview","f11f9502":"> Importing libraries and required word token model","08bc5106":"> Tweet offsets (from tokenizer) padding","955ad535":"> Hybrid roberta with traditional fully dense net, Class definition and inheritance, to be set flexibly with difference json config","c46a1a78":"> Training function and aux util for additional loss function","172741b9":"> Getting Test Set Dataloader","e66491a8":"> Pad the selected text\/tweet and get the letter position of selected tweet in original tweet","35b9a499":"> CV for tuning model (commented since final architecture is obtained)","e79f0b4f":"> Generating Data Loader from all data created with all pre-processing above","f97b8199":"> Jaccard Scoring Function","1e9214e9":"## Model and Training and Hyperparams ##\n> Define net model, loss function, optimizer, epoch and training loop","71692ff8":"> Model Instantiation and Global Hyperparameter #2","beb52d89":"> Data preprocessing with custom token type ids for probably non-roberta embeddings usage","59450282":"> Custom de-\/tokenization functions","a9752907":"> Getting selected tweet from test data"}}