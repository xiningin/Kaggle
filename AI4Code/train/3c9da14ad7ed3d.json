{"cell_type":{"03819514":"code","6d54ca49":"code","906829be":"code","1af91344":"code","1ed9c313":"code","ef542baa":"code","0ae40d70":"code","548e75be":"code","491a3049":"code","8adb32bc":"code","f848a19b":"code","a614cc5a":"code","027c64cd":"code","7941b8b1":"code","c10cfc27":"code","eab0444e":"code","95f01393":"code","b95f970c":"code","5c363b71":"code","42ffcea5":"code","e28b2b10":"code","901bfe4f":"code","090de13f":"code","db10da9c":"code","e6bb6d9f":"code","1ea48dd6":"code","9451cc09":"code","370461b6":"code","872056fb":"code","bad84fdd":"code","74de3677":"code","5e36dd2a":"code","dbcd7a90":"code","1012e010":"code","2f342c54":"code","9f8503b8":"code","b10ff9b7":"code","a74b55c6":"code","b6ac72bb":"code","baef8b99":"code","bb0d35c7":"code","9fa12a59":"code","9e0e8a58":"code","adebe931":"code","5c274410":"code","c20acf95":"code","60604217":"code","d50e2c0f":"code","56777f2f":"code","8c42746a":"markdown","c567e25e":"markdown","81ac4344":"markdown","d1f847b5":"markdown","15d80f0a":"markdown","2fd40613":"markdown","405c1474":"markdown"},"source":{"03819514":"## import all libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","6d54ca49":"train = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")","906829be":"train.shape,test.shape","1af91344":"train.head()","1ed9c313":"test.head()","ef542baa":"train.isnull().sum()","0ae40d70":"train.groupby(['Gender','Var_1'])['Ever_Married'].count().plot(kind='bar')","548e75be":"train.groupby(['Spending_Score','Var_1'])['Ever_Married'].count().plot(kind='bar')","491a3049":"train.isnull().sum()","8adb32bc":"test.isnull().sum()","f848a19b":"# lets create concatenated dataset for easy impute\ndf = pd.concat([train,test])\nprint(df.shape)\ndf.head()","a614cc5a":"# lets take a look into categorical features\ncat_feat = [feat for feat in df.columns if df[feat].dtypes=='O' and feat != 'Segmentation']\nprint(\"Categorical features:\",cat_feat)\nfrom matplotlib import pyplot\nfor f in cat_feat:\n    fig, ax = pyplot.subplots(figsize=(12,4))\n    sns.countplot(df[f],ax=ax)\n    plt.show()","027c64cd":"# lets take a look into numerical features\nnum_feat = [feat for feat in df.columns if df[feat].dtypes !='O' and feat != 'ID']\nprint(\"Categorical features:\",num_feat)\nfrom matplotlib import pyplot\nfor f in num_feat:\n    fig, ax = pyplot.subplots(figsize=(10,4))\n    sns.distplot(df[f],ax=ax)\n    plt.show()","7941b8b1":"#df[num_feat] = np.log(df[num_feat])","c10cfc27":"# fill missing values\ndf['Work_Experience'] = df['Work_Experience'].fillna(df['Work_Experience'].mode()[0])\ndf['Ever_Married'] = df['Ever_Married'].fillna('Missing')\ndf['Graduated'] = df['Graduated'].fillna('Missing')\ndf['Profession'] = df['Profession'].fillna('Missing')\ndf['Family_Size'] = df['Family_Size'].fillna(0)\ndf['Var_1'] = df['Var_1'].fillna('Cat_0')\ndf.head()","eab0444e":"#Adding more Features\ndf['Unique_profession_per_agegroup']=df.groupby(['Age'])['Profession'].transform('nunique')\ndf['Unique_agegroup_per_profession']=df.groupby(['Profession'])['Age'].transform('nunique')\ndf['Age_Family_size']=df.groupby(['Age'])['Family_Size'].transform('nunique')\ndf.head()","95f01393":"df.isnull().sum()","b95f970c":"#df.Segmentation.unique()","5c363b71":"#Encoding Category Variables using frequency encoding\ndef frequency_encoding(col):\n    fe=df.groupby(col).size()\/len(df)\n    df[col]=df[col].apply(lambda x: fe[x])\n    \nfor col in list(df.select_dtypes(include=['object']).columns):\n    if col!='Segmentation':\n        frequency_encoding(col)\n\ndf['Segmentation'] = df['Segmentation'].map({'A':1,'B':2,'C':3,'D':4})","42ffcea5":"df.head()","e28b2b10":"train = df[:8068]\ntest = df[8068:]\nprint(train.shape,test.shape)","901bfe4f":"## 90% Train Data is repeated in Test set so seperating the ID's which are common both in test and train set\n## we will use segements from train data for test id which are common.\n## we will only predict for test ids which are not maching with train data.\nsubmission_df = pd.merge(train,test,on='ID',how='inner')\nsubmission_df=submission_df[['ID','Segmentation_x']]\nsubmission_df.columns=['ID','Segmentation']\nprint(submission_df.shape)\nsubmission_df.head()","090de13f":"### get the test ids which are not found in train data\nmd_df = pd.concat([pd.DataFrame(submission_df['ID']),pd.DataFrame(test['ID'])]).drop_duplicates(keep=False)\nprint(md_df.shape)\nmd_df.head()","db10da9c":"### lets get the whole data columns for these test ids using merge with actual full test file\ntest_df=pd.merge(md_df,test,on='ID',how='inner')\ntest_df","e6bb6d9f":"### create test file for prediction using ensmble methods\ntest_c = test_df.copy()\ntest = test_df.drop(['ID','Segmentation'],axis=1)\n\nX_train = train.drop(['ID','Segmentation'],axis=1)\ny_train = train['Segmentation']","1ea48dd6":"X_train.info()","9451cc09":"# from sklearn.linear_model import Lasso\n# from sklearn.feature_selection import SelectFromModel\n# model = SelectFromModel(Lasso(alpha=0.005,random_state=0))\n# model.fit(X_train,y_train)\n# model.get_support()","370461b6":"# imp = ['Ever_Married','Age','Graduated','Profession','Family_Size']","872056fb":"# from sklearn.model_selection import train_test_split\n# X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=0)","bad84fdd":"# feature1 = ['Gender','Ever_Married','Work_Experience']\n# feature2 = ['Age','Graduated','Profession']\n# feature3 = ['Spending_Score','Family_Size','Var_1']","74de3677":"# X_train1 = X_train[feature1 + feature2]\n# X_train2 = X_train[feature2 + feature3]\n# X_train3 = X_train[feature1 + feature3]\n# X_test1 = test[feature1 + feature2]\n# X_test2 = test[feature2 + feature3]\n# X_test3 = test[feature1 + feature3]","5e36dd2a":"import xgboost\nXGB = xgboost.XGBClassifier(booster='gbtree',verbose=0,learning_rate=0.07,max_depth=8,objective='multi:softmax',\n                  n_estimators=1000,seed=294)\n\nXGB.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred_XGB = XGB.predict(test)\nacc_XGB = round(XGB.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_XGB) + '%')","dbcd7a90":"LGB=LGBMClassifier(boosting_type='gbdt', max_depth=10, learning_rate=0.09, objective='multiclass', reg_alpha=0,\n                  reg_lambda=1, n_jobs=-1, random_state=100, n_estimators=1000)\n\nLGB.fit(X_train,y_train)\n# Predicting the Test set results\ny_pred_LGB = LGB.predict(test)\nacc_LGB = round(LGB.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_LGB) + '%')","1012e010":"from catboost import CatBoostClassifier\n\nCATB=CatBoostClassifier(learning_rate=0.05,depth=8,boosting_type='Plain',eval_metric='Accuracy',n_estimators=1000,random_state=294)\nCATB.fit(X_train,y_train)\n# Predicting the Test set results\ny_pred_CATB = CATB.predict(test)\nacc_CATB = round(CATB.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_CATB) + '%')","2f342c54":"# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n# votingC = VotingClassifier(estimators=[('XGB_1',classifier1),('XGB_2',classifier2),('XGB_3',classifier3)], voting='soft', n_jobs=4)\n\n# votingC = votingC.fit(X_train, y)","9f8503b8":"# vote = votingC.predict(test)\n# submission_v = pd.DataFrame({\n#         \"ID\": test_c[\"ID\"],\n#         \"Segmentation\":vote\n#     })\n\n# submission['Segmentation'] = submission['Segmentation'].map({1:'A',2:'B',3:'C',4:'D'})\n# submission.to_csv('cust_submission.csv', index=False)","b10ff9b7":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import GridSearchCV","a74b55c6":"# import numpy as np\n# from sklearn.model_selection import RandomizedSearchCV\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt','log2']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 50, 5)]\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2,4]\n# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#               'criterion':['entropy','gini']}\n# print(random_grid)\n\n# rf=RandomForestClassifier()\n# rf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=50,cv=3,verbose=2,\n#                                random_state=100,n_jobs=-1)\n# ### fit the randomized model\n# rf_randomcv.fit(X_train,y_train)","b6ac72bb":"#rf_randomcv.best_params_","baef8b99":"#best_random_grid=rf_randomcv.best_estimator_\n#best_random_grid","bb0d35c7":"#rfc = RandomForestClassifier(n_estimators=1000,max_depth=20,random_state=9,verbose=1)\n#rfc.fit(X_train,y_train)\n#y_pred_rfc = rfc.predict(test)\n#score = rfc.score(X_train, y_train)\n#print(score)\n# rfc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#             max_depth=20, max_features='auto', max_leaf_nodes=None,\n#             min_impurity_decrease=0.0, min_impurity_split=None,\n#             min_samples_leaf=4, min_samples_split=10,\n#             min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n#             oob_score=False, random_state=None, verbose=0,\n#             warm_start=False)\n# rfc.fit(X_train,y_train)\n# y_pred_rfc = best_random_grid.predict(test)","9fa12a59":"# #Import knearest neighbors Classifier model\n# from sklearn.metrics import classification_report, confusion_matrix\n# from sklearn.model_selection import cross_val_score\n# from sklearn.neighbors import KNeighborsClassifier\n\n# #Create KNN Classifier\n# knn = KNeighborsClassifier(n_neighbors=4)\n\n# #Train the model using the training sets\n# knn.fit(X_train, y_train)\n\n# #Predict the response for test dataset\n# y_pred_knn = knn.predict(test)","9e0e8a58":"# accuracy_rate = []\n# for i in range(1,40):\n#     knn = KNeighborsClassifier(n_neighbors=i)\n#     score = cross_val_score(knn,X_train, y_train,cv=10)\n#     accuracy_rate.append(score.mean())\n# accuracy_rate","adebe931":"# error_rate = []\n# for i in range(1,40):\n#     knn = KNeighborsClassifier(n_neighbors=i)\n#     score = cross_val_score(knn,X_train, y_train,cv=10)\n#     error_rate.append(1-score.mean())  \n# error_rate","5c274410":"# plt.figure(figsize=(10,6))\n# plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n#         markerfacecolor='red', markersize=10)\n# #plt.plot(range(1,40),accuracy_rate,color='blue', linestyle='dashed', marker='o',\n# #         markerfacecolor='red', markersize=10)\n# plt.title('Error Rate vs. K Value')\n# plt.xlabel('K')\n# plt.ylabel('Error Rate')","c20acf95":"# #Create KNN Classifier\n# knn = KNeighborsClassifier(n_neighbors=10)\n\n# #Train the model using the training sets\n# knn.fit(X_train, y_train)\n\n# #Predict the response for test dataset\n# y_pred_knn = knn.predict(test)","60604217":"d=pd.DataFrame()\nd=pd.concat([d,pd.DataFrame(CATB.predict(test)),pd.DataFrame(XGB.predict(test)),pd.DataFrame(LGB.predict(test))],axis=1)\nd.columns=['1','2','3']\n\nre=d.mode(axis=1)[0]\nre.head()","d50e2c0f":"## create submission Data frame\n\nsubmission = pd.DataFrame({\n        \"ID\": test_c[\"ID\"],\n        \"Segmentation\":re\n    })\n\nsubmission=pd.concat([submission_df,submission])\nsubmission['Segmentation'] = submission['Segmentation'].map({1.0:'A',2.0:'B',3.0:'C',4.0:'D'})\nsubmission.to_csv('Customer_Segmentation_submission.csv', index=False)","56777f2f":"submission.Segmentation.value_counts()","8c42746a":"### LIGHTGBM","c567e25e":"### CATBoost","81ac4344":"### XGBoost","d1f847b5":"### Random forest ","15d80f0a":"## Customer Segmentation\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4 and P5). After intensive market research, they\u2019ve deduced that the behavior of new market is similar to their existing market. \n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for different segment of customers. This strategy has work exceptionally well for them. They plan to use the same strategy on new markets and have identified 2627 new potential customers. ","2fd40613":"### Ensemble of XGBoost, CATBoost, LightGBM","405c1474":"### K-Nearest Neighbor"}}