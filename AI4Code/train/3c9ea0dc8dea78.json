{"cell_type":{"691c9d1c":"code","84a21fcb":"code","23503630":"code","57b1c5da":"code","1ef7ef0c":"code","77beee7a":"code","3692a870":"code","59fe12a8":"code","80895949":"code","7da9ab40":"code","8cb376e7":"code","1f6943e5":"code","aea1581a":"code","b14b5185":"code","112dcb86":"code","b7653845":"code","6c78ede4":"code","66807d2d":"markdown","ca7fbb7c":"markdown","4b41958e":"markdown","2e482fde":"markdown","62d15f14":"markdown"},"source":{"691c9d1c":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport os\nimport gc\nimport datetime\nimport numpy as np\nimport pandas as pd\nfrom catboost import CatBoost, CatBoostClassifier, CatBoostRegressor, Pool\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm\nfrom time import time","84a21fcb":"def create_folds(num_starts, num_splits):\n    \n    folds = []\n    \n    # LOAD FILES\n    train_feats = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n    scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n    drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\n    scored = scored.loc[train_feats['cp_type'] == 'trt_cp', :]\n    drug = drug.loc[train_feats['cp_type'] == 'trt_cp', :]\n    targets = scored.columns[1:]\n    scored = scored.merge(drug, on = 'sig_id', how = 'left') \n\n    # LOCATE DRUGS\n    vc = scored.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    \n    for seed in range(num_starts):\n\n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}; dct2 = {}\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.groupby('drug_id')[targets].mean().loc[vc1]\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits = num_splits, shuffle = True, random_state = seed)\n        tmp = scored.loc[scored.drug_id.isin(vc2)].reset_index(drop = True)\n        for fold,(idxT,idxV) in enumerate(skf.split(tmp,tmp[targets])):\n            dd = {k:fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        scored['fold'] = scored.drug_id.map(dct1)\n        scored.loc[scored.fold.isna(),'fold'] =\\\n            scored.loc[scored.fold.isna(),'sig_id'].map(dct2)\n        scored.fold = scored.fold.astype('int8')\n        folds.append(scored.fold.values)\n        \n        del scored['fold']\n        \n    return np.stack(folds)","23503630":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nss_lr = ss.copy()\n\ncols = [c for c in ss.columns.values if c != 'sig_id']\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","57b1c5da":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n#     df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 0.5, 72: 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\n# def log_loss_metric(y_true, y_pred):\n#     metrics = []\n#     for _target in train_targets.columns:\n#         metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels = [0,1]))\n#     return np.mean(metrics)\n\ndef log_loss_metric(y_true, y_pred):\n    loss = 0\n    y_pred_clip = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    for i in range(y_true.shape[1]):\n        loss += - np.mean(y_true[:, i] * np.log(y_pred_clip[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred_clip[:, i]))\n    return loss \/ y_true.shape[1]\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\ndel train_targets_nonscored['sig_id']","1ef7ef0c":"from sklearn.preprocessing import QuantileTransformer\n\nqt = QuantileTransformer(output_distribution = 'normal', random_state = 42)\nqt.fit(pd.concat([pd.DataFrame(train[GENES+CELLS]), pd.DataFrame(test[GENES+CELLS])]))\ntrain[GENES+CELLS] = qt.transform(train[GENES+CELLS])\ntest[GENES+CELLS] = qt.transform(test[GENES+CELLS])","77beee7a":"train_targets = train_targets.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain_targets_nonscored = train_targets_nonscored.loc[train['cp_type'] == 0].reset_index(drop = True)\ntrain = train.loc[train['cp_type'] == 0].reset_index(drop = True)\n\nprint(train.shape)","3692a870":"top_feats = np.arange(1, train.shape[1])\nprint(top_feats)","59fe12a8":"train.head()","80895949":"N_STARTS = 1\nN_SPLITS = 5\nLBS = 0.0008\nfolds = create_folds(N_STARTS, N_SPLITS)\nprint(folds)","7da9ab40":"params = {'learning_rate': 0.3, \n          'depth': 6, \n          'l2_leaf_reg': 3, \n          'loss_function': 'MultiRMSE', \n          'eval_metric': 'MultiRMSE', \n          'task_type': 'CPU', \n          'iterations': 150,\n          'od_type': 'Iter', \n          'boosting_type': 'Plain', \n          'bootstrap_type': 'Bernoulli', \n          'allow_const_label': True, \n         }","8cb376e7":"res = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor nums, seed in enumerate(range(N_STARTS)):\n    \n#     for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits = N_SPILTS, random_state = 42, shuffle = True).split(train_targets, train_targets)):\n    for n, foldno in enumerate(set(folds[nums])):\n        start_time = time()\n        tr = folds[nums] != foldno\n        te = folds[nums] == foldno\n        \n        x_tr, x_val = train.values[tr][:, top_feats], train.values[te][:, top_feats]\n        y_tr, y_val = train_targets.astype(float).values[tr], train_targets.astype(float).values[te]\n        x_tt = test.values[:, top_feats]\n        \n        # Label Smoothing\n        y_tr = y_tr * (1 - LBS) + 0.5 * LBS\n        \n        cat_tr = Pool(x_tr, label = y_tr)\n        cat_val = Pool(x_val, label = y_val)\n        \n        params['random_state'] = seed\n        model = CatBoostRegressor(**params)\n        fit_model = model.fit(cat_tr, eval_set = cat_val, early_stopping_rounds = 5, \n                              use_best_model = True, verbose = 0)\n        \n        ss.loc[:, train_targets.columns] += fit_model.predict(x_tt) \/ (N_SPLITS * N_STARTS)\n        fold_pred = fit_model.predict(x_val)\n        res.loc[te, train_targets.columns] += fold_pred \/ N_STARTS\n        fold_score = log_loss_metric(train_targets.loc[te].values, fold_pred)\n        print(f'[{str(datetime.timedelta(seconds = time() - start_time))[0:7]}] CatBoost: Seed {seed}, Fold {n}:', fold_score)\n        \n        del model, fit_model\n        x = gc.collect()","1f6943e5":"print(f'CatBoost OOF Metric: {log_loss_metric(train_targets.values, res.values)}')","aea1581a":"X_new = res[cols].values\nx_tt_new = ss[cols].values","b14b5185":"from sklearn.linear_model import LogisticRegression\n\nres_lr = train_targets.copy()\nss_lr.loc[:, train_targets.columns] = 0\nres_lr.loc[:, train_targets.columns] = 0\n\nfor tar in tqdm(range(train_targets.shape[1])):\n    \n    start_time = time()\n    targets = train_targets.values[:, tar]\n    \n    if targets.sum() >= N_SPLITS:\n        \n        for seed in range(N_STARTS):\n\n            skf = StratifiedKFold(n_splits = N_SPLITS, random_state = 42, shuffle = True)\n\n            for n, (tr, te) in enumerate(skf.split(targets, targets)):\n\n                x_tr, x_val = X_new[tr, tar].reshape(-1, 1), X_new[te, tar].reshape(-1, 1)\n                y_tr, y_val = targets[tr], targets[te]\n                \n                model = LogisticRegression(random_state = seed)\n                model.fit(x_tr, y_tr)\n                ss_lr.loc[:, train_targets.columns[tar]] += model.predict_proba(x_tt_new[:, tar].reshape(-1, 1))[:, 1] \/ (N_SPLITS * N_STARTS)\n                res_lr.loc[te, train_targets.columns[tar]] += model.predict_proba(x_val)[:, 1] \/ N_STARTS\n    \n    score = log_loss(train_targets.loc[:, train_targets.columns[tar]].values, res_lr.loc[:, train_targets.columns[tar]].values)\n#     print(f'[{str(datetime.timedelta(seconds = time() - start_time))[2:7]}] LR Target {tar}:', score)","112dcb86":"print(f'LR OOF Metric: {log_loss_metric(train_targets.values, res_lr.values)}')","b7653845":"ss.loc[test['cp_type'] == 1, train_targets.columns] = 0\nss.to_csv('submission_cat.csv', index = False)","6c78ede4":"ss_lr.loc[test['cp_type'] == 1, train_targets.columns] = 0\nss_lr.to_csv('submission.csv', index = False)","66807d2d":"# Multi-Regression CatBoost Test\n\nCatBoost has its own MultiRMSE loss that supports multi-regression tasks. In this notebook, I test its performance.\n\n**Update:** By changing the learning rate to 0.03 and iteration to 1000, CV becomes 0.0168, LB is 0.02016.","ca7fbb7c":"# Logistic Regression Stacked on Regressor\n\nhttps:\/\/www.kaggle.com\/gogo827jz\/rapids-svm-on-gpu-6000-models-in-1-hour","4b41958e":"# Data Preparation","2e482fde":"# Multi-Regression CatBoost","62d15f14":"# Submit"}}