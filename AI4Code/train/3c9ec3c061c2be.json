{"cell_type":{"afdbedbd":"code","2d4b7891":"code","26b1150f":"code","b34187f0":"code","9955ece1":"code","6c94129c":"code","aa4304e0":"code","5a14a152":"code","d584a136":"code","25dc414e":"code","5243fbe0":"code","ccbc5b09":"code","0d289ae9":"markdown","a764c9d6":"markdown","dea41ee0":"markdown","b677cccd":"markdown","0d38f222":"markdown","52081aeb":"markdown","e5880674":"markdown","ebb066b2":"markdown","31f78bec":"markdown","9a30a2b0":"markdown","5e398205":"markdown"},"source":{"afdbedbd":"# math libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.plotting import scatter_matrix\n\n# Training model\n# Check out 'https:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html' for which model to use\nfrom sklearn.linear_model import LinearRegression, LogisticRegression # Linear -> regression, Logistic -> classification\nfrom xgboost import XGBRegressor, XGBClassifier # Regression and Classification\nfrom sklearn.impute import SimpleImputer # for imputing Nan values\nfrom sklearn.utils import shuffle\n\n# validating model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss, accuracy_score\n\n# misc.\nfrom IPython import display\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# display data sources\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","2d4b7891":"data_dir = '..\/input\/'\n# df = pd.read_csv(data_dir + '')","26b1150f":"def discover_data(df):\n    print(\"Head\\n\")\n    display.display(df.head())\n    print(\"\\nDescription\\n\")\n    display.display(df.describe())\n    print(\"\\nInfo\\n\")\n    display.display(df.info())\n\n# df.some_feature.value_counts() shows you all types of values for that feature","b34187f0":"# _ = df.hist(figsize=(20, 15), bins=50)\n\n# _ = df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, \n#                 s=housing[\"population\"]\/100, label=\"population\", # s defines radius of each circle, based on population in this case\n#                 c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True) # c defines color of circle, based on median house value in this case","9955ece1":"# corr_matrix = df.corr()\n# corr_matrix.median_house_value.sort_values(ascending=False)\n# attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n# _ = scatter_matrix(housing[attributes], figsize=(12, 8))\n# _ = housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.2)","6c94129c":"def drop_missing_values(df):\n    new_df = df.copy()\n    cols_with_missing = [col for col in new_df.columns if new_df[col].isnull().any()]\n    return new_df.drop(cols_with_missing, axis=1)\n\ndef impute_missing_values(df):\n    my_imputer = SimpleImputer()\n    new_df = pd.DataFrame(my_imputer.fit_transform(df))\n    new_df.columns = df.columns\n    return new_df\n\ndef impute_with_categorical_indications(df):\n    # make copy to avoid changing original data (when Imputing)\n    new_data = df.copy()\n    # make new columns indicating what will be imputed\n    cols_with_missing = (col for col in new_data.columns if new_data[col].isnull().any())\n    for col in cols_with_missing:\n        new_data[col + '_was_missing'] = new_data[col].isnull()\n    # Imputation\n    new_data = impute_missing_values(new_data)\n    new_data.columns = df.columns\n    return new_data","aa4304e0":"def drop_categoricals(df):\n    return df.select_dtypes(exclude=['object'])\n\ndef one_hot_encode(df):\n    # drop missing values\n    clean_df = drop_missing_values(df)\n    # get numeric cols and unique object cols and merge\n    numeric_cols = [col for col in clean_df.columns if clean_df[col].dtype in ['int64', 'float64']]\n    low_cardinality_cols = [col for col in clean_df.columns if\n                                   clean_df[col].nunique() < 10 and\n                                   clean_df[col].dtype == \"object\"]\n    # merge cols and return one hot encoded version of df\n    my_cols = numeric_cols + low_cardinality_cols\n    df_with_my_cols = clean_df[my_cols]\n    return pd.get_dummies(df_with_my_cols)","5a14a152":"def preprocess_data(df):\n    # Impute for missing values, create synthetic features, etc\n    return df","d584a136":"### Important Globals\n\n# features = []\n# target = \"\"\n# data_dir = '..\/input\/'\n# df = pd.read_csv(data_dir + 'train.csv')\n# x = preprocess_data(df[features])\n# y = df[target]\n# discover_data(df)\n\n### Choose a model\n\n# model = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n# model = XGBRegressor(n_estimators=1000, learning_rate=0.05)\n# model = LogisticRegression()\n# model = LinearRegression()","25dc414e":"def validate_model(x, y):\n    # split test and train data\n    train_x, val_x, train_y, val_y = train_test_split(x, y, random_state=42, test_size=0.2)\n\n    ### Fit linear model\n#     model.fit(train_x, train_y)\n    ### Fit XGB model\n#     model.fit(train_x, train_y, early_stopping_rounds=5, \n#              eval_set=[(val_x, val_y)], verbose=False)\n\n    # Calculate the mean absolute error of your Random Forest model on the validation data\n    val_predictions = model.predict(val_x)\n    print(\"First 5 predictions:\", val_predictions[:5])\n    print(\"First 5 actual values:\", val_y[:5])\n#     print(\"Accuracy: \", accuracy_score(val_predictions, val_y))  # Classification\n#     print(\"Validation MAE for Model: \", mean_absolute_error(val_predictions, val_y))  # Regression\n    \n# validate_model(x, y)","5243fbe0":"def submit_predictions(model, x, y, features):\n    model.fit(x, y)\n\n    # read test data file using pandas\n    test_data_path = '..\/input\/test.csv'\n    test_data = pd.read_csv(test_data_path)\n\n    # create test features and make predictions\n    test_x = test_data[features]\n    processed_test_x = preprocess_data(test_x)\n    test_preds = model.predict(processed_test_x)\n\n    # Save data in format necessary to score in competition\n    ids = test_data[ID]\n    output_dummy = list(zip(ids, test_preds))\n    output = pd.DataFrame()\n    \n    for my_id, pred in output_dummy:\n        dummy_df = pd.Series()\n        dummy_df['PassengerId'] = str(my_id)\n        dummy_df['Survived'] = str(pred)\n        output = output.append(dummy_df, ignore_index=True)\n\n    output.to_csv('submission.csv', index=False)","ccbc5b09":"# submit_predictions(model, x, y, features)","0d289ae9":"# Submit Predictions (after validating model)","a764c9d6":"# Train, Validate, Submit Model","dea41ee0":"### The actual processing function used in training and testing","b677cccd":"### Handle Missing Values","0d38f222":"## Discover Data","52081aeb":"## Load Data","e5880674":"## Visualize Data","ebb066b2":"## Understand Correlations","31f78bec":"# The Data","9a30a2b0":"### Handling text and categorical data","5e398205":"## Prepare Data"}}