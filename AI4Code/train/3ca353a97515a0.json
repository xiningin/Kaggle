{"cell_type":{"4aa53086":"code","bb7c181c":"code","19ac3848":"code","e981b7f2":"code","1b646f63":"code","75c146e1":"code","4196087d":"code","0a9388d9":"code","b38842ff":"code","b6bccacf":"code","95659c18":"code","166c80df":"code","f5ad21a2":"code","59625431":"code","74043a55":"code","b37fbdef":"code","41c262f1":"code","1405eb6d":"code","9c03e404":"code","107ec07b":"code","30d6a658":"code","d74ab57c":"code","9e82f4e9":"code","17191f14":"code","017c6233":"code","c7bf4b29":"code","b4dadfed":"code","750df36a":"code","5fc9cffc":"code","5dfc2f72":"code","35a75574":"code","c1b23219":"code","bae19a04":"code","81d0f496":"code","60471130":"code","96fe71ac":"code","80beece7":"code","32a788be":"code","f653bd50":"code","48839cfa":"code","b5e6d796":"code","08def06c":"code","ffb618af":"code","ea3dc0d9":"code","d7d8b70a":"code","66a67660":"code","af7842a1":"code","fec88a8a":"code","2d672319":"code","cb65b31c":"code","5a115e0e":"code","407f28de":"code","323e023e":"code","1a50520d":"code","a6234bb1":"code","19fa19b1":"code","1a997e06":"code","e836ab41":"code","a2ccf67c":"code","20331dbe":"code","40fbe357":"code","1303b9df":"code","162b2625":"code","36986909":"code","a013e0ba":"code","d9c9b5de":"code","8de8998c":"code","bca6d33c":"code","c6444ba6":"code","971f4667":"code","26bbd095":"code","a5532dac":"code","aeb7e562":"code","e45ea10d":"code","278e811e":"code","c87f55ab":"code","745f3512":"markdown","6c7282e4":"markdown","f3288c55":"markdown","cb47751d":"markdown","ffee78cf":"markdown","ce963d04":"markdown","3548e557":"markdown","4b530f6c":"markdown","bf4245b2":"markdown","dd640e6f":"markdown","5c64d3a5":"markdown","2f58b95a":"markdown","f71db320":"markdown","3e2c00a5":"markdown","0193c474":"markdown","d70862c1":"markdown","719c2086":"markdown","effccfec":"markdown","1da8f649":"markdown","c5c255c1":"markdown","5bc4eb16":"markdown","473e1faa":"markdown","c37c7b19":"markdown","fa83eac1":"markdown","2de49310":"markdown","2492e8c8":"markdown","66d6c727":"markdown","93c1fd61":"markdown","b6681af3":"markdown","bf0daa38":"markdown","ab7a12b2":"markdown","ec20e0c2":"markdown","fc67ccf6":"markdown","54998a93":"markdown","89381270":"markdown","25ed246f":"markdown","6189477c":"markdown","dfbfe481":"markdown","87a93bc3":"markdown","b17bac41":"markdown","035e860b":"markdown","fac382fe":"markdown","345c5b1e":"markdown","1e940021":"markdown","add95d23":"markdown","c75bcf6a":"markdown","53e06a9a":"markdown","5ec1b9a7":"markdown","8f723874":"markdown","f89eeda4":"markdown","c5fcd2f8":"markdown","3d72f295":"markdown","51148718":"markdown","f0ccfb42":"markdown","eb28374b":"markdown","37b2f3c1":"markdown","567ca85b":"markdown","92a8d829":"markdown","f9d9bd92":"markdown","313f795a":"markdown","ed275ee6":"markdown","f3d5c812":"markdown","60f60f26":"markdown","beedd1f1":"markdown","826f6801":"markdown","1bca6031":"markdown","a14a182a":"markdown","8761c09f":"markdown","caf13071":"markdown","3b275424":"markdown","7ef0d4b6":"markdown","3dbca618":"markdown","c9c474c5":"markdown"},"source":{"4aa53086":"!pip install fastai2 --quiet","bb7c181c":"from fastai2.vision.all import *","19ac3848":"path = Path('..\/input\/plant-pathology-2020-fgvc7')","e981b7f2":"train_df = pd.read_csv(path\/'train.csv')","1b646f63":"train_df.head()","75c146e1":"train_df.iloc[0, 1:]","4196087d":"train_df.iloc[0,1:][train_df.iloc[0, 1:]==1].index[0]","0a9388d9":"%%timeit\n_ = train_df.iloc[0,1:][train_df.iloc[0, 1:]==1].index[0]","b38842ff":"df_np = train_df.to_numpy()","b6bccacf":"index2name = {1:'healthy',\n      2:'multiple_diseases',\n      3:'rust',\n      4:'scab'}","95659c18":"df_np[0]","166c80df":"%%timeit\nidx = np.where(df_np[1]==1)[0][0]\ny = index2name[idx]","f5ad21a2":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock))","59625431":"def get_x(fn): print(fn)","74043a55":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                  get_x=get_x)","b37fbdef":"dblock.summary(df_np)","41c262f1":"def get_x(row): return path\/Path('images\/'+row[0]+'.jpg')","1405eb6d":"get_x(df_np[0])","9c03e404":"PILImage.create(get_x(df_np[0]))","107ec07b":"idx = np.where(df_np[1]==1)[0][0]\ny = index2name[idx]","30d6a658":"def get_y(row):\n    idx = np.where(row==1)[0][0]\n    return index2name[idx]","d74ab57c":"get_y(df_np[0])","9e82f4e9":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                  get_x=get_x,\n                  get_y=get_y)","17191f14":"splitter = RandomSplitter(valid_pct=0.2, seed=42)","017c6233":"splitter","c7bf4b29":"idxs = list(range(1,11)); idxs","b4dadfed":"splitter(idxs)","750df36a":"item_tfms = RandomResizedCrop(224)\nbatch_tfms=[*aug_transforms(size=224, flip_vert=True),\n                                   Normalize]","5fc9cffc":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                  get_x=get_x,\n                  get_y=get_y,\n                  splitter=splitter,\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)","5dfc2f72":"dls = dblock.dataloaders(df_np, bs=64)","35a75574":"dls.train.after_batch","c1b23219":"norm = dls.train.after_batch.normalize","bae19a04":"norm.mean, norm.std","81d0f496":"dset = Datasets(items=df_np, tfms=[[get_x, PILImage.create], [get_y, Categorize]])","60471130":"dset[0]","96fe71ac":"dl = TfmdDL(dset, after_item=dls.train.after_item,\n                     after_batch=[IntToFloatTensor(), Normalize()],\n                     bs=64, device='cuda')","80beece7":"dl = TfmdDL(dset, after_item=dls.train.after_item,\n                     after_batch=[IntToFloatTensor(), Normalize()],\n                     bs=len(dset), device='cuda')","32a788be":"norm = dl.after_batch.normalize","f653bd50":"norm.mean, norm.std","48839cfa":"norm.mean.flatten()","b5e6d796":"plant_norm = (norm.mean.flatten(), norm.std.flatten()); plant_norm","08def06c":"item_tfms = RandomResizedCrop(224)\nbatch_tfms=[*aug_transforms(size=224, flip_vert=True),\n                                   Normalize.from_stats(*plant_norm)]","ffb618af":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                  get_x=get_x,\n                  get_y=get_y,\n                  splitter=splitter,\n                  item_tfms=item_tfms,\n                  batch_tfms=batch_tfms)","ea3dc0d9":"dls = dblock.dataloaders(df_np, bs=32)","d7d8b70a":"dls.show_batch()","66a67660":"net = xresnet50(pretrained=False, act_cls=Mish, sa=True, n_out=dls.c)","af7842a1":"@delegates(RAdam)\ndef ranger(p, lr, mom=0.95, wd=0.01, eps=1e-6, **kwargs):\n    \"Convenience method for `Lookahead` with `RAdam`\"\n    return Lookahead(RAdam(p, lr=lr, mom=mom, wd=wd, eps=eps, **kwargs))","fec88a8a":"opt_func = ranger","2d672319":"from sklearn.metrics import roc_auc_score\n\ndef roc_auc(preds, targs, labels=range(4)):\n    targs = np.eye(4)[targs]\n    return np.mean([roc_auc_score(targs[:,i], preds[:,i]) for i in labels])\n\ndef healthy_roc_auc(*args):\n    return roc_auc(*args, labels=[0])\n\ndef multiple_diseases_roc_auc(*args):\n    return roc_auc(*args, labels=[1])\n\ndef rust_roc_auc(*args):\n    return roc_auc(*args, labels=[2])\n\ndef scab_roc_auc(*args):\n    return roc_auc(*args, labels=[3])","cb65b31c":"metric = partial(AccumMetric, flatten=False)","5a115e0e":"metrics=[\n            error_rate,\n            metric(healthy_roc_auc),\n            metric(multiple_diseases_roc_auc),\n            metric(rust_roc_auc),\n            metric(scab_roc_auc),\n            metric(roc_auc)]","407f28de":"learn = Learner(dls, net, opt_func=ranger, loss_func=CrossEntropyLossFlat(),\n               metrics=metrics)","323e023e":"learn.lr_find()","1a50520d":"learn.fit_flat_cos(5, 1e-3)","a6234bb1":"from fastai2.test_utils import synth_learner\none_cycle = synth_learner()\none_cycle.fit_one_cycle(1)","19fa19b1":"one_cycle.recorder.plot_sched()","1a997e06":"flat_cos = synth_learner()\nflat_cos.fit_flat_cos(1)","e836ab41":"flat_cos.recorder.plot_sched()","a2ccf67c":"learn.fit_flat_cos(5, 1e-4)","20331dbe":"preds, targs = learn.tta(ds_idx=1, n=4)","40fbe357":"error_rate(preds, targs)","1303b9df":"from sklearn.model_selection import StratifiedKFold","162b2625":"train_lbls = []\nfor _, lbl in dset:\n    train_lbls.append(lbl)","36986909":"imgs = df_np[:,0]","a013e0ba":"def get_data(idx, size=224, bs=64):\n    dblock = DataBlock(blocks    = (ImageBlock, CategoryBlock),\n                       get_x=get_x,\n                       get_y=get_y,\n                       splitter=IndexSplitter(idx),\n                       item_tfms=RandomResizedCrop(size+64),\n                       batch_tfms=[*aug_transforms(size=size, flip_vert=True),\n                                   Normalize.from_stats(*plant_norm)],\n                      )\n    return dblock.dataloaders(train_df, bs=bs)","d9c9b5de":"test_df = pd.read_csv(path\/'test.csv')","8de8998c":"test_np = test_df.to_numpy()","bca6d33c":"test_dl = learn.dls.test_dl(test_np)","c6444ba6":"test_dl.show_batch()","971f4667":"import gc","26bbd095":"test_preds = []\nskf = StratifiedKFold(n_splits=3, shuffle=True)\nfor _, val_idx in skf.split(imgs, np.array(train_lbls)):\n    dls = get_data(val_idx, 128, 32)\n    net = xresnet50(pretrained=False, act_cls=Mish, sa=True, n_out=dls.c)\n    learn = Learner(dls, net, opt_func=ranger, loss_func=CrossEntropyLossFlat(),\n               metrics=metrics)\n    learn.fit_flat_cos(5, 4e-3, cbs=EarlyStoppingCallback(monitor='roc_auc'))\n    learn.save('initial')\n    del learn\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    dls = get_data(val_idx, 256, 16)\n    learn = Learner(dls, net, opt_func=ranger, loss_func=CrossEntropyLossFlat(),\n               metrics=metrics)\n    learn.load('initial');\n    learn.fit_flat_cos(10, 1e-3, cbs=EarlyStoppingCallback(monitor='roc_auc'))\n    learn.save('stage-1')\n    del learn\n    torch.cuda.empty_cache()\n    gc.collect()\n    dls = get_data(val_idx, 448, 8)\n    learn = Learner(dls, net, opt_func=ranger, loss_func=CrossEntropyLossFlat(),\n               metrics=metrics)\n    learn.fit_flat_cos(5, slice(1e-5, 1e-4), cbs=EarlyStoppingCallback(monitor='roc_auc'))\n    tst_dl = learn.dls.test_dl(test_df)\n    y, _ = learn.tta(dl=tst_dl)\n    test_preds.append(y)\n    del learn\n    torch.cuda.empty_cache()\n    gc.collect()","a5532dac":"tot = test_preds[0]\nfor i in test_preds[1:]:\n    tot += i","aeb7e562":"len(test_preds)","e45ea10d":"tot = tot \/ 3","278e811e":"subm = pd.read_csv(path\/'sample_submission.csv')","c87f55ab":"subm.iloc[:, 1:] = tot\nsubm.to_csv(\"submission.csv\", index=False, float_format='%.2f')","745f3512":"Perfect! Let's integrate both of these into our `DataBlock`:","6c7282e4":"## Working with NumPy\n\nFirst let's convert our `DataFrame` to NumPy:","f3288c55":"### Augmentation\n\nFor our augmentations, we care about `item` and `batch` transforms. \n\nItem transforms are performed on the CPU and done on an individual image-by-image basis, and batch transforms are done on the GPU and done, well, on batches!\n\nIn general, we want our item transforms to simply prepare the data for getting into a batch, such as ensuring all images are the same size. Let's do that here. We'll set `item_tfms` to be a `RandomResizedCrop`, and our `batch_tfms` will have some random augmentation along with normalizing our data","cb47751d":"Next we'll call `learn.lr_find()` to help us find a decent learning rate to use:","ffee78cf":"`tta` will return the predictions and the targets for the data. If no targets are given (such as a test `DataLoader`), they will simply be blank.\n\nNow let's try to calculate the `error_rate` of those values!","ce963d04":"## The Final DataLoaders\n\nNow we have all the pieces in place, let's put them together! This is what our `DataBlock` looks like now, notice the change to `Normalize`:","3548e557":"To build our `DataLoaders` we call `dblock.dataloaders()` and pass in a batch size along with how `get_x` is expecting it's input:","4b530f6c":"As we can see, not *too* far off from what we got earlier off of one batch, but now we know the dataset's statistics! Let's store those away:","bf4245b2":"As you can see, we got down to a little over 20% error, great! So what's next?","dd640e6f":"We're going to write a few functions that allow us to perform Progressive Resizing along with Pre-Sizing as we train. This entails training a model at a low image size for a few epochs, then a higher, until finally the highest we want to train at. And Pre-Sizing is utilizing our RandomResizeCrop on a larger image before cropping it to a smaller one later. We'll also want our `DataBlock` to use an `IndexSplitter` now, as this is what our KFold will give us:","5c64d3a5":"We can see we improved our results a little bit! `tta` in general helps rather than hinders, so it is *always* worth a shot trying. Just remember you're always getting 4 times as many predictions when you do it! (by default)","2f58b95a":"While this is great, I prefer working in numpy as it can be faster, and the larger the dataset the more small times compact. ","f71db320":"It'll return a very long stack trace error, I've pasted below what occurs before it to save on space:\n\n```\nSetting-up type transforms pipelines\nCollecting items from [['Train_0' 0 0 0 1]\n ['Train_1' 0 1 0 0]\n ['Train_2' 1 0 0 0]\n ...\n ['Train_1818' 1 0 0 0]\n ['Train_1819' 0 0 1 0]\n ['Train_1820' 0 0 0 1]]\nFound 1821 items\n2 datasets of sizes 1457,364\nSetting up Pipeline: get_x -> PILBase.create\n['Train_1422' 0 0 1 0]\nSetting up Pipeline: Categorize\n```","3e2c00a5":"And now let's make it into a `DataLoader` by providing the `after_item` and `after_batch` from earlier, along with specifying our device as `cuda` so `Normalize` can work on the GPU. We'll use a special type of `DataLoader` called the `TfmdDL`:","0193c474":"So let's go over what's happening in this script before we run it. We get the data at a size of 128x128 and train for five epochs, keeping track of the ROC AUC score. Then we save that model and clear our memory before training at 256, loading in that initial save, and training for ten epochs. Finally we train for five more at a size of 448. Then we perform our `tta` and add it to `test_preds`. Let's run it:","d70862c1":"### Splitting\n\nNext we need to tell `fastai2` how we want to split our data. We have a variety of options available to us. For our first example we'll split randomly and have 20% of the data as our validation set. Later we'll look at K-Fold Cross Validation and we'll want to use a set of index's instead.\n\n`fastai2`'s split methods look something like so:","719c2086":"Wait, it's a function? Yes! Let's try passing to it a list of indexs, say 1-10 and see what happens:","effccfec":"So we can pass in a `valid_pct` as well as a seed for reproducability. ","1da8f649":"To make things easier, we'll set a `Path` to our data:","c5c255c1":"### `get_x`\n\nNext we'll want to tell our `DataBlock` how to get our `x`'s (or our filenames). Our `x`'s are in that first index of our `NumPy` array, so let's make a `get_x` function. \n\nBut first, how do we know what gets passed? We can make something like so:","5bc4eb16":"We can see that `Normalize` lives here. To get those statistics we simply grab the `mean` and `std`:","473e1faa":"Perfect! Now we have everything we need. Lets build our script. We'll include EarlyStopping too as we only want to use the best models during training:","c37c7b19":"So we have an `image_id` which would correspond to a particular file name, and our labels are given as one of four binary values! That doesn't help us very much though, as we *just* want to use the class (healthy, multiple_diseases, rust, or scab). How do we do that?","fa83eac1":"Finally we'll submit our predictions. To do so we do:","2de49310":"Now we'll want to make this into a NumPy array so it's formatted similar to what we did earlyer","2492e8c8":"Now let's time how long it takes to do just what we did above. We'll use `np.where` to find the particular index in the row where the value is equal to 1, and then return the `index2name` of our label:","66d6c727":"## Training and `xresnet`\n\nNow that we've got the hard part out of the way, let's get to training a model! For this notebook we'll be showing off how to use the `xresnet` (from the Bag Of Tricks paper) and all of the features you can include with it, along with utlizing the new `ranger` optimizer function and Flat Cosine Annealing fit function built into `fastai2`!","93c1fd61":"*Very* different. What's going on? We train at a consistant learning rate before performing a Cosine Anneal at some `pct_start` value, which by default is 75% of the batches.The reason for this is `ranger` already includes the warm up phase that One Cycle had, so we can simply skip it all together. It was found that this trained *much* better when used. Now let's get back to training","b6681af3":"Now one thing we may want to do is since we're training from scratch with `xresnet`, we'll want to calculate the normalization statistics which can get pretty heavy. Can `fastai2` help us with this? Yes! Let's see how!","bf0daa38":"Now let's get our optimizer function. `ranger` is a mix between the `LookAhead` optimizer along with `RAdam`. `fastai2` has a convience function to use for this, but first let's see what it actually looks like:","ab7a12b2":"## One Cycle vs Flat Cosine Annealing\n\n\nWe can build a `synth_learner` to simulate training:","ec20e0c2":"### `get_y`\n\nNow that we can grab our `x`'s, let's setup how to grab our `y`'s. It'll look very similar to our `get_x`, bringing in what we did earlier. Just for a reminder, here's how we grabbed a particular label based on a row:","fc67ccf6":"## Exploring the Data Format\n\nFirst things first, let's explore how our data is given to us! Let's import from the `fastai2` `vision` library to get us some helper libraries:","54998a93":"Now our labels are stored inside of `train.csv`, so let's open it up and look inside:","89381270":"And we can see the first array is our training indexs and the second is our valid indexs. It looks strange, this is because it's an `L` type array, something custom for `fastai2`. For more on `L` read [here](http:\/\/fastcore.fast.ai\/foundation#L)","25ed246f":"Looks straight forward enough! Let's make our `opt_func`:","6189477c":"To use custom metrics, we'll want to wrap them inside of an `AccumMetric` like so:","dfbfe481":"## Calculating our Normalization Statistics\n\nWhen we use `Normalize` as in our `batch_tfms`, we can either call it normally or we can use `from_stats()` and pass in some statistics to use. If we don't it will automatically calculate them based on the first batch of data. Let's see that in action:","87a93bc3":"So we can see that it calculated our statistics for us! Now how can we use that to our advantage? \n\nLet's make one big giant `DataLoader` that has all of our data. First we'll make a dataset of all of our data. We'll want to pass in `PILImage.create` and `Categorize` to our `tfms`. As you can see, the first array specifies our `x` and the second our `y`:","b17bac41":"## Splitting the Data and Augmentation","035e860b":"We can see we follow a cycle, the learning rate starts low, goes high, then goes low again. How about `fit_flat_cos`?","fac382fe":"To use our fold, we'll also want our images. This is as simple as:","345c5b1e":"Very impressive result in simply 5 epochs, ~73% accuracy. But why `fit_flat_cos`? Not the infamous `fit_one_cycle`?\n\nSurprisingly, `ranger` does not like this schedule. Why? Let's compare the two.","1e940021":"So here we have a row of our data's labels. To get the one that matches we'll want to see who's value is equal to 1:","add95d23":"And if we wanted to verify it works, we can call `PILImage.create` to try to open that `Path` with Pillow:","c75bcf6a":"## Test-Time Augmentation\n\n`fastai2` has the ability to perform what is known as Test-Time Augmentation. What that essentially means is we gather a number of sets of predictions from our model. One of which is simply just running through our validation set, the other `n` have the *training* augmentation applied to them. As the augmentations can have a degree of randomness to them, this is why we do it four times. In general, combining these four allows for a higher accuracy. Does that occur here as well? Let's find out!","53e06a9a":"The part we're interested in is that `get_x` call we see there. We can see it prints out a row from our `NumPy` array. Now that we know this, let's build our proper `get_x`!\n\nWe'll want it to return a `Path` to our filename. We'll add a prefix and a suffix to it to allow so:","5ec1b9a7":"Now if we pass in a row from our array, we should get the respective `Path`:","8f723874":"And pass this to our `DataBlock`. Let's see what it does:","f89eeda4":"*Considerably* faster here! Alright! Now let's begin to build some `DataLoaders` for us to use!","c5fcd2f8":"To make sure it works, we'll try to make a `test_dl` with our `DataLoaders` from earlier:","3d72f295":"To use Test-Time Augmentation, simply call `learn.tta()`. You can then pass in a particular `DataLoader` you want to use, if not it will use the validation `DataLoader` by default (or `ds_idx=1`). When we explore Cross Validation we'll see how to perform inference on the test set:","51148718":"Now as we lost the particular column names here, let's find and replace our encoded `y`'s with their names. We can do this with a dictionary of their index:","f0ccfb42":"The last thing we need is how to make our `test_dl` with our test data. First let's grab the test `DataFrame`:","eb28374b":"### `xresnet` and `ranger`\n\n`xresnet` is built upong a variety of papers and is the result of the `ImageWoof`\/`ImageNette` competitions [here](github.com\/fastai\/imagenette). Essentially a super difficult subset of the full ImageNet to allow for quick test of your ideas!\n\nCurrently, `xresnet` allows these customizations:\n  * Dropout\n  * Self-Attention\n  * Activation Class\n  \nFor our model, we'll utilize the self-attention along with the Mish activation function. Let's see how to build our model:","37b2f3c1":"It should be just as simple as replacing `df_np[1]` with `row`. Let's try it out:","567ca85b":"We're not quite done yet, but almost! The last thing we want to do is make our batch size equal to the size of our dataset, this way the `Normalize` is done over the entire thing. Let's do that now:\n\n* Note: This will take a bit to run, this is expected. Most of the time, simply running off of one batch of data has been found to be good enough (Sylvain and Jeremy found), but I wanted to show how it could be done","92a8d829":"And now we can plot that scheduler:","f9d9bd92":"And now we can make our `DataLoaders` and look at a batch!","313f795a":"To get the `Normalization` statistics we want to look inside the `after_batch` of our training dataloader like so:","ed275ee6":"We'll check our total number of predictions just to verify it is still 3:","f3d5c812":"In this notebook, we'll be looking at utilizing the `fastai2` library to train a xresnet model from scratch (as no pretrained models exist currently), and comparing how it's overall accuracy is along with how to utilize a few more techniques in the `fastai2` library to speed up training *and* improve accuracy","60f60f26":"And we're done! I hope this helps you get more familiar with how to take `fastai2` to the next level. If you found this useful please don't forget to upvote, and ask away in the comments if you have any questions. Thanks!","beedd1f1":"We can call `dblock.summary()` here and pass in what we expect our inputs to be (our `NumPy` array) and see what it gives us:","826f6801":"Now we can divide it by 3:","1bca6031":"And now let's build our `Learner`!","a14a182a":"We'll probably want a learning rate of about 1e-3, so we'll try to use that. First we'll fit with `fit_flat_cos` and then we'll look at what's really going on:","8761c09f":"## Training Cont.\n\nNow let's resume training. We'll try fitting for 5 more epochs at a slightly lower learning rate, before calling it there:","caf13071":"## DataBlock and DataLoaders\n\nOne nice benefit (and probably my favorite feature) of `fastai2` is the improved `DataBlock` API. Essentially we lay out a plan for us to use for our data and fit everything together!\n\nFor our particular classification problem, we'll want to use an `ImageBlock` and `CategoryBlock`, meaning that we have our inputs as images and outputs as categories:","3b275424":"### Training\n\nNow we can start training. In general we'll perform the following steps:\n\n1. Build our `Learner` by passing in our `DataLoaders`, a model, a loss function, some metrics, and our optimizer\n\n2. Find a good learning rate via `lr_find()`\n\n3. Fit our model, in this case with `fit_flat_cos`\n\nFor our metrics, we'll use some `roc` scores:","7ef0d4b6":"## Submitting Predictions\n\nGreat, we now have this set of 3 predictions, what do we do from there?\n\nFirst we'll want to average all of those together to ensemble our model like so:","3dbca618":"## Cross Validation\n\nNow that we've got all the building blocks, time to put our faith in the CV! But how do we set it up? Let's take a look. \n\nTo utilize `StratifiedKFold` we're going to want to grab all of our labels from the dataset. We'll utilize our `Dataset` we made earlier:","c9c474c5":"And now let's take a look!"}}