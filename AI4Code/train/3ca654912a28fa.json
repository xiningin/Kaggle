{"cell_type":{"2a6bebd9":"code","f7966fb5":"code","6462dcec":"code","4066795c":"code","1b544fa8":"code","00c3c3cc":"code","e79a9f52":"code","89db2451":"code","d344b83f":"code","b11ffcbe":"code","ff1b1bb5":"code","88ee159e":"markdown"},"source":{"2a6bebd9":"pip install --upgrade -q pytorch-lightning==1.3.1","f7966fb5":"import numpy as np\nimport pandas as pd\nimport pathlib\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json, pickle\n\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n\nfrom sklearn.model_selection import KFold\n\nfrom tqdm.notebook import tqdm\nimport pytorch_lightning as pl\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification\nimport joblib\n\nfrom typing import Optional\nfrom collections import defaultdict","6462dcec":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","4066795c":"class CONFIG:\n    max_length = 300\n    num_targets = 1\n    SEED = 321\n    loader_params = dict(\n        trn=dict(batch_size=4,\n                 num_workers=0,\n                 shuffle=True,\n                 pin_memory=True),\n        val=dict(batch_size=5,\n                 num_workers=0,\n                 shuffle=False,\n                 pin_memory=True),\n        tst=dict(batch_size=5,\n                 num_workers=0,\n                 shuffle=False,\n                 pin_memory=True),\n        all=dict(batch_size=5,\n                 num_workers=0,\n                 shuffle=False,\n                 pin_memory=True)\n    )\n    learning_rate = 5e-5\n    n_folds = 5\n    \nseed_everything(CONFIG.SEED)\n\nroot_path = pathlib.Path('..\/input\/commonlitreadabilityprize')\n\nsave_path = pathlib.Path('.')\n","1b544fa8":"class CommonLitDataset(Dataset):\n    def __init__(self, inputs, masks, targets):\n        assert inputs.shape == masks.shape\n        self.inputs = inputs\n        self.masks = masks\n        self.targets = targets.type(torch.float32)\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        return dict(inputs=self.inputs[idx], masks=self.masks[idx], targets=self.targets[[idx]])\n","00c3c3cc":"class PrintCallback(pl.callbacks.Callback):\n    \"\"\"\n    callback for pytorch lightning which saves and prints out results\n    \"\"\"\n\n    def __init__(self):\n        self.metrics = {}\n\n    def on_epoch_end(self, trainer, pl_module):\n        metrics_dict = {k: v for k, v in trainer.callback_metrics.items() if 'step' not in k}\n        self.metrics[trainer.current_epoch] = metrics_dict\n        msg = f'epoch: {str(trainer.current_epoch).rjust(4)}\\t'\n        msg += '\\t'.join([f'{k}: {v:.5f}' for k, v in metrics_dict.items()])\n        print(msg)\n\n    def to_df(self):\n        return pd.DataFrame(\n            {epoch: {k: v.item() for k, v in metrics.items()} for epoch, metrics in self.metrics.items()}).T\n\n    def plot(self):\n        df = self.to_df()\n        fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n        for column in df.columns:\n            df[column].plot(ax=ax, legend=column)\n        return fig\n","e79a9f52":"class Squeeze(nn.Module):\n    def __init__(self, dims=-1):\n        super().__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        return x.squeeze(self.dims)\n        \nclass AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\n    \n\nclass CNNHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, kernel_size=10, num_targets=1):\n        super().__init__() \n        self.head = nn.Sequential(nn.Conv1d(in_features, hidden_dim, kernel_size=kernel_size),\n                                     nn.AdaptiveMaxPool1d(1),\n                                     Squeeze()\n                                    )\n        self.out_features = hidden_dim\n        \n    def forward(self, x):\n        return self.head(x.permute(0,2,1))\n        \n\nclass LSTMHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, n_layers, num_targets=1):\n        super().__init__()\n        self.lstm = nn.LSTM(in_features,\n                            hidden_dim,\n                            n_layers,\n                            batch_first=True,\n                            bidirectional=False,\n                            dropout=0.2)\n        self.out_features = hidden_dim\n\n    def forward(self, x):\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(x)\n        out = hidden[-1]\n        return out\n        \n        \nclass TransformerHead(nn.Module):\n    def __init__(self, in_features, max_length, num_layers=1, nhead=8, num_targets=1):\n        super().__init__()\n\n        self.transformer = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=in_features,\n                                                                                          nhead=nhead),\n                                                 num_layers=num_layers)\n        self.row_fc = nn.Linear(in_features, 1)\n        self.out_features = max_length\n\n    def forward(self, x):\n        out = self.transformer(x)\n        out = self.row_fc(out).squeeze(-1)\n        return out","89db2451":"class CommonLitModel(pl.LightningModule):\n    def __init__(self,\n                 model_name: str,\n                 fold_id: int,\n                 head,\n                 models_path: pathlib.Path = pathlib.Path('.\/models'),\n                 data_path: pathlib.Path = pathlib.Path('..\/input\/commonlitreadabilityprize')\n                 ):\n        super().__init__()\n        self.model_name = model_name\n        self.num_targets = CONFIG.num_targets\n        self.criterion = nn.MSELoss()\n        self.fold_id = fold_id\n        self.save_hyperparameters()\n        self.use_attn_block = True\n\n        self.model_path = pathlib.Path(models_path) \/ model_name\n        self.state_dict_path = self.model_path \/ f'{head}_fold{fold_id}.pt'\n        self.config_path = self.model_path \/ 'config.pkl'\n        self.tokenizer_path = self.model_path \/ 'tokenizer.pkl'\n\n        self.is_fine_tuned = self.config_path.exists() and self.state_dict_path.exists()\n        if self.is_fine_tuned:\n            with open(self.config_path, 'rb') as f:\n                self.config = pickle.load(f)\n            self.feature_extractor = AutoModelForTokenClassification.from_config(self.config)\n            \n            with open(self.tokenizer_path, 'rb') as f:\n                self.tokenizer = pickle.load(f)\n\n        else:\n            self.feature_extractor = AutoModelForTokenClassification.from_pretrained(model_name)\n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.config = AutoConfig.from_pretrained(model_name)\n            \n\n        in_features = self.feature_extractor.classifier.in_features\n        if head == 'AttentionHead':\n            self.head = AttentionHead(in_features=in_features, hidden_dim=in_features, num_targets=1)\n        if head == 'CNNHead':\n            self.head = CNNHead(in_features=in_features, hidden_dim=in_features\/\/4, kernel_size=10, num_targets=1)\n        if head == 'LSTMHead':\n            self.head = LSTMHead(in_features=in_features, hidden_dim=in_features\/\/4, n_layers=1, num_targets=1)\n        if head == 'TransformerHead':\n            self.head = TransformerHead(in_features=in_features, max_length=CONFIG.max_length, num_layers=1, nhead=8, num_targets=1)\n        self.feature_extractor.classifier = nn.Identity()\n        self.fc = nn.Linear(self.head.out_features, self.num_targets)\n        \n        if self.is_fine_tuned:\n            self.load_state_dict(torch.load(self.state_dict_path, map_location=self.device))\n        self.data_path = data_path\n\n        self.data = self.prepare_data()\n\n        self.masks = dict(trn=(self.data.fold_id.ne(self.fold_id) & self.data.fold_id.ge(0)).values,\n                          val=self.data.fold_id.eq(self.fold_id).values,\n                          tst=self.data.fold_id.eq(-1).values\n                          )\n\n    @staticmethod\n    def set_folds(in_data, cv_obj=KFold(n_splits=5, random_state=CONFIG.SEED, shuffle=True)):\n        df = in_data.copy()\n        df[\"fold_id\"] = -1\n        for fold_id, (_, val_set) in enumerate(cv_obj.split(np.arange(df.index.size))):\n            df.loc[val_set, \"fold_id\"] = fold_id\n        return df\n\n    def prepare_data(self):\n        train_df = (pd.read_csv(self.data_path \/ \"train.csv\")\n                    .pipe(self.set_folds, cv_obj=KFold(n_splits=CONFIG.n_folds, random_state=CONFIG.SEED, shuffle=True))\n                    )\n        test_df = (pd.read_csv(self.data_path \/ \"test.csv\")\n                   .assign(fold_id=-1))\n        df = pd.concat([train_df, test_df], sort=False).set_index('id')\n\n        inputs_ids = dict()\n        attention_masks = dict()\n        for idx, excerpt in zip(df.index, df.excerpt):\n            out = self.tokenizer(excerpt,\n                                 add_special_tokens=True,\n                                 return_tensors=\"pt\",\n                                 max_length=CONFIG.max_length,\n                                 padding=\"max_length\",\n                                 truncation=True)\n            inputs_ids[idx] = out['input_ids']\n            attention_masks[idx] = out['attention_mask']\n\n        df = (df\n                  .join(pd.Series(inputs_ids).to_frame('inputs'))\n                  .join(pd.Series(attention_masks).to_frame('attention_masks'))\n                  .loc[:, ['fold_id', 'inputs', 'attention_masks', 'target']])\n        return df\n\n    def fetch_dataloader(self, typ: str):\n        if typ == 'all':\n            dt = self.data\n        else:\n            dt = self.data.loc[self.masks[typ]]\n\n        ds = CommonLitDataset(inputs=torch.cat(dt['inputs'].tolist()),\n                              masks=torch.cat(dt['attention_masks'].tolist()),\n                              targets=torch.from_numpy(dt['target'].values)\n                              )\n        return DataLoader(ds, **CONFIG.loader_params[typ])\n\n    def train_dataloader(self):\n        return self.fetch_dataloader(typ='trn')\n\n    def val_dataloader(self):\n        return self.fetch_dataloader(typ='val')\n\n    def test_dataloader(self):\n        return self.fetch_dataloader(typ='tst')\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=CONFIG.learning_rate)\n        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=20)\n        return dict(optimizer=optimizer, scheduler=scheduler)\n\n    def forward(self, inputs, masks):\n        x = self.feature_extractor(inputs, masks)[\"logits\"]\n        x = self.head(x)\n        x = self.fc(x)\n        return x\n\n    def shared_step(self, batch, typ):\n        inputs = batch['inputs']\n        masks = batch['masks']\n        targets = batch['targets']\n        outputs = self(inputs, masks)\n        loss = self.criterion(outputs, targets)\n        self.log(f'{typ}_loss', loss, on_step=False, on_epoch=True)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        return self.shared_step(batch, typ='trn')\n\n    def validation_step(self, batch, batch_idx):\n        return self.shared_step(batch, typ='val')\n\n    def save(self):\n        torch.save(self.state_dict(), self.state_dict_path)\n        with open(self.tokenizer_path, 'wb') as f:\n            pickle.dump(self.tokenizer, f)\n        with open(self.config_path, 'wb') as f:\n            pickle.dump(self.config, f)\n\n\n","d344b83f":"\nresults = []\nheads = ['AttentionHead', 'CNNHead', 'LSTMHead', 'TransformerHead']\nmodels = [\n    'roberta-base',\n    'distilbert-base-uncased', \n    'xlnet-base-cased'\n]\n\n\ndir_path = pathlib.Path('.\/models')\n\nfold_id = 0\nmodels_dict = defaultdict(lambda: defaultdict(dict))\nfor model_name in models:\n    model_name = model_name\n    model_path = dir_path \/ model_name\n    model_path.mkdir(exist_ok=True, parents=True)\n    for head in heads:\n        try:        \n            checkpoint_callback = pl.callbacks.ModelCheckpoint()\n            print_callback = PrintCallback()\n            es_callback = pl.callbacks.EarlyStopping(monitor='val_loss', patience=2) \n\n            model = CommonLitModel(model_name, fold_id, head=head)\n            \n            print(model_name, head)\n            if model.is_fine_tuned:\n                model = model.cuda()\n                model.eval()\n                print('\\talready fine tuned')\n                lst = [(model(batch['inputs'].cuda(), batch['masks'].cuda()) - batch['targets'].cuda()).pow(2).cpu().detach().numpy().ravel() for batch in model.val_dataloader()]\n                score = np.sqrt(np.concatenate(lst).mean())\n                models_dict[model_name][head] = model\n                print(model_name, head, score)\n                continue\n            trainer = pl.Trainer(gpus=1 if torch.cuda.is_available() else 0, max_epochs=2, fast_dev_run=False, callbacks=[checkpoint_callback, print_callback, es_callback])\n\n            trainer.fit(model, model.train_dataloader(), model.val_dataloader())\n\n            print_callback.plot()\n            plt.show()\n            \n            model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n            model = model.cuda()\n            model.save()\n            model.eval()\n            lst = [(model(batch['inputs'].cuda(), batch['masks'].cuda()) - batch['targets'].cuda()).pow(2).cpu().detach().numpy().ravel() for batch in model.val_dataloader()]\n            score = np.sqrt(np.concatenate(lst).mean())\n            torch.cuda.empty_cache()\n            models_dict[model_name][head] = model\n            print(model_name, head, score)\n            results.append([model_name, head, score])\n        except Exception as e:\n            print(e)\n            pass","b11ffcbe":"pd.DataFrame(results, columns=['model_name', 'head_name', 'score']).groupby(['head_name', 'model_name']).max().unstack()","ff1b1bb5":"!rm -rf .\/lightning_logs","88ee159e":"This is my first experience with NLP\n\nHere I experimenting with different models (roberta, distilbert, xlnet) + different heads (LSTM, 1D-CNN, Attention, Transformer) + FC layer.\n\nThe pipeline is very flexible, you can change any of components\n\nI run here only single fold, but you can add the loop and train with cross-validation."}}