{"cell_type":{"65e9ab88":"code","48560f3e":"code","d392a5ea":"code","8a1cc305":"code","ca3086df":"code","0041e23e":"code","1e62b843":"code","f0e840ec":"code","75d376c6":"code","719c793c":"code","a93e75b7":"code","0b5f310b":"code","ede6aa27":"code","b885aab6":"code","0420830c":"code","160ca00c":"code","d3da18b9":"code","cb38dd72":"code","f367cc98":"code","5331436e":"code","454aeb93":"code","e5b350a5":"code","514721ed":"code","c4602347":"code","873bf222":"code","1726691a":"code","3c31bde3":"code","c9252c84":"code","67778043":"code","e7d9812d":"code","bd65c912":"code","e0777f19":"code","8711d808":"code","dcb31b89":"code","b2afc270":"code","891283ce":"code","4314de81":"code","db8b8b4c":"code","73dfd20c":"code","79795d15":"code","6161934b":"code","00637ef6":"code","d6527f9d":"code","08e946cc":"code","b9412415":"code","78726110":"code","5bd592cd":"code","24d34e46":"code","b7ca1687":"code","b83ca356":"code","5795451a":"code","ad6014ab":"code","1767f676":"code","b0b63099":"code","f78492c2":"code","e94f37e3":"code","d2e23014":"code","5f7e762c":"code","6c74790d":"code","f8824601":"code","59b3c8ba":"code","bbcb7197":"code","9a23c013":"code","d5c402e3":"code","dc3baef1":"code","874be4ee":"code","3d9e6b6a":"code","571f52f4":"code","94bd85fb":"code","94287fc3":"code","2cc97f45":"markdown","c1e16567":"markdown","2363c454":"markdown","897217af":"markdown","b58a789c":"markdown","fd7b41a8":"markdown","56d07dc4":"markdown","9f1fc760":"markdown","cb1d58aa":"markdown","aa86e206":"markdown","56caadc7":"markdown","b7148d0c":"markdown","dcadd4e5":"markdown","21c4975a":"markdown","bd1ee1d3":"markdown","d68a05e8":"markdown","cacc484d":"markdown","ef70a30c":"markdown","fb1935db":"markdown","5920cddc":"markdown","23942e2b":"markdown","a0f05c3e":"markdown","19de9704":"markdown","c02059b2":"markdown","95907273":"markdown","d367315a":"markdown","047e13d9":"markdown","711dda6b":"markdown","8694be07":"markdown","5e82dd51":"markdown","63c20694":"markdown","5b4535fa":"markdown","5fc177db":"markdown","c4514832":"markdown","7969f9f7":"markdown","6f643599":"markdown","0b146e18":"markdown","0b3cdb04":"markdown","eee52db5":"markdown","1dd7d9d0":"markdown","0ddab66c":"markdown","6376a0f7":"markdown","907f6d1d":"markdown","2405c384":"markdown","48d4440a":"markdown","35df0c18":"markdown","f6ae0e53":"markdown","d7c00a07":"markdown"},"source":{"65e9ab88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection\nimport math\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"\/kaggle\/input\/titanic\"))\n\n# Any results you write to the current directory are saved as output.","48560f3e":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ny = train_data['Survived']\ntest_id = test_data['PassengerId']\ntrain_id = train_data['PassengerId']","d392a5ea":"train_data.head(10)","8a1cc305":"train_data.info()","ca3086df":"test_data.head(10)","0041e23e":"test_data.info()","1e62b843":"train_data.Cabin.unique()","f0e840ec":"train_data.drop(['Cabin', 'Ticket'], axis = 1, inplace = True)\ntest_data.drop(['Cabin', 'Ticket'], axis = 1, inplace = True)","75d376c6":"test_data = test_data.drop(['PassengerId'], axis = 1)\ntrain_data = train_data.drop(['PassengerId'], axis = 1)","719c793c":"train_data.info()","a93e75b7":"train_data['Title'] = train_data['Name'].str.extract(\"([A-za-a]+)\\.\", expand = False) \ntest_data['Title'] = test_data['Name'].str.extract(\"([A-za-a]+)\\.\", expand = False)","0b5f310b":"train_data.Title.unique()","ede6aa27":"test_data.Title.unique()","b885aab6":"train_data.drop(['Name'], axis = 1, inplace = True)\ntest_data.drop(['Name'], axis = 1, inplace = True)","0420830c":"train_data.Title.value_counts()","160ca00c":"test_data.Title.value_counts()","d3da18b9":"train_data['Title'] = train_data['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev', \n                                                   'Countess', 'Don', 'Dona', 'Jonkheer', \n                                                  'Lady',  'Sir'], 'Rare')\n\ntest_data['Title'] = test_data['Title'].replace(['Capt', 'Col', 'Major', 'Dr', 'Rev', \n                                                 'Countess', 'Don', 'Dona', 'Jonkheer', \n                                                  'Lady',  'Sir'], 'Rare')\n\ntrain_data['Title'] = train_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs') \n\ntest_data['Title'] = test_data['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')","cb38dd72":"mfembarked = train_data['Embarked'].value_counts().idxmax()\ntrain_data['Embarked'] = train_data['Embarked'].fillna(mfembarked)\nprint(mfembarked)","f367cc98":"mfembarked = test_data['Embarked'].value_counts().idxmax()\nprint(mfembarked)","5331436e":"test_data['Embarked'] = test_data['Embarked'].fillna(mfembarked)","454aeb93":"train_data['FSize'] = 1 + train_data['SibSp'] + train_data['Parch']\ntest_data['FSize'] = 1 + test_data['SibSp'] + test_data['Parch']\n\ntrain_data['IsAlone'] = train_data['FSize'].apply(lambda x: 1 if x==1 else 0)\ntest_data['IsAlone'] = test_data['FSize'].apply(lambda x: 1 if x==1 else 0)","e5b350a5":"ga = sns.heatmap(train_data.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","514721ed":"train_data[np.isnan(train_data['Age'])]","c4602347":"all_data = pd.concat([train_data, test_data])\n\ngrouped = all_data.iloc[:len(all_data)].groupby(['Pclass', 'Title'])\ngrouped_median = grouped.median()\ngrouped_median = grouped_median.reset_index()[['Pclass','Title', 'Age']]\n\n#print(grouped_median)\n\ndef fill_age(row):\n    condition = (\n        (grouped_median['Pclass'] == row['Pclass']) & \n        (grouped_median['Title'] == row['Title'])\n    ) \n    return grouped_median[condition]['Age'].values[0]\n\ntrain_data['Age'] = train_data.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\ntest_data['Age'] = test_data.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)","873bf222":"grouped_fare = all_data.groupby(['Pclass'])\ngrouped_median_fare = grouped_fare.median()\ngrouped_median_fare = grouped_median_fare.reset_index()[['Pclass', 'Fare']]\n\ndef fill_fare_test(row):\n    condition = (\n        (grouped_median_fare['Pclass'] == row['Pclass'])\n    ) \n    return grouped_median_fare[condition]['Fare'].values[0]\n\ntest_data['Fare'] = test_data.apply(lambda row: fill_fare_test(row) if np.isnan(row['Fare']) else row['Fare'], axis=1)","1726691a":"train_data['Mr'] = train_data['Title'].map(lambda x : 1 if x == 'Mr' else 0)\ntrain_data['Miss'] = train_data['Title'].map(lambda x : 1 if x == 'Miss' else 0)\ntrain_data['Mrs'] = train_data['Title'].map(lambda x : 1 if x == 'Mrs' else 0)\ntrain_data['Master'] = train_data['Title'].map(lambda x : 1 if x == 'Master' else 0)\ntrain_data['Rare'] = train_data['Title'].map(lambda x : 1 if x == 'Rare' else 0)\n\ntrain_data['S'] = train_data['Embarked'].map(lambda x : 1 if x == 'S' else 0)\ntrain_data['C'] = train_data['Embarked'].map(lambda x : 1 if x == 'C' else 0)\ntrain_data['Q'] = train_data['Embarked'].map(lambda x : 1 if x == 'Q' else 0)\n\ntest_data['Mr'] = test_data['Title'].map(lambda x : 1 if x == 'Mr' else 0)\ntest_data['Miss'] = test_data['Title'].map(lambda x : 1 if x == 'Miss' else 0)\ntest_data['Mrs'] = test_data['Title'].map(lambda x : 1 if x == 'Mrs' else 0)\ntest_data['Master'] = test_data['Title'].map(lambda x : 1 if x == 'Master' else 0)\ntest_data['Rare'] = test_data['Title'].map(lambda x : 1 if x == 'Rare' else 0)\n\ntest_data['S'] = test_data['Embarked'].map(lambda x : 1 if x == 'S' else 0)\ntest_data['C'] = test_data['Embarked'].map(lambda x : 1 if x == 'C' else 0)\ntest_data['Q'] = test_data['Embarked'].map(lambda x : 1 if x == 'Q' else 0)","3c31bde3":"pd.qcut(train_data['Age'], 6, duplicates = 'drop')","c9252c84":"sns.distplot(train_data['Age'])","67778043":"pd.qcut(train_data['Fare'], 4)","e7d9812d":"sns.distplot(train_data['Fare'])","bd65c912":"train_data['F1'] = train_data['Fare'].map(lambda x: 1 if x <= 7.91 else 0)\ntrain_data['F2'] = train_data['Fare'].map(lambda x: 1 if (x > 7.91 and x <= 14.454) else 0)\ntrain_data['F3'] = train_data['Fare'].map(lambda x: 1 if (x > 14.454 and x <= 31.0) else 0)\ntrain_data['F4'] = train_data['Fare'].map(lambda x: 1 if (x > 31.0) else 0)\n\ntrain_data['FSize'] = 1 + train_data['SibSp'] + train_data['Parch']\ntrain_data['Single'] = train_data['FSize'].map(lambda x : 1 if x == 1 else 0)\ntrain_data['SmallF'] = train_data['FSize'].map(lambda x : 1 if (x > 1 and x < 5) else 0)\ntrain_data['LargeF'] = train_data['FSize'].map(lambda x : 1 if (x > 4) else 0)\n\ntrain_data['Male'] = train_data['Sex'].map(lambda x: 1 if x == 'male' else 0)\ntrain_data['Female'] = train_data['Sex'].map(lambda x: 1 if x == 'female' else 0)\n\ntrain_data['Class1'] = train_data['Pclass'].map(lambda x: 1 if x == 1 else 0)\ntrain_data['Class2'] = train_data['Pclass'].map(lambda x: 1 if x == 2 else 0)\ntrain_data['Class3'] = train_data['Pclass'].map(lambda x: 1 if x == 3 else 0)\n\ntrain_data['A1'] = train_data['Age'].map(lambda x : 1 if x <= 18 else 0) \ntrain_data['A2'] = train_data['Age'].map(lambda x : 1 if (x <= 24 and x > 18) else 0)\ntrain_data['A3'] = train_data['Age'].map(lambda x : 1 if (x <= 26 and x > 24) else 0)\ntrain_data['A4'] = train_data['Age'].map(lambda x : 1 if (x <= 32.167 and x > 26) else 0)\ntrain_data['A5'] = train_data['Age'].map(lambda x : 1 if (x <= 42 and x > 32.167) else 0)\ntrain_data['A6'] = train_data['Age'].map(lambda x : 1 if x > 42 else 0)\n\ntest_data['F1'] = test_data['Fare'].map(lambda x: 1 if x <= 7.91 else 0)\ntest_data['F2'] = test_data['Fare'].map(lambda x: 1 if (x > 7.91 and x <= 14.454) else 0)\ntest_data['F3'] = test_data['Fare'].map(lambda x: 1 if (x > 14.454 and x <= 31.0) else 0)\ntest_data['F4'] = test_data['Fare'].map(lambda x: 1 if (x > 31.0) else 0)\n\ntest_data['FSize'] = 1 + test_data['SibSp'] + test_data['Parch']\ntest_data['Single'] = test_data['FSize'].map(lambda x : 1 if x == 1 else 0)\ntest_data['SmallF'] = test_data['FSize'].map(lambda x : 1 if (x > 1 and x < 5) else 0)\ntest_data['LargeF'] = test_data['FSize'].map(lambda x : 1 if (x > 4) else 0)\n\ntest_data['Male'] = test_data['Sex'].map(lambda x: 1 if x == 'male' else 0)\ntest_data['Female'] = test_data['Sex'].map(lambda x: 1 if x == 'female' else 0)\n\ntest_data['Class1'] = test_data['Pclass'].map(lambda x: 1 if x == 1 else 0)\ntest_data['Class2'] = test_data['Pclass'].map(lambda x: 1 if x == 2 else 0)\ntest_data['Class3'] = test_data['Pclass'].map(lambda x: 1 if x == 3 else 0)\n\ntest_data['A1'] = test_data['Age'].map(lambda x : 1 if x <= 18 else 0) \ntest_data['A2'] = test_data['Age'].map(lambda x : 1 if (x <= 24 and x > 18) else 0)\ntest_data['A3'] = test_data['Age'].map(lambda x : 1 if (x <= 26 and x > 24) else 0)\ntest_data['A4'] = test_data['Age'].map(lambda x : 1 if (x <= 32.167 and x > 26) else 0)\ntest_data['A5'] = test_data['Age'].map(lambda x : 1 if (x <= 42 and x > 32.167) else 0)\ntest_data['A6'] = test_data['Age'].map(lambda x : 1 if x > 42 else 0)","e0777f19":"sns.barplot(x = 'Sex', y = 'Survived',  data=train_data)","8711d808":"sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train_data)","dcb31b89":"sns.barplot(x = 'FSize', y = 'Survived', data = train_data)","b2afc270":"def farebin(row):\n    if row['F1'] == 1:\n        x = 0\n    elif row['F2'] == 1:\n        x = 1;\n    elif row['F3'] == 1:\n        x = 2;\n    elif row['F4'] == 1:\n        x = 3;\n    else:\n        x = 4\n        \n    return x\n\ntrain_data['Fare_bin'] = train_data.apply(lambda row: farebin(row), axis = 1)\ntest_data['Fare_bin'] = test_data.apply(lambda row: farebin(row), axis = 1)","891283ce":"sns.barplot(x = 'Fare_bin', y = 'Survived', data = train_data)","4314de81":"sns.barplot(x = 'SibSp', y = 'Survived', data = train_data)","db8b8b4c":"sns.barplot(x = 'Parch', y = 'Survived', data = train_data)","73dfd20c":"def agebin(row):\n    if row['Age'] <= 18:\n        x = 0\n    elif row['Age'] <= 24:\n        x = 1\n    elif row['Age'] <= 26:\n        x = 2\n    elif row['Age'] <= 32.167:\n        x = 3\n    elif row['Age'] <= 42:\n        x = 4\n    else:\n        x = 5\n        \n    return x\n\ntrain_data['Age_bin'] = train_data.apply(lambda row: agebin(row), axis = 1)\ntest_data['Age_bin'] = test_data.apply(lambda row: agebin(row), axis = 1)\n\nsns.barplot(x = 'Age_bin', y = 'Survived', data = train_data)","79795d15":"def FamSize(row):\n    if row['FSize'] == 1:\n        x = '1'\n    elif row['FSize'] == 2:\n        x = '2'\n    elif row['FSize'] == 3:\n        x = '3'\n    elif row['FSize'] == 4:\n        x = '4'\n    elif row['FSize'] == 5:\n        x = '5'\n    else:\n        x = '>=6'\n    return x\n\ntrain_data['FamSize'] = train_data.apply(lambda row: FamSize(row), axis = 1)\ntest_data['FamSize'] = test_data.apply(lambda row: FamSize(row), axis = 1)\n\nsns.barplot(x = 'FamSize', y = 'Survived', data = train_data)","6161934b":"def fbin(row):\n    if row['Single'] == 1:\n        x = 0\n    elif row['SmallF'] == 1:\n        x = 1;\n    else:\n        x = 2\n        \n    return x\n\ntrain_data['F_bin'] = train_data.apply(lambda row: fbin(row), axis = 1)\nsns.barplot(x = 'F_bin', y = 'Survived', data = train_data)","00637ef6":"sns.barplot(x = 'Embarked', y = 'Survived', data = train_data)","d6527f9d":"sns.barplot(x = 'Title', y = 'Survived', data = train_data)","08e946cc":"def Title_cat(row):\n    if row['Mr'] == 1 or row['Mrs'] == 1 or row['Miss'] == 1:\n        x = 'Normal'\n    elif row['Master'] == 1:\n        x = 'Child';\n    else:\n        x = 'Powerful';\n    return x\n    \ntrain_data['Title_cat'] = train_data.apply(lambda row: Title_cat(row), axis = 1)\ntest_data['Title_cat'] = test_data.apply(lambda row: Title_cat(row), axis = 1)\n\nsns.barplot(x = 'Title_cat', y = 'Survived', data = train_data[train_data['Male'] == 1])","b9412415":"sns.barplot(x = 'Title_cat', y = 'Survived', data = train_data[train_data['Female'] == 1])","78726110":"train_data.info()","5bd592cd":"train_data = pd.get_dummies(train_data)\ntest_data = pd.get_dummies(test_data)","24d34e46":"feats = ['Age',\n          'Class1', 'Class2', 'Class3', \n          'Male', 'Female', 'Fare', \n          'Single', 'SmallF', 'LargeF', 'SibSp', 'Parch',\n         'S', 'C', 'Q', 'Mr', 'Mrs', 'Miss', 'Master', 'Rare']","b7ca1687":"train = train_data[feats]\ntest = test_data[feats]","b83ca356":"from sklearn.pipeline import make_pipeline\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score","5795451a":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state = 3)\nprint(cross_val_score(rf, train, y, scoring = 'accuracy', cv = 5).mean())","ad6014ab":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\npipeline_rf = Pipeline(\n                    [ \n                     ('sca', RobustScaler()),\n                     ('rf', RandomForestClassifier(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['rf__min_samples_split'] = [2,3,4,5] \nparameters['rf__max_depth'] = [4, 6, 8, None] \nparameters['rf__n_estimators'] = [10, 25, 50, 100] \n\n#CV = GridSearchCV(pipeline_rf, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_) \n\n#Best score and parameter combination = \n#0.8338945005611672\n#{'rf__max_depth': 4, 'rf__min_samples_split': 2, 'rf__n_estimators': 100}","1767f676":"pipeline_rf = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('rf', RandomForestClassifier(random_state = 3, max_depth = 4, min_samples_split = 2, n_estimators = 100))\n                     \n])","b0b63099":"import xgboost as xgb\n\npipeline_xgb = Pipeline(\n                    [ \n                     ('sca', RobustScaler()),\n                     ('xgb', xgb.XGBClassifier(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['xgb__min_child_weight'] = [0, 0.5, 1.0] \nparameters['xgb__max_depth'] = [4,6,8] \nparameters['xgb__eta'] = [0.3, 0.1, 0.05] \nparameters['xgb__subsample'] = [0.5, 0.75, 1.0] \nparameters['xgb__n_estimators'] = [15, 25, 45] \nparameters['xgb__gamma'] = [0, 0.5, 1.0]\nparameters['xgb__colsample_by'] = [0.2, 0.6, 1.0]\n\n#CV = GridSearchCV(pipeline_xgb, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_) \n\n#Best score and parameter combination = \n#0.8462401795735129\n#{'xgb__colsample_by': 0.2, 'xgb__eta': 0.3, 'xgb__gamma': 1.0, 'xgb__max_depth': 6, 'xgb__min_child_weight': 0, 'xgb__n_estimators': 15, 'xgb__subsample': 0.75, 'xgb__tree_method': 'auto'}","f78492c2":"pipeline_xgb = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('xgb', xgb.XGBClassifier(random_state = 3, max_depth = 6, gamma = 1.0, min_child_weight = 0, subsample = 0.75, n_estimators = 15, eta = 0.3, colsample_by = 0.2, tree_method = 'auto'))\n                     \n])","e94f37e3":"from sklearn.svm import SVC\n\npipeline_svc = Pipeline(\n                    [\n                     ('sca', RobustScaler()),\n                     ('svc', SVC(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['svc__kernel'] = ['rbf', 'poly', 'sigmoid', 'linear'] \nparameters['svc__C'] = [0.1, 0.2, 0.4, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0] \n\n#CV = GridSearchCV(pipeline_svc, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_) \n\n#Best score and parameter combination = \n#0.8338945005611672\n#{'svc__C': 3.0, 'svc__kernel': 'poly'}","d2e23014":"pipeline_svc = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('svc', SVC(random_state = 3, kernel = 'poly', C = 3.0))\n                     \n])","5f7e762c":"from sklearn.linear_model import LogisticRegression\n\npipeline_lr = Pipeline(\n                    [\n                     ('sca', RobustScaler()),\n                     ('lr', LogisticRegression(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['lr__l1_ratio'] = [0, 0.5, 1.0]  \n\n#CV = GridSearchCV(pipeline_lr, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_) \n\n#Best score and parameter combination = \n#0.8271604938271605\n#{'lr__l1_ratio': 0}","6c74790d":"pipeline_lr = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('lr', LogisticRegression(random_state = 3, l1_ratio = 0))\n                     \n])","f8824601":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\npipeline_ada = Pipeline(\n                    [\n                     ('sca', RobustScaler()),\n                     ('ada', AdaBoostClassifier(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['ada__learning_rate'] = [0.05, 0.1, 0.25, 0.5, 0.75, 1.0]  \nparameters['ada__n_estimators'] = [5, 10, 15, 20, 25, 35, 45, 60, 80, 100, 120] \nparameters['ada__base_estimator'] = [DecisionTreeClassifier(max_depth = 1), DecisionTreeClassifier(max_depth = 3), \n                                    DecisionTreeClassifier(max_depth = 5), DecisionTreeClassifier(max_depth = None)]\n\n#CV = GridSearchCV(pipeline_ada, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_)\n\n#Best score and parameter combination = \n#0.8305274971941639\n#{'ada__base_estimator': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n#                       max_features=None, max_leaf_nodes=None,\n#                       min_impurity_decrease=0.0, min_impurity_split=None,\n#                       min_samples_leaf=1, min_samples_split=2,\n#                       min_weight_fraction_leaf=0.0, presort=False,\n#                       random_state=None, splitter='best'), 'ada__learning_rate': 0.05, 'ada__n_estimators': 15}","59b3c8ba":"pipeline_ada = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('ada', AdaBoostClassifier(random_state = 3, n_estimators = 15, learning_rate = 0.05, base_estimator = DecisionTreeClassifier(max_depth = 3)))\n                     \n])","bbcb7197":"from sklearn.ensemble import GradientBoostingClassifier\n\npipeline_gb = Pipeline(\n                    [\n                     ('sca', RobustScaler()),\n                     ('gb', GradientBoostingClassifier(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['gb__learning_rate'] = [0.05, 0.1, 0.15, 0.2]  \nparameters['gb__n_estimators'] = [30, 45, 60, 80, 100, 120] \nparameters['gb__max_depth'] = [1,2,3,4,5,6] \nparameters['gb__subsample'] = [0.2, 0.6, 1.0]\nparameters['gb__min_samples_split'] = [2,4,6]\nparameters['gb__min_samples_leaf'] = [1,2,3]\n\n#CV = GridSearchCV(pipeline_gb, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_)\n\n#Best score and parameter combination = \n#0.8451178451178452\n#{'gb__learning_rate': 0.1, 'gb__max_depth': 4, 'gb__min_samples_leaf': 1, 'gb__min_samples_split': 6, 'gb__n_estimators': 30, 'gb__subsample': 0.6}","9a23c013":"pipeline_gb = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('gb', GradientBoostingClassifier(random_state = 3, learning_rate = 0.1, max_depth = 4, n_estimators = 30, min_samples_leaf = 1, min_samples_split = 6, subsample = 0.6))\n                     \n])","d5c402e3":"from sklearn.linear_model import RidgeClassifier\n\npipeline_rc = Pipeline([('sca', RobustScaler()), ('rc', RidgeClassifier(random_state = 1))])\n\ncross_val_score(pipeline_rc, train, y, scoring = 'accuracy', cv = 5).mean()","dc3baef1":"from sklearn.naive_bayes import GaussianNB\n\npipeline_nb = Pipeline([('sca', RobustScaler()), ('nb', GaussianNB())])\n\ncross_val_score(pipeline_nb, train, y, scoring = 'accuracy', cv = 5).mean()","874be4ee":"import lightgbm as lgb\n\npipeline_lgb = Pipeline(\n                    [\n                     ('sca', RobustScaler()),\n                     ('lgb', lgb.LGBMClassifier(random_state = 3))\n                     \n])\n\nparameters = {}\nparameters['lgb__n_estimators'] = [10, 25, 50, 100, 250] \nparameters['lgb__learning_rate'] = [0.25, 0.1, 0.05, 0.01]\nparameters['lgb__subsample'] = [0.2, 0.6, 1.0]\nparameters['lgb__colsample_bytree'] = [0.2, 0.6, 1.0]\nparameters['lgb__min_child_samples'] = [10, 20, 30]\n\n#CV = GridSearchCV(pipeline_lgb, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_)\n\n#Best score and parameter combination = \n#0.8428731762065096\n#{'lgb__colsample_bytree': 1.0, 'lgb__learning_rate': 0.1, 'lgb__min_child_samples': 30, 'lgb__n_estimators': 100, 'lgb__subsample': 0.2}\n","3d9e6b6a":"pipeline_lgb = Pipeline(\n                    [('sca', RobustScaler()),\n                     ('lgb', lgb.LGBMClassifier(random_state = 3, learning_rate = 0.1, subsample = 0.2, n_estimators = 100, min_child_samples = 30, colsample_bytree = 1.0))\n                     \n])","571f52f4":"from sklearn.neighbors import KNeighborsClassifier\n\npipeline_knn = Pipeline(\n                    [\n                     ('sca', RobustScaler()),\n                     ('knn', KNeighborsClassifier())\n                     \n])\n\nparameters = {}\nparameters['knn__n_neighbors'] = [2,3,4,5,6,7,8,9,10]\nparameters['knn__algorithm'] = ['auto', 'ball_tree', 'kd_tree', 'brute']\nparameters['knn__weights'] = ['uniform', 'distance']\n\n#CV = GridSearchCV(pipeline_knn, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_)\n\n#Best score and parameter combination = \n#0.8125701459034792\n#{'knn__algorithm': 'ball_tree', 'knn__n_neighbors': 5, 'knn__weights': 'uniform'}","94bd85fb":"from mlxtend.classifier import StackingCVClassifier\n\nsc = Pipeline( [ ('sca', RobustScaler()), ('sc', StackingCVClassifier(classifiers = [KNeighborsClassifier(),\n                                                                                     SVC(random_state = 1),\n                                                                                     GradientBoostingClassifier(random_state = 3)],\n                                                                      meta_classifier = RandomForestClassifier(random_state = 2))) ])\n\nparameters = {}\n\n\nparameters['sc__meta_classifier__min_samples_split'] = [2] \nparameters['sc__meta_classifier__max_depth'] = [8] \nparameters['sc__meta_classifier__n_estimators'] = [15] \n\nparameters['sc__kneighborsclassifier__n_neighbors'] = [5]\n\nparameters['sc__svc__kernel'] = ['rbf', 'poly', 'sigmoid', 'linear'] \nparameters['sc__svc__C'] = [0.1, 0.5, 1.0, 2.0, 3.0] \n\nparameters['sc__gradientboostingclassifier__subsample'] = [0.5, 0.75, 1.0] \nparameters['sc__gradientboostingclassifier__max_depth'] = [4, 6, 8]\n\n#CV = GridSearchCV(sc, parameters, scoring = 'accuracy', n_jobs= 4, cv = 5, verbose = 10)\n#CV.fit(train, y)   \n\n#print('Best score and parameter combination = ')\n\n#print(CV.best_score_)    \n#print(CV.best_params_)\n\n#Best score and parameter combination = \n#0.8406285072951739\n#{'sc__gradientboostingclassifier__max_depth': 4, 'sc__gradientboostingclassifier__subsample': 0.5,\n#'sc__kneighborsclassifier__n_neighbors': 5, 'sc__meta_classifier__max_depth': 8, \n#'sc__meta_classifier__min_samples_split': 2, 'sc__meta_classifier__n_estimators': 15, \n#'sc__svc__C': 2.0, 'sc__svc__kernel': 'poly'}","94287fc3":"sc = Pipeline( [ ('sca', RobustScaler()), ('sc', StackingCVClassifier(classifiers = [KNeighborsClassifier(),\n                                                                                     SVC(random_state = 1, kernel = 'poly', C = 2.0),\n                                                                                     GradientBoostingClassifier(random_state = 3, max_depth = 4, subsample = 0.5)],\n                                                                      meta_classifier = RandomForestClassifier(random_state = 2, n_estimators = 15, max_depth = 8))) ])","2cc97f45":"Some titles, like Col, Capt, etc. are minority, hence can be classified as \"Rare\". We make corresponding replacements:","c1e16567":"LightGBM:","2363c454":"Reading the input data:","897217af":"Some base models have really good scores (XGBClassifier being the best so far). We can try is stacking models: in this example we do it with KNN, SVM and Naive Bayes, using Random Forest as meta-model.","b58a789c":"From the heatmap, it is clear that age correlates with Pclass more than with any other numerical feature. Moreover, age should be filled in according to Title, as well as to Pclass (e.g. someone with a title Master can not be 27\/28 y.o. which is the median age)","fd7b41a8":"Accuracy is already quite high, and can be improved using grid search. We also use RobustScaler in order to avoid errors due to different scales for different variables.","56d07dc4":"An interesting thing to look at would be the dependence of survival on whether a person's title is Rare or not:","9f1fc760":"We only need to fill in the fare for test data, which we can easily do by replacing with median value grouped by class, since fare correlates with class more than with anything else (see heatmap):","cb1d58aa":"AdaBoost:","aa86e206":"Trying with XGBoost:","56caadc7":"In this kernel I am going to show some simple ideas about extracting important features from data, imputing missing values and choosing suitable ML models, using Titanic dataset.","b7148d0c":"Survival dependence on title:","dcadd4e5":"Clearly, the chances of survival for people age 24-26 are the lowest. This fact might not be captured if we use less age bins, so it might even be reasonable to use age as a continuous variable (i.e. without bins). We might also want to explore survival based on family size, but we encode it as if it was categorical variable:","21c4975a":"Survival depending on whether a person travels on their own, with a small family or with a large family:","bd1ee1d3":"Logistic Regression:","d68a05e8":"We can now try various combinations of features for our model. For example, I try the combination below, but it is totally possible to use any other combination of features. In fact I believe it is possible to achieve better results (on public leaderboard) using some other combination of features\/hyperparameters, so everything I show below is just an example.","cacc484d":"After that, we add extra columns, which correspond to Fare bin, Age bin and Family Size category (i.e. single, small family or large family):","ef70a30c":"Binning the Fare:","fb1935db":"Survival dependence on port of departure:","5920cddc":"Other useful features might be family size and whether a person is travelling alone.","23942e2b":"Overall, there are a few conclusions I would like to say:\n\n1) What makes a model robust is primarily the features we select\/extract. Even fine tuning or using sophisticated algorithm does not improve the result dramatically, and in this work it was done only for learning purposes. \n\n2) There are many ways to improve performance, such as extracting informations about which group each passenger travels with (e.g. https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210), but this goes beyond the simpler model I am considering. In general, one can experiment trying different features.\n\n3) Titanic dataset itself is random to some degree - there are many factors that no model can take into account (such as physical\/psychological condition of people at the moment of the catastrophe), so 80% is already a good score.","a0f05c3e":"Now we have all the data imputed. The next important step would be to actually select the features to train the model. First, we convert some categorical features to numerical, or, more precisely, logical (we can not just assign some number to title like 1,2,3,4 since it might affect the accuracy of classification algorithm).","19de9704":"Naive Bayes:","c02059b2":"Survival based on the fare and age bin:","95907273":"A useful feature we might extract is the title:","d367315a":"Thank you for your time, I hope you found some of the ideas useful! p.s. Please upvote :)","047e13d9":"Ridge Classifier:","711dda6b":"Now we can drop the name column:","8694be07":"Now we can explore how many of each title there are:","5e82dd51":"In this example, the cross-validation accuracy is actually higher for stacked model than for each of the models separately.","63c20694":"Another thing we can do is to bin continuous numerical data using qcut, i.e. splitting into buckets of equal size. Then each Fare, Age, etc. value is assigned the corresponding bin:","5b4535fa":"Quite a lot of data about the cabin is missing:","5fc177db":"We should also drop passenger ID:","c4514832":"Fare distribution:","7969f9f7":"Now we need to fill in the age, which might be a bit tricky. Just filling with median age may not always be correct. For example, someone who has title \"Master\", can not be assigned the median age of 28 (which is the median age of the training set). That is why it makes sense to replace the missing age with median grouped by title and class:","6f643599":"Some insights into training and test set:","0b146e18":"Let us start with something simple, like Random forest:","0b3cdb04":"Very clear trend of survival vs fare bin (with 5 bins for fare it does not work so well).","eee52db5":"We can now see some trends, e.g. medium family size is more likely to survive or more expensive fare increases chances of survival. Now we can start making models.","1dd7d9d0":"Age distribution:","0ddab66c":"In both training and test data S is the most frequent value for embarked, so it can be imputed in both sets:","6376a0f7":"Clearly, someone with a 'Rare' title has a greater chance of survival than someone called 'Mr' but lower chance than children (for males). For females, someone with a rare title has a higher chance of survival as well. This might be due to the fact that someone with Rare title belongs to upper class, buys expensive ticket or other people simply care about them more (as a consequence of the first two :( )","907f6d1d":"The letter the cabin starts with might matter, however a lot of them are missing, and imputing them with the same value does not make much sense. Hence we can safely drop the cabin (and ticket) column: ","2405c384":"Gradient Boosting Classifier:","48d4440a":"K-nearest neighbors classifier:","35df0c18":"Now we can try SVC:","f6ae0e53":"Now we need to fill in age and Embarked in train data. Embarked can just be filled with most frequently occuring value:","d7c00a07":"Exploratory data analysis:"}}