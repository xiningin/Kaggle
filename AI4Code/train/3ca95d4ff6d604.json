{"cell_type":{"b6682121":"code","8cf4ecfb":"code","7e4b3e24":"code","a43968dc":"code","141a5aff":"code","022b5e8e":"code","5cc467cc":"code","a4c71326":"code","9386e2b4":"markdown","1f6f1473":"markdown"},"source":{"b6682121":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport lightgbm as lgb\nfrom pathlib import Path\nimport seaborn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom warnings import simplefilter\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","8cf4ecfb":"!pip install kaggler","7e4b3e24":"import kaggler\nfrom kaggler.model import AutoLGB\nprint(kaggler.__version__)\nplt.style.use('fivethirtyeight')\npd.set_option('max_columns', 100)\nsimplefilter('ignore')","a43968dc":"data_dir = Path('..\/input\/tabular-playground-series-jun-2021')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\nid_col = 'id'\ntarget_col = 'target'\n\nn_fold = 5\nseed = 42","141a5aff":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\nsub = pd.read_csv(sample_file, index_col=id_col)\nprint(trn.shape, tst.shape, sub.shape)","022b5e8e":"n_trn = trn.shape[0]\ndf = pd.concat([trn.drop(target_col, axis=1), tst], axis=0)\nprint(df.shape)","5cc467cc":"cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\nX = df\ny = pd.Series(np.concatenate([np.zeros(n_trn,), np.ones(df.shape[0] - n_trn,)]))\np = np.zeros_like(y, dtype=float)\nfor i, (i_trn, i_val) in enumerate(cv.split(X, y)):\n    if i == 0:\n        clf = AutoLGB(objective='binary', metric='auc', random_state=seed)\n        clf.tune(X.iloc[i_trn], y[i_trn])\n        features = clf.features\n        params = clf.params\n        n_best = clf.n_best\n        print(f'{n_best}')\n        print(f'{params}')\n        print(f'{features}')\n    \n    trn_data = lgb.Dataset(X.iloc[i_trn], y[i_trn])\n    val_data = lgb.Dataset(X.iloc[i_val], y[i_val])\n    clf = lgb.train(params, trn_data, n_best, val_data, verbose_eval=100)\n    p[i_val] = clf.predict(X.iloc[i_val])\n    print(f'CV #{i + 1} AUC: {roc_auc_score(y[i_val], p[i_val]):.6f}')","a4c71326":"print(f'CV AUC: {roc_auc_score(y, p):.6f}')","9386e2b4":"# Reference: https:\/\/www.kaggle.com\/jeongyoonlee\/adversarial-validation-with-lightgbm by Jeong-Yoon Lee","1f6f1473":"# Conclusion\nAdversarial validation AUC score is close to 50%. Therefore, we can say that the training and test data sets are similar in terms of feature distributions. In other words, no big shake-up expected at the end of the competition. :)"}}