{"cell_type":{"37d41a22":"code","d764efda":"code","852eb098":"code","cec3687a":"code","f449b78c":"code","361895cd":"code","28deda2c":"code","8a88880b":"code","64fa59e9":"code","78039c37":"code","c1061d63":"code","3e6875ac":"code","2ac7a702":"code","b1dfb379":"code","37c4f1e8":"code","2dd06af5":"code","dde510d2":"code","8edc2071":"code","d21eab43":"code","40ca51bc":"code","447679b1":"code","d3ea984a":"code","76363c6b":"code","b53ef3ed":"code","9c981bae":"markdown","78a11aca":"markdown","6f523483":"markdown","d859da70":"markdown","60e3a437":"markdown","fd809458":"markdown","11f6e700":"markdown"},"source":{"37d41a22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d764efda":"train = pd.read_csv('\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/train.csv')","852eb098":"train.head()","cec3687a":"import matplotlib.pyplot as plt\nimport seaborn as sns","f449b78c":"data_dir = '\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/train\/'","361895cd":"# function to read data\n\ndef combine_csv():\n    def get_data(index):\n        data = pd.read_csv(data_dir + str(train.segment_id.iloc[index]) + \".csv\")\n        data['time_to_eruption'] = train['time_to_eruption'].iloc[index]\n        for feat in data.drop('time_to_eruption',1).columns:\n            data[feat] = data[feat].mean()\n        data = data.sample(1)\n        return data\n    \n    def combine():\n        data = pd.DataFrame()\n        for i in range(train.shape[0]):\n            df = get_data(i)\n            data=pd.concat([df,data])\n        return data\n    \n    return combine()\n\ndata = combine_csv()","28deda2c":"# filling null vals with median\n\nfor col in data.columns:\n    median = data[col].median()\n    data[col].fillna(median, inplace=True)\n    \ndata.isnull().sum()","8a88880b":"corr_mx = data.corr()\nsns.heatmap(corr_mx)","64fa59e9":"# Modeling\n\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(['time_to_eruption'], inplace=False, axis=1)\ny = data['time_to_eruption']\n","78039c37":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape\nX_train = np.array(X_train)\ny_train = np.array(y_train)\n\nX_train=X_train.reshape(-1,10,1)\n\nX_train.shape","c1061d63":"from sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans, DBSCAN, MiniBatchKMeans\nfrom sklearn.ensemble import AdaBoostRegressor, VotingRegressor, BaggingRegressor, RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import SGD\n\n#model = keras.models.Sequential()\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(2835, activation = \"relu\"))\n#model.add(keras.layers.Dense(1, activation = \"linear\"))\n\n#sgd = SGD(learning_rate = 0.001)\n#model.compile(optimizer= 'adam' ,loss= 'mean_absolute_error',metrics=['mean_absolute_error'])\n#model.fit(X_train, y_train,epochs=5, batch_size=250, validation_data=(X_val, y_val))\n\n\nmodel = keras.models.Sequential([\nkeras.layers.Conv1D(128, 2, activation = \"relu\", padding = 'valid'),\nkeras.layers.MaxPool1D(2),\nkeras.layers.Conv1D(128, 2, activation = \"relu\", padding = 'valid'),\nkeras.layers.Conv1D(128, 2, activation = \"relu\", padding = 'valid'),\nkeras.layers.MaxPool1D(2),\nkeras.layers.Flatten(),\nkeras.layers.Dense(128, activation = \"relu\"),\nkeras.layers.Dense(128, activation = \"relu\"),\nkeras.layers.Dense(128, activation = \"relu\"),\nkeras.layers.Dense(128, activation = \"relu\"),\nkeras.layers.Dense(128, activation = \"relu\"),\nkeras.layers.Dense(128, activation = \"relu\"),\nkeras.layers.Dense(64, activation = \"relu\"),\nkeras.layers.Dense(1)\n])\nsgd = SGD(learning_rate = 0.001)\nmodel.compile(optimizer= 'adam' ,loss= 'mean_absolute_error',metrics=['mean_absolute_error'])\nmodel.fit(X_train, y_train, epochs=30, batch_size=5)\nmodel.summary()","3e6875ac":"# print(f'Decision Tree: {kmeans(dtr)}\\nSupport Vector Machines: {kmeans(SVR())}\\nLinear Regression: {kmeans(LinearRegression())}')","2ac7a702":"# Best params:\n# Decision Tree: (173512024577686.5, {'kmeans__n_clusters': 450})\n# Support Vector Machines: (182206672899156.47, {'kmeans__n_clusters': 1500})\n# Linear Regression: (6.241083175756122e+16, {'kmeans__n_clusters': 150})","b1dfb379":"# I noticed that mse seems to be minimized at around n_clusters=1000 ( Lets forget linear regression )","37c4f1e8":"# Best params:\n# Decision Tree: (170436558910960.75, {'minikmeans__batch_size': 500, 'minikmeans__n_clusters': 100})\n# Support Vector Machines: (182206653608092.66, {'minikmeans__batch_size': 700, 'minikmeans__n_clusters': 1})\n# Linear Regression: (164458165654294.5, {'minikmeans__batch_size': 200, 'minikmeans__n_clusters': 100})\n# Linear regression seems to improve the most, later might tweak n_clusters_param","2dd06af5":"sample = pd.read_csv('\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv')\n","dde510d2":"test = pd.read_csv(\"..\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv\")\n\n'''Function to get test data'''\ndef get_csv_test(index):\n    \n    test_data = pd.read_csv(test_dir + str(test.segment_id.iloc[index]) + \".csv\")\n    \n    for feat in test_data.columns:\n        test_data[feat] = test_data[feat].mean()\n    \n    test_data = test_data.sample()\n    \n    return(test_data)","8edc2071":"data_test = pd.DataFrame()\ntest_dir = '\/kaggle\/input\/predict-volcanic-eruptions-ingv-oe\/test\/'\n\nfor index in range(test.shape[0]):\n    data_test = pd.concat([get_csv_test(index), data_test])","d21eab43":"data_test.shape\n","40ca51bc":"for i in data_test:\n    data_test[i] = data_test[i].replace(np.nan, data_test[i].mean())\ndata_test.isnull().sum()","447679b1":"data_test.head()","d3ea984a":"X = ['sensor_1','sensor_2','sensor_3','sensor_4','sensor_5','sensor_6','sensor_7','sensor_8','sensor_9','sensor_10']\ntest_X = data_test[X]\ntest_X = np.array(test_X)\ntest_X=test_X.reshape(-1,10,1)\npredicted_time = model.predict(test_X)\nprint(predicted_time)","76363c6b":"test['time_to_eruption'] = predicted_time\nsub = test[['segment_id', 'time_to_eruption']]","b53ef3ed":"sub.head()\nsub.to_csv('submission.csv',index=False)","9c981bae":"**Ensemble learning**","78a11aca":"\nAlso recently, I've read about data preprocessing with clustering algorithms, and I am determined to find something usefull here!","6f523483":"First thing i want to see is how this data reacts to reductions in dimensionality, since this is a topic i've recently learned, and i can't wait to see what i can do with it! ( I still don't understand it fully, so for this version my explorations are left quite shallow )","d859da70":"This is second version of this project. My main purposes are to try many different ML algorithms, approaches, and ways of dealing with problems. I am begginer in ML, so if you have cought any errors or have a suggestion, feel free to write it in a comment :)","60e3a437":"Better that just PCA, but worse that just KMeans, which means that dimensionality reduction in case of this model in counterproductive","fd809458":"Not much difrence compared to PCA. Later in this notebook I'll see how PCA\/KPCA scales with my other methods.","11f6e700":"It seems that MiniBatchKMeans works better that regular KMeans"}}