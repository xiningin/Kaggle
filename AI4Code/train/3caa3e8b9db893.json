{"cell_type":{"af5b8365":"code","62a1aaf5":"code","d598d156":"code","1083a5b3":"code","b6bb1ac3":"code","b684c0bb":"code","b449ba8c":"code","0ba3634b":"code","236b3049":"code","17741efd":"code","16f6c345":"markdown","eb2c61fd":"markdown","cb2b5b1a":"markdown","52305545":"markdown","21c5a78f":"markdown","7aa72463":"markdown","79df0673":"markdown","fbe2ea08":"markdown","7b4b9a67":"markdown","7040f72a":"markdown","60d384d8":"markdown","c8777e58":"markdown","53476a4e":"markdown","5eced9b1":"markdown"},"source":{"af5b8365":"def final_velocity_equation(v_initial, acceleration, time):\n    return (v_initial+acceleration*time)\nprint(final_velocity_equation(5,5,2))","62a1aaf5":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nsize = 30000\ninput_size = 3\noutput_size = 1\ninputs = torch.zeros(*(size, input_size))\noutputs = torch.zeros(*(size,output_size))\n#data generation\nfor i in range(size):\n    #randomize the input values\n    # v initial, acceleration, time\n    values = torch.randint(0, 20, (3,))\n    inputs[i]= values\n    v_initial, acceleration, time = values\n    expected_result = final_velocity_equation(v_initial, acceleration, time)\n    #save the output values\n    outputs[i]=torch.Tensor([final_velocity_equation(v_initial, acceleration, time)])\nprint(\"Generated {} training samples\".format(size))","d598d156":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda:0')\n    else:\n        return torch.device('cpu')\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\ndevice = get_default_device()\nprint(device)","1083a5b3":"from torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n#link our input and output tensors\ndataset = TensorDataset(inputs,outputs)\n\n#returns where to split out indicies for testing and for training\ndef split_indices(n, test_pct=0.1, seed=99):\n    test_val = int(test_pct*n)\n    np.random.seed(seed)\n    indices = np.random.permutation(n)\n    return indices[:test_val], indices[test_val:],\ntest_pct = 0.2\nrand_seed = 42\ntest_indicies, train_indicies = split_indices(len(dataset), test_pct, rand_seed)\nprint(len(train_indicies),len(test_indicies))\n\n#batch size of 1000, can be increased but there may be memory limits\nbatch_size = 1000\n#create our dataloaders\ntrain_sampler = SubsetRandomSampler(train_indicies)\ntrain_loader = DeviceDataLoader(DataLoader(dataset, batch_size, sampler=train_sampler, pin_memory=True, num_workers=4), device)\ntest_sampler = SubsetRandomSampler(test_indicies)\ntest_loader = DeviceDataLoader(DataLoader(dataset, batch_size, sampler=test_sampler, pin_memory=True, num_workers=4), device)","b6bb1ac3":"class PhysicsModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear_1 = nn.Linear(3, 16)\n        self.linear_2 = nn.Linear(16, 32)\n        self.linear_3 = nn.Linear(32, 64)\n        self.linear_4 = nn.Linear(64, 128)\n        self.linear_5 = nn.Linear(128, 256)\n        self.linear_6 = nn.Linear(256, 512)\n        self.linear_7 = nn.Linear(512, 256)\n        self.linear_8 = nn.Linear(256, 1)\n    \n    def forward(self, input_tensor):\n        layer_1 = self.linear_1(input_tensor)\n        layer_1_outputs = F.relu(layer_1)\n        layer_2 = self.linear_2(layer_1_outputs)\n        layer_2_outputs = F.relu(layer_2)\n        layer_3 = self.linear_3(layer_2_outputs)\n        layer_3_outputs = F.relu(layer_3)\n        layer_4 = self.linear_4(layer_3_outputs)\n        layer_4_outputs = F.relu(layer_4)\n        layer_5 = self.linear_5(layer_4_outputs)\n        layer_5_outputs = F.relu(layer_5)\n        layer_6 = self.linear_6(layer_5_outputs)\n        layer_6_outputs = F.relu(layer_6)\n        layer_7 = self.linear_7(layer_6_outputs)\n        layer_7_outputs = F.relu(layer_7)\n        layer_8 = self.linear_8(layer_7_outputs)\n        return layer_8\nphysics_model = PhysicsModel()\nto_device(physics_model, device)\nfor values, outputs in train_loader:\n    predictions = physics_model(values)\n    print(\"Predictions: {}\".format(predictions[0:10]))\n    print(\"The loss of the initialized model is {}\".format(F.mse_loss(predictions,outputs)))\n    break","b684c0bb":"import time\ndef accuracy(outputs, labels, threshold=0):\n    return torch.sum(abs(outputs-labels)<=threshold).item() \/ len(outputs)\ndef test_accuracy(model, test_loader):\n    model.eval()\n    accuracies = []\n    for x, y in test_loader:\n        predictions = model(x)\n        accuracies.append(accuracy(predictions, y, threshold=2))\n    model.train()\n    return round(sum(accuracies)\/len(accuracies),3)\ntest_accuracy(physics_model, test_loader)","b449ba8c":"def fit(epochs, model, train_loader, test_loader, optimizer):\n    accuracies = []\n    losses = []\n    for epoch in range(epochs):\n        for x, y in train_loader:\n            #get predictions\n            predictions = model(x)\n            #calculate the loss\n            loss_value = F.mse_loss(predictions, y)\n            #calculate the derivatives of the weights and biases with respect to the loss\n            loss_value.backward()\n            #update the parameters of the model through the use of the built in optimizer\n            optimizer.step()\n            #zero the gradients\n            optimizer.zero_grad()\n            accuracy_value = test_accuracy(model, test_loader)\n        if (epoch+1)%5==0:\n            accuracies.append(accuracy_value)\n            losses.append(loss_value)\n            print(\"Epoch {}: Loss: {} Test Dataset Accuracy: {}%\".format(epoch+1, round(loss_value.item(),4), round(accuracy_value*100,2)))\n    return accuracies, losses\n        \nepochs = 50\noptimizer = torch.optim.Adam(physics_model.parameters(), lr=0.0005)\nhistory = fit(epochs, physics_model, train_loader, test_loader, optimizer)\ntorch.save(physics_model.state_dict(), \"{}.pth\".format(time.time()))","0ba3634b":"import matplotlib.pyplot as plt\ndef plot_accuracies(history):\n    #plot epochs on the x-axis and accuracy and loss along the y-axis\n    plt.plot(list(range(5,epochs+5,5)), history[0], marker=\"x\", color=\"orange\")\n    plt.plot(list(range(5,epochs+5,5)), history[1], marker=\"x\", color=\"blue\")\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy and loss')\n    plt.title('Test Accuracy and Loss vs. No. of epochs')\n    plt.legend([\"Accuracy\",\"Loss\"])\n    ax = plt.gca()\n    leg = ax.get_legend()\n    leg.legendHandles[0].set_color('orange')\n    leg.legendHandles[1].set_color('blue')\nplot_accuracies(history)","236b3049":"def random_test():\n    values = torch.randint(0, 10, (3,)).numpy()\n    prediction = physics_model(torch.tensor([float(x) for x in values]).to(device))\n    actual = final_velocity_equation(*values)\n    #round the values to the hundreths place so that they can be viewed in an easier manner, but the values that are input in the equation are the full length\n    return [round(x,2) for x in values], round(actual,1), prediction.item()\n#makes sure to not alter derivatives or provide inconsistent results\nphysics_model.eval()\nvalues, actual, prediction = random_test()\nprint(\"Values for Initial Velocity, Acceleration, and Time: {}\".format(values))\nprint(\"Correct value: {}\".format(actual))\nprint(\"Prediction: {}\".format(round(prediction,1)))","17741efd":"import matplotlib.pyplot as plt\nimport math\nfor i in range(5):\n    # x, y\n    #basic physics trig to convert a vector to x and y coordinates, angle does not matter for comparisions\n    #but pi\/4 displays the difference great\n    #y = magnitude * sin(45)\n    #x = magnitude * sin(45)\n    values, actual, prediction = random_test()\n    actual_x_component = actual*np.cos(math.pi\/4)\n    actual_y_component = actual*np.sin(math.pi\/4)\n    prediction_x_component = round(prediction,2)*np.cos(math.pi\/4)\n    prediction_y_component = round(prediction,2)*np.sin(math.pi\/4)\n    plt.figure()\n    plt.quiver(actual_x_component, actual_y_component, color=\"red\", angles='xy', scale_units='xy', scale=1)\n    plt.quiver(prediction_x_component, prediction_y_component, color=\"y\", angles='xy', scale_units='xy', scale=1)\n    #set the bounds of the graph to include the vectors\n    lower_x = min(0,actual_x_component,prediction_x_component)\n    lower_y = min(0, actual_y_component, prediction_y_component)\n    plt.xlim(lower_x, max(actual_x_component,prediction_x_component)+10)\n    plt.ylim(lower_y, max(actual_y_component,prediction_y_component)+10)\n    #legend\n    plt.legend([\"Actual Final Velocity\",\"Predicted Final Velocity\"])\n    plt.title(\"Final Velocity Predicted: {} and Final Velocity Actual: {}\".format(round(prediction,1),actual))\n    ax = plt.gca()\n    leg = ax.get_legend()\n    leg.legendHandles[0].set_color('red')\n    leg.legendHandles[1].set_color('y')\n    #show grid\n    plt.grid(b=True, which='major')\nplt.show()","16f6c345":"### The neural network can closely predict the final velocity based on initial velocity, acceleration, and time! But what do these results actually mean? Let's look closer with a visualization.","eb2c61fd":"### Now let's create a tensor with the dimensions (30000,3) which contains initial velocity, acceleration, and time and have them be randomized. We can create random Tensors with our three integers(v_initial, acceleration, time) through the torch.randint method.","cb2b5b1a":"### Before we move our data into a dataloader format, we need to define how we are going to use CUDA to join our GPU compute with our data.","52305545":"### Hi! My name is Yanni Kouloumbis and I am a student at Troy High School in Fullerton, California. Today I wanted to simulate a physics based scenario using deep learning in order to bring coursework from my AP Physics 2 class to life. I am going to be using PyTorch to create the model and MatPlotLib to display the results in a vector-fashion.","21c5a78f":"### Now let's combine our inputs and outputs. After they are combined using PyTorch's TensorDataset, we will split the data into batches using PyTorch's DataLoader.","7aa72463":"### We are going to try to make a model that can figure out the physics equation v_final = v_initial+acceleration*time. Let's define a function that will solve the equation.","79df0673":"## Model, Data, Annotation, and Website by Yanni Kouloumbis\n#### Final Results are available at my [Physics AI Website.](https:\/\/physics-ai.netlify.app)","fbe2ea08":"### Conclusion: I used a 8 layer deep neural network to predict the final velocity based on the intial velocity, acceleration, and time of acceleration. I then checked my results and displayed accuracy and took notes of hyperparameters used. In order to clearly visualize the impact of the model, I plotted the final acceleration vectors and used an angle of pi\/4, although any angle could have been used with the same result as this model does not yet account for gravity. This model could be used to predict the final velocity of a moving object to a sufficent degree if its initial velocity, acceleration, and time are given and gravity is negated in some manner. Overall, this model was pretty cool to make and I like the idea of combining it with a subsequent network that can work with this model to make predictions on video input or statistical data(eg. predicting if objects are on a course for collision).\n","7b4b9a67":"### Now that our model has been trained, and it has a fairly high accuracy, let's visualize its improvements!","7040f72a":"### If you would like to see the final result programmed with an interface and running in the cloud, the final project is available for testing at my [Physics AI Website.](https:\/\/physics-ai.netlify.app)","60d384d8":"### Now let's define our logistic model(A feed forward neural network) and see what our first loss of the random model is.","c8777e58":"### Let's try out the model on a completely random piece of data, and see how close it can get.","53476a4e":"### As expected, the randomly initialized weights don't do so hot! Let's define a method to measure how well the model is doing aside from loss. We will count a result as accurate if the absolute value of the difference between the prediction and the actual value as being less than 2. This will give us a better idea of how the model is progressing.","5eced9b1":"### Now let's train our model. The Adam optimizer will work nicely and mean squared error as our loss function works well for this type of neural network as it is value based, not classification based. Our learning rate can be changed to enhance the model with 0.0005 working well for this model without over adjusting each time the optimizer alters the weights of the neural network."}}