{"cell_type":{"7666aec4":"code","930adf4c":"code","9b45b93d":"code","7683a98b":"code","60bb01f7":"code","121decb4":"code","f356d695":"code","a9c69d9a":"code","01534dbd":"code","7c1249cc":"code","ca8f69a1":"code","2e4bccd3":"code","e97a7606":"code","6761515e":"code","fd84ba17":"code","2cd4e1b1":"code","ce88b391":"code","c1ca69fe":"code","ba1f6293":"code","2c415059":"code","f713de32":"code","96591465":"code","b06374a9":"markdown","5e8f3726":"markdown","c9e4bcd2":"markdown","10904e87":"markdown","d2fca64e":"markdown","505a03bf":"markdown","7dd49022":"markdown","a210bfa2":"markdown","9759c08f":"markdown","2c97b44c":"markdown","fc86381b":"markdown","29f48ee2":"markdown","b62d1d25":"markdown","68bd9f9b":"markdown","dd5ebd61":"markdown","a92a6137":"markdown"},"source":{"7666aec4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","930adf4c":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import SGDClassifier as SGD\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.semi_supervised import SelfTrainingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import log_loss, f1_score","9b45b93d":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import metrics","7683a98b":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv')\ndf_train.head()","60bb01f7":"df_train.info()","121decb4":"print(\"Y unique: {}\".format(np.unique(df_train['target'])))\nfor i in range(50):\n    print(\"feature_{} unique: {}\".format(i, np.unique(df_train['feature_{}'.format(i)])))","f356d695":"mapping = {\n    'Class_1': 0,\n    'Class_2': 1,\n    'Class_3': 2,\n    'Class_4': 3,\n}\ndf_train['target'] = df_train['target'].map(mapping)\ndf_train['target']","a9c69d9a":"y_train = df_train['target']\nX_train = df_train.drop(['target', 'id'], axis=1)\nX_test = df_test.drop('id', axis=1)\ny_sparse_train = np.zeros(4 * y_train.shape[0]).reshape((-1, 4))\nfor y in y_train:\n    y_sparse_train[y] = 1.0","01534dbd":"%%time\nparams = {\n    'alpha': [0.00001, 0.0001, 0.01, 0.1],\n    'early_stopping': [False, True]\n}\n\nsgd_model = SGD(loss='log', shuffle=False)\nclf_SGD = GridSearchCV(sgd_model, params, scoring = 'neg_log_loss', cv=10)\nclf_SGD.fit(X_train, y_train)\nclf_SGD.best_params_","7c1249cc":"y_pred_SGD = clf_SGD.predict_proba(X_test)\ny_pred_SGD","ca8f69a1":"X_unlabeled = X_test\nX_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_train, y_train, test_size=0.90, stratify=y_train)\ny_train_c","2e4bccd3":"iterations = 0\ntrain_f1s = []\ntest_f1s = []\npseudo_labels = []\nhigh_prob = [1]\n\nwhile len(high_prob) > 0 and X_unlabeled.shape[0] > 0:\n    print(\"\u0418\u0442\u0435\u0440\u0430\u0446\u0438\u044f {}\".format(iterations))\n    # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u0438 \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\n    clf = DecisionTreeClassifier()\n    clf.fit(X_train_c, y_train_c)\n    y_hat_train = clf.predict(X_train_c)\n    y_hat_test = clf.predict(X_test_c)\n    \n    # \u0412\u044b\u0447\u0438\u0441\u043b\u044f\u0435\u043c f1_score\n    f1s_train = f1_score(y_train_c, y_hat_train, average=None)\n    f1s_test = f1_score(y_test_c, y_hat_test, average=None)\n    train_f1s.append(f1s_train)\n    test_f1s.append(f1s_test)\n    print('Train f1: {}'.format(f1s_train))\n    print('Test f1: {}'.format(f1s_test))\n    print(np.unique(y_hat_train))\n    print(np.unique(y_hat_test))\n    \n    # \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439\n    pred_probs = clf.predict_proba(X_unlabeled)\n    preds = clf.predict(X_unlabeled)\n    prob_0 = pred_probs[:, 0]\n    prob_1 = pred_probs[:, 1]\n    prob_2 = pred_probs[:, 2]\n    prob_3 = pred_probs[:, 3]\n    \n    # \u0425\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0435\u0439 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u043d\u0438\u0439 \u0432 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0435\n    df_pred_prob = pd.DataFrame([])\n    df_pred_prob['preds'] = preds\n    df_pred_prob['prob_0'] = prob_0\n    df_pred_prob['prob_1'] = prob_1\n    df_pred_prob['prob_2'] = prob_2\n    df_pred_prob['prob_3'] = prob_3\n    df_pred_prob.index = X_unlabeled.index\n    \n    # \u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u043c 99,5% \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e\n    high_prob = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.999],\n                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.999],\n                          df_pred_prob.loc[df_pred_prob['prob_2'] > 0.999],\n                          df_pred_prob.loc[df_pred_prob['prob_3'] > 0.999]],\n                          axis=0)\n    print(\"{} \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u0441 \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e\".format(len(high_prob)))\n    pseudo_labels.append(len(high_prob))\n    \n    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043f\u0441\u0435\u0432\u0434\u043e-\u043c\u0435\u0442\u043a\u0438 \u043a \u0434\u0430\u043d\u043d\u044b\u043c \u0432 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0435\n    X_train_c = pd.concat([X_train_c, X_unlabeled.loc[high_prob.index]], axis=0)\n    y_train_c = pd.concat([y_train_c, high_prob.preds])\n    \n    # \u0423\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u0441\u0435\u0432\u0434\u043e-\u043c\u0435\u0442\u043a\u0438 \u0438\u0437 \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\n    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n    print(\"\u041e\u0441\u0442\u0430\u043b\u043e\u0441\u044c {} \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u0435\u0437 \u043c\u0435\u0442\u043e\u043a\".format(len(X_unlabeled)))\n    \n    # \u0423\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u043c \u0441\u0447\u0435\u0442\u0447\u0438\u043a \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\n    iterations += 1\n    print()","e97a7606":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_STC = clf.predict_proba(X_test)\ny_pred_STC","6761515e":"%%time\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=X_train.shape[1], activation='softsign'))\nmodel.add(Dense(5, activation='softsign'))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\nmodel.fit(X_train, y_sparse_train)\n\nbest_last_error = np.inf\nbest_n_neurons = 1\nh_best = []\n\nfor i in range(1, 25, 3):\n    model = Sequential()\n    model.add(Dense(20, input_dim=X_train.shape[1], activation='softsign'))\n    model.add(Dense(i, activation='softsign'))\n    model.add(Dense(4, activation='softmax'))\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    print('Num neurons: {}'.format(i))\n    %time history = model.fit(X_train, y_sparse_train, epochs=50, verbose=0).history['loss']\n    print(history)\n    last_error = history[-1]\n    if best_last_error > last_error:\n        best_last_error = last_error\n        best_n_neurons = i","fd84ba17":"model = Sequential()\nmodel.add(Dense(20, input_dim=X_train.shape[1], activation='softsign'))\nmodel.add(Dense(best_n_neurons, activation='softsign'))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\nprint('Num neurons: {}'.format(best_n_neurons))\n%time history = model.fit(X_train, y_sparse_train, epochs=200, verbose=0).history['loss']\ny_pred_keras = model.predict(X_test)\ny_pred_keras","2cd4e1b1":"X_test.info()","ce88b391":"output_SGD = pd.DataFrame({'id': df_test.id, 'Class_1': y_pred_SGD[:, 0],'Class_2': y_pred_SGD[:, 1], 'Class_3': y_pred_SGD[:, 2], 'Class_4': y_pred_SGD[:, 3]})\noutput_SGD.to_csv('SGDClassifier.csv', index=False)\noutput_SGD.info()","c1ca69fe":"output_SGD.head()","ba1f6293":"output_STC = pd.DataFrame({'id': df_test.id, 'Class_1': y_pred_STC[:, 0],'Class_2': y_pred_STC[:, 1], 'Class_3': y_pred_STC[:, 2], 'Class_4': y_pred_STC[:, 3]})\noutput_STC.to_csv('SelfTrainingClassifier.csv', index=False)\noutput_STC.info()","2c415059":"output_STC.head()","f713de32":"output_nn = pd.DataFrame({'id': df_test.id, 'Class_1': y_pred_keras[:, 0],'Class_2': y_pred_keras[:, 1], 'Class_3': y_pred_keras[:, 2], 'Class_4': y_pred_keras[:, 3]})\noutput_nn.to_csv('keras.csv', index=False)\noutput_nn.info()","96591465":"output_nn.head()","b06374a9":"\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432. \u041f\u0440\u0435\u0434\u043f\u043e\u043b\u043e\u0436\u0438\u043c, \u0447\u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 $feature_k, k \\in [0, 49]$ \u043e\u0442\u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043f\u043e \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0430\u043d\u0438\u044e \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u0438, \u0442\u043e \u0435\u0441\u0442\u044c \u0435\u0441\u043b\u0438 $feature_k[i] > feature_k[j]$, \u0442\u043e $i$-\u044b\u0439 \u043e\u0431\u044a\u0435\u043a\u0442 \u0431\u043e\u043b\u0435\u0435 \u0437\u043d\u0430\u0447\u0438\u043c \u043f\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0443 $k$, \u0447\u0435\u043c $j$-\u044b\u0439 \u043e\u0431\u044a\u0435\u043a\u0442.","5e8f3726":"# [Keras](https:\/\/keras.io\/api\/)","c9e4bcd2":"# \u0412\u044b\u0432\u043e\u0434\n\n\u041b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0445 \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043b \u043b\u0438\u043d\u0435\u0439\u043d\u044b\u0439 SGD \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440. \u0414\u0435\u0440\u0435\u0432\u043e \u044f\u0432\u043d\u043e \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u043b\u043e\u0441\u044c, \u0430 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c \u043e\u0442\u0440\u0430\u0431\u043e\u0442\u0430\u043b\u0430 \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0445\u0443\u0436\u0435","10904e87":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435, \u043f\u0440\u0438 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e\u0441\u0442\u0438 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0438\u0445","d2fca64e":"\u0414\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0441 [\u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u044c\u044e](https:\/\/www.tensorflow.org\/) \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u0438\u043c\u043f\u043e\u0440\u0442\u044b","505a03bf":"\u0414\u043b\u044f \u043f\u043e\u0434\u0431\u043e\u0440\u0430 \u043c\u0435\u0442\u0440\u0438\u043a \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html).\n\nscoring = 'neg_log_loss' \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u043e [log_loss](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter). \u041f\u043e \u0443\u0441\u043b\u043e\u0432\u0438\u044e \u0437\u0430\u0434\u0430\u0447\u0438 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u0442\u0441\u044f Log loss \u043c\u0435\u0442\u0440\u0438\u043a\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0440\u0438\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0431\u0443\u0434\u0435\u043c \u0438\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u043d\u0435\u0435.\n\n\u0422\u0430\u043a \u043a\u0430\u043a \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e, \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u043b\u0430\u0442\u044c \u0447\u0438\u0441\u043b\u043e \u0444\u043e\u043b\u0434\u043e\u0432, \u0440\u0430\u0432\u043d\u044b\u043c 10.","7dd49022":"# \u0417\u0430\u043f\u0438\u0441\u044c \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0444\u0430\u0439\u043b","a210bfa2":"[\u0421\u0442\u0430\u0442\u044c\u044f](https:\/\/towardsdatascience.com\/a-gentle-introduction-to-self-training-and-semi-supervised-learning-ceee73178b38)","9759c08f":"\u041a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440 \u043e\u0441\u043d\u043e\u0432\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0438 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043c\u0435\u0442\u043e\u043a \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u043c \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0438 \u0438\u0445.\n\n\u0414\u043b\u044f \u0442\u043e\u0433\u043e \u0447\u0442\u043e\u0431\u044b \u043d\u0430\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c, \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u043c\u0435\u0442\u043a\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u043a\u0430\u043a \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u043d\u0430\u0431\u043e\u0440.","2c97b44c":"# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0435\u0439","fc86381b":"\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0443\u0447\u0438\u043b\u0430\u0441\u044c \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e - \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e, \u0434\u0430\u0436\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0438\u043b\u0430\u0441\u044c","29f48ee2":"# \u041f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","b62d1d25":"## [SGDClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)","68bd9f9b":"\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c, \u0447\u0442\u043e \u0434\u0435\u0440\u0435\u0432\u043e \u0440\u0435\u0448\u0435\u043d\u0438\u0439 \u043d\u0435 \u043a\u043e\u043b\u0435\u0431\u043b\u0435\u0442\u0441\u044f \u0432 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0438 \u043c\u0435\u0442\u043e\u043a \u043a\u043b\u0430\u0441\u0441\u043e\u0432 - \u043b\u0435\u0433\u0447\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0435\u0433\u043e.\n\n\u0417\u0430\u043c\u0435\u0442\u0438\u043c, \u0447\u0442\u043e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0432\u044b\u0441\u043e\u043a\u0430","dd5ebd61":"## [SelfTrainingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.semi_supervised.SelfTrainingClassifier.html)","a92a6137":"\u041a\u0430\u043a \u043c\u044b \u0432\u0438\u0434\u0438\u043c, \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f. \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0441\u0430\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f"}}