{"cell_type":{"6dc82088":"code","a6fb7697":"code","b143cfc6":"code","280d3d6e":"code","e28247a7":"code","921ca693":"code","3ae38461":"code","ae11a8c9":"code","dde89844":"code","31c24eec":"code","a8c78bf8":"code","166ff3c2":"code","7bdea99e":"code","ffce5fc1":"code","ddded1b0":"code","5138e384":"code","f8410c37":"code","fbb48e5c":"code","a34fe70a":"code","08010659":"code","5019efad":"code","753a7b52":"code","658910a3":"code","e60d4a09":"code","452c383d":"code","71dbc009":"code","c6727d1b":"code","fedd18dd":"code","70bb919a":"code","879916e4":"code","b4bde09c":"code","cf46174d":"code","a82e3a40":"code","c1f14661":"code","55d14f0e":"code","7903aa9b":"code","6341216c":"code","c95cf531":"code","ad85f24c":"code","2badad41":"code","feeb30fa":"code","e648f2d6":"code","d80f70bc":"code","3e207dde":"code","0e1af0d1":"code","90e3d15e":"code","d144aca8":"code","1ddc024c":"code","63ff5943":"code","9692b28b":"code","e267abde":"code","d252101c":"markdown","72700cd9":"markdown","7016d591":"markdown"},"source":{"6dc82088":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport seaborn as sns\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')","a6fb7697":"base_path = '..\/input\/ieee-minified-data\/'","b143cfc6":"!ls ..\/input\/","280d3d6e":"#train_data = transactions.merge(identity, on='TransactionID', how='left')\n#test_data = transactions_test.merge(identity_test, on='TransactionID', how='left')","e28247a7":"# data joined and minified using https:\/\/www.kaggle.com\/kernels\/svzip\/20704078\n\ntrain_data = pd.read_pickle(base_path + 'ieee_train_data.pkl')\ntest_data = pd.read_pickle(base_path + 'ieee_test_data.pkl')","921ca693":"def combine_string_cols(df, cols, inplace=False):\n    def process_row(row):\n        return '_'.join(map(lambda x: str(x).lower(), row.values))\n    if not inplace:\n        return df[cols].apply(process_row, axis=1)\n\ndef process_raw_data(data):\n    data['transaction_dow'] = (np.floor((data['TransactionDT'] \/ (3600 * 24)) - 1) % 7).astype(int)\n    data['transaction_hour'] = (np.floor((data['TransactionDT'] \/ 3600)) % 24).astype(int)\n    data['transaction_dom'] = np.floor((data['TransactionDT'] \/ (3600 * 24) - 1) % 30).astype(int)\n    ## card use count\n    data.rename(columns={'addr2': 'billing_country', 'addr1': 'billing_region'}, inplace=True)\n    country_counts = data['billing_country'].value_counts()\n    def get_country_bin(country_code):\n        if country_code == 87.0:\n            return 'US'\n        if country_code == 60.0:\n            return 'Canada'\n        if country_code == 96.0:\n            return 'Mexico'\n        if country_code == 'missing':\n            return 'missing'\n        if country_counts[int(country_code)] > 10:\n            return 'other'\n        else:\n            return 'miniscule'\n\n    data['billing_country'] = data['billing_country'].fillna('missing')\n    data['billing_country_bin'] = data['billing_country'].apply(get_country_bin)\n    data['billing_location'] = combine_string_cols(data, ['billing_country', 'billing_region'])\n    \n    #os_type\n    data.rename(columns={'id_30': 'os_type'}, inplace=True)\n    \n    data.loc[data['os_type'].str.contains('Mac', na=False), 'os_type_bin'] = 'mac'\n    data.loc[data['os_type'].str.contains('iOS', na=False), 'os_type_bin'] = 'iOS'\n    data.loc[data['os_type'].str.contains('Android', na=False), 'os_type_bin'] = 'android'\n    data.loc[data['os_type'].str.contains('Windows', na=False), 'os_type_bin'] = 'Windows'\n    data.loc[data['os_type'].str.contains('Linux', na=False), 'os_type_bin'] = 'Linux'\n    \n    #device name\n    data['device_name'] = data['DeviceInfo'].str.split('\/', expand=True)[0]\n\n    data.loc[data['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n    data.loc[data['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n    data.loc[data['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\n    data.loc[data['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n    data.loc[data['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\n    data.loc[data['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\n    data.loc[data['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\n    data.loc[data['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\n    data.loc[data['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\n    data.loc[data['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n    data.loc[data['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n    data.loc[data['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\n    data.loc[data['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\n    data.loc[data['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\n    data.loc[data['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\n    data.loc[data['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\n    data.loc[data['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n\n    data.loc[data.device_name.isin(data.device_name.value_counts()[data.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"\n    \n    #browser\n    data['browser'] = data['id_31'].str.replace('\\d+', '')\n        \n    #https:\/\/www.kaggle.com\/artgor\/eda-and-models#Feature-engineering\n    data['TransactionAmt_to_mean_card1'] = data['TransactionAmt'] \/ data.groupby(['card1'])['TransactionAmt'].transform('mean')\n    data['TransactionAmt_to_mean_card4'] = data['TransactionAmt'] \/ data.groupby(['card4'])['TransactionAmt'].transform('mean')\n    data['TransactionAmt_to_mean_card5'] = data['TransactionAmt'] \/ data.groupby(['card5'])['TransactionAmt'].transform('mean')\n    data['TransactionAmt_to_mean_billing_country'] = data['TransactionAmt'] \/ data.groupby(['billing_country'])['TransactionAmt'].transform('mean')\n    data['TransactionAmt_to_mean_id31'] = data['TransactionAmt'] \/ data.groupby(['id_31'])['TransactionAmt'].transform('mean')\n    data['card1_card2'] = data['card1'].astype(str) + '_' + data['card2'].astype(str)\n    data['billing_country_dist1'] = data['billing_country'].astype(str) + '_' + data['dist1'].astype(str)\n    data['card1_billing_country'] = data['card1'].astype(str) + '_' + data['billing_country'].astype(str)\n    data['card1_billing_region'] = data['card1'].astype(str) + '_' + data['billing_region'].astype(str)\n    data['card2_billing_country'] = data['card2'].astype(str) + '_' + data['billing_country'].astype(str)\n    data['card2_billing_region'] = data['card2'].astype(str) + '_' + data['billing_region'].astype(str)\n    data['card4_billing_country'] = data['card4'].astype(str) + '_' + data['billing_country'].astype(str)\n    data['card4_billing_region'] = data['card4'].astype(str) + '_' + data['billing_region'].astype(str)\n    data['DeviceInfo_P_emaildomain'] = data['DeviceInfo'].astype(str) + '_' + data['P_emaildomain'].astype(str)\n    data['P_emaildomain_billing_country'] = data['P_emaildomain'].astype(str) + '_' + data['billing_country'].astype(str)\n    data['id01_billing_country'] = data['id_01'].astype(str) + '_' + data['billing_country'].astype(str)\n    data['TransactionAmt_to_std_card1'] = data['TransactionAmt'] \/ data.groupby(['card1'])['TransactionAmt'].transform('std')\n    data['TransactionAmt_to_std_card4'] = data['TransactionAmt'] \/ data.groupby(['card4'])['TransactionAmt'].transform('std')\n    data['TransactionAmt_to_std_card5'] = data['TransactionAmt'] \/ data.groupby(['card5'])['TransactionAmt'].transform('std')\n    data['TransactionAmt_to_std_billing_country'] = data['TransactionAmt'] \/ data.groupby(['billing_country'])['TransactionAmt'].transform('std')\n    data['TransactionAmt_to_std_id31'] = data['TransactionAmt'] \/ data.groupby(['id_31'])['TransactionAmt'].transform('std')\n    data['TransactionAmt_decimal'] = ((data['TransactionAmt'] - data['TransactionAmt'].astype(int)) * 1000).astype(int)\n    \n    data['id_02_to_mean_card1'] = data['id_02'] \/ data.groupby(['card1'])['id_02'].transform('mean')\n    data['id_02_to_mean_card4'] = data['id_02'] \/ data.groupby(['card4'])['id_02'].transform('mean')\n    data['id_02_to_std_card1'] = data['id_02'] \/ data.groupby(['card1'])['id_02'].transform('std')\n    data['id_02_to_std_card4'] = data['id_02'] \/ data.groupby(['card4'])['id_02'].transform('std')\n\n    data['D15_to_mean_card1'] = data['D15'] \/ data.groupby(['card1'])['D15'].transform('mean')\n    data['D15_to_mean_card4'] = data['D15'] \/ data.groupby(['card4'])['D15'].transform('mean')\n    data['D15_to_std_card1'] = data['D15'] \/ data.groupby(['card1'])['D15'].transform('std')\n    data['D15_to_std_card4'] = data['D15'] \/ data.groupby(['card4'])['D15'].transform('std')\n\n    data['D15_to_mean_addr1'] = data['D15'] \/ data.groupby(['billing_region'])['D15'].transform('mean')\n    data['D15_to_mean_addr2'] = data['D15'] \/ data.groupby(['billing_country'])['D15'].transform('mean')\n    data['D15_to_std_addr1'] = data['D15'] \/ data.groupby(['billing_region'])['D15'].transform('std')\n    data['D15_to_std_addr2'] = data['D15'] \/ data.groupby(['billing_country'])['D15'].transform('std')\n    \n    data[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = data['P_emaildomain'].str.split('.', expand=True)\n    data[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = data['R_emaildomain'].str.split('.', expand=True)\n    \n    return data","3ae38461":"train = process_raw_data(train_data)\ntarget = train_data['isFraud']\ndel train_data","ae11a8c9":"train.drop('isFraud', axis=1, inplace=True)","dde89844":"test = process_raw_data(test_data)\ndel test_data","31c24eec":"print(train.shape)\nprint(test.shape)\nprint(target.shape)","a8c78bf8":"def get_null_columns(df, threshold=0.9):\n    return df.columns[(df.isnull().sum() \/ df.shape[0]) > 0.9]\n\ndef get_big_top_value_columns(df, threshold=0.9):\n    return [col for col in df.columns if \n                                df[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","166ff3c2":"train_null_columns = get_null_columns(train)\ntest_null_columns = get_null_columns(test)\ntrain_big_val_columns = get_big_top_value_columns(train)\ntest_big_val_columns = get_big_top_value_columns(test)","7bdea99e":"cols_to_drop = set(list(train_null_columns) + list(test_null_columns) + train_big_val_columns + test_big_val_columns)","ffce5fc1":"print(f'number of dropped columns {len(cols_to_drop)}')","ddded1b0":"train.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)","5138e384":"print(train.shape)\nprint(test.shape)","f8410c37":"inferred_category_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n'R_emaildomain', 'card1', 'card2', 'card3', 'card5', 'billing_region', 'billing_country', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9','P_emaildomain_1', 'P_emaildomain_2', \n'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3', 'os_type', 'billing_country_bin', 'billing_location',\n'os_type_bin', 'device_name','browser','card1_card2','billing_country_dist1','card1_billing_country','card1_billing_region',\n'card2_billing_country','card2_billing_region','card4_billing_country','card4_billing_region','DeviceInfo_P_emaildomain',\n'P_emaildomain_billing_country','id01_billing_country']\n\ndef get_high_correlation_cols(df, corrThresh=0.9):\n    numeric_cols = df._get_numeric_data().columns\n    corr_matrix = df.loc[:, numeric_cols].corr().abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    to_drop = [column for column in upper.columns if any(upper[column] > corrThresh)]\n    return to_drop\n\ndef replace_na(data, numeric_replace=-1, categorical_replace='missing', cat_features=[]):\n    numeric_cols = data._get_numeric_data().columns\n    categorical_cols = list(set(list(set(data.columns) - set(numeric_cols)) + cat_features))\n    categorical_cols = [col for col in categorical_cols if col in data.columns]\n    if numeric_replace is not None:\n        data[numeric_cols] = data[numeric_cols].fillna(numeric_replace)\n    data[categorical_cols] = data[categorical_cols].fillna(categorical_replace)\n    return data","fbb48e5c":"train_corr_cols = get_high_correlation_cols(train.drop(['TransactionID', 'TransactionDT'], axis=1))\n# test_corr_cols = get_high_correlation_cols(test.drop(['TransactionID', 'TransactionDT'], axis=1))","a34fe70a":"to_drop_high_corr = set(train_corr_cols)","08010659":"train.drop(to_drop_high_corr, axis=1, inplace=True)\ntest.drop(to_drop_high_corr, axis=1, inplace=True)","5019efad":"print(train.shape)\nprint(test.shape)","753a7b52":"train = replace_na(train, numeric_replace=None, cat_features=inferred_category_cols)\ntest = replace_na(test, numeric_replace=None, cat_features=inferred_category_cols)","658910a3":"from sklearn.preprocessing import LabelEncoder\n\nclass LabelEncoderExt(object):\n    def __init__(self):\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, data_list):\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n        return self\n\n    def transform(self, data_list):\n        unknown_index = ~(data_list.isin(self.classes_))\n        data_list[unknown_index] = 'Unknown'\n        return self.label_encoder.transform(data_list)\n\nclass DataFrameCategoryEncoder:\n    def __init__(self):\n        self.encoder_maps = {}\n        \n    def _get_cat_columns(self, df):\n        numeric_columns = df._get_numeric_data().columns\n        return [c for c in df.columns if c not in numeric_columns]\n    \n    def fit_transform(self, df, cat_columns=None):\n        df = df.copy()\n        if cat_columns is None:\n            cat_columns = self._get_cat_columns(df)\n            print(cat_columns)\n        for col in cat_columns:\n            _le = LabelEncoderExt().fit(df[col])\n            df[col] = _le.transform(df[col])\n            self.encoder_maps[col] = _le\n        return df, cat_columns\n            \n    def transform(self, df, cat_columns=None):\n        df = df.copy()\n        if cat_columns is None:\n            cat_columns = self._get_cat_columns(df)\n        for col in cat_columns:\n            _le = self.encoder_maps.get(col, None)\n            if _le is None:\n                raise ValueError(f'Column not encountered in training - {col}')\n            else:\n                df[col] = _le.transform(df[col])\n        return df","e60d4a09":"_ce = DataFrameCategoryEncoder()\ntrain_en, category_columns = _ce.fit_transform(train)\ntest_en = _ce.transform(test)","452c383d":"del _ce\ngc.collect()","71dbc009":"print(train_en.shape)\nprint(test_en.shape)","c6727d1b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","fedd18dd":"train_en = reduce_mem_usage(train_en)\ntest_en = reduce_mem_usage(test_en)","70bb919a":"from xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, train_test_split","879916e4":"import os\nimport zipfile\nfrom IPython.display import FileLink\n\ndef zipdir(path, ziph):\n    # ziph is zipfile handle\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file))\n\ndef eval_recorder(params, tep, teprob, trp, trprob, scores, fi, name, mode='local'):\n    scores = pd.DataFrame(scores, columns=['score'])\n    mean_score = round(scores['score'].mean(), 6)\n    std_score = round(scores['score'].std(), 4)\n    name_score = str(mean_score) + '_' + str(std_score)\n    if mode == 'local':\n        if not os.path.isdir('results\/' + name + '_' + name_score):\n            os.mkdir('results\/' + name + '_' + name_score)\n        base_path = 'results\/' + name + '_' + name_score + '\/'\n    if mode == 'kaggle':\n        os.chdir(r'\/kaggle\/working')\n        base_path = '\/kaggle\/working\/'\n        if not os.path.isdir(base_path + '\/' + name + '_' + name_score):\n            os.mkdir(name + '_' + name_score)\n        base_path = base_path + '\/' + name + '_' + name_score + '\/'\n    pd.Series(params).to_csv(base_path + 'params.csv', index=False)\n    pd.DataFrame(tep).to_csv(base_path + 'test_predictions.csv', index=False)\n    pd.DataFrame(teprob).to_csv(base_path + 'test_probablity.csv', index=False)\n    if trp is not None:\n        pd.DataFrame({'predition': trp, 'probablity': trprob}).to_csv(base_path + 'train_results.csv')\n    pd.DataFrame(fi).to_csv(base_path + 'feature_importances.csv', index=False)\n    if mode == 'kaggle':\n        zipf = zipfile.ZipFile(name + '_' + name_score + '.zip', 'w', zipfile.ZIP_DEFLATED)\n        zipdir(name + '_' + name_score +'\/', zipf)\n        zipf.close()\n        return name + '_' + name_score + '.zip'\n    \ndef split_eval(train, labels, x_val, y_val, test, clf, params, fit_params, name):\n    scores = []\n    feature_importances = np.zeros(len(train.columns))\n    test_predictions = np.zeros(test.shape[0])\n    test_probablity = np.zeros(test.shape[0])\n    \n    clf.fit(train, labels, eval_set=[(x_val, y_val)], **fit_params)\n    if 'catboost' in name:\n        scores.append(clf.best_score_['validation']['AUC'])\n    if 'xgboost' in name:\n        try:\n            scores.append(clf.best_score)\n        except:\n            scores.append({'valid_0': {'auc': clf.evals_result()['validation_0']['auc'][-1]}})\n    if 'lightgbm' in name:\n        scores.append(clf.best_score_)\n    test_predicts = clf.predict_proba(test)\n    test_predictions = test_predicts[:, 1]\n    test_probablity = test_predicts[:, 0]\n    feature_importances = clf.feature_importances_\n    print('-'*60)\n    if 'lightgbm' in name:\n        scores = [dict(s)['valid_0']['auc'] for s in scores]\n    del clf\n    filename = eval_recorder(params, test_predictions, test_probablity, None, None, scores, feature_importances, name, 'kaggle')\n    return test_predictions, test_probablity, None, None, scores, feature_importances, filename\n\ndef plot_feature_importances(fe, cols):\n    fe = pd.DataFrame(fe, index=cols)\n    if fe.shape[1] > 1:\n        fe = fe.apply(sum, axis=1)\n    else:\n        fe = fe[0]\n    fe.sort_values(ascending=False)[:20].plot(kind='bar')\n\ndef cv_eval(train, labels, test, clf, cv, params, fit_params, name):\n    scores = []\n    feature_importances = np.zeros((len(train.columns), cv.n_splits))\n    train_predictions = np.zeros(train.shape[0])\n    train_probablity = np.zeros(train.shape[0])\n    test_predictions = np.zeros((test.shape[0], cv.n_splits))\n    test_probablity = np.zeros((test.shape[0], cv.n_splits))\n    for i, (train_index, val_index) in enumerate(cv.split(train, labels)):\n        print(f'starting {i} split')\n        x_train = train.iloc[train_index]\n        y_train = labels[train_index]\n        x_val = train.iloc[val_index]\n        y_val = labels[val_index]\n        clf.fit(x_train, y_train, eval_set=[(x_val, y_val)], **fit_params)\n        if 'catboost' in name:\n            scores.append(clf.best_score_['validation']['AUC'])\n        if 'xgboost' in name:\n            try:\n                scores.append(clf.best_score)\n            except:\n                scores.append({'valid_0': {'auc': clf.evals_result()['validation_0']['auc'][-1]}})\n        if 'lightgbm' in name:\n            scores.append(clf.best_score_)\n        val_predictions = clf.predict_proba(x_val)\n        train_predictions[val_index] = val_predictions[:, 1]\n        train_probablity[val_index] = val_predictions[:, 0]\n        test_predicts = clf.predict_proba(test)\n        test_predictions[:, i] = test_predicts[:, 1]\n        test_probablity[:, i] = test_predicts[:, 0]\n        feature_importances[:, i] = clf.feature_importances_\n        print('-'*60)\n        del clf\n    filename = eval_recorder(params, test_predictions, test_probablity, train_predictions, train_probablity, scores, feature_importances, name, 'kaggle')\n    return test_predictions, test_probablity, train_predictions, train_probablity, scores, feature_importances, filename\n\ndef eval_catboost(train, labels, test, cv, params, cat_features, name, eval_set=None):\n    clf = CatBoostClassifier(**params)\n    fit_params = {\n        'cat_features': cat_features,\n        'plot':False\n    }\n    if cv is not None:\n        return cv_eval(train, labels, test, clf, cv, params, fit_params, 'catboost_' + name)\n    return split_eval(train, labels, eval_set[0], eval_set[1], test, clf, params, fit_params, 'catboost_' + name)\n\ndef eval_xgboost(train, labels, test, cv, params, name, eval_set=None):\n    clf = XGBClassifier(**params)\n    fit_params = {\n        'verbose':100, \n        'eval_metric':'auc',\n        'early_stopping_rounds': 300\n    }\n    if cv is not None:\n        return cv_eval(train, labels, test, clf, cv, params, fit_params, 'xgboost_' + name)\n    return split_eval(train, labels, eval_set[0], eval_set[1], test, clf, params, fit_params, 'xgboost_' + name)\n\ndef eval_lightgbm(train, labels, test, cv, params, cat_features, name, eval_set=None):\n    clf = LGBMClassifier(**params)\n    fit_params = {\n        'verbose': 100,\n        'eval_metric': 'auc',\n        #'categorical_feature':cat_features,        \n        'early_stopping_rounds': 300\n    }\n    if cv is not None:\n        return cv_eval(train, labels, test, clf, cv, params, fit_params, 'lightgbm_' + name)\n    return split_eval(train, labels, eval_set[0], eval_set[1], test, clf, params, fit_params, 'lightgbm_' + name)","b4bde09c":"catboost_params = {\n    'iterations': 5000,\n    'loss_function': 'Logloss',\n    'task_type': 'GPU',\n    'eval_metric': 'AUC',\n    'random_seed': 42,\n    'od_type': 'Iter',\n    'early_stopping_rounds': 300,\n    'learning_rate': 0.07,\n    'depth': 8,\n    'random_strength': 0.5,\n    'verbose': 100,\n    'metric_period': 50\n}\n\nxgb_params = {\n    'n_estimators': 5000,\n    'n_job': 6,\n    'max_depth': 8,\n    'learning_rate': 0.05,\n    'colsample_bytree': 0.5,\n    'tree_method': 'gpu_hist'\n}\n\nlgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.05,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'reg_lambda': 0.2\n}","cf46174d":"train = train.sort_values('TransactionDT')\ntest = test.sort_values('TransactionDT')\ntrain_en = train_en.sort_values('TransactionDT')\ntest_en = test_en.sort_values('TransactionDT')\ntarget = target.reindex(train.index)","a82e3a40":"train.drop(['TransactionDT', 'TransactionID'], axis=1, inplace=True)\ntest.drop(['TransactionDT', 'TransactionID'], axis=1, inplace=True)\ntrain_en.drop(['TransactionDT', 'TransactionID'], axis=1, inplace=True)\ntest_en.drop(['TransactionDT', 'TransactionID'], axis=1, inplace=True)","c1f14661":"# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\ntrain_x, val_x, train_y, val_y = train_test_split(train, target, shuffle=False)","55d14f0e":"test_predictions_cat, test_probablity_cal, train_predictions, train_probablity, scores, feature_importances, result_zip = eval_catboost(\n    train_x,\n    train_y,\n    test,\n    None,\n    catboost_params,\n    category_columns,\n    'catboost_tts',\n    (val_x, val_y)\n)","7903aa9b":"plot_feature_importances(feature_importances, train.columns)","6341216c":"del test_probablity_cal, train_predictions, train_probablity, scores, feature_importances","c95cf531":"gc.collect()","ad85f24c":"FileLink(result_zip)","2badad41":"del train\ndel test\ndel train_x\ndel train_y\ndel val_x\ndel val_y","feeb30fa":"gc.collect()","e648f2d6":"train_x_en, val_x_en, train_y_en, val_y_en = train_test_split(train_en, target, shuffle=False)","d80f70bc":"test_predictions_lgb, test_probablity_lgb, train_predictions, train_probablity, scores, feature_importances, result_zip = eval_lightgbm(\n    train_x_en,\n    train_y_en,\n    test_en,\n    None,\n    lgb_params,\n    category_columns,\n    'lightgbm_tts',\n    (val_x_en, val_y_en)\n)","3e207dde":"plot_feature_importances(feature_importances, train_x_en.columns)","0e1af0d1":"del test_probablity_lgb, train_predictions, train_probablity, scores, feature_importances","90e3d15e":"gc.collect()","d144aca8":"train_x_en = train_x_en.fillna(-1)\nval_x_en = val_x_en.fillna(-1)\ntest_en = test_en.fillna(-1)","1ddc024c":"test_predictions_xgb, test_probablity_xgb, train_predictions, train_probablity, scores, feature_importances, result_zip = eval_xgboost(\n    train_x_en,\n    train_y_en,\n    test_en,\n    None,\n    xgb_params,\n    'xgboost_tts',\n    (val_x_en, val_y_en)\n)","63ff5943":"plot_feature_importances(feature_importances, train_en.columns)","9692b28b":"def get_submission_df(predictions):\n    sample_submission = pd.read_csv('..\/input\/ieee-fraud-detection\/' +'sample_submission.csv')\n    print(sample_submission.shape)\n    sample_submission['isFraud'] = predictions\n    return sample_submission\n\ndef ensemble_predictions(preds):\n    ensemble_prediction = np.zeros(preds[0].shape)\n    for pred in preds:\n        ensemble_prediction += pred\n    ensemble_prediction \/= len(preds)\n    return get_submission_df(ensemble_prediction)\n\ndef ensemble_predictions_files(files):\n    ensemble_prediction = np.zeros(506691)\n    for f in files:\n        _s = pd.read_csv('.\/submissions\/'+f)\n        ensemble_prediction += _s['prediction']\n    ensemble_prediction \/= len(files)\n    return get_submission_df(ensemble_prediction)\n\ndef ensemble_predictions_files_gmean(files):\n    ensemble_prediction = np.ones(506691)\n    for f in files:\n        _s = pd.read_csv('.\/submissions\/'+f)\n        ensemble_prediction *= _s['prediction']\n    ensemble_prediction = np.power(ensemble_prediction, 1\/len(files))\n    return get_submission_df(ensemble_prediction)","e267abde":"ensemble_predictions([test_predictions_cat, test_predictions_lgb, test_predictions_xgb]).to_csv('submission.csv', index=False)","d252101c":"### Ensemble Predictions","72700cd9":"## CV Eval","7016d591":"### Encode categorical feature"}}