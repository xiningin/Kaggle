{"cell_type":{"2134c22c":"code","8b07922d":"code","5cbbb729":"code","2ef8fb79":"code","f1ba4a29":"code","6956de44":"code","e1183bf8":"code","22f0bc9f":"code","ac4ee78e":"code","55f75842":"code","293bf094":"code","46afb28b":"code","f045e310":"code","3926a34a":"code","f1d73abf":"code","3e04cdc8":"code","68d686f2":"code","de04ec3b":"code","85457f6c":"code","04400bf5":"code","93c6ca4c":"code","d232e680":"code","1d91b708":"code","56f01f81":"code","28ea5c9c":"code","194489f1":"code","dbd15868":"code","00db8ca7":"code","f0f126b2":"code","6f234614":"code","9007b9a5":"code","6d0fc64e":"code","7c6133e2":"code","76f0d6cb":"code","c1242304":"code","abf20e0c":"code","3725a726":"code","2b03a7ae":"code","dbd958af":"code","6f8f10b0":"code","5faf7f9b":"markdown","f6006eed":"markdown","01fa075f":"markdown","5908f226":"markdown","35697377":"markdown","a3d3ca8b":"markdown","7034e286":"markdown"},"source":{"2134c22c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b07922d":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd, numpy as np, seaborn as sns\nimport math, json, os, random\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nfrom sklearn.cluster import KMeans","5cbbb729":"def seed_everything(seed = 34):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything()","2ef8fb79":"#get comp data\ntrain = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_sub = pd.read_csv('..\/input\/stanford-covid-vaccine\/sample_submission.csv')","f1ba4a29":"def read_bpps_sum(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").sum(axis=1))\n    return bpps_arr\n\ndef read_bpps_max(df):\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps_arr.append(np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\").max(axis=1))\n    return bpps_arr\n\ndef read_bpps_nb(df):\n    #mean and std from https:\/\/www.kaggle.com\/symyksr\/openvaccine-deepergcn \n    bpps_nb_mean = 0.077522\n    bpps_nb_std = 0.08914\n    bpps_arr = []\n    for mol_id in df.id.to_list():\n        bpps = np.load(f\"..\/input\/stanford-covid-vaccine\/bpps\/{mol_id}.npy\")\n        bpps_nb = (bpps > 0).sum(axis=0) \/ bpps.shape[0]\n        bpps_nb = (bpps_nb - bpps_nb_mean) \/ bpps_nb_std\n        bpps_arr.append(bpps_nb)\n    return bpps_arr \n\ntrain['bpps_sum'] = read_bpps_sum(train)\ntest['bpps_sum'] = read_bpps_sum(test)\ntrain['bpps_max'] = read_bpps_max(train)\ntest['bpps_max'] = read_bpps_max(test)\ntrain['bpps_nb'] = read_bpps_nb(train)\ntest['bpps_nb'] = read_bpps_nb(test)\n\n#sanity check\ntrain.head()","6956de44":"from collections import Counter as count","e1183bf8":"pairs_rate = []\n\nfor j in range(len(train)):\n    res = dict(count(train.iloc[j]['structure']))\n    pairs_rate.append(res['('] \/ 53.5)\n    \npairs_rate = pd.DataFrame(pairs_rate, columns=['pairs_rate'])\npairs_rate","22f0bc9f":"loops = []\nfor j in range(len(train)):\n    counts = dict(count(train.iloc[j]['predicted_loop_type']))\n    available = ['E', 'S', 'H', 'B', 'X', 'I', 'M']\n    row = []\n    for item in available:\n        try:\n            row.append(counts[item] \/ train.iloc[j]['seq_length'])\n        except:\n            row.append(0)\n    loops.append(row)\n    \nloops = pd.DataFrame(loops, columns=available)\nloops","ac4ee78e":"bases_a = []\n\nfor j in range(len(train)):\n    counts = dict(count(train.iloc[j]['sequence']))\n    bases_a.append((\n        counts['A'] \/ 107,\n        counts['G'] \/ 107,\n        counts['C'] \/ 107,\n        counts['U'] \/ 107\n    ))\n    \nbases_a = pd.DataFrame(bases_a, columns=['A_percent', 'G_percent', 'C_percent', 'U_percent'])\nbases_a","55f75842":"train = pd.concat([train, pairs_rate, bases_a, loops], axis = 1)","293bf094":"pairs_rate_s = []\n\nfor j in range(len(test)):\n    res_s = dict(count(test.iloc[j]['structure']))\n    pairs_rate_s.append(res_s['('] \/ 53.5)\n    \npairs_rate_s = pd.DataFrame(pairs_rate_s, columns=['pairs_rate'])\npairs_rate_s","46afb28b":"loopa = []\nfor j in range(len(test)):\n    counts = dict(count(test.iloc[j]['predicted_loop_type']))\n    available = ['E', 'S', 'H', 'B', 'X', 'I', 'M']\n    row = []\n    for item in available:\n        try:\n            row.append(counts[item] \/ test.iloc[j]['seq_length'])\n        except:\n            row.append(0)\n    loopa.append(row)\n    \nloopa = pd.DataFrame(loopa, columns=available)\nloopa","f045e310":"bases = []\n\nfor j in range(len(test)):\n    counts = dict(count(test.iloc[j]['sequence']))\n    bases.append((\n        counts['A'] \/ test.seq_length,\n        counts['G'] \/ test.seq_length,\n        counts['C'] \/ test.seq_length,\n        counts['U'] \/ test.seq_length\n    ))\n    \nbases = pd.DataFrame(bases, columns=['A_percent', 'G_percent', 'C_percent', 'U_percent'])\nbases","3926a34a":"test = pd.concat([test, pairs_rate_s, bases, loopa], axis = 1)","f1d73abf":"train.shape, test.shape","3e04cdc8":"AUGMENT=True","68d686f2":"aug_df = pd.read_csv('..\/input\/augmented-data-for-stanford-covid-vaccine\/48k_augment.csv')\nprint(aug_df.shape)\naug_df.head()","de04ec3b":"def aug_data(df):\n    target_df = df.copy()\n    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n                         \n    del target_df['structure']\n    del target_df['predicted_loop_type']\n    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n\n    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n    df['log_gamma'] = 100\n    df['score'] = 1.0\n    df = df.append(new_df[df.columns])\n    return df","85457f6c":"print(f\"Samples in train before augmentation: {len(train)}\")\nprint(f\"Samples in test before augmentation: {len(test)}\")\n\nif AUGMENT:\n    train = aug_data(train)\n    test = aug_data(test)\n\nprint(f\"Samples in train after augmentation: {len(train)}\")\nprint(f\"Samples in test after augmentation: {len(test)}\")\n\nprint(f\"Unique sequences in train: {len(train['sequence'].unique())}\")\nprint(f\"Unique sequences in test: {len(test['sequence'].unique())}\")","04400bf5":"DENOISE = False","93c6ca4c":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","d232e680":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}","1d91b708":"def preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    base_fea = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    bpps_sum_fea = np.array(df['bpps_sum'].to_list())[:,:,np.newaxis]\n    bpps_max_fea = np.array(df['bpps_max'].to_list())[:,:,np.newaxis]\n    return np.concatenate([base_fea,bpps_sum_fea,bpps_max_fea], 2)","56f01f81":"if DENOISE:\n    train = train[train['signal_to_noise'] > .25]","28ea5c9c":"len(token2int)","194489f1":"# https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183211\ndef rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score","dbd15868":"def gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef build_model(rnn='gru', convolve=False, conv_dim=512, \n                dropout=.4, sp_dropout=.2, embed_dim=200,\n                hidden_dim=256, layers=3,\n                seq_len=107, pred_len=68):\n    \n###############################################\n#### Inputs\n###############################################\n\n    inputs = tf.keras.layers.Input(shape=(seq_len, 5))\n    categorical_feats = inputs[:, :, :3]\n    numerical_feats = inputs[:, :, 3:]\n\n    embed = tf.keras.layers.Embedding(input_dim=len(token2int),\n                                      output_dim=embed_dim)(categorical_feats)\n    reshaped = tf.reshape(\n        embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    reshaped = tf.keras.layers.concatenate([reshaped, numerical_feats], axis=2)\n    hidden = tf.keras.layers.SpatialDropout1D(sp_dropout)(reshaped)\n    \n    if convolve:\n        hidden = tf.keras.layers.Conv1D(conv_dim, 5, padding='same', activation=tf.keras.activations.swish)(hidden)\n\n###############################################\n#### RNN Layers\n###############################################\n\n    if rnn is 'gru':\n        for _ in range(layers):\n            hidden = gru_layer(hidden_dim, dropout)(hidden)\n        \n    elif rnn is 'lstm':\n        for _ in range(layers):\n            hidden = lstm_layer(hidden_dim, dropout)(hidden)\n\n###############################################\n#### Output\n###############################################\n\n    out = hidden[:, :pred_len]\n    out = tf.keras.layers.Dense(5, activation='linear')(out)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=out)\n    adam = tf.optimizers.Adam()\n    model.compile(optimizer=adam, loss=mcrmse)\n\n    return model","00db8ca7":"test_model = build_model(rnn='gru')\ntest_model.summary()","f0f126b2":"def train_and_infer(rnn, STRATIFY=True, FOLDS=4, EPOCHS=50, BATCH_SIZE=64,\n                    REPEATS=3, SEED=34, VERBOSE=2):\n\n    #get test now for OOF \n    public_df = test.query(\"seq_length == 107\").copy()\n    private_df = test.query(\"seq_length == 130\").copy()\n    private_preds = np.zeros((private_df.shape[0], 130, 5))\n    public_preds = np.zeros((public_df.shape[0], 107, 5))\n    public_inputs = preprocess_inputs(public_df)\n    private_inputs = preprocess_inputs(private_df)\n\n    #to evaluate TTA effects\/post processing\n    holdouts = []\n    holdout_preds = []\n    \n    #to view learning curves\n    histories = []\n    \n    #put similar RNA in the same fold\n    gkf = GroupKFold(n_splits=FOLDS)\n    kf=KFold(n_splits=FOLDS, random_state=SEED)\n    kmeans_model = KMeans(n_clusters=200, random_state=SEED).fit(preprocess_inputs(train)[:,:,0])\n    train['cluster_id'] = kmeans_model.labels_\n\n    for _ in range(REPEATS):\n        \n        for f, (train_index, val_index) in enumerate((gkf if STRATIFY else kf).split(train,\n                train['reactivity'], train['cluster_id'] if STRATIFY else None)):\n\n            #define training callbacks\n            lr_callback = tf.keras.callbacks.ReduceLROnPlateau(patience=8, \n                                                               factor=.1,\n                                                               #min_lr=1e-5,\n                                                               verbose=VERBOSE)\n            save = tf.keras.callbacks.ModelCheckpoint(f'model-{f}.h5')\n\n            #define sample weight function\n            epsilon = .1\n            sample_weighting = np.log1p(train.iloc[train_index]['signal_to_noise'] + epsilon) \/ 2\n\n            #get train data\n            trn = train.iloc[train_index]\n            trn_ = preprocess_inputs(trn)\n            trn_labs = np.array(trn[target_cols].values.tolist()).transpose((0, 2, 1))\n\n            #get validation data\n            val = train.iloc[val_index]\n            val_all = preprocess_inputs(val)\n            val = val[val.SN_filter == 1]\n            val_ = preprocess_inputs(val)\n            val_labs = np.array(val[target_cols].values.tolist()).transpose((0, 2, 1))\n\n            #pre-build models for different sequence lengths\n            model = build_model(rnn=rnn)\n            model_short = build_model(rnn=rnn,seq_len=107, pred_len=107)\n            model_long = build_model(rnn=rnn,seq_len=130, pred_len=130)\n\n            #train model\n            history = model.fit(\n                trn_, trn_labs,\n                validation_data = (val_, val_labs),\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                sample_weight=sample_weighting,\n                callbacks=[save, lr_callback],\n                verbose=VERBOSE\n            )\n\n            histories.append(history)\n\n            #load best models\n            model.load_weights(f'model-{f}.h5')\n            model_short.load_weights(f'model-{f}.h5')\n            model_long.load_weights(f'model-{f}.h5')\n\n            holdouts.append(train.iloc[val_index])\n            holdout_preds.append(model.predict(val_all))\n\n            public_preds += model_short.predict(public_inputs) \/ (FOLDS * REPEATS)\n            private_preds += model_long.predict(private_inputs) \/ (FOLDS * REPEATS)\n        \n        del model, model_short, model_long\n        \n    return holdouts, holdout_preds, public_df, public_preds, private_df, private_preds, histories","6f234614":"gru_holdouts, gru_holdout_preds, public_df, gru_public_preds, private_df, gru_private_preds, gru_histories = train_and_infer(rnn='gru')","9007b9a5":"def plot_learning_curves(results):\n\n    fig, ax = plt.subplots(1, len(results['histories']), figsize = (20, 10))\n    \n    for i, result in enumerate(results['histories']):\n        for history in result:\n            ax[i].plot(history.history['loss'], color='C0')\n            ax[i].plot(history.history['val_loss'], color='C1')\n            ax[i].set_title(f\"{results['models'][i]}\")\n            ax[i].set_ylabel('MCRMSE')\n            ax[i].set_xlabel('Epoch')\n            ax[i].legend(['train', 'validation'], loc = 'upper right')\n            \nresults = {\n            \"models\" : ['GRU'],    \n            \"histories\" : [gru_histories],\n            }","6d0fc64e":"#https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\ndef format_predictions(test_df, test_preds, val=False):\n    preds = []\n    \n    for df, preds_ in zip(test_df, test_preds):\n        for i, uid in enumerate(df['id']):\n            single_pred = preds_[i]\n\n            single_df = pd.DataFrame(single_pred, columns=target_cols)\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            if val: single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n\n            preds.append(single_df)\n    return pd.concat(preds).groupby('id_seqpos').mean().reset_index() if AUGMENT else pd.concat(preds)","7c6133e2":"def get_error(preds):\n    val = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines=True)\n\n    val_data = []\n    for mol_id in val['id'].unique():\n        sample_data = val.loc[val['id'] == mol_id]\n        sample_seq_length = sample_data.seq_length.values[0]\n        for i in range(68):\n            sample_dict = {\n                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n                           }\n            \n            val_data.append(sample_dict)\n            \n    val_data = pd.DataFrame(val_data)\n    val_data = val_data.merge(preds, on='id_seqpos')\n\n    rmses = []\n    mses = []\n    \n    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n        rmses.append(rmse)\n        mses.append(mse)\n        print(col, rmse, mse)\n    print(np.mean(rmses), np.mean(mses))\n    print('')","76f0d6cb":"plot_learning_curves(results)","c1242304":"gru_val_preds = format_predictions(gru_holdouts, gru_holdout_preds, val=True)\n\nprint('-'*25); print('Unfiltered training results'); print('-'*25)\nprint('GRU training results'); print('')\nget_error(gru_val_preds)\nprint('-'*25); print('SN_filter == 1 training results'); print('-'*25)\nprint('GRU training results'); print('')\nget_error(gru_val_preds[gru_val_preds['SN_filter'] == 1])\n","abf20e0c":"gru_preds = [gru_public_preds, gru_private_preds]\ntest_df = [public_df, private_df]\ngru_preds = format_predictions(test_df, gru_preds)\n","3725a726":"gru_weight = .5\n","2b03a7ae":"blended_preds = pd.DataFrame()\nblended_preds['id_seqpos'] = gru_preds['id_seqpos']\nblended_preds['reactivity'] = gru_weight*gru_preds['reactivity'] \nblended_preds['deg_Mg_pH10'] = gru_weight*gru_preds['deg_Mg_pH10'] \nblended_preds['deg_pH10'] = gru_weight*gru_preds['deg_pH10'] \nblended_preds['deg_Mg_50C'] = gru_weight*gru_preds['deg_Mg_50C'] \nblended_preds['deg_50C'] = gru_weight*gru_preds['deg_50C'] ","dbd958af":"submission = sample_sub[['id_seqpos']].merge(blended_preds, on=['id_seqpos'])\nsubmission.head()","6f8f10b0":"submission.to_csv(f'submission_new.csv', index=False)\nprint('Submission saved')","5faf7f9b":"\n\nfig, ax = plt.subplots(3, figsize=(15, 10))\nsns.kdeplot(np.array(train['bpps_max'].to_list()).reshape(-1),\n            color=\"Blue\", ax=ax[0], label='Train')\nsns.kdeplot(np.array(test[test['seq_length'] == 107]['bpps_max'].to_list()).reshape(-1),\n            color=\"Red\", ax=ax[0], label='Public test')\nsns.kdeplot(np.array(test[test['seq_length'] == 130]['bpps_max'].to_list()).reshape(-1),\n            color=\"Green\", ax=ax[0], label='Private test')\nsns.kdeplot(np.array(train['bpps_sum'].to_list()).reshape(-1),\n            color=\"Blue\", ax=ax[1], label='Train')\nsns.kdeplot(np.array(test[test['seq_length'] == 107]['bpps_sum'].to_list()).reshape(-1),\n            color=\"Red\", ax=ax[1], label='Public test')\nsns.kdeplot(np.array(test[test['seq_length'] == 130]['bpps_sum'].to_list()).reshape(-1),\n            color=\"Green\", ax=ax[1], label='Private test')\nsns.kdeplot(np.array(train['bpps_nb'].to_list()).reshape(-1),\n            color=\"Blue\", ax=ax[2], label='Train')\nsns.kdeplot(np.array(test[test['seq_length'] == 107]['bpps_nb'].to_list()).reshape(-1),\n            color=\"Red\", ax=ax[2], label='Public test')\nsns.kdeplot(np.array(test[test['seq_length'] == 130]['bpps_nb'].to_list()).reshape(-1),\n            color=\"Green\", ax=ax[2], label='Private test')\n\nax[0].set_title('Distribution of bpps_max')\nax[1].set_title('Distribution of bpps_sum')\nax[2].set_title('Distribution of bpps_nb')\nplt.tight_layout();\n\n","f6006eed":"# 2. SN_filter = 1 for data from the train and test due to the fact that the test data are all SN_filter = 1","01fa075f":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.kdeplot(train['signal_to_noise'], shade=True, ax=ax[0])\nsns.countplot(train['SN_filter'], ax=ax[1])\n\nax[0].set_title('Signal\/Noise Distribution')\nax[1].set_title('Signal\/Noise Filter Distribution');\n","5908f226":"# 3. theory : can we use the data from SN_filter != 1 ? ","35697377":"# 1. Notebook by [Tucker arrants ](https:\/\/www.kaggle.com\/tuckerarrants\/competitions)with biological features from structure, loops, pairs_rate as new features of the model\n","a3d3ca8b":"# 4. LSTM work less so I delete it and keep the Gru RNN","7034e286":"print(f\"Samples with signal_to_noise greater than 1: {len(train.loc[(train['signal_to_noise'] > 1 )])}\")\nprint(f\"Samples with SN_filter = 1: {len(train.loc[(train['SN_filter'] == 1 )])}\")\nprint(f\"Samples with signal_to_noise greater than 1, but SN_filter == 0: {len(train.loc[(train['signal_to_noise'] > 1) & (train['SN_filter'] == 0)])}\")"}}