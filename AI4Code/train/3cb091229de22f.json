{"cell_type":{"6e63302a":"code","6f7ee0f8":"code","c29c33b1":"code","1163267c":"code","f1c39056":"code","65b2203b":"code","f9809a31":"code","ce14ada8":"code","e7450182":"code","83763547":"code","c8372313":"code","d2b9094f":"code","f1740ac2":"code","b4540fd4":"code","90c0ad71":"code","dcbb4a03":"code","ac683f12":"code","ec5ddbfa":"code","5f19933d":"code","6e81cbb8":"code","f651dfc1":"code","11876de6":"code","b5825254":"code","8ac8a78b":"code","d296d41d":"code","d363aebb":"code","4aad708c":"code","faa74772":"code","e4360b0c":"code","459ea9e1":"code","a3dc1298":"code","d07aebd0":"code","d496da8f":"code","a9505520":"code","aa096978":"code","2b446eeb":"code","a84cb8b5":"code","3789589e":"code","82dacbd9":"code","d15a56e2":"code","c9015436":"code","b7264e9b":"code","f80250c4":"code","5bd56772":"code","3002e179":"code","b27e39b7":"code","a9cdcbc1":"code","e83b7a9b":"code","1580e7a6":"code","a6e25b97":"code","3cbc90d5":"markdown"},"source":{"6e63302a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6f7ee0f8":"# \u30e9\u30a4\u30d6\u30e9\u30ea\nimport numpy as np\nimport pandas as pd\nimport os\nimport json\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline","c29c33b1":"def read_data():\n    print(f'Read data')\n    train_df = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\n    test_df = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\n    train_labels_df = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\n    specs_df = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\n    sample_submission_df = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')\n    print(f\"train shape: {train_df.shape}\")\n    print(f\"test shape: {test_df.shape}\")\n    print(f\"train labels shape: {train_labels_df.shape}\")\n    print(f\"specs shape: {specs_df.shape}\")\n    print(f\"sample submission shape: {sample_submission_df.shape}\")\n    return train_df, test_df, train_labels_df, specs_df, sample_submission_df","1163267c":"train_df, test_df, train_labels_df, specs_df, sample_submission_df = read_data()","f1c39056":"train_df.head()","65b2203b":"test_df.head()","f9809a31":"train_labels_df.head()","ce14ada8":"specs_df.head()","e7450182":"sample_submission_df.head()","83763547":"print(f\"train installation id: {train_df.installation_id.nunique()}\")\nprint(f\"test installation id: {test_df.installation_id.nunique()}\")\nprint(f\"train label installation id: {train_labels_df.installation_id.nunique()}\")","c8372313":"# train data\u306e\u305d\u308c\u305e\u308c\u306e\u30ab\u30e9\u30e0\u306e\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u5408\u8a08\nfor column in train_df.columns.values:\n    print(f\"[train] Unique values of `{column}` : {train_df[column].nunique()}\")","d2b9094f":"# test data\u306e\u305d\u308c\u305e\u308c\u306e\u30ab\u30e9\u30e0\u306e\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u5408\u8a08\nfor column in test_df.columns.values:\n    print(f\"[test] Unique values of `{column}`: {test_df[column].nunique()}\")","f1740ac2":"# train data\u3068 test data\u306e\u305d\u308c\u305e\u308c\u306e\u7279\u5fb4\u91cf\u306e\u6563\u3089\u3070\u308a(\u4e0a\u4f4d20)\u3092\u30b0\u30e9\u30d5\u3067\u8868\u793a\u3059\u308b\u95a2\u6570\ndef plot_count(feature, title, df, size=1):\n    f, ax = plt.subplots(1,1, figsize=(4*size,4))\n    total = float(len(df))\n    g = sns.countplot(df[feature], order = df[feature].value_counts().index[:20], palette='Set3')\n    g.set_title(\"Number and percentage of {}\".format(title))\n    if(size > 2):\n        plt.xticks(rotation=90, size=8)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(100*height\/total),\n                ha=\"center\") \n    plt.show()  ","b4540fd4":"# title\u306etrain data\u306e\u7279\u5fb4\u91cf\u306e\u7a2e\u985e\u306e\u6563\u3089\u3070\u308a\nplot_count('title', 'title (first most frequent 20 values - train)', train_df, size=4)","90c0ad71":"# title\u306etest data\u306e\u7279\u5fb4\u91cf\u306e\u7a2e\u985e\u306e\u6563\u3089\u3070\u308a\nplot_count('title', 'title (first most frequent 20 values - test)', test_df, size=4)","dcbb4a03":"# type\u306etrain data\u306e\u7279\u5fb4\u91cf\u306e\u6563\u3089\u3070\u308a\nplot_count('type', 'type - train', train_df, size=2)","ac683f12":"# type\u306etest data\u306e\u7279\u5fb4\u91cf\u306e\u6563\u3089\u3070\u308a\nplot_count('type', 'type - test', test_df, size=2)","ec5ddbfa":"# train label data\u306e\u305d\u308c\u305e\u308c\u306e\u30ab\u30e9\u30e0\u306e\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u5408\u8a08\nfor column in train_labels_df.columns.values:\n    print(f\"[train_labels] Unique values of {column} : {train_labels_df[column].nunique()}\")","5f19933d":"# title\u306etrain label data\u306e\u7279\u5fb4\u91cf\u306e\u7a2e\u985e\u306e\u6563\u3089\u3070\u308a\nplot_count('title', 'title - train_labels', train_labels_df, size=3)","6e81cbb8":"train_df['title'].unique()","f651dfc1":"plot_count('accuracy_group', 'accuracy_group - train_labels', train_labels_df, size=2)","11876de6":"# spec data \u306e\u305d\u308c\u305e\u308c\u306e\u30ab\u30e9\u30e0\u306e\u30e6\u30cb\u30fc\u30af\u306a\u5024\u306e\u5408\u8a08\nfor column in specs_df.columns.values:\n    print(f\"[specs] Unique values of `{column}`: {specs_df[column].nunique()}\")","b5825254":"# Todo \n# event_data\u306e\u4e2d\u8eab\u3092\u5206\u6790\u3059\u308b\u3000\u2192 https:\/\/www.kaggle.com\/gpreda\/2019-data-science-bowl-eda\ntrain_df['event_data'].iloc[3]","8ac8a78b":"# Todo \n# spec data\u306eargs\u306e\u4e2d\u8eab\u3092\u5206\u6790\u3059\u308b","d296d41d":"# train data \u3068train label data \u306etimestamp\u306b\u95a2\u3059\u308b\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u3066\u3001\u52a0\u7b97\u3057\u305fdf\u3092\u4f5c\u308b\ndef extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    df['dayofyear'] = df['timestamp'].dt.dayofyear\n    df['quarter'] = df['timestamp'].dt.quarter\n    df['is_month_start'] = df['timestamp'].dt.is_month_start\n    print(f\"shape: {df.shape}\")\n    return df","d363aebb":"train_df = extract_time_features(train_df)\ntest_df = extract_time_features(test_df)","4aad708c":"train_df.head()","faa74772":"test_df.head()","e4360b0c":"# Todo\n# timestamp\u3092\u4f7f\u3063\u305f\u5206\u6790\u3092\u3059\u308b","459ea9e1":"# \u6570\u5024\u30c7\u30fc\u30bf\u3068\u5206\u985e\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\u305f\u3081\u306e\u30ab\u30e9\u30e0\nnumerical_columns = ['game_time', 'month', 'dayofweek', 'hour']\ncategorical_columns = ['type', 'world']\n\n# installation_id\u3092key\u3068\u3057\u305fdf\u3092\u4f5c\u308b\ncomp_train_df = pd.DataFrame({'installation_id': train_df['installation_id'].unique()})\ncomp_train_df.set_index('installation_id', inplace = True)","a3dc1298":"def get_numeric_columns(df, column):\n    df = df.groupby('installation_id').agg({f'{column}': ['mean', 'sum', 'min', 'max', 'std', 'skew']})\n    df[column].fillna(df[column].mean(), inplace = True)\n    df.columns = [f'{column}_mean', f'{column}_sum', f'{column}_min', f'{column}_max', f'{column}_std', f'{column}_skew']\n    return df","d07aebd0":"for i in numerical_columns:\n    comp_train_df = comp_train_df.merge(get_numeric_columns(train_df, i), left_index = True, right_index = True)","d496da8f":"print(f\"comp_train shape: {comp_train_df.shape}\")","a9505520":"pd.get_option(\"display.max_columns\")\npd.set_option('display.max_columns', 50)\ncomp_train_df.head()","aa096978":"# get the mode of the title\nlabels_map = dict(train_labels_df.groupby('title')['accuracy_group'].agg(lambda x:x.value_counts().index[0]))\n# merge target\nlabels = train_labels_df[['installation_id', 'title', 'accuracy_group']]\n# replace title with the mode\nlabels['title'] = labels['title'].map(labels_map)\n# join train with labels\ncomp_train_df = labels.merge(comp_train_df, on = 'installation_id', how = 'left')\nprint('We have {} training rows'.format(comp_train_df.shape[0]))","2b446eeb":"print(f\"comp_train shape: {comp_train_df.shape}\")","a84cb8b5":"comp_train_df.head()","3789589e":"print(f\"comp_train_df shape: {comp_train_df.shape}\")\nfor feature in comp_train_df.columns.values[3:20]:\n    print(f\"{feature} unique values: {comp_train_df[feature].nunique()}\")","82dacbd9":"plt.figure(figsize=(16,6))\n_accuracy_groups = comp_train_df.accuracy_group.unique()\nplt.title(\"Distribution of log(`game time mean`) values (grouped by accuracy group) in the comp train\")\nfor _accuracy_group in _accuracy_groups:\n    red_comp_train_df = comp_train_df.loc[comp_train_df.accuracy_group == _accuracy_group]\n    sns.distplot(np.log(red_comp_train_df['game_time_mean']), kde=True, label=f'accuracy group= {_accuracy_group}')\nplt.legend()\nplt.show()","d15a56e2":"# com_test_data \u3092\u751f\u6210\u3059\u308b\n# installation_id\u3092key\u3068\u3057\u305fdf\u3092\u4f5c\u308b\ncomp_test_df = pd.DataFrame({'installation_id': test_df['installation_id'].unique()})\ncomp_test_df.set_index('installation_id', inplace = True)\n\nfor i in numerical_columns:\n    comp_test_df = comp_test_df.merge(get_numeric_columns(test_df, i), left_index = True, right_index = True)","c9015436":"print(f\"comp_test shape: {comp_test_df.shape}\")\ncomp_test_df.head()","b7264e9b":"a = test_df\nb = a.drop_duplicates(subset='installation_id')\nb","f80250c4":"test_title = b[['installation_id','title']]\n\n\n\n# join train with labels\ncomp_test_df = test_title.merge(comp_test_df, on = 'installation_id', how = 'left')\nprint('We have {} testing rows'.format(comp_test_df.shape[0]))","5bd56772":"print(f\"comp_test shape: {comp_test_df.shape}\")\ncomp_test_df.head()","3002e179":"train = comp_train_df.drop(['installation_id', 'title'], axis=1)\ntrain_x = train.drop(['accuracy_group'], axis=1)\ntrain_y = train['accuracy_group']\ntest_x = comp_test_df.drop(['installation_id', 'title'], axis=1)","b27e39b7":"train_x.head()","a9cdcbc1":"test_x.head()","e83b7a9b":"# train_x\u306f\u5b66\u7fd2\u30c7\u30fc\u30bf\u3001train_y\u306f\u76ee\u7684\u5909\u6570\u3001test_x\u306f\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n# pandas\u306eDataFrame, Series\u3067\u4fdd\u6301\u3057\u307e\u3059\u3002\uff08numpy\u306earray\u3067\u4fdd\u6301\u3059\u308b\u3053\u3068\u3082\u3042\u308a\u307e\u3059\uff09\n\n# installation_id\u3092\u524a\u9664\u3059\u308b\n\ntrain = comp_train_df.drop(['installation_id', 'title'], axis=1)\ntrain_x = train.drop(['accuracy_group'], axis=1)\ntrain_y = train['accuracy_group']\ntest_x = comp_test_df.drop(['installation_id', 'title'], axis=1)\n\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306b\u5206\u3051\u308b\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=4, shuffle=True, random_state=71)\ntr_idx, va_idx = list(kf.split(train_x))[0]\ntr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\ntr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n\n# -----------------------------------\n# lightgbm\u306e\u5b9f\u88c5\n# -----------------------------------\nimport lightgbm as lgb\nfrom sklearn.metrics import log_loss\n\n# \u7279\u5fb4\u91cf\u3068\u76ee\u7684\u5909\u6570\u3092lightgbm\u306e\u30c7\u30fc\u30bf\u69cb\u9020\u306b\u5909\u63db\u3059\u308b\nlgb_train = lgb.Dataset(tr_x, tr_y)\nlgb_eval = lgb.Dataset(va_x, va_y)\n\n# \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u8a2d\u5b9a\nparams = {'objective': 'regression', 'seed': 71, 'verbose': 0}\nnum_round = 100\n\n# \u5b66\u7fd2\u306e\u5b9f\u884c\n# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u30d1\u30e9\u30e1\u30fc\u30bf\u3067\u6307\u5b9a\u3057\u3066\u3044\u308b\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3082\u30e2\u30c7\u30eb\u306b\u6e21\u3057\u3001\u5b66\u7fd2\u306e\u9032\u884c\u3068\u3068\u3082\u306b\u30b9\u30b3\u30a2\u304c\u3069\u3046\u5909\u308f\u308b\u304b\u30e2\u30cb\u30bf\u30ea\u30f3\u30b0\u3059\u308b\n#categorical_features = ['accuracy_group', 'medical_info_b2', 'medical_info_b3']\nmodel = lgb.train(params, lgb_train, num_boost_round=num_round,\n                  # categorical_feature=categorical_features,\n                  valid_names=['train', 'valid'], \n                  valid_sets=[lgb_train, lgb_eval])\n\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3067\u306e\u30b9\u30b3\u30a2\u306e\u78ba\u8a8d\n#va_pred = model.predict(va_x)\n#score = log_loss(va_y, va_pred)\n#print(f'logloss: {score:.4f}')\n\n# \u4e88\u6e2c\npr1 = model.predict(test_x)\n\n\npr1[pr1 <= 1.56] = 0\npr1[np.where(np.logical_and(pr1 > 1.56, pr1 <= 1.77))] = 1\npr1[np.where(np.logical_and(pr1 > 1.77, pr1 <= 2.025))] = 2\npr1[pr1 > 2.025] = 3\n\n\nprint(pr1)","1580e7a6":"sample_submission_df['accuracy_group'] = pr1.astype(int)\nsample_submission_df.to_csv('submission.csv', index=False)","a6e25b97":"sample_submission_df['accuracy_group'].value_counts(normalize=True)","3cbc90d5":"\u30b2\u30fc\u30e0\u6642\u9593\u306eplay\u6642\u9593\u306b\u5fdc\u3058\u3066\u3001accuracy group\u3092\u5206\u985e\u3067\u304d\u305d\u3046\uff01"}}