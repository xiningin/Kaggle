{"cell_type":{"5005c6de":"code","df722d85":"code","63ac3cdd":"code","d54637aa":"code","f115ed29":"code","6a46dd02":"code","2d796feb":"code","992b6da1":"code","0c2f48dc":"code","37039538":"code","3bd725b9":"code","34400680":"code","3ee06cb2":"code","52e38887":"code","9c771754":"code","4e4c6e95":"code","a0c2688a":"code","e83eaf14":"code","d4c4dce4":"code","f061596d":"code","f6f43f63":"code","fb5b2494":"code","7daa7e45":"code","afeb6c3d":"code","fe96a742":"code","c5b5ffaf":"code","540f2e54":"code","be177ba6":"code","b94f78cc":"code","dddec4e6":"code","5d2ab673":"code","c052a186":"code","0f640b7b":"code","c1096d16":"code","780304b1":"code","582227c3":"code","68a70ada":"code","d1e77c4e":"code","6f9f37e9":"code","36a5efe2":"code","996c7bc3":"code","32db364c":"code","b8bf307a":"code","eef2dcca":"code","2f08eac4":"code","8cf8225b":"code","2990bd36":"code","0b07821c":"code","44876971":"code","ea772c5e":"code","fd0aed96":"code","0abad2ec":"markdown","35361d31":"markdown","4c4a486d":"markdown","5ad79834":"markdown","bab682d3":"markdown","228dc5f9":"markdown","e3630b01":"markdown","aa99b1f8":"markdown","7f5fdbef":"markdown"},"source":{"5005c6de":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","df722d85":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pylab import rcParams\nimport pandas as pd\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","63ac3cdd":"# Exibir gr\u00e1ficos dentro do Jupyter Notebook\n%matplotlib inline\n\n# Definir tamanho padr\u00e3o para os gr\u00e1ficos\nrcParams['figure.figsize'] = 17, 4","d54637aa":"train_original = pd.read_csv('..\/input\/data-train-competicao-ml-1-titanic\/train.csv')\ntest_original = pd.read_csv('..\/input\/data-train-competicao-ml-1-titanic\/test.csv')","f115ed29":"# Detec\u00e7\u00e3o de outliers baseado no m\u00e9todo de Tukey\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Recebe um dataframe 'df' de features e retorna uma lista de \u00edndices\n    correspondentes que possuem mais de 'n' outliers de acordo com o m\u00e9todo\n    de Tukey.\n    \n    IMPORTANTE:descobri que deveria ter usado a fun\u00e7\u00e3o 'nanpercentile' para \n    desconsiderar os valores NAN, assim teria 3 outliers e n\u00e3o 2 como \u00e9 mostrado.\n    \"\"\"\n    outlier_indices = []\n    \n    # interagir em cada coluna (feature) passado\n    for col in features:\n        \n        # 1\u00ba quartil (25%)\n        Q1 = np.percentile(df[col], 25) #retorna o valor absoluto do n\u00famero que \u00e9 maior que 25% e menor que os 75% restante\n        print('Q1:{}'.format(Q1))\n        # 3\u00ba quartil (75%)\n        Q3 = np.percentile(df[col],75)  #retorna o valor absoluto do n\u00famero que \u00e9 maior que 75% e menor que os 25% restante\n        print('Q3:{}'.format(Q3))\n        \n        # calculando o interquartil (IQR) = grau de espalhamento dos dados\n        IQR = Q3 - Q1\n        print('IQR:{}'.format(IQR))\n        # aplicando um step no IQR para representar o m\u00e1ximo de diferen\u00e7a que aceitaremos\n        outlier_step = 1.5 * IQR\n        \n        # Detecta um outlier que est\u00e1 acima do limiar superior Q3 + (1.5 * IQR)\n        outlier_list_col = df[(df[col] > Q3 + outlier_step )].index\n        \n        # guarda o \u00edndice do outlier no final da lista\n        outlier_indices.extend(outlier_list_col)\n        \n    #Conta quantas vezes aquele \u00edndice foi um outlier\n    outlier_indices = Counter(outlier_indices)\n    \n    #percorre o outlier_indices (\u00edndice=k,quantas vezes ele aparece=v) e se o n\u00famero de vezes for maior que o n ent\u00e3o considera um ourlier e guarda esse \u00edndice\n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# passao dataset somente de treino, n=2 e as colunas (features)\nOutliers_to_drop = detect_outliers(train_original,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n\n#localiza esses \u00edndices no dataset para printar na tela\ntrain_original.loc[Outliers_to_drop]","6a46dd02":"train_original = train_original.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)   #retiro os outrliers\ntrain_len = len(train_original)                                                           #guardo o tamanho o treino para separar o dataset no final da mesma forma\ndataset =  pd.concat(objs=[train_original, test_original], axis=0).reset_index(drop=True) #junto o treino com o teste para ter somente um dataset e facilitar na cria\u00e7\u00e3o de novas features\ndataset = dataset.fillna(np.nan)                                                          #preencho os valores de Na e NaN com np.nan = NaN","2d796feb":"print(dataset.head())","992b6da1":"print(dataset.dtypes)","0c2f48dc":"# Somente atributos num\u00e9ricos s\u00e3o considerados\nprint(dataset.describe())","37039538":"# Quantidade absoluta\nprint(dataset.isnull().sum())","3bd725b9":"dataset[\"Fare\"].hist()","34400680":"#fa\u00e7o a normaliza\u00e7\u00e3o dos dados(0 e 1) j\u00e1 que existe valores muito diferentes e pode interferir no treinamento\ndataset[\"Fare\"] = (dataset[\"Fare\"]-min(dataset[\"Fare\"]))\/(max(dataset[\"Fare\"])-min(dataset[\"Fare\"]))","3ee06cb2":"dataset[\"Fare\"].hist()","52e38887":"dataset[\"Embarked\"].describe()","9c771754":"#preencho a featura que possui 2 valores NaN com o que mais aparece no dataset\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","4e4c6e95":"#substituo os valores de homem = 0 e mulher = 1\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","a0c2688a":"# Somente atributos num\u00e9ricos s\u00e3o considerados\nplt.suptitle(\"Gr\u00e1fico de Calor das Correla\u00e7\u00f5es entre os Atributos Num\u00e9ricos\")\nsns.heatmap(dataset.drop(labels =[\"PassengerId\",\"Survived\"], axis = 1).corr(), annot=True, cmap='Blues')","e83eaf14":"g = sns.catplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\")","d4c4dce4":"index_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index) #pega todos os \u00edndices que idade \u00e9 igual a NaN\n\nfor i in index_NaN_age :\n    #faz a m\u00e9dia das idades do dataset todo\n    age_med = dataset[\"Age\"].median()\n    #faz a m\u00e9dia da idade em que os features SibSp, Parch e Pclass tem os mesmos valores que o \u00edndice do valor NaN no feature AGE\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    #verifica se age_pred \u00e9 NaN, ou seja, caso tenha encontrado outros \u00edndices e feito a m\u00e9dia, atribui esse valor no dataset. Caso contr\u00e1rio atribui a m\u00e9dia do dataset original\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","f061596d":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]] #retira somente os t\u00edtulos dos nomes para criar uma feature\ndataset[\"Title\"] = pd.Series(dataset_title)                                      #insere essa nova featura no dataset\ndataset[\"Title\"].head()","f6f43f63":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45)","fb5b2494":"Counter(dataset[\"Title\"])","7daa7e45":"#os t\u00edtulos menos comuns (usados na sociedade e n\u00e3o no dataset) vou chamar de Rare\ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n#os t\u00edtulos mais comuns vamos separar por homem e mulher, sendo de homem separados o master de mr pois tem significados diferentes e podem influenciar\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)\nCounter(dataset[\"Title\"])","afeb6c3d":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45)","fe96a742":"dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","c5b5ffaf":"#criamos uma feature chamada de Fsize que representa o tamanho da fam\u00edlia no navio, ou seja num de (irm\u00e3os + esposas) + (pais + filhos) + a pessoa\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","540f2e54":"#quebrar o feature Fsize para obtermos outros features baseados no tamanho delas\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","be177ba6":"#gera o dummies dos t\u00edtulos e se embarcou para transformar em vari\u00e1veis com valores num\u00e9ricas\ndataset = pd.get_dummies(dataset, columns = [\"Title\"])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")","b94f78cc":"dataset.head()","dddec4e6":"print(dataset[\"Cabin\"].describe())","5d2ab673":"#cria feature baseado se a pessoa tem cabine ou n\u00e3o. verifica que se for do tipo NaN (que \u00e9 considerado um float) recebe 0 (n\u00e3o tem cabine) e se for diferente de NaN tem cabine e recebe 1\ndataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","c052a186":"dataset['Has_Cabin'].head()","0f640b7b":"#substitui o nome da cabine pela primeira letra e se for nulo coloca X no lugar\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","c1096d16":"g = sns.catplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")","780304b1":"dataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","582227c3":"#percorre o feature Ticket, se n\u00e3o for d\u00edgito retira . \/ e pega o prefixo. Caso seja um d\u00edgito substitui por X e aloca em Ticket\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0])\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket #substitui pela lista Ticket criada\ndataset[\"Ticket\"].head()","68a70ada":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","d1e77c4e":"dataset.head()","6f9f37e9":"dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\") #transforma os n\u00fameros 0,1,2 e em catergorias para gerar o dummies\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","36a5efe2":"dataset.head()","996c7bc3":"print(dataset.isnull().sum())","32db364c":"#separando o dataset em treino e teste\ntrain_original = dataset[:train_len]\ntest_original = dataset[train_len:]\ntest_original.drop(labels=[\"Survived\"],axis = 1,inplace=True)","b8bf307a":"skf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\nX = train_original.drop('Survived', axis=1).values\ny = train_original['Survived'].values\nscore_val =[]\nscore_test = []\n\ndef treinamento(classificador):\n    for train_index, val_index in skf.split(X,y): #treino recebe 4 partes e o teste em 1 parte (que sempre ser\u00e1 distinta) e intera no for 5 vezes \n        \n        X_train,X_val = X[train_index],X[val_index]    #recebe a parte que treinar\u00e1 o modelo e a parte que testar\u00e1 o modelo treinado (valida\u00e7\u00e3o)\n        y_train,y_val = y[train_index],y[val_index]    #recebe a as respostas correspondetes ao treino e a as respostas correspondetes ao teste (valida\u00e7\u00e3o)\n\n        model = classificador                          #recebe o algoritmo machine learn \n        model.fit(X_train, y_train)                    #treina o modelo\n        \n        y_test = model.predict_proba(X_val)            #recebe a predi\u00e7\u00e3o do teste para comparar com o y_val = respostas da parte de teste\n        y_test = y_test[:,1]                           #pega somente a coluna que contem as % de ter sobrevivido ou n\u00e3o\n        RAC = roc_auc_score(y_val, y_test)             #calcula a \u00e1rea sobre a curva ROC\n        score_val.append(RAC)                          #guarda o valor obtido para fazer a m\u00e9dia no final\n        print(\"Teste = {}\".format(RAC))\n        \n        #uso da parte de teste do dataset original para gerar a predi\u00e7\u00e3o que ser\u00e1 enviada no site\n        y_pred = model.predict_proba(test_original)    #roda o modelo em cima do dataset teste original\n        y_pred = y_pred[:, 1]                          #pega toda as linhas da coluna de 'Survived'\n        score_test.append(y_pred)                      #guarda o valor obtido da predi\u00e7\u00e3o para submeter na competi\u00e7\u00e3o\n\n    print(\"M\u00e9dia do teste = {}\".format(np.mean(score_val)))\n    \n    return score_test","eef2dcca":"\"\"\"\n\u00c9 feito a m\u00e9dia do resultuado (merge) para que eu consiga percorrer 100% no dataset,\nj\u00e1 que eu dividio o meu dataset em 5 partes diferentes, treinando com cada uma delas.\nAl\u00e9m disso, o modelo variar\u00e1 menos do que enviar somente o melhor modelo dos 5 gerados.\n\"\"\"\nfinal_pred_RD = np.mean(treinamento(RandomForestClassifier(random_state=2020)), axis=0) ","2f08eac4":"final_pred_LR = np.mean(treinamento(LogisticRegression(random_state=2020)), axis=0)","8cf8225b":"pegarID = pd.read_csv('..\/input\/data-train-competicao-ml-1-titanic\/test.csv')\nidentificador = pegarID['PassengerId']\nresultado_RD = pd.concat([identificador, pd.DataFrame(final_pred_RD, columns=['Survived'])], axis=1)\nresultado_RD.head()","2990bd36":"resultado_LR = pd.concat([identificador, pd.DataFrame(final_pred_LR, columns=['Survived'])], axis=1)\nresultado_LR.head()","0b07821c":"resultado_ensemble = resultado_LR.copy()\nresultado_ensemble['Survived'] = (resultado_RD['Survived'] +resultado_LR['Survived'])\/2\nresultado_ensemble.head()","44876971":"def posprocess(x):\n    if x<=0.15:\n        return 0\n    elif x>=0.9:\n        return 1\n    else:\n        return x","ea772c5e":"resultado_ensemble['Survived'] = resultado_ensemble['Survived'].apply(posprocess)\nresultado_ensemble.head()","fd0aed96":"resultado_ensemble.to_csv('submission_esem_pos.csv', index=False)","0abad2ec":"### Ensemble\n\n###### A ideia geral do ensemble \u00e9 juntar 2 ou mais modelos e transformar em um \u00fanico. O que eu fa\u00e7o no meu c\u00f3digo \u00e9 pegar 2 modelos diferentes, no caso usando Random Forest e Logistic Regression, e que me deram valida\u00e7\u00f5es pr\u00f3ximas.  Assim, esses modelos percorreram caminhos diferentes mas obtiveram resultados pr\u00f3ximas, ent\u00e3o se eu juntas esses resultados posso obter uma melhor predi\u00e7\u00e3o. \n","35361d31":"### P\u00f3s processamento\n\n###### A ideia do p\u00f3s processamento \u00e9 retirar os erros de casos que tem grande chance de serem 0 ou 1, ou seja, estar vivo ou morto. Vamos supor que um resultado deu 10% de chance de estar vivo, logo esse resultado tem grandes chances de ser na verdade 0. Assim, teria um erro de 0.1 na minha predi\u00e7\u00e3o e que poderia ser retirado e colocada como 0.","4c4a486d":"## Pr\u00e9-Processamento","5ad79834":"### Carregando o dataset de treino","bab682d3":"### Desafio Titanic\n#### Autor: Luan Mota Barbosa Noleto - luanmota36@hotmail.com\n\n### Refer\u00eancias\n##### https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n##### http:\/\/www.lapmec.com.br\/upload\/mod_publicacoes\/251\/5b3e66947f420.pdf\n##### https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html\n##### https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\n##### https:\/\/github.com\/wandersondsm\/Competicao-ML-1---Data-Train\n##### https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n##### https:\/\/stats.stackexchange.com\/questions\/52274\/how-to-choose-a-predictive-model-after-k-fold-cross-validation\n##### https:\/\/machinelearningmastery.com\/out-of-fold-predictions-in-machine-learning\/\n","228dc5f9":"### Treinamento e prei\u00e7\u00e3o do modelo","e3630b01":"### Configura\u00e7\u00e3o de par\u00e2metros","aa99b1f8":"### An\u00e1lise explorat\u00f3ria para conhecer o dataset","7f5fdbef":"### Todos os imports necess\u00e1rios para o projeto"}}