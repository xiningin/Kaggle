{"cell_type":{"d1a69ec0":"code","a67a8412":"code","9416f5ff":"code","936ea1d5":"code","d934f46b":"code","359322ab":"code","8c08c1d2":"code","8862645a":"code","4705547a":"code","33d4d199":"code","73755c8a":"code","9346c309":"code","d3a4ddca":"code","dbf6e944":"code","38c8f49d":"code","9c6e9cb1":"code","a5d347a7":"code","ec9ebb16":"code","f760ad87":"code","f8d342af":"code","387e41b4":"code","94c5e180":"code","7239f8a7":"code","a1113248":"code","c712e52b":"code","f6db6dcc":"code","35282bef":"code","831e7b29":"code","5232a277":"code","f1b20aca":"code","328d2c46":"code","d83f5a97":"code","1d5b8ab7":"code","9ffc4427":"markdown","7ca2568b":"markdown","e1b5d74d":"markdown","d9942289":"markdown","6201eb22":"markdown","b9632f60":"markdown","7381bf54":"markdown","afb6a156":"markdown","b7104cb6":"markdown","9ca0fda5":"markdown","eac7ed0e":"markdown","676e918a":"markdown","3f57081f":"markdown","cbb22372":"markdown","da5dcb8d":"markdown","afbc2ccd":"markdown","19b5bc46":"markdown"},"source":{"d1a69ec0":"import pandas as pd\ndata = pd.read_csv('..\/input\/data.csv')\ndata.head()","a67a8412":"print('Number of rows and columns:',data.shape)","9416f5ff":"print('Number of user:', len(data.user_id.unique()))\nprint('Number of item:', len(data.item_id.unique()))\nprint('Number of category:', len(data.category_id.unique()))\nprint('Number of cusine:', len(data.cusine_id.unique()))\nprint('Number of restaurant:', len(data.restaurant_id.unique()))","936ea1d5":"print('Number of orders on different days')\ndata.dow.value_counts()","d934f46b":"print('Number of items sold on different days:')\nx_d =pd.pivot_table(data, values = 'item_count', index = 'dow', aggfunc = sum)\nx_d.sort_values('item_count', ascending = False)","359322ab":"print('Number of orders on different hours of the day:')\ndata.hod.value_counts()","8c08c1d2":"print('Number of items sold in different hours of the day:')\nx = pd.pivot_table(data, values = 'item_count', index = 'hod', aggfunc = sum)\nx.sort_values('item_count', ascending = False)","8862645a":"import matplotlib.pyplot as plt\nimport seaborn as sns\ndata.dow = data.dow.astype('str')\norder = data.dow.value_counts().index\ncolor = sns.color_palette()[9]\nn_dow = data.dow.shape[0]\ndow_counts = data.dow.value_counts()\nfig = plt.figure(figsize = (20,6))\nplt.subplot(1,2,1)\nsns.countplot(data = data, y = 'dow', order = order, color = color)\nfor i in range(dow_counts.shape[0]):\n    count = dow_counts[i]\n    string = '{:0.1f}%'.format(100*count\/n_dow)\n    plt.text(count+1, i, string)\n    plt.xlabel('Proportion')\n    plt.title('Order (%) on different days of week')\n    \ndata.hod = data.hod.astype('str')\norder_hod = data.hod.value_counts().index\nn_hod = data.hod.shape[0]\nhod_counts = data.hod.value_counts()\nplt.subplot(1,2,2)\nsns.countplot(data = data, y = 'hod', order = order_hod, color = color)\nfor i in range(hod_counts.shape[0]):\n    count_h = hod_counts[i]\n    string_h = '{:.1f}%'.format(100*count_h\/n_hod)\n    plt.text(count_h+i, i, string_h)\n    plt.xlabel('Proportion')\n    plt.title('Order (%) on different hours of the day')","4705547a":"a = data['dow'].astype('str')\nb = data['hod'].astype('str')\nH_D = a + b\ndata.insert(4, 'H_D', H_D)\ndata.head()","33d4d199":"pd.set_option('max_r', 15)\ndata.H_D.value_counts()","73755c8a":"order_H = data.H_D.value_counts().index\nn_H = data.H_D.shape[0]\nH_counts = data.H_D.value_counts()\nfig = plt.figure(figsize = (10,100))\nsns.countplot(data = data, y = 'H_D', order = order_H, color = color)\nfor i in range(H_counts.shape[0]):\n    count_H = H_counts[i]\n    string_H = '{:.1f}%'.format(100*count_H\/n_H)\n    plt.text(count_H+i, i, string_H)\n    plt.xlabel('Proportion')\n    plt.title('Order (%) of different hours of different days')","9346c309":"pd.pivot_table(data, values = 'item_count', index = 'hod', columns = 'dow', aggfunc = sum)","d3a4ddca":"print('Number of missing values:')\ndata.isnull().sum()","dbf6e944":"print('item_id dtype:',data.item_id.dtype)","38c8f49d":"data['userId'] = data['user_id'].astype('category').cat.codes\ndata['itemId'] = data['item_id'].astype('category').cat.codes","9c6e9cb1":"data.head()","a5d347a7":"from sklearn.model_selection import train_test_split\ntrain, cros_val = train_test_split(data, test_size = 0.2, random_state = 1)","ec9ebb16":"train, test = train_test_split(train, test_size = 0.25, random_state = 1)","f760ad87":"print('Splitted dataset into train set, cross validation set and test set')\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)\nprint('cros_val shape:',cros_val.shape)","f8d342af":"import scipy.sparse as sparse\nuser_items = sparse.csr_matrix((train['item_count'].astype(float),(train['userId'], train['itemId'])))\nitem_users = sparse.csr_matrix((train['item_count'].astype(float),(train['itemId'], train['userId'])))","387e41b4":"print(item_users)","94c5e180":"import os\nos.environ['MKL_NUM_THREADS'] = '1' #To avoid multithreading.\nos.environ['OPENBLAS_NUM_THREADS'] = '1'\nimport implicit\nmodel = implicit.als.AlternatingLeastSquares(factors = 500, iterations = 10)\n''''Parameters: (factors=100, regularization=0.01, dtype=<type 'numpy.float32'>, use_native=True, use_cg=True, \nuse_gpu=False, iterations=15, calculate_training_loss=False, num_threads=0)''';","7239f8a7":"alpha = 40\ntrain_conf = (item_users*alpha).astype('double')","a1113248":"model.fit(train_conf)","c712e52b":"import csv\nfields = 'userId', 'item_list'\nfilename = 'rec_train.csv'\nwith open (filename, 'a', newline = '') as f:\n    writer = csv.writer(f)\n    writer.writerow(fields)\n    userId = train['userId'].values.tolist()\n    for user in userId:\n        scores = []\n        items =[]\n        results = []\n        results.append(user)\n        recommendations = model.recommend(user, user_items, N = 5)\n        for item in recommendations:\n            ids, score = item\n            scores.append(score)\n            items.append(ids)\n        results.append(items)\n        writer.writerow(results)","f6db6dcc":"predicted = pd.read_csv('rec_train.csv')\npredicted = predicted['item_list']\nimport ast\npredicted = [ast.literal_eval(a) for a in predicted]\nactual = train['itemId']\nimport numpy as np\nactual = np.array(actual).reshape(193882,1)\nimport ml_metrics\nscore = ml_metrics.mapk(actual, predicted, 5)\nprint('Mean avg. precision at k for train set:','{:.8f}'.format(score))","35282bef":"import csv\nfields = 'user_id', 'item_list'\nfilename = 'rec_cros.csv'\nwith open(filename, 'a', newline = '') as f:\n    writer = csv.writer(f)\n    writer.writerow(fields)\n    userId = cros_val['userId'].values.tolist()\n    for user in userId:\n        scores = []\n        items = []\n        results = []\n        results.append(user)\n        recommendations = model.recommend(user, user_items, N = 5)\n        for item in recommendations:\n            ids, score = item\n            scores.append(score)\n            items.append(ids)\n        results.append(items)\n        writer.writerow(results)","831e7b29":"predicted_c = pd.read_csv('rec_cros.csv')\npredicted_c = predicted_c['item_list']\nimport ast \npredicted_c = [ast.literal_eval(a) for a in predicted_c]\nactual_c = cros_val['itemId']\nimport numpy as np\nactual_c = np.array(actual_c).reshape(64628,1)\nimport ml_metrics\nscore_c = ml_metrics.mapk(actual_c, predicted_c, 5)\nprint('Mean avg. precision at k for cros_val set:','{:.6f}'.format(score_c))","5232a277":"fields = 'user_id', 'item_list'\nfilename = 'rec_test.csv'\nwith open(filename, 'a', newline = '') as f:\n    writer = csv.writer(f)\n    writer.writerow(fields)\n    userId = test['userId'].values.tolist()\n    for user in userId:\n        scores = []\n        items = []\n        results = []\n        results.append(user)\n        recommendations = model.recommend(user, user_items, N = 5)\n        for item in recommendations:\n            ids, score = item\n            scores.append(score)\n            items.append(ids)\n        results.append(items)\n        writer.writerow(results)","f1b20aca":"predicted_t = pd.read_csv('rec_test.csv')\npredicted_t = predicted_t['item_list']\npredicted_t = [ast.literal_eval(a) for a in predicted_t]\nactual_t = test['itemId']\nactual_t = np.array(actual_t).reshape(64628,1)\nscore_t = ml_metrics.mapk(actual_t, predicted_t, 5)\nprint('Mean avg. precision at k for test set:','{:.6f}'.format(score_t))","328d2c46":"model.explain(40428, user_items, 4, N = 5)","d83f5a97":"print('List of similar items for itemId 4:')\nmodel.similar_items(4, N = 5)","1d5b8ab7":"print('List of similar users for userId 40428:')\nmodel.similar_users(40428, N = 5)","9ffc4427":"#### We can check the total predicted score for a specific user\/item pair, e.g., 40428\/4 pair total score is 0.2. Also top 5 (items) contributions for this pair is return by model.explain. So the model can explain the reason behind the recommendation.","7ca2568b":"#### As I am not using category_id and cusine_id I will not impute those missing values.","e1b5d74d":"## Exploratory Data Analysis","d9942289":"#### Splitted the dataset into 60,24,16.","6201eb22":"#### Different hours of different days can be a good indicator for the model, I will stop here and try to incorporate this with the model later.","b9632f60":"#### Fortunately or unfortunately I have lower score in training set. cros_val score and test score are good and consistant. ","7381bf54":"## Data preprocessing","afb6a156":"# Recommender system\n#### I don't have explicit ratings in my dataset, so I need to use implicit feedback from customers. Here item_count considered as implicit feedback from customers, item_counts represents customers's preference or confidence for a specific item. \n\n#### I will use implicit Alternating Least Squares (ALS) as a recommendation model. To go forward we need two csr matrix, user_items (csr_matrix) \u2013 Sparse matrix containing the liked items for the user and item_users (csr_matrix) \u2013 Matrix of confidences for the liked items. This matrix should be a csr_matrix where the rows of the matrix are the item, the columns are the users that liked that item, and the value is the confidence that the user liked the item.","b7104cb6":"#### Number of orders and number of items sold have no significant changes for different days and hours, I mean day 6 or hour 2 has the highest orders and the highest item sold and also true for lowest. I wanted to check if any lower nubmer of order has higher number of sell or not.","9ca0fda5":"#### Different item_id consists of different combination of category, cusine and restaurant so finally I will work with item_id as it consists all the information of category, cusine and restaurant.","eac7ed0e":"#### For my model I need user id and item id as numeric data type instead of category. So I will convert those into numeric.","676e918a":"## Food for thought\n#### 1. Why my training set has lower score?\n#### 2. model.recommend returns recommendation for 1 user only so I needed to write a csv with all recommendation. Is there any way to get recommendations for all user ids without exporting csv? I have tried with different alpha and regularizations.\n#### 3. What are the other evaluation matrics should I use?\n#### 4. How can I incorporate days and hours in my model?\n### If you fork or create a separate kernel you may include any of the above.\n### Thanks for your patience and please let me know your feedback.","3f57081f":"#### Though there are no significant differences among different days of the week, day 6 and 5 have more than 15% orders may be 5 and 6 are friday and saturday respectively. \n#### For hour we can see there is no data for hour 7 to hour 14, may be this period is 12 AM (Night) to 8 AM (Morning). Other than this period we can see 0 to 4 (may be 5 PM to 9 PM) are the pick hours for the orders, within these hours maximum orders have been placed. \n","cbb22372":"# Recommender system by Alternating Least Squares for Implicit Feedback","da5dcb8d":"#### User 40428 bought item 4 6 times already, so s\/he pretty confident about the item 4 or possesses higher preference for item 4.","afbc2ccd":"## Model evaluation \n#### Comparision of train, cros_val and test scores. I used map@k as evaluation matric. map@k considers the order of the predicted items so recommendation accuracy can be obtained easily. ","19b5bc46":"#### I will retain old user_id and item_id so that I can generate recommended list based on old IDs."}}