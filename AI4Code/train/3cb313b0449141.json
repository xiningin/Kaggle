{"cell_type":{"0948c7f9":"code","2d7dde38":"code","f573a921":"code","e139dd64":"code","ad06aeea":"code","a52c3dcf":"code","13f78da5":"code","8140d9bf":"code","5405dba3":"code","6920cd22":"code","4259b118":"code","1f27c858":"code","895343fe":"code","2d1494a5":"code","5daefb9d":"code","7ef2c6bc":"code","e3749894":"code","46652f21":"code","99565b56":"code","ddb88222":"code","a88f2606":"code","855b84e7":"code","5797b411":"code","0d915b54":"code","4376eb4b":"code","82360433":"code","b6334745":"code","91e332df":"code","24047579":"code","7aff27d0":"code","3e21a3d1":"code","6406dcf9":"code","885425dc":"code","d43bc0a1":"code","056f89f5":"code","39d20792":"code","99abe7a0":"code","8e1eba7d":"code","94c0ffa9":"code","f20b19e6":"code","29ff37bf":"markdown","f50b8710":"markdown","abcc78e0":"markdown","1802e8ea":"markdown","665d0b59":"markdown","045a954e":"markdown","f4a7de81":"markdown","26c675ad":"markdown","84d16d43":"markdown","1623896a":"markdown"},"source":{"0948c7f9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport os\nfrom sklearn_pandas import CategoricalImputer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2d7dde38":"df = pd.read_csv('\/kaggle\/input\/videogamesales\/vgsales.csv')\ndf.head()","f573a921":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'column_name': df.columns,\n                               'percent_missing': percent_missing})\nmissing_values","e139dd64":"labels = df['Genre']\ndf['Genre'].value_counts()","ad06aeea":"imputer = CategoricalImputer()\ndf['Year'] = imputer.fit_transform(df['Year'].values)\ndf['Publisher'] = imputer.fit_transform(df['Publisher'].values)\npercent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_values = pd.DataFrame({'column_name': df.columns,\n                               'percent_missing': percent_missing})\nmissing_values","a52c3dcf":"df = df.drop(['Rank', 'Year'], axis=1)\ndf = df.apply(preprocessing.LabelEncoder().fit_transform)\nenc_labels = df['Genre']\ndf = pd.get_dummies(df)\ndf.head()","13f78da5":"labels.unique()","8140d9bf":"plt.rcParams['legend.fontsize'] = '16'","5405dba3":"from pandas.plotting import parallel_coordinates\n\ndf2 = df.drop(['Genre'], axis=1)\ndf2['Genre'] = labels\n\n#df.plot(figsize=(10,10), fontsize=24)\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nax.set_title(\"Parallel Coordinates Example\", fontsize=20)\nax.tick_params(axis='x', rotation=30)\nax.tick_params(axis='both', labelsize=20)\nparallel_coordinates(df2, class_column='Genre', ax=ax)","6920cd22":"X = df.drop(['Genre'], axis=1)\ny = df['Genre']\nscaler = StandardScaler().fit(X)\nX2 = scaler.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.33, random_state=42)","4259b118":"from yellowbrick.features import ParallelCoordinates\n\n# Specify the features of interest and the classes of the target\nfeatures = list(X.columns)\nclasses = list(labels.unique())\n\n# Fit the visualizer and display it\nplt.rcParams['legend.fontsize'] = '16'\nplt.rcParams['axes.titlesize'] = '20'\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(111)\nax.tick_params(axis='x', rotation=30)\nax.tick_params(axis='both', labelsize=20)\n\n# Instantiate the visualizer\nvisualizer = ParallelCoordinates(ax=ax,\n    classes=classes, features=features,\n    normalize='standard', sample=0.05, shuffle=True\n)\n\nvisualizer.fit_transform(X, labels)\nvisualizer.show(fontsize=20)","1f27c858":"clf = RandomForestClassifier(max_depth=5)\nclf.fit(X_train, y_train)\npredictionforest = clf.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))","895343fe":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nrandom_search = {'max_depth': list(np.linspace(10, 1200, 10, dtype = int)) + [None],\n               'min_samples_leaf': [4, 6, 8, 12],\n               'min_samples_split': [5, 7, 10, 14],\n               'max_leaf_nodes': list(np.linspace(10, 120, 10, dtype = int)),\n               'n_estimators': list(np.linspace(151, 1200, 10, dtype = int)),\n                 'bootstrap': [True, False]\n                }\n\nclf = RandomForestClassifier()\nmodel = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 7, \n                               cv = 2, verbose= 5, random_state= 101, n_jobs = -1)\nmodel.fit(X_train,y_train)","2d1494a5":"import seaborn as sns\n\ntable = pd.pivot_table(pd.DataFrame(model.cv_results_),\n    values='mean_test_score', index='param_n_estimators', columns='param_min_samples_leaf')\n     \nsns.heatmap(table)","5daefb9d":"predictionforest = model.best_estimator_.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))","7ef2c6bc":"df2 = pd.DataFrame(model.cv_results_)\ndf2.head()","e3749894":"enc_bootstrap = preprocessing.LabelEncoder().fit_transform(df2['param_bootstrap'])\ndf2 = df2.drop(['params', 'param_bootstrap'], axis=1)\ndf2['param_bootstrap'] = enc_bootstrap\ndf2['mean_test_score'] = df2['mean_test_score']*100\ndf2 = df2[['param_bootstrap', 'param_n_estimators', 'param_min_samples_split', 'param_min_samples_leaf', 'param_max_leaf_nodes',\n           'param_max_depth', 'mean_test_score']].astype(int)","46652f21":"df2['mean_test_score']","99565b56":"# From: https:\/\/stackoverflow.com\/questions\/23547347\/parallel-coordinates-plot-for-continous-data-in-pandas\ndef parallel_coordinates2(frame, class_column, cols=None, ax=None, color=None,\n                     use_columns=False, xticks=None, colormap=None,\n                     **kwds):\n\n    n = len(frame)\n    class_col = frame[class_column]\n    class_min = np.amin(class_col)\n    class_max = np.amax(class_col)\n\n    if cols is None:\n        df = frame.drop(class_column, axis=1)\n    else:\n        df = frame[cols]\n\n    used_legends = set([])\n\n    ncols = len(df.columns)\n\n    # determine values to use for xticks\n    if use_columns is True:\n        if not np.all(np.isreal(list(df.columns))):\n            raise ValueError('Columns must be numeric to be used as xticks')\n        x = df.columns\n    elif xticks is not None:\n        if not np.all(np.isreal(xticks)):\n            raise ValueError('xticks specified must be numeric')\n        elif len(xticks) != ncols:\n            raise ValueError('Length of xticks must match number of columns')\n        x = xticks\n    else:\n        x = range(ncols)\n\n    fig = plt.figure(figsize=(18,7))\n    ax = fig.add_subplot(111)\n\n    Colorm = plt.get_cmap(colormap)\n\n    for i in range(n):\n        y = df.iloc[i].values\n        kls = class_col.iat[i]\n        ax.plot(x, y, color=Colorm((kls - class_min)\/(class_max-class_min)), \n                alpha=(kls - class_min)\/(class_max-class_min), **kwds)\n\n    for i in x:\n        ax.axvline(i, linewidth=1, color='black')\n\n    ax.set_xticks(x)\n    ax.set_xticklabels(df.columns)\n    ax.set_xlim(x[0], x[-1])\n    ax.set_title(\"Parallel Coordinates Example\", fontsize=20)\n    ax.tick_params(axis='x', rotation=30)\n    ax.tick_params(axis='both', labelsize=20)\n    ax.grid(False)\n\n    bounds = np.linspace(class_min,class_max,10)\n    cax,_ = mpl.colorbar.make_axes(ax)\n    cb = mpl.colorbar.ColorbarBase(cax, cmap=Colorm, spacing='proportional', ticks=bounds, \n                                   boundaries=bounds, format='%.2f')\n\n    return fig","ddb88222":"fig = parallel_coordinates2(df2, class_column='mean_test_score',  colormap=\"viridis\")","a88f2606":"!pip install chart-studio","855b84e7":"import chart_studio\nchart_studio.tools.set_credentials_file(username='TODO', api_key='TODO')","5797b411":"import plotly.express as px\nimport chart_studio.plotly as py\n\nfig = px.parallel_coordinates(df2, color=\"mean_test_score\", \n                             labels=dict(zip(list(df2.columns), \n                                             list(['_'.join(i.split('_')[1:]) for i in df2.columns]))),\n                             color_continuous_scale=px.colors.diverging.Tealrose,\n                             color_continuous_midpoint=27)\n#py.plot(fig, filename = 'Parallel Coordinates', auto_open=True)\nfig.show()","0d915b54":"fig = px.parallel_coordinates(df, color=\"Genre\", \n                             labels=list(df.columns),\n                             color_continuous_scale=px.colors.diverging.Tealrose,\n                             color_continuous_midpoint=27)\nfig.show()","4376eb4b":"!pip install datawrapper","82360433":"from datawrapper import Datawrapper\ndw = Datawrapper(access_token = \"TODO\")","b6334745":"df3 = pd.read_csv('\/kaggle\/input\/videogamesales\/vgsales.csv')\n#df3.head()\nplt.barh(df3['Publisher'].value_counts().index[:10], df3['Publisher'].value_counts().values[:10])\nplt.title(\"Most Frequent Publishers\")\nplt.xlabel(\"Number of Games\");","91e332df":"res = {'Publisher Name': df3['Publisher'].value_counts().index[:10], 'Occurrences': df3['Publisher'].value_counts().values[:10]}\nres = pd.DataFrame(data=res)","24047579":"# games_chart = dw.create_chart(title = \"Most Frequent Game Publishers\", chart_type = 'd3-bars', data = res)","7aff27d0":"# dw.update_description(\n#     games_chart['id'],\n#     source_name = 'Video Game Sales',\n#     source_url = 'https:\/\/www.kaggle.com\/gregorut\/videogamesales',\n#     byline = 'Pier Paolo Ippolito',\n# )","3e21a3d1":"from IPython.display import IFrame\n\n# dw.publish_chart(games_chart['id'])","6406dcf9":"from IPython.display import HTML\n\nHTML('<iframe title=\"Most Frequent Game Publishers\" aria-label=\"Bar Chart\" id=\"datawrapper-chart-YEUFF\" src=\"https:\/\/datawrapper.dwcdn.net\/YEUFF\/1\/\" scrolling=\"no\" frameborder=\"0\" style=\"width: 0; min-width: 100% !important; border: none;\" height=\"undefined\"><\/iframe><script type=\"text\/javascript\">!function(){\"use strict\";window.addEventListener(\"message\",(function(a){if(void 0!==a.data[\"datawrapper-height\"])for(var e in a.data[\"datawrapper-height\"]){var t=document.getElementById(\"datawrapper-chart-\"+e)||document.querySelector(\"iframe[src*='\"+e+\"']\");t&&(t.style.height=a.data[\"datawrapper-height\"][e]+\"px\")}}))}();<\/script>')","885425dc":"!pip install dtreeviz","d43bc0a1":"df = pd.read_csv('\/kaggle\/input\/videogamesales\/vgsales.csv')\ndf.drop(df[(df.Genre == 'Racing') | (df.Genre == 'Shooter') | (df.Genre == 'Role-Playing') ].index, inplace=True)\nlabels = df['Genre']\nimputer = CategoricalImputer()\ndf['Year'] = imputer.fit_transform(df['Year'].values)\ndf['Publisher'] = imputer.fit_transform(df['Publisher'].values)\ndf = df.drop(['Rank', 'Year'], axis=1)\ndf = df.apply(preprocessing.LabelEncoder().fit_transform)\nenc_labels = df['Genre']\ndf = pd.get_dummies(df)\ndf.head()","056f89f5":"X = df.drop(['Genre'], axis=1)\ny = df['Genre']\nscaler = StandardScaler().fit(X)\nX2 = scaler.transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X2, y, test_size=0.33, random_state=42)","39d20792":"clf = DecisionTreeClassifier(max_depth=3)\nclf.fit(X_train, y_train)\npredictionforest = clf.predict(X_test)\nprint(confusion_matrix(y_test,predictionforest))\nprint(classification_report(y_test,predictionforest))","99abe7a0":"from dtreeviz.trees import *\n\nviz = dtreeviz(clf,\n               X_train,\n               y_train.values,\n               target_name='Genre',\n               feature_names=list(X.columns),\n               class_names=list(labels.unique()),\n               histtype='bar', \n               orientation ='TD')\n              \nviz","8e1eba7d":"#viz.svg()","94c0ffa9":"!pip install ann_visualizer","f20b19e6":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom ann_visualizer.visualize import ann_viz\n\nmodel = Sequential()\nmodel.add(Dense(units=4,activation='relu',\n                  input_dim=7))\nmodel.add(Dense(units=4,activation='sigmoid'))\nmodel.add(Dense(units=2,activation='relu'))\n\nann_viz(model, view=True, filename=\"example\", title=\"Example ANN\")","29ff37bf":"## dtreeviz : Decision Tree Visualization ","f50b8710":"## Plotly","abcc78e0":"# Machine Learning Visualization","1802e8ea":"## Pandas","665d0b59":"## Data Wrapper","045a954e":"## Custom Matplotlib","f4a7de81":"## Set up","26c675ad":"## Yellowbrick","84d16d43":"Alternatively, it could be possible to use [Weights & Biases Sweeps](https:\/\/www.wandb.com\/articles\/hyperparameter-tuning-as-easy-as-1-2-3)","1623896a":"## Creating Parallel Coordinates Plots in Python"}}