{"cell_type":{"4a5c2718":"code","f5cbc573":"code","a3217054":"code","4641de01":"code","0c2989e8":"code","379c89d3":"code","2e4ca107":"code","a9aff5f5":"code","53ee3f4b":"code","c167f9a9":"code","47da3fcd":"code","ea6f58f4":"code","b8d28202":"code","03e218af":"code","18d0fa85":"code","68e32e66":"code","c0bdfe25":"code","ec893797":"code","95e95541":"code","55529731":"code","748a70bc":"code","7cf6e615":"code","6e67a96c":"code","28ad4889":"code","d20da6e2":"code","db36fde3":"code","94030147":"code","e2ec9ab1":"code","56bb5e4d":"code","2c2a9d37":"code","e48fab27":"code","27254288":"code","8cda3f06":"code","f9aad203":"code","9d4f7888":"code","130643c3":"code","e905bc04":"code","f0cdf179":"code","4e6e8b84":"code","fab28804":"code","509adc33":"code","dc2c471f":"code","87c3e27b":"code","fe815103":"code","714a81f3":"code","e15968cf":"code","594b98a9":"code","499ae9a9":"code","7b7d8c11":"code","e0cde838":"code","e51cdeaf":"code","06b7f552":"code","55934402":"code","4307461e":"code","467f36fd":"code","9df38027":"code","2b0680f3":"markdown","73655e56":"markdown","0b44842c":"markdown","7e2033b9":"markdown","be2835d6":"markdown","0d782bd3":"markdown","a489a807":"markdown","53276d8c":"markdown","471a9317":"markdown","9b4cfea2":"markdown","41e98a29":"markdown","e53f10b3":"markdown","8c0dd32c":"markdown","cfd0e952":"markdown","3e24dd60":"markdown","be6186ed":"markdown","84e1d91f":"markdown","802ef4a6":"markdown","ed48e50f":"markdown","8416c98d":"markdown","c0c36212":"markdown","087a0303":"markdown","4301a594":"markdown","391fb4a9":"markdown"},"source":{"4a5c2718":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","f5cbc573":"## defining constants\nPATH_TRAIN = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\"\nPATH_TEST = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_date_metadata.csv\" # \u043a\u043e\u043b-\u0432\u043e \u0432\u044b\u0437\u0434\u043e\u0440\u043e\u0432\u0435\u0432\u0448\u0438\u0445 \u043f\u043e \u0434\u0430\u0442\u0430\u043c\n\nVAL_DAYS = 7 # \u0434\u043d\u0435\u0439 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\nMAD_FACTOR = 0.5 # \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c\u0430\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000] # \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0434\u043d\u0435\u0439 \u043f\u0440\u043e\u0448\u043b\u043e \u0441 \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u043a\u043e\u0433\u0434\u0430 \u0431\u044b\u043b\u043e \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043e N \u0441\u043b\u0443\u0447\u0430\u0435\u0432 \u0437\u0430\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n\nSEED = 2357\n\n# \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0434\u043b\u044f \u0431\u0443\u0441\u0442\u0438\u043d\u0433\u0430\nLGB_PARAMS = {\"objective\": \"regression\", \n              \"num_leaves\": 5, # \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043b\u0438\u0441\u0442\u044c\u0435\u0432\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91, # \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\n              \"feature_fraction\": 0.81, # \u0441\u044d\u043c\u043f\u043b\u0438\u0440\u0443\u0435\u043c \u0444\u0430\u043a\u0442\u043e\u0440\u044b\n              \"reg_alpha\": 0.13, # \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 L1 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438\n              \"reg_lambda\": 0.13, # \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 L2 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0438\n              \"metric\": \"rmse\", # \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0438\u0440\u0443\u0435\u043c\u0430\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0430\n              \"seed\": SEED\n             }\n","a3217054":"## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)\n","4641de01":"train.head()","0c2989e8":"train.groupby('Country_Region')['Id'].count()","379c89d3":"print(train.Date.min(), train.Date.max())\nprint(test.Date.min(), test.Date.max())","2e4ca107":"region_metadata.head()","a9aff5f5":"region_date_metadata","53ee3f4b":"train[~train.Province_State.isna()]","c167f9a9":"test","47da3fcd":"test.groupby('Country_Region')['ForecastId'].count()","ea6f58f4":"from bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()","b8d28202":"train_dt = train.copy()\ntrain_dt['Date'] = train_dt['Date'].astype('datetime64[ns]')","03e218af":"train_dt.info()","18d0fa85":"output_notebook()\n\ntab_list = []\n\nfor country in ['Italy', 'Russia', 'Ukraine']:\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 Confirmed Cases over time\")\n    v.line(train_dt[train_dt.Country_Region == country].Date, train_dt[train_dt.Country_Region == country].ConfirmedCases, color = \"green\", legend_label = \"CC\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = country)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","68e32e66":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]","c0bdfe25":"df_panel = pd.concat([train, test], sort = False)","ec893797":"df_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)","95e95541":"df_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region","55529731":"df_test = pd.DataFrame([[1, 2, 'y'], [3, 4, 'y'], [1, 1, 'y'], [5, 6, 'x'], [2, 4, 'x'], [1, 1, 'x']], columns=['a', 'b', 'c'])\n\ndf_test.loc[df_test.a > 2, 'b'] = 0\n\ndf_test","748a70bc":"df_test = pd.DataFrame([[1, 2, 'y'], [3, 4, 'y'], [1, 1, 'y'], [5, 6, 'x'], [2, 4, 'x'], [1, 1, 'x']], columns=['a', 'b', 'c'])\n\ndf_test.groupby('c')['a'].cummax()","7cf6e615":"# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")","6e67a96c":"# label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)","28ad4889":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n","d20da6e2":"df_panel","db36fde3":"min_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)","94030147":"min_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)","e2ec9ab1":"n_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())","56bb5e4d":"print(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))","2c2a9d37":"for lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)","e48fab27":"for case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")\n","27254288":"df_panel","8cda3f06":"## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")\n","f9aad203":"df_panel_new = df_panel.copy()\ndf_panel_new[f\"days_since_100_case\"] = (df_panel.Date - df_panel[f\"case_100_date\"]).astype(\"timedelta64[D]\")","9d4f7888":"df_panel_new[f\"days_since_100_case\"] < 1","130643c3":"df_panel_new.loc[df_panel_new[f\"days_since_100_case\"] < 1]","e905bc04":"df_panel_new.loc[df_panel_new[f\"days_since_100_case\"] < 1, f\"days_since_100_case\"] = np.nan","f0cdf179":"df_panel_new[df_panel_new[f\"days_since_100_case\"].isna()]","4e6e8b84":"np.log1p(5)","fab28804":"## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) \/ df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc \/ df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc \/ df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft \/ df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft \/ df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) \/ 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) \/ 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] \/ df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] \/ df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_123_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_123_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df.Date - df[f\"case_{case}_date\"]).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n    df[\"density\"] = df.population \/ df.area\n    \n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases) - np.log1p(df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities) - np.log1p(df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_123_cc\",\n        \"change_123_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]\n","509adc33":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_ft))\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n","dc2c471f":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ n_dates_test\n\n    return y_pred_cc, y_pred_ft\n","87c3e27b":"df_panel[df_panel.ForecastId.isna()]","fe815103":"df_test_full = df_panel[~df_panel.ForecastId.isna()]\ndf_train = df_panel[df_panel.ForecastId.isna()]","714a81f3":"max_date_train = pd.Timestamp('2020-04-15')","e15968cf":"for date in df_test_full.Date.unique():\n    print(\"Processing date:\", date)\n    \n    if date in df_train.Date.values:\n        print(\"already exists\")\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        print(pd.Timestamp(date), max_date_train, gap)\n        \n        break","594b98a9":"max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")","499ae9a9":"## building lag x-days models\n# df_train = df_panel[~df_panel.Id.isna()]\n# df_train = df_panel[df_panel.ForecastId.isna()]\n# df_test_full = df_panel[~df_panel.ForecastId.isna()]\ndf_train = df_panel[df_panel.Date <= max_date_train]\ndf_test_full = df_panel[(df_panel.Date > max_date_train)&(~df_panel.ForecastId.isna())]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        # df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        # df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)\n","7b7d8c11":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) \/ 2, 2))\nprint(\"\\n\")\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) \/ 2, 2))\n","e0cde838":"df_panel[(~df_panel.ConfirmedCases_test_lgb.isna())&(df_panel.geography == 'Italy')]","e51cdeaf":"## feature importance\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.palettes import Spectral3\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\ndf_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\ndf_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n\ndf_fimp_1_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_14_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_28_cc.sort_values(\"importance\", ascending = False, inplace = True)\n\nv1 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_1_cc.feature, title = \"Feature Importance of LGB Model 1\")\nv1.vbar(x = df_fimp_1_cc.feature, top = df_fimp_1_cc.importance, width = 1)\nv1.xaxis.major_label_orientation = 1.3\n\nv14 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_14_cc.feature, title = \"Feature Importance of LGB Model 14\")\nv14.vbar(x = df_fimp_14_cc.feature, top = df_fimp_14_cc.importance, width = 1)\nv14.xaxis.major_label_orientation = 1.3\n\nv28 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_28_cc.feature, title = \"Feature Importance of LGB Model 28\")\nv28.vbar(x = df_fimp_28_cc.feature, top = df_fimp_28_cc.importance, width = 1)\nv28.xaxis.major_label_orientation = 1.3\n\nv = column(v1, v14, v28)\n\nshow(v)\n","06b7f552":"## visualizing ConfirmedCases\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 ConfirmedCases over time\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases, color = \"green\", legend_label = \"CC (Train)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_lgb, color = \"blue\", legend_label = \"CC LGB (Val)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_mad, color = \"purple\", legend_label = \"CC MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"CC LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"CC MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","55934402":"## visualizing Fatalities\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 Fatalities over time\")\n    v.line(df_geography.Date, df_geography.Fatalities, color = \"green\", legend_label = \"FT (Train)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_lgb, color = \"blue\", legend_label = \"FT LGB (Val)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_mad, color = \"purple\", legend_label = \"FT MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"FT LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"FT MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","4307461e":"df_geography.Date","467f36fd":"## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.15 * df_test.ConfirmedCases_test_lgb + 0.85 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.15 * df_test.Fatalities_test_lgb + 0.85 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these countries well\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\n\ndf_submission\n","9df38027":"## writing final submission and complete output\ndf_submission.to_csv(PATH_SUBMISSION, index = False)\ndf_test.to_csv(PATH_OUTPUT, index = False)\n","2b0680f3":"### \u041a\u043e\u043b-\u0432\u043e \u0434\u043d\u0435\u0439 \u0441 \u043c\u043e\u043c\u0435\u043d\u0442\u0430, \u043a\u043e\u0433\u0434\u0430 \u0431\u044b\u043b\u043e \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043e case \u0441\u043b\u0443\u0447\u0430\u0435\u0432 \u0437\u0430\u0440\u0430\u0436\u0435\u043d\u0438\u044f","73655e56":"### \u041f\u043e\u043b\u043d\u044b\u0439 \u043a\u043e\u0434 \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u0430","0b44842c":"## MAD Model\n* This Moving Average with Decay (MAD) model is a simple heuristic using historic values that decays with time.\n\n* It is structured based on my EDA and intuitive feeling of how the Covid-19 trend is likely to move.","7e2033b9":"### \u041d\u0430\u0442\u0443\u0440\u0430\u043b\u044c\u043d\u044b\u0439 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c \u043e\u0442 x + 1","be2835d6":"### \u041f\u0440\u0438\u043c\u0435\u0440 \u0446\u0438\u043a\u043b\u0430 \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f gap","0d782bd3":"## Validation\n* \u0412\u0430\u043b\u0438\u0434\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 LGB \u0438 MAD \u043c\u043e\u0434\u0435\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f RMSLE \u043c\u0435\u0442\u0440\u0438\u043a\u0443.\n\n* \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c feature importance \u0434\u043b\u044f 1, 14 \u0438 28 \u043c\u043e\u0434\u0435\u043b\u0438, \u0447\u0442\u043e\u0431\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c, \u0447\u0442\u043e \u0432\u043b\u0438\u044f\u0435\u0442 \u043d\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0431\u043b\u0438\u0436\u0430\u0439\u0448\u0435\u0433\u043e \u043f\u0435\u0440\u0438\u043e\u0434\u0430, \u0441\u0440\u0435\u0434\u043d\u0435\u0433\u043e \u0438 \u0434\u043e\u043b\u0433\u043e\u0441\u0440\u043e\u0447\u043d\u043e\u0433\u043e \u043f\u0435\u0440\u0438\u043e\u0434\u043e\u0432.","a489a807":"L1 \u0438 L2 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u0441\u0442\u043e:  \nhttps:\/\/medium.com\/@Radiologist\/l1-\u0438-l2-\u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u044f-f6891b354507\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html","53276d8c":"### \u041d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f","471a9317":"### \u0424\u0438\u043a\u0441\u0438\u043c \u043e\u0448\u0438\u0431\u043a\u0438 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e cummax()","9b4cfea2":"## Feature Engineering\n* Lag features are created.\n\n* Several differences, ratios and averages of historic values are created and used.","41e98a29":"## Covid-19 Forecasting (Week 4)","e53f10b3":"## Modelling\n* \u041e\u0434\u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u043a\u0430\u0436\u0434\u0443\u044e \u0434\u0430\u0442\u0443.\n\n* \u0414\u043b\u044f \u0434\u0430\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443\u0436\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 train \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0435 \u043e\u0431\u0443\u0447\u0430\u0435\u043c.\n\n* \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438 \u0430\u043d\u0430\u043b\u043e\u0433\u0438\u0447\u043d\u044b\u043c \u0444\u0440\u0435\u0439\u043c\u0432\u043e\u0440\u043a\u043e\u043c","8c0dd32c":"### \u041a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441\u0442\u0440\u0430\u043d","cfd0e952":"### \u041f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f \u0442\u0438\u043f\u0430 dataframe datetime","3e24dd60":"## Visualizing Predictions\n\n* \u041e\u0442\u043e\u0431\u0440\u0430\u0436\u0430\u0435\u043c \u0442\u0435\u043a\u0443\u0449\u0438\u0435, \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u0441\u0442\u0440\u0430\u043d\u044b \u043f\u043e \u0432\u044b\u044f\u0432\u043b\u0435\u043d\u043d\u044b\u043c \u0441\u043b\u0443\u0447\u0430\u044f\u043c \u0438 \u0441\u043c\u0435\u0440\u0442\u044f\u043c.","be6186ed":"## Prepare Data\n* Unlike many competitions, there are overlapping dates across train and test data. So it is important to be careful while handling the dates.\n\n* Some data issues are fixed using previous max values in case of discrepancy.\n\n* External data from [this dataset](https:\/\/www.kaggle.com\/rohanrao\/covid19-forecasting-metadata) is merged.","84e1d91f":"### \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0441\u0442\u0440\u0430\u043d\u0443 \u0438 \u0448\u0442\u0430\u0442 \u0432 \u043e\u0434\u0438\u043d \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440","802ef4a6":"### Data exploration","ed48e50f":"## LGB Model\n\n* \u0422\u0430\u0440\u0433\u0435\u0442 - \u044d\u0442\u043e \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0442\u0435\u043a\u0443\u0449\u0438\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c ConfirmedCases \/ Fatalities \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u043c \u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435\u043c (\u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0439 \u0434\u0435\u043d\u044c)\n\n* \u041e\u0434\u043d\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u0441\u0442\u0440\u043e\u0438\u0442\u0441\u044f \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0434\u043d\u044f \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0432 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435.\n\n* \u0415\u0434\u0438\u043d\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432 \u0441 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u043c \u0447\u0438\u0441\u043b\u043e\u043c \u043b\u0438\u0441\u0442\u044c\u0435\u0432 \u0438 \u0440\u0435\u0433\u0443\u043b\u044f\u0440\u0438\u0437\u0430\u0446\u0438\u0435\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0442\u0432\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f.","8416c98d":"### \u0414\u0435\u043d\u044c, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0431\u044b\u043b\u0438 \u0437\u0430\u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b case \u0441\u043b\u0443\u0447\u0430\u0435\u0432 \u0437\u0430\u0440\u0430\u0436\u0435\u043d\u0438\u044f","c0c36212":"## Submission\n* Combining the two aapproaches using weights for final submission.\n\n* LGB models don't seem to perform well for certain geographies and are replaced by MAD predictions.","087a0303":"### \u041e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435 \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u043a\u0430\u043a nan","4301a594":"### \u0417\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0430\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043f\u043e\u043b\u044f","391fb4a9":"### \u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u0441\u0434\u0432\u0438\u0433\u0438 \u043d\u0430 lag \u0434\u043d\u0435\u0439 \u043d\u0430\u0437\u0430\u0434"}}