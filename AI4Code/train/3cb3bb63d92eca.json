{"cell_type":{"c25a582c":"code","efa78043":"code","4f08ca00":"code","92ddba7c":"code","d85ccba4":"code","ee0af4c0":"code","f0c7747f":"markdown","6fb36f71":"markdown","5971a429":"markdown","ff7a3813":"markdown","e92de2c3":"markdown"},"source":{"c25a582c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score","efa78043":"class LinearRegress:\n    \n    \"\"\"\n    Linear Regression with Gradient Descent.\n    \"\"\"\n    def __init__(self,bias=True, learning_rate = 0.01, max_iteration=1000, keep_hist = True):\n        \"\"\"\n        Linear Regression with Gradient Descent.\n        \n        params\n        ------\n        bias: bool. Default=True. Whether to add bias variable or not.\n        \n        learning_rate: float. learning rate for gradient descent. \n        \n        max_iteration: scalar. Maximum iteration for Gradient Descent.\n        \n        keep_hist: bool. If true, stores J and W for each iteration.\n        \n        \"\"\"\n        self.bias = bias\n        self.w = None\n        self.error = None\n        self.alpha = learning_rate\n        self.max_iter = max_iteration\n        self.n_samples = None\n        self.n_features = None\n        self.keep_hist = keep_hist\n        self.history = []\n        \n    def gradient_descent(self,X,y):\n        \"\"\"\n        Simple SGD. \n\n        params\n        ------\n        X: Predictors. Shape(n_samples, n_features)\n\n        y: Target. Shape(n_samples)\n\n        \"\"\"\n        n_samples = len(y)\n        \n        # inserts column of 1s for bias.\n        X = np.insert(X,0,1,axis=1) \n        \n        # Weight Matrix. Shape(n_features,1)\n        w = np.random.rand(X.shape[1])\n        \n        for itr in range(self.max_iter):\n            # forward pass\n            yhat = np.dot(X,w) # (n_samples, 1)\n            # objective\n            error = yhat - y\n            J = (1\/(2 * n_samples)) * np.dot(error,error)\n            \n            # backward pass\n            w = w - (self.alpha \/ n_samples) * np.dot(X.T,error)\n            \n            if self.keep_hist is True:\n                self.history.append((J,w))\n            if self.verbose and (itr % 100 == 0 or itr == self.max_iter - 1):\n                print(\"Iteration: %d J: %.2f\" % (itr,J))\n        self.w = w\n\n    def fit(self,X,y,verbose=True):\n        \"\"\"\n        Train linear regression model.\n        \"\"\"\n        self.verbose = verbose\n        self.gradient_descent(X,y)\n\n    def score(self,X,y):\n        \"\"\"\n        Return the coefficient of determination R^2 of the prediction.\n        \n        params\n        ------\n        X: array-like of shape (n_samples, n_features)\n            Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead, shape = (n_samples, n_samples_fitted), where n_samples_fitted is the number of samples used in the fitting for the estimator.\n            \n        y: array-like of shape (n_samples,)\n            True values for X.\n        \n        returns\n        -------\n        score: float.\n            R^2 score.\n        \"\"\"\n        yhat = self.predict(X)\n        return r2_score(y,yhat)\n    \n    def predict(self,X):\n        \"\"\"\n        Predicts using linear model.\n        \n        params\n        ------\n        X: array-like of shape (n_samples, n_features)\n            Test samples.\n        returns\n        -------\n        y: array-like of shape(n_samples,)\n            Output of the model.\n        \"\"\"\n        X = np.insert(X,0,1,axis=1)\n        yhat = np.dot(X,self.w)\n        return yhat","4f08ca00":"# heart data\ndf = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nX = df.iloc[:,:-1].copy()\ny = df.iloc[:,-1].copy()\nX_train,X_test,y_train,y_test = train_test_split(X.values,y.values)","92ddba7c":"model = LinearRegress(learning_rate=0.000001,max_iteration=1000)\nmodel.fit(X_train,y_train,verbose=True)\nmodel.score(X_test,y_test)","d85ccba4":"fig, ax = plt.subplots(facecolor='white')\ncosts = [tpl[0] for tpl in model.history]\ncoefs = [tpl[1] for tpl in model.history]\nax.plot(range(len(costs)),costs,color='green')\nax.set_xlabel('Iteration')\nax.set_ylabel('Objective')\nplt.show()","ee0af4c0":"sk = SGDRegressor()\nsk.fit(X_train,y_train)\nsk.score(X_test,y_test) > model.score(X_test,y_test)","f0c7747f":"# A Simple Implementation of Gradient Descent for Linear Regression","6fb36f71":"Comparing with SKLEARN's SGDRegressor.\n\n*It is obvious that R^2 score is not the only thing to compare in two models. SGDRegressor is better and fits more complex linear data*","5971a429":"## Linear Regression Class","ff7a3813":"## Testing\nOn UCI Heart Dataset. This is a ","e92de2c3":"Plot the history of cost and weights over iteration period"}}