{"cell_type":{"9054e954":"code","cfcc61e0":"code","fc586273":"code","570ad52e":"code","2c24ff34":"code","7b30ebda":"code","e59b5ebe":"code","e5c80000":"code","bfa25e09":"code","18e9a535":"code","bcac680c":"code","8d91998c":"code","94c16f5a":"code","8a26dfb0":"code","43cb245d":"code","4177775b":"code","651c412a":"code","0287135c":"code","33d9c16e":"code","69678829":"code","05c5993d":"code","33dd8450":"code","af853a40":"code","000386c1":"code","244b9899":"code","80bcf417":"code","b0b86df6":"code","8d90e64f":"code","eeb52213":"code","b3480351":"code","d1343273":"code","ce95a560":"code","0f4b2d4b":"code","8a45c9d9":"code","cd2bb759":"code","8dd65908":"code","7d92d494":"code","a48b7d0e":"markdown","417881ba":"markdown","a9056a4f":"markdown","b6e9d51c":"markdown","5a0c7612":"markdown","7b375654":"markdown","09f66213":"markdown","6c0e6986":"markdown"},"source":{"9054e954":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\n#sys.path.append('..\/..')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","cfcc61e0":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sn\n\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom joblib import dump, load","fc586273":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            #print(\"******************************\")\n            #print(\"Column: \",col)\n            #print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n","570ad52e":"\ndef train_short_form_loader(feature_file,target_file,extra_target_file=None):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    train_targets,_ = reduce_mem_usage(train_targets)\n\n\n    if extra_target_file is not None:\n        extra_targets = pd.read_csv(extra_target_file)\n        extra_targets,_ = reduce_mem_usage(extra_targets)\n        train_targets = pd.merge(train_targets,extra_targets,on ='sig_id')\n        del extra_targets\n\n    targets = train_targets.columns[1:]\n\n    train_melt=train_targets.merge(train_features,how=\"left\",on=\"sig_id\")\n\n\n    del train_features,train_targets\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt , targets.to_list()\n\n\n\ndef test_short_form_loader(feature_file):\n    '''takes the original target and features and creates a train dataset \n    in col long format'''\n\n\n    train_features = pd.read_csv(feature_file)\n\n    #train_targets = pd.read_csv(target_file)\n    train_features,_= reduce_mem_usage(train_features)\n    #train_targets,_ = reduce_mem_usage(train_targets)\n\n    train_melt =  train_features.copy()\n    del train_features\n\n\n    train_melt.set_index(\"sig_id\",inplace=True)\n\n    #train_melt[\"variable\"]= train_melt[\"variable\"].astype('category')\n    train_melt[\"cp_type\"]= train_melt[\"cp_type\"].astype('category')\n    train_melt[\"cp_dose\"]= train_melt[\"cp_dose\"].astype('category')\n\n    return train_melt \n","2c24ff34":"#from tools.loaders import train_short_form_loader, test_short_form_loader","7b30ebda":"input_directory = '..\/input\/lish-moa\/'","e59b5ebe":"train,target_cols = train_short_form_loader(input_directory +'train_features.csv',input_directory+'train_targets_scored.csv')\ntest = test_short_form_loader(input_directory +\"test_features.csv\")\n","e5c80000":"GENES = [col for col in train.columns if col.startswith('g-')]\nCELLS = [col for col in train.columns if col.startswith('c-')]","bfa25e09":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer","18e9a535":"from sklearn.compose import make_column_transformer,ColumnTransformer","bcac680c":"def seed_everything(seed=42):\n    random.seed(seed)\n    #os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","8d91998c":"def multifold_indexer(train,target_columns,n_splits=10,random_state=12347,**kwargs):\n    folds = train.copy()\n\n    mskf = MultilabelStratifiedKFold(n_splits=n_splits,random_state=random_state,**kwargs)\n    folds[ 'kfold']=0\n    for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=train[target_columns])):\n        folds.iloc[v_idx,-1] = int(f)\n\n    folds['kfold'] = folds['kfold'].astype(int)\n    return folds\n","94c16f5a":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \n\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","8a26dfb0":"class MoASwapDataset:\n    def __init__(self, features, noise):\n        self.features = features\n        self.noise = noise\n        \n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        \n#         if torch.is_tensor(idx):\n#             idx = idx.tolist()\n        \n        sample = self.features[idx, :].copy()\n        sample = self.swap_sample(sample)\n        \n        dct = {\n            'x' : torch.tensor(sample, dtype=torch.float),\n            'y' : torch.tensor(self.features[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \n    def swap_sample(self,sample):\n            #print(sample.shape)\n            num_samples = self.features.shape[0]\n            num_features = self.features.shape[1]\n            if len(sample.shape) == 2:\n                batch_size = sample.shape[0]\n                random_row = np.random.randint(0, num_samples, size=batch_size)\n                for i in range(batch_size):\n                    random_col = np.random.rand(num_features) < self.noise\n                    #print(random_col)\n                    sample[i, random_col] = self.features[random_row[i], random_col]\n            else:\n                batch_size = 1\n          \n                random_row = np.random.randint(0, num_samples, size=batch_size)\n               \n            \n                random_col = np.random.rand(num_features) < self.noise\n                #print(random_col)\n                #print(random_col)\n       \n                sample[ random_col] = self.features[random_row, random_col]\n                \n            return sample\n\n        \n    ","43cb245d":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        if not  scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n            scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, scheduler, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    if scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n        scheduler.step(final_loss)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","4177775b":"from sklearn.base import BaseEstimator,TransformerMixin\n\nclass CatIntMapper( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ,col,dicti):\n        self.col = col\n        self.dicti = dicti\n        \n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n       \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n    \n    def transform( self, X):\n        assert  X[self.col].isin(self.dicti.keys()).all() \n        \n        return pd.concat([X.drop(self.col,axis=1),X[self.col].map(self.dicti).astype(int).rename(self.col)],axis=1) \n\nclass NamedOutTWrapper( BaseEstimator, TransformerMixin ):\n    \n    def __init__(self,transformer,columns,inplace=False,prefix='_' ):\n        \n        self.transformer = transformer\n        self.cols = columns\n        self.inplace =  inplace\n        self.prefix = prefix\n        self.transformer_name = self._get_transformer_name()\n        \n    def fit(self, X, y = None):\n            \n        self.transformer =   self.transformer.fit(X[self.cols] , y )\n            \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n       \n        \n        transformed_columns = self.transformer.fit_transform(X[self.cols] , y )\n        out=pd.DataFrame(index=X.index)\n        \n       \n        if self.inplace:\n            out = X[self.cols]\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n           \n            for i,values in enumerate(transformed_columns.transpose()):\n            \n                out[ self.transformer_name + self.prefix + str(i)] = values\n        \n       \n        \n            return   pd.concat([X,out],axis=1)\n    \n    def transform( self, X):\n        \n        transformed_columns = self.transformer.transform(X[self.cols]  )\n        \n        out=pd.DataFrame(index=X.index)\n        \n        if self.inplace:\n            out = X[self.cols]\n            out[self.cols] = transformed_columns\n            \n            return pd.concat([X.drop(self.cols,axis=1),out],axis=1)\n        else:\n            for i,values in enumerate(transformed_columns.transpose()):\n\n                out[ self.transformer_name + self.prefix + str(i)] = values\n\n             \n        return   pd.concat([X,out],axis=1)\n            \n    \n    def _get_transformer_name(self):\n        return str(self.transformer.__class__).split('.')[-1][0:-2]\n\n\nclass IdentityTransformer:\n    '''Duummy_tansformer as a filler'''\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        \n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n        return  X\n      \n    \n    def transform( self, X):\n       \n        return  X    \n\nclass SuppressControls( BaseEstimator, TransformerMixin ):\n    #Class constructor method that takes in a list of values as its argument\n    def __init__(self ):\n        pass\n    def fit(self, X, y = None):\n        return self\n    #Return self nothing else to do here\n    def fit_transform( self, X, y = None  ):\n        \n      \n        \n        return   X.loc[X['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1) \n    \n    def transform( self, X):\n       \n       \n        return    X.loc[X['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1)\n\n","651c412a":"exp_name=\"torch_DAE_rankgauss_test\"","0287135c":"import sys\nsys.path.append('..\/..\/input\/iterative-stratification\/iterative-stratification-master')\nsys.path.append('..\/..')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","33d9c16e":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.pipeline import make_pipeline,make_union\n","69678829":"\nmap_controls = CatIntMapper('cp_type',{'ctl_vehicle': 0, 'trt_cp': 1})    \n\nmap_dose = CatIntMapper('cp_dose',{'D1': 1, 'D2': 0})    \nmap_time = CatIntMapper('cp_time',{24: 0, 48: 1, 72: 2})    \n","05c5993d":"train = pd.read_csv(f'{input_directory}\/train_features.csv')","33dd8450":"Rankg_g_tansform =  NamedOutTWrapper( QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),columns= GENES+CELLS,inplace=True)","af853a40":"PCA_g_tansform =  NamedOutTWrapper(PCA(20),columns= GENES,prefix ='_g' )","000386c1":"PCA_c_tansform =  NamedOutTWrapper(PCA(20),columns= CELLS,prefix ='_c' )","244b9899":"#transformers_list=[map_controls,map_dose,map_time,PCA_g_tansform,PCA_c_tansform,Rankg_g_tansform]","80bcf417":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass ColumnDropper( BaseEstimator, TransformerMixin ):\n    #Class Constructor \n    def __init__( self, cols ):\n        self.cols=cols\n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def transform( self, X, y = None ):\n\n        return X.drop(self.cols,axis=1)\n","b0b86df6":"\nCatDropper =ColumnDropper(cols=['cp_type','cp_time','cp_dose'])","8d90e64f":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size,hidden_size2,drop_rate1=0.2,drop_rate2=0.5,drop_rate3=0.8):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        #self.dropout1 = nn.Dropout(drop_rate1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout2 = nn.Dropout(drop_rate2)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size2))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size2)\n        #self.dropout3 = nn.Dropout(drop_rate2)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size2, hidden_size))\n        \n      #  self.batch_norm4 = nn.BatchNorm1d(hidden_size)\n      #  self.dropout4 = nn.Dropout(drop_rate3)\n        self.dense4 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n\n        \n    def forward(self, x,mode='DAE'):\n      #  x = self.batch_norm1(x)\n       # x1 = self.dropout1(x1)\n        x1 = F.relu(self.dense1(x))\n        \n            \n        x2 = self.batch_norm2(x1)\n      #  x = self.dropout2(x)\n        x2 = F.relu(self.dense2(x2))\n        \n        x3 = self.batch_norm3(x2)\n      \n        x3 = F.relu(self.dense3(x3))\n        \n        out = self.dense4(x3)\n        \n        if mode == 'DAE':\n            return out\n        else:\n            return x1,x2,x3\n    \n#     def forwardh2(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x\n    \n#     def forwardh3(self, x):\n#       #  x = self.batch_norm1(x)\n#        # x1 = self.dropout1(x1)\n#         x = F.relu(self.dense1(x))\n        \n#         return x","eeb52213":"def initialize_from_past_model(model,past_model_file, freeze_first_layer=False):\n\n   # pretrained_dict = torch.load('FOLD0_.pth')\n    pretrained_dict = torch.load(past_model_file)\n    model_dict = model.state_dict()\n\n    pretrained_dict['dense3.bias']=pretrained_dict['dense3.bias'][:206]\n\n    pretrained_dict['dense3.weight_g']=pretrained_dict['dense3.weight_g'][:206]\n\n    pretrained_dict['dense3.weight_v']=pretrained_dict['dense3.weight_v'][:206]\n\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n    # 2. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict) \n    # 3. load the new state dict\n    model.load_state_dict(pretrained_dict)\n    \n    \n    if freeze_first_layer:\n        for name,layer in model.named_parameters():\n            if '1' in name:\n                 layer.requires_grad = False\n        \n    ","b3480351":"transformers_list=[SuppressControls(),Rankg_g_tansform,ColumnDropper(cols=['cp_time','cp_dose'])]","d1343273":"\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 200\nBATCH_SIZE = 1024 #640\nLEARNING_RATE = 2e-3\nWEIGHT_DECAY = 1e-8\nNFOLDS = 10\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nGAMMA=0.5\n\n#num_features=len(feature_cols)\n#num_targets=len(target_cols)\nhidden_size=1100\nhidden_size2=1300\n\nPATIENCE = 5\nFACTOR = 0.5\nTHRESHOLD = 1e-4","ce95a560":"#exp_name =  \"test_DAE_all_together\"\nexp_name = 'test_DAE_0.3_all_together'","0f4b2d4b":"def run_training(X_train,X_valid,X_test,fold, seed,verbose=False,**kwargs):\n    \n    seed_everything(seed)\n    \n   \n    \n    train_dataset = MoASwapDataset(X_train, 0.35)\n    valid_dataset = MoASwapDataset(X_valid, 0.35)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features= X_train.shape[1] ,\n        num_targets=  X_train.shape[1],\n        hidden_size=hidden_size,hidden_size2=hidden_size2,**kwargs\n    )\n    \n    model.to(DEVICE)\n    \n    #initialize_from_past_model(model,f\"..\/results\/FOLD{fold}_original_torch_moa_5_folds_AUX.pth\")#,freeze_first_layer=True)\n    #optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    \n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e2, \n                                          #max_lr=5e-4, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=PATIENCE,factor=FACTOR,\n                                                threshold=THRESHOLD,verbose=True)\n    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=10,factor=0.5,threshold=1e2)\n    #scheduler = optim.lr_scheduler.CyclicLR(optimizer,1e-4,5e-3,scale_mode='exp_range',gamma=FACTOR)\n   # loss_val = nn.BCEWithLogitsLoss()\n    \n    #loss_tr = SmoothBCEwLogits(smoothing =0.001)\n    \n    \n    loss_tr = nn.MSELoss()\n    loss_val = nn.MSELoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    #todo el guardado de los resultados se puede mover a kfold que si tiene info de los indices\n    #oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    \n    \n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n        if verbose & (epoch%10==0):\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model,scheduler, loss_val, validloader, DEVICE)\n        if verbose & (epoch%10==0):\n            print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof = valid_preds\n        \n            #if epoch > 0.7*EPOCHS:\n            torch.save(model.state_dict(), f\"FOLD{fold}_{exp_name}.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n   \n   # testdataset = Test(X_test)\n   # testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = Model(\n#          num_features= X_train.shape[1] ,\n#         num_targets=  y_train.shape[1],\n#         hidden_size=hidden_size,**kwargs\n#     )\n    \n#     model.load_state_dict(torch.load(f\"..\/results\/FOLD{fold}_{exp_name}.pth\"))\n    #model.to(DEVICE)\n    \n    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n   # predictions = inference_fn(model, testloader, DEVICE)\n    del model\n    torch.cuda.empty_cache()\n    return #oof, _#predictions\n","8a45c9d9":"def run_k_fold(folds,target_cols,test,transformers_list,NFOLDS, seed,verbose=False,**kwargs):\n    \n    \n    train = folds\n    test_ = test\n    \n    #oof = np.zeros((len(folds), len(target_cols)))\n    oof = train[target_cols].copy()\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    #print(test_.head())\n    for fold in range(NFOLDS):\n        \n        #trn_idx = train[train['kfold'] != fold].reset_index().index\n        #val_idx = train[train['kfold'] == fold].reset_index().index\n    \n        train_df = train[train['kfold'] != fold]#.reset_index(drop=True)\n        valid_df = train[train['kfold'] == fold]#.reset_index(drop=True)\n        \n       # print(len(train_df))\n        #print(len(valid_df))\n        \n        feature_cols = [col  for col in train_df.columns if not (col in target_cols+['kfold'])]\n        \n        #print(feature_cols)\n        \n        pipeline_val = make_pipeline(*transformers_list)\n        \n        X_train, y_train  = train_df[feature_cols], train_df[target_cols]\n        X_valid, y_valid =  valid_df[feature_cols], valid_df[target_cols].values\n        \n      \n       \n        pipeline_val.fit(X_train,y_train)\n        \n        ###############################\n        #### SAVE\/LOAD PREPROCESSING #######\n        dump(pipeline_val,'pipeline_val1_fold%i.joblib'%fold)\n        pipeline_val = load('pipeline_val1_fold%i.joblib'%fold)\n        print('SAVE\/LOAD PIPELINE_VAL_FOLD%i'%fold)\n        print(pipeline_val)\n        ###############################\n        \n        X_train = pipeline_val.transform(X_train)\n        \n        feature_cols = [col  for col in X_train.columns if not (col in target_cols+['kfold'])]\n        \n        X_train = X_train.values\n        \n        \n        X_valid = pipeline_val.transform(X_valid)\n        valid_index = X_valid.index\n        X_valid = X_valid.values\n        \n        y_train = y_train.values\n        \n        \n        X_test = test_[feature_cols].values\n            \n        #oof_, pred_ = \n        run_training(X_train,X_valid,X_test,fold, seed,verbose,**kwargs)\n        \n        #oof.loc[valid_index] = oof_\n        \n        #predictions += pred_ \/ NFOLDS\n        \n        if fold>=6:\n            break #ONLY SEVEN FOLDS\n        \n    return #oof, predictions","cd2bb759":"params ={}","8dd65908":"# Training","7d92d494":"# Averaging on multiple SEEDS\n\n#SEED = [0,12347,565657,123123,78591]\nSEED = [0]\ntrain,target_cols = train_short_form_loader(f'{input_directory}\/train_features.csv',f'{input_directory}\/train_targets_scored.csv')\ntest = test_short_form_loader(f\"{input_directory}\/test_features.csv\")\n\ntrain = pd.concat([train,test])\ntrain[target_cols]= train[target_cols].fillna(0)\npipeline_test = make_pipeline(*transformers_list)\npipeline_test.fit(train)\n\n###############################\n#### SAVE\/LOAD PREPROCESSING #######\ndump(pipeline_test,'pipeline_test1.joblib')\npipeline_test = load('pipeline_test1.joblib')\nprint('SAVE\/LOAD PIPELINE_TEST')\nprint(pipeline_test)\n###############################\n\ntest = pipeline_test.transform(test)\n    \n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n   \n    folds = multifold_indexer(train,target_cols,n_splits=NFOLDS)\n    #oof_, predictions_ =\n    run_k_fold(folds,target_cols,test,transformers_list,NFOLDS, seed,verbose=True,**params)\n   # oof += oof_ \/ len(SEED)\n   # predictions += predictions_ \/ len(SEED)\n    break\n#train[target_cols] = oof\ntest[target_cols] = predictions\n","a48b7d0e":"# CV folds","417881ba":" we define a dataset that swaps instances of a column by another instance of the same column ","a9056a4f":"# Dataset Classes","b6e9d51c":"# Transformers definition","5a0c7612":"we have two forward modes one for training the layers and another to generate the layer activations to then use as features","7b375654":"An implementation of Swap denoising autoencoder as used in the porto seguro's winning submit. In a futuure kernel i will set activation layers as features to obtain a prediction.","09f66213":"# Model definition","6c0e6986":"# Pipeline"}}