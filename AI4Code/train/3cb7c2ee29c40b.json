{"cell_type":{"0b251ab2":"code","29cdabe6":"code","d06b0691":"code","c3fbfcd1":"code","6b75231d":"code","e2245014":"code","dcd30fea":"code","58f4cf5a":"code","5d59c325":"code","7376978e":"code","af46c25c":"code","bfff3364":"code","40c855bb":"code","4e9310b8":"code","74ad0146":"code","4fed75c1":"code","d15d0ce1":"code","7e1568c3":"markdown"},"source":{"0b251ab2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29cdabe6":"import numpy as np\nimport pandas as pd\nimport math\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torch.autograd import Variable\n\nimport matplotlib.pyplot as plt","d06b0691":"device = torch.device('cuda')","c3fbfcd1":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","6b75231d":"X_train, X_val, Y_train, Y_val = train_test_split(train.iloc[:, train.columns!='label'], train['label'], test_size = 0.2, random_state=42)","e2245014":"train_ds = torch.utils.data.TensorDataset(torch.Tensor(X_train.values).view(33600,1,28,28),torch.Tensor(Y_train.values).type(torch.LongTensor))\neval_ds = torch.utils.data.TensorDataset(torch.Tensor(X_val.values).view(8400,1,28,28),torch.Tensor(Y_val.values).type(torch.LongTensor))\ntest_ds = torch.utils.data.TensorDataset(torch.Tensor(test.values).view(28000,1,28,28))","dcd30fea":"trainloader = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True)\nevalloader = torch.utils.data.DataLoader(eval_ds, batch_size=128, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_ds, batch_size=128)","58f4cf5a":"len(testloader)","5d59c325":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\n\nprint(images[1].shape)\nprint(labels)","7376978e":"def imshow(img):\n    plt.imshow(np.transpose(img.numpy(),(1,2,0)),cmap='binary')","af46c25c":"imshow(images[3])","bfff3364":"def evaluation(dataloader):\n    total, correct = 0, 0\n    for data in dataloader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = net(inputs)\n        _, pred = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (pred == labels).sum().item()\n    return 100 * correct \/ total","40c855bb":"class LeNet(nn.Module):\n    def __init__(self): \n        super(LeNet, self).__init__()\n        self.cnn_model = nn.Sequential(\n            nn.Conv2d(1, 6, 5),         # (N, 1, 28, 28) -> (N,  6, 24, 24)\n            nn.BatchNorm2d(6),\n            nn.ReLU(),\n            nn.Dropout(p = 0.5),\n            nn.AvgPool2d(2, stride=2),  # (N, 6, 24, 24) -> (N,  6, 12, 12)\n            nn.Conv2d(6, 16, 5),        # (N, 6, 12, 12) -> (N, 16, 8, 8)  \n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.Dropout(p = 0.5),\n            nn.AvgPool2d(2, stride=2)   # (N,16, 8, 8) -> (N, 16, 4, 4)\n        )\n        self.fc_model = nn.Sequential(\n            nn.Linear(256,120),         # (N, 256) -> (N, 120)\n            nn.BatchNorm1d(120),\n            nn.ReLU(),\n            nn.Dropout(p = 0.5),\n            nn.Linear(120,84),          # (N, 120) -> (N, 84)\n            nn.BatchNorm1d(84),\n            nn.ReLU(),\n            nn.Dropout(p = 0.5),\n            nn.Linear(84,10)            # (N, 84)  -> (N, 10)\n        )\n        \n    def forward(self, x):\n        x = self.cnn_model(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_model(x)\n        return x","4e9310b8":"class Net(nn.Module):    \n    def __init__(self):\n        super(Net, self).__init__()\n          \n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.5),\n            nn.Linear(64 * 7 * 7, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 10),\n        )\n          \n        for m in self.features.children():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        \n        for m in self.classifier.children():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform(m.weight)\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x ","74ad0146":"%%time\nnet = LeNet().to(device)\nloss_fn = nn.CrossEntropyLoss().cuda()\nopt = torch.optim.Adam(net.parameters(),weight_decay=0.003)\n#exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=7, gamma=0.1)\nloss_arr = []\nloss_epoch_arr = []\nmax_epochs = 2500\nfor epoch in range(max_epochs):\n    #exp_lr_scheduler.step()\n    for i,data in enumerate(trainloader,0):\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        opt.zero_grad()\n        outputs = net(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        loss_arr.append(loss.item())\n    loss_epoch_arr.append(loss.item())\n    print(f'Epoch: {epoch}\/{max_epochs}, Test acc: {evaluation(evalloader)}, Train acc: {evaluation(trainloader)}')\n    \nplt.plot(loss_epoch_arr)\nplt.show()","4fed75c1":"def prediciton(data_loader):\n    test_pred = torch.LongTensor().to(device)\n    for data in testloader:\n        temp = data[0].to(device)\n        output = net(temp)\n        pred = output.data.max(1, keepdim=True)[1]\n        test_pred = torch.cat((test_pred, pred), dim=0)\n    return test_pred","d15d0ce1":"test_pred = prediciton(testloader)\nout_df = pd.DataFrame(np.c_[np.arange(1, len(test_ds)+1)[:,None], test_pred.cpu().numpy()],columns=['ImageId', 'Label'])\nout_df.to_csv('submission.csv', index=False)","7e1568c3":"# CNN"}}