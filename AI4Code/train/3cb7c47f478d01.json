{"cell_type":{"a2a419c9":"code","e18f8a59":"code","1f5c3803":"code","8c590995":"code","c3225244":"code","8054944d":"code","a0424e77":"code","c242ceb9":"code","702d37f0":"code","5e50b5a3":"code","16da5a22":"code","4164cd3f":"code","b47c1f1d":"code","bf161293":"code","987a115f":"code","239d0f23":"code","2ed21ed7":"code","0056798b":"code","9c05a712":"code","9a21797d":"code","ff2ca167":"code","6aba648a":"code","ce36697f":"code","5bd6aa63":"code","55f41746":"code","5a98ae29":"code","88eb4903":"code","7bcbfd93":"code","80adc3d5":"code","05624446":"markdown","bb13863c":"markdown","f8dc28d3":"markdown","506dc161":"markdown","7fd1b378":"markdown","876a21f3":"markdown","3e5251bc":"markdown","898096cc":"markdown","bd863500":"markdown","ea8e2a77":"markdown","b5ff1992":"markdown","93b56040":"markdown","9b8c0628":"markdown"},"source":{"a2a419c9":"# Importing the required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\nimport os\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nimport tensorflow as tf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","e18f8a59":"# Discovering the files available\npath = \"..\/input\/Kannada-MNIST\/\"\nos.listdir(path)","1f5c3803":"# Importing the data\ntrain = pd.read_csv(path+\"train.csv\")\ntest = pd.read_csv(path+\"test.csv\")\nsubmission = pd.read_csv(path+\"sample_submission.csv\")","8c590995":"# Check the shape of our data\ntrain.shape, test.shape","c3225244":"# Check the data samples\ntrain.head(3)","8054944d":"# Check for any missing values\ntrain.isnull().sum().sum(), test.isnull().sum().sum()","a0424e77":"# Let us understand our target variable's distribution\nsns.countplot(train[\"label\"])\nplt.show()","c242ceb9":"# Looking at individual samples\ndef visualize_image(ix=0):\n    plt.imshow(train.iloc[ix, 1:].values.reshape(28, 28, 1)[:, :, 0])\n    plt.title(\"Class = \" + str(train[\"label\"].loc[ix]))\n    plt.show()\n\nvisualize_image(1)","702d37f0":"# Setting the seed\nseed = 10","5e50b5a3":"# Splitting target and features\ntarget = train[\"label\"]\nfeatures = train.drop(\"label\", 1)","16da5a22":"from sklearn.decomposition import PCA\nfrom umap import UMAP\n\npca = PCA(random_state=seed, n_components=50)\n%time pca_features = pca.fit_transform(features)\n\numap = UMAP(n_neighbors=10, metric=\"cosine\", random_state=seed, n_epochs=300)\n%time umap_features = umap.fit_transform(pca_features)","4164cd3f":"plt.figure(figsize=(10, 7))\nsns.scatterplot(umap_features[:, 0], umap_features[:, 1], hue=target, palette=sns.color_palette(\"Set1\", target.nunique()))\nplt.show()","b47c1f1d":"# Splitting the data into train, val and test sets\n\nx, x_val, y, y_val = train_test_split(umap_features, target, random_state=seed, stratify=target, test_size=0.2)\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=seed, stratify=y, test_size=0.2)","bf161293":"# Defining the classifiers\nclassifiers = {\"LR\": LogisticRegression(random_state=seed), \"RF\": RandomForestClassifier(random_state=seed), \"KNN\": KNeighborsClassifier(n_neighbors=20, n_jobs=-1)}","987a115f":"# Building the models\nmodels = {}\nval_preds = {}\ntest_preds = {}\nfor k, v in classifiers.items():\n    print(f\"{k}\")\n    %time models[k] = v.fit(x_train, y_train)\n    val_preds[k] = models[k].predict(x_val)\n    test_preds[k] = models[k].predict(x_test)\n    print(f\"Validation Accuracy: {np.round(models[k].score(x_val, y_val), 4)} | Test Accuracy: {np.round(models[k].score(x_test, y_test), 4)}\")","239d0f23":"# Analyzing the confusion matrix\nplt.figure(figsize=(10, 7))\nsns.heatmap(confusion_matrix(y_test, test_preds[\"KNN\"]), annot=True, fmt='d')\nplt.show()","2ed21ed7":"# Plotting the decision boundary\nfrom mlxtend.plotting import plot_decision_regions\n\nplt.figure(figsize=(10, 7))\nplot_decision_regions(x_test, y_test.values, clf=models[\"KNN\"])\nplt.show()","0056798b":"# Scaling the pixel values to a range between 0 and 1\nX = features \/ 255\npayload = test.drop(\"id\", 1) \/ 255","9c05a712":"# Reshaping the data into a 3 dimensional array\nX = X.values.reshape(-1, 28, 28, 1)\npayload = payload.values.reshape(-1, 28, 28, 1)\n\n# Encoding the target variable. This is because we will be using softmax in the output layer and it outputs probabilities for each class\nY = tf.keras.utils.to_categorical(target, num_classes = 10)","9a21797d":"# Splitting the data into train, val and test sets\n\nx, x_val, y, y_val = train_test_split(X, target, random_state=seed, stratify=target, test_size=0.2)\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=seed, stratify=y, test_size=0.2)","ff2ca167":"# Data augmentation\ndata_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n    featurewise_center=False,  # set input mean to 0 over the dataset\n    samplewise_center=False,  # set each sample mean to 0\n    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n    samplewise_std_normalization=False,  # divide each input by its std\n    zca_whitening=False,  # apply ZCA whitening\n    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n    zoom_range = 0.1, # Randomly zoom image \n    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n    horizontal_flip=False,  # randomly flip images\n    vertical_flip=False\n)","6aba648a":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation = tf.nn.relu, input_shape = (28, 28, 1)),\n    tf.keras.layers.MaxPool2D(2, 2),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation = tf.nn.softmax)\n])","ce36697f":"optimizer=tf.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)","5bd6aa63":"model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['acc'])","55f41746":"history = model.fit_generator(data_gen.flow(x_train, y_train, batch_size=64), epochs=5, validation_data=(x_test, y_test), verbose=1, steps_per_epoch=x_train.shape[0]\/\/64)","5a98ae29":"submission[\"label\"] = np.argmax(model.predict(payload), 1)","88eb4903":"submission.to_csv(\"submission.csv\", index=False)","7bcbfd93":"submission.head(3)","80adc3d5":"plt.imshow(test.iloc[1, 1:].values.reshape(28, 28, 1)[:, :, 0])","05624446":"## Method 1: Simple Classifiers\nWe will build some simple classifiers using linear and tree based models and then I will let you know why we need to go for convolutions than sticking with simple models.","bb13863c":"## Method 2: Convolutional Neural Network (CNN)\n\nFor machines an image is an array of pixels stacked as X dimension x Y dimension x No.of channels. Example: 32x32x1 (1 - grayscale, 3 - RGB). If we are working with a 28 x 28 pixels then we would have 784 columns of information about the image. Consider the same for a 1024 x 1024 size image. As the image quality increases the number of dimensions as well. It would be difficult to always use dimensionaltiy reduction techniques to handle huge dimensions as there is a greater chance of loosing valuable information and also when dealing with the fully connected neural network the number of nodes increases depending on the number of neurons in each layer. And that's where CNN comes into play. Also another reason why we move to CNNs are because they learn on their own and requires less feature engineering when compared to traditional classifiers.\n\n![image.png](attachment:image.png)","f8dc28d3":"Let us reduce the dimension from 784 to 2 so that we can visualize our data better.","506dc161":"## Understanding our data\nIn our scenario the data is readily available but that is not the case for all the Machine Learning projects. You need to typically gather relevant data from multiple sources. If needed you might want to generate your data as well.","7fd1b378":"From the confusion matrix above we can infer that our model is finding it hard to classify classes 0, 6, 7 and 9. This might be due to noise or poor feature selection.","876a21f3":"We will be using the Keras API in Tensorflow to create a CNN model. You can also create the same with the help of core TensorFlow, PyTorch and other deep learning libraries as well. Without any further talk let us jump right into the code.","3e5251bc":"CNN consists of **convolution** and **pooling** layers stacked on top of a fully connected neural network. CNN basically reduces the dimensions by retaining only the key features in the image by performing convolution and pooling.\n\n**Important:** For a detailed explanation of CNN I would engourge you to read <a href=\"https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">this<\/a> blog by Sumit Saha. This clearly explains how CNNs work. After reading the blog don't forget to revisit this kernel for the practical application of the techniques learnt.","898096cc":"This is a balanced dataset, which means that we have an equal number of data samples for all the target classes. But in the case of an unbalanced dataset, we must handle that with the help of different sampling techniques as it could affect the model's performance.\n\nFor visualizing the image we need to convert it into a 3 dimensional data where the third dimension denotes the color channel. 1 - Grayscale, 3 - RGB ","bd863500":"## Understanding the objective\n\nLet us take some time to understand what we are trying to achieve in this project. Most of the Machine Learning projects fail just because of not setting a clear objective. Always remember, \n> One who knows where to go, is the one who moves ahead\n\nIn our case, our objective is to build a classification model which inputs an image of handwritten kannada digit and outputs the predicted number in that image.\n\n**Evaluation Metric:** Accuracy \n\n**Metric Definition:** The percentage of correct prediction (Total correct predictions \/ Total samples)","ea8e2a77":"The data contains 785 columns, one for each pixel and a label column (ground-truth) for every data point. To make things simple each data point contains information about 28 x 28 pixel image + a label column.","b5ff1992":"## Introduction\n\nThe goal of this notebook is to introduce you the end-to-end process involved in a Image Classification project. We will be working on the Kannada version of <a href=\"https:\/\/www.kaggle.com\/c\/digit-recognizer\/\">MNIST<\/a> data unlike using the Arabic Numerals. \n\nKannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script.\n\nI will be answering the following questions through code in this kernel:\n- How to  define the objective?\n- How to understand our data?\n- How to prepare the data?\n- How to work with simple models?\n- What are convolutions?\n- How to build one?\n- How to evaluate our model?\n- Why you need to give an upvote for this kernel?\n\n==> If you want to travel along please fork this notebook and execute the cells when needed.","93b56040":"Looks like KNN classifier fits our data much better when compared to others.","9b8c0628":"Look's like there are no missing values in the dataset. Hurray!"}}