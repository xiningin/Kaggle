{"cell_type":{"a55ba36d":"code","ff867e2e":"code","fc5386c9":"code","9ddc92c0":"code","97cf169f":"code","b26dbb7c":"code","1ede2735":"code","e3d78b48":"code","12b54360":"code","9c9470f3":"code","ecb32174":"code","c4b63fec":"code","f175507d":"code","42cd031c":"code","416c0ef4":"code","35c26486":"code","f6c670a2":"code","54824465":"code","552f3454":"code","49b703b5":"code","94ba697d":"code","02dece20":"code","4f41d283":"code","d093d556":"code","3cc1e409":"code","1eb9b35c":"code","8eeace3d":"code","e99bb173":"code","240b4f16":"code","710210f9":"code","6802f6d0":"code","b43894e9":"code","30a393dc":"code","51b0a0a6":"code","3b9ea7ae":"code","5cfb878a":"code","d64fe512":"code","f2251c21":"code","73266b58":"code","b13e578f":"code","cc841b22":"code","fa60a07e":"code","3c83c428":"code","e4b20793":"code","dfd7be39":"code","b19c58fe":"code","cb62ed49":"code","e7031ed8":"code","9a596698":"code","78f526d3":"code","3fbc16d8":"code","9c00a4cf":"code","e193f012":"code","6a8becd6":"code","82910e62":"code","a78df8ee":"code","940013b1":"code","8019622c":"code","d48a4984":"code","51583038":"code","6a460164":"code","69db39b4":"code","d4e6db40":"code","5e6ee0ea":"code","868c63aa":"code","78ba392c":"code","d401ed8f":"code","a3ef4377":"code","668d321a":"markdown","f29eab87":"markdown","36e7d957":"markdown","3d280b59":"markdown","e91d3e29":"markdown","c5e98541":"markdown","6ce9ccd3":"markdown","43e89ce5":"markdown","bebedce8":"markdown","7da3b22d":"markdown","a9dd6e88":"markdown","a4f10462":"markdown","43cb411c":"markdown","9bef8320":"markdown","c1c2db54":"markdown","e8ec3570":"markdown","2aec081a":"markdown","4362a50b":"markdown","578647a4":"markdown","c2fd754f":"markdown","fd8ca058":"markdown","c334def0":"markdown","0bbc2d43":"markdown","be62bf1d":"markdown","cc6e1edb":"markdown","718bcda8":"markdown","63835e1e":"markdown","443df38a":"markdown","7d718290":"markdown","75938a95":"markdown","168d54da":"markdown","8cd77ec8":"markdown","ad8bc5fe":"markdown","d1fecbbe":"markdown","70335dff":"markdown","d361a98f":"markdown","36bda61a":"markdown","97fc4b3f":"markdown","affd17b1":"markdown","f4fa2f68":"markdown","aaef8dfd":"markdown","910823ea":"markdown","ac1305af":"markdown","b53a3252":"markdown","14055416":"markdown","e0844915":"markdown","12a0d09e":"markdown","3c4c6df3":"markdown","1d74fcc6":"markdown","bd7a0c07":"markdown","c090fdf6":"markdown","63f90e9c":"markdown","03b13e09":"markdown","8b01c75d":"markdown","aa5795b3":"markdown","57d3267c":"markdown","23e02af6":"markdown","b25e8ec7":"markdown","1b3d361f":"markdown","34b67e2d":"markdown","387bc13c":"markdown","1cfacc97":"markdown","0f3148e6":"markdown","592abdfb":"markdown"},"source":{"a55ba36d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff867e2e":"import numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\npd.set_option('display.max_rows',None)\npd.set_option('display.max_column',None)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fc5386c9":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","9ddc92c0":"# Show the missing values in train dataset\ntrain.isnull().sum()","97cf169f":"# Show the missing values in test dataset\ntest.isnull().sum()","b26dbb7c":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# Train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# Test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","1ede2735":"# Show which is the most frequent value in Embarked\ntrain['Embarked'].value_counts()","e3d78b48":"# Fill missing values by the most frequent value\ntrain = train.replace({\n    'Embarked' : {np.nan : 'S'}})","12b54360":"# Get information about columns of test dataset\ntest.info()","9c9470f3":"# Show records of the missing values in Fare of test dataset\ntest[test.Fare.isna()]","ecb32174":"# Define the variable that contains the records of Pclass 3 \npclass_3_rows = test[test['Pclass']==3]\n\n# Calculate the mean of Fear for pclass 3\nmean = np.mean(pclass_3_rows['Fare'])\nmean","c4b63fec":"# Fill the missing values of Fare by the mean of pclass 3\ntest = test.replace({\n    'Fare' : {np.nan : mean}})","f175507d":"# Show the left missing values in test dataset\ntest.isnull().sum()","42cd031c":"# Group data by pclass and calculate the mean of age per class\nmean_age =train.groupby('Pclass').agg({'Age': 'mean'})\nmean_age","416c0ef4":"# Function to impute age\ndef impute_age(age_pclass): # passing age_pclass as ['Age', 'Pclass']\n    \n    # Passing age_pclass[0] which is 'Age' to variable 'Age'\n    Age = age_pclass[0]\n    \n    # Passing age_pclass[2] which is 'Pclass' to variable 'Pclass'\n    Pclass = age_pclass[1]\n    \n    #applying condition based on the Age and filling the missing data respectively \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 38\n\n        elif Pclass == 2:\n            return 30\n\n        else:\n            return 25\n\n    else:\n        return Age","35c26486":"# Replace for Train data in Pclass\n# (for train) grab age and apply the impute_age, our custom function \nfor i, row in train.iterrows():\n    # Get the values of pclass in train dataset\n    pclass_train = row['Pclass']\n    \n    # Get the values of Age in train dataset\n    age_train = row['Age']\n    \n    # Combine the values of age and pclass in the list\n    age_pclass = [age_train,pclass_train]\n    \n    # Replace the missing values in age by impute_age function\n    train = train.replace({\n    'Age' : {np.nan : impute_age(age_pclass)}})\n    \n# Replace for Test data in Pclass\n# (for test) grab age and apply the impute_age, our custom function \nfor index, rows in test.iterrows():\n    # Get the values of pclass in train dataset\n    pclass_test = rows['Pclass']\n    \n    # Get the values of Age in train dataset\n    age_test = rows['Age']\n    \n    # Combine the values of age and pclass in the list\n    age_pclass_test = [age_test, pclass_test]\n    \n    # Replace the missing values in age by impute_age function\n    test = test.replace({\n    'Age' : {np.nan : impute_age(age_pclass_test)}})\n","f6c670a2":"# Show the left missing values in train dataset\ntrain.isnull().sum()","54824465":"# Show the left missing values in test dataset\ntest.isnull().sum()","552f3454":"# If the cabin is not null then it is 1 otherwise it is 0 in both datasets\ntrain['Cabin']=train['Cabin'].apply(lambda x:1 if pd.notnull(x) else 0)\ntest['Cabin']=test['Cabin'].apply(lambda x:1 if pd.notnull(x) else 0)","49b703b5":"train.describe()","94ba697d":"# Plot the histogram for age per survived class\ntrain.hist(column=\"Age\",by=\"Survived\",sharey=True,figsize=(15,7), bins=15,color='#b1cbbb')\n\n# Set centered title for the plots\nplt.suptitle('Age Density Distribution grouped by Survived');","02dece20":"train['Age'].mode()","4f41d283":"# Define the dataframe which contains the selected columns\ndf_plot = train[['Pclass','Sex','Parch','Cabin','Embarked','Survived']]\n\nfor i,column in enumerate(df_plot):\n    plt.subplots(figsize = (8,4))\n    ax = sns.countplot(x = df_plot[column], hue=\"Survived\",  data = df_plot,palette=\"husl\")\n    plt.show()","d093d556":"# Doing one hot encode for sex and embarked in both datasets\ntrain_copy = pd.get_dummies(train, columns=[\"Sex\", \"Embarked\"],drop_first=True)\ntest_copy = pd.get_dummies(test, columns=[\"Sex\", \"Embarked\"],drop_first=True)","3cc1e409":"# The selected features in the list will are useless for the target\nfeatures_drop = ['PassengerId','Name', 'Ticket', 'Survived']","1eb9b35c":"# Choose the columns which are not selected to drop it\nselected_features = [column for i,column in enumerate(train_copy.columns) if column not in features_drop]\nselected_features","8eeace3d":"# Define the X_train by using the selected columns\nX_train = train_copy[selected_features]\n\n# Define the y_train or target\ny_train = train_copy['Survived']\n\n# Define the X_test by using the selected columns\nX_test = test_copy[selected_features]","e99bb173":"base = y_train.value_counts()\/len(y_train)\nbase[0] # The most frequent strategy for baseline","240b4f16":"X_test.head()","710210f9":"# Do Scaling for data\nscaler = StandardScaler()\nXs = scaler.fit_transform(X_train)\nXs_test= scaler.transform(X_test)","6802f6d0":"#tuning\ngrid_param ={'max_depth': np.linspace(1,10,num=10,dtype=int),\n             'criterion':['gini', 'entropy'],\n             'max_features': [0.6,0.8,0.9],\n             'min_samples_leaf': [2,5],\n             'n_estimators': [200,250,300]}","b43894e9":"# Build Random Forest model \nrf = RandomForestClassifier(random_state=1)\n\ngs_rf = GridSearchCV(rf, \n                  grid_param, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1,\n                  n_jobs =5)\ngs_rf.fit(X_train, y_train)","30a393dc":"# Getting the best parameters of estimator\ngs_rf.best_params_","51b0a0a6":"# Getting the best cross validation of estimator\nRF=gs_rf.best_score_\nRF","3b9ea7ae":"# Build RF with best parameters we've got from GridSearchCV\nrf2= RandomForestClassifier(criterion= 'entropy',\n                           max_depth= 7,\n                           max_features= 0.8,\n                           min_samples_leaf= 5,\n                           n_estimators= 250,\n                           random_state=1)\nrf2.fit(X_train,y_train)","5cfb878a":"# Perform 5-fold cross validation\nscores = cross_val_score(rf2,X_train, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","d64fe512":"# Get best K on train data\nscores_lst = []\nfor k_ in range(1,20):\n    # Build KNN regression model\n    knn_test = KNeighborsClassifier(n_neighbors=k_)\n    scores_lst.append((k_, np.mean(cross_val_score(knn_test, Xs, y_train, cv=5))))","f2251c21":"scores_lst","73266b58":"# Build KNN regression model with best K\nknn_ = KNeighborsClassifier(n_neighbors=11)\nknn_.fit(Xs, y_train)","b13e578f":"knn_.score(Xs, y_train)","cc841b22":"# Perform 5-fold cross validation\nscores = cross_val_score(knn_, Xs, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","fa60a07e":"# Build KNN regression model\nknn = KNeighborsClassifier()","3c83c428":"#tuning\ngrid={'bootstrap': [False,True],\n       'max_features': [0.6,0.7,0.9],\n       'max_samples': [0.6,0.7,0.9],\n       'n_estimators': [100,200,250]}","e4b20793":"# Apply GridSearchCV on KNN with bagging \ngs2 = GridSearchCV( BaggingClassifier(base_estimator=KNeighborsClassifier(),random_state=1), \n                  grid, \n                  cv=5,            \n                  verbose=1,\n                  n_jobs =5)\ngs2.fit(Xs, y_train)","dfd7be39":"# Getting the best parameters of estimator\ngs2.best_params_","b19c58fe":"# Getting the best cross validation of estimator\nbaggKNN = gs2.best_score_\nbaggKNN","cb62ed49":"knn_bag = BaggingClassifier(base_estimator=knn,\n                           bootstrap= False,\n                           max_features= 0.7,\n                           max_samples= 0.9,\n                           n_estimators= 100,\n                           random_state=1)","e7031ed8":"# Perform 5-fold cross validation\nscores = cross_val_score(knn_bag, Xs, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores of Bagging:\", scores.mean().round(3))","9a596698":"#tuning\ngs3 = GridSearchCV(BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=1), \n                  grid, \n                  cv=5,                  \n                  verbose=1,\n                  n_jobs =5)\n\ngs3.fit(X_train, y_train)","78f526d3":"# Getting the best cross validation of estimator\nbaggDT=gs3.best_score_\nbaggDT","3fbc16d8":"gs3.best_params_","9c00a4cf":"dt = DecisionTreeClassifier()\ndt_bagging = BaggingClassifier(base_estimator=dt,\n                          bootstrap= False,\n                          max_features= 0.7,\n                          max_samples= 0.7,\n                          n_estimators= 200,\n                          random_state=1) # we can use  'max_features': [0.6, 0.8, 1],'max_samples': [1, 0.5, 0.7, 0.8]                     ","e193f012":"# Perform 5-fold cross validation\nscores = cross_val_score(dt_bagging,X_train, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","6a8becd6":"logreg = LogisticRegression(penalty='l2')\n\nlogreg.fit(Xs, y_train)","82910e62":"# Perform 5-fold cross validation\nscores = cross_val_score(logreg,Xs, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())\nlog = scores.mean()","a78df8ee":"gbc = GradientBoostingClassifier()\nparam_grid = {\n    'learning_rate': np.linspace(0.05,0.1,5),\n    'subsample'    : np.linspace(0.5,1.,5),\n    'n_estimators' : np.linspace(50,200,4,dtype=int),\n    'max_depth'    : np.linspace(3,6,4,dtype=int)\n}\ngs_gbc = GridSearchCV(gbc,\n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1,\n                 n_jobs=5)\ngs_gbc.fit(X_train, y_train)","940013b1":"# Getting the best cross validation of estimator\ngbcs= gs_gbc.best_score_\ngbcs","8019622c":"# Getting the best parameters of estimator\ngs_gbc.best_params_","d48a4984":"#model with best params\ngbc_cv = GradientBoostingClassifier(learning_rate= 0.1,\n                                    max_depth= 4,\n                                    n_estimators= 200,\n                                    subsample= 0.875\n                                   )\ngbc_cv.fit(X_train,y_train)","51583038":"# Perform 5-fold cross validation\nscores = cross_val_score(gbc_cv,X_train, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores:\", scores.mean())","6a460164":"param_grid={ 'max_depth': [10,30,40,50],\n        'n_estimators': [100,200,250],\n        'min_samples_leaf': range(20,50,5),\n        'min_samples_split': range(15,36,5),\n           }","69db39b4":"et = ExtraTreesClassifier()\ngs_et = GridSearchCV(et,\n                  param_grid, \n                  cv=5,\n                  scoring='accuracy',\n                  verbose=1,\n                 n_jobs=3)\ngs_et.fit(X_train, y_train)","d4e6db40":"# Getting the best cross validation of estimator\netc=gs_et.best_score_\netc","5e6ee0ea":"# Perform 5-fold cross validation\nscores = cross_val_score(et, X_train, y_train, cv=5)\nprint(\"Cross-validated scores:\", scores)\nprint(\"Mean cross-validated scores ExtraTree:\", scores.mean().round(3))","868c63aa":"# Creating the dictionary for all models and score\ndata ={'Models': ['RF','KNN_bagg','DT_bagg','LogisticR','GBC','ET'],\n      'Score' : [RF,baggKNN,baggDT,log,gbcs,etc]}\n\n# Creating the dataframe for dictionary of models and their score\ndf = pd.DataFrame(data)\ndf['Score'].round(5)","78ba392c":"fig = plt.figure(figsize = (15, 6)) \n\n# Plot the horizontal bar plot for models by score\nplt.barh(df['Models'],df['Score'],color =['#c94c4c','#eea29a','#b1cbbb','#deeaee']) \n\n# Set Labels\nplt.xlabel(\"Score \") \nplt.ylabel(\"Models\") \n\nfor index, value in enumerate(df['Score'].round(4)):\n    # Add the label of score in each bar\n    plt.text(value, index,str(value))\n\nplt.show() ","d401ed8f":"pred = pd.DataFrame(gbc_cv.predict(X_test),columns=['Survived'])\npred.set_index(test['PassengerId'],inplace=True)","a3ef4377":"pred.to_csv('prediction.csv')","668d321a":"We observe the average age of passengers is 28 years old and the eldest passenger is 80 years old, and there might be one or more families with six children with this large number of family members did they all survived 1?\n\nSeems the most of survived passenger between 20 to 30 and the children are younger than 10 years. Could be because of children and women the first? ","f29eab87":"## Load Data\n---","36e7d957":"## Load packages","3d280b59":"<br>","e91d3e29":"In the cell below, use `cross_val_score` to see what accuracy we can expect from our model.","c5e98541":"##### 6.a Decision Tree with Bagging and GridSearchCV","6ce9ccd3":"Let's use these Features to predict, but first let us do dummy variables.","43e89ce5":"#### 8. Gradient Boosting Classifier","bebedce8":"#### 4. Random Forest with Grid search","7da3b22d":"#### 9. How is The Age distributed in Survived Class?","a9dd6e88":"## Conclusions and Recommendations\n---","a4f10462":"#### 7. Modify The Cabin Values in Both datasets","43cb411c":"#### 9. ExtraTree Classifier","9bef8320":"##### 5.b KNN and GridSearchCV with Bagging","c1c2db54":"#### 8. Summary Statistics (train dataset)","e8ec3570":"#### 5. Fill The Missing Values in Test dataset","2aec081a":"## Model Instantiation & Prediction\n---","4362a50b":"##### 5.a Fill The Missing Values in Fare","578647a4":"### Contents:\n- [Load Data](#Load-Data)\n- [Data Cleaning & Exploration](#Data-Cleaning-&-Exploration)\n- [Model Instantiation & Prediction](#Model-Instantiation-&-Prediction)\n- [Conclusions and Recommendations](#Conclusions-and-Recommendations)","c2fd754f":"#### 10. Comparison Between Models","fd8ca058":"From above list we can see the best k value is 11.","c334def0":"### Excutive summary\n\nWhen There was some element of luck involved in surviving such as the cabin and gender and so on, it seems some groups of people were more likely to survive than others. And  by using five features we were able to predict the number of survived passengers.<br>\nExploratory data analysis produces the findings:\n<ol>\n    <li>The mean of age in pclass 3 is the smallest among the pclasses by 25.14.<\/li>\n    <li>The percentage of male death is more than %90.<\/li>\n    <li>The Southampton embarkation port has the highest number of survived among the ports.<\/li>\n<\/ol>","0bbc2d43":"<br>\n\n### Problem statement\n\nSo Kaggle recently hosted an open online competition where the competitors had to design a model based on given training data set which predicted the survival of passengers during the famous Titanic shipwreck. And we will attempt this problem using Machine Learning basic concepts of algorithms (Classification) and data processing.","be62bf1d":"##### 5.c KNN Bagging with The Best Parameters","cc6e1edb":"Number of death more than survived.","718bcda8":"There is small overfitt , let's see it with **Bagging**.","63835e1e":"#### 10. Features Selection","443df38a":"#### 2. The Missing Values in Train Dataset","7d718290":"There is also overfitting.","75938a95":"After applying  basic concepts of Machine Learning algorithms and data processing to predict the survival passengers,  We used 6 different ML models and the **Best** machine learning model among others that gave highest score on Kaggle competition was Gradiant Boosting Classifier with  **0.77033 of accuracy score** . The recommendations are the safety equipments should be around the ship and available, and the emergency plan is required for each ticket class.","168d54da":"**From the above graphs we can see those elemnts approve our theory which is *there are some features are related to the number of survived passenger*.**\n\nHigh number of survived in :\n- First class\n- Female \n- 0 parent\/children\n- without cabin number\n- Embarked S","8cd77ec8":"#### 3. Visualize The Missing Values","ad8bc5fe":"there is an huge overfit in Extra Tree","d1fecbbe":"In the cell below, use `cross_val_score` to see what accuracy we can expect from our KNN model.","70335dff":"#### 6. Decision Tree","d361a98f":"##### 8.a Gradient Boosting with GridSearchCV","36bda61a":"### Save prediction of best model","97fc4b3f":"In the cell below, we use `cross_val_score` to see what accuracy we can expect from our model.","affd17b1":"<br>Staring for here we will be comparing `best_score_` with `cross_val_score` in each model to see if it's over fit or not.","f4fa2f68":"#### 1. The Missing Values in Train Dataset","aaef8dfd":"#### 5. K-Nearest Neighbors","910823ea":"#### 11. One Hot Encoding","ac1305af":"- According to the plot, we can see GradiantBoosting and ExtraTree have best scores, but on Extra Tree no matter much we run with best parameters there is a huge difference when we do Cross Validation Score.  \n- Gradiant Boosting Classifier had the best score even with Cross Validation Score, Therefore we will be using it to do the prediction.","b53a3252":"Let's investigate more in the age. ","14055416":"#### 6. Fill The Missing Values of Age in Both datasets","e0844915":"##### 6.b Decision Tree with The Best Parameters of Bagging","12a0d09e":"#### 2. Baseline Prediction (train data)","3c4c6df3":"<br>","1d74fcc6":"All predictions are saved in `prediction.csv` file.","bd7a0c07":"#### 1. Prepare Datasets","c090fdf6":"## Data Cleaning & Exploration\n---","63f90e9c":"**After Filling all missing values lets see the relation between the Features and our traget.**","03b13e09":"#### 4. Fill The Missing Values in Train dataset","8b01c75d":"##### 5.a KNN with set of K","aa5795b3":"In this part, we show how many the missing values in each column of the train dataset.","57d3267c":"##### 8.b Gradient Boosting with Its Best Parameters","23e02af6":"In the cell below, use `cross_val_score` to see what accuracy we can expect from our model.","b25e8ec7":"In the cell below, use `cross_val_score` to see what accuracy we can expect from our model.","1b3d361f":"Let's try the simplest KNN, since it is KNN we will use the scaled predictors (X).","34b67e2d":"#### 3. Scaled Data","387bc13c":"##### ExtraTree Classifier with GridSearchCV","1cfacc97":"In this part, we show how many the missing values in each column of the test dataset.","0f3148e6":"#### By : Mohammed Al-Ali , Rawan MohammedEid , Ibrahim Alzahrani","592abdfb":"#### 7. Logistic Regression"}}