{"cell_type":{"41b3420d":"code","214def1e":"code","a2f147ad":"code","923db5d1":"code","f9f3a938":"code","e5706e92":"code","ff20a364":"code","6c64a990":"code","e3c87a3f":"code","f45d903c":"code","54dc4447":"code","ad178ada":"code","e5f07b50":"code","28e9a605":"code","50e05d1e":"code","b67fc1f4":"code","18e35653":"code","ae267048":"code","84d46140":"code","49d75e53":"code","928d1734":"code","de41207d":"code","2d87a25a":"code","e14c87ec":"code","0fcccd3c":"code","c5860f0c":"code","6820096a":"code","87d8d638":"markdown","732e30bf":"markdown","cc12b58d":"markdown","969d4a5b":"markdown","aa0e940c":"markdown","9744517b":"markdown","b5d96668":"markdown","820d01dc":"markdown","3ae01c94":"markdown","82e5030a":"markdown","b52a14f0":"markdown","63613d3b":"markdown","b1fb1123":"markdown","7833eb06":"markdown","0b423c57":"markdown","e22f2871":"markdown","00de2b4c":"markdown","982832b5":"markdown","af9b58c1":"markdown","8cd12f16":"markdown","9120e773":"markdown","59beb2b5":"markdown","3122481a":"markdown","67348e21":"markdown","da68bcdf":"markdown"},"source":{"41b3420d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","214def1e":"# Importing the Dataset\nimport pandas as pd\ndataset = pd.read_csv(\"..\/input\/weight-height\/weight-height.csv\")","a2f147ad":"# Displaying the head and tail of the dataset\ndataset.head()","923db5d1":"dataset.tail()","f9f3a938":"# Displaying the shape and datatype for each attribute\nprint(dataset.shape)\ndataset.dtypes","e5706e92":"# Displaying the describe of each attribute\ndataset.describe()","ff20a364":"# Histogram Visualisation For Height Attribute with distribution plot\n\nimport seaborn as sb\nsb.distplot(dataset['Height'])","6c64a990":"sb.distplot(dataset['Weight'])","e3c87a3f":"# Checking the correlation between input and output attributes.\ncorr_value=dataset.corr()\nsb.heatmap(corr_value,square=True)","f45d903c":"# Displaying the Null or empty values \ndataset.info()","54dc4447":"# Displaying the Null or empty values sum\ndataset.isna().sum()","ad178ada":"# encoding gender column\ndataset['Gender'].unique()","e5f07b50":"dataset['Gender']=dataset['Gender'].map({'Male':0,'Female':1})\ndataset['Gender'].unique()","28e9a605":"# Displaying first 5 rows\ndataset.head()","50e05d1e":"# Checking the outliers with each input attribute to output attribute.\n\nplt.plot(dataset['Gender'],dataset['Weight'])\nplt.title(\"Checking Outliers\")\nplt.xlabel(\"Gender\")\nplt.ylabel('Weight')\nplt.show()","b67fc1f4":"# Checking the outliers with Height input attribute and Weight output attribute.\n\nplt.plot(dataset['Height'],dataset['Weight'])\nplt.title(\"Checking Outliers\")\nplt.xlabel(\"Height\")\nplt.ylabel('Weight')\nplt.show()","18e35653":"y=dataset['Weight'].values\nx=dataset.drop(['Weight'],axis=1)","ae267048":"# Splitting dataset into train and test split.\n\ntrain_size=0.80\ntest_size=0.20\nseed=5\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_size,test_size=test_size,random_state=seed)","84d46140":"# Spot Checking and Comparing Algorithms Without Feature Scale\nmodels=[]\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nmodels.append(('linear_reg',LinearRegression()))\nmodels.append(('knn',KNeighborsRegressor()))\nmodels.append(('SVR',SVR()))\nmodels.append((\"decision_tree\",DecisionTreeRegressor()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nerror='neg_mean_squared_error'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","49d75e53":"# Create Pipeline with Standardization Scale and models\n# Standardize the dataset\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import MinMaxScaler\npipelines=[]\npipelines.append(('scaler_lg',Pipeline([('scaler',MinMaxScaler()),('lg',LinearRegression())])))\npipelines.append(('scale_KNN',Pipeline([('scaler',MinMaxScaler()),('KNN',KNeighborsRegressor())])))\npipelines.append(('scale_SVR',Pipeline([('scaler',MinMaxScaler()),('SVR',SVR())])))\npipelines.append(('scale_decision',Pipeline([('scaler',MinMaxScaler()),('decision',DecisionTreeRegressor())])))\n\n# Evaluate Pipelines\npredictions=[]\nnames=[]\nfor name, model in pipelines:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg='%s : %f (%f)'%(name,result.mean(),result.std())\n    print(msg)\n    \n#Visualize the compared algorithms\nfig=plt.figure()\nfig.suptitle(\"Algorithms Comparisions\")\nplt.boxplot(predictions)\nplt.show()","928d1734":"# SVR Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nkernel=['linear','poly','rbf','sigmoid']\nc=[0.2,0.4,0.6,0.8,1.0]\nparam_grid=dict(C=c,kernel=kernel)\nmodel=SVR()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","de41207d":"# Linear Regression Algorithm tuning\n\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nparam_grid=dict()\nmodel=LinearRegression()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","2d87a25a":"# Ensemble and Boosting algorithm to improve performance\n\n\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Ensemble Bagging methods\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',MinMaxScaler()),('AB',AdaBoostRegressor())])))\nensembles.append(('scaledGBR',Pipeline([('scale',MinMaxScaler()),('GBR',GradientBoostingRegressor())])))\nensembles.append(('scaledRF',Pipeline([('scale',MinMaxScaler()),('rf',RandomForestRegressor(n_estimators=10))])))\nensembles.append(('scaledETR',Pipeline([('scale',MinMaxScaler()),('ETR',ExtraTreesRegressor(n_estimators=10))])))\nensembles.append(('scaledRFR',Pipeline([('scale',MinMaxScaler()),('RFR',RandomForestRegressor(n_estimators=10))])))\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","e14c87ec":"# GradientBoostingRegressor Tuning\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nlearning_rate=[0.1,0.2,0.3,0.4,0.5]\nn_estimators=[5,10,15,20,25,30,40,50,100,200]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=GradientBoostingRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","0fcccd3c":"# AdaBoostRegressor Tuning\n\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=MinMaxScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nlearning_rate=[0.1,0.2,0.3,0.4,0.5]\nn_estimators=[5,10,15,20,25,30,40,50,100,200]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=AdaBoostRegressor()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","c5860f0c":"# Finalize Model\n# we will finalize the gradient boosting regression algorithm and evaluate the model for house price predictions.\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nscaler=MinMaxScaler().fit(x_train)\nscaler_x=scaler.transform(x_train)\nmodel=GradientBoostingRegressor(random_state=5,n_estimators=50,learning_rate=0.1)\nmodel.fit(scaler_x,y_train)\n\n#Transform the validation test set data\nscaledx_test=scaler.transform(x_test)\ny_pred=model.predict(scaledx_test)","6820096a":"# Accuracy of algorithm\nfrom math import sqrt\nmse=mean_squared_error(y_test,y_pred)\nrmse=np.sqrt(mse)\nprint(\"rmse\",rmse)\nr2=r2_score(y_test,y_pred)\nprint(\"mse\",mse)\nprint(\"r2_score\",r2)","87d8d638":"# 1(d). Checking Null or Empty Values (Data Cleaning)\n\nChecking the null or empty values and applying data cleaning to our dataset.","732e30bf":"Ada Boost Regression Algorithm : -110.605590 (5.951022)\n\nGradient Boosting Regression Algorithm : -102.850162 (3.451911)\n\nWe are going to apply tuning to this algorithms\n\n# 2(e). Regularisation Tuning For Top 2 Ensemble and Boosting Regression Algorithms.","cc12b58d":"Here we are going to predict the height and weight of the person. We are taking height and gender is the input attributes and weight going to output attribute.\n\nTable Of Content:\n\n*   **1. Feature Enineering\/Data Pre Processing**\n  \n *      1(a). Import Dataset\n *      1(b). Describing Descriptive Statistics\n *      1(c). Visualising Descriptive Statistics\n *      1(d). Checking Null or Empty Values (Data Cleaning)\n *      1(e). Label Encoder\/One Hot Encoder\n *      1(f). Handle Outliers\n *      1(g). Feature Split\n *      1(h). Resample Evaluate performance model\n     \n*    **2. Modeling**\n   \n *      2(a). Regression Models Without Feature Scale.\n *      2(b). Regression Models With Feature Scale.\n *      2(c). Regularisation Tuning For Top 2 Regression Algorithms.\n *      2(d). Ensemble and Boosting Regression Algorithms With Feature Scale.\n *      2(e). Regularisation Tuning For Top 2 Ensemble and Boosting Regression Algorithms.\n *      2(f). Compare All 4 Tunned Algorithms And Selecting The Best Algorithm\n *      2(g). Fit and Predict The Best Algorithm.\n *      2(h). Accuracy Of An Algorithm.","969d4a5b":"# 2(d). Ensemble and Boosting Regression Algorithms With Feature Scale.\n\nEnsemble and boosting algorithms","aa0e940c":"We dont have any outliers as per above plot","9744517b":"Wowww Perfect we got good normal distribution...Now we are going to check weight attribute.","b5d96668":"# 1(f). Handle Outliers","820d01dc":"As per above plot weight is not a normal distribution but it is almost simlar to normal..\n\nIf we want best prediction we need to apply feature scale then only we will get better accuracy.","3ae01c94":"# 2(h). Accuracy Of An Algorithm.","82e5030a":"# 1(b). Describing Descriptive Statistics","b52a14f0":"Perfect we got root mean square error around 10 we done..\n\nIf any questions please let me know...","63613d3b":"# 1(h). Resample Evaluate performance model\n\nNow we are going to split the input and output attribute as training and test set to evaluate model performance..","b1fb1123":"So finally we dont have any outliers in our dataset, So now we can safely go ahead.","7833eb06":"As per above min value is clear that height and weight not starting from zero. it is starting around 50+...","0b423c57":"# 2(f). Compare All 4 Tunned Algorithms And Selecting The Best Algorithm\n\n1. SVR Tuning -104.664036 using {'C': 1.0, 'kernel': 'linear'} \n2. Linear Regression algorithm -100.963799 using {} \n3. GradientBoostingRegressor Tuning :-102.648845 using {'learning_rate': 0.1, 'n_estimators': 50} \n4. AdaBoostRegressor Tuning : -109.112158 using {'learning_rate': 0.5, 'n_estimators': 200} ","e22f2871":"We didn't get good accuracy because we dont have complex values we have only gender and height as input attributes.\n\nLinear Regression Accuracy : -100.963799 (3.484180)\n\nKNN Regressor accuracy: : -121.553066 (4.666317)\n\nSVR accuracy :-104.947244 (3.553198)\n\nDecision Tree : : -200.952203 (7.381938)\n\n\n# 2(c). Regularisation Tuning For Top 2 Regression Algorithms.\n\nTop 2 algorithms Linear regression and SVR so now we are applying tuning to this algorithms.\n","00de2b4c":"We got accuracy very highly now we are going to applying feature scale to same code and lets check how well it performed.\n\n# 2(b). Regression Models With Feature Scale.","982832b5":"# 2. Modeling\n\n#  2(a). Regression Models Without Feature Scale.","af9b58c1":"# 2(g). Fit and Predict The Best Algorithm.","8cd12f16":"# 1. Feature Enineering\/Data Pre Processing\n\n# 1(a). Import Dataset","9120e773":"# 1(g). Feature Split\n\nSplitting the input and output attributes","59beb2b5":"# 1(c). Visualising Descriptive Statistics","3122481a":"We dont have any missing values so now safely we can go ahead...","67348e21":"After Applying tuning to those algorithm we are accuracy like wise\n\nGradientBoostingRegressor Tuning :-102.648845 using {'learning_rate': 0.1, 'n_estimators': 50} \n\nAdaBoostRegressor Tuning : -109.112158 using {'learning_rate': 0.5, 'n_estimators': 200} ","da68bcdf":"# 1(e). Label Encoder\/One Hot Encoder\n\nEncoding the categorical value into numerical value."}}