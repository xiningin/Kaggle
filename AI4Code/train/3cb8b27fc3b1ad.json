{"cell_type":{"d2c582f4":"code","189dabf4":"code","71cfd05e":"code","b6444bef":"code","3842372e":"code","35b093cb":"code","c699b4b0":"code","405bbf7a":"code","2d53bc6c":"code","fe3f0b06":"code","cfdd9d49":"code","fccfd980":"code","dd78f515":"code","88215d87":"code","88131b18":"code","5b2d278a":"code","11137240":"code","ac77adc4":"code","1d4a40e8":"code","6bf2b672":"code","f5b03ae6":"code","c2f1e593":"code","f6dd762d":"code","06655058":"code","181dd730":"code","ccc7b525":"markdown","0605873b":"markdown","6beba6a7":"markdown","ca6d8f17":"markdown","99ee15b5":"markdown","633891c2":"markdown","ba4b14c4":"markdown"},"source":{"d2c582f4":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\ndef ffill(data_df):\n    data_df=data_df.set_index(['time_id', 'seconds_in_bucket'])\n    data_df = data_df.reindex(pd.MultiIndex.from_product([data_df.index.levels[0], np.arange(0,600)], names = ['time_id', 'seconds_in_bucket']), method='ffill')\n    return data_df.reset_index()\n\ndef renamecol(col):\n    if isinstance(col, tuple):\n        col = '_'.join(str(c) for c in col)\n    return col\n\ndef Price_Open(price_data):\n    price_open = price_data.iloc[0]\n    ## price_data.head(1).item()\n    return price_open\n\ndef Price_Close(price_data):\n    price_close = price_data.iat[-1]\n    ## price_data.tail(1).item()\n    \n    return price_close\n\n\ndef CloseToClose_estimator(close, window=1, trading_periods=1, clean=True): \n    log_return = (close \/ close.shift(1)).apply(np.log)\n\n    result = log_return.rolling(\n        window=window,\n        center=False\n    ).std() * math.sqrt(trading_periods)\n\n    if clean:\n        return result.dropna()\n    else:\n        return result\n    \n    \ndef Parkinson_estimator(High,Low, window=1, trading_periods=1, clean=True):\n\n    rs = (1.0 \/ (4.0 * math.log(2.0))) * ((High \/ Low).apply(np.log))**2.0\n\n    def f(v):\n        return trading_periods * v.mean()**0.5\n    \n    result = rs.rolling(\n        window=window,\n        center=False\n    ).apply(func=f)\n    \n    if clean:\n        return result.dropna()\n    else:\n        return result\n    \n\ndef GarmanKlass_estimator(High,Low,Close,Open, window=1, trading_periods=1, clean=True):\n\n    log_hl = (High \/ Low).apply(np.log)\n    log_co = (Close \/ Open).apply(np.log)\n\n    rs = 0.5 * log_hl**2 - (2*math.log(2)-1) * log_co**2\n    \n    def f(v):\n        return (trading_periods * v.mean())**0.5\n    \n    result = rs.rolling(window=window, center=False).apply(func=f)\n    \n    if clean:\n        return result.dropna()\n    else:\n        return result\n\n\n    \ndef RogerSatchell_estimator(High,Low,Close,Open, window=1, trading_periods=1, clean=True):\n    \n    log_ho = (High\/ Open).apply(np.log)\n    log_lo = (Low \/ Open).apply(np.log)\n    log_co = (Close \/Open).apply(np.log)\n    \n    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n\n    def f(v):\n        return trading_periods * v.mean()**0.5\n    \n    result = rs.rolling(\n        window=window,\n        center=False\n    ).apply(func=f)\n    \n    if clean:\n        return result.dropna()\n    else:\n        return result\n    \n    \n    \ndef YangZhang_estimator(High,Low,Close,Open, window=1, trading_periods=1, clean=True):\n\n    log_ho = (High \/ Open).apply(np.log)\n    log_lo = (Low \/ Open).apply(np.log)\n    log_co = (Close\/ Open).apply(np.log)\n    \n    log_oc = (Open \/ Close.shift(1)).apply(np.log)\n    log_oc_sq = log_oc**2\n    \n    log_cc = (Close\/ Close.shift(1)).apply(np.log)\n    log_cc_sq = log_cc**2\n    \n    rs = log_ho * (log_ho - log_co) + log_lo * (log_lo - log_co)\n    \n    close_vol = log_cc_sq.rolling(\n        window=window,\n        center=False\n    ).sum() * (1.0)\n    open_vol = log_oc_sq.rolling(\n        window=window,\n        center=False\n    ).sum() * (1.0)\n    window_rs = rs.rolling(\n        window=window,\n        center=False\n    ).sum() * (1.0)\n\n    k = 0.34 \/ (1)\n    result = (open_vol + k * close_vol + (1 - k) * window_rs).apply(np.sqrt) * math.sqrt(trading_periods)\n\n    if clean:\n        return result.dropna()\n    else:\n        return result\n    \ndef count_unique(series):\n    return len(np.unique(series))\n\n# 25th Percentile\ndef q25(x):\n    return x.quantile(0.25)\n\n# 50th Percentile\ndef q50(x):\n    return x.quantile(0.5)\n\n# 75th Percentile\ndef q75(x):\n    return x.quantile(0.75)","189dabf4":"#Import Libraries and setting correct file path\nimport numpy as np # Math\nimport pandas as pd # data processing\nimport glob \nimport os\nimport gc\nimport math\nimport scipy\n\n\nfrom joblib import Parallel, delayed # Parallel processing\n\nfrom sklearn import preprocessing, model_selection # Model evaluation\nimport lightgbm as lgb # Boosting Models\n\nfrom sklearn.metrics import r2_score # Model evaluation\n\nimport matplotlib.pyplot as plt # Data Visualization\nimport seaborn as sns # Data Visualization\n\nimport pyarrow\nimport tqdm\nseed = 42 \n\npath_data = '..\/input\/optiver-realized-volatility-prediction'\npath_train = '..\/input\/traindata-pre-processing-part2'\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","71cfd05e":"#Core function that will extract and trasform the data\ndef get_stock_stat(stock_id : int,dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book Data\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet\/stock_id={}\/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    #Book data Seconds in buckets features\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) \/ (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) \/ (df_book['bid_size2'] + df_book['ask_size2'])\n    \n    df_book['wap_balance'] = abs(df_book['wap1'] - df_book['wap2'])\n    df_book['bid_spread'] = df_book['bid_price1'] - df_book['bid_price2']\n    df_book['ask_spread'] = df_book['ask_price1'] - df_book['ask_price2']\n    df_book['total_volume'] = (df_book['ask_size1'] + df_book['ask_size2']) + (df_book['bid_size1'] + df_book['bid_size2'])\n    df_book['volume_imbalance'] = abs((df_book['ask_size1'] + df_book['ask_size2']) - (df_book['bid_size1'] + df_book['bid_size2']))\n    \n    df_book['Bid_Ask_Spread'] = df_book['ask_price1'] - df_book['bid_price1']\n    df_book['Bid_Ask_Spread_Pct'] = ((df_book['ask_price1']\/ df_book['bid_price1'])-1)*100\n    df_book['Quoted_Spread'] = (df_book['ask_price1']-df_book['bid_price1'])\/((df_book['ask_price1']+df_book['bid_price1'])\/2)*100\n    df_book['Bid_Ask_Balance'] = (df_book['ask_size1']- df_book['bid_size1'])\n    df_book['Bid_Ask_Balance_Pct'] = ((df_book['ask_size1']\/df_book['bid_size1'])-1)*100\n    df_book['Market_Depth'] = (df_book['bid_size1']+df_book['ask_size1']+df_book['bid_size2']+df_book['ask_size2'])\n    df_book['Order_Volume_bid'] = (df_book['bid_size1']+df_book['bid_size2'])\n    df_book['Order_Volume_ask'] = (df_book['ask_size1']+df_book['ask_size2'])\n    df_book['BestPrice_Difference_bid'] = (df_book['ask_price1'] - df_book['bid_price2'])\n    df_book['BestPrice_Difference_ask'] = (df_book['bid_price1']+df_book['bid_price2'])\n    df_book['BestPrice_Difference_bid_pct'] = ((df_book['ask_price1']\/df_book['bid_price2'])-1)*100\n    df_book['BestPrice_Difference_ask_pct'] = ((df_book['bid_price1']\/df_book['bid_price2'])-1)*100\n    \n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n    \n    #Book data time id aggregation\n    \n    #dict for aggregate\n    create_feature_dict = {\n        'log_return1':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'wap1':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close,scipy.stats.kurtosis,\n                scipy.stats.skew,q25,q50,q75],\n        'wap2':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close,scipy.stats.kurtosis,\n                scipy.stats.skew,q25,q50,q75],\n        'Bid_Ask_Spread':[np.mean,np.var,np.max,np.min],\n        'wap_balance':[np.mean,np.var,np.max,np.min],\n        'bid_spread':[np.mean,np.var,np.max,np.min],\n        'ask_spread':[np.mean,np.var,np.max,np.min],\n        'total_volume':[np.mean,np.var,np.max,np.min],\n        'volume_imbalance':[np.mean,np.var,np.max,np.min],\n        'Bid_Ask_Spread_Pct':[np.mean,np.var,np.max,np.min],\n        'Quoted_Spread':[np.mean,np.var,np.max,np.min],\n        'Bid_Ask_Balance':[np.mean,np.var,np.max,np.min],\n        'Bid_Ask_Balance_Pct':[np.mean,np.var,np.max,np.min],\n        'Market_Depth':[np.mean,np.var,np.max,np.min],\n        'Order_Volume_bid':[np.mean,np.var,np.max,np.min],\n        'Order_Volume_ask':[np.mean,np.var,np.max,np.min],\n        'BestPrice_Difference_bid':[np.mean,np.var,np.max,np.min],\n        'BestPrice_Difference_ask':[np.mean,np.var,np.max,np.min],\n        'BestPrice_Difference_bid_pct':[np.mean,np.var,np.max,np.min],\n        'BestPrice_Difference_ask_pct':[np.mean,np.var,np.max,np.min],\n        'ask_price1':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close],\n        'ask_price2':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close],\n        'bid_price1':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close],\n        'bid_price2':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close]\n            }\n    \n    stock_stat= df_book.groupby(by = ['stock_id', 'time_id']).agg(create_feature_dict).reset_index()\n    stock_stat.columns = map(renamecol, stock_stat.columns)\n    stock_stat=stock_stat.rename({'stock_id_':'stock_id'}, axis=1)\n    stock_stat=stock_stat.rename({'time_id_':'time_id'}, axis=1)\n    \n    stock_stat_150 = df_book[df_book['seconds_in_bucket'] >= 150].groupby(['stock_id','time_id']).agg(create_feature_dict).reset_index()\n    stock_stat_150.columns = map(renamecol, stock_stat_150.columns)\n    stock_stat_150=stock_stat_150.rename({'stock_id_':'stock_id'}, axis=1)\n    stock_stat_150=stock_stat_150.rename({'time_id_':'time_id'}, axis=1)\n    \n    stock_stat_300 = df_book[df_book['seconds_in_bucket'] >= 300].groupby(['stock_id','time_id']).agg(create_feature_dict).reset_index()\n    stock_stat_300.columns = map(renamecol, stock_stat_300.columns)\n    stock_stat_300=stock_stat_300.rename({'stock_id_':'stock_id'}, axis=1)\n    stock_stat_300=stock_stat_300.rename({'time_id_':'time_id'}, axis=1)\n    \n    stock_stat_450 = df_book[df_book['seconds_in_bucket'] >= 450].groupby(['stock_id','time_id']).agg(create_feature_dict).reset_index()\n    stock_stat_450.columns = map(renamecol, stock_stat_450.columns)\n    stock_stat_450=stock_stat_450.rename({'stock_id_':'stock_id'}, axis=1)\n    stock_stat_450=stock_stat_450.rename({'time_id_':'time_id'}, axis=1)\n    \n    stock_stat_150 = stock_stat_150.add_suffix('_150')\n    stock_stat_300 = stock_stat_300.add_suffix('_300')\n    stock_stat_450 = stock_stat_450.add_suffix('_450')\n    \n    stock_stat = stock_stat.merge(stock_stat_150, how = 'left', left_on = ['stock_id','time_id'], right_on = ['stock_id_150','time_id_150'])\n    stock_stat = stock_stat.merge(stock_stat_300, how = 'left', left_on = ['stock_id','time_id'], right_on = ['stock_id_300','time_id_300'])\n    stock_stat = stock_stat.merge(stock_stat_450, how = 'left', left_on = ['stock_id','time_id'], right_on = ['stock_id_450','time_id_450'])\n    stock_stat.drop(['stock_id_150','time_id_150', 'stock_id_300', 'time_id_300','stock_id_450','time_id_450'], axis = 1, inplace = True)\n    \n    \n    \n    #Trade data\n    df_trade =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet\/stock_id={}'.format(dataType, stock_id)))\n    df_trade = df_trade.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    df_trade['stock_id'] = stock_id\n    cols = key + [col for col in df_trade.columns if col not in key]\n    df_trade = df_trade[cols]\n    \n    #Trade data Seconds in buckets features\n    df_trade['trade_log_return1'] = df_trade.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    #Trade data time id aggregation\n    #Trade data time id aggregation\n    create_feature_dict_trade = {\n        'trade_log_return1':[realized_volatility],\n        'seconds_in_bucket':[np.count_nonzero,count_unique],\n        'size':[np.mean,np.var,np.max,np.min,np.sum],\n        'order_count':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close,np.sum],\n        'price':[np.mean,np.var,np.max,np.min,Price_Open,Price_Close,scipy.stats.kurtosis,\n                 scipy.stats.skew,q25,q50,q75]\n            }\n    \n    trade_stat = df_trade.groupby(by = ['stock_id', 'time_id']).agg(create_feature_dict_trade).reset_index()\n    trade_stat.columns = map(renamecol, trade_stat.columns)\n    trade_stat=trade_stat.rename({'stock_id_':'stock_id'}, axis=1)\n    trade_stat=trade_stat.rename({'time_id_':'time_id'}, axis=1)\n    \n    \n    trade_stat_150 = df_trade[df_trade['seconds_in_bucket'] >= 150].groupby(['stock_id','time_id']).agg(create_feature_dict_trade).reset_index()\n    trade_stat_150.columns = map(renamecol, trade_stat_150.columns)\n    trade_stat_150=trade_stat_150.rename({'stock_id_':'stock_id'}, axis=1)\n    trade_stat_150=trade_stat_150.rename({'time_id_':'time_id'}, axis=1)\n    \n    trade_stat_300 = df_trade[df_trade['seconds_in_bucket'] >= 300].groupby(['stock_id','time_id']).agg(create_feature_dict_trade).reset_index()\n    trade_stat_300.columns = map(renamecol, trade_stat_300.columns)\n    trade_stat_300=trade_stat_300.rename({'stock_id_':'stock_id'}, axis=1)\n    trade_stat_300=trade_stat_300.rename({'time_id_':'time_id'}, axis=1)\n    \n    trade_stat_450 = df_trade[df_trade['seconds_in_bucket'] >= 450].groupby(['stock_id','time_id']).agg(create_feature_dict_trade).reset_index()\n    trade_stat_450.columns = map(renamecol, trade_stat_450.columns)\n    trade_stat_450=trade_stat_450.rename({'stock_id_':'stock_id'}, axis=1)\n    trade_stat_450=trade_stat_450.rename({'time_id_':'time_id'}, axis=1)\n    \n    trade_stat_150 = trade_stat_150.add_suffix('_150')\n    trade_stat_300 = trade_stat_300.add_suffix('_300')\n    trade_stat_450 = trade_stat_450.add_suffix('_450')\n    \n    trade_stat = trade_stat.merge(trade_stat_150, how = 'left', left_on = ['stock_id','time_id'], right_on = ['stock_id_150','time_id_150'])\n    trade_stat = trade_stat.merge(trade_stat_300, how = 'left', left_on = ['stock_id','time_id'], right_on = ['stock_id_300','time_id_300'])\n    trade_stat = trade_stat.merge(trade_stat_450, how = 'left', left_on = ['stock_id','time_id'], right_on = ['stock_id_450','time_id_450'])\n    trade_stat.drop(['stock_id_150','time_id_150', 'stock_id_300', 'time_id_300','stock_id_450','time_id_450'], axis = 1, inplace = True)\n    \n    \n    \n    \n    \n    \n    \n    #Joining book and trade feature   \n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(0)\n    \n    return stock_stat\n\n# Making the extraction faster by doing the function on multiple processor unit at the time\n\ndef get_dataSet(stock_ids : list,dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","b6444bef":"%%time\ntrain = pd.read_csv(os.path.join(path_data, 'train.csv'))\ntrain_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(),dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\n#print('Train shape: {}'.format(train.shape))\n#train.drop(train.columns[[0, 1]], axis = 1, inplace = True)\n#train.columns = map(renamecol, train.columns)\n#train.to_csv('TrainingWithFeatures.csv',index=False)\ndisplay(train.head(2))","3842372e":"#Import Libraries and setting correct file path\nimport numpy as np # Math\nimport pandas as pd # data processing\nimport glob \nimport os\nimport gc\nimport math\nimport scipy\n\n\nfrom joblib import Parallel, delayed # Parallel processing\n\nfrom sklearn import preprocessing, model_selection # Model evaluation\nimport lightgbm as lgb # Boosting Models\n\nfrom sklearn.metrics import r2_score # Model evaluation\n\nimport matplotlib.pyplot as plt # Data Visualization\nimport seaborn as sns # Data Visualization\n\nimport pyarrow\nimport tqdm\nseed = 42 \n\npath_data = '..\/input\/optiver-realized-volatility-prediction'\npath_train = '..\/input\/traindata-pre-processing-window'\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","35b093cb":"#FeatureEngineering\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\ntest = test[test.columns.drop(list(test.filter(regex='index__')))]\n\n#test.columns = map(renamecol, test.columns)\n\n\n\ntest['Parkinson_estimator']=  Parkinson_estimator(test.wap1_amin,test.wap1_amax)\ntest['GarmanKlass_estimator']=GarmanKlass_estimator(test.wap1_amin,test.wap1_amax,test.wap1_Price_Close,test.wap1_Price_Open)\ntest['RogerSatchell_estimator']=RogerSatchell_estimator(test.wap1_amin,test.wap1_amax,test.wap1_Price_Close,test.wap1_Price_Open)\ntest['YangZhang_estimator']= YangZhang_estimator(test.wap1_amin,test.wap1_amax,test.wap1_Price_Close,test.wap1_Price_Open)\ntest['Parkinson_estimator_trade']=  Parkinson_estimator(test.price_amin,test.price_amax)\ntest['GarmanKlass_estimator_trade']=GarmanKlass_estimator(test.price_amin,test.price_amax,test.price_Price_Close,test.price_Price_Open)\ntest['Parkinson_estimator_2']=  Parkinson_estimator(test.wap2_amin,test.wap2_amax)\ntest['GarmanKlass_estimator_2']=GarmanKlass_estimator(test.wap2_amin,test.wap2_amax,test.wap2_Price_Close,test.wap2_Price_Open)","c699b4b0":"#Loading pre-processed training data\n#train = pd.read_csv(os.path.join(path_train, 'TrainingWithFeatures.csv'))\n\ntrain['Parkinson_estimator']=  Parkinson_estimator(train.wap1_amin,train.wap1_amax)\ntrain['GarmanKlass_estimator']=GarmanKlass_estimator(train.wap1_amin,train.wap1_amax,train.wap1_Price_Close,train.wap1_Price_Open)\ntrain['RogerSatchell_estimator']=RogerSatchell_estimator(train.wap1_amin,train.wap1_amax,train.wap1_Price_Close,train.wap1_Price_Open)\ntrain['YangZhang_estimator']= YangZhang_estimator(train.wap1_amin,train.wap1_amax,train.wap1_Price_Close,train.wap1_Price_Open)\ntrain['Parkinson_estimator_trade']=  Parkinson_estimator(train.price_amin,train.price_amax)\ntrain['GarmanKlass_estimator_trade']=GarmanKlass_estimator(train.price_amin,train.price_amax,train.price_Price_Close,train.price_Price_Open)\ntrain['Parkinson_estimator_2']=  Parkinson_estimator(train.wap2_amin,train.wap2_amax)\ntrain['GarmanKlass_estimator_2']=GarmanKlass_estimator(train.wap2_amin,train.wap2_amax,train.wap2_Price_Close,train.wap2_Price_Open)\n","405bbf7a":"MarketAverageTrain = train.groupby(by = ['time_id']).agg(realized_volatility_market1=('log_return1_realized_volatility', np.mean),\n                                                         realized_volatility_market2=('log_return2_realized_volatility', np.mean),\n                                                         Parkinson_estimator_market=('Parkinson_estimator', np.mean),\n                                                         GarmanKlass_estimator_market=('GarmanKlass_estimator', np.mean),\n                                                         RogerSatchell_estimator_market=('RogerSatchell_estimator', np.mean),\n                                                         YangZhang_estimator_market=('YangZhang_estimator', np.mean),\n                                                         total_volume_mean_market=('total_volume_mean', np.mean),\n                                                         order_count_sum_market=('order_count_sum', np.mean),  \n                                                         size_sum_market=('size_sum', np.mean), \n                                                         Bid_Ask_Spread_market=('Bid_Ask_Spread_mean', np.mean),\n                                                         realized_volatility_market1_150=('log_return1_realized_volatility_150', np.mean),\n                                                         realized_volatility_market2_150=('log_return2_realized_volatility_150', np.mean),\n                                                         total_volume_mean_market_150=('total_volume_mean_150', np.mean),\n                                                         order_count_sum_market_150=('order_count_sum_150', np.mean),  \n                                                         size_sum_market_150=('size_sum_150', np.mean), \n                                                         Bid_Ask_Spread_market_150=('Bid_Ask_Spread_mean_150', np.mean),\n                                                         realized_volatility_market1_300=('log_return1_realized_volatility_300', np.mean),\n                                                         realized_volatility_market2_300=('log_return2_realized_volatility_300', np.mean),\n                                                         total_volume_mean_market_300=('total_volume_mean_300', np.mean),\n                                                         order_count_sum_market_300=('order_count_sum_300', np.mean),  \n                                                         size_sum_market_300=('size_sum_300', np.mean), \n                                                         Bid_Ask_Spread_market_300=('Bid_Ask_Spread_mean_300', np.mean),\n                                                         realized_volatility_market1_450=('log_return1_realized_volatility_450', np.mean),\n                                                         realized_volatility_market2_450=('log_return2_realized_volatility_450', np.mean),\n                                                         total_volume_mean_market_450=('total_volume_mean_450', np.mean),\n                                                         order_count_sum_market_450=('order_count_sum_450', np.mean),  \n                                                         size_sum_market_450=('size_sum_450', np.mean), \n                                                         Bid_Ask_Spread_market_450=('Bid_Ask_Spread_mean_450', np.mean)\n                                                        ).reset_index()\n                                                     \n\nMarketAverageTest = test.groupby(by = ['time_id']).agg(realized_volatility_market1=('log_return1_realized_volatility', np.mean),\n                                                         realized_volatility_market2=('log_return2_realized_volatility', np.mean),\n                                                         Parkinson_estimator_market=('Parkinson_estimator', np.mean),\n                                                         GarmanKlass_estimator_market=('GarmanKlass_estimator', np.mean),\n                                                         RogerSatchell_estimator_market=('RogerSatchell_estimator', np.mean),\n                                                         YangZhang_estimator_market=('YangZhang_estimator', np.mean),\n                                                         total_volume_mean_market=('total_volume_mean', np.mean),\n                                                         order_count_sum_market=('order_count_sum', np.mean),  \n                                                         size_sum_market=('size_sum', np.mean), \n                                                         Bid_Ask_Spread_market=('Bid_Ask_Spread_mean', np.mean),\n                                                         realized_volatility_market1_150=('log_return1_realized_volatility_150', np.mean),\n                                                         realized_volatility_market2_150=('log_return2_realized_volatility_150', np.mean),\n                                                         total_volume_mean_market_150=('total_volume_mean_150', np.mean),\n                                                         order_count_sum_market_150=('order_count_sum_150', np.mean),  \n                                                         size_sum_market_150=('size_sum_150', np.mean), \n                                                         Bid_Ask_Spread_market_150=('Bid_Ask_Spread_mean_150', np.mean),\n                                                         realized_volatility_market1_300=('log_return1_realized_volatility_300', np.mean),\n                                                         realized_volatility_market2_300=('log_return2_realized_volatility_300', np.mean),\n                                                         total_volume_mean_market_300=('total_volume_mean_300', np.mean),\n                                                         order_count_sum_market_300=('order_count_sum_300', np.mean),  \n                                                         size_sum_market_300=('size_sum_300', np.mean), \n                                                         Bid_Ask_Spread_market_300=('Bid_Ask_Spread_mean_300', np.mean),\n                                                         realized_volatility_market1_450=('log_return1_realized_volatility_450', np.mean),\n                                                         realized_volatility_market2_450=('log_return2_realized_volatility_450', np.mean),\n                                                         total_volume_mean_market_450=('total_volume_mean_450', np.mean),\n                                                         order_count_sum_market_450=('order_count_sum_450', np.mean),  \n                                                         size_sum_market_450=('size_sum_450', np.mean), \n                                                         Bid_Ask_Spread_market_450=('Bid_Ask_Spread_mean_450', np.mean)\n                                                   ).reset_index()\n\n\ntrain = train.merge(MarketAverageTrain, on=['time_id'], how='left').fillna(0)\ntest = test.merge(MarketAverageTest, on=['time_id'], how='left').fillna(0)","2d53bc6c":"del MarketAverageTrain\ndel MarketAverageTest\nimport gc\ngc.collect()\nprint('Test shape: {}'.format(test.shape))\nprint('Train shape: {}'.format(train.shape))","fe3f0b06":"seed0=42\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1,\n    #Optuna Optimization\n    'max_bin': 95,\n    'num_leaves': 323,\n    'bagging_fraction': 0.6949437575460405,\n    'max_depth': 10,\n    'feature_fraction_bynode': 0.7708016312491508,\n    'bagging_freq': 3,\n    'min_data_in_leaf': 296,\n    'reg_alpha': 9.093379148472092,\n    'reg_lambda': 5.395451741231765,\n    'feature_fraction': 0.42414747362283134\n\n}\n\n","cfdd9d49":"ColToExclude={\"time_id\",\n \"target\",\n \"row_id\",\n 'bid_spread_var',\n 'wap2_skew_150',\n 'ask_spread_amin_150',\n 'BestPrice_Difference_bid_amin_150',\n 'ask_spread_mean_150',\n 'Quoted_Spread_var_300',\n 'Market_Depth_amin_150',\n 'size_mean_450',\n 'BestPrice_Difference_bid_pct_var_300',\n 'bid_spread_mean_450',\n 'bid_spread_mean_300',\n 'BestPrice_Difference_ask_pct_var_150',\n 'Order_Volume_ask_mean',\n 'Market_Depth_amax',\n 'price_kurtosis_300',\n 'order_count_Price_Open',\n 'Bid_Ask_Balance_amin',\n 'Bid_Ask_Spread_Pct_amax_300',\n 'Bid_Ask_Balance_var_150',\n 'price_var_150',\n 'BestPrice_Difference_bid_amax_150',\n 'wap2_skew_300',\n 'Bid_Ask_Balance_Pct_var_150',\n 'Market_Depth_var',\n 'BestPrice_Difference_ask_pct_mean_300',\n 'bid_price1_var_300',\n 'order_count_amax_450',\n 'bid_price1_Price_Close_300',\n 'volume_imbalance_mean',\n 'Bid_Ask_Balance_Pct_var_300',\n 'Bid_Ask_Spread_Pct_amin_150',\n 'ask_price1_var_150',\n 'Bid_Ask_Spread_amin_450',\n 'Bid_Ask_Balance_Pct_var',\n 'BestPrice_Difference_ask_pct_var_300',\n 'size_var_450',\n 'ask_spread_var_300',\n 'bid_price2_Price_Close_150',\n 'BestPrice_Difference_bid_pct_amin_150',\n 'ask_price1_amin',\n 'bid_price2_var_150',\n 'price_kurtosis_450',\n 'Market_Depth_amin_450',\n 'Order_Volume_bid_amax_300',\n 'bid_price2_var',\n 'Order_Volume_ask_mean_300',\n 'BestPrice_Difference_bid_pct_amax_150',\n 'Quoted_Spread_var_150',\n 'ask_price2_var',\n 'Order_Volume_ask_mean_150',\n 'Quoted_Spread_var_450',\n 'ask_price2_amax_150',\n 'Bid_Ask_Spread_Pct_var_300',\n 'ask_price2_amax_300',\n 'bid_spread_mean',\n 'wap2_kurtosis_300',\n 'BestPrice_Difference_ask_pct_mean',\n 'BestPrice_Difference_bid_pct_var_150',\n 'wap2_Price_Open_300',\n 'Market_Depth_var_150',\n 'BestPrice_Difference_bid_pct_amin_300',\n 'Bid_Ask_Balance_Pct_var_450',\n 'ask_price2_Price_Open_450',\n 'Order_Volume_bid_amax_150',\n 'BestPrice_Difference_ask_pct_mean_150',\n 'wap1_var',\n 'Bid_Ask_Balance_amin_450',\n 'Bid_Ask_Spread_amax_150',\n 'Order_Volume_ask_var_450',\n 'wap2_Price_Close_150',\n 'ask_price2_Price_Close',\n 'ask_price1_var',\n 'wap2_Price_Open_150',\n 'bid_price2_amax',\n 'Bid_Ask_Balance_mean_450',\n 'bid_spread_var_150',\n 'ask_spread_mean_300',\n 'volume_imbalance_var',\n 'volume_imbalance_amax_450',\n 'price_Price_Close_150',\n 'wap1_Price_Close_150',\n 'ask_price2_amax',\n 'volume_imbalance_mean_150',\n 'volume_imbalance_amax',\n 'order_count_Price_Open_300',\n 'ask_price1_Price_Open_150',\n 'total_volume_amax_150',\n 'Bid_Ask_Spread_var_450',\n 'bid_spread_var_450',\n 'total_volume_mean_450',\n 'BestPrice_Difference_ask_var_150',\n 'Order_Volume_ask_amax',\n 'Bid_Ask_Balance_amax_300',\n 'Order_Volume_ask_amax_450',\n 'BestPrice_Difference_bid_amin_300',\n 'volume_imbalance_mean_450',\n 'volume_imbalance_mean_300',\n 'BestPrice_Difference_bid_var_150',\n 'Market_Depth_mean_300',\n 'Order_Volume_ask_mean_450',\n 'size_sum_150',\n 'Bid_Ask_Balance_Pct_amax_150',\n 'volume_imbalance_var_450',\n 'price_Price_Open_150',\n 'ask_price2_Price_Open_300',\n 'Bid_Ask_Balance_amin_300',\n 'order_count_Price_Close',\n 'volume_imbalance_amax_150',\n 'bid_price1_Price_Open_150',\n 'total_volume_amax',\n 'Order_Volume_ask_amax_150',\n 'order_count_Price_Open_150',\n 'wap2_Price_Close_300',\n 'Bid_Ask_Balance_amax_450',\n 'bid_price1_var_150',\n 'volume_imbalance_var_150',\n 'ask_price2_amin_300',\n 'ask_price2_mean',\n 'Market_Depth_var_300',\n 'bid_price1_Price_Close_450',\n 'Order_Volume_ask_amax_300',\n 'ask_price1_amax_450',\n 'ask_price1_amax_150',\n 'wap2_q25',\n 'Order_Volume_bid_amax_450',\n 'Bid_Ask_Balance_amax',\n 'bid_price2_Price_Open_150',\n 'Bid_Ask_Balance_amax_150',\n 'ask_price2_amin',\n 'Order_Volume_ask_var_300',\n 'ask_price2_amax_450',\n 'bid_price1_amax',\n 'price_amax',\n 'total_volume_var_150',\n 'volume_imbalance_var_300',\n 'wap1_amin',\n 'price_Price_Close',\n 'total_volume_var_300',\n 'Market_Depth_mean_450',\n 'wap2_q25_300',\n 'bid_price1_Price_Open_300',\n 'BestPrice_Difference_ask_var',\n 'BestPrice_Difference_ask_amin_450',\n 'Order_Volume_ask_var_150',\n 'order_count_Price_Close_150',\n 'bid_price2_amin_450',\n 'ask_price1_Price_Close',\n 'order_count_Price_Close_450',\n 'wap1_Price_Open_150',\n 'ask_price1_mean',\n 'price_Price_Close_300',\n 'price_mean_450',\n 'bid_price1_var',\n 'wap1_Price_Close',\n 'order_count_Price_Close_300',\n 'Market_Depth_amax_450',\n 'wap1_amax',\n 'Bid_Ask_Spread_Pct_amin_450',\n 'ask_price2_Price_Close_300',\n 'total_volume_amax_450',\n 'wap2_Price_Close_450',\n 'total_volume_amax_300',\n 'ask_price2_amin_450',\n 'wap1_q50',\n 'wap2_amin',\n 'wap2_q75_150',\n 'price_amin_450',\n 'price_Price_Open_450',\n 'price_amax_150',\n 'price_amin',\n 'wap1_q25',\n 'price_Price_Open_300',\n 'bid_price2_mean_300',\n 'price_q75_450',\n 'bid_price2_amin_150',\n 'ask_price1_Price_Open_300',\n 'bid_price2_Price_Open_300',\n 'Bid_Ask_Balance_var_450',\n 'wap1_amax_300',\n 'bid_price2_amax_450',\n 'bid_price1_amin_450',\n 'ask_price1_mean_150',\n 'ask_price2_amin_150',\n 'price_q25',\n 'wap1_q75_450',\n 'ask_price1_amax',\n 'bid_price1_amin_300',\n 'wap2_Price_Open_450',\n 'bid_price2_amax_300',\n 'wap2_q50_450',\n 'ask_price1_Price_Open_450',\n 'bid_price2_Price_Open_450',\n 'bid_price2_amin',\n 'bid_price2_amax_150',\n 'bid_price2_Price_Close_300',\n 'wap2_q50_150',\n 'price_q25_150',\n 'wap2_mean_300',\n 'ask_price2_mean_450',\n 'ask_price1_Price_Close_150',\n 'ask_price1_amax_300',\n 'wap1_Price_Close_300',\n 'price_q25_450',\n 'wap2_amin_300',\n 'wap1_amax_150',\n 'price_q50_300',\n 'bid_price2_mean',\n 'bid_price1_amin',\n 'BestPrice_Difference_ask_amax',\n 'wap2_q50_300',\n 'bid_price2_Price_Close_450',\n 'price_q50_450',\n 'Market_Depth_amax_300',\n 'ask_price1_amin_450',\n 'wap1_mean',\n 'bid_price2_amin_300',\n 'price_amin_300',\n 'wap1_q50_150',\n 'wap1_Price_Open_300',\n 'price_q75',\n 'wap2_amax',\n 'wap2_q25_150',\n 'BestPrice_Difference_ask_amax_450',\n 'wap1_mean_450',\n 'Market_Depth_var_450',\n 'BestPrice_Difference_ask_amin',\n 'wap2_amax_300',\n 'ask_price2_Price_Close_150',\n 'bid_price1_amax_150',\n 'wap1_q25_300',\n 'wap1_amin_300',\n 'wap2_q75_450',\n 'bid_price1_amin_150',\n 'ask_price2_Price_Close_450',\n 'wap2_q25_450',\n 'bid_price1_Price_Open_450',\n 'Market_Depth_amax_150',\n 'price_q50',\n 'BestPrice_Difference_ask_amin_300',\n 'wap2_amin_150',\n 'price_q25_300',\n 'ask_price1_amin_150',\n 'ask_price1_mean_300',\n 'wap1_q25_450',\n 'ask_price2_mean_150',\n 'price_q50_150',\n 'wap2_amax_450',\n 'wap2_amax_150',\n 'ask_price1_mean_450',\n 'bid_price1_amax_450',\n 'wap1_q75_150',\n 'wap1_q75',\n 'price_amin_150',\n 'wap2_amin_450',\n 'wap2_q75',\n 'BestPrice_Difference_ask_amax_300',\n 'BestPrice_Difference_ask_mean_150',\n 'price_q75_300',\n 'BestPrice_Difference_ask_amin_150',\n 'bid_price1_amax_300',\n 'bid_price2_mean_150',\n 'BestPrice_Difference_ask_amax_150',\n 'wap2_q75_300',\n 'wap1_amin_450',\n 'price_q75_150',\n 'ask_price1_Price_Close_300',\n 'BestPrice_Difference_ask_mean',\n 'ask_price1_amin_300',\n 'bid_price1_mean_300',\n 'wap1_Price_Close_450',\n 'ask_price2_mean_300',\n 'wap1_Price_Open_450',\n 'wap1_q50_300',\n 'wap1_q50_450',\n 'price_amax_450',\n 'wap1_q25_150',\n 'price_mean_150',\n 'bid_price1_mean',\n 'BestPrice_Difference_ask_mean_300',\n 'BestPrice_Difference_ask_mean_450',\n 'bid_price1_mean_450',\n 'wap2_q50',\n 'price_mean',\n 'ask_price1_Price_Close_450',\n 'wap2_mean_450',\n 'wap2_mean_150',\n 'wap2_mean',\n 'bid_price1_mean_150',\n 'wap1_amax_450',\n 'wap1_q75_300',\n 'price_mean_300',\n 'price_amax_300',\n 'bid_price2_mean_450',\n 'wap1_mean_300',\n 'order_count_amin_450',\n 'order_count_amin',\n 'wap1_amin_150',\n 'order_count_amin_150',\n 'order_count_amin_300',\n 'wap1_mean_150'\n             }","fccfd980":"from sklearn.model_selection import GroupKFold\n\nimport lightgbm as lgb\n\n\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    # Hyperparammeters (just basic)\n    \n    #features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    features = [col for col in train.columns if col not in ColToExclude]\n    #features\n    # Create out of folds array\n    y = train['target']\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = GroupKFold(n_splits = 5)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train,groups=train['time_id'])):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=1500,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    #model.save_model('lgb.txt', num_iteration=model.best_iteration) \n    # Return test predictions\n    return test_predictions","dd78f515":"predictions_lgb_0= train_and_evaluate_lgb(train, test,params0)\n#predictions_lgb_1= train_and_evaluate_lgb(train1, test,params1)\n#predictions_lgb_2= train_and_evaluate_lgb(train2, test,params2)\n\n#test['target'] = predictions_lgb_0\n#test['predictions_lgb_1'] = predictions_lgb_1\n#test['predictions_lgb_2'] = predictions_lgb_2\n#test['target'] = np.where(test['realized_volatility_market1']<= 0.0098, test['predictions_lgb_1'], test['predictions_lgb_2'])\n\n#test[['row_id', 'target']].to_csv('submission.csv',index = False)","88215d87":"#Features Importance\n#model = lgb.Booster(model_file='lgb.txt')\n#importances = pd.DataFrame({'Feature': model.feature_name(), \n#                           'Importance': model.feature_importance(importance_type='gain')})\n#importances.sort_values(by = 'Importance', inplace=True)\n#importances2 = importances.nsmallest(300,'Importance', keep='first').sort_values(by='Importance', ascending=False)\n#importances2.Feature.tolist()","88131b18":"!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl\n","5b2d278a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","11137240":"import psutil\npsutil.cpu_count()","ac77adc4":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nprint(gpu_info)","1d4a40e8":"features = [col for col in train.columns if col not in ColToExclude]\nX = train\ny = train['target']\nX_test=test\n","6bf2b672":"gc.collect()\nprint('X shape: {}'.format(X.shape))\nprint('y shape: {}'.format(y.shape))","f5b03ae6":"nunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\n#for col in X.columns:\nfor col in features:\n    if  col == 'stock_id':\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(X_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        \n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]","c2f1e593":"tabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 16,\n    n_a = 16,\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (2e-2)),\n    mask_type = \"entmax\",\n    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 42,\n    verbose = 10\n    \n)","f6dd762d":"class RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) \/ y_true)))\n    \n\n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) \/ y_true) ** 2 )).clone()","06655058":"#kfold = KFold(n_splits = 5, random_state = 42, shuffle = True)\nkfold = GroupKFold(n_splits = 5)\n\n# Create out of folds array\noof_predictions = np.zeros((X.shape[0], 1))\ntest_predictions = np.zeros(X_test.shape[0])\n#feature_importances = pd.DataFrame()\n#feature_importances[\"feature\"] = X[features].columns.tolist()\n#stats = pd.DataFrame()\nexplain_matrices = []\nmasks_ =[]\n\n#for fold, (trn_ind, val_ind) in enumerate(kfold.split(X)):\nfor fold, (trn_ind, val_ind) in enumerate(kfold.split(X,groups=X['time_id'])):\n    print(f'Training fold {fold + 1}')\n    X_train, X_val = X[features].iloc[trn_ind].values, X[features].iloc[val_ind].values\n    y_train, y_val = y.iloc[trn_ind].values.reshape(-1,1), y.iloc[val_ind].values.reshape(-1,1)\n\n\n    clf =  TabNetRegressor(**tabnet_params)\n    clf.fit(\n      X_train, y_train,\n      eval_set=[(X_val, y_val)],\n      max_epochs = 200,\n      patience = 50,\n      batch_size = 1024*20, \n      virtual_batch_size = 128*20,\n      num_workers = 4,\n      drop_last = False,\n      eval_metric=[RMSPE],\n      loss_fn=RMSPELoss\n      )\n    \n    saving_path_name = f\".\/fold{fold}\"\n    saved_filepath = clf.save_model(saving_path_name)\n    \n    explain_matrix, masks = clf.explain(X_val)\n    explain_matrices.append(explain_matrix)\n    masks_.append(masks[0])\n    masks_.append(masks[1])\n      \n    oof_predictions[val_ind] = clf.predict(X_val)\n    test_predictions+=clf.predict(X_test[features].values).flatten()\/5\n    #feature_importances[f\"importance_fold{fold}+1\"] = clf.feature_importances_\n    \n    #stats[f'fold{fold+1}_train_rmspe']=clf.history['loss']\n    #stats[f'fold{fold+1}_val_rmspe']=clf.history['val_0_rmspe']\n    \nprint(f'OOF score across folds: {rmspe(y, oof_predictions.flatten())}')","181dd730":"test['target'] = (test_predictions*0.6) + (predictions_lgb_0*0.4)\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","ccc7b525":"# TabNet","0605873b":"# Data Pre-Processing\n","6beba6a7":"Defining some useful functions for later processing:","ca6d8f17":"## Train and test datasets","99ee15b5":"# Boosting Model","633891c2":"Loading libraries and the challenge data\n","ba4b14c4":"Applying the main function for extracting and processing the data\n"}}