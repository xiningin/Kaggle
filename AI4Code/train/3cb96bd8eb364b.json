{"cell_type":{"608bab05":"code","55071446":"code","e229e4e3":"code","497bdb41":"code","6403efdb":"code","ec51e7a8":"code","13bc30f0":"code","601e0333":"code","20a48538":"code","ad3e34a6":"code","03c4549f":"code","26c4c557":"code","c90a50a8":"code","57546ce0":"code","4a32c46b":"code","b09830c4":"code","b858c2e8":"code","13207205":"code","5fe3a5d0":"code","1024abb4":"code","dff844fc":"code","9e3632c4":"code","129efe71":"code","17f1de7f":"code","03607745":"code","be5c6bc1":"code","2f266e7e":"code","c19daea6":"code","c9f3db5f":"code","d2b440b1":"code","03bd3211":"code","0ad5a56e":"code","1668cd4f":"code","978c8a41":"code","f722f572":"code","396596d7":"code","a4dbb807":"code","ac872c7d":"code","6264cc30":"code","1a99ebe8":"code","0fe5339a":"code","72c8fefb":"code","2496b92e":"markdown","beecfa03":"markdown","3f038da9":"markdown","4dd01cc0":"markdown","f54f671f":"markdown","fcea1199":"markdown","86cbcaae":"markdown","fa94b608":"markdown","7f6139ef":"markdown","0ae534ff":"markdown","4b319801":"markdown","4572bca5":"markdown","ba36688b":"markdown","7a8ecef2":"markdown","e0c4ea37":"markdown","6d1353de":"markdown","0831c83f":"markdown","bd95e7cc":"markdown","0e84959e":"markdown","0352bf54":"markdown","cf2d7687":"markdown","e807becb":"markdown","9f1cbe6e":"markdown"},"source":{"608bab05":"# Prerequisites:\n# import requests","55071446":"# API connections go here:","e229e4e3":"# Prerequisites:\nimport pandas as pd\nimport numpy as np\n# from google.colab import files","497bdb41":"# Load dataframe from csv\n\n# New York City Laguardia Airport Station Data:\ndf = pd.read_csv(\"..\/input\/weather-underground-rain-predict\/datasets\/wu_ny.csv\", delimiter=';')\n\n# S\u00e3o Paulo International Airport Station Data:\n# df = pd.read_csv(\"..\/input\/weather-underground-rain-predict\/datasets\/wu_sp.csv\", delimiter=';')","6403efdb":"df.describe()","ec51e7a8":"print(df.dtypes)","13bc30f0":"print(df)","601e0333":"# 'N\/A' is translated to nan\ndf['Wind'].unique()","20a48538":"df['Condition'].unique()","ad3e34a6":"df.describe()","03c4549f":"from matplotlib import pyplot as plt\ndef show_raw_visualization(data):\n  data = data.drop(['Condition','Wind'], axis=1)\n  time_data = data['Time']\n  data_features = data.keys()\n  data_feature_count = len(data_features)\n  fig, axes = plt.subplots(\n    nrows=int(data_feature_count\/2.0+0.5), ncols=2, figsize=(10, 15), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n  )\n  colors = [\n    \"blue\",\n    \"orange\",\n    \"green\",\n    \"red\",\n    \"purple\",\n    \"brown\",\n    \"pink\",\n    \"gray\",\n    \"olive\",\n    \"cyan\",\n  ]\n  for i in range(len(data.keys())):\n    key = data.keys()[i]\n    c = colors[i % (len(colors))]\n    t_data = data[key]\n    t_data.index = time_data\n    t_data.head()\n    ax = t_data.plot(\n      ax=axes[i \/\/ 2, i % 2],\n      color=c,\n      title=\"{}\".format(key),\n      rot=25,\n      )\n    ax.legend([key])\n  plt.tight_layout()\n\nshow_raw_visualization(df)","26c4c557":"def show_heatmap(data):\n    plt.matshow(data.corr())\n    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)\n    plt.gca().xaxis.tick_bottom()\n    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)\n\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n    plt.show()\n\nshow_heatmap(df.drop(['Condition','Wind'], axis=1))","c90a50a8":"df1 = df.copy(deep=True)","57546ce0":"df1 = df1.dropna()","4a32c46b":"# General data restriction\n# Any numerical data that equals 0.0 is probably a\n# default value for missing data at the server\n\ndf1 = df1[df1['Time'] > 0]\n\ndf1 = df1[df1['Temperature'] > -100]\n\ndf1 = df1[df1['Dew Point'] > -100]\n\ndf1 = df1[df1['Humidity'] >= 0]\n\n# Purged at `.dropna()`\n# df1 = df1[df1['Wind']]\n\ndf1 = df1[df1['Wind Speed'] >= 0]\n\ndf1 = df1[df1['Wind Gust'] >= 0]\n\ndf1 = df1[df1['Pressure'] >= 10]\n\ndf1 = df1[df1['Precip.'] >= 0]\n\n# Purged at `.dropna()`\n# df1 = df1[df1['Condition']]","b09830c4":"## SP specific restrictions:\n\n# We got no freezing temps\n# df1 = df1[df1['Temperature'] > 0]\n# df1 = df1[df1['Dew Point'] > 0]\n\n# Even with variable Atmospheric pressure, the altitude will keep it higher\n# than some threshold (here 700hPa)\n# df1 = df1[df1['Pressure'] > 700]\n\n\n## NYC specific restrictions:\n\n# Removes 0\u00b0F (default value for temp and dew point)\ndf1 = df1.loc[(df1['Temperature'] <= -17.77778) | (df1['Temperature'] >= -17.77776)]\ndf1 = df1.loc[(df1['Dew Point'] <= -17.77778) | (df1['Dew Point'] >= -17.77776)]\n\n# Even with variable Atmospheric pressure, the altitude will keep it higher\n# than some threshold (here 800hPa)\ndf1 = df1[df1['Pressure'] > 800]\n","b858c2e8":"dropped_data_ratio = 1 - df1['Condition'].count() \/ df['Condition'].count()\nprint(\"Dropped datapoints:\", dropped_data_ratio * 100, '%')","13207205":"# Cleaned dataframe\nshow_raw_visualization(df1)","5fe3a5d0":"# Translate from Epoch\ndf1['TimeAgg'] = pd.to_datetime(df1['Time'],unit='s')","1024abb4":"# \"has_rain definition\"\n\n# If it has precipitation -> it has rain\ndef yprecipitations(preciptation):\n  return preciptation > 0\n\n# If it has \"Drizzle\", \"Rain\", \"Shower\" or \"Storm\" in its Condition -> it has rain\ndef yconditions(condition):\n  import re\n  if re.match(\".*(Drizzle|Rain|Shower|Storm).*\", condition):\n    return 1\n  else:\n    return 0\n\n# Checks whether the dataframe has at least 1% of precipitation field filled\nif df1[df1[\"Precip.\"] > 0]['Precip.'].count() \/ df1['Condition'].count() > 0.01:\n  target_field = 'Precip.'\n  threshold_func = yprecipitations\n  print(\"Using Preciptation feature\")\n\n# Otherwise, we need to inspect the conditions\nelse:\n  target_field = 'Condition'\n  threshold_func = yconditions\n  print(\"Using Condition feature\")\n\nlabelfunc = np.vectorize(threshold_func)\nrain_class = labelfunc(df1[[target_field]])\ndf1[['has_rain']] = rain_class","dff844fc":"rain_ratio = df1[df1['has_rain'] == True]['has_rain'].count() \/ df1['has_rain'].count()\nprint('It rains %.2f%% of the time' %(100*rain_ratio))","9e3632c4":"df1.set_index('TimeAgg',inplace=True, drop=True)\nprint(df1)","129efe71":"df6hp = df1[['Temperature',\n             'Humidity',\n             'Pressure',\n             'Precip.',\n             'has_rain']].resample('6h').agg({'min',\n                                              'max',\n                                              'mean', \n                                              lambda x: x.quantile(0.7), \n                                              lambda y: y.quantile(0.9),\n                                              lambda z: z.quantile(0.3),\n                                              lambda o: o.quantile(0.1),\n                                              lambda a: (a > a.shift()).sum(),\n                                              lambda b: (b < b.shift()).sum(),\n                                              lambda c: (c == c.shift()).sum()})\n\n# MIN, MAX, MEAN, 70PERCENTILE, 90PERCENTILE, 30PERCENTILE, 10PERCENTILE, RISE, FALL, STEADY","17f1de7f":"df6hp.columns = [\"_\".join(x) for x in df6hp.columns.ravel()]\n# Selecting has_rain_max as target variable\ndf6hp['has_rain'] = df6hp['has_rain_max']\ndf6hp.dtypes","03607745":"# Data enhancing\n# For each feature and \"lambda\" feature, group the last 8 measurements,\n# by shifting its data: Feature_steady_(N) = Feature_steady_(N-1).shift()\n\nfor feature in ['Pressure', 'Temperature', 'Humidity', 'Precip.']:\n  df6hp['%s_steady_1' % (feature)] = df6hp['%s_mean' % (feature)].shift()\n  df6hp['%s_min_steady_1' % (feature)] = df6hp['%s_min' % (feature)].shift()\n  df6hp['%s_max_steady_1' % (feature)] = df6hp['%s_max' % (feature)].shift()\n  for i in range(7):\n    df6hp['%s_steady_%d' % (feature, i+2)] = df6hp['%s_steady_%d' % (feature, i+1)].shift()\n    df6hp['%s_min_steady_%d' % (feature, i+2)] = df6hp['%s_min_steady_%d' % (feature, i+1)].shift()\n    df6hp['%s_max_steady_%d' % (feature, i+2)] = df6hp['%s_max_steady_%d' % (feature, i+1)].shift()\n  for j in range(7):\n    df6hp['%s_<lambda_%d>_1' % (feature, j)] = df6hp['%s_<lambda_%d>' % (feature, j)].shift()\n    for i in range(7):\n      df6hp['%s_<lambda_%d>_%d' % (feature, j, i+2)] = df6hp['%s_<lambda_%d>_%d' % (feature, j, i+1)].shift()\n","be5c6bc1":"df6hp.keys()","2f266e7e":"df6hp_old = df6hp.copy(deep=True)\n\n# 2nd Data clean, removing NaNs generated from grouping\ndf6hp = df6hp.dropna()\n\n# This column was dropped at df6hp definition\n# df6hp = df6hp.drop(columns=['TimeAgg'])\n\n# Features that have not been grouped are removed\nnon_agg_feat_list = list()\nfor feature in ['Pressure', 'Temperature', 'Humidity', 'Precip.','has_rain']:\n  non_agg_feat_list.append(\"%s_mean\" % (feature))\n  non_agg_feat_list.append(\"%s_min\" % (feature))\n  non_agg_feat_list.append(\"%s_max\" % (feature))\n  for i in range(7):\n    non_agg_feat_list.append(\"%s_<lambda_%d>\" % (feature, i))\n# print(non_agg_feat_list)\ndf6hp.drop(columns=non_agg_feat_list, inplace=True)\n\ndropped_data_ratio = 1 - df6hp['has_rain'].count() \/ df6hp_old['has_rain'].count()\nprint(\"Dropped datapoints:\", dropped_data_ratio * 100, \"%\")","c19daea6":"# Export the enhaced dataset\ndf6hp.to_csv(r'dfFinal.csv')\n# files.download(\"dfFinal.csv\")","c9f3db5f":"# Prerequisites\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler\n# from sklearn.externals import joblib\nimport pandas as pd","d2b440b1":"df6hp = pd.read_csv(\"dfFinal.csv\")\n\n# Drop unecessary index\ndf6hp = df6hp.drop(columns=['TimeAgg'])","03bd3211":"# Splitting the data in train and test data\nmain_data, test_data = train_test_split(df6hp, test_size=.65) ","0ad5a56e":"# Split each of the train and test data into inputs and outputs\nX = main_data.iloc[:, 1:].values\ny = main_data.iloc[:, 0].values\n\nX_test = test_data.iloc[:, 1:].values\ny_test = test_data.iloc[:, 0].values","1668cd4f":"# Rescale the data so it's between 0 and 1\nscaler = StandardScaler()\nscaler_test = StandardScaler()\n\nX = scaler.fit_transform(X) \nX_test = scaler.fit_transform(X_test) ","978c8a41":"# Prerequisites:\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout ","f722f572":"input_dimensions = X.shape[1] \noptimizer = 'rmsprop'\ndropout = 0.05\nkernel_init = 'uniform'\nbatch_size = 100\nepochs = 100 ","396596d7":"# from https:\/\/create.arduino.cc\/projecthub\/danionescu\/diy-rain-prediction-using-arduino-python-and-keras-abdec6\nmodel = Sequential()\ninner_nodes = int(input_dimensions \/ 2)\n\nmodel.add(Dense(inner_nodes, kernel_initializer='uniform', activation='relu', input_dim=input_dimensions))\nmodel.add(Dropout(rate=dropout))\n\nmodel.add(Dense(inner_nodes, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dropout(rate=dropout))\n\nmodel.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\nmodel.compile(optimizer=optimizer, loss='mean_absolute_error', metrics=['accuracy']) ","a4dbb807":"model.fit(X,\n          y,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(X_test, y_test))","ac872c7d":"# save the model for later use\nmodel.save('Model1.h5')\n# files.download(\"Model1.h5\")","6264cc30":"import matplotlib.pyplot as plt\n\nplt.plot(model.history.history['accuracy'], 'r-')\nplt.plot(model.history.history['val_accuracy'], 'g:')\n\nplt.xlabel('Epoch')\n\nplt.legend(['Train', 'Test'], loc='upper left')","1a99ebe8":"# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(X_test, y_test, batch_size=128)\nprint(\"test loss, test acc:\", results)","0fe5339a":"# Generate predictions (probabilities -- the output of the last layer)\n# on new data using `predict`\nprint(\"Generate predictions for 3 samples\")\npredictions = model.predict(X_test[:20])\nprint(\"predictions shape:\", predictions.shape)","72c8fefb":"print(predictions)\nprint(y_test[:20])","2496b92e":"##### Correlation Heatmap","beecfa03":"### Data Summary","3f038da9":"### Save Model","4dd01cc0":"## Model data input prepare","f54f671f":"## Preprocess Data","fcea1199":"### Definitions","86cbcaae":"# TF - toyproject","fa94b608":"## Evaluate Model","7f6139ef":"### Data Grouping and Enhacing","0ae534ff":"## Project","4b319801":"## Collect Data\n(try to store as many data as possible to save quota)\n\n**As we had previously collected the data and stored it in IME's network,\nthis step isn't needed**","4572bca5":"### Repository\nThis project source code and datasets may be found in our [gitlab](https:\/\/gitlab.com\/carybe\/fiot-tf) and [github](https:\/\/github.com\/carybe\/fiot-tf) (mirror) repositories.","ba36688b":"### Model construction","7a8ecef2":"## Train Models","e0c4ea37":"### Model Training","6d1353de":"#### Categorical","0831c83f":"#### Dataframe Viz","bd95e7cc":"## Data Explore\n","0e84959e":"## References:","0352bf54":"\n - [DIY Rain Prediction Using Arduino, Python and Keras](https:\/\/create.arduino.cc\/projecthub\/danionescu\/diy-rain-prediction-using-arduino-python-and-keras-abdec6)\n - [Keras Timeseries forecasting for weather prediction](https:\/\/keras.io\/examples\/timeseries\/timeseries_weather_forecasting\/)\n\n - [TensorFlow Keras Tutorial](https:\/\/www.tensorflow.org\/tutorials\/keras\/classification)","cf2d7687":"### Data Clean","e807becb":"### Load Saved data","9f1cbe6e":"#### Numerical"}}