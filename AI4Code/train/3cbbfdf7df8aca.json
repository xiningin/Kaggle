{"cell_type":{"033301ed":"code","9f2101e3":"code","421d1c0f":"code","4a8c85f3":"code","82b08b1e":"code","7034e601":"code","89d34306":"code","fd0c2e83":"code","7d59d4a6":"code","1fa879fd":"code","6c14ef64":"code","ac8cd7e5":"code","304f8c83":"code","ed8899af":"code","f7d31f07":"code","a1317625":"code","532b0f3e":"code","96b82207":"code","a8754211":"code","b6085c38":"code","f4f224a6":"code","70df3481":"code","d33b7c53":"code","0f235632":"code","89d99423":"code","2e3eaf45":"code","3e9829da":"code","b1b927aa":"code","2ac7a5f9":"code","c69c1f22":"code","b7da7b87":"code","409e066c":"code","a2c579fc":"code","cfce8122":"code","f05380f9":"code","ace1b690":"code","7f8a8438":"code","b339e420":"code","907dfbf8":"code","4d130cdd":"code","52d17746":"code","442b5802":"code","3d72fba2":"code","ac856945":"code","a6b3c289":"code","53873d44":"code","223b238a":"code","a070dff9":"code","802a6aeb":"code","4c362de1":"code","fa8bf357":"code","276ef2f5":"code","9824aa0b":"code","097ed2eb":"code","ac61b498":"code","2ac9450b":"code","49963f10":"code","60bb2f84":"code","942f8d82":"code","30d7aae3":"code","af1588ea":"code","b2f52afa":"code","9eec146f":"code","04feed31":"code","ab5a178b":"code","9fe91792":"code","39eb8f66":"markdown","f1e985a6":"markdown"},"source":{"033301ed":"### import relevant libraries","9f2101e3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","421d1c0f":"df=pd.read_csv(\"..\/input\/fake-news\/train (2).csv\")","4a8c85f3":"df.head()","82b08b1e":"df.shape","7034e601":"df.isnull().sum()","89d34306":"## drop your missing values\ndf.dropna(inplace=True)","fd0c2e83":"df.shape","7d59d4a6":"df.dtypes","1fa879fd":"df['label']=df['label'].astype(str)","6c14ef64":"df.dtypes","ac8cd7e5":"df.head(20)","304f8c83":"messages=df.copy()","ed8899af":"# we reset index, because when you drop the rows the index got deleted as 6 and 8th so to make it in a order , we have to use reset_index\n\nmessages.reset_index(inplace=True)","f7d31f07":"messages.head(10)","a1317625":"messages.drop(['index','id'],axis=1,inplace=True)","532b0f3e":"messages.head()","96b82207":"#note we will consider only title for pre-processing","a8754211":"data=messages['title'][0]\ndata","b6085c38":"import re","f4f224a6":"re.sub('[^a-zA-Z]',' ', data)","70df3481":"data=data.lower()\ndata","d33b7c53":"list=data.split()\nlist","0f235632":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords","89d99423":"ps=PorterStemmer()","2e3eaf45":"review=[ps.stem(word) for word in list if word not in set(stopwords.words('english'))]\nreview","3e9829da":"review=[]\nfor word in list:\n    if word not in set(stopwords.words('english')):\n        review.append(ps.stem(word))\nreview","b1b927aa":"' '.join(review)","2ac7a5f9":"### lets do same task for each & every row","c69c1f22":"corpus=[]\nsentences=[]\nfor i in range(0,len(messages)):\n    review=re.sub('[^a-zA-Z]',' ', messages['title'][i])\n    review=review.lower()\n    list=review.split()\n    review=[ps.stem(word) for word in list if word not in set(stopwords.words('english'))]\n    sentences=' '.join(review)\n    corpus.append(sentences)","b7da7b87":"corpus[0]","409e066c":"corpus","a2c579fc":"len(corpus)","cfce8122":"## Applying Countvectorizer\n# Creating the Bag of Words model\n\nfrom sklearn.feature_extraction.text import CountVectorizer","f05380f9":"## max_features=5000, it means I just need top 5000 features \n#example ABC News is basically 2 words,so ingram,i have Given (1,3),so it will take the combination of 1 word,then 2 words \n#then 3 words\n\ncv=CountVectorizer(max_features=5000,ngram_range=(1,3))","ace1b690":"X=cv.fit_transform(corpus).toarray()","7f8a8438":"X.shape\n#ie we get 5000 features now","b339e420":"X","907dfbf8":"cv.get_feature_names()[0:20]","4d130cdd":"messages.columns","52d17746":"y=messages['label']","442b5802":"#spliting the dataset into Train and Test\nfrom sklearn.model_selection import train_test_split","3d72fba2":"X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25, random_state=42)","ac856945":"X_test","a6b3c289":"X_test.shape","53873d44":"#using the multinomial ALgorithm\n\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier=MultinomialNB()","223b238a":"classifier.fit(X_train,y_train)","a070dff9":"pred=classifier.predict(X_test)\npred","802a6aeb":"from sklearn import metrics","4c362de1":"metrics.accuracy_score(y_test,pred)","fa8bf357":"cm=metrics.confusion_matrix(y_test,pred)\ncm","276ef2f5":"### make your confusion amtrix more user-friendly\n\nplt.imshow(cm,interpolation='nearest',cmap=plt.cm.Blues)\nplt.colorbar()\nplt.title('Confusion Matrix')\nlabels=['positive','negative']\ntick_marks=np.arange(len(labels))\nplt.xticks(tick_marks,labels)\nplt.yticks(tick_marks,labels)","9824aa0b":"labels=['positive','negative']\nnp.arange(len(labels))","097ed2eb":"def plot_confusion_matrix(cm):\n    plt.imshow(cm,interpolation='nearest',cmap=plt.cm.Blues)\n    plt.colorbar()\n    plt.title('Confusion Matrix')\n    labels=['positive','negative']\n    tick_marks=np.arange(len(labels))\n    plt.xticks(tick_marks,labels)\n    plt.yticks(tick_marks,labels)","ac61b498":"plot_confusion_matrix(cm)","2ac9450b":"#this algorithm learns from massive streams of data ","49963f10":"from sklearn.linear_model import PassiveAggressiveClassifier","60bb2f84":"linear_clf=PassiveAggressiveClassifier()","942f8d82":"linear_clf.fit(X_train,y_train)","30d7aae3":"predictions=linear_clf.predict(X_test)","af1588ea":"metrics.accuracy_score(y_test,predictions)","b2f52afa":"cm2=metrics.confusion_matrix(y_test,predictions)\ncm2","9eec146f":"plot_confusion_matrix(cm2)","04feed31":"## Get Features names\n#to detect which fake and which is most real word\n\nfeature_names=cv.get_feature_names()","ab5a178b":"#most negative value is most fake word,if we go towards lower value in -ve,ie we have most fake value\nclassifier.coef_[0]","9fe91792":"# Most 20 real values\nsorted(zip(classifier.coef_[0],feature_names),reverse=True)[0:20]","39eb8f66":"###  MultinomialNB Algo","f1e985a6":"### Passive Aggressive Classifier Algorithm"}}