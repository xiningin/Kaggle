{"cell_type":{"a4dfb672":"code","2e2d034c":"code","a9885b52":"code","e6655fd4":"code","b0ae67bd":"code","1917265e":"code","066bb040":"code","2d5ba0a4":"code","60374cb2":"code","6f1143ad":"markdown","66e3b26c":"markdown","67cb2b13":"markdown","f2b7b9e0":"markdown"},"source":{"a4dfb672":"# requires Internet Access.\n!git clone https:\/\/github.com\/DeNA\/HandyRL.git\n!pip install -r HandyRL\/requirements.txt\n!pip install -r HandyRL\/handyrl\/envs\/kaggle\/requirements.txt","2e2d034c":"%%writefile config.yaml\nenv_args:\n    #env: 'TicTacToe'\n    #env: 'Geister'\n    env: 'HungryGeese'\n    source: 'handyrl.envs.kaggle.hungry_geese'\n    #env: 'handyrl.envs.parallel_tictactoe'  # specify by path\n\ntrain_args:\n    turn_based_training: False\n    observation: True\n    gamma: 0.8\n    forward_steps: 32\n    compress_steps: 4\n    entropy_regularization: 2.0e-3\n    entropy_regularization_decay: 0.3\n    update_episodes: 500\n    batch_size: 128\n    minimum_episodes: 10000\n    maximum_episodes: 80000\n    epochs: 50 # Set more epochs to learn more. -1 for running forever.\n    num_batchers: 4\n    eval_rate: 0.1\n    worker:\n        num_parallel: 4\n    lambda: 0.7\n    policy_target: 'TD' # 'UPGO' 'VTRACE' 'TD' 'MC'\n    value_target: 'TD' # 'VTRACE' 'TD' 'MC'\n    seed: 0\n    restart_epoch: 0\n","a9885b52":"%run HandyRL\/main.py --train","e6655fd4":"!ls models","b0ae67bd":"import torch\nimport pickle\nimport bz2\nimport base64\n\nstate_dict = torch.load('models\/latest.pth') # using latest.pth, you could also use i.pth (i=0, 1, 2, ...)\n# Save model parameters as base64 for submission\nPARAM = base64.b64encode(bz2.compress(pickle.dumps(state_dict)))\n\n# Save param\n# with open(\"PARAM.txt\", \"w\") as f:\n#     f.write(repr(PARAM))\n\nPARAM = repr(PARAM) # prepend `b'` and append `'`\nprint(PARAM[:10] + \"...\" + PARAM[-10:])","1917265e":"%%writefile submission.py\n\n# This is a lightweight ML agent trained by self-play.\n# https:\/\/github.com\/DeNA\/HandyRL\n# copied from https:\/\/www.kaggle.com\/yuricat\/smart-geese-trained-by-reinforcement-learning\n\n\nimport pickle\nimport bz2\nimport base64\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# Neural Network for Hungry Geese\n\nclass TorusConv2d(nn.Module):\n    def __init__(self, input_dim, output_dim, kernel_size, bn):\n        super().__init__()\n        self.edge_size = (kernel_size[0] \/\/ 2, kernel_size[1] \/\/ 2)\n        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=kernel_size)\n        self.bn = nn.BatchNorm2d(output_dim) if bn else None\n\n    def forward(self, x):\n        h = torch.cat([x[:,:,:,-self.edge_size[1]:], x, x[:,:,:,:self.edge_size[1]]], dim=3)\n        h = torch.cat([h[:,:,-self.edge_size[0]:], h, h[:,:,:self.edge_size[0]]], dim=2)\n        h = self.conv(h)\n        h = self.bn(h) if self.bn is not None else h\n        return h\n\n\nclass GeeseNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        layers, filters = 12, 32\n        self.conv0 = TorusConv2d(17, filters, (3, 3), True)\n        self.blocks = nn.ModuleList([TorusConv2d(filters, filters, (3, 3), True) for _ in range(layers)])\n        self.head_p = nn.Linear(filters, 4, bias=False)\n        self.head_v = nn.Linear(filters * 2, 1, bias=False)\n\n    def forward(self, x):\n        h = F.relu_(self.conv0(x))\n        for block in self.blocks:\n            h = F.relu_(h + block(h))\n        h_head = (h * x[:,:1]).view(h.size(0), h.size(1), -1).sum(-1)\n        h_avg = h.view(h.size(0), h.size(1), -1).mean(-1)\n        p = self.head_p(h_head)\n        v = torch.tanh(self.head_v(torch.cat([h_head, h_avg], 1)))\n\n        return {'policy': p, 'value': v}\n\n\n# Input for Neural Network\n\ndef make_input(obses):\n    b = np.zeros((17, 7 * 11), dtype=np.float32)\n    obs = obses[-1]\n\n    for p, pos_list in enumerate(obs['geese']):\n        # head position\n        for pos in pos_list[:1]:\n            b[0 + (p - obs['index']) % 4, pos] = 1\n        # tip position\n        for pos in pos_list[-1:]:\n            b[4 + (p - obs['index']) % 4, pos] = 1\n        # whole position\n        for pos in pos_list:\n            b[8 + (p - obs['index']) % 4, pos] = 1\n            \n    # previous head position\n    if len(obses) > 1:\n        obs_prev = obses[-2]\n        for p, pos_list in enumerate(obs_prev['geese']):\n            for pos in pos_list[:1]:\n                b[12 + (p - obs['index']) % 4, pos] = 1\n\n    # food\n    for pos in obs['food']:\n        b[16, pos] = 1\n\n    return b.reshape(-1, 7, 11)\n\n\n# Load PyTorch Model\n\nPARAM = %PARAM%\n\nstate_dict = pickle.loads(bz2.decompress(base64.b64decode(PARAM)))\nmodel = GeeseNet()\nmodel.load_state_dict(state_dict)\nmodel.eval()\n\n\n# Main Function of Agent\n\nobses = []\n\ndef agent(obs, _):\n    obses.append(obs)\n    x = make_input(obses)\n    with torch.no_grad():\n        xt = torch.from_numpy(x).unsqueeze(0)\n        o = model(xt)\n    p = o['policy'].squeeze(0).detach().numpy()\n\n    actions = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    return actions[np.argmax(p)]","066bb040":"with open('submission.py', 'r+') as f:\n    agent = f.read()\n    f.seek(0.0)\n    f.write(agent.replace('%PARAM%', PARAM))","2d5ba0a4":"from kaggle_environments import make\nenv = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run(['greedy', 'greedy', 'submission.py', 'submission.py'])\nenv.render(mode=\"ipython\", width=800, height=700)","60374cb2":"env.state","6f1143ad":"Place this file where you run main.py\n\n`config.yaml`\n```config.yaml\n\nenv_args:\n    #env: 'TicTacToe'\n    #env: 'Geister'\n    env: 'HungryGeese'\n    source: 'handyrl.envs.kaggle.hungry_geese'\n    #env: 'handyrl.envs.parallel_tictactoe'  # specify by path\n\ntrain_args:\n    turn_based_training: False\n    observation: True\n    gamma: 0.8\n    forward_steps: 32\n    compress_steps: 4\n    entropy_regularization: 2.0e-3\n    entropy_regularization_decay: 0.3\n    update_episodes: 500\n    batch_size: 128\n    minimum_episodes: 10000\n    maximum_episodes: 50000\n    epochs: 10 # increase as needed\n    num_batchers: 2\n    eval_rate: 0.1\n    worker:\n        num_parallel: 6\n    lambda: 0.7\n    policy_target: 'TD' # 'UPGO' 'VTRACE' 'TD' 'MC'\n    value_target: 'TD' # 'VTRACE' 'TD' 'MC'\n    seed: 0\n    restart_epoch: 0\n\n\n```\nRefer to https:\/\/github.com\/DeNA\/HandyRL\/blob\/master\/docs\/parameters.md for meanings\n\nRefer to https:\/\/www.kaggle.com\/c\/hungry-geese\/discussion\/218190 for publicly available models and parameters\n\nchange parallel processing setting according to HW limitations.","66e3b26c":"Train using\n\n```bash\npython3 main.py --train\n```\n\nModels are saved at `models\/*.pth`\n\nBy default, models are evaluated against random player. You may change this to GreedyAgent.\n\n","67cb2b13":"Install libraries\n```bash\ngit clone https:\/\/github.com\/DeNA\/HandyRL.git\ncd HandyRL\npip3 install -r requirements.txt\npip3 install -r handyrl\/envs\/kaggle\/requirements.txt\n```","f2b7b9e0":"The content of this Code is mainly borrowed from https:\/\/www.kaggle.com\/yuricat\/smart-geese-trained-by-reinforcement-learning \n\nThank you for sharing this code public!"}}