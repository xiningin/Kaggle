{"cell_type":{"956cc94a":"code","9ad369e6":"code","ffc32eb4":"code","1f59aac0":"code","fac3c9ec":"code","707100d4":"code","b9d8371a":"code","6a6c61b9":"code","75a97928":"code","a78fdb75":"code","d69743cd":"code","471eb4f7":"code","85723cf9":"markdown","bc1bf44e":"markdown","12d5b0ee":"markdown","4bee9ac5":"markdown","4b4d5167":"markdown","a8d1b4d0":"markdown","1224aaeb":"markdown","dad3fee8":"markdown","f99b1da3":"markdown"},"source":{"956cc94a":"import os\nimport math\nimport pandas as pd\nimport numpy as np\nfrom collections import defaultdict\nfrom scipy.stats import boxcox\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\nfrom statsmodels.tsa.filters.hp_filter import hpfilter\nfrom matplotlib import pyplot as plt\n\nplt.style.use('fivethirtyeight') \nplt.rcParams['xtick.labelsize'] = 20\nplt.rcParams['ytick.labelsize'] = 20\n\n%matplotlib inline ","9ad369e6":"path = '..\/input\/csvs_per_year\/csvs_per_year'\nfiles = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.csv')]\ndf = pd.concat((pd.read_csv(file) for file in files), sort=False)\ndf = df.groupby(['date']).agg('mean')\ndf.index = pd.DatetimeIndex(data= df.index)","ffc32eb4":"'''\n['CH4', 'MXY', 'NO', 'NOx', 'OXY', 'PM25', 'PXY'] \nHave been removed because they were missing large potions of data\n'''\ncol_list = ['BEN', 'CO', 'EBE', 'NMHC', 'NO_2', 'O_3', 'PM10', 'SO_2', 'TCH', 'TOL']\nmonthly_df = df.resample('M').mean()\ndaily_df = df.resample('D').mean()\n\nplt_monthly = monthly_df[col_list]\nplt_monthly.plot(figsize=(15, 10))\nplt.title('Madrid Pollutants from 2001-2019', fontsize=25)\nplt.legend(loc='upper left')\nplt.show()","1f59aac0":"def adf_test(timeseries):\n    print ('Results of Dickey-Fuller Test:')\n    print('Null Hypothesis: Unit Root Present')\n    print('Test Statistic < Critical Value => Reject Null')\n    print('P-Value =< Alpha(.05) => Reject Null\\n')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput[f'Critical Value {key}'] = value\n    print (dfoutput, '\\n')\n\ndef kpss_test(timeseries, regression='c'):\n    # Whether stationary around constant 'c' or trend 'ct\n    print ('Results of KPSS Test:')\n    print('Null Hypothesis: Data is Stationary\/Trend Stationary')\n    print('Test Statistic > Critical Value => Reject Null')\n    print('P-Value =< Alpha(.05) => Reject Null\\n')\n    kpsstest = kpss(timeseries, regression=regression)\n    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n    for key,value in kpsstest[3].items():\n        kpss_output[f'Critical Value {key}'] = value\n    print (kpss_output, '\\n')","fac3c9ec":"sta_so2_df = pd.DataFrame(monthly_df.SO_2)\nsta_so2_df['boxcox_SO_2'], lamda = boxcox(sta_so2_df.SO_2)\nsta_so2_df.plot(figsize=(13, 5), title='SO_2 Original and Box-Cox Transformed')\nplt.show()\nprint('Box-Cox Lambda Value: ', lamda)\nsta_so2_df.plot(figsize=(10, 5), title= 'SO_2 Distribution Before and After Box-Cox', kind='KDE')\nplt.show()\nsta_pm10_df = pd.DataFrame(monthly_df.PM10)\nsta_pm10_df['boxcox_PM10'], pm_lamda = boxcox(sta_pm10_df.PM10)\nsta_pm10_df.plot(figsize=(13, 5), title='PM10 Original and Box-Cox Transformed')\nplt.show()\nprint('Box-Cox Lambda Value: ', pm_lamda)\nsta_pm10_df.plot(figsize=(10, 5), title= 'PM10 Distribution Before and After Box-Cox', kind='KDE')\nplt.show()","707100d4":"sta_so2_df['SO_diff1'] = sta_so2_df['boxcox_SO_2'].diff()\nsta_so2_df.SO_diff1.dropna(inplace=True)\nadf_test(sta_so2_df.SO_diff1)\nkpss_test(sta_so2_df.SO_diff1)\nsta_so2_df[['SO_diff1', 'boxcox_SO_2']].plot(figsize=(13, 10), title='SO_2 Before and After Differencing')\nsta_so2_df[['SO_diff1', 'SO_2']].plot(figsize=(13, 10), title='SO_2 Original and Stationary')","b9d8371a":"sta_pm10_df['PM10_diff1'] = sta_pm10_df['boxcox_PM10'].diff()\nsta_pm10_df.PM10_diff1.dropna(inplace=True)\nadf_test(sta_pm10_df.PM10_diff1)\nkpss_test(sta_pm10_df.PM10_diff1)\nsta_pm10_df[['PM10_diff1', 'boxcox_PM10']].plot(figsize=(13, 10), title='PM10 Before and After Differencing')\nsta_pm10_df[['PM10_diff1', 'PM10']].plot(figsize=(13, 10), title='PM10 Original and Stationary')","6a6c61b9":"acf1 = plot_acf(sta_so2_df['SO_2'])\nacf2 = plot_acf(sta_so2_df['SO_diff1'])\nacf1.suptitle('Autocorrelation SO_2 Original and Stationary', fontsize=25)\nacf1.set_figheight(10)\nacf1.set_figwidth(15)\nacf2.set_figheight(10)\nacf2.set_figwidth(15)\nplt.show()\npacf1 = plot_pacf(sta_so2_df['SO_2'], lags=50)\npacf2 = plot_pacf(sta_so2_df['SO_diff1'], lags=50)\npacf1.suptitle('Partial Autocorrelation SO_2 Original and Stationary', fontsize=25)\npacf1.set_figheight(10)\npacf1.set_figwidth(15)\npacf2.set_figheight(10)\npacf2.set_figwidth(15)\nplt.show()","75a97928":"SMA_df= pd.DataFrame(monthly_df.NO_2)\nfor n in range(1, 6):\n    label = 'window_'+str(n*3)\n    SMA_df[label] = SMA_df['NO_2'].rolling(window=n*3).mean()\n\nSMA_df.plot(figsize=(10, 5), title='NO_2 Moving Averages')\nplt.show()\n\nerror = {}\nfor col in SMA_df.columns[1:]:\n    series = SMA_df[['NO_2', col]]\n    series = series.dropna(axis=0)\n    error[col] = mean_squared_error(series['NO_2'], series[col]) \n    print(f'Moving Average {col} MSE: ', round(error[col], 2))\n\nmetadata = {'mean':[], 'variance':[], 'skew':[], 'kurtosis':[]}\nfor col in SMA_df:\n    metadata['mean'].append(SMA_df[col].mean())\n    metadata['variance'].append(SMA_df[col].var())\n    metadata['skew'].append(SMA_df[col].skew())\n    metadata['kurtosis'].append(SMA_df[col].kurt())\n    \nplt.figure(figsize=(10,10))\nplt.suptitle('Changes in Series Distribution as Window Size Increases')\nplt.subplot(221)\nplt.title('NO2 Mean')\nplt.plot(metadata['mean'])\nplt.subplot(222)\nplt.title('NO2 Variance')\nplt.plot(metadata['variance'])\nplt.subplot(223)\nplt.title('NO2 Skewness')\nplt.plot(metadata['skew'])\nplt.subplot(224)\nplt.title('NO2 Kurtosis')\nplt.plot(metadata['kurtosis'])\nplt.show()\n\nplt.figure(figsize=(10, 15))\nplt.subplot(311)\nplt.title('Original Time Series Distribution')\nSMA_df.NO_2.plot(kind='kde')\nplt.subplot(312)\nplt.title('Moving Average 6 Time Series Distribution')\nSMA_df['window_6'].plot(kind='kde')\nplt.subplot(313)\nplt.title('Moving Average 15 Time Series Distribution')\nSMA_df['window_15'].plot(kind='kde')\nplt.show()","a78fdb75":"smoothing_df = pd.DataFrame(monthly_df.O_3)\nsmoothing_df.plot(figsize=(10, 5), title='Original Monthly O_3 Time Series')\nplt.show()\nes_simple = SimpleExpSmoothing(smoothing_df.O_3).fit()\nes_double = ExponentialSmoothing(smoothing_df.O_3, trend= 'add', damped= True, seasonal= None, seasonal_periods= None).fit()\nes_triple = ExponentialSmoothing(smoothing_df.O_3, trend= 'add', damped= True, seasonal= 'mul', seasonal_periods= 12).fit()\n\nplt.figure(figsize=(10, 5))\nes_simple.fittedvalues.plot(color=[(230\/255,159\/255,0)], legend=True, title='Exponentially Smoothed and Forecast O_3')\nes_simple.forecast(36).plot(color=[(230\/255,159\/255,0)], style='-.')\nes_double.fittedvalues.plot(color=[(0,158\/255,115\/255)])\nes_double.forecast(36).plot(color=[(0,158\/255,115\/255)], style='-.')\nes_triple.fittedvalues.plot(color=[(0,114\/255,178\/255)])\nes_triple.forecast(36).plot(color=[(0,114\/255,178\/255)], style='-.')\n\nplt.legend(['ses', 'ses forecast', 'double exp', 'double exp forecast', \n            'triple exp', 'triple exp forecast'])\nplt.show()\n\nsmoothing_df['simple_es'] = es_simple.fittedvalues\nsmoothing_df['double_es'] = es_double.fittedvalues\nsmoothing_df['triple_es'] = es_triple.fittedvalues\nprint('Simple Exponential Smoothing MSE: ', round(es_simple.sse\/len(es_simple.fittedvalues), 2))\nprint('Double Exponential Smoothing MSE: ', round(es_double.sse\/len(es_double.fittedvalues), 2))\nprint('Triple Exponential Smoothing MSE: ', round(es_triple.sse\/len(es_triple.fittedvalues), 2))\n\nplt.figure(figsize=(10, 20))\nplt.subplot(411)\nplt.title('Original Monthly O_3 Distribution')\nsmoothing_df.O_3.plot(kind='kde')\nplt.subplot(412)\nplt.title('Simple Exponential Smoothing Distribution')\nsmoothing_df['simple_es'].plot(kind='kde')\nplt.subplot(413)\nplt.title('Double Exponential Smoothing Distribution')\nsmoothing_df['double_es'].plot(kind='kde')\nplt.subplot(414)\nplt.title('Triple Exponential Smoothing')\nsmoothing_df['triple_es'].plot(kind='kde')\nplt.show()","d69743cd":"decomposition_df = pd.DataFrame(monthly_df.O_3)\nseasonal_a = seasonal_decompose(decomposition_df, model='additive')\nseasonal_m = seasonal_decompose(decomposition_df, model='multiplicative')\nfig_1 = seasonal_a.plot()\nfig_2 = seasonal_m.plot()\nfig_1.suptitle('Additive Seasonal Decomposition', fontsize=25)\nfig_1.set_figheight(10)\nfig_1.set_figwidth(20)\nfig_2.suptitle('Multiplicative Seasonal Decomposition', fontsize=25)\nfig_2.set_figheight(10)\nfig_2.set_figwidth(20)\nplt.show()\n","471eb4f7":"filter_df = pd.DataFrame(monthly_df.O_3)\nO3_cycle, O3_trend = hpfilter(filter_df, lamb=129600)\nfilter_df['cycle'] = O3_cycle\nfilter_df['trend'] = O3_trend\n\nfilter_df.plot(figsize=(10, 5), title='O_3 Pollutant Plot of Cycle and Trend')","85723cf9":"<a id='to stationarity'><\/a>\n## Towards Stationary\nNow that we know what stationary is, and how to test for it- here is how we can create a stationary series, barring seasonal and strong deterministic components- which we'll get to later. Two common practices are the Box-Cox transformation and differencing. \n\n<a id='box-cox'><\/a>\n#### Box-Cox Transformation\nIf our time series has an underlying quadratic trend, we could take the square root and make it linear, getting the data one step closer to stationary, this is what a Box-Cox transformation is for. A Box-Cox transformation attempts to find the appropriate power transformation to make our data more linear, and more normally distributed. If our data appears to have a power trend then it is a good candidate for a Box-Cox transformation. We can pass Box-Cox an argument for lambda, the transformation parameter, to perform a power transform or we can pass no argument and it will automatically tune and return a lambda value.\n* lambda = -1. is a reciprocal transform.\n* lambda = -0.5 is a reciprocal square root transform.\n* lambda = 0.0 is a log transform.\n* lambda = 0.5 is a square root transform.\n* lambda = 1.0 is no transform.\n\nWe can test for normality in several ways:  \n* Shapiro-Wilk\n* Jarque-Bera: Checks kurtosis and skewedness against a normal distribution\n* Q-Q Plot (Quantile-Quantile): A plot of the quantiles of two distributions, can be used \nNormality isn't necessary for stationarity but it is for many other models and techniques.","bc1bf44e":"<a id='hpf'><\/a>\n### Hodrick-Prescott Filter (HPF)\nThe Hodrick-Prescott Filter is a data-filtering technique for dealing with business cycles from economic data. Unfortunatley our data in this kernel is not a very good candidate for how to use the Hodrick-Prescott filter but I've included it because its incredibly useful in long-term economic data that often has cycles associated with changes in the market. Business cycles are hard to capture and represent a barrier to stationarity- we can use the Hodrick-Prescott Filter to fix that. HPF tries to decompose the time series into trend and cycle components. The key smoothing parameter we provide is lambda, 1600 is the suggested value for annual data and larger values can be used for more volatile data. \n\nOther filters that are popular for economic analysis and decomposing cycles are:\n* Baxter-King Filter\n* Christiano-Fitzgerald Filter","12d5b0ee":"---\n# Madrid Air Quality Exhaustive Tutorial on Stationarity, Smoothing, and Seasonality\n**[Nicholas Holloway](https:\/\/github.com\/nholloway)**\n\n---\n\n### Mission\nTime series modelling is a process of decomposing and representing the statistical relationships between data points in time. For our model to fit our data and reasonably forecast there are constraints about the statistical relationships in the data that must be met. This kernel is an introduction to time series modelling, stationarity, and how we can decompose and test the statistical relationships in our data before applying a model. We will cover several stationarity tests like **augmented dickey-fuller** and **kpss**,  the **box-cox transformation**, **correlograms**, and many other tools and ideas useful for time series modelling and data analysis in general. \n\n\n\n### Table of Contents:\n1. [Stationarity](#stationarity)\n    1. [Unit Root](#unit root)\n    2. [Augmented Dickey-Fuller](#adf)\n    3. [KPSS](#kpss)\n2. [Towards Stationarity](#to stationarity)\n    1. [Box-Cox Transformation](#box-cox)\n    2. [Differencing](#differencing)\n3. [Autocorrelation](#autocorrelation)\n4. [Smoothing](#smoothing)\n    1. [Simple Moving Average](#sma)\n    2. [Exponential Smoothing](#exp smoothing)\n5. [Seasonality](#seasonality)\n    1. [Seasonality Decomposition](#decomposition)\n    2. [Hodrick-Prescott Filter](#hpf)\n\n### Time Series Kernels: 1 of 4 \n* [Stationarity, Smoothing, and Seasonality](https:\/\/www.kaggle.com\/nholloway\/stationarity-smoothing-and-seasonality)\n* [Deconstructing ARIMA](https:\/\/www.kaggle.com\/nholloway\/deconstructing-arima)\n* [Seasonality and SARIMAX]()\n* [Volatility Clustering and GARCH]()\n","4bee9ac5":"<a id='autocorrelation'><\/a>\n## Autocorrelation\nSerial correlation (autocorrelation) is where error terms in a time series are correlated between periods. Such that an underestimate in one period can result in an underestimate in a subsequent period. This can lead to a number of problems with our ability to accurately fit our time series and make forecasts. If this doesn't immediately make sense, make it through the example and the correlograms- which I find really help solidify what autocorrelation means in a time series. \n\nTests for autocorrelation include:\n* Correlograms\n* Lagrange multiplier tests\n* Durbin-Watson test\n* Ljung-Box test\n\n### Correlogram\n#### ACT\nCorrelograms can give us a good idea whether or not pairs of data show autocorrelation. \n\nTo read a correlogram the x-axis shows the lagged observations at prior time steps. So lag 1, or 1 on the x axis, is equivalent to the time step $x_{t-1}$ and the y-axis is the correlation strength between or observation $x_t$ and the various prior time steps, called lags. \n\nWhen reading a correlogram we look at two things: the size of the correlation coefficients on the y-axis and the presence of trend. If there are high coefficients and a visible trend then autocorrelation is highly likely. \n\nAdditionally, confidence intervals are drawn as a cone. By default, this is set to a 95% confidence interval, suggesting that correlation values outside of this cone are very likely a correlation and not a statistical fluke.\n\n#### PACT\nA partial autocorrelation is a summary of the relationship between an observation in a time series and observations at prior time steps with the relationships of intervening observations removed. We can read it much like our ACT.","4b4d5167":"<a id='differencing'><\/a>\n#### Differencing\nDifferencing is subtracting an observation from an observation at the previous time step. Differencing generates a time series of the changes between raw data points and helps us create a time series that is stationary. Normally, the correct amount of differencing is the lowest order of differencing that yields a time series which fluctuates around a well-defined mean value and whose autocorrelation function (ACF) plot decays fairly rapidly to zero. After each differencing operation, like we perform below, we can conduct an Augmented Dickey-Fuller (adf) and Kwiatkowski-Phillips-Schmidt-Shin (kpss) test to check for stationarity. ","a8d1b4d0":"<a id='smoothing'><\/a>\n# Smoothing\n---\nSmoothing is a technique we can use to reduce noise in our time series and perform forecasts. Forecasting with smoothing techniques like linear exponential smoothing(LES) and simple exponential smoothing(SES) can work for nonseasonal data where the time series is locally stationary and has a mild trend. For more complex time series there are techniques like Holt-Winters seasonal methods that build on simple smoothing techniques. We will not delve very deeply into Holt-Winters methods for forecasting but we will introduce them, introduce how they are related to simpler techniques, and hopefully understand how to think about our time series as a combination of components and how smoothing affects the underlying distribution of our data.  \n\nSmoothing techniques are fundamental to time series and are used in a number of applications including the ARMA family of forecasting models. When smoothing, we should pay attention to variance and the shape of our data distributions as excessively smoothed models can exhibit high bias, and smoothing can reduce too much variance depending on our parameters.\n\n<a id='sma'><\/a>\n### Simple Moving Average (SMA)\nThe simple moving average is an equally weighted average of n data terms. The simple moving average is great when data is missing or highly irregular. It's primitive but robust. Our pollutant data is incredibly noisy- particularly at its original hourly scale. SMA is a good preprocessing tool to incrementally smooth our data to the level we want. \n\nI originally used daily data for this example to see how well it smoothed out the volatility, I changed it to monthly to match the other examples but its easy to rerun these cells with different data frequencies and pollutant types and there's a surprising amount of variability in the effect depending on the initial shape of the data. \n ","1224aaeb":"<a id='exp smoothing'><\/a>\n### Simple Exponential Smoothing (SES)\nExponential smoothing is a weighted average of n past data terms, where the weights decay exponentially. We can use simple exponential smoothing when data is nonseasonal and displays a time-varying mean without consistent trend. SES is the same as an ARIMA(0, 1, 1) model without a seasonal constant. We provide SES a value, alpha, a smoothing parameter that controls the rate at which weights decay exponentially. Large values mean the model pays close attention to the most recent past observations, and a smaller alpha considers a greater history. \n\n### Double Exponential Smoothing \nDouble exponential smoothing is for analyzing data that shows a trend, it adds additional equations to simple exponential smoothing to capture the series' level and trend, using parameter beta. Double exponential smoothing is also called Holt's linear trend method and in the statsmodels package we use `ExponentialSmoothing` to implement double and triple exponential smoothing. When using the statsmodels package all we have to indicate is whether the trend is additive or multiplicative. This is discussed later in the kernel but simply, additive and multiplicative trend are about whether the trend is consistent or does it appear to grow- like an exponential function for instance. If the trend is consistent then it is additive, if the trend grows then it is multiplicative. Also part of the statsmodels implementation of exponential smoothing is an argument `damped`. Holt's linear method tends to display a constant trend indefinitely into the future. The `damped` parameter  corrects this infinite trend by dampening the trend. There is a damped parameter between 0 and 1 where values closer to 1 are the same as a non-damped model. We can set these parameters ourselves but in practice, we simply set whether or not to include the damped parameter. \n\n### Triple Exponential Smoothing\nIf your data shows trend and seasonality, triple exponential smoothing builds on the equations used in simple and double and adds a third equation to handle seasonality. Also called Holt-Winters Exponential Smoothing, a new parameter, gamma, is used to influence seasonality. Additionally, we indicate whether the seasonality is additive or multiplicative like we did with trend. ","dad3fee8":"<a id='seasonality'><\/a>\n# Seasonality\n---\n<a id='decomposition'><\/a>\n### Seasonality Decomposition\nWe can think about our time series as composed of a combination of level, trend, seasonality, and noise. \n- level: The average value of the series\n- trend: The increasing or decreasing value in the series\n- seasonality: The repeating short-term cycle in the series\n- noise: The random variation in the series, named residual by the seasonal_decompose function\n\nA season is a fixed length of time that contains the full repetition of your time series' pattern. Pay close attention, your data may repeat every day but if the weekends look different than your weekdays then your season is a week not a day.\n\nSeasonality decompostion is an analysis tool that provides us a framework for how to think about the different components of our time series. We can decompose our series to understand the influence of each component and use that to guide our data preparation, model selection, and model tuning. As we saw in the section on exponential smoothing, models that consider seasonality versus those that don't have a prominent effect on our ability to forecast the series. \n\nDecomposition tools are also used in forecasting by removing a component, like trend, from our time series before modeling and forecasting. Just make sure that if you transform the data, that you reverse that transform before using the series predictions. \n\nThe seasonality decomposition in `statsmodels` requires only that we indicate whether the model is additive or multiplicative.\n\n### Additive vs. Multiplicative\nIn a multiplicative time series, the decomposed components multiply together to make the time series. In a multiplicative series there is increasing trend, the amplitude of seasonal activity increases and everything becomes more exaggerated. Multiplicative trend looks more like an exponential curve and multiplicative seasonality has waves that grow in amplitude of the course of time. \n\nIn an additive model we assume the componenents of the time series have an additive effect, that the amplitude of the seasonal effect is roughly the same, that the size of the residuals are mostly constant.\n\nReal world data won't always be purely additive or mulitplicative, there may be sections of either kind. Its important to try several parameters and see which fits our data better. ","f99b1da3":"<a id='stationarity'><\/a>\n# Stationarity\n---\nA stationary time series is one whose statistical properties such as mean, variance, and autocovariance are all constant and not a function of time. If we can assume that these statistical properties will not change over time then we can predict that the future of the time series will have the same or proportional statistical properties. Put another way, if we think about our time series as a sequence of independent random variables from an identical distribution then (a) lots of results of independent random variables hold true (law of large numbers, central limit theorem, etc.) and (b) it is easy enough to estimate the parameters of the distribution from which we are drawing our data, which we say are IID, or independently and identically distributed. As a result, most statistical forcasting methods are based on the assumption that the time series is *approximately stationary*- but most time series we'll see are not. \n\nNon-stationaries can either come from deterministic changes like trend or seasonal fluctuations, or because the stochastic properties of the process have a unit root. In the first case we can remove the determinitic componenet, and in the second we can test for the presence of the unit root and difference it away. This kernel will explore some of the major ways we can decompose, test, and process our time series. Finding the sequence of transformations needed to stationarize a time series often provides important clues in the search for an appropriate forecasting model.\n\n<a id='unit root'><\/a>\n### Unit Root\nUnit roots are one cause of non-stationarity and important to understand. A unit root is a monomial term that is equal to 1. In a time series a unit root causes a stochastic trend, sometimes called a random walk with drift. Why this is- is more complicated than this kernel will address but we need to understand unit roots because they can lead to errant behavior that's difficult to forecast and because they appear in time series literature. The Agumented Dickey-Fuller test, which we'll address later as a test for stationarity is actually a test looking for unit roots by attempting to fit an autoregressive model to the data. \n\nThere are several tests we can run to check for stationarity: \n* Dickey-Fuller: Runs into problems when there's autocorrelation\n* Augmented Dickey-Fuller: Handles more complex models but has a fairly high false positive error\n* Phillips-Perron (PP): A modification of Dickey-Fuller, corrects for autocorrelation and heteroscedasticity. \n* Kwiatkowski-Phillips-Schmidt-Shin (KPSS): Also has a fairly high false positive (Type I) error, can be used in conjuction with ADF for more confident decisions. \n* Several other tests include Elliot-RothenBerg-Stock Test, DF-GLS, Schmidt-Phillips, and Zivot-Andrews\n\n<a id='adf'><\/a>\n#### Augmented Dickey-Fuller\nThe null hypothesis for this test is that there is a unit root. The alternative hypothesis is that the time series is stationary, or trend stationary. If the test statistic is less than the critical Value, we can reject the null hypothesis and say that the series is stationary. Additionally, if the p-value is less than alpha we can reject the null hypothesis. \n\n<a id='kpss'><\/a>\n#### KPSS\nKPSS figures out if a time series is stationary around a mean or linear trend. The null hypothesis is that the data is stationary, the alternate hypothesis for the test is that the data is not stationary. \n\nTo interpret, if the test statistic is greater than the critical values then the null hypothesis is rejected and the series is non-stationary. We can also use the returned p-value and compare it to the alpha level, where if the p-value is less than our alpha we reject the null hypothesis. \n\nIf we fail to reject the null hypothesis it means our time series is stationary or trend stationary, because KPSS classifies a series as stationary on the absence of a unit root.\n\nBecause there are several types of stationary we can use KPSS and ADF in conjunction to determine what transformations to make. If KPSS = Stationary and ADF = Not Stationary then our time series is trend stationary and we need to remove the trend to be strict stationary. If KPSS = Not Stationary and ADF = Stationary then our time series is difference stationary, and we need to difference our series. \n\n"}}