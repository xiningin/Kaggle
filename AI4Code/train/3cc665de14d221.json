{"cell_type":{"e3dba790":"code","59892a30":"code","d705cd6f":"code","fba7dc4a":"code","08c8ae23":"code","77c78edb":"code","028eff70":"code","44759bb8":"code","16f373db":"code","0d5bff5f":"code","430c6f08":"code","db6098fc":"code","88519c8d":"code","355d0b08":"code","25112afc":"code","d5220ef4":"code","d55df641":"code","1c4976d1":"code","a50b8bee":"code","671bc4ed":"code","99993098":"code","40480ec6":"code","084d8971":"code","8d5992b3":"code","f6480995":"code","7a52a9a3":"code","ce8e40cf":"code","61a6b21b":"code","b14a1894":"code","5b84fcb8":"code","f37c9d20":"code","5f12f293":"code","3d300a67":"code","4c98f6a0":"code","435ba455":"code","51da6bf5":"code","589fd22e":"code","de4cf3f5":"code","39659f9b":"code","0afa68dd":"code","4903eab4":"code","23dceec2":"code","bd97d683":"code","5855b6a0":"code","aa231e8d":"code","52c22482":"code","a5881637":"code","980061c3":"code","7e91f504":"code","241048ea":"code","46b969cc":"code","580256de":"code","f7c60832":"code","974c53b8":"code","8ae61f9c":"code","d6a27f85":"code","a101d366":"code","54ce0106":"code","838c0d96":"code","5f6693eb":"code","452db40c":"code","a9fba600":"code","0820e08c":"markdown","a8f0e5b3":"markdown","c5e7ded0":"markdown","782e5ba2":"markdown","b9686fb7":"markdown","f635f1d4":"markdown","c91911ca":"markdown","e78a7401":"markdown","dc92469c":"markdown","f3695cad":"markdown","6b0869ea":"markdown","649d60f8":"markdown","3a1e4b8f":"markdown","a4acaa8e":"markdown","a11ec95a":"markdown","caa6b9b4":"markdown","fab93c40":"markdown","707d0eca":"markdown","5843bfc0":"markdown","f71119c3":"markdown","efedd880":"markdown","f3ce84ef":"markdown","79190436":"markdown","7af7cfbc":"markdown","7e66726b":"markdown","c23ea85f":"markdown","27eb7215":"markdown","52b0dc76":"markdown","215d7d5d":"markdown","0ff807c1":"markdown","923b1e93":"markdown","f06b3ed1":"markdown","4402b0ed":"markdown","cc1e146e":"markdown","306bd4c2":"markdown","b0dd1047":"markdown","7c44b107":"markdown","966b811a":"markdown","bad414d4":"markdown","bc0bb4a8":"markdown","6b30115f":"markdown","8e6abb85":"markdown","99824b2c":"markdown","c56ebb46":"markdown","5de96594":"markdown"},"source":{"e3dba790":"# Importing the dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.pandas.set_option('display.max_columns',None)\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\nimport sklearn","59892a30":"train_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')   # importing the training data file\nprint(train_df.shape)","d705cd6f":"train_df.head()          # Print the top 5 values of the dataset ","fba7dc4a":"test_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')      # importing the test data file\ntest_df.head()","08c8ae23":"column_with_na = [features for features in train_df.columns if train_df[features].isnull().sum() >1]\nprint(\"Total number of features having some nan values is: \", len(column_with_na))\ncolumn_with_na","77c78edb":"for features in column_with_na:\n    print(\"Column\", features, \"have\", np.round(train_df[features].isnull().mean(), 4)*100, \"% NaN value\")","028eff70":"for features in column_with_na:\n    data = train_df.copy() \n    data[features] = np.where(data[features].isnull(), 1, 0)\n    col = [\"red\", \"green\"]\n    data.groupby(features)['SalePrice'].median().plot.bar(color = col)\n    plt.title(features)\n    plt.show()\n    \n## we can see in the output of this shell that, there's no such specific patterns we're getting. ","44759bb8":"column_with_numerical_values = [features for features in train_df.columns if train_df[features].dtypes != 'O']\nprint(\"Total number of features with numerical values is: \", len(column_with_numerical_values))\n# column_with_numerical_values\ntrain_df[column_with_numerical_values].head()","16f373db":"for feature in column_with_numerical_values:\n    data = train_df.copy()\n    print(feature, len(data[feature].unique()))","0d5bff5f":"column_discrete_values = [feature for feature in column_with_numerical_values if len(train_df[feature].unique()) <25]\nprint(\"Total number of numerical features with discrete values is: \", len(column_discrete_values))\ndata[column_discrete_values].head()\n","430c6f08":"for feature in column_discrete_values:\n    data = train_df.copy()    \n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.ylabel(\"Sales Price\")\n    plt.show()","db6098fc":"column_continuous_values = [feature for feature in column_with_numerical_values if len(train_df[feature].unique()) >=25]\nprint(\"Total number of numerical features with discrete values is: \", len(column_continuous_values))\ndata[column_continuous_values].head()\n","88519c8d":"for feature in column_continuous_values:\n    if feature != 'Id':  \n        data[feature].hist(bins=25)\n        plt.title(feature)\n        plt.xlabel(feature)\n        plt.ylabel(\"Count\")\n        plt.show()","355d0b08":"for feature in column_continuous_values:\n    if feature != 'Id':  \n        data = train_df.copy()\n        if 0 in data[feature].unique():\n            pass\n        else:\n            data[feature] = np.log(data[feature])\n            data['SalePrice'] = np.log(data['SalePrice'])\n            plt.scatter(data[feature], data['SalePrice'])\n            plt.title(feature)\n            plt.xlabel(feature)\n            plt.ylabel(\"Count\") \n            plt.show()\n## We can see in the output of this shell that most of the plots showing linear corelation","25112afc":"for feature in column_continuous_values:\n    if feature != 'Id':  \n        data = train_df.copy()\n        if 0 in data[feature].unique():\n            pass\n        else:\n            data[feature] = np.log(data[feature])\n#             data['SalePrice'] = np.log(data['SalePrice'])\n            data.boxplot(column = feature)\n            plt.title(feature)\n#             plt.xlabel(feature)\n            plt.ylabel(feature) \n            plt.show()","d5220ef4":"column_with_categorical_values = [features for features in train_df.columns if train_df[features].dtypes == 'O']\n\n\nprint(\"Total number of features with numerical values is: \", len(column_with_categorical_values))\n# column_with_categorical_values\ntrain_df[column_with_categorical_values].head()","d55df641":"for feature in column_with_categorical_values:\n    data = train_df.copy()\n    print(feature, len(data[feature].unique()))","1c4976d1":"for feature in column_with_categorical_values:\n    data = train_df.copy()    \n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.ylabel(\"Sales Price\")\n    plt.show()","a50b8bee":"column_with_year = [feature for feature in column_with_numerical_values if 'Yr' in feature or 'Year' in feature]\nprint(\"Total number of features with date entries is: \", len(column_with_year))\ntrain_df[column_with_year]","671bc4ed":"train_df.groupby('YrSold')['SalePrice'].median().plot()\nplt.title('Median Sale Price vs Sold Year')\nplt.ylabel('Sale price')\nplt.xlabel('Year Sold')","99993098":"for feature in column_with_year:\n    if feature != 'YrSold':\n        data = train_df.copy()\n        data[feature] = data['YrSold'] - data[feature]\n        plt.scatter(data[feature], data['SalePrice'])\n        plt.title(\"Year features vs Sales price\")\n        plt.xlabel(feature)\n        plt.ylabel(\"Sale Price\")\n        plt.show()\n# We can see if the year gap is more the price is less","40480ec6":"test_df['SalePrice'] = 0\nprint(\"The shape of given test data is: \", test_df.shape)","084d8971":"## we'll concatenate both table\nfull_df_feature_eng = pd.concat([train_df, test_df], axis=0,sort=False)\nprint(\"The shape of dataset after combining both test and train dataset is: \",full_df_feature_eng.shape)","8d5992b3":"full_df_feature_eng.tail()   # Print the tail end of the combined datsset","f6480995":"columns_nan_in_categorical =[features for features in full_df_feature_eng.columns if full_df_feature_eng[features].isnull().sum()>1 and full_df_feature_eng[features].dtypes=='O']\nprint(\"Total number of categorical features having some nan values is: \", len(columns_nan_in_categorical), \"\\n\")\nfor features in columns_nan_in_categorical:\n    print(\"Column\", features, \"have\", np.round(full_df_feature_eng[features].isnull().mean(), 4)*100, \"% NaN value\")\n","7a52a9a3":"full_df_feature_eng[columns_nan_in_categorical].head()  # Printing all the categorical columns with Nan values","ce8e40cf":"for features in columns_nan_in_categorical:\n    full_df_feature_eng[features] = full_df_feature_eng[features].fillna('Missing')\n\nfull_df_feature_eng[columns_nan_in_categorical].head()","61a6b21b":"columns_nan_in_numerical =[features for features in full_df_feature_eng.columns if full_df_feature_eng[features].isnull().sum()>1 and full_df_feature_eng[features].dtypes!='O']\nprint(\"Total number of numerical features having some nan values is: \", len(columns_nan_in_numerical), \"\\n\")\n\nfor features in columns_nan_in_numerical:\n    print(\"Column\", features, \"have\", np.round(full_df_feature_eng[features].isnull().mean(), 4)*100, \"% NaN value\")","b14a1894":"full_df_feature_eng[columns_nan_in_numerical].head()   # Printing all the numerical columns with Nan values","5b84fcb8":"for features in columns_nan_in_numerical:\n    full_df_feature_eng[features+'Nan'] = np.where(full_df_feature_eng[features].isnull(), 1, 0) \n    full_df_feature_eng[features].fillna(full_df_feature_eng[features].median(), inplace = True)\n    \nfor features in columns_nan_in_numerical:\n    print(\"Column\", features, \"have\", np.round(full_df_feature_eng[features].isnull().mean(), 4)*100, \"% NaN value\")\n","f37c9d20":"full_df_feature_eng.head()      ## we'll print the dataframe after adding 5 new columns that had nan value","5f12f293":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n\nfor feature in num_features:\n    full_df_feature_eng[feature]=np.log(full_df_feature_eng[feature])","3d300a67":"categorical_column_in_full_data = [feature for feature in full_df_feature_eng.columns if full_df_feature_eng[feature].dtype=='O']\nprint(categorical_column_in_full_data)","4c98f6a0":"for features in categorical_column_in_full_data:\n    labels_encode=full_df_feature_eng.groupby([features])['SalePrice'].mean().sort_values().index\n    labels_encode={k:i for i,k in enumerate(labels_encode,0)}\n    full_df_feature_eng[features]=full_df_feature_eng[features].map(labels_encode)\nfull_df_feature_eng.head()","435ba455":"full_df_feature_eng.info()   ## Print all the columns, we can see there's no null values now","51da6bf5":"independent_feature = [features for features in full_df_feature_eng.columns if features not in ['Id', 'SalePrice']]\nprint(independent_feature)","589fd22e":"scaler = MinMaxScaler()\nscaler.fit(full_df_feature_eng[independent_feature])","de4cf3f5":"scaler.transform(full_df_feature_eng[independent_feature])","39659f9b":"data_scaled_independent =  pd.DataFrame(scaler.transform(full_df_feature_eng[independent_feature]), columns=independent_feature)\ndata_scaled_independent","0afa68dd":"concat_data = pd.concat([full_df_feature_eng[['Id', 'SalePrice']].reset_index(drop = True), pd.DataFrame(scaler.transform(full_df_feature_eng[independent_feature]), columns=independent_feature)], axis = 1)","4903eab4":"concat_data.head()","23dceec2":"\ngiven_train = concat_data[0:1460]\ngiven_test = concat_data[1460:2919]","bd97d683":"##  We'll \nX = given_train.drop(['SalePrice',\n                              'Id'], axis = 1) \ntarget = given_train['SalePrice']\nprint(\"Dependent Variables\")\ndisplay(X.head())\nprint(\"Independent Variable\")\ndisplay(target.to_frame().head())","5855b6a0":"! pip install lazypredict             # install Lazypredict","aa231e8d":"# split data\nfrom lazypredict.Supervised import LazyRegressor\nX_train, X_test, Y_train, Y_test = train_test_split(X, target,test_size=.3,random_state =23)\nregr=LazyRegressor(verbose=0,predictions=True)\n","52c22482":"import time\nstart_time_2=time.time()\nmodels_r,predictions_r=regr.fit(X_train, X_test, Y_train, Y_test)\nend_time_2=time.time()","a5881637":"models_r    ## Shows performance table by all the models","980061c3":"predictions_r  ## Print the predicted values from all the models","7e91f504":"submission_train = given_test.drop(['SalePrice','Id'], axis = 1)       ## Drop the two columns that we added while feature engineering in concatenated file\nsubmission_train","241048ea":"xgb =XGBRegressor( booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.6, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1.5, n_estimators=2400,\n             n_jobs=1, nthread=None, objective='reg:linear',\n             reg_alpha=0.6, reg_lambda=0.6, scale_pos_weight=1, \n             silent=None, subsample=0.8, verbosity=1)","46b969cc":"X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, target,test_size=.15,random_state =123)","580256de":"#Fitting\nxgb.fit(X_Train, Y_Train)","f7c60832":"xgb_predict_sample = xgb.predict(X_Test)\nprint('RMSE = ' + str(math.sqrt(metrics.mean_squared_error(Y_Test, xgb_predict_sample))))          ## print the error ","974c53b8":"lgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=3,\n                                       learning_rate=0.006, \n                                       n_estimators=12000, \n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.4,   )","8ae61f9c":"lgbm.fit(X_Train, Y_Train,eval_metric='rmse')","d6a27f85":"lgbm_predict_sample = lgbm.predict(X_Test)\nprint('RMSE = ' + str(math.sqrt(metrics.mean_squared_error(Y_Test, lgbm_predict_sample))))       ## Print the error\n","a101d366":"xgb.fit(X, target)   ## fiting xgb model\nlgbm.fit(X, target,eval_metric='rmse')    ## fitting lgbm model","54ce0106":"lgbm_predict_on_provided_test_data = lgbm.predict(submission_train)     \nxgb_predict_on_provided_test_data = xgb.predict(submission_train)\nreq_prediction = ( lgbm_predict_on_provided_test_data*0.5 + xgb_predict_on_provided_test_data * 0.5)","838c0d96":"antilog_of_prediction = np.exp(req_prediction)","5f6693eb":"final_prediction = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": antilog_of_prediction\n    })\n","452db40c":"print(\"Shape of final dataframe is: \", final_prediction.shape)\nfinal_prediction.head()                                                 ","a9fba600":"final_prediction.to_csv('submission.csv', index=False)","0820e08c":"## Get the percentage of Nan values in the columns we got in above shell","a8f0e5b3":"## Finally we'll submit the predicted results","c5e7ded0":"## Numerical columns with some NaN values","782e5ba2":"## Columns with discrete values","b9686fb7":"## Relationship b\/w columns with continuous values with 'SalePrice' (by plotting Histograms)","f635f1d4":"## Realationship b\/w 'YrSold' column with 'SalePrice' (taking groupby wrt. saleprice)","c91911ca":"### Make two dataframe, one for dependent and other for independent variable","e78a7401":"## We can check the shape of final dataframe, its of two columns, predicted prices and respective Ids","dc92469c":"## Categorical columns with some NaN values","f3695cad":"## Check for Nan values","6b0869ea":"## We'll encode the the categorical variables with ranks generated by taking groupby and mean, as you can see below","649d60f8":"## Find the columns with Numerical feature","3a1e4b8f":"## We'll firsts predict using LazyPredict to see the performance of almost all the regression models","a4acaa8e":"## Here we'll apply both model and will take the average og predicted values from both the model","a11ec95a":"## Apply the LGBM regressor model","caa6b9b4":"## Let's find the reationship b\/w columns with nan values and 'Sale price' column","fab93c40":"## Relationship between columns(we got from above shell) having discrete values with 'SalePrice'","707d0eca":"## Now we'll check for some outliers in the features having continuous values","5843bfc0":"## We'll firstly split our given training dataset into train & test set for our model, and analyse the performance","f71119c3":"## Since we have taken log of the 'SalePrice column while feature engineering, so now we'll take antilog of it to get the final predicted prices","efedd880":"## 1. Let's do some EDA on training dataset","f3ce84ef":"## Number of unique values in Columns with numerical Feature ","79190436":"## Now after doing Feature engn. we'll separate both train and test datasets, for further operation","7af7cfbc":"## Now we'll handle categorical variable\n### Print all the features with categorical variable","7e66726b":"## Make a list of independent variable (excluding 'SalePrice' and 'Id', which is dependent variable)","c23ea85f":"## 3. Prediction","27eb7215":"## Now we'll predict from the required model","52b0dc76":"## First apply the XGB model","215d7d5d":"## Replace all the Nan values of numerical","0ff807c1":"## 2. Feature engineering\n\n### We will do feature engineering after combining both test and training dataset, to avoid repetition ","923b1e93":"## Since we didn't get any good insights from above relationship plots, so we'll make scatter plot by taking log of 'SalePrice' column","f06b3ed1":"## Here we'll calculate the predicted values, and take the average of both","4402b0ed":"## Fill the all the Nan with 'Missing'","cc1e146e":"## Relationship b\/w different year column (subtracted with sold year) with 'SalePrice'","306bd4c2":"## Now we'll make a dataframe of predicted values with the corresponding 'Id'","b0dd1047":"## Now, after doing feature scaling of independent variable, we'll concatenate it with 'SalePrice' & 'Id'","7c44b107":"## ## Number of unique values in Columns with Categorical Variable","966b811a":"## Use whole original given training dataset file for model fitting & further apply the model on testing file to get the required prediction","bad414d4":"## Now apply log operation on some columns with big values","bc0bb4a8":"## Apply same data used in Lazy Predict i.e, only traing data (splitting it into train and test set) in XGB and LGBM regressor model","6b30115f":"# EDA, Data cleaning, visualization and House price Prediction\n## You will see prediction and performance with almost all models using **LazyPredict**","8e6abb85":"## Columns with continuous values","99824b2c":"## Visualize the columns with categorical data with 'SalePrice","c56ebb46":"## We can see models with their performance, HuberRegressor giving the best result","5de96594":"## Now we'll do some feature scaling to get good results"}}