{"cell_type":{"d1e0080e":"code","10e0e2ea":"code","f1c6b6d3":"code","feded8c7":"code","471864cd":"code","b13243cd":"code","47820f17":"code","7933c4e0":"code","1cd3d31d":"code","1b60cc40":"code","e83d58dc":"code","4c938fe2":"code","2b82e259":"code","220f69fd":"code","69871782":"code","9950372f":"code","2bdfb54d":"code","be9c6c2d":"code","02d2acb3":"code","91eba2de":"markdown"},"source":{"d1e0080e":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","10e0e2ea":"from fastai.vision import *\nfrom fastai.metrics import error_rate\nimport os","f1c6b6d3":"bs = 64","feded8c7":"path = Path('..\/input\/chest_xray\/chest_xray')\npath.ls()","471864cd":"img = open_image(path\/'val'\/'NORMAL'\/'NORMAL2-IM-1440-0001.jpeg')\nprint(img.data.shape)\nimg.show()","b13243cd":"tfms = get_transforms()","47820f17":"np.random.seed(7)\ndata = ImageDataBunch.from_folder(path, \n                                  valid='val',\n                                  valid_pct=0.2,\n                                  size=256, bs=bs,\n                                  ds_tfms=tfms).normalize(imagenet_stats)","7933c4e0":"data.show_batch(3, figsize=(6,6))","1cd3d31d":"data.classes, data.c, len(data.train_ds), len(data.valid_ds)","1b60cc40":"learn = cnn_learner(data, models.resnet50, metrics=error_rate, model_dir=\"\/tmp\/model\/\")","e83d58dc":"learn.fit_one_cycle(4)","4c938fe2":"learn.save('stage-1')","2b82e259":"learn.unfreeze()","220f69fd":"learn.lr_find()","69871782":"learn.recorder.plot()","9950372f":"learn.fit_one_cycle(10, max_lr=slice(3e-5, 3e-4))","2bdfb54d":"learn.save('stage-2')","be9c6c2d":"interp = ClassificationInterpretation.from_learner(learn)","02d2acb3":"interp.plot_confusion_matrix()","91eba2de":"> - we have a very small validation set so some augmentation is needed"}}