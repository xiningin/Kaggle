{"cell_type":{"10de8ed1":"code","703c7bc9":"code","3f52d3cf":"code","2115b97d":"code","ea1ac5b1":"code","0a388cd6":"code","4179ceb1":"code","460a7b76":"code","c7e69164":"code","b58a2508":"code","f0229cca":"code","3f4637d5":"code","618e1209":"code","82bf5414":"code","5352b0d2":"code","dc1b56e6":"code","863c2a3b":"code","0268c8db":"code","0ae06575":"code","270f0fd0":"code","2beeadb2":"code","a51f3a14":"code","0a4d53db":"code","5a38a48f":"code","3971a0d9":"code","862fc948":"markdown","e2458e94":"markdown","ad8675e6":"markdown","6eba9849":"markdown","4a5d3a0d":"markdown","ef56277c":"markdown","2d77ecbe":"markdown","8a21f7b1":"markdown","5835491f":"markdown","13828099":"markdown","a951e362":"markdown","122deb10":"markdown","c1ade07c":"markdown","748b1eec":"markdown","539fea14":"markdown","056fab86":"markdown","ae3c3069":"markdown","d1c8032f":"markdown","6726ed0c":"markdown","ff02da81":"markdown","b3865cf0":"markdown"},"source":{"10de8ed1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","703c7bc9":"raw_df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf1 = pd.get_dummies(raw_df)","3f52d3cf":"raw_df.describe()","2115b97d":"df1.info()","ea1ac5b1":"imp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit(df1[['bmi']])\ndf1['bmi'] = imp.transform(df1[['bmi']])","0a388cd6":"df1.info()","4179ceb1":"sns.countplot(x='stroke', data=df1)","460a7b76":"g = sns.FacetGrid(raw_df, col=\"smoking_status\", height=10, aspect=.5)\ng.map(sns.barplot, \"stroke\", \"age\", order=[1,0]) ","c7e69164":"g = sns.FacetGrid(raw_df, col=\"gender\", height=10, aspect=.5, hue=\"gender\")\ng.map(sns.barplot, \"stroke\", \"age\", order=[1,0]) ","b58a2508":"g = sns.FacetGrid(raw_df, col=\"stroke\", height=8, aspect=.5, hue=\"stroke\")\ng.map(sns.barplot, \"work_type\", \"age\" ) ","f0229cca":"sns.jointplot(x='age',y='bmi', data=df1, hue='stroke', alpha=.2, height=15)","3f4637d5":"sns.relplot(data=df1, x=\"age\", y=\"bmi\", col=\"stroke\", alpha=.3)","618e1209":"df1= df1.drop(['gender_Female','ever_married_Yes','Residence_type_Rural'], axis=1)","82bf5414":"corr = df1.corr().round(3)\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, annot = True)","5352b0d2":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX= df1.loc[:, df1.columns != 'stroke']\ny= df1.loc[:,['stroke']]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42, stratify= y)","dc1b56e6":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\n\n# Fit the regressor to the training data\nlinreg.fit(X_train, y_train)\n\n# Predict on the test data: y_pred\ny_pred = linreg.predict(X_test)\n\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(linreg.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))","863c2a3b":"print(linreg.coef_, linreg.intercept_)","0268c8db":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\n\n# Fit model to training data\nlogreg.fit(X_train, y_train)\n\n# Predict test using test data\ny_pred = logreg.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(logreg.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))","0ae06575":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(y_test,y_pred))","270f0fd0":"print(classification_report(y_test,y_pred))","2beeadb2":"# Import necessary modules\nfrom sklearn.metrics import roc_curve\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[: ,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","a51f3a14":"from sklearn.model_selection import GridSearchCV\n# Create the hyperparameter grid\nc_space = np.logspace(-5, 10, 10)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.4, random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg,param_grid, cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train,y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))","0a4d53db":"logreg = LogisticRegression()\n\n# Fit model to training data\nlogreg.fit(X_train, y_train)\n\n# Predict test using test data\ny_pred = logreg.predict(X_test)\n\n# Compute and print R^2 and RMSE\nprint(\"R^2: {}\".format(logreg.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {}\".format(rmse))\n ","5a38a48f":"my_submission = pd.DataFrame({'Id': X_test.id, 'Stroke': y_pred})\nmy_submission= my_submission.reset_index(drop=True)\nmy_submission.to_csv('submission.csv', index=False)","3971a0d9":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,y_pred))","862fc948":"We can see here that the r^2 has dramatically improved from the linear regression. We can see that our model explains 94% of the variance. \nThere are other accuracy metrics that help use assess if the model is performing well. ","e2458e94":"This seems to be an imbalanced dataset. This may skew our accuracy metrics later down the line. ","ad8675e6":"From the output above, we can see that we're missing BMI for some records. Lets impute the missing data with the mean.","6eba9849":"### Linear Regression","4a5d3a0d":"Age is consistently showing some correlation to the target variable. Higher age seems to be correlated with more strokes.","ef56277c":"## Modeling","2d77ecbe":"### Logistic Regression","8a21f7b1":"After creating dummy variables we should remove our categorical features that are binary. This will help the performance of our model as it removes colinearity. The correlation matrix below shows that age has the strongest correlation coeffecient to our target variable.","5835491f":"Lets do some preliminary analysis to understand the significance of the attributes and state any assumptions we're making.","13828099":"Here from the confusion matrix we can see that there accuracy may show a metric of 95% but in fact, we have not captured any true negatives. ","a951e362":"## Inspect Data","122deb10":"After conducting some EDA and cleaning the dataset, I was able to split the dataset into test and train sets and then run a linear regression. However, our objective is to predict for a binary outcome, in which case, a logistic regression is a more appropriate model. After fitting the first logistic regression, I was able to achieve 95% accuracy. However, the underlying model is extremely unbalanced, with far fewer samples where the target is 1 vs 0. As a result, the accuracy metric does not tell the whole story about our model's performance. Of course, the appropriate assessment of performance ultimately depends on the objective of the model. For example, in a case like this where we're predicting something serious like the liklihood of a stroke, we may want to compromise accuracy for more false positives than false negatives. \nAfter the first logistic regression was run, I also conducted a gridsearch to fine tune my hyperparameters using GridSearchCV. After I retrieved my optimal hyper-parameters, I ran the logistic regression again and yielded a slightly lower r^2 score but capture more true negatives. ","c1ade07c":"This dataset looks to predict stroke using attributes related to a person's medical history, demographic information, and type of work. \nFirst I am reading in the raw data, then creating a new dataframe where categorical attributes are turned into binary columns.","748b1eec":"Linear Regression Conclusion:\nUsing a linear model to calculate a binary outcome doesn't make sense and it shows in the results of the model's performance. It would make more sense to use a logistic regression, which is another multi-regression linear model. ","539fea14":"# Import Data ","056fab86":"Printing the coefficients of our linear model and the intercept.","ae3c3069":"## Check for Multicolinearity","d1c8032f":"## EDA","6726ed0c":"Now we can see that we have no null values in our dataset. ","ff02da81":"### Evaluate the Model","b3865cf0":"# Summary"}}