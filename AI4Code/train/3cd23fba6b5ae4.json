{"cell_type":{"64b4701e":"code","77d7c1ea":"code","c866755e":"code","6428cff3":"code","b625c6aa":"code","405230ec":"code","e2be38b1":"code","b90e6a0e":"code","fef7d6de":"code","4765bd5f":"code","4c6e7f09":"code","9011460b":"code","eb445759":"code","812cb5e9":"code","64209333":"code","fc6baea8":"code","c3bbed2b":"code","d627646e":"code","703ff0e3":"code","037a02e4":"code","0c5510b8":"code","0c80b5b0":"code","4efd4e71":"code","9631372d":"code","ec81a28c":"code","9353d15c":"code","92df3c5c":"code","ce7524d0":"code","aad67061":"code","12816152":"code","a6970b78":"code","3863acf0":"code","0da70003":"code","4978ce6c":"code","07a0718e":"code","df80d11e":"code","7fca48f1":"code","f5d5f7ee":"code","2f6dc70a":"markdown","b80a2985":"markdown","3ccefc24":"markdown","9d907b09":"markdown","4334e437":"markdown","5c6e9aa3":"markdown","df0e4d4d":"markdown","fbb05068":"markdown","41b0146f":"markdown","d8427d13":"markdown","b5cf2dbc":"markdown"},"source":{"64b4701e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77d7c1ea":"train_df=pd.read_csv(\"\/kaggle\/input\/hematology\/training_set.csv\")","c866755e":"train_df.head()","6428cff3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n#plt.pie(train_df['SEX'].value_counts(), labels=train_df['SEX'].value_counts(), labels=[\"M\",\"F\"]);\ndef label_function(val):\n    return f'{val \/ 100 * len(train_df):.0f}\\n{val:.0f}%'\n\ntrain_df.groupby('SEX').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 20}, colors=['violet', 'lime'])","b625c6aa":"sns.distplot(train_df['HAEMATOCRIT'], bins=40)","405230ec":"sns.distplot(train_df['HAEMOGLOBINS'], bins=40)","e2be38b1":"sns.distplot(train_df['ERYTHROCYTE'], bins=40)","b90e6a0e":"sns.distplot(train_df['LEUCOCYTE'], bins=40)\n# Problem solver: https:\/\/towardsdatascience.com\/top-3-methods-for-handling-skewed-data-1334e0debf45","fef7d6de":"sns.distplot(train_df['THROMBOCYTE'], bins=40)","4765bd5f":"sns.distplot(train_df['MCH'], bins=40)","4c6e7f09":"sns.distplot(train_df['MCHC'], bins=40)","9011460b":"sns.distplot(train_df['MCV'], bins=40)","eb445759":"sns.distplot(train_df['AGE'], bins=40)","812cb5e9":"#plt.pie(train_df['SEX'].value_counts(), labels=train_df['SEX'].value_counts(), labels=[\"M\",\"F\"]);\ndef label_function(val):\n    return f'{val \/ 100 * len(train_df):.0f}\\n{val:.0f}%'\n\ntrain_df.groupby('SOURCE').size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 20}, colors=['violet', 'lime'])","64209333":"\"\"\"\n#1st way to handling outliers\n\ndef get_lower_upper_bound(x):\n    q1 = np.percentile(x, 25)\n    q3 = np.percentile(x, 75)\n    iqr = q3-q1\n    lower_bound = q1 - (iqr * 1.5)\n    upper_bound = q3 + (iqr * 1.5)\n    return lower_bound, upper_bound\n\ndef get_outliers_iqr(x):\n    lower_bound, upper_bound = get_lower_upper_bound(x)\n    return x[np.where((x > upper_bound) | (x < lower_bound))]\n\ndef get_inliers_iqr(x):\n    lower_bound, upper_bound = get_lower_upper_bound(x)\n    return x[np.where((x < upper_bound) & (x > lower_bound))]\n\n\n#handling outliers (2nd way)\ndef ub(s):\n    iqr = (np.quantile(s, 0.75))-(np.quantile(s, 0.25))\n    upper_bound = np.quantile(s, 0.75)+(1.5*iqr)\n    return upper_bound\ndef lb(s):\n    iqr = (np.quantile(s, 0.75))-(np.quantile(s, 0.25))\n    lower_bound = np.quantile(s, 0.25)-(1.5*iqr)\n    return lower_bound\n# function cap\ndef cap(s):\n    s = np.where(s > ub(s), np.median(s), s)\n    s = np.where(s < lb(s), np.median(s), s)\n    return s\n    \n#get_outliers_iqr(data_df.values)\n\"\"\"\n#data_df_2 = pd.DataFrame(get_inliers_iqr(data_df.values))     \n\ndef cap_outliers(series, zscore_threshold=3, iqr_threshold=1.5, verbose=False, method=\"IQR\"):\n    if method==\"IQR\":\n        #https:\/\/stackoverflow.com\/questions\/59489073\/how-to-not-remove-but-handle-outliers-by-transforming-using-pandas\n        '''Caps outliers to closest existing value within threshold (IQR).'''\n        Q1 = series.quantile(0.25)\n        Q3 = series.quantile(0.75)\n        IQR = Q3 - Q1\n\n        lbound = Q1 - iqr_threshold * IQR\n        ubound = Q3 + iqr_threshold * IQR\n\n        outliers = (series < lbound) | (series > ubound)\n\n        series = series.copy()\n        series.loc[series < lbound] = series.loc[~outliers].min()\n        series.loc[series > ubound] = series.loc[~outliers].max()\n\n        # For comparison purposes.\n        if verbose:\n                print('\\n'.join(\n                    ['Capping outliers by the IQR method:',\n                     f'   IQR threshold: {iqr_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"mean\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].max()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].min() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"zscore\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].mean()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].mean() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"median\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].max()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].min() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"zscore\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].median()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].median() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"log\":\n        #https:\/\/stackoverflow.com\/questions\/37890849\/pandas-series-log-normalize\n        series=series.map(lambda x: np.log(x))\n        \n\n    return series\n\n\n\nfeatures_treating=['LEUCOCYTE','THROMBOCYTE','MCHC']\n\nfor i, col in enumerate(features_treating):\n    train_df[col]=cap_outliers(train_df[col],method=\"mean\", verbose=True)\n\n#train_df['ApplicantIncome']=cap_outliers(train_df[\"ApplicantIncome\"],method=\"log\", verbose=True)\n#train_df2 =cap(train_df)\n\n\n\"\"\"# using Z Score\nnilai_z = np.abs(scipy.stats.zscore(data_df))\n# outliers_df = pd.DataFrame(np.where(outliers>3))\noutliers_df = data_df[(nilai_z>2).all(axis=1)]\noutliers_df.head()\n\"\"\"","fc6baea8":"sns.distplot(train_df['THROMBOCYTE'], bins=40)","c3bbed2b":"corrmat = train_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, annot=True)","d627646e":"train_df.isnull().sum()","703ff0e3":"categorical_columns = train_df.select_dtypes(exclude=np.number).columns\n\ntrain_df = pd.get_dummies(data=train_df, prefix=categorical_columns, drop_first=True)","037a02e4":"train_df.head()","0c5510b8":"from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nX=train_df.drop('SOURCE',axis=1) #axis=1 drop the bulk of column\ny=train_df['SOURCE']\n\n#X=train_df.drop('Loan_Status',axis=1) #axis=1 drop the bulk of column\n#y=train_df['Loan_Status']\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.20,random_state=0)\n\nStSc = StandardScaler()\nX_train  = StSc.fit_transform(X_train)\nX_test  = StSc.fit_transform(X_test)","0c80b5b0":"penalty = ['l2', 'elasticnet']\nC = np.logspace(-4,4,20)\nC =[0.001,0.01,0.1,1,10,100]\nsolver=['lbfgs','liblinear']\nl1_ratio=[0.001, 0.01, 0.1]\n\nhyperparameters = dict(penalty=penalty, C=C, solver=solver)#, l1_ratio=l1_ratio)\n\nlogreg = LogisticRegression()\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nclf = GridSearchCV(logreg, hyperparameters, cv=cv, verbose=False)","4efd4e71":"#best_model=clf.fit(X_test,y_test)\nbest_model=logreg.fit(X_test,y_test)","9631372d":"y_pred=best_model.predict(X_test)","ec81a28c":"from sklearn.metrics import classification_report, roc_auc_score\n\nprint(classification_report(y_test,y_pred))\nroc_auc_score(y_test,y_pred)","9353d15c":"criterion = ['gini','entropy']\nmax_depth=[5,6,7,8,9]\nn_estimator=[50,100,200,300,400,500]\n\nhyperparameters = dict(criterion=criterion, max_depth=max_depth, n_estimators=n_estimator)\n\nrandomforest = RandomForestClassifier()\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nclf2 = GridSearchCV(randomforest, param_grid=hyperparameters, cv=cv, verbose=False,n_jobs=-1)\n\n#estimator.get_params().keys()","92df3c5c":"best_model2=randomforest.fit(X_train, y_train)","ce7524d0":"y_pred2=best_model2.predict(X_test)\n\nprint(classification_report(y_test,y_pred2))\nroc_auc_score(y_test,y_pred2)","aad67061":"max_depth=[2,3,4,5,6,7,8,9,10]\nlearning_rate=[0.001,0.01,0.1,0.2,0.3]\nmin_child_weight=[2,3,4,5,6,7]\n\nhyperparameters = dict(max_depth=max_depth, learning_rate=learning_rate, min_child_weight=min_child_weight)\n\nxgb = XGBClassifier()\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n\nclf3 = GridSearchCV(xgb, param_grid=hyperparameters, cv=cv, verbose=False,n_jobs=-1)","12816152":"#best_model3=clf3.fit(X_train, y_train)\nxgb.fit(X_train, y_train)","a6970b78":"import matplotlib.pyplot as plt\n\nprint(\"Feature importance by XGBoost:->\\n\")\nXGBR = XGBClassifier()\nXGBR.fit(X,y)\nfeatures = XGBR.feature_importances_\nColumns = list(X.columns)\nfor i,j in enumerate(features):\n    print(Columns[i],\"->\",j)\nplt.figure(figsize=(16,5))\nplt.title(label=\"XGBC\")\nplt.bar([x for x in range(len(features))],features)\nplt.show()","3863acf0":"y_pred3=xgb.predict(X_test)\n\nprint(classification_report(y_test,y_pred3))\nroc_auc_score(y_test,y_pred3)","0da70003":"from sklearn.ensemble import GradientBoostingClassifier\n\n\ngbc = GradientBoostingClassifier()\n","4978ce6c":"#best_model3=clf3.fit(X_train, y_train)\ngbc.fit(X_train, y_train)","07a0718e":"y_pred4=gbc.predict(X_test)\n\nprint(classification_report(y_test,y_pred4))\nroc_auc_score(y_test,y_pred4)","df80d11e":"import lightgbm as lgb","7fca48f1":"\n#converting the dataset into proper LGB format \nd_train=lgb.Dataset(X_train, label=y_train)\n#Specifying the parameter\nparams={}\nparams['learning_rate']=0.03\nparams['boosting_type']='gbdt' #GradientBoostingDecisionTree\nparams['objective']='binary' #Binary target feature\nparams['metric']='binary_logloss' #metric for binary classification\nparams['max_depth']=10\n#train the model \nclf=lgb.train(params,d_train,100) #train the model on 100 epocs\n#prediction on the test set\ny_pred=clf.predict(X_test)","f5d5f7ee":"#rounding the values\ny_pred=y_pred.round(0)\n#converting from float to integer\ny_pred=y_pred.astype(int)\n#roc_auc_score metric\nroc_auc_score(y_pred,y_test)","2f6dc70a":"## XGBoost","b80a2985":"# Process Label Encoding","3ccefc24":"# Univariate Analysis","9d907b09":"## Logistic Regression","4334e437":"## Random Forest Classification","5c6e9aa3":"# GBC","df0e4d4d":"# Multivariate Analysis","fbb05068":"# Handling Outliers","41b0146f":"# Detect NA Values","d8427d13":"# LGBM","b5cf2dbc":"# Begin training model"}}