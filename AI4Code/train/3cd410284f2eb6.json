{"cell_type":{"37ca00e1":"code","d3359262":"code","6c44dd46":"code","93bf77f6":"code","83410dad":"code","d1d59dde":"code","82426b3d":"code","fbf2766b":"code","a53e54c0":"code","a099f1ce":"code","72ff25e8":"code","64f80de0":"code","e443230e":"code","da027309":"code","f17c5f7a":"code","98a6675e":"code","4cec4c02":"code","db050224":"code","05efa592":"code","e32cfdf7":"code","b1fcd580":"code","a3d75879":"code","0786a6ae":"code","cf9bdac6":"code","74a93108":"code","ed87c580":"code","dbdd97d4":"markdown","de9b53d1":"markdown","32ad638e":"markdown","8a33e5e7":"markdown","a484be60":"markdown","9c529eaf":"markdown","c025b5ef":"markdown","20a6812f":"markdown","6dfbd13c":"markdown","8d76e019":"markdown","02c781ca":"markdown","030bca73":"markdown","cef14a45":"markdown","b8c273ec":"markdown","cc769817":"markdown","b7d853f5":"markdown","6683d882":"markdown","fb4ba4ab":"markdown","84c1fcd2":"markdown","f6dc92b0":"markdown"},"source":{"37ca00e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3359262":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6c44dd46":"plt.figure(figsize=(15,12))\nplt.imshow(plt.imread(\"..\/input\/linear-reg\/linear_reg.jpeg\"))","93bf77f6":"plt.figure(figsize=(12,8))\nmonths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\nrevenue = [520, 740, 790, 950, 1150, 1100, 1290, 1260, 1470, 1460, 1560, 1840]\nsns.set_style(\"darkgrid\")\nsns.regplot(months,revenue,color=\"red\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"Revenue ($)\")","83410dad":"plt.figure(figsize=(15,12))\nplt.imshow(plt.imread(\"..\/input\/linear2\/linear2.gif\"))","d1d59dde":"plt.figure(figsize=(15,12))\nplt.imshow(plt.imread(\"..\/input\/minimize\/minimize.jpeg\"))","82426b3d":"plt.figure(figsize=(15,12))\nplt.imshow(plt.imread(\"..\/input\/linear3\/linear3.PNG\"))","fbf2766b":"def get_gradient_at_b(x, y, b, m):\n    N = len(x)\n    diff = 0\n    for i in range(N):\n        diff += (y[i] - ((m * x[i]) + b))\n    b_gradient = -(2\/N) * diff  \n    return b_gradient\n\ndef get_gradient_at_m(x, y, b, m):\n    N = len(x)\n    diff = 0\n    for i in range(N):\n        diff += x[i] * (y[i] - ((m * x[i]) + b))\n    m_gradient = -(2\/N) * diff  \n    return m_gradient","a53e54c0":"def step_gradient( x, y, b_current, m_current,learning_rate):\n    b_gradient = get_gradient_at_b(x, y, b_current, m_current)\n    m_gradient = get_gradient_at_m(x, y, b_current, m_current)\n    b = b_current - (learning_rate * b_gradient)\n    m = m_current - (learning_rate * m_gradient)\n    return [b, m]","a099f1ce":"b = 0 #Initial b\nm = 0 #Initial m\nb,m = step_gradient(months,revenue,b,m,learning_rate=0.001)\nprint(b, m)\n","72ff25e8":"#Your gradient_descent function here:  \ndef gradient_descent(x, y, learning_rate, num_iterations):\n    b=0\n    m=0\n    for i in range(num_iterations):\n        b,m = step_gradient(x, y,b, m,learning_rate)\n\n    return b,m\n","64f80de0":"b, m = gradient_descent(months, revenue, 0.001, 500) # we assign learning_rate as 0.1 and number of iterations as 500\nprint(f\"The slope of the linear regression is {m}\")\nprint(f\"The intercept of the linear regression is {m}\")","e443230e":"predictions = [x*m+b for x in months]\npred_df = pd.DataFrame(predictions,columns=[\"Predictions\"])\ny_df = pd.DataFrame(revenue, columns=[\"Real Values\"])\npd.concat([pred_df,y_df],axis=1)","da027309":"plt.figure(figsize=(15,12))\nplt.plot(months, revenue, \"o\",label=\"Real Values\")\nplt.plot(months, predictions,\"+\",label=\"Predictions\")\nplt.legend()\nplt.show()\n","f17c5f7a":"from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\nlinear.fit(np.array(months).reshape(-1,1),np.array(revenue).reshape(-1,1))\nprint(f\"The coeeficient(slope or m) of Builtin Sckitlearn Linear Regression Model is {linear.coef_}\")\nprint(f\"The coeeficient(slope or m) of our own  model is {m}\")\nprint(f\"The intercept(b) of Builtin Sckitlearn Linear Regression Model is {linear.intercept_}\")\nprint(f\"The intercept(b) of our own  model is {b}\")\n","98a6675e":"plt.figure(figsize=(15,12))\nplt.plot(months, revenue, \"o\",label=\"Real Values\")\nplt.plot(months, predictions,\"+\",label=\"Our Model's Predictions\")\nplt.plot(months, np.ravel(linear.predict(np.array(months).reshape(-1,1))),\"<\",label=\"Linear Predictions\")\nplt.title(\"Comparison Plot of Our Own Model and Sckitlearn Model Predictions with Real Values\")\nplt.legend()\nplt.show()","4cec4c02":"df = pd.read_csv(\"..\/input\/honey-production\/honeyproduction.csv\")\ndf.head()","db050224":"df.groupby(\"year\").mean()","05efa592":"df.groupby(\"year\").mean()","e32cfdf7":"df.groupby(\"year\").mean().plot(figsize=(12,10))","b1fcd580":"prod_per_year= df.groupby(\"year\").mean()[\"totalprod\"].reset_index()\nprod_per_year","a3d75879":"X = prod_per_year[\"year\"]\ny = prod_per_year[\"totalprod\"]\nplt.figure(figsize=(12,10))\nsns.regplot(X,y,color=\"red\")","0786a6ae":"b,m = gradient_descent(list(X), list(y), 0.01, 8) # we assign learning_rate as 0.01 and number of iterations as 15\nprint(f\"The slope of the linear regression is {m}\")\nprint(f\"The intercept of the linear regression is {b}\")","cf9bdac6":"predictions = [x*m+b for x in list(X)]\npred_df = pd.DataFrame(predictions,columns=[\"Predictions\"])\ny_df = pd.DataFrame(list(y), columns=[\"Real Values\"])\npd.concat([pred_df,y_df],axis=1)","74a93108":"from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()\nlinear_model.fit(X.values.reshape(-1,1),y.values.reshape(-1,1))\nprint(f\"The coeeficient(slope or m) of Builtin Sckitlearn Linear Regression Model is {linear_model.coef_}\")\nprint(f\"The coeeficient(slope or m) of our own  model is {m}\")\nprint(f\"The intercept(b) of Builtin Sckitlearn Linear Regression Model is {linear_model.intercept_}\")\nprint(f\"The intercept(b) of our own  model is {b}\")","ed87c580":"plt.figure(figsize=(15,12))\nplt.plot(X, y, \"o\",label=\"Real Values\")\nplt.plot(X, linear_model.predict(X.values.reshape(-1,1)),\"<\",label=\"Linear Predictions\")","dbdd97d4":"## 5. TRAINING THE MODEL AND MAKING PREDICTIONS","de9b53d1":"<font color=\"green\">\nIn order to find what the best fit is, for each data point, we calculate loss, a number that measures how bad the model\u2019s prediction is.We can think about loss as the squared distance from the point to the line. We do the squared distance (instead of just the distance) so that points above and below the line both contribute to total loss in the same way.\n    \nThe goal of a linear regression model is to find the slope and intercept pair that minimizes loss on average across all of the data.","32ad638e":"<font color=\"green\">\nThis is is the output of our first iteration, but we need make a iteration that enables us to calculate b and m several times to find the best values as follows:","8a33e5e7":"<font color=\"green\">\nAs we try to minimize loss, we take each parameter we are changing, and move it as long as we are decreasing loss. It\u2019s like we are moving down a hill, and stop once we reach the bottom.The process by which we do this is called gradient descent. We move in the direction that decreases our loss the most. Gradient refers to the slope of the curve at any point.","a484be60":"## 1. Introduction to Linear Regression","9c529eaf":"<font color=\"green\">\nAs we try to minimize loss, we take each parameter we are changing, and move it as long as we are decreasing loss. It\u2019s like we are moving down a hill, and stop once we reach the bottom","c025b5ef":"<font color=\"green\">\nAs you may have already heard, the honeybees are in a precarious state right now. You may have seen articles about the decline of the honeybee population for various reasons. You want to investigate this decline and how the trends of the past predict the future for the honeybees.","20a6812f":"<font color=\"green\">\nLets see the change in total production of honey with Linear Regression","6dfbd13c":"<font color=\"green\">\nTo find new m and b, we need to create a step_gradient function which calculate \n    \n    new b = current_b - (learning_rate * b_gradient) \n    and \n    new m = current_m - (learning_rate * m_gradient) ","8d76e019":"## 6.APPLYING SCKIT-LEARN BUILT IN LINEAR REGRESSION MODEL AND COMPARE WITH OUR RESULTS","02c781ca":"<font color=\"green\">\nAs we can in the figure below, the total honey production decreases. ","030bca73":"## 7. APPLYING REAL LIFE DATA : HONEY PREDICTIONS","cef14a45":"<font color=\"green\">\nRegression is used to predict outputs that are continuous. The outputs are quantities that can be flexibly determined based on the inputs of the model rather than being confined to a set of possible labels.\n\nFor example:\n\nPredict the height of a potted plant from the amount of rainfall\nPredict salary based on someone\u2019s age and availability of high-speed internet\nPredict a car\u2019s MPG (miles per gallon) based on size and model year","b8c273ec":"## 2. Linear Loss Function","cc769817":"<font color=\"green\">\nLets use Linear Regression Model:","b7d853f5":"<font color=\"green\">\nThe purpose of machine learning is often to create a model that explains some real-world data, so that we can predict what may happen next, with different inputs.\n\nIn  Linear Regression we are trying to find a line that fits a set of data best like yhe market price of a house vs. the square footage of a house by predicting how much a house will sell for, given its size.\nA line is a rough approximation, but it allows us the ability to explain and predict variables that have a linear relationship with each other.","6683d882":"<font color=\"green\">\nThe regression line is determined by its slope and its intercept, where m is the slope, and b is the intercept. The aim of Linear Regression is to find the \u201cbest\u201d m and b for our data. After finding m and b, we can easily predict any y values corresponding to any x value.","fb4ba4ab":"## 3. Gradient Descent","84c1fcd2":"<font color=\"green\">\nFrom this line, we can estimate that after the 12. month, the revenue will be approximately between 1800 and 2000.But Linear Regression gives us best prediction for not for a single value but lots of values we want to predict.","f6dc92b0":"## 7.1. Training the Model and Evaluation of Model Performance"}}