{"cell_type":{"9cb14e01":"code","13386bfd":"code","3c09d9a8":"code","0bf6f80c":"code","50524906":"code","b667ee36":"code","804f5dd8":"code","28eb87b6":"code","c3911d22":"code","d3e4666c":"code","3f5dd03b":"code","75c42e8b":"code","965c7f42":"code","8385769b":"code","1c0dc544":"code","20c9b119":"code","f18c7b0a":"code","c2ef50a4":"code","76eef1cd":"code","41849b9d":"code","c5b1c226":"code","d2a9fe06":"code","f8957c52":"code","9606fe8f":"code","0c956a0a":"code","6ce5d3fb":"code","47c9a196":"code","a1619886":"code","6b7e0c0a":"code","897137f8":"code","9d39d1aa":"code","41e3f015":"code","200ceaa5":"code","f471fa03":"code","062971b2":"code","47b116d2":"code","8167a7ca":"markdown","3f46906f":"markdown","3f3e1d1b":"markdown","801b4ba1":"markdown","def1b810":"markdown","c48b8b9f":"markdown","cbf7530f":"markdown","536c4b52":"markdown","c5221fb1":"markdown","91b77a12":"markdown","43bf4c78":"markdown","b54cb650":"markdown","82861348":"markdown","e07f8276":"markdown","b62beda4":"markdown","44697058":"markdown","eb3d1e44":"markdown","6f637fdd":"markdown","42d8fcdc":"markdown","f53ac340":"markdown"},"source":{"9cb14e01":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#import seaborn as sn\nfrom sklearn.model_selection import train_test_split\n\nfrom random import seed\nseed(1)\nseed = 43\n\nimport tensorflow as tf\nfrom tensorflow import keras\nprint(\"Tensorflow Version: \", tf.__version__)\nprint(\"Keras Version: \",keras.__version__)\n\n\nkaggle = 1 # Kaggle path active = 1\n\n# change your local path here\nif kaggle == 1 :\n    MNIST_PATH= '..\/input\/digit-recognizer'\nelse:\n    MNIST_PATH= '..\/Digit_Recognition_with_a_Deep_Neural_Network\/data\/input\/digit-recognizer'\n\n\n\nimport os\nfor dirname, _, filenames in os.walk(MNIST_PATH): \n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","13386bfd":"# Data path and file\nCSV_FILE_TRAIN='train.csv'\nCSV_FILE_TEST='test.csv'\n \ndef load_mnist_data(minist_path, csv_file):\n    csv_path = os.path.join(minist_path, csv_file)\n    return pd.read_csv(csv_path)\n\ndef load_mnist_data_manuel(minist_path, csv_file):\n    csv_path = os.path.join(minist_path, csv_file)\n    csv_file = open(csv_path, 'r')\n    csv_data = csv_file.readlines()\n    csv_file.close()\n    return csv_data\n\ndef split_train_val(data, val_ratio):\n    return \n    \n\ntrain = load_mnist_data(MNIST_PATH,CSV_FILE_TRAIN)\ntest = load_mnist_data(MNIST_PATH,CSV_FILE_TEST)","3c09d9a8":"y = train['label'].copy()\nX = train.drop(['label'], axis=1)\n\nX_test = test.copy()","0bf6f80c":"print(\"Shape of the Features: \",X.shape)\nprint(\"Shape of the Labels: \", y.shape)","50524906":"train.value_counts('label')","b667ee36":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=seed, test_size=0.15\n                                                  , stratify=y\n                                                 )","804f5dd8":"print(\"Train - Set Distribution\")\nprint(y_train.value_counts() \/ y_train.value_counts().sum() )\nprint('--------------------------------------------------------------')\nprint('--------------------------------------------------------------')\nprint('--------------------------------------------------------------')\nprint(\"Val - Set Distribution\")\nprint(y_val.value_counts() \/ y_val.value_counts().sum() )\n","28eb87b6":"print(\"X: \", X.shape)\nprint(\"X_train: \", X_train.shape)\nprint(\"X_val: \", X_val.shape)\n\nprint(\"y_train: \", y_train.shape)\nprint(\"y_val: \", y_val.shape)","c3911d22":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([\n    ('normalizer', Normalizer())\n    #('std_scalar',StandardScaler())\n])","d3e4666c":"X_train_prep = pipeline.fit_transform(X_train)      # fitting the pipeline to the train and transform it\nX_val_prep = pipeline.transform(X_val)              # transform val data with this information","3f5dd03b":"root_logdir = \"..\/..\/tensorboard-logs\"\n\nprint(\"Relative root_logdir: \",root_logdir)\n\ndef get_run_logdir():\n    import time\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n    return os.path.join(root_logdir,run_id)","75c42e8b":"run_logdir = get_run_logdir()\nprint(\"Current run logdir for Tensorboard: \", run_logdir)","965c7f42":"run_logdir","8385769b":"tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)","1c0dc544":"def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[784]):\n    model = keras.models.Sequential()                               # base model structure (Sequential API by Keras)\n\n    model.add(keras.layers.InputLayer(input_shape=input_shape))     # input layer\n\n    for layer in range(n_hidden):                                   # add layers as often as defined in constructor \n        model.add(keras.layers.Dense(n_neurons,activation=\"relu\"))  # add layer with given neurons and relu activation function\n\n    model.add(keras.layers.Dense(10, activation=\"softmax\"))                               # add output layer \n\n    optimizer = keras.optimizers.SGD(learning_rate=learning_rate)   # define optimizer (especially the larning rate for hyperparameter optimization)\n\n    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])                  # make it ready\n\n    return model\n","20c9b119":"# Using keras wrapper as hull \nkeras_cl = keras.wrappers.scikit_learn.KerasClassifier(build_model)","f18c7b0a":"from scipy.stats import reciprocal\n\n# Hyperparameter set\nparam_dist= {\n            \"n_neurons\": range(20, 500, 20)\n            ,\"n_hidden\": range(10, 100, 10)\n            ,\"learning_rate\": [1e-3, 2e-3]\n    }\n\n\nparam_dist_lr= {\n        \"n_neurons\": [10, 50, 100, 150, 300]\n        ,\"n_hidden\": [10, 50, 100, 150]\n        ,\"learning_rate\": [1e-3, 3e-4, 3e-2]\n}\n\n\nparam_dist_bestrun_1 = {\n        \"n_neurons\": [150]\n        ,\"n_hidden\": [30]\n        ,\"learning_rate\": [2e-3]  \n}\n\n\nparam_dist_bestrun_2 = {\n        \"n_neurons\": [100]\n        ,\"n_hidden\": [10]\n        ,\"learning_rate\": [2e-3]  \n}","c2ef50a4":"checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cl_model.h5\", save_best_only=True, save_weights_only=False)","76eef1cd":"from sklearn.model_selection import RandomizedSearchCV\n\nran_ker_cl = RandomizedSearchCV(keras_cl, param_dist_lr, n_iter=10, n_jobs=5, cv=3, random_state=seed, return_train_score=True)\nhistory_ker_cl = ran_ker_cl.fit(X_train_prep, y_train, epochs=50, validation_data=(X_val_prep, y_val), callbacks=[checkpoint_cb, keras.callbacks.EarlyStopping(patience=5), tensorboard_cb])","41849b9d":"history_ker_cl.best_params_","c5b1c226":"history_ker_cl","d2a9fe06":"# Creating wrapped regression model with our function. \nkeras_cl_model = keras_cl.build_fn(n_neurons= 150, n_hidden= 10, learning_rate=0.03)","f8957c52":"keras_cl_model.summary()","9606fe8f":"# creating a new log dir for tensorboard\ntensorboard_cb_f = keras.callbacks.TensorBoard(get_run_logdir())\ncheckpoint_cb_f = keras.callbacks.ModelCheckpoint(\"my_keras_cl_model.h5\", save_best_only=False, save_weights_only=False)","0c956a0a":"# preparing data based on our beautifull trained data pipeline\nX_prep_all = pipeline.transform(X)","6ce5d3fb":"# Train the model again pleeeeease with all you got .... especially the new transformed data matrix X \nkeras_cl_model.fit(X_prep_all, y, epochs=100, callbacks=[tensorboard_cb_f, checkpoint_cb_f])","47c9a196":"X_test_prep = pipeline.transform(X_test)","a1619886":"X_test_prep","6b7e0c0a":"mnist_competition_file = pd.DataFrame(columns=['ImageId','Label'])","897137f8":"plt.imshow(X_test_prep[43].reshape(28,28), cmap='Greys')","9d39d1aa":"print(\"Propability of all lables for given pixels: \", keras_cl_model.predict(X_test_prep[43].reshape(1,-1)))","41e3f015":"print(\"Predicted Digit: \",np.argmax(keras_cl_model.predict(X_test_prep[43].reshape(1,-1))))","200ceaa5":"i = 1\r\nfor row in X_test_prep:\r\n    index = i\r\n    predicted_label = np.argmax(keras_cl_model.predict(row.reshape(1,-1)))\r\n\r\n    mnist_competition_file = mnist_competition_file.append({'ImageId': index, 'Label': predicted_label}, ignore_index = True )\r\n    i = i + 1\r\n    pass","f471fa03":"mnist_competition_file","062971b2":"mnist_competition_file.ImageId = mnist_competition_file.ImageId.astype(int)\r\nmnist_competition_file.Label = mnist_competition_file.Label.astype(int)","47b116d2":"mnist_competition_file.to_csv('mnist_submission.csv', index=False)","8167a7ca":"### Model Training with Full Dataset \r\nIn this part I will train the model with the full dataset. This time I will use the discovered hyperparameters from the randomized search from the previous part.\r\n\r\nBased on the hyperparameter search the following parameters were found:\r\n- n_neurons = 150\r\n- n_hidden = 10\r\n- learning_rate = 0.03","3f46906f":"# Image Prediction of Unknown Data (Test Data)","3f3e1d1b":"# Introduction - MNIST Training Competition\nLink to the topic: https:\/\/www.kaggle.com\/c\/digit-recognizer\/data\n\nThis is another Notebook to take a look into annother algorithm. Here I want to give the Deep Neural Network with the Framework Keras a try. As already mentioned in other notebooks, I will skip some explanations about the data set here. Moreover I will use the already discovered knowledge about the data and transform\/prepare the data rightaway.\n\nIf you are interested in some more clearly analysis of the dataset take a look into my other notebooks about the MNIS-dataset:\n- Another MNIST Try: https:\/\/www.kaggle.com\/skiplik\/another-mnist-try\n- First NN by Detecting Handwritten Characters: https:\/\/www.kaggle.com\/skiplik\/first-nn-by-detecting-handwritten-characters\n...\n\n\n","801b4ba1":"### Label Value Count\r\nVisualizing the label distribution of the full train dataset.","def1b810":"## Model Checkpoints","c48b8b9f":"## Peparing Test Data\r\nThe test data for the competition needs to be prepared as well as did with the training data set. Therefore the trained pipeline (trained only on the training dataset) will be used.","cbf7530f":"## Creating Competition File","536c4b52":"### Keras Callbacks for Tensorboard\nWith Keras there is a way of using Callbacks for the Tensorboard to write log files for the board and visualize the different graphs (loss and val curve)\n","c5221fb1":"Comparing the equally splitted train- and val-sets based on the given label y.","91b77a12":"## Train \/ Val Split","43bf4c78":"## Building Transforming Piplines","b54cb650":"## Prediction of Testdata","82861348":"# Get Data","e07f8276":"## Preparing Model Visualization with Tensorboard (not for Kaggle)","b62beda4":"## Building Model Architecture","44697058":"### Architecture for Hyperparameter Optimization\r\n- Amount of Layers\r\n- Amount of Neurons\r\n- Learningrate\r\n- Checkpoints\r\n- Early Stopping ","eb3d1e44":"## Model Training","6f637fdd":"# Building a Deep Neural Network based on RandomizedSearch","42d8fcdc":"### Hyperparameter Space","f53ac340":"### Randomized Search\r\nFinding best hyperparameters with Randomized search"}}