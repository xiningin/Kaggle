{"cell_type":{"9dd89edd":"code","c4d4a609":"code","473e0c61":"code","0e1e3f5a":"code","282aead6":"code","e07a4aca":"code","69758963":"code","d1c2e469":"code","18063893":"code","8da6fad8":"code","55411e61":"code","203a3f00":"code","d07dd1a4":"code","f4bf1840":"code","c81d82ce":"code","7de31656":"code","4080ba27":"code","15ea702d":"code","86cf5e14":"code","a655137c":"code","6c602df5":"code","c851ae6c":"code","072ea7b3":"code","c11c76e8":"code","1df40c74":"code","0c67ddc0":"code","11fb33b2":"code","8b2cc836":"code","c1da4bce":"code","66400866":"code","f8013f3a":"markdown","f88904a3":"markdown","59fc36d2":"markdown","be38bc6e":"markdown","313ad676":"markdown","51843538":"markdown","aeabd772":"markdown","7d29ca0d":"markdown"},"source":{"9dd89edd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\n#Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport tensorflow as tf\ntf.logging.set_verbosity(tf.logging.INFO)","c4d4a609":"df_train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ndf_test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\ndf_sample = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')","473e0c61":"df_train.head(5)","0e1e3f5a":"df_test.head(5)","282aead6":"df_train['comment_text'][0]","e07a4aca":"lengths = df_train.comment_text.str.len()\nlengths.mean(), lengths.std(), lengths.min(), lengths.max()","69758963":"lengths = df_test.comment_text.str.len()\nlengths.mean(), lengths.std(), lengths.min(), lengths.max()","d1c2e469":"def preprocess_reviews(text):\n    text = re.sub(r'<[^>]*>', ' ', text, re.UNICODE)\n    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n    text = re.sub(r'[^0-9a-zA-Z]+',' ',text, re.UNICODE)\n    text = \" \".join(text.split())\n    text = text.lower()\n    return text\n\ndf_train['comment_text'] = df_train.comment_text.apply(lambda x: preprocess_reviews(x))\ndf_test['comment_text'] = df_test.comment_text.apply(lambda x: preprocess_reviews(x))","18063893":"# force train into cola format, test is fine as it is\ndf_train = df_train[['id', 'target', 'comment_text']]\ndf_train['target'] = np.where(df_train['target']>=0.5,1,0)\n\n#Sampling 30% to save training time\ndf_train = df_train.sample(frac=0.3)\n\n# export as tab seperated\ndf_train.to_csv('train.tsv', sep='\\t', index=False, header=False)\ndf_test.to_csv('test.tsv', sep='\\t', index=False, header=True)","8da6fad8":"df_train.shape, df_test.shape","55411e61":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\nfor f in os.listdir('..\/input\/xlnetcode\/'):\n    try:\n        if f.split('.')[1] in ['py', 'json']:\n            copyfile(src = \"..\/input\/xlnetcode\/\"+f, dst = \"..\/working\/\"+f)\n    except:\n        continue\nprint(os.listdir('..\/working'))","203a3f00":"from absl import flags\nimport xlnet\nfrom run_classifier import *\nimport sys","d07dd1a4":"remaining_args = FLAGS([sys.argv[0]] + [flag for flag in sys.argv if flag.startswith(\"--\")])\nassert(remaining_args == [sys.argv[0]])","f4bf1840":"FLAGS.spiece_model_file = '..\/input\/xlnetcode\/spiece.model'\nFLAGS.model_config_path = '..\/input\/xlnetcode\/xlnet_config.json'\nFLAGS.output_dir =\"..\/\"\nFLAGS.model_dir = \"..\/\"\nFLAGS.data_dir = \"..\/working\/\"\nFLAGS.do_train = False\nFLAGS.train_steps = 1000\nFLAGS.warmup_steps = 0\nFLAGS.learning_rate = 1e-5\nFLAGS.max_save = 999999\nFLAGS.use_tpu = False\n\n#Used not take any of the processors and get from the tasks\nFLAGS.cls_scope = True","c81d82ce":"# Tokenization\nimport sentencepiece as spm\nfrom prepro_utils import preprocess_text, encode_ids\n\nsp = spm.SentencePieceProcessor()\nsp.Load(FLAGS.spiece_model_file)\ndef tokenize_fn(text):\n    text = preprocess_text(text, lower=FLAGS.uncased)\n    return encode_ids(sp, text)","7de31656":"processor = GLUEProcessor()\nlabel_list = processor.get_labels()\nprocessor.label_column = 1\nprocessor.text_a_column = 2\nprocessor.test_text_a_column = 1\ntrain_examples = processor.get_train_examples(FLAGS.data_dir)","4080ba27":"train_examples[0].label, train_examples[0].text_a, train_examples[0].text_b ","15ea702d":"start = time.time()\nprint(\"--------------------------------------------------------\")\nprint(\"Starting to Train\")\nprint(\"--------------------------------------------------------\")","86cf5e14":"train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\")\ntf.logging.info(\"Use tfrecord file {}\".format(train_file))\nnp.random.shuffle(train_examples)\ntf.logging.info(\"Num of train samples: {}\".format(len(train_examples)))\nfile_based_convert_examples_to_features(\n        train_examples, label_list, FLAGS.max_seq_length, tokenize_fn,\n        train_file, FLAGS.num_passes)","a655137c":"# RunConfig contains hyperparameters that could be different between pretraining and finetuning.\ntpu_cluster_resolver = None\nis_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    master=FLAGS.master,\n    model_dir=FLAGS.output_dir,\n    save_checkpoints_steps=FLAGS.save_steps,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=FLAGS.iterations,\n        num_shards=FLAGS.num_core_per_host,\n        per_host_input_for_training=is_per_host))\nmodel_fn = get_model_fn(len(label_list) if label_list is not None else None)","6c602df5":"tf.logging.set_verbosity(tf.logging.INFO)\nestimator = tf.contrib.tpu.TPUEstimator(\n        use_tpu=FLAGS.use_tpu,\n        model_fn=model_fn,\n        config=run_config,\n        train_batch_size=FLAGS.train_batch_size,\n        predict_batch_size=FLAGS.predict_batch_size,\n        eval_batch_size=FLAGS.eval_batch_size)\n\ntf.logging.info(\"***** Running training *****\")\ntf.logging.info(\"  Num examples = %d\", len(train_examples))\ntf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\ntf.logging.info(\"  Num steps = %d\", FLAGS.iterations)","c851ae6c":"train_input_fn = file_based_input_fn_builder(\n        input_file=train_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)","072ea7b3":"estimator.train(input_fn=train_input_fn, max_steps=FLAGS.train_steps)","c11c76e8":"end = time.time()\nprint(\"--------------------------------------------------------\")\nprint(\"Total time taken to complete training - \", end - start, \" seconds\")\nprint(\"--------------------------------------------------------\")","1df40c74":"test_examples = processor.get_test_examples(FLAGS.data_dir)\ntf.logging.info(\"Num of test samples: {}\".format(len(test_examples)))\neval_file = os.path.join(FLAGS.output_dir, \"predict.tf_record\")\nfile_based_convert_examples_to_features(\n        test_examples, label_list, FLAGS.max_seq_length, tokenize_fn,\n        eval_file)","0c67ddc0":"os.path.getsize('..\/predict.tf_record')","11fb33b2":"pred_input_fn = file_based_input_fn_builder(\n        input_file=eval_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=False)\npredict_results = []\nwith tf.gfile.Open(\"test_results.tsv\", \"w\") as fout:\n    fout.write(\"index\\tprediction\\n\")\n\n    for pred_cnt, result in enumerate(estimator.predict(\n        input_fn=pred_input_fn,\n        yield_single_examples=True)):\n        if pred_cnt % 1000 == 0:\n            tf.logging.info(\"Predicting submission for example: {}\".format(\n              pred_cnt))\n\n        logits = [float(x) for x in result[\"logits\"].flat]\n        predict_results.append(logits)\n\n        if len(logits) == 1:\n            label_out = logits[0]\n        elif len(logits) == 2:\n            if logits[1] - logits[0] > FLAGS.predict_threshold:\n                label_out = label_list[1]\n            else:\n                label_out = label_list[0]\n        elif len(logits) > 2:\n            max_index = np.argmax(np.array(logits, dtype=np.float32))\n            label_out = label_list[max_index]\n        else:\n            raise NotImplementedError\n\n        fout.write(\"{}\\t{}\\n\".format(pred_cnt, label_out))","8b2cc836":"len(test_examples), len(predict_results)","c1da4bce":"df_test_out = pd.read_csv('test_results.tsv', sep='\\t')","66400866":"submission = pd.concat([df_sample.iloc[:,0], df_test_out.iloc[:,1]], axis=1)\nsubmission.columns = ['id','prediction']\nsubmission.to_csv('submission.csv', index=False, header=True)","f8013f3a":"# Prediction","f88904a3":"**Let's copy the XLNet files from git repo to working folder for easy reference**","59fc36d2":"# Kernel details\n\nWe will use the GLUEProcessor in XLNet to finetune and train the Unintended Bias Toxicity Classification Dataset.","be38bc6e":"## Using appropriate XLNet implementation from here\n**SentencePiece Tokenizer implementation**","313ad676":"**Preprocess and create TSV files to perform XLNet classification**","51843538":"**Creating submission file**","aeabd772":"**Performing this step to initialise FLAGS in IPython Notebook**","7d29ca0d":"**Initialise GLUEProcessor and specify the column indexes in test and train datasets and create examples**"}}