{"cell_type":{"93500594":"code","9e85b38f":"code","da95d5b0":"code","cee64c25":"code","bd89001d":"code","14b4dc8a":"code","6f4a494c":"code","c0d724f4":"code","cb679caa":"code","e81dea4f":"code","87d0869e":"code","d9969812":"code","5646add6":"code","ac7bd6b4":"code","449adb17":"code","e0ffac37":"code","82a3b036":"code","5d5467e4":"code","2982c98c":"code","0971ed59":"code","a789d5bb":"code","c2c540dd":"markdown","a8305cd0":"markdown","8337198c":"markdown","1869d4cf":"markdown","d431c00d":"markdown","b964069f":"markdown","5dc30ce6":"markdown","9c44283d":"markdown","6b526c4c":"markdown","0dd1f2c4":"markdown","155f733b":"markdown","6565f691":"markdown","694e7e1a":"markdown","5b25e399":"markdown","e0b24320":"markdown","d46f4b9c":"markdown","e0f30d9c":"markdown"},"source":{"93500594":"!pip uninstall -y typing # this should avoid  AttributeError: type object 'Callable' has no attribute '_abc_registry'","9e85b38f":"import sys\nsys.path.insert(0, \"..\/input\/tabnetfeatmultitaskclassification\/tabnet-feat-MultiTaskClassification\")","da95d5b0":"from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\nfrom tqdm.notebook import tqdm\n\nimport os\n\nfrom matplotlib import pyplot as plt\n%matplotlib inline","cee64c25":"dataset_name = \"lish-moa\"\ntrain = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets.drop(columns=[\"sig_id\"], inplace=True)\n\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')","bd89001d":"np.random.seed(42)\nif \"Set\" not in train.columns:\n    train[\"Set\"] = np.random.choice([\"train\", \"valid\"], p =[.8, .2], size=(train.shape[0],))\n\ntrain_indices = train[train.Set==\"train\"].index\nvalid_indices = train[train.Set==\"valid\"].index","14b4dc8a":"# Encoding train set and test set\n\nnunique = train.nunique()\ntypes = train.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in tqdm(train.columns):\n    if types[col] == 'object' or nunique[col] < 200:\n        print(col, train[col].nunique())\n        l_enc = LabelEncoder()\n        train[col] = train[col].fillna(\"VV_likely\")\n        train[col] = l_enc.fit_transform(train[col].values)\n        try:\n            test[col] = test[col].fillna(\"VV_likely\")\n            test[col] = l_enc.transform(test[col].values)\n        except:\n            print(f\"Column {col} does not exist in test set\")\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        training_mean = train.loc[train_indices, col].mean()\n        train.fillna(training_mean, inplace=True)\n        test.fillna(training_mean, inplace=True)","6f4a494c":"unused_feat = ['Set', 'sig_id'] # Let's not use splitting sets and sig_id\n\nfeatures = [ col for col in train.columns if col not in unused_feat] \n\ncat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n","c0d724f4":"\nX_train = train[features].values[train_indices]\ny_train = train_targets.values[train_indices]\n\nX_valid = train[features].values[valid_indices]\ny_valid = train_targets.values[valid_indices]\n\nX_test = test[features].values\n","cb679caa":"\nclf = TabNetMultiTaskClassifier(cat_idxs=cat_idxs,\n                                cat_dims=cat_dims,\n                                cat_emb_dim=1,\n                                optimizer_fn=torch.optim.Adam,\n                                optimizer_params=dict(lr=2e-2),\n                                scheduler_params={\"step_size\":50, # how to use learning rate scheduler\n                                                  \"gamma\":0.9},\n                                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n                                mask_type='entmax', # \"sparsemax\",\n                                lambda_sparse=0, # don't penalize for sparser attention\n                       \n                      )","e81dea4f":"max_epochs = 1000\nclf.fit(\n    X_train=X_train, y_train=y_train,\n    X_valid=X_valid, y_valid=y_valid,\n    max_epochs=max_epochs ,\n    patience=50, # please be patient ^^\n    batch_size=1024,\n    virtual_batch_size=128,\n    num_workers=1,\n    drop_last=False,\n)\n\n# scores displayed here are -average of log loss\n\n# TabNet is not as fast as XGBoost (at least for binary classification and regression problems)\n# If you wish to speed things up you could play with batch_size, virtual_batch_size and num_workers (or create a smaller network with less steps)\n# Another way to speed things up is to improve the source code : please contribute here https:\/\/github.com\/dreamquark-ai\/tabnet\/issues\/183","87d0869e":"# plot losses (drop first epochs to have a nice plot)\nplt.plot(clf.history['train']['loss'][5:])\nplt.plot(clf.history['valid']['loss'][5:])","d9969812":"# plot learning rates\nplt.plot([x for x in clf.history['train']['lr']][5:])","5646add6":"preds_valid = clf.predict_proba(X_valid) # This is a list of results for each task\n\n# We are here getting rid of tasks where only 0 are available in the validation set\nvalid_aucs = [roc_auc_score(y_score=task_pred[:,1], y_true=y_valid[:, task_idx])\n             for task_idx, (task_pred, n_pos) in enumerate(zip(preds_valid, y_valid.sum(axis=0))) if n_pos > 0]\n\nvalid_logloss = [log_loss(y_pred=task_pred[:,1], y_true=y_valid[:, task_idx])\n             for task_idx, (task_pred, n_pos) in enumerate(zip(preds_valid, y_valid.sum(axis=0))) if n_pos > 0]\n\nplt.scatter(y_valid.sum(axis=0)[y_valid.sum(axis=0)>0], valid_aucs)","ac7bd6b4":"# Valid score should match mean log loss - They don't match exactly because we removed some tasks\nprint(f\"BEST VALID SCORE FOR {dataset_name} : {clf.best_cost}\")\nprint(f\"VALIDATION MEAN LOGLOSS SCORES FOR {dataset_name} : {np.mean(valid_logloss)}\")\nprint(f\"VALIDATION MEAN AUC SCORES FOR {dataset_name} : {np.mean(valid_aucs)}\")","449adb17":"preds = clf.predict_proba(X_test)","e0ffac37":"# save tabnet model\nsaving_path_name = \".\/TabNetMultiTaskClassifier_baseline\"\nsaved_filepath = clf.save_model(saving_path_name)","82a3b036":"# define new model with basic parameters and load state dict weights (all parameters will be updated)\nloaded_clf = TabNetMultiTaskClassifier()\nloaded_clf.load_model(saved_filepath)","5d5467e4":"loaded_preds = loaded_clf.predict_proba(X_test)\n\n# Make sure that this is working as expected\nnp.testing.assert_array_equal(preds, loaded_preds)","2982c98c":"clf.feature_importances_","0971ed59":"explain_matrix, masks = clf.explain(X_test)","a789d5bb":"fig, axs = plt.subplots(1, 3, figsize=(20,20))\n\nfor i in range(3):\n    axs[i].imshow(masks[i][:500])\n    axs[i].set_title(f\"mask {i}\")\n","c2c540dd":"# Import libraries","a8305cd0":"# Training","8337198c":"# Local explainability and masks for test set\n\nExplain matrix is not normalized so rows don't sum to 1, feel free to normalize them yourself\n\nYou can see that attention is quite sparse and this visualization with so many columns is not the best","1869d4cf":"## Predictions","d431c00d":"# Save and load Model\n\nJust an example of how to save and load models in order to use them later","b964069f":"# Installing pytorch-tabnet from the MultiTask branch (not official branch)","5dc30ce6":"Please see Optimo's original notebook here: https:\/\/www.kaggle.com\/optimo\/tabnetmultitaskclassifier\n\nThe only changes I've made are to turn off internet and use Optimo's branch from a dataset","9c44283d":"# Creating train\/valid\/test matrices","6b526c4c":"# Download data","0dd1f2c4":"# Define network and parameters\n\n### This is a set of basic parameters, happy tuning!","155f733b":"# Random Split, do something smarter if you want)","6565f691":"# End of notebook\n\nHope this will be usefull!","694e7e1a":"# Define categorical features for categorical embeddings","5b25e399":"# About this notebook\n\nTabNetMultiTaskClassifier is still under development, I'm sharing this now in a co-construction approach.\n\nPlease share your feedback as comments here or directly in the corresponding PR on the github repo : https:\/\/github.com\/dreamquark-ai\/tabnet\/pull\/184\n\nOnce carefully reviewed I'll release this in the official repo.\n\nIf you have questions about the model please first have a look at the README here : https:\/\/github.com\/dreamquark-ai\/tabnet\/blob\/feat\/MultiTaskClassification\/README.md\n\nMaybe have a look at this video for more in depth explanations : https:\/\/youtu.be\/ysBaZO8YmX8 \n\n# About TabNetMultiTaskClassifier\n\nTabNetMultiTaskClassifier is a new class of pytorch-tabnet, it allows you to easily deal with multi task classification problem. (Note: for multi task regression problems you can use TabNetRegressor).\n\nSome of the available features are:\n\n- any number of tasks is allowed\n- each task can have any number of labels\n- you can pass different loss functions for each task by giving a corresponding list of loss function during fit\n\n# About the competition\n\nI am not going to share a full running submission as I did not started the competition yet and I'd like people to try a solution of their own.\n\nThis should be enough for people to start playing around, feel free to fork this in order to get a score.\nIf you end up with a good scoring kernel with a few twick please share it publicly, we are still very early in the competition and TabNet is easy to use and open source.\n\nDisclaimer : I actually don't know how competitive this simple notebook will be, but this is just a baseline to improve upon!\n\n\n# About pytorch-tabnet\n\nPytorch-tabnet is an open source project, feel free to contribute!\n\n\n**Good luck to all, have fun!**","e0b24320":"# Global explainability : feat importance summing to 1","d46f4b9c":"# Validation\n\nI don't know if people have been looking at these AUCs plots but some tasks are harder than others!","e0f30d9c":"# Simple preprocessing\n\n\nCopy\/pasted from my example notebooks, don't know if we have missing values.\n\nLabel encode categorical features and fill empty cells.\n\n\nDo any smarter preprocessing if you want."}}