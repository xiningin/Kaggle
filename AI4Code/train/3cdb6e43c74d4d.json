{"cell_type":{"916fec09":"code","8e4f5737":"code","c0de3183":"code","94ab2d33":"code","c39a1941":"code","76f7fc92":"code","ba3637d0":"code","1c98a064":"code","7f3e0723":"code","aa5074d2":"code","0a46c2e1":"code","d6e24ffd":"code","aff58b89":"code","6c063754":"code","bd21d2bb":"code","5bbe8499":"code","96938823":"code","24a0fbaf":"code","a260d793":"code","f40d7d8a":"code","0c743e4d":"code","cce6db9c":"code","1ba928b7":"code","2e09c1e4":"code","d1b33699":"code","d010bbe5":"code","e5e25cf5":"code","4c40d1aa":"code","f2bd0da2":"code","e08954a7":"code","19418289":"code","92865961":"code","39aba6df":"code","6e23be9d":"code","70b3a9c6":"code","d9b1b6b1":"code","08ac6b9c":"code","cec71fcf":"code","0add94b7":"code","a6e419cd":"code","0d30e66d":"code","04348ee5":"code","fc8759f9":"code","29593c54":"code","d71b6750":"code","b31288b0":"code","66634a6f":"code","188fa4ea":"code","a031ee5e":"code","2225fee4":"code","c6e95ea5":"code","c786e18a":"code","1bdd81c5":"code","e28519b4":"code","c3c1f01b":"code","2b67aa5b":"code","c90c5cbb":"code","15e5ea77":"code","8c4b8bf3":"code","751e85f6":"code","c5b638f2":"code","6a3647fe":"code","4e4e2b75":"code","c2672740":"code","520f721d":"code","d7e54444":"code","9f2bc343":"code","a6e597cb":"code","319970c6":"code","55fe6830":"code","008b997e":"code","0a416af8":"code","1ed0d41c":"code","25ec53dd":"code","a34ddc25":"code","3c382b66":"code","807848e1":"code","af66f3ff":"code","e20ff0de":"code","f57a17c6":"code","b376d79a":"code","853588d5":"code","22a7830e":"code","60502f2a":"markdown","1b0c0a50":"markdown","e36b1903":"markdown","92969289":"markdown","559b1506":"markdown","ea3e718b":"markdown","6f4ed10c":"markdown","8404c26d":"markdown","8b3b6cec":"markdown","57749815":"markdown","da12c4aa":"markdown","fe2823f5":"markdown","e78acf53":"markdown","857b2e44":"markdown","9ee6a050":"markdown","0ec03fbf":"markdown","49f44c1e":"markdown","45946107":"markdown","4987bce4":"markdown","2bc3f3b4":"markdown","ef226367":"markdown","c853a0c4":"markdown","95f1447a":"markdown","20ec199f":"markdown","7d0c2f19":"markdown","efeffcc5":"markdown","ea6ab55d":"markdown","f38f20e8":"markdown","000be416":"markdown","8ca0b081":"markdown","f8a7f1a9":"markdown","36cded53":"markdown","a98f7536":"markdown","6ff7b66c":"markdown","82e77364":"markdown","a75ff0cc":"markdown","b2be814f":"markdown"},"source":{"916fec09":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import preprocessing \nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn import preprocessing \nfrom sklearn.impute import SimpleImputer","8e4f5737":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf","c0de3183":"\n# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    value= 5 \n    datatypes()\n    top_rows(value)\n    bottom_rows(value)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(value)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            ","94ab2d33":"df.head()","c39a1941":"df.tail()","76f7fc92":"df.shape","ba3637d0":"df.size","1c98a064":"df.dtypes","7f3e0723":"df.columns","aa5074d2":"df.info()","0a46c2e1":"df.describe()","d6e24ffd":"df.duplicated().sum()","aff58b89":"df.skew()","6c063754":"df.corr()","bd21d2bb":"df.isnull().sum()","5bbe8499":"# Droping thesee columns since they have many null values\ndf.drop(['PoolQC','Fence','MiscFeature','FireplaceQu','Alley'],axis = 1,inplace = True)","96938823":"# Column with null values and their count\nc = 0\nis_null = []\nfor i in df.columns:\n    if df[i].isnull().sum()>0:\n        is_null.append(i)\n        print(i,df[i].isnull().sum())\n        c = c+1\nprint('Number of columns containing null values are:',c)","24a0fbaf":"# 38 categorical columns\nobj = []\nfor i in df.columns:\n    if df[i].dtypes == 'object':\n        obj.append(i)\nobj#categorical column list","a260d793":"# count of categories in categorical columns\nfor i in obj:\n    print(df[i].value_counts())\n    print('-----------------------------------------------------')","f40d7d8a":"# count plot of categorical values in each column\nfor i in obj:\n    sns.countplot(x = i ,data=df)\n    plt.show()","0c743e4d":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('Sale Price')\n    plt.title(i)\n    plt.show()","cce6db9c":"plt.figure(figsize=(6,8))\nx = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.histplot(x[i],kde = True)\n    plt.show()","1ba928b7":"plt.figure(figsize=(6,8))\nx = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.scatterplot(x = 'SalePrice',y = i,data = x,color = 'Red')\n    plt.show()","2e09c1e4":"plt.figure(figsize=(25,16))\nax = sns.heatmap(df.corr(),annot = True, cmap = 'viridis')\nplt.show()","d1b33699":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.boxplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","d010bbe5":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.violinplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","e5e25cf5":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","4c40d1aa":"for i in is_null:\n    if df[i].dtype == 'int64' or df[i].dtype == 'float64':\n        if i in a:\n                df[i].fillna(df[i].bfill(),inplace=True)\n        else:\n            df[i].fillna(df[i].bfill(),inplace=True)\n    elif df[i].dtype == 'object':\n        df[i].fillna(df[i].mode()[0],inplace=True)","f2bd0da2":"# No null values\ndf.info()","e08954a7":"le = LabelEncoder()\ndf[obj] = df[obj].apply(le.fit_transform)","19418289":"df1 = df.copy()","92865961":"df1","39aba6df":"df2 = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf2","6e23be9d":"#Droping columns with more null values\ndf2.drop(['PoolQC','Fence','MiscFeature','FireplaceQu','Alley'],axis = 1,inplace = True)\ndf2","70b3a9c6":"c1 = 0\nis_null1 = []\nfor i in df2.columns:\n    if df2[i].isnull().sum()>0:\n        is_null1.append(i)\n        print(i,df2[i].isnull().sum())\n        c1 = c1+1\nprint('Number of columns containing null values are:',c1)","d9b1b6b1":"# 38 categorical columns\nobj1 = []\nfor i in df2.columns:\n    if df2[i].dtypes == 'object':\n        obj1.append(i)\nlen(obj1)","08ac6b9c":"x1 = df2.drop(obj,axis = 1)\ndef count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            k.append(i)\n            print('Count of outliers are:',x+y)\nglobal k\nk = []\nfor i in x1.columns:\n    count_outliers(df2,i)","cec71fcf":"for i in is_null1:\n    if df2[i].dtype == 'int64' or df2[i].dtype == 'float64':\n        if i in k:\n                df2[i].fillna(df2[i].bfill(),inplace=True)\n        else:\n            df2[i].fillna(df2[i].bfill(),inplace=True)\n    elif df2[i].dtype == 'object':\n        df2[i].fillna(df2[i].mode()[0],inplace=True)","0add94b7":"df2.info()","a6e419cd":"le = LabelEncoder()\ndf2[obj1] = df2[obj1].apply(le.fit_transform)\ndf2","0d30e66d":"df3 = df2.copy()","04348ee5":"df3","fc8759f9":"v = []\nfor i in df1.columns:\n    if i not in df3.columns:\n        v.append(i)\nv","29593c54":"X = df1.drop(v,axis = 1)#dropping columns that doesnot present in test dataset\nY = df1['SalePrice']\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=44)","d71b6750":"reg = linear_model.LinearRegression()\nreg.fit(X_train, Y_train)","b31288b0":"reg.coef_","66634a6f":"pred = reg.predict(X_test)\npred","188fa4ea":"plt.scatter(Y_test,pred)\nplt.xlabel('Y Test (True Values)')\nplt.ylabel('Predicted values')\nplt.show()","a031ee5e":"#r2 score\nmetrics.explained_variance_score(Y_test,pred)","2225fee4":"# This shows data is normally distributed but right skewd\nsns.displot(Y_test-pred,bins = 50,kde = True)\nplt.show()","c6e95ea5":"cdf = pd.DataFrame(reg.coef_,X.columns,columns = ['coef'])\ncdf","c786e18a":"pred1 = reg.predict(df3)\npred1","1bdd81c5":"# List contains values of Id columns\nf = []\nfor i in df3.Id:\n    f.append(i)\n","e28519b4":"p = pd.DataFrame(data = [pred1])\nd = p.transpose()\nd.insert(loc = 0, column = 'Id', value=f, allow_duplicates = False)\nd.rename(columns={0: 'SalePrice'},inplace = True)\nd.set_index('Id')\n","c3c1f01b":"from sklearn.ensemble import RandomForestRegressor\nforest= RandomForestRegressor(n_estimators =40, random_state = 0)\nforest.fit(X_train,Y_train)  \nY_pred = forest.predict(X_test)\nforest.score(X_test,Y_test)","2b67aa5b":"pred2 = forest.predict(df3)\npred2","c90c5cbb":"p = pd.DataFrame(data = [pred2])\nd = p.transpose()\nd.insert(loc = 0, column = 'Id', value=f, allow_duplicates = False)","15e5ea77":"d.rename(columns={0: 'SalePrice'},inplace = True)\nd.set_index('Id')","8c4b8bf3":"import xgboost as xg\nxg_reg = xg.XGBRegressor(booster='gbtree', \n                    colsample_bylevel=1,\n                    colsample_bynode=1, \n                    colsample_bytree=0.6,\n                    gamma=0,\n                    importance_type='gain', \n                    learning_rate=0.01, \n                    max_delta_step=0,\n                    max_depth=4, \n                    min_child_weight=1.5, \n                    n_estimators=2400,\n                    n_jobs=1, \n                    nthread=None, \n                    objective='reg:squarederror',\n                    reg_alpha=0.6, \n                    reg_lambda=0.6, \n                    scale_pos_weight=1, \n                    silent=None, \n                    subsample=0.8, \n                    verbosity=1)\nxg_reg.fit(X_train, Y_train)\npred = xg_reg.predict(X_test)\nprint(xg_reg.score(X_test, Y_test))","751e85f6":"pred3 = xg_reg.predict(df3)","c5b638f2":"pred3","6a3647fe":"p = pd.DataFrame(data = [pred2])\nd = p.transpose()\nd.insert(loc = 0, column = 'Id', value=f, allow_duplicates = False)\nd.rename(columns={0: 'SalePrice'},inplace = True)\nd.set_index('Id')","4e4e2b75":"from sklearn.linear_model import Ridge,Lasso,ElasticNet\nR=Ridge(alpha=0.1)\nR.fit(X_train,Y_train)\ny_pred1=R.predict(X_test)","c2672740":"print(\"R2 score\",r2_score(Y_test,y_pred1))\nprint(\"RMSE\",np.sqrt(mean_squared_error(Y_test,y_pred1)))","520f721d":"model_lasso = Lasso(alpha=0.1)\nmodel_lasso.fit(X_train, Y_train) \npred_test_lasso= model_lasso.predict(X_test)\nprint(np.sqrt(mean_squared_error(Y_test,pred_test_lasso))) \nprint(r2_score(Y_test, pred_test_lasso))","d7e54444":"#Elastic Net\nmodel_enet = ElasticNet(alpha = 0.1)\nmodel_enet.fit(X_train, Y_train) \npred_test_enet= model_enet.predict(X_test)\nprint(np.sqrt(mean_squared_error(Y_test,pred_test_enet)))\nprint(r2_score(Y_test, pred_test_enet))","9f2bc343":"pred_elastic = model_enet.predict(df3)\npred_elastic","a6e597cb":"p = pd.DataFrame(data = [pred3])\nd = p.transpose()\nd.insert(loc = 0, column = 'Id', value=f, allow_duplicates = False)\nd.rename(columns={0: 'SalePrice'},inplace = True)\nd.set_index('Id')","319970c6":"## define configuration\nPATH_TRAIN = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\nPATH_TEST = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\nPATH_AUTOGLUON_SUBMISSION = 'submission_autogluon.csv'\nPATH_EVALML_SUBMISSION = 'submission_evalml.csv'\nPATH_FLAML_SUBMISSION = 'submission_flaml.csv'\nPATH_H2OAML_SUBMISSION = 'submission_h2oaml.csv'\nPATH_LAML_SUBMISSION = 'submission_laml.csv'\n\nMAX_MODEL_RUNTIME_MINS = 5\nMAX_MODEL_RUNTIME_SECS = MAX_MODEL_RUNTIME_MINS * 60","55fe6830":"## prepare data\nimport gc\nimport os\nimport shutil\nimport datatable as dt\nfrom pathlib import Path\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\ntrain = dt.fread(PATH_TRAIN)[:100000, :]\ntest = dt.fread(PATH_TEST)\n\ntarget = train['SalePrice'].to_numpy().ravel()\ntest_ids = test['Id']\n\ndel train[:, ['Id', 'SalePrice']]\ntest = test[:, train.names]","008b997e":"## install packages\n!python3 -m pip install -q \"mxnet<2.0.0\"\n!python3 -m pip install -q autogluon\n!python3 -m pip install -q -U graphviz\n!python3 -m pip install -q scikit-learn==0.24.2","0a416af8":"! pip install delayed","1ed0d41c":"## import packages\nfrom autogluon.tabular import TabularPredictor","25ec53dd":"## run model\ntrain['SalePrice'] = dt.Frame(target)\n\nmodel_autogluon = TabularPredictor(label='SalePrice')\nmodel_autogluon.fit(train_data=train.to_pandas(), excluded_model_types=['KNN'], time_limit=MAX_MODEL_RUNTIME_SECS)\n\ndel train['SalePrice']","a34ddc25":"## generate predictions\npreds_autogluon = model_autogluon.predict_proba(test.to_pandas())","3c382b66":"## create submission\nsubmission = dt.Frame(id=test_ids, target=dt.Frame(preds_autogluon))\nsubmission.head()","807848e1":"## clear memory\nshutil.rmtree('AutogluonModels')\ndel model_autogluon\n\ngc.collect()","af66f3ff":"## install packages\n!python3 -m pip install -q evalml==0.28.0","e20ff0de":"## import packages\nfrom evalml.automl import AutoMLSearch","f57a17c6":"## run model\nmodel_evalml = AutoMLSearch(X_train=train.to_pandas(), y_train=target, problem_type='regression', max_time=MAX_MODEL_RUNTIME_SECS)\nmodel_evalml.search()","b376d79a":"## check leaderboard\nmodel_evalml.rankings","853588d5":"## generate predictions\npreds_evalml = model_evalml.best_pipeline.predict(test.to_pandas())","22a7830e":"## create submission\nsubmission = dt.Frame(id=test_ids, target=dt.Frame(preds_evalml))\nsubmission.head()","60502f2a":"### Box plot to observe outliers","1b0c0a50":"### Target variable vs numerical columns","e36b1903":"### hist plot of all numerical columns","92969289":"# Feature Selection","559b1506":"### Filling null values with median if there are outliers in that particular column\n### Filling null values with mean if there are no outliers in that particular column\n### Filling null vaues with mode if that column is categorical column","ea3e718b":"### Encoding","6f4ed10c":"# Ridge Regression","8404c26d":"### violin plot","8b3b6cec":"# EDA using builtin function","57749815":"### This is a list which has column names that doesnot present in test dataframe","da12c4aa":"# Lasso Regression","fe2823f5":"# More Machine Learning Techniques","e78acf53":"# Cleaning and preprocessing of Test dataset","857b2e44":"### Target variable vs all categorical columns","9ee6a050":"# Random forest prediction of test dataset","0ec03fbf":"### This states that 1 unit increase in saleprice will decrease LotFrontage by 102.622826 units","49f44c1e":"# AutoGluon ML","45946107":"## Before applying model clean and setup the test dataset same as train for prediction","4987bce4":"# Loading Data","2bc3f3b4":"# Data Preprocessing","ef226367":"# Linear Regression prediction","c853a0c4":"## Here first we will load train data","95f1447a":"## Displaying random forest predictions of test dataset in dataframe","20ec199f":"# Count of outliers in each numerical columns","7d0c2f19":"# Elastic Regression","efeffcc5":"# Xg boost prediction on test data","ea6ab55d":"### Filling Null Values","f38f20e8":"# Xg boost prediction on train data","000be416":"### Heat map of corelation graph","8ca0b081":"## Displaying random forest predictions of test dataset in dataframe","f8a7f1a9":"# EVALML ","36cded53":"### Displaying regression predictions of test dataset in dataframe","a98f7536":"# Exploratory Data Analysis","6ff7b66c":"# Data Visualisation","82e77364":"# Importing Libraries","a75ff0cc":"# Regression prediction for test dataset","b2be814f":"# Random forest prediction of train dataset"}}