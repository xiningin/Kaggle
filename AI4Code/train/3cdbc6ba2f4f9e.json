{"cell_type":{"af2eaa7a":"code","bde61037":"code","8e6c0986":"code","7850b694":"code","ae74e526":"code","75d75ce9":"code","97cdb590":"code","5079ff92":"code","61fee3ad":"code","f444bd33":"code","31e9edfe":"code","32554a71":"code","6e49ede3":"code","8d9e6aa7":"code","ef77652a":"code","df5b3135":"code","b3953bad":"code","5608b21b":"code","523723c2":"code","9a518e90":"code","569754f6":"code","e0da6314":"code","e32c37e3":"code","95dacd42":"code","c70dfe67":"code","bbdb9ab2":"code","b1c1eaf6":"code","73f812dc":"code","0d46e448":"code","340d8663":"code","34e1a7ca":"code","85fbc938":"code","ddcf68a4":"code","2e424d0a":"code","b1131040":"code","d4e05c56":"code","4b5365a5":"code","bfd2d242":"code","a76cd97b":"code","0d9348c4":"code","bb8708a7":"code","e7c6feda":"code","076c4b54":"markdown","0676f2cf":"markdown","b2fa4c4b":"markdown","6fffbba5":"markdown","7a9cacf1":"markdown","c4c85de9":"markdown","d8344448":"markdown","6d83215f":"markdown","2406c043":"markdown","ce2b9222":"markdown","d49f0bb5":"markdown","c76aa08e":"markdown","58f93f35":"markdown","70142ba2":"markdown","b68d61ca":"markdown","0c4634c3":"markdown","661a3042":"markdown","7aef3279":"markdown","b4c4c5e5":"markdown","6f204666":"markdown","c22819e8":"markdown","e40e3c7e":"markdown"},"source":{"af2eaa7a":"import pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nimport matplotlib.pylab as plt\nimport calendar\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport datetime\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn.model_selection import GroupKFold\nfrom typing import Any\n#from numba import jit\nimport lightgbm as lgb\n#import xgboost as xgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn import metrics\nfrom itertools import product\nimport copy\nimport time\n\nimport random\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)","bde61037":"#load training data, training_labels, event-specifications, testdata and sample submisson into pandas dataframes\ntrain = pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv')\ntrain_labels = pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\ntest = pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv')\nspecs = pd.read_csv('..\/input\/data-science-bowl-2019\/specs.csv')\nsample_submission = pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","8e6c0986":"from sklearn.metrics import confusion_matrix\ndef qwk(act,pred,n=4,hist_range=(0,3)):\n    \n    O = confusion_matrix(act,pred)\n    O = np.divide(O,np.sum(O))\n    \n    W = np.zeros((n,n))\n    for i in range(n):\n        for j in range(n):\n            W[i][j] = ((i-j)**2)\/((n-1)**2)\n            \n    act_hist = np.histogram(act,bins=n,range=hist_range)[0]\n    prd_hist = np.histogram(pred,bins=n,range=hist_range)[0]\n    \n    E = np.outer(act_hist,prd_hist)\n    E = np.divide(E,np.sum(E))\n    \n    num = np.sum(np.multiply(W,O))\n    den = np.sum(np.multiply(W,E))\n        \n    return 1-np.divide(num,den)","7850b694":"train.head()","ae74e526":"train.shape","75d75ce9":"ids_all = train[\"installation_id\"].drop_duplicates()\nids_with_assessments = train[train.type == \"Assessment\"][\"installation_id\"].drop_duplicates()","97cdb590":"print(\"There are {} IDs\".format(len(ids_all)))\nprint(\"There are {} IDs with assessment(s)\".format(len(ids_with_assessments)))","5079ff92":"train_clean = pd.merge(train, ids_with_assessments, on=\"installation_id\", how=\"inner\")","61fee3ad":"train_clean.shape","f444bd33":"fig = plt.figure(figsize=(15,10))\nax1 = fig.add_subplot(311)\nax1 = sns.countplot(y=\"type\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by type\")\n\nax2 = fig.add_subplot(312)\nax2 = sns.countplot(y=\"world\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by world\")\n\nax3 = fig.add_subplot(313)\nax3 = sns.countplot(y=\"title\", data=train_clean[train_clean[\"type\"] == \"Assessment\"], color=\"blue\")\nplt.title(\"number of evetns by assessment\")\n\nplt.tight_layout(pad=2)\nplt.show()","31e9edfe":"def add_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['date'] = df['timestamp'].dt.date\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    return df\n    \ntrain_clean = add_time_features(train_clean)\ntest = add_time_features(test)","32554a71":"train_clean.head()","6e49ede3":"fig = plt.figure(figsize=(15,10))\nax1 = fig.add_subplot(311)\nax1 = sns.countplot(x=\"date\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by date\")\n\nax2 = fig.add_subplot(312)\nax2 = sns.countplot(x=\"dayofweek\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by day of week\")\n\nax3 = fig.add_subplot(313)\nax3 = sns.countplot(x=\"hour\", data=train_clean, color=\"blue\")\nplt.title(\"number of events by local daytime\")\n\nplt.tight_layout(pad=2)\nplt.show()","8d9e6aa7":"print(\"There are {} lines in sample submission\".format(sample_submission.shape[0]))","ef77652a":"print(\"There are {} unique installation_ids in testset\".format(test[\"installation_id\"].unique().shape[0]))","df5b3135":"train_labels.head()","b3953bad":"train_labels[\"title\"].unique()","5608b21b":"label_stats = train_labels.groupby(['title', 'accuracy_group'])['accuracy_group'].count().unstack(\"title\")\nlabel_stats.plot.bar(stacked=True, figsize=(10,10))","523723c2":"trainable_ids = train_labels[\"installation_id\"].drop_duplicates()\ntrainable_ids.shape","9a518e90":"train_clean = pd.merge(train_clean, trainable_ids, on=\"installation_id\", how=\"inner\")\ntrain_clean.shape","569754f6":"train_clean.groupby('installation_id').count()['event_id'].plot(kind='hist', bins=200, figsize=(15, 5),\n         title='Num Events by installation_id')\nplt.show()","e0da6314":"sessions_per_id = train_clean[[\"installation_id\", \"game_session\"]].drop_duplicates().groupby([\"installation_id\"])[\"game_session\"].count().plot(kind='hist', bins=200, figsize=(15, 5),\n         title='Num Sessions by installation_id')","e32c37e3":"train_clean","95dacd42":"train_clean.groupby('event_code').count()['event_id'].sort_values().plot(kind='bar', figsize=(15, 5),\n         title='event code count.')\nplt.show()","c70dfe67":"train_labels[train_labels[\"title\"].str.contains(\"Assessment\")]","bbdb9ab2":"print(\"Number of rows in train_labels: {}\".format(train_labels.shape[0]))\nprint(\"Number of unique game_sessions in train_labels: {}\".format(train_labels[\"game_session\"].nunique()))\nprint(\"Number of unique game_sessions that are Assessments in train_labels: {}\".format(train_labels[train_labels[\"title\"].str.contains(\"Assessment\")][\"game_session\"].nunique()))\nprint(\"Number of unique installation_ids in train_labels: {}\".format(train_labels[\"installation_id\"].nunique()))","b1c1eaf6":"# generate a list of all unique titles (=activities), enumerate them and replace occurances by numbers\nlist_of_user_activities = list(set(train_clean['title'].unique()).union(set(test['title'].unique())))\nactivities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n\ntrain_clean['title'] = train_clean['title'].map(activities_map)\ntest['title'] = test['title'].map(activities_map)\ntrain_labels['title'] = train_labels['title'].map(activities_map)","73f812dc":"# create a dictionary that lists winning codes for all titles (activities)\n# every activity has a win_code 4100, only bird-measurer has 4110\nattempt_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\nattempt_code[activities_map['Bird Measurer (Assessment)']] = 4110","0d46e448":"train_clean","340d8663":"# user_sample contains lines with a unique installation_id\n\ndef get_data(user_sample, test_set=False):\n    last_activity = 0\n    user_activities_count = {'Clip':0, 'Activity': 0, 'Assessment': 0, 'Game':0}\n    accuracy_groups = {0:0, 1:0, 2:0, 3:0}\n    all_assessments = []\n    accumulated_accuracy_group = 0\n    accumulated_accuracy=0\n    accumulated_correct_attempts = 0 \n    accumulated_incorrect_attempts = 0 \n    accumulated_events = 0\n    counter = 0\n    durations = []\n    \n    for i, session in user_sample.groupby('game_session', sort=False):\n        #session type and title is unique within each session\n        session_type = session['type'].iloc[0]\n        session_title = session['title'].iloc[0]\n        \n        #create feature vector for (session_type == Assessment) if (Testdata or (Trainingdata & len>1))\n        if (session_type == 'Assessment') & ((test_set == True) | ((test_set == False) & (len(session)>1))):\n            #add statistics of past events to feature vector and then update statistics\n            \n            #start feature vector with minimum content (Clip\/Activity\/Assessment\/Game)\n            features = user_activities_count.copy()\n            \n            # add session_title (as a number)\n            features['session_title'] = session['title'].iloc[0] \n            \n            # get all attempts in current game_session\n            all_attempts = session.query(f'event_code == {attempt_code[session_title]}')\n            # get all \"true\" attempts in current game_session\n            true_attempts = all_attempts['event_data'].str.contains('true').sum()\n            # get all \"false\" attempts in current game_session\n            false_attempts = all_attempts['event_data'].str.contains('false').sum()\n            \n            #compute accuracy of current game_session --> this is the parameter we need to predict later on\n            accuracy = true_attempts\/(true_attempts+false_attempts) if (true_attempts+false_attempts) != 0 else 0\n            if accuracy == 0:\n                features['accuracy_group'] = 0\n            elif accuracy == 1:\n                features['accuracy_group'] = 3\n            elif accuracy == 0.5:\n                features['accuracy_group'] = 2\n            else:\n                features['accuracy_group'] = 1\n            \n            #add accumulated correct attempts that occured before current session\n            features['accumulated_correct_attempts'] = accumulated_correct_attempts\n            #update accumulated attempt values\n            accumulated_correct_attempts += true_attempts \n            #same with incorrect attempts\n            features['accumulated_incorrect_attempts'] = accumulated_incorrect_attempts\n            #update accumulated attempt values\n            accumulated_incorrect_attempts += false_attempts\n\n            #add mean duration of previous sessions\n            if durations == []: #first session, durations is still empty\n                features['duration_mean'] = 0\n            else:\n                features['duration_mean'] = np.mean(durations)\n            #add current duration to list, therefore use the timestamp\n            durations.append((session.iloc[-1, 2] - session.iloc[0, 2] ).seconds) #last timestamp - first timestamp\n\n            \n            features['accumulated_accuracy'] = accumulated_accuracy\/counter if counter > 0 else 0\n            #update accumulated_accuracy with accuracy of current session\n            accumulated_accuracy += accuracy\n            \n            # add accuracy_groups to features\n            features.update(accuracy_groups)\n            # update accuracy groups with current game_session\n            accuracy_groups[features['accuracy_group']] += 1\n            \n            \n            features['accumulated_accuracy_group'] = accumulated_accuracy_group\/counter if counter > 0 else 0\n            accumulated_accuracy_group += features['accuracy_group']\n            \n            \n            features['accumulated_actions'] = accumulated_events\n            if test_set == True:\n                all_assessments.append(features)\n            else:\n                if true_attempts+false_attempts > 0:\n                    all_assessments.append(features)\n                \n            counter += 1\n\n        accumulated_events += len(session)\n        if last_activity != session_type: #cout number of different consecutive user activities\n            user_activities_count[session_type] += 1\n            last_activitiy = session_type\n\n    if test_set:\n        return all_assessments[-1] #for test data return last assessment \n    return all_assessments #for trainingdata return every assessment","34e1a7ca":"from tqdm import tqdm\n\ncompiled_data = []\n#for i, (ins_id, user_sample) in tqdm(enumerate(train_clean.groupby('installation_id', sort=False))):\nfor (ins_id, user_sample) in tqdm(train_clean.groupby('installation_id')):\n    compiled_data += get_data(user_sample)","85fbc938":"new_train = pd.DataFrame(compiled_data)\ndel compiled_data\nnew_train.shape","ddcf68a4":"new_train.head()","2e424d0a":"all_features = [x for x in new_train.columns if x not in ['accuracy_group']]\ncat_features = ['session_title']\nX, y = new_train[all_features], new_train['accuracy_group']\ndel train","b1131040":"X.shape","d4e05c56":"def make_classifier():\n    clf = CatBoostClassifier(\n                               loss_function='MultiClass',\n                               task_type=\"CPU\",\n                               learning_rate=0.01,\n                               iterations=2000,\n                               od_type=\"Iter\",\n                               early_stopping_rounds=500,\n                               random_seed=42\n                              )        \n    return clf","4b5365a5":"from time import time\nfrom sklearn.model_selection import KFold\n\noof = np.zeros(len(X))\nn_folds = 5\nclassifiers = []\n\nfolds = KFold(n_splits=n_folds, shuffle=True, random_state=42)\ntraining_start_time = time()\nfor fold, (train_idx, validate_idx) in enumerate(folds.split(X, y)):\n    start_time = time()\n    print(f'Training on fold {fold+1}')\n    clf = make_classifier()\n    clf.fit(X.loc[train_idx, all_features], y.loc[train_idx], \n            eval_set=(X.loc[validate_idx, all_features], y.loc[validate_idx]),\n            use_best_model=True, verbose=500, cat_features=cat_features)\n    classifiers.append(clf)\n    oof[validate_idx] = clf.predict(X.loc[validate_idx, all_features]).squeeze()\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n\nprint('-' * 30)\nprint('QWK Score on Training Set using K-Fold CV:', qwk(y, oof))\nprint('-' * 30)","bfd2d242":"# process test set\nnew_test = []\nfor ins_id, user_sample in tqdm(test.groupby('installation_id', sort=False)):\n    a = get_data(user_sample, test_set=True)\n    new_test.append(a)\n    \nX_test = pd.DataFrame(new_test)\n#del test","a76cd97b":"# make predictions on test set once\npreds_proba = np.zeros((X_test.shape[0], 4))\n\nfor classifier in classifiers:\n    preds_proba += classifier.predict_proba(X_test)\/len(classifiers)\npreds = np.argmax(preds_proba, axis=1)\n#del X_test","0d9348c4":"sample_submission['accuracy_group'] = np.round(preds).astype('int')\nsample_submission.to_csv('submission.csv', index=None)\nsample_submission.head()","bb8708a7":"sample_submission['accuracy_group'].plot(kind='hist')","e7c6feda":"train_labels['accuracy_group'].plot(kind='hist')","076c4b54":"# The Training Set ","0676f2cf":"Catboost Classifier\n\nhttps:\/\/catboost.ai\/docs\/concepts\/about.html","b2fa4c4b":"Applied metric is quadratic weightes kappa:\n\nhttps:\/\/www.kaggle.com\/aroraaman\/quadratic-kappa-metric-explained-in-5-simple-steps","6fffbba5":"# The Test-Set","7a9cacf1":"There are 8M remaining lines, only containing IDs that did at least 1 assessment","c4c85de9":"# Make submission","d8344448":"There are over 11M lines of training data.\n\nThere are provided certain gaming histories for a number of installation ids. We need to predict the number of attempts of a future assessment with the knowledge of the gaming history.\n\nFrom Kaggle we know that there are installation_id that have not completet any assessments","6d83215f":"Parts of the code are taken and modified from the following notebooks:\n\nhttps:\/\/www.kaggle.com\/erikbruin\/data-science-bowl-2019-eda-and-baseline\n\nhttps:\/\/www.kaggle.com\/robikscube\/2019-data-science-bowl-an-introduction\n\nhttps:\/\/www.kaggle.com\/mhviraf\/a-new-baseline-for-dsb-2019-catboost-model\n\n","2406c043":"What to do:\n\nThere are certain game_sessions that are of type \"assessment\" for which we know the correct label.\n\nWe need to collect all features for such assessments that help to predict the labels for the testset","ce2b9222":"There are 3614 assessment results, but there are 4242 installation_ids in the training set.\nThis means that there are 628 installation_ids that are not trainable because of missing training-labels.\n--> remove all events that belong to any of these untrainable installation_ids","d49f0bb5":"The data provided in these files are as follows:\n- `event_id` - unique identifier for a single event. Infos about event are found in specs table.\n- `game_session` - unique identifier grouping events within a single game or video play session.\n- `timestamp` - datetime (local time as it seems)\n- `event_data` - Semi-structured JSON formatted string containing the events parameters. Default fields are: event_count, event_code, and game_time; otherwise - fields are determined by the event type.\n- `installation_id` - Randomly generated unique identifier grouping game sessions within a single installed application instance.\n- `event_count` - Incremental counter of events within a game session (offset at 1). Extracted from event_data.\n- `event_code` - Identifier of the event 'class'. Unique per game, but may be duplicated across games. E.g. event code '2000' always identifies the 'Start Game' event for all games. Extracted from event_data.\n- `game_time` - Time in milliseconds since the start of the game session. Extracted from event_data.\n- `title` - Title of the game or video.\n- `type` - Media type of the game or video. Possible values are: 'Game', 'Assessment', 'Activity', 'Clip'.\n- `world` - The section of the application the game or video belongs to. Helpful to identify the educational curriculum goals of the media. Possible values are: 'NONE' (at the app's start screen), TREETOPCITY' (Length\/Height), 'MAGMAPEAK' (Capacity\/Displacement), 'CRYSTALCAVES' (Weight).","c76aa08e":"So far, so good, for every installation_id in the testset there is exactly one line in the sample_submission","58f93f35":"\"installation_id\" groups multiple \"game_session\"\n\"game_session\" groups multiple \"event_id\"\n\n\n\"event_data\" details an event\n\n\"event_count\", \"event_code\" and \"game_time\" are extracted from \"event_data\"\n\n\"game time\" is 0 for every start-event with code 2000\n\n\"game time\" reflects total time for games and assessments win_codes 4100 (4110 for bird measurer)\n\n\n\"type\" helps to concentrate on samples we need to train the algorithm on and that we need to predict (= assessments)\n\n\n\"world\" is redundant :\n\n--> \"Cart Balancer\" and \"Chest Sorter\" are in CRYSTALCAVES\n\n--> \"Cauldron Filler\" is in MAGMAPEAK\n\n--> \"Bird Measurer\" and \"Mushroom Sorter\" are in TREETOPCITY","70142ba2":"How many installation ids are in the training data set?\n\nHow many of them did never do any assessments?","b68d61ca":"# Some Feature Engineering","0c4634c3":"# The labels","661a3042":"## Basic Feature Engineering & Cleaning","7aef3279":"# Generate and train the model","b4c4c5e5":"There are 7,7M remaining events","6f204666":"There are 4 classes - accuracy_group\n\n3: solved on 1st attempt\n\n2: solved on 2nd attempt\n\n1: solved after 3 or more attempts\n\n0: never solved","c22819e8":"there are lot of events with code 4070, 4030, 3010, 3110, 4020\n\nare they relevant, or is their count relevant?","e40e3c7e":"\"timestamp\" is of string-type\n\nextract \"date\", \"month\", \"hour\", \"dayofweek\" to get an overview of app-activity\n\nmaybe helpful maybe not, at least we get an overview"}}