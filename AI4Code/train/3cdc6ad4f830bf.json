{"cell_type":{"73764113":"code","d12172d4":"code","c71eb474":"code","36c44ddd":"code","9e5930b5":"code","ac420872":"code","09a135d4":"code","3fdb3e03":"code","8d1da716":"code","a6ebcc14":"code","35fd1cba":"code","4ed9f402":"code","3b018c69":"code","698c7f59":"code","955c38ba":"code","31fd5880":"code","63fe4879":"code","9c863194":"code","90db8aa0":"code","31912194":"code","d215b988":"code","f534ff4c":"code","66d38966":"code","40059beb":"code","e6453765":"code","3a5d0bb6":"markdown","d877a369":"markdown","dff3c7f7":"markdown","fd74e790":"markdown","52666b5a":"markdown","6eddc700":"markdown","79115c25":"markdown","a78bd876":"markdown","928e5170":"markdown","b0077a81":"markdown","1b8ff923":"markdown","da4a8518":"markdown","7aa5d222":"markdown","d8deea73":"markdown","1bbde9fa":"markdown","12cc5331":"markdown","4635a8e9":"markdown","ffdac454":"markdown","07b21efc":"markdown","dede95fe":"markdown"},"source":{"73764113":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d12172d4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\n\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom scipy import ndimage","c71eb474":"path_train = '..\/input\/intel-image-classification\/seg_train\/seg_train'\npath_build = '\/buildings\/'\npath_forest = '\/forest\/'\npath_glacier = '\/glacier\/'\npath_mountain = '\/mountain\/'\npath_sea = '\/sea\/'\npath_street = '\/street\/'\npath_test = '..\/input\/intel-image-classification\/seg_test\/seg_test'","36c44ddd":"class_label = {0:'Building', 1:'Forest', 2:'Glacier', 3:'Mountain', 4:'Sea', 5:'Street'}","9e5930b5":"paths = [path_build, path_forest, path_glacier, path_mountain, path_sea, path_street]\npaths","ac420872":"data = []\nlabels = []\nIMG_SIZE = 150\n\naugmentation = 10\n\n\nfor i, path in enumerate(paths):\n    img_urls = listdir(path_train+path)\n    print(f\"Importing {class_label[i]}s\")\n    for img_name in tqdm(img_urls):\n        img = cv2.imread(path_train+path+img_name)\n        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        #for u in np.linspace(-5,5,num= augmentation):\n        #    im = ndimage.rotate(img, u)\n        #    im = cv2.resize(im,(IMG_SIZE,IMG_SIZE))\n        im = np.array(img)\n        data.append(im)\n        labels.append(i)\n        im = np.array(cv2.flip(img,1))\n        data.append(im)\n        labels.append(i)","09a135d4":"import random\n\nnumber = random.randint(0,len(data))\nplt.imshow(data[number])\nplt.show()\nprint(class_label[labels[number]])","3fdb3e03":"data = np.array(data)\ndata.shape","8d1da716":"labels = np.array(labels)\nlabels.shape","a6ebcc14":"ini = tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=1e-2, seed=None\n)\n\nmodel = Sequential()\nmodel.add(Conv2D(32,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                 input_shape=(IMG_SIZE,IMG_SIZE,3)\n                )\n         )\nmodel.add(Conv2D(32,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                )\n         )\nmodel.add(MaxPool2D((2,2)))\n\nmodel.add(Conv2D(64,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                )\n         )\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(64,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                )\n         )\nmodel.add(MaxPool2D((2,2)))\n\nmodel.add(Dense(64,activation='relu'))\n\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(6, activation= 'sigmoid'))\n\n\noptim = tf.keras.optimizers.Adam(\n    learning_rate=5e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-07\n)\n\nmodel.compile(\n    optimizer=optim, loss='sparse_categorical_crossentropy', metrics=['accuracy']\n)\n\nmodel.summary()","35fd1cba":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True, show_dtype=False,\n    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=50\n)","4ed9f402":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data,labels, test_size = 0.2, shuffle= True)","3b018c69":"x_train.shape, y_train.shape, x_test.shape, y_test.shape","698c7f59":"import random\nnumber = random.randint(0,len(x_train))\n\nplt.imshow(x_train[number])\nplt.show()\nprint(f'Class: {class_label[y_train[number]]}')","955c38ba":"early_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=5, restore_best_weights=True\n)\n\n\nepochs = 100\nbatch_size = 16\n\nhistory = model.fit(\n    x_train,y_train, batch_size=batch_size, epochs=epochs,\n    callbacks=[early_stop], validation_split=0.2, shuffle=True\n)","31fd5880":"history.history.keys()","63fe4879":"plt.figure(figsize= (10,5))\nplt.plot(range(len(history.history['loss'])),history.history['loss'], label= 'Loss')\nplt.plot(range(len(history.history['val_loss'])),history.history['val_loss'], label= 'Val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs Epoch')\nplt.legend()\nplt.grid()\nplt.show()","9c863194":"plt.figure(figsize= (10,5))\nplt.plot(range(len(history.history['accuracy'])),history.history['accuracy'], label= 'Accuracy')\nplt.plot(range(len(history.history['val_accuracy'])),history.history['val_accuracy'], label= 'Val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Acuracy vs Epoch')\nplt.legend()\nplt.grid()\nplt.show()\n\nmax_ac = np.max(history.history['val_accuracy'])\nprint(f'Max Accuracy was: {max_ac}')","90db8aa0":"model.evaluate(x_test,y_test)","31912194":"num_samples = 10\nfor i in range(num_samples):\n    test_num = random.randint(0,len(x_test))\n    pred = model.predict(np.array([x_test[test_num]]))\n    \n    plt.imshow(x_test[test_num])\n    plt.show()\n    print(f'Predccion: {class_label[np.argmax(pred)]} \\nRealidad: {class_label[y_test[test_num]]}')\n    print(f'Numero de test: {test_num}')","d215b988":"test_data = []\ntest_labels = []\nIMG_SIZE = 150\n\nfor i, path in enumerate(paths):\n    img_urls = listdir(path_test+path)\n    print(f\"Importing {class_label[i]}\")\n    for img_name in tqdm(img_urls):\n        img = cv2.imread(path_test+path+img_name)\n        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        im = np.array(img)\n        test_data.append(im)\n        test_labels.append(i)","f534ff4c":"test_data = np.array(test_data)\ntest_data.shape","66d38966":"test_labels = np.array(test_labels)\ntest_labels.shape","40059beb":"model.evaluate(test_data,test_labels)","e6453765":"num_samples = 10\nfor i in range(num_samples):\n    test_num = random.randint(0,len(test_data))\n    pred = model.predict(np.array([test_data[test_num]]))\n    \n    plt.imshow(test_data[test_num])\n    plt.show()\n    print(f'Predccion: {class_label[np.argmax(pred)]} \\nRealidad: {class_label[test_labels[test_num]]}')\n    print(f'Numero de test: {test_num}')","3a5d0bb6":"## Training","d877a369":"Lets reshape our data so hat it fits our convolutional neural network","dff3c7f7":"Here you can observe a diagram of our implemented CNN model","fd74e790":"Let's train, adding an early stopping so no overfitting occurs with a patience of 5 epochs. Notice that even tough we use a patience of 5 epochs, when the early stopping occurs, tensofloe will restore the best weights obtained.","52666b5a":"## Dataset","6eddc700":"Lets show a random image of the dataset and its label","79115c25":"Let's see some examples","a78bd876":"Lets see the accuracy over time","928e5170":"## Verifying Test Set\n\nLet's now verify our original test set that has never been loaded and our CNN model has never seen nor trained with. First, we need to load the images","b0077a81":"Lets see a final check of our dataset","1b8ff923":"## Results","da4a8518":"Let's shuffle and split our dataset on training and testing for better results and to verify that no overfitting is occurring. Keep in mind that this test set is not the final test set, that one is kept on a different folder for final checks and it hasn't even been loaded in memory yet.","7aa5d222":"Let's evaluate this test set","d8deea73":"### Loading the dataset\n\nIn this case we only make data augmentation by mirroring images from left to right","1bbde9fa":"## Imports","12cc5331":"### Evaluation\n\nLet's evaluate our hole test dataset to check how our CNN is performing","4635a8e9":"## Model\n\nLets define our model","ffdac454":"Let's see some examples","07b21efc":"Lets see the Loss and validation loss over epochs to verify no overfitting is ocurring","dede95fe":"# Image Classification With CNNs\n\nNotebook to show easily the implementation of Convolutional Neural Networks achieving a really good accuracy on categorizing images of 6 types"}}