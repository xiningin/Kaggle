{"cell_type":{"791439de":"code","49b88c98":"code","828c003e":"code","bc224138":"code","093cfea3":"code","4a2f976c":"code","c6030ce1":"code","687dd4e5":"code","19c29144":"code","ab417eb8":"code","8f06bc81":"code","5babda7a":"code","2655c8c0":"code","9f6baec8":"code","adf2d13e":"code","3fca2306":"code","8bb5eb9a":"code","9d739e5b":"code","76d62f47":"code","e2061661":"code","0cae9912":"code","5312075f":"code","3831b270":"code","1d81344c":"code","897d3a93":"code","562f80b6":"code","0e8e5197":"code","216f3549":"code","e3320da3":"code","1bddb436":"code","ca869eaa":"code","14c372b0":"code","9fd13985":"code","a7e6ecc9":"code","967fc64e":"code","19f28b90":"code","d8b6695e":"code","ad12ff58":"code","e47cd121":"code","cae3f54c":"code","6ff6dfa7":"code","0e8588bf":"markdown","eb032aeb":"markdown","adb95c6b":"markdown","26562b54":"markdown","ad650cdc":"markdown","5c7c1e43":"markdown","e106951f":"markdown","decf9c86":"markdown","0b88b9a5":"markdown"},"source":{"791439de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","49b88c98":"# we have our CSV file by the name of Reviews.csv, therefore we will use that file and database as database.sqlite\n# importing the important libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n","828c003e":"# connecting to the SQL database as the data size is quite large\ncon = sqlite3.connect('\/kaggle\/input\/amazon-fine-food-reviews\/database.sqlite')","bc224138":"# after making the connection fetch the data into the DataFrame\n# we will only take those reviews whose core is in [1, 2, 4, 5] as we are assuming that 3 is neutral\nfiltered_data = pd.read_sql_query('''select * from Reviews where score != 3''', con)","093cfea3":"# our dataset contains a large amount of data and this is after not taking score = 3 into consideration\nfiltered_data.shape","4a2f976c":"# lets have a look at our data\nfiltered_data.head()","c6030ce1":"# in textual data like this it is always advisable to look weather there are any duplicate rows present in the\n# data or not\n# checking for duplicates\n\nduplicated_data = pd.read_sql_query(\"\"\"\nSELECT UserId, ProductId, ProfileName, Time, Score, Text, COUNT(*)\nFROM Reviews\nGROUP BY UserId\nHAVING COUNT(*)>1\n\"\"\", con)","687dd4e5":"duplicated_data.head()","19c29144":"# lets see that how many times a review repeats itself\nmax(duplicated_data['COUNT(*)'])","ab417eb8":"# which review it is \n# we can see that there is a rivew where a review is duplicated 448 times\n# therefore we need to delete the reviews that are duplicated\nduplicated_data[duplicated_data['COUNT(*)'] == 448]","8f06bc81":"#Sorting data according to ProductId in ascending order\nsorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True,\\\n                                      inplace=False, kind='quicksort', na_position='last')","5babda7a":"#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal.shape","2655c8c0":"# after removing duplicate rows from our data now we are left with only 364173 rows in the dataset","9f6baec8":"# it is also seen that in some reviews HelpfullnessNumerator is greater the HelpfullnessDenominator\n# which is not correct as helpfullnessDenominator is the sum of HelpfullnessNumerator and HelpfullnessDenominator\nfinal=final[final['HelpfulnessNumerator'] <= final['HelpfulnessDenominator']]","adf2d13e":"# in our data there is a column which has the score of the review.\n# the score is like 1, 2, 3, 4, 5\n# from starting we have excluded the reviews with score = 3 as they are neutral and cannot be classified as positive or negative\n# reviews with score of 4, 5 are positive (1)\n# and the reviews with score 1, 2 are negative (0)\n# we will use lambda function to do that \n\nfinal['Score'] = final['Score'].apply(lambda x: 1 if x in [4, 5] else 0)","3fca2306":"# now we can see that score is changed in the new dataframe\nfinal.head()","8bb5eb9a":"# checking wheather this is no other value than 0, 1 in the score columns\n# sometime we have unwanted values in our columns such as \"?\" or \"#\"\n# therefore it is good practice to look the unique values present in that columns\nfinal['Score'].unique()","9d739e5b":"# counting the values of positve and negative reviews\nfinal['Score'].value_counts()","76d62f47":"final['Score'].value_counts().plot(kind = 'bar')","e2061661":"# printing some random reviews\nsent_0 = final['Text'].values[0]\nprint(sent_0)\nprint(\"=\"*50)\n\nsent_1000 = final['Text'].values[1000]\nprint(sent_1000)\nprint(\"=\"*50)\n\nsent_1500 = final['Text'].values[1500]\nprint(sent_1500)\nprint(\"=\"*50)\n\nsent_4900 = final['Text'].values[4900]\nprint(sent_4900)\nprint(\"=\"*50)","0cae9912":"# remove urls from text python: https:\/\/stackoverflow.com\/a\/40823105\/4084039\nsent_0 = re.sub(r\"http\\S+\", \"\", sent_0)\nsent_1000 = re.sub(r\"http\\S+\", \"\", sent_1000)\nsent_150 = re.sub(r\"http\\S+\", \"\", sent_1500)\nsent_4900 = re.sub(r\"http\\S+\", \"\", sent_4900)\n\nprint(sent_4900)","5312075f":"# get_text function of beautiful soup will help to retrieve only text part from reviews and will remove \n# tags like <br>, <\/br> etc\nfrom bs4 import BeautifulSoup\n\nsoup = BeautifulSoup(sent_0, 'lxml')\ntext = soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup = BeautifulSoup(sent_1000, 'lxml')\ntext = soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup = BeautifulSoup(sent_1500, 'lxml')\ntext = soup.get_text()\nprint(text)\nprint(\"=\"*50)\n\nsoup = BeautifulSoup(sent_4900, 'lxml')\ntext = soup.get_text()\nprint(text)","3831b270":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","1d81344c":"sent_1500 = decontracted(sent_1500)\nprint(sent_1500)\nprint(\"=\"*50)","897d3a93":"#remove words with numbers python: https:\/\/stackoverflow.com\/a\/18082370\/4084039\nsent_0 = re.sub(\"\\S*\\d\\S*\", \"\", sent_0).strip()\nprint(sent_0)","562f80b6":"#remove spacial character: https:\/\/stackoverflow.com\/a\/5843547\/4084039\nsent_1500 = re.sub('[^A-Za-z0-9]+', ' ', sent_1500)\nprint(sent_1500)","0e8e5197":"\n# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\n# <br \/><br \/> ==> after the above steps, we are getting \"br br\"\n# we are including them into stop words list\n# instead of <br \/> if we have <br\/> these tags would have revmoved in the 1st step\n\nstopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","216f3549":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_reviews = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(final['Text'].values):\n    sentance = re.sub(r\"http\\S+\", \"\", sentance) # is used to remove the html links from the reviews\n    sentance = BeautifulSoup(sentance, 'lxml').get_text() # is used to fetch only the text part and remove the tags like <br> etc.\n    sentance = decontracted(sentance) # see cell no. 31\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip() # for removing words with numbers\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance) # fro removing every thing that is not in A-Z and a-z and numbers\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords) # remving the stopwords presents and lower the word and join the words into sentence\n    preprocessed_reviews.append(sentance.strip()) # appending the sentences into the list named 'preprocessed_reviews'","e3320da3":"# check for a random review\npreprocessed_reviews[1231]","1bddb436":"#BoW\ncount_vect = CountVectorizer() #in scikit-learn\ncount_vect.fit(preprocessed_reviews)\nprint(\"some feature names \", count_vect.get_feature_names()[:10])\nprint('='*50)\n\nfinal_counts = count_vect.transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_counts))\nprint(\"the shape of out text BOW vectorizer \",final_counts.get_shape())\nprint(\"the number of unique words \", final_counts.get_shape()[1])","ca869eaa":"# we can see that now our no. of columns have been changed to 116756, which are ultimately the unique words in our review","14c372b0":"#bi-gram, tri-gram and n-gram\n\n#removing stop words like \"not\" should be avoided before building n-grams\n# count_vect = CountVectorizer(ngram_range=(1,2))\n# please do read the CountVectorizer documentation http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n\n# you can choose these numebrs min_df=10, max_features=5000, of your choice\ncount_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\nfinal_bigram_counts = count_vect.fit_transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_bigram_counts))\nprint(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])","9fd13985":"# in the above code we have set the max_features = 5000 that s why the unique features are now only 5000\n# if max_features is not set to 5000 then the new features may exceed to 29M because this is the drawback of N-Gram","a7e6ecc9":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\ntf_idf_vect.fit(preprocessed_reviews)\nprint(\"some sample features(unique words in the corpus)\",tf_idf_vect.get_feature_names()[0:10])\nprint('='*50)\n\nfinal_tf_idf = tf_idf_vect.transform(preprocessed_reviews)\nprint(\"the type of count vectorizer \",type(final_tf_idf))\nprint(\"the shape of out text TFIDF vectorizer \",final_tf_idf.get_shape())\nprint(\"the number of unique words including both unigrams and bigrams \", final_tf_idf.get_shape()[1])","967fc64e":"# Train your own Word2Vec model using your own text corpus\n# this will convert every single review into list of words so that individual words can be converted into vectors\ni=0\nlist_of_sentance=[]\nfor sentance in preprocessed_reviews:\n    list_of_sentance.append(sentance.split())","19f28b90":"list_of_sentance[10]","d8b6695e":"# Using Google News Word2Vectors\n\n# in this project we are using a pretrained model by google\n# its 3.3G file, once you load this into your memory \n# it occupies ~9Gb, so please do this step only if you have >12G of ram\n# we will provide a pickle file wich contains a dict , \n# and it contains all our courpus words as keys and  model[word] as values\n# To use this code-snippet, download \"GoogleNews-vectors-negative300.bin\" \n# from https:\/\/drive.google.com\/file\/d\/0B7XkCwpI5KDYNlNUTTlSS21pQmM\/edit\n# it's 1.9GB in size.\n\n\n# http:\/\/kavita-ganesan.com\/gensim-word2vec-tutorial-starter-code\/#.W17SRFAzZPY\n# you can comment this whole cell\n# or change these varible according to your need\n\nis_your_ram_gt_16g=False\nwant_to_use_google_w2v = False\nwant_to_train_w2v = True\n\nif want_to_train_w2v:\n    # min_count = 3 (considers only words that occured atleast 3 times )\n    w2v_model=Word2Vec(list_of_sentance,min_count=3,size=50, workers=4) # workers = 4 will allow your 4 cores of machine to put all the computation here\n    print(w2v_model.wv.most_similar('great'))\n    print('='*50)\n    print(w2v_model.wv.most_similar('worst'))\n    \nelif want_to_use_google_w2v and is_your_ram_gt_16g:\n    if os.path.isfile('GoogleNews-vectors-negative300.bin'):\n        w2v_model=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n        print(w2v_model.wv.most_similar('great'))\n        print(w2v_model.wv.most_similar('worst'))\n    else:\n        print(\"you don't have gogole's word2vec file, keep want_to_train_w2v = True, to train your own w2v \")","ad12ff58":"(preprocessed_reviews)","e47cd121":"# Please write all the code with proper documentation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection  import cross_val_score\nfrom collections import Counter\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn.metrics import roc_auc_score\nX=preprocessed_reviews\ny=np.array(final['Score'])\ntf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\nX_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nX_tr, X_cv, y_tr, y_cv = train_test_split(X_1, y_1, test_size=0.3)\nfinal_Xtr=tf_idf_vect.fit_transform(X_tr)\nfinal_Xcv=tf_idf_vect.transform(X_cv)\nfinal_Xtest=tf_idf_vect.transform(X_test)\nauc_cv=[]\nauc_train=[]\nK=[]\nfor i in range(1,50,4):\n    knn=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute',leaf_size=30, p=2, metric='cosine')\n    knn.fit(final_Xtr, y_tr)\n    pred = knn.predict_proba(final_Xcv)[:,1]\n    pred1=knn.predict_proba(final_Xtr)[:,1]\n    auc_cv.append(roc_auc_score(y_cv,pred))\n    auc_train.append(roc_auc_score(y_tr,pred1))\n    K.append(i)\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(K, auc_train, label='AUC train')\nax.plot(K, auc_cv, label='AUC CV')\nplt.title('AUC vs K')\nplt.xlabel('K')\nplt.ylabel('AUC')\nax.legend()\nplt.show()","cae3f54c":"#ROC curve for k=49\n#from above statistics we take k=49 as our best hyperparameter\nfrom sklearn.metrics import confusion_matrix\nknn=KNeighborsClassifier(n_neighbors=49,weights='uniform',algorithm='brute',leaf_size=30, p=2, metric='cosine')\nknn.fit(final_Xtr,y_tr)\npredi=knn.predict_proba(final_Xtest)[:,1]\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, predi)\npred=knn.predict_proba(final_Xtr)[:,1]\nfpr2,tpr2,thresholds2=metrics.roc_curve(y_tr,pred)\nfig = plt.figure()\nax = plt.subplot(111)\nax.plot(fpr1, tpr1, label='Test ROC ,auc='+str(roc_auc_score(y_test,predi)))\nax.plot(fpr2, tpr2, label='Train ROC ,auc='+str(roc_auc_score(y_tr,pred)))\nplt.title('ROC')\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nax.legend()\nplt.show()","6ff6dfa7":"#Confusion matrix \nfrom sklearn.metrics import confusion_matrix\nknn=KNeighborsClassifier(n_neighbors=49,weights='uniform',algorithm='brute',leaf_size=30, p=2, metric='cosine')\nknn.fit(final_Xtr,y_tr)\npredic=knn.predict(final_Xtest)\nimport seaborn as sns\nconf_mat = confusion_matrix(y_test, predic)\nclass_label = [\"negative\", \"positive\"]\ndf = pd.DataFrame(conf_mat, index = class_label, columns = class_label)\nsns.heatmap(df, annot = True,fmt=\"d\")\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","0e8588bf":"## **Bag of Words**","eb032aeb":"## Preprocessing Review Text","adb95c6b":"## Applying KNN on BoW","26562b54":"## Bi-Grams and N-Grams","ad650cdc":"# Featurization","5c7c1e43":"## [1]. Exploratory Data Analysis","e106951f":"# Preprocessing","decf9c86":"## TF-IDF","0b88b9a5":"* Begin by removing the html tags\n* Remove any punctuations or limited set of special characters like , or . or # etc.\n* Check if the word is made up of english letters and is not alpha-numeric\n* Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n* Convert the word to lowercase\n* Remove Stopwords\n* Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n* After which we collect the words used to describe positive and negative reviews"}}