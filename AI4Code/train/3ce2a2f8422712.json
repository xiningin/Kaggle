{"cell_type":{"56fdb722":"code","463026ad":"code","e090e525":"code","7b8ee187":"code","ba454dac":"code","89135206":"code","b69a9ba4":"code","05e152e9":"code","abf743a3":"code","8f14196d":"code","755536f9":"code","63497700":"code","afb71ae4":"code","8e6b6148":"code","ae0680ff":"code","43119bc1":"code","2f6bf728":"code","aa527784":"code","a48ad27f":"code","cb584bd3":"code","6ff73c46":"code","b95e0019":"code","efe7da3f":"code","47695779":"code","b7b682b0":"code","f06ec4db":"code","14dd3f72":"code","d82b3a45":"code","88095325":"code","2cca70f7":"code","407c4dff":"code","1bb279c2":"code","1c895177":"code","282ace7f":"code","69864300":"code","30a001f1":"code","f16b914b":"code","70dbe8c2":"code","fe21a4aa":"code","2d4b90fa":"code","32596862":"code","10231214":"code","dc84682f":"code","6c36c489":"code","7cbba4d6":"markdown","47865535":"markdown","e8302faa":"markdown","aea2cc6c":"markdown","88a3372c":"markdown","2dad55e1":"markdown","ba6647bd":"markdown","1ccd1ef7":"markdown","0e55d84c":"markdown","5cbc4401":"markdown","09958817":"markdown","62d7e843":"markdown","03259233":"markdown","6ef99459":"markdown","714a9986":"markdown","1829762c":"markdown","269a1f45":"markdown","d07305ba":"markdown","1e3b7176":"markdown","205939c9":"markdown","b2ce52c4":"markdown","6fb22ddf":"markdown","541f444e":"markdown","9a852b2b":"markdown","d9c8a365":"markdown","d8ea2db4":"markdown","3837c601":"markdown","ca622ccf":"markdown","229cbaea":"markdown","b8a8c43c":"markdown","645e969a":"markdown"},"source":{"56fdb722":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","463026ad":"!pip install langdetect","e090e525":"!pip install pandas-profiling ","7b8ee187":"!pip install langdetect","ba454dac":"# Import Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, ENGLISH_STOP_WORDS\nimport math\nimport seaborn as sns\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom langdetect import detect_langs","89135206":"df_train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ndf_train.head()","b69a9ba4":"from pandas_profiling import ProfileReport\nProfileReport(df_train)","05e152e9":"df_train.info()","abf743a3":"df_train.describe()","8f14196d":"# create a copy of the train df to work with, so we will have the original data\ndf_tmp = df_train.copy()","755536f9":"# let's see the whole excerpt\npd.set_option('display.max_colwidth', None)\n# to take it back: pd.reset_option(\"display.max_rows\")","63497700":"# the excerpt with the greatest target value (easiest to read)\ndf_tmp[df_tmp.target == df_tmp.target.max()].excerpt","afb71ae4":"# the excerpt with the lowest target value (hardest to read)\ndf_tmp[df_tmp.target == df_tmp.target.min()].excerpt","8e6b6148":"pd.reset_option(\"display.max_rows\")","ae0680ff":"from nltk import word_tokenize\n\nword_token = [word_tokenize(excerpt) for excerpt in df_tmp.excerpt]\nlen_tokens= [] \n\nfor i in range(len(word_token)):\n    len_tokens.append(len(word_token[i]))\n\ndf_tmp[\"n_tokens\"] = len_tokens\ndf_tmp.head(2)","43119bc1":"df_tmp.corr()","2f6bf728":"# remove punctuation and lowercase everything\ndf_tmp['excerpt'] = df_tmp['excerpt'].str.replace('[^\\w\\s]','')\ndf_tmp['excerpt'] = df_tmp['excerpt'].str.lower()\ndf_tmp.head(1)","aa527784":"# sort the values based on target \ndf_tmp.sort_values(\"target\", ascending=True, inplace = True)\ndf_tmp = df_tmp.reset_index(drop=True)\n\n# and divide it into three different groups (basically creating three different difficulty groups)\ndf_tmp[\"range\"] = \"easy\"\neach_group = int(len(df_tmp) \/ 3)\n\nfor i in range(0, each_group):\n    df_tmp[\"range\"].iloc[i] = \"hard\"\n    \nfor i in range(each_group, 2 * each_group):\n    df_tmp[\"range\"].iloc[i] = \"medium\"","a48ad27f":"df_tmp.range.value_counts()","cb584bd3":"# create dataframes\nhard = df_tmp[0:each_group]\nmedium = df_tmp[each_group: 2 * each_group]\neasy = df_tmp[2 * each_group: 3 * each_group]","6ff73c46":"# create texts\ntext_hard = \" \".join(excerpt for excerpt in hard.excerpt)\ntext_medium = \" \".join(excerpt for excerpt in medium.excerpt)\ntext_easy = \" \".join(excerpt for excerpt in easy.excerpt)","b95e0019":"# set stopwords\n\nstopwords = set(STOPWORDS)\n#stopwords.update([])\n\ncloud_1 = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text_hard)\ncloud_2 = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text_medium)\ncloud_3 = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text_easy)\n\n# plot\n\nwidth= 3\nheight= 3\nrows = 1\ncols = 3\naxes=[]\nfig=plt.figure(figsize=(10, 10))\n\n\nfor a in range(rows*cols):\n    cloud = [cloud_1, cloud_2, cloud_3]\n    axes.append(fig.add_subplot(rows, cols, a+1) )\n    subplot_title=(\"Word Cloud \"+ str(a + 1))\n    axes[-1].set_title(subplot_title)  \n    plt.imshow(cloud[a])\nfig.tight_layout()    \nplt.show()","efe7da3f":"sns.catplot(x=\"target\", y=\"range\", data=df_tmp, kind=\"bar\");","47695779":"from textblob import TextBlob ","b7b682b0":"sentiment = []\n\nfor i in df_tmp.excerpt:\n    text = TextBlob(i)\n    sentiment.append(text.sentiment)","f06ec4db":"sentiment[0]","14dd3f72":"sent = [float(str(i).split(\",\")[0].split(\"=\")[1]) * 100 for i in sentiment]\ndf_tmp[\"sentiment\"] = sent","d82b3a45":"df_tmp[\"sentiment\"].hist();","88095325":"sns.set_palette(\"RdBu\")\nsns.relplot(x=\"target\", y=\"sentiment\", kind=\"scatter\", hue=\"standard_error\", data=df_tmp, ci=None, height=8.27, aspect=11.7\/8.27);","2cca70f7":"excerpt_length = df_tmp[\"excerpt\"].apply(lambda x: len(x.split(\" \")))\ndf_tmp[\"excerpt_len\"] = excerpt_length\ndf_tmp[\"excerpt_len\"].max(), df_tmp[\"excerpt_len\"].min()","407c4dff":"df_tmp.corr()","1bb279c2":"df_tmp.excerpt_len.hist(bins=25);","1c895177":"# from https:\/\/www.kaggle.com\/jitshil143\/submission-score-0-62\ndef syllable_count(word):\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n            if word.endswith(\"e\"):\n                count -= 1\n    if count == 0:\n        count += 1\n    return count","282ace7f":"# nof syllables\ndf_tmp['nof_syllables'] =  df_tmp['excerpt'].apply(lambda s: syllable_count(s))","69864300":"sns.relplot(x=\"nof_syllables\", y=\"target\", hue=\"standard_error\", data=df_tmp, kind=\"scatter\");","30a001f1":"df_tmp.corr()","f16b914b":"# tokenize each word in an excerpt\n\ntokenized_sent = []\nfor s in df_tmp.excerpt:\n    tokenized_sent.append(word_tokenize(s.lower()))\ntokenized_sent[0]","70dbe8c2":"# define the cosine similarity between two words\ndef cosine(u, v):\n    return np.dot(u, v) \/ (np.linalg.norm(u) * np.linalg.norm(v))","fe21a4aa":"! pip install sentence-transformers","2d4b90fa":"from sentence_transformers import SentenceTransformer\n\n#bert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n#excerpt_embeddings = bert_model.encode(df_tmp.excerpt)\n\n# compute cosine similarity: takes very long to run this part since it computes each line, so instead I will examine this on one line.\n\n# test query\nquery = \"I had pizza and pasta\"\nquery_vec = bert_model.encode([query])[0]\n\n\n\n#for excerpt in df_tmp[\"excerpt\"]:\n#   similarity = cosine(paragraph_embeddings, bert_model.encode([excerpt])[0])","32596862":"#  look at only one example\n\nfor excerpt in df_tmp.loc[0:1][\"excerpt\"].values:\n    similarity = cosine(query_vec, bert_model.encode([excerpt])[0])\n    print(excerpt, np.sum(similarity))","10231214":"# remove accented characters\nimport unicodedata\n\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","dc84682f":"for i in range(0, len(df_tmp.excerpt)):\n    df_tmp.excerpt.loc[i] = remove_accented_chars(df_tmp.excerpt.loc[i])","6c36c489":"import re\n\ndef remove_special_characters(text, remove_digits=True):\n    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n    text = re.sub(pattern, '', text)\n    return text\n\nfor i in range(0, len(df_tmp.excerpt)):\n    df_tmp.excerpt.loc[i] = remove_special_characters(df_tmp.excerpt.loc[i])","7cbba4d6":"# 3. Features","47865535":"There are missing values + categorical values (it is a text data after all \ud83e\uddda).\n\n<img width=\"726\" alt=\"Screen Shot 2021-05-15 at 7 55 16 PM\" src=\"https:\/\/user-images.githubusercontent.com\/66208179\/118381313-666c0380-b5f2-11eb-88ae-5d4cb3f3c5ff.png\">","e8302faa":"# 1. Understanding the Data","aea2cc6c":"Based on these results, we see that there is **no correlation** between target and standart error.\n\n<img width=\"520\" alt=\"Screen Shot 2021-05-15 at 7 55 02 PM\" src=\"https:\/\/user-images.githubusercontent.com\/66208179\/118381301-4fc5ac80-b5f2-11eb-8cfa-8410d63bcda9.png\">","88a3372c":"No correlation between ```parag_len``` and any attribute.","2dad55e1":"# 5. Word Embeddings","ba6647bd":"## Paragraph Length","1ccd1ef7":"# 4. Sentiment Analysis","0e55d84c":"## Syllable Count","5cbc4401":"\u2b50\ufe0f**Please comment and let me know if you think there can be further EDA on the relation of emotions and target.**\u2b50\ufe0f","09958817":"At this point, we are aware that there might be some bias from people who rate the excerpts since it is all subjective. I will implement a sentiment analysis to understand if that bias might be based on emotions.","62d7e843":"Let's see if the length of excerpts has any correlation with difficulty.","03259233":"# 2. What is the Standard Error?\n\n\u2705 The ```mean``` of target difficulty is -0.9.\n\n\u2705 ```Min``` target difficulty is -3.676268.\n\n\u2705 ```Max``` target difficulty is 1.711390.\n\nI wasn't sure what standard error can show us exactly, so I used to following resource to understand: https:\/\/s4be.cochrane.org\/blog\/2018\/09\/26\/a-beginners-guide-to-standard-deviation-and-standard-error\/\n\n\n\u2b50\ufe0f The ```standard error``` tells you *how accurate the mean of any given sample from that population is likely to be compared to the true population mean*. When the standard error increases, i.e. the means are more spread out, it becomes more likely that any given mean is an inaccurate representation of the true population mean.\n\nOur target has a **normal distribution** and the majority of the standard error is around 0.5.\n\nOur standard error shows that people's classifications (their ratings for the excerpts) had differences since our standard error is ```skewed left```, meaning we have a larger tail on the left and most of our error is clustered around values > 0.5.","6ef99459":"# Word Cloud","714a9986":"One of the things I wasn't aware of before (yes, I still make novice mistakes and it is okay, we are all here to learn \u2b50\ufe0f) is that I thought saying ```df_tmp = df_train``` is fine. But NO! It creates a *shallow copy*, so any change you make in ```df_train``` will be reflected on ```df_tmp``` in the future. However, ```.copy()``` creates a *deep copy* and that's what we want!","1829762c":"There are many exciting word embedding styles in this article, but I've always wanted to learn BERT so I will go ahead and try it.\n\n## BERT\n\nThis is my first time learning\/ using BERT. I've used different notebooks from Kaggle to understand how BERT works and how it can be applied to our data in this competition.\n\n<img width=\"700\" alt=\"Screen Shot 2021-05-18 at 10 30 18 PM\" src=\"https:\/\/user-images.githubusercontent.com\/66208179\/118747873-80604d00-b863-11eb-8499-7ef4bd470a00.png\">","269a1f45":"Seems like all the text includes one, little, two, see, people etc. ","d07305ba":"There is no correlation between ```n_tokens``` and other numerical fields.","1e3b7176":"We will first try to understand the training set. At this point, to avoid data snooping, just load the test dataframe and never look at it \ud83e\udd13","205939c9":"For word embeddings, I've used [this article](https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/top-4-sentence-embedding-techniques-using-python\/).","b2ce52c4":"I will separate excerpts based on three difficulties and show the most common words in the wordcloud to understand if there are any patterns.","6fb22ddf":"Let's dive deeper into the relation between target and standard error. So far, we found that:\n- there is not correlation between target and standard error\n- our target has a normal distribution \n<img width=\"300\" alt=\"Screen Shot 2021-05-17 at 8 00 29 AM\" src=\"https:\/\/user-images.githubusercontent.com\/66208179\/118485190-c82a8b80-b720-11eb-868b-6636cff0d4f3.png\">\n- the standard error is skewed left\n<img width=\"300\" alt=\"Screen Shot 2021-05-17 at 8 00 15 AM\" src=\"https:\/\/user-images.githubusercontent.com\/66208179\/118485198-c95bb880-b720-11eb-9398-8bed525880e2.png\">\n\n\n","541f444e":"# To Be Continued :)","9a852b2b":"I \u2764\ufe0f pandas profiling! It is a very easy library to use (hence, there is just one line over there!) and it returns a detailed analysis of your current data.","d9c8a365":"### Remove Special Characters","d8ea2db4":"### Remove Accented Characters","3837c601":"# CommonLit Readability Prize \ud83d\udcda\n\n![Green Blue Paint Beauty Skincare Facebook Cover](https:\/\/user-images.githubusercontent.com\/66208179\/118814977-69e4e080-b8b9-11eb-9d35-745b530d2390.png)\n\n# 1. Introduction\n\nIdentify the appropriate reading level of a text based on one feature: text \ud83d\udc53\n\nData includes readers from a wide variety of age groups and a large collection of texts taken from various domains. \n\n\ud83c\udf31```id:``` unique ID for excerpt\n\n\ud83c\udf31```url_legal:``` URL of source - this is blank in the test set.\n\n\ud83c\udf31```license:``` license of source material - this is blank in the test set.\n\n\ud83c\udf31```excerpt:``` text to predict reading ease of\n\n\ud83c\udf31```target:``` reading ease\n\n\ud83c\udf31```standard_error:``` measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n## Evaluation:\n\nThe models will be evaluated by **RMSE (Root Mean Square Error)**. RMSE measures the standard deviation of residuals.\n\n**The lower the value of RMSE, higher the accuracy**. RMSE is a great model to compare the accuracies of different linear regression models.\n\n<img width=\"233\" alt=\"Screen Shot 2021-05-15 at 10 10 43 AM\" src=\"https:\/\/user-images.githubusercontent.com\/66208179\/118364269-7c50d880-b5a0-11eb-864c-036b5ad9c65c.png\">\n\n```Winning models will be sure to incorporate text cohesion and semantics.```","ca622ccf":"**Removing StopWords**: I was about the remove the stopwords as well, until I saw [this post.](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240064)","229cbaea":"# Modeling","b8a8c43c":"In conclusion, this is a prediction problem. We need to predict the target level from a given text.\n\nWe have text data, so it is important to preprocess our data since lots of in-built functions must work with same data type.\n\nWe've already lower-cased each excerpt and removed punctuations to produce word-clouds.\n\nSome of my code is from here and it is a great notebook for text preprocessing: https:\/\/www.kaggle.com\/manishkc06\/text-pre-processing-data-wrangling","645e969a":"As we can see, there are some missing values!"}}