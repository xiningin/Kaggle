{"cell_type":{"4a5940bf":"code","ab2c417e":"code","79ebcbf8":"code","e3cccfd7":"code","4290bf20":"code","8f28ccc8":"code","88ba58d4":"code","0e91b911":"code","d7911a47":"code","2f7ca053":"code","787ea924":"code","1f69912d":"code","d84303be":"code","ca2240be":"code","40829cdd":"code","12ff383c":"code","d36afe33":"code","52f24579":"code","4f9cb405":"code","c60de84a":"code","7796b49b":"code","d91cbf22":"code","8fe1c8d7":"code","286e4b17":"code","c7f25b09":"code","49913ad7":"code","5092de51":"code","a5ff509f":"code","2e3447c8":"code","2d92f608":"code","f4e112db":"code","66449c20":"code","fe9cd96c":"code","8970905e":"code","03d6d973":"code","898f95e9":"code","dcef8b15":"code","60d103cd":"code","26f84799":"code","3ee38ffa":"code","7456a402":"code","2ab72e73":"code","620c9fb5":"markdown","12c0dbd8":"markdown","4fa67c34":"markdown","1010cc5f":"markdown","87914dff":"markdown","0fd3ef08":"markdown","a445e07d":"markdown","b8dc9198":"markdown","319069fb":"markdown","361fa2f3":"markdown","69def489":"markdown","3b4444be":"markdown","81439f3b":"markdown","417b1b85":"markdown","581577d3":"markdown","d4087cab":"markdown","a20d6171":"markdown","5b95e4b0":"markdown","a7c67ea5":"markdown","de6af220":"markdown","f9c0835a":"markdown","855adefd":"markdown","f7d84498":"markdown","64ca7a37":"markdown","0d04631c":"markdown","6861a1dc":"markdown","24f12f9a":"markdown","f5c2f86d":"markdown","bba3bcfd":"markdown","074b34a9":"markdown","8df6482f":"markdown","e798c3d5":"markdown","63452912":"markdown","6992a532":"markdown","f5336ef5":"markdown","f96ed68d":"markdown","77cd0de8":"markdown","e7848a6e":"markdown"},"source":{"4a5940bf":"# import the required libraries:\nimport os, sys\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, GRU, Dense, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt","ab2c417e":"#Execute this script to set values for different parameters:\nBATCH_SIZE = 64\nEPOCHS = 20\nLSTM_NODES =256\nNUM_SENTENCES = 20000\nMAX_SENTENCE_LENGTH = 50\nMAX_NUM_WORDS = 20000\nEMBEDDING_SIZE = 200","79ebcbf8":"input_sentences = []\noutput_sentences = []\noutput_sentences_inputs = []\n\ncount = 0\nfor line in open('..\/input\/fra-eng\/fra.txt', encoding=\"utf-8\"):\n    count += 1\n    if count > NUM_SENTENCES:\n        break\n    if '\\t' not in line:\n        continue\n    input_sentence = line.rstrip().split('\\t')[0]\n    output = line.rstrip().split('\\t')[1]\n\n    output_sentence = output + ' <eos>'\n    output_sentence_input = '<sos> ' + output\n\n    input_sentences.append(input_sentence)\n    output_sentences.append(output_sentence)\n    output_sentences_inputs.append(output_sentence_input)\n\nprint(\"Number of sample input:\", len(input_sentences))\nprint(\"Number of sample output:\", len(output_sentences))\nprint(\"Number of sample output input:\", len(output_sentences_inputs))","e3cccfd7":"print(\"English sentence: \",input_sentences[180])\nprint(\"French translation: \",output_sentences[180])","4290bf20":"# let\u2019s visualise the length of the sentences.\nimport pandas as pd\n\neng_len = []\nfren_len = []\n\n# populate the lists with sentence lengths\nfor i in input_sentences:\n      eng_len.append(len(i.split()))  \n\nfor i in output_sentences:\n      fren_len.append(len(i.split()))\n\nlength_df = pd.DataFrame({'english':eng_len, 'french':fren_len})\n\nlength_df.hist(bins = 20)\nplt.show()","8f28ccc8":"#tokenize the input sentences(input language) \ninput_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\ninput_tokenizer.fit_on_texts(input_sentences)\ninput_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n\n\nword2idx_inputs = input_tokenizer.word_index\nprint('Total unique words in the input: %s' % len(word2idx_inputs))\n\nmax_input_len = max(len(sen) for sen in input_integer_seq)\nprint(\"Length of longest sentence in input: %g\" % max_input_len)\n\n#with open('input_tokenizer_NMT.pickle', 'wb') as handle:\n#    pickle.dump(input_tokenizer, handle, protocol=4)","88ba58d4":"#tokenize the output sentences(Output language)\noutput_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\noutput_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\noutput_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\noutput_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n\nword2idx_outputs = output_tokenizer.word_index\nprint('Total unique words in the output: %s' % len(word2idx_outputs))\n\nnum_words_output = len(word2idx_outputs) + 1\nmax_out_len = max(len(sen) for sen in output_integer_seq)\nprint(\"Length of longest sentence in the output: %g\" % max_out_len)\n\n#with open('output_tokenizer_NMT.pickle', 'wb') as handle:\n#    pickle.dump(output_tokenizer, handle, protocol=4)","0e91b911":"encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\nprint(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\nprint(\"encoder_input_sequences[180]:\", encoder_input_sequences[180])","d7911a47":"print(word2idx_inputs[\"join\"])\nprint(word2idx_inputs[\"us\"])","2f7ca053":"decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\nprint(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\nprint(\"decoder_input_sequences[180]:\", decoder_input_sequences[180])","787ea924":"print(word2idx_outputs[\"<sos>\"])\nprint(word2idx_outputs[\"joignez-vous\"])\nprint(word2idx_outputs[\"\u00e0\"])\nprint(word2idx_outputs[\"nous.\"])","1f69912d":"decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\nprint(\"decoder_output_sequences.shape:\", decoder_output_sequences.shape)\n","d84303be":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\n\nglove_file = open(r'..\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.200d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    rec = line.split()\n    word = rec[0]\n    vector_dimensions = asarray(rec[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()","ca2240be":"num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\nembedding_matrix = zeros((num_words, EMBEDDING_SIZE))\nfor word, index in word2idx_inputs.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","40829cdd":"print(embeddings_dictionary[\"join\"])","12ff383c":"print(embedding_matrix[464])","d36afe33":"embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)","52f24579":"#(number of inputs, length of the output sentence, the number of words in the output)","4f9cb405":"decoder_targets_one_hot = np.zeros((\n        len(input_sentences),\n        max_out_len,\n        num_words_output\n    ),\n    dtype='float32'\n)\ndecoder_targets_one_hot.shape","c60de84a":"for i, d in enumerate(decoder_output_sequences):\n    for t, word in enumerate(d):\n        decoder_targets_one_hot[i, t, word] = 1","7796b49b":"encoder_inputs = Input(shape=(max_input_len,))\nx = embedding_layer(encoder_inputs)\nencoder = LSTM(LSTM_NODES, return_state=True)\n\nencoder_outputs, h, c = encoder(x)\nencoder_states = [h, c]","d91cbf22":"decoder_inputs = Input(shape=(max_out_len,))\n\ndecoder_embedding = Embedding(num_words_output, LSTM_NODES)\ndecoder_inputs_x = decoder_embedding(decoder_inputs)\n\ndecoder_lstm = LSTM(LSTM_NODES, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs_x, initial_state=encoder_states)","8fe1c8d7":"decoder_dense = Dense(num_words_output, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","286e4b17":"#Compile\nmodel = Model([encoder_inputs,decoder_inputs], decoder_outputs)\nmodel.compile(\n    optimizer='rmsprop',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\nmodel.summary()","c7f25b09":"from keras.utils import plot_model\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","49913ad7":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","5092de51":"history = model.fit([encoder_input_sequences, decoder_input_sequences], decoder_targets_one_hot,\n    batch_size=BATCH_SIZE,\n    epochs=20,\n    callbacks=[es],\n    validation_split=0.1,\n)","a5ff509f":"model.save('seq2seq_eng-fra.h5')","2e3447c8":"import matplotlib.pyplot as plt\n# %matplotlib inline\nplt.title('Model Loss')\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","2d92f608":"encoder_model = Model(encoder_inputs, encoder_states)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\nmodel.load_weights('seq2seq_eng-fra.h5')","f4e112db":"decoder_state_input_h = Input(shape=(LSTM_NODES,))\ndecoder_state_input_c = Input(shape=(LSTM_NODES,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\ndecoder_inputs_single = Input(shape=(1,))\ndecoder_inputs_single_x = decoder_embedding(decoder_inputs_single)","66449c20":"decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x, initial_state=decoder_states_inputs)\n","fe9cd96c":"decoder_states = [h, c]\ndecoder_outputs = decoder_dense(decoder_outputs)","8970905e":"decoder_model = Model(\n    [decoder_inputs_single] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states\n)","03d6d973":"from keras.utils import plot_model\nplot_model(decoder_model, to_file='model_plot_dec.png', show_shapes=True, show_layer_names=True)","898f95e9":"idx2word_input = {v:k for k, v in word2idx_inputs.items()}\nidx2word_target = {v:k for k, v in word2idx_outputs.items()}","dcef8b15":"def translate_sentence(input_seq):\n    states_value = encoder_model.predict(input_seq)\n    target_seq = np.zeros((1, 1))\n    target_seq[0, 0] = word2idx_outputs['<sos>']\n    eos = word2idx_outputs['<eos>']\n    output_sentence = []\n\n    for _ in range(max_out_len):\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n        idx = np.argmax(output_tokens[0, 0, :])\n\n        if eos == idx:\n            break\n\n        word = ''\n\n        if idx > 0:\n            word = idx2word_target[idx]\n            output_sentence.append(word)\n\n        target_seq[0, 0] = idx\n        states_value = [h, c]\n\n    return ' '.join(output_sentence)","60d103cd":"i = np.random.choice(len(input_sentences))\ninput_seq = encoder_input_sequences[i:i+1]\ntranslation = translate_sentence(input_seq)\nprint('Input Language : ', input_sentences[i])\nprint('Actual translation : ', output_sentences[i])\nprint('French translation : ', translation)","26f84799":"i = np.random.choice(len(input_sentences))\ninput_seq = encoder_input_sequences[i:i+1]\ntranslation = translate_sentence(input_seq)\nprint('Input Language : ', input_sentences[i])\nprint('Actual translation : ', output_sentences[i])\nprint('French translation : ', translation)","3ee38ffa":"i = np.random.choice(len(input_sentences))\ninput_seq = encoder_input_sequences[i:i+1]\ntranslation = translate_sentence(input_seq)\nprint('Input Language : ', input_sentences[i])\nprint('Actual translation : ', output_sentences[i])\nprint('French translation : ', translation)","7456a402":"i = np.random.choice(len(input_sentences))\ninput_seq = encoder_input_sequences[i:i+1]\ntranslation = translate_sentence(input_seq)\nprint('Input Language : ', input_sentences[i])\nprint('Actual translation : ', output_sentences[i])\nprint('French translation : ', translation)","2ab72e73":"i = np.random.choice(len(input_sentences))\ninput_seq = encoder_input_sequences[i:i+1]\ntranslation = translate_sentence(input_seq)\nprint('Input Language : ', input_sentences[i])\nprint('Actual translation : ', output_sentences[i])\nprint('French translation : ', translation)","620c9fb5":"To make predictions, the final layer of the model will be a dense layer, therefore we need the outputs in the form of one-hot encoded vectors, since we will be using softmax activation function at the dense layer. To create such one-hot encoded output, the next step is to assign 1 to the column number that corresponds to the integer representation of the word. ","12c0dbd8":"Now the lengths of longest sentence can also be varified from the histogram above. And it can be concluded that English sentences are normally shorter and contain a smaller number of words on average, compared to the translated French sentences.","4fa67c34":"# Making Predictions\n","1010cc5f":"In the same way, the decoder outputs and the decoder inputs are padded.\n\n","87914dff":"We need to generate two copies of the translated sentence: one with the start-of-sentence token and the other with the end-of-sentence token.\n\n","0fd3ef08":"Let's plot our model to see how it looks.","a445e07d":"For tokenization, the Tokenizer class from the keras.preprocessing.text library can be used. The tokenizer class performs two tasks:\n\n1. It divides a sentence into the corresponding list of word\n\n2. Then it converts the words to integers\n\nAlso the **word_index** attribute of the Tokenizer class returns a word-to-index dictionary where words are the keys and the corresponding integers are the values. ","b8dc9198":"From the output, you can see that we have two types of input. input_1 is the input placeholder for the encoder, which is embedded and passed through lstm_1 layer, which basically is the encoder LSTM. There are three outputs from the lstm_1 layer: the output, the hidden layer and the cell state. However, only the cell state and the hidden state are passed to the decoder.\n\nHere the lstm_2 layer is the decoder LSTM. The input_2 contains the output sentences with <sos> token appended at the start. The input_2 is also passed through an embedding layer and is used as input to the decoder LSTM, lstm_2. Finally, the output from the decoder LSTM is passed through the dense layer to make predictions.","319069fb":"For English sentences, i.e. the inputs, we will use the GloVe word embeddings. For the translated French sentences in the output, we will use custom word embeddings.","361fa2f3":"The sentence at index 180 of the decoder input is <sos> Joignez-vous \u00e0 nous. If you print the corresponding integers from the word2idx_outputs dictionary, you should see 2, 2028, 20, and 228 printed on the console.","69def489":"Recall that we have 2150 unique words in the input. We will create a matrix where the row number will represent the integer value for the word and the columns will correspond to the dimensions of the word. This matrix will contain the word embeddings for the words in our input sentences.\n\n","3b4444be":"You can see the original sentence, i.e. **Join us**; its corresponding translation in the output, i.e **Joignez-vous \u00e0 nous.** <eos>. Notice, here we have <eos> token at the end of the sentence. Similarly, for the input to the decoder, we have <sos> **Joignez-vous \u00e0 nous.**","81439f3b":"The next step is tokenizing the original and translated sentences and applying padding to the sentences that are longer or shorter than a certain length, which in case of inputs will be the length of the longest input sentence. And for the output this will be the length of the longest sentence in the output.","417b1b85":"We already converted our words into integers. So what's the difference between integer representation and word embeddings?\n\nThere are two main differences between single integer representation and word embeddings. With integer reprensentation, a word is represented only with a single integer. With vector representation a word is represented by a vector of 50, 100, 200, or whatever dimensions you like. Hence, word embeddings capture a lot more information about words. Secondly, the single-integer representation doesn't capture the relationships between different words. On the contrary, word embeddings retain relationships between the words. You can either use custom word embeddings or you can use pretrained word embeddings.","581577d3":"Here we will translate **French** into **Engish** language. Please read this [article](https:\/\/www.theaidream.com\/post\/how-ai-is-changing-personal-data-tracking) to understand step by step approach.","d4087cab":"# Neural machine translation","a20d6171":"**Creates the embedding layer for the input:**","5b95e4b0":"# Creating the Model\n\n","a7c67ea5":"Next, we need to create the encoder and decoders. The input to the encoder will be the sentence in English and the output will be the hidden state and cell state of the LSTM.","de6af220":"Since there are 20,000 sentences in the input and each input sentence is of length 6, the shape of the input is now (20000, 6).","f9c0835a":"The method will accept an input-padded sequence English sentence (in the integer form) and will return the translated French sentence. ","855adefd":"The histogram above shows  maximum length of the French sentences is 12 and that of the English sentence is 6.","f7d84498":"we want our output to be a sequence of words in the French language. To do so, we need to convert the integers back to words. We will create new dictionaries for both inputs and outputs where the keys will be the integers and the corresponding values will be the words.","64ca7a37":"The first thing we need to do is to define our outputs, as we know that the output will be a sequence of words. Recall that the total number of unique words in the output are 9511. Therefore, each word in the output can be any of the 9511 words. The length of an output sentence is 12. And for each input sentence, we need a corresponding output sentence. Therefore, the final shape of the output will be:","0d04631c":"You can see that the values for the 464th row in the embedding matrix are similar to the vector representation of the word **join** in the GloVe dictionary, which confirms that rows in the embedding matrix represent corresponding word embeddings from the GloVe word embedding dictionary. This word embedding matrix will be used to create the embedding layer for our LSTM model.","6861a1dc":"Next, we need to pad the input. The reason behind padding the input and the output is that text sentences can be of varying length, however LSTM expects input instances with the same length. Therefore, we need to convert our sentences into fixed-length vectors. One way to do this is via padding.","24f12f9a":"Dataset: http:\/\/www.manythings.org\/anki\/","f5c2f86d":"In the previous section, we saw that the integer representation for the word **join** is 464. Let's now check the 464th index of the word embedding matrix.\n\n","bba3bcfd":"# Data Preprocessing","074b34a9":"Finally, the output from the decoder LSTM is passed through a dense layer to predict decoder outputs.","8df6482f":"# Word Embeddings\n","e798c3d5":"# Sample Predictions","63452912":"You may recall that the original sentence at index 180 is **join us**. The tokenizer divided the sentence into two words ***join*** and ***us***, converted them to integers, and then applied pre-padding by adding four zeros at the start of the corresponding integer sequence for the sentence at index 180 of the input list.\n\nTo verify that the integer values for ***join*** and ***us*** are 464 and 59 respectively, you can pass the words to the word2index_inputs dictionary, as shown below:","6992a532":"Now randomly print a sentence to analyse your dataset.","f5336ef5":"The language translation model that we are going to develop will translate English sentences into their French language counterparts. To develop such a model, we need a dataset that contains English sentences and their French translations.","f96ed68d":"Let's create word embeddings for the inputs first. To do so, we need to load the GloVe word vectors into memory. We will then create a dictionary where words are the keys and the corresponding vectors are values, ","77cd0de8":"The next step is to define the decoder. The decoder will have two inputs: the hidden state and cell state from the encoder and the input sentence, which actually will be the output sentence with an <sos> token appended at the beginning.","e7848a6e":"# Tokenization and Padding\n"}}