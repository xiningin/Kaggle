{"cell_type":{"2bdef6dc":"code","56a00205":"code","c1ab6d68":"code","256a7a52":"code","76b25876":"code","576d89c1":"code","201af93f":"code","a7e32fdf":"code","c1b54a34":"code","aa62b0d1":"code","eda00b30":"code","585e4048":"code","86ec6c51":"code","df65388e":"code","723a376e":"code","5c75068b":"code","a7a7fba3":"code","7c5bcd28":"code","a15ad645":"code","056592ea":"code","018f1754":"code","581ed366":"code","acf07c7a":"code","ee35dbf0":"code","d50f344f":"code","1210573b":"code","ca3aff49":"markdown","6ac82f1a":"markdown","f700d67a":"markdown","69622350":"markdown","272431c0":"markdown","8535d1e2":"markdown","628ba66d":"markdown","ba5a744a":"markdown","32d28179":"markdown","4af82fef":"markdown","eaae9247":"markdown","74267813":"markdown","29223657":"markdown","9ab9ed0c":"markdown","5fc14354":"markdown","3fc11638":"markdown","ffaf654e":"markdown","e1b70ca9":"markdown","4578830c":"markdown","fcc76471":"markdown","b0960597":"markdown","12aa9d26":"markdown","02490229":"markdown","1e9e3368":"markdown","c18cc6d6":"markdown","3d411057":"markdown","557e3213":"markdown","5db07c82":"markdown","6e33acca":"markdown","14ba57a3":"markdown"},"source":{"2bdef6dc":"import os\nimport shutil\n\nimport cv2\nimport gc\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.layers import (BatchNormalization, Dense, Dropout, Flatten)\nfrom keras.metrics import categorical_accuracy, top_k_categorical_accuracy\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.model_selection import train_test_split","56a00205":"metadata = pd.read_csv(\"\/kaggle\/input\/HAM10000_metadata.csv\")\nmetadata.head()","c1ab6d68":"metadata[\"dx\"].value_counts() \/ metadata.shape[0]","256a7a52":"image_sample = cv2.imread(\"\/kaggle\/input\/ham10000_images_part_1\/ISIC_0027269.jpg\")\nprint(image_sample.shape)","76b25876":"lesion_id_cnt = metadata[\"lesion_id\"].value_counts()\ndef check_duplicates(id):\n    \n    if lesion_id_cnt[id] > 1:\n        return True\n    else:\n        return False\n\nmetadata[\"has_duplicate\"] = metadata[\"lesion_id\"].map(check_duplicates)","576d89c1":"metadata[\"has_duplicate\"].value_counts()","201af93f":"image_folder_1 = \"\/kaggle\/input\/ham10000_images_part_1\"\nimage_folder_2 = \"\/kaggle\/input\/ham10000_images_part_2\"\nmetadata[\"folder\"] = 0\nmetadata.set_index(\"image_id\", drop=False, inplace=True)\n\nfor image in os.listdir(image_folder_1):\n    image_id = image.split(\".\")[0]\n    metadata.loc[image_id, \"folder\"] = \"1\"\n\nfor image in os.listdir(image_folder_2):\n    image_id = image.split(\".\")[0]\n    metadata.loc[image_id, \"folder\"] = \"2\"","a7e32fdf":"metadata.head()","c1b54a34":"data_train_no_dup, data_val = train_test_split(metadata[metadata[\"has_duplicate\"] == False], test_size=0.36, stratify=metadata[metadata[\"has_duplicate\"] == False][\"dx\"]) # 36% of the data with no duplicates is roughly 20% of the total\ndata_train = pd.concat((data_train_no_dup, metadata[metadata[\"has_duplicate\"] == True]), axis=0)\ndata_val, data_test = train_test_split(data_val, test_size=0.5, stratify=data_val[\"dx\"])\nprint(\"Train: \" + str(data_train.shape[0] \/ metadata.shape[0]))\nprint(\"Validation: \" + str(data_val.shape[0] \/ metadata.shape[0]))\nprint(\"Test: \" + str(data_test.shape[0] \/ metadata.shape[0]))\nval_len = data_val.shape[0]\ntest_len = data_test.shape[0]","aa62b0d1":"base_dir = \"base_dir\"\nos.mkdir(base_dir)\n\ntrain_dir = os.path.join(base_dir, \"image_train\")\nos.mkdir(train_dir)\n\nval_dir = os.path.join(base_dir, \"image_val\")\nos.mkdir(val_dir)\n\ntest_dir = os.path.join(base_dir, \"image_test\")\nos.mkdir(test_dir)\n\nlabels = list(metadata[\"dx\"].unique())\n\nfor label in labels:\n    label_path_train = os.path.join(train_dir, label)\n    os.mkdir(label_path_train)\n    label_path_val = os.path.join(val_dir, label)\n    os.mkdir(label_path_val)\n    label_path_test = os.path.join(test_dir, label)\n    os.mkdir(label_path_test)","eda00b30":"image_dir = \"\/kaggle\/input\/ham10000_images_part_\"\n\nfor i in range(data_train.shape[0]):\n    image_name = data_train[\"image_id\"][i] + \".jpg\"\n    src_dir = os.path.join(image_dir + data_train[\"folder\"][i], image_name)\n    dst_dir = os.path.join(train_dir, data_train[\"dx\"][i], image_name)\n    shutil.copyfile(src_dir, dst_dir)\n\nfor i in range(data_val.shape[0]):\n    image_name = data_val[\"image_id\"][i] + \".jpg\"\n    src_dir = os.path.join(image_dir + data_val[\"folder\"][i], image_name)\n    dst_dir = os.path.join(val_dir, data_val[\"dx\"][i], image_name)\n    shutil.copyfile(src_dir, dst_dir)\n    \nfor i in range(data_test.shape[0]):\n    image_name = data_test[\"image_id\"][i] + \".jpg\"\n    src_dir = os.path.join(image_dir + data_test[\"folder\"][i], image_name)\n    dst_dir = os.path.join(test_dir, data_test[\"dx\"][i], image_name)\n    shutil.copyfile(src_dir, dst_dir)","585e4048":"for label in labels:\n    print(label + \" train: \" + str(len(os.listdir(os.path.join(train_dir, label)))))\nprint(\"\\n\")\nfor label in labels:\n    print(label + \" val: \" + str(len(os.listdir(os.path.join(val_dir, label)))))\nprint(\"\\n\")\nfor label in labels:\n    print(label + \" val: \" + str(len(os.listdir(os.path.join(test_dir, label)))))","86ec6c51":"del data_train_no_dup, metadata\ngc.collect()","df65388e":"data_gen_param = {\n    \"rotation_range\": 180,\n    \"width_shift_range\": 0.1,\n    \"height_shift_range\": 0.1,\n    \"zoom_range\": 0.1,\n    \"horizontal_flip\": True,\n    \"vertical_flip\": True\n}\ndata_generator = ImageDataGenerator(**data_gen_param)\nnum_images_each_label = 6000\n\naug_dir = os.path.join(base_dir, \"aug_dir\")\nos.mkdir(aug_dir)\n\nfor label in labels:\n    \n    img_dir = os.path.join(aug_dir, \"aug_img\")\n    os.mkdir(img_dir)\n    \n    src_dir_label = os.path.join(train_dir, label)\n    for image_name in os.listdir(src_dir_label):\n        shutil.copy(os.path.join(src_dir_label, image_name), os.path.join(img_dir, image_name))\n    \n    batch_size = 32\n    data_flow_param = {\n        \"directory\": aug_dir,\n        \"color_mode\": \"rgb\",\n        \"batch_size\": batch_size,\n        \"shuffle\": True,\n        \"save_to_dir\": os.path.join(train_dir, label),\n        \"save_format\": \"jpg\"\n    }\n    aug_data_gen = data_generator.flow_from_directory(**data_flow_param)\n    \n    num_img_aug = num_images_each_label - len(os.listdir(os.path.join(train_dir, label)))\n    num_batch = int(num_img_aug \/ batch_size)\n    \n    for i in range(0, num_batch):\n        next(aug_data_gen)\n    \n    shutil.rmtree(img_dir)","723a376e":"for label in labels:\n    print(label + \" train: \" + str(len(os.listdir(os.path.join(train_dir, label)))))\nprint(\"\\n\")\nfor label in labels:\n    print(label + \" val: \" + str(len(os.listdir(os.path.join(val_dir, label)))))","5c75068b":"IMAGE_SHAPE = (224, 224, 3)\ndata_gen_param = {\n    \"samplewise_center\": True,\n    \"samplewise_std_normalization\": True,\n    \"rotation_range\": 180,\n    \"width_shift_range\": 0.1,\n    \"height_shift_range\": 0.1,\n    \"zoom_range\": 0.1,\n    \"horizontal_flip\": True,\n    \"vertical_flip\": True,\n    \"rescale\": 1.0 \/ 255\n}\ndata_generator = ImageDataGenerator(**data_gen_param)\n\ntrain_flow_param = {\n    \"directory\": train_dir,\n    \"batch_size\": batch_size,\n    \"target_size\": IMAGE_SHAPE[:2],\n    \"shuffle\": True\n}\ntrain_flow = data_generator.flow_from_directory(**train_flow_param)\n\nval_flow_param = {\n    \"directory\": val_dir,\n    \"batch_size\": batch_size,\n    \"target_size\": IMAGE_SHAPE[:2],\n    \"shuffle\": False\n}\nval_flow = data_generator.flow_from_directory(**val_flow_param)\n\ntest_flow_param = {\n    \"directory\": test_dir,\n    \"batch_size\": 1,\n    \"target_size\": IMAGE_SHAPE[:2],\n    \"shuffle\": False\n}\ntest_flow = data_generator.flow_from_directory(**test_flow_param)","a7a7fba3":"dropout_dense = 0.1\n\n# vgg16 = keras.applications.vgg16.VGG16(include_top=False, input_shape=IMAGE_SHAPE, pooling=\"max\")\n\n# model = Sequential()\n# model.add(vgg16)\n# model.add(Dropout(dropout_dense))\n# model.add(BatchNormalization())\n# model.add(Dense(256, activation=\"relu\"))\n# model.add(Dropout(dropout_dense))\n# model.add(BatchNormalization())\n# model.add(Dense(256, activation=\"relu\"))\n# model.add(Dense(7, activation=\"softmax\"))\n\nmobilenet_model = MobileNet(input_shape=IMAGE_SHAPE, include_top=False, pooling=\"max\")\n\nmodel = Sequential()\nmodel.add(mobilenet_model)\nmodel.add(Dropout(dropout_dense))\nmodel.add(BatchNormalization())\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(BatchNormalization())\nmodel.add(Dense(7, activation=\"softmax\"))\n\ndef top_2_acc(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=2)\n\ndef top_3_acc(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\nmodel.compile(Adam(0.01), loss=\"categorical_crossentropy\", metrics=[categorical_accuracy, top_2_acc, top_3_acc])","7c5bcd28":"filepath = \"model.h5\"\n\ncheckpoint_param = {\n    \"filepath\": filepath,\n    \"monitor\": \"val_categorical_accuracy\",\n    \"verbose\": 1,\n    \"save_best_only\": True,\n    \"mode\": \"max\"\n}\ncheckpoint = ModelCheckpoint(**checkpoint_param)\n\nlr_decay_params = {\n    \"monitor\": \"val_loss\",\n    \"factor\": 0.5,\n    \"patience\": 2,\n    \"min_lr\": 1e-5\n}\nlr_decay = ReduceLROnPlateau(**lr_decay_params)\n\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=4, verbose=1)","a15ad645":"fit_params = {\n    \"generator\": train_flow,\n    \"steps_per_epoch\": data_train.shape[0] \/\/ batch_size,\n    \"epochs\": 15,\n    \"verbose\": 1,\n    \"validation_data\": val_flow,\n    \"validation_steps\": data_val.shape[0] \/\/ batch_size,\n    \"callbacks\": [checkpoint, lr_decay, early_stopping]\n}\nprint(\"Training the model...\")\n\nhistory = model.fit_generator(**fit_params)\nprint(\"Done!\")","056592ea":"_, val_acc, val_top_2_acc, val_top_3_acc = model.evaluate_generator(val_flow, steps=len(val_flow))\ny_val_true = val_flow.classes\ny_val_pred = np.argmax(model.predict_generator(val_flow, steps=len(val_flow)), axis=1)\nval_f1_score = f1_score(y_val_true, y_val_pred, average=\"micro\")\n\nprint(\"Validation accuracy: {:.4f}\".format(val_acc))\nprint(\"Validation top-2 accuracy: {:.4f}\".format(val_top_2_acc))\nprint(\"Validation top-3 accuracy: {:.4f}\".format(val_top_3_acc))\nprint(\"Validation F1 score: {:.4f}\".format(val_f1_score))","018f1754":"_, test_acc, test_top_2_acc, test_top_3_acc = model.evaluate_generator(test_flow, steps=len(test_flow))\ny_test_true = test_flow.classes\ny_test_pred = np.argmax(model.predict_generator(test_flow, steps=len(test_flow)), axis=1)\ntest_f1_score = f1_score(y_test_true, y_test_pred, average=\"micro\")\n\nprint(\"Test accuracy: {:.4f}\".format(test_acc))\nprint(\"Test top-2 accuracy: {:.4f}\".format(test_top_2_acc))\nprint(\"Test top-3 accuracy: {:.4f}\".format(test_top_3_acc))\nprint(\"Test F1 score: {:.4f}\".format(test_f1_score))","581ed366":"loss_train = history.history[\"loss\"]\nacc_train = history.history[\"categorical_accuracy\"]\nloss_val = history.history[\"val_loss\"]\nacc_val = history.history[\"val_categorical_accuracy\"]\nepochs = np.arange(1, len(loss_train) + 1)","acf07c7a":"plt.plot(epochs, acc_train, \"bo\", label=\"Training acc\")\nplt.plot(epochs, acc_val, \"b\", label=\"Validation acc\")\nplt.title(\"Accuracy\")\nplt.legend()\nplt.show()","ee35dbf0":"plt.plot(epochs, loss_train, \"bo\", label=\"Training loss\")\nplt.plot(epochs, loss_val, \"b\", label=\"Validation loss\")\nplt.title(\"Losses\")\nplt.legend()\nplt.show()","d50f344f":"conf_mat = confusion_matrix(y_test_true, y_test_pred)\nplt.imshow(conf_mat, interpolation=\"nearest\", cmap=plt.cm.Blues)\nplt.title(\"Confusion matrix\")\nplt.colorbar()\ntick_marks = np.arange(len(labels))\nplt.xticks(tick_marks, labels, rotation=45)\nplt.yticks(tick_marks, labels)\nplt.ylabel(\"y_true\")\nplt.xlabel(\"y_pred\")\nplt.tight_layout()","1210573b":"shutil.rmtree(base_dir)","ca3aff49":"Now set up all the hyper-parameters and train the model. Here I used the pre-trained MobileNet that have been trained on ImageNet classification task as the CNN structure and added a 7-unit softmax activation at the end of the network. Here, I also used top-2 and top-3 accuracy metrics.","6ac82f1a":"# 4 Set Up the Generator","f700d67a":"Now I need to split the data into training\/validation\/test datasets. I split the data in a 80%-10%-10% fashion.","69622350":"Now I need to augment the data and save the augmented images in each folder.","272431c0":"# 6 Evaluate the Model","8535d1e2":"See the accuracies and F1 scores on validation and test sets.","628ba66d":"Now make new directories for train\/val\/test images.","ba5a744a":"Now check the proportion of each label.","32d28179":"It was mentioned that there may be multiple images for one lesion. Thus, we need to use solely those lesions that have no duplicates (i.e. no other images of the same lesion) as the validation set and the test set. Now we add one more column to the table to mark if the lesion have duplicate images.","4af82fef":"First check what columns are in the metadata.","eaae9247":"The dataset is very uneven. Thus, we need to augment the data later.","74267813":"Now check the proportions of each label in each dataset.","29223657":"# 1 Data Exploration","9ab9ed0c":"I added checkpoint to track the performance of the model along each epoch. Also, I used learning rate decay and early stopping the get better convergence and prevent overfitting.","5fc14354":"Copy the images to the new directory.","3fc11638":"Delete the redundant data and collect the RAM.","ffaf654e":"Delete the image folder so that it won't output that many files when committed.","e1b70ca9":"Plot the confusion matrix to see where the model made mistakes.","4578830c":"I used generator to train the model since it saves up RAM.","fcc76471":"Now let's take a look at the data.","b0960597":"# Skin Cancer MNIST: HAM10000--Using Keras CNN\n\nThis is a multi-class classification problem. To faster implement the solution, I used the pre-trained MobileNet structure in Keras. Since the dataset is very imbalanced, I used data augmentation to make the size of each class approximately equal.","12aa9d26":"# 3 Augment the Data","02490229":"Now check if the data is balanced.","1e9e3368":"Check the image format.","c18cc6d6":"# 2 Split the Data","3d411057":"Track the performance on each epoch.","557e3213":"# 5 Train the Model","5db07c82":"Images are stored in 2 different folders. Thus, we need to mark which folder each specific image is in.","6e33acca":"Now train the model.","14ba57a3":"# 7 Future Improvement\n\n- Use stacked voting model to solve the data imbalance;\n- Larger augmented datasets;\n- Testing-time augmentation."}}