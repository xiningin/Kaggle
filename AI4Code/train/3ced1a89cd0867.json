{"cell_type":{"f4096d76":"code","ab7441a8":"code","debd138d":"code","775c91dd":"code","e955e006":"code","9a2d528a":"code","50a8c546":"code","7a2b7e14":"code","d10909fb":"code","d10de56a":"code","b0577fe8":"code","cfd56cf0":"code","7dc17608":"code","68ab5fc4":"code","d84ab9d1":"markdown","eb29a007":"markdown","14a80bb2":"markdown","fa8a81ed":"markdown","8ca96cf0":"markdown","241064f2":"markdown","3012d137":"markdown","90d88165":"markdown","982b6d37":"markdown","36501e9f":"markdown","377e5736":"markdown","41b091ca":"markdown","ff9e78ee":"markdown","d4602848":"markdown","b75351cd":"markdown","a6ee4a8e":"markdown","1ef2b888":"markdown","9b5547c1":"markdown","50ccb4bd":"markdown","fa2b4e6b":"markdown","3eebebcd":"markdown","dc709235":"markdown","f4d27492":"markdown","a5c9549f":"markdown","439df6e7":"markdown","960550d1":"markdown"},"source":{"f4096d76":"# Count i\u015fleminde her bir kelimenin her bir dok\u00fcmanda ka\u00e7 defa ge\u00e7ti\u011fi say\u0131l\u0131r.\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?']\n\n\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\nX.toarray()","ab7441a8":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer='word')\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\n\nX.toarray()","debd138d":"import pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\npd.set_option('display.expand_frame_repr', False)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndf = pd.read_csv(\"..\/input\/the-movies-dataset\/movies_metadata.csv\", low_memory=False)\ndf.head()","775c91dd":"df[\"overview\"].head()","e955e006":"df['overview'].head()\n\ntfidf = TfidfVectorizer(stop_words='english')\ndf['overview'] = df['overview'].fillna('')\ntfidf_matrix = tfidf.fit_transform(df['overview'])\ntfidf_matrix.shape","9a2d528a":"cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)","50a8c546":"#deleting duplicate records in indices\n\n\nindices = pd.Series(df.index, index=df['title'])\n\nindices = indices[~indices.index.duplicated(keep='last')]","7a2b7e14":"indices.shape\nindices[:10]","d10909fb":"indices[\"Sherlock Holmes\"]\n\nmovie_index = indices[\"Sherlock Holmes\"]","d10de56a":"cosine_sim[movie_index]\n\nsimilarity_scores = pd.DataFrame(cosine_sim[movie_index], columns=[\"score\"])","b0577fe8":"movie_indices = similarity_scores.sort_values(\"score\", ascending=False)[1:11].index\n\ndf['title'].iloc[movie_indices]","cfd56cf0":"def content_based_recommender(title, cosine_sim, dataframe):\n    \n    # Creating the indexes \n    indices = pd.Series(dataframe.index, index=dataframe['title'])\n    indices = indices[~indices.index.duplicated(keep='last')]\n    \n    # Capturing the index of the title\n    movie_index = indices[title]\n    \n    # Calculating similarity scores by title\n    similarity_scores = pd.DataFrame(cosine_sim[movie_index], columns=[\"score\"])\n    \n    # fetch the top 10 movies excluding the movie itself\n    movie_indices = similarity_scores.sort_values(\"score\", ascending=False)[1:11].index\n    return dataframe['title'].iloc[movie_indices]\n\ncontent_based_recommender(\"Sherlock Holmes\", cosine_sim, df)","7dc17608":"content_based_recommender(\"The Matrix\", cosine_sim, df)\n\ncontent_based_recommender(\"The Godfather\", cosine_sim, df)\n\ncontent_based_recommender('The Dark Knight Rises', cosine_sim, df)\n\ncontent_based_recommender('The Dark Knight Rises', cosine_sim, df)","68ab5fc4":"def calculate_cosine_sim(dataframe):\n    tfidf = TfidfVectorizer(stop_words='english')\n    dataframe['overview'] = dataframe['overview'].fillna('')\n    tfidf_matrix = tfidf.fit_transform(dataframe['overview'])\n    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n    return cosine_sim\n\n\ncosine_sim = calculate_cosine_sim(df)\n\ncontent_based_recommender('The Dark Knight Rises', cosine_sim, df)","d84ab9d1":"We rank according to this score. So we bring the most similar. We do not take the first place because the movie itself is in the first place by adding the [1:11] at the end of the line","eb29a007":"For all other distances (Manhattan, Euclid vs.), different distance functions can be written so that they can be compared separately and turned into an optimization study. ","14a80bb2":"We can also simply obtain Cosine Similarirty with a short but effective function like the one below and use it in our own calculations.","fa8a81ed":"Let's say we're reviewing comments for a movie. What we can do here is somehow represent these interpretations mathematically and compare their similarities. There are 2 basic methods that we will use for this:\n\n**1-Count-Vector (Counting how many times each word occurs in each document.)**\n\n**2-TF-IDF**","8ca96cf0":"# Count Vector","241064f2":"We look at how many movies there are. We are observing 10 of them","3012d137":"# Recommendation System Based on Movie Overviews","90d88165":"# Functionalizing the Calculations\n\nIf we functionalize all content based recommendation operations, we can easily obtain our outputs like the one below.","982b6d37":"![image.png](attachment:55fffbd0-828c-43d3-bc92-a32790bf7f9d.png)","36501e9f":"# TF-IDF\n\nIn text classification, a text document may partially match many categories. We need to find the best matching category for the text document. The term (word) frequency\/inverse document frequency (TF-IDF) approach is commonly used to weigh each word in the text document according to how unique it is. In other words, the TF-IDF approach captures the relevancy among words, text documents and par- ticular categories. **[1]**\n\n2.1 Term Frequency (TF)\nTF is used to measure that how many times a term is present in a document. Let\u2019s suppose, we have a document \u201cT1\u201d containing 5000 words and the word \u201cAlpha\u201d is present in the document exactly 10 times. It is very well known fact that, the total length of documents can vary from very small to large, so it is a possibility that any term may occur more frequently in large documents in comparison to small documents. So, to rectify this issue, the occurrence of any term in a document is divided by the total terms present in that document, to find the term frequency. So, in this case the term frequency of the word \u201cAlpha\u201d in the document \u201cT1\u201d will be\nTF = 10\/5000 = 0.002\n\n2.2 Inverse Document Frequency (IDF)\nNow, inverse document frequency will be discussed. When the term frequency of a document is calculated, it can be observed that the algorithm treats all keywords equally, doesn\u2019t matter if it is a stop word like \u201cof\u201d, which is incorrect. All keywords have different importance. Let\u2019s say, the stop word \u201cof\u201d is present in a document 2000 times but it is of no use or has a very less significance, that is exactly what IDF is for. The inverse document frequency assigns lower weight to frequent words and assigns greater weight for the words that are infrequent. For example, we have 10 documents and the term \u201ctechnology\u201d is present in 5 of those documents, so the inverse document frequency can be calculated as \nIDF = log_e (10\/5) = 0.3010\n\n2.3 Term Frequency - Inverse Document\nFrequency (TF-IDF)\nFrom Section 2.1 and 2.2, it is understood that, the greater or higher occurrence of a word in documents will give higher term frequency and the less occurrence of word in documents will yield higher importance (IDF) for that keyword searched in particular document. TF-IDF is nothing, but just the multiplication of term frequency (TF) and inverse document frequency (IDF). We have already calculated TF and IDF in Section 2.1 and 2.2, respectively. To calculate the TF-IDF we can do as.\nTF-IDF = 0.002*0.3010 = 0.000602       \n\n**[2]**\n\n\n\nTF = How many times x term occurs in the document \/ Total number of terms in the document\n\nIDF = ln (Total number of documents \/ Total number of documents with the term x in it)","377e5736":"1 - Euclidean Distance\n\n2 - Cosine Similarity\n\n![image.png](attachment:2fe7070d-736a-4106-88aa-a90f5b91758a.png)\n","41b091ca":"# Obtaining the TF-IDF for Our Case","ff9e78ee":"# Creating the TF-IDF matrix","d4602848":"We have obtained the normalized numerical representations.","b75351cd":"Today, companies operating in many different areas, especially e-commerce sites, are trying to analyze the behavior of their customers in order to provide better experiences to their customers.\n\nFor this, in addition to the detailed analyzes made in line with the marketing strategies, recommendation systems are also developed that offer suggestions according to the shopping of the customers. Companies aim to increase their own profitability while trying to meet the needs of customers with hybrid methods.\n\n\nRecommendation systems have been developed to provide efficient, convenient and personalized service to the user. Its most important feature is that while providing personalized recommendations to a user, it can predict the preferences of others by analyzing their behavior. The past preferences and behaviors of users are taken into account in making predictions.","a6ee4a8e":"# REFERENCES\n\n[1] Yun-tao, Z., Ling, G., & Yong-cheng, W. (2005). An improved TF-IDF approach for text classification. Journal of Zhejiang University-Science A, 6(1), 49-55.\n\n[2] Qaiser, S., & Ali, R. (2018). Text mining: use of TF-IDF to examine the relevance of words to documents. International Journal of Computer Applications, 181(1), 25-29.\nISO 690\t\n\n[3] https:\/\/scikit-learn.org\/stable\/modules\/metrics.html\n ","1ef2b888":"# Step by Step TF-IDF Calculation\n\nTF-IDF = TF(t) * IDF(t)\n\nSTEP 1: TF(t) = (Frequency of a term t observed in the relevant document) \/ (Total number of terms in the document) (term frequency)\n\nSTEP 2: IDF(t) = 1 + log_e((Total number of documents + 1) \/ (Number of documents with term t in it + 1) (inverse document frequency)\n\nSTEP 3: TF-IDF = TF(t) * IDF(t)\n\nSTEP 4: L2 normalization to TF-IDF Values.","9b5547c1":"![](https:\/\/miro.medium.com\/max\/1400\/1*Lr6qL0YjY_WqVK5u-AYHAQ.png)","50ccb4bd":"Although content-based filtering approaches require additional information about products and users' preferences, there is no need for a user community or a database of review scores.\n\nLet's explain content-based filtering with an example:\nIn the classical recommendation method, a user who is a member of the movie platform determines the genres he likes and offers him suggestions according to his preferences. So if he says he likes science fiction movies, suggestions are made among popular science fiction movies.\n\nWhen the content-based recommendation system is used, instead of recommending all science fiction movies to the same user, movies with the same subject, directed by similar directors, with similar comments or featuring similar actors can be suggested to the same user.","fa2b4e6b":"# Creating the Cosine Similarity Matrix","3eebebcd":"Let's examine the index of the Sherlock movie and assign it to a variable.","dc709235":"# Content Based Filtering","f4d27492":"> The first purpose is to express texts mathematically. (By using Count Vector or TF-IDF or any other method )\n> \n> Secondly, calculate the similarity of these mathematical expressions.  \n\n\n\nWe extracted these figures, then how do we present the proposal?\n\nWhile presenting the proposal, methods such as Cosine similarity, Euclidean distance, Pearson Correlation can be used. Let's remember briefly, cosine similarity is the cosine value of the angle between two vectors, and Euclidean distance is the measurement of the distance between two points. The Pearson correlation is a number between -1 and 1 that shows how linearly the two variables are related.","a5c9549f":"Oklid, Pearson, Manhattan any of these can be used. These metrics can change from problem to problem. An optimization study can also be made by comparing them. The result we will obtain below by calculating with the Cosine Similarity will be the similarities of a movie with all the others.","439df6e7":"**The disadvantage of the count vector is that high frequency cells create bias in distance calculations. However, this bias can sometimes lead to useful results, but we will not touch on it in this study.**","960550d1":"We bring cosine similarities of Sherlock and other movies."}}