{"cell_type":{"afce3563":"code","bd055ea2":"code","c9c874d3":"code","d8d88bed":"code","4b15f404":"code","af8c9e8b":"code","21d7cc92":"code","3eba6584":"code","4a692be3":"code","cb8b2772":"code","e23100eb":"code","140a3d7a":"code","211a0209":"code","4ba47150":"code","63681075":"code","92e01d85":"code","49c3cf7b":"code","08a1a4ba":"code","a872d5db":"code","32317b76":"code","734f490f":"code","1db35872":"code","2a83c34e":"code","baab39a8":"code","9b065732":"code","6f980072":"code","b4fc9c5a":"code","f51c2dce":"code","8fa30df5":"code","a91201fd":"code","a31e2c85":"code","ffbcf0cb":"code","e12cf5b4":"code","c7bc3868":"code","becc8517":"code","77384de6":"code","45734e64":"code","80810d91":"code","33ed6fbf":"code","1aea474c":"code","f3b2bb7b":"code","4ef0986f":"code","e40496d1":"code","d54a0967":"code","cb427d5c":"code","a8bcb16b":"code","8078b970":"code","5b37fca4":"code","63f8d893":"code","cd09109e":"code","6931ef6d":"code","86ffaaab":"code","36fae890":"code","b5dced20":"code","c829fd4d":"code","09d8e63a":"code","257c1635":"code","ea5f6e92":"code","0da7f5fa":"code","637136e1":"code","d0c3ece6":"code","5bd67715":"code","240ff317":"code","e9d3036d":"code","ccc071f2":"code","c72b1c3d":"code","6040e343":"code","617dd654":"markdown","73dcab32":"markdown"},"source":{"afce3563":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bd055ea2":"from keras.models import *\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nimport time\nimport pickle\nimport re\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()","c9c874d3":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub","d8d88bed":"CRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'","4b15f404":"sample_submission_df = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\ntest_df = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\ntrain_df = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")","af8c9e8b":"sample_submission_df.shape","21d7cc92":"train_df.shape","3eba6584":"# Exploring Data\nsamp_id = 9\nprint('Question Title: %s \\n' % train_df['question_title'].values[samp_id])\nprint('Question Body: %s \\n' % train_df['question_body'].values[samp_id])\nprint('Answer: %s' % train_df['answer'].values[samp_id])","4a692be3":"train_users = set(train_df['question_user_page'].unique())\ntest_users = set(test_df['question_user_page'].unique())\n\nprint('Unique users in train set: %s' % len(train_users))\nprint('Unique users in test set: %s' % len(test_users))\nprint('Users in both sets: %s' % len(train_users & test_users))\nprint('What users are in both sets? %s' % list(train_users & test_users))","cb8b2772":"train_df.head()","e23100eb":"(train_df.head()).T","140a3d7a":"#!pip install transformers","211a0209":"train_df['all_text'] = train_df['question_title'] + \" [SEP] \" + train_df['question_body'] + \" [SEP] \" + train_df['question_user_name'] + \" [SEP] \" + train_df['question_user_page'] + \" [SEP] \" + train_df['question_user_page'] + \" [SEP] \" + train_df['answer'] + \" [SEP] \" + train_df['answer_user_name'] + \" [SEP] \" + train_df['answer_user_page'] + \" [SEP] \" + train_df['category'] + \" [SEP] \" + train_df['host'] \n\ntest_df['all_text'] = test_df['question_title'] + \" [SEP] \" + test_df['question_body'] + \" [SEP] \" + test_df['question_user_name'] + \" [SEP] \" + test_df['question_user_page'] + \" [SEP] \" + test_df['question_user_page'] + \" [SEP] \" + test_df['answer'] + \" [SEP] \" + test_df['answer_user_name'] + \" [SEP] \" + test_df['answer_user_page'] + \" [SEP] \" + test_df['category'] + \" [SEP] \" + test_df['host'] ","4ba47150":"# You can use this pre-processing steps at later stage\n\nimport re\ndef pre_process(text):\n    new_text =re.sub('[0-9]', '', text)\n    new_text = re.sub(r\"\\u200b\",\"\",new_text)\n    new_text = re.sub(r\"\\.+\",\".\",new_text)\n    new_text = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '',new_text, flags=re.MULTILINE)\n    new_text = re.sub(\"'\", \"\", new_text)\n    new_text = re.sub(r'\u2191', '', new_text)\n    new_text = re.sub(\"\\t\", \"\", new_text)\n    new_text = re.sub(\"\\xa0\", \"\", new_text)\n    new_text = re.sub(\"\\(|\\)|\\[|\\]\", \"\", new_text)\n    new_text = re.sub(\"\\n\", \"\", new_text)\n    new_text = re.sub(\"\\.\", \"\", new_text)\n    new_text = re.sub(\"\\,\", \" \", new_text)\n    new_text = re.sub(\"[\/%]\", \" \", new_text)\n    new_text = re.sub('[\/%:;]', '', new_text)\n    new_text = re.sub(' +', ' ', new_text)\n    return new_text","63681075":"# remove URL's from train and test\ntrain_df['all_text'] = train_df['all_text'].apply(lambda x: re.sub(r'http\\S+', '', x))\ntest_df['all_text'] = test_df['all_text'].apply(lambda x: re.sub(r'http\\S+', '', x))","92e01d85":"# remove numbers\ntrain_df['all_text'] = train_df['all_text'].str.replace(\"[0-9]\", \" \")\ntest_df['all_text'] = test_df['all_text'].str.replace(\"[0-9]\", \" \")","49c3cf7b":"# Adjusting the load_embeddings function, to now handle the pickled dict.\n\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words","08a1a4ba":"#Below Functions are to check the embedding\u2019s coverage and building vocabulary \nimport operator \n\ndef check_coverage(vocab,embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n","a872d5db":"# Lets load the embeddings \ntic = time.time()\nglove_embeddings = load_embeddings(GLOVE_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","32317b76":"# Lets check how many words we got covered \nvocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","734f490f":"# Lets replace few words which is not covered in our embedding\u2019s.\n\nreplaceWords1 = { \"won't\":\"will not\",\"$&@*#\":\"in most profane vulgar shitty terms\",\"#$&@*#\":\"shitty\",\n \"can't\":\"cannot\",\"aren't\": 'are not',\n \"Aren't\": 'Are not',\n \"AREN'T\": 'ARE NOT',\n \"C'est\": \"C'est\",\n \"C'mon\": \"C'mon\",\n \"c'mon\": \"c'mon\",\n \"can't\": 'cannot',\n \"Can't\": 'Cannot',\n \"CAN'T\": 'CANNOT',\n \"con't\": 'continued',\n \"cont'd\": 'continued',\n \"could've\": 'could have',\n \"couldn't\": 'could not',\n \"Couldn't\": 'Could not',\n \"didn't\": 'did not',\n \"Didn't\": 'Did not',\n \"DIDN'T\": 'DID NOT',\n \"don't\": 'do not',\n \"Don't\": 'Do not',\n \"DON'T\": 'DO NOT',\n \"doesn't\": 'does not',\n \"Doesn't\": 'Does not',\n \"else's\": 'else',\n \"gov's\": 'government',\n \"Gov's\": 'government',\n \"gov't\": 'government',\n \"Gov't\": 'government',\n \"govt's\": 'government',\n \"gov'ts\": 'governments',\n \"hadn't\": 'had not',\n \"hasn't\": 'has not',\n \"Hasn't\": 'Has not',\n \"haven't\": 'have not',\n \"Haven't\": 'Have not',\n \"he's\": 'he is',\n \"He's\": 'He is',\n \"he'll\": 'he will',\n \"He'll\": 'He will',\n \"he'd\": 'he would',\n \"He'd\": 'He would',\n \"Here's\": 'Here is',\n \"here's\": 'here is',\n \"I'm\": 'I am',\n \"i'm\": 'i am',\n \"I'M\": 'I am',\n \"I've\": 'I have',\n \"i've\": 'i have',\n \"I'll\": 'I will',\n \"i'll\": 'i will',\n \"I'd\": 'I would',\n \"i'd\": 'i would',\n \"ain't\": 'is not',\n \"isn't\": 'is not',\n \"Isn't\": 'Is not',\n \"ISN'T\": 'IS NOT',\n \"it's\": 'it is',\n \"It's\": 'It is',\n \"IT'S\": 'IT IS',\n \"I's\": 'It is',\n \"i's\": 'it is',\n \"it'll\": 'it will',\n \"It'll\": 'It will',\n \"it'd\": 'it would',\n \"It'd\": 'It would',\n \"Let's\": \"Let's\",\n \"let's\": 'let us',\n \"ma'am\": 'madam',\n \"Ma'am\": \"Madam\",\n \"she's\": 'she is',\n \"She's\": 'She is',\n \"she'll\": 'she will',\n \"She'll\": 'She will',\n \"she'd\": 'she would',\n \"She'd\": 'She would',\n \"shouldn't\": 'should not',\n \"that's\": 'that is',\n \"That's\": 'That is',\n \"THAT'S\": 'THAT IS',\n \"THAT's\": 'THAT IS',\n \"that'll\": 'that will',\n \"That'll\": 'That will',\n \"there's\": 'there is',\n \"There's\": 'There is',\n \"there'll\": 'there will',\n \"There'll\": 'There will',\n \"there'd\": 'there would',\n \"they're\": 'they are',\n \"They're\": 'They are',\n \"they've\": 'they have',\n \"They've\": 'They Have',\n \"they'll\": 'they will',\n \"They'll\": 'They will',\n \"they'd\": 'they would',\n \"They'd\": 'They would',\n \"wasn't\": 'was not',\n \"we're\": 'we are',\n \"We're\": 'We are',\n \"we've\": 'we have',\n \"We've\": 'We have',\n \"we'll\": 'we will',\n \"We'll\": 'We will',\n \"we'd\": 'we would',\n \"We'd\": 'We would',\n \"What'll\": 'What will',\n \"weren't\": 'were not',\n \"Weren't\": 'Were not',\n \"what's\": 'what is',\n \"What's\": 'What is',\n \"When's\": 'When is',\n \"Where's\": 'Where is',\n \"where's\": 'where is',\n \"Where'd\": 'Where would',\n \"who're\": 'who are',\n \"who've\": 'who have',\n \"who's\": 'who is',\n \"Who's\": 'Who is',\n \"who'll\": 'who will',\n \"who'd\": 'Who would',\n \"Who'd\": 'Who would',\n \"won't\": 'will not',\n \"Won't\": 'will not',\n \"WON'T\": 'WILL NOT',\n \"would've\": 'would have',\n \"wouldn't\": 'would not',\n \"Wouldn't\": 'Would not',\n \"would't\": 'would not',\n \"Would't\": 'Would not',\n \"y'all\": 'you all',\n \"Y'all\": 'You all',\n \"you're\": 'you are',\n \"You're\": 'You are',\n \"YOU'RE\": 'YOU ARE',\n \"you've\": 'you have',\n \"You've\": 'You have',\n \"y'know\": 'you know',\n \"Y'know\": 'You know',\n \"ya'll\": 'you will',\n \"you'll\": 'you will',\n \"You'll\": 'You will',\n \"you'd\": 'you would',\n \"You'd\": 'You would',\n \"Y'got\": 'You got',\n 'cause': 'because',\n \"had'nt\": 'had not',\n \"Had'nt\": 'Had not',\n \"how'd\": 'how did',\n \"how'd'y\": 'how do you',\n \"how'll\": 'how will',\n \"how's\": 'how is',\n \"I'd've\": 'I would have',\n \"I'll've\": 'I will have',\n \"i'd've\": 'i would have',\n \"i'll've\": 'i will have',\n \"it'd've\": 'it would have',\n \"it'll've\": 'it will have',\n \"mayn't\": 'may not',\n \"might've\": 'might have',\n \"mightn't\": 'might not',\n \"mightn't've\": 'might not have',\n \"must've\": 'must have',\n \"mustn't\": 'must not',\n \"mustn't've\": 'must not have',\n \"needn't\": 'need not',\n \"needn't've\": 'need not have',\n \"o'clock\": 'of the clock',\n \"oughtn't\": 'ought not',\n \"oughtn't've\": 'ought not have',\n \"shan't\": 'shall not',\n \"sha'n't\": 'shall not',\n \"shan't've\": 'shall not have',\n \"she'd've\": 'she would have',\n \"she'll've\": 'she will have',\n \"should've\": 'should have',\n \"shouldn't've\": 'should not have',\n \"so've\": 'so have',\n \"so's\": 'so as',\n \"this's\": 'this is',\n \"that'd\": 'that would',\n \"that'd've\": 'that would have',\n \"there'd've\": 'there would have',\n \"they'd've\": 'they would have',\n \"they'll've\": 'they will have',\n \"to've\": 'to have',\n \"we'd've\": 'we would have',\n \"we'll've\": 'we will have',\n \"what'll\": 'what will',\n \"what'll've\": 'what will have',\n \"what're\": 'what are',\n \"what've\": 'what have',\n \"when's\": 'when is',\n \"when've\": 'when have',\n \"where'd\": 'where did',\n \"where've\": 'where have',\n \"who'll've\": 'who will have',\n \"why's\": 'why is',\n \"why've\": 'why have',\n \"will've\": 'will have',\n \"won't've\": 'will not have',\n \"wouldn't've\": 'would not have',\n \"y'all'd\": 'you all would',\n \"y'all'd've\": 'you all would have',\n \"y'all're\": 'you all are',\n \"y'all've\": 'you all have',\n \"you'd've\": 'you would have',\n \"you'll've\": 'you will have',\n'bebecause':'be because',\n'I\u2019m':'I am',\n              'it\u2019s':'it is',\n                 'I\u2019ve':'I have',\n                 'don\u2019t':'do not',\n                'However':'but',\n                 'It\u2019s':'It is',\n                 'didn\u2019t':'did not',\n                 'can\u2019t':'can not',\n                 'that\u2019s':'that is',\n'doesn\u2019t':'does not',\n'I\u2019d':'I had',\n'isn\u2019t':'is not',\n'wasn\u2019t':'was not'\n                \n                }\n\ndef wordreplace(tweet,replaceWords):\n    for key in replaceWords:\n        tweet = tweet.replace(key,replaceWords[key])\n    return tweet\n\nfor index, row in train_df['all_text'].iteritems():\n    train_df['all_text'][index] = wordreplace(row,replaceWords1)\n    \nfor index, row in test_df['all_text'].iteritems():\n    test_df['all_text'][index] = wordreplace(row,replaceWords1)","1db35872":"# Now lets check if we have improved on our coverage \nvocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","2a83c34e":"import string\nlatin_similar = \"\u2019'\u2018\u00c6\u00d0\u018e\u018f\u0190\u0194\u0132\u014a\u0152\u1e9e\u00de\u01f7\u021c\u00e6\u00f0\u01dd\u0259\u025b\u0263\u0133\u014b\u0153\u0138\u017f\u00df\u00fe\u01bf\u021d\u0104\u0181\u00c7\u0110\u018a\u0118\u0126\u012e\u0198\u0141\u00d8\u01a0\u015e\u0218\u0162\u021a\u0166\u0172\u01afY\u0328\u01b3\u0105\u0253\u00e7\u0111\u0257\u0119\u0127\u012f\u0199\u0142\u00f8\u01a1\u015f\u0219\u0163\u021b\u0167\u0173\u01b0y\u0328\u01b4\u00c1\u00c0\u00c2\u00c4\u01cd\u0102\u0100\u00c3\u00c5\u01fa\u0104\u00c6\u01fc\u01e2\u0181\u0106\u010a\u0108\u010c\u00c7\u010e\u1e0c\u0110\u018a\u00d0\u00c9\u00c8\u0116\u00ca\u00cb\u011a\u0114\u0112\u0118\u1eb8\u018e\u018f\u0190\u0120\u011c\u01e6\u011e\u0122\u0194\u00e1\u00e0\u00e2\u00e4\u01ce\u0103\u0101\u00e3\u00e5\u01fb\u0105\u00e6\u01fd\u01e3\u0253\u0107\u010b\u0109\u010d\u00e7\u010f\u1e0d\u0111\u0257\u00f0\u00e9\u00e8\u0117\u00ea\u00eb\u011b\u0115\u0113\u0119\u1eb9\u01dd\u0259\u025b\u0121\u011d\u01e7\u011f\u0123\u0263\u0124\u1e24\u0126I\u00cd\u00cc\u0130\u00ce\u00cf\u01cf\u012c\u012a\u0128\u012e\u1eca\u0132\u0134\u0136\u0198\u0139\u013b\u0141\u013d\u013f\u02bcN\u0143N\u0308\u0147\u00d1\u0145\u014a\u00d3\u00d2\u00d4\u00d6\u01d1\u014e\u014c\u00d5\u0150\u1ecc\u00d8\u01fe\u01a0\u0152\u0125\u1e25\u0127\u0131\u00ed\u00eci\u00ee\u00ef\u01d0\u012d\u012b\u0129\u012f\u1ecb\u0133\u0135\u0137\u0199\u0138\u013a\u013c\u0142\u013e\u0140\u0149\u0144n\u0308\u0148\u00f1\u0146\u014b\u00f3\u00f2\u00f4\u00f6\u01d2\u014f\u014d\u00f5\u0151\u1ecd\u00f8\u01ff\u01a1\u0153\u0154\u0158\u0156\u015a\u015c\u0160\u015e\u0218\u1e62\u1e9e\u0164\u0162\u1e6c\u0166\u00de\u00da\u00d9\u00db\u00dc\u01d3\u016c\u016a\u0168\u0170\u016e\u0172\u1ee4\u01af\u1e82\u1e80\u0174\u1e84\u01f7\u00dd\u1ef2\u0176\u0178\u0232\u1ef8\u01b3\u0179\u017b\u017d\u1e92\u0155\u0159\u0157\u017f\u015b\u015d\u0161\u015f\u0219\u1e63\u00df\u0165\u0163\u1e6d\u0167\u00fe\u00fa\u00f9\u00fb\u00fc\u01d4\u016d\u016b\u0169\u0171\u016f\u0173\u1ee5\u01b0\u1e83\u1e81\u0175\u1e85\u01bf\u00fd\u1ef3\u0177\u00ff\u0233\u1ef9\u01b4\u017a\u017c\u017e\u1e93\"\nwhite_list = string.ascii_letters + string.digits + latin_similar + ' '\nwhite_list += \"'\"","baab39a8":"glove_chars = ''.join([c for c in tqdm(glove_embeddings) if len(c) == 1])\nglove_symbols = ''.join([c for c in glove_chars if not c in white_list])\nglove_symbols","9b065732":"# Chars available in the embedding\u2019s\njigsaw_chars = build_vocab(list(train_df[\"all_text\"]))\njigsaw_symbols = ''.join([c for c in jigsaw_chars if not c in white_list])\njigsaw_symbols","6f980072":"# Basically we can delete all symbols we have no embeddings for:\nsymbols_to_delete = ''.join([c for c in jigsaw_symbols if not c in glove_symbols])\nsymbols_to_delete","b4fc9c5a":"# The symbols we want to keep we need to isolate from our words. So lets setup a list of those to isolate.\nsymbols_to_isolate = ''.join([c for c in jigsaw_symbols if c in glove_symbols])\nsymbols_to_isolate","f51c2dce":"# Note : Next comes the next trick. Instead of using an inefficient loop of replace we use translate. \n\n\nisolate_dict = {ord(c):f' {c} ' for c in symbols_to_isolate}\nremove_dict = {ord(c):f'' for c in symbols_to_delete}\n\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x","8fa30df5":"#So lets apply that function to our text and reasses the coverage\n\ntrain_df['all_text'] = train_df['all_text'].progress_apply(lambda x:handle_punctuation(x))\ntest_df['all_text'] = test_df['all_text'].progress_apply(lambda x:handle_punctuation(x))","a91201fd":"vocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","a31e2c85":"# Lets apply pre-processing before we tokenize the words\n\nfor index, row in train_df['all_text'].iteritems():\n    train_df['all_text'][index] = pre_process(row)\nfor index, row in test_df['all_text'].iteritems():\n    test_df['all_text'][index] = pre_process(row)","ffbcf0cb":"from nltk.tokenize.treebank import TreebankWordTokenizer\ntokenizer = TreebankWordTokenizer()","e12cf5b4":"def handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x\n","c7bc3868":"train_df['all_text'] = train_df['all_text'].progress_apply(lambda x:handle_contractions(x))\ntest_df['all_text'] = test_df['all_text'].progress_apply(lambda x:handle_contractions(x))","becc8517":"# Lets check after we have tokenize\n\nvocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:20]","77384de6":"def fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x","45734e64":"train_df['all_text'] = train_df['all_text'].progress_apply(lambda x:fix_quote(x.split()))\ntest_df['all_text'] = test_df['all_text'].progress_apply(lambda x:fix_quote(x.split()))","80810d91":"vocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","33ed6fbf":"# Lets also check test data has equal coverage\nvocab = build_vocab(list(test_df['all_text'].apply(lambda x:x.split())),verbose=False)\noov = check_coverage(vocab,glove_embeddings)\noov[:10]","1aea474c":"tic = time.time()\ncrawl_embeddings = load_embeddings(CRAWL_EMBEDDING_PATH)\nprint(f'loaded {len(glove_embeddings)} word vectors in {time.time()-tic}s')","f3b2bb7b":"vocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,crawl_embeddings)\noov[:20]","4ef0986f":"punctuation = '_`'\n\ntrain_df['all_text'] = train_df['all_text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\ntest_df['all_text'] = test_df['all_text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))","e40496d1":"#Lets check the embeddings now\n\nvocab = build_vocab(list(train_df['all_text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,crawl_embeddings)\noov[:10]","d54a0967":"X = train_df['all_text']\ny_columns = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n#y = train_df['question_asker_intent_understanding']\ny = train_df[y_columns]\n#y = train_df['question_fact_seeking']\ntest_pred = test_df['all_text']\n","cb427d5c":"train_df[y_columns].head(n=10)","a8bcb16b":"MAX_LEN = 100\nmax_features = 500000","8078b970":"\n# Its really important that you intitialize the keras tokenizer correctly. Per default it does lower case and removes a lot of symbols. We want neither of that!\n\ntokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","5b37fca4":"tokenizer.fit_on_texts(list(X) + list(test_pred))","63f8d893":"import numpy as np\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nembedding_matrix.shape\n\nimport gc\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","cd09109e":"X = tokenizer.texts_to_sequences(X)\ntest_pred = tokenizer.texts_to_sequences(test_pred)","6931ef6d":"X = sequence.pad_sequences(X, maxlen=MAX_LEN)\ntest_pred = sequence.pad_sequences(test_pred, maxlen=MAX_LEN)","86ffaaab":"checkpoint_predictions = []\nweights = []","36fae890":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, Dropout, add, concatenate,Flatten,Lambda\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D,PReLU,LSTM\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.models import Sequential\nfrom keras.preprocessing import text, sequence\nfrom keras import regularizers\nimport keras\nimport tensorflow as tf\nimport keras.backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.engine.topology import Layer\nimport tensorflow_hub as hub\nfrom keras.layers.normalization import BatchNormalization","b5dced20":"#### testing delete","c829fd4d":"X_train , X_val, y_train  , y_val = train_test_split(X , \n                                                     y.values , \n                                                     #stratify = y.values, \n                                                     train_size = 0.8,\n                                                     random_state = 100)","09d8e63a":"y_train.shape","257c1635":"embedding_matrix.shape","ea5f6e92":"len(X_train[1])","0da7f5fa":"from scipy.stats import spearmanr, rankdata\nfrom keras.callbacks import Callback\n# Compatible with tensorflow backend\nclass SpearmanRhoCallback(Callback):\n    def __init__(self, training_data, validation_data, patience, model_name):\n        self.x = training_data[0]\n        self.y = training_data[1]\n        self.X_val = validation_data[0]\n        self.y_val = validation_data[1]\n        \n        self.patience = patience\n        self.value = -1\n        self.bad_epochs = 0\n        self.model_name = model_name\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_pred_val = self.model.predict(self.X_val)\n        ind = 0\n        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n        if rho_val >= self.value:\n            self.value = rho_val\n        else:\n            self.bad_epochs += 1\n        if self.bad_epochs >= self.patience:\n            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n            self.model.stop_training = True\n            #self.model.save_weights(self.model_name)\n        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n        return rho_val\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return","637136e1":"maxlen = 100\nvocab_size = len(tokenizer.word_index) + 1\nembedding_dim = 100\nnum_class = len(np.unique(y))\ndef build_model(embedding_matrix, num_aux_targets):\n        model = tf.keras.models.Sequential([\n        tf.keras.layers.Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Conv1D(150, 5, activation='relu'),\n        tf.keras.layers.LSTM(128, return_sequences=True),\n        tf.keras.layers.GlobalMaxPooling1D(),\n        tf.keras.layers.Dense(units=120,activation='relu'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(30, activation='sigmoid')])\n        model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])\n        print(model.summary())\n        return model                         ","d0c3ece6":"from keras.callbacks import EarlyStopping \nes = EarlyStopping(monitor='val_loss', mode ='min' ,verbose =1,patience=0.1)\nNUM_MODELS =2\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix,1)\n    for global_epoch in range(200):\n         model.fit(X_train,\n                   y_train,\n                   epochs=200,\n                   verbose=1,\n                   batch_size = 50,\n                   validation_data=(X_val, y_val),\n                   class_weight =\"temporal\",#class_weights,\n                   callbacks=[SpearmanRhoCallback(training_data=(X_train, y_train), validation_data=(X_val, y_val),\n                                       patience=5, model_name=f'pre_trained.hdf5'),\n                              LearningRateScheduler(lambda epoch: 1e-3 * (0.4 ** global_epoch)),\n                       es\n                   ]\n                  )\n                   \n    checkpoint_predictions.append(model.predict(test_pred))\n    weights.append(3 ** global_epoch)","5bd67715":"score = model.evaluate(X_val, y_val, verbose=1)\nprint(\"Test Score:\", score[0])\nprint(\"Test Accuracy:\", score[1])","240ff317":"predictions = np.average(checkpoint_predictions,  axis=0) ","e9d3036d":"predictions","ccc071f2":"sample_submission_df.iloc[:, 1:] = predictions","c72b1c3d":"sample_submission_df.head(n=6)","6040e343":"sample_submission_df.to_csv(\"submission.csv\", index= False)","617dd654":"To handle special characters and emoji\u2019s, we will use an embedding\nOne of the thing to note is that your embeddings might contains few special characters embeddings which if you delete then will lost information.\n\nSo lets try and check how many embedding\u2019s we have coverage","73dcab32":"Checking CRAWL embeddings Coverage"}}