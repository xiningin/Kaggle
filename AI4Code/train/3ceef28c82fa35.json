{"cell_type":{"69970027":"code","e01546e8":"code","d9d00ce8":"code","a31f197f":"code","8b2dc7bc":"code","999f2500":"code","9d86ff8c":"code","5d4202f9":"code","93e0e7bc":"code","caf12708":"code","6e9965fa":"code","860ced96":"code","f4f8634a":"code","814ee68e":"code","977174c7":"code","2c02df4d":"code","d011f612":"code","5740b2b6":"code","846e69bf":"code","bb34e214":"code","cd8a551d":"code","6af09d07":"code","bd0cca2b":"code","03fb416c":"code","1b976ada":"code","6942f8e4":"code","d609e3f2":"code","07c5d3b5":"code","41af68b2":"code","46b08317":"code","00d0a40b":"code","3edc4d82":"code","f4d05b29":"code","6a44154b":"code","9321d5f0":"code","4890c8c5":"code","23e9e2ed":"markdown","52c3aa05":"markdown","219687cd":"markdown","6031a6b9":"markdown","b02b3ea4":"markdown","9625e67a":"markdown","c97c5acc":"markdown","a151cf34":"markdown","9077f489":"markdown","bdc13103":"markdown","8f175efb":"markdown","cb54597c":"markdown","0a12812e":"markdown"},"source":{"69970027":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e01546e8":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport re\nimport collections\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import svm\nfrom nltk.tokenize import word_tokenize\nfrom time import time\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error","d9d00ce8":"train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')","a31f197f":"print(train.shape,test.shape)","8b2dc7bc":"train.head(3)","999f2500":"test.head(3)","9d86ff8c":"#Check if there'is null values\ntrain.isnull().sum()","5d4202f9":"# Target distibution\nplt.figure(figsize=(10,4))\n#plt.xlim(train['target'].min(),train['target'].max())\nsns.kdeplot(train['target'],shade=True)\nplt.show()","93e0e7bc":"# Numbers of word for each sapmle in train & test data\ntrain['text_length'] = train.excerpt.apply(lambda x: len(x.split()))\ntest['text_length'] = test.excerpt.apply(lambda x: len(x.split()))","caf12708":"train['text_length'].describe()","6e9965fa":"test['text_length'].describe()","860ced96":"def plot_word_count(df, data_name):\n  sns.distplot(df['text_length'].values)\n  plt.title(f'Sequence char count: {data_name}')\n  plt.grid(True)","f4f8634a":"#ig = plt.figure(figsize=(16,6))\n#plt.hist(train[\"text_length\"], bins = 30)\n#plt.show()\nplt.subplot(1, 2, 1)\nplot_word_count(train, 'Train')\n\nplt.subplot(1, 2, 2)\nplot_word_count(test, 'Test')\n\nplt.subplots_adjust(right=3.0)\nplt.show()","814ee68e":"# collecting all words in single list\nlist_= []\nfor i in train.excerpt:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)","977174c7":"len(vocabulary)","2c02df4d":"def create_corpus(df,column_name):\n    corpus=[]\n    \n    for x in df[column_name].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","d011f612":"#most frequent 30 words  \nallWords=create_corpus(train,'excerpt')\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(14,10))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:30]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","5740b2b6":"#List of punctuations and we will remove them from our corpus\nstring.punctuation","846e69bf":"#for  example\ntext='hey # how are !you doing ?'\n\"\".join([char for char in text if char not in string.punctuation])","bb34e214":"#for example \ntext='hey 4 look 333 at me0'\nre.sub('[0-9]', '', text)","cd8a551d":"#list of stopwords\nstopwords.words('english')","6af09d07":"#for example\ntext='hey this is me and I am here to help you  '\ntokens = word_tokenize(text)\ntokens=[word for word in tokens if word not in stopwords.words('english')]\n' '.join(tokens)","bd0cca2b":"pstem = PorterStemmer()\ndef clean_text(text):\n    text= text.lower()\n    text= re.sub('[0-9]', '', text)\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    tokens = word_tokenize(text)\n    tokens=[pstem.stem(word) for word in tokens]\n    tokens=[word for word in tokens if word not in stopwords.words('english')]\n    text = ' '.join(tokens)\n    return text","03fb416c":"clean_text(\"hey I am here # ! looks 4 GOOD can't see you!\")","1b976ada":"train[\"clean\"]=train[\"excerpt\"].apply(clean_text)\ntest[\"clean\"]=test[\"excerpt\"].apply(clean_text)","6942f8e4":"#Let's see the effect of cleaning\ntrain[[\"excerpt\",\"clean\"]].head(4)","d609e3f2":"# collecting all words in single list\nlist_= []\nfor i in train.clean:\n    list_ += i\nlist_= ''.join(list_)\nallWords=list_.split()\nvocabulary= set(allWords)\nlen(vocabulary)","07c5d3b5":"#most frequent 30 words after the cleaning of text \nallWords=create_corpus(train,'clean')\nvocabulary= set(allWords)\nvocabulary_list= list(vocabulary)\n\nplt.figure(figsize=(14,10))\ncounter=collections.Counter(allWords)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:30]:\n  x.append(word)\n  y.append(count)\nsns.barplot(x=y,y=x)","41af68b2":"tfidf = TfidfVectorizer(sublinear_tf=True, norm='l2',ngram_range=(1,1))\nfeatures = tfidf.fit_transform(train.clean).toarray()\nfeatures.shape","46b08317":"features_test = tfidf.transform(test.clean).toarray()","00d0a40b":"params = {'metric': 'rmse','random_state': 48,'n_estimators': 20000,'reg_alpha': 0.0010819683712588644,\n          'reg_lambda': 0.004760428916800031, 'colsample_bytree': 0.4, 'subsample': 0.8, 'learning_rate': 0.01,\n          'max_depth': 100, 'num_leaves': 39, 'min_child_samples': 12, 'cat_smooth': 67}","3edc4d82":"preds = np.zeros(test.shape[0])\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nrmse=[]  # list contains rmse for each fold\nn=0\nfor trn_idx, test_idx in kf.split(features,train['target']):\n    X_tr,X_val=features[trn_idx],features[test_idx]\n    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n    model = LGBMRegressor(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(features_test)\/kf.n_splits\n    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n    print(n+1,rmse[n])\n    n+=1","f4d05b29":"np.mean(rmse)","6a44154b":"# Prediction distibution\nplt.figure(figsize=(10,4))\nsns.kdeplot(preds,shade=True)\nplt.show()","9321d5f0":"sub['target']=preds\nsub.to_csv('submission.csv', index=False)","4890c8c5":"sub","23e9e2ed":"## Now let's Build a function that clean our data","52c3aa05":"# Please If you find this kernel helpful, upvote it to help others see it \ud83d\ude0a","219687cd":"# Submission","6031a6b9":"* we reduced our data from 56087 unique words to 22073","b02b3ea4":"# Hi kagglers \ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f and Welcome to this new competition!","9625e67a":"* I just added lower function in order to lowercase all words and stemming","c97c5acc":"# Let's use LightGbm Regressor","a151cf34":"# Let's start with EDA","9077f489":"## 2-Removing Numbers","bdc13103":"## 1-Removing Punctuations","8f175efb":" * We have 56087 different words in our train data","cb54597c":"## 3-Removing Stopwords","0a12812e":"# Data Cleaning"}}