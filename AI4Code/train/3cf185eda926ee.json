{"cell_type":{"22c838e9":"code","906df141":"code","07a6ac55":"code","f1a76784":"code","4ec5206e":"code","61f8ed93":"code","f3d697a5":"code","5e209791":"code","1ec71e49":"code","40ae1d84":"code","0bf47d27":"code","d136be41":"code","85d76f57":"code","f7a26a87":"code","5b3d1687":"code","df2e514d":"code","8cb0b7e8":"code","e955ad33":"code","c6af46f0":"code","6aa34250":"code","dae5e59a":"code","75137a2c":"code","9693496c":"code","98aa4161":"code","3ff1f1e6":"code","1f7d539d":"code","f37112b6":"code","b9250d6f":"code","90b1b93f":"code","2e0fb61f":"code","d7152bf7":"code","8c06d70b":"code","62187a6e":"code","d837632a":"code","4bd9591f":"code","babf3a48":"code","a3a83ee9":"code","766fc300":"code","8c368f21":"code","69d73304":"markdown","ef04f9ad":"markdown","8c4a96d2":"markdown","f7b7a44c":"markdown","ca72b2a1":"markdown","db3bbe8e":"markdown","5eb5b683":"markdown","29ed3d73":"markdown","1b706801":"markdown","751239b1":"markdown","6197ea59":"markdown","7ef5f99f":"markdown","47e65f45":"markdown"},"source":{"22c838e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\n#from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom sklearn.ensemble import RandomForestRegressor\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","906df141":"df_train = pd.read_csv('..\/input\/champs-scalar-coupling\/train.csv')\n\ndf_test = pd.read_csv('..\/input\/champs-scalar-coupling\/test.csv')\nconstant_data = pd.read_csv('..\/input\/champs-scalar-coupling\/scalar_coupling_contributions.csv')\nstructures = pd.read_csv('..\/input\/champs-scalar-coupling\/structures.csv')","07a6ac55":"ids = df_test[\"id\"]","f1a76784":"df_train.info()","4ec5206e":"df_train.type.unique()","61f8ed93":"from tqdm import tqdm_notebook as tqdm\natomic_radius = {'H':0.38, 'C':0.77, 'N':0.75, 'O':0.73, 'F':0.71} # Without fudge factor\n\nfudge_factor = 0.05\natomic_radius = {k:v + fudge_factor for k,v in atomic_radius.items()}\nprint(atomic_radius)\n\nelectronegativity = {'H':2.2, 'C':2.55, 'N':3.04, 'O':3.44, 'F':3.98}\n\n#structures = pd.read_csv(structures, dtype={'atom_index':np.int8})\n\natoms = structures['atom'].values\natoms_en = [electronegativity[x] for x in tqdm(atoms)]\natoms_rad = [atomic_radius[x] for x in tqdm(atoms)]\n\nstructures['EN'] = atoms_en\nstructures['rad'] = atoms_rad\n\ndisplay(structures.head())\n","f3d697a5":"i_atom = structures['atom_index'].values\np = structures[['x', 'y', 'z']].values\np_compare = p\nm = structures['molecule_name'].values\nm_compare = m\nr = structures['rad'].values\nr_compare = r\n\nsource_row = np.arange(len(structures))\nmax_atoms = 28\n\nbonds = np.zeros((len(structures)+1, max_atoms+1), dtype=np.int8)\nbond_dists = np.zeros((len(structures)+1, max_atoms+1), dtype=np.float32)\n\nprint('Calculating the bonds')\n\nfor i in tqdm(range(max_atoms-1)):\n    p_compare = np.roll(p_compare, -1, axis=0)\n    m_compare = np.roll(m_compare, -1, axis=0)\n    r_compare = np.roll(r_compare, -1, axis=0)\n    \n    mask = np.where(m == m_compare, 1, 0) #Are we still comparing atoms in the same molecule?\n    dists = np.linalg.norm(p - p_compare, axis=1) * mask\n    r_bond = r + r_compare\n    \n    bond = np.where(np.logical_and(dists > 0.0001, dists < r_bond), 1, 0)\n    \n    source_row = source_row\n    target_row = source_row + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n    target_row = np.where(np.logical_or(target_row > len(structures), mask==0), len(structures), target_row) #If invalid target, write to dummy row\n    \n    source_atom = i_atom\n    target_atom = i_atom + i + 1 #Note: Will be out of bounds of bonds array for some values of i\n    target_atom = np.where(np.logical_or(target_atom > max_atoms, mask==0), max_atoms, target_atom) #If invalid target, write to dummy col\n    \n    bonds[(source_row, target_atom)] = bond\n    bonds[(target_row, source_atom)] = bond\n    bond_dists[(source_row, target_atom)] = dists\n    bond_dists[(target_row, source_atom)] = dists\n\nbonds = np.delete(bonds, axis=0, obj=-1) #Delete dummy row\nbonds = np.delete(bonds, axis=1, obj=-1) #Delete dummy col\nbond_dists = np.delete(bond_dists, axis=0, obj=-1) #Delete dummy row\nbond_dists = np.delete(bond_dists, axis=1, obj=-1) #Delete dummy col\n\nprint('Counting and condensing bonds')\n\nbonds_numeric = [[i for i,x in enumerate(row) if x] for row in tqdm(bonds)]\nbond_lengths = [[dist for i,dist in enumerate(row) if i in bonds_numeric[j]] for j,row in enumerate(tqdm(bond_dists))]\nbond_lengths_mean = [ np.mean(x) for x in bond_lengths]\nn_bonds = [len(x) for x in bonds_numeric]\n\n#bond_data = {'bond_' + str(i):col for i, col in enumerate(np.transpose(bonds))}\n#bond_data.update({'bonds_numeric':bonds_numeric, 'n_bonds':n_bonds})\n\nbond_data = {'n_bonds':n_bonds, 'bond_lengths_mean': bond_lengths_mean }\nbond_df = pd.DataFrame(bond_data)\nstructures = structures.join(bond_df)\ndisplay(structures.head(20))","5e209791":"del bond_data\ndel bond_df\ndel bonds_numeric\ndel bond_lengths\ndel bond_lengths_mean\ndel i_atom\ndel p, p_compare\ndel m, m_compare\ndel r, r_compare\ndel source_row\ndel bonds\ndel dists\ndel atoms\ndel atoms_en\ndel atoms_rad\n","1ec71e49":"y = df_train['scalar_coupling_constant']","40ae1d84":"len_train = df_train.shape[0]\ncombined = pd.concat([df_train,df_test],sort=False)","0bf47d27":"structures['molecule_name_atom'] = structures['molecule_name'] + \"_\" + structures['atom_index'].apply(str)\ncombined['molecule_name_atom0'] = combined['molecule_name'] + \"_\" + combined['atom_index_0'].apply(str)\ncombined_f = pd.merge(combined, structures, how = 'left', left_on='molecule_name_atom0', right_on='molecule_name_atom')\n#combined_f","d136be41":"combined_f.drop(\"molecule_name_atom0\", axis=1, inplace=True)\ncombined_f.drop(\"molecule_name_y\", axis=1, inplace=True)\ncombined_f.drop(\"atom_index\", axis=1, inplace=True)\ncombined_f.drop(\"molecule_name_atom\", axis=1, inplace=True)\n\ncombined_f.rename(columns = {\"x\":\"x0\",\n                \"EN\":\"EN0\",\n                \"rad\":\"rad0\",\n                \"n_bonds\":\"n_bonds0\",\n                \"bond_lengths_mean\":\"bond_lengths_mean0\",\n                 \"y\":\"y0\",\n                  \"z\":\"z0\",\n                  \"atom\":\"atom0\",\n                  \"molecule_name_x\":\"molecule_name\"\n                 }, inplace=True)\n#combined_f","85d76f57":"combined_f['molecule_name_atom1'] = combined_f['molecule_name'] + \"_\" + combined_f['atom_index_1'].apply(str)\ncombined = pd.merge(combined_f, structures, how = 'left', left_on='molecule_name_atom1', right_on='molecule_name_atom')\n#combined","f7a26a87":"combined.drop(\"molecule_name_atom1\", axis=1, inplace=True)\ncombined.drop(\"molecule_name_y\", axis=1, inplace=True)\ncombined.drop(\"atom_index\", axis=1, inplace=True)\ncombined.drop(\"molecule_name_atom\", axis=1, inplace=True)\n\ncombined.rename(columns = {\"x\":\"x1\",\n                 \"EN\":\"EN1\",\n                \"rad\":\"rad1\",\n                \"n_bonds\":\"n_bonds1\",\n                \"bond_lengths_mean\":\"bond_lengths_mean1\",\n                 \"y\":\"y1\",\n                  \"z\":\"z1\",\n                  \"atom\":\"atom1\",\n                  \"molecule_name_x\":\"molecule_name\"\n                 }, inplace=True)\ncombined","5b3d1687":"combined.info()\ndel combined_f","df2e514d":"combined[\"atom1\"].unique()","8cb0b7e8":"pos_0 = combined[['x0', 'y0', 'z0']].values\npos_1 = combined[['x1', 'y1', 'z1']].values\n\ncombined['dist'] = np.linalg.norm(pos_0 - pos_1, axis=1)\ncombined['dist_x'] = (combined[\"x0\"] - combined[\"x1\"])**2\ncombined['dist_y'] = (combined[\"y0\"] - combined[\"y1\"])**2\ncombined['dist_z'] = (combined[\"z0\"] - combined[\"z1\"])**2\n\n\ncombined[\"atom_indexes\"] = combined[\"atom_index_0\"] + combined[\"atom_index_1\"]\ncombined[\"distance^2\"] = (combined[\"dist\"])**2\ncombined[\"distance_sqrt\"] = np.sqrt(combined[\"dist\"])\ncombined[\"1\/distance^3\"] = 1 \/ (combined[\"dist\"] + 1)**3\ncombined[\"distance^3\"] = (combined[\"dist\"])**3\ncombined['type_0'] = combined['type'].apply(lambda x: x[0])","e955ad33":"combined['num_molecule_bonds'] = combined.groupby('molecule_name')['id'].transform('count')","c6af46f0":"combined['dist_mean'] = combined.groupby('molecule_name')['dist'].transform('mean')\ncombined['dist_min'] = combined.groupby('molecule_name')['dist'].transform('min')\ncombined['dist_max'] = combined.groupby('molecule_name')['dist'].transform('max')\ncombined['dist_std'] = combined.groupby('molecule_name')['dist'].transform('std')\n\ncombined['atom_0_num_bonds'] = combined.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\ncombined['atom_1_num_bonds'] = combined.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n\ncombined[\"bf_dist_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[\"dist\"].transform('mean')\ncombined[\"bf_dist_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[\"dist\"].transform('max')\ncombined[\"bf_dist_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[\"dist\"].transform('min')\ncombined[\"bf_dist_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[\"dist\"].transform('std')\n\ncombined[f\"bf_dist_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[\"dist\"].transform('mean')\ncombined[f\"bf_dist_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[\"dist\"].transform('max')\ncombined[f\"bf_dist_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[\"dist\"].transform('min')\ncombined[f\"bf_dist_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[\"dist\"].transform('std')\n        \ncombined[f\"bf_dist_grouped_atom\"] = combined.groupby(['molecule_name', 'atom1'])[\"dist\"].transform('mean')\ncombined[f\"bf_dist_grouped_atom1\"] = combined.groupby(['molecule_name', 'atom1'])[\"dist\"].transform('max')\ncombined[f\"bf_dist_grouped_atom1\"] = combined.groupby(['molecule_name', 'atom1'])[\"dist\"].transform('min')\ncombined[f\"bf_dist_grouped_atom1\"] = combined.groupby(['molecule_name', 'atom1'])[\"dist\"].transform('std')\n        ","6aa34250":"combined['dist\/maxdist'] = combined['dist'] \/ combined['dist_max']\ncombined['ENs\/dist'] = (combined['EN0'] + combined['EN1']) \/ combined['dist']\ncombined['rads\/dist'] = (combined['rad0'] + combined['rad1']) \/ combined['dist']\ncombined['ENS'] = combined['EN0'] + combined['EN1']\n#combined['atom_num_bonds'] = combined['atom1'] + str(combined['n_bonds1'])","dae5e59a":"numerical = [\"EN0\", \"rad0\", \"bond_lengths_mean0\", \"EN1\", \"rad1\", \"bond_lengths_mean1\", \"1\/distance^3\", \"ENS\"]\nfor col in numerical:\n    for col2 in numerical:\n        if col != col2:\n            combined[f\"bf_{col}_\/_{col2}\"] = combined[col] \/ combined[col2]\n            combined[f\"bf_{col}_*_{col2}\"] = combined[col] * combined[col2]\n            combined[f\"bf_{col}_+_{col2}\"] = combined[col] + combined[col2]\n            combined[f\"bf_{col}_-_{col2}\"] = combined[col] - combined[col2]\n            \n        combined[f\"bf_{col}_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[col].transform('mean')\n        combined[f\"bf_{col}_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[col].transform('max')\n        combined[f\"bf_{col}_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[col].transform('min')\n        combined[f\"bf_{col}_grouped_type\"] = combined.groupby(['molecule_name', 'type'])[col].transform('std')\n\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[col].transform('mean')\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[col].transform('max')\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[col].transform('min')\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'n_bonds1'])[col].transform('std')\n        \n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'atom1'])[col].transform('mean')\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'atom1'])[col].transform('max')\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'atom1'])[col].transform('min')\n        combined[f\"bf_{col}_grouped_n_bonds\"] = combined.groupby(['molecule_name', 'atom1'])[col].transform('std')\n        \n\ncombined.info()\n","75137a2c":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max and c_prec == np.finfo(np.float16).precision:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","9693496c":"combined.drop(\"id\", axis=1, inplace=True)\ncombined.drop(\"n_bonds0\", axis=1, inplace=True)\ncombined.drop(\"atom0\", axis=1, inplace=True)\ncombined.drop(\"molecule_name\", axis=1, inplace=True)\n\ncombined[\"atom_indexes\"] = combined[\"atom_indexes\"].apply(str)\ncombined.drop(\"atom_index_0\", axis=1, inplace=True)\ncombined.drop(\"atom_index_1\", axis=1, inplace=True)\ncombined.drop(\"scalar_coupling_constant\", axis=1, inplace=True)\n#print(combined.head())\n#combined = reduce_mem_usage(combined)\nprint(combined.head())\ncombined=pd.get_dummies(combined)","98aa4161":"\ndf_train=combined[:len_train]\ndf_test=combined[len_train:]\ndel len_train\ndel combined\ndel structures","3ff1f1e6":"x = df_train\n#x = np.array(x)\n#x = x.reshape((-1, 1))\ny_fc = constant_data['fc']\n#y_sd = constant_data['sd']\n#y_pso = constant_data['pso']\n#y_dso = constant_data['dso']\nx_predict = df_test\n#x_predict = np.array(x_predict)\n#x_predict = x_predict.reshape((-1,1))\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, train_size=0.8)\n\n#df_train.info()\ndel constant_data","1f7d539d":"df_train.head()","f37112b6":"df_train_1JHN = df_train[df_train[\"type_1JHN\"] == 1]\ny_fc_1JHN = y_fc[df_train[\"type_1JHN\"] == 1]\ny_1JHN = y[df_train[\"type_1JHN\"] == 1]\n\ndf_train_1JHC_1 = df_train[(df_train[\"type_1JHC\"] == 1) & (df_train[\"dist\"] > 1.065)]\ny_fc_1JHC_1 = y_fc[(df_train[\"type_1JHC\"] == 1) & (df_train[\"dist\"] > 1.065)]\ny_1JHC_1 = y[(df_train[\"type_1JHC\"] == 1) & (df_train[\"dist\"] > 1.065)]\n\ndf_train_1JHC_2 = df_train[(df_train[\"type_1JHC\"] == 1) & (df_train[\"dist\"] <= 1.065)]\ny_fc_1JHC_2 = y_fc[(df_train[\"type_1JHC\"] == 1) & (df_train[\"dist\"] <= 1.065)]\ny_1JHC_2 = y[(df_train[\"type_1JHC\"] == 1) & (df_train[\"dist\"] <= 1.065)]\n\ndf_train_2JHC = df_train[df_train[\"type_2JHC\"] == 1]\ny_fc_2JHC = y_fc[df_train[\"type_2JHC\"] == 1]\ny_2JHC = y[df_train[\"type_2JHC\"] == 1]\n\ndf_train_3JHH = df_train[df_train[\"type_3JHH\"] == 1]\ny_fc_3JHH = y_fc[df_train[\"type_3JHH\"] == 1]\ny_3JHH = y[df_train[\"type_3JHH\"] == 1]\n\ndf_train_3JHC = df_train[df_train[\"type_3JHC\"] == 1]\ny_fc_3JHC = y_fc[df_train[\"type_3JHC\"] == 1]\ny_3JHC = y[df_train[\"type_3JHC\"] == 1]\n\ndf_train_2JHH = df_train[df_train[\"type_2JHH\"] == 1]\ny_fc_2JHH = y_fc[df_train[\"type_2JHH\"] == 1]\ny_2JHH = y[df_train[\"type_2JHH\"] == 1]\n\ndf_train_3JHN = df_train[df_train[\"type_3JHN\"] == 1]\ny_fc_3JHN = y_fc[df_train[\"type_3JHN\"] == 1]\ny_3JHN = y[df_train[\"type_3JHN\"] == 1]\n\ndf_train_2JHN = df_train[df_train[\"type_2JHN\"] == 1]\ny_fc_2JHN = y_fc[df_train[\"type_2JHN\"] == 1]\ny_2JHN = y[df_train[\"type_2JHN\"] == 1]\n","b9250d6f":"df_train_1JHN.info()","90b1b93f":"df_train_1JHN.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_1JHC_1.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_1JHC_2.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_2JHC.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_3JHH.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_3JHC.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_2JHH.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_3JHN.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_train_2JHN.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\n","2e0fb61f":"df_test.insert(0, 'ID', range(1, 1 + len(df_test)))\n\n\ndf_test_1JHN = df_test[df_test[\"type_1JHN\"] == 1]\n\ndf_test_1JHC_1 = df_test[(df_test[\"type_1JHC\"] == 1) & (df_test[\"dist\"] > 1.065)]\n\ndf_test_1JHC_2 = df_test[(df_test[\"type_1JHC\"] == 1) & (df_test[\"dist\"] <= 1.065)]\n\ndf_test_2JHC = df_test[df_test[\"type_2JHC\"] == 1]\n\ndf_test_3JHH = df_test[df_test[\"type_3JHH\"] == 1]\n\ndf_test_3JHC = df_test[df_test[\"type_3JHC\"] == 1]\n\ndf_test_2JHH = df_test[df_test[\"type_2JHH\"] == 1]\n\ndf_test_3JHN = df_test[df_test[\"type_3JHN\"] == 1]\n\ndf_test_2JHN = df_test[df_test[\"type_2JHN\"] == 1]\n","d7152bf7":"df_test_1JHN.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_1JHC_1.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_1JHC_2.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_2JHC.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_3JHH.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_3JHC.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_2JHH.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_3JHN.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)\ndf_test_2JHN.drop([\"type_1JHC\", \"type_1JHN\", \"type_2JHC\", \"type_2JHH\", \"type_2JHN\", \"type_3JHC\", \"type_3JHH\", \"type_3JHN\"], axis=1, inplace=True)","8c06d70b":"def train_type_wise(train_data, y_fc_type, y_type):\n    d_train_fc = lgb.Dataset(train_data, label=y_fc_type)\n    params = {}\n    params['learning_rate'] = 0.2\n    params['boosting_type'] = 'gbdt'\n    params['objective'] = 'regression'\n    params['metric'] = 'mae'\n    params['num_leaves'] = 10\n    params['min_data'] = 50\n    params['max_depth'] = 10\n\n    clf_fc = lgb.train(params, d_train_fc, 2000)\n    \n    del d_train_fc\n    del params\n    \n    train_data[\"fc\"] = clf_fc.predict(train_data)\n    #del clf_fc\n    \n    \n    d_train = lgb.Dataset(train_data, label=y_type)\n    params = {'num_leaves': 128,\n          'min_child_samples': 79,\n          'objective': 'regression',\n          'max_depth': 9,\n          'learning_rate': 0.2,\n          \"boosting_type\": \"gbdt\",\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.1,\n          'reg_lambda': 0.3,\n         }\n    cl = lgb.train(params, d_train, 2000)\n    \n    train_data[\"ridge\"] = cl.predict(train_data)\n\n    del d_train\n    del params\n    #del cl\n\n    \n    d_train = lgb.Dataset(train_data, label=y_type)\n    params = {\n                'boosting_type': 'gbdt',\n                'objective': 'regression',\n                'metric': 'mae',\n                'learning_rate': 0.2,\n                'num_leaves': 20, \n                'reg_alpha': 0.5, \n                'reg_lambda': 0.5, \n                'nthread': 4, \n                'device': 'cpu',\n                'min_child_samples': 45\n            }\n\n    clf = lgb.train(params, d_train, 2000)\n    return clf_fc, cl, clf\n","62187a6e":"model_1JHN_fc, model_1JHN_1, model_1JHN_2 = train_type_wise(df_train_1JHN, y_fc_1JHN, y_1JHN)\n\nmodel_1JHC_1_fc, model_1JHC_1_1, model_1JHC_1_2 = train_type_wise(df_train_1JHC_1, y_fc_1JHC_1, y_1JHC_1)\n\nmodel_1JHC_2_fc, model_1JHC_2_1, model_1JHC_2_2 = train_type_wise(df_train_1JHC_2, y_fc_1JHC_2, y_1JHC_2)\n\nmodel_2JHC_fc, model_2JHC_1, model_2JHC_2 = train_type_wise(df_train_2JHC, y_fc_2JHC, y_2JHC)\n\nmodel_3JHH_fc, model_3JHH_1, model_3JHH_2 = train_type_wise(df_train_3JHH, y_fc_3JHH, y_3JHH)\n\nmodel_3JHC_fc, model_3JHC_1, model_3JHC_2 = train_type_wise(df_train_3JHC, y_fc_3JHC, y_3JHC)\n\nmodel_2JHH_fc, model_2JHH_1, model_2JHH_2 = train_type_wise(df_train_2JHH, y_fc_2JHH, y_2JHH)\n\nmodel_3JHN_fc, model_3JHN_1, model_3JHN_2 = train_type_wise(df_train_3JHN, y_fc_3JHN, y_3JHN)\n\nmodel_2JHN_fc, model_2JHN_1, model_2JHN_2 = train_type_wise(df_train_2JHN, y_fc_2JHN, y_2JHN)","d837632a":"def predict_type_wise(test_data, model_fc, model_1, model_2):\n    x_pred = test_data.drop(\"ID\", axis=1)\n    test_data[\"fc\"] = model_fc.predict(x_pred)\n    x_pred = test_data.drop(\"ID\", axis=1)\n    test_data[\"ridge\"] = model_1.predict(x_pred)\n    x_pred = test_data.drop(\"ID\", axis=1)\n    test_data[\"predictions\"] = model_2.predict(x_pred)\n    return test_data","4bd9591f":"predictions_1JHN = predict_type_wise(df_test_1JHN, model_1JHN_fc, model_1JHN_1, model_1JHN_2)\n\npredictions_1JHC_1 = predict_type_wise(df_test_1JHC_1, model_1JHC_1_fc, model_1JHC_1_1, model_1JHC_1_2)\n\npredictions_1JHC_2 = predict_type_wise(df_test_1JHC_2, model_1JHC_2_fc, model_1JHC_2_1, model_1JHC_2_2)\n\npredictions_2JHC = predict_type_wise(df_test_2JHC, model_2JHC_fc, model_2JHC_1, model_2JHC_2)\n\npredictions_3JHH = predict_type_wise(df_test_3JHH, model_3JHH_fc, model_3JHH_1, model_3JHH_2)\n\npredictions_3JHC = predict_type_wise(df_test_3JHC, model_3JHC_fc, model_3JHC_1, model_3JHC_2)\n\npredictions_2JHH = predict_type_wise(df_test_2JHH, model_2JHH_fc, model_2JHH_1, model_2JHH_2)\n\npredictions_3JHN = predict_type_wise(df_test_3JHN, model_3JHN_fc, model_3JHN_1, model_3JHN_2)\n\npredictions_2JHN = predict_type_wise(df_test_2JHN, model_2JHN_fc, model_2JHN_1, model_2JHN_2)\n","babf3a48":"predicted_data = pd.concat([predictions_1JHN, predictions_1JHC_1, predictions_1JHC_2, predictions_2JHC, predictions_3JHH, predictions_3JHC, predictions_2JHH, predictions_3JHN, predictions_2JHN], ignore_index=True)\n\npredicted_data","a3a83ee9":"final_data = pd.merge(df_test, predicted_data, how='left', left_on=\"ID\", right_on=\"ID\")","766fc300":"final_data.head()","8c368f21":"#prediction_array = np.array(predictions_scalar)\n#temp = prediction_array.flat\n#predictions = list(temp)\n#print(predictions)\npredictions = final_data[\"predictions\"]\nmy_sub = pd.DataFrame({'id':ids, 'scalar_coupling_constant':predictions})\nmy_sub.to_csv('submission.csv', index = False)","69d73304":"Few features taken from the brute force engineering notebook.","ef04f9ad":"Calculating bond data, taken from another notebook.","8c4a96d2":"Applying one-hot encoding for all categorical features.","f7b7a44c":"I have created separate training and testing sets for each type so that it would be easier to make separate models.","ca72b2a1":"Since this was the first time I used lightgbm, I wasn't able to tune any of the parameter and pretty much copied them from other notebooks. I have also not used the QM9 dataset, which I think may be very beneficial.\n\nPlease upvote if you found it helpful. ","db3bbe8e":"Importing data","5eb5b683":"Few variations of the distance feature.","29ed3d73":"\n\n<br>\n<br>\n<br>\n<br>\n<br>\n\n\n\n<b><b><b>MODELS<\/b><\/b><\/b>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n<br>\n\n","1b706801":"I wasn't able to use the reduce_mem_usage method as it transformed the data in an awkward way, which could not be used by the lightgbm model.","751239b1":"So we are making 3 models per type. The first model predicts the fc constant from the scalar_coupling_contributions file. Then the fc predictions make an additional feature for the second model which predicts the scalar coupling constant. The output of this as well forms an additional feature which is used by the final model to make the final predictions. \n\nIdeally, one of the last 2 models should be a different kind of model, like a neural network or maybe even another tree-based model using a different library. However, I was unable to use neural networks successfully.","6197ea59":"In the above code, instead of calculating only the mean, min, max and standard deviation, I have taken a few features and applied the 4 arithmetic operations on them. I was only able to use a few coloumns, since the number of features goes on increasing and is roughly approximated to n!, where n is the number of features in the 'numerical' array.\nI am running this on my personal computer, but if we could add more features, I think the score might go down. ","7ef5f99f":"In this notebook, I have taken insipiration from the brute force engineering notebook. This notebook got a public lb score of -0.56337. However, it has to be run locally.","47e65f45":"Combining structures.csv"}}