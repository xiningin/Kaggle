{"cell_type":{"dc7f2e23":"code","76cd4044":"code","356a2916":"code","4579494b":"code","e05e8c60":"code","f67faf25":"code","5a2cdd8d":"code","be97e4ae":"code","16ed4f82":"code","09c59b1a":"code","bab31c71":"code","823c17aa":"code","7b4087e4":"code","b060cc1a":"code","7c06aaec":"code","73a02a19":"code","cc70fa11":"code","144657fc":"code","8767a47f":"code","0b25f203":"code","921cadc1":"code","46d2aae8":"code","c44601d4":"code","cef04b67":"code","b0902c20":"code","0353703e":"code","04180fa9":"code","347b3bcf":"code","d93742bc":"markdown","02007894":"markdown","000a6983":"markdown","54f4e0c8":"markdown","4e1f13d4":"markdown","e39a3421":"markdown","7a1311b1":"markdown","90df46b3":"markdown","832b5424":"markdown","9dc7cdc3":"markdown","eb4e44ab":"markdown","011511cc":"markdown","b176be77":"markdown","eec758d6":"markdown","43482c3c":"markdown","97ce1d56":"markdown","ac132e44":"markdown","d0b537f7":"markdown","396a1003":"markdown","3bc22c4c":"markdown","fc4cd9e8":"markdown","13f8c04a":"markdown","cdc1b562":"markdown","ce45832e":"markdown","c7a8e46a":"markdown"},"source":{"dc7f2e23":"#importing libraries\nimport pandas as pd # For DataFrame Manipulation\nimport numpy as np #For Data Manipulatoion\nimport matplotlib.pyplot as plt # For Visualization\nimport seaborn as sns #For Visualization\nimport plotly as py  #For Visualization\nimport plotly.graph_objs as go","76cd4044":"## read data from \"multipleChoiceResponses.csv\" file\ndata=pd.read_csv('..\/input\/multipleChoiceResponses.csv', skiprows=[1])","356a2916":"country=data['Q3'].value_counts()\ncountry.sort_index(inplace=True)\n\npy.offline.init_notebook_mode(connected=True)\n\ncountry=pd.DataFrame(country)\ncountry['country']=country.index\ncount = [ dict(\n        type = 'choropleth',\n        locations = country['country'],\n        locationmode='country names',\n        z = country['Q3'],\n        text = country['country'],\n        colorscale = 'Viridis',\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick =False,\n            title = 'Number of Respondents(In Thousands)'),\n      ) ]\nlayout = dict(\n    title = 'Number of Respondent from across the Globe',\n    geo = dict(\n        showframe = True,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\nfig = dict( data=count, layout=layout )\npy.offline.iplot( fig, validate=False, filename='d3-world-map' )","4579494b":"feq=data['Q2'].value_counts()\ntotal=feq.sum(axis=0)\nfeq.sort_index(inplace=True)\nax=feq.plot.bar(figsize=(10, 8))\nplt.title(\"Age wise DS\", fontsize=20)\nplt.xlabel('Age group', fontsize=12)\nplt.ylabel('Number of Respondent', fontsize=12)\nfor i in ax.patches:\n    # get_x pulls left or right; get_height pushes up or down\n    ax.text(i.get_x()-.03, i.get_height()+.5, \\\n            str(round((i.get_height()\/total)*100, 2))+'%', fontsize=10, weight='bold',\n               color='dimgrey')","e05e8c60":"\nfeq=data[['Q2', 'Q3']].rename(columns={'Q2':'Age Group', 'Q3':'Country of Residence'})\nfeq=feq.groupby(['Country of Residence','Age Group'])['Age Group'].size()\nfeq=pd.DataFrame(feq)\nfeq['Country\/Age Group']=feq.index\nfeq.rename(columns={'Age Group':'Count'}, inplace=True)\nfeq['Country'] = feq['Country\/Age Group'].str.get(0)\nfeq['Age Group'] = feq['Country\/Age Group'].str.get(1)\nfeq.drop(['Country\/Age Group'], axis=1, inplace=True)\nfeq1=feq.groupby(['Country'])['Count'].transform(max) == feq['Count']\nfeq=feq[feq1]\nfeq.index = np.arange(0, len(feq))\n\npy.offline.init_notebook_mode(connected=True)\ncount = [ dict(\n        type = 'choropleth',\n        locations = feq['Country'],\n        locationmode='country names',\n        z = feq['Count'],\n        text = feq['Age Group'],\n        colorscale = [\"#f7fbff\",\"#ebf3fb\",\"#deebf7\",\"#d2e3f3\",\"#c6dbef\",\"#b3d2e9\",\"#9ecae1\",\n              \"#85bcdb\",\"#6baed6\",\"#57a0ce\",\"#4292c6\",\"#3082be\",\"#2171b5\",\"#1361a9\",\n              \"#08519c\",\"#0b4083\",\"#08306b\"],\n        autocolorscale = False,\n        reversescale = False,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick =True,\n            title = 'Number of Respondents(In Thousands)'),\n      ) ]\nlayout = dict(\n    title = 'Age Group with Higher Respondent from Each Country',\n    geo = dict(\n        showframe = True,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\nfig = dict( data=count, layout=layout )\npy.offline.iplot( fig, validate=False, filename='d3-world-map' )","f67faf25":"#India : 4417 respondent\ndata_india=data['Q3'].str.contains('India')\ndata_india=data[data_india]\n\n#USA:  4716 respondent\ndata_usa=data['Q3'].str.contains('United States of America')\ndata_usa=data[data_usa]\n\n#China:  1644 respondent\ndata_china=data['Q3'].str.contains('China')\ndata_china=data[data_china]\n\n#Russia:  879 respondent\ndata_russia=data['Q3'].str.contains('Russia')\ndata_russia=data[data_russia]\n\n\n#Brazil:  736 respondent\ndata_brazil=data['Q3'].str.contains('Brazil')\ndata_brazil=data[data_brazil]\n\n#UK:  702 respondent\ndata_uk=data['Q3'].str.contains('United Kingdom')\ndata_uk=data[data_uk]","5a2cdd8d":"age_india=data_india.groupby('Q2').size()\nage_usa=data_usa.groupby('Q2').size()\nage_china=data_china.groupby('Q2').size()\nage_russia=data_russia.groupby('Q2').size()\nage_brazil=data_brazil.groupby('Q2').size()\nage_uk=data_uk.groupby('Q2').size()\n\nage_data=pd.concat([age_india, age_usa, age_china, age_russia, age_brazil, age_uk], axis=1, sort=True)\nage_data.columns=['Ind','USA', 'China', 'Russia', 'Brazil', 'UK']\nage_data['Total'] = age_data.sum(axis=1)\nage_data=age_data.fillna(0)\n\nage_data.drop(['Total'], axis=1).plot(kind='barh', stacked=True, figsize=(20, 15), width=.8)\ndf_total=age_data['Total']\ndf_rel = age_data[age_data.columns[0:6]].div(df_total, 0)*100\nfor n in df_rel:\n    for i, (cs, ab, pc, tot) in enumerate(zip(age_data.iloc[:, 0:].cumsum(1)[n], age_data[n], df_rel[n], df_total)):\n        plt.text(tot, i, str(tot), va='center', fontsize=12)\n        if pc >= 15 and tot>100:\n            plt.text(cs - ab\/2, i, str(np.round(pc, 1)) + '%', va='center', ha='center', fontsize=12, rotation=90)\n        \n        \nplt.legend(fontsize=20)\nplt.xlabel(\"Number of Repondent\", fontsize=20)\nplt.ylabel(\"Age Group\", fontsize=20)\nplt.yticks(rotation=45, fontsize=12)\nplt.xticks(fontsize=12)\nplt.title(\"Number of Respondent from different Countries (Top 6)\", fontsize=25, color='r')","be97e4ae":"degree=pd.DataFrame(data['Q4'].value_counts())\ndegree.sort_index(inplace=True)\n\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = degree.index\nsizes = degree['Q4']\nexplode = (0.01,0.01,0.01,0.01, 0.01, 0.01, 0.01)\nfig1, ax1 = plt.subplots(figsize=(8,8))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=False, startangle=-110, textprops={'weight':'bold'})\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Percenatge of Respondent in Different Degree Programs \", fontsize=14, weight='bold')\nplt.show()","16ed4f82":"major_india=age_india=data_india.groupby('Q4').size()\nmajor_usa=data_usa.groupby('Q4').size()\nmajor_china=data_china.groupby('Q4').size()\nmajor_russia=data_russia.groupby('Q4').size()\nmajor_brazil=data_brazil.groupby('Q4').size()\nmajor_uk=data_uk.groupby('Q4').size()\nmajor_data=pd.concat([major_india, major_usa, major_china, major_russia, major_brazil, major_uk], axis=1, sort=True)\nmajor_data.columns=['Ind','USA', 'China', 'Russia', 'Brazil', 'UK']\nmajor_data['Total'] = major_data.sum(axis=1)\nmajor_data=major_data.fillna(0)\n\nmajor_data.drop(['Total'], axis=1).plot(kind='barh', stacked=True, figsize=(20, 12), width=.8)\ndf_total=major_data['Total']\ndf_rel = major_data[major_data.columns[0:6]].div(df_total, 0)*100\nfor n in df_rel:\n    for i, (cs, ab, pc, tot) in enumerate(zip(major_data.iloc[:, 0:].cumsum(1)[n], major_data[n], df_rel[n], df_total)):\n        plt.text(tot, i, str(tot), va='center', fontsize=15)\n        if pc >= 10 and tot>200:\n            plt.text(cs - ab\/2, i, str(np.round(pc, 1)) + '%', va='center', ha='center', fontsize=15, rotation=90)\n        \n        \nplt.legend(fontsize=20)\nplt.xlabel(\"Number of Repondent\", fontsize=20)\nplt.yticks(rotation=45, fontsize=14, weight='bold')\nplt.ylabel(\"Major Level\", fontsize=20)\nplt.xticks(fontsize=14, weight='bold')\nplt.title(\"Major Level  of Respondents in Top 6 Countries \", fontsize=25, color='r')","09c59b1a":"majors_stream=data['Q5'].value_counts()\nmajors_stream.sort_index(inplace=True)\n#majors_stream.sb.barh(figsize=(10,8))\nplt.figure(figsize=(10,10))\ntotal=majors_stream.sum(axis=0)\nax=sns.barplot(y=majors_stream.index, x=majors_stream.values)\nplt.title(\"Undergarduate Major of Respondents\", fontsize=20, color='r')\nplt.xticks( weight='bold')\nplt.yticks( weight='bold')\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\n\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.3, i.get_y()+.38, \\\n            str(round((i.get_width()\/total)*100, 2))+'%', fontsize=13,\ncolor='black')","bab31c71":"profession= data['Q6'].value_counts()\nplt.figure(figsize=(15,8))\ntot=profession.sum(axis=0)\nax=sns.barplot(y=profession.index, x=profession.values)\nplt.title(\" Current Role of the Respondents\", fontsize=15)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.3, i.get_y()+.38, \\\n            str(round((i.get_width()\/total)*100, 2))+'%', fontsize=12,\ncolor='black')","823c17aa":"df=data.groupby(['Q6','Q7'])['Q6'].count()\ndf=pd.DataFrame(df)\ndf1=pd.DataFrame(df.index)\ndf1.columns=['Role\/Ind']\ndf1['Role'] = df1['Role\/Ind'].str.get(0)\ndf1['Industry'] = df1['Role\/Ind'].str.get(1)\narr=df['Q6'].values\ndf=pd.DataFrame(arr)\ndf.columns=['Count']\nframe=[df, df1]\nresult = pd.concat(frame, axis=1, sort=True)\nresult.drop(['Role\/Ind'], axis=1, inplace=True)\n\ndata_heatmap= result.pivot(values='Count', index='Industry', columns='Role')\nplt.figure(figsize=(14,10))\nplt.xlabel(\"Role\", weight='bold', fontsize=12)\nplt.ylabel(\"Industry\", weight='bold', fontsize=12)\nplt.title(\"Profession Of Respondents in Different Industry\", fontsize=20, color='blue')\nsns.heatmap(data_heatmap, annot=True, linewidths=.5, cmap=\"YlGnBu\", vmin=0, vmax=3800, fmt='g', cbar_kws={'label':'Number of Respondents'})\nplt.show()","7b4087e4":"frame=[data_usa, data_india, data_china, data_russia, data_brazil, data_uk]\ncp= pd.concat(frame, axis=0, sort=False)\narr=['Data Scientist','Student', 'Data Analyst', 'Software Engineer','Research Scientist', 'Consultant']\ncp=cp.loc[cp['Q6'].isin(arr)]\ncp=pd.DataFrame(cp.groupby(['Q3'])['Q6'].value_counts()).rename(columns={'Q6': 'Count'})\ncp['Country\/Profession']=cp.index\ncp.index = np.arange(0, len(cp))\ncp['Country'] = cp['Country\/Profession'].str.get(0)\ncp['Profession'] = cp['Country\/Profession'].str.get(1)\ncp=cp.drop('Country\/Profession', axis=1)\ncp=cp.pivot(values='Count', index='Profession', columns='Country')\ncp['Total'] = cp.sum(axis=1)\n\n\ncp.drop(['Total'], axis=1).plot(kind='barh', stacked=True, figsize=(20, 15), width=.8)\ndf_tot=cp['Total']\ndf_r =cp[cp.columns[0:6]].div(df_tot, 0)*100\nfor n in df_r:\n    for i, (cs, ab, pc, tot) in enumerate(zip(cp.iloc[:, 0:].cumsum(1)[n], cp[n], df_r[n], df_tot)):\n        plt.text(tot, i, str(tot), va='center', fontsize=15)\n        if pc >= 5 and tot>10:\n            plt.text(cs - ab\/2, i, str(np.round(pc, 1)) + '%', va='center', ha='center', fontsize=15, rotation=90)\n        \n        \nplt.legend(fontsize=20)\nplt.xlabel(\"Number of Repondents\", fontsize=20)\nplt.yticks(rotation=45, fontsize=13, weight='bold')\nplt.xticks(fontsize=13, weight='bold')\nplt.ylabel(\"Profession\", fontsize=20)\nplt.title(\"Current Role in Top 6 Countries\", fontsize=25, color='r')\n\nplt.grid(True)","b060cc1a":"experience=pd.DataFrame(data['Q8'].value_counts())\nexperience['Years']=experience.index\nexperience.index = np.arange(0, len(experience))\n\ncompensation=data['Q9'].value_counts()\ncompensation=pd.DataFrame(compensation)\ncompensation[\"Compensation\"]=compensation.index\ncompensation.Compensation.replace('I do not wish to disclose my approximate yearly compensation', 'Undisclosed', inplace=True)\ncompensation.index = np.arange(0, len(compensation))\n\n\nfig, ax =plt.subplots(1,2, figsize=(20,7))\na=sns.barplot(y=experience['Years'], x=experience['Q8'],ax=ax[0])\nb=sns.barplot(y=compensation['Compensation'], x=compensation['Q9'], ax=ax[1])\na.set_xlabel(\"Number of respondents\",fontsize=14)\nb.set_xlabel(\"Number of respondents\",fontsize=14)\na.set_ylabel(\"Experience\",fontsize=14)\nb.set_ylabel(\"Compensation\",fontsize=14)","7c06aaec":"year_in_profession=pd.DataFrame(data.groupby(['Q6','Q8'])['Q8'].count()).rename(columns={'Q6': 'Job ROle', 'Q8':'Count'})\nabc=pd.DataFrame(year_in_profession.index)\nabc.columns=['Role\/Year']\nyear_in_profession = year_in_profession.reset_index(drop=True)\nabc['Job Roles']=abc['Role\/Year'].str.get(0)\nabc['Experience (Years)']=abc['Role\/Year'].str.get(1)\nabc.drop(['Role\/Year'], axis=1, inplace=True)\nframe=[abc, year_in_profession]\nyear_in_profession = pd.concat(frame, axis=1, sort=True)\nndf = year_in_profession.pivot_table('Count', ['Experience (Years)'], 'Job Roles')\nndf=ndf.fillna(0)\n\nplt.figure(figsize=(15,8))\n\n\nplt.title(\"Work Experience in Current Job\", fontsize=20, color='blue')\nax=sns.heatmap(ndf, annot=True, linewidths=.5, cmap=\"BuPu\", vmin=0, vmax=2050, fmt='g', cbar_kws={'label': 'Number of Respondents'})","73a02a19":"count_comp=pd.DataFrame(data.groupby(['Q3'])['Q9'].value_counts()).rename(columns={'Q9':'Count'})\ncount_comp['country\/compensation']=count_comp.index\ncount_comp['Country']=count_comp['country\/compensation'].str.get(0)\ncount_comp['Compensation']=count_comp['country\/compensation'].str.get(1)\ncount_comp.drop('country\/compensation', axis=1, inplace=True)\ncount_comp=count_comp[count_comp['Compensation']!='I do not wish to disclose my approximate yearly compensation']\nind=count_comp.groupby(['Country'])['Count'].transform(max) == count_comp['Count']\ncount_comp=count_comp[ind]\ncount_comp.index= np.arange(0, len(count_comp))\ncount_comp\n\npy.offline.init_notebook_mode(connected=True)\nabc = [ dict(\n        type = 'choropleth',\n        locations = count_comp['Country'],\n        locationmode='country names',\n        z = count_comp['Count'],\n        text = count_comp['Compensation'],\n        colorscale = [[0,\"rgb(5, 10, 172)\"],[0.35,\"rgb(40, 60, 190)\"],[0.5,\"rgb(70, 100, 245)\"],\\\n            [0.6,\"rgb(90, 120, 245)\"],[0.7,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"]],\n        autocolorscale = False,\n        reversescale = True,\n        marker = dict(\n            line = dict (\n                color = 'rgb(180,180,180)',\n                width = 0.5\n            ) ),\n        colorbar = dict(\n            autotick =False,\n            title = 'Number of Respondents'),\n      ) ]\nlayout = dict(\n    title = 'Number of Respondent from across the Globe',\n    geo = dict(\n        showframe = True,\n        showcoastlines = True,\n        projection = dict(\n            type = 'Mercator'\n        )\n    )\n)\nfig = dict( data=abc, layout=layout )\npy.offline.iplot( fig, validate=False, filename='d3-world-map' )\n\n","cc70fa11":"ml_method=data['Q10'].value_counts(sort=False)\nplt.figure(figsize=(10,5))\nml_method.plot.barh()","144657fc":"tools=data['Q12_MULTIPLE_CHOICE'].value_counts(sort=False)\n\nide=data[['Q13_Part_1', 'Q13_Part_2', 'Q13_Part_3', 'Q13_Part_4', 'Q13_Part_5', 'Q13_Part_6', 'Q13_Part_7', 'Q13_Part_8',\n          'Q13_Part_9', 'Q13_Part_10', 'Q13_Part_11', 'Q13_Part_12', 'Q13_Part_13']]\nnide=ide.apply(pd.Series.value_counts)\nnide=nide.fillna(0)\nnide['Number of Respondents']=nide.sum(axis=1)\nnide['IDE']=nide.index\nnide.index = np.arange(0, len(nide))\nnide=nide[1:]\n\n\n\nfig, ax =plt.subplots(2,1, figsize=(10,6))\nplt.suptitle('Pimary Tools and IDEs Used by Respondents',fontsize=15)\na=sns.barplot(x=tools.values, y=tools.index, ax=ax[0])\nb=sns.barplot(x=nide['IDE'], y=nide['Number of Respondents'], ax=ax[1])\nb.set_ylabel(\"Number of respondents\",fontsize=12)\na.set_ylabel(\"Primary Tools to Analyze the Data\",fontsize=12)\nb.set_xlabel(\"IDE\",fontsize=12)\nplt.sca(ax[1])\nplt.xticks(rotation=70)\n","8767a47f":"hosted_notebook=data[['Q14_Part_1', 'Q14_Part_2', 'Q14_Part_3', 'Q14_Part_4', 'Q14_Part_5', 'Q14_Part_6', 'Q14_Part_7', 'Q14_Part_8',\n          'Q14_Part_9', 'Q14_Part_10', 'Q14_Part_11']]\nhosted_notebook=hosted_notebook.fillna('0')\nhosted_notebook=hosted_notebook.apply(pd.Series.value_counts)\nhosted_notebook['Number of Respondents']=hosted_notebook.sum(axis=1)\nhosted_notebook['Hosted Notebook']=hosted_notebook.index\nhosted_notebook=hosted_notebook[hosted_notebook['Hosted Notebook']!= 'None']\nhosted_notebook.index = np.arange(0, len(hosted_notebook))\nhosted_notebook=hosted_notebook[1:]\nplt.figure(figsize=(8,8))\nax=sns.barplot(y=hosted_notebook['Hosted Notebook'], x=hosted_notebook['Number of Respondents'])\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\ntotal=hosted_notebook['Number of Respondents'].sum(axis=0)\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+.3, i.get_y()+.38, \\\n            str(round((i.get_width()\/total)*100, 2))+'%', fontsize=12,\ncolor='black')","0b25f203":"cloud=data[['Q15_Part_1', 'Q15_Part_2', 'Q15_Part_3', 'Q15_Part_4', 'Q15_Part_5', 'Q15_Part_6', 'Q15_Part_7']]\ncloud=cloud.apply(pd.Series.value_counts)\ncloud['Number of Respondents']=cloud.sum(axis=1)\ncloud['Cloud Computing Service Used']=cloud.index\ncloud.index = np.arange(0, len(cloud))\ncloud=cloud.fillna(0)\n\npy.offline.init_notebook_mode(connected=True)\nfig = {\n  \"data\": [\n    {\n      \"values\": cloud['Number of Respondents'],\n      \"labels\": cloud['Cloud Computing Service Used'],\n      \"name\": \"Cloud Service\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .4,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Cloud Computing Service Used\",\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 20\n                },\n                \"showarrow\": False,\n                \"text\":\"\",\n                \"x\": 0.20,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\npy.offline.iplot( fig, validate=False, filename='donut' )\n","921cadc1":"pro_lang=pd.DataFrame(data['Q17'].value_counts())\nrec_lang=pd.DataFrame(data['Q18'].value_counts()).rename(columns={'Q18':'Count'})\n\n\nfig, ax =plt.subplots(1,2, figsize=(15,6))\na=sns.barplot(y=pro_lang.index, x=pro_lang['Q17'],ax=ax[0])\nb=sns.barplot(y=rec_lang.index, x=rec_lang['Count'], ax=ax[1])\nplt.suptitle('Program Languages',fontsize=18)\n\nb.set_xlabel(\"Number of respondents(k)\",fontsize=14)\na.set_xlabel(\"'Number of Respondents (k)\",fontsize=14)\nb.set_ylabel(\"\")\na.title.set_text('Used')\nb.title.set_text('Recommended')","46d2aae8":"ml_library=pd.DataFrame(data['Q20'].value_counts()).rename(columns={'Q20':'Count'})\nvisualization_library=pd.DataFrame(data['Q22'].value_counts()).rename(columns={'Q22':'Count'})\n\nfig, ax =plt.subplots(1,2, figsize=(15,6))\na=sns.barplot(y=ml_library.index, x=ml_library['Count'],ax=ax[0])\nb=sns.barplot(y=visualization_library.index, x=visualization_library['Count'], ax=ax[1])\nb.set_xlabel(\"Number of respondents(k)\",fontsize=14)\na.set_xlabel(\"Number of Respondents (k)\",fontsize=14)\nplt.suptitle(\"Libraries:\", fontsize=16)\na.title.set_text('Machine Learning')\nb.title.set_text('Visualization')\n","c44601d4":"active_coding=pd.DataFrame(data['Q23'].value_counts()).rename(columns={'Q23':'Count'})\nactive_coding['Percent of Time'] = active_coding.index\nactive_coding.index = np.arange(0, len(active_coding))\nactive_coding['Percent of Time'] = pd.DataFrame({'active_coding': ['50%-74%', '25%-49%', '1%-25%', '75%-99%', '0%', '100%']})\n\nconsider_ds=data['Q26'].value_counts()\npy.offline.init_notebook_mode(connected=True)\nfig = {\n  \"data\": [\n    {\n      \"values\": active_coding['Count'],\n      \"labels\": active_coding['Percent of Time'],\n      \"domain\": {\"x\": [.52, 1]},\n      \"name\": \"Time Spend in coding\",\n      \"hoverinfo\":\"label+name\",\n      \"hole\": .6,\n      \"type\": \"pie\"\n    },\n     {\n      \"values\": consider_ds.values,\n      \"labels\": consider_ds.index,\n      \"domain\": {\"x\": [0, .48]},\n      \"name\": \"Data Scientist?\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .6,\n      \"type\": \"pie\"\n    }],\n  \"layout\": {\n        \"title\":\"Are you a Data Scientist? Time Spend in Coding?\",\n        'showlegend': False,\n        \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 20\n                },\n                \"showarrow\": False,\n                \"text\":\"\",\n                \"x\": 0.20,\n                \"y\": 0.5\n            }\n        ]\n    }\n}\npy.offline.iplot( fig, validate=False, filename='donut' )","cef04b67":"%matplotlib inline\nexp_vis=pd.DataFrame(data['Q24']).rename(columns={'Q24':'Experience in Analizing data'})\nexp_vis=exp_vis.apply(pd.Series.value_counts)\nexp_vis['Experience']=exp_vis.index\nexp_vis['Experience'].replace(to_replace=['I have never written code but I want to learn', 'I have never written code and I do not want to learn'], value=['0;Want to learn',\"0;Don't want to learn\"],\n                                         inplace=True)\nexp_vis.index = np.arange(0, len(exp_vis))\nexp_ml=pd.DataFrame(data['Q25']).rename(columns={'Q25':'Experience with ML Libraries'})\nexp_ml=exp_ml.apply(pd.Series.value_counts)\nexp_ml['Experience']=exp_ml.index\nexp_ml['Experience'].replace(to_replace=['I have never studied machine learning but plan to learn in the future', 'I have never studied machine learning and I do not plan to'], value=['0;Plan to learn',\"0;Don't want to learn\"],\n                                         inplace=True)\nexp_ml.index = np.arange(0, len(exp_ml))\n\nfig, ax =plt.subplots(1,2, figsize=(6,8))\nplt.subplots_adjust(left=-1)\na=sns.barplot(y=exp_vis['Experience'], x=exp_vis['Experience in Analizing data'],ax=ax[0])\nb=sns.barplot(y=exp_ml['Experience'], x=exp_ml['Experience with ML Libraries'], ax=ax[1])\na.set_xlabel(\"Number of respondents\",fontsize=12)\nb.set_xlabel(\"Number of respondents\",fontsize=12)\na.set_title(\"Experience in Analizing data\",fontsize=14)\nb.set_title(\"Experience with ML Libraries\",fontsize=14)\na.set_ylabel(\"Experience\",fontsize=14)\nb.set_ylabel(\"\")\nplt.sca(ax[1])\nplt.yticks(rotation=45)\nplt.sca(ax[0])\nplt.yticks(rotation=45)\n","b0902c20":"data_type=data['Q32'].value_counts()\n\nplt.figure(figsize=(18,7))\nax=sns.barplot(x=data_type.index, y=data_type.values)\nax.set_ylabel(\"Number of Respondents\",fontsize=16)\nax.set_xlabel(\"Data Type\",fontsize=16)","0353703e":"data_source=data[['Q33_Part_1', 'Q33_Part_2', 'Q33_Part_3', 'Q33_Part_4', 'Q33_Part_5', 'Q33_Part_6', 'Q33_Part_7',\n                  'Q33_Part_8', 'Q33_Part_9', 'Q33_Part_10', 'Q33_Part_11']]\ndata_source=data_source.apply(pd.Series.value_counts)\ndata_source[\"Number of Respondents\"]=data_source.sum(axis=1)\ndata_source['Data Source']=data_source.index\ndata_source.index=np.arange(0,len(data_source))\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(10,7))\nax=sns.barplot(y=data_source['Data Source'], x=data_source['Number of Respondents'])\nax.set_xlabel(\"Number of Respondents\",fontsize=16)\nax.set_ylabel(\"Data Source\",fontsize=16)","04180fa9":"online_platform=pd.DataFrame(data['Q37'].value_counts())\nplt.figure(figsize=(16,8))\nsns.set_style(\"darkgrid\")\nax=sns.barplot(y=online_platform.index, x=online_platform['Q37'])        \nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(5+p.get_width(), p.get_y()+0.5*p.get_height(),\n             width,\n             ha='left', va='center')\nax.set_xlabel(\"Number of respondent\", fontsize=12)\nax.set_ylabel(\"Online Learning Platform\", fontsize=12)\nplt.show()","347b3bcf":"media_source=data[['Q38_Part_1', 'Q38_Part_2', 'Q38_Part_3', 'Q38_Part_4', 'Q38_Part_5', 'Q38_Part_6', 'Q38_Part_7',\n                  'Q38_Part_8', 'Q38_Part_9', 'Q38_Part_10', 'Q38_Part_11', 'Q38_Part_12', 'Q38_Part_13', 'Q38_Part_14', \n                  'Q38_Part_15', 'Q38_Part_16', 'Q38_Part_17', 'Q38_Part_18']]\nmedia_source=media_source.apply(pd.Series.value_counts)\nmedia_source['Number of Respondents']=media_source.sum(axis=1)\nmedia_source['Media Sources']=media_source.index\n\nmedia_source.index=np.arange(0,len(media_source))\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(15,8))\nax=sns.barplot(y=media_source['Media Sources'], x=media_source['Number of Respondents'])\nfor p in ax.patches:\n    width = p.get_width()\n    plt.text(5+p.get_width(), p.get_y()+0.5*p.get_height(),\n             width,\n             ha='left', va='center')\n\nax.set_xlabel(\"Number of Respondents\",fontsize=16)\nax.set_ylabel(\"Media Sources\",fontsize=16)\nplt.show()","d93742bc":"## What these figures says?\n1. Most of the participants **(25.81%) fall in the age group of 25-29 years**.  United States, Canada , Russia and European countries have higher respondents of this age group. \n2. It is good to see the **youth (18-24 years) participation which make around 34%** of the total respondents. They are the future of this emering field. Most of the participant from Asia Pacific region fall in this age group largely from India.\n3. Also, experienced participants of age 40 and above are active in their work and maybe transfering their knowledge and experinece to the youth through global community like Kaggle.  \n\n### Below is the Graph that shows the age distribution of participants in 6 top countries by number of participants in the survey.","02007894":"We find that  Amazon Web Services is used by 29% of the respondents whereas next to it are the 27.7% respondent who haven't worked on any cloud computing service. Google Cloud has more user than Microsoft Azure.","000a6983":"-  Numericalis the most common types, used by 25.9% of the respondents followed by tabular data. Time series is less used than textual data whereas multimedia data such as audio and video are least worked upon.\n- Platforms such as Kaggle dataset platform, socrata etc are the top sources of data for the respondents. A majority of respondents also search google for datsets while Github is the choices of 36.16% of the respondents","54f4e0c8":"\n## Current Work Role:","4e1f13d4":"## Observation:\n1. Majority of the respondents holds the work experience of 0-2 years.  Most data scientist who participated in this survey falls under this category ( *See heatmap*).** Only 3.56% of the total respondent have work experince of 20+ years.**  Respondent with current role as Software Engineer holds greater experience.\n2.  An important point to note is that respondent who are new in their current role have participated in the survey in greater number than the respondent having relatively more experience. This may imply that these respondent look forward to online cummunity such as Kaggle to expand their learning.\n3. Coming to compensation, **majority of the respondent earn a compensation upto 20,000 USD. **\n4. Most frequent compensation in **Asian countries is 0-10,000 USD** whereas in **American countries and Australia, it is 100-125,000 USD.  **","e39a3421":"## An insight of respondents Work \n#### \"The only source of knowledge is EXPERIENCE\"- Albert Einstein ","7a1311b1":"## Observation:\n1. We clearly see **Python's dominance as the most often used programming language with 54% of the respondents using it**. It is also the the ideal programming language for the aspiring data scientist as per the **recommendation from 75% of the respondents**. This is due its powerful ML and visualization libraries.\n2. Next comes **R programming langauage with 13.4% using and 12.5 % of respondent recommending it**.It somewhat similar significance as Python in data science. SQL comes up as third most used and Recommended language. \n3. Coming to ML libraries, **Python's Scikit-Learn is most used with more than 25% of the respondent using it**. Google's TensorFlow second most used Ml Library and ideal for developing deep learning algorithms. keras also have significan number of users.\n4. In Visual, with **55% matplotlib is one of the most widely used in visualization library in data science** for all kinds of graphics. Despite pythons dominance, R's ggplot do well and is second most used visual library followed by seaborn, another library in Python based on matplotlib that offers scientists a package that enables them to create explanatory graphs from highly complex data. \n","90df46b3":"## What did we find?\n1. Out of 18481 respondents, **51.8% of them consider themselves as data scientists whereas 25.5% do not see themselves as data scientists.** They are 22.63% of the respondents who are unsure about this.\n2. A minority of the respondents **(2.61%) spend their full time in coding** while 29.9% of the respondents spend 50-74% of their time in coding.\n4. Around 53.4% of the respondent have experinece of 0-2 years in analysing data whereas 21.7% holds an experience of 3-5 years. 60.5%  of participant have used ML methods for 0-2 years. It is important point to note here is that 10% of the respondent are want to learn Machine learning and small portion of participant(4.4%) is open to learn data analysis.","832b5424":"## Programmig Language and Libraries:\nHere we'll see which programmig language is the favourite among the programmers and the language which get highest number of reconmndation from the respondents for the aspiring data scientists.\nAlso we'll have a look at the most widely  used ML and Visualization libraries for differnet languages.","9dc7cdc3":"The bar graph shows that majority of the respondent's employers are either exploring  or have recently started using Machine Learning methods which shows that Machine Learning is clearly a growing sector with interest from many businesses.","eb4e44ab":"## Online Learing Platforms and Media Source:","011511cc":"## Cloud Services:","b176be77":"## Which countries took the survey?","eec758d6":"## What Education and Work Role has to say?","43482c3c":"## Obeservations:\n1. ** Highest number of kaggler participating in the survery are from United States and India** \n\n2. China and Russia are in third and fourth place respectively with user less than half than that from United States and India.\n3. Considering the region, **Asia Pacific has more participants** where as **European countries are more active. **\n\n","97ce1d56":"## Data Scientist? Okay... Self evaluation time.","ac132e44":"## Data Science: A Hot Cake in Industries\n###  Through Kaggle ML and DS Survey -2018\n#### About the Dataset:\nThe data was collected from the responses of the **23859 participiant** Kagglers from **147 different countries and terrortries**. The Survey was conducted in the form of a questionaire consisting of **50 question.**  This survey gives the idea about the the popular platform, libraries, programming languages etc in Machine Learning and Data Science. It also provide a glimpse about demographic about the participants , their roles in industry and how they are making use of Data Science, thus making this dataset interesting to explore.\n\n![](https:\/\/www.scnsoft.com\/blog-pictures\/business-intelligence\/real-time-big-data-analytics-01_1.png)\n \n ## Lets get started..!!!\n \n \n ","d0b537f7":"### Incorporation od ML Methods in Business:\n    ","396a1003":"## Age group of the respondents:","3bc22c4c":"## Obeservations:\n1. Out of 19,199 respondent who were asked about the primary tool they use at their work, around **50% work on local or hosted environments such as RStudio, JupyterLab etc. **\n2. Basic statistical softwares such as **Excel and Google Sheets is used by 20.5% of the respondents** which are perfect for doing few calculations or to draw  some charts, but they quickly become ungainly when it comes to woking with large datastets.\n3. **Jupyter\/IPython are RStusio**, which are more compelling, are the **most used IDE** with majority of the people using it forlast 5 years at their work or schools.\n4. Talking about hosted notebooks, **31.99% respondents have used Kaggle Kernels in last 5 years.**\n","fc4cd9e8":"## Data Type and Data Sources\nIn this portion, we'll see the types of data and data sources which respondent used at their work and schools.","13f8c04a":"## Tools, IDE and Hosted Notebooks:\nIn this section we'll have a look at the stats about  primary tools, IDEs and hosted notebook used by tthe respondents in thier work or schools.","cdc1b562":"## Observations:\n1. Highest number of participants **(46%) have attended or plan to attend Master's program**  dlosely followed by **Bachelor's Degree( 30.2%)**\n2. Cosidering the top 6 countries, maximum number of respondents in Unites States have attended\/plan to attend Master's Degree.  With 49.4%, Bachelor's program prevails among the  Indian participants \n3. Taking about Doctorate Program, United States is quite ahead of other countries.\n4.  **Yeah..!! Computer Science.** Majority of the respondent have major in computer science (41.09%).  At second and third place comes the respondent with majors in Engineering(non-computers) and Physics or astronumy respectively.\n5. Respondents with majors in fine arts and humnities left me supriesed and impressed. This shows data science is being looked forwared to in studies other than computer science an engineering. ","ce45832e":"### Observation:\n1. Talking about online learning platforms, **Coursera is the most seeked platform for learning and almost 39% of the respondent use it the most**. Relatively similar number of respondent use Datacamp and Udemy.  It is worthy to note that online University courses have less votes than independents online platforms such as coursera. Thus University reach out users through these online platforms making them a good place for efficient learning.\n2. In media sources, **Kaggle Forums is the first choice of majority of the respondents which are ideal place to share idea and knowledge**. Out of 16,338 total respondents, Media blog post and KDNuggets blog  got thumbs up from  5000+ and 3000+ respondents respectively. It is important note that Twitter is source to greater number of respondents than sources like Journel and O'Relly Data Newsletters.","c7a8e46a":"### Bar -Stack- Heat: What did they reveal?\n1. **Student makes the majority of respondents with 22.89%**. This figure tells us how  populer are  kaggle is among the students who are learning and exploring the horizons of Data Science.  After stdents are the data scientist and software engineers that makes altogether 31.68%\n2. Majority of data scientist, Data Analyst and software engineers are the part of computers and Technology industry. Also, Computers and Technolog industry has the role for person of every profession.\n3. Considering 6 top countries by number of respondents, majority of the participants with current role as **Data Scientist and Data analyst are from United States are (48.8%)**, closely followed by **India where software engineer and students have dominance(40.5% ).**"}}