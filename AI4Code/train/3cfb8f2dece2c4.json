{"cell_type":{"203a7e01":"code","66e097bd":"code","5f5077c0":"code","e3a87cb6":"code","b97e8e50":"code","59c338e8":"code","d26c8939":"code","b0e9e13d":"code","b76bb4cd":"code","6cc5ab36":"code","56af6997":"code","38ce4f19":"code","92456c43":"code","1f0a47fc":"code","04db6984":"code","cd9d7cb4":"code","e4f182c2":"code","ba95813f":"code","11a5fdb1":"code","8fba1e84":"code","5348a5d9":"code","087240ac":"code","40725c2a":"code","48fc864c":"code","217bdde0":"code","b56999ef":"code","3c2dda8b":"code","4a963ed6":"code","80fb9bd5":"code","63d07671":"code","14fa731d":"code","4eb796bc":"code","ad5106ff":"code","6deb31ad":"code","703b6dc2":"code","b53f53cd":"code","f10a1cbf":"code","6f71d84a":"code","29ddb7e7":"code","678a63ef":"code","c40ea681":"code","a4a4aa68":"code","3b0869ad":"code","52abc401":"code","4a2d0fcd":"code","a70e2ec0":"code","5c284deb":"code","bf3a6ef0":"code","51e65a69":"markdown","b57df65a":"markdown","75a504e7":"markdown","603a816d":"markdown","d3ed565a":"markdown","13603fec":"markdown","b8d6f82b":"markdown","d5fea05d":"markdown","f9339c2e":"markdown","9d3d49f4":"markdown","d8a38c89":"markdown","c5f39f57":"markdown","9953a69e":"markdown","9aea7790":"markdown","91e770de":"markdown","ea5750db":"markdown","12f8ff5b":"markdown","c3bb506e":"markdown","f50235f7":"markdown","10f1f2b5":"markdown","52fa3170":"markdown","75993c1d":"markdown","0099cc56":"markdown","88ed05fd":"markdown","66cffb94":"markdown","f40a9c70":"markdown","d50ccce3":"markdown","045e79d1":"markdown","8ae4c0be":"markdown","06b5030e":"markdown","9cec2012":"markdown","1b55b372":"markdown","cc03394c":"markdown","9dd16b38":"markdown","c486b9d0":"markdown","7917e7db":"markdown","2389623f":"markdown"},"source":{"203a7e01":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport io\nimport requests\nimport nibabel as nib\nfrom gzip import GzipFile","66e097bd":"# Define the study path. \nneurovault_url = \"http:\/\/neurovault.org\/api\/collections\/8740\/images\/?format=json\"\n\n# Request the study metadata. \nneurovault_files = []\nwhile neurovault_url:\n    r = requests.get(neurovault_url)\n    d = r.json()\n    neurovault_files += d['results']\n    neurovault_url = d['next']\n    \n# Save the metadata as a DataFrame. \nneurovault_files_df = pd.DataFrame(neurovault_files)","5f5077c0":"# Display information about the DataFrame. \nprint('DataFrame shape:', neurovault_files_df.shape)\nprint('--------------------------------')\nneurovault_files_df.head(1)","e3a87cb6":"# List all constant columns. \nconstant_columns = [column for column in neurovault_files_df.columns \n                    if neurovault_files_df[column].unique().shape[0] == 1]\n\n# Drop constant columns and display the number of dropped columns. \nneurovault_files_df.drop(constant_columns, axis = 1, inplace = True)\nprint(str(len(constant_columns)), 'constant columns were dropped.')\nprint('New DataFrame shape:', neurovault_files_df.shape)\nprint('--------------------------------')\n\n# Display the DataFrame. \nneurovault_files_df.head(1)","b97e8e50":"# Display information about the analysis levels. \nneurovault_files_df['analysis_level'].value_counts()","59c338e8":"# Display the first URL. \npd.set_option('display.max_colwidth', 100)\nneurovault_files_urls = neurovault_files_df['file']\nneurovault_files_urls.head(1)","d26c8939":"# Count how many files contain different substrings. \nfor modality in ['appraisal', 'expression', 'motivation', 'physiology']:\n    print(neurovault_files_urls[neurovault_files_urls.str.find(modality) != -1].shape[0], 'files were found with the substring:', modality)\n\n# List all the files containing the substring 'appraisal'. \nneurovault_files_appraisal = neurovault_files_urls[neurovault_files_urls.str.find('appraisal') != -1]","b0e9e13d":"# Create a DataFrame. \nfiles_df = pd.DataFrame()\n\n# Loop over the files. \nfor file_index in range(0, neurovault_files_appraisal.shape[0]):\n    # Retrieve the subject. \n    files_df.loc[file_index, 'Subject'] = neurovault_files_appraisal.iloc[file_index][45:47]\n    \n    # Retrieve the good\/neutral\/bad condition. \n    if neurovault_files_appraisal.iloc[file_index].find('good') != -1:\n        files_df.loc[file_index, 'Valence'] = 'good'\n    elif neurovault_files_appraisal.iloc[file_index].find('neutral') != -1:\n        files_df.loc[file_index, 'Valence'] = 'neutral'\n    elif neurovault_files_appraisal.iloc[file_index].find('bad') != -1:\n        files_df.loc[file_index, 'Valence'] = 'bad'\n        \n    # Retrieve the no-power\/power condition. \n    if neurovault_files_appraisal.iloc[file_index].find('NoPower') != -1:\n        files_df.loc[file_index, 'Power'] = 'no'\n    else:\n        files_df.loc[file_index, 'Power'] = 'yes'\n        \n    # Retrieve the URL. \n    files_df.loc[file_index, 'URL'] = neurovault_files_appraisal[file_index]\n\n# Convert the Subject column to int. \nfiles_df['Subject'] = files_df['Subject'].astype(int)\n\n# Display the first two subjects. \nfiles_df.head(12)","b76bb4cd":"# Display the number of subjects. \nfiles_df.Subject.nunique()","6cc5ab36":"# Display all the subjects numbers. \nfiles_df.Subject.unique()","56af6997":"# Display the number of files for the good\/neutral\/bad conditions. \nfiles_df.Valence.value_counts()","38ce4f19":"# Display the number of files for the no-power\/power conditions. \nfiles_df.Power.value_counts()","92456c43":"# Display the subject and conditions associated to the first file. \nfiles_df.loc[0, :]","1f0a47fc":"# Request the NIFTI image. \nr = requests.get(files_df.loc[0, 'URL'])\nfp = io.BytesIO(r.content)\ngzfileobj = GzipFile(filename = \"tmp.nii.gz\", mode = 'rb', fileobj = fp)\nnifti_image = nib.Nifti1Image.from_file_map({'image': nib.FileHolder(\"tmp.nii.gz\", gzfileobj)})","04db6984":"# Display the shape of the NIFTI image. \nnifti_image.shape","cd9d7cb4":"# Display the image metadata. \nprint(nifti_image.header)","e4f182c2":"# Extract the image data as a NumPy array, and display its size. \nimage_data = nifti_image.get_fdata()\nimage_data.shape","ba95813f":"# Check that this is the same size as the NIFTI image. \nimage_data.shape == nifti_image.shape","11a5fdb1":"# Check that the image data is indeed a NumPy array. \ntype(image_data) == np.ndarray","8fba1e84":"# Check that the image data is in float64 format. \nimage_data.dtype == np.dtype(np.float64)","5348a5d9":"# Display the total number of values. \nnp.prod(image_data.shape)","087240ac":"# Display the memory usage. \nprint('Image size:', str(image_data.nbytes), 'bytes')","40725c2a":"# Define a function to return the middle sections of an image data array. \ndef middle_sections(image_data):\n    return tuple((np.array(image_data.shape) \/ 2).astype(int))","48fc864c":"# Find the middle sections coordinates of the image data array. \nmiddle_coord = middle_sections(image_data)\nmiddle_coord","217bdde0":"# Display a section of the image with an axial view, using the middle sections coordinates. \nplt.imshow(image_data[:, :, middle_coord[2]], origin = 'lower');","b56999ef":"# Display a selection of sections through the sagittal view. \nsagittal_sections = range(10, 80, 3)\nnb_columns = 4\nfig, axes = plt.subplots(nrows = (len(sagittal_sections) \/\/ nb_columns), ncols = nb_columns, figsize = (12, 18))\n\nsection_count = 0\nfor sagittal_section in sagittal_sections:\n    axes[(section_count \/\/ nb_columns), (section_count % nb_columns)].imshow(image_data[sagittal_section, :, :].T, origin = 'lower')\n    axes[(section_count \/\/ nb_columns), (section_count % nb_columns)].set_title('Section ' + str(sagittal_section))\n    section_count += 1","3c2dda8b":"# Display a selection of sections through the coronal view. \ncoronal_sections = range(10, 105, 4)\nnb_columns = 4\nfig, axes = plt.subplots(nrows = (len(coronal_sections) \/\/ nb_columns), ncols = nb_columns, figsize = (12, 22))\n\nsection_count = 0\nfor coronal_section in coronal_sections:\n    axes[(section_count \/\/ nb_columns), (section_count % nb_columns)].imshow(image_data[:, coronal_section, :].T, origin = 'lower')\n    axes[(section_count \/\/ nb_columns), (section_count % nb_columns)].set_title('Section ' + str(coronal_section))\n    section_count += 1","4a963ed6":"# Display a selection of sections through the axial view. \naxial_sections = range(10, 80, 3)\nnb_columns = 4\nfig, axes = plt.subplots(nrows = (len(axial_sections) \/\/ nb_columns), ncols = nb_columns, figsize = (12, 18))\n\nsection_count = 0\nfor axial_section in axial_sections:\n    axes[(section_count \/\/ nb_columns), (section_count % nb_columns)].imshow(image_data[:, :, axial_section], origin = 'lower')\n    axes[(section_count \/\/ nb_columns), (section_count % nb_columns)].set_title('Section ' + str(axial_section))\n    section_count += 1","80fb9bd5":"# Create a 4D NumPy array to store all the images. \ncollection_data = np.zeros((files_df.shape[0], image_data.shape[0], image_data.shape[1], image_data.shape[2]))\n\n# Loop over the files. \nfor file_index in range(0, files_df.shape[0]):\n    # Display a counter. \n    if file_index % 10 == 0:\n        print('Starting file:', str(file_index), '\/', str(files_df.shape[0]))\n    \n    # Request the data for a particular image. \n    r = requests.get(files_df.loc[file_index, 'URL'])\n    fp = io.BytesIO(r.content)\n    gzfileobj = GzipFile(filename = \"tmp.nii.gz\", mode = 'rb', fileobj = fp)\n    nifti_image = nib.Nifti1Image.from_file_map({'image': nib.FileHolder(\"tmp.nii.gz\", gzfileobj)})\n    \n    # Get the NumPy array. \n    image_data = nifti_image.get_fdata()\n    \n    # Store the NumPy array. \n    collection_data[file_index, :, :, :] = image_data","63d07671":"# Prepare the figure. \nnb_columns = 4\nfig, axes = plt.subplots(nrows = int(np.ceil(files_df.Subject.nunique() \/ nb_columns)), ncols = nb_columns, figsize = (12, 22))\n\n# Loop over all the subjects. \nsubject_index = 0\nwhile subject_index < files_df.Subject.nunique():\n    # For each subject, retrieve the file associated to the bad\/no-power condition. \n    file_index = files_df[(files_df['Subject'] == (subject_index + 1))\n                          & (files_df['Valence'] == 'bad')\n                          & (files_df['Power'] == 'no')].index.values[0]\n    \n    # Display the images through the middle section of the axial view. \n    axes[(subject_index \/\/ nb_columns), (subject_index % nb_columns)].imshow(collection_data[file_index, :, :, middle_coord[2]], origin = 'lower')\n    axes[(subject_index \/\/ nb_columns), (subject_index % nb_columns)].set_title('Subject ' + str(subject_index + 1))\n    subject_index += 1","14fa731d":"# Prepare the figure. \nnb_conditions = 6\nconditions_names = ['bad\/no-power', 'bad\/power', 'good\/no-power', 'good\/power', 'neutral\/no-power', 'neutral\/power']\nnb_columns = 3\nfig, axes = plt.subplots(nrows = int(np.ceil(nb_conditions \/ nb_columns)), ncols = nb_columns, figsize = (12, 8))\n\n# Loop over all the conditions of the first subject. \ncondition_index = 0\nfor condition_index in range(0, nb_conditions):    \n    # Display the images through the middle section of the axial view. \n    axes[(condition_index \/\/ nb_columns), (condition_index % nb_columns)].imshow(collection_data[condition_index, :, :, middle_coord[2]], origin = 'lower')\n    axes[(condition_index \/\/ nb_columns), (condition_index % nb_columns)].set_title(conditions_names[condition_index])\n    condition_index += 1","4eb796bc":"# Define the rescaling function. \ndef rescale_image_by_factor(image_data, factor):\n    # Get the dimensions of the image data, and create a new, rescaled, image data array. \n    dimensions = (np.array(image_data.shape) \/ factor).astype('int')\n    new_image_data = np.zeros((dimensions))\n    \n    # Loop over all voxels in the new image data. \n    for x_coord in range(0, dimensions[0]):\n        for y_coord in range(0, dimensions[1]):\n            for z_coord in range(0, dimensions[2]):\n                new_image_data[x_coord, y_coord, z_coord] = np.mean(image_data[(factor * x_coord):(factor * x_coord + factor),\n                                                                               (factor * y_coord):(factor * y_coord + factor),\n                                                                               (factor * z_coord):(factor * z_coord + factor)])\n                \n    # Return the new image data and its dimensions. \n    return new_image_data, new_image_data.shape","ad5106ff":"# Define a function to display the middle sections of the image through the three neurological views. \ndef display_image_by_factor(image_data, middle_coord, y_lim_coronal):\n    fig, axes = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 6))\n    axes[0].imshow(image_data[middle_coord[0], :, :].T, origin = 'lower')\n    axes[1].imshow(image_data[:, middle_coord[1], :].T, origin = 'lower')\n    axes[1].set_ylim(y_lim_coronal[0], y_lim_coronal[1]) # We adjust the size for a better visualization. \n    axes[2].imshow(image_data[:, :, middle_coord[2]], origin = 'lower');","6deb31ad":"# Select the first image in our dataset, and display its shape. \nimage_data = collection_data[0, :, :, :]\nimage_dimensions = image_data.shape\nprint('Image dimensions:  ', image_dimensions)\n\n# Compute and display the middle coordinates of the image. \nmiddle_coord = middle_sections(image_data)\nprint('Middle coordinates:', middle_coord)","703b6dc2":"# Display the middle sections through the three neurological views. \ndisplay_image_by_factor(image_data, middle_coord, [8, 83])","b53f53cd":"# Rescale the voxel size by a factor 2, and display the shape of the rescaled image. \nimage_data_r2, image_dimensions_r2 = rescale_image_by_factor(image_data, 2)\nprint('Image dimensions:  ', image_dimensions_r2)\n\n# Compute and display the middle coordinates of the image. \nmiddle_coord_r2 = middle_sections(image_data_r2)\nprint('Middle coordinates:', middle_coord_r2)","f10a1cbf":"# Display the middle sections through the three neurological views. \ndisplay_image_by_factor(image_data_r2, middle_coord_r2, [4, 41])","6f71d84a":"# Display the memory usage. \nprint('Original image:', str(image_data.nbytes), 'bytes')\nprint('Rescaled image:', str(image_data_r2.nbytes), 'bytes')\nprint('The original image is', str(np.round(image_data.nbytes \/ image_data_r2.nbytes, 2)), 'times heavier than the rescaled image.')","29ddb7e7":"# Rescale the voxel size by a factor 3, and display the shape of the rescaled image. \nimage_data_r3, image_dimensions_r3 = rescale_image_by_factor(image_data, 3)\nprint('Image dimensions:  ', image_dimensions_r3)\n\n# Compute and display the middle coordinates of the image. \nmiddle_coord_r3 = middle_sections(image_data_r3)\nprint('Middle coordinates:', middle_coord_r3)","678a63ef":"# Display the middle sections through the three neurological views. \ndisplay_image_by_factor(image_data_r3, middle_coord_r3, [3, 28])","c40ea681":"# Display the memory usage. \nprint('Original image:', str(image_data.nbytes), 'bytes')\nprint('Rescaled image:', str(image_data_r3.nbytes), 'bytes')\nprint('The original image is', str(np.round(image_data.nbytes \/ image_data_r3.nbytes, 2)), 'times heavier than the rescaled image.')","a4a4aa68":"# Rescale the voxel size by a factor 4, and display the shape of the rescaled image. \nimage_data_r4, image_dimensions_r4 = rescale_image_by_factor(image_data, 4)\nprint('Image dimensions:  ', image_dimensions_r4)\n\n# Compute and display the middle coordinates of the image. \nmiddle_coord_r4 = middle_sections(image_data_r4)\nprint('Middle coordinates:', middle_coord_r4)","3b0869ad":"# Display the middle sections through the three neurological views. \ndisplay_image_by_factor(image_data_r4, middle_coord_r4, [2, 20])","52abc401":"# Display the memory usage. \nprint('Original image:', str(image_data.nbytes), 'bytes')\nprint('Rescaled image:', str(image_data_r4.nbytes), 'bytes')\nprint('The original image is', str(np.round(image_data.nbytes \/ image_data_r4.nbytes, 2)), 'times heavier than the rescaled image.')","4a2d0fcd":"# Create NumPy arrays to store the images through different voxel sizes. \ncollection_data_r2 = np.zeros((files_df.shape[0], image_data_r2.shape[0], image_data_r2.shape[1], image_data_r2.shape[2]))\ncollection_data_r3 = np.zeros((files_df.shape[0], image_data_r3.shape[0], image_data_r3.shape[1], image_data_r3.shape[2]))\ncollection_data_r4 = np.zeros((files_df.shape[0], image_data_r4.shape[0], image_data_r4.shape[1], image_data_r4.shape[2]))\n\n# Create NumPy arrays to store the flatten images through different voxel sizes. \ncollection_data_flatten = np.zeros((files_df.shape[0], np.prod(image_dimensions)))\ncollection_data_flatten_r2 = np.zeros((files_df.shape[0], np.prod(image_dimensions_r2)))\ncollection_data_flatten_r3 = np.zeros((files_df.shape[0], np.prod(image_dimensions_r3)))\ncollection_data_flatten_r4 = np.zeros((files_df.shape[0], np.prod(image_dimensions_r4)))\n\n# Loop over all the images. \nfor image_index in range(0, files_df.shape[0]):\n    # Display a counter. \n    if image_index % 10 == 0:\n        print('Starting image:', str(image_index), '\/', str(files_df.shape[0]))\n    \n    # Retrieve the images. \n    image_data = collection_data[image_index, :, :, :]\n    \n    # Rescale the images. \n    image_data_r2, _ = rescale_image_by_factor(image_data, 2)\n    image_data_r3, _ = rescale_image_by_factor(image_data, 3)\n    image_data_r4, _ = rescale_image_by_factor(image_data, 4)\n    \n    # Flatten the images. \n    image_data_flatten = image_data.flatten()\n    image_data_flatten_r2 = image_data_r2.flatten()\n    image_data_flatten_r3 = image_data_r3.flatten()\n    image_data_flatten_r4 = image_data_r4.flatten()\n    \n    # Store the images in the NumPy arrays. \n    collection_data_r2[image_index, :, :, :] = image_data_r2\n    collection_data_r3[image_index, :, :, :] = image_data_r3\n    collection_data_r4[image_index, :, :, :] = image_data_r4\n    \n    # Store the flatten images in the NumPy arrays. \n    collection_data_flatten[image_index, :] = image_data_flatten\n    collection_data_flatten_r2[image_index, :] = image_data_flatten_r2\n    collection_data_flatten_r3[image_index, :] = image_data_flatten_r3\n    collection_data_flatten_r4[image_index, :] = image_data_flatten_r4","a70e2ec0":"# Display the size in Mbytes for all the collections. \nprint('Images with original voxel size:       ', str(collection_data.nbytes \/ 10**6), 'Mbytes')\nprint('Images with voxel size rescaling of 2: ', str(collection_data_r2.nbytes \/ 10**6), 'Mbytes')\nprint('Images with voxel size rescaling of 3: ', str(collection_data_r3.nbytes \/ 10**6), 'Mbytes')\nprint('Images with voxel size rescaling of 4: ', str(collection_data_r4.nbytes \/ 10**6), 'Mbytes')\nprint()\nprint('Flatten images with original voxel size:       ', str(collection_data_flatten.nbytes \/ 10**6), 'Mbytes')\nprint('Flatten images with voxel size rescaling of 2: ', str(collection_data_flatten_r2.nbytes \/ 10**6), 'Mbytes')\nprint('Flatten images with voxel size rescaling of 3: ', str(collection_data_flatten_r3.nbytes \/ 10**6), 'Mbytes')\nprint('Flatten images with voxel size rescaling of 4: ', str(collection_data_flatten_r4.nbytes \/ 10**6), 'Mbytes')\nprint()\n\n# Display the total size in Gbytes. \ntotal_size = collection_data.nbytes + collection_data_r2.nbytes + collection_data_r3.nbytes + collection_data_r4.nbytes\ntotal_size += collection_data_flatten.nbytes + collection_data_flatten_r2.nbytes + collection_data_flatten_r3.nbytes + collection_data_flatten_r4.nbytes\nprint('Total size for all the collections: ', str(total_size \/ 10**9), 'Gbytes')","5c284deb":"# Save all the arrays into a .npz file. \nnp.savez('neurovault_dataset_8740_raw_rescaled.npz', \n         # Images through all rescaling modes. \n         collection_data = collection_data, \n         collection_data_r2 = collection_data_r2, \n         collection_data_r3 = collection_data_r3, \n         collection_data_r4 = collection_data_r4, \n         \n         # Flatten images through all rescaling modes. \n         collection_data_flatten = collection_data_flatten, \n         collection_data_flatten_r2 = collection_data_flatten_r2, \n         collection_data_flatten_r3 = collection_data_flatten_r3, \n         collection_data_flatten_r4 = collection_data_flatten_r4)","bf3a6ef0":"# Check that the .npz file was successfully saved. \nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","51e65a69":"Now we can request a **first image from the NeuroVault API**. We select the first file in our DataFrame, corresponding to first subject and the no-power\/bad combination of conditions. ","b57df65a":"### Factor 2","75a504e7":"## **4. Image Overview**","603a816d":"### Reference: Ouroboros fMRI","d3ed565a":"If we plan to apply machine learning models on this dataset, the **high number of features (91 x 109 x 91 = 902629 values per array)** could be challenging. One way to address this problem is to **reduce the size of the arrays by averaging several contiguous voxels into higher-level clusters**. Therefore, we will define a function to perform this operation. Since we will use the NumPy function *np.mean()*, the clusters containing at least one NaN voxel will be entirely considered as NaN, which is the most conservative way to manage missing values during rescaling. ","13603fec":"It seems that **all our statistical maps were correctly retrieved from NeuroVault**. We already notice that even within a single experimental condition, there are important differences between subjects. This is almost always the case in fMRI studies. When a neuroimaging article shows the brain regions involved in a certain cognitive process, most of the time, this region is obtained by averaging the statistical maps of a group of subjects. But at an individual level, there is actually a lot of variation in terms of brain response. ","b8d6f82b":"## **5. Full Dataset**","d5fea05d":"Most of the methods described in this Notebook have been previously explored in the project **Ouroboros fMRI: Predicting human brain activity with machine learning models** (https:\/\/github.com\/maeldonoso\/ouroboros_fmri). The first version of the Ouroboros fMRI project was presented as a Capstone Project for the COS in Applied Data Science: Machine Learning of the EPFL Extension School (https:\/\/www.extensionschool.ch\/). ","f9339c2e":"We have almost reached the end of this Notebook. The only step left is to save the data for further use. We will actually save several versions of the dataset, since we will run our image rescaling function on all the statistical maps, and store the data obtained through different **rescaling modes**. Furthermore, we will not only store the data as 3D arrays, but also **flatten the images** and store them as 1D vectors, in order to prepare our dataset for several machine learning models that require the data to be flattened. ","9d3d49f4":"Now we can **recale the voxel size of our image by a factor 2, 3 and 4**, and display the results. ","d8a38c89":"The metadata contains mostly information specific to the fMRI preprocessing. The attribute *dim* refers to the data dimensionality, starting with the number of dimensions (3), followed by the **lengths of these dimensions (91, 109, 91), which correspond to the shape of the NIFTI image**. The attribute *descrip* refers to the name of the statistical map. A complete definition of the NIFTI header can be found here: https:\/\/nifti.nimh.nih.gov\/pub\/dist\/src\/niftilib\/nifti1.h. \n\nWe will now extract the image data as a **NumPy array**, and display some of its characteristics. ","c5f39f57":"Most of the files contain statistical maps at the single-subject level. Only one file contains a statistical map at the group level. We are interested in single-subject maps, but not all of them, since we want to restrict our analysis to the **'appraisal' maps**. The columns *file* and *name* both contain all the necessary information to select the appropriate files. ","9953a69e":"## **6. Images Rescaling**","9aea7790":"We have successfully extracted the image data as a NumPy array. As we can see, a single statistical map contains almost a million values. Each of these values correspond to a **small volume of observation, called a voxel**. The value of these voxels represents the correlation strength between the experimental condition and the brain activity in this small volume of observation. The detail of how this value is obtained is quite technical, and outside the scope of this Notebook, but interested readers can for example explore the manual of the free, and open source, neuroimaging software SPM (https:\/\/www.fil.ion.ucl.ac.uk\/spm\/). According to the article, the fMRI data of our dataset was acquired with a voxel resolution of 1 x 1 x 1 mm^3, but resampled to a spatial resolution of 2 x 2 x 2 mm^3 during the preprocessing. \n\nIn our statistical map, we have a total of **91 x 109 x 91 = 902629 voxels**. A lot of the peripheral volumes will actually be located outside the brain, simply because the global volume of observation of the fMRI scanner is larger than the dimensions of the brain. In neuroimaging studies, the voxels located outside the brain are often removed through a binary mask, and appear as NaN values in the statistical maps. This is also the case in this study. The dimensions of our statistical map correspond to the **three standard neurological views: sagittal (x), coronal (y), axial (z)**. Knowing these terms is essential when working with fMRI data, since all 2D images that we will visualize are just sections of a larger 3D statistical map, and these sections are generally obtained through one of these three standard neurological views. \n\nNow we can have a look at our NumPy array, by visualizing a **2D section** obtained through the middle section of the axial view. ","91e770de":"These visualizations can help us to get a general idea of the **brain activation structure in this particular statistical map**, even if we only displayed a fraction of the total number of sections through each of the neurological views. When working with fMRI data, it is a good idea to always start by displaying some images of the brain, and verify if some visual patterns are apparent before proceeding to the EDA. In some studies, it may happen that all the relevant brain activations are located in a few regions, or even in one specific area. This is not the case in our dataset, where the activations seem to follow a more complex pattern. ","ea5750db":"And finally through the **axial view**. ","12f8ff5b":"# **Overview of fMRI data with NeuroVault API**\n\n**Ma\u00ebl Donoso**, Ph.D. in Cognitive and Computational Neuroscience\n\nOuroboros Neurotechnologies, Place de la Riponne 5, 1005 Lausanne, Switzerland","c3bb506e":"We can verify that we have indeed 26 subjects, labelled from 1 to 26, and that the distribution of the good\/neutral\/bad conditions and the no-power\/power conditions correspond to what is expected from the 2 x 3 design structure. We can also notice that **the format of each file is .nii.gz, corresponding to a compressed NIFTI file**. In the next section, in order to extract the 3D NumPy arrays from the compressed NIFTI files, we will use **NiBabel, a Python library dedicated to neuroimaging** (https:\/\/nipy.org\/nibabel\/nibabel_images.html). The corresponding package was imported at the beginning of the Notebook. ","f50235f7":"## **3. Files Overview**","10f1f2b5":"The scientific detail of this study is outside the scope of this project, but in summary, the authors investigated the brain regions involved in emotion, by having 26 subjects play an arcade game inside an fMRI scanner. Several types of statistical maps were computed, but the most important ones are the **'appraisal' maps**. These maps give information about the appraisal processes triggered by the game events, and are directly manipulated by the experimental design. \n\nThe 'appraisal' statistical maps follow a **2 x 3 design structure**, crossing 2 levels of 'coping potential' (no-power, power) with 3 levels of 'goal conduciveness' (good, neutral, bad). Therefore, for each of the 26 subjects, we have **6 'appraisal' statistical maps**, corresponding to the 6 possible combinations of conditions: no-power\/good, no-power\/neutral, no-power\/bad, power\/good, power\/neutral, power\/bad. We are particularly interested in the 3 levels of 'goal conduciveness' (good, neutral, bad), because the 3 levels allow for more classification possibilities, but also because these conditions are associated with the valence or affective quality experienced by the subject, which is arguably the most important variable in emotion studies. \n\nOverall, we will work with **156 files: 26 subjects x 6 combinations of conditions per subject**. Each file will contain a single statistical map in the NIFTI format. Therefore, we need a list of all the URLs pointing to these files in the NeuroVault repository. \n\nWe start by using the NeuroVault API to request the metadata of the study. ","52fa3170":"### Factor 4","75993c1d":"Now we simply verify that our dataset was successfully saved. ","0099cc56":"## **7. Conclusion**","88ed05fd":"We will also define a function to display a statistical map through the **middle sections of the sagittal, coronal and axial views**, in order to compare the different rescaling modes. Then, we will select the first image in our dataset, and start by running this function on this image with the original voxel size. ","66cffb94":"### Factor 3","f40a9c70":"## **2. Data Source**","d50ccce3":"In this Notebook, we will use a dataset extracted from **NeuroVault** (https:\/\/neurovault.org\/), an open data repository for brain maps, where researchers can publicly store and share brain images obtained from several neuroimaging techniques, including fMRI. All brain maps are stored in the NIFTI format (.nii) and normalized using the MNI template. This format and this template are both standard tools in neuroimaging studies. Furthermore, all data is distributed under CC0 license (https:\/\/neurovault.org\/FAQ), which means a public domain dedication and no copyright (https:\/\/creativecommons.org\/publicdomain\/zero\/1.0\/). \n\nSpecifically, we will use the following dataset: **https:\/\/neurovault.org\/collections\/8740\/**. This dataset comes from the following study, conducted at the University of Geneva: **Leit\u00e3o, J., Meuleman, B., Van De Ville, D., & Vuilleumier, P. (2020). Computational imaging during video game playing shows dynamic synchronization of cortical and subcortical networks of emotions. PLOS Biology, 18(11), e3000900. doi:10.1371\/journal.pbio.3000900**. \n\nThis dataset has several interesting characteristics, notably: 1) The statistical maps are given for every individual subject, while other studies give only the maps at the group level. 2) There are several experimental conditions, and therefore several statistical maps per subject. 3) One experimental condition has 3 levels, which offers more classification possibilities than 2 levels, and seems therefore more interesting for a classification task. 4) At the time of writing this Notebook, it is a recent study, published in November 2020. 5) The article presenting this study is publicly available, and can be found here: https:\/\/journals.plos.org\/plosbiology\/article?id=10.1371\/journal.pbio.3000900. \n\nLet's get started. ","045e79d1":"We have reached the end of this Notebook. In the next Notebook, we will perform an **EDA on our fMRI data**. ","8ae4c0be":"**Functional Magnetic Resonance Imaging (fMRI)** is one of the major tools used by neuroscientists for brain activity investigation. This neuroimaging technique measures the activity of the brain indirectly, by detecting the changes in the cerebral blood flow. For example, when the blood flow increases in a brain region during a certain experimental condition, in comparison with a baseline condition, we expect that this brain region is more involved during this experimental condition than during the baseline. \n\nIn a typical neuroimaging setup, the raw data obtained from the fMRI scanner is normalized through several preprocessing steps. Then, one or several regression models are applied, in order to discover how much of the brain activity can be explained by the experimental conditions or other relevant variables. The objective is to obtain statistical maps, i.e. a series of **3D images of the brain showing which cerebral regions are active under a defined condition.** These statistical maps are obtained on individual subjects, but can be averaged to perform group analyses. \n\nThe objective of a neuroimaging research using fMRI is to obtain relevant and generalizable knowledge about the brain activity. An extensive literature review is often performed by the authors of neuroimaging articles, in order to compare their results with those of other studies. Data science and machine learning methods provide a complementary approach. Notably, for a given fMRI dataset, **proving that machine learning models can be trained to classify and predict the brain activity could be a strong argument in favor of relevance and generalization.** \n\nIn this Notebook, we extract the fMRI data through the NeuroVault API, and perform a preliminary data overview. In the next Notebooks, we will further explore this dataset and apply **classification and regression models**. ","06b5030e":"We can verify that we have indeed **156 'appraisal' files**. Since we have 26 subjects in this dataset, according to these numbers we can easily guess that each subject is associated to 6 appraisal files, 1 expression file, 4 motivation files and 2 physiology files. Again, the scientific detail of this study is outside the scope of this project, and we will not explore further the other types of files. \n\nThe next step is to **list the URLs of all the appraisal files in a DataFrame**. It would also be interesting to include the subject, the good\/neutral\/bad conditions, and the no-power\/power conditions associated to these files. ","9cec2012":"If we look at the different versions of our statistical map, we notice that the different brain images show, as expected, a **rougher distribution of the same correlation patterns**. Also as expected, the memory gain factor is about 8 (2 x 2 x 2) for the rescaling using the factor 2, about 27 (3 x 3 x 3) for the rescaling using the factor 3, and more than 64 (4 x 4 x 4) for the rescaling using the factor 4. Therefore, the voxel size rescaling seem to preserve some of the statistical map structure, while significantly reducing its size. ","1b55b372":"Now we can do the same through the **coronal view**. ","cc03394c":"We finally get a glimpse of the **brain activity**. In this image, obtained through the middle section of the axial view, we can see that different brain regions show different patterns of correlation strength with the experimental condition, here the combination of conditions 'no-power\/good'. We must remember that this is only a 2D section of a larger 3D image. We also notice immediately that there is a high number of NaN values. As we mentioned earlier, the peripheral voxels are often removed in neuroimaging studies, since they are located outside the brain. Only the central voxels represent actually the brain of the subject. \n\nIt is important to note that the arrays do not represent the brains of the different subjects in their true proportions, but rather a **normalized version using the MNI template**, which is a standardized brain model. In other words, the brain data of every subject is transformed into a standard space, in order to allow meaningful comparisons and statistics. The normalization process is not perfect, but as a corrective measure, the data is typically smoothed with a Gaussian kernel, as it is the case in this study. One key implication of the normalization is that a **(xi, yi, zi) voxel from one subject and one condition is comparable to a (xi, yi, zi) voxel of another subject or another condition**. In other words, thanks to the normalization, we can compare our 3D images elementwise between several subjects or conditions. This elementwise comparison has one limitation, nevertheless. Since the position of the subjects in the fMRI scanner vary, sometimes the brains can move slightly outside the fMRI scope. When that happens, subjects can have missing data, i.e. additional voxels with NaN values. \n\nIn order to gain a better understanding of the structure of our statistical map, we can display a series of sections obtained through the **sagittal view**. ","9dd16b38":"Now we can request **all the images from the NeuroVault API**, and store all the data obtained in a single 4D NumPy array, where the first dimension will be the image index. ","c486b9d0":"If we take a look at the metadata, we can see that a lot of information is associated to each file. In fact, some of the columns contain the same information for all the files of the study. We can get a clearer sense of the metadata by removing these columns, and identifying how many files contain **statistical maps at the single-subject level**. ","7917e7db":"In contrast, the statistical maps of the same subject across different conditions look very similar. If we think about it from a neuroimaging perspective, this is quite logical. The difference between experimental conditions is relatively subtle, since these statistical maps represent variants of the same experimental paradigm. ","2389623f":"## **1. Introduction**"}}