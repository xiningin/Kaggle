{"cell_type":{"ed945c91":"code","c26d4b32":"code","e9f2d37e":"code","a3d2b133":"code","34c6ef98":"code","8c324cda":"code","bca5a548":"code","6878bc8c":"markdown","cc23e748":"markdown","51dacd4b":"markdown","6181e63f":"markdown","615335de":"markdown","dd569586":"markdown","af2b3fe5":"markdown","b7f31d18":"markdown","2b7069c9":"markdown"},"source":{"ed945c91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Model\nfrom keras.layers import Input, Reshape, Dot\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Concatenate, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.regularizers import l2","c26d4b32":"# DataFrame to store all imported data\nif not os.path.isfile('data.csv'):\n    data = open('data.csv', mode='w')\n\nfiles = ['..\/input\/netflix-prize-data\/combined_data_1.txt',\n         '..\/input\/netflix-prize-data\/combined_data_2.txt',\n         '..\/input\/netflix-prize-data\/combined_data_3.txt',\n         '..\/input\/netflix-prize-data\/combined_data_4.txt']\n\n# Remove the line with movie_id: and add a new column of movie_id\n# Combine all data files into a csv file\nfor file in files:\n  print(\"Opening file: {}\".format(file))\n  with open(file) as f:\n    for line in f:\n        line = line.strip()\n        if line.endswith(':'):\n            movie_id = line.replace(':', '')\n        else:\n            data.write(movie_id + ',' + line)\n            data.write('\\n')\ndata.close()\n\n# Read all data into a pd dataframe\ndf = pd.read_csv('data.csv', names=['movie_id', 'user_id','rating','date'])\n\ndf","e9f2d37e":"lite_rating_df = pd.DataFrame()\n\ngroup = df.groupby('user_id')['rating'].count()\ntop_users = group.sort_values(ascending=False)[:10000]\n\ngroup = df.groupby('movie_id')['rating'].count()\ntop_movies = group.sort_values(ascending=False)[:2000]\n\nlite_rating_df = df.join(top_users, rsuffix='_r', how='inner', on='user_id')\nlite_rating_df = lite_rating_df.join(top_movies, rsuffix='_r', how='inner', on='movie_id')\n\n# Re-name the users and movies for uniform name from 0..2000 and 10000\nuser_enc = LabelEncoder()\nlite_rating_df['user'] = user_enc.fit_transform(lite_rating_df['user_id'].values)\nmovie_enc = LabelEncoder()\nlite_rating_df['movie'] = movie_enc.fit_transform(lite_rating_df['movie_id'].values)\n\nn_movies = lite_rating_df['movie'].nunique()\nn_users = lite_rating_df['user'].nunique()\n\n# print(n_movies, n_users)\nlite_rating_df","a3d2b133":"X = lite_rating_df[['user', 'movie']].values\ny = lite_rating_df['rating'].values\n\n# Split train and test data (for test model performance at last)\nX_training, X_test, y_training, y_test = train_test_split(X, y, test_size=0.1)\n\n# Split train and validation data (to monitor model performance in training)\nX_train, X_val, y_train, y_val = train_test_split(X_training, y_training, test_size=0.1)\n\n# Set the embedding dimension d of Matrix factorization\ne_dimension = 50\n\nX_train_array = [X_train[:, 0], X_train[:, 1]]\nX_val_array = [X_val[:, 0], X_val[:, 1]]\nX_test_array = [X_test[:, 0], X_test[:, 1]]\n","34c6ef98":"# Build user and movie embedding matrix\nuser = Input(shape=(1,))\nu = Embedding(n_users, e_dimension, embeddings_initializer='he_normal',\n              embeddings_regularizer=l2(1e-6))(user)\nu = Reshape((e_dimension,))(u)\nmovie = Input(shape=(1,))\nm = Embedding(n_movies, e_dimension, embeddings_initializer='he_normal',\n              embeddings_regularizer=l2(1e-6))(movie)\nm = Reshape((e_dimension,))(m)\n\nx = Dot(axes=1)([u, m])\n# Build last deep learning layers \nx = Dense(128, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(1)(x)\n\nmodel = Model(inputs=[user, movie], outputs=x)\nmodel.compile(loss='mean_squared_error', \n              optimizer=Adam(lr=0.001), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()]\n              )\n\n# Set up for early stop if the validation loss stop improving for more than 1 epoch\ncallbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                patience=1,\n                                                ),\n                  # Saves the weights after every epoch\n                  keras.callbacks.ModelCheckpoint(  \n                      filepath='Model_1',\n                      monitor='val_loss',\n                      save_best_only=True,\n                      )]\n\n# Print model info summary\nmodel.summary()  \n\nhistory = model.fit(x=X_train_array, y=y_train, batch_size=64, epochs=20,\n                    verbose=1, \n                    callbacks=callbacks_list,\n                    validation_data=(X_val_array, y_val)\n                    )\n\n# Save the model (we should make a good habit of always saving our models after training)\nmodel.save(\"Model_1\")\n","8c324cda":"# Visualize the training and validation loss\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\n\nplt.plot(epochs, loss_values, 'ro', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","bca5a548":"m = tf.keras.metrics.RootMeanSquaredError()\nm.update_state(model.predict(X_test_array), y_test)\nm.result().numpy()\n","6878bc8c":"# Importing Data \n\nAs the raw data format is not readable as csv file, we need some pre-process steps to convert it into csv format and then import to pandas dataframe later\n","cc23e748":"# Pre-process data\n\nFrom dataframe df, let's take only a smaller dataset of 2000 top rated movies and 100000 top users (who gave the most rates) and save into new df: lite_rating_df","51dacd4b":"# Conclusion\n\nIn the test result, we can see that our model's RMSE is 0.7731, which is quite good and seemingly so much improved from the Cinematch's performance (0.9514) or the prize winner team \u201dBellKor\u2019s Pragmatic Chaos\u201d (0.8567) but that is not really true. As our model have not trained in the original massive dataset with more sparse matrix and testing in the qualifying dataset (Netflix's test data) is not possible since the competition closed, any comparation would hardly be correct.\n\nTherefore, in this notebook, my main purpose is to show a deep learning approach to the challenge which is simple and effective to apply with a decent accuracy. \nAny comment about further improvement or correction would be very welcome.\n","6181e63f":"# Build and train deep learning model\n\nThe embeddings is used to represent each user and each movie in the data. \nThe dot product of user embedding matrix (size: n_users x e_dimension) and movie embedding matrix (size: n_movies x e_dimension) is a good approximation of the rating from user for movie. The model's goal is to minimize the distqace between this dot product and the ratings (training target)\n","615335de":"Prepare data for training","dd569586":"# Introduction\n\nIn this notebook, I am going to build a recommendation system using deep learning trained on dataset of the famous Netflix Challenge competition.\n\nThe Netflix Challenge was an open competition to find the best collaborative filtering algorithm to predict user ratings for movies.\nThis challenge would be very useful for any practical application of predicting user's future rating and product's recommendation.\n\nThe dataset consists of ratings for 17,770 movies by 2,649,430 users. The challenge's goal is to develop a system that could beat the RMSE accuracy of 0.9514 from their in-house developed recommendation system (the Cinematch) by 10%. In 2009, the prize was awarded to team \u201dBellKor\u2019s Pragmatic Chaos\u201d with RMSE of 0.8567 or 10.06% improvement. However, most of winning solutions at that time was (too) complex and hard to be put into production.\n\nMoreover, to process this whole massive and sparse matrix dataset (only around 1 % of the entries are non-zero\/rated), it would take days to complete the whole training. Therefore for the sake of showing a new solution of deep learning, I would only process a smaller dataset of 2000 top rated movies and 10,000 top users (who gave the most rates). By focusing on this top 11% movies and 0.4% users, I still can retain 10% of the ratings from the whole data.\n\nIn this notebook, I am showing a recommendation system with Keras deep learning, which is quite simple but still can achieve decent accuracy. \n\n","af2b3fe5":"References:\n\n[1] https:\/\/developers.google.com\/machine-learning\/recommendation\/collaborative\/matrix\n\n[2] https:\/\/www.johnwittenauer.net\/deep-learning-with-keras-recommender-systems\/\n\n[3] Nicholas Ampazis. Large Scale Problem Solving with Neural Networks: The Netflix Prize Case, 2010\n\n[4] Cyril GarciaJ, et al. The Netflix Challenge, 2018\n\n[5] Fran\u00e7ois Chollet, Deep Learning with Python, 2018\n","b7f31d18":"Setup:","2b7069c9":"Test final model in the test data:"}}