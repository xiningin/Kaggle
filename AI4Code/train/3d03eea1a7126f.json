{"cell_type":{"4995c1e4":"code","5a627f27":"code","869285e8":"code","0d2a9cc1":"code","78de50ff":"code","01e035ac":"code","7272b291":"code","ca553f8f":"code","21f5ac02":"code","fa3a3569":"code","b3e7cec1":"code","f2344c5b":"code","7da43664":"markdown","9dffd590":"markdown","0bb5804b":"markdown","4919eddd":"markdown","a31329f3":"markdown","9f0fe2d2":"markdown","a78c0fc5":"markdown","f1c93c51":"markdown","0e717ecc":"markdown","fa03f1a2":"markdown","70948881":"markdown","2669d717":"markdown","09a6ff7e":"markdown"},"source":{"4995c1e4":"import pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelBinarizer, MinMaxScaler\nfrom xgboost import XGBClassifier","5a627f27":"# Use this line to install Intel's update to library if needed\n!pip install scikit-learn-intelex --progress-bar off >> \/tmp\/pip_sklearnex.log\n\n# Can give a small (or large) boost to speed depending on the available processor\nfrom sklearnex import patch_sklearn\npatch_sklearn()","869285e8":"df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv', index_col=False).drop(columns=['id'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv')\ntest_ids = test_df.id\ntest_df.drop(columns=['id'], inplace=True)\n\nlb = LabelBinarizer() # Need to map target values (true and false) into 1's and 0's\ndf['target'] = np.ravel(lb.fit_transform(df['target'])) # ravel makes y a 1d vector instead of a column vector\n\ncontinuous_cols = list(df.columns[:-1]) # All columns are continuous besides 'target', which is a binary label\n\ny = df.target\nX = df.drop(columns=['target']).to_numpy()","0d2a9cc1":"def evaluate_model(X, y, model):\n    cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)\n    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv_method, error_score='raise')\n    return scores","78de50ff":"xgb = XGBClassifier(n_estimators=100, tree_method='gpu_hist', gpu_id=0, eval_metric='auc', use_label_encoder=False, verbosity=0)\nprint(\"Cross Validation Score: \", np.mean(evaluate_model(X, y, xgb)))","01e035ac":"p = Pipeline([\n    ('scale', MinMaxScaler()),\n    ('model', XGBClassifier(n_estimators=200, max_depth=4, reg_lambda=60, reg_alpha=60,\n                            tree_method='gpu_hist', gpu_id=0, eval_metric='auc', use_label_encoder=False))\n])\n\nprint(\"Score: \", np.mean(evaluate_model(X, y, p)))","7272b291":"p_2 = Pipeline([\n    ('scale', MinMaxScaler()),\n    ('logr', LogisticRegression(solver='sag', n_jobs=-1))\n])\n\nprint(\"Score: \", np.mean(evaluate_model(X, y, p_2)))","ca553f8f":"p_2.fit(X, y)\npd.DataFrame({'id': test_ids, 'target': p_2.predict_proba(test_df)[:,1]}).to_csv('logregr_submission.csv', index=False)","21f5ac02":"p_3 = Pipeline([\n    ('scale', MinMaxScaler([0,1])), # Naive Bayes' input can't contain negative values, so scale to positive range\n    ('nb', MultinomialNB())\n])\n\nprint(\"Score: \", np.mean(evaluate_model(X, y, p_3)))","fa3a3569":"to_ens = [('xgb', XGBClassifier(n_estimators=200, max_depth=4, reg_lambda=60, reg_alpha=60, tree_method='gpu_hist', gpu_id=0, \n                        eval_metric='auc', use_label_encoder=False)),\n          ('logr', LogisticRegression(solver='sag', n_jobs=-1))]","b3e7cec1":"final_pipe = Pipeline([\n    ('scale', MinMaxScaler()),\n    ('ensemble', VotingClassifier(to_ens, voting='soft'))\n])\n\nprint(\"Score: \", np.mean(evaluate_model(X, y, final_pipe)))","f2344c5b":"final_pipe.fit(X, y)\npd.DataFrame({'id': test_ids, 'target': final_pipe.predict_proba(test_df)[:,1]}).to_csv('ensemble_submission.csv', index=False)","7da43664":"### Getting Competition Data\n\nThe target column contains 'True' and 'False' values instead of 1 and 0, so LabelBinarizer is used to process it. Also the features and target columns are separated into the variables X and y.","9dffd590":"### Voting Ensemble\n\nFirst the list of estimators needs to be created. Add more models to this list to try different ensembles.","0bb5804b":"Using Cross Validation will give us a better idea of whether a parameter change or preprocessing step has improved the model. The evaluate_model function performs multiple cross validations, which gives us more consistent scores to compare models and sets of parameters.","4919eddd":"The ensemble of estimators is not an improvement over all individual estimators. Next step: tuning XGBoost and adding more models to the ensemble. \n\n#### The predictions for LogisticRegression are the current submission for this version of this notebook. \n\n### Thanks for reading!","a31329f3":"Best params so far:\n\nXGBClassifier(n_estimators=200, max_depth=4, reg_lambda=60, reg_alpha=60,\n                            tree_method='gpu_hist', gpu_id=0, eval_metric='auc', use_label_encoder=False)","9f0fe2d2":"### Adding scaling and tuning parameters\n\nThe XGB model for the ensemble has the same parameters as the one defined in this next code cell.","a78c0fc5":" # November TPS - Building an Ensemble","f1c93c51":"#### Baseline XGBoost","0e717ecc":"## Building the Ensemble","fa03f1a2":"Using the 'sag' solver because the scikit-learn documentation it is recommended with medium to large size datasets. The default solver is much slower to train. With default parameters otherwise, this model performs the best.","70948881":"#### Logistic Regression","2669d717":"#### Intel Scikit-learn Patch\nSource: https:\/\/intel.github.io\/scikit-learn-intelex\/","09a6ff7e":"#### Naive Bayes\n\nI experimented with adding naive bayes but it did not improve the ensemble's score. I may try to experiment with the parameters, but it may be more worthwile finding other models that can score above 0.73 without tuning."}}