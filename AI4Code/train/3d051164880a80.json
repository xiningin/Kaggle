{"cell_type":{"7d6d8b02":"code","fefdd7a5":"code","ce825706":"code","841b7604":"code","ad64e548":"code","29bf6a24":"code","d7bcdc64":"code","b02f4dbd":"code","00ded3eb":"code","7990c1c0":"code","8601a3c8":"code","b8cb55cc":"code","fcaeadbf":"code","d6b72db5":"code","2072401f":"code","5696155e":"code","0e484b62":"code","25a2336e":"code","4c835a23":"code","964f8fd2":"code","76241006":"code","8865f7a3":"code","360d5d4c":"code","42c35037":"code","72073471":"code","b5be3cb5":"code","bcae798d":"code","b4421255":"code","adee44e8":"code","cda8e355":"code","590ca991":"code","04f2d38e":"code","7cab24b7":"code","d254c017":"code","755dc97e":"code","e93e791f":"code","dff8f56f":"code","1ca1dce1":"code","06f3e688":"code","d82b0fc0":"code","738a38d4":"code","634cd600":"code","601b2315":"code","12a31f33":"code","b6a81314":"code","c402f295":"code","256d9cbc":"code","71ac11b2":"code","296106db":"code","77ed2ede":"code","658daa0d":"code","14b786ab":"markdown","f7c84836":"markdown","ec8cc445":"markdown","79adf037":"markdown","dbc6e82a":"markdown","5beca9f6":"markdown","589cf2a5":"markdown","bccb55e5":"markdown"},"source":{"7d6d8b02":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\n\nimport spacy\nimport tensorflow_hub as hub\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fefdd7a5":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","ce825706":"train_df.shape","841b7604":"test_df.shape","ad64e548":"submission_df.shape","29bf6a24":"train_df.head()","d7bcdc64":"test_df.head()","b02f4dbd":"submission_df.head()","00ded3eb":"train_df.info()","7990c1c0":"train_df.describe()","8601a3c8":"train_df.isnull().sum()","b8cb55cc":"sns.heatmap(train_df.isnull(), cbar = False, yticklabels = False, cmap = 'plasma')\nplt.show()","fcaeadbf":"train_df.target.value_counts(normalize = True)","d6b72db5":"sns.countplot(train_df.target, edgecolor = (0,0,0), linewidth = 2)\nplt.xlabel('Target')\nplt.ylabel('Frequency')\nplt.title('Distribution of Target values')\nplt.show()","2072401f":"fig, axes = plt.subplots(1, 2, figsize = (14,5))\nax1, ax2 = axes.ravel()\nsns.distplot(train_df[train_df['target'] == 0].text.str.len(), ax = ax1, color = 'green')\nax1.set_xlabel('Non Disaster Tweets')\nax1.set_title('Distribution of Non Disaster tweets')\nsns.distplot(train_df[train_df['target'] == 1].text.str.len(), ax = ax2, color = 'red')\nax2.set_xlabel('Disaster Tweets')\nax2.set_title('Distribution of Disaster tweets')\nax1.set_ylabel('Frequency')\nplt.show()","5696155e":"plt.figure(figsize = (15,15))\nnondisaster_words = ' '.join(word for word in train_df[train_df['target'] == 0].text)\nnondisaster_wordcloud = WordCloud(max_words = 50, max_font_size = 100, background_color = 'white', colormap = 'inferno').\\\ngenerate(nondisaster_words)\nplt.imshow(nondisaster_wordcloud, interpolation = 'bicubic')\nplt.title('Frequent Non Disaster Words (Training Dataset)', fontsize = 24, color = 'green')\nplt.axis('off')\nplt.show()","0e484b62":"plt.figure(figsize = (15,15))\ndisaster_words = ' '.join(word for word in train_df[train_df['target'] == 1].text)\ndisaster_wordcloud = WordCloud(max_words = 50, max_font_size = 75, background_color = 'white', colormap = 'inferno').\\\ngenerate(disaster_words)\nplt.imshow(disaster_wordcloud, interpolation = 'bicubic')\nplt.title('Frequent Disaster Words (Training Dataset)', fontsize = 24, color = 'red')\nplt.axis('off')\nplt.show()","25a2336e":"# Convert to lowercase\ntrain_df.loc[:,'text'] = train_df.text.apply(lambda x : str.lower(x))\ntest_df.loc[:,'text'] = test_df.text.apply(lambda x : str.lower(x))","4c835a23":"# Remove digits\ntrain_df['text'] = train_df['text'].str.replace(r'\\d+', '', regex = True)\ntest_df['text'] = test_df['text'].str.replace(r'\\d+', '', regex = True)","964f8fd2":"# Remove unicode characters\ntrain_df['text'] = train_df['text'].str.replace('\u00fb.*.*', '', regex = True)\ntest_df['text'] = test_df['text'].str.replace('\u00fb.*.*', '', regex = True)","76241006":"# Remove hyperlinks\ntrain_df['text'] = train_df['text'].str.replace('http.*.*', '', regex = True)\ntest_df['text'] = test_df['text'].str.replace('http.*.*', '', regex = True)","8865f7a3":"tokenizer = RegexpTokenizer(r'\\w+')\ntrain_df['tokens'] = train_df['text'].map(tokenizer.tokenize)\ntest_df['tokens'] = test_df['text'].map(tokenizer.tokenize)","360d5d4c":"stops = stopwords.words('english')\ntrain_df['tokens'] = train_df['tokens'].apply(lambda x : [item for item in x if item not in stops])\ntest_df['tokens'] = test_df['tokens'].apply(lambda x : [item for item in x if item not in stops])","42c35037":"train_df.head()","72073471":"test_df.head()","b5be3cb5":"lemmatized_col = []\n\nnlp = spacy.load('en_core_web_sm')\nfor i in range(len(train_df['tokens'])):\n    word = ''\n    sentence = ' '.join(word for word in train_df['tokens'][i])\n    processed_word = nlp(sentence)\n    for token in processed_word:\n        word = word + token.lemma_ + ' '\n    lemmatized_col.append(word)","bcae798d":"train_df['lemmatized'] = lemmatized_col","b4421255":"train_df.head()","adee44e8":"lemmatized_col = []\nfor i in range(len(test_df['tokens'])):\n    word = ''\n    sentence = ' '.join(word for word in test_df['tokens'][i])\n    processed_word = nlp(sentence)\n    for token in processed_word:\n        word = word + token.lemma_ + ' '\n    lemmatized_col.append(word)","cda8e355":"test_df['lemmatized'] = lemmatized_col","590ca991":"X = train_df['lemmatized']\nY = train_df['target']","04f2d38e":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify = Y)","7cab24b7":"TfidfVec = TfidfVectorizer(max_df = 0.9, min_df = 2, max_features = 2000, ngram_range = (1,1))\nTfidf = TfidfVec.fit_transform(X_train)\nTfidf_valid = TfidfVec.transform(X_valid)\nTfidf_test = TfidfVec.transform(test_df['lemmatized'])","d254c017":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver = 'liblinear')\nlr.fit(Tfidf, Y_train)","755dc97e":"from sklearn.metrics import roc_auc_score,roc_curve,f1_score, confusion_matrix\nprint ('Training f-1 score: %.4f' % f1_score(Y_train, lr.predict(Tfidf)))","e93e791f":"# predicting on the validation set\nvalid_prediction = lr.predict(Tfidf_valid)\nvalid_prediction = valid_prediction.astype(np.int)","dff8f56f":"print ('Logistic Regression F1 score: %.4f' % f1_score(Y_valid, valid_prediction))","1ca1dce1":"nb = MultinomialNB(alpha = 0.1).fit(Tfidf, Y_train)","06f3e688":"nb_prediction = nb.predict(Tfidf_valid).astype(int)","d82b0fc0":"print ('NB F1 score: %.4f' % f1_score(Y_valid, nb_prediction))","738a38d4":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","634cd600":"import tokenization","601b2315":"def bert_encode(texts, tokenizer, max_len = 512):\n    tokens = []\n    masks = []\n    segments = []\n    for text in texts:\n        text = tokenizer.tokenize(text)\n        text = text[:max_len - 2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        token_ids = tokenizer.convert_tokens_to_ids(input_sequence)\n        token_ids += [0] * pad_len\n        mask_ids = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        tokens.append(token_ids)\n        masks.append(mask_ids)\n        segments.append(segment_ids)\n        \n    return np.array(tokens), np.array(masks), np.array(segments)","12a31f33":"def build_model(bert_layer, max_len = 512):\n    word_input = Input(shape = (max_len, ), dtype = tf.int32, name = \"word_input\")\n    mask_input = Input(shape = (max_len, ), dtype = tf.int32, name = 'mask_input')\n    segment_input = Input(shape = (max_len, ), dtype = tf.int32, name = 'segment_input')\n    \n    _, sequence_output = bert_layer([word_input, mask_input, segment_input])\n    model_output = sequence_output[:, 0,:]\n    out = Dense(1, activation = 'sigmoid')(model_output)\n    \n    model = Model(inputs = [word_input, mask_input, segment_input], outputs = out)\n    model.compile(Adam(lr = 6e-6), loss = 'binary_crossentropy', metrics = ['accuracy'])\n    \n    return model","b6a81314":"url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert = hub.KerasLayer(url, trainable = True)","c402f295":"vocab_file = bert.resolved_object.vocab_file.asset_path.numpy()\nlower_case = bert.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, lower_case)","256d9cbc":"train_input = bert_encode(train_df.text.values, tokenizer, 160)\ntest_input = bert_encode(test_df.text.values, tokenizer, 160)\nlabels = train_df.target.values","71ac11b2":"model = build_model(bert, max_len = 160)\nmodel.summary()","296106db":"history = model.fit(train_input, labels, validation_split = 0.2, epochs = 3, batch_size = 16)","77ed2ede":"final_predictions = model.predict(test_input)","658daa0d":"submission_df['target'] = final_predictions.round().astype(int)\nsubmission_df.to_csv('lr_sub.csv', index=False)","14b786ab":"**Thus location column has the maximum number of null values**","f7c84836":"**Data cleaning**","ec8cc445":"**Visualization also shows that location column is not contributing any meaningful information**","79adf037":"**Let's check the null values in columns**","dbc6e82a":"**Let's visualize distribution of length of different tweets**","5beca9f6":"**Distribution of target values to check class imbalance**","589cf2a5":"**Let's try with BERT**","bccb55e5":"**Let's try to improve the model with Multinomial NB**"}}