{"cell_type":{"62333137":"code","cd88113e":"code","1bdd5389":"code","0cf45f55":"code","aff0775e":"code","d456f8a1":"code","956c3e86":"code","d0c3572f":"code","de82da94":"code","7f32cd93":"code","cd4d035e":"code","afa7ca71":"code","412b5456":"code","abaf059e":"code","7299002e":"code","9493fcd4":"code","deb1fb16":"code","0c19866b":"code","cc8a964b":"code","05680c16":"code","16255227":"code","4868ea9c":"code","517401f7":"code","29927c7e":"code","219918c9":"code","8f6ec7b7":"code","3a462605":"code","78b2f3af":"code","f52283e5":"code","73556d19":"markdown","30d46dd5":"markdown","4ba76b22":"markdown","769b442d":"markdown","74f20006":"markdown","bb590dff":"markdown","717cc21b":"markdown","cbf9a405":"markdown","651b62b0":"markdown","2a285a26":"markdown","c55496e7":"markdown","17b2e4ed":"markdown","15d35197":"markdown"},"source":{"62333137":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport bz2 # for excating data\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cd88113e":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')","1bdd5389":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","0cf45f55":"train_file_lines[0]","aff0775e":"train_file_lines[10]","d456f8a1":"del train_file, test_file","956c3e86":"train_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","d0c3572f":"train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]","de82da94":"train_sentences[0]","7f32cd93":"import re\n\nfor i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n    \nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])","cd4d035e":"train_sentences[0]","afa7ca71":"for i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","412b5456":"train_sentences[0]","abaf059e":"train_sentences_df = pd.DataFrame(train_sentences,columns = ['review'])\ntrain_sentences_df.head()","7299002e":"train_sentences_df['word_count'] = train_sentences_df['review'].apply(lambda x : len(x.split()))\ntrain_sentences_df['char_count'] = train_sentences_df['review'].apply(lambda x : len(x.replace(\" \",\"\")))\ntrain_sentences_df['word_density'] = train_sentences_df['word_count'] \/ (train_sentences_df['char_count'] + 1)","9493fcd4":"train_sentences_df.head()","deb1fb16":"train_sentences_df.describe()","0c19866b":"import matplotlib.pyplot as plt\nimport seaborn as sns","cc8a964b":"fig, ax = plt.subplots(1, 3, figsize=(16, 6))\ndp=sns.distplot(train_sentences_df['word_count'],ax=ax[0])\ndp=sns.distplot(train_sentences_df['char_count'],ax=ax[1])\ndp=sns.distplot(train_sentences_df['word_density'],ax=ax[2])\nplt.show()","05680c16":"import keras\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import SimpleRNN\nfrom keras import initializers","16255227":"tokenizer = Tokenizer(num_words=30)\ntokenizer.fit_on_texts(train_sentences)\nsequences_train = tokenizer.texts_to_sequences(train_sentences)\n#test data\ntokenizer.fit_on_texts(test_sentences)\nsequences_test = tokenizer.texts_to_sequences(test_sentences)","4868ea9c":"#Identifying maxlenght of reviews\nmaxlen_train = 0\nmaxlen_test = 0\nfor review in range(len(train_sentences)):\n    numberofwords = len(sequences_train[review])\n    if numberofwords > maxlen_train :\n        maxlen_train = numberofwords\n        \n        \nfor review in range(len(test_sentences)):\n    numberofwords = len(sequences_test[review])\n    if numberofwords > maxlen_test :\n        maxlen_test = numberofwords\n        \n        \n        \nprint('Maximun word count train data',maxlen_train)\nprint('Maximun word count test data',maxlen_test)","517401f7":"# This pads (or truncates) the sequences so that they are of the maximum length\nx_train = sequence.pad_sequences(sequences_train, maxlen=maxlen_train)\nx_test = sequence.pad_sequences(sequences_test, maxlen=maxlen_test)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)","29927c7e":"x_train[0,:]","219918c9":"x_test[0,:]","8f6ec7b7":"## Let's build a RNN\nmax_features = 20000\n\nrnn_hidden_dim = 5\nword_embedding_dim = 50\nmodel_rnn = Sequential()\nmodel_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence and embeds it in a 50-dimensional vector\nmodel_rnn.add(SimpleRNN(rnn_hidden_dim,\n                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n                    recurrent_initializer=initializers.Identity(gain=1.0),\n                    activation='relu',\n                    input_shape=x_train.shape[1:]))\n\nmodel_rnn.add(Dense(1, activation='sigmoid'))","3a462605":"## Note that most of the parameters come from the embedding layer\nmodel_rnn.summary()","78b2f3af":"rmsprop = keras.optimizers.RMSprop(lr = .0001)\n\nmodel_rnn.compile(loss='binary_crossentropy',\n              optimizer=rmsprop,\n              metrics=['accuracy'])","f52283e5":"model_rnn.fit(x_train, x_train,\n          epochs=10,\n          validation_data=(x_test, x_test))","73556d19":"Removing any web URL ","30d46dd5":"*Add some feature such as word count, charater count and word density*\n\n> Here we done this for only train data need not to do with test becuase it not affect any sentiment analysis ","4ba76b22":"# EDA","769b442d":"Slipting lable and orginal data.","74f20006":"# Amazon Reviews for Sentiment Analysis","bb590dff":"> 1. Here we create list of train and test and file. \n\n> 2. Here we have label added to file such as __lable__1, and __lable__2.\n\n> 3. After the lable we have reviews for product \n\n> 4. Rviews have many symbols and other character","717cc21b":"**Here is some basic EDA fo data set as it is fully text data so we can not use all data visualization**\n\n> word, character count and and word density does not affect any sentiment analysis where we need only text data","cbf9a405":"*Create Lists containing Train & Test sentences*","651b62b0":"Here we spilt lable and reviews \n\n> we mark label 1 as 1 and label 2 as 0 \n\nSame for test ","2a285a26":"*Now data is ready to train*","c55496e7":"# Model Implemention\n\n> We using RNN ","17b2e4ed":"*Convert from raw binary strings to strings that can be parsed*","15d35197":"*Importing train and test file*"}}