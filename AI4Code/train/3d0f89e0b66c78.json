{"cell_type":{"c567140d":"code","7048aedf":"code","4b03e70d":"code","d30834fc":"code","a5956d28":"code","d50a0106":"code","eda20699":"code","dc0b84d2":"code","428e1e27":"code","e264a6d8":"code","16b48fdb":"code","48e0df5d":"code","53dc5f21":"code","66059602":"code","e2dd214a":"code","84814c12":"code","8b1a0c6c":"code","121ac608":"code","8896af49":"code","cead1850":"code","778382fe":"code","54809b94":"code","0ddad40e":"code","564f1b62":"code","63c708e4":"code","9a1cedf5":"code","fe714f31":"code","d9211bec":"code","a31b62af":"code","29f04d43":"code","d42cd9bc":"code","2f4d09fb":"code","e759974d":"code","6d74ea24":"code","acb5dfd1":"code","cd4bee2e":"code","5715bf07":"code","c96f97e4":"code","d9cc3e04":"code","ea6b757b":"code","fbedce9a":"code","7f16fe64":"code","d59d1066":"code","ac80d540":"code","546b7b21":"code","450c30c2":"code","57b83b04":"code","5a50af32":"code","ca95f00b":"code","7e2125f4":"code","f81502e5":"code","428ce6ab":"code","921f37ba":"code","325f8989":"code","4362f623":"code","b9854dbb":"code","009099e6":"code","5b3964c7":"code","91ccbeed":"code","12fabe50":"code","ca17ecf2":"code","9c26e041":"markdown","12700acb":"markdown","40e43ca4":"markdown","7a247cf9":"markdown","759996e1":"markdown","5a0b43de":"markdown","a394a524":"markdown","bce4526c":"markdown","9a0b83d6":"markdown","43ca0056":"markdown","e4b5bca4":"markdown","11a75f68":"markdown","05fbd3b5":"markdown","c7a29c3a":"markdown","5a19af69":"markdown","765adfb3":"markdown"},"source":{"c567140d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \npd.set_option('display.max_columns',5000)\npd.set_option('display.max_rows',100)\npd.set_option('display.width',10000)\n# Any results you write to the current directory are saved as output.","7048aedf":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/ieee-fraud-detection\/sample_submission.csv\")\ntest_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")\ntest_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntrain_identity = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_transaction = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")","4b03e70d":"test_identity.head(5)","d30834fc":"train_identity.describe()","a5956d28":"#lets look at nulls\n\ntrain_identity.isnull().any()\n\ntrain_identity.isnull().sum()","d50a0106":"train_transaction.head(5)","eda20699":"train_transaction.shape","dc0b84d2":"merge_df = pd.merge(left=train_transaction,right=train_identity,how=\"left\",on=\"TransactionID\")\n","428e1e27":"merge_df_test = pd.merge(left=test_transaction,right=test_identity,how=\"left\",on=\"TransactionID\")","e264a6d8":"merge_df.set_index(keys='TransactionID',inplace=True)\n","16b48fdb":"merge_df_test.set_index(keys='TransactionID',inplace=True)","48e0df5d":"#merge_df.head(5)\nmerge_df_test.head(5)","53dc5f21":"# check how many fraud transactions are there in the dataset\n\nimport seaborn as sns\n\nsns.countplot(x='isFraud',data=merge_df)","66059602":"# so less then 4 perecent of the transaction is fraud \n## this would be interesting as class distribution has a lot of difference\nround((len(merge_df[merge_df['isFraud']==1])\/len(merge_df))*100,2)","e2dd214a":"##intutive column selection for now \nmerge_df.columns","84814c12":"cols = ['TransactionID', 'isFraud', 'TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5',\n         'card6','P_emaildomain','C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14','DeviceType', 'DeviceInfo']\n\n","8b1a0c6c":"merge_df['TransactionAmt'].isnull().any()","121ac608":"merge_df_test['TransactionAmt'].isnull().any()","8896af49":"# lets draw the histogram of transaction amount\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n#plt.hist(x=merge_df['TransactionAmt'])\n\n#plt.scatter(x=merge_df['TransactionAmt'])\n\nmerge_df['TransactionAmt']\n\nsns.boxplot(merge_df['TransactionAmt'])\n","cead1850":"from scipy.stats import iqr\n\niqr(merge_df['TransactionAmt'])\n","778382fe":"merge_df['ProductCD'].isnull().any()","54809b94":"merge_df_test['ProductCD'].isnull().any()","0ddad40e":"# let's look at the unique values\n\nmerge_df['ProductCD'].value_counts()","564f1b62":"# plot to see if any pattern in product type and fraud \n\nsns.countplot(x='ProductCD',hue='isFraud',data=merge_df)\n\n#doesn't seem like there is a pattern here ","63c708e4":"merge_df[['card1','card2','card3','card4','card5','card6']].isnull().any()\n\n#merge_df['card2'].value_counts()\n\n\n# around 1.5% percent values are \n\n","9a1cedf5":"merge_df_test[['card1','card2','card3','card4','card5','card6']].isnull().any()","fe714f31":"\nmerge_df.loc[merge_df['card2'].isna(),'card2']=321","d9211bec":"\nmerge_df_test.loc[merge_df_test['card2'].isna(),'card2']=321","a31b62af":"merge_df.loc[merge_df['card3'].isna(),'card3']=150","29f04d43":"merge_df_test.loc[merge_df_test['card3'].isna(),'card3']=150","d42cd9bc":"merge_df.loc[merge_df['card4'].isna(),'card4']='visa' ","2f4d09fb":"merge_df_test.loc[merge_df_test['card4'].isna(),'card4']='visa' ","e759974d":"merge_df.loc[merge_df['card5'].isna(),'card5']=226","6d74ea24":"merge_df_test.loc[merge_df_test['card5'].isna(),'card5']=226","acb5dfd1":"merge_df.loc[merge_df['card6'].isna(),'card6']='debit'","cd4bee2e":"merge_df_test.loc[merge_df_test['card6'].isna(),'card6']='debit'","5715bf07":"merge_df[['card1','card2','card3','card4','card5','card6']].isnull().any()","c96f97e4":"merge_df_test[['card1','card2','card3','card4','card5','card6']].isnull().any()","d9cc3e04":"cols = ['TransactionAmt','ProductCD','card1', 'card2', 'card3', 'card4', 'card5',\n          'card6','isFraud']","ea6b757b":"# Lets get the subset of the columns\n\nmerge_df = merge_df[cols]\n","fbedce9a":"# Lets get the subset of the columns\ncols_test = ['TransactionAmt','ProductCD','card1', 'card2', 'card3', 'card4', 'card5',\n          'card6']\nmerge_df_test = merge_df_test[cols_test]\n","7f16fe64":"#print(merge_df)\ncor = merge_df.corr()\n#print(cor)\nsns.heatmap(cor)","d59d1066":"merge_df","ac80d540":"from sklearn.preprocessing import LabelEncoder\n\nle1 = LabelEncoder().fit(merge_df['ProductCD'])\nmerge_df['ProductCD'] = le1.transform(merge_df['ProductCD'])\nmerge_df_test['ProductCD'] = le1.transform(merge_df_test['ProductCD'])\n\nle2 = LabelEncoder().fit(merge_df['card4'])\nmerge_df['card4'] = le2.transform(merge_df['card4'])\nmerge_df_test['card4'] = le2.transform(merge_df_test['card4'])\n\nle3 = LabelEncoder().fit(merge_df['card6'])\nmerge_df['card6'] = le3.transform(merge_df['card6'])\nmerge_df_test['card6'] = le3.transform(merge_df_test['card6'])\n\n","546b7b21":"print(merge_df)","450c30c2":"from sklearn.model_selection import train_test_split\n\nX= merge_df.iloc[:,:-1]\ny = merge_df.iloc[:,-1]\n#print(y)\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)","57b83b04":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# rf = RandomForestClassifier()\n# param = {'n_estimators':[150],'max_depth':[9]}\n\n# grid_model = GridSearchCV(rf,param_grid=param,scoring='roc_auc')\n\n# grid_model.fit(X,y)\n\n# print(grid_model.best_estimator_)\n# print(grid_model.best_score_)\n# print(grid_model.best_params_)\n\n","5a50af32":"from sklearn.ensemble import GradientBoostingClassifier\n\ngf = GradientBoostingClassifier(learning_rate=0.1,n_estimators=150,max_depth=9).fit(X_train,y_train)\n\ny_pred = gf.predict(X_test)\n\n","ca95f00b":"print(gf.score(X_test,y_test))","7e2125f4":"## Let's predict using GBf\n\nmerge_df_test","f81502e5":"# so we are getting a score of 86%\nfrom sklearn.metrics import roc_auc_score\ny_score = gf.predict_proba(X_test)[:,1]\nprint(roc_auc_score(y_test,y_score))\n","428ce6ab":"y_test_predict = gf.predict_proba(merge_df_test)[:,1]","921f37ba":"result_df = pd.DataFrame()\nresult_df['TransactionID'] = merge_df_test.index\nresult_df['isFraud'] = y_test_predict","325f8989":"result_df.to_csv('result1.csv',index=False)","4362f623":"len(result_df[result_df['isFraud']>0.5])\/len(result_df)*100","b9854dbb":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X_train)\nX_train_scaled =scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nmerge_df_test_scaled = scaler.transform(merge_df_test)","009099e6":"# from sklearn.neural_network import MLPClassifier\n\n# mlp = MLPClassifier(hidden_layer_sizes=(13,13,13),max_iter=500)\n# mlp.fit(X_train_scaled,y_train)","5b3964c7":"# y_test_predict_nlp","91ccbeed":"# y_test_predict_nlp = mlp.predict_proba(merge_df_test_scaled)[:,1]\n# result_df = pd.DataFrame()\n# result_df['TransactionID'] = merge_df_test.index\n# result_df['isFraud'] = y_test_predict_nlp\n\n# result_df.to_csv('result2.csv',index=False)\n","12fabe50":"import xgboost as xgb\n\nmodel = xgb.XGBClassifier(n_estimators=500,\n                        n_jobs=4,\n                        max_depth=9,\n                        learning_rate=0.05,\n                        subsample=0.9,\n                        colsample_bytree=0.9)\n\nmodel.fit(X,y)\n\n\n","ca17ecf2":"temp =model.predict_proba(merge_df_test)[:,1]\nresult_df3 = pd.DataFrame()\nresult_df3['TransactionID'] = merge_df_test.index\nresult_df3['isFraud'] = temp\nresult_df3.to_csv('result5.csv',index=False)","9c26e041":"All done let's get the score now ","12700acb":"#Before we move any further, let's make a basic baseline model and try to see how it performs ","40e43ca4":"Assigning these null values the most frequently occuring value in the system","7a247cf9":"Lets try to apply XGBoost and see if results are better","759996e1":"Lets start by applying Random forest classifier , we would be using GridsearchCV to get the best model output ","5a0b43de":"Setting indexes which would be used later to submit the prediction ","a394a524":"We are going to join the two dataframes to give us one result set","bce4526c":"Load the files ","9a0b83d6":"Convert stirngs to numerical features","43ca0056":"we are going to apply GBC next , the paramter values are select by running them through GridSearch for simplicity only placing the apply code here","e4b5bca4":"Create train test split","11a75f68":"Let's look at the correlation heat map","05fbd3b5":"Let's get done with the shenanigans ","c7a29c3a":"Let's start looking at the data ","5a19af69":"Checking what percentage of data is actually fraud","765adfb3":"Let's try appling Neural Network to the problem on hand"}}