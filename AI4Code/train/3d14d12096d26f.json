{"cell_type":{"aa6b172c":"code","9cbbc129":"code","6ad97c3e":"code","bd81b602":"code","1e43361d":"code","60cd2821":"code","a84f4e8a":"code","1a03b50a":"code","432d0a08":"code","5461e862":"code","1c929055":"code","19b33fb9":"code","94632a0a":"code","fca65f0e":"code","4497062a":"code","b13a8338":"code","770c6e99":"code","18b278e5":"code","706475ba":"code","d1c95920":"markdown","b5c5fb62":"markdown","5840c679":"markdown","bc3ffe2f":"markdown","cd08ecbe":"markdown","87890d25":"markdown","5257fa1f":"markdown","7b02517a":"markdown","be831ad0":"markdown","c31db996":"markdown","3deb25a9":"markdown"},"source":{"aa6b172c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold","9cbbc129":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","6ad97c3e":"train.head()","bd81b602":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/train\/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/test\/\" + identifier + \".jpg\")","1e43361d":"train.head()","60cd2821":"train[\"Pawpularity\"].hist()","a84f4e8a":"tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\nimage_size = 128\nbatch_size = 128","1a03b50a":"def preprocess(image_url, tabular):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    return (image, tabular[1:]), tf.cast(tabular[0], tf.float32)","432d0a08":"def build_tabular_model(inputs):\n    width = 64\n    depth = 3\n    activation = \"relu\"\n    dropout = 0.1\n    x = keras.layers.Dense(\n            width, \n            activation=activation\n        )(inputs)\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n        x = keras.layers.Dense(\n            width, \n            activation=activation\n        )(x)\n        x = keras.layers.Dropout(dropout)(x)\n        if (i + 1) % 3 == 0:\n            x = keras.layers.BatchNormalization()(x)\n            x = keras.layers.Concatenate()([x, inputs])\n    return x","5461e862":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean((y_true -  y_pred) ** 2))","1c929055":"def block(x, filters, kernel_size, repetitions, pool_size=2, strides=2):\n    for i in range(repetitions):\n        x = tf.keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size, strides)(x)\n    return x","19b33fb9":"def get_model():\n    image_inputs = tf.keras.Input((image_size, image_size , 3))\n    tabular_inputs = tf.keras.Input(len(tabular_columns))\n    image_x = block(image_inputs, 8, 3, 2)\n    image_x = block(image_x, 16, 3, 2)\n    image_x = block(image_x, 32, 3, 2)\n    image_x = block(image_x, 64, 3, 2)\n    image_x = block(image_x, 128, 3, 2)\n    image_x = tf.keras.layers.GlobalAveragePooling2D()(image_x)\n    \n    tabular_x = build_tabular_model(tabular_inputs)\n    \n    x = tf.keras.layers.Concatenate(axis=1)([image_x, tabular_x])\n    output = tf.keras.layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[image_inputs, tabular_inputs], outputs=[output])\n    return model","94632a0a":"model = get_model()\ntf.keras.utils.plot_model(model, show_shapes=True)","fca65f0e":"model.summary()","4497062a":"image = np.random.normal(size=(1, image_size, image_size, 3))\ntabular = np.random.normal(size=(1, len(tabular_columns)))\nprint(image.shape, tabular.shape)\nprint(model((image, tabular)).shape)","b13a8338":"tf.keras.backend.clear_session()\nmodels = []\nhistorys = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n# For the current random state, 5th fold can generate a better validation rmse and faster convergence.\ntrain_best_fold = True\nbest_fold = 0\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_best_fold and index != best_fold:\n        continue\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    \n    optimizer = tf.keras.optimizers.Adam(1e-3)\n    \n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess).shuffle(512).batch(batch_size).cache().prefetch(2)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess).batch(batch_size).cache().prefetch(2)\n    model = get_model()\n    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mae\", rmse, \"mape\"])\n    history = model.fit(train_ds, epochs=300, validation_data=val_ds, callbacks=callbacks)\n    for metrics in [(\"loss\", \"val_loss\"), (\"mae\", \"val_mae\"), (\"mape\", \"val_mape\"), [\"lr\"]]:\n        pd.DataFrame(history.history, columns=metrics).plot()\n        plt.show()\n    model.load_weights(checkpoint_path)\n    historys.append(history)\n    models.append(model)","770c6e99":"def preprocess_test_data(image_url, tabular):\n    print(image_url, tabular)\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (image_size, image_size))\n    # 0 won't be used in prediction, but it's needed in this senario or the tabular variable is treated as label.\n    return (image, tabular), 0","18b278e5":"test_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[tabular_columns])).map(preprocess_test_data).batch(batch_size).cache().prefetch(2)","706475ba":"use_best_result = False\nif use_best_result:\n    if train_best_fold:\n        best_model = models[0]\n    else:\n        best_fold = 0\n        best_score = 10e8\n        for fold, history in enumerate(historys):\n            for val_rmse in history.history[\"val_rmse\"]:\n                if val_rmse < best_score:\n                    best_score = val_rmse\n                    best_fold = fold\n        print(\"Best Score:%.2f Best Fold: %d\"%(best_score, best_fold + 1))\n        best_model = models[best_fold]\n    results = best_model.predict(test_ds).reshape(-1)\nelse:\n    total_results = []\n    for model in models:\n        total_results.append(model.predict(test_ds).reshape(-1))\n    results = np.mean(total_results, axis=0).reshape(-1)\nsample_submission[\"Pawpularity\"] = results\nsample_submission.to_csv(\"submission.csv\", index=False)","d1c95920":"This Model accepts images with shape (image_size, image_size, 3) and tabular information with shape (12) as input. Since it's a Regression problem, it generate output with shape (1). ","b5c5fb62":"## Submission","5840c679":"## Import datasets","bc3ffe2f":"# TensorFlow multi-input Pet Pawpularity Model\n\n## Table of Contents\n- Summary\n- Set up\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n\n\n## Summary\nIn this notebook, I will build a TensorFlow multi-input Model that can receive image inputs and tabular inputs at the same time for training, so that I can get the most out of this dataset.\n## Set up","cd08ecbe":"### Tabular Model","87890d25":"## Data Preprocessing","5257fa1f":"### Preprocess funciton","7b02517a":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","be831ad0":"Let's have a big picture of how this Model looks like.","c31db996":"## Model Development","3deb25a9":"### RMSE"}}