{"cell_type":{"3afcf55d":"code","b10d940c":"code","7a7fe87f":"code","ac3149cc":"code","dc59e2a2":"code","1591c933":"code","bacab3cf":"code","6fb52818":"code","9eeabdbc":"code","f309a579":"code","1891f598":"code","ec363626":"code","f925ef5d":"code","8f3881b2":"code","2a3b49bd":"code","6dd118f3":"code","2957ec76":"code","cee0f7c1":"code","8a8c02b4":"code","6213b7a9":"code","1944b4ea":"code","74c5edd6":"code","18660233":"code","21f758bd":"code","3f595db6":"code","e045b6b1":"markdown","e045c315":"markdown","e61d0270":"markdown","8c40f6f9":"markdown","00b2c99a":"markdown","793a4994":"markdown"},"source":{"3afcf55d":"import numpy as np \nimport pandas as pd \nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, roc_auc_score, accuracy_score, f1_score,precision_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nfrom sklearn.decomposition import PCA","b10d940c":"df = pd.read_csv('..\/input\/indian-liver-patients-feature-selection-2-5\/liver_reduced_features.csv')","7a7fe87f":"X = df.iloc[:,:-1] # independent \ny = df.iloc[:,-1] #target","ac3149cc":"# summarize class distribution\ncounter = Counter(y)\nprint(counter)","dc59e2a2":"over = SMOTE(sampling_strategy = 0.8) #oversample the minority class","1591c933":"X, y = over.fit_resample(X, y)","bacab3cf":"counter = Counter(y)\nprint(counter)","6fb52818":"under = RandomUnderSampler(sampling_strategy=1) #undersample the majority class","9eeabdbc":"X, y = under.fit_resample(X, y)","f309a579":"counter = Counter(y)\nprint(counter)","1891f598":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","ec363626":"pca = PCA(n_components = 2)\npca.fit(X_scaled)","f925ef5d":"X_scaled_pca = pca.transform(X_scaled)","8f3881b2":"sns.set_theme()\nplt.figure(figsize = (10,5))\nsns.scatterplot(x =pd.DataFrame(X_scaled_pca)[0], y = pd.DataFrame(X_scaled_pca)[1], hue = y, palette = 'Set2')\nplt.xlim(-2, 4)\nplt.ylim(-2, 4)\nplt.show()","2a3b49bd":"smote_df = X \nsmote_df['liver_disease'] = y","6dd118f3":"scaler = StandardScaler()\nX = smote_df.iloc[:,:-1].values #features\ny = smote_df.iloc[:,-1].values.reshape(664,1) #targer\n\nX_scaled = StandardScaler().fit_transform(X) #scale features","2957ec76":"#Cross Validation\ncv = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)","cee0f7c1":"lr = LogisticRegression(random_state=1)\nnb = GaussianNB()\nknn = KNeighborsClassifier(n_neighbors=5)\nrf = RandomForestClassifier(max_depth=10, min_samples_split = 5)\n#mlp = MLPClassifier(hidden_layer_sizes=(1,9), activation='logistic', solver='sgd', alpha=1e-5, learning_rate='adaptive', random_state=100, verbose=False)\nclassifiers = [lr, nb, knn, rf]","8a8c02b4":"d = {}\nl = ['Logistic Regression', 'Naive Bayes', 'kNN', 'Random Forest']\ni = 0\nmetrics = pd.DataFrame(columns = ['Accuracy','F1 Score','Sensitivity', 'Specificity', 'AUC']) \nfor clf in classifiers:\n    clf_accuracy = []\n    clf_f1_score = []\n    clf_specificity = []\n    clf_sensitivity = []\n    clf_auc = []\n    tprs = []\n    mean_fpr = []\n    \n    for train_index, test_index in cv.split(X_scaled, y.ravel()):\n        #train test split\n        x_train_fold, x_test_fold = X_scaled[train_index], X_scaled[test_index] \n        y_train_fold, y_test_fold = y.ravel()[train_index], y.ravel()[test_index] \n        \n        #fit the model\n        clf.fit(x_train_fold, y_train_fold)\n        \n        #prediction\n        y_pred = clf.predict(x_test_fold)\n        \n        #confusion matrix\n        cm = confusion_matrix(y_test_fold,y_pred)\n        \n        tp = cm[1][1]\n        tn = cm[0][0]\n        fp = cm[0][1]\n        fn = cm[1][0]\n        \n        accuracy = (tp + tn) \/(tp + tn + fp + fn)\n        precision = (tp) \/ (tp + fp)\n        recall = (tp) \/ (tp + fn)\n        f1score = (2 * precision * recall) \/ (precision + recall)\n        sensitivity = tp \/ (tp + fn)\n        specificity = tn \/ (tn + fp)\n\n        #auc score\n        prediction = clf.fit(x_train_fold,y_train_fold).predict_proba(x_test_fold)\n        \n        fpr, tpr, t = roc_curve(y_test_fold.ravel(), prediction[:, 1])\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        \n        \n        #append the scores\n        clf_accuracy.append(accuracy)\n        clf_f1_score.append(f1score)\n        clf_sensitivity.append(sensitivity)\n        clf_specificity.append(specificity)\n        clf_auc.append(roc_auc)\n    \n    \n    d[l[i]]={'Accuracy': sum(clf_accuracy)\/10,'F1 Score': sum(clf_f1_score)\/10, 'Sensitivity': sum(clf_sensitivity)\/10,'Specificity': sum(clf_specificity)\/10, 'AUC': sum(clf_auc)\/10}\n    i+=1\n    #metrics.loc[-1] = d  # adding a row\n    #metrics.index = metrics.index + 1  # shifting index\n    #metrics = metrics.sort_index()  # sorting by index","6213b7a9":"d","1944b4ea":"for i in l:\n    metrics = metrics.append(d[i], ignore_index = True)","74c5edd6":"metrics['Classifier'] = l","18660233":"metrics","21f758bd":"metrics.plot.bar(figsize=(8,6), colormap = 'viridis')\nplt.xticks(ticks=[0, 1,2,3], labels=metrics['Classifier'])\nplt.show()","3f595db6":"for clf in classifiers:\n    print(clf)\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0,1,100)\n    i = 1\n    for train_index, test_index in cv.split(X_scaled, y.ravel()):\n        prediction = clf.fit(X_scaled[train_index],y.ravel()[train_index]).predict_proba(X_scaled[test_index])\n        fpr, tpr, t = roc_curve(y.ravel()[test_index], prediction[:, 1])\n        tprs.append(np.interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n        i= i+1\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'black')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.2f )' % (mean_auc),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC')\n    plt.legend(loc=\"lower right\")\n    plt.text(0.32,0.7,'More accurate area',fontsize = 12)\n    plt.text(0.63,0.4,'Less accurate area',fontsize = 12)\n    plt.show()","e045b6b1":"## Indian Liver Disease Patients: Smote & Classification (5\/5)","e045c315":"## Scale the Data","e61d0270":"## SMOTE Implementation","8c40f6f9":"## ROC Curves\nIn order to implement roc curves for kfold cross validation, I used the the code from [ROC Curve with k-Fold](http:\/\/www.kaggle.com\/kanncaa1\/roc-curve-with-k-fold-cv) notebook.","00b2c99a":"## Classifiers","793a4994":"## Scale the Data & PCA"}}