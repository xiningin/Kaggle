{"cell_type":{"f2b2a84b":"code","73168c04":"code","c96a9fdc":"code","d319cff9":"code","3a82ca14":"code","8f7cf554":"code","a71a84e8":"code","c357d6f7":"code","a1e58be9":"code","9b0705fe":"code","25458bf7":"code","58e9d73c":"code","3677daa5":"code","cfb4d323":"code","7d5bb186":"code","5f29e54c":"markdown","e196389d":"markdown","1a7bdff3":"markdown","9b15a459":"markdown","a9a1834b":"markdown","1908a162":"markdown","1a4b2e17":"markdown","5dd9c537":"markdown","49ea70e3":"markdown"},"source":{"f2b2a84b":"import math\n\nfrom PIL import Image\nimport requests\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnet50\nimport torchvision.transforms as T\ntorch.set_grad_enabled(False)","73168c04":"# COCO classes\nCLASSES = [\n    'N\/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A',\n    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack',\n    'umbrella', 'N\/A', 'N\/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N\/A', 'wine glass',\n    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n    'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table', 'N\/A',\n    'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A',\n    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n    'toothbrush'\n]\n\n# Colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]","c96a9fdc":"# Image Normalization\ntransform = T.Compose([\n    T.Resize(800),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# For Output Bounding Box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b","d319cff9":"def plot_results(pil_img, prob, boxes):\n    plt.figure(figsize=(16, 10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    colors = COLORS * 100\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        cl = p.argmax()\n        text = f'{CLASSES[cl]}: {p[cl]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=0.5))\n    plt.axis('off')\n    plt.show()","3a82ca14":"model = torch.hub.load('facebookresearch\/detr', 'detr_resnet50', pretrained=True)\nmodel.eval()","8f7cf554":"# Get a Test Image as PIL Image\nurl = 'http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg'\nim = Image.open(requests.get(url, stream=True).raw)","a71a84e8":"# Apply Normalization on the Input Image\nimg = transform(im).unsqueeze(0)\n\n# Input image into the Model\noutputs = model(img)\n\n# Keep Predictions with 0.7+ Confidence (Threshold -> 0.7)\nprobas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\nkeep = probas.max(-1).values > 0.9\n\n# Convert BBoxes from [0, 1] to Image scales\nbboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)","c357d6f7":"plot_results(im, probas[keep], bboxes_scaled)","a1e58be9":"# Use Lists to store the outputs via up-values\nconv_features, enc_attn_weights, dec_attn_weights = [], [], []\n\nhooks = [\n    model.backbone[-2].register_forward_hook(\n        lambda self, input, output: conv_features.append(output)\n    ),\n    model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n        lambda self, input, output: enc_attn_weights.append(output[1])\n    ),\n    model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n        lambda self, input, output: dec_attn_weights.append(output[1])\n    ),\n]\n\n# Propogate throught the Model\noutputs = model(img)\n\nfor hook in hooks:\n    hook.remove()\n    \n# No List Required Now\nconv_features = conv_features[0]\nenc_attn_weights = enc_attn_weights[0]\ndec_attn_weights = dec_attn_weights[0]","9b0705fe":"# Visualize them\n# Get the feature map shape\nh, w = conv_features['0'].tensors.shape[-2:]\n\nfig, axs = plt.subplots(ncols=len(bboxes_scaled), nrows=2, figsize=(22, 7))\ncolors = COLORS * 100\nfor idx, ax_i, (xmin, ymin, xmax, ymax) in zip(keep.nonzero(), axs.T, bboxes_scaled):\n    ax = ax_i[0]\n    ax.imshow(dec_attn_weights[0, idx].view(h, w))\n    ax.axis('off')\n    ax.set_title(f'query id: {idx.item()}')\n    ax = ax_i[1]\n    ax.imshow(im)\n    ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                               fill=False, color='blue', linewidth=3))\n    ax.axis('off')\n    ax.set_title(CLASSES[probas[idx].argmax()])\nfig.tight_layout()","25458bf7":"# Output of the CNN\nf_map = conv_features['0']\nprint(\"Encoder attention:      \", enc_attn_weights[0].shape)\nprint(\"Feature map:            \", f_map.tensors.shape)","58e9d73c":"# Get the HxW shape of the feature maps of the CNN\nshape = f_map.tensors.shape[-2:]\n# Reshape the self-attention to a more interpretable shape\nsattn = enc_attn_weights[0].reshape(shape + shape)\nprint(\"Reshaped self-attention:\", sattn.shape)","3677daa5":"# Downsampling factor for the CNN, is 32 for DETR and 16 for DETR DC5\nfact = 32\n\n# let's select 4 reference points for visualization\nidxs = [(200, 200), (280, 400), (200, 600), (440, 800),]\n\n# here we create the canvas\nfig = plt.figure(constrained_layout=True, figsize=(25 * 0.7, 8.5 * 0.7))\n# and we add one plot per reference point\ngs = fig.add_gridspec(2, 4)\naxs = [\n    fig.add_subplot(gs[0, 0]),\n    fig.add_subplot(gs[1, 0]),\n    fig.add_subplot(gs[0, -1]),\n    fig.add_subplot(gs[1, -1]),\n]\n\n# for each one of the reference points, let's plot the self-attention\n# for that point\nfor idx_o, ax in zip(idxs, axs):\n    idx = (idx_o[0] \/\/ fact, idx_o[1] \/\/ fact)\n    ax.imshow(sattn[..., idx[0], idx[1]], cmap='cividis', interpolation='nearest')\n    ax.axis('off')\n    ax.set_title(f'self-attention{idx_o}')\n\n# and now let's add the central image, with the reference points as red circles\nfcenter_ax = fig.add_subplot(gs[:, 1:-1])\nfcenter_ax.imshow(im)\nfor (y, x) in idxs:\n    scale = im.height \/ img.shape[-2]\n    x = ((x \/\/ fact) + 0.5) * fact\n    y = ((y \/\/ fact) + 0.5) * fact\n    fcenter_ax.add_patch(plt.Circle((x * scale, y * scale), fact \/\/ 2, color='r'))\n    fcenter_ax.axis('off')","cfb4d323":"class AttentionVisualizer:\n    def __init__(self, model, transform):\n        self.model = model\n        self.transform = transform\n\n        self.url = \"\"\n        self.cur_url = None\n        self.pil_img = None\n        self.tensor_img = None\n\n        self.conv_features = None\n        self.enc_attn_weights = None\n        self.dec_attn_weights = None\n\n        self.setup_widgets()\n\n    def setup_widgets(self):\n        self.sliders = [\n            widgets.Text(\n                value='http:\/\/images.cocodataset.org\/val2017\/000000039769.jpg',\n                placeholder='Type something',\n                description='URL (ENTER):',\n                disabled=False,\n                continuous_update=False,\n                layout=widgets.Layout(width='100%')\n            ),\n            widgets.FloatSlider(min=0, max=0.99,\n                        step=0.02, description='X coordinate', value=0.72,\n                        continuous_update=False,\n                        layout=widgets.Layout(width='50%')\n                        ),\n            widgets.FloatSlider(min=0, max=0.99,\n                        step=0.02, description='Y coordinate', value=0.40,\n                        continuous_update=False,\n                        layout=widgets.Layout(width='50%')),\n            widgets.Checkbox(\n              value=False,\n              description='Direction of self attention',\n              disabled=False,\n              indent=False,\n              layout=widgets.Layout(width='50%'),\n          ),\n            widgets.Checkbox(\n              value=True,\n              description='Show red dot in attention',\n              disabled=False,\n              indent=False,\n              layout=widgets.Layout(width='50%'),\n          )\n        ]\n        self.o = widgets.Output()\n\n    def compute_features(self, img):\n        model = self.model\n        # use lists to store the outputs via up-values\n        conv_features, enc_attn_weights, dec_attn_weights = [], [], []\n\n        hooks = [\n            model.backbone[-2].register_forward_hook(\n                lambda self, input, output: conv_features.append(output)\n            ),\n            model.transformer.encoder.layers[-1].self_attn.register_forward_hook(\n                lambda self, input, output: enc_attn_weights.append(output[1])\n            ),\n            model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(\n                lambda self, input, output: dec_attn_weights.append(output[1])\n            ),\n        ]\n        # Propagate through the model\n        outputs = model(img)\n\n        for hook in hooks:\n            hook.remove()\n\n        # don't need the list anymore\n        self.conv_features = conv_features[0]\n        self.dec_attn_weights = dec_attn_weights[0]\n        # get the HxW shape of the feature maps of the CNN\n        shape = self.conv_features['0'].tensors.shape[-2:]\n        # and reshape the self-attention to a more interpretable shape\n        self.enc_attn_weights = enc_attn_weights[0].reshape(shape + shape)\n    \n    def compute_on_image(self, url):\n        if url != self.url:\n            self.url = url\n            self.pil_img = Image.open(requests.get(url, stream=True).raw)\n            # mean-std normalize the input image (batch-size: 1)\n            self.tensor_img = self.transform(self.pil_img).unsqueeze(0)\n            self.compute_features(self.tensor_img)\n    \n    def update_chart(self, change):\n        with self.o:\n            clear_output()\n\n            # j and i are the x and y coordinates of where to look at\n            # sattn_dir is which direction to consider in the self-attention matrix\n            # sattn_dot displays a red dot or not in the self-attention map\n            url, j, i, sattn_dir, sattn_dot = [s.value for s in self.sliders]\n\n            fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(9, 4))\n            self.compute_on_image(url)\n\n            # convert reference point to absolute coordinates\n            j = int(j * self.tensor_img.shape[-1])\n            i = int(i * self.tensor_img.shape[-2])\n\n            # how much was the original image upsampled before feeding it to the model\n            scale = self.pil_img.height \/ self.tensor_img.shape[-2]\n\n            # compute the downsampling factor for the model\n            # it should be 32 for standard DETR and 16 for DC5\n            sattn = self.enc_attn_weights\n            fact = 2 ** round(math.log2(self.tensor_img.shape[-1] \/ sattn.shape[-1]))\n\n            # round the position at the downsampling factor\n            x = ((j \/\/ fact) + 0.5) * fact\n            y = ((i \/\/ fact) + 0.5) * fact\n\n            axs[0].imshow(self.pil_img)\n            axs[0].axis('off')\n            axs[0].add_patch(plt.Circle((x * scale, y * scale), fact \/\/ 2, color='r'))\n\n            idx = (i \/\/ fact, j \/\/ fact)\n            \n            if sattn_dir:\n                sattn_map = sattn[idx[0], idx[1], ...]\n            else:\n                sattn_map = sattn[..., idx[0], idx[1]]\n            \n            axs[1].imshow(sattn_map, cmap='cividis', interpolation='nearest')\n            if sattn_dot:\n                axs[1].add_patch(plt.Circle((idx[1],idx[0]), 1, color='r'))\n            axs[1].axis('off')\n            axs[1].set_title(f'self-attention{(i, j)}')\n\n            plt.show()\n        \n    def run(self):\n        for s in self.sliders:\n            s.observe(self.update_chart, 'value')\n        self.update_chart(None)\n        url, x, y, d, sattn_d = self.sliders\n        res = widgets.VBox(\n        [\n          url,\n          widgets.HBox([x, y]),\n          widgets.HBox([d, sattn_d]),\n          self.o\n        ])\n        return res","7d5bb186":"w = AttentionVisualizer(model, transform)\nw.run()","5f29e54c":"### Detection: Visualize Encoder-Decoder Multi-Head Attention Weights\nWe visualize attention weights of the last decoder layer. This corresponds to visualizing for each detected objects, which part of the image the model was looking at to predict the specific bounding box and class.\n\nWe will use `hooks` to extract attention weights (Averaged over all heads) from the Transformer.","e196389d":"## DETR: End to End Detection with Transformers\nThis is a hands-on Tutorial on `DETR`. A FacebookAI implementation on the Paper https:\/\/arxiv.org\/pdf\/2005.12872.pdf \n\nOfficial Github Repository\nhttps:\/\/github.com\/facebookresearch\/detr\n\nIn this\n1. We use Pretrained models to make Predictions\n2. Visualize the `Attentions` of the model to gain insights on the way it sees the images","1a7bdff3":"### Visualize Encoder Self Attention Weights\nWe visualize the model's self attention. This allows us to gain intuition on how the model encodes the objects. In particular, we see from\nthe attention responsed maps that the `encoder` already participate to the instance seperation process.\n\nFirstly, Visualize the shape of the Encoder Self Attention","9b15a459":"As we can see, the self-attention is a square matrix of size `[H * W, H * W]`, where `H` and `W` are the sizes of the feature map, so we reshape it so that it has a more interpretable representation of `[H, W, H, W]`.","a9a1834b":"### Using the AttentionVisualizer\n\nWe additionally add two options for visualization:\n- Direction of self-attention: the self-attention is a `[H, W, H, W]` matrix, so given a `i, j` point, we can return either `[i, j, :, :]` or `[:, :, i, j]`. This flag let's you chose how you want to visualize it\n- Show red dot in attention: just adds a red dot to the self-attention image to facilitate visualization.","1908a162":"The above Visualization gives an insight that the Encoder might be already performing object seperation due to its Self Attention Mechanism\n## Interactive Visualization of SelfAttention\nIn this, we wrap all the content above into a widget, which would make it easier to insert own image and define reference points\n\n### Defining the AttentionVisualizer\nWe use sliders in this","1a4b2e17":"Now that we have the self-attention into a more interpretable representation, let's try to visualize it. Given that the self-attention is a very large matrix, let's start by selecting a few reference points where we will be looking at the attention.","5dd9c537":"Apply some preprocessing and run the model and filter out the predictions. In particular, we keep only the objects for which the class confidence is higher than `0.9`\n(discounting as \"non-object\" predictions). We can lower the `threshold` for more predictions.","49ea70e3":"### Detection: Using a Pretrained Model from `TorchHub`\nWe will load a Simple Model from TorchHub `detr_resnet50` for fast inference."}}