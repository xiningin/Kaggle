{"cell_type":{"20a88e0e":"code","7c4aa076":"code","d92a96c7":"code","b723ef7a":"code","85d9a790":"code","266fd765":"code","ef2476e4":"code","78313ae9":"code","066b9831":"code","b53b72e8":"code","07088df6":"code","f2641e22":"code","33a6acb3":"code","e01206cd":"code","a97a15fb":"code","c189b4a5":"code","6355d6a6":"code","271df603":"code","a9fadab9":"code","a3f166b9":"code","e80aab86":"code","64cb7675":"code","468d3524":"code","e003ac91":"code","e46e22c1":"code","56119702":"code","2bb81da7":"code","70efba33":"code","4ff743e6":"code","2530c853":"code","1ca7f20e":"code","a645d7e4":"code","a65693be":"code","f702e4b2":"code","96c688c8":"code","edb06078":"code","7227f5ea":"code","e3c3ff51":"code","859a0615":"code","9160007c":"code","d4a80db7":"code","d075f4be":"markdown","4af76a5b":"markdown","e11e6014":"markdown","aea7d0aa":"markdown","0f542086":"markdown","5fb30ae0":"markdown","d1f9297e":"markdown","1ad6e2eb":"markdown","873441be":"markdown","3320703e":"markdown","3eed7d1f":"markdown","fe844328":"markdown","4f8de385":"markdown","7a2ef824":"markdown","d5939c01":"markdown","94327def":"markdown","39905b1a":"markdown","c37ae7b1":"markdown","54a50a63":"markdown","15700119":"markdown","27c030cc":"markdown","0613ce68":"markdown","6988a607":"markdown","956f682b":"markdown","2ca9f491":"markdown","6d2422bb":"markdown","0180315b":"markdown","bb3ede31":"markdown"},"source":{"20a88e0e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","7c4aa076":"full_train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","d92a96c7":"full_train.shape","b723ef7a":"test.shape","85d9a790":"# First, let's explore some data\nfull_train.describe(include=[object])","266fd765":"# How about corellation?\ncorr = full_train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\nprint(corr.SalePrice)","ef2476e4":"# Let's build a heatmap for clarity\nf, ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(corr, vmax=.8, annot=True);","78313ae9":"# Wow! That was big! It is better to find the most correlated features\nmost_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.6]\nplt.figure(figsize=(15,15))\nsns.heatmap(full_train[most_corr_features].corr(),annot=True)","066b9831":"# Let's explore the train and test data together so we get a more holistic picture\nfull_train = full_train.append(test, ignore_index=True)","b53b72e8":"full_train.drop('Id', axis=1, inplace=True)","07088df6":"# NaN values are always bad. Maybe we'll get lucky and they won't be in our data.\nfull_train.isnull().sum()","f2641e22":"# Well, we're going to test a lot of models. Some models cope with missing values, some not.\n\n# To keep things simple, I filled in the missing values without going into too much detail. \n# In other kernels, you can see how to do it better than me.\n\nfull_train[\"LotFrontage\"].fillna(0.0, inplace=True)\nfull_train['PoolQC'].fillna('No', inplace=True)\nfull_train['Alley'].fillna('No', inplace=True)\nfull_train['BsmtCond'].fillna('No', inplace=True)\nfull_train['BsmtExposure'].fillna('No', inplace=True)\nfull_train['BsmtFinSF1'].fillna(0, inplace=True)\nfull_train['BsmtFinSF2'].fillna(0.0, inplace=True)\nfull_train['BsmtFinType1'].fillna('No', inplace=True)\nfull_train['BsmtFinType2'].fillna('No', inplace=True)\nfull_train['BsmtFullBath'].fillna(0, inplace=True)\nfull_train['BsmtHalfBath'].fillna(0, inplace=True)\nfull_train['BsmtQual'].fillna('No', inplace=True)\nfull_train['BsmtUnfSF'].fillna(0, inplace=True)\nfull_train['Electrical'].fillna('No', inplace=True)\nfull_train['Exterior1st'].fillna('No', inplace=True)\nfull_train['Exterior2nd'].fillna('No', inplace=True)\nfull_train['Fence'].fillna('No', inplace=True)\nfull_train['FireplaceQu'].fillna('No', inplace=True)\nfull_train['MSZoning'].fillna(full_train['MSZoning'].mode()[0], inplace=True)\nfull_train['MasVnrArea'].fillna(0, inplace=True)\nfull_train['MasVnrType'].fillna('No', inplace=True)\nfull_train['MiscFeature'].fillna('No', inplace=True)\nfull_train['SaleType'].fillna(full_train['SaleType'].mode()[0], inplace=True)\nfull_train['Utilities'].fillna(full_train['Utilities'].mode()[0], inplace=True)\nfull_train['TotalBsmtSF'].fillna(0, inplace=True)\nfull_train['Functional'].fillna('Typ', inplace=True)\nfull_train['KitchenQual'].fillna(full_train['KitchenQual'].mode()[0], inplace=True)\nfull_train['GarageArea'].fillna(0, inplace=True)\nfull_train['GarageCars'].fillna(0, inplace=True)\nfull_train['GarageYrBlt'].fillna(0, inplace=True)\nfull_train['GarageType'].fillna('No', inplace=True)\nfull_train['GarageQual'].fillna('No', inplace=True)\nfull_train['GarageFinish'].fillna('No', inplace=True)\nfull_train['GarageCond'].fillna('No', inplace=True)","33a6acb3":"# Are we done with NaN?\nfull_train.columns[full_train.isnull().any()].tolist()","e01206cd":"y_train = full_train[:1460].SalePrice\nfull_train.drop('SalePrice', axis=1, inplace=True)","a97a15fb":"# Some models can only work with numeric values. What can we do with, for example, RoofStyle? \n# We can convert it to a number! And pd.get_dummies will help us.\n\nnon_numeric_predictors = full_train.select_dtypes(include=['object']).columns\nfull_train = pd.get_dummies(full_train)","c189b4a5":"full_train.head()","6355d6a6":"# When one parameter is 1000 and the other is 30, some models start to go crazy.\n# We will make the values near zero, this is called normalization\n\nmean = full_train.mean(axis=0)\nstd = full_train.std(axis=0)\nfull_train -= mean\nfull_train = full_train\/std","271df603":"full_train.head()","a9fadab9":"y_mean = y_train.mean(axis=0)\ny_std = y_train.std(axis=0)\n\ny_train -= y_mean\ny_train = y_train\/y_std","a3f166b9":"X_train = full_train[:1460]\nX_test = full_train[1460:]","e80aab86":"# How do we test our models without making a submission every time? We can \n# just split our train set and test models on it\n\nfrom sklearn.model_selection import train_test_split\n\nX_train_valid, X_test_valid, y_train_valid, y_test_valid = train_test_split(\n    X_train, y_train, random_state=42, shuffle=True)","64cb7675":"from sklearn.dummy import DummyRegressor\n\n#  I always use Dummy model like a benchmark. It's our starting point, which we will improve\n\ndummy_majority = DummyRegressor(strategy = 'median')\ndummy_majority.fit(X_train_valid, y_train_valid)\n\nprint(\"score on train: {}\".format(dummy_majority.score(X_train_valid, y_train_valid)))\nprint(\"score on test: {}\".format(dummy_majority.score(X_test_valid, y_test_valid)))","468d3524":"from sklearn.neighbors import KNeighborsRegressor\n\nfor neighbors in range(1,40):\n    knn = KNeighborsRegressor(n_neighbors = neighbors)\n    knn.fit(X_train_valid, y_train_valid)\n    \n    print(\"================================\")\n    print(\"neighbors number: {}\".format(neighbors))\n    print(\"model score on train: {}\".format( knn.score(X_train_valid, y_train_valid)))\n    print(\"model score on test: {}\".format( knn.score(X_test_valid, y_test_valid)))\n    print(\"================================\")\n\nprint('Done fitting!')\n# As we see, with 12 neighbors our model get the best test score","e003ac91":"from sklearn.model_selection import GridSearchCV\n\n# Now let's use another interesting thing - GridSearch. \n# This example is the same as above, but with a slight difference - \n# GridSearch use cross validation inside, what's why we fit it by X_train, not X_train_valid.\n# Also, best scores show us n_neighbors = 7, not 12 as above. Interesting!\n# I think it's because of the cross validation we didn't use in the last example.\n\nknn = KNeighborsRegressor()\n\ntuned_parameters = {'n_neighbors': range(1,40)}\n\ngrid_knn = GridSearchCV(cv=5, estimator=knn, param_grid=tuned_parameters, verbose=2)\n\ngrid_knn.fit(X_train, y_train)\nprint(\"best_scores:{}\".format(grid_knn.best_params_))\nprint(\"================\")\nprint(\"grid scores:{}\".format(grid_knn.grid_scores_))","e46e22c1":"from sklearn.linear_model import Ridge\n\nfor alpha_sample in range(50, 100):\n    linridge = Ridge(alpha=alpha_sample).fit(X_train_valid, y_train_valid)\n    print(\"================\")\n    print(\"Alpha is: {}\".format(alpha_sample))\n    print(\"score on train: {}\".format(linridge.score(X_train_valid, y_train_valid)))\n    print(\"score on test: {}\".format(linridge.score(X_test_valid, y_test_valid)))\n    print(\"================\")\n    \nprint(\"Done fitting!\")","56119702":"from sklearn.svm import SVR\n\nfor C_sample in [0.001, 0.01, 0.1, 1]:\n\n    svr = SVR(kernel = 'linear', C=C_sample).fit(X_train_valid, y_train_valid)   \n    print(\"================================\")\n    print(\"C is : {}\".format(C_sample))\n    print(\"model score on train: {}\".format( svr.score(X_train_valid, y_train_valid)))\n    print(\"model score on test: {}\".format( svr.score(X_test_valid, y_test_valid)))\n    print(\"================================\")\n    \nprint(\"Done fitting!\")","2bb81da7":"from sklearn.svm import LinearSVR\n\nfor C_sample in [0.001, 0.01, 0.1, 1, 10]:\n    print(\"================================\")\n    print(\"C is : {}\".format(C_sample))\n    svc = LinearSVR(C_sample).fit(X_train_valid, y_train_valid)\n    print(\"score on train: {}\".format(svc.score(X_train_valid, y_train_valid)))\n    print(\"score on test: {}\".format(svc.score(X_test_valid, y_test_valid)))\n    print(\"================================\")\n    \nprint(\"Done fitting!\")","70efba33":"from sklearn.tree import DecisionTreeRegressor\n\nregressor = DecisionTreeRegressor()\n\nregressor.fit(X_train_valid, y_train_valid)\n\nprint(\"================================\")\nprint(\"score on train:{}\".format(regressor.score(X_train_valid, y_train_valid)))\nprint(\"score on test:{}\".format(regressor.score(X_test_valid, y_test_valid)))\nprint(\"================================\")","4ff743e6":"from sklearn.ensemble import RandomForestRegressor\n\nfor number in range(200, 500, 50):\n    model_random_forest = RandomForestRegressor(n_estimators=number, n_jobs=-1,\n                                            random_state = 42)\n    model_random_forest.fit(X_train_valid, y_train_valid)\n    \n    print(\"===============\")\n    print(\"Trees number: {}\".format(number))\n    print(\"score on train: {}\".format(model_random_forest.score(X_train_valid, y_train_valid)))\n    print(\"score on test: {}\".format(model_random_forest.score(X_test_valid, y_test_valid)))\n    print(\"===============\")\n\nprint(\"Done fitting!\")","2530c853":"from sklearn.ensemble import GradientBoostingRegressor\n\nfor trees in range(200, 400, 100):\n    model_gradient_boosting = GradientBoostingRegressor(n_estimators=trees, \n                                                        criterion=\"mae\", random_state=42)\n    \n    model_gradient_boosting.fit(X_train_valid, y_train_valid)\n\n    print(\"===============\")\n    print(\"trees_number: {}\".format(trees))\n    print(\"score on train: {}\".format(model_gradient_boosting.score(X_train_valid, y_train_valid)))\n    print(\"score on test: {}\".format(model_gradient_boosting.score(X_test_valid, y_test_valid)))\n    print(\"===============\")\n\nprint(\"Done fitting!\")","1ca7f20e":"from sklearn.neural_network import MLPRegressor\n\nmlpreg = MLPRegressor(hidden_layer_sizes = [100,100],\n                             activation = 'relu',\n                             alpha = 1.0,\n                             solver = 'lbfgs', verbose=2, random_state=42)\n\nmlpreg.fit(X_train_valid, y_train_valid)\nprint(\"score on train:{}\".format(mlpreg.score(X_train_valid, y_train_valid)))\nprint(\"score on test:{}\".format(mlpreg.score(X_test_valid, y_test_valid)))","a645d7e4":"from xgboost import XGBRegressor\n\nfor number in range(300, 600, 100):\n    xgb_regressor = XGBRegressor(seed = 42, n_estimators=number)\n    xgb_regressor.fit(X_train_valid, y_train_valid)\n    \n    print(\"===============\")\n    print(\"trees:{}\".format(number))\n    print(\"score on train:{}\".format(xgb_regressor.score(X_train_valid, y_train_valid)))\n    print(\"score on test:{}\".format(xgb_regressor.score(X_test_valid, y_test_valid)))\n    print(\"===============\")\n    \nprint('Done fitting!')","a65693be":"import xgboost\nfig, ax = plt.subplots(figsize=(12,18))\nxgboost.plot_importance(xgb_regressor, max_num_features=75, height=0.8, ax=ax)\nplt.show()","f702e4b2":"from sklearn.feature_selection import RFECV\n\n# How does this thing work? It trains a model on all the features and test it. \n# Then removes one feature and tests again. Compares the quality of the first and \n# second model - if it does not fall or deteriorate, this feature is considered noise,\n# and more in the training will not participate.\n# Then everything is repeated. At the end we have a list of features without noise.\n\nxgb_feature_choosing = XGBRegressor(random_state = 42, n_estimators=2000, \n                             max_depth=3, learning_rate=0.1, \n                             early_stopping_rounds=5,\n                             silent = True,\n                            n_jobs=-1)\n\n\nfeature_eliminator = RFECV(xgb_feature_choosing,verbose = 1, n_jobs=-1)\n\n\n# WARNING\n# It will take a long time, about 2 hours, because we have a lot of parameters!\n\nfeature_eliminator.fit(X_train, y_train)\n\nfeature_array = feature_eliminator.support_\n\ncolumns_array = X_test.columns\n\nclean_array = columns_array[feature_array]","96c688c8":"# Here I have a clean list in case you don't want to run the code at the top. \n# Clean_features_columns is the same as clean_array\nclean_features_columns = ['1stFlrSF',\n'2ndFlrSF',\n'3SsnPorch',\n'BedroomAbvGr',\n'BsmtFinSF1',\n'BsmtFinSF2',\n'BsmtFullBath',\n'BsmtHalfBath',\n'BsmtUnfSF',\n'EnclosedPorch',\n'Fireplaces',\n'FullBath',\n'GarageArea',\n'GarageCars',\n'GarageYrBlt',\n'GrLivArea',\n'KitchenAbvGr',\n'LotArea',\n'LotFrontage',\n'LowQualFinSF',\n'MSSubClass',\n'MasVnrArea',\n'MiscVal',\n'MoSold',\n'OpenPorchSF',\n'OverallCond',\n'OverallQual',\n'PoolArea',\n'ScreenPorch',\n'TotRmsAbvGrd',\n'TotalBsmtSF',\n'WoodDeckSF',\n'YearBuilt',\n'YearRemodAdd',\n'YrSold',\n'BsmtCond_Fa',\n'BsmtCond_TA',\n'BsmtExposure_Av',\n'BsmtExposure_Gd',\n'BsmtExposure_Mn',\n'BsmtExposure_No',\n'BsmtFinType1_ALQ',\n'BsmtFinType1_GLQ',\n'BsmtFinType1_LwQ',\n'BsmtFinType1_Rec',\n'BsmtQual_Fa',\n'BsmtQual_Gd',\n'Condition1_Artery',\n'Condition1_Norm',\n'Condition1_PosN',\n'Condition1_RRAe',\n'Condition2_Feedr',\n'Electrical_SBrkr',\n'ExterCond_Gd',\n'ExterQual_Gd',\n'Exterior1st_AsbShng',\n'Exterior1st_BrkFace',\n'Exterior1st_MetalSd',\n'Exterior1st_Plywood',\n'Exterior1st_VinylSd',\n'Exterior1st_Wd Sdng',\n'Exterior2nd_HdBoard',\n'Exterior2nd_Stucco',\n'Exterior2nd_Wd Sdng',\n'Fence_MnPrv',\n'Fence_No',\n'FireplaceQu_Fa',\n'FireplaceQu_Gd',\n'FireplaceQu_TA',\n'Functional_Min1',\n'Functional_Typ',\n'GarageCond_Fa',\n'GarageFinish_Fin',\n'GarageFinish_RFn',\n'GarageFinish_Unf',\n'GarageType_Attchd',\n'GarageType_Detchd',\n'HeatingQC_Ex',\n'HeatingQC_Gd',\n'HeatingQC_TA',\n'HouseStyle_1.5Fin',\n'HouseStyle_SLvl',\n'KitchenQual_Ex',\n'KitchenQual_Gd',\n'LandContour_Lvl',\n'LotConfig_Corner',\n'LotConfig_CulDSac',\n'LotConfig_FR2',\n'LotConfig_Inside',\n'LotShape_IR1',\n'LotShape_Reg',\n'MSZoning_C (all)',\n'MSZoning_RH',\n'MasVnrType_BrkFace',\n'Neighborhood_BrkSide',\n'Neighborhood_CollgCr',\n'Neighborhood_Crawfor',\n'Neighborhood_Edwards',\n'Neighborhood_Gilbert',\n'Neighborhood_Mitchel',\n'Neighborhood_NAmes',\n'Neighborhood_NoRidge',\n'Neighborhood_NridgHt',\n'Neighborhood_OldTown',\n'Neighborhood_Sawyer',\n'Neighborhood_SawyerW',\n'Neighborhood_Somerst',\n'Neighborhood_StoneBr',\n'Neighborhood_Timber',\n'PavedDrive_Y',\n'RoofStyle_Gable',\n'SaleCondition_Abnorml',\n'SaleCondition_Family',\n'SaleCondition_Normal',\n'SaleType_New',\n'SaleType_WD']","edb06078":"X_train_clean = X_train[clean_features_columns]\nX_test_clean = X_test[clean_features_columns]","7227f5ea":"# There are lots of models up there! I'll just take one of them for making a prediction.\n\nxgb_regressor = XGBRegressor(seed = 42, n_estimators=400)\nxgb_regressor.fit(X_train_clean, y_train)","e3c3ff51":"predicted_prices = xgb_regressor.predict(X_test_clean)","859a0615":"predicted_prices = predicted_prices * y_std\npredicted_prices += y_mean","9160007c":"predicted_prices","d4a80db7":"submission = pd.DataFrame({\"Id\": test.Id, \"SalePrice\": predicted_prices})\n\nsubmission.to_csv('submission.csv', index=False)\n\nprint(\"Submission Finished\")","d075f4be":"To keep things simple and fast, I'll use a loop, but you can use Grid Search with cross validation.","4af76a5b":"**SVR**","e11e6014":"**Split into X_train and X_test**","aea7d0aa":"# Model fitting","0f542086":"**Set Dummies**","5fb30ae0":"SalePrice is NaN, it's OK, that values is from test","d1f9297e":"**Gradient Boosting**","1ad6e2eb":"**Checking NaN values**","873441be":"**Decision Tree Regressor**","3320703e":"**Random Forest**","3eed7d1f":"**KNN** ","fe844328":"# Recursive Feature Elimination","4f8de385":"**Predict**","7a2ef824":"But be careful with feature eliminator - sometimes  a model trained on clean features  does not show the best result. So, some of the features were still needed. This problem is solved by tuning the eliminator and EDA.","d5939c01":"**Ridge Regression**","94327def":"**Append test data**","39905b1a":"**Linear Support Vector Machine**","c37ae7b1":"** Normalize data**","54a50a63":"**Filling NaN**","15700119":"**XGboost**","27c030cc":"**Split... Again**","0613ce68":"**Dummy Regressor**","6988a607":"Let's see what features  our model has chosen as the most important","956f682b":"**Drop SalePrice**","2ca9f491":"**GridSearch**","6d2422bb":"Some data creates unnecessary noise. It would be good to remove them.","0180315b":"** ID not needed for us**","bb3ede31":"**Neural Network: MLPRegressor**"}}