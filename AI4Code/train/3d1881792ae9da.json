{"cell_type":{"915a3fee":"code","865e87a0":"code","725b9bec":"code","30f6f410":"code","79304b7f":"markdown","e5da12dc":"markdown","503fda97":"markdown","48028024":"markdown","b66d4957":"markdown"},"source":{"915a3fee":"import numpy  as np \nimport pandas as pd \nimport re\nimport sklearn\nimport lightgbm\n\n\npd.options.display.max_rows = 6","865e87a0":"train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv', index_col='id')\ntest_df  = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv',  index_col='id')\n\ncolumns = test_df.columns\nX       = train_df[columns]\nY       = train_df['target']\nX_train, X_valid, Y_train, Y_valid = sklearn.model_selection.train_test_split(X, Y, test_size=0.1, random_state=42)\nX_test  = test_df[columns]\n\ndisplay('train_df')\ndisplay( train_df )\n# display('test_df')\n# display( test_df )","725b9bec":"for seed in [42]:\n# for boosting in ['gbdt', 'goss', 'dart']:                       # gbdt is best\n# for max_depth in [1,2,4,6,8,10,12,16,32,64,-1]:                 # 4+ = 16 is best \n# for tree_learner in ['serial', 'feature', 'data', 'voting']:    # no effect\n# for extra_trees in [True, False]:                               # no effect\n# for learning_rate in [0.001, 0.01, 0.1, 0.5, 0.9]:              # 0.1   is best\n# for max_bin in [64,128,256,512,1024,2048]:                      # 512-1 is best\n# for num_leaves in [32, 64, 128, 256, 512, 1024, 2048, 4096]:    # 64-1  is best\n\n    # DOCS: https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst\n    # DOCS: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\n    parameters = {\n        # 'boosting_type':   boosting,\n        # 'max_depth':       max_depth, \n        # 'tree_learner':    tree_learner,\n        # 'extra_trees':     extra_trees,\n        # 'learning_rate':   learning_rate,\n        # 'max_bin':         max_bin-1,\n        # 'num_leaves':      num_leaves-1,\n    }\n    \n    # DOCS: https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.train.html\n    model = lightgbm.train(\n        {\n            'boosting_type':  'gbdt',  # default\n            'objective':      'regression',\n            'metric':         'rmse',\n            'learning_rate':   0.1,                     \n            'max_depth':       16,\n            'max_bin':         512-1,\n            'num_leaves':      64-1,\n            'seed':            42,\n            'verbose':         -1,\n            **parameters,\n        },\n        train_set  = lightgbm.Dataset(X_train, label=Y_train),\n        valid_sets = lightgbm.Dataset(X_valid, label=Y_valid),\n        num_boost_round       = 5000,\n        early_stopping_rounds = 100,\n        verbose_eval          = False\n    )\n    rmse = sklearn.metrics.mean_squared_error(Y_valid, model.predict(X_valid), squared=False)\n    print(f'{rmse:.5f}', parameters)","30f6f410":"predictions   = model.predict(X_test)\n\nsubmission_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv', index_col='id')\nsubmission_df['target'] = predictions\nsubmission_df.to_csv('submission.csv')\n!head submission.csv","79304b7f":"# LightGBM","e5da12dc":"# Further Reading\n\nThis notebook is part of a series exploring the [Tabular Playground](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021)\n- 0.72935 - [scikit-learn Ensemble](https:\/\/www.kaggle.com\/jamesmcguigan\/tabular-playground-scikit-learn-ensemble)\n- 0.71423 - [Fast.ai Tabular Solver](https:\/\/www.kaggle.com\/jamesmcguigan\/fast-ai-tabular-solver)\n- 0.70426 - [XGBoost](https:\/\/www.kaggle.com\/jamesmcguigan\/tabular-playground-xgboost)\n- [LightGBM](https:\/\/www.kaggle.com\/jamesmcguigan\/tabular-playground-lightgbm)","503fda97":"# Tabular Playground - LightGBM\n\n[LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/) is a popular alterative to [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/), so lets see how it performs","48028024":"# Submission","b66d4957":"# Dataset"}}