{"cell_type":{"4c409911":"code","692962ce":"code","fab87be2":"code","ac9ffba4":"code","d1603c56":"code","01ddea9a":"code","0c35b0f4":"code","f816cb05":"code","4c88e8b1":"code","e0472916":"code","327ff037":"markdown"},"source":{"4c409911":"#My library Templete\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport spacy\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.naive_bayes import GaussianNB\nfrom matplotlib import pyplot\nimport gc\nfrom tensorflow.keras import regularizers\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve as learning_cv\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import roc_curve, auc\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import validation_curve\nimport scikitplot as skplt   \nfrom sklearn.metrics import classification_report   \nimport re\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nimport seaborn as sns\nfrom sklearn import metrics\nimport os\nimport json\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve as learning_cv\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import roc_curve, auc\nfrom gensim.models import Word2Vec\nfrom sklearn.decomposition import PCA\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import validation_curve\nimport scikitplot as skplt   \nfrom sklearn.metrics import classification_report   \nimport random\nimport logging\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom spacy.gold import GoldParse\nfrom spacy.scorer import Scorer\nfrom sklearn.metrics import accuracy_score","692962ce":"#install docx lib to read docx\n!pip install python-docx","fab87be2":"import pandas as pd\nimport docx\ntrain_labels=pd.read_csv('\/kaggle\/input\/rent-agreement\/TrainingTestSet (2).csv')\nval_labels=pd.read_csv('\/kaggle\/input\/rent-agreement\/ValidationSet.csv')\ntrain_text_dir='\/kaggle\/input\/rent-agreement\/Training_data\/Training_data\/'\nval_text_dir='\/kaggle\/input\/rent-agreement\/Validation_data\/Validation_data\/'","ac9ffba4":"import re\ndef clean(text):\n    text=text.replace('\/','.')\n    text=text.replace('\\\\','.')\n    text=text.replace('. ','.')\n    text=text.replace(', ','')\n    text=text.replace(',','')\n    text=text.replace('-','.')\n    text=text.replace('\/','.')\n    text=text.replace('. ','.')\n    #text=text.lower()\n    return(text)","d1603c56":"#prepair data for training\ndata=[]\ndata_index=list(train_labels.columns)\ncount=0\nnew_data=[]\nfor i in os.listdir('\/kaggle\/input\/rent-agreement\/Training_data\/Training_data\/'):\n    doc = docx.Document('\/kaggle\/input\/rent-agreement\/Training_data\/Training_data\/'+i)  # Creating word reader object.\n    paragraph=doc.paragraphs\n    st=\"\"\n    for j in paragraph:\n        st+=clean(str(j.text))+\" \"\n    data.append(st)\n    temp=train_labels.loc[train_labels['File Name'] == i[:-9]]\n    \n    entities=[]\n    for col in data_index[1:]:\n        query=str(tuple(temp[col])[0])\n\n        if(col=='Aggrement Start Date' or col=='Aggrement End Date'):\n            if(st.find(query)!=-1):\n                entities.append([st.find(query),st.find(query)+len(query),col])\n                continue\n\n            \n        \n        if(type(tuple(temp[col])[0])==type(8.0) and tuple(temp[col])[0]>=0): \n                query=str(int(tuple(temp[col])[0]))      \n        if(st.find(query)!=-1 or st.lower().find(query.lower())!=-1):\n                entities.append([st.find(query),st.find(query)+len(query),col])\n\n        else:\n            #print(i,col,query,type(tuple(temp[col])[0]))\n            count+=1\n    new_data.append((st,{'entities':entities}))\nprint(count)\n            ","01ddea9a":"\"\"\"\ndata=[]\ndata_index=list(train_labels.columns)\ncount=0\nfor i in os.listdir('\/kaggle\/input\/rent-agreement\/Training_data\/Training_data\/'):\n    doc = docx.Document('\/kaggle\/input\/rent-agreement\/Training_data\/Training_data\/'+i)  # Creating word reader object.\n    paragraph=doc.paragraphs\n    st=\"\"\n    for i in paragraph:\n        st+=clean(i.text)+\" \"\n    data.append(st)\n    temp=train_labels.loc[train_labels['File Name'] == i]\n    for col in data_index[1:]:\n        query=tuple(temp[col])[0]\n        if(st.lower().find(query.lower())!=-1):\n            print(st.find(query),st.find(query)+len(query),st[st.find(query):st.find(query)+len(query)])\n        else:\n            print(i,query)\n            count+=1\nprint(count)\n\"\"\"            \n\n","0c35b0f4":"TRAIN_DATA=new_data\n#load model\nnlp = spacy.blank('en')\nif 'ner' not in nlp.pipe_names:\n    ner = nlp.create_pipe('ner')\n    nlp.add_pipe(ner, last=True)\n#adding labels\nfor _, labels in TRAIN_DATA:\n     for ent in labels.get('entities'):\n        ner.add_label(ent[2])\n#adding task to piplines\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\ncount=0\nloss_val=[]\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    optimizer = nlp.begin_training()\n    for i in range(100):\n        print(\"iteration :\" ,i,\" Start\")\n        random.shuffle(TRAIN_DATA)\n        losses = {}\n        #start training\n        for text, annotations in TRAIN_DATA:\n            try:                \n                nlp.update(\n                    [text],  # batch of texts\n                    [annotations],  # batch of annotations\n                    drop=0.2,  # dropout - make it harder to memorise data\n                    sgd=optimizer,\n                    # callable to update weights\n                    losses=losses)\n            except:\n                pass\n                count=count+1\n        loss_val.append(list(losses.values())[0])\n        print(\"Loss in iteration :\",i,\"loss : \",list(losses.values())[0])\n#plot loss\nplt.plot(loss_val)","f816cb05":"#load validation data \ndata=[]\ndata_index=list(train_labels.columns)\ncount=0\nnew_data=[]\nfor i in os.listdir('\/kaggle\/input\/rent-agreement\/Validation_Data\/Validation_Data\/'):\n    doc = docx.Document('\/kaggle\/input\/rent-agreement\/Validation_Data\/Validation_Data\/'+i)  # Creating word reader object.\n    paragraph=doc.paragraphs\n    st=\"\"\n    for j in paragraph:\n        st+=clean(str(j.text))+\" \"\n    data.append(st)\n    temp=val_labels.loc[val_labels['File Name'] == i[:-9]]\n    #convert in proper format\n    entities=[]\n    for col in data_index[1:]:\n        query=str(tuple(temp[col])[0])\n\n        if(col=='Aggrement Start Date' or col=='Aggrement End Date'):\n            if(st.find(query)!=-1):\n                entities.append([st.find(query),st.find(query)+len(query),col])\n                continue        \n        if(type(tuple(temp[col])[0])==type(8.0) and tuple(temp[col])[0]>=0): \n                query=str(int(tuple(temp[col])[0]))      \n        if(st.find(query)!=-1 or st.lower().find(query.lower())!=-1):\n                entities.append([st.find(query),st.find(query)+len(query),col])\n\n        else:\n            print(i,col,query,type(tuple(temp[col])[0]))\n            count+=1\n    new_data.append((st,{'entities':entities}))","4c88e8b1":"#test the model and evaluate it","e0472916":"#print predicted values\nexamples =new_data\nc=0    \nfor text,annot in examples:\n        doc_to_test=nlp(text)\n        d={}\n        for ent in doc_to_test.ents:\n            d[ent.label_]=[]\n        for ent in doc_to_test.ents:\n            d[ent.label_].append(ent.text)\n        print(d)\n        c+=1\n","327ff037":"Meta Data Extraction,\nNamed entity Recognition,\nRent Agreement Parsing,\nRent Agreement Meta Data Extraction,\nNLP,\nSpacy NER"}}