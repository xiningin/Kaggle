{"cell_type":{"5f9459fd":"code","4759a9c0":"code","6c2aa68b":"code","b57e19b0":"code","360bfd4d":"code","3d729966":"code","210fcee3":"code","30f0592f":"code","cee5c713":"code","c7f338be":"code","961b274a":"code","1fafdfbc":"code","a9b8c421":"code","d2b58b9b":"code","601d3532":"code","e7495623":"code","8bc21377":"code","820a8fa1":"code","98161291":"code","50c32af5":"code","1fb41bdd":"code","92799e90":"code","06d4c054":"code","ddafd4d7":"code","e810329d":"code","cccfca00":"code","70bef360":"code","c3aa161e":"code","eb4dfb12":"code","b101a156":"code","6ba51dc7":"code","2627240f":"code","28104d7a":"code","bb8ea87b":"markdown","766a385c":"markdown","84596b19":"markdown","65d9bb38":"markdown","efde53a7":"markdown","f083f186":"markdown","4342755d":"markdown","a411fb68":"markdown"},"source":{"5f9459fd":"from torchvision import models","4759a9c0":"# list of all models in torchvision\ndir(models)","6c2aa68b":"# Using the resnet101 function, we\u2019ll now instantiate a 101-layer convolutional neural network\nresnet = models.resnet101(pretrained=True)","b57e19b0":"#structure of resnet.\n# What we are seeing here is modules, one per line. Note that they have nothing in common \n# with Python modules: they are individual operations, the building blocks of a\n# neural network. They are also called layers in other deep learning frameworks.\nresnet","360bfd4d":"# The resnet variable can be called like a function, taking as input one or more\n# images and producing an equal number of scores for each of the 1,000 ImageNet\n# classes. Before we can do that, however, we have to preprocess the input images so\n# they are the right size and so that their values (colors) sit roughly in the same numerical \n# range. In order to do that, the torchvision module provides transforms, which\n# allow us to quickly define pipelines of basic preprocessing functions:\nfrom torchvision import transforms\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n    mean=[0.485, 0.456, 0.406],\n    std=[0.229, 0.224, 0.225]\n    )\n])","3d729966":"!ls ..\/input","210fcee3":"from PIL import Image\nimg = Image.open(\"..\/input\/dogimage\/download.jpeg\")","30f0592f":"img.show()","cee5c713":"img","c7f338be":"img_t = preprocess(img)","961b274a":"import torch\nbatch_t = torch.unsqueeze(img_t, 0)","1fafdfbc":"# The process of running a trained model on new data is called inference in deep learning circles. \n# In order to do inference, we need to put the network in eval mode:\nresnet.eval()","a9b8c421":"# If we forget to do that, some pretrained models, like batch normalization and dropout,\n# will not produce meaningful answers, just because of the way they work internally.\n# Now that eval has been set, we\u2019re ready for inference:\nout = resnet(batch_t)","d2b58b9b":"with open('..\/input\/imagenet\/imagenet_classes.txt') as f:\n    labels = [line.strip() for line in f.readlines()]","601d3532":"# 1,000 labels for the ImageNet dataset classes:\nlabels","e7495623":"# At this point, we need to determine the index corresponding to the maximum score\n# in the out tensor we obtained previously. We can do that using the max function in\n# PyTorch, which outputs the maximum value in a tensor as well as the indices where\n# that maximum value occurred\n_, index = torch.max(out, 1)","8bc21377":"# We can now use the index to access the label. Here, index is not a plain Python number,\n# but a one-element, one-dimensional tensor (specifically, tensor([207])), so we\n# need to get the actual numerical value to use as an index into our labels list using\n# index[0]. We also use torch.nn.functional.softmax (http:\/\/mng.bz\/BYnq) to normalize our \n# outputs to the range [0, 1], and divide by the sum. That gives us something\n# roughly akin to the confidence that the model has in its prediction. \n","820a8fa1":"percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100\nlabels[index[0]], percentage[index[0]].item()","98161291":"_, indices = torch.sort(out, descending=True)\n[(labels[idx], percentage[idx].item()) for idx in indices[0][:5]]","50c32af5":"import torch\nimport torch.nn as nn\n\nclass ResNetBlock(nn.Module): # <1>\n\n    def __init__(self, dim):\n        super(ResNetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim)\n\n    def build_conv_block(self, dim):\n        conv_block = []\n\n        conv_block += [nn.ReflectionPad2d(1)]\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n                       nn.InstanceNorm2d(dim),\n                       nn.ReLU(True)]\n\n        conv_block += [nn.ReflectionPad2d(1)]\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n                       nn.InstanceNorm2d(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x) # <2>\n        return out\n\n\nclass ResNetGenerator(nn.Module):\n\n    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9): # <3> \n\n        assert(n_blocks >= 0)\n        super(ResNetGenerator, self).__init__()\n\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n                 nn.InstanceNorm2d(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=True),\n                      nn.InstanceNorm2d(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResNetBlock(ngf * mult)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult \/ 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=True),\n                      nn.InstanceNorm2d(int(ngf * mult \/ 2)),\n                      nn.ReLU(True)]\n\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input): # <3>\n        return self.model(input)","1fb41bdd":"netG = ResNetGenerator()","92799e90":"model_path = '..\/input\/horse2zebra\/horse2zebra_0.4.0.pth'\nmodel_data = torch.load(model_path)\nnetG.load_state_dict(model_data)","06d4c054":"netG.eval()","ddafd4d7":"from PIL import Image\nfrom torchvision import transforms","e810329d":"preprocess = transforms.Compose([transforms.Resize(256),\ntransforms.ToTensor()])","cccfca00":"img = Image.open(\"..\/input\/horse2zebra\/horse2zebra\/horse2zebra\/testA\/n02381460_1030.jpg\")","70bef360":"img","c3aa161e":"img_t = preprocess(img)","eb4dfb12":"batch_t = torch.unsqueeze(img_t,0)","b101a156":"batch_out = netG(batch_t)","6ba51dc7":"out_t = (batch_out.data.squeeze() + 1.0) \/ 2.0\nout_img = transforms.ToPILImage()(out_t)\nout_img.save('zebra.jpg')\nout_img","2627240f":"out_t = (batch_out.data.squeeze() + 1.0) \/ 2.0\nout_img = transforms.ToPILImage()(out_t)","28104d7a":"out_img","bb8ea87b":"Chapter 2 : Deep learning with Pytorch\n\nLink : https:\/\/pytorch.org\/deep-learning-with-pytorch","766a385c":"### Converting Horse to Zebra","84596b19":"# Gan Game","65d9bb38":"![image.png](attachment:image.png)","efde53a7":"<img src=\"https:\/\/dpzbhybb2pdcj.cloudfront.net\/stevens2\/v-13\/Figures\/p1ch2_cyclegan.png\">","f083f186":"## Resnet101 has 101 bottleneck !!","4342755d":"## Image Recognition","a411fb68":"# Cycle Gan"}}