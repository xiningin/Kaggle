{"cell_type":{"2766b6e6":"code","808e329a":"code","19ec119c":"code","a04126f1":"code","f9dc8572":"code","c7d6e55d":"code","2903d197":"code","b9b9054f":"code","1d0a69f5":"code","5e3a5dc9":"code","ba7d9c4c":"code","ab1aeeac":"code","3118b527":"code","86df3f2f":"code","45e2161e":"code","4dc12b77":"code","e0bd594a":"code","9f523c61":"code","4d856cb3":"code","c942cebf":"code","e3f81c37":"code","672c14c9":"code","fb83e915":"markdown","163891a2":"markdown","13bcb50d":"markdown","b41a0b96":"markdown","81fa0f10":"markdown","c1ff4778":"markdown"},"source":{"2766b6e6":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","808e329a":"from fastai.vision import *","19ec119c":"path = untar_data(URLs.BIWI_HEAD_POSE)","a04126f1":"cal = np.genfromtxt(path\/'01'\/'rgb.cal', skip_footer=6); cal","f9dc8572":"fname = '09\/frame_00667_rgb.jpg'","c7d6e55d":"def img2txt_name(f): return path\/f'{str(f)[:-7]}pose.txt'","2903d197":"img = open_image(path\/fname)\nimg.show()","b9b9054f":"ctr = np.genfromtxt(img2txt_name(fname), skip_header=3); ctr","1d0a69f5":"def convert_biwi(coords):\n    c1 = coords[0] * cal[0][0]\/coords[2] + cal[0][2]\n    c2 = coords[1] * cal[1][1]\/coords[2] + cal[1][2]\n    return tensor([c2,c1])\n\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2txt_name(f), skip_header=3)\n    return convert_biwi(ctr)\n\ndef get_ip(img,pts): return ImagePoints(FlowField(img.size, pts), scale=True)","5e3a5dc9":"get_ctr(fname)","ba7d9c4c":"ctr = get_ctr(fname)\nimg.show(y=get_ip(img, ctr), figsize=(6, 6))","ab1aeeac":"data = (PointsItemList.from_folder(path)\n        .split_by_valid_func(lambda o: o.parent.name=='13')\n        .label_from_func(get_ctr)\n        .transform(get_transforms(), tfm_y=True, size=(120,160))\n        .databunch(num_workers=0).normalize(imagenet_stats)\n       )","3118b527":"data.show_batch(3, figsize=(9,6))","86df3f2f":"learn = create_cnn(data, models.resnet34)","45e2161e":"learn.lr_find()\nlearn.recorder.plot()","4dc12b77":"lr = 2e-2","e0bd594a":"learn.fit_one_cycle(5, slice(lr))","9f523c61":"learn.save('stage-1')","4d856cb3":"learn.load('stage-1');","c942cebf":"learn.show_results()","e3f81c37":"tfms = get_transforms(max_rotate=20, max_zoom=1.5, max_lighting=0.5, max_warp=0.4, p_affine=1., p_lighting=1.)\n\ndata = (PointsItemList.from_folder(path)\n        .split_by_valid_func(lambda o: o.parent.name=='13')\n        .label_from_func(get_ctr)\n        .transform(get_transforms(), tfm_y=True, size=(120,160))\n        .databunch(num_workers=0).normalize(imagenet_stats)\n       )","672c14c9":"def _plot(i,j,ax):\n    x,y = data.train_ds[0]\n    x.show(ax, y=y)\n\nplot_multi(_plot, 3, 3, figsize=(8,6))","fb83e915":"## Creating a dataset","163891a2":"## Data augmentation","13bcb50d":"## Train model","b41a0b96":"## Getting and converting the data","81fa0f10":"This is a more advanced example to show how to create custom datasets and do regression with images. Our task is to find the center of the head in each image. The data comes from the [BIWI head pose dataset](https:\/\/data.vision.ee.ethz.ch\/cvl\/gfanelli\/head_pose\/head_forest.html#db), thanks to Gabriele Fanelli et al. We have converted the images to jpeg format, so you should download the converted dataset from [this link](https:\/\/s3.amazonaws.com\/fast-ai-imagelocal\/biwi_head_pose.tgz).","c1ff4778":"## Regression with BIWI head pose dataset"}}