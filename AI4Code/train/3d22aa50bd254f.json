{"cell_type":{"c01a27bb":"code","3a76cd75":"code","253d79b2":"code","b99d523b":"code","f04c94da":"code","594b58bd":"code","8daeecb0":"code","eb63bb90":"code","396d252c":"code","0d14c92d":"code","3512cc34":"code","5cb0a2f2":"code","5f89122b":"code","48a6b81e":"code","d25d0455":"code","3a1acdeb":"code","544ecd9c":"code","06474e89":"code","7010aaa5":"code","d71dd1e4":"code","cab39e86":"code","ec6b370e":"code","b8e34839":"code","110fb96e":"code","b4f224bc":"code","74c1c5f1":"code","447ad48d":"code","c3121ce9":"code","859c22ee":"code","e29221a4":"code","80ee7c43":"code","f507ffd6":"code","d188a00e":"code","081edb66":"code","6a79bbe4":"code","26eed785":"code","f243859d":"code","16607616":"code","7d6729d1":"code","c52c5011":"code","65ecda57":"code","3efe0f23":"code","d3b54b9f":"code","729eb417":"code","87b64067":"code","c275cfb7":"code","4839eee0":"code","23203d99":"code","ad60ac50":"code","c8fe666c":"code","ee0ec91e":"code","5b7e66db":"code","2e058eb4":"code","bf6582c4":"code","f3eccc0c":"code","35bc1651":"code","6276414c":"code","7818c7fe":"code","37d5e690":"code","fcca0eb4":"code","2de801d1":"code","e9b49855":"code","aa162f6c":"code","66aec0b2":"code","760a90e0":"code","97f687c7":"code","2b80e439":"code","086db79a":"code","e1c8b1bd":"code","6357497b":"code","e54815d2":"code","766fc149":"code","2e472266":"code","49fd1565":"markdown","acdd210c":"markdown","d9e408c1":"markdown","15ca408c":"markdown","91f2a291":"markdown","420e1607":"markdown","bf9ff34f":"markdown","d8f7ab03":"markdown","52d0ce79":"markdown","f7993bde":"markdown","312f77e4":"markdown","c2ec0671":"markdown","270f9a8e":"markdown","e3e5052c":"markdown","5c8a042e":"markdown","8763e51a":"markdown","5bd6cd18":"markdown","c07c4f7f":"markdown","191756a5":"markdown","62dcbaf1":"markdown","6d0e6d74":"markdown","1c751486":"markdown","c3fb5815":"markdown","e00fad28":"markdown","ce07849a":"markdown","af3f023b":"markdown","1d0b1e0e":"markdown","ca1f67f3":"markdown","84f73cc4":"markdown","6fb85c3e":"markdown","dcd3e0e9":"markdown","609f55e8":"markdown","bce6c6f4":"markdown","bfd67247":"markdown","6cb3104b":"markdown","071b0b1c":"markdown","d5cd3db0":"markdown","08ccea1f":"markdown","ad533030":"markdown","a5745b11":"markdown","c6bd2659":"markdown","6740efc7":"markdown","c607c049":"markdown","095e4aa5":"markdown","92c48c1b":"markdown","72556099":"markdown","029f9ecd":"markdown","45e7ade6":"markdown","30530172":"markdown","0f8c1ddb":"markdown","efbce60e":"markdown","99ed1630":"markdown","2a09a39c":"markdown","cd38c2ba":"markdown","0cc1671e":"markdown","32d4cab4":"markdown"},"source":{"c01a27bb":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n# plotting stuff\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\ncolorMap = sns.light_palette(\"blue\", as_cmap=True)\nimport datatable as dt\n# misc\nimport missingno as msno\n# system\nimport warnings\nwarnings.filterwarnings('ignore')\n# garbage collector to keep RAM in check\nimport gc  \nimport matplotlib.gridspec as gridspec","3a76cd75":"from scipy import stats\ndef r2(x, y):\n    return stats.pearsonr(x, y)[0] ** 2\n\nimport seaborn as sns\nclass myjoint(sns.JointGrid):\n    def __init__(self, x, y, data=None,height=7, ratio=5, space=.2,\n                 dropna=True, xlim=None, ylim=None, size=None):\n        super(myjoint, self).__init__(x, y, data,height, ratio, space,\n                 dropna, xlim, ylim, size)\n        plt.close(2)\n        # Set up the subplot grid\n        self.ax_joint = f.add_subplot(gs[1:, :-1])\n        self.ax_marg_x = f.add_subplot(gs[0, :-1], sharex=self.ax_joint)\n        self.ax_marg_y = f.add_subplot(gs[1:, -1], sharey=self.ax_joint)\n\n        # Turn off tick visibility for the measure axis on the marginal plots\n        plt.setp(self.ax_marg_x.get_xticklabels(), visible=False)\n        plt.setp(self.ax_marg_y.get_yticklabels(), visible=False)","253d79b2":"!wc -l ..\/input\/jane-street-market-prediction\/train.csv\n!wc -l ..\/input\/jane-street-market-prediction\/features.csv\n!wc -l ..\/input\/jane-street-market-prediction\/example_sample_submission.csv\n!wc -l ..\/input\/jane-street-market-prediction\/example_test.csv","b99d523b":"train_data = dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\nfeatures_data = dt.fread('..\/input\/jane-street-market-prediction\/features.csv').to_pandas()\nexample_sample_submission = dt.fread('..\/input\/jane-street-market-prediction\/example_sample_submission.csv').to_pandas()\nexample_test = dt.fread('..\/input\/jane-street-market-prediction\/example_test.csv').to_pandas()\ntype(train_data)","f04c94da":"train_data.head()","594b58bd":"train_data.describe()","8daeecb0":"example_sample_submission.head()","eb63bb90":"example_test.head()","396d252c":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"Cumulative return\", fontsize=18);\nbalance.plot(lw=3);","0d14c92d":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nresp_1= pd.Series(train_data['resp_1']).cumsum()\nresp_2= pd.Series(train_data['resp_2']).cumsum()\nresp_3= pd.Series(train_data['resp_3']).cumsum()\nresp_4= pd.Series(train_data['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of resp\", fontsize=18)\nbalance.plot(lw=3)\nresp_1.plot(lw=3)\nresp_2.plot(lw=3)\nresp_3.plot(lw=3)\nresp_4.plot(lw=3)\nplt.legend(loc=\"upper left\");","3512cc34":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nresp_4= pd.Series(train_data['resp_4']).cumsum() \nresp_3= pd.Series(train_data['resp_3']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of resp and time horizons 4 and 3 (500 days)\", fontsize=18)\nbalance.plot(lw=3)\nresp_4.plot(lw=3) \nresp_3.plot(lw=3)\nplt.legend(loc=\"upper left\");","5cb0a2f2":"fig, ax = plt.subplots(figsize=(15, 5))\nbalance= pd.Series(train_data['resp']).cumsum()\nweighted_trend= pd.Series(train_data['weight']*train_data['resp'],name='weighted_trend').cumsum()  \nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of resp and weighted_trend\", fontsize=18)\nbalance.plot(lw=3)\nweighted_trend.plot(lw=3) \nplt.legend(loc=\"upper left\");","5f89122b":"train_data['resp_trend'] = train_data['resp'].cumsum()                                                              \ntrain_data['weighted_trend'] = (train_data['weight']*train_data['resp']).cumsum()                                   \ntrain_data.plot(x='ts_id', y=['resp_trend', 'weighted_trend'],figsize=(15,5))   \nplt.show()","48a6b81e":"# Taking one day sample data from train dataset date 1\nsample_train_data = train_data.query('date == 1')\nsample_train_data.describe()","d25d0455":"# missing values\nn_features = 60\nnan_val = train_data.isna().sum()[train_data.isna().sum() > 0].sort_values(ascending=False)\nprint(nan_val)\nfig, axs = plt.subplots(figsize=(10, 10))\nsns.barplot(y = nan_val.index[0:n_features], \n            x = nan_val.values[0:n_features], \n            alpha = 0.8\n           )\nplt.title('Missing values of train dataset')\nplt.xlabel('# of Missing values')\nplt.show()","3a1acdeb":"#Finding out the features with missing values more than 10%\nnull = sample_train_data.isnull().sum()\nnulls_fl = list(null[null >(0.1 * len(sample_train_data))].index)\nnulls_fl","544ecd9c":"print(\"Number of features with null values:\",np.sum(train_data.isna().sum()>0))","06474e89":"#Here i am using median value to replace missing values as Median is not affected by outliers.\n#Missing Data Handling\ntrain_data = train_data.apply(lambda x: x.fillna(x.mean()),axis=0)\nprint(\"Number of features with null values:\",np.sum(train_data.isna().sum()>0))","7010aaa5":"#Here i am using median value to replace missing values as Median is not affected by outliers.\n#Missing Data Handling\nsample_train_data = sample_train_data.apply(lambda x: x.fillna(x.mean()),axis=0)\nprint(\"Number of features with null values:\",np.sum(sample_train_data.isna().sum()>0))","d71dd1e4":"sample_train_data.iloc[:,7:-2].hist(bins=100,figsize=(30,74),layout=(35,4));","cab39e86":"featstr = [i for i in train_data.columns[7:-2]]\nfig = plt.figure(figsize=(20,80))\nfig.suptitle('Features Box plot with 0.1% 99.9% whiskers',fontsize=22, y=.89)\ngrid =  gridspec.GridSpec(29,4,figure=fig,hspace=.5,wspace=.05)\ncounter = 0\nfor i in range(29):\n    for j in range(4):\n        subf = fig.add_subplot(grid[i, j]);\n        sns.boxplot(x= sample_train_data[featstr[counter]],saturation=.5,color= 'blue', ax= subf,width=.5,whis=(.1,99.9));\n        subf.set_xlabel('')\n        subf.set_title('{}'.format(featstr[counter]),fontsize=16)\n        counter += 1\n        gc.collect();","ec6b370e":"sns.set(rc={'figure.figsize':(10,5)})\nax = sns.distplot(sample_train_data[\"weight\"],color='green')","b8e34839":"y = sns.JointGrid(data=sample_train_data, x=\"weight\", y=\"resp\")\ny.plot_joint(sns.scatterplot, s=100, alpha=.5)\ny.plot_marginals(sns.distplot, kde=True,color='orange')","110fb96e":"sns.scatterplot(data=sample_train_data, x='resp',y='weight', color= 'green', alpha=.3)\nplt.title('Resp vs Weight\\ncorrelation={}'.format(round(sample_train_data.weight.corr(sample_train_data.resp),4)));","b4f224bc":"sns.scatterplot(data=train_data, x='resp',y='weight', color= 'blue', alpha=.3)\nplt.title('Resp vs Weight\\ncorrelation={}'.format(round(train_data.weight.corr(train_data.resp),4)));","74c1c5f1":"ratio=4\nf = plt.figure(figsize=(25,60))\nouter_grid = gridspec.GridSpec(7, 3, wspace=0.3, hspace=0.3)\nfor i, column in enumerate(['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']):\n    gs = gridspec.GridSpecFromSubplotSpec(ratio+1, ratio+1,\n            subplot_spec=outer_grid[i], wspace=0.3, hspace=0.3)\n    g = myjoint(y=\"weight\", x=column, data=train_data, ratio=ratio)\n    g = g.plot(sns.regplot, sns.distplot)\n    r2_score = r2(x=train_data[column],y=train_data[\"weight\"])\n    plt.xlabel(f\"{column} R2 score:{round(r2_score,4)}\")\n","447ad48d":"sample_train_data.iloc[:,2:7].hist(bins=100,figsize=(20,20),color='#ff6645');","c3121ce9":"sns.pairplot(sample_train_data[['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp']],corner=True);","859c22ee":"sns.heatmap(sample_train_data[['resp','resp_1','resp_2','resp_3','resp_4']].corr(), annot = True)","e29221a4":"sns.heatmap(sample_train_data[['resp','resp_1','resp_2','resp_3','resp_4','weight']].corr(), annot = True, vmin=-1, vmax=1, center= 0)","80ee7c43":"sample_train_data[['resp','resp_1','resp_2','resp_3','resp_4','weight']+nulls_fl].corr().style.background_gradient(cmap='coolwarm')","f507ffd6":"respcorr =  pd.Series([train_data.resp.corr(train_data[i]) for i in featstr],index=featstr)","d188a00e":"fig = px.bar(respcorr,color = respcorr, color_continuous_scale=['red','blue'], title= 'Features Correlation with Resp')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.show();","081edb66":"#taking in consideration weights larger than 0\nwcorr = pd.Series([train_data[train_data.weight != 0].weight.corr(train_data[train_data.weight != 0][i]) for i in featstr],index=featstr)\nwcorr.head()","6a79bbe4":"fig = px.bar(wcorr,title= 'Features Correlation with Weight (not including zero weights)')\nfig.layout.xaxis.tickangle = 300\nfig.layout.xaxis. dtick = 5\nfig.layout.xaxis.title = ''\nfig.layout.yaxis.title = 'pearson correlation'\nfig.update(layout_coloraxis_showscale=False)\nfig.update_layout(showlegend=False)\nfig.show()","26eed785":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(train_data[train_data.weight != 0].weight,train_data[train_data.weight != 0].feature_51, color = 'orange', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_51',fontsize=14)\nplt.title('Feature_51 vs Weight\\nCorrelation = {}%'.format(round(train_data[train_data.weight != 0].weight.corr(train_data[train_data.weight != 0].feature_51),4)*100),fontsize=16);","f243859d":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(train_data[train_data.weight != 0].weight,train_data[train_data.weight != 0].feature_50, color = 'orange', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_50',fontsize=14)\nplt.title('Feature_50 vs Weight\\nCorrelation = {}%'.format(round(train_data[train_data.weight != 0].weight.corr(train_data[train_data.weight != 0].feature_50),4)*100),fontsize=16);","16607616":"fig = plt.figure(figsize=(8,6))\nsns.scatterplot(train_data[train_data.weight != 0].weight,train_data[train_data.weight != 0].feature_126, color = 'darkblue', alpha=.3)\nplt.xlabel('Weight',fontsize=14)\nplt.ylabel('Featre_126',fontsize=14)\nplt.title('Feature_126 vs Weight\\nCorrelation{}%'.format(round(train_data[train_data.weight != 0].weight.corr(train_data[train_data.weight != 0].feature_126),4)*100),fontsize=16);","7d6729d1":"respcorr =  pd.Series([train_data.resp.corr(train_data[i]) for i in featstr],index=featstr,name=\"cor\").to_frame()\ngroup_01 = respcorr.loc[(respcorr[\"cor\"] > 0.02) & (respcorr[\"cor\"] < 0.03) ]\ngroup_01.head()","c52c5011":"group_02 = respcorr.loc[(respcorr[\"cor\"] >= 0.03)]\ngroup_02","65ecda57":"class myjoint(sns.JointGrid):\n    def __init__(self, x, y, data=None,height=7, ratio=5, space=.2,\n                 dropna=True, xlim=None, ylim=None, size=None):\n        super(myjoint, self).__init__(x, y, data,height, ratio, space,\n                 dropna, xlim, ylim, size)\n        plt.close(2)\n        # Set up the subplot grid\n        self.ax_joint = f.add_subplot(gs[1:, :-1])\n        self.ax_marg_x = f.add_subplot(gs[0, :-1], sharex=self.ax_joint)\n        self.ax_marg_y = f.add_subplot(gs[1:, -1], sharey=self.ax_joint)\n        # Turn off tick visibility for the measure axis on the marginal plots\n        plt.setp(self.ax_marg_x.get_xticklabels(), visible=False)\n        plt.setp(self.ax_marg_y.get_yticklabels(), visible=False)","3efe0f23":"corr = sample_train_data[group_01.index].corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corr, cmap='BrBG',  center=0,vmin=-1, vmax=1, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","d3b54b9f":"fig, axes = plt.subplots(nrows=3\n                         , ncols=3,figsize=(20,22))\nfor i, column in enumerate(group_01.index):\n    sns.distplot(sample_train_data[column],ax=axes[i\/\/3,i%3],color='blue')","729eb417":"ratio=4\nf = plt.figure(figsize=(25,60))\nouter_grid = gridspec.GridSpec(3, 3, wspace=0.3, hspace=0.3)\nfor i, column in enumerate(group_01.index):\n    gs = gridspec.GridSpecFromSubplotSpec(ratio+1, ratio+1,\n            subplot_spec=outer_grid[i], wspace=0.3, hspace=0.3)\n    g = myjoint(y=\"resp\", x=column, data=sample_train_data, ratio=ratio)\n    g = g.plot(sns.regplot, sns.distplot )\n    r2_score = r2(x=sample_train_data[column],y=sample_train_data[\"resp\"])\n    plt.xlabel(f\"{column} R2 score:{round(r2_score,4)}\")","87b64067":"corr = sample_train_data[group_02.index].corr()\nf, ax = plt.subplots(figsize=(15, 15))\nsns.heatmap(corr, cmap='BrBG',  center=0,vmin=-1, vmax=1, annot=True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c275cfb7":"fig, axes = plt.subplots(nrows=3, ncols=3,figsize=(20,22))\nfor i, column in enumerate(group_02.index):\n    sns.distplot(sample_train_data[column],ax=axes[i\/\/3,i%3],color='gray')","4839eee0":"ratio=4\nf = plt.figure(figsize=(25,60))\nouter_grid = gridspec.GridSpec(3, 3, wspace=0.3, hspace=0.3)\nfor i, column in enumerate(group_02.index):\n    gs = gridspec.GridSpecFromSubplotSpec(ratio+1, ratio+1,\n            subplot_spec=outer_grid[i], wspace=0.3, hspace=0.3)\n    g = myjoint(y=\"resp\", x=column, data=sample_train_data, ratio=ratio)\n    g = g.plot(sns.regplot, sns.distplot)\n    r2_score = r2(x=sample_train_data[column],y=sample_train_data[\"resp\"])\n    plt.xlabel(f\"{column} R2 score:{round(r2_score,4)}\")","23203d99":"fig, ax = plt.subplots(2,figsize=(10, 6))\nax[0].scatter(train_data[\"feature_45\"], train_data[\"resp\"],color = \"blue\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.7);\nax[1].scatter(train_data[\"feature_44\"], train_data[\"resp\"],color = \"black\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.5);","ad60ac50":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10,5))\nfor i, column in enumerate([\"feature_45\",\"feature_44\"]):\n    sns.distplot(sample_train_data[column],ax=axes[i],color='gray')","c8fe666c":"ratio=4\nf = plt.figure(figsize=(25,60))\nouter_grid = gridspec.GridSpec(3, 3, wspace=0.3, hspace=0.3)\nfor i, column in enumerate([\"feature_45\",\"feature_44\"]):\n    gs = gridspec.GridSpecFromSubplotSpec(ratio+1, ratio+1,\n            subplot_spec=outer_grid[i], wspace=0.3, hspace=0.3)\n    g = myjoint(y=\"resp\", x=column, data=sample_train_data, ratio=ratio)\n    g = g.plot(sns.regplot, sns.distplot)\n    r2_score = r2(x=sample_train_data[column],y=sample_train_data[\"resp\"])\n    plt.xlabel(f\"{column} R2 score:{round(r2_score,4)}\")","ee0ec91e":"fig, ax = plt.subplots(4, figsize=(20, 10))\nax[0].scatter(train_data[\"feature_3\"], train_data[\"resp\"],color = \"blue\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.7);\nax[1].scatter(train_data[\"feature_4\"], train_data[\"resp\"],color = \"blue\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.5);\nax[2].scatter(train_data[\"feature_5\"], train_data[\"resp\"],color = \"gray\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.7);\nax[3].scatter(train_data[\"feature_6\"], train_data[\"resp\"],color = \"gray\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.5);","5b7e66db":"ratio=4\nf = plt.figure(figsize=(25,60))\nouter_grid = gridspec.GridSpec(3, 3, wspace=0.3, hspace=0.3)\nfor i, column in enumerate([\"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\"]):\n    gs = gridspec.GridSpecFromSubplotSpec(ratio+1, ratio+1,\n            subplot_spec=outer_grid[i], wspace=0.3, hspace=0.3)\n    g = myjoint(y=\"resp\", x=column, data=sample_train_data, ratio=ratio)\n    g = g.plot(sns.regplot, sns.distplot)\n    r2_score = r2(x=sample_train_data[column],y=sample_train_data[\"resp\"])\n    plt.xlabel(f\"{column} R2 score:{round(r2_score,4)}\")","2e058eb4":"fig, ax = plt.subplots(3,figsize=(15, 6))\nax[0].scatter(train_data[\"feature_41\"], train_data[\"resp\"],color = \"blue\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.7);\nax[1].scatter(train_data[\"feature_42\"], train_data[\"resp\"],color = \"black\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.5);\nax[2].scatter(train_data[\"feature_43\"], train_data[\"resp\"],color = \"black\", edgecolors = \"white\", linewidths = 0.1, alpha = 0.5);","bf6582c4":"fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(10,5))\nfor i, column in enumerate([\"feature_41\",\"feature_42\",\"feature_43\"]):\n    sns.distplot(sample_train_data[column],ax=axes[i],color='gray')","f3eccc0c":"ratio=4\nf = plt.figure(figsize=(25,60))\nouter_grid = gridspec.GridSpec(3, 3, wspace=0.3, hspace=0.3)\nfor i, column in enumerate([\"feature_41\",\"feature_42\",\"feature_43\"]):\n    gs = gridspec.GridSpecFromSubplotSpec(ratio+1, ratio+1,\n            subplot_spec=outer_grid[i], wspace=0.3, hspace=0.3)\n    g = myjoint(y=\"resp\", x=column, data=sample_train_data, ratio=ratio)\n    g = g.plot(sns.regplot, sns.distplot)\n    r2_score = r2(x=sample_train_data[column],y=sample_train_data[\"resp\"])\n    plt.xlabel(f\"{column} R2 score:{round(r2_score,4)}\")","35bc1651":"sample_train_data_pca = sample_train_data\nsample_train_data_pca['action'] = np.where(sample_train_data['resp'] > 0,1,0)\nsample_train_data_pca.action = sample_train_data_pca.action.astype('category')","6276414c":"from sklearn.preprocessing import StandardScaler as scale\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import k_means\nfrom sklearn.model_selection import train_test_split as split \nfrom sklearn.model_selection import GridSearchCV as Grid\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna","7818c7fe":"scaler = scale()","37d5e690":"scaler.fit(sample_train_data_pca[featstr[1:]])","fcca0eb4":"df_pca = pd.DataFrame(scaler.transform(sample_train_data_pca[featstr[1:]]))\ndf_pca.columns = featstr[1:]\ngc.collect()\ndf_pca.head()","2de801d1":"pca =  PCA(n_components= 10).fit(df_pca)\ndf_pca = pd.DataFrame(pca.transform(df_pca))","e9b49855":"pcs = ['pc'+str(i+1) for i in range(10)]","aa162f6c":"df_pca.columns = pcs\ndf_pca['action'] = sample_train_data_pca.action.values\ndf_pca['weight'] = sample_train_data_pca.weight.values\ndf_pca['resp'] = sample_train_data_pca.resp.values\ndf_pca.head()","66aec0b2":"df_pca.corr().style.background_gradient(cmap='coolwarm')","760a90e0":"kmeans = k_means(n_clusters= 10, max_iter= 400, random_state= 0,X=df_pca[pcs])\ndf_pca['cluster'] = kmeans[1]\ndf_pca['cluster'] = df_pca['cluster'].astype('category')\ndf_pca.head()","97f687c7":"fig = plt.figure(figsize=(12,6))\nax = plt.subplot(1,1,1)\nsns.countplot(data=df_pca,x='cluster',hue='action',ax=ax,palette='viridis')\nh, l = plt.gca().get_legend_handles_labels()\nplt.legend(h,['Negative','Positive'],ncol= 1, fontsize= 12, loc= 1,title= 'Resp',title_fontsize=14)\nplt.xlabel('Clusters',fontsize=18)\nplt.ylabel('')\nplt.xticks(fontsize=14) \nplt.title('PCA Clusters and Resp', fontsize=22);","2b80e439":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","086db79a":"train_data_pca = train_data\ntrain_data_pca['action'] = np.where(train_data_pca['resp'] > 0,1,0)\ntrain_data_pca.action = train_data_pca.action.astype('category')","e1c8b1bd":"\nfeatstr = [i for i in train_data.columns[7:-2]]\nscaler.fit(train_data_pca[featstr[1:]])\n \ndf_pca = pd.DataFrame(scaler.transform(train_data_pca[featstr[1:]]))\ndf_pca.columns = featstr[1:]\ngc.collect()\ndf_pca.head()","6357497b":"pca =  PCA(n_components= 40).fit(df_pca)\ndf_pca = pd.DataFrame(pca.transform(df_pca))","e54815d2":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","766fc149":"X_train = train_data.loc[:, train_data.columns.str.contains('feature')]\ny_train = train_data.loc[:, 'action']","2e472266":"sns.set_palette(\"colorblind\")\nax = sns.barplot(y_train.value_counts().index, y_train.value_counts()\/len(y_train))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();\n#Data Is balanced for modeling ","49fd1565":"## Linear Regression Analysis of Group 01\n\n* Here i am going to find out the relationship between features of Group 01 and resp.","acdd210c":"#### We can see that features are not really correlated to Resp","d9e408c1":"# Features Anslysis from LightGBM model\n\n* Below picture is the feature importance clustering of LightGBM Model.\n* The darker labels represent features that have higher feature importance in the LightGBM model.\n* This picture is taken from some other notebook link is given in references.","15ca408c":"Exploring the possible clusters in the PCA dataframe","91f2a291":"## PCA on All Data set","420e1607":"correlation in the PCA dataframe","bf9ff34f":"Negitive relation between weight and resp(target variable) can be seen in Linear Regression analysis above.","d8f7ab03":"# PCA","52d0ce79":"# Cumulative Return of resp","f7993bde":"### Lets see Trends of resp","312f77e4":"### The first 40 principal components explains about 95% of the variation","c2ec0671":"Now we check the lowest correlation","270f9a8e":"# Data Overview","e3e5052c":"### Coorelation charts of resp features","5c8a042e":"#### Reduce the dimensionality of the data to 8 principal components","8763e51a":"\n# Data Description\n\nThis dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n\nIn the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n\nThis is an extensive data analysis for the jane street market dataset, this notebook will go through the train and features csv's for an extensive exploratory data analysis, Also some data cleaning and preprocessing will be done along the way\n\n","5bd6cd18":"# Clustering","c07c4f7f":"## Linear Regression Analysis of Group 02\n\n*     Here i am going to find out the relationship between features of Group 02 and resp.\n","191756a5":"# Tag 6 Group Featues\n\nFeatures 6, 3, 4, 5 belong to Tag 6 ","62dcbaf1":"### Referrences\n\n* https:\/\/www.kaggle.com\/marketneutral\/jane-street-eda-regime-tags-clusters\n* https:\/\/www.kaggle.com\/hamzashabbirbhatti\/eda-a-quant-s-prespective","6d0e6d74":"# Weights\n\nAnalyze the distibution of feature \"weight\"\n","1c751486":"# Resp\nresp is predictive variable. Here i will tried to analyze this group of variables to find out valueable insights.","c3fb5815":" Second highest correlation which belongs to features 50\n","e00fad28":"# Tag 18 Group Featues","ce07849a":"Component and Variance Graph","af3f023b":"# About me\n\nWorking as Data Scientist in IT firm in Pakistan. I was Recently Enguaged with Radix Trading LLC which is a firm just like Jane Street which also work in High frequency algorithmic trading. Where i worked as Quantitative Researcher (Quant) to Capture Price movement in High frequency Algorithmic trading through Alphas. Designed many successful Alpha\/strategies which is trade-able in real Stock market. For more details kindly visit my linkedin profile.\n\nPlease upvote if you find this notebook helpful! \ud83d\ude0a Thank you! I would also be very happy to receive feedback on my work.\n","1d0b1e0e":"These features pair which shows there is redundant information we can drop one feature.","ca1f67f3":"# Tag 14 Group Featues\n\nFeatures 42.43,41 belong to Tag 14 ","84f73cc4":"Bird view of resp features with pair plot","6fb85c3e":"**Relation between features and resp(target variable) is Present in Linear Regression analysis above**\n\n* Positive relation between features 122,34,40,53,32 and resp(target variable) can be seen in Linear Regression analysis above.\n* Feature 34 has highest R2 score in this Group of 0.0321. \n* Features 11 has lowest R2 score of 0.0 in this Group of features.\n* We can see from the distribution is that all 5 of these resp features are perfectly zero reverted.","dcd3e0e9":"# Creating Groups of features on the basic of coorelation with respect to res\n","609f55e8":"![__results___53_0.png](attachment:__results___53_0.png)","bce6c6f4":"### Distribution shows that all of these resp features are perfectly zero reverted**","bfd67247":"**Coorelation charts of resp with features list having missing values more then 10%****","6cb3104b":"\n## Findings:\n\nBased on my Experience as Quantative Researcher it is come to my knowledge that itraday trading patterns in High Frequency Algorithmic Trading are far more linked than iterday Patterns. Since not enough information is given about these features.These packets come in market can be any type usually the main type of packets that we receive of different instrument from stock exchange are about Trade, Delete, update, Reduce Messages Packets about changes in order book.\n\nLooking at the Distribution of dataset it seems that most of the features are Normally distributed (Standardized) and they are mean\/Zero reverted. Later i will try to analyze these features using different categories.\n\nBy looking mutiple features distibution we can said that they are using Zero Mean Reverted Startgies in Trading.\n\nBy looking at the Feature 0 it has two distinct values 0,1 it seems that it is a trading signal.\n\nLooking at the Histogram of features from it seems that dataset has has been provided has features that are Zero reverted.\n\nAll the feature that are in same tag has some correlation with each other.\n\n**I am going to add lot of things (and remember that work in still in progress).**","071b0b1c":"### The first 10 principal components explains about 70% of the variation\n### Lets increase pca size ","d5cd3db0":"# Group 01 w.r.t corr\n\nThis Group included features that are created on basic of coorelation between 0.02 to 0.03 with respect to res .","08ccea1f":"PCA did't really help as principal components still don't have a clear relation with resp","ad533030":"# Features and resp Correlation","a5745b11":"### Now we can take a bird's-eye view of features distributions","c6bd2659":"## Correlation Analysis of Group 01","6740efc7":"there is some kind negative correlation between weight and feature 126 the relation seem to be weak","c607c049":"## Correlation Analysis of Group 02","095e4aa5":"These features 45 and 44 has lowest R2 score 0.0001 in Linear Regression analysis ","92c48c1b":"# Handling Null Values","72556099":"Plotting the relation between clusters and Resp","029f9ecd":"# Linear Regression analysis weight vs resp","45e7ade6":"# Features and Weight\n","30530172":"Above Distribution of dataset it seems that most of the features are Normally distributed (Standardized) and they are mean\/Zero reverted\n\n**Now we make a boxplot grid again with customized .1% : 99.9% whiskers of sample data.**","0f8c1ddb":"Adding weight, Resp and action to the new dataframe","efbce60e":"# Group 02 w.r.t corr\n\nThis Group included features that are created on basic of coorelation grater then 0.03 with respect to res .","99ed1630":"* **Lower weight trades have a much higher dispersion in resp.**\n* **This implies that weight is some kind of predictor of future return volatility**.","2a09a39c":"**Relation between features and resp(target variable) is Present in Linear Regression analysis above**\n\n*     Positive relation between features 27,28,29,31,33,35 and resp(target variable) can be seen in Linear Regression analysis above.\n*     Feature 29 has highest R2 score in this Group of 0.0726.\n*     Features 28 has lowest R2 score of 0.2 in this Group of features.\n*     We can see from the distribution is that all 5 of these resp features are perfectly zero reverted.\n\n","cd38c2ba":"we see with highest correlation which belongs to features 51","0cc1671e":"Due to itraday trading patterns in High Frequency Algorithmic Trading.So thats why i have decided to look at only one data of data for my Explaintory Data Analysis","32d4cab4":"Clusters seem to be also scattered with no clear relation between a cluster and resp"}}