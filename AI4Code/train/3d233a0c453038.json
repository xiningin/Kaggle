{"cell_type":{"85fe9f41":"code","219855ee":"code","2b0ddd0b":"code","8961454f":"code","18ebfa2d":"code","f199c22d":"code","eb84aff6":"code","bff5390a":"code","2f850a2b":"code","669202e1":"code","554353ee":"code","dc320bbe":"code","e4190bfb":"code","979129f6":"code","d3ad65b7":"code","0ad94ca0":"code","a1351d96":"code","e4c3afd6":"code","f00b050e":"code","282d95f4":"code","9b63c26b":"code","854dbb00":"code","63095acf":"code","dd887d91":"code","a948c825":"code","7c685e61":"code","4632b7e7":"code","db936aa2":"code","18cfa46b":"code","59338f28":"code","32c7991f":"code","ebc1573e":"code","bf6b1914":"code","1449e5f5":"code","3a4418d3":"code","b02ffa01":"code","7801b0ec":"code","268fe085":"code","18d66b04":"code","57093a1b":"code","c35eeb11":"code","aac998e6":"code","75ec113d":"code","58c2b030":"markdown","e83fdb22":"markdown","b1a1e349":"markdown","60049a5e":"markdown","98ed923d":"markdown","79895d47":"markdown","fb7522fd":"markdown","449d692e":"markdown","55169530":"markdown","416ed0a7":"markdown","40e4e931":"markdown","a583fe72":"markdown","1e76b685":"markdown","13afea82":"markdown","68d0fc7f":"markdown","b83d2c30":"markdown","2b2bf612":"markdown","72555480":"markdown","030681e4":"markdown","1880b6c8":"markdown","cfae9ac3":"markdown","78bca19c":"markdown","77d8cb64":"markdown","e896d3eb":"markdown","c5424589":"markdown","84e7d558":"markdown","d11cf409":"markdown","6471a288":"markdown","792b3436":"markdown","c75f8c49":"markdown","ca5492e9":"markdown","ba4c92f1":"markdown","e3de9888":"markdown","809c9136":"markdown"},"source":{"85fe9f41":"import pandas            as pd\nimport numpy             as np\nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport statsmodels.api   as sm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection   import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model      import LogisticRegression\nfrom sklearn.metrics           import classification_report\nfrom sklearn.tree              import DecisionTreeClassifier\nfrom sklearn.ensemble          import RandomForestClassifier\nfrom scipy.stats               import randint as sp_randint\nfrom imblearn.over_sampling    import SMOTE","219855ee":"df = pd.read_csv('..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv')\ndf.head()","2b0ddd0b":"print(\"There are {} rows and {} columns in the dataset.\".format(df.shape[0],df.shape[1]))","8961454f":"df.info()","18ebfa2d":"# Five point summary of the dataset\n\ndf.describe().T","f199c22d":"print(\"There are {} missing records in the dataset.\".format(df.isnull().sum().sum()))","eb84aff6":"# Storing feature names in variable 'cols'\n\ncols = df.columns.tolist()","bff5390a":"for i in [ 'SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n    plt.figure(figsize=(10,5))\n    sns.countplot(df[i])\n    plt.show()","2f850a2b":"# Boxplot for Bill_Amt vs Limit_bal\n\nplt.figure(figsize=(10,7))\nsns.boxplot(data=df[['LIMIT_BAL','BILL_AMT1', 'BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']])\nplt.show()","669202e1":"# Boxplot for Pay_Amt vs Limit_bal\n\nplt.figure(figsize=(10,7))\nsns.boxplot(data=df[['LIMIT_BAL','PAY_AMT1','PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']])\nplt.show()","554353ee":"# Boxplot for column 'AGE'\nplt.figure(figsize=(5,5))\nsns.boxplot(data=df['AGE'])\nplt.show()","dc320bbe":"# Outliers on numberical columns\n\nnum_var = df.select_dtypes(exclude='object')\nfor i in num_var:\n    \n    q1 = df[i].quantile(0.25)\n    q3 = df[i].quantile(0.75)\n\n    IQR = q3 - q1\n    UL = q3 + 1.5*IQR\n    LL = q1 - 1.5*IQR\n\n    print('IQR of',i,'= ',IQR)\n    print('UL of',i,'= ',UL)\n    print('LL of',i,'= ',LL)\n    print('Number of Outliers in',i,' = ',(df.shape[0] - df[(df[i]<UL) & (df[i]>LL)].shape[0]))\n    print(' ')","e4190bfb":"mi0 = df[df['default.payment.next.month']==0]\nmi1 = df[df['default.payment.next.month']==1]","979129f6":"con_col=['AGE','LIMIT_BAL','BILL_AMT1', 'BILL_AMT2','BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1',\n       'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n\nfor i in con_col:\n    plt.figure(figsize=(20,5))\n    sns.distplot(mi0[i],color='g')\n    sns.distplot(mi1[i],color='r')\n    plt.show()","d3ad65b7":"plt.figure(figsize=(25,20))\nsns.heatmap(df.corr(),annot=True)\nplt.show()","0ad94ca0":"sns.pairplot(df)\nplt.show()","a1351d96":"def age(x):\n    if x in range(21,41):\n        return 1\n    elif x in range(41,61):\n        return 2\n    elif x in range(61,80):\n        return 3\n\ndf['AGE']=df['AGE'].apply(age)","e4c3afd6":"def bins(x):\n    if x == -2:\n        return 'Paid Duly'\n    if x == 0:\n        return 'Paid Duly'\n    if x == -1:\n        return 'Paid Duly'\n    if x in range(1,4):\n        return '1 to 3'\n    if x in range(4,7):\n        return '4 to 6'\n    if x in range(7,9):\n        return '7 to 9'\n\nfor i in df[['PAY_0','PAY_2','PAY_3','PAY_4','PAY_5','PAY_6']]:\n    df[i]=df[i].apply(bins)","f00b050e":"def rep(x):\n    if x in [0,4,5,6]:\n        return 4\n    else:\n        return x\ndf['EDUCATION']=df.EDUCATION.apply(rep)","282d95f4":"# Dataset after feature engineering\n\ndf.head()","9b63c26b":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in df.select_dtypes(include=object).columns:\n    df[col] = le.fit_transform(df[col])","854dbb00":"X =df.drop('default.payment.next.month',axis=1)\ny = df['default.payment.next.month']","63095acf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","dd887d91":"print((df['default.payment.next.month'].value_counts()\/df['default.payment.next.month'].shape)*100)\nsns.countplot(df['default.payment.next.month'])\nplt.show()","a948c825":"print('Before OverSampling, the shape of train_X: {}'.format(X_train.shape)) \nprint('Before OverSampling, the shape of train_y: {} \\n'.format(y_train.shape))","7c685e61":"smote = SMOTE(sampling_strategy='minority')\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)","4632b7e7":"print('After OverSampling, the shape of train_X: {}'.format(X_train_sm.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_sm.shape))","db936aa2":"logreg = LogisticRegression(solver='liblinear', fit_intercept=True)\n\nlogreg.fit(X_train_sm, y_train_sm)\n\ny_prob_train = logreg.predict_proba(X_train_sm)[:,1]\ny_pred_train = logreg.predict (X_train_sm)\n\nprint('Classification report - Train: ', '\\n', classification_report(y_train_sm, y_pred_train))\n\ny_prob = logreg.predict_proba(X_test)[:,1]\ny_pred = logreg.predict (X_test)\n\nprint('Classification report - Test: ','\\n', classification_report(y_test, y_pred))","18cfa46b":"Xc=sm.add_constant(X_train_sm)\nmodel = sm.Logit ( y_train_sm , Xc ).fit ( )","59338f28":"model.summary ( )","32c7991f":"cols = list(X_train_sm.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X_train_sm[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y_train_sm,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(\"Important features: {}\".format(selected_features_BE))\nprint(\"\\nNumber of important features: {}\".format(len(selected_features_BE)))","ebc1573e":"# Adding target column\n\nselected_features_BE.append('default.payment.next.month')\ndf2=df[selected_features_BE]","bf6b1914":"X = df2.drop('default.payment.next.month',axis=1)\ny = df2['default.payment.next.month']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\nsmote = SMOTE(sampling_strategy='minority')\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)\n\nlogreg = LogisticRegression(solver='liblinear', fit_intercept=True)\n\nlogreg.fit(X_train, y_train)\n\ny_prob_train = logreg.predict_proba(X_train)[:,1]\ny_pred_train = logreg.predict (X_train)\n\nprint('Classification report - Train: ', '\\n', classification_report(y_train, y_pred_train))\n\ny_prob = logreg.predict_proba(X_test)[:,1]\ny_pred = logreg.predict (X_test)\n\nprint('Classification report - Test: ','\\n', classification_report(y_test, y_pred))","1449e5f5":"# Defining an object for DTC and fitting for whole dataset\ndt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=10, random_state=1 )\ndt.fit(X_train_sm, y_train_sm)\n\ny_pred_train = dt.predict(X_train_sm)\ny_pred = dt.predict(X_test)\ny_prob = dt.predict_proba(X_test)","3a4418d3":"#Classification for test before hyperparameter tuning\nprint(classification_report(y_test,y_pred))","b02ffa01":"dt = DecisionTreeClassifier(random_state=1)\n\nparams = {'criterion': ['gini','entropy'],\n          'splitter' : [\"best\", \"random\"],\n          'max_depth' : [2,4,6,8,10,12],\n          'min_samples_split': [2,3,4,5],\n          'min_samples_leaf': [1,2,3,4,5]}\n\nrand_search_dt = RandomizedSearchCV(dt, param_distributions=params, cv=3)\n\nrand_search_dt.fit(X_train_sm,y_train_sm)\n\nrand_search_dt.best_params_","7801b0ec":"# Passing best parameter for the Hyperparameter Tuning\ndt = DecisionTreeClassifier(**rand_search_dt.best_params_, random_state=1)\n\ndt.fit(X_train_sm, y_train_sm)\n\ny_pred = dt.predict(X_test)","268fe085":"#Classification for test after hyperparameter tuning\nprint(classification_report(y_test,y_pred))","18d66b04":"#Create a Gaussian Classifier\nrfc=RandomForestClassifier(n_estimators=100, random_state=1)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nrfc.fit(X_train_sm,y_train_sm)\n\ny_pred = rfc.predict(X_test)","57093a1b":"#Classification for test after hyperparameter tuning\nprint(classification_report(y_test,y_pred))","c35eeb11":"rfc = RandomForestClassifier(random_state=1)\n\nparams = {'n_estimators': sp_randint(5,30),\n          'criterion' : ['gini','entropy'],\n          'max_depth' : sp_randint(2,10),\n          'min_samples_split' : sp_randint(2,20),\n          'min_samples_leaf' : sp_randint(1,20),\n          'max_features' : sp_randint(2,18)}\n\nrand_search_rfc = RandomizedSearchCV(rfc, param_distributions=params, random_state=1, cv=3)\n\nrand_search_rfc.fit(X_train_sm,y_train_sm)\n\nrand_search_rfc.best_params_","aac998e6":"# Passing best parameter for the Hyperparameter Tuning\nrfc = RandomForestClassifier(**rand_search_rfc.best_params_, random_state=1)\n\nrfc.fit(X_train_sm, y_train_sm)\n\ny_pred = rfc.predict(X_test)","75ec113d":"#Classification for test after hyperparameter tuning\nprint(classification_report(y_test,y_pred))","58c2b030":"## Encoding categorical variable:","e83fdb22":"#### Hyperparameter Tuning:","b1a1e349":"# Read the dataset and display first five rows:","60049a5e":"### Base Model using LogisticRegression:","98ed923d":"## Project instructions\n1.\tPerform the required data pre-processing to treat for missing values and outliers\n2.\tPerform exploratory data analysis to visualise the spread of each of the X variables and the relationship between the various X variables and the Y variable\n3.\tIdentify any multi-collinearity and perform dimensionality reduction like PCA if required\n4.\tUse the original X variables or the components from PCA & divide the given data into train and test sets\n5.\tBuild a model to predict the default propensity of a customer\n6.\tEvaluate the model based on model performance measures for classification and recommend the most suitable model.\n7.\tCome up with recommendations \/ actionable insights based on feature importance scores derived from the model.\n","79895d47":"## Feature Engineering:","fb7522fd":"#### Heat map:","449d692e":"__Objective__ \u2013 Predict the probability of a customer defaulting payment for the credit card the subsequent month, based on past information. The past information is provided in the dataset. This probability will help the collections team to prioritise follow up with customers who have a high propensity of defaulting.","55169530":"#### Hyperparameter Tuning:","416ed0a7":"# To see the datatypes of the column:","40e4e931":"#### Distribution Plot:","a583fe72":"Here we can see that the data is imbalanced.","1e76b685":"### Random Forest:","13afea82":"#### Pairplot:","68d0fc7f":"# Exploratory Data Analysis:","b83d2c30":"#### Outliers:","2b2bf612":"## Splitting dataset in dependent and independent variable:","72555480":"## Additional information\n1.\tThe difference between the bill amount and the past pay for a month will give an indication of the pending amount for each month and can be used as an additional X variable\n2.\tThe average \/ sum of bill amount can be used as an additional X variable which will give an indication on the customer value.\n","030681e4":"## Checking if Data is Imbalance:","1880b6c8":"Inference:\n    - There are around 30000 distict credit card clients.\n    - The average value of credit card Limits is Rs 1,67,484.\n    - The Limited Balance has a high Standard deviation as the meadian value is Rs 1,40,000 and the extreme values as Rs 10,00,000.\n    - Here the average is about 35 and meadian is 28 with a standard deviation of 9.2. This difference is explained by some very old people in the data set as given that the maximum age is 79.\n    - Bill Amount and Pay Amount also shows us that there some people with extremely high bill amount which may be because for the higher Credit Limit or because of the pending dues added up. \n    - Bill amount for all the months, the mean is around 40,000 to 50,000 with some extreme amount in bill amount 3 of Rs 16,64,089.\n    - Pay amount for all the months, the mean is around Rs 4800 to Rs 5800, with some extreme values such as Rs 16,64,089.\n    - As the value 0 for default payment means 'not default' and value 1 means 'default', the mean of 0.221 means that there are 22.1% of credit card contracts that will default next month (will verify this in the next sections of this analysis).","cfae9ac3":"#### Binning the 'PAY' column","78bca19c":"# Descriptive Statistics:","77d8cb64":"# About the dataset\nThe dataset contains the following information of 30000 customers:\n1.\t__DEFAULT__ - Default payment next month (Yes=1, No=0)\n2.\t__LIMIT_BAL__ - Amount of the given credit (INR)  \n3.\t__SEX__ - Gender (1 = male; 2 = female)\n4.\t__EDUCATION__ - Education (1 = graduate school; 2 = university; 3 = high school; 4 = others)\n5.\t__MARRIAGE__ - (1 = married; 2 = single; 3 = others)\n6.\t__AGE__ - (year)\n7.\t__PAST_PAY__ - History of repayment status\n    <br> -  PAST_PAY1 = the repayment status in September 2005\n    <br> -  PAST_PAY2 = the repayment status in August 2005\n    <br> -  PAST_PAY6 = the repayment status in April 2005\n    <br> -  The measurement scale for the repayment status is \n         i.  -1 = pay duly <br>\n        ii.\t 1 = payment delay for one month<br>\n        iii. 2 = payment delay for two months <br>\n        iv.\t 8 = payment delay for eight months<br>\n        v.\t 9 = payment delay for nine months and above <br>\n8.\t__BILL_AMT__- Amount of bill statement (INR)\n    <br> -  BILL_AMT1 = amount of bill statement in September 2005\n    <br> -  BILL_AMT2 = amount of bill statement in August 2005\n    <br> -  BILL_AMT6 = amount of bill statement in April 2005\n9.\t__PAY_AMT__ - Amount of previous payment (INR)\n    <br> -  PAY_AMT1 = amount paid in September 2005\n    <br> -  PAY_AMT2 = amount paid in August 2005\n    <br> -  PAY_AMT6 = amount paid in April 2005\n","e896d3eb":"#### Balancing the dataset:","c5424589":"## Modelling:","84e7d558":"## Splitting dataset into train and test split:","d11cf409":"### Feature selection- Backward Elimination:","6471a288":"# $$ \\color{red}{Credit\\ Card\\ Default\\ Propensity\\ Prediction} $$    ","792b3436":"# Importing important libraries:","c75f8c49":"### Decission Tree:","ca5492e9":"#### BOX PLOT - Outliers:","ba4c92f1":"#### Binning the 'AGE' column","e3de9888":"### Logistic Regression with important feature:","809c9136":"#### Replacing 0,5,6 to 4 in education columns"}}