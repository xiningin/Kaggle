{"cell_type":{"f31aca9e":"code","10e99835":"code","82ab2fd2":"code","21cc374a":"code","a32ea449":"code","9fb0f82e":"code","a7093949":"code","acd2c281":"code","77d45ff9":"code","a2470710":"code","8fb75d0a":"code","6ff63cfe":"code","6ba3b836":"code","cad7bff6":"code","a112aa29":"code","6cfcb8fa":"code","6d137933":"code","557df9a6":"code","c287b139":"code","a0893222":"code","3e4964be":"code","75ea2d59":"code","f0b4df0c":"code","d47be187":"code","54a62ab6":"code","1713f4f6":"code","c34b5666":"code","c8f729af":"code","318ec2cb":"code","4202d585":"code","ebc8c3df":"code","8b6678a0":"code","16b21869":"code","b0c59e13":"code","0ab47b66":"code","668a7f28":"code","7330732f":"code","0ad4dc84":"code","f6f243d4":"code","ed9c6afe":"code","f7e31e5b":"code","6ead314f":"code","3cdae6e8":"code","afc5d2a2":"code","aea96549":"code","cacd8a22":"code","19962f18":"code","26e75206":"code","bdbac584":"code","fad3ac1a":"code","ff34c2af":"code","bcdb8828":"code","b63fa142":"code","46b6a345":"code","41be510d":"code","0dd8020e":"code","7d4b033a":"code","51f184b9":"code","cb9b1afe":"code","94ccf14b":"code","ad6a8038":"code","93785f78":"code","495857cf":"code","4c8e47c6":"code","21cf9a2f":"code","97fe784a":"code","8ff2af00":"code","f7a80f44":"code","737e7f40":"code","b75739fc":"markdown","37c5b83f":"markdown","73b4cde2":"markdown","0d1a49a0":"markdown","048f4e65":"markdown","19669e9c":"markdown","2509df4c":"markdown","15886c74":"markdown","3775e2dc":"markdown","a22e972d":"markdown","6ceee621":"markdown","dc566f66":"markdown","2ec04da2":"markdown","e0552e66":"markdown","9ef9b3dc":"markdown","6a6c5b97":"markdown","5d34d5d8":"markdown","c6529373":"markdown","997b3ec7":"markdown","e3f3b035":"markdown","dc012fd0":"markdown","4b976d85":"markdown","8ea505af":"markdown","f380cb3f":"markdown","b7a07d17":"markdown","5a965442":"markdown","c648eb31":"markdown","ba7b434c":"markdown","b40fa632":"markdown","502b6389":"markdown","0bbc725c":"markdown","e2a368ff":"markdown","35ca19ea":"markdown","1668e1b6":"markdown","dcd48535":"markdown","76101c52":"markdown"},"source":{"f31aca9e":"import numpy as np \nimport pandas as pd\n\n# plots\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import FuncFormatter\nfrom matplotlib.ticker import PercentFormatter\nimport matplotlib.ticker as ticker\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nfrom numpy.polynomial.polynomial import polyfit\nfrom matplotlib.patches import Rectangle\n\n# t-test\nfrom scipy import stats\n\n# regression - statsmodels\nimport statsmodels.regression.linear_model as sm\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# regression - sklearn\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# sqrt\nfrom math import sqrt\n\n# Word Cloud\nfrom wordcloud import WordCloud\n\n# function for inserting a row to a location\ndef Insert_row_(row_number, df, row_value): \n    # Slice the upper half of the dataframe \n    df1 = df[0:row_number]   \n    # Store the result of lower half of the dataframe \n    df2 = df[row_number:]  \n    # Inser the row in the upper half dataframe \n    df1.loc[row_number]=row_value \n    # Concat the two dataframes \n    df_result = pd.concat([df1, df2]) \n    # Reassign the index labels \n    df_result.index = [*range(df_result.shape[0])] \n    # Return the updated dataframe \n    return df_result","10e99835":"data_ds=pd.read_csv('..\/input\/data-scientist-jobs\/DataScientist.csv')\ndata_da=pd.read_csv('..\/input\/data-analyst-jobs\/DataAnalyst.csv')\ndata_de=pd.read_csv('..\/input\/data-engineer-jobs\/DataEngineer.csv')\ndata_ba=pd.read_csv('..\/input\/business-analyst-jobs\/BusinessAnalyst.csv')\n\ndata_ds = data_ds.drop(['Unnamed: 0','index'],axis=1)\ndata_da = data_da.drop(['Unnamed: 0'],axis=1)\ndata_ba = data_ba.drop(['Unnamed: 0','index'],axis=1).head(3692) # drop due to columns messed-up\n\ndata = pd.concat([data_ds,data_da,data_de,data_ba]).reset_index(drop=True)","82ab2fd2":"# Check for missing values\ndef missing_values_table(df):\n    # number of missing values\n    mis_val = df.isnull().sum()\n    # % of missing values\n    mis_val_percent = 100 * mis_val \/ len(df)\n    # make table # axis '0' concat along index, '1' column\n    mis_val_table = pd.concat([mis_val,mis_val_percent],axis=1) \n    # rename columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0:'Missing Values',1:'% of Total Values'})\n    # sort by column\n    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1]!=0].sort_values(\n        '% of Total Values',ascending=False).round(1) #Review\n    print(\"Your selected datset has \"+str(df.shape[1])+\" columns and \"+str(len(df))+\" observations.\\n\"\n         \"There are \"+str(mis_val_table_ren_columns.shape[0])+\" columns that have missing values.\")\n    # return the dataframe with missing info\n    return mis_val_table_ren_columns","21cc374a":"# replace rating values in string\/bool to float\ndata['Rating'] = data['Rating'].astype(float)\ndata['Easy Apply'] = data['Easy Apply'].replace('True',1)\n\n# Replace -1 or -1.0 or '-1' to NaN\ndata=data.replace(-1,np.nan)\ndata=data.replace(-1.0,np.nan)\ndata=data.replace('-1',np.nan)","a32ea449":"#Remove Rating values from Company Name. \ndata['Company Name'],_=data['Company Name'].str.split('\\n', 1).str\n# 1st column after split, 2nd column after split (delete when '_')\n# string.split(separator, maxsplit) maxsplit default -1, which means all occurrances\n\n# Split salary into two columns min salary and max salary.\ndata['Salary Estimate'],_=data['Salary Estimate'].str.split('(', 1).str\n\n#exclude hourly rating salaries\ndata=data[(data['Salary Estimate'].str.contains(' Per Hour'))==False].reset_index(drop=True)\n\n# Split salary into two columns min salary and max salary.\n# lstrip is for removing leading characters; rstrip is for removing rear characters\ndata['Min_Salary'],data['Max_Salary']=data['Salary Estimate'].str.split('-').str\ndata['Min_Salary']=data['Min_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\ndata['Max_Salary']=data['Max_Salary'].str.strip(' ').str.lstrip('$').str.rstrip('K').fillna(0).astype('int')\n\n# To estimate the salary with regression and other analysis, better come up with one number: Est_Salary = (Min_Salary+Max_Salary)\/2\ndata['Est_Salary']=(data['Min_Salary']+data['Max_Salary'])\/2\n\n# Create a variable for how many years a firm has been founded\ndata['Years_Founded'] = 2020 - data['Founded'].astype(float)\n\n# Separate 'City' & 'State' from job 'Location'\ndata['City'],data['State'] = data['Location'].str.split(', ',1).str\ndata['HQCity'],data['HQState'] = data['Headquarters'].str.split(', ',1).str\n\n# Clean up duplicated city names in State's name\ndata['State']=data['State'].replace('Arapahoe, CO','CO')\ndata['State']=data['State'].replace('Los Angeles, CA','CA')\ndata['HQState']=data['HQState'].replace('NY (US), NY','NY')\n\n","9fb0f82e":"# Group up number of hires by company's revenue\nRevCount = data.groupby('Revenue')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True)\n#Make the Revenue column clean\nRevCount[\"Revenue_USD\"]=['Unknown','10+ billion','100-500 million','50-100 million','2-5 billion','10-25 million','1-2 billion','1-5 million','25-50 million','5-10 billion','<1 million','0.5-1 billion','5-10 million']\n#Merge the new Revenue back to data\nRevCount2 = RevCount[['Revenue','Revenue_USD']]\nRevCount = RevCount.merge(data, on='Revenue',how='left')\ndata=data.merge(RevCount2,on='Revenue',how='left')","a7093949":"missing_values_table(data)","acd2c281":"# create a new dataset from original data\ntext_Analysis = data[['Job Title','Job Description','Est_Salary','Max_Salary','Min_Salary','City','State','Easy Apply','Revenue_USD','Rating','Size','Industry','Sector','Type of ownership','Years_Founded','Company Name','HQState']]","77d45ff9":"# set minimum sample size for t-test and other statistical computings.\nssize = 30","a2470710":"# write get_keyword function\ndef get_keyword(x, note, VarSet):\n   x_ = x.split(\" \")\n   keywords = []\n   try:\n      for word in x_:\n         if word + note in np.asarray(VarSet):\n            keywords.append(word + note)\n   except:\n      return -1\n\n   return keywords","8fb75d0a":"#Remove special characters.\ntext_Analysis['Revenue_USD'] = text_Analysis['Revenue_USD'].replace('Unknown','RevUnknown')\ntext_Analysis['Size'] = text_Analysis['Size'].replace('Unknown','SizeUnknown')\ntext_Analysis['Sector'] = text_Analysis['Sector'].replace('[^A-Za-z0-9]+', '_',regex=True).replace(['Government','Unknown'],['GovSec','SectorUnknown'])\ntext_Analysis['Industry'] = text_Analysis['Industry'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','IndUnknown')\ntext_Analysis['Type of ownership'] = text_Analysis['Type of ownership'].replace('[^A-Za-z0-9]+', '_',regex=True).replace('Unknown','OwnUnknown')","6ff63cfe":"#Rename column name for running regression later.\ntext_Analysis = text_Analysis.rename(columns={\"Easy Apply\":\"Easy_Apply\"})","6ba3b836":"# remove special characters and unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job Title'].str.upper().replace('[^A-Za-z0-9]+', ' ',regex=True)\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['\u00c2','AND ','WITH ','SYSTEMS','OPERATIONS','ANALYTICS','SERVICES','ENGINEERS','NETWORKS','GAMES','MUSICS','INSIGHTS','SOLUTIONS','JR ','MARKETS','STANDARDS','FINANCE','PRODUCTS','DEVELOPERS','SR ',' 2'],\n    ['','','','SYSTEM','OPERATION','ANALYTIC','SERVICE','ENGINEER','NETWORK','GAME','MUSIC','INSIGHT','SOLUTION','JUNIOR ','MARKET','STANDARD','FINANCIAL','PRODUCT','DEVELOPER','SENIOR ',' II'],regex=True)\n\n# unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['BUSINESS INTELLIGENCE','INFORMATION TECHNOLOGY','QUALITY ASSURANCE','USER EXPERIENCE','USER INTERFACE','DATA WAREHOUSE','DATA ANALYST','DATA BASE','DATA QUALITY','DATA GOVERNANCE','BUSINESS ANALYST','DATA MANAGEMENT','REPORTING ANALYST','BUSINESS DATA','SYSTEM ANALYST','DATA REPORTING','QUALITY ANALYST','DATA ENGINEER','BIG DATA','SOFTWARE ENGINEER','MACHINE LEARNING','FULL STACK','DATA SCIENTIST','DATA SCIENCE','DATA CENTER','ENTRY LEVEL','NEURAL NETWORK','SYSTEM ENGINEER',' ML '],\n    ['BI','IT','QA','UX','UI','DATA_WAREHOUSE','DATA_ANALYST','DATABASE','DATA_QUALITY','DATA_GOVERNANCE','BUSINESS_ANALYST','DATA_MANAGEMENT','REPORTING_ANALYST','BUSINESS_DATA','SYSTEM_ANALYST','DATA_REPORTING','QUALITY_ANALYST','DATA_ENGINEER','BIG_DATA','SOFTWARE_ENGINEER','MACHINE_LEARNING','FULL_STACK','DATA_SCIENTIST','DATA_SCIENCE','DATA_CENTER','ENTRY_LEVEL','NEURAL_NETWORK','SYSTEM_ENGINEER',' MACHINE_LEARNING '],regex=True)\n\n# unify some word use\ntext_Analysis['Job_title_2']= text_Analysis['Job_title_2'].str.upper().replace(\n    ['DATA_ENGINEER JUNIOR','DATA_ENGINEER SENIOR','DATA  REPORTING_ANALYST','DATA ','BIG_DATA '],\n    ['JUNIOR DATA_ENGINEER','SENIOR DATA_ENGINEER','DATA_REPORTING_ANALYST','DATA_','BIG_DATA_'],regex=True)\n\n# get top keywords\ns = text_Analysis['Job_title_2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS = s[s['Count']>ssize]\n\n# get keywords from each row\ntext_Analysis['KW'] = text_Analysis['Job_title_2'].apply(lambda x: get_keyword(x,'',S['KW']))\n\n# create dummy columns by keywords\nkwdummy = pd.get_dummies(text_Analysis['KW'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True).replace(np.nan,0)","cad7bff6":"# Create a list of big data buzzwords to see if those words in JD would influence the salary\nbuzzwords = ['COMPUTER_SCIENCE','MASTER','MBA','SQL','PYTHON','R','PHD','BUSINESS_ANALYTICS','SAS','PMP','SCRUM_MASTER','STATISTICS','MATHEMATICS','MACHINE_LEARNING','ARTIFICIAL_INTELLIGENCE','ECONOMICS','TABEAU','AWS','AZURE','POWER_BI','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','REGRESSION','CLUSTER','ORACLE','EXCEL','TENSORFLOW','HADOOP','SPARK','NOSQL','SAP','ETL','API','PLSQL','MONGODB','POSTGRESQL','ELASTICSEARCH','REDIS','MYSQL','FIREBASE','SQLITE','CASSANDRA','DYNAMODB','OLTP','OLAP','DEVOPS','PLATFORM','NETWORK','APACHE','SECURITY','MARKDOWN']","a112aa29":"# remove special characters and unify some word use\ntext_Analysis['Job_Desc2'] = text_Analysis['Job Description'].replace('[^A-Za-z0-9]+', ' ',regex=True)\ntext_Analysis['Job_Desc2'] = text_Analysis['Job_Desc2'].str.upper().replace(\n    ['COMPUTER SCIENCE','ENGINEERING DEGREE',' MS ','BUSINESS ANALYTICS','SCRUM MASTER','MACHINE LEARNING',' ML ','POWER BI','ARTIFICIAL INTELLIGENCE',' AI ','ALGORITHMS','DEEP LEARNING','NEURAL NETWORK','NATURAL LANGUAGE PROCESSING','DECISION TREE','CLUSTERING','PL SQL'],\n    ['COMPUTER_SCIENCE','ENGINEERING_DEGREE',' MASTER ','BUSINESS_ANALYTICS','SCRUM_MASTER','MACHINE_LEARNING',' MACHINE_LEARNING ','POWER_BI','ARTIFICIAL_INTELLIGENCE',' ARTIFICIAL_INTELLIGENCE ','ALGORITHM','DEEP_LEARNING','NEURAL_NETWORK','NATURAL_LANGUAGE_PROCESSING','DECISION_TREE','CLUSTER','PLSQL'],regex=True)\n\n# Count the JD keywords.\nS2 = text_Analysis['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nS2 = S2[S2['KW'].isin(buzzwords)].reset_index(drop=True)\nS2['KWJD'] = S2['KW'] + '_JD'\nS2 = S2[S2['Count']>ssize]\n\n# get keywords from each row\ntext_Analysis['JDKW'] = text_Analysis['Job_Desc2'].apply(lambda x: get_keyword(x,'_JD',S2['KWJD']))\n\n# create dummy columns by keywords\nkwdummy = pd.get_dummies(text_Analysis['JDKW'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)","6cfcb8fa":"# let's see if number of buzzwords contained or how wordy the JD is would have impact.\ntext_Analysis['JDKWlen']=text_Analysis['JDKW'].str.len()\ntext_Analysis['JDlen']=text_Analysis['Job Description'].str.len()","6d137933":"## State ##\n# Count the states frequency for t-test later\nS3 = text_Analysis['State'].value_counts().reset_index().rename(\n    columns={'index':'State','State':'Count'})\nS3_Top = S3[S3['Count']>ssize]\n\n# create dummy columns by State\nkwdummy = pd.get_dummies(text_Analysis['State'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)\n\n## City ##\ntext_Analysis['City']=text_Analysis['City'].str.replace(' ','_',regex=True)\n\n# Count the city frequency for t-test later\nS35 = text_Analysis['City'].value_counts().reset_index().rename(\n    columns={'index':'City','City':'Count'})\nS35_Top = S35[S35['Count']>ssize]\n\n# create dummy columns by City\nkwdummy = pd.get_dummies(text_Analysis[text_Analysis['City'].isin(np.asarray(S35_Top['City']))]['City'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)\n\n## State where companies Headquarters locate at  ##\n# Count the HQState frequency for t-test later\nS31 = text_Analysis['HQState'].value_counts().reset_index().rename(\n    columns={'index':'HQState','HQState':'Count'}).replace(0,'Unknown_State')\nS31['HQState_HQ'] = [s + '_HQ' for s in S31['HQState']]\nS31_Top = S31[S31['Count']>ssize]\n\n# create dummy columns by HQ State\nkwdummy = pd.get_dummies(S31_Top['HQState_HQ'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\nS31_Top2 = S31_Top.merge(kwdummy,left_index=True,right_index=True,how='left').drop(['Count'],axis=1)\ntext_Analysis = text_Analysis.merge(S31_Top2,on='HQState',how='left').replace(np.nan,0)","557df9a6":"# create dummy columns by Revenue\nkwdummy = pd.get_dummies(text_Analysis['Revenue_USD'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)\n# Count the Revenue frequency for t-test later\nS4 = text_Analysis['Revenue_USD'].value_counts().reset_index().rename(\n    columns={'index':'Revenue_USD','Revenue_USD':'Count'})\nS4_Top = S4[(S4['Count']>ssize) & (S4['Revenue_USD']!=0)]\n\n# create dummy columns by Size\nkwdummy = pd.get_dummies(text_Analysis['Size'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)\n# Count the Size frequency for t-test later\nS5 = text_Analysis['Size'].value_counts().reset_index().rename(\n    columns={'index':'Size','Size':'Count'})\nS5_Top = S5[(S5['Count']>ssize) & (S5['Size']!=0)]\n\n# create dummy columns by Sector\nkwdummy = pd.get_dummies(text_Analysis['Sector'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)\n# Count the Sector frequency for t-test later\nS6 = text_Analysis['Sector'].value_counts().reset_index().rename(\n    columns={'index':'Sector','Sector':'Count'})\nS6 = S6[S6['Sector']!=0]\nS6_Top = S6[(S6['Count']>ssize)]\n\n# create dummy columns by Type of Ownership\nkwdummy = pd.get_dummies(text_Analysis['Type of ownership'].apply(pd.Series).stack()).sum(level=0)\nkwdummy = kwdummy.apply(lambda x: [y if y <= 1 else 1 for y in x])\n# merge back the dummy columns to the main dataset\ntext_Analysis = text_Analysis.merge(kwdummy,left_index=True,right_index=True,how='left').replace(np.nan,0)\n# Count the Type of ownership frequency for t-test later\nS8 = text_Analysis['Type of ownership'].value_counts().reset_index().rename(\n    columns={'index':'Type_of_ownership','Type of ownership':'Count'})\nS8_Top = S8[(S8['Count']>ssize) & (S8['Type_of_ownership']!=0)]","c287b139":"def splittingSets(df, y):\n    X = df.drop([y],axis=1)\n    y = df[y]\n\n    X_int, X_test, y_int, y_test = train_test_split(X,y,shuffle=False,test_size=0.2,random_state=15)\n    X_train, X_val, y_train, y_val = train_test_split(X_int,y_int,shuffle=False,test_size=0.25,random_state=15)\n\n    return X_train, X_val, X_test, y_train, y_val, y_test","a0893222":"def ttest(setX):\n    text_columns = list(X_train.columns)\n    ttests=[]\n    for word in text_columns:\n        if word in set(setX):\n            ttest = stats.ttest_ind(y_train[X_train[word]==1],\n                                    y_train[X_train[word]==0])\n            ttests.append([word,ttest])\n    ttests = pd.DataFrame(ttests,columns=['Var','R'])\n    ttests['R']=ttests['R'].astype(str).replace(['Ttest_indResult\\(statistic=','pvalue=','\\)'],['','',''],regex=True)\n    ttests['Statistic'],ttests['P-value']=ttests['R'].str.split(', ',1).str\n    ttests=ttests.drop(['R'],axis=1).sort_values('P-value',ascending=True)\n    return ttests","3e4964be":"def VarFromTtests(ttests_var,ttest_p):\n    ttest_var_pass = ttests_var[ttests_var['P-value'].astype(float)<ttest_p].reset_index(drop=True)\n    ttest_pass = list(ttest_var_pass['Var'])\n    \n    #build table for variables from ttest results\n    ttests_var_pass_KW = list(ttest_pass)\n    ttests_var_pass_KW.insert(0,'Const')\n    ttests_var_pass_KW = pd.DataFrame(ttests_var_pass_KW,columns=['Var'])\n\n    #list of variables to be put in the model\n    model_var = list(ttest_pass)\n    model_var.insert(0,'Const')\n\n    #list of variables removed\n    var_remove = []\n    \n    return ttests_var_pass_KW, model_var, var_remove, ttest_pass","75ea2d59":"def buildmodel(X_opt, var_name):\n    Mod = sm.OLS(endog = y_train, exog = X_opt).fit() \n    results_summary = Mod.summary(xname=var_name)\n    vif = pd.DataFrame()\n    vif[\"VIF\"] = [variance_inflation_factor(\n        sm.OLS(endog = y_train, exog = X_opt).exog, i) for i in range(\n        1, sm.OLS(endog = y_train, exog = X_opt).exog.shape[1])]\n    vif = Insert_row_(0, vif, np.nan)\n    return results_summary, vif, Mod","f0b4df0c":"def resulttable(results_summary):\n    results_as_html = results_summary.tables[1].as_html()\n    results_as_html = pd.read_html(results_as_html, header=0, index_col=0)[0]\n    result_data = pd.DataFrame(results_as_html)\n    return result_data","d47be187":"def resulttables(results_summary, vif):\n    results_as_html0 = results_summary.tables[0].as_html()\n    results_as_html1 = results_summary.tables[1].as_html()\n    results_as_html2 = results_summary.tables[2].as_html()\n    results_as_html0 = pd.read_html(results_as_html0, header=0, index_col=0)[0]\n    results_as_html1 = pd.read_html(results_as_html1, header=0, index_col=0)[0]\n    results_as_html2 = pd.read_html(results_as_html2, header=0, index_col=0)[0]\n    result_data0 = pd.DataFrame(results_as_html0)\n    result_data1 = pd.DataFrame(results_as_html1).reset_index()\n    result_data1 = result_data1.merge(vif, right_index=True, left_index=True)\n    result_data2 = pd.DataFrame(results_as_html2)\n    return result_data0, result_data1, result_data2","54a62ab6":"# Create Backward Stepwise functions\ndef backwardeliminate(model_var, var_remove, ttest_pass_KW, ttest_pass):\n    for ele in model_var:\n        if ele in var_remove:\n            model_var.remove(ele)\n    ttest_pass_KW = ttest_pass_KW[ttest_pass_KW['Var'].isin(model_var)]\n    var_idx = ttest_pass_KW['Var'].index.tolist()\n    X_train0 = X_train[ttest_pass]\n    X_train1 = np.append(arr = np.ones((len(X_train0), 1)).astype(int),\n                         values = X_train0, axis = 1)\n    X_opt = X_train1[:,var_idx]\n    var_name = list(ttest_pass_KW['Var'])\n    return X_opt, var_name","1713f4f6":"def scalibility(FINAL_MODEL_NAME, Mod_Final_1, Mod_Final_2):\n    global modelperform\n    X_train0 = X_train[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    X_train1 = np.append(arr = np.ones((len(X_train0), 1)).astype(int),values = X_train0, axis = 1)\n\n    X_val0 = X_val[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    X_val1 = np.append(arr = np.ones((len(X_val0), 1)).astype(int),values = X_val0, axis = 1)\n\n    X_test0 = X_test[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    X_test1 = np.append(arr = np.ones((len(X_test0), 1)).astype(int),values = X_test0, axis = 1)\n        \n    new_row = {'Model': FINAL_MODEL_NAME,'Adj. R-squared': Mod_Final_1.iloc[0][2],\n               'AIC': Mod_Final_1.iloc[4][2],'BIC': Mod_Final_1.iloc[5][2],\n               'R2':r2_score(np.asarray(y_train),sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_train1)),\n               'RMSE':sqrt(mean_squared_error(np.asarray(y_train),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_train1))),\n               'R2_val':r2_score(np.asarray(y_val),sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_val1)),\n               'RMSE_val':sqrt(mean_squared_error(np.asarray(y_val),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_val1))),\n               'R2_test':r2_score(np.asarray(y_test),sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_test1)),\n               'RMSE_test':sqrt(mean_squared_error(np.asarray(y_test),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_test1)))}\n\n    pd.options.display.float_format = '{:.4f}'.format\n    modelperform = modelperform.append(new_row, ignore_index=True)\n    \n    return modelperform, X_train1, X_val1, X_test1","c34b5666":"def multicol(Coef_Final):\n    multivlsit = Coef_Final.index.tolist()\n    multivlsit.remove('Const')\n    corr_df = X_train[multivlsit].corr(method='pearson')\n    mask = np.zeros_like(corr_df)\n    mask[np.triu_indices_from(mask)] = True\n    \n    sns.set(style='white')\n    fig = plt.figure(figsize=(13, 13))\n    sns.heatmap(corr_df, cmap='RdYlGn_r', vmax = 1.0, vmin = -1.0, mask = mask, linewidths = 2.5)\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90)\n    plt.show()","c8f729af":"def addTopInteractions():\n    global X_train, X_val, X_test, Interactions_Top\n    X_train0 = X_train[Mod_Final_2[Mod_Final_2['index']!='Const']['index']]\n    baseline = np.mean(cross_val_score(linear_model.LinearRegression(), X_train0, y_train, scoring='r2', cv=3, n_jobs=1))\n    interactions = list()\n    for feature_A in X_train0.columns:\n        for feature_B in X_train0.columns:\n            if feature_A > feature_B:\n                X_train0['interaction'] = X_train0[feature_A] * X_train0[feature_B]\n                score = np.mean(cross_val_score(linear_model.LinearRegression(), X_train0, y_train, scoring='r2',\n                                       cv=3, n_jobs=1))\n                if score > baseline:\n                    interactions.append((feature_A, feature_B, round(score,3)))\n\n    Interactions = pd.DataFrame(interactions)\n    Interactions = Interactions.sort_values(2,ascending=False).reset_index(drop=True)\n    Interactions_Top = Interactions.iloc[0:9,:]\n    Interactions_Top['inter'] = Interactions_Top[0]+'*'+Interactions_Top[1]\n\n    for i in Interactions_Top.index:\n        X_train[Interactions_Top.iloc[i]['inter']] = X_train[\n            Interactions_Top.iloc[i][0]]*X_train[Interactions_Top.iloc[i][1]]\n        X_val[Interactions_Top.iloc[i]['inter']] = X_val[\n            Interactions_Top.iloc[i][0]]*X_val[Interactions_Top.iloc[i][1]]\n        X_test[Interactions_Top.iloc[i]['inter']] = X_test[\n            Interactions_Top.iloc[i][0]]*X_test[Interactions_Top.iloc[i][1]]\n        if sum(X_train[Interactions_Top.iloc[i]['inter']]) < ssize:\n            X_train = X_train.iloc[:,0:-1]\n\n    X_val = X_val[X_train.columns]\n    X_test = X_test[X_test.columns]\n    return X_train, X_val, X_test, Interactions_Top","318ec2cb":"def ElasticNetPlot(alpha_max, alpha_step, L1_wt_value):\n    ols_rmse_val=sqrt(mean_squared_error(np.asarray(y_val),\n                                       sm.OLS(endog = y_train, exog = X_train1).fit().predict(X_val1)))\n\n    EN_rmse_val=[]\n    for alpha in np.arange(0, alpha_max, alpha_step):\n        # L1_wt: (0)Ridge - (1)Lasso\n        EN_rmse_val.append(sqrt(mean_squared_error(np.asarray(y_val),sm.OLS(\n            endog = y_train, exog = X_train1).fit_regularized(method='elastic_net', alpha=alpha, L1_wt = L1_wt_value).predict(X_val1))))\n\n    # plot rmse\n    fig, ax = plt.subplots(figsize=(13, 7))\n    plt.plot(np.arange(0, alpha_max, alpha_step), EN_rmse_val, 'ro')\n    plt.axhline(y=ols_rmse_val, color='g', linestyle='--')\n    plt.title(\"Elatic Net Validation RMSE\", fontsize=16)\n    plt.xlabel(\"Model Simplicity$\\longrightarrow$\")\n    plt.ylabel(\"RMSE\")\n    plt.show()","4202d585":"def coefficientbars(alpha_value, L1_wt_value, Mod_Final_2):\n    mod_min_val_err = sm.OLS(endog = y_train, exog = X_train1).fit_regularized(method='elastic_net', alpha=alpha_value, L1_wt = L1_wt_value)\n    regularized_regression_parameters = mod_min_val_err.params.reset_index()\n\n    sns.set(style='white')\n    reg_fit = Mod_Final_2['index'].reset_index()\n    reg_fit = reg_fit.merge(regularized_regression_parameters, right_index=True, left_index=True).drop(\n        ['level_0','index_y'],axis=1).rename(columns={'index_x':'Var',0:'Coefficient'}).sort_values(\n        'Coefficient',ascending=False).reset_index(drop=True)\n    reg_fit = reg_fit[reg_fit['Var']!='Const']\n\n    fig = plt.figure(figsize=(13, 5))\n    sns.barplot(x='Var',y='Coefficient',data=reg_fit).set(xlabel=\"\",ylabel=\"Salary Performance \\n Against Average\")\n\n    plt.xticks(rotation=45,horizontalalignment='right')\n    plt.show()","ebc8c3df":"ssize = 30 # set minimum sample size for each variable\nttest_p = 0.1 # set p-value threshold for t-test\nregr_p = 0.05 # set p-value threshold for eliminating variables from regression\n\nX_train, X_val, X_test, y_train, y_val, y_test = splittingSets(text_Analysis, 'Est_Salary')\nfinalVar=['Years_Founded','Rating','JDKWlen','JDlen','Easy_Apply']\n\n# Job Titles\nttests_title = ttest(s[(s['Count']>=ssize)&(s['KW']!=0)]['KW'])\nttests_title_pass_KW, model_title, title_remove, ttests_title_pass = VarFromTtests(ttests_title, ttest_p)\nX_opt_title, title_name = backwardeliminate(model_title, title_remove, ttests_title_pass_KW, ttests_title_pass)\nMod_title, VIF_title, mod_title = buildmodel(X_opt_title, title_name)\nCoef_title = resulttable(Mod_title)\n\ni = max(Coef_title['P>|t|'])\nwhile i > regr_p:\n    if Coef_title[Coef_title['P>|t|']==i].index.values[0] != 'Const':\n        title_remove.append(Coef_title[Coef_title['P>|t|']==i].index.values[0])\n        X_opt_title, title_name = backwardeliminate(model_title, title_remove, ttests_title_pass_KW, ttests_title_pass)\n        Mod_title, VIF_title, mod_title = buildmodel(X_opt_title, title_name)\n        Coef_title = resulttable(Mod_title)\n        i = max(Coef_title['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_title['coef'].index.tolist()\nMod_title_1,Mod_title_2,Mod_title_3 = resulttables(Mod_title, VIF_title)\n\n# JD\nttests_JD = ttest(S2[(S2['Count']>=ssize)&(S2['KW']!=0)]['KWJD']) \nttests_JD_pass_KW, model_JD, JD_remove, ttests_JD_pass = VarFromTtests(ttests_JD, ttest_p)\nX_opt_JD, JD_name = backwardeliminate(model_JD, JD_remove, ttests_JD_pass_KW, ttests_JD_pass)\nMod_JD, VIF_JD, mod_JD = buildmodel(X_opt_JD, JD_name)\nCoef_JD = resulttable(Mod_JD)\n\ni = max(Coef_JD['P>|t|'])\nwhile i > regr_p:\n    if Coef_JD[Coef_JD['P>|t|']==i].index.values[0] != 'Const':\n        JD_remove.append(Coef_JD[Coef_JD['P>|t|']==i].index.values[0])\n        X_opt_JD, JD_name = backwardeliminate(model_JD, JD_remove, ttests_JD_pass_KW, ttests_JD_pass)\n        Mod_JD, VIF_JD, mod_JD = buildmodel(X_opt_JD, JD_name)\n        Coef_JD = resulttable(Mod_JD)\n        i = max(Coef_JD['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_JD['coef'].index.tolist()\nMod_JD_1,Mod_JD_2,Mod_JD_3 = resulttables(Mod_JD, VIF_JD)\n\n# state\nttests_State = ttest(S3[(S3['Count']>=ssize)&(S3['State']!=0)]['State'])\nttests_State_pass_KW, model_State, State_remove, ttests_State_pass = VarFromTtests(ttests_State, ttest_p)\nX_opt_State, State_name = backwardeliminate(model_State, State_remove, ttests_State_pass_KW, ttests_State_pass)\nMod_State, VIF_State, mod_State = buildmodel(X_opt_State, State_name)\nCoef_State = resulttable(Mod_State)\n\ni = max(Coef_State['P>|t|'])\nwhile i > regr_p:\n    if Coef_State[Coef_State['P>|t|']==i].index.values[0] != 'Const':\n        State_remove.append(Coef_State[Coef_State['P>|t|']==i].index.values[0])\n        X_opt_State, State_name = backwardeliminate(model_State, State_remove, ttests_State_pass_KW, ttests_State_pass)\n        Mod_State, VIF_State, mod_State = buildmodel(X_opt_State, State_name)\n        Coef_State = resulttable(Mod_State)\n        i = max(Coef_State['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_State['coef'].index.tolist()\nMod_State_1,Mod_State_2,Mod_State_3 = resulttables(Mod_State, VIF_State)\n\n# city\nttests_city = ttest(S35[(S35['Count']>=ssize)&(S35['City']!=0)]['City'])\nttests_city_pass_KW, model_city, city_remove, ttests_city_pass = VarFromTtests(ttests_city, ttest_p)\nX_opt_city, city_name = backwardeliminate(model_city, city_remove, ttests_city_pass_KW, ttests_city_pass)\nMod_city, VIF_city, mod_city = buildmodel(X_opt_city, city_name)\nCoef_city = resulttable(Mod_city)\n\ni = max(Coef_city['P>|t|'])\nwhile i > regr_p:\n    if Coef_city[Coef_city['P>|t|']==i].index.values[0] != 'Const':\n        city_remove.append(Coef_city[Coef_city['P>|t|']==i].index.values[0])\n        X_opt_city, city_name = backwardeliminate(model_city, city_remove, ttests_city_pass_KW, ttests_city_pass)\n        Mod_city, VIF_city, mod_city = buildmodel(X_opt_city, city_name)\n        Coef_city = resulttable(Mod_city)\n        i = max(Coef_city['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_city['coef'].index.tolist()\nMod_city_1,Mod_city_2,Mod_city_3 = resulttables(Mod_city, VIF_city)\n\n# HQ state\nttests_HQState = ttest(S31[(S31['Count']>=ssize)&(S31['HQState_HQ']!=0)]['HQState_HQ'])\nttests_HQState_pass_KW, model_HQState, HQState_remove, ttests_HQState_pass = VarFromTtests(ttests_HQState, ttest_p)\nX_opt_HQState, HQState_name = backwardeliminate(model_HQState, HQState_remove, ttests_HQState_pass_KW, ttests_HQState_pass)\nMod_HQState, VIF_HQState, mod_HQState = buildmodel(X_opt_HQState, HQState_name)\nCoef_HQState = resulttable(Mod_HQState)\n\ni = max(Coef_HQState['P>|t|'])\nwhile i > regr_p:\n    if Coef_HQState[Coef_HQState['P>|t|']==i].index.values[0] != 'Const':\n        HQState_remove.append(Coef_HQState[Coef_HQState['P>|t|']==i].index.values[0])\n        X_opt_HQState, HQState_name = backwardeliminate(model_HQState, HQState_remove, ttests_HQState_pass_KW, ttests_HQState_pass)\n        Mod_HQState, VIF_HQState, mod_HQState = buildmodel(X_opt_HQState, HQState_name)\n        Coef_HQState = resulttable(Mod_HQState)\n        i = max(Coef_HQState['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_HQState['coef'].index.tolist()\nMod_HQState_1,Mod_HQState_2,Mod_HQState_3 = resulttables(Mod_HQState, VIF_HQState)\n\n# Revenue\nttests_Revenue = ttest(S4[(S4['Count']>=ssize)&(S4['Revenue_USD']!=0)]['Revenue_USD'])\nttests_Revenue_pass_KW, model_Revenue, Revenue_remove, ttests_Revenue_pass = VarFromTtests(ttests_Revenue, ttest_p)\nX_opt_Revenue, Revenue_name = backwardeliminate(model_Revenue, Revenue_remove, ttests_Revenue_pass_KW, ttests_Revenue_pass)\nMod_Revenue, VIF_Revenue, mod_Revenue = buildmodel(X_opt_Revenue, Revenue_name)\nCoef_Revenue = resulttable(Mod_Revenue)\n\ni = max(Coef_Revenue['P>|t|'])\nwhile i > regr_p:\n    if Coef_Revenue[Coef_Revenue['P>|t|']==i].index.values[0] != 'Const':\n        Revenue_remove.append(Coef_Revenue[Coef_Revenue['P>|t|']==i].index.values[0])\n        X_opt_Revenue, Revenue_name = backwardeliminate(model_Revenue, Revenue_remove, ttests_Revenue_pass_KW, ttests_Revenue_pass)\n        Mod_Revenue, VIF_Revenue, mod_Revenue = buildmodel(X_opt_Revenue, Revenue_name)\n        Coef_Revenue = resulttable(Mod_Revenue)\n        i = max(Coef_Revenue['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Revenue['coef'].index.tolist()\nMod_Revenue_1,Mod_Revenue_2,Mod_Revenue_3 = resulttables(Mod_Revenue, VIF_Revenue)\n\n# Size\nttests_Size = ttest(S5[(S5['Count']>=ssize)&(S5['Size']!=0)]['Size'])\nttests_Size_pass_KW, model_Size, Size_remove, ttests_Size_pass = VarFromTtests(ttests_Size, ttest_p)\nX_opt_Size, Size_name = backwardeliminate(model_Size, Size_remove, ttests_Size_pass_KW, ttests_Size_pass)\nMod_Size, VIF_Size, mod_Size = buildmodel(X_opt_Size, Size_name)\nCoef_Size = resulttable(Mod_Size)\n\ni = max(Coef_Size['P>|t|'])\nwhile i > regr_p:\n    if Coef_Size[Coef_Size['P>|t|']==i].index.values[0] != 'Const':\n        Size_remove.append(Coef_Size[Coef_Size['P>|t|']==i].index.values[0])\n        X_opt_Size, Size_name = backwardeliminate(model_Size, Size_remove, ttests_Size_pass_KW, ttests_Size_pass)\n        Mod_Size, VIF_Size, mod_Size = buildmodel(X_opt_Size, Size_name)\n        Coef_Size = resulttable(Mod_Size)\n        i = max(Coef_Size['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Size['coef'].index.tolist()\nMod_Size_1,Mod_Size_2,Mod_Size_3 = resulttables(Mod_Size, VIF_Size)\n\n# Sector\nttests_Sector = ttest(S6[(S6['Count']>=ssize)&(S6['Sector']!=0)]['Sector'])\nttests_Sector_pass_KW, model_Sector, Sector_remove, ttests_Sector_pass = VarFromTtests(ttests_Sector, ttest_p)\nX_opt_Sector, Sector_name = backwardeliminate(model_Sector, Sector_remove, ttests_Sector_pass_KW, ttests_Sector_pass)\nMod_Sector, VIF_Sector, mod_Sector = buildmodel(X_opt_Sector, Sector_name)\nCoef_Sector = resulttable(Mod_Sector)\n\ni = max(Coef_Sector['P>|t|'])\nwhile i > regr_p:\n    if Coef_Sector[Coef_Sector['P>|t|']==i].index.values[0] != 'Const':\n        Sector_remove.append(Coef_Sector[Coef_Sector['P>|t|']==i].index.values[0])\n        X_opt_Sector, Sector_name = backwardeliminate(model_Sector, Sector_remove, ttests_Sector_pass_KW, ttests_Sector_pass)\n        Mod_Sector, VIF_Sector, mod_Sector = buildmodel(X_opt_Sector, Sector_name)\n        Coef_Sector = resulttable(Mod_Sector)\n        i = max(Coef_Sector['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Sector['coef'].index.tolist()\nMod_Sector_1,Mod_Sector_2,Mod_Sector_3 = resulttables(Mod_Sector, VIF_Sector)\n\n# Type of Ownership\nttests_Own = ttest(S8[(S8['Count']>=ssize)&(S8['Type_of_ownership']!=0)]['Type_of_ownership'])\nttests_Own_pass_KW, model_Own, Own_remove, ttests_Own_pass = VarFromTtests(ttests_Own, ttest_p)\nX_opt_Own, Own_name = backwardeliminate(model_Own, Own_remove, ttests_Own_pass_KW, ttests_Own_pass)\nMod_Own, VIF_Own, mod_Own = buildmodel(X_opt_Own, Own_name)\nCoef_Own = resulttable(Mod_Own)\n\ni = max(Coef_Own['P>|t|'])\nwhile i > regr_p:\n    if Coef_Own[Coef_Own['P>|t|']==i].index.values[0] != 'Const':\n        Own_remove.append(Coef_Own[Coef_Own['P>|t|']==i].index.values[0])\n        X_opt_Own, Own_name = backwardeliminate(model_Own, Own_remove, ttests_Own_pass_KW, ttests_Own_pass)\n        Mod_Own, VIF_Own, mod_Own = buildmodel(X_opt_Own, Own_name)\n        Coef_Own = resulttable(Mod_Own)\n        i = max(Coef_Own['P>|t|'])\n    else:\n        continue\n\nfinalVar += Coef_Own['coef'].index.tolist()\nMod_Own_1,Mod_Own_2,Mod_Own_3 = resulttables(Mod_Own, VIF_Own)\n\n# Final Combined Model\nwhile 'Const' in finalVar:\n    finalVar.remove('Const')\n\n#reset the table for variables\nttest_Final_pass_KW = list(finalVar)\nttest_Final_pass_KW.insert(0,'Const')\nttest_Final_pass_KW = pd.DataFrame(ttest_Final_pass_KW,columns=['Var'])\n\n#reset list of variables to be put in the model\nmodel_Final = list(finalVar)\nmodel_Final.insert(0,'Const')\n\nFinal_remove = ['Jacksonville','NY'] # default removed due to Multicollinearity\n\nX_opt_Final, Final_name = backwardeliminate(model_Final, Final_remove, ttest_Final_pass_KW, finalVar)\nMod_Final, VIF_Final, mod_Final = buildmodel(X_opt_Final, Final_name)\nCoef_Final = resulttable(Mod_Final)\n\ni = max(Coef_Final['P>|t|'])\nwhile i > regr_p:\n    if Coef_Final[Coef_Final['P>|t|']==i].index.values[0] != 'Const':\n        Final_remove.append(Coef_Final[Coef_Final['P>|t|']==i].index.values[0])\n        X_opt_Final, Final_name = backwardeliminate(model_Final, Final_remove, ttest_Final_pass_KW, finalVar)\n        Mod_Final, VIF_Final, mod_Final = buildmodel(X_opt_Final, Final_name)\n        Coef_Final = resulttable(Mod_Final)\n        i = max(Coef_Final['P>|t|'])\n    else:\n        continue\n\nMod_Final_1,Mod_Final_2,Mod_Final_3 = resulttables(Mod_Final, VIF_Final)","8b6678a0":"SA = s[s['KW'].str.contains('ANALYST')].reset_index(drop=True).drop([0,2,4,8,10,11,15])\nSBA = s[s['KW'].str.contains('BUSINESS_ANALYST')].reset_index(drop=True)\nSDA = s[s['KW'].str.contains('DATA_ANALYST')].reset_index(drop=True)\nSDE1 = s[s['KW'].str.contains('DATA_ENGINEER')].reset_index(drop=True).drop([2,4])\nSDS = s[s['KW'].str.contains('DATA_SCIENTIST')].reset_index(drop=True)\nSSE = s[s['KW'].str.contains('SOFTWARE_ENGINEER')].reset_index(drop=True).drop([1])\n\nSDE = []\nSDE = pd.concat([SDE1,SSE])\n\nSA['Group']='Other Analysts'\nSBA['Group']='Business Analysts'\nSDA['Group']='Data Analysts'\nSDE['Group']='Data Engineers'\nSDS['Group']='Data Scientists'\n\nAll_DS_Jobs=[]\nAll_DS_Jobs=pd.concat([SA,SBA,SDA,SDE,SDS]).sort_values('Count',ascending=False).reset_index(drop=True)\nAll_DS_Jobs_sum=All_DS_Jobs.groupby('Group')['Count'].sum().reset_index().sort_values(\n    'Count',ascending=False).reset_index(drop=True).rename(columns={'Count':'Hires','Group':'Positions'})\nAll_DS_Jobs_sum=All_DS_Jobs_sum[['Positions','Hires']]\n\nfig = plt.figure(figsize=(7, 7))\nlabels = All_DS_Jobs_sum['Positions']\nplt.pie(All_DS_Jobs_sum['Hires'], shadow=True, startangle=90, autopct='%1.1f%%', labels=labels)\nplt.title('Data Science Job Composition\\n(% in number of hires)',color='darkblue',fontsize=16)\nplt.show()","16b21869":"position_sal = pd.DataFrame(text_Analysis['Job_title_2'].str.split(expand=True).stack()).reset_index()\nposition_sal = position_sal.merge(text_Analysis[['Est_Salary','MACHINE_LEARNING','Company Name','Size','Revenue_USD','Job_Desc2','State']], left_on=['level_0'], right_on=text_Analysis.index, how='left')\nposition_sal = position_sal.merge(All_DS_Jobs, left_on=[0], right_on=['KW'], how='left')\nposition_sal = position_sal[position_sal.KW.notnull()]\n\ndas = position_sal[position_sal['Group']=='Data Analysts']['Est_Salary']\ndes = position_sal[position_sal['Group']=='Data Engineers']['Est_Salary']\noas = position_sal[position_sal['Group']=='Other Analysts']['Est_Salary']\nbas = position_sal[position_sal['Group']=='Business Analysts']['Est_Salary']\ndss = position_sal[position_sal['Group']=='Data Scientists']['Est_Salary']\n\nass = pd.concat([das,oas,bas])\n\nsns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,7))\nmean=position_sal['Est_Salary'].mean()\nmedian=position_sal['Est_Salary'].median()\n\nbph = sns.boxplot(position_sal['Est_Salary'], ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\nass1 = sns.distplot(ass,ax=ax_hist, color=\"r\").set(xlabel=\"Est. Salary ($'000)\")\ndes1 = sns.distplot(des,ax=ax_hist, color=\"g\").set(xlabel=\"Est. Salary ($'000)\")\ndss1 = sns.distplot(dss,ax=ax_hist, color=\"y\").set(xlabel=\"Est. Salary ($'000)\")\n\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (All): $'+str(int(mean))+'K':mean,'Median (All): $'+str(int(median))+'K':median,\n            'Analysts (Business, Data and others)':ass,'Data Engineers':des,'Data Scientists':dss})\nplt.xlim(0,226)\nplt.xticks(np.arange(0,226,step=10))\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]) #Adjust the padding between and around subplots\nplt.suptitle('Data Science Job Salary Distribution', fontsize=20,color='darkblue')\nplt.show()","b0c59e13":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,7))\nmean=das.mean()\nmedian=das.median()\n\nbph = sns.boxplot(das, ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndas1 = sns.distplot(das,ax=ax_hist, color=\"r\").set(xlabel=\"Est. Salary ($'000)\")\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (Data Analysts): $'+str(int(mean))+'K':mean,\n            'Median (Data Analysts): $'+str(int(median))+'K':median,'Data Analysts':das})\nplt.xlim(0,226)\nplt.xticks(np.arange(0,226,step=10))\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]) #Adjust the padding between and around subplots\nplt.suptitle('Data Analyst Salary Distribution', fontsize=20,color='darkblue')\nplt.show()","0ab47b66":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,7))\nmean=bas.mean()\nmedian=bas.median()\n\nbph = sns.boxplot(bas, ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\nbas1 = sns.distplot(bas,ax=ax_hist, color=\"purple\").set(xlabel=\"Est. Salary ($'000)\")\n\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (Business Analysts): $'+str(int(mean))+'K':mean,\n            'Median (Business Analysts): $'+str(int(median))+'K':median,'Business Analysts':bas})\nplt.xlim(0,226)\nplt.xticks(np.arange(0,226,step=10))\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]) #Adjust the padding between and around subplots\nplt.suptitle('Busincess Analyst Salary Distribution', fontsize=20,color='darkblue')\nplt.show()","668a7f28":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,7))\nmean=des.mean()\nmedian=des.median()\n\nbph = sns.boxplot(des, ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndes1 = sns.distplot(des,ax=ax_hist, color=\"g\").set(xlabel=\"Est. Salary ($'000)\")\n\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (Data Engineers): $'+str(int(mean))+'K':mean,\n            'Median (Data Engineers): $'+str(int(median))+'K':median,'Data Engineers':des})\nplt.xlim(0,226)\nplt.xticks(np.arange(0,226,step=10))\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]) #Adjust the padding between and around subplots\nplt.suptitle('Data Engineer Salary Distribution', fontsize=20,color='darkblue')\nplt.show()","7330732f":"sns.set(style='white')\n\nf, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)},figsize=(13,7))\nmean=dss.mean()\nmedian=dss.median()\n\nbph = sns.boxplot(dss, ax=ax_box).set(xlabel=\"\")\nax_box.axvline(mean, color='k', linestyle='--')\nax_box.axvline(median, color='y', linestyle='-')\n\ndes1 = sns.distplot(dss,ax=ax_hist, color=\"y\").set(xlabel=\"Est. Salary ($'000)\")\n\nax_hist.axvline(mean, color='k', linestyle='--')\nax_hist.axvline(median, color='y', linestyle='-')\n\nplt.legend({'Mean (Data Scientists): $'+str(int(mean))+'K':mean,\n            'Median (Data Scientists): $'+str(int(median))+'K':median,'Data Scientists':dss})\nplt.xlim(0,226)\nplt.xticks(np.arange(0,226,step=10))\nplt.tight_layout(rect=[0, 0.03, 1, 0.95]) #Adjust the padding between and around subplots\nplt.suptitle('Data Scientist Salary Distribution', fontsize=20,color='darkblue')\nplt.show()","0ad4dc84":"ds_sal = position_sal[position_sal['Group']=='Data Scientists']\nhids_sal = position_sal[(position_sal['Est_Salary']>=90)&(position_sal['Group']=='Data Scientists')]\nlowds_sal = position_sal[(position_sal['Est_Salary']<90)&(position_sal['Group']=='Data Scientists')]\nda_sal = position_sal[position_sal['Group']=='Data Analysts']\nba_sal = position_sal[position_sal['Group']=='Business Analysts']\nde_sal = position_sal[position_sal['Group']=='Data Engineers']","f6f243d4":"lowds_loc = lowds_sal['State'].value_counts().reset_index().rename(columns={'index':'State','State':'Count'})\nlowds_loc['Percentage'] = (lowds_loc['Count']\/sum(lowds_loc['Count']))*100\nlowds_loc = lowds_loc.iloc[0:3,:]\n\nhids_loc = hids_sal['State'].value_counts().reset_index().rename(columns={'index':'State','State':'Count'})\nhids_loc['Percentage'] = (hids_loc['Count']\/sum(hids_loc['Count']))*100\nhids_loc = hids_loc.iloc[0:3,:]\n\nf, axs = plt.subplots(ncols=2, sharey=True, figsize=(13,6.5))\n\nsns.set(style=\"white\")\nsns.barplot(x = lowds_loc['State'],y = lowds_loc['Percentage'],ax=axs[0],\n            palette=['royalblue','green','brown']).set(\n    ylabel='Frequency (%)',title='Top 3 Job Locations of Underpaid Data Scientists (< $90K)')\nsns.barplot(x = hids_loc['State'],y = hids_loc['Percentage'],\n            palette=['orange','olive','royalblue'],\n            ax=axs[1]).set(ylabel='',title='Top 3 Job Locations of High-paid Data Scientists (>= $90K)')\n\nplt.tight_layout(w_pad=0)\nplt.show()","ed9c6afe":"errtable = Coef_title[1:].sort_values('coef',ascending=True)\n\nfig = plt.figure(figsize=(13, 7))\nplt.errorbar(y = errtable.index.tolist(), x = errtable['coef'], xerr = errtable['std err']*2, fmt='o')\nplt.axvline(x=0, color='r',linestyle='--')\nplt.title(\"Keywords on Job Titles vs. Salaries\\n(Coefficients with Errors [0.025 - 0.975])\",\n          fontsize='15',color='darkblue')\nplt.xlabel('Reference: USD 97.79K',color='r')\nplt.xticks(np.arange(-50, 50, step=10))\nplt.show()\n\nprint(\"Insignificant Keywords on job titles (removed): \"+ str(title_remove))","f7e31e5b":"MLs = position_sal.pivot_table(columns='MACHINE_LEARNING',index='Group',values='Est_Salary',aggfunc=[np.mean,'count'])\nMLs = MLs.iloc[0:4,:]\nMLDADE = MLs.iloc[2:,0:2]\n\nf, axs = plt.subplots(ncols = 2, gridspec_kw= {\"width_ratios\":(1,0.5)},figsize=(13,7))\n\naxs[0].pie(MLs.iloc[:,3], shadow=True, startangle=0, autopct='%1.1f%%',\n           labels=MLs.index.tolist(),labeldistance=1.05)\naxs[0].set_title('Data Science Jobs with \"Machine Learning\" on Titles',color='darkblue')\n\nMLDADE.plot(ax=axs[1],kind='bar',color=['c','orange']).legend(\n    title='\"Machine Learning\" on Titles',labels=['No','Yes'])\naxs[1].set_title('Salary Variation with \"Machine Learning\" on Titles\\n(Unit: Thousand USD a Year)',\n                color='darkblue')\n\nplt.xlabel('')\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.show()","6ead314f":"titlebar = Coef_JD[1:].reset_index().replace('_JD','',regex=True)\ntitlebar = titlebar.sort_values('coef',ascending=True).reset_index(drop=True)\n\nsns.set(style='white')\nfig = plt.figure(figsize=(13, 7))\nsns.barplot(x=titlebar['index'], y=titlebar['coef'], yerr=titlebar['std err']*2,error_kw=dict(\n    lw=1, capsize=5, capthick=1))\nplt.ylabel('Est. Salary (USD\\'000) Impact')\nplt.xlabel('Keywords in Job Descriptions')\nplt.title('Correlation: Salaries vs Keywords in Job Description\\n(Coefficients with Errors [0.025 - 0.975])',\n          fontsize=18)\nplt.xticks(rotation=45,horizontalalignment='right')\nplt.show()\nprint(\"Insignificant Keywords in job descrptions (removed): \"+ str(JD_remove))","3cdae6e8":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(S2['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Hot Keywords in Data Science Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","afc5d2a2":"dskw = ds_sal['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\ndskw = dskw[dskw['KW'].isin(buzzwords)].reset_index(drop=True)\n\ndakw = da_sal['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\ndakw = dakw[dakw['KW'].isin(buzzwords)].reset_index(drop=True)\n\nbakw = ba_sal['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\nbakw = bakw[bakw['KW'].isin(buzzwords)].reset_index(drop=True)\n\ndekw = de_sal['Job_Desc2'].str.split(expand=True).stack().value_counts().reset_index().rename(\n    columns={'index':'KW',0:'Count'})\ndekw = dekw[dekw['KW'].isin(buzzwords)].reset_index(drop=True)","aea96549":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(dskw['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Hot Keywords in Data Scientist Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","cacd8a22":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(dakw['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Hot Keywords in Data Analyst Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","19962f18":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(bakw['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Hot Keywords in Business Analyst Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","26e75206":"wordCloud = WordCloud(width=450,height= 300).generate(' '.join(dekw['KW']))\nplt.figure(figsize=(19,9))\nplt.axis('off')\nplt.title(\"Hot Keywords in Data Engineer Job Descriptions\",fontsize=20)\nplt.imshow(wordCloud)\nplt.show()","bdbac584":"stateCount = data.groupby('State')[['Job Title']].count().reset_index().rename(columns={'Job Title':'Hires'}).sort_values(\n    'Hires', ascending=False).reset_index(drop=True).head(10)\nstateCount = stateCount.merge(data, on='State',how='left')\n\nsns.set(style=\"white\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='State',data=stateCount,ax=ax_bar)\nsns.pointplot(x='Est_Salary',y='State',data=stateCount, join=False,ax=ax_point).set(ylabel=\"\",xlabel=\"Salary ($'000)\")\nplt.suptitle('Top 10 States Hiring Data Science Jobs',fontsize=20,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","fad3ac1a":"df_by_city=data.groupby('Location')['Job Title'].count().reset_index().sort_values(\n    'Job Title',ascending=False).head(20).rename(columns={'Job Title':'Hires'})\nSal_by_city = df_by_city.merge(data,on='Location',how='left')\n\nsns.set(style=\"white\")\nf, (ax_bar, ax_point) = plt.subplots(ncols=2, sharey=True, gridspec_kw= {\"width_ratios\":(0.6,1)},figsize=(13,7))\nsns.barplot(x='Hires',y='Location',data=Sal_by_city,ax=ax_bar, palette='Set2').set(ylabel=\"\")\nsns.pointplot(x='Est_Salary',y='Location',data=Sal_by_city, join=False,ax=ax_point).set(\n    ylabel=\"\",xlabel=\"Salary ($'000)\")\nplt.suptitle('Top 20 Cities Hiring Data Science Jobs',fontsize=20,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","ff34c2af":"fig = plt.figure(figsize=(15, 7))\nsns.barplot(x='Rating', y='Est_Salary', data=data)\nplt.ylabel('Est. Salary (USD\\'000)')\nplt.xlabel('Company Rating (Glassdoor)')\nplt.title('Correlation: Company Ratings vs Salary',fontsize=18)\nprint('Positive correlation is obvious from rating 2.5 and higher...')\nplt.show()","bcdb8828":"Firm_Size = position_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size.drop([0])\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\nFirm_Size_Sal = position_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal.drop([0])\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\nf, axs = plt.subplots(ncols=2, sharey=True, figsize=(13,6.5))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0]).set(title=\"Number of Companies\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1]).set(title=\"Avg. Salaries\",ylabel='')\n\naxs[0].add_patch(Rectangle((6,10), 1, 2, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[0].add_patch(Rectangle((0,12), 5, 1, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((6,10), 1, 2, fill=False, edgecolor='blue', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((0,12), 5, 1, fill=False, edgecolor='blue', lw=3, clip_on=False))\n\nplt.setp([a.get_xticklabels() for a in axs[:]],rotation=45,ha='right')\nplt.suptitle('Company Scales, Hires and Salaries (Overall Data Science Jobs)',fontsize=18,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","b63fa142":"Firm_Size = ds_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size.drop([0])\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\nFirm_Size_Sal = ds_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal.drop([0])\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\nf, axs = plt.subplots(ncols=2, sharey=True, figsize=(13,6.5))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0]).set(title=\"Number of Companies\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1]).set(title=\"Avg. Salaries\",ylabel='')\n\naxs[0].add_patch(Rectangle((6,10), 1, 2, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[0].add_patch(Rectangle((0,12), 6, 1, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((6,10), 1, 2, fill=False, edgecolor='blue', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((0,12), 6, 1, fill=False, edgecolor='blue', lw=3, clip_on=False))\n\nplt.setp([a.get_xticklabels() for a in axs[:]],rotation=45,ha='right')\nplt.suptitle('Company Scales, Hires and Salaries (Data Scientists)',fontsize=18,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","46b6a345":"Firm_Size = da_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size.drop([0])\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\nFirm_Size_Sal = da_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal.drop([0])\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\nf, axs = plt.subplots(ncols=2, sharey=True, figsize=(13,6.5))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0]).set(title=\"Number of Companies\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1]).set(title=\"Avg. Salaries\",ylabel='')\n\naxs[0].add_patch(Rectangle((0,12), 3, 1, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((0,12), 3, 1, fill=False, edgecolor='blue', lw=3, clip_on=False))\n\nplt.setp([a.get_xticklabels() for a in axs[:]],rotation=45,ha='right')\nplt.suptitle('Company Scales, Hires and Salaries (Data Analysts)',fontsize=18,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","41be510d":"Firm_Size = ba_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size.drop([0])\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\nFirm_Size_Sal = ba_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal.drop([0])\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\nf, axs = plt.subplots(ncols=2, sharey=True, figsize=(13,6.5))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0]).set(title=\"Number of Companies\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1]).set(title=\"Avg. Salaries\",ylabel='')\n\naxs[0].add_patch(Rectangle((0,0), 1, 2, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[0].add_patch(Rectangle((1,8), 2, 1, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[0].add_patch(Rectangle((0,12), 2, 1, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((0,0), 1, 2, fill=False, edgecolor='blue', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((1,8), 2, 1, fill=False, edgecolor='blue', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((0,12), 2, 1, fill=False, edgecolor='blue', lw=3, clip_on=False))\n\nplt.setp([a.get_xticklabels() for a in axs[:]],rotation=45,ha='right')\nplt.suptitle('Company Scales, Hires and Salaries (Business Analysts)',fontsize=18,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","0dd8020e":"Firm_Size = de_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Company Name\",aggfunc=pd.Series.nunique).reset_index()\nFirm_Size = Firm_Size.drop([0])\nFirm_Size = Firm_Size[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size = Firm_Size.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size = Firm_Size.set_index('Revenue_USD').replace(np.nan,0)\n\nFirm_Size_Sal = de_sal.pivot_table(columns=\"Size\",index=\"Revenue_USD\",values=\"Est_Salary\",aggfunc=np.mean).reset_index()\nFirm_Size_Sal = Firm_Size_Sal.drop([0])\nFirm_Size_Sal = Firm_Size_Sal[['Revenue_USD','1 to 50 employees','51 to 200 employees','201 to 500 employees','501 to 1000 employees','1001 to 5000 employees','5001 to 10000 employees','10000+ employees']]\nFirm_Size_Sal = Firm_Size_Sal.reindex([12,3,10,5,8,11,6,1,2,7,9,4,13])\nFirm_Size_Sal = Firm_Size_Sal.set_index('Revenue_USD').replace(np.nan,0)\n\nf, axs = plt.subplots(ncols=2, sharey=True, figsize=(13,6.5))\n\nfs = sns.heatmap(Firm_Size,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"YlGnBu\", ax=axs[0]).set(title=\"Number of Companies\")\nfss = sns.heatmap(Firm_Size_Sal,annot=True,fmt='.0f',annot_kws={\"size\": 12},cmap=\"Oranges\",ax=axs[1]).set(title=\"Avg. Salaries\",ylabel='')\n\naxs[0].add_patch(Rectangle((1,2), 1, 5, fill=False, edgecolor='red', lw=3, clip_on=False))\naxs[1].add_patch(Rectangle((1,2), 1, 5, fill=False, edgecolor='blue', lw=3, clip_on=False))\n\nplt.setp([a.get_xticklabels() for a in axs[:]],rotation=45,ha='right')\nplt.suptitle('Company Scales, Hires and Salaries (Data Engineers)',fontsize=18,color='darkblue')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","7d4b033a":"Mod_Final","51f184b9":"modelperform = pd.DataFrame(\n        columns = [\n            'Model','Adj. R-squared','AIC','BIC','R2','RMSE','R2_val','RMSE_val','R2_test','RMSE_test'])\n\nmodelperform, X_train1, X_val1, X_test1 = scalibility('Combined Model',Mod_Final_1, Mod_Final_2)\nmodelperform","cb9b1afe":"ElasticNetPlot(alpha_max = 0.05, alpha_step = 0.001, L1_wt_value = 0.5)","94ccf14b":"coefficientbars(alpha_value = 0.008, L1_wt_value = 1, Mod_Final_2 = Mod_Final_2)","ad6a8038":"X_train, X_val, X_test, Interactions_Top = addTopInteractions()","93785f78":"# Add interaction terms to regression's variable list\nmodel_var_intr = Interactions_Top[Interactions_Top['inter'].isin(X_train.columns)]['inter']\nFinalVar_intr = list(Mod_Final_2['index']) + list(model_var_intr)\nFinalVar_intr2 = list(Mod_Final_2[Mod_Final_2['index']!='Const']['index']) + list(model_var_intr)\nFinalVar_intr_table = pd.DataFrame(FinalVar_intr,columns=['Var'])","495857cf":"intr_remove = []\n\nX_opt_Final_t, Final_name_t = backwardeliminate(FinalVar_intr, intr_remove, FinalVar_intr_table, FinalVar_intr2)\nMod_Final_t, VIF_Final_t, mod_Final_t = buildmodel(X_opt_Final_t, Final_name_t)\nCoef_Final_t = resulttable(Mod_Final_t)\n\ni = max(Coef_Final_t[Coef_Final_t.index.isin(model_var_intr)]['P>|t|'])\nwhile i > regr_p:\n    intr_remove.append(Coef_Final_t.index[Coef_Final_t['P>|t|']==i].values[0])\n    X_opt_Final_t, Final_name_t = backwardeliminate(\n        FinalVar_intr, intr_remove, FinalVar_intr_table, FinalVar_intr2)\n    Mod_Final_t, VIF_Final_t, mod_Final_t = buildmodel(X_opt_Final_t, Final_name_t)\n    Coef_Final_t = resulttable(Mod_Final_t)\n    i = max(Coef_Final_t[Coef_Final_t.index.isin(model_var_intr)]['P>|t|'])\n\nMod_Final_1_t,Mod_Final_2_t,Mod_Final_3_t = resulttables(Mod_Final_t, VIF_Final_t)\nMod_Final_t","4c8e47c6":"modelperform, X_train1, X_val1, X_test1 = scalibility('Final Model (Tuned)',Mod_Final_1_t,Mod_Final_2_t)\nmodelperform","21cf9a2f":"ElasticNetPlot(alpha_max = 0.05, alpha_step = 0.001, L1_wt_value = 0.5)","97fe784a":"coefficientbars(alpha_value = 0.01, L1_wt_value = 1, Mod_Final_2 = Mod_Final_2_t)","8ff2af00":"multicol(Coef_Final)","f7a80f44":"# label for final regression visualisation\nMod_Final_2['Label'] = ['Const',\n 'Company Rating (Glassdoor)',\n 'Has \"Easy Apply\"',\n '\"Data Science\" on Title',\n 'SYSTEM_ANALYST',\n '\"Business Intelligence\" on Title',\n 'DATA_QUALITY_ANALYST',\n '\"MACHINE_LEARNING\" on Title',\n 'DATA_REPORTING_ANALYST',\n 'DATA_SCIENTIST',\n 'DATA_ANALYST',\n 'Other Analysts',\n 'BUSINESS_ANALYST',\n '\"STAFF\" on Title',\n 'BUSINESS_DATA_ANALYST',\n '\"OLTP\" in JD',\n '\"PHD\" in JD',\n '\"ECONOMICS\" in JD',\n 'Job Location: CA',\n 'Job Location: FL',\n 'Job Location: WA',\n 'Job Location: VA',                        \n 'Job Location: UT',\n 'Job Location: Newark',\n 'Job Location: Dallas',                        \n 'Job Location: Mountain_View',                       \n 'Job Location: San_Jose',\n 'Job Location: New_York',\n 'Job Location: San_Diego',\n 'Job Location: Plano',\n 'Job Location: Menlo_Park',\n 'Job Location: Palo_Alto',\n 'Job Location: Fort_Worth',\n 'Job Location: Sunnyvale',\n 'Job Location: Santa_Clara',\n 'Job Location: Fremont',\n 'Job Location: Los Angeles',\n 'Job Location: Redwood_City',\n 'OH-based firm',\n 'IL-based firm',\n 'Revenue USD5-10B',\n 'College\/University',\n 'Nonprofit_Organization']","737e7f40":"plt_coef = Mod_Final_2[Mod_Final_2['Label']!='Const'].sort_values(\n    ['coef','std err'],ascending=True).reset_index(drop=True)\n\nsns.set()\nfig = plt.figure(figsize=(13, 13))\nplt.errorbar(y = plt_coef.iloc[:,8], x = plt_coef.iloc[:,1], \n             xerr = plt_coef.iloc[:,2]*2, fmt='o', color='b')\nplt.axvline(x=0, color='r',linestyle='--')\nplt.title(\"Coefficients with Errors [0.025 - 0.975]\",fontsize='15')\nplt.xlabel('Reference: USD 94.61K', color='r')\nplt.xticks(np.arange(-50, 50, step=10))\nplt.show()","b75739fc":"# Model Tuning <a id=\"tune1\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","37c5b83f":"### Get More Variables: Revenue, Size, Sector and Type of Ownership","73b4cde2":"Though the second model which the interaction terms are introduced has slightly higher R-squared and lower errors in the training set, both scores in validation and test sets are worse. This shows the second model might have made things too complicated and overfit the training data.","0d1a49a0":"## Hot Keywords in Job Descriptions: PHD, Master, Python, Machine Learning, SQL and more <a id=\"JD\"><\/a> \n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","048f4e65":"Elastic Net Regression suggests that the validation error of this model is close to optimal but could be lower if alpha increases to 0.008, implying some variable removal may make the model better.","19669e9c":"# About <a id=\"about\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>\n## Dataset\nThis dataset was created by [picklesueat](https:\/\/github.com\/picklesueat\/data_jobs_data) and contains 12,000+ job listing for data science jobs (all assumed to be open positions at the time the dataset was published in July 2020), with features such as:\n\n* Salary Estimate\n* Location\n* Company Rating\n* Job Description\n  and more.\n\n## Objectives\n* What kind of Data Science jobs get higher salaries? (Job Title, Job Description, EasyApply)\n* What kind of companies pay more? (Rating, Company, Size, *Years established (now - Founded)*, Type of ownership, Industry & Sector, Revenue)\n* Does job\/headquarters location matter to salaries?\n\n## Limitations and Assumptions\n\n* The results only reflet the outcome at the time the dataset was published, which is pressumed to be July 2020. Seasonal variation is disregarded (not a time-series data).\n* Somehow remote positions are not found in this dataset, so the impact of pandemic (more jobs becoming remote) on salary cannot be measured.\n* The salary estimates come from Glassdoor, which may not reflect the actual salaries.\n* The dataset is assumed to reflect the traits of the actual job market.\n* The salaries are nominal, not adjusted by living costs or consumer price index.","2509df4c":"## Introduce interaction terms to the first combined model <a id=\"int\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","15886c74":"## Tune the first combined model with Elastic Net <a id=\"EN1\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","3775e2dc":"# Table of Contents<a id=\"TOC\"><\/a>\n* [About](#about)\n* [Prepare Data](#1)\n    * [Import Libraries and Datasets](#1-1)\n    * [Clean Data](#1-2)\n    * [Prepare Variables for Regression Analysis](#1-3)\n* [Methodology](#method)\n* [Analysis Process (Codes)](#ap)\n* [Key Preliminary Results](#pl)\n    * [Job Titles vs Salaries (pie chart, distribution and errorbars)](#title)\n    * [Job Descriptions vs Salaries (bar chart and wordcloud](#JD)\n    * [Job Locations vs Salaries (bar chart with errorbars)](#Location)\n    * [First Combined Regression Model Output](#model1)\n* [Model Tuning](#tune1)\n    * [Tune the first combined model with Elastic Net](#EN1)\n    * [Introduce interaction terms](#int)\n    * [Final Model Output](#finaltab)\n    * [Multicollinearity](#multi)\n* [Final Regression Result in Visualisation](#final)","a22e972d":"# Upvote if you like my work!","6ceee621":"The avg. salary of all data science jobs is about 90K, and the median is around 80K. We can see distinguished salary distributions among Data Analysts (Red, Lower), Data Engineers (Green, Middle) and Data Scientists (Yellow, Higher).","dc566f66":"### Get Job Location Variables","2ec04da2":"## [Heatmaps] Company Size, Scale and Salaries <a id=\"hm\"><\/a> \n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","e0552e66":"### Get Keyword Variables from Job Descriptions","9ef9b3dc":"## Salary Comparison: Data Scientists > Data Engineers > Analysts <a id=\"title\"><\/a>\n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","6a6c5b97":"### Data Science Job Salary Distribution","5d34d5d8":"However, even with Lasso at alpha=0.008, no variables is necessariliy to be removed. Nevertheless, let's introduce some relevant interaction terms to see if it can get us more explainability.","c6529373":"## First Combined Regression Model Output <a id=\"model1\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","997b3ec7":"## Final Model (Tuned) <a id=\"finaltab\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","e3f3b035":"# Methodology: Backward-stepwise Regression <a id=\"method\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>\n\nThis Backward-stepwise Regression is designed for the scenario that**, within a limited time, needs to run regression analysis with the dataset containing lots of categorical data as independent variables.** \n\nVisit [this notebook](https:\/\/www.kaggle.com\/gawainlai\/backward-stepwise-regression-v1-0) for more details.\n\nKey steps:\n\n1. Categorical variable groups: for example, city, state, job title keywords...etc.\n2. Create dummy variables for each value whose presence in the dataset is at least >29.\n3. After splitting training\/validating\/testing datasets, perform T-tests and remove variables by p-value threshold 0.1.\n4. Perform backward-stepwise regression (p-value threshold 0.05) on variables that passed the T-tests in each variable group.\n5. Include all the variables having passed the preliminary regression models and re-run backward-stepwise regression.\n6. Plot the validation errors by Elastic Net alpha's to see how to tune the model (remove or add variables). At the same time, observe multicollinearity, consider interaction terms...etc.\n7. Compare the models and make the final decision.\n![image.png](attachment:image.png)","dc012fd0":"## Salary Comparison: California Cities at the top <a id=\"Location\"><\/a> \n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","4b976d85":"## Higher Company Rating is associated with slightly higher salaries <a id=\"rating\"><\/a> \n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","8ea505af":"# Data Preparation <a id=\"1\"><\/a>","f380cb3f":"* Bars are coefficient of the variables over salary, and lines represent errors (or range estimates).\n* 'OLTP' appearance in job descriptions has the highest average salary impact but also has the biggest vairance.\n* Positions require\/prefer PHD degrees obviously are paid more.","b7a07d17":"Some variables have high correlations, but mostly are just like City vs State. The only interesting correlations are (1) positive correlation between 'PHD' mentioned in job description and data scientist as job title; (2) negative correlation between 'PHD' and Data Analyst. This may imply that PHD's are more likely to become a data scientist than become a data analyst.","5a965442":"# Preliminary Results with Exploratory Visualisations <a id=\"pl\"><\/a> ","c648eb31":"### Keywords on Job Titles vs. Salaries\n* 'Data Scientist' as job title would have averagely 25K higher salaries holding other variables constant, whereas Analysts' salaries are significantly lower than other data science jobs.\n* Points are coefficient of the variables over salary, and lines represent errors (or range estimates).\n* 43 out of 51 positions with 'SECTOR' on titles are Data Scientist positions from IBM.\n* 'BI' = Business Intelligence.","ba7b434c":"# Analysis Process (Codes) <a id=\"ap\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","b40fa632":"## Clean Data <a id=\"1-2\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","502b6389":"Still the Elastic Net shows errors can be reduced when alpha is tuned to 0.01. The Coefficient Bars below shows that only 'IL-HQ','Rating' and 'PHD_JD' can be removed, but we actually can't because it needs to stay there as a reference to other interaction terms such as 'PHD_JD * IL_HQ' and 'IL_HQ * DATA_ANALYST'.","0bbc725c":"Doing nothing, we return to the first model ('Combined Model') and have a final look at its Multicollinearity although the regression output summary suggests it not severe. <a id=\"multi\"><\/a> ","e2a368ff":"# Key Insights <a id=\"Top\"><\/a>\n### [1. Salary Comparison: Data Scientists > Data Engineers > Analysts](#title)\n### [2. Salary Comparison: California Cities at the top](#Location)\n### [3. Hot Keywords in Job Descriptions: PHD, Master, Python, Machine Learning, SQL and more](#JD)\n### [4. Higher Company Rating is associated with slightly higher salaries](#rating)\n### [5. [Heatmaps] Company Size, Scale and Salaries](#hm)\n### [6. Data Science Job Salary Impact Map (Final Regression Result)](#final)","35ca19ea":"## Import Libraries and Dataset <a id=\"1-1\"><\/a>\n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","1668e1b6":"## Data Science Job Salary Impact Map (Final Regression Result) <a id=\"final\"><\/a> \n<a href=\"#Top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>","dcd48535":"### Get Keyword Variables from Job Titles","76101c52":"## Prepare Variables for Regression Analysis <a id=\"1-3\"><\/a> \n<a href=\"#TOC\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Back to Top<\/a>"}}