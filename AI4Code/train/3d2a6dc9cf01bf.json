{"cell_type":{"e4a990a4":"code","4f0299e6":"code","1d2a19c2":"code","7757ddea":"code","86aa1338":"code","091f1a6c":"code","99b78faa":"code","b099bcd2":"code","7275d833":"code","cca061d2":"code","c8c46966":"code","44cd5919":"code","eea8263d":"code","50edced1":"code","d41433fb":"code","a56340bb":"code","0717bf12":"code","e7820daf":"code","f1fe3d8a":"code","e6cecf77":"code","ef42084a":"code","377f1adb":"code","23b054e2":"code","196752ba":"code","74b37f99":"code","4f84039a":"code","5d3ad938":"markdown","b7893f10":"markdown","39bbdfb7":"markdown","5f1a3eeb":"markdown","aefb8b9e":"markdown","9c055e23":"markdown"},"source":{"e4a990a4":"import numpy as np \nimport pandas as pd \nfrom pandas import datetime\npd.set_option('display.max_columns', None)  \npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom copy import deepcopy\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator # Generates batches for sequence data\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n#import warnings\n#warnings.filterwarnings(\"ignore\")","4f0299e6":"prices = pd.read_csv('\/kaggle\/input\/nyse\/prices-split-adjusted.csv')\nfundamentals = pd.read_csv('\/kaggle\/input\/nyse\/fundamentals.csv')\nsecurities = pd.read_csv('\/kaggle\/input\/nyse\/securities.csv')","1d2a19c2":"prices.head(10)","7757ddea":"securities.head(10)","86aa1338":"fundamentals.head(10)","091f1a6c":"securities[securities['GICS Sector'] == 'Information Technology']['Security'].unique()","99b78faa":"#let's have a look at how things went for 'Visa Inc.'\nsecurities[securities['Security']=='Visa Inc.'] #to get ticker symbol, it's \"V\"\nvisa = prices[prices['symbol']==\"V\"] #making a subset of prices as a new set for Visa Inc\nvisa = visa.drop('symbol', 1)\nvisa","b099bcd2":"import pandas_profiling\nprofile = pandas_profiling.ProfileReport(visa)\nprofile","7275d833":"visa['date'] = pd.to_datetime(visa['date'])\nvisa.info()","cca061d2":"px.line(visa, x = 'date', y = ['open', 'close'])","c8c46966":"px.line(visa, x = 'date', y = ['low', 'high'])","44cd5919":"px.line(visa, x = 'date', y = ['volume'])","eea8263d":"df1 = deepcopy(visa) #for Neural Prophet, contains 'date'\ndf = deepcopy(visa) #for LSTM, dropping 'date' as a column, using as index\ndf.set_index(\"date\", inplace=True)","50edced1":"#scaling values in order to look at all of them on one graph\nfor feature in ['open', 'close', 'high', 'low', 'volume']:\n    sc = MinMaxScaler()\n    visa[feature] = sc.fit_transform(visa[feature].values.reshape(-1,1))\n    \n#looking at all numerical features at once    \npx.line(visa, x = 'date', y = ['open', 'close', 'high', 'low', 'volume'])","d41433fb":"len(df) # 1762\ntest_point = 50 #We want to predict for the last 50 values\ntest_index = int(len(df) - test_point) #1712\ntrain = df.iloc[:test_index,1:2] #to get the first 1712 values of the index and vwap\ntest = df.iloc[test_index:,1:2] #to get the last 50 values of the index and vwap\ntrain #to look at output (under cut)","a56340bb":"scaler = MinMaxScaler() #scaler for df dataset\nscaler.fit(train) #fit the scaler on train set\nMinMaxScaler(copy=True, feature_range=(0, 1))\nscaled_train = scaler.transform(train) #transform on train and test sets separately\nscaled_test = scaler.transform(test)","0717bf12":"length = 49 #for input shape\nbatch_size = 1\nn_features=1 #target variable\n\n#creating generators for fitting a model\ngenerator = TimeseriesGenerator(scaled_train,scaled_train, length=length,batch_size=batch_size)\nvalidation_generator = TimeseriesGenerator(scaled_test,scaled_test,length=length,batch_size=batch_size)\n\n#very simple Sequantial model\nmodel = Sequential()\nmodel.add(LSTM(50,input_shape=(length,n_features)))\nmodel.add(Dense(1))\n\n#adam optimizer, loss = mean squared error\nmodel.compile(optimizer = 'adam',loss='mse',metrics=['accuracy'])\n\nmodel.fit_generator(generator,epochs=20,validation_data=validation_generator)","e7820daf":"test_predictions = [] #variable for predictions\nfirst_eval_batch = scaled_train[-length:] #(49,1)\ncurrent_batch = first_eval_batch.reshape(1,length,n_features) #reshape to (1,49,1)\n\n#using a loop to predict: predict the first value,add the resut to test predictions \n#and replace the value in current batch. To make next predictions on the previous predictions\nfor i in range(len(test)):\n    current_pred = model.predict(current_batch)[0]\n    test_predictions.append(current_pred)\n    current_batch = np.append(current_batch[:,1:,:],\n                              [[current_pred]],axis = 1)","f1fe3d8a":"#inversing predictions from scaled to normal\ntrue_predictions = scaler.inverse_transform(test_predictions)\ntest['LSTM Predictions'] = true_predictions #add predictions to test set\ntest.plot(figsize=(12,8)) #plotting","e6cecf77":"df1 = df1[[\"date\", \"close\"]]\ndf1.rename(columns={\"date\": \"ds\", \"close\": \"y\"}, inplace=True)\ndf_train = df1.iloc[:test_index] #splitting data for not scaled train set\n#we already have test set as test","ef42084a":"!pip install git+https:\/\/github.com\/ourownstory\/neural_prophet.git  \n!pip install livelossplot    #it will allow us to use plot_live_live parameter \n#in the train function to get live training and validation plots.","377f1adb":"from neuralprophet import NeuralProphet","23b054e2":"model = NeuralProphet(n_changepoints=10,\n                      trend_reg=0.1,\n                      yearly_seasonality=False,\n                      weekly_seasonality=False,\n                      daily_seasonality=True)\nmetrics = model.fit(df_train, validate_each_epoch=True, \n                    valid_p=0.2, freq='D', \n                    plot_live_loss=True, \n                    epochs=100)","196752ba":"future = model.make_future_dataframe(df_train, \n                periods=50) #predict for 50 days\nforecast = model.predict(future)","74b37f99":"test[\"Forecast_Prophet\"] = forecast.yhat1.values\ntest.plot(figsize=(14, 7)) #plotting","4f84039a":"from sklearn.metrics import mean_absolute_error\nprint(\"Results for the test set:\")\nprint(\"MAE of LSTM:\", mean_absolute_error(test.close, \n                            test['LSTM Predictions']))\nprint(\"MAE of Neural Prophet:\", \n      mean_absolute_error(test.close, test.Forecast_Prophet))","5d3ad938":"It was written that prices-split-adjusted was done on the basis of prices, so we will use only 'adjusted' one. Let's load the other data and see if it is useful to predict close price.","b7893f10":"General trend shows increasing of prices year by year while the volume is decreasing (with some outliers).","39bbdfb7":"From profile report we can conclude that there are no missing values, prices have range from 16 to 84 and there is a strong correlation between prices variables. Another important note is that date column is categorical, not a datetime object. We need to fix this.","5f1a3eeb":"So, we have seen that 'prices' contains date, symbol, volume and prices variables like open, close, high and low. \n\n\"Fundamentals' doesn't look very promicing in predicting price.\n\n'Securities' contains additional info about the companies, like their sectors, full titles, headquarters, etc. Let's explore what companies are in Information Technology sector.","aefb8b9e":"We can look at MAE scores and admit that LSTM does a better job, but here it is even more obvious if we look on the graph. LSTM shows a nice generalization for 50 days. However, if we needed prediction for 20 days, Neural Prophet could be a better choice.","9c055e23":"We got 1762 rows and 7 columns, let's explore them."}}