{"cell_type":{"45a4ffcd":"code","6e06146a":"code","56ff263e":"code","edaa0e4a":"code","5bb9926e":"code","ef0b44f7":"code","87026974":"code","a57183e1":"code","7ec85c51":"code","5f3155e5":"code","0a7636dc":"code","a6487947":"code","5384fbcd":"code","34627a2a":"code","c36c174c":"code","d08f9e52":"code","011a1405":"code","3e7e7421":"code","352cf138":"code","aa5959f6":"code","7b9cb4b2":"code","3440327f":"code","47cd4bec":"code","bf3aaee4":"markdown","1eee01e6":"markdown","9add1c0d":"markdown","4e0475ec":"markdown","42ad0c1d":"markdown","107849ad":"markdown","a23a9a5b":"markdown","14f5a68f":"markdown","01ba43cc":"markdown","3475f41e":"markdown","48ec6b09":"markdown","58c8e67f":"markdown","1b015855":"markdown","f3219f73":"markdown","e32191ba":"markdown","58dbf6e6":"markdown","fe1509dd":"markdown","459fd2b7":"markdown","13be7b04":"markdown","f80fefaa":"markdown","abc3a4dc":"markdown","b677c5cc":"markdown","9fad82f8":"markdown"},"source":{"45a4ffcd":"# system imports\nimport os\nimport sys\n\n# data science\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\nimport seaborn as sns\n\n# signal processing\nfrom scipy import signal\nfrom scipy.ndimage import label\nfrom scipy.stats import zscore\nfrom scipy.interpolate import interp1d\nfrom scipy.integrate import trapz\n\n# misc\nimport warnings","6e06146a":"# style settings\nsns.set(style='whitegrid', rc={'axes.facecolor': '#EFF2F7'})\n\n# sample frequency for ECG sensor\nsettings = {}\nsettings['fs'] = 500","56ff263e":"# data recorded with the ECG sensor\ndf = pd.read_csv(\"..\/input\/sample-ecg-data\/ecg.csv\", sep=\";\", index_col=\"ms\")","edaa0e4a":"plt.figure(figsize=(20, 7))\nstart = 0\nstop = 5000\nduration = (stop-start) \/ settings['fs']\nplt.title(\"ECG signal, slice of %.1f seconds\" % duration)\nplt.plot(df[start:stop].index, df[start:stop].heartrate, color=\"#51A6D8\", linewidth=1)\nplt.xlabel(\"Time (ms)\", fontsize=16)\nplt.ylabel(\"Amplitude (arbitrary unit)\")\nplt.show()","5bb9926e":"def detect_peaks(ecg_signal, threshold=0.3, qrs_filter=None):\n    '''\n    Peak detection algorithm using cross corrrelation and threshold \n    '''\n    if qrs_filter is None:\n        # create default qrs filter, which is just a part of the sine function\n        t = np.linspace(1.5 * np.pi, 3.5 * np.pi, 15)\n        qrs_filter = np.sin(t)\n    \n    # normalize data\n    ecg_signal = (ecg_signal - ecg_signal.mean()) \/ ecg_signal.std()\n\n    # calculate cross correlation\n    similarity = np.correlate(ecg_signal, qrs_filter, mode=\"same\")\n    similarity = similarity \/ np.max(similarity)\n\n    # return peaks (values in ms) using threshold\n    return ecg_signal[similarity > threshold].index, similarity","ef0b44f7":"def get_plot_ranges(start=10, end=20, n=5):\n    '''\n    Make an iterator that divides into n or n+1 ranges. \n    - if end-start is divisible by steps, return n ranges\n    - if end-start is not divisible by steps, return n+1 ranges, where the last range is smaller and ends at n\n    \n    # Example:\n    >> list(get_plot_ranges())\n    >> [(0.0, 3.0), (3.0, 6.0), (6.0, 9.0)]\n\n    '''\n    distance = end - start\n    for i in np.arange(start, end, np.floor(distance\/n)):\n        yield (int(i), int(np.minimum(end, np.floor(distance\/n) + i)))","87026974":"sampfrom = 60000\nsampto = 70000\nnr_plots = 1\n\nfor start, stop in get_plot_ranges(sampfrom, sampto, nr_plots):\n    # get slice data of ECG data\n    cond_slice = (df.index >= start) & (df.index < stop) \n    ecg_slice = df.heartrate[cond_slice]\n\n    # detect peaks\n    peaks, similarity = detect_peaks(ecg_slice, threshold=0.3)\n    \n    # plot similarity\n    plt.figure(figsize=(20, 15))\n\n    plt.subplot(211)\n    plt.title(\"ECG signal with found peaks\")\n    plt.plot(ecg_slice.index, ecg_slice, label=\"ECG\", color=\"#51A6D8\", linewidth=1)\n    plt.plot(peaks, np.repeat(600, peaks.shape[0]), label=\"peaks\", color=\"orange\", marker=\"o\", linestyle=\"None\")\n    plt.legend(loc=\"upper right\")\n    plt.xlabel(\"Time (milliseconds)\")\n    plt.ylabel(\"Amplitude (arbitrary unit)\")\n    \n    plt.subplot(212)\n    plt.title('Similarity with QRS template')\n    plt.plot(ecg_slice.index, similarity, label=\"Similarity with QRS filter\", color=\"olive\", linewidth=1)\n    plt.legend(loc=\"upper right\")\n    plt.xlabel(\"Time (milliseconds)\")\n    plt.ylabel(\"Similarity (normalized)\")\n    ","a57183e1":"def group_peaks(p, threshold=5):\n    '''\n    The peak detection algorithm finds multiple peaks for each QRS complex. \n    Here we group collections of peaks that are very near (within threshold) and we take the median index \n    '''\n    # initialize output\n    output = np.empty(0)\n\n    # label groups of sample that belong to the same peak\n    peak_groups, num_groups = label(np.diff(p) < threshold)\n\n    # iterate through groups and take the mean as peak index\n    for i in np.unique(peak_groups)[1:]:\n        peak_group = p[np.where(peak_groups == i)]\n        output = np.append(output, np.median(peak_group))\n    return output","7ec85c51":"# detect peaks\npeaks, similarity = detect_peaks(df.heartrate, threshold=0.3)\n\n# group peaks\ngrouped_peaks = group_peaks(peaks)\n\n# plot peaks\nplt.figure(figsize=(20, 7))\nplt.title(\"Group similar peaks together\")\nplt.plot(df.index, df.heartrate, label=\"ECG\", color=\"#51A6D8\", linewidth=2)\nplt.plot(peaks, np.repeat(600, peaks.shape[0]),label=\"samples above threshold (found peaks)\", color=\"orange\", marker=\"o\", linestyle=\"None\")\nplt.plot(grouped_peaks, np.repeat(620, grouped_peaks.shape[0]), label=\"median of found peaks\", color=\"k\", marker=\"v\", linestyle=\"None\")\nplt.legend(loc=\"upper right\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"Amplitude (arbitrary unit)\")\nplt.gca().set_xlim(0, 200)\nplt.show()","5f3155e5":"# detect peaks\npeaks, similarity = detect_peaks(df.heartrate, threshold=0.3)\n\n# group peaks so we get a single peak per beat (hopefully)\ngrouped_peaks = group_peaks(peaks)\n\n# RR-intervals are the differences between successive peaks\nrr = np.diff(grouped_peaks)\n\n# plot RR-intervals\nplt.figure(figsize=(20, 7))\nplt.title(\"RR-intervals\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"RR-interval (ms)\")\n\nplt.plot(np.cumsum(rr), rr, label=\"RR-interval\", color=\"#A651D8\")\nplt.show()","0a7636dc":"plt.figure(figsize=(20, 7))\nplt.title(\"Distribution of RR-intervals\")\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\") # ignore FutureWarning \n    sns.kdeplot(rr, label=\"rr-intervals\", color=\"#A651D8\", shade=True)\n\noutlier_low = np.mean(rr) - 2 * np.std(rr)\noutlier_high = np.mean(rr) + 2 * np.std(rr)\n\nplt.axvline(x=outlier_low)\nplt.axvline(x=outlier_high, label=\"outlier boundary\")\nplt.text(outlier_low - 370, 0.004, \"outliers low (< mean - 2 sigma)\")\nplt.text(outlier_high + 20, 0.004, \"outliers high (> mean + 2 sigma)\")\n\nplt.xlabel(\"RR-interval (ms)\")\nplt.ylabel(\"Density\")\n\nplt.legend()\nplt.show()","a6487947":"plt.figure(figsize=(20, 7))\n\nrr_corrected = rr.copy()\nrr_corrected[np.abs(zscore(rr)) > 2] = np.median(rr)\n\nplt.title(\"RR-intervals\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"RR-interval (ms)\")\n\nplt.plot(rr, color=\"red\", label=\"RR-intervals\")\nplt.plot(rr_corrected, color=\"green\",  label=\"RR-intervals after correction\")\nplt.legend()\nplt.show()","5384fbcd":"rr_manual = np.loadtxt(\"..\/input\/manually-corrected-rrintervals\/manual-correction-rr.txt\", dtype=int)","34627a2a":"sampfrom = 240000\nsampto = 250000\nnr_plots = 1\n\n# detect peaks\npeaks, similarity = detect_peaks(df.heartrate, threshold=0.3)\n\n# group peaks so we get a single peak per beat (hopefully)\ngrouped_peaks = group_peaks(peaks)\n\n# RR-intervals are the differences between successive peaks\nrr = np.diff(grouped_peaks)\n\nfor start, stop in get_plot_ranges(sampfrom, sampto, nr_plots):\n    # plot similarity\n    plt.figure(figsize=(20, 10))\n\n    plt.title(\"ECG signal & RR-intervals\")\n    plt.plot(df.index, df.heartrate, label=\"ECG\", color=\"#51A6D8\", linewidth=1)\n    plt.plot(grouped_peaks, np.repeat(600, grouped_peaks.shape[0]), markersize=10, label=\"Found peaks\", color=\"orange\", marker=\"o\", linestyle=\"None\")\n    plt.legend(loc=\"upper left\")\n    plt.xlabel(\"Time (milliseconds)\", fontsize=16)\n    plt.ylabel(\"Amplitude (arbitrary unit)\", fontsize=16)\n    plt.gca().set_ylim(400, 800)\n    \n    ax2 = plt.gca().twinx()\n    ax2.plot(np.cumsum(rr_manual)+peaks[0], rr_manual, label=\"Corrected RR-intervals\", fillstyle=\"none\", color=\"#A651D8\", markeredgewidth=1, marker=\"o\", markersize=12)\n    ax2.plot(np.cumsum(rr)+peaks[0], rr, label=\"RR-intervals\", color=\"k\", linewidth=2, marker=\".\", markersize=8)\n    \n    ax2.set_xlim(start, stop)\n    ax2.set_ylim(-2000, 2000)\n    ax2.legend(loc=\"upper right\")\n    plt.xlabel(\"Time (ms)\")\n    plt.ylabel(\"RR-interval (ms)\")","c36c174c":"def timedomain(rr):\n    results = {}\n\n    hr = 60000\/rr\n    \n    results['Mean RR (ms)'] = np.mean(rr)\n    results['STD RR\/SDNN (ms)'] = np.std(rr)\n    results['Mean HR (Kubios\\' style) (beats\/min)'] = 60000\/np.mean(rr)\n    results['Mean HR (beats\/min)'] = np.mean(hr)\n    results['STD HR (beats\/min)'] = np.std(hr)\n    results['Min HR (beats\/min)'] = np.min(hr)\n    results['Max HR (beats\/min)'] = np.max(hr)\n    results['RMSSD (ms)'] = np.sqrt(np.mean(np.square(np.diff(rr))))\n    results['NNxx'] = np.sum(np.abs(np.diff(rr)) > 50)*1\n    results['pNNxx (%)'] = 100 * np.sum((np.abs(np.diff(rr)) > 50)*1) \/ len(rr)\n    return results","d08f9e52":"print(\"Time domain metrics - automatically corrected RR-intervals:\")\nfor k, v in timedomain(rr).items():\n    print(\"- %s: %.2f\" % (k, v))\n\nprint()\nprint(\"Time domain metrics - manually corrected RR-intervals:\")\nfor k, v in timedomain(rr_manual).items():\n    print(\"- %s: %.2f\" % (k, v))","011a1405":"# create interpolation function based on the rr-samples. \nx = np.cumsum(rr_manual) \/ 1000.0\nf = interp1d(x, rr_manual, kind='cubic')","3e7e7421":"# sample rate for interpolation\nfs = 4.0\nsteps = 1 \/ fs\n\n# now we can sample from interpolation function\nxx = np.arange(1, np.max(x), steps)\nrr_interpolated = f(xx)","352cf138":"plt.figure(figsize=(20, 15))\n\nplt.subplot(211)\nplt.title(\"RR intervals\")\nplt.plot(x, rr_manual, color=\"k\", markerfacecolor=\"#A651D8\", markeredgewidth=0, marker=\"o\", markersize=8)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"RR-interval (ms)\")\nplt.title(\"Interpolated\")\nplt.gca().set_xlim(0, 20)\n\nplt.subplot(212)\nplt.title(\"RR-Intervals (cubic interpolation)\")\nplt.plot(xx, rr_interpolated, color=\"k\", markerfacecolor=\"#51A6D8\", markeredgewidth=0, marker=\"o\", markersize=8)\nplt.gca().set_xlim(0, 20)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"RR-interval (ms)\")\nplt.show()","aa5959f6":"def frequency_domain(rri, fs=4):\n    # Estimate the spectral density using Welch's method\n    fxx, pxx = signal.welch(x=rr_interpolated, fs=fs)\n    \n    '''\n    Segement found frequencies in the bands \n     - Very Low Frequency (VLF): 0-0.04Hz \n     - Low Frequency (LF): 0.04-0.15Hz \n     - High Frequency (HF): 0.15-0.4Hz\n    '''\n    cond_vlf = (fxx >= 0) & (fxx < 0.04)\n    cond_lf = (fxx >= 0.04) & (fxx < 0.15)\n    cond_hf = (fxx >= 0.15) & (fxx < 0.4)\n    \n    # calculate power in each band by integrating the spectral density \n    vlf = trapz(pxx[cond_vlf], fxx[cond_vlf])\n    lf = trapz(pxx[cond_lf], fxx[cond_lf])\n    hf = trapz(pxx[cond_hf], fxx[cond_hf])\n    \n    # sum these up to get total power\n    total_power = vlf + lf + hf\n\n    # find which frequency has the most power in each band\n    peak_vlf = fxx[cond_vlf][np.argmax(pxx[cond_vlf])]\n    peak_lf = fxx[cond_lf][np.argmax(pxx[cond_lf])]\n    peak_hf = fxx[cond_hf][np.argmax(pxx[cond_hf])]\n\n    # fraction of lf and hf\n    lf_nu = 100 * lf \/ (lf + hf)\n    hf_nu = 100 * hf \/ (lf + hf)\n    \n    results = {}\n    results['Power VLF (ms2)'] = vlf\n    results['Power LF (ms2)'] = lf\n    results['Power HF (ms2)'] = hf   \n    results['Power Total (ms2)'] = total_power\n\n    results['LF\/HF'] = (lf\/hf)\n    results['Peak VLF (Hz)'] = peak_vlf\n    results['Peak LF (Hz)'] = peak_lf\n    results['Peak HF (Hz)'] = peak_hf\n\n    results['Fraction LF (nu)'] = lf_nu\n    results['Fraction HF (nu)'] = hf_nu\n    return results, fxx, pxx","7b9cb4b2":"print(\"Frequency domain metrics:\")\nresults, fxx, pxx = frequency_domain(rr_interpolated)\n\nfor k, v in results.items():\n    print(\"- %s: %.2f\" % (k, v))","3440327f":"plt.figure(figsize=(20, 7))\nplt.plot(fxx, pxx, color=\"k\", linewidth=0.3)\nplt.title(\"FFT Spectrum (Welch's periodogram)\")\n\n# create interpolation function for plotting frequency bands\npsd_f = interp1d(fxx, pxx)\n\n# setup frequency bands for plotting\nx_vlf = np.linspace(0, 0.04, 100)\nx_lf = np.linspace(0.04, 0.15, 100)\nx_hf = np.linspace(0.15, 0.4, 100)\n\nplt.gca().fill_between(x_vlf, psd_f(x_vlf), alpha=0.2, color=\"#A651D8\", label=\"VLF\")\nplt.gca().fill_between(x_lf, psd_f(x_lf), alpha=0.2, color=\"#51A6D8\", label=\"LF\")\nplt.gca().fill_between(x_hf, psd_f(x_hf), alpha=0.2, color=\"#D8A651\", label=\"HF\")\n\nplt.gca().set_xlim(0, 0.5)\nplt.gca().set_ylim(0)\nplt.xlabel(\"Frequency (Hz)\")\nplt.ylabel(\"Density\")\nplt.legend()\nplt.show()","47cd4bec":"def plot_poincare(rr):\n    rr_n = rr[:-1]\n    rr_n1 = rr[1:]\n\n    sd1 = np.sqrt(0.5) * np.std(rr_n1 - rr_n)\n    sd2 = np.sqrt(0.5) * np.std(rr_n1 + rr_n)\n\n    m = np.mean(rr)\n    min_rr = np.min(rr)\n    max_rr = np.max(rr)\n    \n    plt.figure(figsize=(10, 10))\n    plt.title(\"Poincare plot\")\n\n    sns.scatterplot(x=rr_n, y=rr_n1, color=\"#51A6D8\")\n\n    plt.xlabel(r'$RR_n (ms)$')\n    plt.ylabel(r'$RR_{n+1} (ms)$')\n\n    e1 = Ellipse((m, m), 2*sd1, 2*sd2, angle=-45, linewidth=1.2, fill=False, color=\"k\")\n    plt.gca().add_patch(e1)\n\n    plt.arrow(m, m, (max_rr-min_rr)*0.4, (max_rr-min_rr)*0.4, color=\"k\", linewidth=0.8, head_width=5, head_length=5)\n    plt.arrow(m, m, (min_rr-max_rr)*0.4, (max_rr-min_rr)*0.4, color=\"k\", linewidth=0.8, head_width=5, head_length=5)\n\n    plt.arrow(m, m, sd2 * np.sqrt(0.5), sd2 * np.sqrt(0.5), color=\"green\", linewidth=5)\n    plt.arrow(m, m, -sd1 * np.sqrt(0.5), sd1 * np.sqrt(0.5), color=\"red\", linewidth=5)\n\n    plt.text(max_rr, max_rr, \"SD2\", fontsize=20, color=\"green\")\n    plt.text(m-(max_rr-min_rr)*0.4-20, max_rr, \"SD1\", fontsize=20, color=\"red\")\n    \n    return sd1, sd2\n\nsd1, sd2 = plot_poincare(rr_manual)\nprint(\"SD1: %.3f ms\" % sd1)\nprint(\"SD2: %.3f ms\" % sd2)","bf3aaee4":"# Exploring Heart Rate Variability using Python\n\n## Introduction\n\n*One of the topics that got my interest last year was \u201cHeart Rate Variability\u201d or in short HRV. It is a popular biomarker that is used in many clinical trials and research for many years and is associated with a wide range of illnesses like diabetes, cardiovascular disease, obesity, chronic pain and stress-related diseases.*\n\n*I am particularly interested in how you can calculate HRV manually from a raw ECG signal. In this kernel I will share my findings with you by going through a step-by-step derivation of HRV using python.*\n\n*This kernel contains the code for my [blogpost on Medium](https:\/\/blog.orikami.nl\/exploring-heart-rate-variability-using-python-483a7037c64d).*","1eee01e6":"What\u2019s interesting, is that there are some rather suppressed R-peaks that still have a large similarity (around 66.000 ms). If we would just use thresholding on the original signal, we\u2019d definitely miss those peaks. Template matching **amplifies** the peaks, so it separates the features from the rest. In stage 2 we can pick these up with a threshold.\n\nNow, as template for the peak, I just used a sine wave, which is a very simplified model for a QRS segment. Ideally, we come up with a filter that generalises well over all sorts of variants we might encounter in the world.\n\nThis makes me wonder if we could find such a \u201cmother of all (normal) QRS segments\u201d using labeled ECG data automatically. As I mentioned earlier, this should be possible using Convolutional Networks. \n\nAfter all, in my humble dataset this simplified QRS filter works quite well, so I\u2019ll just use it to extract the RR-intervals.","9add1c0d":"### RR intervals: difference between successive peaks","4e0475ec":"## Non linear methods\n\n### Poincare plot\n\nIn Poincar\u00e9 HRV each RR interval is plotted against the next RR interval. The resulting shape of the plot is the essential feature, and can be used to identify certain types of heart failures and illnesses, is can be seen in this example.\n\n<img src=\"https:\/\/i.ibb.co\/3pFYDYV\/poincare-example.png\" alt=\"effects of illness and age on geometry of poincare plot\" style=\"width:600px;\"\/>","42ad0c1d":"### Error correction\n\nIf there is only a small amount of errors, we can correct them. There are different ways to correct the errors, and as a first attempt I replaced the outliers with the median value of the RR-intervals. For this, I use the [zscore](https:\/\/nl.wikipedia.org\/wiki\/Z-score), which is a metric for the **distance between a value and the mean of a distribution**, measured in standard deviations. By selecting RR-intervals with an absolute zscore larger than 2, we find the outliers and we can correct these by setting it to the median value.\n","107849ad":"This seems like a clean ECG signal and I suppose finding peaks won\u2019t be much of a problem. This ECG sensor returns values as a [arbitrary units](https:\/\/en.wikipedia.org\/wiki\/Arbitrary_unit), so these are not real voltages. We should take just interest in the relative values. \n\nLet\u2019s find out how we can find the peaks.","a23a9a5b":"### Peak detection using template matching\n\n*There are a [myriad](https:\/\/scholar.google.nl\/scholar?as_sdt=1,5&q=peak+detection+ECG&hl=nl&as_ylo=2018) of different QRS detectors. Most of those consists of two stages:*\n\n* **Stage 1 \u2014 Signal transformation\n**Construct a signal that maximises the features of interest, in our case this is the QRS-complex.\n\n* **Stage 2 \u2014 Decision rule \n**Use a threshold to separate the desired features from the rest of the signal\n\nA lot of peak detectors use a very neat trick for transforming the signal (stage 1) and I found it worth diving into that. It\u2019s called [template matching](https:\/\/en.wikipedia.org\/wiki\/Template_matching).\n\nTemplate matching is widely used in pattern recognition, for example in [particle analysis](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/15065671), cryptanalysis, [computer vision](https:\/\/www.researchgate.net\/publication\/272496016_Template_Matching_Techniques_in_Computer_Vision_Theory_and_Practice) and [computational neuroscience](https:\/\/www.hindawi.com\/journals\/cin\/2014\/919406\/). It\u2019s an elegant and powerful technique that uses a filter (also known as template or kernel), that contains a **specific feature** and use that filter to find this feature in a larger signal.\n\n![Cross-correlation is a measure of the similarity between two signals (image taken from [Giphy](https:\/\/media.giphy.com\/media\/VVPKOXc6aY1Lq\/source.gif))](https:\/\/cdn-images-1.medium.com\/max\/2000\/1*mWsGTGVdAsy6KoF3n3MyLA.gif)\n\nBasic idea is to slide the filter along a signal and compute the [cross-correlation](https:\/\/en.wikipedia.org\/wiki\/Cross-correlation) between the filter and the signal. If the filter matches a part of the signal closely, there is a strong correlation, which is another way of saying this part of the signal looks a lot like the feature in the filter. By setting a threshold (stage 2), you are able to find features you\u2019re interested in.\n>  Cross-correlation is often referred to as **convolution**, which is a slightly different operation where the filter is reversed f(t) -\u203a f(-t) before sliding it along the signal. Convolutional Neural Networks should actually be named Cross-correlation Networks, but i guess that didn\u2019t make it through the marketing department :-)\n\nIn images you could use this technique to find horizontal or vertical edges by designing specific filters. Convolutional Networks take this approach even further and learn filters that are relevant for certain classes automatically using back propagation.\n\n*Allright, so template matching seems great. Lets see how we can use it to find some RR-intervals!*","14f5a68f":"### Welch periodogram","01ba43cc":"### Load manually corrected rr-values","3475f41e":"### Load data","48ec6b09":"### Conclusion","58c8e67f":"### Imports","1b015855":"## Time domain analysis","f3219f73":"### Plot ECG signal with RR intervals","e32191ba":"Downside of this approach is that the timings are no longer precise. For anl HRV analysis, this doesn\u2019t have to be a big issue, but because I want to plot the RR-intervals on top of the ECG data, the alignment needs to be exact. Therefore, I corrected the errors manually as well.","58dbf6e6":"Time domain methods use RR-intervals and measure a whole range of metrics, that have something to say about the variability. These metrics were standardised in a [special report of the Task Force of ESC\/NASPE](https:\/\/academic.oup.com\/eurheartj\/article\/17\/3\/354\/485572) in 1996.\n\n![Differences between successive RR-intervals](https:\/\/cdn-images-1.medium.com\/max\/3448\/1*AKiF2Ht_r8V3IAOJowFIJQ.png)\n\n**RMSSD** \n\nThe most popular HRV metric is the Root Mean Square of Successive Differences or RMSSD. ****It\u2019s a measure for how much variation there exists in the heart rate. In a healthy heart, there is a natural variation, which is due to a balance between the sympathetic nervous system (SNS) and parasympathetic parts (PSNS) of the Autonomous Nervous System. If your body experiences stress, then the sympathetic system will activate, to prepare for fight or flight behaviour, and your heartrate will increase. The parasympathetic controls your body\u2019s \u201crest and digest\u201d responses and is associated with recovery. Parasympathetic activation conserves energy, constricts pupils, aids digestion, and slows your heart rate. These two parts of the nervous system are normally in a healthy balance, causing a natural variation in heart. If this balance is disturbed for any reason, this variance will change. A lower RMSSD is associated with stress and various illnesses.\n\n**Other metrics**\n\nRMSSD is often used as the score that represents your \u201cHRV\u201d. It\u2019s the most important one and it\u2019s used in a lot of research. Here\u2019s a list of other metrics, that are used for time domain analysis:\n\n* **Mean RR**: mean of RR-interval\n\n* **SDNN: **standard deviation of the RR-intervals\n\n* **Mean HR**: the well-known mean heartrate, measured in Beats Per Minute\n\n* **STD HR**: standard deviation of the heartrate\n\n* **Min HR**: lowest heartrate\n\n* **Max HR**: highest heartrate\n\n* **NN50**: The number of pairs of successive RR-intervals that differ by more than 50 ms. *(normal RR-intervals are often called NN-intervals)*\n\n* **PNN50**: The proportion of NN50 divided by the total number of RR-intervals.","fe1509dd":"Using an ECG sensor, I recorded 5 minutes of my own heart rate, while laying down in a park next my office \u2600\ufe0f, and it worked! Let\u2019s have a look:","459fd2b7":"## Frequency domain analysis\n\nAnother approach for analysing heart rate variability is to study the power distribution in the frequency domain. It shows how much of signal lies within a certain frequency band. High (HF) frequencies between 0.15\u20130.40 Hz are associated with Parasympathetic activity (recovery) and lower frequencies (LF) between 0.04\u20130.15 Hz are linked to both sympathetic and parasympathetic activity. The ratio of LF\/HF is believed to be a measure for the Autonomic Nervous System balance. A higher HF and a lower LF\/HF ratio indicate an increased HRV, which means your body is recovering.\n\n*Let\u2019s get to it and do a frequency domain analysis on our data.*\n","13be7b04":"In this kernel, I focussed on deriving HRV from a raw ECG signal and I\u2019ve learned a great deal:\n\n* Diving into template matching was great fun. Previous learnings when studying convolutional networks just got new meaning, and I learned how to create my own personal peak detection algorithm, yay!\n* I just love plotting with matplotlib and seaborn. You always learn a great deal when you try to create meaningful plots.\n* I learned that there are still many things left to explore. Most importantly, what say about the physiological state if we measure HRV? How can we use HRV in practice as an indicator for stress or illnesses? \n\nI hope you found this kernel useful and if you have any questions of comments please let me know! You can find the blogpost on this topic on [Medium](https:\/\/blog.orikami.nl\/exploring-heart-rate-variability-using-python-483a7037c64d).\n\nCheers,\n\nSalomon Tetelepta\n","f80fefaa":"From this plot, we can see that the average RR-intervals is around 850 ms, which is about 70 BPM. We also see a couple of outliers, so I guess the peak detection is not flawless after all. When the detection algorithm misses a peak, some intervals are very large (around twice the mean). Some peaks are found very near two each other, and in that case the RR-intervals are very short.","abc3a4dc":"### Group similar peaks\n\nWhen using thresholding to extract the peaks, there are still multiple samples found for a single peak. To get single a value for each peak, I'll group the samples that are very near.","b677c5cc":"### Settings","9fad82f8":"### Interpolation\n\nWe can\u2019t just use the RR-intervals and do a Fourier Transform, because an FFT needs evenly sampled data. We can achieve this by interpolating the data. To interpolate, we first transform our list with RR-intervals to a time series were we arrange RR-intervals over time. To find the time points for each interval, we sum up the values of the RR-intervals. Next, we create an interpolation function that we can use to sample from with any resolution we want. We use a resolution of 4 samples per second. Now we can create an evenly spaced set of datapoints that we can use for the frequency analysis.\n"}}