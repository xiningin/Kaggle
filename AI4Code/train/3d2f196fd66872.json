{"cell_type":{"72e6e697":"code","1ad9eef4":"code","ca2306d8":"code","2e5f0f7e":"code","39c7d868":"code","64dffa25":"code","36d07c11":"code","1c55b39c":"code","369336d0":"code","055344a2":"code","4e33a298":"code","bc6f55bd":"code","db733980":"code","ce5ff24f":"code","3cd35c15":"code","51b5d1b9":"code","36d0cdf8":"code","46eae510":"code","61ea56a3":"code","a976ba80":"code","da86b45d":"code","198abcdb":"markdown","4a42a2d9":"markdown","14efef79":"markdown","6a7d278f":"markdown","580ce7bd":"markdown","e898bef8":"markdown","9726ae86":"markdown","adec6c84":"markdown","af504e44":"markdown","ee873a8a":"markdown","bc7ac1b8":"markdown","f284d464":"markdown","17d2b1b4":"markdown","ee40920a":"markdown","fa6836f4":"markdown","66ea22ae":"markdown","ed05b33f":"markdown","1c03d762":"markdown","16d4afb4":"markdown","51a890de":"markdown","cfcf76e1":"markdown","44ee3c3e":"markdown","709667d0":"markdown","265c46b4":"markdown","dec733c8":"markdown"},"source":{"72e6e697":"import os, warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score, confusion_matrix\nimport scikitplot as skplt\nfrom imblearn.over_sampling import SMOTE\n\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")","1ad9eef4":"train = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv').set_index(\"id\")\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv').set_index(\"id\")\n\nsample_submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n\ntrain[\"song_duration_ms\"] = train[\"song_duration_ms\"] \/ (1000*60)\ntest[\"song_duration_ms\"] = test[\"song_duration_ms\"] \/ (1000*60)\n\ntrain.rename(columns={\"song_duration_ms\":\"song_duration_m\"}, inplace=True)\ntest.rename(columns={\"song_duration_ms\":\"song_duration_m\"}, inplace=True)\n\nfeature_cols = test.columns.tolist()","ca2306d8":"ax = sns.countplot(data=train, x=train.song_popularity)\nfor p in ax.patches:\n    ax.annotate(\"{} ({}%)\".format(p.get_height(),p.get_height()\/train.shape[0]*100), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title(\"Train Dataset Target Counts\")\nplt.show()","2e5f0f7e":"x_train, x_valid, y_train, y_valid = train_test_split(train[feature_cols], \n                                                      train[\"song_popularity\"], \n                                                      test_size=0.2, \n                                                      random_state=42, \n                                                      stratify=train.song_popularity)\nxgb_params = {\n        'eval_metric': 'auc', \n        'objective': 'binary:logistic', \n        'tree_method': 'gpu_hist', \n        'gpu_id': 0, \n        'predictor': 'gpu_predictor', \n        'n_estimators': 10000, \n        'learning_rate': 0.01, \n        'gamma': 0.4, \n        'max_depth': 3, \n        'seed': 42,       \n        'min_child_weight': 300, \n        'subsample': 0.7, \n        'colsample_bytree': 0.8, \n        'colsample_bylevel': 0.9, \n        'use_label_encoder': False,\n        'lambda': 0, \n        'alpha': 10\n}\n\nxgb_model = XGBClassifier(**xgb_params)\nxgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n\npreds_train = xgb_model.predict_proba(x_train)[:,1]\npreds = xgb_model.predict_proba(x_valid)\npreds_valid = preds[:,1]\nauc_train = roc_auc_score(y_train, preds_train)\nauc_valid = roc_auc_score(y_valid, preds_valid)\nprint(f\"AUC -> train: {auc_train:.5f} | valid: {auc_valid:.5f}\")","39c7d868":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nax = sns.countplot(xgb_model.predict(x_valid))\nfor p in ax.patches:\n    ax.annotate(\"{} ({}%)\".format(p.get_height(),p.get_height()\/x_valid.shape[0]*100), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title(\"model.predict() with threshold 0.5\")\nplt.subplot(1,2,2)\nsns.histplot(preds_valid, kde=True, color='orange')\nplt.title(\"model.predict_poba() result\")\nplt.suptitle(\"Validation Set Prediction Distribution\")\nplt.show()","64dffa25":"df = pd.concat([train.loc[x_valid.index], pd.DataFrame(preds_valid, columns=[\"predict_proba\"], index=y_valid.index)], axis=1)\ndf[\"predict\"] = df[\"predict_proba\"].apply(lambda x: 1 if x>0.5 else 0)\ndf[\"is_prediction_correct\"] = df[\"song_popularity\"] == df[\"predict\"]\ndf.head()","36d07c11":"print(\"Pred 1: {} ({:.2f}%)\\nTrue 1: {} ({:.2f}%)\".\\\n      format(df[\"predict\"].sum(), df[\"predict\"].sum()\/df.shape[0]*100,\n             df[\"song_popularity\"].sum(), df[\"song_popularity\"].sum()\/df.shape[0]*100))\nprint()\nprint(\"Pred 0: {} ({:.2f}%)\\nTrue 0: {} ({:.2f}%)\".\\\n      format((df[\"predict\"]==0).sum(), (df[\"predict\"]==0).sum()\/df.shape[0]*100,\n             (df[\"song_popularity\"]==0).sum(), (df[\"song_popularity\"]==0).sum()\/df.shape[0]*100))\n            \nprint(\"\\nCorrect Predicted: {} of {} samples ({:.2f}%)\".\\\n      format(df[\"is_prediction_correct\"].sum(), \n             df.shape[0], df[\"is_prediction_correct\"].sum()\/df.shape[0]*100))","1c55b39c":"plt.style.use(\"default\")\ncm = confusion_matrix(df[\"song_popularity\"], df[\"predict\"])\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.subplot(1,2,2)\nsns.heatmap(cm\/np.sum(cm), annot=True, fmt='.2%')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.suptitle(\"Confusion Matrix for Validation Dataset\")\nplt.show()","369336d0":"sns.set_style('darkgrid')\nskplt.metrics.plot_roc(df[\"song_popularity\"].values.ravel(), preds, figsize=(5,5), plot_micro=False, plot_macro=False, classes_to_plot=1)\nplt.show()","055344a2":"plt.figure(figsize=(12,25))\nfor i, col in enumerate(feature_cols):\n    plt.subplot(7,2,i+1)\n    sns.kdeplot(data=df, x=col, hue='is_prediction_correct', shade=True)\nplt.show()","4e33a298":"plt.figure(figsize=(12,25))\nfor i, col in enumerate(feature_cols):\n    plt.subplot(7,2,i+1)\n    sns.kdeplot(data=df[df.song_popularity==0], x=col, hue='is_prediction_correct', shade=True, multiple=\"stack\")\nplt.show()","bc6f55bd":"sns.set_style('darkgrid')\nplt.figure(figsize=(12,25))\nfor i, col in enumerate(feature_cols):\n    plt.subplot(7,2,i+1)\n    sns.kdeplot(data=df[df.song_popularity==1], x=col, hue='is_prediction_correct', shade=True, multiple=\"stack\")\n    \nplt.show()","db733980":"ax = sns.countplot(data=train, x=train.song_popularity)\nfor p in ax.patches:\n    ax.annotate(\"{} ({}%)\".format(p.get_height(),p.get_height()\/train.shape[0]*100), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title(\"Train Dataset Target Counts\")\nplt.show()","ce5ff24f":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nax = sns.countplot(xgb_model.predict(test[feature_cols]))\nfor p in ax.patches:\n    ax.annotate(\"{} ({}%)\".format(p.get_height(),p.get_height()\/test.shape[0]*100), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title(\"model.predict() with threshold 0.5\")\nplt.subplot(1,2,2)\nsns.histplot(xgb_model.predict_proba(test[feature_cols])[:,1], kde=True, color='orange')\nplt.title(\"model.predict_poba() result\")\nplt.suptitle(\"Test Set Prediction Distribution\")\nplt.show()","3cd35c15":"sample_submission[\"song_popularity\"] = xgb_model.predict_proba(test[feature_cols])[:,1]\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()  #57.8 PL","51b5d1b9":"train_undersampled = train.drop(train[train.song_popularity==0].sample(n=10848, random_state=42).index)\ntrain_undersampled = train_undersampled.reset_index(drop=True)","36d0cdf8":"ax = sns.countplot(data=train_undersampled, x=train_undersampled.song_popularity)\nfor p in ax.patches:\n    ax.annotate(\"{} ({}%)\".format(p.get_height(),p.get_height()\/train_undersampled.shape[0]*100), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title(\"Train Dataset Target Counts\")\nplt.show()","46eae510":"x_train, x_valid, y_train, y_valid = train_test_split(train_undersampled[feature_cols], \n                                                      train_undersampled[\"song_popularity\"], \n                                                      test_size=0.2, \n                                                      random_state=42, \n                                                      stratify=train_undersampled.song_popularity)","61ea56a3":"xgb_params = {\n    'eval_metric': 'auc', \n    'objective': 'binary:logistic', \n    'tree_method': 'gpu_hist', \n    'gpu_id': 0, \n    'predictor': 'gpu_predictor', \n    'n_estimators': 10000, \n    'learning_rate': 0.01, \n    'gamma': 0.4, \n    'max_depth': 3, \n    'seed': 42,       \n    'min_child_weight': 300, \n    'subsample': 0.7, \n    'colsample_bytree': 0.8, \n    'colsample_bylevel': 0.9, \n    'use_label_encoder': False,\n    'lambda': 0, \n    'alpha': 10\n}\n\nxgb_model = XGBClassifier(**xgb_params)\nxgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n\npreds_train = xgb_model.predict_proba(x_train)[:,1]\npreds = xgb_model.predict_proba(x_valid)\npreds_valid = preds[:,1]\nauc_train = roc_auc_score(y_train, preds_train)\nauc_valid = roc_auc_score(y_valid, preds_valid)\nprint(f\"AUC -> train: {auc_train:.5f} | valid: {auc_valid:.5f}\")","a976ba80":"sample_submission[\"song_popularity\"] = xgb_model.predict_proba(test[feature_cols])[:,1]\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()  #58.8 PL","da86b45d":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nax = sns.countplot(xgb_model.predict(test[feature_cols]))\nfor p in ax.patches:\n    ax.annotate(\"{} ({}%)\".format(p.get_height(),p.get_height()\/test.shape[0]*100), (p.get_x() + p.get_width() \/ 2., p.get_height()), \n       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nplt.title(\"model.predict() with threshold 0.5\")\nplt.subplot(1,2,2)\nsns.histplot(xgb_model.predict_proba(test[feature_cols])[:,1], kde=True, color='orange')\nplt.title(\"model.predict_poba() result\")\nplt.suptitle(\"Test Set Prediction Distribution\")\nplt.show()","198abcdb":"# Confusion Matrix","4a42a2d9":"There is no significant difference between distribution of featuress in _false predicted_ vs _true predicted_.","14efef79":"Mostly predicted correctly. Orange plot is Unpopular songs predicted correctly, and the blue one stack on top of it. rarely of unpopular songs predicted as popular. ","6a7d278f":"# Load Data","580ce7bd":"# Tackle Imbalanced Data\n\nSo, one problem is the skewed dataset. The easiest way to tackle this problem, is over-sampling or under-sampling:\n\n[![0-49-Lg-Gs-Y4l09s-Nwc-R.png](https:\/\/i.postimg.cc\/6qDLZjWD\/0-49-Lg-Gs-Y4l09s-Nwc-R.png)](https:\/\/postimg.cc\/30Fv5ZDB)\n\nLets see what happens if we under-sample the data. \n\nThere are many methods to under-sampling and over-sampling. You can find some of them implemented in the `imbalanced-learn` library [here](https:\/\/imbalanced-learn.org\/stable\/references\/index.html#api)","e898bef8":"Validation set prediction has a normal distribution with the mean around 0.35. ","9726ae86":"Test prediction is a normal distribution, which its mean is around 0.38, likewise the validation set prediction.  ","adec6c84":"# Examine Class 0 (Unpopular Songs)","af504e44":"The prediction distribution is still normal, but the mean has changed to around ~0.5","ee873a8a":"Now we can see class 1 (Popular song) is poorly classified.","bc7ac1b8":"May we need more samples from class 1 (Popular Songs).","f284d464":"# Error Analysis\n\nSo far, most works are getting an `AUC` around ~0.57 in notebooks and ~0.60 in public leaderborad. Let's check the 60% of data which are predicted correctly and the rest.","17d2b1b4":"# Explore Validation Set Prediction","ee40920a":"# Train Model","fa6836f4":"# Under-Sampling","66ea22ae":"We considered the threshold as 0.5 for assigning prediction probability to popular or unpopulr. If we change the threshold, we will get different results. \n\nThe better way is evaluating model with `roc auc` metric. `AUC` stands for **area under curve** and you can see it gets 0.57, as the area under the green line. ","ed05b33f":"# Examine Class 1 (Popular Songs)","1c03d762":"## Work in progress....","16d4afb4":"Uh. We can see most of class 1 (popular songs) are predicted false (unpopular=0)","51a890de":"We got a better accuracy by under-sampling (even with removing ~10K samples from dataset)","cfcf76e1":"# Distribution of Features in True and False predictions\n\nCan we see a significant distribution in features of true and false predicted samples?","44ee3c3e":"# Test Set Prediction","709667d0":"We are getting 63% accuracy, because 63.56% of data are in class 0. That's why `accuracy` metric is not appropriate for evaluate model. The better way for inspecting how the model predicted is plotting confusion matrix to examine each class separately.","265c46b4":"We got `AUC` 57 for validation dataset. ","dec733c8":"# AUC"}}