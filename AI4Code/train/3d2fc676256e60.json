{"cell_type":{"14e99245":"code","234eb3e3":"code","bb27ae5c":"code","3b861532":"code","68e87f5a":"code","8c5583ab":"code","de5ab5ee":"code","10c80a6a":"code","c25317c0":"code","ab00ce35":"code","d8e616e2":"code","a7217b3a":"code","78aae9ae":"code","67d1519f":"code","5bd97fa9":"code","ee49d448":"code","82629f2d":"code","becc464b":"code","fcc301d3":"markdown","c3aea264":"markdown","19564082":"markdown","5dfcb4ca":"markdown","9669c65e":"markdown","c3d181e1":"markdown","5c392111":"markdown","75efc782":"markdown","4943df28":"markdown","25c36ebf":"markdown","cf1fc028":"markdown","a6bee8f3":"markdown","29f7237f":"markdown"},"source":{"14e99245":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","234eb3e3":"import pandas as pd\nimport numpy as np\nfrom numpy import array\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom datetime import datetime, timedelta\nfrom sklearn.preprocessing import MinMaxScaler # pre-traitement des donn\u00e9es\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM # LSTM = Long Short Term Memory","bb27ae5c":"train = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/train.csv')\nassets = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv')","3b861532":"display(train.head(10), assets)","68e87f5a":"train[\"datetime\"] =  pd.to_datetime(train['timestamp'], unit='s')\n# fusionner les deux df\nprices = pd.merge(train, assets, how=\"outer\", on = [\"Asset_ID\"])\n\nprices.index = prices['timestamp']\nprices = prices.sort_index()\nind = prices.index.unique()","8c5583ab":"prices","de5ab5ee":"# selectionner seulement le bitcoin\nbtc = prices[prices[\"Asset_Name\"] == \"Bitcoin\"]\nbtc_price = btc[[\"datetime\", \"Close\"]]\n\n# afficher les prix sur l'ensemble du range de temps (i.e. 01-01-2018 au 21-09-2021)\nplt.figure(figsize=(12,8))\nplt.plot(btc_price[\"Close\"], color=\"green\", label=\"Bitcoin\")\nplt.legend()\nplt.title(\"Prix historiques du Bitcoin $\")\nplt.xlabel('Temps (min)')\nplt.ylabel(\"Price ($)\")\nplt.show()","10c80a6a":"btc_price","c25317c0":"# filtrer nos donn\u00e9es sur 01-2018\ntest_from_date = pd.Timestamp(2018,1,1,0,0) \ntest_to_date = pd.Timestamp(2018,1,31,23,59)\nbtc_price_train = btc_price[btc_price.datetime <= test_to_date] \nprint(len(btc_price_train))\n\n# LSTM utilisant tangente hyperbolique (tanh) qui est une mesure sensible \u00e0 la magnitude des donn\u00e9es, nous devons les normaliser les prix en un vecteur compris entre -1 et 1\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(btc_price_train[\"Close\"].values.reshape(-1,1))\n\n# definir l'intervalle de temps \u00e0 pr\u00e9dire : 15 minutes. \n# nous ne pouvons pr\u00e9dire qu'une seule minute dans dans le futur en se servant des 14 derni\u00e8res minutes.\nprediction_min = 15","ab00ce35":"# comme requis pour les r\u00e9seaux LSTM, nous devons transformer les donn\u00e9es d'entr\u00e9e en (n_\u00e9chantillons, pas de temps, n_caract\u00e9ristiques). \n# dans cet exemple, les n_caract\u00e9ristiques sont 1 et le pas de temps de 14 (donn\u00e9es des derni\u00e8res minutes utilis\u00e9es pour l'entra\u00eenement).\n\n# cr\u00e9er deux listes vides et les remplir avec les donn\u00e9es d'entrainement format\u00e9es\nx_train  = [] # liste de donn\u00e9es pour entrainer le mod\u00e8le\ny_train = [] # liste de donn\u00e9es que le mod\u00e8le doit pr\u00e9dire pendant l'entrainement \nfor x in range(prediction_min, len(scaled_data)):\n    x_train.append(scaled_data[x-prediction_min:x, 0])\n    y_train.append(scaled_data[x, 0])\n        \nx_train, y_train = np.array(x_train), np.array(y_train) # tableau numpy\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n\n# x_train est de dimension (44624, 15, 1). 44624 car nous regardons les 15 minutes pass\u00e9s (44639 - 15 = 44624).\n# y_train est seulement de dimension (44624,) car le mod\u00e8le ne pr\u00e9dit qu'une seule valeur chaque minute.\nprint('trainX shape == {}.'.format(x_train.shape))\nprint('trainY shape == {}.'.format(y_train.shape))","d8e616e2":"# cr\u00e9er le mod\u00e8le puis l'entrainer\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))\nmodel.add(Dropout(0.2)) #Drop-out est utilis\u00e9 pour contr\u00f4ler le sur-ajustement pendant l'entra\u00eenement\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(units=50))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=1)) # prediction du prix de la prochaine minute\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n# fit le mod\u00e8le\ntrained = model.fit(x_train, y_train, epochs=15, batch_size=32, validation_split=0.1, verbose=1)","a7217b3a":"plt.plot(trained.history['loss'], label='Perte entrainement')\nplt.plot(trained.history['val_loss'], label='Perte validation')\nplt.legend()","78aae9ae":"# filtrer nos donn\u00e9es sur 02-2018\ntest_from_date = pd.Timestamp(2018,2,2,0,0) \ntest_to_date = pd.Timestamp(2018,2,28,23,59) \ntest_data = btc_price[(btc_price.datetime >= test_from_date) & (btc_price.datetime <= test_to_date)] \nactual_prices = test_data[\"Close\"].values\ntotal_dataset = pd.concat((btc_price_train[\"Close\"], test_data[\"Close\"]), axis=0)\n\nmodel_inputs = total_dataset[len(total_dataset) - len(test_data) - prediction_min:].values\nmodel_inputs = model_inputs.reshape(-1,1)\nmodel_inputs = scaler.transform(model_inputs)\n\nx_test = []\nfor x in range(prediction_min, len(model_inputs)):\n    x_test.append(model_inputs[x-prediction_min:x, 0])\n    \nx_test = np.array(x_test)\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\npredicted_prices = model.predict(x_test)\npredicted_prices = scaler.inverse_transform(predicted_prices)","67d1519f":"plt.figure(figsize=(12,8))\nplt.plot(actual_prices, color=\"black\", label=\"Actual bitcoin Price\")\nplt.plot(predicted_prices, color=\"green\", label=\"Predicted bitcoin Price\")\nplt.legend()\nplt.title(\"Bitcoin prediction\")\nplt.xlabel('Temps')\nplt.ylabel(\"Price\")","5bd97fa9":"reel = actual_prices.copy()\npred = np.resize(predicted_prices, predicted_prices.shape[0])        \nnp.corrcoef(reel, pred)[0,1]","ee49d448":"real_data = [model_inputs[len(model_inputs) - 100:len(model_inputs), 0]]\nreal_data = np.array(real_data)\nreal_data = np.reshape(real_data, (real_data.shape[0], real_data.shape[1], 1))\nprediction = model.predict(real_data)\nprediction = scaler.inverse_transform(prediction)\n\nx_input = real_data.copy()\ntemp_input = real_data.copy().reshape(1,-1)\ntemp_input = list(temp_input)\ntemp_input = temp_input[0].tolist()\ntemp_input\n\nlst_output =[]\nn_steps=100\ni=0\nnb_min_pred = 15\n\nwhile(i<nb_min_pred):\n    if(len(temp_input)>100):\n        x_input = np.array(temp_input[1:])\n        x_input = x_input.reshape(-1,1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        yhat = model.predict(x_input, verbose=0)\n        temp_input.extend(yhat[0].tolist())\n        temp_input = temp_input[1:]\n        lst_output.extend(yhat.tolist())\n        i += 1\n    else:\n        x_input = x_input.reshape((1, n_steps, 1))\n        yhat = model.predict(x_input, verbose=0)\n        temp_input.extend(yhat[0].tolist())\n        lst_output.extend(yhat.tolist())\n        i+=1\nprint(\"Done\")\noutput = scaler.inverse_transform(lst_output)\noutput = np.resize(output, output.shape[0])  ","82629f2d":"day_new = np.arange(0, 100)\nday_pred = np.arange(85, 100)\nreel_data = test_data.Close[len(test_data.Close)-100:len(test_data.Close)]\n\nplt.figure(figsize=(12,8))\nplt.plot(day_new, reel_data, color=\"black\", label=\"Prix actuel Bitcoin\")\nplt.plot(day_pred, output, color=\"green\", label=\"Prix pr\u00e9dits Bitcoin\")\nplt.legend()\nplt.title(\"Bitcoin prediction $\")\nplt.xlabel('Temps (min)')\nplt.ylabel(\"Price ($)\")\nplt.show()","becc464b":"print(np.corrcoef(test_data.Close[len(test_data.Close)-15:len(test_data.Close)].values,output)[0,1])","fcc301d3":"Nous utilisons par la suite la m\u00e9moire longue \u00e0 court terme (LSTM) qui est un r\u00e9seau de neurones complexe utilis\u00e9 dans le domaine du Deep Learning.\nUne unit\u00e9 LSTM commune est compos\u00e9e d'une cellule, d'une porte d'entr\u00e9e, d'une porte de sortie et d'une porte d'oubli. La cellule se souvient des valeurs sur des intervalles de temps arbitraires et les trois portes r\u00e9gulent le flux d'informations entrant et sortant de la cellule.\nLes r\u00e9seaux LSTM sont bien adapt\u00e9s \u00e0 la classification, au traitement et \u00e0 la r\u00e9alisation de pr\u00e9dictions bas\u00e9es sur des donn\u00e9es de s\u00e9ries chronologiques, car il peut y avoir des d\u00e9calages de dur\u00e9e inconnue entre des \u00e9v\u00e9nements importants dans une s\u00e9rie chronologique.\n","c3aea264":"# INTRODUCTION\nLes objectifs de la comp\u00e9tition \"G-Research Crypto Forecasting\" \u00e9tant tr\u00e8s compliqu\u00e9s, nous avons simplifi\u00e9 les donn\u00e9es \npour nous concentrer seulement sur 1 cryptomonnaie et pour la pr\u00e9dire sur 1 mois.\nLes r\u00e9sultats ne sont pas vraiment repr\u00e9sentatifs du programme car nous l'entrainons sur \nseulement 1 mois de jeu de donn\u00e9es.\nSi on l'entrainait sur 3 ans, nos pr\u00e9dictions seraient bien plus fiables mais cela prends \nbeaucoup trop de temps.\nNous nous sommes principalement aid\u00e9es d'un tutoriel ainsi que d'autres codes pr\u00e9sents \nsur la plateforme.","19564082":"## Import des librairies","5dfcb4ca":"Le mod\u00e8le arrive \u00e0 pr\u00e9dire les 15 prochaines minutes mais semble ne pas \u00eatre robuste (correlation de 7% environ). Nous pourrions am\u00e9liorer la pr\u00e9cision du mod\u00e8le en l'entrainant sur un jeu de donn\u00e9es plus grand (ici qu'un seul mois), le fit plus longtemps (ici epochs=15), ou bien cr\u00e9er un mod\u00e8le prenant en compte les volumes trad\u00e9s ainsi que la corr\u00e9lation entre les autres actifs.\n\nIl faut aussi prendre en compte le fait qu'il est tr\u00e8s compliqu\u00e9 de pr\u00e9voir les prix qui sont tr\u00e8s volatiles. Il n'est donc pas surprenant d'avaoir une corr\u00e9lation si faible.","9669c65e":"# V. Pr\u00e9cision de la pr\u00e9diction (correlation)","c3d181e1":"# I. Pre-processing","5c392111":"## Import des donn\u00e9es","75efc782":"# II. Entrainer le mod\u00e8le\n\nEntrainement sur le mois de janvier 2018 pour ensuite tester le mod\u00e8le sur les prix du mois de f\u00e9vrier. Selectionner 1 mois (i.e. 44 640 minutes) pour entrainer le mod\u00e8le. D\u00e9but : 1er janvier 2018 \u00e0 00h00 \/ Fin : 31 janvier 2018 \u00e0 23h59","4943df28":"# IV. Predisons r\u00e9ellement les 15 prochaines minutes","25c36ebf":"## Pr\u00e9dire les prix du Bitcoin","cf1fc028":"Le mod\u00e8le semble fonctionner parfaitement sur l'ensemble de donn\u00e9es d'entra\u00eenement et tr\u00e8s bien sur l'ensemble de donn\u00e9es de test (correlation de 99%). Cependant, nous devons souligner que le mod\u00e8le pr\u00e9dit une seule observation \u00e0 l'avance. Cela signifie que pour chaque nouvelle observation qu'il doit pr\u00e9dire, il prend en entr\u00e9e les 14 minutes pr\u00e9c\u00e9dentes. Dans la vie r\u00e9elle, cela est un peu inutile puisque nous voulons principalement pr\u00e9dire de nombreuses observations \u00e0 l'avance.","a6bee8f3":"## Processing des donn\u00e9es","29f7237f":"# III. Verifier les performances du mod\u00e8le\n\nPour cela, gr\u00e2ce au mod\u00e8le, nous allons tester les pr\u00e9dictions sur le mois de fevrier 2018."}}