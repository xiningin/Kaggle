{"cell_type":{"2118e905":"code","f89c1c6e":"code","cf59d04e":"code","3ff78cde":"code","74318935":"code","d3349cdf":"code","6564c2c3":"code","95c2b93b":"code","2750f8de":"code","76f81472":"code","60525d8f":"code","2b84c741":"code","4dd950dd":"code","34c3c3e1":"code","77799a52":"code","add87884":"code","af65499f":"code","4b4534d3":"code","e23bc73f":"code","b8029114":"code","e4f316e8":"code","d9b3a663":"code","65c11768":"markdown","a4500861":"markdown","d215eacb":"markdown","6ee26cd8":"markdown","dcd39830":"markdown","fc308029":"markdown","e2613711":"markdown","c758c3c4":"markdown","0316bac2":"markdown","19c7a670":"markdown","77f4ed0c":"markdown","f84da641":"markdown","c18027c0":"markdown","3b9bf775":"markdown"},"source":{"2118e905":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f89c1c6e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","cf59d04e":"train = pd.read_csv(\"..\/input\/speed-dating-experiment\/Speed Dating Data.csv\", encoding = 'ISO-8859-1')","3ff78cde":"train.head()","74318935":"#Finding Fields with NULL values\ntrain.isnull().sum()","d3349cdf":"date = pd.concat([train.iloc[:, 0],train.iloc[:, 2],train.iloc[:,11:28],train.iloc[:,30:36],train.iloc[:,39:43],train.iloc[:,45:68],train.iloc[:,69:75],\n                  train.iloc[:,81:92],train.iloc[:,97:102],train.iloc[:,104:108]], axis=1)","6564c2c3":"#Removing null rows now that the nulls are in the hundreds and not the thousands.\ndate2 = date.dropna()","95c2b93b":"f, axes = plt.subplots(1,6,figsize=(15,6))\nf.subplots_adjust(hspace=0.4,wspace=0.7)\nattributes = ['attr1_1','sinc1_1','intel1_1','fun1_1','amb1_1','shar1_1']\nfor i in range(6):\n    sns.barplot(y=attributes[i], x= \"gender\", data=date2 , ax=axes[i])","2750f8de":"f, axes = plt.subplots(1,2,figsize=(10,5))\nsns.barplot(y=\"like_o\", x= \"gender\",data=date2,ax=axes[0])\nsns.barplot(y=\"dec_o\", x= \"gender\",data=date2,ax=axes[1])\n","76f81472":"f, axes = plt.subplots(figsize=(15,5))\nsns.barplot(y=\"dec_o\", x= \"age\",data=date2,ax=axes)","60525d8f":"fig, axes = plt.subplots(1,5,figsize=(20,5))\nx_attributes = ['attr3_1','sinc3_1','intel3_1','fun3_1','amb3_1']\ny_attributes = ['attr1_1','sinc1_1','intel1_1','fun1_1','amb1_1']\nfor i in range(5):\n    sns.regplot(x=x_attributes[i], y=y_attributes[i],data=date2, ax=axes[i])\n    ","2b84c741":"corrmat = date2.corr()\nplt.subplots(figsize=(20,40))\nsns.heatmap(corrmat, vmax=0.9, square=True)\n#print(corrmat)","4dd950dd":"fig, axes = plt.subplots(3,3,figsize=(20,10))\nattributes = ['pf_o_att','pf_o_sin','dec','pf_o_fun','like_o','dec','met','dec_o','met_o']\n\nfor i in range(3):\n    sns.regplot(y=\"match\", x=attributes[i],data=date2, ax=axes[0][i])\nfor i in range(3):\n    sns.regplot(y=\"match\", x=attributes[i+3],data=date2, ax=axes[1][i])\nfor i in range(3):\n    sns.regplot(y=\"match\", x=attributes[i+6],data=date2, ax=axes[2][i])\n    ","34c3c3e1":"#We drop the fields that have no correlation with the 'MATCH' field.\ndate3 = date2.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', \n                    'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music', \n                   'shopping', 'yoga'], axis=1)","77799a52":"#To find percentage of people who got matched.\npd.crosstab(index=date3['match'],columns=\"count\")","add87884":"no_love_count = len(date3[(date3['dec_o']==0) & (date3['dec']==1)]) \n+ len(date3[(date3['dec_o']==1) & (date3['dec']==0)])\nperc_broken_heart = no_love_count \/ len(date3.index)\nperc_broken_heart*100","af65499f":"#Taking only the important fields into consideration.\nY=date3['match']\nX=date3[['like','dec','met','met_o']]\n#X.drop(['match'],axis=1,inplace=True)\n","4b4534d3":"from sklearn.model_selection import GridSearchCV\ndef hyper_parameter_tuning(parameters,model,c_v):\n    grid_search = GridSearchCV(model,\n                               parameters,\n                               cv = c_v,\n                               n_jobs = 10,\n                               verbose = True)\n    grid_search.fit(X,Y)\n    #print(\"All Scores =\",grid_search.cv_results_)\n    print(\"Best Score =\",grid_search.best_score_)\n    print(\"Best Params =\",grid_search.best_params_)\n    return(grid_search.best_score_,grid_search.best_params_)","e23bc73f":"#Hyper-Parameter-Tuning for Logistic Regression.\nhyper_parameter_tuning({'C':[0.001,0.01,1,10],\n                        'max_iter':[100,200,5000],\n                        'random_state':[0,1,2,3]},LogisticRegression(),5)","b8029114":"#Hyper-Parameter-Tuning for XGBClassifier.\nhyper_parameter_tuning({'learning_rate':[0.01,0.1], \n                        'n_estimators':[140,200], \n                        'max_depth':[4,5,7],\n                        'min_child_weight':[2,3,4], \n                        'gamma':[0.2], \n                        'subsample':[0.6,0.8], \n                        'colsample_bytree':[0.7,1.0],\n                        'objective':['binary:logistic'], \n                        'seed':[27]},XGBClassifier(),5)","e4f316e8":"#Hyper-Parameter-Tuning for RandomForestClassifier.\nhyper_parameter_tuning({'bootstrap': [True, False],\n                        'max_depth': [40, 60, None],\n                        'max_features': ['auto', 'sqrt'],\n                        'min_samples_leaf': [1, 2],\n                        'min_samples_split': [2, 5],\n                        'n_estimators': [200, 600]},RandomForestClassifier(),5)","d9b3a663":"Xgboost = XGBClassifier(learning_rate =0.01, \n                        colsample_bytree=1, \n                        gamma=0.2, \n                        max_depth=4, \n                        min_child_weight=2, \n                        n_estimators=140, \n                        objective='binary:logistic',\n                        seed=27,\n                        subsample=0.8 )","65c11768":"Looks like Female Candidates are liked more by the opposite gender rather than men.","a4500861":"**Conclusion**\n1. Women have the advantage of being liked more by men, whereas it is tougher for me to be liked.\n2. Your impression of yourself is often wrong.\n3. There is no one trait that makes you likeable.\n4. More participants experienced one-sided love than those that found love.\n5. In the end what matters the most is both partner's decision. Other attributes have very less impact. ","d215eacb":"People in their early 20's were liked more than people from other age group.","6ee26cd8":"**Hyper-Parameter-Tuning and Model Creation**","dcd39830":"**EDA and Data Cleaning**","fc308029":"From the above plots we find that fields like 'dec' and 'like' affect the probability of getting matched a little bit.","e2613711":"From above we find that about 26% of the people liked their partner but they did not match in the end. That means 26% of the participants unfortunately had their heart broken. This percentage is more than people who got matched.","c758c3c4":"There are a ton of fields with NaNs. A lot of NaNs. There are 8,378 rows and a bunch of fields have thousands of NaNs and probably bad practice to use imputation to guess the values. I'll just drop fields with over 4000 null values from the dataset and narrow my analysis to the fields that I can use.","0316bac2":"I have used Grid Search for hyper-parameter-tuning which uses K-Fold Cross validation technique to perform Cross Validation.","19c7a670":"Looks like only 20% of the people found love.","77f4ed0c":"Above plot shows that most people have a wrong a impression of themselves.","f84da641":"This accuracy is plausible considering 26% people had their heart broken.","c18027c0":"From the above heatmap following are the observations:\n    1. No field has direct correlation with the **Match** field.\n    2. Fields like 'met_o' and 'met' are a little bit related.","3b9bf775":"From above it looks like males look for attraction more than females who prefer ambitious and sincere attribute more."}}