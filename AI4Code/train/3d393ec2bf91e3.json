{"cell_type":{"57d7ecf2":"code","2ed5da85":"code","c05ff6ce":"code","e7b8fee6":"code","a3a7f38b":"code","936dab06":"code","56bb666f":"code","67b0e7ef":"code","f9329fa1":"code","89c369de":"code","48f033e6":"code","9624177b":"code","87ebed17":"code","1ccdd3e6":"code","c24f3835":"code","49d174a4":"code","eef69c6e":"code","ff9ca1d3":"code","ced389e5":"code","7f28c597":"code","688d4fca":"code","0fc7493c":"code","3aee4877":"code","61f45b9e":"code","3578d3f2":"code","83a976a8":"code","1275c7be":"code","744b863b":"code","6847d990":"code","d11670b7":"code","4d17e460":"code","4494fff1":"code","c450ba93":"code","cb798e77":"code","1bf4717e":"code","f93ab687":"code","7d012b94":"code","75ebffdc":"code","2ca96934":"code","ea6bc940":"code","452817d3":"code","4a05d1c5":"code","cc9cdf81":"code","fd465d3a":"code","22297b1c":"code","50047ca1":"code","003ac47b":"code","e26c0466":"code","aa2b6108":"code","ad51c90b":"code","97b2a131":"code","275cdfd2":"code","6933d431":"code","08836e3a":"code","3caa455f":"code","bcebab55":"code","2073801b":"code","f62bce5b":"code","dbaf202d":"code","ed1a67df":"code","f7df28b2":"code","232cfa43":"code","ad1aec80":"code","eb87c60d":"code","78f5f1f4":"code","682efd53":"code","2429f5f5":"code","fe32a1fd":"code","f6fc6d31":"code","cc0b5261":"markdown","1cbeccc5":"markdown","548ca39d":"markdown","30f54afb":"markdown","4b131c9a":"markdown","fb88cbe6":"markdown","b750c557":"markdown","dea5285d":"markdown","4914fc69":"markdown","d0deef0b":"markdown","ad3be190":"markdown","bee70827":"markdown","92af0fe6":"markdown","ed625e70":"markdown","cc661a41":"markdown","4184a737":"markdown","5c1ce8a7":"markdown","00d37107":"markdown","4594c841":"markdown","80142f07":"markdown","f52e4aac":"markdown","ce504c1a":"markdown","d0877548":"markdown","f20a8473":"markdown","8be89684":"markdown","0feab279":"markdown","dacb5d50":"markdown","d0fc76a2":"markdown","75fd0c79":"markdown","de5d4148":"markdown","45654525":"markdown","987994c1":"markdown","446780c2":"markdown","c17d747d":"markdown","f9cf21a5":"markdown","c7b347b7":"markdown","5a28d4ad":"markdown","43945e84":"markdown","9f9e2652":"markdown","8bfa5f4f":"markdown","70251970":"markdown","6aa91d39":"markdown","0f0b5bf7":"markdown","5fe91a36":"markdown","d1585067":"markdown","d908dbac":"markdown","b52afbd0":"markdown","0bcaff2e":"markdown","fb8537f6":"markdown","228ca035":"markdown","4ce6838b":"markdown","7b96d4ca":"markdown","3ebfdb01":"markdown","a7525178":"markdown","7319cc8e":"markdown","55ab6638":"markdown","2ad6ff65":"markdown","e4590bed":"markdown","e9c3944a":"markdown","28be511c":"markdown","e11d5061":"markdown","8e95f6fc":"markdown","575ecfb4":"markdown","d68a0719":"markdown","0229b99f":"markdown","6d874f53":"markdown","9b944be8":"markdown","47d46079":"markdown","997640b3":"markdown","f4a3275b":"markdown","5d5d45c1":"markdown","2c15fcd2":"markdown","dd66941c":"markdown"},"source":{"57d7ecf2":"#import numpy,matlplotlib,etc.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\n\nfrom IPython.display import display, Markdown\nimport math\nimport matplotlib.pyplot as plt\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nimport plotly.express as px\nimport seaborn as sns \n# from dataprep.eda import create_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2ed5da85":"#for AutoComplete\n%config Completer.use_jedi = False\n","c05ff6ce":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_data\n","e7b8fee6":"train_data.keys()","a3a7f38b":"print(train_data['Survived'])","936dab06":"train_data.describe()","56bb666f":"display(train_data['Name'])\ndisplay(train_data['Ticket'])","67b0e7ef":"train_data.drop(columns=['Name','Ticket'],axis=1,inplace=True)\ntest_df.drop(columns=['Name','Ticket'],axis=1,inplace=True)","f9329fa1":"duplicate_data = train_data.duplicated()\nprint(duplicate_data.sum())\n\nduplicate_data = test_df.duplicated()\nprint(duplicate_data.sum())","89c369de":"display(train_data.isna().any()) #say if there is nan in x column,(any do OR)\ndisplay(test_df.isna().any())","48f033e6":"# get the number of missing data points per column\nmissing_values_count = train_data.isnull().sum()\n\n# look at the # of missing points in the first ten columns\nmissing_values_count[0:12]","9624177b":"#AVG of # NaN \nnan_counts = train_data.isna().sum().sort_values(ascending = False)\/len(train_data)\n#Setting size of plot \nplt.figure(figsize=(16,8))\nplt.xticks(np.arange(len(nan_counts)),nan_counts.index,rotation='vertical')\nplt.ylabel('fraction of rows with missing data')\n#show nan values at bar-plot\nplt.bar(np.arange(len(nan_counts)),nan_counts)\n\n","87ebed17":"total_cells = np.product(train_data.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing\/total_cells) * 100\nprint(f'NaN data : {percent_missing} %')","1ccdd3e6":"def fill_random_value_imputation(feature,data):\n    sample=data[feature].dropna().sample(data[feature].isnull().sum())               \n    sample.index=data[data[feature].isnull()].index\n    data.loc[data[feature].isnull(),feature]=sample","c24f3835":"#-----------------------------------------Train data----------------------------------------------------\ntrain_data.drop(['Cabin'],axis=1,inplace = True)\ntrain_data.head()\nfor col in train_data:\n    fill_random_value_imputation(col,train_data)\nnull_info = pd.DataFrame({'NaN Values' : train_data.isna().sum().sort_values(ascending=False), 'Percentage NaN Values' : (train_data.isna().sum().sort_values(ascending=False)) \/ (train_data.shape[0]) * (100)})\ndisplay(null_info)\ndisplay(train_data.isna().any()) \n\n#AVG of # NaN \nnan_counts = train_data.isna().sum().sort_values(ascending = False)\/len(train_data)\n#Setting size of plot \nplt.figure(figsize=(16,8))\nplt.xticks(np.arange(len(nan_counts))+0.5,nan_counts.index,rotation='vertical')\nplt.ylabel('fraction of rows with missing data')\n#show nan values at bar-plot\nplt.bar(np.arange(len(nan_counts)),nan_counts)","49d174a4":"#-----------------------------------------Test data----------------------------------------------------\ntest_df.drop(['Cabin'],axis=1,inplace = True)\ntest_df.head()\nfor col in test_df:\n    fill_random_value_imputation(col,test_df)\nnull_info = pd.DataFrame({'NaN Values' : test_df.isna().sum().sort_values(ascending=False), 'Percentage NaN Values' : (test_df.isna().sum().sort_values(ascending=False)) \/ (test_df.shape[0]) * (100)})\ndisplay(null_info)\ndisplay(test_df.isna().any())","eef69c6e":"print(f'train.dtypes:\\n {train_data.dtypes}\\n\\ntest.dtypes:\\n {test_df.dtypes}')","ff9ca1d3":"categorial_list = (train_data.dtypes == 'object')\nobject_cols = list(categorial_list[categorial_list].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","ced389e5":"#-----------------------------------------------------------------Train data-------------------------------------------------------------\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder( sparse=False,drop = 'first')\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_data[object_cols]),columns = OH_encoder.get_feature_names())\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = train_data.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_train_data = train_data.drop(object_cols, axis=1)\nOH_cols_train = OH_cols_train.set_axis(['Sex', 'Embarked_Q', 'Embarked_S'], axis=1)\n# Add one-hot encoded columns to numerical features\nOH_train_data = pd.concat([num_train_data, OH_cols_train], axis=1)\nOH_train_data\n","7f28c597":"#-----------------------------------------------------------------Test data-------------------------------------------------------------\nOH_cols_test = pd.DataFrame(OH_encoder.fit_transform(test_df[object_cols]),columns = OH_encoder.get_feature_names())\n\n# One-hot encoding removed index; put it back\nOH_cols_test.index = test_df.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_test_data = test_df.drop(object_cols, axis=1)\nOH_cols_test = OH_cols_test.set_axis(['Sex', 'Embarked_Q', 'Embarked_S'], axis=1)\n# Add one-hot encoded columns to numerical features\nOH_test_data = pd.concat([num_test_data, OH_cols_test], axis=1)\nOH_test_data\n","688d4fca":"# get color map\ndef get_sns_cmap(n, name='muted'):\n    return sns.color_palette(palette=name, n_colors=n)\n\n# plot with regression line target values by each feature\ndef plot_reg_target_values_by_each_feature(df, target_column_name):\n    nrows = math.ceil(math.sqrt(len(df.columns)-1))\n    ncols = math.ceil((len(df.columns)-1)\/nrows)\n    fig, axes = plt.subplots(nrows, ncols)\n    plt.subplots_adjust(top=3.2, bottom=0, left=0, right=2.5)\n    colors = get_sns_cmap(len(df.columns))\n  \n    for i in range(len(df.columns)-1):\n        ax = sns.regplot(x=df.columns[i], y=target_column_name, data=df, color=colors[i], ax=axes[i\/\/nrows, i%nrows], scatter_kws={\"s\": 40})\n        ax.set_title(label=f'{df.columns[i]} by {target_column_name}', fontsize=10)\n   \n    for i in range(len(df.columns)-1, nrows*ncols):\n        fig.delaxes(axes.flatten()[i])\n\nplot_reg_target_values_by_each_feature(OH_train_data, 'Survived')\n","0fc7493c":"plt.subplots(figsize=(22, 5))\nsns.histplot(data=OH_train_data[\"Age\"],discrete = True,binwidth=15)\nplt.show()\n# age_range_bins= [0,3,16,30,45,90]\nage_range_bins= [0,11,18,27,33,40,66,90]\n# 0-11: Children, 11-18:\n# labels = ['Infant','Children','Young Adults','Middle-Aged Adults','Old Adults']\nlabels = ['Children','Teens','Young Adults','Later Young Adulys','Middle-Aged Adults','Adults','Old Adults']\n\n\nOH_train_data['AgeGroup'] = pd.cut(OH_train_data['Age'], bins=age_range_bins, labels=labels, right=False)\ndisplay(OH_train_data)\nplt.subplots(figsize=(15, 5))\nax = sns.histplot(data=OH_train_data[\"AgeGroup\"],discrete = True,binwidth=5)\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n","3aee4877":"#-----------------------------------------------------------------Train data-------------------------------------------------------------\n# define ordinal encoding\nencoder = OrdinalEncoder()\n# transform data\nresult = pd.DataFrame(encoder.fit_transform(OH_train_data[['AgeGroup']]))\nresult.rename(columns={0: \"AgeGroupEncoding\"},inplace=True)\nresult.index = OH_train_data.index\nOH_train_data = pd.concat([OH_train_data, result], axis=1)\n\ndisplay(OH_train_data)\nax = sns.histplot(data=OH_train_data[\"AgeGroupEncoding\"],discrete = True,binwidth=5)\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\nOH_train_data[['Age','AgeGroup',\t'AgeGroupEncoding']].loc[7]","61f45b9e":"#-----------------------------------------------------------------Test data-------------------------------------------------------------\nOH_test_data['AgeGroup'] = pd.cut(OH_test_data['Age'], bins=age_range_bins, labels=labels, right=False)\n# transform data\nresult = pd.DataFrame(encoder.fit_transform(OH_test_data[['AgeGroup']]))\nresult.rename(columns={0: \"AgeGroupEncoding\"},inplace=True)\nresult.index = OH_test_data.index\nOH_test_data = pd.concat([OH_test_data, result], axis=1)\ndisplay(OH_test_data)","3578d3f2":"#-----------------------------------------------------------------Train data-------------------------------------------------------------\nplt.subplots(figsize=(22, 5))\nax = sns.histplot(data=OH_train_data, x=\"Fare\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nax = sns.countplot(data=OH_train_data, x=\"Fare\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\n#create rang fare\nfare_range_bins = pd.qcut(OH_train_data['Fare'], q=5, retbins=True)[1]\nprint(fare_range_bins)\nplt.subplots(figsize=(22, 5))\nOH_train_data[\"Fare_new\"] = pd.cut(OH_train_data['Fare'], bins=fare_range_bins, include_lowest=True)\n\n#ordinal Encoding fare \nresult = pd.DataFrame(encoder.fit_transform(OH_train_data[['Fare_new']]))\nresult.rename(columns={0: \"FareEncoding\"},inplace=True)\nresult.index = OH_train_data.index\nOH_train_data = pd.concat([OH_train_data, result], axis=1)\ndisplay(OH_train_data)\n\n\nax = sns.countplot(data=OH_train_data, x=\"FareEncoding\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()","83a976a8":"#-----------------------------------------------------------------Test data-------------------------------------------------------------\n#create range fare according train set\nOH_test_data[\"Fare_new\"] = pd.cut(OH_test_data['Fare'], bins=fare_range_bins, include_lowest=True)\n\n#ordinal Encoding fare \nresult = pd.DataFrame(encoder.fit_transform(OH_test_data[['Fare_new']]))\nresult.rename(columns={0: \"FareEncoding\"},inplace=True)\nresult.index = OH_test_data.index\nOH_test_data = pd.concat([OH_test_data, result], axis=1)\ndisplay(OH_test_data)\n","1275c7be":"# Set the figure size\nplt.figure(figsize=(14, 8))\ntrain_data_cp = train_data.copy()\ntrain_data_cp.drop(['Survived'],axis = 1,inplace = True)\ntarget_data_cp = train_data['Survived'].copy()\n# plot a bar chart\nax = sns.barplot(x=target_data_cp, y=target_data_cp.value_counts(), data=train_data_cp, estimator=np.mean, capsize=.2,palette=\"Greens\")\nfor container in ax.containers:\n    ax.bar_label(container)\n","744b863b":"# get color map\ndef get_cmap(n, name='hsv'):\n    return plt.cm.get_cmap(name, n)\n\n# plot target values by each feature\ndef plot_target_values_by_each_feature(df, target_column,target_column_name):\n    nrows = math.ceil(math.sqrt(len(df.columns)-1))\n    ncols = math.ceil((len(df.columns)-1)\/nrows)\n    plt.style.use('seaborn')\n    fig, axes = plt.subplots(nrows, ncols)\n    plt.subplots_adjust(top=3, bottom=0, left=0, right=2.5)\n    colors = get_cmap(len(df.columns))\n\n    for i in range(len(df.columns)-1):\n        ax = sns.barplot(x=target_column, y=df.columns[i],data = df,  ax=axes[i\/\/nrows, i%nrows],palette=\"Greens\")\n        ax.set_title(f'{df.columns[i]} by {target_column_name}');\n        for container in ax.containers:\n            ax.bar_label(container)\n        axes[i\/\/nrows, i%nrows].tick_params(axis='both', labelsize=10)\n        axes[i\/\/nrows, i%nrows].xaxis.label.set_size(10)\n        axes[i\/\/nrows, i%nrows].yaxis.label.set_size(10)\n        axes[i\/\/nrows, i%nrows].title.set_fontsize(10)\n\n    for i in range(len(df.columns)-1, nrows*ncols): \n        fig.delaxes(axes.flatten()[i]) # Flattening so we can access axes array as a 1-d array to delete unused axes objects\n\nplot_target_values_by_each_feature(train_data, target_data_cp,'Survived')\n","6847d990":"corrMatrix = OH_train_data.corr()\n\nplt.subplots(figsize=(22, 10))\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","d11670b7":"#------------------------------------------------------------------------PCLASS--------------------------------------------------------------------------------\n# Probability of survival from each class\nplt.subplots(figsize=(13, 5))\nax = sns.barplot(data=train_data, x='Pclass', y='Survived')\nax.set_title(f'Pclass by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n#show amount of men vs women in pclass\nax = sns.catplot(data=OH_train_data, x='Pclass',  col=\"Survived\",kind = 'count',hue = 'Sex',palette=\"muted\", height=4, aspect=2)\nplt.show()\n#Concentrations of ages in class\nax = sns.histplot(data=OH_train_data, x=\"AgeGroup\", hue=\"Pclass\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n#expect - Expensive price = first class\nax = sns.barplot(data=OH_train_data, x=\"Pclass\",y=\"Fare\", hue=\"Survived\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()","4d17e460":"#------------------------------------------------------------------------Gender--------------------------------------------------------------------------------\nplt.subplots(figsize=(13, 5))\nax = sns.barplot(data=train_data, x='Sex', y='Survived')\nax.set_title(f'Sex by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()","4494fff1":"ax = sns.barplot(data=OH_train_data, x='AgeGroup', y='Survived')\nax.set_title(f'age group by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nax = sns.catplot(data=OH_train_data, x='AgeGroup',  col=\"Survived\",kind = \"count\",hue = 'Sex', height=4, aspect=2)\nplt.show()","c450ba93":"plt.subplots(figsize=(13, 5))\nax = sns.barplot(data=train_data, x='SibSp', y='Survived')\nax.set_title(f'SibSp by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nplt.subplots(figsize=(13, 5))\nax = sns.barplot(data=train_data, x='Parch', y='Survived')\nax.set_title(f'Parch by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()","cb798e77":"OH_test_data['Family_Size'] = OH_test_data['Parch'] + OH_test_data['SibSp']\nOH_train_data['Family_Size'] = OH_train_data['Parch'] + OH_train_data['SibSp']\n\ndisplay(OH_train_data)\ncorrMatrix = OH_train_data.corr()\n\nplt.subplots(figsize=(22, 10))\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\n\n#Exploratory data analysis\nplt.subplots(figsize=(13, 5))\nax = sns.barplot(data=OH_train_data, x='Family_Size', y='Survived')\nax.set_title(f'Family_Size by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nax = sns.catplot(data=OH_train_data, x='Family_Size',  col=\"Survived\",kind = \"count\",hue = 'Sex', height=4, aspect=2)\nplt.show()\n","1bf4717e":"OH_train_data['Lonely_passenger'] = OH_train_data['Family_Size'] ==0\ndisplay(OH_train_data)\ncorrMatrix = OH_train_data.corr()\nplt.subplots(figsize=(22, 10))\nsns.heatmap(corrMatrix, annot=True)\nplt.show()\nax = sns.catplot(x=\"Survived\", col=\"Lonely_passenger\",\n                data=OH_train_data, kind=\"count\")\nplt.show\n","f93ab687":"plt.subplots(figsize=(13, 5))\nax = sns.barplot(data=train_data, x='Embarked', y='Survived')\nax.set_title(f'Embarked by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nax = sns.catplot(data=train_data, x='Embarked',  col=\"Survived\",kind = \"count\",hue = 'Sex', height=4, aspect=2)\nplt.show()\n\nax = sns.countplot(data=train_data, x=\"Embarked\", hue=\"Sex\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\nax = sns.countplot(data=train_data, x=\"Embarked\", hue=\"Survived\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\nax = sns.countplot(data=train_data, x=\"Embarked\",hue = \"Pclass\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\nax = sns.catplot(data=train_data, x=\"Embarked\",col=\"Survived\",kind = \"count\",hue = \"Pclass\")\n\nplt.show()\n","7d012b94":"plt.subplots(figsize=(13, 5))\nax = sns.barplot(data=OH_train_data, x='Fare_new', y='Survived')\nax.set_title(f'Fare_new by Survived');\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nax = sns.catplot(data=OH_train_data, x='FareEncoding',  col=\"Survived\",kind = \"count\",hue = 'Sex', height=4, aspect=2)\nplt.show()\n\nax = sns.countplot(data=OH_train_data, x=\"FareEncoding\", hue=\"Sex\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\n\nax = sns.countplot(data=OH_train_data, x=\"FareEncoding\",hue = \"Pclass\")\nfor container in ax.containers:\n    ax.bar_label(container)\nplt.show()\nax = sns.catplot(data=OH_train_data, x=\"FareEncoding\",col=\"Survived\",kind = \"count\",hue = \"Pclass\")\n\nplt.show()\n\n","75ebffdc":"display(OH_train_data)\ndisplay(OH_test_data)","2ca96934":"OH_train_data.drop(columns=['PassengerId','AgeGroup','Lonely_passenger'],axis=1,inplace=True)\ndisplay(OH_train_data)\n\ntest_df_PassengerId = test_df['PassengerId']\nOH_test_data.drop(columns=['PassengerId','AgeGroup'],axis=1,inplace=True)\ndisplay(OH_test_data)","ea6bc940":"# Removal for linear model\n# X = OH_train_data.drop(columns=['SibSp','Parch','AgeGroupEncoding','Pclass','Embarked_Q','Embarked_S','Fare_new','FareEncoding'])\n# display(X)\n\n#option 1\n#Removal for logistics model\n# X = OH_train_data.drop(columns=['SibSp','Parch','Age','Fare','Fare_new','Embarked_Q','Embarked_S','Family_Size','FareEncoding'])\n# display(X)\n# X_test = OH_test_data.drop(columns=['SibSp','Parch','Age','Embarked_Q','Embarked_S','Fare','Fare_new','Family_Size','FareEncoding'])\n# display(X_test)\n\n#option 2\n#Removal for logistics model- score = 79.8 (0.35 for valid)\n# X = OH_train_data.drop(columns=['SibSp','Parch','Age','Fare','Fare_new','FareEncoding'])\n# X_test = OH_test_data.drop(columns=['SibSp','Parch','Age','Fare','Fare_new','FareEncoding'])\n# display(X)\n\n#option 2\n#Removal for logistics model- score = 79.8 (0.35 for valid)\n# saving the dataframes-befor drastic changes\ntrain_df_cpy, test_df_cpy = OH_train_data.copy(), OH_test_data.copy()\n\nX = OH_train_data.drop(columns=['SibSp','Parch','Age','Fare','Fare_new','FareEncoding'])\nX_test = OH_test_data.drop(columns=['SibSp','Parch','Age','Fare','Fare_new','FareEncoding'])\ndisplay(X)","452817d3":"t = OH_train_data['Survived'].copy()\nX = X.drop(['Survived'],axis = 1)","4a05d1c5":"# print 4 graphs: mse of train\/test and r2 of train\/test\ndef print_graphs_r2_mse(graph_points):\n    for k, v in graph_points.items():\n        best_value = max(v.values()) if 'R2' in k else min(v.values())\n        best_index = np.argmax(list(v.values())) if 'R2' in k else np.argmin(list(v.values()))\n        color = 'red' if 'train' in k else 'blue'\n        if('R2' in k):\n            fig = px.scatter(x=v.keys(), y=v.values(), title=f'{k}, best value: x={best_index + 1}, y={best_value}', color_discrete_sequence=[color], labels={\n                    'x': \"Data distribution values\",\n                    'y': \"Score\"\n                    \n                 })\n        else:\n            fig = px.scatter(x=v.keys(), y=v.values(), title=f'{k}, best value: x={best_index + 1}, y={best_value}', color_discrete_sequence=[color], labels={\n                    'x': \"Data distribution values\",\n                    'y': \"MSE\"\n                    \n                 })\n        fig.data[0].update(mode='markers+lines')\n        fig.show()\n        \n# plot the score by split and the loss by split\ndef plot_score_and_loss_by_split(X, t):\n    graph_points = {\n                    'train_MSE':{},\n                    'val_MSE': {},\n                    'train_R2': {},\n                    'val_R2': {}\n                    }\n    for size in range(10, 100, 10):\n        X_train, X_val, t_train, t_val = model_selection.train_test_split(X, t, test_size=size\/100, random_state=42)\n        NE_reg = linear_model.LogisticRegression().fit(X_train, t_train)\n        y_train = NE_reg.predict(X_train)\n        y_val = NE_reg.predict(X_val)\n        graph_points['train_MSE'][size\/100] = metrics.mean_squared_error(t_train, y_train)\n        graph_points['val_MSE'][size\/100] = metrics.mean_squared_error(t_val, y_val)\n        graph_points['train_R2'][size\/100] = NE_reg.score(X_train, t_train)\n        graph_points['val_R2'][size\/100] = NE_reg.score(X_val, t_val)\n    print_graphs_r2_mse(graph_points)\n\nplot_score_and_loss_by_split(X, t)","cc9cdf81":"def fit_and_eval_ne(X_train, X_val, t_train, t_val):\n  NE_reg = linear_model.LinearRegression().fit(X_train, t_train)\n  y_train = NE_reg.predict(X_train)\n  y_val = NE_reg.predict(X_val)\n  print(\"MSE Train:\", metrics.mean_squared_error(t_train, y_train))\n  print(\"MSE Valid:\", metrics.mean_squared_error(t_val, y_val))\n  print(\"R^2 Train:\", NE_reg.score(X_train, t_train))\n  print(\"R^2 Valid:\", NE_reg.score(X_val, t_val))","fd465d3a":"X_train, X_val, t_train, t_val = model_selection.train_test_split(X, t, test_size=0.35, random_state=42)\nfit_and_eval_ne(X_train, X_val, t_train, t_val)","22297b1c":"print(f'X_train:{X_train.index}\\nt_train:{t_train.index}\\nX_val:{X_val.index}\\nt_val:{t_val.index}')\n","50047ca1":"def predictFunc(model,x_train,y_train,x_test,y_test):          \n    model.fit(x_train, y_train)\n    \n    # Make predictions\n    predictions = model.predict(x_test)\n    \n     # Compute accuracy\n    print(f'Model Accuracy: {accuracy_score(predictions,y_test)*100}%')\n    print('\\n')\n    \nlog_reg = LogisticRegression()\npredictFunc(log_reg,X_train,t_train,X_val,t_val)","003ac47b":"#with solver 'sgd', the accuracy = 79.48718% + convergenceWarnning\n#with solver 'lbfgs' tried to do another encoding- did not work\n#with solver adam, the accuracy = 80.1282% + convergenceWarnning\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(solver='adam', alpha=1e-5,\n                     hidden_layer_sizes=(50,50), random_state=1)\nmlp.fit(X_train,t_train)\npredictFunc(mlp,X_train,t_train,X_val,t_val)","e26c0466":"display(test_df)\ndisplay(OH_test_data)\ndisplay(X_test)","aa2b6108":"log_reg.fit(X, t)\npred_test_logistic_regression = log_reg.predict(X_test).copy()\n\nsubmission_log_reg = pd.DataFrame({\n        'PassengerId': test_df_PassengerId,\n        'Survived': pred_test_logistic_regression\n    })\nsubmission_log_reg.Survived = submission_log_reg.Survived.astype(int)\nprint(submission_log_reg.shape)\ndisplay(submission_log_reg)\n\n# submission_log_reg.to_csv('submission.csv', index=False)","ad51c90b":"mlp.fit(X, t)\npred_test_mlp = mlp.predict(X_test).copy()\n\nsubmission_mlp = pd.DataFrame({\n        'PassengerId': test_df_PassengerId,\n        'Survived': pred_test_mlp\n    })\nsubmission_mlp.Survived = submission_mlp.Survived.astype(int)\nprint(submission_mlp.shape)\ndisplay(submission_mlp)\n\n# submission_mlp.to_csv('submission.csv', index=False)","97b2a131":"from tqdm.auto import tqdm\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import RepeatedKFold\nimport plotly.graph_objects as go\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","275cdfd2":"display(train_df_cpy)\nt = train_df_cpy['Survived'].copy()\nx = train_df_cpy.drop(['Survived','Fare_new'],axis = 1).copy()\ndisplay(x)\nx.dtypes\n#test_df_cpy ","6933d431":"def find_generator_len(generator, use_pbar=True):\n    i = 0\n    \n    if use_pbar:\n        pbar = tqdm(desc='Calculating Length', ncols=1000, bar_format='{desc}{bar:10}{r_bar}')\n\n    for a in generator:\n        i += 1\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    return i\n# calculate score and loss from cv (KFold or LPO) and display graphs\ndef get_cv_score_and_loss(X, t, model, transformer=None, k=None, p=None, show_score_loss_graphs=False, use_pbar=True):\n    scores_losses_df = pd.DataFrame(columns=['fold_id', 'split', 'score', 'loss'])\n\n    if k is not None:\n        cv = KFold(n_splits=k, shuffle=True, random_state=1)\n    elif p is not None:\n        cv = LeavePOut(p)\n    else:\n        raise ValueError('you need to specify k or p in order for the cv to work')\n\n    if use_pbar:\n        pbar = tqdm(desc='Computing Models', total=find_generator_len(cv.split(X)))\n\n    for i, (train_ids, val_ids) in enumerate(cv.split(X)):\n        X_train = X.loc[train_ids]\n        t_train = t.loc[train_ids]\n        X_val = X.loc[val_ids]\n        t_val = t.loc[val_ids]\n\n        model.fit(X_train, t_train)\n\n        y_train = model.predict(X_train)\n        y_val = model.predict(X_val)\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'train', model.score(X_train, t_train), mean_squared_error(t_train, y_train)]\n        scores_losses_df.loc[len(scores_losses_df)] = [i, 'val', model.score(X_val, t_val), mean_squared_error(t_val, y_val)]\n\n        if use_pbar:\n            pbar.update()\n\n    if use_pbar:\n        pbar.close()\n\n    val_scores_losses_df = scores_losses_df[scores_losses_df['split']=='val']\n    train_scores_losses_df = scores_losses_df[scores_losses_df['split']=='train']\n\n    mean_val_score = val_scores_losses_df['score'].mean()\n    mean_val_loss = val_scores_losses_df['loss'].mean()\n    mean_train_score = train_scores_losses_df['score'].mean()\n    mean_train_loss = train_scores_losses_df['loss'].mean()\n\n    if show_score_loss_graphs:\n        fig = px.line(scores_losses_df, x='fold_id', y='score', color='split', title=f'Mean Val Score: {mean_val_score:.2f}, Mean Train Score: {mean_train_score:.2f}')\n        fig.show()\n        fig = px.line(scores_losses_df, x='fold_id', y='loss', color='split', title=f'Mean Val Loss: {mean_val_loss:.2f}, Mean Train Loss: {mean_train_loss:.2f}')\n        fig.show()\n\n    return mean_val_score, mean_val_loss, mean_train_score, mean_train_loss","08836e3a":"categorical_cols = x.select_dtypes(include=['object', 'bool']).columns\nnumerical_cols = x.select_dtypes(include=['int64', 'float64']).columns\nct = ColumnTransformer([\n    (\"encoding_cat\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_cols),\n    (\"standard_poly\", make_pipeline(PolynomialFeatures(degree=2), StandardScaler()), numerical_cols)],\n    remainder='passthrough')\nmodel_pipe = make_pipeline(ct,  SGDClassifier(loss='log', random_state=2))\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(x, t, model_pipe, transformer=ct, k=10,\n                                                                     show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv val score: {train_score:.2f}\\nmean cv val loss {train_loss:.2f}')","3caa455f":"# train with grid search and get best parameters\n\nhyper_parameters = { 'penalty': ('l1', 'l2', 'elasticnet'),\n                    'alpha': [0.1, 0.01, 0.001, 0.0001],\n                    'learning_rate': ('constant', 'optimal', 'invscaling', 'adaptive'),\n                    'eta0': [0.1, 0.01, 0.001, 0.0001]}\nX_ct = pd.DataFrame(ct.fit_transform(x))\n#For later use in RandomizedSearchCV\nx_ct2 = X_ct.copy()\ngs_model = GridSearchCV(SGDClassifier(loss='log', random_state=2), hyper_parameters).fit(X_ct, t)\nprint('grid search score:', gs_model.best_score_)\nprint('grid search params:', gs_model.best_params_)\ngs_params = gs_model.best_params_","bcebab55":"# train with random search and get best parameters\n\nnp.random.seed(1)\ndistributions = dict(alpha=uniform(loc=0, scale=1), penalty=['l2', 'l1', 'elasticnet'])\n\nrs_model = RandomizedSearchCV(SGDClassifier(), distributions, random_state=1).fit(x_ct2, t)\nprint('Accuracy score for classification:')\nprint('rs_model', rs_model.best_score_)\nprint('best params', rs_model.best_params_)\nrs_params = rs_model.best_params_","2073801b":"\nsgd_C = SGDClassifier(random_state=1 )\nsgd_C = sgd_C.set_params(**gs_params)\nselector = RFECV(sgd_C, cv=RepeatedKFold(n_splits=10, n_repeats=10, random_state=1)).fit(X_ct, t)\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=[i for i in range(1, len(selector.grid_scores_) + 1)], y=selector.grid_scores_))\nfig.update_xaxes(title_text=\"Number of features\")\nfig.update_yaxes(title_text=\"Cross validation score (nb of correct classifications)\")\nfig.show()\n#support_ = ndarray of shape (n_features,) The mask of selected features.\ndisplay(X_ct.loc[:, selector.support_].keys())\nprint(f'#features: {(len(X_ct.loc[:, selector.support_].keys()))}')","f62bce5b":"x_best = X_ct.loc[:, selector.support_]\nsgd_C = SGDClassifier(random_state=1 )\nsgd_C = sgd_C.set_params(**gs_params)\nmodel_pipe = make_pipeline(sgd_C)\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(x_best, t, model_pipe, k=10, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv val score: {train_score:.2f}\\nmean cv val loss {train_loss:.2f}')\n\nsgd_C = SGDClassifier(random_state=1 )\nsgd_C = sgd_C.set_params(**gs_params)\nmodel_pipe = make_pipeline(sgd_C)\nval_score, val_loss, train_score, train_loss = get_cv_score_and_loss(X_ct, t, model_pipe, k=10, show_score_loss_graphs=True)\nprint(f'mean cv val score: {val_score:.2f}\\nmean cv val loss {val_loss:.2f}')\nprint(f'mean cv train score: {train_score:.2f}\\nmean cv train loss {train_loss:.2f}')","dbaf202d":"from sklearn.model_selection import cross_val_score\n# print lasso, ridge and elasticnet scores as classification \nsgd_lasso_cls =SGDClassifier(penalty='l1', random_state=1)\nsgd_ridge_cls = SGDClassifier(penalty='l2', random_state=1)\nsgd_elastic_cls = SGDClassifier(penalty='elasticnet', random_state=1)\n\nprint(\"Accuracy score for classification:\")\nprint('sgd_lasso', cross_val_score(make_pipeline(StandardScaler(), sgd_lasso_cls), x_best, t, cv=15).mean())\nprint('sgd_ridge', cross_val_score(make_pipeline(StandardScaler(), sgd_ridge_cls), x_best, t, cv=15).mean())\nprint('sgd_elastic', cross_val_score(make_pipeline(StandardScaler(), sgd_elastic_cls), x_best, t, cv=15).mean())","ed1a67df":"def show_accuracy_by_ensembel(X_train, t_train, X_val, t_val, ensembels):\n    accuracy_df = pd.DataFrame(columns=['ensembel_name', 'split', 'accuracy'])\n\n    for ensembel_name, ensembel_model in ensembels.items():\n#         model_pipe = make_pipeline(ensembel, SGDClassifier(random_state=1))\n#         model_pipe.fit(X_train, t_train)\n        accuracy_df.loc[len(accuracy_df)] = [ensembel_name, 'train', ensembel_model.score(X_train, t_train)]\n        accuracy_df.loc[len(accuracy_df)] = [ensembel_name, 'val', ensembel_model.score(X_val, t_val)]\n\n    fig = px.bar(accuracy_df, x='ensembel_name', y='accuracy', color='split', barmode='group')\n    fig.show()","f7df28b2":"\nhyper_parameters = {'n_estimators': [x for x in range(50, 501, 50)]}\nsgd_C = SGDClassifier(random_state=1 )\nsgd_C = sgd_C.set_params(**gs_params)\n\nbag_fold_gs = GridSearchCV(BaggingClassifier(base_estimator=sgd_C,random_state=1, bootstrap=False), hyper_parameters).fit(x_best, t)\nbag_boot_gs = GridSearchCV(BaggingClassifier(base_estimator=sgd_C,random_state=1, bootstrap=True), hyper_parameters).fit(x_best, t)\nada_boost_gs = GridSearchCV(AdaBoostClassifier(random_state=1), hyper_parameters).fit(x_best, t)","232cfa43":"# get score with nfold bagging\nbag_fold_model = BaggingClassifier(**bag_fold_gs.best_params_,base_estimator=SGDClassifier(), random_state=1, bootstrap=False).fit(x_best, t)\nprint('Accuracy score for classification:')\nprint('bag_fold_model', bag_fold_model.score(x_best, t).mean())","ad1aec80":"# get score with bootstrap bagging\nbag_boot_model = BaggingClassifier(**bag_boot_gs.best_params_,base_estimator=sgd_C, random_state=1, bootstrap=True).fit(x_best, t)\nprint('Accuracy score for classification:')\nprint('bag_boot_model', bag_boot_model.score(x_best, t).mean())\n","eb87c60d":"# get score with ada boosting\nada_boost_model = AdaBoostClassifier(**ada_boost_gs.best_params_, random_state=1).fit(x_best, t)\nprint('Accuracy score for classification:')\nprint('ada_boost_model', ada_boost_model.score(x_best, t).mean())","78f5f1f4":"ensembels = {'Bag_Fold': bag_fold_model, 'Bag_Boost': bag_boot_model, 'AdaBoost': ada_boost_model}\nx_train, x_val, t_train, t_val = model_selection.train_test_split(x_best, t, test_size=0.2, random_state=2)\nshow_accuracy_by_ensembel(x_train, t_train, x_val, t_val, ensembels)","682efd53":"#TODO: \n#show confusion matrix\n# calculate cm for train and val\nfrom sklearn.metrics import confusion_matrix\n\nmodel_pipe = make_pipeline(sgd_C).fit(x_train, t_train)\ny_train = model_pipe.predict(x_train)\ny_val = model_pipe.predict(x_val)\ncm_train = confusion_matrix(t_train, y_train)\ncm_val = confusion_matrix(t_val, y_val)\n\n# lot confusion matrix of train and val as heatmaps in seaborn\ncm_train_df = pd.DataFrame(cm_train, index=['actual_not_survived', 'actual_survived'], columns=['predicted_not_survived',\n                                                                                                'predicted_survived'])\ncm_val_df = pd.DataFrame(cm_val, index=['actual_not_survived', 'actual_survived'], columns=['predicted_not_survived',\n                                                                                            'predicted_survived'])\n\nsns.set(font_scale=2)\nplt.figure(figsize = (15,12))\nfig = sns.heatmap(cm_train_df, annot=True, cmap=plt.cm.Pastel1, fmt='g')\nfig.set_title(\"cm_train\")\nplt.show()\nprint()\nplt.figure(figsize = (15,12))\nfig = sns.heatmap(cm_val_df, annot=True, cmap=plt.cm.Pastel2, fmt='g')\nfig.set_title(\"cm_val\")\nplt.show()\n\n# show tn, fp, fn and tp for train and val\ntn_train, fp_train, fn_train, tp_train = cm_train.ravel()\ntn_val, fp_val, fn_val, tp_val = cm_val.ravel()\n\nprint(f'Train: TN {tn_train:4}, FP {fp_train:4}, FN {fn_train:4}, TP {tp_train:4}')\nprint(f'Val:   TN {tn_val:4}, FP {fp_val:4}, FN {fn_val:4}, TP {tp_val:4}')\n","2429f5f5":"#TODO: \n#build final model- evaluate KNN\n# run KNN on the dataset and find best K by accuracy\nfrom sklearn.neighbors import KNeighborsClassifier\n\nhyper_parameters = {'n_neighbors': list(range(1, 20))}\n\ngs_neigh_model = GridSearchCV(KNeighborsClassifier(n_neighbors=5), hyper_parameters).fit(x_train, t_train)\nprint('Accuracy score for classification:')\nprint('gs_neigh_model', gs_neigh_model.best_score_)\nprint('best params', gs_neigh_model.best_params_)","fe32a1fd":"# run KNN on the dataset and find best K by R2 and accuracy\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import make_scorer, accuracy_score\ndef get_accurate_ordinal_preds_from_numeric_preds(preds, min=None, max=None):\n    if min is None:\n        min = round(min(preds))\n    if max is None:\n        max = round(max(preds))\n    preds = np.asarray(preds).ravel()\n    return np.array([round(p) if min <= p and p <= max else min if p < min else max for p in preds])\n\ndef accuracy_for_ordinal(y, y_pred):\n    min_ord = min(y)\n    max_ord = max(y)\n    y_pred_ord = get_accurate_ordinal_preds_from_numeric_preds(y_pred, min=min_ord, max=max_ord)\n    return accuracy_score(y, y_pred_ord)\n\nhyper_parameters = {'n_neighbors': list(range(1, 20))}\n\ngs_neigh_model1 = GridSearchCV(KNeighborsRegressor(n_neighbors=5, weights='distance'), hyper_parameters).fit(x_train, t_train)\nprint('R2 score for regression:')\nprint('gs_neigh_model', gs_neigh_model.best_score_)\nprint('best params', gs_neigh_model.best_params_)\nprint()\ngs_neigh_model2 = GridSearchCV(KNeighborsRegressor(n_neighbors=5, weights='distance'), hyper_parameters, scoring=make_scorer(accuracy_for_ordinal)).fit(x_train, t_train)\nprint('Accuracy score for regression:')\nprint('gs_neigh_model', gs_neigh_model.best_score_)\nprint('best params', gs_neigh_model.best_params_)","f6fc6d31":"ct = ColumnTransformer([\n    (\"encoding_cat\", OneHotEncoder(sparse=False, handle_unknown='ignore'), categorical_cols),\n    (\"standard_poly\", make_pipeline(PolynomialFeatures(degree=2), StandardScaler()), numerical_cols)],\n    remainder='passthrough')\ntest_d = test_df_cpy.drop(['Fare_new'],axis = 1).copy()\ntest_e = pd.DataFrame(ct.fit_transform(test_d))\ntest_best = test_e.loc[:, selector.support_]\n\n# final_model = KNeighborsClassifier(**gs_neigh_model.best_params_).fit(x_best, t)\n# y_pred = final_model.predict(test_best).astype('int64')\n# submission = pd.DataFrame({\n#         \"PassengerId\": test_df_PassengerId,\n#         \"Survived\": y_pred\n#     })\n\n# submission.to_csv('Submission.csv', index=False)\nfinal_model = AdaBoostClassifier(**ada_boost_gs.best_params_, random_state=1).fit(x_best, t)\ny_pred = final_model.predict(test_best).astype('int64')\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df_PassengerId,\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('adaSubmission.csv', index=False)\n","cc0b5261":"# Submitting results:  ","1cbeccc5":"># 1. Introduction\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.  \nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.  \nWhile there was some element of luck involved in surviving, **it seems some groups of people were more likely to survive than others.**    \n***The ultimate goal:***  \nBuild a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","548ca39d":"## Important notes:  \n***pclass***:  \nmean the Socio-economic level: \n1 = High,2 = Middle, 3 = Low  \n\n***sibsp:***\nThe dataset defines family relations in this way...  \n* Sibling = brother, sister, stepbrother, stepsister  \n* Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)  \n  \n***parch:***\nThe dataset defines family relations in this way...  \n* Parent = mother, father  \n* Child = daughter, son, stepdaughter, stepson  \n* Some children travelled only with a nanny, therefore parch=0 for them.  ","30f54afb":"\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1ptmVpl-j6JakGSVpWLTdbM0BrIoaHHGW)  \n  \n**That's it, thanks for your time.**  ","4b131c9a":"# Hyper-Parameter Tunning:\nMost of our models have a lot of parameters that can be adjusted.  \nEach parameter value can make our model better (or worse).  \nWe want to be able to find the best hyperparameters for our models.  \nWe have two approaches:  \n1. Grid Search\n2. Random Search\n\n### Grid Search\nWhen we want to check every parameter possible, we will use Grid Search.  \nWe will try all combinations of parameters and find the best one, that gives us the best score.  \nThis may be a little exhaustive, especially when we want to check a lot of parameters and values.  \n\n### Random Search\nWe can choose to get random combinations of parameters and check the score on them.  \nThis will not be as accurate as Grid Search, but it will take less time.","fb88cbe6":"# Choose Regularization method:  \nIn Hyper-Tunning section - I got (using grid search) the best parameters: {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'penalty': 'elasticnet'}  \n**Now, I will make sure that using the elasticnet method is indeed a priority over the rest**  \n(now the Data-Frame will consist only the best features).","b750c557":"*Check the types of the features:*","dea5285d":"# Leaderboard:   \nThe best result so far has been obtained using the MLP model. \n\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1zHWqPGYCNsIAPgThI9hXxToXkFU7x5N2)  \n\n\nPosition in Leaderboard:  \n\n\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1kKIRP_zFqntFo4qE6RbcPOguT8p05OzI)","4914fc69":"> ### Let's see if a passenger who arrived without a family improves his chances of survival  ","d0deef0b":"### Inference:\n* It can be infer that with GridSearch you get better results,\n    * I will use later:`grid search params: {'alpha': 0.01, 'eta0': 0.01, 'learning_rate': 'adaptive', 'penalty': 'elasticnet'}` \n    * Unless different parameters came out for the different models (these parameters were obtained for-`SGDClassifier`)","ad3be190":"# Work Summary:\nI started from an in-depth investigation of the data to understand the context for the survival findings.  \nI performed a lot of division group validation and training options.  \nAttached are links explaining the topic.  \nEventually I realized that the chances of survival converge to the following parameters:  \n* Your location on the ship (Pclass)  \n* Gender of the passenger.  \n* Family size.  \n* Age.  \n* The person's origin (Embarked).  \nAt first I divided the age by groups I thought would describe the distribution of the feature but this division did not contribute to the model predicting the validation group.  ","bee70827":"## Conclusion:\n* SGDClassifier with k = 7 gives nice results.\n* I'll try to increase the score, and decrease the loss.\n* using feature selection methods & hyper-tunning.","92af0fe6":"> # ***3.Preprocessing & Data Cleaning***","ed625e70":"> ## Fare:  \nAccording to the correlation I saw above, the correlation between ticket price (fare) and the chance of survival is positive, which means that the more a person pays, the higher his chances of being saved.\nMy hypothesis- the more a person paid he got to first class and was saved due to the class' location on the ship.","cc661a41":">we have 891 rows, it's seem like Cabin have alot of NaN values.  \nNext, display the percentage of the values in our dataset were missing to give us a better sense of the scale of this problem:","4184a737":"> First I'll drop the non-numeric columns: 'Lonely_passenger','AgeGroup'.  \n> Next, I will remove the columns that do not contribute to my data analysis: 'PassengerId','Age'  \n> Finally, I will remove the columns I created from new columns that are more useful: 'Parch', 'Sibsp'\n> * note: 'Age' contributes to the data but only when categorized so I left 'AgeGroupEncoding'.","5c1ce8a7":"# Sources:  \n[Confusion Matrix](https:\/\/colab.research.google.com\/drive\/1lVeHS_IkqDJ3TcnB4Wa4YkiqtDAqxc7A?usp=sharing)  \n[Feature Selection,Cross-Validation](https:\/\/colab.research.google.com\/drive\/19qIprMZhayfIjuR3lKFe-N2qD9j3KLwj?usp=sharing)  \n[Hyper-Parameters Search,Ensembles,KNN](https:\/\/colab.research.google.com\/drive\/1KbGA4I-bvilaia_6xZwIMabhLGTZTEbx?usp=sharing)","00d37107":"#### **Filling in missing values**  \nFirst, I'll drop cabin column because it has above 70% of missing values.  \nNext, I'll fill all the missing values in other columns imputation method--> to maintain distribution for each feature.  \n( For each column, that have less than half of the entries are missing. Thus, dropping the columns removes a lot of useful information, and so it makes sense that imputation would perform better.)  \n\n* note: Any final change in training data will also be made to test data","4594c841":"### Inference:\nAs can be seen, for the best features there is a preference for using **lasso** regularization","80142f07":"# Preliminary knowledge\n* **Correlation**:  \n    * **positive correlation:**  \n     relationship between two variables in which both variables move in the same direction.  \n     Therefore, when one variable increases as the other variable increases, or one variable decreases while the other decreases.  \n     For example fare & Survived:  \n     The correlation between them is 0.26 and is the highest among the adapters to the Survived feature,  \n     probably a passenger who paid more fare had a greater chance  of being saved (perhaps because he was in a better position on the ship).  \n     \n    * **negative correlation:**  \n    relationship between two variables in which an increase in one variable is associated with a decrease in the other. \n    For example fare & pClass:  \n    The correlation between them is -0.55, that is, the more a passenger paid, the lower class (numerically) he received.  \n    let's say you paid the maximum price - you got first class (according to the value, this is number 1).  \n    \n    *  **zero correlation:**  \n    exists when there is no relationship between two variables.  \n     For example there is no relationship between the amount of tea drunk and level of intelligence.","f52e4aac":"# Cross-Validation:","ce504c1a":"> ## Multi Layer Perceptron (MLP):","d0877548":"## Load Data","f20a8473":"# Final Model:  \n","8be89684":"# Feature Selection\n\nWe want to choose the best features for our use case.  \nWe have learned 3 methods of Feature Selection:  \n1. **Forward Feature Selection**\n2. **Backward Feature Selection**\n3. **Hybrid Feature Selection**\n\nIn **Forward Feature Selection** we start from zero features and add features until we reach the number of maximum features or until we reach the best score.  \nIn **Backward Feature Selection** we start from the full feature set and remove features until we reach the number of minimum features or until we reach the best score.  \nIn **Hybrid Feature Selection**, we start from zero features and add\/remove features until we reach the best score.  \n[More on RFE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html)","0feab279":"# 6. Build Model\n> In this challenge i'll try with logistic regression model. \n> ## Logistic Regression:","dacb5d50":">### Delete data duplicated.  \nFirst we will check if there are duplicate data  \n","d0fc76a2":"**step 1: Get list of categorical variables**\n","75fd0c79":"> ### Sizes:  \n> * Training group = 612 rows  \n> * Validation group = 205 rows\n","de5d4148":"### pclass:  \n* First class = white, Second class = yellow ,Third = pink\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1YExuY_v_xkE02sIlg-pWduUVZ-NfvEsc)\n","45654525":"> ### Feature Union-","987994c1":"> **As we can see- we need to change 'Sex','Embarked' to numeric values.**  \nnote: The object dtype indicates a column has text ","446780c2":"> ### I will now see the correlation between each feature and feature","c17d747d":"> # 2. **A quick and general look at the data**\nThe features are:  \n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1ZgxiFAe2CSyRM7eOdHpkk6vjg5l_dD64)\n","f9cf21a5":"> ## SibSp, Parch:  \n**We have a good correlation between the properties: parch, sbsp.  \nRemember, parch indicates the number of relatives (children and parents), and sibsp indicates the number of cousins.  \nIn total, it is possible to unite them in a new feature to be called - family size**\n","c7b347b7":"### **Conclusions:** \n**At each point of boarding the men did not survive significantly compared to women.**  \n\n* **Boarding from Southampton:**\n    * 72% of the people boarded the ship at this point.\n    * 69% of them are men, 31% of them are women\n    * It is evident that 66% did not survive, and 33% survived.\n    * There is a significant difference between the surviving women and the men at this embark point.\n    * More than half of the people were in a third class.\n    * It is also seen that a third class did not survive compared to classes 2 and 1.  \n\n* **Boarding from Cherbourg:**  \n    * 19% of all the people on the ship boarded the ship at this point.  \n    * 56% of them are men, 44% of them are women.  \n    * Here, there was a better chance of being saved - 44% did not survive, 55% survived.  \n    * Many of the people who came up at this embark were in first class \n    * It is seen that first class leads in the chances of survival over the rest.   \n\n* **Boarding from Queenstown:**\n    * 9% of the people on the ship boarded at this point.\n    * 53% men, 47% women.\n    * Slight chance of being rescued - over half of the people who ascended at this point did not survive.\n    * Almost everyone held third-class tickets except for 5 people.\n    \n","5a28d4ad":"### **Conclusions:**  \n* If you are a Class 3 and a man - a high probability that you will not survive in the Titanic.\n* If you are from Class 1 and a woman - there is a high probability that you will survive.\n* In each class women survived more than men, and inversely men did not survive more than women.\n* High concentration of young people in class 3 (perhaps because it is cheaper).\n","43945e84":"> # **TITANIC-CHALLENGE**\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=16sJxE9ISnk0RP1LR7XjX8xbZPioc9w8J)\n\n","9f9e2652":"### After many runs and attempts to remove the following columns - leads to a good result  \nnote: fare removal-  \nFrom the assumption - the division of first, second and third class is according to the cost of a ticket (better class = higher payment).  \nAt the same time, there are passengers who have boarded from 3 collection points, assuming that each collection point of   payment is different, so at the moment we have disabled the cost feature to get a better prediction according to key parameters.","8bfa5f4f":"We will perform for column \"Fare\" the same process we performed for column \"Age\"\n","70251970":"# Confusion Matrix\n\nWe want to be able to explain the results of a classifier.  \nWhen talking about binary classification, the classification type of a sample can be one out of four:\n1. **TP (True Positive**) - The model classified correctly that a sample is positive.  \n2. **TN (True Negative)** - The model classified correctly that a sample is negative.   \n3. **FP (False Positive)** - The model classified a sample as positive but the sample is actually negative.  \n4. **FN (False Negative)** - The model classified a sample as negative but the sample is actually positive.  \n\n\n","6aa91d39":"**As we can see we have NaN values in the columns (train data):**  \n` Age, Cabin, Embarked `   \n**we have NaN values in the columns (test data):**  \n`Age, Cabin, Fare`\nNext, let's see how many we have in each column.\n* note:  there is a difference between train and test data.","0f0b5bf7":"KNN model  \n![image.png](attachment:43badf17-4f09-4ca8-8e85-8fde80062250.png)\nadaBoost model  \n![image.png](attachment:138c1cfa-e84f-4866-9176-3dd00b03112c.png)","5fe91a36":"# Sources:\n* [Inspiration from notebook](https:\/\/www.kaggle.com\/watcher2747\/titanic-knn-and-linear-regression) \n* [Creating the graphs](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.to_csv.html)\n* [Documentation of the Titanic story](https:\/\/www.thoughtco.com\/titanic-timeline-1779210)\n* [Ship structure](https:\/\/www.datavis.ca\/papers\/titanic\/)\n* [Data Cleaning - Course](https:\/\/www.kaggle.com\/learn\/data-cleaning)\n* [Intermediate Machine Learning - Course](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)  \n\n","d1585067":"# Choose Ensemble method:  \n## Ensembles - \"The wisdom of the crowd\":  \nwe use classification,then we can take the mean of all the probabilities of the model or choose the class that most of the models chose for some sample.   \nOne model may be wrong, but a lot of different models are less prone to errors.  \n\nI'll check two types of ensembles:  \n1. Bagging (with NFold or with Bootstrap).\n2. Boosting.\n\n### Bagging\nWe create a few bags of samples from the original dataset.  \nWe train a model on each of the bags of samples, and we return the combined score.\n[BaggingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html). \n### Boosting\nWe create a model and train it on the data.  \nWe take the samples that the model predicted incorrectly and multiply them (thus giving them more weight in the next training).  \nWe do this until we have few models, each of them is an expert on some type of samples.  \nWe combine all the model's predictions and return a combined score.  \nThere are few boosting algorithems ([AdaBoost](https:\/\/www.youtube.com\/watch?v=LsK-xG1cLYA&ab_channel=StatQuestwithJoshStarmer), [GradientBoost](https:\/\/www.youtube.com\/watch?v=3CC4N4z3GJc&ab_channel=StatQuestwithJoshStarmer), etc.).    \n**Hyper-Parameter = n-estimator**","d908dbac":">### Drop unrelated values.  \n ","b52afbd0":"KNeighborsClassifier did better than KNeighborsRegressor on this dataset. \nAccording to the investigation of the data - the survival of a person on a ship depends on the department in which he is.  \nTherefore, it makes sense to use a model that takes into account the neighbors.","0bcaff2e":"> ## Pclass\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1YExuY_v_xkE02sIlg-pWduUVZ-NfvEsc)\n* As I saw in the picture of the structure of the ship -  \n  the location of the classes on the ship were found in different places for example a third class was at the stern of the ship.  \n  I'll investigate the fact that a person is in a particular class against the chances of survival","fb8537f6":"> **As can be seen 342 of all people in train group (891) survived while 549 did not survive.**  \nThis is 38.38% percent survival","228ca035":"> **First and foremost -  \n> Data Slicing**  \nWe can not check the performence of the model on the same dataset that the model was   trained on.  \nThis will result in wrong estimation for the model generalization capabilities.  \nIn order to check our prediction and fine-tune the model parameters, we need to slice   the dataset into 2 groups:  \n  \n> 1. train  \n> 2. validation   \n\n> We will train on the train data and check the performance on the validation data.  \nWe will slice the dataset with Scikit-learn train_test_split.  \nFirst, let's split the data to features X and target t.  ","4ce6838b":"> ## Sex  \n* As seen in the graphs for all the features:  \n  The thing - that more women survived than men will be repeated. Here I will show only the chances of survival for each gender.\n  I will put sex comparisons in all the graphs for the rest of the features (using hue).","7b96d4ca":"> **Decision moment:**  \n[Explanation of data splitting](http:\/\/https:\/\/towardsdatascience.com\/train-validation-and-test-sets-72cb40cba9e7)  \nLogistic regression does not really have any critical hyperparameters to tune.  \nSometimes, you can see useful differences in performance or convergence with different solvers (solver). Regularization (penalty) can sometimes be helpful.   \nIn saying this we can conclude that it will be easy to adjust, and therefore the size of the validation set can be reduced.  \n[Explanation of Hyperparameter Tuning](http:\/\/https:\/\/www.analyticsvidhya.com\/blog\/2021\/04\/evaluating-machine-learning-models-hyperparameter-tuning\/)  \nThe pair that yields a relatively good result in terms of MSE and score and also the differences (between training set and validation set) are relatively minor:  \n**train_size = 0.7 (70% from the data), validation_size = 0.3 (35% from the data)**  \nSo let's take a good place in the middle where you see improvement and it is:  \nvalid = 0.35.  \nThe claim is consistent with the above statement.","3ebfdb01":"# Improving results (assignment 3):  \n**The goal: to try to improve results using KNN, LDA, NBC models**  \nUsing feature selection methods, cv and more.","a7525178":"**step 2: One Hot Encoding**  \n* 'Sex' encoding:  \n    * 'Male' = 1.0  \n    * 'Female' = 0.0  \n* 'Embarked' encoding:  \n    * Q = 1.0,0.0   \n    * S = 0.0,1.0  \n    * C = 0.0,0.0","7319cc8e":"## *convert data to numeric values*","55ab6638":"> **We will now want to prepare the data in terms of row arrangement and indexes.**   \n> Focus on:  \n>* Delete data duplicated.\n>* Checking for Nan values.\n>* Drop outlier.\n>* Drop unrelated values.","2ad6ff65":"# Conclusion:  \n\nwith KNN model I succeeded to make an improvement although my best score were with manually feature selection.  \nIn training we saw that adaBoost gives amazing results (0.9), but in practice on the test brought relatively low results.  \nThere is no doubt that the definition of the task fits this model because the model takes into account the neighboring examples - that is, if a group of examples survived it makes sense that the current example would also survive.  ","e4590bed":"### Conclusions:  \n* **Age distribution:**  \n    * High concentration is in the 16-30 age category (Young Adults).\n    * It can be seen that the old adults category is higher than the sum of the categories - Infant & children.\n    \n### ***I'll use ordinal enoder for AgeGroup category:***\n#### Encoding age fearue:  ","e9c3944a":"## ***Drop column***","28be511c":"Submitter Details:  \nYarden Dahan, 208730523  \n[My Kaggle Link](https:\/\/www.kaggle.com\/jordandahan)","e11d5061":"### Inference:\n**As you can see, I significantly improved the score average, and also reduced the loss value. This is done by selecting the best features (using the RFE method- that is based on the *Backward* Feature Selection.).**","8e95f6fc":"> ## Embarked:  \nBefore I dive into the data I will try to understand what this feature means in terms of the ship's story.  \nLet's take a look at the Titanic's schedule:  \n\n![picture](https:\/\/drive.google.com\/\/uc?export=view&id=1zqF6A8o6has0m2EvQzlk4jq9Giph97g5)","575ecfb4":"> # **4.Data Investigation**","d68a0719":"### Inference:\nclearly- AdaBoost give the best accuracy result (close to 0.9!!).","0229b99f":"> # **5.Data Slicing**","6d874f53":"### Inference:    \n**In train-set:**   \n* I predicted that 42 survived while they did not.  \n* I predicted that the 407 did not survive - and indeed it is true.  \n* I predicted that 71 did not survive while they survived.  \n* I predicted that 192 survived - and indeed it is true.  \n\n**In val-set:**     \n* I predicted that 9 survived while they did not.  \n* I predicted that the 91 did not survive - and indeed it is true.  \n* I predicted that 31 did not survive while they survived.  \n* I predicted that 48 survived - and indeed it is true.  ","9b944be8":">No removal is required because there is no duplicate data.  \n>### checking for Nan values","47d46079":"Now, we can split the data to train and validation.  \nWe can choose number of values for the test_size argument.  \nLet's check few of them with NE and MSE.  \n* We can plot the data with Plotly scatter.  ","997640b3":"> # **7. Submitting results**  \n\n> Now, after analyzing all the data and performing a \"mini-test\" on the data (using the Validation group). It's time for us to test our model for real this time.  \n* Note: For the Logistic regression we removed a lot of features and left only the critical ones:  \n    * Family size (I showed the relationship of size to survival above)\n    * Gender (male \/ female)\n    * PClass\n        * These features clearly showed the chances of survival of the passenger on the Titanic.\n             * fare- I canceled the pclass feature because of the correlation between them (described above).\n    * AgeGroupEncoding\n    * Embarked\n       ","f4a3275b":"> ## Age  \n* Here I will show only the chances of survival for each age group.\n  I will put age comparisons in all the graphs for the rest of the features (using hue).","5d5d45c1":"**note: As you can see presenting the data using regplot does not give us too much information because there are 2 options -  \nsurvived or not survived, so we chose to display the data using barplot.**  \n> ### **note:From the centralization of the values it can be seen that the following features should be classified into groups:Age, Fare.**   \nIn addition, passenger ID is irrelevant for train_data\n## Classification of ages into categories:   \n* Infants = (0,3] --> encoded as 1.0 \n* Children = (3,16] --> encoded as 0.0\n* Young Adults = (16,30]--> encoded as 4.0 \n* Middle-Aged Adults = (30,45]--> encoded as 2.0 \n* Old Adults = (45,90] -->encoded as 3.0","2c15fcd2":"> ## *Analyze data and present them in an understandable way*  \n**I will now introduce the connection between each feature and the chance of survival**","dd66941c":"## **Conclusions:**  \n* **For those who did not survive:**  \n    * Men without a family - a high percentage of non-survivors, significantly.\n    * As the family grew, more men did not survive than women.  \n    * Men led over women, and the large amount who did not survive from men came from the Young Adults age category.  \n    \n\n* **For those who survived:**  \n    * The lower the family size, the more women survived the men.\n    * As the size of the family increased, the number of survivors of women and men decreased.  \n    * Women survived more than men in all age categories except Infant.  \n         There is significance in the same age range of women's survival."}}