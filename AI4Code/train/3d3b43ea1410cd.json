{"cell_type":{"6ad65ef2":"code","262eed57":"code","6db92e86":"code","00b7a18e":"code","f9af6d1f":"code","5a8c8eab":"code","5ae56592":"code","30a9dea9":"code","c8371f72":"code","4e571256":"code","819b00af":"code","7280ca20":"code","d051c740":"code","0ebf36ba":"code","69180c1b":"code","61a33a73":"code","7d496790":"code","c995fe83":"code","af55439e":"code","8ad0a87b":"code","d40e710d":"code","f0bc21bf":"code","0ddb6af6":"code","9afc6f00":"code","3d054ccb":"code","e6f2b142":"code","23f7f4e5":"markdown","1e384378":"markdown","fc4f3b3f":"markdown","2b1ba66f":"markdown","a35cd7e2":"markdown","665cc139":"markdown","28ce9dce":"markdown","172c438c":"markdown","626cedbc":"markdown","251e15bc":"markdown","63a85fb9":"markdown","643de563":"markdown","2b631b06":"markdown","c7bed722":"markdown"},"source":{"6ad65ef2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","262eed57":"import pandas as pd\ntrain = pd.read_csv('..\/input\/analytics-vidya-nlp-data\/train_E6oV3lV.csv');\ntest = pd.read_csv('..\/input\/analytics-vidya-nlp-data\/test_tweets_anuFYb8.csv');\nss = pd.read_csv('..\/input\/analytics-vidya-nlp-data\/sample_submission_gfvA5FD.csv')","6db92e86":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy\nimport string","00b7a18e":"train[\"text_lower\"] = train[\"tweet\"].str.lower();\ntest[\"text_lower\"] = test[\"tweet\"].str.lower()\ntrain.head()","f9af6d1f":"PUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ntrain[\"text_wo_punct\"] = train[\"text_lower\"].apply(lambda text: remove_punctuation(text));\ntest[\"text_wo_punct\"] = test[\"text_lower\"].apply(lambda text: remove_punctuation(text))\ntrain.head()","5a8c8eab":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","5ae56592":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntrain[\"text_wo_stop\"] = train[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ntest[\"text_wo_stop\"] = test[\"text_wo_punct\"].apply(lambda text: remove_stopwords(text))\ntrain.head()","30a9dea9":"tr = train.drop(columns = ['tweet', 'text_lower', 'text_wo_stop'], axis = 1);\nte = test.drop(columns = ['tweet', 'text_lower', 'text_wo_stop'], axis = 1);","c8371f72":"from nltk.stem.porter import PorterStemmer\n\n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ntr[\"text_stemmed\"] = tr[\"text_wo_punct\"].apply(lambda text: stem_words(text));\nte[\"text_stemmed\"] = te[\"text_wo_punct\"].apply(lambda text: stem_words(text))\ntr.head()","4e571256":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ntr[\"text_lemmatized\"] = tr[\"text_stemmed\"].apply(lambda text: lemmatize_words(text));\nte[\"text_lemmatized\"] = te[\"text_stemmed\"].apply(lambda text: lemmatize_words(text))\ntr.head()","819b00af":"import random\nfrom sklearn.impute import SimpleImputer;\nfrom sklearn.compose import ColumnTransformer;\nfrom sklearn.pipeline import Pipeline;\nfrom sklearn.preprocessing import LabelEncoder;\nfrom sklearn.preprocessing import StandardScaler;\nfrom sklearn.preprocessing import MinMaxScaler;\nfrom sklearn.model_selection import train_test_split;\nfrom sklearn.linear_model import LinearRegression ;\nfrom sklearn.linear_model import Ridge, Lasso;\nfrom sklearn.metrics import mean_squared_error;\nfrom sklearn.metrics import r2_score;\nfrom sklearn.preprocessing import PolynomialFeatures;\nfrom sklearn.svm import SVR;\nfrom sklearn.svm import SVC;\nfrom sklearn.tree import DecisionTreeClassifier;\nfrom sklearn.ensemble import RandomForestClassifier;\nfrom sklearn.ensemble import RandomForestRegressor;\nfrom sklearn.neighbors import KNeighborsClassifier;\nfrom sklearn.naive_bayes import GaussianNB;\nimport xgboost as xgb;\nfrom xgboost import XGBClassifier;\nfrom xgboost import XGBRegressor;","7280ca20":"x = tr.text_lemmatized.values;\ny = tr.label.values","d051c740":"lab = LabelEncoder();\nx = lab.fit_transform(x)","0ebf36ba":"x","69180c1b":"x = x.reshape(-1, 1)","61a33a73":"y","7d496790":"z = te.text_lemmatized.values","c995fe83":"z = lab.fit_transform(z)","af55439e":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state = 55)","8ad0a87b":"xtrain","d40e710d":"ytrain","f0bc21bf":"rfc = RandomForestClassifier();\nrfc.fit(xtrain, ytrain);\nrfc.score(xtest, ytest)","0ddb6af6":"xx = XGBClassifier();\nxx.fit(xtrain, ytrain);\nxx.score(xtest, ytest)","9afc6f00":"from lightgbm import LGBMClassifier\nlgb = LGBMClassifier(n_estimators=10)\nlgb.fit(xtrain,ytrain);\nlgb.score(xtest, ytest)","3d054ccb":"knn = KNeighborsClassifier();\nknn.fit(xtrain, ytrain);\nknn.score(xtest, ytest)","e6f2b142":"dc = DecisionTreeClassifier();\ndc.fit(xtrain, ytrain);\ndc.score(xtest, ytest)","23f7f4e5":"# KNEARESTNEIGHBOUR","1e384378":"# LEMMATIZING","fc4f3b3f":"# LABEL ENCODING AND SPLITING INTO TEST AND TRAIN DATA","2b1ba66f":"Let us begin with importing the libraries and datasets","a35cd7e2":"We are getting best results from xgboost.Its better and safer to go with that one I guess.","665cc139":"# STEMMING","28ce9dce":"# LIGHTGBM","172c438c":"# DECISION TREE","626cedbc":"I want to thank [@srk](http:\/\/www.kaggle.com\/sudalairajkumar) for his wonderful kernel on how to get started in NLP. Really learnt a lot and tried to apply some of them to process text data.\n\nYou can find the kernel [here](http:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing)","251e15bc":"# REMOVING PUNCTUATION","63a85fb9":"# LOWERING ","643de563":"# STOPWORDS","2b631b06":"# XGBOOST","c7bed722":"# RANDOMFORREST"}}