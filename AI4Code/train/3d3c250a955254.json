{"cell_type":{"5c1ee77a":"code","6784c4d3":"code","41708fa2":"code","7d0b2720":"code","303623a5":"code","645ab655":"code","46027bb5":"code","ba73f97c":"code","5a146aec":"code","9aa36f10":"code","477c841f":"code","8b903ec4":"code","2a3dc426":"code","dfeba786":"code","5b360a5b":"markdown","63373569":"markdown","a0cfb009":"markdown","c6ae2327":"markdown","c5f1d8fe":"markdown","fe347334":"markdown","a23ee1a8":"markdown","869eb1c7":"markdown"},"source":{"5c1ee77a":"import os\nfrom os.path import join\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression","6784c4d3":"DATA_DIR = '..\/input\/champs-scalar-coupling'\ntrain_all = pd.read_csv(join(DATA_DIR, 'train.csv'))\ntest = pd.read_csv(join(DATA_DIR, 'test.csv'))\nstructures = pd.read_csv(join(DATA_DIR, 'structures.csv'))","41708fa2":"def map_atom_info(df, atom_idx, structures_df=structures):\n    \"\"\"\n    From this kernel:\n    https:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark\n    \"\"\"\n    df = pd.merge(df, structures_df, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\n\ntrain_all = map_atom_info(train_all, 0)\ntrain_all = map_atom_info(train_all, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)\n\ntrain_all.head()","7d0b2720":"# code in this cell is from:\n# https:\/\/www.kaggle.com\/rpeer333\/brute-force-feature-engineering\/edit\n\ntrain_p_0 = train_all[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train_all[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test[['x_1', 'y_1', 'z_1']].values\n\ntrain_all['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)","303623a5":"def add_sc_type_features(df, one_hot=False):\n    assert 'type' in df.columns\n    \n    df['bonds']    = df.type.map(lambda x: int(x[0]))\n    df['atom_pair'] = df.type.map(lambda x: x[2:4])\n    \n    if one_hot:\n        df = pd.get_dummies(df, prefix='bonds')\n        df = pd.get_dummies(df, prefix='atom_pair')\n    \n    return df\n\n\ntrain_all = add_sc_type_features(train_all)\ntest  = add_sc_type_features(test)\ntrain_all.head()","645ab655":"is_train = np.random.rand(len(train_all)) < 0.8\n\nval   = train_all[~is_train].copy()\ntrain = train_all[is_train].copy()\nprint(f'training-fraction: {len(train) \/ (len(train) + len(val)):.4f}')","46027bb5":"def metric(df, preds):\n    \"\"\"\n    function from: https:\/\/www.kaggle.com\/abhishek\/competition-metric\n    \"\"\"\n    \n    df[\"prediction\"] = preds\n    maes = []\n    for t in df.type.unique():\n        y_true = df[df.type==t].scalar_coupling_constant.values\n        y_pred = df[df.type==t].prediction.values\n        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n        maes.append(mae)\n        \n    return np.mean(maes)","ba73f97c":"median_prediction = np.tile(train.scalar_coupling_constant.median(), len(val))\nmetric(val, median_prediction)","5a146aec":"mean_prediction = np.tile(train.scalar_coupling_constant.mean(), len(val))\nmetric(val, mean_prediction)","9aa36f10":"sns.boxplot(data=train, x='type', y='scalar_coupling_constant', fliersize=0.5);","477c841f":"group_medians = train.groupby('type').apply(lambda df: df.scalar_coupling_constant.median())\ngroup_medians","8b903ec4":"group_median_prediction = val.type.map(group_medians)\ntype_median_score = metric(val, group_median_prediction)\ntype_median_score","2a3dc426":"lin_reg = LinearRegression()\nlin_reg.fit(X=np.reshape(train.dist.values, (-1, 1)),\n            y=train.scalar_coupling_constant.values)\ny_hat = lin_reg.predict(np.reshape(val.dist.values, (-1, 1)))\n\nmetric(val, y_hat)","dfeba786":"X_train = pd.get_dummies(train[['dist', 'type']], prefix='type', drop_first=True).values\nX_val   = pd.get_dummies(val[['dist', 'type']], prefix='type', drop_first=True).values\n\nlin_reg = LinearRegression()\nlin_reg.fit(X=X_train, y=train.scalar_coupling_constant.values)\ny_hat = lin_reg.predict(X_val)\n\nscore = metric(val, y_hat)\nprint(f'score: {score}')\nprint(f'improvement over type-mean-prediction: {type_median_score - score}')","5b360a5b":"### EDA+: make 'trivial predictions' to get a baseline\n\n(for basic EDA, see this kernel: https:\/\/www.kaggle.com\/rpeer333\/molecular-properties-detailed-eda)\n\nAfter EDA but efore starting to build models, it makes sense to get an idea, how large the loss would when making trivial predictions, such as:\n\n* always predicting the median\n* only using the distance between the j-coupled atoms for (univariate) regression\n* always prediting the median per type of j-coupling\n* etc.\n\nOnly features that are already available easy to compute are used for this purpose to get an idea of their relatlive importance.\n\nIt also establishes a base-line: Any model that is not way better than these trivial predictions is clearly not working and it's time to move on to the next idea.","63373569":"# Predict median sc-constant for each type\n\nThe range of sc-constants varies from type to type. Therefore, predicting the median per type will give better results than simply predicting the global median.\n\n","a0cfb009":"# Predict median sc-constant\n\nWe use the median rather than the mean because the comptetion metric is based on the mean absolute error which is minimized by the median (the arithemtic mean would minimize the the mean squared error).","c6ae2327":"# Linear Regression on euclidean distance\n\nInterestingly regression on the distance between atom-0 and atom-1 is only barely better than the predicting the mean (which is equivalent to 'fitting' the best regression line with zero slope).","c5f1d8fe":"# Linear regression on distance and sc-type indicator-varibles\n\nInterestinly, this model is even worse than the per-type-median prediction. Probably due to the fact that linear regression minimizes the mean squared error rather than the mean absolute error used in the competition metric.","fe347334":"## Context:\n\n#### From Competition description:\n![image.png](attachment:image.png)\n\nBest current score: -2.975  \nCurrent medal-score: <= -1.566\n\n#### Type-weights\n\nNote that the MAEs for all 8 types of sc are weighted equally regadless of their (very differnt) frequencies in the data-set.\n\nThe trivial predictions could probably be improved by simply adding weights to the data points such that the sum of weights for each of the 8 types is exactly 1\/8.","a23ee1a8":"# Split into training and validation-set","869eb1c7":"# Add simple features"}}