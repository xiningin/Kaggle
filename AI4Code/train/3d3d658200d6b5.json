{"cell_type":{"061dbd6a":"code","27800e3d":"code","c17940a6":"code","23fc5cd4":"code","0ecc27a8":"code","2fa2fdd0":"code","8adac92d":"code","1d7a4877":"code","becf98d9":"code","1ad74499":"code","e1bdc1f3":"code","eab84538":"code","66ede39e":"code","25e11ee0":"code","ccc4ac12":"code","281f5ddd":"code","f7cc6564":"code","a283299e":"markdown","7aa7da10":"markdown","1b0044bc":"markdown","3d8201f8":"markdown","b53c9da0":"markdown","290e56c2":"markdown","ddf941a8":"markdown","bb94ea98":"markdown","ba48e16a":"markdown","c70c868b":"markdown","2985492c":"markdown","4a327527":"markdown","84f4451a":"markdown","9f6e6344":"markdown","acda3f4d":"markdown","eefd0ec3":"markdown","dab6a8ab":"markdown","bed52778":"markdown"},"source":{"061dbd6a":"import numpy as np #for math functions, linear algebra\nimport pandas as pd #for dataframe management, data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # for graphical visualization\n\nimport matplotlib.pyplot as plt # for graphical visualization\n%matplotlib inline\n\nfrom digit_recognizer_utils import are_unique_values, have_nan, draw_example_digits, plot_acuracy_progress","27800e3d":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","c17940a6":"#get the information about each column of the dataset,\n\ntrain.info()\nprint(\"train columns: \", train.columns)\nprint('Train Data Shape: {}'.format(train.shape))\ntrain.head()\ntrain.describe()","23fc5cd4":"test.info()\nprint(\"test columns: \", test.columns)\nprint('Test Data Shape: {}'.format(test.shape))\ntest.head()\ntest.describe()","0ecc27a8":"# Extra verifications\nare_unique_values(train, test)\nhave_nan(train, test)","2fa2fdd0":"# How many labels are?\n#Change size of the plot in seaborn\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n\nplt.figure(figsize=(6, 4))\nsns.countplot(data=train, x='label', palette=\"bright\")\nplt.xlabel('Label')\nplt.ylabel('Count')","8adac92d":"# Extracting the labels and features from the train dataset\ny_train = train.label.astype('float32')\nX_train = train.drop(labels = 'label', axis=1).astype('float32')\nX_test = test.astype('float32')","1d7a4877":"X_train = X_train.values.reshape(-1, 28, 28, 1)\nX_train = X_train \/ 255.0\n\nX_test = X_test.values.reshape(-1, 28, 28, 1)\nX_test = X_test \/ 255.0","becf98d9":"m, _ = test.shape\n\ndraw_example_digits(X_train, y_train, m)","1ad74499":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Activation, Dropout, Conv2D, MaxPooling2D, BatchNormalization","e1bdc1f3":"y_train = tf.keras.utils.to_categorical(y_train, 10)","eab84538":"model = Sequential([\n        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)), \n        Conv2D(32, (3, 3), activation='relu'),\n        MaxPooling2D(2, 2), \n        Conv2D(64, (3, 3), activation='relu', padding='Same'), \n        Conv2D(64, (3, 3), activation='relu', padding='Same'), \n        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n        Dropout(0.25), \n        Conv2D(64, (3, 3), activation='relu', padding='Same'), \n        Conv2D(64, (3, 3), activation='relu', padding='Same'), \n        MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n        Dropout(0.25), \n        Flatten(),\n        Dense(256, activation='relu'), \n        Dense(256, activation='relu'), \n        Dropout(0.50),\n        Dense(10, activation=\"softmax\"),  \n    ])","66ede39e":"# Configurations\nstrategy = tf.distribute.get_strategy()\nEPOCHS = 50\nBATCH_SIZE = 128 * strategy.num_replicas_in_sync\nSTEPS_PER_EPOCH = 42000 \/\/ BATCH_SIZE\n\n# Class to stop fitting when it needs\nclass Callback(tf.keras.callbacks.Callback):\n    def end_if(self, epoch, logs={}):\n        if (logs.get('accuracy') > 0.999):\n            print('Reached 0.99 accuracy')\n            self.model.stop_training = True\n\noptimizer = tf.keras.optimizers.Adam(\n                    learning_rate=0.0001, \n                    beta_1=0.9, \n                    beta_2=0.999, \n                    epsilon=1e-10, \n                    name='Adam'\n            )\n\ndatagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=40,\n    width_shift_range=0.20,\n    shear_range=15,\n    zoom_range=0.10,\n    validation_split=0.25,\n    horizontal_flip=False\n)\n\ntrain_generator = datagen.flow(\n    X_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    subset='training',\n)\n\nvalidation_generator = datagen.flow(\n    X_train,\n    y_train,\n    batch_size=BATCH_SIZE,\n    subset='validation',\n)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n                                                 factor=0.1,\n                                                 patience=5,\n                                                 min_lr=0.000001,\n                                                 verbose=1)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath='model.hdf5',\n                                                monitor='val_loss',\n                                                save_best_only=True,\n                                                save_weights_only=True,\n                                                verbose=1)","25e11ee0":"import time\n\ndef train_model(model):\n    start = time.time()\n    history = model.fit(\n        train_generator, \n        epochs=EPOCHS,\n        validation_data=validation_generator,\n        callbacks=[reduce_lr,checkpoint, Callback()], \n        verbose=1)\n    print(\"Total time: \", time.time() - start, \"seconds\")\n    return history\n\nmodel.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nwith strategy.scope():\n    if ( tf.test.is_gpu_available() ) :\n        print(\"Using '\/GPU:0'\")\n        with tf.device('\/GPU:0'):\n            history = train_model(model)\n    else:\n        history = train_model(model)","ccc4ac12":"results = model.predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")","281f5ddd":"model.load_weights('model.hdf5')\n\nplot_acuracy_progress(history)\n\nfinal_loss, final_acc = model.evaluate(X_train,  y_train, verbose=2)\nprint(\"Model accuracy: \", final_acc, \", model loss: \", final_loss)","f7cc6564":"submission = pd.concat([pd.Series(range(1, 28001), name='ImageID'), results], axis=1)\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head(10)","a283299e":"**For Train Dataset**\n\n**It seems like we have:**\n785 columns,\n42000 rows.\n\n**We have `label` column with the anotated value**\n\n**The remaining columns are each of the pixels on the image**\n","7aa7da10":"# 7 - Training","1b0044bc":"## Resize the dataset\n","3d8201f8":"# 1 - Packages needed","b53c9da0":"Check\n\n","290e56c2":"**For Test Dataset**\n\n**It seems like we have:**\n784 columns,\n42000 rows.\n\n**We DON'T have a `label` column with the anotated value** as is the target!\n\n**The columns present are each of the pixels on the image**","ddf941a8":"Modeling","bb94ea98":"Data augmentation","ba48e16a":"# 2 - Load our data and define datasets\n\nLet's define our datasets for training and testing:","c70c868b":"# 5 - Preparing our model","2985492c":"Now, as in class' notebook lets visualize some examples:","4a327527":"Some initial observations:\n\n- **Data is already clean**. Each pixel is an `int64` and seems lke we do't need to worry too much for cleaning dataset.\n- **We have to concentrate on the pixels**. As there is not different types of data or we don't have to drop columns that doesn't contribute, we can start working right away on solving the problem.\n- **No need to optimize for overflows**. Data set is not trivial but not huge. Also calculations are on integers so we don't expect problems there.\n\nWe have some previous questions for this dataset:\n\n- how many labels are?\n- what is the way the pixels are encoding?\n","84f4451a":"predict","9f6e6344":"# 0 - Our first Kaggle competition!\n\nAs part of our [FullStack Labs - Machine Learning Bootcamp](http:\/\/campusvirtual.learningcode.tech\/course\/view.php?id=2) work, we need to participate in [Digit Recognizer](https:\/\/www.kaggle.com\/c\/digit-recognizer) competition on [Kaggle](https:\/\/www.kaggle.com\/). We are more than pleasure to anyone looking at this notebook and hope it helps you to start as it helped us!","acda3f4d":"Results","eefd0ec3":"## One Hot Encoding Target Values\n\nBy using one-hot encoding, we are converting each categorical values into new categorical column with binary values of 1 or 0.","dab6a8ab":"# 4 - Normalize the dataset\n\nThe inputs with the large integer values can slow down the learning process,so it's a good practice to normalize the pixel values with values between 0 and 1.\n\nAlso known as *Reshaping* or *Standardization*.","bed52778":"# 3 - Study our data"}}