{"cell_type":{"936fb72e":"code","2693c2f2":"code","565a1021":"code","bd46ed47":"code","573754d2":"code","acfd9d4c":"code","d201f5c0":"code","86cd92f5":"code","baa041d2":"code","25706007":"code","2f1c9f9c":"code","ff4c78d2":"code","4eca2418":"code","def197da":"code","7f3ffb1d":"markdown"},"source":{"936fb72e":"# GFootball environment.\n!pip install kaggle_environments\n!apt-get update -y\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!git clone -b v2.3 https:\/\/github.com\/google-research\/football.git\n!mkdir -p football\/third_party\/gfootball_engine\/lib\n!wget https:\/\/storage.googleapis.com\/gfootball\/prebuilt_gameplayfootball_v2.3.so -O football\/third_party\/gfootball_engine\/lib\/prebuilt_gameplayfootball.so\n!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# Some helper code\n!git clone https:\/\/github.com\/garethjns\/kaggle-football.git\n!pip install reinforcement_learning_keras==0.6.0","2693c2f2":"import numpy as np\nimport os\nimport torch\n\ntorch.manual_seed(0)\nnp.random.seed(0)","565a1021":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Actor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=8, kernel_size=8, stride=4, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4, stride=2, padding=0, bias=True)\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(1280+4, 256, bias=True)\n        self.fc2 = nn.Linear(256, 19)\n        self.relu = nn.ReLU()\n        self.b_1 = nn.BatchNorm2d(4)\n        self.b_2 = nn.BatchNorm2d(8)\n        self.b_3 = nn.BatchNorm2d(16)\n        self.b_4 = nn.BatchNorm1d(1280+4)\n        self.b_5 = nn.BatchNorm1d(256)\n        \n    def forward(self, x, scalar):\n        x = torch.tensor(x).float()  # normalize\n        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n        x = self.b_1(x)\n        x = self.relu(self.conv1(x))\n        x = self.b_2(x)\n        x = self.relu(self.conv2(x))\n        x = self.b_3(x)\n        x = self.relu(self.conv3(x))\n        x = x.reshape(x.shape[0], -1)  # flatten\n        x = self.b_4(torch.cat([x, scalar], 1))\n        x = self.relu(self.fc1(x))\n        x = self.b_5(x)\n        x = self.fc2(x)\n        return F.softmax(x, dim = -1)\n    \n    \nclass Critic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=8, kernel_size=8, stride=4, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4, stride=2, padding=0, bias=True)\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(1280+4, 256, bias=True)\n        self.fc2 = nn.Linear(256, 1)\n        self.relu = nn.ReLU()\n        self.b_1 = nn.BatchNorm2d(4)\n        self.b_2 = nn.BatchNorm2d(8)\n        self.b_3 = nn.BatchNorm2d(16)\n        self.b_4 = nn.BatchNorm1d(1280+4)\n        self.b_5 = nn.BatchNorm1d(256)\n        \n    def forward(self, x, scalar):\n        x = torch.tensor(x).float()  # normalize\n        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n        x = self.b_1(x)\n        x = self.relu(self.conv1(x))\n        x = self.b_2(x)\n        x = self.relu(self.conv2(x))\n        x = self.b_3(x)\n        x = self.relu(self.conv3(x))\n        x = x.reshape(x.shape[0], -1)  # flatten\n        x = self.b_4(torch.cat([x, scalar], 1))\n        x = self.relu(self.fc1(x))\n        x = self.b_5(x)\n        x = self.fc2(x)\n        return x","bd46ed47":"from kaggle_environments.envs.football.helpers import *\nfrom math import sqrt\n\ndirections = [\n[Action.TopLeft, Action.Top, Action.TopRight],\n[Action.Left, Action.Idle, Action.Right],\n[Action.BottomLeft, Action.Bottom, Action.BottomRight]]\n\ndirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)\n\nenemyGoal = [1, 0]\nperfectRange = [[0.61, 1], [-0.2, 0.2]]\n\ndef inside(pos, area):\n    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]\n\ndef get_distance(pos1,pos2):\n    return ((pos1[0]-pos2[0])**2+(pos1[1]-pos2[1])**2)**0.5\n\ndef player_direction(obs):\n    controlled_player_pos = obs['left_team'][obs['active']]\n    controlled_player_dir = obs['left_team_direction'][obs['active']]\n    x = controlled_player_pos[0]\n    y = controlled_player_pos[1]\n    dx = controlled_player_dir[0]\n    dy = controlled_player_dir[1]\n    \n    if x <= dx:\n        return 0\n    if x > dx:\n        return 1\n\ndef run_pass(left_team,right_team,x,y):\n    ###Are there defenders dead ahead?\n    defenders=0\n    for i in range(len(right_team)):\n        if right_team[i][0] > x and y +.01 >= right_team[i][1] and right_team[i][1]>= y - .01:\n            if abs(right_team[i][0] - x) <.01:\n                defenders=defenders+1\n    if defenders == 0:\n        return Action.Right\n    \n    teammateL=0\n    teammateR=0\n    for i in range(len(left_team)):\n        #is there a teamate close to left\n        if left_team[i][0] >= x:\n            if left_team[i][1] < y:\n                if abs(left_team[i][1] - x) <.05:\n                    teammateL=teammateL+1\n        \n        #is there a teamate to right\n        if left_team[i][0] >= x:\n            if left_team[i][1] > y:\n                if abs(left_team[i][1] - x) <.05:\n                    teammateR=teammateR+1\n    #pass only close to goal\n    if x >.75:\n        if teammateL > 0 or teammateR > 0:\n            return Action.ShortPass\n    \n    if defenders > 0 and y>=0:\n        return Action.TopRight\n    \n    if defenders > 0 and y<0:\n        return Action.BottomRight\n\n\ndef agent(obs):\n    controlled_player_pos = obs['left_team'][obs['active']]\n    \n    # special plays\n    if obs[\"game_mode\"] == GameMode.Penalty:\n        return Action.Shot\n    if obs[\"game_mode\"] == GameMode.Corner:\n        if controlled_player_pos[0] > 0:\n            return Action.Shot\n    if obs[\"game_mode\"] == GameMode.FreeKick:\n        return Action.Shot\n    \n    # Make sure player is running.\n    if  0 < controlled_player_pos[0] < 0.6 and Action.Sprint not in obs['sticky_actions']:\n        return Action.Sprint\n    elif 0.6 < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:\n        return Action.ReleaseSprint\n\n    # Does the player we control have the ball?\n    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:\n        \n        goalkeeper = 0\n        #if in the zone near goal shoot\n        if inside(controlled_player_pos, perfectRange) and controlled_player_pos[0] < obs['ball'][0]:\n            return Action.Shot\n        #if the goalie is coming out on player near goal shoot\n        elif abs(obs['right_team'][goalkeeper][0] - 1) > 0.2 and controlled_player_pos[0] > 0.4 and abs(controlled_player_pos[1]) < 0.2:\n            return Action.Shot\n        # if close to goal and too wide for shot pass the ball\n        if controlled_player_pos[0] >.75 and controlled_player_pos[1] >.20 or controlled_player_pos[0] >.75 and controlled_player_pos[1] <-.20 :\n            return Action.ShortPass\n        # if near our goal and moving away long pass to get out of our zone\n        if player_direction(obs)==1 and controlled_player_pos[0]<-.3:\n            return Action.LongPass\n        # which way should we run or pass\n        else:\n            return run_pass(obs['left_team'],obs['right_team'],controlled_player_pos[0],controlled_player_pos[1])\n    else:\n        #vector where ball is going\n        ball_targetx=obs['ball'][0]+obs['ball_direction'][0]\n        ball_targety=obs['ball'][1]+obs['ball_direction'][1]\n        \n        #euclidian distance to the ball so we head off movement until very close\n        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])\n        \n        #if not close to ball move to where it is going\n        if e_dist >.005:\n            # Run where ball will be\n            xdir = dirsign(ball_targetx - controlled_player_pos[0])\n            ydir = dirsign(ball_targety - controlled_player_pos[1])\n            return directions[ydir][xdir]\n        #if close to ball go to ball\n        else:\n            # Run towards the ball.\n            xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])\n            ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])\n            return directions[ydir][xdir]","573754d2":"w_step = 2\/96.0\nh_step = 0.84\/72\n\ndef get_coordinates(arr):\n    x, y = arr\n    x_i = 0\n    y_i = 0\n    for i in range(1, 96):\n        if x <-1 or x>1:\n            if x<-1:\n                x_i = 0\n            else:\n                x_i = 95\n        else:\n            if -1+ (i-1)*w_step <= x <= -1 + i*w_step:\n                x_i = i\n                break\n\n    for i in range(1, 72):\n        if y <-0.42 or y>0.42:\n            if y<-0.42:\n                y_i = 0\n            else:\n                y_i = 71\n        else:\n            if -0.42+ (i-1)*h_step <= y <= -0.42 + i*h_step:\n                y_i = i\n                break\n    return [y_i, x_i]\n\n\ndef get_team_coordinates(team_arr):\n    answ = []\n    for j in range(len(team_arr)):\n        answ.append(get_coordinates(team_arr[j]))\n    return answ\n\n\nimport math\nimport numpy as np\n\ndef angle(src, tgt):\n    dx = tgt[0] - src[0]\n    dy = tgt[1] - src[1]\n    theta = round(math.atan2(dx, -dy) * 180 \/ math.pi, 2)\n    while theta < 0:\n        theta += 360\n    return theta\n\ndef direction(src, tgt):\n    actions = [3, 4, 5, \n               6,  7, \n               8, 1, 2]\n    theta = angle(src, tgt)\n    index = int(((theta+45\/2)%360)\/45)\n    return actions[index]\n\n\ndef create_obs(obs):\n    ball_coord = get_coordinates(obs['ball'][:-1])\n    left_team_coord = get_team_coordinates(obs['left_team'])\n    right_team_coord = get_team_coordinates(obs['right_team'])\n    player_coord =  get_coordinates(obs['left_team'][obs['active']])\n    \n    \n    obs_1 = np.zeros(shape = (1, 72, 96, 4))\n    \n    obs_1[0, ball_coord[0], ball_coord[1], 0] = 1\n            \n    obs_1[0, player_coord[0], player_coord[1], 0] = 1\n    \n    for i, l in enumerate(left_team_coord):\n        \n        obs_1[0, l[0], l[1], 2] = 1\n\n    for i, r in enumerate(right_team_coord):\n        obs_1[0, r[0], r[1], 3] = 1\n\n    ball_next_coord = get_coordinates([obs['ball'][0] + obs['ball_direction'][0], obs['ball'][1] + obs['ball_direction'][1]])\n\n    left_team_next_coord = []\n    for i in range(len(obs['left_team'])):\n        left_team_next_coord.append([obs['left_team'][i][0] + obs['left_team_direction'][i][0], obs['left_team'][i][1] + obs['left_team_direction'][i][1]])\n    \n    right_team_next_coord = []\n    for i in range(len(obs['right_team'])):\n        right_team_next_coord.append([obs['right_team'][i][0] + obs['right_team_direction'][i][0], obs['right_team'][i][1] + obs['right_team_direction'][i][1]])\n        \n        \n    scalar = np.zeros(shape = (1, 4))\n    scalar[0,0] = obs['ball_owned_team']\n    scalar[0,1] = obs['game_mode']\n    scalar[0,2] = direction(obs['ball'][:-1], obs['ball_direction'][:-1])  \n    scalar[0,3] = direction(obs['left_team'][obs['active']], obs['left_team_direction'][obs['active']])\n        \n    return obs_1, scalar","acfd9d4c":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport gfootball.env as football_env\nfrom gfootball.env import observation_preprocessing\n\n\n\n\n\nenv = football_env.create_environment(\n   env_name=\"11_vs_11_kaggle\",\n   representation='raw',\n   stacked=False,\n   logdir='.',\n   write_goal_dumps=False,\n   write_full_episode_dumps=False,\n   render=False,\n   number_of_left_players_agent_controls=1,\n   dump_frequency=0)\n\nobs = env.reset()\n\ncreated_obs = create_obs(obs[0])","d201f5c0":"created_obs[0].shape","86cd92f5":"actor = Actor()\ncritic = Critic()\n\n# actor.load_state_dict(torch.load('actor.pth'))\n# critic.load_state_dict(torch.load('critic.pth'))","baa041d2":"env  = football_env.create_environment(\n   env_name=\"11_vs_11_kaggle\",\n   representation='raw',\n   stacked=False,\n   logdir='.',\n   write_goal_dumps=False,\n   write_full_episode_dumps=False,\n   render=False,\n   number_of_left_players_agent_controls=1,\n   number_of_right_players_agent_controls=1,\n   dump_frequency=0)\n\nobs = env.reset()\n\n\n\nadam_actor = torch.optim.Adam(actor.parameters(), lr=1e-3)\nadam_critic = torch.optim.Adam(critic.parameters(), lr=1e-3)\ngamma = 0.99\n\n\nstep_done = 0\nrewards_for_plot = []\nfor steps_done in range(64):\n    states = []\n    actions = []\n    rewards = []\n    next_states = []\n    dones = []\n\n    games_play = 0\n    wins = 0\n    loses = 0\n    obs = env.reset()\n    values = []\n    log_probs = []\n    done = False\n    while not done:\n\n        converted_obs = create_obs(obs[0])\n        actor.eval()\n        prob = actor(torch.as_tensor(converted_obs[0], dtype = torch.float32), torch.as_tensor(converted_obs[1], dtype = torch.float32))\n        actor.train()\n        dist = torch.distributions.Categorical(probs = prob)\n        act = dist.sample()\n\n\n        new_obs, reward, done, _ = env.step([act.detach().data.numpy()[0], (agent(obs[1])).value])\n        if reward[0]==-1:\n            loses+=1\n            done = True\n        if reward[0] == 1:\n            wins+=1\n            done = True\n        if reward[0]==0 and done:\n            reward[0] = 0.25\n            \n        last_q_val = 0\n        if done:\n            converted_next_obs = create_obs(new_obs[0])\n            critic.eval()\n            last_q_val = critic(torch.as_tensor(converted_next_obs[0], dtype = torch.float32), torch.as_tensor(converted_next_obs[1], dtype = torch.float32))\n            last_q_val = last_q_val.detach().data.numpy()\n            critic.train()\n\n        states.append(obs[0])\n        action_arr = np.zeros(19)\n        action_arr[act] = 1\n        actions.append(action_arr)\n        rewards.append(reward[0])\n        next_states.append(new_obs[0])\n        dones.append(1 - int(done))\n\n        obs = new_obs\n        if done:\n            obs = env.reset()\n            break\n            \n    rewards = np.array(rewards)\n    states = np.array(states)\n    actions = np.array(actions)\n    next_states = np.array(next_states)\n    dones = np.array(dones)\n    \n    print('epoch '+ str(steps_done)+ '\\t' +'reward_mean ' + str(np.mean(rewards)) + '\\t' + 'games_count ' + str(games_play) + '\\t' + 'total_wins ' + str(wins) + '\\t'+ 'total_loses ' + str(loses))\n    rewards_for_plot.append(np.mean(rewards))\n    #train\n    q_vals = np.zeros((len(rewards), 1))\n    for i in range(len(rewards)-1, 0, -1):\n        last_q_val = rewards[i] + dones[i]*gamma*last_q_val\n        q_vals[i] = last_q_val\n\n    action_tensor = torch.as_tensor(actions, dtype=torch.float32)\n\n    obs_playgraund_tensor = torch.as_tensor(np.array([create_obs(states[i])[0][0] for i in range(len(rewards))]), dtype=torch.float32)\n\n    obs_scalar_tensor = torch.as_tensor(np.array([create_obs(states[i])[1][0] for i in range(len(rewards))]), dtype=torch.float32)\n\n    val = critic(obs_playgraund_tensor, obs_scalar_tensor)\n    \n    probs = actor(obs_playgraund_tensor, obs_scalar_tensor)\n    \n    advantage = torch.Tensor(q_vals) - val\n    \n    critic_loss = advantage.pow(2).mean()\n    adam_critic.zero_grad()\n    critic_loss.backward()\n    adam_critic.step()\n    \n    \n    actor_loss = (-torch.log(probs)*advantage.detach()).mean()\n    adam_actor.zero_grad()\n    actor_loss.backward(retain_graph=True)\n    adam_actor.step()\n    \n    \n\n#         soft_update(actor, target_actor, 0.8)\n#         soft_update(critic, target_critic, 0.8)\n        \n    if steps_done!=0 and steps_done%50 == 0:\n        torch.save(actor.state_dict(), 'actor.pth')\n\n        torch.save(critic.state_dict(), 'critic.pth')","25706007":"torch.save(actor.state_dict(), 'actor.pth')\n\ntorch.save(critic.state_dict(), 'critic.pth')","2f1c9f9c":"import matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(rewards_for_plot)\nplt.title('Train graphic', fontsize=22)\n\nplt.xlabel('epoch',fontsize=18)\nplt.ylabel('mean_reward',fontsize=18)","ff4c78d2":"%%writefile main.py\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport gfootball.env as football_env\nfrom gfootball.env import observation_preprocessing\nimport torch.nn.functional as F\nimport math\nimport numpy as np\n\n#Transform\n\n\n\nw_step = 2\/96.0\nh_step = 0.84\/72\n\ndef get_coordinates(arr):\n    x, y = arr\n    x_i = 0\n    y_i = 0\n    for i in range(1, 96):\n        if x <-1 or x>1:\n            if x<-1:\n                x_i = 0\n            else:\n                x_i = 95\n        else:\n            if -1+ (i-1)*w_step <= x <= -1 + i*w_step:\n                x_i = i\n                break\n\n    for i in range(1, 72):\n        if y <-0.42 or y>0.42:\n            if y<-0.42:\n                y_i = 0\n            else:\n                y_i = 71\n        else:\n            if -0.42+ (i-1)*h_step <= y <= -0.42 + i*h_step:\n                y_i = i\n                break\n    return [y_i, x_i]\n\n\ndef get_team_coordinates(team_arr):\n    answ = []\n    for j in range(len(team_arr)):\n        answ.append(get_coordinates(team_arr[j]))\n    return answ\n\n\n\n\ndef angle(src, tgt):\n    dx = tgt[0] - src[0]\n    dy = tgt[1] - src[1]\n    theta = round(math.atan2(dx, -dy) * 180 \/ math.pi, 2)\n    while theta < 0:\n        theta += 360\n    return theta\n\ndef direction(src, tgt):\n    actions = [3, 4, 5, \n               6,  7, \n               8, 1, 2]\n    theta = angle(src, tgt)\n    index = int(((theta+45\/2)%360)\/45)\n    return actions[index]\n\n\ndef create_obs(obs):\n    ball_coord = get_coordinates(obs['ball'][:-1])\n    left_team_coord = get_team_coordinates(obs['left_team'])\n    right_team_coord = get_team_coordinates(obs['right_team'])\n    player_coord =  get_coordinates(obs['left_team'][obs['active']])\n    \n    \n    obs_1 = np.zeros(shape = (1, 72, 96, 4))\n    \n    obs_1[0, ball_coord[0], ball_coord[1], 0] = 1\n            \n    obs_1[0, player_coord[0], player_coord[1], 0] = 1\n    \n    for i, l in enumerate(left_team_coord):\n        \n        obs_1[0, l[0], l[1], 2] = 1\n\n    for i, r in enumerate(right_team_coord):\n        obs_1[0, r[0], r[1], 3] = 1\n\n    ball_next_coord = get_coordinates([obs['ball'][0] + obs['ball_direction'][0], obs['ball'][1] + obs['ball_direction'][1]])\n\n    left_team_next_coord = []\n    for i in range(len(obs['left_team'])):\n        left_team_next_coord.append([obs['left_team'][i][0] + obs['left_team_direction'][i][0], obs['left_team'][i][1] + obs['left_team_direction'][i][1]])\n    \n    right_team_next_coord = []\n    for i in range(len(obs['right_team'])):\n        right_team_next_coord.append([obs['right_team'][i][0] + obs['right_team_direction'][i][0], obs['right_team'][i][1] + obs['right_team_direction'][i][1]])\n        \n        \n    scalar = np.zeros(shape = (1, 4))\n    scalar[0,0] = obs['ball_owned_team']\n    scalar[0,1] = obs['game_mode']\n    scalar[0,2] = direction(obs['ball'][:-1], obs['ball_direction'][:-1])  \n    scalar[0,3] = direction(obs['left_team'][obs['active']], obs['left_team_direction'][obs['active']])\n        \n    return obs_1, scalar\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Actor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=8, kernel_size=8, stride=4, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4, stride=2, padding=0, bias=True)\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(1280+4, 256, bias=True)\n        self.fc2 = nn.Linear(256, 19)\n        self.relu = nn.ReLU()\n        self.b_1 = nn.BatchNorm2d(4)\n        self.b_2 = nn.BatchNorm2d(8)\n        self.b_3 = nn.BatchNorm2d(16)\n        self.b_4 = nn.BatchNorm1d(1280+4)\n        self.b_5 = nn.BatchNorm1d(256)\n        \n    def forward(self, x, scalar):\n        x = torch.tensor(x).float()  # normalize\n        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n        x = self.b_1(x)\n        x = self.relu(self.conv1(x))\n        x = self.b_2(x)\n        x = self.relu(self.conv2(x))\n        x = self.b_3(x)\n        x = self.relu(self.conv3(x))\n        x = x.reshape(x.shape[0], -1)  # flatten\n        x = self.b_4(torch.cat([x, scalar], 1))\n        x = self.relu(self.fc1(x))\n        x = self.b_5(x)\n        x = self.fc2(x)\n        return F.softmax(x, dim = 1)\n    \n    \nclass Critic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=4,  out_channels=8, kernel_size=8, stride=4, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=4, stride=2, padding=0, bias=True)\n        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(1280+4, 256, bias=True)\n        self.fc2 = nn.Linear(256, 1)\n        self.relu = nn.ReLU()\n        self.b_1 = nn.BatchNorm2d(4)\n        self.b_2 = nn.BatchNorm2d(8)\n        self.b_3 = nn.BatchNorm2d(16)\n        self.b_4 = nn.BatchNorm1d(1280+4)\n        self.b_5 = nn.BatchNorm1d(256)\n        \n    def forward(self, x, scalar):\n        x = torch.tensor(x).float()  # normalize\n        x = x.permute(0, 3, 1, 2).contiguous()  # 1 x channels x height x width\n        x = self.b_1(x)\n        x = self.relu(self.conv1(x))\n        x = self.b_2(x)\n        x = self.relu(self.conv2(x))\n        x = self.b_3(x)\n        x = self.relu(self.conv3(x))\n        x = x.reshape(x.shape[0], -1)  # flatten\n        x = self.b_4(torch.cat([x, scalar], 1))\n        x = self.relu(self.fc1(x))\n        x = self.b_5(x)\n        x = self.fc2(x)\n        return x\n    \nactor = Actor()\nactor.load_state_dict(torch.load('actor.pth'))\nactor = actor.float().to('cpu').eval()\n\n\ndef agent(obs):\n    # Get observations for the first (and only one) player we control.\n    obs = obs['players_raw'][0]\n    # Agent we trained uses Super Mini Map (SMM) representation.\n    # See https:\/\/github.com\/google-research\/seed_rl\/blob\/master\/football\/env.py for details.\n    converted_obs = create_obs(obs)\n    policy = actor(torch.as_tensor(converted_obs[0], dtype = torch.float32), torch.as_tensor(converted_obs[1], dtype = torch.float32))\n    return [int(policy.argmax())]","4eca2418":"%%writefile submission.py\nfrom kaggle_environments.envs.football.helpers import *\nfrom math import sqrt\n\ndirections = [\n[Action.TopLeft, Action.Top, Action.TopRight],\n[Action.Left, Action.Idle, Action.Right],\n[Action.BottomLeft, Action.Bottom, Action.BottomRight]]\n\ndirsign = lambda x: 1 if abs(x) < 0.01 else (0 if x < 0 else 2)\n\nenemyGoal = [1, 0]\nperfectRange = [[0.61, 1], [-0.2, 0.2]]\n\ndef inside(pos, area):\n    return area[0][0] <= pos[0] <= area[0][1] and area[1][0] <= pos[1] <= area[1][1]\n\ndef get_distance(pos1,pos2):\n    return ((pos1[0]-pos2[0])**2+(pos1[1]-pos2[1])**2)**0.5\n\ndef player_direction(obs):\n    controlled_player_pos = obs['left_team'][obs['active']]\n    controlled_player_dir = obs['left_team_direction'][obs['active']]\n    x = controlled_player_pos[0]\n    y = controlled_player_pos[1]\n    dx = controlled_player_dir[0]\n    dy = controlled_player_dir[1]\n    \n    if x <= dx:\n        return 0\n    if x > dx:\n        return 1\n\ndef run_pass(left_team,right_team,x,y):\n    ###Are there defenders dead ahead?\n    defenders=0\n    for i in range(len(right_team)):\n        if right_team[i][0] > x and y +.01 >= right_team[i][1] and right_team[i][1]>= y - .01:\n            if abs(right_team[i][0] - x) <.01:\n                defenders=defenders+1\n    if defenders == 0:\n        return Action.Right\n    \n    teammateL=0\n    teammateR=0\n    for i in range(len(left_team)):\n        #is there a teamate close to left\n        if left_team[i][0] >= x:\n            if left_team[i][1] < y:\n                if abs(left_team[i][1] - x) <.05:\n                    teammateL=teammateL+1\n        \n        #is there a teamate to right\n        if left_team[i][0] >= x:\n            if left_team[i][1] > y:\n                if abs(left_team[i][1] - x) <.05:\n                    teammateR=teammateR+1\n    #pass only close to goal\n    if x >.75:\n        if teammateL > 0 or teammateR > 0:\n            return Action.ShortPass\n    \n    if defenders > 0 and y>=0:\n        return Action.TopRight\n    \n    if defenders > 0 and y<0:\n        return Action.BottomRight\n\n@human_readable_agent\ndef agent(obs):\n    controlled_player_pos = obs['left_team'][obs['active']]\n    \n    # special plays\n    if obs[\"game_mode\"] == GameMode.Penalty:\n        return Action.Shot\n    if obs[\"game_mode\"] == GameMode.Corner:\n        if controlled_player_pos[0] > 0:\n            return Action.Shot\n    if obs[\"game_mode\"] == GameMode.FreeKick:\n        return Action.Shot\n    \n    # Make sure player is running.\n    if  0 < controlled_player_pos[0] < 0.6 and Action.Sprint not in obs['sticky_actions']:\n        return Action.Sprint\n    elif 0.6 < controlled_player_pos[0] and Action.Sprint in obs['sticky_actions']:\n        return Action.ReleaseSprint\n\n    # Does the player we control have the ball?\n    if obs['ball_owned_player'] == obs['active'] and obs['ball_owned_team'] == 0:\n        \n        goalkeeper = 0\n        #if in the zone near goal shoot\n        if inside(controlled_player_pos, perfectRange) and controlled_player_pos[0] < obs['ball'][0]:\n            return Action.Shot\n        #if the goalie is coming out on player near goal shoot\n        elif abs(obs['right_team'][goalkeeper][0] - 1) > 0.2 and controlled_player_pos[0] > 0.4 and abs(controlled_player_pos[1]) < 0.2:\n            return Action.Shot\n        # if close to goal and too wide for shot pass the ball\n        if controlled_player_pos[0] >.75 and controlled_player_pos[1] >.20 or controlled_player_pos[0] >.75 and controlled_player_pos[1] <-.20 :\n            return Action.ShortPass\n        # if near our goal and moving away long pass to get out of our zone\n        if player_direction(obs)==1 and controlled_player_pos[0]<-.3:\n            return Action.LongPass\n        # which way should we run or pass\n        else:\n            return run_pass(obs['left_team'],obs['right_team'],controlled_player_pos[0],controlled_player_pos[1])\n    else:\n        #vector where ball is going\n        ball_targetx=obs['ball'][0]+obs['ball_direction'][0]\n        ball_targety=obs['ball'][1]+obs['ball_direction'][1]\n        \n        #euclidian distance to the ball so we head off movement until very close\n        e_dist=get_distance(obs['left_team'][obs['active']],obs['ball'])\n        \n        #if not close to ball move to where it is going\n        if e_dist >.005:\n            # Run where ball will be\n            xdir = dirsign(ball_targetx - controlled_player_pos[0])\n            ydir = dirsign(ball_targety - controlled_player_pos[1])\n            return directions[ydir][xdir]\n        #if close to ball go to ball\n        else:\n            # Run towards the ball.\n            xdir = dirsign(obs['ball'][0] - controlled_player_pos[0])\n            ydir = dirsign(obs['ball'][1] - controlled_player_pos[1])\n            return directions[ydir][xdir]","def197da":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\nenv.run([\"\/kaggle\/working\/main.py\", \"\/kaggle\/working\/submission.py\"])\nenv.render(mode=\"human\", width=800, height=600)","7f3ffb1d":"Hello this is A2C method for solve GFootball competition!\n\nI use ConvNet whith some 1d data such us ball_owned_team, game_mode, angle controlled player and angle the ball.\n\nFor train my agent i also use https:\/\/www.kaggle.com\/mlconsult\/best-open-rules-bot-score-1020-7 this notebook as an enemy for my agent. Thanks for sharing open rule bot https:\/\/www.kaggle.com\/mlconsult Ken Miller.\n\n**The main flaw in this agent that it dosen't score (may be it's because agent train on small epoch count), if you know hove omprove this agent please write some advice.**\n\n"}}