{"cell_type":{"772e08ac":"code","e3012601":"code","6608d3b1":"code","5346aad8":"code","dd25de0a":"code","85577fdd":"code","d374f04f":"code","ac5e0cff":"code","cdbeab23":"code","e7fdc3c1":"code","1a1e3f46":"code","f6ac3b47":"code","965d1669":"code","19908bb0":"code","115128f1":"code","d0275fba":"code","1bc1c145":"code","17ec7a90":"code","f7707021":"code","5a38ba46":"code","0bd2dea9":"code","04045cab":"code","602f9fd5":"code","804dbe69":"code","9b937f23":"code","7bbdb496":"code","e0002353":"code","d65bf6df":"code","a5944e34":"code","622f224f":"code","f8a58941":"code","96ed4447":"code","dfa14505":"markdown","f50d6ba0":"markdown","54646c62":"markdown","7d1a5824":"markdown","f0637282":"markdown","9797515f":"markdown","41c14b87":"markdown","f9c7120f":"markdown","03e4ea11":"markdown","ac71fadd":"markdown","1e7823dc":"markdown","537b1f90":"markdown","6ab6353a":"markdown","6dc5e259":"markdown","e3d953a9":"markdown","408f7079":"markdown"},"source":{"772e08ac":"import pandas as pd\nfrom sklearn import tree\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e3012601":"wine = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","6608d3b1":"wine.head()","5346aad8":"wine.info()","dd25de0a":"qual = wine.quality.unique()\nqual.sort()\nqual","85577fdd":"wine_quality_map = {\n    3:0,\n    4:1,\n    5:2,\n    6:3,\n    7:4,\n    8:5\n}\n\nwine.quality = wine.quality.map(wine_quality_map)","d374f04f":"qual_map = wine.quality.unique()\nqual_map.sort()\nqual_map","ac5e0cff":"wine_train = wine.head(1000)\nwine_train_label = wine_train['quality']\nwine_train = wine_train.drop('quality', axis = 1)","cdbeab23":"wine_train.head()","e7fdc3c1":"wine_test = wine.tail(599)\nwine_test_label = wine_test['quality']\nwine_test = wine_test.drop('quality', axis = 1)","1a1e3f46":"wine_test.head()","f6ac3b47":"train_accuracy = [0.5]\ntest_accuracy = [0.5]\n\nfor depth in range(1, 20):\n    clf = tree.DecisionTreeClassifier(max_depth = depth)\n    \n    clf.fit(wine_train, wine_train_label)\n    \n    train_pred = clf.predict(wine_train)\n    acc_train = metrics.accuracy_score(wine_train_label, train_pred)\n    \n    test_pred = clf.predict(wine_test)\n    acc_test = metrics.accuracy_score(wine_test_label, test_pred)\n    \n    train_accuracy.append(acc_train)\n    test_accuracy.append(acc_test)","965d1669":"plt.figure(figsize = (10, 5))\nsns.set_style('whitegrid')\nplt.plot(train_accuracy, label = 'train accuracy')\nplt.plot(test_accuracy, label = 'test accuracy')\nplt.legend(loc = 'upper left', prop = {'size': 15})\nplt.xticks(range(0, 20, 5))\nplt.xlabel('max_depth', size = 20)\nplt.ylabel('accuracy', size = 20)\nplt.show()","19908bb0":"# Let's Apply K-Fold Cross Validation\n\n# For this create a new column K-Fold with entry -1\nwine['kFold'] = -1\n\n# Shuffle the Data\nwine = wine.sample(frac = 1).reset_index(drop = True)\n\n# Split the data into 4 Folds\nkfolds= model_selection.KFold(n_splits = 5)\nfor fold, (t, v)in enumerate(kfolds.split(X = wine)):\n    wine.loc[v, 'kFold'] = fold\n    \n# Saving the data for further use\nwine.to_csv('k_fold.csv', index = False)","115128f1":"def check(fold):\n    dt = pd.read_csv('.\/k_fold.csv')\n    dt_train = dt[dt.kFold != fold].reset_index(drop = True)\n    dt_test = dt[dt.kFold == fold].reset_index(drop = True)  \n    \n    y_train = dt_train.quality.values\n    x_train = dt_train.drop('quality', axis = 1).values\n    \n    y_valid = dt_test.quality.values\n    x_valid = dt_test.drop('quality', axis = 1).values\n    \n    ktrain_acc = [0.5]\n    ktest_acc = [0.5]\n    for depth in range(1, 20):\n        clf = tree.DecisionTreeClassifier(max_depth = depth)\n\n        clf.fit(x_train, y_train)\n\n        train_pred = clf.predict(x_train)\n        acc_train = metrics.accuracy_score(y_train, train_pred)\n\n        test_pred = clf.predict(x_valid)\n        acc_test = metrics.accuracy_score(y_valid, test_pred)\n\n        ktrain_acc.append(acc_train)\n        ktest_acc.append(acc_test)\n    plt.figure(figsize=(10,5))\n    sns.set_style('whitegrid')\n    plt.plot(ktrain_acc, label = 'train accuracy')\n    plt.plot(ktest_acc, label = 'test accuracy')\n    plt.legend(loc='upper left', prop = {'size': 15})\n    plt.xticks(range(0, 20, 5))\n    plt.xlabel('max_depth', size = 20)\n    plt.ylabel('accuracy', size = 20)\n    plt.show()","d0275fba":"# Taking fold 0 as test set and rest as training set\ncheck(fold = 0)","1bc1c145":"# Taking fold 1 as test set and rest as training set\ncheck(fold = 1)","17ec7a90":"# Taking fold 2 as test set and rest as training set\ncheck(fold = 2)","f7707021":"# Taking fold 3 as test set and rest as training set\ncheck(fold = 3)","5a38ba46":"#Taking fold 4 as test set and rest as training set\ncheck(fold = 4)","0bd2dea9":"X = pd.read_csv('.\/k_fold.csv')\ny = X.quality.values\nX = X.drop('quality', axis = 1).values\nclf = tree.DecisionTreeClassifier(max_depth = 25)\nscores = model_selection.cross_val_score(clf, X, y, cv = 4)\nprint(scores)","04045cab":"# Calculating the average\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","602f9fd5":"plt.figure(figsize=(10,5))\nsns.countplot(x = wine['quality'])","804dbe69":"# Applying stratified k-Fold Cross validation\ny_data = wine.quality.values\nkf = model_selection.StratifiedKFold(n_splits = 5)\n\nfor fold, (t, v) in enumerate(kf.split(X = wine, y = y_data)):\n    wine.loc[v, 'kfold'] = fold\n\nwine.to_csv('st_fold.csv', index = False)","9b937f23":"def check_stratified(fold):\n    st = pd.read_csv('.\/st_fold.csv')\n    st_train = st[st.kFold != fold].reset_index(drop = True)\n    st_test = st[st.kFold == fold].reset_index(drop = True)  \n    \n    y_train = st_train.quality.values\n    x_train = st_train.drop('quality', axis = 1).values\n    \n    y_valid = st_test.quality.values\n    x_valid = st_test.drop('quality', axis = 1).values\n    \n    sktrain_acc = [0.5]\n    sktest_acc = [0.5]\n    for depth in range(1, 20):\n        clf = tree.DecisionTreeClassifier(max_depth = depth)\n\n        clf.fit(x_train, y_train)\n\n        train_pred = clf.predict(x_train)\n        acc_train = metrics.accuracy_score(y_train, train_pred)\n\n        test_pred = clf.predict(x_valid)\n        acc_test = metrics.accuracy_score(y_valid, test_pred)\n\n        sktrain_acc.append(acc_train)\n        sktest_acc.append(acc_test)\n    plt.figure(figsize=(10,5))\n    sns.set_style('whitegrid')\n    plt.plot(sktrain_acc, label = 'train accuracy')\n    plt.plot(sktest_acc, label = 'test accuracy')\n    plt.legend(loc='upper left', prop = {'size': 15})\n    plt.xticks(range(0, 20, 5))\n    plt.xlabel('max_depth', size = 20)\n    plt.ylabel('accuracy', size = 20)","7bbdb496":"# Taking fold 0 as test set and rest as training set\ncheck_stratified(fold = 0)","e0002353":"# aking fold 1 as test set and rest as training set\ncheck_stratified(fold = 1)","d65bf6df":"# Taking fold 2 as test set and rest as training set\ncheck_stratified(fold = 2)","a5944e34":"# Taking fold 3 as test set and rest as training set\ncheck_stratified(fold = 3)","622f224f":"# Taking fold 4 as test set and rest as training set\ncheck_stratified(fold = 4)","f8a58941":"X = pd.read_csv('.\/st_fold.csv')\ny = X.quality.values\nX = X.drop('quality', axis = 1).values\nclf = tree.DecisionTreeClassifier(max_depth = 25)\nscores = model_selection.cross_val_score(clf, X, y, cv=5)\nprint(scores)","96ed4447":"# Calculating the average\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","dfa14505":"## ***The steps involved in model selection are stated below:***\n\n1) Reserve a sample data set.\n\n2) Train the model using the remaining part of the dataset.\n\n3) Use the reserve sample of the test (validation) set to test the effectiveness of your model\u2019s performance.","f50d6ba0":"Let's import some important libraries","54646c62":"### **Simple model selection technique**\n\nFor simple model selection technique I have divided the data(that contains 1599 entries) into train and test set.\n\nIn the Train set contain first 1000 entries and test set contain last 599 entries.\n\nWe will train the model on first 1000 entries and check the model by predicting and comparing with test set.","7d1a5824":"### Stratified K-Fold Cross Validation:\n\nFrom the Graph above it is quite clear that the data is skewed for a classification problem as there is very less data availble for the wine with quality index 0.\n\nWine with quality index 1 and 5 also have a little sample, while the wine with quality index 2 and 3 have a huge amount of samples availble. \n\nFor this classification purpose, simple k-Fold validation doesn't produced good results. So we move to another cross validation technique called stratified K-Fold cross validation.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.","f0637282":"It is the process of determining the degree to which the model corresponds to the real system referred.\n\nIt represents how your model acts to the real world data and helps in determining how good the model is trained.","9797515f":"#### **Insight:**\n\nFrom the graph plotted above it is quite clear that the model fails in predicting most real world data accurately, with the highest accuracy of approx 0.57 at the max depth of 5.\n\nThe model overfits as the train accuracy is much higher than the test accuracy.","41c14b87":"Now we will only use the DecisionTreeClassifier to show how the model accuracy varies with respect to different model validation methods.","f9c7120f":"# *Model Validation*","03e4ea11":"This notebook is based on the **'Red Wine Quality dataset'** which contanis only physicochemical (inputs) and sensory (the output) data for red wine.\n\n### **Input variables (based on physicochemical tests):**\n\n1) fixed acidity\n\n2) volatile acidity\n\n3) citric acid\n\n4) residual sugar\n\n5) chlorides\n\n6) free sulfur dioxide\n\n7) total sulfur dioxide\n\n8) density\n\n9) pH\n\n10) sulphates\n\n11) alcohol\n\n### **Output variable (based on sensory data):**\n\n1) quality (score between 0 and 10)","ac71fadd":"## Any suggestions will be great for me to learn","1e7823dc":"### **K-Fold Cross Validation:**\n\nIt involves random k-Fold Cross Validation dividing the set of observations into k groups, or Folds of approximately equal size.\n\nThe first fold is treated as a validation set, and the machine learing model is fit on the remaining (k-1) Folds.\n\nThis procedure is repeated for (k) times. Each time, a different group of observations is treated as a validation set.","537b1f90":"You can see from above graph visualisation and the accuracy score that stratified k-Fold Cross Validation produced much better result than the k-Fold Cross Validation technique of model validation.\n\nThus whenever there is an uneven distribution of targets choose stratified k-Fold Cross Valit=datin instead of simple k-Fold Cross Validation.","6ab6353a":"The performance measure reported by k-Fold Cross Validation is then the average of the values computed in the loop. The code below shows the result at the max depth of 12 and the accuracy is at differnt folds taken one at a time and the the average is calculated.","6dc5e259":"Now, since the quality varies from 3 to 8, let's map these quantities in range 0 to 5 for better predictions.","e3d953a9":"First we will see the accuracy by taking different training and validation sets and its accuracy with the increasing depth of decision tree.","408f7079":"### **Cross Validation:**\n\nCross validation is a technique in the process of building any machine learning model which ensures that the model fits the data accurately and doesn't overfit the data.\n\nIn this notebook we will only look at K-Fold cross validation and stratified K-Fold cross validation."}}