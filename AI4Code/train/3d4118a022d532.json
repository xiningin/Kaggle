{"cell_type":{"b8d0fc39":"code","26bb7902":"code","536d503d":"code","a146d439":"code","49ce536f":"code","3b73303d":"code","f1f3f5ac":"code","9a4c81c0":"code","c06d34e7":"code","efa932e4":"code","0514261f":"code","3c08cf76":"code","748691d6":"code","02b1d671":"code","c7886296":"code","a8907949":"code","6b794dc6":"code","4fd316d1":"code","829522c0":"code","a5bfbdfb":"code","c6e5bb96":"code","aa7129df":"code","57bdc62a":"code","b9667c96":"code","97f218da":"code","0c362e5d":"code","997234fa":"code","3a2a3363":"code","4ec21ee6":"code","6570029d":"code","07b7cc87":"code","92face91":"code","d7251b7b":"code","14243d0d":"code","a8f91ec3":"code","977cb284":"code","1cdc25c5":"code","3813c356":"code","8f15b380":"code","ed3ad35e":"code","435965a5":"code","51c9abbc":"code","a3d25d39":"code","2f830765":"code","538f7a31":"code","4c7348ef":"code","31f176b1":"code","82f19f89":"code","e07857bf":"code","ee0835fe":"code","bcc7227e":"code","6c8a211d":"code","8a7b874e":"markdown","b6109900":"markdown","467722b6":"markdown","c2a9b3e7":"markdown","931842b4":"markdown","09d49c00":"markdown","17eaacdb":"markdown","416376b7":"markdown","aceef064":"markdown","efcde7b2":"markdown","2d0ea3de":"markdown","75c1dd90":"markdown","b71cc26b":"markdown","477aecb1":"markdown","2e791d6c":"markdown","3faf285e":"markdown","8e4df102":"markdown","b470a23e":"markdown","a882ce63":"markdown","d5851482":"markdown","c05cd2dd":"markdown"},"source":{"b8d0fc39":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ast\npd.options.display.max_columns=100\npd.options.display.max_rows=100","26bb7902":"trainDF=pd.read_csv('..\/input\/train.csv')\nunseenTestDF=pd.read_csv('..\/input\/test.csv')","536d503d":"def GetCSVFromListOfDict(keyNameToFetch,column,columnName):\n    column=column.copy()\n    column=column.fillna('[{}]')\n    columnList=[]\n    for index,row in column.iteritems():\n        columnStr=''\n        listofDict=ast.literal_eval(row)\n        for dic in listofDict:\n\n            if(keyNameToFetch in dic.keys()):\n                columnStr=columnStr+';'+str(dic[keyNameToFetch]) \n        columnStr=columnStr.strip(';') # trim leading ;\n        columnList.append(columnStr)\n\n    tempDF=pd.DataFrame(columnList,columns=[columnName])\n    return tempDF[columnName]\n\n\n#GetCSVFromListOfDict('iso_639_1',trainDF.spoken_languages,'spoken_languages')","a146d439":"trainDF['belongs_to_collection']=GetCSVFromListOfDict('name',trainDF.belongs_to_collection,'belongs_to_collection')\ntrainDF['genres']=GetCSVFromListOfDict('name',trainDF.genres,'genres')\ntrainDF['production_companies']=GetCSVFromListOfDict('name',trainDF.production_companies,'production_companies')\ntrainDF['production_countries']=GetCSVFromListOfDict('name',trainDF.production_countries,'production_countries')\ntrainDF['spoken_languages']=GetCSVFromListOfDict('iso_639_1',trainDF.spoken_languages,'spoken_languages')\ntrainDF['Keywords']=GetCSVFromListOfDict('name',trainDF.Keywords,'Keywords')\ntrainDF['Crew_Dept']=GetCSVFromListOfDict('department',trainDF.crew,'crew')\ntrainDF['Crew_Job']=GetCSVFromListOfDict('job',trainDF.crew,'crew')\ntrainDF['Crew_Name']=GetCSVFromListOfDict('name',trainDF.crew,'crew')\ntrainDF['Crew_Gender']=GetCSVFromListOfDict('gender',trainDF.crew,'crew')\n\n\nunseenTestDF['belongs_to_collection']=GetCSVFromListOfDict('name',unseenTestDF.belongs_to_collection,'belongs_to_collection')\nunseenTestDF['genres']=GetCSVFromListOfDict('name',unseenTestDF.genres,'genres')\nunseenTestDF['production_companies']=GetCSVFromListOfDict('name',unseenTestDF.production_companies,'production_companies')\nunseenTestDF['production_countries']=GetCSVFromListOfDict('name',unseenTestDF.production_countries,'production_countries')\nunseenTestDF['spoken_languages']=GetCSVFromListOfDict('iso_639_1',unseenTestDF.spoken_languages,'spoken_languages')\nunseenTestDF['Keywords']=GetCSVFromListOfDict('name',unseenTestDF.Keywords,'Keywords')\nunseenTestDF['Crew_Dept']=GetCSVFromListOfDict('department',unseenTestDF.crew,'crew')\nunseenTestDF['Crew_Job']=GetCSVFromListOfDict('job',unseenTestDF.crew,'crew')\nunseenTestDF['Crew_Name']=GetCSVFromListOfDict('name',unseenTestDF.crew,'crew')\nunseenTestDF['Crew_Gender']=GetCSVFromListOfDict('gender',unseenTestDF.crew,'crew')\n\n\n\n\n","49ce536f":"display(trainDF.head(1))\ndisplay(unseenTestDF.head(1))","3b73303d":"\nprint(len(trainDF.belongs_to_collection))\ntrainDF.belongs_to_collection.value_counts()\n# Out of 3000 total 2396 missing values. i.e. 79% missing values.\n# Lets check whether missing value vs. present value has effect on revenue?","f1f3f5ac":"trainDF['belongs_to_collection_ISMISSING']=(trainDF.belongs_to_collection.str.strip()=='').astype(int)\nunseenTestDF['belongs_to_collection_ISMISSING']=(unseenTestDF.belongs_to_collection.str.strip()=='').astype(int)\n","9a4c81c0":"trainDF[['belongs_to_collection_ISMISSING','revenue']].corr()\n","c06d34e7":"trainDF.drop(columns=['belongs_to_collection'],inplace=True)\nunseenTestDF.drop(columns=['belongs_to_collection'],inplace=True)","efa932e4":"print(len(trainDF.genres))\nprint(trainDF.genres.isna().sum())\ntrainDF.genres.value_counts().head()\n# No missing values. Good","0514261f":"trainDF['genres']=trainDF.genres.str.replace(' ','_') # so bigrams will act as unigram, and it wont become 2 columns\ntrainDF['genres']=trainDF.genres.str.replace(';',' ')\n","3c08cf76":"\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectFeatures = CountVectorizer(max_features=10)\nvectFeatures.fit(trainDF['genres'])\n\nfeaturesTrainSplit=vectFeatures.transform(trainDF['genres'])\nfeaturesUnseenTestSplit=vectFeatures.transform(unseenTestDF['genres'])\n\n","748691d6":"featuresTrainDF=pd.DataFrame(featuresTrainSplit.toarray(),columns=vectFeatures.get_feature_names())\nfeaturesUnseenTestDF=pd.DataFrame(featuresUnseenTestSplit.toarray(),columns=vectFeatures.get_feature_names())\n","02b1d671":"featuresTrainDF.columns='genres_'+featuresTrainDF.columns\nfeaturesUnseenTestDF.columns='genres_'+featuresUnseenTestDF.columns","c7886296":"trainDF=pd.concat([trainDF,featuresTrainDF],axis=1)\nunseenTestDF=pd.concat([unseenTestDF,featuresUnseenTestDF],axis=1)","a8907949":"trainDF.drop(columns=['genres'],inplace=True)\nunseenTestDF.drop(columns=['genres'],inplace=True)","6b794dc6":"print(len(trainDF.production_companies))\ntrainDF.production_companies.value_counts().head(20)\n# 156 missing values out of 3000","4fd316d1":"print(len(trainDF.production_countries))\ntrainDF.production_countries.value_counts().head(20)\n# 55 Missing values","829522c0":"trainDF['production_countries']=trainDF.production_countries.str.replace(' ','_') # so bigrams will act as unigram, and it wont become 2 columns\ntrainDF['production_countries']=trainDF.production_countries.str.replace(';',' ')\n\n\nunseenTestDF['production_countries']=unseenTestDF.production_countries.str.replace(' ','_') # so bigrams will act as unigram, and it wont become 2 columns\nunseenTestDF['production_countries']=unseenTestDF.production_countries.str.replace(';',' ')\n","a5bfbdfb":"trainDF['IsProductionFromUSA']=(trainDF['production_countries']=='united_states_of_america').astype(int)\nunseenTestDF['IsProductionFromUSA']=(unseenTestDF['production_countries']=='united_states_of_america').astype(int)","c6e5bb96":"trainDF.drop(columns=['production_countries'],inplace=True)\nunseenTestDF.drop(columns=['production_countries'],inplace=True)","aa7129df":"trainDF['IsEnglishLanguage']=(\n                    (trainDF['spoken_languages'].str.contains('en'))\n                    & \n                    (trainDF['original_language']=='en')).astype(int)\n\n\n\nunseenTestDF['IsEnglishLanguage']=(\n                    (unseenTestDF['spoken_languages'].str.contains('en'))\n                    &\n                    (unseenTestDF['original_language']=='en')).astype(int)","57bdc62a":"trainDF[['IsEnglishLanguage','revenue']].corr()","b9667c96":"trainDF.drop(columns=['spoken_languages','original_language'],inplace=True)\nunseenTestDF.drop(columns=['spoken_languages','original_language'],inplace=True)","97f218da":"trainDF['Keywords']=trainDF.Keywords.str.replace(' ','_') # so bigrams will act as unigram, and it wont become 2 columns\ntrainDF['Keywords']=trainDF.Keywords.str.replace(';',' ')\ntrainDF['Keywords']=trainDF['Keywords'].str.lower()\n\n\nunseenTestDF['Keywords']=unseenTestDF.Keywords.str.replace(' ','_') # so bigrams will act as unigram, and it wont become 2 columns\nunseenTestDF['Keywords']=unseenTestDF.Keywords.str.replace(';',' ')\nunseenTestDF['Keywords']=unseenTestDF['Keywords'].str.lower()\n","0c362e5d":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectFeatures = CountVectorizer(max_features=20)\nvectFeatures.fit(trainDF['Keywords'].str.lower())\n\nfeaturesTrainSplit=vectFeatures.transform(trainDF['Keywords'])\nfeaturesUnseenTestSplit=vectFeatures.transform(unseenTestDF['Keywords'])\n\n\n\nfeaturesTrainDF=pd.DataFrame(featuresTrainSplit.toarray(),columns=vectFeatures.get_feature_names())\nfeaturesUnseenTestDF=pd.DataFrame(featuresUnseenTestSplit.toarray(),columns=vectFeatures.get_feature_names())\n\n\nfeaturesTrainDF.columns='Keywords'+featuresTrainDF.columns\nfeaturesUnseenTestDF.columns='Keywords'+featuresUnseenTestDF.columns\n\n\n","997234fa":"trainDF=pd.concat([trainDF,featuresTrainDF],axis=1)\nunseenTestDF=pd.concat([unseenTestDF,featuresUnseenTestDF],axis=1)\n\ntrainDF.drop(columns=['Keywords'],inplace=True)\nunseenTestDF.drop(columns=['Keywords'],inplace=True)\n","3a2a3363":"trainDF.homepage.isna().sum()","4ec21ee6":"trainDF.homepage","6570029d":"trainDF['IsHomePageAvailable']=(trainDF.homepage.isna()==False).astype(int)\nunseenTestDF['IsHomePageAvailable']=(unseenTestDF.homepage.isna()==False).astype(int)","07b7cc87":"trainDF[['IsHomePageAvailable','revenue']].corr()","92face91":"dateSplit=trainDF.release_date.str.extract('([0-9]+)\/([0-9]+)\/([0-9]+)')\ndateSplit.columns=['ReleaseMonth','ReleaseDate','ReleaseYear']\n\ndateSplit.loc[dateSplit.ReleaseYear.astype(int)>20,'ReleaseYear']='19'+dateSplit.loc[dateSplit.ReleaseYear.astype(int)>20,'ReleaseYear']\ndateSplit.loc[dateSplit.ReleaseYear.astype(int)<=20,'ReleaseYear']='20'+dateSplit.loc[dateSplit.ReleaseYear.astype(int)<=20,'ReleaseYear']\n\ntrainDF.drop(columns=['release_date'],inplace=True)\ntrainDF=pd.concat([trainDF,dateSplit.astype(int)],axis=1)","d7251b7b":"print(unseenTestDF.release_date.mode())\nunseenTestDF['release_date'].fillna('9\/9\/11',inplace=True)","14243d0d":"unseenTestDF['release_date'].isna().sum()","a8f91ec3":"dateSplit=unseenTestDF.release_date.str.extract('([0-9]+)\/([0-9]+)\/([0-9]+)')\ndateSplit.columns=['ReleaseMonth','ReleaseDate','ReleaseYear']\n\n\ndateSplit.loc[dateSplit.ReleaseYear.astype(int)>20,'ReleaseYear']='19'+dateSplit.loc[dateSplit.ReleaseYear.astype(int)>20,'ReleaseYear']\ndateSplit.loc[dateSplit.ReleaseYear.astype(int)<=20,'ReleaseYear']='20'+dateSplit.loc[dateSplit.ReleaseYear.astype(int)<=20,'ReleaseYear']\n\n\nunseenTestDF.drop(columns=['release_date'],inplace=True)\nunseenTestDF=pd.concat([unseenTestDF,dateSplit.astype(int)],axis=1)\n","977cb284":"## Month -- > SeasonEnd feature engg","1cdc25c5":"pd.concat([pd.get_dummies(trainDF['ReleaseMonth'].astype(str)),trainDF.revenue],axis=1).corr()['revenue']","3813c356":"trainDF.groupby(by='ReleaseMonth')['revenue'].mean()","8f15b380":"\n\npd.concat([((trainDF.ReleaseMonth==6) |\n            (trainDF.ReleaseMonth==12)|\n           (trainDF.ReleaseMonth==7)\n           ).astype(int),trainDF.revenue],axis=1).corr()['revenue']\n\n\n\n\n","ed3ad35e":"trainDF['IsReleaseMonthSeasonEnd']=((trainDF.ReleaseMonth==6) |\n            (trainDF.ReleaseMonth==12)|\n           (trainDF.ReleaseMonth==7)\n           ).astype(int)\n\nunseenTestDF['IsReleaseMonthSeasonEnd']=((unseenTestDF.ReleaseMonth==6) |\n            (unseenTestDF.ReleaseMonth==12)|\n           (unseenTestDF.ReleaseMonth==7)\n           ).astype(int)\n\n\ntrainDF.drop(columns=['ReleaseMonth'],inplace=True)\nunseenTestDF.drop(columns=['ReleaseMonth'],inplace=True)","435965a5":"trainDF.drop(columns=['ReleaseDate'],inplace=True)\nunseenTestDF.drop(columns=['ReleaseDate'],inplace=True)","51c9abbc":"trainDF['revenue']=np.log1p(trainDF.revenue)\n\ntrainDF['budget']=np.log1p(trainDF.budget)\nunseenTestDF['budget']=np.log1p(unseenTestDF.budget)\n\n\ntrainDF['popularity']=np.log1p(trainDF.popularity)\nunseenTestDF['popularity']=np.log1p(unseenTestDF.popularity)","a3d25d39":"\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","2f830765":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ntrainDFNum=trainDF.select_dtypes(include=numerics)\nunseenTestDFNum=unseenTestDF.select_dtypes(include=numerics)\n","538f7a31":"trainDFNum.drop(columns=['id'],inplace=True)\nunseenTestDFNum.drop(columns=['id'],inplace=True)","4c7348ef":"trainDFNum=trainDFNum.fillna(trainDFNum.median())\nunseenTestDFNum=unseenTestDFNum.fillna(trainDFNum.median())","31f176b1":"from sklearn import model_selection # for splitting into train and test\nimport sklearn\n# Split-out validation dataset\nX = trainDFNum.drop(columns=['revenue'])\nY = trainDFNum['revenue']\n\nvalidation_size = 0.2\nseed = 100\nX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)","82f19f89":"import xgboost\nmodel_XG = xgboost.XGBRegressor() \nmodel_XG.fit(X_train, Y_train)","e07857bf":"\n# make predictions for test data\n\ntrainResult_XG = model_XG.predict(X_train)\ntestResult_XG = model_XG.predict(X_test)\nunseenTestResult_XG=model_XG.predict(unseenTestDFNum)","ee0835fe":"\n    \n\n########## TRAIN DATA RESULT ##########\n\nprint('---------- TRAIN DATA RESULT ----------')\n# The mean squared error\nprint(\"Mean squared error: %.5f\"%np.sqrt( mean_squared_error(Y_train, trainResult_XG)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.4f' % r2_score(Y_train, trainResult_XG))\n\n\n\n\n########## TEST DATA RESULT ##########\n\nprint('---------- TEST DATA RESULT ----------')\n# The mean squared error\nprint(\"Mean squared error: %.5f\"% np.sqrt(mean_squared_error(Y_test, testResult_XG)))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.4f' % r2_score(Y_test, testResult_XG))\n\n\n\n\n","bcc7227e":"unseenTestResult_XG=np.expm1(unseenTestResult_XG)","6c8a211d":"submission=pd.DataFrame([unseenTestDF.id,unseenTestResult_XG]).T\n\nsubmission.columns=['id','revenue']\n\nsubmission.id=submission.id.astype(int)\n\nsubmission.to_csv('submission.csv',index=False)\n","8a7b874e":"I think this column is important.\nJust like genres we will make boolean column for comma separated countries","b6109900":"# genres","467722b6":"Parameters to function <strong>GetCSVFromListOfDict(keyNameToFetch,columnName)<\/strong>\n\n<strong>keyNameToFetch :<\/strong> what value do you want to fetch (e.g. iso_639_1 in above case )\n<br><strong>column :<\/strong> The actual column\n<br><strong>columnName :<\/strong> column name which you want to process","c2a9b3e7":"# homepage","931842b4":"# production_companies","09d49c00":"# original_language and spoken_languages","17eaacdb":"-0.33 means it has small coefficient of correlation. Hence we will take this column into consideration\n\nHypothese - Null values in belongs to colletion will have low revenue, while value present in this column means high revenue","416376b7":"# Model","aceef064":" Following function takes object column, which is actually of type 'List of Dictionaries'. \n <br>And convert it into comma separated values.\n\nEg. \n<br><strong>Input<\/strong>\n<br>[{'iso_639_1': 'en', 'name': 'English'}, {'iso_639_1': 'de', 'name': 'Deutsch'}]\n<br><strong>Output<\/strong> will be the comma separated values\n<br>en,de\n\nThe reason of getting language id instead of name is there are some unicode values : <br>\n{'iso_639_1': 'ar', 'name': '\u0627\u0644\u0639\u0631\u0628\u064a\u0629'}","efcde7b2":" We can split these values as separate column (genres_drama, generes_comedy) and provide boolean values (0:absent 1:present)","2d0ea3de":"# Submission","75c1dd90":"The following columns are of type object. However the actual type is <strong>\"list of dictionaries\"<\/strong>\n\n* belongs_to_collection\n* genres\n* production_companies\n* production_countries\n* spoken_languages\n* Keywords\n* cast\n* crew","b71cc26b":"# production_countries","477aecb1":"\n# belongs_to_colletion\nObservation and analysis","2e791d6c":"# Feature Enginearing","3faf285e":"\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n       max_depth=3, min_child_weight=1, missing=None, n_estimators=10000,\n       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n       silent=True, subsample=1)\n-----\non website\ncolsample_bytree=0.4,\n                 gamma=0,                 \n                 learning_rate=0.07,\n                 max_depth=3,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=0.75,\n                 reg_lambda=0.45,\n                 subsample=0.6,\n                 seed=42","8e4df102":"# Log Scaling","b470a23e":"# Keywords","a882ce63":"## XGBoost","d5851482":"\nCoefficient, r\n<br>Strength of Association\t\n<br>Small\t.1 to .3\t-0.1 to -0.3\n<br>Medium\t.3 to .5\t-0.3 to -0.5\n<br>Large\t.5 to 1.0\t-0.5 to -1.0\n","c05cd2dd":"# Date"}}