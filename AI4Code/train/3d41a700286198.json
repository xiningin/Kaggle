{"cell_type":{"e7194823":"code","3c61fc8c":"code","4e81a0ef":"code","0a06919e":"code","cf877e0e":"code","99923328":"code","9a5ca23a":"code","aacc9931":"code","8fd1bd1a":"code","50ed8735":"code","9f2fe5a0":"code","1b404344":"code","4634846a":"code","139938a2":"code","b94c8efc":"code","ece41d79":"markdown","c9d9649d":"markdown","e49f046a":"markdown","8f192445":"markdown","f05c87f0":"markdown","f743a7c4":"markdown","110f1074":"markdown"},"source":{"e7194823":"!pip install albumentations==0.4.6\n!pip install wandb -q\n!pip install efficientnet-pytorch\n\nimport os\nimport wandb\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport torch\nimport cv2\nfrom efficientnet_pytorch import EfficientNet\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensorV2","3c61fc8c":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" \nDATASET_PATH = '..\/input\/chess-pieces-detection-images-dataset'\nheight = 224\nwidth = 224\nSEED = 42\n\nprint(DEVICE)","4e81a0ef":"# seeding in order to reproduce\ndef seeding(SEED):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(0)\n    torch.backends.cudnn.deterministic = True\n    print('seeding done!!!')\nseeding(SEED)","0a06919e":"#Create dataframe\nlabels = [dir_name for dir_name in os.listdir(DATASET_PATH)]\nlabel_map = dict(zip(labels, [i for i in range(len(labels))]))\nclasses = {v: k for k, v in label_map.items()}\npath_list = []\nlabel_list = []\n\nfor label in labels:\n    for image_list in os.walk(os.path.join(DATASET_PATH, label)):\n        for image in image_list[2]:\n            if image[-3:] == 'jpg':\n                path_list.append(f'{DATASET_PATH}\/{label}\/{image}')\n                label_list.append(label)\n\nchess_dataframe = pd.DataFrame({'path':path_list, 'label':label_list})","cf877e0e":"print(f'Label map:   {label_map}')\nprint('Examples of dataframe', chess_dataframe.sample(3))","99923328":"#show images\nfig, axes = plt.subplots(2,4, figsize=(16, 7))\nrandom_idxs = [random.randint(0, len(chess_dataframe['path'])) for _ in axes.flat]\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(chess_dataframe['path'].iloc[random_idxs[i]]))\n    ax.set_title(chess_dataframe['label'].iloc[random_idxs[i]])\nplt.tight_layout()\nplt.show()","9a5ca23a":"#Data augumentation \ntransform = Compose([\n        Resize(height, width),\n        HorizontalFlip(p=0.5),\n        ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.9),\n        RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n        OneOf([GaussianBlur(),GaussNoise()], p=0.2),\n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ])\n\ntarget_transform = Compose([\n        Resize(height, width),       \n        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2()\n    ]) ","aacc9931":"# Create pytorch dataset\nclass Chess_dataset(Dataset):\n    def __init__(self, paths, labels, transform=None, target_transform=None):\n        self.image_paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n\n        image = cv2.imread(self.image_paths.iloc[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = label_map[self.labels.iloc[idx]]\n        if self.transform:\n            transformed = self.transform(image=image)\n            image = transformed[\"image\"].float() \n        if self.target_transform:\n            transformed = self.target_transform(image=image)\n            image = transformed[\"image\"].float() \n        return image, label","8fd1bd1a":"#Train and test split\nX_train, X_val, y_train, y_val = train_test_split(\n    chess_dataframe['path'],\n    chess_dataframe['label'],\n    test_size=0.15,\n    random_state=SEED\n)","50ed8735":"#create train datasets and test dataset\ntrain_dataset = Chess_dataset(\n     X_train, y_train, transform=transform\n)\nval_dataset = Chess_dataset(\n     X_val, y_val, target_transform=target_transform\n)","9f2fe5a0":"class ClassificationModel(nn.Module):\n    def __init__(self, n_features, eff_name, dropout_probability):\n        super(ClassificationModel, self).__init__()\n        self.model = EfficientNet.from_pretrained(f'efficientnet-{eff_name}')\n        self.model.classifier= nn.Sequential(\n            nn.Linear(1000, n_features),\n            nn.ReLU(inplace = True),\n            nn.Dropout(dropout_probability),\n            nn.Linear(n_features, len(labels)),\n        )\n        \n    def forward(self, input):\n        return self.model(input)","1b404344":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WB\")\n\nwandb.login(key=secret_value_0)","4634846a":"#Create our config for seaching optimal hyperparameters\nimport math\nsweep_config = {\n                'method': 'random',\n                'metric': {'goal': 'minimize', 'name': 'Test Loss'},\n                'parameters': {\n                    'batch_size': {\n                        'distribution': 'q_log_uniform',\n                        'max': math.log(70),\n                        'min': math.log(16),\n                        'q': 1\n                    },\n                    'epochs': {'value': 20},\n                    'eff_name':{'values':['b1','b3']},\n                    'dropout_probability':{'values':[0.1, 0.2, 0.05]},\n                    'n_features': {'values': [256, 512]},\n                    'learning_rate': {'distribution': 'uniform',\n                                      'max': 0.0002,\n                                      'min': 0.00001},\n                    'optimizer': {'value': 'adam'},\n                    'decay':{'distribution': 'uniform',\n                                      'max': 0.0001,\n                                      'min': 0.000001},\n                }\n }\nsweep_id = wandb.sweep(sweep_config, project=\"test\")","139938a2":"import time\n\ndef train(config=None):\n    start_time_sec = time.time()\n    print('Train called')\n    \n    #Start a new run\n    with wandb.init(project='test', config=config):\n        config = wandb.config\n\n        #Create dataloader\n        train_dataloader = DataLoader(\n            train_dataset, \n            batch_size=config.batch_size, \n            shuffle=True\n            )\n        val_dataloader = DataLoader(\n            val_dataset, \n            batch_size=config.batch_size, \n            shuffle=True)\n        \n        #Initialize the model\n        torch.cuda.empty_cache()\n        model = ClassificationModel(config.n_features, config.eff_name, config.dropout_probability)\n        model.to(DEVICE)\n\n        #Set optimizer and loss \n        if config.optimizer == \"sgd\":\n            optimizer = torch.optim.SGD(\n                model.parameters(),\n                lr=config.learning_rate, \n                momentum=0.9,\n                weight_decay=config.decay\n                )\n        elif config.optimizer == \"adam\":\n            optimizer = torch.optim.Adam(\n                model.parameters(), \n                lr=config.learning_rate,\n                weight_decay=config.decay)\n        loss_fn = nn.CrossEntropyLoss()\n\n        for epoch in range(1, config.epochs+1):\n\n            # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n            model.train()\n            train_loss = 0.0\n            num_train_correct = 0\n            num_train_examples = 0\n\n            for batch in train_dataloader:\n                # Compute prediction error\n                x = batch[0].to(DEVICE)\n                y = batch[1].to(DEVICE)\n                yhat = model(x)\n                loss = loss_fn(yhat, y)\n                \n                # Backpropagation\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                train_loss += loss.data.item() * x.size(0)\n                num_train_correct += (torch.max(yhat, 1)[1] == y).sum().item()\n                num_train_examples += x.shape[0]\n\n            train_acc = num_train_correct \/ num_train_examples\n            train_loss = train_loss \/ len(train_dataloader.dataset)\n\n            # --- EVALUATE ON VALIDATION SET -------------------------------------\n            model.eval()\n            val_loss = 0.0\n            num_val_correct = 0\n            num_val_examples = 0\n            example_images =[]\n\n            for batch in val_dataloader:\n                x = batch[0].to(DEVICE)\n                y = batch[1].to(DEVICE)\n                yhat = model(x)\n                loss = loss_fn(yhat, y)\n                pred = yhat.max(1, keepdim=True)[1]\n\n                val_loss += loss.data.item() * x.size(0)\n                num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n                num_val_examples += y.shape[0]\n\n            val_acc = num_val_correct \/ num_val_examples\n            val_loss = val_loss \/ len(val_dataloader.dataset)\n            \n            wandb.log({\n                \"Test Accuracy\": val_acc,\n                \"Test Loss\": val_loss\n                })\n          \n        # END OF TRAINING LOOP\n        end_time_sec = time.time()\n        total_time_sec = end_time_sec - start_time_sec\n        print()\n        print('Time total:     %5.2f sec' % (total_time_sec))","b94c8efc":"wandb.agent(sweep_id, function=train, count=6)","ece41d79":"## This way you can try automatically generate many experiments to get better results ","c9d9649d":"[How to login with secret](https:\/\/www.kaggle.com\/product-feedback\/114053\/)","e49f046a":"[My best sweep](https:\/\/wandb.ai\/roman_siomko\/test\/sweeps\/fc5ihn8v?workspace=user-roman_siomko)","8f192445":"# Let's look at the magic\n","f05c87f0":"[![Image from Gyazo](https:\/\/i.gyazo.com\/8ad4f7418984a57ad25f2351c352e309.png)](https:\/\/gyazo.com\/8ad4f7418984a57ad25f2351c352e309)","f743a7c4":"[![Image from Gyazo](https:\/\/i.gyazo.com\/f08bda29c79b1fc1eff0217741d4d96b.png)](https:\/\/gyazo.com\/f08bda29c79b1fc1eff0217741d4d96b)","110f1074":"[![Image from Gyazo](https:\/\/i.gyazo.com\/f727bbd5c604a36759dd34ebba3db376.png)](https:\/\/gyazo.com\/f727bbd5c604a36759dd34ebba3db376)\n# Weights & Biases\nWeights & Biases is the machine learning platform for developers to build better models faster. Use W&B's lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues.\n"}}