{"cell_type":{"847b19b0":"code","f15fddb4":"code","326248a3":"code","4c288db1":"code","7aa72f61":"code","31d043f1":"code","e175c1b6":"code","3f312328":"code","c4992fc5":"code","8e331b93":"code","8ee361be":"code","32f53d66":"code","29e52994":"code","c0a1a122":"code","ba8abdfe":"code","b7b964ae":"code","3309fa48":"code","2e8200aa":"code","93a39d3d":"code","114c72a0":"code","91fc53e2":"code","09d39ced":"code","6ac05f21":"code","24ad4aba":"code","2ee4305a":"code","efceb00c":"code","1ea33924":"code","70b3ce20":"code","1b8660a6":"code","723ab993":"code","a20f60ed":"code","b7b06847":"code","73d57aca":"code","94f3c5f3":"code","7efe034d":"code","f8f37a64":"code","696a688f":"code","5b67723e":"code","5f50523d":"code","49af1ec8":"code","1c8533a5":"code","022c4da2":"code","1efdbf31":"code","b1036529":"code","978553b6":"markdown","f6571623":"markdown","8d710fcc":"markdown","e7e5f21e":"markdown","d4caffd3":"markdown","333e98d5":"markdown","bd9533dd":"markdown","05a63bec":"markdown","a4c23070":"markdown","7f4d4e31":"markdown","8fcfccdb":"markdown","fbc2578a":"markdown","79bb8713":"markdown","b467391a":"markdown","2da7cf7b":"markdown","2db9ac3d":"markdown","8e0e1564":"markdown","ad6ac3ba":"markdown","66e51305":"markdown","52250fd9":"markdown","77872d52":"markdown","4187b5a9":"markdown","f906b1fd":"markdown","2d3ddba0":"markdown","2317d810":"markdown","bef3d689":"markdown","3be91a53":"markdown","2cf58f32":"markdown","a83ade38":"markdown","210724cd":"markdown","78ab3681":"markdown","37fa68f3":"markdown","92c3cc08":"markdown","070e469e":"markdown","7e605d95":"markdown","14c704dd":"markdown","3028a6d2":"markdown","6387681e":"markdown","31403f51":"markdown","7c893256":"markdown","147d148b":"markdown","1b0a466b":"markdown","34fefc32":"markdown","3407d9ac":"markdown","9555c895":"markdown","895a9a74":"markdown"},"source":{"847b19b0":"import numpy as np\nimport pandas as pd\n#basic data science libraries for data manipulation\n\nimport warnings #Used to hide unnecessary warning messages\n\nwarnings.simplefilter('ignore')\n\n\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n#plotting libraries\nsns.set(style='darkgrid', palette='Set3')\n\n\n\nimport plotly as py \nfrom plotly import subplots\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objects as go  \n#for interactive plotting\ninit_notebook_mode(connected=True)\npy.offline.init_notebook_mode (connected = True)\n\n\n\n\nfrom wordcloud import WordCloud\nimport nltk\nimport string\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nimport random\nimport statistics\nfrom nltk.classify import NaiveBayesClassifier\n#text data manipulation, visualization and classification\n\n\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential\nfrom keras.layers import SpatialDropout1D\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\n#deep learning libraries for further application\n\n","f15fddb4":"languages=pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelLanguages.csv')\nkernels=pd.read_csv('\/kaggle\/input\/meta-kaggle\/Kernels.csv')\nkernelTags=pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelTags.csv')\ntags=pd.read_csv('\/kaggle\/input\/meta-kaggle\/Tags.csv')\nversions=pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelVersions.csv')","326248a3":"languages.tail()","4c288db1":"languages= languages[['Id','DisplayName','IsNotebook']]","7aa72f61":"kernels.tail()","31d043f1":"kernels=kernels[['Id','CurrentKernelVersionId','FirstKernelVersionId',\n                'MadePublicDate','Medal','TotalViews','TotalComments','TotalVotes']]\n","e175c1b6":"kernelTags.tail()","3f312328":"tags.tail()","c4992fc5":"tags=tags[['Id','Name']]","8e331b93":"versions.tail()","8ee361be":"versions= versions[['ScriptId','ScriptLanguageId','AuthorUserId','VersionNumber','Title']]","32f53d66":"#Starting by merging the data\n\nversions.drop_duplicates(subset='ScriptId',keep='last',inplace=True)\n\nversions =versions.join(languages.set_index('Id'), on='ScriptLanguageId', how='left')\n\nkernels =versions.join(kernels.set_index('Id'), on='ScriptId')\n\nkernelTags= kernelTags.join(tags.set_index('Id'),on='TagId', how='left')\n\n","29e52994":"#Stacking the Tags into rows and merging into a main dataframe\n\na=kernelTags[kernelTags.duplicated('KernelId')] [['KernelId','Name']]\nc=kernelTags.drop_duplicates(subset='KernelId', keep='first').join(a.set_index('KernelId'),on='KernelId', how='left',rsuffix='_2')\n\na=c[c.duplicated('KernelId')] [['KernelId','Name_2']]\nd=c.drop_duplicates(subset='KernelId', keep='first').join(a.set_index('KernelId'),on='KernelId', how='left',rsuffix='_3')\n\na=d[d.duplicated('KernelId')] [['KernelId','Name_2_3']]\ne=d.drop_duplicates(subset='KernelId', keep='first').join(a.set_index('KernelId'),on='KernelId', how='left',rsuffix='_4')\n\na=e[e.duplicated('KernelId')] [['KernelId','Name_2_3_4']]\nf=e.drop_duplicates(subset='KernelId', keep='first').join(a.set_index('KernelId'),on='KernelId', how='left',rsuffix='_5')\n\ntags=f.drop_duplicates('KernelId',keep='first')\nNames={'Name':'Tag1','Name_2':'Tag2','Name_2_3': 'Tag3','Name_2_3_4':'Tag4','Name_2_3_4_5':'Tag5'}\ntags.rename(Names, axis=1, inplace=True)\ntags.drop(columns=['TagId','Id'],inplace=True)","c0a1a122":"kernels =kernels.join(tags.set_index('KernelId'), on='ScriptId', how='left')\nkernels.drop(columns=['ScriptLanguageId','CurrentKernelVersionId','FirstKernelVersionId'],inplace=True)\nkernels.tail()","ba8abdfe":"# We only take in consideration published works\nkernels.dropna(axis=0,subset=['MadePublicDate'], inplace=True)\n\n# we fill the missing data in Versions with 1 \nkernels.loc[kernels['VersionNumber'].isna()].fillna(1, inplace=True)\n\n# we fill the missing data in language with other and in Isnotbook as False\nkernels['DisplayName'].fillna('Other', inplace=True)\nkernels['IsNotebook'].fillna(False,inplace=True)","b7b964ae":"print(kernels['Medal'].isna().count())","3309fa48":"kernels.drop('Medal',axis=1,inplace=True)\nkernels.reset_index(drop=True,inplace=True)\nkernels.head()","2e8200aa":"label=['notebook','script']\ntrace1=go.Pie(labels=kernels['DisplayName'],domain=dict(x=[0, 0.5]),name='Language',marker_colors=['#ADFFA2','darkred'])\ntrace2=go.Pie(labels=kernels['IsNotebook'], domain=dict(x=[0.5, 1.0]), name='Notebook', marker_colors=['#6ED1E9','grey'])\n\nfig1 = subplots.make_subplots(rows = 1, cols = 2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\nfig1.add_trace(trace1,row=1,col=1)\nfig1.add_trace(trace2,row=1,col=2)\n\nfig1.update_traces(hoverinfo='label+percent+name', textinfo='label',textfont_size=20,marker=dict(line=dict(color='#000000', width=2)))\nfig1.update(layout_title_text='Language and notebook types',layout_showlegend=False)\n\niplot(fig1)","93a39d3d":"languagePlot= kernels.groupby('DisplayName').sum().reset_index()\nnotebookPlot= kernels.groupby('IsNotebook').sum().reset_index()\n\nlanguageCount= kernels.groupby('DisplayName').count().reset_index()\nnotebookCount= kernels.groupby('IsNotebook').count().reset_index()\n\ntrace3= go.Bar(x=languagePlot['DisplayName'],y=languagePlot['TotalVotes']\/languageCount['TotalVotes'], name='Votes',marker_color='lightgreen')\ntrace4= go.Bar(x=languagePlot['DisplayName'],y=languagePlot['TotalComments']\/languageCount['TotalComments'], name ='Comments')\n\ntrace5= go.Bar(x=notebookPlot['IsNotebook'],y=notebookPlot['TotalVotes']\/notebookCount['TotalVotes'], name='Votes',marker_color='#FF9700')\ntrace6= go.Bar(x=notebookPlot['IsNotebook'],y=notebookPlot['TotalComments']\/notebookCount['TotalComments'], name ='Comments',marker_color='purple')\nfig2= subplots.make_subplots(rows= 1 , cols = 2)\n\nfig2.append_trace(trace3,row=1,col=1)\nfig2.append_trace(trace4,row=1,col=1)\nfig2.append_trace(trace5,row=1,col=2)\nfig2.append_trace(trace6,row=1,col=2)\n\nfig2.update(layout_title_text='Average popularity of languages',layout_showlegend=False)\nfig2.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig2.update_yaxes(showticklabels=False)\n\niplot(fig2)","114c72a0":"kernels['AuthorUserId']= kernels['AuthorUserId'].astype(str)\n\nauthorPlot=kernels.groupby(by=kernels['AuthorUserId']).count().reset_index()\nauthorPlot.sort_values(by='Title', ascending=False,inplace=True)\n\n\nauthorPlot['Title'].head()\nauthorPlot= authorPlot[1:]\nprint('Average number of kernels per user:',authorPlot['Title'].mean())\n","91fc53e2":"authorPlot= authorPlot[:30]\n\n\nf=plt.subplots(figsize=(18,8))\nf=sns.barplot(x=authorPlot['AuthorUserId'],y=authorPlot['Title'],color='orange')\nf.set_title('Number of lernels by most active users')\nf.set_xticklabels(f.get_xticklabels(), rotation=45)\na=f.set(xlabel='User Id', ylabel='Number of Kernels')","09d39ced":"authorPlot=kernels.groupby(by=kernels['AuthorUserId']).sum().reset_index()\nauthorPlot.sort_values(by='TotalVotes', ascending=False,inplace=True)\nauthorPlot= authorPlot[:30]\n\nf=plt.subplots(figsize=(18,8))\nf=sns.barplot(x=authorPlot['AuthorUserId'],y=authorPlot['TotalVotes'],color='orange')\nf.set_title('Number of votes by users')\nf.set_xticklabels(f.get_xticklabels(), rotation=45)\na=f.set(xlabel='User Id', ylabel='Number of votes')","6ac05f21":"kernels['MadePublicDate']=pd.to_datetime(kernels['MadePublicDate'])\ndates=kernels.groupby(kernels['MadePublicDate']).count().reset_index()\n\ntrace7=go.Line(y=dates['Title'],x=dates['MadePublicDate'], name='Published kernels')\ntrace8=go.Line(y=dates['Title'][-425:-365],x=dates['MadePublicDate'][-425:-365],name='Published kernels')\n\nfig3 = subplots.make_subplots(rows = 2, cols = 1)\nfig3.add_trace(trace7,row=1,col=1)\nfig3.add_trace(trace8,row=2,col=1)\n\nfig3.update(layout_title_text='Trends with a zoom on a month in 2019',layout_showlegend=False)\niplot(fig3)","24ad4aba":"#we use regular plotting here as the y axis is not important since we're doing a 6 months rolling mean to see the evolution\nplt.figure(figsize=(20,6))\nplt.plot(dates['MadePublicDate'],dates['Title'].rolling(180).mean(),linewidth=3, c='black')\na=plt.title('6 months rolled mean analysis of the Number of published kernels ')","2ee4305a":"trace9=go.Line(y=dates['TotalViews'],x=dates['MadePublicDate'],name='Views')\ntrace10=go.Line(y=dates['TotalVotes'],x=dates['MadePublicDate'],name='Votes')\ntrace11=go.Line(y=dates['TotalComments'],x=dates['MadePublicDate'], name='Comments')\n\n\nfig4 = subplots.make_subplots(rows = 3, cols = 1)\nfig4.add_trace(trace9,row=1,col=1)\nfig4.add_trace(trace10,row=2,col=1)\nfig4.add_trace(trace11,row=3,col=1)\n\nfig4.update(layout_title_text='Views, votes and comments over time',layout_showlegend=False)\niplot(fig4)","efceb00c":"#as we did earlier, 6 months roll are represented in basic plotting\ndates=kernels.groupby(['MadePublicDate']).sum().reset_index()\nf=plt.figure(figsize=(20,10))\nax1=f.add_subplot(2,1,1)\nax1.plot(dates['MadePublicDate'],dates['TotalViews'].rolling(180).mean(),label='Views', linewidth=3, c= 'blue')\nax1.set_title('6 months rolled mean analysis of the Views of published kernels ')\nax1.legend()\n\nax2=f.add_subplot(2,1,2)\nax2.plot(dates['MadePublicDate'],dates['TotalVotes'].rolling(180).mean(),label='Votes', linewidth=3, c= 'red')\nax2.plot(dates['MadePublicDate'],dates['TotalComments'].rolling(180).mean(), label='Comments', linewidth=3, c= 'green')\nax2.legend()\na=ax2.set_title('6 months rolled mean analysis of the Votes and comments of published kernels ')","1ea33924":"percentageVotes=kernels['TotalVotes'].sum()\/kernels['TotalViews'].sum() * 100\npercentageComments=kernels['TotalComments'].sum()\/kernels['TotalViews'].sum() * 100\n\nprint(float(\"{0:.4f}\".format(percentageVotes)),'%   votes\/views')\nprint(float(\"{0:.4f}\".format(percentageComments)),'%   comments\/views')\n\n\ncorrelations=kernels[['TotalViews','TotalVotes','TotalComments']]\n\nf=plt.figure(figsize=(10,8))\nf=sns.heatmap(correlations.corr(),annot=True)\na=f.set_title('Matrix of correlation')","70b3ce20":"versionsPlot=kernels\nversionsPlot = kernels[kernels['VersionNumber'] < kernels['VersionNumber'].quantile(0.9999)]\nversionsPlot = versionsPlot[kernels['TotalVotes'] < versionsPlot['TotalVotes'].quantile(0.9999)]\n\n\n\ntrace12=go.Scattergl(x=versionsPlot['VersionNumber'],y=versionsPlot['TotalVotes'], mode='markers', name='Votes', marker_color='#3C8E6A')\ntrace13=go.Scattergl(x=versionsPlot['VersionNumber'],y=versionsPlot['TotalComments'], mode='markers', name='Comments',marker_color='#893636')\n# an hex plot might have been clearer\nfig5= go.Figure()\nfig5.add_trace(trace12)\nfig5.add_trace(trace13)\nfig5.update_layout(title='Distribution of votes\/comments by version number')\nfig5.update_xaxes(title_text='Version')\nfig5.update_traces(marker=dict(line=dict(color='#000000', width=0.5)))\n\n\niplot(fig5)","1b8660a6":"tagsPlot= kernelTags.groupby(['Name']).count().reset_index()\ntagsPlot.sort_values(by='KernelId',ascending=False, inplace=True)\ntagsPlot=tagsPlot.head(12)\n\n\nplt.figure(figsize=(20,7))\nf=sns.barplot(x=tagsPlot['Name'],y=tagsPlot['KernelId'])\nf.set_title('Most used tags')\nf.set_xlabel('Tag')\na=f.set_xticklabels(f.get_xticklabels(), rotation=15)","723ab993":"kernelTags =kernels.join(kernelTags.set_index('KernelId'), on='ScriptId', how='left')\nkernelTags= kernelTags[['ScriptId','TotalVotes','Name']]\n\ntagsPlot= kernelTags.groupby(['Name']).sum().reset_index()\ntagsPlot.sort_values(by='TotalVotes',ascending=False, inplace=True)\ntagsPlot=tagsPlot.head(12)\n\n\nplt.figure(figsize=(20,7))\nf=sns.barplot(x=tagsPlot['Name'],y=tagsPlot['TotalVotes'])\nf.set_title('Tags attracting most votes')\nf.set_xlabel('Tag')\na=f.set_xticklabels(f.get_xticklabels(), rotation=15)","a20f60ed":"kernels.reset_index(drop=True, inplace=True)\nkernels['Title']= kernels['Title'].astype(str)\n\nkernels['Title'].replace('_', ' ', inplace=True)\nkernels['Title'].replace('.', ' ', inplace=True)\n\nmore=[' ',\"'s\",\"''\",\"``\",'-','_','\u2018\u2019',',']\nstop= set(stopwords.words('english')+list(string.punctuation)+more)\n","b7b06847":"words=list()\nfor i in range(0,len(kernels)):\n    for w in word_tokenize(kernels['Title'][i]):\n        if w.lower() not in stop:\n            words.append(w.lower())\n            \nfrequent=pd.DataFrame(pd.Series(nltk.FreqDist(words)))\nfrequent.reset_index(inplace=True)\nfrequent.sort_values(by=(0),ascending=False, inplace= True)\n\nf,ax=plt.subplots(figsize=(20,8))\nf=sns.barplot(frequent['index'][:12],frequent[0][:12])\na=ax.set(title='Barplot of the most used words',xlabel='Words', ylabel='')","73d57aca":"#wordcloud\nwordcloud = WordCloud(background_color = 'white',width=2000, height=900, max_words = 100).generate(' '.join(kernels['Title']))\nf,ax=plt.subplots(figsize = (200, 120) ) \nplt.imshow(wordcloud) \na=plt.axis('off')","94f3c5f3":"kernelsBins=kernels.copy()\nbins=[-1,0,100000]\nlabels=['1','2']\nkernelsBins['TotalVotes']= pd.cut(kernelsBins[\"TotalVotes\"], bins , labels=labels)\n\n#We only take the most recent 100k as the old data might be misleading and\/or out of date\nrecentKernels=kernelsBins[-100000:].copy()\n\n\n#Sampling 15k data with same distribution as the population\nsampleML=kernelsBins[320000:335000]\nsampleML.reset_index(inplace=True, drop=True)\n\n#Displaying the percentage\nsampPercent=sampleML.loc[sampleML['TotalVotes']=='2'].count() [1]\/sampleML['TotalVotes'].count() \nfullPercent=recentKernels.loc[recentKernels['TotalVotes']=='2'].count() [1] \/recentKernels['TotalVotes'].count() \n\nprint('General percentage' ,fullPercent*100,'%')\nprint('Percentage of the sample',sampPercent*100,'%')\n","7efe034d":"#extracting the word frequence distribution to create features later on\nwords=list()\nfor i in range(0,len(sampleML)):\n    for w in word_tokenize(sampleML['Title'][i]):\n        if w.lower() not in stop:\n            words.append(w.lower())\n            \nfreq=list(dict(sorted(nltk.FreqDist(words).items(), key=lambda item: item[1], reverse=True)))[:5000]\n\n#a function to find the features from the words\ndef find_feat(text):\n    word=set(text)\n    features={}\n    for w in freq:\n        features[w]=(w in word)\n    return features\n\n#a function to clean the data from stopwords after tokenizing\ndef clean(text):\n    l=list()\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            l.append(w.lower())\n    return l\n            \n","f8f37a64":"# preprocessing, extracting features and splitting the data into train\/test \ndocuments=list()\nfor i in range(0,len(sampleML)):\n     documents.append([clean(sampleML['Title'][i]), sampleML['TotalVotes'][i]])\n\ndoc=list()\nfor i in range(0,len(sampleML)):\n    for w in clean(sampleML['Title'][i]):\n        doc.append((w.lower()))\nratio=int(len(doc)\/15)\n\nrandom.shuffle(documents)\nfeaturesets = [(find_feat(title), votes) for (title, votes) in documents]\nrandom.shuffle(featuresets)\n\nsplit=int(len(featuresets)-len(featuresets)\/5)\ntrain=featuresets[:split]\ntest=featuresets[split:]\n\n","696a688f":"clf_naiveBayes=nltk.NaiveBayesClassifier.train(train)\nprint(\"Classifier accuracy percent:\",(nltk.classify.accuracy(clf_naiveBayes, test))*100)","5b67723e":"clf_naiveBayes.show_most_informative_features(20)","5f50523d":"#we use 100k data \n#we clean and prepare data before using it\nrecentKernels.reset_index(drop=True,inplace=True)\nfor i in range (0,len(recentKernels)):\n    recentKernels['Title'][i]=clean(recentKernels['Title'][i])\nfor i in range (0,len(recentKernels)):\n    space=' '\n    recentKernels['Title'][i]=space.join(recentKernels['Title'][i])\n\n\n\nmax_features = 5000\ntokenizer = Tokenizer(nb_words=max_features, split=' ')\ntokenizer.fit_on_texts(recentKernels['Title'].values)\nX = tokenizer.texts_to_sequences(recentKernels['Title'].values)\nX = pad_sequences(X, maxlen=15)\nY = pd.get_dummies(recentKernels['TotalVotes']).values\nx_train, x_test, y_train, y_test = train_test_split(X,Y, random_state = 42,shuffle=True)\n\nprint(x_train.shape,y_train.shape)","49af1ec8":"#LSTM squential model\nlstmModel = Sequential()\nlstmModel.add(Embedding(max_features, 100,input_length=X.shape[1]))\nlstmModel.add(SpatialDropout1D(0.4))\n\nlstmModel.add(LSTM(100, dropout=0.3, recurrent_dropout=0.2))\n\nlstmModel.add(Dense(2,activation='softmax'))\nlstmModel.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n#Using categorical_crossentropy for our classification problem\n\nearlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n#Define an earlystop as our data does not carry an obvious pattern and might give our model a hard time learninf\n\nprint(lstmModel.summary())\nhistory=lstmModel.fit(x_train, y_train, validation_split=0.1 ,batch_size=256, epochs=100, callbacks=[earlystop])\n\nscore,accu = lstmModel.evaluate(x_test, y_test, batch_size = 64)\nprint((accu)*100)","1c8533a5":"f=plt.figure(figsize=(20,6))\nplt.plot(history.history['accuracy'], label='train', c='green',linewidth=3)\nplt.plot(history.history['val_accuracy'], label='test' , c='blue',linewidth=3)\nplt.title('LSTM model Accuracy')\na=plt.legend()","022c4da2":"# GRU with glove embeddings and two dense layers\ngruModel = Sequential()\ngruModel.add(Embedding(max_features, 100,input_length=X.shape[1]))\ngruModel.add(SpatialDropout1D(0.3))\n\ngruModel.add(GRU(50,dropout=0.1, recurrent_dropout=0.2, return_sequences=True))\ngruModel.add(GRU(100, dropout=0.1, recurrent_dropout=0.2))\n\n\ngruModel.add(Dense(2,activation='softmax'))\ngruModel.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n#Using categorical_crossentropy for our classification problem\n\nearlystop = EarlyStopping(monitor='val_loss', patience=5, verbose=0)\n#Define an earlystop as our data does not carry an obvious pattern and might give our model a hard time learninf\n\nprint(gruModel.summary())\nhistory=gruModel.fit(x_train, y_train, validation_split=0.1, batch_size=256, epochs=100, callbacks=[earlystop])\n\nscore,acc = gruModel.evaluate(x_test, y_test, batch_size = 64)\nprint((acc)*100)","1efdbf31":"plt.figure(figsize=(20,6))\nplt.plot(history.history['accuracy'], label='train', c='green',linewidth=3)\nplt.plot(history.history['val_accuracy'], label='test' , c='blue',linewidth=3)\nplt.title('gru model Accuracy')\na=plt.legend()","b1036529":"newTitles=[['There is only one thing we say to death: Not today'],\n          ['How to choose a topic and layout for a new Kaggle kernel'],\n          ['Kaggle meta data analysis to boost your kernel exposure'],\n          ['What is dead may never die'],\n          ['Attracting Kaggle users - Using plotly visualisations and DL NLP predictions'],#Using the words from the NaiveBayes suggestions\n          ['You know nothing, Jon Snow']] \n\nlabels=['0 votes','1 vote or more']\n\nprint ('LSTM model')\nfor i in range (0,len(newTitles)):\n    title=newTitles[i]\n    sequence=tokenizer.texts_to_sequences(title)\n    padded=pad_sequences(sequence, maxlen=15)\n    prediction=lstmModel.predict(padded)\n    print( title,'   ==>   ',labels[np.argmax(prediction)],'\\n')\n    \nprint ('\\n \\ngru model')\nfor i in range (0,len(newTitles)):\n    title=newTitles[i]\n    sequence=tokenizer.texts_to_sequences(title)\n    padded=pad_sequences(sequence, maxlen=15)\n    prediction=gruModel.predict(padded)\n    print( title,' ==> ',labels[np.argmax(prediction)],'\\n')\n","978553b6":"The medal feature does not make sense (93% missing data), so we drop it.","f6571623":"## Votes Views Comments","8d710fcc":"## Classification","e7e5f21e":"* In order for our model to make sense, we divide our data into 2 bins, a bin with 0 votes and a bin with at least 1 vote.\n* Then we sample the data as we have more than 390k row and that is practically useless for most models.\n* We take the last 100k entries as the old data can be outdated misleading.\n* We sample 15k for our Naive Bayes classiier model.","d4caffd3":"* We only take in consideration published works as the end goal is to evaluate the popularity of kernels.\n* The kernels left are all published, so they have at least 1 version, we fill the missing data in Versions with 1 as the first version.\n* We notice some kernels do not have a Language, after checking some example, we find that there are other languages used that kaggle does not classify. Example:\n\n![Screen%20Shot%202020-10-19%20at%2012.42.43%20AM.png](attachment:Screen%20Shot%202020-10-19%20at%2012.42.43%20AM.png)\n\n* Those languages are naturally not adapted to Notebooks, so we fill the Notebook missing values with False.","333e98d5":"![kaggle.png](attachment:kaggle.png)","bd9533dd":"We try to define some patterns and then see how some features influence the popularity of a kernel (views, votes and comments)","05a63bec":"# 5. NLP Deep learning application","a4c23070":"In other words, in average, a vote comes from 163 views, and a comment comes from 704 views.","7f4d4e31":"In this section we deal with\n* Missind data\n* Illogical data \n* Outliers\n","8fcfccdb":"This dataset will act as an intermediate to map the tags used","fbc2578a":"# 0. Data exploration","79bb8713":"#### Getting familiar with data and reducing it","b467391a":"# 4. Text classification","2da7cf7b":"## Value of words","2db9ac3d":"As a student or an aspiring data scientist, looking for a topic to tackle can be a bit challenging,\nespecially since most kernels do not get the attention, upvotes, or views they deserve.\n\nIn this kernel, we will analyse the meta data behind that might boost the popularity of a Kaggle kernels.\n\nThe work will be focusing on logical and practical sides, we analyse the features that might influence people into clicking to open the kernel page.\n\nWe will also analyse the title data through NLP and in the end, we try to train a classifier for attractive titles.\n\n\n","8e0e1564":"* All our models have around 70% accuracy.\n* After all, our text data alone does not have a treshold or obvious or logical difference on the votes.\n* The models have a tough time learning anything from data.\n* However, a 70% accuracy is a rather good score for classification, and our example confirmed the good but not great job of our model.\n* 'You know nothing, Jon Snow' got a False positive prediction, our model might like some Game of Thrones characters more than others.\n* We use the title who had the unanimity between the 2 DL models and our previous ML model.\n","ad6ac3ba":"First, data have to be cleaned from stopwords, punctuation and other noise data","66e51305":"![](https:\/\/i.pinimg.com\/originals\/8d\/e1\/07\/8de10711546dd4a1acbee5f2a2529da9.gif)","52250fd9":"This dataset has the required data to map different values and also sum up the kernels into their final version form.","77872d52":"Our data is numerous, diverse ,repetitive and divided into 5 dataframes, in this part, we will gather the most valuable data into one main dataframe that will be used for the analysis.","4187b5a9":"* Notice the trend gets the lowest on the 1, 8, 15, 21, 29 of September 2019... Those are sundays, Users tend to publish less on Sundays.\n* There is no noticeable seasonality on a higher scale, but we notice an evolution.\n* With the 6 months rolling mean, we notice that Kaggle was growing so fast until 2017 where it dropped suddenly.\n* Kaggle notoriety started growing again and recorded a tremendous grow during the 2020 pandemic period (March 2020)","f906b1fd":"* Most of the upvoted and commented kernels have a version superior than 1, this means that coreccting, improving or modifying kernels is a thing that helps kernel stay on the first page and brings more viewers, votes and comments.\n* On the other hand, a lot of version does necessarily imply more votes or comments.","2d3ddba0":"After seeing that, we try a more pratical application using some neural network to train a deep learning model and apply it to predict a good title for this kernel.","2317d810":"This analysis will focus on those points:\n\n* Is it better to write the kernel using Python or R?  \n* Is it better to make it a notebook or a script?     \n* Publication date (time series analysis) \n* Views - votes - coments correlation and association\n* Do more experienced users have more appeal? \n* What are the most important tags? \n* Is it better to keep updating the kernel?\n\n\n* Does the title matter? (NLP analysis, classification and application)","bef3d689":"This is the last version of our main dataframe, clean and ready to use.","3be91a53":"* 'gpu' is the most used tag as it is automatically enabled if gpu is used in kernel. (over 45k kernels)\n* 'beginner' is the second most used tag, it also is the tag attracting most people in Kaggle.\n* Data science basics tags are also a good way to attract people as they rank very high, examples: 'data visualization, exploratory data analysis, featureenginnering and data cleaning\n* There are also some Machine learning and Deep learning tagged kernels who get attention, examples: ' Classification, deep learning, learn, nlp, cnn, xgboost'\n* We conclude that the most visited kernels are the tutorials, guides and beginner kernels.","2cf58f32":"* NaiveBayes gives a rough idea about the words values.\n* The second column represents the bin the word is most likely to belong to (2: 1 vote or more, 1:0 votes).\n* The third column is the probablity of the prediction.\n* Naive Bayes classifier is unstable and another execution of the same code mighr give different outcome, but the results are reliable.\n* Notice how the word \"Visualization\" is more liked as \"visualisation\". (this might not appear in all executions)","a83ade38":"#### Importing libraries and data","210724cd":"## Dates","78ab3681":"The visualizations will be done interactively with plotly, however, as plotly might get slow and consuming, we will use the regular plotting libraries for more general plotting.","37fa68f3":"* We build an LSTM model and double gru layers model.\n* We use a rather low number of neutrons as the data we have is not very expressive and a very deep or big model might be an overkill.","92c3cc08":"We end up with a main dataframe with 15 columns resuming our data in a very clear way and carrying the most noticeable or valuable data. <br>\nWe will be using copies of this dataframe for most analysis, however, we might need another dataframe at some point.","070e469e":"Has valuable data : Medals, Votes, Date, Views\nAnd has a lot of feihua data to clean\n\nData to add : Language (R\/Python)\n              Style (Script\/Notebook)\n              Tags\n              \nData to handle : Clean normal process (missing data, features engineering, types, distributions)\n                 Text data manipulation (Create title case and manipulate) \n                ","7e605d95":"# 1. Data wrangling","14c704dd":"* We have a clear value that is an outlier and does not represent the rest of data, so we take it off\n* In average, users creates and publishes 2 kernels, however, some users are more active than others\n\n* There are nevertheless many users with over a 100 different published kernels.\n\n* 2603295, 1162990, 505747, 495305, 417337 appear in both graphs, it logically yields the most kernels a User publishes, the more votes he would get, however, it's not always true as 25\/30 did not appear in both lists.","3028a6d2":"Languages dataset contains the coding language and notebook options, these are important features but has to mapped into the main dataset.","6387681e":"# 3. Analysis and visualizations","31403f51":"* Python is used in 90% of the kernels, showing a large domination.\n* Notebooks are only used in 42% of the cases.\n* Python and R have almost the same average votes and comments, tiny advantage for Python.\n* The R kernels have in average more comments than Python.\n* Other languages do not get as much importance as the main 2.\n* Scripts have almost twice the popularity of Notebooks, (97% more votes and 91% more comments)\n* Writing in a script is likely to get you twice votes and coments as notebook.\n","7c893256":"## Tags","147d148b":"Gotta join tags tab and tags Ids to later join them with main tab data","1b0a466b":"## Version number","34fefc32":"## Author Users","3407d9ac":"## Languages","9555c895":"# 2. Cleaning","895a9a74":"* As the number of published kernels, the votes, views and comments follow the same trends.\n* These last 6 months (April-October 2020) are the best time to publish a kernel, as the number of comments and votes is reaching its maximum so far.\n* In April October 2020, there are 60% more votes in average than 2018-2020 period."}}