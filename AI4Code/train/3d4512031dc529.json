{"cell_type":{"763dd417":"code","c3b5ef2c":"code","9857cc61":"code","439c0850":"code","1c57fe2d":"code","2dc0a87a":"code","9452a2b2":"code","6e18e904":"code","c04bc6f6":"code","99b92af8":"code","40fb8663":"code","d3380fbb":"code","f7b5194c":"code","b4652e05":"code","2cfc160a":"code","933d9740":"code","3e4bee00":"code","cbd61e2a":"code","b5cb0db7":"code","8326857e":"code","af3f919f":"code","edfd7745":"code","09dcfac9":"code","6aa05b06":"code","cf5b41b1":"code","7a931c6c":"code","cd679656":"code","2f3be683":"code","4505d989":"code","320abb89":"code","dba63ddd":"code","e02e8828":"code","c8dc2b89":"code","facba2e7":"code","de106a8c":"code","4017cddc":"code","7658dc22":"code","bf44dff2":"code","cbc1ee1f":"code","cdd1dfa7":"code","8f4b5056":"code","885def49":"code","af659c6d":"code","0fc182c2":"code","dd2ed4d9":"code","b9c44879":"code","e58bdc75":"code","77cede06":"code","a976efd1":"code","d3d035a7":"markdown","ad412756":"markdown","b78a17d9":"markdown","ad8b56d1":"markdown","4ebd75b9":"markdown","d6931877":"markdown","5a434a02":"markdown","34c460c6":"markdown","002eb45d":"markdown","521bde6c":"markdown","d8403f11":"markdown","6ed091ae":"markdown","428beaa7":"markdown","4cda977d":"markdown","2afe2caf":"markdown","d6b97578":"markdown"},"source":{"763dd417":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3b5ef2c":"!ls ..\/input\/seaborn\/","9857cc61":"!pip uninstall -y seaborn\n!mkdir -p \/tmp\/pip\/cache\/\n!cp ..\/input\/seaborn\/seaborn-0.11.0-py3-none-any.whl \/tmp\/pip\/cache\/","439c0850":"!pip install --no-index --find-links \/tmp\/pip\/cache\/ seaborn","1c57fe2d":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2dc0a87a":"def color_generator(number_of_colors):\n        return ['#'+''.join(random.choice('0123456789ABCDEF') for x in range(6)) for i in range(0,number_of_colors)]","9452a2b2":"color_generator(6)","6e18e904":"train_data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')","c04bc6f6":"train_data.head()","99b92af8":"test_data.head()","40fb8663":"train_data.info()","d3380fbb":"train_data.describe()","f7b5194c":"train_data.dropna(inplace=True)","b4652e05":"train_data.groupby('sentiment')['textID'].count().reset_index()","2cfc160a":"data = train_data.groupby('sentiment')['textID'].count().reset_index()\ndata.columns = ['sentiment','count']\nsns.barplot(x=\"sentiment\", y=\"count\", data=data)","933d9740":"train_data.head()","3e4bee00":"train_data.head()","cbd61e2a":"train_data['text_words'] = train_data['text'].apply(lambda x:len(str(x).split()))\ntrain_data['selected_text_words'] = train_data['selected_text'].apply(lambda x:len(str(x).split()))\n","b5cb0db7":"train_data.head()","8326857e":"# train_data[train_data.text_words <= 1]","af3f919f":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    try:\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n    except:\n        print(str1)\n        return 1","edfd7745":"train_data","09dcfac9":"train_data['jaccard_similarity'] = train_data[['text','selected_text']].apply(lambda x:jaccard(x['text'],x['selected_text']),axis = 1)","6aa05b06":"train_data['difference_of_words'] = train_data['text_words'] - train_data['selected_text_words']","cf5b41b1":"train_data","7a931c6c":"sns.displot(train_data, x=\"text_words\", kind='kde',hue=\"sentiment\", fill=True)","cd679656":"sns.displot(train_data, x=\"selected_text_words\", kind='kde',hue=\"sentiment\", fill=True)","2f3be683":"sns.displot(train_data, x=\"difference_of_words\", kind='kde',hue=\"sentiment\", fill=True)","4505d989":"sns.displot(train_data[train_data.sentiment == 'neutral'], x=\"jaccard_similarity\", kind='kde',hue=\"sentiment\", fill=True)","320abb89":"sns.displot(train_data[train_data.sentiment != 'neutral'], x=\"jaccard_similarity\", kind='kde',hue=\"sentiment\", fill=True)","dba63ddd":"# train_data[train_data.sentiment != 'neutral'].shape\nperct = train_data[(train_data.sentiment != 'neutral') & (train_data.jaccard_similarity > 0.95)].shape[0]\/train_data[train_data.sentiment != 'neutral'].shape[0]\nprint(\"No of Positive and Negative Sentiment Tweets with Jaccard Similarity greater then 0.95 are \"+ str(round(perct*100,2)) + \"%\")","e02e8828":"train_data[(train_data.sentiment != 'neutral') & (train_data.jaccard_similarity > 0.95)]['text_words'].quantile([.1, .5,0.8,0.9,1])","c8dc2b89":"import re\nimport string\ndef clean_text(text):\n    \n    \n    text = text.lower()\n    \n#     text = re.sub(r\"won\\'t\", \"will not\", text)\n#     text = re.sub(r\"can\\'t\", \"can not\", text)\n\n#     # general\n#     text = re.sub(r\"n\\`t\", \" not\", text)\n#     text = re.sub(r\"\\`re\", \" are\", text)\n#     text = re.sub(r\"\\`s\", \" is\", text)\n    \n#     text = re.sub(r\"\\'d\", \" would\", text)\n#     text = re.sub(r\"\\`d\", \" would\", text)\n    \n#     text = re.sub(r\"\\'ll\", \" will\", text)\n#     text = re.sub(r\"\\`ll\", \" will\", text)\n    \n#     text = re.sub(r\"\\`t\", \" not\", text)\n#     text = re.sub(r\"\\'t\", \" not\", text)\n    \n#     text = re.sub(r\"\\'ve\", \" have\", text)\n#     text = re.sub(r\"\\`ve\", \" have\", text)\n    \n#     text = re.sub(r\"\\'m\", \" am\", text)\n#     text = re.sub(r\"\\`m\", \" am\", text)\n    \n    text = re.sub(r\"\\d+\", \"\", text) #removing numbers\n    text = text.translate(str.maketrans('', '', string.punctuation)) #removing punctuation\n    text = text.strip() #removing white spaces\n    \n    text = re.sub(r'\\b\\w{1,3}\\b', '', text) #removing words with less then 3 characters\n\n    \n    \n    return text\n    ","facba2e7":"train_data['clean_text'] = train_data['text'].map(clean_text)\ntrain_data['clean_selected_text'] = train_data['selected_text'].map(clean_text)","de106a8c":"from nltk.probability import FreqDist\n\nwords = []\n\nfor sentence in train_data[train_data.sentiment == 'positive']['clean_text']:\n    words.extend(sentence.split())\n\n# print(len(words))\nfdist = FreqDist(words)\n\nwords = pd.DataFrame(fdist.most_common(len(words)),columns = ['word','count'])\nwords['len'] = words['word'].str.len()\nwords.style.background_gradient(cmap='Blues')\ntop_words = words.head(20)\nleast_common_words = words.tail(20)\n\nplt.figure(figsize = (15,6))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"word\", data=top_words).set_title('Most common words in Positive Sentiment Sentences')\n","4017cddc":"plt.figure(figsize = (15,6))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"word\", data=words[(words['len'] > 8) & (words['count'] > 10)].head(20)).set_title('Unique words in Positive Sentiment Sentences')","7658dc22":"tuples = [tuple(x) for x in words[['word','count']].values]\nwordcloud = WordCloud(background_color='white').generate_from_frequencies(dict(tuples))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud, interpolation=\"bilinear\")","bf44dff2":"from nltk.probability import FreqDist\n\nwords = []\n\nfor sentence in train_data[train_data.sentiment == 'negative']['clean_text']:\n    words.extend(sentence.split())\n\n# print(len(words))\nfdist = FreqDist(words)\n\nwords = pd.DataFrame(fdist.most_common(len(words)),columns = ['word','count'])\nwords['len'] = words['word'].str.len()\nwords.style.background_gradient(cmap='Blues')\ntop_words = words.head(20)\n\nplt.figure(figsize = (15,6))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"word\", data=top_words).set_title('Most common words in Negative Sentiment Sentences')\n","cbc1ee1f":"plt.figure(figsize = (15,6))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"word\", data=words[(words['len'] > 8) & (words['count'] > 10)].head(20)).set_title('Unique words in Negative Sentiment Sentences')","cdd1dfa7":"tuples = [tuple(x) for x in words[['word','count']].values]\nwordcloud = WordCloud(background_color='white').generate_from_frequencies(dict(tuples))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud, interpolation=\"bilinear\")","8f4b5056":"from nltk.probability import FreqDist\n\nwords = []\n\nfor sentence in train_data[train_data.sentiment == 'neutral']['clean_text']:\n    words.extend(sentence.split())\n\n# print(len(words))\nfdist = FreqDist(words)\n\nwords = pd.DataFrame(fdist.most_common(len(words)),columns = ['word','count'])\nwords.style.background_gradient(cmap='Blues')\nwords['len'] = words['word'].str.len()\ntop_words = words.head(20)\nleast_common_words = words.tail(20)\n\nplt.figure(figsize = (15,6))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"word\", data=top_words).set_title('Most common words in Neutral Sentiment Sentences')\n","885def49":"plt.figure(figsize = (15,6))\nsns.set_theme(style=\"whitegrid\")\nsns.barplot(x=\"count\", y=\"word\", data=words[(words['len'] > 8) & (words['count'] > 10)].head(20)).set_title('Unique words in Neutral Sentiment Sentences')","af659c6d":"tuples = [tuple(x) for x in words[['word','count']].values]\nwordcloud = WordCloud(background_color='white').generate_from_frequencies(dict(tuples))\nplt.figure(figsize = (12, 12), facecolor = None) \nplt.imshow(wordcloud, interpolation=\"bilinear\")","0fc182c2":"pos_words = {}\nneg_words = {}\nneutral_words = {}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfv = TfidfVectorizer(max_df=0.95, min_df=2,\n                                     max_features=10000,\n                                     stop_words='english',use_idf=True)\n\n\n\nx_pos_idf = tfv.fit_transform(train_data[train_data.sentiment == 'positive']['text'])\n# pos_words = dict(zip(tfv.get_feature_names(),np.array(1\/((tfv.idf_)))))\npos_words = dict(zip(map(str, tfv.get_feature_names()),1\/(2**np.array(tfv.idf_))))\n\nx_neg_idf = tfv.fit_transform(train_data[train_data.sentiment == 'negative']['text'])\nneg_words = dict(zip(map(str, tfv.get_feature_names()),1\/(2**np.array(tfv.idf_))))\n\nx_neutral_idf = tfv.fit_transform(train_data[train_data.sentiment == 'neutral']['text'])\nneutral_words = dict(zip(map(str, tfv.get_feature_names()),1\/(2**np.array(tfv.idf_))))\n\npos_words_new = {}\nneg_words_new = {}\nneutral_words_new = {}\n\nfor word in pos_words:\n    if word not in neg_words:neg_words[word] = 0\n    if word not in neutral_words:neutral_words[word] = 0\n    pos_words_new[word] = pos_words[word] - (neg_words[word]+neutral_words[word])\n    \nfor word in neg_words:\n    if(neg_words[word] == 0): continue\n    if word not in pos_words:pos_words[word] = 0\n    if word not in neutral_words:neutral_words[word] = 0\n    neg_words_new[word] = neg_words[word] - (pos_words[word]+neutral_words[word])\n    \nfor word in neutral_words:\n    if(neutral_words[word] == 0): continue\n    if word not in pos_words:pos_words[word] = 0\n    if word not in neg_words:neg_words[word] = 0\n    neutral_words_new[word] = neutral_words[word] - (pos_words[word]+neg_words[word])\n\n\n\n\n# pos_count_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())\n# for word in cv.get_feature_names():\n    \n#     pos_words[word] = 0","dd2ed4d9":"def calculate_selected_text(df_row, tol = 0.001):\n        tweet = df_row['text']\n        sentiment = df_row['sentiment']\n\n        if(sentiment == 'neutral'):\n            return tweet\n        elif(len(tweet.split()) <= 3):\n            return tweet\n        else:\n            words = tweet.lower().split()\n            words_len = len(words)\n            subsets = [words[i:j+1] for i in range(words_len) for j in range(i,words_len)]\n            \n            if(sentiment == 'positive'):\n                dict_to_use = pos_words_new # Calculate word weights using the pos_words dictionary\n            elif(sentiment == 'negative'):\n                dict_to_use = neg_words_new\n\n            score = 0\n            selection_str = '' # This will be our choice\n            lst = sorted(subsets, key = len) # Sort candidates by length\n#             print(subsets)\n\n            for i in range(len(subsets)):\n\n                new_sum = 0 # Sum for the current substring\n\n                # Calculate the sum of weights for each word in the substring\n                for p in range(len(lst[i])):\n                    if(lst[i][p].translate(str.maketrans('','',string.punctuation)) in dict_to_use.keys()):\n                        new_sum += dict_to_use[lst[i][p].translate(str.maketrans('','',string.punctuation))]\n                        \n\n                # If the sum is greater than the score, update our current selection\n                if(new_sum > score + tol):\n                    score = new_sum\n                    selection_str = lst[i]\n\n            # If we didn't find good substrings, return the whole text\n            if(len(selection_str) == 0):\n                    selection_str = words\n\n            return ' '.join(selection_str)","b9c44879":"# calculate_selected_text(train_data.iloc[4], tol = 0.001)\ntrain_data['derived'] = train_data.apply(calculate_selected_text,axis = 1)","e58bdc75":"test_data['selected_text'] = test_data.apply(calculate_selected_text,axis = 1)","77cede06":"test_data.head()","a976efd1":"test_data[['textID','selected_text']].to_csv('submission.csv',index = False)","d3d035a7":"Commond Words and Least Common words in Positive Words","ad412756":"**Interpretations**\n\n1. Most of the Neutral Sentiment Sentences have similar selected sentences and have a jaccard similarity of 1.\n2. There are few cluster of tweets with positive and negative sentiment which have similar no of words in actual text and selected text. (Peak on the right in the KDE Plot)","b78a17d9":"Removing Null Values Data","ad8b56d1":"Adding Meta features to the dataset \n1. Number of Words in Text\n2. Number of Words in Selected Text\n3. Difference in Words between full text and selected text\n4. Jaccard Score of Text","4ebd75b9":"This is my second notebook on kaggle and first notebook on NLP. I went through a lot of notebooks before trying this on my own. \n\nAcknowledging a few notebooks below through which i learnt the most.\n\nhttps:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model\n\n\nhttps:\/\/www.kaggle.com\/nkoprowicz\/a-simple-solution-using-only-word-counts","d6931877":"**Builduing the Model**","5a434a02":"**Understanding the clusters in negative and positive sentiment which have jaccard similarity score of 1**","34c460c6":"**Wordcloud in Negative Tweets**","002eb45d":"**EDA**","521bde6c":"**Wordcloud in positive tweets**","d8403f11":"Cleaning the Text and Understanding Common Words","6ed091ae":"1. For each word - calcualte a weightage (No of times that word appearead in each class\/no of tweets in that class) through idf scores\n2. Adjust that weightage by subtracting weights of same word in other classes","428beaa7":"1. 50% of the sentences with high jaccard score have less then 7 words.\n2. Even longer sentences have higher jaccard_similarity scores","4cda977d":"Distribution of Text Words","2afe2caf":"Analysing different Sentiments","d6b97578":"**Wordcloud for Neutral Tweets**"}}