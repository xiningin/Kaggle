{"cell_type":{"39c3225a":"code","d114da8f":"code","98005554":"code","b260aad6":"code","bb543554":"code","39ef5c8a":"code","33493366":"code","35688068":"code","af5e676c":"code","1d9e9686":"code","20d4cd47":"code","c642913f":"code","f0061d7b":"code","048bc97c":"code","3e031c92":"code","292f38d9":"code","36320eeb":"code","16e94190":"code","48beaef6":"code","293e008b":"code","3e6e0321":"code","a3222272":"markdown","4b2a2832":"markdown","b3002db2":"markdown","44bd325d":"markdown","bbd0dfbe":"markdown","e6cb44d4":"markdown","bc411679":"markdown","f62717e7":"markdown","4da1e80e":"markdown","41518798":"markdown","47191fea":"markdown","99293228":"markdown","3a5ee498":"markdown","d1d6a65e":"markdown","4f1500d2":"markdown"},"source":{"39c3225a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d114da8f":"from keras.models import Sequential\nfrom keras.layers import (Dense, Conv2D, BatchNormalization, Dropout, MaxPooling2D, Flatten)\nfrom keras.preprocessing import image","98005554":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","b260aad6":"from keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndata_train = pd.read_csv('..\/input\/fashion-mnist_train.csv')\ndata_test = pd.read_csv('..\/input\/fashion-mnist_test.csv')\n\nimg_rows, img_cols = 28, 28\ninput_shape = (img_rows, img_cols, 1)\n\nX = np.array(data_train.iloc[:, 1:])\n# OneHot encoding the data\ny = to_categorical(np.array(data_train.iloc[:, 0]))\n\n#Here we split validation data to optimiza classifier during training\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=13)\n\n#Test data\nX_test = np.array(data_test.iloc[:, 1:])\ny_test = to_categorical(np.array(data_test.iloc[:, 0]))\n\n\n\nX_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\nX_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\nX_val = X_val.reshape(X_val.shape[0], img_rows, img_cols, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_val = X_val.astype('float32')\n\n# Standardising the input variables\nX_train \/= 255.0\nX_test \/= 255.0\nX_val \/= 255.0","bb543554":"fig = plt.figure(figsize = (10, 5))\nsns.countplot(y.argmax(1))","39ef5c8a":"g = plt.imshow(X_train[10][:,:,0])","33493366":"model = Sequential()\n# Tier one \nmodel.add(Conv2D(32, kernel_size=5, input_shape = (28, 28, 1), activation='relu', padding = 'Same' ))\nmodel.add(Conv2D(64, kernel_size=5, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.33))\n\nmodel.add(Conv2D(128, kernel_size=3, activation='relu'))\nmodel.add(Conv2D(256, kernel_size=3, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size= (2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.33))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.33))\n\nmodel.add(Dense(units = 10, activation ='softmax'))\n\nmodel.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","35688068":"model.summary()","af5e676c":"img_gen = image.ImageDataGenerator(rotation_range=10,\n                                  width_shift_range=0.10, \n                                  shear_range=0.5,\n                                  height_shift_range=0.25, \n                                  zoom_range=0.20)","1d9e9686":"model.optimizer.lr = 0.001","20d4cd47":"batches = img_gen.flow(X_train, y_train, batch_size=64)\nhistory = model.fit_generator(batches,steps_per_epoch=500, epochs=150, verbose=1)","c642913f":"predictions = model.predict_classes(X_val)","f0061d7b":"fig = plt.figure(figsize = (10, 5))\nplt.plot([i*100 for i in history.history['acc']])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')","048bc97c":"print(\"The Final Accuracy : \", history.history['acc'][-1])","3e031c92":"from sklearn.metrics import classification_report\nprint(classification_report(y_val.argmax(1), predictions))","292f38d9":"smodel = Sequential()\n# Tier one \nsmodel.add(Conv2D(1, kernel_size=5, input_shape = (28, 28, 1), activation='relu', padding = 'Same' ))\nsmodel.add(Conv2D(1, kernel_size=5, activation='relu'))\nsmodel.add(BatchNormalization())\nsmodel.add(MaxPooling2D(pool_size = (2, 2)))\nsmodel.add(Dropout(0.33))\n\nsmodel.add(Conv2D(1, kernel_size=3, activation='relu'))\nsmodel.add(Conv2D(1, kernel_size=3, activation = 'relu'))\nsmodel.add(BatchNormalization())\nsmodel.add(MaxPooling2D(pool_size= (2,2)))\nsmodel.add(Dropout(0.2))","36320eeb":"smodel.summary()","16e94190":"def visualizer(model ,image):\n    img_batch = np.expand_dims(image, axis = 0)\n    conv_image = model.predict(img_batch)\n    \n    conv_image = np.squeeze(conv_image, axis = 0)\n    print(conv_image.shape)\n    \n    conv_image = conv_image.reshape(conv_image.shape[:2])\n    print(conv_image.shape)\n    \n    plt.imshow(conv_image)","48beaef6":"from random import randint\nimgIndex = randint(1, 54000)\nourImage = X_train[imgIndex][:,:,0]","293e008b":"plt.imshow(ourImage, vmin = 0.0, vmax = 1.0)\nplt.xlabel(y_train[imgIndex])","3e6e0321":"ourImage = ourImage.reshape(28, 28, 1)\nvisualizer(smodel, ourImage)","a3222272":"We can eaily see how well each of the data label is distributed among the overall data. This perfect distribution provides maximum output from the CNN model","4b2a2832":"# Data Preprocessing\n\nInspired by basic MNIST data preprocessing","b3002db2":"This is how our Random Image looks like. (The one hot encoded vector displays the category in which the cloth really lies in)","44bd325d":"# Visualising what our CNN sees?","bbd0dfbe":"## A sample of what our picture looks like","e6cb44d4":"# Final Predictions!","bc411679":"Now, Let's just consider a random image out of the box for this purpose of CNN visualisation","f62717e7":"# Having a look at the category distribution of various types of the clothes available","4da1e80e":"# Conclusion","41518798":"# Data Augmentation","47191fea":"# Putting in the CNN imports!","99293228":"And this is how our **CNN** Looks at the above piece of clothing.","3a5ee498":"# The CNN Model\n\n### Summary : Our particular model will be training about  ~5.2M parameters, using Adam optimizer at its heart and starting off with a learning rate  of 0.001","d1d6a65e":"# Let's have a look at the new MNIST Fashion dataset!\n\n### Why this dataset?\nFashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nThe original MNIST dataset contains a lot of handwritten digits. Members of the AI\/ML\/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n\nZalando seeks to replace the original MNIST dataset\n\n![A glimpse of the dataset](https:\/\/cdn-images-1.medium.com\/max\/750\/1*GNLB2jtcfb_xTqgQd9ntJA.png)","4f1500d2":"## The summary of our dummy model which we will be using to visualise how our model tokenises the input image"}}