{"cell_type":{"1bd7ccd6":"code","29d7d987":"code","8146fc31":"code","b8b99b49":"code","4f11c950":"code","27a52cd2":"code","bfc5d34e":"code","2a528ab7":"code","a84159a1":"code","30338fb7":"code","86b1f665":"code","ac684d58":"code","4b9f7b07":"code","c0413765":"markdown","7afe177c":"markdown","f6a27b50":"markdown","1ed79caa":"markdown","da62b3f0":"markdown","9d825785":"markdown","1836b7eb":"markdown","615786d4":"markdown","38682f2d":"markdown","a2e5458f":"markdown","2ea81c6d":"markdown","ea611dcd":"markdown","ea37fe5b":"markdown","4fd15a27":"markdown","d49183f4":"markdown","93e1adce":"markdown"},"source":{"1bd7ccd6":"from keras.models import Model, Sequential\nfrom keras.layers import Activation, Dense, BatchNormalization, Dropout, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Input, Reshape\nfrom keras.callbacks import EarlyStopping\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom tensorflow.metrics import mean_iou\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport glob\nimport PIL\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","29d7d987":"import re\nnumbers = re.compile(r'(\\d+)')\ndef numericalSort(value):\n    parts = numbers.split(value)\n    parts[1::2] = map(int, parts[1::2])\n    return parts","8146fc31":"filelist_trainx = sorted(glob.glob('..\/input\/*\/trainx\/*.bmp'), key=numericalSort)\nX_train = np.array([np.array(Image.open(fname)) for fname in filelist_trainx])\n\nfilelist_trainy = sorted(glob.glob('..\/input\/*\/trainy\/*.bmp'), key=numericalSort)\nY_train = np.array([np.array(Image.open(fname)) for fname in filelist_trainy])","b8b99b49":"x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size = 0.25, random_state = 101)","4f11c950":"plt.figure(figsize=(20,9))\nplt.subplot(2,4,1)\nplt.imshow(X_train[0])\nplt.subplot(2,4,2)\nplt.imshow(X_train[3])\nplt.subplot(2,4,3)\nplt.imshow(X_train[54])\nplt.subplot(2,4,4)\nplt.imshow(X_train[77])\nplt.subplot(2,4,5)\nplt.imshow(X_train[100])\nplt.subplot(2,4,6)\nplt.imshow(X_train[125])\nplt.subplot(2,4,7)\nplt.imshow(X_train[130])\nplt.subplot(2,4,8)\nplt.imshow(X_train[149])\nplt.show()","27a52cd2":"plt.figure(figsize=(20,9))\nplt.subplot(2,4,1)\nplt.imshow(Y_train[0], cmap = plt.cm.binary_r)\nplt.subplot(2,4,2)\nplt.imshow(Y_train[3], cmap = plt.cm.binary_r)\nplt.subplot(2,4,3)\nplt.imshow(Y_train[54], cmap = plt.cm.binary_r)\nplt.subplot(2,4,4)\nplt.imshow(Y_train[77], cmap = plt.cm.binary_r)\nplt.subplot(2,4,5)\nplt.imshow(Y_train[100], cmap = plt.cm.binary_r)\nplt.subplot(2,4,6)\nplt.imshow(Y_train[125], cmap = plt.cm.binary_r)\nplt.subplot(2,4,7)\nplt.imshow(Y_train[130], cmap = plt.cm.binary_r)\nplt.subplot(2,4,8)\nplt.imshow(Y_train[149], cmap = plt.cm.binary_r)\nplt.show()","bfc5d34e":"def iou(y_true, y_pred, smooth = 100):\n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    sum_ = K.sum(K.square(y_true), axis = -1) + K.sum(K.square(y_pred), axis=-1)\n    jac = (intersection + smooth) \/ (sum_ - intersection + smooth)\n    return jac","2a528ab7":"def dice_coef(y_true, y_pred, smooth = 100):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)","a84159a1":"def model_seg(epochs_num,savename):\n\n    img_input = Input(shape= (192, 256, 3))\n    # Block 1\n    x = Conv2D(64, (3, 3), padding='same', name='conv1',strides= (1,1))(img_input)\n    x = Activation('relu')(x)\n    x = Conv2D(64, (3, 3), padding='same', name='conv2',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    \n    # Block 2\n    x = Conv2D(128, (3, 3), padding='same', name='conv3',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(128, (3, 3), padding='same', name='conv4',strides= (1,1))(x)\n    x = Activation('relu')(x)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), padding='same', name='conv5',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(256, (3, 3), padding='same', name='conv6',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(256, (3, 3), padding='same', name='conv7',strides= (1,1))(x)\n    x = Activation('relu')(x)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), padding='same', name='conv8',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(512, (3, 3), padding='same', name='conv9',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(512, (3, 3), padding='same', name='conv10',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(512, (3, 3), padding='same', name='conv11',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    \n    # Block 5\n    x = Conv2D(512, (3, 3), padding='same', name='conv12',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(512, (3, 3), padding='same', name='conv13',strides= (1,1))(x)\n    x = Activation('relu')(x)\n    x = Conv2D(4096, (7, 7), padding='same', name='conv14',strides= (1,1))(x)\n    x = Dropout(0.5)(x)\n    \n    \n    # Block 6\n    x = Conv2D(4096, (1, 1), padding='same', name='conv15',strides= (1,1))(x)\n    x = Dropout(0.5)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(1, (1, 1), padding='same', name='conv16',strides= (1,1))(x)\n    x = Activation('softmax')(x)\n    \n\n    pred = Reshape((192, 256))(x)\n    \n    model = Model(inputs=img_input, outputs=pred)\n    \n    model.compile(optimizer= Adam(lr = 0.003), loss= ['binary_crossentropy'], metrics=[iou, dice_coef])\n    model.summary()\n    hist = model.fit(x_train, y_train, epochs= epochs_num, batch_size= 1, verbose=1)\n\n    model.save(savename)","30338fb7":"model_seg(epochs_num= 1, savename= 'model_epochs_1.h5')","86b1f665":"img_input = Input(shape= (192, 256, 3))\n# Block 1\nx = Conv2D(64, (3, 3), padding='same', name='conv1',strides= (1,1))(img_input)\nx = Activation('relu')(x)\nx = Conv2D(64, (3, 3), padding='same', name='conv2',strides= (1,1))(x)\nx = Activation('relu')(x)\n\n# Block 2\nx = Conv2D(128, (3, 3), padding='same', name='conv3',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(128, (3, 3), padding='same', name='conv4',strides= (1,1))(x)\nx = Activation('relu')(x)\n\n# Block 3\nx = Conv2D(256, (3, 3), padding='same', name='conv5',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(256, (3, 3), padding='same', name='conv6',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(256, (3, 3), padding='same', name='conv7',strides= (1,1))(x)\nx = Activation('relu')(x)\n\n# Block 4\nx = Conv2D(512, (3, 3), padding='same', name='conv8',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(512, (3, 3), padding='same', name='conv9',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(512, (3, 3), padding='same', name='conv10',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(512, (3, 3), padding='same', name='conv11',strides= (1,1))(x)\nx = Activation('relu')(x)\n\n# Block 5\nx = Conv2D(512, (3, 3), padding='same', name='conv12',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(512, (3, 3), padding='same', name='conv13',strides= (1,1))(x)\nx = Activation('relu')(x)\nx = Conv2D(4096, (7, 7), padding='same', name='conv14',strides= (1,1))(x)\nx = Dropout(0.5)(x)\n\n\n# Block 6\nx = Conv2D(4096, (1, 1), padding='same', name='conv15',strides= (1,1))(x)\nx = Dropout(0.5)(x)\nx = Activation('relu')(x)\nx = Conv2D(1, (1, 1), padding='same', name='conv16',strides= (1,1))(x)\nx = Activation('softmax')(x)\n\n\npred = Reshape((192, 256))(x)\n","ac684d58":"model_0 = Model(inputs=img_input, outputs=pred)\nmodel_0.compile(optimizer= Adam(lr = 0.003), loss= ['binary_crossentropy'], metrics=[iou, dice_coef])\nmodel_0.load_weights('model_epochs_1.h5')","4b9f7b07":"print('~~~~~~~~~~~~~~~~~~~~~~~~Stats after 1 epoch~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n\nprint('\\n----------------------------On Train Set -----------------------------------\\n')\nres = model_0.evaluate(x_train, y_train, batch_size= 1)\nprint('IOU: {:.2f}'.format(res[1]*100))\nprint(\"Dice Coefficient: {:.2f}\".format(res[2]*100))\n\nprint('\\n-----------------------------On Test Set ------------------------------------\\n')\nres = model_0.evaluate(x_test, y_test, batch_size= 1)\nprint('IOU: {:.2f}'.format(res[1]*100))\nprint(\"Dice Coefficient: {:.2f}\".format(res[2]*100))","c0413765":"## Evaluating the performance for 1 epoch","7afe177c":"## Importing the Libraries","f6a27b50":"![arch](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S0169260718304267-fx1_lrg.jpg)","1ed79caa":"Calling the function","da62b3f0":"## Loading the Model","9d825785":"## Loading the Data","1836b7eb":"Splitting the data","615786d4":"## Model for More Epochs\n\nSince the model takes a lot of time for training it is not very practical for us personally to train for more epochs.    \n\n** Problems**   \n* The model architecture proposed does not include **downsampling** or **pooling** which results in evaluation of every pixel for every image in each of the 16 convolution layers.\n* As a result the number of parameters in the model are **134,264,641** which is around *25 times* more than the **Fully Convolutional Network** we have tried before.\n* We have trained till now just one epoch with the batch size of *1* rather than the suggested *20*.  \nWe cannot increase the batch size in training as the system gives out **ResourceExhaustError** which means the current environment resources are not enough to train such a complex model.\n* The resources we have tried are the **Kaggle** itself and **Google Colab** which lags behind even more and couldn't manage even a single epoch.\n","38682f2d":"## Defining Evaluation Metrics","a2e5458f":"### Intersection over Union \nIntersection over Union is an evaluation metric used to measure the accuracy of an object detector on a particular dataset.\n![iou](https:\/\/www.d2l.ai\/_images\/iou.svg)","2ea81c6d":"### Dice coefficient  \nThe Dice score is not only a measure of how many positives you find, but it also penalizes for the false positives that the method finds, similar to precision. so it is more similar to precision than accuracy.\n![dice](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*Z1hkDvyhFBogT9EkzVkX2A.png)","ea611dcd":"Defining a function to load the data in sorted order","ea37fe5b":"____","4fd15a27":"## The Model Architecture","d49183f4":"Defining the model in a function which takes two arguments when called\n\n**epoch_num**: number of epochs to run\n**savename**: the name of the model for saving after training\n\n### Optimizer and Learning Rate\n* We adopt adam optimization algorithm or adaptive moments, to adjust the learning rate.\n* Stochastic gradient descent (SGD) with mini-batch is usually employed as the optimization algorithm for neural network training.\n* It is well known that learning rate is one of the critical hyperparameters that have a signi\ufb01cant impact on classi\ufb01cation performance.\n* Adam is fairly robust to the choice of hyperparameters, and set the learning rate \u03b1 as 0.003 to speed up the training procedure in this study.","93e1adce":"Let's plot an image to see how the data looks"}}