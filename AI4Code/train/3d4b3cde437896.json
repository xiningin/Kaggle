{"cell_type":{"80845d47":"code","0ac4802e":"code","083d122a":"code","a2417eae":"code","43fa318f":"code","592689ef":"code","97921aaf":"code","d128cdbb":"code","6c7d934b":"code","b6cdb9ed":"code","b56f4d5d":"code","7816e419":"code","6c9677b3":"code","3e670d13":"code","d8127e24":"code","8cf82caa":"code","9d07f095":"code","045dfa8e":"code","0839cd1b":"code","20d1bff9":"code","0997fe5f":"code","4d385d51":"code","bf916bc0":"code","de485aff":"code","f26a7a7f":"code","9d6fd400":"markdown","e95c18e0":"markdown","9dd7f12f":"markdown","a1175a64":"markdown","1de9741a":"markdown","b4ea7e9f":"markdown","05f6da2b":"markdown","1b5eca17":"markdown","b58657df":"markdown"},"source":{"80845d47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ac4802e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport tensorflow as tf","083d122a":"# Load the dataset \n\ncreditcard = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n\ncreditcard.head()","a2417eae":"# Lets Check the data structure\ncreditcard.info()","43fa318f":"# Check for any missing values\ncreditcard.isna().sum()","592689ef":"# Explore the response variable\n\nplt.figure(figsize=(10, 6))\nsns.countplot(x='Class', data=creditcard)\nplt.grid()\nplt.title(\"Distribution of the target Variable\")\n\n# Value Counts\nprint(creditcard['Class'].value_counts())\nprint(f\"Fraud Transactions are - {len(creditcard[creditcard['Class'] == 1])\/len(creditcard) * 100} % of Total\")","97921aaf":"# We would see the interaction between V1 and V2\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=\"V1\", y=\"V2\", hue=\"Class\", data=creditcard)","d128cdbb":"# We would observe the interaction between V5 and V6 through lmplot\nplt.figure(figsize=(10, 6))\nsns.lmplot(x=\"V5\", y=\"V6\", hue=\"Class\", data=creditcard, fit_reg=False)","6c7d934b":"# Taking two more attributes\nsns.lmplot(x=\"V15\", y=\"V16\", hue='Class', data=creditcard, fit_reg=False)","b6cdb9ed":"# We will introduce a log of the Amount column, use the column in modelling and drop the original Amount column\n# However in order to do this, there are some values with 0 in Amount and Log(0) is infinity and hence we add a very small\n# negligible amount to the log to avoid this infinity error\n\nadjustments = 0.001\n\ncreditcard['Log Amount'] = np.log(creditcard['Amount'] + adjustments)","b56f4d5d":"sns.displot(x=\"Log Amount\", hue='Class', data=creditcard)","7816e419":"# Dropping Columns like Time, Amount\n\ncreditcard = creditcard.drop(labels=[\"Time\", \"Amount\"], axis=1)","6c9677b3":"# Extracting features and target\n\nfeatures = creditcard.drop(labels=['Class'], axis = 1)\ntarget = creditcard['Class']","3e670d13":"# Perform feature scaling\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nfeatures = sc.fit_transform(features)\nfeatures","d8127e24":"# Perform the training, validation and test splits - We do stratified Sampling\n\nfrom sklearn.model_selection import train_test_split\nfeatures_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.3, \n                                                                            random_state=101, stratify=target)\n\nfeatures_train, features_validation, target_train, target_validation = train_test_split(features_train, target_train,\n                                                                                        test_size=0.3, random_state=101, \n                                                                                        stratify=target_train)","8cf82caa":"# Define the Model Constants\n\nINPUT_SHAPE = (features_train.shape[1], )\n\nOUTPUT_UNITS = 1\nHIDDEN_UNITS = 16\nACTIVATION_HIDDEN = tf.keras.activations.relu\nACTIVATION_OUTPUT = tf.keras.activations.sigmoid\nLEARNING_RATE = 1e-3\nOPTIMIZER = tf.keras.optimizers.Adam(LEARNING_RATE)\nLOSS_FUNCTION = tf.keras.losses.BinaryCrossentropy()\nDROPOUT_RATE = 0.5\n\nEPOCHS = 100\nBATCH_SIZE = 2048","9d07f095":"# Define the Metrics\n\nMETRICS = [tf.keras.metrics.TruePositives(name='tp'),\n          tf.keras.metrics.FalsePositives(name='fp'),\n          tf.keras.metrics.TrueNegatives(name='tn'),\n          tf.keras.metrics.FalseNegatives(name='fn'), \n          tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n          tf.keras.metrics.Precision(name='precision'),\n          tf.keras.metrics.Recall(name='recall'),\n          tf.keras.metrics.AUC(name='auc')]","045dfa8e":"# Function which will build and compile the model\n\ndef make_model(metrics=METRICS, output_bias=None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    model = tf.keras.models.Sequential([\n           tf.keras.layers.Dense(input_shape=INPUT_SHAPE, units=HIDDEN_UNITS, activation=ACTIVATION_HIDDEN),\n           tf.keras.layers.Dropout(DROPOUT_RATE),\n           tf.keras.layers.Dense(units=OUTPUT_UNITS, activation=ACTIVATION_OUTPUT, bias_initializer=output_bias)\n       ])\n    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=metrics)\n    return model","0839cd1b":"# Lets build the model and see the mmodel summary\n\nmodel = make_model()\nmodel.summary()","20d1bff9":"# we will now train the model on training and validation data\n\nepochs_history = model.fit(features_train, target_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                          validation_data=(features_validation, target_validation),\n                          verbose=1)","0997fe5f":"plt.figure(figsize=(12, 8))\n\nloss_train = epochs_history.history['loss']\nloss_val = epochs_history.history['val_loss']\n\nepochs = range(1,101)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='Validation loss')\nplt.title('Training Loss vs Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()","4d385d51":"plt.figure(figsize=(12, 8))\n\nloss_train = epochs_history.history['accuracy']\nloss_val = epochs_history.history['val_accuracy']\n\nepochs = range(1,101)\nplt.plot(epochs, loss_train, 'g', label='Training Accuracy')\nplt.plot(epochs, loss_val, 'b', label='Validation Accuracy')\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()","bf916bc0":"# Let us run the predictions\n\ntarget_predictions = model.predict(features_test)","de485aff":"# Let us visualize the Confusion Matrix and detail out some key metrices including classification report\n\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score, balanced_accuracy_score\n\ndef plot_cm(labels, predictions, threshold=0.5):\n    cm = confusion_matrix(labels, predictions > threshold)\n    plt.figure(figsize=(5, 5))\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title(\"Confusion Matrix %0.2f\" %threshold)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"Actual Label\")\n    \n    print('Legitimate Transactions Detected (True Negatives): ', cm[0][0])\n    print('Legitimate Transactions Incorrectly Detected (False Positives): ', cm[0][1])\n    print('Fraudulent Transactions Missed (False Negatives): ', cm[1][0])\n    print('Fraudulent Transactions Detected (True Positives): ', cm[1][1])\n    print('Total Fraudulent Transactions: ', np.sum(cm[1]))\n    print(\"\\n\")\n    print(\"F1-Score\")\n    print(f1_score(target_test, target_predictions > threshold))\n    print(\"\\n\")\n    print(\"Accuracy Score\")\n    print(accuracy_score(target_test, target_predictions > threshold))\n    print(\"\\n\")\n    print(\"Balanced Accuracy Score\")\n    print(balanced_accuracy_score(target_test, target_predictions > threshold))\n    print(\"\\n\")\n    print(\"Classification Report\")\n    print(classification_report(target_test, target_predictions > threshold))","f26a7a7f":"# Now use the function to plot the confusion matrix\n\nplot_cm (target_test, target_predictions)","9d6fd400":"### Check some of the performance graphs","e95c18e0":"## Library imports","9dd7f12f":"## Build and Train the Neural Network Model","a1175a64":"## Exploratory Data Analysis - With Few attributes","1de9741a":"#### Each of the above plot shows that fraudulent transactions are more or less concentrated at one place","b4ea7e9f":"## Load the dataset and validate the data load","05f6da2b":"**Model is fit using a larger than default batch size.. We train with a size of 2048, this is important to ensure that each batch has a decent chance of containing a few positive samples. If the batch size was too small, they would likely have no fraudulent transactions to learn from**","1b5eca17":"## Train-Test Split and Feature Scaling","b58657df":"## Model Validation"}}