{"cell_type":{"9aac156e":"code","98099be8":"code","578bb842":"code","4c684a87":"code","a4cfeead":"code","1582f6ab":"code","7e2ef19d":"code","db5970ff":"code","065cc6c0":"code","94a447b7":"code","ae9dec83":"code","0f52e110":"code","ca431e82":"code","321559e3":"code","648d50ad":"code","f51336c3":"code","9b9d1d1c":"code","f7925e3c":"code","6703f05c":"code","70fef2fb":"markdown","c5afe015":"markdown","0da822cd":"markdown","870be8c8":"markdown","5cf7ad00":"markdown","dc25515c":"markdown"},"source":{"9aac156e":"import pandas as pd\n\ntrain_df = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")","98099be8":"# First step assigns a discourse type to each word\ndef add_discourse_type(example):\n    \n    id_ = example[\"id\"]\n    \n    features = train_df[train_df[\"id\"]==id_]\n\n    with open(f\"..\/input\/feedback-prize-2021\/train\/{id_}.txt\") as fp:\n        text = fp.read()\n\n    words = text.split()\n\n    labels = [\"O\"]*len(words)\n\n    for discourse, predictions in features[[\"discourse_type\", \"predictionstring\"]].values:\n        idx_iter = map(int, predictions.split())\n        for idx in idx_iter:\n            labels[idx] = discourse \n        \n    return {\n        \"id\" : id_,\n        \"labels\": labels,\n        \"words\": words,\n    }","578bb842":"%%time\n\nfrom datasets import Dataset\n\nds = Dataset.from_dict({\"id\": train_df[\"id\"].unique()}) \n\ntagged_ds = ds.map(add_discourse_type, num_proc=4)","4c684a87":"# Second step adds B or I as prefix to the label\ndef add_bi(example):\n    new_tags = []\n    \n    if example[\"labels\"][0] != \"O\":\n        new_tags.append(f\"B-{example['labels'][0]}\")\n    else:\n        new_tags.append(\"O\")\n    \n    for idx in range(1, len(example[\"labels\"])):\n        current_tag = example['labels'][idx]\n        if current_tag == \"O\":\n            new_tags.append(current_tag)\n        elif example[\"labels\"][idx-1] != current_tag:\n            new_tags.append(f\"B-{current_tag}\")\n        else:\n            new_tags.append(f\"I-{current_tag}\")\n            \n    example[\"bio\"] = new_tags\n    return example","a4cfeead":"%%time\n\nbio_dataset = tagged_ds.map(add_bi, remove_columns=[\"labels\"], num_proc=4)\nbio_dataset","1582f6ab":"# Let's look at some\n{key: vals[:10] for key, vals in bio_dataset[11].items()}","7e2ef19d":"# Let's check what tags were added\nfrom itertools import chain\nfrom collections import Counter\n\nall_tags_no_whitespace = list(chain(*bio_dataset[\"bio\"]))\nmost_common_no_whitespace = Counter(all_tags_no_whitespace).most_common(20)\nmost_common_no_whitespace","db5970ff":"bio_dataset.to_json(\"split_at_whitespace.json\")","065cc6c0":"labels = train_df[\"discourse_type\"].unique()\nlabels = [f\"B-{label}\" for label in labels] + [f\"I-{label}\" for label in labels]\nlabels.append(\"O\")\n\nlen(labels), labels","94a447b7":"from transformers import AutoTokenizer\n\n# This uses a bigbird tokenizer, but it will work with any tokenizer type \ntokenizer = AutoTokenizer.from_pretrained(\"google\/bigbird-roberta-base\")","ae9dec83":"import string\n\nWHITESPACE = set(string.whitespace + \"\\xa0\")\ndef chars_to_word_ids(text, char_label_preds):\n    \n    all_word_preds = []\n    current_id = 0\n    current_word_preds = []\n    for char, pred in zip(text, char_preds):\n        \n        # If we are not in whitespace and have do\/don't have predictions\n        if char not in WHITESPACE:\n            current_word_preds.append(pred)\n            \n        # If we reach whitespace and have predictions\n        elif current_word_preds and char in WHITESPACE:\n            pass # Figure out a label for this word\n            \n            # Add that label to the list of word preds\n            all_word_preds.append(LABEL)\n            current_word_preds = []\n            current_id += 1\n            \n        # If we are in whitespace and do not have predictions\n        elif not current_word_preds and char in WHITESPACE:\n            pass # do nothing\n            \n        \n\n    return all_word_preds\n\ndef add_labels(example):\n    \n    id_ = example[\"id\"]\n\n    text = open(f\"..\/input\/feedback-prize-2021\/train\/{id_}.txt\").read()\n    \n    tokenized = tokenizer(text, truncation=True, padding=\"max_length\", max_length=1024, return_offsets_mapping=True)\n    \n    discourse_data = train_df[train_df[\"id\"]==id_]\n    \n    char_labels = [\"O\"]*len(text)\n    \n    num_chars = len(text)\n    \n    for start, end, label in discourse_data[[\"discourse_start\", \"discourse_end\", \"discourse_type\"]].values:\n        for idx in range(int(start), int(end)):\n            if idx >= num_chars:\n                break\n            char_labels[idx] = f\"I-{label}\"\n        char_labels[int(start)] = f\"B-{label}\"\n    \n    token_labels = [\"O\"]*len(tokenized[\"input_ids\"])\n    token_offsets = tokenized[\"offset_mapping\"]\n    for idx, (start_offset, end_offset) in enumerate(token_offsets):\n        if start_offset == end_offset and start_offset == 0:\n            continue\n        for char_label in char_labels[start_offset:end_offset]:\n            token_labels[idx] = char_label\n            if char_label.startswith(\"B-\"):\n                break\n    \n    return {\n        \"input_ids\": tokenized[\"input_ids\"],\n        \"attention_mask\": tokenized[\"attention_mask\"],\n        \"offset_mapping\": token_offsets,\n        \"labels\": token_labels\n    }","0f52e110":"%%time\n\nfrom datasets import Dataset\n\nds = Dataset.from_dict({\"id\": list(train_df[\"id\"].unique())})\n\ntokenized_ds = ds.map(add_labels, num_proc=4)","ca431e82":"# Let's look at some\n\nid_ = ds[\"id\"][101]\n\nsample_labels = add_labels({\"id\":id_})\n\nlist(zip(\n    tokenizer.convert_ids_to_tokens(sample_labels[\"input_ids\"]), \n    sample_labels[\"offset_mapping\"], \n    sample_labels[\"labels\"]\n))[:100]","321559e3":"# Let's check what tags were added\n\nall_tags_with_whitespace = list(chain(*tokenized_ds[\"labels\"]))\nmost_common_with_whitespace = Counter(all_tags_with_whitespace).most_common(20)\nmost_common_with_whitespace","648d50ad":"tokenized_ds.to_json(\"include_whitespace.json\")","f51336c3":"list(zip(most_common_no_whitespace, most_common_with_whitespace))","9b9d1d1c":"# Move into folder\n%mkdir original\n%mv *.json original\/","f7925e3c":"new_train_df = pd.read_csv(\"..\/input\/feedback-prize-corrected-train-csv\/corrected_train.csv\")\n\nprint(train_df.shape, new_train_df.shape)\n\ntrain_df = new_train_df.drop(columns=[\"discourse_start\", \"discourse_end\", \"predictionstring\"])\ndel new_train_df\n\ntrain_df = train_df.rename(columns={\"new_start\": \"discourse_start\", \"new_end\": \"discourse_end\", \"new_predictionstring\": \"predictionstring\"})\n\nds = Dataset.from_dict({\"id\": train_df[\"id\"].unique()}) \n\ntagged_ds = ds.map(add_discourse_type, num_proc=4)\nbio_dataset = tagged_ds.map(add_bi, remove_columns=[\"labels\"], num_proc=4)\n\ncorrected_dir = \"corrected\"\n%mkdir $corrected_dir\n\nbio_dataset.to_json(f\".\/{corrected_dir}\/split_at_whitespace.json\")\n\nall_tags_no_whitespace = list(chain(*bio_dataset[\"bio\"]))\nmost_common_no_whitespace = Counter(all_tags_no_whitespace).most_common(20)\n\nds = Dataset.from_dict({\"id\": list(train_df[\"id\"].unique())})\ntokenized_ds = ds.map(add_labels, num_proc=4)\n\nall_tags_with_whitespace = list(chain(*tokenized_ds[\"labels\"]))\nmost_common_with_whitespace = Counter(all_tags_with_whitespace).most_common(20)\n\ntokenized_ds.to_json(f\".\/{corrected_dir}\/include_whitespace.json\")\n\nlist(zip(most_common_no_whitespace, most_common_with_whitespace))","6703f05c":"counter = 0\nfor example in tokenized_ds:\n    if counter > 10: break\n    \n    prev = None\n    for label in example[\"labels\"]:\n        if prev is not None:\n            if label.startswith(\"I-\") and prev.replace(\"B-\", \"\").replace(\"I-\", \"\") != label.replace(\"B-\", \"\").replace(\"I-\", \"\"):\n                if label != \"O\":\n                    print(prev, label, example)\n                    counter += 1\n        prev = label","70fef2fb":"# This creates data in the BIO format for training an NER model\n\nB = beginning  \nI = inside  \nO = outside   \n\nSome call it IOB - same thing. https:\/\/en.wikipedia.org\/wiki\/Inside%E2%80%93outside%E2%80%93beginning_(tagging)\n\nThis format can be used with the Hugging Face [example scripts for token classification](https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples\/pytorch\/token-classification)\n\nNER has been shown to get around ~~0.5~~ 0.6 on public LB already :)  https:\/\/www.kaggle.com\/zzy990106\/pytorch-ner-infer\n\nIf you want just the dataset, use this: https:\/\/www.kaggle.com\/nbroad\/feedbackprize-bio-ner-train-data\n\n### I include 2 different ways of tokenizing. The first tokenizes after breaking at whitespace, the second includes whitespace. \n\nI'm running some experiments to see which method is better. I have a feeling that the whitespace is valuable information that will get lost when using the first method. \n\n`split_at_whitespace.json` first splits the text at whitespace and then assigns a label to each token. This does not pass through a Tokenizer object.  \n`include_whitespace.json` does not split at whitespace. This output comes out of a Tokenizer. This is tokenizer-specific so make sure you use the right tokenizer :)\n\n\n### Update: Dec 23 - I came up with a way of correcting the misaligned labels, so I will run my code twice: once on the original file and once on the corrected file\n\nHere is my notebook that corrects the data: https:\/\/www.kaggle.com\/nbroad\/corrected-train-csv-feedback-prize\n\nFolder `original` uses the data given by the hosts.  \nFolder `corrected` uses the data from my corrected notebook.","c5afe015":"# Option 1: Splitting at whitespace","0da822cd":"# Checking if there are any I- labels that don't have a B- before them","870be8c8":"# Now with the corrected data","5cf7ad00":"## Comparison of tags\n\nThere are huge differences in \"O\" and \"I-\" tags because words get broken into multiple tokens, but there should roughly be the same number of \"B-\" tags. The values are close enough for me, but if you can think of a better way of labeling it, let me know! While I was doing this, I did notice some issues which I listed here: https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/296524","dc25515c":"# Option 2: Keeping whitespace"}}