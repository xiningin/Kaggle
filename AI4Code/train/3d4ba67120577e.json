{"cell_type":{"d99406f0":"code","8e920f68":"code","d46975db":"code","515b63c0":"code","70c0a0bb":"code","c1b1826c":"code","780ca7af":"code","e470cc51":"code","cfa26232":"code","07f618d3":"code","6d3e9b7d":"code","834f9c8f":"code","c7374f88":"code","75f31c09":"code","24e149a8":"code","8e4ec8db":"code","27b5a31c":"code","28919c03":"code","9f3a60c8":"code","508f0e94":"code","6e392adb":"code","4d3b14ee":"code","2da90e8a":"code","e77bab5d":"code","cd750476":"markdown","5df5b1b9":"markdown","52568e73":"markdown","d395098b":"markdown","7536e242":"markdown","1884d27d":"markdown","67afdabd":"markdown","123184ae":"markdown","13e1aea1":"markdown","f5044ac0":"markdown","42a478bd":"markdown","db177233":"markdown","9e61153f":"markdown","7d72b4df":"markdown","10154f9e":"markdown","dd061dec":"markdown","a50754de":"markdown","d03e3687":"markdown","8f63fcce":"markdown","51e50806":"markdown","f5cc5cd8":"markdown","2505f37f":"markdown","a66c342f":"markdown","4d2c492b":"markdown","e31adfb1":"markdown","8ca9074d":"markdown","c753bb3a":"markdown","7f9ab588":"markdown","4bb09366":"markdown","fbb6bf0f":"markdown","175d08e5":"markdown","916c1eae":"markdown","9b5a442c":"markdown","74563702":"markdown","86b76975":"markdown","8cc8cfe2":"markdown","409ca92e":"markdown","8094e1d7":"markdown"},"source":{"d99406f0":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom collections import Counter\nimport os\n\nimport torch\nfrom torchvision import datasets,transforms,models\nfrom torch.utils.data import Dataset,DataLoader\nfrom PIL import Image\n\nimport sys\nimport torch.optim as optim","8e920f68":"print(\"PyTorch Version: \",torch.__version__)","d46975db":"!pip install xmltodict\n#hgalo re","515b63c0":"import xmltodict","70c0a0bb":"img_names=[] \nxml_names=[] \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if os.path.join(dirname, filename)[-3:]!=\"xml\":\n            img_names.append(filename)\n        else:\n            xml_names.append(filename)","c1b1826c":"path_annotations=\"\/kaggle\/input\/face-mask-detection\/annotations\/\" \nlisting=[]\nfor i in img_names[:]:\n    with open(path_annotations+i[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            listing.append(temp[i][\"name\"])\n    else:\n        listing.append(temp[\"name\"])\n        \n\nItems = Counter(listing).keys()\nvalues = Counter(listing).values()\nprint(Items,'\\n',values)","780ca7af":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14,6))\nbackground_color = '#faf9f4'\nax1.set_facecolor(background_color)\nax2.set_facecolor(background_color) \nax1.pie(values,wedgeprops=dict(width=0.3, edgecolor='w') ,\n        labels=Items, radius=1, startangle = 120, autopct='%1.2f%%')\n\nax2 = plt.bar(Items, list(values),\n              color ='maroon',width = 0.4)\n\nplt.show()\n","e470cc51":"path_image=\"\/kaggle\/input\/face-mask-detection\/images\/\"  \ndef face_cas(img): \n    with open(path_annotations+img[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    image=plt.imread(os.path.join(path_image+img))\n    fig,ax=plt.subplots(1)\n    ax.axis(\"off\")\n    fig.set_size_inches(10,5)\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            ###with_mask\n            if temp[i][\"name\"]==\"with_mask\":\n                x,y,w,h=list(map(int,temp[i][\"bndbox\"].values()))\n                mpatch=mpatches.Rectangle((x,y),w-x,h-y,linewidth=1, edgecolor='g',facecolor=\"none\",lw=2,)\n                ax.add_patch(mpatch)\n                rx, ry = mpatch.get_xy()\n                ax.annotate(\"with_mask\", (rx, ry), color='green', weight='bold', fontsize=10, ha='left', va='baseline')\n            ###without_mask\n            if temp[i][\"name\"]==\"without_mask\":\n                x,y,w,h=list(map(int,temp[i][\"bndbox\"].values()))     \n                mpatch=mpatches.Rectangle((x,y),w-x,h-y,linewidth=1, edgecolor='r',facecolor=\"none\",lw=2,)\n                ax.add_patch(mpatch)\n                rx, ry = mpatch.get_xy()\n                ax.annotate(\"without_mask\", (rx, ry), color='red', weight='bold', fontsize=10, ha='left', va='baseline')\n            ###mask_weared_incorrect\n            if temp[i][\"name\"]==\"mask_weared_incorrect\":\n                x,y,w,h=list(map(int,temp[i][\"bndbox\"].values()))\n                mpatch=mpatches.Rectangle((x,y),w-x,h-y,linewidth=1, edgecolor='y',facecolor=\"none\",lw=2,)\n                ax.add_patch(mpatch)\n                rx, ry = mpatch.get_xy()\n                ax.annotate(\"mask_weared_incorrect\", (rx, ry), color='yellow', weight='bold', fontsize=10, ha='left', va='baseline')\n    else:\n        x,y,w,h=list(map(int,temp[\"bndbox\"].values()))\n        edgecolor={\"with_mask\":\"g\",\"without_mask\":\"r\",\"mask_weared_incorrect\":\"y\"}\n        mpatch=mpatches.Rectangle((x,y),w-x,h-y,linewidth=1, edgecolor=edgecolor[temp[\"name\"]],facecolor=\"none\",)\n    ax.imshow(image)\n    ax.add_patch(mpatch)\n\nfun_images = img_names.copy()\nfor i in range(1,8):\n    face_cas(fun_images[i])","cfa26232":"options={\"with_mask\":0,\"without_mask\":1,\"mask_weared_incorrect\":2} ","07f618d3":"def dataset_creation(image_list): \n    image_tensor=[]\n    label_tensor=[]\n    for i,j in enumerate(image_list):\n        with open(path_annotations+j[:-4]+\".xml\") as fd:\n            doc=xmltodict.parse(fd.read())\n        if type(doc[\"annotation\"][\"object\"])!=list:\n            temp=doc[\"annotation\"][\"object\"]\n            x,y,w,h=list(map(int,temp[\"bndbox\"].values()))\n            label=options[temp[\"name\"]]\n            image=transforms.functional.crop(Image.open(path_image+j).convert(\"RGB\"), y,x,h-y,w-x)\n            image_tensor.append(my_transform(image))\n            label_tensor.append(torch.tensor(label))\n        else:\n            temp=doc[\"annotation\"][\"object\"]\n            for k in range(len(temp)):\n                x,y,w,h=list(map(int,temp[k][\"bndbox\"].values()))\n                label=options[temp[k][\"name\"]]\n                image=transforms.functional.crop(Image.open(path_image+j).convert(\"RGB\"),y,x,h-y,w-x)\n                image_tensor.append(my_transform(image))\n                label_tensor.append(torch.tensor(label))\n                \n    final_dataset=[[k,l] for k,l in zip(image_tensor,label_tensor)]\n    return tuple(final_dataset)\n\n\nmy_transform=transforms.Compose([transforms.Resize((226,226)),\n                                 transforms.ToTensor()])\n\nmydataset=dataset_creation(img_names)","6d3e9b7d":"mydataset[0]","834f9c8f":"train_size=int(len(mydataset)*0.7)\ntest_size=len(mydataset)-train_size\nprint('Length of dataset is', len(mydataset), '\\nLength of training set is :',train_size,'\\nLength of test set is :', test_size)","c7374f88":"trainset,testset=torch.utils.data.random_split(mydataset,[train_size,test_size])","75f31c09":"train_dataloader =DataLoader(dataset=trainset,batch_size=32,shuffle=True,num_workers=4)\ntest_dataloader =DataLoader(dataset=testset,batch_size=32,shuffle=True,num_workers=4)","24e149a8":"device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n                               else \"cpu\")","8e4ec8db":"import sys\ntrain_features, train_labels = next(iter(train_dataloader))\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")","27b5a31c":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n    \ntrain_features_np=train_features.numpy()\n\nfig=plt.figure(figsize=(25,4))\nfor idx in np.arange(3):\n    ax=fig.add_subplot(2,20\/2,idx+1,xticks=[],yticks=[])\n    plt.imshow(np.transpose(train_features_np[idx],(1,2,0)))\n    ","28919c03":"model=models.resnet34(pretrained=True)","9f3a60c8":"for param in model.parameters():\n    param.requires_grad=False","508f0e94":"model","6e392adb":"import torch.nn as nn\nn_inputs=model.fc.in_features\nlast_layer=nn.Linear(n_inputs,3)\n\nmodel.fc.out_features=last_layer\nprint('reinitialize model with output features as 3 :', model.fc.out_features)","4d3b14ee":"criterion=nn.CrossEntropyLoss()\noptimizer=optim.SGD(model.parameters(),lr=0.001,momentum=0.9)","2da90e8a":"param.requires_grad=True\nct = 0\nfor child in model.children():\n    ct += 1\n    if ct < 7:\n        for param in child.parameters():\n            param.requires_grad = False","e77bab5d":"#n_epochs=1\n\nfor epoch in range(1,2): \n    running_loss = 0.0\n    train_losses = []\n    for i, (inputs, labels) in enumerate(train_dataloader):\n        \n        if torch.cuda.is_available():\n            inputs , labels = inputs.cuda(), labels.cuda()\n        \n        #inputs = inputs.to(device)\n        #labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() \n        if i % 20 == 19:    \n                \n                print(\"Epoch {}, batch {}, training loss {}\".format(epoch, i+1,running_loss\/20))\n        \n        running_loss = 0.0\n     \n    \n\nprint('\\nFinished Training')","cd750476":"# Image Preprocessing","5df5b1b9":"\n\nThe Residual Network, or ResNet for short, is a model that makes use of the residual module.There are several variants of different sizes, including Resnet18, Resnet34, Resnet50, Resnet101, and Resnet152, all of which are available from torchvision models. **Here Resnet34 model is used.**\n\nWhen deeper networks are able to start converging, degradation problem occurs. As the network depth increases, the accuracy gets saturated and then degrades rapidly. These degradation is not caused by overfitting, and adding\nmore layers to a suitably deep model leads to higher training error. Degradation indicates **not** all systems are similarly easy to optimize.\n\nDeep residual learning framework address the degradation problem. Instead of hoping each few stacked layers directly fit a desired underlying mapping,   these layers are explicitly made to fit a residual mapping.","52568e73":"Functions used for preprocessing are :\n\n*    **xmltodict.parse()** : This is used to parse the given XML input and convert it into a dictionary. \n\n    Generak syntax of xmltodict.parse() is \n    \n    xmltodict.parse(xml_input, encoding='utf-8', expat=expat, process_namespaces=False, namespace_separator=':', **kwargs)\n    *     **xml_input** : It can be a either be a string or a file-like object. Here we are using read() method, which reads at most n bytes from file desciptor and return a string containing the bytes read. If the end of file referred to by fd has been reached, an empty string is returned.\n    *    **torchvision.transforms.Compose()** : torchvision.transforms is used for common image transformations and when Compose is chained with it to Compose several transforms together.\n    *    **transforms.ToTensor()**: This just converts input image to PyTorch tensor.\n    *    **torch.tensor()**:  It infers the dtype automatically. It always copies the data and torch.tensor(l) is equivalent to l.clone().detach().\n    *    **transforms.Resize()** : The default interpolation is InterpolationMode.BILINEAR. It resize the input image as per the height and width provided. \n    *    **transforms.functional.crop()** : Crop the given image at specified location and output size and it returs torch.Tensor.\n    \n    \nWith preprocessed Images dataset is created and then we can split the dataset to training and test set. \n  \n    ","d395098b":"The loss for the model is to be set, then run the training and validation function for the set number of epochs. Notice, depending on the number of epochs this step may take a while on a CPU. Also, the default learning rate is not optimal for all of the models, so to achieve maximum accuracy it would be necessary to tune for each model separately.","7536e242":"In mathematics, a tensor is an algebraic object that describes a multilinear relationship between sets of algebraic objects related to a vector space. The dataset created will be of tensor.","1884d27d":"# Reference","67afdabd":"To split dataset, torch.utils.data.random_split() is used which randomly splits the dataset into non-overlapping new datasets of given lengths.","123184ae":"# Download the resnet34 layers pre-trained model","13e1aea1":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#9041A0;\n           font-size:110%;\n           font-family:Segoe UI;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\n    ResNet - Deep Residual Learning for Image Recognition\n             <\/p> <\/div>","f5044ac0":"\nTo view different layers representing different operations 'model name' is to be executed.","42a478bd":"# Visualisation and Analysis of Target Class","db177233":"All the images are stored in img_names list and annotations in xml_names list.","9e61153f":"# Model Building","7d72b4df":"![image.png](attachment:1d1635ad-f60a-416e-955a-ec8cc905368b.png)","10154f9e":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#9041A0;\n           font-size:110%;\n           font-family:Segoe UI;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\n    Dataset contain images :<li style=\"padding-left:1em\">with_mask : 79.37%<\/li>\n    <li style=\"padding-left:1em\">mask_weared_incorrect : 3.02% <\/li>\n    <li style=\"padding-left:1em\">without_mask : 17.61%<\/li>\n             <\/p> <\/div>","dd061dec":"DataLoader() is an iterable that abstracts this complexity for us in an easy API. When shuffle is made true data is shuffled, after the iteration is over for all batches.\n","a50754de":"The OS module in Python provides functions for creating and removing a directory (folder), fetching its contents, changing and identifying the current directory, etc.\n\nHere we are using os.walk() and os.path.join()\n\n* os.walk() returns a generator, that creates a 3-tuple\n    * dirpath :- It is a string, with provides the path to the directory.\n    * dirnames(in dirpath) : - It is a list of the names of the subdirectories in dirpath (excluding '.' and '..')\n    * filenames(in dirpath):- It is a list of the names of the non-directory files in dirpath. \n\nEvery time the generator is called it will follow each directory recursively until no further sub-directories are available from the initial directory that walk was called upon.\n\n* os.path.join(dirpath, name) is used to get a full path (which begins with top) to a file or directory in dirpath. \n\nYou can read more from [here](https:\/\/docs.python.org\/3\/library\/os.html#os.walk)\n","d03e3687":"The Image is preprocessed as PyTorch tensors. The transforms function of torchvision is used to define pipeline of basic data preprocessing.","8f63fcce":"**Work in Progress, Please upvote if you find this notebook useful!!!!**","51e50806":"# Splitting Dataset into Training and Test Set","f5cc5cd8":"# Importing Libraries","2505f37f":"# Extraction of Images and Annotations","a66c342f":"![image.png](attachment:e4337b59-e25c-4c22-afa6-45260ecb3a90.png)","4d2c492b":"https:\/\/arxiv.org\/pdf\/1512.03385.pdf\n\n","e31adfb1":"In torchvision.models.resnet34(), when pretrained is provided as True, it returns a model pre-trained on ImageNet.\n","8ca9074d":"As there are 3 classes in target, we can use 3 colors for cascading the face.\n* Red --> without_mask\n* Green --> with_mask\n* Yellow -- > mask_weared_incorrect","c753bb3a":"# Samples in Training Set","7f9ab588":"# Model Details","4bb09366":"The last layer of the downloaded model screenshot is below \n\n![image.png](attachment:646b72d0-ae1e-4bd6-982a-a1999a84d42b.png)\n\nIt indicates out_features is equal to 1000, so it must reinitialize model to be a Linear layer with 512 input features and 3 output class.\n\n**torch.nn.Linear()** applies a linear transformation to the incoming data: \n\n![image.png](attachment:2c6a2000-f0d3-4a1d-ab99-d7fc3fbccb77.png)\n\n\n","fbb6bf0f":"Here annotations are in xml and images are in png. So xmltodict can be installed for which is a Python module that makes working with XML feel like working with JSON.xmltodict also lets roundtrip back to XML with the unparse function, has a streaming mode suitable for handling files that don\u2019t fit in memory, and supports XML namespaces.","175d08e5":"param.requires_grad will be which is by default True for feature extraction we have set it False. When it is True  backpropagration will be there. In order for the model to avoide overfitting, layers is to be freezed.\n\nIn order to freeze a layer, requires_grad is to be set to False. Here setting freezeing layers from 1 to 6.","916c1eae":"# Training Model","9b5a442c":"![image.png](attachment:76dca67b-113b-434a-b433-e117cb02fc7d.png)","74563702":"# Feature Extraction","86b76975":"The formulation of F(x) +x can be realized by feedforward neural networks with \u201cshortcut connections\u201d. The shortcut connections here simply perform identity mapping, and their outputs are added to the outputs of the stacked layers.The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries without modifying the solvers.","8cc8cfe2":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#9041A0;\n           font-size:110%;\n           font-family:Segoe UI;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\n    Target Classes belongs to :<li style=\"padding-left:1em\">with_mask<\/li>\n    <li style=\"padding-left:1em\">mask_weared_incorrect<\/li>\n    <li style=\"padding-left:1em\">without_mask<\/li>\n             <\/p> <\/div>","409ca92e":"# Images Identification with Target Class","8094e1d7":" Setting Model Parameters, attribute requires_grad to false when feature extraction is done."}}