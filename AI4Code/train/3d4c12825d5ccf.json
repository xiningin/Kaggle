{"cell_type":{"eae86b7b":"code","4a9482c0":"code","ad24cca5":"code","c23bbb65":"code","c62ce8a4":"code","a033fdb9":"code","1a5f3a4d":"code","4e4b739d":"code","2fe4ddec":"code","a9083663":"code","c190fb54":"code","3fe3d035":"code","539967bb":"code","b1752a73":"code","e2a756cf":"code","d8205302":"code","3844e228":"code","0c85e5ad":"code","ccdcc5ab":"code","dc2dfd99":"code","db5bfd3f":"code","cec1113a":"code","79f2a220":"code","21053819":"code","474c71a2":"code","eaa9cd03":"code","7632401a":"code","d0bdc40d":"code","56b69a15":"code","81aabbbb":"code","5f7683f0":"code","9b361b23":"code","7615eafc":"code","4a7f16fd":"code","43eda8d1":"code","1e621d69":"code","a1211bfa":"code","a5810b24":"code","adf5942f":"markdown","d1b3619d":"markdown","405e56ce":"markdown","2c3f1bcf":"markdown","9ac8e57c":"markdown","33392ca3":"markdown","c9b92e87":"markdown","bccf079f":"markdown","864e6e91":"markdown","8481e784":"markdown","1332ba02":"markdown","fb46728b":"markdown","05ab24fe":"markdown","5bb91c45":"markdown","8c169110":"markdown","a7fec908":"markdown","12f37ac4":"markdown","e3e5103c":"markdown","57a508d8":"markdown","5f01178d":"markdown"},"source":{"eae86b7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom matplotlib.pyplot import figure\nnp.random.seed(1)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4a9482c0":"train = pd.read_csv('..\/input\/drugsComTrain_raw.csv')\ntest = pd.read_csv('..\/input\/drugsComTest_raw.csv')\ntrain.head()","ad24cca5":"train.describe()","c23bbb65":"plt.hist(train.rating)\nplt.show()","c62ce8a4":"plt.figure(figsize=(10,6))\nplt.hist(train.usefulCount, bins=60)\nplt.show()","a033fdb9":"plt.scatter(train.rating, train.usefulCount)\nplt.show()","1a5f3a4d":"\ndrugcounts = train['drugName'].value_counts()\ndrugcounts_filtered = drugcounts[drugcounts >= 10]\n\nplt.figure(figsize=(20,6))\nplt.bar(drugcounts[:30].index, drugcounts[:30])\nplt.xticks(rotation=70)\nplt.show()","4e4b739d":"plt.figure(figsize=(20, 6))\nplt.hist(drugcounts, bins=200)\nplt.show()","2fe4ddec":"filtered_train = train[train['drugName'].isin(drugcounts_filtered.index)]\nprint(train.shape)\nprint(filtered_train.shape)","a9083663":"rating_avgs = (filtered_train['rating'].groupby(filtered_train['drugName']).mean())\nrating_avgs.sort_values()","c190fb54":"plt.hist(rating_avgs)","3fe3d035":"illnesscounts = train['condition'].value_counts()\nillnesscounts = illnesscounts[illnesscounts > 20]\nprint(illnesscounts)\n","539967bb":"filtered_train = train[train['condition'].isin(illnesscounts.index)]\ncurability = (filtered_train['rating'].groupby(filtered_train['condition']).mean())\ncurability.sort_values()","b1752a73":"plt.hist(curability)\nplt.show()","e2a756cf":"sns.boxplot(curability)","d8205302":"sns.boxplot(rating_avgs)","3844e228":"plt.scatter(rating_avgs, drugcounts_filtered)\nplt.show()","0c85e5ad":"b = \"'@#$%^()&*;!.-\"\nX_train = np.array(filtered_train['review'])\nX_test = np.array(test['review'])\n\ndef clean(X):\n    for index, review in enumerate(X):\n        for char in b:\n            X[index] = X[index].replace(char, \"\")\n    return(X)\n\nX_train = clean(X_train)\nX_test = clean(X_test)\nprint(X_train[:50])","ccdcc5ab":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Embedding\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom keras.utils import to_categorical\nfrom gensim.models import Word2Vec\nfrom nltk.cluster import KMeansClusterer\nimport nltk\n\nX_train = filtered_train['review']\nX_test = test['review']\nvectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'),\n                             lowercase=True, max_features=5000)\ntest_train = pd.concat([X_train, X_test],ignore_index=True)\nprint(filtered_train['review'].shape)\nprint(test['review'].shape)\nprint(test_train.shape)\nX_onehot = vectorizer.fit_transform(test_train)\nstop_words = vectorizer.get_stop_words()\nprint(type(X_onehot))\n","dc2dfd99":"print(X_onehot.shape)\nprint(X_onehot.toarray())","db5bfd3f":"names_list = vectorizer.get_feature_names()\nnames = [[i] for i in names_list]\nnames = Word2Vec(names, min_count=1)\nprint(len(list(names.wv.vocab)))\nprint(list(names.wv.vocab)[:5])","cec1113a":"def score_transform(X):\n    y_reshaped = np.reshape(X['rating'].values, (-1, 1))\n    for index, val in enumerate(y_reshaped):\n        if val >= 8:\n            y_reshaped[index] = 1\n        elif val >= 5:\n            y_reshaped[index] = 2\n        else:\n            y_reshaped[index] = 0\n    y_result = to_categorical(y_reshaped)\n    return y_result\n    \n    print(X_onehot)","79f2a220":"\ny_train_test = pd.concat([filtered_train, test], ignore_index=True)\ny_train = score_transform(y_train_test)\nprint(y_train)\nprint(y_train.shape)\n","21053819":"from numpy.random import seed\nfrom keras.layers import Dropout, Flatten\nfrom keras.preprocessing import sequence\nfrom keras.layers.embeddings import Embedding\n\nnp.random.seed(1)\nmodel = Sequential()\nmodel.add(Dense(units=256, activation='relu', input_dim=len(vectorizer.get_feature_names())))\nmodel.add(Dense(units=3, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary","474c71a2":"history = model.fit(X_onehot[:-53866], y_train[:-53866], epochs=5, batch_size=128, verbose=1, validation_data=(X_onehot[157382:157482], y_train[157382:157482]))","eaa9cd03":"scores = model.evaluate(X_onehot[157482:], y_train[157482:], verbose=1)","7632401a":"scores[1]","d0bdc40d":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","56b69a15":"all_names = [i.split() for i in X_train]\nfeature_vectors = names[names.wv.vocab]\nprint(feature_vectors[0])","81aabbbb":"np.random.seed(1)\nall_names_rand = [all_names[np.random.randint(low=1, high=150000)] for i in range(5000)]\nprint(len(all_names_rand))","5f7683f0":"all_names_list = Word2Vec(all_names_rand, min_count=1)\nall_names_vec = all_names_list[all_names_list.wv.vocab]\nprint(all_names[:2])","9b361b23":"kclusterer_all = KMeansClusterer(5, distance=nltk.cluster.util.cosine_distance, repeats=10)\nassigned_clusters_all = kclusterer_all.cluster(all_names_vec, assign_clusters=True)\nprint(len(assigned_clusters_all))","7615eafc":"def generate_df(feature_names):\n    \n    all_words_dict = dict(zip(all_names_list.wv.vocab, assigned_clusters_all))\n    for key in list(all_words_dict.keys()):\n        if key in list(feature_names):\n            pass\n        else:\n            del all_words_dict[key]\n    sorted_names = []\n    \n    for cluster in range(5):\n        cluster_list = []\n        for key, value in all_words_dict.items():\n            if value == cluster:\n                \n                cluster_list.append(key)\n        sorted_names.append(cluster_list)\n        \n    for index, entry in enumerate(sorted_names):\n        entry.sort()\n    df_all = pd.DataFrame(sorted_names).T\n    print(df_all[:50])\n    return df_all, sorted_names\n    \n\ndf,sorted_names_all = generate_df(names.wv.vocab)","4a7f16fd":"def test_clusters(cluster_list):\n    \n    score_list = []\n    lens = []\n    \n    reviewnum = 15000\n    \n    np.random.seed(3)\n    indicies = [np.random.randint(low=1, high=150000) for i in range(reviewnum)]\n    X_sample =  test_train[indicies]\n    y_sample = y_train[indicies]\n    \n    for cluster in cluster_list:\n        \n        lens.append(len(cluster))\n\n        X_onehot = vectorizer.fit_transform(X_sample)\n        \n        X_onehot = X_onehot.toarray()\n\n        cluster_indexes = []\n        print(len(list(names.wv.vocab)))\n\n        for index, feature_name in enumerate(list(names.wv.vocab)):\n            if feature_name in cluster:\n                cluster_indexes.append(index)\n\n        features = len(cluster_indexes)\n        \n        X_onehot = X_onehot[:, cluster_indexes]\n\n        model_cluster = Sequential()\n        model_cluster.add(Dense(units=256, activation='relu', input_dim=features))\n        model_cluster.add(Dense(units=3, activation='softmax'))\n\n        model_cluster.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n        model_cluster.summary\n        \n        history = model_cluster.fit(X_onehot[:reviewnum-2000], y_sample[:reviewnum-2000], epochs=10, batch_size=128, verbose=2, validation_data=(X_onehot[reviewnum-2000:reviewnum-1900], y_sample[reviewnum-2000:reviewnum-1900]))\n        \n        y_test = score_transform(test)\n        scores = model_cluster.evaluate(X_onehot[reviewnum-2000:], y_sample[reviewnum-2000:], verbose=1)\n        \n        score_list.append(scores[1])\n    for index, entry in enumerate(score_list):\n        print(\"cluster\", index + 1, \"accuracy: \", str(entry) + \". number of words for cluster: \", lens[index])","43eda8d1":"test_clusters(sorted_names_all)","1e621d69":"test_clusters([list(names.wv.vocab)[:1000], list(names.wv.vocab)[2000:3000], list(names.wv.vocab)[3000:4000], list(names.wv.vocab)[4000:5000]])","a1211bfa":"vectorizer = CountVectorizer(binary=True, stop_words=stopwords.words('english'),\n                             lowercase=True, min_df=3, max_df=0.9, max_features=500)\nX_onehot = vectorizer.fit_transform(test_train)\nnames_list = vectorizer.get_feature_names()\nnames = [[i] for i in names_list]\nnames = Word2Vec(names, min_count=1)\n\ndf, sorted_names_all = generate_df(names.wv.vocab)\n","a5810b24":"test_clusters(sorted_names_all)","adf5942f":"Now our distribution looks extremely normal compared to our overall ratings histogram. This makes sense considering most reviews were either high or low.","d1b3619d":"Sanity checking to see if the dimensions make sense, then peeking at what the actual X_onehot matrix looks like.\nWe have our trimmed m number of examples, as well as 5000 columns so this looks right!","405e56ce":"Series containing the effectiveness of each drug, based on average of ratings.","2c3f1bcf":"I use a CountVectorizer to vectorize each of the different reviews into 5000 feature row vectors. It's set to not add in very common words such as \"and\" and \"the\", and also ignore very rare words.","9ac8e57c":"Seems like the more positive reviews are found by people to be more useful.","33392ca3":"I experimented with different numbers of epochs and batch sizes as well, and found roughly 7 epochs and a batch size of 128 to be optimal. Anything more than 7 epochs resulted in the model beginning to overtrain.","c9b92e87":"Seeing which illnesses are most effective at being treated with medicine.","bccf079f":"When the entire dataset plotted, we can see that there is a huge right skew in our data for how common certain drugs are.","864e6e91":"Checking to see what the most common illnesses are as well as dropping anything with under 20 cases.","8481e784":"Checking to see if there's any correlation between average drug rating and the amount of times it is used. Not really seeing anything from this plot.","1332ba02":"Creating a helper function to easily create our Y labels. We need to transform the review column to a m number of reviews by 10 columns, with a 1 in the place of the index of the label. If it were a rating of 5, the corresponding 1 would be in index 5.","fb46728b":"Interesting distribution, almost like the opposite of a bellcurve. People only leave reviews if the medicine was either really good or really bad it seems.","05ab24fe":"Incredibly symmetric histogram of illness effectiveness, with its mean being centered around 7.25.","5bb91c45":"Bar chart of top 30 most common drugs.","8c169110":"This model predicts the if the rievew is positive, negative or neutral with a ~92% accuracy, according to this test set and output score.","a7fec908":"Next is exploring model architectures and hyperparameters for a model predicting a specific 1-10 rating based on review.\nThis is a classification NLP problem and I'll be using Keras.","12f37ac4":"As expected, extremely right skewed graph of usefulness.","e3e5103c":"Checking if our helper function works. Seems good.","57a508d8":"FIltering out drugs with less than 10 instances from our dataset.","5f01178d":"I experimented a lot with different kinds of simple NN architecture. It seemed like ~256 units was the sweet spot for accuracy without overfitting. Any units above would give me less accuracy as the model continued to train as well as overall performing worse, and less units also just gave me worse accuracy overall.\nI also discovered that adding any additional layers to the model resulted in a loss of accuracy, even when I kept the number extremely small as to avoid overfitting.\nI'm assuming that the data is not complicated enough for a deep network, and a single layer NN is better suited."}}