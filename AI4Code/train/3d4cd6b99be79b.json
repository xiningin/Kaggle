{"cell_type":{"a03f67f1":"code","267032a4":"code","d40527e5":"code","fcdbc8c7":"code","28869003":"code","f1098068":"code","8f88c80b":"code","efc8993e":"code","657b8503":"code","68fc6d2c":"code","9e7025a8":"code","c79f8d21":"code","87d17dae":"code","f0a88478":"code","00f9ac12":"code","f4e6e355":"code","76faaeb5":"code","23aa136e":"code","51bb2249":"code","c9349c4d":"code","82a3c903":"code","b46c69d7":"code","63db3ef8":"code","d8650395":"code","e0619781":"code","a0e28326":"code","d5ca4942":"code","49de978e":"code","db51af5d":"code","91f13201":"code","14f174f4":"code","d0a64973":"code","0003c72b":"code","3a78a37a":"code","97261651":"code","2434bd12":"code","13734551":"code","4e15dc75":"code","3b5fa491":"code","100cbabe":"code","04f7da67":"markdown","8bdb64c5":"markdown","e1fea86f":"markdown","ca0977e9":"markdown","f8aeac39":"markdown","1c2efc2d":"markdown","59f6cdb0":"markdown","da8d8f42":"markdown","f46afb2c":"markdown","1c56d748":"markdown","16c8b6ba":"markdown","6147b5b3":"markdown","8f31b6bc":"markdown","a47f97ad":"markdown","3679c289":"markdown","224f8fb3":"markdown"},"source":{"a03f67f1":"import os\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, confusion_matrix\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import train_test_split, GridSearchCV","267032a4":"df_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# columns:\nprint('columns for train:\\n', df_train.columns.values)\nprint()\nprint('columns for test:\\n', df_test.columns.values)","d40527e5":"df_train.head(5)","fcdbc8c7":"df_train.info()\n\n#df_train.select_dtypes(\"object\").columns","28869003":"np.round((df_train.isnull().sum() \/ df_train.shape[0]) * 100, 2)","f1098068":"missing_data = np.round((df_train.isnull().sum() \/ df_train.shape[0]) * 100, 2)\nmissing_data = missing_data.sort_values(ascending = False)\nsns.barplot(missing_data.values, missing_data.index, color = 'b')","8f88c80b":"fea_con = ['Age', 'Fare']\n\nfor fea in fea_con:\n    print('{}: mean: {:.2f}, median: {:.2f}, 25%: {:.2f}, 75%: {:.2f}'.format(fea,\n        df_train[fea].mean(),df_train[fea].median(),df_train[fea].quantile(0.25),\n        df_train[fea].quantile(0.75)))","efc8993e":"fea_cat = ['Survived','Pclass','Sex','SibSp','Parch','Embarked'] # 'Cabin' - removed\n\nfor fea in fea_cat:\n    print('{}: unique cat-s:\\n {} \\n probs: \\n {}'.format(fea, df_train[fea].unique(),\n                                                         df_train[fea].value_counts()))\n    print()","657b8503":"vars = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n\nfor var in vars:\n    if var in ['Age', 'Fare']:\n        #print(var)\n        print()\n        print(df_train[['Survived', var]].groupby(['Survived'], as_index = False).mean())\n    else:\n        print()\n        print(df_train[['Survived', var]].groupby([var], as_index = False).mean()\n              .sort_values(by = ['Survived'], ascending = False))","68fc6d2c":"plt.figure(figsize = (10,4))\n# Age by Sex:\nplt.subplot(1,2,1)\nplt.hist(df_train[(df_train['Survived'] == 1) & (df_train['Sex'] == 'female')]['Age'], \n             label = 'surv', alpha = 0.5, edgecolor = 'grey')\nplt.hist(df_train[(df_train['Survived'] == 0) & (df_train['Sex'] == 'female')]['Age'], \n             label = 'no_surv', alpha = 0.5, edgecolor = 'grey')\nplt.title('Female Psgs')\nplt.ylim(0,100)\nplt.ylabel('No Psgs')\nplt.xlabel('Age')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.hist(df_train[(df_train['Survived'] == 1) & (df_train['Sex'] == 'male')]['Age'], \n             label = 'surv', alpha = 0.5, edgecolor = 'grey')\nplt.hist(df_train[(df_train['Survived'] == 0) & (df_train['Sex'] == 'male')]['Age'], \n             label = 'no_surv', alpha = 0.5, edgecolor = 'grey')\nplt.title('Male Psgs')\nplt.ylim(0,100)\nplt.xlabel('Age')\nplt.legend()\n\nplt.show()","9e7025a8":"# Pclass:\nbase, top = [],[]\nfor i in np.sort(df_train['Pclass'].unique()):\n    #print(i)\n    base_ = df_train[(df_train['Survived'] == 0) & (df_train['Pclass'] == i)]['Pclass'].count()\/df_train[df_train['Pclass'] == i]['Pclass'].count()\n    base.append(base_*100)\n    top.append((1 - base_)*100)\n    #print(base_)\n\nplt.bar(['1st', '2nd', '3rd'], base, label = 'no_surv')\nplt.bar(['1st', '2nd', '3rd'], top, bottom = base, label = 'surv')\nplt.title('Class & Survival')\nplt.legend(loc = 4)\nplt.xlabel('class')\nplt.show()","c79f8d21":"base, top = [],[]\nfor i in np.sort(df_train['SibSp'].unique()):\n    #print(i)\n    base_ = df_train[(df_train['Survived'] == 0) & (df_train['SibSp'] == i)]['SibSp'].count()\/df_train[df_train['SibSp'] == i]['SibSp'].count()\n    base.append(base_*100)\n    top.append((1 - base_)*100)\n    \nplt.bar(['0', '1', '2', '3','4', '5', '8'], base, label = 'no_surv')\nplt.bar(['0', '1', '2', '3','4', '5', '8'], top, bottom = base, label = 'surv')\nplt.title('SibSp & Survival')\nplt.legend(loc = 4)\nplt.xlabel('no. of sib\/sp')\nplt.show()","87d17dae":"#df_train.isnull().sum()\ndf_train['Embarked'].value_counts() # S is dominant >> impute NaN with S\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode()[0])","f0a88478":"#df_test.isnull().sum()\ndf_test[df_test['Fare'].isna()] # the individual is Pclass = 3.\n#plt.hist(df_test[df_test['Pclass'] == 3]['Fare']) # median better than mean\n#plt.show()\n\nclass_fare = df_test[['Pclass','Fare']].groupby(['Pclass'], as_index = False).median()\nfare_3rd = class_fare[class_fare['Pclass'] == 3]['Fare'].values[0]\ndf_test['Fare'] = df_test['Fare'].fillna(fare_3rd)","00f9ac12":"dict_sex = {'female':1, 'male': 2}\ndf_train['Sex'] = df_train['Sex'].map(dict_sex)\ndf_test['Sex'] = df_test['Sex'].map(dict_sex)\n\n# df_train['Embarked'].value_counts()\ndict_emb = {'S':1, 'C':2, 'Q':3}\ndf_train['Embarked'] = df_train['Embarked'].map(dict_emb)\ndf_test['Embarked'] = df_test['Embarked'].map(dict_emb)","f4e6e355":"for dataset in (df_train, df_test):\n    dataset['Title'] = dataset['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    print('')\n    print(dataset['Title'].value_counts())","76faaeb5":"# group titles are suggested by others:\ndic_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"}\n\nfor dataset in (df_train, df_test):\n    dataset['Title'] = dataset['Title'].map(dic_titles)\n    print('')\n    print(dataset['Title'].value_counts())","23aa136e":"#df_train['Title'].value_counts()\ndf_train[['Title','Survived']].groupby(['Title'],as_index = False).mean()","51bb2249":"df_train[['Title','Age']].groupby(['Title']).median()\n\n# Title might be important for predicting age\n# use the median as if gaussian distribution mean = median, \n# otherwise if non gaussian - median better.","c9349c4d":"titles_ages = df_train[['Title','Age']].groupby(['Title']).median()#.do_dict\n\n# create a dictionary with title & mean age\ndict_title_age = {}\nfor ind in titles_ages.index:\n    #print(ind)\n    dict_title_age[ind] = np.round(titles_ages.loc[ind]['Age'],2)\n\nprint(dict_title_age)\n\n# create a new variable with numericals:\nfor dataset in (df_train, df_test):\n    dataset['Title_age'] = dataset['Title'].map(dict_title_age)","82a3c903":"# df_train['Cabin'].value_counts() - vary many different observations.\n\nfor dataset in (df_train, df_test):\n    # find missing and assign to Unknown\n    dataset['Cabin'] = dataset['Cabin'].fillna('Un')\n    # get only first letter of cabin\n    dataset['Cabin'] = dataset['Cabin'].map(lambda x: x[0])\n\ndf_train['Cabin'].value_counts() # normalize = True    ","b46c69d7":"# asign Unknown cabin to 0\ndic_cabin = {'U': 0}\n\n# new feature - cabin = 1; unknown = 0:\nfor dataset in (df_train, df_test):\n    dataset['Cabin_bin'] = dataset['Cabin'].map(dic_cabin).fillna(1)\n\n#df_test\ndf_train[['Cabin_bin','Survived']].groupby(['Cabin_bin'], as_index = False).mean()\n\n# those with cabins were much more likely to survive compared to those with unknown cabins.","63db3ef8":"for dataset in (df_test, df_train):\n    # new variable - family_size\n    dataset['family_size'] = dataset['SibSp'] + dataset['Parch'] + 1\n    # new variable alone\n    dataset['alone'] = 0    \n    dataset.loc[dataset['family_size'] == 1 , 'alone'] = 1\n    \n# Now let's examine which is better for predicting survival:","d8650395":"print(df_train[['Survived', 'family_size']].groupby(['family_size'], as_index = False).mean())\nprint()\nprint(df_train[['Survived','alone']].groupby(['alone'], as_index = False).mean())\n\n# family size > 3 - low survival rate \n# but low survival rate for singletons too. maybe include sex as na interaction later on?","e0619781":"# to remember the coding: dict_sex \npd.pivot_table(df_train[['Survived','Sex','family_size']], values=['Survived'], index=['family_size'],\n                    columns=['Sex'], aggfunc=np.mean)\n\n### close to 80% of the 'alone' females survived compared to ~15% of 'alone' males\n### female survival declines drastically if family_size > 3\n### highest survival rate for males if family_size = 3.","a0e28326":"df_train.columns","d5ca4942":"# drop observation with missing Age:\ntrain_imp = df_train.dropna(subset = ['Age'])\n# train_imp.shape\n\ny_train_imp = train_imp['Age']\nX_train_imp = train_imp.drop(columns = ['PassengerId', 'Survived', 'Cabin', 'Name', \n                                        'Ticket', 'Age', 'Embarked', 'Title','Cabin_bin', \n                                        'alone'])","49de978e":"X_train_imp.columns.values","db51af5d":"rfr = RandomForestRegressor(random_state = 42)\nrfr.fit(X_train_imp, y_train_imp)\ny_train_imp_pred = rfr.predict(X_train_imp)\n\nprint('RFR rho: {:.3f}'.format(spearmanr(y_train_imp,y_train_imp_pred)[0]))\nprint('R2 score: {:.3f}'.format(r2_score(y_train_imp,y_train_imp_pred)))\nprint('MAE: {:.3f}'.format(mean_absolute_error(y_train_imp,y_train_imp_pred)))\nprint()\nplt.scatter(y_train_imp,y_train_imp_pred)\nplt.xlabel('observed'), plt.ylabel('RF prediction')\nplt.show()","91f13201":"cols_drop_train = ['PassengerId', 'Cabin', 'Name', 'Ticket', 'Age', 'Embarked','Survived', 'Title','Cabin_bin','alone']\ncols_drop_test = ['PassengerId', 'Cabin', 'Name', 'Ticket', 'Age', 'Embarked','Title','Cabin_bin', 'alone']\n\nfor num, dataset in zip([1,2],(df_train, df_test)):\n    print('dataset ',num)\n    \n    # ensure right cols:\n    if 'Survived' in dataset.columns:\n        cols = cols_drop_train\n    else:\n        cols = cols_drop_test\n        \n    # select Psgs:  \n    for psg in dataset['PassengerId'].values:\n        # print(psg)\n        if dataset[dataset['PassengerId'] == psg]['Age'].isnull().values[0] == True:\n            set2impute = dataset[dataset['PassengerId'] == psg].drop(columns = cols) \n\n            # predicts Age:\n            psg_pred_age = np.round(rfr.predict(set2impute)[0],1)\n        \n            # assign the prediction:\n            dataset.loc[dataset['PassengerId'] == psg, ['Age']] = psg_pred_age","14f174f4":"X = df_train.drop(columns = ['Survived', 'PassengerId','Name','Ticket','Cabin','Title','alone', 'SibSp','Parch'])\ny = df_train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)\nprint('X_train shape: ', X_train.shape)\nprint('X_test shape: ', X_test.shape)\nprint('X columns: ', X_train.columns.values)","d0a64973":"rfc_base = RandomForestClassifier()\n\nrfc_base.fit(X_train, y_train)\ny_train_rfc_pred = rfc_base.predict(X_train)\n\nprint('Accuracy RFC base: {:.3f}'.format(accuracy_score(y_train, y_train_rfc_pred)))\n\n# really, really overfitting.","0003c72b":"from sklearn.inspection import permutation_importance\n\nresult = permutation_importance(rfc_base, X_train, y_train, n_repeats=5,\n                                random_state=42)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","3a78a37a":"fea_imp_base = dict(zip(X.columns.values, rfc_base.feature_importances_))\n\nx_range = range(len(X.columns.values))\nplt.bar(x_range, rfc_base.feature_importances_)\nplt.xticks(x_range, X.columns.values)\nplt.xticks(rotation = 45)\nplt.title('Feature Importance base model')\nplt.show()","97261651":"search_param = {'n_estimators': [10, 100, 500, 1000],\n               'max_features':('auto', 'sqrt'),\n               'max_depth': [5, 15, 20, 50]}\n\nsearch = GridSearchCV(rfc_base, param_grid = search_param,\n                     cv = 5, scoring = 'accuracy')\n\nsearch.fit(X_train, y_train)","2434bd12":"print(search.best_params_)\nprint(search.best_score_) # probs improves generalisation","13734551":"rfc_updated = RandomForestClassifier(n_estimators = 1000, max_features = 'auto',\n                                     max_depth = 5, random_state = 42)\n\nrfc_updated.fit(X_train, y_train)\ny_train_rfc_upd_pred = rfc_updated.predict(X_train)\n\nprint('Accuracy RFC base TRAIN: {:.3f}'.format(accuracy_score(y_train, y_train_rfc_pred)))\nprint('Accuracy RFC updated TRAIN: {:.3f}'.format(accuracy_score(y_train, y_train_rfc_upd_pred)))","4e15dc75":"y_test_rfc_upd_pred = rfc_updated.predict(X_test)\nprint('Accuracy RFC updated TEST: {:.3f}'.format(accuracy_score(y_test, y_test_rfc_upd_pred)))\nprint()\nprint(confusion_matrix(y_test, y_test_rfc_upd_pred))","3b5fa491":"result = permutation_importance(rfc_updated, X_test, y_test, n_repeats=5,\n                                random_state=42)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=X_test.columns[sorted_idx])\nax.set_title(\"Permutation Importances (test set)\")\nfig.tight_layout()\nplt.show()","100cbabe":"#df_test.head(n=2)\n#df_test.isnull().sum()\n\npred_to_subm = rfc_updated.predict(df_test.drop(columns = ['PassengerId','Name','Ticket',\n                                                           'Cabin','Title','alone','Parch',\n                                                           'SibSp']))\n\nfirst_submission = pd.DataFrame({'PassengerId': df_test['PassengerId'],\n                                'Survived': pred_to_subm})\nfirst_submission.to_csv('submission_final.csv', index = False)","04f7da67":"*transform `Sex` & `Embarked` to numerical:*","8bdb64c5":"## Create a feature labelled `Title` based on Name \n\nthis could help with imputing `Age` as well as predict `Survived`? As described [here](https:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9).","e1fea86f":"## Feature Importance:\n\nusing the default [permutation_importance](https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py) and default *.feature_importance*\n\n- `Sex`, `Fare`, `Age` &`Pclass` are \"strongest\" predictors\n- `SibSp` & `Parch` are less important, perphaps merge?","ca0977e9":"-------------------------------\n#### Quick look at missing values in `df_train`:*\n\n- `cabin` > 75% is missing >> drop Var?\n- `Age` ~ 20% is missing >> more complex imputation?\n- `Embarked` < 1% is missing >> most frequent imputation?\n","f8aeac39":"## Optimise Hyperparameters using `GrisSearchCV`:\n\n(using very limited number of hyperparams and values)","1c2efc2d":"## Update RandomForestClassifier post GridSearchCV:","59f6cdb0":"# Summary: Predicting *Survived* in the **Titanic** dataset:\n\n**For best score go to notebook *Verion 6* on your right**\n\nI am messing around with feature engineering, which sadly often doesn't improve the model :\/\n \n1. Exploratory Data analysis\n    - summary \n    - basic visualisation\n2. Imputations of missing values & Feature Engineering:\n    - imputation using first order statistics (mean\/median\/most frequent) or Random Forest for `Age`\n    \n3. Training base `RandomForestClassifier` & Optimising hyperparamters using `GridSearchCV`\n4. Predicting `Survived` in hold-out dataset & in test dataset","da8d8f42":"## Impute Missing Values in `Age`\n\nrather than using the mean\/median of `Age` to impute missing values, more complicated methods might be \nbetter here. For some interesting suggestions such as calculating mean age of a group of 'similar passengers' [this](https:\/\/www.kaggle.com\/allohvk\/titanic-missing-age-imputation-tutorial-advanced) discussion on Kaggle. \n\nHere, we train Random Forest regression using the data where age information is present to predict instances where age is missing. the model might not have the best generalisation but could do better than simply taking the mean.\n\nusing the `df_train`, split into train (no NaNs) and test (NaNs) to quickly train a RF to predict `Age`:","f46afb2c":"*input the missing values based on the RFR prediction:*","1c56d748":"## Look at `SibSpl` and `Parch` to find better combinations such as `family_size` or `travel_alone`?","16c8b6ba":"## Tweek `Cabin` a bit:\n\nwhich is best - a binary cabin vs no cabin or multi cabins?\n\nWe can either do `labeled` or `one-hot` [encoding](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) for this categorical feature. Given that `Cabin` is not an ordinal feature and that Random Forest (our model of choice) does not handle well one-hot-encoding (or dummy variables), we might consider just including a binary (cabin - no cabin) feature?","6147b5b3":"## Fit model to test data and submit prediction:","8f31b6bc":"------------------\n# RandomForestClassifier:\n\n- split training data to train & test \n- run base RFC model \n- use `GridSearchCV` to find optimal Hparams\n- run final model on hold-out data\n- run final model on test data & submit ","a47f97ad":"----------------------------\n# Exploratory data analysis:\n\nMixture of categorical & conitnious variables:","3679c289":"--------------------------------------------------\n# Fit updated model on the hold-out & test datasets:","224f8fb3":"## Missing Values:"}}