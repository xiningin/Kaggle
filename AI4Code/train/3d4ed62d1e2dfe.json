{"cell_type":{"1743d63f":"code","23e9fda7":"code","19e6c23d":"code","8ef0b3e1":"code","cdfbf23e":"code","93c3c9b1":"code","a84a3724":"code","3320b30a":"markdown","c5a88b90":"markdown","69b9f6b6":"markdown","d095a571":"markdown","9d3b8a9f":"markdown","fe58ca5e":"markdown","c424d397":"markdown","864c3b4f":"markdown","ac347b44":"markdown"},"source":{"1743d63f":"import gc\nimport os\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport math\nfrom keras.callbacks import Callback\nfrom keras import backend\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, GlobalAveragePooling2D, BatchNormalization, Input\nfrom keras.optimizers import Adam, SGD, Nadam\nfrom keras.metrics import categorical_accuracy\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, TensorBoard\n\nimport cv2\nimport PIL\nfrom PIL import ImageOps, ImageFilter, ImageDraw\n\nfrom keras import backend as K\nwarnings.filterwarnings(action='ignore')\n\n# Efficient Net\uc740 \uc131\ub2a5\ub3c4 \uc88b\uace0, \uac00\ubcbc\uc6b4 \ube44\uad50\uc801 \ucd5c\uc2e0 \ubaa8\ub378\uc785\ub2c8\ub2e4.\n!pip install git+https:\/\/github.com\/qubvel\/efficientnet\nfrom efficientnet import EfficientNetB3\n\nK.image_data_format()","23e9fda7":"model_path = '.\/model\/'\n\nif(not os.path.exists(model_path)):\n    os.mkdir(model_path)\n    \nimg_path = os.listdir('..\/input')\nprint(img_path)\n\nCROP_PATH = '..\/input\/' + img_path[0]\nDATA_PATH = '..\/input\/' + img_path[1]\n\n# image folder path\nTRAIN_IMG_PATH = CROP_PATH +'\/train_crop\/'\nTEST_IMG_PATH = CROP_PATH +'\/test_crop\/'\n\n# read csv\ndf_train = pd.read_csv(DATA_PATH + '\/train.csv')\ndf_test = pd.read_csv(DATA_PATH + '\/test.csv')\ndf_class = pd.read_csv(DATA_PATH + '\/class.csv')\n\nfrom sklearn.model_selection import train_test_split\n\ndf_train[\"class\"] = df_train[\"class\"].astype('str')\n\ndf_train = df_train[['img_file', 'class']]\ndf_test = df_test[['img_file']]\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Parameter\nnb_test_samples = len(df_test)\n\n# batch size\ub294 \uc81c\uac00 \uc0ac\uc6a9\ud558\uae30\ub860 32\uac00 \ucd5c\ub300\uc600\uc2b5\ub2c8\ub2e4. \uc591\uc0c8\uc6d0\ub2d8 \ucee4\ub110\uc744 \ubcf4\uba74 128\ub85c \ub418\uc5b4\uc788\ub294\ub370,\n# \uc5b4\ub5bb\uac8c \ud070 \ubc30\uce58\ub97c \uc0ac\uc6a9\ud558\uc167\ub294\uc9c0 \uc2e0\uae30\ud560 \ub530\ub984\uc785\ub2c8\ub2e4..\nbatch_size = 32\n\n# Define Generator config\n# ImageGenerator\uc5d0\ub294 \uc5ec\ub7ec\uac00\uc9c0 param\uc774 \uc874\uc7ac\ud569\ub2c8\ub2e4. \uacf5\uc2dd API\ubb38\uc11c\ub97c \ucc38\uace0\ud558\uc2dc\uae38 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4.\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 60,\n#     shear_range = 0.25,\n    width_shift_range=0.30,\n    height_shift_range=0.30,\n    horizontal_flip = True, \n    vertical_flip = False,\n    zoom_range=0.25,\n    fill_mode = 'nearest',\n    rescale = 1.\/255)\n\nval_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ndef get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0 :\n        return (num_samples \/\/ batch_size) + 1\n    else :\n        return num_samples \/\/ batch_size\n    \nbase_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n\n# \ub370\uc774\ud130\uc14b\uc774 \ud3b8\ud5a5\ub418\uc5b4\uc788\ub2e4\ub294 \ubd84\uc11d\uae00\uc744 \ubcf8 \ud6c4\uc5d0 \uba87 \uac00\uc9c0 \ucc3e\uc544\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n# sklearn\uc744 \ud65c\uc6a9\ud558\uc5ec class weight\ub97c \uc801\uc6a9\ud574\uc8fc\ub294 \ubc29\uc2dd\uc785\ub2c8\ub2e4.\n# \uc0ac\uc2e4 \uc788\uace0 \uc5c6\uace0, \uc131\ub2a5 \ud5a5\uc0c1\uc740 \uac00\uc838\ub2e4\uc8fc\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(df_train['class']),\n                                                 df_train['class'])","19e6c23d":"# init params\nlr = 2e-4\n\nepochs = 100\nT_max = 10 # Cosine Annealing\uc744 \uc704\ud55c param, \uc77c\ubd80\ub7ec \uc9c0\uc6b0\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\nn_cycles = epochs \/ 100\n\ndef get_callback(model_path):\n    callback_list = [\n              ModelCheckpoint(filepath=model_path, monitor='val_loss',\n                      verbose=1, save_best_only=True),\n              ReduceLROnPlateau(monitor='val_loss',\n                        factor=0.2,\n                        patience=3,\n                        min_lr=1e-6,\n                        cooldown=1,\n                        verbose=1),\n      EarlyStopping(monitor = 'val_f1_m', patience = 5)\n              ]\n    return callback_list\n\ndef get_model(input_size):\n    inputs = Input(shape = (input_size, input_size, 3), name = 'input_1')\n    x = base_model(inputs)\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(2048, kernel_initializer='he_normal')(x)\n    x = Dropout(0.3)(x)\n    x = Activation('relu')(x)\n    x = Dense(196, activation = 'softmax')(x)\n\n    model = Model(inputs = inputs, outputs = x)\n#     sgd = SGD(lr=lr, decay=1e-8, momentum=0.9, nesterov=True)\n    nadam = Nadam(lr = lr)\n    model.compile(optimizer= nadam, loss='categorical_crossentropy', metrics=[categorical_accuracy,f1_m, precision_m, recall_m])\n    return model\n\nfrom sklearn.model_selection import StratifiedKFold\n\nk_folds = 5\nimg_size = (299, 299)\nskf = StratifiedKFold(k_folds, random_state = 2019)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory='..\/input\/' + img_path[0] + '\/test_crop',\n    x_col='img_file',\n    y_col=None,\n    target_size= img_size,\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)","8ef0b3e1":"j = 1\n\nimg_size = (299, 299)\nmodel_names = []\nfor (train_index, valid_index) in skf.split(\n    df_train['img_file'], \n    df_train['class']):\n    \n    traindf = df_train.iloc[train_index, :].reset_index()\n    validdf = df_train.iloc[valid_index, :].reset_index()\n    nb_train_samples = len(traindf)\n    nb_validation_samples = len(validdf)\n\n    print(\"=========================================\")\n    print(\"====== K Fold Validation step => %d\/%d =======\" % (j,k_folds))\n    print(\"=========================================\")\n\n    # Make Generator\n    train_generator_299 = train_datagen.flow_from_dataframe(\n        dataframe=traindf, \n        directory=TRAIN_IMG_PATH,\n        x_col = 'img_file',\n        y_col = 'class',\n        target_size = img_size,\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=batch_size,\n        seed=42\n    )\n\n    validation_generator_299 = val_datagen.flow_from_dataframe(\n        dataframe=validdf, \n        directory=TRAIN_IMG_PATH,\n        x_col = 'img_file',\n        y_col = 'class',\n        target_size = img_size,\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=batch_size,\n        shuffle=True\n    )\n\n    model_name = model_path + str(j) + '_EFFnet_f1.hdf5'\n    model_names.append(model_name)\n    model_EFF = get_model(img_size[0])\n    \n    try:\n        model_EFF.load_weights(model_name)\n    except:\n        pass\n\n    history = model_xception.fit_generator(\n    train_generator_299,\n    steps_per_epoch = get_steps(nb_train_samples, 32),\n    epochs=epochs,\n    validation_data = validation_generator_299,\n    validation_steps = get_steps(nb_validation_samples, 32),\n    callbacks =  get_callback(model_name),\n    class_weight = class_weights\n      )\n        \n    j+=1\n    print(gc.collect())","cdfbf23e":"# \uc0dd\uc131\uc790\uc758 n_cycles\ub97c \uc8fc\uae30\ub85c \uc791\ub3d9\ud558\uac8c \ub429\ub2c8\ub2e4.\n# \uc774 \ucf54\ub4dc\uc758 \ub2e8\uc810\uc740 min_lr \ud30c\ub77c\ubbf8\ud130\uac00 \uc5c6\ub294\uc810.\nclass CosineAnnealingLearningRateSchedule(Callback):\n    # constructor\n    def __init__(self, n_epochs, n_cycles, lrate_max, verbose = 0):\n        self.epochs = n_epochs\n        self.cycles=  n_cycles\n        self.lr_max = lrate_max\n        self.lrates = list()\n    \n    # caculate learning rate for an epoch\n    def cosine_annealing(self, epoch, n_epochs, n_cycles, lrate_max):\n        epochs_per_cycle = math.floor(n_epochs\/n_cycles)\n        cos_inner = (math.pi * (epoch % epochs_per_cycle)) \/ (epochs_per_cycle)\n        return lrate_max\/2 * (math.cos(cos_inner) + 1)\n  \n    # calculate and set learning rate at the start of the epoch\n    def on_epoch_begin(self, epoch, logs = None):\n        if(epoch < 101):\n            # calculate learning rate\n            lr = self.cosine_annealing(epoch, self.epochs, self.cycles, self.lr_max)\n            print('\\nEpoch %05d: CosineAnnealingScheduler setting learng rate to %s.' % (epoch + 1, lr))\n        \n        # \uc774 \ubc11\uc758 \ucf54\ub4dc\ub294 \ud559\uc2b5\uc774 \ub108\ubb34 \ub290\ub9ac\uace0, cycle\uc744 \ub3cc\uae30\ub584\ubb38\uc5d0 \uc77c\uc815 epoch\uc774\uc0c1\uc740 \uace0\uc815\ub41c lr\uc744 \uc0ac\uc6a9\ud558\ub824\uace0 \ud55c \ud754\uc801\uc785\ub2c8\ub2e4.\n        #     elif((epoch >= 65) and (epoch < 75)):\n        #       lr = 1e-5\n        #       print('\\n No CosineAnnealingScheduler set lr 1e-5')\n        #     elif((epoch >= 75) and (epoch < 85)):\n        #       lr = 1e-6\n        #       print('\\n No CosineAnnealingScheduler set lr 1e-6')\n        #     elif((epoch >= 85)):\n        #       lr = 1e-7\n        #       print('\\n No CosineAnnealingScheduler set lr 1e-7')\n\n        # set learning rate\n        backend.set_value(self.model.optimizer.lr, lr)\n        # log value\n        self.lrates.append(lr)","93c3c9b1":"# T_max\ub97c \uc8fc\uae30\ub85c \ub3cc\uac8c \ub429\ub2c8\ub2e4.\n# \uc774\ub54c, T_mult\ub294 \uc81c\uac00 \uc54c\uae30\ub860 rewarmstarting\uc744 \uc704\ud55c \ud30c\ub77c\ubbf8\ud130\ub85c\uc11c,\n# \uac80\uc0c9\ud574\ubcf4\uba74 \uc5b4\ub290 \ube14\ub85c\uadf8\uc5d0\uc11c \uc774 parameter\uc758 \ubcc0\ud654\uc5d0 \ub530\ub978 \uc131\ub2a5\ubcc0\ud654\ub97c \uc815\ub9ac\ud55c \uae00\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n# \uc608\ub97c \ub4e4\uc5b4\uc11c T_max\uac00 10\uc774\uace0, T_mult = 2\uc774\uba74,\n# \uccab \uc8fc\uae30 10, \ub2e4\uc74c \uc8fc\uae30 20, \uadf8 \ub2e4\uc74c\uc8fc\uae30 40\n# \uc774\ub7f0\uc2dd\uc73c\ub85c lr\uc758 \ubcc0\ud654\ud3ed\uc774 \uc810\uc810 \uc904\uc5b4\ub4e4\uac8c \ub429\ub2c8\ub2e4.\n# \uc704 \ucf54\ub4dc\ubcf4\ub2e4 \ud3b8\ub9ac\ud55c\uc810\uc740 \ub354 \uae54\ub054\ud558\uace0, lr_min(eta_min)\uc744 \uc801\uc6a9\ud560 \uc218 \uc788\ub2e4\ub294 \uc810\n\nclass CosineAnnealingLearningRateSchedule(Callback):\n    def __init__(self, n_epochs, init_lr, T_mult = 1, eta_min = 0,restart_decay = 0, verbose = 0):\n        self.T_max = n_epochs\n        self.T_mult = T_mult\n        self.cycle_cnt = 0\n        self.restart_decay = restart_decay\n        self.init_lr = init_lr\n        self.eta_min = eta_min\n        self.lrates = list()\n  # caculate learning rate for an epoch\n\n    def cosine_annealing(self, epoch):\n        lr = self.eta_min + (self.init_lr - self.eta_min) * (1 + math.cos(math.pi * (epoch \/ self.T_max))) \/ 2\n        if(epoch == self.T_max):\n            self.cycle_cnt += 1\n            self.T_max = self.T_mult * self.T_max\n\n        if(self.restart_decay >0):\n            self.init_lr *= self.restart_decay\n            print('change init learning rate {}'.format(self.init_lr))\n\n    return lr\n  # calculate and set learning rate at the start of the epoch\n\n    def on_epoch_begin(self, epoch, logs = None):\n        lr = self.cosine_annealing(epoch)\n        print('\\nEpoch %05d: CosineAnnealingScheduler setting learng rate to %s.' % (epoch + 1, lr))\n        # set learning rate\n        backend.set_value(self.model.optimizer.lr, lr)\n        # log value\n        self.lrates.append(lr)","a84a3724":"from keras.models import load_model\n\nincepres_model = ['_incepres.hdf5', InceptionResNetV2]\nxception_model = ['_Xception.hdf5', Xception]\neff_model = ['_EFFnet.hdf5', EfficientNetB3]\n\nmodel_list = [xception_model, incepres_model, eff_model]\n# total predictions list\npreds_list = []\n\nlr = 1e-4\nTTA_STEPS = 10\n\nfor model_name, base_model in model_list:\n    print(model_name)\n    \n    # prediction each fold\n    predictions = []\n    if(model_name == '_EFFnet.hdf5'):\n        model_load_dir = MODEL_EFF_PATH\n    else:\n        model_load_dir = MODEL_PATH\n    for i in range(1, 6):\n        model = get_model(base_model, 299, False)\n        model.load_weights(os.path.join(model_load_dir, str(i)) + model_name)\n        # tta prediction list\n        tta_preds = []\n        for _ in range(TTA_STEPS):\n            test_generator_299.reset()\n            pred = model.predict_generator(\n            generator = test_generator_299, \n            steps = get_steps(nb_test_samples, batch_size),\n            verbose = 1\n            )\n            tta_preds.append(pred) # (5, 6150, 196)\n        tta_preds = np.mean(tta_preds, axis = 0) # (6150, 196)\n        \n        # for memory leaky\n        del model # \ubcc4 \ud6a8\uacfc \uc5c6\uc74c\n        for _ in range(10): # \ubcc4 \ud6a8\uacfc \uc5c6\uc74c\n            gc.collect()\n        K.clear_session() # \uc774 \uce5c\uad6c\uac00 \ub300\uc7a5\n        predictions.append(tta_preds) # (5, 6150, 196)\n    preds_list.append(np.mean(predictions, axis = 0))","3320b30a":"---------------------------------------------------------------------------------\n\ub2e4\uc74c\uc740 \uc81c\uac00 \uba87\uc2dc\uac04\uc744 \ub4e4\uc5ec \uc0ac\uc6a9\ud574\ubcf4\ub824\ub358 **Cosine Annealing**\uc785\ub2c8\ub2e4.\n\nCNN\uc5d0 \ud544\uc694\ud55c \uba87\uac00\uc9c0 \uae30\uc220\ub4e4\uc5d0 \uad00\ud574 \uc801\uc5b4\ub193\uc740 \ub17c\ubb38\uc744 \ubcf4\uace0 \uaf2d \uc0ac\uc6a9\ud574\ubcf4\uace0 \uc2f6\uc5c8\ub294\ub370, \ub9c8\uce68 \uc774 \ub300\ud68c\uac00 \uc5f4\ub824\uc11c \uc2dc\ub3c4\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\n\n\uacb0\uacfc\ubd80\ud130 \ub9d0\uc500\ub4dc\ub9ac\uc790\uba74 \uacb0\uad6d \uc88b\uc740 \uc131\ub2a5\uc744 \uc5bb\uc9c4 \ubabb\ud588\ub124\uc694..  \n\n\ud639\uc2dc \uc4f0\uc2e4\uc77c\uc774 \uc788\uc73c\uc2dc\ub2e4\uba74 \ucc38\uace0\ub9cc \ud574\uc8fc\uc2dc\uba74 \ub429\ub2c8\ub2e4. \uc644\uc804\ud55c \ucf54\ub4dc\ub77c\uace0\ub294 \ub9d0\uc500 \ubabb\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4.\n\nCosine Annealing\uc744 \uc0ac\uc6a9\ud558\uae30 \uc704\ud574 \uac80\uc0c9\ud574\ubcf4\uba74, \uba87\uba87 \ub300\ud45c\uc801\uc778 \ube14\ub85c\uadf8\uc640 github\uac00 \uc788\ub294\ub370,\n\n\uc804\ubd80 \ud45c\ud604\uc774 \ub2e4\ub985\ub2c8\ub2e4.\n\n\ub530\ub77c\uc11c \uc81c\uac00 \uc0ac\uc6a9\ud574\ubcf8 \uacb0\uacfc pytorch\uc758 \ucf54\ub4dc\ub97c \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4.","c5a88b90":"---------------\n**\ub354\ud558\uc5ec\uc11c, **\n+ \ucee4\ub110\uc740 gpu\ub97c \uc0ac\uc6a9\ud558\uc5ec \ucd5c\ub300 \uba87\uac1c\uae4c\uc9c0 running\uc2dc\ud0ac \uc218 \uc788\ub098\uc694??\n+ keras tuner https:\/\/github.com\/keras-team\/keras-tuner \ub97c \uc801\uc6a9\ud574\ubcf4\ub824\uace0 \ud588\uc2b5\ub2c8\ub2e4\ub9cc, OOM.....  \n+ \ucf00\ub77c\uc2a4\ub97c \uc774\uc6a9\ud558\ub824\uace0 \ud558\uc2dc\ub294 \ubd84\ub4e4\uc774\ub77c\uba74 \uaf2d \uc608\uc81c\ub97c \ud574\ubcf4\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4.\n(+ \ucf00\ub77c\uc2a4 \ucf54\ub9ac\uc544\uc758 \uc6b4\uc601\uc9c4\uc774\uc2e0 \uae40\ud0dc\uc601\ub2d8\uc758 \ube14\ub85c\uadf8\uc5d0\ub3c4 \uc798 \uc124\uba85\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4)","69b9f6b6":"**\uc131\ub2a5\ud5a5\uc0c1\uc5d0 \ub3c4\uc6c0\uc744 \uc900 \uac83\ub4e4**\n+ TTA\n+ EarlyStopping\uc744 \ud1b5\ud55c \ube60\ub978 \ud559\uc2b5\n+ SKfold(good)","d095a571":"\ucf54\ub4dc\ubcf4\ub2e4\ub294 \uc911\uac04\uc911\uac04 \uc81c\uac00 \uc801\uc740 \uae00\uc744 \uc77d\uc5b4\uc8fc\uc138\uc694.  \n\n5\ubd84\ub3c4 \uc548\uac78\ub9b4\uac83 \uac19\uc2b5\ub2c8\ub2e4. \n\n\ud5e4\ub9e8\ub2e4\ub294 \uac74 \uc88b\uc740 \uc758\ubbf8\uae34 \ud558\uc9c0\ub9cc \uc800\ubcf4\ub2e4 \ub354 \uc88b\uc740 \ud5e4\ub9f4\uc744 \uaf2d \ud558\uc2dc\uae38 \ubc14\ub77c\uba74\uc11c....\n\n\uac10\uc0ac\ud569\ub2c8\ub2e4.","9d3b8a9f":"---------\n**keras memory issue**\n\n\uc800\ub294 \ud604\uc7ac inceptionResnetV2 + Xception + Efficient\ub97c \uc11e\uc5b4\uc11c submission\uc744 \uc0dd\uc131\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n3 \ubaa8\ub378\uc744 \uc804\ubd80 \ubd80\ub974\uae30\uc5d4 RAM \ucd08\uacfc\ub85c \uc778\ud574 Kernel Died \ud604\uc0c1\uc774 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.\n\n\uc790\uc138\ud55c \uc774\uc720\ub294 \ubaa8\ub974\uc9c0\ub9cc \ucf00\ub77c\uc2a4 \ub0b4\ubd80\uc801\uc73c\ub85c model\uc774 \uac00\uc9c0\uace0 \uc788\ub294 RAM\uc744 \uc804\ubd80 \uc81c\uac70\ud558\uc9c0 \ubabb\ud558\ub294 \ubc84\uadf8\uac00 \uc788\ub2e4 \uce74\ub354\ub77c...?\n\n\uad6c\uae00 \uac80\uc0c9\uc744 \ud558\uba74 \uc5ec\ub7ec\uac00\uc9c0\uac00 \ub098\uc624\ub294\ub370\uc694.\n\n1. gc.collect()\n2. gc.collect \uc5ec\ub7ec\ubc88\n3. tensorflow\uc758 as_default_graph() \uc548\uc5d0\uc11c \ucc98\ub9ac\n4. K.clear_session()\n\n\uc704\uc758 \uac83\uc744 \uc804\ubd80 \ud558\ub098\uc529 \ub450\uace0 \uc0ac\uc6a9\ud574\ubcf8 \uacb0\uacfc. gc.collect()\ub294 \ubcc4 \ud6a8\uacfc\uac00 \uc5c6\uc5c8\uc2b5\ub2c8\ub2e4.\n\nkeras\ub97c \uc0ac\uc6a9\ud558\uc2dc\uace0 \uc5ec\ub7ec\ubc88 \ubaa8\ub378\uc744 load\ud560 \ud544\uc694\uac00 \uc788\uc73c\uc2dc\ub2e4\uba74 K.clear_session() \uc744 \uac15\ub825 \ucd94\ucc9c\ub4dc\ub9bd\ub2c8\ub2e4.\n(\uc2e4\uc81c\ub85c \uc800\uac19\uc740 \uacbd\uc6b0\uc5d4 RAM 7GB\uc774\uc0c1 \uc7a1\uc544\uba39\uc9c0 \uc54a\uac8c \ud574\uc8fc\uc5c8\uc2b5\ub2c8\ub2e4)","fe58ca5e":"**2. Train \uacfc\uc815**  \n\n\ud559\uc2b5\uacfc\uc815\uc740 \uad73\uc774 \uacfc\uc815\uc744 \uc124\uba85\ud558\uc9c0 \uc54a\uc544\ub3c4 \uc704\uc5d0 \ub9c1\ud06c\ud574\ub454 \uace0\uc218\ub2d8\ub4e4 \ucee4\ub110\uc744 \ubcf4\uc2dc\uba74 \ucda9\ubd84\ud560 \uac83 \uac19\uc2b5\ub2c8\ub2e4.  \n\n\uc0ac\uc2e4 \uc800\ub294 \uc81c \ud504\ub85c\uc81d\ud2b8\uc5d0\uc11c EalryStopping\uc744 \uc798 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc5b8\uc81c \uc5b4\ub514\uc11c \uc62c\ub77c\uac08\uc9c0 \ubaa8\ub974\uae30 \ub584\ubb38\uc774\ub77c\ub294 \uae30\ub300\uac10\uc5d0\uc694..  \n\n\uadf8\ub9ac\uace0 \ub098\uc11c 2\uc77c\uc815\ub3c4 \ub3cc\ub824\ub193\uc73c\uba74 \uc54c\uc544\uc11c \uc644\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.  \n\n\uadfc\ub370 \ucee4\ub110\uc744 \uc0ac\uc6a9\ud558\ub294 \uacbd\uc6b0\uc5d0\ub294 \uc774\uac8c \uc0c1\ub2f9\ud788 \uc911\uc694\ud558\ub2e4\ub294 \uac83\uc744 \ub290\uaf07\uc2b5\ub2c8\ub2e4.  \n\n\ud639\uc2dc\ub77c\ub3c4 \uc8fd\uc5b4\ubc84\ub9ac\uba74 \ub108\ubb34....\n\n\ucc98\uc74c\uc5d4 patience\ub3c4 10\uc73c\ub85c \uace0\uc815\uc2dc\ucf30\ub2e4\uac00 \ud655 \uc904\uc600\uc2b5\ub2c8\ub2e4. \ub3c4\uc800\ud788 \uac10\ub2f9\uc774 \uc548\ub418\ub294 \uac83\uc744 \uc9c1\uc811 \uccb4\ud5d8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(\ud55c\ubc88 \ud574\ubcf4\uc138\uc694!)\n\n\uc800\ub294 Dropout\ubcf4\ub2e4\ub294 BatchNormalization\uc744 \uc6b0\uc120\uc801\uc73c\ub85c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.  \n\n\ud558\uc9c0\ub9cc \ud574\ubcf8 \uacb0\uacfc BN\ubcf4\ub2e4\ub294 Dropout\uc758 \uacb0\uacfc\uac00 \ub354 \uc88b\uc558\uc2b5\ub2c8\ub2e4 :)\n\n+ SGD\ub97c \ud1b5\ud574 \uac70\uc758 12\uc2dc\uac04\uc744 \ud559\uc2b5\uc2dc\ucf1c\ubcf4\uc558\uc9c0\ub9cc local minima\uc5d0\uc11c \ube60\uc838\ub098\uc624\uc9c8 \ubabb\ud588\uc2b5\ub2c8\ub2e4.\n+ \uace0\uc218\ub2d8\ub4e4\uc740 \uc5b4\ub5bb\uac8c \uc774 optimizer\ub97c \uc0ac\uc6a9\ud558\uc2dc\ub294\uc9c0 \ub610 \ud55c\ubc88 \uac10\ud0c4!","c424d397":"**# \uacbd\ub85c\ub54c\ubb38\uc5d0 \ud5e4\ub9e4\ub2e4.**  \n\ucee4\ub110\uc744 \ucc98\uc74c \uc0dd\uc131\ud558\uace0 \ube44\uad50\uc801 \uc811\uadfc\uc131\uc774 \uc27d\uace0 \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294 jupyter notebook \ud615\ud0dc\uc758 \ucee4\ub110\uc744 \uc0ac\uc6a9\ud558\uac8c \ub429\ub2c8\ub2e4.  \n\uacbd\ub85c\ub294 \uc54c\uae30 \uc27d\uac8c \ud574\uc8fc\ub294 \ucf54\ub4dc\uac00 \ubcf4\uc600\uc2b5\ub2c8\ub2e4. # os.listdir(path)\n\n\uc774\ud6c4 mkdir\uc744 \uc774\uc6a9\ud558\uace0, \uc9c1\uc811 \uacbd\ub85c\uac00 \uc874\uc7ac\ud558\ub294 \uac83 \uae4c\uc9c0 \ud655\uc778. \ud559\uc2b5 \uc774\ud6c4\uc5d0 \ubaa8\ub378\uc744 \uc800\uc7a5\ud558\uace0  \n\ubd88\ub7ec\uc640\uc11c \uc0ac\uc6a9\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4. \n\n\uc5ec\uae30\uc11c \uc800\uc758 \uba4d\uccad\ud568\uc774 +1 \ub418\uc5b4 Commit\uc758 \uc874\uc7ac\ub97c \ubaa8\ub978\uccb4 \uc140\ub9cc \uc2e4\ud589\uc2dc\ud0a4\uae30 \uc2dc\uc791\ud569\ub2c8\ub2e4.  \n\ud559\uc2b5\uc744 \uc2dc\uc791\ud558\uace0 9\uc2dc\uac04\uc758 \uc5ec\uc815\uc774 \ub05d\ub0ac\ub124\uc694.  \n\ubd88\ub7ec\uc640\uc11c prediction\uc744 \ud574\uc57c\uaca0\ub294\ub370.... \uc5c6\uc2b5\ub2c8\ub2e4. \uc544\ubb34\ub9ac \ucc3e\uc544\ubd10\ub3c4 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n\uc774\ub807\uac8c \ud558\ub8e8 \ubc18\uc815\ub3c4\ub97c \ub0a0\ub838\ub124\uc694.   \n\n\ub2f5\ub2f5\ud568\uc5d0 \uae00\uc744 \uc62c\ub9ac\uace0 \uae40\uc7ac\uad00\ub2d8\uc758 \ub2f5\uae00\uc5d0 \ub290\ub08c\uc744 \ubc1b\uc544 \uc5b4\ub5bb\uac8c \uc0ac\uc6a9\ud558\ub294\uc9c0 \uc54c\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\nCommit\uc744 \ub204\ub974\uace0 \uc644\ub8cc\uac00 \ub418\uba74 open version\uc73c\ub85c \ub4e4\uc5b4\uac00\uc11c, output tab\uc5d0\uc11c \uc800\uc7a5\ub41c \ud30c\uc77c\uc744 \ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uc544 Dataset\uc73c\ub85c \ucd94\uac00\ud558\uba74 \ub429\ub2c8\ub2e4.  \n+ \ucee4\ub110\uc5d0 \ubcf4\uc2dc\uba74 Help -> Feedback\uc774 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc5d0 \uae00\uc744 \uc62c\ub9ac\uc2dc\uba74 \ube44\uad50\uc801 \ube60\ub974\uac8c Kaggle Team\uc5d0\uc11c \uc9c1\uc811 \ub2f5\ubcc0\ud574\uc90d\ub2c8\ub2e4.  \n+ \ub2e4\ub9cc, \uc601\uc5b4\ub85c \uc9c8\ubb38\uc744..\n+ \uc544\ubb34\ud2bc \uc800 \uac19\uc740 \ucd08\ubcf4\uc790\uc5d0\uac8c \uc911\uc694\ud55c \uac83\uc740 Commit\uc758 \uc874\uc7ac\uc720\ubb34(\uc800\ub9cc \uc774\ub7f0 \uc2e4\uc218\ub97c \ud588\uc744 \uc218\ub3c4 \uc788\uaca0\ub124\uc694 \u314e\u314e)\n+ Commit \ud6c4 Log\ub97c \uc720\uc2ec\ud788 \uad00\ucc30\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4. Traceback ~ \uc73c\ub85c \uc774\uc5b4\uc9c0\ub294 \uc5d0\ub7ec \uba54\uc2dc\uc9c0\uac00 \ub098\uc628\ub2e4\uba74-->\n+ output tab\uc774 \uc0dd\uae30\uc9c0 \uc54a\uc544 \uae30\ub2e4\ub824\ub3c4 \uacb0\uacfc\ub97c \uc5bb\uc73c\uc2e4 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.(\uadf8\ub098\ub9c8 \uc5d0\ub7ec\ub85c \uc8fd\uc73c\uba74 \ub2e4\ud589\uc785\ub2c8\ub2e4)\n+ \uc5d0\ub7ec\uac00 \uc5c6\ub2e4\uba74 \ud398\uc774\uc9c0\ub97c \ub044\uace0 \uba4d\ub54c\ub9ac\uc154\ub3c4 \ub429\ub2c8\ub2e4. \uc800\ub294 \uc8fc\ub85c \uba4d\ub54c\ub9ac\ub2e4\uac00 \ub2e4\ub978\uac78 \ud574\uc57c\uc9c0 \ud558\uace0 \uc815\uc2e0\uc744 \ucc28\ub9bd\ub2c8\ub2e4.\n+ \uce90\uae00 \ucee4\ub110\uc774 \uc88b\uc740 \uc810\uc740 loss\uc758 \ubcc0\ud654\ub97c \ubcf4\uc5ec\uc8fc\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0, \ub2e4\ub978\uac78 \ud560 \uc218 \uc788\uac8c \ud574\uc918\uc694","864c3b4f":"\uc548\ub155\ud558\uc138\uc694. \n\n\uba3c\uc800 \uc774 \uae00\uc740 \uc81c\uac00 \uc5ec\ud0dc\uae4c\uc9c0 \uacaa\uc5c8\ub358 \uac83\uc744 \ucd1d \uc815\ub9ac\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.  \n\n\ud5a5\ud6c4, \ub300\ud68c\uac00 \ub05d\ub098\uace0 tutorial\ub85c \uc811\uadfc\ud558\ub824\ub294 \ubd84\ub4e4\uc5d0\uac8c \ub3c4\uc6c0\uc774 \ub418\uc5c8\uc73c\uba74 \ud569\ub2c8\ub2e4.\n\nlibrary\ub294 Keras\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\uace0\uc218\ub2d8\ub4e4\uc758 \ucee4\ub110\uc740 \uc544\ub798\uc5d0 \uc788\uc73c\ub2c8 \uaf2d \uc77d\uc5b4\ubcf4\uc2dc\uace0 \uc5c5\ubcf4\ud2b8 \ub204\ub974\uc2dc\uae38 \ubc14\ub78d\ub2c8\ub2e4!(\uc800\ub3c4 \ub20c\ub800\uc2b5\ub2c8\ub2e4)\n\n\ub610\ud55c, \ucee4\ub110\uc744 \ud1b5\ud574 \uc815\ubcf4\ub97c \uacf5\uc720\ud574\uc8fc\uc2e0 \ubaa8\ub4e0 \ucee4\ub110 author\ub2d8\ub4e4\uaed8 \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4.\n\nhttps:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline\nbaseline kernel\uc744 \uc77d\ub294 \uac83\uc740 \ub2f9\uc5f0!\n\n\uc800\ub294 \uc8fc\ub85c Local server\uc640 colab\uc744 \uc8fc\ub85c \uc0ac\uc6a9\ud558\uc600\uace0, \uc774 \ub300\ud68c\ub97c \ud1b5\ud574 \uce90\uae00 \ucee4\ub110\uc744 \ucc98\uc74c \uc811\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.  \n\ub530\ub77c\uc11c, Commit\uc774 \ubb34\uc5c7\uc778\uc9c0 Dataset\uc744 \uc5b4\ub5bb\uac8c \ucd94\uac00\ud558\ub294\uc9c0\uc5d0 \ub300\ud574 \uc804\ud600 \ubaa8\ub974\uace0 \uc2dc\uc791\ud558\uc5ec \uc2dc\uac04\uc744 \ub9ce\uc774 \uc18c\ube44\ud558\uc600\uc2b5\ub2c8\ub2e4 \u3160\u3160\n\n**## Add Crop Dataset  \n**\n\n\ucc98\uc74c\uc5d0 \ub370\uc774\ud130\uc14b \ucd94\uac00\ub97c \ud560\uc904 \ubab0\ub790\ub358\uc9c0\ub77c \uc9c1\uc811 \uc544\ub798 \ucee4\ub110\uc758 \ucf54\ub4dc\ub97c \ub2e4\uc6b4\ubc1b\uc544 local\uc5d0\uc11c \ub9cc\ub4e0 \ub4a4, dataset\uc744 \ucd94\uac00\ud558\uc5ec \uc0ac\uc6a9\ud558\ub824\uace0 \ud588\uc2b5\ub2c8\ub2e4.  \n\ud558\uc9c0\ub9cc, \ubb34\uc2a8 \ubb38\uc81c \ub54c\ubb38\uc778\uc9c0 \uc790\uafb8 \ub2e4\uc6b4\ub418\uc5b4 \uc5c5\ub85c\ub4dc\uac00 \ub418\uc9c0 \uc54a\uc544\uc11c \ub2e4\uc2dc colab\uc73c\ub85c \ub3cc\uc544\uac14\uc2b5\ub2c8\ub2e4...  \n\nhttps:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping-updated-7-10 # \ud5c8\ud0dc\uba85\ub2d8\nhttps:\/\/www.kaggle.com\/seriousran\/xception-model-in-keras-for-car-classification # \uae40\ucc2c\ub780\ub2d8\nhttps:\/\/www.kaggle.com\/yangsaewon\/pytorch-baseline-updated-7-10 # \uc591\uc0c8\uc6d0\ub2d8  \nhttps:\/\/www.kaggle.com\/janged\/3rd-ml-month-car-model-classification-xception # Jang \ub2d8\n\n\ub610 \ubbf8\uc219\ud55c \uc9c8\ubb38\uc5d0 \ube60\ub978 \ub2f5\ubcc0\uc744 \uac00\uc838\ub2e4 \uc8fc\uc168\ub358 \uae40\uc7ac\uad00\ub2d8\ub3c4 \uac10\uc0ac\ub4dc\ub9bd\ub2c8\ub2e4.","ac347b44":"\uc77c\uc815 \uc218\uc900\uc758 epoch\uae4c\uc9c0 Cossine Annealing\uc744 \ud55c \ud6c4, fine tuning(\uace0\uc815 lr)\uc744 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\ub2e4\uace0 \uc0dd\uac01\ub429\ub2c8\ub2e4.\n\n\uc81c \ub098\ub984 \ucc3e\uc544\ubcf8 \uacb0\uacfc optimizer\uc744 \ubcc0\uacbd\ud558\uae30 \uc704\ud574\uc120 model.compile\uc744 \ub2e4\uc2dc\ud574\uc57c\ud55c\ub2e4\uace0 \ud558\ub294\ub370\uc694...\n\n\uc774\ub7f4 \ub550, tensorflow low api\uac00 \uc5ed\uc2dc.... \ud639\uc2dc \ub2e4\ub978 \ubc29\ubc95\uc774 \uc788\uc744\uae4c\uc694??\n\n\uc544\ub798 \ucf54\ub4dc\ub294 inference\uc2dc \uc0ac\uc6a9\ud55c \ucf54\ub4dc\uc785\ub2c8\ub2e4. \ud5f7\uac08\ub9b4\ub54c\ub9cc \ucc38\uace0\ud574\uc8fc\uc138\uc694"}}