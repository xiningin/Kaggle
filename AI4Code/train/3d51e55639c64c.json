{"cell_type":{"7c4e9316":"code","39c1c964":"code","6bbaaf97":"code","7dc8a2fa":"code","9b4a8499":"code","658f59a3":"code","84e947ee":"code","e3003d02":"code","6a1afb21":"code","ef400a1c":"code","7e2a2beb":"code","c9839c0a":"code","806473c5":"code","3182357e":"code","44624628":"code","b5590f27":"code","bce2aa82":"code","81f62c9a":"code","0864eef3":"code","7db565d4":"code","d205e01e":"code","d9f729ca":"code","acf6e0b0":"code","374981d7":"code","692e9f85":"code","091b295f":"code","90024012":"code","7d5cc0f3":"code","bfbe3f24":"code","c002dfd3":"code","30832e56":"code","0b139f1e":"code","b89eb94b":"code","78e20fa0":"code","9247309b":"code","5931d486":"code","fc7613d0":"markdown","0e7dca3d":"markdown","f283b218":"markdown","20b6557b":"markdown","cb9f9f79":"markdown","0e1b2f28":"markdown","dd6f9fa2":"markdown","55f94433":"markdown","cb98a6c8":"markdown","382464ac":"markdown","85f3e0c9":"markdown","29fdee7b":"markdown","87dab0fe":"markdown","c1b0bf12":"markdown","2c6e080c":"markdown","e9dea27c":"markdown","5ece26d2":"markdown","e20281c3":"markdown","5b575f48":"markdown","54ebc07a":"markdown","b9291f90":"markdown","d9eaff22":"markdown","29c8ea04":"markdown","9148c604":"markdown","be1fd24a":"markdown","9c782346":"markdown","8b0ad137":"markdown"},"source":{"7c4e9316":"# data exploration \/ preprocessing\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import (StandardScaler, RobustScaler,\n                                   FunctionTransformer, QuantileTransformer)\n\n# model fitting \/ evaluation \/ export\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import (RandomForestClassifier, RandomForestRegressor, \n                              VotingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.svm import SVC, SVR, LinearSVC, LinearSVR\nfrom lightgbm import LGBMClassifier, LGBMRegressor, plot_metric, plot_importance\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import (train_test_split, StratifiedKFold, KFold,\n                                     cross_validate,\n                                     RandomizedSearchCV, GridSearchCV)\nfrom sklearn.metrics import (plot_confusion_matrix, classification_report,\n                             plot_roc_curve)\nimport pickle\n\n# misc\nfrom IPython.display import display_html","39c1c964":"smart_grid_orig = pd.read_csv(\"..\/input\/ucis-electrical-grid-stability-simulated-data\/Data_for_UCI_named.csv\")\n\nsmart_grid_orig.head(3)","6bbaaf97":"assert smart_grid_orig.isna().sum().sum() == 0, \"some data missing\"\n\nstab_fine = max(smart_grid_orig.query(\"stabf == 'stable'\")[\"stab\"]) < 0\nstabf_fine = min(smart_grid_orig.query(\"stabf == 'unstable'\")[\"stab\"]) > 0\nassert (stab_fine & stabf_fine), \"unexpected stab\/stabf relationship\"\n\nprint(\"As expected, no missing data and `stab` values of less than 0 are labelled 'stable' in `stabf`.\")","7dc8a2fa":"fig, axs = plt.subplots(1, 4, figsize=(15, 3))\n\nfor axs_ind, feature_group in enumerate([\"tau\", \"p\", \"g\"]):\n    smart_grid_orig.boxplot(\n        column=[feature_group + str(i + 1) for i in range(4)], \n        ax= axs[axs_ind]\n    )\nsmart_grid_orig.boxplot(column=\"stab\", ax= axs[3])\n\nfor axs_ind, title in enumerate([\"reaction time\", \"power production\/consumption\",\n                                 \"willingness to adapt\", \"grid stability\"]):\n    axs[axs_ind].set(title=title);","9b4a8499":"print(smart_grid_orig[\"stabf\"].value_counts(normalize=True))  # pretty balanced","658f59a3":"smart_grid = smart_grid_orig.assign(stabf = lambda x: x.stabf.replace({\"unstable\": 0, \"stable\": 1}))\n\nsmart_grid.columns = (smart_grid.columns\n                      .str.replace(\"tau\", \"delay\")\n                      .str.replace(\"p\", \"power\")\n                      .str.replace(\"g\", \"adapt\"))","84e947ee":"g = sns.PairGrid(smart_grid, diag_sharey=False,\n                 corner=True, height=0.6, aspect=1)\ng.map_lower(sns.scatterplot, s=1)\ng.map_diag(sns.histplot);","e3003d02":"plt.figure(figsize = (10, 5))\nsns.heatmap(smart_grid.corr(), fmt=\".2f\", annot=True);  # default pearson","6a1afb21":"# https:\/\/www.researchgate.net\/post\/Multicollinearity_issues_is_a_value_less_than_10_acceptable_for_VIF\nfeat_cols = smart_grid.drop([\"stab\", \"stabf\"], axis=1).columns\n\n(pd.DataFrame({\n    \"variables\": smart_grid[feat_cols].columns,\n    \"VIF\": [variance_inflation_factor(smart_grid[feat_cols].values, ind)\n            for ind in range(len(feat_cols))]\n})\n .rename({\"variables\": \"\"}, axis=1)\n .set_index(\"\")\n .transpose()\n)","ef400a1c":"fig, axs = plt.subplots(3, 4, figsize=(10, 5))\nplt.subplots_adjust(wspace=0.3, hspace=0.5)\n\nfor row_ind, feat_type in enumerate([\"delay\", \"power\", \"adapt\"]):\n    for col_ind in range(4):\n        show_legend = True if (row_ind == 0) & (col_ind == 3) else False\n        sns.histplot(\n            smart_grid, x=feat_type + str(col_ind + 1), hue=\"stabf\",\n            multiple=\"fill\", legend=show_legend,\n            ax=axs[row_ind, col_ind]\n        )\n        \n        if col_ind > 0:\n            axs[row_ind, col_ind].set_ylabel(\"\")","7e2a2beb":"(smart_grid\n .assign(\n     sum_delay = lambda x: x[\"delay1\"] + x[\"delay2\"] + x[\"delay3\"] + x[\"delay4\"],\n     sum_adapt = lambda x: x[\"adapt1\"] + x[\"adapt2\"] + x[\"adapt3\"] + x[\"adapt4\"]\n )\n .pipe((sns.scatterplot, \"data\"), \n       x=\"sum_delay\", y=\"sum_adapt\", hue=\"stabf\", alpha=0.2)\n);","c9839c0a":"# following conventions, X contains the features and y contains the labels\nX = smart_grid.drop([\"stab\", \"stabf\"], axis=1)\ny = smart_grid[[\"stab\", \"stabf\"]]\n\nX_train_val_, X_test_, y_train_val, y_test = \\\n    train_test_split(X, y, test_size=0.2, stratify=y[\"stabf\"], random_state=0)\n\n# unsure about how X should be processed at the moment\nX_train_val_w_pwr = X_train_val_.drop([\"power1\"], axis=1)\nX_train_val_no_pwr = (X_train_val_\n                      .drop([\"power1\", \"power2\", \"power3\", \"power4\"], axis=1))\n# labels for both tasks\nclf_y_train_val, clf_y_test = y_train_val[\"stabf\"], y_test[\"stabf\"]\nreg_y_train_val, reg_y_test = y_train_val[\"stab\"], y_test[\"stab\"]\n\nassert all(clf_y_train_val.value_counts(normalize=True) == \\\n    clf_y_test.value_counts(normalize=True)), \\\n    \"inconsistent class share afer split\"","806473c5":"# classifiers\nlog_reg = LogisticRegression(random_state=1)\nrfc = RandomForestClassifier(random_state=1)\nknc = KNeighborsClassifier()\nlgbc = LGBMClassifier(random_state=1)\nsvc = SVC()\n\n# regressors\nlin_reg = LinearRegression()\nrfr = RandomForestRegressor(random_state=1)\nknr = KNeighborsRegressor()\nlgbr = LGBMRegressor(random_state=1)\nsvr = SVR()\n\n# cross-validation splitters\nskf = StratifiedKFold(random_state=1, shuffle=True)\nkf = KFold(random_state=1, shuffle=True)\nprint(\"For classifiers:\", skf)\nprint(\"For regressors:\", kf)","3182357e":"def get_baseline_scores(est_names, X_train, y_train_val, cv):\n    baseline_scores_list = []\n    for est_name in est_names:\n        pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n                               (\"estimator\", globals()[est_name])])\n        \n        est_res = cross_validate(pipe,\n                                 # https:\/\/github.com\/dmlc\/xgboost\/issues\/6908,\n                                 np.ascontiguousarray(X_train),\n                                 y_train_val,\n                                 cv=cv, return_train_score=True)\n        baseline_scores_list.append(\n            pd.DataFrame({\"estimator\": est_name, \n                          \"train_score\": est_res.get(\"train_score\"),\n                          \"val_score\": est_res.get(\"test_score\"), \n                          \"fit_time_s\": est_res.get(\"fit_time\")\n                         }))\n    baseline_scores_df = (pd.concat(baseline_scores_list)\n                          .sort_values(\"val_score\", ascending=False))\n    return baseline_scores_df\n\n\ndef fmt_bl_scores(scores_df, caption):\n    metric_cols = [\"train_score\", \"val_score\", \"fit_time_s\"]\n    final_df = (scores_df\n                .pipe(pd.pivot_table, \n                      values=metric_cols, \n                      index=\"estimator\", \n                      aggfunc={columns: np.mean for columns in metric_cols})\n                .sort_values(\"val_score\", ascending=False)\n                \n                # display options\n                .style.format(\"{:.1%}\", subset=[\"train_score\", \"val_score\"])\n                .set_table_attributes(\"style='display:inline'\")\n                .set_caption(caption)._repr_html_())\n    return final_df\n\nblc_w_pwr = get_baseline_scores([\"log_reg\", \"rfc\", \"knc\", \"lgbc\", \"svc\"],\n                                X_train_val_w_pwr, clf_y_train_val, skf)\nblc_no_pwr = get_baseline_scores([\"log_reg\", \"rfc\", \"knc\", \"lgbc\", \"svc\"],\n                                 X_train_val_no_pwr, clf_y_train_val, skf)\n\nblr_w_pwr = get_baseline_scores([\"lin_reg\", \"rfr\", \"knr\", \"lgbr\", \"svr\"],\n                                X_train_val_w_pwr, reg_y_train_val, kf)\nblr_no_pwr = get_baseline_scores([\"lin_reg\", \"rfr\", \"knr\", \"lgbr\", \"svr\"],\n                                 X_train_val_no_pwr, reg_y_train_val, kf)\n        \ndisplay_html(\n    fmt_bl_scores(blc_w_pwr, \n                  \"Baseline classifier accuracy (with power columns)\") +\n    \"\\xa0\" * 2 +  # so both still fit in a line on small screens\n    fmt_bl_scores(blc_no_pwr, \n                  \"Baseline classifier accuracy (no power columns)\") +\n    \"<br\/><br\/>\" +\n    fmt_bl_scores(blr_w_pwr,\n                  \"Baseline regressor R2 (with power columns)\") +\n    \"\\xa0\" * 2 +\n    fmt_bl_scores(blr_no_pwr, \n                  \"Baseline regressor R2 (no power columns)\"),\n    raw=True\n)","44624628":"X_train_wp, X_val_wp, clf_y_train_wp, clf_y_val_wp = \\\n    train_test_split(X_train_val_w_pwr, clf_y_train_val, \n                     test_size = 0.3, random_state=0)\nX_train_wp, X_val_wp, reg_y_train_wp, reg_y_val_wp = \\\n    train_test_split(X_train_val_w_pwr, reg_y_train_val, \n                     test_size = 0.3, random_state=0)\n\n## LightGBM feature importances\nlgbc.fit(X_train_wp, clf_y_train_wp)\n\n## RF permutation importances (model agnostic method)\nrfr.fit(X_train_wp, reg_y_train_wp)\nrfr_pi_res = permutation_importance(rfr, X_train_wp, reg_y_train_wp,\n                                   n_repeats=10, random_state=0)\nrfr_pi_df = (pd.DataFrame(rfr_pi_res.get(\"importances\"),\n                         index=X_train_wp.columns,\n                         columns=[i + 1 for i in range(10)])\n            .reset_index()\n            .pipe(pd.melt,\n                  id_vars=[\"index\"], value_vars=[i + 1 for i in range(10)],\n                  var_name=\"rep\", value_name=\"permutation_importance\")\n            .rename(columns={\"index\": \"features\"}))\nrfr_pi_order = (rfr_pi_df.groupby(\"features\")\n               .mean(\"permutation_importance\")\n               .sort_values(\"permutation_importance\", ascending=False).index)\n\n## Logistic Regression\/Linear SVC coefficients (L1-based)\nss = StandardScaler()\ndef plot_abs_coefs(model_inst, model_name, y_train, ax):\n    coefs_df = pd.DataFrame(\n        {\"features\": X_train_wp.columns,\n         \"coefficients\": (model_inst\n                          .fit(ss.fit_transform(X_train_wp), clf_y_train_wp)\n                          .coef_.flatten())}\n    ).assign(abs_coef = lambda x: x[\"coefficients\"].map(abs))\n    feat_order = (coefs_df\n                  .groupby(\"features\").mean(\"abs_coef\")\n                  .sort_values(\"abs_coef\", ascending=False).index)\n    (sns.barplot(data=coefs_df, x=\"abs_coef\", y=\"features\",\n                color=\"lightgrey\", order=feat_order, ax=ax)\n     .set(title=f\"Absolute values of {model_name} coefficients\"))\n    \n## Plots\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\nplt.subplots_adjust(wspace=0.3, hspace=0.4)\n\nplot_importance(lgbc, xlabel=\"Total gains of splits which use the feature\", \n                title=\"LightGBM Classifier feature importance\",\n                importance_type=\"gain\", precision=0, grid=False, ax=axs[0, 0])\nplot_abs_coefs(model_inst=LogisticRegression(solver=\"liblinear\", penalty=\"l1\"),\n               model_name=\"logistic regression\", \n               y_train=clf_y_train_wp, ax=axs[0, 1])\n\nsns.barplot(data=rfr_pi_df, x=\"permutation_importance\", y=\"features\",\n            ci=\"sd\", order=rfr_pi_order, color=\"lightgrey\",\n            ax=axs[1, 0]).set(title=\"RF Regressor permutation importance\")\nplot_abs_coefs(model_inst=LinearSVR(random_state=1, max_iter=2000),\n               model_name=\"linear SVR\", y_train=reg_y_train_wp, ax=axs[1, 1]);","b5590f27":"def process_X(X_df):\n    final_df = X_df.drop([\"power1\", \"power2\", \"power3\", \"power4\"], axis=1)\n    return final_df\n\nX_train_val = process_X(X_train_val_)\nX_test = process_X(X_test_)","bce2aa82":"fig, axs = plt.subplots(2, 1, figsize=(10, 8))\nplt.subplots_adjust(hspace=0.3)\nfor metric_df, ax_pos, estimator, metric in \\\n    zip([blc_no_pwr, blr_no_pwr], [0, 1],\n        [\"classifier\", \"regressor\"], [\"accuracy\", \"R-squared\"]):\n    (metric_df\n     .pipe(pd.melt, \n           id_vars=\"estimator\", \n           value_vars=[\"train_score\", \"val_score\"], \n           var_name=\"metric_name\", value_name=\"metric_value\")\n     .pipe((sns.boxplot, \"data\"), x=\"metric_value\", y=\"estimator\", \n           hue=\"metric_name\", ax=axs[ax_pos])\n     .set(title=f\"Baseline {estimator} performance\",\n         xlim=(-0.05, None), xlabel=metric, ylabel=estimator))","81f62c9a":"X_train, X_val, clf_y_train, clf_y_val = \\\n    train_test_split(X_train_val, clf_y_train_val, \n                     test_size = 0.3, random_state=0)\nX_train, X_val, reg_y_train, reg_y_val = \\\n    train_test_split(X_train_val, reg_y_train_val, \n                     test_size = 0.3, random_state=0)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nplt.subplots_adjust(wspace=0.4)\nlgbc.fit(X_train, clf_y_train, \n        eval_set=[(X_train, clf_y_train), (X_val, clf_y_val)], \n        eval_names=[\"train set\", \"validation set\"], verbose=0)\nlgbr.fit(X_train, reg_y_train, \n        eval_set=[(X_train, reg_y_train), (X_val, reg_y_val)], \n        eval_names=[\"train set\", \"validation set\"], verbose=0)\nplot_metric(lgbc, title=\"Baseline LightGBM Classifier\",\n            xlabel=\"n_estimators\", grid=False, ax=axs[0])\nplot_metric(lgbr, title=\"Baseline LightGBM Regression\",\n            xlabel=\"n_estimators\", ylabel=\"MSE\", grid=False, ax=axs[1]);","0864eef3":"rfc.fit(X_train, clf_y_train)\nrfc_oob = RandomForestClassifier(oob_score=True, random_state=0)\nrfc_oob.fit(X_train, clf_y_train)\n\nrfr.fit(X_train, reg_y_train)\nrfr_oob = RandomForestRegressor(oob_score=True, random_state=0)\nrfr_oob.fit(X_train, reg_y_train)\n\npd.DataFrame({\n    \"classifier\": [rfc.score(X_train, clf_y_train),\n                   rfc_oob.oob_score_,\n                   rfc.score(X_val, clf_y_val)],\n    \"regressor\": [rfr.score(X_train, reg_y_train),\n                  rfr_oob.oob_score_,\n                  rfr.score(X_val, reg_y_val)],\n}, index=[\"training set score\", \"training set OOB score\",\n          \"validation set score\"]\n).style.format(\"{:.3f}\").set_caption(\"Baseline RF performance\")","7db565d4":"def gen_search_eval(estimator_name: str, search_params,\n                    pred_task:str, search_mode: str):\n\n    cv = skf if pred_task == \"clf\" else kf\n    y_train_val = clf_y_train_val if pred_task == \"clf\" else reg_y_train_val\n    \n    pipeline_steps = Pipeline(\n        steps=[(\"transformer\", StandardScaler()),\n                (\"estimator\", globals()[estimator_name])]\n    )\n    \n    common_args = dict(estimator=pipeline_steps, n_jobs=-1, \n                       cv=cv, verbose=1,\n                       return_train_score=True,  # to check for overfitting\n                       error_score=\"raise\")  # if error during fitting\n    random_cv = RandomizedSearchCV(param_distributions=search_params, \n                                   n_iter=60, random_state=1, **common_args)\n    grid_cv = GridSearchCV(param_grid=search_params, **common_args)\n    search_cv = grid_cv if search_mode == \"grid\" else random_cv\n\n    search_cv.fit(X_train_val, y_train_val)\n    return search_cv\n\n\ndef fmt_search_res(search_res):\n    summary_df = pd.DataFrame(\n        {key: search_res.cv_results_.get(key) \n         for key in [\"params\", \"rank_test_score\", \n                     \"mean_fit_time\", \"mean_score_time\",\n                     \"mean_train_score\", \"mean_test_score\",\n                     \"std_train_score\", \"std_test_score\"]}\n    )\n    \n    final_df = (pd.DataFrame(summary_df[\"params\"].tolist())  # expand params\n                .reset_index()\n                .merge(summary_df.reset_index(drop=True).reset_index())\n                .drop([\"index\", \"params\"], axis=1)\n                .sort_values([\"mean_test_score\", \"mean_fit_time\"], \n                             ascending=[False, True]))\n    return final_df\n\n\n# feature transformations important for distance-based algo eg. SVM, KNN\ntransformers = [StandardScaler(),  # very sensitive to outliers\n                RobustScaler(),  # not affected by a few extreme outliers\n                FunctionTransformer(lambda x: x),  # do nothing\n                QuantileTransformer()]  # collapses outliers","d205e01e":"svm_params = [{\"estimator__kernel\": [\"rbf\", \"sigmoid\"],  # default rbf\n               # gamma reshapes decision boundary (high overfits)\n               \"estimator__gamma\": [\"auto\", \"scale\"]},  # default scale\n              {\"estimator__kernel\": [\"linear\"]}]  # gamma not affect linear\n\nfor param_dict in [0, 1]:\n    svm_params[param_dict].update({\"transformer\": transformers,\n                                   \"estimator__C\": np.logspace(-2, 2, 5)})\n\nsvc_raw_clf_res = gen_search_eval(\"svc\", svm_params, \"clf\", \"random\")\nprint(f\"\\nBest classifier pipeline: {svc_raw_clf_res.best_estimator_}\")\n\nsvc_clf_res = fmt_search_res(svc_raw_clf_res)\nsvc_clf_res.head(3)","d9f729ca":"svr_raw_reg_res = gen_search_eval(\"svr\", svm_params, \"reg\", \"random\")\nprint(f\"\\nBest regressor pipeline: {svr_raw_reg_res.best_estimator_}\")\n\nsvr_reg_res = fmt_search_res(svr_raw_reg_res)\nsvr_reg_res.head(3)","acf6e0b0":"lgb_params = {\n    \"transformer\": transformers,\n    # generally, slower learning rates need more trees\n    \"estimator__n_estimators\": [50, 100, 250, 500],  # default 100\n    \"estimator__learning_rate\": [0.001, 0.01, 0.1, 0.2, 0.3],  # default 0.1\n    \"estimator__boosting_type\": [\"gbdt\", \"dart\"],  # default gbdt\n    # restrict tree growth and adjust regularisation to prevent overfitting\n    \"estimator__max_depth\": [25, 50, -1],  # default -1 (no limit)\n    \"estimator__min_child_samples\": [20, 30, 40],  # default 20\n    \"estimator__reg_alpha\": np.logspace(-2, 2, 5),  # lambda_l1; default 0\n    \"estimator__reg_lambda\": np.logspace(-2, 2, 5)  # lambda_l2; default 0\n}\n\nlgbc_raw_clf_res = gen_search_eval(\"lgbc\", lgb_params, \"clf\", \"random\")\nprint(f\"\\nBest classifier pipeline: {lgbc_raw_clf_res.best_estimator_}\")\n\nlgbc_clf_res = fmt_search_res(lgbc_raw_clf_res)\nlgbc_clf_res.head(3)","374981d7":"lgbr_raw_reg_res = gen_search_eval(\"lgbr\", lgb_params, \"reg\", \"random\")\nprint(f\"\\nBest regressor pipeline: {lgbr_raw_reg_res.best_estimator_}\")\n\nlgbr_reg_res = fmt_search_res(lgbr_raw_reg_res)\nlgbr_reg_res.head(3)","692e9f85":"# take just estimator parts since plot_metric doesn't work with pipelines\ntuned_lgbc = lgbc_raw_clf_res.best_estimator_[1]\ntuned_lgbr = lgbr_raw_reg_res.best_estimator_[1]\nqt = QuantileTransformer()\n\ntuned_lgbc.fit(X_train, clf_y_train, \n           eval_set=[(X_train, clf_y_train), \n                     (X_val, clf_y_val)], \n           eval_names=[\"train set\", \"validation set\"],\n           verbose=0)\ntuned_lgbr.fit(qt.fit_transform(X_train), reg_y_train, \n           eval_set=[(qt.fit_transform(X_train), reg_y_train), \n                     (qt.transform(X_val), reg_y_val)], \n           eval_names=[\"train set\", \"validation set\"],\n           verbose=0)\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nplt.subplots_adjust(wspace=0.4)\nplot_metric(tuned_lgbc, title=\"Tuned LightGBM Classifier\",\n            xlabel=\"n_estimators\", grid=False, ax=axs[0])\nplot_metric(tuned_lgbr, title=\"Tuned LightGBM Regressor\",\n            xlabel=\"n_estimators\", ylabel=\"MSE\", grid=False, ax=axs[1]);","091b295f":"rf_params = {\n    \"transformer\": transformers,\n    \"estimator__n_estimators\": [50, 100, 250, 500],  # default 100\n    \"estimator__max_depth\": [15, 50, None],\n    \"estimator__min_samples_split\": [2, 5, 10],  # default 2\n    \"estimator__min_samples_leaf\": [1, 2, 5]  # default 1\n}\n\nrfc_raw_clf_res = gen_search_eval(\"rfc\", rf_params, \"clf\", \"random\")\nprint(f\"\\nBest classifier pipeline: {rfc_raw_clf_res.best_estimator_}\")\n\nrfc_clf_res = fmt_search_res(rfc_raw_clf_res)\nrfc_clf_res.head(3)","90024012":"rfr_raw_reg_res = gen_search_eval(\"rfr\", rf_params, \"reg\", \"random\")\nprint(f\"\\nBest regressor pipeline: {rfr_raw_reg_res.best_estimator_}\")\n\nrfr_reg_res = fmt_search_res(rfr_raw_reg_res)\nrfr_reg_res.head(3)","7d5cc0f3":"tuned_rfc = rfc_raw_clf_res.best_estimator_[1]\ntuned_rfc_oob = rfc_raw_clf_res.best_estimator_[1]\ntuned_rfc_oob.set_params(oob_score=True)\ntuned_rfr = rfr_raw_reg_res.best_estimator_[1]\ntuned_rfr_oob = rfr_raw_reg_res.best_estimator_[1]\ntuned_rfr_oob.set_params(oob_score=True)\n\nrs = RobustScaler()\ntuned_rfc.fit(rs.fit_transform(X_train), clf_y_train)\ntuned_rfc_oob.fit(rs.fit_transform(X_train), clf_y_train)\nqt = QuantileTransformer()\ntuned_rfr.fit(qt.fit_transform(X_train), reg_y_train)\ntuned_rfr_oob.fit(qt.fit_transform(X_train), reg_y_train)\n\npd.DataFrame({\n    \"classifier\": [tuned_rfc.score(rs.transform(X_train), clf_y_train),\n                   tuned_rfc_oob.oob_score_,\n                   tuned_rfc.score(rs.transform(X_val), clf_y_val)],\n    \"regressor\": [tuned_rfr.score(qt.transform(X_train), reg_y_train),\n                  tuned_rfr_oob.oob_score_,\n                  tuned_rfr.score(qt.transform(X_val), reg_y_val)],\n}, index=[\"training set score\", \"training set OOB score\",\n          \"validation set score\"]\n).style.format(\"{:.3f}\").set_caption(\"Tuned RF performance\")","bfbe3f24":"# typical k to start with (eg. https:\/\/stackoverflow.com\/a\/11569262):\nprint(\"Typical `k` to start with (square root of number of samples): \",\n      np.sqrt(X_train_val.shape[0]))\n\nknn_params = {\n    \"transformer\": transformers,\n    \"estimator__n_neighbors\": [5, 15, 45, 90],  # default 5\n    # distance function, default minkowski\n    \"estimator__metric\": [\"minkowski\", \"euclidean\", \"manhattan\"],\n    \"estimator__weights\": [\"uniform\", \"distance\"],  # default uniform\n    \"estimator__algorithm\" : [\"auto\", \"ball_tree\", \"kd_tree\"]  # default auto\n}\n\nknc_raw_clf_res = gen_search_eval(\"knc\", knn_params, \"clf\", \"random\")\nprint(f\"\\nBest classifier pipeline: {knc_raw_clf_res.best_estimator_}\")\n\nknc_clf_res = fmt_search_res(knc_raw_clf_res)\nknc_clf_res.head(3)","c002dfd3":"knr_raw_reg_res = gen_search_eval(\"knr\", knn_params, \"reg\", \"random\")\nprint(f\"\\nBest regressor pipeline: {knr_raw_reg_res.best_estimator_}\")\n\nknr_reg_res = fmt_search_res(knr_raw_reg_res)\nknr_reg_res.head(3)","30832e56":"log_reg_params = [\n    # compatible with both l1 (lasso) and l2 (ridge)\n    {\"estimator__solver\": [\"saga\", \"liblinear\"],\n     \"estimator__penalty\": [\"l1\", \"l2\"]},\n    {\"estimator__solver\": [\"lbfgs\", \"newton-cg\", \"sag\"],  # default lbfgs\n     \"estimator__penalty\": [\"l2\"]}]\n    \nfor param_dict in [0, 1]:\n    log_reg_params[param_dict].update({\"transformer\": transformers,\n                                       # inverse of lambda, default 1\n                                      \"estimator__C\": np.logspace(-2, 2, 5)})\n\nlog_reg_raw_clf_res = gen_search_eval(\"log_reg\", log_reg_params, \"clf\", \"grid\")\nprint(f\"\\nBest performing pipeline: {log_reg_raw_clf_res.best_estimator_}\")\n\nlog_reg_clf_res = fmt_search_res(log_reg_raw_clf_res)\nlog_reg_clf_res.head(3)","0b139f1e":"lin_reg_params = [\n    {\"estimator\": [LinearRegression()]},\n    {\"estimator\": [Lasso(random_state=1)],\n     \"estimator__alpha\": np.logspace(-2, 2, 5)},\n    {\"estimator\": [Ridge(random_state=1)],\n     \"estimator__alpha\": np.logspace(-2, 2, 5),\n     \"estimator__solver\": [\"auto\", \"svd\", \"sparse_cg\", \"lsqr\", \"sag\"]} # default auto\n    ]\n    \nlin_reg_raw_reg_res = gen_search_eval(\"lin_reg\", lin_reg_params, \"reg\", \"grid\")\nprint(f\"\\nBest performing pipeline: {lin_reg_raw_reg_res.best_estimator_}\")\n\nlin_reg_reg_res = fmt_search_res(lin_reg_raw_reg_res)\nlin_reg_reg_res.head(3)","b89eb94b":"def get_tuned_score(est_name:str, task:str):\n    score = globals()[f\"{est_name}_raw_{task}_res\"].best_score_\n    return score\n\nsummary_df_display_text = \"\"\n\nfor bl_df, mode in zip([blc_no_pwr, blr_no_pwr], [\"clf\", \"reg\"]):\n    caption = \"classifier summary\" if mode == \"clf\" else \"regressor summary\"\n    html_text = (\n        bl_df\n        .pipe(pd.pivot_table,\n              values=\"val_score\",\n              index=\"estimator\",\n              aggfunc=np.mean)\n        .rename(columns={\"val_score\": \"baseline\"})\n        .reset_index()\n        .sort_values(\"baseline\", ascending=False)\n        .assign(tuned = lambda x: x[\"estimator\"].apply(get_tuned_score, task=mode),\n                diff = lambda x: x[\"tuned\"] - x[\"baseline\"])\n        \n        # display options\n        .style.format(\"{:.1%}\", subset = [\"baseline\", \"tuned\", \"diff\"])\n        .set_table_attributes(\"style='display:inline'\")\n        .set_caption(caption)._repr_html_()\n    )\n    summary_df_display_text = summary_df_display_text + html_text + \"\\xa0\" * 2\n    \ndisplay_html(summary_df_display_text, raw=True)","78e20fa0":"best_clf = svc_raw_clf_res.best_estimator_\nbest_clf.fit(X_train_val, clf_y_train_val)\nprint(\"SVM classifier test score: \", best_clf.score(X_test, clf_y_test), \"\\n\")\n\nprint(classification_report(\n    y_true=clf_y_test, y_pred=best_clf.predict(X_test),\n    labels=[0, 1], target_names=[\"Unstable\", \"Stable\"]\n))\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\nplt.subplots_adjust(wspace=0.3)\nplot_confusion_matrix(best_clf, X_test, clf_y_test, ax=axs[0])\nplot_roc_curve(best_clf, X_test, clf_y_test, ax=axs[1]);","9247309b":"best_reg = lgbr_raw_reg_res.best_estimator_\nbest_reg.fit(X_train_val, reg_y_train_val)\nprint(\"LightGBM regressor test score: \", best_reg.score(X_test, reg_y_test))","5931d486":"pickle.dump(best_clf, open(\"grid_clf.pkl\", \"wb\"))\npickle.dump(best_reg, open(\"grid_reg.pkl\", \"wb\"))","fc7613d0":"Both Pearson correlation and variance inflation factor suggested `power[x]` columns may be collinear, so I should remove the producer `power1` column.","0e7dca3d":"<a id=\"hyperparameter-tuning-summary\"><\/a>\n**Optimisation summary**","f283b218":"**Support Vector Machine**\n\nSVM generates hyperplanes that separate data points from each class for classification problems, or minimise distance of all points from the plane for regression problems. Some of the most important hyperparameters are probably the kernel that is used to transform the data, and a few parameters that adjust the regularisation.","20b6557b":"**KNN**\n\nk-nearest neighbours makes predictions based on values of the nearest neighbours. For classification, this is summarised by a simple majority vote, whereas averaging is used for regression. This of course means hyperparameters related to the number of neighbours, how distances are calculated and how much weight is put on each neighbour's value affect the performance.","cb9f9f79":"Testing the final classifier on the held out test data scored 98.3% accuracy. Precision, recall and the resulting F1 scores are high in both classes, and the ROC curve looks great. Of course a score this high is extremely rare when working with real data, and suggests there isn't a lot of noise in this simulated data set. Still, [in this publication](https:\/\/link.springer.com\/article\/10.1007\/s42979-021-00463-5\/tables\/3), a deep learning model showed similar performance (97.5% accuracy, 98.7% precision, 98.2% F1-score) on the same data.\n\nThe final regressor also performed well, scoring 95.9% R<sup>2<\/sup> on held out test data.\n\nI then pickled both models:","0e1b2f28":"Overall, optimisation maintained or slightly improved accuracy in all classifiers (by 0.1 - 1.5%) and R<sup>2<\/sup> in all regressors (by 0.0 - 1.4%), with the best performances coming from the SVM classifier and LightGBM regressor.","dd6f9fa2":"**Random Forest**\n\nRandom forest models build decision trees from random subsets of samples and features, and also have many hyperparameters to tune. Of course the impact of each parameter may vary depending on the data set. For example, with only eight features in our data set, setting `max_features` to \"auto\", \"sqrt\" or \"log2\" should have no significant impact on performance - in all cases, three features should be considered at each split (though `max_features` could still be set to a specific number instead).","55f94433":"In this notebook, I use UCI's [Electrical Grid Stability data](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Electrical+Grid+Stability+Simulated+Data+) to predict if a given combination of power system conditions would result in an unstable grid - and therefore risk causing blackouts or damaging equipment. After brief data exploration and processing, I ran baseline models, confirmed my feature selection, then optimised the models. The best classifier scored 98.3% accuracy on held out data, similar to a deep learning model [in this publication](https:\/\/link.springer.com\/article\/10.1007\/s42979-021-00463-5\/tables\/3), while the best regressor gave an R<sup>2<\/sup> score of 95.9%. Finally, I pickled the best models and created [a simple app](https:\/\/share.streamlit.io\/sowla\/grid_stability_app\/main\/grid_stability.py), where users can adjust model inputs and see how resulting predictions are affected (source code and tests\/github actions are [here](https:\/\/github.com\/sowla\/grid_stability_app)).\n\n<a id=\"overview\"><\/a>\n### Overview\n\n* [Introduction](#introduction)\n* [Quick EDA](#quick-eda)\n* [Build baseline models](#build-baseline-models) (fit baseline models, check feature selection and overfitting)\n* [Optimise models](#optimise-models) (with randomised and grid searches, results summarised [here](#hyperparameter-tuning-summary))\n* [Test final model](#test-final-model)\n* [Interesting links](#interesting-links) (related to power grids)\n\n<a id=\"introduction\"><\/a>\n### Introduction\n\nShare of renewable electricity production in Germany has grown from [9% in 2002 to 46% in 2019](https:\/\/energy-charts.info\/charts\/renewable_share\/chart.htm?l=en&c=DE&interval=year), an [important progress for meeting climate targets](https:\/\/2022.entsos-tyndp-scenarios.eu\/wp-content\/uploads\/2021\/04\/entsog_entso-e_TYNDP2022_Joint_Scenarios_Final_Storyline_Report_210421.pdf). However, intermittency of weather-dependent renewable sources makes it [harder and more expensive](https:\/\/www.drax.com\/wp-content\/uploads\/2020\/08\/200828_Drax20_Q2_Report_005.pdf) to maintain grid stability (a balance of electricity production and consumption). The [Decentral Smart Grid Control](https:\/\/iopscience.iop.org\/article\/10.1088\/1367-2630\/17\/1\/015002#njp505903s5) (DSGC) concept was proposed as a way to adjust price based on supply and demand in a decentralised way - giving consumers an incentive to adjust their usage and help stabilise the grid without needing to centrally collect their usage data.\n\nThe data set I'm using [was originally simulated](https:\/\/arxiv.org\/pdf\/1508.02217v1.pdf) to explore if grid stability can be maintained under DSGC, assuming a 4-node architecture: one producer providing electricity to three consumers. There are 10,000 instances and 12 attributes:  \n- `p[x]` (`p1` to `p4`): power produced or consumed; `p1 = abs(p2 + p3 + p4)`\n- `g[x]` (`g1` to `g4`): willingness of each node to adapt their consumption or production per second (gamma, proportional to price elasticity)  \n- `tau[x]` (`tau1` to `tau4`): how long it takes for each node to adapt their production or consumption in seconds\n\nwhere `p1`, `g1` and `tau1` are related to the electricity producer; the rest are related to the electricity consumers.\n\nThere are also two target variables:  \n- `stab`: a number representing grid stability (positive if unstable)\n- `stabf`: a categorical version of `stab`\n\nand I've worked on both the classification and regression problems.","cb98a6c8":"Together, very high or very low sums of `delay[x]` and `adapt[x]` values should be indicators of (in)stability. Without further processing, these summarised values would highly correlate with existing features, so I'd rather keep the individual features. In contrast, it might be worth removing all `power[x]` columns if they're unhelpful.","382464ac":"As suggested by the data exploration and baseline model performances, reaction delay and willingness to adapt seem to be much more relevant than amount of power consumed for both the classification and regression tasks. To me, this suggests the `power[x]` columns were adding more noise than signal, and should be removed moving forward:","85f3e0c9":"Back to [Overview](#overview)\n\n<a id=\"optimise-models\"><\/a>\n### Optimise models\n*Hyperparameter tuning with randomised and grid searches*\n\nTo search for optimised hyperparameters and feature transformers, I used exhaustive grid searches when the search space is small, otherwise randomly searched 60 conditions, which should give me [a close approximate](https:\/\/www.oreilly.com\/library\/view\/evaluating-machine-learning\/9781492048756\/ch04.html) if at least 5% of the total conditions in the search space are close-to-optimal.\n\nThe feature transformations I explored were: standardising the feature so it has a mean of 0 and standard deviation of 1 (`StandardScaler`), rescaling the feature using outlier-insensitive statistics (`RobustScaler`) or [collapsing any outliers](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html?highlight=scalers#quantiletransformer-uniform-output) (`QuantileTransformer`). For context, I also added `FunctionTransformer`, which allows me to exclude the transformation step.","29fdee7b":"The correlation between stability (`stab` or `stabf`) and `delay[x]` or `adapt[x]` columns were weak, whereas there was no obvious relationship between `power[x]` columns and stability.\n\n`power1` (power generated) was correlated with the other `power[x]` columns (power consumed), as expected, but there was no obvious correlation within `delay[x]` or `adapt[x]` columns.","87dab0fe":"**Logistic\/Linear Regression**\n\nLinear regression linearly combines the input features. In logistic regression, the results are then transformed into probabilities. I explored parameters controlling regularisation type and strength, as well as the algorithm used to find the optimal coefficients.","c1b0bf12":"Focusing on models that were fitted on the data without power consumption columns, all classifiers scored over 80% on validation accuracy. Support Vector (`svc`), Light Gradient Boosting Machine (LightGBM, `lgbc`) and random forest (RF, `rfc`) models gave the best scores. Validation R<sup>2<\/sup> were extremely varied across regressors, with LightGBM (`lgbr`), RF (`rfr`) and K-Nearest Neighbour (`knr`) being the best performers.\n\nFor both classification and regression, the LightGBM and RF models had close-to-perfect train scores. This could suggest overfitting, but is not uncommon for such ensemble models, since enough trees could have trained on each training case to outweigh those that didn't. Instead of scoring based on the default accuracy for classification and R<sup>2<\/sup> for regression, I could look at log loss and mean squared error (MSE), for example:","2c6e080c":"Back to [Overview](#overview)\n\n<a id=\"test-final-model\"><\/a>\n### Test final model","e9dea27c":"Share of unstable events:\n- increased with reaction delay (of both producers and consumers) until roughly 5 seconds, after which the share of unstable events was relatively unaffected by further increases in delay times\n- increased linearly with willingness to adapt (of both producers and consumers)\n- still seemed uncorrelated with amount of power produced or consumed","5ece26d2":"Back to [Overview](#overview)\n\n<a id=\"quick-eda\"><\/a>\n### Quick EDA\n\nSince the data was simulated and very clean, I only briefly explored and processed it, e.g. basic quality checks, look at class balance, encode labels and rename columns for clarity:","e20281c3":"To get a better understanding of the data and which models perform well, I fitted a few models using 5-fold cross validation to two versions of the data (with or without the consumer `power[x]` features):","5b575f48":"For the LightGBM classifier, neither training nor validation log loss had decreased to a stable level. If anything, I think it was probably underfitted, so I can increase the learning rate and\/or number of trees. The fit looks much better for the LightGBM regressor, but the validation MSE doesn't start to deviate from the train MSE again, so I think it's still not overfitted.\n\nAnother way check the fit on training samples is to look at out-of-bag (OOB) scores in RF models (ie. calculate scores using only trees that didn't train on the particular training samples), for example:","54ebc07a":"Neither tuned RF classifier nor regressor seem overfitted.","b9291f90":"With the tuned parameters, both training and validation loss decrease to a plateau for the best tuned classifier and regressor. The classifier fit improved from the baseline model; the regressor fit seems similar, but importantly, not overfitted.","d9eaff22":"**Light Gradient Boosting Machine**\n\nGradient boosting models iteratively build weak prediction models, minimising loss at each stage. Of the gradient boosting models available, I picked LightGBM, since it tends to be [much faster than XGBoost](https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Experiments.rst#comparison-experiment) as well as [CatBoost](https:\/\/publications.waset.org\/10009954\/comparison-between-xgboost-lightgbm-and-catboost-using-a-home-credit-datasethttps:\/\/publications.waset.org\/10009954\/comparison-between-xgboost-lightgbm-and-catboost-using-a-home-credit-dataset), while giving similar results. LightGBM has an [overwhelming number of adjustable parameters](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html), some that adjust the ensemble model itself and others that put constraints on the individual trees.","29c8ea04":"Removing the consumer `power[x]` columns increased accuracy scores for the top four out of five classifiers, and maintained or increased the R<sup>2<\/sup> score for all regressors. To get a better feel for whether the `power[x]` columns add useful information, I looked at the feature\/permutation importance in tree-based models and absolute values of coefficients in L1-penalised linear models for each feature. As examples, I'm showing one type each for classification and regression models:","9148c604":"Back to [Overview](#overview)\n\n<a id=\"build-baseline-models\"><\/a>\n### Build baseline models\n\n*Fit baseline models (include feature scaling in pipeline to avoid data leakage but allow for easy adjustments when optimising), use feature\/permutation importance and coefficients to confirm feature selection, check for overfitting*\n\nI first split the data, holding out 20% as the test set and using stratification to keep a consistent class share for the classification task:","be1fd24a":"Back to [Overview](#overview)\n\n<a id=\"interesting-links\"><\/a>\n### Interesting links\n\nI thought I'd share a few links\/facts I found while doing this project in case they're interesting to anyone else :)\n\n- In Germany, commercial electricity producers and consumers have to give [quarter-hourly forecasts](https:\/\/www.amprion.net\/Energy-Market\/Balancing-Groups\/Balancing-Group-Price\/Important-Information.html) of the amount of electricity they'll produce\/consume. You can see current data [here](https:\/\/www.smard.de\/home\/marktdaten?marketDataAttributes=%7B%22resolution%22:%22hour%22,%22region%22:%22DE%22,%22moduleIds%22:%5B1000100,1000101,1000102,1000103,1000104,1000108,1000109,1000110,1000111,1000112,1000113,1000121,5000410,1001226,1001228,1001227,1001223,1001224,1001225,1004066,1004067,1004068,1004069,1004071,1004070,2000122,6000411%5D,%22selectedCategory%22:null,%22activeChart%22:true,%22style%22:%22color%22%7D) (for periods I've checked, predictions for power consumption is a lot more accurate than for power generation, which makes sense).\n- Transmission system operators (TSOs) have to constantly keep track of and react to changes within [their own grids as well as their neighbours'](https:\/\/www.entsoe.eu\/regions\/). So much coordination has to go on that they form \"[Regional Security Coordinator](https:\/\/www.entsoe.eu\/major-projects\/rscis\/#why-do-we-need-to-strengthen-regional-coordination-now)\" (RSC) companies together.\n- \"Prosumers\", individuals and businesses that act as consumers *and* producers, can [contribute to the energy system](https:\/\/smarten.eu\/wp-content\/uploads\/2020\/05\/Smart_Energy_Prosumers_2020.pdf) in many ways.","9c782346":"so that I could easily use them in [a simple app](https:\/\/share.streamlit.io\/sowla\/grid_stability_app\/main\/grid_stability.py) that can be used to see how adjusting model inputs affect the resulting predictions:\n\n<img src=\"https:\/\/raw.githubusercontent.com\/sowla\/grid_stability_app\/main\/img\/app.png\" alt=\"streamlit app screenshot\" width=\"70%\"\/>\n\nThe source code, as well as the associated tests and github actions, live [here](https:\/\/github.com\/sowla\/grid_stability_app).","8b0ad137":"Unlike the default training scores, the OOB training scores were slightly lower than the validation scores for both tasks, so I don't think the baseline RF models were overfitted either."}}