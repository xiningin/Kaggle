{"cell_type":{"1533d440":"code","d33186d8":"code","2d856cb8":"code","56b291f5":"code","388d2190":"code","062f5adb":"code","d6133f4e":"code","1d5102c3":"code","a1b3e4e6":"code","e19621a1":"code","02180855":"code","a88865f0":"code","7f7f39c1":"code","d5b6712f":"code","723e8220":"code","21ac50f3":"code","4e2d6db1":"code","b7572561":"code","435694a2":"code","373ad4e9":"code","44c9353c":"code","98d401e7":"code","50586e20":"code","bfb2e559":"code","7402245f":"code","288b12a2":"markdown","08255d73":"markdown","75c14375":"markdown","72468a08":"markdown","30808763":"markdown"},"source":{"1533d440":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d33186d8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\npd.pandas.set_option('display.max_columns', None)\nimport seaborn as sns\nsns.set(color_codes = True)\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder,OrdinalEncoder\n\npd.pandas.set_option('display.max_columns', None)\ntrain = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')\ntrain.head()","2d856cb8":"train.describe()","56b291f5":"test.info()","388d2190":"train['bin_3'] = pd.get_dummies(train['bin_3'],drop_first = True)\ntrain['bin_4'] = pd.get_dummies(train['bin_4'],drop_first = True)","062f5adb":"plt.subplots(figsize=(20,10))\n\nplt.subplot(231)\nax = sns.countplot(x=\"nom_0\", data=train,palette = 'rocket')\n\nplt.subplot(232)\nax = sns.countplot(x=\"nom_1\", data=train,palette = 'Purples')\n\nplt.subplot(233)\nax = sns.countplot(x=\"nom_2\", data=train,palette = 'flare')\n\nplt.subplot(234)\nax = sns.countplot(x=\"nom_3\", data=train,palette = 'gray')\n\nplt.subplot(235)\nax = sns.countplot(x=\"nom_4\", data=train,palette = 'rainbow')","d6133f4e":"count_1 = train.nom_0.value_counts().to_dict()\ncount_1\ntrain.nom_0 = train.nom_0.map(count_1)\n\ncount_2 = train.nom_1.value_counts().to_dict()\ncount_2\ntrain.nom_1 = train.nom_1.map(count_2)\n\ncount_3 = train.nom_2.value_counts().to_dict()\ncount_3\ntrain.nom_2 = train.nom_2.map(count_3)\n\ncount_4 = train.nom_3.value_counts().to_dict()\ncount_4\ntrain.nom_3 = train.nom_3.map(count_4)\n\ncount_5 = train.nom_4.value_counts().to_dict()\ncount_5\ntrain.nom_4 = train.nom_4.map(count_5)\n\ncount_6 = train.nom_5.value_counts().to_dict()\ncount_6\ntrain.nom_5 = train.nom_5.map(count_6)\n\ncount_7 = train.nom_6.value_counts().to_dict()\ncount_7\ntrain.nom_6 = train.nom_6.map(count_7)\n\ncount_8 = train.nom_7.value_counts().to_dict()\ncount_8\ntrain.nom_7 = train.nom_7.map(count_8)\n\ncount_9 = train.nom_8.value_counts().to_dict()\ncount_9\ntrain.nom_8 = train.nom_8.map(count_9)\n\ncount_10 = train.nom_9.value_counts().to_dict()\ncount_10\ntrain.nom_9 = train.nom_9.map(count_10)","1d5102c3":"plt.subplots(figsize=(20,10))\n\nplt.subplot(231)\nax = sns.countplot(x=\"ord_1\", data=train,palette = 'copper')\n\nplt.subplot(232)\nax = sns.countplot(x=\"ord_2\", data=train,palette = 'Blues')\n\nplt.subplot(233)\nax = sns.countplot(x=\"ord_3\", data=train,palette = 'flare')\n\nplt.subplot(234)\nax = sns.countplot(x=\"ord_4\", data=train,palette = 'cool')\n","a1b3e4e6":"encoder = LabelEncoder()\ntrain['ord_1'] = encoder.fit_transform(train['ord_1'])\ntrain['ord_2'] = encoder.fit_transform(train['ord_2'])\ntrain['ord_3'] = encoder.fit_transform(train['ord_3'])\ntrain['ord_4'] = encoder.fit_transform(train['ord_4'])\ntrain['ord_5'] = encoder.fit_transform(train['ord_5'])","e19621a1":"train.head()","02180855":"train = train.drop('id',axis = 1)","a88865f0":"x = train.drop('target',axis = 1)\ny = train.iloc[:,-1]","7f7f39c1":"from sklearn.preprocessing import StandardScaler\n\nstd = StandardScaler()\n\nx_std = std.fit_transform(x)\nx_std.shape","d5b6712f":"print(x_std)","723e8220":"from sklearn.model_selection import train_test_split\n#Split data into Train and test format\nx_train,x_test,y_train,y_test = train_test_split(x_std,y,test_size = 0.20,random_state =42)\n\nprint('Shape of Training Xs:{}'.format(x_train.shape))\nprint('shape of Test:{}'.format(x_test.shape))","21ac50f3":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LeakyReLU,PReLU,ELU\nfrom keras.layers import Dropout","4e2d6db1":"# Initialising the ANN\nclassifier = Sequential()","b7572561":"# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 60, kernel_initializer = 'he_uniform',activation='relu',input_dim = 23))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 45, kernel_initializer = 'he_uniform',activation='relu'))\n\n# Adding the Third hidden layer\nclassifier.add(Dense(units = 30, kernel_initializer = 'he_normal',activation='relu'))\n\n# Adding the fourth hidden layer\nclassifier.add(Dense(units = 20, kernel_initializer = 'he_uniform',activation='relu'))\n\n# Adding the fourth hidden layer\nclassifier.add(Dense(units = 10, kernel_initializer = 'he_normal',activation='relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])","435694a2":"# Fitting the ANN to the Training set\nmodel_history=classifier.fit(x_train, y_train,validation_split=0.33, batch_size = 10, epochs = 50)","373ad4e9":"print(model_history.history.keys())","44c9353c":"print(model_history.history.keys())\n# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","98d401e7":"# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","50586e20":"# Predicting the Test set results\ny_pred = classifier.predict(x_test)\ny_pred = (y_pred > 0.5)","bfb2e559":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","7402245f":"# Calculate the Accuracy\nfrom sklearn.metrics import accuracy_score\nscore=accuracy_score(y_pred,y_test)\nprint(score)","288b12a2":"# 1. Import Libraries and Data","08255d73":"# 3.Train Model\n","75c14375":"firstly we will perform binary classification over here will apply one hot encoding or dummy encoding to convert feature into numerical form.","72468a08":"# 2. Data Visualization","30808763":"In below data you can see there are binary, nominal and ordinal classfication are present accordingly will perform preprocessing"}}