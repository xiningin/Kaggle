{"cell_type":{"77680b0c":"code","7a1d5934":"code","c34aa689":"code","c8e57578":"code","cb824f98":"code","3234f6ff":"code","4d10df42":"code","e41afcd8":"code","7a9eee3e":"code","60b49cc6":"code","1d5fc57a":"code","631126f7":"code","ef14438e":"code","abac5f9b":"code","c6e5f749":"code","bff374fd":"code","cc423b56":"code","7639f66c":"code","5204c5c8":"code","6152b973":"code","7f60b3ec":"code","17c022d6":"code","e21ec912":"code","725ea8a9":"code","ecc80dcf":"code","aaa843f7":"code","d41af8a0":"code","d25485de":"markdown","c3d8af73":"markdown","e405b6bd":"markdown"},"source":{"77680b0c":"import numpy as np\nimport pandas as pd \nfrom tqdm import tqdm_notebook\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly import tools\nimport plotly.figure_factory as ff\n\nfrom scipy.signal import hilbert, hann, convolve\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport time\nimport datetime\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nprint(os.listdir(\"..\/input\"))","7a1d5934":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.float32, 'time_to_failure': np.float32})","c34aa689":"train.head()","c8e57578":"def classic_sta_lta_py(a, nsta, nlta):\n    \"\"\"\n    Computes the standard STA\/LTA from a given input array a. The length of\n    the STA is given by nsta in samples, respectively is the length of the\n    LTA given by nlta in samples. Written in Python.\n    .. note::\n        There exists a faster version of this trigger wrapped in C\n        called :func:`~obspy.signal.trigger.classic_sta_lta` in this module!\n    :type a: NumPy :class:`~numpy.ndarray`\n    :param a: Seismic Trace\n    :type nsta: int\n    :param nsta: Length of short time average window in samples\n    :type nlta: int\n    :param nlta: Length of long time average window in samples\n    :rtype: NumPy :class:`~numpy.ndarray`\n    :return: Characteristic function of classic STA\/LTA\n    \"\"\"\n    # The cumulative sum can be exploited to calculate a moving average (the\n    # cumsum function is quite efficient)\n    sta = np.cumsum(a ** 2)\n\n    # Convert to float\n    sta = np.require(sta, dtype=np.float)\n\n    # Copy for LTA\n    lta = sta.copy()\n\n    # Compute the STA and the LTA\n    sta[nsta:] = sta[nsta:] - sta[:-nsta]\n    sta \/= nsta\n    lta[nlta:] = lta[nlta:] - lta[:-nlta]\n    lta \/= nlta\n\n    # Pad zeros\n    sta[:nlta - 1] = 0\n\n    # Avoid division by zero by setting zero values to tiny float\n    dtiny = np.finfo(0.0).tiny\n    idx = lta < dtiny\n    lta[idx] = dtiny\n\n    return sta \/ lta","cb824f98":"per = 0.000005","3234f6ff":"#Calculate Hilbert transform\nsignal = train.acoustic_data[:int(len(train)*per)]\nanalytic_signal = hilbert(signal)\namplitude_envelope = np.abs(analytic_signal)\n\n#Calculate Hann func\nwin = hann(50)\nfiltered = convolve(signal, win, mode='same') \/ sum(win)\n\n#Calculate STA\/LTA\nsta_lta = classic_sta_lta_py(signal, 50, 1000)","4d10df42":"trace0 = go.Scatter(\n    y = signal,\n    name = 'signal'\n)\n\ntrace1 = go.Scatter(\n    y = amplitude_envelope,\n    name = 'amplitude_envelope'\n)\n\n\ntrace3 = go.Scatter(\n    y = filtered,\n    name= 'filtered'\n) \n\ntrace4 = go.Scatter(\n    y = sta_lta,\n    name= 'sta_lta'\n) \n\n\ntrace_time = go.Scatter(\n    y = train.time_to_failure[:int(len(train)*per)],\n)\n\ndata = [trace0, trace1, trace3,trace4]\n\nlayout = go.Layout(\n    title = \"Part acoustic_data\"\n)\n\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig, filename = \"Part acoustic_data\")","e41afcd8":"col_feat = ['ave','med', 'std', 'max', 'min',\n                               'av_change_abs', 'av_change_rate', 'abs_max', 'abs_min',\n                               'std_first_50000', 'std_last_50000', 'std_first_10000', 'std_last_10000',\n                               'avg_first_50000', 'avg_last_50000', 'avg_first_10000', 'avg_last_10000',\n                               'min_first_50000', 'min_last_50000', 'min_first_10000', 'min_last_10000',\n                               'max_first_50000', 'max_last_50000', 'max_first_10000', 'max_last_10000',\n                               ]\nname_lines = ['','amp_env','filt','sta_lta']\ncolumns = []\nfor cf in col_feat:\n    for name in name_lines:\n        columns.append(cf+'_'+name)\n        ","7a9eee3e":"len(columns)","60b49cc6":"rows = 150_000\nsegments = int(np.floor(train.shape[0] \/ rows))\n\ny_tr = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\nX_tr = pd.DataFrame()","1d5fc57a":"\ndef get_feature(x,segment, name_lines):\n    X_tr1 = pd.DataFrame(dtype=np.float64)\n    X_tr1.loc[segment, 'ave'+'_'+name_lines] = x.mean()\n    X_tr1.loc[segment, 'ave'+'_'+name_lines] = np.median(x)\n    X_tr1.loc[segment, 'std'+'_'+name_lines] = x.std()\n    X_tr1.loc[segment, 'max'+'_'+name_lines] = x.max()\n    X_tr1.loc[segment, 'min'+'_'+name_lines] = x.min()\n    \n    X_tr1.loc[segment, 'av_change_abs'+'_'+name_lines] = np.mean(np.diff(x))\n    X_tr1.loc[segment, 'av_change_rate'+'_'+name_lines] = np.mean(np.nonzero((np.diff(x) \/ x[:-1]))[0])\n    X_tr1.loc[segment, 'abs_max'+'_'+name_lines] = np.abs(x).max()\n    X_tr1.loc[segment, 'abs_min'+'_'+name_lines] = np.abs(x).min()\n    \n    X_tr1.loc[segment, 'std_first_50000'+'_'+name_lines] = x[:50000].std()\n    X_tr1.loc[segment, 'std_last_50000'+'_'+name_lines] = x[50000:].std()\n    X_tr1.loc[segment, 'std_first_10000'+'_'+name_lines] = x[:10000].std()\n    X_tr1.loc[segment, 'std_last_10000'+'_'+name_lines] = x[10000:].std()\n    \n    X_tr1.loc[segment, 'avg_first_50000'+'_'+name_lines] = x[:50000].mean()\n    X_tr1.loc[segment, 'avg_last_50000'+'_'+name_lines] = x[50000:].mean()\n    X_tr1.loc[segment, 'avg_first_10000'+'_'+name_lines] = x[:10000].mean()\n    X_tr1.loc[segment, 'avg_last_10000'+'_'+name_lines] = x[10000:].mean()\n    \n    X_tr1.loc[segment, 'min_first_50000'+'_'+name_lines] = x[:50000].min()\n    X_tr1.loc[segment, 'min_last_50000'+'_'+name_lines] = x[50000:].min()\n    X_tr1.loc[segment, 'min_first_10000'+'_'+name_lines] = x[:10000].min()\n    X_tr1.loc[segment, 'min_last_10000'+'_'+name_lines] = x[10000:].min()\n    \n    X_tr1.loc[segment, 'max_first_50000'+'_'+name_lines] = x[:50000].max()\n    X_tr1.loc[segment, 'max_last_50000'+'_'+name_lines] = x[50000:].max()\n    X_tr1.loc[segment, 'max_first_10000'+'_'+name_lines] = x[:10000].max()\n    X_tr1.loc[segment, 'max_last_10000'+'_'+name_lines] = x[10000:].max()\n    return X_tr1","631126f7":"np.random.seed(42)\nfor segment in tqdm_notebook(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    y_tr.loc[segment, 'time_to_failure'] = y\n    \n    x_ac_signal = get_feature(x,segment, 'data')\n    \n    analytic_signal = hilbert(x)\n    amplitude_envelope = np.abs(analytic_signal)\n    x_amp_env = get_feature(amplitude_envelope,segment, 'amp_env')\n    \n    win = hann(100)\n    filtered = convolve(x, win, mode='same') \/ sum(win)\n    x_filt_hann = get_feature(filtered,segment, 'filt')\n    \n    sta_lta = classic_sta_lta_py(x, 1000, 5000)\n    x_sta_lta = get_feature(sta_lta,segment, 'sta_lta')\n    \n    df_loc = pd.concat([x_ac_signal, x_amp_env, x_filt_hann, x_sta_lta], axis = 1)\n    X_tr = X_tr.append(df_loc)","ef14438e":"segments = 10000\n\ny_tr_more = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\nX_tr_more = pd.DataFrame()\nnp.random.seed(42)\nfor segment in tqdm_notebook(range(segments)):\n    ind = np.random.randint(0, train.shape[0]-150001)\n    seg = train.iloc[ind:ind+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_tr_more.loc[segment, 'time_to_failure'] = y\n    x_ac_signal = get_feature(x,segment, 'data')\n    \n    analytic_signal = hilbert(x)\n    amplitude_envelope = np.abs(analytic_signal)\n    x_amp_env = get_feature(amplitude_envelope,segment, 'amp_env')\n    \n    win = hann(100)\n    filtered = convolve(x, win, mode='same') \/ sum(win)\n    x_filt_hann = get_feature(filtered,segment, 'filt')\n    \n    sta_lta = classic_sta_lta_py(x, 1000, 5000)\n    x_sta_lta = get_feature(sta_lta,segment, 'sta_lta')\n    \n    df_loc = pd.concat([x_ac_signal, x_amp_env, x_filt_hann, x_sta_lta], axis = 1)\n    X_tr_more = X_tr_more.append(df_loc)","abac5f9b":"X_tr = X_tr.append(X_tr_more)\ny_tr = y_tr.append(y_tr_more)\nprint(f'{X_tr.shape[0]} samples in new train data now.')","c6e5f749":"X_tr.tail()","bff374fd":"scaler = StandardScaler()\nscaler.fit(X_tr)\nX_train_scaled = pd.DataFrame(scaler.transform(X_tr), columns=X_tr.columns)","cc423b56":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')\nX_test = pd.DataFrame()","7639f66c":"for i, seg_id in enumerate(tqdm_notebook(submission.index)):\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    \n    x_ac_signal = get_feature(x,seg_id, 'data')\n    \n    analytic_signal = hilbert(x)\n    amplitude_envelope = np.abs(analytic_signal)\n    x_amp_env = get_feature(amplitude_envelope,seg_id, 'amp_env')\n    \n    win = hann(100)\n    filtered = convolve(x, win, mode='same') \/ sum(win)\n    x_filt_hann = get_feature(filtered,seg_id, 'filt')\n    \n    sta_lta = classic_sta_lta_py(x, 1000, 5000)\n    x_sta_lta = get_feature(sta_lta,seg_id, 'sta_lta')\n    \n    df_loc = pd.concat([x_ac_signal, x_amp_env, x_filt_hann, x_sta_lta], axis = 1)\n    X_test = X_test.append(df_loc)","5204c5c8":"X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","6152b973":"len(X_test_scaled)","7f60b3ec":"len(X_train_scaled)","17c022d6":"X_train_scaled = X_train_scaled.fillna(0)\nX_test_scaled = X_test_scaled.fillna(0)","e21ec912":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=11)","725ea8a9":"def train_model(X=X_train_scaled, X_test=X_test_scaled, y=y_tr, params=None, folds=folds, model_type='lgb', plot_feature_importance=False, model=None):\n\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n        print('Fold', fold_n, 'started at', time.ctime())\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        if model_type == 'lgb':\n            model = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n                    verbose=10000, early_stopping_rounds=200)\n            \n            y_pred_valid = model.predict(X_valid)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        \n        oof[valid_index] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n\n        prediction += y_pred\n        \n        if model_type == 'lgb':\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = X.columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n    prediction \/= n_fold\n    \n    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n    \n    if model_type == 'lgb':\n        feature_importance[\"importance\"] \/= n_fold\n        if plot_feature_importance:\n            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n                by=\"importance\", ascending=False)[:50].index\n\n            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n            plt.figure(figsize=(16, 12));\n            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n            plt.title('LGB Features (avg over folds)');\n        \n            return oof, prediction, feature_importance\n        return oof, prediction\n    \n    else:\n        return oof, prediction","ecc80dcf":"params = {'num_leaves': 54,\n         'min_data_in_leaf': 79,\n         'objective': 'huber',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         # \"feature_fraction\": 0.8354507676881442,\n         \"bagging_freq\": 3,\n         \"bagging_fraction\": 0.8126672064208567,\n         \"bagging_seed\": 11,\n         \"metric\": 'mae',\n         \"verbosity\": -1,\n         'reg_alpha': 1.1302650970728192,\n         'reg_lambda': 0.3603427518866501\n         }\noof_lgb, prediction_lgb, feature_importance = train_model(params=params, model_type='lgb', plot_feature_importance=True)","aaa843f7":"len(prediction_lgb)","d41af8a0":"submission['time_to_failure'] = prediction_lgb\nprint(submission.head())\nsubmission.to_csv('submission.csv')","d25485de":"# Intro\nThis kernel is dedicated to exploration of LANL Earthquake Prediction Challenge. \nThis kernel is different in that I suggest trying out different methods and functions that are used to process signals for feature extraction:\n* [Hilbert transform](http:\/en.wikipedia.org\/wiki\/Analytic_signal) \n* Smooth a pulse using a [Hann](http:\/\/en.wikipedia.org\/wiki\/Hann_function) window \n* Use trigger [classic STA\/LTA](http:\/\/docs.obspy.org\/tutorial\/code_snippets\/trigger_tutorial.html#available-methods)\n\nThank [ Andrew Lukyanenko](http:\/\/www.kaggle.com\/artgor) for his [kernel](http:\/\/www.kaggle.com\/artgor\/seismic-data-eda-and-baseline\/output)  - my decision is based on his.\n\nThank [Vishy](http:\/\/www.kaggle.com\/viswanathravindran) for his [discuss](http:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/77267) and links.\n\nYou can view and discuss these features in my [kernel](https:\/\/www.kaggle.com\/nikitagribov\/analysis-function-for-signal-data) or [discuss](https:\/\/www.kaggle.com\/c\/LANL-Earthquake-Prediction\/discussion\/77267#455024) [Vishy](http:\/\/www.kaggle.com\/viswanathravindran).","c3d8af73":"![](https:\/\/pp.userapi.com\/c848636\/v848636381\/10387a\/82TkN23uVpQ.jpg)","e405b6bd":"# New Features example"}}