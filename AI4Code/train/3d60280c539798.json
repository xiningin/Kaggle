{"cell_type":{"005c1e86":"code","2553d7b3":"code","febb25be":"code","434c1358":"code","1b636fe8":"code","28e96122":"code","75b263be":"code","ffb66c51":"code","f5d4f29e":"code","ea27f991":"code","ae2ac4b4":"code","672838b3":"code","8527ca9b":"code","6d894892":"code","fc68617d":"code","bb1738a5":"code","194c2bad":"code","753d8652":"code","330e8b1f":"code","81750914":"code","cf571263":"code","4e64caee":"code","f885e067":"code","1ab40ed5":"code","c3247dbd":"code","570f837d":"code","809bf49b":"code","8fa8f79a":"code","20901b2e":"code","94baff78":"code","f69f50ec":"code","272a61c8":"code","84785368":"code","6a56ca40":"code","9e42f056":"code","7e97db17":"code","1b383b8c":"code","0f59ea61":"markdown","6c490657":"markdown","2223ba44":"markdown","bb982ec3":"markdown","d254e18e":"markdown","0c837532":"markdown","91f37d93":"markdown","071d57d7":"markdown","02999a8c":"markdown","1f116238":"markdown","16f88c9a":"markdown","bbd1ad9b":"markdown","4b79abb4":"markdown","20e123d2":"markdown","70fddf27":"markdown","bcf50729":"markdown","d717564d":"markdown","d273c749":"markdown","b20fdc9a":"markdown","180f66ec":"markdown","6ba679e3":"markdown","b2f9ab82":"markdown","791c81d4":"markdown","7b6ff543":"markdown","ac5b78bb":"markdown","56ad907c":"markdown","2956fc24":"markdown"},"source":{"005c1e86":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport time\nfrom itertools import product\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import base\nimport numpy as np","2553d7b3":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","febb25be":"INPUTFOLDER = '..\/input\/competitive-data-science-predict-future-sales\/'\n\nitem_categories = pd.read_csv(os.path.join(INPUTFOLDER, 'item_categories.csv'))\nitems           = pd.read_csv(os.path.join(INPUTFOLDER, 'items.csv'))\nsales           = pd.read_csv(os.path.join(INPUTFOLDER, 'sales_train.csv'))\nshops           = pd.read_csv(os.path.join(INPUTFOLDER, 'shops.csv'))\ntest            = pd.read_csv(os.path.join(INPUTFOLDER, 'test.csv'))","434c1358":"MONTHS = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\nLINEWIDTH=2\nALPHA=.6\n\ndata = sales[['date', 'date_block_num','item_cnt_day']].copy()\n\n# Extract the year and the month from the date column into indepedent columns\ndata['date']  = pd.to_datetime(data['date'], format='%d.%m.%Y')\ndata['year']  = data['date'].dt.year\ndata['month'] = data['date'].dt.month\ndata.drop(['date'], axis=1, inplace=True)","1b636fe8":"\ndata = data.groupby('date_block_num', as_index=False)\\\n       .agg({'year':'first', 'month':'first', 'item_cnt_day':'sum'})\\\n       .rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)","28e96122":"plt.figure(figsize=(16,6))\n# Plot the sales of the year 2013\nplt.plot(MONTHS, data[data.year==2013].item_cnt_month, '-o', color='thistle', linewidth=LINEWIDTH, alpha=ALPHA,label='2013')\n# Plot the sales of the year 2014\nplt.plot(MONTHS, data[data.year==2014].item_cnt_month, '-o', color='green', linewidth=LINEWIDTH, alpha=ALPHA,label='2014')\n# Plot the sales of the year 2015 until October\nplt.plot(MONTHS[:10], data[data.year==2015].item_cnt_month, '-o', color='maroon', linewidth=LINEWIDTH, alpha=ALPHA,label='2015')\n\n\n# Axes parameters\nax = plt.gca()\nax.set_title('Sales per month')\nax.set_ylabel('# of items')\nax.grid(axis='y', color='gray', alpha=.2)\n    \n# Remove the frame off the chart\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nplt.legend(loc=2, title='Legend')\nplt.show()\n\ndel data","75b263be":"\n# Top N \nN=15\n\ndef get_ratio(year, topn, N):\n    # Get total sold items for each year\n    total = data.loc[year].item_cnt_year.sum()\n    ratio = topn\/total*100\n    return \"{0}: the total of the top {1} best selling items is {2} over a total of {3} for that year, which represents {4:.2f}%\".format(year, N, topn, total, ratio)\n\ndata = sales[['date', 'item_id', 'item_cnt_day']].copy()\ncats = item_categories.copy()\n\n# Extract the year from the date column\ndata['year'] = pd.to_datetime(data['date'], format='%d.%m.%Y').dt.year\ndata.drop('date', axis=1, inplace=True)\ndata.item_cnt_day = data.item_cnt_day.astype(int)\n\n# Remove returns\ndata = data[data.item_cnt_day>0]\n\n# Add the category of each item\ndata = data.merge(items[['item_id','item_category_id']], how='left', on='item_id')\n\n# Number of categories sold each year\ndata = data.groupby(['year', 'item_category_id'])\\\n       .agg({'item_cnt_day':'sum'})\\\n       .rename(columns={'item_cnt_day':'item_cnt_year'}, inplace=False)\n\n# Top N categories sold \ntop = data['item_cnt_year'].groupby('year', group_keys=False).nlargest(N)\n# Convert top to a dataframe\ntop = pd.DataFrame(top).reset_index()\n# Add category type to be plotted lated\ntop = top.merge(cats[['item_category_id','item_category_name']], how='left', on='item_category_id')","ffb66c51":"years = [2013, 2014, 2015]\nfig, axes = plt.subplots(1, 3, figsize=(16,6))\n\n#Prepare colors for the top N\ncolors = [[] for i in range(3)]\nfor alpha in np.arange(N, 0, -1)\/N:\n    colors[0].append((.275, .51, .706, alpha))\n    colors[1].append((.18, .55, .34, alpha))\n    colors[2].append((.5, 0, 0, alpha))\n    \nfor ax, year, cs in zip(axes, years, colors):\n    # Get top items for each year\n    year_filter = top[top.year==year]\n    plot_sizes = year_filter.item_cnt_year\n    plot_labels = year_filter.item_category_name.str[:15]\n    \n    # Get the ratio\n    print(get_ratio(year, plot_sizes.sum(), N))\n    \n    # Plot the pie\n    ax.pie(plot_sizes, labels=plot_labels, radius=1.5, colors=cs,labeldistance=.5, rotatelabels=True, startangle=90, wedgeprops={\"edgecolor\":\"1\",'linewidth': .5})\n    # Set titles below pies\n    ax.set_title(year, y=-0.2)\n\n# Space pies\nfig.tight_layout()\nfig.suptitle('Top selling categories for each year', fontsize=16)\nplt.show()\n\ndel data","f5d4f29e":"sales = sales[(sales.item_price<100000)&(sales.item_price>0)]\nsales = sales[(sales.item_cnt_day>0)&(sales.item_cnt_day<1000)]\n\n# Remove duplicate shops\nsales.loc[sales.shop_id==0, 'shop_id'] = 57\ntest.loc[test.shop_id==0, 'shop_id'] = 57\n\nsales.loc[sales.shop_id==1, 'shop_id'] = 58\ntest.loc[test.shop_id==1, 'shop_id'] = 58\n\nsales.loc[sales.shop_id==10, 'shop_id'] = 11\ntest.loc[test.shop_id==10, 'shop_id'] = 11","ea27f991":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops[\"shop_city\"] = shops.shop_name.str.split(' ').map(lambda x: x[0])\nshops[\"shop_category\"] = shops.shop_name.str.split(\" \").map(lambda x: x[1])\nshops.loc[shops.shop_city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"shop_city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\" ","ae2ac4b4":"shops[\"shop_city\"] = LabelEncoder().fit_transform(shops.shop_city)\nshops[\"shop_category\"] = LabelEncoder().fit_transform(shops.shop_category)\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\nshops.head()","672838b3":"item_categories[\"category_type\"] = item_categories.item_category_name.apply(lambda x: x.split(\" \")[0]).astype(str)\n# The category_type \"Gamming\" and \"accesoires\" becomes \"Games\"\nitem_categories.loc[(item_categories.category_type==\"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")|(item_categories.category_type==\"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category_type\"] = \"\u0418\u0433\u0440\u044b\"\nitem_categories[\"split\"] = item_categories.item_category_name.apply(lambda x: x.split(\"-\"))\nitem_categories[\"category_subtype\"] = item_categories.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())","8527ca9b":"item_categories[\"category_type\"] = LabelEncoder().fit_transform(item_categories.category_type)\nitem_categories[\"category_subtype\"] = LabelEncoder().fit_transform(item_categories.category_subtype)\nitem_categories = item_categories[[\"item_category_id\", \"category_type\", \"category_subtype\"]]\nitem_categories.head()","6d894892":"sales = sales.groupby(['date_block_num', 'shop_id', 'item_id'], as_index=False)\\\n          .agg({'item_cnt_day':'sum'})\\\n          .rename(columns={'item_cnt_day':'item_cnt_month'}, inplace=False)\n        \ntest['date_block_num'] = 34\ntest['item_cnt_month'] = 0\ndel test['ID']\n\ndf = sales.append(test)\ndf","fc68617d":"matrix = []\nfor num in df['date_block_num'].unique(): \n    tmp = df[df.date_block_num==num]\n    matrix.append(np.array(list(product([num], tmp.shop_id.unique(), tmp.item_id.unique())), dtype='int16'))\n    \n# Turn the grid into a dataframe\nmatrix = pd.DataFrame(np.vstack(matrix), columns=['date_block_num', 'shop_id', 'item_id'], dtype=np.int16)\n\n# Add the features from sales data to the matrix\nmatrix = matrix.merge(df, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)\n\n#Merge features from shops, items and item_categories:\nmatrix = matrix.merge(shops, how='left', on='shop_id')\nmatrix = matrix.merge(items[['item_id','item_category_id']], how='left', on='item_id')\nmatrix = matrix.merge(item_categories, how='left', on='item_category_id')\n\n# Add month\nmatrix['month'] = matrix.date_block_num%12\n# Clip counts\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0, 20)","bb1738a5":"\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix['month'] = matrix['month'].astype(np.int8)\nmatrix['item_cnt_month'] = matrix['item_cnt_month'].astype(np.int32)\nmatrix['shop_category'] = matrix['shop_category'].astype(np.int8)\nmatrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['category_type'] = matrix['category_type'].astype(np.int8)\nmatrix['category_subtype'] = matrix['category_subtype'].astype(np.int8)\nmatrix","194c2bad":"print('{0:.2f}'.format(matrix.memory_usage(index=False, deep=True).sum()\/(2**20)), 'MB')","753d8652":"def lag_feature(df, lags, col):\n    print(col)\n    for i in lags:\n        shifted = df[[\"date_block_num\", \"shop_id\", \"item_id\", col]].copy()\n        shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col+\"_lag_\"+str(i)]\n        shifted.date_block_num += i\n        df = df.merge(shifted, on=['date_block_num','shop_id','item_id'], how='left').fillna(0)\n    return df","330e8b1f":"matrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'item_cnt_month')","81750914":"gb = matrix.groupby(['shop_id', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_shop'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_shop')\nmatrix.drop('cnt_block_shop', axis=1, inplace=True)\ngb = matrix.groupby(['item_id', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_item'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_item')\nmatrix.drop('cnt_block_item', axis=1, inplace=True)\ngb = matrix.groupby(['category_type', 'date_block_num'],as_index=False)\\\n          .agg({'item_cnt_month':'sum'})\\\n          .rename(columns={'item_cnt_month':'cnt_block_category'}, inplace=False)\nmatrix = matrix.merge(gb, how='left', on=['category_type', 'date_block_num']).fillna(0)\nmatrix = lag_feature(matrix, [1, 2, 3, 4, 5, 12], 'cnt_block_category')\nmatrix.drop('cnt_block_category', axis=1, inplace=True)","cf571263":"matrix.to_csv('matrix.csv', index=False)\nmatrix = pd.read_csv('matrix.csv')\nmatrix","4e64caee":"from sklearn.preprocessing import StandardScaler\n\ndef standard_mean_enc(df, col):\n    mean_enc = df.groupby(col).agg({'item_cnt_month': 'mean'})\n    scaler = StandardScaler().fit(mean_enc)\n    return {v: k[0] for v, k in enumerate(scaler.transform(mean_enc))}\n\ncols_to_mean_encode = ['shop_category', 'shop_city', 'item_category_id', 'category_type', 'category_subtype']\n\nfor col in cols_to_mean_encode:\n    # Train on the train data\n    mean_enc = standard_mean_enc(matrix[matrix.date_block_num < 33].copy(), col) # X_train, y_train\n    # Apply to Train, Validation and Test\n    matrix[col] = matrix[col].map(mean_enc)\nmatrix","f885e067":"# Remove the 2013's sales data\nmatrix = matrix[matrix.date_block_num>=12] \nmatrix.reset_index(drop=True, inplace=True)\nmatrix","1ab40ed5":"X_train = matrix[matrix.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = matrix[matrix.date_block_num < 33]['item_cnt_month']\nX_val = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val =  matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)","c3247dbd":"X_train.drop('date_block_num', axis=1, inplace=True)\nX_val.drop('date_block_num', axis=1, inplace=True)\nX_test.drop('date_block_num', axis=1, inplace=True)","570f837d":"splits = []\nfor block in [25, 26, 27, 28, 29, 30]:\n    train_idxs = matrix[matrix.date_block_num < block].index.values\n    test_idxs = matrix[matrix.date_block_num == block].index.values\n    splits.append((train_idxs, test_idxs))\nsplits","809bf49b":"from sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\n\nhyper_params = {'max_depth': [3, 4, 5, 6, 7, 8, 9], \n                'gamma': [0, 0.5, 1, 1.5, 2, 5], \n                'subsample': [0.6, 0.7, 0.8, 0.9, 1], \n                'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1], \n                'learning_rate': [0.01, 0.1, 0.2, 0.3],\n                'max_bin' : [256, 512, 1024]\n               }\n\nxgbr = XGBRegressor(seed = 13, tree_method = \"hist\") \nclf = RandomizedSearchCV(estimator = xgbr, \n                   param_distributions = hyper_params,\n                   n_iter = 4, #\n                   scoring = 'neg_root_mean_squared_error',\n                   cv = splits,\n                   verbose=3)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters:\", clf.best_params_)\nprint(\"Lowest RMSE: \", -clf.best_score_)","8fa8f79a":"from sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nyhat_val_lr = lr.predict(X_val).clip(0, 20)\nprint('Validation RMSE:', mean_squared_error(y_val, yhat_val_lr, squared=False)) #Validation RMSE: 0.9645168655662141\nyhat_test_lr = lr.predict(X_test).clip(0, 20)","20901b2e":"from xgboost import XGBRegressor\n\nts = time.time()\n\nxgb = XGBRegressor(seed = 13, \n    tree_method = \"hist\", \n    subsample = 0.7,\n    max_depth = 9,\n    learning_rate = 0.3,\n    gamma = 5,\n    colsample_bytree = 0.8,\n      max_bin=512             \n    )\nxgb.fit(\n    X_train,y_train,\n    eval_metric=\"rmse\",\n    eval_set=[(X_train, y_train), (X_val, y_val)],\n    verbose=True,\n    early_stopping_rounds = 10\n    )\nprint('Training took: {0}s'.format(time.time()-ts))\nyhat_val_xgb = xgb.predict(X_val).clip(0, 20)\nprint('Valdation RMSE:', mean_squared_error(y_val, yhat_val_xgb, squared=False)) #Valdation RMSE: 0.9409594444278176\nyhat_test_xgb = xgb.predict(X_test).clip(0, 20)","94baff78":"import pickle\npickle.dump(xgb, open(\"xgboost.pickle.dat\", \"wb\"))\n#loaded_model = pickle.load(open(\"xgboost_base.pickle.dat\", \"rb\"))","f69f50ec":"from xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xgb, (10,14))","272a61c8":"y_train_meta = matrix[matrix.date_block_num.isin([25, 26, 27, 28, 29, 30])].item_cnt_month","84785368":"X_train_meta = [[],[]]\nfor block in [25, 26, 27, 28, 29, 30]:\n    print('Block:', block)\n    # X and y Train for blocks from 12 to block\n    X_train_block = matrix[matrix.date_block_num < block].drop(['date_block_num', 'item_cnt_month'], axis=1)\n    y_train_block = matrix[matrix.date_block_num < block].item_cnt_month\n    # X and y Test for block\n    X_val_block = matrix[matrix.date_block_num == block].drop(['date_block_num', 'item_cnt_month'], axis=1)\n    #y_test_block = matrix[matrix.date_block_num == block].item_cnt_month\n    \n    # Fit first model \n    print(' LR fitting ...')\n    lr.fit(X_train_block, y_train_block)\n    print(' LR fitting ... done')\n    # Append prediction results on X_val_block to X_train_meta (first column)\n    X_train_meta[0] += list(lr.predict(X_val_block).clip(0, 20))\n    \n    # Fit second model\n    print(' XGB fitting ...')\n    xgb.fit(\n        X_train_block, y_train_block,\n        eval_metric=\"rmse\",\n        eval_set=[(X_train_block, y_train_block)],\n        #eval_set=[(X_train_block, y_train_block), (X_val_block, y_test_block)],\n        verbose=0,\n        early_stopping_rounds = 10\n    )\n    print(' XGB fitting ... done')\n    # Append prediction results on X_val_block to X_train_meta (second column)\n    X_train_meta[1] += list(xgb.predict(X_val_block).clip(0, 20))\n# Turn list into dataframe\nX_train_meta = pd.DataFrame({'yhat_lr': X_train_meta[0], 'yhat_xgb': X_train_meta[1]})","6a56ca40":"plt.scatter(X_train_meta.yhat_lr, X_train_meta.yhat_xgb)\nplt.show()","9e42f056":"stacking = LinearRegression()\nstacking.fit(X_train_meta, y_train_meta)\n\n#Squared: If True returns MSE value, if False returns RMSE value.\nyhat_train_meta = stacking.predict(X_train_meta).clip(0, 20)\nprint('Meta Training RMSE:', mean_squared_error(y_train_meta, yhat_train_meta, squared=False))\n# Meta Training RMSE: 0.7959949995252207\n\nyhat_val_meta = stacking.predict(np.vstack((yhat_val_lr, yhat_val_xgb)).T).clip(0, 20)\nprint('Meta Validation RMSE:', mean_squared_error(y_val, yhat_val_meta, squared=False))\n# Meta Validation RMSE: 0.9313002364522425\n\nyhat_test_meta = stacking.predict(np.vstack((yhat_test_lr, yhat_test_xgb)).T).clip(0, 20)","7e97db17":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": yhat_test_meta\n})\nsubmission.to_csv('submission_stacking.csv', index=False)\n","1b383b8c":"submission","0f59ea61":"# EXTRACTING THE YEAR AND THE MONTH FROM THE DATE COLUMN","6c490657":"# Sum the number of sold items for each date_block_num ","2223ba44":"# Handling outliers","bb982ec3":"# PLOTING THE SALES PER MONTH FOR EACH YEAR","d254e18e":"# Add Sub_type and Item_Category","0c837532":"* ENSEMBLING","91f37d93":"# Creation of Feature Matrix","071d57d7":"# Encoding","02999a8c":"# Stacking","1f116238":"# pre-processing\n* fix the names of the shops","16f88c9a":"# LIBRARIES","bbd1ad9b":"# Spliting the data","4b79abb4":"* lag the the target item_cnt_month","20e123d2":"# TOP SOLD ITEMS","70fddf27":"# XGBOOST","bcf50729":"# Model Evaluation\n* Linear Regression","d717564d":"# Lagged Features","d273c749":"# TO PRINT THE TOP SELLING CATEGORIES","b20fdc9a":"Label mean Encoding","180f66ec":"# Feature Engeneering","6ba679e3":"# Feature Importance plot","b2f9ab82":"# OBJECTIVES \n\nUPON INITIAL INSPECTION OF OUR DATA WE CAN IMMEDIATELY ASK A FEW QUESTIONS THAT ARE OF IMPORTANCE.\n\n* What is the overall trend of the sales?\n* What are the top 10 products by sales?\n* What are the most selling products?\n* Which is the most preferred Ship Mode?\n* Which are the most profitable category and Sub-category?\n\n","791c81d4":"# Set columns types to control the matrix' size","7b6ff543":"# Submission","ac5b78bb":"Removing date_block_num from our data","56ad907c":"# Deserialize and serialize the XGBOOST model ","2956fc24":"# Parameter Tuning"}}