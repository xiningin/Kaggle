{"cell_type":{"1b412e5e":"code","cea258a7":"code","67246f6f":"code","0e03d9c9":"code","629c9543":"code","0c036694":"code","2b69b4d9":"code","77ebdc02":"code","2016e22c":"code","f709aece":"code","166080b4":"code","1f54fe85":"code","01973273":"code","e0ff22f3":"code","d503b4bc":"code","68cb55b3":"code","aa11cedb":"code","52c6fe69":"code","12955a37":"code","644dcb25":"code","59a582c6":"code","958b703c":"code","4a524776":"code","a8d0c462":"code","6a09efbc":"code","770d65b5":"markdown","ce9f60ac":"markdown","7ad60b6d":"markdown","720c0928":"markdown","847f9754":"markdown","0cbab6f5":"markdown","2a035689":"markdown","363af686":"markdown","b6b407c7":"markdown","78643044":"markdown","a5b369f9":"markdown","bf226c45":"markdown","3429c79e":"markdown","12adfad7":"markdown","142fe390":"markdown","4dbbd2f2":"markdown","1fe78e57":"markdown","4821bdd8":"markdown","85eabede":"markdown","b86539ba":"markdown","8421528f":"markdown","1570b1b3":"markdown","e1375a8a":"markdown","002a2436":"markdown","4847b744":"markdown","37fd660e":"markdown","ef157d14":"markdown","a0bd305d":"markdown","088bed4f":"markdown","82d7574d":"markdown","596bfb45":"markdown","32b2267a":"markdown"},"source":{"1b412e5e":"! pip install datasets transformers\n! pip install cloud-tpu-client==0.10 https:\/\/storage.googleapis.com\/tpu-pytorch\/wheels\/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n! pip install git+https:\/\/github.com\/huggingface\/accelerate","cea258a7":"import torch\nfrom torch.utils.data import DataLoader\n\nfrom accelerate import Accelerator, DistributedType\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    AdamW,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    set_seed,\n)\n\nfrom tqdm.auto import tqdm\n\nimport datasets\nimport transformers","67246f6f":"model_checkpoint = \"bert-base-cased\"","0e03d9c9":"raw_datasets = load_dataset(\"glue\", \"mrpc\")","629c9543":"raw_datasets","0c036694":"raw_datasets[\"train\"][0]","2b69b4d9":"import datasets\nimport random\nimport pandas as pd\nfrom IPython.display import display, HTML\n\ndef show_random_elements(dataset, num_examples=10):\n    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n    picks = []\n    for _ in range(num_examples):\n        pick = random.randint(0, len(dataset)-1)\n        while pick in picks:\n            pick = random.randint(0, len(dataset)-1)\n        picks.append(pick)\n    \n    df = pd.DataFrame(dataset[picks])\n    for column, typ in dataset.features.items():\n        if isinstance(typ, datasets.ClassLabel):\n            df[column] = df[column].transform(lambda i: typ.names[i])\n    display(HTML(df.to_html()))\n\nshow_random_elements(raw_datasets[\"train\"])","77ebdc02":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","2016e22c":"tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")","f709aece":"def tokenize_function(examples):\n    outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n    return outputs","166080b4":"tokenize_function(raw_datasets['train'][:5])","1f54fe85":"tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"idx\", \"sentence1\", \"sentence2\"])","01973273":"tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")","e0ff22f3":"tokenized_datasets[\"train\"].features","d503b4bc":"tokenized_datasets.set_format(\"torch\")","68cb55b3":"from transformers import AutoModelForSequenceClassification\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)","aa11cedb":"def create_dataloaders(train_batch_size=8, eval_batch_size=32):\n    train_dataloader = DataLoader(\n        tokenized_datasets[\"train\"], shuffle=True, batch_size=train_batch_size\n    )\n    eval_dataloader = DataLoader(\n        tokenized_datasets[\"validation\"], shuffle=False, batch_size=eval_batch_size\n    )\n    return train_dataloader, eval_dataloader","52c6fe69":"train_dataloader, eval_dataloader = create_dataloaders()","12955a37":"for batch in train_dataloader:\n    print({k: v.shape for k, v in batch.items()})\n    outputs = model(**batch)\n    break","644dcb25":"outputs","59a582c6":"metric = load_metric(\"glue\", \"mrpc\")","958b703c":"predictions = outputs.logits.detach().argmax(dim=-1)\nmetric.compute(predictions=predictions, references=batch[\"labels\"])","4a524776":"hyperparameters = {\n    \"learning_rate\": 2e-5,\n    \"num_epochs\": 3,\n    \"train_batch_size\": 8, # Actual batch size will this x 8\n    \"eval_batch_size\": 32, # Actual batch size will this x 8\n    \"seed\": 42,\n}","a8d0c462":"def training_function():\n    # Initialize accelerator\n    accelerator = Accelerator()\n\n    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n    # to INFO for the main process only.\n    if accelerator.is_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    train_dataloader, eval_dataloader = create_dataloaders(\n        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n    )\n    # The seed need to be set before we instantiate the model, as it will determine the random head.\n    set_seed(hyperparameters[\"seed\"])\n\n    # Instantiate the model, let Accelerate handle the device placement.\n    model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n\n    # Instantiate optimizer\n    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"])\n\n    # Prepare everything\n    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n    # prepare method.\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    num_epochs = hyperparameters[\"num_epochs\"]\n    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n    # may change its length.\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=100,\n        num_training_steps=len(train_dataloader) * num_epochs,\n    )\n\n    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n    # process to avoid having 8 progress bars.\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n    # Now we train the model\n    for epoch in range(num_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            \n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n\n        model.eval()\n        all_predictions = []\n        all_labels = []\n\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n\n            # We gather predictions and labels from the 8 TPUs to have them all.\n            all_predictions.append(accelerator.gather(predictions))\n            all_labels.append(accelerator.gather(batch[\"labels\"]))\n\n        # Concatenate all predictions and labels.\n        # The last thing we need to do is to truncate the predictions and labels we concatenated\n        # together as the prepared evaluation dataloader has a little bit more elements to make\n        # batches of the same size on each process.\n        all_predictions = torch.cat(all_predictions)[:len(tokenized_datasets[\"validation\"])]\n        all_labels = torch.cat(all_labels)[:len(tokenized_datasets[\"validation\"])]\n\n        eval_metric = metric.compute(predictions=all_predictions, references=all_labels)\n\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)","6a09efbc":"from accelerate import notebook_launcher\n\nnotebook_launcher(training_function)","770d65b5":"Unsurpringly, our model with its random head does not perform well, which is why we need to fine-tune it!","ce9f60ac":"Before we can browse the rest of the notebook, we need to install the dependencies: this example uses `datasets` and `transformers`. To use TPUs on colab, we need to install `torch_xla` and the last line install `accelerate` from source since we the features we are using are very recent and not released yet.","7ad60b6d":"To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.","720c0928":"Before we can feed those texts to our model, we need to preprocess them. This is done by a \ud83e\udd17 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n\nTo do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n\nwe get a tokenizer that corresponds to the model architecture we want to use,\nwe download the vocabulary used when pretraining this specific checkpoint.\nThat vocabulary will be cached, so it's not downloaded again the next time we run the cell.","847f9754":"The last piece we will need for the model evaluation is the metric. The `datasets` library provides a function `load_metric` that allows us to easily create a `datasets.Metric` object we can use.","0cbab6f5":"To load the dataset, we use the `load_dataset` function from \ud83e\udd17 Datasets. It will download and cache it (so the download won't happen if we restart the notebook).","2a035689":"Let's have a look at our train and evaluation dataloaders to check a batch can go through the model.","363af686":"To use this object on some predictions we call the `compute` methode to get our metric results:","b6b407c7":"The output of our model is a `SequenceClassifierOutput`, with the `loss` (since we provided labels) and `logits` (of shape 8, our batch size, by 2, the number of labels).","78643044":"The model we will be using is a `BertModelForSequenceClassification`. We can check its signature in the [Transformers documentation](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertForSequenceClassification) and all seems to be right! The last step is to set our datasets in the `\"torch\"` format, so that each item in it is now a dictionary with tensor values.","a5b369f9":"We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. We also need all of our samples to have the same length (we will train on TPU and they need fixed shapes so we won't pad to the maximum length of a batch) which is done with `padding=True`. The `max_length` argument is used both for the truncation and padding (short inputs are padded to that length and long inputs are truncated to it).\n","bf226c45":"We are now ready to fine-tune this model on our dataset. As mentioned before, everything related to training needs to be in one big training function that will be executed on each TPU core, thanks to our `notebook_launcher`.\n\nIt will use this dictionary of hyperparameters, so tweak anything you like in here!","3429c79e":"By default (unless you pass `use_fast=Fast` to the call above) it will use one of the fast tokenizers (backed by Rust) from the \ud83e\udd17 Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument.\n\nYou can directly call this tokenizer on one sentence or a pair of sentences:","12adfad7":"To apply this function on all the sentences (or pairs of sentences) in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command.\n","142fe390":"This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:","4dbbd2f2":"And we're ready for launch! It's super easy with the `notebook_launcher` from the Accelerate library.","1fe78e57":"The next two sections explain how we load and prepare our data for our model, If you are only interested on seeing how \ud83e\udd17 Accelerate works, feel free to skip them (but make sure to execute all cells!)","4821bdd8":"## A first look at the model","85eabede":"Here are all the imports we will need for this notebook.","b86539ba":"## Load the data","8421528f":"Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https:\/\/huggingface.co\/transformers\/preprocessing.html) if you're interested.\n","1570b1b3":"This notebook can run with any model checkpoint on the [model hub](https:\/\/huggingface.co\/models) that has a version with a classification head. Here we select [`bert-base-cased`](https:\/\/huggingface.co\/bert-base-cased).","e1375a8a":"The `raw_datasets` object itself is [`DatasetDict`](https:\/\/huggingface.co\/docs\/datasets\/package_reference\/main_classes.html#datasetdict), which contains one key for the training, validation and test set (with more keys for the mismatched validation and test set in the special case of `mnli`).\n","002a2436":"Even better, the results are automatically cached by the \ud83e\udd17 Datasets library to avoid spending time on this step the next time you run your notebook. The \ud83e\udd17 Datasets library is normally smart enough to detect when the function you pass to map has changed (and thus requires to not use the cache data). For instance, it will properly detect if you change the task in the first cell and rerun the notebook. \ud83e\udd17 Datasets warns you when it uses cached files, you can pass `load_from_cache_file=False` in the call to `map` to not use the cached files and force the preprocessing to be applied again.\n\nNote that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently.\n\nLastly, we remove the columns that our model will not use. We also need to rename the `label` column to `labels` as this is what our model will expect.","4847b744":"## Preprocess the data","37fd660e":"We just loop through one batch. Since our datasets elements are dictionaries of tensors, it's the same for our batch and we can have a quick look at all the shapes. Note that this cell takes a bit of time to execute since we run a batch of our data through the model on the CPU (if you changed the checkpoint to a bigger model, it might take too much time so comment it out).","ef157d14":"The most important thing to remember for training on TPUs is that your `accelerator` object as to be defined inside the training function. If you define it in another cell that gets executed before the final launch (for debugging), you will need to restart your notebook as the line `accelerator = Accelerator()` needs to be executed for the first time inside the training function spwaned on each TPU core.\n\nThis is because that line will look for a TPU device, and if you set it outside of the distributed training launched by `notebook_launcher`, it will perform setup that cannot be undone in your runtime and you will only have access to one TPU core until you restart the notebook.\n\nSince we can't explore each piece in separate cells, comments have been left in the code. This is all pretty standard and you will notice how little the code changes from a regular training loop! The main lines added are:\n\n- `accelerator = Accelerator()` to initalize the distributed setup,\n- sending all objects to `accelerator.prepare`,\n- replace `loss.backward()` with `accelerator.backward(loss)`,\n- use `accelerator.gather` to gather all predictions and labels before storing them in our list of predictions\/labels,\n- truncate predictions and labels as the prepared evaluation dataloader has a few more samples to make batches of the same size on each process.\n\nThe first three are for distributed training, the last two for distributed evaluation. If you don't care about distributed evaluation, you can also just replace that part by your standard evaluation loop launched on the main process only.\n\nOther changes (which are purely cosmetic to make the output of the training readable) are:\n\n- some logging behavior behind a `if accelerator.is_main_process:`,\n- disable the progress bar if `accelerator.is_main_process` is `False`,\n- use `accelerator.print` instead of `print`.","a0bd305d":"Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is 2 here):","088bed4f":"To double-check we only have columns that are accepted as arguments for the model we will instantiate, we can look at them here.","82d7574d":"## Fine-tuning the model","596bfb45":"To access an actual element, you need to select a split first, then give an index:","32b2267a":"The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\n\nNote that we will are only creating the model here to look at it and debug problems. We will create the model we will train inside our training function: to train on TPU in colab, we have to create a big training function that will be executed on each code of the TPU. It's fine to do use the datasets defined before (they will be copied to each TPU core) but the model itself will need to be re-instantiated and placed on each device for it to work.\n\nNow to get the data we need to define our training and evaluation dataloaders. Again, we only create them here for debugging purposes, they will be re-instantiated in our training function, which is why we define a function that builds them."}}