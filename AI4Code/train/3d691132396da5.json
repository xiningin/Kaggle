{"cell_type":{"7bc2f4c3":"code","37748aa3":"code","55cc24ea":"code","c7fcc669":"code","8c87c209":"code","7c69edbb":"code","e77791c1":"code","90a5f31b":"code","1a2c42c7":"code","3cbc0f35":"code","80ec7079":"code","4a163f2b":"code","c7695f2e":"code","499d772e":"code","04a69287":"code","4159b6b5":"code","7312d4ca":"code","94f03a6f":"code","e6948a00":"code","8a6ce755":"code","05add91b":"code","f42ed144":"code","1cccabc1":"code","e4b2ad25":"code","033b6c5d":"code","1bc70d4c":"code","11a2ec1d":"code","97bf0706":"code","48b29e2c":"code","24b873ee":"code","d725c145":"code","a98f94e9":"code","d134ac21":"code","ee9fcf6a":"code","4fb47944":"code","80f44d81":"code","8e064dc1":"code","62e9f039":"code","ef8462ab":"code","55846113":"code","a646088f":"code","01a8b08f":"code","cb04ec6e":"code","d4e4f9f6":"code","c4fbe7de":"code","5d8ded55":"code","8ed45421":"code","73b80e88":"code","afdf0936":"code","019bb0d2":"code","6b80ab77":"code","86de1484":"code","95ed0733":"code","a51671ab":"code","c1f240cb":"code","c61b8217":"code","82069072":"code","6867a7ce":"code","4deac685":"code","17bf5002":"code","720a7c9c":"code","e2d16bfc":"code","a64f6902":"code","fa313fa3":"code","8ce38e8d":"code","75012b2e":"code","a7d10583":"code","cfb1d762":"code","f4b94a31":"code","d6e26d08":"code","a703eca0":"code","a91061e4":"code","2a044f45":"code","efffedec":"code","5fca795e":"code","190cd61b":"code","085fd207":"code","15904efe":"code","43165a1b":"code","31e4145c":"markdown","c679372f":"markdown","fdba24f0":"markdown","55339fb7":"markdown","011b7957":"markdown","2ed585f9":"markdown","875ec615":"markdown"},"source":{"7bc2f4c3":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n \nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\n \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n \nimport warnings\nwarnings.filterwarnings('ignore')","37748aa3":"data_dir = '..\/input\/lish-moa\/'\nos.listdir(data_dir)","55cc24ea":"!pip install \/kaggle\/input\/iterative-stratification\/iterative-stratification-master\/\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","c7fcc669":"train_features = pd.read_csv(data_dir + 'train_features.csv')\ntrain_targets_scored = pd.read_csv(data_dir + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(data_dir + 'train_targets_nonscored.csv')\ntrain_drug = pd.read_csv(data_dir + 'train_drug.csv')\ntest_features = pd.read_csv(data_dir + 'test_features.csv')\nsample_submission = pd.read_csv(data_dir + 'sample_submission.csv')\n\nprint('train_features: {}'.format(train_features.shape))\nprint('train_targets_scored: {}'.format(train_targets_scored.shape))\nprint('train_targets_nonscored: {}'.format(train_targets_nonscored.shape))\nprint('train_drug: {}'.format(train_drug.shape))\nprint('test_features: {}'.format(test_features.shape))\nprint('sample_submission: {}'.format(sample_submission.shape))","8c87c209":"train_features2=train_features.copy()\ntest_features2=test_features.copy()","7c69edbb":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","e77791c1":"qt = QuantileTransformer(n_quantiles=100,random_state=42,output_distribution='normal')\ndata = pd.concat([pd.DataFrame(train_features[GENES+CELLS]), pd.DataFrame(test_features[GENES+CELLS])])\ndata2 = qt.fit_transform(data[GENES+CELLS])\ntrain_features[GENES+CELLS] = pd.DataFrame(data2[:train_features.shape[0]])\ntest_features[GENES+CELLS] = pd.DataFrame(data2[-test_features.shape[0]:])","90a5f31b":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n\n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    \nseed_everything(42)","1a2c42c7":"n_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; \ntest2 = data2[-test_features.shape[0]:]\n\ntrain_gpca = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest_gpca = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train_gpca), axis=1)\ntest_features = pd.concat((test_features, test_gpca), axis=1)","3cbc0f35":"n_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain_cpca = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest_cpca = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train_cpca), axis=1)\ntest_features = pd.concat((test_features, test_cpca), axis=1)","80ec7079":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.85)  #<-- Update\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n","4a163f2b":"from sklearn.cluster import KMeans\ndef fe_cluster(train, test, n_clusters_g = 22, n_clusters_c = 4, SEED = 42):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    def create_cluster(train, test, features, kind = 'g', n_clusters = n_clusters_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_{kind}'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_{kind}'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_{kind}'])\n        test = pd.get_dummies(test, columns = [f'clusters_{kind}'])\n        return train, test\n    \n    train, test = create_cluster(train, test, features_g, kind = 'g', n_clusters = n_clusters_g)\n    train, test = create_cluster(train, test, features_c, kind = 'c', n_clusters = n_clusters_c)\n    return train, test\n\ntrain_features2 ,test_features2=fe_cluster(train_features2,test_features2)","c7695f2e":"train_pca=pd.concat((train_gpca,train_cpca),axis=1)\ntest_pca=pd.concat((test_gpca,test_cpca),axis=1)","499d772e":"def fe_cluster_pca(train, test,n_clusters=5,SEED = 42):\n        data=pd.concat([train,test],axis=0)\n        kmeans = KMeans(n_clusters = n_clusters, random_state = SEED).fit(data)\n        train[f'clusters_pca'] = kmeans.labels_[:train.shape[0]]\n        test[f'clusters_pca'] = kmeans.labels_[train.shape[0]:]\n        train = pd.get_dummies(train, columns = [f'clusters_pca'])\n        test = pd.get_dummies(test, columns = [f'clusters_pca'])\n        return train, test\ntrain_cluster_pca ,test_cluster_pca = fe_cluster_pca(train_pca,test_pca)","04a69287":"train_cluster_pca = train_cluster_pca.iloc[:,650:]\ntest_cluster_pca = test_cluster_pca.iloc[:,650:]","4159b6b5":"train_features_cluster=train_features2.iloc[:,876:]\ntest_features_cluster=test_features2.iloc[:,876:]","7312d4ca":"gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","94f03a6f":"def fe_stats(train, test):\n    \n    features_g = GENES\n    features_c = CELLS\n    \n    for df in train, test:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n        df['c52_c42'] = df['c-52'] * df['c-42']\n        df['c13_c73'] = df['c-13'] * df['c-73']\n        df['c26_c13'] = df['c-23'] * df['c-13']\n        df['c33_c6'] = df['c-33'] * df['c-6']\n        df['c11_c55'] = df['c-11'] * df['c-55']\n        df['c38_c63'] = df['c-38'] * df['c-63']\n        df['c38_c94'] = df['c-38'] * df['c-94']\n        df['c13_c94'] = df['c-13'] * df['c-94']\n        df['c4_c52'] = df['c-4'] * df['c-52']\n        df['c4_c42'] = df['c-4'] * df['c-42']\n        df['c13_c38'] = df['c-13'] * df['c-38']\n        df['c55_c2'] = df['c-55'] * df['c-2']\n        df['c55_c4'] = df['c-55'] * df['c-4']\n        df['c4_c13'] = df['c-4'] * df['c-13']\n        df['c82_c42'] = df['c-82'] * df['c-42']\n        df['c66_c42'] = df['c-66'] * df['c-42']\n        df['c6_c38'] = df['c-6'] * df['c-38']\n        df['c2_c13'] = df['c-2'] * df['c-13']\n        df['c62_c42'] = df['c-62'] * df['c-42']\n        df['c90_c55'] = df['c-90'] * df['c-55']\n        \n        \n        for feature in features_c:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n        for feature in gsquarecols:\n            df[f'{feature}_squared'] = df[feature] ** 2        \n        \n    return train, test\n\ntrain_features2,test_features2=fe_stats(train_features2,test_features2)","e6948a00":"train_features_stats=train_features2.iloc[:,902:]\ntest_features_stats=test_features2.iloc[:,902:]\n","8a6ce755":"train_features = pd.concat((train_features, train_features_cluster,train_cluster_pca,train_features_stats), axis=1)\ntest_features = pd.concat((test_features, test_features_cluster,test_cluster_pca,test_features_stats), axis=1)","05add91b":"train_drug.head()","f42ed144":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train.merge(train_targets_nonscored, on='sig_id')\ntrain = train.merge(train_drug, on='sig_id')\ntrain = train[train['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type'] != 'ctl_vehicle'].reset_index(drop=True)","1cccabc1":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","e4b2ad25":"train.head()","033b6c5d":"target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)\n\nprint('num_targets: {}'.format(num_targets))\nprint('num_aux_targets: {}'.format(num_aux_targets))\nprint('num_all_targets: {}'.format(num_all_targets))","1bc70d4c":"print(train.shape)\nprint(test.shape)\nprint(sample_submission.shape)","11a2ec1d":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n        \n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n\n        return dct","97bf0706":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    return final_loss\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n\n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    return preds","48b29e2c":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n            \n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","24b873ee":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets):\n        super(Model, self).__init__()\n        self.hidden_size = [1500, 1250, 1000, 750]\n        self.dropout_value = [0.5, 0.35, 0.3, 0.25]\n\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dense1 = nn.Linear(num_features, self.hidden_size[0])\n        \n        self.batch_norm2 = nn.BatchNorm1d(self.hidden_size[0])\n        self.dropout2 = nn.Dropout(self.dropout_value[0])\n        self.dense2 = nn.Linear(self.hidden_size[0], self.hidden_size[1])\n\n        self.batch_norm3 = nn.BatchNorm1d(self.hidden_size[1])\n        self.dropout3 = nn.Dropout(self.dropout_value[1])\n        self.dense3 = nn.Linear(self.hidden_size[1], self.hidden_size[2])\n\n        self.batch_norm4 = nn.BatchNorm1d(self.hidden_size[2])\n        self.dropout4 = nn.Dropout(self.dropout_value[2])\n        self.dense4 = nn.Linear(self.hidden_size[2], self.hidden_size[3])\n\n        self.batch_norm5 = nn.BatchNorm1d(self.hidden_size[3])\n        self.dropout5 = nn.Dropout(self.dropout_value[3])\n        self.dense5 = nn.utils.weight_norm(nn.Linear(self.hidden_size[3], num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = F.leaky_relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.dense2(x))\n\n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = F.leaky_relu(self.dense3(x))\n\n        x = self.batch_norm4(x)\n        x = self.dropout4(x)\n        x = F.leaky_relu(self.dense4(x))\n\n        x = self.batch_norm5(x)\n        x = self.dropout5(x)\n        x = self.dense5(x)\n        return x\n    \nclass LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n\n        with torch.no_grad():\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n            \n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","d725c145":"class FineTuneScheduler:\n    def __init__(self, epochs):\n        self.epochs = epochs\n        self.epochs_per_step = 0\n        self.frozen_layers = []\n\n    def copy_without_top(self, model, num_features, num_targets, num_targets_new):\n        self.frozen_layers = []\n\n        model_new = Model(num_features, num_targets)\n        model_new.load_state_dict(model.state_dict())\n\n        # Freeze all weights\n        for name, param in model_new.named_parameters():\n            layer_index = name.split('.')[0][-1]\n\n            if layer_index == 5:\n                continue\n\n            param.requires_grad = False\n\n            # Save frozen layer names\n            if layer_index not in self.frozen_layers:\n                self.frozen_layers.append(layer_index)\n\n        self.epochs_per_step = self.epochs \/\/ len(self.frozen_layers)\n\n        # Replace the top layers with another ones\n        model_new.batch_norm5 = nn.BatchNorm1d(model_new.hidden_size[3])\n        model_new.dropout5 = nn.Dropout(model_new.dropout_value[3])\n        model_new.dense5 = nn.utils.weight_norm(nn.Linear(model_new.hidden_size[-1], num_targets_new))\n        model_new.to(DEVICE)\n        return model_new\n\n    def step(self, epoch, model):\n        if len(self.frozen_layers) == 0:\n            return\n\n        if epoch % self.epochs_per_step == 0:\n            last_frozen_index = self.frozen_layers[-1]\n            \n            # Unfreeze parameters of the last frozen layer\n            for name, param in model.named_parameters():\n                layer_index = name.split('.')[0][-1]\n\n                if layer_index == last_frozen_index:\n                    param.requires_grad = True\n\n            del self.frozen_layers[-1]  # Remove the last layer as unfrozen","a98f94e9":"def process_data(data):\n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n    return data","d134ac21":"feature_cols = [c for c in process_data(train).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold', 'sig_id', 'drug_id']]\nnum_features = len(feature_cols)\nnum_features","ee9fcf6a":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 24\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","4fb47944":"# Show model architecture\nmodel = Model(num_features, num_all_targets)\nmodel","80f44d81":"from sklearn.model_selection import KFold\n\ndef make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH):\n    vc = train.drug_id.value_counts()\n    vc1 = vc.loc[vc <= DRUG_THRESH].index.sort_values()\n    vc2 = vc.loc[vc > DRUG_THRESH].index.sort_values()\n\n    for seed_id in range(SEEDS):\n        kfold_col = 'kfold_{}'.format(seed_id)\n        \n        # STRATIFY DRUGS 18X OR LESS\n        dct1 = {}\n        dct2 = {}\n\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.groupby('drug_id')[target_cols].mean().loc[vc1]\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.index[idxV].values}\n            dct1.update(dd)\n\n        # STRATIFY DRUGS MORE THAN 18X\n        skf = MultilabelStratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=seed_id)\n        tmp = train.loc[train.drug_id.isin(vc2)].reset_index(drop=True)\n\n        for fold,(idxT, idxV) in enumerate(skf.split(tmp, tmp[target_cols])):\n            dd = {k: fold for k in tmp.sig_id[idxV].values}\n            dct2.update(dd)\n\n        # ASSIGN FOLDS\n        train[kfold_col] = train.drug_id.map(dct1)\n        train.loc[train[kfold_col].isna(), kfold_col] = train.loc[train[kfold_col].isna(), 'sig_id'].map(dct2)\n        train[kfold_col] = train[kfold_col].astype('int8')\n        \n    return train\n\nSEEDS = 7\nNFOLDS = 7\nDRUG_THRESH = 18\n\ntrain = make_cv_folds(train, SEEDS, NFOLDS, DRUG_THRESH)\ntrain.head()","8e064dc1":"def run_training(fold_id, seed_id):\n    seed_everything(seed_id)\n    \n    train_ = process_data(train)\n    test_ = process_data(test)\n    \n    kfold_col = f'kfold_{seed_id}'\n    trn_idx = train_[train_[kfold_col] != fold_id].index\n    val_idx = train_[train_[kfold_col] == fold_id].index\n    \n    train_df = train_[train_[kfold_col] != fold_id].reset_index(drop=True)\n    valid_df = train_[train_[kfold_col] == fold_id].reset_index(drop=True)\n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):\n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed_id}, FOLD: {fold_id}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold_id}_.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold_id}_.pth\"))\n    pretrained_model.to(DEVICE)\n\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, num_features, num_all_targets, num_targets)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n\n    # Load the fine-tuned model with the best loss\n    model = Model(num_features, num_targets)\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold_id}_.pth\"))\n    model.to(DEVICE)\n\n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    predictions = np.zeros((len(test_), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n    return oof, predictions","62e9f039":"def run_k_fold(NFOLDS, seed_id):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold_id in range(NFOLDS):\n        oof_, pred_ = run_training(fold_id, seed_id)\n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","ef8462ab":"from time import time\n\n# Averaging on multiple SEEDS\nSEED = [0, 1, 2, 3, 4, 5, 6]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\ntime_begin = time()\n\nfor seed_id in SEED:\n    oof_, predictions_ = run_k_fold(NFOLDS, seed_id)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntime_diff = time() - time_begin\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions","55846113":"from datetime import timedelta\nstr(timedelta(seconds=time_diff))","a646088f":"train_targets_scored.head()","01a8b08f":"len(target_cols)","cb04ec6e":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\n\nfor i in range(len(target_cols)):\n    score += log_loss(y_true[:, i], y_pred[:, i])\n\nprint(\"CV log_loss: \", score \/ y_pred.shape[1])","d4e4f9f6":"sub1 = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)","c4fbe7de":"! pip install ..\/input\/pytorch16gpu\/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl","5d8ded55":"!pip install ..\/input\/iterative-stratification-tabnet\/iterative_stratification-0.1.6-py3-none-any.whl","8ed45421":"import sys\nsys.path.insert(0, \"..\/input\/tabnetdevelop\/tabnet-develop\")","73b80e88":"import pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import RobustScaler\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport os\nfrom sklearn.preprocessing import QuantileTransformer\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","afdf0936":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\nGENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n    \ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n\n\nfrom sklearn.decomposition import PCA\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\n\n\nn_comp = 50  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(0.8)  #<-- Update\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features.shape","019bb0d2":"def get_ratio_labels(df):\n    columns = list(df.columns)\n    columns.pop(0)\n    ratios = []\n    toremove = []\n    for c in columns:\n        counts = df[c].value_counts()\n        if len(counts) != 1:\n            ratios.append(counts[0]\/counts[1])\n        else:\n            toremove.append(c)\n    print(f\"remove {len(toremove)} columns\")\n    \n    for t in toremove:\n        columns.remove(t)\n    return columns, np.array(ratios).astype(np.int32)\n\ncolumns, ratios = get_ratio_labels(train_targets_scored)\ncolumns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)","6b80ab77":"remove_vehicle=True\ndef transform_data(train, test, col, normalize=True, removed_vehicle=False):\n    \"\"\"\n        the first 3 columns represents categories, the others numericals features\n    \"\"\"\n    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n               \"cp_time\":{48:0, 72:1, 24:2},\n               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n    \n    if removed_vehicle:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n    else:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n    \n    max_ = 10.\n    min_ = -10.\n   \n    if removed_vehicle:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    else:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    if normalize:\n        numerical_tr = (numerical_tr-min_)\/(max_ - min_)\n        numerical_test = (numerical_test-min_)\/(max_ - min_)\n    return categories_tr, categories_test, numerical_tr, numerical_test\ncol_features = list(train_features.columns)[1:]\ncat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle)\ntargets_tr = train_targets_scored[columns].values.astype(np.float32)\ntargets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)","86de1484":"def inference_fn(model, X ,verbose=True):\n    with torch.no_grad():\n        y_preds = model.predict( X )\n        y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n    return y_preds","95ed0733":"def log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()\n","a51671ab":"def log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\n        ","c1f240cb":"def check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True","c61b8217":"def auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()","82069072":"import torch\nimport numpy as np\nfrom scipy.sparse import csc_matrix\nimport time\nfrom abc import abstractmethod\nfrom pytorch_tabnet import tab_network\nfrom pytorch_tabnet.multiclass_utils import unique_labels\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\nfrom torch.nn.utils import clip_grad_norm_\nfrom pytorch_tabnet.utils import (PredictDataset,\n                                  create_dataloaders,\n                                  create_explain_matrix)\nfrom sklearn.base import BaseEstimator\nfrom torch.utils.data import DataLoader\nfrom copy import deepcopy\nimport io\nimport json\nfrom pathlib import Path\nimport shutil\nimport zipfile\n\nclass TabModel(BaseEstimator):\n    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n                 lambda_sparse=1e-3, seed=0,\n                 clip_value=1, verbose=1,\n                 optimizer_fn=torch.optim.Adam,\n                 optimizer_params=dict(lr=2e-2),\n                 scheduler_params=None, scheduler_fn=None,\n                 mask_type=\"sparsemax\",\n                 input_dim=None, output_dim=None,\n                 device_name='auto'):\n        \"\"\" Class for TabNet model\n        Parameters\n        ----------\n            device_name: str\n                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n        \"\"\"\n\n        self.n_d = n_d\n        self.n_a = n_a\n        self.n_steps = n_steps\n        self.gamma = gamma\n        self.cat_idxs = cat_idxs\n        self.cat_dims = cat_dims\n        self.cat_emb_dim = cat_emb_dim\n        self.n_independent = n_independent\n        self.n_shared = n_shared\n        self.epsilon = epsilon\n        self.momentum = momentum\n        self.lambda_sparse = lambda_sparse\n        self.clip_value = clip_value\n        self.verbose = verbose\n        self.optimizer_fn = optimizer_fn\n        self.optimizer_params = optimizer_params\n        self.device_name = device_name\n        self.scheduler_params = scheduler_params\n        self.scheduler_fn = scheduler_fn\n        self.mask_type = mask_type\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n        self.batch_size = 1024\n\n        self.seed = seed\n        torch.manual_seed(self.seed)\n        # Defining device\n        if device_name == 'auto':\n            if torch.cuda.is_available():\n                device_name = 'cuda'\n            else:\n                device_name = 'cpu'\n        self.device = torch.device(device_name)\n        print(f\"Device used : {self.device}\")\n\n    @abstractmethod\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n                          weights, batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        raise NotImplementedError('users must define construct_loaders to use this base class')\n\n    def init_network(\n                     self,\n                     input_dim,\n                     output_dim,\n                     n_d,\n                     n_a,\n                     n_steps,\n                     gamma,\n                     cat_idxs,\n                     cat_dims,\n                     cat_emb_dim,\n                     n_independent,\n                     n_shared,\n                     epsilon,\n                     virtual_batch_size,\n                     momentum,\n                     device_name,\n                     mask_type,\n                     ):\n        self.network = tab_network.TabNet(\n            input_dim,\n            output_dim,\n            n_d=n_d,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=cat_emb_dim,\n            n_independent=n_independent,\n            n_shared=n_shared,\n            epsilon=epsilon,\n            virtual_batch_size=virtual_batch_size,\n            momentum=momentum,\n            device_name=device_name,\n            mask_type=mask_type).to(self.device)\n\n        self.reducing_matrix = create_explain_matrix(\n            self.network.input_dim,\n            self.network.cat_emb_dim,\n            self.network.cat_idxs,\n            self.network.post_embed_dim)\n\n    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n            weights=0, max_epochs=100, patience=10, batch_size=1024,\n            virtual_batch_size=128, num_workers=0, drop_last=False):\n        \"\"\"Train a neural network stored in self.network\n        Using train_dataloader for training data and\n        valid_dataloader for validation.\n        Parameters\n        ----------\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            X_train: np.ndarray\n                Train set\n            y_train : np.array\n                Train targets\n            weights : bool or dictionnary\n                0 for no balancing\n                1 for automated balancing\n                dict for custom weights per class\n            max_epochs : int\n                Maximum number of epochs during training\n            patience : int\n                Number of consecutive non improving epoch before early stopping\n            batch_size : int\n                Training batch size\n            virtual_batch_size : int\n                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n            num_workers : int\n                Number of workers used in torch.utils.data.DataLoader\n            drop_last : bool\n                Whether to drop last batch during training\n        \"\"\"\n        # update model name\n\n        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n                               weights, max_epochs, patience, batch_size,\n                               virtual_batch_size, num_workers, drop_last)\n\n        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n                                                                    y_train,\n                                                                    X_valid,\n                                                                    y_valid,\n                                                                    self.updated_weights,\n                                                                    self.batch_size,\n                                                                    self.num_workers,\n                                                                    self.drop_last)\n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=self.virtual_batch_size,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n\n        self.optimizer = self.optimizer_fn(self.network.parameters(),\n                                           **self.optimizer_params)\n\n        if self.scheduler_fn:\n            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n        else:\n            self.scheduler = None\n\n        self.losses_train = []\n        self.losses_valid = []\n        self.learning_rates = []\n        self.metrics_train = []\n        self.metrics_valid = []\n\n        if self.verbose > 0:\n            print(\"Will train until validation stopping metric\",\n                  f\"hasn't improved in {self.patience} rounds.\")\n            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n            print('---------------------------------------')\n            print(msg_epoch)\n\n        total_time = 0\n        while (self.epoch < self.max_epochs and\n               self.patience_counter < self.patience):\n            starting_time = time.time()\n            # updates learning rate history\n            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n\n            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n\n            # leaving it here, may be used for callbacks later\n            self.losses_train.append(fit_metrics['train']['loss_avg'])\n            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n\n            stopping_loss = fit_metrics['valid']['stopping_loss']\n            if stopping_loss < self.best_cost:\n                self.best_cost = stopping_loss\n                self.patience_counter = 0\n                # Saving model\n                self.best_network = deepcopy(self.network)\n                has_improved = True\n            else:\n                self.patience_counter += 1\n                has_improved=False\n            self.epoch += 1\n            total_time += time.time() - starting_time\n            if self.verbose > 0:\n                if self.epoch % self.verbose == 0:\n                    separator = \"|\"\n                    msg_epoch = f\"| {self.epoch:<5} | \"\n                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n                    msg_epoch += f' {separator:<2} '\n                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n                    msg_epoch += f\" {has_improved}\"\n                    print(msg_epoch)\n\n        if self.verbose > 0:\n            if self.patience_counter == self.patience:\n                print(f\"Early stopping occured at epoch {self.epoch}\")\n            print(f\"Training done in {total_time:.3f} seconds.\")\n            print('---------------------------------------')\n\n        self.history = {\"train\": {\"loss\": self.losses_train,\n                                  \"metric\": self.metrics_train,\n                                  \"lr\": self.learning_rates},\n                        \"valid\": {\"loss\": self.losses_valid,\n                                  \"metric\": self.metrics_valid}}\n        # load best models post training\n        self.load_best_model()\n\n        # compute feature importance once the best model is defined\n        self._compute_feature_importances(train_dataloader)\n\n    def save_model(self, path):\n        \"\"\"\n        Saving model with two distinct files.\n        \"\"\"\n        saved_params = {}\n        for key, val in self.get_params().items():\n            if isinstance(val, type):\n                # Don't save torch specific params\n                continue\n            else:\n                saved_params[key] = val\n\n        # Create folder\n        Path(path).mkdir(parents=True, exist_ok=True)\n\n        # Save models params\n        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n            json.dump(saved_params, f)\n\n        # Save state_dict\n        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n        shutil.make_archive(path, 'zip', path)\n        shutil.rmtree(path)\n        print(f\"Successfully saved model at {path}.zip\")\n        return f\"{path}.zip\"\n\n    def load_model(self, filepath):\n\n        try:\n            try:\n                with zipfile.ZipFile(filepath) as z:\n                    with z.open(\"model_params.json\") as f:\n                        loaded_params = json.load(f)\n                    with z.open(\"network.pt\") as f:\n                        try:\n                            saved_state_dict = torch.load(f)\n                        except io.UnsupportedOperation:\n                            # In Python <3.7, the returned file object is not seekable (which at least\n                            # some versions of PyTorch require) - so we'll try buffering it in to a\n                            # BytesIO instead:\n                            saved_state_dict = torch.load(io.BytesIO(f.read()))\n                            \n            except:\n                with open(os.path.join(filepath, \"model_params.json\")) as f:\n                        loaded_params = json.load(f)\n\n                saved_state_dict = torch.load(os.path.join(filepath, \"network.pt\"), map_location=\"cpu\")\n \n        except KeyError:\n            raise KeyError(\"Your zip file is missing at least one component\")\n\n        #print(loaded_params)\n        if torch.cuda.is_available():\n            device_name = 'cuda'\n        else:\n            device_name = 'cpu'\n        loaded_params[\"device_name\"] = device_name\n        self.__init__(**loaded_params)\n        \n        \n\n        self.init_network(\n            input_dim=self.input_dim,\n            output_dim=self.output_dim,\n            n_d=self.n_d,\n            n_a=self.n_a,\n            n_steps=self.n_steps,\n            gamma=self.gamma,\n            cat_idxs=self.cat_idxs,\n            cat_dims=self.cat_dims,\n            cat_emb_dim=self.cat_emb_dim,\n            n_independent=self.n_independent,\n            n_shared=self.n_shared,\n            epsilon=self.epsilon,\n            virtual_batch_size=1024,\n            momentum=self.momentum,\n            device_name=self.device_name,\n            mask_type=self.mask_type\n        )\n        self.network.load_state_dict(saved_state_dict)\n        self.network.eval()\n        return\n\n    def fit_epoch(self, train_dataloader, valid_dataloader):\n        \"\"\"\n        Evaluates and updates network for one epoch.\n        Parameters\n        ----------\n            train_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with valid set\n        \"\"\"\n        train_metrics = self.train_epoch(train_dataloader)\n        valid_metrics = self.predict_epoch(valid_dataloader)\n\n        fit_metrics = {'train': train_metrics,\n                       'valid': valid_metrics}\n\n        return fit_metrics\n\n    @abstractmethod\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n        raise NotImplementedError('users must define train_epoch to use this base class')\n\n    @abstractmethod\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        raise NotImplementedError('users must define train_batch to use this base class')\n\n    @abstractmethod\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        raise NotImplementedError('users must define predict_epoch to use this base class')\n\n    @abstractmethod\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        raise NotImplementedError('users must define predict_batch to use this base class')\n\n    def load_best_model(self):\n        if self.best_network is not None:\n            self.network = self.best_network\n\n    @abstractmethod\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem or the last class\n        \"\"\"\n        raise NotImplementedError('users must define predict to use this base class')\n\n    def explain(self, X):\n        \"\"\"\n        Return local explanation\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            M_explain: matrix\n                Importance per sample, per columns.\n            masks: matrix\n                Sparse matrix showing attention masks used by network.\n        \"\"\"\n        self.network.eval()\n\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            M_explain, masks = self.network.forward_masks(data)\n            for key, value in masks.items():\n                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n                                            self.reducing_matrix)\n\n            if batch_nb == 0:\n                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                             self.reducing_matrix)\n                res_masks = masks\n            else:\n                res_explain = np.vstack([res_explain,\n                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n                                                        self.reducing_matrix)])\n                for key, value in masks.items():\n                    res_masks[key] = np.vstack([res_masks[key], value])\n        return res_explain, res_masks\n\n    def _compute_feature_importances(self, loader):\n        self.network.eval()\n        feature_importances_ = np.zeros((self.network.post_embed_dim))\n        for data, targets in loader:\n            data = data.to(self.device).float()\n            M_explain, masks = self.network.forward_masks(data)\n            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n\n        feature_importances_ = csc_matrix.dot(feature_importances_,\n                                              self.reducing_matrix)\n        self.feature_importances_ = feature_importances_ \/ np.sum(feature_importances_)\n        \n        \nclass TabNetRegressor(TabModel):\n\n    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n                          batch_size, num_workers, drop_last):\n        \"\"\"\n        Returns\n        -------\n        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n            Training and validation dataloaders\n        -------\n        \"\"\"\n        if isinstance(weights, int):\n            if weights == 1:\n                raise ValueError(\"Please provide a list of weights for regression.\")\n        if isinstance(weights, dict):\n            raise ValueError(\"Please provide a list of weights for regression.\")\n\n        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n                                                                y_train,\n                                                                X_valid,\n                                                                y_valid,\n                                                                weights,\n                                                                batch_size,\n                                                                num_workers,\n                                                                drop_last)\n        return train_dataloader, valid_dataloader\n\n    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n                          weights, max_epochs, patience,\n                          batch_size, virtual_batch_size, num_workers, drop_last):\n\n        if loss_fn is None:\n            self.loss_fn = torch.nn.functional.mse_loss\n        else:\n            self.loss_fn = loss_fn\n\n        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n        self.input_dim = X_train.shape[1]\n\n        if len(y_train.shape) == 1:\n            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n                                if doing single regression.\"\"\")\n        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n        self.output_dim = y_train.shape[1]\n\n        self.updated_weights = weights\n\n        self.max_epochs = max_epochs\n        self.patience = patience\n        self.batch_size = batch_size\n        self.virtual_batch_size = virtual_batch_size\n        # Initialize counters and histories.\n        self.patience_counter = 0\n        self.epoch = 0\n        self.best_cost = np.inf\n        self.num_workers = num_workers\n        self.drop_last = drop_last\n\n    def train_epoch(self, train_loader):\n        \"\"\"\n        Trains one epoch of the network in self.network\n        Parameters\n        ----------\n            train_loader: a :class: `torch.utils.data.Dataloader`\n                DataLoader with train set\n        \"\"\"\n\n        self.network.train()\n        y_preds = []\n        ys = []\n        total_loss = 0\n\n        for data, targets in train_loader:\n            batch_outs = self.train_batch(data, targets)\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n            total_loss += batch_outs[\"loss\"]\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  )\n        total_loss = total_loss \/ len(train_loader)\n        epoch_metrics = {'loss_avg': total_loss,\n                         'stopping_loss': total_loss,\n                         }\n\n        if self.scheduler is not None:\n            self.scheduler.step()\n        return epoch_metrics\n\n    def train_batch(self, data, targets):\n        \"\"\"\n        Trains one batch of data\n        Parameters\n        ----------\n            data: a :tensor: `torch.tensor`\n                Input data\n            target: a :tensor: `torch.tensor`\n                Target data\n        \"\"\"\n        self.network.train()\n        data = data.to(self.device).float()\n\n        targets = targets.to(self.device).float()\n        self.optimizer.zero_grad()\n\n        output, M_loss = self.network(data)\n\n        loss = self.loss_fn(output, targets)\n        \n        loss -= self.lambda_sparse*M_loss\n\n        loss.backward()\n        if self.clip_value:\n            clip_grad_norm_(self.network.parameters(), self.clip_value)\n        self.optimizer.step()\n\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict_epoch(self, loader):\n        \"\"\"\n        Validates one epoch of the network in self.network\n        Parameters\n        ----------\n            loader: a :class: `torch.utils.data.Dataloader`\n                    DataLoader with validation set\n        \"\"\"\n        y_preds = []\n        ys = []\n        self.network.eval()\n        total_loss = 0\n\n        for data, targets in loader:\n            batch_outs = self.predict_batch(data, targets)\n            total_loss += batch_outs[\"loss\"]\n            y_preds.append(batch_outs[\"y_preds\"].cpu().detach().numpy())\n            ys.append(batch_outs[\"y\"].cpu().detach().numpy())\n\n        y_preds = np.vstack(y_preds)\n        ys = np.vstack(ys)\n\n        stopping_loss =log_loss_multi(ys, torch.sigmoid(torch.as_tensor(y_preds)).numpy()  ) #mean_squared_error(y_true=ys, y_pred=y_preds)\n\n        total_loss = total_loss \/ len(loader)\n        epoch_metrics = {'total_loss': total_loss,\n                         'stopping_loss': stopping_loss}\n\n        return epoch_metrics\n\n    def predict_batch(self, data, targets):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            batch_outs: dict\n        \"\"\"\n        self.network.eval()\n        data = data.to(self.device).float()\n        targets = targets.to(self.device).float()\n\n        output, M_loss = self.network(data)\n       \n        loss = self.loss_fn(output, targets)\n        #print(self.loss_fn, loss)\n        loss -= self.lambda_sparse*M_loss\n        #print(loss)\n        loss_value = loss.item()\n        batch_outs = {'loss': loss_value,\n                      'y_preds': output,\n                      'y': targets}\n        return batch_outs\n\n    def predict(self, X):\n        \"\"\"\n        Make predictions on a batch (valid)\n        Parameters\n        ----------\n            data: a :tensor: `torch.Tensor`\n                Input data\n            target: a :tensor: `torch.Tensor`\n                Target data\n        Returns\n        -------\n            predictions: np.array\n                Predictions of the regression problem\n        \"\"\"\n        self.network.eval()\n        dataloader = DataLoader(PredictDataset(X),\n                                batch_size=self.batch_size, shuffle=False)\n\n        results = []\n        for batch_nb, data in enumerate(dataloader):\n            data = data.to(self.device).float()\n\n            output, M_loss = self.network(data)\n            predictions = output.cpu().detach().numpy()\n            results.append(predictions)\n        res = np.vstack(results)\n        return res","6867a7ce":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","4deac685":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.verbose=False\n        #\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.SPLITS = 10\n        self.EPOCHS = 200\n        self.num_ensembling = 1\n        self.seed = 0\n        # Parameters model\n        self.cat_emb_dim=[1] * cat_tr.shape[1] #to choose\n        self.cats_idx = list(range(cat_tr.shape[1]))\n        self.cat_dims = [len(np.unique(cat_tr[:, i])) for i in range(cat_tr.shape[1])]\n        self.num_numericals= numerical_tr.shape[1]\n        # save\n        self.save_name = \"..\/input\/tabnet-smoothing\/tabnet_raw_step1\"\n        \n        self.strategy = \"KFOLD\" # \ncfg = Config()","17bf5002":"X_test = np.concatenate([cat_test, numerical_test ], axis=1)","720a7c9c":"SEED  = [0,1,2,3,4,5,6]\nif cfg.strategy == \"KFOLD\":\n    oof_preds_all = []\n    oof_targets_all = []\n    scores_all =  []\n    scores_auc_all= []\n    preds_test = []\n    for seed in SEED:\n        print(\"## SEED : \", seed)\n        mskf = MultilabelStratifiedKFold(n_splits=5, random_state=cfg.seed+seed, shuffle=True)\n        oof_preds = []\n        oof_targets = []\n        scores = []\n        scores_auc = []\n        p = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n\n            ## model\n            X_train, y_train = torch.as_tensor(np.concatenate([cat_tr[train_idx], numerical_tr[train_idx] ], axis=1)), torch.as_tensor(targets_tr[train_idx])\n            X_val, y_val = torch.as_tensor(np.concatenate([cat_tr[val_idx], numerical_tr[val_idx] ], axis=1)), torch.as_tensor(targets_tr[val_idx])\n            model = TabNetRegressor(n_d=24, n_a=24, n_steps=1, gamma=1.3, lambda_sparse=0, cat_dims=cfg.cat_dims, cat_emb_dim=cfg.cat_emb_dim, cat_idxs=cfg.cats_idx, optimizer_fn=torch.optim.Adam,\n                                   optimizer_params=dict(lr=2e-2), mask_type='entmax', device_name=cfg.device, scheduler_params=dict(milestones=[ 50,100,150], gamma=0.9), scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n            #'sparsemax'\n            \n            name = cfg.save_name + f\"_fold{j}_{seed}\"\n            model.load_model(name)\n            # preds on val\n            preds = model.predict(X_val)\n            preds = torch.sigmoid(torch.as_tensor(preds)).detach().cpu().numpy()\n            score = log_loss_multi(y_val, preds)\n            \n            # preds on test\n            temp = model.predict(X_test)\n            p.append(torch.sigmoid(torch.as_tensor(temp)).detach().cpu().numpy())\n            ## save oof to compute the CV later\n            oof_preds.append(preds)\n            oof_targets.append(y_val)\n            scores.append(score)\n            scores_auc.append(auc_multi(y_val,preds))\n            print(f\"validation fold {j} : {score}\")\n        p = np.stack(p)\n        preds_test.append(p)\n        oof_preds_all.append(np.concatenate(oof_preds))\n        oof_targets_all.append(np.concatenate(oof_targets))\n        scores_all.append(np.array(scores))\n        scores_auc_all.append(np.array(scores_auc))\n    preds_test = np.stack(preds_test)","e2d16bfc":"if cfg.strategy == \"KFOLD\":\n\n    for i in range(cfg.num_ensembling): \n        print(\"CV score fold : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n        print(\"auc mean : \", sum(scores_auc_all[i])\/len(scores_auc_all[i]))","a64f6902":"submission[columns] = preds_test.mean(1).mean(0)\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0","fa313fa3":"sub2  = submission","8ce38e8d":"import sys\nsys.path.append('..\/input')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom sklearn.metrics import log_loss\nimport random\nimport os\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nimport warnings\nwarnings.filterwarnings('ignore')","75012b2e":"FOLDS = 10\n# Number of epochs to train each model\nEPOCHS = 80\n# Batch size\nBATCH_SIZE = 124\n# Learning rate\nLR = 0.001\n# Verbosity\nVERBOSE = 0\n# Seed for deterministic results\n# Seed for deterministic results\nSEEDS1 = [1, 2, 3, 4, 5, 6, 7]\nSEEDS2 = [8, 9, 10, 11, 12, 13, 14]\nSEEDS3 = [15, 16, 17, 18, 19, 20, 21]\nSEEDS4 = [22, 23, 24, 25, 26, 27, 28]\nSEEDS5 = [29, 30, 31, 32, 33, 34, 35]\n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n","a7d10583":"def mapping_and_filter(train, train_targets, test):\n    cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n    cp_dose = {'D1': 0, 'D2': 1}\n    for df in [train, test]:\n        df['cp_type'] = df['cp_type'].map(cp_type)\n        df['cp_dose'] = df['cp_dose'].map(cp_dose)\n    train_targets = train_targets[train['cp_type'] == 0].reset_index(drop = True)\n    train = train[train['cp_type'] == 0].reset_index(drop = True)\n    train_targets.drop(['sig_id'], inplace = True, axis = 1)\n    return train, train_targets, test\n\n# Function to scale our data\ndef scaling(train, test):\n    features = train.columns[2:]\n    scaler = RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n# Function to extract pca features\ndef fe_pca(train, test, n_components_g = 70, n_components_c = 10, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        pca = PCA(n_components = n_components,  random_state = SEED)\n        data = pca.fit_transform(data)\n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns = columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n        train = pd.concat([train, train_], axis = 1)\n        test = pd.concat([test, test_], axis = 1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n    return train, test\n\n# Function to extract common stats features\ndef fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in [train, test]:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ndef c_squared(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_squared'] = df[feature] ** 2\n    return train, test\n\n# Function to calculate the mean log loss of the targets including clipping\ndef mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)\n\ndef create_model_rs(shape1, shape2):    \n    input_1 = tf.keras.layers.Input(shape = (shape1))\n    input_2 = tf.keras.layers.Input(shape = (shape2))\n    \n    head_1 = tf.keras.layers.BatchNormalization()(input_1)\n    head_1 = tf.keras.layers.Dropout(0.2)(head_1)\n    head_1 = tf.keras.layers.Dense(512, activation = \"elu\")(head_1)\n    head_1 = tf.keras.layers.BatchNormalization()(head_1)\n    input_3 = tf.keras.layers.Dense(256, activation = \"elu\")(head_1)\n\n    input_3_concat = tf.keras.layers.Concatenate()([input_2, input_3])\n    \n    head_2 = tf.keras.layers.BatchNormalization()(input_3_concat)\n    head_2 = tf.keras.layers.Dropout(0.3)(head_2)\n    head_2 = tf.keras.layers.Dense(512, \"relu\")(head_2)\n    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n    head_2 = tf.keras.layers.Dense(512, \"elu\")(head_2)\n    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n    head_2 = tf.keras.layers.Dense(256, \"relu\")(head_2)\n    head_2 = tf.keras.layers.BatchNormalization()(head_2)\n    input_4 = tf.keras.layers.Dense(256, \"elu\")(head_2)\n\n    input_4_avg = tf.keras.layers.Average()([input_3, input_4]) \n    \n    head_3 = tf.keras.layers.BatchNormalization()(input_4_avg)\n    head_3 = tf.keras.layers.Dense(256, kernel_initializer = 'lecun_normal', activation = 'selu')(head_3)\n    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n    head_3 = tf.keras.layers.Dense(206, kernel_initializer = 'lecun_normal', activation = 'selu')(head_3)\n    head_3 = tf.keras.layers.BatchNormalization()(head_3)\n    output = tf.keras.layers.Dense(206, activation = \"sigmoid\")(head_3)\n\n    model = tf.keras.models.Model(inputs = [input_1, input_2], outputs = output)\n    opt = tf.optimizers.Adam(learning_rate = LR)\n    model.compile(optimizer = opt, \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015), \n                  metrics = tf.keras.metrics.BinaryCrossentropy())\n    \n    return model\n\n\n# Function to create our 5 layer dnn model\ndef create_model_5l(shape):\n    inp = tf.keras.layers.Input(shape = (shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(2560, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(2048, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1524, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1012, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(780, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(206, activation = 'sigmoid')(x)\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    opt = tf.optimizers.Adam(learning_rate = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(optimizer = opt, \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0020),\n                  metrics = tf.keras.metrics.BinaryCrossentropy())\n    return model\n\n# Function to create our 4 layer dnn model\ndef create_model_4l(shape):\n    inp = tf.keras.layers.Input(shape = (shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(2048, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1524, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1012, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4)(x)\n    x = tf.keras.layers.Dense(1012, activation = 'relu')(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(206, activation = 'sigmoid')(x)\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    opt = tf.optimizers.Adam(learning_rate = LR)\n    model.compile(optimizer = opt, \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0020),\n                  metrics = tf.keras.metrics.BinaryCrossentropy())\n    return model\n\n# Function to create our 3 layer dnn model\ndef create_model_3l(shape):\n    inp = tf.keras.layers.Input(shape = (shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.4914099166744246)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1159, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.18817607797795838)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(960, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.12542057776853896)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1811, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.20175242230280122)(x)\n    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = 'sigmoid'))(x)\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    opt = tf.optimizers.Adam(learning_rate = LR)\n    opt = tfa.optimizers.Lookahead(opt, sync_period = 10)\n    model.compile(optimizer = opt, \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015),\n                  metrics = tf.keras.metrics.BinaryCrossentropy())\n    return model\n\n# Function to create our 2 layer dnn model\ndef create_model_2l(shape):\n    inp = tf.keras.layers.Input(shape = (shape))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(0.2688628097505064)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1292, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4598218403250696)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(983, activation = 'relu'))(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.4703144018483698)(x)\n    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation = 'sigmoid'))(x)\n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    opt = tf.optimizers.Adam(learning_rate = LR)\n    opt = tfa.optimizers.Lookahead(opt, sync_period = 10)\n    model.compile(optimizer = opt, \n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015),\n                  metrics = tf.keras.metrics.BinaryCrossentropy())\n    return model\n\n\n# Function to train our dnn\ndef train_and_evaluate(train, test, train_targets, features, start_predictors, SEED = 123, MODEL = '3l'):\n    seed_everything(SEED)\n    oof_pred = np.zeros((train.shape[0], 206))\n    test_pred = np.zeros((test.shape[0], 206))   \n    for fold, (trn_ind, val_ind) in enumerate(MultilabelStratifiedKFold(n_splits = FOLDS, \n                                                                        random_state = SEED, \n                                                                        shuffle = True)\\\n                                              .split(train_targets, train_targets)):\n        K.clear_session()\n        if MODEL == '5l':\n            model = create_model_5l(len(features))\n        elif MODEL == '4l':\n            model = create_model_4l(len(features))\n        elif MODEL == '3l':\n            model = create_model_3l(len(features))\n        elif MODEL == '2l':\n            model = create_model_2l(len(features))\n        elif MODEL == \"rs\":\n            model = create_model_rs(len(features), len(start_predictors))\n        \n        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_binary_crossentropy',\n                                                          mode = 'min',\n                                                          patience = 10,\n                                                          restore_best_weights = False,\n                                                          verbose = VERBOSE)\n        \n        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_binary_crossentropy',\n                                                         mode = 'min',\n                                                         factor = 0.3,\n                                                         patience = 3,\n                                                         verbose = VERBOSE)\n        \n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'{MODEL}_{fold}_{SEED}.h5',\n                                                        monitor = 'val_binary_crossentropy',\n                                                        verbose = VERBOSE,\n                                                        save_best_only = True,\n                                                        save_weights_only = True)\n        \n        x_train, x_val = train[features].values[trn_ind], train[features].values[val_ind]\n        y_train, y_val = train_targets.values[trn_ind], train_targets.values[val_ind]\n        \n        if MODEL == \"rs\":\n            x_train_, x_val_ = train[start_predictors].values[trn_ind], train[start_predictors].values[val_ind]\n\n            model.fit([x_train, x_train_], y_train,\n                  validation_data = ([x_val, x_val_], y_val),\n                  epochs = EPOCHS, \n                  batch_size = BATCH_SIZE,\n                  callbacks = [early_stopping, reduce_lr,  checkpoint],\n                  verbose = VERBOSE)\n        \n            model.load_weights(f'{MODEL}_{fold}_{SEED}.h5')\n\n            oof_pred[val_ind] = model.predict([x_val, x_val_])\n            test_pred += model.predict([test[features].values, test[start_predictors].values]) \/ FOLDS\n            \n        else:\n            model.fit(x_train, y_train,\n                  validation_data = (x_val, y_val),\n                  epochs = EPOCHS, \n                  batch_size = BATCH_SIZE,\n                  callbacks = [early_stopping, reduce_lr,  checkpoint],\n                  verbose = VERBOSE)\n        \n            model.load_weights(f'{MODEL}_{fold}_{SEED}.h5')\n\n            oof_pred[val_ind] = model.predict(x_val)\n            test_pred += model.predict(test[features].values) \/ FOLDS\n\n\n    oof_score = mean_log_loss(train_targets.values, oof_pred)\n    print(f'Our out of folds mean log loss score is {oof_score}')\n    \n    return test_pred, oof_pred\n\n# Function to train our dnn\ndef inference(train, test, train_targets, features, start_predictors, SEED = 123, MODEL = '3l', PATH = '..\/input\/moa-3layer'):\n    seed_everything(SEED)\n    oof_pred = np.zeros((train.shape[0], 206))\n    test_pred = np.zeros((test.shape[0], 206))   \n    for fold, (trn_ind, val_ind) in enumerate(MultilabelStratifiedKFold(n_splits = FOLDS, \n                                                                        random_state = SEED, \n                                                                        shuffle = True)\\\n                                              .split(train_targets, train_targets)):\n        K.clear_session()\n        if MODEL == '5l':\n            model = create_model_5l(len(features))\n        elif MODEL == '4l':\n            model = create_model_4l(len(features))\n        elif MODEL == '3l':\n            model = create_model_3l(len(features))\n        elif MODEL == '2l':\n            model = create_model_2l(len(features))\n        elif MODEL == \"rs\":\n            model = create_model_rs(len(features), len(start_predictors))\n\n\n        x_train, x_val = train[features].values[trn_ind], train[features].values[val_ind]\n        y_train, y_val = train_targets.values[trn_ind], train_targets.values[val_ind]\n        \n        model.load_weights('..\/input\/moa-5seed\/Model_Weights'+'\/'+MODEL+'_'+str(fold)+'_'+str(SEED)+'.h5')\n        \n        if MODEL == \"rs\":\n            x_train_, x_val_ = train[start_predictors].values[trn_ind], train[start_predictors].values[val_ind]\n            oof_pred[val_ind] = model.predict([x_val, x_val_])\n            test_pred += model.predict([test[features].values, test[start_predictors].values]) \/ FOLDS\n        else:\n            oof_pred[val_ind] = model.predict(x_val)\n            test_pred += model.predict(test[features].values) \/ FOLDS\n\n    oof_score = mean_log_loss(train_targets.values, oof_pred)\n    print(f'Our out of folds mean log loss score is {oof_score}')\n    \n    return test_pred, oof_pred\n    \n    \n\n# Function to train our model with multiple seeds and average the predictions\ndef run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = [123], MODEL = '3l', PATH = '..\/input\/moa-3layer'):\n    \n    test_pred = []\n    oof_pred = []\n    \n    for SEED in SEEDS:\n        print(f'Using model {MODEL} with seed {SEED} for inference')\n        print(f'Trained with {len(features)} features')\n        test_pred_, oof_pred_ = inference(train, test, train_targets, features, start_predictors, SEED = SEED, MODEL = MODEL, PATH = PATH)\n        test_pred.append(test_pred_)\n        oof_pred.append(oof_pred_)\n        print('-'*50)\n        print('\\n')\n        \n    test_pred = np.average(test_pred, axis = 0)\n    oof_pred = np.average(oof_pred, axis = 0)\n        \n    seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n    print(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')\n    \n    return test_pred, oof_pred\n\ndef submission(test_pred):\n    sample_submission.loc[:, train_targets.columns] = test_pred\n    sample_submission.loc[test['cp_type'] == 1, train_targets.columns] = 0\n    sample_submission.to_csv('submission.csv', index = False)\n    return sample_submission\n\n# Got this predictors from public kernels for the resnet type model\nstart_predictors = [\"g-0\", \"g-7\", \"g-8\", \"g-10\", \"g-13\", \"g-17\", \"g-20\", \"g-22\", \"g-24\", \"g-26\", \"g-28\", \"g-29\", \"g-30\", \"g-31\", \"g-32\", \"g-34\", \"g-35\", \"g-36\", \"g-37\", \"g-38\",\n                    \"g-39\",\"g-41\", \"g-46\", \"g-48\", \"g-50\", \"g-51\", \"g-52\", \"g-55\", \"g-58\", \"g-59\", \"g-61\", \"g-62\", \"g-63\", \"g-65\", \"g-66\", \"g-67\", \"g-68\", \"g-70\", \"g-72\", \"g-74\", \n                    \"g-75\", \"g-79\", \"g-83\", \"g-84\", \"g-85\", \"g-86\", \"g-90\", \"g-91\", \"g-94\", \"g-95\", \"g-96\", \"g-97\", \"g-98\", \"g-100\", \"g-102\", \"g-105\", \"g-106\", \"g-112\", \"g-113\", \n                    \"g-114\", \"g-116\", \"g-121\", \"g-123\", \"g-126\", \"g-128\", \"g-131\", \"g-132\", \"g-134\", \"g-135\", \"g-138\", \"g-139\", \"g-140\", \"g-142\", \"g-144\", \"g-145\", \"g-146\", \n                    \"g-147\", \"g-148\", \"g-152\", \"g-155\", \"g-157\", \"g-158\", \"g-160\", \"g-163\", \"g-164\", \"g-165\", \"g-170\", \"g-173\", \"g-174\", \"g-175\", \"g-177\", \"g-178\", \"g-181\", \n                    \"g-183\", \"g-185\", \"g-186\", \"g-189\", \"g-192\", \"g-194\", \"g-195\", \"g-196\", \"g-197\", \"g-199\", \"g-201\", \"g-202\", \"g-206\", \"g-208\", \"g-210\", \"g-213\", \"g-214\", \n                    \"g-215\", \"g-220\", \"g-226\", \"g-228\", \"g-229\", \"g-235\", \"g-238\", \"g-241\", \"g-242\", \"g-243\", \"g-244\", \"g-245\", \"g-248\", \"g-250\", \"g-251\", \"g-254\", \"g-257\", \n                    \"g-259\", \"g-261\", \"g-266\", \"g-270\", \"g-271\", \"g-272\", \"g-275\", \"g-278\", \"g-282\", \"g-287\", \"g-288\", \"g-289\", \"g-291\", \"g-293\", \"g-294\", \"g-297\", \"g-298\",\n                    \"g-301\", \"g-303\", \"g-304\", \"g-306\", \"g-308\", \"g-309\", \"g-310\", \"g-311\", \"g-314\", \"g-315\", \"g-316\", \"g-317\", \"g-320\", \"g-321\", \"g-322\", \"g-327\", \"g-328\", \n                    \"g-329\", \"g-332\", \"g-334\", \"g-335\", \"g-336\", \"g-337\", \"g-339\", \"g-342\", \"g-344\", \"g-349\", \"g-350\", \"g-351\", \"g-353\", \"g-354\", \"g-355\", \"g-357\", \"g-359\", \n                    \"g-360\", \"g-364\", \"g-365\", \"g-366\", \"g-367\", \"g-368\", \"g-369\", \"g-374\", \"g-375\", \"g-377\", \"g-379\", \"g-385\", \"g-386\", \"g-390\", \"g-392\", \"g-393\", \"g-400\", \n                    \"g-402\", \"g-406\", \"g-407\", \"g-409\", \"g-410\", \"g-411\", \"g-414\", \"g-417\", \"g-418\", \"g-421\", \"g-423\", \"g-424\", \"g-427\", \"g-429\", \"g-431\", \"g-432\", \"g-433\", \n                    \"g-434\", \"g-437\", \"g-439\", \"g-440\", \"g-443\", \"g-449\", \"g-458\", \"g-459\", \"g-460\", \"g-461\", \"g-464\", \"g-467\", \"g-468\", \"g-470\", \"g-473\", \"g-477\", \"g-478\", \n                    \"g-479\", \"g-484\", \"g-485\", \"g-486\", \"g-488\", \"g-489\", \"g-491\", \"g-494\", \"g-496\", \"g-498\", \"g-500\", \"g-503\", \"g-504\", \"g-506\", \"g-508\", \"g-509\", \"g-512\", \n                    \"g-522\", \"g-529\", \"g-531\", \"g-534\", \"g-539\", \"g-541\", \"g-546\", \"g-551\", \"g-553\", \"g-554\", \"g-559\", \"g-561\", \"g-562\", \"g-565\", \"g-568\", \"g-569\", \"g-574\", \n                    \"g-577\", \"g-578\", \"g-586\", \"g-588\", \"g-590\", \"g-594\", \"g-595\", \"g-596\", \"g-597\", \"g-599\", \"g-600\", \"g-603\", \"g-607\", \"g-615\", \"g-618\", \"g-619\", \"g-620\", \n                    \"g-625\", \"g-628\", \"g-629\", \"g-632\", \"g-634\", \"g-635\", \"g-636\", \"g-638\", \"g-639\", \"g-641\", \"g-643\", \"g-644\", \"g-645\", \"g-646\", \"g-647\", \"g-648\", \"g-663\", \n                    \"g-664\", \"g-665\", \"g-668\", \"g-669\", \"g-670\", \"g-671\", \"g-672\", \"g-673\", \"g-674\", \"g-677\", \"g-678\", \"g-680\", \"g-683\", \"g-689\", \"g-691\", \"g-693\", \"g-695\", \n                    \"g-701\", \"g-702\", \"g-703\", \"g-704\", \"g-705\", \"g-706\", \"g-708\", \"g-711\", \"g-712\", \"g-720\", \"g-721\", \"g-723\", \"g-724\", \"g-726\", \"g-728\", \"g-731\", \"g-733\", \n                    \"g-738\", \"g-739\", \"g-742\", \"g-743\", \"g-744\", \"g-745\", \"g-749\", \"g-750\", \"g-752\", \"g-760\", \"g-761\", \"g-764\", \"g-766\", \"g-768\", \"g-770\", \"g-771\", \"c-0\", \n                    \"c-1\", \"c-2\", \"c-3\", \"c-4\", \"c-5\", \"c-6\", \"c-7\", \"c-8\", \"c-9\", \"c-10\", \"c-11\", \"c-12\", \"c-13\", \"c-14\", \"c-15\", \"c-16\", \"c-17\", \"c-18\", \"c-19\", \"c-20\", \n                    \"c-21\", \"c-22\", \"c-23\", \"c-24\", \"c-25\", \"c-26\", \"c-27\", \"c-28\", \"c-29\", \"c-30\", \"c-31\", \"c-32\", \"c-33\", \"c-34\", \"c-35\", \"c-36\", \"c-37\", \"c-38\", \"c-39\", \n                    \"c-40\", \"c-41\", \"c-42\", \"c-43\", \"c-44\", \"c-45\", \"c-46\", \"c-47\", \"c-48\", \"c-49\", \"c-50\", \"c-51\", \"c-52\", \"c-53\", \"c-54\", \"c-55\", \"c-56\", \"c-57\", \"c-58\", \n                    \"c-59\", \"c-60\", \"c-61\", \"c-62\", \"c-63\", \"c-64\", \"c-65\", \"c-66\", \"c-67\", \"c-68\", \"c-69\", \"c-70\", \"c-71\", \"c-72\", \"c-73\", \"c-74\", \"c-75\", \"c-76\", \"c-77\", \n                    \"c-78\", \"c-79\", \"c-80\", \"c-81\", \"c-82\", \"c-83\", \"c-84\", \"c-85\", \"c-86\", \"c-87\", \"c-88\", \"c-89\", \"c-90\", \"c-91\", \"c-92\", \"c-93\", \"c-94\", \"c-95\", \"c-96\", \n                    \"c-97\", \"c-98\", \"c-99\"]\n\n\ntrain = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntrain, train_targets, test = mapping_and_filter(train, train_targets, test)\ntrain, test = fe_stats(train, test)\ntrain, test = c_squared(train, test)\ntrain, test = fe_pca(train, test, n_components_g = 70, n_components_c = 10, SEED = 123)\ntrain, test, features = scaling(train, test)\n\n# Inference time\ntest_pred_5l, oof_pred_5l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS1, MODEL = '5l', PATH = '..\/input\/moaaaaa\/5l')\ntest_pred_4l, oof_pred_4l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS2, MODEL = '4l', PATH = '..\/input\/moaaaaa\/4l')\ntest_pred_3l, oof_pred_3l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS3, MODEL = '3l', PATH = '..\/input\/moaaaaa\/3l')\ntest_pred_2l, oof_pred_2l = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS4, MODEL = '2l', PATH = '..\/input\/moaaaaa\/2l')\ntest_pred_rs, oof_pred_rs = run_multiple_seeds(train, test, train_targets, features, start_predictors, SEEDS = SEEDS5, MODEL = 'rs', PATH = '..\/input\/moaaaaa\/rs')\n\n# Blend 5l, 4l, 3l and l2 dnn model\noof_pred = np.average([oof_pred_5l, oof_pred_4l, oof_pred_3l, oof_pred_2l], axis = 0)\nseed_log_loss = mean_log_loss(train_targets.values, oof_pred)\nprint(f'Our final out of folds log loss for our classic dnn blend is {seed_log_loss}')\ntest_pred = np.average([test_pred_5l, test_pred_4l, test_pred_3l, test_pred_2l], axis = 0)\n\n# Blend the result of the previous model with the dnn resnet type model\noof_pred = np.average([oof_pred, oof_pred_rs], axis = 0)\nseed_log_loss = mean_log_loss(train_targets.values, oof_pred)\nprint(f'Our final out of folds log loss for our classic dnn + dnn resnet type model is {seed_log_loss}')\ntest_pred = np.average([test_pred, test_pred_rs], axis = 0)\n\nsample_submission = submission(test_pred)\nsample_submission.head()\n","cfb1d762":"sub3  = sample_submission","f4b94a31":"submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","d6e26d08":"public_ids = list(submission['sig_id'].values)\n\nsubmission.shape","a703eca0":"test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n\ntest_ids = list(test['sig_id'].values)\n\nprivate_ids = list(set(test_ids)-set(public_ids))\n\nlen(private_ids)","a91061e4":"predictions = [sub3,sub2,sub1]","2a044f45":"target_columns = list(submission.columns)\ntarget_columns.remove('sig_id')","efffedec":"y_pred = pd.DataFrame()\ny_pred['sig_id'] = predictions[0]['sig_id']\n\nfor column in target_columns:\n    column_data = []\n    for i in range(len(predictions)):\n        column_data.append(predictions[i][column])\n    y_pred[column] = np.mean(column_data, axis=0)\n\ny_pred.shape","5fca795e":"weights = [0.3345993850605896, 0.33387592693072893, 0.3315246880086815]","190cd61b":"weighted_y_pred = pd.DataFrame()\nweighted_y_pred['sig_id'] = predictions[0]['sig_id']\n\nfor column in target_columns:\n    column_data = []\n    for i in range(len(predictions)):\n        column_data.append(predictions[i][column] * weights[i])\n    weighted_y_pred[column] = np.sum(column_data, axis=0)\n\nweighted_y_pred.shape","085fd207":"y_pred = weighted_y_pred","15904efe":"submission = pd.DataFrame(index = public_ids + private_ids, columns=target_columns)\nsubmission.index.name = 'sig_id'\n\nsubmission[:] = 0\n\nsubmission.loc[y_pred.sig_id,:] = y_pred[target_columns].values\n\nsubmission.loc[test[test.cp_type == 'ctl_vehicle'].sig_id] = 0\n\nsubmission.to_csv('submission.csv', index=True)","43165a1b":"submission","31e4145c":"# If you Like It Please Upvote It","c679372f":"# 1. MODEL1 CV [0.01562060391771847] LB [0.01833]\n# 2. MODEL2 CV [0.01581384091258266] LB [0.01849]\n# 3. MODEL3 CV [0.01590705560544343] LB [0.01850]","fdba24f0":"# Preprocessing steps","55339fb7":"# Dataset Classes","011b7957":"# Model","2ed585f9":"# Single fold training","875ec615":"# BLEND LB 0.01825"}}