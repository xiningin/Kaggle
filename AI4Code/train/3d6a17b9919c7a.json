{"cell_type":{"e2fea21b":"code","6d064a64":"code","217697a9":"code","fcd028a0":"code","cbcf07d3":"code","7e458976":"code","cfd2705f":"code","31057ee8":"code","65d05e29":"code","0267489b":"code","5b6c493f":"code","6b1a7328":"code","86814c42":"code","1a11a08e":"code","47e5f811":"code","3ed8b654":"code","6b283c8c":"code","5d7a3607":"code","d3a66fdc":"code","3efd0f50":"code","bb5b56fb":"code","5439a5a5":"code","7250dd4b":"code","57117fba":"code","f179699d":"code","1bb75657":"code","047c1ca4":"code","2325854f":"code","0f3f80ad":"code","03be6458":"code","697fbeeb":"code","323c62e7":"code","afc4c364":"code","8a1963b7":"code","a28b1355":"code","bd78492d":"code","765231cd":"code","7c5b4e87":"code","3bef21b9":"code","08fc347a":"code","7f81044a":"code","70b0a39d":"code","12e6a0b8":"code","5c1edd43":"code","5bd78658":"code","564a60d9":"markdown","e2cf07d2":"markdown","81fae195":"markdown","64bc80f9":"markdown","b5a63f03":"markdown","19b9db56":"markdown","ed25a385":"markdown","87243bf2":"markdown","124170ee":"markdown","4af909a2":"markdown","8f2b798e":"markdown","952c0ca2":"markdown","99b728ee":"markdown","41cdca28":"markdown","481cee51":"markdown","662e308b":"markdown","a45c0319":"markdown","9a1692b3":"markdown","24b8e783":"markdown","53fce5f3":"markdown","9c51f2f9":"markdown","7eb18839":"markdown","4c8fb1fc":"markdown","ed0a5592":"markdown","13caeff4":"markdown","a702bf87":"markdown","a38ec023":"markdown","fb624a02":"markdown"},"source":{"e2fea21b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso,Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom mlxtend.regressor import StackingRegressor\nimport xgboost as xgb\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal point","6d064a64":"train=pd.read_csv('..\/input\/train.csv')\ntest=pd.read_csv('..\/input\/test.csv')","217697a9":"print(\"Train : \"+str(train.shape))\n\n#checking for duplicates\nidUn = len(set(train.Id))\nidTo = train.shape[0]\nidDup = idTo - idUn\nprint(str(idDup)+\" duplicates available in this dataset\")","fcd028a0":"#Select the Numerical & Categorical Features\n\nnumerical_features = train.select_dtypes(exclude = ['object']).columns\ncategorical_features = train.select_dtypes(include = ['object']).columns","cbcf07d3":"# Plotting the numerical columns\nfig = plt.figure(figsize = (15,15))\nax = fig.gca()\ntrain[numerical_features].hist(ax=ax,edgecolor=\"black\")\nfig.tight_layout()\nfig.show()\n\nfig.savefig('numeric_hist.png')","7e458976":"#plot the Numeric columns against SalePrice Using ScatterPlot\nfig = plt.figure(figsize=(30,50))\nfor i, col in enumerate(numerical_features[1:]):\n    fig.add_subplot(11,4,1+i)\n    plt.scatter(train[col], train['SalePrice'])\n    plt.xlabel(col)\n    plt.ylabel('SalePrice')\nfig.tight_layout()\nfig.show()\n\nfig.savefig('numeric_scatter.png')","cfd2705f":"fig = plt.figure(figsize=(15,50))\nfor i, col in enumerate(categorical_features):\n    fig.add_subplot(11,4,1+i)\n    train.groupby(col).mean()['SalePrice'].plot.bar(yerr = train.groupby(col).std())\nfig.tight_layout()\nfig.show()\n\nfig.savefig('categorical_bar.png')","31057ee8":"#Checking for Outliers\n\nplt.scatter(train.GrLivArea,train.SalePrice, c= \"blue\" , marker = \"s\")\nplt.title(\"Looking for Outlier\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","65d05e29":"#remove the Outliers\n\ntrain = train[train['GrLivArea']<4500]\ntrain.shape","0267489b":"#Outlier has been removed\n\nplt.scatter(train.GrLivArea,train.SalePrice)\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.show()","5b6c493f":"train_ID = train['Id']\ntest_ID = test['Id']\n\n#Delete the ID Column\ntrain.drop('Id',axis=1,inplace = True)\ntest.drop('Id', axis=1, inplace = True)\n\n#After dropping Id Column\nprint(\"Train Data: \"+str(train.shape))\nprint(\"Test Data: \"+str(test.shape))","6b1a7328":"train['SalePrice'].describe()","86814c42":"sns.distplot(train['SalePrice'])","1a11a08e":"#Skewness & Kurtosis\n\nprint(\"Skewness : %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","47e5f811":"from scipy import stats\nfrom scipy.stats import norm\n#Normal Distribution of Sales Price\nmu, sigma = norm.fit(train['SalePrice'])\nprint(\"Mu : {:.2f}\\nSigma : {:.2f}\".format(mu,sigma))\n\n#Visualization\nsns.distplot(train['SalePrice'],fit=norm);\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\Sigma=$ {:.2f})'.format(mu,sigma)],loc = 'best')\nplt.xlabel('SalePrice Distribution')\nplt.ylabel('Frequency')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'],plot=plt)\nplt.show()","3ed8b654":"train['SalePrice'] = np.log1p(train['SalePrice'])\n\n#Normal Distribution of New Sales Price\nmu, sigma = norm.fit(train['SalePrice'])\nprint(\"Mu : {:.2f}\\nSigma : {:.2f}\".format(mu,sigma))\n\n#Visualization\nsns.distplot(train['SalePrice'],fit=norm);\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\Sigma=$ {:.2f})'.format(mu,sigma)],loc = 'best')\nplt.xlabel('SalePrice Distribution')\nplt.ylabel('Frequency')\n\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'],plot=plt)\nplt.show()","6b283c8c":"train_n = train.shape[0]\ntest_n = test.shape[0]\nprint(test_n)\ny = train.SalePrice.values\nall_data = pd.concat((train,test)).reset_index(drop = True)\nall_data.drop(['SalePrice'], axis=1, inplace = True)\nprint(\"all_data size is : {}\".format(all_data.shape))","5d7a3607":"all_data_na_values = all_data.isnull().sum()\nall_data_na_values = all_data_na_values.drop(all_data_na_values[all_data_na_values == 0].index).sort_values(ascending=False)[:30]\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na,'Missing Values' :all_data_na_values})\nmissing_data.head(20)","d3a66fdc":"f, ax = plt.subplots(figsize = (15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index,y=all_data_na)\nplt.xlabel('Features',fontsize=15)\nplt.ylabel('Percent of Missing Values', fontsize=15)\nplt.title('% of Misssing data by Features', fontsize=15)\n\nfig.savefig('Missing_data.png')","3efd0f50":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(30,20))\nsns.heatmap(corrmat, vmax=0.9, square=True, annot=True, fmt=\".2f\")","bb5b56fb":"# Fill the PoolQC Values\n\nall_data['PoolQC'] = all_data['PoolQC'].fillna(\"None\")\n\n# Fill the MiscFeature Values\n\nall_data['MiscFeature'] = all_data['MiscFeature'].fillna(\"None\")\n\n# Fill the Alley Values\n\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n\n# Fill the Fence Values\n\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n\n# Fill the FireplaceQu Values\n\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n# Fill the LotFrontage Values\n\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# Fill the GarageType, GarageFinish,  GarageQual , GarageCond Values\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\n# Fill the GarageYrBlt, GarageArea,  GarageQual , GarageCars Values\n\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \n# Fill the BsmtCond, BsmtExposure,  BsmtQual , BsmtFinType1, BsmtFinType2  \n\nfor col in ('BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1'):\n    all_data[col] = all_data[col].fillna('None')\n\n    \n# Fill the BsmtHalfBath, BsmtFullBath,  BsmtUnfSF , BsmtFinSF1,BsmtFinSF1, TotalBsmtSF\nfor col in ('BsmtHalfBath', 'BsmtFullBath', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1'):\n    all_data[col] = all_data[col].fillna(0)\n\n# Fill the MasVnrType Values\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n\n\n# Fill the MasVnrArea Values\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n\n# Fill the MSZoning Values\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n#Utilities is not so needed,So we have to drop it\nall_data = all_data.drop(['Utilities'], axis=1)\n\n# Fill the Functional Values\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n\n# Fill the Electrical Values\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n\n# Fill the KitchenQual Values\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n\n# Fill the Exterior1st, Exterior2nd Values\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n\n# Fill the SaleType Values\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","5439a5a5":"all_data_na_values = all_data.isnull().sum()\nall_data_na_values = all_data_na_values.drop(all_data_na_values[all_data_na_values == 0].index).sort_values(ascending=False)[:30]\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na,'Missing Values' :all_data_na_values,'Data_type':all_data_na.dtype})\nmissing_data.head()","7250dd4b":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n","57117fba":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n","f179699d":"#Adding Total sqfoot feature\nall_data['TotalSF'] = all_data['TotalBsmtSF']+all_data['1stFlrSF']+all_data['2ndFlrSF']","1bb75657":"from scipy.stats import skew\nnum = all_data.dtypes[all_data.dtypes != 'object'].index\n\n#Skew all the Numerical Features\nskew_feat = all_data[num].apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\n\nsk = pd.DataFrame({'Skewness' :skew_feat})\nsk.head(10)","047c1ca4":"sk_new = sk[abs(sk) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(sk_new.shape[0]))\n\nfrom scipy.special import boxcox1p\nsk_feat = sk_new.index\nlam = 0.15\nfor feat in sk_feat:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","2325854f":"#should\/need to define categorical columns list\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","0f3f80ad":"train_new = all_data[:train_n]\ntest_new = all_data[train_n:]\ny_train = y[:train_n]\ny_test = y[:test_n]\nprint(train_new.shape)\nprint(test_new.shape)\nprint(y_train.shape)\nprint(y_test.shape)","03be6458":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.2,gamma=0.0,learning_rate=0.01,max_depth=4,min_child_weight=1.5,\n                 n_estimators=7200,reg_alpha=0.9,reg_lambda=0.6,subsample=0.2,seed=42,silent=1)\nmodel_xgb.fit(train_new,y_train)\ny_pred_xgb = model_xgb.predict(train_new)\nscore_xgb = np.sqrt(mean_squared_error(y_train, y_pred_xgb))\nprint(\"XGB Score :\",score_xgb)\n\ny_pred_xgb_test = model_xgb.predict(test_new)\ny_pred_xgb_test = np.exp(y_pred_xgb_test)","697fbeeb":"pred_df_xgb = pd.DataFrame(y_pred_xgb_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_xgb.to_csv('output_HPO_XGB.csv', header=True, index_label='Id')","323c62e7":"model_lasso = Lasso(alpha =0.0005, random_state=1)\nmodel_lasso.fit(train_new,y_train)\ny_pred_lasso = model_lasso.predict(train_new)\nscore_lasso = np.sqrt(mean_squared_error(y_train, y_pred_lasso))\nprint(\"Lasso Score(On Training DataSet) :\",score_lasso)\n\ny_pred_lasso_test = model_lasso.predict(test_new)\ny_pred_lasso_test = np.exp(y_pred_lasso_test)","afc4c364":"pred_df_lasso = pd.DataFrame(y_pred_lasso_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_lasso.to_csv('output_HPO_Lasso.csv', header=True, index_label='Id')","8a1963b7":"model_rd = Ridge(alpha = 4.84)\nmodel_rd.fit(train_new,y_train)\ny_pred_rd = model_rd.predict(train_new)\nscore_rd = np.sqrt(mean_squared_error(y_train, y_pred_rd))\nprint(\"Ridge Score(On Training DataSet) :\",score_rd)\n\ny_pred_rd_test = model_rd.predict(test_new)\ny_pred_rd_test = np.exp(y_pred_rd_test)","a28b1355":"pred_df_rd = pd.DataFrame(y_pred_rd_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_rd.to_csv('output_HPO_RD.csv', header=True, index_label='Id')","bd78492d":"model_enet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nmodel_enet.fit(train_new,y_train)\ny_pred_enet = model_enet.predict(train_new)\nscore_enet = np.sqrt(mean_squared_error(y_train, y_pred_enet))\nprint(\"ElasticNet Score(On Training DataSet)  :\",score_enet)\n\ny_pred_enet_test = model_enet.predict(test_new)\ny_pred_enet_test = np.exp(y_pred_enet_test)","765231cd":"pred_df_enet = pd.DataFrame(y_pred_enet_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_enet.to_csv('output_HPO_ENET.csv', header=True, index_label='Id')","7c5b4e87":"model_rf = RandomForestRegressor(n_estimators = 12,max_depth = 3,n_jobs = -1)\nmodel_rf.fit(train_new,y_train)\ny_pred_rf = model_rf.predict(train_new)\nscore_rf = np.sqrt(mean_squared_error(y_train, y_pred_rf))\nprint(\"RandomForest Score(On Training DataSet)  :\",score_rf)\n\ny_pred_rf_test = model_rf.predict(test_new)\ny_pred_rf_test = np.exp(y_pred_rf_test)","3bef21b9":"pred_df_rf = pd.DataFrame(y_pred_rf_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_rf.to_csv('output_HPO_RF.csv', header=True, index_label='Id')","08fc347a":"model_gb = GradientBoostingRegressor(n_estimators = 40,max_depth = 2)\nmodel_gb.fit(train_new,y_train)\ny_pred_gb = model_gb.predict(train_new)\nscore_gb = np.sqrt(mean_squared_error(y_train, y_pred_gb))\nprint(\"GradientBoosting Score(On Training DataSet)  :\",score_gb)\n\ny_pred_gb_test = model_gb.predict(test_new)\ny_pred_gb_test = np.exp(y_pred_gb_test)","7f81044a":"pred_df_gb = pd.DataFrame(y_pred_gb_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_gb.to_csv('output_HPO_GB.csv', header=True, index_label='Id')","70b0a39d":"model_nn = MLPRegressor(hidden_layer_sizes = (90, 90),alpha = 2.75)\nmodel_nn.fit(train_new,y_train)\ny_pred_nn = model_nn.predict(train_new)\nscore_nn = np.sqrt(mean_squared_error(y_train, y_pred_nn))\nprint(\"Multi-layer Perceptron Score(On Training DataSet) :\",score_nn)\n\ny_pred_nn_test = model_nn.predict(test_new)\ny_pred_nn_test = np.exp(y_pred_nn_test)","12e6a0b8":"pred_df_nn = pd.DataFrame(y_pred_nn_test, index=test_ID, columns=[\"SalePrice\"])\npred_df_nn.to_csv('output_HPO_MLP.csv', header=True, index_label='Id')","5c1edd43":"lr = LinearRegression(n_jobs = -1)\nmodel_stack = StackingRegressor(regressors=[model_rf, model_gb, model_nn, model_enet,model_lasso,model_rd,model_xgb], meta_regressor=lr)\n\n# Fit the model on our data\nmodel_stack.fit(train_new, y_train)\ny_pred_stack = model_stack.predict(train_new)\nscore_stack = np.sqrt(mean_squared_error(y_train, y_pred_stack))\nprint(\"StackingRegressor Score(On Training DataSet) : \",score_stack)\n\ny_pred_stack_test = model_gb.predict(test_new)\ny_pred_stack_test = np.exp(y_pred_stack_test)","5bd78658":"pred_df = pd.DataFrame(y_pred_stack_test, index=test_ID, columns=[\"SalePrice\"])\npred_df.to_csv('output_HPO_Stack.csv', header=True, index_label='Id')","564a60d9":"<h3>2. Lasso Model<\/h3>","e2cf07d2":"<h3>3. Ridge Model<\/h3>","81fae195":"<h4>Write the Output ElasticNet of into Submission file<\/h4>","64bc80f9":"# Target Variable :- SalePrice\n<h4>b. We need to Predict the SalePrice First<\/h4>","b5a63f03":"<h4>Write the Output GradientBoosting of into Submission file<\/h4>","19b9db56":"<h1>Ensembling all the Regressor<\/h1>","ed25a385":"# Use bar plots to plot categorical features against SalePrice.","87243bf2":"<h4>Getting new training & testing Dataset<\/h4>","124170ee":"<h4>Write the Output Ridge of into Submission file<\/h4>","4af909a2":"<h4>c. Misssing Data<\/h4>","8f2b798e":"# Data Preprocessing\n<h4>a. Checking for Outliers<\/h4>","952c0ca2":"<h3>6. GradientBoosting Model<\/h3>","99b728ee":"<h4>Write the Output MultiLayerPerceptron of into Submission file<\/h4>","41cdca28":"<h3>5. RandomForest Model<\/h3>","481cee51":"<h4>Write the Output XGB of into Submission file<\/h4>","662e308b":"<h3>4. ElasticNet Model<\/h3>","a45c0319":"<h4>Write the Output StackRegressor of into Submission file<\/h4>","9a1692b3":"<h3>7. Multi-layer Perceptron Model<\/h3>","24b8e783":"<h3>1. XGBoost Model<\/h3>","53fce5f3":"<h3>Fill The Missing Data<\/h3>","9c51f2f9":"<h3>Correlation between Columns<\/h3>","7eb18839":"<h4>Write the Output RandomForest of into Submission file<\/h4>","4c8fb1fc":"Let's Concatenate train & test data","ed0a5592":"<h4>Write the Output Lasso of into Submission file<\/h4>","13caeff4":"<h2>**HOUSEPRICE PRDEICTION USING ENSEMBLE TECCHNIQUE**<\/h2>","a702bf87":"<h3>Checking for any Remaining Missing Data<\/h3>","a38ec023":"<h3>Create dummy variables for categorical columns<\/h3>","fb624a02":"# Plotting Numerical Data"}}