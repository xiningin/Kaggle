{"cell_type":{"4a5a7925":"code","67e0d697":"code","a6df30a7":"code","d7f6daaa":"code","9b027264":"code","0e6e37bd":"code","7c739a13":"code","4c24d1d5":"code","e412619d":"code","89265eb1":"code","3fc961e4":"code","bd786e52":"code","6dbea2b8":"code","0cf7f246":"code","bbb88173":"code","5419f29a":"code","963a97e7":"code","19397bc1":"code","91796cef":"code","f2d65871":"code","7d7e7212":"code","fd4e2de7":"code","9b5d43d3":"code","c4296e65":"code","32ba272f":"code","43e6dc3d":"code","2d4de7e9":"code","fb4c2350":"markdown","2e173cbe":"markdown","7d7ee01f":"markdown","1d973c4f":"markdown","de7e35a2":"markdown"},"source":{"4a5a7925":"# ","67e0d697":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6df30a7":"chinese_mnist=pd.read_csv('..\/input\/chinese-mnist\/chinese_mnist.csv')","d7f6daaa":"chinese_mnist","9b027264":"import imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\nfrom matplotlib import pyplot as plt","0e6e37bd":"IMAGE_PATH = '..\/\/input\/\/chinese-mnist\/\/data\/\/data\/\/'\nIMAGE_WIDTH = 64\nIMAGE_HEIGHT = 64\nIMAGE_CHANNELS = 1","7c739a13":"image_files = list(os.listdir(IMAGE_PATH))\nprint(\"# of image files:{}\".format(len(image_files)))","4c24d1d5":"def create_file_name(x):\n    file_name=f\"input_{x[0]}_{x[1]}_{x[2]}.jpg\"\n    return file_name","e412619d":"import copy\ndata_df=copy.copy(chinese_mnist)\ndata_df[\"file\"]=data_df.apply(create_file_name,axis=1)","89265eb1":"data_df.head()","3fc961e4":"def read_image_sizes(path,file_name):\n    image = skimage.io.imread(path+file_name)\n    return list(image.shape)\nm=np.stack(data_df['file'].apply(lambda f:read_image_sizes(IMAGE_PATH,f)))\ndf = pd.DataFrame(m,columns=['w','h'])\ndata_df=pd.concat([data_df,df],axis=1,sort=False)\n    ","bd786e52":"data_df.head()","6dbea2b8":"print(f\"Number of suites: {data_df.suite_id.nunique()}\")\nprint(f\"Samples: {data_df.sample_id.unique()}\")","0cf7f246":"num_images=len(data_df)\nimage_dim=64\n\nimage_array=np.zeros((num_images,image_dim,image_dim))\nfor im_dex in range(num_images):\n    image_array[im_dex,:,:]=\\\n    skimage.io.imread(IMAGE_PATH+data_df['file'][im_dex])","bbb88173":"code_array=np.array(data_df['code'])\n#code_array","5419f29a":"plt.imshow(image_array[1,:,:])\nimage_array[1,:,:]","963a97e7":"import random\nrand_perm=np.random.permutation(len(image_array))\nimage_array=image_array[rand_perm,:,:]\ncode_array=code_array[rand_perm]\ntrain_im=image_array[:int(np.floor(len(image_array)\/2)),:,:]\ntest_im=image_array[int(np.floor(len(image_array)\/2)):,:,:]\ntrain_code=code_array[:int(np.floor(len(image_array)\/2))]\ntest_code=code_array[int(np.floor(len(image_array)\/2)):]\ntrain=np.reshape(train_im,\n                 (int(np.floor(len(image_array)\/2)),64**2))\ntest=np.reshape(test_im,\n                 (int(np.ceil(len(image_array)\/2)),64**2))","19397bc1":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs=-1, n_estimators=300)\nrfc.fit(train,train_code)\nprint('test result: ',rfc.score(test,test_code),'train result: ', rfc.score(train,train_code))","91796cef":"import random\nrand_perm=np.random.permutation(len(image_array))\nimage_array=image_array[rand_perm,:,:]\ncode_array=code_array[rand_perm]\ntrain_im=image_array[:int(np.floor(len(image_array)\/2)),:,:]\ntest_im=image_array[int(np.floor(len(image_array)\/2)):,:,:]\ntrain_code=code_array[:int(np.floor(len(image_array)\/2))]\ntest_code=code_array[int(np.floor(len(image_array)\/2)):]\n\nfrom keras.utils import to_categorical\ntrain_images=train_im.reshape((7500,64,64,1))\ntrain_images=train_images.astype('float32')\ntrain_code=train_code-1\ntrain_labels=to_categorical(train_code)\n\ntest_images=test_im.reshape((7500,64,64,1))\ntest_images=test_images.astype('float32')\ntest_code=test_code-1\ntest_labels=to_categorical(test_code)","f2d65871":"np.shape(train_labels)","7d7e7212":"from keras import models, layers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32,(9,9),activation='relu', \n                        input_shape=(64,64,1)))\n\n\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(9,9),activation='relu'))\n\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(layers.Conv2D(64,(9,9),activation='relu'))\nmodel.add(layers.MaxPooling2D((2,2)))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(15,activation='softmax'))\n#model.add(layers.Flatten())\nmodel.compile(optimizer='rmsprop',\n             loss='categorical_crossentropy',\n             metrics=['accuracy'])\n\nmodel.summary()\n\n","fd4e2de7":"model.fit(train_images,train_labels,epochs=7,batch_size=64)","9b5d43d3":"test_loss,test_acc=model.evaluate(test_images,test_labels)","c4296e65":"rand_perm=np.random.permutation(len(image_array))\nimage_array=image_array[rand_perm,:,:]\ncode_array=code_array[rand_perm]\ntrain_im=image_array[:int(np.floor(len(image_array)\/2)),:,:]\ntest_im=image_array[int(np.floor(len(image_array)\/2)):,:,:]\ntrain_code=code_array[:int(np.floor(len(image_array)\/2))]\ntest_code=code_array[int(np.floor(len(image_array)\/2)):]\ntrain=np.reshape(train_im,\n                 (int(np.floor(len(image_array)\/2)),64**2))\ntest=np.reshape(test_im,\n                 (int(np.ceil(len(image_array)\/2)),64**2))\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nprint('start kmeans')\nKM_model=KMeans(init='k-means++',n_clusters=15,n_init=10)\nKM_model.fit(train) \nprint('end kmeans')\nprint('start PCA')\nreduced_train=PCA(n_components=2).fit_transform(train)\nprint('end PCA')","32ba272f":"plt.plot(reduced_train[:, 0], reduced_train[:, 1], 'k.', markersize=2)","43e6dc3d":"\nh=2\nx_min, x_max = reduced_train[:, 0].min() - 1, reduced_train[:, 0].max() + 1\ny_min, y_max = reduced_train[:, 1].min() - 1, reduced_train[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\nPCA_train=PCA(n_components=2).fit(train)\nproj_train=PCA_train.inverse_transform(np.c_[xx.ravel(),yy.ravel()])\n# Obtain labels for each point in mesh. Use last trained model.\nZ = KM_model.predict(proj_train)","2d4de7e9":"plt.figure(figsize=(10,10))\nZ = Z.reshape(xx.shape)\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=plt.cm.Paired,\n           aspect='auto', origin='lower')\nfor l in range(1,16):\n    plt.plot(reduced_train[train_code==l, 0], reduced_train[train_code==l, 1], 'x', markersize=5)\n","fb4c2350":"# Convnet approach: 97.8% accurate on test","2e173cbe":"# Classification practice\nPractice using \n* Random forest\n* SVM\n* Convnet\n* Precompose with dimensional reduction e.g. PCA\n\n# Summary of results so far:\n* Random forest is not bad considering simplicity and lack of concern\n\n","7d7ee01f":"# Random forest approach: up to 64.6%. You can probably need more but then you will need a lot of trees and computation time.\n\n# Apparently random forest is >90% accurate on Arabic MNIST. Can we find out how to do this in a tractable manner for this data-set.","1d973c4f":"# K-Means- a sparse non-negative matrix factorization\n","de7e35a2":"# Setting up data for model\nThe data we have is a pandas dataset. We wish to form a predictive map\n64x64 np.array -> code\n\nFor the first project. Let's split the dataset into train and test.\n\nWe will now load the images into a 3d np array \n\nnum_images x 64 x 64."}}