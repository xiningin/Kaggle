{"cell_type":{"d46c6500":"code","16a12bb4":"code","713e8d55":"code","7a09801e":"code","88666327":"code","a1d38fcd":"code","3520703e":"code","1b39caf9":"code","fd7b6d97":"code","1264711b":"code","1441d535":"code","cbfc4e2c":"code","4bd6f978":"code","db749e41":"code","c58421e5":"code","41cb568b":"code","76966b18":"code","f86fc495":"code","b21c0522":"code","3f8c1df2":"code","f5197b8f":"code","96fd4845":"code","e96c67ba":"markdown","a28a198e":"markdown","02c021fd":"markdown","8b73c083":"markdown","a5d7cb8c":"markdown","be3874f0":"markdown","7b26e4a0":"markdown","83cd6718":"markdown","681ea150":"markdown","6901d4ae":"markdown","e3d3bf1e":"markdown","caaeba44":"markdown","dfd54eb8":"markdown","ec454e28":"markdown"},"source":{"d46c6500":"!nvidia-smi","16a12bb4":"import sys\nimport os\nIN_COLAB = 'google.colab' in sys.modules\nIN_KAGGLE = 'kaggle_web_client' in sys.modules\nLOCAL = not (IN_KAGGLE or IN_COLAB)\nprint(f'IN_COLAB:{IN_COLAB}, IN_KAGGLE:{IN_KAGGLE}, LOCAL:{LOCAL}')","713e8d55":"# # For Colab Download some datasets\n# # ================================\n# if IN_COLAB:\n#     # mount googledrive\n#     from google.colab import drive\n#     drive.mount('\/content\/drive')\n#     # copy kaggle.json from googledrive\n#     ! pip install --upgrade --force-reinstall --no-deps  kaggle > \/dev\/null\n#     ! mkdir ~\/.kaggle\n#     ! cp \"\/content\/drive\/MyDrive\/kaggle\/kaggle.json\" ~\/.kaggle\/\n#     ! chmod 600 ~\/.kaggle\/kaggle.json\n    \n#     if not os.path.exists(\"\/content\/input\/\"):\n#         !mkdir input\n#         !mkdir input\/features\n#         !kaggle datasets download -d teyosan1229\/ventilator-pressure\n#         !unzip \/content\/ventilator-pressure.zip -d input\/features\n#         !kaggle competitions download -c ventilator-pressure-prediction\n#         !unzip \/content\/ventilator-pressure-prediction.zip -d input","7a09801e":"if IN_KAGGLE or IN_COLAB:\n    !pip install --upgrade -q wandb\n    !pip install -q pytorch-lightning\n    !pip install torch_optimizer","88666327":"# Hide Warning\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n\n# Python Libraries\nimport os\nimport math\nimport random\nimport glob\nimport pickle\nfrom collections import defaultdict\nfrom pathlib import Path\n\n# Third party\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Utilities and Metrics\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.preprocessing import RobustScaler, normalize, QuantileTransformer\nfrom sklearn.metrics import mean_absolute_error #[roc_auc_score, accuracy_score]\n\n# Pytorch \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\nfrom torch.optim.optimizer import Optimizer, required\nimport torch_optimizer as optim\n\n# Pytorch Lightning\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Callback, seed_everything\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.loggers import WandbLogger, CSVLogger\n\n# Weights and Biases Tool\nimport wandb\n#os.environ[\"WANDB_API_KEY\"]='hoge'\n#wandb.login()","a1d38fcd":"class CFG:\n    debug = False\n    competition='ventilator'\n    exp_name = \"public\"\n    seed = 29\n    \n    # data\n    target_col = 'pressure'\n    target_size = 1\n    \n    # optimizer\n    optimizer_name = 'RAdam'#['RAdam', 'sgd']\n    lr = 5e-3\n    weight_decay = 1e-5\n    amsgrad = False\n    \n    # scheduler\n    epochs = 300\n    scheduler = 'CosineAnnealingLR'\n    T_max = 300\n    min_lr = 1e-5\n    # criterion\n    # u_out = 1 \u3092\u8003\u616e\u3057\u306a\u3044Loss\n    criterion_name = 'CustomLoss1'\n    \n    # training\n    train = True\n    inference = True\n    n_fold = 5\n    trn_fold = [0]\n    precision = 16 #[16, 32, 64]\n    grad_acc = 1\n    # DataLoader\n    loader = {\n        \"train\": {\n            \"batch_size\": 1024,\n            \"num_workers\": 0,\n            \"shuffle\": True,\n            \"pin_memory\": True,\n            \"drop_last\": True\n        },\n        \"valid\": {\n            \"batch_size\": 1024,\n            \"num_workers\": 0,\n            \"shuffle\": False,\n            \"pin_memory\": True,\n            \"drop_last\": False\n        }\n    }\n    # pl\n    trainer = {\n        'gpus': 1,\n        'progress_bar_refresh_rate': 1,\n        'benchmark': False,\n        'deterministic': True,\n        }\n    # LSTM\n    num_layers = 4\n\n    cate_cols = ['u_out'] + \\\n                ['u_out_lag','u_out_lag2','u_out_lag3','u_out_lag_back','u_out_lag_back2','u_out_lag_back3'] + \\\n                ['R_20', 'R_5', 'R_50', 'C_10', 'C_20', 'C_50', 'RC_2010', 'RC_2020', 'RC_2050', 'RC_5010', 'RC_5020', 'RC_5050', 'RC_510', 'RC_520', 'RC_550']\n\n    cont_cols =['time_step', 'u_in'] + ['area'] + ['cross', 'cross2'] + ['u_in_cumsum', 'u_in_cummean'] + \\\n               ['u_in_lag','u_in_lag2','u_in_lag3','u_in_lag_back','u_in_lag_back2','u_in_lag_back3'] + \\\n               ['breath_time', 'u_in_time'] + ['u_out0_mean', 'u_out0_max', 'u_out0_std', 'u_out1_mean', 'u_out1_max', 'u_out1_std'] + \\\n               ['u_in_lag1_diff', 'u_in_lag2_diff','u_in_lag3_diff', 'u_in_lag4_diff'] + \\\n               ['u_in_rolling_mean2', 'u_in_rolling_mean4','u_in_rolling_mean10', 'u_in_rolling_max2', 'u_in_rolling_max4', 'u_in_rolling_max10',\n                'u_in_rolling_min2', 'u_in_rolling_min4', 'u_in_rolling_min10', 'u_in_rolling_std2', 'u_in_rolling_std4', 'u_in_rolling_std10'] + \\\n               ['breath_id__u_in__max', 'breath_id__u_out__max']\n\n    feature_cols = cate_cols + cont_cols\n    dense_dim = 512\n    hidden_size = 512\n    logit_dim = 512\n    \nseed_everything(CFG.seed)\nif not LOCAL:\n    CFG.loader[\"train\"][\"num_workers\"] = 4\n    CFG.loader[\"valid\"][\"num_workers\"] = 4","3520703e":"len(CFG.feature_cols),CFG.loader[\"train\"][\"num_workers\"]","1b39caf9":"if IN_KAGGLE:\n    INPUT_DIR = Path('..\/input\/ventilator-pressure-prediction')\n    FEAT_DIR = Path('..\/input\/ventilator-pressure')\n    OUTPUT_DIR = '.\/'\nelif IN_COLAB:\n    INPUT_DIR = Path('\/content\/input\/')\n    FEAT_DIR = Path('\/content\/input\/features\/')\n    OUTPUT_DIR = f'\/content\/drive\/MyDrive\/kaggle\/Ventilator Pressure\/{CFG.exp_name}\/'\nif LOCAL:\n    INPUT_DIR = Path(\"F:\/Kaggle\/ventilator-pressure-prediction\/data\/input\/\")\n    FEAT_DIR = Path(\"F:\/Kaggle\/ventilator-pressure-prediction\/data\/input\/features\/\")\n    OUTPUT_DIR = f'F:\/Kaggle\/ventilator-pressure-prediction\/data\/output\/{CFG.exp_name}\/'\n    \ndef load_datasets(feats):\n    dfs = [pd.read_feather(FEAT_DIR \/ f'{f}_train.ftr') for f in feats]\n    X_train = pd.concat(dfs, axis=1)\n    dfs = [pd.read_feather(FEAT_DIR \/ f'{f}_test.ftr') for f in feats]\n    X_test = pd.concat(dfs, axis=1)\n    return X_train, X_test\n\n\n# you can use own feature engineeringed data\nfeats = ['Base', 'Area', 'Cross', 'U_in_cumsum_mean', 'U_in_Lag', 'U_out_Lag', 'RC_OHE', 'U_out_stat', 'Time', 'U_in_Lag_Diff', 'U_in_Rolling', 'U_inout_max']\ndf_train, df_test = load_datasets(feats)\ndf_oof = df_train[[\"id\",\"breath_id\",\"u_out\", \"pressure\", \"fold\"]].copy()\n\nsubmission = pd.read_csv(INPUT_DIR \/ \"sample_submission.csv\")\ndisplay(df_train.head())\ndisplay(df_test.head())\n\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nif CFG.debug:\n    CFG.epochs = 5\n    #CFG.inference = False\n    #df_train = df_train.head(240000)","fd7b6d97":"df_train.to_csv('train.csv',index=False)\ndf_test.to_csv('test.csv',index=False)","1264711b":"# # LINE\u306b\u901a\u77e5\n# import requests\n# def send_line_notification(message):\n#     env = \"\"\n#     if IN_COLAB: env = \"colab\"\n#     elif IN_KAGGLE: env = \"kaggle\"\n#     elif LOCAL: env = \"local\"\n        \n#     line_token = 'hoge'\n#     endpoint = 'https:\/\/notify-api.line.me\/api\/notify'\n#     message = f\"[{env}]{message}\"\n#     payload = {'message': message}\n#     headers = {'Authorization': 'Bearer {}'.format(line_token)}\n#     requests.post(endpoint, data=payload, headers=headers)","1441d535":"# df_train[\"fold\"] = -1\n# Fold = GroupKFold(n_splits=CFG.n_fold)\n# for n, (train_index, val_index) in enumerate(Fold.split(df_train, df_train[CFG.target_col], groups=df_train.breath_id.values)):\n#      df_train.loc[val_index, 'fold'] = int(n)\n# df_train['fold'] = df_train['fold'].astype(int)\n# df_oof = df_train[[\"id\",\"breath_id\",\"u_out\", \"pressure\", \"fold\"]].copy()\n# print(df_train.groupby(['fold', 'breath_id']).size())","cbfc4e2c":"class TrainDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n        self.u_out = self.X[:,:,2]\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        x = torch.FloatTensor(self.X[idx])\n        u_out = torch.LongTensor(self.u_out[idx])\n        label = torch.FloatTensor(self.y[idx]).squeeze(1)\n        return x, u_out, label\n    \nclass TestDataset(Dataset):\n    def __init__(self, X):\n        self.X = X\n        self.u_out = self.X[:,:,2]\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return torch.FloatTensor(self.X[idx])","4bd6f978":"for col in tqdm(CFG.cont_cols):\n    qt = QuantileTransformer(random_state=0, output_distribution='normal')\n    df_train[[col]] = qt.fit_transform(df_train[[col]])\n    df_test[[col]] = qt.transform(df_test[[col]])\n#display(df_train.head())\n#display(df_test.head())\n#df_train.describe().T","db749e41":"X = np.float32(df_train[CFG.feature_cols]).reshape(-1, 80, len(CFG.feature_cols))\ntest_X = np.float32(df_test[CFG.feature_cols]).reshape(-1, 80, len(CFG.feature_cols))\ny = np.float32(df_train[\"pressure\"]).reshape(-1, 80, 1)\nFold = np.int16(df_train[\"fold\"]).reshape(-1, 80, 1)\nFold = Fold.mean(axis=1).flatten()\nprint(X.shape, y.shape, test_X.shape, Fold.shape)","c58421e5":"# X, u_out, y \u306b\u306a\u3063\u3066\u3044\u308b\u304b\u78ba\u8a8d\nds = TrainDataset(X,y)\nfor i in range(3):\n    print(\"=\"*50)\n    print(ds[0][i])\ndel ds","41cb568b":"class DataModule(pl.LightningDataModule):\n    \"\"\"\n    numpy array\u3067\u53d7\u3051\u53d6\u308b\n    \"\"\"\n    def __init__(self, tr_X, tr_y, val_X, val_y, test_X, cfg):\n        super().__init__()\n        self.train_data = tr_X\n        self.train_label = tr_y\n        self.valid_data = val_X\n        self.valid_label = val_y\n        self.test_data = test_X\n        self.cfg = cfg\n        \n    def setup(self, stage=None):\n        self.train_dataset = TrainDataset(self.train_data, self.train_label)\n        self.valid_dataset = TrainDataset(self.valid_data, self.valid_label)\n        self.test_dataset = TestDataset(self.test_data)\n        \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, **self.cfg.loader['train'])\n\n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset, **self.cfg.loader['valid'])\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, **self.cfg.loader['valid'])","76966b18":"# ====================================================\n# model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.dense_dim = cfg.dense_dim\n        self.hidden_size = cfg.hidden_size\n        self.num_layers = cfg.num_layers\n        self.logit_dim = cfg.logit_dim\n        self.mlp = nn.Sequential(\n            nn.Linear(len(cfg.feature_cols), self.dense_dim \/\/ 2),\n            nn.ReLU(),\n            #nn.Dropout(0.2),\n            nn.Linear(self.dense_dim \/\/ 2, self.dense_dim),\n            nn.ReLU(),\n        )\n        self.lstm1 = nn.LSTM(self.dense_dim, self.dense_dim,\n                            dropout=0., batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(self.dense_dim * 2, self.dense_dim\/\/2,\n                            dropout=0., batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(self.dense_dim\/\/2 * 2, self.dense_dim\/\/4,\n                            dropout=0., batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(self.dense_dim\/\/4 * 2, self.dense_dim\/\/8,\n                            dropout=0., batch_first=True, bidirectional=True)\n        self.head = nn.Sequential(\n            nn.LayerNorm(self.hidden_size\/\/8 * 2),\n            nn.GELU(),\n            nn.Linear(self.hidden_size\/\/8 * 2, 1),\n        )\n        # LSTM\u3084GRU\u306f\u76f4\u4ea4\u884c\u5217\u306b\u521d\u671f\u5316\u3059\u308b\n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n            elif isinstance(m, nn.GRU):\n                print(f\"init {m}\")\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.mlp(x)\n        features, _ = self.lstm1(features)\n        features, _ = self.lstm2(features)\n        features, _ = self.lstm3(features)\n        features, _ = self.lstm4(features)\n        output = self.head(features).view(bs, -1)\n        return output\n    \ndef get_model(cfg):\n    model = CustomModel(cfg)\n    return model\n\n# ====================================================\n# criterion\n# ====================================================\ndef compute_metric(df, preds):\n    \"\"\"\n    Metric for the problem, as I understood it.\n    \"\"\"\n    \n    y = np.array(df['pressure'].values.tolist())\n    w = 1 - np.array(df['u_out'].values.tolist())\n    \n    assert y.shape == preds.shape and w.shape == y.shape, (y.shape, preds.shape, w.shape)\n    \n    mae = w * np.abs(y - preds)\n    mae = mae.sum() \/ w.sum()\n    \n    return mae\n\n\nclass VentilatorLoss(nn.Module):\n    \"\"\"\n    Directly optimizes the competition metric\n    \"\"\"\n    def __call__(self, preds, y, u_out):\n        w = 1 - u_out\n        mae = w * (y - preds).abs()\n        mae = mae.sum(-1) \/ w.sum(-1)\n\n        return mae\n\ndef get_criterion():\n    if CFG.criterion_name == 'BCEWithLogitsLoss':\n        criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")\n    if CFG.criterion_name == 'CrossEntropyLoss':\n        criterion = nn.CrossEntropyLoss()\n    if CFG.criterion_name == 'CustomLoss1':\n        # [reference]https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter-simple-lstm\n        criterion = VentilatorLoss()\n    else:\n        raise NotImplementedError\n    return criterion\n# ====================================================\n# optimizer\n# ====================================================\ndef get_optimizer(model: nn.Module, config: dict):\n    \"\"\"\n    input:\n    model:model\n    config:optimizer_name\u3084lr\u304c\u5165\u3063\u305f\u3082\u306e\u3092\u6e21\u3059\n    \n    output:optimizer\n    \"\"\"\n    optimizer_name = config.optimizer_name\n    if 'Adam' == optimizer_name:\n        return Adam(model.parameters(),\n                    lr=config.lr,\n                    weight_decay=config.weight_decay,\n                    amsgrad=config.amsgrad)\n    elif 'RAdam' == optimizer_name:\n        return optim.RAdam(model.parameters(),\n                           lr=config.lr,\n                           weight_decay=config.weight_decay)\n    elif 'sgd' == optimizer_name:\n        return SGD(model.parameters(),\n                   lr=config.lr,\n                   momentum=0.9,\n                   nesterov=True,\n                   weight_decay=config.weight_decay,)\n    else:\n        raise NotImplementedError\n\n# ====================================================\n# scheduler\n# ====================================================\ndef get_scheduler(optimizer):\n    if CFG.scheduler=='ReduceLROnPlateau':\n        \"\"\"\n        factor : \u5b66\u7fd2\u7387\u306e\u6e1b\u8870\u7387\n        patience : \u4f55\u30b9\u30c6\u30c3\u30d7\u5411\u4e0a\u3057\u306a\u3051\u308c\u3070\u6e1b\u8870\u3059\u308b\u304b\u306e\u5024\n        eps : nan\u3068\u304bInf\u56de\u907f\u7528\u306e\u5fae\u5c0f\u6570\n        \"\"\"\n        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n    elif CFG.scheduler=='CosineAnnealingLR':\n        \"\"\"\n        T_max : 1 \u534a\u5468\u671f\u306e\u30b9\u30c6\u30c3\u30d7\u30b5\u30a4\u30ba\n        eta_min : \u6700\u5c0f\u5b66\u7fd2\u7387(\u6975\u5c0f\u5024)\n        \"\"\"\n        scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n    elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n        \"\"\"\n        T_0 : \u521d\u671f\u306e\u7e70\u308a\u304b\u3048\u3057\u56de\u6570\n        T_mult : \u30b5\u30a4\u30af\u30eb\u306e\u30b9\u30b1\u30fc\u30eb\u500d\u7387\n        \"\"\"\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n    else:\n        raise NotImplementedError\n    return scheduler","f86fc495":"class Trainer(pl.LightningModule):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.model = get_model(cfg)\n        self.criterion = get_criterion()\n    \n    def forward(self, x):\n        output = self.model(x)\n        return output\n    \n    def training_step(self, batch, batch_idx):\n        x, u_out, y = batch\n        output = self.forward(x)\n        labels = y\n        loss = self.criterion(output, labels ,u_out).mean()\n        \n        self.log('train_loss', loss, on_step=True, prog_bar=True, logger=True)\n        return {\"loss\": loss, \"predictions\": output, \"labels\": labels}\n    \n    def training_epoch_end(self, outputs):\n        self.log(\"lr\", self.optimizer.param_groups[0]['lr'], prog_bar=True, logger=True)\n    \n    def validation_step(self, batch, batch_idx):\n        x, u_out, y = batch\n        output = self.forward(x)\n        labels = y\n        loss = self.criterion(output,labels ,u_out).mean()\n        self.log('val_loss', loss, on_step= True, prog_bar=True, logger=True)\n        return {\"predictions\": output,\n                \"labels\": labels,\n                \"loss\": loss.item()}\n    \n    def validation_epoch_end(self, outputs):\n        preds = []\n        labels = []\n        loss = 0\n        for output in outputs:\n            preds += output['predictions']\n            labels += output['labels']\n            loss += output['loss']\n\n        labels = torch.stack(labels)\n        preds = torch.stack(preds)\n        loss = loss\/len(outputs)\n        \n        self.log(\"val_loss_epoch\", loss, prog_bar=True, logger=True)\n        \n    def predict_step(self, batch, batch_idx):\n        x = batch\n        output = self.forward(x)\n        return output\n        \n    def test_step(self, batch, batch_idx):\n        x = batch       \n        output = self.forward(x)\n        return output\n    \n    def configure_optimizers(self):\n        self.optimizer = get_optimizer(self, self.cfg)\n        self.scheduler = {'scheduler': get_scheduler(self.optimizer),\n                          'interval': 'step', # or 'epoch'\n                          'frequency': 1}\n        return {'optimizer': self.optimizer, 'lr_scheduler': self.scheduler}","b21c0522":"def train() -> None:\n    for fold in range(CFG.n_fold):\n        if not fold in CFG.trn_fold:\n            continue\n        print(f\"{'='*38} Fold: {fold} {'='*38}\")\n        # Logger\n        #======================================================\n        lr_monitor = LearningRateMonitor(logging_interval='step')\n        \n        loss_checkpoint = ModelCheckpoint(\n            dirpath=OUTPUT_DIR,\n            filename=f\"best_loss_fold{fold}\",\n            monitor=\"val_loss\",\n            save_last=True,\n            save_top_k=1,\n            save_weights_only=True,\n            mode=\"min\",\n        )\n        \n#         wandb_logger = WandbLogger(\n#             project=f'{CFG.competition}',\n#             group= f'{CFG.exp_name}',\n#             name = f'Fold{fold}',\n#             save_dir=OUTPUT_DIR\n#         )\n        data_module = DataModule(X[Fold!=fold], y[Fold!=fold],\n                                 X[Fold==fold], y[Fold==fold],\n                                 test_X,\n                                 CFG\n                                )\n        data_module.setup()\n        \n        CFG.T_max = int(math.ceil(len(data_module.train_dataloader())\/CFG.grad_acc)*CFG.epochs)\n        print(f\"set schedular T_max {CFG.T_max}\")\n        #early_stopping_callback = EarlyStopping(monitor='val_loss_epoch',mode=\"min\", patience=5)\n        \n        trainer = pl.Trainer(\n#             logger=wandb_logger,\n            callbacks=[loss_checkpoint],#lr_monitor,early_stopping_callback\n            default_root_dir=OUTPUT_DIR,\n            accumulate_grad_batches=CFG.grad_acc,\n            max_epochs=CFG.epochs,\n            precision=CFG.precision,\n            **CFG.trainer\n        )\n        # \u5b66\u7fd2\n        model = Trainer(CFG)\n        trainer.fit(model, data_module)\n        torch.save(model.model.state_dict(),OUTPUT_DIR + '\/' + f'{CFG.exp_name}_fold{fold}.pth')\n        \n        del model, data_module\n        \n        if CFG.inference:\n            data_module = DataModule(X[0:1], y[0:1], X[0:1], y[0:1], test_X, CFG)\n            data_module.setup()\n            # Road best loss model\n            best_model = Trainer.load_from_checkpoint(cfg=CFG,checkpoint_path=loss_checkpoint.best_model_path)\n            predictions = trainer.predict(best_model, data_module.test_dataloader())\n            preds = []\n            for p in predictions:\n                preds += p\n            preds = torch.stack(preds).flatten()\n            submission['pressure'] = preds.to('cpu').detach().numpy()\n            submission.to_csv(OUTPUT_DIR + '\/' + f'submission_fold{fold}.csv',index=False)\n            \n            # oof\n            data_module = DataModule(X[0:1], y[0:1], X[0:1], y[0:1], X[Fold==fold], CFG)\n            data_module.setup()\n            predictions = trainer.predict(best_model, data_module.test_dataloader())\n            preds = []\n            for p in predictions:\n                preds += p\n            preds = torch.stack(preds).flatten()\n            df_oof.loc[df_oof[\"fold\"] == fold, ['pred']] = preds.to('cpu').detach().numpy()\n            df_oof.to_csv(OUTPUT_DIR + '\/' + f'oof.csv',index=False)\n        \n        wandb.finish()\n\n        ","3f8c1df2":"train()\n#send_line_notification(\"finished\")\nwandb.finish()","f5197b8f":"# oof = pd.read_csv(OUTPUT_DIR + 'oof.csv')\n# cv = compute_metric(oof,oof[\"pred\"])\n# print(f\"cv:{cv}\")","96fd4845":"# # median\n# submission = pd.read_csv(INPUT_DIR \/ \"sample_submission.csv\")\n# preds = [pd.read_csv(f'.\/exp037sub_fold{i}.csv') for i in range(5)]\n# print(preds[0].pressure[0],preds[1].pressure[0],preds[2].pressure[0],preds[3].pressure[0],preds[4].pressure[0])\n# submission['pressure'] = np.median([preds[0].pressure,preds[1].pressure,preds[2].pressure,preds[3].pressure,preds[4].pressure],axis=0)\n# submission.to_csv('.\/exp037_5fold_median.csv',index=False)\n# submission.head()","e96c67ba":"## Import Libraries","a28a198e":"### reference\n\nThank you for publishing a very educational notebook\n\n+ https:\/\/www.kaggle.com\/yasufuminakama\/ventilator-pressure-lstm-starter\n+ https:\/\/www.kaggle.com\/hirayukis\/pytorch-lstm-cv-0-1942-lb-0-193\n+ https:\/\/www.kaggle.com\/artgor\/ventilator-pressure-prediction-eda-fe-and-models\n+ https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter-simple-lstm\n+ https:\/\/www.kaggle.com\/cdeotte\/ensemble-folds-with-median-0-153","02c021fd":"## Dataset","8b73c083":"## Utils","a5d7cb8c":"## Train","be3874f0":"## Rankgauss & Reshape\npandas.DataFrame\u304b\u3089numpy\u306b\u5909\u63db\u3001\u30b7\u30fc\u30b1\u30f3\u30b9\u5206\u3092\u307e\u3068\u3081\u308b","7b26e4a0":"## Config","83cd6718":"## Pytorch Lightning Module","681ea150":"## DataModule","6901d4ae":"## Transforms","e3d3bf1e":"## About this notebook\n\n+ using PyTorch Lightning\n+ wandb\n    + However, I don't know how to hide the API KEY, so I commented it out.\n+ Can be run on kaggle, google colab, local machine\n+ I'm using the features of a private dataset, but I've output the data :)\n+ my LOCAL CV\n  + fold0:0.2326\n  + fold1:0.221\n  + fold2:0.2289\n  + fold3:0.2295\n  + fold4:0.2263\n  + Ensemble Folds with MEDIAN:0.192\n  \n### sorry. You may get a cpu memory error\n\nIt may be solved by not making oof","caaeba44":"## CV Split","dfd54eb8":"## Directory & LoadData","ec454e28":"## Get env"}}