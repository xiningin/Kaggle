{"cell_type":{"90373a3b":"code","48827072":"code","fbff519a":"code","c8f6cb39":"code","a6b05d43":"code","0a6a7ee8":"code","e19ac36a":"code","da5cbacb":"code","045e9da4":"code","0b487f89":"code","3d720634":"code","47e91868":"code","fc5c8ebc":"code","a5e9211f":"code","1a5b670c":"code","59f3de54":"code","d70f83ed":"code","18b5dbea":"code","ef419301":"code","039649f9":"code","0bb0f591":"code","07c6113e":"code","dcb687bf":"code","13e1a00e":"code","e4235348":"code","3bd5040b":"code","4799693d":"code","c7a45547":"code","4700c7b5":"code","11d951f2":"code","df658f68":"code","f41e2dfe":"code","78cac27c":"code","fb003d76":"markdown","a05de937":"markdown","6d89c9e8":"markdown","98e68ee4":"markdown","212a2650":"markdown","de4c3d4a":"markdown","5cafcff9":"markdown","cb0554bd":"markdown","82a8f614":"markdown","12206bb4":"markdown","fc816258":"markdown","aece4f0f":"markdown","53a7ae69":"markdown","a6c68f79":"markdown","452c7065":"markdown","0c406bb7":"markdown","4e01646a":"markdown","ad9cf5f5":"markdown","c6004c4b":"markdown","27d09631":"markdown","080ecfd5":"markdown","011f49d6":"markdown","8e7bcfe3":"markdown","86a79995":"markdown"},"source":{"90373a3b":"# importing necessary data processing libraries\nimport numpy as np\nimport pandas as pd\nimport time\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Importing data visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model libraries\n# import xgboost\nfrom sklearn.svm import SVC\n\n# Hyperparameter tunning libraries\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Model evaluation libraries\nfrom sklearn.metrics import classification_report, matthews_corrcoef","48827072":"data1 = pd.read_csv('..\/input\/prmldatacon\/PRML_Datacontest_RB_Jul_2021\/Dataset_1_Training.csv')\ndata2 = pd.read_csv('..\/input\/prmldatacon\/PRML_Datacontest_RB_Jul_2021\/Dataset_2_Training.csv')\npred_data1 = pd.read_csv('..\/input\/prmldatacon\/PRML_Datacontest_RB_Jul_2021\/Dataset_1_Testing.csv')\npred_data2 = pd.read_csv('..\/input\/prmldatacon\/PRML_Datacontest_RB_Jul_2021\/Dataset_2_Testing.csv')","fbff519a":"data1.head()","c8f6cb39":"data1.tail()","a6b05d43":"pred_data1.head()","0a6a7ee8":"data2.head()","e19ac36a":"data2.tail()","da5cbacb":"pred_data2.head()","045e9da4":"print('train1 : ', data1.shape)\nprint('pred_data1 : ',pred_data1.shape)\nprint('train2 : ',data2.shape)\nprint('pred_data2 : ',pred_data2.shape)","0b487f89":"data1 = data1.T\ndata2 = data2.T\npred_data1 = pred_data1.T\npred_data2 = pred_data2.T","3d720634":"data1.head()","47e91868":"data1.set_index(pd.Series(range(0,131)), inplace = True)\ndata2.set_index(pd.Series(range(0,341)), inplace = True)\npred_data1.set_index(pd.Series(range(0,101)), inplace = True)\npred_data2.set_index(pd.Series(range(0,215)), inplace = True)","fc5c8ebc":"data1.drop([0],inplace = True, axis = 0)\ndata2.drop([0],inplace = True, axis = 0)\npred_data1.drop([0],inplace = True, axis = 0)\npred_data2.drop([0],inplace = True, axis = 0)","a5e9211f":"data1.head()","1a5b670c":"data2.head()","59f3de54":"data1.reset_index(drop=True, inplace=True)\ndata2.reset_index(drop=True, inplace=True)\npred_data1.reset_index(drop=True, inplace=True)\npred_data2.reset_index(drop=True, inplace=True)","d70f83ed":"train1 = data1.drop([22284], axis = 1)\ntrain2 = data1.drop([22283], axis = 1)\ntrain3 = data2.drop([54676,54677, 54678], axis = 1)\ntrain4 = data2.drop([54675,54677, 54678], axis = 1)\ntrain5 = data2.drop([54675,54676, 54678], axis = 1)\ntrain6 = data2.drop([54675,54676, 54677], axis = 1)\n\n# Renaming the target variable\ntrain1.rename({22283 : 'target'}, axis = 1, inplace = True)\ntrain2.rename({22284 : 'target'}, axis = 1, inplace = True)\ntrain3.rename({54675 : 'target'}, axis = 1, inplace = True)\ntrain4.rename({54676 : 'target'}, axis = 1, inplace = True)\ntrain5.rename({54677 : 'target'}, axis = 1, inplace = True)\ntrain6.rename({54678 : 'target'}, axis = 1, inplace = True)\n\n# Making a list of data frames\ntrain = [train1, train2, train3, train4, train5, train6]\ntrain[0]","18b5dbea":"for i in range(6):\n    train[i] = train[i].astype(float)","ef419301":"pred_data1 = pred_data1.astype(float)\npred_data2 = pred_data2.astype(float)\n\ntest_data = [pred_data1, pred_data1, pred_data2, pred_data2, pred_data2, pred_data2]","039649f9":"# train[0].describe()","0bb0f591":"g = sns.catplot(5, col='target', col_wrap=4,\n                data=train[0],\n                kind=\"violin\", height=7, aspect=.8, \n                palette='tab20')\ng.set_axis_labels(\"Target Categories\", \"Count\")\ng.set_titles(\"Relation between 'target' feature and '{col_name}' '{col_var}'\")\nplt.show()","07c6113e":"g = sns.catplot(9, col='target', col_wrap=4,\n                data=train[0],\n                kind=\"strip\", height=7, aspect=.8, \n                palette='tab20')\ng.set_axis_labels(\"Target Categories\", \"Count\")\ng.set_titles(\"Relation between 'target' feature and '{col_name}' '{col_var}'\")\nplt.show()","dcb687bf":"\"\"\"\npca = PCA(n_components = 0.99, svd_solver ='full')\n\nfor i in range(1,2):\n    y = train[i]['target'].tolist()\n    train[i].drop(['target'], axis = 1, inplace=True)\n    train[i] = pd.DataFrame(pca.fit_transform(train[i]))\n    train[i]['target'] = y\n    test_data[i] = pd.DataFrame(pca.transform(test_data[i]))\n    \ntest_data[0] = test_data[1].copy()\n        \ny = train[0]['target'].tolist()\ntrain[0] = train[1].drop(['target'], axis = 1)\ntrain[0]['target'] = y\n\npca = PCA(n_components = 0.99, svd_solver ='full')\n\nfor i in range(2,3):\n    y = train[i]['target'].tolist()\n    train[i].drop(['target'], axis = 1, inplace=True)\n    train[i] = pd.DataFrame(pca.fit_transform(train[i]))\n    train[i]['target'] = y\n    test_data[i] = pd.DataFrame(pca.transform(test_data[i]))\n    \nfor i in range(3,6):\n    y = train[i]['target'].tolist()\n    train[i] = train[2].drop(['target'], axis = 1)\n    train[i]['target'] = y\n    test_data[i] = test_data[2].copy()\n    \nfor i in range(6):\n    print('train'+str(i)+\" : \", str(train[i].shape))\n    print('test'+str(i)+\" : \", str(test_data[i].shape))\n    print(\"\")\n\"\"\"","13e1a00e":"# Combining the data\ncomplete_data1 = pd.concat([train[0].drop(['target'], axis = 1), test_data[0]])\ncomplete_data2 = pd.concat([train[2].drop(['target'], axis = 1), test_data[2]])\n\npca = PCA(n_components = 0.99, svd_solver ='full')\n\ncomplete_data1 = pd.DataFrame(pca.fit_transform(complete_data1))\nfor i in range(0,2):\n    y = train[i]['target'].tolist()\n    train[i].drop(['target'], axis = 1, inplace=True)\n    train[i] = pd.DataFrame(pca.transform(train[i]))\n    test_data[i] = pd.DataFrame(pca.transform(test_data[i]))\n    train[i]['target'] = y\n    \ncomplete_data2 = pd.DataFrame(pca.fit_transform(complete_data2))\nfor i in range(2,6):\n    y = train[i]['target'].tolist()\n    train[i].drop(['target'], axis = 1, inplace=True)\n    train[i] = pd.DataFrame(pca.transform(train[i]))\n    test_data[i] = pd.DataFrame(pca.transform(test_data[i]))\n    train[i]['target'] = y\n    \nfor i in range(6):\n    print('train'+str(i)+\" : \", str(train[i].shape))\n    print('test'+str(i)+\" : \", str(test_data[i].shape))\n    print(\"\")","e4235348":"\"\"\"\nlda = LinearDiscriminantAnalysis()\n\nfor i in range(6):\n    y = train[i]['target'].tolist()\n    train[i].drop(['target'], axis = 1, inplace = True)\n    train[i] = pd.DataFrame(lda.fit_transform(train[i], y))\n    test[i] = pd.DataFrame(lda.transform(test_data[i]))\n    train[i]['target'] = y\n\"\"\"","3bd5040b":"# Splitting the data using stratified split\nsplit = StratifiedShuffleSplit(n_splits=1, test_size = 0.2, random_state = 21)\nX_train = [train1, train2, train3, train4, train5, train6]\nX_test = [train1, train2, train3, train4, train5, train6]\ny_train = [[0]]*6\ny_test = [[0]]*6\nfor i in range(6):\n    for train_index, test_index in split.split(train[i], train[i][\"target\"]):\n        X_train[i] = train[i].loc[train_index]\n        X_test[i] = train[i].loc[test_index]\n    \n    y_train[i] = X_train[i]['target'].tolist()\n    y_test[i] = X_test[i]['target'].tolist()\n    X_train[i].drop([\"target\"], axis = 1, inplace = True)\n    X_test[i].drop([\"target\"], axis = 1, inplace = True)\n    \nfor i in range(6):\n    print('X_train'+str(i)+\" : \", str(X_train[i].shape))\n    print('X_test'+str(i)+\" : \", str(X_test[i].shape))","4799693d":"# scaler = StandardScaler()\n\n# for i in range(6):\n#     X_train[i] = pd.DataFrame(scaler.fit_transform(X_train[i]))\n#     X_test[i] = pd.DataFrame(scaler.fit_transform(X_test[i]))","c7a45547":"def customEval(estimator, xtest, ytest):\n    val_probs = estimator.predict_proba(xtest)\n    val1 = ytest\n    predictions = val_probs[:, 1]\n    thresholds = np.arange(0.01, 1, 0.01)\n    best_thres = 0\n    best_score = 0\n    for thresh in thresholds:\n        oofs_rounded = (predictions > thresh) * 1\n        thresh_score = matthews_corrcoef(val1, oofs_rounded)\n        if thresh_score > best_score:\n            best_score = thresh_score\n            best_thres = thresh\n    return best_score","4700c7b5":"#Starting the timer\nstart_time = time.perf_counter()\n\nsvc_clf = SVC()\nparams = [{\n    'C':np.arange(0.01,10.0,0.1),\n    'kernel':['linear','poly','rbf','sigmoid'],\n    'degree':range(1,20),\n    \"gamma\" : ['scale','auto'],\n    'probability':[True],\n    'class_weight':['balanced'],\n    'random_state':[21],\n    'max_iter':[-1],\n}]\n\nbest_models = []\nfor i in range(6):\n    random_search_svc_clf = RandomizedSearchCV(svc_clf, scoring = customEval, param_distributions=params, n_jobs = -1, return_train_score = True, n_iter=60, cv=2, random_state=21)\n    random_search_svc_clf.fit(X_train[i],y_train[i])\n    print(\"Best parameters scores:\")\n    print(random_search_svc_clf.best_params_)\n    df = pd.DataFrame(random_search_svc_clf.cv_results_)\n    print(\"Mean Train Score:\", random_search_svc_clf.cv_results_['mean_train_score'][df[df['mean_test_score']==random_search_svc_clf.best_score_].index][0])\n    print(\"Mean Validation score:\", random_search_svc_clf.best_score_)\n    best_models.append(random_search_svc_clf.best_estimator_)\n    \n    \n#Ending the timer\nend_time = time.perf_counter()\ntotal_time = end_time-start_time\n\nprint(\"It took {} secs for completing SVC hyperparameter tuning.\".format(total_time))","11d951f2":"y_pred_train = [[0]]*6\ny_pred_test = [[0]]*6\n\nfor i in range(6):\n    best_models[i].fit(X_train[i],y_train[i])\n    y_pred_train[i] = best_models[i].predict(X_train[i])\n    print(\"Classification Report (Training Data): \")\n    print(classification_report(y_train[i], y_pred_train[i]))\n    print(\"Matthew's correlation coefficient: \", matthews_corrcoef(y_train[i], y_pred_train[i]))\n    y_pred_test[i] = best_models[i].predict(X_test[i])\n    print(\"Classification Report (Test Data): \")\n    print(classification_report(y_test[i], y_pred_test[i]))\n    print(\"Matthew's correlation coefficient: \", matthews_corrcoef(y_test[i], y_pred_test[i]))","df658f68":"# for i in range(6):\n#     best_models[i].save_model('model'+str(i)+'.txt')","f41e2dfe":"y_pred = [[0]]*6\ny_pred_train = [[0]]*6\ny = [[0]]*6\n\nfor i in range(6):\n    y[i] = train[i]['target'].tolist()\n    train[i].drop(['target'], axis = 1, inplace = True)\n    \n#model_svc = SVC()\nfor i in range(6):\n    #model_svc.load_model('model'+str(i)+'.txt')\n    model_svc = best_models[i]\n    model_svc.fit(train[i],y[i])\n    y_pred_train[i] = model_svc.predict(train[i])\n    print(\"Classification Report (Training Data): \")\n    print(classification_report(y[i], y_pred_train[i]))\n    print(\"Matthew's correlation coefficient: \", matthews_corrcoef(y[i], y_pred_train[i]))\n    y_pred[i] = model_svc.predict(test_data[i])\n    \n\nans = []\nfor i in range(6):\n    ans.extend(y_pred[i])\n    \nans = [int(ans[i]) for i in range(len(ans))]\n\nresults = pd.DataFrame({\n    'Id':range(0,1056),\n    'Predicted':ans\n})\nresults.to_csv('results.csv', index = False)","78cac27c":"results.head()","fb003d76":"# Importing libraries","a05de937":"# Applying PCA (Fitting the pca after combining both the train and test data set)","6d89c9e8":"# EDA","98e68ee4":"# Building the final models","212a2650":"# Building the SVC models","de4c3d4a":"***The average of the box plot for category 0 is near to 8 and that for category 1 is in between 8-9. Also the the entries in category 0 in column 1 is more spread that for category 0***","5cafcff9":"# Hyperparameter tunning the SVC models","cb0554bd":"# Applying PCA (Fitting the pca only on the train data set)","82a8f614":"# Making 6 different test data sets","12206bb4":"# Carrying out LDA","fc816258":"***Basically in the given data set the rows are columns and column are rows. We need to predict CO1 and CO2 on the test1 data set using tthe rain1 data set and we need to predict CO3, CO4, CO5 and CO6 on the test2 data set using the train2 data set.***","aece4f0f":"# Building a custom evaluation function","53a7ae69":"# Importing data set","a6c68f79":"# Taking transpose of the data set","452c7065":"# Making 6 different data sets for training 6 models","0c406bb7":"# Dropping first row of all the data sets","4e01646a":"# Splitting the data into training and test set","ad9cf5f5":"# Predictions","c6004c4b":"**For column 1**","27d09631":"# Reseting the row index to start from 0","080ecfd5":"# Saving the best models","011f49d6":"# Normalizing the features","8e7bcfe3":"# Reindexing the data frames","86a79995":"# Taking a look into the data set"}}