{"cell_type":{"2061a3fe":"code","b1d5c106":"code","16fa1676":"code","ee8ebb6c":"code","dc9906cf":"code","4935eb75":"code","edf5777b":"code","266baa15":"code","1c172df6":"code","4caecd49":"code","d9104fc1":"code","3d9cbf46":"code","b659c21d":"code","2d4a8575":"code","f94306d6":"code","150c398a":"code","6cfaa5e6":"code","8b3ea34f":"code","76d025a7":"code","c78e8cef":"code","48472a4f":"code","b2ebf16d":"code","0a0d19db":"code","ca197e85":"code","d300aa8f":"code","06c25014":"code","1c249690":"code","ee262848":"code","20b35e0a":"code","fd6c2ff8":"code","ab78a552":"code","4713214e":"code","ca466815":"code","d5a6521f":"code","87894b9c":"code","7cf7ec7a":"code","cd370a68":"code","25607402":"code","263fa37c":"code","caa3cd62":"code","7b4bb2dc":"code","042d4a4c":"code","eac387c1":"code","5afe4806":"code","53116828":"code","8b07d00f":"code","ce4f1ff1":"code","f9f81bad":"code","9b1aa0a6":"code","4f35e5fb":"code","12464656":"code","a418b5fa":"code","f07c3b26":"code","52de1ad8":"code","94fbf46b":"code","eefb5df5":"code","4a3f7fe7":"code","70cd4bc8":"code","a68b1153":"code","89db2d11":"code","bac3dc8b":"code","4254a05f":"code","4a764728":"code","d9552233":"code","ee856d87":"code","b3306050":"code","d9bbfd87":"code","c9f22600":"code","3a4a0668":"code","76b676e5":"code","a0a1e5c5":"code","f268625d":"code","b861eee5":"code","cbe58fe1":"code","74bba2cb":"code","65cac02b":"code","30d79e31":"code","037e80b6":"code","b414e7df":"code","19422852":"code","01c8bb3f":"code","ac6b4e7f":"code","6ec4d263":"code","082d6292":"code","35169c82":"code","e7b4a1b5":"code","a66a1a2c":"code","b32b9021":"code","f6c3fbeb":"code","f674fd1e":"code","e4d1bac0":"code","f1e2849c":"code","052d5adb":"code","cac272f8":"code","f9d9fb29":"code","335d52d6":"code","bf7cd817":"markdown","44d3a44e":"markdown","eb6a43ec":"markdown","491306a5":"markdown","96d4fd79":"markdown","c2f629cf":"markdown","18490ea1":"markdown","bfd554e9":"markdown","1f1c7bb7":"markdown","e2d36cae":"markdown","b222af63":"markdown","66e179fd":"markdown","99bce4bb":"markdown","39d33f38":"markdown","5ec742a0":"markdown","b8d45384":"markdown","28670df8":"markdown","f671ef0d":"markdown","f27803d3":"markdown","a27e1be0":"markdown","e35dd9c3":"markdown","86b05424":"markdown","5abc2d5f":"markdown","42a34f26":"markdown","efa92fdb":"markdown","e0c8d29e":"markdown","bddeba96":"markdown","6ca1972f":"markdown","bcf59a21":"markdown","11f05792":"markdown","bfd1c7ef":"markdown","3de2685c":"markdown","ea7f6e8a":"markdown","55f5bd60":"markdown","6890a44a":"markdown","3e1c8ceb":"markdown","c830f229":"markdown","6e97ad5f":"markdown","4fdb4b1e":"markdown","34fd85af":"markdown","a88899fb":"markdown","e318757b":"markdown","8ce3ce91":"markdown","a7fd5cb7":"markdown","c6cf8553":"markdown","5d0be913":"markdown","f76fb888":"markdown","e63a9e2c":"markdown","785f38e8":"markdown","3d64f45f":"markdown","10a5c101":"markdown","8705569d":"markdown","2c7ef09b":"markdown","c5e0a438":"markdown","63ac4e8d":"markdown","8dbcb5d1":"markdown","6f4e097a":"markdown","893817fa":"markdown","e30f4e3c":"markdown","334eed8c":"markdown","34b9527f":"markdown","de0d5b83":"markdown","7e531381":"markdown","2007eabb":"markdown","e3f8891c":"markdown","4be69304":"markdown","6997eec3":"markdown","72c64ca6":"markdown","dc28e1ec":"markdown","7597e2cf":"markdown","d11993a0":"markdown","96165e5c":"markdown","21b38f9c":"markdown","8d6c6b8f":"markdown","152a9053":"markdown","22d7feaa":"markdown","e0b8f8da":"markdown","f4a925fd":"markdown","3a551b3d":"markdown","924fd3fb":"markdown","b0f5ae45":"markdown","20ef91bf":"markdown","618d05cc":"markdown","a54acd82":"markdown","7e92b39a":"markdown","9ee5bc9a":"markdown","71f3e29d":"markdown","44b666b4":"markdown","8478668a":"markdown","45b6054e":"markdown","46c22b54":"markdown","e29daa87":"markdown","7da43d51":"markdown","ef9911f6":"markdown","c35b528a":"markdown","f86e135f":"markdown","82e6b118":"markdown","9b26575f":"markdown"},"source":{"2061a3fe":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"Setup complete\")","b1d5c106":"#Data\ntrain = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\n\n#testing data\n#test = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","16fa1676":"train.head()","ee8ebb6c":"print(\"Train data missing values: \\n\",train.isna().sum())","dc9906cf":"train.count()","4935eb75":"train[\"city\"].unique()","edf5777b":"train['city'] = train['city'].map(lambda row: row.replace('city_',''))","266baa15":"train['city'] = train['city'].astype(int)","1c172df6":"print(train[\"city\"].unique())\nprint(\"We have {} unique variables:\".format(len(train[\"city\"].unique())))","4caecd49":"plt.figure(figsize=(16,8))\ng = sns.distplot(train.city,kde=False, color=\"red\")\ng = (g.set(xlim=(0,185),xticks=range(0,190,10)))\nplt.xlabel(\"City Number\")\nplt.ylabel(\"Distribution\")\nplt.show()","d9104fc1":"print(\"Most common cities are:\\n\",train['city'].value_counts())","3d9cbf46":"train.sort_values(by='city')","b659c21d":"train.loc[train.city == 103,'city_development_index']","2d4a8575":"train['city'] = train['city'].astype(np.int8)","f94306d6":"sns.lineplot(x='target', y='city_development_index',data=train)\nplt.show()","150c398a":"train = train.drop(labels='city_development_index', axis=1)","6cfaa5e6":"plt.figure(figsize=(12,6))\nsns.violinplot(x='gender', y='target', palette='Set2', data=train)\nplt.show()","8b3ea34f":"train['gender'].hist()\nplt.show()","76d025a7":"train['gender'] = train['gender'].fillna('Other')","c78e8cef":"from sklearn.preprocessing import LabelEncoder","48472a4f":"label_encoder = LabelEncoder()\ntrain[\"gender\"] = label_encoder.fit_transform(train[\"gender\"])","b2ebf16d":"train.isna().sum()","0a0d19db":"train.head()","ca197e85":"train['relevent_experience'].unique()","d300aa8f":"train[\"relevent_experience\"] = train[\"relevent_experience\"].map({\"Has relevent experience\":1, \"No relevent experience\":0})","06c25014":"train['enrolled_university'].unique()","1c249690":"sns.countplot(x='enrolled_university', data=train)\nplt.show()","ee262848":"sns.lineplot(x='enrolled_university', y='target', palette='Set2', data=train)\nplt.show()","20b35e0a":"train[\"enrolled_university\"]=train[\"enrolled_university\"].fillna('no_enrollment')","fd6c2ff8":"train[\"enrolled_university\"] = label_encoder.fit_transform(train[\"enrolled_university\"])","ab78a552":"train['education_level'].unique()","4713214e":"train['education_level'].hist()\nplt.show()","ca466815":"sns.lineplot(x='education_level', y='target', palette='Set2', data=train)\nplt.show()","d5a6521f":"train[\"education_level\"]=train[\"education_level\"].fillna('Other')","87894b9c":"train[\"education_level\"] = label_encoder.fit_transform(train[\"education_level\"])","7cf7ec7a":"train['major_discipline'].unique()","cd370a68":"train['major_discipline'].hist()","25607402":"train['major_discipline'] = train['major_discipline'].fillna('Other')","263fa37c":"train[\"major_discipline\"] = label_encoder.fit_transform(train[\"major_discipline\"])","caa3cd62":"train.head()","7b4bb2dc":"train['experience'] = train['experience'].astype(str)","042d4a4c":"train['experience'] = train['experience'].apply(lambda col: col.replace('>',''))\ntrain['experience'] = train['experience'].apply(lambda col: col.replace('<',''))","eac387c1":"train.head()","5afe4806":"train['experience'] = train['experience'].apply(lambda col: col.replace('nan','0'))","53116828":"train['experience'] = pd.to_numeric(train['experience'])","8b07d00f":"train['company_size'].unique()","ce4f1ff1":"sns.countplot(y='company_size', data=train)\nplt.show()","f9f81bad":"train['company_size'] = train['company_size'].map({\"50-99\":0, \"<10\":1, \"10000+\":2, \"5000-9999\":3, \"1000-4999\":4, \"10\/49\":5, \"100-500\":6, \"500-999\":7})","9b1aa0a6":"train.shape","4f35e5fb":"train['company_size'] = train['company_size'].fillna(8)","12464656":"sns.countplot(y='company_type', data=train)\nplt.show()","a418b5fa":"train['company_type'] = train['company_type'].map({\"Pvt Ltd\":0, \"Funded Startup\":1, \"Early Stage Startup\":2, \"Public Sector\":3, \"NGO\":4, \"Other\":5})","f07c3b26":"plt.figure(figsize=(12,6))\nsns.violinplot(x='company_size', y='company_type',data=train)\nplt.show()","52de1ad8":"train['company_type'] = train['company_type'].fillna(0)","94fbf46b":"train['last_new_job'].unique()","eefb5df5":"train['last_new_job'] = train['last_new_job'].fillna('never')","4a3f7fe7":"train['last_new_job'] = train['last_new_job'].map({\"1\":1, \">4\": 5, \"never\":0, \"4\":4, \"3\":3, \"2\":2})","70cd4bc8":"g = sns.catplot(y=\"target\",x=\"last_new_job\",data=train, kind=\"bar\")","a68b1153":"train.head()","89db2d11":"train.dtypes","bac3dc8b":"pred = train['target']","4254a05f":"train = train.drop(labels='target', axis=1)","4a764728":"train = train.astype(np.int8)","d9552233":"from sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split","ee856d87":"KFold_Score = pd.DataFrame()\nclassifiers = ['Linear SVM', 'LogisticRegression', 'RandomForestClassifier', 'XGBoostClassifier','GradientBoostingClassifier']\nmodels = [svm.SVC(kernel='linear'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          xgb.XGBClassifier(n_estimators=100),\n          GradientBoostingClassifier(random_state=0)\n         ]","b3306050":"#j = 0\n#for i in models:\n    #model = i\n    #cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    #KFold_Score[classifiers[j]] = (cross_val_score(model, train, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    #j = j+1","d9bbfd87":"#mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\n#KFold_Score = pd.concat([KFold_Score,mean.T])\n#KFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\n#KFold_Score.T.sort_values(by=['Mean'], ascending = False)","c9f22600":"X_train, X_test, y_train, y_test = train_test_split(train, pred, test_size=0.3, random_state=42)","3a4a0668":"y_train.head()","76b676e5":"mymodel = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10)","a0a1e5c5":"predictors = [x for x in X_train.columns if x not in [\"enrollee_id\"]]","f268625d":"predictors","b861eee5":"param_test1 = {'n_estimators':range(10,100,10)}","cbe58fe1":"from sklearn.model_selection import GridSearchCV\nCV_gbc = GridSearchCV(estimator=mymodel, param_grid=param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv= 5)\nCV_gbc.fit(X_train[predictors],y_train)\nCV_gbc.best_params_, CV_gbc.best_score_","74bba2cb":"CV_gbc.cv_results_","65cac02b":"dir(CV_gbc)","30d79e31":"mymodel0 = GradientBoostingClassifier(learning_rate=0.2, min_samples_split=500,min_samples_leaf=50,max_depth=8,max_features='sqrt',subsample=0.8,random_state=10)","037e80b6":"from sklearn.model_selection import GridSearchCV\nCV_gbc0 = GridSearchCV(estimator=mymodel0, param_grid=param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv= 5)\nCV_gbc0.fit(X_train[predictors],y_train)\nCV_gbc0.best_params_, CV_gbc0.best_score_","b414e7df":"param_test2 = {'max_depth':range(5,9,1), 'min_samples_split':range(400,800,100)}\ngbc = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=60, max_features='sqrt', subsample=0.8, random_state=10), \nparam_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngbc.fit(X_train[predictors],y_train)\ngbc.best_params_, gbc.best_score_","19422852":"param_test3 = {'min_samples_split':range(400,1200,100), 'min_samples_leaf':range(30,71,10)}\ngsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=60,max_depth=6, max_features='sqrt', subsample=0.8, random_state=10), \nparam_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train[predictors],y_train)\ngsearch3.best_params_, gsearch3.best_score_","01c8bb3f":"from sklearn.model_selection import cross_validate","ac6b4e7f":"def modelfit(alg, dtrain, pred, predictors, performCV=True, printFeatureImportance=True, cv_folds=5):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], pred)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n    \n    #Perform cross-validation:\n    if performCV:\n        cv_score = cross_validate(alg, dtrain[predictors], pred, cv=cv_folds, scoring='roc_auc')\n    \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Accuracy :\",metrics.accuracy_score(pred.values, dtrain_predictions))\n    print(\"AUC Score (Train):\", metrics.roc_auc_score(pred, dtrain_predprob))\n    print(\"cv Score: \", np.mean(cv_score['test_score']))\n        \n    #Print Feature Importance:\n    if printFeatureImportance:\n        feat_imp = pd.Series(alg.feature_importances_, predictors).sort_values(ascending=False)\n        feat_imp.plot(kind='bar', title='Feature Importances')\n        plt.ylabel('Feature Importance Score')","6ec4d263":"modelfit(gsearch3.best_estimator_, X_train, y_train, predictors)","082d6292":"param_test4 = {'max_features':range(7,20,2)}\ngsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=60,max_depth=6, min_samples_split=900, min_samples_leaf=60, subsample=0.8, random_state=10),\nparam_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(X_train[predictors],y_train)\ngsearch4.best_params_, gsearch4.best_score_","35169c82":"param_test5 = {'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\ngsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.2, n_estimators=60,max_depth=6,min_samples_split=900, min_samples_leaf=60, subsample=0.8, random_state=10,max_features=9),\nparam_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch5.fit(X_train[predictors],y_train)\ngsearch5.best_params_, gsearch5.best_score_","e7b4a1b5":"gbm_tuned_1 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=140,max_depth=6, min_samples_split=900,min_samples_leaf=60, subsample=0.8, random_state=10, max_features=9)\nmodelfit(gbm_tuned_1, X_train, y_train, predictors)","a66a1a2c":"gbm_tuned_2 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1200,max_depth=6, min_samples_split=900,min_samples_leaf=60, subsample=0.8, random_state=10, max_features=9)\nmodelfit(gbm_tuned_2, X_train, y_train, predictors)","b32b9021":"gbm_tuned_3 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=1500,max_depth=6, min_samples_split=900,min_samples_leaf=60, subsample=0.8, random_state=10, max_features=9)\nmodelfit(gbm_tuned_3, X_train, y_train, predictors)","f6c3fbeb":"gbm_tuned_1.fit(X_train,y_train)","f674fd1e":"X_test[predictors]","e4d1bac0":"preds = gbm_tuned_1.predict(X_test)","f1e2849c":"p = gbm_tuned_1.predict(X_train)","052d5adb":"print(\"Accuracy on training set is:\")\nmetrics.accuracy_score(y_train, p)","cac272f8":"metrics.accuracy_score(y_test, preds)","f9d9fb29":"output = pd.DataFrame({'enrollee_id ': X_test.enrollee_id , 'target': preds})","335d52d6":"output.sample(7)","bf7cd817":"### Encoding:","44d3a44e":"Lets have a look at what we have achieved so far:","eb6a43ec":"### Tree-specific parameters","491306a5":"### Boosting parameters:","96d4fd79":"So we have taken care of all categorical data and missing values, lets take a look at what we have done so far:","c2f629cf":"Let's take a closer look to our data.","18490ea1":"The next step is to find the max_depth and min_samples_split.","bfd554e9":"30% of candidates didn't mention if they had experience or not, so we will assume that these candidates have no experience.","1f1c7bb7":"# City:","e2d36cae":"We got 0.8 as the optimum subsample value.\nNow, we need to lower the learning rate and increase the number of estimators to see if we get better results.","b222af63":"#### Private limited companies are of different sizes, from less than ten people to +10 000! So we can't really find a relation between company size and type.\n#### We will fill missing values in company_type with 0(private limited comapany).","66e179fd":"First we have to look for an optimal number of estimators:","99bce4bb":"So we got a maximum depth of 6 and minimum samples split is 700.","39d33f38":"The candidates from cities with low development index tend to look for a job change and vice versa. Now let's just drop this column.","5ec742a0":"First we have to convert the column to string.","b8d45384":"We assume that missing values are from candidates that had no job.","28670df8":"#### We can see that most of candidates had no enrollment more likely aren't looking for a job change.","f671ef0d":"Grid search:","f27803d3":"<a href=\"https:\/\/ibb.co\/PgstW8k\"><img src=\"https:\/\/i.ibb.co\/WfNsz1C\/boosting.png\" alt=\"boosting\" border=\"0\"><\/a>","a27e1be0":"AUC represents the probability that a random positive  example is positioned to the right of a random negative example. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0. [https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc]","e35dd9c3":"It looks like more men don't look for a job change but actually we can't conclude that from this violinplot since most of the candidates are men.","86b05424":"The overall parameters of gradient boosting can be devided into three categories:\n* Tree-Specific Parameters: These affect each individual tree in the model.\n* Boosting Parameters: These affect the boosting operation in the model.\n* Miscellaneous Parameters: Other parameters for overall functioning.","5abc2d5f":"#### The experience variable is an object indicating the minimum or maximum years of experience a candidate had, so deleting the operators won't make a big difference.","42a34f26":"* loss\n* init\n* random_state\n* verbose","efa92fdb":"So the city with the majority of candidates is a well developped city, but do we have a relationship between the city development index and the chance of the candidate looking for another job?","e0c8d29e":"And here we can see that each city has specific city_development_index, so deleting this column won't make any difference to the model, however we can visualize what is the development index for the cities with the majority of candidates:","bddeba96":"**STEM:** Science, technology, engineering, and mathematics","6ca1972f":"Grid search is best described as exhuastive guess and check. We have a problem: find the hyperparameters that result in the best cross validation score, and a set of values to try in the hyperparameter grid - the domain. The grid search method for finding the answer is to try all combinations of values in the domain and hope that the best combination is in the grid (in reality, we will never know if we found the best settings unless we have an infinite hyperparameter grid which would then require an infinite amount of time to run).\n\nGrid search suffers from one limiting problem: it is extremely computationally expensive because we have to perform cross validation with every single combination of hyperparameters in the grid!","bcf59a21":"We can't tell if the missing data is left out or the candidates had no enrollment, but also we don't want to create a new value (like 'OTHER') because it can create a pattern that doesn't exist.\nI will fill the missing values with the no_enrollment value.","11f05792":"We can see a slight improvement in Accuracy and cv score, lets descrease the learning rate and increase number of estimators one more time.","bfd1c7ef":"It is obvious we have to encode the caregorical data and take care of missing data, there are a lot of ways to handle gender missing values such as replacing them with most common gender in the dataset, deleting those rows...etc\nBut I prefer to fill the gender missing values with \"Other\" since we may have candidates identify as non-binary.\nFirst lets take a look at our gender column:","3de2685c":"#### pre modeling steps:","ea7f6e8a":"# Experience:","55f5bd60":"#### Encode:","6890a44a":"# Major Discipline:","3e1c8ceb":"### The result:","c830f229":"The next step would be try different subsample values.","6e97ad5f":"# Enrolled university:","4fdb4b1e":"Most of our candidates are STEM majors.","34fd85af":"# Hyperparameter Tuning using GridSearchCV:","a88899fb":"# Last new job:","e318757b":"Most candidates are graduates.","8ce3ce91":"\nWith this we have the final tree-parameters as:\n\n    min_samples_split: 900\n    min_samples_leaf: 60\n    max_depth: 6\n    max_features: 9\n","a7fd5cb7":"Lets fit the model.","c6cf8553":"From the first glence we can see that most of the data is categorical, and we have several missing values in different columns.\nLet's take a look at the number of missing values in each column:","5d0be913":"Let's import our train and test data.","f76fb888":"increasing the number of estimators got us a slightly better model.","e63a9e2c":"# What is Gradient Boosting?","785f38e8":"# Plan:\n\nWe will treat each column in two phases:\n### Data preprocessing:\n* Imputing missing values.\n* Handling categorical data.\n    \n### Data Visualization\n\n### Model Creation:\n\nThen we will test several model and choose the one with the best performance:\n* Models: Logistic Regression-SVM-Gradient Boosting-KNN-RandomForest-XGBoost classifier.\n* Hyperparameter tuning.\n* Use the model to predict target column in Test set.","3d64f45f":"Lets fill the missing values with 0.","10a5c101":"Make predictions on test and training set:","8705569d":"**Ensemble learning techniques:** They combine the predictions of multiple machine learning models.\n\n**Boosting:** Boosting algorithms play a crucial role in dealing with bias variance trade-off.  Unlike bagging algorithms, which only controls for high variance in a model, boosting controls both the aspects (bias & variance), and is considered to be more effective.\nBoosting is an ensemble learning technique.\n\nBoosting is a sequential technique which works on the principle of ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher.\n","2c7ef09b":"# Company type:","c5e0a438":"As you can see that here we got 90 as the optimal estimators for 0.1 learning rate, it is close to 100 so we will increase the learning rate to 0.2. (I tried working with number of estimators as 90 but when I increased the learning rate I got slightly better results, I won't include the whole process because it will take a lot of time to run)","63ac4e8d":"Graduates and Masters are most likely to look for a job change, but people with Phd or primary school aren't.","8dbcb5d1":"Save results to a dataframe:","6f4e097a":"We shouldn't use the enrollee_id since it is unique for each candidate.","893817fa":"### Miscellaneous parameters","e30f4e3c":"#### Most of the candidates had no university enrollment","334eed8c":"#### We have 5938 missing values in the company_size column, before encoding categorical data we have to handle the missing values.","34b9527f":"# Gender:","de0d5b83":"Identify each interval with a number:","7e531381":"## Features\n\n* enrollee_id : Unique ID for candidate\n* city: City code\n* city_ development _index : Developement index of the city (scaled)\n* gender: Gender of candidate\n* relevent_experience: Relevant experience of candidate\n* enrolled_university: Type of University course enrolled if any\n* education_level: Education level of candidate\n* major_discipline :Education major discipline of candidate\n* experience: Candidate total experience in years\n* company_size: No of employees in current employer's company\n* company_type : Type of current employer\n* lastnewjob: Difference in years between previous job and current job\n* training_hours: training hours completed\n* target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","2007eabb":"#### Female : 0\n#### Male : 1\n#### Other : 2","e3f8891c":"Lets run the function on the model we got till now:","4be69304":"![image.png](attachment:db48545e-aa56-4fdd-90ca-edcfa2fe1433.png)","6997eec3":"![image.png](attachment:ac6db08b-df3f-44b6-b060-701a3fb0f13c.png)","72c64ca6":"![image.png](attachment:a8ca723c-a80b-4da7-b172-f57b95643249.png)","dc28e1ec":"We can see that candidates from the city number **103** are the majority.","7597e2cf":"# Omar El Yousfi","d11993a0":"Since we don't have any missing values left in the gender column, let's encode the column, I will use for this one LabelEncoder of sklearn:","96165e5c":"## Context:\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which are conducted by the company. Many people sign up for their training. Company wants to know which of these candidates really want to work for the company after training, because it helps reducing the cost and time as well as increasing the quality of training and the planning of the courses. Information related to demographics, education, experience are in hands from candidates signup and enrollment.","21b38f9c":"**GradientBoostingClassifier**(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)","8d6c6b8f":"**Pros:**\n- We don't have to write a long (nested for loops) code.\n- Finds the best model within the grid.\n\n**Cons:**\n- Computationally expensive.\n- It is 'uninformed'. Results of one model don't help creating the next model.","152a9053":"# Relevant experience:","22d7feaa":"* learning_rate\n* n_estimators\n* subsample","e0b8f8da":"Lets write a function that returns the accuracy, auc score and the importance of each variable:","f4a925fd":"# Education level:","3a551b3d":"Before making any decision we have to look at the values of this column:","924fd3fb":"min_samples_leaf:","b0f5ae45":"I have commented previous cells because it take a lot of time to execute(and I keep runing out of memory) = Difficult to commit the notebook. <\/br>\nAnyways, Gradient Boosting gives the best result.","20ef91bf":"**Visually:**","618d05cc":"We can see that every city has a specific number, with a prefix \"city_\", so first we have to delete the prefix then transform the data from object type to integer.","a54acd82":"## Gradient Boosting:","7e92b39a":"Now let's fill the missing values with 'Other':","9ee5bc9a":"# Models testing:","71f3e29d":"# Company size:","44b666b4":"Lets get started, first lets import our typical libraries:","8478668a":"## Boosting:","45b6054e":"*Boosting reference:* https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\n\n*Patrick Winston, MIT:* https:\/\/www.youtube.com\/watch?v=UHBmv7qCey4","46c22b54":"Lets try different models and see what works best:","e29daa87":"Delete the symbols.","7da43d51":"This column has missing values, lets take care of them before encoding.","ef9911f6":"Convert the values to Integer.","c35b528a":"#### Most of candidates work in Private limited company type (pvt ltd)","f86e135f":"Lets initialize our model with these parameters:","82e6b118":"#### We can see that most candidates work in small companies (between 50-500)","9b26575f":"So it only has two values, and no missing data."}}