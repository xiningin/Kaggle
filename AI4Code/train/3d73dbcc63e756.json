{"cell_type":{"dd4f0fa3":"code","01c9bc8f":"code","2930421a":"code","ee937dfd":"code","78a32a17":"code","162c03f2":"code","68a8d5a1":"code","09e0b145":"code","98b2ca21":"code","e54d1f39":"code","9460f8ce":"code","aaf91a80":"code","aba2039e":"code","3ad0b96a":"code","98f0b8b6":"code","0d4b34a9":"code","9323ff60":"code","3dbb4933":"code","aa25ed8a":"code","ceb7abc5":"code","713fd5ea":"code","9edb10ab":"code","f52d99d8":"code","15bfdcfa":"code","6cfa0d7f":"code","f24e8bb3":"markdown"},"source":{"dd4f0fa3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01c9bc8f":"import pandas as pd\nimport numpy as np\n\n\ndf = pd.read_csv('\/kaggle\/input\/housepricesdata\/housepricedata.csv')","2930421a":"df","ee937dfd":"dataset = df.values","78a32a17":"dataset","162c03f2":"X = dataset[:,0:10]\nY = dataset[:,10]","68a8d5a1":"#Normalizing and Scaling the dataset are very important as there are no missing values in the dataset\n#Normalization\/Scaling makes input features to be on same order of magnitude\n#Mim max scaler scales our data values to be between 0 and 1\nfrom sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(X)","09e0b145":"X_scale","98b2ca21":"#Data Splitting into Train, Validaion and Testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.3)\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)","e54d1f39":"#Getting the rows of all Train, Validation and testing\nprint(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)","9460f8ce":"#Keras is an open-source neural-network library written in Python. It is capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, R, Theano, or PlaidML. \n#Designed to enable fast experimentation with deep neural networks, it focuses on being user-friendly, modular, and extensible\n\n#We will be using Keras to build Neural Network architecture using TensorFlow backend\n!pip install Keras","aaf91a80":"!pip install tensorflow","aba2039e":"#Importing required modules from Keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","3ad0b96a":"#Model 1: Sequential Model\n#In Sequential Model, we need to describe the number of layers above in the sequence\n# Here we used 2 Hidden layers with 30 neurons (ReLU activation), and 1 output layer with 1 neuron (Sigmoid function)\n\nmodel = Sequential([\n    Dense(32, activation='relu', input_shape=(10,)),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\n#Now, we got our architecture specified- we need to configure the model \n#optimizer=sgd (Stochastic gradient descent)\n#loss=binary_crossentropy (giving the type of loss function to use)\n#metrics=accuracy (Other metrics which you want to check apart from loss function)\n\nmodel.compile(optimizer='sgd',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n\n#Fitting the model\n#epochs= tells how long we want to train the model\n\nhist = model.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","98f0b8b6":"model.evaluate(X_test, Y_test)[1]","0d4b34a9":"import matplotlib.pyplot as plt\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","9323ff60":"plt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","3dbb4933":"#Model 2: Training model with many hidden layers to check whether the model will overfit\n\nmodel_2 = Sequential([\n    Dense(1000, activation='relu', input_shape=(10,)),\n    Dense(1000, activation='relu'),\n    Dense(1000, activation='relu'),\n    Dense(1000, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\nmodel_2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nhist_2 = model_2.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","aa25ed8a":"plt.plot(hist_2.history['loss'])\nplt.plot(hist_2.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","ceb7abc5":"plt.plot(hist_2.history['accuracy'])\nplt.plot(hist_2.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","713fd5ea":"#Now, you can see that Model 2 is over fitting when compared to Model 1 due to additional hidden layers added to Model 2 when compared to Model 1","9edb10ab":"#Model 3: Adding L2 regularization to same paramaters given to Model 2 thereby reducing the Overfitting in Model 2.\n#L2 regularization also referred to as Ridge regression\n#Ridge regression adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function\n#L2 Regularization adds the term Lambda and Lambda need to be choosen carefully.\n#L2 regularization forces the weights to be small but does not make them zero and does non sparse solution.\n\nfrom keras.layers import Dropout\nfrom keras import regularizers\n\nmodel_3 = Sequential([\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(10,)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n])\n\n\nmodel_3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nhist_3 = model_3.fit(X_train, Y_train,\n          batch_size=32, epochs=100,\n          validation_data=(X_val, Y_val))","f52d99d8":"plt.plot(hist_3.history['loss'])\nplt.plot(hist_3.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.ylim(top=1.2, bottom=0)\nplt.show()","15bfdcfa":"#As you can see that the Model 3 (With Regularization) is less overfitting when compared to Model 2 with many hidden layers (no regularization)\n#Regularization improves the performance of the models","6cfa0d7f":"plt.plot(hist_3.history['accuracy'])\nplt.plot(hist_3.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","f24e8bb3":"![keras.png](attachment:keras.png)"}}