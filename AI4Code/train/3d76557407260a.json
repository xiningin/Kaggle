{"cell_type":{"49aae242":"code","d5decc76":"code","b546d8f6":"code","72d4fd73":"code","d0d6ab2d":"code","c777636c":"code","28664789":"code","4fb3cee6":"code","9fc35963":"code","7a7fd989":"code","96b5c792":"code","7b84179c":"code","e26295b9":"code","1bf6e619":"code","8097d0da":"code","722fd6d4":"code","31ae5a9f":"code","fccaf37b":"code","a70ceb90":"code","939f93c0":"code","c99458b6":"code","2f718d00":"code","dbdc8abd":"code","9958f204":"code","aaacbc95":"code","e035d7ba":"code","21e80fdf":"code","ab6352a3":"code","1e4f9fad":"code","33c6edf1":"code","94187b03":"code","2771b9d9":"code","ae58e148":"code","d3c90658":"code","33ad5a05":"code","dc60e08d":"code","899631fe":"code","c8c60e21":"code","0884bf75":"code","f12146b5":"code","ba36aa9f":"code","9aebbf0e":"code","7eae487a":"code","0ad256a4":"code","1cd7bcb7":"code","f5bb106d":"markdown","f92b6c8f":"markdown","46d9db0c":"markdown","2897d5b9":"markdown","e2d8c169":"markdown","0553296f":"markdown","394a7226":"markdown","2bcce986":"markdown","dafca785":"markdown","2b9369bf":"markdown","27ddb169":"markdown","b1b4086f":"markdown","c88fb8a9":"markdown","393151fc":"markdown","6e1169d6":"markdown","3615337f":"markdown","6d7145cd":"markdown","7ba15471":"markdown","6e79c42f":"markdown","96c6a9a9":"markdown","743fb361":"markdown","00261134":"markdown","9b6d3a42":"markdown","45333de0":"markdown","2490c49e":"markdown","38ebea3b":"markdown","e7635198":"markdown","13aace0b":"markdown","5b727611":"markdown","e0841cba":"markdown","638f60e5":"markdown","c0ee8f96":"markdown","f0c0af58":"markdown","6d4f308a":"markdown","b8ac7c97":"markdown","87e87c1b":"markdown","95d1bc63":"markdown","afa6e030":"markdown","6ca6ed4a":"markdown","e959ecd5":"markdown","3482d1ca":"markdown","a23341e5":"markdown","b59ca3cd":"markdown","5d60ef05":"markdown"},"source":{"49aae242":"import numpy as np\nimport pandas as pd","d5decc76":"!pip install seaborn --upgrade","b546d8f6":"df = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/train.csv\", index_col=['Id'])","72d4fd73":"df.head()","d0d6ab2d":"df.info()","c777636c":"df.describe()","28664789":"import matplotlib.pyplot as plt\nimport seaborn as sns","4fb3cee6":"mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n\nplt.figure(figsize=(10,10))\n\nsns.heatmap(df.corr(), mask=mask, square = True, annot=True, vmin=-1, vmax=1, cmap='autumn')\nplt.show()","9fc35963":"g = sns.jointplot(data=df, x=\"median_income\", y=\"median_house_value\", kind=\"hex\")\ng.plot_joint(sns.regplot, scatter=False)","7a7fd989":"sns.pairplot(data=df, x_vars=df.columns, y_vars=['median_house_value'], kind=\"hist\")","96b5c792":"import matplotlib.image as mpimg\n\n# Obtendo uma imagem da Calif\u00f3rnia\ncalifornia_img = mpimg.imread('https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1b\/California_Locator_Map.PNG')\n\n# Montando o gr\u00e1fico\nax = df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,7),\n                       s=df['population']\/100, label=\"Population\",\n                       c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n                       colorbar=False, alpha=0.4,\n                      )\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=0.5)\n\n# Legendas\ntick_values = np.linspace(df[\"median_house_value\"].min(), df[\"median_house_value\"].max(), 11)\ncbar = plt.colorbar()\ncbar.ax.set_yticklabels([\"$%dk\"%(round(v\/1000)) for v in tick_values], fontsize=14)\ncbar.set_label('Median House Value', fontsize=16)\n\nplt.legend(fontsize=16)\nplt.show()","7b84179c":"import plotly.express as px\n\nfig = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", color=\"median_house_value\", size=\"population\",\n                  color_continuous_scale=px.colors.sequential.Sunsetdark, size_max=15, zoom=4,\n                  mapbox_style=\"carto-positron\")\nfig.show()","e26295b9":"from mpl_toolkits.basemap import Basemap\n\n# Obtendo as coordenadas dos pontos do litoral\nplt.ioff()\nm = Basemap(projection='robin',lon_0=0,resolution='l')\ncoast = m.drawcoastlines()\ncoordinates = np.vstack(coast.get_segments())\nlons,lats = m(coordinates[:,0],coordinates[:,1],inverse=True)\n\n# Reduzindo as coordenadas para somente aquelas pr\u00f3ximas \u00e0 Calif\u00f3rnia\ncalifornia = np.logical_and(np.logical_and(lons > -130, lons < -110), np.logical_and(lats > 30, lats < 45))\n\nocean_df = pd.DataFrame()\nocean_df['longitude'] = lons[california]\nocean_df['latitude'] = lats[california]\nplt.close()","1bf6e619":"def distance_from_coast(row):\n    dists = np.sqrt((row['latitude'] - ocean_df['latitude'])**2+(row['longitude'] - ocean_df['longitude'])**2)*111.12\n    \n    return min(dists)","8097d0da":"df['distance_from_coast'] = pd.concat([df['latitude'], df['longitude']], axis=1).apply(distance_from_coast, axis=1)","722fd6d4":"g = sns.jointplot(data=df, x=\"distance_from_coast\", y=\"median_house_value\", kind=\"kde\", fill=True)\ng.plot_joint(sns.regplot, scatter=False)","31ae5a9f":"big_cities = {'city':      ['Los Angeles', 'San Diego', 'San Jose', 'San Francisco'], \n     'latitude':  [34.0194, 32.8153, 37.335480, 37.7272], \n     'longitude': [-118.411, -117.135, -121.893028, -123.032]}\nbig_cities_df = pd.DataFrame(data=big_cities)","fccaf37b":"def distance_from_big_city(row):\n    dists = np.sqrt((row['latitude'] - big_cities['latitude'])**2+(row['longitude'] - big_cities['longitude'])**2)*111.12\n    \n    return min(dists)","a70ceb90":"df['distance_from_big_city'] = pd.concat([df['latitude'], df['longitude']], axis=1).apply(distance_from_big_city, axis=1)","939f93c0":"g = sns.jointplot(data=df, x=\"distance_from_big_city\", y=\"median_house_value\", kind=\"kde\", fill=True)\ng.plot_joint(sns.regplot, scatter=False)","c99458b6":"df['rooms_per_household'] = df['total_rooms']\/df['households']\ndf['bedrooms_per_room'] = df['total_bedrooms']\/df['total_rooms']\ndf = df.drop(['total_bedrooms'], axis=1)","2f718d00":"df.corr()['median_house_value'].sort_values()","dbdc8abd":"df.drop_duplicates(keep='first', inplace=True)","9958f204":"Y = df.pop('median_house_value').values.astype(float)\n\nX = df","aaacbc95":"from sklearn.preprocessing import StandardScaler\n\npreprocessor = StandardScaler()","e035d7ba":"X = preprocessor.fit_transform(X)","21e80fdf":"from sklearn.linear_model import LinearRegression\nfrom sklearn.compose import TransformedTargetRegressor\n\nmodel = LinearRegression()\n\ndef constrain(array):\n    return array * (array > 0)\n\nregr = TransformedTargetRegressor(regressor=model, func=constrain, inverse_func=constrain)","ab6352a3":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\nscore = cross_val_score(regr, X, Y, cv = cv, scoring=\"neg_mean_squared_log_error\")\nprint(\"RMSLE:\", ((-score)**0.5).mean())","1e4f9fad":"from sklearn.linear_model import ElasticNet\n    \nmodel = ElasticNet(random_state=42)\n\nenr = TransformedTargetRegressor(regressor=model, func=constrain, inverse_func=constrain)","33c6edf1":"score = cross_val_score(enr, X, Y, cv = cv, scoring=\"neg_mean_squared_log_error\")\nprint(\"RMSLE:\", ((-score)**0.5).mean())","94187b03":"from sklearn.neighbors import KNeighborsRegressor\n\nknnr = KNeighborsRegressor(n_neighbors=11)","2771b9d9":"score = cross_val_score(knnr, X, Y, cv = cv, scoring=\"neg_mean_squared_log_error\")\nprint(\"RMSLE:\", ((-score)**0.5).mean())","ae58e148":"from sklearn.ensemble import RandomForestRegressor\n\nrfr = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=42)","d3c90658":"score = cross_val_score(rfr, X, Y, cv = cv, scoring=\"neg_mean_squared_log_error\")\nprint(\"RMSLE:\", ((-score)**0.5).mean())","33ad5a05":"rfr.fit(X, Y)","dc60e08d":"test_data = pd.read_csv(\"..\/input\/atividade-regressao-PMR3508\/test.csv\", index_col=['Id'])","899631fe":"test_data['distance_from_coast'] = pd.concat([test_data['latitude'], test_data['longitude']], axis=1).apply(distance_from_coast, axis=1)\ntest_data['distance_from_big_city'] = pd.concat([test_data['latitude'], test_data['longitude']], axis=1).apply(distance_from_big_city, axis=1)","c8c60e21":"test_data['rooms_per_household'] = test_data['total_rooms']\/test_data['households']\ntest_data['bedrooms_per_room'] = test_data['total_bedrooms']\/test_data['total_rooms']\nX_test = test_data.drop(['total_bedrooms'], axis=1)","0884bf75":"X_test = preprocessor.transform(X_test)","f12146b5":"predictions = rfr.predict(X_test)","ba36aa9f":"predictions","9aebbf0e":"submission = pd.DataFrame()","7eae487a":"submission[0] = test_data.index\nsubmission[1] = predictions\nsubmission.columns = ['Id','median_house_value']","0ad256a4":"submission.head()","1cd7bcb7":"submission.to_csv('submission.csv',index = False)","f5bb106d":"<a id=\"section-two\"><\/a>\n# \ud83d\udcca Visualiza\u00e7\u00e3o de Dados\n\nPara fazer a visualiza\u00e7\u00e3o de dados, vamos importar duas bibliotecas importantes: **matplotlib** e **seaborn**.","f92b6c8f":"E vamos normalizar nossos dados com um **StandardScaler**:","46d9db0c":"Por fim, podemos utilizar o m\u00e9todo `describe` para obter uma descri\u00e7\u00e3o estat\u00edstica simples sobre os nossos dados:","2897d5b9":"Muitas das nossas novas features possuem uma correla\u00e7\u00e3o bem alta com o nosso target!","e2d8c169":"\u00c9 poss\u00edvel notar alguns polos principais de distritos com maior popula\u00e7\u00e3o e maior custo das casas, principalmente perto do mar onde as cidades principais do estado da Calif\u00f3rnia se localizam.","0553296f":"Vamos definir uma fun\u00e7\u00e3o que calcule a dist\u00e2ncia m\u00ednima at\u00e9 as cidades:","394a7226":"Muitas features parecem ter pouca correla\u00e7\u00e3o com a nossa label, e possuem alta correla\u00e7\u00e3o entre si. Apesar disso, podemos perceber que a vari\u00e1vel de **median_income** tem uma forte correla\u00e7\u00e3o com o pre\u00e7o das casas. Vamos analisar um pouco melhor essa rela\u00e7\u00e3o:","2bcce986":"Como o Random Forest foi o nosso melhor modelo, vamos ajust\u00e1-lo para fazer a predi\u00e7\u00e3o dos dados de teste!","dafca785":"Nosso resultado \u00e9 esse:","2b9369bf":"Com o m\u00e9todo `info`, podemos observar o tipo de cada vari\u00e1vel da nossa tabela, bem como quantos valores n\u00e3o faltantes n\u00f3s temos. Nesse caso, n\u00e3o temos nenhum.","27ddb169":"<a id=\"section-one\"><\/a>\n# \ud83c\udfb2 Importando os Dados\n\nAntes de come\u00e7ar a fazer qualquer an\u00e1lise, precisamos antes importar os nossos dados. Para isto, utilizamos o m\u00e9todo `read_csv` da biblioteca **pandas** com o local do nosso arquivo. Tamb\u00e9m especificaremos que a coluna 'Id' s\u00e3o os \u00edndices da nossa tabela.","b1b4086f":"<a id=\"section-two-half\"><\/a>\n# \ud83d\udea7 Feature Engineering\n\nVamos aproveitar os dados da nossa base para criar mais features importantes!","c88fb8a9":"# \ud83d\udcc8 Regress\u00e3o - California Housing\n\n### Bernardo Coutinho - PMR3508-2020-151\n\n### \u00cdndice\n- [\ud83c\udfb2 Importando os Dados](#section-one)\n- [\ud83d\udcc8 Visualiza\u00e7\u00e3o de Dados](#section-two)\n  - [\ud83d\uddfe Mapas](#subsection-two-one)\n- [\ud83d\udea7 Feature Engineering](#section-two-half)\n  - [\ud83c\udf0a Dist\u00e2ncia at\u00e9 o Mar](#subsection-two-half-one)\n  - [\ud83c\udf06 Dist\u00e2ncia at\u00e9 uma Cidade Grande](#subsection-two-half-two)\n- [\u2697\ufe0f Pr\u00e9-Processamento](#section-three)\n- [\ud83d\udd2e Predi\u00e7\u00e3o](#section-four)\n- [\ud83d\udcdd Teste](#section-five)","393151fc":"\u00c9 poss\u00edvel perceber que nossa nova feature tem uma certa correla\u00e7\u00e3o negativa com a nossa vari\u00e1vel de target.","6e1169d6":"E por fim podemos pass\u00e1-la por nosso dataframe para criar uma outra coluna:","3615337f":"Finalmente! S\u00f3 basta exportar nosso DataFrame:","6d7145cd":"E, por fim, podemos fazer a predi\u00e7\u00e3o dos dados de teste!","7ba15471":"Primeiramente, importamos as bibliotecas **numpy** e **pandas**.","6e79c42f":"<a id=\"subsection-two-half-two\"><\/a>\n## \ud83c\udf06 Dist\u00e2ncia at\u00e9 uma Cidade Grande\n\nSeguindo o mesmo processo que antes, vamos calcular a dist\u00e2ncia m\u00ednima de cada distrito at\u00e9 uma cidade grande da California. Para isto, vamos definir as coordenadas das maiores cidades do estado:","96c6a9a9":"Agora, vamos definir uma fun\u00e7\u00e3o que, dado uma determinada linha do nosso DataFrame, retorna a dist\u00e2ncia m\u00ednima desse ponto at\u00e9 o mar.","743fb361":"A renda m\u00e9dia parece ter uma rela\u00e7\u00e3o quase linear com o pre\u00e7o m\u00e9dio das casas!\n\nAgora, vamos plotar a rela\u00e7\u00e3o de todas as nossas features com o target:","00261134":"## Regress\u00e3o Linear","9b6d3a42":"<a id=\"subsection-two-half-one\"><\/a>\n## \ud83c\udf0a Dist\u00e2ncia at\u00e9 o Mar\n\nA primeira feature que vamos criar \u00e9 a dist\u00e2ncia m\u00ednima de cada cidade at\u00e9 o oceano. Para isto, vamos obter as coordenadas de determinados pontos da costa da calif\u00f3rnia usando o **Basemap** da biblioteca **matplotlib**.","45333de0":"<a id=\"subsection-two-one\"><\/a>\n## \ud83d\uddfe Mapas\n\nPara entender melhor os nossos dados, \u00e9 bem importante entender sua localiza\u00e7\u00e3o! Para isto, vamos plotar geograficamente cada distrito para entender melhor a rela\u00e7\u00e3o do seu pre\u00e7o com sua posi\u00e7\u00e3o espacial.\n\nPrimeiramente, vamos utilizar o **matplotlib** para tra\u00e7ar um gr\u00e1fico mostrando a popula\u00e7\u00e3o e o pre\u00e7o m\u00e9dio de cada dado:","2490c49e":"Casas pr\u00f3ximas ao oceano e a grandes centros urbanos costumam ter um valor maior! Seria interessante se consegu\u00edssemos extrair essas informa\u00e7\u00f5es para a predi\u00e7\u00e3o.","38ebea3b":"## Random Forest","e7635198":"## \ud83d\udcc4 Submiss\u00e3o\n\nPara submeter nossa predi\u00e7\u00e3o, vamos export\u00e1-la no formato de .csv. Para isso, primeiro vamos criar um DataFrame:","13aace0b":"Depois vamos preench\u00ea-lo com o \u00edndice de cada dado e seu respectivo r\u00f3tulo:","5b727611":"### Criando um Gr\u00e1fico Interativo com o Plotly\n\nO Plotly \u00e9 outra ferramenta muito \u00fatil para visualiza\u00e7\u00e3o! Nesse caso, vamos criar um mapa interativo com os nossos distritos:","e0841cba":"Em seguida, podemos dar uma primeira olhada nos nossos dados com o m\u00e9todo `head` do nosso dataframe.","638f60e5":"Depois, normalizaremos nossos dados:","c0ee8f96":"Tamb\u00e9m precisaremos atualizar a biblioteca **seaborn**.","f0c0af58":"Assim como a dist\u00e2ncia at\u00e9 o oceano, a nossa nova vari\u00e1vel tamb\u00e9m parece ter uma correla\u00e7\u00e3o negativa com o nosso target!","6d4f308a":"Com essa fun\u00e7\u00e3o definida, podemos criar uma nova coluna com a dist\u00e2ncia m\u00ednima at\u00e9 o litoral para cada distrito:","b8ac7c97":"## ElasticNet (Regress\u00e3o com Regulariza\u00e7\u00e3o L1 e L2)","87e87c1b":"<a id=\"section-four\"><\/a>\n# \ud83d\udd2e Predi\u00e7\u00e3o\n\nPor fim, podemos testar v\u00e1rios regressores e compar\u00e1-los!","95d1bc63":"<a id=\"section-five\"><\/a>\n# \ud83d\udcdd Teste\n\nFinalmente, agora podemos predizer os nossos dados teste e enviar uma submiss\u00e3o para o Kaggle.\n\nPrimeiramente, vamos importar os nossos dados:","afa6e030":"Primeiramente, vamos plotar a matriz de correla\u00e7\u00e3o dos nossos dados:","6ca6ed4a":"Em seguida, vamos dividir nosso dataset em features e label:","e959ecd5":"## KNN","3482d1ca":"Em seguida, vamos criar as mesmas features que criamos anteriormente para os dados de treino:","a23341e5":"## Outras Features\n\nPor fim, vamos criar algumas features extras usando as vari\u00e1veis que j\u00e1 t\u00ednhamos:","b59ca3cd":"<a id=\"section-three\"><\/a>\n# \u2697\ufe0f Pr\u00e9-Processamento\n\nDepois de adicionar nossas novas features, podemos passar para o pr\u00e9-processamento dos nossos dados.\n\nAntes de tudo, vamos eliminar as linhas duplicadas:","5d60ef05":"Conseguimos obter nosso resultado!"}}