{"cell_type":{"f0d7ccc6":"code","8570ddc1":"code","b19e429d":"code","ebeaf1f7":"code","d4c344fc":"code","fd224c75":"code","80567313":"code","d513bb97":"code","16ee5782":"code","9b1ee163":"code","bf449596":"code","e757b5d7":"code","d6002a82":"code","fd191e55":"code","8c890a04":"code","e7ff9993":"code","aa37cfad":"code","bdfdec2c":"markdown","bb5f4037":"markdown","0ca04a79":"markdown","98373cb1":"markdown","def04074":"markdown","36c07b20":"markdown","b5a39582":"markdown","dafde26f":"markdown","860f727d":"markdown","3305de29":"markdown","78374177":"markdown","75e21f6e":"markdown","54da87ec":"markdown","118d9ac6":"markdown","a3585dad":"markdown","3ed5f9be":"markdown"},"source":{"f0d7ccc6":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 70)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,GradientBoostingClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\nimport seaborn as sns\nfrom sklearn.utils import class_weight\nimport warnings\nwarnings.simplefilter(action='ignore')\n\ntrain = pd.read_csv(r'..\/input\/learn-together\/train.csv')\ntest = pd.read_csv(r'..\/input\/learn-together\/test.csv')\n\n","8570ddc1":"cover_type = {1:'Spruce\/Fir', 2:'Lodgepole Pine',3:'Ponderosa Pine',4:'Cottonwood\/Willow',5:'Aspen',6:'Douglas-fir',7:'Krummholz'}\ntrain['Cover_type_description'] = train['Cover_Type'].map(cover_type)\n\n# I put together train and test to work simultaneously on both of them\ncombined_data = [train, test]\n\ndef distance(a,b):\n    return np.sqrt(np.power(a,2)+np.power(b,2))\n\nextremely_stony = [1,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,0,1,1,1,1,1]\nstony = [0,1,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nrubbly = [0,0,1,1,1,0,0,0,0,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\nother = [0,0,0,0,0,0,1,1,0,0,0,0,0,1,0,1,1,0,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0]\n\nfor data in combined_data:\n\n    data['mean_Hillshade'] = (data['Hillshade_9am']+ data['Hillshade_Noon']+data['Hillshade_3pm'])\/3\n    data['Distance_to_hidrology'] = distance(data['Horizontal_Distance_To_Hydrology'],data['Vertical_Distance_To_Hydrology'])\n    data['Distance_hydrology_roads'] =distance(data['Vertical_Distance_To_Hydrology'], data['Horizontal_Distance_To_Roadways'])\n    data['Distance_hydrology_fire'] = distance(data['Vertical_Distance_To_Hydrology'], data['Horizontal_Distance_To_Fire_Points'])\n    data['extremely_stony_level'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@extremely_stony\n    data['stony'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@stony\n    data['rubbly'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@rubbly\n    data['other'] = data[[col for col in data.columns if col.startswith(\"Soil\")]]@other\n    data['Hillshade_noon_3pm'] = data['Hillshade_Noon']- data['Hillshade_3pm']\n    data['Hillshade_3pm_9am'] = data['Hillshade_3pm']- data['Hillshade_9am']\n    data['Hillshade_9am_noon'] = data['Hillshade_9am']- data['Hillshade_Noon']\n    data['Up_the_water'] = data['Vertical_Distance_To_Hydrology'] > 0\n    data['Horizontal_plus_vertical_distance_to_hydrology'] = data['Horizontal_Distance_To_Hydrology'] + data['Vertical_Distance_To_Hydrology']\n    data['Total_horizontal_distance'] = data['Horizontal_Distance_To_Hydrology']+ data['Horizontal_Distance_To_Roadways']+ data['Horizontal_Distance_To_Fire_Points']\n    data['Elevation_of_hydrology'] = data['Elevation']+ data['Vertical_Distance_To_Hydrology']\n    data['Elevation_of_hydrology2'] = data['Elevation']- data['Vertical_Distance_To_Hydrology']\n    data['Distance_to_firepoints plus Distance_to_roads'] = data['Horizontal_Distance_To_Fire_Points']+ data['Horizontal_Distance_To_Roadways']\n    data['Distance_to_roads plus distance_to_hydrology'] = data['Horizontal_Distance_To_Roadways'] + data['Horizontal_Distance_To_Hydrology']\n    data['Distance_to_firepoints minus Distance_to_roads'] = data['Horizontal_Distance_To_Fire_Points']- data['Horizontal_Distance_To_Roadways']\n    data['Distance_to_roads minus distance_to_hydrology'] = data['Horizontal_Distance_To_Roadways'] - data['Horizontal_Distance_To_Hydrology']\n    data['Elevation_plus_slope'] = data['Elevation']+ data['Slope']\n    data['Elevation_plus_aspect'] =data['Elevation']+ data['Aspect']\n    data['Elevation_Aspect_Slope'] = data['Elevation']+ data['Aspect']+ data['Slope']\n    data['Slope_plus_aspect'] = data['Slope']+ data['Aspect']\n    data['Hillshade9_plus_hillshadenoon_plus_hillshade3'] = data['Hillshade_9am'] + data['Hillshade_Noon']+ data['Hillshade_3pm']\n    data['Aspen'] = data['Soil_Type11']+data['Soil_Type13']+data['Soil_Type18']+data['Soil_Type19']+data['Soil_Type26']+data['Soil_Type30']\n    data['Cottonwood\/Willow'] = data['Soil_Type1']+data['Soil_Type3']+data['Soil_Type14']+data['Soil_Type17']\n    data['Douglas-fir'] = data['Soil_Type5']+data['Soil_Type10']+data['Soil_Type16']\n    data['Krummholz'] = data['Soil_Type35']+data['Soil_Type36']+data['Soil_Type37']+data['Soil_Type38']+data['Soil_Type39']+data['Soil_Type40']\n    data['Lodgepole Pine'] = data['Soil_Type8']+data['Soil_Type9']+data['Soil_Type12']+data['Soil_Type20']+data['Soil_Type25']+data['Soil_Type28']+data['Soil_Type29']+data['Soil_Type32']+data['Soil_Type33']+data['Soil_Type34']\n    data['Ponderosa Pine'] = data['Soil_Type2']+data['Soil_Type4']+data['Soil_Type6']\n    data['Spruce\/Fir'] = data['Soil_Type21']+data['Soil_Type22']+data['Soil_Type23']+data['Soil_Type24']+data['Soil_Type27']+data['Soil_Type31']\n    data['Spruce\/Fir2'] = data['Soil_Type19']+data['Soil_Type21']+data['Soil_Type22']+data['Soil_Type23']+data['Soil_Type24']+data['Soil_Type27']+data['Soil_Type31']+data['Soil_Type35']+data['Soil_Type38']+data['Soil_Type39']+data['Soil_Type40']\n    data['Lodgepole Pine2'] = data['Soil_Type2']+data['Soil_Type3']+data['Soil_Type4']+data['Soil_Type6']+data['Soil_Type8']+data['Soil_Type9']+data['Soil_Type10']+data['Soil_Type11']+data['Soil_Type12']+data['Soil_Type13']+data['Soil_Type14']+data['Soil_Type16']+data['Soil_Type17']+data['Soil_Type18']+data['Soil_Type20']+data['Soil_Type25']+data['Soil_Type26']+data['Soil_Type28']+data['Soil_Type29']+data['Soil_Type30']+data['Soil_Type32']+data['Soil_Type33']+data['Soil_Type34']+data['Soil_Type36']\n    data['Aspen_elev'] = data['Aspen'] * data['Elevation']\n    data['Cottonwood\/Willow_elev'] = data['Cottonwood\/Willow'] * data['Elevation']\n    data['Douglas-fir_elev'] = data['Douglas-fir'] * data['Elevation']\n    data['Krummholz_elev'] = data['Krummholz'] * data['Elevation']\n    data['Lodgepole Pine_elev'] = data['Lodgepole Pine'] * data['Elevation']\n    data['Ponderosa Pine_elev'] =data['Ponderosa Pine'] * data['Elevation']\n    data['Spruce\/Fir_elev'] = data['Spruce\/Fir'] * data['Elevation']\n    data['Aspen_elev2'] = data['Aspen'] * data['Elevation_of_hydrology2']\n    data['Cottonwood\/Willow_elev2'] = data['Cottonwood\/Willow'] * data['Elevation_of_hydrology2']\n    data['Douglas-fir_elev2'] = data['Douglas-fir'] * data['Elevation_of_hydrology2']\n    data['Krummholz_elev2'] = data['Krummholz'] * data['Elevation_of_hydrology2']\n    data['Lodgepole Pine_elev2'] = data['Lodgepole Pine'] * data['Elevation_of_hydrology2']\n    data['Ponderosa Pine_elev2'] =data['Ponderosa Pine'] * data['Elevation_of_hydrology2']\n    data['Spruce\/Fir_elev2'] = data['Spruce\/Fir'] * data['Elevation_of_hydrology2']\n\nsoil_columns = [col for col in train.columns if col.startswith('Soil')]\n\n# we can drop soil columns because we have group each soil according to their stoyness\nfor data in combined_data:\n    data.drop(columns = soil_columns, inplace=True)\n\n# for categorical value (up the water) we can encode them into dummy values  \ncolumns_to_encode = ['Up_the_water']\n        \ntrain = pd.get_dummies(train,columns = columns_to_encode)\ntest = pd.get_dummies(test,columns = columns_to_encode)\n\n# once we have encoded the columns we can drop them from both training and testing dataset\n\nfor data in combined_data:\n    data.drop(columns= columns_to_encode, inplace=True)\n","b19e429d":"target = 'Cover_Type'\nfeatures = [ col for col in train.columns if col not in ['Id','Cover_Type','Cover_type_description']]\n\nX = train[features]\ny = train[target]\n\n# distribution of cover types in training dataset\nax = sns.countplot(x = train['Cover_type_description'])\nax.set_title(\"Distribution of covert type in training dataset\")\nplt.xticks(rotation=90)\nplt.show()","ebeaf1f7":"X_train, X_test,y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n\nrandom_forest = RandomForestClassifier(n_estimators = 5, criterion ='entropy', max_depth =35)\nrandom_forest.fit(X_train,y_train)\n\ny_pred = random_forest.predict(X_test)\nprint(\"Accuracy random forest :\", accuracy_score(y_test, y_pred))","d4c344fc":"# we can put predictions in a dataframe along with the true values of cover types from testing dataset\nprediction = pd.DataFrame({'y_test':y_test, 'y_pred':y_pred})\nprediction['Cover_type_y_test'] = prediction['y_test'].map(cover_type)\ncover_type_desc = ['Spruce\/Fir','Lodgepole Pine','Ponderosa Pine','Cottonwood\/Willow','Aspen','Douglas-fir','Krummholz']\n\nfor cover in cover_type_desc:\n    print(\"Accuracy score for {} is: \".format(cover), accuracy_score(prediction[prediction['Cover_type_y_test']== cover]['y_test'],prediction[prediction['Cover_type_y_test']== cover]['y_pred']))\n","fd224c75":"confusion_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred), index = cover_type_desc, columns= cover_type_desc)\n\nprint(\"Confusion matrix for whole dataset :\\n\", confusion_matrix)","80567313":"feat_importances = pd.Series(random_forest.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","d513bb97":"#relationship for training dataset\ncolumns = ['Elevation', 'Aspect','Slope', 'mean_Hillshade']\n#for cover_type in list_of_cover_type:\nsns.pairplot(train, hue = \"Cover_type_description\", vars = columns)\nplt.legend()\nplt.show()","16ee5782":"X_sub = test[features]\ny_pred_sub = random_forest.predict(X_sub)\n   \nprediction_test = pd.DataFrame( {'prediction_test': y_pred_sub}, index=None)\nprediction_test['Cover_type_description'] = prediction_test['prediction_test'].map(cover_type)","9b1ee163":"ax = sns.countplot(x = prediction_test['Cover_type_description'])\nax.set_title(\"Distribution of covert type in testing dataset prediction\")\nplt.xticks(rotation=90)\nplt.show()","bf449596":"cover_type_count =prediction_test.groupby(prediction_test['prediction_test'], as_index=True).agg('count')\ncovert_type_weights = prediction_test.groupby(prediction_test['prediction_test'],as_index=True).agg('count')\/len(prediction_test)\n\nprint(\"Cover type count :\\n\")\nprint(cover_type_count)\n\nprint(\"Cover type weights:\\n\")\nprint(covert_type_weights)","e757b5d7":"#RANDOM FOREST\n\nsample_weights = {1: 0.372046, 2:0.398975,3:0.064180, 4:0.003768,5:0.059755,6:0.043810,7:0.057465}\n    \nforest_weights = RandomForestClassifier(n_estimators = 200, criterion ='entropy', max_depth =50,class_weight=sample_weights)\nforest_weights.fit(X_train, y_train)\n            \ny_pred = forest_weights.predict(X_test)\n        \nprint(\"Accuracy random forest with weights:\", accuracy_score(y_test, y_pred))","d6002a82":"#XGBOOST\nfrom xgboost import XGBClassifier \nxgbclassifier = XGBClassifier()\nxgbclassifier = XGBClassifier(n_estimators=200, random_state=0,learning_rate=0.03)\nxgbclassifier.fit(X_train, y_train)\ny_pred = xgbclassifier.predict(X_test)\n\nprint(\"Accuracy xgboost:\", accuracy_score(y_test, y_pred))","fd191e55":"#extra tree classifier\nextra_tree = ExtraTreesClassifier(n_estimators=200, max_depth= 30,criterion='entropy',bootstrap = True,class_weight=sample_weights)\nextra_tree.fit(X_train, y_train)\ny_pred = extra_tree.predict(X_test)\n\nprint(\"Accuracy extra tree:\", accuracy_score(y_test, y_pred))","8c890a04":"#gradient boosting\ngradient_boosting = GradientBoostingClassifier(n_estimators=200,max_depth= 20)\ngradient_boosting.fit(X_train, y_train)\ny_pred = gradient_boosting.predict(X_test)\n\nprint(\"Accuracy gradient boosting:\", accuracy_score(y_test, y_pred))","e7ff9993":"#STACKING MODELS TOGETHER\nfrom vecstack import stacking\n\nclassifiers = [forest_weights,xgbclassifier,extra_tree,gradient_boosting]\nX_train, X_test = train_test_split(train, test_size=0.2, random_state=5)\ny_train = X_train[target]\ny_test = X_test[target]\n\nS_train, S_test = stacking(classifiers,X_train[features], y_train, test[features],regression=False,mode='oof_pred_bag',needs_proba=False,save_dir=None,metric=accuracy_score,n_folds=3,stratified=True,shuffle=True,random_state=0,verbose=2)\nmodel = RandomForestClassifier(n_estimators = 300, criterion ='entropy', max_depth =35,class_weight=sample_weights)\nmodel = model.fit(S_train, y_train)\ny_pred = model.predict(S_test)","aa37cfad":"\n#submission\nsub = pd.DataFrame({'Id': test.Id, 'Cover_Type': y_pred})\nsub.to_csv('submission.csv', index = False)","bdfdec2c":"As we can see cover types are perfectly distribuited in training dataset, so it means that dataset is balanced. This is something that we need to keep in mind in the next parts of the kernel.\n\nAs the dataset is perfectly balanced we can reduce test_size to 20%; this will allow us to have more data to train the model with.\n","bb5f4037":"# Roosevelt National Forest classification","0ca04a79":"As we can see accuracy is really different among cover types! \nIn particular we can see that Spruce\/Fir and Lodgepole Pine have an accuracy which is by far lower than the others cover types.  This is really important. \nNow let's look at confusion matrix.","98373cb1":"Using random forest with weights there is a slight improvement in the accuracy of the model becasuse we don't weight each cover type  the same (balanced weight) but we weight more Spruce\/Fir and Lodgepole Pine.","def04074":"For Spuce\/Fir and Lodgepole Pine weights respectively 37% and 39% of the whole testing dataset (this percentages may be slightly different according to the model but the takeaway is that testing dataset is inbalanced).\n\nHow can we overcome this?\n\nWe can build a random forest with weights in which each weight is the weight of covert type in the testing dataset. Then we can create a xgboost model and stack together the two models to have a better classification","36c07b20":"From confusion matrix we have a confirmation that the classification between Spruce\/Fir and Lodgepole Pine is critical for the accuracy of the model.\nWhat are the most important features for the classification? Let's plot the top 10.","b5a39582":"In my latest kernel https:\/\/www.kaggle.com\/obiaf88\/stacking-classifiers-with-gridsearch I stacked together different classifiers in order to build a better model for Roosevel Forest classification. \nKernel had an accuracy score of 86% in the testing dataset but only a 76,6% as public score.  \nFor sure part of the difference is related to overfitting but is this the only reason for the difference in the accuracy score? \nIn this kernel I want to investigate deeper inside the results to understand if there may be others reasons.\n","dafde26f":"Let's take a look at the distribution of cover types in the testing dataframe.","860f727d":"Now we can apply the model to test dataset and put the predictions in a dataframe","3305de29":"As we can see the testing dataset is really imbalanced, unlike training dataset. \nThe most important cover types are Spuce\/Fir and Lodgepole Pine which are the cover types that are more difficult to differentiate and have a lower accuracy.\n**This is the reason why our model, beside overfitting, is perfoming so bad in the testing dataset compared to training dataset!**\nCover types counts and weights in the testing dataset are the following.\n","78374177":"If you have found this kernel useful, please upvote!","75e21f6e":"Accuracy of the model is not bad, but what is the accuracy for different cover types? \nAre there any differences? \nLet's see what is accuracy among cover types.","54da87ec":"### Features engineering","118d9ac6":"From the pairplot above we can see that elevation can differentiate between cover types pretty well but there are two cover types which are ovelapping: Spruce\/Fir and Lodgepole Pine. They grow at the same elevation so, in order to differentiate between them, we should introduce new fetures in the dataset which are different for the cover types or use the existing ones and create new useful ones with feautures engineering.","a3585dad":"All features related to elevation are really important for forests classification","3ed5f9be":"## Model building - Random Forest classifier"}}