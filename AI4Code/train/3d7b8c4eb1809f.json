{"cell_type":{"4ac5dc95":"code","ab01331e":"code","23a69a7c":"code","da20b86d":"code","797888db":"code","867d7039":"code","2bc437ad":"code","9416b8ed":"code","26d6aca0":"code","56964a02":"code","cedf6864":"code","9e921911":"code","38cd2baa":"code","74a11b1b":"code","55c28eef":"code","38b683f4":"markdown","54917269":"markdown","437cbb5b":"markdown","2999e9e4":"markdown","528f21c3":"markdown","99a5ba5e":"markdown","55f9931f":"markdown","61aabba1":"markdown","f136dd17":"markdown","0349100a":"markdown","361bb000":"markdown","3be0a497":"markdown","3af34bef":"markdown","0116a680":"markdown","7c8b9fe6":"markdown","7b4a9fc1":"markdown","68329b74":"markdown"},"source":{"4ac5dc95":"import numpy as np \nimport pandas as pd \nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\n    \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ab01331e":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","23a69a7c":"df.head()","da20b86d":"hist = df.hist(bins=100, figsize = (20,20))","797888db":"df_distributed = df[[\"V11\",\"V13\",\"V15\",\"V18\",\"V19\",\"V12\",\"Class\"]]","867d7039":"negative_df = df_distributed.loc[df_distributed['Class'] == 0]\npositive_df = df_distributed.loc[df_distributed['Class'] == 1]","2bc437ad":"from sklearn.model_selection import train_test_split\n\ny_negative = negative_df[\"Class\"]\ny_positive = positive_df[\"Class\"]\nnegative_df.drop([\"Class\"], axis=1, inplace=True)\npositive_df.drop([\"Class\"], axis=1, inplace=True)\n\n# 90% of good data are training data to estimate Gaussian factors (mean, Standard deviation and variance)\nnegative_df_training, negative_df_testing, y_negative_training, y_negative_testing = train_test_split(negative_df,\n                                                                                                      y_negative,\n                                                                                                      test_size=0.1,\n                                                                                                      random_state=0)\n# 5% for CV dataset, 5% for testing dataset\nnegative_df_cv, negative_df_testing, y_negative_cv, y_negative_testing = train_test_split(negative_df_testing,\n                                                                                          y_negative_testing,\n                                                                                          test_size=0.5,\n                                                                                          random_state=0)\n\n# while 50% the anomalies data will be added to CV dataset and the other 50% will be added to Testing dataset\npositive_df_cv, positive_df_testing, y_positive_cv, y_positive_testing = train_test_split(positive_df,\n                                                                                          y_positive,\n                                                                                          test_size=0.5,\n                                                                                          random_state=0)\n\ndf_cv = pd.concat([positive_df_cv, negative_df_cv], ignore_index=True)\ndf_cv_y = pd.concat([y_positive_cv, y_negative_cv], ignore_index=True)\ndf_test = pd.concat([positive_df_testing, negative_df_testing], ignore_index=True)\ndf_test_y = pd.concat([y_positive_testing, y_negative_testing], ignore_index=True)\n\ny_negative_training = y_negative_training.values.reshape(y_negative_training.shape[0], 1)\ndf_cv_y = df_cv_y.values.reshape(df_cv_y.shape[0], 1)\ndf_test_y = df_test_y.values.reshape(df_test_y.shape[0], 1)","9416b8ed":"def estimateGaussian(X):\n    stds=[]\n    mean = []\n    variance =[]\n    \n    mean = X.mean(axis=0)\n    stds =X.std(axis=0)\n    variance = stds **2\n    \n    stds = stds.values.reshape(stds.shape[0], 1)\n    mean = mean.values.reshape(mean.shape[0], 1)\n    variance = variance.values.reshape(variance.shape[0], 1)\n    return stds,mean,variance","26d6aca0":"stds,mean,variance = estimateGaussian(negative_df_training)","56964a02":"print(stds.shape)\nprint(stds.shape)\nprint(stds.shape)","cedf6864":"def multivariateGaussian(stds, mean, variance, df_cv):\n    probability = []\n    for i in range(df_cv.shape[0]):\n        result = 1\n        for j in range(df_cv.shape[1]):\n            var1 = 1\/(np.sqrt(2* np.pi)* stds[j])\n            var2 = (df_cv.iloc[i,j]-mean[j])**2\n            var3 = 2*variance[j]\n\n            result *= (var1) * np.exp(-(var2\/var3))\n        result = float(result)\n        probability.append(result)\n    return probability","9e921911":"def selectEpsilon(y_actual, y_probability):\n    best_epi = 0\n    best_F1 = 0\n    best_rec = 0\n    best_pre = 0\n    \n    stepsize = (max(y_probability) -min(y_probability))\/1000000\n    epi_range = np.arange(min(y_probability),max(y_probability),stepsize)\n    for epi in epi_range:\n        predictions = (y_probability<epi)[:,np.newaxis]\n        tp = np.sum(predictions[y_actual==1]==1)\n        fp = np.sum(predictions[y_actual==0]==1)\n        fn = np.sum(predictions[y_actual==1]==0)\n        \n        prec = tp\/(tp+fp)\n        rec = tp\/(tp+fn)\n        \n        if prec > best_pre:\n            best_pre =prec\n            best_epi_prec = epi\n            \n        if rec > best_rec:\n            best_rec =rec\n            best_epi_rec = epi\n            \n        F1 = (2*prec*rec)\/(prec+rec)\n        \n        if F1 > best_F1:\n            best_F1 =F1\n            best_epi = epi\n        \n    return best_epi, best_F1,best_pre,best_epi_prec,best_rec,best_epi_rec","38cd2baa":"probability = multivariateGaussian(stds, mean, variance, df_cv)\nbest_epi, best_F1,best_pre,best_epi_prec,best_rec,best_epi_rec = selectEpsilon(df_cv_y, probability)\nprint(\"The best epsilon Threshold over the croos validation set is :\",best_epi)\nprint(\"The best F1 score over the croos validation set is :\",best_F1)\nprint(\"The best epsilon Threshold over the croos validation set is for recall :\",best_epi_rec)\nprint(\"The best Recall score over the croos validation set is :\",best_rec)\nprint(\"The best epsilon Threshold over the croos validation set is for precision:\",best_epi_prec)\nprint(\"The best Precision score over the croos validation set is :\",best_pre)","74a11b1b":"def prediction_scores(y_actual, y_probability, epsilon):\n    predictions = (y_probability<epsilon)[:,np.newaxis]\n    tp = np.sum(predictions[y_actual==1]==1)\n    fp = np.sum(predictions[y_actual==0]==1)\n    fn = np.sum(predictions[y_actual==1]==0)\n        \n    prec = tp\/(tp+fp)\n    rec = tp\/(tp+fn) \n    F1 = (2*prec*rec)\/(prec+rec)\n        \n    return prec,rec,F1","55c28eef":"epsilon = best_epi\nprobability = multivariateGaussian(stds, mean, variance, df_test)\nprec,rec,F1 = prediction_scores(df_test_y, probability,epsilon)\nprint(\"Percision on Testing Set:\",prec)\nprint(\"Recall on Testing Set:\",rec)\nprint(\"F1 on Testing Set:\",F1)","38b683f4":"# ![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/74\/Normal_Distribution_PDF.svg\/340px-Normal_Distribution_PDF.svg.png)","54917269":"# run the estimate Gaussians and return the results (mean, Standered diviation and variance)","437cbb5b":"# Show the first 5 data rows","2999e9e4":"# **prepare the data by dividing them into Training, Testing and Cross validation datasets**","528f21c3":"# Read the data file","99a5ba5e":"# Select the data with the best Gaussian distribution","55f9931f":"RUN the code and print out the Results for the test set","61aabba1":" the data are very sekwed as we have almost 450 anominal samples and more than 200000 good results\n so we will divide the data as following\n \n 90% of good data are training data to estimate Gaussians factors (mean, Standered diviation and variance)\n \n 5% for CV dataset and 5% for testing dataset\n \n while 50% the anominal data will be added to CV dataset and the other 50% will be added to Testing dataset","f136dd17":"# estimate Gaussian parameters (mean, Standered diviation and variance)","0349100a":"# use the probability that we had to be used with the epsilon selection process\n","361bb000":"# Calculate the PROBABILITY for any new data CV or Testing using the factor that we have calculated and using the GAUSSIAN NORMAL DISTRIBUTION algorithm","3be0a497":"# F1, Percision and Recall for testing data","3af34bef":"# select the best EPSILON by calculation the F1, PRECESION and RECALL and select the beat epsilon for each using CV DATASET","0116a680":"# show the Histogram for each columns","7c8b9fe6":"# Import the needed libraries and show the files on the directory","7b4a9fc1":"# **Fraud Detection using Gaussian Normal Destribution**","68329b74":"# divide data into positive and nigative datasets"}}