{"cell_type":{"f3540853":"code","f29e576a":"code","d339dc23":"code","008117f3":"code","5e140a6d":"code","d9f7be1c":"code","8bc90de0":"code","981fd610":"code","af7eeb6a":"code","67bfb791":"code","e043114f":"code","bba3ebec":"code","21a3afcd":"code","7a77ff36":"code","9df4a0ed":"code","e958d267":"code","5bfa8d45":"code","459ea202":"code","a46b7171":"code","f1f75eea":"code","b959e896":"code","500f9553":"code","aefa2486":"code","ad8fb254":"code","4af8410f":"code","5a3ec2ea":"code","13a06bf8":"code","9abcbe57":"code","ee06db0f":"code","a7e37eef":"code","2c384ab9":"code","6fd8bc4b":"code","e3232333":"code","d133db3d":"code","cdf959ab":"code","9d15f3f6":"code","5a88c107":"code","a78ac74e":"code","0e10629e":"code","6278fff6":"code","595e04e7":"code","db9f0c9e":"code","39854b8d":"code","810b3421":"code","c7cef91a":"code","64b49719":"code","7d858fde":"code","fbc0c31c":"code","c85716d2":"code","175d69c2":"code","66300d7d":"markdown","7afd12ae":"markdown","27d8ee8f":"markdown","f15c9f2c":"markdown","b592e776":"markdown","56393002":"markdown","073606ab":"markdown","0bd2e230":"markdown","3a5efd31":"markdown","45fc028a":"markdown","3ca45753":"markdown","05de7e36":"markdown","600841e1":"markdown","14c902b8":"markdown","7b86d648":"markdown","382b8215":"markdown","e68affd2":"markdown","8d7bccf7":"markdown","2dcdca27":"markdown","395aaf7d":"markdown","19d559c8":"markdown","0037b416":"markdown","522b6942":"markdown","9f078ae4":"markdown","f893a80b":"markdown","a0e88d3d":"markdown","9038af1c":"markdown","ea75dec3":"markdown","77e6452c":"markdown","6d623cd8":"markdown","39092f04":"markdown","891f577c":"markdown","f0907d1c":"markdown","23ebdc98":"markdown","3baf354a":"markdown","61fd61ec":"markdown","96c4320a":"markdown","73260968":"markdown","bdb66d3e":"markdown","0914bc25":"markdown","d0e5c6dc":"markdown","402ec568":"markdown","1957b012":"markdown","73fa8818":"markdown","bd4de97b":"markdown","aaa53b39":"markdown","c107b609":"markdown","bd9cde0d":"markdown"},"source":{"f3540853":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport warnings\nwarnings.simplefilter(action='ignore', category = FutureWarning)\n\nplt.rcParams.update({'figure.max_open_warning': 0})","f29e576a":"from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, recall_score, f1_score \nfrom sklearn.metrics import plot_confusion_matrix, roc_auc_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nimport xgboost as xgb","d339dc23":"df = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndf.drop([\"id\", \"Unnamed: 32\"], axis = 1, inplace = True)\ndf.sample(10)","008117f3":"df.shape","5e140a6d":"df[df.duplicated()]","d9f7be1c":"df.info()","8bc90de0":"df.describe()","981fd610":"df.diagnosis.value_counts()","af7eeb6a":"fig, ax = plt.subplots(figsize = (8, 8))\n\nax.pie(df.diagnosis.value_counts(), labels = [\"Benign\", \"Malignant\"], \n       autopct = '%1.2f%%', startangle = 180, colors = [\"#0EB8F1\", \"#F1480F\"])\n\nax.set_title(\"Diagnosis\")\nplt.show()","67bfb791":"df[\"diagnosis\"] = df[\"diagnosis\"].map({\"M\": 1, \"B\": 0})","e043114f":"target = \"diagnosis\"\npredictors = [col for col in df.columns if col != target]","bba3ebec":"def feature_dist_clas(df, col, hue):\n    \n    fig, axes = plt.subplots(1, 4, figsize = (25, 5))\n    order = sorted(df[hue].unique())\n    palette = [\"#0EB8F1\", \"#F1480F\"]\n    \n    sns.histplot(x = col, hue = hue, data = df, ax = axes[0], palette = palette, edgecolor=\"black\", linewidth=0.5)\n    sns.kdeplot(x = col, hue = hue, data = df, fill = True, ax = axes[1], palette = palette, linewidth = 2)\n    sns.boxplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[2], \n                palette = palette, linewidth = 2, flierprops = dict(marker = \"x\", markersize = 3.5))\n    \n    sns.violinplot(y = col, hue = hue, data = df, x = [\"\"] * len(df), ax = axes[3], palette = palette)\n    \n    fig.suptitle(\"For Feature:  \" + col)\n    axes[0].set_title(\"Histogram For Feature \" + col)\n    axes[1].set_title(\"KDE Plot For Feature \" + col)   \n    axes[2].set_title(\"Boxplot For Feature \" + col)   \n    axes[3].set_title(\"Violinplot For Feature \" + col)   \n    \n    for ax in axes:\n        ax.set_facecolor(\"#C7D3D4FF\")\n        ax.grid(linewidth = 0.25)","21a3afcd":"for col in predictors:\n    feature_dist_clas(df, col, \"diagnosis\")","7a77ff36":"def feature_distribution(df, col):\n    \n    skewness = np.round(df[col].skew(), 3)\n    kurtosis = np.round(df[col].kurtosis(), 3)\n\n    fig, axes = plt.subplots(1, 3, figsize = (18, 6))\n    \n    sns.kdeplot(data = df, x = col, fill = True, ax = axes[0], color = \"#603F83\", linewidth=2)\n    sns.boxplot(data = df, y = col, ax = axes[1], color = \"#603F83\",\n                linewidth = 2, flierprops = dict(marker = \"x\", markersize = 3.5))\n    stats.probplot(df[col], plot = axes[2])\n\n    axes[0].set_title(\"Distribution \\nSkewness: \" + str(skewness) + \"\\nKurtosis: \" + str(kurtosis))\n    axes[1].set_title(\"Boxplot\")\n    axes[2].set_title(\"Probability Plot\")\n    fig.suptitle(\"For Feature:  \" + col)\n    \n    for ax in axes:\n        ax.set_facecolor(\"#C7D3D4FF\")\n        ax.grid(linewidth = 0.1)\n    \n    axes[2].get_lines()[0].set_markerfacecolor('#8157AE')\n    axes[2].get_lines()[0].set_markeredgecolor('#603F83')\n    axes[2].get_lines()[0].set_markeredgewidth(0.1)\n    axes[2].get_lines()[1].set_color('#F1480F')\n    axes[2].get_lines()[1].set_linewidth(3)\n    \n    sns.despine(top = True, right = True, left = True, bottom = True)\n    plt.show()","9df4a0ed":"for col in predictors:\n    feature_distribution(df, col)","e958d267":"def heatmap(df):\n    \n    fig, ax = plt.subplots(figsize = (15, 15))\n    \n    sns.heatmap(df.corr(), cmap = \"coolwarm\", annot = True, fmt = \".2f\", annot_kws = {\"fontsize\": 9},\n                vmin = -1, vmax = 1, square = True, linewidths = 0.25, linecolor = \"black\", cbar = False)\n    \n    sns.despine(top = True, right = True, left = True, bottom = True)\n    \nheatmap(df)","5bfa8d45":"X_train, X_test, y_train, y_test = train_test_split(df[predictors],\n                                                    df[target],\n                                                    test_size = 0.3,\n                                                    random_state = 42,\n                                                    stratify = df[target])","459ea202":"train_skew, test_skew = [], []\ntrain_kurtosis, test_kurtosis = [], []\nfor col in predictors:\n    \n    train_skew.append(X_train[col].skew())\n    test_skew.append(X_test[col].skew())\n    train_kurtosis.append(X_train[col].kurtosis())\n    test_kurtosis.append(X_test[col].kurtosis())\n    \nskew_df = pd.DataFrame({\"Feature\": predictors, \"TrainSkewness\": train_skew, \"TestSkewness\": test_skew,\n                        \"TrainKurtosis\": train_kurtosis, \"TestKurtosis\": test_kurtosis})\nskewed = skew_df[skew_df.TrainSkewness.abs() >= 0.5]\nskewed","a46b7171":"train_skew_yeoj, test_skew_yeoj = [], []\ntrain_kurtosis_yeoj, test_kurtosis_yeoj = [], []\n\nfor col in skewed.Feature.tolist():\n    \n    X_train[col], fitted_lambda = stats.yeojohnson(X_train[col])\n    X_test[col] = stats.yeojohnson(X_test[col], fitted_lambda)\n    \n    train_skew_yeoj.append(X_train[col].skew())\n    test_skew_yeoj.append(X_test[col].skew())    \n    train_kurtosis_yeoj.append(X_train[col].kurtosis())\n    test_kurtosis_yeoj.append(X_test[col].kurtosis())\n    \nskewed.loc[:, \"TrainSkew_Transformed\"] = train_skew_yeoj\nskewed.loc[:, \"TestSkew_Transformed\"] = test_skew_yeoj\nskewed.loc[:, \"TrainKurtosis_Transformed\"] = train_kurtosis_yeoj\nskewed.loc[:, \"TestKurtosis_Transformed\"] = test_kurtosis_yeoj\n\nskewed","f1f75eea":"for col in predictors:   \n    \n    scaler = RobustScaler()\n    X_train[col] = scaler.fit_transform(X_train[col].values.reshape(-1, 1))\n    X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))","b959e896":"fig, ax1 = plt.subplots(figsize=(15, 8))\nax1.set_facecolor(\"#393838\")\n\nX = X_train.values\ny = y_train.values\n\nf1 = 17\nf2 = 23\n\nax1.scatter(X[y == 0, f1], X[y == 0, f2], label = \"Benign\", alpha = 1, linewidth = 0, c = \"#0EB8F1\")\nax1.scatter(X[y == 1, f1], X[y == 1, f2], label = \"Malignant\", alpha = 1, linewidth = 0, c = '#F1480F', marker = \"X\")\n\nax1.legend()\nsns.despine(top = True, right = True, left = True, bottom = True)","500f9553":"X_pca = df[predictors].copy()\npca = PCA(n_components = 2, random_state = 42)\ncomponents = pca.fit_transform(X_pca)\ncomp_df = pd.DataFrame(components, columns=[\"X1\", \"X2\"])\nplot_df = pd.concat([comp_df, df[target]], axis=1)\n\n\nfig, ax = plt.subplots(figsize=(14, 8))\nax.set_facecolor(\"#393838\")\n\nsns.scatterplot(x = \"X1\", y = \"X2\", alpha = 0.8, data = plot_df[plot_df.diagnosis == 0], \n                ax = ax, label = \"Benign\", linewidth = 0, color = \"#0EB8F1\")\n\nsns.scatterplot(x = \"X1\", y = \"X2\", alpha = 0.8, data = plot_df[plot_df.diagnosis == 1], \n                ax = ax, label = \"Malignant\", marker=\"X\", linewidth = 0, color = '#F1480F')\n\nax.set_title(\"PCA with 2 Component, Explained Variance:  \\n\" + \n             str(pca.explained_variance_ratio_[0].round(5)) + \", \" +\n             str(pca.explained_variance_ratio_[1].round(5)))\n\nsns.despine(top = True, right = True, left = True, bottom = True)","aefa2486":"skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n\ndef cv_model(model, X = X_train, y = y_train, cv = skf, scoring = \"f1\"):\n    return cross_val_score(model, X, y, scoring = scoring, cv = cv, n_jobs = -1).mean()","ad8fb254":"logreg = LogisticRegression(random_state = 42)\nsvc = SVC(random_state=42, probability = True)\ngnb = GaussianNB()\nrfc = RandomForestClassifier(random_state = 42)\nknnc = KNeighborsClassifier(n_jobs = -1)\nlgbc = lgb.LGBMClassifier(random_state = 42, n_jobs = -1)\ndtc = DecisionTreeClassifier(random_state = 42)\nxgbc = xgb.XGBClassifier(random_state = 42, n_jobs = -1, use_label_encoder = False, eval_metric = \"logloss\")\n\n\nmodels = [logreg, svc, gnb, rfc, knnc, lgbc, dtc, xgbc]","4af8410f":"def model_results(model, xtrain = X_train, ytrain = y_train, xtest = X_test, ytest = y_test):\n\n    sns.set_style(\"white\") \n    import matplotlib\n    matplotlib.rcParams.update({'font.size': 12})\n\n    model.fit(xtrain, ytrain)\n    train_preds = model.predict(xtrain)\n    test_preds = model.predict(xtest)\n    \n    fig, axes = plt.subplots(1, 2, figsize = (15, 5))\n\n    plot_confusion_matrix(model, xtrain, ytrain, \n                          cmap = plt.cm.Reds, display_labels = [\"Benign\", \"Malignant\"], \n                          values_format = \"6d\", normalize = None, ax = axes[0], colorbar = False)\n    \n    plot_confusion_matrix(model, xtest, ytest, \n                          cmap = plt.cm.Reds, display_labels = [\"Benign\", \"Malignant\"], \n                          values_format = \"6d\", normalize = None, ax = axes[1], colorbar = False)\n    \n    print(\"For training set: \\n\")\n    print(classification_report(ytrain, train_preds, target_names = [\"Benign\", \"Malignant\"], digits = 3))\n    print(\"Roc-Auc Score: \" , roc_auc_score(ytrain, train_preds).round(3))\n    print(\"5 Fold CV Score (Avg): \" , cv_model(model, xtrain, ytrain).round(3))\n    \n \n    print(\"\\nFor test set: \\n\")\n    print(classification_report(ytest, test_preds, target_names = [\"Benign\", \"Malignant\"], digits = 3))    \n    print(\"Roc-Auc Score: \" , roc_auc_score(ytest, test_preds).round(3))\n    \n    axes[0].set_title(\"Training Set\")\n    axes[1].set_title(\"Test Set\")\n    fig.suptitle(\"For model \" + type(model).__name__)\n    sns.despine(top = True, right = True, left = True, bottom = True)\n    plt.show()","5a3ec2ea":"model_results(logreg)","13a06bf8":"model_results(svc)","9abcbe57":"model_results(xgbc)","ee06db0f":"train_accuracy, test_accuracy, train_recall, test_recall = {}, {}, {}, {}\ntrain_f1, test_f1, train_auc, test_auc = {}, {}, {}, {}\ncv_score_f1, cv_accuracy, cv_recall = {}, {}, {}\n\nfor model in models:\n    \n    name = type(model).__name__\n    model.fit(X_train, y_train)\n    train_preds = model.predict(X_train)\n    test_preds = model.predict(X_test)\n    \n    train_accuracy[name] = accuracy_score(y_train, train_preds).round(4)\n    test_accuracy[name] = accuracy_score(y_test, test_preds).round(4)\n    train_recall[name] = recall_score(y_train, train_preds).round(4)\n    test_recall[name] = recall_score(y_test, test_preds).round(4)\n    train_f1[name] = f1_score(y_train, train_preds).round(4)\n    test_f1[name] = f1_score(y_test, test_preds).round(4)\n    train_auc[name] = roc_auc_score(y_train, train_preds).round(4)\n    test_auc[name] = roc_auc_score(y_test, test_preds).round(4)    \n    cv_score_f1[name] = cv_model(model, scoring = \"f1\")\n    cv_accuracy[name] = cv_model(model, scoring = \"accuracy\")\n    cv_recall[name] = cv_model(model, scoring = \"recall\")\n    \nscores = pd.DataFrame(\n    [cv_score_f1, cv_accuracy, cv_recall, train_accuracy, test_accuracy, \n     train_recall, test_recall, train_f1, test_f1, train_auc, test_auc], \n     [\"CVF1\", \"CVAccuracy\", \"CVRecall\", \"TrainAccuracy\", \"TestAccuracy\", \"TrainRecall\", \n      \"TestRecall\", \"TrainF1\", \"TestF1\", \"TrainAuc\", \"TestAuc\"]).T","a7e37eef":"scores","2c384ab9":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(svc, random_state = 42).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(), top = 30)","6fd8bc4b":"ax = lgb.plot_importance(lgbc, max_num_features = 30, grid = False, figsize = (12, 8), \n                         edgecolor = \"#603F83\", color = \"#603F83\",\n                         title = \"Feature Importances for Lightgbm Classifier\")\n\nax.set_facecolor(\"#C7D3D4\")\nplt.show()","e3232333":"lgbc.feature_importances_","d133db3d":"fig, ax = plt.subplots(figsize = (12, 8))\nax.set_facecolor(\"#C7D3D4\")\n\nxgb.plot_importance(xgbc, ax = ax, grid = False, max_num_features = 30, edgecolor = \"#603F83\", color = \"#603F83\",\n                    title = \"Feature Importances for XGBoost Classifier\")","cdf959ab":"from sklearn import tree\nimport graphviz\n\ntree_graph = tree.export_graphviz(dtc, out_file = None, feature_names = predictors)\ngraphviz.Source(tree_graph)","9d15f3f6":"lgb.create_tree_digraph(lgbc, show_info = [\"split_gain\", \"data_percentage\"])","5a88c107":"fig, ax = plt.subplots(figsize = (20, 12))\n\nxgb.plot_tree(xgbc, ax = ax)","a78ac74e":"from pdpbox import pdp, get_dataset, info_plots\n\npdp_ = pdp.pdp_isolate(model = svc, dataset = X_test, model_features = predictors, feature = 'texture_mean')\n\npdp.pdp_plot(pdp_, 'texture_mean')\nplt.show()","0e10629e":"pdp_ = pdp.pdp_isolate(model = lgbc, dataset = X_test, model_features = predictors, feature = 'texture_mean')\n\npdp.pdp_plot(pdp_, 'texture_mean')\nplt.show()","6278fff6":"pdp_ = pdp.pdp_isolate(model = svc, dataset = X_test, model_features = predictors, feature = 'concavity_se')\n\npdp.pdp_plot(pdp_, 'concavity_se')\nplt.show()","595e04e7":"pdp_ = pdp.pdp_isolate(model = xgbc, dataset = X_test, model_features = predictors, feature = 'concavity_se')\n\npdp.pdp_plot(pdp_, 'concavity_se')\nplt.show()","db9f0c9e":"import shap\n\ndata_point = X_test.iloc[111].values.reshape(1, -1)","39854b8d":"explainer = shap.TreeExplainer(lgbc)\n\nshap_values = explainer.shap_values(data_point)\n\nprint(lgbc.predict_proba(data_point).round(3))\n\nshap.initjs()\n\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_point, feature_names = X_test.columns, link = \"logit\")","810b3421":"explainer = shap.TreeExplainer(rfc)\n\nshap_values = explainer.shap_values(data_point)\n\nprint(rfc.predict_proba(data_point).round(3))\n\nshap.initjs()\n\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_point, feature_names = X_test.columns)","c7cef91a":"k_explainer = shap.KernelExplainer(svc.predict_proba, X_train)\n\nprint(svc.predict_proba(data_point).round(3))\n\nk_shap_values = k_explainer.shap_values(data_point)\n\nshap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_point, feature_names = X_test.columns)","64b49719":"k_explainer = shap.KernelExplainer(rfc.predict_proba, X_train)\nk_shap_values = k_explainer.shap_values(data_point)\nshap.force_plot(k_explainer.expected_value[1], k_shap_values[1], data_point, feature_names = X_test.columns)","7d858fde":"explainer = shap.TreeExplainer(lgbc)\n\nshap_values = explainer.shap_values(X_test)\n\nshap.summary_plot(shap_values[1], X_test)","fbc0c31c":"svc.fit(X_train, y_train)\n\n# X_train_summary = shap.kmeans(X_train, 25)\n\nexplainer = shap.KernelExplainer(svc.predict_proba, X_train[:100])\nshap_values = explainer.shap_values(X_test[:100])\nshap.force_plot(explainer.expected_value[0], shap_values[0], X_test[:100])","c85716d2":"explainer = shap.TreeExplainer(lgbc)\n\nshap_values = explainer.shap_values(X_test)\n\nshap.dependence_plot('perimeter_worst', shap_values[1], X_test)","175d69c2":"shap.dependence_plot('texture_mean', shap_values[1], X_test)","66300d7d":"**Note:** I am not comfortable with below codes. I have to spend time on this subject.","7afd12ae":"### **Handling skewness with using Yeo-Johnson transformation**","27d8ee8f":"**y-axis** - change in prediction\n\n**blue shaded are** - level of confidence","f15c9f2c":"For above graphs;\n\nWe can easily say that, our target's classes are quitely balanced.\n\nFor PCA graph, we can detect malignants easily.","b592e776":"# 5. Preprocessing","56393002":"## 6.2 Plotting with PCA, 2 Component","073606ab":"## 2.1 Features","0bd2e230":"# 7. Model Comparison","3a5efd31":"https:\/\/www.kaggle.com\/dansbecker\/permutation-importance\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html\n\nhttps:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html","45fc028a":"### 8.3.1 For Decision Tree","3ca45753":"### 8.2.1 For Lightgbm","05de7e36":"# 1. Packages","600841e1":"shap_values[1] --> for Malignant Class","14c902b8":"For above graphs;\n\nFirst graph is partial dependence plot of **concavity_se** for model Support Vector Classifier (svc)\n\nSecond graph is partial dependence plot of **concavity_se** for model XGBClassifier (xgbc)\n\nRemember, **concavity_se** variable is one of the least important features for svc permutation importance and least important feature for xgbc.\n\nIf we look at the first plot, we can say that, malignant probability are increases consistently. (about 0.025)\n\nIf we look at second plot, for xgbc, **concavity_se** feature has no impact for detecting malignance until 0.1 concavity_se. After, probability of malignance decreases too little when concavity_se increases. But, this decrease is too little, it is about 0.0001.\n\n\nFinally, I just want to say, variables could have a different impact for different models. A feature could be the most important feature for some model and have significant impact for prediction, but on the contrary that feature could be least important feature for different models and maybe it has too little impact for prediction.","7b86d648":"https:\/\/www.kaggle.com\/dansbecker\/partial-plots\n\nhttps:\/\/pdpbox.readthedocs.io\/en\/latest\/pdp_isolate.html","382b8215":"## 8.2 Feature Importances","e68affd2":"**Scaling predictors with using Robust Scaler**","8d7bccf7":"### 4.1 Takeaways - Features I","2dcdca27":"# 8.3 Visualizing Trees","395aaf7d":"# 6. EDA for Target","19d559c8":"### 8.3.2. For Lightgbm","0037b416":"# 8. Model Explainability\n\nInspiration: \n\nhttps:\/\/www.kaggle.com\/tentotheminus9\/what-causes-heart-disease-explaining-the-model\n\nhttps:\/\/www.kaggle.com\/learn\/machine-learning-explainability","522b6942":"radius_mean --> larger values for malignant class, easily separable\n\ntexture_mean --> larger values for malignant class, but benign class has lots of outliers, it could be a problem\n\nperimeter_mean --> larger values for malignant class, easily separable\n\narea_mean --> larger values for malignant class, easily separable\n\nsmoothness_mean --> almost similar distributions for both class, a little bit larger median for malignant, hardly separable\n\ncompactness_mean --> larger values for malignant class, separable\n\nconcavity_mean --> larger values for malignant class, but benign class has lots of outliers, but it seems easily separable\n\nconcave points_mean --> larger values for malignant class, easily separable\n\nsymmetry_mean --> almost similar distributions for both class\n\nfractal_dimension_mean --> similar, benign class has lots of outliers, malignant class has wider range\n\nGenerally, benign's kurtosis greater than malignant's kurtosis, so that malignant classes have wider range.\n\nAlso, we have a little skewness and kurtosis problem, but distributions' shape similar to normal (not normal). With transformations, we minimize this problem","9f078ae4":"## 8.1 Permutation Importance","f893a80b":"Lots of features have problem with skewness and kurtosis. Generally, we have right skewed and large kurtosis features. To solve this problem and get normal distributed features, I'll apply transformations.","a0e88d3d":"### 4.1 Takeaways - Features II","9038af1c":"This codes helps us to visualize target distribution with respect to two features.\n\nYou can free to change f1 and f2 (feature1 and feature2) for examining two features.","ea75dec3":"https:\/\/www.kaggle.com\/dansbecker\/shap-values\n\nhttps:\/\/shap-lrjball.readthedocs.io\/en\/docs_update\/examples.html\n\nhttps:\/\/www.kaggle.com\/dansbecker\/advanced-uses-of-shap-values","77e6452c":"**LogisticRegression**, **SVC**, **KNeighborsClassifier** and **LGBMClassifier** have best cv scores for f1-score. (over 95%)\n\nThey also have best accuracy and recall scores on cv.\n\n**RandomForestClassifier** has nearly 96% accuracy on test data, but its recall score on test is just 89%. In this problem, recall, basically means that the power of detecting malignancy. So, RandomForestClassifier is not a good model for this problem. \n\n**KNeighborsClassifier and LGBMClassifier** have good scores on cv but their performances on test data are not very good.\n\n**XGBClassifier** is also good model. It has great cv scores and it performs well on test data.\n\n**LogisticRegression** has great cv scores and it also performs well on test data.\n\n**DecisionTreeClassifier is worst model for this problem**. Smallest cv scores and overfitting problem.\n\n**SVC is best model for this problem**. It has best cv scores (nearly 95%, min.) and great results on test data. SVC model has nearly **99% accuracy**, **97% recall** and nearly **98% f1 score** **on test data**.","6d623cd8":"### 8.3.3. For XGBoost","39092f04":"# 8.5 Shap Package","891f577c":"For above graphs;\n\nFirst graph is partial dependence plot of **texture_mean** for model Support Vector Classifier (svc)\n\nSecond graph is partial dependence plot of **texture_mean** for model LGBClassifier (lgbc)\n\nRemember, **texture_mean** variable is the most important feature for lgbc feature importances, and third most important feature for model svc with respect to permutation importance.\n\nIf we look at the first plot, we can say that, malignant probability are increases consistently.\n\nIf we look second plot, for model lgbc, malignant probability are generally increases when texture_mean increas. But, until -0.6 this feature doesn't have an effect on our predictions. Also, more than 0.5 texture_mean doesn't have significant effect on beign malignant. ","f0907d1c":"https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nhttps:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\n\n**1) ID number**\n\n**2) Diagnosis** (M = malignant, B = benign)\n\nTen real-valued features are computed for each cell nucleus:\n\n**a) radius** (mean of distances from center to points on the perimeter)\n\n**b) texture** (standard deviation of gray-scale values)\n\n**c) perimeter**\n\n**d) area**\n\n**e) smoothness** (local variation in radius lengths)\n\n**f) compactness** (perimeter^2 \/ area - 1.0)\n\n**g) concavity** (severity of concave portions of the contour)\n\n**h) concave points** (number of concave portions of the contour)\n\n**i) symmetry**\n\n**j) fractal dimension** (\"coastline approximation\" - 1)\n\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","23ebdc98":"##  6.1 Plotting with Two Features","3baf354a":"For permutation importance;\n\nFeatures on top like texture_worst, smoothness_worst et cetera, are most important features. Features on bottom like compactness_se or fractal_dimension_se have least importance","61fd61ec":"# 8.4 Partial Dependence Plot (PDP)","96c4320a":"### 7.1 Metric\n\nTarget variable has 37-63 class distribution, we can use accuracy as a metric. But in this problem, we need more than accuracy. **For biostatistical problems or subjects with related to health; precision, recall, and f1-score are more important.**\n\nPrecision: Observations we correctly identify as a malignant out of all the malignant observations.\n\nRecall : Ratio of successfully detected malignant observations over all malignant observations.\n\nF1 is harmonic average of precision and recall.","73260968":"**texture_mean**, **concave_points_worst**, **texture_worst**, **concave_points_mean**, and **perimeter_worst** are top five most important features for lightgbm classifier model.\n\nTest Accuracy: 0.9649\n\nTest F1: 0.9508\n\nTest Recall: 0.9062\n\nfor lgbmclassifier model with default parameters.","bdb66d3e":"Color represents feature's value; blue for low, purple for high values\n\nHorizontal line represents impact on model.\n\nFor example;\nIf we look at upper right part of the graph, **perimeter_worst** and **concave_points_worst** variables has an great impact on predicting malignancy when their values are great.","0914bc25":"There are no duplicated rows and missing values.","d0e5c6dc":"All of our features are numerical","402ec568":"### 7.2 Takeaways - Model Comparison","1957b012":"# 3. Target","73fa8818":"### 8.2.2 For XGBoost","bd4de97b":"# 4. EDA","aaa53b39":"**texture_worst**, **area_se**, **texture_mean**, **concave_points_mean**, and **compactness_se** are top five most important features for XGBoost classifier. \n\nTest Accuracy: 0.9825\n\nTest F1: 0.9760\n\nTest Recall: 0.9531\n\nfor xgbclassifier model with default parameters.","c107b609":"# 2. General Infos About Data","bd9cde0d":"37-63 target distribution. I will use stratification on splitting data and cross validation, but we don't need to resample data."}}