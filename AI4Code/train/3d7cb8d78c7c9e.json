{"cell_type":{"5418cca9":"code","6a94c557":"code","996b8464":"code","15f82689":"code","ca812131":"code","bd0da698":"code","4f84351f":"code","26cf88a1":"code","7fbe721a":"code","f70a131a":"code","4e9bd1f4":"code","04ad626b":"code","f7b85e24":"code","e6341802":"code","c000f43e":"code","6d67c922":"code","030610ab":"code","2c027b34":"code","8bf8c6ab":"code","b4aed738":"code","257de334":"code","74bf1925":"code","ac01946b":"code","9851f66b":"code","7d41abe0":"code","bdd40d4e":"code","3adf5414":"code","a13cc443":"markdown","1bc0c50a":"markdown","eed950e9":"markdown","3364498f":"markdown","7646ec71":"markdown","1ef8d444":"markdown","a96c18f3":"markdown","2e8cac73":"markdown","ae0abba1":"markdown","4786171b":"markdown","6803e071":"markdown","e56f9a5c":"markdown","e0166dd1":"markdown"},"source":{"5418cca9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n #       print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a94c557":"%matplotlib inline\nimport scipy, matplotlib.pyplot as plt, sklearn, IPython.display as ipd\nimport librosa, librosa.display\nfrom sklearn.preprocessing import MinMaxScaler\n# import stanford_mir; stanford_mir.init()\n\n\ndef init():\n    plt.style.use('seaborn-muted')\n    #plt.rcParams['figure.figsize'] = (14, 5)\n    plt.rcParams['axes.grid'] = True\n    plt.rcParams['axes.spines.left'] = False\n    plt.rcParams['axes.spines.right'] = False\n    plt.rcParams['axes.spines.bottom'] = False\n    plt.rcParams['axes.spines.top'] = False\n    plt.rcParams['axes.xmargin'] = 0\n    plt.rcParams['axes.ymargin'] = 0\n    plt.rcParams['image.cmap'] = 'gray'\n    plt.rcParams['image.interpolation'] = None\n\ninit() ","996b8464":"#x, sr = librosa.load(r'\/content\/drive\/My Drive\/dataset\/audio\/instrument\/c_strum.wav')\nx, sr = librosa.load(r'..\/input\/audio-cats-and-dogs\/cats_dogs\/train\/cat\/cat_1.wav')\n\nipd.Audio(x, rate=sr)","15f82689":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr, alpha=0.8)","ca812131":"ipd.Image(\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/ea\/ADSR_parameter.svg\/640px-ADSR_parameter.svg.png\")","bd0da698":"T = 2.0 # seconds\nf0 = 1047.0\nsr = 22050\nt = np.linspace(0, T, int(T*sr), endpoint=False) # time variable\nx = 0.1*np.sin(2*np.pi*f0*t) # y = A * 2 * (pie)(omega) * t\nprint(x.shape)\nipd.Audio(x, rate=sr)\n","4f84351f":"X = scipy.fft(x[:4096])\nX_mag = np.absolute(X)        # spectral magnitude\nf = np.linspace(0, sr, 4096)  # frequency variable\nplt.figure(figsize=(14, 5))\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Amplitude')","26cf88a1":"x, sr = librosa.load(r'..\/input\/free-spoken-digits\/free-spoken-digit-dataset-master\/recordings\/0_george_10.wav') # c_strum\nprint(x.shape)\nipd.Audio(x, rate=sr)","7fbe721a":"X = scipy.fft(x[10000:14096])\nX_mag = np.absolute(X)\nplt.figure(figsize=(14, 5))\nplt.plot(f[:2000], X_mag[:2000]) # magnitude spectrum\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Amplitude')","f70a131a":"ipd.HTML('<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/T9x2rvdhaIE\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","4e9bd1f4":"ipd.HTML('<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/AA4BAqN3FSw\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","04ad626b":"x, sr = librosa.load(r'..\/input\/wavfiles-of-instruments-audio\/10Organ.wav')\nipd.Audio(x, rate=sr)\nx.size","f7b85e24":"hop_length = 512 #  frame size or the size of the FFT\nn_fft = 2048 # length of the windowed signal after padding with zeros\nX = librosa.stft(x, n_fft=n_fft, hop_length=hop_length)","e6341802":"float(hop_length)\/sr # units of seconds","c000f43e":"float(n_fft)\/sr  # units of seconds","6d67c922":"X.shape # Gives frequency bins and frames in time of fft.","030610ab":"S = librosa.amplitude_to_db(abs(X))\nplt.figure(figsize=(15, 5))\nlibrosa.display.specshow(S, sr=sr, hop_length=hop_length, x_axis='time', y_axis='linear')\nplt.colorbar(format='%+2.0f dB')","2c027b34":"hop_length = 256\nS = librosa.feature.melspectrogram(x, sr=sr, n_fft=4096, hop_length=hop_length)","8bf8c6ab":"logS = librosa.power_to_db(abs(S)) # Human perception of sound is logarithamic in nature","b4aed738":"plt.figure(figsize=(15, 5))\nlibrosa.display.specshow(logS, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\nplt.colorbar(format='%+2.0f dB')","257de334":"#plt.rcParams['figure.figsize'] = (14,4)\nx, sr = librosa.load(r'..\/input\/wavfiles-of-instruments-audio\/11Electric_Guitar.wav')\nlibrosa.display.waveplot(x, sr=sr)","74bf1925":"ipd.Audio(x, rate=sr)","ac01946b":"mfccs = librosa.feature.mfcc(x, sr=sr, n_mfcc=13)\nprint(mfccs.shape) # computed MFCCs over frames.","9851f66b":"librosa.display.specshow(mfccs, sr=sr, x_axis='time')","7d41abe0":"def min_max_scaler_mfcc(mfcc):\n  #print('Before transformation', mfccs[0][:10])\n  scaler = MinMaxScaler(feature_range=(0,1))\n  scaler = scaler.fit(mfcc)\n  scaledMfcc = scaler.transform(mfcc)\n  #print('After transformation', data[0][:10])\n  return scaledMfcc\nmfccs = min_max_scaler_mfcc(mfccs) \nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time') ","bdd40d4e":"mfccs = sklearn.preprocessing.scale(mfccs, axis=1)\nprint(mfccs.mean(axis=1))\nprint(mfccs.var(axis=1))","3adf5414":"librosa.display.specshow(mfccs, sr=sr, x_axis='time')","a13cc443":"\n**Timbre** is the quality of sound that distinguishes the tone of different instruments and voices even if the sounds have the same pitch and loudness.","1bc0c50a":"**Frequency domain representation**\n\n**Fourier Transform**  \n\nAn audio signal is a resultant of multiple frequencies.We record the resultant amplitude of sound waves.Fourier transform decompose the signal into its constituent frequencies along with the magnitude of each frequency.  \n\nIn the below code snipet you many see the use of fft(FastFouriertransform) function from Scipy.When FT(Fourier Transform) acts on continous signal, FFT acts on discretized signal to produce Discrete Fourier Transform(DFT).\n","eed950e9":"# Short-Time Fourier Transform\n\n\nAudio signals are highly non-stationary, i.e. their statistics change over time. It would be rather meaningless to compute a single Fourier transform over an entire 10-minute song.\n\nThe **short-time Fourier transform (STFT)** (Wikipedia; FMP, p. 53) is obtained by computing the Fourier transform for successive frames in a signal.\n\n                X(m,\u03c9)=\u2211nx(n)w(n\u2212m)e\u2212j\u03c9n\n\nAs we increase m\n, we slide the **window** function w to the right. For the resulting frame, x(n)w(n\u2212m), we compute the Fourier transform. Therefore, the STFT X is a function of both time, m, and frequency, \u03c9.\n","3364498f":"## Mel-spectrogram [Time frequency representation]\n Using  y_axis=mel plots the y-axis on the [mel scale](https:\/\/en.wikipedia.org\/wiki\/Mel_scale) which is similar to the log(1+f) function:  \n\n                ``m = 2595log(base10)(1+f\/700)``\nThe human perception of sound intensity is logarithmic in nature. Therefore, we are often interested in the log amplitude.","7646ec71":"## Spectrogram [Time frequency representation]\n\nThe [spectrogram](https:\/\/en.wikipedia.org\/wiki\/Spectrogram) shows the the intensity of frequencies over time. A spectrogram is simply the squared magnitude of the STFT:\n\n                    S(m,\u03c9)=|X(m,\u03c9)|2\nThis representation gives time along x-axis, frequency along y-axis and corresponding amplitudes are represented with color. For speech recognition tasks the window used for FFT is 20-30ms as *humans can't utter more than one phoneme in this time frame*.Windows are overlaped at 50% for it. It can vary between 25% to 75% as per usecase.\nIf s = 16Khz and window duraion = 25ms, them samples in window = 16000 * 25 * 0.001 = 400 units.If window overlap is 50% then 400*50% = 200 units is the hope for the window.200 units of frequency bins that represents a B < 16000\/2.","1ef8d444":" **ADSR Model**\n- One characteristic of timbre is its temporal evolution. The envelope of a signal is a smooth curve that approximates the amplitude extremes of a waveform over time. ","a96c18f3":"# Waveform","2e8cac73":"**Spectral Characteristics**\n\nTimbre characterized by the existence of **partials** and their **relative strengths**. Partials are the *dominant frequencies* in a musical tone with the lowest partial being the fundamental frequency.\nThe partials of a sound are visualized with a spectrogram. A [spectrogram](https:\/\/en.wikipedia.org\/wiki\/Spectrogram) shows the intensity of frequency components over time.\n\n[Demo : Spectrogram](https:\/\/mtg.github.io\/essentia.js-examples\/spectral.html)","ae0abba1":"# Mel Frequency Cepstral Coefficients (MFCCs)\n The mel frequency cepstral coefficients ([MFCCs](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum)) of a signal are a small set of features (usually about 10-20) which concisely describe the overall shape of a spectral envelope. ","4786171b":"**Temporal Characteristics**","6803e071":"**Feature scaling**  \n\n  MFCCs can be scaled such that each coefficient dimension has zero mean and unit variance","e56f9a5c":"- Digital computers can only capture audio data in discrete moments in time. The rate at which a computer captures audio data is called the **[sampling frequency](https:\/\/en.wikipedia.org\/wiki\/Sample_rate)** (often abbreviated fs) or **sampling rate** (often abbreviated sr). If B is the [bandwidh](https:\/\/en.wikipedia.org\/wiki\/Bandwidth_(signal_processing)) of signal  and f<sub>s<\/sub> is the sampling frequency then **B < f<sub>s<\/sub> \/ 2** by [Nyquist\u2013Shannon sampling theorem](https:\/\/en.wikipedia.org\/wiki\/Nyquist%E2%80%93Shannon_sampling_theorem)\n\n**Time domain representation**","e0166dd1":"In both fft and stft we *lost the time information*.In the next section we will be looking at a representation that preserves both time,frequency and amplitude of the signal."}}