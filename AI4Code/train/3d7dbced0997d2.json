{"cell_type":{"063494eb":"code","ecfb1434":"code","9a0b928a":"code","51bfda5c":"code","dbe8d630":"code","8a6eec8f":"code","2d31879e":"code","d59e312b":"code","b2bda128":"code","1de27194":"code","df4e81cd":"code","7e47c542":"markdown","6f09cab2":"markdown","509f44d9":"markdown","140d28a3":"markdown","cc28fbcf":"markdown","c5dc8c07":"markdown"},"source":{"063494eb":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport pip._internal as pip\npip.main(['install', '--upgrade', 'numpy==1.17.2'])\nimport numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom tqdm import tqdm_notebook\nimport pickle\n\nfrom lwoku import get_accuracy, add_features","ecfb1434":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable \ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","9a0b928a":"# Add new features\nprint('     Number of features (before adding): {}'.format(len(X_train.columns)))\nX_train = add_features(X_train)\nX_test = add_features(X_test)\nprint('     Number of features (after adding): {}'.format(len(X_train.columns)))","51bfda5c":"print('  -- Drop features')\nprint('    -- Drop columns with few values (or almost)')\nfeatures_to_drop = ['Soil_Type7', 'Soil_Type8', 'Soil_Type15', 'Soil_Type27', 'Soil_Type37']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('    -- Drop columns with low importance')\nfeatures_to_drop = ['Soil_Type5', 'Slope', 'Soil_Type39', 'Soil_Type18', 'Soil_Type20', 'Soil_Type35', 'Soil_Type11',\n                    'Soil_Type31']\nX_train.drop(features_to_drop, axis='columns', inplace=True)\nX_test.drop(features_to_drop, axis='columns', inplace=True)\nprint('     Number of features (after drop): {}'.format(len(X_train.columns)))","dbe8d630":"with open('..\/input\/tactic-03-hyperparameter-optimization-lightgbm\/clf.pickle', 'rb') as fp:\n    clf = pickle.load(fp)\nlg_clf = clf.best_estimator_\nlg_clf","8a6eec8f":"lg_clf_fast = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.9,\n               importance_type='split', learning_rate=0.7, max_depth=21,\n               min_child_samples=5, min_child_weight=1e-15, min_split_gain=0.0,\n               n_estimators=150, n_jobs=-1, num_leaves=30, objective=None,\n               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=987, subsample_freq=0,\n               verbosity=1)\nlg_clf_fast","2d31879e":"%%time\n## Get accuracy for the original data without feature engineering\naccuracy = get_accuracy(lg_clf_fast, X_train, y_train)\nprint('Accuracy without feature engineering: {:.2f}\\u00A0%'.format(accuracy * 100))","d59e312b":"# noinspection PyPep8Naming\ndef accuracy_table_by_reducing():\n    \"\"\"\n    This function calculates the accuracy of the current set of features, and put it in 'Base' column.\n    Then it calculates the accuracy if only one feature is dropped.\n    The feature set with the greatest accuracy, means that a new set of features without this feature, scores more.\n    Then this feature is dropped.\n    Finally, the functions continues with this new set until there are only two values left. \n    :return: A table with the base accuracy and the accuracy of the dropped feature for each iteration\n    \"\"\"\n    table = pd.DataFrame(columns=['Base'])\n    X_features = X_train.copy()\n\n    progress_bar = tqdm_notebook(total=(len(X_features.columns) + 1) * len(X_features.columns) \/ 2)\n    while len(X_features.columns) > 1:\n        print(len(X_features.columns))\n        row = {'Base': get_accuracy(lg_clf_fast, X_features, y_train)}\n        for feature in X_features.columns:\n            row[feature] = get_accuracy(lg_clf_fast, X_features.drop(feature, axis='columns'), y_train)\n        progress_bar.update(len(X_features.columns))\n#         print(row)\n        table = table.append(row, ignore_index=True)\n        feature_to_drop = table.iloc[-1][1:].idxmax()\n        print(feature_to_drop)\n        X_features.drop(feature_to_drop, axis='columns', inplace=True)\n    progress_bar.close()\n    return table\n\n\n# Get the table for reducing\nreducing_table = accuracy_table_by_reducing()","b2bda128":"reducing_table.to_csv('reducing_table.csv', index=False)","1de27194":"# Get the largest accuracy\nreducing_table_accuracy = reducing_table['Base'].max()\nprint('Maximum accuracy: {:.2f}\\u00A0%'.format(reducing_table_accuracy * 100))\n\n# Get the features set with largest accuracy\nreducing_table_best_row = reducing_table.iloc[reducing_table['Base'].idxmax()]\nreducing_table_best_row.dropna(inplace=True)\nreducing_table_best_row.drop('Base', inplace=True)\nreducing_table_features = reducing_table_best_row.axes[0].tolist()\nprint('For this set of features: {}'.format(reducing_table_features))","df4e81cd":"# Predict with test data\nlg_clf.fit(X_train[reducing_table_features], y_train)\ny_test_pred = pd.Series(lg_clf.predict(X_test[reducing_table_features]))\n\n# Submit\noutput = pd.DataFrame({'ID': X_test.index, 'Cover_Type': y_test_pred})\n\noutput.to_csv('submission.csv', index=False)","7e47c542":"# Export","6f09cab2":"# LightGBM\n\nExamine the full hyperparameter optimization details for this model in the following notebook: [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)","509f44d9":"# Get the features set with largest accuracy","140d28a3":"# Prepare data","cc28fbcf":"# Submit","c5dc8c07":"# Introduction\n\nThe aim of this notebook is to select the features that maximizes the score,\nby backward elimination.\n\nThe script will begin with all the features\nand it will discard the feature which its absence contribute more to the score.\n\nFirst the accuracy of the current set of features is calculated.\nThen, for all the features of the set,\nit calculates the accuracy if only this feature is dropped.\nThe feature set with the greatest accuracy,\nmeans that a new set of features without this feature,\nscores more.\nThen this feature is dropped.\n\nFinally, the functions continues with this new set until there are only two values left.\n    \nThe models are fitted and predicted with the optimized parameters and new features of the notebook ([Tactic 06. Feature engineering](https:\/\/www.kaggle.com\/juanmah\/tactic-06-feature-engineering\/)).\n\nThe results are collected at [Tactic 99. Summary](https:\/\/www.kaggle.com\/juanmah\/tactic-99-summary)."}}