{"cell_type":{"8fd68292":"code","2e2d05d7":"code","b8f64df5":"code","e35ffa36":"code","742d6034":"code","918370e6":"code","62204ff5":"code","bedcd4e4":"code","aaa6fa36":"code","58ad79ac":"code","69062cca":"code","ee8fc928":"code","544d030d":"code","f67e0b3c":"code","7f090040":"code","f2b57903":"code","c1d58b01":"code","c0db9f3a":"code","23287df4":"code","32c5c628":"markdown","77f19cce":"markdown","5aad4ab6":"markdown","5c9bfa6b":"markdown","7fa2c6eb":"markdown"},"source":{"8fd68292":"import pandas as pd\nimport numpy as np\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline","2e2d05d7":"train = pd.read_csv('..\/input\/cross-sell-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/cross-sell-prediction\/test.csv')","b8f64df5":"df = pd.merge(train,test,on=[x for x in train.columns if x not in ['Response']],how='outer')\ndf.head()","e35ffa36":"#Check the data distribution in the dataframe\ndf.describe()","742d6034":"sns.set_style('whitegrid')\nsns.boxplot(x=df['Annual_Premium'])","918370e6":"#Set an upper limit of 1,50,000 to the premium paid by the customer\ndf.loc[df['Annual_Premium']>150000,'Annual_Premium'] = 150000","62204ff5":"#Normalising the data of Age, Annual Premium and Vintage columns \ndf['log_age'] = np.log(df['Age'])\ndf['sqrt_premium'] = np.sqrt(df['Annual_Premium'])\ndf['log_vintage'] = np.log(df['Vintage'])","bedcd4e4":"#Calculating mean and std of premium paid and vintage on Previously Insured per Sales Channel used\ngroup = df.groupby(['Policy_Sales_Channel','Previously_Insured'])['Annual_Premium'].agg(['mean','std'])\ngroup.columns = [x + '_channel_insured_premium' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Policy_Sales_Channel','Previously_Insured'],how='left')\n\ngroup = df.groupby(['Policy_Sales_Channel','Previously_Insured'])['Vintage'].agg(['mean','std'])\ngroup.columns = [x + '_channel_insured_vintage' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Policy_Sales_Channel','Previously_Insured'],how='left')","aaa6fa36":"#Calculating mean and std of premium paid and vintage on Previously Insured per Region\ngroup = df.groupby(['Region_Code','Previously_Insured'])['Annual_Premium'].agg(['mean','std'])\ngroup.columns = [x +'_region_insured_premium' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Region_Code','Previously_Insured'],how='left')\n\ngroup = df.groupby(['Region_Code','Previously_Insured'])['Vintage'].agg(['mean','std'])\ngroup.columns = [x +'_region_insured_vintage' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Region_Code','Previously_Insured'],how='left')","58ad79ac":"#Calculating mean and std of premium paid and vintage on Vehicle Damage per Region\ngroup = df.groupby(['Region_Code','Vehicle_Damage'])['Annual_Premium'].agg(['mean','std'])\ngroup.columns = [x +'_region_damage_premium' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Region_Code','Vehicle_Damage'],how='left')\n\ngroup = df.groupby(['Region_Code','Vehicle_Damage'])['Vintage'].agg(['mean','std'])\ngroup.columns = [x +'_region_damage_vintage' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Region_Code','Vehicle_Damage'],how='left')","69062cca":"#Calculating mean and std of premium paid and vintage on the basis of Vehicle Damage\ngroup = df.groupby(['Vehicle_Damage'])['Annual_Premium'].agg(['mean','std'])\ngroup.columns = [x + '_damage_premium' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Vehicle_Damage'],how='left')\n\ngroup = df.groupby(['Vehicle_Damage'])['Vintage'].agg(['mean','std'])\ngroup.columns = [x + '_damage_vintage' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Vehicle_Damage'],how='left')","ee8fc928":"#Calculating mean and std of premium paid and vintage on Vehicle Damage per customer Age\ngroup = df.groupby(['Age','Vehicle_Damage'])['Vintage'].agg(['mean','std'])\ngroup.columns = [x +'_age_damage_vintage' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Age','Vehicle_Damage'],how='left')\n\ngroup = df.groupby(['Age','Vehicle_Damage'])['Annual_Premium'].agg(['mean','std'])\ngroup.columns = [x +'_age_damage_premium' for x in group.columns.ravel()]\ndf = pd.merge(df,group,on=['Age','Vehicle_Damage'],how='left')","544d030d":"#Counting the number of customers for Previously Insured column agewise\ngroup = df.groupby(['Age']).agg(cnt_age_insured = ('Previously_Insured','count'))\ndf = pd.merge(df,group,on=['Age'],how='left')\n\ngroup = df.groupby(['Region_Code']).agg(cnt_region_insured = ('Previously_Insured','count'))\ndf = pd.merge(df,group,on=['Region_Code'],how='left')","f67e0b3c":"le = LabelEncoder()\nfor i in ['Gender','Vehicle_Age','Vehicle_Damage']:\n  df[i] = le.fit_transform(df[i])","7f090040":"#Converting float dtypes column to int for the catboost model to accept it\ndf['Region_Code'] = df['Region_Code'].astype(int)\ndf['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].astype(int)","f2b57903":"X = df[df['Response'].notnull()]\nX_valid = df[df['Response'].isnull()]","c1d58b01":"cat_col = ['Gender','Driving_License','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Policy_Sales_Channel']\n\n#cv_data = cb.Pool(X.drop(columns=['id','Response']),label=X['Response'],cat_features=cat_col)\n\nparams = {'iterations':2000,\n         'learning_rate':0.05,\n         'thread_count':4,\n         'eval_metric':'AUC',\n          'loss_function':'Logloss'}\n\n#cv_res = cb.cv(dtrain=cv_data,early_stopping_rounds=100,nfold=5,params=params,plot=True)","c0db9f3a":"#The cross validation gave an AUC score of 0.8589179 for the model of 384 trees with a learning rate of 0.05 as the best one\ncat_col = ['Gender','Driving_License','Region_Code','Previously_Insured','Vehicle_Age','Vehicle_Damage','Policy_Sales_Channel']\n\ncbc_new = cb.CatBoostClassifier(iterations=384,learning_rate=0.05,thread_count=4,cat_features=cat_col,eval_metric='AUC')\ncbc_new.fit(X.drop(columns=['id','Response']),X['Response'])\n\ny_pred_cbc_new = cbc_new.predict_proba(X_valid.drop(columns=['id','Response']))[:,1]\ntest_res = test['id']\ntest_res = pd.concat([test_res,pd.DataFrame(y_pred_cbc_new,columns=['Response'])],axis=1)\ntest_res.set_index('id',inplace=True)\ntest_res.to_csv('sub_cbc_final.csv')","23287df4":"#Plotting Feature Importance\nscore_dict = {}\nfeature_importances = cbc_new.get_feature_importance()\nfeature_names = X.drop(columns=['id','Response']).columns\n\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n  score_dict.update({name:score})\n\nscore_list = score_dict.items()\nx,y = zip(*score_list)\n\nplt.figure(figsize=(8,12))\nplt.title('Feature Importance')\nplt.barh(x,y)\nplt.xlabel('Feature Score')\nplt.ylabel('Features')","32c5c628":"Here the Annual Premium column seems to contain some outliers as the maximum value is way too high than the mean of the data.<br>So visualising the column values by boxplot.","77f19cce":"## Model Testing and Fitting","5aad4ab6":"This notebook contains code for the Cross Sell Prediction data-hackathon organised by Analytics Vidhya. The dataset can be found [here](https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-cross-sell-prediction\/#ProblemStatement).<br><br>The competition was to predict whether a Health Insurance Policy customer of a company would be interested in the Vehicle Insurance by the same. The dataset consisted of columns describing the customer details like age, gender, region of stay, premium paid, days the customer is linked to the company, etc. There were a large number of categorical variables in the dataset.<br><br>We made use of the CatBoost gradient boosting algorithm here to make the prediction. This algorithm gave better results than XGBoost and LightGBM.<br><br>This model gave us a public AUC score of 0.858872 and a private AUC score of 0.863522. We landed at the 18th position in the leaderboard.","5c9bfa6b":"## Feature Engineering","7fa2c6eb":"Out of the different models fitted, the CatBoost model gave the best auc score for the given dataset."}}