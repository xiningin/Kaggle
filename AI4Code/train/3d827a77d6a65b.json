{"cell_type":{"f24c4626":"code","c633da55":"code","7a937549":"code","6e18ac8b":"code","2d955967":"code","4dca936c":"code","247085bd":"code","85aeed1e":"code","3f31c6d6":"code","46f4b9e2":"code","03b55eb0":"code","c9f2ccd8":"code","02520f57":"code","2b1426f2":"code","142a1d9a":"code","8330ddb9":"code","56b94ac0":"code","dde9669d":"code","52054a9e":"code","1b96b647":"code","6a156a3d":"code","077b4750":"code","5acc21f4":"code","c4dfdf50":"code","04ec5efa":"code","861c99a3":"code","68df4342":"code","2720fc09":"code","583f8e4e":"code","983dc920":"code","ced0ab67":"code","79c5d1a6":"code","919d244c":"code","3d0060ab":"code","50801da2":"code","397382fc":"markdown","50e6bf31":"markdown","4b936082":"markdown","d25888ff":"markdown","fab214a0":"markdown"},"source":{"f24c4626":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c633da55":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators import H2OGradientBoostingEstimator","7a937549":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","6e18ac8b":"\n\n# dimensions\ndf_train.shape\n\n","2d955967":"df_train.info()","4dca936c":"# basic stats\ndf_train.target.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])\n","247085bd":"# histogram of target\ndf_train.target.plot(kind='hist', bins=100)\nplt.title('Target - Histogram')\nplt.grid()\nplt.show()","85aeed1e":"# boxplot of target => looking for outliers\ndf_train.target.plot(kind='box')\nplt.title('Target - Boxplot')\nplt.grid()\nplt.show()\n","3f31c6d6":"df_zero = df_train[df_train.target==0]\ndf_zero\n","46f4b9e2":"# let's remove this one observation\ndf_train = df_train[df_train.target>0]\ndf_train.target.describe()\n","03b55eb0":"features_num = ['cont0', 'cont1', 'cont2', 'cont3', \n                'cont4', 'cont5', 'cont6', 'cont7',\n                'cont8', 'cont9', 'cont10', 'cont11',\n                'cont12', 'cont13']\n\n# plot distribution of numerical features\nfor f in features_num:\n    plt.figure(figsize=(8,4))\n    df_train[f].plot(kind='hist', bins=100)\n    plt.title(f)\n    plt.grid()\n    plt.show()\n\n","c9f2ccd8":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')\n\nfig = plt.figure(figsize = (12,9))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()\n\n","02520f57":"fig = plt.figure(figsize = (12,9))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()\n","2b1426f2":"# example of scatter plot - we pick pair having highest (Pearson) correlation\nsns.jointplot(data=df_train, x='cont5', y='cont12',\n              joint_kws = {'alpha': 0.1})\nplt.show()","142a1d9a":"# different visualization\nsns.jointplot(data=df_train, x='cont5', y='cont12', kind='hex')\nplt.show()\n","8330ddb9":"features_cat = ['cat0', 'cat1', 'cat2', 'cat3',\n                'cat4', 'cat5', 'cat6', 'cat7',\n                'cat8', 'cat9']\n\n# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(8,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()\n\n","56b94ac0":"# scatter plot of target vs each feature + show correlation\nfor f in features_num:\n    c = df_train[f].corr(df_train.target, method='pearson')\n    c = np.round(c,4)\n    plt.figure(figsize=(7,7))\n    plt.scatter(df_train[f], df_train.target, alpha=0.01)\n    plt.title('Target vs ' + f + ' \/ corr = ' + str(c))\n    plt.xlabel(f)\n    plt.ylabel('Target')\n    plt.grid()\n    plt.show()","dde9669d":"for f in features_num:\n    new_var = f + '_bin'\n    df_train[new_var] = pd.cut(df_train[f], bins=10, include_lowest=True)\n    plt.figure(figsize=(7,7))\n    sns.boxplot(data=df_train, x=new_var, y='target')\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()","52054a9e":"\n\nfor f in features_cat:\n    plt.figure(figsize=(10,5))\n    sns.boxplot(data=df_train, x=f, y='target')\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()\n\n","1b96b647":"# Check mean of target as trivial prediction\nm0 = df_train.target.mean()\nprint('Mean of target:', np.round(m0,6))","6a156a3d":"\n\n# Metrics on training data\nfoo = df_train.target - m0 # difference target vs. trivial mean prediction\nfoo = (foo*foo).mean() # mean squared error\nprint('RMSE(train) - Trivial Benchmark: ', np.round(np.sqrt(foo),6))\n\n","077b4750":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","5acc21f4":"\n\n# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores\n\n","c4dfdf50":"# upload training and test data in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n","04ec5efa":"# define Gradient Boosting model\nfit_1 = H2OGradientBoostingEstimator(ntrees = 1000,\n                                     max_depth=9,\n                                     min_rows=1,\n                                     learn_rate=0.01, # default: 0.1\n                                     sample_rate=1,\n                                     col_sample_rate=0.7,\n                                     nfolds=5,\n                                     seed=999)\n\n# train model - this takes some time...\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n\n","861c99a3":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","68df4342":"# show scoring history - training vs cross validations\nfor i in range(5):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [RMSE]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_rmse, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_rmse, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n","2720fc09":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","583f8e4e":"# predict on training data\npred_train = fit_1.predict(train_hex)\ny_train_pred = pred_train.as_data_frame().predict.values # predictions\n\n# and add prediction to original data frame\ndf_train['prediction'] = y_train_pred","983dc920":"\n\n# plot predictions vs actual\np=sns.jointplot(data=df_train, x='target', y='prediction',\n              joint_kws={'alpha' : 0.1})\np.fig.suptitle('Prediction vs Actual - Training Data')\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.show()\n\n","ced0ab67":"# predict on test data\npred_test = fit_1.predict(test_hex)\ny_test_pred = pred_test.as_data_frame().predict.values # predictions\n\n# and plot distribution of predictions\nplt.hist(y_test_pred, bins=100)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()\n","79c5d1a6":"plt.hist(y_train_pred, bins=100)\nplt.title('Predictions on Training Data')\nplt.grid()\nplt.show()\n","919d244c":"# prepare submission\ndf_sub.target = y_test_pred\ndf_sub.head(10)","3d0060ab":"# stats\ndf_sub.target.describe()\n","50801da2":"# save to file for submission\ndf_sub.to_csv('submission.csv', index=False)\n","397382fc":"# Model Building","50e6bf31":"## Target Value","4b936082":"# Correlation","d25888ff":"# Scatter Plot Vs Feature","fab214a0":"# Categorical Feature"}}