{"cell_type":{"73bf10d1":"code","39246035":"code","03fab42c":"code","49c42bbe":"code","3314e6ae":"code","11e61b1d":"code","2b73ef66":"code","c3b56435":"code","67a409b2":"code","66e7096d":"code","f12c9097":"code","4a6cb9a2":"code","537dd120":"code","db96313d":"code","dfac6126":"code","a577acc9":"code","e353b145":"code","a066bf39":"code","67559f39":"code","cef58936":"code","eef8fd4d":"code","e66c5b50":"code","c242a2fc":"code","5024f9a2":"markdown","068d4986":"markdown","db0a35b4":"markdown","14f81886":"markdown","bf875c3c":"markdown","fc9ba8eb":"markdown","f1c98bf7":"markdown","b254c3af":"markdown","781978ef":"markdown","afa1075d":"markdown","a793e1ac":"markdown","e6c3817b":"markdown","dd2079da":"markdown","0822f128":"markdown","e9b30d4b":"markdown","1b5b05f5":"markdown","c55927e0":"markdown","979a7c05":"markdown","088e7cb5":"markdown","3504d06f":"markdown","3c9df025":"markdown"},"source":{"73bf10d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\n\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","39246035":"\ndf_train = pd.read_csv(dirname+'\/train.csv')\ndf_train['fn'] = df_train.index\ndf_train.head()","03fab42c":"df_train['label'].hist()","49c42bbe":"def valid_split(df, valid_ratio = 0.1):\n    valid_idx = []\n    for i in df['label'].unique():\n        valid_idx+=list(df[df['label']==i].sample(frac=valid_ratio, random_state=2020)['fn'].values)\n    return valid_idx\n\nvalid_idx = valid_split(df_train, valid_ratio = 0.2)","3314e6ae":"class PixelImageItemList(ImageList):\n    \n    def __init__(self, myimages = {}, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.myimages = myimages \n    \n    def open(self,fn):\n        return self.myimages.get(fn)\n    \n    @classmethod\n    def from_df(cls, df:DataFrame, path:PathOrStr, cols:IntsOrStrs=0, folder:PathOrStr=None, suffix:str='', **kwargs)->'ItemList':\n        \"Get the filenames in `cols` of `df` with `folder` in front of them, `suffix` at the end.\"\n        res = super().from_df(df, path=path, cols=cols, **kwargs)\n        \n        # FULL LOAD of all images\n        for i, row in df.drop(labels=['label','fn'],axis=1).iterrows():\n            # Numpy to Image conversion from\n            # https:\/\/www.kaggle.com\/heye0507\/fastai-1-0-with-customized-itemlist\n            img_pixel = row.values.reshape(28,28)\n            img_pixel = np.stack((img_pixel,)*1,axis=-1)\n            ## mark for convolution test\n            img_pixel[1,1]=255\n            \n            res.myimages[res.items[i]]=vision.Image(pil2tensor(img_pixel,np.float32).div_(255))\n\n        return res","11e61b1d":"%%time\ndata = (PixelImageItemList.from_df(df=df_train,path='.',cols='fn')\n        .split_by_idx(valid_idx=valid_idx) #.split_by_rand_pct()\n        .label_from_df(cols='label')\n        .databunch(bs=128))\ndata.dataset[0]","2b73ef66":"data.show_batch(rows=3, figsize=(5,5), cmap='gray')","c3b56435":"#https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\/layers.py#L111\ndef myconv_layer(ni:int, nf:int, ks:int=3, stride:int=1):\n    layers = [init_default(nn.Conv2d(ni,nf,stride=stride,kernel_size=ks,padding=1, bias=False),nn.init.kaiming_normal_)]\n    \n    # make sure the ReLU doesn't override the conv2d activations, so we keep more details for later\n    layers.append(nn.ReLU(inplace=False))\n    layers.append(nn.BatchNorm2d(nf))\n    \n    return nn.Sequential(*layers)","67a409b2":"# Adapted from https:\/\/www.kaggle.com\/melissarajaram\/fastai-pytorch-with-best-original-mnist-arch\n\nleak = 0.15\nmodel = nn.Sequential(\n    \n    myconv_layer(1,32),\n    conv_layer(32,32,stride=1,ks=3,leaky=leak),\n    conv_layer(32,32,stride=2,ks=5,leaky=leak),\n    nn.Dropout(0.4),\n    \n    conv_layer(32,64,stride=1,ks=3,leaky=leak),\n    conv_layer(64,64,stride=1,ks=3,leaky=leak),\n    conv_layer(64,64,stride=2,ks=5,leaky=leak),\n    nn.Dropout(0.4),\n    \n    Flatten(),\n    nn.Linear(3136, 128),\n    relu(inplace=True),\n    nn.BatchNorm1d(128),\n    nn.Dropout(0.4),\n    nn.Linear(128,10)\n)","66e7096d":"learn = Learner(data, model, loss_func = nn.CrossEntropyLoss() , metrics=[accuracy])","f12c9097":"learn.lr_find()\nlearn.recorder.plot()","4a6cb9a2":"learn.fit_one_cycle(3, 0.5e-3)","537dd120":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_top_losses(12, figsize=(7,6))\ninterp.plot_confusion_matrix()","db96313d":"conv_kernels=list(learn.model.parameters())[0] # alternativly: learn.model[0][0].weight.data\n# convert to numpy\nconv_kernels=conv_kernels.cpu().detach().numpy()\n# example:\nprint('Shape kernels first con layer:', conv_kernels.shape)\nprint('First kernel:\\n', conv_kernels[0,0,:,:])","dfac6126":"for i in range(conv_kernels.shape[0]):\n    ax = plt.subplot(conv_kernels.shape[0]\/8, 8, i+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    conv_kernel = conv_kernels[i,0,:,:]\n    plt.imshow(conv_kernels[i,0,:,:], cmap='gray')\n        \nplt.show()","a577acc9":"learn.model[0:2]","e353b145":"example_img = 0\n\n#\nm = learn.model.eval()\nx,y = data.train_ds[example_img]\nxb,_ = data.one_item(x)\nxb = xb.cuda()\n\ndata.train_ds.get(example_img)","a066bf39":"mblock = 0 # first Sequential\ninv_layer = m[mblock][0] # conv2d layer in first block\n\ndef hooked():\n    with hook_output(inv_layer) as hook_forward:\n        preds = m(xb)\n    return hook_forward\n\nacts = hooked().stored[0].cpu()\n\ninv_layer","67559f39":"acts[0:5][0,:5,:5]","cef58936":"conv_kernels[0,0,:,:]","eef8fd4d":"\ndef img_activations(m:nn.Module, img_id:Image, data = data, ds=data.train_ds):\n    # create batch with one image\n    xb,_ = data.one_item(x)\n    xb = xb.cuda()\n    \n    # flatten to get activations of children\n    with hook_outputs(flatten_model(m)) as hook_forward:\n        preds=m.eval()(xb)\n    \n    return [i.cpu() for i in hook_forward.stored[:]]\n\nm = learn.model.eval()\nx,_ = data.train_ds[0]\nacts = img_activations(m, x,data)\n    ","e66c5b50":"# apply ReLU to first layer and compare to second layer\ntorch.max(torch.zeros(3,3),acts[0][0,0,:3,:3]) == acts[1][0,0,:3,:3]","c242a2fc":"import warnings\nwarnings.filterwarnings('ignore')\n\ndef scale_color(im,mn=None,mx=None):\n    if mn == None:\n        mn = im.min()\n    if mx == None:\n        mx = im.max()\n    return (im-mn)\/(mx-mn)\n\nfig = plt.figure(figsize=(12, 6))\nims=[]\nfor i in range(32):#acts[0]):\n    \n    krnl = scale_color(np.copy(conv_kernels[i,0,:,:]))\n    org_im = x.data.numpy()[0,:,:] \n    layer_1_im = acts[0][0,i,:,:].numpy()\n    layer_2_im = acts[1][0,i,:,:].numpy()\n    \n    org_im[:3,:3] = krnl\n    layer_1_im = scale_color(layer_1_im) \n    layer_1_im[:3,:3] = krnl #[:3,:3] \n    layer_2_im = scale_color(layer_2_im) \n    layer_2_im[:3,:3] = krnl #[:3,:3] \n    \n    #layer_1_im=np.concatenate((org_im,layer_1_im,layer_2_im), axis=1)\n    \n    ax1 = plt.subplot(131, frameon=False)\n    ax1.set_title('(0) Input')\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n    im1 = plt.imshow(org_im,animated=True, cmap='gray')\n    \n    ax2 = plt.subplot(132, frameon=False)\n    ax2.set_title('(1) Conv2d(1,32)')\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    im2 = plt.imshow(layer_1_im,animated=True, cmap='gray')\n    \n    ax3 = plt.subplot(133, frameon=False)\n    ax3.set_title('(2) ReLU(32)')\n    ax3.set_xticks([])\n    ax3.set_yticks([])\n    im3 = plt.imshow(layer_2_im,animated=True, cmap='gray')\n      \n    ims.append([im1,im2, im3]) #im1,\n\nani = animation.ArtistAnimation(fig, ims, interval=500, blit=False, repeat_delay=1000)\n\nfrom IPython.display import HTML\nHTML(ani.to_jshtml())  \n\n","5024f9a2":"We build a customized conv-layer since the one from fastai uses the ReLU with the parameter inplace = True. Which overrides the activations of the preciding conv layer. We set leaky to 0 so we can easily see the changes of the activations from conv2d-Layer to ReLU-layer.","068d4986":"# About\nWhen I'm coding, I'm led by intuition.\n\nSometimes it\u2019s good to dig a bit into the libraries to get a better feeling about what\u2019s going on inside.\n\nEven though there are endless articles, videos or books I won\u2019t get a good intuition unless I investigate a bit by myself.\n\nIn this kernel I simply wan\u2019t to see how an input Image changes through kernel convolution inside a neural net. As said it\u2019s done many times before but the notebook might be a source for somebody trying similar things on this dataset or with fastai. \n\nAnd as expected facing all traps and hurdles which come up along the path enhance the understanding of the framework additionaly.","db0a35b4":"Let's see the kernels of layer one (be aware that the values are internaly scaled to gray scale):","14f81886":"Check label distribution:","bf875c3c":"We do a check on the function if expected layers were hooked.\nTherefore we compare activation of debug area (left upper corner) for first layer (Conv2d(1,1)) and second layer (ReLU(leaky=0)):","fc9ba8eb":"More to be done ...","f1c98bf7":"The activations are the outputs of a layer when an image is passed through the net. The activations can by grabed with hooks in fastai as done [here](https:\/\/github.com\/fastai\/fastai\/blob\/master\/tests\/test_callbacks_hooks.py#L74).\n\nLets check the first few layers of the model:","b254c3af":"Looks good!","781978ef":"## Activations","afa1075d":"Apply the hook to get the activation of the first layer (conv2d):","a793e1ac":"Next we build a function to get all activations:","e6c3817b":"with the first kernel","dd2079da":"# Look inside the model","0822f128":"Now lets compare the **activation** of the debug pixel [[0,0,0,..],[0,1,0,..],[0,0,0,..]] in the left corner, ","e9b30d4b":"Finaly we print the activations of all 32 kernels of the first (conv2d) and second (Relu) layer next to the Input image. See how the Relu cuts off some (the negative) activations. Be aware that the activations are scaled internaly to fit gray scale.","1b5b05f5":"Load the entire dataset in memory to speedup training (see https:\/\/www.kaggle.com\/joatom\/kannada-mnist-speed-up-fastai-image-processing). Load one channel only (gray scale).\n\nSince we want to investigat in convolution we add a pixel (value 255, later scaled to 1) one pix apart from the let top corner (1,1) for debug purpose. So we can later easily check if a 3x3 kernel convolves as expexted.","c55927e0":"## Kernels","979a7c05":"Unique distribution. Build an uniquely distributed validation set:","088e7cb5":"Now get an image example for the test:","3504d06f":"There are several ways to get the kernels. Here are the 32 kernels of the first convolution.","3c9df025":"# Setup Data and Model"}}