{"cell_type":{"27eb30d6":"code","957c374f":"code","6f649386":"code","837bd348":"code","2f18d369":"code","f3088226":"code","179c76ed":"code","335e12d4":"code","055b3380":"code","f0663777":"code","a910288c":"code","fa7772be":"code","3ea6fd4c":"code","88225c4e":"code","164d23fb":"code","6cff12a0":"code","7a91e720":"code","a8458bc6":"code","a1a56a46":"code","e0e28407":"code","1fdcea0e":"code","6fab3bca":"code","e662374b":"code","2a2d314d":"code","81626e1f":"code","9a0a0675":"code","70205fab":"code","27336191":"code","63dff1fc":"code","cef447f3":"code","7cb58013":"code","0eeecfbd":"code","34b008f3":"code","e8969ab1":"code","6805b38b":"code","239d095d":"code","7283e2ea":"markdown","e0b962e1":"markdown","180c9aa3":"markdown","c9f6ef3f":"markdown","60263a68":"markdown","79aa6dba":"markdown","e379d91c":"markdown","725ef3c0":"markdown","61028334":"markdown","4c02ccf0":"markdown","5a05858d":"markdown","b410b624":"markdown","b4892f41":"markdown","21407fa3":"markdown","75bd40d0":"markdown","4e3270ed":"markdown","cf26fc83":"markdown","23be7f85":"markdown","f92e1ceb":"markdown"},"source":{"27eb30d6":"import tensorflow as tf\nfrom tensorflow.keras import utils \nfrom tensorflow.keras.datasets import mnist \nimport seaborn as sns\nfrom tensorflow.keras.initializers import RandomNormal","957c374f":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","6f649386":"(X_train, y_train), (X_test, y_test) = mnist.load_data()","837bd348":"print(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d, %d)\"%(X_train.shape[1], X_train.shape[2]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d, %d)\"%(X_test.shape[1], X_test.shape[2]))","2f18d369":"X_train = X_train.reshape(X_train.shape[0], X_train.shape[1]*X_train.shape[2]) \nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1]*X_test.shape[2]) ","f3088226":"# after converting the input images from 3d to 2d vectors\n\nprint(\"Number of training examples :\", X_train.shape[0], \"and each image is of shape (%d)\"%(X_train.shape[1]))\nprint(\"Number of training examples :\", X_test.shape[0], \"and each image is of shape (%d)\"%(X_test.shape[1]))","179c76ed":"# An example data point\nprint(X_train[0])","335e12d4":"X_train = X_train\/255\nX_test = X_test\/255","055b3380":"# example data point after normlizing\nprint(X_train[0])","f0663777":"# here we are having a class number for each image\nprint(\"Class label of first image :\", y_train[0])\n\n# lets convert this into a 10 dimensional vector\n# ex: consider an image is 5 convert it into 5 => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n# this conversion needed for MLPs \n\nY_train = utils.to_categorical(y_train, 10) \nY_test = utils.to_categorical(y_test, 10)\n\nprint(\"After converting the output into a vector : \",Y_train[0])","a910288c":"\nfrom tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Dense, Activation \n","fa7772be":"# some model parameters\n\noutput_dim = 10\ninput_dim = X_train.shape[1]\n\nbatch_size = 128 \nnb_epoch = 20","3ea6fd4c":"# start building a model\nmodel = Sequential()\n\nmodel.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))","88225c4e":"model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test)) \n","164d23fb":"score = model.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)\n","6cff12a0":"# Multilayer perceptron\n\nmodel_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()","7a91e720":"model_sigmoid.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","a8458bc6":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","a1a56a46":"model_sigmoid = Sequential()\nmodel_sigmoid.add(Dense(512, activation='sigmoid', input_shape=(input_dim,)))\nmodel_sigmoid.add(Dense(128, activation='sigmoid'))\nmodel_sigmoid.add(Dense(output_dim, activation='softmax'))\n\nmodel_sigmoid.summary()\n\nmodel_sigmoid.compile(optimizer='adam', \n                      loss='categorical_crossentropy', \n                      metrics=['accuracy'])\n\nhistory = model_sigmoid.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","e0e28407":"score = model_sigmoid.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","1fdcea0e":"# https:\/\/arxiv.org\/pdf\/1707.09725.pdf#page=95\n\nmodel_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nmodel_relu.summary()","6fab3bca":"model_relu.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","e662374b":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","2a2d314d":"model_relu = Sequential()\nmodel_relu.add(Dense(512, activation='relu', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\nmodel_relu.add(Dense(128, activation='relu', kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\nmodel_relu.add(Dense(output_dim, activation='softmax'))\n\nprint(model_relu.summary())\n\nmodel_relu.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_relu.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","81626e1f":"score = model_relu.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","9a0a0675":"from tensorflow.keras.layers import BatchNormalization\n\nmodel_batch = Sequential()\n\nmodel_batch.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_batch.add(BatchNormalization())\n\nmodel_batch.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_batch.summary()","70205fab":"model_batch.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_batch.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","27336191":"score = model_batch.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","63dff1fc":"# https:\/\/stackoverflow.com\/questions\/34716454\/where-do-i-call-the-batchnormalization-function-in-keras\n\nfrom tensorflow.keras.layers import Dropout\n\nmodel_drop = Sequential()\n\nmodel_drop.add(Dense(512, activation='sigmoid', input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.039, seed=None)))\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(128, activation='sigmoid', kernel_initializer=RandomNormal(mean=0.0, stddev=0.55, seed=None)) )\nmodel_drop.add(BatchNormalization())\nmodel_drop.add(Dropout(0.5))\n\nmodel_drop.add(Dense(output_dim, activation='softmax'))\n\n\nmodel_drop.summary()","cef447f3":"model_drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))","7cb58013":"score = model_drop.evaluate(X_test, Y_test, verbose=0) \nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","0eeecfbd":"from tensorflow.keras.optimizers import Adam,RMSprop,SGD\ndef best_hyperparameters(activ):\n\n    model = Sequential()\n    model.add(Dense(512, activation=activ, input_shape=(input_dim,), kernel_initializer=RandomNormal(mean=0.0, stddev=0.062, seed=None)))\n    model.add(Dense(128, activation=activ, kernel_initializer=RandomNormal(mean=0.0, stddev=0.125, seed=None)) )\n    model.add(Dense(output_dim, activation='softmax'))\n\n\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    \n    return model","34b008f3":"# https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\n\nactiv = ['sigmoid','relu']\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = KerasClassifier(build_fn=best_hyperparameters, epochs=nb_epoch, batch_size=batch_size, verbose=0)\nparam_grid = dict(activ=activ)\n\n# if you are using CPU\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n# if you are using GPU dont use the n_jobs parameter\n\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs = -1)\ngrid_result = grid.fit(X_train, Y_train)","e8969ab1":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","6805b38b":"import pandas as pd \ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntest = test\/255","239d095d":"predictions = model_sigmoid.predict_classes(test)\nresult = pd.DataFrame({\n    'ImageId' : range(1,len(predictions)+1), \n    'Label' : predictions\n})\nresult.to_csv('result.csv', index = False, encoding = 'utf-8')","7283e2ea":"<h2> A simple model with just a Softmax classifier  <\/h2>","e0b962e1":"- the data, shuffled and split between train and test sets ","180c9aa3":"<h2>MLP + Sigmoid activation + ADAM <\/h2>","c9f6ef3f":"<h2> MLP + ReLU +SGD <\/h2>","60263a68":"- we will get val_loss and val_acc only when you pass the paramter validation_data\n- val_loss : validation loss\n- val_acc : validation accuracy","79aa6dba":"- for relu layers If we sample weights from a normal distribution N(0,\u03c3) we satisfy this condition with \u03c3=\u221a(2\/(ni). \n```\nh1 =>  \u03c3=\u221a(2\/(fan_in) = 0.062  => N(0,\u03c3) = N(0,0.062)\nh2 =>  \u03c3=\u221a(2\/(fan_in) = 0.125  => N(0,\u03c3) = N(0,0.125)\nout =>  \u03c3=\u221a(2\/(fan_in+1) = 0.120  => N(0,\u03c3) = N(0,0.120)\n```","e379d91c":" <h3>  MLP with  Sigmoid activation and SGDOptimizer <\/h3>","725ef3c0":"<h1> Hyper-parameter tuning of Keras models using Sklearn <\/h1>","61028334":"* if we observe the above matrix each cell is having a value between 0-255\n* before we move to apply machine learning algorithms lets try to normalize the data\n```\nX => (X - Xmin)\/(Xmax-Xmin) = X\/255\n```","4c02ccf0":"- Before training a model, you need to configure the learning process, which is done with the compile method\n\nIt receives three arguments: \n1. An optimizer. This could be the string identifier of an existing optimizer \n1. A loss function. This is the objective that the model will try to minimize.\n1. A list of metrics. For any classification problem you will want to set this to metrics=['accuracy'].\n\n* Note: when using the categorical_crossentropy loss, your targets should be in categorical format \n(e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros except \nfor a 1 at the index corresponding to the class of the sample).\nthat is why we converted out labels into vectors","5a05858d":"1. if you observe the input shape its 2 dimensional vector\n1. for each image we have a (28*28) vector\n1. we will convert the (28*28) vector into single dimensional vector of 1 * 784 ","b410b624":"The model needs to know what input shape it should expect. \nFor this reason, the first layer in a Sequential model \n(and only the first, because following layers can do automatic shape inference)\nneeds to receive information about its input shape. \nyou can use input_shape and input_dim to pass the shape of input\n\noutput_dim represent the number of nodes need in that layer\nhere we have 10 nodes","b4892f41":"<h2> 5. MLP + Dropout + AdamOptimizer <\/h2>","21407fa3":"<h2> MLP + ReLU + ADAM <\/h2>","75bd40d0":"##### There are a lot of hyperparamater tuning when it comes to Keras such as :\n- Number of layers \n- number of Activation Units in Each Layer\n- Type of Activation Function units\n- Dropout Rate\n- The list goes on as we delve deeper into Keras \n","4e3270ed":"- Keras models are trained on Numpy arrays of input data and labels. \n- For training a model, you will typically use the  fit function\n\n```python\nfit(self, x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, \nvalidation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, \nvalidation_steps=None)\n```\n\n- fit() function Trains the model for a fixed number of epochs (iterations on a dataset).\n\n- it returns A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n\n","cf26fc83":"##### Importing the necessary dependancies:","23be7f85":"- The Sequential model is a linear stack of layers.\n- you can create a Sequential model by passing a list of layer instances to the constructor:\n\n```python\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])\n```\n- You can also simply add layers via the .add() method:\n\n```python\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n\ntensorflow.keras.layers.Dense(units, \n                   activation=None,use_bias=True, \n                   kernel_initializer='glorot_uniform',bias_initializer='zeros', \n                   kernel_regularizer=None, bias_regularizer=None, \n                   activity_regularizer=None, kernel_constraint=None, bias_constraint=None)```\n\n- Dense implements the operation: \n\n```python \noutput = activation(dot(input, kernel) + bias)``` \n\n- where activation is the element-wise activation function passed as the activation argument, \n- kernel is a weights matrix created by the layer, and \n- bias is a bias vector created by the layer (only applicable if use_bias is True).\n\n```\noutput = activation(dot(input, kernel) + bias)  => y = activation(WT. X + b)\n```\n\n- Activations can either be used through an Activation layer, or through the activation argument supported by all forward layers\n\n```python\nfrom tensorflow.keras.layers import Activation, Dense\n\nmodel.add(Dense(64))\nmodel.add(Activation('tanh'))\n\nThis is equivalent to:\nmodel.add(Dense(64, activation='tanh'))\n```\n- there are many activation functions ar available ex: tanh, relu, softmax\n\n","f92e1ceb":"<h2> MLP with Batch-Normalization on hidden Layers + AdamOptimizer <\/2>"}}