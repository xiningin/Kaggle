{"cell_type":{"67c8ea44":"code","33f39ef4":"code","e348cb6d":"code","73b38b77":"code","9a430d60":"code","958416ad":"code","5bd39983":"code","3cca36a3":"code","dcef95d1":"code","5e1e1f85":"code","7ba50e89":"code","4c8c543f":"code","48887a06":"code","af7d0730":"code","7233121f":"code","1303b3a0":"code","ff5f3786":"code","45cac2e7":"code","c595283b":"code","4531f2e5":"code","d30ddb63":"code","440558a2":"code","e0a3dacf":"code","19fd108f":"code","bdd75dce":"code","5c042f5d":"code","a2382e3d":"code","723dbea7":"code","2fbc0022":"code","8ca8807c":"code","86962faf":"code","8ee91c75":"code","ca840c78":"code","d5a63c12":"code","ef269f8c":"code","cae8a4f2":"code","70d2a8bb":"code","06d502ef":"code","d9e835c5":"code","5a2c5fb6":"code","08cff8a5":"code","2e15a55e":"code","b35dd7c6":"code","fe8710b4":"code","03bcffde":"code","f80e8289":"code","7a8f6fa7":"code","00d611c7":"code","c20686f8":"code","2c68e810":"code","e13a31d9":"code","c659b5ca":"code","756c8966":"code","b1ef5c1d":"code","7e98a28a":"code","fc2cbc78":"code","77b49982":"markdown","f8aeac0e":"markdown","e7c6f319":"markdown","b1dde842":"markdown","bfece8d2":"markdown","bf00b47e":"markdown","d844ec76":"markdown","880841cb":"markdown","c5e2b9d7":"markdown","090bcb52":"markdown","a54fb7a7":"markdown","5d15c947":"markdown","a5ad8e3d":"markdown","0d1ed5b7":"markdown","8578187f":"markdown","d68e78b7":"markdown","50b4df4a":"markdown","7e4ecf7f":"markdown"},"source":{"67c8ea44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split,GridSearchCV\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nlabel=LabelEncoder()\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","33f39ef4":"#path of file\ntrain_file_path = '..\/input\/train.csv'\ntest_file_path = '..\/input\/test.csv'\n#train data frame\ntrain = pd.read_csv(train_file_path)\ntest = pd.read_csv(test_file_path)\n#As test has only one missing value so lets fill it..\ntest.Fare.fillna(test.Fare.mean(), inplace=True)\ndata_df = train.append(test) # The entire data: train + test.\npassenger_id=test['PassengerId']\n\n## We will drop PassengerID and Ticket since it will be useless for our data. \ntrain.drop(['PassengerId'], axis=1, inplace=True)\ntest.drop(['PassengerId'], axis=1, inplace=True)\ntest.shape\nprint(\"Successfully file import\")","e348cb6d":"print (train.isnull().sum())\nprint (''.center(20, \"*\"))\nprint (test.isnull().sum())\nsns.boxplot(x='Survived',y='Fare',data=train)","73b38b77":"train=train[train['Fare']<400]","9a430d60":"train['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)","958416ad":"pd.options.display.max_columns = 99\ntest['Fare'].fillna(test['Fare'].mean(),inplace=True)\ntrain.head()","5bd39983":"for name_string in data_df['Name']:\n    data_df['Title']=data_df['Name'].str.extract('([A-Za-z]+)\\.',expand=True)\n    \n    \nprint(data_df['Title'].value_counts())\n#replacing the rare title with more common one.\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\"\"\"\ndf.replace({'A': 0, 'B': 5}, 100)\n     A    B  C\n0  100  100  a\n1    1    6  b\n2    2    7  c\n3    3    8  d\n4    4    9  e\n\"\"\"\n\ndata_df.replace({'Title': mapping}, inplace=True)\n\ndata_df['Title'].value_counts()\ntrain['Title']=data_df['Title'][:891]\ntest['Title']=data_df['Title'][891:]\n\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\nfor title in titles:\n    age_to_impute = data_df.groupby('Title')['Age'].median()[titles.index(title)]\n    #print(age_to_impute)\n    data_df.loc[(data_df['Age'].isnull()) & (data_df['Title'] == title), 'Age'] = age_to_impute\ndata_df.isnull().sum()\n\n\n\ntrain['Age']=data_df['Age'][:891]\ntest['Age']=data_df['Age'][891:]\ntest.isnull().sum()\n","3cca36a3":"train.describe()","dcef95d1":"train.groupby('Survived').mean()","5e1e1f85":"train.groupby('Sex').mean()","7ba50e89":"train.corr()","4c8c543f":"plt.subplots(figsize = (15,8))\nsns.heatmap(train.corr(), annot=True,cmap=\"PiYG\")\nplt.title(\"Correlations Among Features\", fontsize = 20)","48887a06":"plt.subplots(figsize = (15,8))\nsns.barplot(x = \"Sex\", y = \"Survived\", data=train, edgecolor=(0,0,0), linewidth=2)\nplt.title(\"Survived\/Non-Survived Passenger Gender Distribution\", fontsize = 25)\nlabels = ['Female', 'Male']\nplt.ylabel(\"% of passenger survived\", fontsize = 15)\nplt.xlabel(\"Gender\",fontsize = 15)\nplt.xticks(sorted(train.Sex.unique()), labels)\n\n# 1 is for male and 0 is for female.","af7d0730":"sns.set(style='darkgrid')\nplt.subplots(figsize = (15,8))\nax=sns.countplot(x='Sex',data=train,hue='Survived',edgecolor=(0,0,0),linewidth=2)\ntrain.shape\n## Fixing title, xlabel and ylabel\nplt.title('Passenger distribution of survived vs not-survived',fontsize=25)\nplt.xlabel('Gender',fontsize=15)\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\nlabels = ['Female', 'Male']\n#Fixing xticks.\nplt.xticks(sorted(train.Survived.unique()),labels)\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title('Survived')\nlegs=leg.texts\nlegs[0].set_text('No')\nlegs[1].set_text('Yes')\n","7233121f":"sns.set(style='darkgrid')\nplt.subplots(figsize = (8,8))\nax=sns.countplot(x='Pclass',data=train,hue='Survived',edgecolor=(0,0,0),linewidth=2)\ntrain.shape\n## Fixing title, xlabel and ylabel\nplt.title('Pclass distribution of survived vs not-survived',fontsize=25)\nplt.xlabel('Pclass',fontsize=15)\nplt.ylabel(\"Count\", fontsize = 15)\n\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title('Survived')\nlegs=leg.texts\nlegs[0].set_text('No')\nlegs[1].set_text('Yes')","1303b3a0":"plt.subplots(figsize=(10,8))\nsns.kdeplot(train.loc[(train['Survived'] == 0),'Pclass'],shade=True,color='r',label='Not Survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'],shade=True,color='b',label='Survived' )\n\nlabels = ['First', 'Second', 'Third']\nplt.xticks(sorted(train.Pclass.unique()),labels)","ff5f3786":"plt.subplots(figsize=(15,10))\n\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'],color='r',shade=True,label='Not Survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'],color='b',shade=True,label='Survived' )\nplt.title('Fare Distribution Survived vs Non Survived',fontsize=25)\nplt.ylabel('Frequency of Passenger Survived',fontsize=20)\nplt.xlabel('Fare',fontsize=20)","45cac2e7":"#fig,axs=plt.subplots(nrows=2)\nfig,axs=plt.subplots(figsize=(10,8))\nsns.set_style(style='darkgrid')\nsns.kdeplot(train.loc[(train['Survived']==0),'Age'],color='r',shade=True,label='Not Survived')\nsns.kdeplot(train.loc[(train['Survived']==1),'Age'],color='b',shade=True,label='Survived')","c595283b":"## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1","4531f2e5":"def family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)\n","d30ddb63":"train['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]","440558a2":"## We are going to create a new feature \"age\" from the Age feature. \ntrain['child'] = [1 if i<16 else 0 for i in train.Age]\ntest['child'] = [1 if i<16 else 0 for i in test.Age]\ntrain.child.value_counts()","e0a3dacf":"train.head()","19fd108f":"train['calculated_fare'] = train.Fare\/train.family_size\ntest['calculated_fare'] = test.Fare\/test.family_size","bdd75dce":"train.calculated_fare.mean()","5c042f5d":"train.calculated_fare.mode()","a2382e3d":"def fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)","723dbea7":"train = pd.get_dummies(train, columns=['Title',\"Pclass\",'Embarked', 'family_group', 'fare_group'], drop_first=True)\ntest = pd.get_dummies(test, columns=['Title',\"Pclass\",'Embarked', 'family_group', 'fare_group'], drop_first=True)\ntrain.drop(['Cabin', 'family_size','Ticket','Name', 'Fare'], axis=1, inplace=True)\ntest.drop(['Ticket','Name','family_size',\"Fare\",'Cabin'], axis=1, inplace=True)","2fbc0022":"pd.options.display.max_columns = 99","8ca8807c":"def age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a","86962faf":"train['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)","8ee91c75":"train = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True)\n#Lets try all after dropping few of the column.\ntrain.drop(['Age','calculated_fare'],axis=1,inplace=True)\ntest.drop(['Age','calculated_fare'],axis=1,inplace=True)","ca840c78":"train.head()","d5a63c12":"#age=pd.cut(data_df['Age'],4)\n#data_df['Age2']=label.fit_transform(age)\n#fare=pd.cut(data_df['Fare'],4)\n#data_df['Fare2']=label.fit_transform(fare)\n#train['Age']=data_df['Age2'][:891]\n#train['Fare']=data_df['Fare2'][:891]\n#test['Age']=data_df['Age2'][891:]\n#test['Fare']=data_df['Fare2'][891:]\n#train = pd.get_dummies(train,columns=['Age','Fare'], drop_first=True)\n#test_df = pd.get_dummies(test,columns=['Age','Fare'], drop_first=True)\n#print(test.shape)\n#print(train.shape)\n\ntrain.drop(['Title_Rev','age_group_old','age_group_teenager','age_group_senior_citizen','Embarked_Q'],axis=1,inplace=True)\ntest.drop(['Title_Rev','age_group_old','age_group_teenager','age_group_senior_citizen','Embarked_Q'],axis=1,inplace=True)\n","ef269f8c":"X = train.drop('Survived', 1)\ny = train['Survived']","cae8a4f2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit,train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    svm.SVC(probability=True),\n    DecisionTreeClassifier(),\n    CatBoostClassifier(),\n    XGBClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n    \n\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog= pd.DataFrame(columns=log_cols)","70d2a8bb":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit\n\nSSplit=StratifiedShuffleSplit(test_size=0.3,random_state=7)\nacc_dict = {}\n\nfor train_index,test_index in SSplit.split(X,y):\n    X_train,X_test=X.iloc[train_index],X.iloc[test_index]\n    y_train,y_test=y.iloc[train_index],y.iloc[test_index]\n    \n    for clf in classifiers:\n        name = clf.__class__.__name__\n          \n        clf.fit(X_train,y_train)\n        predict=clf.predict(X_test)\n        acc=accuracy_score(y_test,predict)\n        if name in acc_dict:\n            acc_dict[name]+=acc\n        else:\n            acc_dict[name]=acc","06d502ef":"log['Classifier']=acc_dict.keys()\nlog['Accuracy']=acc_dict.values()\n#log.set_index([[0,1,2,3,4,5,6,7,8,9]])\n%matplotlib inline\nsns.set_color_codes(\"muted\")\nax=plt.subplots(figsize=(10,8))\nax=sns.barplot(y='Classifier',x='Accuracy',data=log,color='b')\nax.set_xlabel('Accuracy',fontsize=20)\nplt.ylabel('Classifier',fontsize=20)\nplt.grid(color='r', linestyle='-', linewidth=0.5)\nplt.title('Classifier Accuracy',fontsize=20)","d9e835c5":"## Necessary modules for creating models. \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix","5a2c5fb6":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\ntestframe = std_scaler.fit_transform(test)\ntestframe.shape","08cff8a5":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=1000)","2e15a55e":"\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score,recall_score,confusion_matrix\nlogreg = LogisticRegression(solver='liblinear', penalty='l1')\nlogreg.fit(X_train,y_train)\npredict=logreg.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","b35dd7c6":"C_vals = [0.0001, 0.001, 0.01, 0.1,0.13,0.2, .15, .25, .275, .33, 0.5, .66, 0.75, 1.0, 2.5, 4.0,4.5,5.0,5.1,5.5,6.0, 10.0, 100.0, 1000.0]\npenalties = ['l1','l2']\n\nparam = {'penalty': penalties, 'C': C_vals, }\ngrid = GridSearchCV(logreg, param,verbose=False, cv = StratifiedKFold(n_splits=5,random_state=10,shuffle=True), n_jobs=1,scoring='accuracy')\n\ngrid.fit(X_train,y_train)\nprint (grid.best_params_)\nprint (grid.best_score_)\nprint(grid.best_estimator_)","fe8710b4":"#grid.best_estimator_.fit(X_train,y_train)\n#predict=grid.best_estimator_.predict(X_test)\n#print(accuracy_score(y_test,predict))\nlogreg_grid = LogisticRegression(penalty=grid.best_params_['penalty'], C=grid.best_params_['C'])\nlogreg_grid.fit(X_train,y_train)\ny_pred = logreg_grid.predict(X_test)\nlogreg_accy = round(accuracy_score(y_test, y_pred), 3)\nprint (logreg_accy)\nprint(confusion_matrix(y_test,y_pred))\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))","03bcffde":"ABC=AdaBoostClassifier()\n\nABC.fit(X_train,y_train)\npredict=ABC.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))","f80e8289":"from sklearn.tree import DecisionTreeClassifier\nn_estimator=[50,60,100,150,200,300]\nlearning_rate=[0.001,0.01,0.1,0.2,]\nhyperparam={'n_estimators':n_estimator,'learning_rate':learning_rate}\ngridBoost=GridSearchCV(ABC,param_grid=hyperparam,verbose=False, cv = StratifiedKFold(n_splits=5,random_state=15,shuffle=True), n_jobs=1,scoring='accuracy')\n\ngridBoost.fit(X_train,y_train)\nprint(gridBoost.best_score_)\nprint(gridBoost.best_estimator_)","7a8f6fa7":"gridBoost.best_estimator_.fit(X_train,y_train)\npredict=gridBoost.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test,predict))","00d611c7":"xgb=XGBClassifier(max_depth=2, n_estimators=700, learning_rate=0.009,nthread=-1,subsample=1,colsample_bytree=0.8)\nxgb.fit(X_train,y_train)\npredict=xgb.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(confusion_matrix(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","c20686f8":"lda=LinearDiscriminantAnalysis()\nlda.fit(X_train,y_train)\npredict=lda.predict(X_test)\nprint(accuracy_score(y_test,predict))\nprint(precision_score(y_test,predict))\nprint(recall_score(y_test,predict))","2c68e810":"#Decision Tree\n#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree = DecisionTreeClassifier( criterion=\"entropy\",\n                                 max_depth=5,\n                                class_weight = 'balanced',\n                                min_weight_fraction_leaf = 0.009,\n                                random_state=2000)\ndectree.fit(X_train, y_train)\ny_pred = dectree.predict(X_test)\ndectree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(dectree_accy)\nprint(confusion_matrix(y_test,y_pred))\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\n","e13a31d9":"#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import precision_score,recall_score,confusion_matrix\n#randomforest = RandomForestClassifier(n_estimators=100,max_depth=9,min_samples_split=6, min_samples_leaf=4)\n##randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n#randomforest.fit(X_train, y_train)\n#y_pred = randomforest.predict(X_test)\n#random_accy = round(accuracy_score(y_pred, y_test), 3)\n#print (random_accy)\n#print(confusion_matrix(y_test,y_pred))\nfrom sklearn.ensemble import RandomForestClassifier\nrandomforest = RandomForestClassifier(n_estimators=100,max_depth=5,min_samples_split=20,max_features=0.2, min_samples_leaf=8,random_state=20)\n#randomforest = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nrandom_accy = round(accuracy_score(y_pred, y_test), 3)\nprint (random_accy)\nprint(precision_score(y_test,y_pred))\nprint(recall_score(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","c659b5ca":"from sklearn.ensemble import BaggingClassifier\nBaggingClassifier = BaggingClassifier()\nBaggingClassifier.fit(X_train, y_train)\ny_pred = BaggingClassifier.predict(X_test)\nbagging_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(bagging_accy)","756c8966":"# Prediction with catboost algorithm.\nfrom catboost import CatBoostClassifier\nmodel = CatBoostClassifier(verbose=False, one_hot_max_size=3)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nacc = round(accuracy_score(y_pred, y_test), 3)\nprint(acc)","b1ef5c1d":"y_predict=(model.predict(testframe)).astype(int)","7e98a28a":"y_predict","fc2cbc78":"temp = pd.DataFrame(pd.DataFrame({\n        \"PassengerId\": passenger_id,\n        \"Survived\": y_predict\n    }))\n\ntemp.to_csv(\"submission2.csv\", index = False)","77b49982":"**Models**","f8aeac0e":"2.1 Data Cleaning","e7c6f319":"**Conclusion**: Only 38% of the total traveller is survived in disaster","b1dde842":"**Conclusion:** 74% of woman are survived","bfece8d2":"Decision Tree","bf00b47e":"If you like my work please give upvote and if you have any query feel free to ask.\nAny suggestion also welcome","d844ec76":"Ada boost classifier","880841cb":"**Data visualization**","c5e2b9d7":"Grid search on ada boost classifier","090bcb52":"LogisticRegression","a54fb7a7":"\n**Introduction**\n\nI have deided to work with the Titanic dataset again. this kernel is focusing on comparing the performance of several machine learning algorithms. I use several clasification model to create a model predicting survival on the Titanic. I am hoping to learn a lot from this site, so feedback is very welcome! This kernel is always improving because of your feedback!!!\n\nThere are three parts to my script as follows:\n\n    1. Load the library and data\n    2. Data cleaning\n    3. Data spliting\n    4. Training,testing, and Peformance comparison\n    5. Tuning the algorithm\n\nIn this section the library and the data used are loaded into the sytem\n\n1.1 Load the library","5d15c947":"1.3 Data exploration","a5ad8e3d":"1.2 Load the data","0d1ed5b7":"Grid search","8578187f":"Linear Discriminanat Analysis","d68e78b7":"Random Forest Classifier","50b4df4a":"XGB Classifier","7e4ecf7f":"Bagging Classifer"}}