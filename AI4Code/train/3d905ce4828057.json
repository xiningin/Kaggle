{"cell_type":{"6e2ce099":"code","bb51306b":"code","187be05f":"code","4d03eef4":"code","6d689015":"code","541a843a":"code","1ef16b73":"code","0ade765b":"code","c006d017":"code","e9c9a52d":"code","2056dd1a":"code","eda6fc09":"code","f21c7581":"code","eb74500a":"code","36f9993c":"code","3f4708d6":"code","f917623d":"code","be965fa5":"code","12b377c0":"code","338c10db":"code","e036032c":"code","a65edf64":"code","51d58554":"code","54248817":"code","bcbfeb68":"code","97ce4054":"code","ec10b6fb":"code","ec95081f":"code","4c9c3aff":"code","ca3d6910":"code","85636778":"code","1a7c165d":"code","ba23139f":"code","47b2c5dd":"code","afe6bb77":"code","11a6c838":"code","bd3e485e":"code","e0206185":"code","96653ddf":"code","405a2402":"code","31cc4635":"code","283d9ef1":"code","21515119":"code","4fb5c154":"code","347d0482":"code","83cc14b3":"code","00a80f20":"markdown","fea0f919":"markdown","e3caed07":"markdown","80d07842":"markdown","44bb0698":"markdown","681395a7":"markdown","3e93358e":"markdown","59b14111":"markdown","cbf3d3f4":"markdown","b4171354":"markdown","221b399d":"markdown","5f7f15b1":"markdown","b410f97b":"markdown","9eb0ea48":"markdown","3c867ae7":"markdown","56355b09":"markdown","1f4fa396":"markdown","55a94166":"markdown","54c00f74":"markdown","1b84a982":"markdown","6098ba01":"markdown","72030cad":"markdown"},"source":{"6e2ce099":"pip install Lifetimes","bb51306b":"# First of all, let's import the necessary libraries.\nimport datetime as dt\nimport pandas as pd","187be05f":"from sqlalchemy import create_engine\nfrom lifetimes import BetaGeoFitter\nfrom lifetimes import GammaGammaFitter\nfrom sklearn.preprocessing import MinMaxScaler\nfrom lifetimes.plotting import plot_period_transactions","4d03eef4":"pd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\npd.set_option('display.float_format', lambda x: '%.4f' % x)","6d689015":"# We use these functions to ignore outliers\ndef outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","541a843a":"# This is a function used to clear data.\ndef retail_data_prep(dataframe):\n    dataframe.dropna(inplace=True)\n    dataframe = dataframe[~dataframe[\"Invoice\"].str.contains(\"C\", na=False)]\n    dataframe = dataframe[dataframe[\"Quantity\"] > 0]\n    dataframe = dataframe[dataframe[\"Price\"] > 0]\n    replace_with_thresholds(dataframe, \"Quantity\")\n    replace_with_thresholds(dataframe, \"Price\")\n    return dataframe","1ef16b73":"# Read the 2010-2011 data in the Online Retail II excel. Make a copy of the dataframe you created.\n\ndf_= pd.read_csv('..\/input\/online-retail-ii\/online_retail_II.csv')\ndf= df_[df_[\"InvoiceDate\"] >= '2010-12-01']\ndf.head()","0ade765b":"# The Unnamed column has been deleted.\ndf.columns.str.match(\"Unnamed\")\ndf=df.loc[:,~df.columns.str.match(\"Unnamed\")]\ndf.head()","c006d017":"# so we need to clear our data from these values.\ndf= retail_data_prep(df)\ndf.head() # I brought the cleaned df.","e9c9a52d":"# We are choosing United Kingdom as the country.\ndf = df[df['Country'] == 'United Kingdom']\nprint(df.head())","2056dd1a":"df.describe().T","eda6fc09":"# We choose our reference date 2 days after the maximum invoicedate in order not to encounter an error in the transactions.\n\ndf['InvoiceDate']=pd.to_datetime(df['InvoiceDate'])\ntoday = df['InvoiceDate'].max() + pd.DateOffset(days=2)","f21c7581":"df[\"TotalPrice\"] = df[\"Quantity\"] * df[\"Price\"]\ndf.head()","eb74500a":"# Now let's create the recency ,T, frequeny and monetary variables according to the above formulas. \n# While doing this, let's create our variable by taking the dataframe as groupby according to the customer id.\ncltv_df = df.groupby('Customer ID').agg({'InvoiceDate' : [lambda recency : (recency.max()-recency.min()).days,\n                                                         lambda customer_age : (today - customer_age.min()).days],\n                                             'Invoice' : lambda frequency : frequency.nunique(),\n                                          'TotalPrice' : lambda total_price : total_price.sum()})\ncltv_df.head()","36f9993c":"# Let's edit the column names\ncltv_df.columns = cltv_df.columns.droplevel(0)\ncltv_df.head()","3f4708d6":"# Let's rename the column names.\ncltv_df.columns = ['recency', 'T', 'frequency', 'monetary']\n\ncltv_df.head()","f917623d":"# Expressing monetary value as average earnings per purchase\ncltv_df[\"monetary\"] = cltv_df[\"monetary\"] \/ cltv_df[\"frequency\"]","be965fa5":"# Choosing monetary values greater than zero\ncltv_df = cltv_df[cltv_df[\"monetary\"] > 0]\ncltv_df.head()","12b377c0":"# Weekly expression of recency and T for BGNBD\ncltv_df[\"recency\"] = cltv_df[\"recency\"] \/ 7\ncltv_df[\"T\"] = cltv_df[\"T\"] \/ 7","338c10db":"# frequency must be greater than 1\ncltv_df = cltv_df[(cltv_df['frequency'] > 1)]\ncltv_df.head()","e036032c":"bgf = BetaGeoFitter(penalizer_coef=0.001)\nbgf.fit(cltv_df['frequency'],\n        cltv_df['recency'],\n        cltv_df['T'])","a65edf64":"###### Adding expected_purc_1_week and expected_purc_1_month to DF##########\n# for 1 month \n############\ncltv_df[\"expected_purc_1_month\"] = bgf.predict(4,\n                                               cltv_df['frequency'],\n                                               cltv_df['recency'],\n                                               cltv_df['T'])\n\n\ncltv_df.sort_values(\"expected_purc_1_month\", ascending=False).head(10)","51d58554":"# for 1 week\n####################\n\ncltv_df[\"expected_purc_1_week\"] = bgf.predict(1,\n                                               cltv_df['frequency'],\n                                               cltv_df['recency'],\n                                               cltv_df['T'])\n\n\ncltv_df.sort_values(\"expected_purc_1_week\", ascending=False).head(10)","54248817":"ggf = GammaGammaFitter(penalizer_coef=0.01)\nggf.fit(cltv_df['frequency'], cltv_df['monetary'])\n\nggf.conditional_expected_average_profit(cltv_df['frequency'],\n                                        cltv_df['monetary']).head(10)","bcbfeb68":"ggf.conditional_expected_average_profit(cltv_df['frequency'],\n                                        cltv_df['monetary']).sort_values(ascending=False).head(10)","97ce4054":"cltv_df[\"expected_average_profit\"] = ggf.conditional_expected_average_profit(cltv_df['frequency'],\n                                                                             cltv_df['monetary'])\n\ncltv_df.sort_values(\"expected_average_profit\", ascending=False).head(10)","ec10b6fb":"# Calculation of CLTV with BG-NBD and GG model.\ncltv = ggf.customer_lifetime_value(bgf,\n                                   cltv_df['frequency'],\n                                   cltv_df['recency'],\n                                   cltv_df['T'],\n                                   cltv_df['monetary'],\n                                   time=6,  # 6 month\n                                   freq=\"W\",  # frequency information of T.\n                                   discount_rate=0.01).sort_values(ascending=False) \n# # discount_rate = The interest rate used to calculate the present value of the future cash flow. \n#                  This number is usually between 8% and 15%. This value assumes that prices will not increase in the near future.\ncltv.head()","ec95081f":"cltv= cltv.reset_index()\ncltv.head()\n# According to customer life time value, the most important customers","4c9c3aff":"cltv.shape","ca3d6910":"cltv_final = cltv_df.merge(cltv, on=\"Customer ID\", how=\"left\")\ncltv_final.sort_values(by=\"clv\", ascending=False).head(10)","85636778":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(cltv_final[[\"clv\"]])\ncltv_final[\"scaled_clv\"] = scaler.transform(cltv_final[[\"clv\"]])","1a7c165d":"cltv_final.sort_values(by=\"scaled_clv\", ascending=False).head(10)","ba23139f":"cltv_final.describe().T","47b2c5dd":"# for 1 month\ncltv1 = ggf.customer_lifetime_value(bgf,\n                                    cltv_df['frequency'],\n                                    cltv_df['recency'],\n                                    cltv_df['T'],\n                                    cltv_df['monetary'],\n                                    time=1,  # months\n                                    freq=\"W\",  # T haftal\u0131k\n                                    discount_rate=0.01)\n\ncltv1_final = cltv_df.merge(cltv1, on=\"Customer ID\", how=\"left\")\ncltv1_final.sort_values(by=\"clv\", ascending=False).head()","afe6bb77":"cltv12 = ggf.customer_lifetime_value(bgf,\n                                     cltv_df['frequency'],\n                                     cltv_df['recency'],\n                                     cltv_df['T'],\n                                     cltv_df['monetary'],\n                                     time=12,  # months\n                                     freq=\"W\",  # T haftal\u0131k\n                                     discount_rate=0.01)\n\ncltv12_final = cltv_df.merge(cltv12, on=\"Customer ID\", how=\"left\")\ncltv12_final.head()","11a6c838":"cltv1_final.sort_values(\"clv\", ascending=False).head(10)","bd3e485e":"cltv12_final.sort_values(\"clv\", ascending=False).head(10)","e0206185":"##############################################################\n#1. For 2010-2011 UK customers, divide all your customers into 4 groups (segments) \n# according to 6-month CLTV and add the group names to the dataset.\n##############################################################\n","96653ddf":"cltv_final[\"cltv_segment\"] = pd.qcut(cltv_final[\"scaled_clv\"], 4, labels=[\"D\", \"C\", \"B\", \"A\"])\ncltv_final.head()","405a2402":"cltv_final.sort_values(by=\"scaled_clv\", ascending=False).head(10)","31cc4635":"cltv_final.describe().T","283d9ef1":"cltv_final2 = cltv_final.sort_values(by=\"scaled_clv\", ascending=False)\ncltv_final2[cltv_final2['cltv_segment'] == \"A\"].head(25).describe().T","21515119":"cltv_final2[cltv_final2['cltv_segment'] == \"A\"].describe().T","4fb5c154":"cltv_final2[cltv_final2['cltv_segment'] == \"D\"].head(25).describe().T\n","347d0482":"cltv_final2[cltv_final2['cltv_segment'] == \"D\"].describe().T","83cc14b3":"# # credentials.\ncreds = {'user': 'emilie',\n         'passwd': 'emilie123',\n         'host': '12.12.125.125',\n         'port': 1234,\n         'db': 'emilie'}\n\n# MySQL conection string.\nconnstr = 'mysql+mysqlconnector:\/\/{user}:{passwd}@{host}:{port}\/{db}'\n\n# # sqlalchemy engine for MySQL connection.\n# conn = create_engine(connstr.format(**creds))\n\n# # conn.close()\n\n# pd.read_sql_query(\"show databases;\", conn)\n# pd.read_sql_query(\"show tables\", conn)\n\n# ## In this step, we send our database data that we connect remotely.\n# cltv_final.to_sql(name='esrin_erdem', con=conn,\n#                   if_exists='replace', index=False)\n\n# pd.read_sql(\"show tables\", conn)\n# pd.read_sql_query(\"select * from esrin_erdem limit 10\", conn)","00a80f20":"# **BUSINESS PROBLEM**\n**An e-commerce site wants to make a forward-looking projection for customer actions according to the CLTV values of its customers. Is it possible to identify the customers who can bring the most revenue within 1-month or 6-month time periods with the given dataset?**\n","fea0f919":"**CLTV = (Customer Value \/ Churn Rate) * Profit Margin**","e3caed07":"Our goal is to capture the buying behavior of the whole audience in a dataset and to make it possible to predict when individual characteristics come.\nWe will use probabilistic methods for this process.\n\n**CLTV = (BG\/NBD Model) * (Gamma Gamma Submodel)**","80d07842":"# **CLTV (Customer Lifetime Value)**\n![image.png](attachment:d447bb98-997b-4646-bda8-67ee1ed6b3df.png)\n\nIt means the lifetime value of a brand to the customer. It allows us to have information about customers. It is the monetary value that a customer will bring to this company during the relationship-communication with a company. In the RFM analysis, we segmented the customers. However, we could only determine special strategies and marketing strategies from these segments. We were not making any predictions for the future. In other words, with CLTV, we can calculate how much added value our customers can provide us from a wider perspective, namely time projection.","44bb0698":"\n# MISSION 3\n\n**1. For 2010-2011 UK customers, divide all your customers into 4 groups (segments) according to 6-month CLTV and add the group names to the dataset.**\n\n**2. Does it make sense to divide customers into 4 groups based on CLTV scores?Should it be less or more? Please comment.**\n\n**3. Make short 6-month action suggestions to the management for 2 groups you will choose from among 4 groups.**","681395a7":"The BG\/NBD Model (Expected Number of Transaction) allows us to express the number of purchases as probabilistic.\nGamma Gamma Submodel (Conditional Expected Average Profit) allows us to express the average amount of snow as probabilistic.\nData must be unique on a per customer basis. Every customer;\n\n**Recency value** (last shopping date\u200a\u2014\u200afirst shopping date)\n\n**Tenure value** i.e. customer age (today's date\u200a\u2014\u200adate of first purchase)\n\n**Purchase quantity**\n\n**Total income** is calculated.","3e93358e":"# Comment\n\n**Top 25 and general description of segment A were examined. As a result, based on the mean, std deviation and max values, we saw that the mean and std deviation values of the A segment are very far from each other and that it is not homogeneous within itself. In order to obtain more reliable and homogeneous segments in terms of action, this segment needs to be divided a little more.**","59b14111":"# 2. Establishment of BG-NBD Model","cbf3d3f4":"# Submitting records to database","b4171354":"# **Dataset Story**\n\n**The dataset named Online Retail II includes the sales of a UK-based online store between 01\/12\/2009 - 09\/12\/2011.\nThe product catalog of this company includes souvenirs. They can also be considered as promotional items. There is also information that most of its customers are wholesalers.**","221b399d":"# Comments\nWhen we look at the scaled clv in the D segment, we can say that the mean and median values are very close to each other and this segment is homogeneously distributed.\n\n# Action Comment\nWhen we compare the A and D segments, since the value added by the D segment to the company is very low, a feasibility study can be organized in order to increase the number of sales and to promote other products with high prices. We can determine the products to be promoted according to the needs of the customers by conducting a survey. Thus, we can gradually move those in this segment to the next segment.","5f7f15b1":"# **VARIABLES**\n**InvoiceNo**: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter \u2018c\u2019, it indicates a cancellation.\n\n**StockCode**: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n\n**Description**: Product (item) name. Nominal.\n\n**Quantity**: The quantities of each product (item) per transaction. Numeric.\n\n**InvoiceDate**: Invoice Date and time. Numeric, the day and time when each transaction was generated.\n\n**UnitPrice**: Unit price. Numeric, Product price per unit in sterling.\n\n**CustomerID**: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n\n**Country**: Country name. Nominal, the name of the country where each customer resides.","b410f97b":"#  Interpret and evaluate the results you have obtained.\nLooking at the estimation results, we can see that avg monetary and expected avg_profit are close to values, thus making a consistent estimation.\nWhen we look at the scaled_clv value, we see that the average is 1\/100. The reason for this is that the majority of the customers make small-scale purchases and accordingly, the clv is low. This can be explained by Pareto analysis; So the company's top 20% customers account for 80% of sales. Since there is no homogeneous structure between customers, clv is quite low.","9eb0ea48":"# COMMENT\nWhen we look at 1-Month and 12-Month cltv forecast values, 12-Month clv values are estimated over 1-Month clv and there is not much difference between these reports. From this, we can deduce that the clv values of the customers have increased steadily, not exponentially, over time.","3c867ae7":"** First of all, let's import the necessary libraries.**","56355b09":"# **Preparation of Life Time Dataset**\n\n* recency: The time elapsed since the last purchase. Weekly. (according to analysis day before, here is user specific)\n* T: How long before the analysis date the first purchase was made.Weekly.\n* frequency: total number of repeat purchases (frequency>1)\n* monetary_value: average earnings per purchase","1f4fa396":"# **MISSION**\n**6 months CLTV Prediction**\n\n\u25aa Make a 6-month CLTV prediction for 2010-2011 UK customers.\n\n\u25aa Interpret and evaluate the results you have obtained","55a94166":"***Now, let's apply what we have told on an example and interpret the results.***","54c00f74":"# MISSION 2\n\n**1. Calculate 1-month and 12-month CLTV for 2010-2011 UK customers.**\n\n**2. Analyze the 10 highest individuals at 1 month CLTV and the 10 highest at 12 months. Is there a difference?If there is, why do you think it might be?**","1b84a982":"# ** GAMMA-GAMMA Modelinin Kurulmas\u0131**","6098ba01":"# **CLTV Estimation with BGNBD & GG and Sending Results to Remote Server**","72030cad":"\n# **Data Preprocessing**"}}