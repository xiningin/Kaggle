{"cell_type":{"fc29e9c8":"code","c465c615":"code","db721369":"code","3bcd864a":"code","670e83fa":"code","4916ec11":"code","163c6da4":"code","7a717de6":"code","cdfcf58c":"code","1730c79b":"code","4f75294f":"code","379cbcc8":"code","c6940f4e":"code","9ae0bc34":"code","7d5778f0":"code","1d81fd72":"code","da99a5b8":"code","4e970633":"code","6705479d":"code","a96e0c0c":"code","126f5710":"code","c1b26be9":"code","d6f2fea6":"code","5fce2388":"code","f1b653b3":"code","0007dc55":"code","9f2006a0":"code","72462beb":"code","1df9db10":"code","ae8f9353":"code","88b90419":"code","26975c2e":"code","3511ed0e":"code","cc88ddd8":"code","cac33549":"code","9d0d2c90":"code","788f7cc7":"code","18560cfa":"code","82b6c385":"code","2c15bf7c":"code","e2a71397":"code","4f18366b":"code","5387b9e7":"markdown","d7ecc6ce":"markdown","bca98b1d":"markdown","e1dea386":"markdown","e416caf6":"markdown","fffb3680":"markdown","e24ee266":"markdown","fe3ba52f":"markdown","50543027":"markdown","f0e5ec9b":"markdown","307fb230":"markdown","f97025f9":"markdown","1123895b":"markdown","2df5e59a":"markdown","59c4d6f2":"markdown","165fddef":"markdown","f2753f5b":"markdown"},"source":{"fc29e9c8":"!pip install contractions -q","c465c615":"#Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re,string,unicodedata\nimport contractions #import contractions_dict\n\n#FastAI\nimport fastai\nfrom fastai import *\nfrom fastai.text import * \n\n#Functional Tool\nfrom functools import partial\n\n#Garbage\nimport gc\n\n#NLTK\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\n\n#SK Learn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn import metrics\nfrom sklearn.compose import ColumnTransformer\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","db721369":"#Load data\nurl = '..\/input\/kickstarter-nlp\/df_text_eng.csv'\nraw_data = pd.read_csv(url, header='infer')","3bcd864a":"#checking the columns\nraw_data.columns","670e83fa":"#creating a seperate dataframe\ndata = raw_data[['blurb','state']]","4916ec11":"#inspect the shape of the dataframe\ndata.shape","163c6da4":"#Check for null\/missing values in the new dataframe\ndata.isna().sum()","7a717de6":"#Dropping the records with null\/missing values\ndata = data.dropna()","cdfcf58c":"#Checking the records per state\ndata.groupby('state').size()","1730c79b":"#Encoding the State label to convert them to numerical values\nlabel_encoder = LabelEncoder() \n\n#Applying to the dataset\ndata['state']= label_encoder.fit_transform(data['state']) ","4f75294f":"#inspect the newly created dataframe\ndata.head()","379cbcc8":"#Remove special characters & retain alphabets\ndata['blurb'] = data['blurb'].str.replace(\"[^a-zA-Z]\", \" \")","c6940f4e":"#Lowering the case\ndata['blurb'] = data['blurb'].str.lower()\n\n#stripping leading spaces (if any)\ndata['blurb'] = data['blurb'].str.strip()","9ae0bc34":"#removing punctuations\nfrom string import punctuation\n\ndef remove_punct(text):\n  for punctuations in punctuation:\n    text = text.replace(punctuations, '')\n  return text\n\n#apply to the dataset\ndata['blurb'] = data['blurb'].apply(remove_punct)","7d5778f0":"#function to remove macrons & accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\n\n#applying the function on the clean dataset\ndata['blurb'] = data['blurb'].apply(remove_accented_chars)","1d81fd72":"#Function to expand contractions\ndef expand_contractions(con_text):\n  con_text = contractions.fix(con_text)\n  return con_text\n\n#applying the function on the clean dataset\ndata['blurb'] = data['blurb'].apply(expand_contractions) ","da99a5b8":"#Removing Stopwords\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords \n#stop_words = stopwords.words('english')\nstopword_list = set(stopwords.words('english'))","4e970633":"#instantiating the tokenizer function\ntokenizer = ToktokTokenizer()","6705479d":"#function to remove stopwords\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n#applying the function\ndata['blurb_norm'] = data['blurb'].apply(remove_stopwords) ","a96e0c0c":"#Dropping the \"blurb\" column\ndata = data.drop(['blurb'], axis=1)","126f5710":"#Inspect the dataframe after stopword removal\ndata.head()","c1b26be9":"#Databack\ndata_bkup = data.copy()","d6f2fea6":"#data split\ntrain_data, test_data = train_test_split(data, test_size = 0.1, random_state = 12, stratify=data['state'])","5fce2388":"train_data.head()","f1b653b3":"#reseting index for test_data\ntest_data.reset_index(drop=True, inplace=True)\n\n#resting index for train_data\ntrain_data.reset_index(drop=True, inplace=True)","0007dc55":"#Shape of train & test data\nprint(\"Training Data Shape - \",train_data.shape, \" Test Data Shape - \", test_data.shape)","9f2006a0":"#Language Model\nlang_mod = TextLMDataBunch.from_df(train_df= train_data, valid_df=test_data, path='')\n\n#Classification Model\nclass_mod = TextClasDataBunch.from_df(path='', train_df=train_data, valid_df=test_data, vocab=lang_mod.train_ds.vocab, bs=32)","72462beb":"lang_learner = language_model_learner(lang_mod, arch = AWD_LSTM, pretrained = True, drop_mult=0.3)","1df9db10":"#finding the learning rate for language learner\nlang_learner.lr_find()","ae8f9353":"#Plotting the Recorder Plot\nlang_learner.recorder.plot()","88b90419":"#Training the language learner model\nlang_learner.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","26975c2e":"#Saving the language learner encoder\nlang_learner.save_encoder('fai_langlrn_enc')","3511ed0e":"class_learner = text_classifier_learner(class_mod, drop_mult=0.3, arch = AWD_LSTM, pretrained = True)\nclass_learner.load_encoder('fai_langlrn_enc')","cc88ddd8":"#finding the learning rate of this class_learner\nclass_learner.lr_find()","cac33549":"#Plotting the Recorder Plot for the class learner\nclass_learner.recorder.plot()","9d0d2c90":"#Training the Class Learner Model\nclass_learner.fit_one_cycle(1, 1e-3, moms=(0.8,0.7))","788f7cc7":"#saving the Class Learner Model\nclass_learner.save_encoder('fai_classlrn_enc_tuned')","18560cfa":"#free memory\ngc.collect()\n","82b6c385":"class_learner.show_results()","2c15bf7c":"# predictions\npred, trgt = class_learner.get_preds()","e2a71397":"#Confusion matrix\nprediction = np.argmax(pred, axis = 1)\npd.crosstab (prediction, trgt)","4f18366b":"#Prediction on Test Dataset\ntest_dataset = pd.DataFrame({'blurb': test_data['blurb_norm'], 'actual_state' : test_data['state'] })\ntest_dataset = pd.concat([test_dataset, pd.DataFrame(prediction, columns = ['predicted_state'])], axis=1)\n\ntest_dataset.head()","5387b9e7":"As we can observe the accuracy has increased drastically. The current accuracy is at ~ 64% which is strictly OK for the purpose of this tutorial.","d7ecc6ce":"Now, here comes the fun part ..\n\n![](https:\/\/i2.wp.com\/neptune.ai\/wp-content\/uploads\/fastai_logo.png?fit=406%2C194&ssl=1)\n\n\nWe need to prep our text data for 2 different models i.e. **Language & Classification Model**. This can be done using the FastAI libraries\n","bca98b1d":"# FastAI Text Classification - Beginner Tutorial\n\nBuilding on the knowledge & experience gained from the [FastAI Image Classification](https:\/\/www.kaggle.com\/kkhandekar\/fastai-beginner-tutorial) tutorial, we shall attempt to perform a text classification in this notebook.\n\nAnd to keep things interesting, we shall be using the [Kick Starter NLP Dataset](https:\/\/www.kaggle.com\/oscarvilla\/kickstarter-nlp).\n\nOnwards with the scripting ...\n\nCourse of action:\n\n* Libraries\n* Load, Prep & Explore Data\n* Text Data Pre-Processing\n* Build & Train Model\n* Predictions\n\n","e1dea386":"Note: Observe that we have achieved an accuracy of ~ 14% , which really bad.","e416caf6":"And just like in the FastAI Image Classification tutorial we will use the **One Cycle** approach to train our language learner.  The learning rate is chosen based on the plot above.\nThe learning rate = 1e-2","fffb3680":"## Load, Prep & Explore Data","e24ee266":"For the purpose of this tutorial, we are only interested in the \"blurb\" & the \"state\" columns. So, we shall create a seperate dataframe that will only consist these 2 columns.","fe3ba52f":"Now, let's use the \"class_mod\" object created above to build a classifier and then fine-tune our language learner.","50543027":"At this stage, our \"blurb\" column is cleaned and ready to be fed to the Model. We will take a backup of this dataset just incase something goes wrong !","f0e5ec9b":"The learning rate from the above plot is 1e-03","307fb230":"## Build & Train Model\n\nBefore building the model we need to split the dataset into Training & Test\/Validation data. We will split the data into 90:10 ratio where 90% of the data will be used for training & remaining 10% for test\/validation.\n","f97025f9":"Creating a language learner based on the language model (lang_mod) created above.","1123895b":"**Conclusion:** Here we conclude the tutorial for Text Classification using FastAI. As we can clearly observe that the model has an average accuracy due to which not all of the predicted_state is correct. \n\nThe next step from here is to fine-tune the model to increase the accuracy but that I shall keep it for some other day. Thank you for reading.","2df5e59a":"## Libraries","59c4d6f2":"## Predictions\n\nWe will now try to get the predictions for the validation set  (test data) from our learner object","165fddef":"The stopword remover function ingests the text in bite size portions. To achieve this we will have to tokenize (split) our text. Tokenization can be done in 2 ways\n\n1. Using the Split function\n2. Using a Tokenizer function\n\nWe shall implement the tokenization using the second option. NLTK provides a functions for doing just that (check the libraries section above!)\n","f2753f5b":"## Text Data Pre-Processing\n\nWe all have the habit of cleaning our fruits\/veggies before eating them, so in the similar way it is always a good practice to clean text data before feeding it to the model. This will stop the Model from spewing incorrect results. \n\nIn this step we will only focus on cleaning\/pre-processing the \"blurb\" column since the \"state\" columns is a categorical column.\n"}}