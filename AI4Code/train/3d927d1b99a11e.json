{"cell_type":{"5b35bb53":"code","b6217cf4":"code","6e0484b4":"code","5699283b":"code","3d9560db":"code","7142327e":"code","4c1cb96a":"code","86913afe":"code","829885ff":"code","42a51676":"code","61c59bc9":"code","8ee67d6e":"code","da04d0ce":"code","27badb9b":"code","b1648e67":"code","73bcc88f":"code","57518b29":"markdown","8c5495ad":"markdown","de894381":"markdown","e1dd5f8c":"markdown","714896aa":"markdown","7d7e550f":"markdown","352439b6":"markdown"},"source":{"5b35bb53":"reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n                22,24,25,26,27,41,29,\n                32,35,37,40,48,49,47,\n                55,51,52,53,60,61,62,103,65,66,67,69,\n                70,71,74,78,79,\n                82,84,89,90,91,94,95,96,97,99,\n                105,106,110,111,112,118,119,125,128,\n                130,133,134,135,137,\n                140,144,145,147,151,155,157,159,\n                161,162,163,164,167,168,\n                170,171,173,175,176,179,\n                180,181,184,185,187,189,\n                190,191,195,196,199]","b6217cf4":"import numpy as np, pandas as pd\ntrain_file = '..\/input\/train.csv'\ntest_file = '..\/input\/test.csv'\n\ntrain = pd.read_csv(train_file, index_col='ID_code')\nX_test = pd.read_csv(test_file, index_col='ID_code')","6e0484b4":"y_train = train.iloc[:,0]\nX_train = train.iloc[:,1:]","5699283b":"X_all = pd.concat([X_test, X_train])","3d9560db":"from sklearn.preprocessing import StandardScaler\n\nx = X_all.values #returns a numpy array\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\nX_all = pd.DataFrame(x_scaled).set_index(X_all.index)","7142327e":"X_all.loc[:,reverse_list] = X_all.loc[:,reverse_list]*-1","4c1cb96a":"X_all.columns = X_train.columns","86913afe":"test = X_all.loc[X_test.index,:].reset_index(drop=True)\ntrain = X_all.loc[X_train.index,:].reset_index(drop=True)","829885ff":"train0 = train[y_train.values==0].copy()\ntrain1 = train[y_train.values==1].copy()","42a51676":"# CALCULATE MEANS AND STANDARD DEVIATIONS\ns = [0]*200\nm = [0]*200\nfor i in range(200):\n    s[i] = np.std(train['var_'+str(i)])\n    m[i] = np.mean(train['var_'+str(i)])\n    \n# CALCULATE PROB(TARGET=1 | X)\ndef getp(i,x):\n    c = 3 #smoothing factor\n    a = len( train1[ (train1['var_'+str(i)]>x-s[i]\/c)&(train1['var_'+str(i)]<x+s[i]\/c) ] ) \n    b = len( train0[ (train0['var_'+str(i)]>x-s[i]\/c)&(train0['var_'+str(i)]<x+s[i]\/c) ] )\n    if a+b<500: return 0.1 #smoothing factor\n    # RETURN PROBABILITY\n    return a \/ (a+b)\n    # ALTERNATIVELY RETURN ODDS\n    # return a \/ b\n    \n# SMOOTH A DISCRETE FUNCTION\ndef smooth(x,st=1):\n    for j in range(st):\n        x2 = np.ones(len(x)) * 0.1\n        for i in range(len(x)-2):\n            x2[i+1] = 0.25*x[i]+0.5*x[i+1]+0.25*x[i+2]\n        x = x2.copy()\n    return x","61c59bc9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# DRAW PLOTS, YES OR NO\nPicture = True\n# DATA HAS Z-SCORE RANGE OF -4.5 TO 4.5\nrmin=-5; rmax=5; \n# CALCULATE PROBABILITIES FOR 501 BINS\nres=501\n# STORE PROBABILITIES IN PR\npr = 0.1 * np.ones((200,res))\npr2 = pr.copy()\nxr = np.zeros((200,res))\nxr2 = xr.copy()\nct2 = 0\nfor j in range(50):\n    if Picture: plt.figure(figsize=(15,8))\n    for v in range(4):\n        ct = 0\n        # CALCULATE PROBABILITY FUNCTION FOR VAR\n        for i in np.linspace(rmin,rmax,res):\n            pr[v+4*j,ct] = getp(v+4*j,m[v+4*j]+i*s[v+4*j])\n            xr[v+4*j,ct] = m[v+4*j]+i*s[v+4*j]\n            xr2[v+4*j,ct] = i\n            ct += 1\n            pr2[v+4*j,:] = smooth(pr[v+4*j,:],res\/\/10)\n        if Picture:\n            # SMOOTH FUNCTION FOR PRETTIER DISPLAY\n            # BUT USE UNSMOOTHED FUNCTION FOR PREDICTION\n\n            # DISPLAY PROBABILITY FUNCTION\n            plt.subplot(2, 4, ct2%4+5)\n            plt.plot(xr[v+4*j,:],pr2[v+4*j,:],'-')\n            plt.title('P( t=1 | var_'+str(v+4*j)+' )')\n            xx = plt.xlim()\n            # DISPLAY TARGET DENSITIES\n           \n        if (ct2%8==0): print('Showing vars',ct2,'to',ct2+7,'...')\n        ct2 += 1\n    \n    if Picture: plt.show()\n    if(j==0):\n        break","8ee67d6e":"pr2[0]","da04d0ce":"new_df = pd.DataFrame()\nfor i in range(0,200):\n    new_df['var_pb_'+str(i)] = pr2[i]","27badb9b":"new_df.info()","b1648e67":"corr_mat = new_df.corr(method='pearson')\ncorr_mat","73bcc88f":"corr_mat.to_csv('prob_based_cormat.csv',index=True, float_format='%.6f')\nnew_df.to_csv('prob_by_var.csv', index=False, float_format='%.6f')","57518b29":"# Load data","8c5495ad":"# Afterthoughts\nThese does not look random. I have not tried to go beyond this, Yet, if it works, don't forget to say thanks to Chris Deotte.","de894381":"# Scale and flip\nI doubt that standard scaler is the correct way to restore the original scale for the frames. But alternatives looked even worse.","e1dd5f8c":"# The frames: Display Target Density and Target Probability\nAs described by Chris: \"Below are two plots for each of the 200 variables. The first is the density of `target=1` versus `target=0`. The second gives the probability that `target=1` given different values for `var_k`.\" \n\nNote how the shape and the range of graphs are consistnent. All we need to do is to sort them in the right order. Does not it remind you of the last Santander? I am pretty sure the correct order can be restored.","714896aa":"# The list of columns that have to be reversed\nIf you take a look at the orginal graphs by Chris Deotte, notice how similar the probability graphs are. But some are directed to the right, while others are directed to the left. We will flip the graphs with the higest probabilty on the right. And we'll bring everything to one scale to make similarities more pronounced.","7d7e550f":"# Are vars mixed up time intervals? Lets sort it out!\n\nLately, I was gazing at the beatiful graphs created by Chris Deotte here:\nhttps:\/\/www.kaggle.com\/cdeotte\/modified-naive-bayes-santander-0-899\/notebook\n\n..and noticed that they look like mixed up cartoon frames. They have similar pattern, proportions, but they are just mixed up, on different scale and sometimes flipped. So I have cleaned it up; here is the result. ","352439b6":"# Statistical Functions\nBelow are functions to calcuate various statistical things."}}