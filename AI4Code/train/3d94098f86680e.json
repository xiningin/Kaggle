{"cell_type":{"8431f80f":"code","567de7f5":"code","fd183846":"code","d6eb046f":"code","c56a990f":"code","a8553918":"code","3bd32fc5":"code","04ea0649":"code","885f8d5b":"code","f3cb3594":"code","123df450":"code","a6426b5b":"code","7c9832ee":"code","7ad3585b":"code","08c186ff":"code","541eeaf2":"code","81b3ed9b":"code","133673ea":"code","b68edfb6":"code","24f03699":"code","8caa0b09":"code","04908d85":"code","aea77a8d":"code","1b3a2b5f":"code","9e1628bf":"code","5acc715a":"code","544924ed":"code","949258c8":"code","8d30c1c0":"code","49669b5c":"markdown","87dbbc57":"markdown","c248f74b":"markdown","58d2a15a":"markdown","1559f51c":"markdown","3f8c24aa":"markdown","6f423ddc":"markdown","7b6482a4":"markdown","5daa42de":"markdown","828d56a8":"markdown","0e8f02a4":"markdown","41f236ca":"markdown","72b12561":"markdown","4fb8f9f8":"markdown","3adbc02a":"markdown"},"source":{"8431f80f":"import logging\nimport os\nfrom os.path import splitext\nfrom os import listdir\nimport sys\nimport scipy.io\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torch.utils.data import DataLoader, random_split\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\nfrom glob import glob\nfrom PIL import Image, ImageFilter, ImageEnhance\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport cv2","567de7f5":"from PIL import ImageFilter\nimg = Image.open('..\/input\/edesmtsv-loc\/imgs_train\/patient001_frame01_7.png')\nmask = Image.open('..\/input\/edesmtsv-loc\/masks_train\/patient001_frame01_7.png')\nfig, ax = plt.subplots(2,2,figsize=(10,8))\n#print(img.mode)\nax[0,0].imshow(img)\nax[0,1].imshow(mask)\n\nimg_enhanced = ImageEnhance.Brightness(img).enhance(1)\nimg_enhanced = ImageEnhance.Contrast(img_enhanced).enhance(0.9)\nimg_enhanced = ImageEnhance.Sharpness(img_enhanced).enhance(0.9)\n#img_enhanced = img_enhanced.filter(ImageFilter.UnsharpMask(radius=0.5, percent=100, threshold=2))\n#img_enhanced = img_enhanced.filter(ImageFilter.EDGE_ENHANCE)\nax[1,0].imshow(img_enhanced)\nax[1,1].imshow(mask)\nplt.show()\n\n# for i in range(1):\n#     img_filtered = img_enhanced.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n#     img_filtered = img_filtered.filter(ImageFilter.GaussianBlur(radius=1))\n    \n# plt.imshow(img_filtered)\n# plt.show()\n","fd183846":"def enable_dropout(model):\n  for m in model.modules():\n    if m.__class__.__name__.startswith('Dropout'):\n      m.train()","d6eb046f":"\"\"\" Parts of the U-Net model \"\"\"\n\nclass DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n            \n        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n        nn.init.xavier_uniform_(self.conv1.weight)\n        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        \n        self.double_conv = nn.Sequential(\n            self.conv1,\n            #nn.Dropout(p=0.1),\n            nn.BatchNorm2d(mid_channels),\n            nn.ReLU(inplace=True),\n            self.conv2,\n            #nn.Dropout(p=0.1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels \/\/ 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels , in_channels \/\/ 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])\n        # if you have padding issues, see\n        # https:\/\/github.com\/HaiyongJiang\/U-Net-Pytorch-Unstructured-Buggy\/commit\/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https:\/\/github.com\/xiaopeng-liao\/Pytorch-UNet\/commit\/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)","c56a990f":"\"\"\" Full assembly of the parts to form the complete network \"\"\"\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 \/\/ factor)\n        self.up1 = Up(1024, 512 \/\/ factor, bilinear)\n        self.up2 = Up(512, 256 \/\/ factor, bilinear)\n        self.up3 = Up(256, 128 \/\/ factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits","a8553918":"from skimage.transform import rescale, rotate\nfrom torchvision.transforms import Compose\n\ndef transforms(scale=None, angle=None, flip_prob=None):\n    transform_list = []\n\n    if scale is not None:\n        transform_list.append(Scale(scale))\n    if angle is not None:\n        transform_list.append(Rotate(angle))\n    if flip_prob is not None:\n        transform_list.append(HorizontalFlip(flip_prob))\n\n    return Compose(transform_list)\n\n\nclass Scale(object):\n\n    def __init__(self, scale):\n        self.scale = scale\n\n    def __call__(self, sample):\n        image, mask = sample\n\n        img_size = image.shape[0]\n\n        #scale = np.random.uniform(low=1.0 - self.scale, high=1.0 + self.scale)\n        scale = np.random.uniform(low=0.5, high=0.8)\n        #print(scale,scale)\n        \n        image = rescale(\n            image,\n            (scale, scale),\n            multichannel=True,\n            preserve_range=True,\n            mode=\"constant\",\n            anti_aliasing=False,\n        )\n        mask = rescale(\n            mask,\n            (scale, scale),\n            order=0,\n            multichannel=True,\n            preserve_range=True,\n            mode=\"constant\",\n            anti_aliasing=False,\n        )\n\n        if scale < 1.0:\n            diff = (img_size - image.shape[0]) \/ 2.0\n            padding = ((int(np.floor(diff)), int(np.ceil(diff))),) * 2 + ((0, 0),)\n            image = np.pad(image, padding, mode=\"constant\", constant_values=0)\n            mask = np.pad(mask, padding, mode=\"constant\", constant_values=0)\n        else:\n            x_min = (image.shape[0] - img_size) \/\/ 2\n            x_max = x_min + img_size\n            image = image[x_min:x_max, x_min:x_max, ...]\n            mask = mask[x_min:x_max, x_min:x_max, ...]\n\n        return image, mask\n\n\nclass Rotate(object):\n\n    def __init__(self, angle):\n        self.angle = angle\n\n    def __call__(self, sample):\n        image, mask = sample\n\n        angle = np.random.uniform(low=-self.angle, high=self.angle)\n        image = rotate(image, angle, resize=False, preserve_range=True, mode=\"constant\")\n        mask = rotate(\n            mask, angle, resize=False, order=0, preserve_range=True, mode=\"constant\"\n        )\n        return image, mask\n\n\nclass HorizontalFlip(object):\n\n    def __init__(self, flip_prob):\n        self.flip_prob = flip_prob\n\n    def __call__(self, sample):\n        image, mask = sample\n\n        if np.random.rand() > self.flip_prob:\n            return image, mask\n\n        image = np.fliplr(image).copy()\n        mask = np.fliplr(mask).copy()\n\n        return image, mask\n    ","3bd32fc5":"# img = Image.open('..\/input\/train-val-split\/imgs_train\/imgs_train\/010201.png')\n# mask = Image.open('..\/input\/train-val-split\/masks_train\/masks_train\/010201.png')\n# img = np.array(img)\n# mask = np.array(mask)\n# img = img.reshape([img.shape[0],img.shape[1],1]) # the order of the dimenstions can't be changed\n# mask = mask.reshape([mask.shape[0],mask.shape[1],1])\n\n# TF = transforms(scale=0.4,angle=40,flip_prob=0.5)\n# img1,mask1 = TF((img, mask))\n\n# print(img.shape,img1.shape)\n\n# fig, ax = plt.subplots(2, 2, figsize=(10,8))\n# ax[0,0].imshow(img.squeeze(axis=2))\n# ax[0,1].imshow(mask.squeeze(axis=2))\n# ax[1,0].imshow(img1.squeeze(axis=2))\n# ax[1,1].imshow(mask1.squeeze(axis=2))","04ea0649":"class BasicDataset(Dataset):\n    def __init__(self, imgs_dir, masks_dir, scale=1, enh_factor=None, mask_suffix='', transform=None):\n        self.imgs_dir = imgs_dir\n        self.masks_dir = masks_dir\n        self.scale = scale\n        self.enh_factor = enh_factor\n        self.mask_suffix = mask_suffix\n        self.transform = transform\n        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n\n        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n                    if not file.startswith('.')]\n        logging.info(f'Creating dataset with {len(self.ids)} examples')\n\n    def __len__(self):\n        return len(self.ids)\n\n    @classmethod\n    def preprocess(cls, pil_img, pil_mask, scale, enh_factor=None):\n        #resize\n        #w, h = pil_img.size\n        #newW, newH = int(scale * w), int(scale * h)\n        newW, newH = 256, 256\n        assert newW > 0 and newH > 0, 'Scale is too small'\n        pil_img = pil_img.resize((newW, newH))\n        pil_mask = pil_mask.resize((newW, newH))\n        #enhance brightness, contrast, sharpness\n        if enh_factor is not None:\n            enh = np.random.uniform(low=1.0\/(1.0 + enh_factor), high=1.0 + enh_factor)\n            #pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n            pil_img = ImageEnhance.Contrast(pil_img).enhance(enh)\n            pil_img = ImageEnhance.Sharpness(pil_img).enhance(enh)\n            #pil_img = pil_img.filter(ImageFilter.GaussianBlur(radius=2))\n        \n        img_nd = np.array(pil_img)\n        mask_nd = np.array(pil_mask)\n        \n        #clahe = cv2.createCLAHE(clipLimit =2.0, tileGridSize=(8,8))\n        #img_nd = clahe.apply(img_nd)\n\n        if len(img_nd.shape) == 2:\n            img_nd = np.expand_dims(img_nd, axis=2)\n            mask_nd = np.expand_dims(mask_nd, axis=2)\n\n        # HWC to CHW\n        img_trans = img_nd.transpose((2, 0, 1))\n        mask_trans = mask_nd.transpose((2, 0, 1))\n        \n        if img_trans.max() > 1:\n            img_trans = img_trans \/ 255\n            \n        mask_trans = 1.* (mask_trans > 0)\n\n        return img_trans, mask_trans\n\n\n    def __getitem__(self, i):\n        idx = self.ids[i]\n        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n        img_file = glob(self.imgs_dir + idx + '.*')\n\n        assert len(mask_file) == 1, \\\n            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n        assert len(img_file) == 1, \\\n            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n        mask = Image.open(mask_file[0])\n        img = Image.open(img_file[0])\n\n        assert img.size == mask.size, \\\n            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n        \n        img, mask = self.preprocess(img, mask, self.scale, self.enh_factor)\n        \n        if self.transform is not None:          \n            img, mask = self.transform((img.transpose((1, 2, 0)), mask.transpose((1, 2, 0))))\n            img = img.transpose((2, 0, 1))\n            mask = mask.transpose((2, 0, 1))\n\n        return {\n            'image': torch.from_numpy(img).type(torch.FloatTensor),\n            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n        }","885f8d5b":"from torch.autograd import Function\n\nclass DiceCoeff(Function):\n    \"\"\"Dice coeff for individual examples\"\"\"\n\n    def forward(self, input, target):\n        self.save_for_backward(input, target)\n        eps = 0.0001\n        self.inter = torch.dot(input.view(-1), target.view(-1))\n        self.union = torch.sum(input) + torch.sum(target) + eps\n\n        t = (2 * self.inter.float() + eps) \/ self.union.float()\n        return t\n\n    # This function has only a single output, so it gets only one gradient\n    def backward(self, grad_output):\n\n        input, target = self.saved_variables\n        grad_input = grad_target = None\n\n        if self.needs_input_grad[0]:\n            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n                         \/ (self.union * self.union)\n        if self.needs_input_grad[1]:\n            grad_target = None\n\n        return grad_input, grad_target\n\n\ndef dice_coeff(input, target):\n    \"\"\"Dice coeff for batches\"\"\"\n    if input.is_cuda:\n        s = torch.FloatTensor(1).cuda().zero_()\n    else:\n        s = torch.FloatTensor(1).zero_()\n\n    for i, c in enumerate(zip(input, target)):\n        s = s + DiceCoeff().forward(c[0], c[1])\n\n    return s \/ (i + 1)","f3cb3594":"def eval_net(net, loader, device, dropout=None):\n    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n    net.eval()\n    \n    if dropout is not None:\n        enable_dropout(net)\n        \n    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n    n_val = len(loader)  # the number of batch\n    tot = 0\n\n    #with tqdm(total=n_val, desc='Validation round', unit='batch', leave=False) as pbar:\n    for batch in loader:\n        imgs, true_masks = batch['image'], batch['mask']\n        imgs = imgs.to(device=device, dtype=torch.float32)\n        true_masks = true_masks.to(device=device, dtype=mask_type)\n\n        with torch.no_grad():\n            mask_pred = net(imgs)\n\n        if net.n_classes > 1:\n            tot += F.cross_entropy(mask_pred, true_masks).item()\n        else:\n            pred = torch.sigmoid(mask_pred)\n            pred = (pred > 0.5).float()\n            tot += dice_coeff(pred, true_masks).item()\n            #pbar.update()\n\n    net.train()\n    return tot \/ n_val","123df450":"# dir_img = '..\/input\/dataset\/imgs\/imgs\/'\n# dir_mask = '..\/input\/dataset\/masks\/masks\/'\n\n# dataset = BasicDataset(dir_img, dir_mask, 1)\n# n_val = int(len(dataset) * 0.1)\n# n_train = len(dataset) - n_val\n# train, val = random_split(dataset, [n_train, n_val])\n# print(val)\n# print(val[0]['mask'].shape)","a6426b5b":"dir_img_train = '..\/input\/train-val-split\/imgs_train\/imgs_train\/'\ndir_img_val = '..\/input\/train-val-split\/imgs_val\/imgs_val\/'\ndir_mask_train = '..\/input\/train-val-split\/masks_train\/masks_train\/'\ndir_mask_val = '..\/input\/train-val-split\/masks_val\/masks_val\/'\ndir_checkpoint = 'checkpoints\/'\n# dir_img_train = '..\/input\/ededmtsv\/imgs_train\/'\n# dir_img_val = '..\/input\/ededmtsv\/ESimgs_val\/'\n# dir_mask_train = '..\/input\/ededmtsv\/masks_train\/'\n# dir_mask_val = '..\/input\/ededmtsv\/ESmasks_val\/'\n# dir_checkpoint = 'checkpoints\/'\n# dir_img_train = '..\/input\/ed-es-split\/ED\/imgs_train\/'\n# dir_img_val = '..\/input\/ed-es-split\/ED\/imgs_val\/'\n# dir_mask_train = '..\/input\/ed-es-split\/ED\/masks_train\/'\n# dir_mask_val = '..\/input\/ed-es-split\/ED\/masks_val\/'\n# dir_checkpoint = 'checkpoints\/'\n\ndef train_net(net,\n              device,\n              epochs=5,\n              batch_size=1,\n              lr=0.001,\n              val_percent=0.1,\n              save_cp=True,\n              img_scale=1):\n    #dataset = BasicDataset(dir_img, dir_mask, img_scale)\n#     dataset = BasicDataset(dir_img, dir_mask, img_scale, ifblur=True)\n#     n_val = int(len(dataset) * val_percent)\n#     n_train = len(dataset) - n_val\n#    train, val = random_split(dataset, [n_train, n_val]) #,transform=transforms(scale=0.5,angle=40,flip_prob=0.5)\n#     dataset_transformed1 = BasicDataset(dir_img, dir_mask, img_scale, ifblur=True, transform=transforms(scale=0.1,angle=10,flip_prob=0.1))\n#     dataset_transformed2 = BasicDataset(dir_img, dir_mask, img_scale, ifblur=True, transform=transforms(scale=0.1,angle=10,flip_prob=0.1))\n#     dataset_transformed3 = BasicDataset(dir_img, dir_mask, img_scale, ifblur=True, transform=transforms(scale=0.1,angle=10,flip_prob=0.1))\n#     dataset_transformed4 = BasicDataset(dir_img, dir_mask, img_scale, ifblur=True, transform=transforms(scale=0.1,angle=10,flip_prob=0.1))\n#     train = ConcatDataset([train,dataset_transformed1,dataset_transformed2,dataset_transformed3,dataset_transformed4])\n#     n_train = len(train)\n\n    train = BasicDataset(dir_img_train, dir_mask_train, img_scale)\n    for kk in range(2):\n        train_tf = BasicDataset(dir_img_train, dir_mask_train, img_scale, enh_factor=1., transform=transforms(scale=0.1,angle=15, flip_prob=0.5))\n        train = ConcatDataset([train,train_tf])\n        \n    val = BasicDataset(dir_img_val, dir_mask_val, img_scale)\n    n_train = int(len(train))\n    n_val = int(len(val))\n    \n    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n\n    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n    global_step = 0\n\n    print(f'''Starting training:\n        Epochs:          {epochs}\n        Batch size:      {batch_size}\n        Learning rate:   {lr}\n        Training size:   {n_train}\n        Validation size: {n_val}\n        Checkpoints:     {save_cp}\n        Device:          {device.type}\n        Images scaling:  {img_scale}\n    ''')\n\n    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n    #optimizer = optim.Adagrad(net.parameters(), lr=lr, lr_decay=0, weight_decay=1e-8)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n    if net.n_classes > 1:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n        \n        \n    # validation score\n    progresses = []\n    valScores = []\n    val_score_max = -1\n    \n    for epoch in range(epochs):\n        net.train()\n\n        epoch_loss = 0\n        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}\/{epochs}', unit='img') as pbar:\n            for batch in train_loader:\n                imgs = batch['image']\n                true_masks = batch['mask']\n                assert imgs.shape[1] == net.n_channels, \\\n                    f'Network has been defined with {net.n_channels} input channels, ' \\\n                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n                    'the images are loaded correctly.'\n\n                imgs = imgs.to(device=device, dtype=torch.float32)\n                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n                true_masks = true_masks.to(device=device, dtype=mask_type)\n\n                masks_pred = net(imgs)\n                loss = criterion(masks_pred, true_masks)\n                epoch_loss += loss.item()\n                writer.add_scalar('Loss\/train', loss.item(), global_step)\n\n                pbar.set_postfix(**{'loss (batch)': loss.item()})\n\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n                optimizer.step()\n\n                pbar.update(imgs.shape[0])\n                global_step += 1\n                if global_step % (n_train \/\/ (10 * batch_size)) == 0:\n                    for tag, value in net.named_parameters():\n                        tag = tag.replace('.', '\/')\n                        writer.add_histogram('weights\/' + tag, value.data.cpu().numpy(), global_step)\n                        writer.add_histogram('grads\/' + tag, value.grad.data.cpu().numpy(), global_step)\n                    val_score = eval_net(net, val_loader, device)\n                    scheduler.step(val_score)\n                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n                    progress = 1 + global_step * batch_size \/ n_train\n                    if net.n_classes > 1:\n                        print('Validation cross entropy: {}'.format(val_score))\n                        writer.add_scalar('Loss\/test', val_score, global_step)\n                        progresses.append(progress)\n                        valScores.append(val_score)\n                    else:\n                        print('Validation Dice Coeff: {}'.format(val_score))\n                        writer.add_scalar('Dice\/test', val_score, global_step)\n                        progresses.append(progress)\n                        valScores.append(val_score)\n\n                    writer.add_images('images', imgs, global_step)\n                    if net.n_classes == 1:\n                        writer.add_images('masks\/true', true_masks, global_step)\n                        writer.add_images('masks\/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n                    \n                    if val_score > val_score_max:\n                        val_score_max = val_score\n                        print(val_score_max)\n                        try:\n                            os.mkdir(dir_checkpoint)\n                            logging.info('Created checkpoint directory')\n                        except OSError:\n                            pass\n                        torch.save(net.state_dict(),\n                                   dir_checkpoint + f'maxValScore.pth')\n                        logging.info(f'maxValScore{val_score_max} saved !')\n            \n        if save_cp:\n            try:\n                os.mkdir(dir_checkpoint)\n                logging.info('Created checkpoint directory')\n            except OSError:\n                pass\n            torch.save(net.state_dict(),\n                       dir_checkpoint + f'CP_epoch{epoch + 1}.pth')\n            logging.info(f'Checkpoint {epoch + 1} saved !')\n\n    print(progresses,valScores)\n    df = pd.DataFrame({'progress':progresses,'val_score': valScores}) \n    df.to_csv(f'valScores_batchSize{batch_size}.csv')\n    \n    writer.close()\n    \n    \n    \nif __name__ == '__main__':\n    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f'Using device {device}')\n\n    # Change here to adapt to your data\n    # n_channels=3 for RGB images\n    # n_classes is the number of probabilities you want to get per pixel\n    #   - For 1 class and background, use n_classes=1\n    #   - For 2 classes, use n_classes=1\n    #   - For N > 2 classes, use n_classes=N\n    net = UNet(n_channels=1, n_classes=1, bilinear=True)\n    print(f'Network:\\n'\n                 f'\\t{net.n_channels} input channels\\n'\n                 f'\\t{net.n_classes} output channels (classes)\\n'\n                 f'\\t{\"Bilinear\" if net.bilinear else \"Transposed conv\"} upscaling')\n    net.load_state_dict(torch.load('..\/input\/acdc-trained-model\/maxValScore.pth')) #..\/input\/maxvalscore\n    #net.load_state_dict(torch.load('checkpoints\/maxValScore.pth'))\n    net.to(device=device)\n    # faster convolutions, but more memory\n    # cudnn.benchmark = True\n    img_scale_train_predict=1\n    try:\n        train_net(net=net,device=device,lr=0.0005,epochs=10,batch_size=8,img_scale=img_scale_train_predict)\n\n    except KeyboardInterrupt:\n        torch.save(net.state_dict(), 'INTERRUPTED.pth')\n        print('Saved interrupt')\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)","7c9832ee":"valScores = [[1.099149453219927, 1.1982989064398542, 1.2974483596597812, 1.3965978128797083, 1.4957472660996354, 1.5948967193195625, 1.6940461725394895, 1.7931956257594168, 1.892345078979344, 1.991494532199271, 2.090643985419198, 2.189793438639125, 2.288942891859052, 2.388092345078979, 2.487241798298906, 2.5863912515188336, 2.6855407047387607, 2.784690157958688, 2.883839611178615, 2.982989064398542], [0.8910663019527089, 0.8759025064381686, 0.8953686464916576, 0.8888186433098533, 0.8933178728277033, 0.8816984620961276, 0.8923546130006964, 0.8980839902704413, 0.8964133641936562, 0.8969235420227051, 0.8976814367554404, 0.8983002684333108, 0.9012646729295904, 0.8998090353879061, 0.8982585885308005, 0.9001506675373424, 0.8992686217481439, 0.900595637885007, 0.8987779238007285, 0.8999676487662576]]\nplt.plot(valScores[0],valScores[1])\nplt.show()","7ad3585b":"# def crop(img, mask):\n    \n#     sum0 = np.sum(mask,axis=0)\n#     left = next((i for i, x in enumerate(sum0) if x), None)\n#     right = len(sum0)-1-next((i for i, x in enumerate(np.flipud(sum0)) if x), None)\n    \n#     sum1 = np.sum(mask,axis=1)\n#     up = next((i for i, x in enumerate(sum1) if x), None)\n#     down = len(sum1)-1-next((i for i, x in enumerate(np.flipud(sum1)) if x), None)\n    \n#     center = [(up+down)\/\/2,(left+right)\/\/2]\n    \n#     figsize_max = min([center[0],len(sum1)-1-center[0],center[1],len(sum0)-1-center[1]])\n#     figsize_min = max(abs(down-up)\/\/2,abs(right-left)\/\/2)\n#     figsize = min(int(figsize_min*1.2),figsize_max)\n    \n#     img_cropped = img[center[0]-figsize:center[0]+figsize,center[1]-figsize:center[1]+figsize]\n#     mask_cropped = mask[center[0]-figsize:center[0]+figsize,center[1]-figsize:center[1]+figsize]\n    \n#     return img_cropped, mask_cropped\n\n\n# net = UNet(n_channels=1, n_classes=1)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# net.load_state_dict(torch.load('checkpoints\/maxValScore.pth'))\n# net.to(device=device)\n\n# img = Image.open(\"..\/input\/ededmtsv\/EDimgs_val\/patient002_frame01_1.png\")\n# gt = Image.open(\"..\/input\/ededmtsv\/EDmasks_val\/patient002_frame01_1.png\")\n# mask = predict_img(net=net,full_img=img,device=device,scale_factor=img_scale_train_predict)\n\n# img_cropped, mask_cropped = crop(np.array(img),np.array(mask))\n\n# fig, ax = plt.subplots(2, 2, figsize=(10,8))\n# ax[0,0].imshow(img)\n# ax[0,1].imshow(mask)\n# ax[1,0].imshow(img_cropped)\n# ax[1,1].imshow(mask_cropped)\n# plt.show()","08c186ff":"# for i in [1,2,4,8,16,32]:\n#     val_scores = pd.read_csv(f'valScores_batchSize{i}.csv')\n#     plt.plot(val_scores['progress'], val_scores['val_score'], 'o-')\n#     print(np.mean(val_scores['val_score']))\n# plt.show()","541eeaf2":"# plt.semilogx([1,2,4,8,16,32], [0.62,0.82,0.833,0.825,0.818,0.815], marker = \".\",\n#              markersize = 15,\n#              color = \"green\")\n# plt.show()","81b3ed9b":"# val_scores = pd.read_csv('valScores.csv')\n# val_scores.plot(x='progress', y='val_score', style='o-')","133673ea":"# val_coef = np.array([3.34e-7,3.34e-7,0.4466,0.3693,0.4906,0.5440,0.7359,0.5261,0.5154,0.7295,0.7068,0.6698,0.6609,0.6973,0.6816,0.6799,0.682,0.665,0.691,0.6896,0.6861,0.6811,0.6811,0.6921,0.6924,0.67,0.6806,0.6704,0.6963,0.6762,0.6716,0.6852,0.6826,0.6790,0.6742,0.6961,0.6843,0.658,0.6967,0.6912,0.6921,0.6841,0.675,0.6891,0.6865,0.6779,0.6886,0.6958,0.6741])\n# val_coef.shape\n# batch = 4859*np.linspace(0.1,4.9,49)\n# batch.shape\n\n# np.mean(val_coef[25:])\n# fig, ax = plt.subplots(figsize=(8,6))\n# ax.plot(batch,val_coef)\n# ax.set_xlabel(r'$N_{img}$')\n# ax.set_ylabel(r'dice coef')\n# ax.grid(True)\n\n# plt.tight_layout()\n# plt.show()\n\n# Dice coefficients for data augmented training\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n# def avg(l):\n#     return sum(l)\/len(l)\n    \n# c1 = [8.342151795079794e-8,8.342151795079794e-8,0.13550481214310328,0.00017268870217869217,0.0015030856875223027,0.038649870776008656,0.06837620400518463,0.0718836730376438,0.22253820497421217,0.2750045597937712]\n# c2 = [0.34545829179865056,0.4216722132922242,0.28074908897886175,0.2974776866250931,0.2985949849702192,0.3005134752355682,0.3322705547367274,0.29018301265237456,0.19106506848827096,0.28742873524974144]\n# c3 = [0.3170816924170218,0.35009495418882297,0.2582368093731146,0.32543829847403666,0.27291185468554585,0.27967138140826725,0.2890430426462474,0.2716689384326936,0.2970210781208818,0.32076279743938946]\n# c4 = [0.28576749230755905,0.27819131504373423,0.21861903234308186,0.27356915392493036,0.3175197874384265,0.28742572673470657,0.3162825230981673,0.370911373314276,0.24157139401266933,0.35551855510612157]\n# c5 = [0.30452941410460843,0.3338409652793703,0.25483457670274234,0.2726094572846208,0.3371369826074238,0.33076721109667195,0.39150443941577495,0.369756573147652,0.39325224444879603,0.4382415868578259]\n# c6 = [0.34762836339397035,0.3348697291924539,0.31881733830255204,0.2765474835452054,0.3317019976654448,0.2923832739499613,0.3133022528564475,0.36813803657488453,0.33383825089530494,0.3533321650088963]\n# c7 = [0.26461611698844023,0.2791357851971695,0.3447723474296858,0.34020194411330423,0.3144985244491214,0.2613987688457661,0.3128248557221889,0.35307294628278446,0.240462706653908,0.3047865303078538]\n# c8 = [0.33926115453068456,0.35884991919113896,0.26687436426765915,0.35377476914532285,0.2421639991622388,0.3091718649434974,0.3658187734121096,0.36940642097084103,0.23713118934338906,0.29319130597433135]\n# c9 = [0.27923191847006834,0.2686756506015369,0.29434194002517833,0.3932739068846964,0.2707524486221539,0.27501566963879776,0.35513839992633145,0.30591233908777016,0.33454056205151206,0.2999326561715095]\n# c10 = [0.2857081029047295,0.22442107186575952,0.3122294713744179,0.2930739675248698,0.32494003255529713,0.39224808578253667,0.39799212413134605,0.2730237404955227,0.2879872918954579,0.3345421023803078]\n# c_mean = [avg(ele) for ele in [c1,c2,c3,c4,c5,c6,c7,c8,c9,c10]]\n# print(c_mean)\n# plt.plot(list(range(1,len(c_mean)+1)),c_mean)\n# plt.show()\n\n# #import numpy as np\n# #from scipy.interpolate import make_interp_spline\n\n# c = c1 + c2 + c3 + c4 + c5 + c6 + c7 + c8 + c9 + c10\n\n# progress = list(range(1,len(c)+1))\n\n# plt.plot(progress,c)\n# plt.plot(progress,np.convolve(c, np.ones(7)\/7)[:len(c)])\n# plt.show()\n","b68edfb6":"def plot_img_and_mask(img, mask, real_mask):\n    classes = mask.shape[2] if len(mask.shape) > 2 else 1\n    fig, ax = plt.subplots(1, classes + 2, figsize=(13,4))\n    #ax[0].set_title('Input image')\n    ax[0].imshow(img)\n    ax[0].set_axis_off()\n    if classes > 1:\n        for i in range(classes):\n            ax[i+1].set_title(f'Output mask (class {i+1})')\n            ax[i+1].imshow(mask[:, :, i])\n    else:\n        #ax[1].set_title(f'Output mask')\n        ax[1].imshow(mask)\n        ax[1].set_axis_off()\n    #ax[-1].set_title(f'Real mask')\n    ax[-1].imshow(real_mask)\n    ax[-1].set_axis_off()\n    #plt.xticks([]), plt.yticks([])\n    plt.show()","24f03699":"from torchvision import transforms as trans\n\n\ndef predict_img(net,\n                full_img,\n                full_gt,\n                device,\n                scale_factor=1,#this should be in consistent with the one in train.py\n                out_threshold=0.5):\n    net.eval()\n    #enable_dropout(net)\n\n    img, gt = BasicDataset.preprocess(full_img, full_gt, scale_factor)\n    img = torch.from_numpy(img)\n    gt = torch.from_numpy(gt)\n    \n    img = img.unsqueeze(0)\n    img = img.to(device=device, dtype=torch.float32)\n    gt = gt.unsqueeze(0)\n    gt = gt.to(device=device, dtype=torch.float32)\n\n    with torch.no_grad():\n        output = net(img)\n\n        if net.n_classes > 1:\n            probs = F.softmax(output, dim=1)\n        else:\n            probs = torch.sigmoid(output)\n            \n        dc = DiceCoeff().forward((probs > out_threshold).float(), gt)\n        \n        probs = probs.squeeze(0)\n        tf = trans.Compose(\n            [\n                trans.ToPILImage(),\n                trans.Resize(full_img.size[1]),\n                trans.ToTensor()\n            ]\n        )\n\n        probs = tf(probs.cpu())\n        full_mask = probs.squeeze().cpu().numpy()\n\n    return full_mask > out_threshold, dc\n\nif __name__ == \"__main__\":\n    net = UNet(n_channels=1, n_classes=1)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    net.load_state_dict(torch.load('..\/input\/edesmtsv-loc-model\/maxValScore0.92.pth')) #, map_location=device\n    net.to(device=device)\n#     path_imgs = '..\/input\/train-val-split\/imgs_val\/imgs_val\/'\n#     path_masks = '..\/input\/train-val-split\/masks_val\/masks_val\/'\n    path_imgs = '..\/input\/edesmtsv-loc\/imgs_train\/'\n    path_masks = path_imgs.replace('imgs','masks')\n    imgs = os.listdir(path_imgs)\n    N = 5 #len(imgs)\n    print(N)\n    dcs = np.zeros([N,1])\n    for ii in range(N):\n        try:\n            img_id = ii #np.random.randint(len(imgs))\n            img = Image.open(path_imgs + imgs[img_id])\n            gt = Image.open(path_masks + imgs[img_id])\n            mask, dc = predict_img(net=net,full_img=img, full_gt=gt, device=device, scale_factor=1)#img_scale_train_predict\n            #print(mask.shape,np.array(gt).shape)\n            #gt_np = 1.0 * (np.array(gt)>0)\n            #print(dc.item(),2*np.sum(gt_np*mask)\/(np.sum(gt_np)+np.sum(mask)))\n            #plot_img_and_mask(img, mask, gt)\n            dcs[ii] = dc.item()\n        except:\n            print('wrong')\n    #mask_and = -1.* mask1 * mask + 1.* mask\n    #plot_img_and_mask(img, mask_and, real_mask)\n    fig, ax = plt.subplots(1, 1, figsize=(8,4))\n    ax.hist(dcs)\n    ax.set_xlabel('dice coefficient')\n    ax.set_ylabel('counts')\n    print(np.mean(dcs)) #0.91939","8caa0b09":"img = Image.open('..\/input\/ededmtsv\/imgs_train\/patient063_frame16_5.png')\nmask = Image.open('..\/input\/ededmtsv\/masks_train\/patient063_frame16_5.png')\nimg_enhanced = ImageEnhance.Contrast(img).enhance(2)\nplt.imshow(img_enhanced)\nplt.show()\nplt.imshow(mask)\nplt.show()","04908d85":"fig, ax = plt.subplots(1, 1, figsize=(8,4))\nax.hist(dcs)\nax.set_xlabel('dice coefficient')\nax.set_ylabel('counts')\nprint(np.mean(dcs),np.std(dcs))","aea77a8d":"net = UNet(n_channels=1, n_classes=1)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnet.load_state_dict(torch.load('checkpoints\/maxValScore.pth')) #, map_location=device\nnet.to(device=device)\n\n# path_imgs = '..\/input\/train-val-split\/imgs_val\/imgs_val\/'\n# path_masks = '..\/input\/train-val-split\/masks_val\/masks_val\/'\npath_imgs = '..\/input\/edesmtsv-loc\/ESimgs_val\/'\npath_masks = path_imgs.replace('imgs_val','masks_val')\nval = BasicDataset(path_imgs, path_masks, scale=1)\nval_loader = DataLoader(val, batch_size=1, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)  \nN = 1\ndcs = np.zeros([N,1])\nfor ii in range(N):\n    dcs[ii] = eval_net(net,val_loader,device)\n    print(dcs[ii])\n\nfig, ax = plt.subplots(1, 1, figsize=(8,4))\nax.hist(dcs)\nax.set_xlabel('batch averaged dice coefficient')\nax.set_ylabel('counts')\nprint(np.mean(dcs),np.std(dcs))","1b3a2b5f":"def plot_img_mask(img, mask):\n    classes = 1\n    fig, ax = plt.subplots(1, classes + 1, figsize=(8,4))\n    #ax[0].set_title('Image')\n    ax[0].imshow(img)\n    ax[0].set_axis_off()\n    if classes > 1:\n        for i in range(classes):\n            ax[i+1].set_title(f'Mask (class {i+1})')\n            ax[i+1].imshow(mask[:, :, i])\n    else:\n        #ax[1].set_title(f'Mask')\n        ax[1].imshow(mask)\n        ax[1].set_axis_off()\n    #ax[-1].set_title(f'Real mask')\n    plt.xticks([]), plt.yticks([])\n    plt.savefig('dataset.png')\n    plt.show()\n    \n    \npath_imgs = '..\/input\/train-val-split\/imgs_train\/imgs_train\/'\npath_masks = '..\/input\/train-val-split\/masks_train\/masks_train\/'\nimgs = os.listdir('..\/input\/train-val-split\/imgs_train\/imgs_train\/')\nN = 1\nfor ii in range(N):\n    img_id = np.random.randint(len(imgs))\n    img = Image.open(path_imgs + '010701.png')\n    mask = Image.open(path_masks + '010701.png')\n    plot_img_mask(img, mask)\n","9e1628bf":"# from PIL import ImageOps\n# path_imgs = \"..\/..\/input\/mri-images-masks\/imgs\/imgs\/\"\n# dirs = os.listdir(path_imgs)\n\n# for file in dirs:\n#    img = Image.open( path_imgs + file )\n#    cropped = img.crop((144, 43, 542, 441))\n#    img_gray = ImageOps.grayscale(cropped)\n#    img_gray.save(file)\n    \n# path_masks = \"..\/..\/input\/mri-images-masks\/masks\/masks\/\"\n# dirs = os.listdir(path_masks)    \n# for file in dirs:\n#    img = Image.open( path_masks + file )\n#    cropped = img.crop((144, 43, 542, 441))\n#    img_gray = ImageOps.grayscale(cropped)\n#    img_gray.save(file)","5acc715a":"# # mask_real = Image.open('..\/input\/mri-images-masks\/masks\/masks\/010216.png')\n# # BasicDataset.preprocess(mask_real, 1).shape\n# # read image\n# img = Image.open('..\/input\/dataset\/imgs\/imgs\/010216.png')\n# print(img.size)\n# plt.imshow(img)\n# plt.show()\n# #Image.fromarray(np.uint8(img), 'L').save(\"file.png\")\n# print(im1.size)\n# wet = ImageOps.grayscale(im1)\n# wet.save('file.png')\n# # mask_real = torch.from_numpy(BasicDataset.preprocess(mask_real, 1))\n# # mask_real = mask_real.reshape((542, 691)).numpy()\n# #print(img[442,144],img[442,543],img[43,144],img[43,543])\n# #print(img[43:442,144:543].shape)\n# # plt.imshow(img)#(43,144),(43,543),(442,144),(442,543)\n# # plt.show()\n# # plt.imshow(mask)\n# # plt.show()","544924ed":"# img = plt.imread('file.png')\n# print(img.shape)\n# plt.imshow(img)\n# plt.show()\n# img = Image.open('..\/input\/mri-images-masks\/imgs\/imgs\/010216.png')\n# img = BasicDataset.preprocess(img, 1)\n# print(img.shape)\n# #plt.imshow(img)\n# plt.show()\n# plt.imshow(mask)\n# plt.show()\n# print(mask.shape)\n# mask_real = Image.open('..\/input\/mri-images-masks\/masks\/masks\/010216.png')\n# plt.imshow(mask_real)\n# plt.show()","949258c8":"ls","8d30c1c0":"from IPython.display import FileLink\nFileLink(r'checkpoints\/maxValScore.pth')","49669b5c":"# Predict","87dbbc57":"# Routine for cropping images. \nDon't run it in this Notebook because it's hard to extract out the ouput. Better to run on your own workstation.","c248f74b":"# Enable dropout of the model\nused in the function eval_net() and predict_img()","58d2a15a":"# Data augmentation (transform)","1559f51c":"# Download output files from Kaggle\n1. Copy the target file to the directory \/kaggle\/working\n2. Run the FileLink code\n","3f8c24aa":"# Averaged dice coefficient over the validation set","6f423ddc":"# Plot out dice coef v.s. progress","7b6482a4":"# Dataloader","5daa42de":"# Dice loss","828d56a8":"# Import modules","0e8f02a4":"# Visualizal predictions","41f236ca":"# Transfrom test","72b12561":"# Train model","4fb8f9f8":"# Evaluation","3adbc02a":"# Unet Model"}}