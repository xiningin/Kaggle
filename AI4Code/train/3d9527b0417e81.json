{"cell_type":{"0c0325f0":"code","b9170b6f":"code","806dd44d":"code","1018e6b4":"code","22238953":"code","e05061f2":"code","3cdb6c8e":"code","8437a5ff":"code","b0147fae":"code","5d1a95b0":"code","98c8019d":"code","8fc11bbe":"code","af326fd7":"code","eb6be372":"code","f79ac0b2":"code","01907b0d":"code","5f80b44e":"code","bf07a6f5":"code","8ddf28f5":"code","3d089c57":"code","4442a2e4":"code","7b5c7857":"code","94106ed3":"code","5c9bea22":"code","c4a0da77":"code","be843427":"code","402bd4d6":"code","0ae68186":"code","ea1ad7a0":"code","48b7ce2e":"code","40060b57":"code","db99316e":"code","2a3df252":"code","3e84d1df":"code","ee155230":"code","d722a8d0":"markdown","4a4bb8fd":"markdown","b1184d06":"markdown","e35d93c8":"markdown","8cbb55a1":"markdown","e85d4c10":"markdown","d1e71f89":"markdown","e0259b64":"markdown","e4cd5820":"markdown","4879f77b":"markdown","a3228e6b":"markdown","ed2b2c63":"markdown","511d1cda":"markdown","baa26324":"markdown","a05a7e8e":"markdown","54f24172":"markdown","4c3d3232":"markdown"},"source":{"0c0325f0":"import re\n\nexample = '''guru999 get\nguru99 give\nguru Selenium'''","b9170b6f":"x = re.match(r'\\sS\\w+', example)\nif x != None:\n    print(x.group())\nelse:\n    print('x is None')\n    \n    \ny = re.search(r'\\sS\\w+', example)\nif y != None:\n    print(y.group())\nelse:\n    print('y is None')","806dd44d":"x = re.search(r'g\\w+' ,example)\nprint(x.group())\n\ny = re.search(r'\\d{2}', example)\nprint(y.group())","1018e6b4":"x = re.findall(r'u\\d{2}', example)\nprint(x)","22238953":"another_ex = 'Hi, my99 name is Dominic'\n\nre.split(r'\\d{2}', another_ex)","e05061f2":"from nltk.tokenize import sent_tokenize     # Break sentences -> 1 token = sentences\nfrom nltk.tokenize import word_tokenize     # Break sentence  -> 1 token = word\nfrom nltk.tokenize import regexp_tokenize\nfrom nltk.tokenize import TweetTokenizer","3cdb6c8e":"# Tokenize the sentences using NLTK\n# nltk.sent_tokenize -> break by looking for PUNCTUATION = (';', ':', ',', '.', '!', '?')\n\nsong = '''Fly me to the moon. Let me play among the stars\nLet me see what spring is like. On a, Jupiter and Mars'''\n\nsentences = sent_tokenize(song)\nwords = word_tokenize(song)\n\nsentences    ","8437a5ff":"# Mannually split the sentences using regex\n\nre.split(r'[;.!?\\n]', song)","b0147fae":"print(words)","5d1a95b0":"word_tokenize(\"I dont't like Sam's shoes.\")","98c8019d":"re.findall('(\\d{2}\\w)|(\\w+)', 'He has 11 cats. 00x is a James Bond')","8fc11bbe":"from collections import defaultdict, Counter\ncounts = defaultdict(int)\n\ntext = \"The cat is in the box. The cat likes the box. The box is over the cat.\"\nwords = word_tokenize(text)\n\nfor e in words:\n    counts[e]+=1\n\nprint(counts)\n\n# Use Counter class\nprint(Counter(words))","af326fd7":"Counter(words).most_common(2)","eb6be372":"import matplotlib.pyplot as plt\n%matplotlib inline\nword_freq = Counter(words)\n\nplt.bar(x=word_freq.keys(), height=word_freq.values())\nplt.show()","f79ac0b2":"from nltk.corpus import stopwords\n\nprint('text =',text)\n\nwords = word_tokenize(text.lower())\nprint('words =',words)\n\nwords = [e for e in words if e.isalpha()]\nprint('words =',words)\n\nwords = [e for e in words if e not in stopwords.words('english')]\nprint('words =',words)","01907b0d":"from gensim.corpora.dictionary import Dictionary\n\nmy_documents = ['The movie was about a spaceship and aliens.',\n'I really liked the movie!',\n'Awesome action scenes, but boring characters.',\n'The movie was awful! I hate alien films.',\n'Space is cool! I liked the movie.',\n'More space films, please!']\n\nsentences = [word_tokenize(e.lower()) for e in my_documents]\n\nsentences_noStop = []\nfor sent in sentences:\n    temp = []\n    for word in sent:\n        if word not in stopwords.words('english') and word.isalpha():\n            temp.append(word)\n    sentences_noStop.append(temp)\n\n# Build the dictionary\n# Gensim's dictionary can be updated\ndictionary = Dictionary(sentences_noStop)","5f80b44e":"sentences_noStop","bf07a6f5":"print('vocab : id')\nprint(dictionary.token2id)","8ddf28f5":"# Translate each sentence in sentences by dictionary just created \n# bow = bag-of-word\ncorpus = [dictionary.doc2bow(doc) for doc in sentences_noStop]\n\ncorpus","3d089c57":"from gensim.models.tfidfmodel import TfidfModel\n\ntfidf = TfidfModel(corpus)","4442a2e4":"print(corpus[1])\nprint(tfidf[corpus[1]])","7b5c7857":"import nltk\ntext = '''In New York, I like to ride the Metro to\nvisit MOMA and some restaurants rated\nwell by Ruth Reichl.\n'''\n\n# Use nltk.pos_tag()\nwords = word_tokenize(text)\npos_tagged = nltk.pos_tag(words)","94106ed3":"pos_tagged","5c9bea22":"print(nltk.ne_chunk(pos_tagged))","c4a0da77":"# Extract Entities\n\nfor e in nltk.ne_chunk(pos_tagged):\n    if hasattr(e, 'label'):\n        print(e)","be843427":"!python -m spacy download en","402bd4d6":"import spacy\n\nnlp = spacy.load('en_core_web_sm')\nnlp.entity","0ae68186":"doc = nlp('''So Sally can wait\nShe knows it's too late\nAs she's walking on by''')\n\nprint(doc.ents)\nprint(doc.ents[0].label_)","ea1ad7a0":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnews = pd.read_csv('\/kaggle\/input\/fake-news-or-real\/fake_or_real_news.csv')\nnews.head()","48b7ce2e":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(news[['title','text']], news[['label']], test_size=0.2, stratify=news[['label']])\n\nall_train = pd.concat([X_train.title, X_train.text], axis=0)","40060b57":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(stop_words='english').fit(all_train)\n\ntitle_train_vector = vectorizer.transform(X_train['title'])\ntext_train_vector = vectorizer.transform(X_train['text'])\ntrain_vector = np.concatenate([title_train_vector.A, text_train_vector.A], axis=1)\n\ntitle_test_vector = vectorizer.transform(X_test['title'])\ntext_test_vector = vectorizer.transform(X_test['text'])\ntest_vector = np.concatenate([title_test_vector.A, text_test_vector.A], axis=1)","db99316e":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\ncf = MultinomialNB()\ncf.fit(train_vector, y_train)","2a3df252":"y_pred = cf.predict(test_vector)\nmetrics.accuracy_score(y_test, y_pred)","3e84d1df":"print(metrics.confusion_matrix(y_test, y_pred))","ee155230":"print(metrics.classification_report(y_test, y_pred))","d722a8d0":"| := logical OR <br>\n() := group <br>\n[] := range <br>\n\n(\\d+|w+) := **\u0e43\u0e19 1 token** \u0e08\u0e30\u0e21\u0e35 digit 1,more \u0e15\u0e31\u0e27 OR \u0e21\u0e35 character 1,more \u0e15\u0e31\u0e27","4a4bb8fd":"We then need to remove unwanted tokens.","b1184d06":"## 3.1) Part-of-speech tagging","e35d93c8":"# Part 3 : Named Entity Recognition","8cbb55a1":"More example regex","e85d4c10":"### Gensim\n- Building document or word vectors\n- Performing topic identication and document comparison\n","d1e71f89":"## 1.2) Tokenization\n\nThe definition of \"token\" depends on the problem we're working on. The common definition is a word(1 token = 1 word). \n","e0259b64":"# Part 2 : Word counts, Bag-of-words","e4cd5820":"***SpaCy*** has different entity type compared to *nltk*. It also work for **informal language corpora** & quickly growing.","4879f77b":"\n\n## 1.1) Basic re function\n- re.match()  ->  search the regular expression pattern and ***return the first occurrence in the first line***. <br>\nIf a match is found in the first line, it returns the match object. But if a match is found in some other line, the Python RegEx Match function returns null.\n\n- re.search()  ->  search the regular expression pattern and ***return the first occurrence.***<br>\nIt will check all lines of the input string. re.search() function returns a match object when the pattern is found and \u201cnull\u201d if the pattern is not found\n\n- re.findall()  ->  ***search for \u201call\u201d occurrences*** that match a given pattern. <br>\nSearch() module will only return the first occurrence that matches the specified pattern. findall() will iterate over all the lines of the file and will return all non-overlapping matches of pattern in a single step.\n","a3228e6b":"## 2.1) Bag-of-words\n- Basic method for finding topics of text\n- Create token, count each token, the more frequent token the more important it might be.","ed2b2c63":"# Part 1 : Regex ","511d1cda":"# Part 4: Classifying fake news\n\n**Plan** <br>\n1. Collect data\n2. Split data -> train, test\n3. Extract the features from text (Bag-of-word)\n4. Evaluate using test set","baa26324":"We can access EntityReconizer via .entity attribute.","a05a7e8e":"## 3.2) SpaCy","54f24172":"### Ti-idf with Gensim\n\nTerm frequency - inverse document frequency <br>\n- Allows you to determine important words in each document.\n- Unimportant words are not just Stopwords. If a word appears in every document, it's unimportant(low variance).\n- These words should be down-weighted in importance\n- Keeps document-specic frequent words weighted high","4c3d3232":"## 2.2) Text-preprocessing steps\n- Tokenization\n- Lowercasing\n- Lemmatization\/ Stemming\n- Removing stop words, punctuation, or unwanted tokens"}}