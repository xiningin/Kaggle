{"cell_type":{"ca6424df":"code","12a715bf":"code","080446e6":"code","0e9f289b":"code","cc73ff86":"code","81664f32":"code","cba68670":"code","64bf9189":"code","ffa4b87f":"code","2edf275d":"code","e62fa233":"code","4966adfe":"code","21ae83c6":"code","6de860bb":"code","a7c25a37":"code","4a8ccf51":"code","c5b7fd83":"code","fba49a09":"code","07b0fc05":"code","026b4dd8":"code","f357d7b8":"code","3cc8dc9e":"code","c2b953b9":"code","dbe21243":"code","8b068b13":"code","94889c45":"code","574db3a9":"code","4f69f7f1":"code","488c5767":"code","cf1e2b9d":"markdown","2966fdc7":"markdown","976fd4b1":"markdown","0c84ffde":"markdown","c4a75b11":"markdown","1ddb3c66":"markdown","dd1fa7b3":"markdown","de394f65":"markdown","f0e143f3":"markdown","77b9eab5":"markdown","7f210743":"markdown","7fb3043c":"markdown","e2e5d581":"markdown","7e5f9f67":"markdown","71a5d818":"markdown","007fe488":"markdown","26a43766":"markdown"},"source":{"ca6424df":"import os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings('ignore')","12a715bf":"%matplotlib inline\nimport torch\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","080446e6":"dataset = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')","0e9f289b":"# display first 5 rows of dataset to see what we are looking at\ndataset.head()","cc73ff86":"# show the distributions of data \ndataset.describe()","81664f32":"# The serial No. adds nothing to predicting the chance of admittance\ndataset = dataset.drop('Serial No.',axis=1)","cba68670":"sns.pairplot(dataset,diag_kind='kde',plot_kws={'alpha': .2});","64bf9189":"sns.factorplot(y=\"Chance of Admit \",x='Research',data=dataset,kind='box');\ndataset.loc[:,['Research','Chance of Admit ']].groupby('Research').describe()","ffa4b87f":"sns.distplot(dataset[dataset.loc[:,'Research'] == 1].loc[:,['Chance of Admit ']],kde=True);\nsns.distplot(dataset[dataset.loc[:,'Research'] == 0].loc[:,['Chance of Admit ']],kde=True);\nplt.xlabel('Chance of Admittance')\nplt.ylabel('Count')\nplt.title('Research vs No Research');\nplt.legend(['Research','No Research']);","2edf275d":"sns.factorplot(x='University Rating',y='Chance of Admit ',kind='box',data=dataset);\nsns.factorplot(x='SOP',y='Chance of Admit ',kind='box',data=dataset);\nsns.factorplot(x='LOR ',y='Chance of Admit ',kind='box',data=dataset);","e62fa233":"target = dataset.pop('Chance of Admit ')","4966adfe":"# split data into train test \nX_train,X_test,y_train,y_test = train_test_split(dataset.values.astype(np.float32),\n                                                 target.values.reshape(-1,1).astype(np.float32),\n                                                 test_size=.2,\n                                                random_state=42)","21ae83c6":"# normalize data to 0 mean and unit std\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","6de860bb":"import skorch\nfrom skorch import NeuralNetRegressor\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","a7c25a37":"class MyModule(nn.Module):\n    def __init__(self,num_units=10,nonlin=F.relu,drop=.5):\n        super(MyModule,self).__init__()\n        \n        self.module = nn.Sequential(\n            nn.Linear(7,num_units),\n            nn.LeakyReLU(),\n            nn.Dropout(p=drop),\n            nn.Linear(num_units,1),\n        )\n        \n    def forward(self,X):\n        X = self.module(X)\n        return X","4a8ccf51":"net = NeuralNetRegressor(\n    MyModule,\n    criterion=nn.MSELoss,\n    max_epochs=10,\n    optimizer=optim.Adam,\n    optimizer__lr = .005\n)","c5b7fd83":"lr = (10**np.random.uniform(-5,-2.5,1000)).tolist()\nparams = {\n    'optimizer__lr': lr,\n    'max_epochs':[300,400,500],\n    'module__num_units': [14,20,28,36,42],\n    'module__drop' : [0,.1,.2,.3,.4]\n}\n\ngs = RandomizedSearchCV(net,params,refit=True,cv=3,scoring='neg_mean_squared_error',n_iter=100)","fba49a09":"%%capture\ngs.fit(X_train_scaled,y_train);","07b0fc05":"# Utility function to report best scores (found online)\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","026b4dd8":"# review top 10 results and parameters associated\nreport(gs.cv_results_,10)","f357d7b8":"# get training and validation loss\nepochs = [i for i in range(len(gs.best_estimator_.history))]\ntrain_loss = gs.best_estimator_.history[:,'train_loss']\nvalid_loss = gs.best_estimator_.history[:,'valid_loss']","3cc8dc9e":"plt.plot(epochs,train_loss,'g-');\nplt.plot(epochs,valid_loss,'r-');\nplt.title('Training Loss Curves');\nplt.xlabel('Epochs');\nplt.ylabel('Mean Squared Error');\nplt.legend(['Train','Validation']);","c2b953b9":"from sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score","dbe21243":"# predict on test data\ny_pred = gs.best_estimator_.predict(X_test_scaled.astype(np.float32))","8b068b13":"# get RMSE\nMSE(y_test,y_pred)**(1\/2)","94889c45":"sns.kdeplot(y_pred.squeeze(), label='estimate', shade=True)\nsns.kdeplot(y_test.squeeze(), label='true', shade=True)\nplt.xlabel('Admission');","574db3a9":"sns.distplot(y_test.squeeze()-y_pred.squeeze(),label='error');\nplt.xlabel('Admission Error');","4f69f7f1":"# show R^2 plot\nprint(r2_score(y_test,y_pred))\nplt.plot(y_pred,y_test,'g*')\nplt.xlabel('predicted')\nplt.ylabel('actual')\nplt.title('$R^{2}$ visual');\n","488c5767":"# show where the big errors were\nerrors = np.where(abs(y_test-y_pred)>.2)\nfor tup in zip(y_test[errors],y_pred[errors]):\n    print(tup)","cf1e2b9d":"# Dataset Analysis and Visualization","2966fdc7":"# Wrap Pytorch Neural Network in Skorch Wrapper","976fd4b1":"As seen by the box and whisker plots below, there does seems to be a positive linear correlation between **University Rating , SOP, and LOR** with **Chance of Admit**","0c84ffde":"# Display Learning curves to see if overfitting or underfitting data\n* By observing the learning curves, I can tell if the Neural Network overfitted or underfitted the data.\n* **Overfit** : if the training loss curve is significantly lower than the validation loss curve.\n* **Underfit**: if both the training loss curve and the validation loss curve are very high loss.\n* **Ideal**: both the training loss and validation loss curves have a minimal gap between them and converge to a very low loss.","c4a75b11":"# Randomized Hyperparameter Search\n* Search the hyperparameter space of learning rate, epochs, number of hidden units, and dropout rate in the hidden layer to find the optimal hyperparameter combination that will minimize the expected loss.\n* 3 fold cross validation is used to get the mean validation loss and mean training loss over the training dataset.","1ddb3c66":"The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are:  \n1. GRE Scores ( out of 340 )\n2. TOEFL Scores ( out of 120 )\n3. University Rating ( out of 5 ) \n4. Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n5. Undergraduate GPA ( out of 10 )\n6. Research Experience ( either 0 or 1 ) \n7. Chance of Admit ( ranging from 0 to 1 )","dd1fa7b3":"# Standard Normalization Preprocess\n* Standard normalization makes all the features have zero-mean and unit-variance. \n* The range of the raw values in the data vary widely, and in deep learning of the feature values are too large depending on the activation function of the neuron the neurons may become saturated (and not perform backpropogation to update the weights). If the feature values are too small, depending on the activation function of the neuron the neurons may output zero and be considered \"dead neurons\". These dead neurons will not train the model.","de394f65":"* The **GRE Score** seems to have a high mean (close to the max GRE score) with an extremely low standard deviation\n* The **TOEFL Score** seems to have a high mean (close to the max TOEFL Score) with an extremely low standard deviation\n * The **GRE** and **TOEFL** score makes sense, as top students apply for masters programs at UCLA\n* **SOP,LOR, and University rating** have more of a spread than other variables\n * This makes sense, as these are subjective aspects of the student \/ university\n* **CPGA** seems to have a high mean with low standard deviation\n* **Research** a little over 50% of applicants do research","f0e143f3":"The distribution for the students who participate in research have on average a higher chance of admittance than the students who do not participate in research, as shown by the histogram and box and whisker plots below. The mean chance of admit for students who participate is **.79**, whereas the mean chance of admit for students who do not partipate in research is **.63**","77b9eab5":"* The **$R^{2}$** plot should have a positive slope of 1. The best possibe score is 1, whereas the worst score is 0.\n* **$R^{2}$** is a goodness of fit measure, and says how well the regression model explains the variability of the dataset","7f210743":"# See Regression Metrics to evaluate on test dataset","7fb3043c":"**Root mean squared error** (RMSE) says how far from the regression line the data points are on average. A lower RMSE is better.","e2e5d581":"# Set up Neural Network\n* One Hidden Layer\n* The nonlinearity activation function of the neurons in the hidden layer is a LeakyReLU. This was done to avoid saturation for inputs to the activation function less than 0.\n* Dropout is employed as a regularization technique to prevent the Neural Network from overfitting to the data (the neural network is such a powerful model that it may find trends in the noise of the data and try to fit to the noise as well as the features of the dataset)\n* The loss function of the Neural Network is mean absolute error, as this is a common loss function employed in regression tasks.","7e5f9f67":"# Split into train\/test datasets\n* Split the dataset such that 80 percent of the data is the training set and 20 percent of the data is the test set. This step is done so that the when measuring the predictive power of the model, it is predicintg based on new data, and therefore the model is more generalizable (and not limited to predicting data only seen in the dataset).","71a5d818":"Displaying the pairwise plots of all the variables (including the target variable chance of admission) it is clear that there is a linear correlation between the target variable **Chance of Admittance** and the data features (maybe the relationship is not clear between the binary categorical feature **Research** and **Chance of Admittance**\n* By observing the pairplot, it may be see that there is a fairly strong linear correlation between many variables, which may suggest redundant information","007fe488":"Try to see how well the models predicted probability distribution overlaps with the actual probability distribution of the test set.","26a43766":"# Load Dataset"}}