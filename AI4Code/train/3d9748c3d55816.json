{"cell_type":{"210273f3":"code","aef1e695":"code","64cfa3ec":"code","f0a43b1d":"code","68772a9e":"code","674f1ebf":"code","6a751f92":"code","31e64f68":"code","6a5248cd":"code","ed4fbf0c":"code","899dc046":"code","44e7b6ae":"code","342f1408":"code","2b874b23":"code","5e794ad3":"code","628f0769":"code","346b198a":"code","3c9a8323":"code","f68a8e45":"code","fa67470b":"code","c3fd0160":"code","d0d78840":"code","f1b70116":"code","7d8f4606":"code","1f116294":"code","0c818697":"code","73fa4715":"code","303e69f4":"code","0fddb3c6":"code","a5529e6a":"code","c1bf5a89":"code","c8c99a01":"code","39c2db11":"code","bb939ea4":"code","9b90dd99":"code","f2a43b2a":"code","6a59f466":"code","bc4d720e":"code","75eca594":"code","a20003a9":"code","d92c03c7":"code","29f4f988":"code","a74864dd":"code","2c1ac372":"code","14f55be7":"code","d5ea4d93":"code","caa22587":"code","2b757a79":"code","4802d884":"code","d5da3ee2":"code","0db5056f":"code","d2ce279f":"code","0b4589c6":"code","c95d53a0":"code","07cd0a00":"code","6b7cc6dc":"code","ad162f0c":"code","fe30326e":"code","475cb42d":"code","daaaa5b0":"code","2b3ff353":"markdown","06b86e60":"markdown","ac3d013f":"markdown","59c30441":"markdown","81abbd0e":"markdown","0c6ce8b6":"markdown","feca3f23":"markdown","a927f16c":"markdown","c36a0e23":"markdown","70f8bd74":"markdown","14e0e82b":"markdown","8e4c6db5":"markdown","4fb8aac7":"markdown","c9d9b315":"markdown","11ae119c":"markdown","05088bdf":"markdown","5ff225df":"markdown","60b861ad":"markdown","a6bb5b04":"markdown","503aaf97":"markdown","658a147e":"markdown","f8b2f234":"markdown","5a2bfd5c":"markdown","ae20cdb8":"markdown","8edc4f02":"markdown","7d6d5f81":"markdown","24eb2971":"markdown","d4c6f275":"markdown","96b1dbff":"markdown"},"source":{"210273f3":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","aef1e695":"# Training data\napp_train = pd.read_csv('..\/input\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","64cfa3ec":"# Testing data features\napp_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","f0a43b1d":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","68772a9e":"# Missing values statistics\nmissing_values = missing_values_table(app_train)\nmissing_values.head(20)","674f1ebf":"# Create a label encoder object\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(app_train[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(app_train[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","6a751f92":"# one-hot encoding of categorical variables\napp_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","31e64f68":"train_labels = app_train[\"TARGET\"]","6a5248cd":"#aligning the two data frames to keep only the colkumns that are there in both the dataframes\napp_train,app_test= app_train.align(app_test,join='inner',axis=1)\n#adding back the target\napp_train[\"TARGET\"] = train_labels\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","ed4fbf0c":"(app_train['DAYS_BIRTH']\/-365).describe()","899dc046":"app_train['DAYS_EMPLOYED'].describe()","44e7b6ae":"app_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram');\nplt.xlabel('Days Employment');","342f1408":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","2b874b23":"#boolean flagging the anomaly of the \"number of employement days\"\napp_train['DAYS_EMPLOYED_ANOMALIES'] = app_train['DAYS_EMPLOYED']== 365243\n#now we replace the anomalies values with the NAN\napp_train['DAYS_EMPLOYED'].replace({365243:np.nan},inplace = True)\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'days employement histogram')\nplt.xlabel(\"Days of Employement\")\n","5e794ad3":"app_test['DAYS_EMPLOYED_ANOMALIES'] = app_test['DAYS_EMPLOYED']==365243\napp_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n\n#printing the total number of anomalies \nprint(\"There are %d number of anomalie in the total of %d entries\"%(app_test['DAYS_EMPLOYED_ANOMALIES'].sum(),len(app_test)))","628f0769":"correlation = app_train.corr()['TARGET']","346b198a":"correlation = correlation.sort_values()","3c9a8323":"#the columns with maximum correlation\ncorrelation.tail(15)","f68a8e45":"#the columns with the least correlation are\ncorrelation.head(15)","fa67470b":"#here we check the correlation after the values of the columns are changed to absolute values\napp_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])\napp_train['DAYS_BIRTH'].corr(app_train['TARGET'])","c3fd0160":"# Set the style of plots\n#plt.style.use('fivethirtyeight')\n\n# Plot the distribution of ages in years\nplt.hist(app_train['DAYS_BIRTH'] \/ 365, edgecolor = 'k', bins = 25)\nplt.title('Age of Client'); plt.xlabel('Age (years)'); plt.ylabel('Count');","d0d78840":"plt.figure(figsize=(10,8))\n#the loans that were paid on time\nsns.kdeplot(app_train.loc[app_train['TARGET'] == 0, 'DAYS_BIRTH']\/365,label='target=0')\n#the loans that were not paid on time\nsns.kdeplot(app_train.loc[app_train['TARGET']==1,'DAYS_BIRTH']\/365,label='target=1')\nplt.xlabel('Age(Year)'),plt.ylabel('Density'),plt.title('distribution of Ages')","f1b70116":"#we create a separate dataframe with the age information and the target\nage_data = app_train[['TARGET','DAYS_BIRTH']]\nage_data['YEAR_BIRTH'] = age_data['DAYS_BIRTH']\/365\n","7d8f4606":"# Bin the age data\n#we knew that there the range of age is between (20,70) and thus we have divided the age into bins and then the appropriate bin was assigned to each row\nage_data['YEARS_BINNED'] = pd.cut(age_data['YEAR_BIRTH'], bins = np.linspace(20, 70, num = 11))\nage_data.head(10)","1f116294":"age_data.head()","0c818697":"age_group = age_data.groupby(\"YEARS_BINNED\").mean()\nage_group","73fa4715":"plt.figure(figsize = (8, 8))\n\n# Graph the age bins and the average of the target as a bar plot\nplt.bar(age_group.index.astype(str), 100 * age_group['TARGET'])\n\n# Plot labeling\nplt.xticks(rotation = 75); plt.xlabel('Age Group (years)'); plt.ylabel('Failure to Repay (%)')\nplt.title('Failure to Repay by Age Group');","303e69f4":"external_data = app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','DAYS_BIRTH']]","0fddb3c6":"external_DataCorr = external_data.corr()","a5529e6a":"external_DataCorr","c1bf5a89":"plt.figure(figsize = (8, 6))\n\n# Heatmap of correlations\nsns.heatmap(external_DataCorr, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","c8c99a01":"plt.figure(figsize = (10, 12))\n\n# iterate through the sources\nfor i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n    \n    # create a new subplot for each source\n    plt.subplot(3, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % source)\n    plt.xlabel('%s' % source); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","39c2db11":"# Copy the data for plotting\nplot_data = external_data.drop(columns = ['DAYS_BIRTH']).copy()\n\n# Add in the age of the client in years\nplot_data['YEARS_BIRTH'] = age_data['YEAR_BIRTH']\n\n# Drop na values and limit to first 100000 rows\nplot_data = plot_data.dropna().loc[:100000, :]\n\n# Function to calculate correlation coefficient between two columns\ndef corr_func(x, y, **kwargs):\n    r = np.corrcoef(x, y)[0][1]\n    ax = plt.gca()\n    ax.annotate(\"r = {:.2f}\".format(r),\n                xy=(.2, .8), xycoords=ax.transAxes,\n                size = 20)\n\n# Create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 3, diag_sharey=False,\n                    hue = 'TARGET', \n                    vars = [x for x in list(plot_data.columns) if x != 'TARGET'])\n\n# Upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.2)\n\n# Diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# Bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n\nplt.suptitle('Ext Source and Age Features Pairs Plot', size = 32, y = 1.05);","bb939ea4":"## Make a new dataframe for polynomial features\npoly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\n\n# imputer for handling missing values\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\n\npoly_features = poly_features.drop(columns = ['TARGET'])\n\n# Need to impute missing values\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3)","9b90dd99":"#poly_features = imputer.fit_transform(poly_features)\n#the fit method calculates the internal parameters to be used for the transformation and saves as an internal object\n#the transformation transforms the values using the value calculated by the fit function\n#the fit_transform does the two things to a dataset at the same time\n#poly_features_test = imputer.transform(poly_features_test)\n","f2a43b2a":"# Train the polynomial features\npoly_transformer.fit(poly_features)\n\n# Transform the features\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","6a59f466":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]","bc4d720e":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Add in the target\npoly_features['TARGET'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\n# Display most negative and most positive\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","75eca594":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n# Merge polynomial features into training dataframe\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n\n# Align the dataframes\napp_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n\n# Print out the new shapes\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","a20003a9":"#using some of the domain knowledge\napp_train_domain =  app_train.copy()\napp_test_domain = app_test.copy()\n","d92c03c7":"app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_INCOME_TOTAL']\napp_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] \/ app_train_domain['AMT_CREDIT']\napp_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] \/ app_train_domain['DAYS_BIRTH']","29f4f988":"app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_INCOME_TOTAL']\napp_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] \/ app_test_domain['AMT_CREDIT']\napp_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] \/ app_test_domain['DAYS_BIRTH']","a74864dd":"plt.figure(figsize = (12, 20))\n# iterate through the new features\nfor i, feature in enumerate(['CREDIT_INCOME_PERCENT', 'ANNUITY_INCOME_PERCENT', 'CREDIT_TERM', 'DAYS_EMPLOYED_PERCENT']):\n    \n    # create a new subplot for each source\n    plt.subplot(4, 1, i + 1)\n    # plot repaid loans\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(app_train_domain.loc[app_train_domain['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","2c1ac372":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns = ['TARGET'])\nelse:\n    train = app_train.copy()","14f55be7":"# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))","d5ea4d93":"# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)","caa22587":"# Repeat with the scaler\n#the fit method calculates the minimum and maximum needed to compute the transformed value between 0 and 1\n#the transform function transforms the value using the calculated minimum and the maximum\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","2b757a79":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","4802d884":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","d5da3ee2":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","0db5056f":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","d2ce279f":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","0b4589c6":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","c95d53a0":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline.csv', index = False)","07cd0a00":"poly_features_names = list(app_train_poly.columns)\n\n# Impute the polynomial features\nimputer = Imputer(strategy = 'median')\n\npoly_features = imputer.fit_transform(app_train_poly)\npoly_features_test = imputer.transform(app_test_poly)\n\n# Scale the polynomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\npoly_features = scaler.fit_transform(poly_features)\npoly_features_test = scaler.transform(poly_features_test)\n\nrandom_forest_poly = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","6b7cc6dc":"app_train_domain = app_train_domain.drop(columns = 'TARGET')\n\ndomain_features_names = list(app_train_domain.columns)\n\n# Impute the domainnomial features\nimputer = Imputer(strategy = 'median')\n\ndomain_features = imputer.fit_transform(app_train_domain)\ndomain_features_test = imputer.transform(app_test_domain)\n\n# Scale the domainnomial features\nscaler = MinMaxScaler(feature_range = (0, 1))\n\ndomain_features = scaler.fit_transform(domain_features)\ndomain_features_test = scaler.transform(domain_features_test)\n\nrandom_forest_domain = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\n\n# Train on the training data\nrandom_forest_domain.fit(domain_features, train_labels)\n\n# Extract feature importances\nfeature_importance_values_domain = random_forest_domain.feature_importances_\nfeature_importances_domain = pd.DataFrame({'feature': domain_features_names, 'importance': feature_importance_values_domain})\n\n# Make predictions on the test data\npredictions = random_forest_domain.predict_proba(domain_features_test)[:, 1]","ad162f0c":"# Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\n\n# Save the submission dataframe\nsubmit.to_csv('random_forest_baseline_domain.csv', index = False)","fe30326e":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","475cb42d":"# Show the feature importances for the default features\nfeature_importances_sorted = plot_feature_importances(feature_importances)","daaaa5b0":"feature_importances_domain_sorted = plot_feature_importances(feature_importances_domain)","2b3ff353":"> now here we change the days of birth to the year ","06b86e60":"* .00-.19 \u201cvery weak\u201d\n*  .20-.39 \u201cweak\u201d\n*  .40-.59 \u201cmoderate\u201d\n*  .60-.79 \u201cstrong\u201d\n* .80-1.0 \u201cvery strong\u201d\n","ac3d013f":"## This notebook concludes thanks @willkoehrsen","59c30441":"> we have to do the necessary changes to the test data also","81abbd0e":"## Exploratory data analysis","0c6ce8b6":">here we can very well see that the younger clients tend to have more delay as compared to the  older clients, the younger clients should be given the loan with more precaution","feca3f23":"## the correlation between the DAYS_Of_Birth","a927f16c":">it can be seen that as the value of the value of the EXT_SOURCE 1,2 and 3 will increase the clients are more likely to repay the loan","c36a0e23":">we use the scikit learn labelencoder for label encoding and the pandas dummy variable for the one hot label encoding.\n","70f8bd74":">while xtreme gradient boosting machine takes care of the null values the lgbm takes care of the categorical features\n>the approach to encode the categorical features is either label encoding where each unique label is assigned a unique integer but then we can also use the one hot encoding where each unique label is assigne a column and then the value 1 and 0 is used to indicate its presence and absence.\n>for the label encoding if there are only two categories then it is safe to use it but when there are more than two categories then the model may assign arbitrary weight to the categories which we dont want hence, we prefer the one hot encoding for more than one unique labels","14e0e82b":"## finding the anomalies in the *DAYS_EMPLOYED* column","8e4c6db5":"## now we have some external data source which in some sense is the measure of the credit","4fb8aac7":">now here we plot the age bracketwise distribution of the loan repayment and if any delay in loan repayments","c9d9b315":"## as the client grows older he tends to pay the loan more timely","11ae119c":"> an approach to flag the anomalies","05088bdf":">we can see that here the plot of target==1 is a bit skewed towards the younger age, though it does not tells us much about the correlation(-0.07) but we know that it does affects the target value in some way so now we further explore the variable\n\n>now we will look at the failure to pay the loan by the age bracket\n\n>to plot this graph we divide the age category into the bins of 5 years each and then we calculate the average payment by each group and then it in turn gives the ratio of the loan that was not paid by each category","5ff225df":">the histogram does not give us much detail about the distribution of the age of the client other than the obeservation that there are no outliners\n\n>now we observe the KDE plot it is nothing but the histogram that had been smoothed it is usually calculated by finding the gaussian kernel at every point and then smoothing them to get the plot","60b861ad":"> Though the correlation is not the right measure of the dependence between the variable and the target, it does gives the idea about what is effectively more important than the other.","a6bb5b04":"## Using the engineered feature","503aaf97":">completed this notebook  4 Aug '18 :)\n\n>[Reference1](https:\/\/www.youtube.com\/watch?v=OAl6eAyP-yo)\n\n>[Reference2](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction)","658a147e":">we explore the correlation between different columns of the external data sources","f8b2f234":"> in the earlier notebook we have already found the number of unique labels and the datatype of each columns","5a2bfd5c":"## The only way to say if some faeture will work is to try them ","ae20cdb8":"## so now we have some features, we will try and see if these features will improve the model","8edc4f02":"> now we have equal number of features in the training dataframe as well as the test dataframe","7d6d5f81":">in case of the exploding number or the number of features after one hot coding we can use the PCA to or any other dimension reduction technique to to reduce the number of columns\n>here we will use the feature encoding for only two unique labels and then one hot encoding for more than two unique labels","24eb2971":"## Random forest","d4c6f275":"## the evaluation metric","96b1dbff":"- ROC curve, plot of *True positive rate* on Y axis versus *False positive rate*, true positive means how many times the classifier predicted positive when the actual value was positive, the false positive means hown many times the actual result is negative when we predicted the positive.\n\n- Both the true positive rate and false positive rate range from 0 to 1\n\n- to generate the entire ROC curve we have to plot the True positive rates versus false positive rate for the entire range of Threshold from 1 to 0\n\n- ROC curve visualizes the misclassification for every threshold while the singlew misclassification metric evaluates the misclassification for a single threshold\n\n- The thresholds used to generate the ROC are not visible on the ROC curve itself\n\n- A better classifier has the ROC curve on the far upper left corner away from the line (x=y, the random classifier) where as the poor classifier has the ROC curve close to the line (x=y,representing the random classifier) .\n\n- thus for the ease of a single valued evaluation metric we consider the area under the AUC curve ,it follows from the above stated point that the good classifier has area more close to 1 (e.g. 0.8,0.9..or so) while a poor classifier has the area close to 0.5 (e.g. 0.55,0.58,0.6..or so).\n\n- even when the classes are not balanced (as in the case of this competition when the two classes are divided as 92% and 8%) the ROC gives the correct measure of misclassification.\n\n- the ROC curve gives the correct measure even when the range of the probabilities predicted is on different scale (0.9 to 1 or some other scale) other than 0 to 1 as long as the ordering of the classification is preserved. Meaning that the ROC curve is only sensitive to rank ordering\n\n- ROC AU is the probability that the classifier will rank the randomly chosen positive obeservation higher than the randomly chosen negative observation.\n\n- ROC curve can be used to represent more than two classes using the 1 versus all approach.\n\n    - that is if we have 3 classes then we can have 3 ROC curves showing 1 vs (2 and 3), 2 vs (1and 3), 3 vs (1 and 2)\n    \n- Choosing the threshold based on the ROC curve is a business decision ( whether we want to Maximize true positive or we want to minimise false positive)\n\n    - e.g. If we want to flag a fraudulant credit card transaction then we might want to maximize the False positive so as to avoid missing even a single case of fraudulant transaction."}}