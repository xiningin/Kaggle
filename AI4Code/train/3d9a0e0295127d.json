{"cell_type":{"1d51d52d":"code","785af922":"code","39b8fded":"code","4ef688ae":"code","651d6b67":"code","2a870088":"code","1964a155":"code","e68ff1c9":"code","35727368":"code","52b4d0d9":"code","72bb1bf3":"code","7801f566":"code","6afe18ba":"code","e4ff4448":"code","22fec2d7":"code","29581a2d":"code","e619efab":"code","0eeeef08":"code","129b6b20":"code","8a810872":"code","ce9fe759":"code","f5d4db06":"code","8ff87afb":"code","e3ae3f2d":"code","c5615518":"code","1ce9c585":"code","043c1b4a":"code","f3d5bd8e":"code","336bad23":"code","cd8acbc6":"code","8738d35f":"code","925b947e":"code","147b558d":"code","0d7da293":"code","964916cf":"code","8a435c0e":"code","ca24fc16":"markdown"},"source":{"1d51d52d":"# !pip install torchsummary\n# !pip install --upgrade efficientnet-pytorch","785af922":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport gc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.utils.data as data_utils\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torch.optim import lr_scheduler\nfrom torch import nn\n\n# from torchsummary import summary\nfrom torchvision import transforms,models\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom torch import Tensor\n\n# from efficientnet_pytorch import EfficientNet\nfrom collections import OrderedDict\n\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","39b8fded":"full_train_df = pd.read_feather('\/kaggle\/input\/full-bengali-graphemes-normalized\/full_train_df.feather')\ntarget_cols = ['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']\nX_train = full_train_df.drop(target_cols, axis=1)\nY_train = full_train_df[target_cols]","4ef688ae":"del full_train_df\ngc.collect()","651d6b67":"X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.002, random_state=666)\ngc.collect()","2a870088":"IMG_SIZE = 64\nCHANNELS = 1\nW, H = IMG_SIZE, IMG_SIZE\nBATCH_SIZE=512","1964a155":"# Convert to PyTorch tensors\nX_train = torch.from_numpy(X_train.values.reshape(-1, CHANNELS, IMG_SIZE, IMG_SIZE))\nX_val = torch.from_numpy(X_val.values.reshape(-1, CHANNELS, IMG_SIZE, IMG_SIZE))\nY_train = torch.from_numpy(Y_train.values)\nY_val = torch.from_numpy(Y_val.values)","e68ff1c9":"print(f'Size of X_train: {X_train.shape}')\nprint(f'Size of Y_train: {Y_train.shape}')\nprint(f'Size of X_val: {X_val.shape}')\nprint(f'Size of Y_val: {Y_val.shape}')","35727368":"# Visualize few samples of training dataset\nfig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\ncount=0\nfor row in ax:\n    for col in row:\n        col.imshow(X_train[count].reshape(IMG_SIZE, IMG_SIZE).cpu().detach().numpy().astype(np.float64))\n        col.set_title(str(Y_train[count].cpu().detach().numpy()))\n        count += 1\nplt.show()","52b4d0d9":"class GraphemesDataset(Dataset):\n    \"\"\"\n    Custom Graphemes dataset\n    \"\"\"\n    def __init__(self, X, Y):\n        self.X = X\n        self.Y = Y\n    \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, index):\n        return self.X[index], self.Y[index]","72bb1bf3":"train_dataset = GraphemesDataset(X_train, Y_train)\ntrain_loader = data_utils.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","7801f566":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","6afe18ba":"class _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n","e4ff4448":"class _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                                           growth_rate, kernel_size=1, stride=1,\n                                           bias=False)),\n        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),\n        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                           kernel_size=3, stride=1, padding=1,\n                                           bias=False)),\n        self.drop_rate = float(drop_rate)\n        self.memory_efficient = memory_efficient\n\n    def bn_function(self, inputs):\n        # type: (List[Tensor]) -> Tensor\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = self.conv1(self.relu1(self.norm1(concated_features)))  # noqa: T484\n        return bottleneck_output\n\n    # todo: rewrite when torchscript supports any\n    def any_requires_grad(self, input):\n        # type: (List[Tensor]) -> bool\n        for tensor in input:\n            if tensor.requires_grad:\n                return True\n        return False\n\n    @torch.jit.unused  # noqa: T484\n    def call_checkpoint_bottleneck(self, input):\n        # type: (List[Tensor]) -> Tensor\n        def closure(*inputs):\n            return self.bn_function(*inputs)\n\n        return cp.checkpoint(closure, input)\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, input):\n        # type: (List[Tensor]) -> (Tensor)\n        pass\n\n    @torch.jit._overload_method  # noqa: F811\n    def forward(self, input):\n        # type: (Tensor) -> (Tensor)\n        pass\n\n    # torchscript does not yet support *args, so we overload method\n    # allowing it to take either a List[Tensor] or single Tensor\n    def forward(self, input):  # noqa: F811\n        if isinstance(input, Tensor):\n            prev_features = [input]\n        else:\n            prev_features = input\n\n        if self.memory_efficient and self.any_requires_grad(prev_features):\n            if torch.jit.is_scripting():\n                raise Exception(\"Memory Efficient not supported in JIT\")\n\n            bottleneck_output = self.call_checkpoint_bottleneck(prev_features)\n        else:\n            bottleneck_output = self.bn_function(prev_features)\n\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate,\n                                     training=self.training)\n        return new_features","22fec2d7":"class _DenseBlock(nn.ModuleDict):\n    _version = 2\n\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n            )\n            self.add_module('denselayer%d' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.items():\n            new_features = layer(features)\n            features.append(new_features)\n        return torch.cat(features, 1)","29581a2d":"class DenseNet(nn.Module):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" <https:\/\/arxiv.org\/pdf\/1608.06993.pdf>`_\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `\"paper\" <https:\/\/arxiv.org\/pdf\/1707.06990.pdf>`_\n    \"\"\"\n\n    __constants__ = ['features']\n\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000, memory_efficient=False):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(num_init_features)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient\n            )\n            self.features.add_module('denseblock%d' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features \/\/ 2)\n                self.features.add_module('transition%d' % (i + 1), trans)\n                num_features = num_features \/\/ 2\n\n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        out = torch.flatten(out, 1)\n        out = self.classifier(out)\n        return out","e619efab":"def _densenet(growth_rate, block_config, num_init_features, **kwargs):\n    return DenseNet(growth_rate, block_config, num_init_features, **kwargs)","0eeeef08":"def densenet121(**kwargs):\n    r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https:\/\/arxiv.org\/pdf\/1608.06993.pdf>`_\n    \"\"\"\n    return _densenet(32, (6, 12, 24, 16), 64, **kwargs)","129b6b20":"class DenseNet121Wrapper(nn.Module):\n    def __init__(self):\n        super(DenseNet121Wrapper, self).__init__()\n        \n        # Load imagenet pre-trained model \n        self.effNet = densenet121()\n        \n        # Appdend output layers based on our date\n        self.fc_root = nn.Linear(in_features=1000, out_features=168)\n        self.fc_vowel = nn.Linear(in_features=1000, out_features=11)\n        self.fc_consonant = nn.Linear(in_features=1000, out_features=7)\n        \n    def forward(self, X):\n        output = self.effNet(X)\n        output_root = self.fc_root(output)\n        output_vowel = self.fc_vowel(output)\n        output_consonant = self.fc_consonant(output)\n        \n        return output_root, output_vowel, output_consonant","8a810872":"model = DenseNet121Wrapper().to(device)\n\n# Print summary of our model\n# summary(model, input_size=(CHANNELS, IMG_SIZE, IMG_SIZE))","ce9fe759":"torch.cuda.empty_cache()","f5d4db06":"LEARNING_RATE = 0.02\nEPOCHS = 100\nCUTMIX_ALPHA = 1","8ff87afb":"model = nn.DataParallel(model)\noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0.00001, eps=1e-08)\ncriterion = nn.CrossEntropyLoss()","e3ae3f2d":"def get_accuracy(root_preds, target_root, vowel_pred, target_vowel, consonant_pred, target_consonant):\n    assert len(root_preds) == len(target_root) and len(vowel_pred) == len(target_vowel) and len(consonant_pred) == len(target_consonant)\n    \n    total = len(target_root) + len(target_vowel) + len(target_consonant)\n    _, predicted_root = torch.max(root_preds.data, axis=1)\n    _, predicted_vowel = torch.max(vowel_pred.data, axis=1)\n    _, predicted_consonant = torch.max(consonant_pred.data, axis=1)\n    \n    del root_preds\n    del vowel_pred\n    del consonant_pred\n    torch.cuda.empty_cache()\n\n    correct = (predicted_root == target_root).sum().item() + (predicted_vowel == target_vowel).sum().item() + (predicted_consonant == target_consonant).sum().item()\n    \n    del target_root\n    del target_vowel\n    del target_consonant\n    torch.cuda.empty_cache()\n    return correct \/ total","c5615518":"def shuffle_minibatch(x, y):\n    assert x.size(0)== y.size(0) # Size should be equal\n    indices = torch.randperm(x.size(0))\n    return x[indices], y[indices]","1ce9c585":"del X_train\ndel Y_train\ngc.collect()","043c1b4a":"def clear_cache():\n    gc.collect()\n    torch.cuda.empty_cache()","f3d5bd8e":"X_val = X_val.to(device)\nY_val = Y_val.to(device)\n\n# Split validation's Y into 3 separate targets\ntarget_val_root, target_val_vowel, target_val_consonant = Y_val[:, 0], Y_val[:, 1], Y_val[:, 2]\ndel Y_val\nclear_cache()","336bad23":"def rand_bbox(size, lam):\n    W = size[2]\n    H = size[3]\n    cut_rat = np.sqrt(1. - lam)\n    cut_w = np.int(W * cut_rat)\n    cut_h = np.int(H * cut_rat)\n\n    # uniform\n    cx = np.random.randint(W)\n    cy = np.random.randint(H)\n\n    bbx1 = np.clip(cx - cut_w \/\/ 2, 0, W)\n    bby1 = np.clip(cy - cut_h \/\/ 2, 0, H)\n    bbx2 = np.clip(cx + cut_w \/\/ 2, 0, W)\n    bby2 = np.clip(cy + cut_h \/\/ 2, 0, H)\n\n    return bbx1, bby1, bbx2, bby2","cd8acbc6":"def cutmix(data, targets1, targets2, targets3, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets1 = targets1[indices]\n    shuffled_targets2 = targets2[indices]\n    shuffled_targets3 = targets3[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n    # adjust lambda to exactly match pixel ratio\n    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) \/ (data.size()[-1] * data.size()[-2]))\n\n    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n    return data, targets","8738d35f":"def mixup(data, targets1, targets2, targets3, alpha):\n    indices = torch.randperm(data.size(0))\n    shuffled_data = data[indices]\n    shuffled_targets1 = targets1[indices]\n    shuffled_targets2 = targets2[indices]\n    shuffled_targets3 = targets3[indices]\n\n    lam = np.random.beta(alpha, alpha)\n    data = data * lam + shuffled_data * (1 - lam)\n    targets = [targets1, shuffled_targets1, targets2, shuffled_targets2, targets3, shuffled_targets3, lam]\n\n    return data, targets","925b947e":"def cutmix_criterion(preds1,preds2,preds3, targets):\n    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n    criterion = nn.CrossEntropyLoss(reduction='mean')\n    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) + lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) + lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)","147b558d":"def mixup_criterion(preds1,preds2,preds3, targets):\n    targets1, targets2,targets3, targets4,targets5, targets6, lam = targets[0], targets[1], targets[2], targets[3], targets[4], targets[5], targets[6]\n    criterion = nn.CrossEntropyLoss(reduction='mean')\n    return lam * criterion(preds1, targets1) + (1 - lam) * criterion(preds1, targets2) + lam * criterion(preds2, targets3) + (1 - lam) * criterion(preds2, targets4) + lam * criterion(preds3, targets5) + (1 - lam) * criterion(preds3, targets6)","0d7da293":"total_steps = len(train_loader)\nval_acc_list = []\nis_mixup, is_cutmix = True, True\nfor epoch in range(EPOCHS):\n    for i, (x_train, y_train) in tqdm(enumerate(train_loader), total=total_steps):\n        x_train = x_train.to(device)\n        target_root = y_train[:, 0].to(device, dtype=torch.long)\n        target_vowel = y_train[:, 1].to(device, dtype=torch.long)\n        target_consonant = y_train[:, 2].to(device, dtype=torch.long)\n        \n        if np.random.rand()<0.6:\n            images, targets = mixup(x_train, target_root, target_vowel, target_consonant, CUTMIX_ALPHA)\n            if is_mixup:\n                # Visualize few samples of training dataset\n                fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n                count=0\n                for row in ax:\n                    for col in row:\n                        col.imshow(images[count].reshape(IMG_SIZE, IMG_SIZE).cpu().detach().numpy().astype(np.float64))\n                        count += 1\n                plt.show()\n                is_mixup = False\n            # Forward pass\n            root_preds, vowel_pred, consonant_pred = model(images)\n            loss = mixup_criterion(root_preds, vowel_pred, consonant_pred, targets) \n        else:\n            images, targets = cutmix(x_train, target_root, target_vowel, target_consonant, CUTMIX_ALPHA)\n            if is_cutmix:\n                # Visualize few samples of training dataset\n                fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n                count=0\n                for row in ax:\n                    for col in row:\n                        col.imshow(images[count].reshape(IMG_SIZE, IMG_SIZE).cpu().detach().numpy().astype(np.float64))\n                        count += 1\n                plt.show()\n                is_cutmix = False\n            # Forward pass\n            root_preds, vowel_pred, consonant_pred = model(images)\n            loss = cutmix_criterion(root_preds, vowel_pred, consonant_pred, targets)\n        \n        del x_train\n        clear_cache()\n        \n        # Backpropagate\n        optimizer.zero_grad()  # Reason: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch\n        loss.backward()\n        optimizer.step()\n    \n    lr_scheduler.step(loss.item())\n    del root_preds\n    del target_root\n    del vowel_pred\n    del target_vowel\n    del consonant_pred\n    del target_consonant\n    clear_cache()\n\n    # Calculate validation accuracy after each epoch\n    # Predict on validation set\n    root_val_preds, vowel_val_pred, consonant_val_pred = model(X_val)\n\n    val_acc = get_accuracy(root_val_preds, target_val_root, vowel_val_pred, target_val_vowel, consonant_val_pred, target_val_consonant)\n    val_acc_list.append(val_acc)\n\n    del root_val_preds\n    del vowel_val_pred\n    del consonant_val_pred\n    clear_cache()\n\n    print('Epoch [{}\/{}], Loss: {:.4f}, Validation accuracy: {:.2f}%'\n          .format(epoch + 1, EPOCHS, loss.item(), val_acc * 100))","964916cf":"plt.style.use('ggplot')\nplt.figure()\nplt.plot(np.arange(0, EPOCHS), val_acc_list, label='val_accuracy')\n\nplt.title('Accuracy')\nplt.xlabel('# of epochs')\nplt.ylabel('Accuracy')\nplt.legend(loc='upper right')\nplt.show()","8a435c0e":"torch.save(model.state_dict(), '100epochs_densenet121_lr.pth')","ca24fc16":"## Creating custom PyTorch data generator and Data Loader"}}