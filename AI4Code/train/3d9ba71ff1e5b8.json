{"cell_type":{"54af3c42":"code","cbfae9de":"code","b55c8dad":"code","6648000f":"code","b4a6f824":"code","42275d21":"code","21ed3b68":"code","64e07f48":"code","09366bd6":"markdown","9d1ae0fb":"markdown","4718ba49":"markdown","a71cfce6":"markdown","37bd4088":"markdown"},"source":{"54af3c42":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as lines\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nplt.rcParams['font.family'] = 'serif'\n\ncmap = sns.color_palette(\"ch:start=.2,rot=-.3\")\nsns.set_palette(cmap)","cbfae9de":"%%time\n# read dataframe\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","b55c8dad":"# prepare dataframe for modeling\nX = df_train.drop(columns=['id','claim']).copy()\ny = df_train['claim'].copy()\n\ntest_data = df_test.drop(columns=['id']).copy()","6648000f":"# feature Engineering\ndef get_stats_per_row(data):\n    data['mv_row'] = data.isna().sum(axis=1)\n    data['min_row'] = data.min(axis=1)\n    data['std_row'] = data.std(axis=1)\n    return data\n\nX = get_stats_per_row(X)\ntest_data = get_stats_per_row(test_data)","b4a6f824":"# create preprocessing pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\npipeline = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\n\nX = pd.DataFrame(columns=X.columns, data=pipeline.fit_transform(X))\ntest_data = pd.DataFrame(columns=test_data.columns, data=pipeline.transform(test_data))","42275d21":"# params from optuna study, i've done earlier\nbest_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}","21ed3b68":"%%time\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_curve, auc\nfrom catboost import CatBoostClassifier\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\npred_tmp = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = CatBoostClassifier(**best_params)\n    model.fit(X_train, y_train)\n\n    # validation prediction\n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('::'*20)\n    \n    # test prediction\n    y_hat = model.predict_proba(test_data)[:,1]\n    pred_tmp.append(y_hat)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","64e07f48":"# average predictions over all folds\npredictions = np.mean(np.column_stack(pred_tmp),axis=1)\n\n# create submission file\nsample_submission['claim'] = predictions\nsample_submission.to_csv('.\/catb_baseline.csv', index=False)","09366bd6":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Modeling<\/div>","9d1ae0fb":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Preprocessing<\/div>","4718ba49":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Introduction<\/div>","a71cfce6":"## <div style='background:#2b6684;color:white;padding:0.5em;border-radius:0.2em'>Import Data<\/div>","37bd4088":"**Hi**,<br>\nthis is my current solution - just a simple catboost classifier.<br>\nThe most important part here is feature engineering, where I'm calculating the sum of missing values per row.<br><br>\n**If you like this notebook or copy some parts of it, please leave an upvote.**<br>\n\nBest Regards."}}