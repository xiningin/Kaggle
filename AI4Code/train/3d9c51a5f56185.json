{"cell_type":{"a810cc7d":"code","d710fc7d":"code","d3981670":"code","9c52fd59":"code","b6dd7050":"code","d4f1018e":"code","269041f5":"code","e65eae1a":"code","18facf9b":"code","d934bb60":"code","c33b2dcd":"markdown","d28ca5f7":"markdown","740b3dc3":"markdown"},"source":{"a810cc7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d710fc7d":"def labelencoder(df, partitions):\n  from sklearn.preprocessing import LabelEncoder\n  for col in partitions:\n    print(col)\n    le = LabelEncoder()\n    k = df.pop(col)\n    le.fit(k)\n    k = pd.DataFrame(le.transform(k))\n    df = pd.concat([df, k] , axis=1)\n  return df","d3981670":"import pandas as pd\nX = pd.read_csv('..\/input\/lstm-sensor\/sensor.csv')\n\nX =  X.drop(['Unnamed: 0', 'sensor_50', 'sensor_15'], axis=1)\nX = labelencoder(X, ['machine_status'])\nX","9c52fd59":"#without dropna\nfrom datetime import *\nX['timestamp'] = X['timestamp'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\nX['timestamp'][0]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.lineplot(x=X['timestamp'], y=X[0])","b6dd7050":"def labelencoder(df, partitions):\n  from sklearn.preprocessing import LabelEncoder\n  for col in partitions:\n    print(col)\n    le = LabelEncoder()\n    k = df.pop(col)\n    le.fit(k)\n    k = pd.DataFrame(le.transform(k))\n    df = pd.concat([df, k] , axis=1)\n  return df","d4f1018e":"#preprocessing\nimport pandas as pd\nX = pd.read_csv('..\/input\/lstm-sensor\/sensor.csv')\n\nX =  X.drop(['Unnamed: 0', 'timestamp', 'sensor_50', 'sensor_15'], axis=1)\nX = labelencoder(X, ['machine_status'])\n\n#filling Nan\nX.interpolate(method='linear', inplace=True)\nX\n\n#making backup\nK = X.copy()\nX","269041f5":"def scale(partitions, scaler='MinMaxScaler', df=pd.DataFrame(), to_float=False, return_df=False):\n  #return_df == True, allora output = scaler, df\n  #return_df == False, allora output = df\n  from sklearn.preprocessing import RobustScaler\n  from sklearn.preprocessing import MinMaxScaler\n  from sklearn.preprocessing import StandardScaler\n  \n  if scaler == 'RobustScaler':\n    f_transformer = RobustScaler()\n  elif scaler == 'MinMaxScaler':\n    f_transformer = MinMaxScaler(feature_range=(0, 1))\n  elif scaler == 'StandardScaler':\n    f_transformer = StandardScaler()\n  \n  #partitions = 'all_df', le fa tutte insieme e trasforma il df in un numpy.array\n  if partitions == 'all_df':\n    if to_float == True:\n      df = df.astype('float32')\n    if df.empty == True:\n      X = df.copy()\n    #tutto df deve essere con float32\n    df_col = df.columns\n    df = f_transformer.fit_transform(df.values) #ne esce un inspiegabile numpy array\n    df = pd.DataFrame(df)\n    df.columns = df_col\n    if return_df == True:\n      return f_transformer, df\n    else:\n      X = df.copy()\n    return f_transformer\n  else:\n    #partitions = ['col1', 'col2'], fa solo partizioni specificate\n    pass\n\ndef transform_to_stationary(df):\n  #create a differenced series\n  def difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n      value = dataset[i] - dataset[i - interval]\n      diff.append(value)\n    return pd.DataFrame(diff)\n  \n  df = df.values #al di fuori delle funzioni voglio operare solo su un DataFrame\n  df = difference(df, 1) #X ritorna ad essere un df\n  return df\n\n# convert series to supervised learning\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True, drop_col=False, y_var=1):\n  n_features = int(len(data.columns))\n  n_vars = 1 if type(data) is list else data.shape[1]\n  df = pd.DataFrame(data)\n  cols, names = list(), list()\n  # input sequence (t-n, ... t-1)\n  for i in range(n_in, 0, -1):\n    cols.append(df.shift(i))\n    names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n  # forecast sequence (t, t+1, ... t+n)\n  for i in range(0, n_out):\n    cols.append(df.shift(-i))\n    if i == 0:\n      names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n    else:\n      names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n  # put it all together\n  agg = pd.concat(cols, axis=1)\n  agg.columns = names\n  # drop rows with NaN values\n  if dropnan:\n    agg.dropna(inplace=True)\n    data = agg.copy()\n    \n  if drop_col == True:\n    tot = n_features*n_in+n_features #24+8 = 32\n\n    y_name = list(data.columns)[n_features*n_in-1 + y_var]\n    y = data[y_name]\n    for i in range(n_features*n_in, tot):\n      data.drop(data.columns[[tot-n_features]], axis=1, inplace=True)\n    data = pd.concat([data, y], axis=1)\n  return data\n\ndef split(df, test_size):\n  df = df.values\n  len_df = df.shape[0]\n  test_size = int(len_df*test_size)\n  train, test = df[0:-test_size], df[-test_size:]\n  return train, test","e65eae1a":"#retrieve backup\nX = K.copy()\n\n#   scaling, no stationary\nraw_values = X.copy().values\nscaler, X = scale('all_df', scaler='MinMaxScaler', df=X, to_float=True, return_df=True)\n#X = transform_to_stationary(X)\nX = series_to_supervised(X, 10, 1, drop_col=False)\n\n#X, y\ny = pd.DataFrame(X.pop('var51(t)'))\nvar_list = ['var'+str(x)+'(t)' for x in range(1, 51)]\nX = X.drop(var_list, axis=1)\n\n#train, test\nX_train_, X_test_ = split(X, .1)\ny_train_, y_test_ = split(y, .1) #sembra non servire a nulla\nprint(X_train_.shape, X_test_.shape, y_train_.shape, y_test_.shape)\nX","18facf9b":"#retrieve backup\nX = K.copy()\n\n#   scaling, stationary\nraw_values = X.copy().values\nscaler, X = scale('all_df', scaler='MinMaxScaler', df=X, to_float=True, return_df=True)\nX = transform_to_stationary(X)\nX = series_to_supervised(X, 10, 1, drop_col=False)\n\n#X, y\ny = pd.DataFrame(X.pop('var51(t)'))\nvar_list = ['var'+str(x)+'(t)' for x in range(1, 51)]\nX = X.drop(var_list, axis=1)\n\n#train, test\nX_train, X_test = split(X, .1)\ny_train, y_test = split(y, .1) #sembra non servire a nulla\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\nX","d934bb60":"#reshape [samples, n_input_timesteps, n_features]\nX_train = X_train.reshape((198279, 510, 1)) #che e.X_ e e.X abbiano eguali righe o una in pi\u00f9 \u00e8 irrilevante, non ci sconvolgiamo per questo\ny_train = y_train.reshape((198279, 1, 1))\nprint(X_train.shape, y_train.shape)\n#ogni singolo sample ha dimensioni [1, 6, 1]\n\n#LSTM\n%tensorflow_version 2.x\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(LSTM(10, activation='relu', batch_input_shape=(1, 510, 1)))\nmodel.add(RepeatVector(1)) #numero di output\nmodel.add(LSTM(10, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(1)))\nmodel.compile(loss='mse', optimizer='adam')\n#model.compile(optimizer='adam', loss='mse')\n\nmodel.fit(X_train, y_train, epochs=3, batch_size=1, verbose=2, shuffle=False)\nmodel.reset_states()\n\nX_test = X_test.reshape(22030, 510, 1)\ny_test = y_test.reshape(22030, 1, 1)\nprint(X_test.shape, y_test.shape)","c33b2dcd":"Graphing","d28ca5f7":"Preprocessing","740b3dc3":"To Supervised"}}