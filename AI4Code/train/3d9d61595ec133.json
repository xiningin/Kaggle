{"cell_type":{"870c270a":"code","0bd86807":"code","917b2cbd":"code","2f8b1e35":"code","7fc621c0":"code","c4e98c16":"code","c7a135f4":"markdown"},"source":{"870c270a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\n\n!pip install imblearn\nfrom imblearn.over_sampling import SMOTE\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        inp= os.path.join(dirname, filename)\n\n# Any results you write to the current directory are saved as output.\n\ninp_file = pd.read_csv(inp, sep=';')\n","0bd86807":"for i in ['job', 'marital', 'education', 'contact']:\n    plt.figure(figsize=(10,4))\n    sns.countplot(x=i,hue='y', data=inp_file)\n    \ncorr = inp_file.corr()\nplt.figure(figsize=(20,10))\nsns.heatmap(corr, annot=True)","917b2cbd":"inp_file = pd.get_dummies(inp_file, columns=['job', 'marital', 'education', 'default', 'housing', 'loan',\n       'contact', 'month', 'day_of_week', 'poutcome'], drop_first=True)\nlabels = inp_file['y'].unique().tolist()\nmapping = dict( zip(labels,range(len(labels))) )\ninp_file.replace({'y': mapping},inplace=True)\n\n#Train basic logistic regression model.\n\ntrain, test = train_test_split(inp_file, test_size=0.2, random_state=0, \n                               stratify=inp_file['y'])\ntrain_x=train.drop(columns={'y'})\ntrain_y=train['y']\n\ntest_x=test.drop(columns={'y'})\ntest_y=test['y']\n\nbasemodel = LogisticRegression(solver='lbfgs',max_iter=10000)\nbasemodel.fit(train_x, train_y)\n\npredictions_bm=basemodel.predict(test_x)\nscore_bm = basemodel.score(test_x, test['y'])\nprint(\"Score of base model is:\"+str(score_bm))\n\ny_probas = basemodel.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\n\ncm_bm = metrics.confusion_matrix(test_y, predictions_bm, [0,1])\nprint(\"Confusion Matrix of base model:\")\nprint(cm_bm)","2f8b1e35":"# Apply grid search cv and run logistic reg\n\nlogistic = LogisticRegression(solver='lbfgs')\npenalty = ['l2']\nmax_iter=[10000]\n    \n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty, max_iter=max_iter)\nmodel_gs = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\nbest_model = model_gs.fit(train_x,train_y)\n\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\nscore_best = best_model.score(test_x, test_y)\nprint(\"Best model score:\"+str(score_best))\n\npredictions_best=best_model.predict(test_x)\ny_probas = best_model.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\n\ncm_best = metrics.confusion_matrix(test_y, predictions_best, [0,1])\nprint(\"Confusion matrix of best model:\")\nprint(cm_best)\n","7fc621c0":"#Oversampling as true positve rate is low for 'yes' and then fit basic log reg\n\nX_resampled, y_resampled = SMOTE().fit_resample(train_x, train_y)\nbasemodel = LogisticRegression(solver='lbfgs',max_iter=10000)\nbasemodel.fit(X_resampled, y_resampled)\npredictions_bm=basemodel.predict(test_x)\nscore_bm = basemodel.score(test_x, test['y'])\nprint(\"Score of base model after oversampling is:\"+str(score_bm))\n\ny_probas = basemodel.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\n\ncm_bm = metrics.confusion_matrix(test_y, predictions_bm, [0,1])\nprint(\"Confusion Matrix of base model after oversampling:\")\nprint(cm_bm)\n\n","c4e98c16":"#Resampling, hyperparameter tuning with grid search cv to pick best model.\n\nlogistic = LogisticRegression(solver='lbfgs')\npenalty = ['l2']\nmax_iter=[10000]\n    \n# Create regularization hyperparameter space\nC = np.logspace(0, 4, 10)\n\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty, max_iter=max_iter)\nmodel_gs = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\nbest_model = model_gs.fit(X_resampled,y_resampled)\n\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\nscore_best = best_model.score(test_x, test_y)\nprint(\"Best model score:\"+str(score_best))\n\npredictions_best=best_model.predict(test_x)\ny_probas = best_model.predict_proba(test_x)\nskplt.metrics.plot_roc(test_y, y_probas)\nplt.show()\n\ncm_best = metrics.confusion_matrix(test_y, predictions_best, [0,1])\nprint(\"Confusion matrix of best model:\")\nprint(cm_best)","c7a135f4":"**Bank Marketing Campaign Analysis**\n\n**Introduction**\nThis dataset describle Portugal Bank marketing campaign results. The campaigns were conducted mostly on direct phone calls to offer clients a term deposit in the bank. If clients agreed, the result is marked as 'yes' or else 'no'.\nClient specific information is gathered like job, age, education, marital status, if there was a previous effort etc.\n\nTask: Predict if a customer will be willing to open a term deposit given certain information about the client. This way, we can target certain people who can be potential customers.\n\nApproach:\nInitially, load the dataset and do some EDA\nPerform encoding on categorical data\nFit a logistic regression model\nPerform Grid Search CV using Stratified Kfold and check if there is any score improvement.\n\nNow, to improve score, lets try feature engineering, ensemble techniques. As the classes are imbalanced (we have more 'no' values then 'yes'), lets consider oversampling for 'yes' class and see if score improved.\n\n"}}