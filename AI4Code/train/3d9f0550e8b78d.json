{"cell_type":{"0cc87605":"code","1bda0e77":"code","32662904":"code","e8fb2b1f":"code","bdb785de":"code","d7b84445":"code","cd944260":"code","c3c0d4c1":"code","38617257":"code","d9ea761a":"markdown","c312655c":"markdown","cd5be687":"markdown","ab560325":"markdown","562923ad":"markdown","b74d9931":"markdown","2bd50832":"markdown","fe261354":"markdown","573a11b2":"markdown","ebf27742":"markdown"},"source":{"0cc87605":"from random import randint, seed\nfrom math import ceil\nfrom math import log10\n\nimport numpy as np\n\nfrom keras.utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import TimeDistributed\nfrom keras.layers import RepeatVector","1bda0e77":"# generate lists of random integers and their sum\ndef random_sum_pairs(n_examples, n_numbers, largest):\n    X, y = list(), list()\n    for _ in range(n_examples):\n        in_pattern = [randint(1,largest) for _ in range(n_numbers)]\n        out_pattern = sum(in_pattern)\n        X.append(in_pattern)\n        y.append(out_pattern)\n    return X, y\n\n\n# convert data to strings\ndef to_string(X, y, n_numbers, largest):\n    max_length = int(n_numbers * ceil(log10(largest+1)) + n_numbers - 1)\n    Xstr = list()\n    for pattern in X:\n        strp = '+'.join([str(n) for n in pattern])\n        strp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp\n        Xstr.append(strp)\n    max_length = int(ceil(log10(n_numbers * (largest+1))))\n    ystr = list()\n    for pattern in y:\n        strp = str(pattern)\n        strp = ''.join([' ' for _ in range(max_length-len(strp))]) + strp \n        ystr.append(strp)\n    return Xstr, ystr\n\n# integer encode strings\ndef integer_encode(X, y, alphabet):\n    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n    Xenc = list()\n    for pattern in X:\n        integer_encoded = [char_to_int[char] for char in pattern]\n        Xenc.append(integer_encoded)\n        yenc = list()\n    for pattern in y:\n        integer_encoded = [char_to_int[char] for char in pattern]\n        yenc.append(integer_encoded)\n    return Xenc, yenc\n\nseed(1)\nn_samples = 1\nn_numbers = 2\nlargest = 10\n# generate pairs\nX, y = random_sum_pairs(n_samples, n_numbers, largest)\nprint('Numbers & their Sum:', X, y)\n# convert to strings\nX, y = to_string(X, y, n_numbers, largest)\nprint('Converted to String : ',X, y)\n# integer encode\nalphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', ' '] \nX, y = integer_encode(X, y, alphabet)\nprint('Encoded String values : ', X, y)","32662904":"def one_hot_encode(X, y, max_int):\n    return to_categorical(X, num_classes=max_int), to_categorical(y, num_classes=max_int)","e8fb2b1f":"# generate an encoded dataset\ndef generate_data(n_samples, n_numbers, largest, alphabet):\n    # generate pairs\n    X, y = random_sum_pairs(n_samples, n_numbers, largest)\n    # convert to strings\n    X, y = to_string(X, y, n_numbers, largest)\n    # integer encode\n    X, y = integer_encode(X, y, alphabet)\n    # one hot encode\n    X, y = one_hot_encode(X, y, len(alphabet))\n    return X, y\n\nX1, y1 = generate_data(2,3,9,alphabet)\nprint(X1)\nprint(y1)","bdb785de":"# invert encoding\ndef invert(seq, alphabet):\n    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n    strings = list()\n    for pattern in seq:\n        for x in pattern:\n            string = int_to_char[np.argmax(x)]\n            strings.append(string) \n    return ''.join(strings)","d7b84445":"invert(X1,alphabet)","cd944260":"# number of math terms\nn_terms = 3\n# largest value for any single input digit\nlargest = 10\n# size of alphabet: (12 for 0-9, + and ' ')\nn_chars = len(alphabet)\n# length of encoded input sequence (8 for '10+10+10)\nn_in_seq_length = int(n_terms * ceil(log10(largest+1)) + n_terms - 1) \n# length of encoded output sequence (2 for '30')\nn_out_seq_length = int(ceil(log10(n_terms * (largest+1))))\n\n\nmodel = Sequential()\nmodel.add(LSTM(75, input_shape=(n_in_seq_length, n_chars)))\nmodel.add(RepeatVector(n_out_seq_length))\nmodel.add(LSTM(50, return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_chars, activation='softmax')))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \nmodel.summary()","c3c0d4c1":"# fit LSTM\nX, y = generate_data(90000, n_terms, largest, alphabet)\nX_test, y_test = generate_data(10000, n_terms, largest, alphabet)\nmodel.fit(X, y, epochs=3, batch_size=32, validation_data=(X_test, y_test))","38617257":"for _ in range(10):\n    X, y = generate_data(1, n_terms, largest, alphabet)\n    yhat = model.predict(X)\n    # decode input, expected and predicted\n    in_seq = invert(X, alphabet)\n    out_seq = invert(y, alphabet)\n    predicted = invert(yhat, alphabet)\n    print('%s = %s (expected %s)' % (in_seq, predicted, out_seq))\n    ","d9ea761a":"<center><h3>Modelling","c312655c":"<center><h3>Generating Sequences","cd5be687":"<h3><center>Architecture<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nOne approach to seq2seq prediction problems that has proven very effective is called the Encoder- Decoder LSTM. This architecture is comprised of two models: one for reading the input sequence and encoding it into a fixed-length vector, and a second for decoding the fixed-length vector and outputting the predicted sequence. The use of the models in concert gives the architecture its name of Encoder-Decoder LSTM designed specifically for seq2seq problems.\n<\/div>\n\n![image.png](attachment:image.png)\n\n<h3>Repeated Vector<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.8px;\">\nWe must connect the encoder to the decoder, and they do not fit. That is, the encoder will produce a 2-dimensional matrix of outputs, where the length is defined by the number of memory cells in the layer. The decoder is an LSTM layer that expects a 3D input of [samples, time steps, features] in order to produce a decoded sequence of some different length defined by the problem.\nIf you try to force these pieces together, you get an error indicating that the output of the decoder is 2D and 3D input to the decoder is required. We can solve this using a RepeatVector layer. This layer simply repeats the provided 2D input multiple times to create a 3D output.\n<\/div>\n<br>\n<h3>Time Distributed Wrapper<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.8px;\">\nThe decoder must transform the learned internal representation of the input sequence into the correct output sequence. One or more LSTM layers can also be used to implement the decoder model. This model reads from the fixed sized output from the encoder model. As with the Vanilla LSTM, a Dense layer is used as the output for the network. The same weights can be used to output each time step in the output sequence by wrapping the Dense layer in a TimeDistributed wrapper\n<\/div>","ab560325":"<center><h3>One Hot Encoding","562923ad":"<center><h3>Invert Encoding","b74d9931":"![image.png](attachment:image.png)","2bd50832":"<center><h3>Predictions","fe261354":"<center><h2>Addition Prediction Problem<\/h2><\/center>\n<br>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThe addition problem is a sequence-to-sequence, or seq2seq, prediction problem.<br>\nThe problem is defined as calculating the sum output of two input numbers. This is challenging as each digit and mathematical symbol is provided as a character and the expected output is also expected as characters. For example, the input 10+6 with the output 16 would be represented by the sequences:\n<\/div>\n\n![image.png](attachment:image.png)","573a11b2":"<center><h3>Importing Libraries","ebf27742":"<center><h3>Sequence Generating Pipeline"}}