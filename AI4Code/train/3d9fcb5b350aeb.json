{"cell_type":{"aaefbe7d":"code","f98248bb":"code","5575268d":"code","b409b212":"code","080ed4cc":"code","6d7f1dc2":"code","4e1fe9dc":"code","d6d46c53":"code","370b0d3d":"code","9e54f13b":"code","c9b3753d":"code","b927f527":"code","e06b71fb":"code","d2f0ffb6":"code","87b5fc05":"code","34e46b4b":"code","86edd635":"code","94df2f7a":"code","c0763c81":"code","b45c7bce":"markdown","dca560f9":"markdown","c214bb4d":"markdown","103ac268":"markdown","36ea4f96":"markdown"},"source":{"aaefbe7d":"from sys import stdout\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.signal import savgol_filter\n\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score","f98248bb":"data = pd.read_csv(\"..\/input\/peach-nir-spectra-brix-values\/peach_spectrabrixvalues.csv\")","5575268d":"data.head()","b409b212":"y = data['Brix'].values\nX = data.values[:, 1:]","080ed4cc":"y.shape","6d7f1dc2":"X.shape","4e1fe9dc":"# Plot the data\nwl = np.arange(1100, 2300, 2)\nprint(len(wl))","d6d46c53":"with plt.style.context('ggplot'):\n    plt.plot(wl, X.T)\n    plt.xlabel(\"Wavelengths (nm)\")\n    plt.ylabel(\"Absorbance\")","370b0d3d":"X2 = savgol_filter(X, 17, polyorder=2, deriv=2)","9e54f13b":"# plot and see\nplt.figure(figsize=(8, 4.5))\nwith plt.style.context('ggplot'):\n    plt.plot(wl, X2.T)\n    plt.xlabel(\"Wavelengths (nm)\")\n    plt.ylabel(\"D2 Absorbance\")\n    plt.show()","c9b3753d":"def optimise_pls_cv(X, y, n_comp):\n    # Define PLS object\n    pls = PLSRegression(n_components=n_comp)\n\n    # Cross-validation\n    y_cv = cross_val_predict(pls, X, y, cv=10)\n\n    # Calculate scores\n    r2 = r2_score(y, y_cv)\n    mse = mean_squared_error(y, y_cv)\n    rpd = y.std()\/np.sqrt(mse)\n    \n    return (y_cv, r2, mse, rpd)","b927f527":"# test with 40 components\nr2s = []\nmses = []\nrpds = []\nxticks = np.arange(1, 41)\nfor n_comp in xticks:\n    y_cv, r2, mse, rpd = optimise_pls_cv(X2, y, n_comp)\n    r2s.append(r2)\n    mses.append(mse)\n    rpds.append(rpd)","e06b71fb":"# Plot the mses\ndef plot_metrics(vals, ylabel, objective):\n    with plt.style.context('ggplot'):\n        plt.plot(xticks, np.array(vals), '-v', color='blue', mfc='blue')\n        if objective=='min':\n            idx = np.argmin(vals)\n        else:\n            idx = np.argmax(vals)\n        plt.plot(xticks[idx], np.array(vals)[idx], 'P', ms=10, mfc='red')\n\n        plt.xlabel('Number of PLS components')\n        plt.xticks = xticks\n        plt.ylabel(ylabel)\n        plt.title('PLS')\n\n    plt.show()","d2f0ffb6":"plot_metrics(mses, 'MSE', 'min')","87b5fc05":"plot_metrics(rpds, 'RPD', 'max')","34e46b4b":"plot_metrics(r2s, 'R2', 'max')","86edd635":"y_cv, r2, mse, rpd = optimise_pls_cv(X2, y, 7)","94df2f7a":"print('R2: %0.4f, MSE: %0.4f, RPD: %0.4f' %(r2, mse, rpd))","c0763c81":"plt.figure(figsize=(6, 6))\nwith plt.style.context('ggplot'):\n    plt.scatter(y, y_cv, color='red')\n    plt.plot(y, y, '-g', label='Expected regression line')\n    z = np.polyfit(y, y_cv, 1)\n    plt.plot(np.polyval(z, y), y, color='blue', label='Predicted regression line')\n    plt.xlabel('Actual')\n    plt.ylabel('Predicted')\n    plt.legend()\n    plt.plot()","b45c7bce":"Notice that all the metrics confirm that 7 components is the best option.\nWe now apply it to our solution.","dca560f9":"# Variable selection method for PLS in Python\nRPD of 1.3495 is low and not very stable. Therefore, we could use PLS for variable selection.","c214bb4d":"The offset is gone and the data look more bunched together.\n\nNow it's time to get to the optimisation of the PLS regression.","103ac268":"If required, data can be easily sorted by PCA and corrected with multiplicative scatter correction, however, another simple yet effective way to get rid of baseline and linear variations is to perform second derivative on the data.","36ea4f96":"# Partial Least Squares Regression in Python\nLearned from: https:\/\/nirpyresearch.com\/partial-least-squares-regression-python\/<br\/>\nPLS, acronym of Partial Least Squares, is a widespread regression technique used to analyze near-infrared spectroscopy data.\n\nPCR is quite simply a regression model built using a number of principal components derived using PCA. PCR is nice and simple but it does not tak einto account anything other than the regression data (e.g., not taking into account about the labels or values need to be predicted - y). That is, our primary reference data are not considered when building a PCR model. That is obviously not optimal, and PLS is a way to fix that.\n\nIn thsi post, we are going to show how to build a simple regression model using PLS in Python.\n1. Mathematical introduction to the difference between PCR and PLS regression\n2. Present the basic code for PLS\n3. Discuss the data we want to analyze and the pre-processing required\n4. We will build our model using a cross-validation approach\n\n# Difference between PCR and PLS regression\nBefore working on some code, let's briefly discuss the mathematical difference between PCR and PLS.\n\nBoth PLS and PCR perform multiple linear regression, that is they build a linear model, Y = XB + E. Using a common language in statistics, X is the predictor and Y is the response. In NIR analysis, X is the set of spectra, Y is th equantity - or quantities - we want to calibrate for (in our case the brix values). Finally E is an error.\n\nThe matrix X contains highly correlated data and this correlation (unrelated to brix) may obscure the variations we want to measure, that is the variations of the brix content. Both PCR and PLS will get rid of the correlation.\n\nIn PCR, the set of measurements X is treansformed into equivalent $X'=XW$ by a linear transformation $W$, such that all the new 'spectra' (which are the principal components) are linear independent. In statistics $X'$ is called the **factor scores**.\n\nThe linear transformation in PCR is such that it minimises the covariance between the diffrent rows of $X'$. That means this process only uses the spectral data, not the response values.\n\nThis is the key difference between PCR and PLS regression. PLS is based on finding a similar linear transformation, but accomplishes the same task by maximising the covariance between $Y$ and $X'$. In other words, PLS takes into account both spectra and response values and in doing so will improve on some of the limitation on PCR. For these reasons PLS is one of the staples of modern chemometrics.\n\n# PLS in Python\n`sklearn` already has got a PLS package, so we go ahead and use it without reinventing the wheel. So, first we define teh number of components we want to keep in our PLS regression. Once the PLS object is defined, we fit the regression to the data `x` (the preditor) and `y` (the known response). The third step is to use the model we jsut built to run a cross-validation experiment iusign 10 fold cross-validation.\n\nWhen we do not have a large number of spectra, cross-validation is a good way to test the predictive capability of our model.\n\n"}}