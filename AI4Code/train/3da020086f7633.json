{"cell_type":{"8a047e19":"code","991f16c0":"code","348cb3ef":"code","350517ee":"code","e3e29463":"code","01deaf3d":"code","e948ec83":"code","6f8162ea":"code","75397905":"code","d553ae9e":"code","f77b6e86":"code","c8869712":"code","a3abdb8b":"code","826adbca":"code","d5611e1f":"code","1613a115":"code","d674ebc8":"code","c3a87700":"code","0b23a896":"code","ea639732":"code","1e6f613c":"code","515c7814":"code","1747252b":"code","998e078e":"code","dc7ea1e0":"code","b557defd":"code","4a6c08ca":"code","53ac69ef":"code","3e708d8d":"code","8dfc6cb9":"markdown","aef1bb21":"markdown","2cee5de7":"markdown","e297a386":"markdown","e1f91d3a":"markdown","ed8f338a":"markdown","8b453139":"markdown","760775c0":"markdown","9ad77824":"markdown","6ecc90b5":"markdown","c9175aae":"markdown","bc6d1130":"markdown","f5952510":"markdown","4c981bc4":"markdown","5924b412":"markdown","a0ae1cb9":"markdown","12f2c47f":"markdown","e6885a40":"markdown","f61f6d7f":"markdown","3a3902d1":"markdown","1f676d65":"markdown","f3c55db1":"markdown","9e546ed8":"markdown","682aca76":"markdown","ac478896":"markdown","4c8b7526":"markdown","da091817":"markdown","07666a21":"markdown","f56eb51e":"markdown","6535bb7d":"markdown","222f0a6a":"markdown","a56f7d6f":"markdown","c8c4c9b1":"markdown","ad8191e6":"markdown","120ce688":"markdown","602005fc":"markdown","45c12316":"markdown","39370a69":"markdown"},"source":{"8a047e19":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport textblob\nfrom textblob import TextBlob, Word\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","991f16c0":"def avg_word_len (sentence):\n    words = sentence.split()\n    avg_len = sum(len(word) for word in words)\/len(words)\n    return avg_len\n\ndef extract_ngrams(data, num):\n    '''\n    Function to generate n-grams from sentences\n    '''\n    n_grams = TextBlob(data).ngrams(num)\n    return [ ' '.join(grams) for grams in n_grams]","348cb3ef":"# Dataset\n\ntrain = pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')","350517ee":"print(train.shape)\ntrain.head()","e3e29463":"# Creating a copy of train dataset for text analysis\n\ndf_train = train.copy()","01deaf3d":"df_train['char_count'] = df_train['tweet'].str.len()\ndf_train_sort_charcount = df_train.sort_values(by='char_count', ascending=False)\ndf_train_sort_charcount[['tweet', 'char_count']].head()","e948ec83":"df_train['word_count'] = df_train['tweet'].apply(lambda x: len(str(x).split(\" \")))\ndf_train_sort_wordcount = df_train.sort_values(by='word_count', ascending=False)\ndf_train_sort_wordcount[['tweet','word_count']].head()","6f8162ea":"# Number of hashtags in a tweet\n\ndf_train['hashtags'] = df_train['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ndf_train_sort_hashtags = df_train.sort_values(by='hashtags', ascending=False)\ndf_train_sort_hashtags[['tweet', 'hashtags']].head()","75397905":"stop_words = stopwords.words('english')\n\ndf_train['stopwords'] = df_train['tweet'].apply(lambda x: len([i for i in x.split() if i in stop_words]))\ndf_train_sort_stopwords = df_train.sort_values(by='stopwords', ascending=False)\ndf_train_sort_stopwords[['tweet', 'stopwords']].head()","d553ae9e":"df_train['number_count'] = df_train['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndf_train_sort_number_count = df_train.sort_values(by='number_count', ascending=False)\ndf_train_sort_number_count[['tweet', 'number_count']].head()","f77b6e86":"df_train['upper_word'] = df_train['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ndf_train_sort_uppercase = df_train.sort_values(by='upper_word', ascending=False)\ndf_train_sort_uppercase[['tweet', 'upper_word']].head()","c8869712":"df_train['avg_word_len'] = df_train['tweet'].apply(lambda x: round(avg_word_len(x),1))\ndf_train_sort_avg_word_len = df_train.sort_values(by='avg_word_len', ascending=True)\ndf_train_sort_avg_word_len[['tweet', 'avg_word_len']].head()","a3abdb8b":"data = df_train['tweet'][0]\n \nprint(\"1-gram: \", extract_ngrams(data, 1))\nprint(\"2-gram: \", extract_ngrams(data, 2))\nprint(\"3-gram: \", extract_ngrams(data, 3))\nprint(\"4-gram: \", extract_ngrams(data, 4))","826adbca":"tf = df_train['tweet'][1:2].apply(lambda x: pd.value_counts(x.split())\/len(x.split())).sum(axis=0).reset_index()\ntf.columns = ['words', 'tf']\ntf","d5611e1f":"for i,word in enumerate(tf['words']):\n    tf.loc[i, 'idf'] = np.log(df_train.shape[0]\/(len(df_train[df_train['tweet'].str.contains(word)])))    \ntf","1613a115":"tfidf = TfidfVectorizer(max_features=10000, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\ndf_train_tfidf = tfidf.fit_transform(df_train['tweet'])\ndf_train_tfidf","d674ebc8":"bag_of_words = CountVectorizer(max_features=10000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ndf_train_bag_of_words = bag_of_words.fit_transform(df_train['tweet'])\ndf_train_bag_of_words","c3a87700":"df_train['sentiment'] = df_train['tweet'][:20].apply(lambda x: TextBlob(x).sentiment[0])\ndf_train[['tweet','sentiment']].head(5)","0b23a896":"df_train.head(3)","ea639732":"# Creating a copy of dataset to preprocess the data\n\ndf_train_dpp = df_train.copy()","1e6f613c":"df_train_dpp['tweet_lower'] = df_train_dpp['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf_train_dpp[['tweet', 'tweet_lower']].head()\n","515c7814":"stop_words = stopwords.words('english')\n\ndf_train_dpp['tweet_stopwords'] = df_train_dpp['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\ndf_train_dpp[['tweet', 'tweet_stopwords']].head()","1747252b":"df_train_dpp['tweet_punc'] = df_train_dpp['tweet_stopwords'].str.replace('[^\\w\\s]', '')\ndf_train_dpp[['tweet', 'tweet_punc']].head()","998e078e":"# Frequency of common words in all the tweets\n\ncommon_top20 = pd.Series(' '.join(df_train_dpp['tweet_punc']).split()).value_counts()[:20]\nprint(common_top20)\n\n\n# Remove these top 20 freq words\ncommon = list(common_top20.index)\n\ndf_train_dpp['tweet_comm_remv'] = df_train_dpp['tweet_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in common))\ndf_train_dpp[['tweet','tweet_comm_remv']].head()","dc7ea1e0":"# Frequency of common words in all the tweets\nrare_top20 = pd.Series(\" \".join(df_train_dpp['tweet_comm_remv']).split()).value_counts()[-20:]\nrare_top20\n\n# Remove these top 20 common words\nrare = list(rare_top20.index)\n\ndf_train_dpp['tweet_rare_remv'] = df_train_dpp['tweet_comm_remv'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\ndf_train_dpp[['tweet','tweet_rare_remv']].head()","b557defd":"# Using textblob\n\ndf_train_dpp['tweet_rare_remv'][:10].apply(lambda x: str(TextBlob(x).correct()))\n","4a6c08ca":"df_train_dpp['tweet_rare_remv'][:10].apply(lambda x: TextBlob(x).words)","53ac69ef":"st = PorterStemmer()\ndf_train_dpp['tweet_rare_remv'][:10].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","3e708d8d":"df_train_dpp['tweet_rare_remv'][:10].apply(lambda x: \" \".join(Word(word) for word in x.split()))","8dfc6cb9":"### 2.8 Lemmatization <a id=\"Lemmatization\"><\/a>","aef1bb21":"### 2.7 Stemming <a id=\"Stemming\"><\/a>","2cee5de7":"## 2. Pre-processing <a id=\"Pre-processing\"><\/a>","e297a386":"### Thank you for your <font color='red'>Upvote<\/font> ","e1f91d3a":"![image.png](attachment:image.png)\nimage credit: https:\/\/www.blumeglobal.com\/learning\/natural-language-processing\/","ed8f338a":"### 1.12 Sentiment Analysis <a id=\"Sentimentanalysis\"><\/a>","8b453139":"### 1.6 Average Word Length <a id=\"Averagewordlength\"><\/a>","760775c0":"### 2.6 Tokenization <a id=\"Tokenization\"><\/a>","9ad77824":"#### Extract features using NLP techniques below","6ecc90b5":"### 1.9 Inverse Document Frequency <a id=\"InverseDocumentFrequency\"><\/a>","c9175aae":"### 2.1 Stopwords Removal <a id=\"Stopwordsremoval\"><\/a>","bc6d1130":"### 1.5 Uppercase word count <a id=\"Uppercasewordcount\"><\/a>","f5952510":"* Sentiment(polarity=0.8, subjectivity=0.75)\n\n* We can see that polarity is 0.8, which means that the statement is positive and 0.75 subjectivity refers that mostly it is a public opinion and not a factual information.\n\nSentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.  \nThe sentiment function of textblob returns two properties, polarity, and subjectivity.   \nPolarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement.    Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].   ","4c981bc4":"## User Defined Functions","5924b412":"## Table of Contents\n\n### 1. <a href='#FeatureExtraction'><font color='blue'>Feature Extraction<\/font><\/a>  \n1.0 <a href='#Charactercount'>Character count<\/a>  \n1.1 <a href='#WordCount'>Word count<\/a>  \n1.2 <a href='#Specialcharactercount'>Special character count<\/a>   \n1.3 <a href='#Stopwordcount'>Stopword count<\/a>   \n1.4 <a href='#Numbercount'>Number count<\/a>    \n1.5 <a href='#Uppercasewordcount'>Uppercase word count<\/a>   \n1.6 <a href='#Averagewordlength'>Average word length<\/a>   \n1.7 <a href='#N-grams'>N-grams<\/a>   \n1.8 <a href='#TermFrequency'>Term Frequency<\/a>    \n1.9 <a href='#InverseDocumentFrequency'>Inverse Document Frequency<\/a>    \n1.10 <a href='#TF-IDF'>TF-IDF<\/a>    \n1.11 <a href='#BagofWords'>Bag of Words<\/a>    \n1.12 <a href='#Sentimentanalysis'>Sentiment Analysis<\/a>   \n\n### 2. <a href='#Pre-processing'><font color='blue'>Pre-processing<\/font><\/a> \n2.0 <a href='#ConvertthesentencetoLowercase'>Convert the sentence to Lower case<\/a>  \n2.1 <a href='#Stopwordsremoval'>Stopwords removal<\/a>  \n2.2 <a href='#Punctuationremoval'>Punctuation removal<\/a>   \n2.3 <a href='#Commonwordremoval'>Common word removal<\/a>   \n2.4 <a href='#Rarewordsremoval'>Rare words removal<\/a>    \n2.5 <a href='#Spellingcorrection'>Spelling correction<\/a>   \n2.6 <a href='#Tokenization'>Tokenization<\/a>   \n2.7 <a href='#Stemming'>Stemming<\/a>   \n2.8 <a href='#Lemmatization'>Lemmatization<\/a>","a0ae1cb9":"* TF = (Number of times term T appears in the particular row) \/ (number of terms in that row)","12f2c47f":"### <font color='orange'>Please Upvote<\/font> if you find this kernel useful. ","e6885a40":"### 1.8 Term Frequency <a id=\"TermFrequency\"><\/a>","f61f6d7f":"## NLP - Basic Feature Creation and Preprocessing","3a3902d1":"### 2.2 Punctuation Removal <a id=\"Punctuationremoval\"><\/a>","1f676d65":"### 2.0 Convert the sentence to Lower case <a id=\"ConvertthesentencetoLowercase\"><\/a>","f3c55db1":"### 2.5 Spelling correction <a id=\"Spellingcorrection\"><\/a>","9e546ed8":"### 1.4 Number Count <a id=\"Numbercount\"><\/a>","682aca76":"## Import Data","ac478896":"### 1.3 Stopword Count <a id=\"Stopwordcount\"><\/a>","4c8b7526":"### 1.1 Word Count <a id=\"WordCount\"><\/a>","da091817":"#### Hope this helps in your NLP journey.\n* See you next time !","07666a21":"* IDF = log(N\/n), where, N is the total number of rows and n is the number of rows in which the word was present\n","f56eb51e":"### 1.0 Character count   <a id=\"Charactercount\"><\/a>","6535bb7d":"### 1.10 TF-IDF <a id=\"TF-IDF\"><\/a>","222f0a6a":"### 1.11 Bag of Words <a id=\"BagofWords\"><\/a>","a56f7d6f":"### Let's Start","c8c4c9b1":"## 1. Feature Extraction <a id='FeatureExtraction'><\/a>\n\n* Creation of brand new features","ad8191e6":"### 2.4 Rare words removal <a id=\"Rarewordsremoval\"><\/a>","120ce688":"### 1.2 Special Character Count <a id=\"Specialcharactercount\"><\/a>","602005fc":"### 1.7 N-grams <a id=\"N-grams\"><\/a>","45c12316":"## Libraries","39370a69":"### 2.3 Common word removal <a id=\"Commonwordremoval\"><\/a>"}}