{"cell_type":{"a06b6911":"code","65d4c11c":"code","3065338a":"code","24b7cca9":"code","fbfdeb14":"code","c550b544":"code","a1071efb":"code","4778acfa":"code","0edabc67":"code","9c3bf078":"code","ac651a7c":"code","8ed92363":"code","1c076724":"code","729b1db7":"code","873f7937":"code","0a137b0a":"code","6303be1b":"code","3438ac47":"code","a4a9ccc7":"code","57575e22":"code","cc36077f":"code","ade086df":"code","aed97e5e":"code","7de312d2":"code","24a8aaba":"code","7971035c":"code","943f67ec":"code","2ed9c514":"code","6fddd45a":"code","6b616b45":"code","2b5edc0b":"code","dfe14e90":"code","217ba70f":"code","b3febaf0":"code","fc788b63":"markdown","7599a205":"markdown","e1adcbe7":"markdown","5b0c8a15":"markdown","48780914":"markdown","649fc8ec":"markdown","26c53c13":"markdown","632159a0":"markdown","0b20333f":"markdown","75502121":"markdown","4e752691":"markdown","be44ecda":"markdown","11bb3dbd":"markdown","4d3fa171":"markdown","9d64110c":"markdown","441efb6e":"markdown","5f0e483b":"markdown","21b1b6fc":"markdown","0f6dce9f":"markdown","df759b15":"markdown","fc2ab9aa":"markdown","01ece6ca":"markdown","645c0876":"markdown","865eedfb":"markdown"},"source":{"a06b6911":"import pandas as pd\nimport findspark \nimport os\nfrom pyspark import SparkContext \nfrom pyspark.sql import SparkSession\nfindspark.init() \nspark = SparkSession\\\n    .builder\\\n    .appName(\"HomeCreditApp\")\\\n    .config(\"spark.driver.memory\", \"8G\")\\\n    .config(\"spark.executor.memory\", \"4G\")\\\n    .getOrCreate()","65d4c11c":"sparkApplicationData = spark\\\n            .read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .option(\"delimiter\", \",\")\\\n            .csv(\"Data\/application_train.csv\")\\\n            .cache()","3065338a":"sparkApplicationDataTest = spark\\\n            .read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .option(\"delimiter\", \",\")\\\n            .csv(\"Data\/application_test.csv\")\\\n            .cache()","24b7cca9":"sparkPosCashData = spark\\\n            .read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .option(\"delimiter\", \",\")\\\n            .csv(\"Data\/POS_CASH_balance.csv\")\\\n            .cache()\nsparkPosCashData1 = sparkPosCashData.withColumnRenamed(\"SK_ID_CURR\",\"SK_ID_CURR_MIN\")\nsparkPosCashData2 = sparkPosCashData.withColumnRenamed(\"SK_ID_CURR\",\"SK_ID_CURR_MAX\")\nsparkPosCashDataMin = sparkPosCashData1.groupBy('SK_ID_CURR_MIN').min()\nsparkPosCashDataMax = sparkPosCashData2.groupBy('SK_ID_CURR_MAX').max()","fbfdeb14":"sparkBureauData = spark\\\n            .read\\\n            .option(\"header\", \"true\")\\\n            .option(\"inferSchema\", \"true\")\\\n            .option(\"delimiter\", \",\")\\\n            .csv(\"Data\/bureau.csv\")\\\n            .cache()\nsparkBureauData1 = sparkBureauData.withColumnRenamed(\"SK_ID_CURR\",\"SK_ID_CURR_BUR_MIN\")\nsparkBureauData2 = sparkBureauData.withColumnRenamed(\"SK_ID_CURR\",\"SK_ID_CURR_BUR_MAX\")\nsparkBureauDataMin = sparkBureauData1.groupBy('SK_ID_CURR_BUR_MIN').min()\nsparkBureauDataMax = sparkBureauData2.groupBy('SK_ID_CURR_BUR_MAX').max()","c550b544":"sparkApplicationData = sparkApplicationData.join(sparkPosCashDataMin, sparkApplicationData.SK_ID_CURR == sparkPosCashDataMin.SK_ID_CURR_MIN, how='left')\nsparkApplicationData = sparkApplicationData.join(sparkPosCashDataMax, sparkApplicationData.SK_ID_CURR == sparkPosCashDataMax.SK_ID_CURR_MAX, how='left')\nsparkApplicationData = sparkApplicationData.join(sparkBureauDataMin, sparkApplicationData.SK_ID_CURR == sparkBureauDataMin.SK_ID_CURR_BUR_MIN, how='left')\nsparkApplicationData = sparkApplicationData.join(sparkBureauDataMax, sparkApplicationData.SK_ID_CURR == sparkBureauDataMax.SK_ID_CURR_BUR_MAX, how='left')","a1071efb":"sparkApplicationDataTest = sparkApplicationDataTest.join(sparkPosCashDataMin, sparkApplicationDataTest.SK_ID_CURR == sparkPosCashDataMin.SK_ID_CURR_MIN, how='left')\nsparkApplicationDataTest = sparkApplicationDataTest.join(sparkPosCashDataMax, sparkApplicationDataTest.SK_ID_CURR == sparkPosCashDataMax.SK_ID_CURR_MAX, how='left')\nsparkApplicationDataTest = sparkApplicationDataTest.join(sparkBureauDataMin, sparkApplicationDataTest.SK_ID_CURR == sparkBureauDataMin.SK_ID_CURR_BUR_MIN, how='left')\nsparkApplicationDataTest = sparkApplicationDataTest.join(sparkBureauDataMax, sparkApplicationDataTest.SK_ID_CURR == sparkBureauDataMax.SK_ID_CURR_BUR_MAX, how='left')","4778acfa":"sparkApplicationData\\\n  .printSchema()","0edabc67":"from pyspark.sql.functions import isnull\nfrom IPython.core.display import HTML\nfor c in sparkApplicationData.columns:\n  nullCnt = sparkApplicationData.where(isnull(c)).count()\n  display(HTML('Column <b>{}<\/b> has <b>{}<\/b> null records'.format(str(c), nullCnt))) ","9c3bf078":"def displayPandas(sparkData):\n    display(\n    sparkData\n      .limit(5)\n      .toPandas()\n    )","ac651a7c":"displayPandas(sparkApplicationData)","8ed92363":"print(sparkApplicationData.count())","1c076724":"## filter numeric cols\nnum_cols = [col_type[0] for col_type in filter(lambda dtype: dtype[1] in {\"bigint\", \"double\", \"int\"}, sparkApplicationData.dtypes)]\n### Compute a dict with <col_name, median_value>\nmedian_dict = dict()\nfor c in num_cols:\n   median_dict[c] = sparkApplicationData.stat.approxQuantile(c, [0.5], 0.001)[0]","729b1db7":"sparkApplicationData = sparkApplicationData.na.fill(median_dict)","873f7937":"sparkApplicationData.write\\\n        .format(\"com.databricks.spark.csv\")\\\n        .option(\"header\", \"true\")\\\n        .mode(\"overwrite\")\\\n        .save('Data\/preparedTrain.csv')\\","0a137b0a":"import h2o\nh2o.init()","6303be1b":"df_train = h2o.import_file('Data\/preparedTrain.csv')","3438ac47":"df_train.describe()","a4a9ccc7":"df_train.head()","57575e22":"notFeatures = ['TARGET', 'SK_ID_CURR', 'SK_ID_CURR_MIN', 'SK_ID_CURR_MAX', 'SK_ID_CURR_BUR_MIN', 'SK_ID_CURR_BUR_MAX']\npredictors = df_train.drop(notFeatures).names","cc36077f":"df_train[\"TARGET\"] = df_train[\"TARGET\"].asfactor()","ade086df":"from h2o.estimators import H2OGradientBoostingEstimator\n\ngbm_model = H2OGradientBoostingEstimator(stopping_metric=\"logloss\",\n                                         stopping_rounds= 5,  # early stopping\n                                         score_tree_interval=5,\n                                         ntrees=40,\n                                         model_id=\"gbm\",\n                                         nfolds=5,\n                                         seed=25,\n                                         fold_assignment='stratified')\ngbm_model.train(\n    x = predictors,\n    y = \"TARGET\",\n    training_frame = df_train\n)","aed97e5e":"from h2o.automl import H2OAutoML\n\naml = H2OAutoML(max_models=3, seed=42)\naml.train(\n    x=predictors, \n    y='TARGET', \n    training_frame = df_train\n)","7de312d2":"gbm_model","24a8aaba":"print(\"Training Data\")\ngbm_model.model_performance(train = True).plot()\nprint(\"X-Val\")\ngbm_model.model_performance(xval=True).plot()","7971035c":"gbm_model.varimp_plot(20)","943f67ec":"aml.leaderboard","2ed9c514":"print(\"Training Data\")\naml.leader.model_performance(train = True).plot()\nprint(\"X-Val\")\naml.leader.model_performance(xval=True).plot()","6fddd45a":"try:\n    aml.leader.varimp_plot(20)\nexcept:\n    print('No variable importance')","6b616b45":"notFeatures = ['TARGET', 'SK_ID_CURR', 'SK_ID_CURR_MIN', 'SK_ID_CURR_MAX', 'SK_ID_CURR_BUR_MIN', 'SK_ID_CURR_BUR_MAX']\nfor notFeature in notFeatures:\n    if notFeature in median_dict:\n        del median_dict[notFeature]\nsparkApplicationDataTest = sparkApplicationDataTest.na.fill(median_dict)\nsparkApplicationDataTest.write\\\n        .format(\"com.databricks.spark.csv\")\\\n        .option(\"header\", \"true\")\\\n        .mode(\"overwrite\")\\\n        .save('Data\/preparedTest.csv')\ndf_test = h2o.import_file('Data\/preparedTest.csv')\npred = aml.leader.predict(df_test)","2b5edc0b":"pred['SK_ID_CURR'] = df_test['SK_ID_CURR']\npredFrame = pred.as_data_frame()\npredFrame.to_csv('Data\/pred.csv', index=False) ","dfe14e90":"pred2 = gbm_model.predict(df_test)","217ba70f":"pred2['SK_ID_CURR'] = df_test['SK_ID_CURR']\npredFrame2 = pred2.as_data_frame()\npredFrame2.to_csv('Data\/pred2.csv', index=False)","b3febaf0":"spark.stop() #closing the spark session","fc788b63":"# Evaluation","7599a205":"# Modeling","e1adcbe7":"Join tables","5b0c8a15":"Select predictors:","48780914":"## Gradient Boosting","649fc8ec":"Data is transfered to H20","26c53c13":"## Auto ML","632159a0":"# Data Understanding\n\napplication_{train|test}.csv\n\n- This is the main table, broken into two files for Train (with TARGET) and Test (without TARGET).\n- Static data for all applications. One row represents one loan in our data sample.\n\nbureau.csv\n\n- All client's previous credits provided by other financial institutions that were reported to Credit Bureau (for clients who have a loan in our sample).\n- For every loan in our sample, there are as many rows as number of credits the client had in Credit Bureau before the application date.\n\nbureau_balance.csv\n\n- Monthly balances of previous credits in Credit Bureau.\n- This table has one row for each month of history of every previous credit reported to Credit Bureau \u2013 i.e the table has (#loans in sample * # of relative previous credits * # of months where we have some history observable for the previous credits) rows.\n\nPOS_CASH_balance.csv\n\n- Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.\n- This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credits * # of months in which we have some history observable for the previous credits) rows.\n\ncredit_card_balance.csv\n\n- Monthly balance snapshots of previous credit cards that the applicant has with Home Credit.\n- This table has one row for each month of history of every previous credit in Home Credit (consumer credit and cash loans) related to loans in our sample \u2013 i.e. the table has (#loans in sample * # of relative previous credit cards * # of months where we have some history observable for the previous credit card) rows.\n\nprevious_application.csv\n\n- All previous applications for Home Credit loans of clients who have loans in our sample.\n- There is one row for each previous application related to loans in our data sample.\n\ninstallments_payments.csv\n\n- Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.\n- There is a) one row for every payment that was made plus b) one row each for missed payment.\n- One row is equivalent to one payment of one installment OR one installment corresponding to one payment of one previous Home Credit credit related to loans in our sample.\n\nHomeCredit_columns_description.csv\n\n- This file contains descriptions for the columns in the various data files.","0b20333f":"First 5 rows.","75502121":"Auto ML","4e752691":"# Business Understanding\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.","be44ecda":"Use the model for new data.","11bb3dbd":"Columns and null values","4d3fa171":"# Introduction\nThe author follows the CRISP-DM methodology.","9d64110c":"Gradient Boosting","441efb6e":"# Data Preparation","5f0e483b":"Number of rows","21b1b6fc":"Nominal target","0f6dce9f":"# Deployment","df759b15":"## Gradient Boosting","fc2ab9aa":"Null values replaced with average value.","01ece6ca":"## Auto ML","645c0876":"Start Spark","865eedfb":"The performance of models."}}