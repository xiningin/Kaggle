{"cell_type":{"db11bc23":"code","a1813d86":"code","b3d8918a":"code","655da0a7":"code","aa6263a1":"code","49e14261":"code","4241c7e3":"code","3a36e18a":"code","c791582a":"code","f103b74f":"code","6804cf9a":"code","8a16a636":"code","271f03a1":"code","476634cd":"code","99d4f8e5":"code","8bf16d5b":"code","1ecb74c4":"code","8bcd4859":"code","9bb34a3b":"code","2d11c235":"code","17161c68":"code","706f8be3":"code","c85efa2f":"code","492e1117":"code","c408099d":"code","d6954afb":"code","43e2b327":"code","11eaa741":"markdown","d098a121":"markdown","6766aac9":"markdown","e6665fda":"markdown","84af3e45":"markdown","b6da6d48":"markdown","022ed35c":"markdown","ecb20f9f":"markdown","0e4f8803":"markdown","ba95d922":"markdown","ce58ee69":"markdown","209ad916":"markdown","9cd58e05":"markdown","11b0130e":"markdown","5e44d986":"markdown","bb548e18":"markdown","ca396d21":"markdown","9884ee9e":"markdown","cf82da0b":"markdown","da275d20":"markdown","c9b83b4a":"markdown","456ca04b":"markdown","10fc860e":"markdown","419bd967":"markdown","d4542ce6":"markdown","9f9718a0":"markdown"},"source":{"db11bc23":"from google.colab import drive\ndrive.mount('\/content\/drive', force_remount=True)","a1813d86":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\nfrom tensorflow.keras.utils import to_categorical","b3d8918a":"print(tf.__version__)","655da0a7":"import h5py\nimport numpy as np\n# Open the file as readonly\n# Make changes in path as required\nh5f = h5py.File('\/content\/drive\/MyDrive\/ColabNotebooks\/SVHN_single_grey1.h5', 'r')\n# Load the training and the test set\nX_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\n# Close this file\nh5f.close()","aa6263a1":"len(X_train), len(X_test)","49e14261":"# visualizing the first 10 images in the dataset and their labels\nplt.figure(figsize=(10, 1))\n\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i], cmap=\"gray\")\n    plt.axis('off')\n\nplt.show()\nprint('label for each of the above image: %s' % (y_train[0:10]))","4241c7e3":"# Shape of the images and the first image\n\nprint(\"Shape:\", X_train[0].shape)\nprint()\nprint(\"First image:\\n\", X_train[0])","3a36e18a":"# Reshaping the dataset to flatten them. Remember that we are trying to reshape the 2D image data into a 1D array\n\nX_train = X_train.reshape(X_train.shape[0], 1024)\nX_test = X_test.reshape(X_test.shape[0], 1024)\nX_train.shape","c791582a":"# Normalize inputs from 0-255 to 0-1\n\nX_train = X_train\/255\nX_test = X_test\/255\nX_train.shape","f103b74f":"# New shape \n\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Test set:', X_test.shape, y_test.shape)","6804cf9a":"# one hot encode output\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\n\n# no.of classes\ny_test","8a16a636":"#Fixing the seed for random number generators\nnp.random.seed(42)\nimport random\nrandom.seed(42)\ntf.random.set_seed(42)","271f03a1":"#Importing losses and optimizers modules\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import optimizers\n\n#Define the function\ndef nn_model_1():\n    model = Sequential() \n    #Add layers as per the architecture mentioned above in the same sequence\n    model.add(Dense(64, activation='relu', input_shape=(1024, )))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n    #declare adam optimizer with learning rate of 0.001 \n    adam = optimizers.Adam(learning_rate = 0.001)\n    \n    #compile the model\n    model.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n    \n    return model","476634cd":"# Build the model\nmodel_1 = nn_model_1()","99d4f8e5":"#Print the summary\nmodel_1.summary()","8bf16d5b":"# Fit the model\nhistory_model_1 = model_1.fit(X_train, \n                    y_train,\n                    validation_split=0.2, \n                    batch_size = 128,\n                    verbose=1, \n                    epochs=20)","1ecb74c4":"# plotting the accuracies\n\ndict_hist = history_model_1.history\nlist_ep = [i for i in range(1,21)]\n\nplt.figure(figsize = (8,8))\nplt.plot(list_ep,dict_hist['accuracy'],ls = '--', label = 'accuracy')\nplt.plot(list_ep,dict_hist['val_accuracy'],ls = '--', label = 'val_accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","8bcd4859":"#Clearing backend\nfrom tensorflow.keras import backend\nbackend.clear_session()","9bb34a3b":"#Fixing the seed for random number generators\nnp.random.seed(42)\nimport random\nrandom.seed(42)\ntf.random.set_seed(42)","2d11c235":"#Importing losses and optimizers modules\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import optimizers\n\n#Define the function\ndef nn_model_2():\n    model = Sequential() \n    #Add layers as per the architecture mentioned above in the same sequence\n    model.add(Dense(256, activation='relu', input_shape=(1024, )))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(10, activation='softmax'))\n    #declare adam optimizer with learning rate of 0.001 \n    adam = optimizers.Adam(learning_rate = 0.0005)\n    \n    #compile the model\n    model.compile(loss = 'categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n    \n    return model","17161c68":"# Build the model\nmodel_2 = nn_model_2()","706f8be3":"#Print the model summary\nmodel_2.summary()","c85efa2f":"# Fit the model\nhistory_model_2 =  model_2.fit(X_train, \n                    y_train,\n                    validation_split=0.2, \n                    batch_size = 128,\n                    verbose=1, \n                    epochs=30)","492e1117":"# plotting the accuracies\n\ndict_hist = history_model_2.history\nlist_ep = [i for i in range(1,31)]\n\nplt.figure(figsize = (8,8))\nplt.plot(list_ep,dict_hist['accuracy'],ls = '--', label = 'accuracy')\nplt.plot(list_ep,dict_hist['val_accuracy'],ls = '--', label = 'val_accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","c408099d":"test_pred = model_2.predict(X_test)\n\ntest_pred = np.argmax(test_pred, axis=-1)","d6954afb":"#Converting each entry to single label from one-hot encoded vector\ny_test = np.argmax(y_test, axis=-1)","43e2b327":"#importing required functions\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n#Printing the classification report\nprint(classification_report(y_test, test_pred))\n\n#Plotting the heatmap using confusion matrix\ncm = confusion_matrix(y_test, test_pred) #Write the code for creating confusion matrix using actual labels and predicted labels\n\nplt.figure(figsize=(8,5))\nsns.heatmap(cm, annot=True,  fmt='.0f')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","11eaa741":"## **Model Building**\n\nNow that we have done the data preprocessing, let's build an ANN model.","d098a121":"## **Load the dataset**\n- Let us now load the dataset that is available as a .h5 file.\n- Split the data into train and the test dataset","6766aac9":"#### **Observations:**\n\n- The model is giving about **77% accuracy on the test data** which is **comparable to the accuracy of the validation data.** This implies that the model is giving a generalized performance.\n- The **recall has a high range** (72-84)% which implies that the **model is good at identifying some objects while poor at identifying some other objects**. For example, the **model is able to identify about 84% of image 4 but can only identify only ~72% of image 3 and 8**.\n- Overall the model could distiguish individual image well.","e6665fda":"- Notice that each entry of y_test is a one-hot encoded vector instead of a single label.","84af3e45":"## **Mount the drive**\nLet us start by mounting the drive and importing the necessary libraries.","b6da6d48":"**Observations**:\n\n- The Training accuracy is increasing with the increase in epochs.\n- The validation accuracy is bit fluctuating and also is increasing with the increase in epochs.\n- The model is giving around ~62% accuracy at 20 echos. At 20 epochs, the accuracy of the model on the training data is about 63% and the validation accuracy is ~62%.\n- Maximum accuracy for both train and validation data for 20 Epochs is 62% which is an alright performance rate.\n- The validation accuracy is closer to the training accuracy. This indicates that the model is giving a generalized performance.","022ed35c":"- There are 42,000 images in the training data and 18,000 images in the testing data. ","ecb20f9f":"#### **Question 5: Write your observations on the below plot (2 Marks)**","0e4f8803":"# **Project - Artificial Neural Networks: Street View Housing Number Digit Recognition**\n\nOne of the most interesting tasks in deep learning is to recognize objects in natural scenes. The ability to process visual information using machine learning algorithms can be very useful as demonstrated in various applications.\n\nThe SVHN dataset contains over 600,000 labeled digits cropped from street level photos. It is one of the most popular image recognition datasets. It has been used in neural networks created by Google to improve map quality by automatically transcribing the address numbers from a patch of pixels. The transcribed number with a known street address helps pinpoint the location of the building it represents. \n\n----------------\n### **Objective:**\n----------------\n\nBuild a feed foward neural network model that can identify the digits in the images. \n\n-------------\n### **Dataset**\n-------------\nHere, we will use a subset of the original data to save some computation time. The dataset is provided as a .h5 file. The basic preprocessing steps have been done.","ba95d922":"#### Build and train the new ANN model as per the above mentioned architecture ","ce58ee69":"### **Model Architecture**\n- Write a function that returns a sequential model with the following architecture\n - First hidden layer with **64 nodes and relu activation** and the input shape which is used above\n - Second hidden layer with **32 nodes and relu activation**\n - Output layer with **softmax activation and number of nodes equal to the number of classes**\n -Compile the model with the **categorical_crossentropy loss, adam optimizer (learning_rate = 0.001), and accuracy metric**. Do not fit the model here, just return the compiled model.\n- Call the function and store the model in a new variable \n- Print the summary of the model\n- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and 20 epochs**. Store the model building history to use later for visualization.","209ad916":"Let us check for the version of TensorFlow.","9cd58e05":"## **Importing libraries**","11b0130e":"## **Visualizing images**\n- Use X_train to visualize the first 10 images\n- Use Y_train to print the first 10 labels","5e44d986":"#### Normalize the train and test data ","bb548e18":"Let's check the number of images in the training and testing data.","ca396d21":"### **Second Model Architecture**\n- Write a function that returns a sequential model with the following architecture\n - First hidden layer with **256 nodes and relu activation**\n - Second hidden layer with **128 nodes and relu activation**\n - Add the **Dropout layer with rate equal to 0.2**\n - Third hidden layer with **64 nodes and relu activation**\n - Fourth hidden layer with **64 nodes and relu activation**\n - Fifth hidden layer with **32 nodes and relu activation**\n - Add the **BatchNormalization layer**\n - Output layer with **softmax activation and number of nodes equal to the number of classes**\n -Compile the model with the **categorical_crossentropy loss, adam optimizer (learning_rate = 0.0005), and accuracy metric**. Do not fit the model here, just return the compiled model.\n- Call the function and store the model in a new variable \n- Print the summary of the model\n- Fit on the train data with a **validation split of 0.2, batch size = 128, verbose = 1, and 30 epochs**. Store the model building history to use later for visualization.","9884ee9e":"Let's build one more model with higher complexity and see if we can improve the performance of the model. \n\nFirst, we need to clear the previous model's history from the keras backend. Also, let's fix the seed again after clearing the backend.","cf82da0b":"#### The classification report and the confusion matrix for the test predictions.","da275d20":"### **Plotting the validation and training accuracies**","c9b83b4a":"### **Plotting the validation and training accuracies**","456ca04b":"**Note:** Earlier, we noticed that each entry of the test data is a one-hot encoded vector but to print the classification report and confusion matrix, we must convert each entry of y_test to a single label.","10fc860e":"## **Data preparation**\n\n- Print the first image in the train image and figure out the shape of the images\n- Reshape the train and the test dataset to flatten them. Figure out the required shape\n- Normalise the train and the test dataset by dividing by 255\n- Print the new shapes of the train and the test set\n- One-hot encode the target variable","419bd967":"**Observations**: FIX THIS\n\n- The overal accuracy of this model is better than the 1st model.\n- The Training accuracy is increasing with the increase in epochs. It has a rapid increase up to around 8 epochs and then has lower increase.  \n- The validation accuracy is bit fluctuating but overall it is also increasing with the increase in epochs. Same as training accuracy, validation accuracy also has a rapid increase up to around 8 epochs and then has lower increase.\n- The model is giving around ~75% accuracy at 30 epochs. At 30 epochs, the accuracy of the model on the training data is about 76% and the validation accuracy is 77%.\n- The validation accuracy is closer to the training accuracy. This indicates that the model is giving a generalized performance.","d4542ce6":"#### Build and train a ANN model as per the above mentioned architecture ","9f9718a0":"## **Predictions on the test data**\n\n- Make predictions on the test set using the second model\n- Print the obtained results using classification report and the confusion matrix\n- Final observations from the obtained results"}}