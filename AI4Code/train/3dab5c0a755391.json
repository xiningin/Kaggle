{"cell_type":{"595d6b8c":"code","21b5915f":"code","46119dc0":"code","210969cf":"code","5ac335e8":"code","b3760920":"code","a840c432":"code","c60fbdef":"code","fa22dd26":"code","9b3b9ca7":"markdown","f105a134":"markdown","a49dd194":"markdown"},"source":{"595d6b8c":"# Basic packages\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport time\nimport random\nimport glob\nimport sys\nimport os\nimport gc\n\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input, Flatten, Dropout,Conv1D, BatchNormalization ,Activation, Add,Reshape, Average,Lambda, concatenate\nfrom keras import callbacks\nfrom keras import optimizers\nimport keras.backend as K\n\n# visualization packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# execution progress bar\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport tensorflow as tf\n","21b5915f":"# System Setup\n%matplotlib inline\n%precision 4\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')\nnp.set_printoptions(suppress=True)\npd.set_option(\"display.precision\", 15)","46119dc0":"#LOAD DATA\ndf_train = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv', index_col=0)\ny_train = df_train.pop('target')\n\n\nlen_train = len(df_train)\ndf_test = pd.read_csv('.\/..\/input\/santander-customer-transaction-prediction\/test.csv', index_col=0)\ndf_all = pd.concat((df_train, df_test), sort=False)\nprev_cols = df_all.columns\n\n# PREPROCESS\nscaler = StandardScaler()\ndf_all[prev_cols] = scaler.fit_transform(df_all[prev_cols])\ndf_train = df_all[0:len_train]\ndf_test = df_all[len_train:]\n\n","210969cf":" def augment_train(df_train, y_train):   \n    t0 = df_train[y_train == 0].copy()\n    t1 = df_train[y_train == 1].copy()\n    i = 0\n    N = 3\n    for I in range(0):  # augment data into 2x\n        for col in df_train.columns:\n            i = i + 1000\n            np.random.seed(i)\n            np.random.shuffle(t0[col].values)\n            np.random.shuffle(t1[col].values)\n        df_train = pd.concat([df_train, t0.copy()])\n        df_train = pd.concat([df_train, t1.copy()])\n        y_train = pd.concat([y_train, pd.Series([0] * t0.shape[0]), pd.Series([1] * t1.shape[0])])\n    return df_train, y_train","5ac335e8":"features = [c for c in df_train.columns if c not in [\"ID_code\",\"target\"]]\ndef detect_test(test_df):\n    df_test=test_df.values\n    unique_count = np.zeros_like(df_test)\n    for feature in tqdm(range(df_test.shape[1])):\n        _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n        unique_count[index_[count_ == 1], feature] += 1\n\n    # Samples which have unique values are real the others are fake\n    real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n    synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n    return real_samples_indexes,synthetic_samples_indexes\ndef generate_fe(trn, tst):\n    #tst,target=augment_train(tst,y_train=target)\n    real,syn = detect_test(df_test[features])\n    al = pd.concat([trn,tst,df_test.iloc[real]],axis=0)\n    trn_fe = pd.DataFrame()\n    tst_fe = pd.DataFrame()\n    for c in features:\n        trn[c+\"_test\"]=trn[c].map(al[c].value_counts())\n        trn[c+\"_multi\"] = trn[c+\"_test\"]*trn[c]\n        #trn[c+\"_div\"] = trn[c]\/trn[c+\"_test\"]\n        trn_fe[c] = trn[c]\n        trn_fe[c+\"_test\"] = trn[c+\"_test\"]\n        trn_fe[c+\"_muti\"] = trn[c+\"_multi\"]\n        #trn_fe[c+\"_div\"] = trn[c+\"_div\"]\n        tst[c+\"_test\"]=tst[c].map(al[c].value_counts())\n        #tst[c+\"_test\"] = tst[c+\"_test\"]*tst[c]\n        tst_fe[c] = tst[c]\n        tst[c+\"_multi\"] = tst[c+\"_test\"]*tst[c]\n        #tst[c+\"_div\"] = tst[c]\/tst[c+\"_test\"]\n        tst_fe[c+\"_test\"] = tst[c+\"_test\"]\n        tst_fe[c+\"_muti\"] = tst[c+\"_multi\"]\n        #tst_fe[c+\"_div\"] = tst[c+\"_div\"]\n    return trn_fe, tst_fe\n","b3760920":"def generate_fe_test(tst):\n    re,sy =  detect_test(tst[features])\n    al = pd.concat([df_train,df_test.iloc[re]],axis=0)\n    tst_fe = pd.DataFrame()\n    for c in features:\n        tst[c+\"_test\"]=tst[c].map(al[c].value_counts())\n        #tst[c+\"_test\"] = tst[c+\"_test\"]*tst[c]\n        tst_fe[c] = tst[c]\n        tst[c+\"_multi\"] = tst[c+\"_test\"]*tst[c]\n        #tst[c+\"_div\"] = tst[c]\/tst[c+\"_test\"]\n        tst_fe[c+\"_test\"] = tst[c+\"_test\"]\n        tst_fe[c+\"_muti\"] = tst[c+\"_multi\"]\n        #tst_fe[c+\"_div\"] = tst[c+\"_div\"]\n    return tst_fe\ntest_fe = generate_fe_test(df_test[features])","a840c432":"# MODEL DEF\ndef auc(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\ndef _Model():\n    inp = Input(shape=(200,3))\n    d1 = Dense(32,activation='relu')(inp)\n    d1 = BatchNormalization()(d1)\n    '''d1 = Dense(128,activation='relu')(d1)\n    d1 = Dense(64,activation='relu')(d1)\n    d1 = Dense(32,activation='relu')(d1)\n    d1 = Dense(16,activation='relu')(d1)'''\n    #d2 = Lambda(lambda x: x, output_shape=(400,1))(inp)\n    #d2 = Dense(256,activation='relu')(inp)\n    #d3 = concatenate([d1, d2], axis = 2)\n    ''' d3 = Dense(256,activation=\"relu\")(d3)\n    d3 = Dense(128,activation=\"relu\")(d3)\n    d3 = Dense(64,activation=\"relu\")(d3)\n    d3 = Dense(32,activation=\"relu\")(d3)'''\n    d1 = Dense(8,activation=\"relu\")(d1)\n    d1 = BatchNormalization()(d1)\n    d4 = Flatten()(d1)\n    preds = Dense(1, activation=\"sigmoid\")(d4)\n    model = Model(inputs=inp, outputs=preds)\n    adam = optimizers.Adam(lr=0.009)\n    model.compile(optimizer=adam, loss=K.binary_crossentropy,metrics=[\"acc\"])\n    model.summary()\n    return model","c60fbdef":"# LOGGER\nclass Logger(callbacks.Callback):\n    def __init__(self, out_path='.\/', patience=30, lr_patience=3, out_fn='', log_fn=''):\n        self.auc = 0\n        self.path = out_path\n        self.fn = out_fn\n        self.patience = patience\n        self.lr_patience = lr_patience\n        self.no_improve = 0\n        self.no_improve_lr = 0\n\n    def on_train_begin(self, logs={}):\n        return\n\n    def on_train_end(self, logs={}):\n        return\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_batch_begin(self, batch, logs={}):\n        return\n\n    def on_batch_end(self, batch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        cv_pred = self.model.predict(self.validation_data[0], batch_size=1024)\n        cv_true = self.validation_data[1]\n        auc_val = roc_auc_score(cv_true, cv_pred)\n        if self.auc < auc_val:\n            self.no_improve = 0\n            self.no_improve_lr = 0\n            print(\"Epoch %s - best AUC: %s\" % (epoch, round(auc_val, 4)))\n            self.auc = auc_val\n            self.model.save(self.path + self.fn, overwrite=True)\n        else:\n            self.no_improve += 1\n            self.no_improve_lr += 1\n            print(\"Epoch %s - current AUC: %s\" % (epoch, round(auc_val, 4)))\n            if self.no_improve >= self.patience:\n                self.model.stop_training = True\n            if self.no_improve_lr >= self.lr_patience:\n                lr = float(K.get_value(self.model.optimizer.lr))\n                K.set_value(self.model.optimizer.lr, 0.75*lr)\n                print(\"Setting lr to {}\".format(0.75*lr))\n                self.no_improve_lr = 0\n\n        return","fa22dd26":"#RUN\npreds = []\nc = 0\noof_preds = np.zeros((len(df_train), 1))\ncv = StratifiedKFold(n_splits=5,shuffle=True, random_state=3263)\nfor train, valid in cv.split(df_train, y_train):\n    print(\"VAL %s\" % c)\n    trn = df_train.iloc[train]\n    tst = df_train.iloc[valid]\n    trn, tst = generate_fe(trn, tst)\n    X_train = np.reshape(trn.values, (-1,200,3))\n    y_train_ = y_train.iloc[train].values\n    X_valid = np.reshape(tst.values, (-1,200,3))\n    y_valid = y_train.iloc[valid].values\n    model = _Model()\n    logger = Logger(patience=30, out_path='.\/', out_fn='cv_{}.h5'.format(c))\n    model.fit(X_train, y_train_, validation_data=(X_valid, y_valid), epochs=150, verbose=2, batch_size=1024,\n              callbacks=[logger])\n    model.load_weights('cv_{}.h5'.format(c))\n    fe = [c for c in test_fe.columns if c not in [\"ID_code\",\"target\"]]\n    X_test = np.reshape(test_fe[fe].values, (200000, 200, 3))\n    curr_preds = model.predict(X_test, batch_size=2048)\n    oof_preds[valid] = model.predict(X_valid)\n    preds.append(curr_preds)\n    c += 1\npd.DataFrame(oof_preds).to_csv(\"NN_oof_preds.csv\", index = False)\nauc = roc_auc_score(y_train, oof_preds)\nprint(\"CV_AUC: {}\".format(auc))\n\n# SAVE DATA\npreds = np.asarray(preds)\npreds = preds.reshape((5, 200000))\npreds_final = np.mean(preds.T, axis=1)\nsubmission = pd.read_csv('.\/..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\nsubmission['target'] = preds_final\nsubmission.to_csv('submission.csv', index=False)\n","9b3b9ca7":"### Neural Net","f105a134":"## Load Data","a49dd194":"kkjjjsubmission = pd.read_csv('.\/..\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\nsubmission['target'] = sub1.target \nsubmission.to_csv('submission.csv', index=False)"}}