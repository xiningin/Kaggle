{"cell_type":{"69b7506a":"code","581c8283":"code","34f61df7":"code","b0c38151":"code","478b44af":"code","febacb45":"code","d71f6deb":"code","053c1933":"code","0d441419":"code","13245cfc":"code","a8aa934f":"code","a2c8b80d":"code","276f80d1":"code","714e60dc":"code","0634aa01":"markdown","ca7c90bc":"markdown","73248773":"markdown","3ee16e88":"markdown","d307a9dc":"markdown","625467e8":"markdown"},"source":{"69b7506a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","581c8283":"data = pd.read_csv('..\/input\/column-2C-weka.csv')","34f61df7":"data.head()","b0c38151":"data.tail()","478b44af":"data.info()","febacb45":"Normal   =   data[data[\"class\"] == 'Normal']\nAbnormal =   data[data[\"class\"] == 'Abnormal']","d71f6deb":"f,ax = plt.subplots(figsize = (12,12))\nsns.heatmap (data.corr(),annot = True , fmt = '.1f' ,ax=ax)\nplt.show()","053c1933":"plt.scatter(Normal.pelvic_incidence ,Normal.degree_spondylolisthesis , label = \"Normal\" ,color = \"red\" ,alpha = 0.3)\nplt.scatter(Abnormal.pelvic_incidence  ,Abnormal.degree_spondylolisthesis , label = \"Abnormal\",color = \"green\" ,alpha =0.3)\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"sacral_slope\")\nplt.legend()\nplt.show()","0d441419":"data[\"class\"] = [1 if each == \"Normal\" else 0  for each in data[\"class\"]]\ny = data[\"class\"].values\nx_data = data.drop(\"class\" , axis =1 )","13245cfc":"x = (x_data - np.min(x_data) \/ np.max(x_data) - np.min(x_data)) ","a8aa934f":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state = 1,test_size =0.3)","a2c8b80d":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 17) #n_neighbors is our k value.\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)","276f80d1":"print( \"if k == {} , knn score is = {}\".format(17,knn.score(x_test,y_test)) )","714e60dc":"score_list = []\nfor each in range (1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,20),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","0634aa01":"**Finding the best k value by trying in a row.**","ca7c90bc":"**Normalization**","73248773":"# K - Nearest Neighbour Algorithm Train\n    **Basically we will try to imply this classification algorithm to our data which has normal or abnormal output**\n    ** Our roadmap is goes like; **\n    * Pick up  one k value.\n    * Find the closest data values to our k values.\n    * Count how many classes you have to the closest k neighbour.\n    * Obtain the tested data in which class is it.","3ee16e88":"**Sci-kit learn part**","d307a9dc":"**Correlation map**","625467e8":"# CONCLUSION\n\n        **As you can see in the graph lowest accuracy is around 11 k value on the other hand highest is around 17.That's why will choose 17 as our k value.**"}}