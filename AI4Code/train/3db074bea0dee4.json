{"cell_type":{"12f3fa71":"code","ffd9fdec":"code","0ce67980":"code","c76f6b1a":"code","4c83ec3f":"code","d62e2aa6":"code","f472e6f4":"code","abbc2c52":"code","1118d713":"code","2a98ddd4":"code","f13abe21":"code","237d01e3":"code","e56d82e2":"code","a26c9aaf":"code","80987021":"code","583ff60a":"code","89bb353f":"code","5761b808":"code","613a9ea5":"code","27e186e1":"code","c6ea4572":"code","ac5fdf60":"code","16306334":"code","e0951fc2":"code","36e36cd2":"code","59fd0af1":"code","fd580fbb":"code","8a096afd":"code","c4efd535":"code","b7c79201":"code","209329df":"code","c0e5404f":"code","551eabab":"code","215adb5a":"code","705392e9":"code","99ac7780":"code","a9063190":"code","c85d7f86":"code","016744a9":"code","35a93fed":"code","1c77c04b":"markdown","62a1c4cf":"markdown","d074eaa9":"markdown","d8dfe49d":"markdown","391f99c0":"markdown","e50039d9":"markdown","1c1cef6e":"markdown","9e8825d6":"markdown","3e4372ab":"markdown","67d0ff0c":"markdown","2f097275":"markdown","d1415501":"markdown","3db9d894":"markdown","c845ba2c":"markdown","3a1cd884":"markdown","e05fb0e7":"markdown","12adc07c":"markdown","807ae687":"markdown","c5009da4":"markdown","82441970":"markdown","49348a59":"markdown","5a3b582c":"markdown","32a40716":"markdown","2a3d70bb":"markdown","c8f039cd":"markdown","ffc07644":"markdown","e3d99fb1":"markdown","a780fab5":"markdown","6bbb3f90":"markdown","8213f959":"markdown","a0eb2fb6":"markdown","927af9d6":"markdown","11a1bdc5":"markdown"},"source":{"12f3fa71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ffd9fdec":"## PCA\n## pandas\n## numpy\n## matplotlib.pyplot\n## seaborn\n\n%matplotlib inline","0ce67980":"import pandas as pd\n\ndf = pd.read_excel(\"\/kaggle\/input\/ml-training-vlib\/autos_acp.xls\", sheet_name=0,header=0,index_col=0)\nfinition = df['FINITION']\ndf = df.drop('FINITION', axis = 1)\ndf.head(), df.info(), df.describe(), df.tail(), df.columns, df.shape, df.shape[0], df.shape[1]","c76f6b1a":"#classe pour standardisation\nfrom sklearn.preprocessing import StandardScaler\n#instanciation\nsc = StandardScaler()\n#transformation \u2013 centrage-r\u00e9duction\nZ = sc.fit_transform(df)\n\n#v\u00e9rification - librairie numpy\nimport numpy\n#moyenne\nprint(numpy.mean(Z,axis=0))\n\n#\u00e9cart-type\nprint(numpy.std(Z,axis=0))\n\nprint(Z)","4c83ec3f":"#classe pour l'ACP\nfrom sklearn.decomposition import PCA\n#instanciation\nacp = PCA()\n\n#calculs\ncoord = acp.fit_transform(Z)\n#nombre de composantes calcul\u00e9es\nprint(acp.n_components_) ","d62e2aa6":"#variance expliqu\u00e9e\nprint(acp.explained_variance_)","f472e6f4":"list_acp = [\"CP1\",\"CP2\",\"CP3\",\"CP4\",\"CP5\",\"CP6\",\"CP7\",\"CP8\"]\ndf_acp = pd.DataFrame(list_acp, columns = [\"ACP\"])\ndf_acp['explained_variance'] = acp.explained_variance_","abbc2c52":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\")\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(6, 8))\n\n# Plot the total crashes\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"explained_variance\", y=\"ACP\", data=df_acp,\n            label=\"Total\", color=\"b\")","1118d713":"#cumul de variance expliqu\u00e9e\nplt.plot(list_acp,numpy.cumsum(acp.explained_variance_ratio_))\nplt.title(\"Explained variance vs. # of factors\")\nplt.ylabel(\"Cumsum explained variance ratio\")\nplt.xlabel(\"Factor number\")\nplt.show()","2a98ddd4":"#positionnement des individus dans le premier plan\nfig, axes = plt.subplots(figsize=(12,12))\naxes.set_xlim(-6,6) #m\u00eame limites en abscisse\naxes.set_ylim(-6,6) #et en ordonn\u00e9e\n#placement des \u00e9tiquettes des observations\nfor i in range(18):\n    plt.annotate(df.index[i],(coord[i,0],coord[i,1]))\n#ajouter les axes\nplt.plot([-6,6],[0,0],color='silver',linestyle='-',linewidth=1)\nplt.plot([0,0],[-6,6],color='silver',linestyle='-',linewidth=1)\n#affichage\nplt.show()\n","f13abe21":"di = np.sum(Z**2,axis=1)\ndf_ctr_ind = pd.DataFrame({'ID':df.index,'d_i':di})\ndf_ctr_ind","237d01e3":"df_ctr_ind[df_ctr_ind['d_i'] > df_ctr_ind.d_i.mean()]","e56d82e2":"#contributions aux axes\neigval = (18-1)\/18*acp.explained_variance_\n\nctr = coord**2\nfor j in range(8):\n    ctr[:,j] = ctr[:,j]\/(18*eigval[j])\n\ndf_ctr_cp1cp2 = pd.DataFrame({'id':df.index,'CTR_1':ctr[:,0],'CTR_2':ctr[:,1]})","a26c9aaf":"df_ctr_cp1cp2[df_ctr_cp1cp2['CTR_1'] > df_ctr_cp1cp2.CTR_1.mean()]","80987021":"df_ctr_cp1cp2[df_ctr_cp1cp2['CTR_2'] > df_ctr_cp1cp2.CTR_2.mean()]","583ff60a":"#qualit\u00e9 de repr\u00e9sentation des individus - COS2\ncos2 = coord**2\nfor j in range(8):\n    cos2[:,j] = cos2[:,j]\/di\n    df_ctr_12 = pd.DataFrame({'id':df.index,'COS2_1':cos2[:,0],'COS2_2':cos2[:,1]})\ndf_ctr_12","89bb353f":"df_ctr_12[df_ctr_12['COS2_1'] > df_ctr_12.COS2_1.mean()]","5761b808":"df_ctr_12[df_ctr_12['COS2_2'] > df_ctr_12.COS2_2.mean()]","613a9ea5":"df_ctr_12[df_ctr_12['COS2_1'] > df_ctr_12.COS2_1.mean()].id.append(df_ctr_12[df_ctr_12['COS2_2'] > df_ctr_12.COS2_2.mean()].id).drop_duplicates().reset_index(drop = True)","27e186e1":"#racine carr\u00e9e des valeurs propres\nsqrt_eigval = np.sqrt(eigval)\n\n#corr\u00e9lation des variables avec les axes\ncorvar = np.zeros((8,8))\nfor k in range(8):\n    corvar[:,k] = acp.components_[k,:] * sqrt_eigval[k]\n\n#afficher la matrice des corr\u00e9lations variables x facteurs\nprint(pd.DataFrame(corvar))","c6ea4572":"#on affiche pour les deux premiers axes\nprint(pd.DataFrame({'id':df.columns,'COR_1':corvar[:,0],'COR_2':corvar[:,1]}))","ac5fdf60":"#cercle des corr\u00e9lations\nfig, axes = plt.subplots(figsize=(8,8))\naxes.set_xlim(-1,1)\naxes.set_ylim(-1,1)\n#affichage des \u00e9tiquettes (noms des variables)\nfor j in range(8):\n    plt.annotate(df.columns[j],(corvar[j,0],corvar[j,1]))\n\n#ajouter les axes\nplt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\nplt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n\n#ajouter un cercle\ncercle = plt.Circle((0,0),1,color='blue',fill=False)\naxes.add_artist(cercle)\n#affichage\nplt.show()","16306334":"#cosinus carr\u00e9 des variables\ncos2var = corvar**2\ndf_ctr_variables = pd.DataFrame({'id':df.columns,'COS2_1':cos2var[:,0],'COS2_2':cos2var[:,1]})","e0951fc2":"df_ctr_variables[df_ctr_variables['COS2_1'] > df_ctr_variables['COS2_1'].mean()]","36e36cd2":"df_ctr_variables[df_ctr_variables['COS2_2'] > df_ctr_variables['COS2_2'].mean()]","59fd0af1":"print(finition)","fd580fbb":"#modalit\u00e9s de la variable qualitative\nmodalites = numpy.unique(finition)\nprint(modalites)\n","8a096afd":"#liste des couleurs\ncouleurs = ['red','green','blue']\n\n#faire un graphique en coloriant les points\nfig, axes = plt.subplots(figsize=(12,12))\naxes.set_xlim(-6,6)\naxes.set_ylim(-6,6)\n\n#pour chaque modalit\u00e9 de la var. illustrative\nfor c in range(len(modalites)):\n #num\u00e9ro des individus concern\u00e9s\n numero = numpy.where(finition == modalites[c])\n #les passer en revue pour affichage\n for i in numero[0]:\n    plt.annotate(df.index[i],(coord[i,0],coord[i,1]),color=couleurs[c])\n\n#ajouter les axes\nplt.plot([-6,6],[0,0],color='silver',linestyle='-',linewidth=1)\nplt.plot([0,0],[-6,6],color='silver',linestyle='-',linewidth=1)\n\n#affichage\nplt.show()","c4efd535":"from sklearn import datasets\n\ndigits = datasets.load_digits()\ndigits","b7c79201":"X_digits = digits.data\ny_digits = digits.target","209329df":"X_digits.shape","c0e5404f":"y_digits.shape","551eabab":"pca = PCA()\npca.fit(X_digits)","215adb5a":"pca.explained_variance_ratio_","705392e9":"plt.figure(1, figsize=(8, 5))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_.cumsum(), linewidth=2)\nplt.axis('tight')\nplt.xlabel('n_components')\nplt.ylabel('explained_variance_')","99ac7780":"from sklearn.pipeline import Pipeline\nfrom sklearn import linear_model\n\n# Cr\u00e9ation du pipeline et d\u00e9termination des meilleurs param\u00e8tres\nlogistic = linear_model.LogisticRegression()\npca = PCA()\npipe = Pipeline(steps=[('pca', pca), ('logistic', logistic)])","a9063190":"from sklearn.model_selection import GridSearchCV\n\nn_components = [20, 40, 64]\nCs = np.logspace(-4, 4, 3)\npenalties = [\"l1\", \"l2\"]\n\n#Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:\nestimator = GridSearchCV(pipe,\n                         dict(pca__n_components=n_components,\n                              logistic__C=Cs,\n                              logistic__penalty=penalties), scoring = 'accuracy')\n\nestimator.fit(X_digits, y_digits)","c85d7f86":"print(estimator.best_estimator_)\nprint(X_digits.shape, y_digits.shape)","016744a9":"pd.DataFrame(estimator.cv_results_)","35a93fed":"print(\"Best parameters set found on development set:\")\nprint()\nprint(estimator.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\nprint()\nmeans = estimator.cv_results_['mean_test_score']\nstds = estimator.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, estimator.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"% (mean, std * 2, params))","1c77c04b":"### Question 1:\nImporter Pandas assign\u00e9 \u00e0 pd, numpy assign\u00e9 \u00e0 np, matplotlib.pyplot assign\u00e9 \u00e0 plt, seaborn assign\u00e9 \u00e0 sns et PCA (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html)","62a1c4cf":"# Question 5:\nEn ajoutant cette \u00e9tape nous ajoutonsun param\u00e8tre \u00e0 notre mod\u00e8le de Machine Learning. Pour trouver le bon nombre de composante \u00e0 garder nous allons utiliser un GridSearchCV.\n\n- Importer GridSearchCV\n- Cr\u00e9er une liste n_components avec les valeurs 20, 40, 64\n- Remplir le GridSearchCV et le faire fitter","d074eaa9":"# Question 4:\nLe param\u00e8tre (svd_solver = \u2018full\u2019) indique l\u2019algorithme utilis\u00e9 pour la d\u00e9composition en valeurs singuli\u00e8res. Nous choisissons la m\u00e9thode \u2018\u2019exacte\u2019\u2019, s\u00e9lectionn\u00e9e de toute mani\u00e8re par d\u00e9faut pour l\u2019appr\u00e9hension des bases de taille r\u00e9duite. D\u2019autres approches sont disponibles pour le traitement des grands ensembles de donn\u00e9es. Le nombre de composantes (K) n\u2019\u00e9tant pas sp\u00e9cifi\u00e9 (n_components = None), il est par d\u00e9faut \u00e9gal au nombre de variables (K = p). \n\n- Instancier la PCA() dans une variable appel\u00e9 acp\n- Utiliser fit_transform() sur votre donn\u00e9e et la stock\u00e9 dans la variable coord\n- Afficher le nombre de composantes avec .n_components_","d8dfe49d":"# Question 4:\nNous allons utiliser un mod\u00e8le de classification. La r\u00e9gression logistique. \n\n- Importer la regression logistic avec sklearn https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n- Importer Pipeline de sklearn https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n- Instancier une PCA et une regression logistique\n- Cr\u00e9er un Pipeline avec une \u00e9tape 'pca' et une \u00e9tape 'LR'\n\nL'id\u00e9e du Pipeline est de faire plusieurs t\u00e2ches \u00e0 la suite via une seule et unique fonction.\n\n![](https:\/\/i.ytimg.com\/vi\/WZVVzOa5kK0\/maxresdefault.jpg)","391f99c0":"# Question 7:\nBien que la premi\u00e8re composante repr\u00e9sente plus de 50% de l'information, par convention nous prendrons 2 composantes afin de pouvoir r\u00e9aliser une projection et une analyse dans ce plan. Coordonn\u00e9es factorielles. Les coordonn\u00e9es factorielles (Fik) des individus ont \u00e9t\u00e9 collect\u00e9es dans la variable coord. Nous les positionnons dans le premier plan factoriel avec leurs labels pour situer et comprendre les proximit\u00e9s entre les v\u00e9hicules.\n\nJe ferai deux commentaires au pr\u00e9alable :\n\n1. L\u2019ajout d\u2019une \u00e9tiquette dans un graphique nuage de points n\u2019est pas tr\u00e8s pratique sous Python (librairie Matplotlib).\n\n2. Les outils graphiques calculent souvent automatiquement les \u00e9chelles en fonction des plages de valeurs. Ce n\u2019est pas une bonne id\u00e9e en ce qui concerne l\u2019ACP. En effet, les axes n\u2019ont pas la m\u00eame importance (% de variance restitu\u00e9e). Pour ne pas fausser la perception des proximit\u00e9s, il est tr\u00e8s important de veiller \u00e0 ce que les \u00e9chelles soient identiques en abscisse et en ordonn\u00e9e. Respecter cette r\u00e8gle nous dispense de faire afficher les pourcentages de variance port\u00e9s par les axes. Nous nous rendons compte directement dans notre graphique que les dispersions des individus sont nettement plus marqu\u00e9es sur le premier axe, en abscisse.\n","e50039d9":"L'analyse en composantes principales (ACP ou PCA en anglais pour principal component analysis) est une m\u00e9thode de la famille de l'analyse des donn\u00e9es et plus g\u00e9n\u00e9ralement de la statistique multivari\u00e9e, qui consiste \u00e0 transformer des variables li\u00e9es entre elles (dites \u00ab corr\u00e9l\u00e9es \u00bb en statistique) en nouvelles variables d\u00e9corr\u00e9l\u00e9es les unes des autres. Ces nouvelles variables sont nomm\u00e9es \u00ab composantes principales \u00bb, ou axes principaux. Elle permet au praticien de r\u00e9duire le nombre de variables et de rendre l'information moins redondante.\n\nElle s'utilise dans deux cas:\n- L'analyse Multivari\u00e9e. Dans la partie analyse exploratoire de donn\u00e9es les m\u00e9thodes utilis\u00e9es sont le calcul de statistiques univari\u00e9es (Min, Max etc..), Bi-vari\u00e9es (Corr\u00e9lation, Scatter plot etc...). Cependant pour \u00e9tudier des corr\u00e9lations entre plus de deux variables il est n\u00e9cessaire d'utiliser l'ACP.\n- En big data ou grande dimensionnalit\u00e9, afin de r\u00e9duire le nombre de variables et permettre la mise en place d'un mod\u00e8le de Machine Learning rapidement. Cependant le niveau d'interpr\u00e9tabilit\u00e9 est moindre que si on utlisait les variables bruts.","1c1cef6e":"R\u00e9ponse: 85%","9e8825d6":"# Question 6:\nLe graphique ci dessus nous montre la quantit\u00e9 d'information pour chaque composante. Nous avons besoin d'afficher la quantit\u00e9 d'information mais cette fois ci cumul\u00e9e. Suite \u00e0 cela nous allons pouvoir choisir le nombre de composante gard\u00e9e pour notre analyse statistique.\n\n- Utiliser la fonction .cumsum() pour avoir la quantit\u00e9 de variance expliqu\u00e9e de fa\u00e7on cumulative\n- Afficher avec un graphique - Un simple plot de matplotlib sera suffisant. Utiliser list_acp pour l'axe x","3e4372ab":"# Question 13:\nIl est possible d'ajouter des variables ou des indivus de fa\u00e7on \"illustratives\". Ils ne vont pas rentrer en compte dans le calcul des composantes principales. Par exemple nous pouvons croiser notre ACP avec des variables cat\u00e9gorielle comme celle que nous avons supprim\u00e9 \"FINITION\" afin d'en d\u00e9duire certaines corr\u00e9lations.\n\n- Afficher finition\n- Analyser la nouvelle projection avec les couleurs associ\u00e9s. Que peut on en d\u00e9duire?\n\n","67d0ff0c":"Combien de variance expliqu\u00e9e a t'on pour le plan CP1 & CP2?","2f097275":"Nous repr\u00e9sentons les individus dans le plan factoriel, colori\u00e9es selon la modalit\u00e9 associ\u00e9e de la variable illustrative.","d1415501":"R\u00e9ponse: La variable finition semble corr\u00e9l\u00e9 aux variables de nos composantes principales. Ainsi on retrouvera plutot des individus {1_M : rouge, 2_B : vert, 3_TB : bleu} avec une finition TB \u00e0 droite et des individus avec une finition M \u00e0 gauche. Il semble que la finition soit corr\u00e9l\u00e9 plut\u00f4t \u00e0 la taille du v\u00e9hicule (effet taille sur l'axe 1) qu'\u00e0 la performance brut (effet sportivit\u00e9 sur l'axe 2).\n","3db9d894":"# Question 5:\nLe pourcentage de variance expliqu\u00e9e est la somme des pourcentages de variance expliqu\u00e9e par les axes de la repr\u00e9sentation nous indique, la part de l'information restitu\u00e9e par l'ACP. Plus celle est proche de 100%,meilleure est notre analyse. Souvent, on cherche une repr\u00e9sentation qui restitue au moins 50% de la variance, soit au moins la moiti\u00e9 de l'information totale. Cela permet de quantifier le pourcentage d'information gard\u00e9 ou perdu par notre r\u00e9duction de dimension.\n\n- Afficher le pourcentage de variance expliqu\u00e9e par nos axes. Voir la documentation PCA scikit learn\n- Afficher cela sous forme de graphique (diagrame \u00e0 bar). Pour cela: Cr\u00e9er une liste avec le nom des 6 CP, puis cr\u00e9er un DataFrame avec cette liste. Appeler la colonne: \"ACP\". Dans une seconde colonne portant le nom de \"explained_variance\" stock\u00e9 la variance expliqu\u00e9. Puis utiliser un barplot() de seaborn - https:\/\/seaborn.pydata.org\/examples\/horizontal_barplot.html\n- Que remarquez vous?","c845ba2c":"### Question 3:\nLes donn\u00e9es \u00e9tant exprim\u00e9es dans des unit\u00e9s diff\u00e9rentes, nous avons int\u00e9r\u00eat \u00e0 r\u00e9aliser une ACP norm\u00e9e. Pour cela nous avons besoin de retrancher la moyenne \u00e0 chaque variable puis de diviser par son \u00e9cart type.\n\n- Utiliser StandardScaler() pour normer vos donn\u00e9es - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n- V\u00e9rifier que la moyenne de chaque variables est nulle et que l'\u00e9cart type est de 1.\n\nNB: Il est toujours int\u00e9ressant d'utiliser ce genre de transformation m\u00eame dans un pipeline de Machine Learning.","3a1cd884":"R\u00e9ponse: On associera la CP1 \u00e0 CYL, PUSS, LONG, POIDS & PRIX. On associera la CP2 \u00e0 R-POD PUIS et dans une moindre mesur LONG, LARG, V-MAX.","e05fb0e7":"# Premi\u00e8re partie: Analyse Multivari\u00e9e","12adc07c":"# Question 6:\n- Afficher les r\u00e9sultats du GridSearcheCV - le tableau avec l'ensemble des r\u00e9sultats\n- Afficher les hyper param\u00e8tres\n- A votre avis qu'elles sont les \u00e9tapes suivantes?","807ae687":"# Question 1:\nNous allons r\u00e9aliser un probl\u00e8me de classification avec les variables Composante Principale. Dans notre exemple nous cherchons \u00e0 classer des images repr\u00e9sentants des chiffre compris entre zero et dix. Il est possible de faire cela sans transformation avec du Deep Learning, cependant nous allons utiliser une m\u00e9thode de Machine Learning coupler \u00e0 une ACP afin de r\u00e9pondre \u00e0 notre probl\u00e9matique.\n\nVoici la donn\u00e9e d'entr\u00e9e: \n![](https:\/\/miro.medium.com\/max\/594\/1*Ft2rLuO82eItlvJn5HOi9A.png)!\n\nChaque image est compos\u00e9 de 8*8 pixels soit 64 pixels. Pour pouvoir utiliser une m\u00e9thode de Machine Learning nous devons avoir un individus correspondant \u00e0 une ligne de notre tableau. Pour cela nous allons rendre flat notre image et ainsi avoir 64 variables, 64 descripteurs correspondant \u00e0 l'intensit\u00e9 de noir sur un pixel donn\u00e9e.\n\n- Importer datasets provenant de sklearn\n- Charg\u00e9 la data avec load_digits() et la stocker dans la variable digits\n- Afficher le contenu de digits","c5009da4":"# Question 3:\n- Effectuer une ACP sur X_digits\n- Afficher la variance expliqu\u00e9 par les axes (explained_variance_ratio_)\n- Afficher la variances cumul\u00e9ss expliqu\u00e9es par les axes\n- Emettre une hypoth\u00e8se sur le nombre d'axe repr\u00e9sentant au mieux l'information","82441970":"# Question 8:\nAvant de pouvoir interpr\u00e9ter les r\u00e9sultats il est n\u00e9cessaire de faire 2 choses: Identifier les individus interpr\u00e9tables et identifier les variables de nos composantes principales. Nous allons tout d'abord la contribution de chaque individu \u00e0 la cr\u00e9ation des composante principale. Seul les individus avec une contribution sup\u00e9rieur \u00e0 la moyenne seront interpr\u00e9tables. En ce qui concerne les autres, ils le sont peut \u00eatre sur d'autres plans (CP3 etc...). Qualit\u00e9 de repr\u00e9sentation \u2013 Les COS\u00b2 (cosinus carr\u00e9). Pour calculer la qualit\u00e9 de repr\u00e9sentation des individus sur les axes, nous devons d\u2019abord calculer les carr\u00e9s des distances \u00e0 l\u2019origine des individus, qui correspondent \u00e9galement \u00e0 leur contribution dans l\u2019inertie totale.\n\n- Filtrer les individus avec d_i > \u00e0 la moyenne de d_i\n- Afficher les ID des ces individus\n- Combien d'individus pourront \u00eatre exploit\u00e9s? Ou se trouvent ces individus dans notre plan?","49348a59":"![](https:\/\/slideplayer.fr\/slide\/1172021\/3\/images\/1\/Analyse+en+Composantes+Principales.jpg)!","5a3b582c":"R\u00e9ponse: 40 Semble correct. On a environ 100% de l'information. Les autres axes n'embarques presque rien et semble donc peu d\u00e9terminant pour comparer nos individus","32a40716":"# Deuxi\u00e8me partie: R\u00e9duction de dimension\nAucune analyse sera men\u00e9 dans cette seconde partie. Nous allons nous concentrer sur la r\u00e9duction de dimension et son impl\u00e9mentation dans un pipeline de Machine Learning. Le but ici \u00e9tant avant tout de mettre en place un mod\u00e8le pr\u00e9dictif utilisant moins de dimension que l'on peut retrouver dans notre donn\u00e9e brut. Cela permettra \u00e0 notre mod\u00e8le moins de calcul car moins de dimensions, moins de variables.","2a3d70bb":"# Analyse en composante Principale - ACP","c8f039cd":"# Question 10:\nLes individus ne sont pas tous bien repr\u00e9sent\u00e9s sur nos composantes principales. Dans un premier temps nous avons afficher tous les individus sur nos axes CP1 & CP2. Nous voulons juste afficher ceux qui seront interpr\u00e9tables. Pour cela nous allons calculer la qualit\u00e9 de repr\u00e9sentations d'un individus dans le plan CP1 & CP2.\n\n- Filtrer les individus ayant une bonne repr\u00e9sentation sur l'axe 1\n- Filtrer les individus ayant une bonne repr\u00e9sentation sur l'axe 2\n- Afficher les individus bien repr\u00e9sent\u00e9 sur l'axe1 et axe 2. ","ffc07644":"Vous avez pour mission de manipuler une base de donn\u00e9es de forte volum\u00e9trie et vous souhaitez en tirer des informations pertinentes. Non, pas sous forme d\u2019un banal tableau avec des pourcentages agr\u00e9ment\u00e9 d\u2019un histogramme et d'une moyenne, mais d\u2019une analyse multidimensionnelle, de d\u00e9tection de liaisons qui permettent de segmenter une client\u00e8le afin d\u2019adapter les variables de votre marketing mix, de booster votre knowledge management, bref, de passer \u00e0 la vitesse sup\u00e9rieure.\n\nL\u2019ACP sur les variables constitue \u00e0 cet \u00e9gard un outil puissant, d\u00e8s lors que les variables disponibles sont num\u00e9riques. Voici un exemple avec peu de variables afin de bien comprendre le pipeline d'une Analyse en Composante Principale.\n\nL\u2019ACP consistant  en une r\u00e9duction de dimensionnalit\u00e9, le statisticien peut g\u00e9n\u00e9ralement visualiser l\u2019essentiel de l\u2019espace des variables sur un, deux, voire trois plans comme nous allons le voir. Graphiquement, les points sont projet\u00e9s sur des axes norm\u00e9s sur lesquels sont lues les nouvelles coordonn\u00e9es. Pris deux \u00e0 deux, ceux-ci d\u00e9finissent des plans. L'espace vectoriel des variables est un dual de celui des individus.","e3d99fb1":"R\u00e9ponse sur l'analyse: On retrouvera a droite des individus avec un prix, cylindre, poids, puissance, large, long \u00e9lev\u00e9. A gauche un prix, cylindre, poids, puissance, large, long faible. On retrouvera des individus avec R-Poid.puis \u00e9lev\u00e9 en bas et faible en haut.\n\nOn per\u00e7oit clairement l\u2019effet taille sur le premier axe : les voitures puissantes et rapides sont aussi les plus lourdes et imposantes, la relation globale entre les variables est en r\u00e9alit\u00e9 d\u00e9termin\u00e9e par la cylindr\u00e9e (CYL).","a780fab5":"# Question 9:\nCe que nous venons de voir correspond \u00e0 la contribution de chaque individus pour l'ensemble de l'ACP. Nous nous sommes retreint \u00e0 deux composantes. Nous allons calculer la contribution de chaque individus dans notre plan CP1 et CP2. Puis nous d\u00e9duirons les individus interpr\u00e9table dans ce plan.\n\n- Filtrer les individus avec une contribtuions sup\u00e9rieur \u00e0 la moyenne pour la CP1\n- Idem pour la CP2\n- Afficher ces individus","6bbb3f90":"# Question 12:\nNous allons maintenant \u00e9tudier la qualit\u00e9 de repr\u00e9sentation de chaque variable.\n\n- Filtrer les variables avec un taux de repr\u00e9sentation sup\u00e9rier \u00e0 la moyenne sur l'axe 1\n- Filtrer les variables avec un taux de repr\u00e9sentation sup\u00e9rier \u00e0 la moyenne sur l'axe 2\n- Afficher les variables\n- Affiner l'analyse de la question 11","8213f959":"### Question 2:\nImporter le fichier autos_acp.\n- Importer le fichier - Attention ici nous avons un fichier excel et non un csv. Utiliser pd.read_excel() - Sp\u00e9cifier les options suivantes: sheet_name=0,header=0,index_col=0\n- Afficher le nombre de ligne, de colonnes,\n- Afficher le nom des colonnes\n- D\u00e9crire le dataset avec .info()\n- Sortir les statistiques descriptives basiques\n- Afficher les 5 premi\u00e8re lignes\n- Afficher les 5 derni\u00e8res lignes\n- Stocker la variable FINITION dans finition\n- Supprimer la variable FINITION       ","a0eb2fb6":"# Question 11:\nNous avons une id\u00e9e des individus que nous allons pouvoir \u00e9tudier. La prochaine \u00e9tape est de caract\u00e9riser nos composantes principales. Pour rappel, chaque composante est compos\u00e9 d'une combinaison lin\u00e9aire des variables corr\u00e9l\u00e9s de notre jeu de donn\u00e9es d'entr\u00e9. Chaque composante est d\u00e9corr\u00e9l\u00e9.\n\n- Analyser le cercle des corr\u00e9lation et en d\u00e9duire qu'elles individus seront \u00e0 droite de l'axe 1 et les individus \u00e0 gauche\n- Analyser le cercle des corr\u00e9lation et en d\u00e9duire qu'elles individus seront en haut de l'axe 2 et les individus en bas","927af9d6":"# Question 2:\n- Stcoker la donn\u00e9e (data) de digits dans X_digits. Cela correspond \u00e0 nos variables explicatives\n- Stocker les labels (target) de digits dans Y_digits. Cela correspond \u00e0 notre variable \u00e0 pr\u00e9dire\n- Afficher la taille de X_digits & Y_digits ","11a1bdc5":"La plus grande quantit\u00e9 d'information est toujours repr\u00e9sent\u00e9 par notre premi\u00e8re composante."}}