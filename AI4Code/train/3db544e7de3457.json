{"cell_type":{"7b61c22f":"code","f6426cee":"code","caa245d6":"code","234a9ed4":"code","90da62f4":"code","fb0ac4f7":"code","647e4c29":"code","847e257c":"code","ddbad156":"code","3b20c456":"code","ade7c85d":"code","d9233dfd":"code","69685efe":"code","d51997a8":"code","0d58bb62":"code","04628772":"code","258f418a":"code","4a3ebbb9":"code","4172fd2b":"code","ed327433":"code","cb0f5f2a":"code","88d07dc7":"code","c26a8007":"code","a8e2d2c9":"code","71fc6369":"code","08983552":"code","a3f3fe85":"code","cc8cc100":"code","eae8f527":"code","589afd96":"code","eabd6b80":"code","78035c60":"code","ae55e390":"code","355a9ac2":"code","4a923829":"code","ea447ea5":"code","d9b7ec41":"code","011fc719":"code","22562140":"code","87a142ba":"code","e9bf5872":"code","30c4de8b":"code","64898868":"code","ccb89da1":"code","53e75bbb":"code","7923dab2":"code","26ac8f72":"code","49e6ddf8":"code","e3e52a4b":"code","c97af9a8":"code","eca3e51b":"code","c075808c":"code","d114bcc8":"code","8c7ee9e6":"code","be46d646":"code","229ab946":"code","0828e5e7":"code","f98852e3":"code","22200b65":"code","5248bdc6":"code","f5b8eec1":"code","322dc94e":"code","ff8dbf86":"code","89ec5ab4":"code","6e6854a5":"code","7d77865e":"code","0e8fecbf":"code","8feab73f":"code","d790a9ce":"code","e77463c9":"code","bf2ed484":"code","ed36c27d":"code","0e3e7bef":"code","75ded358":"markdown","ac2b4b15":"markdown","783d6df1":"markdown","d352a22f":"markdown","503f6305":"markdown","f63ac82e":"markdown","4f7d2974":"markdown","b4ad7e50":"markdown","671724d9":"markdown","42de9fdd":"markdown","4fe3ddc6":"markdown","aea228f7":"markdown","097444f4":"markdown","7d02fa1a":"markdown","bce67f8b":"markdown","c91f6d8d":"markdown","2973b930":"markdown","0491a5f4":"markdown","afeb6083":"markdown","f9e8951d":"markdown","7f86432f":"markdown","47fe86ba":"markdown","543190db":"markdown","df643e32":"markdown","b09cdc83":"markdown","bdb412e1":"markdown","6b89287b":"markdown","83773440":"markdown","5fbc48b6":"markdown","4564199a":"markdown","6e1318fd":"markdown","32d07ee8":"markdown","ea394756":"markdown","092cd792":"markdown","53a1f790":"markdown","f1fdc080":"markdown","eed94468":"markdown","1c1fa904":"markdown","4cb44f06":"markdown","5ae136be":"markdown","55eece5f":"markdown","82e26d00":"markdown","a2bf2772":"markdown","b41debdd":"markdown"},"source":{"7b61c22f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n\nimport re\nimport string \nfrom collections import Counter\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly import tools\ninit_notebook_mode(connected=True)","f6426cee":"emails = pd.read_csv('..\/input\/emails.csv')\nquestions = pd.read_csv('..\/input\/questions.csv')\nprofessionals = pd.read_csv('..\/input\/professionals.csv')\ncomments = pd.read_csv('..\/input\/comments.csv')\ntag_users = pd.read_csv('..\/input\/tag_users.csv')\ngroup_memberships = pd.read_csv('..\/input\/group_memberships.csv')\ntags = pd.read_csv('..\/input\/tags.csv')\nstudents = pd.read_csv('..\/input\/students.csv')\ngroups = pd.read_csv('..\/input\/groups.csv')\ntag_questions = pd.read_csv('..\/input\/tag_questions.csv')\nmatches = pd.read_csv('..\/input\/matches.csv')\nanswers = pd.read_csv('..\/input\/answers.csv')\nschool_memberships = pd.read_csv('..\/input\/school_memberships.csv')","caa245d6":"def merging(df1, df2, left, right):\n    return df1.merge(df2, how=\"inner\", left_on=left, right_on=right)","234a9ed4":"qa = merging(questions, answers, \"questions_id\", \"answers_question_id\")\nqa.head(3).T","90da62f4":"qa['questions_date_added'] = pd.to_datetime(qa['questions_date_added'])\nqa['answers_date_added'] = pd.to_datetime(qa['answers_date_added'])\nqa['qa_duration'] = (qa['answers_date_added'] - qa['questions_date_added']).dt.days\n\nqa.head().T","fb0ac4f7":"# after groupby, head(1) returns the first occurrence\nfirst_qa = qa.groupby('questions_id').head(1)\n\n# let's explore data from last year\nfirst_qa = first_qa[first_qa['questions_date_added'] >= pd.datetime(2018, 1, 1)]\n\nfirst_qa.loc[(first_qa['qa_duration'] <= 7), 'week'] = 1\nfirst_qa.loc[(first_qa['qa_duration'] > 7) & (first_qa['qa_duration'] <= 14), 'week'] = 2\nfirst_qa.loc[(first_qa['qa_duration'] > 14) & (first_qa['qa_duration'] <= 21), 'week'] = 3\nfirst_qa.loc[(first_qa['qa_duration'] > 21) & (first_qa['qa_duration'] <= 28), 'week'] = 4\nfirst_qa.loc[(first_qa['qa_duration'] > 28), 'week'] = 5","647e4c29":"week_val_cnt = first_qa['week'].value_counts().sort_index()\n\nplt.figure(figsize=(8,6))\nsns.barplot(week_val_cnt.index, \n            week_val_cnt.values)\n\nplt.xlabel('Week')\nplt.ylabel('Responses')\nplt.title('Responses vs Week')\nplt.show()","847e257c":"def process_text(df, col):\n    df[col] = df[col].str.replace('[^\\w\\s]','') # replacing punctuations\n    df[col] = df[col].str.replace('-',' ') # replacing dashes\n    df[col] = df[col].str.replace('\\d+','') # replacing digits\n    df[col] = df[col].str.lower().str.split() # convert all str to lowercase    \n    df[col] = df[col].apply(lambda x: [item for item in x if item not in stop]) # remove stopwords    \n    df[col] = df[col].apply(' '.join) # convert list to str\n    return df","ddbad156":"first_qa['questions_body'] = process_text(first_qa, 'questions_body')['questions_body']\n\nfast_resp = pd.Series(first_qa[first_qa['week'] == 1]['questions_body'].tolist()).astype(str)\nslow_resp = pd.Series(first_qa[first_qa['week'] == 5]['questions_body'].tolist()).astype(str)\n\ndist_fast = fast_resp.apply(lambda x: len(x.split(' ')))\ndist_slow = slow_resp.apply(lambda x: len(x.split(' ')))","3b20c456":"pal = sns.color_palette()\n\nplt.figure(figsize=(18, 8))\nplt.hist(dist_fast, bins=40, range=[0, 80], color=pal[9], normed=True, label='fast')\nplt.hist(dist_slow, bins=40, range=[0, 80], color=pal[1], normed=True, alpha=0.5, label='slow')\nplt.title('Normalised histogram of word count in question_body', fontsize=15)\nplt.legend()\nplt.xlabel('Number of words', fontsize=15)\nplt.ylabel('Probability', fontsize=15)","ade7c85d":"from wordcloud import WordCloud\n\nall_q = process_text(first_qa, 'questions_body')['questions_body']\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(all_q.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","d9233dfd":"tf = TfidfVectorizer(analyzer='word',\n                     min_df=3,\n                     max_df=0.9,\n                     stop_words='english')\n\n# generate a matrix of sentences and a score for each word\nfast_tfidf_matrix = tf.fit_transform(fast_resp)\n\n# generate a list of words from the vectorizer\nfast_vocab = tf.get_feature_names()\n\n# repeat for slow response\nslow_tfidf_matrix = tf.fit_transform(slow_resp)\nslow_vocab = tf.get_feature_names()","69685efe":"# sum of the scores of each word\n# each row represents a sentence\n# each column represents a word\n# we have to sum across all columns\n\ndef word_score_pair(matrix, vocab):\n    mat_to_arr = matrix.toarray() # convert the 2d matrix to a 2d array\n    word_score = list(map(sum,zip(*mat_to_arr))) # fastest way to sum across all columns\n    rank_words_idx = np.argsort(word_score)\n    idx_list = rank_words_idx[:10]\n    \n    for idx in idx_list:\n        print(\"word: {0}, score: {1:.3f}\".format(vocab[idx], word_score[idx]))","d51997a8":"print('fast_vocab\\'s words and score:')\nword_score_pair(fast_tfidf_matrix, fast_vocab)","0d58bb62":"print('slow_vocab\\'s words and score:')\nword_score_pair(slow_tfidf_matrix, slow_vocab)","04628772":"state_codes = {'District of Columbia' : 'DC','Mississippi': 'MS', 'Oklahoma': 'OK', \n               'Delaware': 'DE', 'Minnesota': 'MN', 'Illinois': 'IL', 'Arkansas': 'AR', \n               'New Mexico': 'NM', 'Indiana': 'IN', 'Maryland': 'MD', 'Louisiana': 'LA', \n               'Idaho': 'ID', 'Wyoming': 'WY', 'Tennessee': 'TN', 'Arizona': 'AZ', \n               'Iowa': 'IA', 'Michigan': 'MI', 'Kansas': 'KS', 'Utah': 'UT', \n               'Virginia': 'VA', 'Oregon': 'OR', 'Connecticut': 'CT', 'Montana': 'MT', \n               'California': 'CA', 'Massachusetts': 'MA', 'West Virginia': 'WV', \n               'South Carolina': 'SC', 'New Hampshire': 'NH', 'Wisconsin': 'WI',\n               'Vermont': 'VT', 'Georgia': 'GA', 'North Dakota': 'ND', \n               'Pennsylvania': 'PA', 'Florida': 'FL', 'Alaska': 'AK', 'Kentucky': 'KY', \n               'Hawaii': 'HI', 'Nebraska': 'NE', 'Missouri': 'MO', 'Ohio': 'OH', \n               'Alabama': 'AL', 'Rhode Island': 'RI', 'South Dakota': 'SD', \n               'Colorado': 'CO', 'New Jersey': 'NJ', 'Washington': 'WA', \n               'North Carolina': 'NC', 'New York': 'NY', 'Texas': 'TX', \n               'Nevada': 'NV', 'Maine': 'ME'}","258f418a":"students['students_location'] = students['students_location'].fillna('')\nstudents['students_location'] = students['students_location'].str.split(',').str[1]\nstudents['students_location'] = students['students_location'].str.lstrip() # remove first white space\n\ns_val_cnt = students['students_location'].value_counts()\ns_val_cnt[:10]","4a3ebbb9":"us_states = []\n\n# only get the location if it's in US\nfor s in s_val_cnt.index.tolist():\n    if s in state_codes:\n        us_states.append(s)","4172fd2b":"df = pd.DataFrame({'states': s_val_cnt.index,\n                   'count': s_val_cnt.values})\n\ndf = df[df['states'].isin(us_states)]\ndf['states'] = df['states'].apply(lambda x: state_codes[x])","ed327433":"data = [ dict(\n        type='choropleth',\n        autocolorscale = True,\n        locations = df['states'], \n        z = df['count'].astype(float), \n        locationmode = 'USA-states', \n        text = df['states'], \n        marker = dict(\n            line = dict (\n                color = 'rgb(255,255,255)',\n                width = 2\n            ) ),\n        colorbar = dict(  \n            title = \"count\")  \n        ) ]\n\nlayout = dict(\n        title = 'Number of Students by State<br>(Hover for breakdown)',\n        geo = dict(\n            scope='usa',\n            projection=dict( type='albers usa' ),\n            showlakes = True,\n            lakecolor = 'rgb(255, 255, 255)'),\n             )\n\nfig = dict(data=data, layout=layout)\niplot(fig)","cb0f5f2a":"qap = merging(qa, professionals, \"answers_author_id\", \"professionals_id\")\nqap.head(3).T","88d07dc7":"p_industry_cnt = professionals['professionals_industry'].value_counts()\n\nplt.figure(figsize=(10,8))\nsns.barplot(p_industry_cnt.index, \n            p_industry_cnt.values,\n            order=p_industry_cnt.iloc[:10].index)\n\nplt.xticks(rotation=90)\nplt.xlabel('professionals_industry', fontsize=16)\nplt.ylabel('counts', fontsize=16)\nplt.title('counts vs professionals_industry', fontsize=18)\nplt.show()","c26a8007":"p_cnt = professionals['professionals_headline'].value_counts()\n\nplt.figure(figsize=(10,8))\nsns.barplot(p_cnt.index, \n            p_cnt.values,\n            order=p_cnt.iloc[1:11].index) # 1 to 11 because we remove NaNs\n\nplt.xticks(rotation=90)\nplt.xlabel('professionals_headline', fontsize=16)\nplt.ylabel('counts', fontsize=16)\nplt.title('counts vs professionals_headline', fontsize=18)\nplt.show()","a8e2d2c9":"qap_author_id = qap['answers_author_id'].value_counts()\n\nplt.figure(figsize=(10,8))\nsns.barplot(qap_author_id.index, \n            qap_author_id.values,\n            order=qap_author_id.iloc[:10].index)\n\nplt.xticks(rotation=90)\nplt.xlabel('answers_author_id', fontsize=16)\nplt.ylabel('counts', fontsize=16)\nplt.title('counts vs answers_author_id', fontsize=18)\nplt.show()","71fc6369":"p = professionals.copy()\np['professionals_location'] = p['professionals_location'].str.split(',').str[1]\n\np_cnt = p['professionals_location'].value_counts()\n\nplt.figure(figsize=(10,8))\nsns.barplot(p_cnt.index, \n            p_cnt.values,\n            order=p_cnt.iloc[0:10].index) # 1 to 11 because we remove NaNs\n\nplt.xticks(rotation=90)\nplt.xlabel('professionals_location', fontsize=16)\nplt.ylabel('counts', fontsize=16)\nplt.title('counts vs professionals_location', fontsize=18)\nplt.show()","08983552":"pa = merging(professionals, answers, \"professionals_id\", \"answers_author_id\")\n\n# get active authors\npa['ans_cnt'] = 1\np = pa.groupby('professionals_id')['ans_cnt'].sum()\nactive_p = (p[p > 5].index).tolist()\nactive_p\n\n# get an updated list of authors\npa['answers_date_added'] = pd.to_datetime(pa['answers_date_added'])\nrecent_p = (pa[pa['answers_date_added'] >= pd.datetime(2018, 1, 1)]['professionals_id']).tolist()\nrecent_p\n\n# get the intersection of both recent and active authors\nactive_recent_p = list(set(recent_p) & set(active_p))\n\nlen(recent_p), len(active_p), len(active_recent_p)","a3f3fe85":"pa = merging(professionals, answers, \"professionals_id\", \"answers_author_id\")\npa.head().T","cc8cc100":"before = pa.iloc[0]['answers_body'][:496]\nbefore","eae8f527":"uri_re = r'(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))'\n\ndef strip_html(s):\n    return re.sub(uri_re, ' ', str(s))","589afd96":"pa['answers_body'] = pa['answers_body'].apply(strip_html)\npa['answers_body'] = pa['answers_body'].str.replace('[^\\w\\s\\n\\t]',' ') # replace punctuations\npa['answers_body'] = pa['answers_body'].str.lower().str.split() # convert all str to lowercase\npa['answers_body'] = pa['answers_body'].apply(lambda x: [item for item in x if item not in stop]) # remove stopwords\npa['answers_body'] = pa['answers_body'].apply(' '.join) # convert list to str","eabd6b80":"after = pa.iloc[0]['answers_body'][:496]\nafter","78035c60":"all_a = pa['answers_body']\ncloud = WordCloud(width=1440, height=1080).generate(\" \".join(all_a.astype(str)))\nplt.figure(figsize=(20, 15))\nplt.imshow(cloud)\nplt.axis('off')","ae55e390":"ttq = merging(tags, tag_questions, \"tags_tag_id\", \"tag_questions_tag_id\")\nqttq = merging(questions, ttq, \"questions_id\", \"tag_questions_question_id\")","355a9ac2":"tqq_list = ttq['tag_questions_question_id'].tolist()\nquestions.shape[0], questions[~questions['questions_id'].isin(tqq_list)].shape[0]","4a923829":"val_cnt = ttq['tags_tag_name'].value_counts()\nto_replace = val_cnt[val_cnt <= 5].index.tolist()\n\nprint(\"Top 10 most popular tags:\")\nprint(val_cnt[:10], '\\n')\nprint(\"Number of unique tags: \", ttq['tags_tag_name'].nunique())\nprint(\"Number of tags that occur 5 times and below: \", len(to_replace))","ea447ea5":"top_10_val_cnt = val_cnt[:10]\n\nfig = {\n    \"data\": [\n    {\n      \"values\": top_10_val_cnt.values,\n      \"labels\": top_10_val_cnt.index,\n      \"domain\": {\"x\": [0, .48]},\n      \"marker\" : dict(colors=[\"#f77b9c\" ,'#ab97db',  '#b0b1b2']),\n      \"name\": \"tag count\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .5,        \n      \"type\": \"pie\"\n    }],\n    \"layout\": {\n      \"title\":\"Tags and Count\",\n      \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 20\n                },\n                \"showarrow\": False,\n                \"text\": \"Tags\",\n                \"x\": 0.2,\n                \"y\": 0.5\n            }]\n    }\n}\n        \niplot(fig, filename='plot-0')","d9b7ec41":"def search_pat(pat, tags_list):\n    sim_pat = []\n    for s in tags_list:\n        if pat in s:\n            sim_pat.append(s)    \n    return sim_pat","011fc719":"tags_list = val_cnt.index.tolist()\nc_idx = []\nc_val = []\n\nfor c in search_pat(\"college\", tags_list)[:10]:\n    c_idx.append(c)\n    c_val.append(val_cnt[c])\n\ndf = pd.DataFrame({'variation of #college': c_idx,\n                   'counts': c_val})","22562140":"fig = {\n    \"data\": [\n    {\n      \"values\": df['counts'],\n      \"labels\": df['variation of #college'],\n      \"domain\": {\"x\": [0, .48]},\n      \"marker\" : dict(colors=[\"#f77b9c\",\"#efbc56\", \"#81a7e8\", \"#e295d0\"]),\n      \"name\": \"count\",\n      \"hoverinfo\":\"label+percent+name\",\n      \"hole\": .5,        \n      \"type\": \"pie\"\n    }],\n    \"layout\": {\n      \"title\":\"#college and Count\",\n      \"annotations\": [\n            {\n                \"font\": {\n                    \"size\": 20\n                },\n                \"showarrow\": False,\n                \"text\": \"#college\",\n                \"x\": 0.16,\n                \"y\": 0.5\n            }]\n    }\n}\n        \niplot(fig, filename='plot-1')","87a142ba":"def multi_single_tags(df, tag):\n    without_tag = df[df['tags_tag_name'] != tag]['tag_questions_question_id'].tolist()\n    with_tag = df[df['tags_tag_name'] == tag]['tag_questions_question_id'].tolist()\n    \n    only_tag = df[~df['tag_questions_question_id'].isin(without_tag)]['tag_questions_question_id'].tolist()\n\n    multiple_tags = list(set(with_tag) - set(only_tag))\n    \n    return multiple_tags, only_tag","e9bf5872":"def remove_multiple(df, tag, ids_multiple, ids_single):\n    df = df[((df['questions_id'].isin(ids_multiple)) & (df['tags_tag_name'] != tag)) | \n             (df['tags_tag_name'] != tag) |\n             (df['questions_id'].isin(ids_single))]\n    return df","30c4de8b":"college_ids_multiple, college_ids_single = multi_single_tags(ttq, \"college\")\n\nprint('Before removing multiple tags containing #college, we have {} questions.'.format(qttq.shape[0]))\n\nqttq = remove_multiple(qttq, \"college\", college_ids_multiple, college_ids_single)\n\nprint('After removing multiple tags containing #college, we are left with {} questions.'.format(qttq.shape[0]))","64898868":"def combine_tags(df):\n    grouped = df.groupby('questions_id')['tags_tag_name'].apply(lambda x: \"%s\" % ', '.join(x))\n    df_c = merging(questions, pd.DataFrame(grouped), \"questions_id\", \"questions_id\")\n    return df_c","ccb89da1":"combine_qttq = combine_tags(qttq)\ncombine_qttq.head().T","53e75bbb":"# qapttq = merging(qap, ttq, \"questions_id\", \"tag_questions_question_id\")\nqapttq = merging(answers, combine_qttq, \"answers_question_id\", \"questions_id\")\nqapttq.head().T","7923dab2":"qapttq.shape[0], combine_qttq.shape[0]","26ac8f72":"noise = ['school','would','like', 'want', 'dont', \n         'become','sure','go', 'get', 'college', \n         'career', 'wanted', 'im', 'ing', 'ive',\n         'know', 'high', 'becom', 'job', 'best',\n         'day', 'hi', 'name', 'help', 'people',\n         'year', 'years', 'next', 'interested', \n         'question', 'questions', 'take', 'even',\n         'though', 'please', 'tell']","49e6ddf8":"def another_process_text(df, col):\n    df[col] = df[col].str.replace('[^\\w\\s]','') # replacing punctuations\n    df[col] = df[col].str.replace('-',' ') # replacing dashes\n    df[col] = df[col].str.replace('\\d+','') # replacing digits\n    df[col] = df[col].str.lower().str.split() # convert all str to lowercase    \n    df[col] = df[col].apply(lambda x: [item for item in x if item not in stop]) # remove stopwords\n    df[col] = df[col].apply(lambda x: [item for item in x if item not in noise])\n    df[col] = df[col].apply(' '.join) # convert list to str\n    return df\n\ndef generate_ngrams(text, N):\n    grams = [text[i:i+N] for i in range(len(text)-N+1)]\n    grams = [\" \".join(b) for b in grams]\n    return grams","e3e52a4b":"df = another_process_text(questions, 'questions_body')","c97af9a8":"df['bigrams'] = df['questions_body'].apply(lambda x : generate_ngrams(x.split(), 2))","eca3e51b":"all_bigrams = []\n\nfor each in df['bigrams']:\n    all_bigrams.extend(each)\n    \nt1 = Counter(all_bigrams).most_common(20)\nx1 = [a[0] for a in t1]\ny1 = [a[1] for a in t1]","c075808c":"fig, axes = plt.subplots(figsize=(15,10))\n\nbar = sns.barplot(y=x1, x=y1)\nbar.set(ylabel='Most frequent bigrams', xlabel='Frequency')","d114bcc8":"def tag_chart(df, what_tag, top):\n    \"\"\"\n    df: the DataFrame\n    what_tag: tags we are looking for\n    top: number of professionals in the chart after filtering\n    \"\"\"    \n    mod_df = df[['answers_author_id', 'tags_tag_name']].copy()\n    mod_df['tag_count'] = 1\n    grouped = mod_df.groupby(['tags_tag_name', 'answers_author_id']).sum()\n    grouped_df = (grouped.reset_index()\n                         .sort_values(['tags_tag_name', 'tag_count'], ascending=False)\n                         .set_index(['answers_author_id']))\n\n    grouped_filter = grouped_df[grouped_df['tags_tag_name'] == what_tag]['tag_count'].reset_index()\n    return grouped_filter.head(top)","8c7ee9e6":"# remerge our qapttq since we modified the tags in the previous one\nqapttq = merging(qap, ttq, \"questions_id\", \"tag_questions_question_id\")\n\ntag_chart(qapttq, \"college\", 5), tag_chart(qapttq, \"engineering\", 5) ","be46d646":"def combine_authors(df):\n    c = df.groupby('questions_id')['answers_author_id'].apply(list)\n    df_c = merging(df, pd.DataFrame(c), 'questions_id', 'questions_id')\n    df_c.drop('answers_author_id_x', axis=1, inplace=True)\n    df_c['answers_author_id_y'] = df_c['answers_author_id_y'].apply(', '.join)\n    df_c.drop_duplicates(inplace=True)\n    return df_c","229ab946":"qa_sub = qa[['questions_title', 'questions_body', 'answers_author_id', 'questions_id']].copy()\n\nqa_cbr = combine_authors(qa_sub)\n\nauthors_link = qa_cbr[['questions_id', 'answers_author_id_y']].copy()\n\nqa_cbr.drop('answers_author_id_y', axis=1, inplace=True)\n\nqa_cbr.head()","0828e5e7":"# hacky way to remove authors who are not linked\nauthors_link = authors_link[authors_link['answers_author_id_y'].str.len() > 33] \n\nauthors_link_dic = authors_link.set_index('questions_id').T.to_dict()","f98852e3":"qa_cbr = process_text(qa_cbr, \"questions_title\") \nqa_cbr = process_text(qa_cbr, \"questions_body\") \n\nqa_cbr.head()","22200b65":"tf = TfidfVectorizer(analyzer='word',\n                     ngram_range=(1,2),\n                     min_df=3,\n                     max_df=0.9,\n                     stop_words='english')\n\ntfidf_matrix = tf.fit_transform(qa_cbr['questions_body'])\ntfidf_matrix.shape","5248bdc6":"cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","f5b8eec1":"# qa_cbr = qa_cbr.reset_index()\nq_titles = qa_cbr['questions_title']\nq_ids = qa_cbr['questions_id']\nindices = pd.Series(qa_cbr.index, index=qa_cbr['questions_title'])\n\nqa_cbr.head()","322dc94e":"def get_recommendations_idx(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    q_indices = [i[0] for i in sim_scores]\n    return q_indices\n\ndef get_recommendations(title):\n    return q_titles.iloc[get_recommendations_idx(title)]\n    \ndef get_questions_id(title):\n    return q_ids.iloc[get_recommendations_idx(title)]    ","ff8dbf86":"get_recommendations('want become army officer become army officer').head(10)","89ec5ab4":"get_questions_id('want become army officer become army officer').head(10)","6e6854a5":"def get_sim_authors(qids):\n    sim_authors = []\n    for qid in qids:\n        if qid in authors_link_dic:\n            sim_authors.append(authors_link_dic[qid]['answers_author_id_y'])\n    return sim_authors","7d77865e":"qids = get_questions_id('want become army officer become army officer').tolist()\n\nqa[qa['questions_id'].isin(qids)].head()","0e8fecbf":"sim_ids = []\nfor all_ids in get_sim_authors(qids):\n    for each_id in all_ids.split(','):\n        sim_ids.append(each_id)\n\nsim_ids = set(sim_ids)\n\nsim_active_recent = set(active_recent_p) & set(sim_ids)\n\nsim_active_recent\n\nprofessionals[professionals['professionals_id'].isin(sim_active_recent)].T","8feab73f":"from sklearn.manifold import TSNE\n\ntsne = TSNE(random_state=0, n_iter=250, metric=\"cosine\")","d790a9ce":"g_q_sample = qa_cbr.sample(frac=.4, random_state=43)\n\ntf = TfidfVectorizer(analyzer='word',\n                     ngram_range=(1,2),\n                     min_df=0,\n                     stop_words='english')\n\ntfidf_matrix = tf.fit_transform(g_q_sample['questions_body'])\ntfidf_matrix.shape\n\ntm = tfidf_matrix.toarray()\n\ntsne_matrix = tsne.fit_transform(tm)\n\ntsne_matrix","e77463c9":"df = g_q_sample.copy()\n\ndf['x'] = tsne_matrix[:, 0]\ndf['y'] = tsne_matrix[:, 1]","bf2ed484":"FS = (10, 8)\nfig, ax = plt.subplots(figsize=FS)\n# Make points translucent so we can visually identify regions with a high density of overlapping points\nax.scatter(df.x, df.y, alpha=.1)","ed36c27d":"FS = (18, 8)\ndef plot_region(x0, x1, y0, y1, text=True):\n    \"\"\"\n    Plot the region of the mapping space bounded by the given x and y limits.\n    \"\"\"    \n    pts = df[\n        (df.x >= x0) & (df.x <= x1)\n        & (df.y >= y0) & (df.y <= y1)\n    ]\n    fig, ax = plt.subplots(figsize=FS)\n    ax.scatter(pts.x, pts.y, alpha=.6)\n    ax.set_xlim(x0, x1)\n    ax.set_ylim(y0, y1)\n    if text:\n        texts = []\n        for label, x, y in zip(pts.questions_title.values, pts.x.values, pts.y.values):\n            t = ax.annotate(label, xy=(x, y))\n            texts.append(t)\n    return ax\n\ndef plot_region_around(title, margin=5, **kwargs):\n    \"\"\"\n    Plot the region of the mapping space in the neighbourhood of the the questions_title. \n    The margin parameter controls the size of the neighbourhood around the movie.\n    \"\"\"\n    xmargin = ymargin = margin\n    match = df[df.questions_title == title]\n    assert len(match) == 1\n    row = match.iloc[0]\n    return plot_region(row.x-xmargin, row.x+xmargin, row.y-ymargin, row.y+ymargin, **kwargs)","0e3e7bef":"# df\nplot_region_around('lifestyle pediatric surgeon', .00005)","75ded358":"# Content Based Recommender\nIdeally, we would like to associate a question to professionals who have been actively answering similar questions.","ac2b4b15":"We are now left with 23,242 unique questions and 49,323 rows - indicating some questions receive multiple answers.","783d6df1":"Merging questions with answers.","d352a22f":"The following block of code basically strips html, replace punctuations, convert all string to lowercase and remove stopwords!","503f6305":"What industry do the professionals come from?","f63ac82e":"Out of the 23,931 questions, 643 questions do not have tags.","4f7d2974":"# Exploring professionals and answers\nLet's take a closer look at the professionals' answers.","b4ad7e50":"Where do the professionals come from? Let's remove the city to get a better context.","671724d9":"After generating a list of questions with multiple tags that contain the tag `college` and a list of questions with only the tag `college`, we proceed to remove those entries that have multiple tags that contain `college`. ","42de9fdd":"`college` is the most popular tag. That might just be an abuse of the tag which does not give a good indication of the nature of the question but its variation does! We should remove `college` tag from questions with multiple tags.","4fe3ddc6":"Wait a second kid, you are asking about machine learning, data analysis, and big data...?\n\n![](https:\/\/pics.me.me\/machine-learnin-machine-learning-everywhere-emegenerator-net-29035157.png)\n\n## IMPORTANT OBSERVATION:\n* Question body of slow responses are LONGER than that of fast response.\n* Question body of slow responses are DIFFICULT to answer! They require more expertise to answer! ","aea228f7":"One, work, good luck. Awesome! ","097444f4":"Who are our biggest heroes?","7d02fa1a":"Unsurprisingly, we see big `college` and `career`.\n\nWhat else differentiates a fast response and slow response question?","bce67f8b":"# Exploring questions_answers (1)\nCan we find out how long does it take for a question to be answered?","c91f6d8d":"# Building tag_chart\nThe below function takes in a tag and returns the professionals who have answered the most questions in the tag category.","2973b930":"Seems like most points are clustered around the center region. Can we generate a better plot?","0491a5f4":"Undeniably, they are related to army!","afeb6083":"Comparing before and after, we did a grea job!","f9e8951d":"# How can we utilize this to pair future questions with authors?\n## To be continued...\n### Please upvote if you find the work useful. Thanks! :))","7f86432f":"To prevent running out of memory, we sample 40% of our data for visualization purposes. Since t-SNE takes in an embedding matrix, we feed it with our tf-idf matrix.","47fe86ba":"# t-SNE visualization\nLet's explore some t-SNE visualization. Essentially, t-SNE learns a mapping from a set of high-dimensional vectors and output the outcome to a space with in 2 dimensions. Credits to DanB for sharing the t-SNE plot code.","543190db":"Hero \"36ff3b3666df400f956f8335cf53e09e\" has answered a total of 693 questions under the tag \"college\" while hero \"c3b4e11154f74a858779be7ba9b6f00c\" has answered a total of 194 questions under the tag \"engineering\".","df643e32":"# Exploring questions and answers (2) and mind-blowing observation","b09cdc83":"We have 17,225 authors who have answered more than 5 questions and 1,766 authors who at least answered a question in year 2018 since the data was collected up to January 31st of 2019.","bdb412e1":"We continue exploring the question body to understand why some questions have longer response time.","6b89287b":"The following recommender system was the work of Rounak Banik. A huge thanks (and credits) to Rounak!","83773440":"# Exploring students\nWhere do the students come from if they are from the US? ","5fbc48b6":"Reading in all the csvs.","4564199a":"We add the x- and y-coordinate calculated by t-SNE to our dataframe.","6e1318fd":"# Introduction\nThe U.S. has almost 500 students for every guidance counselor. Underserved youth lack the network to find their career role models, making CareerVillage.org the only option for millions of young people in America and around the globe with nowhere else to turn.\n\nOur goal is to develop a method to recommend relevant questions to the professionals who are most likely to answer them.\n\nOutline of kernel is as follows:\n\n* Exploring questions and answers (1)\n* Exploring questions and answers (2) and mind-blowing observation\n* Exploring students\n* Exploring professionals (added filtering active professionals)\n* Exploring professionals and answers\n* Exploring tags\n* Exploring questions bigram\n* Building tag_chart\n* Content Based Recommender (added get similar professionals)\n* t-SNE visualization\n\nPlease remember to upvote if you find the work useful! Thank you for visiting.","32d07ee8":"# Questions\nLet's explore the questions body by looking at the most common bigrams. Here, I remove some \"noise\" that are common words, polite expressions, and others deem as unhelpful.","ea394756":"# Exploring professionals\nMerging questions_answers with professionals.","092cd792":"Let's check for active professionals.","53a1f790":"Create a function for merging tables more easily.","f1fdc080":"What are some variations of #college?","eed94468":"3,252 questions have responses within the first week while 3,140 took more than a month.","1c1fa904":"What a messy answers_body! Let's clean it up by stripping html, remove punctuations and stopwords. Credits to Matteo Tosi for his regex pattern. Let's go! ","4cb44f06":"# Exploring tags\nLet's explore the tags by first merging the tags with tag_questions and then merge the tag of each question to our questions_answers_professionals and remove some not so useful features.","5ae136be":"What are the professionals' headline?","55eece5f":"What are some common and rare tags?","82e26d00":"Wow! Seems like longer questions tend to have longer response time.","a2bf2772":"So these are the professionals who have been answering questions related to \"army\"!","b41debdd":"I would say points within the .00005 region of \"lifestyle pediatric surgeon\" are pretty much related."}}