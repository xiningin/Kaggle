{"cell_type":{"7fb03300":"code","0edc493e":"code","af55f88a":"code","dc2bde81":"code","86d3b7a5":"code","7b9e04c3":"code","2bda0b61":"code","419ea1a6":"code","e3838d81":"code","8f5951c2":"code","4a95c274":"code","dddb1e40":"code","cbe35bc6":"code","2890f749":"code","82e4824d":"code","c27bb548":"code","062bce9e":"code","b4f0030a":"code","b8bf82a0":"code","94b11850":"code","7f189583":"code","9dd439b6":"code","52fd9c77":"code","3da1838b":"code","16c5af9d":"code","8b9fc0a7":"code","088c99b6":"code","da93f992":"code","e130f178":"code","481ce246":"code","2585e08d":"code","f19919a3":"code","5feb0c1d":"code","4bf778dd":"code","22770cf1":"code","f2572198":"code","837865ed":"code","013dccbf":"code","eae4c8bb":"code","d23764ce":"markdown","0d99f870":"markdown","a9c096ba":"markdown","249f2232":"markdown","5843cb48":"markdown","e06e1a8d":"markdown","59860034":"markdown","ee70bb26":"markdown","e1041481":"markdown"},"source":{"7fb03300":"import numpy as np\nimport pandas as pd\nimport plotly.express as px\n\nimport re\nfrom nltk.stem import PorterStemmer\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\nimport tensorflow as tf","0edc493e":"data = pd.read_csv('..\/input\/ramen-ratings\/ramen-ratings.csv')","af55f88a":"data","dc2bde81":"data.info()","86d3b7a5":"data.isna().sum()","7b9e04c3":"data['Top Ten'].unique()","2bda0b61":"data['Top Ten'] = data['Top Ten'].replace('\\n', np.NaN)\n\ndata['isTopTen'] = data['Top Ten'].apply(lambda x: 0 if str(x) == 'nan' else 1)\ndata = data.drop('Top Ten', axis=1)\n\ndata = data.drop('Review #', axis=1)","419ea1a6":"data.isna().sum()","e3838d81":"data = data.dropna(axis=0).reset_index(drop=True)","8f5951c2":"data.query(\"Stars == 'Unrated'\")","4a95c274":"data = data.drop(data.query(\"Stars == 'Unrated'\").index, axis=0).reset_index(drop=True)","dddb1e40":"print(\"Total missing values:\", data.isna().sum().sum())","cbe35bc6":"data","2890f749":"ramen_names = data.loc[:, 'Variety']\nramen_names","82e4824d":"ps = PorterStemmer()\n\ndef process_name(name):\n    new_name = name.lower() # Make name lowercase\n    new_name = re.sub(r'[^a-z0-9\\s]', '', new_name) # Remove punctuation\n    new_name = re.sub(r'[0-9]+', 'number', new_name) # Change numerical words to \"number\"\n    new_name = new_name.split(\" \") # Make string into a list of words\n    new_name = list(map(lambda x: ps.stem(x), new_name)) # Stem each word\n    new_name = list(map(lambda x: x.strip(), new_name)) # Removing leading and trailing whitespace\n    for i in range(len(new_name)):\n        if new_name[i] == 'flavour':\n            new_name[i] = 'flavor'\n    if '' in new_name:\n        new_name.remove('') # Remove the empty string if it exists\n    return new_name","c27bb548":"ramen_names = ramen_names.apply(process_name)\nramen_names","062bce9e":"# Getting the number of unique words in our list of ramen names\nvocabulary = set()\n\nfor name in ramen_names:\n    for word in name:\n        if word not in vocabulary:\n            vocabulary.add(word)\n\nvocab_length = len(vocabulary)\n\n\n# Getting the maximum length of a single ramen name\nmax_seq_length = max(ramen_names.apply(lambda x: len(x)))\n\n\n# Print results\nprint(\"       Vocab length:\", vocab_length)\nprint(\"Max sequence length:\", max_seq_length)","b4f0030a":"tokenizer = Tokenizer(num_words=vocab_length)\ntokenizer.fit_on_texts(ramen_names)\n\nword_index = tokenizer.word_index\n\nsequences = tokenizer.texts_to_sequences(ramen_names)\n\n\nname_features = pad_sequences(sequences, maxlen=max_seq_length, padding='post')","b8bf82a0":"name_features","94b11850":"data = data.drop('Variety', axis=1)","7f189583":"data","9dd439b6":"def onehot_encode(df, columns, prefixes):\n    df = df.copy()\n    for column, prefix in zip(columns, prefixes):\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df","52fd9c77":"data = onehot_encode(\n    data,\n    ['Brand', 'Style', 'Country'],\n    ['B', 'S', 'C']\n)","3da1838b":"data","16c5af9d":"labels = data.loc[:, 'isTopTen']\n\nother_features = data.drop('isTopTen', axis=1)","8b9fc0a7":"scaler = StandardScaler()\n\nother_features = pd.DataFrame(scaler.fit_transform(other_features), columns=other_features.columns)","088c99b6":"name_features_series = pd.Series(list(name_features), name='Name')","da93f992":"features = pd.concat([name_features_series, other_features], axis=1)\nfeatures","e130f178":"X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.7, random_state=34)","481ce246":"X_train_1 = np.stack(X_train['Name'].to_numpy())\nX_train_2 = X_train.drop('Name', axis=1)\n\nX_test_1 = np.stack(X_test['Name'].to_numpy())\nX_test_2 = X_test.drop('Name', axis=1)","2585e08d":"name_features.shape","f19919a3":"other_features.shape","5feb0c1d":"class_weights = dict(\n    enumerate(\n        class_weight.compute_class_weight(\n            'balanced',\n            y_train.unique(),\n            y_train\n        )\n    )\n)\n\nclass_weights","4bf778dd":"embedding_dim = 64\n\n# Training on name features\nname_inputs = tf.keras.Input(shape=(13,), name='name_inputs')\n\nbatch_norm = tf.keras.layers.BatchNormalization(name='batch_norm')(name_inputs)\n\nname_embedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length,\n    name='name_embedding'\n)(batch_norm)\n\nname_outputs = tf.keras.layers.Flatten(name='name_flatten')(name_embedding)\n\n# Training on other features\nother_inputs = tf.keras.Input(shape=(401,), name='other_inputs')\n\nhidden = tf.keras.layers.Dense(64, activation='relu', name='dense_1')(other_inputs)\nother_outputs = tf.keras.layers.Dense(64, activation='relu', name='dense_2')(hidden)\n\n\n# Concatenate outputs and make predictions\nconcat = tf.keras.layers.concatenate([name_outputs, other_outputs], name='concatenate')\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')(concat)\n\n\n# Constructing and plotting model\nmodel = tf.keras.Model(inputs=[name_inputs, other_inputs], outputs=outputs)\n\ntf.keras.utils.plot_model(model)","22770cf1":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc'),\n        tf.keras.metrics.Precision(name='prec'),\n        tf.keras.metrics.Recall(name='rec')\n    ]\n)\n\n\nbatch_size = 64\nepochs = 100\n\nhistory = model.fit(\n    [X_train_1, X_train_2],\n    y_train,\n    validation_split=0.2,\n    class_weight=class_weights,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau()\n    ],\n    verbose=0\n)","f2572198":"fig = px.line(\n    history.history,\n    y=['loss', 'val_loss'],\n    labels={'x': \"Epoch\", 'y': \"Loss\"},\n    title=\"Loss Over Time\"\n)\n\nfig.show()","837865ed":"results = model.evaluate([X_test_1, X_test_2], y_test)\n\nprint(f\"\\n Accuracy: {results[1]:.5f}\")\nprint(f\"      AUC: {results[2]:.5f}\")\nprint(f\"Precision: {results[3]:.5f}\")\nprint(f\"   Recall: {results[4]:.5f}\")","013dccbf":"y_test.value_counts()","eae4c8bb":"print(f\"Percent of ramens that are Top 10: {y_test.mean() * 100:.1f}%\")","d23764ce":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/qFBWzxy6CDY","0d99f870":"A high recall is very important here, since we care much more about identifying which ramens are Top 10 than we do about getting predictions correct.  \n  \nBelow, we can see that positive examples (Top 10 ramens) are very scarce in the data.","a9c096ba":"# Engineering Variety Features","249f2232":"# Getting Remaining Features Ready","5843cb48":"# Task for Today  \n\n***\n\n## Ramen Top 10 Prediction  \n\nGiven *data about ramen ratings*, let's try to predict whether a given ramen will be a **Top 10 ramen** or not.  \n  \nWe will use a multi-input TensorFlow ANN to make our predictions.","e06e1a8d":"# Preprocessing","59860034":"# Getting Started","ee70bb26":"# Results","e1041481":"# Modeling and Training"}}