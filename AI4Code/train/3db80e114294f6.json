{"cell_type":{"d95c089e":"code","75e46e2c":"code","c295bc55":"code","8bded543":"code","3c42910b":"code","3ddd268e":"code","e047fbd3":"code","0dbfe2b9":"code","c9739b90":"code","7565989c":"code","5cafc52d":"code","0be34bea":"code","0747008a":"code","22b7bfd7":"code","a2fe63ec":"code","3f2ea8d6":"code","d207bc26":"code","214b7ee5":"code","2c9aed73":"code","a9f44d8d":"code","89a7fb1d":"code","713c1fe3":"code","4379c537":"code","a654440d":"code","86f419ca":"code","3eb4c4b8":"code","b5c7b919":"code","c9f54935":"code","7d06523f":"code","0bfa5b22":"code","e414bfae":"code","44b040ed":"code","eb89460a":"code","54eb0b8b":"code","d3764ac6":"code","f2b16bd2":"code","d5b7460a":"code","71f40e4c":"code","c676ecf0":"code","e14a35f9":"code","d0bd0ccd":"code","6997be27":"code","52723939":"code","261e3978":"code","82a170d3":"code","b03cffec":"code","4556f54c":"code","2632dd18":"code","26ef180d":"code","20b6f08c":"code","b0dec7f3":"code","0a7d8eec":"code","92cabc6a":"code","63a59361":"code","8c9f6df4":"code","9bc3f1cb":"code","b1897c65":"code","9abb8fbe":"code","dfc7d2f5":"code","99f0e587":"code","9bbeb988":"code","24999cac":"code","0abaa196":"code","7baeb446":"code","d39cf470":"code","80f1619d":"code","93b43bf7":"code","8bd632d0":"code","74b0714d":"code","29d16e93":"code","20b9b433":"code","abdf0481":"code","160dd260":"code","80d7c309":"code","bf8a77f2":"code","6c89bfd3":"code","27dc83e1":"code","ab79f10b":"code","f549f9d3":"code","a3ff2295":"code","429967fc":"code","04b6aee3":"code","641a2872":"code","18a06833":"code","d18d8943":"code","7d9cf3dd":"code","789a2758":"code","823c75af":"code","fabb8e7f":"code","76999282":"code","dd5289f6":"code","bc281874":"code","37315900":"code","d6a9cf40":"code","176f6657":"code","4e95be4e":"code","993f0f6b":"code","ccb9b386":"code","eb8df8b3":"code","78bcee16":"code","66ecce78":"code","9ea26ec1":"code","10a91bca":"code","d341a65a":"code","88ebbf25":"code","59aed429":"code","431e639c":"code","41c6f4b1":"markdown","6cb667a6":"markdown","5b58be75":"markdown","deff9db5":"markdown","ff5a04ba":"markdown","7b470396":"markdown","3842215b":"markdown","da86f6f7":"markdown","f185f8d1":"markdown","da936fea":"markdown","0636a441":"markdown","c9c855d9":"markdown","059675b0":"markdown","4a1ed989":"markdown"},"source":{"d95c089e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","75e46e2c":"data = pd.read_csv(\"..\/input\/paris-housing-classification\/ParisHousingClass.csv\")\ndata.head()","c295bc55":"data.info()","8bded543":"data.describe()","3c42910b":"data.shape","3ddd268e":"dict = {}\nfor i in list(data.columns):\n  dict[i] = data[i].value_counts().shape[0]\n\npd.DataFrame(dict , index=['count']).transpose()","e047fbd3":"data.isnull().sum()","0dbfe2b9":"sns.heatmap(data.isnull())","c9739b90":"len(data)-len(data.drop_duplicates())\n","7565989c":"data.columns","5cafc52d":"data.hist(bins = 50, figsize = (20,20))\nplt.show()","0be34bea":"data.plot(kind = \"box\" , subplots = True , figsize = (18,15) , layout =(6,3))\nplt.show()","0747008a":"# Boxplot showed no outlier","22b7bfd7":"data.plot(kind = \"density\" , subplots = True , figsize = (12,8) , layout = (6,3))","a2fe63ec":"data['category'].value_counts()","3f2ea8d6":"sns.countplot(x='category', data=data)","d207bc26":"data['category'].hist(bins=10)\nplt.show()","214b7ee5":"sns.countplot(x='hasYard', hue='category',data=data)","2c9aed73":"# all luxury has yard","a9f44d8d":"sns.countplot(x='hasStorageRoom', hue='category',data=data)","89a7fb1d":"# NO effect of storage room on luxury","713c1fe3":"sns.countplot(x='hasPool', hue='category',data=data)","4379c537":"# all luxury has pool","a654440d":"sns.countplot(x='isNewBuilt', hue='category',data=data)","86f419ca":"# all luxury is new built","3eb4c4b8":"plt.figure(figsize=(15,8))\nsns.countplot(x='made', hue='category',data=data)\nplt.show()","b5c7b919":"#Made year has no effect","c9f54935":"plt.figure(figsize=(15,8))\nsns.heatmap(data.corr(), annot= True, cbar=True)\nplt.show()","7d06523f":"#only price and squaremeters have strong dependencies","0bfa5b22":"sns.countplot(x='isNewBuilt', hue='category',data=data)","e414bfae":"# all luxury are new built","44b040ed":"from sklearn.preprocessing import LabelEncoder","eb89460a":"le = LabelEncoder()","54eb0b8b":"Activity_le=LabelEncoder()\ndata['category'] = Activity_le.fit_transform(data['category'])\ndata","d3764ac6":"plt.figure(figsize=(15,8))\nsns.heatmap(data.corr(), annot= True, cbar=True)\nplt.show()","f2b16bd2":"# Target( luxury or basic is correlated to hasYard, hasPool and isnewBuilt)\n#There is a stong correlation between price and squareMeters. so, we decided to drop squareMeter to apply the model","d5b7460a":"data.drop(['squareMeters'], axis=1, inplace =True)\ndata","71f40e4c":"X =data.drop(['category'], axis = 1).values\ny = data['category'].values","c676ecf0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","e14a35f9":"# Feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\n\nX_test = sc.transform(X_test)","d0bd0ccd":"from sklearn.tree import DecisionTreeClassifier","6997be27":"dt = DecisionTreeClassifier(max_depth = 5, max_features= 7)","52723939":"dt.fit(X_train, y_train)","261e3978":"dt.score(X_train, y_train)","82a170d3":"dt.score(X_test, y_test)\n","b03cffec":"dt_pred = dt.predict(X_test)","4556f54c":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, dt_pred)\nprint(cm)","2632dd18":"plt.figure(figsize = (8,4))\nsns.heatmap(cm , annot = True, cmap=\"Blues_r\")","26ef180d":"from sklearn import tree\nfig = plt.figure(figsize = (15,12))\ntree.plot_tree(dt , filled = True)","20b6f08c":"from matplotlib import pyplot as plt\n\ndef f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\n# whatever your features are called\nfeatures_names = ['squareMeters', 'numberOfRooms', 'hasYard', 'hasPool', 'floors',\n       'cityCode', 'cityPartRange', 'numPrevOwners', 'made', 'isNewBuilt',\n       'hasStormProtector', 'basement', 'attic', 'garage', 'hasStorageRoom',\n       'hasGuestRoom', 'price']\n    \n#rf =RandomForestClassifier(n_estimators=4 , max_depth=3 , min_samples_split=25 , max_features=4)\n#rf.fit(x_train,y_train)\n\n# Specify your top n features you want to visualize.\n# You can also discard the abs() function \n# if you are interested in negative contribution of features\nf_importances(abs(dt.feature_importances_), features_names, top=12)","b0dec7f3":"from sklearn.ensemble import RandomForestClassifier","0a7d8eec":"rf = RandomForestClassifier(max_depth= 3 , max_features= 6 , random_state= 42)","92cabc6a":"rf.fit(X_train , y_train)","63a59361":"rf.score(X_train,y_train)","8c9f6df4":"rf.score(X_test,y_test)","9bc3f1cb":"from matplotlib import pyplot as plt\n\ndef f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\n# whatever your features are called\nfeatures_names = ['squareMeters', 'numberOfRooms', 'hasYard', 'hasPool', 'floors',\n       'cityCode', 'cityPartRange', 'numPrevOwners', 'made', 'isNewBuilt',\n       'hasStormProtector', 'basement', 'attic', 'garage', 'hasStorageRoom',\n       'hasGuestRoom', 'price']\n\n# Specify your top n features you want to visualize.\n# You can also discard the abs() function \n# if you are interested in negative contribution of features\nf_importances(abs(rf.feature_importances_), features_names, top=12)","b1897c65":"rf_pred = rf.predict(X_test)","9abb8fbe":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, rf_pred)\nprint(cm)","dfc7d2f5":"plt.figure(figsize = (8,4))\nsns.heatmap(cm , annot = True, cmap=\"Oranges_r\")","99f0e587":"data.head()","9bbeb988":"# predict new houses category\nnew_house = [4,1,0,5,27939,9,3,2018,1,1,500,10000,120,1,1,6574642]\n","24999cac":"rf.predict([new_house])","0abaa196":"rf.predict_proba([new_house])\n","7baeb446":"pip install xgboost","d39cf470":"from xgboost import XGBClassifier","80f1619d":"xgb = XGBClassifier(learning_rate= 0.4 , max_depth= 2 , objective=\"binary:logistic\")","93b43bf7":"xgb.fit(X_train , y_train)","8bd632d0":"xgb.score(X_train,y_train)","74b0714d":"xgb.score(X_test,y_test)\n","29d16e93":"xg_pred = xgb.predict(X_test)","20b9b433":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, xg_pred)\nprint(cm)","abdf0481":"plt.figure(figsize = (8,4))\nsns.heatmap(cm , annot = True, cmap=\"YlOrBr\")","160dd260":"from matplotlib import pyplot as plt\n\ndef f_importances(coef, names, top=-1):\n    imp = coef\n    imp, names = zip(*sorted(list(zip(imp, names))))\n\n    # Show all features\n    if top == -1:\n        top = len(names)\n\n    plt.barh(range(top), imp[::-1][0:top], align='center')\n    plt.yticks(range(top), names[::-1][0:top])\n    plt.title('feature importances')\n    plt.show()\n\n# whatever your features are called\nfeatures_names = ['squareMeters', 'numberOfRooms', 'hasYard', 'hasPool', 'floors',\n       'cityCode', 'cityPartRange', 'numPrevOwners', 'made', 'isNewBuilt',\n       'hasStormProtector', 'basement', 'attic', 'garage', 'hasStorageRoom',\n       'hasGuestRoom', 'price']\n\n# Specify your top n features you want to visualize.\n# You can also discard the abs() function \n# if you are interested in negative contribution of features\nf_importances(abs(xgb.feature_importances_), features_names, top=12)\n","80d7c309":"#Changing learning rate\nxgb = XGBClassifier(learning_rate= 0.5 , max_depth= 3 , objective=\"binary:logistic\")","bf8a77f2":"xgb.fit(X_train , y_train)","6c89bfd3":"xgb.score(X_train,y_train)","27dc83e1":"xgb.score(X_test,y_test)\n","ab79f10b":"# Fitting SVM to the Training set\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = \"linear\", random_state = 0)\nclassifier.fit(X_train, y_train)\n","f549f9d3":"classifier.score(X_train,y_train)","a3ff2295":"classifier.score(X_test,y_test)\n","429967fc":"sv_pred = classifier.predict(X_test)","04b6aee3":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, sv_pred)\nprint(cm)","641a2872":"plt.figure(figsize = (8,4))\nsns.heatmap(cm , annot = True, cmap=\"Greens_r\")","18a06833":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)","d18d8943":"classifier.fit(X_train, y_train)","7d9cf3dd":"y_pred = classifier.predict(X_test)","789a2758":"classifier.classes_","823c75af":"classifier.intercept_","fabb8e7f":"classifier= LogisticRegression(C=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(classifier.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(classifier.score(X_test, y_test)))","76999282":"classifier= LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(classifier.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(classifier.score(X_test, y_test)))","dd5289f6":"lg_pred = classifier.predict(X_test)","bc281874":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, lg_pred)\nprint(cm)","37315900":"plt.figure(figsize = (8,4))\nsns.heatmap(cm , annot = True, cmap=\"viridis\")","d6a9cf40":"from sklearn.neighbors import KNeighborsClassifier\n","176f6657":"knn = KNeighborsClassifier(n_neighbors=5)\n","4e95be4e":"knn.fit(X_train,y_train)\n","993f0f6b":"print(knn.score(X_test, y_test))\n","ccb9b386":"print(knn.score(X_train, y_train))\n","eb8df8b3":"# Setup arrays to store train and test accuracies\nn_neighbors = np.arange(1, 20)\ntrain_accuracy = np.empty(len(n_neighbors))\ntest_accuracy = np.empty(len(n_neighbors))\n# Loop over different values of k\nfor i, k in enumerate(n_neighbors):\n# Setup a k-NN Classifier with k neighbors: knn\n knn = KNeighborsClassifier(k)\n# Fit the classifier to the training data\n knn.fit(X_train, y_train)\n train_accuracy[i] = knn.score(X_train,y_train)\n test_accuracy[i] = knn.score(X_test, y_test)\n","78bcee16":"# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(n_neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(n_neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","66ecce78":"kn_pred = knn.predict(X_test)","9ea26ec1":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, kn_pred)\nprint(cm)","10a91bca":"plt.figure(figsize = (8,4))\nsns.heatmap(cm , annot = True, cmap=\"Reds_r\")","d341a65a":"from sklearn.metrics import accuracy_score ","88ebbf25":"f1_score=accuracy_score\ndt_f1 =f1_score(y_test, dt_pred)\nknn_f1 = f1_score(y_test, kn_pred)\nsvm_f1 = f1_score(y_test, sv_pred)\nRF_f1 = f1_score(y_test, rf_pred)\nlogreg_f1 = f1_score(y_test, lg_pred)","59aed429":"x=['Decision Tree','KNN','Random Forest','SVM','Logistic Regression']\ny=[dt_f1,knn_f1,RF_f1,svm_f1,logreg_f1]\n","431e639c":"plt.figure(figsize=(20,20))\nfig, ax = plt.subplots()\nax.bar(x, y, width=0.8)\nplt.title('F1 Score Of Our Model')\nplt.xlabel('Model')\nplt.ylabel('F1 Score')\nplt.show()","41c6f4b1":"# KNeighbors\n","6cb667a6":"#### 1.2 Task <a id=3><\/a>\nTo predict house category","5b58be75":"#### 1.1 Data Dictionary <a id=2><\/a>\n\n'squareMeters'\n\n'numberOfRooms'\n\n'hasYard'\n\n'hasPool'\n\n'floors' - number of floors\n\n'cityCode' - zip code\n\n'cityPartRange' - the higher the range, the more exclusive the neighbourhood is\n\n'numPrevOwners' - number of prevoious owners\n\n'made' - year\n\n'isNewBuilt'\n\n'hasStormProtector'\n\n'basement' - basement square meters\n\n'attic' - attic square meteres\n\n'garage' - garage size\n\n'hasStorageRoom'\n\n'hasGuestRoom' - number of guest rooms\n\n'price' - price of a house\n\n'category' - Luxury or Basic","deff9db5":"# Models","ff5a04ba":"#### 2.1 Packages <a id=5><\/a>","7b470396":"# XGBoost","3842215b":"# Label encoding","da86f6f7":"<h1 align=\"center\">Paris housing category  <\/h1>","f185f8d1":"# Random Force","da936fea":"# Decision Tree","0636a441":"# EDA","c9c855d9":"# Logistic function","059675b0":"# Support Victor Machine","4a1ed989":"#### 2.2 Data <a id=5><\/a>"}}