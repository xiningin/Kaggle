{"cell_type":{"833728d0":"code","47fa9689":"code","2961f5a5":"code","a4343e22":"code","ebcc4191":"code","e0858f64":"code","ffb102e3":"code","752e37a4":"code","896afb1e":"code","47bfa45d":"code","b4ea89bb":"code","56af7f4d":"code","18bc5962":"code","6770335d":"code","6481e92c":"code","329e1cd2":"code","995bd852":"code","41815f98":"code","2c4f59ab":"code","d1037114":"code","1d4ae4d4":"code","e9631041":"code","a22410ed":"code","02b342a6":"code","4823c5a3":"code","5c59d357":"code","8a82f050":"code","39a7523d":"code","682df0cb":"code","1bff3b44":"code","33ed5396":"code","308cc08e":"code","30670a2c":"markdown","9d2de0ac":"markdown"},"source":{"833728d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import StratifiedKFold\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","47fa9689":"data=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\n","2961f5a5":"## check sentences with less than 5 characters\n\nfor i in range(len(data)):\n    if len(data[\"premise\"][i])<5 or len(data[\"hypothesis\"][i])<5:\n        print(data[\"premise\"][i]+ \">>\"+ data[\"hypothesis\"][i])","a4343e22":"## check max avg and min len of sentences\npremise=[]\nhypothesis=[]\n\nfor i in range(len(data)):\n    premise.append(len(data[\"premise\"][i]))\n    hypothesis.append(len(data[\"hypothesis\"][i]))\n    \nprint(\"Average len of characters in premise\",sum(premise)\/\/len(premise))\nprint(\"Maximum len of characters in premise\",max(premise))\nprint(\"Minimum len of characters in premise\",min(premise),end=\"\\n\\n\")\n\nprint(\"Average len of characters in hypothesis\",sum(hypothesis)\/\/len(hypothesis))\nprint(\"Maximum len of characters in hypothesis\",max(hypothesis))\nprint(\"Minimum len of characters in hypothesis\",min(hypothesis))\n\n","ebcc4191":"language_data=pd.DataFrame(data[\"language\"].value_counts()).reset_index().rename(columns={\"index\":\"language\",\"language\":\"counts\"})","e0858f64":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(20,5))\n\nsns.set(style=\"whitegrid\")\nax = sns.barplot(x=language_data[\"language\"], y=language_data[\"counts\"])","ffb102e3":"languages4plot=[]\nPercentage=[]\nfor ind in range(len(language_data)):\n    Percentage.append(round((language_data[\"counts\"][ind]\/sum(language_data[\"counts\"]))*100,2))\n    languages4plot.append(language_data['language'][ind])\n    ","752e37a4":"explode=np.random.uniform(0,0,len(Percentage))\nplt.figure(figsize=(10,10))\nplt.pie(Percentage, explode=explode, labels=languages4plot, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","896afb1e":"from transformers import RobertaConfig, RobertaModel,RobertaTokenizer\nfrom transformers import BertTokenizer, TFBertModel, AutoTokenizer, TFAutoModel\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\",do_lower_case=True)\n","47bfa45d":"tokenize_premise=[]\ntokenize_hypothesis=[]\nlanguages=[]\n\nfor ind in range(len(data)):\n    tokenize_premise.append(len(list(tokenizer.tokenize(data[\"premise\"][ind], return_tensors=\"tf\"))))\n    tokenize_hypothesis.append(len(list(tokenizer.tokenize(data[\"hypothesis\"][ind], return_tensors=\"tf\"))))\n    languages.append(data[\"language\"][ind])\n    ","b4ea89bb":"print(\"Average len of premise\",sum(tokenize_premise)\/\/len(tokenize_premise))\nprint(\"Maximum len of premise\",max(tokenize_premise))\nprint(\"Minimum len of premise\",min(tokenize_premise),end=\"\\n\\n\")\n\nprint(\"Average len of hypothesis\",sum(tokenize_hypothesis)\/\/len(tokenize_hypothesis))\nprint(\"Maximum len of hypothesis\",max(tokenize_hypothesis))\nprint(\"Minimum len of hypothesis\",min(tokenize_hypothesis))\n","56af7f4d":"plt.figure(figsize=(20,5))\n\nsns.set(style=\"whitegrid\")\nax = sns.scatterplot(x=languages, y=tokenize_premise)","18bc5962":"plt.figure(figsize=(20,5))\n\nsns.set(style=\"whitegrid\")\nax = sns.scatterplot(x=languages, y=tokenize_hypothesis)\n","6770335d":"def encode_sentence(s):\n   tokens = list(tokenizer.tokenize(s))\n   tokens.append('[SEP]')\n   token_ids=tokenizer.convert_tokens_to_ids(tokens)\n   return token_ids","6481e92c":"def bert_encode(hypotheses, premises, tokenizer):\n    \n  num_examples = len(hypotheses)\n  \n  sentence1 = tf.ragged.constant([\n      encode_sentence(s)\n      for s in np.array(hypotheses)])\n  sentence2 = tf.ragged.constant([\n      encode_sentence(s)\n       for s in np.array(premises)])\n    \n  \n\n  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n  input_mask = tf.ones_like(input_word_ids).to_tensor()\n\n  type_cls = tf.zeros_like(cls)\n  type_s1 = tf.zeros_like(sentence1)\n  type_s2 = tf.ones_like(sentence2)\n  input_type_ids = tf.concat(\n      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n  inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_mask': input_mask,\n      'input_type_ids': input_type_ids}\n\n  return inputs","329e1cd2":"data=data.sample(frac=1).reset_index(drop=True)\n","995bd852":"X_data=data[[\"premise\", \"hypothesis\"]]\nY_data=data[\"label\"]","41815f98":"count_df=pd.DataFrame(Y_data.value_counts()).reset_index().rename(columns={\"index\":\"label\",\"label\":\"counts\"})\nlabels=list(count_df[\"label\"])\nsizes=[(i\/len(Y_data))*100 for i in count_df[\"counts\"]]\nexplode = (0, 0.1, 0)","2c4f59ab":"plt.figure(figsize=(10,5))\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nplt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","d1037114":"tab_data=pd.get_dummies(data[\"language\"]).values","1d4ae4d4":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nskf = StratifiedKFold(n_splits=5)\n\n","e9631041":"import tensorflow as tf\ninputs=bert_encode(data.hypothesis.values, data.premise.values, tokenizer)","a22410ed":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Number of replicas:', strategy.num_replicas_in_sync)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","02b342a6":"from tensorflow.keras import regularizers\nfrom tensorflow.keras.regularizers import l2\nimport tensorflow_addons as tfa\n\nmax_len = 50\n\ndef build_model():\n#     bert_encoder = TFAutoModel.from_pretrained(\"roberta-large-mnli\")\n    bert_encoder = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    \n    for layer in bert_encoder.layers:\n        layer.trainable=True\n    \n    \n    \n    \n#     embedding = bert_encoder(input_word_ids)[0]\n    embedding = bert_encoder([input_word_ids,input_mask,input_type_ids])[0]\n    d1=tf.keras.layers.Dropout(0.1)(embedding[:,0,:])\n#     first = tf.keras.layers.Dense(256, activation='relu')(embedding[:,0,:])\n    \n#     second = tf.keras.layers.Dense(64, activation='relu',kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01))(embedding[:,0,:])\n    \n\n    output = tf.keras.layers.Dense(3, activation='softmax')(d1)\n    \n  \n\n# ...\n\n#     optimizer = tfa.optimizers.Adagrad(learning_rate=1e-3, weight_decay=1e-2)\n# ...\n\n    optimizer = tf.optimizers.Adagrad(learning_rate=0.001)\n    \n    \n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n#     model = tf.keras.Model(inputs=input_word_ids, outputs=output)\n\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","4823c5a3":"# CUSTOM LEARNING SCHEUDLE\nimport matplotlib.pyplot as plt\n\n\ndef build_lrfn(lr_start=0.0001, lr_max=0.001, \n               lr_min=0.00001, lr_rampup_epochs=4, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\nplt.figure(figsize=(10, 7))\n\n_lrfn = build_lrfn()\nplt.plot([i for i in range(35)], [_lrfn(i) for i in range(35)]);","5c59d357":"X_test=test_data\nX_test=train_inputs=bert_encode(X_test.premise.values, X_test.hypothesis.values, tokenizer)","8a82f050":"len(test_data)","39a7523d":"import tensorflow as tf\ntest_pred=np.zeros((len(test_data),3))\n\nEPOCHS=10\na=0\nscore=[]\nfor train_index, test_index in skf.split(X_data, Y_data):\n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        model = build_model()\n    \n    print(\"\",end=\"\\n\\n\")\n        \n    print(f\"Generating Inputs for fold {a}\")\n    \n    print(\"==\"*20)\n    \n    train_inputs=bert_encode(X_data.iloc[train_index].premise.values, X_data.iloc[train_index].hypothesis.values, tokenizer)\n  \n    \n    train_labels=Y_data[train_index]\n    \n    val_inputs=bert_encode(X_data.iloc[test_index].premise.values, X_data.iloc[test_index].hypothesis.values, tokenizer)\n    test_labels=Y_data[test_index]\n    \n    \n    \n    lr2 = tf.keras.callbacks.LearningRateScheduler(_lrfn, verbose = True)\n    \n    lr=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                            factor=0.5, \n                                            patience=5)\n    early_stopping=tf.keras.callbacks.EarlyStopping(patience=5)\n    model.fit(train_inputs, train_labels,epochs=50, verbose = 1,\n              validation_data=(val_inputs,test_labels),\n              batch_size = 128,callbacks=[early_stopping,lr])\n    results=model.evaluate(val_inputs,test_labels)\n    print(\"====== predicting test===========\")\n    test_pred += model.predict(X_test)\n    \n  \n    print(\"====== predicting test Done===========\")\n    print(f\"score for fold {a} is\",results[1])\n    \n    print(\"===\"*20)\n    score.append(results[1])\n    \n    a+=1\n    \nprint(\"final LB score\",np.mean(score))\n    \n    \n       \n    ","682df0cb":"import gc\ngc.collect()","1bff3b44":"test_preds=test_pred\/5","33ed5396":"final_predictions=np.argmax(test_preds,axis=1)","308cc08e":"submission_data=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/sample_submission.csv\")\nsubmission_data[\"prediction\"]=final_predictions\n\nsubmission_data.to_csv(\"submission.csv\",index=False)","30670a2c":"Here is the plot of average length of the Sentences according the the languages for premise column","9d2de0ac":"Average Sentence length for hypothesis"}}