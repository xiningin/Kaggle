{"cell_type":{"9ff48609":"code","3ad00c0e":"code","b3b10844":"code","61b9e701":"code","cc3fe389":"code","966b7325":"code","a97bcf10":"code","c615294b":"code","122a1ff6":"code","76eb0d96":"code","c5de29c7":"code","0688fbf5":"code","7f29cef7":"code","ab945f10":"code","ca20015b":"code","75b83c51":"code","b5f445d2":"code","722b1111":"code","a3865d95":"code","6edae8fc":"markdown","6cba54fc":"markdown","e73d1f6b":"markdown","7a5e5dda":"markdown","83772be0":"markdown","488576a1":"markdown","6d90d898":"markdown","58a7d4de":"markdown","8fcbf020":"markdown","576470bb":"markdown","0cda2976":"markdown","d52ec4ce":"markdown","7331477e":"markdown","fb79d2b1":"markdown","d82dbfbd":"markdown","37cde710":"markdown","97458613":"markdown","f224c134":"markdown","6fb0d117":"markdown","d0008b41":"markdown"},"source":{"9ff48609":"# Standard library\nimport copy\nimport glob\nimport multiprocessing\nimport os\nimport time\nimport zipfile\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\n\n# Related third party\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm","3ad00c0e":"base_dir = '..\/input\/dogs-vs-cats-redux-kernels-edition'\nwith zipfile.ZipFile(os.path.join(base_dir, 'train.zip')) as train_zip:\n    train_zip.extractall('..\/data')\nwith zipfile.ZipFile(os.path.join(base_dir, 'test.zip')) as test_zip:\n    test_zip.extractall('..\/data')\n\ntrain_dir = '..\/data\/train'\ntest_dir = '..\/data\/test'","b3b10844":"# with zipfile.ZipFile('.\/drive\/My Drive\/Data Set\/dogs-vs-cats-redux-kernels-edition.zip') as entire_zip:\n#     entire_zip.extractall('.')\n# with zipfile.ZipFile('.\/train.zip') as train_zip:\n#     train_zip.extractall('.')\n# with zipfile.ZipFile('.\/test.zip') as test_zip:\n#     test_zip.extractall('.')\n\n# train_dir = '.\/train'\n# test_dir = '.\/test'","61b9e701":"input_size = 224\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n# Number of classes in the dataset\nnum_classes = 2 # dog, cat\n\n# Batch size for training (change depending on how much memory you have)\nbatch_size = 32\n\n# Number of epochs to train for\nnum_epochs = 2\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True\n\n# Switch to perform multi-process data loading\nnum_workers = multiprocessing.cpu_count()","cc3fe389":"# train data file looks '.\/train\/dog.10435.jpg'\n# test data file looks '.\/test\/10435.jpg'\ndef extract_class_from(path):\n    file = path.split('\/')[-1]\n    return file.split('.')[0]","966b7325":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    history = {'accuracy': [],\n               'val_accuracy': [],\n               'loss': [],\n               'val_loss': []}\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            if phase == 'train':\n                history['accuracy'].append(epoch_acc.item())\n                history['loss'].append(epoch_loss)\n            else:\n                history['val_accuracy'].append(epoch_acc.item())\n                history['val_loss'].append(epoch_loss) \n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","a97bcf10":"all_train_files = glob.glob(os.path.join(train_dir, '*.jpg'))\ntrain_list, val_list = train_test_split(all_train_files, random_state=42)","c615294b":"print(len(train_list))\nprint(len(val_list))","122a1ff6":"fig, axes = plt.subplots(nrows=2,\n                         ncols=3,\n                         figsize=(18, 12))\nfor img_path, ax in zip(train_list, axes.ravel()):\n    ax.set_title(img_path)\n    ax.imshow(Image.open(img_path))","76eb0d96":"class DogVsCatDataset(Dataset):\n  \n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.file_list)\n  \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n       \n        img_name = self.file_list[idx]\n        image = Image.open(img_name)\n        if self.transform:\n            image = self.transform(image)\n    \n        label_category = extract_class_from(img_name)\n        label = 1 if label_category == 'dog' else 0\n    \n        return image, label","c5de29c7":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(input_size, scale=(0.5, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n}","0688fbf5":"# Create training and validation datasets\nimage_datasets = {\n    'train': DogVsCatDataset(train_list,\n                             transform=data_transforms['train']),\n    'val': DogVsCatDataset(val_list,\n                           transform=data_transforms['val'])\n}\n\n# Create training and validation dataloaders\ndataloaders_dict = {x: DataLoader(image_datasets[x],\n                                  batch_size=batch_size,\n                                  shuffle=True,\n                                  num_workers=num_workers) for x in ['train', 'val']}\n\n# Detect if we have a GPU available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","7f29cef7":"model_ft = models.vgg16(pretrained=True)\nmodel_ft.classifier[6] = nn.Linear(4096, num_classes)","ab945f10":"# Send the model to GPU\nmodel_ft = model_ft.to(device)\n\n# Gather the parameters to be optimized\/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)","ca20015b":"# Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)","75b83c51":"acc = hist['accuracy']\nval_acc = hist['val_accuracy']\nloss = hist['loss']\nval_loss = hist['val_loss']\nepochs_range = range(num_epochs)\n\nplt.figure(figsize=(24, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","b5f445d2":"test_list = glob.glob(os.path.join(test_dir, '*.jpg'))\ntest_data_transform = data_transforms['val']\n\nids = []\nlabels = []\n\nwith torch.no_grad():\n    for test_path in tqdm(test_list):\n        img = Image.open(test_path)\n        img = test_data_transform(img)\n        img = img.unsqueeze(0)\n        img = img.to(device)\n\n        model_ft.eval()\n        outputs = model_ft(img)\n        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n\n        test_id = extract_class_from(test_path)\n        ids.append(int(test_id))\n        labels.append(preds[0])","722b1111":"template = '\"{}\" with {:.2%} confidence'\ndef pred_result_message(pred):\n    if pred > 0.5:\n        return template.format('dog', pred)\n    else:\n        return template.format('cat', 1 - pred)\n\nfig, axes = plt.subplots(nrows=2,\n                         ncols=3,\n                         figsize=(18, 12))\nfor img_path, label, ax in zip(test_list, labels, axes.ravel()):\n    ax.set_title(pred_result_message(label))\n    ax.imshow(Image.open(img_path))","a3865d95":"output = pd.DataFrame({'id': ids,\n                       'label': np.round(labels)})\n\noutput.sort_values(by='id', inplace=True)\noutput.reset_index(drop=True, inplace=True)\n\noutput.to_csv('submission.csv', index=False)","6edae8fc":"# Initialize and Reshape the Networks\n\nRegarding tuning VGG, see https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#vgg.\n\n","6cba54fc":"# Run Training and Validation Step","e73d1f6b":"## Check what train data looks like","7a5e5dda":"# Predict","83772be0":"### For Google Colab\n\nIt assumes that downloaded data will be located under `'My Drive\/Data Set\/'`","488576a1":"# Create the Optimizer\n\nSee https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#create-the-optimizer for details.","6d90d898":"# Global Declarations","58a7d4de":"## Create dataloaders\n\nSee https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#load-data for details.","8fcbf020":"## Visualize training results\n\nThis procedure comes from https:\/\/www.tensorflow.org\/tutorials\/images\/classification#visualize_training_results.","576470bb":"## Helper Functions","0cda2976":"## Constants\n\nRegarding `input_size`, `mean` and `std`, all pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. See [torchvision.models](https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html) for details. ","d52ec4ce":"# Generate submittion.csv","7331477e":"# Pre process for each environment","fb79d2b1":"# References\n\n- The competition is https:\/\/www.kaggle.com\/c\/dogs-vs-cats-redux-kernels-edition (it ended long time ago)\n- Pytorch official docs:\n    - [Finetuning Torchvision Models](https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html)\n    - [Writing Custom Datasets, DataLoaders and Transforms](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)\n    - Any other docs refered to train model or whatever, left comments on each section.\n- Refered kernels:\n    - [Dog_vs_Cat Transfer Learning - VGG16 by Pytorch](https:\/\/www.kaggle.com\/bootiu\/dog-vs-cat-transfer-learning-vgg16-by-pytorch)","d82dbfbd":"This `train_model` function comes from https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code.","37cde710":"### For Kaggle kernel","97458613":"## Check how well the prediction went","f224c134":"# Import libraries","6fb0d117":"## Dataset class\n\nSee https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html#dataset-class for details.","d0008b41":"# Load Data"}}