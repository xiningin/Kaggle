{"cell_type":{"18f569c2":"code","2f13c6bb":"code","fb5dda7b":"code","60caf34b":"code","9e7e1b2c":"code","4c02e23a":"code","87240b51":"code","737d9890":"code","82242a88":"code","05dcc57c":"code","943c318a":"code","1c263755":"code","f6829156":"code","00b969f5":"code","45475b86":"code","51491f7f":"code","c4a6f310":"code","9229f492":"code","877832b6":"code","edbd83eb":"code","21095f67":"code","87e13f35":"code","1fda05f5":"code","39159133":"code","5b925f25":"code","1de3626d":"code","86a822e7":"code","8efeb865":"code","1cce614f":"code","38dbe4b5":"code","a62c7278":"code","799945ba":"code","5154e5ea":"code","fc529cf6":"code","c42b416c":"code","43969782":"code","bb1cdb4f":"markdown","3afd925b":"markdown","fa200d7f":"markdown","87ae3dc0":"markdown","0fb7f934":"markdown","2f14c693":"markdown","6b723f3f":"markdown","7bda3624":"markdown","ab0cd928":"markdown","e096cdc2":"markdown","ffd9e3b6":"markdown","afe85151":"markdown","f38118f2":"markdown","aa0f1251":"markdown","7da20f43":"markdown","aaddc64e":"markdown","a6a9e77b":"markdown","cc74aa5d":"markdown","43d439de":"markdown","c230bcd0":"markdown","f5b21033":"markdown","93cf87f0":"markdown","e7c93a00":"markdown","0fd626e2":"markdown","cdc9aa33":"markdown","18616185":"markdown","74935e54":"markdown","00620696":"markdown","2a79db64":"markdown","022e2a29":"markdown","9da1eff7":"markdown","156e422d":"markdown","6e5d9dc0":"markdown","a72d6830":"markdown","577ae864":"markdown","e8a4f6ec":"markdown","55d41742":"markdown","5d31c101":"markdown","ec3b0c5c":"markdown","ec872e20":"markdown","386b5da0":"markdown","6ecf173a":"markdown"},"source":{"18f569c2":"from fastai.vision.all import *","2f13c6bb":"set_seed(16)","fb5dda7b":"path = Path(\"..\/input\")","60caf34b":"path.ls()","9e7e1b2c":"data_path = path\/'cassava-leaf-disease-classification'\ndata_path.ls()","4c02e23a":"df = pd.read_csv(data_path\/'train.csv')","87240b51":"df.head()","737d9890":"def get_x(row): return data_path\/'train_images'\/row['image_id']","82242a88":"PILImage.create(get_x(df.iloc[0]))","05dcc57c":"df.head()","943c318a":"df['image_id'] = df['image_id'].apply(lambda x: f'train_images\/{x}')","1c263755":"df.head()","f6829156":"idx2lbl = {0:\"Cassava Bacterial Blight (CBB)\",\n          1:\"Cassava Brown Streak Disease (CBSD)\",\n          2:\"Cassava Green Mottle (CGM)\",\n          3:\"Cassava Mosaic Disease (CMD)\",\n          4:\"Healthy\"}\n\ndf['label'].replace(idx2lbl, inplace=True)","00b969f5":"df.head()","45475b86":"blocks = (ImageBlock, CategoryBlock)","51491f7f":"splitter = RandomSplitter(valid_pct=0.2)","c4a6f310":"def get_x(row): return data_path\/row['image_id']\n\ndef get_y(row): return row['label']","9229f492":"item_tfms = [Resize(448)]\nbatch_tfms = [RandomResizedCropGPU(224), *aug_transforms(), Normalize.from_stats(*imagenet_stats)]","877832b6":"block = DataBlock(blocks = blocks,\n                 get_x = get_x,\n                 get_y = get_y,\n                 splitter = splitter,\n                 item_tfms = item_tfms,\n                 batch_tfms = batch_tfms)","edbd83eb":"dls = block.dataloaders(df, bs=64)","21095f67":"dls.show_batch(figsize=(12,12))","87e13f35":"# Making pretrained weights work without needing to find the default filename\nif not os.path.exists('\/root\/.cache\/torch\/hub\/checkpoints\/'):\n        os.makedirs('\/root\/.cache\/torch\/hub\/checkpoints\/')\n!cp '..\/input\/resnet50\/resnet50.pth' '\/root\/.cache\/torch\/hub\/checkpoints\/resnet50-19c8e357.pth'","1fda05f5":"learn = cnn_learner(dls, resnet50, opt_func=ranger, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)","39159133":"def fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `freeze` for `freeze_epochs` then with `unfreeze` from `epochs` using discriminative LR\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr \/= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr\/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)","5b925f25":"@patch\ndef fine_tune_flat(self:Learner, epochs, base_lr=4e-3, freeze_epochs=1, lr_mult=100, pct_start=0.75, \n                   first_callbacks = [], second_callbacks = [], **kwargs):\n    \"Fine-tune applied to `fit_flat_cos`\"\n    self.freeze()\n    self.fit_flat_cos(freeze_epochs, slice(base_lr), pct_start=0.99, cbs=first_callbacks, **kwargs)\n    base_lr \/= 2\n    self.unfreeze()\n    self.fit_flat_cos(epochs, slice(base_lr\/lr_mult, base_lr), pct_start=pct_start, cbs=second_callbacks)","1de3626d":"learn.lr_find()","86a822e7":"cbs1 = [MixUp(alpha = 0.7)]\ncbs2 = [MixUp(alpha = 0.3)]","8efeb865":"learn.fine_tune_flat(5, base_lr=1e-3, start_pct=0.72, first_callbacks=cbs1, second_callbacks=cbs2)","1cce614f":"sample_df = pd.read_csv(data_path\/'sample_submission.csv')\nsample_df.head()","38dbe4b5":"sample_copy = sample_df.copy()","a62c7278":"sample_copy['image_id'] = sample_copy['image_id'].apply(lambda x: f'test_images\/{x}')","799945ba":"test_dl = learn.dls.test_dl(sample_copy)","5154e5ea":"test_dl.show_batch()","fc529cf6":"preds, _ = learn.tta(dl=test_dl)","c42b416c":"sample_df['label'] = preds.argmax(dim=-1).numpy()","43969782":"sample_df.to_csv('submission.csv',index=False)","bb1cdb4f":"`fastai` has a `fit_flat_cos` function designed to best utilize the `ranger` optimizer function. Jeremy and Sylvain also came up with a `fine_tune` function best utilized for transfer learning which uses the `fit_one_cycle`, or One-Cycle Policy. We're going to create our own hybrid `fine_tune` method that will do a similar paradigm.\n\nWe can also tie it to our `Learner` objects through the `@patch` functionality. First we'll look at what `fine_tune`'s source code looks like, and rewrite it for `fit_flat_cos`:","3afd925b":"We'll want this to be similar to our training data so let's prepend that `test`:","fa200d7f":"We have an `image_id` and a `label`. We're going to modify our values in `image_id` to make our lives easier when it comes to running inference. \n\nWhy? \n\n\nIn fastai we have a `get_x` and a `get_y` and this will dictate how it will *always* look for our data, regardless of how it is stored. If we built a `get_y` based on the current `DataFrame`, it would look something like so:","87ae3dc0":"Next up we'll move to submissions","0fb7f934":"## Setting up our data\n\nAlright, now that we have our imports let's go ahead and look at our data.\n\nFirst we'll make a `Path` object so we can see what all we have available to us:","2f14c693":"For these results to be reproducable on your end, we will go ahead and set the `random`, `torch`, and `numpy` seeds with the `set_seed` function","6b723f3f":"And take a look:","7bda3624":"## Building the `DataBlock`\n\nLet's think about how our problem looks. `fastai` provides blocks to center around *most* situations, and this is no exception.\n\nWe know our input is an image and our output is a category, so let's use `ImageBlock` and `CategoryBlock`:","ab0cd928":"We can see that the resnet50 pretrained model is there and the competition data. We'll make a `data_path` to point to this directory and take another peek:","e096cdc2":"> I have also added in the potential for callbacks, we will see more on why later","ffd9e3b6":"Now we won't run into an issue when we're testing. \n","afe85151":"Let's look at a batch of data to make sure everything looks alright:","f38118f2":"Next we'll want to split our data somehow. We'll use a `RandomSplitter` and split our data 80\/20","aa0f1251":"Now we're good to train! Let's find a learning rate first:","7da20f43":"Finally, let's build the `DataBlock`!","aaddc64e":"## Submitting some results","a6a9e77b":"And we can see it work below:","cc74aa5d":"Looks great! Let's move onto training our model","43d439de":"Next we'll make an inference dataloader through the `test_dl` method:","c230bcd0":"And that's it! We looked at a few of the neat tricks fastai can offer while also taking a look at how the `DataBlock` API can be used for such a problem.\n\nIf you enjoyed this notebook or it helped you get started please leave an upvote and if there are any questions please leave a comment! Thanks!","f5b21033":"# Cassava Classification - fastai Starter\n\n\nThe datasets for this notebook are from [this](https:\/\/www.kaggle.com\/tanlikesmath\/cassava-classification-eda-fastai-starter) notebook, while this tutorial will be a twist on what has already been done!","93cf87f0":"## What will this tutorial cover?\n\nWe will be looking at how to use the high-level `DataBlock` API for this challenge, how to use some advanced training features in the library, as well as some advanced inference features.","e7c93a00":"And now we're good to go! Let's build the `DataBlock`","0fd626e2":"And now we can see our new table:","cdc9aa33":"### Adjusting our label\n\nWhat else can we do?\n\nLet's change our lables into something more readable through a dictionary (these come from the `json` file):","18616185":"And now we can submit them:","74935e54":"We'll look at a batch of data to make sure it all looks okay:","00620696":"Now that our weights are setup, let's look at how to use `cnn_learner`. We're going to use a few tricks during our training that fastai can help us out with. \n\nSpecifically we will be using the `ranger` optimizer function and `LabelSmoothingCrossEntropy` as our loss function.\n\nAlong with these we'll be using the `accuracy` metric as this is how this competition will grade our results with:","2a79db64":"Our images are stored away in `train_images` and `test_images`, and we have a `train.csv` for our labels. Let's load it into `pandas`:","022e2a29":"### Adjusting the `image_id`","9da1eff7":"We can see that when we write custom `get_` functions, it will accept one *row* of our `DataFrame` to look at, and so we can filter as a result.\n\nNext we'll come up with some basic data augmentations. \n\nOur `item_tfms` should ensure everything is ready to go into a batch, so we will use `Resize`.\n\nOur `batch_tfms` should apply any extra augmentations we may want. We'll use `RandomResizedCropGPU`, `aug_transforms`, and apply our `Normalize`:\n> We will normalize our data based on ImageNet, since that is what our pretrained model was trained with","156e422d":"## Importing the Library\n\nFirst, let's import the `fastai.vision` library for us to use and work with:","6e5d9dc0":"Let's look at the sample submission dataframe first:","a72d6830":"That's great! ***But*** there is a very large issue here. We always have our `get_x` tied to the training directory which makes it more complicated for us to work with our `test_images` directory.\n\nWhat's the solution? \n\nAdd `train_images` into the dataframe through a `lambda` function:","577ae864":"Our `DataBlock` is also going to want to know how to get our data. Since our data all stems from a `csv`, we will make a `get_x` and `get_y` function:\n\n> We already made our `get_x` earlier, so I have brought it down here","e8a4f6ec":"Let's train for 1 epoch frozen and 10 unfrozen and a `start_pct` of 0.72:","55d41742":"We can then run an `ls()` (a monkey-patched function by `fastcore`) to see all the files and directories in here:","5d31c101":"Here's what it rewritten as looks like:","ec3b0c5c":"We'll choose a learning rate of roughly 4e-3 to start. \n\nFinally, remember how we had those extra callback parameters? We're going to utilize the `MixUp` training methodology with a decreasing `MixUp` percentage:","ec872e20":"Great! Next we'll grab some predictions. We will use the `.tta` method to run test-time-augmentation which can help boost our accuracy some:","386b5da0":"## The Model\n\nThe code here is from tanlikesmath's notebook linked at the start. This will move our pretrained weights to where fastai will expect it:","6ecf173a":"And now we can turn this into some `DataLoaders`. We're going to pass in some items (which in our case is our `DataFrame`) and a batch size to use. We will use 64:"}}