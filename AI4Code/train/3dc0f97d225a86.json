{"cell_type":{"6faa2470":"code","eaf7d605":"code","93511bd5":"code","3dc1675c":"code","805bb2f3":"code","b8af403a":"code","e72141f2":"code","842b36a5":"code","996c976e":"code","446f9126":"code","ac9012fc":"code","7f74f7af":"code","0f9de066":"code","b000eae7":"code","9fca7173":"code","0d4b163c":"code","b7123e39":"code","82020db5":"markdown","6a4705bb":"markdown","6bb43cc3":"markdown","b7936f80":"markdown"},"source":{"6faa2470":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tqdm.auto import tqdm\n\ntqdm.pandas()\npd.options.display.max_colwidth = None\nsns.set_style('darkgrid')","eaf7d605":"dtrain = pd.read_csv('..\/input\/quora-question-pairs\/train.csv.zip')\nprint(dtrain.shape)\ndtrain.head()","93511bd5":"dtest = pd.read_csv('..\/input\/quora-question-pairs\/test.csv')\nprint(dtest.shape)\ndtest.head()","3dc1675c":"%%time\n\nall_ques = pd.read_csv('..\/input\/qqp-cleaned\/quora-ques-pair-all-ques.csv')\nall_ques.head()","805bb2f3":"%%time\n\ntext_map = {x:y for x, y in zip(all_ques['RawText'].values, all_ques['CleanedText'].values)}\n\ndtrain['question1'] = dtrain['question1'].apply(lambda x: text_map[x])\ndtrain['question2'] = dtrain['question2'].apply(lambda x: text_map[x])\n\ndtest['question1'] = dtest['question1'].apply(lambda x: text_map[x])\ndtest['question2'] = dtest['question2'].apply(lambda x: text_map[x])\n\ndel text_map","b8af403a":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.01, random_state=19)\ntrain_index, valid_index = list(sss.split(dtrain[['question1', 'question2']].values, dtrain['is_duplicate']))[0]","e72141f2":"from sklearn.metrics import confusion_matrix, classification_report\n\ndef evaluate_model(model, x_train, x_valid, y_train, y_valid):\n    print('Train Set:')\n    print()\n    print(classification_report(y_train, model.predict(x_train)))\n    \n    print()\n    print()\n    \n    print('Validation Set:')\n    print()\n    print(classification_report(y_valid, model.predict(x_valid)))","842b36a5":"%%time\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer(\n    ngram_range=(1, 1),\n    min_df=1,\n    max_df=1.0,\n    sublinear_tf=True\n).fit(all_ques['CleanedText'].fillna('').values)\n\nx_train = dtrain[['question1', 'question2']].iloc[train_index].reset_index(drop=True)\nx_valid = dtrain[['question1', 'question2']].iloc[valid_index].reset_index(drop=True)\n\ny_train = dtrain['is_duplicate'].iloc[train_index].reset_index(drop=True).values\ny_valid = dtrain['is_duplicate'].iloc[valid_index].reset_index(drop=True).values\n\ndel all_ques\n\ny_train.shape, y_valid.shape","996c976e":"def sparse_tensor(X):\n    coo = X.tocoo()\n    indices = np.mat([coo.row, coo.col]).transpose()\n    return tf.sparse.reorder(tf.SparseTensor(indices, coo.data, coo.shape))","446f9126":"%%time\n\nfrom scipy import sparse\n\nx_train_1 = vectorizer.transform(x_train['question1'].fillna(''))\nx_train_2 = vectorizer.transform(x_train['question2'].fillna(''))\n\nx_valid_1 = vectorizer.transform(x_valid['question1'].fillna(''))\nx_valid_2 = vectorizer.transform(x_valid['question2'].fillna(''))\n\nx_test_1 = vectorizer.transform(dtest['question1'].fillna(''))\nx_test_2 = vectorizer.transform(dtest['question2'].fillna(''))\n\nx_train = [sparse_tensor(x_train_1), sparse_tensor(x_train_2)]\nx_valid = [sparse_tensor(x_valid_1), sparse_tensor(x_valid_2)]\nx_test = [sparse_tensor(x_test_1), sparse_tensor(x_test_2)]\n\nx = [\n    sparse_tensor(sparse.vstack([x_train_1, x_valid_1])), \n    sparse_tensor(sparse.vstack([x_train_2, x_valid_2]))\n]\ny = np.concatenate([y_train, y_valid])\n\ndel x_train_1, x_train_2, x_valid_1, x_valid_2, x_test_1, x_test_2\n\nx_train[0].shape, x_valid[0].shape, x_test[0].shape","ac9012fc":"import tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import layers, utils, callbacks, optimizers, regularizers","7f74f7af":"def euclidean_distance(vectors):\n    (featsA, featsB) = vectors\n    sumSquared = K.sum(K.square(featsA - featsB), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n\ndef cosine_similarity(vectors):\n    (featsA, featsB) = vectors\n    featsA = K.l2_normalize(featsA, axis=-1)\n    featsB = K.l2_normalize(featsB, axis=-1)\n    return K.mean(featsA * featsB, axis=-1, keepdims=True)","0f9de066":"class SiameseNetwork(Model):\n    def __init__(self, inputShape, featExtractorConfig):\n        super(SiameseNetwork, self).__init__()\n        \n        inpA = layers.Input(shape=inputShape)\n        inpB = layers.Input(shape=inputShape)\n        featureExtractor = self.build_feature_extractor(inputShape, featExtractorConfig)\n        featsA = featureExtractor(inpA)\n        featsB = featureExtractor(inpB)\n        distance = layers.Concatenate()([featsA, featsB])\n        outputs = layers.Dense(1, activation=\"sigmoid\")(distance)\n        self.model = Model(inputs=[inpA, inpB], outputs=outputs)        \n        \n    def build_feature_extractor(self, inputShape, featExtractorConfig):\n        \n        layers_config = [layers.Input(inputShape)]\n        for i, n_units in enumerate(featExtractorConfig):\n            layers_config.append(layers.Dense(n_units))\n            layers_config.append(layers.Dropout(0.5))\n            layers_config.append(layers.BatchNormalization())\n            layers_config.append(layers.Activation('relu'))\n        \n        model = Sequential(layers_config, name='feature_extractor')\n\n        return model  \n        \n    def call(self, x):\n        return self.model(x)\n\nmodel = SiameseNetwork(inputShape=x_train[0].shape[1], featExtractorConfig=[100])\nmodel.compile(\n    loss=\"binary_crossentropy\", \n    optimizer=optimizers.Adam(learning_rate=0.0001),\n    metrics=[\"accuracy\"]\n)","b000eae7":"model.model.layers[2].summary()\nmodel.model.summary()\nutils.plot_model(model.model, show_shapes=True, expand_nested=True)","9fca7173":"es = callbacks.EarlyStopping(\n    monitor='val_loss', patience=5, verbose=1, restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=2, min_lr=1e-10, mode='min', verbose=1\n)\n\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_valid, y_valid),\n    batch_size=32, \n    epochs=100,\n    callbacks=[es, rlp]\n)","0d4b163c":"fig, ax = plt.subplots(2, 1, figsize=(20, 8))\ndf = pd.DataFrame(history.history)\ndf[['accuracy', 'val_accuracy']].plot(ax=ax[0])\ndf[['loss', 'val_loss']].plot(ax=ax[1])\nax[0].set_title('Model Accuracy', fontsize=12)\nax[1].set_title('Model Loss', fontsize=12)\nfig.suptitle('Siamese Network: Learning Curve', fontsize=18);","b7123e39":"%%time\n\nsubmission = pd.DataFrame({\n    'test_id': dtest.test_id.values,\n    'is_duplicate': np.ravel(model.predict(x_test, batch_size=32))\n})\nsubmission.to_csv('submission.csv', index=False)","82020db5":"# Cross Validation","6a4705bb":"# Vectorization","6bb43cc3":"# Modelling","b7936f80":"# Text Cleaning"}}