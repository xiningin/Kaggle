{"cell_type":{"fd1f7646":"code","7b81c205":"code","d881eb1c":"code","f6c8a3a2":"code","1155ea97":"code","2e960a74":"markdown","f4304af9":"markdown","bbffec82":"markdown","708f27a9":"markdown"},"source":{"fd1f7646":"from gensim.models import Word2Vec\nfrom multiprocessing import Pool\nimport sqlite3 as sql\nimport numpy as np\nimport logging\nimport time\nimport re\n\ndb = '''..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'''\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","7b81c205":"def get_query(select, db=db):\n    '''\n    1. Connects to SQLite database (db)\n    2. Executes select statement\n    3. Return results and column names\n    \n    Input: 'select * from analytics limit 2'\n    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n    '''\n    with sql.connect(db) as conn:\n        c = conn.cursor()\n        c.execute(select)\n        col_names = [str(name[0]).lower() for name in c.description]\n    return c.fetchall(), col_names\n\ndef tokenize(text, lower=True):\n    '''\n    1. Strips apostrophes\n    2. Searches for all alpha tokens (exception for underscore)\n    3. Return list of tokens\n\n    Input: 'The 3 dogs jumped over Scott's tent!'\n    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n    '''\n    text = re.sub(\"'\", \"\", text)\n    if lower:\n        tokens = re.findall('''[a-z_]+''', text.lower())\n    else:\n        tokens = re.findall('''[A-Za-z_]''', text)\n    return tokens\n    \ndef get_section(rowid):\n    '''\n    1. Construct select statement\n    2. Retrieves section_text\n    3. Tokenizes section_text\n    4. Returns list of tokens\n\n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where rowid=%d''' % rowid\n    doc, _ = get_query(select)\n    tokens = tokenize(doc[0][0])\n    return tokens\n       \nclass Corpus():\n    def __init__(self, rowids):\n        self.rowids = rowids\n        self.len = len(rowids)\n\n    def __iter__(self):\n        rowids = np.random.choice(self.rowids, self.len, replace=False)\n        with Pool(processes=4) as pool:\n            docs = pool.imap_unordered(get_section, rowids)\n            for doc in docs:\n                yield doc\n\n    def __len__(self):\n        return self.len","d881eb1c":"select = '''select distinct rowid from articles'''\nrowids, _ = get_query(select)\nrowids = [rowid[0] for rowid in rowids]","f6c8a3a2":"start = time.time()\n# To keep training time reasonable, let's just look at a random 10K section text sample.\nsample_rowids = np.random.choice(rowids, 10000, replace=False)\ndocs = Corpus(sample_rowids)\nword2vec = Word2Vec(docs, min_count=100, size=100)\nend = time.time()\nprint('Time to train word2vec from generator: %0.2fs' % (end - start))","1155ea97":"word2vec = Word2Vec.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20_word2vec.model')","2e960a74":"# Tutorial: Word2Vec\nThis is a basic guide to efficiently training a Word2Vec model on the English Wikipedia dump using Gensim.","f4304af9":"First step, grab the index we'll be iterating over. In this case, we want to use section text, so let's use the implicit column: **rowid**.","bbffec82":"Now let's train a Word2Vec. Ideally, we'd split the section text into sentences, but feeding section text as a block performs well. ","708f27a9":"For now, let's load a pre-trained model and explore how to use it."}}