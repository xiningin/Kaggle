{"cell_type":{"e98d7423":"code","66e41109":"code","5a1cb533":"code","92018798":"code","7d59010a":"code","6f217da4":"code","d910ca23":"code","22a754ba":"code","af82a9ca":"code","098485ab":"code","92f5564c":"code","331ab340":"code","42cfb00f":"code","6090795f":"code","28e6eace":"code","55070f4f":"code","2a454f42":"code","2e21ac82":"code","d6c6afb3":"code","65a92015":"code","e322fcd0":"code","1d8dfc25":"code","033a0c26":"code","c8a5d8a7":"code","f4d9663f":"code","9caba549":"code","8e360cc9":"code","749e679f":"code","ca291e11":"code","c2a11133":"code","983ce63c":"code","ea9e4fb8":"code","83bb22b9":"code","252b89d6":"code","4a738986":"code","92f22033":"code","6f6530ac":"code","69abc1c1":"code","7d5ed8d0":"code","11c794a0":"code","2c905e7a":"code","77a7254f":"code","791d8cf0":"code","8c50c7ad":"code","3ca00867":"code","d4423fa6":"code","6e05026f":"code","14d79c25":"code","131093c0":"code","e3c9e9c6":"code","5e0c89dc":"code","709b23b0":"code","103dbf8a":"code","311a78a5":"code","c5668f30":"code","ee781dde":"code","121fb8a3":"code","af75bbe6":"code","6b54d031":"code","5fc791d5":"code","7f619cb4":"code","47c26b7b":"code","32e711b5":"code","2f8a3272":"code","46d9c5fa":"code","d6fec4f3":"markdown","d676bdbc":"markdown","b98c1704":"markdown","fe348933":"markdown","7ed85e6d":"markdown","9e8d3ae3":"markdown","53d1d16b":"markdown","0a8ebf32":"markdown","67a3d0d9":"markdown","68ffa022":"markdown","481071d1":"markdown","584958c6":"markdown","eb6bb1cb":"markdown","f163b07c":"markdown","69e7179e":"markdown","7176afba":"markdown","ac4be660":"markdown","e9b9903b":"markdown","e44b060a":"markdown","0585786d":"markdown","6a4f99a9":"markdown","742c3050":"markdown","36b37da8":"markdown","1980f4b7":"markdown","c060b084":"markdown","386820f9":"markdown","7eac32cd":"markdown","ef522222":"markdown","76113956":"markdown","399585ad":"markdown","8fc06345":"markdown","fbe5fc1b":"markdown"},"source":{"e98d7423":"import numpy as np\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import make_grid\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch","66e41109":"# number of cpu (in kaggle server)\n!cat \/proc\/cpuinfo | grep processor","5a1cb533":"n_cpu = 2 # number of cpu threads to use during batch generation","92018798":"# data (path)\ndataset_name = 'gan-getting-started'\nroot = '..\/input\/'+dataset_name\n\n# data (img)\nimg_height = 256\nimg_width = 256\nchannels = 3\n\n# training\nepoch = 0 # epoch to start training from\nn_epochs = 5 # number of epochs of training\nbatch_size = 1 # size of the batches\nlr = 0.0002 # adam : learning rate\nb1 = 0.5 # adam : decay of first order momentum of gradient\nb2 = 0.999 # adam : decay of first order momentum of gradient\ndecay_epoch = 3 # suggested default : 100 (suggested 'n_epochs' is 200)\n                 # epoch from which to start lr decay\n","7d59010a":"class ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n        \n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1), # Pads the input tensor using the reflection of the input boundary\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features), \n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, num_residual_block):\n        super(GeneratorResNet, self).__init__()\n        \n        channels = input_shape[0]\n        \n        # Initial Convolution Block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_features, 7),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True)\n        ]\n        in_features = out_features\n        \n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True)\n            ]\n            in_features = out_features\n        \n        # Residual blocks\n        for _ in range(num_residual_block):\n            model += [ResidualBlock(out_features)]\n            \n        # Upsampling\n        for _ in range(2):\n            out_features \/\/= 2\n            model += [\n                nn.Upsample(scale_factor=2), # --> width*2, heigh*2\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.ReLU(inplace=True)\n            ]\n            in_features = out_features\n            \n        # Output Layer\n        model += [nn.ReflectionPad2d(channels),\n                  nn.Conv2d(out_features, channels, 7),\n                  nn.Tanh()\n                 ]\n        \n        # Unpacking\n        self.model = nn.Sequential(*model) \n        \n    def forward(self, x):\n        return self.model(x)","6f217da4":"tensor = torch.from_numpy(np.random.randn(2,2))","d910ca23":"tensor","22a754ba":"tensor = tensor.view(1,1,2,2)\ntensor # nn.Upsample get only 3\/4\/5D tensor","af82a9ca":"test_m = nn.Upsample(scale_factor=2)\ntest_m(tensor)","098485ab":"class Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n        \n        channels, height, width = input_shape\n        \n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height\/\/2**4, width\/\/2**4)\n        \n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n        \n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128,256),\n            *discriminator_block(256,512),\n            nn.ZeroPad2d((1,0,1,0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n        \n    def forward(self, img):\n        return self.model(img)","92f5564c":"tensor = torch.from_numpy(np.random.randn(2,2))\ntensor","331ab340":"test_m = nn.ZeroPad2d((1,0,0,0)) # Padding_left\ntest_m(tensor)","42cfb00f":"test_m = nn.ZeroPad2d((0,1,0,0)) # Padding_right\ntest_m(tensor)","6090795f":"test_m = nn.ZeroPad2d((0,0,1,0)) # Padding_top\ntest_m(tensor)","28e6eace":"test_m = nn.ZeroPad2d((0,0,0,1)) # Padding_bottom\ntest_m(tensor)","55070f4f":"test_m = nn.ZeroPad2d((1,0,1,0)) # Padding_left and Padding_top\ntest_m(tensor)","2a454f42":"criterion_GAN = torch.nn.MSELoss()\ncriterion_cycle = torch.nn.L1Loss()\ncriterion_identity = torch.nn.L1Loss()","2e21ac82":"input_shape = (channels, img_height, img_width) # (3,256,256)\nn_residual_blocks = 9 # suggested default, number of residual blocks in generator\n\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\nG_BA = GeneratorResNet(input_shape, n_residual_blocks)\nD_A = Discriminator(input_shape)\nD_B = Discriminator(input_shape)","d6c6afb3":"cuda = torch.cuda.is_available()\n\nif cuda:\n    G_AB = G_AB.cuda()\n    G_BA = G_BA.cuda()\n    D_A = D_A.cuda()\n    D_B = D_B.cuda()\n    \n    criterion_GAN.cuda()\n    criterion_cycle.cuda()\n    criterion_identity.cuda()","65a92015":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution\n        if hasattr(m, 'bias') and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0) # reset Conv2d's bias(tensor) with Constant(0)\n        elif classname.find('BatchNorm2d') != -1:\n            torch.nn.init.normal_(m.weight.data, 1.0, 0.02) # reset BatchNorm2d's weight(tensor) with Gaussian Distribution\n            torch.nn.init.constant_(m.bias.data, 0.0) # reset BatchNorm2d's bias(tensor) with Constant(0)","e322fcd0":"G_AB.apply(weights_init_normal)\nG_BA.apply(weights_init_normal)\nD_A.apply(weights_init_normal)\nD_B.apply(weights_init_normal)","1d8dfc25":"def temp_weights_init_normal(m):\n    classname =  m.__class__.__name__\n    print(classname)","033a0c26":"G_AB.apply(temp_weights_init_normal);","c8a5d8a7":"temp_w = torch.ones(2,5)\ntemp_w","f4d9663f":"nn.init.normal_(temp_w)","9caba549":"temp_w2 = torch.empty(2,5)\ntemp_w2","8e360cc9":"nn.init.constant_(temp_w2, 1)","749e679f":"import itertools\n# lr = 0.0002\n# b1 = 0.5\n# b2 = 0.999\n\noptimizer_G = torch.optim.Adam(\n    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n)\n\noptimizer_D_A = torch.optim.Adam(\n    D_A.parameters(), lr=lr, betas=(b1,b2)\n)\noptimizer_D_B = torch.optim.Adam(\n    D_B.parameters(), lr=lr, betas=(b1,b2)\n)","ca291e11":"class LambdaLR:\n    def __init__(self, n_epochs, offset, decay_start_epoch):\n        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n        self.n_epochs = n_epochs\n        self.offset = offset\n        self.decay_start_epoch = decay_start_epoch\n        \n    def step(self, epoch):\n        return 1.0 - max(0, epoch+self.offset - self.decay_start_epoch)\/(self.n_epochs - self.decay_start_epoch)","c2a11133":"# n_epochs = 10\n# epoch = 0\n# decay_epoch = 5\n\n\nlr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n    optimizer_G,\n    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n)\n\nlr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n    optimizer_D_A,\n    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n)\nlr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n    optimizer_D_B,\n    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n)","983ce63c":"from PIL import Image\nimport torchvision.transforms as transforms\n\ntransforms_ = [\n    transforms.Resize(int(img_height*1.12), Image.BICUBIC),\n    transforms.RandomCrop((img_height, img_width)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n]","ea9e4fb8":"def to_rgb(image):\n    rgb_image = Image.new(\"RGB\", image.size)\n    rgb_image.paste(image)\n    return rgb_image","83bb22b9":"import os\nimport glob","252b89d6":"print(root+'\/monet_jpg')","4a738986":"len(glob.glob(os.path.join(root+'\/monet_jpg')+'\/*.*'))","92f22033":"len(glob.glob(os.path.join(root+'\/photo_jpg')+'\/*.*'))","6f6530ac":"from torch.utils.data import Dataset\n\nclass ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n        self.transform = transforms.Compose(transforms_)\n        self.unaligned = unaligned\n        self.mode = mode\n        if self.mode == 'train':\n            self.files_A = sorted(glob.glob(os.path.join(root+'\/monet_jpg')+'\/*.*')[:250])\n            self.files_B = sorted(glob.glob(os.path.join(root+'\/photo_jpg')+'\/*.*')[:250])\n        elif self.mode == 'test':\n            self.files_A = sorted(glob.glob(os.path.join(root+'\/monet_jpg')+'\/*.*')[250:])\n            self.files_B = sorted(glob.glob(os.path.join(root+'\/photo_jpg')+'\/*.*')[250:301])\n\n    def  __getitem__(self, index):\n        image_A = Image.open(self.files_A[index % len(self.files_A)])\n        \n        if self.unaligned:\n            image_B = Image.open(self.files_B[np.random.randint(0, len(self.files_B)-1)])\n        else:\n            image_B = Image.open(self.files_B[index % len(self.files_B)])\n        if image_A.mode != 'RGB':\n            image_A = to_rgb(image_A)\n        if image_B.mode != 'RGB':\n            image_B = to_rgb(image_B)\n            \n        item_A = self.transform(image_A)\n        item_B = self.transform(image_B)\n        return {'A':item_A, 'B':item_B}\n    \n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))\n            ","69abc1c1":"dataloader = DataLoader(\n    ImageDataset(root, transforms_=transforms_, unaligned=True),\n    batch_size=1, # 1\n    shuffle=True,\n    num_workers=n_cpu # 3\n)\n\nval_dataloader = DataLoader(\n    ImageDataset(root, transforms_=transforms_, unaligned=True, mode='test'),\n    batch_size=5,\n    shuffle=True,\n    num_workers=n_cpu\n)","7d5ed8d0":"import matplotlib.pyplot as plt","11c794a0":"Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor","2c905e7a":"def sample_images():\n    \"\"\"show a generated sample from the test set\"\"\"\n    imgs = next(iter(val_dataloader))\n    G_AB.eval()\n    G_BA.eval()\n    real_A = imgs['A'].type(Tensor) # A : monet\n    fake_B = G_AB(real_A).detach()\n    real_B = imgs['B'].type(Tensor) # B : photo\n    fake_A = G_BA(real_B).detach()\n    # Arange images along x-axis\n    real_A = make_grid(real_A, nrow=5, normalize=True)\n    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n    real_B = make_grid(real_B, nrow=5, normalize=True)\n    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n    # Arange images along y-axis    \n    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n    plt.imshow(image_grid.cpu().permute(1,2,0))\n    plt.title('Real A vs Fake B | Real B vs Fake A')\n    plt.axis('off')\n    plt.show();","77a7254f":"temp_imgs = next(iter(val_dataloader))","791d8cf0":"G_AB.eval() # test mode \nG_BA.eval() # test mode\nprint(temp_imgs['A'].shape)\nprint(temp_imgs['B'].shape)\n","8c50c7ad":"temp_real_A = temp_imgs['A'].type(Tensor) # A : monet\ntemp_fake_B = G_AB(temp_real_A).detach()\ntemp_real_B = temp_imgs['B'].type(Tensor) # B : photo\ntemp_fake_A = G_BA(temp_real_B).detach()","3ca00867":"print(temp_real_A.shape)\nprint(temp_fake_B.shape)\nprint(temp_real_B.shape)\nprint(temp_fake_A.shape)","d4423fa6":"temp_real_A = make_grid(temp_real_A, nrow=5, normalize=True)\ntemp_real_B = make_grid(temp_real_B, nrow=5, normalize=True)\ntemp_fake_A = make_grid(temp_fake_A, nrow=5, normalize=True)\ntemp_fake_B = make_grid(temp_fake_B, nrow=5, normalize=True)","6e05026f":"type(temp_real_A)","14d79c25":"plt.imshow(temp_real_A.cpu().permute(1,2,0))\nplt.title('Real A')\nplt.axis('off');","131093c0":"print(temp_real_A.shape)\nprint(temp_fake_B.shape)\nprint(temp_real_B.shape)\nprint(temp_fake_A.shape)","e3c9e9c6":"temp_image_grid = torch.cat((temp_real_A, temp_fake_A, temp_real_B, temp_fake_B), 1)\nprint(temp_image_grid.shape)","5e0c89dc":"temp_image_grid.cpu().permute(1,2,0).shape","709b23b0":"plt.imshow(temp_image_grid.cpu().permute(1,2,0))\nplt.title('Real A | Fake B | Real B | Fake A ')\nplt.axis('off');","103dbf8a":"import matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport warnings","311a78a5":"for epoch in range(epoch, n_epochs):\n    for i, batch in enumerate(tqdm(dataloader)):\n        \n        # Set model input\n        real_A = batch['A'].type(Tensor)\n        real_B = batch['B'].type(Tensor)\n        \n        # Adversarial ground truths\n        valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n        fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n        \n# -----------------\n# Train Generators\n# -----------------\n        G_AB.train() # train mode\n        G_BA.train() # train mode\n        \n        optimizer_G.zero_grad() # Integrated optimizer(G_AB, G_BA)\n        \n        # Identity Loss\n        loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,\n        loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.\n                                                             # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').\n        loss_identity = (loss_id_A + loss_id_B)\/2\n        \n        # GAN Loss\n        fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'\n        \n        loss_GAN = (loss_GAN_AB + loss_GAN_BA)\/2\n        \n        # Cycle Loss\n        recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo\n        loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        \n        loss_cycle = (loss_cycle_A + loss_cycle_B)\/2\n        \n# ------> Total Loss\n        loss_G = loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)\n        \n        loss_G.backward()\n        optimizer_G.step()\n        \n# -----------------\n# Train Discriminator A\n# -----------------\n        optimizer_D_A.zero_grad()\n    \n        loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real\n        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake\n        \n        loss_D_A = (loss_real + loss_fake)\/2\n        \n        loss_D_A.backward()\n        optimizer_D_A.step()\n\n# -----------------\n# Train Discriminator B\n# -----------------\n        optimizer_D_B.zero_grad()\n    \n        loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real\n        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake\n        \n        loss_D_B = (loss_real + loss_fake)\/2\n        \n        loss_D_B.backward()\n        optimizer_D_B.step()\n        \n# ------> Total Loss\n        loss_D = (loss_D_A + loss_D_B)\/2\n    \n# -----------------\n# Show Progress\n# -----------------\n        if (i+1) % 50 == 0:\n            sample_images()\n            print('[Epoch %d\/%d] [Batch %d\/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'\n                    %(epoch+1,n_epochs,       # [Epoch -]\n                      i+1,len(dataloader),   # [Batch -]\n                      loss_D.item(),       # [D loss -]\n                      loss_G.item(),       # [G loss -]\n                      loss_GAN.item(),     # [adv -]\n                      loss_cycle.item(),   # [cycle -]\n                      loss_identity.item(),# [identity -]\n                     ))\n\n","c5668f30":"for i, batch in enumerate(dataloader):\n    print('iter : {}  A.size : {}'.format(i,batch['A'].size()))\n    print('iter : {}  B.size : {}'.format(i,batch['B'].size()))\n    if i == 10:\n        break","ee781dde":"temp_A = batch['A'].type(Tensor)","121fb8a3":"temp_A.shape","af75bbe6":"temp_img = temp_A.squeeze()\ntemp_img = temp_img.cpu().permute(1,2,0).numpy()","6b54d031":"temp_img.shape","5fc791d5":"plt.imshow(temp_img)\nplt.axis('off');","7f619cb4":"D_A.output_shape","47c26b7b":"temp_A.size()","32e711b5":"temp_A.size(0) # batch_size","2f8a3272":"Tensor(np.ones((temp_A.size(0), *D_A.output_shape)))","46d9c5fa":"Tensor(np.ones((temp_A.size(0), *D_A.output_shape))).shape","d6fec4f3":"### Step 13. Define function to get sample images","d676bdbc":"> TEST CODE : nn.Upsample","b98c1704":"> TEST CODE : Adversarial Ground Truths","fe348933":"We will implement the following structure with `PyTorch`","7ed85e6d":"## Main Reference\n1. [PyTorch-GAN | Github\/eriklindernoren | Collection of PyTorch implementations of GAN](https:\/\/github.com\/sw-song\/PyTorch-GAN)\n2. [CycleGAN | Github\/junyanz | Torch implementation for learning an image-to-image translation without input-output pairs](https:\/\/github.com\/junyanz\/CycleGAN)","9e8d3ae3":"### Step 5. Define Loss","53d1d16b":"### Step 9. Configure Optimizers","0a8ebf32":"So far we have looked at the cyclegan model, which does not require pairs. \n\nIt is a very important model among various gan architectures, and will be the basis for stargan.\n\n\nIf you are satisfied with this cyclegan tutorial, I recommend that you continue learning the next [stargan tutorial](https:\/\/www.kaggle.com\/songseungwon\/stargan-tutorial-with-15-steps-make-fake-images)\n\nThank you!","67a3d0d9":"### Step 1. Import Libraries","68ffa022":"# CycleGAN : Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n\n[--> Paper](https:\/\/openaccess.thecvf.com\/content_ICCV_2017\/papers\/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)\n\n![--> Paper](https:\/\/openaccess.thecvf.com\/content_ICCV_2017\/papers\/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.pdf)","481071d1":"### Step 8. Weight Setting","584958c6":"### Step 3. Define Generator","eb6bb1cb":"## Index\n```\nStep 1. Import Libraries\nStep 2. Initial Setting\nStep 3. Define Generator\nStep 4. Define Discriminator\nStep 5. Define Loss Function\nStep 6. Initialize Generator and Discriminator\nStep 7. GPU Setting\nStep 8. Weight Setting\nStep 9. Configure Optimizer\nStep 10. Learning Rate Scheduler Setting\nStep 11. Image Transformation Setting\nStep 12. DataLoader Setting\nStep 13. Define function to get sample images\nStep 14. Training\n```\n---","f163b07c":"`torch.nn.init.normal_(tensor, mean=0.0, std=1.0)`\n- Fills the input Tensor with values drawn from the normal distribution N(mean, std^2)","69e7179e":"### Step 10. Learning Rate Scheduler Setting","7176afba":"> TEST CODE : nn.ZeroPad2d","ac4be660":"### Step 2. Initial Setting","e9b9903b":"> TEST CODE : check iteration and image shape","e44b060a":"### Step 11. Image Transformation Setting","0585786d":"### Step 6. Initialize Generator and Discriminator","6a4f99a9":"> Read More\n- [Torch.nn.init](https:\/\/pytorch.org\/docs\/stable\/nn.init.html)","742c3050":"> Read More\n- [The Effect of the Identity Mapping Loss on Monet's painting](https:\/\/www.researchgate.net\/figure\/The-effect-of-the-identity-mapping-loss-on-Monets-painting-photos-From-left-to-right_fig3_322060135)","36b37da8":"### Step 4. Define Discriminator","1980f4b7":"> TEST CODE : `__class__.__name__`\n","c060b084":"### Step 7. GPU Setting","386820f9":"# CycleGAN Tutorial from Scratch\ncyclegan is a pix2pix image generation model that generates images from datasets that do not require pairs.\n\nWe will implement this cyclegan directly from the ground up with pytorch, and practice converting monet pictures into pictures and pictures into monet pictures.\n","7eac32cd":"![image](https:\/\/miro.medium.com\/max\/1838\/0*S5gn5i6UhfyoRr9S.png)","ef522222":"`torch.nn.init.constant_(tensor, val)`\n- Fills the input Tensor with value `val`\n","76113956":"### Step 12. DataLoader Setting","399585ad":"> Read More\n- [torch.nn.ReflectionPad2d(padding)](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.ReflectionPad2d.html)\n- [torch.nn.InstanceNorm2d(num_features, ...)](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.InstanceNorm2d.html)\n    - Example : Compare 4dim-Normalization methods,https:\/\/wdprogrammer.tistory.com\/67\n\n![image](https:\/\/www.notion.so\/image\/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa23e53c6-47a1-4ae6-96b0-f4836b5d82a8%2FUntitled.png?table=block&id=2e5405fd-24c7-49e1-aa2e-69a80d8b4bf4&width=2800&cache=v2)\n- [torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None)](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Upsample.html)","8fc06345":"### Step 14. Training","fbe5fc1b":"> TEST CODE : show image data"}}