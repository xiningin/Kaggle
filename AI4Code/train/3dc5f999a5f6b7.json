{"cell_type":{"98b33682":"code","3e5190cd":"code","128fb8b8":"code","4329c8fc":"code","29d9303b":"code","708b7b9e":"code","c2b85090":"code","6937cbbb":"code","9f669ae2":"code","c70d1081":"code","ebb1942b":"code","32633138":"markdown","bffc8305":"markdown","d0575b65":"markdown","8ca3ce90":"markdown","3856d783":"markdown","0d2e1384":"markdown","cdeac202":"markdown","059ba5ac":"markdown","8431bc22":"markdown","3e7b2d2c":"markdown"},"source":{"98b33682":"import numpy as np\nimport pandas as pd \nimport glob\nfrom skimage.transform import resize\nfrom matplotlib.pyplot import imread,imshow\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3e5190cd":"#Cat = 0 , Dog = 1\ntrain_cats_list = glob.glob(\"..\/input\/cat-and-dog\/training_set\/training_set\/cats\/*.jpg\")\ntrain_dogs_list = glob.glob(\"..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/*.jpg\")\n\ntest_cats_list = glob.glob(\"..\/input\/cat-and-dog\/test_set\/test_set\/cats\/*.jpg\")\ntest_dogs_list = glob.glob(\"..\/input\/cat-and-dog\/test_set\/test_set\/dogs\/*.jpg\")\n\n\ny_train = np.concatenate(( np.zeros((1, len(train_cats_list))), np.ones((1, len(train_dogs_list)), dtype=int)), axis =1) \ny_test = np.concatenate(( np.zeros((1, len(test_cats_list))), np.ones((1, len(test_dogs_list)), dtype=int)), axis =1)\n\nX = train_cats_list + train_dogs_list + test_cats_list + test_dogs_list\nY = np.concatenate((y_train, y_test),axis =1).T\n\nx = []\nfor image in X:\n    x.append(resize(imread(image),(64,64,1)))","128fb8b8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( np.array(x), Y, test_size=0.23, random_state=53)\n\nX_train = X_train.reshape((X_train.shape[0],64*64)).T\nX_test = X_test.reshape((X_test.shape[0],64*64)).T\n\n\ny_train = y_train.T\ny_test = y_test.T\n\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","4329c8fc":"def initialize_weights_bias(dimension):\n    w = np.full((dimension, 1),0.01)\n    b = 0.0\n    return w,b","29d9303b":"#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    return (1\/(1+np.exp(-z)))","708b7b9e":"#Steps;\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\ndef forward_propagation(w,b,X_train,y_train):\n    z = np.dot(w.T,X_train) + b\n    y_head = sigmoid(z)\n    loss =(-(1 -  y_train)*np.log(1 - y_head)) - (y_train * np.log(y_head))\n    cost = np.sum(loss)\/X_train.shape[1]\n    return cost, y_head","c2b85090":"#We will find the derivative from the weight and bias.\ndef backward_propagation(X_train,y_train, y_head):\n    derivative_weight = (np.dot(X_train,((y_head-y_train).T)))\/X_train.shape[1] # x_train.shape[0]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/X_train.shape[1]                 # x_train.shape[0]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return gradients","6937cbbb":"def update(w, b, x_train, y_train, learning_rate, number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost, y_head = forward_propagation(w,b,x_train,y_train)\n        gradients = backward_propagation(x_train, y_train, y_head)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 20 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            #print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    \n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients, cost_list","9f669ae2":"def predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","c70d1081":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(X_train, y_train, X_test, y_test,learning_rate = 0.01, num_iterations = 150)","ebb1942b":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0, max_iter=150, solver='liblinear')\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\nprint(\"test accuracy:  \",(clf.fit(X_train.T, y_train.T).score(X_test.T, y_test.T)))","32633138":"## **Logistic Regression without Sklearn**","bffc8305":"***Time to Try***","d0575b65":"# ***Data Preprocessing***","8ca3ce90":"# ***Logistic Regression with Sklearn***","3856d783":"***Sigmoid Function***","0d2e1384":"***Backward Propagation***","cdeac202":"***Forward Propagation***","059ba5ac":"***Updating Parameters***","8431bc22":"***Prediction***","3e7b2d2c":"***Initializing Parameters***"}}