{"cell_type":{"f36d212a":"code","3b9af602":"code","e64f1d29":"code","fde4f6a2":"code","c9dd1d62":"code","77a50a17":"code","efbe487d":"code","1526e844":"code","6359cdbf":"code","b59f22b4":"code","6fefb553":"code","213a9f04":"code","8ba92877":"code","ee5141e7":"code","ebb0e188":"code","87d2f615":"code","e5a7ff32":"code","7ba1775c":"code","d8bd3023":"code","2590517e":"code","c91de14a":"code","b6e784d4":"code","59f1004a":"code","3c5e1857":"code","aadd95dd":"code","35ecf07c":"code","c6d3bea0":"code","1ec2b39e":"code","282ee43b":"code","642c799d":"code","508c53bc":"code","5598ee6b":"code","d469885e":"code","26a507bb":"code","c2a286bb":"code","9a9c9ddc":"code","527fb58c":"code","f328489a":"code","1831af44":"code","c03de599":"code","7586c88b":"code","723c31db":"code","a1c00b50":"code","62c43524":"code","3abe397a":"code","80816978":"code","b63c9b10":"code","1f1cdc9a":"code","612ae567":"code","4b470d7c":"code","045f30b7":"code","d80ca1d0":"code","0c1e8dcf":"code","2fdbcd09":"code","04425feb":"code","d14edf4f":"code","da4c85f3":"code","d59e2391":"code","9fd58f07":"code","784d7739":"code","dc392e56":"code","380e315d":"code","8c66c72d":"code","8ac962ce":"code","9e92bdb6":"code","38f3ff2a":"code","7bf9843d":"code","f8af38b3":"code","2d4f4f00":"code","a1176c5f":"code","9e921358":"code","694c1bf3":"code","21934902":"code","5dbef255":"code","25156f14":"code","e2dc98c9":"code","9d313468":"code","af3f908f":"code","43cc50e5":"code","aa3cbc28":"code","bfe4e875":"code","22ea0c1f":"code","da03896e":"code","1e28ca22":"code","a005310f":"code","576379ef":"code","405d118c":"code","5e5288db":"code","73d45a72":"code","e51126f0":"code","f75781cb":"code","cfd979f3":"markdown","72438528":"markdown","5dbed361":"markdown","b97ea76e":"markdown","152dfa05":"markdown","34f2277d":"markdown","d0bb8b73":"markdown","aa65fc48":"markdown","878e55ce":"markdown","8695476b":"markdown","e5ac255f":"markdown","9442dca3":"markdown","fba5577c":"markdown","97d1225b":"markdown","a57a3a37":"markdown","b622d424":"markdown","812846b3":"markdown","5316dbb8":"markdown","186a530f":"markdown","c5323d97":"markdown","032ed42a":"markdown","d7474382":"markdown","b25279d8":"markdown","c2f5d96a":"markdown","ef4c1f02":"markdown","4b99b8fa":"markdown","9dee6d21":"markdown","e3a998cf":"markdown","13ecdd1f":"markdown","40e3913e":"markdown","61ee4b32":"markdown","68a48dab":"markdown","19847199":"markdown","ed26b177":"markdown","449a45fa":"markdown","35f3ace4":"markdown","c760f021":"markdown","96614352":"markdown","2f1ade70":"markdown","739dc467":"markdown","0d0504cf":"markdown","dd0e4a34":"markdown","a7cde82a":"markdown","1cffa7e4":"markdown","3524e698":"markdown","2f1d8035":"markdown","dbf0bc5c":"markdown","b5abc25a":"markdown","4dabee43":"markdown","bdb25dfc":"markdown","9af6346e":"markdown","b3169db2":"markdown","0b58f569":"markdown","79244fbb":"markdown","fb9fa38a":"markdown","a26b6d82":"markdown","f8635fd2":"markdown","9ae59128":"markdown","b85531e1":"markdown","e913f58e":"markdown","7a8b01fc":"markdown","fbd3b459":"markdown","7f89cd70":"markdown","43762453":"markdown"},"source":{"f36d212a":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom sklearn.model_selection import cross_val_score,GridSearchCV","3b9af602":"train_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","e64f1d29":"print(train_data.shape, test_data.shape)","fde4f6a2":"train_data.head()","c9dd1d62":"test_data.head()","77a50a17":"SalePrice = train_data['SalePrice']\nSalePrice.describe()","efbe487d":"ax = sns.boxplot(y = 'SalePrice', data = train_data)","1526e844":"train_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])","6359cdbf":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nsns.distplot(SalePrice, ax=ax[0])\nsns.distplot(train_data[\"SalePrice\"], ax=ax[1], axlabel = 'Transformation SalePrice')","b59f22b4":"fig = plt.figure()\nres = stats.probplot(SalePrice, plot=plt)\nplt.show()\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","6fefb553":"print(train_data['Id'].nunique() == train_data['Id'].count())\nprint(test_data['Id'].nunique() == test_data['Id'].count())","213a9f04":"#Save the 'Id' column\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train_data.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test_data.shape))","8ba92877":"ntrain = train_data.shape[0]\nntest = test_data.shape[0]\n\nY_train = train_data.SalePrice.values\n\nall_data = pd.concat((train_data, test_data)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","ee5141e7":"total = all_data.isnull().sum().sort_values(ascending = False)\npercent = (all_data.isnull().sum()\/all_data.shape[0]).sort_values(ascending = False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","ebb0e188":"all_data = all_data.drop((missing_data[missing_data['Percent'] >= 0.15]).index,1)\nall_data.head()","87d2f615":"for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    all_data[col] = all_data[col].fillna('None')\n","e5a7ff32":"for col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    all_data[col] = all_data[col].fillna(0)","7ba1775c":"for col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    all_data[col] = all_data[col].fillna('None')","d8bd3023":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","2590517e":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","c91de14a":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","b6e784d4":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","59f1004a":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","3c5e1857":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","aadd95dd":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","35ecf07c":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","c6d3bea0":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","1ec2b39e":"all_data = all_data.drop(['Utilities'], axis=1)","282ee43b":"all_data.isnull().sum().max()","642c799d":"ax = sns.boxplot(y = 'LotArea', data = train_data)","508c53bc":"#bivariate analysis saleprice\/LotArea\n# var = 'LotArea'\n# data = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\n# data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n","5598ee6b":"x = 3\nupper_lim = train_data['LotArea'].mean () + train_data['LotArea'].std () * x\nlower_lim = train_data['LotArea'].mean () - train_data['LotArea'].std () * x\nprint(upper_lim)\noutliers = train_data[train_data['LotArea'] > upper_lim]['LotArea']\nprint(outliers)","d469885e":"#Dropping the outlier rows with Percentiles\nupper_lim = train_data['LotArea'].quantile(.95)\nlower_lim = train_data['LotArea'].quantile(.05)\n\nprint(upper_lim)\n\noutliers = train_data[train_data['LotArea'] > upper_lim]['LotArea']\nprint(outliers)","26a507bb":"# Delete OutLiers\n# train_data = train_data[(train_data['LotArea'] < upper_lim) & (train_data['LotArea'] > lower_lim)]","c2a286bb":"\n# MSSubClass type of dwelling\nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\n\n# Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].apply(str)\n\n# Year and month sold are transform into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n","9a9c9ddc":"# Adding total sqfootage feature\n\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","527fb58c":"from sklearn.preprocessing import LabelEncoder\ncols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","f328489a":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","1831af44":"skewed_features = skewness.index\nall_data[skewed_features] = np.log1p(all_data[skewed_features])","c03de599":"all_data = pd.get_dummies(all_data)\nall_data","7586c88b":"train_data = all_data[:ntrain]\ntest_data = all_data[ntrain:]","723c31db":"# Train Test split\nX_train, X_test, y_train, y_test = train_test_split(train_data, Y_train, test_size=0.2, random_state=42)","a1c00b50":"# from sklearn.feature_selection import SelectPercentile\n# select = SelectPercentile(percentile=80)\n# select.fit(X_train, y_train)\n# X_train = select.transform(X_train)\n# X_test = select.transform(X_test)","62c43524":"X_train.shape","3abe397a":"# Linear Regression Model\nlr = LinearRegression()\nlr.fit(X_train, y_train)","80816978":"#Prediction\ny_pred = lr.predict(X_test)","b63c9b10":"# R square\nR2 = lr.score(X_test, y_test)\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(R2))","1f1cdc9a":"# Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))","612ae567":"# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error: ', mae)","4b470d7c":"E1 = ['Linear Regression', R2, rmse, mae ]","045f30b7":"ridge = Ridge().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(ridge.coef_ != 0)))","d80ca1d0":"best_score = 0\nbest_alpha = 0\nfor alpha in [0.01, 0.1, 1, 10, 100]:\n    ridge = Ridge(alpha=alpha).fit(X_train, y_train)\n    score = ridge.score(X_test, y_test)\n    if score > best_score:\n        best_score = score\n        best_alpha = alpha\nprint('Best score: {:.2f}'.format(best_score))\nprint('Best alpha: {:.2f}'.format(best_alpha))","0c1e8dcf":"ridge = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(ridge.coef_ != 0)))","2fdbcd09":"#Prediction\ny_pred = ridge.predict(X_test)","04425feb":"# R square\nR2 = ridge.score(X_test, y_test)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(R2))","d14edf4f":"# Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))","da4c85f3":"# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error: ', mae)","d59e2391":"E2 = ['Ridge Regression', R2, rmse, mae ]","9fd58f07":"lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))","784d7739":"lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))","dc392e56":"\ny_pred = lasso00001.predict(X_test)","380e315d":"# R square\nR2 = lasso00001.score(X_test, y_test)\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))","8c66c72d":"# Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))","8ac962ce":"# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error: ', mae)","9e92bdb6":"E3 = ['Lasso Regression', R2, rmse, mae ]","38f3ff2a":"tree = DecisionTreeRegressor().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(tree.score(X_test, y_test)))","7bf9843d":"tree = DecisionTreeRegressor(max_depth=5,random_state = 42).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(tree.score(X_test, y_test)))","f8af38b3":"y_pred = tree.predict(X_test)","2d4f4f00":"# R square\nR2 = tree.score(X_test, y_test)\nprint(\"Training set score: {:.2f}\".format(tree.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(tree.score(X_test, y_test)))","a1176c5f":"# Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))","9e921358":"# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error: ', mae)","694c1bf3":"E4 = ['Decision Tree Regression', R2, rmse, mae ]","21934902":"forest = RandomForestRegressor(n_estimators=10, random_state=42)\nforest.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))","5dbef255":"forest = RandomForestRegressor(n_estimators=100, random_state=42)\nforest.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))","25156f14":"y_pred = forest.predict(X_test)","e2dc98c9":"# R square\nR2 = forest.score(X_test, y_test)\nprint(\"Training set score: {:.2f}\".format(forest.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(forest.score(X_test, y_test)))","9d313468":"# Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))","af3f908f":"# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error: ', mae)","43cc50e5":"E5 = ['Random Forests', R2, rmse, mae ]","aa3cbc28":"import xgboost\nxgb1 = xgboost.XGBRegressor()","bfe4e875":"parameters = {\n              'learning_rate': [.01, 0.1], #so called `eta` value\n              'max_depth': [4,5],\n              'n_estimators': [100]}","22ea0c1f":"xgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 5,\n                        n_jobs = -1,\n                        verbose=True)\nxgb_grid.fit(X_train, y_train)","da03896e":"print(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","1e28ca22":"y_pred = xgb_grid.predict(X_test)","a005310f":"# R square\nR2 = xgb_grid.score(X_test, y_test)\nprint(\"Training set score: {:.2f}\".format(xgb_grid.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(xgb_grid.score(X_test, y_test)))","576379ef":"# Root Mean Squared Error\nmse = mean_squared_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"Root Mean Squared Error: {:.2f}\".format(rmse))","405d118c":"# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test, y_pred)\nprint('Mean Absolute Error: ', mae)","5e5288db":"E6 = ['XGboost', R2, rmse, mae ]","73d45a72":"df = pd.DataFrame([E1, E2, E3, E4, E5, E6],\n               columns =['Model', 'R Square', 'RMSE', 'MAE'])\ndf","e51126f0":"result = np.expm1(lasso00001.predict(test_data.values))\nresult","f75781cb":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = result\nsub.to_csv('submission.csv',index=False)","cfd979f3":"**Evaluate Lasso Regression**","72438528":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","5dbed361":"3 Best Metrics to evaluate Regression Model\n\n1. R Square\n\n    R square is a good measure to determine how well the model fits the dependent variables. However, it doesn't take into consideration of overfitting problem.\n    <img src=\"https:\/\/latex.codecogs.com\/svg.latex?\\Large&space; R^2= 1 - \\frac{\\sum_{i} (y_i-y_{ipred})^2}{\\sum_{i} (y_i-y_{mean})^2}\"\/>\n    \n2. Mean Square Error \/ Root Mean Square Error(RMSE)\n    \n    Mean Square Error is an absolute measure of the goodness for the fit.\n\n    <img src=\"https:\/\/latex.codecogs.com\/svg.latex?\\Large&space; MSE= \\frac{1}{N} \\sum^{N}_{i=1}  (y_i-y_{ipred})^2 \"\/>\n    \n    RMSE is square root of Mean Square Error\n\n3. Mean Absolute Error(MAE)\n\n    Compare to MSE\/RMSE, MAE is more direct representation of sum of error terms.\n    MSE gives larger penalisation to big prediction error by square it while MAE treats all error the same\n    <img src=\"https:\/\/latex.codecogs.com\/svg.latex?\\Large&space; MAE= \\frac{1}{N} \\sum^{N}_{i=1}  |y_i-y_{ipred}| \"\/>","b97ea76e":"# Import Libraries","152dfa05":"### Decision Tree Regression\n","34f2277d":"### Submission","d0bb8b73":"**Deleting variables with missing values**\n\nwe are going to delete varibles with ratio of missing values is greater than 15%.\n","aa65fc48":"**Prediction**","878e55ce":"- **SaleType** : Fill in again with most frequent which is \"WD\"","8695476b":"**Evaluate Linear Regression**","e5ac255f":"**Log-Transformation the target variable**\n\n\n\n\n\n","9442dca3":"- **BsmtQual,BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2:** for all these categorical basement-related feartures, Nan means that there is no basement.","fba5577c":" ## Handling categorical variables\n\nget dummies all categorical variables","97d1225b":"As expected, the accuracy on training set is 100% because the leaves are pure, the tree was grown deeo enough that it could perfectly memorize all the labels on the traning data. The test accuracy is around 81% accuracy. this model is overfitting.\n\nTo avoid overfitting and not generalizing well to new data, we are going to restrict the depth of a decision tree, the tree can become arbitrarily deep and complex. Now let's apply pre-pruning to the tree, which will stop developing the tree before we perfectly fit to the training data. Here we set max_depth = 5. Limiting the depth of the tree decreases overfitting. this leads to lower accuracy on the training set but improvement on the test set.","a57a3a37":"The main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. A random forest is essentially a collection of decision trees, where each tree is slightly different from the others.","b622d424":"- **GarageYrBlt, GarageArea and GarageCars:** Replacing missing data with 0 since no garage is no cars in such garage","812846b3":"### Adding one more important feature\n\nSince area related fetures are very important to determin house prices, we add one more feature which is the total are of basement, first and second floor areas of each house","5316dbb8":"- **Electrical :**  It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","186a530f":"### Transforming numerical variables that are categorical","c5323d97":"### Exploring Target Variable\n","032ed42a":"**Note**\n\nOutliers removel is not always safe. We can decided to delete them as they are very huge and bad. For example, 1000 as an age of a person.\n\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why, instead of removing them all. we will just mange to make some of our models robust on them.","d7474382":"### Random Forests","b25279d8":"**Prediction**","c2f5d96a":"**Evaluate Decision Tree Regression**","ef4c1f02":"### Lasso\n\nAn alternative to Ridge for regularizing linear regression. As with ridge regression, using the Lasso also retricts coefficients to be close to zero, but in slightly different way, called L1 regularization. The consequence of L1 regulazition is that using the Lasso, some coefficients are exactly zero. This means some fearures are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to interpret and can reveal the most important features of model. ","4b99b8fa":"The some values with bigger 'LotArea' seem strange and they are not following the crowd. we can define them as outliers and delete them.","9dee6d21":"**Imputing missing values**\n\n\n\n\n\n\n","e3a998cf":"**Evaluate Ridge Regression**","13ecdd1f":"# Data Preprocessing","40e3913e":"- **Functional**: data description says NA means typical","61ee4b32":"- target variable is positively skew\n- Normalizing target\/dependent variable by logarit transformation","68a48dab":"**Examples. Detect outliers by visualizing**","19847199":"**Concat train dataset and test dataset**","ed26b177":"**How to deal with missing data**\n\nThere are 2 ways we can deal with missing data.\n\n1. Delete variables or observations.\n\nin cases of deleting variables, when more than 50 or 30% of the data is missing, we should delete the corresponding variable. The point here is 'will we miss this data?' No. it is likely that these variables (e.g. 'PoolQC', 'MiscFeature') are not important aspects to consider when buying a house.\n\nIn what concerns the remaining cases, less percentage of missing data, we can delete these observations with missing data. Consider reducing the size of dataset\n\n2. Impute missing values\n\nThere is many options we could consider when replacing a missing value\n- a constant value that has meaning within the domain, such as ), distinct from all other values\n- a value from another randomly selected record\n- a mean, median or mode value for a column\n- a value estimated by another predictive model\n\n\nNotes: Not all algorithms fail when there is missing data.\nThere  are algorithms that can be made rebust to missing data, such as k-Nearest Neighbors that can ignore a column from a distance measure when a value is missing. Naive Bayes can aslo support missing values when making a prediction.\nSadly, the scikit-learn implementations of naive bayes, decision tree and k-Nearest Neighbor are not rebust to missing values. Although t is being considered,\n\n\n","449a45fa":"**Prediction**","35f3ace4":"- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no basement.","c760f021":"### Split Data","96614352":"### Missing Data\n\n\nImportant questions when thinking about missing data:\n\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?","2f1ade70":"# Evaluate models","739dc467":"### Linear Regression (Ordinary least squares)\n\nLinear regression is the simplest and most classic linear method for regression. Linear regression finds the parameters w and b that minimize the **mean squared error** between predictions and the true regression targets,y, on the training set. \n\nThe mean squared error is the sum of the squared differences between the predictions and the true values.","0d0504cf":"**Examples. Detect outliers with Percentiles**\nAnother mathematical method to detect outliers is to use percentiles.\n\nwe can assume that a certain percent of the value from the top or the bottom as an outlier. the key point is here to set the percaentage value once again and this pepends on the distribution of the data.\n\nAdditionally, a common mistake is using the percentiles according to the range of the data. In other words, if your data ranges from 0 to 100, your top 5% is not the values between 96 and 100. Top 5% means here the values that are out of the 95th percentile of data.\n\n\n","dd0e4a34":"**Prediction**","a7cde82a":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","1cffa7e4":"### Skewed Features","3524e698":"**Examples. Detect outliers with Standard Deviation**\n\nIf a value has a distance to the average higher than **x * standard deviation**, it can be assumed as an outlier. Then what **x** should be?\n\nthere is no trivial solution for **x**, but usually, a value between 2 and 4 seems practicals. the default value is 3.","2f1d8035":"**Searching best parameter for Ridge Regession**","dbf0bc5c":"### XGboost","b5abc25a":"### Checking Duplicate ","4dabee43":"### Lable Encoding some categorial variables which are ordial","bdb25dfc":"**Evaluate Random Forests**","9af6346e":"As you can see, the training ser score of Ridge is lower than for Linear Regression, while the test set score is higher. A less complex model means worse performance on training set, but better generalization. As we are only interested in generalization performance, we should choose the Rigde model over the LinearRegression model.\n\n\nIn this example, we used the defaul paramater alpha = 1.0. The optimim setting of alpha depends on the particular dataset we are using. Increasing alpha forces coefficients to move toward zero, which decreases training set performance but might help generalization.","b3169db2":"- **MasVnrArea and MasVnrType** NA most likely menas no masonry veneer for these houses, We can fill ) for the area and Non for the type","0b58f569":"# Model Building\n\n","79244fbb":"### Automatic Feature Selection\n\nThe purposes of selecting features optimizing to get high performance of models and remove irrelevant features. It is also help reduce dimensions. \n\nThere are three basic strategies: **univariate statistic, model-based selection and iterative selection**\n\n**Univariate Statistics**\n\n\nIn univariate statistics, we compute whether there is a statistically significant relationship between each feature to the target. Then the features that are related with highest confident are selected.\n\nUnivariate tests are often very fasr to compute, and don't require building a model. On the other hand, they are completely independent of the model that you might want to apply after the feature selection.","fb9fa38a":"### Ridge Regression\nRidge regression is also a linear model for regression, so the formula it ues to make predictions is the same one for ordinary least squares. we want the magnitue of coefficients to be as small as possible, in other words, all entries of w should be close to zero. This means each feature should have as little effect on the outcome as possible, while still predicting well. Regularization means explicitly restricting a model to avoid overfitting. The particular kind used by ridge regression is known as L2 regularization.","a26b6d82":"## Handling Outliers\n\nBefore mentioning how outliers can be handeled, it is true that the best way to detect the outliers is to demonstrate the data visually. All other statistical methodologies are open to making mistakes, whereas visualizing the outliers gives a chance to take a secision with high precision. \n\nStatistical mothodologies are less precise but on the other hand, they have a superioty and fast.\n\nThere are two diffirent ways of handing outliers detecting them using **standard deviation** and **percentiles**\n\nAfter detect outlier then we can decide to to delete outliers","f8635fd2":"Deleting Outliers","9ae59128":"# Import Data","b85531e1":"As we can see, Although the test set score increase to 76% accuracy, it's still low but this model is better than the previous model .","e913f58e":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","7a8b01fc":"- **GarageType, GarageFinish, GarageQual and GarageCond:** Replacing missing data with None\n","fbd3b459":"- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","7f89cd70":"**Is there any remaining missing value?**","43762453":"- **MSZoning (The general zoning classification):** 'RL' is by far the most common value. So we can fill in missing values with 'RL"}}