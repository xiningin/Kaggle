{"cell_type":{"3d86be6b":"code","c802b28b":"code","74cbede0":"code","db86d3a0":"code","5cd8e9bf":"code","415bfeee":"code","13780331":"code","b04f2c10":"code","6b2da509":"code","24a1694c":"code","e966e15d":"code","20039d4d":"code","21f2197f":"code","dc2815f1":"code","f5671e3d":"code","c7843efc":"code","35d5a7e2":"code","1c17254a":"code","91c1b569":"code","f1e437e3":"code","7b530cb3":"code","abd5754b":"code","55ef0d14":"code","12922446":"code","6586e7be":"code","40ae05f2":"code","552be9c8":"code","f16677ee":"code","cc1c29d9":"code","7aa31c2e":"code","60043cb7":"markdown","b8b49abd":"markdown","faff21fc":"markdown","d6820d3c":"markdown","41cd8e12":"markdown","fe0f0958":"markdown","9747dd3f":"markdown","4f9241d2":"markdown","3c20e451":"markdown","da2ca57e":"markdown","745bb682":"markdown","ad5c4faa":"markdown","9760ac68":"markdown","216a8a79":"markdown","b2688dee":"markdown","173187a4":"markdown","6f2d64f2":"markdown","0c044957":"markdown","60f50788":"markdown","2d1c853d":"markdown","00ac8b45":"markdown","10e06d53":"markdown","b6adb9d0":"markdown"},"source":{"3d86be6b":"# to prevent unnecessary warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport subprocess\nimport cv2\nimport json\nimport requests\nfrom tqdm import tqdm\n\nprint(tf.__version__)","c802b28b":"fashion_mnist = tf.keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n\nprint('\\nTrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\nprint('Test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))","74cbede0":"# reshape for feeding into the model\ntrain_images_gr = train_images.reshape(train_images.shape[0], 28, 28, 1)\ntest_images_gr = test_images.reshape(test_images.shape[0], 28, 28, 1)\n\nprint('\\nTrain_images.shape: {}, of {}'.format(train_images_gr.shape, train_images_gr.dtype))\nprint('Test_images.shape: {}, of {}'.format(test_images_gr.shape, test_images_gr.dtype))","db86d3a0":"fig, ax = plt.subplots(2, 5, figsize=(12, 6))\nc = 0\nfor i in range(10):\n    idx = i \/\/ 5\n    idy = i % 5 \n    ax[idx, idy].imshow(train_images_gr[i].reshape(28,28))\n    ax[idx, idy].set_title(class_names[train_labels[i]])","5cd8e9bf":"# define input shape\nINPUT_SHAPE = (28, 28, 1)\n\n# define sequential model\nmodel = tf.keras.models.Sequential()\n# define conv-pool layers - set 1\nmodel.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), \n                                activation='relu', padding='valid', input_shape=INPUT_SHAPE))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n# define conv-pool layers - set 2\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), \n                                activation='relu', padding='valid', input_shape=INPUT_SHAPE))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n# add flatten layer\nmodel.add(tf.keras.layers.Flatten())\n\n# add dense layers with some dropout\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=0.3))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=0.3))\n\n# add output layer\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n# compile model\nmodel.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\n# view model layers\nmodel.summary()","415bfeee":"EPOCHS = 20\ntrain_images_scaled = train_images_gr \/ 255.\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, \n                                               restore_best_weights=True,\n                                               verbose=1)\n\nhistory = model.fit(train_images_scaled, train_labels,\n                    batch_size=32,\n                    callbacks=[es_callback], \n                    validation_split=0.1, epochs=EPOCHS,\n                    verbose=1)","13780331":"import pandas as pd\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot(kind='line', ax=ax[0])","b04f2c10":"import pandas as pd\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot(kind='line', ax=ax[0])\nhistory_df[['accuracy', 'val_accuracy']].plot(kind='line', ax=ax[1]);","6b2da509":"test_images_scaled = test_images_gr \/ 255.\npredictions = model.predict(test_images_scaled)\npredictions[:5]","24a1694c":"prediction_labels = np.argmax(predictions, axis=1)\nprediction_labels[:5]","e966e15d":"from sklearn.metrics import confusion_matrix, classification_report\nimport pandas as pd\n\nprint(classification_report(test_labels, prediction_labels, target_names=class_names))\npd.DataFrame(confusion_matrix(test_labels, prediction_labels), index=class_names, columns=class_names)","20039d4d":"print(test_labels[:100])","21f2197f":"test_labels","dc2815f1":"test_image_idxs = [0, 23, 28]\ntest_labels[test_image_idxs]","f5671e3d":"layer_outputs = [layer.output for layer in model.layers]\nactivation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)","c7843efc":"layer_outputs \nactivation_model ","35d5a7e2":"f, axarr = plt.subplots(3,4, figsize=(8, 5))\n\nFIRST_IMAGE=0\nSECOND_IMAGE=23\nTHIRD_IMAGE=28\nCONVOLUTION_NUMBER = 13\n\nfor x in range(0,4):\n  f1 = activation_model.predict(test_images_scaled[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[0,x].grid(False)\n  axarr[0,x].set_title(class_names[test_labels[FIRST_IMAGE]])\n  f2 = activation_model.predict(test_images_scaled[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[1,x].grid(False)\n  axarr[1,x].set_title(class_names[test_labels[SECOND_IMAGE]])\n  f3 = activation_model.predict(test_images_scaled[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[2,x].grid(False)\n  axarr[2,x].set_title(class_names[test_labels[THIRD_IMAGE]])\nplt.tight_layout()","1c17254a":"f, axarr = plt.subplots(3,4, figsize=(8, 5))\n\nFIRST_IMAGE=2\nSECOND_IMAGE=3\nTHIRD_IMAGE=5\nCONVOLUTION_NUMBER = 13\n\nfor x in range(0,4):\n  f1 = activation_model.predict(test_images_scaled[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[0,x].grid(False)\n  axarr[0,x].set_title(class_names[test_labels[FIRST_IMAGE]])\n  f2 = activation_model.predict(test_images_scaled[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[1,x].grid(False)\n  axarr[1,x].set_title(class_names[test_labels[SECOND_IMAGE]])\n  f3 = activation_model.predict(test_images_scaled[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[2,x].grid(False)\n  axarr[2,x].set_title(class_names[test_labels[THIRD_IMAGE]])\nplt.tight_layout()","91c1b569":"f, axarr = plt.subplots(3,4, figsize=(8, 5))\n\nFIRST_IMAGE=2\nSECOND_IMAGE=3\nTHIRD_IMAGE=5\nCONVOLUTION_NUMBER = 3\n\nfor x in range(0,4):\n  f1 = activation_model.predict(test_images_scaled[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[0,x].grid(False)\n  axarr[0,x].set_title(class_names[test_labels[FIRST_IMAGE]])\n  f2 = activation_model.predict(test_images_scaled[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[1,x].grid(False)\n  axarr[1,x].set_title(class_names[test_labels[SECOND_IMAGE]])\n  f3 = activation_model.predict(test_images_scaled[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='binary_r')\n  axarr[2,x].grid(False)\n  axarr[2,x].set_title(class_names[test_labels[THIRD_IMAGE]])\nplt.tight_layout()","f1e437e3":"train_images_3ch = np.stack([train_images]*3, axis=-1)\ntest_images_3ch = np.stack([test_images]*3, axis=-1)\n\nprint('\\nTrain_images.shape: {}, of {}'.format(train_images_3ch.shape, train_images_3ch.dtype))\nprint('Test_images.shape: {}, of {}'.format(test_images_3ch.shape, test_images_3ch.dtype))","7b530cb3":"import cv2\n\ndef resize_image_array(img, img_size_dims):\n    img = cv2.resize(img, dsize=img_size_dims, \n                     interpolation=cv2.INTER_CUBIC)\n    img = np.array(img, dtype=np.float32)\n    return img","abd5754b":"%%time\n\nIMG_DIMS = (32, 32)\n\ntrain_images_3ch = np.array([resize_image_array(img, img_size_dims=IMG_DIMS) for img in train_images_3ch])\ntest_images_3ch = np.array([resize_image_array(img, img_size_dims=IMG_DIMS) for img in test_images_3ch])\n\nprint('\\nTrain_images.shape: {}, of {}'.format(train_images_3ch.shape, train_images_3ch.dtype))\nprint('Test_images.shape: {}, of {}'.format(test_images_3ch.shape, test_images_3ch.dtype))","55ef0d14":"# define input shape\nINPUT_SHAPE = (32, 32, 3)\n\n# get the VGG19 model\nvgg_layers = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=False, \n                                               input_shape=INPUT_SHAPE)\n\nvgg_layers.summary()","12922446":"# Fine-tune all the layers\nfor layer in vgg_layers.layers:\n    layer.trainable = True\n\n# Check the trainable status of the individual layers\nfor layer in vgg_layers.layers:\n    print(layer, layer.trainable)","6586e7be":"# define sequential model\nmodel = tf.keras.models.Sequential()\n\n# Add the vgg convolutional base model\nmodel.add(vgg_layers)\n\n# add flatten layer\nmodel.add(tf.keras.layers.Flatten())\n\n# add dense layers with some dropout\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=0.3))\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dropout(rate=0.3))\n\n# add output layer\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n# compile model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\n# view model layers\nmodel.summary()","40ae05f2":"EPOCHS = 20\ntrain_images_3ch_scaled = train_images_3ch \/ 255.\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, \n                                               restore_best_weights=True,\n                                               verbose=1)\n\nhistory = model.fit(train_images_3ch_scaled, train_labels,\n                    batch_size=32,\n                    callbacks=[es_callback], \n                    validation_split=0.1, epochs=EPOCHS,\n                    verbose=1)","552be9c8":"fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df[['loss', 'val_loss']].plot(kind='line', ax=ax[0])\nhistory_df[['accuracy', 'val_accuracy']].plot(kind='line', ax=ax[1]);","f16677ee":"test_images_3ch_scaled = test_images_3ch \/ 255.\npredictions = model.predict(test_images_3ch_scaled)\npredictions[:5]","cc1c29d9":"prediction_labels = np.argmax(predictions, axis=1)\nprediction_labels[:5]","7aa31c2e":"print(classification_report(test_labels, prediction_labels, target_names=class_names))\npd.DataFrame(confusion_matrix(test_labels, prediction_labels), index=class_names, columns=class_names)","60043cb7":"## Reshaping Image Data for Modeling\n\nWe do need to reshape our data before we train our model. Here we will convert the images to 3-channel images (image pixel tensors) as the VGG model was originally trained on RGB images","b8b49abd":"## Build CNN Model Architecture\n\nWe will now build our CNN model architecture customizing the VGG-19 model.","faff21fc":"We will build the following two deep learning CNN (Convolutional Neural Network) classifiers in this notebook.\n- A simple CNN trained from scratch\n- Fine-tuning a pre-trained VGG-19 CNN\n\nThe idea is to look at how to build your own CNN from scratch and also to briefly look at transfer learning where you fine-tune a pre-trained model to adapt it to classify images based on your dataset.","d6820d3c":"## Model Training\n\nLet\u2019s train our model for 100 epochs and look at the performance. We do apply an early-stopping to stop the model training immediately once we don't see an improvement in validation-loss over the last 2 epochs using the `EarlyStopping` callback.","41cd8e12":"### Build CNN model on top of VGG19","fe0f0958":"# 2. Fine-tuning a pre-trained VGG-19 CNN Model\n\nHere, we will use a VGG-19 model which was pre-trained on the ImageNet dataset by fine-tuning it on the Fashion-MNIST dataset. ","9747dd3f":"### Build Cut-VGG19 Model","4f9241d2":"# Main Objective \u2014 Building & Deploying an Apparel Classifier\n\nWe will keep things simple here with regard to the key objective. We will build a simple apparel classifier by training models on the very famous [Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist) dataset based on Zalando\u2019s article images \u2014 consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. The idea is to classify these images into an apparel category amongst 10 categories on which we will be training our models on.\n\nHere's an example how the data looks (each class takes three-rows):\n\n<table>\n  <tr><td>\n    <img src=\"https:\/\/raw.githubusercontent.com\/zalandoresearch\/fashion-mnist\/master\/doc\/img\/fashion-mnist-sprite.png\"\n         alt=\"Fashion MNIST sprite\"  width=\"600\">\n  <\/td><\/tr>\n  <tr><td align=\"center\">\n    <a href=\"https:\/\/github.com\/zalandoresearch\/fashion-mnist\">Fashion-MNIST samples<\/a> (by Zalando, MIT License).<br\/>&nbsp;\n  <\/td><\/tr>\n<\/table>\n\nFashion MNIST is intended as a drop-in replacement for the classic [MNIST](http:\/\/yann.lecun.com\/exdb\/mnist\/) dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision. You can access the Fashion MNIST directly from TensorFlow.\n\n__Note:__ Although these are really images, they are loaded as NumPy arrays and not binary image objects.","3c20e451":"## Reshaping Image Data for Modeling\n\nWe do need to reshape our data before we train our model. Here we will work on grayscale, 1-channel images (image pixel tensors)","da2ca57e":"## Plot Learning Curves","745bb682":"# Convolutional Neural Networks - Building CNN Classifiers\n\n\nThe most popular deep learning models leveraged for computer vision problems are convolutional neural networks (CNNs)!\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*rO65zGl2k7xVBRhJZggQSA.gif)\n\nWe will look at how we can build, train and evaluate a multi-class CNN classifier in this notebook.\n\n<div style=\"text-align: right\"><i><font size=\"2\">Tutorial by: <a href=\"https:\/\/www.linkedin.com\/in\/dipanzan\" target=\"_parent\">Dipanjan (DJ) Sarkar<\/a><\/font><\/i><\/div>\n","ad5c4faa":"# Load Dependencies \n\nThis leverages the __`tf.keras`__ API style and hence it is recommended you try this out on TensorFlow 2.x","9760ac68":"# 1. Training a simple CNN Classifier from Scratch\n\nIn this section, we will train a basic 2-layer CNN model from scratch.","216a8a79":"## Transfer Learning\n\n![](https:\/\/i.imgur.com\/WcUabml.png)","b2688dee":"## Model Architecture Details\n\n![](https:\/\/i.imgur.com\/1VZ7MlO.png)","173187a4":"## Resizing Image Data for Modeling\n\nThe minimum image size expected by the VGG model is 32x32 so we need to resize our images","6f2d64f2":"# Brief on CNNs\n\nCNNs typically consist of multiple convolution and pooling layers which help the deep learning model in automatically extracting relevant features from visual data like images. Due to this multi-layered architecture, CNNs learn a robust hierarchy of features, which are spatial, rotation, and translation invariant.\n\n![](https:\/\/miro.medium.com\/max\/1456\/1*NKL76WYQwH5LuqyaQTjBNw.png)\n\nThe key operations in a CNN model are depicted in the figure above. Any image can be represented as a tensor of pixel values. The convolution layers help in extracting features from this image (forms feature maps). Shallower layers (closer to the input data) in the network learn very generic features like edges, corners and so on. Deeper layers in the network (closer to the output layer) learn very specific features pertaining to the input image. The following graphic helps summarize the key aspects of any CNN model.\n\n![](https:\/\/miro.medium.com\/max\/1366\/1*nCQeDMjKoTGst1RiCDo9Fw.png)\n\nWe will be building a CNN from scratch as well as leverage the power of transfer learning, where we use pre-trained deep learning CNNs in this tutorial. ","0c044957":"## Evaluate Model Performance on Test Data","60f50788":"# Load Dataset\nHere we will leverage an out of the box dataset to keep things simple as we mentioned earlier by leveraging the Fashion-MNIST dataset.","2d1c853d":"## Evaluate Model Performance on Test Data","00ac8b45":"## Build CNN Model Architecture\n\nWe will now build our basic 2-layer CNN model architecture.","10e06d53":"## Plot Learning Curves","b6adb9d0":"### Set layers to trainable to enable fine-tuning"}}