{"cell_type":{"15b728f0":"code","b20357fd":"code","d859d566":"code","d9d94e57":"code","88d42523":"code","d8b175ff":"code","b9694d91":"code","4a4e31cf":"code","a7a4dcf0":"code","2c408d0b":"code","15aee45c":"code","03522558":"code","7a55ae2a":"code","9019afc8":"code","2ae41483":"markdown","a4a5288d":"markdown","cf3696b8":"markdown","ffd2f7e5":"markdown","072c7182":"markdown","7767965f":"markdown","a7cf2757":"markdown","4b620557":"markdown","2daffc28":"markdown","795d2ecf":"markdown","6504b3e0":"markdown","4ab4383a":"markdown","edc59f0d":"markdown","683224bf":"markdown"},"source":{"15b728f0":"import pandas as pd\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instead of data gathering, just check input\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b20357fd":"pd.options.display.max_rows = None\npd.options.display.max_columns = None\n\n# Path of the file to read\niowa_file_path = '..\/input\/home-data-for-ml-course\/train.csv'\n# Load data\nhome_data = pd.read_csv(iowa_file_path)\n# Print the list of columns in the dataset to find the name of the prediction target\nprint(home_data.columns)\nhome_data.head()","d859d566":"def report_structure(df):\n    '''\n    Function report_structure \n    - takes an input dataframe\n    - sets up anxiliary functions\n    - returns a new dataframe with description of the input dataframe's columns.\n    '''\n\n    descriptors = []\n    def myCount(f):\n        try:\n            return df[f].count()\n        except:\n            return 'Not applicable'\n    \n    def cUnique(f):\n        try:\n            return df[f].nunique()\n        except:\n            return 'Not applicable'\n\n    def min_item(f):\n        try:\n            return df[f].min()\n        except:\n            return 'Not applicable'\n\n    def max_item(f):\n        try:\n            return df[f].max()\n        except:\n            return 'Not applicable'\n\n    def top_item(f):\n        try:\n            return df[f].value_counts().index[0]\n        except:\n            return 'Not applicable'\n\n    def top_freq(f):\n        try:\n            return df[f].value_counts().values[0]\n        except:\n            return 'Not applicable'\n    \n    def unique(f):\n        try:\n            l = list(df[f].unique())\n            if len(l) < 10:\n                return l\n            else:\n                return l[:2]+['...']+l[-2:]\n        except:\n            return 'Not applicable'\n\n    for col in list(df.columns):\n        descriptors.append([myCount(col), cUnique(col), min_item(col), max_item(col), top_item(col), top_freq(col), unique(col)])\n    out = pd.DataFrame.from_records(descriptors, index=df.columns, columns=['count','cUnique','min', 'max', 'top','freq', 'unique'])\n    return out\n","d9d94e57":"report_structure(home_data)","88d42523":"# Set up a target object and call it y\ny = home_data.SalePrice\n# Select features and create X\nfeature_names = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\nX = home_data[feature_names]\n# Review data\n# print description or statistics from X\nX.describe()","d8b175ff":"# print the top few lines\nX.head()","b9694d91":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1)\nprint(f'The original dataframe X with {len(X)} observations has been splitted with the ratio {round(len(train_X)\/len(X)*100)} : {round(len(val_X)\/len(X)*100)}.')\nprint('Shapes of tranining and validation datasets:',[df.shape for df in [train_X, val_X, train_y, val_y]])","4a4e31cf":"# For model reproducibility, set a numeric value for random_state when specifying the model\niowa_model = DecisionTreeRegressor(random_state=1)\n\n# Fit the model\niowa_model.fit(train_X, train_y)","a7a4dcf0":"val_predictions = iowa_model.predict(val_X)\nmae = mean_absolute_error(val_y, val_predictions)\nprint(f'MAE = ${mae:,.2f} and y.mean = ${y.mean():,.2f}, so inaccuracy is {mae\/y.mean():.1%} !')","2c408d0b":"def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\nscores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\nprint([str(k)+' : $'+str(round(v)) for k,v in scores.items()])\n\nbest_tree_size = min(scores, key=scores.get)\nprint('With best tree size:', best_tree_size)\n\niowa_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\niowa_model.fit(train_X, train_y)\nval_predictions = iowa_model.predict(val_X)\nmae = mean_absolute_error(val_y, val_predictions)\nprint(f'MAE = ${mae:,.2f} and y.mean = ${y.mean():,.2f}, so inaccuracy is {mae\/y.mean():.1%} !')","15aee45c":"# Fill in argument to make optimal size and fit the final model\nfinal_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size)\nfinal_model.fit(X, y)\niowa_preds = final_model.predict(val_X)\nmae_f = mean_absolute_error(val_y, iowa_preds)\nprint('There is no sense in validating the final model as')\nprint(f'MAE = ${mae_f:,.2f} and y.mean = ${y.mean():,.2f}, so inaccuracy is now only {mae_f\/y.mean():.1%}. This is a very biased result!!')","03522558":"forest_model = RandomForestRegressor()\nforest_model.fit(train_X, train_y)\niowa_preds = forest_model.predict(val_X)\nmae_rf = mean_absolute_error(val_y, iowa_preds)\nprint(f'MAE = ${mae_rf:,.2f} and y.mean = ${y.mean():,.2f}, so inaccuracy is {mae_rf\/y.mean():.1%} !')","7a55ae2a":"# To improve accuracy, create a new Random Forest model which you will train on all training data\nrf_model_on_full_data = RandomForestRegressor()\n\n# Fit rf_model_on_full_data on all data from the training data\nrf_model_on_full_data.fit(X, y)\n# Path to file you will use for predictions\ntest_data_path = '..\/input\/home-data-for-ml-course\/test.csv'\n\n# Read test data file using pandas\ntest_data = pd.read_csv(test_data_path)\n\n# Create test_X which comes from test_data but includes only the columns you used for prediction.\n# The list of columns is stored in a variable\ntest_X = test_data[feature_names]\n\n# Make predictions which we will submit. \ntest_preds = rf_model_on_full_data.predict(test_X)\n\n# The lines below shows how to save predictions in format used for competition scoring\n\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': test_preds})\noutput.to_csv('hpc_submission.csv', index=False)","9019afc8":"print('Output shape:', output.shape)\noutput.head()","2ae41483":"# Introduction\n## Purpose\nThe \"Intro to Machine Learning\" tutorial's purpose is to **learn the core ideas in machine learning, to build your first models, and to submit predictions for a Kaggle competitions.**\n\nThe purpose of this notebook is to give a \"one-page\" summary of what I've learned.\n\n## Context\n>Your cousin has made millions of dollars speculating on real estate. He's offered to become business partners with you because of your interest in data science. He'll supply the money, and you'll supply models that predict how much various houses are worth.\n\n## Concept Model of Machine Learning\n![](http:\/\/www.plantuml.com\/plantuml\/png\/NP9FYzim4CNl_XJ3zh1sMvkUGrYMGaeB_PF3mkx14BJsE16L9QCP1zBIxzuPAQwtUuhUQ7vldYQ-9pQHvz4L9ziZT3Ps3YaB72U-m8ZZCqOgYjllePUhpXaYk6azm3SfE83Mtu0X69FwNTGG9hQZ_OMnh42a2qI7OVOTs-3BglYpU2I-zViGUGZEXglD58QbunCQdYEsVkUFrYD6wu-fQy2bvI4QwwKClM6JxlGWETx1bIPu0b4F9XwHuLBKjQYfRoAQ_j3HkQn4izeS68aFD3dBQyvrHElLcf1rJ2PapiBAaELuMTdTsRZPwCl_fwMihFuAcGylkHyJneGPbq4emqHLOkMGM4qhy0h1taGp8cCKePsJQaX_oYFQuua97a74Hsi82PvNzFD51awly5Cg2Fu6lgA9QSsI2aL_yPQjST05trlDIR2QxIFsjikFbiu18_eEisUOUPevvMyuvFqBMTpk-YDIaVv-U5lNr-zf0s4Gdk2k612ahBkGAiyUjPYxf7y9cwzW--9ckN3wXXMgeswalbiYRM0qFv5Qty2K7r3c3LV_aAsuwVOtqX4w93NJGAlNadecTO9ci5m-lRzKiCsDaYT_9Aiy6mL6v6WxokMc_50rEgcuHz_Fe_iB)\n\nThis diagram includes key concepts used throughout the tutorial and throughout the article [The 7 Steps of Machine Learning](https:\/\/towardsdatascience.com\/the-7-steps-of-machine-learning-2877d7e5548e).\n\n## Notebook overview\nWe'll start with a model called the [Decision Tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree). There are fancier models that give more accurate predictions. But decision trees are easy to understand, and they are the basic building block for some of the best models in data science. We'll try also [Random forest](https:\/\/en.wikipedia.org\/wiki\/Random_forest) before preparing submission for competition scoring. The notebook is structured to these parts:\n+ Set Up the Environment\n+ Prepare the Data\n+ Predict with Decision Tree Model\n+ Predict with Random Forest Model\n+ Prepare a Submission For the Competition\n\n# Set Up the Environment\nWe'll need these modules and objects:\n![](http:\/\/www.plantuml.com\/plantuml\/png\/RL3BRiCm3BlxAuoUza3_eOUY7r3i7YWsjnZpmv2K3SMmVv_ZfkXQz1ABf4WfseaIwvoYcOA7TO5TX9m1KjMJJKWZM8nyXbo9ATc9il_ce8fibMVyarB9nKrS4ivAaoA8ittPRXi3cENJqHukI2ZvZO4ZFXWYXM_wK_68Wo32QMiqTtZDf907XUWWDGQz3O1oS6BMT-Ke3pHAYRDY0FFTK6H16YFCfUZiaR8lwL0OejTbZOiaOZUrgIGRIsAhDzLt2uFyuEHhRTgJqe4fmVwOL-lQ-9IbLJ9HHNkzdYeMOcg-f-U5WJWE8phGicIrydVDlmPUrkZdwUdlGRkfCdnmsbtcuRtjvSpcuHzUvHsodrFy0m00)","a4a5288d":"# Prepare the Data\n## Load and Understand Your Data","cf3696b8":"## Fit the Final Model\nYou know the best tree size. If you were going to deploy this model in practice, you would make it even more accurate by using all of the data and keeping that tree size. That is, you don't need to hold out the validation data now that you've made all your modeling decisions.","ffd2f7e5":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/161285) to chat with other Learners.*","072c7182":"# Predict with Decision Tree Model\n## Specify and Fit the Model","7767965f":"## Set Up Features and a Target","a7cf2757":"There is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.\n\nSo far, you have followed specific instructions at each step of your project. This helped learn key ideas and build your first model, but now you know enough to try things on your own.\n\nMachine Learning competitions are a great way to try your own ideas and learn more as you independently navigate a machine learning project.\n\n# Prepare a Submission For the Competition","4b620557":"As `the DataFrame.descibe()` function reports only numerical columns, I have the `report_structure` function to get deeper insight:","2daffc28":"**This notebook was an exercise in the [Introduction to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning) course.**\n\n---\n","795d2ecf":"You've tuned this model and improved your results. But we are still using Decision Tree models, which are not very sophisticated by modern machine learning standards. In the next step you will learn to use Random Forests to improve your models even more.","6504b3e0":"## Split Features X into Training and Validation Datasets\nSplit data into training and validation data, for both features and target. The split is based on a random number generator. Supplying a numeric value to the random_state argument guarantees we get the same split every time we run this script.","4ab4383a":"## Make Validation Predictions and Calculate MAE","edc59f0d":"# Predict with Random Forest Model\nThe random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.","683224bf":"## Optimize the Model\nThere are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way **to control overfitting vs underfitting**. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.\n\nWe can use a utility function to help compare MAE scores from different values for max_leaf_nodes:"}}