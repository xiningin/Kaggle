{"cell_type":{"7ebc3bef":"code","6f332fcd":"code","a2ae4494":"code","0dd26f15":"code","2537dca8":"code","eb218c77":"code","44575c89":"code","1a697d4c":"code","2b1d70ac":"code","04d64e87":"code","da449b9d":"code","385d8b01":"code","d7ea7d4d":"code","aa40c5f3":"code","4c17648b":"code","3af840b3":"code","e3b0346d":"code","f5058e3b":"code","4486a75f":"code","ea002887":"code","17e53860":"code","46969fca":"code","47a7df58":"code","343f91cc":"code","a795138b":"code","3a9f1b06":"code","d0162df1":"code","6bac53c7":"code","6ce3c31d":"code","7dc28381":"code","f8f0f9e3":"code","8ab5c14a":"code","90499859":"code","82547de2":"markdown","c3aa2679":"markdown","01cc1830":"markdown","254849a3":"markdown","910c8b6d":"markdown","b1e4a80c":"markdown","e234f344":"markdown","819dd30c":"markdown","009f72fa":"markdown","4c0f5aaf":"markdown"},"source":{"7ebc3bef":"!pip install imageio-ffmpeg\n# !pip install clone+https:\/\/github.com\/luna983\/stitch-aerial-photos.git","6f332fcd":"import os\nimport cv2\nimport imageio\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom tqdm.auto import tqdm","a2ae4494":"## https:\/\/www.kaggle.com\/go5kuramubon\/merge-label-and-tracking-data\n\n# Read in data files\nBASE_DIR = '..\/input\/nfl-health-and-safety-helmet-assignment'\n\n# Labels and sample submission\nlabels = pd.read_csv(f'{BASE_DIR}\/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}\/sample_submission.csv')\n\n# Player tracking data\ntr_tracking = pd.read_csv(f'{BASE_DIR}\/train_player_tracking.csv')\nte_tracking = pd.read_csv(f'{BASE_DIR}\/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntr_helmets = pd.read_csv(f'{BASE_DIR}\/train_baseline_helmets.csv')\nte_helmets = pd.read_csv(f'{BASE_DIR}\/test_baseline_helmets.csv')\n\n# Extra image labels\nimg_labels = pd.read_csv(f'{BASE_DIR}\/image_labels.csv')","0dd26f15":"##https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) \/ 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\n\ntr_tracking = add_track_features(tr_tracking)\nte_tracking = add_track_features(te_tracking)\n","2537dca8":"def merge_label_and_tracking(tracking_df, label_df):\n\n    tracking_with_game_index = tracking_df.set_index([\"gameKey\", \"playID\", \"player\"])\n\n    df_list = []\n\n    for key, _label_df in tqdm(label_df.groupby([\"gameKey\", \"playID\", \"view\", \"label\"])):\n        # skip because there are sideline player\n        if key[3] == \"H00\" or key[3] == \"V00\":\n            continue\n\n        tracking_data = tracking_with_game_index.loc[(key[0], key[1], key[3])]\n        _label_df = _label_df.sort_values(\"frame\")\n\n        # merge with frame and est_frame\n        merged_df = pd.merge_asof(\n            _label_df,\n            tracking_data,\n            left_on=\"frame\",\n            right_on=\"est_frame\",\n            direction='nearest',\n        )\n        df_list.append(merged_df)\n\n    all_merged_df = pd.concat(df_list)\n    all_merged_df = all_merged_df.sort_values([\"video_frame\", \"label\"], ignore_index=True)\n    \n    return all_merged_df","eb218c77":"merged_df = merge_label_and_tracking(tr_tracking, labels)","44575c89":"merged_df.frame","1a697d4c":"unique_gameKeys = merged_df.gameKey.unique()\ncheck_frame = 1\nhomography_df = merged_df[(merged_df.gameKey == unique_gameKeys[0]) & (merged_df.frame == check_frame) & (merged_df.view =='Sideline')].copy()\nhomography_df.head()","2b1d70ac":"trakcing_coordinate = np.float32(list(zip(homography_df['x'],53.33-homography_df['y']))).reshape(-1,1,2)\nlabel_coordinate =  np.float32(list(zip(homography_df['left']+homography_df['width']\/2,homography_df['top']-homography_df['height']\/2))).reshape(-1,1,2)","04d64e87":"H, mask = cv2.findHomography(label_coordinate, trakcing_coordinate)\ntransformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","da449b9d":"print(H)","385d8b01":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","d7ea7d4d":"video_name = homography_df.video.unique()\nvideo_path = f\"{BASE_DIR}\/train\/{video_name[0]}\"\n\nvid = imageio.get_reader(video_path, 'ffmpeg')\nimg = vid.get_data(check_frame - 1)\nplt.figure(figsize=(12, 10))\nplt.imshow(img)","aa40c5f3":"line_numbers = [[110, 600],  ## Home Sideline 20\n                [550, 630],  ## Home Sideline 30\n                [990, 680],  ## Home Sideline 40\n                [1150, 200], ## Victory Sideline 40\n                [770, 200]]  ## Victory Sideline 30\nfor line_number in line_numbers:\n    img = cv2.circle(img, (line_number[0],line_number[1]), radius=2, color=(0, 255, 255), thickness=10)\n\nplt.figure(figsize=(12, 10))\nplt.imshow(img)    ","4c17648b":"projection_numbers = [[30, 53.3-10], ## Home Sideline 20\n                      [40, 53.3-10], ## Home Sideline 30\n                      [50, 53.3-10], ## Home Sideline 40\n                      [50, 10],      ## Victory Sideline 40\n                      [40, 10]]      ## Victory Sideline 30","3af840b3":"H, mask = cv2.findHomography(np.float32(line_numbers).reshape(5, 2), np.float32(projection_numbers).reshape(5, 2))","e3b0346d":"print(H)","f5058e3b":"transformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","4486a75f":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","ea002887":"class SiftKpDesc():\n    def __init__(self, kp, desc):\n        # List of keypoints in (x,y) crd -> N x 2\n        self.kp = kp\n\n        # List of Descriptors at keypoints : N x 128\n        self.desc = desc\n\n\nclass SiftMatching:\n\n    _BLUE = [255, 0, 0]\n    _GREEN = [0, 255, 0]\n    _RED = [0, 0, 255]\n    _CYAN = [255, 255, 0]\n\n    _line_thickness = 2\n    _radius = 5\n    _circ_thickness = 2\n\n\n    def __init__(self, img_1, img_2, results_fldr='', nfeatures=2000, gamma=0.8):\n\n#         fname_1 = os.path.basename(img_1_path)\n#         fname_2 = os.path.basename(img_2_path)\n\n#         if not results_fldr:\n#             results_fldr = os.path.split(img_1_path)[0]\n\n#         self.result_fldr = os.path.join(results_fldr, 'results')\n\n#         self.prefix = fname_1.split('.')[0] + '_' + fname_2.split('.')[0]\n\n#         if not os.path.exists(self.result_fldr):\n#             os.makedirs(self.result_fldr)\n\n        self.img_1_bgr = img_1\n        self.img_2_bgr = img_2\n\n        self.nfeatures = nfeatures\n        self.gamma = gamma\n\n\n    def read_image(self, img_path):\n\n        img_bgr = cv2.imread(img_path, cv2.IMREAD_COLOR)\n\n        return img_bgr\n\n\n\n    def get_sift_features(self, img_bgr, nfeatures=2000):\n\n        img_gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n\n        sift_obj = cv2.xfeatures2d.SIFT_create(nfeatures)\n\n        # kp_list_obj is a list of \"KeyPoint\" objects with location stored as tuple in \"pt\" attribute\n        kp_list_obj, desc = sift_obj.detectAndCompute(image=img_gray, mask=None)\n\n        kp = [x.pt for x in kp_list_obj]\n\n        return SiftKpDesc(kp, desc)\n\n\n    def match_features(self, sift_kp_desc_obj1, sift_kp_desc_obj2, gamma=0.8):\n        correspondence = []  # list of lists of [x1, y1, x2, y2]\n\n        for i in range(len(sift_kp_desc_obj1.kp)):\n            sc = np.linalg.norm(sift_kp_desc_obj1.desc[i] - sift_kp_desc_obj2.desc, axis=1)\n            idx = np.argsort(sc)\n\n            val = sc[idx[0]] \/ sc[idx[1]]\n\n            if val <= gamma:\n                correspondence.append([*sift_kp_desc_obj1.kp[i], *sift_kp_desc_obj2.kp[idx[0]]])\n\n        return correspondence\n\n\n    def draw_correspondence(self, correspondence, img_1, img_2):\n\n        if len(img_1.shape) == 2:\n            img_1 = np.repeat(img_1[:, :, np.newaxis], 3, axis=2)\n\n        if len(img_2.shape) == 2:\n            img_2 = np.repeat(img_2[:, :, np.newaxis], 3, axis=2)\n\n        h, w, _ = img_1.shape\n\n        img_stack = np.hstack((img_1, img_2))\n\n        for x1, y1, x2, y2 in correspondence:\n            x1_d = int(round(x1))\n            y1_d = int(round(y1))\n\n            x2_d = int(round(x2) + w)\n            y2_d = int(round(y2))\n\n            cv2.circle(img_stack, (x1_d, y1_d), radius=self._radius, color=self._BLUE,\n                       thickness=self._circ_thickness, lineType=cv2.LINE_AA)\n\n            cv2.circle(img_stack, (x2_d, y2_d), radius=self._radius, color=self._BLUE,\n                       thickness=self._circ_thickness, lineType=cv2.LINE_AA)\n\n            cv2.line(img_stack, (x1_d, y1_d), (x2_d, y2_d), color=self._CYAN,\n                     thickness=self._line_thickness)\n\n        fname = os.path.join(self.result_fldr, self.prefix + '_sift_corr.jpg')\n        cv2.imwrite(fname, img_stack)\n\n\n    def run(self):\n\n        sift_kp_desc_obj1 = self.get_sift_features(self.img_1_bgr, nfeatures=self.nfeatures)\n        sift_kp_desc_obj2 = self.get_sift_features(self.img_2_bgr, nfeatures=self.nfeatures)\n\n        correspondence = self.match_features(sift_kp_desc_obj1, sift_kp_desc_obj2, gamma=self.gamma)\n\n        self.draw_correspondence(correspondence, self.img_1_bgr, self.img_2_bgr)\n\n        return correspondence","17e53860":"import numpy as np","46969fca":"import cv2\nimg = vid.get_data(check_frame - 1)\nimg2 = vid.get_data(check_frame)\n","47a7df58":"import cv2\n   \nvideo_name = homography_df.video.unique()\n# video_path = f\"{BASE_DIR}\/train\/57905_002404_Sideline.mp4\"\nvideo_path = f\"{BASE_DIR}\/train\/57584_000336_Sideline.mp4\"\n\nvid = imageio.get_reader(video_path, 'ffmpeg')\n    ","343f91cc":"# Read the query image as query_img\n# and traing image This query image\n# is what you need to find in train image\n# Save it in the same directory\n# with the name image.jpg  \nquery_img = np.array(vid.get_data(0))\ntrain_img = np.array(vid.get_data(100))\n\n# query_img = cv2.resize(query_img,(360,640))\n# train_img = cv2.resize(train_img,(360,640))\n\n# alpha = 1 # Contrast control (1.0-3.0)\n# beta = 64.0 # Brightness control (0-100)\n\n# train_img = cv2.convertScaleAbs(train_img, alpha=alpha, beta=beta)\n# query_img = cv2.convertScaleAbs(query_img, alpha=alpha, beta=beta)\n\n\n\n# Convert it to grayscale\nquery_img_bw = cv2.cvtColor(query_img,cv2.COLOR_BGR2GRAY)\ntrain_img_bw = cv2.cvtColor(train_img, cv2.COLOR_BGR2GRAY)\n\nquery_img_bw = cv2.GaussianBlur(query_img_bw,(35,35),cv2.BORDER_DEFAULT)\ntrain_img_bw = cv2.GaussianBlur(train_img_bw,(35,35),cv2.BORDER_DEFAULT)\n\n# query_img_bw = cv2.adaptiveThreshold(query_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,51,28)\n# train_img_bw = cv2.adaptiveThreshold(train_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,51,28)\n\n# kernel = np.ones((2, 2), 'uint8')\n\n# query_img_bw = cv2.adaptiveThreshold(query_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n# query_img_bw = cv2.dilate(query_img_bw, kernel, iterations=1)\n\n# train_img_bw = cv2.adaptiveThreshold(train_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,11,2)\n# train_img_bw = cv2.dilate(train_img_bw, kernel, iterations=1)\n\n# se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n# se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n# query_img_bw = cv2.morphologyEx(query_img_bw, cv2.MORPH_CLOSE, se1)\n# query_img_bw = cv2.morphologyEx(query_img_bw, cv2.MORPH_OPEN, se2)\n# kernel = np.ones((3, 3), np.uint8)\n# query_img_bw = query_img_bw-cv2.erode(query_img_bw, kernel)\n# query_img_bw[query_img_bw < 0] = 255\n\n# se1 = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5))\n# se2 = cv2.getStructuringElement(cv2.MORPH_RECT, (1,1))\n# train_img_bw = cv2.morphologyEx(train_img_bw, cv2.MORPH_CLOSE, se1)\n# train_img_bw = cv2.morphologyEx(train_img_bw, cv2.MORPH_OPEN, se2)\n# kernel = np.ones((3, 3), np.uint8)\n# train_img_bw = train_img_bw-cv2.erode(train_img_bw, kernel)\n# train_img_bw[train_img_bw < 0] = 255\n\n# Initialize the ORB detector algorithm\norb = cv2.ORB_create(150)\n   \n# Now detect the keypoints and compute\n# the descriptors for the query image\n# and train image\nqueryKeypoints, queryDescriptors = orb.detectAndCompute(query_img_bw,None)\ntrainKeypoints, trainDescriptors = orb.detectAndCompute(train_img_bw,None)\n\n# Initialize the Matcher for matching\n# the keypoints and then match the\n# keypoints\nmatcher = cv2.BFMatcher()\nmatches = matcher.match(queryDescriptors,trainDescriptors)\n   \n# draw the matches to the final image\n# containing both the images the drawMatches()\n# function takes both images and keypoints\n# and outputs the matched query image with\n# its train image\nfinal_img = cv2.drawMatches(query_img, queryKeypoints, \ntrain_img, trainKeypoints, matches[0:3],None)\n   \nfinal_img = cv2.resize(final_img, (1000,650))\n  \n# Show the final image\nplt.figure(figsize=(12,10))\nplt.imshow(final_img)","a795138b":"print(queryKeypoints[0].pt)\nprint(trainKeypoints[0].pt)","3a9f1b06":"train_img.shape\nprint(720\/2,1280\/2)","d0162df1":"# a = query_img_bw.astype(np.float)\n# gray = np.float32(query_img)\n# dst = cv2.cornerHarris(query_img_bw,2,3,0.04)\n# dst = cv2.dilate(dst,None)\n# query_img[dst>0.01*dst.max()]=[0,0,255]\n# plt.figure(figsize=(12,10))\n# plt.imshow(query_img)\n\nkernel = np.ones((2, 2), 'uint8')\nimg = cv2.adaptiveThreshold(query_img_bw,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY,51,28)\n# img = cv2.dilate(img, kernel, iterations=1)\n\nplt.figure(figsize=(12,10))\nplt.imshow(img)","6bac53c7":"gray = query_img_bw.copy()\nimg = query_img.copy()\n\nedges = cv2.Canny( gray,50,150,apertureSize = 3)\n\n# Show result\nplt.figure(figsize=(15,10))\nplt.imshow(edges)","6ce3c31d":"print(queryKeypoints[matches[2].queryIdx].pt)\nprint(trainKeypoints[matches[2].trainIdx].pt)\n# queryDescriptors,\n# trainDescriptors","7dc28381":"arr = []\nfor i in range(len(matches)):\n    if matches[i].trainIdx in arr:\n        print(\"arr!!!!\",matches[i].trainIdx)\n        arr.append(matches[i].trainIdx) \n    else:\n        arr.append(matches[i].trainIdx) ","f8f0f9e3":"arr[92]","8ab5c14a":"plt.figure(figsize=(15,10))\nplt.imshow(query_img_bw)","90499859":"os.listdir(f\"{BASE_DIR}\/train\/\")","82547de2":"## ","c3aa2679":"## Video to Frame","01cc1830":"## Finding filed line number points in tracking data\n![images](https:\/\/drive.google.com\/uc?export=view&id=1IdUQHo9G673ifp-mIrwiG_ep0q88H13N)\n\nIf we treat tracking `x`, `y`  as pair points in this images, we can guess that Finding filed line number points.","254849a3":"But important thing is that we can't match each keypoints exactly becaues we don't have a enough information to find homography.\n\nIf we can match specific pair points with tracking images, side & endzone images, we might find good homography.\n\nI'll use filed line numbers to match both images","910c8b6d":"## Finding filed line number points in sideline","b1e4a80c":"## PerspectiveTransform\n\nIf we know matched Keypoints in the images, we can find homography `H` using `cv2.findHomography`. \n\nbelow code show how we can transform sideline helmet boxes to tracking data scale.","e234f344":"## Next to \n- Matching label using homography information\n- Build filed number detection model ? \n- Merging with MOT models like deepsort, FairMOT","819dd30c":"# Prepare","009f72fa":"In this notebook I will share one idea to merging traking data with sideline helmet label information.\n\nMain approch is that if we can find specific 4 pair points(cx, cy) which is matching with `tracking images` & `side or endline images`, we can find homography `H` for Perspective Transformation.\n\nin this notebook I'll use field line numbers to find homography `H` between `tracking images` and `sideline images`\n\nReference: \n- https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n- https:\/\/www.kaggle.com\/c\/nfl-health-and-safety-helmet-assignment\/discussion\/264361#1467283\n- https:\/\/www.kaggle.com\/coldfir3\/camera-tracking-matching-with-gradient-descent\n- https:\/\/www.kaggle.com\/go5kuramubon\/merge-label-and-tracking-data","4c0f5aaf":"matches**Feature Matching**"}}