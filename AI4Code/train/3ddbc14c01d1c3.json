{"cell_type":{"d6eb9b6d":"code","e66e98a4":"code","fdb27380":"code","7e448926":"code","158431f0":"code","ad06899e":"code","ca5ea541":"code","46a03e57":"code","4c31e2d9":"code","75b8a78d":"code","e27c86d3":"code","04fe839b":"code","ce58c250":"code","d9faa6fe":"code","f9f738a3":"code","e96785e0":"code","f8f71fd8":"code","f6a1af75":"code","98696987":"code","b80a854b":"code","a5460ec9":"code","f3337a4b":"code","6d12a4b8":"markdown","0ba8dac1":"markdown","310c9335":"markdown","30f62a9c":"markdown","22793be5":"markdown","9f4e3018":"markdown","526e17b7":"markdown","413bc121":"markdown","2cd50958":"markdown","09a7f7e8":"markdown","e8cdf0fa":"markdown","301dc84d":"markdown","a77057cd":"markdown"},"source":{"d6eb9b6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e66e98a4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler","fdb27380":"df=pd.read_csv('..\/input\/the-boston-houseprice-data\/boston.csv')","7e448926":"df.describe().T","158431f0":"sns.set(rc={'figure.figsize':(14,8)})","ad06899e":"sns.heatmap(df.corr(),annot=True, cmap=\"YlGnBu\")","ca5ea541":"df.columns","46a03e57":"x=df.loc[:,['LSTAT','PTRATIO','RM']]","4c31e2d9":"# Initialise the Scaler\nscaler = StandardScaler()\n \n# To scale data\nscaler.fit(x)","75b8a78d":"y=df['MEDV']","e27c86d3":"def cost_fun(theta,X,Y):\n    m=len(X)\n    J=(1\/(2*m))*(np.sum(np.square(np.dot(X,theta)-Y)))\n    return J","04fe839b":"def gradient_descent(X,Y,alpha,iteration,lamb):\n    cl=[]\n    m=len(X)\n    n=X.shape[1]\n    theta=np.random.randn(n)\n    tl=[theta]\n    for i in range(iteration):\n        theta=(1-(alpha\/lamb))*theta-alpha*(1\/m)*np.sum(np.dot(X.T,((np.dot(X,theta)-Y))))\n        j=cost_fun(theta,X,Y)\n        cl.append(j)\n        tl.append(theta)\n    return cl,tl    ","ce58c250":"cost_list,theta_list=gradient_descent(x,y,0.001,1000,100)","d9faa6fe":"a=theta_list[-1]","f9f738a3":"plt.plot(cost_list)\nplt.xlim([0,20])","e96785e0":"cost_list[999]","f8f71fd8":"y_pred=np.dot(x,a)","f6a1af75":"y_pred=pd.Series(y_pred,name=\"pred\")","98696987":"ndf = pd.merge(y, y_pred, right_index = True,left_index = True)","b80a854b":"ndf.head()","a5460ec9":"def r_squared(Y, YP):\n    ssr, sse, r_sqr = [0]*3\n    YM = np.mean(Y)\n#     ssr = sum([(y_hat - y_)**2 for y_hat in Y_HAT])\n#     sse = sum([(y - y_hat)**2 for y,y_hat in zip(Y, Y_HAT)])\n    sse=np.sum(np.square(YP-YM))\n#     sst = sum([(y - y_)**2 for y in Y])\n    sst=np.sum(np.square(Y-YM))\n    \n    r_sqr = 1 - (sse \/ sst)\n    \n    return r_sqr","f3337a4b":"r_squared(y,y_pred)","6d12a4b8":"MEDV is response variable","0ba8dac1":"# Feature Selection","310c9335":"Feature selection using the filter method i am selecting the 3 highly correlated features as my predictor variable.","30f62a9c":"Thank you!","22793be5":"# Gradient Descent Function with Regularization Parameter","9f4e3018":"Loading Required Library","526e17b7":"Analyzing the correlation in features","413bc121":"Fixing the plot size of figure","2cd50958":"Basic statistics of features in a dataset","09a7f7e8":"# Developing a Cost Function","e8cdf0fa":"Since i will be using linear regression and optimizing parameters with gradient descent which is distance based algorithm thus scaling for reducing complication and enhancing efficiency.","301dc84d":"# The accuracy of the training set is 92.32%","a77057cd":"Loading dataset"}}