{"cell_type":{"f3353ad7":"code","8d63bdea":"code","425add60":"code","726e83f6":"code","67b0c7cf":"code","9b1399ff":"code","ca1bbd7e":"code","c99fc8e7":"code","4c29401a":"code","f53ebda6":"code","f45d76cd":"code","53cc141a":"code","3d53452e":"code","18da2f5b":"code","eb708444":"code","af9a7566":"code","5679335e":"code","818d111e":"code","d6b3282f":"code","0f9be40e":"code","954228fa":"code","0d856b68":"code","b9c51fc2":"code","5661ab00":"code","12c08780":"code","1b0bfc70":"code","8286361d":"code","8f5cb350":"code","e1bf34ea":"code","121adcb5":"code","d208782f":"code","03107a59":"code","50ad3b75":"code","7a43b4f9":"code","94db9b8f":"code","69e771f1":"code","55903bba":"code","f18fef02":"code","4c7806f9":"code","dfc5da03":"code","db637cb3":"code","3fa1beda":"code","b0aed77e":"code","42b28256":"code","27eca058":"code","41539d74":"code","bcf93fed":"code","62e3582e":"code","7679c0c0":"code","44e18a73":"code","4b0c38df":"code","5964a96f":"code","c5389bc4":"code","60057657":"code","01713bac":"code","572bb5e7":"code","a4360b7d":"code","f1980058":"code","4b6832df":"code","32e37670":"code","5fb030d2":"code","61869a4a":"code","76956152":"code","b8214acd":"code","a5f9c762":"code","2b39f24f":"code","4d4fbe17":"code","237bfcc3":"code","aa345dd7":"code","3f4ee325":"code","ee1c345f":"code","d19ba633":"code","fd126dce":"code","5537a151":"code","75612620":"code","2fa91f97":"code","1d5595d1":"code","b5e11971":"code","7cf8be88":"code","53c88f13":"code","ed1c6870":"code","32aa2a05":"code","75873578":"code","f51fa687":"code","eceb247c":"markdown","ae1a5ef9":"markdown","82841431":"markdown","c53dcafc":"markdown","d78d1630":"markdown","9ca0986c":"markdown","905e757f":"markdown","b685e50f":"markdown","e3a0ecc2":"markdown","942d89f3":"markdown","579158a2":"markdown","57e691a0":"markdown","072bf6f8":"markdown","8d004453":"markdown","4e0cc1fa":"markdown","8ecf120b":"markdown","b8416e2f":"markdown","1197f028":"markdown","15a409a9":"markdown","c4cd84a8":"markdown","46020607":"markdown","a9bfb1fb":"markdown","9d7e7567":"markdown","69bce7c4":"markdown","172eae61":"markdown","b9940431":"markdown","f8972dd7":"markdown","5432c411":"markdown","fb101538":"markdown","774d846d":"markdown","721a4489":"markdown","eda3a000":"markdown","25da7a07":"markdown","729f7245":"markdown","ac91c376":"markdown","2df683ed":"markdown","4f6e6ec3":"markdown","09d78f2a":"markdown","a1517556":"markdown"},"source":{"f3353ad7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# Cleaning Data Tools\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer \nfrom nltk.tokenize import RegexpTokenizer\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nstopwords = stopwords.words('english')\n\n# Sentiment Analysis \n!pip install neattext\n!pip install vaderSentiment\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nimport neattext.functions as nfx\nfrom textblob import TextBlob\nimport emoji\n\nfrom gensim.models.phrases import Phrases, Phraser\n\n# Word Embedding\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer #for TF-IDF\nfrom sklearn.feature_extraction.text import CountVectorizer  #For Bag of words\nfrom gensim.models import Word2Vec  #For Word2Vec\nfrom gensim.models import FastText  #For Fast Text\n\n# Scaling and Evaluation Methods\nfrom sklearn import preprocessing\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score,confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n# ML Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline","8d63bdea":"tweet_data = pd.read_csv(\"\/kaggle\/input\/covid19-tweets\/covid_tweets_anglais.csv\") ","425add60":"tweet_data.head()","726e83f6":"tweet_data.info()","67b0c7cf":"tweet_data.columns","9b1399ff":"# First of all let's drop the columns which we don't required\n\nwaste_col =[\n           'url', 'date', 'renderedContent', 'id', 'user', 'replyCount',\n           'retweetCount', 'likeCount', 'quoteCount', 'conversationId', 'lang',\n           'source', 'sourceUrl', 'sourceLabel', 'outlinks', 'tcooutlinks',\n           'media', 'retweetedTweet', 'quotedTweet', 'inReplyToTweetId',\n           'inReplyToUser', 'mentionedUsers', 'coordinates', 'place', 'hashtags',\n           'cashtags'\n           ]\n","ca1bbd7e":"data = tweet_data.drop(waste_col,axis=1)","c99fc8e7":"len(data.loc[data.duplicated()])","4c29401a":"data.drop_duplicates(keep=\"first\",inplace=True)\ndata.reset_index(drop=True , inplace = True)","f53ebda6":"#input\n# 1  aaaa\n# 2  wwwwwwww\n# 3  helloooooooo\n\n# output\n# 1       \n# 2   \n# 3  hello\n\ndef clean(string):\n    if (len(string)==0):\n        return ''\n    if (set(string) == set(string[0])):\n        return ''    \n    prev = None\n    letters = [l for l in string]\n    counter = 1\n    new = []\n    for l in letters:\n        if l==prev:\n            counter+=1\n        else:\n            if (counter==2):\n                new.append(prev)\n            counter=1\n            new.append(l)\n            prev = l\n    return ''.join(new)","f45d76cd":"# Cleaning Text: Multiple hashtags\ndata['clean_tweet'] = data['content'].apply(nfx.remove_hashtags)\n\n# Cleaning Text: userhandles\ndata['clean_tweet'] = data['clean_tweet'].apply(lambda x: nfx.remove_userhandles(x))\n\n# Cleaning Text : Remove urls\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_urls)\n\n# Cleaning Text : custom remove special characters (':', ',', ';', '.', '|','-','_','^', [&amp, &yen, ....])\ndata['clean_tweet'] = data['clean_tweet'].apply(lambda x: nfx.remove_custom_pattern(x,':+|\\,+|\\;+|\\.+|\\\"+|\\|+|\\-+|\\_+|\\%+|\\^|\\*|\\&[a-zA-Z]*'))\ndata['clean_tweet'] = data['clean_tweet'].apply(lambda x: nfx.remove_custom_words(x,'\\n'))\n\n# Cleaning Text: Punctuations\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_puncts)\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_punctuations)\n\n# Cleaning Text: dates\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_dates)\n\n# Cleaning Text: Emails\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_emails)\n\n# Cleaning Text: Numbers\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_numbers)\n\n# data['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_special_characters)\n\n#Remove words made up of repetitive letters\ndata['clean_tweet'] = data['clean_tweet'].fillna('').map(clean)\n\n# Cleaning Text: Multiple WhiteSpaces\ndata['clean_tweet'] = data['clean_tweet'].apply(nfx.remove_multiple_spaces)\n","53cc141a":"print(data.content[58])\nprint(\"=====\")\nprint(data.clean_tweet[58])","3d53452e":"vader_obj = SentimentIntensityAnalyzer()","18da2f5b":"def get_sentiment(tweet):\n    \n    text = emoji.demojize(tweet, delimiters=(\"\", \"\")).replace(\"_\" , \" \")\n    \n    blob = TextBlob(text)\n    sentiment_dict = vader_obj.polarity_scores(text)\n    \n    Compound = sentiment_dict['compound']\n    sentiment_subjectivity = blob.sentiment.subjectivity\n    \n    if sentiment_subjectivity >= 0.25:\n        if Compound >= 0.05:\n            sentiment_label = 'Positive'\n        elif Compound <= - 0.05:\n            sentiment_label = 'Negative'\n        else:\n            sentiment_label = 'Neutral'\n    else: \n        sentiment_label = 'Objective'\n        \n\n    return sentiment_label","eb708444":"# Text\nex1 = data['clean_tweet'][17]\nex1 ","af9a7566":"get_sentiment(ex1)","5679335e":"%%time\ndata['sentiment'] = data['clean_tweet'].apply(get_sentiment)","818d111e":"data.head()","d6b3282f":"# data = data.join(pd.json_normalize(data['sentiment_results']))","0f9be40e":"data = data.loc[data.sentiment != \"Objective\"]","954228fa":"data.head()","0d856b68":"final_data = data.drop([\"content\"],axis=1)","b9c51fc2":"final_data.reset_index(drop=True , inplace = True)\nfinal_data","5661ab00":"lemmatizer = WordNetLemmatizer()\nstemmer = SnowballStemmer('english')\ntags = \"[^A-Za-z]+\"\n\ndays=['monday','tuesday','wednesday','thursday','friday','saturday','sunday']\nmonths=['january','february','march', 'april','may','june','july','august','september','october','november','december']\n\n\ndef preprocess_text(sentence, stem = True):\n    \n    sentence = re.sub(tags,' ', str(sentence).lower()).strip()\n    text = []\n    w=\"\"\n    for word in sentence.split():\n        \n        if word not in stopwords + days + months and len(word) >= 3:\n            \n            if stem:\n                w=lemmatizer.lemmatize(word)\n                text.append(stemmer.stem(w))\n                w=\"\"\n            else:\n                text.append(word)\n                \n    return \" \".join([str(i) for i in text])","12c08780":"print(f\"Orignal Text : {final_data.clean_tweet[7]}\")\nprint(\"\\nAfter Preprocessed : \\n\")\nprint(f\"Preprocessed Text : {preprocess_text(final_data.clean_tweet[7])}\")","1b0bfc70":"%%time\nfinal_data.clean_tweet = final_data.clean_tweet.map(preprocess_text)\nfinal_data.head()","8286361d":"from collections import defaultdict\nsentiment_positive_unigrams = defaultdict(int)\nfor tweet in final_data.loc[final_data.sentiment == 'Positive'].clean_tweet:\n    for word in tweet.split(\" \"):\n        sentiment_positive_unigrams[word] += 1\n        \ndf_sentiment_positive_unigrams = pd.DataFrame(sorted(sentiment_positive_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_positive_100 = df_sentiment_positive_unigrams[:20]\n\n\n\n\nsentiment_negative_unigrams = defaultdict(int)\nfor tweet in final_data.loc[final_data.sentiment == 'Negative'].clean_tweet:\n    for word in tweet.split(\" \"):\n        sentiment_negative_unigrams[word] += 1\n        \ndf_sentiment_negative_unigrams = pd.DataFrame(sorted(sentiment_negative_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_negative_100 = df_sentiment_negative_unigrams[:20]\n","8f5cb350":"fig, axes = plt.subplots(ncols=2, figsize=(18, 20\/\/2), dpi=80)\nplt.tight_layout()\n\nsns.barplot(y=unigrams_positive_100[0], x=unigrams_positive_100[1], ax=axes[0], color='green')\nsns.barplot(y=unigrams_negative_100[0], x=unigrams_negative_100[1], ax=axes[1], color='red')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {20} most common unigrams in positive tweets', fontsize=13)\naxes[1].set_title(f'Top {20} most common unigrams in negative tweets', fontsize=13)\n\nplt.show()","e1bf34ea":"import requests\nfrom io import BytesIO\nfrom wordcloud import WordCloud\nfrom PIL import Image\ntry:\n    url=\"https:\/\/user-images.githubusercontent.com\/74188336\/142692890-641ebc21-2e47-4556-9d37-1c0b9e1a0587.jpeg\"\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n\n    text = ' '.join(final_data.clean_tweet.values)\n    mask = np.array(img)\n    wordcloud = WordCloud(max_font_size=50, max_words=1000, background_color=\"white\",mask=mask,  colormap='BuGn').generate(text.lower())\n    plt.figure(figsize=(15,15))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()\nexcept Exception as e:\n    pass","121adcb5":"final_data.isnull().sum()","d208782f":"le = preprocessing.LabelEncoder()\nfinal_data.sentiment = le.fit_transform(final_data.sentiment)","03107a59":"final_data.head()","50ad3b75":"print(\"Samples per class {}\".format(np.bincount(final_data.sentiment)))","7a43b4f9":"final_data.sentiment.value_counts().plot.pie(autopct='%1.1f%%', labels=None, legend=True)\nplt.tight_layout ()","94db9b8f":"vect = CountVectorizer()","69e771f1":"#we need first to split our data to train and test sets\nX_train, X_test, y_train, y_test = train_test_split(final_data.clean_tweet, final_data.sentiment, random_state=42, test_size=0.2)\n\nprint(X_train.shape, X_test.shape)","55903bba":"print(\"Samples per class in train {}\".format(np.bincount(y_train)))\nprint(\"Samples per class in test {}\".format(np.bincount(y_test)))","f18fef02":"X_train = vect.fit_transform(X_train)\nX_test = vect.transform(X_test)","4c7806f9":"bow_df = pd.DataFrame(X_train.toarray(), columns = vect.get_feature_names())","dfc5da03":"bow_df.head()","db637cb3":"feature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))","3fa1beda":"# vect.vocabulary_","b0aed77e":"# Use multiple classifiers and grid search for prediction\ndef ML_modeling(models, params, X_train, X_test, y_train, y_test):    \n    \n    if not set(models.keys()).issubset(set(params.keys())):\n        raise ValueError('Some estimators are missing parameters')\n\n    for key in models.keys():\n    \n        model = models[key]\n        param = params[key]\n        gs = GridSearchCV(model, param, cv=5, error_score=0, refit=True)\n        gs.fit(X_train, y_train)\n        y_pred = gs.predict(X_test)\n        \n        # Print scores for the classifier\n        print(key, ':', gs.best_params_)\n        print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))\n    \n    return","42b28256":"## Preparing to make a pipeline \nmodels = {\n    'Random Forest Classifier': RandomForestClassifier(),\n    'Naive Bayes': MultinomialNB(),\n    'logistic regression' : LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier(),  \n    'Gradient Boosting': GradientBoostingClassifier()\n}\n\nparams = {\n    'Random Forest Classifier': {'criterion': ['gini', 'entropy']},\n    'Naive Bayes': { 'alpha': [0.5, 1], 'fit_prior': [True, False] }, \n    'logistic regression' : {'max_iter':[2000]},\n    'Decision Tree': { 'min_samples_split': [2, 5, 7] }, \n    'Gradient Boosting': { 'learning_rate': [0.05, 0.1] }\n}","27eca058":"%%time\nprint(\"==============Bag of Words==============\\n\")\nML_modeling(models, params, X_train, X_test, y_train, y_test)","41539d74":"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,2))","bcf93fed":"# #we need first to split our data to train and test sets\nX_train, X_test, y_train, y_test = train_test_split(final_data.clean_tweet, final_data.sentiment, random_state=42, test_size=0.2)\n\nprint(X_train.shape, X_test.shape)","62e3582e":"X_train = tfidf_vec.fit_transform(X_train)\nX_test = tfidf_vec.transform(X_test)","7679c0c0":"tfidf_df = pd.DataFrame(X_train.toarray(), columns = tfidf_vec.get_feature_names())","44e18a73":"len(tfidf_vec.get_feature_names())","4b0c38df":"tfidf_df.head()","5964a96f":"# %%time\n# print(\"==============TF-IDF==============\\n\")\n# ML_modeling(models, params, X_train, X_test, y_train, y_test)","c5389bc4":"sent = [row.split() for row in final_data['clean_tweet']]","60057657":"phrases = Phrases(sent, min_count=30, progress_per=10000)","01713bac":"bigram = Phraser(phrases)","572bb5e7":"sentences = bigram[sent]","a4360b7d":"w2v_model = Word2Vec(sentences=sentences,\n                     min_count=9,\n                     vector_size=300,\n                     window=2,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20\n                    )","f1980058":"w2v_model.build_vocab(sentences, progress_per=20000)","4b6832df":"w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=45, report_delay=1)","32e37670":"# w2v_model.wv.key_to_index","5fb030d2":"w2v_model.wv.most_similar(\"vaccin\")","61869a4a":"w2v_model.wv.similarity(\"covid\", 'patient')","76956152":"w2v_model.wv.doesnt_match(['world', 'vaccin', 'covid'])","b8214acd":"w=w2v_model.wv[\"covid\"]","a5f9c762":"len(w)","2b39f24f":"sns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","4d4fbe17":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 17 dimensions with PCA\n    reduc = PCA(n_components=20).fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(14, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 30,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n            \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    ","237bfcc3":"tsnescatterplot(w2v_model, 'covid', ['die', 'test', 'stop', 'china', 'good', 'year', 'medic', 'lockdown', 'friend', 'peopl'])","aa345dd7":"# tsnescatterplot(w2v_model, 'covid',list(w2v_model.wv.key_to_index.keys())[100:1000:12])","3f4ee325":"tsnescatterplot(w2v_model, 'vaccin', [i[0] for i in w2v_model.wv.most_similar(negative=[\"vaccin\"])])","ee1c345f":"sentences_df = pd.DataFrame({\"sentences\":sentences,\"sentiment\":final_data.sentiment})","d19ba633":"## splitting the dataset into 80:20.  have kept shuffle=True , so that the data is randomly sampled or simply said shuffled , and then split.\nX_train, X_test, y_train, y_test = train_test_split(sentences_df.sentences, sentences_df.sentiment, test_size=0.2, shuffle=True)","fd126dce":"w2v = dict(zip(w2v_model.wv.index_to_key, w2v_model.wv.vectors))","5537a151":"class Vectorizer(object):\n    \n    def __init__(self, vec):\n        self.vec = vec\n        self.dim = 300\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([np.mean([self.vec[w] for w in words if w in self.vec] or [np.zeros(self.dim)], axis=0) for words in X])\n\n\n## for any Classifier , we need intialise the model with the parameters. \n## Further I am applying GridSearchCV for 5 runs (i.e 1\/5th data used each time for testing) \n## So the model gets trained over 5 runs \n## as well we are predicting also over 5 runs\nclass Classifier(object):\n    \n    def __init__(self, model, param):\n        self.model = model\n        self.param = param\n        self.gs = GridSearchCV(self.model, self.param, cv=5, error_score=0, refit=True)        \n\n    def fit(self, X, y):   \n        return self.gs.fit(X, y)\n\n    def predict(self, X):\n        return self.gs.predict(X)\n    \n\n## Preparing to make a pipeline \n## What to know about Pipelining : see this https:\/\/www.youtube.com\/watch?v=Y4iJfKX_QeQ&t=52s\nclf_models = {\n    'SVM': SVC(),\n    'Random Forest Classifier': RandomForestClassifier(),\n    'Naive Bayes': GaussianNB(),  \n    'logistic regression' : LogisticRegression(),\n    'Decision Tree': DecisionTreeClassifier()\n#     'Gradient Boosting': GradientBoostingClassifier()\n}\n\nclf_params = {\n    'SVM': { 'kernel': ['linear', 'rbf'] },\n    'Random Forest Classifier': {'criterion': ['gini', 'entropy'] },\n    'Naive Bayes': { }, \n    'logistic regression' : {'max_iter':[2000]},\n    'Decision Tree': { 'min_samples_split': [2, 5] }, \n    'Gradient Boosting': { 'learning_rate': [0.05, 0.1]}\n}","75612620":"%%time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"==============Word2Vec==============\\n\")\n## for loop traverses , each and every classifier and its corresponding parameters.\nfor key in clf_models.keys():\n    \n    clf = Pipeline([('Word2Vec vectorizer', Vectorizer(w2v)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n    \n    clf.fit(X_train, y_train)  ## Note : we are calling user defined fit method. This fit method uses Cross Validation\n    y_pred = clf.predict(X_test)  ## Note : we are calling user defined predict method. This predict method uses Cross Validation\n    \n    ## printing performance metrics for each classifier \n    print(key, ':')\n    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))","2fa91f97":"fasttext_model = FastText(sentences=sentences,\n                          window=2,\n                          vector_size=300,\n                          min_count=6,\n                          sample=6e-5,\n                          alpha=0.03, \n                          min_alpha=0.0007,\n                          epochs=10\n                         )","1d5595d1":"fasttext_model.build_vocab(sentences, progress_per=20000)","b5e11971":"fasttext_model.train(sentences, total_examples=fasttext_model.corpus_count, epochs=45, report_delay=1)","7cf8be88":"fasttext_model.wv.most_similar(\"corona\")","53c88f13":"tsnescatterplot(fasttext_model, 'covid', ['die', 'test', 'stop', 'china', 'good', 'year', 'medic', 'lockdown', 'friend', 'peopl'])","ed1c6870":"tsnescatterplot(fasttext_model, 'covid', [i[0] for i in w2v_model.wv.most_similar(negative=[\"covid\"])])","32aa2a05":"## splitting the dataset into 80:20.  have kept shuffle=True , so that the data is randomly sampled or simply said shuffled , and then split.\nX_train, X_test, y_train, y_test = train_test_split(sentences_df.sentences, sentences_df.sentiment, test_size=0.2, shuffle=True)","75873578":"ftv = dict(zip(fasttext_model.wv.index_to_key, fasttext_model.wv.vectors))","f51fa687":"%%time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"==============FastText==============\\n\")\n## for loop traverses , each and every classifier and its corresponding parameters.\nfor key in clf_models.keys():\n    \n    clf = Pipeline([('FastText vectorizer', Vectorizer(ftv)), ('Classifier', Classifier(clf_models[key], clf_params[key]))])\n    \n    clf.fit(X_train, y_train)  ## Note : we are calling user defined fit method. This fit method uses Cross Validation\n    y_pred = clf.predict(X_test)  ## Note : we are calling user defined predict method. This predict method uses Cross Validation\n    \n    ## printing performance metrics for each classifier \n    print(key, ':')\n    print(\"Accuracy: %1.3f \\tPrecision: %1.3f \\tRecall: %1.3f \\t\\tF1: %1.3f\\n\" % (accuracy_score(y_test, y_pred), precision_score(y_test, y_pred, average='macro'), recall_score(y_test, y_pred, average='macro'), f1_score(y_test, y_pred, average='macro')))","eceb247c":"As Phrases() takes a list of list of words as input:","ae1a5ef9":"# Gensim FastText Implementation\nA word can be represented by its constituent character ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n-grams together and views a word\u2019s embedding vector as an aggregation of its constituent character n-grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there\u2019s a word, \u201cgregarious,\u201d that\u2019s not found in the embedding\u2019s word vocabulary. We break it into character n-grams\u2014gre, reg, ega, \u2026.ous\u2014and combine these embeddings of the ngrams to arrive at the embedding of \u201cgregarious.\u201d\n\nHow FastText Works?\n\nFastText is a modified version of word2vec (i.e.. Skip-Gram and CBOW). The only difference between fastText vs word2vec is its pooling strategies (what are the input, output, and dictionary of the model). In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram.","82841431":"**Building the Vocabulary Table:**<br>\n\n\nWord2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):","c53dcafc":"**Odd-One-Out:**\n\nHere, we ask our model to give us the word that does not belong to the list!","d78d1630":"the 10 most similar words to Homer ends up around him","9ca0986c":"**1. Sentiment Analysis**","905e757f":"**Use Word2vec to train a machine learning classifier**\n* 1. Naive Bayes\n* 2. Decision Tree\n* 3. Random Forest\n* 4. Gradient Boosting\n* 5. Logistic Regression\n* 6. SVM\n","b685e50f":"Our goal in this section is to plot our 100 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.\nFor that we are going to use t-SNE implementation from scikit-learn.\n\nTo make the visualizations more relevant, we will look at the relationships between a query word (in **red**), its most similar words in the model (in **blue**), and other words from the vocabulary (in **green**).","e3a0ecc2":"**t-SNE visualizations**","942d89f3":"**Stop-Word Removal, Lower Casing, Stemming, Tokenization.**","579158a2":"# Bigrams\n\nWe are using Gensim Phrases package to automatically detect common phrases (bigrams) from a list of sentences.","57e691a0":"# Text Pre-Proces\n\n**1. Sentiment Analysis**\n\n**2. Stop-Word Removal :** In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n\n**3. Lower Casing :** Convert all words into the lower case because the upper or lower case may not make a difference for the problem. And we are reducing vocabulary size by doing so.\n\n**4. Stemming :** Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., \u201cwalk\u201d and \u201cwalking\u201d are both reduced to \u201cwalk\u201d).\n\n**5. Tokenization :** NLP software typically analyzes text by breaking it up into words (tokens) and sentences.\n\n","072bf6f8":"**Exploring the model**","8d004453":"* **word2vec makes each word a vector**","4e0cc1fa":"The goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task:","8ecf120b":"# BAG OF WORDS\n\nIn BoW we construct a dictionary that contains set of all unique words from our text review dataset.The frequency of the word is counted here. if there are d unique words in our dictionary then for every sentence or review the vector will be of length d and count of word from review is stored at its particular location in vector. The vector will be highly sparse in such case.\n","b8416e2f":"**Training of the model:**\n\n\nParameters of the training:\n\n* **total_examples** = int - Count of sentences.\n* **epochs** = int - Number of iterations (epochs).","1197f028":"**Exploring the model**\n\n*Most similar to:*\n\nHere, we will ask our model to find the word most similar to Corona!","15a409a9":"**Building the Vocabulary Table**","c4cd84a8":"**t-SNE visualizations**","46020607":"**Similarities:**\n\nHere, we will see how similar are two words to each other :","a9bfb1fb":"##### ","9d7e7567":"**Ml Models**","69bce7c4":"**Train Models**","172eae61":"Creates the relevant phrases from the list of sentences:","b9940431":"Now we have preprocessed textual data so now we can proceed further in this notebook and discuss various text representation approaches in detail","f8972dd7":"* 10 Most similar words vs. 10 Random words","5432c411":"# Gensim Word2Vec Implementation\n\n**Word Embeddings :** They are a real-valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real-valued vector\n\nWord2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n\nIt is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.","fb101538":"# TF-IDF","774d846d":"![center](https:\/\/bensonruan.com\/wp-content\/uploads\/2019\/10\/Twitter-Sentiment-Analysis.jpg)","721a4489":"* **So, we are using the 100 number vector, which can be seen for the word covid**","eda3a000":"Transform the corpus based on the bigrams detected:","25da7a07":"**10 Most similar words vs. 10 Random words**","729f7245":"**10 Most similar words vs. 10 Most dissimilar**","ac91c376":"**Training ML Models**\n* 1. Naive Bayes\n* 2. Decision Tree\n* 3. Random Forest\n* 4. Gradient Boosting\n* 5. Logistic Regression","2df683ed":"**The parameters:**<br>\n> * ***min_count*** = int - Ignores all words with total absolute frequency lower than this - (2, 100)<br>\n> * ***window*** = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)<br>\n> * ***size*** = int - Dimensionality of the feature vectors. - (50, 300)<br>\n> * ***sample*** = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)<br>\n> * ***alpha*** = float - The initial learning rate - (0.01, 0.05)<br>\n> * ***min_alpha*** = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00<br>\n> * ***negative*** = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)<br>\n> * ***workers*** = int - Use these many worker threads to train the model (=faster training with multicore machines)<br>","4f6e6ec3":"**Business Goal**\n\nCreate classification models that will be used to predict population behavior in relation to new changes in the corona virus \n\n**Below are the steps which we will be basically following:**\n\n> 1. Step 1: Reading and Understanding the Data\n>\n> 2. Step 2: Text Pre-Proces\n>\n>    * Sentiment Analysis\n>    * Stop-Word Removal \n>    * Lower Casing \n>    * Stemming \n>    * Lemmatization \n>    * Tokenization \n>\n>\n> 3. Step 3: BAG OF WORDS\n>\n>     * Split Data\n>     * Training ML Models\n>         1. Naive Bayes\n>         2. Decision Tree\n>         3. Random Forest\n>         4. Gradient Boosting\n>         5. Logistic Regression\n>\n> \n> 4. Step 4: TF-IDF\n>\n>     * Split Data\n>     * Training ML Models\n>         1. Naive Bayes\n>         2. Decision Tree\n>         3. Random Forest\n>         4. Gradient Boosting\n>         5. Logistic Regression\n> \n> \n> 5. Step 5: Bigrams\n> \n> 6. Step 6: Gensim Word2Vec\n> \n>     * Building the Vocabulary Table\n>     * Training of the model:\n>     * Exploring the model\n>     * Similarities\n>     * Odd-One-Out\n>     * t-SNE visualizations\n>     * Train ML models\n>         1. Naive Bayes\n>         2. Decision Tree\n>         3. Random Forest\n>         4. Gradient Boosting\n>         5. Logistic Regression\n>         6. SVM\n> \n>\n> 7. Step 7: Gensim FastText\n> \n>     * Building the Vocabulary Table\n>     * Training of the model:\n>     * Exploring the model\n>     * t-SNE visualizations\n>     * Train ML models\n>         1. Naive Bayes\n>         2. Decision Tree\n>         3. Random Forest\n>         4. Gradient Boosting\n>         5. Logistic Regression\n>         6. SVM\n>\n>\n> 8. Step 8: Evaluation\n\n","09d78f2a":"**10 Most similar words vs. 10 Most dissimilar**<br>\nThis time, let's compare where the vector representation of \"*vaccine*\" and her 10 most similar words from the model lies compare to the vector representation of the 10 most dissimilar words to \"*vaccine*\"","a1517556":"**Training of the model:**"}}