{"cell_type":{"7ed68308":"code","d008a93f":"code","5244fc6d":"code","25366a97":"code","2ed836cb":"code","26f31244":"code","50bcd9c6":"code","bdc89bbb":"code","8a3921ae":"code","0f9d3753":"code","0340276a":"code","fa66e010":"code","50634217":"code","92f5abfe":"code","bd538981":"code","2859d1e4":"code","ec3d22ea":"code","066267e3":"code","02e67b25":"code","77e70476":"code","3a90db4a":"code","386dcd56":"code","2f28b0a6":"code","8c9557af":"code","f9756617":"code","ffb0ff7c":"code","9d57e973":"code","f4543437":"code","efb46042":"code","c02c1e35":"code","f59c4541":"code","098ee003":"code","fb9c2774":"code","3c192401":"code","7d2a4669":"code","c7a70f57":"code","70ba713c":"code","52bcdf42":"code","24c5a35c":"code","380fe2e4":"code","98ea25b4":"code","326c8c87":"code","501d04ad":"code","37ca92e1":"code","78ac3bb6":"code","e1d5ed6b":"code","9dfdb06e":"code","7cfe3ef4":"code","13479294":"code","4ad6ac50":"code","b8f78c52":"code","366f0e84":"code","f16cfe53":"code","2e27949c":"code","f07b22e5":"code","709b0a06":"code","71c2b921":"markdown","0267142b":"markdown","c935698f":"markdown","2ba6dde0":"markdown","72aa87a3":"markdown","3c62f020":"markdown","5fd05768":"markdown","653bef05":"markdown","cafa19c1":"markdown","16ec507a":"markdown","52ff1724":"markdown","0132a5bd":"markdown","f0a73de0":"markdown","7705f541":"markdown","9942bcc3":"markdown","545951ef":"markdown","e3b34e53":"markdown","0dff1b11":"markdown","4c33f6b3":"markdown","5c25d6c2":"markdown","0d538ba0":"markdown","345182ad":"markdown","fde765c0":"markdown","d7aa9910":"markdown","dc639a0f":"markdown","618bd39c":"markdown","10e401d9":"markdown","e6930e94":"markdown","a9520a52":"markdown","efd3a667":"markdown","621f216d":"markdown"},"source":{"7ed68308":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as pe\nfrom wordcloud import WordCloud, STOPWORDS \n\nfrom sklearn import model_selection\n\nfrom transformers import (BertTokenizer,BertModel,AdamW,get_linear_schedule_with_warmup)\n\nimport torch\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader,Dataset\nfrom pytorch_lightning.metrics.functional.classification import auroc\nimport warnings\nfrom pylab import rcParams","d008a93f":"warnings.filterwarnings('ignore')\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nRANDOM_SEED =42\nsns.set(style='whitegrid',palette='muted',font_scale=1.2)\nHAPPY_COLORS_PALETTE = ['#f0d407','#fbec7e','#04345b','#596e3e','#948304','#2f524f']\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize']=12,8\n\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\nBERT_MODEL_NAME = 'bert-base-cased'\nBATCH_SIZE = 32\nN_EPOCHS = 5","5244fc6d":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25366a97":"PARENT_DIR = '..\/input\/jigsaw-toxic-comment-classification-challenge'","2ed836cb":"train_df = pd.read_csv(os.path.join(PARENT_DIR,'train.csv.zip'))\ntest_df = pd.read_csv(os.path.join(PARENT_DIR,'test.csv.zip'))\ntest_lab_df = pd.read_csv(os.path.join(PARENT_DIR,'test_labels.csv.zip'))\nsample_df = pd.read_csv(os.path.join(PARENT_DIR,'sample_submission.csv.zip'))","26f31244":"train_df.head(7)","50bcd9c6":"test_df.head(2)","bdc89bbb":"test_lab_df.head()","8a3921ae":"sample_df.head()","0f9d3753":"sample_toxic_words = train_df.head(100)[train_df.head(5000).sum(axis=1)>=3]\nsample_toxic_words","0340276a":"def wordcloud(df):\n    comment_words = ''   \n    stopwords = set(STOPWORDS) \n    # iterate through the csv file \n    for val in df.comment_text: \n        # typecaste each val to string \n        val = str(val) \n\n        # split the value \n        tokens = val.split() \n\n        # Converts each token into lowercase \n        for i in range(len(tokens)): \n            tokens[i] = tokens[i].lower() \n\n        comment_words += \" \".join(tokens)+\" \"\n\n    wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white',  \n                stopwords=stopwords,\n                min_font_size = 10).generate(comment_words) \n\n\n    # plot the WordCloud image                        \n    plt.figure(figsize = (8, 8), facecolor = None) \n    plt.imshow(wordcloud) \n    plt.axis(\"off\") \n    plt.tight_layout(pad = 0) \n\n    plt.show() ","fa66e010":"wordcloud(sample_toxic_words)","50634217":"# Also there is exactly one comment from each ID,s\ntrain_df.id.value_counts()[train_df.id.value_counts(ascending=False)>1]","92f5abfe":"train_df,val_df = model_selection.train_test_split(train_df,test_size=0.05)","bd538981":"train_df.shape,val_df.shape","2859d1e4":"LABEL_COLUMNS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']","ec3d22ea":"lab_c = pd.DataFrame(train_df[LABEL_COLUMNS].sum()).reset_index()\nlab_c.columns = ['Type','Count']\nfig = pe.bar(lab_c, x='Type', y='Count')\nfig.show()","066267e3":"lab_toxic_clean_mix = pd.DataFrame(train_df[LABEL_COLUMNS].sum()).reset_index()\nlab_toxic_clean_mix.columns = ['Type','Count'] \nlab_toxic_clean_mix.loc[len(lab_toxic_clean_mix.index)] = ['Clean',len(train_df) -lab_toxic_clean_mix.Count.sum()]","02e67b25":"fig = pe.bar(lab_toxic_clean_mix, x='Type', y='Count')\nfig.show()","77e70476":"toxic_df = train_df[train_df[LABEL_COLUMNS].sum(axis=1)>0]\nclean_df = train_df[train_df[LABEL_COLUMNS].sum(axis=1)==0]\n\ntrain_df = pd.concat([\n    toxic_df,\n    clean_df.sample(15_000)\n])","3a90db4a":"train_df.shape","386dcd56":"sample_row = train_df[train_df.id=='325cd3656d865766']\nsample_row = sample_row.iloc[0]\nsample_comment = sample_row.comment_text\nsample_labels = sample_row[LABEL_COLUMNS]\n\nprint(sample_comment)\nprint()\nprint(sample_labels.to_dict())","2f28b0a6":"tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)","8c9557af":"encoding  = tokenizer.encode_plus(sample_comment,\n                     add_special_tokens=True,\n                     max_length=512,\n                     return_token_type_ids=False,\n                     padding = 'max_length',\n                     return_attention_mask=True,\n                     return_tensors='pt'\n                     )","f9756617":"# Encoding consist of input_ids and attenssion mask\nencoding.keys()","ffb0ff7c":"encoding['input_ids'].shape,encoding['attention_mask'].shape","9d57e973":"print(encoding['input_ids'].squeeze()[:50])\nprint(encoding['attention_mask'].squeeze()[:50])","f4543437":"print(tokenizer.convert_ids_to_tokens(encoding['input_ids'].squeeze()[:50]))","efb46042":"from nltk.tokenize import word_tokenize\nlength = []\nfor sent in train_df.comment_text:\n    lent = len(word_tokenize(sent))\n    length.append(lent)","c02c1e35":"sns.distplot(length)","f59c4541":"np.mean(length)","098ee003":"class ToxicCommentsDataset(Dataset):\n    def __init__(self,data:pd.DataFrame,tokenizer:BertTokenizer,max_token_len:int=128):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_token_len = max_token_len\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self,idx:int):\n        data_row = self.data.iloc[idx]\n        comment_text = data_row.comment_text\n        labels = data_row[LABEL_COLUMNS]\n\n        encoding = self.tokenizer.encode_plus(\n                 comment_text,\n                 add_special_tokens=True,\n                 max_length=self.max_token_len,\n                 return_token_type_ids=False,\n                 padding = 'max_length',\n                 truncation = True,\n                 return_attention_mask=True,\n                 return_tensors='pt'\n                 )\n\n        return dict(\n            comment_text = comment_text,\n            input_ids = encoding['input_ids'].flatten(),\n            attention_mask = encoding['attention_mask'].flatten(),\n            labels = torch.FloatTensor(labels)\n\n            )","fb9c2774":"train_dataset = ToxicCommentsDataset(train_df,tokenizer)\nsample_row = train_dataset[0]","3c192401":"sample_row['input_ids'].shape,sample_row['attention_mask'].shape,sample_row['labels'].shape","7d2a4669":"comment_text = sample_row['comment_text']\ninput_id = sample_row['input_ids']\nattention_m = sample_row['attention_mask']\ntext_lab = sample_row['labels']\n\nprint(comment_text)\nprint()\nprint(input_id)\nprint()\nprint(attention_m)\nprint()\nprint(text_lab)","c7a70f57":"bert_model = BertModel.from_pretrained(BERT_MODEL_NAME,return_dict = True)","70ba713c":"bert_model","52bcdf42":"sample_prediction = bert_model(sample_row['input_ids'].unsqueeze(dim=0),sample_row['attention_mask'].unsqueeze(dim=0))","24c5a35c":"sample_prediction.last_hidden_state.shape,sample_prediction.pooler_output.shape","380fe2e4":"class ToxicCommentDataModule(pl.LightningDataModule):\n    def __init__(self,train_df,test_df,tokenizer,batch_size=8,max_token_len=128):\n        super().__init__()\n        \n        self.train_df = train_df\n        self.test_df = test_df\n        self.tokenizer = tokenizer\n        self.batch_size = batch_size\n        self.max_token_len=max_token_len\n        \n    def setup(self):\n        self.train_dataset = ToxicCommentsDataset(\n            self.train_df,\n            self.tokenizer,\n            self.max_token_len\n        )\n        self.test_dataset = ToxicCommentsDataset(\n            self.test_df,\n            self.tokenizer,\n            self.max_token_len\n        )\n        \n    def train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            batch_size = self.batch_size,\n            shuffle=True,\n            num_workers = 4\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(self.test_dataset,batch_size = 1,num_workers = 4)\n    \n    \n    def test_dataloader(self):\n        return DataLoader(self.test_dataset,batch_size = 1,num_workers = 4)","98ea25b4":"data_module = ToxicCommentDataModule(train_df,val_df,tokenizer,batch_size=BATCH_SIZE)\ndata_module.setup()","326c8c87":"class ToxicCommentClassifier(pl.LightningModule):\n    def __init__(self,n_classes:int,steps_per_epoch:None,n_epochs=None):\n        super().__init__()\n        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME,return_dict = True)\n        self.classifier = torch.nn.Linear(self.bert.config.hidden_size,n_classes)\n        \n        self.steps_per_epoch = steps_per_epoch\n        self.n_epochs = n_epochs\n        \n        self.criterion = torch.nn.BCELoss()\n        \n        \n    def forward(self,input_ids,attention_mask,labels=None):\n        output = self.bert(input_ids,attention_mask=attention_mask)\n        output = self.classifier(output.pooler_output)\n        output = torch.sigmoid(output)\n        \n        loss = 0\n        if labels is not None:\n            loss = self.criterion(output,labels)\n        return loss ,output\n    \n    def training_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        loss,outputs = self(input_ids,attention_mask,labels)\n        self.log('train_loss',loss,prog_bar=True,logger=True)\n        return {\"loss\":loss,\"predictions\":outputs,\"labels\":labels}\n    \n    def validation_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        loss,outputs = self(input_ids,attention_mask,labels)\n        self.log('val_loss',loss,prog_bar=True,logger=True)\n        return loss\n    \n    \n    def test_step(self,batch,batch_idx):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        loss,outputs = self(input_ids,attention_mask,labels)\n        self.log('test_loss',loss,prog_bar=True,logger=True)\n        return loss\n    \n    \n    def training_epoch_end(self,outputs):\n        labels = []\n        predictions=[]\n        \n        for output in outputs:\n            for out_labels in output['labels'].detach().cpu():\n                labels.append(out_labels)\n            for out_predictions in output['predictions'].detach().cpu():\n                predictions.append(out_predictions)\n                \n        labels = torch.stack(labels)\n        predictions = torch.stack(predictions)\n        \n        for i,name in enumerate(LABEL_COLUMNS):\n            roc_score = auroc(predictions[:,i],labels[:,i])\n            self.logger.experiment.add_scalar(f\"{name}_roc_auc\/Train\",roc_score,self.current_epoch)\n            \n        \n    def configure_optimizers(self):\n        optimizer =  AdamW(self.parameters(), lr=2e-5)\n        \n        \n        warmup_steps = self.steps_per_epoch \/\/ 3\n        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n        \n        \n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            warmup_steps,\n            total_steps\n        )\n        \n        return [optimizer],[scheduler]","501d04ad":"model = ToxicCommentClassifier(n_classes=6,\n                               steps_per_epoch=len(train_df)\/\/BATCH_SIZE,\n                              n_epochs=N_EPOCHS\n                              )","37ca92e1":"trainer = pl.Trainer(max_epochs=N_EPOCHS,\n                     gpus=1,\n                     progress_bar_refresh_rate=30\n#                      fast_dev_run=True\n                    )","78ac3bb6":"trainer.fit(model,data_module)","e1d5ed6b":"# Load the extension and start TensorBoard\n# %load_ext tensorboard\n# %tensorboard --logdir .\/lightning_logs\n# %reload_ext tensorboard\n","9dfdb06e":"trainer.test()","7cfe3ef4":"trainer.save_checkpoint(\"final_checkpoint.ckpt\")","13479294":"trained_model = ToxicCommentClassifier.load_from_checkpoint('.\/final_checkpoint.ckpt',\n                                                            n_classes=6,\n                                                            steps_per_epoch=len(train_df)\/\/BATCH_SIZE)","4ad6ac50":"test_comment = \"Nonsense cocksucker Fuck? Kiss off,geek.What i said is true.I will have you account terminated.\"","b8f78c52":"encoding  = tokenizer.encode_plus(test_comment,\n                     add_special_tokens=True,\n                     max_length=128,\n                     return_token_type_ids=False,\n                     padding = 'max_length',\n                     return_attention_mask=True,\n                     return_tensors='pt'\n                     )","366f0e84":"encoding.keys()","f16cfe53":"_,prediction = trained_model(encoding['input_ids'],encoding['attention_mask'])\nprediction","2e27949c":"test_prediction = prediction.detach().numpy()\ntest_prediction","f07b22e5":"prediction_labels = []\n\nfor i,label in enumerate(LABEL_COLUMNS):\n    label_prob = test_prediction[:,i]\n    \n    if label_prob>0.5:\n        prediction_labels.append(label)","709b0a06":"prediction_labels","71c2b921":"*Lets see the size of our return values*","0267142b":"*Lets take a look into the data*","c935698f":"*Load the model*","2ba6dde0":"*Lets create class for actually model configuration , loss and metrics*","72aa87a3":"*Here we can notoce that the mean length is very less so i am going to use my max token length as 128*","3c62f020":"*Lets Handle the imbalence in the dataset*","5fd05768":"*This is my first kernel on Pytorch-Lightning*","653bef05":"*Save the best model*","cafa19c1":"*Lets check the files present for this competetion*","16ec507a":"*Lets build a wordcloud for a visual representation of toxic words*","52ff1724":"Please UpVote if you like this kernel...","0132a5bd":"*General Configuration parameters*","f0a73de0":"*Below some sample of toxic comment can be seen*","7705f541":"*Below is the code to create dataloader which will take Dataset class as input*","9942bcc3":"*Lets plot toxic and clean data count*","545951ef":"*Before Strting the dataset creation lets find out the max length from dataframe*","e3b34e53":"*In Below plot we can clearly notice that there is a huge count gap between the toxic and regular comments*","0dff1b11":"*Lets check the distribution of different type of toxic comments*","4c33f6b3":"*Importing the datasets* ","5c25d6c2":"*Lets test on a small sample *","0d538ba0":"*Lets Define our bert model for testing , Ignore below section not requied*","345182ad":"*Import  the required Libraries*","fde765c0":"*Defining the tokenizer*","d7aa9910":"*Now lets create our dataset for modeling , We can override few functions as per our need to create a custom dataset*","dc639a0f":"*I am creating a dataframe with equal number of samples from clean and toxic lables*","618bd39c":"*Lets create a list of labels for modelling purpose*","10e401d9":"# Work in progress below i have just checked for a single comment","e6930e94":"*Lets Experiment with a single comment*","a9520a52":"*Lets run our trainer enable fast_dev_run if you want to just do a quick run its helps in dubugging the code *","efd3a667":"*Now lets split the data training dataset into training and validation*","621f216d":"*Lets check of there is multiple comment present from same ID,s*"}}