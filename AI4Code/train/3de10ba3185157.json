{"cell_type":{"e09d1e1a":"code","0a62b1ff":"code","241e18f0":"code","d55fe9bc":"code","f80a6e77":"code","39a74fa7":"code","702c5107":"code","fc8615a8":"code","ab18e24b":"code","cb8198fd":"code","8402b13f":"code","bc21519b":"code","4d71c97f":"code","b829048e":"code","87ea15e2":"code","f5696ec3":"code","12ede66a":"code","9b56e0fb":"code","eb971315":"code","41c32d60":"code","34ef9e9c":"code","c1df74f0":"markdown","46bb7a21":"markdown","dc7b726f":"markdown","dfcd5af2":"markdown","e1671891":"markdown","d5ec00ab":"markdown","68109e37":"markdown","1ebf8c76":"markdown","7c9a9e0d":"markdown","7b28c0aa":"markdown","362fabb5":"markdown","1393ac20":"markdown","0baf6758":"markdown","d97c0bbc":"markdown","64a8be9a":"markdown","3f9955a3":"markdown","7fcb8230":"markdown","2c8d1fac":"markdown","cfbe215a":"markdown","36887b52":"markdown","21b51601":"markdown","d53fac1f":"markdown","534b9daf":"markdown","278dfdaf":"markdown","2d373c58":"markdown","90092252":"markdown","655f8a2a":"markdown","ceff06b7":"markdown","045d357f":"markdown","2e213437":"markdown"},"source":{"e09d1e1a":"# Basic packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine learning\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","0a62b1ff":"# Define data paths\nTRAIN_PATH = '\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv'\nTEST_PATH = '\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv'\n\n# Load data sets from CSV\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","241e18f0":"# Print first 5 rows\ntrain.head()","d55fe9bc":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","f80a6e77":"# Which columns containing NA\ntrain.columns[train.isnull().any(axis=0)]","39a74fa7":"print('Size of training set:', train.shape)\nprint('Maximum loss:', train.loss.max())","702c5107":"train.loss.value_counts().plot(figsize=(12,9), kind='bar')\nplt.xlabel('loss', fontsize=20)\nplt.ylabel('freq', fontsize=20)\nplt.show()","fc8615a8":"int_col = []\nfor c in train.columns:\n    if (train[c] == train[c].astype('int')).all(): # check no decimal place\n        int_col.append(c)\nint_col","ab18e24b":"for c in int_col[:-1]:\n    print('Number unique values in {}:'.format(c), train[c].nunique())","cb8198fd":"for c in ['f1','f86']:\n    print('Max value count in {}: {} ({}%)'.format(c, train[c].value_counts().max(), train[c].value_counts().max()\/train.shape[0]*100))\n    train[c].value_counts().sort_values(ascending=False).plot(figsize=(12, 9), kind='bar')\n    plt.show()","8402b13f":"corr = train.corr() # correlation matrix\n\n# create mask matrix to mask out half of the entries in correlation matrix\nmask = np.zeros_like(corr, dtype=np.bool) # zero boolean matrix with the same size as corr\nmask[np.triu_indices_from(mask)] = True # triangularize the mask matrix\n\nfig, ax = plt.subplots(1, 1, figsize=(12 , 12))\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .82}, mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()\n","bc21519b":"fig, axes = plt.subplots(10, 10, figsize=(15,15))\naxes = axes.flatten() # flatten from 2D to 1D for looping\n\nmerge_df = train[train.columns[:-1]].append(test[train.columns[:-1]])\nmerge_df['training'] = 1\nmerge_df['training'][train.shape[0]:] = 0\n\nfor idx, ax in enumerate(axes):\n    sns.kdeplot(data=merge_df, x=f'f{idx}', fill=True, \n                ax=ax, alpha=.5, hue='training')\n    \n    # remove ticks and labels\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    # set y-axis invisible\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc='right', weight='bold', fontsize=10)\n    \nfig.tight_layout()\nplt.show()","4d71c97f":"X = train[train.columns[:-1]]\ny = train['loss']\nX_test = test[train.columns[:-1]]\n\nX_combine = X.append(X_test) # combine X_train and X_test for data cleaning","b829048e":"scaler = RobustScaler()\nX_combine = scaler.fit_transform(X_combine)","87ea15e2":"X = X_combine[:train.shape[0]]\nX_test = X_combine[train.shape[0]:]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.50, random_state=123) ","f5696ec3":"xgb = XGBRegressor(max_depth=7,\n                   n_estimators=2500,\n                   learning_rate=0.008,\n                   subsample=0.84,\n                   booster= 'gbtree',\n                   colsample_bytree= 0.70,\n                   reg_lambda= 5,\n                   reg_alpha= 32,\n                   n_jobs=-1,  \n                   alpha=0.5)    \n    \nxgb.fit(X_train, y_train)\nxgb_val_pred = xgb.predict(X_val)\nxgb_val_pred = np.clip(xgb_val_pred, y.min(), y.max()) # cap at the range of the loss\n\nprint(f'MSE: {np.sqrt(mean_squared_error(y_val, xgb_val_pred))}')","12ede66a":"xgb = XGBRegressor(max_depth=7,\n                   n_estimators=2500,\n                   learning_rate=0.008,\n                   subsample=0.84,\n                   booster= 'gbtree',\n                   colsample_bytree= 0.70,\n                   reg_lambda= 5,\n                   reg_alpha= 32,\n                   n_jobs=-1,  \n                   alpha=0.5)    \n    \nxgb.fit(X, y)","9b56e0fb":"catbr = CatBoostRegressor(depth=6,\n                            iterations=1600,\n                            learning_rate=0.024,\n                            l2_leaf_reg=20,\n                            random_strength=1.5,\n                            grow_policy='Depthwise',\n                            leaf_estimation_method='Newton', \n                            bootstrap_type='Bernoulli',\n                            thread_count=4,\n                            verbose=False,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            od_type='Iter',\n                            early_stopping_rounds=500)    \n\ncatbr.fit(X, y, verbose=200)","eb971315":"y_pred = catbr.predict(X_test)\ny_pred = np.clip(y_pred, y.min(), y.max())","41c32d60":"submission_csv = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\nsubmission_csv['loss'] = y_pred","34ef9e9c":"submission_csv.to_csv('\/kaggle\/working\/submission.csv', index=False)","c1df74f0":"We are dealing with 42 classes, which is a bit too many for a classification problem.","46bb7a21":"There are 2 common ways of inspecting the correlation between features:\n1. using `sns.FacetGrid()`\n2. using heatmap\nSince there are too many features in this problem, also each feautre does not have their own meaning, we decide to use the 2nd method.","dc7b726f":"### Exploratory Data Analysis (EDA)","dfcd5af2":"### Scaling","e1671891":"Different features have different range of values. It is better to normalize the features. There are several scalers we can use:\n\n1. `MinMaxScaler`\n2. `StandardScaler`\n3. `RobustScaler`\n\nSince we don't know any outliers, we assume there are some. We decide to use `RobustScaler`.","d5ec00ab":"There are 2 main reasons we should NOT treat it as a classification problem:\n1. Too many classes\n2. The data set is highly imbalanced.","68109e37":"### Loading training and test sets","1ebf8c76":"From this data set, we can see that all columns do not have any meaning. It means that we cannot easily see the connection between each feature and the target. Therefore, we cannot perform feature engineering based on the knowledge of the problem.\n\nHowever, there are several characteristic we can see from the data set:\n1. The data type of the target `loss` is integer. It implies that this problem can be potentially be both regression problem or multiclass classification problem.\n2. The data type of feature `f1` is also integer. It can be treated as categorical data. Also, `f1` might not be the only integral feature.","7c9a9e0d":"I am inspired by https:\/\/www.kaggle.com\/subinium\/tps-aug-simple-eda that even column with data type `float` can also be integral data type if there is no decimal point. Therefore, we should check as below:","7b28c0aa":"### Loading Python packages","362fabb5":"### Data Set Preparation","1393ac20":"The columns `f1` and `f86` look more categorical. We should also check how many classes for those two columns. If too many, only a few data points will be in each class. There will be no reason for us to consider them as categorical.","0baf6758":"## Model Creation","d97c0bbc":"The distributions in both training and test sets are similar.","64a8be9a":"### Checking missing values","3f9955a3":"Thanks to https:\/\/www.kaggle.com\/subinium\/tps-aug-simple-eda\n    \nThanks to https:\/\/www.kaggle.com\/somayyehgholami\/1-tps-aug-21-xgboost-catboost","7fcb8230":"## Acknowledge","2c8d1fac":"Before building our model, we should better check our both training set and test set share similar distribution on each feature.","cfbe215a":"#### Correlation between features","36887b52":"### Inspection of data set","21b51601":"You can see that only ~1% data, at max, are in the same category (if we treat them as a categorical type). Therefore, we better consider them as continuous.","d53fac1f":"Next, we are trying to see how many features having integral data type.","534b9daf":"Same as the target `loss`, the integral columns can be both considered as categorical and semi-continuous columns. \n\nBy checking the number of unique values on each column data, we can have some ideas on whether that column is categorical or semi-continuous.","278dfdaf":"### Data Set Splitting","2d373c58":"Drop the unrelated column `id`.","90092252":"To decide whether we should treat it as a classification problem or a regression problem, we should check to see if the data set is a balanced data set or not. We plot the histogram of the `loss` labels. If the distribution is uniform, they are balanced.","655f8a2a":"This notebook is the analysis for the Tabular Playground Series (TPS) released in August 2021.","ceff06b7":"To study the first statement, we can simply inspect how many unique values of `loss` we are dealing with. And it is the number of classes","045d357f":"We can see that there is no missing values appearing in our data set.","2e213437":"### Feature Distribution"}}