{"cell_type":{"f4afd706":"code","b90a44bf":"code","e92af6cd":"code","6421bbd8":"code","b03fb7f9":"code","30ed1411":"code","71a1720e":"code","e6eb0cb3":"code","f4af8eb6":"code","fb3eb3d1":"code","8d78e9db":"code","27992bd3":"code","5ae50dcd":"markdown","180a03e5":"markdown","b1ddd1d2":"markdown","5187f848":"markdown","b26c54d1":"markdown","95887fe7":"markdown","f581dbc2":"markdown","66f6b700":"markdown","80c0cca7":"markdown"},"source":{"f4afd706":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b90a44bf":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport lib_abscur as abscur\nimport lib_prepare as prepare","e92af6cd":"data = abscur.get_abs_curses()\ndata","6421bbd8":"d_data = data.pct_change()\nd_data","b03fb7f9":"output = np.array((d_data > 0) * 1)\nprint('output',output.shape)\n\ninpData1 = prepare.data_to_window(d_data,10)\nprint('inpData1',inpData1.shape)\n\ninpData2 = prepare.date_to_input(d_data.index)\nprint('inpData2',inpData2.shape)\n\ninpData1Learn,inpData2Learn,inpData1Calc,inpData2Calc = prepare.split_learn_calc(inpData1,inpData2)\nprint('inpData1Learn,inpData2Learn,inpData1Calc,inpData2Calc',[v.shape for v in (inpData1Learn,inpData2Learn,inpData1Calc,inpData2Calc)])\n\ninpData1Learn,inpData2Learn,output,learnDates = prepare.chistim_pustoty(inpData1Learn,inpData2Learn,output,d_data.index.values[:,np.newaxis])\nprint('inpData1Learn,inpData2Learn,output,learnDates',[v.shape for v in (inpData1Learn,inpData2Learn,output,learnDates)])","30ed1411":"X1_train, X1_test, X2_train, X2_test, y_train, y_test, d_train, d_test = train_test_split(inpData1Learn,inpData2Learn,output,learnDates,test_size=0.33,shuffle=False)","71a1720e":"normLayer = keras.layers.experimental.preprocessing.Normalization(axis=-1)\nnormLayer.adapt(X1_train)","e6eb0cb3":"def build_model(hp=None):\n    input1 = keras.layers.Input(shape=X1_train.shape[1:])\n    x1 = normLayer(input1)\n\n    if hp is None:\n        units = 30\n        kernel = 3\n    else:\n        units = hp.Choice('units',range(10,100))\n        kernel = hp.Choice('kernel',range(2,5+1))\n\n    x1 = keras.layers.Conv1D(units,\n                             kernel,\n                             padding='same',\n                             activation='relu')(x1)\n\n    x1 = keras.layers.Flatten()(x1)\n\n    input2 = keras.layers.Input(shape=(X2_train.shape[1],))\n\n    x = keras.layers.Concatenate()([x1,input2])\n\n    output = []\n    for n in range(y_train.shape[1]):\n        output.append(keras.layers.Dense(1,name=data.columns[n])(x))\n\n    model = keras.Model((input1,input2),output)\n    model.compile(optimizer=tf.optimizers.Adam(),\n                loss=tf.losses.BinaryCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n    # model.summary()\n    return model\n\n# build_model()","f4af8eb6":"pn = 'RandomSearch'\n\ncb = [tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=100,restore_best_weights=True,verbose=False),\n      tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=5,verbose=False)]\n\nmodel = build_model()\n\nhistory = model.fit((X1_train,X2_train),list(y_train.T),\n                    epochs=1000,\n                    validation_split=0.3,\n                    verbose=False,\n                    callbacks = cb)\n\ntrain_kach = model.evaluate([X1_train,X2_train],list(y_train.T),verbose=1)\ntest_kach = model.evaluate([X1_test,X2_test],list(y_test.T),verbose=1)\n\n","fb3eb3d1":"hs = pd.DataFrame(history.history,index=range(1,len(history.history['loss'])+1))\nhs['accuracy'] = hs[[col+'_accuracy' for col in data.columns]].mean(axis=1)\nhs['val_accuracy'] = hs[['val_'+col+'_accuracy' for col in data.columns]].mean(axis=1)\nhs","8d78e9db":"fig = plt.figure(figsize=(15,6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nhs[['loss','val_loss']].plot(ax=ax1)\nhs[['accuracy','val_accuracy']].plot(ax=ax2)\nax1.grid()\nax2.grid()\nax1.set_title('loss')\nax2.set_title('accuracy')\nplt.show()","27992bd3":"\ndf = pd.DataFrame(np.array([train_kach[1:-len(data.columns)],\n                            train_kach[-len(data.columns):],\n                            test_kach[1:-len(data.columns)],\n                            test_kach[-len(data.columns):]]).T,columns=['train_loss','train_accuracy','test_loss','test_accuracy'],index=data.columns)\ndf.sort_values('test_accuracy',inplace=True,ascending=False)\ndf\n\nfig = plt.figure(figsize=(15,10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\ndf[['train_loss','test_loss']].plot(kind=\"bar\", ax=ax1)\ndf[['train_accuracy','test_accuracy']].plot(kind='bar',ax=ax2)\nax1.grid()\nax2.grid()\nax1.set_title('loss')\nax2.set_title('accuracy')\nplt.show()\n\nfig = plt.figure(figsize=(20,7))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\ndf[['train_loss','test_loss']].plot(kind=\"scatter\", ax=ax1, x='train_loss', y='test_loss')\ndf[['train_accuracy','test_accuracy']].plot(kind='scatter', ax=ax2, x='train_accuracy', y='test_accuracy')\nax1.grid()\nax2.grid()\nax1.set_title('loss')\nax2.set_title('accuracy')\nplt.show()","5ae50dcd":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043a\u043e\u0442\u0438\u0440\u043e\u0432\u043e\u043a","180a03e5":"## \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u0442\u0435\u0441\u0442","b1ddd1d2":"# \u041f\u043e\u0434\u043a\u043b\u044e\u0447\u0430\u0435\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438","5187f848":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f","b26c54d1":"# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","95887fe7":"# \u041d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 (\u0441\u043b\u043e\u0439 \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u043a\u0438 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0432\u0445\u043e\u0434\u0430)","f581dbc2":"# \u041a\u0430\u0447\u0435\u0441\u0442\u0432\u043e \u0432 \u0442\u0440\u0435\u0439\u043d\u0435 \u0438 \u0442\u0435\u0441\u0442\u0435","66f6b700":"# \u0418\u0441\u0442\u043e\u0440\u0438\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","80c0cca7":"# \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435"}}