{"cell_type":{"12312390":"code","c6f1fa6f":"code","e1966fb5":"code","6a4f4339":"code","4e6938b7":"code","bb341ee9":"code","d322b5c5":"code","46d59b7a":"code","8f3989b9":"code","27ca31ff":"code","29204e3d":"code","6cfa2f28":"code","b3dbfffb":"code","585db581":"code","1d9d38f4":"code","bdf02988":"code","934588bb":"code","c25295b5":"markdown","1e6afbe2":"markdown","15dfa249":"markdown","b1aa6daf":"markdown","419317e3":"markdown","d08f3cd0":"markdown","06c01d08":"markdown","6de69dae":"markdown","54a0c442":"markdown","b10cbc72":"markdown","246f4a69":"markdown","be0a7a3e":"markdown","75c997ea":"markdown","08757739":"markdown","baf220eb":"markdown"},"source":{"12312390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6f1fa6f":"\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom keras import optimizers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras import backend as K\n\nimport matplotlib.pyplot as plt","e1966fb5":"df = pd.read_csv('..\/input\/electrical-fault-detection-and-classification\/classData.csv')\ndf.head()","6a4f4339":"x = df.iloc[:, 4:]\nx.head()","4e6938b7":"df['fault_type'] = df['G'].astype('str') + df['C'].astype('str') + df['B'].astype('str') + df['A'].astype('str')\ndf.head()","bb341ee9":"df['fault_type'] = df['fault_type'].astype('category')\ndf['fault_type']","d322b5c5":"fault_type_dictionarie = {'0000': 'No Fault',\n                          '1001': 'LG',\n                          '0011': 'LL',\n                          '1011': 'LLG',\n                          '0111': 'LLL',\n                          '1111': 'LLLG'}","46d59b7a":"y_bin = LabelBinarizer().fit_transform(df['fault_type'])\ny_bin","8f3989b9":"x_train, x_test = train_test_split(x, test_size=0.2, random_state=42)\ny_train, y_test = train_test_split(y_bin, test_size=0.2, random_state=42)","27ca31ff":"t = MinMaxScaler()\nt.fit(x_train)\nx_train_scaled = t.transform(x_train)\nx_test_scaled = t.transform(x_test)\nx_scaled = t.transform(x)","29204e3d":"def model_creator():\n    input = Input(shape = 6, name = 'inputLayer')\n    e = Dense(6, name = 'hiddenLayer_1', activation = 'relu')(input)\n    e = Dense(6, name = 'hiddenLayer_2', activation = 'relu')(e)\n    e = Dense(6, name = 'hiddenLayer_3', activation = 'relu')(e)\n    classifier = Dense(6, activation= 'softmax', name = 'classifer') (e)\n\n    model = Model(inputs = input, outputs = classifier)\n    return model","6cfa2f28":"model = model_creator()\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","b3dbfffb":"model_history = model.fit(x_train_scaled,\n                               y_train,\n                               validation_data=(x_test_scaled, y_test), epochs=750)","585db581":"plt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.rcParams['figure.dpi'] = 500\nplt.show()\n        \nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.rcParams['figure.dpi'] = 500\nplt.show()","1d9d38f4":"recompiling_optimizer = optimizers.Adam(learning_rate=0.00001)\nmodel.compile(loss='categorical_crossentropy', optimizer=recompiling_optimizer, metrics=['accuracy'])","bdf02988":"model_history_recompiled = model.fit(x_train_scaled,\n                               y_train,\n                               validation_data=(x_test_scaled, y_test), epochs=750)","934588bb":"plt.plot(model_history.history['accuracy'] + model_history_recompiled.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'] + model_history_recompiled.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.rcParams['figure.dpi'] = 500\nplt.show()\n        \nplt.plot(model_history.history['loss'] + model_history_recompiled.history['loss'])\nplt.plot(model_history.history['val_loss'] + model_history_recompiled.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.rcParams['figure.dpi'] = 500\nplt.show()","c25295b5":"## Binarizing the Target Column","1e6afbe2":"## Recompiling the Model","15dfa249":"## Data","b1aa6daf":"## Train\/Test Splitting","419317e3":"## Extracting the Feature Space","d08f3cd0":"## Scaling of the Feature","06c01d08":"## Instantation and Compiling","6de69dae":"# Model Creation and Training","54a0c442":"# Data Preprocessing","b10cbc72":"## Libraries","246f4a69":"## Creation","be0a7a3e":"As it is easily recognizable by checking the **accuracy** and **loss** plots of the model, around the 100 th epoch of the training process, the plots are experiencing spookeness which gets more severe while the training process continues. This is generally the result of having a significantly high learning rate, according to the situation. This can be solved either by decreasing the learning rate initially which makes the training process extremely slow, or recompiling the model and defining a new optimizer with lower learning rate to continue the training process.\n\nThe second approach is taken and its effectiveness can be understood due to the fact that the spookeness of both **accuracy** and **loss** have became weaken, considerably.\n\nAccording to the overall trend of the plots, this model does not seem to be capable of achieving higher levels of accuracy.","75c997ea":"# Importings","08757739":"## Mining the Target Column","baf220eb":"## Training"}}