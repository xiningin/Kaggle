{"cell_type":{"0db318ed":"code","fc1cee7b":"code","a72a5014":"code","1ce07019":"code","e75a3c48":"markdown","5b6d2c4d":"markdown","6ae30fa7":"markdown","060fb33c":"markdown","c85fc61f":"markdown","03d61166":"markdown","1cbb804c":"markdown","5f949556":"markdown"},"source":{"0db318ed":"import numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import make_classification\n%matplotlib inline\nplt.style.use('default')","fc1cee7b":"###########################################\ndef sigmoid(z):\n    return 1\/(1+np.exp(-z))\n###########################################\n\n# plot \nx=np.linspace(-8,8,1000)\ny=sigmoid(x)\nfig, ax = plt.subplots(figsize=(8,5))\nax.spines['left'].set_position(('data', 0.0))\nax.spines['bottom'].set_position(('data', 0.0))\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nplt.plot(x,y,linestyle='-')\nplt.show()","a72a5014":"import numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn.datasets import make_classification\n\n\ndef sigmoid(z):\n    return 1\/(1+np.exp(-z))\n\n\ndef train(X, y, num_iterations = 2000, learning_rate = 0.5, record = False):\n\n\n    # Initialisation\n    nbr_params , nbr_examples = X.shape\n    w, b = np.full((nbr_params,1),0.01) , 0\n    record = {\"ws\": [],\"bs\":[],\"costs\": []}\n\n\n    # Loop over num_iterations\n    for i in range(num_iterations+1):\n        \n        # Forward propagation\n        A = sigmoid(np.dot(w.T,X)+b)                                    \n\n\n        # Compute cost\n        cost = np.sum(-y*np.log(A)-(1-y)*np.log(1-A))\/nbr_examples                      \n    \n        # Backward propagation\n        # Compute derivatives and update the weights and bias (Gradient Descent Step)\n        dw = np.dot(X,(A-y).T)\/nbr_examples\n        db = np.sum(A-y)\/nbr_examples\n            \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        # Recording\n        if record and i % 100 == 0:\n            record[\"costs\"].append(cost)\n            record[\"ws\"].append(w)\n            record[\"bs\"].append(b)\n\n\n    return w,b,record\n\n\ndef predict(w, b, X):\n    \n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T,X)+b)\n    \n    for i in range(A.shape[1]):\n        \n        if A[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    return Y_prediction\n\n# Initialisation - generate random 2D samples\nX,y0 = make_classification(n_samples=1000,n_features=2,n_informative=2,n_redundant=0)\ny = y0.reshape(1,y0.shape[0])\nX=X.T\n\n\n# Compute W and b\nw,b,record=train(X,y,num_iterations=2000,learning_rate=0.0001,record = True)\n\nx1=np.linspace(-4,4,1000)\nws=record[\"ws\"]\nbs=record[\"bs\"]\ncosts = record[\"costs\"]\n\n\n# Print graph\n# plt.figure(figsize=(12,8))\n# plt.scatter(X[0,:],X[1,:],c=y0.T,marker='o',s=25,edgecolor='k')\n# for i in range(len(costs)):\n#     y1=-(x1*ws[i][0,0]\/ws[i][1,0]+bs[i]\/ws[i][1,0])\n#     plt.plot(x1,y1,linestyle='-')\n       \n# plt.show()\n","1ce07019":"from matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML\n\nfig, ax = plt.subplots(figsize=(12,8))\nxdata, ydata = [], []\nln, = plt.plot([], [], 'r', animated=True)\nf = list(range(len(costs)))\nx1=np.linspace(-4,4,1000)\n\ndef init():\n    ax.scatter(X[0,:],X[1,:],c=y0.T,marker='o',s=25,edgecolor='k')\n    ln.set_data(xdata,ydata)\n    return ln,\n\ndef update(i):\n    y1=-(x1*ws[i][0,0]\/ws[i][1,0]+bs[i]\/ws[i][1,0])\n    ln.set_data(x1, y1)\n    return ln,\nplt.close()\n\nanim = FuncAnimation(fig, update, frames=f,\n                    init_func=init, blit=True, interval = 100,repeat=True)\n\nHTML(anim.to_jshtml())\n","e75a3c48":"We replace each term (1),(2),(3) into (0)\n\n\n$\\frac{\\partial\\ J\\left(W,b\\right)}{\\partial\\ w_i}=\\frac{1}{m}\\sum_{i=1}^{m}\\left[{\\frac{\\partial\\ Loss\\left(y_{predict}^i,y_{true}^i\\right)}{\\partial\\ y}\\times\\frac{\\partial\\sigma(z)}{\\partial\\ z}\\times\\frac{\\partial(w^t\\bullet\\ X+b)}{\\partial\\ w_i}}\\right]$\n\n\n$=\\frac{1}{m}\\sum_{i=1}^{m}\\left[{\\frac{y_{true}-y}{y(1-y)} \\times y\\left(1-y\\right) \\times x_i }\\right]$\n\nfinally,\n\n$\\frac{\\partial\\ J\\left(W,b\\right)}{\\partial\\ w_i}=\\frac{1}{m}\\sum_{i=1}^{m}(y_{true}-y) \\times x_i $\n\n$\\frac{\\partial\\ J\\left(W,b\\right)}{\\partial\\ b}=\\frac{1}{m}\\sum_{i=1}^{m}(y_{true}-y) $","5b6d2c4d":"## Backward propagation\n\nHow to decrease the cost using the gradient descent algorithm ?\n\nThe principle of the \"gradient descent\" algorithm is based on the idea that to find the minimum of a function, you must follow the direction of the descent, step by step. When we can no longer descend in any direction, we are at a minimum. \n\nThe direction of descent is indicated by the derivative of the function multiplied by -1.\n\nThis algorithm is based on strong assumptions: the function is differentiable and convex (bowl-shaped)\n\n>so, we have to calculate the partial derivatives of J(W,b) with respect to every wi and b. \n\n$\\Delta_{wi}=\\frac{\\partial\\ J\\left(W,b\\right)}{\\partial\\ w_i}$\n\n$\\Delta_{b}=\\frac{\\partial\\ J\\left(W,b\\right)}{\\partial\\ b}$\n\nand a step of the gradient decent is : \n\n$w_i \\leftarrow w_i + LearninRate \\times \\Delta_{wi}$\n\n$b \\leftarrow b + LearninRate \\times \\Delta_{b}$\n\n\n##### We need the composition of derivatives\n\nThe derivative of $(fog)(x)=f(g(x))$\n\nis $(fog)'(x)=f'(g(x)) \\times g'(x) $\n\nwith y=g(x), we can write $(fog)'(x)=f'(y) \\times g'(x) $  \n\nWith partial derivatives notation \n\n$\\frac{\\partial\\ (fog)(x)}{\\partial\\ x}= \\frac{\\partial\\ f(y)}{\\partial\\ y} \\times \\frac{\\partial\\ g(x)}{\\partial\\ x}$ \n\n$\\frac{\\partial\\ (fog)(x)}{\\partial\\ x}= \\frac{\\partial\\ f(y)}{\\partial\\ y} \\times \\frac{\\partial\\ y}{\\partial\\ x}$ \n\n\n##### Partial derivatives of J(W,b)\n\nSo,to calculate the partial derivatives of J(W,b) with respect to every wi, we use a double composition\n\n$\\frac{\\partial\\ J\\left(W,b\\right)}{\\partial\\ w_i}=\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial\\ Loss\\left(y_{predict}^i,y_{true}^i\\right)}{\\partial\\ w_i}$\n\nwith\n\n$y=\\sigma(z)$\n\n$z=w^t\\bullet X+b$  \n\n$=\\frac{1}{m}\\sum_{i=1}^{m}\\left[{\\frac{\\partial\\ Loss\\left(y_{predict}^i,y_{true}^i\\right)}{\\partial\\ y}\\times\\frac{\\partial\\sigma(z)}{\\partial\\ z}\\times\\frac{\\partial(w^t\\bullet\\ X+b)}{\\partial\\ w_i}}\\right]$ (0)\n\n**we calculate each term**\n","6ae30fa7":"# The code of the neuron\n\n### The neuron in action on a 2D example\n\nIf a point is taken at random in the plan below, will it belong to the yellow points or to the black points? \n\nTo answer this question, the neuron will calibrate its weights (w1, w2, b) using existing data. \n\nIn other words, because we are in 2D, we must find the line w1 \u00d7 x + w2 \u00d7 y + b = 0 which best separates the existing points.\n\nOn the graph, this line is plotted every 100 iterations of the learning phase of the neuron, to approach this \"best\" line.","060fb33c":"## Compute cost \n\nThe cost for the prediction of one observation is defined by :\n$$Loss(y_{predict},y_{true}) = -(y_{true} \\times ln(y_{predict}) + (1-y_{true}) \\times ln(1-y_{predict}) )  $$\n\nThe overall cost for the m observations is defined by J(W,b) :\n$$ J(W,b) = \\frac{1}{m} \\sum_{i=1}^m Loss(y_{predict}^i,y_{true}^i) $$","c85fc61f":"\n\n**The purpose of this notebook is to detail the mathematics behind the neuron**\n\n# What is a neuron ?\n\nIt is a basic machine learning algorithm.\n\n* The machine takes as input n values {x1, x2,\u2026, xn}\n* These n entries are each multiplied by the weights {w1,\u2026, wn} and then summed\n* A bias b is added\n* The result is passed to a function called activation to give the result.\n\nThe activation function is the sigmoid function. It activates the decision. If we look at its curve, it transforms\n\n* negative values into a value close to 0 (No)\n* positive values into a value close to 1 (Yes)\n* values close to 0 to a value close to 0.5 (I don't know)\n\n![NN.png](attachment:NN.png)\n\n**How to calibrate weights and bias?**\n>by finding the weights that minimize the error between the desired result and the result.\n\n* The known data are passed into the machine. (Forward propagation)\n* The result obtained is compared to the desired result using a function called cost. (Compute cost)\n* The weights of the neuron are adapted with one step to decrease (Gradient Descent) the cost function. (Backward propagation)\n* And we start again ...\n* The process stops when the error goes below a threshold.\n\n\n# The math of the neuron\n\n$\\ x=\\ \\left(\\begin{matrix}x_1\\\\x_2\\\\\\begin{matrix}x_3\\\\\\ldots\\\\x_n\\\\\\end{matrix}\\\\\\end{matrix}\\right)is\\ a\\ example\\ of\\ input\\ data,\\ dim=\\left(n,1\\right)$\n\n$X=\\ \\left(\\begin{matrix}x_1^1&\\cdots&x_1^m\\\\\\vdots&\\ddots&\\vdots\\\\x_n^1&\\cdots&x_n^m\\\\\\end{matrix}\\right)\\ is\\ the\\ matrix\\ of\\ m\\ example\\ of\\ input\\ data\\ ,\\dim{=}\\ (n,m)$\n\n$y=\\ \\left(\\begin{matrix}y^1\\ \\ y^2&\\ldots&y^m\\\\\\end{matrix}\\right)\\ is\\ the\\ ouput\\ vector\\ of\\ the\\ m\\ examples,\\dim{=}\\ \\left(1,m\\right)$\n\n$W=\\ \\left(\\begin{matrix}w_1\\\\w_2\\\\\\begin{matrix}w_3\\\\\\ldots\\\\w_n\\\\\\end{matrix}\\\\\\end{matrix}\\right)is\\ the\\ vector\\ of\\ weigth\\ of\\ the\\ neuron,\\ \\dim{=}\\ (n,1)$\n\n$The\\ Sigmoid\\ is\\ a\\ function\\ from\\ R^n\\ to\\ R^n\\ defined\\ by\\ \\sigma(z) = \\frac{1}{1+e^{-z}}$","03d61166":"# Animated line approaching the optimal position to separates the existing points.","1cbb804c":"(1) $\\frac{\\partial Loss\\left(y^i,y_{true}^i\\right)}{\\partial y}=\\left[-(y_{true} \\times ln(y) + (1-y_{true}) \\times ln(1-y))\\right]'$ (derivative according to y)\n\n$=-\\left(\\frac{y_{true}}{y}+\\left(1-y_{true}\\right)\\times\\left(\\frac{-1}{1-y}\\right)\\right)=-\\left(\\frac{y_{true}}{y}+\\frac{y_{true}-1}{1-y}\\right)=\\frac{y_{true}-y}{y(1-y)}$\n\n(2) $\\frac{\\partial\\sigma(z)}{\\partial z}=\\left[\\frac{1}{1+e^{-z}}\\right]'$ (derivative according to z)\n\n$=\\frac{e^{-z}}{\\left(1+e^{-z}\\right)^2}=\\frac{1}{\\left(1+e^{-z}\\right)}\\times\\frac{(1-1+e^{-z})}{\\left(1+e^{-z}\\right)}$\n\n$=\\frac{1}{\\left(1+e^{-z}\\right)}\\times \\left[1- \\frac{1}{\\left(1+e^{-z}\\right)}\\right]$\n\n$=y\\left(1-y\\right)$ with $y=\\sigma(z)$\n\n(3) $\\frac{\\partial(w^t\\bullet X+b)}{\\partial w_i}=x_i$\n\n(3 according to b) $\\frac{\\partial(w^t\\bullet X+b)}{\\partial b}=1$\n","5f949556":"## Forward propagation\n\n$Y_{predict} = \\sigma(w^t \\bullet X + B)$\n\nThe dimension of $Y_{predict}$ is (1,m)"}}