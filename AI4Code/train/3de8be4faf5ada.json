{"cell_type":{"7ede53bf":"code","ab880f32":"code","b511912d":"code","5c4500d0":"code","93a33dd0":"code","a1cb969b":"code","56a4bd55":"code","a6ea72bd":"code","b25a5ca8":"code","c895405c":"code","c0d20938":"code","b06051f1":"code","321a01a1":"code","3a35388c":"code","b1ee6f58":"code","f8a50bb8":"code","6b902292":"code","fcf9a533":"code","2ac0d12a":"code","e1f25bcb":"code","a0fe7998":"code","4b491eac":"code","840a607f":"code","0b867e22":"code","3a4ea5eb":"code","014b5e1b":"code","7e2f54f7":"code","cd223569":"code","f85d6914":"code","a7ff4e3c":"code","a1f185c5":"code","6be7627d":"code","bc01134d":"code","4b0c6520":"code","f0bd51a1":"code","0efd3c32":"code","ad30f8cf":"code","f9fb861a":"code","5a061849":"code","1f43fe43":"code","3b0a7ba8":"code","535484cc":"code","fca77edc":"code","8ed7628e":"code","f6b9cabe":"code","6bf6ad0f":"code","af4ed603":"code","4c9aadc4":"code","d138d760":"code","82623dcd":"code","7094c8a3":"code","462d4935":"code","0d675bdb":"code","5b17e20c":"code","1e542847":"code","5bd83aab":"code","ce1c0de5":"code","c5b82f9f":"code","b03e647b":"markdown","1635d60e":"markdown","e3c922b6":"markdown","618cae93":"markdown","e46e286d":"markdown","d628e312":"markdown","4e422bd1":"markdown","b4ad3abb":"markdown","6b183efb":"markdown","0a3cdad4":"markdown","755de37f":"markdown","1545aaf2":"markdown","5d842277":"markdown","ab62f257":"markdown","0a87865d":"markdown","cc9e7b5d":"markdown","afc2c3c7":"markdown","fbe54ce8":"markdown","6f23852e":"markdown","99a543f7":"markdown","9eac1139":"markdown","b93c13ab":"markdown","d712bbd9":"markdown","f4817e3b":"markdown","98fcdd64":"markdown","7305e25b":"markdown","a353d8ce":"markdown","90625f4e":"markdown","3ba6bc3f":"markdown","04311206":"markdown","4d113190":"markdown","061fba15":"markdown","1cca0021":"markdown","9016e5ad":"markdown","0c9301b7":"markdown","22993c23":"markdown","b1aa151e":"markdown","43e47aee":"markdown","b1a60275":"markdown","2e45bc52":"markdown","0411c9f6":"markdown","244bfb8b":"markdown","a993b0b0":"markdown","51aa27c0":"markdown","b1ffcd18":"markdown","e0f7270b":"markdown","38a0a250":"markdown","4adc505f":"markdown","d01323ed":"markdown","0a7ce0a6":"markdown","3f5a3068":"markdown","9b15e8b7":"markdown","23a45ac8":"markdown","51e4cb1e":"markdown","b69e9fcd":"markdown","7f793da9":"markdown","a39df55a":"markdown","bf1ac7f7":"markdown","c24008f6":"markdown"},"source":{"7ede53bf":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import jaccard_similarity_score\n\nfrom scipy.stats import randint, uniform\n\nfrom xgboost import XGBClassifier\n\nimport warnings\n\n\n# Silence pesky deprecation warnings from sklearn\nwarnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\nsns.set_palette('deep')\nrandom_state = 1","ab880f32":"_train = pd.read_csv(\"..\/input\/train.csv\")\n_test = pd.read_csv(\"..\/input\/test.csv\")\n\n# Make a copy to be modified\ntrain = _train.copy()\ntest = _test.copy()","b511912d":"train_len = len(train)\nprint(\"Training dataset size = {}\".format(train_len))\ntrain.head()","5c4500d0":"test_len = len(test)\nprint(\"Test dataset size = {}\".format(test_len))\ntest.head()","93a33dd0":"# Remove PassengerId since it's not a feature\ntrain.drop(columns='PassengerId', inplace=True)\ntest.drop(columns='PassengerId', inplace=True)","a1cb969b":"# Concatenate the train and test datasets. Survived is not a feature so we drop it\ndataset = pd.concat([train, test], sort=False).drop(columns='Survived')\npd.DataFrame({'No. NaN': dataset.isna().sum(), '%': dataset.isna().sum() \/ len(dataset)})","56a4bd55":"train.drop(columns='Cabin', inplace=True)\ntest.drop(columns='Cabin', inplace=True)","a6ea72bd":"with sns.axes_style(\"darkgrid\"):\n    g = sns.FacetGrid(train, hue='Survived', height=5, aspect=2.5)\n    g.map(sns.kdeplot, 'Age', shade=True)\n    g.add_legend()\n    g.set(xticks=np.arange(0, train['Age'].max() + 1, 5), xlim=(0, train['Age'].max()))","b25a5ca8":"# Use KMeans to cluster `Age`\nnum_clusters = 4\nX = train[[ 'Age', 'Survived']].dropna()\nkmeans = KMeans(n_clusters=num_clusters, random_state=random_state)\nkmeans.fit(X)\nX['AgeCluster'] = kmeans.labels_\n\n# Plot the decision boundary\n# See http:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_digits.html\nplt.figure(figsize=(10,5))\nh = 0.01\nx_min, x_max = X['Age'].min() - h, X['Age'].max() + h\ny_min, y_max = X['Survived'].min() - h, X['Survived'].max() + h\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n# Predict the age cluster for each point in a mesh\nZ = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\ncmap = sns.cubehelix_palette(start=2.8, rot=.1, as_cmap=True)\nZ = Z.reshape(xx.shape)\nplt.imshow(Z, interpolation='nearest',\n           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n           cmap=cmap, aspect='auto')\n\n# Plot the ages\nsns.scatterplot(x='Age', y='Survived', hue='AgeCluster', data=X, palette=cmap)\n\n# Plot the centroids as a white X\ncentroids = kmeans.cluster_centers_\nplt.scatter(centroids[:, 0], centroids[:, 1],\n            marker='x', s=169, linewidths=3,\n            color='w')\nplt.yticks([0, 1])\nplt.title(\"Age clusters and decision boundaries\")\nplt.show()","c895405c":"# Convert K-means clusters to age bands\nage_bands = []\nfor k in range(num_clusters):\n    age_bands.append(xx[Z==k].min())\n\n# Since the clusters are not sorted we sort the intervals\nage_bands.sort()\n\n# Set the lower bound of the first interval to 0\nage_bands[0] = 0\n\n# Set the higher bound of the last interval to infinite just in case there are older older passengers in the test set\nage_bands.append(np.inf)\n\n# Convert list to numpy array\nprint(\"Age bands: {}\".format(np.array(age_bands)))","c0d20938":"# Use both the training and test dataset to fill the missing Age values\ndataset = pd.concat([train, test], sort=True)\ndataset['AgeBand'] = pd.cut(dataset['Age'], age_bands)\n\ndataset.groupby('AgeBand')['Survived'].mean()","b06051f1":"# Use both the training and test dataset to fill the missing Age values\nfill_age_df = dataset[['Name', 'AgeBand', 'Pclass', 'SibSp', 'Parch', 'Sex']].copy()\n\n# Get the titles of the passengers\nfill_age_df['Title'] = fill_age_df['Name'].apply(lambda x: x[x.find(', ') + 2:x.find('.')])\n\npd.crosstab(fill_age_df['Title'], fill_age_df['Sex']).transpose()","321a01a1":"# Join Mlle (Mademoiselle) and Ms with Miss - Mlle and Miss both indicate a unmarried status while Ms is more generic\n# it can mean both Miss and Mrs, I chose Miss because it's more frequent\nfill_age_df['Title'].replace(to_replace=['Mlle', 'Ms'], value='Miss', inplace=True)\n\n# Join Mme (Madame) with Mrs - Mme and Mrs both indicate a married status\nfill_age_df['Title'].replace(to_replace='Mme', value='Mrs', inplace=True)\n\n# Join the remaining titles with low frequencies\nfill_age_df['Title'].replace(to_replace=['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir',\n                                         'Col', 'Capt', 'the Countess', 'Jonkheer', 'Dona'],\n                             value='Rare', inplace=True)\n\ntitle_dummies = pd.get_dummies(fill_age_df['Title'], drop_first=True)\nfill_age_df = pd.concat([fill_age_df, title_dummies], axis=1)\nfill_age_df.head(2)","3a35388c":"# Encode ages and sex\nfill_age_df['IsMale'] = fill_age_df['Sex'].astype('category').cat.codes\nfill_age_df['AgeBand'] = fill_age_df['AgeBand'].astype('category').cat.codes\n\n# Drop columns we no longer need\nfill_age_df.drop(columns=['Name', 'Sex', 'Title'], inplace=True)\n\nfill_age_df.head(2)","b1ee6f58":"# Drop all rows with unknown age bands (-1) from the training set\nX_train = fill_age_df.loc[fill_age_df['AgeBand'] != -1].drop(columns='AgeBand')\nY_train = fill_age_df['AgeBand'].loc[fill_age_df['AgeBand'] != -1]\n\n# Get all rows with unknown age bands (-1) for the test set\nX = fill_age_df.loc[fill_age_df['AgeBand'] == -1].drop(columns='AgeBand')\n\n# Some shapes to double-check\nprint(\"Training samples shape: {}\".format(X_train.shape))\nprint(\"Training labels shape: {}\".format(Y_train.shape))\nprint(\"Samples to predict shape: {}\".format(X.shape))","f8a50bb8":"logreg = LogisticRegression(random_state=random_state)\nlogreg_scores = cross_val_score(logreg, X_train, Y_train, cv=10)\nprint(\"Logistic Regression cross-validation scores: {:.3f}\".format(logreg_scores.mean()))\n\nknn = KNeighborsClassifier()\nknn_scores = cross_val_score(knn, X_train, Y_train, cv=10)\nprint(\"KNeighbors cross-validation scores: {:.3f}\".format(knn_scores.mean()))\n\ntree = DecisionTreeClassifier(random_state=random_state)\ntree_scores = cross_val_score(tree, X_train, Y_train, cv=10)\nprint(\"Tree Classifier cross-validation scores: {:.3f}\".format(tree_scores.mean()))","6b902292":"# Make predictions using our best model (Decision Tree)\ntree.fit(X_train, Y_train)\nY_pred = tree.predict(X_train)\n\n# Compute the per-class\/per-age band accuracy\ntotal = np.bincount(Y_train.values, minlength=4)\ncorrect = np.bincount(Y_pred[Y_pred == Y_train.values], minlength=4)\nclass_acc = correct \/ total\npd.DataFrame({'AgeBand': np.arange(4),\n              'AgeInterval': dataset['AgeBand'].cat.categories,\n              'PerClassAcc.': class_acc})","fcf9a533":"# Feature importance\ng = sns.barplot(x=tree.feature_importances_, y=X.columns, orient='h')\n_ = g.set_xlabel('Relative importance')\n_ = g.set_ylabel('Features')\n_ = g.set_title('Feature Importance')","2ac0d12a":"Y = tree.predict(X)\nfill_age_df.loc[fill_age_df['AgeBand'] == -1, 'AgeBand'] = Y","e1f25bcb":"train['AgeBand'] = fill_age_df.iloc[:train_len, 0]\ntest['AgeBand'] = fill_age_df.iloc[train_len:, 0]\n\n# Remove Age column\ntrain.drop(columns='Age', inplace=True)\ntest.drop(columns='Age', inplace=True)\n\ntrain.head(2)","a0fe7998":"_ = sns.countplot(x='Sex', hue='Survived', data=train)\ntrain.groupby('Sex')['Survived'].mean()","4b491eac":"# One-encode using dummies\ntrain['IsMale'] = pd.get_dummies(train['Sex'], drop_first=True)\ntest['IsMale'] = pd.get_dummies(test['Sex'], drop_first=True)\n\n# Drop the Sex column\ntrain.drop(columns='Sex', inplace=True)\ntest.drop(columns='Sex', inplace=True)","840a607f":"_ = sns.countplot(x='Pclass', hue='Survived', data=train)\ntrain.groupby('Pclass')['Survived'].mean()","0b867e22":"_ = sns.countplot(x='Embarked', hue='Survived', data=train)","3a4ea5eb":"# Plot relationship between Embarked, Pclass, and IsMale\n_ = sns.factorplot(x='Embarked', col='Pclass', row='IsMale', data=train, kind='count')","014b5e1b":"# Passengers grouped by Embarked, Pclass, and IsMale\nembarked_corr = (train[['Survived', 'Embarked', 'Pclass', 'IsMale']].groupby(['Embarked', 'Pclass', 'IsMale'])                                            \n                                                                    .agg(['count', 'sum', 'mean']))\nembarked_corr.columns = embarked_corr.columns.droplevel(0)\nembarked_corr.columns = ['Total', 'Survived', 'Rate']\nembarked_corr","7e2f54f7":"train.drop(columns='Embarked', inplace=True)\ntest.drop(columns='Embarked', inplace=True)","cd223569":"dataset = pd.concat([train, test], sort=True)\ndataset['TicketFreq'] = dataset.groupby('Ticket')['Ticket'].transform('count')\ndataset['PassengerFare'] = dataset['Fare'] \/ dataset['TicketFreq']\n\ntrain.head(2)","f85d6914":"num_fare_bins = 3\ndataset['FareBand'], fare_bins = pd.qcut(dataset['PassengerFare'], num_fare_bins, retbins=True)\n_ = sns.countplot(x='FareBand', hue='Survived', data=dataset)\n_ = plt.xticks(rotation=30, ha='right')","a7ff4e3c":"# Plot relationship between FareBand and Pclass\ng = sns.factorplot(x='FareBand', col='Pclass', data=dataset, kind='count')\n_ = g.set_xticklabels(rotation=30, ha='right')","a1f185c5":"band = pd.Interval(left=7.775, right=13.0)\nmask = (dataset['FareBand'] == band) & (dataset['Pclass'] != 1)\n\ndataset.loc[mask, ['FareBand', 'Pclass', 'PassengerFare']].groupby(['FareBand', 'Pclass']).agg(['mean'])","6be7627d":"band1 = pd.Interval(left=0, right=7.775)\nband2 = pd.Interval(left=7.775, right=13.0)\nmask = ((dataset['FareBand'] == band1) | (dataset['FareBand'] == band2)) & (dataset['Pclass'] == 3)\n\ndataset.loc[mask, ['FareBand', 'Pclass', 'Survived']].groupby(['FareBand', 'Pclass']).agg(['mean'])","bc01134d":"dataset.loc[mask, ['FareBand', 'Pclass', 'AgeBand']].groupby(['FareBand', 'Pclass']).agg(['value_counts']).sort_index()","4b0c6520":"train.drop(columns='Fare', inplace=True)\ntest.drop(columns='Fare', inplace=True)\n\ntrain.head(2)","f0bd51a1":"# Create the new feature for the whole dataset \n# (I am using _train and _test because train and test no longer contain 'Embarked')\ndataset = pd.concat([_train, _test], sort=True, ignore_index=True)","0efd3c32":"surname = dataset['Name'].apply(lambda x: x[:x.find(',')])\nticket = dataset['Ticket'].apply(lambda x: x[:-1])\n\ndataset['SPTE'] = (surname.astype(str) + '-' + dataset['Pclass'].astype(str) + '-'\n           + ticket.astype(str) + '-' + dataset['Embarked'].astype(str))\n\nspte_count = dataset['SPTE'].value_counts(sort=False)\n\ndef spte_group_lebeler(group):\n    group_elements = dataset.loc[dataset['SPTE'] == group, 'PassengerId']\n    if len(group_elements) == 1:\n        return 0\n    else:\n        return group_elements.min()\n\ndataset['GroupId'] = dataset['SPTE'].apply(spte_group_lebeler)\ndataset.drop(columns='SPTE', inplace=True)\ndataset.tail()","ad30f8cf":"# Groups that share the same ticket number\ndef ticket_group_labeler(group):\n    unique_groups = group.unique()\n    if len(unique_groups) == 1:\n        return unique_groups[0]\n    elif len(unique_groups) == 2 and min(unique_groups) == 0:\n        return dataset.loc[group.index, 'PassengerId'].min()\n    else:\n        raise ValueError(\"Found conflict between SPTE and ticket grouping:\\n\\n{}\".format(dataset.loc[group.index]))\n\ndataset['GroupId'] = dataset.groupby('Ticket')['GroupId'].transform(ticket_group_labeler)\ndataset.tail()","f9fb861a":"# Calculate the size of each group\ndataset['GroupSize'] = dataset.groupby('GroupId')['GroupId'].transform('count')\ndataset.loc[dataset['GroupId'] == 0, 'GroupSize'] = 1\n\n# InGroup is 1 for groups with more than one member\ndataset['InGroup'] = (dataset['GroupSize'] > 1).astype(int)\n\n# Add to the train and test datasets\ntrain['InGroup'] = dataset.iloc[:train_len, -1]\ntest['InGroup'] = dataset.iloc[train_len:, -1].reset_index(drop=True)\n\n_ = sns.countplot(x='InGroup', hue='Survived', data=train)\ntrain.groupby('InGroup')['Survived'].mean()","5a061849":"# Get the titles of the passengers\ndataset['Title'] = dataset['Name'].apply(lambda x: x[x.find(', ') + 2:x.find('.')])\n\n# Create a mask to account only for females or boys in groups\nmask = (dataset['GroupId'] != 0) & ((dataset['Title'] == 'Master') | (dataset['Sex'] == 'female'))\n\n# Get the number of females and boys in each group, discard groups with only one member\nwcg_groups = dataset.loc[mask, 'GroupId'].value_counts()\nwcg_groups = wcg_groups[wcg_groups > 1]\n\n# Update the mask to discard groups with only one female or boy\nmask = mask & (dataset['GroupId'].isin(wcg_groups.index))\n\n# Create the new feature using the updated mask\ndataset['InWcg'] = 0\ndataset.loc[mask, 'InWcg'] = 1\n\nprint(\"Number of woman-child-groups found:\", len(wcg_groups))\nprint(\"Number of passengers in woman-child-groups:\", len(dataset.loc[dataset['InWcg'] == 1]))\n\n# Add to the train and test datasets\ntrain['InWcg'] = dataset.iloc[:train_len, -1]\ntest['InWcg'] = dataset.iloc[train_len:, -1].reset_index(drop=True)","1f43fe43":"dataset['WcgAllSurvived'] = dataset.loc[dataset['InWcg'] == 1].groupby('GroupId')['Survived'].transform(np.nanmean)\n\n# `np.nanmean` returns NaN for groups without survival information (test set only groups)\n# Replace the NaN with 0\ndataset.loc[dataset['WcgAllSurvived'].isna(), 'WcgAllSurvived'] = 0\ndataset['WcgAllSurvived'] = dataset['WcgAllSurvived'].astype(int)\n\n# Add to the train and test datasets\ntrain['WcgAllSurvived'] = dataset.iloc[:train_len, -1]\ntest['WcgAllSurvived'] = dataset.iloc[train_len:, -1].reset_index(drop=True)","3b0a7ba8":"dataset['WcgAllDied'] = (1 - dataset.loc[dataset['InWcg'] == 1].groupby('GroupId')['Survived'].transform(np.nanmean))\n\n# `np.nanmean` returns NaN for groups without survival information (test set only groups)\n# Replace the NaN with 0\ndataset.loc[dataset['WcgAllDied'].isna(), 'WcgAllDied'] = 0\ndataset['WcgAllDied'] = dataset['WcgAllDied'].astype(int)\n\n# Add to the train and test datasets\ntrain['WcgAllDied'] = dataset.iloc[:train_len, -1]\ntest['WcgAllDied'] = dataset.iloc[train_len:, -1].reset_index(drop=True)","535484cc":"train.drop(columns=['Name', 'SibSp', 'Parch', 'Ticket'], inplace=True)\ntest.drop(columns=['Name', 'SibSp', 'Parch', 'Ticket'], inplace=True)","fca77edc":"train.head()","8ed7628e":"test.head()","f6b9cabe":"# Split the training set into samples and targets\nX_train = train.drop(columns='Survived')\nY_train = train['Survived'].astype(int)\n\n# Test set samples to predict\nX_test = test\n\n# Scale features such that the mean is 0 and standard deviation is \nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Number of cross-validation folds\nk_folds = 10\n\n# Number of estimators for tree-based ensembles\nn_estimators = 100\n\n# Create a dictionary containing the instance of the models, scores, mean accuracy and standard deviation\nclassifiers = {\n    'name': ['DecisionTree', 'RandomForest', 'ExtraTrees', 'AdaBoost', 'LogReg', 'KNN', 'SVC',\n             'XGBoost', 'GradientBoost'],\n    'models': [DecisionTreeClassifier(random_state=random_state),\n               RandomForestClassifier(random_state=random_state, n_estimators=n_estimators),\n               ExtraTreesClassifier(random_state=random_state, n_estimators=n_estimators),\n               AdaBoostClassifier(random_state=random_state, n_estimators=n_estimators),\n               LogisticRegression(random_state=random_state),\n               KNeighborsClassifier(),\n               SVC(random_state=random_state),\n               XGBClassifier(random_state=random_state, n_estimators=n_estimators),\n               GradientBoostingClassifier(random_state=random_state, n_estimators=n_estimators)], \n    'scores': [],\n    'acc_mean': [],\n    'acc_std': []\n}\n\n# Run cross-validation and store the scores\nfor model in classifiers['models']:\n    score = cross_val_score(model, X_train, Y_train, cv=k_folds, n_jobs=4)\n    classifiers['scores'].append(score)\n    classifiers['acc_mean'].append(score.mean())\n    classifiers['acc_std'].append(score.std())    \n\n# Create a nice table with the results\nclassifiers_df = pd.DataFrame({\n    'Model Name': classifiers['name'],\n    'Accuracy': classifiers['acc_mean'],\n    'Std': classifiers['acc_std']\n}, columns=['Model Name', 'Accuracy', 'Std']).set_index('Model Name')\n\nclassifiers_df.sort_values('Accuracy', ascending=False)","6bf6ad0f":"# Utility function to report best scores\n# Source: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py\ndef report(results, n_top=3, limit=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        if limit is not None:\n            candidates = candidates[:limit]\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.4f} (std: {1:.4f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print()\n\n# Number of iterations\nn_iter_search = 200","af4ed603":"logreg = LogisticRegression(random_state=random_state)\nrand_param = {\n    'penalty': ['l1', 'l2'],\n    'C': uniform(0.01, 10)\n }\n\nlogreg_search = RandomizedSearchCV(logreg, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\nlogreg_search.fit(X_train, Y_train)\nreport(logreg_search.cv_results_)\n\nlogreg_best = logreg_search.best_estimator_","4c9aadc4":"svc = SVC(random_state=random_state, probability=True)\nrand_param = {\n    'C': uniform(0.01, 10),\n    'gamma': uniform(0.01, 10)\n }\n\nsvc_search = RandomizedSearchCV(svc, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\nsvc_search.fit(X_train, Y_train)\nreport(svc_search.cv_results_)\n\nsvc_best = svc_search.best_estimator_","d138d760":"knn = KNeighborsClassifier()\nrand_param = {\n    'n_neighbors': randint(1, 25),\n    'leaf_size': randint(1, 50),\n    'weights': ['uniform', 'distance']\n}\n\nknn_search = RandomizedSearchCV(knn, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\nknn_search.fit(X_train, Y_train)\nreport(knn_search.cv_results_)\n\nknn_best = knn_search.best_estimator_","82623dcd":"ada = AdaBoostClassifier(random_state=random_state, n_estimators=n_estimators)\nrand_param = {\n    'learning_rate': uniform(0.1, 10),\n}\n\nada_search = RandomizedSearchCV(ada, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\nada_search.fit(X_train, Y_train)\nreport(ada_search.cv_results_)\n\nada_best = ada_search.best_estimator_","7094c8a3":"g = sns.barplot(x=ada_best.feature_importances_, y=test.columns, orient='h')\n_ = g.set_xlabel('Relative importance')\n_ = g.set_ylabel('Features')","462d4935":"etc = ExtraTreesClassifier(random_state=random_state, n_estimators=n_estimators)\nrand_param = {\n    'bootstrap': [True, False],\n    'max_depth': np.append(randint(1, 10).rvs(10), None),\n    'max_features': randint(1, X_train.shape[1]), # From 1 to number of features is a good range\n    'min_samples_split': randint(2, 10)\n}\n\netc_search = RandomizedSearchCV(etc, param_distributions=rand_param, n_iter=n_iter_search, cv=k_folds, n_jobs=4, verbose=1)\netc_search.fit(X_train, Y_train)\nreport(etc_search.cv_results_)\n\netc_best = etc_search.best_estimator_","0d675bdb":"g = sns.barplot(x=etc_best.feature_importances_, y=test.columns, orient='h')\n_ = g.set_xlabel('Relative importance')\n_ = g.set_ylabel('Features')","5b17e20c":"logreg_pred = logreg_best.predict(X_test)\nsvc_pred = svc_best.predict(X_test)\nknn_pred = knn_best.predict(X_test)\nada_pred = ada_best.predict(X_test)\netc_pred = etc_best.predict(X_test)\n\n# Make a data frame with the predictions from all models\npred_df = pd.DataFrame({\n    'Logistic Regression': logreg_pred,\n    'SVC': svc_pred,\n    'KNN': knn_pred,\n    'AdaBoost': ada_pred,\n    'Extra Tress': etc_pred\n})\n\njsim_df = pd.DataFrame(np.nan, columns=pred_df.columns, index=pred_df.columns)\nfor i in pred_df.columns:\n    for j in pred_df.loc[:, i:].columns:\n        jsim_df.loc[i, j] = jaccard_similarity_score(pred_df[i], pred_df[j])\n        jsim_df.loc[j, i] = jsim_df.loc[i, j]\n\n_ = sns.heatmap(jsim_df, linewidths=0.1, vmax=1.0, vmin=0, square=True, linecolor='white', annot=True, cmap='coolwarm')","1e542847":"estimators = [\n    ('Logistic Regression', logreg_best),\n    ('SVC', svc_best),\n    ('KNN', knn_best),\n    ('AdaBoost', ada_best),\n    ('Extra Trees', etc_best)\n]\n\neclf = VotingClassifier(estimators=estimators)\nensemble_param = {'voting': ['hard', 'soft']}\n\neclf_search = GridSearchCV(eclf, param_grid=ensemble_param, cv=k_folds, n_jobs=4, verbose=1)\neclf_search.fit(X_train, Y_train)\nreport(eclf_search.cv_results_)\n\neclf_best = eclf_search.best_estimator_","5bd83aab":"best_model = KNeighborsClassifier(leaf_size=3, weights='uniform', n_neighbors=19)\nscore = cross_val_score(best_model, X_train, Y_train, cv=k_folds, n_jobs=4)\nbest_model.fit(X_train, Y_train)\nprint(\"Cross-validation accuracy: {0:.4f}\".format(score.mean()))\n\n# Prediction\nbest_pred = best_model.predict(X_test)","ce1c0de5":"submission_df = pd.DataFrame({'PassengerId': _test['PassengerId'], 'Survived': best_pred})\nsubmission_df.to_csv(\"submission.csv\", index=False)","c5b82f9f":"train.to_csv(\"submission_train.csv\", index=False)\ntest.to_csv(\"submission_test.csv\", index=False)","b03e647b":"These results were quite surprising to me. Passengers that overpaid for their tickets shouldn't have substantially different chances of survival; and to make it even more surprising, those that overpaid have lower chances of survival. If we look at the other features and their relationship with these two groups we find something interesting about the distribution of children.","1635d60e":"Despite belonging to the same fare band, 3rd class passengers paid on average less than 2nd class passengers. There is a clear boundary between the two, just not enough to segment them into different bands.  We can also check if there is any correlation between 3rd class passengers that paid higher fares and survival.","e3c922b6":"# 3. Data Analysis <a class=\"anchor\" id=\"data-analysis\"><\/a>\n\nLike most kernels, we will start by analyzing the features from the dataset, one-by-one, focusing on:\n- Finding correlations, not only between each feature and survival but also between the features themselves to understand if the information the feature provides is not redundant.\n- Creating visualizations that help us understand the data.\n- Cleaning the dataset by removing redundant features.\n- Filling missing data when needed.\n- Converting data types when advantageous (e.g. converting categorical data to numerical).","618cae93":"### 5.2.3 K-nearest neighbors <a class=\"anchor\" id=\"knn\"><\/a>","e46e286d":"Passengers with higher ticket class (lower `Pclass`) have higher chances of survival:\n- 1st class: 63%\n- 2nd class: 48%\n- 3rd class: 24%\n\nKeep the feature as-is, `Pclass` is already encoded in integers (one-hot encoding would leave out the natural order of ticket class which is an important element of the feature).","d628e312":"### 3.2.1 Filing missing Age values <a class=\"anchor\" id=\"fill-age\"><\/a>\n\nTo fill the missing 20% age values, we are going to estimate the age band using the most accurate of the following classifiers:\n- Logistic regression\n- K-nearest neighbors\n- Decision tree\n\nwith the following features:\n- Age\n- Pclass\n- SibSp\n- Parch\n- Sex\n- Title (engineered from Name)\n\nThere are many ways to fill the missing ages, I chose this one because it was an excuse to do more some stuff with machine learning models and it worked well enough.","4e422bd1":"## 3.3 Sex <a class=\"anchor\" id=\"sex\"><\/a>","b4ad3abb":"## 4.5 Engineer WcgAllDied <a class=\"anchor\" id=\"wcgalldied\"><\/a>\n\n`WcgAllDied` is just the opposite of `WcgAllSurvived`. For a given passenger in a woman-child-group, `WcgAllDied` is equal to `1` if all members of that group died; otherwise is `0`. ","6b183efb":"## 5.3 Ensemble modeling <a class=\"anchor\" id=\"ensemble\"><\/a>\n\nWe can combine the best estimators into a single model using a Soft Voting\/Majority Rule classifier (`VotingClassifier`). Ensembles can only improve over individual models if they are not highly correlated. So, let's start by evaluating the Jaccard similarity between the predictions of each individual model.","0a3cdad4":"### 5.2.1 Logistic Regression <a class=\"anchor\" id=\"logreg\"><\/a>","755de37f":"## 3.4 Ticket class (Pclass) <a class=\"anchor\" id=\"pclass\"><\/a>","1545aaf2":"After looking at this chart my first thought was that it was a useful feature, we can see that passengers from different ports have varying chances of survival. But it intuitively, it doesn't really make sense, why would the port make any difference on whether someone survives or not? I think that what this chart shows is just the combination of `IsMale` and `Pclass`, see the next chart and table.","5d842277":"For tree-based classifiers, like this one, we can check how important each feature is to the model using the `feature_importances_` attribute,","ab62f257":"# 6. Predictions and submission <a class=\"anchor\" id=\"submission\"><\/a>\n\nWe can now generate predictions and the submission file using our best model. Since we are using `RandomizedSearchCV` we don't always get the same optimal hyperparameters. The best cross-validation score I have seen is a `KNeighborsClassifier` with the following hyperparameters:\n- `leaf_size = 3`\n- `weights = 'uniform'`\n- `n_neighbors = 19`","0a87865d":"#  1. Introduction <a class=\"anchor\" id=\"introduction\"><\/a>\n\nAs a newbie to data science and a newcomer to Kaggle, this is my first competition and kernel. *Titanic: Machine Learning from Disaster* seemed like the perfect competition to start with given the large number of kernels that cover many of the skills I want to learn. Besides knowledge, those kernels were a source of inspiration and motivation as I was writing my own.","cc9e7b5d":"Also, store the training and test datasets (not needed but might come in handy),","afc2c3c7":"# 5. Modeling <a class=\"anchor\" id=\"models\"><\/a>\n\nLet's check out our final datasets. The training set:","fbe54ce8":"Feature importance:","6f23852e":"The dataset for age prediction is almost done, the last step is to split it into training samples, training labels, and testing samples.","99a543f7":"`FareBand` describes `Pclass` 1 and 2 almost perfectly. But why are there so many passengers with `Pclass = 3` tickets with fares expected for `Pclass = 2`? Let's compare the mean passenger fare and survival for passengers in 2nd and 3rd class in the same fare range (`(7.775, 13.0]`).","9eac1139":"# 7. Conclusion  <a class=\"anchor\" id=\"conclusion\"><\/a>\n\nThis submission scores 0.80861 on the public leaderboard, it's not the highest but not bad for a newcomer.\n\nI enjoyed writing this kernel, hopefully you've enjoyed reading it. Thank you for coming along. Feel free to comment and give sugestions, those are always welcome.","b93c13ab":"## 5.2 Tuning hyperparameters <a class=\"anchor\" id=\"tuning\"><\/a>\n\nTo further improve the models we can tune their hyperparameters using randomized parameter optimization or grid search. I chose randomized parameter optimization because it typically performs just as well as grid search but with much fewer iterations, furthermore, the number of iterations in a parameter that we control. Obviously, more iterations are always better but if we want to make quick tests we can easily reduce the tuning time by lowering this parameter instead of reducing all of the hyperparameter search ranges.","d712bbd9":"From the tested models we'll keep the best five:\n- Logistic Regression\n- SVC\n- K-nearest neighbors\n- AdaBoost\n- Extra Trees (Decision Tree, Random Forest, and Extra Trees all share the same CV score. Random Forest and Extra Trees tend to perform better as they are ensembles of Decision Trees; between Random Forest and Extra Trees there isn't much of a difference, so I decided to go with Extra Trees)","f4817e3b":"### 5.2.4 AdaBoost <a class=\"anchor\" id=\"ada\"><\/a>","98fcdd64":"Now we submit the predictions,","7305e25b":"There are 83 children (`AgeBand = 0`) with 3rd class ticket  in `FareBand = (-0.001, 7.775]` and only 2 in `FareBand = (7.775, 13.0]`. Given that children have ~60% survival rate, the lack of them in `FareBand = (7.775, 13.0]` compared to `FareBand = (-0.001, 7.775]`  explains the lower survival rate.\n\nWe can also explain why some 3rd class passengers seem to have overpaid for their tickets...they didn't, the passengers in `FareBand = (-0.001, 7.775]` paid less because children had a discounted price lowering the passenger fare. and isolating them in `FareBand = (-0.001, 7.775]`.\n\nIn conclusion, `Fare` is correlated with the other features, therefore, we'll drop it from our training and test datasets.","a353d8ce":"Now that we have the clusters we have to convert them into age ranges and feed those ranges to `pandas.cut` to cut both the training and test datasets.","90625f4e":"## 4.3 Engineer InWcg <a class=\"anchor\" id=\"inwcg\"><\/a>\n\nWoman-child-groups are groups of passengers traveling together whose members are either females or boys (males with a `Master` title). Chris Deotte found that members of woman-child-groups are more likely to share the same fate than other groups. He presented the idea in [Titanic using Name only \\[0.81818\\]](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818\/notebook) and perfected the definition of these groups in [Titantic Mega Model - \\[0.84210\\]\n](https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210).\n\n`InWcg` is `1` if the passenger is in a woman-child-group; otherwise is `0`.","3ba6bc3f":"# Table of Contents\n\n* [1. Introduction](#introduction)\n* [2. Loading the Data](#loading-data)\n* [3. Data Analysis](#data-analysis)\n    * [3.1 Missing Data](#missing-data)\n    * [3.2 Age](#age)\n        * [3.2.1 Filling missing Age values](#fill-age)\n    * [3.3 Sex](#sex)\n    * [3.4 Ticket class (Pclass)](#pclass)\n    * [3.5 Port of Embarkation (Embarked)](#embarked)\n    * [3.6 Fare](#fare)\n* [4. Engineering Groups of Passengers](#engineer)\n    * [4.1 Identifying groups of passengers](#identify-groups)\n        * [4.1.1 Identifying groups by surname, PClass, ticket number (excluding last digit), and embarked (SPTE)](#spte)\n        * [4.1.2 Identifying groups by ticket number](#ticket)\n    * [4.2 Engineer InGroup](#ingroup)\n    * [4.3 Engineer InWcg](#inwcg)\n    * [4.4 Engineer WcgAllSurvived](#wcgallsurvived)\n    * [4.5 Engineer WcgAllDied](#wcgalldied)\n* [5. Modeling](#models)\n    * [5.1 Cross-validation](#cv)\n    * [5.2 Tuning hyperparameters](#tuning)\n        * [5.2.1 Logistic Regression](#logreg)\n        * [5.2.2 SVC](#svc)\n        * [5.2.3 K-nearest neighbors](#knn)\n        * [5.2.4 Ada Boost](#ada)\n        * [5.2.5 Extra Trees](#extra-trees)\n    * [5.3 Ensemble modeling](#ensemble)\n* [6. Predictions and submission](#submission)\n* [7. Conclusion](#conclusion)\n* [8. Credits](#credits)","04311206":"#### Feature selection\nStart by getting the title of each passenger using `Name`.","4d113190":"Most passengers have one of four titles:\n- Master\n- Miss\n- Mr\n- Mrs\n\nWe can join `Mlle` with `Miss` since they both typically refer to an unmarried female; `Mme` with `Mrs` since they both typically refer to a married female; and, `Ms` and `Miss`, `Ms` is a more generic title that can either refer to `Miss` or `Mrs` but given that `Miss` is more frequent we'll go with that.\n\nFor the remaining titles, we are going to gather them all in a title named `Rare` to convey that these passengers have a unique social status.\n\nFinally, `Title` is converted to dummies.","061fba15":"# 2. Loading the Data <a class=\"anchor\" id=\"loading-data\"><\/a>","1cca0021":"Just as expected, it performs about the same as our individual models.","9016e5ad":"# 8. Credits  <a class=\"anchor\" id=\"credits\"><\/a>\n\nThe following kernels played an important role in writing this one:\n\n* [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions) by Manav Sehgal  \n* [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy) by LD Freeman  \n* [Blood is thicker than water & friendship forever](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever) by S.Xu  \n* [Titanic: 2nd degree families and majority voting](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting) by Erik Bruin  \n* [Titanic \\[0.82\\] - \\[0.83\\]](https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83) by Konstantin  \n* [Titanic using Name only \\[0.81818\\]](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818) by Chris Deotte  \n* [Titantic Mega Model - \\[0.84210\\]](https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210) by Chris Deotte  ","0c9301b7":"### 5.2.5 Extra Trees <a class=\"anchor\" id=\"extra-trees\"><\/a>","22993c23":"## 3.5 Port of Embarkation (Embarked) <a class=\"anchor\" id=\"embarked\"><\/a>","b1aa151e":"Some observations:\n\n- Children (age < 5) have much higher chances of survival than any other age group\n- Passengers between the ages of 13 to 30 are more likely to die than to survive\n- From 30 to 60 the survival rate is close to 50%\n- Passengers older than 60 are more likely to die than to survive\n\n`Age` can give us some important information, particularly for children and younger adults where there is clearly a trend.\n\nKeeping the previous observations in mind we'll split `Age` in 4 bins\/clusters. I have found that `KMeans` works better than the more frequentely used `pandas.cut` or `pandas.qcut`. It doesn't play a big role in the final score but is particularly important when filling the missing age values.","43e47aee":"# 4. Engineering Groups of Passengers <a class=\"anchor\" id=\"engineer\"><\/a>\n\nMany kernels have explored `SibSp` and `Parch` to engineer family related features. However, in recent months the top kernels have found that `SibSp` and `Parch` contain inconsistencies (see the excellent work by [Erik Bruin](https:\/\/www.kaggle.com\/erikbruin\/titanic-2nd-degree-families-and-majority-voting)) and that looking at groups of passengers travelling together instead of just families yields better results (see [Blood is thicker than water & friendship forever](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever), [Titanic \\[0.82\\] - \\[0.83\\]](https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83), [Titanic using Name only \\[0.81818\\]](https:\/\/www.kaggle.com\/cdeotte\/titanic-using-name-only-0-81818\/notebook), and [Titantic Mega Model - \\[0.84210\\]\n](https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210) for some great work using groups of passengers).\n\nWith this knowledge we are going to engineer four new features derived from groups of passengers:\n- `InGroup`: the passenger is traveling with other passengers or not;\n- `InWcg`: the passenger is a member of a woman-child group or not;\n- `WcgAllSurvived`: whether the members of the woman-child group all survived;\n- `WcgAllDied`: whether the members of the woman-child group all died.\n\n## 4.1 Identifying groups of passengers <a class=\"anchor\" id=\"identify-groups\"><\/a>\n\nTo engineer these features we first have to define what's a group and identify which passengers belong to which group. Based on [Blood is thicker than water & friendship forever](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever) and [Titantic Mega Model - \\[0.84210\\]\n](https:\/\/www.kaggle.com\/cdeotte\/titantic-mega-model-0-84210) we are going to define a group as a set of passengers that all share at least one of the following conditions:\n1. Combination of: surname, PClass, ticket number (excluding the last digit), and embarked;\n2. Ticket number.\n\nThe first condition will identify mostly families while the second will identify groups of friends that share the same ticket, including passengers travelling with families that are not part of the family.","b1a60275":"Also, convert `Sex` to a dummy (`IsMale`) and encode `AgeBand` in integers.","2e45bc52":"The test set:","0411c9f6":"## 3.1 Missing data <a class=\"anchor\" id=\"missing-data\"><\/a>","244bfb8b":"We see that as `Fare` increases, the survival rate also increases, awesome...well, it's also likely that this feature is highly related to `Pclass`, lower `Pclass` should translate to higher `PassengerFare`. ","a993b0b0":"### 4.1.1 Identifying groups by surname, Pclass, ticket number (excluding last digit), and embarked (SPTE) <a class=\"anchor\" id=\"spte\"><\/a>","51aa27c0":"Doesn't look good for ensembling, the predictions are very similar between the models; therefore, the ensemble will output essentially the same predictions.\n\nAs a learning experience, let's create the ensemble anyway and check how it performs.","b1ffcd18":"We can now drop `Name`, `SibSp`, `Parch`, and `Ticket` since we will no longer need them.","e0f7270b":"Overall, none of the models achieves high-accuracy; however, if we check the per-class\/per-age band accuracy, we see that our best model (`DecisionTreeClassifier`) is really good at predicting children (95.7% accuracy for passengers between the ages of 0 and 13.47) and young adults (84.5% accuracy for passengers between the ages of 13.47 and 28.52). As discussed previously, these are the most relevant age bands; therefore, a low overall accuracy shouldn't be a big issue as long as we have high accuracy in these age bands.","38a0a250":"## 4.2 Engineer InGroup <a class=\"anchor\" id=\"ingroup\"><\/a>\n\n`InGroup` is `1` if the passenger is in a group (`GroupId` is not unique); otherwise is `0`.","4adc505f":"We see that in `Southampton (S)` a high number of 3rd class males embarked on the Titanic, so a low survival rate is to be expected. A high number of 1st class passengers embarked in `Cherbourg (C)`, so we see a high survival rate. In `Queenstown (Q)`, however, the chart shows something that I was not expecting given the following observations:\n- the vast majority of passengers are 3rd class passengers;\n- the number of males and females is comparable.\n\nGiven these conditions, the survival rate should be correlated only to `IsMale`, and that is true for females; males however end-up with half the survival rate. Though this is a sample of 42 males in a population of 577, it's not enough to convince me that `Embarked` is independently correlated with survival.","d01323ed":"## 3.6 Fare <a class=\"anchor\" id=\"fare\"><\/a>\nDespite the fact that the [Titanic: Machine Learning from Disaster data page](https:\/\/www.kaggle.com\/c\/titanic\/data) states that `Fare` is the passenger fare, I'm fairly sure that it's actually the ticket fare. Therefore, we will divide `Fare` by the ticket frequency (`TicketFreq`) to compute the actual passenger fare (`PassengerFare`).","0a7ce0a6":"\n### 4.1.2 Identifying groups by ticket number <a class=\"anchor\" id=\"ticket\"><\/a>","3f5a3068":"Females have much higher survival rates as expected, all we have to do is one-hot encode `Sex` so all machine learning algorithms can handle it.","9b15e8b7":"#### Age band modeling\n\nTime to fill the missing age values!\n\nAs stated previously, we are going to create three simple classifier models and use cross-validation to analyze which one is better.","23a45ac8":"## 3.2 Age <a class=\"anchor\" id=\"age\"><\/a>","51e4cb1e":"Apply the integer encoded age bands to the train and test datasets.","b69e9fcd":"### 5.2.2 SVC <a class=\"anchor\" id=\"svc\"><\/a>","7f793da9":"## 4.4 Engineer WcgAllSurvived <a class=\"anchor\" id=\"wcgallsurvived\"><\/a>\n\nFor a given passenger in a woman-child-group, `WcgAllSurvived` is equal to `1` if all members of that group survived; otherwise is `0`. \n\nNote that passengers from the test set are ignored. `WcgAllSurvived` is based on the training set data only.","a39df55a":"We can now predict the age band for passengers with missing data:","bf1ac7f7":"## 5.1 Cross-validation <a class=\"anchor\" id=\"cv\"><\/a>\n\nEvaluate, using cross-validation, which of the following classifiers performs best with our training data:\n- Decision Tree\n- Random Forest\n- Extra Trees\n- AdaBoost\n- Logistic Regression\n- K-nearest neighbors\n- SVC\n- Gradient Boosting\n- eXtreme Gradient Boosting","c24008f6":"Features with missing data:\n- **Cabin (~77%)**: Drop it. Too much data is missing to fill without introducing a significant amount of noise.\n- **Age (~20%)**: A significant amount of data is missing, but if it's useful we can try to fill it.\n- **Fare (1 data point)**: Since only one value is missing we can fill it with something simple like a median.\n- **Embarked (2 data points)**: Since only two values are missing we can fill them with something simple like a median."}}