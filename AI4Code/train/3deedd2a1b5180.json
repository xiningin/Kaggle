{"cell_type":{"628ed0c3":"code","38c54191":"code","9a77d9d0":"code","37dc9cd6":"code","acd663c9":"code","23f9b3a5":"code","7d2c2fdc":"code","2f898438":"code","a5fdf1bc":"code","b0abb44c":"code","952ea1d2":"code","d73e46f5":"code","112b3414":"code","c9bdb259":"code","3ea10c22":"code","717cf8ac":"code","fe3f8a39":"code","96e197fd":"code","11c0c1a6":"code","2484f138":"code","ef04f629":"code","ad2c0689":"code","3c764d02":"code","7a5006b3":"code","9aaf5f6a":"code","226681bf":"code","6e4e0cbe":"code","bfd2ed15":"code","34ccafd2":"code","51de6c71":"code","185c390e":"code","56466ebf":"code","0164043b":"code","2098c7b4":"code","e086c7ea":"code","80d0efd4":"code","e95ae402":"code","04ba1aee":"code","b51af716":"code","dacb4dc6":"code","e6a703a7":"code","83dc4ea0":"code","36dfe897":"code","4c51a077":"code","048f9ae3":"code","2d0ce56d":"code","c0747a5d":"code","502a67c9":"code","20d8d3d2":"code","a687e797":"code","fb9d4285":"code","3c7e38db":"code","3ac574a0":"code","b5fc4c5c":"code","b6579ebc":"code","9c2e958f":"code","94e81b71":"code","abab4329":"code","77598c79":"code","c17da994":"markdown","b4ba027c":"markdown","21074241":"markdown","9fab4814":"markdown","af338542":"markdown","558c8af6":"markdown","dac97e34":"markdown","c8fc46f6":"markdown","e32fdbaa":"markdown","1e6afe0a":"markdown","50324f24":"markdown","fd91e14b":"markdown","d24ffca7":"markdown","d0f93144":"markdown","d3c94a38":"markdown","f60732b0":"markdown","4391ebb1":"markdown","736be31d":"markdown","da72dbe7":"markdown","7fabea00":"markdown","ccc1763d":"markdown","2613522b":"markdown","8d42581d":"markdown","9bdda57b":"markdown"},"source":{"628ed0c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38c54191":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nsns.set(style=\"whitegrid\")","9a77d9d0":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","37dc9cd6":"df.describe(percentiles=[0.1,0.25,0.45,0.55,0.75,0.95]).T\n","acd663c9":"df.columns\n","23f9b3a5":"df=df.drop('Unnamed: 32', axis=1)","7d2c2fdc":"df.shape","2f898438":"col = \"diagnosis\"\ngrouped = df[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0], marker=dict(colors=[\"#6ad49b\", \"#a678de\"]))\nlayout = go.Layout(title=\"\", height=600, legend=dict(x=0.1, y=1.1))\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","a5fdf1bc":"for i in df:\n    null=df[i].isnull().sum()\/len(df)\n    if null>0:\n        print(\"{} 's null rate {} %\".format(i, null))","b0abb44c":"corr = df.corr().round(2)\n\n# Mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set figure size\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Define custom colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap\nsns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.tight_layout()","952ea1d2":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ncols=['diagnosis']\nX=df.drop(cols, axis=1)","d73e46f5":"X.columns","112b3414":"vif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\nprint(vif_data)","c9bdb259":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","3ea10c22":"df['diagnosis'].value_counts()","717cf8ac":"X=df.drop('diagnosis', axis=1)\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","fe3f8a39":"from sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,ConfusionMatrixDisplay","96e197fd":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf","11c0c1a6":"lr=LogisticRegression()\nlr.fit(X_train, y_train)","2484f138":"y_pred=lr.predict(X_train)","ef04f629":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","ad2c0689":"y_test_pred=lr.predict(x_test)","3c764d02":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","7a5006b3":"cfm=confusion_matrix(y_test, y_test_pred)","9aaf5f6a":"disp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot() ","226681bf":"disp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot() ","6e4e0cbe":"y_test_pred_prob=lr.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve","bfd2ed15":"metrics.roc_auc_score(y_test, y_test_pred_prob)","34ccafd2":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","51de6c71":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","185c390e":"# predict the test data and show the first 5 predictions\npredict=lr.predict(x_test)\npredict[1:6]","56466ebf":"#Convert the numericalinto nominal value and check the few result\n\nprediction_nominal=['M' if x<0.1 else 'B' for x in predict ]\nprediction_nominal[1:6]","0164043b":"# step forward feature selection\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","2098c7b4":"df.head()","e086c7ea":"df.columns","80d0efd4":"# In practice, feature selection should be done after data pre-processing,\n# so ideally, all the categorical variables are encoded into numbers,\n# and then you can assess how deterministic they are of the target\n\n# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(df.select_dtypes(include=numerics).columns)\ndata = df[numerical_vars]\ndata.shape","e95ae402":"data.columns","04ba1aee":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","b51af716":"X=data[col]\ny=data['diagnosis']","dacb4dc6":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=0)\n\nX_train.shape, X_test.shape","e6a703a7":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)))","83dc4ea0":"X_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","36dfe897":"\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(RandomForestRegressor(), \n           k_features=10, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train), y_train)","4c51a077":"sfs1.k_feature_idx_","048f9ae3":"X_train.columns[list(sfs1.k_feature_idx_)]","2d0ce56d":"col1=['radius_mean', 'texture_mean', 'smoothness_mean', 'compactness_mean',\n       'symmetry_mean', 'radius_se', 'texture_se', 'concave points_se',\n       'symmetry_se', 'symmetry_worst']\nX=df[col1]\ny=df['diagnosis']\nX_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=10)","c0747a5d":"lr=LogisticRegression(solver='lbfgs')\nlr.fit(X_train, y_train)","502a67c9":"import statsmodels.api as sm\nlr_model=sm.Logit(y, X)\nresult=lr_model.fit()\nprint(result.summary())","20d8d3d2":"y_pred=lr.predict(X_train)\n","a687e797":"print(\"Accuracy Score:-\", metrics.accuracy_score(y_train, y_pred))\nprint(\"F1 Score:-\", metrics.f1_score(y_train, y_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_train, y_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_train, y_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_train, y_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_train, y_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_train, y_pred))","fb9d4285":"y_test_pred=lr.predict(x_test)\n","3c7e38db":"lr_acc=metrics.accuracy_score(y_test, y_test_pred)\nprint(\"Accuracy Score:-\",lr_acc)\nprint(\"F1 Score:-\", metrics.f1_score(y_test, y_test_pred))\nprint(\"Average Precision Score:-\", metrics.average_precision_score(y_test, y_test_pred))\nprint(\"Log Loss:-\", metrics.log_loss(y_test, y_test_pred))\nprint(\"Precision Score:-\", metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall Score:-\", metrics.recall_score(y_test, y_test_pred))\nprint(\"ROC-AUC Score:-\", metrics.roc_auc_score(y_test, y_test_pred))","3ac574a0":"cfm=confusion_matrix(y_test, y_test_pred)\n","b5fc4c5c":"disp = ConfusionMatrixDisplay(confusion_matrix=cfm,\n                               display_labels=lr.classes_)\ndisp.plot()","b6579ebc":"y_test_pred_prob=lr.predict_proba(x_test)[:,1]\ny_test_pred_prob\n\nfrom sklearn.metrics import roc_curve","9c2e958f":"np.mean(y_test_pred_prob)","94e81b71":"metrics.roc_auc_score(y_test, y_test_pred_prob)\n","abab4329":"fpr, tpr,thresholds=roc_curve(y_test,y_test_pred_prob)\nplt.figure(figsize=(10,10))\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr, tpr, label='Logistic Regression')\nplt.xlabel(\"fpr (False Possitive rate)\")\nplt.ylabel(\"tpr-(True Positive rate)\")\nplt.title(\"ROC_AUC\")\nplt.show()","77598c79":"from sklearn.metrics import precision_recall_curve\nno_skill=len(y==1)\/len(y)\ny_test_prob=lr.predict_proba(x_test)[:,1]\nplt.figure(figsize=(10,8))\nplt.plot([0,1],[no_skill, no_skill], label=\"No Skill\")\nprecision, recall,_ =precision_recall_curve(y_test, y_test_prob)\nplt.plot(recall, precision, marker='',label=\"Logistic Regression\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Recall-Precision Curve\")\nplt.legend()\nplt.show()","c17da994":"# Summary Staistics","b4ba027c":"# Obs\n* it is not balanced dataset.","21074241":"# Removed Correlated  features\n","9fab4814":"# Missing Value","af338542":"# Label Encoding ","558c8af6":"# To Test of Individual Regression Coefficients\n* Hypothesis is stated as\n* Ho- bj=0\n* H1:- bj !=0\n* The test statistic for testing above hypothesis is t-test\n* \ud835\udc610 =(bj-0)\/s.e(bj) * s.e is standard error\n","dac97e34":"# Features Names","c8fc46f6":"# Find and Remove correlated features\n","e32fdbaa":"# Feed into the model\n","1e6afe0a":"# Obs\n* We get the coef for all the features and would to conclude that\n* We can see that some of the features whole coef value sis quitte high,\n* The P- value for the few coef is more than the significant values , hence those feture will be removed in order to get the important features.","50324f24":"# Response variables","fd91e14b":"# Droping unnecessary features.","d24ffca7":"* We can see that forward feature selection results in the above columns being selected from all the given columns.\n* 10 features are selected","d0f93144":"# Multicollinearity","d3c94a38":"No missing values","f60732b0":"# Correlation","4391ebb1":"# Obs\n* we can see that all the features are numerical features.\\\n* We don't have any categorical features. This features are very good and as we no need to do any data preprocessing work.\n","736be31d":"# Obs\n* As we can see there is improvement is the accuracy score, log-loss, Precision Score, ROC-AUC score.","da72dbe7":"# Size of the dataset","7fabea00":"# Step Forward feature selection\n","ccc1763d":"# Logistic Regresion","2613522b":"# Forward Selection Method","8d42581d":"# Threshold Probability","9bdda57b":"# To Test of Individual Regression Coefficients\n* Hypothesis is stated as\n        1 Ho- bj=0\n        2.H1:- bj !=0\n* The test statistic for testing above hypothesis is t-test\n* \ud835\udc610 =(bj-0)\/s.e(bj) * s.e is standard error"}}