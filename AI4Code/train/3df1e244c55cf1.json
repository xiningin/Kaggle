{"cell_type":{"35c5e6a2":"code","0ddfe92b":"code","ca2db411":"code","4a0a0eca":"code","bbccdf0b":"code","8a934906":"code","c752484c":"code","df0b37f4":"code","166705f6":"code","75bbb9d6":"code","2e948ac4":"code","ad784017":"code","bc257b94":"code","5185cc18":"code","b1a14263":"code","8124c1e5":"code","5596700e":"code","c972fbb7":"code","fa1e9f9c":"code","d1413496":"code","18efcd1e":"code","cbf7b67a":"code","e733fd8f":"code","dae5e45b":"code","8b1d5f98":"code","917270e6":"code","f7a84d2b":"code","553f9884":"code","4703131e":"code","0c270ab4":"code","5b6a70f4":"code","f0a8405e":"code","7ebae7c2":"code","ff2512f2":"code","25d65405":"code","a8473556":"code","d793ed69":"code","64344884":"code","7156ffc7":"code","efccdf66":"code","d884ffb2":"code","2f139f50":"code","31898cff":"code","ab727538":"code","1b1f131c":"code","c4d5c663":"code","5b023b2b":"code","e6c0ab04":"code","3187af82":"code","f4b1b202":"code","be679fe1":"code","ec69a4d5":"code","e378f9e3":"code","fadb3f21":"code","0776e7cc":"code","d465fd29":"code","70985974":"code","3fd51743":"code","6bc75d0f":"code","c98f2d91":"code","8819d1e1":"code","f733feb1":"code","17a85d7e":"code","5701a986":"code","0f3b3454":"code","8097cc97":"code","2b6acad2":"markdown","422d22ee":"markdown","d4e2f6cf":"markdown","54cef265":"markdown","4e1fb4f4":"markdown","4c476e18":"markdown","15f00edf":"markdown","8658bc56":"markdown","0e271081":"markdown","0f500cca":"markdown","0321a861":"markdown","4b10ae81":"markdown","cb8089c4":"markdown","a92bf2c5":"markdown","9ad87a27":"markdown","9e799438":"markdown","bd14bac5":"markdown","d9fc52cc":"markdown","77045761":"markdown","c3f49ef8":"markdown","d3274608":"markdown","bbfe4646":"markdown","dface4fc":"markdown","e4bb4ccf":"markdown","ccd50a12":"markdown","1f0ff3a0":"markdown","7d2fe7a9":"markdown","ce689221":"markdown","5c41aaf9":"markdown"},"source":{"35c5e6a2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom sklearn.metrics import plot_roc_curve\nfrom numpy import interp\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc","0ddfe92b":"# load data\n\ndata = pd.read_csv(\"..\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv\")","ca2db411":"data.head()","4a0a0eca":"data.info()","bbccdf0b":"(data.isnull().sum() \/ len(data)) * 100","8a934906":"del data['id']\ndel data['author_flair_text']\ndel data['removed_by']\ndel data['total_awards_received']\ndel data['awarders']\ndel data['created_utc']\ndel data['full_link']","c752484c":"data = data.dropna()","df0b37f4":"data.head()","166705f6":"data.info()","75bbb9d6":"# check out numeric columns\n\ndata.describe().T","2e948ac4":"len(data)","ad784017":"plt.figure(figsize=(13,5))\n\nsns.kdeplot(data['score'], shade=  True)","bc257b94":"print(len(data[data['score'] < 10]), 'Posts with less than 10 votes')\nprint(len(data[data['score'] > 10]), 'Posts with more than 10 votes')","5185cc18":"plt.figure(figsize=(13,5))\n\nsns.kdeplot(data['num_comments'], shade=  True)","b1a14263":"print(len(data[data['num_comments'] < 10]), 'Posts with less than 10 comments')\nprint(len(data[data['num_comments'] > 10]), 'Posts with more than 10 comments')","8124c1e5":"# post with the most comments\n\ndata[data['score'] == data['score'].max()]['title'].iloc[0]","5596700e":"\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%\n\n","c972fbb7":"data['clean_title'] = pd.Series([clean_text(i) for i in tqdm(data['title'])])","fa1e9f9c":"words = data[\"clean_title\"].values","d1413496":"ls = []\n\nfor i in words:\n    ls.append(str(i))","18efcd1e":"ls[:5]","cbf7b67a":"# The wordcloud of Cthulhu\/squidy thing for HP Lovecraft\nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"Most discussed terms\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","e733fd8f":"most_pop = data.sort_values('score', ascending =False)[['title', 'score']].head(12)\n\nmost_pop['score1'] = most_pop['score']\/1000","dae5e45b":"plt.figure(figsize = (20,25))\n\nsns.barplot(data = most_pop, y = 'title', x = 'score1', color = 'c')\nplt.xticks(fontsize=27, rotation=0)\nplt.yticks(fontsize=31, rotation=0)\nplt.xlabel('Votes in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Most popular posts', fontsize = 30)","8b1d5f98":"data.head()","917270e6":"most_com = data.sort_values('num_comments', ascending =False)[['title', 'num_comments', 'author']].head(12)\nmost_com['num_comments1'] = most_com['num_comments']\/1000","f7a84d2b":"type(most_com)","553f9884":"x = data.reset_index()\nx[x['index'] == 92800]","4703131e":"most_com = most_com[most_com.author != 'dinoignacio']","0c270ab4":"plt.figure(figsize = (20,25))\n\nsns.barplot(data = most_com, y = 'title', x = 'num_comments1', color = 'y')\nplt.xticks(fontsize=28, rotation=0)\nplt.yticks(fontsize=30, rotation=0)\nplt.xlabel('Comments in Thousands', fontsize = 21)\nplt.ylabel('')\nplt.title('Most commented posts', fontsize = 30)","5b6a70f4":"most_com.head(10)","f0a8405e":"n = data.sort_values('score', ascending =False)\n\nn['score1'] = n['score']\/1000\nn['num_comments1'] = n['num_comments']\/1000","7ebae7c2":"plt.figure(figsize = (15,15))\n\nsns.regplot(data = n, y = 'score1', x = 'num_comments1', color = 'purple')\nplt.xticks(fontsize=14, rotation=0)\nplt.yticks(fontsize=14, rotation=0)\nplt.xlabel('Comments in Thousands', fontsize = 15)\nplt.ylabel('Votes in Thousands')\nplt.title('Comments and votes', fontsize = 14)","ff2512f2":"data[data['num_comments'] == data['num_comments'].max()]","25d65405":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk\n","a8473556":"stemmer = SnowballStemmer('english')","d793ed69":"nltk.download('wordnet')","64344884":"def lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","7156ffc7":"data['title'].iloc[1]","efccdf66":"doc_sample = data['title'].iloc[1]\nprint('original document: ')\n\nwords = []\n\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \n    \nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","d884ffb2":"data.info()","2f139f50":"data['clean_title'] = data['clean_title'].astype(str)","31898cff":"words = []\n\nfor i in data['clean_title']:\n        words.append(i.split(' '))","ab727538":"dictionary = gensim.corpora.Dictionary(words)\n\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","1b1f131c":"# Filter out tokens in the dictionary by their frequency.\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","c4d5c663":"bow_corpus = [dictionary.doc2bow(doc) for doc in words]\nbow_corpus[4310]","5b023b2b":"bow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                               dictionary[bow_doc_4310[i][0]], \nbow_doc_4310[i][1]))","e6c0ab04":"from gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nfrom pprint import pprint\n\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","3187af82":"lda_model = gensim.models.LdaMulticore(bow_corpus,\n                                       num_topics=10,\n                                       id2word=dictionary,\n                                       passes=2,\n                                       workers=2)","f4b1b202":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","be679fe1":"lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                             num_topics=10,\n                                             id2word=dictionary,\n                                             passes=2,\n                                             workers=4)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","ec69a4d5":"unseen_document = 'How a Pentagon deal became an identity crisis for Google'\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","e378f9e3":"data['over_18'] = data['over_18'].astype(int)","fadb3f21":"data['over_18'] = pd.Categorical(data['over_18']) ","0776e7cc":"(data['over_18'].value_counts(normalize=True))","d465fd29":"from sklearn.utils import resample","70985974":"\n# Separate majority and minority classes\ndf_majority = data[data.over_18==0]\ndf_minority = data[data.over_18==1]\n \n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=180000) # reproducible results\n \n# Combine majority class with upsampled minority class\ndata_n = pd.concat([df_majority, df_minority_upsampled])\n \n# Display new class counts\ndata_n['over_18'].value_counts()","3fd51743":"(data_n['over_18'].value_counts(normalize=True))","6bc75d0f":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer","c98f2d91":"processed_text = data_n['clean_title']","8819d1e1":"vectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform(processed_text)\nprint(tfidf.shape)\nprint('\\n')\n#print(vectorizer.get_feature_names())","f733feb1":"data_n['over_18']","17a85d7e":"y = data_n['over_18']","5701a986":"X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(tfidf, y, test_size=0.2, random_state=42)","0f3b3454":"# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nNaive.fit(X_train_tf,y_train_tf)\n# predict the labels on validation dataset\npredictions_NB_tf = Naive.predict(X_test_tf)\n# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy -> \",accuracy_score(predictions_NB_tf, y_test_tf)*100)\nprint(classification_report(predictions_NB_tf,y_test_tf))","8097cc97":"logmodel = LogisticRegression()\nlogmodel.fit(X_train_tf, y_train_tf)\n\npredictions_LR_tf = logmodel.predict(X_test_tf)\n\nprint(\"LR Accuracy -> \",accuracy_score(predictions_LR_tf, y_test_tf)*100)\nprint(classification_report(predictions_LR_tf,y_test_tf))","2b6acad2":"The titles with the most cpmments seem to be more controversial, which makes sense.","422d22ee":"### Same with the comments ","d4e2f6cf":"## Predict 'over_18' titles","54cef265":"# NLP prediction and topic modeling","4e1fb4f4":"We see some columns having too many missing values. We will drop those.","4c476e18":"University to 'univers' --> not too good","15f00edf":"### Lets check out with how much certainty the model predicts a new title to belong to one of the created topics","8658bc56":"In this notebook we will take alook at text data. We will do:\n\n    1) Topic Modeling\n    2) Classification for title that are rated as 'over 18'","0e271081":"The parallelization uses multiprocessing; in case this doesn\u2019t work for you for some reason, try the gensim.models.ldamodel.LdaModel class which is an equivalent, but more straightforward and single-core implementation.","0f500cca":"Now using list comprehension we clean the titles and append the cleaned text as columns to the df.","0321a861":"### Check class balance","4b10ae81":"# Natural Language Processing ","cb8089c4":"Now we have only the relevant information and no missing values.","a92bf2c5":"Both columns look like they are heavily skewed when comparing the 75 percentile and max values.","9ad87a27":"## TF\/IDF","9e799438":"Data, Visualization, USA and Time are popular terms.\nOriginal content seems to be very popular as well, as can be seen in the graph below.","bd14bac5":"## Now we show the output of the model","d9fc52cc":"### Create the dictionary\n\n\n\n--> every unique word in titles","77045761":"## Create TF\/IDF again","c3f49ef8":"## Topic Modeling","d3274608":"These are useful cleaning functions","bbfe4646":"Roughly 75% of the most popular titles are Original Content","dface4fc":"### The score distribution is heavily skewed ","e4bb4ccf":"### Wordcloud","ccd50a12":"As we can see in the output, we should definitely also clean the text for links --> 'https:' ","1f0ff3a0":"## Cleaning","7d2fe7a9":"\n## Create Corpus -> term document frequency\n\ndoc2bow() simply counts the number of occurrences of each distinct word, \nconverts the word to its integer word ID and returns the result as a sparse vector.","ce689221":"### Most popular posts","5c41aaf9":"I am not sure if its possible to get reliable predictions with one class being only represented in 0.5% of the titles.\n\nI tried resampling, but the results at the end look way too good to be true."}}