{"cell_type":{"cc19de36":"code","0e607518":"code","525d9a29":"code","de975a2a":"code","89ab327c":"code","48b67821":"code","b6235ba1":"code","75a51d82":"code","fd11b291":"code","30b7bc28":"code","1face4d7":"code","68fe756f":"code","3bf8a8c3":"code","89eccacb":"code","ee49a5bb":"code","d8e9f500":"code","8677a6fb":"code","eb63efc2":"code","d3ca3ef7":"code","f28869f9":"code","c23f4da2":"code","64690ee4":"code","04c2fc6d":"code","2488cdda":"code","8d32bdc3":"code","9d07a1e9":"code","534491fe":"code","6628203d":"code","2fdabc8e":"code","dfcb3f4d":"code","a80d9aa5":"code","7dfea75d":"code","d5f132a9":"code","f94358bc":"code","11634cc3":"markdown","a74399b3":"markdown","c4662958":"markdown"},"source":{"cc19de36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0e607518":"# Load the dataset\ndata = pd.read_csv('..\/input\/Automobile_data.csv')\n# List the available columns\nlist(data)","525d9a29":"# Preprocess the dataset by coercing the important columns to numeric values\ndata['horsepower'] = pd.to_numeric(data['horsepower'], errors='coerce')\ndata['price'] = pd.to_numeric(data['price'], errors='coerce')\n# And removing any rows which contain missing data\ndata.dropna(subset=['price', 'horsepower'], inplace=True)","de975a2a":"from scipy.stats.stats import pearsonr\npearsonr(data['horsepower'], data['price'])","89ab327c":"from bokeh.io import output_notebook\nfrom bokeh.plotting import ColumnDataSource, figure, show\n\n# enable notebook output\noutput_notebook()\n\nsource = ColumnDataSource(data=dict(\n    x=data['horsepower'],\n    y=data['price'],\n    make=data['make'],\n))\n\ntooltips = [\n    ('make', '@make'),\n    ('horsepower', '$x'),\n    ('price', '$y{$0}')\n]\n\np = figure(plot_width=600, plot_height=400, tooltips=tooltips)\np.xaxis.axis_label = 'Horsepower'\np.yaxis.axis_label = 'Price'\n\n# add a square renderer with a size, color, and alpha\np.circle('x', 'y', source=source, size=8, color='blue', alpha=0.5)\n\n# show the results\nshow(p)","48b67821":"# split our data into train (75%) and test (25%) sets\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(data, test_size=0.25)","b6235ba1":"from sklearn import linear_model\nmodel = linear_model.LinearRegression()\n# the linear regression model expects a 2d array, so we add an extra dimension with reshape\n# input: [1, 2, 3], output: [ [1], [2], [3] ]\n# this allows us to regress on multiple independent variables later\ntraining_x = np.array(train['horsepower']).reshape(-1, 1)\ntraining_y = np.array(train['price'])\n# perform linear regression\nmodel.fit(training_x, training_y)\n# output is a nested array in the form of [ [1] ]\n# squeeze removes all zero dimensions -> [1]\n# asscalar turns a single number array into a number -> 1\nslope = np.asscalar(np.squeeze(model.coef_))\nintercept = model.intercept_\nprint('slope:', slope, 'intercept:', intercept)","75a51d82":"# Now let's add the line to our graph\nfrom bokeh.models import Slope\nbest_line = Slope(gradient=slope, y_intercept=intercept, line_color='red', line_width=3)\np.add_layout(best_line)\nshow(p)","fd11b291":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# define a function to generate a prediction and then compute the desired metrics\ndef predict_metrics(lr, x, y):\n    pred = lr.predict(x)\n    mae = mean_absolute_error(y, pred)\n    mse = mean_squared_error(y, pred)\n    r2 = r2_score(y, pred)\n    return mae, mse, r2\n\ntraining_mae, training_mse, training_r2 = predict_metrics(model, training_x, training_y)\n\n# calculate with test data so we can compare\ntest_x = np.array(test['horsepower']).reshape(-1, 1)\ntest_y = np.array(test['price'])\ntest_mae, test_mse, test_r2 = predict_metrics(model, test_x, test_y)\n\nprint('training mean error:', training_mae, 'training mse:', training_mse, 'training r2:', training_r2)\nprint('test mean error:', test_mae, 'test mse:', test_mse, 'test r2:', test_r2)","30b7bc28":"cols = ['horsepower', 'engine-size', 'peak-rpm', 'length', 'width', 'height']\n# preprocess the data as before (coerce to number)\nfor col in cols:\n    data[col] = pd.to_numeric(data[col], errors='coerce')\n# And removing any rows which contain missing data\ndata.dropna(subset=['price', 'horsepower'], inplace=True)\n\n# Let's see how strongly each column is correlated to price\nfor col in cols:\n    print(col, pearsonr(data[col], data['price']))","1face4d7":"# split train and test data as before\nmodel_cols = ['horsepower', 'engine-size', 'length', 'width']\nmulti_x = np.column_stack(tuple(data[col] for col in model_cols))\nmulti_train_x, multi_test_x, multi_train_y, multi_test_y = \\\n    train_test_split(multi_x, data['price'], test_size=0.25)","68fe756f":"# fit the model as before\nmulti_model = linear_model.LinearRegression()\nmulti_model.fit(multi_train_x, multi_train_y)\nmulti_intercept = multi_model.intercept_\nmulti_coeffs = dict(zip(model_cols, multi_model.coef_))\nprint('intercept:', multi_intercept)\nprint('coefficients:', multi_coeffs)","3bf8a8c3":"# calculate error metrics\nmulti_train_mae, multi_train_mse, multi_train_r2 = predict_metrics(multi_model, multi_train_x, multi_train_y)\nmulti_test_mae, multi_test_mse, multi_test_r2 = predict_metrics(multi_model, multi_test_x, multi_test_y)\nprint('training mean error:', multi_train_mae, 'training mse:', multi_train_mse, 'training r2:', multi_train_r2)\nprint('test mean error:', multi_test_mae, 'test mse:', multi_test_mse, 'test r2:', multi_test_r2)","89eccacb":"import statsmodels","ee49a5bb":"from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF","d8e9f500":"data_for_check= data[cols]                        ### the function's author, variance_inflation_factor expects the presence of a constant in the matrix of explanatory variables\ndata_for_check=data_for_check.assign(const=1)","8677a6fb":"result=[VIF(data_for_check.values, ix) for ix in range(data_for_check.shape[1])]\nprint(result)","eb63efc2":"from sklearn.model_selection import GridSearchCV as GC","d3ca3ef7":"from sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso","f28869f9":"ridge=Ridge()\nlasso=Lasso()","c23f4da2":"data_for_check.drop(columns=['const'],inplace=True)","64690ee4":"data_for_check['price']=data['price']","04c2fc6d":"train,test=train_test_split(data_for_check,test_size=0.25)","2488cdda":"train_Y=train['price']\ntest_Y=test['price']","8d32bdc3":"train.drop(columns=['price'],inplace=True)\ntest.drop(columns=['price'],inplace=True)\ntrain_X=train ; test_X=test\n","9d07a1e9":"parameters={'alpha':[0.005,0.01,0.05,0.1,0.5,0.8,1,1.5,2,5,10]}","534491fe":"grid_search_1=GC(ridge,parameters,scoring='neg_mean_squared_error',cv=5)\ngrid_search_1.fit(train_X.values,train_Y)\n","6628203d":"grid_search_2=GC(lasso,parameters,scoring='neg_mean_squared_error',cv=5)\ngrid_search_2.fit(train_X,train_Y)","2fdabc8e":"print(grid_search_1.best_params_,grid_search_2.best_params_)\nprint(grid_search_1.best_score_,grid_search_2.best_score_)","dfcb3f4d":"ridge_reg = Ridge(alpha=10)\nridge_reg.fit(train_X, train_Y)\n","a80d9aa5":"lasso_reg=Lasso(alpha=10)\nlasso_reg.fit(train_X,train_Y)","7dfea75d":"ridge_train_mae, ridge_train_mse, ridge_train_r2 = predict_metrics(ridge_reg, train_X, train_Y)\nridge_test_mae, ridge_test_mse, ridge_test_r2 = predict_metrics(ridge_reg, test_X, test_Y)\nprint('training mean error:', ridge_train_mae, 'training mse:', ridge_train_mse, 'training r2:', ridge_train_r2)\nprint('test mean error:', ridge_test_mae, 'test mse:', ridge_test_mse, 'test r2:', ridge_test_r2)","d5f132a9":"lasso_train_mae, lasso_train_mse, lasso_train_r2 = predict_metrics(lasso_reg, train_X, train_Y)\nlasso_test_mae, lasso_test_mse, lasso_test_r2 = predict_metrics(lasso_reg, test_X, test_Y)\nprint('training mean error:', lasso_train_mae, 'training mse:', lasso_train_mse, 'training r2:', lasso_train_r2)\nprint('test mean error:', lasso_test_mae, 'test mse:', lasso_test_mse, 'test r2:', lasso_test_r2)","f94358bc":"print('single linear model :',test_mae,\n     'Multi_linear_model:',multi_test_mae,\n     'ridge_regression:',ridge_test_mae,\n     \"lasso_regression:\",lasso_test_mae)","11634cc3":"### As there is some mild multicollinearity shown by values sligtly greater than 5 so we shall apply ridge and lasso regression to see if the results (MSE) improve.","a74399b3":"### Lets see if there is any multicollinearity that is affecting the results ","c4662958":"### Hence as it can be seen LASSO Regresion Model has decreased the mean error to minimum"}}