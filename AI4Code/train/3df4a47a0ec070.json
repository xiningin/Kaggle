{"cell_type":{"2779b93c":"code","340b52f2":"code","11729d96":"code","a004c2f8":"code","4921d7ac":"code","5c935fa3":"code","e5808c9f":"code","ad8de92a":"code","e4600a32":"code","6fd465a5":"code","7954ee0f":"code","7ecdcaeb":"code","714d6f46":"code","b5457271":"code","478e93c5":"code","6c0a1742":"markdown","940ab0e1":"markdown","e023a68b":"markdown","ac333537":"markdown","531ed50b":"markdown","4403f71b":"markdown","51b59ffa":"markdown","d557ee5a":"markdown","fbb33a14":"markdown","b8040399":"markdown","7cc2fb53":"markdown","7b1f10af":"markdown","5d808aea":"markdown","9fbcca3e":"markdown","c0bf2723":"markdown","502033cc":"markdown","e8794acf":"markdown","8d79f198":"markdown","aa23b0c9":"markdown","47dded03":"markdown"},"source":{"2779b93c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","340b52f2":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","11729d96":"#checking the data size of train and test\nprint(train.shape)\nprint(test.shape)","a004c2f8":"train.head()","4921d7ac":"train.info()","5c935fa3":"train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","e5808c9f":"fig = px.histogram(\n    train, \n    x=train['target'], \n    color=train['target'],\n)\nfig.update_layout(\n    title_text='Target distribution', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    \n)\nfig.show()","ad8de92a":"rename_labels = {val:idx for idx, val in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(rename_labels)","e4600a32":"fig, ax = plt.subplots(figsize=(12 , 12))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\"\"\"\nsns.heatmap(corr,\n        square=True, center=0, linewidth=0.2,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        mask=mask, ax=ax) \n    \"\"\"\n\nsns.heatmap(corr,square=True, center=0, \n            linewidth=0.2, cmap='coolwarm',\n           mask=mask, ax=ax) \n\nax.set_title('Feature Correlation Matrix ', loc='left')\nplt.show()","6fd465a5":"train.describe()","7954ee0f":"plt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 2))\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n\n# KDE plots\nax0_sns = sns.kdeplot(ax=ax0, x=train['feature_14'], zorder=2, shade=True)\nax0_sns = sns.kdeplot(ax=ax0, x=test['feature_14'], zorder=2, shade=True)\n\n# Axis and grid customization\nax0_sns.set_xlabel(\"feature_14\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\n\n# Legend params\nax0.legend(['train', 'test'], prop={'size': 5})\nax0_sns.tick_params(labelsize=5)\n\nplt.show()","7ecdcaeb":"plt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 2))\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n\n# KDE plots\nax0_sns = sns.kdeplot(ax=ax0, x=train['feature_38'], zorder=2, shade=True)\nax0_sns = sns.kdeplot(ax=ax0, x=test['feature_38'], zorder=2, shade=True)\n\n# Axis and grid customization\nax0_sns.set_xlabel(\"feature_38\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\n\n# Legend params\nax0.legend(['train', 'test'], prop={'size': 5})\nax0_sns.tick_params(labelsize=5)\n\nplt.show()","714d6f46":"skewed_features = train.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","b5457271":"# 10 bins\ntrain[\"f14_bin_10\"] = pd.cut(train[\"feature_14\"], bins=10, labels=False)\ntrain[\"f38_bin_100\"] = pd.cut(train[\"feature_38\"], bins=10, labels=False)","478e93c5":"#Let\u2019s take a look at the variance without and with the log transformation.\nprint(\"Feature 14 variance : \" , train.feature_14.var())\ntrain['feature_14'] = train.feature_14.apply(lambda x: np.log(1 + x))\nprint(\"Feature 14 variance after apply log(1 + x): \" ,train.feature_14.var())\n","6c0a1742":"<a id='1.2'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.2 Correlation Matrix<\/p>\n\n","940ab0e1":"Binning:\n- When you bin, you can use both the bin and the original feature.Binning also enables you to treat numerical features as categorical.","e023a68b":"\n<a id='7'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:orange; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Stay Tuned and upvote if you like this notebook<\/p>\n\n# Thanks for reading, will keep posting and updating this notebook with more visualizations, Features with diferent techniques.\u00b6\n\n\n","ac333537":"<a id='1.4'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2 Feature Engineering<\/p>\n\n","531ed50b":"<a id='table-of-contents'><\/a>\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Table of Content<\/p>\n\n* [1. Data visualization \ud83d\udcca](#1)\n    * [1.1 Target](#1.1)\n    * [1.2 Numerical Columns](#1.2)\n    * [1.3 Correlation matrix](#1.3)\n    * [1.4 Stats and Skewness](#1.4)\n* [2. Feature Engineering \ud83d\udee0](#2)\n    * [2.1 Binning](#2.1)\n    * [2.2 log transformation](#2.2)\n* [3. Model building](#3)\n","4403f71b":"- Both the features are left skewed and considerably larger data range compared to other columns.Handling skewness will be a key for better results","51b59ffa":"<a id='1.1'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.1 Target Distribution<\/p>\n\n","d557ee5a":"<a id='3'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">3 Model Building<\/p>\n\n# In Progress...","fbb33a14":"## Feature14","b8040399":"<a id='1.3'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.3 Stats and Skewness<\/p>\n\n","7cc2fb53":"# <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Tabular Playground Series  - May 2021 <\/p>\n\n","7b1f10af":"observations:\n\n- column names doesn't make much sense as all of columns are named by integer with prefix as feature.so from domain stand-point, cannot interpret much information from column names.\n- no missing values in the dataset\n- all the columns are of type integer\n- Dimensionality reduction can be a better idea since all 50 columns of the data is of type integer\n\n\n\n","5d808aea":"observations:\n- The mean of the all the features are closer to zero.\n- There is low variance across all the features.\n- The median is mostly 0 except two columns","9fbcca3e":" - In the target variable, class2 has more data points compared to the remaining labels.so, we probably have to address class imbalance problem.\n ","c0bf2723":"<a id='1.4'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:turquoise; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.4 Numerical Columns<\/p>\n\n- From the statistical summary, we know that the data range of Feature 38 and Feature 14 column is considerably larger than the other numeric columns.\n- Feature 38 and Feature 14 has median 1 and all the remaining columns median is zero.","502033cc":"## Feature38","e8794acf":"Observations:\n- Features are highly skewed.Applying feature transformations can help us build a better model.","8d79f198":"observations:\n- The correlation between the continuos variables are mostly moderate and few of them are highly correlated.\n- The correlation between this continuos features and the target are not strong.\n- The variables are not high correlated with the class, so we are not going to delete any variable.","aa23b0c9":"log transformation\n- All the features except feature 14 and 38 has low variance.Thus, we would want to reduce the variance of these columns, and that can be done by taking a log transformation.\n","47dded03":"dropping the id value as it doesn't add any value"}}