{"cell_type":{"f83e57ee":"code","5d4da7cd":"code","0a75de3e":"code","0787df15":"code","8f5ff56a":"code","05948f17":"code","aef5bcd5":"code","a1c2b38a":"code","7a05c6ff":"code","e8c1d896":"code","7cb6796c":"code","c03edb66":"code","f76650a5":"code","93a8da96":"code","b325f7d2":"code","adf00114":"code","712c3791":"code","ec89ffaa":"code","9d8b2edf":"code","b32feb80":"code","b58b6090":"code","4284299c":"code","7140146f":"code","48acc4d7":"markdown","9b865f94":"markdown","d2d0292a":"markdown","5e10e735":"markdown","b70f707f":"markdown","798b8d5a":"markdown","88080947":"markdown","d8115415":"markdown"},"source":{"f83e57ee":"# Libraries\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Import data\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\npassID = test_data['PassengerId']\n\n# Print number of rows and columns\nprint(f\"Shape of Train Data: {train_data.shape}\")\nprint(f\"Shape of Test Data: {test_data.shape}\")","5d4da7cd":"train_data.head()","0a75de3e":"# Missing Values in TRAIN\nprint(\"Missing Values in Train Data\")\nfor column in train_data.columns:\n    mask = train_data[column].isna().sum()\n    if mask != 0:\n        print(f\"Column `{column}` has {mask} missing values, representing {round(mask\/train_data.shape[1], 2)}% of the data\")\nprint(\"\")\nprint(\"Missing Values in Test Data\")\n# Missing Values in TEST\nfor column in test_data.columns:\n    mask = train_data[column].isna().sum()\n    if mask != 0:\n        print(f\"Column `{column}` has {mask} missing values, representing {round(mask\/train_data.shape[1], 2)}% of the data\")","0787df15":"# Fill two missing values in `Embarked` with S (majority)\ntrain_data['Embarked'] = train_data['Embarked'].fillna(\"S\")\ntest_data['Embarked'] = train_data['Embarked'].fillna(\"S\")","8f5ff56a":"# Fill age with medianand change into integer values\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].median()).apply(int)\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].median()).apply(int)","05948f17":"# Drop non-numeric columns (Name, Ticket), columns with too many NA values (Cabin), and correlated columns (Fare)\ntrain_data = train_data.drop(columns = ['Cabin', 'Name', 'Ticket', 'Fare'])\ntest_data = test_data.drop(columns = ['Cabin', 'Name', 'Ticket', 'Fare'])","aef5bcd5":"# Adding sibling and parent column\ntrain_data['Family'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['Family'] = test_data['SibSp'] + test_data['Parch'] + 1","a1c2b38a":"train_data['Sex:Male'] = pd.get_dummies(train_data['Sex'])['male']\ntest_data['Sex:Male'] = pd.get_dummies(test_data['Sex'])['male']","7a05c6ff":"# Adding dummified columns\ntrain_data['Embarked:C'] = pd.get_dummies(train_data['Embarked'])['C']\ntrain_data['Embarked:S'] = pd.get_dummies(train_data['Embarked'])['S']\n# Same but to test data\ntest_data['Embarked:C'] = pd.get_dummies(test_data['Embarked'])['C']\ntest_data['Embarked:S'] = pd.get_dummies(test_data['Embarked'])['S']","e8c1d896":"columns_to_drop = ['Sex', 'SibSp', 'Parch', 'Embarked']\ntrain_data = train_data.drop(columns = columns_to_drop)\ntest_data = test_data.drop(columns = columns_to_drop)","7cb6796c":"train_data","c03edb66":"# Split X & y\nfrom sklearn.model_selection import train_test_split\nX = train_data.drop(columns = ['Survived'])\ny = train_data['Survived']\n\n# Train\/Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n","f76650a5":"correlation = X_train.corr()\nplt.figure(figsize = (8,8))\nsns.heatmap(correlation, square = True, cmap = 'mako').set(title='Correlation of Training Data Variables')","93a8da96":"# Scaling Data\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\ntest_data = scaler.transform(test_data)","b325f7d2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","adf00114":"# Fitting Logistic Regression\nlog_reg_model = LogisticRegression()\nlog_reg_model.fit(X_train, y_train)\n\n\n# Scoring\ntrain_prediction = log_reg_model.predict(X_train)\ntest_prediction = log_reg_model.predict(X_test)\naccuracy_train = accuracy_score(train_prediction, y_train)\naccuracy_test = accuracy_score(test_prediction, y_test)\n\nprint(f\"Score on training set: {accuracy_train}\")\nprint(f\"Score on test set: {accuracy_test}\")","712c3791":"from sklearn.tree import DecisionTreeClassifier\n\n# lists for scoring\ntraining_scores = []\ntest_scores = []\n\n# finding optimal depth\nfor i in range(1,15):\n    decision_tree = DecisionTreeClassifier(max_depth=i)\n    decision_tree.fit(X_train, y_train)\n    training_scores.append(decision_tree.score(X_train,y_train)*100)\n    test_scores.append(decision_tree.score(X_test,y_test)*100)\n\n# Plotting training and test scores against max_depth = i\nplt.figure()\n\nplt.plot(training_scores, label = 'Training Scores')\nplt.plot(test_scores, label = 'Test Scores')\nplt.xticks(range(1,15,1))\nplt.title('Relationship between training score and test score vs max_depth')\nplt.xlabel('Maximum Depth')\nplt.ylabel('Score')\nplt.legend()\n\nplt.show()\n\n# Print best test score:\nhighest_score = max(test_scores)\nhighest_index = test_scores.index(highest_score)\n\nprint(f\"Best Accuracy Score: Max Depth of {highest_index + 1}\")\nprint(f\"Training accuracy of {round(training_scores[highest_index], 2)}% (max_depth = {highest_index + 1})\")\nprint(f\"Test accuracy of {round(test_scores[highest_index],2)}% (max_depth = {highest_index + 1})\")","ec89ffaa":"decision_tree_3 = DecisionTreeClassifier(max_depth=3)\ndecision_tree_3.fit(X_train, y_train)\n","9d8b2edf":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n\nAB_model = AdaBoostClassifier()\nAB_model.fit(X_train, y_train)\n\ngrad_boost_model = GradientBoostingClassifier()\ngrad_boost_model.fit(X_train, y_train)\n\nXGB_model = XGBClassifier(\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\nXGB_model.fit(X_train, y_train)\n\n\nprint(\"==\"*20)\nprint(f\"AdaBoost Training score: {AB_model.score(X_train,y_train)}\")\nprint(f\"AdaBoost Testing score: {AB_model.score(X_test,y_test)}\")\nprint(\"==\"*20)\nprint(f\"Gradient Boosting Training score: {grad_boost_model.score(X_train,y_train)}\")\nprint(f\"Gradient Boosting Testing score: {grad_boost_model.score(X_test,y_test)}\")\nprint(\"==\"*20)\nprint(f\"XGBoost Training score: {XGB_model.score(X_train,y_train)}\")\nprint(f\"XGBoost Testing score: {XGB_model.score(X_test,y_test)}\")\n","b32feb80":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n\nAB_model = AdaBoostClassifier()\nAB_model.fit(X_train, y_train)\n\ngrad_boost_model = GradientBoostingClassifier()\ngrad_boost_model.fit(X_train, y_train)\n\nXGB_model = XGBClassifier()\nXGB_model.fit(X_train, y_train)\n\nprint(\"==\"*20)\nprint(f\"AdaBoost Training score: {AB_model.score(X_train,y_train)}\")\nprint(f\"AdaBoost Testing score: {AB_model.score(X_test,y_test)}\")\nprint(\"==\"*20)\nprint(f\"Gradient Boosting Training score: {grad_boost_model.score(X_train,y_train)}\")\nprint(f\"Gradient Boosting Testing score: {grad_boost_model.score(X_test,y_test)}\")\nprint(\"==\"*20)\nprint(f\"XGBoost Training score: {XGB_model.score(X_train,y_train)}\")\nprint(f\"XGBoost Testing score: {XGB_model.score(X_test,y_test)}\")\n","b58b6090":"pred = decision_tree_3.predict(test_data)\nsubmission_dict = {\"PassengerId\": passID, \"Survived\": pred }\nsubmission = pd.DataFrame(submission_dict)\nsubmission","4284299c":"submission.to_csv(\"titanic_submission.csv\", index = False)","7140146f":"pred = XGB_model.predict(test_data)\nsubmission_dict = {\"PassengerId\": passID, \"Survived\": pred }\nxg_submission = pd.DataFrame(submission_dict)\nxg_submission.to_csv(\"titanic_submission_xg.csv\", index = False)","48acc4d7":"Way too much overfitting.","9b865f94":"## Decision Trees","d2d0292a":"Taking a look at the data:","5e10e735":"## Test Submission","b70f707f":"# Predicting Titanic Survivors With Machine Learning\n\nIn this notebook, I will employ several classification models (logistic regression, decision trees, XGBoosting, AdaBoosting, Gradient Boosting) to predict passenger survival. Steps include:\n\n- EDA\n- Feature Engineering\n- Train\/Test split\n- Modelling","798b8d5a":"Some hyperparameter optimizations:","88080947":"## Boosting Algorithms","d8115415":"## Logistic Regression"}}