{"cell_type":{"592c0ff7":"code","952e423c":"code","69157641":"code","465a1d2c":"code","569fbf66":"code","d4fb83ba":"code","de01af13":"code","b2c41c64":"code","69be41c1":"code","9c75b17a":"code","6951982c":"code","5b2ba85f":"code","7ed184fa":"code","38a5268e":"code","af9c192d":"code","e054fe78":"code","f2b8b23d":"code","b97503b6":"code","17748ef1":"code","0b679d5b":"code","0cc64529":"code","7c71c00d":"code","65b4c139":"markdown","180dc0fd":"markdown","b5d08556":"markdown","3ac49690":"markdown","88744239":"markdown","25435b42":"markdown","a0337eb4":"markdown","ec84c3e3":"markdown","999c4414":"markdown","5a671f4a":"markdown","0d61dee6":"markdown","4eeaffbc":"markdown"},"source":{"592c0ff7":"import numpy as np\nimport pandas as pd\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt","952e423c":"class Config:\n    is_kaggle_platform = os.path.exists(\"\/kaggle\/input\")\n    dataset_name = \"tabular-playground-series-dec-2021\"\n    data_path = \"\/kaggle\/input\/%s\/\"%(dataset_name) if is_kaggle_platform else \"\"\n    submit_filename = \"submission.csv\"\n    label_name = \"Cover_Type\"\n    id_field = \"Id\"\nconfig = Config()","69157641":"if not config.is_kaggle_platform:\n  try:\n    import kaggle\n  except:\n    !pip install kaggle\n  if not os.path.exists(\"\/root\/.kaggle\/kaggle.json\"):\n    !echo \"{\"username\":\"{your username}\",\"key\":\"{your apikey}\"}\" >> \/root\/.kaggle\/kaggle.json\n    !chmod 600 \/root\/.kaggle\/kaggle.json\n  !kaggle competitions download -c $config.dataset_name\n  !unzip test.csv.zip\n  !unzip train.csv.zip\n  !unzip sample_submission.csv.zip","465a1d2c":"train = pd.read_csv(config.data_path + \"train.csv\")\ntest = pd.read_csv(config.data_path + \"test.csv\")\nsample_submission = pd.read_csv(config.data_path + \"sample_submission.csv\")","569fbf66":"train.head()","d4fb83ba":"train.info()","de01af13":"train.describe()","b2c41c64":"corr = train.corr()\ncorr","69be41c1":"corr.sort_values(ascending=False, inplace=True, by=config.label_name, key= lambda x: abs(x))\ncorr[config.label_name]","9c75b17a":"correlated_columns = corr[config.label_name][corr[config.label_name].abs() > 0.05].index\ncorrelated_columns, len(correlated_columns)","6951982c":"correlation_score = train.corr()\ncorrelated_features = correlation_score[config.label_name].sort_values(ascending=False).dropna()\ncorrelated_columns = list(correlated_features[correlated_features.abs() > 0.05].index)\ncorrelated_columns.remove(config.label_name)\nprint(correlated_columns)","5b2ba85f":"corr2 = train[correlated_columns].corr()\ncorr2","7ed184fa":"plt.figure(figsize=(20, 20))\nsns.heatmap(corr2, annot=True)","38a5268e":"sns.countplot(x=config.label_name, data=train)","af9c192d":"train[config.label_name].value_counts()","e054fe78":"train = train.drop(index = int(np.where(train[config.label_name] == 5)[0]))","f2b8b23d":"train.pop(config.id_field)\n_ = test.pop(config.id_field)\ntrain.pop(\"Soil_Type7\")\ntrain.pop(\"Soil_Type15\")\ntest.pop(\"Soil_Type7\")\n_ = test.pop(\"Soil_Type15\")","b97503b6":"null_counts = train.isnull().sum()\nprint(null_counts[null_counts > 0])\nnull_counts = test.isnull().sum()\nprint(null_counts[null_counts > 0])","17748ef1":"train_features, val_features = train_test_split(train, test_size=0.15, random_state=42)\ntrain_targets = train_features.pop(config.label_name)\nval_targets = val_features.pop(config.label_name)\ntrain_features.head()","0b679d5b":"cols = train_features.columns\nfor data in [train_features, val_features, test]:\n    data[\"mean\"] = data[cols].mean(axis=1)\n    data[\"min\"] = data[cols].min(axis=1)\n    data[\"max\"] = data[cols].max(axis=1)\n    data[\"std\"] = data[cols].std(axis=1)","0cc64529":"cat_params = {\n    'iterations': 20000,\n    'learning_rate': 0.03,\n    'od_wait': 1000,\n    'depth': 7,\n    'task_type' : 'GPU',\n    'l2_leaf_reg': 5,\n    'eval_metric': 'Accuracy',\n    'devices' : '0',\n    'verbose' : 1000\n}\ncat = CatBoostClassifier(**cat_params)\ncat.fit(train_features, train_targets, eval_set=(val_features, val_targets))\n","7c71c00d":"y_pred = cat.predict(test)\nsample_submission[config.label_name] = y_pred.reshape(-1)\nsample_submission.to_csv(config.submit_filename, index=False)\nif not config.is_kaggle_platform:\n  !kaggle competitions submit $config.dataset_name -m \"Submission\" -f $config.submit_filename","65b4c139":"\n#  TPS-12-21: Catboost\n## Table of Contents\n* [1. Overview](#1.)\n* [2. Setup](#2.)\n* [3. EDA & Preprocessing](#3.)\n\t* [3.1 Statistic Info](#3.1)\n\t* [3.2 Correlation Score](#3.2)\n\t* [3.3 Distribution of Label](#3.3)\n\t* [3.4 Drop unused columns](#3.4)\n    * [3.5 Data Wrangling](#3.5)\n\t* [3.6 Train Validation Split](#3.6)\n    * [3.7 Add new features](#3.7)\n* [4. Model Development](#4.)\n* [5. Submission](#5.)\n\n<a id=\"1.\"><\/a>\n## 1. Overview\nIn this Notebook, I will build a Tabular Prediciton Model using Catboost. \n\nThis dataset is generated by CTGAN learning from [Forest Cover Type Prediction dataset](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction), they have the same fields. I have built a notebook [Forest Cover Type Prediction with CatBoost](https:\/\/www.kaggle.com\/lonnieqin\/forest-cover-type-prediction-with-catboost) based on this notebook and they are working fine. If you are interested, you can have a look at it.\n<a id=\"2.\"><\/a>\n## 2. Setup","180dc0fd":"<a id=\"3.\"><\/a>\n## 3. EDA & Preprocessing","b5d08556":"<a id=\"5.\"><\/a>\n## 5. Submission","3ac49690":"<a id=\"3.2\"><\/a>\n### 3.2 Correlation Score","88744239":"<a id=\"3.6\"><\/a>\n### 3.6 Train Validation Split","25435b42":"<a id=\"3.3\"><\/a>\n### 3.3 Distribution of Label\nThere are only 1 sample for Label 5 and 377 samples for Label 4, which is so imbalanced. I am going to remove this sample from Label 5 for now. But it's meanful to study how to do data augmentation to this sample so that we can predict this type correctly, what if the private test set contains  a lot of Label 5.  ","a0337eb4":"**If you found my notebook useful, give me an upvote.**\n\nIf you are interested, You may have a look at some of my TPS notebooks before.\n\n- [Tabular Playground Series Prediction(Aug 2021)](https:\/\/www.kaggle.com\/lonnieqin\/tabular-playground-series-prediction)\n- [Tabular Playground Prediction(Sep 2021) with CatBoost](https:\/\/www.kaggle.com\/lonnieqin\/catboost-tabular-playground-prediction-sep-2021)\n- [Tabular Prediction(Oct 2021) with CatBoost](https:\/\/www.kaggle.com\/lonnieqin\/catboost-tabular-prediction-oct-2021)\n- [TPS Prediction with DNN and KerasTuner (Oct 2021)](https:\/\/www.kaggle.com\/lonnieqin\/tps-prediction-with-dnn-and-kerastuner-oct-2021)\n- [TPS-10-21: DNN](https:\/\/www.kaggle.com\/lonnieqin\/tps-10-21-dnn)\n","ec84c3e3":"<a id=\"3.1\"><\/a>\n### 3.1 Statistic Info","999c4414":"<a id=\"3.7\"><\/a>\n### 3.7 Add new features","5a671f4a":"<a id=\"3.5\"><\/a>\n### 3.5 Data Wrangling\nLuckily There isn't any missing value, we don't need to worry about Data Wrangling.","0d61dee6":"<a id=\"4.\"><\/a>\n## 4. Model Development","4eeaffbc":"<a id=\"3.4\"><\/a>\n### 3.4 Drop unused columns\nID is not needed. So remove this column. So is Soil_Type7 and Soil_Type15."}}