{"cell_type":{"efd49e66":"code","72368a44":"code","7b43b505":"code","581b392b":"code","488dc5f6":"code","9ea4ae4f":"code","74556b79":"code","5e54bf4a":"code","fb877100":"code","8973c5f2":"code","07283227":"code","c6dd6715":"code","34fd2f09":"code","88a5d8e2":"code","304cbe3b":"code","a6ab7969":"code","b9e185e5":"code","bb4bac70":"code","489f552b":"code","65f82531":"code","a13108ef":"code","f3993592":"code","3e389fc5":"code","b42f3a6e":"code","544e7325":"code","f987c1cb":"code","570446bc":"code","ac687f42":"code","f13327ae":"code","0328ee92":"code","1cc2c58e":"code","f4de7ebe":"code","af36996b":"code","cf3433cc":"code","c4961c71":"code","719412d6":"code","f7dcfd6c":"code","4ab53cf5":"code","ba211094":"code","c5b1345b":"code","17fb9f7d":"code","91b0aa40":"code","d1a1318d":"code","75c892d0":"code","a0466c29":"code","07e4552f":"code","9ffc7b57":"code","32422e79":"code","22ebfb02":"code","d76f0b91":"code","b208ee77":"code","f1279d3a":"code","97a4834e":"code","2e73a1eb":"code","1b9edaa7":"code","88705768":"code","01e1d81e":"code","5d8071f4":"code","50e57bb3":"code","b99fea10":"code","db27a959":"code","27544ec3":"code","e4cad0e6":"code","69269105":"code","9935ec7e":"code","aa2d5b38":"code","031fe979":"markdown","ee2a4f53":"markdown","db3a2702":"markdown","ce6b7041":"markdown","fa03409d":"markdown","9c38dec1":"markdown","887494c3":"markdown","9fc67dfd":"markdown","cc8fed12":"markdown","1874c5fe":"markdown","7eb10fa0":"markdown","473d9f09":"markdown","b5a4bb63":"markdown","230747f0":"markdown","7e02bc43":"markdown","8e51ea34":"markdown","6062a80d":"markdown","011f20fb":"markdown","306161ef":"markdown","fee57367":"markdown","c1a3dc6c":"markdown","e3e04b91":"markdown","9ea37a0d":"markdown","86b31d2f":"markdown","1a581bdd":"markdown","ea2f6493":"markdown","c179ff59":"markdown","64e2c5ba":"markdown","20af17d2":"markdown","93a92e9c":"markdown","8c3e4599":"markdown","28657a7a":"markdown"},"source":{"efd49e66":"#importing packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","72368a44":"#importing data from kaggle\ndf = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head(5)","7b43b505":"df = df.drop(\"Time\", axis=1)","581b392b":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()","488dc5f6":"#standard scaling\ndf['std_Amount'] = scaler.fit_transform(df['Amount'].values.reshape (-1,1))\n\n#removing Amount\ndf = df.drop(\"Amount\", axis=1)","9ea4ae4f":"sns.countplot(x=\"Class\", data=df)","74556b79":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler \n\nundersample = RandomUnderSampler(sampling_strategy=0.5)","5e54bf4a":"cols = df.columns.tolist()\ncols = [c for c in cols if c not in [\"Class\"]]\ntarget = \"Class\"\n","fb877100":"#define X and Y\nX = df[cols]\nY = df[target]\n\n# I would include a PCA convertion of the X variables; as there are some high correlation between some of the varibles\n# In that way we can avoid colineality:\n\nfrom sklearn.decomposition import PCA\npca=PCA(n_components=15, svd_solver='full')\npca = pca.fit_transform(X)\n\nJ = pd.DataFrame(pca)\n","8973c5f2":"#undersample\nX_under, Y_under = undersample.fit_resample(X, Y)\nX_pca, Y_pca = undersample.fit_resample(J, Y)","07283227":"from pandas import DataFrame\ntest = pd.DataFrame(Y_under, columns = ['Class'])","c6dd6715":"#visualizing undersampling results\nfig, axs = plt.subplots(ncols=2, figsize=(13,4.5))\nsns.countplot(x=\"Class\", data=df, ax=axs[0])\nsns.countplot(x=\"Class\", data=test, ax=axs[1])\n\nfig.suptitle(\"Class repartition before and after undersampling\")\na1=fig.axes[0]\na1.set_title(\"Before\")\na2=fig.axes[1]\na2.set_title(\"After\")","34fd2f09":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_under, Y_under, test_size=0.2, random_state=1)\n\n# Let's create a train_test_split for the data with PCA:\n\nX_trainP, X_testP, y_trainP, y_testP = train_test_split(X_pca, Y_pca, test_size=0.2, random_state=1)","88a5d8e2":"#importing packages for modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import BatchNormalization\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve","304cbe3b":"#train the model\nmodel1 = LogisticRegression(random_state=2)\nlogit = model1.fit(X_trainP, y_trainP)","a6ab7969":"#predictions\ny_pred_logit = model1.predict(X_testP) ","b9e185e5":"#scores\nprint(\"Accuracy Logit:\",metrics.accuracy_score(y_testP, y_pred_logit))\nprint(\"Precision Logit:\",metrics.precision_score(y_testP, y_pred_logit))\nprint(\"Recall Logit:\",metrics.recall_score(y_testP, y_pred_logit))\nprint(\"F1 Score Logit:\",metrics.f1_score(y_testP, y_pred_logit))","bb4bac70":"### Metrics had improved with PCA technique.","489f552b":"#print CM\nmatrix_logit = confusion_matrix(y_test, y_pred_logit)\ncm_logit = pd.DataFrame(matrix_logit, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_logit, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix Logit\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","65f82531":"#AUC\ny_pred_logit_proba = model1.predict_proba(X_test)[::,1]\nfpr_logit, tpr_logit, _ = metrics.roc_curve(y_test,  y_pred_logit_proba)\nauc_logit = metrics.roc_auc_score(y_test, y_pred_logit_proba)\nprint(\"AUC Logistic Regression :\", auc_logit)","a13108ef":"#ROC\nplt.plot(fpr_logit,tpr_logit,label=\"Logistic Regression, auc={:.3f})\".format(auc_logit))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Logistic Regression ROC curve')\nplt.legend(loc=4)\nplt.show()","f3993592":"logit_precision, logit_recall, _ = precision_recall_curve(y_test, y_pred_logit_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(logit_recall, logit_precision, color='orange', label='Logistic')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","3e389fc5":"#train the model\nmodel2 = SVC(probability=True, random_state=2)\nsvm = model2.fit(X_trainP, y_trainP)","b42f3a6e":"#predictions\ny_pred_svm = model2.predict(X_testP)","544e7325":"#scores\nprint(\"Accuracy SVM:\",metrics.accuracy_score(y_testP, y_pred_svm))\nprint(\"Precision SVM:\",metrics.precision_score(y_testP, y_pred_svm))\nprint(\"Recall SVM:\",metrics.recall_score(y_testP, y_pred_svm))\nprint(\"F1 Score SVM:\",metrics.f1_score(y_testP, y_pred_svm))","f987c1cb":"# PCA has improved SVM results.","570446bc":"#CM matrix\nmatrix_svm = confusion_matrix(y_testP, y_pred_svm)\ncm_svm = pd.DataFrame(matrix_svm, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_svm, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix SVM\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","ac687f42":"#AUC\ny_pred_svm_proba = model2.predict_proba(X_testP)[::,1]\nfpr_svm, tpr_svm, _ = metrics.roc_curve(y_testP,  y_pred_svm_proba)\nauc_svm = metrics.roc_auc_score(y_testP, y_pred_svm_proba)\nprint(\"AUC SVM :\", auc_svm)","f13327ae":"#ROC\nplt.plot(fpr_svm,tpr_svm,label=\"SVM, auc={:.3f})\".format(auc_svm))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('SVM ROC curve')\nplt.legend(loc=4)\nplt.show()","0328ee92":"svm_precision, svm_recall, _ = precision_recall_curve(y_test, y_pred_svm_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(svm_recall, svm_precision, color='orange', label='SVM')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","1cc2c58e":"#train the model\nmodel3 = RandomForestClassifier(random_state=2)\nrf = model3.fit(X_trainP, y_trainP)","f4de7ebe":"#predictions\ny_pred_rf = model3.predict(X_testP)","af36996b":"#scores\nprint(\"Accuracy RF:\",metrics.accuracy_score(y_testP, y_pred_rf))\nprint(\"Precision RF:\",metrics.precision_score(y_testP, y_pred_rf))\nprint(\"Recall RF:\",metrics.recall_score(y_testP, y_pred_rf))\nprint(\"F1 Score RF:\",metrics.f1_score(y_testP, y_pred_rf))","cf3433cc":"# PCA has improved the results of Bagging.","c4961c71":"#CM matrix\nmatrix_rf = confusion_matrix(y_testP, y_pred_rf)\ncm_rf = pd.DataFrame(matrix_rf, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_rf, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix RF\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","719412d6":"#AUC\ny_pred_rf_proba = model3.predict_proba(X_testP)[::,1]\nfpr_rf, tpr_rf, _ = metrics.roc_curve(y_testP,  y_pred_rf_proba)\nauc_rf = metrics.roc_auc_score(y_testP, y_pred_rf_proba)\nprint(\"AUC Random Forest :\", auc_rf)","f7dcfd6c":"#ROC\nplt.plot(fpr_rf,tpr_rf,label=\"Random Forest, auc={:.3f})\".format(auc_rf))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Random Forest ROC curve')\nplt.legend(loc=4)\nplt.show()","4ab53cf5":"rf_precision, rf_recall, _ = precision_recall_curve(y_testP, y_pred_rf_proba)\nno_skill = len(y_test[y_testP==1]) \/ len(y_testP)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(rf_recall, rf_precision, color='orange', label='RF')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","ba211094":"#train the model\nmodel4 = XGBClassifier(random_state=2)\nxgb = model4.fit(X_train, y_train)","c5b1345b":"#predictions\ny_pred_xgb = model4.predict(X_test) ","17fb9f7d":"#scores\nprint(\"Accuracy XGB:\",metrics.accuracy_score(y_test, y_pred_xgb))\nprint(\"Precision XGB:\",metrics.precision_score(y_test, y_pred_xgb))\nprint(\"Recall XGB:\",metrics.recall_score(y_test, y_pred_xgb))\nprint(\"F1 Score XGB:\",metrics.f1_score(y_test, y_pred_xgb))","91b0aa40":"#CM matrix\nmatrix_xgb = confusion_matrix(y_test, y_pred_xgb)\ncm_xgb = pd.DataFrame(matrix_xgb, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_xgb, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix XGBoost\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","d1a1318d":"#AUC\ny_pred_xgb_proba = model4.predict_proba(X_test)[::,1]\nfpr_xgb, tpr_xgb, _ = metrics.roc_curve(y_test,  y_pred_xgb_proba)\nauc_xgb = metrics.roc_auc_score(y_test, y_pred_xgb_proba)\nprint(\"AUC XGBoost :\", auc_xgb)","75c892d0":"#ROC\nplt.plot(fpr_xgb,tpr_xgb,label=\"XGBoost, auc={:.3f})\".format(auc_xgb))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('XGBoost ROC curve')\nplt.legend(loc=4)\nplt.show()","a0466c29":"xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, y_pred_xgb_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(xgb_recall, xgb_precision, color='orange', label='XGB')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","07e4552f":"#train the model\nmodel5 = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(100,100), random_state=2)\nmlp = model5.fit(X_train, y_train)","9ffc7b57":"model5.get_params(deep=True)","32422e79":"#predictions\ny_pred_mlp = model5.predict(X_test)","22ebfb02":"#scores\nprint(\"Accuracy MLP:\",metrics.accuracy_score(y_test, y_pred_mlp))\nprint(\"Precision MLP:\",metrics.precision_score(y_test, y_pred_mlp))\nprint(\"Recall MLP:\",metrics.recall_score(y_test, y_pred_mlp))\nprint(\"F1 Score MLP:\",metrics.f1_score(y_test, y_pred_mlp))","d76f0b91":"#CM matrix\nmatrix_mlp = confusion_matrix(y_test, y_pred_mlp)\ncm_mlp = pd.DataFrame(matrix_mlp, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_mlp, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix MLP\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","b208ee77":"#AUC\ny_pred_mlp_proba = model5.predict_proba(X_test)[::,1]\nfpr_mlp, tpr_mlp, _ = metrics.roc_curve(y_test,  y_pred_mlp_proba)\nauc_mlp = metrics.roc_auc_score(y_test, y_pred_mlp_proba)\nprint(\"AUC MLP :\", auc_mlp)","f1279d3a":"#ROC\nplt.plot(fpr_mlp,tpr_mlp,label=\"MLPC, auc={:.3f})\".format(auc_mlp))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Multilayer Perceptron ROC curve')\nplt.legend(loc=4)\nplt.show()","97a4834e":"mlp_precision, mlp_recall, _ = precision_recall_curve(y_test, y_pred_mlp_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(mlp_recall, mlp_precision, color='orange', label='MLP')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","2e73a1eb":"#train the model\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(29,), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","1b9edaa7":"opt = tf.keras.optimizers.Adam(learning_rate=0.001) #optimizer\n\nmodel.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy']) #metrics","88705768":"earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=15, verbose=1,mode='auto', baseline=None, restore_best_weights=False)","01e1d81e":"history = model.fit(X_train.values, y_train.values, epochs = 6, batch_size=5, validation_split = 0.15, verbose = 0,\n                    callbacks = [earlystopper])\nhistory_dict = history.history","5d8071f4":"loss_values = history_dict['loss']\nval_loss_values=history_dict['val_loss']\nplt.plot(loss_values,'b',label='training loss')\nplt.plot(val_loss_values,'r',label='val training loss')\nplt.legend()\nplt.xlabel(\"Epochs\")","50e57bb3":"accuracy_values = history_dict['accuracy']\nval_accuracy_values=history_dict['val_accuracy']\nplt.plot(val_accuracy_values,'-r',label='val_accuracy')\nplt.plot(accuracy_values,'-b',label='accuracy')\nplt.legend()\nplt.xlabel(\"Epochs\")","b99fea10":"#predictions\ny_pred_nn = model.predict_classes(X_test)","db27a959":"#scores\nprint(\"Accuracy Neural Net:\",metrics.accuracy_score(y_test, y_pred_nn))\nprint(\"Precision Neural Net:\",metrics.precision_score(y_test, y_pred_nn))\nprint(\"Recall Neural Net:\",metrics.recall_score(y_test, y_pred_nn))\nprint(\"F1 Score Neural Net:\",metrics.f1_score(y_test, y_pred_nn))","27544ec3":"#CM matrix\nmatrix_nn = confusion_matrix(y_test, y_pred_nn)\ncm_nn = pd.DataFrame(matrix_nn, index=['not_fraud', 'fraud'], columns=['not_fraud', 'fraud'])\n\nsns.heatmap(cm_nn, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix Neural Network\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","e4cad0e6":"#AUC\ny_pred_nn_proba = model.predict_proba(X_test)\nfpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test,y_pred_nn_proba)\nauc_keras = auc(fpr_keras, tpr_keras)\nprint('AUC Neural Net: ', auc_keras)","69269105":"#ROC\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Neural Net ROC curve')\nplt.legend(loc='best')\nplt.show()","9935ec7e":"nn_precision, nn_recall, _ = precision_recall_curve(y_test, y_pred_nn_proba)\nno_skill = len(y_test[y_test==1]) \/ len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', color='black', label='No Skill')\nplt.plot(nn_recall, nn_precision, color='orange', label='TF NN')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve')\nplt.legend()\nplt.show()","aa2d5b38":"# PCA has improved the results of Linear Regression, Suport Vector Machine and Bagging; making the new \n# winner Random Forest (Bagging) algorithm with an accuracy of 95,9%.\n# This is due to removing colinealities algorithms works better. \n# The \"bad\" thing of applying PCA is that we lose explainability of our model, but if we don't really need it\n# is a good option :D","031fe979":"Now we are ready for modeling ! Let's try to predict credit card frauds with different models. ","ee2a4f53":"# 3. Ensemble learning : Bagging (Random Forest)","db3a2702":"Classification metrics for SVM (rounded down) :\n- Accuracy : 0.94\n- F1 score : 0.92\n- AUC : 0.97","ce6b7041":"Classification metrics for XGBoost (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.93\n- AUC : 0.97","fa03409d":"# 4. Ensemble learning : Boosting (XGBoost)","9c38dec1":"### <center> How SVM works : <\/center>\n\n![](https:\/\/vitalflux.com\/wp-content\/uploads\/2020\/07\/Screenshot-2020-07-07-at-3.44.38-PM.png)\n\nSVM Classifier uses a technique called the kernel trick to transform the data and then based on these transformations it finds an optimal boundary (hyper-plane) between the possible outputs.\nSupport vector machines focus only on the points that are the most difficult to tell apart, whereas other classifiers pay attention to all of the points.","887494c3":"### <center> How Logistic regression works :<\/center>\n\n![](https:\/\/saedsayad.com\/images\/LogReg_1.png)\n\n    \n\nIn Logistic Regression, input values (X) are combined linearly using weights or coefficient values to predict an output value (y). \nA key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value. Logistic regression is a linear method, but the predictions are transformed using the logistic function.","9fc67dfd":"### Data processing and undersampling","cc8fed12":"# 1. Logistic Regression","1874c5fe":"The dataset is highly imbalanced ! \nIt's a big problem because classifiers will always predict the most common class without performing any analysis of the features and it will have a high accuracy rate, obviously not the correct one. To change that, I will proceed to random undersampling.  \n\nThe simplest undersampling technique involves randomly selecting examples from the majority class and deleting them from the training dataset. This is referred to as random undersampling.\n\nAlthough simple and effective, a limitation of this technique is that examples are removed without any concern for how useful or important they might be in determining the decision boundary between the classes. This means it is possible, or even likely, that useful information will be deleted.\n\n### <center>How undersampling works :<\/center>\n![](https:\/\/miro.medium.com\/max\/335\/1*YH_vPYQEDIW0JoUYMeLz_A.png)\n\n\nTo undersample, we can use the package imblearn with RandomUnderSampler function !","7eb10fa0":"## <center>Welcome on my notebook! <\/center>\n\nToday, our objective is to create the best classifier for credit car fraud detection. To do it, we'll compare classification models from different methods :\n* Logistic regression\n* Support Vector Machine\n* Bagging (Random Forest)\n* Boosting (XGBoost)\n* Neural Network (tensorflow\/keras)\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. I decided to proceed to an undersampling strategy to re-balance the class.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data.\n\nHere are some other notebooks that helped me build this one :\n- https:\/\/www.kaggle.com\/jdelamorena\/recall-97-by-using-undersampling-neural-network\n- https:\/\/www.kaggle.com\/marbatlle\/fraud-prediction-undersampling-and-svm\n\nIf you found this notebook helpful or you just liked it , some upvotes would be very appreciated !","473d9f09":"Now, let's have a look at the class :","b5a4bb63":"# 2. Support Vector Machine","230747f0":"# 6. Multilayer Neural Network with Tensorflow\/Keras","7e02bc43":"# <center>CREDIT CARD FRAUD DETECTION USING MACHINE LEARNING METHODS<\/center>\n\n <p style=\"color:blue;\">Nils LEFEUVRE - March 2020<\/p>","8e51ea34":"We need to standardize the 'Amount' feature before modelling. \nFor that, we use the StandardScaler function from sklearn. Then, we just have to drop the old feature :","6062a80d":"### Importing packages and data","011f20fb":"Classification metrics for Multi Layer Perceptron (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.94\n- AUC : 0.98","306161ef":"# <center>And the winner is...<\/center>","fee57367":"Great ! Our dataset is now perfectly balanced !\n\nThe last step before modelling is now to split the data intro train and test samples. The test set will be composed of 20% of the data.\n\nWe will use the train dataset to train our models and then evaluate them of the test set : \n![](https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2018\/08\/1-16.png)\n\nTo split the data, we can use train_test_split function from sklearn !","c1a3dc6c":"Classification metrics for Random Forest (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.93\n- AUC : 0.97","e3e04b91":"Time is not needed for classification so I simply remove the feature from the dataset :","9ea37a0d":"Now let's visualize our multilayer network. \n\nThe hidden layers are composed of an activation function called ReLU. It'is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. The last node has a sigmoid function that turns values to 0 or 1 (for binary classification).","86b31d2f":"### <center> How XGBoost works :<\/center>\n\n![](https:\/\/d1rwhvwstyk9gu.cloudfront.net\/2020\/02\/XG-Boost-FINAL-01.png)\n\nThe sequential ensemble methods, also known as \u201cboosting\u201d, creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence. The first model is built on training data, the second model improves the first model, the third model improves the second, and so on. ","1a581bdd":"# 5. Multi Layer Perceptron","ea2f6493":"### <center> How Neural network works : <\/center>\n\n<center><img src= \"https:\/\/victorzhou.com\/27cf280166d7159c0465a58c68f99b39\/network3.svg\">\n\nThe layers of a neural network are made of nodes. \nA node combines input from the data with a set of coefficients and bias, that either amplify or dampen that input, thereby assigning significance to inputs with regard to the task the algorithm is trying to learn. These input-weight products are summed and then the sum is passed through a node\u2019s so-called activation function, to determine whether and to what extent that signal should progress further through the network to affect the ultimate outcome, say, an act of classification. If the signals passes through, the neuron has been \u201cactivated.\u201d","c179ff59":"![](https:\/\/media.istockphoto.com\/vectors\/best-simple-champion-cup-winner-trophy-award-and-victory-vector-id1025281982?k=6&m=1025281982&s=612x612&w=0&h=Oos26qFlAJI7GG62Gs_k_aWRthTVJOsVKGx7AXVc6W8=)\n\n# <center> Neural Network !!!<\/center>\n\nThe multilayer neural network has the best performance according to our three most important classification metrics (Accuracy, F1-score and AUC). The Multi Layer Perceptron from sklearn is the one that minimizes the most the false negatives so I decided to keep this model to predict credit card frauds.\n\nAll the models can be improved by tuning hyper-parameters. Please upvote this notebook if you want me to update it with model tuning ! ","64e2c5ba":"![](https:\/\/www.howardbank.com\/sites\/default\/files\/HB-SecurityCenter-DebCredCardSecurity-WEB.jpg)","20af17d2":"Classification metrics for Neural Network (rounded down) :\n- Accuracy : 0.95\n- F1 score : 0.94\n- AUC : 0.98","93a92e9c":"### <center> How Random Forest works : <\/center>\n\n![](https:\/\/miro.medium.com\/max\/567\/1*Mb8awDiY9T6rsOjtNTRcIg.png)\n\n<center>\"A large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models\"<\/center>\n\nRandom forest consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model\u2019s prediction.","8c3e4599":"![r\u00e9seau final.png](attachment:d0166ebc-403f-4565-b302-6fdbe678d218.png)","28657a7a":"Classification metrics for Logistic Regression (rounded down) :\n- Accuracy : 0.94\n- F1 score : 0.92\n- AUC : 0.96"}}