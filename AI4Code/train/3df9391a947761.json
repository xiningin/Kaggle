{"cell_type":{"e5df6a31":"code","f49f8fda":"code","efe09967":"code","480305ef":"code","dd54a244":"code","f410e707":"code","ca1236bf":"code","9f2632e7":"code","df182cd3":"code","0f7e1700":"code","a99e7b01":"code","26162d92":"code","4bce8128":"code","95e107c5":"code","76e31b2e":"code","11155f8d":"code","5f0860bc":"code","7a2a7d1a":"code","edc76103":"code","364827e3":"code","6768b539":"code","ac513c93":"code","607531f9":"code","1c5b69e8":"code","0ff10b2c":"code","698c12ca":"code","4e6fbe17":"code","9fe71344":"code","e94d623c":"code","8882c711":"markdown","399b0abc":"markdown","d515d45a":"markdown","ea0a6c9d":"markdown","e109508e":"markdown","3db35086":"markdown"},"source":{"e5df6a31":"\n\n# Load packages\nimport numpy as np                               # vectors and matrices\nimport pandas as pd                              # tables and data manipulations\nimport matplotlib.pyplot as plt                  # plots\nimport seaborn as sns                            # more plots\nimport warnings                                  # do not disturbe mode\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n\nfrom dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\n\n# Importing everything from forecasting quality metrics\nfrom sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\nfrom sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error","f49f8fda":"from dateutil.relativedelta import relativedelta # working with dates with style\nfrom scipy.optimize import minimize              # for function minimization\n\nimport statsmodels.formula.api as smf            # statistics and econometrics\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nfrom itertools import product                    # some useful functions\nfrom tqdm import tqdm_notebook\nimport matplotlib\nmatplotlib.rcParams['axes.labelsize']=14\nmatplotlib.rcParams['xtick.labelsize']=12\nmatplotlib.rcParams['ytick.labelsize']=12\nmatplotlib.rcParams['text.color']='k'\n\n\n","efe09967":"# MAPE\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n    \ndef tsplot(y, lags=None, figsize=(12, 7), style='bmh'):\n    \"\"\"\n        Plot time series, its ACF and PACF, calculate Dickey\u2013Fuller test\n        \n        y - timeseries\n        lags - how many lags to include in ACF, PACF calculation\n    \"\"\"\n    \n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n        \n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        layout = (2, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n        y.plot(ax=ts_ax)\n        p_value = sm.tsa.stattools.adfuller(y)[1]\n        ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n        plt.tight_layout()","480305ef":"ads = pd.read_csv(\"..\/input\/population-time-series-data\/POP.csv\", index_col=['date'], parse_dates=['date'])\n\n","dd54a244":"ads.head()","f410e707":"col=[\"realtime_start\", \"realtime_end\"]\nads.drop(col, axis=1,inplace=True)","ca1236bf":"ads.head()","9f2632e7":"ads.shape\nads.describe()","df182cd3":"ads.info()","0f7e1700":"#No messing value\nads.isnull().sum()","a99e7b01":"plt.figure(figsize=(18, 6))\nplt.plot(ads)\nplt.title(\"Montly Value\")\nplt.show()","26162d92":"\n#There is a strengh Trend\ntsplot(ads.value,lags=10)\n\n","4bce8128":"ads_log=np.log(ads.value)\ntsplot(ads_log,lags=60)","95e107c5":"# The daily difference\nads_log_diff = ads_log - ads_log.shift(24)\ntsplot(ads_log_diff[24:], lags=60)","76e31b2e":"ads_log_diff = ads_log_diff - ads_log_diff.shift(1)\ntsplot(ads_log_diff[24+1:], lags = 60)","11155f8d":"from pylab import rcParams\nrcParams['figure.figsize'] = 18, 8\ndecomposition = sm.tsa.seasonal_decompose(ads, model='additive')\nfig = decomposition.plot()\nplt.show()","5f0860bc":"# setting initial values and some bounds for them\nps = range(2, 5)\nd=1 #first Differencing\nqs = range(2, 5)\nPs = range(0, 2)\nD=1 #second Differecing\nQs = range(0, 2)\ns = 24 # season length is still 24\n\n# creating list with all the possible combinations of parameters\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)#List of ARIMA Parameter\nlen(parameters_list)#Lenght of the List\n\n\ndef optimizeSARIMA(y, parameters_list, d, D, s):\n    \"\"\"Return dataframe with parameters and corresponding AIC\n        \n        y - time series\n        parameters_list - list with (p, q, P, Q) tuples\n        d - integration order in ARIMA model\n        D - seasonal integration order \n        s - length of season\n    \"\"\"\n    \n    results = []\n    best_aic = float(\"inf\")\n\n    for param in tqdm_notebook(parameters_list):\n        # we need try-except because on some combinations model fails to converge\n        try:\n            model=sm.tsa.statespace.SARIMAX(y, order=(param[0], d, param[1]), \n                                            seasonal_order=(param[2], D, param[3], s)).fit(disp=-1)\n        except:\n            continue\n        aic = model.aic\n        # saving best model, AIC and parameters\n        if aic < best_aic:\n            best_model = model\n            best_aic = aic\n            best_param = param\n        results.append([param, model.aic])\n\n    result_table = pd.DataFrame(results)\n    result_table.columns = ['parameters', 'aic']\n    # sorting in ascending order, the lower AIC is - the better\n    result_table = result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n    \n    return result_table\n\n","7a2a7d1a":"%%time\nwarnings.filterwarnings(\"ignore\") \nresult_table = optimizeSARIMA(ads, parameters_list, d, D, s)","edc76103":"#Parameter and AIC\nresult_table.head()\nmin(result_table.aic)","364827e3":"# set the parameters that give the lowest AIC\n#The Best Model has the parameter (2,1,2)(0,1,1,24)\np, q, P, Q = result_table.parameters[0]\n\nbest_model=sm.tsa.statespace.SARIMAX(ads.value, order=(p, d, q), \n                                        seasonal_order=(P, D, Q, s)).fit(disp=-1)\nprint(best_model.summary())","6768b539":"mod=sm.tsa.statespace.SARIMAX(ads, order=(3, 1, 2), \n                                        seasonal_order=(0, 1, 1, 24),\n                             enforce_stationarity=False,\n                             enforce_invertibility=False)\n\n#Fitting the model\nresult=mod.fit()","ac513c93":"print(result.summary().tables[1])","607531f9":"result.plot_diagnostics(figsize=(16, 8))\nplt.show()","1c5b69e8":"\ntsplot(best_model.resid[24+1:], lags=60)","0ff10b2c":"def plotSARIMA(series, model, n_steps):\n    \"\"\"Plots model vs predicted values\n        \n        series - dataset with timeseries\n        model - fitted SARIMA model\n        n_steps - number of steps to predict in the future    \n    \"\"\"\n    \n    # adding model values\n    data = series.copy()\n    data.columns = ['actual']\n    data['sarima_model'] = model.fittedvalues\n    # making a shift on s+d steps, because these values were unobserved by the model\n    # due to the differentiating\n    data['sarima_model'][:s+d] = np.NaN\n    \n    # forecasting on n_steps forward \n    forecast = model.predict(start = data.shape[0], end = data.shape[0]+n_steps)\n    forecast = data.sarima_model.append(forecast)\n    # calculate error, again having shifted on s+d steps from the beginning\n    error = mean_absolute_percentage_error(data['actual'][s+d:], data['sarima_model'][s+d:])\n\n    plt.figure(figsize=(15, 7))\n    plt.title(\"Mean Absolute Percentage Error: {0:.2f}%\".format(error))\n    plt.plot(forecast, color='r', label=\"model\")\n    plt.axvspan(data.index[-1], forecast.index[-1], alpha=0.5, color='lightgrey')\n    plt.plot(data.actual, label=\"actual\")\n    plt.legend()\n    ","698c12ca":"#Forecast for 120 Day\nplotSARIMA(ads, best_model, 120)","4e6fbe17":"\npred = result.get_prediction(start=pd.to_datetime('2010-01-01'),dynamic=True)\npred_ci= pred.conf_int()\nax = ads['1990':].plot(label='observed')\npred.predicted_mean.plot(ax = ax, label='Forecast', alpha=.7, figsize=(14, 7))\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\n\nax.set_xlabel('Date ')\nax.set_ylabel('value of Pop')\nplt.legend()\nplt.show()","9fe71344":"\npred_u=result.get_forecast(steps=120)\npred_ci= pred_u.conf_int()\nax = ads.plot(label='observed',figsize=(14, 7))\npred_u.predicted_mean.plot(ax = ax, label='Forecast')\n\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\n\nax.set_xlabel('Date ')\nax.set_ylabel('value of Pop')\nplt.legend()\nplt.show()","e94d623c":"#predict Value\npred_ci","8882c711":"## [Work-Source](https:\/\/github.com\/Alro10\/deep-learning-time-series\/blob\/master\/notebooks\/SARIMA.ipynb)","399b0abc":"### We can use decomposition Method to plot distinct the tree component of the Time Serie: Trend, Saisonality and noise\nwe cann see that, there is no saisonality","d515d45a":"### Function the search the parameters q,p,Q,P for SARIMA-Model","ea0a6c9d":"### Forecasts visualizition","e109508e":"## The Data\n### we are using Population Time Series Data\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.","3db35086":"## Model diagnostic to investigate any ununsual issues\n#### The Residual has a normal Distribution, that mean that our Model is god"}}