{"cell_type":{"d9b423bb":"code","4edef9f7":"code","8190eabc":"code","ffc5fdab":"code","00c44f65":"code","5d44cba4":"code","2b2a08dd":"code","39f03145":"code","66055817":"code","77e303a8":"code","9fdca7e4":"code","599e6619":"code","814a895d":"code","ef8109e3":"code","07f50e2c":"code","1cccfd6e":"code","9827507f":"code","84f11522":"code","f776ed0b":"markdown","96d25842":"markdown","22ca846d":"markdown","591a54e7":"markdown","6e047dcf":"markdown","dff8b4c6":"markdown"},"source":{"d9b423bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4edef9f7":"import os\ndef log(*args):\n    os.system(f'echo \\\"{args}\\\"')\n    print(*args)","8190eabc":"! pip install --upgrade pip\n! pip install tensorflow_datasets","ffc5fdab":"import os\nimport pickle\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom joblib import dump, load\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator,RegressorMixin\nfrom sklearn.preprocessing import PolynomialFeatures\nimport multiprocessing as mp\nimport gc\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_regression\nimport tensorflow as tf\nimport lightgbm as lgb\nimport tensorflow_datasets.public_api as tfds","00c44f65":"np.random.seed(12345)\ntf.random.set_seed(12345)\nencoders = pickle.load(open('\/kaggle\/input\/data-preparation\/encoders.pkl','rb'))\ncolumns = pickle.load(open('\/kaggle\/input\/data-preparation\/columns.pkl','rb'))\nmax_day = 1941","5d44cba4":"# We will build one model for each store\nprint(columns[2]) # The store id is the third columns\ngroup_index = 2   # We will group our data by this column\nn_groups = len(encoders['store_id'].classes_)\nkeys = list(range(n_groups))\nencoders['store_id'].classes_","2b2a08dd":"log(columns)\nlog(columns[3]) # We retrieve our target column\ntarget_col = 3","39f03145":"# This function is just a wrapper to a function that calls add_time_steps\ndef preprocess(lookback, delay, target_col=target_col ,lookback_step=1,test=False,val_days=0,return_key=False):\n    # It takes as input a single time series data and applies some transformations to add time step\n    def fn(inputs):\n        # Remember _generate_examples of MyFullDataset we yielded a dictionary containing key and input\n        values = inputs['input'] \n        key = inputs['key'] \n        return add_time_steps((key,values),\n                              lookback,delay,\n                              lookback_step=lookback_step,\n                              target_col=target_col,\n                              val_days=val_days,\n                              test=test,\n                              return_key=return_key)\n    return fn\n   \n# This function takes an item with all it 1941 days salles, prices, and calendar data adds lookback \n# and generate the inputs and targets. \ndef add_time_steps(inputs, lookback, delay,target_col=target_col,test=False,lookback_step=1,val_days=0,return_key=False):\n    key,values = inputs\n    max_index = values.shape[0] - 1\n    min_index = 0\n    y=None\n    idx = tf.signal.frame(tf.range(min_index,max_index),lookback,lookback_step)\n    if not test:\n        idx = idx[tf.reduce_all(idx+delay <= max_index,axis=-1)]\n        if val_days:\n            val_idx = idx[-val_days:]\n            val_y_idx = val_idx[...,-1]+delay\n            y_val = tf.gather(values, val_y_idx)[...,target_col]\n            X_val = tf.gather(values, val_idx)\n            # remove val_days from training\n            idx = idx[:-val_days]\n        y_idx = idx[...,-1]+delay\n        y = tf.gather(values, y_idx)[...,target_col]\n    else:\n        idx = idx[-delay:]\n    X = tf.gather(values, idx)\n    if not test and return_key:\n        return (key,(X,y),(X_val,y_val))\n    return ((X,y),(X_val,y_val)) if not test else (key,X)","66055817":"# We need again this class\nclass MyFullDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'train',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        shape = (max_day,len(columns))\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"input\": tfds.features.Tensor(shape=shape,dtype=tf.float32),\n                \"key\": tfds.features.Tensor(shape=(),dtype=tf.int32),\n            }),\n        )\n    \n   \n    def _generate_examples(self,**args):\n        # We no longer need this function because we already build our dataset\n        pass","77e303a8":"log(\"Download dataset\")\nds_path = '\/kaggle\/input\/data-preparation\/'\nbuilder = MyFullDataset(data_dir=ds_path)\ndataset_ = builder.as_dataset()[f'train'].repeat()\nbuilder.download_and_prepare()\nlog(builder.info)","9fdca7e4":"total_num_examples = 30490\nval_days = 28*2\nlookback = 1\nlookback_step = 1\ndelay = 28\ngpu = False","599e6619":"def getData(key):\n    preprocessor = preprocess(lookback, delay,lookback_step=lookback_step,val_days=val_days)\n    fn_key_filter = lambda item: item['input'][0][group_index]==key\n    dataset = dataset_.take(total_num_examples).filter(fn_key_filter)\n    dataset = dataset.map(preprocessor,num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    dataset = dataset.batch(3049)\n    for (X_train, y_train),(X_val, y_val) in dataset:\n        log(\"X_train : \",X_train.shape,'; y_train : ',y_train.shape)\n        log(\"X_val : \",X_val.shape,'; y_train : ',y_val.shape)\n        X_train, y_train = X_train.numpy(), y_train.numpy()\n        X_val, y_val = X_val.numpy(), y_val.numpy()\n    del dataset\n    tf.keras.backend.clear_session()\n    gc.collect()\n    return key,(X_train, y_train),(X_val, y_val)\n    \ndef train(key,nthread=1,save=True,verbose_eval=True,gpu=False):\n    train_(getData(key),nthread=nthread,save=save,verbose_eval=verbose_eval,gpu=gpu)\n    \ndef train_(input_,nthread=1,save=True,verbose_eval=False,gpu=False):\n    key,(X_train, y_train),(X_val, y_val) = input_\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'poisson',# regression\n        'metric':{'rmse',},\n        'tree': 'feature_parallel',\n        'num_leaves':1000,\n        'learning_rate': 0.02,\n        'feature_fraction': 0.5,\n        'bagging_fraction': 0.8,\n        \"min_data_in_leaf\":1000,\n        'bagging_freq': 9,\n        \"bagging_seed\" : 1234,\n        'seed':1234,\n        'verbosity': 1,\n    }\n    if gpu:\n        params['device']='gpu'\n        params['gpu_platform_id']=0\n        params['gpu_device_id']=0\n    else:\n        params['nthread'] = nthread\n        \n    path = f'regressor-{key}.model'\n    pipe = Pipeline([\n        ('kbest',SelectKBest(f_regression, k=15)),\n        #('poly',PolynomialFeatures(degree=2,include_bias=True,interaction_only=True,)),\n        ('scaller',StandardScaler()),\n    ])\n     \n    # Let correct the shape\n    X_train = X_train.reshape(-1,X_train.shape[-1]*lookback)\n    y_train = y_train.reshape((-1,))\n    idx = np.arange(X_train.shape[0])\n    # We shuffle\n    np.random.shuffle(idx)\n    X_train,y_train = X_train[idx],y_train[idx]\n    log(\"X_train : \",X_train.shape)\n    X_train = pipe.fit_transform(X_train,y_train)\n    log(\"X_train : \",X_train.shape)\n    #log(\"Scores : \",pipe.named_steps['kbest'].scores_)\n    \n    # No need to shuffle validation data\n    X_val = X_val.reshape(-1,X_val.shape[-1]*lookback)\n    y_val = y_val.reshape((-1,))\n    X_val = pipe.transform(X_val)\n    log(\"X_val : \",X_val.shape)\n    \n    # Training\n    train_data = lgb.Dataset(X_train, label=y_train)\n    test_data = lgb.Dataset(X_val, label=y_val)\n    model = lgb.train(params, \n                      train_data,\n                      valid_sets=[test_data],\n                      num_boost_round=3000,\n                      early_stopping_rounds=100,\n                      verbose_eval=verbose_eval)\n    \n    mse = mean_squared_error(y_train,model.predict(X_train,num_iteration_predict=model.best_iteration))\n    val_mse = mean_squared_error(y_val,model.predict(X_val,num_iteration_predict=model.best_iteration))\n    log(f'Key {key};  MSE: {mse}, VAL_MSE: {val_mse}')\n    if not save:\n        return (key,model)\n    else:\n        with open(path,'wb') as f:\n            dump({\n                'model':model,\n                'num_iteration_predict':model.best_iteration,\n                'pipe':pipe\n            },f,compress=9)\n            \n        del model\n    gc.collect()","814a895d":"for key in keys:\n    train(key,nthread=-1,verbose_eval=True,gpu=gpu)\n    tf.keras.backend.clear_session()\n    gc.collect()","ef8109e3":"gc.collect()","07f50e2c":"log('Loading models')\nmodels = {}\nfor key in keys:\n    path = f'regressor-{key}.model'\n    models[key] = load(path)","1cccfd6e":"# Evaluate our validation data with the leaderboard\nval_days = 28\ndf_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv',index_col=0)\ndf_val.iloc[total_num_examples:] = 0 # set evaluation to 0\n    \npreprocessor = preprocess(lookback, delay,lookback_step=lookback_step,val_days=val_days,return_key=True)\ndataset = dataset_.take(total_num_examples)\ndataset = dataset.map(preprocessor,num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(1000)\nc = 1  \nfor idx,_,(X_val, y_val) in dataset:\n    idx = idx.numpy()\n    X_val, y_val = X_val.numpy(), y_val.numpy()\n    X_val = X_val.reshape(-1,X_val.shape[-1]*lookback)\n    \n    group_key = X_val[0][group_index]\n    pipe = models[group_key]['pipe']\n    model = models[group_key]['model']\n    num_iteration_predict = models[group_key]['num_iteration_predict']\n    X_val = pipe.transform(X_val)\n    df_val.iloc[idx] = model.predict(X_val,num_iteration_predict=num_iteration_predict).flatten()\n    print(f'{c}\\r',end='')\n    c += 1\ndf_val.to_csv('validation_submission.csv')\ndel dataset\ngc.collect()","9827507f":"def generate_submission(models,group_index, lookback, delay, lookback_step,xgb=False,out_path='submission.csv'):\n    df_sub = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv',index_col=0)\n    df_sub.iloc[:total_num_examples,:] = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv',index_col=0).iloc[:,-28:].values\n    df_sub.iloc[total_num_examples:] = 0\n    \n    preprocessor = preprocess(lookback, delay,lookback_step=lookback_step,val_days=val_days,test=True)\n    dataset = dataset_.take(total_num_examples)\n    dataset = dataset.map(preprocessor,num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(1000)\n    \n    c = 1\n    for idx,X in dataset:\n        idx = idx.numpy()\n        X = X.numpy()\n        X = X.reshape(-1,X.shape[-1]*lookback)\n        group_key = X[0][group_index]\n        pipe = models[group_key]['pipe']\n        model = models[group_key]['model']\n        num_iteration_predict = models[group_key]['num_iteration_predict']\n        X = pipe.transform(X)\n        df_sub.iloc[total_num_examples+idx] = model.predict(X,num_iteration_predict=num_iteration_predict).flatten()\n        print(f'{c}\\r',end='')\n        c += 1\n    df_sub.to_csv(out_path)","84f11522":"log('Generate submission')\ngenerate_submission(models,group_index, lookback, delay,lookback_step)","f776ed0b":"### Train","96d25842":"### Submission","22ca846d":"### Let define some utils","591a54e7":"We will use different split of the dataset we've prepared this kernel, to build boosting models","6e047dcf":"### We install the TFDS library and import required modules:","dff8b4c6":"## Prepare for training"}}