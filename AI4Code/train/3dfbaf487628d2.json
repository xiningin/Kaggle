{"cell_type":{"4e996376":"code","05a9cb4f":"code","e0964718":"code","72748145":"code","0ceb8f93":"code","bac203fc":"code","0590ec6a":"code","47448a91":"code","0d8a0dc8":"code","dcfa362d":"code","086ff77a":"code","2ede74fa":"code","6d50a2b0":"code","a8651cc0":"code","0d40eac4":"code","880937a2":"code","dc5a83b9":"code","5e79ac49":"code","375bf847":"code","d7bc7af5":"code","6653fcb7":"code","f13acf0e":"code","e0c607e0":"code","9ec19e8a":"code","bd3378d7":"markdown","c987d3bd":"markdown","dd30a014":"markdown","a23a1724":"markdown","354eb80f":"markdown","f18ea971":"markdown","8c345e48":"markdown","97543c87":"markdown","0f23e429":"markdown","c4a93a47":"markdown","f61a2052":"markdown","451efec9":"markdown","eee08a6f":"markdown","843752a5":"markdown","ad023696":"markdown","a9f88655":"markdown","858d11ce":"markdown","f08d62a8":"markdown","cb9dbd12":"markdown"},"source":{"4e996376":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\ntrain = pd.read_csv(\"..\/input\/bike-sharing-demand\/train.csv\")\ntest = pd.read_csv(\"..\/input\/bike-sharing-demand\/test.csv\")\ntrain.head()","05a9cb4f":"train.info()","e0964718":"test.info()","72748145":"train.describe()","0ceb8f93":"test.describe()","bac203fc":"for df in [train, test]:\n    df[\"datetime\"] =  pd.DatetimeIndex(df[\"datetime\"])\n    df[\"hour\"] = [x.hour for x in df[\"datetime\"]]\n    df[\"weekday\"] = [x.dayofweek for x in df[\"datetime\"]]\n    df[\"month\"] = [x.month for x in df[\"datetime\"]]\n    df[\"year\"] = [x.year for x in df[\"datetime\"]]\n    df['year_season'] = df['year'].astype(str) + \"_\" +  df['season'].astype(str) \n    df[\"year\"] = df[\"year\"].map({2011:1, 2012:0})\n    df.drop('datetime',axis=1,inplace=True)","0590ec6a":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(15,10))\nplt.suptitle('variables distribution')\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\nfor i, col in enumerate(train.columns[:11]):\n    plt.subplot(3,4,i+1)\n    if str(train[col].dtypes)[:3]=='int':\n        if len(train[col].unique()) > 5:\n            sns.distplot(train[col])\n        else:\n            sns.countplot(train[col])\n    else:\n        sns.distplot(train[col])\n    plt.ylabel(col)","47448a91":"plt.figure(figsize=(13,20))\nplt.suptitle('casual vs registered vs count')\nplt.subplots_adjust(hspace = 0.5, wspace = 0.3)\ncol_list = [\"season\",\"holiday\",\"workingday\",\"weather\",\"year\",\"year_season\",\"month\",\"weekday\",\"hour\"]\ncount_list = [\"casual\",\"registered\",\"count\"]\n\nfor i, col in enumerate(col_list):\n    for j, con in enumerate(count_list):\n        plt.subplot(9,3,3*i+j+1)\n        sns.barplot(train[col],train[con])","0d8a0dc8":"plt.figure(figsize=(15,6))\nplt.subplot(121)\nsns.barplot(x=\"weekday\", y=\"casual\", hue=\"workingday\", data=train)\nplt.subplot(122)\nsns.barplot(x=\"weekday\", y=\"registered\", hue=\"workingday\", data=train)","dcfa362d":"train.head()","086ff77a":"plt.figure(figsize=(18,11))\nplt.subplot(221)\nsns.pointplot(x=\"hour\", y=\"casual\", hue=\"workingday\", data=train)\nplt.subplot(222)\nsns.pointplot(x=\"hour\", y=\"casual\", hue=\"holiday\", data=train)\nplt.subplot(223)\nsns.pointplot(x=\"hour\", y=\"registered\", hue=\"workingday\", data=train)\nplt.subplot(224)\nsns.pointplot(x=\"hour\", y=\"registered\", hue=\"holiday\", data=train)\n# train.pivot_table(index=\"hour\", columns=\"workingday\", aggfunc=\"size\")","2ede74fa":"plt.figure(figsize=(11,11))\nsns.heatmap(train.corr(),annot=True,cmap=\"Blues\")","6d50a2b0":"for i, df in enumerate([train,test]):\n    plt.subplot(1,2,i+1)\n    sns.scatterplot(x = 'temp', y = 'atemp',data = df)","a8651cc0":"df_list = {\"train\":None, \"test\" : None}\nfor name, df in zip(df_list.keys(),[train, test]):\n    df['windspeed'] = np.log(df['windspeed']+1)\n    df[\"weekday_working\"] = df[\"weekday\"]*df[\"workingday\"]\n    df[\"weekday_holiday\"] = df[\"weekday\"]*df[\"holiday\"]\n    df['casual_workhour'] = df[['hour', 'workingday']].apply(lambda x: int(x['workingday'] == 0 and 10 <= x['hour'] <= 19), axis=1)\n    df['casual_holi_hour'] = df[['hour', 'holiday']].apply(lambda x: int(x['holiday'] == 1 and 9 <= x['hour'] <= 22), axis=1)\n    df['register_workhour'] = df[['hour', 'workingday']].apply(\n      lambda x:int((x['workingday'] == 1 and (6 <= x['hour'] <= 8 or 17 <= x['hour'] <= 20))\n        or (x['workingday'] == 0 and 10 <= x['hour'] <= 15)), axis=1)\n    df['register_holi_hour'] = df[['hour', 'holiday']].apply(\n      lambda x:int(x['holiday'] == 0 and (7 <= x['hour'] <= 8 or 17 <= x['hour'] <= 18)), axis=1)\n    df.drop('atemp',axis=1,inplace=True)\nby_season = train.groupby('year_season')[['count']].median()\nby_season.columns = ['count_season']\ntrain1 = train.join(by_season, on='year_season').drop('year_season',axis=1)\ntest1 = test.join(by_season, on='year_season').drop('year_season',axis=1)","0d40eac4":"from sklearn.model_selection import train_test_split\ny_list = [\"casual\",\"registered\",\"count\"]\ntrain_x = train1[[col for col in train1.columns if col not in ['casual','registered', 'count']]]\ntrain_y = np.log(train1[y_list]+1)","880937a2":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 123)\nrms1,rms2 = [],[]\nmodels1,models2 = [], []\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train1)) :\n    x_train, y_train = train_x.ix[trn_idx], train_y.ix[trn_idx] \n    x_val, y_val = train_x.ix[val_idx], train_y.ix[val_idx]\n    \n    lgb_param = {'boosting_type':'gbdt',\n             'num_leaves': 45,\n             'max_depth': 30,\n            'learning_rate': 0.01, \n            'bagging_fraction' : 0.9,\n            'bagging_freq': 20,\n            'colsample_bytree': 0.9,\n             'metric': 'rmse',\n            'min_child_weight': 1,\n            'min_child_samples': 10,\n             'zero_as_missing': True,\n            'objective': 'regression',\n            }\n    train_set1 = lgb.Dataset(x_train, y_train[\"registered\"], silent=False)\n    valid_set1 = lgb.Dataset(x_val, y_val[\"registered\"], silent=False)\n    lgb_model1 = lgb.train(params = lgb_param, train_set = train_set1 , num_boost_round=5000, early_stopping_rounds=100,verbose_eval=500, valid_sets=valid_set1)\n    train_set2 = lgb.Dataset(x_train, y_train[\"casual\"], silent=False)\n    valid_set2 = lgb.Dataset(x_val, y_val[\"casual\"], silent=False)\n    lgb_model2 = lgb.train(params = lgb_param, train_set = train_set2 , num_boost_round=5000, early_stopping_rounds=100,verbose_eval=500, valid_sets=valid_set2)\n    models1.append(lgb_model1)\n    models2.append(lgb_model2)","dc5a83b9":"tmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': lgb_model1.feature_importance()})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (15,15))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)","5e79ac49":"preds = []\nfor model in models1:\n    regi_pred = model.predict(test1)\n    preds.append(regi_pred)\nfin_casual = np.mean(preds, axis=0)\n\npreds = []\nfor model in models2:\n    casual_pred = model.predict(test1)\n    preds.append(casual_pred)\nfin_regi = np.mean(preds, axis=0)\ncount_pred1 = np.exp(fin_casual) + np.exp(fin_regi) - 2","375bf847":"from sklearn.model_selection import KFold \nfrom sklearn.metrics import mean_squared_error\ndef lgb_cv(num_leaves, learning_rate, n_estimators, reg_alpha, reg_lambda, min_split_gain, min_child_weight,min_child_samples, colsample_bytree, x_data=None, y_data=None, n_splits=5, output='score'):\n    score = 0\n    kf = KFold(n_splits=n_splits)\n    models = []\n    for train_index, valid_index in kf.split(x_data):\n        x_train, y_train = x_data.iloc[train_index], y_data[train_index]\n        x_valid, y_valid = x_data.iloc[valid_index], y_data[valid_index]\n        \n        model = lgb.LGBMRegressor(\n            num_leaves = int(num_leaves), \n            learning_rate = learning_rate, \n            n_estimators = int(n_estimators), \n            reg_alpha = reg_alpha, \n            reg_lambda = reg_lambda,\n            min_split_gain= min_split_gain,\n            min_child_weight = min_child_weight,\n            min_child_samples = int(min_child_samples),\n            colsample_bytree = np.clip(colsample_bytree, 0, 1), \n        )\n        \n        model.fit(x_train, y_train)\n        models.append(model)\n        \n        pred = model.predict(x_valid)\n        true = y_valid\n        score -= mean_squared_error(true, pred)\/n_splits\n    \n    if output == 'score':\n        return score\n    if output == 'model':\n        return models","d7bc7af5":"from functools import partial \nfrom bayes_opt import BayesianOptimization\nfunc_fixed1 = partial(lgb_cv, x_data=train_x, y_data=train_y[\"casual\"], n_splits=5, output='score')\nfunc_fixed2 = partial(lgb_cv, x_data=train_x, y_data=train_y[\"registered\"], n_splits=5, output='score')\nlgbBO = BayesianOptimization(\n    func_fixed1, \n    {\n        'num_leaves': (30, 100),    \n        'learning_rate': (0.001, 0.015),  \n        'n_estimators': (1000, 3000),                        \n        'reg_alpha': (0.0001, 1),       \n        'reg_lambda': (0.0001, 1), \n        'min_split_gain' : (0.001, 0.1),\n        'min_child_weight' : (0.001, 0.1),\n        'min_child_samples' : (10,25),\n        'colsample_bytree': (0.85, 1.0),\n    }, \n    random_state=4321            \n)\nlgbBO.maximize(init_points=5, n_iter=20)\nlgbB1 = BayesianOptimization(\n    func_fixed2, \n    {\n        'num_leaves': (30, 100),    \n        'learning_rate': (0.001, 0.015),  \n        'n_estimators': (1000, 3000),                        \n        'reg_alpha': (0.0001, 1),       \n        'reg_lambda': (0.0001, 1), \n        'min_split_gain' : (0.001, 0.1),\n        'min_child_weight' : (0.001, 0.1),\n        'min_child_samples' : (10,25),\n        'colsample_bytree': (0.85, 1.0),\n    }, \n    random_state=4321            \n)\nlgbB1.maximize(init_points=5, n_iter=20)","6653fcb7":"params1 = lgbBO.max['params']\nparams2 = lgbB1.max['params']\nlgb_models1 = lgb_cv(\n    params1['num_leaves'], \n    params1['learning_rate'], \n    params1['n_estimators'], \n    params1['reg_alpha'], \n    params1['reg_lambda'], \n    params1['min_split_gain'], \n    params1['min_child_weight'],\n    params1['min_child_samples'],\n    params1['colsample_bytree'],\n    x_data=train_x, y_data=train_y[\"casual\"], n_splits=5, output='model')\nlgb_models2 = lgb_cv(\n    params2['num_leaves'], \n    params2['learning_rate'], \n    params2['n_estimators'], \n    params2['reg_alpha'], \n    params2['reg_lambda'], \n    params2['min_split_gain'], \n    params2['min_child_weight'],\n    params2['min_child_samples'],\n    params2['colsample_bytree'],\n    x_data=train_x, y_data=train_y[\"registered\"], n_splits=5, output='model')\npreds = []\nfor model in lgb_models1:\n    pred = model.predict(test1)\n    preds.append(pred)\ncasual_pred = np.mean(preds, axis=0)\npreds = []\nfor model in lgb_models2:\n    pred = model.predict(test1)\n    preds.append(pred)\nregistered_pred = np.mean(preds, axis=0)\ncount_pred2 = np.exp(casual_pred) + np.exp(registered_pred) - 2","f13acf0e":"from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\npreds = {}\nregs = {\"gbdt\": GradientBoostingRegressor(random_state=0),\n        \"rf\": RandomForestRegressor(random_state=0, n_jobs=-1)}\nfor name, reg in regs.items():\n    if name == 'gbdt':\n        reg.set_params(n_estimators=1500, min_samples_leaf=6)\n    elif name == 'rf':\n        reg.set_params(n_estimators=1500, min_samples_leaf=2)\n    reg.fit(train_x, train_y['casual'])\n    pred_casual = reg.predict(test1)\n    pred_casual = np.exp(pred_casual) - 1\n    pred_casual[pred_casual < 0] = 0\n    if name == 'gbdt':\n        reg.set_params(n_estimators=1500, min_samples_leaf=6)\n    elif name == 'rf':\n        reg.set_params(n_estimators=1500, min_samples_leaf=2)\n    reg.fit(train_x, train_y['registered'])\n    pred_registered = reg.predict(test1)\n    pred_registered = np.exp(pred_registered) - 1\n    pred_registered[pred_registered < 0] = 0\n    preds[name] = pred_casual + pred_registered","e0c607e0":"tmp = pd.DataFrame({'Feature': x_train.columns, 'Feature importance': reg.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (15,15))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)","9ec19e8a":"pred_mean = (count_pred1 + count_pred2 + preds['gbdt'] + preds['rf'])\/4\nsample = pd.read_csv(\"..\/input\/bike-sharing-demand\/sampleSubmission.csv\")\nsample[\"count\"] = pred_mean\nsample.to_csv(\"sample.csv\",index=False)","bd3378d7":"temp and atemp have high correlation and register and have too.\nAnd windspeed and outcomes have low correlation(<=0.1)\nSee scatterplot of temp and atemp.","c987d3bd":"# Bike demand predict","dd30a014":"see relationship between hour and each count by workingday and holiday","a23a1724":"## Modeling\n\n#### - 1. lightgbm + cross validation\n\nUse lightgbm model, and use cross-validation to prevent overfitting","354eb80f":"see relation of categorical predictors and outcomes by countplot","f18ea971":"#### Divide predictors and outcomes. And take logging outcomes to normalize.","8c345e48":"There is no holiday in Tuesday and Thursday.\nAnd there is differences when Monday, Wednesday, and Friday.","97543c87":"Result rmsle is 0.38081","0f23e429":"Based on the above results, make new variable.","c4a93a47":"In train data, there is strange pattern, but not in test.\nIt seems to be haved wrong value in atemp.\nSo based on correlation and scatterplot, judged to remove atemp","f61a2052":"#### - 2. lgbmRegressor + crossvalidation + Bayesian optimization","451efec9":"In count of holiday, workingday and weekday, there is no difference depending on categories.\nbut in registered and casual, it depend of the categories. So need to look at this part differently.","eee08a6f":"## correlation","843752a5":"Range of variable in train and test is similar. So for now, i don't remove outlier\n\n<br><br>\n\n## Create variables and visualization\ncreate columns from datetime","ad023696":"The number of registered and casual according to workingday and holiday show the opposite pattern.\nAnd there are differences in the number of registered according to workingday at the closing hour and the office-going hour.\nSo many registered is can be expected to workers.","a9f88655":"See variables's distribution by distplot and countplot","858d11ce":"#### - 3. randomforest and gradientboostingregressor","f08d62a8":"see feature importance","cb9dbd12":"see relationship between weekday and each count by workingday and holiday"}}