{"cell_type":{"f6076371":"code","83bbbde4":"code","f023538a":"code","f0c7935f":"code","82461c41":"code","28a42af7":"code","c2c9b63a":"code","6334c3f4":"code","b05a8039":"code","0feede9b":"code","1268a8b5":"code","7814f0fc":"code","fd253c97":"code","d39a7648":"code","d089f73c":"code","0a2839eb":"code","c89d4db3":"code","4218570c":"code","17a19f89":"code","6ba5e0db":"code","15380514":"code","333cdca2":"code","51d1a354":"code","52a9a223":"code","7922e714":"code","4c7753bf":"code","c23c7f79":"code","2f27ee0a":"code","c0480f4c":"code","76870dcd":"code","2f737f8d":"code","44af02b2":"code","c207f1be":"code","c1a5930d":"code","469e6468":"code","67e3c47b":"code","8c150b73":"code","998b4272":"code","f73242f5":"code","7ff86952":"code","6649c634":"code","cbd2b848":"code","c76eb480":"code","2f5b1335":"code","89fc54a6":"code","7338021c":"code","ea8b5ad2":"code","6b60cb7e":"code","80024bd8":"code","d4e89b01":"code","0a13f8e0":"markdown","c698422d":"markdown","3a45ddc2":"markdown","b167fc04":"markdown","873277b8":"markdown","a7cf01c3":"markdown"},"source":{"f6076371":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom time import time\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","83bbbde4":"path = \"\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv\"\ndf = pd.DataFrame()\ndf = pd.read_csv(path)\ndf.shape","f023538a":"df = df.sample(400000)\ndf.shape","f0c7935f":"df = df[['Text', 'Summary']]\ndf.dropna(axis=0, inplace=True)                    \ndf.drop_duplicates(subset=['Summary'], inplace=True)  \ndf.reset_index(drop=1, inplace=True)\ndf.head(5)","82461c41":"df.shape","28a42af7":"print(df['Text'][0])","c2c9b63a":"print(df['Summary'][0])","6334c3f4":"import spacy\nimport nltk\nimport re\n# nlp = spacy.load('en_core_web_sm')\n# nlp = spacy.load('en_core_web_lg')\nnltk.download('stopwords')\nnltk.download('punkt')","b05a8039":"stop_words = nltk.corpus.stopwords.words('english')\n# nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"can not\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\npunctuations = '!\"#$%&\\'()*+,-\/:;<=>?@[\\\\]^_`{|}~'","0feede9b":"print(stop_words)","1268a8b5":"print(contraction_mapping)","7814f0fc":"print(punctuations)","fd253c97":"def cleansing_text(text):\n    text = text.lower() # Convert to lowercase\n    text = re.sub('<pre>.*?<\/pre>', '', text, flags = re.DOTALL)  # Remove HTML tags\n    text = re.sub('<code>.*?<\/code>', '', text, flags = re.DOTALL)\n    text = re.sub('<[^>]+>', '',text ,flags = re.DOTALL)\n    text = ' '.join([contraction_mapping[i] if i in contraction_mapping else i for i in text.split(\" \")]) # Contraction mapping \n    text = re.sub(r\"'s\\b\", \"\", text)  # Remove (\u2018s) \n    text = re.sub(\"[^a-zA-Z]\" ,\" \", text) # Remove punctuations and special characters\n    text = ' '.join([i for i in text.split() if i not in punctuations]) # Remove punctuations\n    text = ' '.join([i for i in text.split() if i not in stop_words]) # Remove stop_words\n#     text = ''.join([str(doc) for doc in nlp.pipe(text, batch_size = 5000, n_threads=-1)])\n    return text\n\ndef cleansing_summary(summary):\n    summary = summary.lower() # Convert to lowercase\n    summary = re.sub('<pre>.*?<\/pre>', '', summary, flags = re.DOTALL)  # Remove HTML tags\n    summary = re.sub('<code>.*?<\/code>', '', summary, flags = re.DOTALL)\n    summary = re.sub('<[^>]+>', '',summary ,flags = re.DOTALL)\n    summary = ' '.join([contraction_mapping[i] if i in contraction_mapping else i for i in summary.split(\" \")]) # Contraction mapping \n    summary = re.sub(r\"'s\\b\", \"\", summary)  # Remove (\u2018s) \n    summary = re.sub(\"[^a-zA-Z]\" ,\" \", summary) # Remove punctuations and special characters\n    summary = ' '.join([i for i in summary.split() if i not in punctuations]) # Remove personal punctuations\n    summary = ' '.join([i for i in summary.split() if i not in stop_words]) # Remove stop_words\n#     summary = ''.join([str(doc) for doc in nlp.pipe(summary, batch_size = 5000, n_threads=-1)])\n#     summary = 'START_ ' + str(summary) + ' END_'\n    return summary","d39a7648":"from tqdm.notebook import tqdm\n\ntexts = []\nfor text in tqdm(df['Text']):\n    texts.append(cleansing_text(text))\ndf['Text_Cleaned'] = texts  \nprint(\"::::: Text_Cleaned :::::\")\nprint(df['Text_Cleaned'][0:5], \"\\n\")\n\nsummaries = []\nfor text in tqdm(df['Summary']):\n    summaries.append(cleansing_summary(text))\ndf['Summary_Cleaned'] =  summaries \nprint(\"::::: Summary :::::\")\nprint(df['Summary_Cleaned'][0:5], \"\\n\")\n\ncorpus = list(df['Text_Cleaned'])","d089f73c":"print(df['Text_Cleaned'][0])","0a2839eb":"print(df['Summary_Cleaned'][0])","c89d4db3":"text_count = []\nsummary_count = []\n\nfor sent in df['Text_Cleaned']:\n    text_count.append(len(sent.split()))\nfor sent in df['Summary_Cleaned']:\n    summary_count.append(len(sent.split()))\n\ngraph_df = pd.DataFrame()\ngraph_df['text'] = text_count\ngraph_df['summary'] = summary_count","4218570c":"graph_df['text'].describe()","17a19f89":"graph_df['summary'].describe()","6ba5e0db":"graph_df['text'].hist(bins = 25, range=(0, 200))\nplt.show()","15380514":"graph_df['summary'].hist(bins = 15, range=(0, 15))\nplt.show()","333cdca2":"# Check how much % of text have 10-100 words\ncount = 0\nfor i in graph_df['text']:\n    if i > 10 and i <= 100:\n        count = count + 1\nprint(count \/ len(graph_df['text']))","51d1a354":"# Check how much % of summary have 2-10 words\ncount = 0\nfor i in graph_df['summary']:\n    if i > 1 and i <= 10:\n        count = count + 1\nprint(count \/ len(graph_df['summary']))\n","52a9a223":"# Model to summarize  \n# 11 - 100 words for Text\n# 2 - 10 words for Summary \n\nmax_text_len = 100\nmax_summary_len = 10\n\ncleaned_text = np.array(df['Text_Cleaned'])\ncleaned_summary = np.array(df['Summary_Cleaned'])\n\nshort_text = []\nshort_summary = []\n\nfor i in range(len(cleaned_text)):\n    if(len(cleaned_summary[i].split()) <= max_summary_len \n       and len(cleaned_summary[i].split()) > 1 \n       and len(cleaned_text[i].split()) <= max_text_len \n       and len(cleaned_text[i].split()) > 10):\n        short_text.append(cleaned_text[i])\n        short_summary.append(cleaned_summary[i])\n        \npost_pre = pd.DataFrame({'text':short_text,'summary':short_summary})","7922e714":"# Add sostok and eostok\npost_pre['summary'] = post_pre['summary'].apply(lambda x : 'sostok '+ x + ' eostok')","4c7753bf":"post_pre.shape","c23c7f79":"post_pre","2f27ee0a":"from sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\n\n# train test split\nx_tr,x_test,y_tr,y_test = train_test_split(np.array(post_pre['text']),\n                                         np.array(post_pre['summary']),\n                                         test_size = 0.2,\n                                         random_state = 0,\n                                         shuffle = True)\n# train validation split\nx_tr,x_val,y_tr,y_val = train_test_split(x_tr,\n                                         y_tr,\n                                         test_size = 0.2,\n                                         random_state = 0,\n                                         shuffle = True)","c0480f4c":"x_tr.shape","76870dcd":"x_test.shape","2f737f8d":"x_val.shape","44af02b2":"# Tokenize text to get the vocab count\n#prepare a tokenizer for training data\nx_tokenizer = Tokenizer() \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer()   \ny_tokenizer.fit_on_texts(list(y_tr))","c207f1be":"thresh = 6\ncnt = 0\n# tot_cnt = 0\ntot_cnt = len(x_tokenizer.word_counts)\nfreq = 0\ntot_freq = 0\n\nkeys = []\nvalues = []\n\nfor key,value in x_tokenizer.word_counts.items():\n    keys.append(key)\n    values.append(value)\n    if(value < thresh):\n        cnt = cnt + 1\n\ndf_frequency = pd.DataFrame({'word':keys,'frequency':values})\ndf_frequency.sort_values(by='frequency', ascending=False, inplace=True)\ndf_frequency.reset_index(inplace=True, drop=0)\ndf_frequency","c1a5930d":"print(\"% Rare words in vocabulary:\",(cnt \/ tot_cnt) * 100)\ntot_cnt, cnt","469e6468":"fig, ax = plt.subplots(figsize=(6,10), ncols=1, nrows=1)\nsns.barplot(x='frequency',y='word',data=df_frequency[:20], palette='Reds_r', ax=ax);","67e3c47b":"#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\nx_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\nx_test_seq = x_tokenizer.texts_to_sequences(x_test)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\nx_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\nx_test = pad_sequences(x_test_seq, maxlen=max_text_len, padding='post')\n\n#size of vocabulary ( +1 for padding token)\nx_voc   =  x_tokenizer.num_words + 1\n\nprint(\"Size of vocabulary in X = {}\".format(x_voc))","8c150b73":"thresh = 3\ncnt = 0\ntot_cnt = len(y_tokenizer.word_counts)\nfreq = 0\ntot_freq = 0\n\nkeys = []\nvalues = []\n\nfor key,value in y_tokenizer.word_counts.items():\n    keys.append(key)\n    values.append(value)\n    if(value < thresh):\n        cnt = cnt + 1\n\ndf_frequency = pd.DataFrame({'word':keys,'frequency':values})\ndf_frequency.sort_values(by='frequency', ascending=False, inplace=True)\ndf_frequency.reset_index(inplace=True, drop=0)\ndf_frequency","998b4272":"print(\"% Rare words in vocabulary:\",(cnt \/ tot_cnt) * 100)\ntot_cnt, cnt","f73242f5":"print(\"% Rare words in vocabulary:\",(cnt \/ tot_cnt) * 100)\ntot_cnt, cnt","7ff86952":"fig, ax = plt.subplots(figsize=(6,10), ncols=1, nrows=1)\nsns.barplot(x='frequency',y='word',data=df_frequency[3:20], palette='Reds_r', ax=ax);","6649c634":"#prepare a tokenizer for reviews on training data\ny_tokenizer = Tokenizer(num_words = tot_cnt-cnt) \ny_tokenizer.fit_on_texts(list(y_tr))\n\n#convert text sequences into integer sequences (i.e one hot encode the text in Y)\ny_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \ny_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \ny_test_seq = y_tokenizer.texts_to_sequences(y_test) \n\n#padding zero upto maximum length\ny_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\ny_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\ny_test = pad_sequences(y_test_seq, maxlen=max_summary_len, padding='post')\n\n#size of vocabulary\ny_voc  =   y_tokenizer.num_words +1\nprint(\"Size of vocabulary in Y = {}\".format(y_voc))","cbd2b848":"from tensorflow.keras.backend import clear_session\nimport gensim\nfrom numpy import *\nimport numpy as np\nimport pandas as pd \nimport re\nfrom bs4 import BeautifulSoup\nfrom keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c76eb480":"print(f\"Size of vocabulary from the w2v model = {x_voc}\")\n\nclear_session()\n\nlatent_dim = 256\nembedding_dim = 128\n\n# Encoder\nencoder_inputs = Input(shape=(max_text_len,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary()","2f5b1335":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)\n\nhistory=model.fit([x_tr,y_tr[:,:-1]], \n                  y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:],\n                  epochs=10,\n                  callbacks=[es],\n                  batch_size=128, \n                  validation_data=([x_val,y_val[:,:-1]], \n                                   y_val.reshape(y_val.shape[0],\n                                                 y_val.shape[1], \n                                                 1)[:,1:]))","89fc54a6":"from matplotlib import pyplot\npyplot.plot(history.history['loss'], label='train')\npyplot.plot(history.history['val_loss'], label='test')\npyplot.legend()\npyplot.show()","7338021c":"reverse_target_word_index=y_tokenizer.index_word\nreverse_source_word_index=x_tokenizer.index_word\ntarget_word_index=y_tokenizer.word_index\n\n# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_outputs2) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","ea8b5ad2":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['sostok']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='eostok'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","6b60cb7e":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","80024bd8":"totle = []\ntotle_predicted = []\naccuracy = []\n\n# sample 5000 test\nfor i in tqdm(range(0, 5000)):\n    review = seq2text(x_test[i])\n    original_summary = seq2summary(y_test[i])\n    predicted_summary = decode_sequence(x_test[i].reshape(1, max_text_len))\n    print(\"Review:\", review)\n    print(\"Original summary:\", original_summary)\n    print(\"Predicted summary:\", predicted_summary)\n    \n#     if len(original_summary.split()) != 0:\n    count = 0\n    for j in predicted_summary.split():\n        if j in review:\n            count += 1\n#     count = 0\n#     for k in decode_sequence(x_tr[i].reshape(1, max_text_len)).split():\n#         if k in original_summary:\n#             count += 1\n    totle.append(len(predicted_summary.split()))\n    accuracy.append(count\/len(predicted_summary.split()))\n    print(f\"{count} \/ {len(predicted_summary.split())}\")\n    print(\"\\n\")","d4e89b01":"sum(accuracy)\/len(accuracy)","0a13f8e0":"# Text Preprocessing (Count word in Sentences)","c698422d":"# Text Preprocessing (Train Test Split)","3a45ddc2":"# Model","b167fc04":"# Making Summaries","873277b8":"# Text Preprocessing","a7cf01c3":"# Text Preprocessing (Rare Word Analysis)\ntot_cnt = Size of vocabulary (unique words in the text)\n\ncnt = No. of rare words whose count falls below threshold\n\ntot_cnt - cnt = The top most common words"}}