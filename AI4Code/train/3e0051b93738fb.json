{"cell_type":{"78077151":"code","59890631":"code","f9364def":"code","12ec7ded":"code","2fee728f":"code","baf02430":"code","97e7781e":"code","28476d55":"code","3b003287":"code","2acae984":"code","00b0ed8c":"code","e8713db1":"code","38385a94":"code","03fdb418":"code","d976648d":"code","86044954":"code","5d224410":"code","706ec118":"code","5998e008":"code","b24ca6e1":"code","c8713dd4":"code","182cf9ea":"code","fa4298a3":"code","e13802cb":"code","1bd4666d":"code","513bd317":"code","879f537a":"code","e3c0b778":"markdown","a497e658":"markdown","a7c6f672":"markdown","51f98688":"markdown","cdcf0449":"markdown","1f87721b":"markdown","b43af13a":"markdown","1834597e":"markdown","4b9ea649":"markdown","88202a8c":"markdown","f670b982":"markdown","7fba6119":"markdown","c4754657":"markdown","7dd03bce":"markdown","37fc224c":"markdown","1b4a5705":"markdown","0f84d62e":"markdown","868637f8":"markdown","83608fa8":"markdown","5e4b883a":"markdown","f591089f":"markdown","5149766f":"markdown"},"source":{"78077151":"#General\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport statsmodels as sm\nimport math\nfrom sklearn import metrics\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\n\n#Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier","59890631":"column_names = ['Id', 'age', 'workclass', 'final_weight', 'education', 'education_num', 'marital_status',\n                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week',\n                'native_country', 'income']","f9364def":"adult = pd.read_csv(\"Databases\/train_data.csv\", names = column_names, na_values='?').drop(0, axis = 0).reset_index(drop = True)","12ec7ded":"adult.shape","2fee728f":"adult.head()","baf02430":"adult.describe()","97e7781e":"def count_null_values(data):\n    '''\n    Return a DataFrame with count of null values\n    '''\n    \n    counts_null = []\n    for column in data.columns:\n        counts_null.append(data[column].isnull().sum())\n    counts_null = np.asarray(counts_null)\n\n    counts_null = pd.DataFrame({'feature': data.columns, 'count.': counts_null,\n                                'freq. [%]': 100.0*counts_null\/data.shape[0]}).set_index('feature', drop = True)\n    counts_null = counts_null.sort_values(by = 'count.', ascending = False)\n    \n    return counts_null","28476d55":"count_null_values(adult).head()","3b003287":"fig = plt.figure(figsize=(20,15))\ncols = 5\nrows = math.ceil(float(adult.shape[1]) \/ cols)\nfor i, column in enumerate(adult.columns):\n    ax = fig.add_subplot(rows, cols, i + 1)\n    ax.set_title(column)\n    if adult.dtypes[column] == np.object:\n        adult[column].value_counts().plot(kind=\"bar\", axes=ax)\n    else:\n        adult[column].hist(axes=ax)\n        plt.xticks(rotation=\"vertical\")\nplt.subplots_adjust(hspace=0.7, wspace=0.2)","2acae984":"def work_missing_values(data):\n    '''\n    Return new data with no missing values for this problem\n    '''\n    \n    aux = data.copy()\n    # select index of rows that workclass is nan\n    aux_index = aux[aux['workclass'].isna()].index\n    \n    # fill nan with 'unknown'\n    aux['occupation'].loc[aux_index] = 'unknown'\n    \n    # complete missing of native_country and occupation with most frequent\n    cols = ['native_country', 'workclass', 'occupation']\n    for col in cols:\n        top = aux[col].value_counts().index[0]\n        aux[col] = aux[col].fillna(top)\n    aux.reset_index(drop = True)\n    \n    return aux","00b0ed8c":"adult = work_missing_values(adult)","e8713db1":"count_null_values(adult).head()","38385a94":"# Encode the categorical features as numbers\ndef number_encode_features(df):\n    result = df.copy()\n    encoders = {}\n    for column in result.columns:\n        if result.dtypes[column] == np.object:\n            encoders[column] = LabelEncoder()\n            result[column] = encoders[column].fit_transform(result[column])\n    return result, encoders\n\n# Calculate the correlation and plot it\nencoded_data, _ = number_encode_features(adult)\nsns.heatmap(encoded_data.corr(), square=True)\nplt.show()\n","03fdb418":"testAdult = pd.read_csv(\"Databases\/test_data.csv\", names = column_names, na_values='?').drop(0, axis = 0).reset_index(drop = True)","d976648d":"testAdult.shape","86044954":"testAdult = work_missing_values(testAdult)","5d224410":"testAdult.head()","706ec118":"Xadult = adult[[\"age\",\"education_num\",\"capital_gain\", \"capital_loss\", \"hours_per_week\"]]","5998e008":"Yadult = adult.income","b24ca6e1":"XtestAdult = testAdult[[\"age\",\"education_num\",\"capital_gain\", \"capital_loss\", \"hours_per_week\"]]","c8713dd4":"YtestAdult = testAdult.income","182cf9ea":"LogClf = LogisticRegression(solver = 'lbfgs', C = 1.0, penalty = 'l2', warm_start =  True)\n\nLogCV = cross_val_score(LogClf, Xadult, Yadult, cv = 10)\n\nLogClf.fit(Xadult, Yadult)\n\ncv_accuracy = [LogCV.mean()]\ncv_std = [LogCV.std()]\n\ncv_values = {}\ncv_values['LogCV'] = LogCV\nprint('Logistic Regression CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(LogCV.mean(), LogCV.std()))","fa4298a3":"adb = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n\nadbCV = cross_val_score(adb, Xadult, Yadult, cv = 10)\n\nadb.fit(Xadult, Yadult)\n\ncv_accuracy.append(adbCV.mean())\ncv_std.append(adbCV.std())\ncv_values['adbCV'] = adbCV\nprint('Adaboost CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(adbCV.mean(), adbCV.std()))","e13802cb":"rf = RandomForestClassifier(n_estimators = 700, max_depth = 12)\n\nrfCV = cross_val_score(rf, Xadult, Yadult, cv = 10)\n\nrf.fit(Xadult, Yadult)\n\ncv_accuracy.append(rfCV.mean())\ncv_std.append(rfCV.std())\ncv_values['rfCV'] = rfCV\nprint('Random Forest CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(rfCV.mean(), rfCV.std()))","1bd4666d":"Ypred = LogClf.predict(XtestAdult)\nsavepath = \"logPredictions.csv\" \nprev = pd.DataFrame(Ypred, columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") \nprev.head()","513bd317":"Ypred = adb.predict(XtestAdult)\nsavepath = \"adbPredictions.csv\" \nprev = pd.DataFrame(Ypred, columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") \nprev.head()","879f537a":"Ypred = rf.predict(XtestAdult)\nsavepath = \"rfPredictions.csv\" \nprev = pd.DataFrame(Ypred, columns = [\"income\"]) \nprev.to_csv(savepath, index_label=\"Id\") \nprev.head()","e3c0b778":"##### Enquanto a regress\u00e3o log\u00edstica e o Adaboost tiveram tempos de execu\u00e7\u00e3o parecidos, a random forest demorou consideravelmente mais para construir o classificador. Mesmo que a random forest tenha bem mais estimadores do que o Adaboost, ela teve performance praticamente igual a ela. Ainda, o Adaboost demonstrou mais precis\u00e3o do que a regress\u00e3o log\u00edstica.\n\n##### Portanto, j\u00e1 que a implementa\u00e7\u00e3o \u00e9 a mesma em todos os casos, pode-se dizer que a melhor op\u00e7\u00e3o dentre estes 3 testados, \u00e9 o Adaboost","a497e658":"## 2.1. Leitura dos dados ","a7c6f672":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es","51f98688":"#### Temos que \"age\",\"education_num\",\"capital_gain\", \"capital_loss\", \"hours_per_week\" s\u00e3o os atributos que melhor identificam income","cdcf0449":"# 5. Classificadores","1f87721b":"## 6.2 Random Forest ","b43af13a":"# 6. Predi\u00e7\u00f5es ","1834597e":"## 5.2 Adaboost ","4b9ea649":"#### Aplica\u00e7\u00e3o de tr\u00eas classificadores diferentespara a base adult","88202a8c":"#### \u00c9 poss\u00edvel notar que para 'occupation' podemos criar uma nova profiss\u00e3o 'unknown'. J\u00e1 para country e workclass faz mais sentido seguir com substituindo as linhas pelo valor mais frequente. J\u00e1 que uma grande discrep\u00e2ncia entre o primeiro valor mais frequente e os demais","f670b982":"#  4. Prepara\u00e7\u00e3o das bases para uso dos classificadores","7fba6119":"# 3. Leitura da base de teste","c4754657":"#### Antes de extrair os valores nulos, analisaremos os histogramas de todas as features:","7dd03bce":"## 6.1 Regress\u00e3o Log\u00edstica","37fc224c":"## 2.2 An\u00e1lise de correla\u00e7\u00e3o entre as features","1b4a5705":"# 2. Importa\u00e7\u00e3o dos dados e an\u00e1lise ","0f84d62e":"## 2.2 Missing data ","868637f8":"## 5.1 Regress\u00e3o log\u00edstica","83608fa8":"## 6.2 Adaboost ","5e4b883a":"# 7. Conclus\u00f5es","f591089f":"##  5.3 Random Forest","5149766f":"# 1. Importa\u00e7\u00e3o dos pacotes"}}