{"cell_type":{"95a31186":"code","8af00bec":"code","751be842":"code","e2216f93":"code","9e2eee64":"code","28eb6826":"code","59ddd9a2":"code","36e3caa6":"code","825a709b":"code","f780fe41":"code","f122130a":"code","5c38d325":"code","559abc05":"code","2bafd4dd":"code","b32ef9ba":"code","8e3367a4":"code","7cbc3d6e":"code","8f13876f":"code","1871276f":"code","ac924c22":"code","90fc568f":"code","465bac35":"code","e861badd":"code","8da07b3d":"code","6156ed28":"code","d9af7eba":"code","08a88349":"code","51a10cfc":"code","0e351522":"code","21e70482":"code","36ffa2d2":"code","0159a1f1":"code","916bef68":"code","b1df54b2":"code","914829f7":"code","904df797":"code","f48b5df9":"code","57ea9cb6":"markdown","4b49cc5d":"markdown","6c14c782":"markdown","ac9a56b8":"markdown","8d2879ce":"markdown","59701c26":"markdown","9325ffbe":"markdown","fd248fc1":"markdown","a01b35b3":"markdown","26d80d48":"markdown","607407a4":"markdown","1e5f5bf3":"markdown","628ce8cf":"markdown","40edb04e":"markdown","b9bfce7a":"markdown","65c47a01":"markdown","3e2de717":"markdown","0a168dd8":"markdown","1acf4818":"markdown","5c8513c9":"markdown","9934703f":"markdown","468474ec":"markdown","3bf4c304":"markdown","7ab81f4a":"markdown","241d3dd5":"markdown","948433f5":"markdown","ba6fe9dc":"markdown","116db76e":"markdown","a5064dfb":"markdown","6c4841af":"markdown","38dfaace":"markdown","07c4eddc":"markdown","ab8317ab":"markdown"},"source":{"95a31186":"# Work with Data - the main Python libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n\n# Modeling and Prediction\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import confusion_matrix\n\n","8af00bec":"# Download training data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')","751be842":"# Display the first 5 rows of the training dataframe.\ntrain.head()","e2216f93":"# Display basic information about training data\ntrain.info()","9e2eee64":"# Download test data\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.tail(7)  # Display the 7 last rows of the training dataframe","28eb6826":"# Display basic information about the test data\ntest.info()","59ddd9a2":"# Download submission sample file\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","36e3caa6":"# Display the 10 first rows of the submission dataframe\nsubmission.head(10)","825a709b":"def highlight(value):\n    # The painting of the cell in different colors depending on the value: \n    # >= 0.5 - palegreen, < 0.5 - pink\n    \n    if value >= 0.5:\n        style = 'background-color: palegreen'\n    else:\n        style = 'background-color: pink'\n    return style","f780fe41":"# Pivot table\npd.pivot_table(train, values='Survived', index=['Sex']).style.applymap(highlight)","f122130a":"# Calculation the new feature \"Family_size\" as the size of each passenger's family by \n# the number of his \/ her siblings (\"SibSp\") and his \/ her parents and descendants (\"Parch\")\ntrain['Family_size'] = train['SibSp'] + train['Parch'] + 1","5c38d325":"# Pivot table for input features 'Sex' and 'Family_size' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Sex', 'Family_size']).style.applymap(highlight)","559abc05":"# Analysis missing data for feature \"Age\"\nmissing_age_values = train['Age'].isnull().sum() ","2bafd4dd":"# Calculate what percentage of the value of missing_age_values is relative \n# to the total number of values (dataframe length) and round it to two decimal places. \n# Save it into missing_age_values_per_cent.\nmissing_age_values_per_cent = round(missing_age_values * 100 \/ len(train), 2)","b32ef9ba":"# Output missing_age_values_per_cent\nprint(f'Feature \"Age\" has {missing_age_values_per_cent}, %')","8e3367a4":"# Calculation the average (mean) value of all data of Age. Save it into mean_age.\nmean_age = train['Age'].mean()","7cbc3d6e":"# Round mean_age to two decimal places and print it.\nprint('Average age, year', round(mean_age,2))","8f13876f":"# Statistics for training data\ntrain.describe()","1871276f":"# Number of unique values of age\nnumber_age_unique_values = len(train['Age'].unique())\nnumber_age_unique_values","ac924c22":"# Divide all age values train['Age'] by 8 and save in new feature train['Age_8'].\ntrain['Embarked'] = train['Age'] \/\/ 8","90fc568f":"train['Embarked'] = train['Embarked'].fillna(mean_age \/\/ 8).astype('int')","465bac35":"# Print the number of unique values of the feature \"Age\"\nprint(len(train['Embarked'].unique()))","e861badd":"# Pivot table for input features 'Sex', 'Family_size' and 'Age' and output feature 'Survived'\npd.pivot_table(train, values='Survived', index=['Sex', 'Pclass', 'Embarked']).style.applymap(highlight)","8da07b3d":"# Decision trees work with numbers, not words, so we must encode feature \"Sex\" by numbers\ntrain['Sex'].replace({'male': 0, 'female': 1}).head()","6156ed28":"# Copy commands (see above) to this function to calculate new features or process existing ones\n\ndef df_transform(df):\n    # FE for df\n    \n    # Number of family members - feature \"Family_size\"\n    df['Pclass'] = df['SibSp'] + df['Parch'] + 1\n    \n    # Age multiple of 7\n    df['Embarked'] = df['Age'] \/\/ 8\n    \n    # Average age of all dataset multiple of 7\n    mean_age = df['Age'].mean() \/\/ 8\n    \n    # Replace missing age values to average age and rounding them to integers\n    df['Embarked'] = df['Embarked'].fillna(mean_age).astype('int')\n    \n    # Encoding feature \"Sex\" by numbers\n    df['Sex'] = df['Sex'].replace({'male': 0, 'female': 1})\n    \n    # Select the main features\n    df = df[['Pclass','Embarked','Sex']]\n    \n    return df","d9af7eba":"# Selecting a target featute and removing it from training dataset\ntarget = train.pop('Survived')","08a88349":"# FE to training dataset\ntrain = df_transform(train)\n\n# Statistics of training dataset\ntrain.info()","51a10cfc":"train","0e351522":"# FE to test dataset\ntest = df_transform(test)\ntest.info()","21e70482":"test","36ffa2d2":"# Select model as Decision Tree Classifier \n# \"Classifier\" because target has limited (integer) number of classes, in this case 2 classes = [0, 1] or [\"No Survived\", \"Survived\"]\n# For a small amount of data, it is better to choose a smaller parameter max_depth - from 3 to 5, let give 4\n# at a more complex Level we will teach the program to calculate it automatically\nmodel = DecisionTreeClassifier(max_depth=5, random_state=42)\n\n# Training model\nmodel.fit(train, target)","0159a1f1":"# Visualization - build a plot with Decision Tree\nplt.figure(figsize=(18,13))\nplot_tree(model, filled=True, rounded=True, class_names=[\"No Survived\", \"Survived\"], feature_names=train.columns) ","916bef68":"# Prediction for training data\ny_train = model.predict(train).astype(int)","b1df54b2":"confusion_matrix(target, y_train)","914829f7":"# Prediction of target for test data\ny_pred = model.predict(test).astype(int)","904df797":"# Saving the result into submission file\nsubmission[\"Survived\"] = y_pred\nsubmission.to_csv('submission.csv', index=False) # Competition rules require that no index number be saved\n\n# Building the Histogram of predicted target values for test data\nsubmission['Survived'].hist()","f48b5df9":"# Calculation of the mean value of forecasting data\nsubmission['Survived'].mean()","57ea9cb6":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download data](#2)\n1. [EDA & FE](#3)\n1. [Modeling](#4)\n1. [Prediction & Submission](#5)","4b49cc5d":"### Feature \"Family_size\"","6c14c782":"## 1. Import libraries<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","ac9a56b8":"The result of FE is usually combined into one function so that it is convenient to apply to different datasets","8d2879ce":"**TASK:** Calculate what percentage of the value of missing_age_values is relative to the total number of values (dataframe length) and round it to two decimal places. Save it into missing_age_values_per_cent.","59701c26":"**TASK:** Divide all age values train['Age'] by 7 and save in new feature train['Age_7'].","9325ffbe":"**TASK:** Determine how many unique values the feature \"Age\" contains and print it.","fd248fc1":"### It is recommended to start working with this notebook from study:\n* the [description of the data](https:\/\/www.kaggle.com\/c\/titanic\/data)\n* the [task](https:\/\/www.kaggle.com\/c\/titanic) of the competition\n* the [my lecture](https:\/\/www.youtube.com\/watch?v=WERtPBptOWw&list=PL4DHq-xU-ebUiB6T6vjd0SoDha4GOm8zV&index=2&t=951s) about this notebook in YouTube (in Ukrainain).","a01b35b3":"**TASK:** Display basic information about the test data","26d80d48":"**TASK:** Display the first 5 rows of the training dataframe.","607407a4":"**TASK:** Display the 10 first rows of the submission dataframe","1e5f5bf3":"## 5. Prediction & Submission<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","628ce8cf":"### Confusion matrix","40edb04e":"# [L1A: Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/typographic-digits-first-10-fonts)\nMy changes:\n* layers of the CNN\n* epochs_num, validation_split_part","b9bfce7a":"**TASK:** Correct the formula that determines the size of each passenger's family (the new feature: train['Family_size']) by the number  of his \/ her siblings (\"SibSp\") and his \/ her parents and descendants (\"Parch\")","65c47a01":"### Feature \"Sex\"","3e2de717":"Bad solution, but the easiest way is to replace the missing data with the average.","0a168dd8":"## Thanks to Kaggle GM, Prof. [@vbmokin](https:\/\/www.kaggle.com\/vbmokin)","1acf4818":"**TASK:** Build a pivot table for input features 'Sex', 'Family_size' and 'Age' and output feature 'Survived'.","5c8513c9":"**ADDITIONAL TASKS:**\n1. Experiment with resizing (in the \"figsize = ()\") the drawing with the decision tree.\n2. Try changing the value of max_depth above so that the number of correct predictions (prediction values 0 when 0 and 1 when 1 ib the confusion matrix) is greater.","9934703f":"**TASK:** Round mean_age to two decimal places and print it.","468474ec":"## 2. Download data<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","3bf4c304":"**ADDITIONAL TASKS:**\n1. Replace the feature \"Family_size\" to cabin class \"Pclass\" and \"Age_7\" to \"Embarked\".\n2. Replace the value of feature \"Embarked\" with the numbers 0, 1 and 2 (for example: 0 => C = Cherbourg, 1 => Q = Queenstown, 2 => S = Southampton). ","7ab81f4a":"## Acknowledgements\n* [Data Science for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques)\n* [EDA for tabular data: Advanced Techniques](https:\/\/www.kaggle.com\/vbmokin\/eda-for-tabular-data-advanced-techniques)\n* [Titanic - Top score : one line of the prediction](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-score-one-line-of-the-prediction)\n* [Three lines of code for Titanic Top 25%](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1-titanic-decision-tree)\n* [Three lines of code for Titanic Top 20%](https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20)","241d3dd5":"We reduce the number of unique age values by 8 times (8 because children under 16 (16 \/\/ 8 = 2) were considered a child at that time).","948433f5":"### Feature \"Age\"","ba6fe9dc":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","116db76e":"**TASK:** Calculation the average (mean) value of all data of Age. Save it into mean_age.","a5064dfb":"## 4. Modeling<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","6c4841af":"**TASK:** Copy your commands (see above) to this function to calculate new features or process existing ones","38dfaace":"### Decision Tree Visualization","07c4eddc":"Let's make the following assumptions:\n- **Women** have a better chance of surviving ==> feature \"**Sex**\" is important\n- **Single people** have a worse chance of surviving ==> the **number of family members** is important\n- **Children** have a better chance of survival ==> **Age** is important","ab8317ab":"**It is important to make sure** that all features in the training and test datasets:\n* do not have missing values (number of non-null values = number of entries of index) \n* all features have a numeric data type (int8, int16, int32, int64 or float16, float32, float64)."}}