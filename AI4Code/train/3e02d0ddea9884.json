{"cell_type":{"bfab19f8":"code","d85a8397":"code","86928f0e":"code","24af58ce":"code","28f4520b":"code","53183567":"code","30376b58":"code","5e80a6ab":"code","a0295ab3":"code","5a7d204b":"code","00ddac60":"code","007364cd":"code","a0a71eeb":"code","7d7b7de6":"code","b733055e":"code","f98226fc":"code","f781d913":"code","8b52cac6":"code","f068ccc8":"code","b9af59a4":"code","c17eb0bd":"code","0647f5b2":"code","ba281360":"code","7c907f6f":"code","4caf7703":"code","441eaebb":"code","a67c34da":"code","b15aaf8f":"code","0c797963":"markdown","95b70bb0":"markdown","726289af":"markdown","1adbbbff":"markdown"},"source":{"bfab19f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d85a8397":"import matplotlib.pyplot as plt\nimport seaborn as sn","86928f0e":"train = pd.read_json(\"\/kaggle\/input\/facebook-hateful-meme-dataset\/data\/train.jsonl\",lines=True)\ntest = pd.read_json(\"\/kaggle\/input\/facebook-hateful-meme-dataset\/data\/test.jsonl\",lines=True)\n","24af58ce":"val = pd.read_json(\"\/kaggle\/input\/facebook-hateful-meme-dataset\/data\/dev.jsonl\",lines=True)","28f4520b":"train.head()","53183567":"train[\"label\"].value_counts().plot(kind=\"bar\")","30376b58":"plt.figure(figsize=(10,6))\nimg = plt.imread(f\"\/kaggle\/input\/facebook-hateful-meme-dataset\/data\/img\/42953.png\")\nplt.imshow(img)","5e80a6ab":"def return_len(data):\n    data = data.split()\n    return len(data)","a0295ab3":"train[\"lensequence\"] = train[\"text\"].apply(return_len)","5a7d204b":"train.head()","00ddac60":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader","007364cd":"!pip install pytorch-transformers","a0a71eeb":"from pytorch_transformers import RobertaModel, RobertaTokenizer\nfrom pytorch_transformers import RobertaForSequenceClassification, RobertaConfig","7d7b7de6":"config = RobertaConfig.from_pretrained(\"roberta-base\")\nconfig.num_labels = 2","b733055e":"config","f98226fc":"tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\nmodel = RobertaForSequenceClassification(config)","f781d913":"def prepare_features(seq_1, max_seq_length = 30, \n             zero_pad = False, include_CLS_token = True, include_SEP_token = True):\n    tokens_a = tokenizer.tokenize(seq_1)\n    if len(tokens_a) > max_seq_length - 2:\n        tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n    tokens = []\n    if include_CLS_token:\n        tokens.append(tokenizer.cls_token)\n\n    for token in tokens_a:\n        tokens.append(token)\n\n    if include_SEP_token:\n        tokens.append(tokenizer.sep_token)\n\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n    input_mask = [1] * len(input_ids)\n\n    if zero_pad:\n        while len(input_ids) < max_seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n    return torch.tensor(input_ids).unsqueeze(0), input_mask","8b52cac6":"prepare_features(\"Hey I am Ashutosh\")","f068ccc8":"plt.figure(figsize=(10,6))\nsn.distplot(train[\"lensequence\"])","b9af59a4":"max_len = 30\nclass HateDataset(Dataset):\n    def __init__(self,dataframe):\n        self.df = dataframe\n    \n    def __getitem__(self,index):\n        text = self.df.text[index]\n        text, _ = prepare_features(text,max_seq_length=35,zero_pad=True)\n        target = self.df.label[index]\n        \n        return text,target\n    \n    def __len__(self):\n        return len(self.df)","c17eb0bd":"train_ds = HateDataset(train)\nval_ds = HateDataset(val)","0647f5b2":"# Parameters\nparams = {'batch_size': 1,\n          'shuffle': True,\n          'drop_last': False,\n          'num_workers': 4}\ntrain_dl = DataLoader(train_ds,**params)\nval_dl = DataLoader(val_ds,**params)","ba281360":"train_ds.__getitem__(0)[0].shape","7c907f6f":"loss_function = nn.CrossEntropyLoss()\nlearning_rate = 1e-05\noptimizer = optim.Adam(params =  model.parameters(), lr=learning_rate)","4caf7703":"device= torch.device(\"cuda\")","441eaebb":"inp = train_ds.__getitem__(0)[0].cuda()\noutput = model(inp)[0]\nprint(output.shape)","a67c34da":"data,label = next(iter(train_dl))\nprint(data,label)","b15aaf8f":"from tqdm import tqdm_notebook\nmax_epochs = 5\nmodel.to(device)\n\nfor epoch in tqdm_notebook(range(max_epochs)):\n    print(\"EPOCH -- {} \".format(epoch))\n    \n    for i,(text,label) in enumerate(train_dl):\n            train_loss= 0\n            optimizer.zero_grad()\n            \n            text = text.squeeze(0)\n            text = text.to(device)\n            \n            label = label.to(device)\n            \n            output = model(text)[0]\n            \n            _,predicted = torch.max(output,1)\n            \n            loss = loss_function(output,label)\n            loss.backward()\n            \n            train_loss += loss.item()\n            optimizer.step()\n            \n            print(\"Train loss:-- {}\".format(train_loss))\n            \n            if i%100 == 0:\n                correct = 0\n                total = 0\n                for text, label in val_dl:\n                    text = text.squeeze(0)\n\n                    text = text.to(device)\n                    label = label.to(device)\n\n                    output = model.forward(text)[0]\n                    _, predicted = torch.max(output.data, 1)\n                    total += label.size(0)\n                    correct += (predicted.cpu() == label.cpu()).sum()\n                accuracy = 100.00 * correct.numpy() \/ total\n                print('Iteration: {}. Loss: {}. Accuracy: {}%'.format(i, loss.item(), accuracy))\n            \n            ","0c797963":"# This was funny as the model was not able go beyond 50%(accuracy) which make sense as the competition also has images(multimodal task).","95b70bb0":"*I just wanted to see how only text data does in the competition and it did as expected.*\n*The competition is a multimodal task in which you have to predict if a facebook post(image+text) is a hateful meme or not.* ","726289af":"Link to the competition \n[https:\/\/www.drivendata.org\/competitions\/64\/hateful-memes\/page\/205\/](http:\/\/)","1adbbbff":"# The competetion is hosted on Data Driven."}}