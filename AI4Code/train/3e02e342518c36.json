{"cell_type":{"9c66f179":"code","705fac85":"code","14049cd9":"code","5647ea63":"code","c018119d":"code","3120322b":"code","cbc3b51a":"code","654613fe":"code","aac09faf":"code","24cb55ab":"code","0461c8d5":"code","4cadf0cd":"code","ea3cd015":"code","026a6e39":"code","d6dc25fe":"code","119dc241":"code","3fb3927e":"code","12653bb7":"code","043207dd":"code","fbf29df4":"code","f3dcad1e":"code","f7795761":"code","9aaab672":"code","e1620263":"code","dd5e4f82":"code","96caae33":"code","134f3ed6":"markdown","f4231c44":"markdown","fc06ff32":"markdown","7f039cc2":"markdown"},"source":{"9c66f179":"# can't train the model using Kaggle - only one gpu core (this kernel is cpu)\nimport torch\ndevice = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\" )\n#assert (torch.cuda.is_available())\nprint(f'device type: {device}')\nprint('PyTorch Version:', torch.__version__)\nprint('cuDNN Version:', torch.backends.cudnn.version())\nprint('Number of cuda Devices:', torch.cuda.device_count())","705fac85":"%%bash\npip install tokenizers","14049cd9":"import os\nimport re\nimport glob\nimport subprocess \nfrom tokenizers import BertWordPieceTokenizer\nimport multiprocessing\nmultiprocessing.cpu_count()","5647ea63":"%%time \n# if you want to train your own vocab (takes sometime)\nfiles = [name for name in glob.glob('\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/*.txt')] \n# Initialize an empty tokenizer\ntokenizer = BertWordPieceTokenizer(\n    clean_text=False, # was true\n    handle_chinese_chars=False,\n    strip_accents=False, # was true\n    lowercase=False,\n    )\n\n# And then train\ntrainer = tokenizer.train(\n    files,\n    vocab_size=32000,\n    min_frequency=2,\n    show_progress=True,\n    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]' ],\n    limit_alphabet=1000,\n    wordpieces_prefix='##',\n)\n\n# Save the files\ntokenizer.save('\/kaggle\/working\/', 'ar_bert32k')","c018119d":"# vocab size is 32k (pre-trained from dataset, or use your own as above)\nvocab = '\/kaggle\/input\/arabic-bert-corpus\/ar_bert32k-vocab.txt'","3120322b":"# Huggingface tokenizers library was used to tokenize the corpus (BERTWordPiece)\n# https:\/\/github.com\/huggingface\/tokenizers\ntokenizer = BertWordPieceTokenizer(vocab, strip_accents=False)","cbc3b51a":"encoded_ar = tokenizer.encode(\".\u0627\u0644\u0642\u062f\u0633 \u0645\u062f\u064a\u0646\u0629 \u062c\u0645\u064a\u0644\u0629\u060c \u0648\u0647\u064a \u0639\u0627\u0635\u0645\u0629 \u0641\u0644\u0633\u0637\u064a\u0646 \u0627\u0644\u0623\u0628\u062f\u064a\u0629\")\nencoded_ar2 = tokenizer.encode(\"\u0648\u064e\u0627\u062d\u064e\u0631\u0651 \u0642\u064e\u0644\u0652\u0628\u0627\u0647\u064f \u0645\u0645\u0651\u0646\u0652 \u0642\u064e\u0644\u0652\u0628\u064f\u0647\u064f \u0634\u064e\u0628\u0650\u0645\u064f \u0648\u064e\u0645\u064e\u0646\u0652 \u0628\u062c\u0650\u0633\u0652\u0645\u064a \u0648\u064e\u062d\u0627\u0644\u064a \u0639\u0650\u0646\u062f\u064e\u0647\u064f \u0633\u064e\u0642\u064e\u0645\u064f\")\nencoded_ar3 = tokenizer.encode(\"\u0643\u064a\u0641 \u062a\u062a\u0645 \u062a\u062c\u0632\u0626\u0629 \u0627\u0644\u0633\u0624\u0627\u0644 \u0648\u0627\u0644\u0625\u062c\u0627\u0628\u0629\u061f\")\nencoded_en = tokenizer.encode(\"Jerusalem is a nice city. It is the capital of Palestine.\")\n\nprint(encoded_ar.tokens)\nprint(encoded_ar2.tokens) \nprint(encoded_ar3.tokens) # in this version, supported Hamza was removed (not inetended)\nprint(encoded_en.tokens) # no foreign language in the vocab","654613fe":"# for downstream tasks (ex. classification): this is how text was cleaned \naccents = re.compile(r'[\\u064b-\\u0652\\u0640]') # harakaat and tatweel (kashida) to remove\narabic_punc = re.compile(r'[\\u0621-\\u063A\\u0641-\\u064A\\u061b\\u061f\\u060c\\u003A\\u003D\\u002E\\u002F\\u007C]+') # all Arabic to keep\n# example (made up string to see cleaning effect)\ntest_text = \"\u0627\u0644\u0645\u0627\u0621\u064f \u0633\u0640\u0640\u0640\u0627\u0626\u0650\u0644\u064c \u0634\u064e\u0641\u0651\u0627\u0641\u064c \u0648\u0644\u064e\u0645\u0652 \u0623\u062c\u0650\u062f\u0652 \u0634\u064a\u0626\u0627\u064b \u063a\u064a\u0631\u064e \u0642\u0627\u0628\u0644\u064d \u0644\u0644\u0633\u0624\u0627\u0644 \u0648\u0647\u0630\u0627 \u0646\u0635\u0651\u064c \u0625\u0646\u062c\u0644\u064a\u0632\u064a How are you?\"\n' '.join(arabic_punc.findall(accents.sub('',test_text)))","aac09faf":"!ls -la ","24cb55ab":"#need to bring a number of files (very similar to Nvidia ones, some minor changes for Ar - ex. tokenized texts => vocab)\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/create_pretraining_data.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/tokenization.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/file_utils.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/run_pretraining.sh\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/run_pretraining.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/modeling.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/schedulers.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/utils.py\n!wget https:\/\/raw.githubusercontent.com\/abedkhooli\/ds2\/master\/bert\/bert_config.json    ","0461c8d5":"#!pwd # = \/kaggle\/working\n!mkdir hdf5_1\n!mkdir hdf5_2\n#!ls -la","4cadf0cd":"#%%time\n# hdf5 files are created like this (phase 1): Training\n#!python create_pretraining_data.py --input_file=\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/wiki_books_training_99.txt --output_file=\/kaggle\/working\/hdf5_1\/wiki_books_training_99.hdf5 --vocab_file=\/kaggle\/input\/arabic-bert-corpus\/ar_bert32k-vocab.txt --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5","ea3cd015":"#%%time \n# hdf5 files are created like this (phase 1): Test\n#!python create_pretraining_data.py --input_file=\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/wiki_books_test_0.txt --output_file=\/kaggle\/working\/hdf5_1\/wiki_books_test_0.hdf5 --vocab_file='\/kaggle\/input\/arabic-bert-corpus\/ar_bert32k-vocab.txt' --max_seq_length=128 --max_predictions_per_seq=20 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5","026a6e39":"#!ls -la \/kaggle\/working\/hdf5_1","d6dc25fe":"# delete if you want to run batch below\n#!rm \/kaggle\/working\/hdf5_1\/wiki_books_test_0.hdf5\n#!rm \/kaggle\/working\/hdf5_1\/wiki_books_training_99.hdf5","119dc241":"#%%time \n# training phase 2\n#!python create_pretraining_data.py --input_file=\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/wiki_books_training_99.txt --output_file=\/kaggle\/working\/hdf5_2\/wiki_books_training_99.hdf5 --vocab_file=\/kaggle\/input\/arabic-bert-corpus\/ar_bert32k-vocab.txt --max_seq_length=512 --max_predictions_per_seq=80 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5","3fb3927e":"#%%time \n# test phase 2\n#!python create_pretraining_data.py --input_file=\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/wiki_books_test_0.txt --output_file=\/kaggle\/working\/hdf5_2\/wiki_books_test_0.hdf5 --vocab_file='\/kaggle\/input\/arabic-bert-corpus\/ar_bert32k-vocab.txt' --max_seq_length=512 --max_predictions_per_seq=80 --masked_lm_prob=0.15 --random_seed=12345 --dupe_factor=5","12653bb7":"#!ls -la \/kaggle\/working\/hdf5_2","043207dd":"# delete if you want to run batch below\n#!rm \/kaggle\/working\/hdf5_2\/wiki_books_test_0.hdf5\n#!rm \/kaggle\/working\/hdf5_2\/wiki_books_training_99.hdf5","fbf29df4":"# The next step is to create the hdf5 files for the two phases of training. \n# This method takes the sharded files and the BERT tokenized vocab to produce the numericalized arrays. \n# trying to generate the training hdf5 files only as kernel stops if both ...... \n# also, seems the bert_preprocessing_process.wait() is behaving badly if files take different times to process","f3dcad1e":"%%time \n# Create HDF5 files Phase 1 \nlast_process = None\nmax_seq_length = 128 # default 128\nmax_predictions_per_seq = 20 # default 20\nmasked_lm_prob = 0.15 # default 0.15\nrandom_seed = 12345 # default 12345\ndupe_factor = 5 # default 10, nvidia has it 5\ndo_lower_case = False\nvocab_file = '\/kaggle\/input\/arabic-bert-corpus\/ar_bert32k-vocab.txt' # was Google's vocab.txt file \nn_processes = 4 # default 4\nn_training_shards = 256 # default 256 \nn_test_shards = 256 # default 256\n\ndef create_record_worker(filename_prefix, shard_id, output_format='hdf5'):\n    bert_preprocessing_command = 'python create_pretraining_data.py'\n    bert_preprocessing_command += ' --input_file=' + '\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/' + filename_prefix + '_' + str(shard_id) + '.txt'\n    bert_preprocessing_command += ' --output_file='  + '\/kaggle\/working\/hdf5_1\/' + filename_prefix + '_' + str(shard_id) + '.' + output_format\n    bert_preprocessing_command += ' --vocab_file=' + vocab_file\n    bert_preprocessing_command += ' --do_lower_case' if do_lower_case else ''\n    bert_preprocessing_command += ' --max_seq_length=' + str(max_seq_length)\n    bert_preprocessing_command += ' --max_predictions_per_seq=' + str(max_predictions_per_seq)\n    bert_preprocessing_command += ' --masked_lm_prob=' + str(masked_lm_prob)\n    bert_preprocessing_command += ' --random_seed=' + str(random_seed)\n    bert_preprocessing_command += ' --dupe_factor=' + str(dupe_factor)\n    bert_preprocessing_process = subprocess.Popen(bert_preprocessing_command, shell=True)\n    last_process = bert_preprocessing_process\n        \n    print(f\"processing shard No. {icount} ...\" )\n    if icount % 50 == 0:\n        print(icount, bert_preprocessing_command)\n        \n    # This could be better optimized (fine if all take equal time)\n    if shard_id % n_processes == 0 and shard_id > 0:\n        bert_preprocessing_process.wait()\n    return last_process\n\noutput_file_prefix = 'wiki_books'\n\n# # actual training loops\n# icount = 0\n# for i in range(n_training_shards):\n#     last_process = create_record_worker(output_file_prefix + '_training', i)\n#     icount += 1\n\n# last_process.wait()\n\n# icount = 0\n# for i in range(n_test_shards):\n#     last_process = create_record_worker(output_file_prefix + '_test', i)\n#     icount += 1\n\n# last_process.wait()","f7795761":"#ctrl-A, ctrl+\/ to comment whole cell, same again to uncomment","9aaab672":"%%time \n# Create HDF5 files Phase 2\nlast_process = None\nmax_seq_length = 512 # default 512 phase 2\nmax_predictions_per_seq = 80 # default 80 phase 2\nmasked_lm_prob = 0.15 # default 0.15\nrandom_seed = 12345 # default 12345\ndupe_factor = 5 # default 10, nvidia has it 5\ndo_lower_case = False\nvocab_file = '\/kaggle\/working\/ar_bert32k-vocab.txt' # was Google's vocab.txt file \nn_processes = 4 # default 4\nn_training_shards = 256 # default 256 \nn_test_shards = 256 # default 256\n\ndef create_record_worker(filename_prefix, shard_id, output_format='hdf5'):\n    bert_preprocessing_command = 'python create_pretraining_data.py'\n    bert_preprocessing_command += ' --input_file=' + '\/kaggle\/input\/arabic-bert-corpus\/arwiki_books_shards\/content\/sharded\/' + filename_prefix + '_' + str(shard_id) + '.txt'\n    bert_preprocessing_command += ' --output_file='  + '\/kaggle\/working\/hdf5_2\/' + filename_prefix + '_' + str(shard_id) + '.' + output_format\n    bert_preprocessing_command += ' --vocab_file=' + vocab_file\n    bert_preprocessing_command += ' --do_lower_case' if do_lower_case else ''\n    bert_preprocessing_command += ' --max_seq_length=' + str(max_seq_length)\n    bert_preprocessing_command += ' --max_predictions_per_seq=' + str(max_predictions_per_seq)\n    bert_preprocessing_command += ' --masked_lm_prob=' + str(masked_lm_prob)\n    bert_preprocessing_command += ' --random_seed=' + str(random_seed)\n    bert_preprocessing_command += ' --dupe_factor=' + str(dupe_factor)\n    bert_preprocessing_process = subprocess.Popen(bert_preprocessing_command, shell=True)\n    last_process = bert_preprocessing_process\n\n  \n    if icount % 50 == 0:\n        print(icount, bert_preprocessing_command)\n    # This could be better optimized (fine if all take equal time)\n    if shard_id % n_processes == 0 and shard_id > 0:\n        bert_preprocessing_process.wait()\n    return last_process\n\noutput_file_prefix = 'wiki_books'\n\n# icount = 0\n# for i in range(n_training_shards):\n#     last_process = create_record_worker(output_file_prefix + '_training', i)\n#     icount += 1\n\n# last_process.wait()\n\n# icount = 0\n# for i in range(n_test_shards):\n#     last_process = create_record_worker(output_file_prefix + '_test', i)\n#     icount += 1\n\n# last_process.wait()","e1620263":"#!du -sh \/kaggle\/working\/hdf5_1","dd5e4f82":"#!du -sh \/kaggle\/working\/hdf5_2","96caae33":"%%bash \n# actual training: run this shell but make sure you edit it first to:\n# 1. identify where your hdf_1 and hdf2_files are (lines 37 and 162)\n# 2. specify the number of cores available ( 2 or more) - line 20\n#bash run_pretraining.sh","134f3ed6":"## Exploratory Analysis\nThere are 512 total shards in this corpus (256 training and 256 test). You can generate your vocab from them (or use the one already generated with this dataset).","f4231c44":"## Note (not done yet):    \nThis kernel stops here. You need to have enough disk space, processing power and time to generate all hdf5 files. The two phases will produce 14 GB in 4 hours.","fc06ff32":"## Introduction\nGreetings from the Arabic BERT Corpus publisher! \nThis dataset is based on Arabic Wikipedia dump (February 4, 2020) and a collection of **2080 books**. The books are mostly classic and were cleaned in a simple way to deal with bad formatting. From an original list of over 2800, some were completely removed and top\/bottom lines from others were chopped.  \nThere are two vocab files generated from with this dataset (32k and 64k). The 32k one is recommended.   \n\nThis approach is adapted from the NVIDIA PyTorch inolementation using NVIDIA hardware optimization. See https:\/\/github.com\/NVIDIA\/DeepLearningExamples\/tree\/master\/PyTorch\/LanguageModeling\/BERT","7f039cc2":"## Conclusion\nThis kernel presented an Arabic dataset that can be used to train a BERT language model. You need to generate the hdf5 files for phase 1 and phase 2 and train the model as indicated in the **intro** section.  \nIf you have questions or comments, please feel free to post them here or under the dataset. Curently, I have no access to storage or compute resources that can complete the training"}}