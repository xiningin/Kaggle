{"cell_type":{"b0b7f33e":"code","036ac9dc":"code","a358bb89":"code","612bd54c":"code","8d5c283d":"code","c6c759a5":"code","5b0bb72b":"markdown","5877d2bd":"markdown","62f98120":"markdown","e7b1fde3":"markdown"},"source":{"b0b7f33e":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport gc\nimport pickle\nimport time\nfrom joblib import dump, load\nimport lightgbm as lgb\n\nfrom sklearn.metrics import mean_squared_error","036ac9dc":"# This cell is only for displaying package versions. You can ignore it.\n\nimport pkg_resources\nimport types\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n\n        # Some packages are weird and have different\n        # imported names vs. system names\n        if name == \"PIL\":\n            name = \"Pillow\"\n        elif name == \"sklearn\":\n            name = \"scikit-learn\"\n\n        yield name\nimports = list(set(get_imports()))\n\nrequirements = []\nfor m in pkg_resources.working_set:\n    if m.project_name in imports and m.project_name!=\"pip\":\n        requirements.append((m.project_name, m.version))\n\nfor r in requirements:\n    print(\"{}=={}\".format(*r))","a358bb89":"data = pd.read_pickle('..\/input\/eda-preprocessing-feature-engineering\/all_data.pkl')\n# Dropping the first 6 months because they were used for lags\ndata = data[data.date_block_num > 5]\ntest  = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').set_index('ID')\n\n# dropping some of the columns that didn't give any improvement\ndropcols = [\n            \"item_cnt_month_lag_12\",\n            \"item_cnt_month_lag_12_adv\",\n            \"date_item_target_enc_lag_12\",\n            \"date_shop_target_enc_lag_12\",\n            \"date_city_target_enc_lag_1\",\n            \"date_city_target_enc_lag_2\",\n            \"date_city_target_enc_lag_3\",\n            \"date_type_target_enc_lag_1\",\n            \"date_subtype_target_enc_lag_1\",\n            \"new_item_cat_avg_lag_1\",\n            \"new_item_cat_avg_lag_2\",\n            \"new_item_cat_avg_lag_3\",\n            \"new_item_shop_cat_avg_lag_1\",\n            \"new_item_shop_cat_avg_lag_2\",\n            \"new_item_shop_cat_avg_lag_3\",\n           ]\n\n# Doing the time based train-val-test split\nX_train = data[data.date_block_num < 33].drop(['item_cnt_month']+dropcols, axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month']+dropcols, axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month']+dropcols, axis=1)\n\ndel data\ngc.collect()","612bd54c":"# Printing features\nX_train.info()","8d5c283d":"start_time = time.time()\n\n# https:\/\/www.kaggle.com\/tylerssssss\/feature-engineering-lightgbm\nfeature_name = X_train.columns.tolist()\n\nparams = {\n    'objective': 'mse',\n    'metric': 'rmse',\n    'num_leaves': 2 ** 7 - 1,\n    'learning_rate': 0.005,\n    'feature_fraction': 0.73,\n    'bagging_fraction': 0.75,\n    'bagging_freq': 5,\n    'seed': 0,\n    'verbose': 1\n}\n\nfeature_name_indexes = [ \n                        'country_part', \n                        'city_code',\n                        'type_code',\n                        'subtype_code',\n                        'category_code',\n                        'item_category_id', \n]\n\nlgb_train = lgb.Dataset(X_train[feature_name], Y_train)\nlgb_eval = lgb.Dataset(X_valid[feature_name], Y_valid, reference=lgb_train)\n\nevals_result = {}\ngbm = lgb.train(\n        params, \n        lgb_train,\n        num_boost_round=3000,\n        valid_sets=(lgb_train, lgb_eval), \n        feature_name = feature_name,\n        categorical_feature = feature_name_indexes,\n        verbose_eval=5, \n        evals_result = evals_result,\n        early_stopping_rounds = 100)\n\nprint(f\"Training took {time.time() - start_time} s\")\n\nstart_time = time.time()\nY_train_pred = gbm.predict(X_train).clip(0, 20)\nprint(f\"Predicting on train set took {time.time() - start_time} s\")\n\nstart_time = time.time()\nY_valid_pred = gbm.predict(X_valid).clip(0, 20)\nprint(f\"Predicting on valid set took {time.time() - start_time} s\")\n\nprint(f\"TRAIN RMSE: {round(np.sqrt(mean_squared_error(Y_train, Y_train_pred)), 5)}, VALID RMSE: {round(np.sqrt(mean_squared_error(Y_valid, Y_valid_pred)), 5)}\")\n\n\n# Saving the trained model to disk\ndump(gbm, 'lightgbm_model.joblib') ","c6c759a5":"start_time = time.time()\nY_test = gbm.predict(X_test).clip(0, 20)\nprint(f\"Predicting test set took {time.time() - start_time} s\")\n\n\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('gbm_y_test.csv', index=False)\n\ntrain_preds = pd.DataFrame({\n    \"ID\": X_train.index, \n    \"item_cnt_month\": Y_train_pred\n})\ntrain_preds.to_csv('gbm_y_train.csv', index=False)\n\nvalid_preds = pd.DataFrame({\n    \"ID\": X_valid.index, \n    \"item_cnt_month\": Y_valid_pred\n})\nvalid_preds.to_csv('gbm_y_valid.csv', index=False)\n","5b0bb72b":"# Predicting","5877d2bd":"# Imports","62f98120":"# Training","e7b1fde3":"# Loading the preprocessed data"}}