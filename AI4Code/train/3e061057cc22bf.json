{"cell_type":{"0c3ad3fa":"code","93e9ff61":"code","8a467eb5":"code","a2e39338":"code","d0a6abf6":"code","2d2e3141":"code","35e1fb0a":"code","000d8230":"code","f3665ffa":"code","9176241f":"code","54473e73":"code","84849e7b":"code","4dd06a72":"code","e3402a99":"code","cb82104e":"code","c201a0c0":"code","8f186a83":"code","70f14914":"code","00693179":"code","a87acef3":"code","b201db2b":"code","5656acad":"code","85ceeebd":"code","7846b35a":"code","1133a56a":"code","3acc73bc":"code","27d909d0":"code","4677939b":"code","7f3632e0":"code","cae1dfe2":"code","ccba9412":"code","eb556f09":"code","86b6dab2":"code","fa0652e2":"code","f4dd75a6":"code","9b030bad":"code","a9c4f464":"code","d3125678":"code","5ee331e3":"code","596d9cda":"code","48cdcb96":"code","853a5d30":"code","bf43db4b":"code","5d37aaa4":"code","d87ec675":"code","db71eb05":"code","8fbd4c9f":"code","6500b175":"code","3d43b3c3":"code","f0a26f18":"code","60ceb1bc":"code","49efb40e":"code","5ff67c68":"code","548aa1e1":"markdown","5ce76689":"markdown","baab25ee":"markdown","fcbd72c0":"markdown","9bcf84a6":"markdown","c3ac44fc":"markdown","65caba1f":"markdown","b9deb286":"markdown","4119c305":"markdown","1a2653f3":"markdown","4708a8af":"markdown","46e1b6c8":"markdown","cff2cbe2":"markdown","c57b797f":"markdown","1c04f38d":"markdown","bdf34632":"markdown","b3830528":"markdown","e370b365":"markdown","16b307ad":"markdown","246862c7":"markdown","eaf19437":"markdown","e456be35":"markdown","a7dd45ac":"markdown","1726bf41":"markdown","2ecdd342":"markdown","81de4980":"markdown","6730d4df":"markdown","62f1f6c8":"markdown","98b58e3e":"markdown","fc5589c5":"markdown","ce5acd61":"markdown","08522a7e":"markdown","93179465":"markdown","695ab757":"markdown","c539e58f":"markdown","5127622d":"markdown"},"source":{"0c3ad3fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93e9ff61":"data = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\n\n# for others analyse save we the original data\ndata_new = data.copy()\n\ndata.head()","8a467eb5":"# Data infos\ndata.info()","a2e39338":"# Data features scatter plot\nsns.set(style=\"ticks\")\nsns.pairplot(data,hue='class',diag_kind='hist',palette='husl',markers='D');","d0a6abf6":"# Staitistical infos of Data\ndata.describe().T","2d2e3141":"# Feature counts of class\ndata['class'].value_counts()","35e1fb0a":"# Visualization of class data-value counts\nsns.countplot(data['class']);","000d8230":"# NaN's feature of Data\ndata.isnull().any().sum()","f3665ffa":"# classification of class (make binary features of 0,1)\ndata['class'] = [1 if i == 'Abnormal'else 0 for i in data['class']]\n\n# labels or dependet features of Data\ny = data[['class']]\n\n# independet features of Data\nx_data = data.drop(['class'],axis = 1)\n\n# and normalization of feature\nx_norm = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\n","9176241f":"# Modul import for encode\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 'Abnormal' and 'Normal' features\nbinary_values = data_new[['class']]\n\n# model\nohe = OneHotEncoder()\n\nbinary = ohe.fit_transform(binary_values).toarray()\n\nbinary[:10]","54473e73":"# correlations between features of data\ndata.corr()","84849e7b":"# visualization data correlation\nsns.heatmap(data.corr(),annot = True,fmt='.2f',linewidths=0.5,linecolor='b')\nplt.title('Correlation Heatmap');","4dd06a72":"# now Correlation analyse with p-Value\n\n# dependet feature\ny = data[['class']]\n\n# independet features of Data\nx_data = data.drop(['class'],axis = 1)\n\nimport statsmodels.api as sm\n\n# model\nanalyse = sm.OLS(y,x_data).fit()\n\nanalyse.summary()","e3402a99":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","cb82104e":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","c201a0c0":"# modul imort\nfrom sklearn.linear_model import LogisticRegression\n\n# model\nlg = LogisticRegression()\n\n# fit\nlg.fit(x_train,y_train)\n\n# predicts\nlog_predicts = lg.predict(x_test)\n\n# accuracy with Logistic Regression Model\naccuracy_log = lg.score(x_test,y_test)\n\n# confusion metrics of Logistic Regression Model\nfrom sklearn.metrics import confusion_matrix\n\ncm_log = confusion_matrix(y_test,log_predicts)\n\n# correlation the predicts values with x_test of data\nlg_analyse = sm.OLS(log_predicts,x_test).fit()\n\nlg_analyse.summary()","8f186a83":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.484) and degree_spondylolisthesis(0.884)\n\nx_train_b = x_train.drop(['lumbar_lordosis_angle','degree_spondylolisthesis'],axis = 1)\nx_test_b = x_test.drop(['lumbar_lordosis_angle','degree_spondylolisthesis'],axis = 1)\n\n# model\nlg_b = LogisticRegression()\n\n# fit\nlg_b.fit(x_train_b,y_train)\n\n# predicts\nlog_predicts_b = lg_b.predict(x_test_b)\n\n# accuracy with Logistic Regression Model\naccuracy_log_b = lg_b.score(x_test_b,y_test)\n\n# confusion metrics of Logistic Regression Model\nfrom sklearn.metrics import confusion_matrix\n\ncm_log_b = confusion_matrix(y_test,log_predicts_b)\n\n# correlation the predicts values with x_test of data\nlg_analyse_b = sm.OLS(log_predicts_b,x_test_b).fit()\n\nlg_analyse_b.summary()\n# we have now bigger R-squared ","70f14914":" # Grid Cross Validation\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","00693179":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","a87acef3":"# modul import\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# find best k\nk_neighbors = np.arange(1,10)\nscore_list = []\nfor i in k_neighbors:\n    \n    knn_i = KNeighborsClassifier(n_neighbors = i)\n    knn_i.fit(x_train,y_train)\n    score_list.append(knn_i.score(x_test,y_test))\n    \nplt.plot(k_neighbors,score_list)\nplt.xlabel('k_number')\nplt.ylabel('score of k');","b201db2b":"# we have the best k = 6\nknn = KNeighborsClassifier(n_neighbors = 6)\n\nknn.fit(x_train,y_train)\n\nknn_predicts = knn.predict(x_test)\n\naccuracy_knn = knn.score(x_test,y_test)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_knn = confusion_matrix(y_test,knn_predicts)\n\n# correlation the predicts values with x_test of data\nknn_analyse = sm.OLS(knn_predicts,x_test).fit()\n\nknn_analyse.summary()","5656acad":"# Backward elimination of features bigger than p-value (0.05)\n# Now elimination of features : lumbar_lordosis_angle(0.088)\n\nx_train_b1 = x_train.drop(['lumbar_lordosis_angle'],axis=1)\nx_test_b1 = x_test.drop(['lumbar_lordosis_angle'],axis=1)\n\n# we have the best k = 6\nknn_b = KNeighborsClassifier(n_neighbors = 6)\n\nknn_b.fit(x_train_b1,y_train)\n\nknn_predicts_b = knn_b.predict(x_test_b1)\n\naccuracy_knn_b = knn_b.score(x_test_b1,y_test)\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_knn_b = confusion_matrix(y_test,knn_predicts_b)\n\n# correlation the predicts values with x_test of data\nknn_analyse_b = sm.OLS(knn_predicts_b,x_test_b).fit()\n\nknn_analyse_b.summary()","85ceeebd":"# Grid Cross Validation\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn= KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv = 10)  # GridSearchCV\nknn_cv.fit(x_train,y_train)\n\n#%% print hyperparameter KNN \nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",knn_cv.best_score_)","7846b35a":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","1133a56a":"# modul import\nfrom sklearn.svm import SVC\n\n# model\nsvm = SVC(random_state = 42)\n\n# fit\nsvm.fit(x_train,y_train)\n\n# predicts\nsvm_predicts = svm.predict(x_test)\n\n# accuracy\naccuracy_svm = svm.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_svm = confusion_matrix(y_test,svm_predicts)\n\n# correlation the predicts values with x_test of data\nsvm_analyse = sm.OLS(svm_predicts,x_test).fit()\n\nsvm_analyse.summary()","3acc73bc":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.334)\n\nx_train_b2 = x_train.drop(['lumbar_lordosis_angle'],axis=1)\nx_test_b2 = x_test.drop(['lumbar_lordosis_angle'],axis=1)\n\n# modul import\nfrom sklearn.svm import SVC\n\n# model\nsvm_b = SVC(random_state = 42)\n\n# fit\nsvm_b.fit(x_train_b2,y_train)\n\n# predicts\nsvm_predicts_b = svm_b.predict(x_test_b2)\n\n# accuracy\naccuracy_svm_b = svm_b.score(x_test_b2,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_svm_b = confusion_matrix(y_test,svm_predicts_b)\n\n# correlation the predicts values with x_test of data\nsvm_analyse_b = sm.OLS(svm_predicts_b,x_test_b).fit()\n\nsvm_analyse_b.summary()","27d909d0":"# model\nsvm = SVC(random_state = 42)\n\n# fit\nsvm.fit(x_train,y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = svm, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n","4677939b":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","7f3632e0":"# modul import\nfrom sklearn.naive_bayes import GaussianNB\n\n# model\nnb = GaussianNB()\n\n# fit\nnb.fit(x_train,y_train)\n\n# predicts\nnb_predicts = nb.predict(x_test)\n\n# accuracy\naccuracy_nb = nb.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_nb = confusion_matrix(y_test,nb_predicts)\n\n# correlation the predicts values with x_test of data\nnb_analyse = sm.OLS(nb_predicts,x_test).fit()\n\nnb_analyse.summary()","cae1dfe2":"# model\nnb = GaussianNB()\n\n# fit\nnb.fit(x_train,y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = nb, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","ccba9412":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","eb556f09":"# modul import\nfrom sklearn.tree import DecisionTreeClassifier\n\n# model\nd_tree = DecisionTreeClassifier()\n\n# fit\nd_tree.fit(x_train,y_train)\n\n# predicts\nd_tree_predicts = d_tree.predict(x_test)\n\n# accuracy\naccuracy_d_tree = d_tree.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_tree = confusion_matrix(y_test,d_tree_predicts)\n\n# correlation the predicts values with x_test of data\nd_tree_analyse = sm.OLS(d_tree_predicts,x_test).fit()\n\nd_tree_analyse.summary()","86b6dab2":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.992),pelvic_radius(0.384)\n\nx_train_b3 = x_train.drop(['lumbar_lordosis_angle','pelvic_radius'],axis=1)\nx_test_b3 = x_test.drop(['lumbar_lordosis_angle','pelvic_radius'],axis=1)\n\n# modul import\nfrom sklearn.tree import DecisionTreeClassifier\n\n# model\nd_tree_b = DecisionTreeClassifier()\n\n# fit\nd_tree_b.fit(x_train_b3,y_train)\n\n# predicts\nd_tree_predicts_b = d_tree_b.predict(x_test_b3)\n\n# accuracy\naccuracy_d_tree_b = d_tree_b.score(x_test_b3,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_tree_b = confusion_matrix(y_test,d_tree_predicts_b)\n\n# correlation the predicts values with x_test of data\nd_tree_analyse_b = sm.OLS(d_tree_predicts_b,x_test_b3).fit()\n\nd_tree_analyse_b.summary()\n","fa0652e2":"# model\nd_tree = DecisionTreeClassifier()\n\n# fit\nd_tree.fit(x_train,y_train)\n\nfrom sklearn.model_selection import cross_val_score\n\n\naccuracies = cross_val_score(estimator = d_tree, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))","f4dd75a6":"# for training ours model, splittin as train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_norm,y,test_size = 0.25,random_state = 42)","9b030bad":"# modul import\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model\nr_forest = RandomForestClassifier(n_estimators = 100,random_state = 42)\n\n# fit\nr_forest.fit(x_train,y_train)\n\n# predicts\nr_forest_predicts = r_forest.predict(x_test)\n\n# accuracy\naccuracy_r_forest = r_forest.score(x_test,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_forest = confusion_matrix(y_test,r_forest_predicts)\n\n# correlation the predicts values with x_test of data\nr_forest_analyse = sm.OLS(r_forest_predicts,x_test).fit()\n\nr_forest_analyse.summary()","a9c4f464":"# Backward elimination of features bigger than p-value (0.05)\n# Elimination of features : lumbar_lordosis_angle(0.693)\n\nx_train_b4 = x_train.drop(['lumbar_lordosis_angle'],axis=1)\nx_test_b4 = x_test.drop(['lumbar_lordosis_angle'],axis=1)\n\n# modul import\nfrom sklearn.ensemble import RandomForestClassifier\n\n# model\nr_forest_b = RandomForestClassifier(n_estimators = 100,random_state = 42)\n\n# fit\nr_forest_b.fit(x_train_b4,y_train)\n\n# predicts\nr_forest_predicts_b = r_forest_b.predict(x_test_b4)\n\n# accuracy\naccuracy_r_forest_b = r_forest_b.score(x_test_b4,y_test)\n\n# confusion metrics\nfrom sklearn.metrics import confusion_matrix\n\ncm_forest_b = confusion_matrix(y_test,r_forest_predicts_b)\n\n# correlation the predicts values with x_test of data\nr_forest_analyse_b = sm.OLS(r_forest_predicts_b,x_test_b).fit()\n\nr_forest_analyse_b.summary()","d3125678":"grid = {\"n_estimators\":np.arange(1,50)}\nrandom_f= RandomForestClassifier()\n\nrandom_cv = GridSearchCV(random_f, grid, cv = 10)  # GridSearchCV\nrandom_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameter K: \",random_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",random_cv.best_score_)","5ee331e3":"accuracys = ({'accuracy_log':accuracy_log,'accuracy_knn':accuracy_knn,\n             'accuracy_svm':accuracy_svm,'accuracy_nb':accuracy_nb,\n             'accuracy_d_tree':accuracy_d_tree,'accuracy_r_forest':accuracy_r_forest})\n\n#confusions = [cm_log,cm_knn,cm_svm,cm_nb,cm_tree,cm_forest]\n\naccuracy = pd.DataFrame.from_dict(accuracys,orient='index')\n\nplt.figure(figsize = (10,6))\n\nplt.plot(accuracy,color='r')\nplt.scatter(accuracy.index,accuracy.values);\n             ","596d9cda":"# confusions of Models\nconfusions = {'cm_log':cm_log,'cm_knn':cm_knn,'cm_svm':cm_svm,'cm_nb':cm_nb,'cm_tree':cm_tree,'cm_forest':cm_forest}\n\nfor i,j in confusions.items():\n\n    sns.heatmap(j,annot=True,fmt='.0f',linewidths=0.5,linecolor='r')\n    plt.title('Confusion of {}'.format(i))\n    plt.show()","48cdcb96":"# confusion performans of models\n\n# number of normal features is 21\nn_normal = 21\n\n# number of abnormal is 57\nn_abnormal = 57\n\n# True predicts of normal is true\/21 and for abnormal true\/57\n# performans is equal : true\/21 + true\/57\n\nconfus_performans = []\n\nfor i ,j  in confusions.items():\n    \n    performans_normal = j[0][0] \/ n_normal\n    \n    performans_abnormal = j[1][1] \/ n_abnormal\n    \n    total_performans = performans_normal + performans_abnormal\n    \n    confus_performans.append((i,total_performans))\n    \nconfus_performans\n    ","853a5d30":"# Best cunfusion performans\n\nconf = pd.DataFrame(confus_performans)\n\nconf.set_index(0)\n\nplt.plot(conf[0],conf[1],color = 'r')\nplt.scatter(conf[0],conf[1]);\nplt.title('True predicts of Normal vs Abnormal')\nplt.ylabel('Sum of True predicts');","bf43db4b":"accuracys_b = ({'accuracy_log_b':accuracy_log_b,'accuracy_knn_b':accuracy_knn_b,\n             'accuracy_svm_b':accuracy_svm_b,'accuracy_nb':accuracy_nb,\n             'accuracy_d_tree_b':accuracy_d_tree_b,'accuracy_r_forest_b':accuracy_r_forest_b})\n\naccuracy_b = pd.DataFrame.from_dict(accuracys_b,orient='index')\n\nplt.figure(figsize = (16,6))\n\nplt.plot(accuracy_b,color='g')\nplt.scatter(accuracy_b.index,accuracy_b.values);\n\n","5d37aaa4":"# confusions of Models with Backward Elimination\nconfusions_b = {'cm_log_b':cm_log_b,'cm_knn_b':cm_knn_b,'cm_svm_b':cm_svm_b,'cm_nb':cm_nb,'cm_tree_b':cm_tree_b,'cm_forest_b':cm_forest_b}\n\nfor i,j in confusions_b.items():\n\n    sns.heatmap(j,annot=True,fmt='.0f',linewidths=0.5,linecolor='r',cmap=\"Greens\")\n    plt.title('Confusion of {}'.format(i))\n    plt.show()","d87ec675":"# confusion performans of models with Backwards Elimination\n\n# number of normal features is 21\nn_normal_b = 21\n\n# number of abnormal is 57\nn_abnormal_b = 57\n\n# True predicts of normal is true\/21 and for abnormal true\/57\n# performans is equal : true\/21 + true\/57\n\nconfus_performans_b = []\n\nfor i ,j  in confusions_b.items():\n    \n    performans_normal_b = j[0][0] \/ n_normal_b\n    \n    performans_abnormal_b = j[1][1] \/ n_abnormal_b\n    \n    total_performans_b = performans_normal_b + performans_abnormal_b\n    \n    confus_performans_b.append((i,total_performans_b))\n    \nconfus_performans_b","db71eb05":"# Best cunfusion performans with Backward Elimination\n\nconf_b = pd.DataFrame(confus_performans_b)\n\nconf_b.set_index(0)\n\nplt.plot(conf_b[0],conf_b[1],color = 'green')\nplt.scatter(conf_b[0],conf_b[1]);\nplt.title('True predicts of Normal vs Abnormal')\nplt.ylabel('Sum of True predicts');","8fbd4c9f":"# Now compare the confusion performans of models vs models with backward elimination\n\ncompare = list(zip((confus_performans[:6],confus_performans_b)))\n\nprint('The first list is for Models,and second list for Models with Bacward Elimination:\\n')\nfor i in range(len(compare)):\n    print('List {} : {}'.format(i+1,compare[i]))\n    ","6500b175":"# parameters for cros validation\nrandom_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(probability=True,random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","3d43b3c3":"# Best parameters\ncv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = (GridSearchCV(\n                       #probability=True,\n                       classifier[i],\n                       param_grid=classifier_param[i],\n                       cv = StratifiedKFold(n_splits = 10),\n                       scoring = \"accuracy\", n_jobs = -1,verbose = 1))\n    \n    clf.fit(x_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","f0a26f18":"# Visualisation\ncv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n\ng = sns.barplot(\"Cross Validation Means\",\"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\");","60ceb1bc":"# the bests\nvotingC = VotingClassifier(estimators = [(\"SVM\",best_estimators[1]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3]),\n                                        ('knn',best_estimators[4])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(x_train, y_train)\n","49efb40e":"y_predict = votingC.predict(x_test)","5ff67c68":"accuracy_score(y_predict,y_test)","548aa1e1":"* <span style=\"color:deepskyblue\">Categorical Variable: <\/span>class\n* <span style=\"color:deepskyblue\">Numerical Variable: <\/span>pelvic_incidence,pelvic_tilt,numeric,lumbar_lordosis_angle,sacral_slope,pelvic_radius,degree_spondylolisthesis \n","5ce76689":"<a id='5'><\/a>\n# Modeling together with Backward Elimination","baab25ee":"<a id='1'><\/a>\n# Load and Check Data","fcbd72c0":"<a id='8'><\/a>\n## Support Vector Machine Model","9bcf84a6":"* **Random Forest with Backward Elimination**","c3ac44fc":"<a id='13'><\/a>\n## Analyse accuracy and confusion of Models with Backwards Elimination","65caba1f":"<a id='4'><\/a>\n# Train Test Split","b9deb286":"<a id='10'><\/a>\n## Decision Tree Model","4119c305":"* **K-Fold Cross Validation**","1a2653f3":"* **KNN Model with Backward Elimination** ","4708a8af":"* Second Method for binary features","46e1b6c8":"* **C and Penalty with Grid Cross Validation**","cff2cbe2":"* **K-Value with Grid Cross Validation**","c57b797f":"* **Support Vector Machine with Bacward Elimination**","1c04f38d":"* **K-Fold Cross Validation**","bdf34632":"<a id='2'><\/a>\n# Variable Analyse","b3830528":"* **N-estimators value with Grid Cross Validation**","e370b365":"* **","16b307ad":"<a id='3'><\/a>\n## Numerical Variable and Categorical Variable","246862c7":"* **Naive Bayes Classification Model with Backward Elimination**\n* Now we have not features than p_Value is bigger than 0.05 !","eaf19437":"<a id='7'><\/a>\n## KNN Classification Model","e456be35":"*     **Logistic Regression Model with Backward Elimination !**","a7dd45ac":"* Features who have bigger p-Value is : pelvic_incidence,pelvic_tilt numeric,sacral_slope !","1726bf41":"* **Decision Tree Model with Backward Elimination**","2ecdd342":"* Of graph seems that max is by cm_nb - naive bayes models","81de4980":"<a id='14'><\/a>\n# Hyperparameter Tunning,CrossValidation,GridCross","6730d4df":"<a id='9'><\/a>\n## Naive Bayes Classification Model","62f1f6c8":"<a id='12'><\/a>\n# Analyse the best model with accuracy and confusion","98b58e3e":"* class have with pelvic_radius negativ correlation","fc5589c5":"<a id='11'><\/a>\n## Random Forest Model","ce5acd61":"# Introduction\n\nAnalyse the Biomechanical Data with all Model Classifiers!\n\n<font color='blue'>\nContent:\n\n1. [Load and check Data](#1)<br>\n1. [Variable Analyse](#2)\n   * [Numerical Variable and Categorical Variable](#3)\n1. [Train Test Split](#4)    \n1. [Modeling together with Backward Elimination](#5)\n    * [Logistic Regression Model](#6)\n    * [KNN Classification Model](#7)\n    * [Support Vector Machine Model](#8)\n    * [Naive Bayes Classification Model](#9)\n    * [Decision Tree Model](#10)\n    * [Random Forest Model](#11)\n1. [Analyse the best model with accuracy and confusion](#12)\n    * [Analyse accuracy and confusion of Models with Backwards Elimination](#13)\n1. [Hyperparameter Tunning,Cross Validation,GridCross](#14)\n    * [Ensemble Model](#15)","08522a7e":"* **K-Fold Cross Validation**","93179465":"Dtypes: <br>\n\n* <span style=\"color:deepskyblue\"> Float64(2):<\/span> pelvic_incidence, pelvic_tilt numeric,lumbar_lordosis_angle,sacral_slope,pelvic_radius,degree_spondylolisthesis  <br>\n* <span style=\"color:deepskyblue\"> object(5):<\/span>class  <br>","695ab757":"* From Graph seems that Best Accuracy have Random Forest Model","c539e58f":"<a id='6'><\/a>\n## Logistic Regression Model","5127622d":"<a id='15'><\/a>\n## Ensemble Modeling"}}