{"cell_type":{"74d1eb73":"code","3e5d1cc0":"code","e36eec21":"code","b469c11b":"code","1f60825a":"code","57bfae1b":"code","c9eeb6e1":"code","32f8b3df":"code","595b9008":"code","8c03b850":"code","08edf6d5":"markdown","29f28f60":"markdown","c2350c24":"markdown","68b6fc84":"markdown","e646844d":"markdown","f3b12fa5":"markdown","1347afe4":"markdown","5d336daa":"markdown","8ef3f95f":"markdown","893b69ab":"markdown","af99e4ef":"markdown"},"source":{"74d1eb73":"import itertools\nimport numpy as np\nimport pandas as pd\n\n\n!wget http:\/\/webhotel4.ruc.dk\/~keld\/research\/LKH-3\/LKH-3.0.7.tgz\n!tar xvfz LKH-3.0.7.tgz\n!cd LKH-3.0.7; make; cp LKH ..","3e5d1cc0":"LETTERS = {\n    1: '\ud83c\udf85',  # father christmas\n    2: '\ud83e\udd36',  # mother christmas\n    3: '\ud83e\udd8c',  # reindeer\n    4: '\ud83e\udddd',  # elf\n    5: '\ud83c\udf84',  # christmas tree\n    6: '\ud83c\udf81',  # gift\n    7: '\ud83c\udf80',  # ribbon\n    8: '\ud83c\udf1f',  # star\n}\nINV_LETTERS = {v: k for k, v in LETTERS.items()}\n\nsolution = pd.read_csv('..\/input\/santa-baseline\/submission_baseline.csv')\nstrings = [[INV_LETTERS[c] for c in s] for s in solution.schedule]\nstrings.sort(key=len, reverse=True)\nprint(f'Strings lengths are {[len(_) for _ in strings]}.')","e36eec21":"def find_strings_perms(strings, verbose=False):\n    all_perms = set(itertools.permutations(range(1, 8), 7))\n    perms = []\n    for s in strings:\n        perms.append([])\n        for i in range(len(s)-6):\n            p = tuple(s[i:i+7])\n            if p in all_perms:\n                perms[-1].append(p)\n    if verbose:\n        lens = [len(_) for _ in  perms]\n        print(f'There are {lens} permutations in strings, {sum(lens)} in total.')\n        lens = [len(set(_)) for _ in  perms]\n        print(f'There are {lens} unique permutations in strings, {sum(lens)} in total.')\n    return perms\n\nstrings_perms = find_strings_perms(strings, verbose=True)","b469c11b":"def rebalance_perms(strings_perms, verbose=False):\n    # convert to dicts for fast lookup and to keep permutations order\n    strings_perms = [dict.fromkeys(_) for _ in strings_perms] \n    for p in strings_perms[0].copy():  # iterate over the copy to allow modification during iteration\n        if p[:2] != (1, 2) and (p in strings_perms[1] or p in strings_perms[2]):\n            strings_perms[0].pop(p)\n    for p in strings_perms[1].copy():\n        if p[:2] != (1, 2) and p in strings_perms[2]:\n            strings_perms[1].pop(p)\n    if verbose:\n        lens = [len(_) for _ in  strings_perms]\n        print(f'There are {lens} permutations left in strings after rebalancing, {sum(lens)} in total.')\n    return [list(_) for _ in strings_perms] \n\nstrings_perms = rebalance_perms(strings_perms, verbose=True)","1f60825a":"def perm_dist(p, q):\n    i = p.index(q[0])\n    return i if p[i:] == q[:7-i] else 7\n\ndef perms_to_string(perms):\n    perms = list(perms)\n    s = [*perms[0]]\n    for p, q in zip(perms, perms[1:]):\n        d = perm_dist(p, q)\n        s.extend(q[-d:])\n    return s\n\ndef distances_matrix(perms):\n    m = np.zeros((len(perms), len(perms)), dtype='int8')\n    for i, p in enumerate(perms):\n        for j, q in enumerate(perms):\n            m[i, j] = perm_dist(p, q)\n    return m\n\ndef write_params_file():\n    with open('santa.par', 'w') as f:\n        print('PROBLEM_FILE = santa.atsp', file=f)\n        print('TOUR_FILE = best_tour.txt', file=f)\n        print('INITIAL_TOUR_FILE = initial_tour.txt', file=f)\n        print('PATCHING_C = 4', file=f)\n        print('PATCHING_A = 3', file=f)\n        print('GAIN23 = YES', file=f)\n        print('SEED = 42', file=f)\n        print('MAX_TRIALS = 100000', file=f)\n        print('TIME_LIMIT = 300', file=f) #seconds\n        print('TRACE_LEVEL = 1', file=f)\n\ndef write_problem_file(distances):\n    with open('santa.atsp', 'w') as f:\n        print('TYPE: ATSP', file=f)\n        print(f'DIMENSION: {len(distances)}', file=f)\n        print('EDGE_WEIGHT_TYPE: EXPLICIT', file=f)\n        print('EDGE_WEIGHT_FORMAT: FULL_MATRIX\\n', file=f)\n        print('EDGE_WEIGHT_SECTION', file=f)\n        for row in distances:\n            print(' '.join(str(_) for _ in row), file=f)\n\ndef write_initial_tour_file(perms):\n    with open('initial_tour.txt', 'w') as f:\n        print('TOUR_SECTION', file=f)\n        print(' '.join(str(_) for _ in range(1, len(perms)+1)), -1, file=f)\n\ndef read_output_tour(perms):\n    perms = list(perms)\n    with open('best_tour.txt') as f:\n        lines = f.readlines()\n    tour = lines[lines.index('TOUR_SECTION\\n')+1:-2]\n    return [perms[int(_) - 1] for _ in tour] \n    \ndef solve_atsp(perms, verbose=False):\n    write_params_file()\n    distances = distances_matrix(perms)\n    write_problem_file(distances)\n    write_initial_tour_file(perms)\n    \n    # Run LKH-3 to solve ATSP instance\n    if verbose:\n        !.\/LKH santa.par\n    else:\n        !touch lkh.log\n        !.\/LKH santa.par >> lkh.log\n    tour = read_output_tour(perms)\n    return perms_to_string(tour)","57bfae1b":"improved, old_lens = True, [len(_) for _ in strings]\nwhile improved:\n    print('='*91)\n    new_strings = [solve_atsp(_) for _ in strings_perms]\n    new_strings.sort(key=len, reverse=True)\n    new_lens = [len(_) for _ in new_strings]\n    if new_lens < old_lens:\n        print(f'Improved strings lengths from {old_lens} to {new_lens}.')\n        strings, old_lens = new_strings, new_lens\n        strings_perms = find_strings_perms(strings, verbose=True)\n        strings_perms = rebalance_perms(strings_perms, verbose=True)\n    else:\n        improved = False","c9eeb6e1":"all_perms = set(itertools.permutations(range(1, 8), 7))\nmandatory_perms = set((1, 2) +  _ for _ in itertools.permutations(range(3, 8), 5))\n\nstrings_perms = [set(_) for _ in find_strings_perms(strings)]\nfor i, s in enumerate(strings_perms):\n    if mandatory_perms - s:\n        print(f'String #{i} is missing {mandatory_perms - s}.')\nif all_perms - set.union(*strings_perms):\n    print(f'Strings are missing {all_perms - set.union(*strings_perms)}.')","32f8b3df":"sub = pd.DataFrame()\nsub['schedule'] = [''.join(LETTERS[x] for x in s) for s in strings]\nsub_name = f'submission_no_wildcards_{\"_\".join(str(len(_)) for _ in strings)}.csv'\nsub.to_csv(sub_name, index=False)","595b9008":"import torch\nimport torch.nn.functional as F\n\n\nperms = list(map(lambda p: \"\".join(p), itertools.permutations(\"1234567\")))\nperm2id = {p: i for i, p in enumerate(perms)}\n\nperms_arr = np.array([list(map(int, p)) for p in perms])\nperms_arr.shape\n\nperms_onehot = np.eye(7)[perms_arr-1, :].transpose(0, 2, 1)\nassert np.allclose(perms_onehot[:,0,:].astype(np.int64), (perms_arr == 1).astype(np.int64))\n\n# print(\"onehot 1234567:\")\n# print(perms_onehot[perm2id[\"1234567\"]])\n\n# print(\"onehot 5671234:\")\n# print(perms_onehot[perm2id[\"5671234\"]])\n\n# print(\"correlate between 1234567 and 5671234\")\nleft = perms_onehot[perm2id[\"1234567\"]]\nright = perms_onehot[perm2id[\"5671234\"]]\nmatches = F.conv2d(\n    F.pad(torch.Tensor(left[None, None, :, :]), (7, 7)),\n    torch.Tensor(right[None, None, :, :]),\n    padding=\"valid\"\n).numpy().reshape(-1)\n# print(matches)\nmust_match_left2right = np.array([-1, -1, -1, -1, -1, -1, -1, 7, 6, 5, 4, 3, 2, 1, 0])\nmust_match_right2left = np.array([0, 1, 2, 3, 4, 5, 6, 7, -1, -1, -1, -1, -1, -1, -1])\ncost_ifmatch = np.array([7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7])\n# print(\"cost of 1234567 -> 5671234:\", min(cost_ifmatch[np.equal(must_match_left2right, matches)]))\n# print(\"cost of 5671234 -> 1234567:\", min(cost_ifmatch[np.equal(must_match_right2left, matches)]))\n\n\nM = F.conv2d(\n    F.pad(torch.Tensor(perms_onehot[:, None, :, :]), (7, 7)),\n    torch.Tensor(perms_onehot[:, None, :, :]),\n    padding=\"valid\"\n).squeeze().numpy()\n\n# M.shape\n\n\nmust_match_left2right = np.array([-1, -1, -1, -1, -1, -1, -1, 7, 6, 5, 4, 3, 2, 1, 0])\nmust_match_left2right_wild = np.array([-1, -1, -1, -1, -1, -1, -1, 6, 5, 4, 3, 2, 1, 0, 0])\n\ncost_ifmatch = np.array([7, 6, 5, 4, 3, 2, 1, 0, 1, 2, 3, 4, 5, 6, 7])\n\ncostMat = np.where(M == must_match_left2right, cost_ifmatch, np.inf).min(axis=-1).astype(np.int8)\ncostMatWild = np.minimum(costMat, np.where(M == must_match_left2right_wild, cost_ifmatch, np.inf).min(axis=-1)).astype(np.int8)\n\nsymbols = \"\ud83c\udf85\ud83e\udd36\ud83e\udd8c\ud83e\udddd\ud83c\udf84\ud83c\udf81\ud83c\udf80\"\nschedule = sub.schedule\nwords = [s.translate(str.maketrans(symbols, \"1234567\")) for s in schedule]\n\nnodes_list = []\ntable_list = []\nfor i in range(3):\n    word = words[i]\n    nodes = []\n    for i in range(len(word)-6):\n        p = word[i:i+7]\n        if p in perm2id:\n            nodes.append(perm2id[p])\n    table = np.zeros((len(nodes), 10), np.int64)\n    table[0, :] = 7\n    for i in range(1, len(nodes)):\n        e = costMat[nodes[i-1], nodes[i]]\n        ew = costMatWild[nodes[i-1], nodes[i]]\n        table[i,0] = table[i-1,0] + e\n        table[i,1] = min(table[i-1,1] + e, table[i-1,0] + ew)\n        table[i,2] = min(table[i-1,2], table[i-1,1]) + e # TODO: better transition\n        table[i,3] = min(table[i-1,3], table[i-1,2]) + e\n        table[i,4] = min(table[i-1,4], table[i-1,3]) + e\n        table[i,5] = min(table[i-1,5], table[i-1,4]) + e\n        table[i,6] = min(table[i-1,6], table[i-1,5]) + e\n        table[i,7] = min(table[i-1,7], table[i-1,6]) + e\n        table[i,8] = min(table[i-1,8], table[i-1,7]) + e\n        table[i,9] = min(table[i-1,9] + e, table[i-1,8] + ew)\n#     print(table[-1].min(), table[-1])\n    nodes_list.append(nodes)\n    table_list.append(table)\n    \n# backtrack\nnew_words = []\nwilds = []\nfor nodes, table in zip(nodes_list, table_list):\n    ns = [perms[nodes[-1]]]\n    track = np.argmin(table[-1])\n    wild = []\n    for i in range(len(nodes)-2, -1, -1):\n        e = costMat[nodes[i], nodes[i+1]]\n        ew = costMatWild[nodes[i], nodes[i+1]]\n        if track == 0:\n            ns.append(perms[nodes[i]][:e])\n        elif track == 1:\n            if table[i, 1] + e < table[i, 0] + ew:\n                ns.append(perms[nodes[i]][:e])\n            else:\n                left = np.array(list(map(int, perms[nodes[i]][ew:])))\n                right = np.array(list(map(int, perms[nodes[i+1]][:-ew])))\n                mis = np.where(left != right)[0][0]\n                wild.append(table[i, track-1]-7+ew+mis)\n                ns.append(perms[nodes[i]][:ew])\n                track = track - 1\n        elif 2 <= track <= 8:\n            if table[i, track] >= table[i, track-1]:\n                track = track - 1\n            ns.append(perms[nodes[i]][:e])\n        elif track == 9:\n            if table[i, 9] + e < table[i, 8] + ew:\n                ns.append(perms[nodes[i]][:e])\n            else:\n                ns.append(perms[nodes[i]][:ew])\n                left = np.array(list(map(int, perms[nodes[i]][ew:])))\n                right = np.array(list(map(int, perms[nodes[i+1]][:-ew])))\n                mis = np.where(left != right)[0][0]\n                wild.append(table[i, track-1]-7+ew+mis)\n                track = track - 1\n        else:\n            assert False\n    assert track == 0\n    wilds.append(wild)\n    nsw = list(\"\".join(ns[::-1]))\n    for w in wild:\n        nsw[w] = \"8\"\n    new_words.append(\"\".join(nsw))\n    \nstrings = [[int(_) for _ in s] for s in new_words]\nstrings.sort(key=len, reverse=True)\nnew_lens = [len(_) for _ in strings]\nprint(f'Improved strings lengths from {old_lens} to {new_lens}.')","8c03b850":"sub = pd.DataFrame()\nsub['schedule'] = [''.join(LETTERS[x] for x in s) for s in strings]\nsub_name = f'submission_with_wildcards_{\"_\".join(str(len(_)) for _ in strings)}.csv'\nsub.to_csv(sub_name, index=False)","08edf6d5":"# Peek closely at the permutations in each string","29f28f60":"# Wildcards optimization\n\nWe'll use the code from the [notebook](https:\/\/www.kaggle.com\/yosshi999\/wildcard-postprocessing-using-dynamic-programming) created by [Yosshi999](https:\/\/www.kaggle.com\/yosshi999) to improve our solution with wildcards.","c2350c24":"# Final thoughts\n\nWhat one can also try is to use smarter rebalancing of permutations between strings, instead of just moving duplicates around we can somehow find permutations in one string that 'belong' more to the other string.\n\nThat's it, thank you for reading, please upvote if you find it useful.","68b6fc84":"Wow, we improved the solution quite a bit. Let's check it's the correct one and we haven't lost any permutations along the way.","e646844d":"# Load the solution\n\nAt the moment of writing this notebook such a solution is the [one](https:\/\/www.kaggle.com\/cdeotte\/santa-2021-tsp-baseline-2500\/notebook) created by the [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte). Let's load it. ","f3b12fa5":"As everything is ok, now we'll save the found solution and then try to improve it even more using wildcards.","1347afe4":"# Permutations rebalancing\n\n5772 permutations and 5709 unique permutations is more than 5280 that should be placed in the strings, there are some duplicates, let's leave each duplicate in the shortest string it occurs in.","5d336daa":"# Santa Movie Challenge as Travelling Salesman Problem\n\nThis notebook is an attempt to improve the best currently published solution that doesn't use wildcards by peeking closely to repeating permutations in it and trying to rebalance them.","8ef3f95f":"# Use LKH to solve ATSP\n\nWe'll use [LKH-3](http:\/\/webhotel4.ruc.dk\/~keld\/research\/LKH-3\/) to try to improve current solution and will pass current solution to LKH as initial tour to start optimization from.\n\nWe'll keep rebalancing permutations and feeding it again to LKH while improvements are made.","893b69ab":"We've used wildcards, let's save this solution.","af99e4ef":"Well, 1647 and 1696 is much less than 1884 and 1888. Let's feed it to the LKH solver again."}}