{"cell_type":{"85d75da3":"code","dd642a20":"code","f0bfbf9b":"code","e324541f":"code","398a0f95":"code","760c7f19":"code","0305e1af":"code","c242618f":"code","39d65abb":"code","c06a27c2":"code","478e672e":"code","8f199baa":"code","b8c22199":"code","7449b8e0":"code","37132d61":"code","7b34d7b7":"code","c57b34a3":"code","8fbd391c":"code","f4d60e57":"code","72f2dea1":"code","6e5e0bd3":"code","da61072c":"code","8b9ca19a":"code","67443d27":"code","75bc1170":"code","f6175bdd":"code","3ef4fb06":"code","77244f32":"code","32f56989":"code","d84bcc21":"markdown","eeecb4ef":"markdown","ee3aa96d":"markdown","f3c0a246":"markdown","1e4edb95":"markdown","c589f97f":"markdown","a819de94":"markdown","7b63a955":"markdown"},"source":{"85d75da3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dd642a20":"pd.set_option(\"display.max_columns\", 200)\npd.set_option(\"display.max_rows\", 200)","f0bfbf9b":"country_info = pd.read_csv(\"\/kaggle\/input\/countryinfo\/covid19countryinfo.csv\")\ncountry_info = country_info.drop([col for col in country_info.columns if \"Unnamed\" in col], axis=1)\ncountry_info[\"pop\"] = country_info[\"pop\"].str.replace(',', '').astype(float)\n\npollution = pd.read_csv(\"\/kaggle\/input\/pollution-by-country-for-covid19-analysis\/region_pollution.csv\")\npollution = pollution.rename({\"Region\": \"country\",\n                             \"Outdoor Pollution (deaths per 100000)\": \"outdoor_pol\",\n                             \"Indoor Pollution (deaths per 100000)\": \"indoor_pol\"}, axis=1)\n\neconomy = pd.read_csv(\"\/kaggle\/input\/the-economic-freedom-index\/economic_freedom_index2019_data.csv\", engine='python')\neconomy_cols = [col for col in economy.columns if economy[col].dtype == \"float64\"] + [\"Country\"]\neconomy = economy[economy_cols]\neconomy = economy.rename({\"Country\": \"country\"}, axis=1)\n\ndef append_external_data(df):\n    df = pd.merge(df, country_info, on=\"country\", how=\"left\")\n    df = pd.merge(df, pollution, on=\"country\", how=\"left\")\n    df = pd.merge(df, economy, on=\"country\", how=\"left\")\n    return df","e324541f":"country_info.head()","398a0f95":"def aggregate_label(df):\n    country_df = df[[\"country\", \"Date\", \"ConfirmedCases\", \"Fatalities\"]].groupby([\"country\", \"Date\"], as_index=False).sum()\n    country_df = country_df.rename({\"ConfirmedCases\": \"country_cases\", \"Fatalities\": \"country_fatalities\"}, axis=1)\n    df = pd.merge(df, country_df, on=[\"country\", \"Date\"], how=\"left\")\n    return df","760c7f19":"def calculate_days_since_event(df, feature_name, casualties, casualties_amount, groupby=[\"country\"]):\n    cases_df = df.loc[df[casualties] > casualties_amount][groupby + [\"Date\"]].groupby(groupby, as_index=False).min()\n    cases_df = cases_df.rename({\"Date\": \"relevant_date\"}, axis=1)\n    df = pd.merge(df, cases_df, on=groupby, how=\"left\")\n    df[feature_name] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"relevant_date\"])).dt.days\n    df.loc[df[feature_name] < 0, feature_name] = 0\n    df = df.drop(\"relevant_date\", axis = 1)\n    return df","0305e1af":"def generate_time_features(df):\n    df = calculate_days_since_event(df, \"days_from_first_death\", \"Fatalities\", 0, [\"country\"])\n    df = calculate_days_since_event(df, \"days_from_first_case\", \"ConfirmedCases\", 0, [\"country\"])\n    df = calculate_days_since_event(df, \"days_from_first_case_province\", \"ConfirmedCases\", 0, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_first_death_province\", \"Fatalities\", 0, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_case\", \"ConfirmedCases\", 99, [\"country\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_case_province\", \"Fatalities\", 99, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_daily_cases_province\", \"ConfirmedCases_daily\", 99, [\"country\", \"state\"])\n    df = calculate_days_since_event(df, \"days_from_centenary_daily_cases\", \"ConfirmedCases_daily\", 99, [\"country\"])\n    \n    # Days from first detected case\n    df[\"days_from_first_ever_case\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(\"2019-12-01\")).dt.days\n    df.loc[df[\"days_from_first_ever_case\"] < 0, \"days_from_first_ever_case\"] = 0\n    \n    #Days from quarantine, school closures and restrictions\n    df[\"days_from_quarantine\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"quarantine\"])).dt.days\n    df[\"days_from_quarantine\"].fillna(0)\n    df.loc[df[\"days_from_quarantine\"] < 30, \"days_from_quarantine\"] = 0\n    df.loc[df[\"days_from_quarantine\"] >= 30, \"days_from_quarantine\"] = 1\n\n    df[\"days_from_school\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"schools\"])).dt.days\n    df[\"days_from_school\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_school\"] < 30, \"days_from_school\"] = 0\n    df.loc[df[\"days_from_school\"] >= 30, \"days_from_school\"] = 1\n\n    df[\"days_from_publicplace\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"publicplace\"])).dt.days\n    df[\"days_from_publicplace\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_publicplace\"] < 30, \"days_from_publicplace\"] = 0\n    df.loc[df[\"days_from_publicplace\"] >= 30, \"days_from_publicplace\"] = 1\n    \n    df[\"days_from_gathering\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"gathering\"])).dt.days\n    df[\"days_from_gathering\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_gathering\"] < 30, \"days_from_gathering\"] = 0\n    df.loc[df[\"days_from_gathering\"] >= 30, \"days_from_gathering\"] = 1\n    \n    df[\"days_from_nonessential\"] = (pd.to_datetime(df[\"Date\"]) - pd.to_datetime(df[\"nonessential\"])).dt.days\n    df[\"days_from_nonessential\"].fillna(df[\"days_from_quarantine\"])\n    df.loc[df[\"days_from_nonessential\"] < 30, \"days_from_nonessential\"] = 0\n    df.loc[df[\"days_from_nonessential\"] >= 30, \"days_from_nonessential\"] = 1\n    \n    return df","c242618f":"def generate_ar_features(df, group_by_cols, value_cols):\n    \n    # Daily cases\n    diff_df = df.groupby(group_by_cols)[value_cols].diff().fillna(0)\n    diff_df.columns = [col + \"_daily\" for col in value_cols]\n    value_cols += [col + \"_daily\" for col in value_cols]\n    df = pd.concat([df, diff_df], axis=1)\n    \n    # Daily percentage increase\n    pct_df = df.groupby(group_by_cols)[value_cols].pct_change().fillna(0)\n    pct_df.columns = [col + \"_pct_change\" for col in value_cols]\n    value_cols += [col + \"_pct_change\" for col in value_cols]\n    df = pd.concat([df, pct_df], axis=1)\n\n    # Shift to yesterday's data\n    yesterday_df = df.groupby(group_by_cols)[value_cols].shift()\n    value_cols = [col + \"_yesterday\" for col in value_cols]\n    yesterday_df.columns = value_cols\n    df = pd.concat([df, yesterday_df], axis=1)\n\n    # Average of the percentage change in the last 3 days\n    three_days_avg = df.groupby(group_by_cols)[value_cols].rolling(3).mean()\n    three_days_avg = three_days_avg.reset_index()[value_cols]\n    three_days_avg.columns = [col + \"_3_day_avg\" for col in three_days_avg.columns]\n    df = pd.concat([df, three_days_avg], axis=1)\n\n    # Average of the percentage change in the last 7 days\n    seven_days_avg = df.groupby(group_by_cols)[value_cols].rolling(7).mean()\n    seven_days_avg = seven_days_avg.reset_index()[value_cols]\n    seven_days_avg.columns = [col + \"_7_day_avg\" for col in seven_days_avg.columns]\n    df = pd.concat([df, seven_days_avg], axis=1)\n    \n    df = df.replace([np.inf, -np.inf], 0)\n    \n    return df","39d65abb":"def generate_features(df):\n    group_by_cols = [\"state\",\"country\"]\n    value_cols = [\"ConfirmedCases\", \"Fatalities\", \"country_cases\", \"country_fatalities\"]\n    \n    df = aggregate_label(df)\n    df = append_external_data(df)\n    df = generate_ar_features(df, group_by_cols, value_cols)\n    df = generate_time_features(df)\n    df[\"dow\"] = pd.to_datetime(df[\"Date\"]).dt.dayofweek\n    df = df.fillna(-1)\n    df.loc[df[\"ConfirmedCases_yesterday\"]<0, \"ConfirmedCases_yesterday\"] = 0\n    df.loc[df[\"Fatalities_yesterday\"]<0, \"Fatalities_yesterday\"] = 0\n    return df","c06a27c2":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv\")\ntrain = train.rename({\"Province\/State\": \"state\", \"Country\/Region\": \"country\"}, axis=1)\ntrain.loc[train[\"state\"].isna(), \"state\"] = \"Unknown\"\ntrain = generate_features(train)\nprint(train[\"Date\"].min(), \"-\", train[\"Date\"].max())\ntrain.loc[train[\"country\"] == \"Italy\"].tail()","478e672e":"test = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv\")\ntest = test.rename({\"Province\/State\": \"state\", \"Country\/Region\": \"country\"}, axis=1)\ntest.loc[test[\"state\"].isna(), \"state\"] = \"Unknown\"\nprint(test[\"Date\"].min(), \"-\", test[\"Date\"].max())\ntest.head()","8f199baa":"train.loc[train[\"Date\"]<\"2020-03-19\", \"split\"] = \"train\"\ntrain.loc[train[\"Date\"]>=\"2020-03-19\", \"split\"] = \"test\"","b8c22199":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfeatures = [col for col in train.columns if (\"yesterday\" in col) | (\"days_from\" in col)]\nfeatures += country_info.select_dtypes(include=numerics).columns.tolist()\nfeatures += pollution.select_dtypes(include=numerics).columns.tolist()\nfeatures += economy.select_dtypes(include=numerics).columns.tolist()\nfeatures += [\"Lat\", \"Long\", \"dow\"]","7449b8e0":"len(features)","37132d61":"import lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error","7b34d7b7":"def train_model(df, label, base_label, features=features, **kwargs):\n    X_train = df.loc[df[\"split\"] == \"train\"][features]\n    y_train = np.log(df.loc[df[\"split\"] == \"train\"][label] + 1)\n    b_train = np.log(df.loc[df[\"split\"] == \"train\"][base_label] + 1)\n    X_test = df.loc[df[\"split\"] == \"test\"][features]\n    y_test = np.log(df.loc[df[\"split\"] == \"test\", label] + 1)\n    b_test = np.log(df.loc[df[\"split\"] == \"test\", base_label] + 1)\n    print(kwargs)\n    model = lgb.LGBMRegressor(**kwargs)\n    model.fit(X_train, y_train, init_score = b_train)\n    y_pred = model.predict(X_test)\n    print(np.sqrt(mean_squared_error(y_test, y_pred + b_test)))\n    return model","c57b34a3":"lgb_model_cases = train_model(train, \"ConfirmedCases\", \"ConfirmedCases_yesterday\",\n                                   num_leaves=100,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8,\n                                   min_data_in_leaf=1)","8fbd391c":"import shap\nexplainer = shap.TreeExplainer(lgb_model_cases)\nshap_values = explainer.shap_values(train.loc[train[\"split\"] == \"test\"][features])\nshap.summary_plot(\n    shap_values,\n    train.loc[train[\"split\"] == \"test\"][features],\n    max_display=110,\n    show=True,\n)","f4d60e57":"lgb_model_fatalities = train_model(train, \"Fatalities\", \"Fatalities_yesterday\", features,\n                                   num_leaves=100,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8,\n                                   min_data_in_leaf=1                             \n                                  )","72f2dea1":"import shap\nexplainer = shap.TreeExplainer(lgb_model_fatalities)\nshap_values = explainer.shap_values(train.loc[train[\"split\"] == \"test\"][features])\nshap.summary_plot(\n    shap_values,\n    train.loc[train[\"split\"] == \"test\"][features],\n    max_display=110,\n    show=True,\n)","6e5e0bd3":"X_train = train[features]\ny_train = np.log(train[\"ConfirmedCases\"] + 1)\nb_train = np.log(train[\"ConfirmedCases_yesterday\"] + 1)\ncases_model = lgb.LGBMRegressor(num_leaves=100,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8,\n                                   min_data_in_leaf=1)\ncases_model.fit(X_train, y_train, init_score = b_train)\n\nX_train = train[features]\ny_train = np.log(train[\"Fatalities\"] + 1)\nb_train = np.log(train[\"Fatalities_yesterday\"] + 1)\nfatalities_model = lgb.LGBMRegressor(num_leaves=100,\n                                   colsample_bytree=0.8,\n                                   learning_rate=0.1,\n                                   n_estimators=1000,\n                                   subsample=0.8,\n                                   min_data_in_leaf=1)\nfatalities_model.fit(X_train, y_train, init_score = b_train)","da61072c":"base_df = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv\")\nbase_df = base_df.rename({\"Province\/State\": \"state\", \"Country\/Region\": \"country\"}, axis=1)\nbase_df.loc[base_df[\"state\"].isna(), \"state\"] = \"Unknown\"\nscoring_dates = test.loc[test[\"Date\"]>\"2020-03-24\", \"Date\"].unique()\nbase_df = base_df.loc[base_df[\"Date\"]<=\"2020-03-24\"]","8b9ca19a":"scoring_dates","67443d27":"pred_df = pd.DataFrame(columns=base_df.columns)\nfor date in scoring_dates.tolist():\n    print(date)\n    new_df = base_df.copy()\n    curr_date_df = test.loc[test[\"Date\"] == date].copy()\n    curr_date_df[\"ConfirmedCases\"] = 0\n    curr_date_df[\"Fatalities\"] = 0\n    new_df = new_df.append(curr_date_df).reset_index(drop=True)\n    new_df = generate_features(new_df)\n    predictions = cases_model.predict(new_df[features]) + np.log(new_df[\"ConfirmedCases_yesterday\"] + 1)\n    new_df[\"predicted_cases\"] = round(np.maximum(np.exp(predictions) - 1, new_df[\"ConfirmedCases_yesterday\"]))\n    predictions = fatalities_model.predict(new_df[features]) + np.log(new_df[\"Fatalities_yesterday\"] + 1)\n    new_df[\"predicted_fatalities\"] = np.maximum(np.exp(predictions) - 1, new_df[\"Fatalities_yesterday\"])\n    new_df[\"predicted_fatalities\"] = round(np.minimum(new_df[\"predicted_fatalities\"], new_df[\"predicted_cases\"]*0.2))\n    new_df.loc[new_df[\"Date\"] == date, \"ConfirmedCases\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_cases\"]\n    new_df.loc[new_df[\"Date\"] == date, \"Fatalities\"] = new_df.loc[new_df[\"Date\"] == date, \"predicted_fatalities\"]\n    pred_df = pred_df.append(new_df.loc[new_df[\"Date\"] == date][pred_df.columns.tolist()])\n    base_df = base_df.append(new_df.loc[new_df[\"Date\"] == date][base_df.columns.tolist()])","75bc1170":"pred_df.loc[pred_df[\"state\"] == \"Hubei\"]","f6175bdd":"pred_df.loc[pred_df[\"country\"] == \"Italy\"]","3ef4fb06":"pred_df.loc[pred_df[\"country\"] == \"Israel\"]","77244f32":"pred_df.loc[pred_df[\"country\"] == \"Argentina\"]","32f56989":"pred_df.loc[pred_df[\"country\"] == \"Uruguay\"]","d84bcc21":"# Feature Engineering","eeecb4ef":"## Cases","ee3aa96d":"# Data Gathering","f3c0a246":"## Predict submission dates","1e4edb95":"## Train on full data","c589f97f":"## Fatalities","a819de94":"## LightGBM","7b63a955":"# Modeling"}}