{"cell_type":{"d488186d":"code","6538e624":"code","49d6cd66":"code","82cc4f0b":"code","5b0bda4c":"code","3eec795f":"code","e4077642":"code","066f8f92":"code","ee137963":"code","d9a44bd8":"code","03fe9cd9":"code","6f9927ca":"code","01313c8c":"code","258da04f":"code","a26becf1":"code","a209e727":"code","4b4f19d9":"code","68c7087c":"code","285521ed":"code","5eb6ea20":"code","af90877d":"code","5ebf7d4b":"code","64f1b032":"code","c74e666a":"code","c6168bae":"code","c304bed0":"code","42d67dc9":"code","e4475b00":"code","d07f543e":"code","6af95af7":"markdown","caba7cfd":"markdown","859997e4":"markdown","76118531":"markdown","7a6d7dc3":"markdown","546b4da5":"markdown"},"source":{"d488186d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6538e624":"import matplotlib.pyplot as plt\n%matplotlib inline","49d6cd66":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","82cc4f0b":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","5b0bda4c":"print(train_features.shape) # (23814, 876)\nprint(train_targets.shape) # (23814, 207)\nprint(test_features.shape) # (3982, 876)","3eec795f":"def preprocess(df):\n    \"\"\"Returns preprocessed data frame\"\"\"\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n    del df['sig_id']\n    return df","e4077642":"train = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']","066f8f92":"train_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","ee137963":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(train)\n\n# scale train data\ntrain = scaler.transform(train)\n# scale test data\ntest = scaler.transform(test)","d9a44bd8":"targets = [col for col in train_targets.columns]","03fe9cd9":"print(train.shape) # (21948, 875)\nprint(test.shape) # (3982, 875)\nprint(train_targets.shape) # (21948, 206)","6f9927ca":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'input': torch.tensor(self.features[idx, :], dtype=torch.float),\n            'target': torch.tensor(self.targets[idx, :], dtype=torch.float)\n        }\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n    \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return {\n            'input': torch.tensor(self.features[idx, :], dtype=torch.float)\n        }","01313c8c":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","258da04f":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2) # 0.2\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.2) # 0.2\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.2) # 0.2\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.weight = torch.tensor([0.5]).to(device)\n        \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        \n        x = F.prelu(self.dense1(x), self.weight) # relu -> prelu\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.prelu(self.dense2(x), self.weight) # relu -> prelu\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","a26becf1":"#train = train.values\n#test = test.values\ntrain_targets = train_targets.values","a209e727":"def set_seed(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)","4b4f19d9":"def get_dataloaders(num_workers, batch_size, x_train, y_train, x_valid, y_valid):\n    \"\"\"Return training and valid dataloader\"\"\"\n    \n    # load the training and valid datasets\n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n\n    # prepare data loaders\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers)\n\n    # define loaders\n    loader = {\n        \"train\": train_loader,\n        \"valid\": valid_loader\n    }\n    \n    return loader","68c7087c":"def get_testloaders(num_workers, batch_size, x_test):\n    \"\"\"Return test dataloader\"\"\"\n    \n    # load the test datasets\n    test_dataset = TestDataset(x_test)\n    \n    # prepare test loader\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n    \n    # define loaders\n    loader = {\n        'test': test_loader\n    }\n    return loader","285521ed":"from torch.optim.lr_scheduler import StepLR\n\ndef train_model(n_epochs, loaders, model, optimizer, criterion, device, save_path):\n    \"\"\"Returns a trained model\"\"\"        \n    scheduler = StepLR(optimizer, step_size=2, gamma=0.96)\n    \n    # initialize tracker for minimum validation loss\n    valid_loss_min = np.Inf\n    print(valid_loss_min)\n    for epoch in range(1, n_epochs + 1):\n        # decay Learning Rate\n        scheduler.step()\n        # print(f'Epoch: \\t{epoch}\\tLR: {scheduler.get_lr()}')\n        \n        # initialize variables to monitor training and validation loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        \n        # train the model\n        model.train()\n        \n        #for batch_idx, (data, target) in enumerate(loaders['train']):\n        for data in loaders['train']:\n            data_input, data_target = data['input'].to(device), data['target'].to(device)\n            \n            # initialize weights to zero: clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            \n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data_input)\n\n            # calcuate loss\n            loss = criterion(output, data_target)\n            \n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            \n            # perform a single optimization step\n            optimizer.step()\n            \n            # TODO: scheduler.step()\n            \n            # update running training loss\n            # print(\"train loss : \", loss.item())\n            train_loss += (loss.item() \/ len(loaders['train']))\n\n        # validate the model\n        model.eval()\n        \n        for data in loaders['valid']:\n            data_input, data_target = data['input'].to(device), data['target'].to(device)\n            \n            # update the average validation loss\n            output = model(data_input)\n            \n            # calculate loss\n            loss = criterion(output, data_target)\n            \n            # update running validation loss\n            # print(\"validation loss : \", loss.item())\n            valid_loss += (loss.item() \/ len(loaders['valid']))\n        \n        # print training\/validation statistics\n        # print(f'Epoch: \\t{epoch}\\tTraining Loss: {train_loss}\\tValidation Loss:{valid_loss}')\n        \n        # save the model if validation loss has descrased\n        if valid_loss < valid_loss_min:\n            print(f'Epoch: \\t{epoch}\\tValidation loss decreased ({valid_loss_min} -> {valid_loss}). Saving the model...')\n            torch.save(model.state_dict(), save_path)\n            valid_loss_min = valid_loss\n\n    # return trained model\n    return model","5eb6ea20":"def run_training(seed, kfold, batch_size, epochs, learning_rate, weight_decay):\n    set_seed(seed)\n\n    for fold, (train_idx, valid_idx) in enumerate(kfold.split(X=train, y=train_targets)):\n        x_train, x_valid = train[train_idx], train[valid_idx]\n        y_train, y_valid = train_targets[train_idx], train_targets[valid_idx]\n        \n        # get dataloaders\n        dataloaders = get_dataloaders(0, batch_size, x_train, y_train, x_valid, y_valid)\n        \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model = Model(875, 206, 1024).to(device)\n        \n        criterion_moa = nn.BCEWithLogitsLoss() # for multi-lable classfication\n        optimizer_moa = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        # train the model\n        train_model(epochs, dataloaders, model, optimizer_moa, criterion_moa, device, f'models\/model_seed_{seed}_fold_{fold}.pt')","af90877d":"# hyper parameters\n\nFOLDS = 5\nWORKERS = 0\nBATCH_SIZE = 128\nEPOCHS = 50\nLEARNING_RATE = 0.0002\nWEIGHT_DECAY = 0.00001\nSEED = 42","5ebf7d4b":"%mkdir models","64f1b032":"mskf = MultilabelStratifiedKFold(n_splits=FOLDS)\n\nfor seed in range(40, 45):\n    run_training(seed, mskf, BATCH_SIZE, EPOCHS, LEARNING_RATE, WEIGHT_DECAY)","c74e666a":"def inference(loaders, model, device):\n    \"\"\"Return a prediction\"\"\"\n    \n    model.eval()\n    preds = []\n    \n    for data in  loaders['test']:\n        data_input = data['input'].to(device)\n        \n        # forward pass: compute predicted outputs by passing inputs to the model\n        with torch.no_grad():\n            output = model(data_input)\n        \n        pred = output.sigmoid().detach().cpu().numpy()\n        preds.append(pred)\n        \n    return np.concatenate(preds)","c6168bae":"def run_inferencing(seed):\n    set_seed(seed)\n    \n    preds = np.zeros((len(test), 206))\n    \n    #for fold, (train_idx, valid_idx) in enumerate(mskf.split(X=train, y=train_targets)):\n    for i in range(0, FOLDS):    \n        # get dataloaders\n        dataloaders = get_testloaders(0, 128, test)\n        \n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        model = Model(875, 206, 1024)\n        \n        model.load_state_dict(torch.load(f'models\/model_seed_{seed}_fold_{i}.pt'))\n        model.to(device)\n        \n        pred = inference(dataloaders, model, device)\n        \n        preds += pred\n        \n    preds = preds \/ FOLDS\n    return preds","c304bed0":"#preds = run_inferencing(SEED)","42d67dc9":"preds = np.zeros((len(test), 206))\nfor seed in range(40, 45):\n    preds += run_inferencing(seed)\npreds = preds \/ 5","e4475b00":"sample_submission[targets] = preds\nsample_submission.loc[test_features['cp_type']=='ctl_vehicle', targets] = 0\nsample_submission.to_csv('submission.csv', index=False)","d07f543e":"sample_submission","6af95af7":"## Pre-processed Data","caba7cfd":"### Import","859997e4":"## Define Model","76118531":"## Training","7a6d7dc3":"## Inferencing","546b4da5":"## Submission"}}