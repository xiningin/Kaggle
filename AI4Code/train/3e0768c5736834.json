{"cell_type":{"8b0c19ac":"code","d320db55":"code","db97a36f":"code","bf6008c1":"code","449120f2":"code","aadbb013":"code","f838fd6d":"code","cbbbff8a":"code","7de11ffc":"code","f24888b1":"code","5e7bad6c":"code","2732e33e":"code","f565d867":"code","f00bedd8":"code","6cc5054d":"code","a6874685":"code","8ea39d75":"code","7ac2d0a1":"code","e08fe636":"code","6cd00362":"code","9fc28597":"code","c9f9cf6a":"code","d11d68ce":"code","36dbbb3b":"code","4bd6151d":"code","e830400c":"code","ac571221":"code","2660ef68":"code","24c760fb":"code","c12eb695":"code","a229fb70":"code","e8e64104":"code","c71061f3":"code","a75447b6":"code","0f34ff45":"code","0cffbdef":"code","c7199ff7":"code","71a7cee5":"code","115f739b":"code","108a29ae":"code","9d9a9fc6":"code","f4848834":"code","522179c7":"code","5600d83f":"code","57520ce5":"code","8077aca4":"code","f3c9810d":"code","ea0d2fdc":"code","1bfa4aa8":"code","bab9acfb":"code","1f005ee8":"code","c38386ff":"code","b2f84a3d":"code","0545eb21":"code","6ea97219":"code","f23c0c31":"code","f2ab5a40":"code","a304e239":"code","53dfd595":"code","ad18d82a":"code","e75802f5":"code","8ff83cd3":"code","ac4fd404":"code","71f9e566":"code","7b98c4a6":"code","91e2cc25":"code","ef441a98":"code","cdc78da9":"code","9e5e6904":"code","990c99ef":"code","6241cb56":"code","0bf7375e":"code","2466d587":"markdown","c0c06f9b":"markdown","567112d0":"markdown","0530763f":"markdown","066b8126":"markdown","d92a2a18":"markdown","baf94912":"markdown","8e7ea98a":"markdown","1f077e5d":"markdown","b096057f":"markdown","e3615b9c":"markdown","03dd99b4":"markdown","6b9890cb":"markdown","7f91f25e":"markdown","9eb0b31f":"markdown","af12a8e0":"markdown","8f28b54d":"markdown","aae51512":"markdown"},"source":{"8b0c19ac":"# import library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\n\n# import data\ndata = pd.read_csv(\"..\/input\/linearregressiondataset3\/linear-regression-dataset.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","d320db55":"# plot data\nplt.scatter(data.deneyim,data.maas)\nplt.xlabel(\"deneyim\")\nplt.ylabel(\"maas\")\nplt.show()","db97a36f":"#%% linear regression\n\n# sklearn library\nfrom sklearn.linear_model import LinearRegression\n# linear regression model\nlinear_reg = LinearRegression()\n\nx = data.deneyim.values.reshape(-1,1)\ny = data.maas.values.reshape(-1,1)\n\nlinear_reg.fit(x,y)\n\nprint('R sq: ', linear_reg.score(x, y))\nprint('Correlation: ', math.sqrt(linear_reg.score(x, y)))","bf6008c1":"#%% prediction\nimport numpy as np\n\nprint(\"Coefficient for X: \", linear_reg.coef_)\nprint(\"Intercept for X: \", linear_reg.intercept_)\nprint(\"Regression line is: y = \" + str(linear_reg.intercept_[0]) + \" + (x * \" + str(linear_reg.coef_[0][0]) + \")\")\n\n# maas = 1663 + 1138*deneyim \nmaas_yeni = 1663 + 1138*11\nprint(maas_yeni)\n\narray = np.array([11]).reshape(-1,1)\nprint(linear_reg.predict(array))","449120f2":"# visualize line\narray = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]).reshape(-1,1)  # deneyim\n\nplt.scatter(x,y)\n#plt.show()\ny_head = linear_reg.predict(array)  # maas\nplt.plot(array, y_head,color = \"red\")\narray = np.array([100]).reshape(-1,1)\nlinear_reg.predict(array)","aadbb013":"y_head = linear_reg.predict(x)  # maas\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","f838fd6d":"import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\n\ndata = pd.read_csv(\"..\/input\/multiplelinearregressiondataset\/multiple-linear-regression-dataset.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","cbbbff8a":"x = data.iloc[:,[0,2]].values\ny = data.maas.values.reshape(-1,1)\n\nmultiple_linear_regression = LinearRegression()\nmultiple_linear_regression.fit(x,y)\n\nprint(\"b0: \",multiple_linear_regression.intercept_)\nprint(\"b1: \", multiple_linear_regression.coef_)\n\n#predict\nx_ = np.array([[10,35],[5,35]])\nmultiple_linear_regression.predict(x_)\n\ny_head = multiple_linear_regression.predict(x) \nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","7de11ffc":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/polynomialregressioncsv\/polynomial-regression.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","f24888b1":"x = data.araba_fiyat.values.reshape(-1,1)\ny = data.araba_max_hiz.values.reshape(-1,1)\n\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()","5e7bad6c":"# polynomial regression =  y = b0 + b1*x +b2*x^2 + b3*x^3 + ... + bn*x^n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\n\npolynominal_regression = PolynomialFeatures(degree=4)\nx_polynomial = polynominal_regression.fit_transform(x,y)\n\n# %% fit\nlinear_regression = LinearRegression()\nlinear_regression.fit(x_polynomial,y)\n# %%\ny_head2 = linear_regression.predict(x_polynomial)\n\nplt.plot(x,y_head2,color= \"green\",label = \"poly\")\nplt.legend()\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head2))\n\n","2732e33e":"import pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = pd.read_csv(\"..\/input\/support-vector-regression\/maaslar.csv\")\nprint(data.info())\nprint(data.head())\n#print(data.describe())","f565d867":"x = data.iloc[:,1:2].values\ny = data.iloc[:,2:].values\n\nplt.scatter(x,y)\nplt.xlabel(\"araba_max_hiz\")\nplt.ylabel(\"araba_fiyat\")\nplt.show()","f00bedd8":"#verilerin olceklenmesi\nfrom sklearn.preprocessing import StandardScaler\nsc1 = StandardScaler()\nx_olcekli = sc1.fit_transform(x)\nsc2 = StandardScaler()\ny_olcekli = sc2.fit_transform(y)\n\n#%% SVR\nfrom sklearn.svm import SVR\n\nsvr_reg = SVR(kernel = 'rbf')\nsvr_reg.fit(x_olcekli,y_olcekli)\n\ny_head = svr_reg.predict(x_olcekli)\n\n# visualize line\nplt.plot(x_olcekli,y_head,color= \"green\",label = \"SVR\")\nplt.legend()\nplt.scatter(x_olcekli,y_olcekli,color='red')\nplt.show()\n\nprint('R sq: ', svr_reg.score(x_olcekli, y_olcekli))","6cc5054d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/decisiontreeregressiondataset\/decision-tree-regression-dataset.csv\", header=None)\nprint(data.info())\nprint(data.head())\n#print(data.describe())\n","a6874685":"x = data.iloc[:,[0]].values.reshape(-1,1)\ny = data.iloc[:,[1]].values.reshape(-1,1)","8ea39d75":"#%%  decision tree regression\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(x,y)\n\nprint(tree_reg.predict(np.array([5.5]).reshape(-1,1)))","7ac2d0a1":"x_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\n#print(x)\ny_head = tree_reg.predict(x_)\n#print(y_head)\n\n# %% visualize\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_head,color = \"green\")\nplt.xlabel(\"tribun level\")\nplt.ylabel(\"ucret\")\nplt.show()\n\ny_head = tree_reg.predict(x)\n#from sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y,y_head))","e08fe636":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/randomforestregressiondataset\/random-forest-regression-dataset.csv\", header=None)\nprint(data.info())\nprint(data.head())\n#print(data.describe())","6cd00362":"x = data.iloc[:,0].values.reshape(-1,1)\ny = data.iloc[:,1].values.reshape(-1,1)","9fc28597":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators = 100, random_state= 42) \nrf.fit(x,y)\n\nprint(\"7.8 seviyesinde fiyat\u0131n ne kadar oldu\u011fu: \",rf.predict(np.array([7.8]).reshape(-1,1)))\n\nx_ = np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head = rf.predict(x_)","c9f9cf6a":"# visualize\nplt.scatter(x,y,color=\"red\")\nplt.plot(x_,y_head,color=\"green\")\nplt.xlabel(\"tribun level\")\nplt.ylabel(\"ucret\")\nplt.show()","d11d68ce":"y_head = rf.predict(x)\nfrom sklearn.metrics import r2_score\nprint(\"r_score: \", r2_score(y,y_head))","36dbbb3b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","4bd6151d":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","e830400c":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","ac571221":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %%\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","2660ef68":"# %%\n# find k value\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","24c760fb":"# %%\n# knn model\nknn = KNeighborsClassifier(n_neighbors = 8) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {} \".format(3,knn.score(x_test,y_test)))","c12eb695":"#%% confusion matrix\ny_pred = knn.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","a229fb70":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","e8e64104":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","c71061f3":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","a75447b6":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %% SVM\nfrom sklearn.svm import SVC\n \nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n# %% test\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\n","0f34ff45":"#%% confusion matrix\ny_pred = svm.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","0cffbdef":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","c7199ff7":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","71a7cee5":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","115f739b":"#%%\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3,random_state=1)\n\n# %% Naive bayes \nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nnb.score(x_test,y_test)\n # %% test\nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))","108a29ae":"#%% confusion matrix\ny_pred = nb.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","9d9a9fc6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","f4848834":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","522179c7":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","5600d83f":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\n\n#%%\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))","57520ce5":"#%% confusion matrix\ny_pred = dt.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","8077aca4":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/classification\/data.csv\")\n#print(data.info())\n#print(data.head())\n#print(data.describe())\n# %%\ndata.drop([\"id\",\"Unnamed: 32\"],axis=1,inplace=True)\ndata.tail()\n# malignant = M  kotu huylu tumor\n# benign = B     iyi huylu tumor","f3c9810d":"# %%\nM = data[data.diagnosis == \"M\"]\nB = data[data.diagnosis == \"B\"]\n# scatter plot\nplt.scatter(M.radius_mean,M.texture_mean,color=\"red\",label=\"kotu\",alpha= 0.3)\nplt.scatter(B.radius_mean,B.texture_mean,color=\"green\",label=\"iyi\",alpha= 0.3)\nplt.xlabel(\"radius_mean\")\nplt.ylabel(\"texture_mean\")\nplt.legend()\nplt.show()","ea0d2fdc":"# %%\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop([\"diagnosis\"],axis=1)\n\n# %%\n# normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","1bfa4aa8":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.15,random_state = 42)\n\n#%%  random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))","bab9acfb":"#%% confusion matrix\ny_pred = rf.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","1f005ee8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %% create dataset\n# class1\nx1 = np.random.normal(25,5,1000)\ny1 = np.random.normal(25,5,1000)\n\n# class2\nx2 = np.random.normal(55,5,1000)\ny2 = np.random.normal(60,5,1000)\n\n# class3\nx3 = np.random.normal(55,5,1000)\ny3 = np.random.normal(15,5,1000)\n\nx = np.concatenate((x1,x2,x3),axis = 0)\ny = np.concatenate((y1,y2,y3),axis = 0)\n\ndictionary = {\"x\":x,\"y\":y}\ndata = pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1)\nplt.scatter(x2,y2)\nplt.scatter(x3,y3)\nplt.show()","c38386ff":"# %% KMEANS\n\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(data)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k (cluster) value\")\nplt.ylabel(\"wcss\")\nplt.show()","b2f84a3d":"#%% k = 3 icin modelim\nkmeans2 = KMeans(n_clusters=3)\nclusters = kmeans2.fit_predict(data)\n\ndata[\"label\"] = clusters\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\nplt.scatter(kmeans2.cluster_centers_[:,0],kmeans2.cluster_centers_[:,1],color = \"yellow\")\nplt.show()","0545eb21":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# %% create dataset\n# class1\nx1 = np.random.normal(25,5,100)\ny1 = np.random.normal(25,5,100)\n\n# class2\nx2 = np.random.normal(55,5,100)\ny2 = np.random.normal(60,5,100)\n\n# class3\nx3 = np.random.normal(55,5,100)\ny3 = np.random.normal(15,5,100)\n\nx = np.concatenate((x1,x2,x3),axis = 0)\ny = np.concatenate((y1,y2,y3),axis = 0)\n\ndictionary = {\"x\":x,\"y\":y}\n\ndata = pd.DataFrame(dictionary)\n\nplt.scatter(x1,y1,color=\"black\")\nplt.scatter(x2,y2,color=\"black\")\nplt.scatter(x3,y3,color=\"black\")\nplt.show()","6ea97219":"# %% dendogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()","f23c0c31":"# %% HC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\ncluster = hiyerartical_cluster.fit_predict(data)\n\ndata[\"label\"] = cluster\n\nplt.scatter(data.x[data.label == 0 ],data.y[data.label == 0],color = \"red\")\nplt.scatter(data.x[data.label == 1 ],data.y[data.label == 1],color = \"green\")\nplt.scatter(data.x[data.label == 2 ],data.y[data.label == 2],color = \"blue\")\n#plt.scatter(data.x[data.label == 3 ],data.y[data.label == 3],color = \"black\")\nplt.show()","f2ab5a40":"import pandas as pd\n# %% import twitter data\ndata = pd.read_csv(\"..\/input\/natural-language-process-nlp\/gender-classifier.csv\",encoding = \"latin1\")\ndata = pd.concat([data.gender,data.description],axis=1)\ndata.dropna(axis = 0,inplace = True)\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender]\nprint(data.info())\nprint(data.head())\n#print(data.describe())","a304e239":"import nltk # natural language tool kit\n#nltk.download(\"stopwords\")      # corpus diye bir kalsore indiriliyor\nfrom nltk.corpus import stopwords  # sonra ben corpus klasorunden import ediyorum\nimport re\ndescription_list = []\nfor description in data.description:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description) # regular expression RE mesela \"[^a-zA-Z]\"\n    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n    description = nltk.word_tokenize(description)# split kullan\u0131rsak \"shouldn't \" gibi kelimeler \"should\" ve \"not\" diye ikiye ayr\u0131lmaz ama word_tokenize() kullanirsak ayrilir\n    description = [ word for word in description if not word in set(stopwords.words(\"english\"))] # greksiz kelimeleri cikar\n    lemma = nltk.WordNetLemmatizer() # lemmatazation loved => love   gitmeyecegim = > git\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description)\n#print(description_list)","53dfd595":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer # bag of words yaratmak icin kullandigim metot\nmax_features = 5000\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x\n\n#print(\"en sik kullanilan {} kelimeler: {}\".format(max_features,count_vectorizer.get_feature_names()))\n","ad18d82a":"# %%\ny = data.iloc[:,0].values   # male or female classes\nx = sparce_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)","e75802f5":"# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n#%% prediction\ny_pred = nb.predict(x_test)\nprint(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))","8ff83cd3":"from sklearn.datasets import load_iris\nimport pandas as pd\n# %%\niris = load_iris()\n\nfeature_names = iris.feature_names\ny = iris.target\n\ndata = pd.DataFrame(iris.data,columns = feature_names)\ndata[\"sinif\"] = y\n\nx = iris.data\nprint(data.info())\nprint(data.head())\n#print(data.describe())","ac4fd404":"#%% PCA\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2, whiten= True )  # whitten = normalize\npca.fit(x)\n\nx_pca = pca.transform(x)\n\nprint(\"variance ratio: \", pca.explained_variance_ratio_)\nprint(\"sum: \",sum(pca.explained_variance_ratio_))","71f9e566":"#%% 2D\ndata[\"p1\"] = x_pca[:,0]\ndata[\"p2\"] = x_pca[:,1]\n\ncolor = [\"red\",\"green\",\"blue\"]\n\nimport matplotlib.pyplot as plt\nfor each in range(3):\n    plt.scatter(data.p1[data.sinif == each],data.p2[data.sinif == each],color = color[each],label = iris.target_names[each])\n    \nplt.legend()\nplt.xlabel(\"p1\")\nplt.ylabel(\"p2\")\nplt.show()","7b98c4a6":"from sklearn.datasets import load_iris\nimport pandas as pd\nimport numpy as np\n#%%\niris = load_iris()\nx = iris.data\ny = iris.target\n\ndata = pd.DataFrame(iris.data,columns = feature_names)\ndata[\"sinif\"] = y\n\nprint(data.info())\nprint(data.head())\n#print(data.describe())\n\n# %% normalization\nx = (x-np.min(x))\/(np.max(x)-np.min(x))","91e2cc25":"# %% train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3)\n\n# knn model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 13) # n_neighbors = k\n\n# %% K fold CV K = 10\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X = x_train, y= y_train, cv = 10)\nprint(\"average accuracy: \",np.mean(accuracies))\nprint(\"average std: \",np.std(accuracies))\n\nknn.fit(x_train,y_train)\nprint(\"test accuracy: \",knn.score(x_test,y_test))","ef441a98":"#Model Selection  grid search cross validation for knn\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = {\"n_neighbors\":np.arange(1,50)}\nknn= KNeighborsClassifier()\n\nknn_cv = GridSearchCV(knn, grid, cv = 10)  # GridSearchCV\nknn_cv.fit(x,y)\n\n#%% print hyperparameter KNN algoritmasindaki K degeri\nprint(\"tuned hyperparameter K: \",knn_cv.best_params_)\nprint(\"tuned parametreye gore en iyi accuracy (best score): \",knn_cv.best_score_)","cdc78da9":"#Model Selection Grid search CV with logistic regression\nx = x[:100,:]\ny = y[:100] \n\nfrom sklearn.linear_model import LogisticRegression\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}  # l1 = lasso ve l2 = ridge\n\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,grid,cv = 10)\nlogreg_cv.fit(x,y)\n\nprint(\"tuned hyperparameters: (best parameters): \",logreg_cv.best_params_)\nprint(\"accuracy: \",logreg_cv.best_score_)","9e5e6904":"import pandas as pd\nimport os\nprint(os.listdir(\"..\/input\/movielens-20m-dataset\/\"))\n# import movie data set and look at columns\nmovie = pd.read_csv(\"..\/input\/movielens-20m-dataset\/movie.csv\")\nprint(movie.columns)\nmovie = movie.loc[:,[\"movieId\",\"title\"]]\nmovie.head(10)","990c99ef":"# import rating data and look at columsn\nrating = pd.read_csv(\"..\/input\/movielens-20m-dataset\/rating.csv\")\nprint(rating.columns)\n# what we need is that user id, movie id and rating\nrating = rating.loc[:,[\"userId\",\"movieId\",\"rating\"]]\nrating.head(10)","6241cb56":"# then merge movie and rating data\ndata = pd.merge(movie,rating)\n# now lets look at our data \ndata.head(10)\nprint(data.shape)\ndata = data.iloc[:1000000,:]\n# lets make a pivot table in order to make rows are users and columns are movies. And values are rating\npivot_table = data.pivot_table(index = [\"userId\"],columns = [\"title\"],values = \"rating\")\npivot_table.head(10)","0bf7375e":"movie_watched = pivot_table[\"Bad Boys (1995)\"]\nsimilarity_with_other_movies = pivot_table.corrwith(movie_watched)  # find correlation between \"Bad Boys (1995)\" and other movies\nsimilarity_with_other_movies = similarity_with_other_movies.sort_values(ascending=False)\nsimilarity_with_other_movies.head()","2466d587":"\n<a class=\"anchor\" id=\"2.\"><\/a> \n# Multiple Linear Regression","c0c06f9b":"# Classification\n<a class=\"anchor\" id=\"7.\"><\/a> \n# K-Nearest Neighbour (KNN) Classification","567112d0":"\n<a class=\"anchor\" id=\"6.\"><\/a> \n# Random Forest Regression","0530763f":"\n<a class=\"anchor\" id=\"10.\"><\/a> \n# Decision Tree Classification","066b8126":"\n<a class=\"anchor\" id=\"8.\"><\/a> \n# Support Vector Machine (SVM) Classification","d92a2a18":"\n<a class=\"anchor\" id=\"3.\"><\/a> \n# Polynomial Linear Regression","baf94912":"# Other Content\n<a class=\"anchor\" id=\"14.\"><\/a> \n# Natural Language Process (NLP)","8e7ea98a":"\n<a class=\"anchor\" id=\"9.\"><\/a> \n# Naive Bayes Classification","1f077e5d":"# Clustering\n<a class=\"anchor\" id=\"12.\"><\/a> \n# K-Means Clustering","b096057f":"\n<a class=\"anchor\" id=\"11.\"><\/a> \n# Random Forest Classification","e3615b9c":"\n\n<a class=\"anchor\" id=\"17.\"><\/a> \n# Recommendation Systems","03dd99b4":"\nIt is the kernel that I have tried and compiled from the courses of [DATAI Team](https:\/\/www.udemy.com\/user\/datai-team\/) (Language of the courses is Turkish: [Machine Learning ve Python: A'dan Z'ye Makine \u00d6\u011frenmesi](https:\/\/www.udemy.com\/machine-learning-ve-python-adan-zye-makine-ogrenmesi-4)), which is [Grandmaster on Kaggle](https:\/\/www.kaggle.com\/kanncaa1) and has more than 15 courses on Udemy.\n\n# **Content**\n\n## Regression\n\n* [Linear Regression](#1.)\n* [Multiple Linear Regression](#2.)\n* [Polynomial Linear Regression](#3.)\n* [Support Vector Regression](#4.)\n* [Decision Tree Regression](#5.)\n* [Random Forest Regression](#6.)\n\n![](https:\/\/iili.io\/J1bpse.md.png)\n\n\n\n\n## Classification\n\n* [K-Nearest Neighbour (KNN) Classification](#7.)\n* [Support Vector Machine (SVM) Classification](#8.)\n* [Naive Bayes Classification](#9.)\n* [Decision Tree Classification](#10.)\n* [Random Forest Classification](#11.)\n\n![](https:\/\/iili.io\/J1bmX9.png)\n\n\n\n\n\n## Clustering\n\n* [K-Means Clustering](#12.)\n* [Hierarchical Clustering](#13.)\n\n\n![](https:\/\/iili.io\/J1m9qu.png)\n\n\n\n\n## Other Content\n\n* [Natural Language Process (NLP)](#14.)\n* [Principal Component Analysis (PCA)](#15.)\n* [Model Selection](#16.)\n* [Recommendation Systems](#17.)\n\n\n","6b9890cb":"\n<a class=\"anchor\" id=\"4.\"><\/a> \n# Support Vector Regression","7f91f25e":"\n<a class=\"anchor\" id=\"15.\"><\/a> \n# Principal Component Analysis (PCA)","9eb0b31f":"# Regression\n<a class=\"anchor\" id=\"1.\"><\/a> \n# Linear Regression\n\n","af12a8e0":"\n<a class=\"anchor\" id=\"16.\"><\/a> \n# Model Selection","8f28b54d":"\n<a class=\"anchor\" id=\"5.\"><\/a> \n# Decision Tree Regression","aae51512":"\n<a class=\"anchor\" id=\"13.\"><\/a> \n# Hierarchical Clustering"}}