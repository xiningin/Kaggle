{"cell_type":{"2502f27a":"code","c70ab67e":"code","3453b384":"code","c9f7dc86":"code","ce405167":"code","9ddb0cb0":"code","6074f7cf":"code","58ed6e88":"code","85e8f2cc":"code","684483ea":"code","04d1370a":"code","a97bea3e":"code","c0f846cd":"code","a676a034":"code","37a04c23":"code","5e86262f":"code","14adbe15":"code","a268c12e":"code","f1930d5f":"code","6810b9a0":"code","efba5bde":"code","d74b9db0":"code","508b9712":"code","24ba443c":"code","abece0b9":"code","83bf7e61":"code","7b50cc8f":"code","d6c7cb9b":"code","1c6e5671":"markdown","0d85e135":"markdown","3cbbd776":"markdown","1c789b07":"markdown","c04ea625":"markdown","605192d4":"markdown","b4c9d47b":"markdown","868138ff":"markdown","8976f5da":"markdown","59934e0c":"markdown","9977f858":"markdown","8aa5b7cb":"markdown","da538be5":"markdown","7d865a62":"markdown","bc0f1f56":"markdown","e528d630":"markdown","3a546f9f":"markdown","c735cd16":"markdown","c11aeb00":"markdown","1cc949b4":"markdown","3d0457bb":"markdown","302df0c0":"markdown","36d774de":"markdown","4ab0db1e":"markdown","e9909797":"markdown","7f5bf575":"markdown","7ac3baaf":"markdown","c1c4a28b":"markdown","74aa5fe6":"markdown","4c863388":"markdown","9b7ea007":"markdown","f5426a06":"markdown","9ad28d05":"markdown","54e82ca9":"markdown","fe8057d5":"markdown"},"source":{"2502f27a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","c70ab67e":"data = pd.read_csv('..\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')\n\ndata = data.drop(0,axis=0)\n\ndata.head()","3453b384":"fig = make_subplots(rows=1,cols=2,shared_yaxes=True,\n                   subplot_titles=['Programming Experience among Data scientists','ML Experience among Data scientists'])\n\nprog_exp = data['Q6'].dropna()\nml_exp = data['Q15'].dropna()\n\nfig.add_trace(go.Bar(x=prog_exp.value_counts().keys(), \n             y=prog_exp.value_counts().values,\n             text=prog_exp.value_counts().values,\n                    textposition='auto',showlegend=False),1,1)\n\nfig.add_trace(go.Bar(x=ml_exp.value_counts().keys(), \n             y=ml_exp.value_counts().values,\n             text=ml_exp.value_counts().values,\n                    textposition='auto',showlegend=False),1,2)\nfig.update_xaxes(title_text=\"Programming Experience\", row=1, col=1)\nfig.update_yaxes(title_text=\"Number of people\", row=1, col=1)\n\nfig.update_xaxes(title_text=\"ML Experience\", row=1, col=2)\nfig.show()","c9f7dc86":"exp = data[['Q6',\n            'Q15']]\nexp.columns= ['prog_exp','ml_exp']\nexp.dropna(inplace=True)\nfig = go.Figure()\nfor prog_exp in exp['prog_exp'].unique():\n    pg_group = exp[exp['prog_exp']==prog_exp]\n    dat =pg_group['ml_exp'].value_counts()\n    fig.add_trace(go.Bar(x=dat.index, y=dat.values,name='Program Exp: '+prog_exp,\n                        text=dat.values,\n                        textposition='auto'))\nfig.update_xaxes(title_text=\"ML Experience\")\nfig.update_yaxes(title_text=\"Number of People\")\nfig.update_layout(title='Machine learning Experice Vs Programming Experience')\nfig.show()","ce405167":"exp = data[['Q6',\n            'Q15']]\nexp.columns= ['prog_exp','ml_exp']\nexp.dropna(inplace=True)\nfig = go.Figure()\nfor ml_ex in exp['ml_exp'].unique():\n    pg_group = exp[exp['ml_exp']==ml_ex]\n    dat =pg_group['prog_exp'].value_counts()\n    fig.add_trace(go.Bar(x=dat.index, y=dat.values,name='ML Exp: '+ml_ex,\n                        text=dat.values,\n                        textposition='auto'))\nfig.update_xaxes(title_text=\"Programming Experience\")\nfig.update_yaxes(title_text=\"Number of People\")\nfig.update_layout(title='Programming Experice Vs ML Experience')\nfig.show()","9ddb0cb0":"\nmap_keys={}\nmap_keys['$0 ($USD)']= '0'\nmap_keys['$1-$99'] ='1-99'\nmap_keys['$10,000-$99,999'] = '10,000-99,999'\nmap_keys['$100-$999'] = '100-999'\nmap_keys['$1000-$9,999']= '1000-9,999'\nmap_keys['$100,000 or more ($USD)'] = '>100,000'\n\n\nexp = data[['Q25',\n            'Q15']]\nexp.columns= ['money_spent','ml_exp']\n# exp.head()\nexp.dropna(inplace=True)\n\nfig = go.Figure()\nfor ml_ex in exp['ml_exp'].unique():\n    pg_group = exp[exp['ml_exp']==ml_ex]\n    dat =pg_group['money_spent'].value_counts()\n    fig.add_trace(go.Bar(x=[map_keys[i] for i in dat.index], y=dat.values,name='ML Exp: '+ml_ex,\n                        text=dat.values,\n                        textposition='auto'))\nfig.update_xaxes(title_text=\"Money Spent in USD\")\nfig.update_yaxes(title_text=\"Number of People\")\nfig.update_layout(title='Machine learning Experice Vs Money spent')\nfig.show()\n","6074f7cf":"\nsubs = 'Q37' \ncols = [i for i in list(data.columns) if subs in i]\nlearn =data[cols]\nplatform={}\nfor col in cols:\n    platform.update(learn[col].value_counts())\nplatform = dict( sorted(platform.items(),key=lambda item: item[1],\n                           reverse=True)) \n\ncourse_platforms = list(platform.keys())[-3:]\n\nfig = go.Figure(go.Bar(x=list(platform.values())\n                       , y=list(platform.keys()), orientation='h'))\nfig.update_layout(showlegend=False,title_text=\"Competitive Course platforms\")\nfig.show()\nprint(course_platforms)","58ed6e88":"\nsubs = 'Q7' \ncols = [i for i in list(data.columns) if subs in i]\nlearn =data[cols]\nplatform={}\n\nfor col in cols:\n    platform.update(learn[col].value_counts())\nplatform = dict( sorted(platform.items(),key=lambda item: item[1],\n                           reverse=True)) \n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=['Programming language in Practice',\n                                    'Programming language for Aspiring Data scientist'])\nfig.add_trace((go.Bar(x=list(platform.values())\n                       , y=list(platform.keys()), orientation='h')),1,1)\n\n\nprog = data['Q8'].dropna()\n\nfig.add_trace((go.Bar(x=prog.value_counts().keys(), y=prog.value_counts().values)),1,2)\nfig.update_layout(showlegend=False,title_text=\"Programming language in Practice Vs Programming language for Aspiring Data scientist\")\nfig.show()\ndat = dict( sorted(prog.value_counts().items(),key=lambda item: item[1],\n                           reverse=True)) \n\npr_in_pract =  list(dat.keys())[-3:]\nprint(pr_in_pract)","85e8f2cc":"ml_tools={}\nquestions ={}\nquestions['Q9']='IDE for Data scientists'\nquestions['Q10']= 'Host notebooks for Data scientists'\nquestions['Q11']= 'Cloud computing for Data scientists'\nquestions['Q12']='Hardware for Data scientists'\nquestions['Q13']= 'TPU Usage by Data scientists'\nquestions['Q14']='Visualization for Data scientists'\nquestions['Q16']='ML Frameworks for Data scientists'\nquestions['Q17']='ML algorithms for Data scientists'\nquestions['Q26_A']='Cloud Computing platforms for Data scientists'\nquestions['Q27_A']='Cloud computing products for Data scientists'\nquestions['Q28_A']='ML  Products for Data scientists'\nquestions['Q33_A']= 'Auto ML usage for Data scientists'\nquestions['Q34_A']='Auto ML Tools for Data scientists'\nquestions['Q35_A']='Managment tools in ML for Data scientists'\nquestions['Q19']='NLP for Data scientists'\nquestions['Q18'] = 'CV for Data scientists'\nfor subs in questions.keys():\n    \n    cols = [i for i in list(data.columns) if subs in i]\n    if len(cols)>1:\n        learn =data[cols]\n        platform={}\n\n        for col in cols:\n            platform.update(learn[col].value_counts())\n        platform = dict( sorted(platform.items(),key=lambda item: item[1],\n                           reverse=True)) \n        ml_tools[subs]=list(platform.keys())[-2:]\n\n        fig = go.Figure(data=[go.Pie(labels=list(platform.keys()), values=list(platform.values()),\n                                    textinfo='label+percent',\n                             insidetextorientation='radial')])\n            \n        fig.update_layout(showlegend=False,title_text=questions[subs])\n        fig.show()\n    else:\n        \n        prog = data[cols[0]].dropna()\n        platform=dict( sorted(prog.value_counts().items(),key=lambda item: item[1],\n                           reverse=True))\n        ml_tools[subs]=list(platform.keys())[-2:]\n        fig = go.Figure(data=[go.Pie(labels=prog.value_counts().keys(), \n                                     values=prog.value_counts().values,\n                             textinfo='label+percent',\n                             insidetextorientation='radial')])\n        fig.update_layout(showlegend=False,title_text=questions[subs])\n        fig.show()\n    \n","684483ea":"\nsubs = 'Q29_A'\ncols = [i for i in list(data.columns) if subs in i]\nlearn =data[cols]\nplatform={}\n\nfor col in cols:\n    platform.update(learn[col].value_counts())\nplatform = dict( sorted(platform.items(),key=lambda item: item[1],\n                           reverse=True)) \n\n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=['Big Data products in Practice',\n                                    'Big data often used by Aspiring Data scientist'],\n                   horizontal_spacing=0.4)\nfig.add_trace((go.Bar(x=list(platform.values())\n                       , y=list(platform.keys()), orientation='h')),1,1)\n\n\nprog = data['Q30'].dropna()\n\nfig.add_trace((go.Bar(x=prog.value_counts().values, y=prog.value_counts().keys(), orientation='h')),1,2)\nfig.update_layout(showlegend=False,title_text=\"Big Data produts in Practice Vs Big Data products for Aspiring Data scientist\")\nfig.show()\n\nplatform = dict( sorted(prog.value_counts().items(),key=lambda item: item[1],\n                           reverse=True))\nbd_tools=list(platform.keys())[-3:]\nprint(bd_tools)","04d1370a":"prog = data['Q38'].dropna()\nfig = go.Figure(data=[go.Pie(labels=prog.value_counts().keys(), \n                             values=prog.value_counts().values,\n                     textinfo='label+percent',\n                     insidetextorientation='radial')])\nfig.update_layout(showlegend=False,title_text='Data analysis tool for Data scientists')\nfig.show()\nplatform = dict( sorted(prog.value_counts().items(),key=lambda item: item[1],\n                           reverse=True))\nbd_analy=list(platform.keys())[-3:]\nprint(bd_analy)","a97bea3e":"\n\nsubs = 'Q31'\ncols = [i for i in list(data.columns) if subs in i]\nlearn =data[cols]\nplatform={}\n\nfor col in cols:\n    platform.update(learn[col].value_counts())\nplatform = dict( sorted(platform.items(),key=lambda item: item[1],\n                           reverse=True)) \n\nfig = make_subplots(rows=1, cols=2,\n                    subplot_titles=['BI tools in Practice',\n                                    'BI tools often used by Aspiring Data scientist'],\n                   horizontal_spacing=0.4)\nfig.add_trace((go.Bar(x=list(platform.values())\n                       , y=list(platform.keys()), orientation='h')),1,1)\n\n\nprog = data['Q32'].dropna()\n\nfig.add_trace((go.Bar(x=prog.value_counts().values, y=prog.value_counts().keys(), orientation='h')),1,2)\nfig.update_layout(showlegend=False,title_text=\"BI tools in Practice Vs BI tools for Aspiring Data scientist\")\nfig.show()\n\n\nplatform = dict( sorted(prog.value_counts().items(),key=lambda item: item[1],\n                           reverse=True))\nbi_tools=list(platform.keys())[-3:]\nprint(bi_tools)","c0f846cd":"conf={}\nquestions ={}\nquestions['Q36']='Sharing platforms for Data scientists'\nquestions['Q37']= 'EdTech for Data scientists'\nquestions['Q39']= 'Media sources for Data scientists'\n\nfor subs in questions.keys():\n    cols = [i for i in list(data.columns) if subs in i]\n    if len(cols)>1:\n        learn =data[cols]\n        platform={}\n\n        for col in cols:\n            platform.update(learn[col].value_counts())\n        platform = dict( sorted(platform.items(),key=lambda item: item[1],\n                           reverse=True)) \n        conf[subs]= list(platform.keys())[-3:]\n\n        fig = go.Figure(data=[go.Pie(labels=list(platform.keys()), values=list(platform.values()),\n                                    textinfo='label+percent',\n                             insidetextorientation='radial')])\n            \n        fig.update_layout(showlegend=False,title_text=questions[subs])\n        fig.show()\n    else:\n        \n        prog = data[cols[0]].dropna()\n        platform = dict(sorted(prog.value_counts().items(),key=lambda item: item[1],\n                           reverse=True)) \n        conf[subs]= list(platform.keys())[-3:]\n        fig = go.Figure(data=[go.Pie(labels=prog.value_counts().keys(), \n                                     values=prog.value_counts().values,\n                             textinfo='label+percent',\n                             insidetextorientation='radial')])\n        fig.update_layout(showlegend=False,title_text=questions[subs])\n        fig.show()\nprint(conf)","a676a034":"\nmean, median, mini, maxi = [data['Time from Start to Finish (seconds)'].astype('int').mean(),\n                      data['Time from Start to Finish (seconds)'].astype('int').median(),\n                         data['Time from Start to Finish (seconds)'].astype('int').min(),\n                                 data['Time from Start to Finish (seconds)'].astype('int').max()]\ndf = pd.DataFrame(data['Time from Start to Finish (seconds)'].astype('int'))\n\n                      \nprint('Stats of the Duration in seconds')\ndf['Time from Start to Finish (seconds)'].describe()","37a04c23":"\ndata['Time from Start to Finish (seconds)']=data['Time from Start to Finish (seconds)'].astype('int')\ncounts, bin_edges = np.histogram(data[data['Time from Start to Finish (seconds)']>=200]['Time from Start to Finish (seconds)'].values, bins=3)\nconfused={}\nkeys = ['no','bit','yes']\nfor i in range(len(bin_edges)-1):\n    confused[keys[i]] =[bin_edges[i] , bin_edges[i+1]]\n\n\ndata['Time from Start to Finish (seconds)'].hist()\nprint(confused)","5e86262f":"def combine(x,lst):\n    if len(set(x)&set(lst))>=1:\n        return 1\n    else:\n        return 0\n    \nmult_cols =['Q37','Q9','Q10','Q12','Q14','Q16','Q17','Q26_A','Q27_A','Q28_A',\n            'Q29_A','Q31','Q33_A','Q34_A','Q35_A','Q19','Q18', 'Q36','Q37','Q39'\n           ]\nfor subs in mult_cols:\n    cols = [i for i in list(data.columns) if subs in i]\n    learn =data[cols].astype('str')\n    data[subs] = learn.apply(lambda x: '+'.join(x), axis = 1)\n\n    data[subs] = data[subs].str.split('+')\n","14adbe15":"mlvsprgm = {}\nexp = data[['Q6',\n            'Q15']]\nexp.columns= ['prog_exp','ml_exp']\nexp.dropna(inplace=True)\nfig = make_subplots(rows=3, cols=3,\n                    subplot_titles=tuple(exp['ml_exp'].unique()),\n                    specs=[[{\"type\": \"table\"},{\"type\": \"table\"},{\"type\": \"table\"}],\n                           [{\"type\": \"table\"},{\"type\": \"table\"},{\"type\": \"table\"}],\n                           [{\"type\": \"table\"},{\"type\": \"table\"},{\"type\": \"table\"}]])\n\nfor ml_ex in exp['ml_exp'].unique():\n    pg_group = exp[exp['ml_exp']==ml_ex]['prog_exp']\n    dat =pg_group.value_counts().sort_index()\n    dat =dict( sorted(dat.items(),key=lambda item: item[1],\n                           reverse=True))\n    if ml_ex!='I do not use machine learning methods':\n        mlvsprgm[ml_ex]=list(dat.keys())[-3:]\n\nmlvsspend={}\nexp = data[['Q25',\n            'Q15']]\nexp.columns= ['money_spent','ml_exp']\nexp.dropna(inplace=True)\nfor ml_ex in exp['ml_exp'].unique():\n    pg_group = exp[exp['ml_exp']==ml_ex]['money_spent']\n    dat =pg_group.value_counts().sort_index()\n    dat =dict( sorted(dat.items(),key=lambda item: item[1],\n                           reverse=True))\n    mlvsspend[ml_ex]=list(dat.keys())[-3:]","a268c12e":"def Anaylse_competitiveness(data):\n    programming_improvement=[]\n    perseverence_needed=[]\n    ML_improvement={}\n    Big_data=[]\n    BI=[]\n    Confidence=[]\n    Confused={}\n    education=['Doctoral degree', 'Master\u2019s degree', 'Bachelor\u2019s degree',\n               'Some college\/university study without earning a bachelor\u2019s degree',\n           'Professional degree']\n    for k, v in mlvsprgm.items():\n        ind_k = data[data['Q15']==k].index\n        ind_v = data.loc[ind_k][data.loc[ind_k]['Q6'].isin(v)].index\n        programming_improvement.extend(ind_v)\n        ind_v = data.loc[ind_k][data.loc[ind_k]['Q8'].isin(pr_in_pract)].index\n        programming_improvement.extend(ind_v)\n    for k, v in mlvsspend.items():\n        ind_e = data[-data['Q4'].isin(education)].index\n        data_ed = data.loc[ind_e]\n        ind_k = data_ed[data_ed['Q15']==k].index\n        ind_v = data_ed.loc[ind_k][data_ed.loc[ind_k]['Q25'].isin(v)].index\n        perseverence_needed.extend(ind_v)\n        data['Q37_res'] = data_ed.loc[ind_k]['Q37'].apply(lambda x: combine(x,course_platforms))\n        ind_v = data[data['Q37_res']==1].index\n        perseverence_needed.extend(ind_v)\n\n    cloud_computing=['Q9','Q11','Q26_A','Q27_A','Q13','Q10','Q13']\n    ML_core = ['Q16','Q17','Q28_A','Q33_A','Q34_A','Q35_A']\n    nlp=['Q19']\n    cv=['Q18']\n    cl, ml_c, NLP, CV=[],[],[],[]\n    for k,v in ml_tools.items():\n        if k in cloud_computing:\n            if k in mult_cols:\n                col =k+'res'\n                data[col] = data[k].apply(lambda x: combine(x,v))\n                ind_v = data[data[col]==1].index\n                cl.extend(ind_v)\n            else:\n                ind_v = data[data[k].isin(v)].index\n                cl.extend(ind_v)\n        elif k in ML_core:\n            if k in mult_cols:\n                col =k+'res'\n                data[col] = data[k].apply(lambda x: combine(x,v))\n                ind_v = data[data[col]==1].index\n                ml_c.extend(ind_v)\n            else:\n                ind_v = data[data[k].isin(v)].index\n                ml_c.extend(ind_v)\n        elif k in nlp:\n            if k in mult_cols:\n                col =k+'res'\n                data[col] = data[k].apply(lambda x: combine(x,v))\n                ind_v = data[data[col]==1].index\n                NLP.extend(ind_v)\n            else:\n                ind_v = data[data[k].isin(v)].index\n                NLP.extend(ind_v)\n        elif k in cv:\n            if k in mult_cols:\n                col =k+'res'\n                data[col] = data[k].apply(lambda x: combine(x,v))\n                ind_v = data[data[col]==1].index\n                CV.extend(ind_v)\n            else:\n                ind_v = data[data[k].isin(v)].index\n                CV.extend(ind_v)\n    ML_improvement['Cloud_computing'] = np.unique(cl)\n    ML_improvement['ML_core'] = np.unique(ml_c)\n    ML_improvement['NLP'] = np.unique(NLP)\n    ML_improvement['CV'] = np.unique(CV)\n\n\n    data['Q29_A_res'] = data['Q29_A'].apply(lambda x: combine(x,bd_tools))\n    ind_v = data[data['Q29_A_res']==1].index\n    Big_data.extend(ind_v)\n    ind_v =  data[data['Q38'].isin(bd_analy)].index\n    Big_data.extend(ind_v)\n    data['Q31_res'] = data['Q31'].apply(lambda x: combine(x,bi_tools))\n    ind_v = data[data['Q31_res']==1].index\n    BI.extend(ind_v)\n\n\n    for k,v in conf.items():\n        if k in mult_cols:\n            col =k+'res'\n            data[col] = data[k].apply(lambda x: combine(x,v))\n            ind_v = data[data[col]==1].index\n            Confidence.extend(ind_v)\n        else:\n            ind_v = data[data[k].isin(v)].index\n            Confidence.extend(ind_v)\n    data['Time from Start to Finish (seconds)'] = data['Time from Start to Finish (seconds)'].astype('int')\n    for k,v in confused.items():\n        start =v[0]\n        end=v[1]\n        ind_v = data[(data['Time from Start to Finish (seconds)']>=start) &\n             (data['Time from Start to Finish (seconds)']<=end)].index\n        Confused[k]=ind_v\n    ind_v = data[data['Time from Start to Finish (seconds)']<=200].index\n    Confused['yes'] = list(Confused['yes'])+list(ind_v)\n\n    programming_improvement =set(programming_improvement)    \n    perseverence_needed = set(perseverence_needed)\n    Big_data = set(Big_data)\n    BI = set(BI)\n    Confidence = set(Confidence)\n    return programming_improvement, perseverence_needed, Big_data, BI, Confidence, ML_improvement,Confused\n","f1930d5f":"programming_improvement, perseverence_needed, Big_data, BI, Confidence, ML_improvement,Confused = Anaylse_competitiveness(data)\n","6810b9a0":"Comp = []\nComp.extend(list(programming_improvement))\nComp.extend(list(perseverence_needed))\nComp.extend(list(Big_data))\nComp.extend(list(BI))\nComp.extend(list(ML_improvement['Cloud_computing']))\nComp.extend(list(ML_improvement['ML_core']))\nComp.extend(list(ML_improvement['NLP']))\nComp.extend(list(ML_improvement['CV']))\nComp.extend(list(Confidence))\nComp.extend(list(Confused['yes']))\nComp.extend(list(Confused['bit']))\nComp =np.unique(Comp)\nfig = go.Figure(data=[go.Pie(labels=['Less Competitive Data scientists','Competitive Data scientists'], \n                             values=[len(Comp),len(data)-len(Comp)],\n                                    textinfo='label+percent',\n                             insidetextorientation='radial')])\n            \nfig.update_layout(showlegend=False,title_text='Data scientists found Competitive based on the assumptions')\nfig.show()","efba5bde":"Analysis={}\nAnalysis['Need to improve programming'] = len(programming_improvement)\nAnalysis['Need to be more determined'] = len(perseverence_needed)\nAnalysis['Need to be improve with CLoud Computing'] = len(ML_improvement['Cloud_computing'])\nAnalysis['Need to be improve with Core ML'] = len(ML_improvement['ML_core'])\nAnalysis['Need to be improve with NLP'] = len(ML_improvement['NLP'])\nAnalysis['Need to be improve with CV'] = len(ML_improvement['CV'])\nAnalysis['Need to be improve with BigData'] = len(Big_data)\nAnalysis['Need to be improve with Business Intelligence'] = len(BI)\nAnalysis['Need to be more Confident'] = len(Confidence)\nAnalysis['Need to be more outspoken'] = len(Confused['yes']) + len(Confused['bit'])\n\nfig = go.Figure(data=[go.Pie(labels=list(Analysis.keys()), \n                             values=list(Analysis.values()),\n                                    textinfo='label+percent',\n                             insidetextorientation='radial')])\n            \nfig.update_layout(showlegend=False,title_text='Analysis on Less Competitive Data scientists based on the assumptions')\nfig.show()\n","d74b9db0":"LessComp_data = data.loc[Comp]\n\nk,v = 'Q1','Age Group'\ndat = LessComp_data[k].dropna()\ndat = LessComp_data[k].value_counts()\nfig = go.Figure(data=[go.Pie(labels=list(dat.keys()), values=list(dat.values),textinfo='none')])\nfig.update_layout(showlegend=True,title_text='Analysis on Less Competitive Data scientists - '+v+'(in years)')\nfig.show()","508b9712":"LessComp_data = data.loc[Comp]\n\nk,v = 'Q2','Gender'\ndat = LessComp_data[k].dropna()\ndat = LessComp_data[k].value_counts()\nfig = go.Figure(data=[go.Pie(labels=list(dat.keys()), values=list(dat.values),textinfo='none')])\nfig.update_layout(showlegend=True,title_text='Analysis on Less Competitive Data scientists - '+v+'(in years)')\nfig.show()","24ba443c":"LessComp_data = data.loc[Comp]\n\nk,v = 'Q3','Country'\ndat = LessComp_data[k].dropna()\ndat = LessComp_data[k].value_counts()\nfig = go.Figure(data=[go.Pie(labels=list(dat.keys()), values=list(dat.values),textinfo='none')])\nfig.update_layout(showlegend=True,title_text='Analysis on Less Competitive Data scientists - '+v+'(in years)')\nfig.show()","abece0b9":"LessComp_data = data.loc[Comp]\n\nk,v = 'Q5','Job role'\ndat = LessComp_data[k].dropna()\ndat = LessComp_data[k].value_counts()\nfig = go.Figure(data=[go.Pie(labels=list(dat.keys()), values=list(dat.values),textinfo='none')])\nfig.update_layout(showlegend=True,title_text='Analysis on Less Competitive Data scientists - '+v+'(in years)')\nfig.show()","83bf7e61":"mydata = pd.read_csv('..\/input\/my-data\/mydata.csv')\nheader= mydata.loc[0]\nmydata = mydata.drop(0,axis=0)\nmydata.columns= header\nmydata.head()","7b50cc8f":"mult_cols =['Q37','Q9','Q10','Q12','Q14','Q16','Q17','Q26_A','Q27_A','Q28_A',\n            'Q29_A','Q31','Q33_A','Q34_A','Q35_A','Q19','Q18', 'Q36','Q37','Q39'\n           ]\nfor subs in mult_cols:\n    cols = [i for i in list(mydata.columns) if subs in i]\n    learn =mydata[cols].astype('str')\n    mydata[subs] = learn.apply(lambda x: '+'.join(x), axis = 1)\n\n    mydata[subs] = mydata[subs].str.split('+')\n    ","d6c7cb9b":"programming_improvement, perseverence_needed, Big_data, BI, Confidence, ML_improvement,Confused = Anaylse_competitiveness(mydata)\n\n\nprint('Report of the Analysis')\n\nprint('In Programming:-')\nif len(programming_improvement)>0:\n    print('Improvement Needed')\nelse:\n    print('So far good')\n    \nprint('In Determination:-')\nif len(perseverence_needed)>0:\n    print('Improvement Needed')\nelse:\n    print('So far good')    \nprint('In Machine Learning:-')\nprint('Improvement needed in the following areas...')\nfor k,v in ML_improvement.items():\n    if len(v)>0:\n        print(k)\n        \nprint('In Confidence:-')\nfor k,v in Confused.items():\n    if len(v)>0:\n        if k=='no':\n            print('You are confident enough')\n        if k=='yes' or k=='bit':\n            print('You need to boost your confidence more')","1c6e5671":"# Helping Data scientists to be Competitive\n\nBased on the insights gained by the Survey data, the following are the factors indicating that you are less competitive in your field.\n1. If one is performing less important tasks in your field rather than the most important ones.\n2. If one lack in perseverence.\n3. If one is less confident in sharing\/participating in your community or confused\n\n\nUnder these assumption, Lets try to find the features influencing competitive nature of the  Data scientists and help them with a report on where to develop their skills on,\n\nThe factors in Data science affecting the above mentioned statements are,\n1. Less important tasks - If the tools and products used by one is in the bottom ranks of the most used by Data scientists in the world.\n2. Lack of perseverence - If one do not engage in learning and spending with what you want.\n3. Less confident - If one does not participate in Data science community and Confused with oneself- If one had taken very long duration in this survey\n\n## Table of Contents\n\n* [Explonatory Data Analysis](#section-one)\n    - [Experience in ML Vs Experience in Programming](#subsection-one)\n    - [Features affecting Perseverence in a Data Scientist](#subsection-two)\n    - [Finding less important tools\/products used by Data Scientists](#subsection-three)\n        - [In Machine Learning](#subsection-three-subone)\n        - [In Big Data](#subsection-three-subtwo)\n        - [In Business Intelligence](#subsection-three-subthree)\n    - [Features affecting Confidence in a Data Scientist](#subsection-four)\n    - [Were you in confusion?](#subsection-five)\n    \n* [Analysis of Competence in Data Scientists](#section-two)\n    - [Data Processing and Categorization](#subsection-oneintwo)\n    - [Visualizing the Findings from the above observations](#subsection-twointwo)\n    - [Deep Dive into the Less Competitive Data scientists](#subsection-threeintwo)\n    - [Let me see What category I fit into?](#subsection-fourintwo)\n* [Disclaimer and Conclusion](#section-three)\n\n","0d85e135":"If the programming language in practice is in the top 3 programming languages used by an aspiring data scientist makes you competetitive.","3cbbd776":"Adult Data scientists are more competitive compared to middle age categories. Obviously growing up teaches the improtance of determination!","1c789b07":"<a id=\"subsection-four\"><\/a>\n## Features affecting Confidence in a Data Scientist\n\nAnalysis performed: \nWe consider the sharing and learning platforms used by Data scientists. Further more social media usage by Data scientists is also considered. These factors shows the confidence of any data scientist on getting involved with the Data Science Community\n\nQuestions from the survey under analysis:\n\n1. Where do you publicly share or deploy your data analysis or machine learning applications?\t\n2.\tOn which platforms have you begun or completed data science courses? \n3.\tWho\/what are your favorite media sources that report on data science topics? \n","c04ea625":"<a id=\"subsection-three-subone\"><\/a>\n### In Machine Learning","605192d4":"<a id=\"section-two\"><\/a>\n# Analysis of Competence in Data Scientists\n","b4c9d47b":"Being a Big data practitioner, the data analysis tool is the most useful in practice and so it is needed to get in touch with the top used tool.","868138ff":"Similar to programming in ML,  if the Big data tool\/products in practice falls on the top used products tags you being competitive big data practitioner\n","8976f5da":"<a id=\"subsection-three-subthree\"><\/a>\n### In Business Intelligence","59934e0c":"We find about 63% of Data scientists in the survey as Less competitive. No offense, it is just based on the assumptions and from the survey data.","9977f858":"If you are not in any education course, involved with an top data science course platforms make you one Determined Data scientist. ","8aa5b7cb":"<a id=\"subsection-twointwo\"><\/a>\n## Visualizing the Findings from the above observations","da538be5":"## Thank You!!!","7d865a62":"If the Business Intelligence tool\/products in practice falls on the top used products tags you being competitive business intelligence.\n","bc0f1f56":"As you can see, the performance in ML as well as Programming is more likely equally distributed for a competitive data scientist. \n\n**For example**:  \nThe data scientists with ML experience under one year (first group in the bar) has the highest of programming experience of about 1-2 years or under 1 year.\n\nNow,let us see the relationship vice versa (Programming Vs ML)","e528d630":"<a id=\"subsection-three\"><\/a>\n## Finding less important tools\/products used by Data scientists\nAnalysis performed:\nWe consider the ranking of most used tools\/products by the Data scientists in the world.\n\n**In Machine Learning**\n\nQuestions from the survey under analysis:\n1. What programming languages do you use on a regular basis?\n2. What programming language would you recommend an aspiring data scientist to learn first? \n3. Which of the following integrated development environments (IDE's) do you use on a regular basis?  4. Which of the following hosted notebook products do you use on a regular basis?  \n5.\tWhat type of computing platform do you use most often for your data science projects?\t\n6.\tWhich types of specialized hardware do you use on a regular basis?  \n7.\tApproximately how many times have you used a TPU (tensor processing unit)?\t\n8.\tWhat data visualization libraries or tools do you use on a regular basis?  \n9.\tWhich of the following machine learning frameworks do you use on a regular basis? \n10.\tWhich of the following ML algorithms do you use on a regular basis?  \n11.\tWhich categories of computer vision methods do you use on a regular basis?  \n12.\tWhich of the following natural language processing (NLP) methods do you use on a regular basis?  \n13.\tWhich of the following cloud computing platforms do you use on a regular basis\n14.\tDo you use any of the following cloud computing products on a regular basis? \n15.\tDo you use any of the following machine learning products on a regular basis? \n16.\tDo you use any automated machine learning tools (or partial AutoML tools) on a regular basis\t\n17.\tWhich of the following automated machine learning tools (or partial AutoML tools) do you use on a regular basis?  \n18.\tDo you use any tools to help manage machine learning experiments? \n\n\n\n**In Big Data**\n \nQuestions from the survey under analysis:\n\n1. Which of the following big data products (relational databases, data warehouses, data lakes, or similar) do you use on a regular basis? \n2.\tWhich of the following big data products (relational database, data warehouse, data lake, or similar) do you use most often? \n3. What is the primary tool that you use at work or school to analyze data? \n\n\n\n**In Business Intelligence**\n\nQuestions from the survey under analysis:\n\n1.\tWhich of the following business intelligence tools do you use on a regular basis\n2.\tWhich of the following business intelligence tools do you use most often? \n","3a546f9f":"From the 63% of less competitive data scientists found based on the assumptions, we have analysed and reported the advice for their competence in various fields such as Cloud computing, programming, ML core, etc...","c735cd16":"<a id=\"subsection-five\"><\/a>\n## Where you in confusion?\n\nAnalysis performed: \nWe consider the time taken by the Data scientists and group them into three categories such as \n\n* Highly confused - if you are in a hurry to complete the survey or you took lot of time to complete the same\n* Confused - if you take more time than normal range taken by data scientists\n* Not confused - if your time taken is in the normal range taken by data scientists\n\nFetures considered:\n\n* Duration of Seconds\n","c11aeb00":"<a id=\"subsection-one\"><\/a>\n## Experience in ML Vs Experience in Programming\n\nAnalysis performed:\nFinding the relation between the Coding experience and Machine Learning Experience with the Data scientists in the world\n\nQuestions from the survey under analysis:\n\n1. For how many years have you been writing code and\/or programming?\n2. For how many years have you used machine learning methods?","1cc949b4":"**Tools and Products in ML**  \nThe ranking of top tools and products in practice among the data scientists in the world, gives an idea on where to concentrate for an ML practitioner","3d0457bb":"<a id=\"section-one\"><\/a>\n# Explonatory Data Analysis","302df0c0":"As the percentage of Gender-Male is high in the survey data, and so it reflects in the competitive split. ","36d774de":"<a id=\"subsection-five\"><\/a>\n# Disclaimer and Conclusion\n\n### Disclaimer:\nThe complete analysis are based on assumptions\/insights gained by analysing DS survey data. And the results\/observations shown are based on categorizing the Survey data in this Notebook. Please do not assume or take any observation literally.\n\n\n### Conclusion:\nAll the chart analysis gives us a report or tell us what should we concentrate more on during the upcoming challenges in Data science. A main drawback would be dropping the missing values. It is really great if this helps. If you have even a slightest positive feeling towards this notebook, don't forget to Upvote!!!","4ab0db1e":"From the above plots, we can analyse one's competence with respect to Machine learning, Big data and Business intelligence.","e9909797":"<a id=\"subsection-three-subtwo\"><\/a>\n### In Big Data","7f5bf575":"Both Machine learning experience is correlating with the Programming experience. Obviously you need programming experience if you have machine learning experience. If not,you need to improve your programming skills. And Vice Versa!\n","7ac3baaf":"---","c1c4a28b":"<a id=\"subsection-two\"><\/a>\n## Features affecting perseverance in a Data scientist\nAnalysis performed:\nWe consider the distribution  of money spent by experienced Data scientists and the learning curve of any Data scientist.\n\nQuestions from the survey under analysis:\n1. Approximately how much money have you (or your team) spent on machine learning and\/or cloud computing services at home (or at work) in the past 5 years (approximate USD)?\n2. On which platforms have you begun or completed data science courses?\n3. What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\n\n","74aa5fe6":"Another feature to be noted is how much you learn in ML?\n\nIf the learning have not started or in the bottom three ranks of the survey determines the perseverance on becoming a Data scientist. As this is an individual choice, the weightage to competitive is low. For example, if you are in any of the education (be it Data science or not) a seperate course is not needed which is determined by the below question in the survey\n\n**What is the highest level of formal education that you have attained or plan to attain within the next 2 years?**","4c863388":"The ranking of money spent to gain good experience in Data science is noted and based on the survey and the important aspect of being competitive is \"**Perseverance**\", bottom 3 ranks in the money spent vs ML experience takes the tag \"**Need to be more determined**\". ","9b7ea007":"<a id=\"subsection-threeintwo\"><\/a>\n## Deep Dive into the Less Competitive Data scientists\n\n","f5426a06":"<a id=\"subsection-oneintwo\"><\/a>\n## Data Processing and Categorization\n","9ad28d05":"From the above graph, it is evident that the number of years with respect to Programming and ML cannot make assumption on the competence of a data scientist. Hence, an analysis on the inter-relation between the ML and Programming experience is needed as below.","54e82ca9":"From the above ranking with respect to Sharing platforms, EdTech and Social media for a confident Data scientist is noted.","fe8057d5":"<a id=\"subsection-fourintwo\"><\/a>\n## Let me see What category I fit into?"}}