{"cell_type":{"f4f2d054":"code","6dba87c6":"code","26ed180e":"code","36956317":"code","50959fb2":"code","ed09e531":"code","774b3460":"code","030a0b3e":"code","32eaa080":"code","4e7a3d26":"code","9e70d607":"code","cc476d86":"code","d47a4ce1":"code","0701ed5e":"code","4b5ef8f6":"code","f7bdc08d":"code","ae6ec6ac":"code","3ed57461":"code","17649381":"code","ec8db2da":"code","6a6c2c93":"code","243c61e4":"code","52f24e12":"code","141421f0":"code","5a4c9f86":"code","69e22615":"code","a4843f2f":"code","35caeac3":"code","9b6398ba":"code","b9f4de57":"code","27a7f414":"code","a4e3c78f":"code","0012b41c":"code","54cc5c25":"code","33050d04":"code","dab3eec3":"code","574b9d5d":"code","6b93c247":"code","132ca256":"code","ca9dfb97":"code","d7a234b9":"markdown","db0c64b8":"markdown","0f4cbef1":"markdown","3d73a40b":"markdown","7b168cf8":"markdown","430ac526":"markdown","85e005fa":"markdown","e3874054":"markdown"},"source":{"f4f2d054":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6dba87c6":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nplt.style.use('fivethirtyeight')\n\nfrom mpl_toolkits.basemap import Basemap\n\nfrom numpy import array\nfrom matplotlib import cm\nfrom matplotlib.dates import date2num\nfrom mpl_toolkits.basemap import Basemap\n\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.colors import Normalize\nimport warnings\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning","26ed180e":"df=pd.read_csv('..\/input\/ataljalyojana\/Atal Jal 31 March 2021 .xlsx - Sheet1.csv')\n","36956317":"df.head().T","50959fb2":"df['State']=df['State'].str.lower()\n","ed09e531":"df['State'].value_counts()","774b3460":"df_guj=df[df['State']=='gujarat']\ndf_maha=df[df['State']=='maharashtra']\ndf_raj=df[df['State']=='rajasthan']\ndf_har=df[df['State']=='haryana']\ndf_mp=df[df['State']=='madhya pradesh']\ndf_up=df[df['State']=='uttar pradesh']\ndf_kar=df[df['State']=='karnataka']","030a0b3e":"df.columns.values","32eaa080":"col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n      'Mean_Pre_2015_depth':[df_guj['Pre_2015'].mean(),df_maha['Pre_2015'].mean(),\n                         df_kar['Pre_2015'].mean(),df_har['Pre_2015'].mean(),\n                         df_mp['Pre_2015'].mean(),df_up['Pre_2015'].mean(),df_raj['Pre_2015'].mean()],\n      'Mean_pst_2015_depth':[df_guj['Pst_2015'].mean(),df_maha['Pst_2015'].mean(),\n                         df_kar['Pst_2015'].mean(),df_har['Pst_2015'].mean(),\n                         df_mp['Pst_2015'].mean(),df_up['Pst_2015'].mean(),df_raj['Pst_2015'].mean()]}\n      ","4e7a3d26":"con_table1=pd.DataFrame(col2)\ncon_table1.head()","9e70d607":"con_table1['total']=con_table1['Mean_Pre_2015_depth']+con_table1['Mean_pst_2015_depth']","cc476d86":"con_table1","d47a4ce1":"con_table1.sum()","0701ed5e":"n=len(con_table1)*2","4b5ef8f6":"total_sum=np.sum(con_table1['total'])\nprint(total_sum)\ngrand_mean=np.mean(con_table1['total'])\ngrand_mean","f7bdc08d":"s1=0\nfor i in con_table1['Mean_Pre_2015_depth']:\n    s1=s1+i**2\nprint(s1)\ns2=0\nfor i in con_table1['Mean_pst_2015_depth']:\n    s2=s2+i**2\nprint(s2)\n\nsqure_sum=s1+s2\nsqure_sum\n    ","ae6ec6ac":"correction_factor=(total_sum**2)\/n\ncorrection_factor","3ed57461":"tss=squre_sum-correction_factor\ntss","17649381":"con_table1['total'][1]","ec8db2da":"a=con_table1['total'][0]**2+con_table1['total'][1]**2+con_table1['total'][2]**2+con_table1['total'][3]**2+con_table1['total'][4]**2+con_table1['total'][5]**2+con_table1['total'][6]**2\nSSt=(a\/2)-correction_factor\nSSt","6a6c2c93":"SSe=tss-SSt\nSSe","243c61e4":"col={'Sources of Variation':['Among the State', 'Error', 'total'], \"Dof\":[len(con_table1)-1, n-len(con_table1), n-1],\n           \"Sum Of Square\":[SSt, SSe, tss]}\nanova_table=pd.DataFrame(col)\nanova_table\n        ","52f24e12":"anova_table['Mea Square']=anova_table['Sum Of Square']\/anova_table['Dof']\nanova_table","141421f0":"anova_table['F value']=anova_table['Mea Square'][0]\/anova_table['Mea Square'][1]","5a4c9f86":"anova_table","69e22615":"def anova_analysis(data, a, b):\n    n=len(data)*2\n    \n    data['total']=data[a]+data[b]\n    \n    total_sum=np.sum(data['total'])\n    \n    s1=0\n    for i in data[a]:\n        s1=s1+i**2\n        #print(s1)\n    s2=0\n    for i in data[b]:\n        s2=s2+i**2\n        #print(s2)\n\n    squre_sum=s1+s2\n    #squre_sum\n    correction_factor=(total_sum**2)\/n\n    #correction_factor\n    tss=squre_sum-correction_factor\n    #tss\n    s3=0\n    for i in data['total']:\n        s3=s3+i**2\n    # to calculate sst   \n    SSt=(s3\/2)-correction_factor\n    # to calculate error\n    SSe=tss-SSt\n    # making Anova table\n    col={'Sources of Variation':['Among the State', 'Error', 'total'], \"Dof\":[len(data)-1, n-len(data), n-1],\n           \"Sum Of Square\":[SSt, SSe, tss]}\n    anova_table=pd.DataFrame(col)\n    #anova_table\n    anova_table['Mea Square']=anova_table['Sum Of Square']\/anova_table['Dof']\n    #anova_table\n    anova_table['F value']=anova_table['Mea Square'][0]\/anova_table['Mea Square'][1]\n    return anova_table\n","a4843f2f":"col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n      'Mean_Pre_2015_depth':[df_guj['Pre_2015'].mean(),df_maha['Pre_2015'].mean(),\n                         df_kar['Pre_2015'].mean(),df_har['Pre_2015'].mean(),\n                         df_mp['Pre_2015'].mean(),df_up['Pre_2015'].mean(),df_raj['Pre_2015'].mean()],\n      'Mean_pst_2015_depth':[df_guj['Pst_2015'].mean(),df_maha['Pst_2015'].mean(),\n                         df_kar['Pst_2015'].mean(),df_har['Pst_2015'].mean(),\n                         df_mp['Pst_2015'].mean(),df_up['Pst_2015'].mean(),df_raj['Pst_2015'].mean()]}\n      \ncon_table2=pd.DataFrame(col2)","35caeac3":"con_table2.columns","9b6398ba":"anova_analysis(con_table2, 'Mean_Pre_2015_depth', 'Mean_pst_2015_depth')","b9f4de57":"col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n      'Mean_Pre_2016_depth':[df_guj['Pre_2016'].mean(),df_maha['Pre_2016'].mean(),\n                         df_kar['Pre_2016'].mean(),df_har['Pre_2016'].mean(),\n                         df_mp['Pre_2016'].mean(),df_up['Pre_2016'].mean(),df_raj['Pre_2016'].mean()],\n      'Mean_pst_2016_depth':[df_guj['Pst_2016'].mean(),df_maha['Pst_2016'].mean(),\n                         df_kar['Pst_2016'].mean(),df_har['Pst_2016'].mean(),\n                         df_mp['Pst_2016'].mean(),df_up['Pst_2016'].mean(),df_raj['Pst_2016'].mean()]}\ncon_table3=pd.DataFrame(col2)\ncon_table3.columns","27a7f414":"anova_analysis(con_table3, 'Mean_Pre_2016_depth', 'Mean_pst_2016_depth')","a4e3c78f":"def con_table(a, b, c1, c2):\n    \n    col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n          c1:[df_guj[a].mean(),df_maha[a].mean(),\n                             df_kar[a].mean(),df_har[a].mean(),\n                             df_mp[a].mean(),df_up[a].mean(),df_raj[a].mean()],\n          c2 :[df_guj[b].mean(),df_maha[b].mean(),\n                             df_kar[b].mean(),df_har[b].mean(),\n                             df_mp[b].mean(),df_up[b].mean(),df_raj[b].mean()]}\n    con_table1=pd.DataFrame(col2)\n    return con_table1","0012b41c":"df.columns","54cc5c25":"def anova_analysis(data, a, b):\n    n=len(data)*2\n    \n    data['total']=data[a]+data[b]\n    \n    total_sum=np.sum(data['total'])\n    \n    s1=0\n    for i in data[a]:\n        s1=s1+i**2\n        #print(s1)\n    s2=0\n    for i in data[b]:\n        s2=s2+i**2\n        #print(s2)\n\n    squre_sum=s1+s2\n    #squre_sum\n    correction_factor=(total_sum**2)\/n\n    #correction_factor\n    tss=squre_sum-correction_factor\n    #tss\n    s3=0\n    for i in data['total']:\n        s3=s3+i**2\n    # to calculate sst   \n    SSt=(s3\/2)-correction_factor\n    # to calculate error\n    SSe=tss-SSt\n    # making Anova table\n    col={'Sources of Variation':['Among the State', 'Error', 'total'], \"Dof\":[len(data)-1, n-len(data), n-1],\n           \"Sum Of Square\":[SSt, SSe, tss]}\n    anova_table=pd.DataFrame(col)\n    #anova_table\n    anova_table['Mea Square']=anova_table['Sum Of Square']\/anova_table['Dof']\n    #anova_table\n    anova_table['F value']=anova_table['Mea Square'][0]\/anova_table['Mea Square'][1]\n    return anova_table\n","33050d04":"col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n      'Mean_Pre_2017_depth':[df_guj['Pre_2017'].mean(),df_maha['Pre_2017'].mean(),\n                         df_kar['Pre_2017'].mean(),df_har['Pre_2017'].mean(),\n                         df_mp['Pre_2017'].mean(),df_up['Pre_2017'].mean(),df_raj['Pre_2017'].mean()],\n      'Mean_pst_2017_depth':[df_guj['Pst_2017'].mean(),df_maha['Pst_2017'].mean(),\n                         df_kar['Pst_2017'].mean(),df_har['Pst_2017'].mean(),\n                         df_mp['Pst_2017'].mean(),df_up['Pst_2017'].mean(),df_raj['Pst_2017'].mean()]}\ncon_table3=pd.DataFrame(col2)\ncon_table3.columns","dab3eec3":"anova_analysis(con_table3, 'Mean_Pre_2017_depth', 'Mean_pst_2017_depth' )","574b9d5d":"col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n      'Mean_Pre_2018_depth':[df_guj['Pre_2018'].mean(),df_maha['Pre_2018'].mean(),\n                         df_kar['Pre_2018'].mean(),df_har['Pre_2018'].mean(),\n                         df_mp['Pre_2018'].mean(),df_up['Pre_2018'].mean(),df_raj['Pre_2018'].mean()],\n      'Mean_pst_2018_depth':[df_guj['Pst_2018'].mean(),df_maha['Pst_2018'].mean(),\n                         df_kar['Pst_2018'].mean(),df_har['Pst_2018'].mean(),\n                         df_mp['Pst_2018'].mean(),df_up['Pst_2018'].mean(),df_raj['Pst_2018'].mean()]}\ncon_table3=pd.DataFrame(col2)\ncon_table3.columns","6b93c247":"anova_analysis(con_table3, 'Mean_Pre_2018_depth', 'Mean_pst_2018_depth' )","132ca256":"col2={'State':['Gujarat', 'Maharastra', 'Karnataka', 'Harayna','MP', 'UP', 'Rajasthan'],\n      'Mean_Pre_2019_depth':[df_guj['Pre_2019'].mean(),df_maha['Pre_2019'].mean(),\n                         df_kar['Pre_2019'].mean(),df_har['Pre_2019'].mean(),\n                         df_mp['Pre_2019'].mean(),df_up['Pre_2019'].mean(),df_raj['Pre_2019'].mean()],\n      'Mean_pst_2019_depth':[df_guj['Pst_2019'].mean(),df_maha['Pst_2019'].mean(),\n                         df_kar['Pst_2019'].mean(),df_har['Pst_2019'].mean(),\n                         df_mp['Pst_2019'].mean(),df_up['Pst_2019'].mean(),df_raj['Pst_2019'].mean()]}\ncon_table3=pd.DataFrame(col2)\ncon_table3.columns","ca9dfb97":"anova_analysis(con_table3, 'Mean_Pre_2019_depth', 'Mean_pst_2019_depth' )","d7a234b9":"# Obs\n* Reject the null hypothesis and conclude that we there is significat difference between the Pre_2016 and Pst_2016","db0c64b8":"# Obs\n* Reject our null hypothesis.","0f4cbef1":"# Obs\n* as we can see that Calculated F value is greater than observed F value. \n* The Observed F- value from f table we get F {0.05, 6, 7} for alpha=0.95 we have 3.87\n* We can reject our null hypothesis.\n* We can conclude that there is difference in mean of these two features. ","3d73a40b":"# Annova Code","7b168cf8":"# ANOVA Table\n* We take two columns Pre_2015 and Pst_2015.\n* Ho-> There is no difference between the mean of two columns.\n* H1->  There is difference netween the mean of the two columns. ","430ac526":"# Obs\n* Reject our null hypothesis.\n","85e005fa":"# Assumptions of ANOVA\nThe assumptions of the ANOVA test are the same as the general assumptions for any parametric test:\n\n* Independence of observations: the data were collected using statistically-valid methods, and there are no hidden relationships among observations. If your data fail to meet this assumption because you have a confounding variable that you need to control for statistically, use an ANOVA with blocking variables.\n* Normally-distributed response variable: The values of the dependent variable follow a normal distribution.\n* Homogeneity of variance: The variation within each group being compared is similar for every group. If the variances are different among the groups, then ANOVA probably isn\u2019t the right fit for the data.","e3874054":"# Obs\n* Reject our null hypothesis."}}