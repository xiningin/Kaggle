{"cell_type":{"44fd5ac3":"code","53440977":"code","fc6569f5":"code","12705c2b":"code","9bdf6ed4":"code","3fc8de60":"code","c7f79fbf":"code","b816041c":"code","70d2b352":"code","2386b84e":"code","56d8778d":"code","1f16365a":"code","dbd9fe2a":"code","feeb73a2":"code","f12cc4bb":"code","63d54eab":"code","7c9d0696":"code","c42f3f13":"code","d682f638":"code","2a0c52c3":"code","ec90f5c1":"code","08bbddda":"code","1de0ebb1":"code","882738f4":"code","f1595176":"code","b06d3f43":"code","6a267b8b":"code","f6eb7fdd":"code","a13f783e":"code","80973340":"code","fbbdca81":"code","f931f79c":"code","0d32806a":"code","368fdc93":"code","9e4e50bb":"code","df34a3ce":"code","a660ce24":"code","b4a2f986":"code","236895cf":"code","40f06ce1":"code","fbfcedc2":"code","57930ef0":"code","b80fa49f":"code","8c8ed3f3":"code","f0e4a246":"code","30311408":"code","6896d093":"code","670652b9":"code","86b8d158":"code","bc2a16c0":"code","07afe81f":"code","ab56ae07":"code","cc1471e8":"code","5995deb6":"code","faf93c28":"code","3ac845f9":"markdown","e28ac07d":"markdown","e860d586":"markdown","d4f70f29":"markdown","38c20115":"markdown","dba01917":"markdown","5899b861":"markdown","21965267":"markdown","938a3568":"markdown","67e5052a":"markdown","7619c4a7":"markdown","60153928":"markdown","e4ee906e":"markdown","cc7d0c32":"markdown","e630bcca":"markdown","f95b1117":"markdown","9307203a":"markdown","a51a5079":"markdown","c4d71a86":"markdown"},"source":{"44fd5ac3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","53440977":"train_df=pd.read_csv('..\/input\/churn-risk-rate-hackerearth-ml\/train.csv')\ntrain_df.head()","fc6569f5":"test_df=pd.read_csv('..\/input\/churn-risk-rate-hackerearth-ml\/test.csv')\ntest_df.head()","12705c2b":"train_df.info()","9bdf6ed4":"test_df.info()","3fc8de60":"train_df['churn_risk_score'].unique()","c7f79fbf":"for col in test_df.columns:\n    if train_df[col].isnull().sum()!=0 or test_df[col].isnull().sum()!=0:\n        if train_df[col].dtype=='int64':\n            value=int(np.mean(train_df[col]))\n            train_df[col].fillna(value,inplace=True)\n            test_df[col].fillna(value,inplace=True)\n        elif train_df[col].dtype=='float64':\n            value=np.mean(train_df[col])\n            train_df[col].fillna(value,inplace=True)\n            test_df[col].fillna(value,inplace=True)\n        else:\n            train_df[col].fillna('None',inplace=True)\n            test_df[col].fillna('None',inplace=True)","b816041c":"train_df.isnull().sum()","70d2b352":"test_df.isnull().sum()","2386b84e":"train_df.shape","56d8778d":"cl_train_df=train_df.drop(labels=['customer_id','Name','security_no','referral_id','feedback'],axis=1)\ncl_test_df=test_df.drop(labels=['customer_id','Name','security_no','referral_id','feedback'],axis=1)","1f16365a":"cl_train_df.shape","dbd9fe2a":"type(cl_train_df['joining_date'][0])","feeb73a2":"def add_dates(data):\n    df=data\n    day=[]\n    month=[]\n    year=[]\n    for i in range(len(data)):\n        year.append(int(data['joining_date'][i][:4]))\n        month.append(int(data['joining_date'][i][5:7]))\n        day.append(int(data['joining_date'][i][8:10]))\n    df['day']=day\n    df['month']=month\n    df['year']=year\n    return df","f12cc4bb":"train_1=add_dates(cl_train_df)\ntest_1=add_dates(cl_test_df)","63d54eab":"train_1.drop('joining_date',1,inplace=True)\ntest_1.drop('joining_date',1,inplace=True)","7c9d0696":"train_1.head()\n    ","c42f3f13":"def add_time(data):\n    df=data\n    hour=[]\n    mint=[]\n    second=[]\n    for i in range(len(data)):\n        hour.append(int(data['last_visit_time'][i][:2]))\n        mint.append(int(data['last_visit_time'][i][3:5]))\n        second.append(int(data['last_visit_time'][i][6:8]))\n        \n    data['minute']=mint\n    data['hour']=hour\n    data['sec']=second\n    data.drop('last_visit_time',1,inplace=True)\n    return df","d682f638":"train=add_time(train_1)\ntest=add_time(test_1)","2a0c52c3":"train.head()","ec90f5c1":"for col in test_1.columns:\n    if train_1[col].dtype=='object':\n        print(col)","08bbddda":"for col in test.columns:\n    if train[col].dtype=='object':\n        if train[col].nunique() >20:\n            train.drop(col,1,inplace=True)\n            test.drop(col,1,inplace=True)\n        else:\n            k=0\n            for val in train[col].value_counts().index:\n                train[col].replace(val,k,inplace=True)\n                test[col].replace(val,k,inplace=True)\n                k+=1\n            \n            \n        \n    ","1de0ebb1":"train.head()","882738f4":"test.head()","f1595176":"X_train=train.drop('churn_risk_score',1)\ny_train=train['churn_risk_score']\nX_test=test","b06d3f43":"X_train.shape,X_test.shape,y_train.shape","6a267b8b":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport re","f6eb7fdd":"paragraph=[]\nfor line in train_df['feedback']:\n    paragraph.append(line)","a13f783e":"paragraph","80973340":"wordnet=WordNetLemmatizer()","fbbdca81":"corpus=[]\n\nfor i in range(len(paragraph)):\n    review=re.sub('[^a-zA-Z]',' ',paragraph[i])\n    review=review.lower()\n    review=review.split()\n    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review=' '.join(review)\n    corpus.append(review)\n    \ncorpus","f931f79c":"xx=pd.DataFrame(corpus)","0d32806a":"xx.columns=['name']","368fdc93":"xx.head()","9e4e50bb":"xx['name'].nunique()","df34a3ce":"feedback=xx['name'].unique()\nfeedback","a660ce24":"for i in range(9):\n    xx.replace(feedback[i],i,inplace=True)\nxx.head()","b4a2f986":"df1=pd.DataFrame({'1':xx['name'],'2':train_df['feedback']})\ndf1.head(15)","236895cf":"X_train['feedback']=xx['name']","40f06ce1":"paragraph=[]\nfor line in test_df['feedback']:\n    paragraph.append(line)","fbfcedc2":"corpus=[]\n\nfor i in range(len(paragraph)):\n    review=re.sub('[^a-zA-Z]',' ',paragraph[i])\n    review=review.lower()\n    review=review.split()\n    review=[wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n    review=' '.join(review)\n    corpus.append(review)\n    \ncorpus","57930ef0":"xx=pd.DataFrame({'name':corpus})","b80fa49f":"for i in range(9):\n    xx.replace(feedback[i],i,inplace=True)\nxx.head()","8c8ed3f3":"df1=pd.DataFrame({'1':xx['name'],'2':test_df['feedback']})\ndf1.head(15)","f0e4a246":"X_test['feedback']=xx['name']","30311408":"X_train.head()","6896d093":"X_test.head()","670652b9":"train_1=pd.concat([X_train,train_df['churn_risk_score']],axis=1)\ntrain_1.head()","86b8d158":"def create_submission(test,model,file_name):\n    y_pred=model.predict(X_test)\n    y_pred=y_pred.reshape(y_pred.shape[0])\n    y_pred=y_pred.tolist()\n    subs=pd.DataFrame({'customer_id':test_df['customer_id'],'churn_risk_score':y_pred})\n    subs.to_csv('file_name',index=False)\n    \n    \n    \ndef train_model(epochs,df,model):\n    for i in range(epochs):\n        print('Epoch ',i+1,' initiated................................')\n        func=model\n        df=train_1.sample(frac=0.8)\n        x_train=df.drop('churn_risk_score',1)\n        y_train=df['churn_risk_score']\n        func.fit(x_train,y_train)\n        print('Accuracy over this random data : ',func.score(x_train,y_train) )\n    return model","bc2a16c0":"X_test.head()","07afe81f":"from catboost import CatBoostClassifier as cbtc\nfrom sklearn.ensemble import RandomForestClassifier as rfc","ab56ae07":"df1=X_train.drop(labels=['joined_through_referral','year','month' ,  'minute', 'sec',],axis=1)\ndf2=X_test.drop(labels=['joined_through_referral','year', 'month', 'minute', 'sec',],axis=1)","cc1471e8":"model=cbtc(verbose=0)\nmodel.fit(df1,y_train)\nprint(model.score(df1,y_train))\n","5995deb6":"\nmodel=cbtc()\nmodel.fit(df1,y_train)\nprint(model.score(df1,y_train))\ny_pred=model.predict(df2)\ny_pred=y_pred.reshape(y_pred.shape[0])\ny_pred=y_pred.tolist()\nsubs=pd.DataFrame({'customer_id':test_df['customer_id'][:10],'churn_risk_score':y_pred[:10]})\nsubs.to_csv('submission_catboost.csv',index=False)","faf93c28":"model=rfc(random_state=0,n_jobs=2,n_estimators=700,verbose=2)\nmodel.fit(df1,y_train)\nprint(model.score(df1,y_train))\ny_pred=model.predict(df2)\nsubs=pd.DataFrame({'customer_id':test_df['customer_id'][:10],'churn_risk_score':y_pred[:10]})\nsubs.to_csv('submission_rfc.csv',index=False)","3ac845f9":"Now we have used catboost over the train and test data.Lets see what accuracy we cam achieve.","e28ac07d":"#### P.S. we can change the feedback column too.Which I have missed previously.Following kernels will be using NLP to encode those features.","e860d586":"It is visible that there are 6 target values and we have to classify them.\n\nIt is the final **objective**.","d4f70f29":"Basically what we did here is lemmatize every sentences and then encoded them.","38c20115":"#### Filling leakages :\n\nAs the data have lekagaes we have to fill those.\nand in this process we have taken two techniques. --\n\n|Data Types|Will be filled with|\n|---|---|\n|Categorical data|a string named 'None'|\n|Numerical Data|Mean of the present values|\n","dba01917":"After checking for hours we have taken selected features in our dataset , so that it may predict the best solution.","5899b861":"# Customer Churn Prediction :\n\nIn this project we'll see how to perform a data preprocessing and data prediction in intermediate level.\nSo, in this dataset we have train on the dataset which has multiple numerical and categorical feaatures and predict over the data.\n\n\n![](https:\/\/miro.medium.com\/max\/750\/1*8_Md5Ns2OKeW9F8XRRCMKg.jpeg)\n\n\n## UPVOTE if you like this notebook :)\nYou can see my other works in [sagnik1511](https:\/\/kaggle.com\/sagnik1511\/notebooks) or in [github](github.com\/sagnik1511)","21965267":"# Change the time series datas :\n\nIn this part we are going to numerate and create seperate columns for each value.","938a3568":"#### Dropping Unnecessary Fearures:\n\nwe have seen that there are some name and id features, when we are going to predict over the data , we can definitely tell these features only helps to label the dta , but it won't be helpful when it is time for prediction so we are going to drop those features.","67e5052a":"Now we are going to check the number of output\/target values has.","7619c4a7":"As the type of the data isn't datettime series , we have to do it manually.","60153928":"## Data Gathering & Primary Visualization:\n\nAt first we have to read the data .\nThen we'll overview on the data and prepare a process how to edit the data.","e4ee906e":"#### Feature Encoding :\n\nNow we have check if any categorical feature has more than 20 unique values , then we will omit that cause too much variety in dtaa will simply make the dataset more complex to predict correctly.","cc7d0c32":"# Data Preprocessing :\n\nAfter visualizing the data we have planned some moves to process the data. \n\nIn this process we have seen that both the train and test data leakage.\n\nSo , basically we are going to take the following moves to prepare trainable and predictable data.","e630bcca":"## Train-Test splitting :","f95b1117":"Now we are going to do this on RandomForestClassifier .Lets see how much accuracy we can achieve.","9307203a":"## Libraries :","a51a5079":"HURRAH !!!!!\n\nWe've completed the whole project.\n\nIf you like this do not forget to upvote .\n\nand if you have any query or feedback , do comment.\n\n# Thank You for visiting :)","c4d71a86":"# Data Prediction :\n\nNow we are goinf to predict the dataset using two type of classifiers.\n\n\n1. RandomForestClassifier    ( from sklearn.ensemble )\n2. CatBoostClassifier        ( from catboost )"}}