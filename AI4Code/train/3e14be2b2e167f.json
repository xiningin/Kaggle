{"cell_type":{"530e7c73":"code","05e869af":"code","aa58a9fa":"code","edfafdea":"code","43c6bfdb":"code","e6800c8d":"code","1c806fb7":"code","aeda7662":"code","28465b71":"code","11c09c0d":"code","24dd1fcc":"code","3545c72e":"code","e65af22c":"code","3e4c6a88":"code","ba1b49b6":"code","1deff997":"code","7ebf1780":"code","0739ea29":"code","992ee681":"code","ee06334f":"code","eac73af6":"code","15b38940":"code","b9cedd09":"code","f0c0e87a":"code","040987ea":"code","8334e5e0":"markdown","eea227ca":"markdown","2324f5b0":"markdown","aee30677":"markdown","a15c3d35":"markdown","3474faf0":"markdown","aea753e9":"markdown","b7b288ea":"markdown","6a626e96":"markdown","d5ca253e":"markdown","e9afa65d":"markdown","b0e8aaac":"markdown","b0b9ab4a":"markdown","4f5a9d38":"markdown","2458112e":"markdown","0473f71e":"markdown","7f8d9897":"markdown","011a084a":"markdown","2da4c1fb":"markdown","8a6592e8":"markdown","43bd5d02":"markdown","6b7b2cd0":"markdown","dcc8beb1":"markdown","c70103d8":"markdown","67cf3142":"markdown","ee52796d":"markdown"},"source":{"530e7c73":"import math\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#warnings supression\nimport warnings\nwarnings.filterwarnings('ignore')","05e869af":"# import data\ndata = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndata. head()\n","aa58a9fa":"data.info()\n","edfafdea":"data.isnull().sum()","43c6bfdb":"print(\"The data shape: \", data.shape)","e6800c8d":"cat_col = data.select_dtypes(include=['object'])\nvalues = cat_col.nunique()\nprint(values)","1c806fb7":"for column in cat_col:\n    print('Feature: ', column)\n    print('Feature Values: ', data[column].unique())","aeda7662":"data = data.replace('Existing Customer', 1)\ndata = data.replace('Attrited Customer', 0)","28465b71":"data = data.replace('M', 1)\ndata = data.replace('F', 0)\ndata","11c09c0d":"# Change 'College'=14 'Doctorate'=21 'Graduate'=16 'High School'=12 'Post-Graduate'=18 'Uneducated'=8 'Unknown'= Mode (most common)\ndata.loc[data['Education_Level'] == 'College',       'Education_Level'] = 15\ndata.loc[data['Education_Level'] == 'Doctorate',     'Education_Level'] = 20\ndata.loc[data['Education_Level'] == 'Graduate',      'Education_Level'] = 16\ndata.loc[data['Education_Level'] == 'High School',   'Education_Level'] = 12\ndata.loc[data['Education_Level'] == 'Post-Graduate', 'Education_Level'] = 18\ndata.loc[data['Education_Level'] == 'Uneducated',    'Education_Level'] = 8\ndata.loc[data['Education_Level'] == 'Unknown',       'Education_Level'] = data['Education_Level'].mode()[0]\ndata['Education_Level'] = data['Education_Level'].astype(int)","24dd1fcc":"data = data.replace('Blue', 1)\ndata = data.replace('Silver', 2)\ndata = data.replace('Gold', 3)\ndata = data.replace('Platinum', 4)\ndata","3545c72e":"# Feature:  Income_Category\n# Feature Values: \n # 'Less than $40K' = 20\n # '$60K - $80K' =  70 \n # '$40K - $60K' = 50\n # '$80K - $120K' = 100\n # '$120K +' = 140\ndata = data.replace('Less than $40K', 20)\ndata = data.replace('$40K - $60K', 50)\ndata = data.replace('$60K - $80K', 70)\ndata = data.replace('$80K - $120K', 100)\ndata = data.replace('$120K +', 150)\ndata.loc[data['Income_Category'] == 'Unknown', 'Income_Category'] = data['Income_Category'].mode()[0]\ndata['Income_Category'] = data['Income_Category'].astype(int)","e65af22c":"data = pd.get_dummies(data, columns=['Marital_Status'], drop_first = True)\ndata","3e4c6a88":"A = 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'\nB = 'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'\nC = 'CLIENTNUM'\ndata = data.drop([A, B, C], axis=1)","ba1b49b6":"corrmat = data.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)\nplt.show()","1deff997":"# split into X and y\ny = data['Attrition_Flag']\nX = data.drop(columns='Attrition_Flag')","7ebf1780":"print(\"the percent of 1s in the label: {} %\".format(y.mean()*100))","0739ea29":"from imblearn.over_sampling import SMOTE\noversample = SMOTE(random_state=69)\nX_s, y_s = oversample.fit_resample(X, y)\nX_s = X_s.to_numpy()\ny_s = y_s.to_numpy()\nprint(\"the percent of 1s in the label after using SMOTE: {} %\".format(y_s.mean()*100))","992ee681":"for col in range(X.shape[1]):\n    min = X_s[:, col].min()\n    max = X_s[:, col].max()\n    X_s[:, col] = (X_s[:, col] - min) \/ (max - min)","ee06334f":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split (X_s, y_s, train_size=0.75, random_state=69)\nprint(\"The training data size : {} \".format(X_train.shape))\nprint(\"The test data size : {} \".format(X_test.shape))","eac73af6":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=None).fit(x_train, y_train)\ny_prediction = dt.predict(x_test)\n\ndt_training_score = dt.score(X_test, y_test)\ndt_test_score = dt.score(X_train, y_train)\n\nprint ('Max Depth:', dt.get_depth())\n\nprint('Decision Tree Accuracy: ')\nprint('Train {}'.format(np.around(dt_training_score * 100, decimals=2)))\nprint('Test {}'.format(np.around(dt_test_score * 100, decimals=2)))\n\n","15b38940":"max_depth = dt.get_depth()\nacc_train_list = []\nacc_test_list = []\n\nfor i in range(1, max_depth + 1):\n    dt = DecisionTreeClassifier(max_depth = i).fit(x_train, y_train)\n    y_prediction = dt.predict(x_test)\n    \n    dt_training_score = 100 * dt.score(X_test, y_test)\n    dt_test_score = 100 * dt.score(X_train, y_train)\n    acc_train_list.append(dt_training_score)\n    acc_test_list.append(dt_test_score)\n    \n\nplt.plot (acc_train_list)\nplt.plot(acc_test_list)\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()","b9cedd09":"from sklearn.ensemble import RandomForestClassifier\n\n# First we will check the number of estimator is the best to use\n\nestimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\nacc_train_list = []\nacc_test_list = []\n\nfor i in estimators:\n    rnf = RandomForestClassifier(n_estimators=i).fit(x_train, y_train)\n    y_prediction = rnf.predict(x_test)\n    \n    train_accuracy = rnf.score(x_train, y_train)\n    test_accuracy = rnf.score(x_test, y_test)\n    acc_train_list.append(train_accuracy)\n    acc_test_list.append(test_accuracy)\n\nplt.plot (estimators, acc_train_list)\nplt.plot(estimators, acc_test_list)\nplt.xlabel(\"number of estimators\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()\n","f0c0e87a":"from sklearn.ensemble import RandomForestClassifier\n\n# Now we will check the number of estimator is the best to use\n\nacc_train_list = []\nacc_test_list = []\n\nfor i in range(1, 33):\n    rnf = RandomForestClassifier(max_depth=i).fit(x_train, y_train)\n    y_prediction = rnf.predict(x_test)\n    \n    train_accuracy = rnf.score(x_train, y_train)\n    test_accuracy = rnf.score(x_test, y_test)\n    acc_train_list.append(train_accuracy)\n    acc_test_list.append(test_accuracy)\n\nx = np.linspace(1, 32, 32)\n\nplt.plot (x, acc_train_list)\nplt.plot(x, acc_test_list)\nplt.xlabel(\"Max Depth\")\nplt.ylabel(\"Accuracy Score\")\nplt.show()","040987ea":"# checking the accuracy with the best parameters\nrnf = RandomForestClassifier(n_estimators=30, max_depth=7).fit(x_train, y_train)\ny_prediction = rnf.predict(x_test)\n    \ntrain_accuracy = rnf.score(x_train, y_train)\ntest_accuracy = rnf.score(x_test, y_test)\n\nprint('Random Forest Accuracy: ')\nprint('Train {}'.format(np.around(train_accuracy * 100, decimals=2)))\nprint('Test {}'.format(np.around(test_accuracy * 100, decimals=2)))\n","8334e5e0":"We got 100% accuracy at test - seems that we have overfiting","eea227ca":"* Feature:  Card_Category - depends on its importance\n* Feature Values:  Blue = 1 Silver = 2 Gold = 3 Platinum = 4","2324f5b0":"the label is the attrition flag which has 2 categorise: we will change it to 0, 1.","aee30677":"We can see that around max depth = 5 the train-test starting to split.","a15c3d35":"* **Split into train and test**","3474faf0":"* Martial status has no clear order therefor I use Hotkey encoding.","aea753e9":"Lets plot the accuracy score to find out","b7b288ea":"* We can see that we have some correlated features such as 'Avg_Open_To_Buy' with 'Credit_limit'. \n* additionally, the martial categories are anti correlated to each other.","6a626e96":"We can see that the data is imbalanced - we will use SMOTE to balance.","d5ca253e":"We can see that most of the columns are numerical but we have some categorical.","e9afa65d":"# **Preprocessing**","b0e8aaac":"# Exploring the dataset\n1. figure out the columns and their type.\n2. verify if there is any missing data.\n3. to know the dataset dim","b0b9ab4a":"# **Dealing with categorical columns**","4f5a9d38":"The 2 last Naive bayes's columns looks to be a predictors therefore we will drop them and the first column (clientnum) which doesn't give us any information.","2458112e":"The gender has 2 unique values as well","0473f71e":"* **Lets check if the data is balanced**","7f8d9897":"* **Next - normalize data**","011a084a":"Lets have a better look of the categorical columns:","2da4c1fb":"# **Modeling**","8a6592e8":"* # **Decision Tree model**","43bd5d02":"**the entire categorical columns has a logic order (except marital status) that we can give them a numerical order**","6b7b2cd0":"Income_Category: because we have a range for every value we will take a representative value for each one","dcc8beb1":"We can see the accuracy score getting stable from max depth ~7","c70103d8":"We can see that from ~30 estimators the accuracy is getting stable","67cf3142":"Education level has 7 unique values - I will modify every education level with its number of years to be educated","ee52796d":"* # **Random Forest**"}}