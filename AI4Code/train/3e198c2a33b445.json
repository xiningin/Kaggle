{"cell_type":{"e6ee4601":"code","e06285e3":"code","a8b2f3d7":"code","9442ee8c":"code","997c63fa":"code","552f53ff":"code","d23faeae":"code","0175833d":"code","1e740e36":"code","29c92426":"code","aa8029b3":"code","888225e0":"code","deb7fb62":"code","d2717bd0":"code","dc91ba86":"code","b3388c03":"code","27f4298d":"code","72256f5f":"code","b88631bc":"code","a151c227":"code","9c895691":"code","17dc7eb6":"code","bc9dd9d1":"code","f3bed172":"code","1295524d":"code","5d96f428":"code","4b3d97a7":"code","2d3c76d3":"code","789b2aac":"code","2891d50b":"code","d53e710d":"code","76ee0e2f":"code","b4221f14":"code","85ca05d7":"code","fb26d56a":"code","483bba86":"code","4967e2c2":"code","322d9021":"code","a59abcb0":"code","4abebc00":"markdown","e2df69ac":"markdown","8de8faf6":"markdown","a1d2a47a":"markdown","a722b89e":"markdown","caad786d":"markdown","b84f7bbd":"markdown"},"source":{"e6ee4601":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns","e06285e3":"df = pd.read_csv('..\/input\/autompg\/auto-mpg.csv')\ndf.head().style.background_gradient(axis=0)","a8b2f3d7":"# As we see horseboower is object , so we have to convert it to numeric : \ndf.info()","9442ee8c":"df['horsepower'] = pd.to_numeric(df['horsepower'],errors='coerce')","997c63fa":"df.info()","552f53ff":"df.isnull().sum()","d23faeae":"# As we see there is no NAN values : \nimport missingno as msgo\nmsgo.matrix(df)","0175833d":"df = df.drop('car name',axis =1 )\ndf","1e740e36":"df = df.dropna()\ndf","29c92426":"X = df.drop('mpg',axis=1)\ny = df['mpg']","aa8029b3":"sns.pairplot(data = df )","888225e0":"from sklearn.model_selection import train_test_split","deb7fb62":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","d2717bd0":"from sklearn.linear_model import LinearRegression\nlinear_model = LinearRegression()","dc91ba86":"linear_model.fit(X_train,y_train)","b3388c03":"test_predections = linear_model.predict(X_test)\ntest_predections","27f4298d":"# As we see errors are randomly distributed , so we can use linear_regression : \nresidual = y_test - test_predections\nsns.scatterplot(data = df  , x = y_test , y =residual)\nplt.axhline(y=0 , color = 'r')","72256f5f":"from sklearn.metrics import mean_absolute_error , mean_squared_error","b88631bc":"MAE = mean_absolute_error(y_test,test_predections)\nMAE","a151c227":"MSE = mean_squared_error(y_test,test_predections)\nMSE","9c895691":"RMSE = np.sqrt(MSE)\nRMSE","17dc7eb6":"final_model = LinearRegression()\nfinal_model.fit(X,y)\ny_hat = final_model.predict(X)\ndf['linear_model_predection'] = y_hat\ndf","bc9dd9d1":"from sklearn.preprocessing import PolynomialFeatures","f3bed172":"poly_conv = PolynomialFeatures(degree = 3 , include_bias= False)\npoly_feature = poly_conv.fit_transform(X)","1295524d":"X_train, X_test, y_train, y_test = train_test_split(poly_feature, y, test_size=0.3, random_state=101)","5d96f428":"model = LinearRegression()\nmodel.fit(X_train,y_train)\npoly_test_predection = model.predict(X_test)\nMAE_test = mean_absolute_error(y_test,poly_test_predection)\nprint(MAE_test)\nMSE_test = mean_squared_error(y_test,poly_test_predection)\nprint(MSE_test)","4b3d97a7":"train_rmse_error = []\ntest_rmse_error = []\n\nfor d in range (1,10):\n    polynomial_converter = PolynomialFeatures(degree = d , include_bias= False )\n    poly_features = polynomial_converter.fit_transform(X)\n    \n    # Split poly_set :\n    X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.3, random_state=101)\n    \n    # train on new poly_set : \n    model = LinearRegression(fit_intercept= True)\n    model.fit(X_train,y_train)\n    \n    # Predict on train and test : \n    train_pred = model.predict(X_train)\n    test_pred = model.predict(X_test)\n    \n    # errors : \n    \n    train_rmse = np.sqrt(mean_squared_error(y_train,train_pred)) \n    test_rmse = np.sqrt(mean_squared_error(y_test,test_pred))\n    \n    # Append error : \n    \n    train_rmse_error.append(train_rmse)\n    test_rmse_error.append(test_rmse)","2d3c76d3":"plt.figure(figsize=(6,4),dpi = 150)\nplt.plot(range(1,6),train_rmse_error[:5],label='TRAIN')\nplt.plot(range(1,6),test_rmse_error[:5],label='TEST')\nplt.xlabel(\"Polynomial Complexity\")\nplt.ylabel(\"RMSE\")\nplt.legend()","789b2aac":"poly_conv = PolynomialFeatures(degree = 2 , include_bias= False)\npoly_feature = poly_conv.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(poly_feature, y, test_size=0.3, random_state=101)","2891d50b":"model = LinearRegression()\nmodel.fit(X_train,y_train)\npoly_test_predection = model.predict(X_test)\nMAE_poly = mean_absolute_error(y_test,poly_test_predection)\nprint(MAE_poly)\nMSE_poly = mean_squared_error(y_test,poly_test_predection)\nprint(MSE_poly)\nRMSE_poly = np.sqrt(MSE)\nprint(RMSE_poly)","d53e710d":"print(MAE)\nprint(MSE)\nprint(RMSE)","76ee0e2f":"final_poly_converter = PolynomialFeatures(degree = 2 , include_bias= False )","b4221f14":"final_model = LinearRegression()","85ca05d7":"final_model.fit(final_poly_converter.fit_transform(X),y)","fb26d56a":"from joblib import dump , load \ndump(final_model,'final_poly_model.joblib')\ndump(final_poly_converter,'poly_converter.joblib')","483bba86":"loaded_poly = load('poly_converter.joblib')\nloadeed_model = load('final_poly_model.joblib')","4967e2c2":"campaign = X","322d9021":"campaign_poly = loaded_poly.transform(campaign)","a59abcb0":"df['final_model_predection'] = loadeed_model.predict(campaign_poly)\ndf","4abebc00":"# Now we are going to get best MAE MSE but based on best_poly_degree : ","e2df69ac":"# From the below we can get the best polynomial_degree (2):\n# This is the degree of best low bias and low variance : ","8de8faf6":"# # Get MAE MSE and RMSE with Poly_features :\n# Now we are going to use Polynomial_regression to try to get lower eroors metrics : ","a1d2a47a":"# first we will use simple **Linear Regression** and then we will use **Polynomial Regression** to be able to get best machine learning** model_result** :","a722b89e":"# ERROR METRICS : ","caad786d":"\n# Errors are higher than simple linear_model , so lets do more efforts to get lower_error : \n","b84f7bbd":"# Wooow we did it D: , the below Errors are lower than last_models : "}}