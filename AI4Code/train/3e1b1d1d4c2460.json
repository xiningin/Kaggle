{"cell_type":{"36b3d833":"code","b3a00156":"code","bd4428af":"code","3a212375":"code","f198b906":"code","cd19d8d5":"code","c4b63550":"code","8111ad8f":"code","5224f1ce":"code","67054f4e":"code","9d34c9bb":"code","9a5878f6":"code","02bd6a3e":"code","a64e93e1":"code","e458eb9d":"code","402760c6":"code","2d60f9a3":"code","6aa3242f":"code","f32ed109":"code","de0046b4":"code","25eaae6e":"code","186e07fd":"code","a237f54d":"code","fe8879b2":"code","9a55e4aa":"code","b38fdbd3":"code","db34e03e":"code","1ba23684":"markdown","214b4844":"markdown","b9f7b07e":"markdown","ef019464":"markdown","7ff6aca9":"markdown","1936bf31":"markdown","bc56f428":"markdown","a69e322b":"markdown","7752f742":"markdown","456025bd":"markdown","90810001":"markdown","23aed48d":"markdown","c03e2e11":"markdown","e3c391a5":"markdown","d9b3974a":"markdown","05d582a1":"markdown","6f1eaa45":"markdown","7c7c7ede":"markdown","3e59fb6c":"markdown"},"source":{"36b3d833":"!pip install -q sentence-transformers\n!pip install -q tqdm\n!pip install -q sqlitedict\n!pip install -q transformers\n!pip install -q gensim==3.8.2","b3a00156":"import numpy as np\nimport pandas as pd\nimport glob\nimport os\nimport json\nimport shelve\nimport nltk\nimport pickle\nfrom tqdm.notebook import tqdm\nfrom google.cloud import storage\nfrom sentence_transformers import SentenceTransformer\nfrom io import BytesIO\nfrom gensim.summarization.bm25 import BM25\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.preprocessing import minmax_scale\nfrom sqlitedict import SqliteDict\nimport torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer\nimport logging\nfrom scipy.spatial.distance import cosine\nfrom IPython.core.display import display, HTML\n\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)\nnltk.download('punkt')\nnltk.download('stopwords')","bd4428af":"class GcsBucket:\n\n    def __init__(self, bucket):\n        self.bucket = bucket\n        \n        \n    def upload_file(self, path, key):\n        blob = self.bucket.blob(key)\n        return blob.upload_from_filename(path)\n\n\n    def get_file(self, key):\n        blob = storage.blob.Blob(key, self.bucket)\n        content = blob.download_as_string()\n        return BytesIO(content)\n    \n    \n    def download_file(self, key, path):\n        blob = storage.blob.Blob(key, self.bucket)\n        return blob.download_to_filename(path)\n\n    \n    def file_exists(self, key):    \n        blob = self.bucket.blob(key)\n        return blob.exists()\n    \n    \n    def remove_file(self, key):\n        blob = self.bucket.blob(path)\n        return blob.delete()\n\n\ngcs_client = storage.Client(project='nomadic-zoo-255800')\nbucket = GcsBucket(\n    bucket=gcs_client.get_bucket(\"yukku\")\n)","3a212375":"QUESTION_TOPICS = \"COVID-19, coronavirus, SARS-CoV-2, novel coronavirus\"\nMODEL_NAME = \"roberta-large-nli-stsb-mean-tokens\"\n\nDATASET_DIR = \"\/kaggle\/input\/CORD-19-research-challenge\"\nDATA_DIR = \"\/kaggle\/working\/data\"\n\nGCS_DATA_DIR = \"covid_19\"\n\nDATASET_PATH = os.path.join(DATA_DIR, \"CORD-19-research-challenge-2.csv\")\nGCS_DATASET_PATH = os.path.join(GCS_DATA_DIR, \"CORD-19-research-challenge-2.csv\")\n\nDATABASE_PATH = os.path.join(DATA_DIR, \"database-2.db\")\nGCS_DATABASE_PATH = os.path.join(GCS_DATA_DIR, \"database-2.db\")\n\nBM25_PATH = os.path.join(DATA_DIR, \"bm25-2.pickle\")\nGCS_BM25_PATH = os.path.join(GCS_DATA_DIR, \"bm25-2.pickle\")\n\n\n!mkdir -p \"{DATA_DIR}\"","f198b906":"def create_dataset(json_filenames):\n    \n    dataframe = pd.DataFrame([], columns=[\n        \"doc_id\", \n        \"title\", \n        \"abstract\", \n        \"text_body\"\n    ])\n    \n    for file_name in tqdm(json_filenames):\n        \n        with open(file_name) as json_data:\n            data = json.load(json_data)\n            row = {\n                \"doc_id\": None, \n                \"title\": None,\n                \"abstract\": None, \n                \"text_body\": None\n            }\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n            \n            if 'abstract' in data:\n                row['abstract'] = \"\\n \".join(\n                    [abst['text'] for abst in data['abstract']]\n                )\n  \n            row['text_body'] = \"\\n \".join(\n                [bt['text'] for bt in data['body_text']]\n            )\n                                    \n            dataframe = dataframe.append(\n                row, \n                ignore_index=True\n            )\n    \n    return dataframe\n\n\ndef remove_file(path):\n    \n    if os.path.exists(path):\n        os.remove(path)\n        \n        \ndef save_dataset(dataframe, path):\n    \n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    dataframe.to_csv(path, header=None, index=None)\n\n    \ndef load_dataset(reload=False):\n    if bucket.file_exists(GCS_DATASET_PATH) and not reload:\n        dataframe = pd.read_csv(\n            bucket.get_file(GCS_DATASET_PATH), \n            names=[\n                \"doc_id\", \n                \"title\", \n                \"abstract\", \n                \"text_body\"\n            ]\n        )\n    else:\n        dataframe = create_dataset(\n            glob.glob(\n                f\"{DATASET_DIR}\/**\/custom_license\/pdf_json\/*.json\", \n                recursive=True\n            )\n        )\n        save_dataset(dataframe, DATASET_PATH)\n        response = bucket.upload_file(\n            DATASET_PATH, \n            GCS_DATASET_PATH\n        )\n        remove_file(DATASET_PATH)    \n        \n    return dataframe\n\n\ndataset = load_dataset(reload=False)","cd19d8d5":"print(f\"total document of {len(dataset)}\")\ndataset.head()","c4b63550":"def create_corpus(dataset):\n    df = dataset[\"title\"].fillna('') \\\n        + dataset[\"abstract\"].fillna('') \\\n        + dataset[\"text_body\"].fillna('')\n    for index, text in tqdm(df.iteritems(), total=len(df)):\n        yield word_tokenize(text)\n\n\ndef load_bm25(save_path, gcs_path, reload=False):\n    if bucket.file_exists(gcs_path) and not reload:\n        bucket.download_file(gcs_path, save_path)\n        with open(save_path, 'rb') as file:\n            return pickle.load(file)\n    else:\n        \n        bm25 = BM25(create_corpus(dataset))\n        with open(save_path, 'wb') as file:\n            pickle.dump(bm25, file)\n\n        bucket.upload_file(save_path, gcs_path)\n        return bm25\n\n\nbm25 = load_bm25(\n    save_path=BM25_PATH, \n    gcs_path=GCS_BM25_PATH,\n    reload=False\n)","8111ad8f":"model = SentenceTransformer(MODEL_NAME)\ntensors = model.cuda()","5224f1ce":"def get_sentences(texts):\n    results = []\n    if not pd.isnull(texts): \n        results = nltk.tokenize.sent_tokenize(texts)\n    return results\n    \n\ndef iter_sentences(dataset):\n    for index, row in dataset.iterrows():\n        doc_id = row[0]\n        title = row[1]\n        abstract = row[2]\n        \n        for index, sentence in enumerate(get_sentences(title)):\n            yield (f\"{doc_id}-title-{index}\", sentence)\n\n        for index, sentence in enumerate(get_sentences(abstract)):\n            yield (f\"{doc_id}-abstract-{index}\", sentence)\n            \n\ndef iter_embeddings(model, dataset):\n    batch_length = 50000\n    \n    doc_ids = []\n    sentences = []\n    count = 0\n    for index, (doc_id, sentence) in enumerate(iter_sentences(dataset)): \n        doc_ids.append(doc_id)\n        sentences.append(sentence)\n            \n        if len(doc_ids) >= batch_length or index == len(dataset) - 1:\n            print(f\"processing {count * batch_length} to {(count + 1) * batch_length}\")\n\n            embeddings = model.encode(sentences)  \n            embedding_ids = doc_ids\n            doc_ids = []\n            sentences = []\n            count += 1\n            for doc_id, embedding in zip(embedding_ids, embeddings):\n                yield (doc_id, embedding)\n           \n                \ndef create_and_save_embeddings(model, dataset, database):\n    for doc_id, embedding in iter_embeddings(model, dataset):\n        database[doc_id] = embedding\n\n    database.commit()\n \n\ndef load_database(model, dataset, save_path, gcs_path, reload=False):\n    if bucket.file_exists(gcs_path) and not reload:\n        bucket.download_file(gcs_path, save_path)\n    else:\n        database = SqliteDict(save_path)\n        create_and_save_embeddings(model, dataset, database)\n        database.close()\n        bucket.upload_file(save_path, gcs_path)\n\n    return SqliteDict(save_path)\n        \n\ndatabase = load_database(\n    model=model, \n    dataset=dataset, \n    save_path=DATABASE_PATH, \n    gcs_path=GCS_DATABASE_PATH\n)","67054f4e":"def build_get_relevant_documents(model, dataset, database):\n    \n    def get_relevant_documents(text, max_search):\n        target_embedding = model.encode([text])[0]\n\n        semantic_similarity_scores = {}\n        for key, embedding in tqdm(database.iteritems(), total=len(database)):\n            similarity = 1 - cosine(target_embedding, embedding)\n            [doc_id, doc_type, doc_index] = key.split(\"-\")\n            \n            has_score = doc_id in semantic_similarity_scores\n            doc_higher_score = has_score and semantic_similarity_scores[doc_id] < similarity\n            if not has_score or doc_higher_score:\n                semantic_similarity_scores[doc_id] = similarity\n       \n        dataset.loc[:, \"score\"] = 0\n        dataset[\"score\"] = dataset[\"doc_id\"] \\\n            .map(semantic_similarity_scores)\n\n        bm25_scores = minmax_scale(bm25.get_scores(word_tokenize(text)), feature_range=(0,1))\n        dataset[\"score\"] = (dataset[\"score\"] + bm25_scores)\/2\n\n        return dataset.sort_values(\"score\", ascending=False).head(n=max_search)\n        \n    return get_relevant_documents\n\n\nget_relevant_documents = build_get_relevant_documents(\n    model=model,\n    dataset=dataset,\n    database=database\n)","9d34c9bb":"qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\nqa_tokeniser = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\ntensors = qa_model.to('cuda' if torch.cuda.is_available() else 'cpu')\ntensors_eval = qa_model.eval()","9a5878f6":"def create_features(question, document):\n    input_ids = qa_tokeniser.encode(question, document)\n    tokens = qa_tokeniser.convert_ids_to_tokens(input_ids)  \n    sep_index = input_ids.index(qa_tokeniser.sep_token_id)\n\n    seg_start_count = sep_index + 1\n    seg_end_count = len(input_ids) - seg_start_count\n    segment_ids = [0]*seg_start_count + [1]*seg_end_count\n\n    return input_ids, tokens, segment_ids\n\n\ndef get_paragraphs(question, document):\n\n    sentences = get_sentences(document)\n    paragraphs = []\n    \n    while len(sentences) > 0:\n        expected_length = len(\n            qa_tokeniser.encode(\n                question, \" \".join(paragraphs) + \" \" + sentences[0]\n            )\n        )\n        if expected_length < qa_tokeniser.max_len  and len(sentences) > 0:\n            paragraphs.append(sentences.pop(0)) \n        elif len(paragraphs) == 0 and len(sentences) > 0:\n            sentences.pop(0)\n        else:\n            out = paragraphs\n            paragraphs = [] \n            yield \" \".join(out)\n\n\ndef get_answer(question, document, min_confidence):\n    \n    torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    answers = []\n    answers_all = []\n    confidences = []\n\n    for paragraph in get_paragraphs(question, document):\n        \n        input_ids, tokens, segment_ids = create_features(question, paragraph)   \n\n        start_scores, end_scores = qa_model(\n            torch.tensor([input_ids]).to(torch_device), \n            token_type_ids=torch.tensor([segment_ids]).to(torch_device)\n        )\n\n        start_scores = start_scores[:,1:-1]\n        end_scores = end_scores[:,1:-1]\n\n        answer_start = torch.argmax(start_scores)\n        answer_end = torch.argmax(end_scores) + 1\n\n        answer = qa_tokeniser.decode(\n            input_ids[answer_start:][:answer_end], \n            skip_special_tokens=True, \n            clean_up_tokenization_spaces=True\n        )\n        answers.append(answer)\n\n        answer_all = qa_tokeniser.decode(\n            input_ids[tokens.index('[SEP]') + 1:][:-1], \n            skip_special_tokens=True, \n            clean_up_tokenization_spaces=True\n        )\n        answers_all.append(answer_all)\n\n        ## to do: find out issue of the following error\n        ## IndexError: index 478 is out of bounds for dimension 1 with size 478\n        try:\n            confidences.append(\n                start_scores[0,answer_start].item() + end_scores[0,answer_end].item()\n            )\n        except IndexError:\n            confidences.append(\n                start_scores[0,answer_start].item() + end_scores[0,answer_end - 1].item()\n            )\n        \n\n\n    if len(answers) == 0:\n        return None\n        \n    confidence = max(confidences)\n    final_answer = answers[confidences.index(confidence)]\n    answer_all = answers_all[confidences.index(confidence)]\n\n    if len(final_answer) > 0 and confidence > min_confidence:\n        return {\n            \"confidence\": confidence, \n            \"text\": final_answer, \n            \"paragraph\": answer_all\n        }\n    else:\n        return None\n\n\n\ndef get_answers(question_topics, question_context, question, max_search, min_confidence):\n\n    relevant_docs = get_relevant_documents(\n        f\"{question_topics} {question_context} {question}\",\n        max_search\n    )\n\n    for index, row in relevant_docs.iterrows():\n        document = row[\"text_body\"] or row[\"abstract\"]\n        if not pd.isna(document):\n            answer = get_answer(question, document, min_confidence)\n            if answer:\n                yield row[\"doc_id\"], row[\"title\"], answer[\"text\"], answer[\"paragraph\"], answer[\"confidence\"]\n    ","02bd6a3e":"def create_highlighted(text, text_all):\n    split_text = text_all.split(text)\n    beginning_text = \"\"\n    end_text = \"\"\n\n    if len(split_text) > 0:\n        beginning_text = split_text[0]\n    \n    if len(split_text) > 1:\n        end_text = split_text[1]\n    \n    return f\"<div>\" \\\n        f\"<font color='#bbbbbb'>{beginning_text}<\/font>\" \\\n        f\"{text}\" \\\n        f\"<font color='#bbbbbb'>{end_text}<\/font>\" \\\n        f\"<\/div>\"\n\n    \ndef display_answers(\n        question_context, \n        question,\n        question_topics=QUESTION_TOPICS, \n        max_search=50, \n        min_confidence=0\n    ):\n    \n    data = []\n    \n    answers = list(get_answers(question_topics, question_context, question, max_search, min_confidence))\n    answers = sorted(answers, key=lambda x: x[4], reverse=True)[0:10]\n    \n    if len(answers) == 0:\n        display(\n            HTML(\n                f\"<h4>Confident enough results not found<\/h4>\"\n            )\n        )\n        return None\n\n    for doc_id, title, text, paragraph, confidence in answers:\n        data.append([title, confidence, create_highlighted(text, paragraph)])\n\n    dataframe = pd.DataFrame(\n        data, \n        columns = [\"title\", \"confidence\", \"answer\"]\n    )\n\n    display(\n        HTML(\n            f\"<br\/><h2>{question_context}<\/h2><br\/>\" \\\n            f\"<h3> - {question}<\/h3>\"\n        )\n    )\n\n    display(\n        HTML(\n            dataframe.to_html(\n                render_links=True, \n                escape=False\n            )\n        )\n    )","a64e93e1":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Range of incubation periods for the disease in humans\",\n    question_topics=f\"{QUESTION_TOPICS}, how this varies across age and health status and how long individuals are contagious, even after recovery\",\n    max_search=50,\n    min_confidence=0\n)","e458eb9d":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Prevalence of asymptomatic shedding and transmission\",\n    question_topics=f\"{QUESTION_TOPICS}, children\",\n    max_search=50,\n    min_confidence=0\n)","402760c6":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"What do we know about Seasonality of transmission\",\n    question_topics=f\"{QUESTION_TOPICS}\",\n    max_search=50,\n    min_confidence=0\n)","2d60f9a3":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Physical science of the coronavirus\",\n    question_topics=f\"{QUESTION_TOPICS}, charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding\",\n    max_search=50,\n    min_confidence=0\n)","6aa3242f":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Persistence and stability on a multitude of substrates and sources\",\n    question_topics=f\"{QUESTION_TOPICS}, nasal discharge, sputum, urine, fecal matter, blood\",\n    max_search=50,\n    min_confidence=0\n)","f32ed109":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Persistence of virus on surfaces of different materials\",\n    question_topics=f\"{QUESTION_TOPICS}, copper, stainless steel, plastic\",\n    max_search=50,\n    min_confidence=0\n)","de0046b4":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Natural history of the virus and shedding of it from an infected person\",\n    question_topics=f\"{QUESTION_TOPICS}\",\n    max_search=50,\n    min_confidence=0\n)","25eaae6e":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Implementation of diagnostics and products to improve clinical processes\",\n    question_topics=f\"{QUESTION_TOPICS}\",\n    max_search=50,\n    min_confidence=0\n)","186e07fd":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"What do we know about disease models, including animal models\",\n    question_topics=f\"{QUESTION_TOPICS}, infection, disease and transmission\",\n    max_search=50,\n    min_confidence=0\n)","a237f54d":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"What do we know about Tools and studies to monitor phenotypic change\",\n    question_topics=f\"{QUESTION_TOPICS}, potential adaptation of the virus\",\n    max_search=50,\n    min_confidence=0\n)\n","fe8879b2":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"What do we know about Immune response and immunity\",\n    question_topics=f\"{QUESTION_TOPICS}\",\n    max_search=50,\n    min_confidence=0\n)\n","9a55e4aa":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Effectiveness of movement control strategies to prevent secondary transmission\",\n    question_topics=f\"{QUESTION_TOPICS}, health care and community settings\",\n    max_search=50,\n    min_confidence=0\n)\n","b38fdbd3":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\",\n    question_topics=f\"{QUESTION_TOPICS}\",\n    max_search=50,\n    min_confidence=0\n)\n","db34e03e":"display_answers(\n    question_context=\"What is known about transmission, incubation, and environmental stability?\",\n    question=\"What do we know about Role of the environment in transmission\",\n    question_topics=f\"{QUESTION_TOPICS}\",\n    max_search=50,\n    min_confidence=0\n)","1ba23684":"### GCS Client","214b4844":"### Install packages","b9f7b07e":"### Instanticate Sentence Transformer","ef019464":"# COVID-19 Open Research Dataset - Text and Data Mining Tools\n\n- Text and data mining tools that can help the medical community develop answers to high priority scientific questions\n- Text mining tools to provide insights on these questions\n- Find answers to questions within\n- Connect insights across","7ff6aca9":"### Imports","1936bf31":"## Lastly \n\nThere were few other variations of these approaches I wanted to try and test different parameters.  \n\nThe results seems quite convincing however virology isn't my area of expertise so would be interesting to find out if the results are any useful to those reseacher who are in the front line of the fileds.\n\nAlso thanks to the challenge and participators. It was inspirational to me to see and learn different approaches and techniques. ","bc56f428":"### Load embedding database","a69e322b":"## Bag-of-words retrieval \n\nWe will also be using [BM25](https:\/\/en.wikipedia.org\/wiki\/Okapi_BM25) in order to search relevant articles by frequency of words used in the question.\n\nMy finding was, Semantic Similarity Search gives higher recall results however those ranked at the top might not be most relevant to topic of the question. So by combining the score from Semantic Similarity Search and B25 might give diverse results within the topic.","7752f742":"### BM25","456025bd":"# Results","90810001":"## Semantic Similarity Search using RoBERTa\n\nWe will be using [Sentence Transformers](https:\/\/github.com\/UKPLab\/sentence-transformers) in order to create sentence embeddings for semantic similarity search task. The library enable us to use fine-tuned BERT \/ RoBERTa \/ DistilBERT \/ ALBERT \/ XLNet models for semantic textual similarity tasks. \n\nWe then find distance from question to article by cosine distance to rank the articles.\n\n","23aed48d":"### Result display","c03e2e11":"### Load dataset","e3c391a5":"### Instanciate Q&A BERT model","d9b3974a":"### Constants","05d582a1":"### Scoring using cosine distance between embeddings and BM25","6f1eaa45":"## Conclusions\n\n\nGiven the limited RAM, Disk space available on Kaggle Kernal environment, I only manage to process dataset in `custom_license` folder. \n\nThe approach I took was for my initial exploration and reseach into these NLP methods and I'm pretty sure improvements can easily made and save lot's of time complexity and space complexity \ud83d\ude13. \n\nBERT Q&A model sometimes didn't find confident enough answers for some of the questions specially when the query sentense isn't question format and short in number of words. I have prefixed with phrase \"What do we know about\" for those question as a simple workaround.\n","7c7c7ede":"## Question Answering with BERT\n\n\nAfter ranking articles by these 2 methods, then we combine the scores by averaging them in order to find most relevant articles. \n\nWe then use [Transformers](https:\/\/huggingface.co\/transformers\/) BertForQuestionAnswering to find answers or relevant section in the article.\n","3e59fb6c":"## Intro\n\nFrom questions, I'm interested to find semantically similar words, phrases, context then find wider range of articles for researchers to be able explore the search results. Then using Q&A model to find relevant answer or section of the articles.\n\nI tried to focus on retriving wider range of semantically relevant articles rather than focuing on accuracy. When a question is challenging to answer, it may be useful to see the range of results.\n"}}