{"cell_type":{"7492e427":"code","479a7df0":"code","e37541d2":"code","28bb0d61":"code","8146ffc6":"code","935c7ef4":"code","e9d36c96":"code","406d25de":"code","7863739a":"code","929d3089":"code","0f04d811":"code","9e81064e":"code","3b912b9e":"code","8d06334e":"code","0be24c9b":"code","85af2e32":"code","5f0cc93b":"code","04842fcc":"code","601056cb":"code","1d09ec0c":"code","99bc35b1":"code","cea288d0":"code","6bb828b3":"code","926dbc35":"code","023504e4":"code","f080fda9":"code","20712b21":"code","533c3783":"code","02a93730":"code","b54e910e":"code","78cad7f2":"code","91bda1fe":"code","9065968a":"code","ea82a4ca":"code","ae271a77":"code","a5521959":"code","e33b8827":"code","1221004e":"code","bad0228f":"code","417c5cc5":"markdown","e3590119":"markdown","c3e6bfa5":"markdown","9e05d6ff":"markdown","e69808e1":"markdown","748121d5":"markdown"},"source":{"7492e427":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.stem import PorterStemmer #normalize word form\nfrom nltk.probability import FreqDist #frequency word count\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords #stop words\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet\nfrom nltk.probability import FreqDist \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport string\nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","479a7df0":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","e37541d2":"# remove hyperlink\n\ndef text_cleaning_hyperlink(text):\n    \n    return re.sub(r\"http\\S+\",\"\",text) ","28bb0d61":"# remove punctuation marks\n\ndef text_cleaning_punctuation(text):\n    \n    translator = str.maketrans('', '', string.punctuation) #remove punctuation\n    \n    return text.translate(translator)","8146ffc6":"# clean stopwords\n\ndef text_cleaning_stopwords(text):\n    \n    stop_words = set(stopwords.words('english'))\n    \n    word_token = word_tokenize(text)\n    \n    filtered_sentence = [w for w in word_token if not w in stop_words]\n    \n    return ' '.join(filtered_sentence) #return string of no stopwords\n","935c7ef4":"# convert all letters into lowercase ones\n\ndef text_cleaning_lowercase(text):\n    \n    return text.lower()","e9d36c96":"def text_extract(text_lst):\n    txt = []\n    for i,x in enumerate(text_lst):\n        \n        for j,p in enumerate(x):\n            \n            txt.append(p)\n    \n    return txt\n    ","406d25de":"# remove digits from the text\n\ndef remove_digits(txt):\n    \n    no_digits = ''.join(i for i in txt if not i.isdigit())\n    \n    return no_digits","7863739a":"# we clean the keywords for the TRAINING data\n\ntrain.keyword = train.keyword.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))) if type(x) == str else x)","929d3089":"# we clean the text for the TRAINING data\n\ntrain.text = train.text.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))))\ntrain.text = train.text.apply(lambda x: list(set(x.split(' '))))","0f04d811":"# # add keywords to the text of the TRAINING data\n# train['k_t'] = train.apply(lambda x : x['text'] + [x['keyword']] if type(x['keyword']) == str else x['text'],axis=1) #add keyword to text content","9e81064e":"# add keywords to the text of the TRAINING data\ntrain['k_t'] = train.apply(lambda x : x['text'] + [x['keyword']] if type(x['keyword']) == str else x['text'], axis=1) ","3b912b9e":"# lemmatize the text of the TRAINING set\n\nps_1 = PorterStemmer()\nwnl_1 = WordNetLemmatizer()\ntext_reconstruct = []\n\nfor i,x in enumerate(train.k_t.values):\n    \n    try:\n        \n        a = wnl_1.lemmatize(ps_1.stem(x))\n\n        \n    except AttributeError:\n        \n        a = list(set([wnl_1.lemmatize(ps_1.stem(word)) for j,word in enumerate(x)]))\n        \n    \n    text_reconstruct.append(a)","8d06334e":"# append the keywords to the text of the TRAINING set\n\ntrain.k_t = text_reconstruct\ntrain_word = train.k_t.apply(lambda x: ' '.join(x))","0be24c9b":"# clean keywords of the TEST data\n\ntest.keyword = test.keyword.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))) if type(x) == str else x)","85af2e32":"# clean the text of the TEST data\n\ntest.text = test.text.apply(lambda x: text_cleaning_stopwords(text_cleaning_punctuation(text_cleaning_hyperlink(remove_digits(x.lower())))))\ntest.text = test.text.apply(lambda x: list(set(x.split(' '))))","5f0cc93b":"# add (weighted) keywords to the text of the TEST data\n\ntest['k_t'] = test.apply(lambda x : x['text'] + [x['keyword']+x['keyword']+x['keyword']] if type(x['keyword']) == str else x['text'],axis=1) ","04842fcc":"# lemmatize the text of the TEST set\n\nps_2 = PorterStemmer()\nwnl_2 = WordNetLemmatizer()\ntext_reconstruct_test = []\n\nfor i,x in enumerate(test.k_t.values):\n    \n    try:\n        \n        a = wnl_2.lemmatize(ps_2.stem(x))\n\n        \n    except AttributeError:\n        \n        a = list(set([wnl_2.lemmatize(ps_2.stem(word)) for j,word in enumerate(x)]))\n        \n    \n    text_reconstruct_test.append(a)","601056cb":"# append the keywords to the text of the TEST set\n\ntest.k_t = text_reconstruct_test\ntest_word = test.k_t.apply(lambda x: ' '.join(x))","1d09ec0c":"import gensim\n\npath_for_word2vec = \"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\"\nword2vec = gensim.models.KeyedVectors.load_word2vec_format(path_for_word2vec, binary = True)","99bc35b1":"# prepare to map words to vectors\n\ndef average_w2v(list_of_tokens, vct, generate_missing = False, dimentions = 300):\n    if len(list_of_tokens) < 1:\n        return np.zeros(dimentions)\n    if generate_missing:\n        vector = [vct[item] if item in vct else np.random.rand(dimentions) for item in list_of_tokens]\n    else:\n        vector = [vct[item] if item in vct else np.zeros(dimentions) for item in list_of_tokens]\n    total_length = len(vector)\n    sum_of_vectors = np.sum(vector, axis=0)\n    average = np.divide(sum_of_vectors, total_length)\n    return average\n","cea288d0":"# map words to vectors (using the googlenewsvectorsnegative300 database)\n\ndef word2vec_mapping(vect, our_word, generate_missing = False):\n    mapping = our_word.apply(lambda x: average_w2v(x, vect, generate_missing = generate_missing))\n    return list(mapping)","6bb828b3":"from nltk.tokenize import RegexpTokenizer\nour_tokenizer = RegexpTokenizer(r'\\w+')\ntokenized_input_train = train_word.apply(our_tokenizer.tokenize)  # tokenize the TRAINING set\ntokenized_input_test = test_word.apply(our_tokenizer.tokenize)    # tokenize the TEST set","926dbc35":"mapped_train = word2vec_mapping(word2vec, train.text) # vectorize the TRAINING set\nmapped_test = word2vec_mapping(word2vec,test.text)    # vectorize the TEST set","023504e4":"from sklearn.svm import SVC\nclassifier_SVM = SVC(C = 2,probability = True)\nclassifier_SVM.fit(mapped_train, train.target)\ny_predicted_SVM = classifier_SVM.predict(mapped_test)","f080fda9":"from sklearn.linear_model import LogisticRegression\nclassifier_LogisticRegression = LogisticRegression(C=30.0, class_weight = 'balanced', solver = 'newton-cg', \n                                                   multi_class = 'multinomial', n_jobs = -1, random_state = 42)\nclassifier_LogisticRegression.fit(mapped_train, train.target)\ny_predicted_LogisticRegression = classifier_LogisticRegression.predict(mapped_test)","20712b21":"train = train.fillna('Zilch')\nimportant_key_words = train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'count', 'target':'frequency'})","533c3783":"additional_list = ['bushfires','evacuated','forestfire','hostages','rescuers','sinkhole','thunderstorm']","02a93730":"prob_disaster = 0.95\nkeyword_list_disaster95 = list(important_key_words[important_key_words['frequency']>prob_disaster].index) + additional_list","b54e910e":"numbers_of_95certain_disasters = test['id'][test.keyword.isin(keyword_list_disaster95)]","78cad7f2":"y_predicted = np.zeros(len(y_predicted_SVM))\n\nfor i in range(0,len(y_predicted_SVM)):\n    if i in numbers_of_95certain_disasters:\n        y_predicted[i] = 1\n    else:\n        y_predicted[i] = y_predicted_SVM[i]  \n","91bda1fe":"sample_submission[\"target\"] = [int(i) for i in y_predicted]","9065968a":"sample_submission.to_csv(\"submission.csv\", index = False)","ea82a4ca":"import itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion Matrix',\n                          cmap=plt.cm.spring):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=26)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=20)\n    plt.yticks(tick_marks, classes, fontsize=20)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n    \n    plt.tight_layout()\n    plt.ylabel('True labels', fontsize=24)\n    plt.xlabel('Predicted labels', fontsize=24)\n\n    return plt","ae271a77":"holy_grail = pd.read_csv(\"..\/input\/holygrail\/submission.csv\")\ny_test = holy_grail[\"target\"]","a5521959":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef scores(y_test, y_predicted):  \n    accuracy = accuracy_score(y_test, y_predicted)\n    f1 = f1_score(y_test, y_predicted, pos_label = None, average = 'weighted')\n    precision = precision_score(y_test, y_predicted, pos_label = None, average = 'weighted')             \n    recall = recall_score(y_test, y_predicted, pos_label = None, average = 'weighted')\n    print(\"accuracy = %.4f, f1 = %.4f, precision = %.4f, recall = %.4f\" % (accuracy, f1, precision, recall))\n    return accuracy, f1, precision, recall","e33b8827":"accuracy, f1, precision, recall = scores(y_test, y_predicted)","1221004e":"cm = confusion_matrix(y_test, y_predicted)\nfig = plt.figure(figsize=(8, 8))\nplot = plot_confusion_matrix(cm, classes=['Irrelevant','Disaster'], normalize=False, title='Confusion matrix\\n for word2vec with SVM')\nplt.show()","bad0228f":"from sklearn.metrics import roc_curve\nimport seaborn as sns\n\n# predict_proba gives the probabilities for the target (0 and 1 in our case) as a list (array). \n# The number of probabilities for each row is equal to the number of categories in target variable (2 in our case).\n\nprobabilities_SVM = classifier_SVM.predict_proba(mapped_test)\nprobabilities_LogisticRegression = classifier_LogisticRegression.predict_proba(mapped_test)\n\n# Using [:,1] gives us the probabilities of getting the output as 1\n\nprobability_of_ones_SVM = probabilities_SVM[:,1] \nprobability_of_ones_LogisticRegression = probabilities_LogisticRegression[:,1] \n\n\n# roc_curve returns:\n# - false positive rates (FPrates), i.e., the false positive rate of predictions with score >= thresholds[i]\n# - true positive rates (TPrates), i.e., the true positive rate of predictions with score >= thresholds[i]\n# - thresholds \n\nFPrates_SVM, TPrates_SVM, thresholds_SVM = roc_curve(y_test, probability_of_ones_SVM)\nFPrates_LogisticRegression, TPrates_LogisticRegression, thresholds_LogisticRegression = roc_curve(y_test, probability_of_ones_LogisticRegression)\n\n\n\n# plotting the ROC Curve to visualize all the methods\n\nsns.set_style('whitegrid')\nplt.figure(figsize = (10, 8))\n\nplt.plot(FPrates_SVM, TPrates_SVM, label = 'SVM')\nplt.plot(FPrates_LogisticRegression, TPrates_LogisticRegression, label = 'Logistic Regression')\n\n\nplt.plot([0, 1], [0, 1], color = 'blue', linestyle = '--')\nplt.axis([0, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontsize = 14)\nplt.ylabel('True Positive Rate', fontsize = 14)\nplt.title('ROC Curve', fontsize = 14)\nplt.legend(loc = \"lower right\")\nplt.show()","417c5cc5":"# PostPreditiction corrections that take into accout important keywords","e3590119":"# Then we preprocess the TEST data","c3e6bfa5":"# Here The SVM + Word2Vec Model is applied","9e05d6ff":"# Here we map the tokenized data to vectors (to capture some semantic similarities)","e69808e1":"# Now we preprocess the TRAINING data ","748121d5":"# The functions used for text preprosessing"}}