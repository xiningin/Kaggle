{"cell_type":{"0307b3d2":"code","4cb7ef4e":"code","eae4f2ba":"code","8d3db2e7":"code","cf610c26":"code","29c02d27":"code","3b4f0c36":"code","94e41143":"code","9d809e7b":"code","5533235b":"code","4009dffd":"code","3878e842":"code","e66ce71a":"code","8c8bc657":"code","2db967c4":"code","a55f890d":"code","ee506915":"code","4b931b46":"code","766f4d4f":"code","d0f85b41":"code","f510bda8":"code","0771325b":"code","2974af4a":"code","040cca55":"code","2566235f":"code","e2d9b4de":"code","50f96392":"code","7585b036":"code","58b12c13":"code","0ce9939e":"code","22c81a0f":"code","bea3b8d3":"code","5150cb2d":"code","ddad22fa":"code","4bc59b7c":"code","e29080b2":"code","79d59f84":"code","9669a113":"code","53e16b42":"markdown","f7daeef9":"markdown","02aeeef2":"markdown","d0923f10":"markdown","7c41286f":"markdown","99dccfe0":"markdown","a473153e":"markdown","48a1454f":"markdown","804efbec":"markdown","bae40853":"markdown","0edd2945":"markdown","c5755c00":"markdown","c4b2276a":"markdown","fdef0a8e":"markdown","9b510156":"markdown","553aadba":"markdown","36e606a2":"markdown","f96ae251":"markdown","c6c2cd0b":"markdown","04af3a31":"markdown","cc210418":"markdown","dd05c82c":"markdown","b5b36942":"markdown","c2304e13":"markdown","a841c99a":"markdown","c1551a96":"markdown","ea037317":"markdown","8c37456b":"markdown","ff29416e":"markdown","d0b32b61":"markdown","11a9dba5":"markdown","73b06d6d":"markdown"},"source":{"0307b3d2":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb","4cb7ef4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eae4f2ba":"%%time\ndatadir = Path('\/kaggle\/input\/google-quest-challenge')\n\n# Read in the data CSV files\ntrain = pd.read_csv(datadir\/'train.csv')\ntest = pd.read_csv(datadir\/'test.csv')\nsample_submission = pd.read_csv(datadir\/'sample_submission.csv')","8d3db2e7":"print('train', train.shape)\nprint('test', test.shape)\nprint('sample_submission', sample_submission.shape)","cf610c26":"sample_submission.head()","29c02d27":"sample_submission.columns","3b4f0c36":"feature_columns = [col for col in train.columns if col not in sample_submission.columns]\nprint('Feature columns: ', feature_columns)","94e41143":"train[feature_columns].head()","9d809e7b":"train0 = train.iloc[0]\n\nprint('URL           : ', train0['url'])\nprint('question_title: ', train0['question_title'])\nprint('question_body : ', train0['question_body'])","5533235b":"print('answer        : ', train0['answer'])","4009dffd":"train[['url', 'question_user_name', 'question_user_page', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host']]","3878e842":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","e66ce71a":"train[target_cols]","8c8bc657":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(target_cols):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","2db967c4":"train.isna().sum()","a55f890d":"test.isna().sum()","ee506915":"train_category = train['category'].value_counts()\ntest_category = test['category'].value_counts()","4b931b46":"fig, axes = plt.subplots(1, 2, figsize=(12, 6))\ntrain_category.plot(kind='bar', ax=axes[0])\naxes[0].set_title('Train')\ntest_category.plot(kind='bar', ax=axes[1])\naxes[1].set_title('Test')\nprint('Train\/Test category distribution')","766f4d4f":"from wordcloud import WordCloud\n\n\ndef plot_wordcloud(text, ax, title=None):\n    wordcloud = WordCloud(max_font_size=None, background_color='white',\n                          width=1200, height=1000).generate(text_cat)\n    ax.imshow(wordcloud)\n    if title is not None:\n        ax.set_title(title)\n    ax.axis(\"off\")","d0f85b41":"print('Training data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(train['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(train['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(train['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","f510bda8":"print('Test data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(test['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(test['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(test['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","0771325b":"fig, ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(train[target_cols].corr(), ax=ax)","2974af4a":"train_question_user = train['question_user_name'].unique()\ntest_question_user = test['question_user_name'].unique()\n\nprint('Number of unique question user in train: ', len(train_question_user))\nprint('Number of unique question user in test : ', len(test_question_user))\nprint('Number of unique question user in both train & test : ', len(set(train_question_user) & set(test_question_user)))","040cca55":"train_answer_user = train['answer_user_name'].unique()\ntest_answer_user = test['answer_user_name'].unique()\n\nprint('Number of unique answer user in train: ', len(train_answer_user))\nprint('Number of unique answer user in test : ', len(test_answer_user))\nprint('Number of unique answer user in both train & test : ', len(set(train_answer_user) & set(test_answer_user)))","2566235f":"print('Number of unique user in both question & anser in train  : ', len(set(train_answer_user) & set(train_question_user)))\nprint('Number of unique user in both question & anser in train  : ', len(set(test_answer_user) & set(test_question_user)))","e2d9b4de":"def char_count(s):\n    return len(s)\n\ndef word_count(s):\n    return s.count(' ')","50f96392":"train['question_title_n_chars'] = train['question_title'].apply(char_count)\ntrain['question_title_n_words'] = train['question_title'].apply(word_count)\ntrain['question_body_n_chars'] = train['question_body'].apply(char_count)\ntrain['question_body_n_words'] = train['question_body'].apply(word_count)\ntrain['answer_n_chars'] = train['answer'].apply(char_count)\ntrain['answer_n_words'] = train['answer'].apply(word_count)\n\ntest['question_title_n_chars'] = test['question_title'].apply(char_count)\ntest['question_title_n_words'] = test['question_title'].apply(word_count)\ntest['question_body_n_chars'] = test['question_body'].apply(char_count)\ntest['question_body_n_words'] = test['question_body'].apply(word_count)\ntest['answer_n_chars'] = test['answer'].apply(char_count)\ntest['answer_n_words'] = test['answer'].apply(word_count)","7585b036":"fig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_title_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['question_title_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['question_title_n_words'], label='train', ax=axes[1])\nsns.distplot(test['question_title_n_words'], label='test', ax=axes[1])\naxes[1].legend()","58b12c13":"fig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_body_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['question_body_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['question_body_n_words'], label='train', ax=axes[1])\nsns.distplot(test['question_body_n_words'], label='test', ax=axes[1])\naxes[1].legend()","0ce9939e":"train['question_body_n_chars'].clip(0, 5000, inplace=True)\ntest['question_body_n_chars'].clip(0, 5000, inplace=True)\ntrain['question_body_n_words'].clip(0, 1000, inplace=True)\ntest['question_body_n_words'].clip(0, 1000, inplace=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_body_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['question_body_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['question_body_n_words'], label='train', ax=axes[1])\nsns.distplot(test['question_body_n_words'], label='test', ax=axes[1])\naxes[1].legend()","22c81a0f":"train['answer_n_chars'].clip(0, 5000, inplace=True)\ntest['answer_n_chars'].clip(0, 5000, inplace=True)\ntrain['answer_n_words'].clip(0, 1000, inplace=True)\ntest['answer_n_words'].clip(0, 1000, inplace=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['answer_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['answer_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['answer_n_words'], label='train', ax=axes[1])\nsns.distplot(test['answer_n_words'], label='test', ax=axes[1])\naxes[1].legend()","bea3b8d3":"from scipy.spatial.distance import cdist\n\ndef calc_corr(df, x_cols, y_cols):\n    arr1 = df[x_cols].T.values\n    arr2 = df[y_cols].T.values\n    corr_df = pd.DataFrame(1 - cdist(arr2, arr1, metric='correlation'), index=y_cols, columns=x_cols)\n    return corr_df","5150cb2d":"number_feature_cols = ['question_title_n_chars', 'question_title_n_words', 'question_body_n_chars', 'question_body_n_words', 'answer_n_chars', 'answer_n_words']\n# train[number_feature_cols].corrwith(train[target_cols], axis=0)\n\ncorr_df = calc_corr(train, target_cols, number_feature_cols)","ddad22fa":"corr_df","4bc59b7c":"fig, ax = plt.subplots(figsize=(25, 5))\nsns.heatmap(corr_df, ax=ax)","e29080b2":"num_question = train['question_user_name'].value_counts()\nnum_answer = train['answer_user_name'].value_counts()\n\ntrain['num_answer_user'] = train['answer_user_name'].map(num_answer)\ntrain['num_question_user'] = train['question_user_name'].map(num_question)\ntest['num_answer_user'] = test['answer_user_name'].map(num_answer)\ntest['num_question_user'] = test['question_user_name'].map(num_question)\n\n# # map is done by train data, we need to fill value for user which does not appear in train data...\n# test['num_answer_user'].fillna(1, inplace=True)\n# test['num_question_user'].fillna(1, inplace=True)","79d59f84":"number_feature_cols = ['num_answer_user', 'num_question_user']\n# train[number_feature_cols].corrwith(train[target_cols], axis=0)\n\ncorr_df = calc_corr(train, target_cols, number_feature_cols)","9669a113":"fig, ax = plt.subplots(figsize=(30, 2))\nsns.heatmap(corr_df, ax=ax)","53e16b42":"## User check\n\nThe dataset contains question user and answer user information. This may be because user attribution is impotant, same user tend to answer same kind of question and same answer user tends to answer in similar quality.\n\nLet's check if how the user are distributed, and the user are duplicated in train\/test or not.","f7daeef9":"That's all for the start introduction of this competition!\n\n<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated :)<br>Thanks!<\/h3>","02aeeef2":"Other columns are metadata, which shows **question user** property, **answer user** property and **category** of question.","d0923f10":"# Exploratory Data Analysis\n\nLet's check each column of the data more carefully.","7c41286f":"# Simple feature engineering\n\nNow, I will proceed simple feature engineering and check if it explains data well or not.\n\n - Number of words in question title, body and answer.\n - question_user's question count in train.\n - answer_user's answer count in train.\n \nWork in progress... Maybe I will write in another kernel...","99dccfe0":"## target label distribution","a473153e":"## Category\n\nThe dataset consists of 5 categories: \"Technology\", \"Stackoverflow\", \"Culture\", \"Science\", \"Life arts\".<br\/>\nTrain\/Test distribution is almost same.","48a1454f":"## target labels\n\nLet's check target labels at first.\n\nEach row is identified by question id: `qa_id`, and other 30 columns are target labels.","804efbec":"## Correlation in target labels\n\nI could find following 3 pairs are **correlated**:\n\n - \"question_type_instructions\" & \"answer_type_instructions\"\n - \"question_type_procedure\" & \"answer_type_procedure\"\n - \"question_type_reason_explanation\" & \"answer_type_reason_explanation\" \n\nThis is reasonable that same evaluation on both question & answer are correlated.\n\nOn the other hand, **Anticorrelation** pattern can be found on following pairs:\n\n - \"question_fact_seeking\" & \"question_opinion_seeking\"\n - \"answer_type_instruction\" & \"answer_type_reason_explanation\"\n\nI think this is also reasonable that question that asks fact & opinion conflicts.<br\/>\nAnd answer which shows instruction or reason explanation also conflicts.","bae40853":"## Nan values\n\nThere is no nan values in the data.","0edd2945":"30 target labels consist of 21 question related labels and 9 answer related labels.\n\nNOTE: the labels are given in the continuous range from [0, 1]. NOT binary value.\n\n> This is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.","c5755c00":"Are these feature useful for predicting target values?<br\/>\nLet's check correlation with target values.","c4b2276a":"## feature columns\n\nLet's check feature columns one by one.","fdef0a8e":"Let's check each data size.\n\nTrain and test data consists of 6079 rows and 476 rows respectively.<br\/>\nWe have 30 different target labels to predict.<br\/>\nRest 10 columns are given as feature.","9b510156":"Outlier has too long, let's cut these outlier for visualization.","553aadba":"It seems common word usage distribution is similar between train & test dataset!","36e606a2":"Let's focus on the first row of the data. You can access original page mentioned in the `url` column.\n\n - [https:\/\/photo.stackexchange.com\/questions\/9169\/what-am-i-losing-when-using-extension-tubes-instead-of-a-macro-lens](https:\/\/photo.stackexchange.com\/questions\/9169\/what-am-i-losing-when-using-extension-tubes-instead-of-a-macro-lens)\n \n \n Only the question contains \"title\" (`question_title`), and we have `question_body` and `answer` which is given by sentences.","f96ae251":"## Word Cloud visualization\n\nLet's see what kind of word are used for question and answer. Also let's check the difference between train and test.","c6c2cd0b":"## Number of question or answer by user","04af3a31":"We can see following relationship\n\n - length of answer is correlated with \"answer_level_of_information\".\n - length of question_title is correlated with \"question_body_critical\" and length of question body is anticorrelated with it.\n - length of question_body is anticorrelated with \"question_well_written\"","cc210418":"Although correlation scale is small and it might not be a \"true correlation\", I can see following pattern:\n\n - `num_question_user` and `question_conversational` is correlated: People who post question a lot tend to ask question in conversational form.","dd05c82c":"**Number of chars and words in Question body**","b5b36942":"## Number of words","c2304e13":"# Google QUEST Q&A Labeling\n\n**Improving automated understanding of complex question answer content**\n\n> Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences.\n> ...\n> In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering.\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/google-research\/human_computable_dimensions_1.png)\n\n\nThe competition is **Notebook-only competition**. Your Notebook will re-run automatically against an unseen test set.\n\nThis competition data is small, only made of 6079 rows of train dataset.<br\/>\nSo I think this competition is **easy for beginners to participate** in terms of computational resource (unless you use BERT or any other heavy models to get good score), compared to the past competition hosted by Google like Open Image Challenges which requires a lot of GPU resources to train the model.","a841c99a":"It seems some of the labels are quite imbalanced. For example \"question_not_really_a_question\" is almost always 0, which means most of the question in the data is not a noisy data but an \"actual question\".","c1551a96":"**Number of chars and words in answer**\n\nAnswer number chars\/words distribution is similar to question body.","ea037317":"When you access to the URL, you can understand that multiple answer to the single question is given in the page. But only one answer is sampled in the dataset.\nAlso this answer may not be the most popular answer. We can find the answer of this data in the relatively bottom part of the homepage.","8c37456b":"**Number of chars and words in Question title**","ff29416e":"So these user information maybe important to predict `test` dataset!","d0b32b61":"Each row contains question and answer information together with the original Q&A page URL of the StackExchange properties.","11a9dba5":"Seems several users are in both train & test dataset.\n\nAlso, it seems many users ask question and answer.","73b06d6d":"# Data loading and data explanation"}}