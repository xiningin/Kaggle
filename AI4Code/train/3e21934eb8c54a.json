{"cell_type":{"5fbdf184":"code","934a9d88":"code","3a4b0268":"code","14bca45e":"code","3838c593":"code","225d0ade":"code","6550b1b1":"code","f5831a01":"code","c77ae8e8":"code","930ff2af":"code","74c9937b":"code","17f54c2b":"code","7b6ee00a":"code","02c10666":"code","ce10017c":"code","7c8e15cb":"code","d6714b15":"code","84040cce":"code","e24acd07":"code","8c21be5a":"code","30f4337b":"code","b5c6b5ec":"code","769dc13a":"code","4b814496":"code","0218d2ba":"code","3a59d617":"code","7c947c28":"code","76b4ba0b":"code","44629606":"code","b7cf2046":"code","99b57371":"code","a0f39d09":"code","1228c8a1":"code","853744a8":"code","c57839d0":"code","eb84bf66":"code","2c3b2805":"code","0dc3efef":"code","fbaf3fbd":"code","1e0a6e09":"code","92c380d1":"code","281b76e7":"code","be3cc5c9":"code","70c66eda":"code","99c5ae44":"code","800e34e5":"code","ada18a3f":"code","096bf5cf":"code","1b7ddb81":"code","ddd49be7":"code","f958ae93":"code","c7044ff0":"code","07e2b3a6":"code","2696b371":"code","c8109122":"code","f0bc99b7":"code","1fdca025":"code","f4f0a952":"code","51a926ce":"code","11ef622f":"code","65b9abb2":"code","b523b148":"markdown","c49e0e2f":"markdown","0c76f8bd":"markdown","31e81816":"markdown","b896e026":"markdown","958b0acd":"markdown","c6f19f22":"markdown","07d269e4":"markdown","827772e9":"markdown","a0f4d7d5":"markdown","c4dda3cb":"markdown","65c0b883":"markdown","a1863d3d":"markdown","f862097f":"markdown","98faa8b2":"markdown","f25add13":"markdown","c8e510fd":"markdown","64d27ff2":"markdown","201574d0":"markdown","9c51c3ee":"markdown","91cdb8d6":"markdown","4fe8d5c6":"markdown"},"source":{"5fbdf184":"import os\nimport random\n\n# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as cb\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\n\ndef set_seed(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed_value)\nSEED = 42\nset_seed(SEED)","934a9d88":"df = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ndf.head()","3a4b0268":"df.info()","14bca45e":"sns.distplot(df['battery_power'])","3838c593":"sns.distplot(df['blue'])","225d0ade":"sns.distplot(df['clock_speed'])","6550b1b1":"sns.distplot(df['dual_sim'])","f5831a01":"sns.distplot(df['fc'])","c77ae8e8":"sns.distplot(df['four_g'])","930ff2af":"sns.distplot(df['int_memory'])","74c9937b":"sns.distplot(df['m_dep'])","17f54c2b":"sns.distplot(df['mobile_wt'])","7b6ee00a":"sns.distplot(df['n_cores'])","02c10666":"sns.distplot(df['pc'])","ce10017c":"sns.distplot(df['px_height'])","7c8e15cb":"sns.distplot(df['px_width'])","d6714b15":"sns.distplot(df['ram'])","84040cce":"sns.distplot(df['sc_h'])","e24acd07":"sns.distplot(df['sc_w'])","8c21be5a":"sns.distplot(df['talk_time'])","30f4337b":"sns.distplot(df['three_g'])","b5c6b5ec":"sns.distplot(df['touch_screen'])","769dc13a":"sns.distplot(df['wifi'])","4b814496":"sns.distplot(df['price_range'])","0218d2ba":"sns.countplot(x = 'blue', data = df)","3a59d617":"sns.countplot(x = 'dual_sim', data = df)","7c947c28":"sns.countplot(x = 'four_g', data = df)","76b4ba0b":"sns.countplot(x = 'three_g', data = df)","44629606":"sns.countplot(x = 'touch_screen', data = df)","b7cf2046":"sns.countplot(x = 'wifi', data = df)","99b57371":"sns.boxplot(x = 'blue', y = 'price_range', data = df)","a0f39d09":"sns.boxplot(x = 'dual_sim', y = 'price_range', data = df)","1228c8a1":"sns.boxplot(x = 'four_g', y = 'price_range', data = df)","853744a8":"sns.boxplot(x = 'three_g', y = 'price_range', data = df)","c57839d0":"sns.boxplot(x = 'touch_screen', y = 'price_range', data = df)","eb84bf66":"sns.boxplot(x = 'wifi', y = 'price_range', data = df)","2c3b2805":"df.price_range.unique()","0dc3efef":"df.isnull().sum()","fbaf3fbd":"df.corr()['price_range'].sort_values(ascending = False)","1e0a6e09":"std = StandardScaler()\ndf_std = std.fit_transform(df)\ndf_std = pd.DataFrame(df_std, columns = df.columns)","92c380d1":"X = df.drop('price_range', axis = 1)\ny = df.price_range","281b76e7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","be3cc5c9":"import statsmodels.api as sm\nregressor = sm.OLS(y_train, X_train).fit()\nprint(regressor.summary())\n\nX_train_dropped = X_train.copy()","70c66eda":"while True:\n    if max(regressor.pvalues) > 0.05:\n        drop_variable = regressor.pvalues[regressor.pvalues == max(regressor.pvalues)]\n        print(\"Dropping \" + drop_variable.index[0] + \" and running regression again because pvalue is: \" + str(drop_variable[0]))\n        X_train_dropped = X_train_dropped.drop(columns = [drop_variable.index[0]])\n        regressor = sm.OLS(y_train, X_train_dropped).fit()\n    else:\n        print(\"All p values less than 0.05\")\n        break","99c5ae44":"print(regressor.summary())","800e34e5":"models = [LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    pred=clf.predict(X_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Model':model_names,'Accuracy':acc}\nd","ada18a3f":"acc_frame=pd.DataFrame(d)\nacc_frame.sort_values(by = 'Accuracy', ascending = False)","096bf5cf":"sns.barplot(y='Model',x='Accuracy',data=acc_frame.sort_values(by = 'Accuracy', ascending = False))","1b7ddb81":"sns.factorplot(x='Model',y='Accuracy',data=acc_frame.sort_values(by = 'Accuracy', ascending = False),kind='point',size=4,aspect=3.5)","ddd49be7":"cross_valid_scores = {}","f958ae93":"categorical_columns = ['dual_sim', 'three_g', 'touch_screen', 'wifi']","c7044ff0":"%%time\nparameters = {\n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_desicion_tree = DecisionTreeClassifier(\n    random_state=SEED,\n    class_weight='balanced',\n)\n\nmodel_desicion_tree = GridSearchCV(\n    model_desicion_tree, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_desicion_tree.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_desicion_tree.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \\\n    f'{model_desicion_tree.best_score_:.3f}'\n)\ncross_valid_scores['desicion_tree'] = model_desicion_tree.best_score_\nprint('-----')\n","07e2b3a6":"%%time\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25], \n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_random_forest = RandomForestClassifier(\n    random_state=SEED,\n    class_weight='balanced',\n)\n\nmodel_random_forest = GridSearchCV(\n    model_random_forest, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_random_forest.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_random_forest.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{model_random_forest.best_score_:.3f}'\n)\ncross_valid_scores['random_forest'] = model_random_forest.best_score_\nprint('-----')","2696b371":"%%time\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25, 50, 75, 100], \n    \"learning_rate\": [0.001, 0.01, 0.1, 1.],\n}\n\nmodel_adaboost = AdaBoostClassifier(\n    random_state=SEED,\n)\n\nmodel_adaboost = GridSearchCV(\n    model_adaboost, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_adaboost.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_adaboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{model_adaboost.best_score_:.3f}'\n)\ncross_valid_scores['ada_boost'] = model_adaboost.best_score_\nprint('-----')","c8109122":"%%time\nparameters = {\n    'max_depth': [3, 5, 7, 9], \n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1]\n}\n\nmodel_xgb = xgb.XGBClassifier(\n    random_state=SEED, verbosity = 0\n)\n\nmodel_xgb = GridSearchCV(\n    model_xgb, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_xgb.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_xgb.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_xgb.best_score_:.3f}'\n)\ncross_valid_scores['xgboost'] = model_xgb.best_score_\nprint('-----')","f0bc99b7":"%%time\nparameters = {\n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [7, 15, 31],\n}\n\nmodel_lgbm = lgbm.LGBMClassifier(\n    random_state=SEED,\n    class_weight='balanced',\n)\n\nmodel_lgbm = GridSearchCV(\n    model_lgbm, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_lgbm.fit(\n    X_train, \n    y_train,\n    categorical_feature=categorical_columns\n)\n\nprint('-----')\nprint(f'Best parameters {model_lgbm.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_lgbm.best_score_:.3f}'\n)\ncross_valid_scores['lightgbm'] = model_lgbm.best_score_\nprint('-----')","1fdca025":"%%time\nparameters = {\n    'iterations': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'depth': [3, 5, 7, 9, 11, 13],\n}\n\nmodel_catboost = cb.CatBoostClassifier(\n    verbose=False,\n)\n\nmodel_catboost = GridSearchCV(\n    model_catboost, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_catboost.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_catboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_catboost.best_score_:.3f}'\n)\ncross_valid_scores['catboost'] = model_catboost.best_score_\nprint('-----')","f4f0a952":"%%time\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1.],\n    \"penalty\": [\"l1\", \"l2\"]\n}\n\nmodel_logistic_regression = LogisticRegression(\n    random_state=SEED,\n    class_weight=\"balanced\",\n    solver=\"liblinear\",\n)\n\nmodel_logistic_regression = GridSearchCV(\n    model_logistic_regression, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_logistic_regression.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_logistic_regression.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_logistic_regression.best_score_:.3f}'\n)\ncross_valid_scores['logistic_regression'] = model_logistic_regression.best_score_\nprint('-----')","51a926ce":"%%time\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1.],\n    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n    \"gamma\": [\"scale\", \"auto\"],\n}\n\nmodel_svc = SVC(\n    random_state=SEED,\n    class_weight=\"balanced\",\n    probability=True,\n)\n\nmodel_svc = GridSearchCV(\n    model_svc, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_svc.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_svc.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_svc.best_score_:.3f}'\n)\ncross_valid_scores['svc'] = model_svc.best_score_\nprint('-----')","11ef622f":"%%time\nparameters = {\n    \"weights\": [\"uniform\", \"distance\"],\n}\n\nmodel_k_neighbors = KNeighborsClassifier(\n)\n\nmodel_k_neighbors = GridSearchCV(\n    model_k_neighbors, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_k_neighbors.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_k_neighbors.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_k_neighbors.best_score_:.3f}'\n)\ncross_valid_scores['k_neighbors'] = model_k_neighbors.best_score_\nprint('-----')","65b9abb2":"submit = pd.DataFrame(cross_valid_scores, index=['cross_valid_score']).T\nround(submit.sort_values(by = 'cross_valid_score', ascending = False),3)","b523b148":"### Logistic Regression","c49e0e2f":"# Data loading and overview","0c76f8bd":"# Data preprocessing","31e81816":"Okey. We have 6 categorial's features:\n* blue\n* dual sim\n* four_g\n* three_g\n* touch_screen\n* wifi","b896e026":"### XGBoost","958b0acd":"# Countplot","c6f19f22":"Touch phones are cheaper. It's interesting","07d269e4":"# Grid Search","827772e9":"# Import Libs","a0f4d7d5":"### Adaboost","c4dda3cb":"### Random Forest","65c0b883":"# Boxplot","a1863d3d":"# Model","f862097f":"### Decision Tree","98faa8b2":"### SVC","f25add13":"### Function removes features with high p-value","c8e510fd":"blue phones are expensive","64d27ff2":"# EDA\n## Distplot\nWe look at the distribution","201574d0":"### KNN ","9c51c3ee":"# Thanks for watching!\n## If you liked my fork then upvoted or write your opinion.","91cdb8d6":"### LightGBM","4fe8d5c6":"### Catboost"}}