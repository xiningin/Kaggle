{"cell_type":{"3083cb12":"code","2a356e34":"code","710e8ad3":"code","cccca2fe":"code","95a9fc49":"code","6d0cf4f0":"code","bccfbc29":"code","6ae74536":"code","140dcc1c":"code","0368ac4a":"code","3ddf7ad8":"code","127c60fa":"code","e1486f39":"code","d85789a5":"code","dea4c18e":"code","1bed4146":"code","337a24b7":"code","10859d09":"code","1dd171d9":"code","53266e7b":"markdown","8e60ce40":"markdown","a22d55fe":"markdown","51103faf":"markdown","a0392a0e":"markdown","755d297a":"markdown","0839ca86":"markdown"},"source":{"3083cb12":"import glob\nimport numpy as np\nimport os\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom tensorflow import keras\n","2a356e34":"list_order_book_file_train = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/*')","710e8ad3":"train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')","cccca2fe":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef realized_volatility_rolling(series_log_return):\n    return np.sqrt((series_log_return**2).expanding().mean())\n\ndef add_features_and_aggregate_data(df):\n    # spread between ask and bis price on first level in order book\n    df['price_spread_l1'] = df['ask_price1'] - df['bid_price1']\n    # added price spread as log difference to make it independent\n    df['price_spread_l1_log_diff'] = df.groupby('time_id')['price_spread_l1'].transform(log_return)\n    # I tried to aggregate the data in buckets of 50 seconds and called the bucket index timeslice\n    # I would like to reduce memory consumption and train time with this approach\n    # There is a maximum of 600(0-599) Seconds in every training bucket(stock_id, time_id)\n    # So there should be a maximum of 12 Buckets\n    df['timeslice'] = df['seconds_in_bucket'] \/\/ 50 \n    # calculated the weighted average price\n    df['wap'] = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1']+ df['ask_size1'])\n    # calculated log return\n    df['log_return'] = df.groupby(['time_id'])['wap'].transform(log_return)\n    # drop rows with na. The na gets created by the diff function by calculating the log return\n    # use inplace to save memory\n    df.dropna(subset=['log_return', 'price_spread_l1_log_diff'], inplace=True)\n    #calculate realized voltality for every bucket\n    df['realized_vol'] = df.groupby(['time_id', 'timeslice'])['log_return'].transform(realized_volatility_rolling)\n    \n    return df.groupby(['time_id', 'timeslice']).agg(\n                stock_id=('stock_id', 'max'),\n                min_price_spread_l1_log_diff=('price_spread_l1_log_diff', 'min'),\n                max_price_spread_l1_log_diff=('price_spread_l1_log_diff', 'max'),\n                mean_price_spread_l1_log_diff=('price_spread_l1_log_diff', 'mean'),\n                min_realized_vol=('realized_vol', 'min'),\n                max_realized_vol=('realized_vol', 'max'),\n                mean_realized_vol=('realized_vol', 'mean'),\n    ).reset_index()","95a9fc49":"feature_columns = ['stock_id', 'min_price_spread_l1_log_diff', 'max_price_spread_l1_log_diff', \n           'mean_price_spread_l1_log_diff', 'min_realized_vol', 'max_realized_vol', 'mean_realized_vol']","6d0cf4f0":"def get_input_data(list_file):\n    df_input = pd.DataFrame()\n    for file in list_file:\n        # read only needed columns to save memory\n        df_input_file = pd.read_parquet(file, \n                                        columns=['time_id', 'seconds_in_bucket', \n                                                 'bid_size1' ,'bid_price1', \n                                                 'ask_size1', 'ask_price1'])\n        # get stock id from filename\n        df_input_file['stock_id'] = int(file.split('=')[1])\n        # add features and aggregate data\n        df_input = pd.concat([df_input,\n                              add_features_and_aggregate_data(df_input_file)], \n                              ignore_index=True, copy=False)\n    return df_input","bccfbc29":"df_input = get_input_data(list_file=list_order_book_file_train)","6ae74536":"# add row id and targets to data\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ndf_input = df_input.merge(train, on=['time_id', 'stock_id'], how = 'left')","140dcc1c":"# split train and validation set groupwise by row-id\ntrain_inds, val_inds = next(GroupShuffleSplit(test_size=.20, n_splits=2, random_state = 7)\n                            .split(df_input, groups=df_input['row_id'])\n                           )\n\ntrain = df_input.iloc[train_inds]\nvalidation = df_input.iloc[val_inds]","0368ac4a":"# fit transformer on columns in train dataset\ncolumn_transformer = make_column_transformer(\n    (MinMaxScaler(), ['min_price_spread_l1_log_diff', 'max_price_spread_l1_log_diff', \n                        'mean_price_spread_l1_log_diff', 'min_realized_vol', \n                        'max_realized_vol', 'mean_realized_vol']),\n    remainder='passthrough')\ncolumn_transformer = column_transformer.fit(train[feature_columns])\n","3ddf7ad8":"# save memory\ndel(df_input)","127c60fa":"# reshape and transform columns groupwise to get the needed shape for LSTM [batch, timesteps, feature]\n# Pad to length 12, the max of seconds_in_bucket\/50, to get equal sized sequences.\ntrain_np = np.array([keras.preprocessing.sequence.pad_sequences(\n    column_transformer.transform(\n        x[feature_columns]\n    ).transpose(), \n    maxlen=12, \n    dtype='float32', \n    value=0.0).transpose() for _, x in train.groupby('row_id')])\nval_np = np.array([keras.preprocessing.sequence.pad_sequences(column_transformer.transform(x[feature_columns]).transpose(), \n                                                               maxlen=12, \n                                                               dtype='float32',\n                                                               value=0.0).transpose() for _, x in validation.groupby('row_id')])\n# scale targets\ntarget_scaler = StandardScaler()\ntarget_train = target_scaler.fit_transform(\n    train.groupby(['stock_id', 'time_id'])['target'].first().values.reshape(-1,1)\n).reshape(-1)\ntarget_val = target_scaler.transform(\n    validation.groupby(['stock_id', 'time_id'])['target'].first().values.reshape(-1,1)\n).reshape(-1)","e1486f39":"# save memory\ndel(train, validation)","d85789a5":"# some simple LSTM\n# The architecture is mostly random. I don't know how to create a good architecture for this problem\nlearning_rate = 0.03\n\ninputs_lstm = keras.layers.Input(shape=(train_np.shape[1], train_np.shape[2]))\nmasking = keras.layers.Masking(mask_value=0.0, input_shape=(train_np.shape[1], train_np.shape[2]))(inputs_lstm)\nlstm_1_out = keras.layers.LSTM(128, return_sequences=True)(masking)\nlstm_2_out = keras.layers.LSTM(64, return_sequences=True)(lstm_1_out)\nlstm_3_out = keras.layers.LSTM(10, activation='relu')(lstm_2_out)\noutputs = keras.layers.Dense(1)(lstm_3_out)\n\nmodel = keras.Model(inputs=inputs_lstm, outputs=outputs)\n\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=keras.metrics.mean_squared_error)\nmodel.summary()","dea4c18e":"# early stopping and fit function\ndef run_trainings_batch(dataset_train, target, val, epochs):\n    es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=5)\n\n    history = model.fit(\n        dataset_train,\n        target,\n        epochs=epochs,\n        batch_size=1000,\n        validation_data=val,\n        callbacks=[es_callback],\n    )","1bed4146":"# train model\nrun_trainings_batch(train_np, target_train, (val_np, target_val), 100)","337a24b7":"# save memory\ndel(train_np, target_train, val_np, target_val)","10859d09":"list_order_book_file_test = glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_test.parquet\/*')\n# get prediction for every file\ndef get_predictions(list_file):\n    prediction = pd.DataFrame()\n    for file in list_file:\n        df_input_test = get_input_data(list_file=[file])\n        df_input_test['row_id'] = df_input_test['stock_id'].astype(str) + '-' + df_input_test['time_id'].astype(str)\n        df_pred_np = np.array([keras.preprocessing.sequence.pad_sequences(\n            column_transformer.transform(x[feature_columns]).transpose(), \n            maxlen=12, \n            dtype='float32', \n            value=0.0).transpose() for _, x in df_input_test.groupby('row_id')])\n        prediction_new = pd.DataFrame()\n        prediction_new['row_id'] = df_input_test['row_id'].unique()\n        prediction_new['target'] = model.predict(df_pred_np).reshape(-1)\n        prediction = pd.concat([prediction, prediction_new])\n    prediction['target'] = target_scaler.inverse_transform(prediction['target'])\n    return prediction\n","1dd171d9":"# save submission\nget_predictions(list_file=list_order_book_file_test).to_csv('submission.csv',index = False)","53266e7b":"# LSTM Baseline","8e60ce40":"# LSTM","a22d55fe":"## Preprocess Input Data","51103faf":"# Calculate additional features","a0392a0e":"This is my first LSTM model. I tried to describe what I have done and hope for some helpful feedback and an upvote if you like.\nPlease feel free to fork this notebook.","755d297a":"Calculate the features from the Optiver examples","0839ca86":"# Submission"}}