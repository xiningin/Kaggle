{"cell_type":{"6d5101fd":"code","eccfed0c":"code","6e50a25f":"code","4082e10f":"code","86d8c452":"code","186a3acd":"code","aca1c29a":"code","288b056f":"code","1206d363":"code","c7b0dceb":"code","20f0b5b8":"code","1474943c":"code","8c2fd7de":"markdown","75ab2903":"markdown"},"source":{"6d5101fd":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\nfrom keras import Sequential\nimport PIL\nimport PIL.Image\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport pandas as pd","eccfed0c":"# Hypterparameters\ndata_dir = \"..\/input\/face-mask-dataset\/data\"\nBATCH_SIZE = 32\nimg_height = 224\nimg_width = 224\nEPOCHS = 30","6e50a25f":"# View image\nimg = PIL.Image.open(\"..\/input\/face-mask-dataset\/data\/with_mask\/with_mask_1.jpg\")\nimg","4082e10f":"# Dataset Loading, 80% training set, 20% test set\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  label_mode='binary',\n  shuffle=True,\n  image_size=(img_height, img_width),\n  batch_size=BATCH_SIZE)\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  label_mode='binary',\n  shuffle=True,\n  image_size=(img_height, img_width),\n  batch_size=BATCH_SIZE)\n","86d8c452":"# Data Preprocessing Layers\nfrom tensorflow.keras import layers\n\nresize_and_rescale = tf.keras.Sequential([\n  layers.experimental.preprocessing.Resizing(img_height, img_width),\n  layers.experimental.preprocessing.Rescaling(1.\/255)\n])\ndata_augmentation = tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  layers.experimental.preprocessing.RandomRotation(0.2),\n])\n","186a3acd":"## Configuring dataset for performance\nAUTOTUNE = tf.data.experimental.AUTOTUNE\ntraining_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntesting_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)","aca1c29a":"# Build model\nmodel = tf.keras.Sequential(\n    [resize_and_rescale,\n     data_augmentation,\n     layers.Conv2D(32, 3, activation=\"relu\", padding=\"SAME\", input_shape=(img_height, img_width, 3)),\n     layers.MaxPool2D(),\n     layers.Conv2D(64, 3, activation=\"relu\", padding=\"SAME\"),\n     layers.MaxPool2D(),\n     layers.Conv2D(128, 3, activation=\"relu\", padding=\"SAME\"),\n     layers.MaxPool2D(),\n     layers.Flatten(),\n     layers.Dense(256, activation=\"relu\"),\n     layers.Dense(1, activation=\"sigmoid\")]\n)","288b056f":"model.compile(optimizer=\"adam\",\n             loss=\"binary_crossentropy\",\n             metrics=[\"accuracy\"])","1206d363":"# Training model\nhistory = model.fit(training_ds,\n                 batch_size=BATCH_SIZE,\n                 validation_data=testing_ds,\n                 epochs=EPOCHS)","c7b0dceb":"model.summary()","20f0b5b8":"# Visualize the model\nval_loss = history.history['val_loss']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\naccuracy = history.history['accuracy']\nepochs_ranges = range(EPOCHS)\n\nplt.figure(figsize=(8, 16))\nplt.subplot(2, 1, 1)\nplt.plot(epochs_ranges, val_loss, label='Validation loss')\nplt.plot(epochs_ranges, loss, label='Training loss')\nplt.title(\"Training and Validation Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()\n\nplt.figure(figsize=(8, 16))\nplt.subplot(2, 1, 2)\nplt.plot(epochs_ranges, val_accuracy, label='Validation Accuracy')\nplt.plot(epochs_ranges, accuracy, label='Training Accuracy')\nplt.title(\"Training and Validation Accuracy\")\nplt.legend(loc=\"lower right\")\nplt.show()","1474943c":"# Evaluate model\nmodel.evaluate(test_ds)","8c2fd7de":"**Summary**:\nThe model achives 93.4% accuracy after 30 epochs. The model was likely to get overfitting at epochs 27.\n\nIn this notebook, I used data argumentation to get more images for training. I chose a layer with one unit and an activation function \"sigmoid\" as the output layer, becuase this is a binary classification problem.\n\nI tried different hypterparameters such as different image size and epochs, numbers of filter. The accuracy of all models are around 91%. \n\nTo improve this model, we can get more no mask and masked data from other datasets, or use mask icon to create a masked face by the no mask images.\n\nWe also can try advanced blocks such as MLP or residual net, or a deeper network with more Dropout layers.  ","75ab2903":"Resize Images into 528 x 528"}}