{"cell_type":{"2de593bc":"code","203b9012":"code","3342601b":"code","031b7d68":"code","f1af43ac":"code","fe6e825b":"code","1394efb8":"markdown","8b9b8f35":"markdown"},"source":{"2de593bc":"# FT_EMBEDDING_PATH = '..\/input\/fasttextcrawl300d2m\/crawl-300d-2m.vec'\n\n# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\n# def load_embeddings(embed_dir):\n#     embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n#     return embedding_index\n\n# ft = load_embeddings(FT_EMBEDDING_PATH)","203b9012":"import pickle\nfrom time import time\n\nt = time()\nwith open('..\/input\/crawl-300d-2M.pkl', 'rb') as fp:\n    ftext = pickle.load(fp)\nprint(time()-t)","3342601b":"len(ftext)","031b7d68":"list(ftext.keys())[1]","f1af43ac":"ftext[',']","fe6e825b":"Happy Kaggling","1394efb8":"## Introduction\nLoading FT Crawl from .txt files typically takes +3min on Kernels:","8b9b8f35":"Now, you can load from the pickled file directly. It loads the entire embedding, so if you only wish to use a subset, be sure to `del ftext` and `gc.collect()` once you're done with it:"}}