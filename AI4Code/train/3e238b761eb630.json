{"cell_type":{"2b0bc522":"code","4fb4e71c":"code","ea92c18b":"code","b305eb4f":"code","7a86d453":"code","852e4e17":"code","e92219d2":"code","a1a4461a":"code","9b98f331":"code","dc7a4829":"code","3eaff6ac":"code","dc1f1af8":"code","00121aa4":"code","9d9f4931":"code","3031d1b7":"code","6ed8f4c8":"code","6e9a3876":"code","413e4600":"code","653feec5":"code","fecd5744":"code","fbae27c9":"code","68c4b674":"code","362abf4d":"code","904adf01":"code","a2fa3d14":"code","0cb289f3":"markdown","e9e7486a":"markdown","272e8466":"markdown","c1dcc12e":"markdown","f11af15f":"markdown","98fae973":"markdown","59cc8714":"markdown","04ff4810":"markdown","87f66ff2":"markdown","6de2244b":"markdown","38a9f779":"markdown","d04596c9":"markdown","5633f565":"markdown","c9aebb72":"markdown","febd928a":"markdown"},"source":{"2b0bc522":"import pandas as pd","4fb4e71c":"df = pd.read_csv(\"..\/input\/fraud-detection-bank-dataset-20k-records-binary\/fraud_detection_bank_dataset.csv\")\ndf.head()","ea92c18b":"X = df.drop([\"targets\"], axis = 1).values\ny = df[\"targets\"].values","b305eb4f":"from sklearn.model_selection import validation_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nimport numpy as np","7a86d453":"param_range = np.array([70, 65, 60, 55, 50, 40, 35, 30, 25, 20, 15, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\n\ntrain_scores, test_scores = validation_curve(KNeighborsClassifier(), X, y, param_name = \"n_neighbors\", param_range = param_range)","852e4e17":"print(train_scores)","e92219d2":"print(np.mean(train_scores, axis = 1))","a1a4461a":"print(test_scores)","9b98f331":"print(np.mean(test_scores, axis = 1))","dc7a4829":"%matplotlib inline\nimport matplotlib.pyplot as plt","3eaff6ac":"plt.figure(figsize = (11, 9))\n\nplt.plot(param_range, np.mean(train_scores, axis = 1))\nplt.plot(param_range, np.mean(test_scores, axis = 1))\n\n# Reversing the X-axis (now going from 40 to 1 backwards)\nplt.xlim(np.max(param_range), 0)\nplt.ylim(0.9, 1)\n\nplt.title(\"How does k influence the accuracy?\", fontsize = 20)\nplt.xlabel(\"number of neighbors (k)\", fontsize = 15)\nplt.ylabel(\"model accuracy\", fontsize = 15)\n\n# Adding a legend\nplt.legend([\"train\", \"test\"], loc = \"upper left\", fontsize = 12)\n\nplt.show()","dc1f1af8":"X = df.drop([\"targets\"], axis = 1).values\ny = df[\"targets\"].values","00121aa4":"from sklearn.model_selection import validation_curve\nfrom sklearn.tree import DecisionTreeClassifier\nimport numpy as np","9d9f4931":"param_range = np.arange(1, 100)\n\ntrain_scores, test_scores = validation_curve(DecisionTreeClassifier(), X, y, param_name = \"max_depth\", param_range = param_range)","3031d1b7":"print(train_scores)","6ed8f4c8":"print(np.mean(train_scores, axis = 1))","6e9a3876":"print(test_scores)","413e4600":"print(np.mean(test_scores, axis = 1))","653feec5":"%matplotlib inline\nimport matplotlib.pyplot as plt","fecd5744":"plt.figure(figsize = (11, 9))\n\nplt.plot(param_range, np.mean(train_scores, axis = 1))\nplt.plot(param_range, np.mean(test_scores, axis = 1))\n\nplt.title(\"How does the tree depth influence the accuracy?\", fontsize = 20)\nplt.xlabel(\"depth levels of model\", fontsize = 15)\nplt.ylabel(\"model accuracy\", fontsize = 15)\n\n# Adding a legend\nplt.legend([\"train\", \"test\"], loc = \"upper left\", fontsize = 12)\n\nplt.show()","fbae27c9":"df","68c4b674":"X = df.drop([\"targets\"], axis = 1).values\ny = df[\"targets\"].values","362abf4d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0, test_size = 0.25)","904adf01":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","a2fa3d14":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(solver = \"liblinear\")\nmodel.fit(X_train, y_train)\n\nprint(model.score(X_test, y_test))","0cb289f3":"## **Logistic Regression**","e9e7486a":"#### Visualizing Validation Curve (Bias-Variance Dilemma) dependant from depth level of Decision Tree (max_depth argument)","272e8466":"## **Decision Tree**","c1dcc12e":"# Binary Classification with Validation Curve (Bias-Variance Dilemma)\n\n#### Loading and Displaying data","f11af15f":"#### Visualizing Validation Curve (Bias-Variance Dilemma) dependant from number of neighbors (k argument)","98fae973":"#### Applying KNN Algorithm and Analyzing results through Validation Curve","59cc8714":"Choosing k = 40 seems to be a good middle-way since both train and test accuracy are high at that point (91% and 98.5%). Higher k tend to produce a bigger bias and the model is more underfitted, lower k tend to overfit the model, since train accuracy continues to rise while test accuracy is declining towards smaller k values and thereby increasing the data variance - just notice the instability at k < 10.","04ff4810":"#### Scaling data","87f66ff2":"## Conclusion","6de2244b":"#### Defining X and y","38a9f779":"The Logistic Regression model is outperforming both the Decision Tree and the KNN algorithm. While DT and KNN are both strong on train data, they lack a higher accuracy on unknown data, whereas the Logistic Regression can show its dominance by achieving almost 100 % accuracy...","d04596c9":"## **K-Nearest-Neighbor (KNN)**","5633f565":"#### Conclusion","c9aebb72":"Answer: The Decision Tree depth is not as relevant as previously thought","febd928a":"#### Applying Logistic Regression"}}