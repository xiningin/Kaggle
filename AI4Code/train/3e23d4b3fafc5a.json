{"cell_type":{"f2cc1602":"code","8f0aa6be":"code","44d84a10":"code","5f18863c":"code","19ee4c5e":"code","c45da081":"code","0d7dbb0f":"code","979722d2":"code","eed229f0":"code","20b741b2":"code","8a1c6b17":"code","de5f35d1":"code","00a6d41c":"code","6e1d8f78":"code","2c58ddcc":"code","67f3c6fd":"code","d46236b7":"code","5faeed09":"code","4edbd4e0":"code","ac30c8d1":"code","38c52928":"code","16d3a50c":"code","993feeb0":"code","f274238c":"code","72959ec1":"code","f0daaf77":"code","1037c42c":"code","19a5a0cc":"code","57efff05":"code","c2642627":"code","412eea9c":"markdown","a0b0baa5":"markdown","d6820eb1":"markdown","a4fa686b":"markdown","bbdd1e4e":"markdown","8336b58f":"markdown","5838e65f":"markdown","e9408ac6":"markdown"},"source":{"f2cc1602":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nsns.set_style('darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f0aa6be":"import numpy as np # linear algebra\nimport pandas as pd # Data preprocessing\n","44d84a10":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_ID = train['PassengerId']\ntest_ID = test['PassengerId']\ntrain.drop(\"PassengerId\", axis = 1, inplace = True)\ntest.drop(\"PassengerId\", axis = 1, inplace = True)","5f18863c":"train.head()","19ee4c5e":"test.head(5)","c45da081":"print(\"The train data size is : {} \".format(train.shape))\nprint(\"The test data size is : {} \".format(test.shape))","0d7dbb0f":"sns.countplot(x=train[\"Pclass\"], data=train)","979722d2":"sns.countplot(x=train[\"Pclass\"], hue=train[\"Sex\"], data=train)","eed229f0":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, annot=True, fmt='.2f', vmax=0.9, square=True)","20b741b2":"sns.swarmplot(x=\"Survived\", y=\"Age\", data=train)","8a1c6b17":"sns.swarmplot(x=\"Survived\", y=\"Fare\", data=train)","de5f35d1":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Survived.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['Survived'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","00a6d41c":"all_data.isnull().sum().sort_values(ascending=False)[0:19]","6e1d8f78":"fare_nan_data = all_data[all_data['Fare'].isnull()]\nfare_nan_data_input = all_data[(all_data['Embarked'] == fare_nan_data['Embarked'].values[0]) & \n                            (all_data['Pclass'] == fare_nan_data['Pclass'].values[0])].mean()\nall_data.loc[fare_nan_data.index, 'Fare'] = fare_nan_data_input['Fare']\nall_data.loc[fare_nan_data.index, 'Fare']","2c58ddcc":"all_data['FamilyName'] = all_data['Name'].apply(lambda st: st[0:st.find(\",\")])\nall_data = all_data.drop('Name', 1)\nall_data = all_data.drop('Age', 1)\nall_data = all_data.drop('Cabin', 1)\nall_data = all_data.drop('Embarked', 1)\n\nall_data.head()","67f3c6fd":"all_data.isnull().sum().sort_values(ascending=False)[0:19]","d46236b7":"all_data = pd.get_dummies(all_data)\nlist(all_data.columns.values)","5faeed09":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","4edbd4e0":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgb\n\n","ac30c8d1":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","38c52928":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nRF = make_pipeline(RobustScaler(), RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=5))\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","16d3a50c":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=200)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","993feeb0":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, lasso),\n                                                 meta_model = RF)\n    \nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","f274238c":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","72959ec1":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n# stacked_pred = np.expm1(stacked_averaged_models.predict(train.values))\nprint(\"stacked_train_pred: {:.4f}\".format(rmsle(y_train, stacked_train_pred)))","f0daaf77":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\n# xgb_pred = np.expm1(model_xgb.predict(train))\nprint(\"xgb_train_pred: {:.4f}\".format(rmsle(y_train, xgb_train_pred)))","1037c42c":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\n# lgb_pred = np.expm1(model_lgb.predict(train.values))\nprint(\"lgb_train_pred: {:.4f}\".format(rmsle(y_train, lgb_train_pred)))","19a5a0cc":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","57efff05":"for n, i in enumerate(ensemble):\n    if i >= 0.5:\n        ensemble[n] = 1\n    else:\n        ensemble[n] = 0\nprint(ensemble)","c2642627":"sub = pd.DataFrame()\nsub['PassengerId'] = test_ID\nsub['Survived'] = list(map(int, ensemble))\nsub.to_csv('submission.csv',index=False)","412eea9c":"## Stacking Models\nusing only one model doesn't give me good results so i will use stacking model here","a0b0baa5":"get FamilyName out of Name and remove Name, Age, Cabin, Embarked since it has better results without it","d6820eb1":"## Modelling","a4fa686b":"## Data Processing","bbdd1e4e":"## Exploring Data","8336b58f":"looks like we have many empty data on Cabin and Age","5838e65f":"get dummies for categorical variables","e9408ac6":"Merge train and test data into one Dataframe"}}