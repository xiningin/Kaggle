{"cell_type":{"e8dcf6a6":"code","056b3e16":"code","6223e664":"code","65ea16ff":"code","18dccded":"code","0c6b9b92":"code","a7275f51":"code","571ac566":"code","1235cb47":"code","573ce05e":"code","ed79e17c":"code","aac49863":"code","bd4b1158":"code","8ae26a04":"code","73f4f8d1":"code","d159b098":"code","f9378529":"code","278b7d9b":"code","29376baf":"code","471f4469":"code","1557194c":"code","4a406419":"code","721d369d":"code","fb8339f8":"code","3719fe60":"code","bfc1aaf1":"code","85a5d86c":"code","de2ff806":"code","58c19ac2":"code","37073740":"code","f14fd7dc":"code","7c629da4":"code","5fb11e4d":"code","57e40738":"code","1caf5392":"code","7522b0bb":"code","6463e6e7":"code","c9a1df67":"code","271708c9":"code","1c1e36aa":"code","dae66d09":"code","9ad2df5b":"markdown","e2fa7124":"markdown","fbc75048":"markdown","e7cc7538":"markdown","76878829":"markdown","1e9db5bc":"markdown","f2730ec8":"markdown","a462093f":"markdown","b4a8fa0a":"markdown","495d13da":"markdown","34fe0d73":"markdown","c247148c":"markdown","9b233014":"markdown","48fd2638":"markdown","5c766e88":"markdown","4a611b43":"markdown","f8d1898a":"markdown"},"source":{"e8dcf6a6":"import numpy as np\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport sys\nfrom textblob import TextBlob \n\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom wordcloud import WordCloud, STOPWORDS \nfrom IPython.display import display\n\nimport spacy\n\n# Create an empty model\nnlp = spacy.load('en')","056b3e16":"\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","6223e664":"print('train:')\nprint(train.shape)\ntrain.head()","65ea16ff":"print('test:')\nprint(test.shape)\ntest.head()","18dccded":"print('train:')\ndf_null = pd.DataFrame(train.isnull().sum())\ndf_null = df_null.rename(columns={0:'Number of null values'})\ndf_null['Percentage null values'] = round(train.isnull().sum()\/train.id.count()*100,2)\ndf_null","0c6b9b92":"print('test:')\ndf_null = pd.DataFrame(test.isnull().sum())\ndf_null = df_null.rename(columns={0:'Number of null values'})\ndf_null['Percentage null values'] = round(test.isnull().sum()\/test.id.count()*100,2)\ndf_null","a7275f51":"train.info()","571ac566":"test.info()","1235cb47":"print('- Real')\ndisplay(train[train['target']==1].groupby('keyword')['keyword'].agg(['count']))\nprint('- Fake')\ndisplay(train[train['target']==0].groupby('keyword')['keyword'].agg(['count']))\n","573ce05e":"print('- Real')\ndisplay(train[train['target']==1].groupby('location')['location'].agg(['count']))\nprint('- Fake')\ndisplay(train[train['target']==0].groupby('location')['location'].agg(['count']))","ed79e17c":"Per_Real = round((train[train.target==1].target.count()\/train.target.count())*100,2)\nPer_Fake = round((train[train.target==0].target.count()\/train.target.count())*100,2)\n\ntable_data=[\n    [\"Percentage of Reals (1)\", Per_Real],\n    [\"Percentage of Fakes (0)\", Per_Fake]\n]\n\nfig = plt.figure()\n# definitions for the axes\nleft, width = 0.10, 1.5\nbottom, height = 0.1, .8\nbottom_h = left_h = left + width + 0.02\n\nrect_cones = [left, bottom, width, height]\nrect_box = [left_h, bottom, 0.17, height]\n\n# plot\nax1 = plt.axes(rect_cones)\ntrain.groupby('target')['target'].agg(['count']).plot.bar(ax=ax1)\nplt.title('Frequency of target')\n\nax2 = plt.axes(rect_box)\nmy_table = ax2.table(cellText = table_data, loc ='right')\nmy_table.set_fontsize(40)\nmy_table.scale(4,4)\nax2.axis('off')\nplt.show()","aac49863":"num_caracters = []\nnum_words = [] \nfor i in train.text:\n    aux = nlp(i)\n    num_words.append(len(aux))\n    num_caracters.append(len(i))","bd4b1158":"train['num_caracters'] = num_caracters\n\ntrain['num_words'] = num_words","8ae26a04":"fig, axes = plt.subplots(nrows=1,ncols=1, figsize=(12,5))\ntrain[train.target == 0].num_caracters.plot.hist(bins=150,  ax = axes, label = 'Fake', color = 'blue')\ntrain[train.target == 1].num_caracters.plot.hist(bins=150,  ax = axes, label = 'Real', color = 'orange')\nplt.xlim(0,max(train.num_caracters))\naxes.legend()\nplt.title('# Characters tweeds Histogram')\nplt.show()","73f4f8d1":"fig, axes = plt.subplots(nrows=1,ncols=2, figsize=(12,5))\ntrain[train.target == 0].num_words.plot.hist(bins=40,  ax = axes[0], label = 'Fake', color='blue')\naxes[0].set_title('# Words tweeds Histogram (Fake)')\ntrain[train.target == 1].num_words.plot.hist(bins=40,  ax = axes[1], label = 'Real', color = 'orange')\nplt.xlim(0,max(train.num_words))\nplt.title('# Words tweeds Histogram (Real)')\nplt.show()\nprint('mean(# Words tweeds (fake)){}'.format(round(train[train.target == 0].num_words.mean(),2)))\nprint('std(# Words tweeds(fake)){}'.format(round(train[train.target == 0].num_words.std(),2)))\nprint('mean(# Words tweeds (real)){}'.format(round(train[train.target == 1].num_words.mean(),2)))\nprint('std(# Words tweeds(real)){}'.format(round(train[train.target == 1].num_words.std(),2)))","d159b098":"#creating a list where we classify the words which are stop or not.\n\ndef classify_word(serie):\n    '''you pass a serie from dataframe that contains words\n        returns two list: 1. non stop words and 2. stop words\n    '''\n    not_stop_list = []\n    stop_list = []\n    for line in serie:\n        for token in nlp(line):\n            if token.text != \" \":\n                if token.is_stop:\n                    stop_list.append(token)\n                else:\n                    not_stop_list.append(token)\n    \n    return(not_stop_list, stop_list)","f9378529":"train.text = train.text.str.lower() # convert all words in lowercase","278b7d9b":"not_stopwords_real_list, stopwords_real_list = classify_word(train[train.target==1].text)\n#not_stopwords_real_list = [str(x).lower() for x in not_stopwords_real_list] # convert everything in lowercase\n#stopwords_real_list = [str(x).lower() for x in stopwords_real_list] #convert everything in lowercase\nnot_stopwords_fake_list, stopwords_fake_list = classify_word(train[train.target==0].text)\n#not_stopwords_fake_list = [str(x).lower() for x in not_stopwords_fake_list] # convert everything in lowercase\n#stopwords_fake_list = [str(x).lower() for x in stopwords_fake_list] #convert everything in lowercase","29376baf":"class count_words():\n    '''\n        It is a class that contains two function. Both of functions return dict with the frequency of the list\n        or token you pass. Example return: key: word and value: number of repetitions of the word. \n    '''\n    \n    def __init__(self, list_pass):\n        self.list_pass = list_pass\n    \n    def count_token(list_pass):\n        '''you pass a token'''\n        count = {}\n        for word in list_pass:\n            if word.text in count :\n                count[word.text] += 1\n            else:\n                count[word.text] = 1\n        return(count)  \n    def count_list(list_pass):\n        '''you pass a str'''\n        count = {}\n        for word in list_pass:\n            if word in count :\n                count[word] += 1\n            else:\n                count[word] = 1\n        return(count)","471f4469":"#counting words \nstopwords_real_dict = count_words.count_token(stopwords_real_list)\nstopwords_fake_dict = count_words.count_token(stopwords_fake_list)","1557194c":"#sorting dictionary by values\nstopwords_real_dict = {k:stopwords_real_dict[k] for k in sorted(stopwords_real_dict, key=stopwords_real_dict.get, reverse = True)}\nstopwords_fake_dict = {k:stopwords_fake_dict[k] for k in sorted(stopwords_fake_dict, key=stopwords_fake_dict.get, reverse = True)}\n\n#getting 20th first\nstopwords_fake_dict_20 = {k: stopwords_fake_dict[k] for k in list(stopwords_fake_dict)[:20]}\nstopwords_real_dict_20 = {k: stopwords_real_dict[k] for k in list(stopwords_real_dict)[:20]}","4a406419":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(12,5))\nplt.subplots_adjust(hspace = .4)\naxes[0].bar(stopwords_fake_dict_20.keys(), stopwords_fake_dict_20.values(), width = .5, color = 'b')\naxes[0].set_title('20th stopwords more populars from Fake tweets')\naxes[1].bar(stopwords_real_dict_20.keys(), stopwords_real_dict_20.values(), width = .5, color = 'orange')\naxes[1].set_title('20th stopwords more populars from Real tweets')\nplt.show()","721d369d":"def punctuation_funct(serie):\n    ''' \n        function you pass a serie from pandas and\n        it returns a list which contains punctuation symbols\n    '''\n    punctuation_list = []\n    \n    for line in serie:\n        line = line.split(' ')\n        for word in line:\n            if word in string.punctuation:\n                punctuation_list.append(word)\n    return(punctuation_list)\n","fb8339f8":"punctuation_fake_list = punctuation_funct(train[train.target == 0].text)\npunctuation_real_list = punctuation_funct(train[train.target == 1].text)\n\n# dictionary with the frequency\npunctuation_fake_dict = count_words.count_list(punctuation_fake_list)\npunctuation_real_dict = count_words.count_list(punctuation_real_list)\n#sorting dictionary by values\npunctuation_real_dict = {k:punctuation_real_dict[k] for k in sorted(punctuation_real_dict, key=punctuation_real_dict.get, reverse = True)}\npunctuation_fake_dict = {k:punctuation_fake_dict[k] for k in sorted(punctuation_fake_dict, key=punctuation_fake_dict.get, reverse = True)}\n","3719fe60":"fig, axes = plt.subplots(nrows=2,ncols=1, figsize=(12,5))\nplt.subplots_adjust(hspace = .4)\naxes[0].bar(punctuation_fake_dict.keys(), punctuation_fake_dict.values(), width = .5, color = 'b')\naxes[0].set_title('Punctuation from Fake tweets')\naxes[1].bar(punctuation_real_dict.keys(), punctuation_real_dict.values(), width = .5, color = 'orange')\naxes[1].set_title('Punctuation from Real tweets')\nplt.show()","bfc1aaf1":"class removing():\n    def __init__(sel,texto):\n        self.text = texto\n    def remove_url(texto):\n        return(re.sub(r'http:\/\/\\S+|https:\/\/\\S+','', texto))\n    def remove_emoji(texto):\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', texto)\n    def remove_punctuation(texto):\n        return(re.sub(r'[^\\w\\s]','',texto))","85a5d86c":"def spelling_correcting(texto):\n    return(str(TextBlob(texto).correct()))","de2ff806":"train['text_not_'] = train.text.apply(lambda x:removing.remove_url(x))\ntrain['text_not_'] = train.text_not_.apply(lambda x: removing.remove_punctuation(x))\ntrain['text_not_'] = train.text_not_.apply(lambda x: removing.remove_emoji(x))\n\n# doing the same with test set\ntest['text_not_'] = test.text.apply(lambda x:removing.remove_url(x))\ntest['text_not_'] = test.text_not_.apply(lambda x: removing.remove_punctuation(x))\ntest['text_not_'] = test.text_not_.apply(lambda x: removing.remove_emoji(x))","58c19ac2":"#converting all words in lower\ntrain['text_not_'] = train.text_not_.str.lower()\n#converting all words in lower\ntest['text_not_'] = test.text_not_.str.lower()","37073740":"#train['text_not_correct'] = train.text_not_.apply(lambda x: spelling_correcting(x))\n#test['text_not_correct'] = test.text_not_.apply(lambda x: spelling_correcting(x))","f14fd7dc":"# We are doing this because the step before takes a long time to get the values. Therefore, it has been executed\n# and saved the results.\n\n#train.to_csv('..input\/set-modifies\/train_aux.csv', index=None, header=True)\n#test.to_csv('..input\/set-modifies\/test_aux.csv', index=None, header=True)\n\n\ntrain = pd.read_csv('..\/input\/set-modifies\/train_aux.csv')\ntest = pd.read_csv('..\/input\/set-modifies\/test_aux.csv')","7c629da4":"not_stop_list_real, stop_list_real = classify_word(train[train.target == 1].text_not_correct)\nnot_stop_list_fake, stop_list_fake = classify_word(train[train.target == 0].text_not_correct)","5fb11e4d":"def show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        collocations = False,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n    return(wordcloud)\n    ","57e40738":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 12))\n#real\naxes[0].axis('off')\naxes[0].imshow(show_wordcloud(not_stop_list_real))\naxes[0].set_title('Words from Real text without stop words', fontsize=20)\n#fake\naxes[1].axis('off')\naxes[1].imshow(show_wordcloud(not_stop_list_fake))\naxes[1].set_title('Words from Fake text without stop words', fontsize=20)\nplt.show()","1caf5392":"not_stop_dict_real = count_words.count_token(not_stop_list_real)\nnot_stop_dict_fake = count_words.count_token(not_stop_list_fake)","7522b0bb":"def isdict1_notdict2(dict1, dict2):\n    ''' Passing two dictionaries, \n        Returning list with all elements keys and frequency the values \n        Example: {d:5}=[d,d,d,d,d]\n    '''\n    dict_aux = {}\n    for key, value in dict1.items():\n        if key not in dict2.keys():\n            dict_aux[key] = value\n            \n    list_aux = []\n    for key, value in dict_aux.items():\n        list_aux = list_aux + ([key] * value)\n    return(list_aux)","6463e6e7":"real_not_fake_list = isdict1_notdict2(not_stop_dict_real,not_stop_dict_fake) #it is in real and not in fake\nfake_not_real_list = isdict1_notdict2(not_stop_dict_fake, not_stop_dict_real) #it is in fake and not in real","c9a1df67":"fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(15, 12))\n#real\naxes[0].axis('off')\naxes[0].imshow(show_wordcloud(' '.join(sorted(real_not_fake_list))))\naxes[0].set_title('Words are in Real Not in Fake (not stopwords)', fontsize=20)\n#fake\naxes[1].axis('off')\naxes[1].imshow(show_wordcloud(' '.join(sorted(fake_not_real_list))))\naxes[1].set_title('Words are in Fake Not in Real (not stopwords)', fontsize=20)\nplt.show()\n","271708c9":"import numpy as np\n\nnp.random.seed(0)\ntrain = train.reindex(np.random.permutation(train.index))\n\n\nX_train = train.loc[:6852, 'text_not_correct'].values\ny_train = train.loc[:6852, 'target'].values\nX_test = train.loc[6852:, 'text_not_correct'].values\ny_test = train.loc[6852:, 'target'].values\n\nfrom nltk.stem.porter import PorterStemmer\n\nporter = PorterStemmer()\n\ndef tokenizer(text):\n    return text.split()\n\n\ndef tokenizer_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\n\ntfidf = TfidfVectorizer(strip_accents=None,\n                        lowercase=False,\n                        preprocessor=None)\n\nparam_grid = [{'vect__ngram_range': [(1, 1)],\n               'vect__stop_words': [stop, None],\n               'vect__tokenizer': [tokenizer, tokenizer_porter],\n               'clf__penalty': ['l1', 'l2'],\n               'clf__C': [1.0, 10.0, 100.0]},\n              {'vect__ngram_range': [(1, 1)],\n               'vect__stop_words': [stop, None],\n               'vect__tokenizer': [tokenizer, tokenizer_porter],\n               'vect__use_idf':[False],\n               'vect__norm':[None],\n               'clf__penalty': ['l1', 'l2'],\n               'clf__C': [1.0, 10.0, 100.0]},\n              ]\n\nlr_tfidf = Pipeline([('vect', tfidf),\n                     ('clf', LogisticRegression(random_state=0))])\n\ngs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n                           scoring='accuracy',\n                           cv=5,\n                           verbose=1,\n                           n_jobs=-1)\n\ngs_lr_tfidf.fit(X_train, y_train)\n\nclf = gs_lr_tfidf.best_estimator_\nprint('Test Accuracy: %.3f' % clf.score(X_test, y_test))\n","1c1e36aa":"y_pred = gs_lr_tfidf.predict(test.text_not_)","dae66d09":"submission = pd.DataFrame({\n        \"id\":test['id'],\n        \"target\": y_pred\n    })","9ad2df5b":"- Target","e2fa7124":"### -  Features and Target\n\nGetting type of featues.\n","fbc75048":"- Stopwords (Example: the, be and so on).","e7cc7538":"## 4. Modelling<a id='section4'><\/a>\n\nThis code is based on the examples from Sebastian Raschka's *Python Machine Learning* book. ","76878829":"### - Null values\n\nNumber of null values contain each variable from different dataframes.","1e9db5bc":"## Index\n- [1. Import libraries and download data](#section1)\n- [2. EDA](#section2)\n- [3. Cleaning Data](#section3)\n- [4. Modelling](#section4)\n","f2730ec8":"- Number of words contain each tweed \n","a462093f":"## 1. Import libraries and download data<a id='section1'><\/a>","b4a8fa0a":"- Number of characters contain each tweed","495d13da":"#### - text\n\nThis feature refers to tweets which people write.","34fe0d73":"### - Shape and head\n\nFirstly, we are going to see the shape and the head of both dataset, train and test.","c247148c":"## 2. EDA<a id='section2'><\/a>","9b233014":"#### - keyword","48fd2638":"## 3. Cleaning <a id='section3'><\/a>","5c766e88":"- Punctuation","4a611b43":"#### - location","f8d1898a":"# NLP with Disaster Tweets"}}