{"cell_type":{"0e60b967":"code","50e87592":"code","9744cf5b":"code","cc1cc457":"code","8b696d93":"code","7d72ddae":"code","b1035ee7":"code","80aa3dd7":"code","d081819e":"code","108ee846":"code","509d45c3":"code","e5c3f6d8":"code","ea07e647":"code","389218a4":"code","63dfae3e":"code","929aca8f":"code","3b79d69b":"code","e525e650":"code","15e180ce":"code","45ea7f67":"code","48596a66":"code","3af41d7d":"code","0848541d":"code","446c6e88":"code","854ebef3":"code","8ca9d798":"code","4773a623":"code","587c56c1":"code","ce41cc7b":"code","0876b206":"code","971f4a5e":"code","80dc4515":"code","34d41855":"code","754a554d":"code","6e5bb007":"code","bdb8f0ab":"code","0f30121a":"code","140d2f5e":"code","04f55e4f":"code","0b0e83b7":"code","03a33139":"code","65e74c8d":"code","c098d6a6":"code","03a5ebec":"code","43c32ae6":"code","05168f9e":"code","18fdfd94":"code","32d9cfdf":"code","8e17d784":"code","4bf93c24":"code","01d8c6e5":"markdown","a58394ba":"markdown","af5aeb43":"markdown","9e191ba3":"markdown","171106d4":"markdown","da1f24a6":"markdown","08a6cd11":"markdown","c3d4d641":"markdown","86ce1ca3":"markdown","1fdb6d28":"markdown","d6486b53":"markdown","0033070a":"markdown","488c7256":"markdown","a0a4dc52":"markdown","c92bb7f1":"markdown","ca9aef77":"markdown"},"source":{"0e60b967":"#not required\n#!pip install kaggle\n!mkdir ~\/.kaggle\n!cp '\/content\/drive\/My Drive\/AI\/kaggle.json' ~\/.kaggle\n!chmod 600 ~\/.kaggle\/kaggle.json","50e87592":"import pandas as pd\nimport numpy as np\nimport scipy\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# from pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom IPython.display import display\n\nfrom sklearn import metrics\n\n# Install fastai - Already done\n# !pip3 install fastai\n!pip install pandas-summary","9744cf5b":"PATH = \"\/content\/drive\/My Drive\/AI\/\"\n!ls {PATH}","cc1cc457":"types = {'PassengerId': 'int32',\n        'Survived': 'int8', # bool gives object :()\n        'Pclass': 'int8',\n        'Sex_male': 'int8',\n        'Sex_female': 'int8',\n        'Age': 'float32',\n        'SibSp': 'int8',\n        'Parch': 'int8',\n        'Fare': 'float32',\n        'CabinKnown': 'int8',\n        'Deck': 'int8',\n        'Title': 'int8',\n        'Embarked_C': 'int8',\n        'Embarked_Q': 'int8',\n        'Embarked_S': 'int8'\n        }","8b696d93":"# Load from drive\n\n%%time\n\ntrain_full = pd.read_csv(f'{PATH}titanic_train_full.csv', index_col='PassengerId', low_memory=False, dtype=types )\ntrain      = pd.read_csv(f'{PATH}titanic_train.csv',      index_col='PassengerId', low_memory=False, dtype=types )\nvalid      = pd.read_csv(f'{PATH}titanic_valid.csv',      index_col='PassengerId', low_memory=False, dtype=types )\ntest       = pd.read_csv(f'{PATH}titanic_test.csv',       index_col='PassengerId', low_memory=False, dtype=types )\n\ntrain_full.info()","7d72ddae":"# Check correlation \n\nplt.figure(figsize=(12,8))\nsns.heatmap(data=train.corr(), annot=True)","b1035ee7":"from sklearn.metrics import precision_score, recall_score, accuracy_score\n\ndef score ( truth, predition, dataset ):\n  print(\"Accuracy \" + dataset + \" \",  accuracy_score(truth, predition))\n  print(\"Precision \" + dataset + \" \", precision_score(truth, predition))\n  print(\"Recall \" + dataset + \" \",    recall_score(truth, predition))\n  print(\".\")","80aa3dd7":"#Drop ids and time_steps\n\nX = train.drop ( 'Survived', axis=1 )\nX_valid = valid.drop ( 'Survived', axis=1 )\n\ny = train[ 'Survived' ]\ny_valid = valid[ 'Survived' ]","d081819e":"rf = RandomForestClassifier(n_jobs=-1, random_state=42, oob_score=True )\n# %prun \nrf.fit(X, y)\n\nY = rf.predict(X)\nY_valid = rf.predict(X_valid)\n\nscore ( y, Y, 'train' )\nscore ( y_valid, Y_valid, 'valid' )\n\nrf.oob_score_","108ee846":"%time preds = np.stack( [ t.predict(X_valid) for t in rf.estimators_ ] )\nnp.mean(preds[:,0]), np.std(preds[:,0])\n\n# todo parallel task without old fast AI","509d45c3":"preds.shape","e5c3f6d8":"x = valid.copy ( )\nx['pred_std'] = np.std(preds, axis=0)\nx['pred'] = np.mean(preds, axis=0)","ea07e647":"x['Sex_male'].value_counts().plot.barh()","389218a4":"sexConfidenceFields = ['Sex_male', 'Survived', 'pred', 'pred_std']\nsexConfidence = x[sexConfidenceFields].groupby(sexConfidenceFields[0]).mean()\nsexConfidence.plot('Survived', 'pred', 'barh', xerr='pred_std', xlim=(0,1.0) )\n\nsexConfidence","63dfae3e":"x['Title'].value_counts().plot.barh()","929aca8f":"titleConfidenceFields = ['Title', 'Survived', 'pred', 'pred_std']\ntitleConfidence = x[titleConfidenceFields].groupby(titleConfidenceFields[0]).mean()\ntitleConfidence.plot('Survived', 'pred', 'barh', xerr='pred_std', xlim=(0,1.0) )\n\ntitleConfidence","3b79d69b":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\nforest_importances = pd.DataFrame ( importances, index=X.columns )\nforest_importances.plot.bar(yerr=std)\n\n# TODO : try to Shuffle column one by one.\n\n# TODO : Create one hot encoding (create dummies) and see feature importance.","e525e650":"from scipy.cluster import hierarchy as hc","15e180ce":"corr = np.round(scipy.stats.spearmanr ( X ).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=X.columns, orientation='left', leaf_font_size=16)\nplt.show()","45ea7f67":"# Remove one feature , fit a random forest and check oob_score\n\ndef get_oob(df,removed):\n\n  x = df.copy ( )\n  if removed != None:\n    x = x.drop ( removed, axis=1 )\n\n  # From video added random_state\n  m = RandomForestClassifier ( n_jobs=-1, oob_score=True, random_state=42 )\n  m.fit(x, y)\n\n  return m.oob_score_","48596a66":"# baseline\n\nget_oob(X, None)","3af41d7d":"for c in ('CabinKown', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Sex_female', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Deck', 'Title'):\n    print( \"Dropping \" + c, get_oob(X, c))","0848541d":"# Save load np array\n\n# np.save('tmp\/keep_cols.npy', np.array(df_keep.columns))\n# keep_cols = np.load('tmp\/keep_cols.npy')\n# df_keep = df_trn[keep_cols]","446c6e88":"train.plot('SibSp', 'Parch', 'scatter', alpha=0.5, figsize=(10,8))","854ebef3":"train.plot('Fare', 'Pclass', 'scatter', alpha=0.5, figsize=(10,8))","8ca9d798":"!pip install ggplot","4773a623":"%matplotlib inline\nfrom ggplot import *","587c56c1":"# ggplot(train, aes('Fare', 'Survived'))+stat_smooth(se=True, method='loess')\n\n# Fare\nplt.figure(figsize=(16,8))\nsns.lineplot(x=X['Deck'], y=y)","ce41cc7b":"!pip install pdpbox","0876b206":"from pdpbox import pdp, get_dataset","971f4a5e":"def plot_pdp(feat, clusters=None, feat_name=None):\n    feat_name = feat_name or feat\n    p = pdp.pdp_isolate(rf, X, feature=feat_name, model_features=X.columns)\n    return pdp.pdp_plot(p, feat_name, plot_lines=True, \n                        cluster=clusters is not None, n_cluster_centers=clusters)","80dc4515":"plot_pdp('Fare')","34d41855":"plot_pdp('Age')","754a554d":"feats = ['Fare', 'Age']\np = pdp.pdp_interact(rf, X, features=feats, model_features=X.columns)\npdp.pdp_interact_plot(p, feats)","6e5bb007":"# need one hot encoding - and use column names instead of 'Fare' 'Sex' 'Age'\nplot_pdp(['Fare', 'Sex_female', 'Age'], 5)","bdb8f0ab":"!pip install treeinterpreter","0f30121a":"from treeinterpreter import treeinterpreter as ti","140d2f5e":"row = X_valid.values[None,0]\nrow","04f55e4f":"rf.fit ( X, y )","0b0e83b7":"prediction, bias, contributions = ti.predict(rf, X_valid)","03a33139":"prediction[0], bias[0]","65e74c8d":"contributions.shape","c098d6a6":"idxs = np.argsort(contributions[0])","03a5ebec":"# For regressor Only\n# [o for o in zip(tdl.valid.xs.columns[idxs], tdl.valid.xs.iloc[0][idxs], contributions[0][idxs])]","43c32ae6":"random_sample = np.random.randint(1, len(X_valid))\nprint(\"Selected Sample     : %d\"%random_sample)\nprint(\"Actual Target Value : %s\"%y_valid.iloc[random_sample])\nprint(\"Predicted Value     : %s\"%rf.predict([X_valid.iloc[random_sample]]))\n\ndef create_contrbutions_df(contributions, random_sample, feature_names):\n    contribs = contributions[random_sample].tolist()\n    contribs.insert(0, bias[random_sample])\n    contribs = np.array(contribs)\n\n    print(\"feature_names : %s\"%feature_names.insert(0,'bias'))\n    print(\"contribs : %d\"%contribs.shape[0])\n\n    contrib_df = pd.DataFrame(data=contribs, index=feature_names.insert(0,'bias'), columns=[\"Contributions_0\", \"Contributions_1\"])\n    prediction = contrib_df[[\"Contributions_0\", \"Contributions_1\"]].sum()\n    contrib_df.loc[\"Prediction\"] = prediction\n    return contrib_df\n\ncontrib_df = create_contrbutions_df(contributions, random_sample, X_valid.columns)\ncontrib_df","05168f9e":"#After several trys and trials, score are better without this features\n\ntrain_full = train_full.drop ( [ 'Parch', 'CabinKown', 'Deck', \"Title\", 'Embarked_C', 'Sex_male', 'Embarked_Q', 'Embarked_S'], axis=1 )\ntrain      = train.drop      ( [ 'Parch', 'CabinKown', 'Deck', \"Title\", 'Embarked_C', 'Sex_male', 'Embarked_Q', 'Embarked_S'], axis=1 )\nvalid      = valid.drop      ( [ 'Parch', 'CabinKown', 'Deck', \"Title\", 'Embarked_C', 'Sex_male', 'Embarked_Q', 'Embarked_S'], axis=1 )\ntest       = test.drop       ( [ 'Parch', 'CabinKown', 'Deck', \"Title\", 'Embarked_C', 'Sex_male', 'Embarked_Q', 'Embarked_S'], axis=1 )","18fdfd94":"train_full.to_csv ( '\/content\/drive\/My Drive\/AI\/titanic_train_full_optimized.csv' )\ntest.to_csv ( '\/content\/drive\/My Drive\/AI\/titanic_test_optimized.csv' )\ntrain.to_csv ( '\/content\/drive\/My Drive\/AI\/titanic_train_optimized.csv' )\nvalid.to_csv ( '\/content\/drive\/My Drive\/AI\/titanic_valid_optimized.csv' )","32d9cfdf":"#Drop ids and time_steps\n\nX = train.drop ( 'Survived', axis=1 )\nX_valid = valid.drop ( 'Survived', axis=1 )\n\ny = train[ 'Survived' ]\ny_valid = valid[ 'Survived' ]","8e17d784":"rf = RandomForestClassifier(n_jobs=-1, random_state=42, oob_score=True )\n#%prun \nrf.fit(X, y)\n\nY = rf.predict(X)\nY_valid = rf.predict(X_valid)\n\nscore ( y, Y, 'train' )\nscore ( y_valid, Y_valid, 'valid' )\n\nrf.oob_score_","4bf93c24":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\nforest_importances = pd.DataFrame ( importances, index=X.columns )\nforest_importances.plot.bar(yerr=std)","01d8c6e5":"Setup Kaggle","a58394ba":"## Try to drop data from feature analysis","af5aeb43":"## Remove redundant feature - Hierarchical clustering from rank correlation","9e191ba3":"## Feature importance \n( Already attempted while the correlation step but not the same result )","171106d4":"import lib","da1f24a6":"Split Independant\/Dependant Variables","08a6cd11":"## Loading Data","c3d4d641":"## Confidence\n*   Print the variance of prediction along estimators","86ce1ca3":"Metrics","1fdb6d28":"## Tree Interpreter","d6486b53":"## Titanic competition second part\n\nThis notebook is the firs part of 3 notebooks\n1.   Data Exploration\n2.   Feature Analysis\n3.   Models\n\nThe purpose to this part analyse the Features of the dataset thanks to random forest (based on Fast AI random forest course)\n\n*   Confidence\n*   Feature importance\n*   Redundant features\n*   Tree interpreter\n\n\n","0033070a":"Simple Random Forest Classifier","488c7256":"Data Visualization","a0a4dc52":"We can remove\n\n'SibSp', 'Embarked_S'","c92bb7f1":"## Partial dependence","ca9aef77":"Test improvments"}}