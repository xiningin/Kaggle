{"cell_type":{"d2b40e34":"code","dbf433d6":"code","ae72ae9f":"code","11a79e7b":"code","73b29991":"code","8643a47d":"code","acc57db3":"code","00005610":"code","c4c1d436":"code","f659ce46":"code","bd338341":"code","daccf1ae":"code","924f6df8":"code","32871eaf":"code","4d59a784":"code","ddbaf2c0":"code","26b2bd97":"code","4e1744c8":"code","abfa53c6":"code","a8411e4b":"code","0758756d":"code","a9f1a9af":"code","b0fec0fd":"code","101a23ab":"code","ed270708":"code","95314213":"code","ee71e27b":"code","b821143c":"code","100a3ba0":"code","603bcf16":"code","5e5ad8c2":"code","59fc70dd":"code","f3bb9f86":"code","bfb4f3fa":"code","9d1a3ab0":"code","67919616":"code","833cb337":"code","b0b700a7":"code","46f6d8f6":"code","8beb5198":"code","93a644c9":"code","ae609e24":"code","34764a17":"code","fab32380":"code","01ce12c5":"code","ccb7090b":"code","4f016ece":"code","33381ec3":"code","bb089dfe":"code","32674ac0":"code","ed60ef6a":"code","a47b9c97":"code","12a47cc3":"code","58f32b66":"code","498af95b":"code","a04c3d83":"code","fb5568ad":"code","dd2ccdd1":"code","5bcfd093":"code","9386fadf":"code","4206ba22":"code","e3503793":"code","1fb0601f":"code","ff9f561a":"code","4032a957":"code","0b78eafc":"code","c905325c":"code","713cb9dc":"code","a0ae59b9":"code","37dcecba":"code","df2eab17":"code","6e6f3b18":"code","9af8d4d0":"code","6620df43":"code","a0ae6236":"code","4113ff97":"code","ad020d6b":"code","9edf6eef":"code","c33f7468":"code","355ba7ab":"code","abc333f0":"code","3bc4de4a":"code","ed4eab18":"code","a597dce6":"code","3be8dd74":"code","8e3853a4":"code","88eba497":"code","4a4b2d19":"code","69c6327d":"code","5eacbdb3":"code","5e81bfe1":"code","402ad7e7":"code","7a5313cd":"code","65696c7e":"code","ac8e57bd":"code","22e9e12e":"code","3fbacde8":"code","81dedbfd":"code","a13a0cd1":"code","31ceac5e":"code","0bf97065":"code","74cb29f1":"code","e98cb475":"code","68fe6b50":"code","9eb222e6":"code","7fc7e1c5":"code","a50b5f7b":"code","7cd3b0f1":"code","dc63e89f":"markdown","8baecd40":"markdown","7e387ff6":"markdown","00245ba1":"markdown","46c61455":"markdown","ff6cf8fb":"markdown","ba2e6188":"markdown","475c33cb":"markdown","9cd5244d":"markdown","6cd475bb":"markdown","d27b558c":"markdown","2ce02d30":"markdown","8a82aa32":"markdown","597ca34f":"markdown","aa86a316":"markdown","47e03fe1":"markdown","7a862a98":"markdown","d973fc03":"markdown","7dc4e2f3":"markdown","0c900517":"markdown","14a82e17":"markdown","0f0e8615":"markdown","b5e055be":"markdown","20c0abb8":"markdown","c20fc4f1":"markdown","05a57c8b":"markdown","39977565":"markdown","512fdb6f":"markdown","6dac8069":"markdown","24022638":"markdown","c47e0f5d":"markdown","3fd55c45":"markdown","595c0d22":"markdown","c7199aba":"markdown","da377f8c":"markdown","7d9f4490":"markdown","6b8e4b27":"markdown","7b1e6f08":"markdown","78beb656":"markdown","a95c0a2d":"markdown","a14feb01":"markdown","c2916b22":"markdown","efdfcdda":"markdown","703c0d46":"markdown","53bb1a26":"markdown","1df26619":"markdown"},"source":{"d2b40e34":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","dbf433d6":"test = pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv')\ntrain = pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv')","ae72ae9f":"sns.set_style('darkgrid')\ntrain.head(2)","11a79e7b":"old_cols = train.columns.tolist()\ncols = train.iloc[0, :].values.tolist()\nnew_col_dict = dict(zip(old_cols, cols))","73b29991":"new_col_dict.pop('DateReported')\nnew_col_dict.pop('DependentsOther')\nnew_col_dict.pop('DaysWorkedPerWeek')\ntrain.rename(columns = new_col_dict, \n            inplace = True)","8643a47d":"train.head(2)","acc57db3":"old_data_size = train.shape\ntrain = train.drop(0).reset_index().drop('index', axis = 1)\nnew_data_size = train.shape \nold_data_size[0]-1 == new_data_size[0]","00005610":"train.head(2)","c4c1d436":"# Checking if duplicates are present\ntrain.shape == train.drop_duplicates().shape","f659ce46":"train.describe()","bd338341":"# Checking if columns have right datatypes\ntrain.info()","daccf1ae":"train['DateTimeOfAccident'] = pd.to_datetime(train['DateTimeOfAccident'])\ntrain['DateReported'] = pd.to_datetime(train['DateReported'])\n\n#pd.Series(map(len, train['Age'])).value_counts()  -> making sure all values are of same size \ntrain['Age'] = train['Age'].astype(int)\n\n#pd.Series(map(len,train['DependentChildren'])).value_counts() -> making sure all values are of same size \ntrain['DependentChildren'] = train['DependentChildren'].astype(int)\n\ntrain['DependentsOther'] = train['DependentsOther'].astype(int)\ntrain['HoursWorkedPerWeek'] = train['HoursWorkedPerWeek'].astype(float)\ntrain['InitialIncurredCalimsCost'] = train['InitialIncurredCalimsCost'].astype(float)\ntrain['UltimateIncurredClaimCost'] = train['UltimateIncurredClaimCost'].astype(float)\ntrain['WeeklyWages'] = train['WeeklyWages'].astype(float)","924f6df8":"train.info()","32871eaf":"# Checking for nulls\ntrain.isnull().sum()","4d59a784":"# Very small percentage of null values\n(train.isnull().sum()\/train.shape[0])*100","ddbaf2c0":"df = train.copy() ","26b2bd97":"fill_cols = ['MaritalStatus', 'WeeklyWages', 'HoursWorkedPerWeek']\nfor col in fill_cols:\n    df[f'{col}'] = df[f'{col}'].fillna(df[f'{col}'].mode()[0])","4e1744c8":"df.isnull().sum().sum()","abfa53c6":"df.head(2)","a8411e4b":"df['ClaimNumber'].nunique() == df.shape[0]","0758756d":"sns.histplot(df['ClaimNumber'].value_counts().values)","a9f1a9af":"df['ClaimNumber'].value_counts()","b0fec0fd":"claims_6 = df['ClaimNumber'].value_counts()[df['ClaimNumber'].value_counts()==6].index[0]\ndf[df['ClaimNumber'] == claims_6]","101a23ab":"df[df['ClaimNumber'] == claims_6]['UltimateIncurredClaimCost'].describe()","ed270708":"sns.histplot(df[df['ClaimNumber'] == claims_6]['UltimateIncurredClaimCost'])\nplt.title('Target distribution for ClaimNumber = WC7301773')","95314213":"claims_5 = df['ClaimNumber'].value_counts()[df['ClaimNumber'].value_counts() == 5].index.tolist()\ndf['ClaimNumber'].value_counts()[df['ClaimNumber'].value_counts() == 5]","ee71e27b":"df[df['ClaimNumber'].isin(claims_5)].head()","b821143c":"g = sns.FacetGrid(data = df[df['ClaimNumber'].isin(claims_5)], col = 'ClaimNumber')\ng.map(sns.histplot, 'UltimateIncurredClaimCost')","100a3ba0":"df.DateTimeOfAccident.dt.day.value_counts().plot.bar()  \nplt.title('Distribution of accidents across days')","603bcf16":"df.DateTimeOfAccident.dt.month.value_counts().plot.bar()\nplt.title('Distribution of accidents across months')","5e5ad8c2":"df.DateTimeOfAccident.dt.year.value_counts().plot.bar()\nplt.title('Distribution of accidents across years')","59fc70dd":"df['DateReported'].dt.day.value_counts().plot.bar()\nplt.title('Distribution of reports across days')","f3bb9f86":"df['DateReported'].dt.month.value_counts().plot.bar()\nplt.title('Distribution of reports across months')","bfb4f3fa":"df['DateReported'].dt.day.value_counts().plot.bar()\nplt.title('Distribution of reports across days')","9d1a3ab0":"df['DateReported'].dt.year.value_counts().plot.bar()\nplt.title('Distribution of reports across years')","67919616":"year_2006 = df[df['DateReported'].dt.year == 2006]\nyear_2006['DateReported'].dt.month.value_counts().plot.bar()\nplt.title('Distribution of months of reports in 2006')","833cb337":"# accident in 2005 and reported in 2006\nx = df[df['DateTimeOfAccident'].dt.year==2005]\nx[x['DateReported'].dt.year==2006].shape","b0b700a7":"year_2006.shape","46f6d8f6":"df['DateTimeOfAccident'].dt.year.max()","8beb5198":"year_2006['DateTimeOfAccident'].dt.year.value_counts().plot.bar()","93a644c9":"plt.figure(figsize=(15,5))\nsns.countplot(df['Age'])","ae609e24":"sns.countplot(df['Gender'])","34764a17":"df['Gender'].replace('U', 'M', inplace = True)\ndf['Gender'].value_counts()","fab32380":"sns.countplot(df['MaritalStatus'])","01ce12c5":"sns.catplot(data = df, \n            col = 'MaritalStatus', \n            y = 'UltimateIncurredClaimCost', \n            kind = 'box')","ccb7090b":"indx = df[df['MaritalStatus']=='M']['UltimateIncurredClaimCost'].idxmax()\ndf.iloc[indx, :]","4f016ece":"df.groupby('MaritalStatus')['UltimateIncurredClaimCost'].median()","33381ec3":"df['DependentChildren'].value_counts()\/df.shape[0]","bb089dfe":"df['DependentsOther'].value_counts()\/df.shape[0]","32674ac0":"df[df['MaritalStatus']=='M']['DependentChildren'].value_counts().plot.pie()\nplt.title('No. of dependent children for married')","ed60ef6a":"df[df['MaritalStatus']=='M']['DependentsOther'].value_counts().plot.pie()\nplt.title('No. of other dependents for married')","a47b9c97":"sns.histplot(df['WeeklyWages'])","12a47cc3":"(df[(df['WeeklyWages']>=0)  & (df['WeeklyWages']<=210)].shape[0]\/df.shape[0])*100","58f32b66":"(df[(df['WeeklyWages']>=210)  & (df['WeeklyWages']<=400)].shape[0]\/df.shape[0])*100","498af95b":"(df[(df['WeeklyWages']>=400)  & (df['WeeklyWages']<=500)].shape[0]\/df.shape[0])*100","a04c3d83":"(df[(df['WeeklyWages']>=500)  & (df['WeeklyWages']<=700)].shape[0]\/df.shape[0])*100","fb5568ad":"(df[(df['WeeklyWages']>=700)].shape[0]\/df.shape[0])*100","dd2ccdd1":"df['binned_wages'] = pd.cut(df['WeeklyWages'],\n      bins = (0, 210, 400, 500, 700, 8000),\n      labels = ['Very_low', 'Low', 'Medium', 'High', 'Very_high'])","5bcfd093":"sns.histplot(df['binned_wages'])","9386fadf":"sns.catplot(data = df, \n           col = 'binned_wages',\n           y = 'UltimateIncurredClaimCost', kind = 'box')","4206ba22":"sns.countplot(df['PartTimeFullTime'])","e3503793":"df['HoursWorkedPerWeek'].describe()","1fb0601f":"sns.boxplot(df['HoursWorkedPerWeek'])","ff9f561a":"p1 = np.percentile(df['HoursWorkedPerWeek'], 1)\np2 = np.percentile(df['HoursWorkedPerWeek'], 99)\npercent1 = (df[df['HoursWorkedPerWeek'] < p1].shape[0]\/df.shape[0])*100\npercent2 = (df[df['HoursWorkedPerWeek'] > p2].shape[0]\/df.shape[0])*100\nwithin = (df[(df['HoursWorkedPerWeek'] >= p1) & (df['HoursWorkedPerWeek'] <= p2)].shape[0]\/df.shape[0])*100\n\nprint(f'{p1} is the first percentile and {p2} is the second percentile of HoursWorkedPerWeek')\nprint('{:.3f}% of points lie below the first percentile'.format(percent1))\nprint('{:.3f}% of points lie above the ninety-nineth percentile'.format(percent2))\nprint('{:.3f}% points lie within this percentile range'.format(within))","4032a957":"df[df['HoursWorkedPerWeek']<10].describe(include = 'all').fillna('-')","0b78eafc":"p2 = np.percentile(df['HoursWorkedPerWeek'], 99)\nmedian_hours_worked = df['HoursWorkedPerWeek'].median()\nfor i in df['HoursWorkedPerWeek']:\n    if i > p2:\n        df['HoursWorkedPerWeek'] = df['HoursWorkedPerWeek'].replace(i, median_hours_worked)","c905325c":"df['HoursWorkedPerWeek'].describe()","713cb9dc":"sns.countplot(df['DaysWorkedPerWeek'])","a0ae59b9":"df['ClaimDescription'].shape[0]==df.shape[0]","37dcecba":"sns.distplot(df['InitialIncurredCalimsCost'])","df2eab17":"corr = df.corr()\nsns.heatmap(corr)","6e6f3b18":"df['Time_taken_to_report'] =(df['DateReported'] - df['DateTimeOfAccident']).dt.days\ndf['Total_dependents'] = df['DependentsOther'] + df['DependentChildren']\ndf['UltimateIncurredClaimCost'].idxmax()","9af8d4d0":"df.iloc[11009, :]","6620df43":"df_no_out = df.copy()\n\ndf_no_out.drop(11009, axis = 0, inplace = True)\n\ndf_no_out = df_no_out.reset_index().drop('index', axis = 1)\n\nsns.pairplot(df_no_out)","a0ae6236":"df_no_out['Prod_dependents'] = df_no_out['DependentChildren'] * df_no_out['DependentsOther']\ndf_no_out.info()","4113ff97":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n#df_no_out['Encoded_ClaimNumber'] = encoder.fit_transform(df_no_out['ClaimNumber'])\ndf_no_out['Encoded_Gender'] = encoder.fit_transform(df_no_out['Gender'])\ndf_no_out['Encoded_MaritalStatus'] = encoder.fit_transform(df_no_out['MaritalStatus'])\ndf_no_out['Encoded_PartTimeFullTime'] = encoder.fit_transform(df_no_out['PartTimeFullTime'])\ndf_no_out['Encoded_binned_wages'] = encoder.fit_transform(df_no_out['binned_wages'])","ad020d6b":"df_no_out.info()","9edf6eef":"# 'DependentChildren', 'DependentsOther' are correlated with two features already\nfeature_cols = ['Age', 'WeeklyWages', 'HoursWorkedPerWeek', \n             'DaysWorkedPerWeek', 'InitialIncurredCalimsCost', \n              'Time_taken_to_report', 'Total_dependents', 'Prod_dependents', \n                'Encoded_Gender', 'Encoded_MaritalStatus', \n              'Encoded_PartTimeFullTime',  'Encoded_binned_wages']\ntarget = df_no_out['UltimateIncurredClaimCost']","c33f7468":"from sklearn.preprocessing import minmax_scale\ndf_ml = df_no_out[feature_cols]\ndf_inter = minmax_scale(df_ml.values)\ndf_norm = pd.DataFrame(data = df_inter, columns = df_ml.columns.tolist())","355ba7ab":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nx_train, x_test, y_train, y_test = train_test_split(df_norm[feature_cols],\n                                                    target,\n                                                    random_state = 0,\n                                                    train_size = 0.8)\nmodel = LinearRegression()\nmodel.fit(x_train, y_train)\n#rmse(y_pred = model.predict(x_test)\nprint('Train score:', mean_squared_error(y_pred = model.predict(x_test) , y_true = y_test, squared =False))\nprint('test score:', mean_squared_error(y_pred = model.predict(x_test) , y_true = y_test, squared =False))","abc333f0":"def reg_pipe(data, cols):\n    model = LinearRegression()\n    x_train, x_test, y_train, y_test = train_test_split(data[cols],\n                                                    target,\n                                                    random_state = 0,\n                                                    train_size = 0.8)\n    model.fit(x_train, y_train)\n    print('Train score:', mean_squared_error(y_pred = model.predict(x_train) , y_true = y_train, squared =False))\n    print('Test score:', mean_squared_error(y_pred = model.predict(x_test) , y_true = y_test, squared =False))","3bc4de4a":"reg_pipe(df_norm, feature_cols)","ed4eab18":"reg_pipe(df_norm, ['Age',\n                   'WeeklyWages',\n     'HoursWorkedPerWeek',\n     'DaysWorkedPerWeek',\n     'InitialIncurredCalimsCost',\n     'Time_taken_to_report',\n     'Total_dependents',\n     'Prod_dependents',])","a597dce6":"reg_pipe(df_norm, [ 'Encoded_Gender',\n     'Encoded_MaritalStatus',\n     'Encoded_PartTimeFullTime',\n     'Encoded_binned_wages'])","3be8dd74":"reg_pipe(df_norm, ['Age',\n 'WeeklyWages',\n 'HoursWorkedPerWeek',\n 'DaysWorkedPerWeek',\n 'InitialIncurredCalimsCost',\n 'Time_taken_to_report',\n 'Encoded_Gender',\n 'Encoded_MaritalStatus',\n 'Encoded_PartTimeFullTime',\n 'Encoded_binned_wages'])","8e3853a4":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score","88eba497":"x_train, x_test, y_train, y_test = train_test_split(df_norm[feature_cols],\n                                                    target,\n                                                    random_state = 0,\n                                                    train_size = 0.8)","4a4b2d19":"from sklearn.linear_model import Ridge\nridge = Ridge()\nridge.fit(x_train, y_train)\nprint('Train score:', mean_squared_error(y_pred = ridge.predict(x_train) , y_true = y_train, squared =False))\nprint('Test score:', mean_squared_error(y_pred = ridge.predict(x_test) , y_true = y_test, squared =False))","69c6327d":"parameters = {'alpha': [0.1,0.5,0.6,0.3,0.8, 0.9], \n       'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}","5eacbdb3":"ridge = Ridge()\ncv = GridSearchCV(ridge,parameters,cv=5)\ncv.fit(x_train,y_train)","5e81bfe1":"cv.best_params_","402ad7e7":"#pd.DataFrame(cv.fit(x_train,y_train).cv_results_)\nridge = Ridge(alpha = 0.5, solver = 'saga')\nridge.fit(x_train, y_train)\nprint('Train score:', mean_squared_error(y_pred = ridge.predict(x_train) , y_true = y_train, squared =False))\nprint('Test score:', mean_squared_error(y_pred = ridge.predict(x_test) , y_true = y_test, squared =False))","7a5313cd":"target.head()","65696c7e":"final_model = Ridge(alpha = 0.5, solver = 'saga')\nfinal_model.fit(df_norm, target)\nprint('In-sample score:', mean_squared_error(y_pred = final_model.predict(df_norm) , y_true = target, squared =False))","ac8e57bd":"#cross_val_score(ridge, df_norm, target, cv = 4, n_jobs= -1, scoring='neg_root_mean_squared_error\u2019')","22e9e12e":"test['DateTimeOfAccident'] = pd.to_datetime(test['DateTimeOfAccident'])\ntest['DateReported'] = pd.to_datetime(test['DateReported'])\n\n#pd.Series(map(len, train['Age'])).value_counts()\ntest['Age'] = test['Age'].astype(int)\n\n#pd.Series(map(len,train['DependentChildren'])).value_counts()\ntest['DependentChildren'] = test['DependentChildren'].astype(int)\n\ntest['DependentsOther'] = test['DependentsOther'].astype(int)\ntest['HoursWorkedPerWeek'] = test['HoursWorkedPerWeek'].astype(float)\ntest['InitialIncurredCalimsCost'] = test['InitialIncurredCalimsCost'].astype(float)\ntest['WeeklyWages'] = test['WeeklyWages'].astype(float)","3fbacde8":"test.isnull().sum()","81dedbfd":"test['MaritalStatus'] = test['MaritalStatus'].fillna(test['MaritalStatus'].mode()[0])","a13a0cd1":"test.isnull().sum()","31ceac5e":"test['Gender'].replace('U', 'M', inplace = True)\ntest['Gender'].value_counts()","0bf97065":"test['binned_wages'] = pd.cut(test['WeeklyWages'],\n      bins = (0, 210, 400, 500, 700, 8000),\n      labels = ['Very_low', 'Low', 'Medium', 'High', 'Very_high'])","74cb29f1":"p2 = np.percentile(test['HoursWorkedPerWeek'], 99)\nmedian_hours_worked = test['HoursWorkedPerWeek'].median()\nfor i in test['HoursWorkedPerWeek']:\n    if i > p2:\n        test['HoursWorkedPerWeek'] = test['HoursWorkedPerWeek'].replace(i, median_hours_worked)","e98cb475":"test['Time_taken_to_report'] =(test['DateReported'] - test['DateTimeOfAccident']).dt.days\ntest['Total_dependents'] = test['DependentsOther'] + test['DependentChildren']\ntest['Prod_dependents'] = test['DependentChildren'] * test['DependentsOther']\nencoder = LabelEncoder()\n\ntest['Encoded_Gender'] = encoder.fit_transform(test['Gender'])\ntest['Encoded_MaritalStatus'] = encoder.fit_transform(test['MaritalStatus'])\ntest['Encoded_PartTimeFullTime'] = encoder.fit_transform(test['PartTimeFullTime'])\ntest['Encoded_binned_wages'] = encoder.fit_transform(test['binned_wages'])\nfeature_cols = ['Age', 'WeeklyWages', 'HoursWorkedPerWeek', \n             'DaysWorkedPerWeek', 'InitialIncurredCalimsCost', \n              'Time_taken_to_report', 'Total_dependents', 'Prod_dependents', \n               'Encoded_Gender', 'Encoded_MaritalStatus', \n              'Encoded_PartTimeFullTime',  'Encoded_binned_wages']","68fe6b50":"test_ml = test[feature_cols]\ntest_inter = minmax_scale(test_ml.values)\ntest_norm = pd.DataFrame(data = test_inter, columns = test_ml.columns.tolist())","9eb222e6":"#sub = {'ClaimNumber':test['ClaimNumber'],\n#       'UltimateIncurredClaimCost':final_model.predict(test_norm)}\n#Submission = pd.DataFrame(sub)","7fc7e1c5":"#Submission.to_csv('.\/submission.csv', index = False)","a50b5f7b":"#test[test['ClaimNumber'] == 'WC937279317820']","7cd3b0f1":"submission = pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv')\nsubmission['UltimateIncurredClaimCost'] = final_model.predict(test_norm)\nsubmission.to_csv('.\/submission.csv', index = False)","dc63e89f":"- regularize  \n- gridsearch  \n- cross val","8baecd40":"`'PartTimeFullTime'`","7e387ff6":"`'DaysWorkedPerWeek'`","00245ba1":"Hence, need only to treat the outliers beyond the $99^{th}$ percentile.","46c61455":"`'MaritalStatus'`","ff6cf8fb":"from sklearn.ensemble import RandomForestRegressor\nRFR = RandomForestRegressor()\nRFR.fit(x_train, y_train)\nprint('Train score:', mean_squared_error(y_pred = RFR.predict(x_test) , y_true = y_test, squared= False))\nprint('test score:', mean_squared_error(y_pred = RFR.predict(x_test) , y_true = y_test, squared = False))","ba2e6188":"Since very few claimants have high weekly wages even greater than 1000 units per week, it's better to bin this column.","475c33cb":"## EDA","9cd5244d":"Grad boost\n","6cd475bb":"As expected, the time of occurance of the accidents are almost evenly distributed among the days, months and years. The reason 31st shows much lower accidents could probably be because not all months have 31 days.","d27b558c":"## Model optimization","2ce02d30":"`'HoursWorkedPerWeek'`","8a82aa32":"Assuming 'U' means unknown, impute 'U' with the mode of gender, since there's only one row with 'U' for gender. ","597ca34f":"Most are solitary and have no dependents.","aa86a316":"This corroborates the earlier observation that most of the claimants (about 90%) work full time.","47e03fe1":"Moving onto `DateTimeOfAccident`","7a862a98":"Dropping this row as it skews the relationships.","d973fc03":"Being only descriptive, we are better off dropping this columns while building the model.","7dc4e2f3":"## ML Model","0c900517":"Linear regression","14a82e17":"`'WeeklyWages'`","0f0e8615":"Mode imputation","b5e055be":"### Importing the libraries","20c0abb8":"## Feature Engineering","c20fc4f1":"There appers to be some significant variance of the target for the policy identified as `WC7301773`. As seen above, for the same policy the target isn't evenly distributed. There may be a confounding varible causing the variance.","05a57c8b":"Coming to`'Age'`","39977565":"It appears that the data contains no information of the reports in the year 2006 after September. The above plot also seems to suggest that the most number of reports were recorded in January and that the number reduces month by month.","512fdb6f":"Similarly, checking for policies that occur five times.","6dac8069":"from sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\nGBR.fit(x_train, y_train)\nprint('Train score:', mean_squared_error(y_pred = GBR.predict(x_test) , y_true = y_test, squared= False))\nprint('test score:', mean_squared_error(y_pred = GBR.predict(x_test) , y_true = y_test, squared= False))","24022638":"`'DependentsOther'`","c47e0f5d":"Now, `'DateReported'`","3fd55c45":"`'ClaimDescription'`","595c0d22":"`'Gender'`","c7199aba":"Therefore except one, all accidents that were reported in 2006 happend in 2005. There's one accident that happend in 2004 and was reported in 2006. On inspection, it was found that the test data also follows this pattern close enough.","da377f8c":"Since there are only 168 hours per week some points are outliers. The proportion of outliers lying beyond the $1^{st}$ and the $99^{th}$ percentiles can be seen below. Will need to cap the percentiles by imputing the outliers with the median values. ","7d9f4490":"## Data Loading","6b8e4b27":"Each row doesn't correspond to a unique claim. ","7b1e6f08":"However, it can be seen below that points that lie below the first percentile can't really be treated as outliers as they are the points belonging \n1) A relatively younger group (since the mean and median are around 25) and thus maybe people who aren't working in a full time job  \n2) Most of them are Part time employees  \n3) Most of them work few days a week (Both mean and median are around 2 days per week)  \n4) Most of them earn very low weekly wages\n","78beb656":"## Applying to test data","a95c0a2d":"Most are childless.","a14feb01":"On inspecting the above and certain sample values of each column, it's clear that you need to change datatypes of  `'DateTimeOfAccident',  'DateReported', 'Age', 'DependentChildren', 'WeeklyWages', 'DependentsOther', 'HoursWorkedPerWeek', 'InitialIncurredCalimsCost', 'UltimateIncurredClaimCost'`","c2916b22":"Will need to study each column and how they relate to each other before building a machine learning model.","efdfcdda":"## Data Processing","703c0d46":"`'DependentChildren'`  ","53bb1a26":"random forest","1df26619":"`'InitialIncurredCalimsCost'`"}}