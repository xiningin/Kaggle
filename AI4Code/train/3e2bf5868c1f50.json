{"cell_type":{"6c5678e9":"code","f5f9b749":"code","4562866e":"code","e84f676d":"code","3661ae69":"code","002f2ba1":"code","a50bdbfb":"code","57f639f3":"code","0a5dc779":"code","8bc40a3c":"code","151acdf2":"code","c29cd347":"code","8d4a6df2":"code","40bba96c":"code","f0f95ec0":"code","d3fb7816":"code","9cb484a7":"code","c6fc2bf0":"code","ff5a9067":"code","638b9d27":"code","f35769c9":"code","52e2e7a5":"code","4145eed2":"code","36a3c88b":"code","c8f7c326":"code","9e95ac91":"code","bd7f8f93":"code","dc34855d":"code","dfe11233":"code","aa15e238":"code","d9a8563a":"code","bdd4385b":"code","33cc97f2":"markdown","cb890ca6":"markdown","0d35c9e4":"markdown","39bf783f":"markdown","a9b022df":"markdown","5f880cc6":"markdown","2d7376d9":"markdown","9e305a56":"markdown","d5830eaf":"markdown"},"source":{"6c5678e9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\", {'axes.grid' : False})\nsns.set_palette(\"bright\")\nimport torch.nn\nimport random\n","f5f9b749":"#Loading the Data\ndf = pd.read_json(\"data\/train.json\")\ndf_test = pd.read_json(\"data\/test.json\")\nprint(len(df))\nprint(len(df_test))\ndf.head()\n","4562866e":"#Different Cuisines present and their counts\ndf[\"cuisine\"].value_counts()","e84f676d":"ingredients = df[\"ingredients\"].tolist()\ntest_ingredients = df_test[\"ingredients\"].tolist()\ningredients = ingredients + test_ingredients\n\n# The Vector space has to include all ingredients from both Train and Test","3661ae69":"ingredients_dict = {}\nfor recipe in ingredients:\n    for ingredient in recipe:\n        ingredients_dict[ingredient] = ingredients_dict.get(ingredient,0)+ 1\n\ning_df = pd.DataFrame(data = ingredients_dict.values(),index = ingredients_dict.keys(),columns = [\"Counts\"])\ning_df.sort_values([\"Counts\"],ascending = False, inplace = True)\ning_df","002f2ba1":"stopwords= [\"fresh\",\"chopped\",\"large\",\"all-purpose\",\"grated\",\"freshly\",\"crushed\",\"minced\",\"skinless\"\n           \"sodium\",\"low\",\"diced\",\"unsalted\",\"coarse\",\"low-fat\",\"medium\",\"powdered\",\"finely\",\"fine\",\n           \"pitted\",\"plain\",\"low-fat\",\"full-fat\",\"nonfat\",\"fat-free\"]\ndef find_occurence(word,recipe_list): \n    #Utility function to check if an ingredient is present in the list of recipes\n    result = {}\n    for recipe in recipe_list:\n        for ingredient in recipe:\n            if word in ingredient:\n                result[ingredient] = result.get(ingredient,0) + 1\n    return list(result.keys())\n\ningredients2 = []\nfor index,i in enumerate(ingredients):\n    recipe = []\n    for j in i:\n        ing_word = j.split(\" \")\n        ing_word = [i for i in ing_word if i not in stopwords]\n        recipe.append(\" \".join(ing_word))\n    ingredients2.append(recipe)\ningredients = ingredients2[:]\n\ningredients_dict2 = {}\nfor recipe in ingredients:\n    for ingredient in recipe:\n        ingredients_dict2[ingredient] = ingredients_dict2.get(ingredient,0)+ 1\ning_df = pd.DataFrame(data = ingredients_dict2.values(),index = ingredients_dict2.keys(),columns = [\"Counts\"])\ning_df.sort_values([\"Counts\"],ascending = False, inplace = True)\ndf[\"ingredients\"]= ingredients[:len(df)] #Append the \"cleaned\" list of ingredients to the dataframe\ningredients_map = {k:v for k,v in zip(ing_df.index,range(len(ing_df)))}\n\ndef convert_recipe(recipe):\n    '''\n    Convert Recipe from a List of String Ingredients to a Vector\n    recipe: List of Ingredients\n    output: 7137x1 Vector\n    '''\n    output = np.zeros(7137)\n    for ingredient in recipe:\n        output[ingredients_map[ingredient]] = 1\n    return output\n    \ndf[\"Vector\"] = df[\"ingredients\"].apply(convert_recipe) # Convert each recipe to a OHE Sparse Vector Form","a50bdbfb":"#Store all the vectors as a Matrix of M x 7137\nmat = list(df[\"Vector\"])\nmat = np.array(mat)\nmat.shape","57f639f3":"from sklearn.decomposition import PCA\npca = PCA(32)\nmat_32 = pca.fit_transform(mat)","0a5dc779":"from sklearn.svm import SVC\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(mat_32,df['cuisine'],\n                                                    test_size=0.30)\nsvc = SVC(kernel = \"rbf\")\nsvc.fit(X_train,y_train)","8bc40a3c":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\npredictions = svc.predict(X_test)\nprint(classification_report(y_test,predictions))\ncr = classification_report(y_test,predictions,output_dict= True)","151acdf2":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset","c29cd347":"df = pd.concat([df,df_test]) # This is to train the Language Model on the entire set of ingredients\n\nstopwords= [\"fresh\",\"chopped\",\"large\",\"all-purpose\",\"grated\",\"freshly\",\"crushed\",\"minced\",\"skinless\"\n           \"sodium\",\"low\",\"diced\",\"unsalted\",\"coarse\",\"low-fat\",\"medium\",\"powdered\",\"finely\",\"fine\",\n           \"pitted\",\"plain\",\"low-fat\",\"full-fat\",\"nonfat\",\"fat-free\"]\ndef find_occurence(word,recipe_list):\n    result = {}\n    for recipe in recipe_list:\n        for ingredient in recipe:\n            if word in ingredient:\n                result[ingredient] = result.get(ingredient,0) + 1\n    return list(result.keys())\n\ningredients2 = []\nfor index,i in enumerate(ingredients):\n    recipe = []\n    for j in i:\n        ing_word = j.split(\" \")\n        ing_word = [i for i in ing_word if i not in stopwords]\n        recipe.append(\" \".join(ing_word))\n    ingredients2.append(recipe)\ningredients = ingredients2[:]\n\ningredients_dict2 = {}\nfor recipe in ingredients2:\n    for ingredient in recipe:\n        ingredients_dict2[ingredient] = ingredients_dict2.get(ingredient,0)+ 1\ning_df = pd.DataFrame(data = ingredients_dict2.values(),index = ingredients_dict2.keys(),columns = [\"Counts\"])\ning_df.sort_values([\"Counts\"],ascending = False, inplace = True)\ndf[\"ingredients\"]= ingredients[:len(df)] #Append the \"cleaned\" list of ingredients to the dataframe\ningredients_map = {k:v for k,v in zip(ing_df.index,range(len(ing_df)))}\n\ndef convert_recipe(recipe):\n    '''\n    Convert Recipe from a List of String Ingredients to a Vector\n    recipe: List of Ingredients\n    output: 7137x1 Vector\n    '''\n    output = np.zeros(7137)\n    for ingredient in recipe:\n        output[ingredients_map[ingredient]] = 1\n    return output\n    \ndf[\"Vector\"] = df[\"ingredients\"].apply(convert_recipe) # Convert each recipe to a OHE Sparse Vector Form","8d4a6df2":"#Helper Functions\nCONTEXT_SIZE = 5\n#The sampling function to get context words from a recipe\ndef sample(recipe,ingredient,samples):\n    '''\n    Input: Recipe -> List of Ingredients\n           Ingredient -> String \n           Samples: Int for number of samples\n    Output: List of Strings\n    '''\n    recipe = recipe[:] #Copy the recipe to prevent alteration\n    recipe.remove(ingredient)\n    if len(recipe) < CONTEXT_SIZE+1:\n        context = random.choices(recipe, k=samples) #With Replacement when they are insufficient ingredients in the recipe\n    else: \n        context = random.sample(recipe, k=samples) #Without Replacement \n    return context\ndef createTensor(context):\n    idxs = [ingredients_map[w] for w in context]\n    return torch.tensor(idxs, dtype=torch.long)\nclass RecipeDataset(Dataset):\n    def __init__(self, samples):\n        self.samples = samples\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        return self.samples[idx]","40bba96c":"def loadData(dataCol,batch_size):\n    samples = []\n    for recipe in dataCol:\n        for ingredient in recipe:\n            if len(recipe) > 2:\n                samples.append([sample(recipe,ingredient,CONTEXT_SIZE),ingredient])\n    samples_idx = []\n    for context,target in samples:\n        target_idx = (torch.tensor([ingredients_map[target]], dtype=torch.long))\n        samples_idx.append([createTensor(context),target_idx])\n    \n    sample_DS = RecipeDataset(samples_idx)\n    print(samples_idx)\n    train_loader = DataLoader(dataset=sample_DS, batch_size=batch_size, shuffle=True)\n    return train_loader\n","f0f95ec0":"train_loader= loadData(df[\"ingredients\"],64)","d3fb7816":"VOCAB_SIZE = len(ingredients_dict2)\nEMBED_DIM = 32\n\nclass CBOWModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, context_size):\n        super(CBOWModel, self).__init__()\n        self.context_size = context_size\n        self.embedding_dim = embedding_dim\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim) #Transform to Lower Dimension Embeddings\n        self.linear1 = nn.Linear(embedding_dim, 128)\n        self.linear2 = nn.Linear(128, vocab_size)\n        \n    def forward(self, inputs):\n        embeds = self.embeddings(inputs).sum(dim = 1) #Sum over all the context\n        out = F.relu(self.linear1(embeds))\n        out = self.linear2(out)\n        return out\n    def getEmbeds(self,wordIdx):\n        return self.embeddings(wordIdx)\n        \ncbow = CBOWModel(VOCAB_SIZE,EMBED_DIM,CONTEXT_SIZE)","9cb484a7":"#Grid Search to find Ideal Learning Rate \n\nl_rs = [0.1,0.25,0.5,1,2]\n#Code to find ideal learning Rate for the model\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ntorch.cuda.set_device(0)\ncbow.to(device)\nloss_dict = {}\nnum_epochs = 25\nBATCH_SIZE = 64\ntrain_loader= loadData(df[\"ingredients\"],BATCH_SIZE)\ncriterion = torch.nn.CrossEntropyLoss()\n\n\nfor l_r in l_rs:\n    losses = []\n    cbow = CBOWModel(VOCAB_SIZE,EMBED_DIM,CONTEXT_SIZE)\n    cbow.to(device)\n    optimizer = torch.optim.SGD(cbow.parameters(), lr=l_r)\n    iter_ = 0\n    print(\"Learning Rate:\",l_r)\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for i,(context,target) in enumerate(train_loader):\n            \n            context = context.to(device)\n            target =target.to(device)\n            optimizer.zero_grad()\n            outputs = cbow(context)\n            target = target.view(-1)\n            #print(target.size(),outputs.size())\n            loss = criterion(outputs,target)\n            loss.backward()\n            optimizer.step()\n            iter_  += 1\n            total_loss +=loss.item()\n            if iter_%2500 ==0:\n                print(\"Progress:\",iter_\/(num_epochs*len(train_loader))*100,\"%\")\n    \n        losses.append(total_loss)\n    loss_dict[l_r] = losses\n","c6fc2bf0":"for l_r,losses in loss_dict.items():\n    plt.plot(losses,label = l_r)\nplt.legend()","ff5a9067":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncbow = CBOWModel(VOCAB_SIZE,EMBED_DIM,CONTEXT_SIZE)\ncbow.to(device)\nlosses = []\nnum_epochs = 80\niter_ = 0\nBATCH_SIZE = 128\nlearning_rate = 1\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(cbow.parameters(), lr=learning_rate)  \nfor epoch in range(num_epochs):\n    total_loss = 0\n    if epoch%4 == 0:\n        train_loader = loadData(df[\"ingredients\"],BATCH_SIZE)\n    for i,(context,target) in enumerate(train_loader):\n        context = context.to(device)\n        target =target.to(device)\n        optimizer.zero_grad()\n        outputs = cbow(context)\n        target = target.view(-1)\n        loss = criterion(outputs,target)\n        loss.backward()\n        optimizer.step()\n        iter_  += 1\n        total_loss +=loss.item()\n        if iter_%500 ==0:\n            print(\"Progress:\",iter_\/(num_epochs*len(train_loader))*100,\"%\")\n    \n    losses.append(total_loss)\n\nprint(losses)","638b9d27":"plt.plot(losses)\n\n","f35769c9":"cbow.to(\"cpu\")\ndf = df[df[\"cuisine\"].notna()]\ning_tensor_list = df[\"ingredients\"].apply(createTensor).tolist()\nembed_array = []\nfor recipe in ing_tensor_list:\n    embeds = cbow.getEmbeds(recipe)\n    embeds = embeds.mean(axis = 0).detach().numpy()\n    embed_array.append(embeds)","52e2e7a5":"from sklearn.manifold import TSNE","4145eed2":"embed_array = np.array(embed_array)\nnlp_vis = TSNE(n_components= 2,perplexity= 40).fit_transform(embed_array)","36a3c88b":"set1 = list(df[\"cuisine\"].value_counts().index[:10])\nset2 = list(df[\"cuisine\"].value_counts().index[10:])","c8f7c326":"mat_vis =TSNE(n_components= 2,perplexity= 40).fit_transform(mat_32)","9e95ac91":"sns.set_style(\"dark\")\nfig,ax = plt.subplots(figsize = (12,8))\nvis_df  = {\"x\":nlp_vis[:,0],\"y\" : nlp_vis[:,1],\"target\" : df[\"cuisine\"]}\nvis_df = pd.DataFrame(vis_df)\nvis_df = vis_df[vis_df[\"target\"].isin(set1)]\nsns.scatterplot(data  = vis_df,x = \"x\",y=\"y\",hue = \"target\",alpha = 0.75,palette=sns.color_palette(\"bright\"))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n","bd7f8f93":"fig,ax = plt.subplots(figsize = (12,8))\nvis_df  = {\"x\":nlp_vis[:,0],\"y\" : nlp_vis[:,1],\"target\" : df[\"cuisine\"]}\nvis_df = pd.DataFrame(vis_df)\nvis_df = vis_df[vis_df[\"target\"].isin(set2)]\nsns.scatterplot(data  = vis_df,x = \"x\",y=\"y\",hue = \"target\",alpha = 0.75,palette=sns.color_palette(\"bright\"))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","dc34855d":"fig,ax = plt.subplots(figsize = (12,8))\nvis_df  = {\"x\":mat_vis[:,0],\"y\" : mat_vis[:,1],\"target\" : df[\"cuisine\"]}\nvis_df = pd.DataFrame(vis_df)\nvis_df = vis_df[vis_df[\"target\"].isin(set1)]\nsns.scatterplot(data  = vis_df,x = \"x\",y=\"y\",hue = \"target\",alpha = 0.75,palette=sns.color_palette(\"bright\"))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","dfe11233":"fig,ax = plt.subplots(figsize = (12,8))\nvis_df  = {\"x\":mat_vis[:,0],\"y\" : mat_vis[:,1],\"target\" : df[\"cuisine\"]}\nvis_df = pd.DataFrame(vis_df)\nvis_df = vis_df[vis_df[\"target\"].isin(set2)]\nsns.scatterplot(data  = vis_df,x = \"x\",y=\"y\",hue = \"target\",alpha = 0.75,palette=sns.color_palette(\"bright\"))\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","aa15e238":"\ndef getRecipeEmbedding(OH_tensor):\n    return cbow.getEmbeds(OH_tensor).mean(axis =0).detach().numpy()\ndf[\"Tensor\"] = df[\"ingredients\"].apply(createTensor)\ndf[\"Embeds\"] = df[\"Tensor\"].apply(getRecipeEmbedding)","d9a8563a":"mat_embeds = list(df[\"Embeds\"])\nmat_embeds = np.array(mat_embeds)\n\nX_train, X_test, y_train, y_test = train_test_split(mat_embeds,df['cuisine'],\n                                                    test_size=0.30)\nsvc = SVC(kernel = \"rbf\")\nsvc.fit(X_train,y_train)","bdd4385b":"predictions = svc.predict(X_test)\nprint(classification_report(y_test,predictions))\ncr = classification_report(y_test,predictions,output_dict= True)","33cc97f2":"As can be seen the model improves significantly from using the NLP embeddings. Of course higher performance can be achieved by cleaning the dataset more and optimizing the hyperparameters of the SVC but this notebook is to mainly learn about the CBOW model and how to implement it using PyTorch. Enjoy!","cb890ca6":"## Data Preprocessing\n","0d35c9e4":"Before looking at dimensionality reduction tools and \nNow that most of the preprocessing is done, traditional ML methods are used as a baseline effectiveness for the classification task so that we can compare the performance using Deep Learning.","39bf783f":"Now that we have verified that the NLP model is working well, we can use it for our classification problem","a9b022df":"# What's Cooking Kaggle Challenge\n\nLink: https:\/\/www.kaggle.com\/c\/whats-cooking\/overview <br>\nMedium Article: https:\/\/towardsdatascience.com\/whats-cooking-categorizing-cuisines-with-nlp-ac44affe6b1d <br>\nGithub: https:\/\/github.com\/reoneo97\/Whats_Cooking <br>\nIn this notebook I'll be looking at the dataset from the Kaggle What's Cooking Challenge which is a supervised problem of predicting cuisines from ingredients list. The dataset is a .json file with only 2 important columns being the cuisine as well as the ingredients. In this notebook we will be doing exploratory data analysis followed by using some Natural Language Processing techniques to solve the problem. I will be using PyTorch to implement the deep learning model to learn the vector representations of the words.\n\n## Contents: \n\n- Data Preprocessing\n- Preliminary Model\n- CBOW Model\n- Evaluation\n- Model with NLP Vectors\n\n","5f880cc6":"As can be seen from the dataframe, there are currently 7137 differnt types of ingredients present in the dataset. However many of them are repeated but have a slightly different name in the recipe. (Eg. Garlic vs Chopped Garlic). Below you can see a list of stopwords which are redundant. This results in a reduction of the number of ingredients by around 200 which is relatively sizeable and will help the model learn better later by reducing the number of obscure words.","2d7376d9":"As we can see, the accuracy of this model is quite poor and that is to be expected. By reducing the number of dimensions with PCA, we lose a lot of information which translates to the model having significantly poorer performance. ","9e305a56":"## Principal Component Analysis and Preliminary Model\nBecause of the curse of dimensionality, training a model on this vector space will take way too long. For this we first use a linear dimensionality reduction tool PCA and later in the Natural Language Processing Section we can see how this actually compares with deep non-linear methods such as using the Continuous Bag of Words model later","d5830eaf":"## Natural Language Processing\n\nNow that we have seen how something like PCA can make sense of the data, we come to the interesting part which is to try to find a better vector representation of the words in a recipe.\n\nHere we will use the Common Bag Of Words (CBOW) model in order to learn the representation of the words. The idea behind CBOW is to use the context of a word to learn what the word actually means. For example a sentence such as \"I like to eat pasta\", in order to learn the representation of the word \"eat\", we look at \"pasta\" and \"like\" as context words. In this case because the recipes are inherently unordered, the context words will be obtained randomly from the sample.\n\nThe implementation of this model is to use an autoencoder architecture where the One-Hot encoded word vectors are used in the model as inputs and encoded into the learnt representations and then the decoder will try to recreate the target word. \n\nLink: https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n\n![image.png](attachment:image.png)\n    \n\nRecipe Sampling, Randomly choose 5 other ingredients that are in the same recipe and try to use the words to learn the representation of that word"}}