{"cell_type":{"ce105d68":"code","94d1c6c9":"code","e8e12c34":"code","c4d8c110":"code","9b23fade":"code","06662dde":"code","997d0585":"code","3058c3f5":"code","ead5996d":"code","e46bf321":"code","c59b448e":"code","e12acd3a":"code","be8f5496":"code","d50e41b8":"code","1272eab7":"code","653d9ba5":"code","4032c728":"code","43cd901e":"code","e8743511":"code","e3237baf":"code","3405069e":"code","e744e3d1":"code","0c201ca4":"code","aca00141":"code","b4d13d05":"code","284daca2":"code","0d88c552":"code","f07c1979":"code","b8c2bf97":"code","2c6b5ca5":"code","2ed2cfd6":"code","7470dfed":"markdown","deaa2ac3":"markdown","328df1f5":"markdown","dd276dd0":"markdown","7c031f1f":"markdown","6ec1fe30":"markdown","94b93443":"markdown","c16ce1e1":"markdown","57c03ca6":"markdown","f4d08521":"markdown","299603ab":"markdown","42eb04fe":"markdown","3cf482c7":"markdown","6050d7c5":"markdown","1fe1dce1":"markdown","f9d4959d":"markdown","bed07bb7":"markdown","feefee54":"markdown","026584ae":"markdown","66835591":"markdown","8deaf91b":"markdown","9ecaaa83":"markdown","8ee3b9dc":"markdown","09b58f42":"markdown","559d6dde":"markdown","961c51f0":"markdown","7634c482":"markdown","05eb906c":"markdown","6a5ef14b":"markdown","253bf37e":"markdown","6808afaf":"markdown","472e4173":"markdown","a4f90ca7":"markdown","81c872c8":"markdown","eaaf933d":"markdown"},"source":{"ce105d68":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","94d1c6c9":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.info()","e8e12c34":"df.describe()","c4d8c110":"fraud = df.Class[df.Class == 1].count() \/ df.Class.count() * 100\nnon_fraud = 100 - fraud\nprint('% of fraud transactions: ', fraud)\nprint('% of non-fraud transactions: ', non_fraud)\n\nsb.countplot('Class', data=df, palette='RdBu_r')\nplt.show()","9b23fade":"sb.boxplot(x = 'Class', y ='Amount', data = df, showfliers = False)\nplt.show()","06662dde":"plt.figure(figsize = [10, 8])\nsb.pairplot(df[['Time','Amount','Class']], hue='Class', palette = 'husl')\nplt.show()","997d0585":"df['Amount'] = RobustScaler().fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time'] = RobustScaler().fit_transform(df['Time'].values.reshape(-1,1))","3058c3f5":"y = df.Class\nX = df.drop('Class', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","ead5996d":"tr_fraud = y_train[y_train == 1].count() \/ y_train.count() * 100\ntr_non_fraud = 100 - tr_fraud\n\nprint('% of train fraud transactions: ', tr_fraud)\nprint('% of train non-fraud transactions: ', tr_non_fraud)\nprint('\\n')\n\nte_fraud = y_test[y_test == 1].count() \/ y_test.count() * 100\nte_non_fraud = 100 - te_fraud\nprint('% of test fraud transactions: ', te_fraud)\nprint('% of test non-fraud transactions: ', te_non_fraud)","e46bf321":"y_baseline = y_test.copy()\ny_baseline[:] = 0\n\nprint('Accuracy:', metrics.accuracy_score(y_test, y_baseline))\nprint('Recall:', metrics.recall_score(y_test, y_baseline))\nprint('Precision:', metrics.precision_score(y_test, y_baseline))\nprint('f1-score:', metrics.f1_score(y_test, y_baseline))","c59b448e":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train, y_train)\ny_pred_lr = log_reg.predict(X_test)","e12acd3a":"print('Accuracy:', metrics.accuracy_score(y_test, y_pred_lr))\nprint('Recall:', metrics.recall_score(y_test, y_pred_lr))\nprint('Precision:', metrics.precision_score(y_test, y_pred_lr))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_lr))","be8f5496":"log_reg_cw = LogisticRegression(solver='liblinear', class_weight='balanced')\nlog_reg_cw.fit(X_train, y_train)\ny_pred_lr_cw = log_reg_cw.predict(X_test)\n\nprint('Accuracy:', metrics.accuracy_score(y_test, y_pred_lr_cw))\nprint('Recall:', metrics.recall_score(y_test, y_pred_lr_cw))\nprint('Precision:', metrics.precision_score(y_test, y_pred_lr_cw))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_lr_cw))","d50e41b8":"rand_forst = RandomForestClassifier(n_estimators=50, random_state=0)\nrand_forst.fit(X_train, y_train)\ny_pred_rf = rand_forst.predict(X_test)","1272eab7":"print('Accuracy:', metrics.accuracy_score(y_test, y_pred_rf))\nprint('Recall:', metrics.recall_score(y_test, y_pred_rf))\nprint('Precision:', metrics.precision_score(y_test, y_pred_rf))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_rf))","653d9ba5":"#y_pred = cross_val_predict(rand_forst, X_train, y_train)\nsb.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, cmap='Greens')\nplt.xlabel('Modeled counts')\nplt.ylabel('True counts')\nplt.show()","4032c728":"df_train = pd.concat([X_train, y_train], axis=1)\n\nfraid_tr = df_train[df_train.Class == 1]\nnon_fraid_tr = df_train[df_train.Class == 0]\n\nfraud_tr_os = resample(fraid_tr, n_samples=len(non_fraid_tr), replace=True, random_state=0)\n\ndf_tr_os = pd.concat([fraud_tr_os, non_fraid_tr])","43cd901e":"fraud = fraud_tr_os.Class.count() \/ df_tr_os.Class.count() * 100\nnon_fraud = 100 - fraud\nprint('% of fraud transactions: ', fraud)\nprint('% of non-fraud transactions: ', non_fraud)\n\nsb.countplot('Class', data=df_tr_os, palette='hls')\nplt.show()","e8743511":"plt.figure(figsize = [10, 8])\nsb.heatmap(df_tr_os.corr(), vmin=-1, vmax=1, cmap = 'RdBu_r') #annot=True\nplt.show()","e3237baf":"plt.figure(figsize = [10, 8])\nsb.pairplot(df_tr_os[['V4','V11','V12','V14','Class']], hue='Class', palette = 'husl')\nplt.show()","3405069e":"fig, axis = plt.subplots(1, 3,figsize=(15,5))\nsb.boxplot(x = 'Class', y ='V4', data = df_tr_os, ax = axis[0], showfliers = False, palette = 'hls')\nsb.boxplot(x = 'Class', y ='V11', data = df_tr_os, ax = axis[1], showfliers = False, palette = 'RdBu_r')\nsb.boxplot(x = 'Class', y ='V14', data = df_tr_os, ax = axis[2], showfliers = False)\nplt.show()","e744e3d1":"X_tr = df_tr_os.drop('Class', axis=1)\ny_tr = df_tr_os.Class\n\nlog_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_tr, y_tr)\ny_pred_lr_os = log_reg.predict(X_test)\n\nprint('Accuracy:', metrics.accuracy_score(y_test, y_pred_lr_os))\nprint('Recall:', metrics.recall_score(y_test, y_pred_lr_os))\nprint('Precision:', metrics.precision_score(y_test, y_pred_lr_os))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_lr_os))","0c201ca4":"rand_forst = RandomForestClassifier(n_estimators=50, random_state=0)\nrand_forst.fit(X_tr, y_tr)\ny_pred_rf_os = rand_forst.predict(X_test)\n\nprint('Accuracy:', metrics.accuracy_score(y_test, y_pred_rf_os))\nprint('Recall:', metrics.recall_score(y_test, y_pred_rf_os))\nprint('Precision:', metrics.precision_score(y_test, y_pred_rf_os))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_rf_os))","aca00141":"sb.heatmap(confusion_matrix(y_test, y_pred_rf_os), annot=True, cmap='RdPu')\nplt.xlabel('Modeled counts')\nplt.ylabel('True counts')\nplt.show()","b4d13d05":"smote = SMOTE(sampling_strategy=1.0, random_state=0)\nX_train_sm, y_train_sm = smote.fit_sample(X_train, y_train)","284daca2":"fraud = y_train_sm[y_train_sm == 1].count() \/ y_train_sm.count() * 100\nnon_fraud = 100 - fraud\nprint('% of fraud transactions: ', fraud)\nprint('% of non-fraud transactions: ', non_fraud)","0d88c552":"log_reg = LogisticRegression(solver='liblinear')\nlog_reg.fit(X_train_sm, y_train_sm)\ny_pred_lr_sm = log_reg.predict(X_test)\n\nprint('Accuracy:', metrics.accuracy_score(y_test, y_pred_lr_sm))\nprint('Recall:', metrics.recall_score(y_test, y_pred_lr_sm))\nprint('Precision:', metrics.precision_score(y_test, y_pred_lr_sm))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_lr_sm))","f07c1979":"rand_forst = RandomForestClassifier(n_estimators=50, random_state=0)\nrand_forst.fit(X_train_sm, y_train_sm)\ny_pred_rf_sm = rand_forst.predict(X_test)\n\nprint('Accuracy:', metrics.accuracy_score(y_test, y_pred_rf_sm))\nprint('Recall:', metrics.recall_score(y_test, y_pred_rf_sm))\nprint('Precision:', metrics.precision_score(y_test, y_pred_rf_sm))\nprint('f1-score:', metrics.f1_score(y_test, y_pred_rf_sm))","b8c2bf97":"sb.heatmap(confusion_matrix(y_test, y_pred_rf_sm), annot=True, cmap='Purples')\nplt.xlabel('Modeled counts')\nplt.ylabel('True counts')\nplt.show()","2c6b5ca5":"print('Logistic Regression: ', roc_auc_score(y_test, y_pred_lr))\nprint('Improved Logistic Regression: ', roc_auc_score(y_test, y_pred_lr_cw))\nprint('Random Forest: ', roc_auc_score(y_test, y_pred_rf))\nprint('Logistic Regression after Oversampling: ', roc_auc_score(y_test, y_pred_lr_os))\nprint('Random Forest after Oversampling: ', roc_auc_score(y_test, y_pred_rf_os))\nprint('Logistic Regression after SMOTE: ', roc_auc_score(y_test, y_pred_lr_sm))\nprint('Random Forest after SMOTE: ', roc_auc_score(y_test, y_pred_rf_sm))\n","2ed2cfd6":"fpr_lr, tpr_lr, thr_lr          = roc_curve(y_test, y_pred_lr)\nfpr_lr_os, tpr_lr_os, thr_lr_os = roc_curve(y_test, y_pred_lr_os)\nfpr_lr_sm, tpr_lr_sm, thr_lr_sm = roc_curve(y_test, y_pred_lr_sm)\nfpr_rf, tpr_rf, thr_rf          = roc_curve(y_test, y_pred_rf)\nfpr_rf_os, tpr_rf_os, thr_rf_os = roc_curve(y_test, y_pred_rf_os)\nfpr_rf_sm, tpr_rf_sm, thr_rf_sm = roc_curve(y_test, y_pred_rf_sm)\n\nplt.figure(figsize=(10,8))\nplt.plot(fpr_lr, tpr_lr,       label='Logistic regression')\nplt.plot(fpr_lr_os, tpr_lr_os, label='Logistic regression after oversampling')\nplt.plot(fpr_lr_sm, tpr_lr_sm, label='Logistic regression after SMOTE')\nplt.plot(fpr_rf, tpr_rf,       label='Random forest')\nplt.plot(fpr_rf_os, tpr_rf_os, label='Random forest after oversampling')\nplt.plot(fpr_rf_sm, tpr_rf_sm, label='Random forest after SMOTE')\nplt.plot([0, 1], [0, 1], 'k:')\n\nplt.xlim([-0.05, 1])\nplt.ylim([0, 1])\nplt.legend(fontsize=12)\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.show()","7470dfed":"### 1. Defining a baseline:\n\nHere, the baseline is a naive model that always gives non-fraud as the answer!\nWhen we implement an MLA, our final result should be much better than the baseline; otherwise our model is useles.","deaa2ac3":"### 4. Changing the MLAs (Here, Random Forest)\n\nSome MLAs are not proper for imbalanced data (e.g. logistic regresison), but some others perform better on both balanced and imbalanced data (decision tree, and random forest).","328df1f5":"Although recall, precision, and f1-ssocre are not ideal, random forest provided the highest scores, so far.","dd276dd0":"For the Amount and Class, the data description makes sense, and the mean, min, and max values are reasonable. For example, there is no negative value.","7c031f1f":"### Random Forest after Oversampling:","6ec1fe30":"## Resampling the data:\n\n**Important:**\n\nTo avoid overfitting, we need to split the data into test and train sets BEFORE resampling. Resampling should be performed on train set, NOT test set.","94b93443":"Similar to oversampling, SMOTE has improved the recall significantly, but at the expense of lowering the precision.","c16ce1e1":"### Logistic Regression after Oversampling:","57c03ca6":"### Changing the performance metrics.\n\nWhen working with an imbalanced data, accuracy is very misleading. Instead, we use recall, precision, and f1-score. ","f4d08521":"- The result of confusion matrix is good when the values on the upper left and lower right (True negative and true positive, respectively) are high and other values are low, which is the case here.\n- This verifies that there is no systematic error that our MLA always predicts non-fraud when there is fraud.\n- False negative is not ideal (~ one fifth of all negatives), which is why recall is not too high.","299603ab":"### Scaling Amount and Time:","42eb04fe":"# Unbalanced Classification for Fraud Detection\n\n## Objective\n\nThe input dataset contains features such as credit card transaction time, amount, and other hidden features.\nThe outcome is class (fraud or not).\n\nUsing machine learning, we will predict whether a transaction is fraud or not.\n\nIn particular, we need to deal with an imbalanced dataset. To do so, we consider these approaches:\n- Collecting more data, if possible\n- Changing the performance metrics (precision, recall, f1-score, ...)\n- Resampling the data (oversampling, undersampling, SMOTE, ...)\n- Changing the MLAs\n\n","3cf482c7":"There seems to be no significant difference in the amount of transaction between non-fraud and fraud transactions.","6050d7c5":"### Summary: ROC Curve","1fe1dce1":"## Implementing MLAs:\n\n### Spliting the data into test and train sets:\n\n- We use basic split method, which is \"train_test_split\".\n- Since the data is imbalanced, there is a risk that the ratio of fraud to non-fraud changes by using \"train_test_split\". What if there is no fraud case in test set?\n- To address this concern, we calculate the percentage of fraud and non-fraud cases in test and train sets. Fortunately, the percentage of classes is unchanged in test and train sets.\n- Alternatively, you can use stratified sampling.","f9d4959d":"We see that the outcome Class is strongly dependent of features such as V4, V11, V12, and V14.","bed07bb7":"### No Feature Engineering:\n\nTo keep the privacy of customers, the input data data was scaled and the feature names were hidden. So, we have no information about the nature of features, and feature engineering is not practical in this project.","feefee54":"- Oversampling slightly improves all three scores (recall, precision, f1-score).\n- The confusion matrix shows that oversampling decreases the false negatives:","026584ae":"No gap or missing value exists in any variable.","66835591":"## Data Cleaning","8deaf91b":"Recall is improved at the expense of lowering precision (which is close to zero); so this is not satisfactory.","9ecaaa83":"- The number of fraud cases are increased and is now equal to the number of non-fraud cases.\n- The balanced dataset shows higher correlation between some variables, compared to imbalanced dataset. For example, V4 and V11 show relatively high positive correlation with Class, whereas V12 and V14 show relatively high negative correlation with Class:","8ee3b9dc":"### a) Oversampling:\n\n- Oversampling increases the number of cases of minority class by adding copies of them to the dataset.\n- If you do not have a huge dataset (of the order of million rows), then oversampling might work for you.","09b58f42":"### 2. Implementing Logistic Regression:","559d6dde":"### c) SMOTE\n\n- SMOTE or Synthetic Minority Oversampling Technique uses the k-nearest neighbors technique to create synthetic and new data points.","961c51f0":"- Compared to oversampling, SMOTE slightly improves recall, but precision slightly decreases.\n- Compared to oversampling, the confusion matrix shows that SMOTE slightly decreases the false negatives, but slightly increases false posivies.","7634c482":"### b) Undersampling\n\n- Another resampling method is undersampling, which removes some data from the majority class.\n- Pros: works well when we have a huge dataset (e.g. of the order of million rows).\n- cons: we are losing valuable informaiton by removing data points, and this might cause underfitting.\n- Therefore, we don't explore undersampling in this project.","05eb906c":"- Among resampling methods, SMOTE leads to highest TPR.\n- Among MLAs, ramdom forest has lower FPR, but logistic regression has higher TPR.\n- The trade-off between higher TPR and lower FPR depends on the project objective. What percentage of false positives is acceptable? \n- In the case of fraud detection, it is not desirable to increase true positive rate at the expense of increasing false positive rate. We do not want our MLA to falsely detect many transactions as fraud!\n- Therefore, I am willing to select random forest after SMOTE as the best MLA. Note that SMOTE method is computationally-expensive, so if you have a huge dataset, you might want to use random forest after oversampling or undersampling.","6a5ef14b":"### Confusion Matrix for Random Forest:","253bf37e":"The oversampling has improved the recall significantly, but at the expense of lowering the precision.","6808afaf":"### Logistic Regression after SMOTE:","472e4173":"The imbalanced dataset causes low recall, precision, and f1-score.","a4f90ca7":"## Exploratory Data Analysis","81c872c8":"### Random Forest after SMOTE:","eaaf933d":"### 3. Improved Logistic Regression:\n\nWe will use the hyperparameter class_weight, so that our MLA will be more appropriate for imbalanced data."}}