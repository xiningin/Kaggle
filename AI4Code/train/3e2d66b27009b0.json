{"cell_type":{"56f90041":"code","25221db9":"code","b63c1858":"code","50c55b35":"code","68979522":"code","3d6a2ac8":"code","499c0937":"code","652aa1c2":"code","85fac35d":"code","5f585b8f":"code","328a8d95":"code","be30aff9":"code","ba971a05":"code","374a73dc":"code","6f96b08b":"code","1f498b26":"code","ddf25e53":"code","a086f61f":"code","ad33c595":"code","2d512785":"code","c2dca434":"code","aed6c5f2":"code","122c6dfc":"code","eedd2a71":"code","ae591867":"code","158d3a2d":"code","0b3a9fdc":"code","36b24010":"code","651ccdef":"code","3d93c381":"code","0182a028":"code","9f4944e2":"code","046428e1":"code","506f4038":"code","e699b5d5":"code","fc1a2487":"code","3fdf71c4":"code","7706e3d3":"code","c70d62f9":"code","bf02070e":"code","d455b193":"code","cb92cf5d":"code","ae157cb0":"code","82bfd16e":"code","7456f851":"code","34488c96":"code","b1b21928":"markdown","3f5d51f8":"markdown","0996997a":"markdown","9798a1ae":"markdown","9c03dcb8":"markdown","40b1e0a8":"markdown","59206f9f":"markdown","afcc2b2f":"markdown","09d4b878":"markdown","12d8fc88":"markdown","06452aa4":"markdown","405d5730":"markdown","84fad642":"markdown","30a0dc0b":"markdown","13798ff6":"markdown","14059ec9":"markdown","34d80353":"markdown","c2019b72":"markdown","6e83f90d":"markdown","8e502814":"markdown","fb290001":"markdown","dc4ff55b":"markdown","15d8df5a":"markdown","c64a0cb3":"markdown","2f2d2fb6":"markdown","49df0261":"markdown","659bbe0c":"markdown","fd9aab10":"markdown"},"source":{"56f90041":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","25221db9":"items = pd.read_csv('..\/input\/items.csv')\nshops = pd.read_csv('..\/input\/shops.csv')\ncats = pd.read_csv('..\/input\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/sales_train.csv')\n# set index to ID to avoid droping it later\ntest  = pd.read_csv('..\/input\/test.csv').set_index('ID')","b63c1858":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","50c55b35":"train = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1001]","68979522":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median","3d6a2ac8":"# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","499c0937":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\ncats['split'] = cats['item_category_name'].str.split('-')\ncats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats['type_code'] = LabelEncoder().fit_transform(cats['type'])\n# if subtype is nan then type\ncats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id','type_code', 'subtype_code']]\n\nitems.drop(['item_name'], axis=1, inplace=True)","652aa1c2":"len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test)","85fac35d":"ts = time.time()\nmatrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)\ntime.time() - ts","5f585b8f":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","328a8d95":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))\ntime.time() - ts","be30aff9":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","ba971a05":"ts = time.time()\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\ntime.time() - ts","374a73dc":"ts = time.time()\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\ntime.time() - ts","6f96b08b":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","1f498b26":"ts = time.time()\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\ntime.time() - ts","ddf25e53":"ts = time.time()\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","a086f61f":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","ad33c595":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","2d512785":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","c2dca434":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","aed6c5f2":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","122c6dfc":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","eedd2a71":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","ae591867":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","158d3a2d":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","0b3a9fdc":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","36b24010":"ts = time.time()\ngroup = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\ngroup = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n\ndef select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    \nmatrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)\n\n# https:\/\/stackoverflow.com\/questions\/31828240\/first-non-null-value-per-row-from-a-list-of-pandas-columns\/31828559\n# matrix['price_trend'] = matrix[['delta_price_lag_1','delta_price_lag_2','delta_price_lag_3']].bfill(axis=1).iloc[:, 0]\n# Invalid dtype for backfill_2d [float16]\n\nfetures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    fetures_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    fetures_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(fetures_to_drop, axis=1, inplace=True)\n\ntime.time() - ts","651ccdef":"ts = time.time()\ngroup = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)\ntime.time() - ts","3d93c381":"matrix['month'] = matrix['date_block_num'] % 12","0182a028":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","9f4944e2":"ts = time.time()\ncache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num         \ntime.time() - ts","046428e1":"ts = time.time()\ncache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num         \ntime.time() - ts","506f4038":"ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","e699b5d5":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\ntime.time() - ts","fc1a2487":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","3fdf71c4":"matrix.columns","7706e3d3":"matrix.info()","c70d62f9":"matrix.to_pickle('data.pkl')\ndel matrix\ndel cache\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","bf02070e":"data = pd.read_pickle('data.pkl')","d455b193":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]","cb92cf5d":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","ae157cb0":"del data\ngc.collect();","82bfd16e":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","7456f851":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","34488c96":"plot_features(model, (10,14))","b1b21928":"#1","3f5d51f8":"Number of days in a month. There are no leap years.","0996997a":"# Part 1, perfect features","9798a1ae":"\uac01\uac01 \ub370\uc774\ud130\ub4e4\uc758 \ud55c\uac1c\uc529 \ud2b9\uc774\ud55c \ucf00\uc774\uc2a4\uac00 \uc874\uc7ac\ud568\uc744 \uc54c \uc218 \uc788\uc5c8\ub2e4.","9c03dcb8":"## Monthly sales\ntest set\ub294 34\uac1c\uc6d4 \uc774\ub0b4\uc5d0 \uc77c\ubd80 \uc0c1\uc810\uacfc \uc77c\ubd80 \ud488\ubaa9\uc758 \uc81c\ud488\uc774\ub2e4. 5100\uac1c\uc758 \uc544\uc774\ud15c\uc774 \uc788\ub2e4 * 42\uac1c\uc758 \uc20d = 214200\uc30d\uc774\ub2e4. train set\uc640 \ube44\uad50\ud558\uba74 363\uac1c \ud488\ubaa9\uc774 \uc0c8\ub85c \ub4e4\uc5b4\uc628\ub2e4. \ub530\ub77c\uc11c test set \ub300\uc0c1 \uac12\uc758 \ub300\ubd80\ubd84\uc5d0 \ub300\ud574\uc11c\ub294 0\uc774 \ub418\uc5b4\uc57c \ud55c\ub2e4. \ub2e4\ub978 \ud55c\ud3b8\uc73c\ub85c, train set\ub294 \uacfc\uac70\uc5d0 \ud314\ub838\uac70\ub098 \ubc18\ud658\ub41c \uc30d\ub9cc \ud3ec\ud568\ud55c\ub2e4. \uc6d4\ubcc4 \ub9e4\ucd9c\uc744 \uacc4\uc0b0\ud558\uc5ec \uc6d4\ubcc4 \ud310\ub9e4\ub7c9\uc744 0\uc73c\ub85c \ud655\ub300\ud558\ub294 \uac83\uc774 \uc8fc\ub41c \uc0dd\uac01\uc774\ub2e4. \uc774\ub7ec\ud55c \ubc29\uc2dd\uc73c\ub85c \uc5f4\ucc28 \ub370\uc774\ud130\ub294 test data\uc640 \uc720\uc0ac\ud560 \uac83\uc774\ub2e4.","40b1e0a8":"## Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","59206f9f":"\uc774\uc0c1\ud55c \uac00\uaca9\uacfc \ud310\ub9e4\ub97c \ud558\ub294 \ubb3c\uac74\ub4e4\uc774 \uc788\ub2e4.\n\nprice > 100000, \ud310\ub9e4 \ub41c \uc81c\ud488 \uc218(item_cnt) > 1001 (1 000\uc740 \u3131\u314a)\uc758 \ud488\ubaa9\uc744 \uc81c\uac70\ud558\uace0\uc790 \ud55c\ub2e4.","afcc2b2f":"## #3 shop\/item\/item_category\uc758 \uac00\uc815\n\n1. \uac01 shop_name\uc740 \ub3c4\uc2dc \uc774\ub984\uc73c\ub85c \uc2dc\uc791\ud55c\ub2e4.\n2. \uac01 \ubc94\uc8fc\uc5d0\ub294 \uc774\ub984\uc5d0 \uc720\ud615\uacfc \ud558\uc704 \uc720\ud615\uc774 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4.","09d4b878":"## Outliers","12d8fc88":"Producing lags brings a lot of nulls.","06452aa4":"## Traget lags","405d5730":"\uba87 \uac1c\uc758 \uac00\uac8c\ub294 \uc11c\ub85c \uc911\ubcf5\uc774\uae30 \ub54c\ubb38\uc5d0, train\uacfc test set\uc744 \uace0\uc815\ud574\uc57c\ud55c\ub2e4.","84fad642":"Select perfect features","30a0dc0b":"## Special features","13798ff6":"Last month shop revenue trend","14059ec9":"# Part 2, xgboost","34d80353":"Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train.","c2019b72":"\uac00\uaca9\uc774 0\ubcf4\ub2e4 \ub0ae\uc740 \ud56d\ubaa9\uc774 \ud558\ub098 \uc788\ub2e4. median\uc73c\ub85c \ucc44\uc6b4\ub2e4.","6e83f90d":"## Shops\/Items\/Cats features","8e502814":"#4","fb290001":"## Trend features","dc4ff55b":"\ub370\uc774\ud130 \ubd84\uc11d\uc758 \uc9c4\ud589\uc740 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\n1. \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\n2. \ub370\uc774\ud130 \uc218\uc815 \ubc0f \ud2b9\uc774\uce58 \uc81c\uac70\ud558\uae30\n3. shop\/item\/item_category\uc758 \uc81c\ud488\uacfc \ud2b9\uc9d5\uc744 \uac00\uc9c0\uace0 \uc791\uc5c5\ud558\uae30\n4. the train set\uc5d0\uc11c item\/shop \uace0\uc720 \uc2dd\ubcc4\ub85c matrix\ub97c \uc791\uc131\ud55c\ub2e4.\n5. the train set\uc5d0\uc11c item\/shop\uc5d0 \ub300\ud55c \uc6d4\ubcc4 \ud310\ub9e4\ub97c \ubc1b\uc544 \ub9e4\ud2b8\ub9ad\uc2a4\uc5d0 \ubcd1\ud569\ud55c\ub2e4.\n6. item_cnt_month\ub97c (0,20)\uc73c\ub85c \uc790\ub974\uae30\n7. test\uc5d0 matrix\ub97c \ucd94\uac00\ud558\uae30, 34\uac1c\uc6d4\uc758 \ube48\uacf3(N\/A)\uc5d0 0\uc744 \ucc44\uc6b4\ub2e4.\n8. matrix\uc5d0 shop\/item\/item_category\uc744 \ucd94\uac00\ud558\uae30(merge)\n9. add target lag features(\ubaa9\ud45c \uc9c0\uc5f0 \uae30\ub2a5 \ucd94\uac00)\n10. add mean encoded features(\uc778\ucf54\ub529\ub41c \ud3c9\uade0 \ud53c\uccd0 \ucd94\uac00)\n11. add price trend features(\uac00\uaca9 \ucd94\uc138 \ud2b9\uc9d5 \ucd94\uac00)\n12. add month(\uc6d4\uc744 \ub354\ud558\ub2e4)\n13. add days(\uc77c\uc218\ub97c \ub298\ub9ac\ub2e4)\n14. add months since last sale\/months since first sale features(\uccab \ubc88\uc9f8 \ud310\ub9e4 \uae30\ub2a5 \uc774\ud6c4 \ub9c8\uc9c0\ub9c9 \ud310\ub9e4\/\uc6d4 \uc774\ud6c4 \uc6d4\uc744 \ub354\ud558\ub2e4)\n15. test set\uc5d0 \ub300\ud574 \uacc4\uc0b0\ud560 \uc218 \uc5c6\ub294 \uccab\ubc88\uc9f8 \ub144\ub3c4\uc640 \uce7c\ub7fc\uc744 \uc81c\uac70\ud55c\ub2e4.\n16. \uac00\uc7a5 \uc88b\uc740 features\uc744 \uace0\ub978\ub2e4.\n17. set validation strategy 34 test, 33 validation, less than 33 train\n18. fit the model, predict and clip targets for the test set\n\n#\ub2e4\uc74c \ubc88\ud638\ub97c \ubd99\uc5ec, \uc21c\uc11c \ud750\ub984\uc744 \uc54c \uc218 \uc788\uac8c \ud45c\uc2dc\ud574 \ub450\uc5c8\ub2e4.\n\ucc38\uace0 : https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost","15d8df5a":"## Test set\nTo use time tricks append test pairs to the matrix.","c64a0cb3":"Months since the last sale for each shop\/item pair and for item only. I use programing approach.\n\n<i>Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.<\/i>","2f2d2fb6":"## Mean encoded features","49df0261":"Months since the first sale for each shop\/item pair and for item only.","659bbe0c":"\ud56d\ubaa9_cnt_month\uc5d0 \ub300\ud55c ins \ub300\uc2e0 floind\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub098\uc911\uc5d0 \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc640 \uacb0\ud569\ud55c \ud6c4 \ub2e4\uc6b4\uce90\uc2a4\ud305(downcasting)\ub418\ub294 \uac83\uc744 \ud53c\ud55c\ub2e4. \ub9cc\uc57d \uadf8\uac83\uc774 16\uc774 \ub41c\ub2e4\uba74, NaN \uac12\uacfc \uacb0\ud569\ud55c \ud6c4\uc5d0 \uadf8\uac83\uc740 64\uac00 \ub418\uc9c0\ub9cc, foat16\uc740 NaNs\ub97c \uac00\uc9c0\uace0\ub3c4 float16\uc774 \ub418\uae30 \ub54c\ubb38\uc774\ub2e4.","fd9aab10":"Price trend for the last six months."}}