{"cell_type":{"27347ebf":"code","3b8e6afa":"code","e10d6f87":"code","94f1b8db":"code","4132b87f":"code","f78a332a":"code","292968a0":"code","8d75f475":"code","166ff7be":"code","ec76e5c4":"code","beac5286":"code","ce3bcc96":"code","3ae9dea5":"code","0bb396ac":"code","19dc6bf4":"code","a3dc563b":"code","54ad74c3":"code","681caf23":"code","de70823c":"code","05891920":"code","e326bd00":"code","4b303e25":"code","8b936823":"code","5c2ac97b":"code","12d353e6":"code","562e0dfd":"code","8938d7b7":"code","0e916de6":"code","0a129c94":"code","575bdce6":"code","5ff31729":"code","713c60b7":"markdown","c353ff4e":"markdown","ad0cc93e":"markdown","0d37e2ee":"markdown","3cc118af":"markdown","2fbad8f0":"markdown","772ace82":"markdown"},"source":{"27347ebf":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Input, Concatenate\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","3b8e6afa":"housing = fetch_california_housing()\nXtrain, Xtest, ytrain, ytest = train_test_split(housing.data, housing.target, test_size = .2)\nXtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size=.2)","e10d6f87":"print(housing.DESCR)","94f1b8db":"# Let's do some scaling\nscaler = StandardScaler()\nXtrain = scaler.fit_transform(Xtrain)\nXtest = scaler.transform(Xtest)\nXval = scaler.transform(Xval)","4132b87f":"model = Sequential([\n    Dense(30, activation = 'relu', input_shape= Xtrain.shape[1:]),\n    Dense(1)\n])","f78a332a":"model.summary()","292968a0":"model.compile(loss='mean_squared_error', optimizer='adam')","8d75f475":"history = model.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs=20)","166ff7be":"model.save('Sequential.h5')","ec76e5c4":"mse_test = model.evaluate(Xtest, ytest)\nmse_test","beac5286":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\nplt.show()","ce3bcc96":"X_new = Xtest[:10] # pretend that this is new data\nynew = model.predict(X_new)","3ae9dea5":"np.sqrt(np.sum((ynew - ytest[:10].reshape(-1,1))**2, axis=0)\/10) # MSE on the new dataset","0bb396ac":"# Define the model.\ninput_ = Input(shape= Xtrain.shape[1:])\nhidden1 = Dense(30, activation='relu')(input_)\nhidden2 = Dense(30, activation='relu')(hidden1)\nconcat = Concatenate()([input_, hidden2])\noutput = Dense(1)(concat)\nfmodel = keras.Model(inputs=[input_], outputs=[output])","19dc6bf4":"fmodel.summary()","a3dc563b":"fmodel.compile(loss='mean_squared_error', optimizer='adam')","54ad74c3":"fhistory = fmodel.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs=20)","681caf23":"fmodel.save('Functional.h5')","de70823c":"mse_test = fmodel.evaluate(Xtest, ytest)\nprint(f'MSE of test is {mse_test}')\nX_new = Xtest[:10] # pretend that this is new data\nynew = fmodel.predict(X_new)\n\nmse = np.sqrt(np.sum((ynew - ytest[:10].reshape(-1,1))**2, axis=0)\/10) # MSE on the new dataset\nprint(f'MSE of the new data is {mse}')","05891920":"# Define the model.\ninputA = Input(shape=[5], name='wide_input')\ninputB = Input(shape=[6], name='deep_input')\nhidden1 = Dense(30, activation='relu')(inputB)\nhidden2 = Dense(30, activation='relu')(hidden1)\nconcat = Concatenate()([inputA, hidden2])\noutput = Dense(1, name='output')(concat)\nf1model = keras.Model(inputs=[inputA, inputB], outputs=[output])","e326bd00":"f1model.summary()","4b303e25":"XtrainA, XtrainB = Xtrain[:,:5], Xtrain[:,2:]\nXvalA, XvalB = Xval[:,:5], Xval[:,2:]\nXtestA, XtestB = Xtest[:,:5], Xtest[:,2:]\nXnewA, XnewB = XtestA[:10], XtestB[:10]","8b936823":"f1model.compile(loss='mean_squared_error', optimizer='adam')\nf1history = f1model.fit((XtrainA, XtrainB), ytrain, epochs=20, validation_data=((XvalA, XvalB),yval))","5c2ac97b":"f1model.save('FunctMultInp.h5')","12d353e6":"# Seem to be doing worse than the last model. Let us check on test data now.\nmse_test = f1model.evaluate((XtestA, XtestB), ytest)\nprint(f'MSE for test is {mse_test}')\nynew = f1model.predict((XnewA,XnewB))\n\nmse = np.sqrt(np.sum((ynew - ytest[:10].reshape(-1,1))**2, axis=0)\/10) # MSE on the new dataset\nprint(f'MSE of the new data is {mse}')","562e0dfd":"# Define the model.\ninputA = Input(shape=[5], name='wide_input')\ninputB = Input(shape=[6], name='deep_input')\nhidden1 = Dense(30, activation='relu')(inputB)\nhidden2 = Dense(30, activation='relu')(hidden1)\nconcat = Concatenate()([inputA, hidden2])\noutput = Dense(1, name='main_output')(concat)\naux_output = Dense(1, name='aux_output')(hidden2)\nmodel = keras.Model(inputs=[inputA, inputB], outputs=[output, aux_output])","8938d7b7":"model.summary()","0e916de6":"model.compile(loss=['mse', 'mse'], loss_weights=[0.9, 0.1], optimizer='adam')","0a129c94":"history = model.fit([XtrainA, XtrainB],\n                   [ytrain, ytrain],\n                   epochs=20,\n                   validation_data=([XvalA, XvalB], [yval, yval]))","575bdce6":"model.save('AuxOut.h5')","5ff31729":"# Seem to be doing worse than the last model. Let us check on test data now.\ntotal_loss, main_loss, aux_loss = model.evaluate((XtestA, XtestB), (ytest,ytest))\nprint(f'Total loss is {total_loss}, main_loss is {main_loss} and auxiliary loss is {aux_loss}')\ny_pred_main, y_pred_aux = model.predict((XnewA,XnewB))\n\nmse = np.sqrt(np.sum((y_pred_main - ytest[:10].reshape(-1,1))**2, axis=0)\/10) # MSE on the new dataset\nprint(f'MAIN MSE of the new data is {mse}')\nmse = np.sqrt(np.sum((y_pred_aux - ytest[:10].reshape(-1,1))**2, axis=0)\/10) # MSE on the new dataset\nprint(f'AUX MSE of the new data is {mse}') # Recall loss and mse are same thing here.","713c60b7":"Seems that loss is getting stagnant on validation as well as training side. So continuing might give overfit.","c353ff4e":"So the last model is definitely not a good idea. Maybe tuning some parameters could give us something better. Feel free to tune it.","ad0cc93e":"# More complex modeling using Functional API.","0d37e2ee":"# Auxiliary output in Functional API.","3cc118af":"# Functional API with more than one input.","2fbad8f0":"So test and validation losses are nearby and hence it is a good sign that may be things are working well.","772ace82":"So functional API seems to be better than the sequential modeling. Next, we will do even more modification."}}