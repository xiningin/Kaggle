{"cell_type":{"a1945e54":"code","450367be":"code","b635544a":"code","bcb37e67":"code","48dc4642":"code","6bd1a80c":"code","ba48cc34":"code","4830449b":"code","92c89b0d":"code","0813d625":"code","0cc66a93":"code","0fa786a5":"code","b678434f":"code","ee5af23b":"code","2ef05b31":"markdown","a2caf9ce":"markdown","7a56baa5":"markdown","d571f34c":"markdown","355ae70e":"markdown","4b0cd5f6":"markdown","ce0928af":"markdown","b0d1cfd7":"markdown","42df8f89":"markdown","431e9f53":"markdown","21aa1569":"markdown","9bf0cc6a":"markdown"},"source":{"a1945e54":"# Important libraries to use for this project.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression#for one vs all section\n\nimport matplotlib.pyplot as plt","450367be":"data = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\ndata = data.drop(\"Id\", axis=1)\nclasses = [c for c in data[\"Species\"].unique()]\nprint(data.columns)\nprint(data.shape)\nprint(f\"# columns: {len(data.columns)}\")\ndata[\"Species\"] = data[\"Species\"].map({\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2})\ndata.describe()","b635544a":"#if we didn't print anything in the loop then there are no missing values.\nprint(f\"# columns: {len(data.columns)}\")\n# Check for missing values\nfor column in data.columns:\n    if data[column].isna().values.sum() > 0:\n        print(f\"Missing values at {column}\")\n        print(f\"#Missing values is {data[column].isna().values.sum()}\")","bcb37e67":"# Splitting the data into training set and test set\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_data = None\ntest_data = None\nfor train_ind, test_ind in splitter.split(data, data['Species']):\n    train_data = data.iloc[train_ind]\n    test_data = data.iloc[test_ind]","48dc4642":"fig, axes = plt.subplots(1, 2)\ntrain_data[\"Species\"].hist(ax=axes[0], density=True)\naxes[0].set_title(\"Distribution for the labels of the training set\")\ntest_data[\"Species\"].hist(ax=axes[1], density=True)\naxes[1].set_title(\"Distribution for the labels of the test set\")\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=2, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\nplt.show()","6bd1a80c":"# ","ba48cc34":"#Normalizing the dataset since all columns of the dataset are numeric we can standard normalize it to be of mean 0 with unit variance\nX_train = np.asarray(train_data.drop(\"Species\", axis=1))\ny_train = np.asarray(train_data[\"Species\"])\nX_test = np.asarray(test_data.drop(\"Species\", axis=1))\ny_test = np.asarray(test_data[\"Species\"])\n\nstandarize = StandardScaler()\nstandarize.fit(X_train)\nX_train = standarize.transform(X_train)\nX_test = standarize.transform(X_test)\nprint(np.mean(X_train, axis=0))\nprint(np.std(X_train, axis=0))","4830449b":"class SoftmaxRegressor(object):\n    \n    def __init__(self, m, K):\n        np.random.seed(42)\n        self.W = np.random.rand(K, m+1)\n        self.W[:, 0] = 0#bias term\n        \n        \n    def fit(self, alpha, epochs, mini_batch_size, X, y):\n        y = y.reshape(-1, 1)\n        y_mask = np.zeros((y.shape[0], self.W.shape[0]))\n        for ind, v in enumerate(y):\n            y_mask[ind, v] = 1\n        assert(y.shape[0] == X.shape[0])\n        \n        #Data is already shuffled\n        m = self.W.shape[1]\n        X_padded = np.ones((X.shape[0], X.shape[1] + 1))\n        X_padded[:, 1:] = X.copy()\n        n = X_padded.shape[0]\n\n        number_batches = (int)(n\/mini_batch_size)\n        print(number_batches)\n        costs = []\n        for epoch in range(0, epochs):\n            counter = 0\n            cost = []\n            for batch in range(0, number_batches):\n                X_t = X_padded[counter:counter+mini_batch_size, :]\n                y_t = y_mask[counter:counter+mini_batch_size]\n                self.W = self.W + alpha * (np.multiply(SoftmaxRegressor.softmax(self.W, X_t), y_t) - y_t).T.dot(X_t)\n                cost.append(SoftmaxRegressor.crossEntropy(self.W, X_t, y_t))\n                counter += mini_batch_size\n                \n            if counter < len(X_padded):\n                X_t = X_padded[counter:, :]\n                y_t = y_mask[counter:]\n                self.W = self.W + alpha * (np.multiply(SoftmaxRegressor.softmax(self.W, X_t), y_t) - y_t).T.dot(X_t)\n                cost.append(SoftmaxRegressor.crossEntropy(self.W, X_t, y_t))\n                \n            costs.append(np.mean(cost))\n            if (epoch % 2) == 0:\n                print(f\"Cost at {epoch} is: {costs[epoch]}\")\n        return costs\n    \n    def predict(self, X):\n        X_t = X\n        if abs(X.shape[1] - self.W.shape[1]) == 1:\n            #Pad it \n            X_t = np.ones((X.shape[0], X.shape[1] + 1))\n            X_t[:, 1:] = X\n        #print(SoftmaxRegressor.softmax(self.W, X_t))\n        return np.argmax(SoftmaxRegressor.softmax(self.W, X_t), axis=1)#assign to the class with the highest probability\n    \n    @staticmethod\n    def softmax(W, X):\n        assert(X.shape[1] == W.shape[1])\n        result = X.dot(W.T)#nxk\n        #Ensure operation are done element wise\n        #Needed to multiply by negative value because I had problems with values getting extermely large\n        result = np.exp(-1*result)\/np.sum(np.exp(-1*result), axis=1).reshape(-1, 1)\n        return result\n    \n    @staticmethod\n    def crossEntropy(W, X, y):\n        return np.mean(-1* np.log(np.max(np.multiply(np.exp(-1*X.dot(W.T)), y), axis=1)\/np.sum(np.exp(-1*X.dot(W.T)),axis=1).reshape(-1, 1)))\n            \n            ","92c89b0d":"model = SoftmaxRegressor(X_train.shape[1], 3)\ncosts = model.fit(0.8, 10, 64, X_train, y_train)\nprediction = model.predict(X_train)\ncnf = confusion_matrix(y_train, prediction)\nprint(f\"Accuracy: {np.round(np.sum(np.diag(cnf))\/np.sum(cnf), 4) * 100 }%\")\ncnf","0813d625":"prediction = model.predict(X_test)\ncnf = confusion_matrix(y_test, prediction)\nprint(f\"Accuracy: {np.round(np.sum(np.diag(cnf))\/np.sum(cnf), 4) * 100 }%\")\ncnf","0cc66a93":"models = [LogisticRegression(), LogisticRegression()]\ny_train_1vsa = []\nfor i in range(0, 2):\n    y_train_1vsa.append([1 if v == i else 0 for v in list(y_train)])","0fa786a5":"models[0].fit(X_train, y_train_1vsa[0])\nmodels[1].fit(X_train, y_train_1vsa[1])","b678434f":"pred_0 = models[0].predict(X_train)\npred_1 = models[1].predict(X_train)\nprediction = []\nfor i in range(0, len(X_train)):\n    if pred_0[i] == 1:\n        prediction.append(0)\n    elif pred_1[i] == 1:\n        prediction.append(1)\n    else:\n        prediction.append(2)\ncnf = confusion_matrix(y_train, prediction)\nprint(f\"Accuracy: {np.round(np.sum(np.diag(cnf))\/np.sum(cnf), 4) * 100 }%\")\ncnf","ee5af23b":"pred_0 = models[0].predict(X_test)\npred_1 = models[1].predict(X_test)\nprediction = []\nfor i in range(0, len(X_test)):\n    if pred_0[i] == 1:\n        prediction.append(0)\n    elif pred_1[i] == 1:\n        prediction.append(1)\n    else:\n        prediction.append(2)\ncnf = confusion_matrix(y_test, prediction)\nprint(f\"Accuracy: {np.round(np.sum(np.diag(cnf))\/np.sum(cnf), 4) * 100 }%\")\ncnf","2ef05b31":"# Softmax regression\nAs is known in logistic regression, we want to describe the conditional distribution of the target class given the input in order to deal with discriminator models. Softmax regression will deal with datasets that have more than one class, in which each observation is described by the following formula:- $p(\\underline{t}|\\underline{x};\\mu_1,...,\\mu_k)=\\prod_{k=1}^{K}\\mu_k^{t_k}$. In which **t** is hot encoded by 1 at the index that correspond to the class number, in which it will act like an indicator function$I(t_1==1)=1\\ else\\ 0$. Due to the constrain of $||\\mu||_1=1$ then knowing the mu's of class 1 to K-1 will be sufficient to know $\\mu_k$, hence, this class will be considered as the reference class and it will not have any parameters as was the case for the negative class in logistic regression. As we know from logistic regression, we need to transform the distribution of the conditional distribution to the expoonential family, in order to deal with this probelm in the generalized linear model (GLM's) sense, and this will end up resulting to what is known as the softmax formula.\n\\begin{align}\n\\begin{split}\n&Exponential\\ distrubution:\\ p(t|x;\\eta)=h(t)g(\\eta)exp(\\eta^Tu(t))\\\\\n&Expressing\\ the\\ categorical\\ distribution\\ in\\ terms\\ of\\ the\\ Exponential\\ family:\\\\\n&p(\\underline{t}|x;\\eta)=exp(log(p(\\underline{t}|x;\\mu)))=exp(\\sum_{k=1}^{K}t_klog(\\mu_k));\\ by\\ \\sum_{i}\\mu_i=1:\\\\\n& t_K = 1- \\sum_{i=1}^{K-1}t_i\\ because\\ at\\ least\\ 1\\ of\\ them\\ is\\ 1\\\\\n&p(\\underline{t}|x;\\eta)=exp(\\sum_{i=1}^{K-1}t_i log(\\mu_i) + (1 -\\sum_{k=1}^{K-1}t_k)log(\\mu_k));\\ by\\ log(\\frac{a}{b})=log(a)-log(b)\\\\\n&p(\\underline{t}|x;\\eta)=\\mu_Kexp\\Big(\\sum_{i=1}^{K-1}log(\\frac{\\mu_i}{\\mu_K})\\Big)\\\\\\\\\n&\\eta=\n\\begin{pmatrix}\n&log(\\frac{\\mu_1}{\\mu_k})\\\\\n&:\\\\\n&log(\\frac{\\mu_{K-1}}{\\mu_k})\n\\end{pmatrix}\\\\\\\\\n&u(t)=(t_1, t_2,..., t_{K-1})^T\\rightarrow Sufficient\\ statistics\\\\\n&Canonical\\ Link\\ function \\rightarrow\\ \\eta_i=log(\\frac{\\mu_i}{\\mu_K})=\\underline{w_i}^T\\underline{x},\\ by\\ taking\\ exponential\\ of\\ both\\ sides\\\\\n&\\mu_i=\\mu_Kexp(w_i^Tx);\\ by\\ summing\\ across\\ all\\ \\mu^{'}s\\ and \\  \\mu_K=\\frac{1}{\\sum_{i}^{K}exp(w_i^Tx)}\\ because\\ w_K=\\underline{0}\\\\\n& \\mu_i=\\frac{exp(w_i^Tx)}{1+\\sum_{k=1}^{K-1}exp(w_i^Tx)};\\ i.e\\ the\\ softmax\\ function\\\\\n&\\because p(t=1|x;\\eta)=\\mu_1\\rightarrow E[u(t_1)|x;\\eta]=E[1(t =1)|x;\\eta]=0 * p(1(y \\neq 1)|x;\\eta))+p(1(y=1)|x;\\eta))=\\mu_1\\\\\n&\\therefore E[u(y)|x;\\eta]=(\\mu_1,..,\\mu_{K-1})^T\n\\end{split}\n\\end{align}\n","a2caf9ce":"# One-vs-All performance on the test-set","7a56baa5":"# References \n* Chapter 1, chapter 2 and Chapter 4 from Bishop, C. (2006). Pattern Recognition and Machine Learning. Cambridge: Springer.\n* Andrew Ng, Lec 1: (https:\/\/www.youtube.com\/watch?v=UzxYlbK2c7E)\n* Andrew Ng, Lec 2: (https:\/\/www.youtube.com\/watch?v=5u4G23_OohI)\n* Andrew Ng, Lec 3: (https:\/\/www.youtube.com\/watch?v=HZ4cvaztQEs)\n* Andrew Ng, Lec 4: (https:\/\/www.youtube.com\/watch?v=nLKOQfKLUks)\n","d571f34c":"# Estimating the Parameters \nWe will use gradient descent to minimize the cross entropy cost function, in which we need to maximize the log-likelihood for the samples that are drawn i.i.d from a caregroical distribution, and this is done K-1 times with respect to the parameters of each class except for the Kth class.\n\n\\begin{align}\n\\begin{split}\n& all\\ w's\\ are\\ vectors\\\\\n&\\nabla_{w_i}\\ell(w_1, w_2, ..., w_{K-1}) = -\\nabla_{w_i}\\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk}log(\\frac{exp(\\eta_{nk})}{1+\\sum_{j=1}^{K-1}exp(\\eta_{nj})}); by\\ \\frac{d logx}{dx} =(\\frac{1}{x})*1\\\\\\\\\n&\\nabla_{w_1}\\ell(w_1, w_2, ..., w_{K-1})=-\\sum_{n=1}^{N}t_{n1} \\frac{\\sum_{j}exp(w_j^Tx_n)}{exp(w_1^Tx_n)} \n\\Big(\n     \\frac{x\\ exp(w_1^Tx_n)\\sum_{j}exp(w_j^Tx_n)  -  x\\ exp(w_1^Tx_n)exp(w_1^Tx_n)}\n     {(\\sum_{j}exp(w_j^Tx_n) )^2}\\Big)\n     +t_{n2} \\big(\n          \\frac{-x\\ exp(w_2^Tx)exp(w_1^Tx)}\n          {exp(w_2^tx)\\sum_{j}exp(w_j^Tx)} \n           \\big) + ... \n ;\\\\\\\\\n&\\nabla_{w_1}\\ell(w1, w2, ..., w_{K-1})=-\\sum_{n=1}^{N}(x_nt_{n1}(1-\\mu_1)-x_nt_{n2}\\mu_1-x_nt_{n3}\\mu_1...)=-\\sum_{n=1}^{N}x_n(t_{n1} - \\mu_1(t_{n1}+t_{n2} + ...));\\ by\\ the\\ principle\\ of\\ one\\ hot\\ encoded\\ \\sum{i}t_i=1\\\\\\\\\n&\\nabla_{w_1}\\ell(w1, w2, ..., w_{K-1}) =-\\sum_{n=1}^{N}(t_{n1} -\\mu_1)x_n\\\\\\\\\n&w_1^{t+1}=w_1^{t}+\\alpha\\nabla_{w_1^{t}}\\ell(w_1^t, w_2, ..., w_{K-1})\\\\\n&In\\ matrix\\ form\\ assuming\\ functions\\ are\\ applied\\ element\\ wise\\ and\\ \\ W\\in R^{kxm}\\ and X\\in R^{nxm}\\ and\\ y\\in R^{nxk}\\\\\n&W^{t+1}=W^{t} + \\alpha\\frac{1}{n}*(softmax(X\\cdot (W^{t})^T)\\circ y - y)^T\\cdot X;\\ where\\ is\\ \\circ\\ is\\ the\\ Hadamard\\ product\n\\end{split}\n\\end{align}\n\n\n\nAs can be seen from the last equation, we arrived to the same exact equation for gradient descent step that was seen in logistic regression and linear regression. \n","355ae70e":"# Loading Data, visualization and transformation","4b0cd5f6":"# Cross Entropy Loss function\n\n\\begin{align}\n\\begin{split}\n&\\ell(w_1, w_2, ..., w_{K-1}) = -\\frac{1}{n}\\sum_{n=1}^{N}\\sum_{k=1}^{K}t_{nk}log(\\frac{exp(\\eta_{nk})}{1+\\sum_{j=1}^{K-1}exp(\\eta_{nj})});\\ let\\ t\\ to\\ be\\ of\\ ordinal\\ scale,\\ starting\\ from\\ 0\\\\\n&\\ell(w_1, w_2, ..., w_{K-1}) = -\\frac{1}{n}\\sum_{n=1}^{N}log(\\frac{exp(\\underline{\\eta_{n}}(t_{n}))}{1+\\sum_{j=1}^{K-1}exp(\\underline{\\eta_{n}}(j))})\\rightarrow the\\ one\\ used\\ in\\ PyTorch\n\\end{split}\n\\end{align}","ce0928af":"# Categorical Distribution\nBefore getting into Softmax regression, we need to define what is a Categorical distribution which is a special case of Multinomial distribution in which \\#trials=1. A discrete random variable is said to have a multinomial distribution when it have multiple levels instead of 2 levels as was the case in the binomial distribution. This discrete r.v is usually encoded in what is known as a one hot encoder in which this vector would have 1 at the index that correspond to the specified level and the rest of the entries would be zeros, hence, $\\sum_{k=1}^{K}t_i=1$. In which each level is represented by an index in this vector. So, the probability of occurance of a single observation is given by the following $p(\\underline{x}|\\underline{\\mu})=\\prod_{k=1}^{K}\\mu_k^{x_k}$. And to be a valid distribution the normalization axiom should be satisfied, hence, $\\sum_{k=1}^{K}\\mu_k=1$. For a dataset that are coonstructed from N observations is represented as follows:-\n\n\\begin{align}\n\\begin{split}\n&p(D|\\underline{x};\\underline{\\mu})=\\prod_{n=1}^{N}\\prod_{k=1}^K\\mu_k^{x_{nk}}=\\prod_{k=1}^{K}\\prod_{n=1}^{N}\\mu_k^{x_{nk}};\\ by \\ using\\ x^ax^b=x^{a+b}\\\\\n&p(D|\\underline{x};\\underline{\\mu})=\\prod_{k=1}^{K}\\mu_k^{\\sum_{n=1}^{N}x_{nk}};\\ let\\ \\sum_{n=1}^{N}x_{nk}=m_k\\\\\n&p(D|\\underline{x};\\underline{\\mu})=\\prod_{k=1}^{K}\\mu_k^{m_k}\\\\\n\\end{split}\n\\end{align}\n\nIn order to find the point estimate for the $\\mu^{'s}$, we need to maximize the log-likelihood of the conditional distribution while taking care of the constrain on $||\\mu||_1=1$. This can be done by using lagrangian on log-likelihood $p(D|\\underline{x};\\underline{\\mu})$, which can be derived as follows:-\n\\begin{align}\n\\begin{split}\n&\\nabla_{\\mu_k}L(\\underline{\\mu}, \\lambda)=\\nabla_{\\mu_k}\\Big( \\sum_{k=1}^{K}m_klog(\\mu_k)+\\lambda(\\sum_{i}\\mu_i -1) \\Big)=0\\rightarrow\\mu_k=\\frac{-m_k}{\\lambda}\\\\\n&\\sum_{k}\\mu_k=1\\rightarrow\\sum_{k}\\mu_k=\\sum_{k}\\frac{-m_k}{\\lambda}=\\sum_{k}\\frac{-\\sum_{n}x_{nk}}{\\lambda}=1\\rightarrow \\lambda=-N\\\\\n&\\therefore\\mu_k=\\frac{\\sum_{n}x_{nk}}{N}\\\\\n\\end{split}\n\\end{align}\nAs can be seen from the last equation, it is clearly sufficient to estimate the probability of each class target as the fraction of observation in those classes, hence, the name sufficient statistics that is used for $m_k$\n\nTherefore, the point estimate for each $\\mu$ is described by the proportion of the number of observations of each class with respect to the total number of observations. The Categorical distribution is discribed by the following equation, in which we would have a $\\mu$ for each class and the they should sum to one.\n\\begin{align}\nCat(\\underline{\\mu})=\\prod_{k=1}^{K}\\mu_k^{x_k}\n\\end{align}","b0d1cfd7":"# Prediction\nWe will make K prediction in which the class that would be chosen is the class that gave the highest probability.\n\\begin{align}\n\\begin{split}\n&p(y=1|x;\\eta)=\\mu_1=\\frac{exp(w_1^Tx)}{1+\\sum_{k=1}^{K-1}exp(w_k^Tx)}\\\\\n&:\\\\\n&p(y=K|x;\\eta)=\\mu_K=1-\\sum_{i=1}^{K-1}\\frac{exp(w_k^Tx)}{1+\\sum_{k=1}^{K-1}exp(w_k^Tx)}; from\\ ||\\mu||_1=1 \\\\\n\\end{split}\n\\end{align}","42df8f89":"# Building Softmax regression from scratch and training the model","431e9f53":"# Introduction\nHave you ever wondered from where the softmax function have risen?! Hopefully this notebook will help you in understanding the softmax function and the necessity of it for classification tasks. In order to understand what I am going to explain, you need a bit of knowledge in linear algebra and probability. Don't forget linear algebra exist for optimization and visualization of higher dimensional space, hence, my implementation will use numpy array and hopefully you willn't see too many \"for loops\" in this notebook ;). A small note, anything with an underline indicate that we are dealing with a vector, and $\\underline{w}$ will contain b(i.e bias term).\n\nFor logistic regression construction, please see my explaination of it in the following link [https:\/\/www.kaggle.com\/rasheedaz\/logistic-regression-mathematics-and-implementation](https:\/\/www.kaggle.com\/rasheedaz\/logistic-regression-mathematics-and-implementation)\n## Prerequisite:\n    * Linear Algebra\n    * Applied probabilistic models","21aa1569":"# Performance of the model on the test set","9bf0cc6a":"# One-vs-One model and One-vs-All model\nSoftmax regression is considered to be a one-vs-one model in which we directly model the conditional distribution on all of the classes, where is one-vs-all models are models that divide the problem into sub-problems. In which, we design K-1 models by taking K-1 copies of the dataset and then we design each model for a specific class (we consider this class to be the positive class) and then we designate the rest of the classes as the negative class. And in the prediction step we assign the observation to a class that achieved 1 in the prediction step. It is quite evident from the previous statement this method is a hassle in terms of both computation and memory. Also, a subtle problem arises in practice in which more than one model output a positive prediction, i.e classes are overlapped. Where is in the case of one-vs-one we can mitigate the problem of overlapping by assigning a decision weights for the classes probability. But believe it or not most of machine learning algorithms are using One-vs-All approach because they are easier to train, and most likely you have dealt with on of them---have you ever heard of support vector machines!!!"}}