{"cell_type":{"5e965454":"code","e7955c88":"code","43125f03":"code","25b7d54e":"code","31b7d333":"code","df3d588b":"code","8e85417b":"code","c4c6d2ed":"code","f901398c":"code","1861c67d":"code","c82fed36":"code","7a02a73b":"code","9c500b67":"code","9abe89be":"code","2ee24393":"code","86627914":"code","2457de26":"code","5c740509":"code","0219755d":"code","132217d0":"code","0bbece06":"code","bd56dd71":"code","191e56c6":"code","aca912f3":"code","8cf775f9":"code","daa81eb0":"code","ecf91ec1":"code","4634c3bb":"code","356993be":"code","3513b55e":"code","a0c6cedb":"code","773f8080":"code","11319557":"code","2c9a876d":"code","da2f9666":"code","2123350b":"code","e52ab7f6":"code","e3ab0a26":"code","beb0efa0":"code","dc13aa27":"code","fc513bb1":"code","b9790a34":"code","1bf38ea6":"code","1f70db01":"markdown","0ef1f8c2":"markdown","5315743d":"markdown","ecac930b":"markdown","7e0c39f3":"markdown","467e2ea5":"markdown","d8f4ac14":"markdown","9d826c02":"markdown","45eb6dc4":"markdown","18fc4a8d":"markdown","e5bf3836":"markdown"},"source":{"5e965454":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e7955c88":"df=pd.read_csv('..\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv')","43125f03":"df.head()","25b7d54e":"df.tail()","31b7d333":"df.dtypes","df3d588b":"df[['Close']].boxplot()","8e85417b":"df['Date']=pd.to_datetime(df['Timestamp'],unit='s').dt.date\ndf.head()","c4c6d2ed":"group=df.groupby('Date')\ngroup.head()","f901398c":"data=group['Close'].mean()\ndata.head()","1861c67d":"data.shape","c82fed36":"data.isnull().sum()","7a02a73b":"data.head()","9c500b67":"len(data)","9abe89be":"x_train=data.iloc[:len(data)-50]\nx_test=data.iloc[len(x_train):]","2ee24393":"x_train.shape","86627914":"x_test.shape","2457de26":"x_train=np.array(x_train)\nx_train.shape","5c740509":"x_train=x_train.reshape(x_train.shape[0],1)\nx_train.shape","0219755d":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler(feature_range=(0,1))\nxtrain_scaled=scaler.fit_transform(x_train)","132217d0":"type(xtrain_scaled)","0bbece06":"xtrain_scaled.shape","bd56dd71":"timestep=50\nx_train=[]\ny_train=[]\n\nfor i in range(timestep,xtrain_scaled.shape[0]):\n    x_train.append(xtrain_scaled[i-timestep:i,0])\n    y_train.append(xtrain_scaled[i,0])","191e56c6":"len(x_train)","aca912f3":"x_train","8cf775f9":"x_train,y_train=np.array(x_train),np.array(y_train)","daa81eb0":"x_train.shape","ecf91ec1":"y_train","4634c3bb":"y_train","356993be":"x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1) #reshaped for RNN\nprint(\"x_train shape= \",x_train.shape)\nprint(\"y_train shape= \",y_train.shape)","3513b55e":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,SimpleRNN,Dropout,Flatten","a0c6cedb":"reg=Sequential()\n\nreg.add(SimpleRNN(128,activation='relu',return_sequences=True,input_shape=(x_train.shape[1],1)))\nreg.add(Dropout(0.25))\n\nreg.add(SimpleRNN(256,return_sequences=True,activation='relu'))\nreg.add(Dropout(0.25))\n\nreg.add(SimpleRNN(512,return_sequences=True,activation='relu'))\nreg.add(Dropout(0.35))\n\nreg.add(SimpleRNN(256,return_sequences=True,activation='relu'))\nreg.add(Dropout(0.25))\n\nreg.add(SimpleRNN(128,return_sequences=True,activation='relu'))\nreg.add(Dropout(0.25))\n\nreg.add(Flatten())\n\nreg.add(Dense(1))\n\n\nreg.compile(optimizer='adam',loss='mean_squared_error')\nreg.fit(x_train,y_train,epochs=100,batch_size=64)\n","773f8080":"inputs=data[len(data)-len(x_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)","11319557":"xtest=[]\nfor i in range(timestep,inputs.shape[0]):\n    xtest.append(inputs[i-timestep:i,0])\nxtest=np.array(xtest)\nxtest=xtest.reshape(xtest.shape[0],xtest.shape[1],1)","2c9a876d":"predicted_data=reg.predict(xtest)\npredicted_data=scaler.inverse_transform(predicted_data)","da2f9666":"data_test=np.array(x_test)\ndata_test=data_test.reshape(len(data_test),1)","2123350b":"import matplotlib.pyplot as plt","e52ab7f6":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.plot(predicted_data,color=\"b\",label=\"predicted result\")\nplt.legend()\nplt.xlabel(\"Time(50 days)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\nplt.show()","e3ab0a26":"from sklearn.metrics import mean_absolute_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout,Flatten\n\nmodel=Sequential()\n\nmodel.add(LSTM(10,input_shape=(None,1),activation=\"relu\"))\n\nmodel.add(Dense(1))\n\nmodel.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n\nmodel.fit(x_train,y_train,epochs=100,batch_size=32)","beb0efa0":"inputs=data[len(data)-len(x_test)-timestep:]\ninputs=inputs.values.reshape(-1,1)\ninputs=scaler.transform(inputs)","dc13aa27":"xtest=[]\nfor i in range(timestep,inputs.shape[0]):\n    xtest.append(inputs[i-timestep:i,0])\nxtest=np.array(xtest)\nxtest=xtest.reshape(xtest.shape[0],xtest.shape[1],1)","fc513bb1":"predicted_data=model.predict(xtest)\npredicted_data=scaler.inverse_transform(predicted_data)","b9790a34":"data_test=np.array(x_test)\ndata_test=data_test.reshape(len(data_test),1)","1bf38ea6":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(data_test,color=\"r\",label=\"true result\")\nplt.plot(predicted_data,color=\"b\",label=\"predicted result\")\nplt.legend()\nplt.xlabel(\"Time(50 days)\")\nplt.ylabel(\"Close Values\")\nplt.grid(True)\nplt.show()","1f70db01":"**As timestamp field is of int type, we have to convert that to date type**","0ef1f8c2":"# Processing test data for prediction","5315743d":"# Bit coin close prediction using RNN & LSTM","ecac930b":"# Using LSTM","7e0c39f3":"**In RNN,as the gradient of the training samples gets propagated backward through our network, it gets weaker and weaker, by the time it gets to those neurons that represent older data points in our time-series it has no juice to adjust them properly. This problem is called Vanishing Gradient. A LSTM cell is a type of RNN which stores important information about the past and forgets the unimportant pieces. In this way, when gradient back-propagates, it won\u2019t be consumed by unnecessary information.**","467e2ea5":"**Taking last 50 records for test & remaining for train set**","d8f4ac14":"# RNN model","9d826c02":"**In the above loop, we took the previous value of a set at y and nxt 50 samples at x, for training the RNN**","45eb6dc4":"**From the above two plots, we can able to see that the LSTM model performs well than the RNN model**","18fc4a8d":"# Train test split","e5bf3836":"# Feature scaling"}}