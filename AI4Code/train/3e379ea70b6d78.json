{"cell_type":{"5f23b79f":"code","1b27b894":"code","53e8035a":"code","5172e4d9":"code","0ae2b7de":"code","8449b167":"code","82553636":"code","ada54beb":"code","e6b25c56":"code","c819b028":"code","81781c08":"code","29a4049c":"code","ada81971":"code","2b309ef7":"code","4450b043":"code","beba5608":"code","0693e739":"code","938c80a1":"code","d8ba6d18":"code","4524870a":"code","35e1bd99":"code","e87e227b":"code","d15230d7":"code","8480b2ea":"code","644e3b6e":"code","a8816a09":"code","1a9b4c65":"code","f88ff4d2":"code","b741b3bc":"code","8aa40018":"code","6bfa0a8e":"code","69a72830":"code","507c4c45":"code","2c47452b":"code","c8e49549":"code","6eca1c8f":"code","cae4748a":"code","92d28490":"code","8d5695f7":"code","427bfbdb":"code","79758ef6":"markdown","1b2654a8":"markdown","945e5409":"markdown","d85fb115":"markdown","963f7aa1":"markdown","26afb980":"markdown","1bb9791e":"markdown","f9dd11ee":"markdown","43d4fdc7":"markdown","fcc6ab4d":"markdown","903fd03a":"markdown","a6eabe62":"markdown"},"source":{"5f23b79f":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import chi2, SelectKBest\nimport xgboost\nimport sklearn","1b27b894":"df = pd.read_csv('..\/input\/company-bankruptcy-prediction\/data.csv')","53e8035a":"df.head(5)","5172e4d9":"df.info()","0ae2b7de":"plt.figure(figsize=(25,25))\nsns.heatmap(df.corr())","8449b167":"df['Bankrupt?'].value_counts()","82553636":"X = df.drop('Bankrupt?',axis=1)\ny = df['Bankrupt?']","ada54beb":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e6b25c56":"model = xgboost.XGBRegressor()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\nsklearn.metrics.roc_auc_score(y_test, pred)","c819b028":"standard_scaler = sklearn.preprocessing.StandardScaler()\nrobust_scaler = sklearn.preprocessing.RobustScaler()\nminmax_scaler = sklearn.preprocessing.MinMaxScaler()\nnormalizer_scaler = sklearn.preprocessing.Normalizer()","81781c08":"bestfeatures = SelectKBest(score_func=chi2, k=30)","29a4049c":"fit = bestfeatures.fit(X,y)\ndfscore = pd.DataFrame(fit.scores_)\ndfcolumn = pd.DataFrame(X.columns)\nfeatureScore = pd.concat([dfcolumn,dfscore],axis=1)\nfeatureScore.columns = ['Features','Score']","ada81971":"featureScore.sort_values('Score',ascending=False,inplace=True)\nfeatureScore.reset_index(drop=True,inplace=True)","2b309ef7":"featureScore","4450b043":"new_feature = []\nfor i in range (0, len(featureScore.Features.to_list()),3):\n    new_feature.append(featureScore.Features.to_list()[i])","beba5608":"X_train1, X_test1, y_train1, y_test1 = train_test_split(df[new_feature], y, test_size=0.3, random_state=42)","0693e739":"standard_scaler.fit(X_train1)\nX_train1 = standard_scaler.transform(X_train1)\nstandard_scaler.fit(X_test1)\nX_test1 = standard_scaler.transform(X_test1)","938c80a1":"model = xgboost.XGBRegressor()\nmodel.fit(X_train1, y_train1)\npred1 = model.predict(X_test1)\nsklearn.metrics.roc_auc_score(y_test1, pred1)","d8ba6d18":"new_feature1 = []\nfor i in range (0, len(featureScore.Features.to_list()),2):\n    new_feature1.append(featureScore.Features.to_list()[i])","4524870a":"df[new_feature1]","35e1bd99":"X_train2, X_test2, y_train2, y_test2 = train_test_split(df[new_feature1], y, test_size=0.3, random_state=42)","e87e227b":"standard_scaler.fit(X_train2)\nX_train2 = standard_scaler.transform(X_train2)\nstandard_scaler.fit(X_test2)\nX_test2 = standard_scaler.transform(X_test2)","d15230d7":"model = xgboost.XGBRegressor()\nmodel.fit(X_train2, y_train2)\npred2 = model.predict(X_test2)\nsklearn.metrics.roc_auc_score(y_test2, pred2)","8480b2ea":"new_feature2 = []\nfor i in range (0, len(featureScore.Features.to_list()),4):\n    new_feature2.append(featureScore.Features.to_list()[i])","644e3b6e":"df[new_feature2]","a8816a09":"X_train3, X_test3, y_train3, y_test3 = train_test_split(df[new_feature2], y, test_size=0.3, random_state=42)","1a9b4c65":"standard_scaler.fit(X_train3)\nX_train3 = standard_scaler.transform(X_train3)\nstandard_scaler.fit(X_test3)\nX_test3 = standard_scaler.transform(X_test3)","f88ff4d2":"model = xgboost.XGBRegressor()\nmodel.fit(X_train3, y_train3)\npred3 = model.predict(X_test3)\nsklearn.metrics.roc_auc_score(y_test3, pred3)","b741b3bc":"for i in range(50,80):\n    top_features = featureScore.Features.to_list()[:i]\n    X_trains, X_tests, y_trains, y_tests = train_test_split(df[top_features], y, test_size=0.3, random_state=42)\n    standard_scaler.fit(X_trains)\n    X_trains = standard_scaler.transform(X_trains)\n    standard_scaler.fit(X_tests)\n    X_tests = standard_scaler.transform(X_tests)\n    model.fit(X_trains, y_trains)\n    preds = model.predict(X_tests)\n    results = sklearn.metrics.roc_auc_score(y_tests, preds)\n    print(i, results)","8aa40018":"top_76_features = featureScore.Features.to_list()[:76]","6bfa0a8e":"X_train4, X_test4, y_train4, y_test4 = train_test_split(df[top_76_features], y, test_size=0.3, random_state=42)\n\nstandard_scaler.fit(X_train4)\nX_train4 = standard_scaler.transform(X_train4)\nstandard_scaler.fit(X_test4)\nX_test4 = standard_scaler.transform(X_test4)","69a72830":"model.fit(X_train4, y_train4)\npred4 = model.predict(X_test4)\nsklearn.metrics.roc_auc_score(y_test4, pred4)","507c4c45":"from sklearn.feature_selection import mutual_info_classif","2c47452b":"mutual_info =  mutual_info_classif(X, y)","c8e49549":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info = mutual_info.sort_values(ascending=False)","6eca1c8f":"top_33_features = mutual_info.index[:33].to_list()","cae4748a":"X_train5, X_test5, y_train5, y_test5 = train_test_split(df[top_33_features], y, test_size=0.3, random_state=42)","92d28490":"standard_scaler.fit(X_train5)\nX_train5 = standard_scaler.transform(X_train5)\nstandard_scaler.fit(X_test5)\nX_test5 = standard_scaler.transform(X_test5)","8d5695f7":"model.fit(X_train5, y_train5)\npred5 = model.predict(X_test5)\nsklearn.metrics.roc_auc_score(y_test5, pred5)","427bfbdb":"top_76_features","79758ef6":"### 1.1 Selecting one features from features with similar Score","1b2654a8":"## Model AUC with all the features included(base)\n## We are using XGBOOST, as it has overall good performance in this type of data.","945e5409":"### 1.1.2 Using every 2nd columns","d85fb115":"## 1.Feature selection using sklearn's SelectKBest","963f7aa1":"### 1.1.3 Using every 4th columns","26afb980":"#  **Conclusion**\n\n\n### Machine learning algorithm doesn't understand what feature means and what they effect in decision making, what it understand is numbers and the patterns within it. So, after trying different combinations and few selection techniqies, the highest Area Under Curve accuracy in acheived was 92.86% with the Sklearn's SelectKBest's top 76 features according to their features score in Chi square Test. All features are given below.","1bb9791e":"## Using different transformers to see which one give the highest AUC","f9dd11ee":"### There are so many features that correlations matrix doesn't make much sense here.","43d4fdc7":"# Feature Selection Using Information Gain","fcc6ab4d":"### 1.1.1 Using every 3rd column","903fd03a":"## **So using every 2nd column gives us the best AUC score**\n====================================================================================================================================","a6eabe62":"### 1.1.4 Using random top features"}}