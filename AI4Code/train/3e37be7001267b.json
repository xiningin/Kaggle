{"cell_type":{"77da4262":"code","fc3cd465":"code","8dc6937c":"code","98eec55d":"code","c4477366":"code","cfb72aa0":"code","a18e3272":"code","64148a9c":"code","7dda2c3d":"code","d8cd68b4":"code","1e75e03e":"code","64c85b20":"code","9f201509":"code","140ca2d2":"code","3da9fd5c":"code","cf7511b8":"code","2978c32b":"code","90e6c980":"code","4f8640e5":"code","696a5ac9":"code","c3638ec4":"code","2eadd579":"code","5ad37685":"code","37baaf94":"code","d53b1d7b":"code","d32bf18b":"code","039ed9bf":"code","fd07f175":"markdown"},"source":{"77da4262":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc3cd465":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as pld\nimport seaborn as sns \nfrom datetime import datetime\nfrom datetime import timedelta","8dc6937c":"transaction_dataset = pd.read_csv(\"..\/input\/hangikredi-hiring-challange\/transactions.csv\")","98eec55d":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head ######################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail ######################\")\n    print(dataframe.tail(head))\n    print(\"###################### NA #######################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","c4477366":"check_df(transaction_dataset)","cfb72aa0":"transaction_dataset.nunique()","a18e3272":"transaction_dataset.value_counts()","64148a9c":"transaction_dataset[\"date\"] = pd.to_datetime(transaction_dataset[\"ga_datehour\"].astype('str').str[:8], format = '%Y%m%d')\ntransaction_dataset[\"hour\"] = transaction_dataset[\"ga_datehour\"].astype('str').str[8:10]","7dda2c3d":"transaction_dataset_sub = transaction_dataset[(pd.DatetimeIndex(transaction_dataset['date']).year == 2021) & (pd.DatetimeIndex(transaction_dataset['date']).month.isin([5,6,7]))]\nprint(transaction_dataset_sub.head())\nprint(transaction_dataset_sub.info())\ntransaction_dataset_sub.reset_index(drop = True)","d8cd68b4":"print(transaction_dataset_sub[transaction_dataset_sub['date'] == '2021-06-25'])\ntransaction_dataset_grouped = transaction_dataset_sub.groupby(['date', 'ga_products', 'ga_channels'], as_index = False)['ga_itemquantity'].sum()\nprint(transaction_dataset_grouped.head(20))","1e75e03e":"transaction_join = transaction_dataset_grouped.groupby(['date', 'ga_products'], as_index = False)['ga_itemquantity'].sum()\ntransaction_join['join'] = transaction_join['date'] + timedelta(days = 1)\ntransaction_join.drop(columns = ['date'], inplace = True)\nprint(transaction_join.head(10))","64c85b20":"transaction_dataset_grouped = transaction_dataset_grouped.merge(transaction_join, left_on = ['ga_products','date'], right_on = ['ga_products','join'], how = 'left')\nprint(transaction_dataset_grouped.tail(10))","9f201509":"transaction_dataset_grouped.drop(columns = ['join'], inplace = True)\nprint(transaction_dataset_grouped.tail(10))","140ca2d2":"transaction_dataset_y = transaction_dataset_grouped.groupby(['date', 'ga_channels'], as_index = False)['ga_itemquantity_x'].sum()\nprint(transaction_dataset_y.tail(20))","3da9fd5c":"print(transaction_dataset_grouped.tail(20))\ntransaction_dataset_grouped_v2 = transaction_dataset_grouped.pivot_table(index = ['date', 'ga_channels'], columns = 'ga_products', values = 'ga_itemquantity_y', aggfunc = 'first').reset_index().rename_axis(None, axis = 1)\ntransaction_dataset_grouped_v2['total'] = transaction_dataset_grouped_v2.iloc[:, -6:].sum(axis=1)\nprint(transaction_dataset_grouped_v2.tail(20))","cf7511b8":"transaction_dataset_grouped_v2['a_day_pct'] = transaction_dataset_grouped_v2['Product A']\/(transaction_dataset_grouped_v2['total'])\ntransaction_dataset_grouped_v2['b_day_pct'] = transaction_dataset_grouped_v2['Product B']\/(transaction_dataset_grouped_v2['total'])\ntransaction_dataset_grouped_v2['c_day_pct'] = transaction_dataset_grouped_v2['Product C']\/(transaction_dataset_grouped_v2['total'])\ntransaction_dataset_grouped_v2['d_day_pct'] = transaction_dataset_grouped_v2['Product D']\/(transaction_dataset_grouped_v2['total'])\ntransaction_dataset_grouped_v2['e_day_pct'] = transaction_dataset_grouped_v2['Product E']\/(transaction_dataset_grouped_v2['total'])\ntransaction_dataset_grouped_v2['f_day_pct'] = transaction_dataset_grouped_v2['Product F']\/(transaction_dataset_grouped_v2['total'])\nprint(transaction_dataset_grouped_v2.tail(10))","2978c32b":"transaction_dataset_final = transaction_dataset_grouped_v2.merge(transaction_dataset_y, on = ['date', 'ga_channels'], how = 'inner')\ntransaction_dataset_final['weekday'] = pd.DatetimeIndex(transaction_dataset_final['date']).weekday\ntransaction_dataset_final['month'] = pd.DatetimeIndex(transaction_dataset_final['date']).month\ntransaction_dataset_final['quarter'] = pd.DatetimeIndex(transaction_dataset_final['date']).quarter\nprint(transaction_dataset_final.tail(10))","90e6c980":"transaction_dataset_final.drop(columns = ['Product A', 'Product B', 'Product C', 'Product D', 'Product E', 'Product F'], inplace = True)\nprint(transaction_dataset_final.head())","4f8640e5":"transaction_dataset_grouped['proda'] = transaction_dataset_grouped.groupby(['date', 'ga_products'])['ga_itemquantity_y'].transform('sum')\neconomic_calendar = pd.read_csv(\"..\/input\/hangikredi-hiring-challange\/economic_calendar.csv\")\nprint(economic_calendar[economic_calendar['priority'].isin([3]) & (economic_calendar['expectation'].isnull() == False)].head(5))\nprint(economic_calendar[economic_calendar['priority'].isin([3]) & (economic_calendar['expectation'].isnull() == False)].info())\nprint(economic_calendar[economic_calendar['priority'].isin([3]) & (economic_calendar['expectation'].isnull() == False)].nunique())","696a5ac9":"economic_calendar_sub = economic_calendar[economic_calendar['priority'].isin([3,2,1]) & (economic_calendar['expectation'].isnull() == False)]\neconomic_calendar_sub['cty_ind'] = economic_calendar_sub['country'].astype(str) + '||' + economic_calendar_sub['indicator'].astype(str)\neconomic_calendar_sub.drop(columns = ['pk', 'time', 'country', 'indicator','explained'], inplace = True)\nprint(economic_calendar_sub.head())\nprint(economic_calendar_sub['expectation'].value_counts().head(20))\n#economic_calendar_sub_v2 = economic_calendar_sub.pivot_table(index = ['date'], columns = 'cty_ind', values = 'expectation', aggfunc = 'first').reset_index().rename_axis(None, axis = 1)\n#print(economic_calendar_sub_v2.head(10))\neconomic_calendar_sub['is_neg'] = economic_calendar_sub['expectation'].astype(str).str.contains('-')\nprint(economic_calendar_sub.head(20))","c3638ec4":"economic_calendar_sub_test = pd.DataFrame()\neconomic_calendar_sub_test = economic_calendar_sub\neconomic_calendar_sub_test['prio_neg'] = economic_calendar_sub['priority'].astype(str) + '||' + economic_calendar_sub['is_neg'].astype(str)\neconomic_calendar_sub_test.drop(columns = ['priority', 'expectation', 'previous', 'cty_ind', 'is_neg'], inplace = True)\neconomic_calendar_sub_test = pd.DataFrame(economic_calendar_sub_test)\nprint(economic_calendar_sub_test)","2eadd579":"economic_calendar_sub_test_v2 = pd.pivot_table(economic_calendar_sub_test, index = 'date', columns = 'prio_neg', aggfunc = 'size', fill_value = 0)\nprint(economic_calendar_sub_test_v2)","5ad37685":"economic_variables = pd.read_csv(\"..\/input\/hangikredi-hiring-challange\/economic_variables.csv\")","37baaf94":"print(economic_variables.sort_values(by = ['date', 'hour']).head(20))\nprint(economic_variables.info())\nprint(economic_variables.nunique())","d53b1d7b":"economic_variables_grouped = economic_variables.groupby(['date'], as_index = False)['bist100', 'usdtry', 'eurtry', 'eurusd', 'faiz', 'xau', 'brent', 'bky'].mean()\neconomic_variables_grouped = economic_variables_grouped.sort_values(by = ['date'])\neconomic_variables_grouped[['bist100_yest', 'usdtry_yest', 'eurtry_yest', 'eurusd_yest', 'faiz_yest', 'xau_yest', 'brent_yest', 'bky_yest']] = economic_variables_grouped[['bist100', 'usdtry', 'eurtry', 'eurusd', 'faiz', 'xau', 'brent', 'bky']].shift(1)\nprint(economic_variables_grouped.head(20))","d32bf18b":"economic_variables_grouped_v2 = economic_variables_grouped\neconomic_variables_grouped_v2['bist100_chg'] = (economic_variables_grouped['bist100'] - economic_variables_grouped['bist100_yest'])\/(economic_variables_grouped['bist100_yest'])\neconomic_variables_grouped_v2['usdtry_chg'] = (economic_variables_grouped['usdtry'] - economic_variables_grouped['usdtry_yest'])\/(economic_variables_grouped['usdtry_yest'])\neconomic_variables_grouped_v2['eurtry_chg'] = (economic_variables_grouped['eurtry'] - economic_variables_grouped['eurtry_yest'])\/(economic_variables_grouped['eurtry_yest'])\neconomic_variables_grouped_v2['eurusd_chg'] = (economic_variables_grouped['eurusd'] - economic_variables_grouped['eurusd_yest'])\/(economic_variables_grouped['eurusd_yest'])\neconomic_variables_grouped_v2['faiz_chg'] = (economic_variables_grouped['faiz'] - economic_variables_grouped['faiz_yest'])\/(economic_variables_grouped['faiz_yest'])\neconomic_variables_grouped_v2['xau_chg'] = (economic_variables_grouped['xau'] - economic_variables_grouped['xau_yest'])\/(economic_variables_grouped['xau_yest'])\neconomic_variables_grouped_v2['brent_chg'] = (economic_variables_grouped['brent'] - economic_variables_grouped['brent_yest'])\/(economic_variables_grouped['brent_yest'])\neconomic_variables_grouped_v2['bky_chg'] = (economic_variables_grouped['bky'] - economic_variables_grouped['bky_yest'])\/(economic_variables_grouped['bky_yest'])\neconomic_variables_grouped_v2.drop(columns = ['bist100_yest', 'usdtry_yest', 'eurtry_yest', 'eurusd_yest', 'faiz_yest', 'xau_yest', 'brent_yest', 'bky_yest', 'bist100', 'usdtry', 'eurtry', 'eurusd', 'faiz', 'xau', 'brent', 'bky'], inplace = True)\nprint(economic_variables_grouped_v2)","039ed9bf":"live_digital_campaigns = pd.read_csv(\"..\/input\/hangikredi-hiring-challange\/live_digital_campaigns.csv\")\nprint(live_digital_campaigns.head())\nprint(live_digital_campaigns.info())\nprint(live_digital_campaigns.nunique())","fd07f175":"Data Scientist Hiring Challenge\n\nBackground\nAn E-commerce website has hundreds of thousands of visitors everyday. The visitors come from several marketing channels such as digital campaigns on social media, referrals from publishers, organic search and CRM.\n\n\nThe main business here is collecting traffic from different sources\/channels and converting those visitors to leads. A marketing lead is a person who shows interest in a brand's products or services, which makes a visitor a potential customer for the seller\/service provider. The primary goal of any company is to generate as many leads as possible to ultimately increase conversion rates in the sales funnel.\n\n\nThe main aim in the challenge is to develop a model that can forecast the next day number of leads.\n\n \n\nDatasets and Features\nThere is one main dataset and three others as auxiliary:\n\nTransactions dataset is the main dataset that stores the number of leads for each minute broken down according to products and channels. The data goes back till 29.09.2020.\nFeatures:\n\npk: primary key and unique ID in the database table\n\nga_transactionid: the id of the transaction from google Analytics\n\nga_datehour: the time of the transaction in yyyymmddHH format\n\nga_products: name of the products (Product A, Product B, Product C, Product D, Product E, Product F)\n\nga_channels: the channel a visitor comes for (Facebook, Google Ads, Organic search, Direct, CRM)\n\nga_itemquantity: number of leads\n\n\nEconomic calendar dataset keeps record of all events that may affect economic variables such as currency exchange rate, interest rate, and stockes in the market.\nFeatures:\n\npk: primary key and unique ID in the database table  \n\ndate: starts from 28.04.2021\n\ntime: when the event takes place\n\ncountry: where the event happened\n\nindicator: the name of the event\n\npriority: there are three levels (1, 2, 3) where 3 is the highest priority\n\nexception: anticipated market impact \n\nprevious: represents the previous market impact either positive or negative\n \n\nEconomic variables dataset observes and keeps track of the changes in terms of important variables such as USDTRY or BIST100. The dataset stores the variables daily at three different hours (09, 12, 15) hrs.\nFeatures:\n\npk: primary key and unique ID in the database table\n\ndate: starts from 28.04.2021\n\nhour: (09, 12, 15) hrs\n\nbist100: Borsa Istanbul stock exchange\n\nusdtry: usd and try exchange rate\n\neurtry: eur and try exchange rate\n\neurusd: eur and usd exchange rate\n\nfaiz: interest rate in Turkey\n\nxau: gold price in ounce\n\nbrent: Atlantic basin crude oils price\n\n\nLive Digital campaigns dataset that has the number of live digital campaigns for everyday since 29.09.2020.\nFeatures\n\ndate: since 29.09.2020\n\nlive_campaigns: numeric value of the number of campaign\n\n\nTasks\n\nGive some analysis on the relationship between the economical events and variables and their impacts on the daily number of visitors.\n\nUsing Transaction dataset, forecast the next day leads for each channel (Facebook, CRM and so on).\n\nGiven the number of live digital campaigns and other auxiliary datasets, try to optimise the performance of your forecasting model (or even develop a new model).\n\nDeliverables\n\n\nWrite your solution on jupyter notebooks for each task (analysis and model development) and make it clear you explain what you are doing properly.\n\n\nYour jupyter notebooks for each task should be named in the following format: Task1.ipynb, Task2.ipynb and Task3.ipynb\n\n\nMake sure that your code is replicable and you document your approach and code in a clear way."}}