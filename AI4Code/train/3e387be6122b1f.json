{"cell_type":{"ac28d0f3":"code","7a601dd0":"code","8ef6bdeb":"code","51f93fc2":"code","37959017":"code","39cfefed":"code","39970dd1":"code","4b96fcda":"code","60a395ad":"code","c41bfd0e":"code","d5b911bb":"code","101a0914":"code","ee0df03d":"code","f83eee6a":"code","d6a56e30":"code","d40c307a":"code","f336985a":"code","3d68afb6":"code","cb5d6dc9":"code","41952bdd":"code","f320798b":"code","69c8e383":"code","6c7a97ad":"code","5113e176":"code","5c71fa0f":"code","e1c3205d":"code","b5438415":"code","ff4e0817":"code","3fd2f092":"code","8205ca4c":"code","37db830c":"code","9429e440":"code","a8027911":"code","18d2122c":"code","29728c77":"code","89e901b4":"code","ca205c4f":"code","2e48ee08":"code","abda5e42":"code","5cdabf40":"code","25f0fb4f":"code","a7db44d0":"code","831deb3b":"code","e66769ee":"code","c7a322d0":"code","77e3cc1b":"code","0e6bfe8c":"code","40613dee":"code","fefaa263":"code","292f955e":"code","44a619d7":"markdown","66425547":"markdown","e056716d":"markdown","672fab3a":"markdown","45afa711":"markdown","e833a9b6":"markdown","2ed0cd54":"markdown","1eb1bd37":"markdown","64160917":"markdown","1d2ef4f0":"markdown","2b987f84":"markdown","3e44b546":"markdown","7c04c36c":"markdown","769853a2":"markdown","9ef3e19f":"markdown","3a8f4822":"markdown","3bec61f8":"markdown","e481b143":"markdown","01b7c9c9":"markdown","d88b1a01":"markdown","36ee94df":"markdown"},"source":{"ac28d0f3":"# Import library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport gc\nfrom sklearn.metrics import mean_squared_error\nimport statsmodels.api as sm\nimport lightgbm as lgb\nplt.style.use('ggplot')\n%matplotlib inline\nseed = 433","7a601dd0":"#path ='dataset\/'\npath ='..\/input\/'\ntrain = pd.read_csv(path + 'train.csv',parse_dates=[0],nrows=None)\ntest = pd.read_csv(path+ 'test.csv',parse_dates=[1], nrows=None )\nprint('Number of rows and columns in train dataset are:',train.shape)\nprint('Number of rows and columns in test dataset are:', test.shape)","8ef6bdeb":"def basic_details(df):\n    \"\"\"Find number of missing value,dtyeps, unique value in \n    dataset\"\"\"\n    k = pd.DataFrame()\n    k['Missing value'] = df.isnull().sum()\n    k['% Missing value'] = df.isnull().sum()\/df.shape[0]\n    k['dtype'] = df.dtypes\n    k['N unique'] = df.nunique()\n    return k","51f93fc2":"def agg_stats(df,statistics,groupby_column):\n    \"\"\"Aggregate a column by unit sales statistics such as \n    'mean','sum','min','max', 'var', 'std',\"\"\"\n    f,ax = plt.subplots(3,2,figsize=(14,8))\n    ax =ax.ravel()\n    for i,s in enumerate(statistics):\n        tmp = (df\n         .groupby(groupby_column)\n         .agg({'sales':s})\n         )\n        tmp.columns = ['sales_{}'.format(s)]\n        sns.lineplot(x=tmp.index, y = tmp.iloc[:,0],color='blue',ax=ax[i])\n        ax[i].set_xticks(tmp.index)\n        for ticks in ax[i].get_xticklabels(): ticks.set_rotation(90)\n        #plt.xticks(rotation=90)\n        ax[i].set_title('sales_{}'.format(s))\n        ax[i].set_ylabel('')\n    plt.tight_layout()","37959017":"### date_time_feat\ndef date_time_feat(df,column):\n    \"Extract date time feature\"\n    df['day'] = df[column].dt.day\n    df['dayofweek'] = df[column].dt.dayofweek\n    df['month'] = df[column].dt.month\n    df['year'] = df[column].dt.year\n    \n    df['is_month_end'] = df[column].dt.is_month_end.astype('int8')\n    df['is_month_start'] = df[column].dt.is_month_start.astype('int8')\n    df['weekofyear'] = df[column].dt.weekofyear\n    # conver to category\n    #df['dayofweek'] = pd.Categorical(df['dayofweek'],\n     #       categories=['Monday','Tuesday', 'Wednesday', 'Thursday', 'Friday','Saturday', 'Sunday',])\n","39cfefed":"# Reduce memory of dataset\ndef reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe \"\"\"\n    intial_memory = df.memory_usage().sum()\/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()\/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')","39970dd1":"train.head()","4b96fcda":"test.head()","60a395ad":"basic_details(test) # test dataset","c41bfd0e":"train.describe() # descriptive statistics about features","d5b911bb":"print('Time series start time: \"{}\" and end time: \"{}\"'.format(train['date'].min(), train['date'].max()))\nprint('Time series start time: \"{}\" and end time: \"{}\"'.format(test['date'].min(), test['date'].max()))","101a0914":"# Generate date time feature\ndate_time_feat(train,'date')\ndate_time_feat(test,'date')\ntrain.head()","ee0df03d":"plt.figure(figsize=(14,4))\ntrain.set_index('date')['sales'].plot(kind='line')","f83eee6a":"f,ax = plt.subplots(1,3,figsize=(14,4))\nsns.distplot(train['sales'],ax =ax[0])\nsns.distplot(np.log(train['sales']+1),ax=ax[1], color='b')\nsns.boxenplot(train['sales'],ax =ax[2])","d6a56e30":"(train\n .groupby(['year',])\n .agg({'sales':['sum',]})\n .unstack()\n .plot(kind='bar',cmap='viridis'))","d40c307a":"(train\n .groupby(['month','year'])\n .agg({'sales':'mean'})\n .unstack()\n .plot(figsize=(14,5)))","f336985a":"(train\n .groupby(['dayofweek','year'])\n .agg({'sales':'mean'})\n .unstack()\n .plot(figsize=(14,5)))","3d68afb6":"(train\n.groupby(['day'])\n.agg({'sales':['mean','max']})\n.plot(figsize=(14,4),kind='bar',stacked=True,cmap='coolwarm'))","cb5d6dc9":"agg_stats(train,statistics=['mean','sum','min','max', 'var', 'count'],groupby_column=['day'])","41952bdd":"(train.groupby('month')\n.agg({'sales':['min','mean','max']})\n .plot(figsize=(14,4),kind='bar',stacked=True))","f320798b":"agg_stats(train,statistics=['mean','sum','min','max', 'var', 'count'],groupby_column=['month'])","69c8e383":"(train\n .groupby(['store','month'])\n .agg({'sales':['sum']})\n .unstack()\n .plot(figsize=(14,3),kind='box',stacked=True,cmap='viridis'))\nplt.xticks(rotation=90);","6c7a97ad":"(train\n .groupby(['store','dayofweek'])\n .agg({'sales':['sum']})\n .unstack()\n .plot(figsize=(14,3),kind='box',stacked=True,cmap='viridis'))\nplt.xticks(rotation=90);","5113e176":"(train\n .groupby(['store','year'])\n .agg({'sales':['sum']})\n .unstack()\n .plot(figsize=(14,3),kind='box',stacked=True,cmap='viridis'))\nplt.xticks(rotation=90);","5c71fa0f":"(train\n .groupby('store')\n .agg({'sales':['min','mean','max']})\n .plot(figsize=(14,4),kind='bar',stacked=True,cmap='magma'))","e1c3205d":"agg_stats(train,statistics=['mean','sum','min','max', 'var', 'count'],groupby_column=['store'])","b5438415":"(train\n .groupby('item')\n .agg({'sales':['min','mean','max']})\n .plot(figsize=(14,4),kind='bar',stacked=True,cmap='viridis'))","ff4e0817":"agg_stats(train,statistics=['mean','sum','min','max', 'var', 'count'],groupby_column=['item'])","3fd2f092":"(train\n .groupby(['item','month'])\n .agg({'sales':['sum']})\n .unstack()\n .plot(figsize=(14,4),kind='box',stacked=True,cmap='magma'))\nplt.xticks(rotation=90);","8205ca4c":"(train\n .groupby(['item','store'])\n .agg({'sales':'mean'})\n .unstack()\n .plot(figsize=(14,5),kind='line'))\nplt.savefig('agg.png')","37db830c":"train1 =train.copy()\ntrain1['month'] = train1['date'].dt.month_name()\nplt.figure(figsize=(14,6))\npd.plotting.parallel_coordinates(train1[['dayofweek','store','sales','item','month']][:1000]\n                                 ,'month',colormap='rainbow')\ndel train1","9429e440":"plt.figure(figsize=(14,5))\ntrain['sales'].head(1000).plot(color='darkgray')\ntrain['sales'].head(1000).rolling(window=12).mean().plot(label='mean')\n#train['sales'].head(1000).rolling(window=12).median().plot(label='median')\ntrain['sales'].head(1000).rolling(window=7).min().plot(label='min',color='g')\ntrain['sales'].head(1000).rolling(window=7).max().plot(label='max',color='b')\ntrain['sales'].head(1000).rolling(window=7).std().plot(label='std',color='yellow')\nplt.legend()\n#plt.savefig('Rolling window.png')","a8027911":"# Expanding window\nplt.figure(figsize=(14,5))\ntrain['sales'].head(1000).plot(color='darkgray')\ntrain['sales'].head(1000).expanding().mean().plot(label='mean')\n#train['sales'].head(1000).rolling(window=12).median().plot(label='median')\ntrain['sales'].head(1000).expanding().min().plot(label='min',color='g')\ntrain['sales'].head(1000).expanding().max().plot(label='max',color='b')\ntrain['sales'].head(1000).expanding().std().plot(label='std',color='yellow')\nplt.legend()","18d2122c":"# Claculate groupby statics for lag date \ndef calc_stats(df, end,window,groupby=None,aggregates='mean',value='sales'):\n    \n    # dates\n    last_date = pd.to_datetime(end) - pd.Timedelta(days=1)\n    first_date = pd.to_datetime(end) - pd.Timedelta(days= window)\n    # Aggregate\n    df1 = df[(df.date >=first_date) & (df.date<= last_date) ]\n    df_agg = df1.groupby(groupby)[value].agg(aggregates)\n    # Change name of columns\n    df_agg.name =  str(end).split(' ')[0]+'_' + '_'.join(groupby)+'_'+aggregates+'_'+ str(window)\n    return df_agg.reset_index()\n\n#sales_by_store_item\ndef sales_by_store_item(df, end, aggregates='mean', value='sales'):\n    \n    print('Adding sales by store item')\n    data = calc_stats(df,end, window=1,aggregates=aggregates, \n                      groupby=['store','item'], value=value)\n    print('window 1 added')\n    \n    for window in  [3,7,14,28,90,180,365]:\n        agg = calc_stats(df,end, window=window, aggregates=aggregates,\n                         groupby=['store','item'], value=value )\n        data = pd.merge(data,agg)\n        print('window %d added'% window)\n    return data\n\n# sales by store item dayofweek\ndef sales_by_store_item_dayofweek(df, end, aggregates='mean', value='sales'):\n    \n    print('Adding sales by store item dayofweek')\n    data = calc_stats(df,end, window=7, aggregates=aggregates,\n                      groupby = ['store','item','dayofweek'], value=value)\n    print('window 7 added')\n    \n    for window in  [14,28,28*2,28*3,28*6,28*12]:\n        agg = calc_stats(df,end, window=window, aggregates=aggregates,\n                         groupby=['store','item','dayofweek'], value=value )\n        data = pd.merge(data,agg)\n        print('window %d added'% window)\n    return data\n\n# sales_by_store_item_day\ndef sales_by_store_item_day(df, end, aggregates='mean', value='sales'):\n    \n    print('Adding sales by store item day')\n    data = calc_stats(df,end, window=365, aggregates=aggregates,\n                      groupby = ['store','item','day'], value=value)\n    print('window 365 added')\n    \n    return data\n\n# Sales by item\ndef sales_by_item(df, end, aggregates='mean', value='sales'):\n    \n    print('Adding sales by item ')\n    data = calc_stats(df,end, window=7, aggregates=aggregates,\n                      groupby = ['item'], value=value)\n    print('window 7 added')\n    \n    for window in  [14,28,28*2]:\n        agg = calc_stats(df,end, window=window, aggregates=aggregates,\n                         groupby=['item'], value=value )\n        data = pd.merge(data,agg)\n        print('window %d added'% window)\n    return data","29728c77":"def calc_roll_stat(df,end,groupby=None,window=1,aggregate='mean'):\n    # Rolling statistics method\n    last_date = pd.to_datetime(end) - pd.Timedelta(days=1)\n    first_date = pd.to_datetime(end) - pd.Timedelta(days=window)\n    df1 = df[(df.date >= first_date) & (df.date <= last_date)]\n    \n    dfPivot = df1.set_index(['date']+groupby)['sales'].unstack().unstack()\n    dfPivot = dfPivot.rolling(window=window).mean().fillna(method='bfill')\n    return dfPivot.stack().stack().rename(aggregate+str(window))\n\ndef calc_expand_stat(df,end,window=1,aggregate='mean'):\n    # Expanding statistics method\n    last_date = pd.to_datetime(end) - pd.Timedelta(days=1)\n    first_date = pd.to_datetime(end) - pd.Timedelta(days=window)\n    df1 = df[(df.date >= first_date) & (df.date <= last_date)]\n    \n    dfPivot = df1.set_index(['date','store','item'])['sales'].unstack().unstack()\n    dfPivot = dfPivot.expanding(min_periods=window).mean().fillna(method='bfill')\n    dfPivot = dfPivot.stack().stack().rename(aggregate+'_'+str(window)).reset_index()\n    return dfPivot\n\ndef sales_by_store_item_expading(df,end,aggregate = 'mean', value = 'sales'):\n    print('Adding sales by expanding')\n    data =calc_expand_stat(df,end,window=3, aggregate='mean')\n    return data\n# https:\/\/stackoverflow.com\/questions\/25917287\/pandas-groupby-expanding-mean-by-column-value","89e901b4":"def create_data1(sales,test,date):\n    \n    # Date input\n    for i in range(2):\n        end = pd.to_datetime(date) - pd.Timedelta(days=7*i+1)\n        print(end)\n    \n        # Rolling feature\n        #for aggregates in ['mean','min','max','sum','std']:\n        for aggregates in ['mean','sum']:\n\n            # store\/item\n            print('-'*20+'Aggregate by '+aggregates+'-'*20)\n            data = sales_by_store_item(sales,end, aggregates=aggregates,value='sales')\n            sales = pd.merge(sales,data,on=['store','item'],how='left')\n            test = pd.merge(test,data,on=['store','item'], how='left')\n\n            # store\/item\/dayofweek\n            df = sales_by_store_item_dayofweek(sales,end, aggregates=aggregates,value='sales')\n            #data = pd.merge(data,df,)\n            sales = pd.merge(sales,df,on=['store','item','dayofweek'],how='left')\n            test = pd.merge(test,df,on=['store','item','dayofweek'], how='left')\n\n            # store\/item\/day\n            df = sales_by_store_item_day(sales,end, aggregates=aggregates,value='sales')\n            #data = pd.merge(data,df)\n            sales = pd.merge(sales,df,on=['store','item','day'],how='left')\n            test = pd.merge(test,df,on=['store','item','day'], how='left')\n\n            # sales\/item\n            df = sales_by_item(sales,end, aggregates=aggregates, value='sales')\n            data = pd.merge(data,df)\n            #data = pd.merge(sales,data)\n            sales = pd.merge(sales,df, on=['item'],how='left')\n            test = pd.merge(test,df, on=['item'], how='left')\n\n    return sales,test","ca205c4f":"#Time series start time: \"2013-01-01 00:00:00\" and end time: \"2017-12-31 00:00:00\"\n#Time series start time: \"2018-01-01 00:00:00\" and end time: \"2018-03-31 00:00:00\"\ntes_start = '2018-01-01'","2e48ee08":"# Rolling aggregation or lag feature for diffirend window size\ntrain1,test1 = create_data1(train,test,tes_start)","abda5e42":"train1['id'] = np.nan\ntrain1['is_train'] = True\ntest1['is_train'] = False\ntest1['sales'] = np.nan\n\n# concat train,test\ntrain_test = pd.concat([train1,test1],axis=0)\n\n#Log transform\ntrain_test['sales_log'] = np.log(train_test['sales']+1)\ngc.collect()\ntrain_test.shape\n\ndef one_hot_encoding(df,columns):\n    print('Original shape',df.shape)\n    df = pd.get_dummies(df,drop_first=True,columns=columns)\n    print('After OHE', df.shape)\n    return df","5cdabf40":"gc.collect()\ntrain_test = one_hot_encoding(train_test,columns=['month','dayofweek'])","25f0fb4f":"reduce_memory_usage(train_test)","a7db44d0":"#plt.figure(figsize=(14,10))\n#sns.heatmap(train_test1.corr(), cmap='coolwarm', annot=True,fmt='.2f')","831deb3b":"# Model\ncol_drop = ['id','is_train','sales','sales_log']\nX = train_test[train_test['is_train'] == True].drop(col_drop, axis=1)\ny = train_test[train_test['is_train'] == True]['sales_log']\ntest_new = train_test[train_test['is_train'] == False].drop(col_drop +['date'],axis=1)\n\n# Time series based split\n#Time series start time: \"2013-01-01 00:00:00\" and end time: \"2017-12-31 00:00:00\"\n#Time series start time: \"2018-01-01 00:00:00\" and end time: \"2018-03-31 00:00:00\"\ntra_start, tra_end = '2013-01-01','2016-12-31'\nval_start, val_end = '2017-01-01','2017-12-31'\ntes_start = '2018-01-01'\n\nX_train = X[X.date.isin(pd.date_range(tra_start,tra_end))].drop(['date'],axis=1)\nX_valid = X[X.date.isin(pd.date_range(val_start, val_end))].drop(['date'],axis=1)\ny_train = y[X.date.isin(pd.date_range(tra_start,tra_end))]\ny_valid = y[X.date.isin(pd.date_range(val_start, val_end))]\ngc.collect()\nX.shape,test_new.shape","e66769ee":"# SMAPE Systematic mean absolute Persent error\ndef smape(y_true,y_pred):\n    \n    n = len(y_pred)\n    masked_arr = ~((y_pred==0)&(y_true==0))\n    y_pred, y_true = y_pred[masked_arr], y_true[masked_arr]\n    nom = np.abs(y_true - y_pred)\n    denom = np.abs(y_true) + np.abs(y_pred)\n    smape = 200\/n * np.sum(nom\/denom)\n    return smape\ndef lgb_smape(pred,train_data):\n    ''' \n    Custom evaluvation function\n    '''\n    label = train_data.get_label()\n    smape_val = smape(np.expm1(pred), np.expm1(label))\n    return 'SMAPE',smape_val, False","c7a322d0":"def lgb_model(X_train, X_valid, y_valid, y_test,test_new):\n    lgb_param = {}\n    lgb_param['boosting_type'] ='gbdt'\n    lgb_param['max_depth'] = 7\n    lgb_param['num_leaves'] = 2**7\n    lgb_param['learning_rate'] = 0.05\n    #lgb_param['n_estimators'] = 3000\n    lgb_param['feature_fraction'] = 0.9\n    lgb_param['bagging_fraction'] = 0.9\n    lgb_param['lambda_l1'] = 0.06\n    lgb_param['lambda_l2'] =  0.1\n    lgb_param['random_state'] = seed\n    lgb_param['n_jobs'] = 4\n    lgb_param['silent'] = -1\n    lgb_param['verbose'] = -1\n    lgb_param['metric'] = 'mae'\n    \n    model = lgb.LGBMRegressor(**lgb_param)\n    lgb_train = lgb.Dataset(X_train,y_train)\n    lgb_valid = lgb.Dataset(X_valid,y_valid)\n    valid_set = [lgb_train,lgb_valid]\n    model = lgb.train(params=lgb_param,train_set=lgb_train,valid_sets=valid_set,num_boost_round= 300,\n                      feval=lgb_smape,early_stopping_rounds=20,)\n    print('-'*10,'*'*20,'-'*10)\n    #model.fit(X_train,y_train, eval_set= [(X_train,y_train),(X_valid,y_valid)],\n    #          eval_metric ='rmse',early_stopping_rounds=20,verbose=100)\n    \n    y_pred = model.predict(X_valid)\n    print('Root mean_squared_error','-'*20 ,np.sqrt(mean_squared_error(y_valid, y_pred)))\n    y_pred_new = model.predict(test_new)\n    return y_pred_new, model","77e3cc1b":"# Model training\ny_pred_new, model = lgb_model(X_train, X_valid, y_valid, y_valid,test_new)","0e6bfe8c":"#print('Root mean_squared_error',np.sqrt(mean_squared_error(y_test, y_pred)))","40613dee":"# Feature importance\nlgb.plot_importance(model,max_num_features=20);","fefaa263":"sns.distplot(y_pred_new)","292f955e":"y_pred_new1 = np.exp(y_pred_new)-1\nsubmit = pd.DataFrame({'id': test['id'], 'sales':(y_pred_new1)})\nsubmit.to_csv('store_submit.csv',index=False)\nsubmit.head()","44a619d7":"### 2.5 Rolling window","66425547":"The test dataset contains id column but train dataset does not contains id column. While importing dataset parse_date is assigned with perticular column index.","e056716d":"## 5.0 Model","672fab3a":"## Store item demand forcast","45afa711":"## 3.0 Data preprocessing","e833a9b6":"## 4.0 Model selection","2ed0cd54":"### 1.2 Load dataset","1eb1bd37":"### 2.2 Aggregate sales statistics by day","64160917":"### 2.1 Sales","1d2ef4f0":"### 6.0 Model evaluation","2b987f84":"### 3.0 Aggregate \/ Rolling function","3e44b546":"### 2.6 Expanding window","7c04c36c":"## Thank you","769853a2":"## 2.0 Exploratory data analysis\nGlimpse dataset","9ef3e19f":"### 1.3 Useful function","3a8f4822":"### 2.3 Store","3bec61f8":"### 3.1 One hot encoding","e481b143":"### 2.1 Date\nLet's extract day, week, month, year from date feature","01b7c9c9":"### 2.4 item","d88b1a01":"There are 50 diffirent item in 10 diffirent stores. The maximum number of items sold is 231 and average item sold is 52.25.","36ee94df":"### 1.0 Import library"}}