{"cell_type":{"e5a0820c":"code","95d70e38":"code","dd090f95":"code","588b3abe":"code","74e7aad0":"code","6a60f5d6":"code","76db6edb":"code","8399d588":"code","6aae17cb":"code","f08bcf47":"code","be989191":"code","8cd30294":"code","83d64489":"code","6321038f":"code","0f1dfa5c":"code","4cfde061":"code","07f281f4":"code","3467b881":"code","4a1e7328":"code","47db5c62":"code","484f954c":"code","4eb662e8":"code","c3aeff50":"code","fb0ef96d":"code","b97af3ca":"code","0421a7c6":"code","d06d1ef5":"code","761c8032":"code","803af6c6":"code","879b40b9":"code","2e24e18d":"code","2417ab0c":"code","ee424482":"code","086e17b6":"code","3a571984":"code","cbb4ce82":"code","455f8219":"code","4e9692b9":"code","72a345cf":"code","7a9189d0":"code","4f4c5193":"code","5baa28f2":"code","33fec48e":"code","35fdaa53":"code","7f26be63":"code","c5dd5f3e":"code","5e7e1703":"code","6e33a421":"code","9d211459":"code","1b81503c":"code","de394ee5":"code","7d043215":"code","70f3157b":"code","2e32ae91":"code","d4017794":"code","3e437c8f":"code","b1c602c5":"code","bc9a8b6a":"code","c1c17e06":"code","8e156588":"code","fbcfa875":"code","95447d54":"code","67cf0bf6":"code","25286480":"code","a65ecd01":"code","70d7751b":"code","24ae7cc9":"code","e81e1675":"code","aa812117":"code","b22bdb61":"code","8eaf6d42":"code","3993050d":"code","d7ae85e8":"code","4bfa58ff":"code","1480fc88":"code","f933e54a":"code","13b1e57b":"code","cd597898":"code","18e58ec5":"code","8cce8d6e":"markdown","9355b831":"markdown","d84058a7":"markdown","c5ebd307":"markdown","de44df40":"markdown","328b2b3d":"markdown","6517edde":"markdown","faa16b61":"markdown","1f7cdc69":"markdown","e04438a1":"markdown","f06ed2b0":"markdown","6574ee41":"markdown","9b833f0f":"markdown","f0c7de8a":"markdown","d85a942d":"markdown"},"source":{"e5a0820c":"# Dataset located here: https:\/\/www.kaggle.com\/rmisra\/news-category-dataset\/version\/2\n# Based on tutorial from https:\/\/www.kaggle.com\/divsinha\n\nimport pandas as pd\nimport json\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport pprint\npp = pprint.PrettyPrinter(indent=4)\n% matplotlib inline","95d70e38":"PATH = \"..\/input\/News_Category_Dataset_v2.json\"","dd090f95":"container = []\n\nwith open(PATH) as fr:\n    for line in fr.readlines():\n        row_obj = json.loads(line)\n        container.append(row_obj)\n\ndf = pd.DataFrame(data=container, columns=[\"category\", \"headline\", \"authors\", \"link\", \"short_description\", \"date\"])","588b3abe":"df.head(50)","74e7aad0":"def show_5_category(category):\n    return df[(df[\"category\"] == category)][:5]","6a60f5d6":"categories, categorical_data = df['category'].unique().tolist(), dict()\n\nfor category in categories:\n    categorical_data[\"df_{}\".format(category.lower().replace(\" \", \"\"))] = df[(df['category'] == category)]\n    \ncategorical_data.keys()","76db6edb":"# TODO Get this working. Should produce a sample of 5 of each category of article\ndf_samp5 = pd.DataFrame(columns=[\"category\", \"headline\", \"authors\", \"link\", \"short_description\", \"date\"])\n\nfor category in categorical_data.keys():\n    df_samp5.append(categorical_data[category][:5])\n\ndf_samp5","8399d588":"df.info()","6aae17cb":"df.describe()","f08bcf47":"df.isnull().sum()","be989191":"df['category'].value_counts().sort_values(ascending=False).plot","8cd30294":"df['category'].value_counts().sort_values(ascending=False).plot.bar","83d64489":"plt.figure(figsize=(25,15))\ncmapper = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\ndf['category'].value_counts().sort_values(ascending=False).plot.bar(color=cmapper)\nplt.xticks(rotation=50)\nplt.xlabel(\"Category of News\")\nplt.ylabel(\"Number of Articles\")","6321038f":"# TODO make option for image\n\ndef create_wordcloud(category):\n    text = \" \".join(desc for desc in categorical_data[category]['short_description'])\n    wordcloud = WordCloud(width=1500, height=800, max_font_size=200, background_color = 'white', stopwords = STOPWORDS).generate(text)\n    plt.figure(figsize=(20,15))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","0f1dfa5c":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","4cfde061":"# Count Vectorizer for entire model \ncvector = CountVectorizer(min_df = 0.0, max_df = 1.0, ngram_range=(1,2), stop_words = STOPWORDS)\ncvector.fit(df['headline'])","07f281f4":"len(cvector.get_feature_names())","3467b881":"def create_tf_matrix(category):\n    return cvector.transform(df[df.category == category].headline)","4a1e7328":"crime_matrix = create_tf_matrix('CRIME')\nentertainment_matrix = create_tf_matrix('ENTERTAINMENT')\nworld_news_matrix = create_tf_matrix('WORLD NEWS')\nimpact_matrix = create_tf_matrix('IMPACT')\npolitics_matrix = create_tf_matrix('POLITICS')\nweird_news_matrix = create_tf_matrix('WEIRD NEWS')\nblack_voices_matrix = create_tf_matrix('BLACK VOICES')\nwomen_matrix = create_tf_matrix('WOMEN')\ncomedy_matrix = create_tf_matrix('COMEDY')\nqueer_voices_matrix = create_tf_matrix('QUEER VOICES')\nsports_matrix = create_tf_matrix('SPORTS')\nbusiness_matrix = create_tf_matrix('BUSINESS')\ntravel_matrix = create_tf_matrix('TRAVEL')\nmedia_matrix = create_tf_matrix('MEDIA')\ntech_matrix = create_tf_matrix('TECH')\nreligion_matrix = create_tf_matrix('RELIGION')\nscience_matrix = create_tf_matrix('SCIENCE')\nlatino_voices_matrix = create_tf_matrix('LATINO VOICES')\neducation_matrix = create_tf_matrix('EDUCATION')\ncollege_matrix = create_tf_matrix('COLLEGE')\nparents_matrix = create_tf_matrix('PARENTS')\narts_and_culture_matrix = create_tf_matrix('ARTS & CULTURE')\nstyle_matrix = create_tf_matrix('STYLE')\ngreen_matrix = create_tf_matrix('GREEN')\ntaste_matrix = create_tf_matrix('TASTE')\nhealthy_living_matrix = create_tf_matrix('HEALTHY LIVING')\nthe_worldpost_matrix = create_tf_matrix('THE WORLDPOST')\ngood_news_matrix = create_tf_matrix('GOOD NEWS')\nworldpost_matrix = create_tf_matrix('WORLDPOST')\nfifty_matrix = create_tf_matrix('FIFTY')\narts_matrix = create_tf_matrix('ARTS')\nwellness_matrix = create_tf_matrix('WELLNESS')\nparenting_matrix = create_tf_matrix('PARENTING')\nhome_and_living_matrix = create_tf_matrix('HOME & LIVING')\nstyle_and_beauty_matrix = create_tf_matrix('STYLE & BEAUTY')\ndivorce_matrix = create_tf_matrix('DIVORCE')\nweddings_matrix = create_tf_matrix('WEDDINGS')\nfood_and_drink_matrix = create_tf_matrix('FOOD & DRINK')\nmoney_matrix = create_tf_matrix('MONEY')\nenvironment_matrix = create_tf_matrix('ENVIRONMENT')\nculture_and_arts_matrix = create_tf_matrix('CULTURE & ARTS')","47db5c62":"def create_term_freq(matrix):\n    category_words = matrix.sum(axis=0)\n    category_words_freq = [(word, category_words[0, idx]) for word, idx in cvector.vocabulary_.items()]\n    return pd.DataFrame(list(sorted(category_words_freq, key = lambda x: x[1], reverse=True)),columns=['Terms', 'Frequency'])","484f954c":"create_term_freq(crime_matrix), create_wordcloud('df_crime')","4eb662e8":"create_term_freq(entertainment_matrix).head(10), create_wordcloud('df_entertainment')","c3aeff50":"create_term_freq(world_news_matrix).head(10), create_wordcloud('df_worldnews')","fb0ef96d":"create_term_freq(impact_matrix).head(10), create_wordcloud('df_impact')","b97af3ca":"create_term_freq(politics_matrix).head(10), create_wordcloud('df_politics')","0421a7c6":"create_term_freq(weird_news_matrix).head(10), create_wordcloud('df_weirdnews')","d06d1ef5":"create_term_freq(black_voices_matrix).head(10), create_wordcloud('df_blackvoices')","761c8032":"create_term_freq(women_matrix).head(10), create_wordcloud('df_women')","803af6c6":"create_term_freq(comedy_matrix).head(10), create_wordcloud('df_comedy')","879b40b9":"create_term_freq(queer_voices_matrix).head(10), create_wordcloud('df_queervoices')","2e24e18d":"create_term_freq(sports_matrix).head(10), create_wordcloud('df_sports')","2417ab0c":"create_term_freq(business_matrix).head(10), create_wordcloud('df_business')","ee424482":"create_term_freq(travel_matrix).head(10), create_wordcloud('df_travel')","086e17b6":"create_term_freq(media_matrix).head(10), create_wordcloud('df_media')","3a571984":"create_term_freq(tech_matrix).head(10), create_wordcloud('df_tech')","cbb4ce82":"create_term_freq(religion_matrix).head(10), create_wordcloud('df_religion')","455f8219":"create_term_freq(science_matrix).head(10), create_wordcloud('df_science')","4e9692b9":"create_term_freq(latino_voices_matrix).head(10), create_wordcloud('df_latinovoices')","72a345cf":"create_term_freq(education_matrix).head(10), create_wordcloud('df_education')","7a9189d0":"create_term_freq(college_matrix).head(10), create_wordcloud('df_college')","4f4c5193":"create_term_freq(parents_matrix).head(10), create_wordcloud('df_parents')","5baa28f2":"create_term_freq(arts_and_culture_matrix).head(10), create_wordcloud('df_arts&culture')","33fec48e":"create_term_freq(style_matrix).head(10), create_wordcloud('df_style')","35fdaa53":"create_term_freq(green_matrix).head(10), create_wordcloud('df_green')","7f26be63":"create_term_freq(taste_matrix).head(10), create_wordcloud('df_taste')","c5dd5f3e":"create_term_freq(healthy_living_matrix).head(10), create_wordcloud('df_healthyliving')","5e7e1703":"create_term_freq(the_worldpost_matrix).head(10), create_wordcloud('df_theworldpost')","6e33a421":"create_term_freq(good_news_matrix).head(10), create_wordcloud('df_goodnews')","9d211459":"create_term_freq(worldpost_matrix).head(10), create_wordcloud('df_worldpost')","1b81503c":"create_term_freq(fifty_matrix).head(10), create_wordcloud('df_fifty')","de394ee5":"create_term_freq(arts_matrix).head(10), create_wordcloud('df_arts')","7d043215":"create_term_freq(wellness_matrix).head(10), create_wordcloud('df_wellness')","70f3157b":"create_term_freq(parenting_matrix).head(10), create_wordcloud('df_parenting')","2e32ae91":"create_term_freq(home_and_living_matrix).head(10), create_wordcloud('df_home&living')","d4017794":"create_term_freq(style_and_beauty_matrix).head(10), create_wordcloud('df_style&beauty')","3e437c8f":"create_term_freq(divorce_matrix).head(10), create_wordcloud('df_divorce')","b1c602c5":"create_term_freq(weddings_matrix).head(10), create_wordcloud('df_weddings')","bc9a8b6a":"create_term_freq(food_and_drink_matrix).head(10), create_wordcloud('df_food&drink')","c1c17e06":"create_term_freq(money_matrix).head(10), create_wordcloud('df_money')","8e156588":"create_term_freq(environment_matrix).head(10), create_wordcloud('df_environment')","fbcfa875":"create_term_freq(culture_and_arts_matrix).head(10), create_wordcloud('df_culture&arts')","95447d54":"headline = np.array(df['headline'])\ncategory = np.array(df['category'])\n# build train and test datasets\n\n# Train\/test splitting for 41 categories of news\nfrom sklearn.model_selection import train_test_split    \nheadline_train, headline_test, category_train, category_test = train_test_split(headline, category, test_size=0.2, random_state=41)","67cf0bf6":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n## Build Bag-Of-Words on train phrases\ncv = CountVectorizer(stop_words='english',max_features=10000)\ncv_train_features = cv.fit_transform(headline_train)","25286480":"# build TFIDF features on train reviews\ntv = TfidfVectorizer(min_df=0.0, max_df=1.0, ngram_range=(1,2),\n                     sublinear_tf=True)\ntv_train_features = tv.fit_transform(headline_train)","a65ecd01":"cv_test_features = cv.transform(headline_test)\ntv_test_features = tv.transform(headline_test)","70d7751b":"print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\nprint('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)","24ae7cc9":"from sklearn import metrics\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import clone\nfrom sklearn.preprocessing import label_binarize\nfrom scipy import interp\nfrom sklearn.metrics import roc_curve, auc \n\n\ndef get_metrics(true_labels, predicted_labels):\n    \n    print('Accuracy:', np.round(\n                        metrics.accuracy_score(true_labels, \n                                               predicted_labels),\n                        4))\n    print('Precision:', np.round(\n                        metrics.precision_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'),\n                        4))\n    print('Recall:', np.round(\n                        metrics.recall_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'),\n                        4))\n    print('F1 Score:', np.round(\n                        metrics.f1_score(true_labels, \n                                               predicted_labels,\n                                               average='weighted'),\n                        4))\n                        \n\ndef train_predict_model(classifier, \n                        train_features, train_labels, \n                        test_features, test_labels):\n    # build model    \n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features) \n    return predictions    \n\n\ndef display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n    \n    total_classes = len(classes)\n    level_labels = [total_classes*[0], list(range(total_classes))]\n\n    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n                                  labels=classes)\n    cm_frame = pd.DataFrame(data=cm, \n                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n                                                  labels=level_labels), \n                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n                                                labels=level_labels)) \n    print(cm_frame) \n    \ndef display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n\n    report = metrics.classification_report(y_true=true_labels, \n                                           y_pred=predicted_labels, \n                                           labels=classes) \n    print(report)\n    \n    \n    \ndef display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n    print('Model Performance metrics:')\n    print('-'*30)\n    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n    print('\\nModel Classification report:')\n    print('-'*30)\n    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n                                  classes=classes)\n    print('\\nPrediction Confusion Matrix:')\n    print('-'*30)\n    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n                             classes=classes)\n\n\ndef plot_model_decision_surface(clf, train_features, train_labels,\n                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n                                markers=None, alphas=None, colors=None):\n    \n    if train_features.shape[1] != 2:\n        raise ValueError(\"X_train should have exactly 2 columnns!\")\n    \n    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    clf_est = clone(clf)\n    clf_est.fit(train_features,train_labels)\n    if hasattr(clf_est, 'predict_proba'):\n        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n    else:\n        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n    \n    le = LabelEncoder()\n    y_enc = le.fit_transform(train_labels)\n    n_classes = len(le.classes_)\n    plot_colors = ''.join(colors) if colors else [None] * n_classes\n    label_names = le.classes_\n    markers = markers if markers else [None] * n_classes\n    alphas = alphas if alphas else [None] * n_classes\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y_enc == i)\n        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n                    label=label_names[i], cmap=cmap, edgecolors='black', \n                    marker=markers[i], alpha=alphas[i])\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n    \n    ## Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    if hasattr(clf, 'classes_'):\n        class_labels = clf.classes_\n    elif label_encoder:\n        class_labels = label_encoder.classes_\n    elif class_names:\n        class_labels = class_names\n    else:\n        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n    n_classes = len(class_labels)\n    y_test = label_binarize(true_labels, classes=class_labels)\n    if n_classes == 2:\n        if hasattr(clf, 'predict_proba'):\n            prob = clf.predict_proba(features)\n            y_score = prob[:, prob.shape[1]-1] \n        elif hasattr(clf, 'decision_function'):\n            prob = clf.decision_function(features)\n            y_score = prob[:, prob.shape[1]-1]\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n        \n        fpr, tpr, _ = roc_curve(y_test, y_score)      \n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n                                 ''.format(roc_auc),\n                 linewidth=2.5)\n        \n    elif n_classes > 2:\n        if hasattr(clf, 'predict_proba'):\n            y_score = clf.predict_proba(features)\n        elif hasattr(clf, 'decision_function'):\n            y_score = clf.decision_function(features)\n        else:\n            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        ## Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n        ## Compute macro-average ROC curve and ROC area\n        # First aggregate all false positive rates\n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n        # Finally average it and compute AUC\n        mean_tpr \/= n_classes\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        ## Plot ROC curves\n        plt.figure(figsize=(6, 4))\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n                 label='micro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n                 label='macro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n\n        for i, label in enumerate(class_labels):\n            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n                                           ''.format(label, roc_auc[i]), \n                     linewidth=2, linestyle=':')\n    else:\n        raise ValueError('Number of classes should be atleast 2 or more')\n        \n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","e81e1675":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(penalty='l2', max_iter=100, C=1)","aa812117":"lr_bow_predictions = train_predict_model(classifier=lr, \n                                             train_features=cv_train_features, train_labels=category_train,\n                                             test_features=cv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=lr_bow_predictions,\n                                      classes=df['category'].unique())","b22bdb61":"lr_tfidf_predictions = train_predict_model(classifier=lr, \n                                               train_features=tv_train_features, train_labels=category_train,\n                                               test_features=tv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=lr_tfidf_predictions,\n                                      classes=df['category'].unique())","8eaf6d42":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier(loss='hinge', n_iter=100)","3993050d":"sgd_bow_predictions = train_predict_model(classifier=sgd, \n                                             train_features=cv_train_features, train_labels=category_train,\n                                             test_features=cv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=sgd_bow_predictions,\n                                      classes=df['category'].unique())","d7ae85e8":"sgd_tfidf_predictions = train_predict_model(classifier=sgd, \n                                                train_features=tv_train_features, train_labels=category_train,\n                                                test_features=tv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=sgd_tfidf_predictions,\n                                      classes=df['category'].unique())","4bfa58ff":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs=-1)","1480fc88":"rfc_bow_predictions = train_predict_model(classifier=rfc, \n                                             train_features=cv_train_features, train_labels=category_train,\n                                             test_features=cv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=rfc_bow_predictions,\n                                      classes=df['category'].unique())","f933e54a":"rfc_tfidf_predictions = train_predict_model(classifier=rfc, \n                                                train_features=tv_train_features, train_labels=category_train,\n                                                test_features=tv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=rfc_tfidf_predictions,\n                                      classes=df['category'].unique())","13b1e57b":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)","cd597898":"nb_bow_predictions = train_predict_model(classifier=nb, \n                                             train_features=cv_train_features, train_labels=category_train,\n                                             test_features=cv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=nb_bow_predictions,\n                                      classes=df['category'].unique())","18e58ec5":"nb_tfidf_predictions = train_predict_model(classifier=nb, \n                                                train_features=tv_train_features, train_labels=category_train,\n                                                test_features=tv_test_features, test_labels=category_test)\ndisplay_model_performance_metrics(true_labels=category_test, predicted_labels=nb_tfidf_predictions,\n                                      classes=df['category'].unique())","8cce8d6e":"# Prediction Model\n## PSEUDO BRAINSTORM\n- [X] TF IDF analysis on headlines\n- [ ] TF IDF analysis on descriptions \n- [ ] Concatenate each headline vector with corresponding description vector\n- [ ] Multi class logistic regression\n- [X] Try 4 different classifiers (naive, svm, etc)","9355b831":"# Gettting Term Frequencies","d84058a7":"# Evaluation","c5ebd307":"# Naive Bayes on Bag-of-Words","de44df40":"# Naive Bayes on TF-IDF","328b2b3d":"# Logistic Regression model on Bag-of-Words","6517edde":"# PSEUDO BRAINSTORM\n- [X] Display graphs of commonly used words per category\n- [ ] Identify different writing styles, words used for various categories\n- [ ] Articles per year graph\n- [X] Articles per category graph","faa16b61":"# Wordcloud followed by top 10 words per category","1f7cdc69":"# SGD model on TF-IDF","e04438a1":"# Random Forest on TF-IDF","f06ed2b0":"# Setup for tokenizing","6574ee41":"# SGD model on Bag-of-Words","9b833f0f":"# Initial Data Exploration","f0c7de8a":"# Logistic Regression model on TF-IDF","d85a942d":"# Random Forest on Bag-of-Words"}}