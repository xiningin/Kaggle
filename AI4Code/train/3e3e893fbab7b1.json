{"cell_type":{"e1a412ef":"code","8ed6a8c1":"code","db16f7b5":"code","aa6fd4cf":"code","fcce41de":"code","278a975f":"code","ab7afff0":"code","4950fc54":"code","43aa8c6e":"code","bfa1a9a8":"code","a217f08c":"code","e8ddc94b":"code","48e1bab9":"code","16e807cc":"code","30003aa4":"code","bb4e6173":"code","85902e9f":"code","8b9d7411":"code","be11692e":"code","9bb6b146":"code","99cdacd3":"code","e7a6a9df":"code","fff1e356":"code","eebe6523":"code","dd2131ea":"code","54f383c7":"code","78cba920":"code","a4654b40":"code","a84f0bbb":"code","a7770282":"code","95aaf50f":"code","b0179412":"code","e7a7f7ee":"markdown","e3751299":"markdown","fdcdb8e2":"markdown"},"source":{"e1a412ef":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\n\nfrom pathlib import Path\n\nbase_dir = Path(\"\/kaggle\/input\/trendyol-coderspace-datathon\")","8ed6a8c1":"products = dt.fread(\"..\/input\/trendyol-coderspace-datathon\/products.csv\").to_pandas()","db16f7b5":"transactios_wo_variant = pd.read_parquet(\"..\/input\/tr-yol-cache\/transactions_wo_variant.parquet\")","aa6fd4cf":"product_variant_cnts = products.groupby(\"product_content_id\")[\"product_variant_id\"].nunique().to_frame(\"product_variant_cnt\")\n#product_variant_cnts[\"is_test_product\"] = product_variant_cnts.index.isin(test_products) * 1\nproduct_variant_cnts = product_variant_cnts.join(transactios_wo_variant.groupby([\"product_content_id\"])[\"is_returned\"].size().to_frame(\"tx_cnt\"))\nproduct_variant_cnts = product_variant_cnts.join(transactios_wo_variant[transactios_wo_variant[\"is_returned\"] == 1].groupby([\"product_content_id\"]).size().to_frame(\"ret_tx_cnt\"))","fcce41de":"products[\"attribute_string_value\"] = (\n    products[\"attribute_value\"]\n    .str.replace(\"\\d+\", \"\")\n    .str.replace(\"\\s+\", \"\")\n    .str.lower()\n    .str.replace(\"-ya\u015f\", \"ya\u015f\")\n    .str.replace(\"-ay\", \"ay\")\n    .str.replace(\"extra\", \"x\")\n    .replace(\"-,\", \"\")\n    .replace(\"-\", \"\")\n    .replace(\"k\u0131sa\", \"s\", regex=True)\n    .replace(\"small\", \"s\", regex=True)\n    .replace(\"k\u00fc\u00e7\u00fckboy\", \"s\", regex=True)\n    .replace(\"medium\", \"m\", regex=True)\n    .replace(\"ortaboy\", \"m\", regex=True)\n    .replace(\"large\", \"l\", regex=True)\n    .replace(\"\/\", \"-\", regex=True)\n    .replace(\"tekbeden\", \"tekebat\", regex=True)\n    .replace(\"premature\", \"premat\u00fcre\", regex=True)\n)\n\nproducts.loc[products[\"attribute_string_value\"].fillna(\"\").str.startswith(\"-\"), \"attribute_string_value\"] = products.loc[products[\"attribute_string_value\"].fillna(\"\").str.startswith(\"-\"), \"attribute_string_value\"].str.replace(\"--\", \"\").str.replace(\"-\", \"\").str.replace(\"+\", \"\").str.replace(\",\", \"\").str.replace(\",-,\", \"\")\nproducts.loc[products[\"attribute_string_value\"].fillna(\"\").str.endswith(\"-\"), \"attribute_string_value\"] = products.loc[products[\"attribute_string_value\"].fillna(\"\").str.endswith(\"-\"), \"attribute_string_value\"].str.replace(\"-\", \"\").str.replace(\"+\", \"\")\n\nproducts = products.assign(is_there_color_alternative=lambda x: x.groupby([\"product_content_id\"])[\"color_id\"].transform(\"nunique\"),\n                           is_there_beden_alternative=lambda x: x.groupby([\"product_content_id\"])[\"attribute_value\"].transform(\"nunique\"))","278a975f":"products[\"attribute_int_value\"] = (\n    products[\"attribute_value\"]\n    .str.replace(\"[a-zA-Z\u00d6\u015e\u011e\u00dc\u00c7\u00fc\u011f\u015f\u00e7\u00f6]\", \"\")\n    .replace(\"\/\", \"-\", regex=True)\n    .str.replace(\"\\s+\", \"-\")\n)\n\nproducts.loc[\n    products[\"attribute_int_value\"].fillna(\"\").str.endswith(\"-\"), \"attribute_int_value\"\n] = (\n    products.loc[\n        products[\"attribute_int_value\"].fillna(\"\").str.endswith(\"-\"),\n        \"attribute_int_value\",\n    ]\n    .str.slice(0, -1)\n    .str.replace(\"+\", \"\")\n)\nproducts.loc[\n    products[\"attribute_int_value\"].fillna(\"\").str.startswith(\"-\"),\n    \"attribute_int_value\",\n] = (\n    products.loc[\n        products[\"attribute_int_value\"].fillna(\"\").str.startswith(\"-\"),\n        \"attribute_int_value\",\n    ]\n    .str.replace(\"\\-?\", \"\")\n    .str.replace(\"\\?\", \"-\")\n    .str.replace(\"--\", \"\")\n    .str.replace(\"-\", \"\")\n    .str.replace(\"+\", \"\")\n    .str.replace(\",\", \"\")\n    .str.replace(\",-,\", \"\")\n)\n\nproducts[\"attribute_int_value\"] = (\n    products[\"attribute_int_value\"]\n    .str.replace(\"---\", \"-\")\n    .str.replace(\"+\", \"\")  # , regex=True)\n    # .str.replace(\"-?\", \"\")\n    .str.replace(\",-\", \"\")\n)","ab7afff0":"products.loc[\n    (products[\"attribute_int_value\"].apply(len) == 4)\n    & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n    \"attribute_int_value\",\n] = (\n    products.loc[\n        (products[\"attribute_int_value\"].apply(len) == 4)\n        & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n        \"attribute_int_value\",\n    ].str.slice(0, 2)\n    + \"-\"\n    + products.loc[\n        (products[\"attribute_int_value\"].apply(len) == 4)\n        & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n        \"attribute_int_value\",\n    ].str.slice(2)\n)\n\nproducts.loc[\n    (products[\"attribute_int_value\"].apply(len) == 5)\n    & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n    \"attribute_int_value\",\n] = (\n    products.loc[\n        (products[\"attribute_int_value\"].apply(len) == 5)\n        & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n        \"attribute_int_value\",\n    ].str.slice(0, 2)\n    + \"-\"\n    + products.loc[\n        (products[\"attribute_int_value\"].apply(len) == 5)\n        & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n        \"attribute_int_value\",\n    ].str.slice(2)\n)\n\nproducts.loc[\n    (products[\"attribute_int_value\"].apply(len) == 6)\n    & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n    \"attribute_int_value\",\n] = (\n    products.loc[\n        (products[\"attribute_int_value\"].apply(len) == 6)\n        & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n        \"attribute_int_value\",\n    ].str.slice(0, 3)\n    + \"-\"\n    + products.loc[\n        (products[\"attribute_int_value\"].apply(len) == 6)\n        & ~products[\"attribute_int_value\"].str.contains(\"-\"),\n        \"attribute_int_value\",\n    ].str.slice(3)\n)","4950fc54":"products.loc[\n    (products[\"attribute_string_value\"] == \"\")\n    & ((products[\"attribute_int_value\"] != \"\")),\n    \"is_attribute_specified_by_what\",\n] = \"str\"\nproducts.loc[\n    (products[\"attribute_string_value\"] != \"\")\n    & ((products[\"attribute_int_value\"] == \"\")),\n    \"is_attribute_specified_by_what\",\n] = \"int\"\nproducts.loc[\n    (products[\"attribute_string_value\"] != \"\")\n    & ((products[\"attribute_int_value\"] != \"\")),\n    \"is_attribute_specified_by_what\",\n] = \"both\"","43aa8c6e":"products[\"is_ranged_product\"] = products[\"attribute_string_value\"].str.contains(\n    \"-\"\n) | products[\"attribute_int_value\"].str.contains(\"-\")","bfa1a9a8":"product_variant_cnts = (\n    products.groupby(\"product_content_id\")[\"product_variant_id\"]\n    .nunique()\n    .to_frame(\"product_variant_cnt\")\n)\n\nproduct_variant_cnts = product_variant_cnts.join(\n    transactios_wo_variant.groupby([\"product_content_id\"])[\"is_returned\"]\n    .size()\n    .to_frame(\"tx_cnt\")\n)\nproduct_variant_cnts = product_variant_cnts.join(\n    transactios_wo_variant[transactios_wo_variant[\"is_returned\"] == 1]\n    .groupby([\"product_content_id\"])\n    .size()\n    .to_frame(\"ret_tx_cnt\")\n)\nproduct_variant_cnts[\"ret_ratio\"] = product_variant_cnts[\"ret_tx_cnt\"].fillna(0) \/ (\n    product_variant_cnts.loc[:, [\"tx_cnt\", \"ret_tx_cnt\"]].sum(axis=1)\n)\nproduct_variant_cnts = product_variant_cnts.join(\n    products.loc[\n        :,\n        [\n            \"product_content_id\",\n            \"category_name\",\n            \"is_there_color_alternative\",\n            \"is_there_beden_alternative\",\n            \"is_ranged_product\",\n            \"is_attribute_specified_by_what\",\n            \"attribute_string_value\",\n            \"attribute_int_value\",\n        ],\n    ]\n    .drop_duplicates()\n    .set_index(\"product_content_id\")\n)","a217f08c":"beden_str = (\n    product_variant_cnts.groupby(\n        [\"category_name\", \"is_ranged_product\", \"attribute_string_value\"]\n    )[\"ret_ratio\"]\n    .describe()\n).sort_values(\n    \"mean\"\n)  # .sort_index([\"category_name\", \"is_ranged_product\"])\n\nbeden_str[\"mean_sum\"] = beden_str.groupby(level=[\"category_name\", \"is_ranged_product\"])[\n    \"mean\"\n].transform(\"sum\")\n\nbeden_str[\"mean_perc\"] = beden_str[\"mean\"] \/ beden_str[\"mean_sum\"]\nbeden_str[\"cumsum\"] = beden_str.groupby(level=[\"category_name\", \"is_ranged_product\"])[\n    \"mean_perc\"\n].cumsum()\n\nbeden_str[\"cumsum\"] = beden_str[\"cumsum\"].fillna(0)\n\nbeden_str[\"beden_group\"] = beden_str.groupby(level=[\"category_name\", \"is_ranged_product\"])[\n    \"cumsum\"\n].transform(lambda x: pd.cut(x, bins=[0, 0.1, 0.25, 0.5, 0.75, 1], include_lowest=True))\n\nbeden_str = beden_str.loc[:, [\"mean\", \"std\", \"cumsum\", \"beden_group\"]].reset_index()","e8ddc94b":"beden_int = (\n    product_variant_cnts.groupby(\n        [\"category_name\", \"is_ranged_product\", \"attribute_int_value\"]\n    )[\"ret_ratio\"]\n    .describe()\n).sort_values(\n    \"mean\"\n)  # .sort_index([\"category_name\", \"is_ranged_product\"])\n\nbeden_int[\"mean_sum\"] = beden_int.groupby(level=[\"category_name\", \"is_ranged_product\"])[\n    \"mean\"\n].transform(\"sum\")\n\nbeden_int[\"mean_perc\"] = beden_int[\"mean\"] \/ beden_int[\"mean_sum\"]\nbeden_int[\"cumsum\"] = beden_int.groupby(level=[\"category_name\", \"is_ranged_product\"])[\n    \"mean_perc\"\n].cumsum()\n\nbeden_int[\"cumsum\"] = beden_int[\"cumsum\"].fillna(0)\n\nbeden_int[\"beden_group\"] = beden_int.groupby(level=[\"category_name\", \"is_ranged_product\"])[\n    \"cumsum\"\n].transform(lambda x: pd.cut(x, bins=[0, 0.1, 0.25, 0.5, 0.75, 1], include_lowest=True))\n\nbeden_int = beden_int.loc[:, [\"mean\", \"std\", \"cumsum\", \"beden_group\"]].reset_index()","48e1bab9":"beden_str[\"beden_group\"] = beden_str[\"beden_group\"].cat.codes\nbeden_int[\"beden_group\"] = beden_int[\"beden_group\"].cat.codes","16e807cc":"beden_str.to_csv(\"products_size_fe__by_str.csv\")\nbeden_int.to_csv(\"products_size_fe__by_number.csv\")\nproducts.to_parquet(\"products.parquet\")","30003aa4":"del transactios_wo_variant, beden_int, beden_str, product_variant_cnts","bb4e6173":"tx_grps = [\n    \"order_date\",\n    \"order_parent_id\",\n    \"category_name\",\n    \"product_metacat_id\",\n    \"product_metacat_sub_id\",\n    \"product_metacat_sub2_id\",\n]\n\n\ndef preprocess_fe_source(grp_idx=\"*\", path_suffix=\"*\", user_idx_list=None):\n\n    path = (\n        f\"grp_*\/trx__*_{path_suffix}\"\n        if grp_idx == \"*\"\n        else f\"grp_{grp_idx}\/trx__{grp_idx}_{path_suffix}\"\n    )\n    df_fe = reduce(\n        lambda x, y: pd.concat([x, y]),\n        map(\n            pd.read_parquet,\n            list(Path(\"..\/input\/tr-yol-fe\/\").glob(path)),\n        ),\n    )\n\n    if not user_idx_list:\n        user_idx_list = df_fe.user_id.unique()\n\n    if path_suffix != \"extending_sum.parquet\":\n\n        idx_cols = [\n            \"order_date\",\n            \"order_parent_id\",\n        ]\n\n        merge_cols = (\n            df_fe.columns.intersection(pd.Index(tx_grps))\n            .union([\"user_id\"] + idx_cols)\n            .tolist()\n        )\n\n        df_fe = df_fe[df_fe[\"user_id\"].isin(user_idx_list)]\n\n    else:\n        idx_cols = [\n            \"order_date\",\n            \"order_parent_id\",\n        ]\n\n        merge_cols = [\n            \"gender_id\",\n            \"product_content_id\",\n            \"supplier_id\",\n            \"user_id\",\n            \"brand_id\",\n        ] + idx_cols\n\n        # df_fe = df_fe[\n        #    sorted(\n        #        df_fe.columns[\n        #            df_fe.columns.str.contains(\"|\".join([\"return__\", \"_id\"]))\n        #        ],\n        #        key=lambda x: x.split(\"__\")[-1],\n        #    )\n        # ]\n        # .drop_duplicates(\n        #    merge_cols,\n        #    keep=\"last\",\n        # )\n\n    col_duos = np.array(\n        sorted(\n            df_fe.columns[\n                df_fe.columns.str.contains(\"return__\")\n                & ~df_fe.columns.str.contains(\"gender\")\n            ],\n            key=lambda x: x.split(\"__\")[-1],\n        )\n    ).reshape(-1, 2)\n\n    for not_return_col, return_col in col_duos:\n\n        # if not_return_col.split(_)[-1] == return_col.split(_)[-1]:\n        df_fe.loc[:, \"total_cnt__\" + not_return_col.split(\"__\")[-1]] = df_fe.loc[\n            :, [not_return_col, return_col]\n        ].sum(axis=1)\n\n        df_fe.loc[:, \"return_ratio__\" + not_return_col.split(\"__\")[-1]] = (\n            df_fe[return_col] \/ df_fe[\"total_cnt__\" + not_return_col.split(\"__\")[-1]]\n        )\n\n    return df_fe, merge_cols","85902e9f":"transactions = pd.read_parquet(\"..\/input\/tr-yol-cache\/transactions_wo_variant.parquet\", columns=[\"brand_id\", \"product_content_id\", \"supplier_id\", \"product_variant_id\", \"is_returned\",\"user_id\", \"category_name\",  \"original_price\"]).query(\"is_returned < 2\")","8b9d7411":"#transactions = transactions.drop_duplicates([\"product_content_id\", \"product_variant_id\"])#.groupby([\"brand_id\", \"category_name\"])\n\ntransactions = transactions.assign(is_there_product_variant=lambda x: x.groupby([\"product_content_id\"])[\"product_variant_id\"].transform(\"nunique\"),    \n                       \n              brand_price_elasticy__q7=lambda x: x.groupby([\"brand_id\", \"category_name\"])[\"original_price\"].transform(lambda x: np.percentile(x.unique(), 0.7)),\n              brand_price_elasticy__q3=lambda x: x.groupby([\"brand_id\", \"category_name\"])[\"original_price\"].transform(lambda x: np.percentile(x.unique(), 0.3)),\n              total_products_cat_potential=lambda x: x.groupby([\"brand_id\", \"category_name\"])[\"product_content_id\"].transform(\"nunique\"),\n              total_products_cat_user_sold=lambda x: x.groupby([\"brand_id\", \"category_name\"])[\"user_id\"].transform(\"nunique\") \/ x.groupby([\"category_name\"])[\"user_id\"].transform(\"nunique\"),\n              total_products_cat_supplier_sold=lambda x: x.groupby([\"brand_id\", \"category_name\"])[\"supplier_id\"].transform(\"nunique\") \/ x.groupby([\"category_name\"])[\"user_id\"].transform(\"nunique\"),\n              brand_return_ratio=lambda x: x.groupby([\"brand_id\"])[\"is_returned\"].transform(lambda x: (x == 1).sum() \/ x.shape[0]),                                                                                 \n              brand_cat_return_ratio=lambda x: x.groupby([\"brand_id\", \"category_name\"])[\"is_returned\"].transform(lambda x: (x == 1).sum() \/ x.shape[0]))\n\nbrands = transactions.loc[:, [\"brand_id\", \"category_name\", \"brand_price_elasticy__q7\", \"brand_price_elasticy__q3\", \"is_there_product_variant\", \"total_products_cat_potential\", \"total_products_cat_user_sold\", \"total_products_cat_supplier_sold\", \n              \"brand_return_ratio\", \n              \"brand_cat_return_ratio\"]].drop_duplicates()\n\nbrands.to_parquet(\"brand_fe.parquet\")","be11692e":"transactions = transactions.assign(\n              supplier_price_elasticy__q7=lambda x: x.groupby([\"supplier_id\", \"category_name\"])[\"original_price\"].transform(lambda x: np.percentile(x.unique(), 0.7)),\n              supplier_price_elasticy__q3=lambda x: x.groupby([\"supplier_id\", \"category_name\"])[\"original_price\"].transform(lambda x: np.percentile(x.unique(), 0.3)),\n              sup_total_products_cat_potential=lambda x: x.groupby([\"supplier_id\", \"category_name\"])[\"product_content_id\"].transform(\"nunique\"),\n              sup_total_products_cat_user_sold=lambda x: x.groupby([\"supplier_id\", \"category_name\"])[\"user_id\"].transform(\"nunique\") \/ x.groupby([\"category_name\"])[\"user_id\"].transform(\"nunique\"),\n              sup_total_products_cat_supplier_sold=lambda x: x.groupby([\"supplier_id\", \"category_name\"])[\"supplier_id\"].transform(\"nunique\") \/ x.groupby([\"category_name\"])[\"user_id\"].transform(\"nunique\"),\n              supplier_return_ratio=lambda x: x.groupby([\"supplier_id\"])[\"is_returned\"].transform(lambda x: (x == 1).sum() \/ x.shape[0]),                                                                                 \n              supplier_cat_return_ratio=lambda x: x.groupby([\"supplier_id\", \"category_name\"])[\"is_returned\"].transform(lambda x: (x == 1).sum() \/ x.shape[0]))\n\nsuppliers = transactions.loc[:, [\"supplier_id\", \n                        \"category_name\", \n                              \"supplier_price_elasticy__q7\",\n                              \"supplier_price_elasticy__q3\",\n                        \"sup_total_products_cat_potential\", \n                        \"sup_total_products_cat_user_sold\", \n                        \"sup_total_products_cat_supplier_sold\", \n              \"supplier_return_ratio\", \n              \"supplier_cat_return_ratio\"]].drop_duplicates()\n\nsuppliers.to_parquet(\"supplier_fe.parquet\")","9bb6b146":"del transactions, brands, suppliers","99cdacd3":"from functools import reduce \n\ndf_list = []\n\nfor i in range(10):\n    grp = i\n\n    product_metadata = pd.read_csv(\"..\/input\/tr-yol-cache\/products_metadata.csv\")\n\n    fe_list = list(\n                        set(\n                            list(\n                                map(\n                                    lambda x: x.parts[-1].replace(x.parts[-1][:7], \"\")\n                                    if not x.parts[-1].endswith(\"extending_sum.parquet\")\n                                    else \"\",\n                                    Path(\"..\/input\/tr-yol-fe\/\").rglob(f\"grp_{grp}\/*\"),\n                                )\n                            )\n                        )\n                    )\n\n    product_metadata = product_metadata.rename(\n        columns=dict(zip(product_metadata.columns[1:], tx_grps[-3:]))\n    )\n\n    grp_idx = str(grp)\n    df = preprocess_fe_source(grp_idx=grp_idx, path_suffix=\"extending_sum.parquet\")[\n        0\n    ].merge(product_metadata, on=\"category_name\")\n\n    for fe in fe_list:\n        if fe:\n            fe0, merge_cols = preprocess_fe_source(grp_idx=grp_idx, path_suffix=fe)\n            df = df.merge(fe0, on=merge_cols, how=\"left\")\n\n    del fe0, merge_cols\n\n    df = df.dropna(subset=[\"is_returned\"]).sort_values(\"order_date\").drop_duplicates(\n        [\"user_id\", \"category_name\"], keep=\"last\",\n        ignore_index=True\n    )\n    \n    df_list.append(df)\n    \ndf_list = pd.concat(df_list).reset_index(drop=True)\ndf_list.to_csv(\"trx_train.csv\", index=False)\n","e7a6a9df":"qa = dt.fread(\"..\/input\/trendyol-coderspace-datathon\/qa.csv\").to_pandas()","fff1e356":"qa = dt.fread(\"..\/input\/trendyol-coderspace-datathon\/qa.csv\").to_pandas()\n#qa = qa.merge(products.loc[:, [\"product_content_id\", \"category_name\"]].drop_duplicates(),\n#         on=[\"product_content_id\"],\n#         how=\"left\")\n\nqa[\"answer_qa_char_len\"] = qa[\"answer\"].str.len()\nqa[\"answer_qa_word_len\"] = qa[\"answer\"].str.split(\" \").apply(len)\n\nqa[\"question_vs_supplier__median_char_cnt\"] = qa.groupby([\"supplier_id\"])[\"answer_qa_char_len\"].transform(lambda x: x.quantile(0.5))\nqa[\"question_vs_supplier__median_word_cnt\"] = qa.groupby([\"supplier_id\"])[\"answer_qa_word_len\"].transform(lambda x: x.quantile(0.5))\n\nqa[\"question_by_supplier\"] = qa.groupby([\"supplier_id\"])[\"answer\"].transform(\"count\")\n\nqa = qa.drop(columns=[\"product_content_id\",\"answer_qa_char_len\",  \"answer_qa_word_len\", \"platform\", \"question\", \"answer\"]).drop_duplicates()","eebe6523":"#qa[\"avg_char_per_word\"] = qa[\"question_vs_supplier__median_word_cnt\"] \/ qa[\"question_vs_supplier__median_char_cnt\"]","dd2131ea":"reviews = dt.fread(\"..\/input\/trendyol-coderspace-datathon\/reviews.csv\").to_pandas()\n\nreviews = reviews.merge(products.loc[:, [\"product_content_id\", \"category_name\"]].drop_duplicates(),\n         on=[\"product_content_id\"],\n         how=\"left\")\n\nreviews[\"comment_char_len\"] = reviews[\"comment\"].str.len()\n#reviews[\"comment_word_len\"] = reviews[\"comment\"].str.split(\" \").apply(len)\n\nreviews[\"sentiment\"] = reviews.rate.map({1: \"negative\",\n                                         2: \"negative\",\n                                         3: \"positive\",\n                                         4: \"positive\",\n                                         5: \"positive\"})\n\nreviews = reviews.query(\"sentiment != 'notr'\")\n\nreviews[\"sentiment_by_supplier\"] = reviews.groupby([\"supplier_id\"])[\"sentiment\"].transform(\"count\")\nreviews[\"sentiment_by_supplier_cat\"] = reviews.groupby([\"supplier_id\", \"category_name\"])[\"sentiment\"].transform(\"count\")\n\nreviews = reviews.merge(reviews.groupby([\"supplier_id\", \"sentiment\"]).size().to_frame(\"sentiment_vs_supplier\").reset_index(),\n              on=[\"supplier_id\", \"sentiment\"],\n              how=\"left\")\n\nreviews = reviews.merge(reviews.groupby([\"supplier_id\", \"category_name\", \"sentiment\"]).size().to_frame(\"sentiment_vs_supplier_cat\").reset_index(),\n              on=[\"supplier_id\", \"category_name\", \"sentiment\"],\n              how=\"left\")\n\nreviews[\"sentiment_vs_supplier_like_cnt\"] = reviews.groupby([\"supplier_id\", \"sentiment\"])[\"review_like_count\"].transform(\"sum\")\nreviews[\"sentiment_vs_supplier__median_char_cnt\"] = reviews.groupby([\"supplier_id\", \"sentiment\"])[\"comment_char_len\"].transform(lambda x: x.quantile(0.5))\n#reviews[\"sentiment_vs_supplier__median_word_cnt\"] = reviews.groupby([\"supplier_id\", \"sentiment\"])[\"comment_word_len\"].transform(lambda x: x.quantile(0.5))\n\n\nreviews[\"sentiment_vs_supplier_like_cnt_cat\"] = reviews.groupby([\"supplier_id\", \"sentiment\", \"category_name\"])[\"review_like_count\"].transform(\"sum\")\nreviews[\"sentiment_vs_supplier__median_char_cnt_cat\"] = reviews.groupby([\"supplier_id\", \"sentiment\", \"category_name\"])[\"comment_char_len\"].transform(lambda x: x.quantile(0.5))\n#reviews[\"sentiment_vs_supplier__median_word_cnt_cat\"] = reviews.groupby([\"supplier_id\", \"sentiment\", \"category_name\"])[\"comment_word_len\"].transform(lambda x: x.quantile(0.5))\n\n\nreviews[\"sentiment_vs_supplier_like_cnt\"] = reviews.groupby([\"supplier_id\", \"sentiment\"])[\"review_like_count\"].transform(\"sum\")","54f383c7":"reviews = reviews.assign(supplier_sentiment_ratio=lambda df: df[\"sentiment_vs_supplier\"] \/ df[\"sentiment_by_supplier\"],\n                         supplier_sentiment_support_ratio=lambda df: df[\"sentiment_vs_supplier_like_cnt\"] \/ df[\"sentiment_vs_supplier\"],\n                         supplier_sentiment_ratio_cat=lambda df: df[\"sentiment_vs_supplier_cat\"] \/ df[\"sentiment_by_supplier_cat\"],\n                         supplier_sentiment_support_ratio_cat=lambda df: df[\"sentiment_vs_supplier_like_cnt_cat\"] \/ df[\"sentiment_vs_supplier_cat\"])","78cba920":"reviews = reviews.drop(columns=[\"product_content_id\",\n                         \"rate\",\n                         \"comment\",\n                         \"review_like_count\",\n                         \"comment_char_len\",\n                        # \"comment_word_len\",\n                         \"sentiment_vs_supplier\",\n                         \"sentiment_vs_supplier_like_cnt\",\n                         \"sentiment_vs_supplier_cat\",\n                         \"sentiment_by_supplier\",\n                         \"sentiment_by_supplier_cat\",\n                         \"sentiment_vs_supplier_like_cnt_cat\",\n                         ]).drop_duplicates()\n","a4654b40":"r1_cols = reviews.columns[~reviews.columns.str.contains(\"|\".join([\"_cat\", \"category\"]))]\nr2_cols = reviews.columns[reviews.columns.str.contains(\"|\".join([\"_cat\", \"category\"]))].union([\"supplier_id\", \"sentiment\"])\n\nr1 = reviews.loc[:, r1_cols].set_index([\"supplier_id\", \"sentiment\"]).drop_duplicates().unstack(\"sentiment\")\nr1.columns = [\"__\".join(cols) for cols in r1.columns]\nr1 = r1.loc[:, sorted(r1.columns, key=lambda x: x.split(\"_\")[-1])]\n\n\nr2 = reviews.loc[:, r2_cols].set_index([\"supplier_id\", \"category_name\", \"sentiment\"]).drop_duplicates().unstack(\"sentiment\")\nr2.columns = [\"__\".join(cols) for cols in r2.columns]\nr2 = r2.loc[:, sorted(r2.columns, key=lambda x: x.split(\"_\")[-1])]\n\nfor col in [\"supplier_sentiment_ratio\",\n            \"supplier_sentiment_support_ratio\",\n            \"sentiment_vs_supplier__median_char_cnt\"]:\n    if \"ratio\" in col:\n        r1[col +\"__diff_ratio\"] = - r1[col + \"__negative\"].fillna(0) + r1[col + \"__positive\"]\n    \n    else:\n        r1[col +\"__diff_ratio\"] = r1[col + \"__negative\"].fillna(0) \/ r1[col + \"__positive\"]\n    \nfor col in [\"supplier_sentiment_ratio\",\n            \"supplier_sentiment_support_ratio\",\n            \"sentiment_vs_supplier__median_char_cnt\"]:\n    if \"ratio\" in col:\n        r2[col +\"_cat__diff_ratio\"] = - r2[col + \"_cat__negative\"].fillna(0) + r2[col + \"_cat__positive\"]\n    \n    else:\n        r2[col +\"_cat__diff_ratio\"] = r2[col + \"_cat__negative\"].fillna(0) \/ r2[col + \"_cat__positive\"]","a84f0bbb":"r1.loc[:, r1.columns.str.contains(\"__diff_ratio\")].dropna(how=\"all\", axis=1).sort_values(\"supplier_sentiment_ratio__diff_ratio\").to_csv(\"reviews_supplier_fe.csv\")\nr2.loc[:, r2.columns.str.contains(\"__diff_ratio\")].dropna(how=\"all\", axis=1).sort_values(\"supplier_sentiment_ratio_cat__diff_ratio\").to_csv(\"reviews_supplier_category_fe.csv\")","a7770282":"supplier_return_rate = pd.read_csv(\"..\/input\/trendyol-coderspace-datathon\/supplier_defective_return.csv\")\\\n    .merge(\n        pd.read_csv(\"..\/input\/trendyol-coderspace-datathon\/supplier_return.csv\"),\n        on=\"supplier_id\",\n        suffixes=(\"defected_rate\", \"overall_rate\"),\n    )\\\n    .merge(\n        pd.read_csv(\n            \"..\/input\/trendyol-coderspace-datathon\/supplier_disputed_return.csv\"\n        )\n        .assign(\n            accepted_claim=lambda df: df.loc[\n                :, [\"unresolved_claim\", \"unresolved_accepted_claim\"]\n            ].sum(axis=1),\n            claim_accept_ratio=lambda df: (df[\"accepted_claim\"] \/ df[\"total_claim\"]),\n        )\n        .drop(\n            columns=[\n                \"accepted_claim\",\n                \"unresolved_claim\",\n                \"unresolved_accepted_claim\",\n                \"total_claim\",\n            ]\n        ),\n        on=[\"supplier_id\"],\n        how=\"left\",\n    )","95aaf50f":"supplier_return_rate.merge(qa, on=\"supplier_id\", how=\"left\").to_csv(\"supplier_fe.csv\", index=False)","b0179412":"del supplier_return_rate, reviews, qa","e7a7f7ee":"## Products ","e3751299":"## Supplier ","fdcdb8e2":"## Brands "}}