{"cell_type":{"bdf05cb9":"code","79f05264":"code","8797f07d":"code","16317ea8":"code","b0b2578a":"code","11bbe717":"code","5fc4a105":"code","3d6463c9":"code","8cff89bf":"code","c873cddb":"code","df8f2a30":"code","fd0439e6":"code","c05b5ccc":"code","409beb16":"code","e45e1bce":"code","e8a33f35":"code","5fba9077":"code","2924707f":"code","a8a9c765":"code","ab85b393":"code","176b008c":"code","9b5a38c2":"code","f2805aa1":"code","459014f1":"code","82aae7e0":"code","aa4c348b":"code","9e0576f6":"code","19a3dd90":"code","e5c1a7c1":"code","b369ed6e":"code","139f9946":"code","1796e5d6":"code","59faa39f":"code","bd92b01c":"code","177826a0":"code","8dd113d6":"code","a532fe17":"code","e40fa9d3":"code","d58af939":"code","263d41bf":"code","ca172af5":"code","99a439b4":"code","8cd24e6f":"code","6eb7a533":"code","d58a1ac8":"code","89092701":"code","49abb255":"code","972391af":"code","fa4ec173":"code","4f110d01":"code","aaf7bacb":"code","26fc1831":"code","bba9186e":"code","5caeeb17":"code","68e0c14c":"code","2d37297c":"code","a9e036ae":"code","bd9a91a7":"code","e86a0fdd":"code","a3c896ae":"code","15a94b60":"code","e4337492":"code","a4a43543":"code","d5967061":"code","98def041":"code","7b843db6":"code","8cf72f6f":"code","3c52afe1":"code","5d3a9bb8":"code","731c978b":"code","0f39d1f6":"code","43484ce7":"code","8f0db7f5":"code","9c438153":"code","0b9ec73e":"code","c4189198":"code","7f29eadd":"code","7ee2ac18":"code","a95bc95f":"code","cab60d98":"code","78c65998":"code","2b1dce44":"code","d81cb0ed":"code","1a37f923":"code","0a53b105":"code","19d59eb3":"code","41460411":"code","239a4d27":"code","f3d3ac75":"code","0b926b3a":"code","661d7104":"code","e6ff1a76":"code","a2d10a30":"code","5cb2657d":"code","01011178":"code","187c050c":"code","7dfba1f3":"code","ac4093b1":"code","f487a677":"code","c966667b":"code","7a4f250d":"code","02a0179c":"code","cb432392":"code","b21f0dff":"code","fe06b897":"code","59d24a52":"code","405f3f7e":"code","011703ff":"code","f97ac49d":"code","b0ce13d7":"code","f7c3c085":"code","4d5f6942":"code","fba94027":"code","072c4b89":"code","a71e15d5":"code","2eb0f024":"code","3047f6bd":"code","6594de12":"code","ffdbec65":"code","55915e85":"code","a24158e3":"code","1dcfbfbf":"code","39afbae7":"code","064a1225":"code","4f26c94d":"code","33126f60":"code","cfe8f47d":"code","c53d3ec1":"code","5f004ef1":"code","6423da4c":"code","09565319":"code","f242733c":"code","8ffb55ce":"code","3c2df20b":"code","ba06bab5":"code","32102901":"code","6f4fd913":"code","eefd4dd0":"code","a961df4f":"code","f53da82f":"code","2b6dceb4":"code","efc60d60":"code","82cc7424":"code","75999730":"code","7c3a717c":"code","bf18951a":"code","5ed37b5d":"code","5f2105d8":"code","cacc7cce":"code","cb05ba9e":"code","56b1987c":"code","8a20424b":"code","6f4d9bd7":"code","3970df4b":"code","0993f1bb":"code","b9365b41":"code","621dcaba":"code","fb8084a2":"code","24c3fcf3":"code","9680b486":"code","0b6fbf34":"code","2f4889c2":"code","48602366":"code","b4282947":"code","a0533b90":"code","d206060d":"code","190b7718":"code","33c8d196":"code","8ab65a0c":"code","60d23bde":"code","1dda7b79":"code","440092e8":"code","cb6cd0f8":"code","aced073d":"code","2bb15a4a":"code","dee32d8f":"code","2a57d6af":"code","9c7ff2eb":"code","7d698f4d":"code","044077e8":"code","be9b05f7":"code","544f018c":"code","88ba4c5e":"code","fe7d7acc":"code","e3e0f3e6":"code","bf161c8b":"code","a48a7638":"code","1b679943":"code","61095d0b":"code","9070b933":"code","c7b290b3":"code","372d6864":"code","1bcedefd":"code","6c15e260":"code","8b555a8e":"code","fcd54825":"code","4e5a8852":"code","26bbb720":"code","604b1da8":"code","0c594056":"code","0ab640a8":"code","6c32e7c4":"code","ff6f9baf":"code","7b8db41c":"code","ba0c986e":"code","38d93ee1":"code","d8978837":"code","c4a90ac2":"code","b2641a2d":"code","788e8cca":"code","0e80209e":"code","78261f36":"code","c5a9e8cc":"code","0cba012f":"code","fa832342":"code","93a5a3ce":"code","3df38707":"code","88bf1c93":"code","fa1fe597":"code","2264e70c":"code","7384fea2":"code","a54359b7":"code","baf90a31":"code","c4798a9e":"code","d01b038b":"code","2d3b16c3":"code","b1b6f256":"code","b3e1e0d3":"code","9da7cf75":"code","83963f6f":"code","55496940":"code","0a7d8be0":"code","6dd4c8f1":"code","00884a67":"code","7e11018f":"code","618a112b":"code","0c86cd95":"code","d3ee1c34":"code","5d6a1e03":"code","21f45244":"code","27a0055a":"code","6e60e03b":"code","10bc3593":"code","8a008c70":"code","20847398":"code","32bbc8df":"code","36feb013":"code","008b1ef0":"code","43406f97":"code","f973b9e8":"code","0520aa91":"code","6fdaa7d0":"code","e9207537":"code","e3d39cb9":"code","cc7944c2":"code","57c5f767":"code","d9ee3e03":"code","8eb51c21":"code","f7afbde9":"code","557c49b9":"code","456f3bc5":"code","bfc78cc7":"code","83ba9b86":"code","1ffc4fb6":"code","590c3eb9":"code","e18457cd":"code","64ed74b7":"code","4b55f4b5":"code","2f044015":"code","c204de18":"code","912b6663":"code","84190896":"code","1f7c787d":"code","6470c122":"code","006ed3fe":"code","8750753d":"code","6bb03d3b":"code","1db25aa5":"code","ecf2c4ad":"code","1c45672f":"code","faf21b0e":"code","c948021d":"code","1cf13780":"code","e0dea6ed":"code","1bf46b30":"code","04f18a54":"code","0cb144aa":"code","19ff6c85":"code","bd04f84c":"code","5d27258f":"code","155c83cc":"code","4d99736d":"code","5dfd65f7":"code","57500eaa":"code","6c27ce2e":"code","5ed48994":"code","eb3b281b":"code","4dc720a2":"code","adda25aa":"code","28fb3e8f":"code","272e0746":"code","11688267":"code","cc61bf81":"code","056ac2c8":"code","fdd7a018":"code","9a490643":"code","d452f0ad":"code","6e5955af":"code","905eb936":"code","d6196441":"code","6b5bb4ea":"code","246f8df1":"code","4a0acf89":"code","da45cf01":"code","be1d5ad1":"code","6f578e90":"code","39c785a7":"code","1911e44d":"code","aa280d5d":"code","5e363a2a":"code","7bd38a46":"code","af4e03fb":"code","5f834278":"code","834931b2":"code","0344090d":"code","212846dd":"code","260c857c":"code","149c7667":"code","225b01b6":"code","e5e77495":"code","9c1550e0":"code","ee0c7ed4":"code","cd6557d1":"code","7d960277":"code","a250e55e":"code","34e0e90b":"code","ea90842c":"code","d510a6c2":"code","3f4fefe9":"code","db02f7e9":"code","0cafc697":"code","92335d0d":"code","09ede59c":"code","fc1ed44f":"code","a9aa0370":"code","1711b312":"code","7ad96500":"code","52f42247":"code","9be8b3b1":"code","893c67d6":"code","02b71926":"code","8b2c082c":"code","7e3d5bb9":"code","ba2d4594":"code","c17e06cb":"code","8318263b":"code","ea8d5d05":"code","3cdf2879":"code","611f80fa":"code","ad270d7e":"code","78cc67dd":"code","f96243c0":"code","3201ecf9":"code","5722dd31":"code","ed60a414":"code","d23a9b05":"code","9257a4d1":"code","5473662c":"code","3d99508c":"code","9cd16343":"code","d781683d":"code","488ec6b9":"code","67528a71":"code","32784236":"code","6966ff35":"code","0053e057":"code","7e113f6e":"code","b32d0744":"code","9eeebc55":"code","086d1a46":"code","4e408e24":"code","5c6fb273":"code","39d370aa":"code","549fc523":"code","7d5705ba":"code","2d13b88b":"code","7b5ce496":"code","24a4b3da":"code","420454f2":"code","809ef7d8":"code","7b4c3c54":"code","df9aee52":"code","c459739d":"code","b9145a1e":"code","81c591a4":"code","6aa137d5":"code","50c5a2f6":"code","e6b426e5":"code","f8f13639":"code","41260bd8":"code","6296208b":"code","57b81a88":"code","251533b8":"code","d3d057d4":"code","6ef0f81c":"code","00a77e20":"code","b62f58ec":"code","e2b65811":"code","71a59224":"code","fe4f75f4":"code","f8892009":"code","768a4b98":"code","7e75475f":"code","74ea7d7e":"code","ca7ae8cf":"code","a696ce5b":"code","d50183ee":"code","7b104ce3":"code","32701aa8":"code","e672a2e8":"code","73664ba8":"code","2a3f6d0d":"code","eae8b804":"code","31f424f3":"code","be2d336f":"code","91fc4c64":"code","cef77469":"code","c1d73e2f":"code","09a11fe1":"code","bcf61357":"code","3be7a32b":"code","09869b7a":"code","5ef3e0ae":"code","1d773f9d":"code","26c5b2f0":"code","c90f4c16":"code","42a4f850":"code","a6124747":"code","57be9708":"code","29e16157":"code","9e83c9f0":"code","798531b8":"code","c26d15f2":"code","04d18a1d":"code","419b32c5":"code","1dda111a":"code","d13eff8b":"code","591797d5":"code","fdbb5f63":"code","98ea01e7":"code","3c0d0086":"code","0d614f1a":"code","1e54e904":"code","3129f62e":"code","af9fd4ef":"code","382eb724":"code","55d5f23b":"code","e68ed3c5":"code","8bdffa7c":"code","7ecc7fa8":"code","858eb331":"code","194da4df":"code","1c1caf92":"code","52fe6a9b":"code","2edd56dc":"code","ac591466":"code","97ac7ecb":"code","b30f1b7c":"code","00dbab0f":"code","64e3e468":"code","43c99c37":"code","6687e462":"code","822867f3":"code","4e33c353":"code","93a522f8":"code","c16aa0f7":"code","4b3bd0fa":"code","b2502fc3":"code","b96a18d4":"code","8dff5725":"code","106de838":"code","1882c0dc":"code","f8b0c993":"code","92d7e31e":"code","4a7a8cbe":"code","60fe54e6":"code","3a5dde3f":"code","cab2e607":"code","12a0df41":"code","45ec41be":"code","100a2317":"code","dd5e4e72":"code","12a31774":"code","d74ed380":"code","8ff4a58a":"code","4b9c8dcd":"code","dc697f5c":"code","4d699c1a":"code","cd8e5660":"code","4934bb7f":"code","a6db9407":"code","6add6bcc":"code","90cc29f6":"code","1e35bef2":"code","e9f5fde0":"code","bacff60f":"code","1154907c":"code","abffa777":"code","ad0995f6":"code","580d3c3c":"code","236ebba8":"code","c1bd0b5a":"code","f7d55d6b":"code","c2e3032d":"code","570ede57":"code","60728650":"code","3238b107":"code","93fd3642":"code","2e893340":"code","75b2cc25":"code","1f03a747":"code","d93ea53d":"code","62724e2e":"code","c47bff7e":"code","90e806ff":"code","248674e5":"code","a061e0b4":"code","7bd6a501":"code","29898a30":"code","dfb08728":"code","b88c622a":"code","39eac5bb":"code","2e32de69":"code","d66db225":"code","07431b69":"code","a24af3d7":"code","5cab9ed2":"code","8013d47a":"code","00e58403":"code","942fe29d":"code","74f98ab3":"code","ec633e85":"code","16596983":"code","d65c91ad":"code","8010d045":"code","a5e6981e":"code","4f493074":"code","11a0fa3a":"code","6a1054b1":"code","51065142":"code","0015b6c6":"code","779ca9be":"code","96ce0183":"code","f9df9797":"code","33e574f6":"code","8ead6040":"code","28f87ce0":"code","1891fd1a":"code","ddad05b1":"code","ad7609fb":"code","6ad76f17":"code","7427f564":"code","3df9256d":"code","81713361":"code","0fab4bf8":"code","9d130625":"code","2c3925a2":"code","5954b9c0":"code","13909524":"code","38692e60":"code","6713e3b9":"code","8f385fb9":"code","5aa2eea1":"code","b6acf44c":"code","25579945":"code","e81e786c":"code","e9a46ef8":"code","d0f21754":"code","58183ae4":"code","6753e5fd":"code","eb1a74b5":"code","30e701e5":"code","9a3dd2e5":"code","1ac2638d":"code","ed392b00":"code","b3c13135":"code","102bb292":"code","89f6230e":"code","e87ad27a":"code","837acfe1":"code","95be508f":"code","20ad1566":"code","6ad1e485":"code","aecf1d73":"code","ef218b57":"code","d031cdad":"code","7585d418":"code","f9c4243c":"code","6e255fcc":"code","e6040ed9":"code","1fd79153":"code","71f8f037":"code","6f12f890":"code","0fa2eaca":"code","d3780456":"code","2449aa5d":"code","c1215c90":"code","432339e2":"code","bd50c400":"code","a5d00b58":"markdown","1450823e":"markdown","d5b5e10c":"markdown","0f98cbb1":"markdown","71a98073":"markdown","c7cfa696":"markdown","e8f324be":"markdown","e84aaf53":"markdown","e7d04d91":"markdown","fbb945f1":"markdown","19c06af3":"markdown","f09eb5a2":"markdown","76120a4e":"markdown","0b09d35b":"markdown","7ba5df7a":"markdown","d326d47d":"markdown","f7dacde3":"markdown","2b1a5911":"markdown","d5c1c52a":"markdown","13c4a50a":"markdown","6daf04f7":"markdown","b6605daa":"markdown","51673d00":"markdown","e1343eac":"markdown","63356503":"markdown","0a4f9c02":"markdown","7d93f0cf":"markdown","9a1ce8f4":"markdown","4e1e5225":"markdown","483bae53":"markdown","f02a34d5":"markdown","b23f48bc":"markdown","084a0e13":"markdown","c8af4d60":"markdown","03882e19":"markdown","e27057b2":"markdown","50b010e9":"markdown","b0fb5854":"markdown","248fa0c0":"markdown","e12bbaa9":"markdown","6b409ab0":"markdown","1716b4aa":"markdown","31b45db3":"markdown","41f3ce64":"markdown","d3c4c5e0":"markdown","9ca18282":"markdown","2b2b9283":"markdown","67857753":"markdown","4057974f":"markdown","24400300":"markdown","e99c0796":"markdown","c5327274":"markdown","a8477cf9":"markdown","e3ad2cd6":"markdown","2cc11525":"markdown","b0bb23ff":"markdown","ed1e2830":"markdown","1f89e3fc":"markdown","cca1fcad":"markdown","83e925f1":"markdown","9556d78c":"markdown","b85a6ace":"markdown","d7bc50fe":"markdown","2a552066":"markdown","45957495":"markdown","adeaac87":"markdown","957b2610":"markdown","9e779430":"markdown","27d3e87c":"markdown","562f7d77":"markdown","38b9ce23":"markdown","28041333":"markdown","4e1bcc70":"markdown","c8c05196":"markdown","137c7fcf":"markdown","8f18e7c0":"markdown","f4d796de":"markdown","9a1b5827":"markdown","5dc77abb":"markdown","9af69efa":"markdown","e4fb64b5":"markdown","4142728d":"markdown","c30be6c8":"markdown","a7fff539":"markdown","1e693ba7":"markdown","685a286b":"markdown","828eabcd":"markdown","9a1e8352":"markdown","9c5c9644":"markdown","2b07b44d":"markdown","36c0bfb0":"markdown","7e291a30":"markdown","ba2ad2aa":"markdown","1c5a9b18":"markdown","0dbb29d6":"markdown","3692bcd0":"markdown","a2b416d1":"markdown","fd16512b":"markdown","4a43483c":"markdown","bdd6748c":"markdown","991f0da6":"markdown","9cec7b57":"markdown","2fba3810":"markdown","8620088c":"markdown","82865922":"markdown","a55597fb":"markdown","3a70d268":"markdown","b3452703":"markdown","e454c8b0":"markdown","162240e2":"markdown","9e344d3a":"markdown","a9d116ac":"markdown","fec81dad":"markdown","ceebc1ce":"markdown","2460fd3d":"markdown","e3b701fa":"markdown","0ac8a5f3":"markdown","eeb2bb26":"markdown","900a2239":"markdown","aa66efee":"markdown","7faff070":"markdown","790490e4":"markdown","13c272a6":"markdown","a1439f3f":"markdown","9eec8ca1":"markdown","34294d6e":"markdown","0d17d262":"markdown","0a16fb44":"markdown","67dd8a19":"markdown","d0646c6f":"markdown","0419f850":"markdown","e55076ba":"markdown","6bf31514":"markdown","5efbf39e":"markdown","a13754fc":"markdown","1e736a6c":"markdown","29f42c5c":"markdown","9240fc33":"markdown","3a5f2942":"markdown","bf4aebe9":"markdown","a593dd88":"markdown","4627a73f":"markdown","2e3ae936":"markdown","b611f82e":"markdown","193b74eb":"markdown","f9957ae0":"markdown","9c67da7e":"markdown","04a17f52":"markdown","c11a3fd1":"markdown","61d2dc28":"markdown","a04a8134":"markdown","098c9e35":"markdown","a8fb3a56":"markdown","8825e240":"markdown","bf005971":"markdown","a69e732c":"markdown","959a2ed4":"markdown","f86ddf7b":"markdown","51472f4f":"markdown","a987cc11":"markdown","22abd4f2":"markdown","0a769165":"markdown","f4a66b84":"markdown","a4f1546d":"markdown","a0f69cf2":"markdown","581b2f6b":"markdown","25f3a193":"markdown","7bb9a691":"markdown","aa7881e4":"markdown","82101698":"markdown","a08e0ad9":"markdown","dca3d2ad":"markdown","3b68ad83":"markdown","7b880182":"markdown","f3a8921c":"markdown","51831be5":"markdown","4492e2ef":"markdown","dfc16594":"markdown","c0d7be7f":"markdown","1e6c3c31":"markdown","6fae26b2":"markdown","b226a0d5":"markdown","f2b3b8d6":"markdown","bd0279df":"markdown","e88afc16":"markdown","97a81f3e":"markdown","d324d028":"markdown","328c6965":"markdown","8ce2399a":"markdown","dedec350":"markdown","cf667044":"markdown","d30d425b":"markdown","38d4ecc8":"markdown","f568679e":"markdown","21a9e799":"markdown","1edea80c":"markdown","7fb9c03c":"markdown","a49436e8":"markdown","9e4496a8":"markdown","28de48f4":"markdown","7116a1d4":"markdown","15fbd4ce":"markdown","1d8a4254":"markdown","5ed88d0d":"markdown","01020c80":"markdown","6e2316df":"markdown","3953fca2":"markdown","01048032":"markdown","bec4f1ce":"markdown","fd45cc12":"markdown","228b633a":"markdown","48fbd121":"markdown","ef1b599e":"markdown","bf35c59e":"markdown","0e88b72b":"markdown","d6885f97":"markdown","077e2b35":"markdown","49cc3dfe":"markdown","22342dce":"markdown","4e743efe":"markdown","48b9e195":"markdown","71276226":"markdown","e6ecdcbd":"markdown","6d5b2e1d":"markdown","d7eb17f7":"markdown","1838dd33":"markdown","c0379c8f":"markdown","043c2659":"markdown","137e361c":"markdown","8898f09d":"markdown","75c24083":"markdown","e874c69e":"markdown","1a272179":"markdown","28f228ce":"markdown","bc08174c":"markdown","c03db8dd":"markdown","17c1ebd1":"markdown","3ba7aa48":"markdown","01b0a0aa":"markdown","92d52b4d":"markdown","25921a9c":"markdown","a730ab41":"markdown","dbd9fef9":"markdown","05e90d06":"markdown","3b8c7e4d":"markdown","11fc8a78":"markdown","78e34b6d":"markdown","7c48d8f6":"markdown","755e1104":"markdown","33bac540":"markdown","baa117ed":"markdown","0223367f":"markdown","1ff74c11":"markdown","21884b1f":"markdown","212cec32":"markdown","fce7a5d8":"markdown","add4fbbf":"markdown","551d3a9c":"markdown","92d27208":"markdown","ffe88508":"markdown","b35c7164":"markdown","3824f3f5":"markdown","a33c1016":"markdown","244fc640":"markdown","5ca34cae":"markdown","9b5414ac":"markdown","c3e991ad":"markdown","05967cba":"markdown","0594c4f7":"markdown","d2541bdc":"markdown","e460e4d5":"markdown","a462c98c":"markdown","b95383a2":"markdown","a61cbc4f":"markdown","951fb18e":"markdown","e3be3c7d":"markdown","c5f0d789":"markdown","81ef0a45":"markdown","c34f427a":"markdown","2aefb928":"markdown","3a7e15a3":"markdown","7599bb2b":"markdown","9d1934c0":"markdown"},"source":{"bdf05cb9":"# mounting GDrive to our Colab Notebook\nfrom google.colab import drive\ndrive.mount('\/content\/drive', force_remount = True)","79f05264":"# filtering out the warnings after cell execution\nimport warnings\nwarnings.filterwarnings('ignore')","8797f07d":"# Installing the Dependencies\n!pip install -r '\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/requirements.txt'","16317ea8":"# Importing Libraries\n\n# General Commonly Used Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Preprocessing Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tqdm import tqdm_notebook\n\n# EDA\nimport scipy.stats as stats \nimport plotly.io as pio\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom pandas.plotting import scatter_matrix\nfrom sklearn import base\nfrom sklearn.model_selection import KFold\n\n# Feature Engineering and Selection\nfrom sklearn.feature_selection import SelectKBest, chi2, RFECV\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom feature_engine.discretisers import EqualFrequencyDiscretiser\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS, ExhaustiveFeatureSelector\nfrom sklearn.decomposition import PCA, KernelPCA as kp\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.manifold import TSNE\nfrom scipy.stats import chi2_contingency\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.utils import class_weight\n\n# Modeling & Accuracy Metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier as knc\nfrom sklearn.tree import DecisionTreeClassifier as dtc\nfrom sklearn.ensemble import RandomForestClassifier as rfc, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgbm\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import f1_score, classification_report, confusion_matrix\n\n# Validation and Hyperparameter Tuning\nfrom sklearn.model_selection import KFold, cross_val_score as cvs, RandomizedSearchCV, GridSearchCV\n\n# Saving the Model - Pickling\nimport pickle\n\n# Utility Libraries\nfrom collections import Counter","b0b2578a":"# Reading the Train Dataset into a pandas dataframe\ntrain_set = pd.read_csv('\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Datasets\/train_LZdllcl.csv', encoding = 'ISO-8859-1', error_bad_lines = False)","11bbe717":"# Reading the Test Dataset into a pandas dataframe\ntest_set = pd.read_csv('\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Datasets\/test_2umaH9m.csv', encoding = 'ISO-8859-1', error_bad_lines = False)","5fc4a105":"# to find out the shape of the dataframes\ntrain_set.shape, test_set.shape","3d6463c9":"# Shape of the Given Train Dataset\ntrain_set.shape","8cff89bf":"# Spliting Train Dataset into Train and Train-Remain (For further Validation and Test Spliting)\ntrain, train_remain = train_test_split(train_set, test_size = 0.3, random_state = 0, shuffle = True)","c873cddb":"# Spliting Train_Remain into Validation and Test \nval, test = train_test_split(train_remain, test_size = 0.5, random_state = 0)","df8f2a30":"# Shapes of Train, Validation and Test\ntrain.shape, val.shape, test.shape","fd0439e6":"# Figuring out the % of Split\n\nprint(f\"Split % of Train Set : {round(train.shape[0]\/train_set.shape[0] * 100)} %\")\nprint(f\"Split % of Validation Set : {round(val.shape[0]\/train_set.shape[0] * 100)} %\")\nprint(f\"Split % of Test Set : {round(test.shape[0]\/train_set.shape[0] * 100)} %\")","c05b5ccc":"# resetting the index of the newly formed dataframes\ntrain.reset_index(inplace = True)\nval.reset_index(inplace = True)\ntest.reset_index(inplace = True)","409beb16":"# dropping the extra redundant feature - 'index'\ntrain.drop(['index'], axis = 1, inplace = True)\nval.drop(['index'], axis = 1, inplace = True)\ntest.drop(['index'], axis = 1, inplace = True)","e45e1bce":"# displaying the first 10 records of the dataframe\ntrain.head(10)","e8a33f35":"# displaying the last 10 records of the dataframe\ntrain.tail(10)","5fba9077":"# to know about the datatypes of each feature\ntrain.info()","2924707f":"# to derive statistical distributions of the features\ntrain.describe()","a8a9c765":"# count for unique elements per feature\nunique_Counts = train.nunique(dropna = False) # dropna = False - makes nunique treat NaNs as a distinct value\nunique_Counts.sort_values()","ab85b393":"const_Features = unique_Counts.loc[unique_Counts == 1].index.tolist()\nprint(const_Features)","176b008c":"# To Find out Repeatative Features (Duplicate)\ntrain_enc =  pd.DataFrame(index = train.index)\n\nfor col in tqdm_notebook(train.columns):\n    train_enc[col] = train[col].factorize()[0]\n\ndup_cols = {}\n\nfor i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n    for c2 in train_enc.columns[i + 1:]:\n        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n            dup_cols[c2] = c1","9b5a38c2":"dup_cols","f2805aa1":"# to see how many values are missing in each column.\ntrain.isnull().sum()","459014f1":"# visualizing and observing the null elements in the dataset\nnullPlot = sns.heatmap(train.isnull(), cbar = False, cmap = 'YlGnBu')   # ploting missing data && # cbar, cmap = colour bar, colour map\nnullPlot.set_xticklabels(labels = train.columns, rotation = 30)\nplt.gcf().set_size_inches(20, 5)","82aae7e0":"# to find out the percentage of missing values for each feature\ntrain.isnull().mean()","aa4c348b":"# max of nans for a particular record\ntrain.isnull().sum(axis = 1).sort_values()[-1:]","9e0576f6":"# percentage of null values in their respective feature\npercentage_Of_Null_Education = train.education.isnull().mean()\npercentage_Of_Null_Prev_Year_Rating = train.previous_year_rating.isnull().mean()\n\nprint(f\"% of 'Education' Records having 'NULL' values : {round(percentage_Of_Null_Education * 100, 2)} %\")\nprint(f\"% of 'Previous Year Rating' Records having 'NULL' values : {round(percentage_Of_Null_Prev_Year_Rating * 100, 2)} %\")","19a3dd90":"# finding out insights about Education\nprint(f\"No. of Categories of Education : {train.education.nunique()} \\nThe Slabs are : {list(train.education.unique())}\")","e5c1a7c1":"# Imputing NULL Values of 'Education' as 'Others'\nedu_impute = pd.DataFrame(train[\"education\"].fillna(\"Others\"))\nedu_impute.education.unique()","b369ed6e":"# Before Imputation\ntrain.education.unique(), val.education.unique(), test.education.unique()","139f9946":"# imputing the same in all the dataframes as it's independent of the central limit tendency \ntrain[\"education\"].fillna(\"Others\", inplace = True)\nval[\"education\"].fillna(\"Others\", inplace = True)\ntest[\"education\"].fillna(\"Others\", inplace = True)","1796e5d6":"# After Imputation\ntrain.education.unique(), val.education.unique(), test.education.unique()","59faa39f":"# deleting redundant dataframe\ndel edu_impute","bd92b01c":"# finding out insights about previous year's rating\nprint(f\"No. of categories of employees' previous years' rating : {train.previous_year_rating.nunique()} \\nThe different ratings are : {list(train.previous_year_rating.unique())}\")","177826a0":"# Finding out the Mean, Median, Mode of Previous Year Ratings\ntrain.previous_year_rating.mean(), train.previous_year_rating.median(), train.previous_year_rating.mode().iloc[0]","8dd113d6":"# trying out different imputations to find out best fit\nprev_Year_Ratings_impu = pd.DataFrame()\nprev_Year_Ratings_impu['previous_year_rating'+'_mean'] = train['previous_year_rating'].fillna(train.previous_year_rating.mean())\nprev_Year_Ratings_impu['previous_year_rating'+'_median'] = train['previous_year_rating'].fillna(train.previous_year_rating.median())\nprev_Year_Ratings_impu['previous_year_rating'+'_mode'] = train['previous_year_rating'].fillna(train.previous_year_rating.mode().iloc[0])\nprev_Year_Ratings_impu['previous_year_rating'+'_zero'] = train['previous_year_rating'].fillna(0)\n\n# KNN Imputer\nimputer = KNNImputer(n_neighbors = 3)\nx = np.array(train['previous_year_rating'])\nx = x.reshape(-1,1)\nprev_Year_Ratings_impu['previous_year_rating'+'_knn'] = imputer.fit_transform(x)","a532fe17":"prev_Year_Ratings_impu.head(10)","e40fa9d3":"# Distribution Plot of all the Imputation Types\nfig = plt.figure(figsize = (20,8))\nax = fig.add_subplot(111)\ntrain['previous_year_rating'].plot(kind = 'kde', ax = ax, color = 'red')\nprev_Year_Ratings_impu.previous_year_rating_zero.plot(kind = 'kde', ax = ax, color = 'green')\nprev_Year_Ratings_impu.previous_year_rating_mean.plot(kind = 'kde', ax = ax, color = 'blue')\nprev_Year_Ratings_impu.previous_year_rating_median.plot(kind = 'kde', ax = ax, color = 'yellow')\nprev_Year_Ratings_impu.previous_year_rating_mode.plot(kind = 'kde', ax = ax, color = 'black')\nprev_Year_Ratings_impu.previous_year_rating_knn.plot(kind = 'kde', ax = ax, color = 'pink')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc = 'best')","d58af939":"# we can see that the distribution has changed slightly with now more values accumulating towards the median\nfig = plt.figure(figsize = (20,8))\nax = fig.add_subplot(111)\ntrain['previous_year_rating'].plot(kind = 'kde', ax = ax, color = 'red')\nprev_Year_Ratings_impu.previous_year_rating_zero.plot(kind = 'kde', ax = ax, color = 'green')\nprev_Year_Ratings_impu.previous_year_rating_mean.plot(kind = 'kde', ax = ax, color = 'blue')\n# prev_Year_Ratings_impu.previous_year_rating_median.plot(kind = 'kde', ax = ax, color = 'yellow')\n# prev_Year_Ratings_impu.previous_year_rating_mode.plot(kind = 'kde', ax = ax, color = 'black')\nprev_Year_Ratings_impu.previous_year_rating_knn.plot(kind = 'kde', ax = ax, color = 'pink')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc = 'best')","263d41bf":"# Relating Length of Service with Ratings to find out relations\ntemp = train.copy()\ntemp.previous_year_rating.fillna(-1, inplace = True)\ntemp.groupby(['previous_year_rating']).length_of_service.value_counts()","ca172af5":"# No. of Employee who had no Rating\ntrain.previous_year_rating.isnull().sum()","99a439b4":"# Before Imputation\ntrain.previous_year_rating.unique(), val.previous_year_rating.unique(), test.previous_year_rating.unique()","8cd24e6f":"# imputing the same in all the dataframes as it's independent of the central limit tendency \n# Imputing NULL Values of 'Previous Year's' as '0'\ntrain[\"previous_year_rating\"].fillna(0, inplace = True)\nval[\"previous_year_rating\"].fillna(0, inplace = True)\ntest[\"previous_year_rating\"].fillna(0, inplace = True)","6eb7a533":"# After Imputation\ntrain.previous_year_rating.unique(), val.previous_year_rating.unique(), test.previous_year_rating.unique()","d58a1ac8":"# deleting redundant dataframes\ndel prev_Year_Ratings_impu, temp","89092701":"# Checking out if any NULL value is still present in the dataframe\ntrain.isnull().sum(), val.isnull().sum(), test.isnull().sum()","49abb255":"# temporarily segregating into independent vector to take out quasi constant features\nx = train.drop(['is_promoted'], axis = 1)\nx.head(10)","972391af":"# Multi-Feature Label Encoder Class\n\nclass multiFeatureLabelEncoder:\n\n    def __init__(self, cols = None):\n        self.cols = cols\n\n    def fit(self, x, y = None):\n        return self\n\n    def transform(self, x):\n        output = x.copy()\n        if self.cols is not None:\n            for col in self.cols:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname, col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self, x, y = None):\n        z = self.fit(x,y).transform(x)\n        print(z.info())\n        return z","fa4ec173":"# displaying the head aiding to get string features\nx.head(1)","4f110d01":"# Label Encoding Multiple Feature\nx = multiFeatureLabelEncoder(['department', 'region', 'education', 'gender', 'recruitment_channel']).fit_transform(x)","aaf7bacb":"# Finding out Quasi Constant Features\nquasiDetect = VarianceThreshold(threshold = 0.01)  \nquasiDetect.fit(x)","26fc1831":"# No. of Columns Retained ie non-quasi features\nlen(x.columns[quasiDetect.get_support()])","bba9186e":"# Displaying the Quasi Constant Features\nprint(f\"No. of Quasi-Constant Features : {len([y for y in x.columns if y not in x.columns[quasiDetect.get_support()]])}\")\nprint(f\"Quasi-Constant Features : {[y for y in x.columns if y not in x.columns[quasiDetect.get_support()]]}\")","5caeeb17":"# Deleting off redundant objects\/variables - Memory Efficient Techniques\ndel x, unique_Counts, const_Features, train_enc, dup_cols, nullPlot, percentage_Of_Null_Education, percentage_Of_Null_Prev_Year_Rating, imputer, fig, ax, quasiDetect","68e0c14c":"# Displaying out the columns of train\ntrain.columns","2d37297c":"# Statistical Info about train\ntrain.describe()","a9e036ae":"# checking out if there's any duplicacy in employee ID, ie it should be 38365\ntrain.employee_id.nunique()","bd9a91a7":"# finding out insights about departments present in the company\nprint(f\"No. of Departments : {train.department.nunique()} \\nThe Departments are : {list(train.department.unique())}\")","e86a0fdd":"# count plot to visualise the employee counts of the department\nplot = sns.catplot(data = train, kind = 'count', x = 'department')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(20, 5)","a3c896ae":"# count of no of people in each department\n\n# train['department'].value_counts().sort_values(ascending = False)\nx = []\ndept = train['department'].unique()\nfor i in dept:\n  x.append(train.loc[train.department == i, 'department'].count())\n\ni=0\nwhile i < len(dept):\n  #print(i)\n  print(\"No of Employees in the Department of \", train['department'].unique()[i],\" are : \", x[i])\n  i+=1","15a94b60":"# % of the departments having highest and lowest employees\nsales = train.loc[train.department == 'Sales & Marketing', 'department'].count()\nrnd = train.loc[train.department == 'R&D', 'department'].count()\n\nprint(f\"% of Employees in 'Sales & Marketing' : {round(sales\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees in 'R&D' : {round(rnd\/train.shape[0]*100, 2)} %\")","e4337492":"# finding out insights about regions\nprint(f\"No. of Regions : {train.region.nunique()} \\nThe Regions are : {list(train.region.unique())}\")","a4a43543":"# sort the regions in ascending order\npd.Series(train.region.unique()).sort_values()","d5967061":"# count plot to visualise the counts of the regions\nplot = sns.catplot(data = train, kind = 'count', x = 'region')\nplot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(20, 5)","98def041":"# No. of Employees per Region\ntrain['region'].value_counts().sort_values(ascending = False)","7b843db6":"# % of the regions having highest and lowest employees\nregion_2 = train.loc[train.region == 'region_2', 'region'].count()\nregion_18 = train.loc[train.region == 'region_18', 'region'].count()\n\nprint(f\"% of Employees from 'region_2' : {round(region_2\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees from 'region_18' : {round(region_18\/train.shape[0]*100, 2)} %\")","8cf72f6f":"# finding out insights about Education\nprint(f\"No. of Categories of Education : {train.education.nunique()} \\nThe Slabs are : {list(train.education.unique())}\")","3c52afe1":"# count plot to visualise the counts of the slabs of education\nplot = sns.catplot(data = train, kind = 'count', x = 'education')\n#plot.set_xticklabels(rotation = 30)\nplt.gcf().set_size_inches(20, 5)","5d3a9bb8":"train['education'].value_counts().sort_values(ascending = False)","731c978b":"# % of the slabs of education having highest and lowest employees\nbachelors = train.loc[train.education == \"Bachelor's\", 'education'].count()\nbelow_Secondary = train.loc[train.education == 'Below Secondary', 'education'].count()\n\nprint(f\"% of Employees from 'Bachelors' : {round(bachelors\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees from 'Below Secondary' : {round(below_Secondary\/train.shape[0]*100, 2)} %\")","0f39d1f6":"# finding out insights about Gender\nprint(f\"No. of Categories for Gender : {train.gender.nunique()} \\nThe Categories are : {list(train.gender.unique())}\")","43484ce7":"# count plot to visualise the counts of Gender\nplot = sns.catplot(data = train, kind = 'count', x = 'gender')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","8f0db7f5":"train['gender'].value_counts()","9c438153":"# Gender % of Employees\nmale = train.loc[train.gender == 'm', 'gender'].count()\nfemale = train.loc[train.gender == 'f', 'gender'].count()\n\nprint(f\"% of 'Male' Employees : {round(male\/train.shape[0]*100, 2)} %\")\nprint(f\"% of 'Female' Employees : {round(female\/train.shape[0]*100, 2)} %\")","0b9ec73e":"# finding out insights about Recruitment Channel\nprint(f\"No. of Categories for Recruitment Channel : {train.recruitment_channel.nunique()} \\nThe Recruitment Channels are : {list(train.recruitment_channel.unique())}\")","c4189198":"# count plot to visualise the counts of Gender\nplot = sns.catplot(data = train, kind = 'count', x = 'recruitment_channel')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","7f29eadd":"train['recruitment_channel'].value_counts().sort_values(ascending = False)","7ee2ac18":"# % of the Employee Recruitment_Channel having highest and lowest employees\nother = train.loc[train.recruitment_channel == 'other', 'recruitment_channel'].count()\nreff = train.loc[train.recruitment_channel == 'referred', 'recruitment_channel'].count()\n\nprint(f\"% of Employees recruited through 'other' sources : {round(other\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees recruited through 'Reference' : {round(reff\/train.shape[0]*100, 2)} %\")","a95bc95f":"# finding out insights about no of trainings employees have\nprint(f\"No. of Unique No. of Training : {train.no_of_trainings.nunique()} \\nNo. of Trainings : {list(train.no_of_trainings.unique())}\")","cab60d98":"# count plot to visualise the counts of Gender\nplot = sns.catplot(data = train, kind = 'count', y = 'no_of_trainings')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","78c65998":"# Scatter Plot to Visualise the Points in detail\nplt.figure(figsize = (18, 7))\nplt.scatter(range(train.shape[0]), train['no_of_trainings'])","2b1dce44":"train['no_of_trainings'].value_counts().sort_values(ascending = False)","d81cb0ed":"# % of the Employees having 1 training\none_Train = train.loc[train.no_of_trainings == 1, 'no_of_trainings'].count()\n\nprint(f\"% of Employees having 1 training : {round(one_Train\/train.shape[0]*100, 2)} %\")","1a37f923":"# Box Plot to find more information about it's Quartiles and also detect Outliers\nplt.figure(figsize = (8, 8))\nplt.title('No of Trainings Distribution Spread')\nsns.boxplot(y = train.no_of_trainings)","0a53b105":"# trying out and observing the distribution plot of \u2018no_of_trainings\u2019 after boxcox transformation.\nboxc = stats.boxcox(train['no_of_trainings'])[0]\nprint(f\"Skewness after Transformation : {pd.Series(boxc).skew()}\")\nplt.title('No of Trainings Distribution Spread')\nsns.boxplot(y = boxc)","19d59eb3":"# finding out insights about age of employees working\nprint(f\"No. of unique value of age amonsgt the employees : {train.age.nunique()}\")","41460411":"# Binnig\nrange = ['<25', '26-30', '30-35', '36-40', '41-45', '46-50', '51-55', '56-60']\nbins = [20, 26, 31, 36, 41, 46, 51, 56, 61] # [20, 26, 31, 36, 50] \nage_EDA = pd.DataFrame(pd.cut(x = train['age'], bins = bins, labels = range, include_lowest = True))\n#X_train['age'] = pd.cut(x=X_train['age'], bins=[20, 30, 39, 49], labels=['20', '30', '40'] )\n#X_test['age']  = pd.cut(x=X_test['age'], bins=[20, 30, 39, 49], labels=['20', '30', '40'] )","239a4d27":"# No. of Unique Age Groups\nage_EDA.nunique()","f3d3ac75":"# displaying the 1st 10 records after binning\ntrain.age.head(10)","0b926b3a":"# displaying the 1st 10 records after binning\nage_EDA.head(10)","661d7104":"# displaying no. of employees in each age category\nage_EDA.age.value_counts().sort_values(ascending = False)","e6ff1a76":"# count plot to visualise the counts of Gender\nplot = sns.catplot(data = age_EDA, kind = 'count', y = 'age')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","a2d10a30":"# % of the Slabs of Age Group having highest and lowest employees\ncat26To30 = age_EDA.loc[age_EDA.age == '26-30', 'age'].count()\ncat55To60 = age_EDA.loc[age_EDA.age == '56-60', 'age'].count()\n\nprint(f\"% of Employees under the age-group '26-30' : {round(cat26To30\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees under the age-group '55-60' : {round(cat55To60\/train.shape[0]*100, 2)} %\")","5cb2657d":"# deleting redundant dataframe\ndel age_EDA","01011178":"# Box Plot to find more information about it's Quartiles and also detect Outliers\nplt.figure(figsize = (8, 8))\nplt.title('Age Distribution Spread')\nsns.boxplot(y = train.age)","187c050c":"# For IQR Analysis, we need to sort our feature vector first\narr = np.array(sorted(train.age))","7dfba1f3":"# Defining the Quartile Ranges\nquantile_1, quantile_3 = np.percentile(arr, [25, 75])\nprint(f\"1st Quartile (25th Percentile) of Age Feature : {quantile_1} \\n3rd Quartile (75th Percentile) of Age Feature : {quantile_3}\")","ac4093b1":"# Defining the IQR\niqr = quantile_3 - quantile_1\nprint(iqr)","f487a677":"# Finding out the Lower Bound Value and the Higher Bound Value\nlower_Bound = quantile_1 - (1.5 * iqr) \nupper_Bound = quantile_3 + (1.5 * iqr)\nprint(f\"Lower Bound Value : {lower_Bound} \\nUpper Bound Value : {upper_Bound}\")","c966667b":"# Any number below the Lower Bound or above the Upper Bound would be considered as an Outlier\noutliers = train.age[(train.age < lower_Bound) | (train.age > upper_Bound)]\nprint(f\"Number of Outliers : {len(outliers)} \\nThe Outliers are : {outliers.unique()}\")","7a4f250d":"# % of entries which are outliers\nprint(f\"% of Records which are been considered Outliers : {round(len(outliers)\/train.shape[0] * 100, 2)} %\")","02a0179c":"# Distribution Plot to visualise the distribution of age of employees\nsns.distplot(train.age)\n# plot.set_xticklabels(rotation = 30)\nplt.gcf().set_size_inches(20, 5)","cb432392":"# Finding out age's distribution statistics\nplt.hist(train.age, bins = 'auto')\n\nprint(\"mean : \", np.mean(train.age))\nprint(\"var  : \", np.var(train.age))\nprint(\"skew : \", stats.skew(train.age))\nprint(\"kurt : \", stats.kurtosis(train.age), \"\\n\")","b21f0dff":"# trying out square-root and log transformations to reduce skewness\nlog = np.log(train['age'])\nsqr = np.sqrt(train['age'])\nprint(log.skew(), sqr.skew())","fe06b897":"# trying out and observing the distribution plot of \u2018age\u2019 after boxcox transformation.\nboxc = stats.boxcox(train['age'])[0]\nprint(f\"Skewness after Transformation : {pd.Series(boxc).skew()}\")\nsns.distplot(boxc)","59d24a52":"# count plot to visualise the counts of the slabs of education\nplot = sns.catplot(data = train, kind = 'count', x = 'previous_year_rating')\n#plot.set_xticklabels(rotation = 30)\nplt.gcf().set_size_inches(20, 5)","405f3f7e":"# Showing the counts of employee per ratings\ntrain['previous_year_rating'].value_counts().sort_values(ascending = False)","011703ff":"# % of the previous years' ratings of employees having highest and lowest ratings along with the employees having 5 rating\nhigh_Per_Rating = train.loc[train.previous_year_rating == 3.0, 'previous_year_rating'].count()\nlow_Per_Rating = train.loc[train.previous_year_rating == 0.0, 'previous_year_rating'].count()\n_5_Rating = train.loc[train.previous_year_rating == 5.0, 'previous_year_rating'].count()\n\nprint(f\"% of Employees having Rating '3.0' : {round(high_Per_Rating\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees having Rating '0.0' or 'NO Rating' : {round(low_Per_Rating\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees having Rating '5.0' : {round(_5_Rating\/train.shape[0]*100, 2)} %\")","f97ac49d":"train.previous_year_rating.unique()","b0ce13d7":"# finding out insights about length of service\nprint(f\"No. of unique value for employees' length of service : {train.length_of_service.nunique()}\")","f7c3c085":"print(f\"Max Service Length : {max(train.length_of_service)} \\nMin Service Length : {min(train.length_of_service)}\")","4d5f6942":"# count plot to visualise the distribution of length of service\nsns.distplot(train.length_of_service)\n# plot.set_xticklabels(rotation = 30)\nplt.gcf().set_size_inches(20, 5)","fba94027":"# Finding out LOS's distribution statistics\nplt.hist(train.length_of_service, bins = 'auto')\n\nprint(\"mean : \", np.mean(train.length_of_service))\nprint(\"var  : \", np.var(train.length_of_service))\nprint(\"skew : \", stats.skew(train.length_of_service))\nprint(\"kurt : \", stats.kurtosis(train.length_of_service), \"\\n\")","072c4b89":"# trying out square-root and log transformations to reduce skewness\nlog = np.log(train.length_of_service)\nsqr = np.sqrt(train.length_of_service)\nprint(log.skew(), sqr.skew())","a71e15d5":"# trying out and observing the distribution plot of \u2018LOS\u2019 after boxcox transformation.\nboxc = stats.boxcox(train.length_of_service)[0]\nprint(f\"Skewness after Transformation : {pd.Series(boxc).skew()}\")\nsns.distplot(boxc)","2eb0f024":"# Box Plot to find more information about it's Quartiles and also detect Outliers\nplt.figure(figsize = (8, 8))\nplt.title('Length of Service Distribution Spread')\nsns.boxplot(y = train.length_of_service)","3047f6bd":"# For IQR Analysis, we need to sort our feature vector first\narr = np.array(sorted(train.length_of_service))","6594de12":"# Defining the Quartile Ranges\nquantile_1, quantile_3= np.percentile(arr, [25, 75])\nprint(f\"1st Quartile (25th Percentile) of Age Feature : {quantile_1} \\n3rd Quartile (75th Percentile) of Age Feature : {quantile_3}\")","ffdbec65":"# Defining the IQR\niqr = quantile_3 - quantile_1\nprint(iqr)","55915e85":"# Finding out the Lower Bound Value and the Higher Bound Value\nlower_Bound = quantile_1 - (1.5 * iqr) \nupper_Bound = quantile_3 + (1.5 * iqr)\nprint(f\"Lower Bound Value : {lower_Bound} \\nUpper Bound Value : {upper_Bound}\")","a24158e3":"# Any number below the Lower Bound or above the Upper Bound would be considered as an Outlier\noutliers = train.length_of_service[(train.length_of_service < lower_Bound) | (train.length_of_service > upper_Bound)]\nprint(f\"Number of Outliers : {len(outliers)} \\nThe Outliers are : {sorted(outliers.unique())}\")","1dcfbfbf":"# % of entries which are outliers\nprint(f\"% of Records which are been considered Outliers : {round(len(outliers)\/train.shape[0] * 100, 2)} %\")","39afbae7":"# finding out insights about KPIs met\nprint(f\"No. of unique value for KPIs met by employees : {train['KPIs_met >80%'].nunique()}\")","064a1225":"# count plot to visualise the counts of 'KPIs_met >80%'\nplot = sns.catplot(data = train, kind = 'count', x = 'KPIs_met >80%')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","4f26c94d":"# count the occurence for each category\ntrain['KPIs_met >80%'].value_counts()","33126f60":"# 'KPIs_met >80%' % of Employees\nkpis_Met = train.loc[train['KPIs_met >80%'] == 0, 'KPIs_met >80%'].count()\nkpis_Not_Met = train.loc[train['KPIs_met >80%'] != 0, 'KPIs_met >80%'].count()\n\nprint(f\"% of Employees who satisfied KPIs : {round(kpis_Met\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees who didn't satisfied KPIs : {round(kpis_Not_Met\/train.shape[0]*100, 2)} %\")","cfe8f47d":"# finding out insights about awards won by employees\nprint(f\"No. of unique value for awards won by employees : {train['awards_won?'].nunique()}\")","c53d3ec1":"# count plot to visualise the counts of 'awards_won?'\nplot = sns.catplot(data = train, kind = 'count', x = 'awards_won?')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","5f004ef1":"# count the occurence for each category\ntrain['awards_won?'].value_counts()","6423da4c":"# 'awards_won?' % of Employees\nemp_Win = train.loc[train['awards_won?'] == 0, 'awards_won?'].count()\nemp_Not_Win = train.loc[train['awards_won?'] != 0, 'awards_won?'].count()\n\nprint(f\"% of Employees who have won atleast an award : {round(emp_Win\/train.shape[0]*100, 2)} %\")\nprint(f\"% of Employees who haven't won any award : {round(emp_Not_Win\/train.shape[0]*100, 2)} %\")","09565319":"# finding out insights about average training scores\nprint(f\"No. of unique value for employees' average training scores : {train.avg_training_score.nunique()}\")","f242733c":"print(f\"Max Average Training Score : {max(train.avg_training_score)} \\nMin Average Training Score : {min(train.avg_training_score)}\")","8ffb55ce":"# count plot to visualise the distribution of average training score\nsns.distplot(train.avg_training_score)\n# plot.set_xticklabels(rotation = 30)\nplt.gcf().set_size_inches(20, 5)","3c2df20b":"# Finding out Avg Training Score distribution statistics\nplt.hist(train.avg_training_score, bins = 100)\n\nprint(\"mean : \", np.mean(train.avg_training_score))\nprint(\"var  : \", np.var(train.avg_training_score))\nprint(\"skew : \", stats.skew(train.avg_training_score))\nprint(\"kurt : \", stats.kurtosis(train.avg_training_score), \"\\n\")","ba06bab5":"# Box Plot to find more information about it's Quartiles and also detect Outliers\nplt.figure(figsize = (8, 8))\nplt.title('Length of Average Training Score Distribution Spread')\nsns.boxplot(y = train.avg_training_score)","32102901":"# Binnig to find out more information about the same.\nrange = ['<40', '41-50', '51-60', '61-70', '71-80', '81-90', '91-100']\nbins = [31, 41, 51, 61, 71, 81, 91, 101] # [20, 26, 31, 36, 50] \navg_Tr_Score_EDA = pd.DataFrame(pd.cut(x = train['avg_training_score'], bins = bins, labels = range, include_lowest = True))\n#X_train['age'] = pd.cut(x=X_train['age'], bins=[20, 30, 39, 49], labels=['20', '30', '40'] )\n#X_test['age']  = pd.cut(x=X_test['age'], bins=[20, 30, 39, 49], labels=['20', '30', '40'] )","6f4fd913":"avg_Tr_Score_EDA.nunique()","eefd4dd0":"avg_Tr_Score_EDA.avg_training_score.value_counts().sort_values(ascending = False)","a961df4f":"# count plot to visualise the counts of Gender\nplot = sns.catplot(data = avg_Tr_Score_EDA, kind = 'count', y = 'avg_training_score')\n# plot.set_xticklabels(rotation = 75)\nplt.gcf().set_size_inches(15, 5)","f53da82f":"# % of the Slabs of Employees having Highest and Lowest Average Test Scores\ncat51To60 = avg_Tr_Score_EDA.loc[avg_Tr_Score_EDA.avg_training_score == '51-60', 'avg_training_score'].count()\ncatless40 = avg_Tr_Score_EDA.loc[avg_Tr_Score_EDA.avg_training_score == '<40', 'avg_training_score'].count()\n\nprint(f\"% of Average Test Score of  Employees under the group '51-60' : {round(cat51To60\/avg_Tr_Score_EDA.shape[0]*100, 2)} %\")\nprint(f\"% of Average Test Score of  Employees under the group '<40' : {round(catless40\/avg_Tr_Score_EDA.shape[0]*100, 2)} %\")","2b6dceb4":"# No. of Employees having Max Score\ntrain.loc[train.avg_training_score == max(train.avg_training_score.values), 'avg_training_score'].count()","efc60d60":"# deleting redundant dataframe\ndel avg_Tr_Score_EDA","82cc7424":"# finding out insights about average training scores\nprint(f\"Unique values for target variable is_promoted : {train.is_promoted.nunique()}\")","75999730":"# count plot to visualise the counts of the categories\nsns.catplot(data = train, kind = 'count', x = 'is_promoted')","7c3a717c":"# to have a count for how many people got promoted or hot\ngotPromoted = train.loc[train.is_promoted == 1, 'is_promoted'].count()\ndidNotGetPromoted = train.loc[train.is_promoted != 1, 'is_promoted'].count()\n\nprint(f\"No. of Employees that got Promoted : {gotPromoted} and it's % : {round(gotPromoted\/train.shape[0]*100, 2)}\")\nprint(f\"No. of Employees that didn't get Promoted : {didNotGetPromoted} and it's % : {round(didNotGetPromoted\/train.shape[0]*100, 2)}\")","bf18951a":"# Deleting off redundant objects\/variables - Memory Efficient Techniques\ndel plot, x, dept, region_2, region_18, bachelors, below_Secondary, male, female, other, reff, one_Train, boxc, range, bins, cat26To30, cat55To60, arr, quantile_1, quantile_3, iqr, lower_Bound , upper_Bound, outliers, log, sqr, high_Per_Rating, low_Per_Rating, _5_Rating, kpis_Met, kpis_Not_Met, emp_Win, emp_Not_Win, cat51To60, catless40, gotPromoted, didNotGetPromoted","5ed37b5d":"# Displaying the Columns\ntrain.columns","5f2105d8":"# Displaying the 'Object' Columns for encoding (for EDA)\nlist(train.select_dtypes(include = ['object']).columns)","cacc7cce":"# Encoding 'Education'\ndummy_Train = train.copy()  # to keep the integrity of main train intact.\n\n# Mannualy Assigning Weight and Encoding 'ordinal' Feature 'Education'\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\ndummy_Train['education'] = train['education'].map(edu_enc)\ndummy_Train.education.unique()","cb05ba9e":"# Label Encoding 'Gender'\nl = LabelEncoder()\ndummy_Train.loc[:, 'gender'] = l.fit_transform(train.loc[:, 'gender'])\ndummy_Train.gender.unique()","56b1987c":"# Defining K-Fold Target Encoding Class for Train (K-Fold as for Regularization)\n\nclass KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n\n    def __init__(self, colname, targetName, n_fold = 5):\n\n        self.colnames = colname\n        self.targetName = targetName\n        self.n_fold = n_fold\n\n    def fit(self, x, y = None):\n        return self\n\n    def transform(self, x):\n        assert(type(self.targetName) == str)\n        assert(type(self.colnames) == str)\n        assert(self.colnames in x.columns)\n        assert(self.targetName in x.columns)\n\n        mean_of_target = x[self.targetName].mean()\n        kf = KFold(n_splits = self.n_fold, shuffle = False, random_state=0)\n\n        col_mean_name = 'tgt_' + self.colnames\n        x[col_mean_name] = np.nan\n\n        for tr_ind, val_ind in kf.split(x):\n            x_tr, x_val = x.iloc[tr_ind], x.iloc[val_ind]\n            x.loc[x.index[val_ind], col_mean_name] = x_val[self.colnames].map(x_tr.groupby(self.colnames)[self.targetName].mean())\n\n        x[col_mean_name].fillna(mean_of_target, inplace = True)\n\n        return x","8a20424b":"# K-Fold Target Encoding 'recruitment_channel'\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ndummy_Train = targetc.fit_transform(dummy_Train)\ndummy_Train.drop(['recruitment_channel'], axis = 1, inplace = True)","6f4d9bd7":"# K-Fold Target Encoding 'region'\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ndummy_Train = targetc.fit_transform(dummy_Train)\ndummy_Train.drop(['region'], axis = 1, inplace = True)","3970df4b":"# K-Fold Target Encoding 'department'\ntargetc = KFoldTargetEncoderTrain('department', 'is_promoted', n_fold = 5)\ndummy_Train = targetc.fit_transform(dummy_Train)\ndummy_Train.drop(['department'], axis = 1, inplace = True)","0993f1bb":"# The Modified Dummy Dataset for Multivariate EDA\ndummy_Train.head(10)","b9365b41":"# Plotty Interactive Visualisation to find out the inter-relations amongst all the features to dependent vector\nfig = px.parallel_categories(train[train.columns], color = \"is_promoted\", color_continuous_scale = px.colors.sequential.Aggrnyl)\nfig.show()","621dcaba":"# plotting pairwise relationships in train with respect to the dependent vector\nsns.pairplot(dummy_Train.drop(['employee_id'], axis = 1), hue = 'is_promoted')","fb8084a2":"# Correlation using heatmap\nhm = dummy_Train.corr().where(np.tril(np.ones(dummy_Train.corr().shape)).astype(np.bool)) # to delete the upper triangle\nplot = sns.heatmap(hm, annot = True, cmap=\"YlGnBu\")\nplt.setp(plot.get_xticklabels(), rotation=45)\nplt.gcf().set_size_inches(15, 8)","24c3fcf3":"# Covariance using Heatmap\ndummy_Train = dummy_Train.round(decimals = 2)\ncovMatrix = pd.DataFrame.cov(dummy_Train.drop(['employee_id'], axis = 1))\nsns.heatmap(covMatrix, annot = True, fmt = 'g')\nplt.gcf().set_size_inches(30, 15)","9680b486":"# Scatter Plots of the whole train set with respect to each other.\nscatter_matrix(dummy_Train.drop(['employee_id'], axis = 1), alpha = 0.2, figsize = (25, 25))","0b6fbf34":"# displaying column names for reference\ndummy_Train.columns","2f4889c2":"# Bi-Variate Analysis on 'Age' and 'Average Training Score' which are inversely related\nsns.jointplot(x = 'age', y = 'avg_training_score', data = train, kind = 'reg')","48602366":"# Multivariate Analysis of - 'is_promoted', 'length_of_service', 'awards_won?'\nsns.FacetGrid(dummy_Train, hue = \"is_promoted\", size = 5).map(plt.scatter, \"length_of_service\", \"awards_won?\").add_legend()\nplt.show()","b4282947":"# Multivariate Analysis of - 'is_promoted', 'no_of_trainings', 'previous_year_rating'\nsns.FacetGrid(train, hue = \"is_promoted\", size = 5).map(plt.scatter, \"no_of_trainings\", \"previous_year_rating\").add_legend()\nplt.show()","a0533b90":"# Multivariate Analysis of - 'is_promoted', 'average training score', 'age'\nsns.FacetGrid(train, hue = \"is_promoted\", size = 5).map(plt.scatter, \"avg_training_score\", \"age\").add_legend()\nplt.show()","d206060d":"# Checking out the distribution of 'is_promoted' across the newly Encoded Variables\nplt.figure(figsize=(25, 6))\n\ndf = pd.DataFrame(train.groupby(['department'])['is_promoted'].mean().sort_values(ascending = False))\ndf.plot.bar()\nplt.title('Dept. vs Promotion')\nplt.show()\n\ndf = pd.DataFrame(train.groupby(['region'])['is_promoted'].mean().sort_values(ascending = False))\ndf.plot.bar()\nplt.title('Region vs Promotion')\nplt.show()\n\ndf = pd.DataFrame(train.groupby(['gender'])['is_promoted'].mean().sort_values(ascending = False))\ndf.plot.bar()\nplt.title('Gender vs Promotion')\nplt.show()\n\ndf = pd.DataFrame(train.groupby(['education'])['is_promoted'].mean().sort_values(ascending = False))\ndf.plot.bar()\nplt.title('Education vs Promotion')\nplt.show()\n\ndf = pd.DataFrame(train.groupby(['recruitment_channel'])['is_promoted'].mean().sort_values(ascending = False))\ndf.plot.bar()\nplt.title('Recruitment Channel vs Promotion')\nplt.show()\n\n# Deleting redundant dataframe\ndel df","190b7718":"# Deleting off redundant objects\/variables - Memory Efficient Techniques\ndel l, targetc, fig, hm, covMatrix","33c8d196":"# Dividing Train into Independent and Dependent Features\nx = train.drop(['is_promoted'], axis = 1)\ny = train['is_promoted'].values.reshape(-1, 1)\nx.shape, y.shape","8ab65a0c":"# Print out the Names of Object Columns\nobjectColumns = []\nfor i in x.columns[x.dtypes == 'object']:   objectColumns.append(i)\nprint(f\"Object Column Names : {objectColumns}\")\n\n# Preserve the Name of the Columns\ncolNames = x.columns\nprint(f\"Column Names : {list(colNames)}\")","60d23bde":"# Temporarily Encoding Department and Preserving Information about each Variable.\nl_dept = LabelEncoder()\nx['department'] = l_dept.fit_transform(x['department'])\n# Temporarily Encoding Region\nl_region = LabelEncoder()\nx['region'] = l_region.fit_transform(x['region'])\n# Temporarily Encoding Education\nl_edu = LabelEncoder()\nx['education'] = l_edu.fit_transform(x['education'])\n# Temporarily Encoding Gender\nl_gender = LabelEncoder()\nx['gender'] = l_gender.fit_transform(x['gender'])\n# Temporarily Encoding Recruitment\nl_rc = LabelEncoder()\nx['recruitment_channel'] = l_rc.fit_transform(x['recruitment_channel'])","1dda7b79":"# Listing the Different Classes\nprint(list(l_dept.classes_))\nprint(list(l_region.classes_))\nprint(list(l_edu.classes_))\nprint(list(l_gender.classes_))\nprint(list(l_rc.classes_))","440092e8":"# Visualising the 1st two Records of the Dataframe\nx.head(2)","cb6cd0f8":"# Decoding and Testing the Intregity\n'''\n# Temporarily Decoding Department\nx['department'] = l_dept.inverse_transform(x['department'])\n# Temporarily Decoding Region\nx['region'] = l_region.inverse_transform(x['region'])\n# Temporarily Decoding Education\nx['education'] = l_edu.inverse_transform(x['education'])\n# Temporarily Decoding Gender\nx['gender'] = l_gender.inverse_transform(x['gender'])\n# Temporarily Decoding Recruitment\nx['recruitment_channel'] = l_rc.inverse_transform(x['recruitment_channel'])\nx.head(2)\n'''","aced073d":"# Over-Sampling to Increase the Number of '1' Samples\n\nsm = SMOTE(sampling_strategy = 'minority', random_state = 0, n_jobs = -1)\nx_os_train, y_os_train = sm.fit_sample(x, y)","2bb15a4a":"# Converting the Over-Sampled Array into a Dataframe for further Operations\ntrain_OS = pd.DataFrame(x_os_train, columns = colNames)\ntrain_OS.rename(index = str).index\ntrain_OS.columns","dee32d8f":"train_OS.info()","2a57d6af":"# Maintaining the Types as Old DataFrame\ntrain_OS = train_OS.astype(int)\ntrain_OS['previous_year_rating'] = train_OS['previous_year_rating'].astype(float)\ntrain_OS.head(1)","9c7ff2eb":"# Decoding the Features as Old DataFrame\n\ntrain_OS['department'] = l_dept.inverse_transform(train_OS['department'])\n# Temporarily Decoding Region\ntrain_OS['region'] = l_region.inverse_transform(train_OS['region'])\n# Temporarily Decoding Education\ntrain_OS['education'] = l_edu.inverse_transform(train_OS['education'])\n'''\n# Not Required as it'll be label encoded to the same later.\n## Temporarily Decoding Gender\ntrain_OS['gender'] = l_gender.inverse_transform(train_OS['gender'])\n'''\n# Temporarily Decoding Recruitment\ntrain_OS['recruitment_channel'] = l_rc.inverse_transform(train_OS['recruitment_channel'])\n\n# Printing the 1st 2 records of the transformed Dataframe\ntrain_OS.head(2)","7d698f4d":"# After Sampling Size\ntrain_OS['is_promoted'] = y_os_train\nprint(f\"Shape of the Over-Sampled DataFrame : {train_OS.shape}\")\nprint(f\"No. of Records Simulated as Compared to Old : {train_OS.shape[0] - x.shape[0]}\")","044077e8":"# count plot to visualise the counts of the categories\nsns.catplot(data = train_OS, kind = 'count', x = 'is_promoted')\nprint(f\"No. of People Not Promoted vs Promoted : {Counter(train_OS.is_promoted)}\")","be9b05f7":"# Visualising Items for Occurence per Categorical Feature\nfor i in objectColumns:\n  print(\"No. of Records per Category of {} is {}\".format(i, Counter(train_OS[i])))","544f018c":"# visualising to know the features types and names\ntrain_OS.head(1)","88ba4c5e":"# creating another dummy dataframe to maintain the integrity of the main dataframe\ntrans_Train_OS = train_OS.copy()\ntrans_Train_OS.head(1)","fe7d7acc":"# Box Plot to find more information about it's Quartiles and also detect Outliers\n'''\nplt.figure(figsize = (15, 5))\nplt.title('Age Distribution Spread')\nsns.boxplot(x = trans_Train.age)\nprint(\"1st Quartile : {}\".format(trans_Train_OS.age.quantile(0.25)))\nprint(\"3rd Quartile : {}\".format(trans_Train.age.quantile(0.75)))\n'''\n# For Age\nfig = px.box(trans_Train_OS, x = \"is_promoted\", y = \"age\", points = \"outliers\")\nfig.show()\n\n# For Length of Service\nfig = px.box(trans_Train_OS, x = \"is_promoted\", y = \"length_of_service\", points = \"outliers\")\nfig.show()\n\n# Avg Training Score\nfig = px.box(trans_Train_OS, x = \"is_promoted\", y = \"avg_training_score\", points = \"outliers\")\nfig.show()","e3e0f3e6":"# Applying Boxcox Transformation and Checking for Result\nboxc_Age = stats.boxcox(trans_Train_OS['age'])[0]\nboxc_LOS = stats.boxcox(trans_Train_OS['length_of_service'])[0]\n\n# Creating a Copy for Testing\nxyz = trans_Train_OS.copy()\n\n# Loading the Results\nxyz['age'] = boxc_Age\nxyz['length_of_service'] = boxc_LOS\n\n# Printing out the Results\nprint(f\"After Transformation, \\nSkewness of Age : {xyz['age'].skew()} \\nSkewness of LOS : {xyz['length_of_service'].skew()}\")","bf161c8b":"# For Age\nfig = px.box(xyz, x = \"is_promoted\", y = \"age\", points = \"outliers\")\nfig.show()\n\n# For Length of Service\nfig = px.box(xyz, x = \"is_promoted\", y = \"length_of_service\", points = \"outliers\")\nfig.show()","a48a7638":"print(\"***** For LOS *****\")\n# For IQR Analysis, we need to sort our feature vector first\narr = np.array(sorted(xyz.length_of_service))\n\n# Defining the Quartile Ranges\nquantile_1, quantile_3 = np.percentile(arr, [25, 75])\nprint(f\"1st Quartile (25th Percentile) of Age Feature : {quantile_1} \\n3rd Quartile (75th Percentile) of Age Feature : {quantile_3}\")\n\n# Defining the IQR\niqr = quantile_3 - quantile_1\nprint(f\"IQR = {iqr}\")\n\n# Finding out the Lower Bound Value and the Higher Bound Value\nlower_Bound = quantile_1 - (1.5 * iqr) \nupper_Bound = quantile_3 + (1.5 * iqr)\nprint(f\"Lower Bound Value : {lower_Bound} \\nUpper Bound Value : {upper_Bound}\")\n\n# Any number below the Lower Bound or above the Upper Bound would be considered as an Outlier\noutliers = xyz.length_of_service[(xyz.length_of_service < lower_Bound) | (xyz.length_of_service > upper_Bound)]\nprint(f\"Number of Outliers : {len(outliers)}\")\n## print(f\"The Outliers are : {sorted(outliers.unique())}\")\n\n# % of entries which are outliers\nprint(f\"% of Records which are been considered Outliers : {round(len(outliers)\/xyz.shape[0] * 100, 2)} %\")\n\nprint(\"*\" * 10)\n\nprint(\"***** For Age *****\")\n# For IQR Analysis, we need to sort our feature vector first\narr = np.array(sorted(xyz.age))\n\n# Defining the Quartile Ranges\nquantile_1, quantile_3 = np.percentile(arr, [25, 75])\nprint(f\"1st Quartile (25th Percentile) of Age Feature : {quantile_1} \\n3rd Quartile (75th Percentile) of Age Feature : {quantile_3}\")\n\n# Defining the IQR\niqr = quantile_3 - quantile_1\nprint(f\"IQR = {iqr}\")\n\n# Finding out the Lower Bound Value and the Higher Bound Value\nlower_Bound = quantile_1 - (1.5 * iqr) \nupper_Bound = quantile_3 + (1.5 * iqr)\nprint(f\"Lower Bound Value : {lower_Bound} \\nUpper Bound Value : {upper_Bound}\")\n\n# Any number below the Lower Bound or above the Upper Bound would be considered as an Outlier\noutliers = xyz.age[(xyz.age < lower_Bound) | (xyz.age > upper_Bound)]\nprint(f\"Number of Outliers : {len(outliers)}\")\n## print(f\"The Outliers are : {sorted(outliers.unique())}\")\n\n# % of entries which are outliers\nprint(f\"% of Records which are been considered Outliers : {round(len(outliers)\/xyz.shape[0] * 100, 2)} %\")","1b679943":"# Adding these Transformed Features in the Dataset\ntrans_Train_OS['age_boxcox'] = boxc_Age\ntrans_Train_OS['length_of_service_boxcox'] = boxc_LOS\n\ntrans_Train_OS.head(2)","61095d0b":"# Equal Frequency Discretization\ndiscretizer = EqualFrequencyDiscretiser(q = 10, variables = ['age', 'length_of_service'], return_object = True, return_boundaries = True)\ndiscretizer","9070b933":"# fit the discretization transformer\ndiscretizer.fit(trans_Train_OS)\n\n# transform the data\ntrain_t = discretizer.transform(trans_Train_OS)\n\n# Visualising the Bins\nprint(f\"Age : {list(train_t.age.unique())} \\nLOS : {list(train_t.length_of_service.unique())}\")","c7b290b3":"# Adding Binned Categories to the Dataframe\ntrans_Train_OS['age_bins'] = train_t.age\ntrans_Train_OS['length_of_service_bins'] = train_t.length_of_service\n\n# Deleting Redundant Dataframe\ndel train_t\n\n# Visualising the 1st 5 records of transformed dataframe\ntrans_Train_OS.head(5)","372d6864":"# IGNORE -> Will do Data Leakage and not insightful.\n# Checked at Last\n\n##  Probability of Promotion Per Dept.\n'''\ntemp = pd.DataFrame()\n\n# trans_Train_OS['prob_per_dept'] = train.groupby(['department'])['is_promoted'].mean()\n# trans_Train_OS.prob_per_dept.unique()\n\ntemp['prob_per_dept'] = train.groupby(['department'])['is_promoted'].apply(lambda x : x.mean())\ntrans_Train_OS = pd.merge(trans_Train_OS, temp, on = ['department'], how = 'left')\ntrans_Train_OS['prob_per_dept'].fillna(np.median(temp['prob_per_dept']), inplace = True)\ntrans_Train_OS['prob_per_dept'].unique()\n'''","1bcedefd":"# IGNORE -> Will do Data Leakage and not insightful.\n# Checked at Last\n\n##  # Probability of Promotion Per Dept Per Region.\n'''\ntemp = pd.DataFrame()\n\n# temp['prob_per_dept_per_region'] = trans_Train_OS.groupby(['region', 'department'])['is_promoted'].mean()\n# temp.head(10)\n\ntemp['prob_per_dept_per_region'] = trans_Train_OS.groupby(['region', 'department'])['is_promoted'].mean()\ntrans_Train_OS = pd.merge(trans_Train_OS, temp, on = ['region', 'department'], how = 'left')\ntrans_Train_OS['prob_per_dept_per_region'].fillna(np.median(temp['prob_per_dept_per_region']), inplace = True)\n# trans_Train_OS['prob_per_dept_per_region'].unique()\ntemp.head(10)\n'''\n'''\n# K-Fold Cross Validified Promotion Per Dept Per Region.\ntemp = pd.DataFrame()\ntemp['prob_per_dept_per_region'] = np.nan\n\nkf = KFold(n_splits = 5, shuffle = False, random_state = 0)\nfor train_in, val_in in kf.split(trans_Train_OS):\n  x_tr, x_val = trans_Train_OS.iloc[train_in], trans_Train_OS.iloc[val_in]\n  trans_Train_OS.loc[trans_Train_OS.index[val_in], 'prob_per_dept_per_region'] = x_val['prob_per_dept_per_region'].map(x_tr.groupby(['region', 'department'])['is_promoted'].mean())\n\ntrans_Train_OS.prob_per_dept_per_region.fillna(0, inplace = True)\ntemp.head(10)\n'''","6c15e260":"# Cummulative Training Score\ntemp = pd.DataFrame()\n\n# temp = trans_Train_OS.groupby(['recruitment_channel'])['no_of_trainings'].mean()\ntrans_Train_OS[\"cummulative_train'_score\"] = trans_Train_OS.no_of_trainings * trans_Train_OS.avg_training_score\n\n# Visualising the 1st 5 records after transformation\ntrans_Train_OS.head(5)","8b555a8e":"# Checking out for the Region Counts\ntrans_Train_OS.region.value_counts()","fcd54825":"# Checking out where % of employees in a particular region to country is more than 1%\n643\/trans_Train_OS.shape[0] * 100   # Region_12 & Below -> regions = [10,1,8,9]","4e5a8852":"# Groupby Function to Club Regions and Promotions\ntemp = trans_Train_OS.groupby(['region', 'is_promoted'])['is_promoted'].count().sort_values(ascending = False).unstack()    #.apply(lambda r : r\/r.sum())\n# temp.tail(10)\npd.crosstab(trans_Train_OS['region'], trans_Train_OS.is_promoted).apply(lambda r: r\/r.sum(), axis = 1)","26bbb720":"# IGNORE -> Will do Data Leakage and not insightful.\n# Checked at Last\n\n## Promotion Ratio Per Region\n'''\n# K-Fold Cross Validified Ratio\nkf = KFold(n_splits = 5, shuffle = False, random_state = 0)\ntemp['prom_ratio_per_region_XXX'] = np.nan\n\nfor train_in, val_in in kf.split(temp):\n  x_tr, x_val = temp.iloc[train_in], temp.iloc[val_in]\n  temp.loc[temp.index[val_in], 'prom_ratio_per_region'] = temp.iloc[:,1] \/ ((temp.iloc[:,0] + temp.iloc[:,1]) * 100)\n\ntemp.fillna(0, axis = 1, inplace = True)\n#temp.sort_values(by=['prom_ratio_per_region'], inplace = True, ascending = True)\n'''\n'''\n# Assigning Values\ntemp['prom_ratio_per_region'] = temp.iloc[:,1] \/ ((temp.iloc[:,0] + temp.iloc[:,1]) * 100)\ntemp.fillna(0, axis = 1, inplace = True)\ntemp.sort_values(by=['prom_ratio_per_region'], inplace = True, ascending = True)\ntemp.tail(5)\n'''","604b1da8":"# KPI and Award Concatenation\ntrans_Train_OS[\"KPI_n_Award\"] = np.where(((trans_Train_OS[\"KPIs_met >80%\"] == 1) & (trans_Train_OS[\"awards_won?\"] == 1)), 1, 0)\ntrans_Train_OS.head(3)","0c594056":"# Gender - No of Training - Promotion Relation\ntrans_Train_OS.groupby(['gender', 'no_of_trainings'])['is_promoted'].sum()","0ab640a8":"# Seggregating no_of_trainings > 4\ntrans_Train_OS['trainings>4?'] = np.where(trans_Train_OS.no_of_trainings > 4, 1, 0)  # 1 if True else 0","6c32e7c4":"# IGNORE -> Got Reversed after SMOTE\n'''\n# Reject Region_18 - as noone get's promoted from there\ntrans_Train_OS['is_Region_18?'] = np.where(trans_Train_OS.region == 'region_18', 1, 0)     # 1 if True else 0 \n'''","ff6f9baf":"# IGNORE - Not a insightful feature\n'''\n# KPI Per Dept.\n\ntemp = pd.DataFrame()\ntemp['KPI_per_dept'] = trans_Train_OS.groupby(['department', 'KPIs_met >80%'])['KPIs_met >80%'].count()\n#trans_Train_OS = pd.merge(trans_Train_OS, temp, on = ['department'], how = 'left')\n#trans_Train_OS['prob_per_dept'].fillna(np.median(temp['prob_per_dept']), inplace = True)\n#trans_Train_OS['prob_per_dept'].unique()\n#temp\npd.crosstab([trans_Train_OS['KPIs_met >80%'], trans_Train_OS.is_promoted], trans_Train_OS.department, margins = True).style.background_gradient(cmap = 'summer_r')\n'''","7b8db41c":"# Categorise Employees having good overall performance\ntrans_Train_OS['good_overall_performance?'] = np.where((trans_Train_OS.previous_year_rating >= 3) & (trans_Train_OS['awards_won?'] == 1) & \n                                                   (trans_Train_OS.avg_training_score >= trans_Train_OS.avg_training_score.quantile(0.25)), 1, 0)","ba0c986e":"# IGNORE - Not a insightful feature\n'''\ntemp['KPI_per_dept'] = train.groupby(['department', 'KPIs_met >80%'])['KPIs_met >80%'].agg(['count', 'mean'])\ntemp\n'''\n'''\ntrain.groupby(['department', 'KPIs_met >80%']).count().unstack()\n# train.groupby(['department', 'KPIs_met >80%'])['KPIs_met >80%'].agg(['mean', 'count'])\ntrain.groupby(['department', 'KPIs_met >80%']).size().reset_index(name='counts')\n'''","38d93ee1":"# Mean KPI by Department\ntrans_Train_OS['mean_kpi_by_dept'] = trans_Train_OS['department'].map(trans_Train_OS.groupby('department')['KPIs_met >80%'].mean())","d8978837":"# Mean Training by Department\ntrans_Train_OS['mean_training_by_dept'] = trans_Train_OS['department'].map(trans_Train_OS.groupby('department')['avg_training_score'].mean())","c4a90ac2":"# Mean Rating by Department\ntrans_Train_OS['mean_rating_by_dept'] = trans_Train_OS['department'].map(trans_Train_OS.groupby('department')['previous_year_rating'].mean())","b2641a2d":"# Prev Years' Rating by Department\ntrans_Train_OS['dept_rating_mean_ratio'] = trans_Train_OS['previous_year_rating'] \/ trans_Train_OS['mean_rating_by_dept']","788e8cca":"# Visualizing the First Record of Transformed Train\ntrans_Train_OS.head(1)","0e80209e":"# K-Fold Target Encoding 'recruitment_channel', 'region' and 'department'\n\n# 'recruitment_channel'\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ntrans_Train_OS = targetc.fit_transform(trans_Train_OS)\n## trans_Train_OS.drop(['recruitment_channel'], axis = 1, inplace = True)\n\n# 'region'\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ntrans_Train_OS = targetc.fit_transform(trans_Train_OS)\n## trans_Train_OS.drop(['region'], axis = 1, inplace = True)\n\n# 'department'\ntargetc = KFoldTargetEncoderTrain('department', 'is_promoted', n_fold = 5)\ntrans_Train_OS = targetc.fit_transform(trans_Train_OS)\n## trans_Train_OS.drop(['department'], axis = 1, inplace = True)\n\ntrans_Train_OS.head(1)","78261f36":"# IGNORE -> Already Done at First\n'''\n# Label Encoding 'Gender'\nl = LabelEncoder()\ntrans_Train_OS.loc[:, 'gender'] = l.fit_transform(trans_Train_OS.loc[:, 'gender'])\n'''","c5a9e8cc":"# Encoding 'Education'\n\n# Mannualy Assigning Weight and Encoding 'ordinal' Feature 'Education'\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\ntrans_Train_OS['education'] = trans_Train_OS['education'].map(edu_enc)\ntrans_Train_OS.education.unique()","0cba012f":"trans_Train_OS.head(1)","fa832342":"# K-Fold Target Encoding 'age_bin', 'length_of_service_bins'\n\n# 'age_bin'\ntargetc = KFoldTargetEncoderTrain('age_bins', 'is_promoted', n_fold = 5)\ntrans_Train_OS = targetc.fit_transform(trans_Train_OS)\n## trans_Train_OS.drop(['age_bins'], axis = 1, inplace = True)\n\n# 'length_of_service_bins'\ntargetc = KFoldTargetEncoderTrain('length_of_service_bins', 'is_promoted', n_fold = 5)\ntrans_Train_OS = targetc.fit_transform(trans_Train_OS)\n## trans_Train_OS.drop(['length_of_service_bins'], axis = 1, inplace = True)\n\ntrans_Train_OS.head(1)","93a5a3ce":"# Before and After Feature Engineering Comparison\nprint(\"Shape of Train Before FE : {}\".format(train_OS.shape))\nprint(f\"Shape of Train Before FE : {trans_Train_OS.shape}\")\nprint(\"No of Features added during FE : \", (trans_Train_OS.shape[1] - train_OS.shape[1]))","3df38707":"# Removing Redundant Feature 'Employee ID' and other un-encoded categorical features from Trans-Train-OS\ntrans_Train_OS.drop(['employee_id',                                  # Unique Identifier for Every Feature\n                     'department', 'region', 'recruitment_channel',  # Un-Encoded Categorical Feature\n                     'age_bins', 'length_of_service_bins'], \n                    axis = 1, inplace = True)","88bf1c93":"# Visualising the Columns of Trans-Train-OS [Feature Engineered DataFrame]\ntrans_Train_OS.columns","fa1fe597":"# Spliting into Dependent and Independent Features Vector\nx = trans_Train_OS.drop(['is_promoted'], axis = 1)\ny = trans_Train_OS['is_promoted'].values.reshape(-1, 1)","2264e70c":"# Visualising Independent Vector Information\nx.info()","7384fea2":"# Segregating into Categorical and Continuous Features ->\ncat = [\"education\", \"gender\", \"no_of_trainings\", \"previous_year_rating\", \"KPIs_met >80%\", \"awards_won?\", \n       \"is_promoted\", \"KPI_n_Award\", \"trainings>4?\", \"good_overall_performance?\",\n       \"tgt_recruitment_channel\", \"tgt_region\", \"tgt_department\", \"tgt_age_bins\", \"tgt_length_of_service_bins\",\n       \"mean_kpi_by_dept\", \"mean_training_by_dept\", \"mean_rating_by_dept\"]\n\ncont = [\"age\", \"length_of_service\", \"avg_training_score\", \"age_boxcox\", \"length_of_service_boxcox\", \"cummulative_train'_score\", \n        \"dept_rating_mean_ratio\"]\n\nprint(f\"{len(cat) + len(cont)} & {trans_Train_OS.shape[1]}\")","a54359b7":"# Fitting Variance Threshold to our Dataframe\nconst_thres = VarianceThreshold(threshold = 0).fit(trans_Train_OS)\nlen(trans_Train_OS[trans_Train_OS.select_dtypes([np.number]).columns].columns[const_thres.get_support()])","baf90a31":"# Printing out Constant Columns if any\nconstant_columns = [col for col in trans_Train_OS.columns if col not in trans_Train_OS.columns[const_thres.get_support()]]\nconstant_columns","c4798a9e":"# Finding out Quasi Constant Features\nquasiDetect = VarianceThreshold(threshold = 0.01)  \nquasiDetect.fit(trans_Train_OS)\nprint(f\"No. of Non Quasi Constant Features : {len(trans_Train_OS.columns[quasiDetect.get_support()])}\")\nprint(f\"Quasi-Constant Features : {[col for col in trans_Train_OS.columns if col not in trans_Train_OS.columns[quasiDetect.get_support()]]}\")","d01b038b":"# The features we would be dropping are 'trainings>4?' and 'mean_kpi_by_dept' as 'age_boxcox' is a multilabel target encoded class. \ntrans_Train_OS.drop(['trainings>4?', 'mean_kpi_by_dept'], axis = 1, inplace = True)","2d3b16c3":"# Updating Categorical Columns List\ncat = [e for e in cat if e not in ['trainings>4?', 'mean_kpi_by_dept']]\ncat","b1b6f256":"# To Find out Repeatative Features (Duplicate)\ntrain_enc =  pd.DataFrame(index = trans_Train_OS.index)\n\ndup_cols = {}\n\nfor i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n    for c2 in train_enc.columns[i + 1:]:\n        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n            dup_cols[c2] = c1\ndup_cols","b3e1e0d3":"# Correlation using heatmap - Continuous Variable & Target Variable - Pearson Correlation\nhm = trans_Train_OS[cont + [\"is_promoted\"]].corr().where(np.tril(np.ones(trans_Train_OS[cont + [\"is_promoted\"]].corr().shape)).astype(np.bool)) # to delete the upper triangle\nplot = sns.heatmap(hm, annot = True, cmap = \"YlGnBu\")\nplt.setp(plot.get_xticklabels(), rotation = 90)\nplt.gcf().set_size_inches(15, 8)","9da7cf75":"# Covariance using Heatmap - Continuous Variable & Target Variable\ntrans_Train_x = trans_Train_OS[cont + [\"is_promoted\"]].round(decimals = 2)\ncovMatrix = pd.DataFrame.cov(trans_Train_x[cont + [\"is_promoted\"]])\nsns.heatmap(covMatrix, annot = True, fmt = 'g')\nplt.gcf().set_size_inches(15, 8)\ndel trans_Train_x  # Deleting Redundant Object","83963f6f":"# Correlation using heatmap - Categorical Variable & Target Variable - Cramer's V Correlation\n\ndef cramers_V(var1, var2):\n    crosstab = np.array(pd.crosstab(var1, var2, rownames = None, colnames = None)) \n    stat = chi2_contingency(crosstab)[0]\n    obs = np.sum(crosstab)\n    mini = min(crosstab.shape) - 1 \n    return (stat \/ (obs * mini))\n\nrows = []\ndata_encoded = trans_Train_OS.copy()\ndata_encoded = data_encoded[cat]\nfor var1 in data_encoded:\n  col = []\n  for var2 in data_encoded :\n    cramers = cramers_V(data_encoded[var1], data_encoded[var2]) # Cramer's V test\n    col.append(round(cramers, 2))  \n  rows.append(col)\n  \ncramers_results = np.array(rows)      # Results of Cramer's V Test\n\n# Cramer's V Test Transformed Dataframe\ndf = pd.DataFrame(cramers_results, columns = data_encoded.columns, index = data_encoded.columns)\n\n# HeatMap  Visualisation\nmask = np.zeros_like(df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplot = sns.heatmap(df, mask = mask, vmin = 0., vmax = 1, \n                   annot = True, cmap = \"YlGnBu\", square = True)\nplt.setp(plot.get_xticklabels(), rotation = 90)\nplt.gcf().set_size_inches(15, 8)","55496940":"# Correlation using heatmap - Continuous Variable & Target Variable - Overall Correlation.\nhm = trans_Train_OS.corr().where(np.tril(np.ones(trans_Train_OS.corr().shape)).astype(np.bool)) # to delete the upper triangle\nplot = sns.heatmap(hm, annot = True, cmap = \"YlGnBu\")\nplt.setp(plot.get_xticklabels(), rotation = 90)\nplt.gcf().set_size_inches(15, 8)","0a7d8be0":"# Removing Highly Correlated Independent Features [corr > 0.6]\n# Also, Correlation has been checked between target variable in case they're similar\ntrans_Train_OS.drop([\"KPI_n_Award\", \"tgt_length_of_service_bins\", \"mean_training_by_dept\",                  # Categorical Features\n                     \"mean_rating_by_dept\", \"tgt_department\", \"good_overall_performance?\",\n                     \"age\", \"length_of_service\", \"length_of_service_boxcox\", \"cummulative_train'_score\",    # Continuous Features\n                     \"dept_rating_mean_ratio\"       \n                     ], axis = 1, inplace = True)","6dd4c8f1":"trans_Train_OS.head(1)","00884a67":"# Dropping Features that Mean [Target Encoded and Bin, both present] the Same -> 'tgt_age_bins' as it is having the lowest correlation wrt target variable\ntrans_Train_OS.drop(['tgt_age_bins'], axis = 1, inplace = True)","7e11018f":"# Updating Features List\ncat = [e for e in cat if e not in [\"KPI_n_Award\", \"tgt_length_of_service_bins\", \"mean_training_by_dept\",\n                                   \"mean_rating_by_dept\", \"tgt_department\", \"good_overall_performance?\", \n                                   \"tgt_age_bins\"]]\ncont = [e for e in cont if e not in [\"age\", \"length_of_service\", \"length_of_service_boxcox\", \"cummulative_train'_score\",\n                                     \"dept_rating_mean_ratio\"]]","618a112b":"# Printing the Remaining Column Names\nprint(f\"Categorical : {cat} \\nContinuous : {cont}\")\nprint(f\"No. of Categorical Features Left : {len(cat)} \\nNo. of Continuous Features Left : {len(cont)}\")\nprint(\"No. of Features in Total Now : {}\".format(trans_Train_OS.shape[1]))","0c86cd95":"# Not Done as Drop Done Manually\n# Automated Drop - Correlated Features\n'''\ncol_corr = set() # Set of all the names of deleted columns\ncorr_matrix = trans_Train[cont + [\"is_promoted\"]].corr()\nfor i in range(len(corr_matrix.columns)):\n  for j in range(i):\n    if corr_matrix.iloc[i, j] >= threshold and (corr_matrix.columns[j] not in col_corr):\n      colname = corr_matrix.columns[i] # getting the name of column\n      col_corr.add(colname)\n      #if colname in dataset.columns:\n      #del dataset[colname] # deleting the column from the dataset\n'''","d3ee1c34":"# Visualising Head of the Remaining DataFrame\ntrans_Train_OS.head(1)","5d6a1e03":"# Not Done\n## Anova\n'''\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nanova = ols('SepalLengthCm ~ C(Species) + SepalWidthCm + PetalLengthCm + PetalWidthCm', data = trans_Train).fit()\nsm.stats.anova_lm(anova, typ = 2)\nanova.summary()\n'''","21f45244":"# Dividing Trans-Train-OS into Independent and Dependent Features\nx = trans_Train_OS.drop(['is_promoted'], axis = 1)\ny = trans_Train_OS['is_promoted'].values.reshape(-1, 1)\nx.shape, y.shape","27a0055a":"# IGNORE -> Not Done as it takes a huge amount of time.\n'''\nefs = ExhaustiveFeatureSelector(rfc(), \n           min_features = 4,\n           max_features = 10, \n           scoring = 'f1',\n           cv = 5)\n\n# fit the object to the training data.\nefs = efs.fit(x, y)\n\n# print the selected features.\nselected_features = x.columns[list(efs.k_feature_idx_)]\nprint(selected_features)\n'''","6e60e03b":"# Sequential Feature Selector Object and Configuring the Parameters -> Forward Elimination\nsfs_fw = SFS(rfc(random_state = 0),\n          k_features = 10,\n          forward = True, \n          floating = False,\n          verbose = 2,\n          scoring = 'f1',\n          cv = 5,\n          n_jobs = -1)\n\n# Fit the object to the Training Data.\nsfs_fw.fit(x, y)","10bc3593":"# Print the Selected Features.\nselected_features = x.columns[list(sfs_fw.k_feature_idx_)]\nprint(selected_features)\n\n# Print the Final Prediction Score.\nprint(sfs_fw.k_score_)","8a008c70":"# IGNORE -> Not now, after all the methods are been evaluated\n'''\n# Transform to the newly Selected Features.\nx_sfs = sfs.transform(x)\n'''","20847398":"# Sequential Feature Selector Object and Configuring the Parameters -> Backward Elimination\nsfs_bw = SFS(rfc(random_state = 0, n_jobs = -1),\n          k_features = 10,\n          forward = False, \n          floating = False,\n          verbose = 2,\n          scoring = 'f1',\n          cv = 5,\n          n_jobs = -1)\n\n# Fit the object to the Training Data.\nsfs_bw.fit(x.values, y)","32bbc8df":"# Print the Selected Features.\nselected_features_BW = x.columns[list(sfs_bw.k_feature_idx_)]\nprint(selected_features_BW)\n\n# Print the Final Prediction Score.\nprint(sfs_bw.k_score_)","36feb013":"# IGNORE -> Not now, after all the methods are been evaluated\n'''\n# Transform to the newly Selected Features.\nx_sfs = sfs.transform(x)\n'''","008b1ef0":"# Fitting Random Forest Algorithm into our dataset\nrfc_wr = rfc(random_state = 0, n_jobs = -1)\nrfc_wr.fit(x, y)\n\n# Visualising the Importance of Features by Plot Graph - for Dummy Train\nfeatures_Imp_wr = pd.Series(rfc_wr.feature_importances_, index = x.columns)\nfeatures_Imp_wr.nlargest(10).plot(kind = 'barh')\nplt.show()","43406f97":"# Show the Whole List\nfeatures_Imp_wr * 100","f973b9e8":"# The \"f1\" scoring is proportional to the number of correct classifications per class\nrf_r = rfc(random_state = 0, n_jobs = -1) \nrfecv = RFECV(estimator = rf_r, step = 1, cv = 5, scoring = 'f1')   # 5-fold cross-validation\nrfecv = rfecv.fit(x, y)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x.columns[rfecv.support_])","0520aa91":"# Using SelectkBest Method\n  \n# Applying SelectKBest to extract top 10 best features\nbest_Features = SelectKBest(score_func = chi2, k = 10)    # using mectric of chi-square\nfit = best_Features.fit(x, y)\ndf_Scores = pd.DataFrame(fit.scores_)\ndf_Columns = pd.DataFrame(x.columns)\n\n# Concat two dataframes for better visualization \nfeature_Scores_skb = pd.concat([df_Columns, df_Scores], axis = 1)\nfeature_Scores_skb.columns = ['Columns', 'Score']\nprint(feature_Scores_skb)","6fdaa7d0":"# Displaying the Score of Best 15 Features\nprint(feature_Scores_skb.nlargest(15, 'Score'))","e9207537":"# Fiting Dummy set into Extra Trees\nmodel = ExtraTreesClassifier()\nmodel.fit(x, y)","e3d39cb9":"# Visualising the Importance of Features by Plot Graph\nfeat_importances = pd.Series(model.feature_importances_, index = x.columns)\nfeat_importances.nlargest(10).plot(kind ='barh')\nplt.show()","cc7944c2":"feat_importances = pd.Series(model.feature_importances_, index = x.columns) * 100\nfeat_importances","57c5f767":"# Important Features after Feature Selection according to the Algorithms\nprint(f\"*** Results from Filter Methods *** \\n{x.columns}\\n\")                         # Filter Method\nprint(f\"*** Results from Wrapper Methods *** \\n{selected_features_BW}\\n\")             # Backward Selection Method\nprint(f\"*** Results from Embedded Methods*** \\n{x.columns[rfecv.support_]}\\n\")        # Random Forest CV Method","d9ee3e03":"# Common Features in both Wrapper and Embedded Methods\nset_A = set(selected_features_BW)        # Wrapper \nset_B = set(x.columns[rfecv.support_])   # Embedded\nset_A & set_B                            # Common Entries","8eb51c21":"# Creating a list for the Final List of Features\nf_features_OS = ['KPIs_met >80%', 'age_boxcox', 'avg_training_score', 'awards_won?', 'education',\n                 'gender', 'no_of_trainings', 'previous_year_rating', 'tgt_recruitment_channel', 'tgt_region']\n\n# Printing the Final List of Features\nprint(f\"Printing the Final List of Features : {f_features_OS}\")","f7afbde9":"# Taking in Only the Important Independent Features\nx_dim = x.copy()         # Saving the Dataframe for Dimensionality Reducing Techniques.\nx_OS = x[f_features_OS]\nx_OS.head(1)","557c49b9":"# Shape of the Independent Features Set\nx_OS.shape","456f3bc5":"# Shape of the Dependent Feature Set\ny_OS = y\ny_OS.shape","bfc78cc7":"# Pickling Over-Sampled Plain DataFrame\nx_OS['is_promoted'] = y_OS\nx_OS.to_pickle(\"OS_Plain.pkl\")","83ba9b86":"# creating a copy\nx_scale = x_dim.copy()","1ffc4fb6":"# Standard Scaling all the Features for Dimensionality Reduction\nsc_x = StandardScaler()\nx_scale = sc_x.fit_transform(x_scale)\nx_scale","590c3eb9":"# Normal - PCA\npca = PCA(n_components = 5, random_state = 0)\npca_OS = pca.fit_transform(x_scale)\nexplained_variance = pca.explained_variance_ratio_\nprint(f\"Variance Explained per Principal Component : {explained_variance}\")","e18457cd":"# Visualising PCA Segregation\nplt.figure(figsize = (10, 5))\nplt.scatter(pca_OS[:,0], pca_OS[:,1], c = y, s = 0.5)","64ed74b7":"# IGNORE -> Takes up More than 10GB+ of RAM and crashes the kernel\n'''\n# Kernel - PCA\nkpca = kp(n_components = 5, kernel = 'rbf')\nx_kpca = kpca.fit_transform(x_scale)\nexplained_variance = kpca.explained_variance_ratio_\n'''","4b55f4b5":"# t-SNE\ntsne_OS = TSNE(random_state = 0).fit_transform(pca_OS)\n# tsne = TSNE(n_components = 5, init = 'pca', random_state = 0, learning_rate = 150, perplexity)","2f044015":"# Visualising t-SNE Segregation\nplt.figure(figsize = (10, 5))\nplt.scatter(tsne_OS[:,0], tsne_OS[:,1], c = y, s = 0.5)","c204de18":"# Pickling Over-Sampled Plain DataFrame\ntsne_OS = pd.DataFrame(tsne_OS)\ntsne_OS['target'] = y\ntsne_OS.to_pickle(\"OS_tsne.pkl\")","912b6663":"# Deleting off redundant objects\/variables - Memory Efficient Techniques\ndel objectColumns, colNames, sm, x_os_train, y_os_train, train_OS, trans_Train_OS, fig, boxc_Age, boxc_LOS, xyz, arr, quantile_1, quantile_3, iqr, lower_Bound, upper_Bound, outliers, discretizer, temp, targetc, cat, cont, dup_cols, train_enc, hm, plot, covMatrix, mask, sfs_fw, selected_features, sfs_bw, selected_features_BW, rfc_wr, rf_r, rfecv, best_Features, fit, df_Scores, df_Columns, feature_Scores_skb, model, feat_importances, set_A, set_B, x_scale, sc_x, pca, explained_variance, x_dim","84190896":"# visualising to know the features types and names\ntrain.head(1)","1f7c787d":"# visualising to know the features types and names\ndummy_Train.head(1)","6470c122":"# creating another dummy dataframe to maintain the integrity of the main dataframe\ntrans_Train = train.copy()\ntrans_Train.head(1)","006ed3fe":"# Box Plot to find more information about it's Quartiles and also detect Outliers\n'''\nplt.figure(figsize = (15, 5))\nplt.title('Age Distribution Spread')\nsns.boxplot(x = trans_Train.age)\nprint(\"1st Quartile : {}\".format(trans_Train.age.quantile(0.25)))\nprint(\"3rd Quartile : {}\".format(trans_Train.age.quantile(0.75)))\n'''\n# For Age\nfig = px.box(train, x = \"is_promoted\", y = \"age\", points = \"outliers\")\nfig.show()\n\n# For Length of Service\nfig = px.box(train, x = \"is_promoted\", y = \"length_of_service\", points = \"outliers\")\nfig.show()","8750753d":"# Applying Boxcox Transformation and Checking for Result\nboxc_Age = stats.boxcox(trans_Train['age'])[0]\nboxc_LOS = stats.boxcox(trans_Train['length_of_service'])[0]\n\n# Creating a Copy for Testing\nxyz = trans_Train.copy()\n\n# Loading the Results\nxyz['age'] = boxc_Age\nxyz['length_of_service'] = boxc_LOS","6bb03d3b":"# For Age\nfig = px.box(xyz, x = \"is_promoted\", y = \"age\", points = \"outliers\")\nfig.show()\n\n# For Length of Service\nfig = px.box(xyz, x = \"is_promoted\", y = \"length_of_service\", points = \"outliers\")\nfig.show()","1db25aa5":"print(\"***** For LOS *****\")\n# For IQR Analysis, we need to sort our feature vector first\narr = np.array(sorted(xyz.length_of_service))\n\n# Defining the Quartile Ranges\nquantile_1, quantile_3 = np.percentile(arr, [25, 75])\nprint(f\"1st Quartile (25th Percentile) of Age Feature : {quantile_1} \\n3rd Quartile (75th Percentile) of Age Feature : {quantile_3}\")\n\n# Defining the IQR\niqr = quantile_3 - quantile_1\nprint(f\"IQR = {iqr}\")\n\n# Finding out the Lower Bound Value and the Higher Bound Value\nlower_Bound = quantile_1 - (1.5 * iqr) \nupper_Bound = quantile_3 + (1.5 * iqr)\nprint(f\"Lower Bound Value : {lower_Bound} \\nUpper Bound Value : {upper_Bound}\")\n\n# Any number below the Lower Bound or above the Upper Bound would be considered as an Outlier\noutliers = xyz.length_of_service[(xyz.length_of_service < lower_Bound) | (xyz.length_of_service > upper_Bound)]\nprint(f\"Number of Outliers : {len(outliers)}\")\n## print(f\"The Outliers are : {sorted(outliers.unique())}\")\n\n# % of entries which are outliers\nprint(f\"% of Records which are been considered Outliers : {round(len(outliers)\/xyz.shape[0] * 100, 2)} %\")\n\nprint(\"*\" * 10)\n\nprint(\"***** For Age *****\")\n# For IQR Analysis, we need to sort our feature vector first\narr = np.array(sorted(xyz.age))\n\n# Defining the Quartile Ranges\nquantile_1, quantile_3 = np.percentile(arr, [25, 75])\nprint(f\"1st Quartile (25th Percentile) of Age Feature : {quantile_1} \\n3rd Quartile (75th Percentile) of Age Feature : {quantile_3}\")\n\n# Defining the IQR\niqr = quantile_3 - quantile_1\nprint(f\"IQR = {iqr}\")\n\n# Finding out the Lower Bound Value and the Higher Bound Value\nlower_Bound = quantile_1 - (1.5 * iqr) \nupper_Bound = quantile_3 + (1.5 * iqr)\nprint(f\"Lower Bound Value : {lower_Bound} \\nUpper Bound Value : {upper_Bound}\")\n\n# Any number below the Lower Bound or above the Upper Bound would be considered as an Outlier\noutliers = xyz.age[(xyz.age < lower_Bound) | (xyz.age > upper_Bound)]\nprint(f\"Number of Outliers : {len(outliers)}\")\n## print(f\"The Outliers are : {sorted(outliers.unique())}\")\n\n# % of entries which are outliers\nprint(f\"% of Records which are been considered Outliers : {round(len(outliers)\/xyz.shape[0] * 100, 2)} %\")","ecf2c4ad":"# Adding these Transformed Features in the Dataset\ntrans_Train['age_boxcox'] = boxc_Age\ntrans_Train['length_of_service_boxcox'] = boxc_LOS\n\ntrans_Train.head(2)","1c45672f":"# Equal Frequency Discretization\ndiscretizer = EqualFrequencyDiscretiser(q = 10, variables = ['age', 'length_of_service'], return_object = True, return_boundaries = True)\ndiscretizer","faf21b0e":"# fit the discretization transformer\ndiscretizer.fit(trans_Train)\n\n# transform the data\ntrain_t = discretizer.transform(trans_Train)\n\n# Visualising the Bins\nprint(f\"Age : {list(train_t.age.unique())} \\nLOS : {list(train_t.length_of_service.unique())}\")","c948021d":"# Adding Binned Categories to the Dataframe\ntrans_Train['age_bins'] = train_t.age\ntrans_Train['length_of_service_bins'] = train_t.length_of_service\n\n# Deleting Redundant Dataframe\ndel train_t\n\n# Visualising the 1st 5 records of transformed dataframe\ntrans_Train.head(5)","1cf13780":"# IGNORE -> Will do Data Leakage and not insightful.\n# Checked at Last\n\n##  Probability of Promotion Per Dept.\n'''\ntemp = pd.DataFrame()\n\n# trans_Train['prob_per_dept'] = train.groupby(['department'])['is_promoted'].mean()\n# trans_Train.prob_per_dept.unique()\n\ntemp['prob_per_dept'] = train.groupby(['department'])['is_promoted'].apply(lambda x : x.mean())\ntrans_Train = pd.merge(trans_Train, temp, on = ['department'], how = 'left')\ntrans_Train['prob_per_dept'].fillna(np.median(temp['prob_per_dept']), inplace = True)\ntrans_Train['prob_per_dept'].unique()\n'''","e0dea6ed":"# IGNORE -> Will do Data Leakage and not insightful.\n# Checked at Last\n\n##  # Probability of Promotion Per Dept Per Region.\n'''\ntemp = pd.DataFrame()\n\n# temp['prob_per_dept_per_region'] = train.groupby(['region', 'department'])['is_promoted'].mean()\n# temp.head(10)\n\ntemp['prob_per_dept_per_region'] = train.groupby(['region', 'department'])['is_promoted'].mean()\ntrans_Train = pd.merge(trans_Train, temp, on = ['region', 'department'], how = 'left')\ntrans_Train['prob_per_dept_per_region'].fillna(np.median(temp['prob_per_dept_per_region']), inplace = True)\n# trans_Train['prob_per_dept_per_region'].unique()\ntemp.head(10)\n'''\n'''\n# K-Fold Cross Validified Promotion Per Dept Per Region.\ntemp = pd.DataFrame()\ntemp['prob_per_dept_per_region'] = np.nan\n\nkf = KFold(n_splits = 5, shuffle = False, random_state = 0)\nfor train_in, val_in in kf.split(trans_Train):\n  x_tr, x_val = trans_Train.iloc[train_in], trans_Train.iloc[val_in]\n  trans_Train.loc[trans_Train.index[val_in], 'prob_per_dept_per_region'] = x_val['prob_per_dept_per_region'].map(x_tr.groupby(['region', 'department'])['is_promoted'].mean())\n\ntrans_Train.prob_per_dept_per_region.fillna(0, inplace = True)\ntemp.head(10)\n'''","1bf46b30":"# Cummulative Training Score\ntemp = pd.DataFrame()\n\n# temp = train.groupby(['recruitment_channel'])['no_of_trainings'].mean()\ntrans_Train[\"cummulative_train'_score\"] = trans_Train.no_of_trainings * trans_Train.avg_training_score\n\n# Visualising the 1st 5 records after transformation\ntrans_Train.head(5)","04f18a54":"# Checking out for the Region Counts\ntrans_Train.region.value_counts()","0cb144aa":"# Checking out where % of employees in a particular region to country is more than 1%\n372\/train.shape[0] * 100        # Region_24 & Below -> regions = [24,12,9,21,3,34,33]","19ff6c85":"# Groupby Function to Club Regions and Promotions\ntemp = trans_Train.groupby(['region', 'is_promoted'])['is_promoted'].count().sort_values(ascending = False).unstack()    #.apply(lambda r : r\/r.sum())\n# temp.tail(10)\npd.crosstab(trans_Train['region'], trans_Train.is_promoted).apply(lambda r: r\/r.sum(), axis=1)","bd04f84c":"# IGNORE -> Will do Data Leakage and not insightful.\n# Checked at Last\n\n## Promotion Ratio Per Region\n'''\n# K-Fold Cross Validified Ratio\nkf = KFold(n_splits = 5, shuffle = False, random_state = 0)\ntemp['prom_ratio_per_region_XXX'] = np.nan\n\nfor train_in, val_in in kf.split(temp):\n  x_tr, x_val = temp.iloc[train_in], temp.iloc[val_in]\n  temp.loc[temp.index[val_in], 'prom_ratio_per_region'] = temp.iloc[:,1] \/ ((temp.iloc[:,0] + temp.iloc[:,1]) * 100)\n\ntemp.fillna(0, axis = 1, inplace = True)\n#temp.sort_values(by=['prom_ratio_per_region'], inplace = True, ascending = True)\n'''\n'''\n# Assigning Values\ntemp['prom_ratio_per_region'] = temp.iloc[:,1] \/ ((temp.iloc[:,0] + temp.iloc[:,1]) * 100)\ntemp.fillna(0, axis = 1, inplace = True)\ntemp.sort_values(by=['prom_ratio_per_region'], inplace = True, ascending = True)\ntemp.tail(5)\n'''","5d27258f":"# KPI and Award Concatenation\ntrans_Train[\"KPI_n_Award\"] = np.where(((trans_Train[\"KPIs_met >80%\"] == 1) & (trans_Train[\"awards_won?\"] == 1)), 1, 0)\ntrans_Train.head(3)","155c83cc":"# Gender - No of Training - Promotion Relation\ntrans_Train.groupby(['gender', 'no_of_trainings'])['is_promoted'].sum()","4d99736d":"# Seggregating no_of_trainings > 4\ntrans_Train['trainings>4?'] = np.where(trans_Train.no_of_trainings > 4, 1, 0)","5dfd65f7":"# IGNORE - Got changed when reshuffled the dataset\n'''\n# Reject Region_18 - as noone get's promoted from there\ntrans_Train['is_Region_18?'] = np.where(trans_Train.region == 'region_18', 1, 0)\n'''","57500eaa":"# IGNORE - Not a insightful feature\n'''\n# KPI Per Dept.\n\ntemp = pd.DataFrame()\ntemp['KPI_per_dept'] = train.groupby(['department', 'KPIs_met >80%'])['KPIs_met >80%'].count()\n#trans_Train = pd.merge(trans_Train, temp, on = ['department'], how = 'left')\n#trans_Train['prob_per_dept'].fillna(np.median(temp['prob_per_dept']), inplace = True)\n#trans_Train['prob_per_dept'].unique()\n#temp\npd.crosstab([train['KPIs_met >80%'], train.is_promoted], train.department, margins = True).style.background_gradient(cmap = 'summer_r')\n'''","6c27ce2e":"# Categorise Employees having good overall performance\ntrans_Train['good_overall_performance?'] = np.where((trans_Train.previous_year_rating >= 3) & (trans_Train['awards_won?'] == 1) & \n                                                   (trans_Train.avg_training_score >= trans_Train.avg_training_score.quantile(0.25)), 1, 0)","5ed48994":"# IGNORE - Not a insightful feature\n'''\ntemp['KPI_per_dept'] = train.groupby(['department', 'KPIs_met >80%'])['KPIs_met >80%'].agg(['count', 'mean'])\ntemp\n'''\n'''\ntrain.groupby(['department', 'KPIs_met >80%']).count().unstack()\n# train.groupby(['department', 'KPIs_met >80%'])['KPIs_met >80%'].agg(['mean', 'count'])\ntrain.groupby(['department', 'KPIs_met >80%']).size().reset_index(name='counts')\n'''","eb3b281b":"# Mean KPI by Department\ntrans_Train['mean_kpi_by_dept'] = trans_Train['department'].map(trans_Train.groupby('department')['KPIs_met >80%'].mean())","4dc720a2":"# Mean Training Score by Department\ntrans_Train['mean_training_by_dept'] = trans_Train['department'].map(trans_Train.groupby('department')['avg_training_score'].mean())","adda25aa":"# Mean Rating by Department\ntrans_Train['mean_rating_by_dept'] = trans_Train['department'].map(trans_Train.groupby('department')['previous_year_rating'].mean())","28fb3e8f":"# Prev Years' Rating by Department\ntrans_Train['dept_rating_mean_ratio'] = trans_Train['previous_year_rating'] \/ trans_Train['mean_rating_by_dept']","272e0746":"# Visualizing the First Record of Transformed Train\ntrans_Train.head(1)","11688267":"# K-Fold Target Encoding 'recruitment_channel', 'region' and 'department'\n\n# 'recruitment_channel'\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ntrans_Train = targetc.fit_transform(trans_Train)\n# trans_Train.drop(['recruitment_channel'], axis = 1, inplace = True)\n\n# 'region'\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ntrans_Train = targetc.fit_transform(trans_Train)\n# trans_Train.drop(['region'], axis = 1, inplace = True)\n\n# 'department'\ntargetc = KFoldTargetEncoderTrain('department', 'is_promoted', n_fold = 5)\ntrans_Train = targetc.fit_transform(trans_Train)\n# trans_Train.drop(['department'], axis = 1, inplace = True)\n\ntrans_Train.head(1)","cc61bf81":"# Label Encoding 'Gender'\nl = LabelEncoder()\ntrans_Train.loc[:, 'gender'] = l.fit_transform(train.loc[:, 'gender'])","056ac2c8":"# Encoding 'Education'\n\n# Mannualy Assigning Weight and Encoding 'ordinal' Feature 'Education'\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\ntrans_Train['education'] = trans_Train['education'].map(edu_enc)\ntrans_Train.education.unique()","fdd7a018":"trans_Train.head(1)","9a490643":"# K-Fold Target Encoding 'age_bin', 'length_of_service_bins'\n\n# 'age_bin'\ntargetc = KFoldTargetEncoderTrain('age_bins', 'is_promoted', n_fold = 5)\ntrans_Train = targetc.fit_transform(trans_Train)\n# trans_Train.drop(['age_bin'], axis = 1, inplace = True)\n\n# 'length_of_service_bins'\ntargetc = KFoldTargetEncoderTrain('length_of_service_bins', 'is_promoted', n_fold = 5)\ntrans_Train = targetc.fit_transform(trans_Train)\n# trans_Train.drop(['length_of_service_bins'], axis = 1, inplace = True)\n\ntrans_Train.head(1)","d452f0ad":"# Before and After Feature Engineering Comparison\nprint(\"Shape of Train Before FE : {}\".format(train.shape))\nprint(f\"Shape of Train Before FE : {trans_Train.shape}\")\nprint(\"No of features added during FE : \", (trans_Train.shape[1] - train.shape[1]))","6e5955af":"# Removing Redundant Feature 'Employee ID' and other un-encoded categorical features - Trans-Train\ntrans_Train.drop(['employee_id',                           # Unique Identifier for Every Feature\n                  'department', 'region', 'recruitment_channel',  # Un-Encoded Categorical Feature\n                  'age_bins', 'length_of_service_bins'], \n                 axis = 1, inplace = True)","905eb936":"# Visualising the Columns of Trans-Train [Feature Engineered DataFrame]\ntrans_Train.columns","d6196441":"# Visualising the Columns of Dummy-Train [Plain Raw Encoded DataFrame]\ndummy_Train.drop(['employee_id'],           # Unique Identifier for Every Feature\n                 axis = 1, inplace = True)   \ndummy_Train.columns","6b5bb4ea":"# Spliting into Dependent and Independent Features Vector\nx = trans_Train.drop(['is_promoted'], axis = 1)\ny = trans_Train['is_promoted'].values.reshape(-1, 1)\n\nx1 = dummy_Train.drop(['is_promoted'], axis = 1)\ny1 = dummy_Train['is_promoted'].values.reshape(-1, 1)","246f8df1":"trans_Train.info()","4a0acf89":"# Segregating into Categorical and Continuous Features ->\ncat = [\"education\", \"gender\", \"no_of_trainings\", \"previous_year_rating\", \"KPIs_met >80%\", \"awards_won?\", \n       \"is_promoted\", \"KPI_n_Award\", \"trainings>4?\", \"good_overall_performance?\",\n       \"tgt_recruitment_channel\", \"tgt_region\", \"tgt_department\", \"tgt_age_bins\", \"tgt_length_of_service_bins\",\n       \"mean_kpi_by_dept\", \"mean_training_by_dept\", \"mean_rating_by_dept\"]\n\ncont = [\"age\", \"length_of_service\", \"avg_training_score\", \"age_boxcox\", \"length_of_service_boxcox\", \"cummulative_train'_score\", \n        \"dept_rating_mean_ratio\"]\n\nprint(f\"{len(cat) + len(cont)} & {trans_Train.shape[1]}\")","da45cf01":"# Fitting Variance Threshold to our Dataframe\nconst_thres = VarianceThreshold(threshold = 0).fit(trans_Train)\nlen(trans_Train[trans_Train.select_dtypes([np.number]).columns].columns[const_thres.get_support()])","be1d5ad1":"# Printing out Constant Columns if any\nconstant_columns = [col for col in trans_Train.columns if col not in trans_Train.columns[const_thres.get_support()]]\nconstant_columns","6f578e90":"# Finding out Quasi Constant Features\nquasiDetect = VarianceThreshold(threshold = 0.01)  \nquasiDetect.fit(trans_Train)\nprint(f\"No. of Non Quasi Constant Features : {len(trans_Train.columns[quasiDetect.get_support()])}\")\nprint(f\"Quasi-Constant Features : {[col for col in trans_Train.columns if col not in trans_Train.columns[quasiDetect.get_support()]]}\")","39c785a7":"# The features we would be dropping are only the binary ones -> 'trainings>4?', 'mean_kpi_by_dept', 'age_boxcox' as others are multilabel encoded classes \ntrans_Train.drop(['trainings>4?', 'mean_kpi_by_dept', 'age_boxcox'], axis = 1, inplace = True)","1911e44d":"# Updating Categorical Features List\ncat = [e for e in cat if e not in ['trainings>4?', 'mean_kpi_by_dept']]\ncat","aa280d5d":"# Updating Continuous Features List\ncont = [e for e in cont if e not in ['age_boxcox']]\ncont","5e363a2a":"# To Find out Repeatative Features (Duplicate)\ntrain_enc =  pd.DataFrame(index = trans_Train.index)\n\ndup_cols = {}\n\nfor i, c1 in enumerate(tqdm_notebook(train_enc.columns)):\n    for c2 in train_enc.columns[i + 1:]:\n        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):\n            dup_cols[c2] = c1\ndup_cols","7bd38a46":"# Correlation using heatmap - Continuous Variable & Target Variable - Pearson Correlation\nhm = trans_Train[cont + [\"is_promoted\"]].corr().where(np.tril(np.ones(trans_Train[cont + [\"is_promoted\"]].corr().shape)).astype(np.bool)) # to delete the upper triangle\nplot = sns.heatmap(hm, annot = True, cmap = \"YlGnBu\")\nplt.setp(plot.get_xticklabels(), rotation = 90)\nplt.gcf().set_size_inches(15, 8)","af4e03fb":"# Covariance using Heatmap - Continuous Variable & Target Variable\ntrans_Train_x = trans_Train[cont + [\"is_promoted\"]].round(decimals = 2)\ncovMatrix = pd.DataFrame.cov(trans_Train_x[cont + [\"is_promoted\"]])\nsns.heatmap(covMatrix, annot = True, fmt = 'g')\nplt.gcf().set_size_inches(15, 8)","5f834278":"# Correlation using heatmap - Categorical Variable & Target Variable - Cramer's V Correlation\n\ndef cramers_V(var1, var2):\n    crosstab = np.array(pd.crosstab(var1, var2, rownames = None, colnames = None)) \n    stat = chi2_contingency(crosstab)[0]\n    obs = np.sum(crosstab)\n    mini = min(crosstab.shape) - 1 \n    return (stat \/ (obs * mini))\n\nrows = []\ndata_encoded = trans_Train.copy()\ndata_encoded = data_encoded[cat]\nfor var1 in data_encoded:\n  col = []\n  for var2 in data_encoded :\n    cramers = cramers_V(data_encoded[var1], data_encoded[var2]) # Cramer's V test\n    col.append(round(cramers, 2))  \n  rows.append(col)\n  \ncramers_results = np.array(rows)\n\n# Cramer's V Test Transformed Dataframe\ndf = pd.DataFrame(cramers_results, columns = data_encoded.columns, index =data_encoded.columns)\n\n# HeatMap  Visualisation\nmask = np.zeros_like(df, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplot = sns.heatmap(df, mask = mask, vmin = 0., vmax = 1, \n                   annot = True, cmap = \"YlGnBu\", square = True)\nplt.setp(plot.get_xticklabels(), rotation = 90)\nplt.gcf().set_size_inches(15, 8)","834931b2":"# Correlation using heatmap - Continuous Variable & Target Variable - Overall Correlation.\nhm = trans_Train.corr().where(np.tril(np.ones(trans_Train.corr().shape)).astype(np.bool)) # to delete the upper triangle\nplot = sns.heatmap(hm, annot = True, cmap = \"YlGnBu\")\nplt.setp(plot.get_xticklabels(), rotation = 90)\nplt.gcf().set_size_inches(15, 8)","0344090d":"trans_Train.columns","212846dd":"# Removing Highly Correlated Independent Features [corr > 0.6]\n# Also, Correlation has been checked between target variable in case they're similar\ntrans_Train.drop([\"awards_won?\", \"tgt_department\", \"KPI_n_Award\", \"mean_training_by_dept\", \"mean_rating_by_dept\",     # Categorical Features\n                  \"age\", \"length_of_service\", \"no_of_trainings\",                                                     # Continuous Features\n                  ], axis = 1, inplace = True)\n\n# Updating Features List\ncat = [e for e in cat if e not in [\"awards_won?\", \"tgt_department\", \"KPI_n_Award\", \"mean_training_by_dept\", \"mean_rating_by_dept\", \"no_of_trainings\"]]\ncont = [e for e in cont if e not in [\"age\", \"length_of_service\"]]","260c857c":"# Dropping Features that Mean [Target Encoded and Bin, both present] the Same -> 'length_of_service_boxcox' as it is having the lowest correlation wrt target variable\ntrans_Train.drop([\"length_of_service_boxcox\"], axis = 1, inplace = True)","149c7667":"# Updating Features List\ncont = [e for e in cont if e not in [\"length_of_service_boxcox\"]]","225b01b6":"# Printing the Remaining Column Names\nprint(f\"Categorical : {cat} \\nContinuous : {cont}\")\nprint(f\"No. of Categorical Features Left : {len(cat)} \\nNo. of Continuous Features Left : {len(cont)}\")\nprint(\"No. of Features in Total Now : {}\".format(trans_Train.shape[1]))","e5e77495":"# Not Done as Drop Done Manually\n# Automated Drop - Correlated Features\n'''\ncol_corr = set() # Set of all the names of deleted columns\ncorr_matrix = trans_Train[cont + [\"is_promoted\"]].corr()\nfor i in range(len(corr_matrix.columns)):\n  for j in range(i):\n    if corr_matrix.iloc[i, j] >= threshold and (corr_matrix.columns[j] not in col_corr):\n      colname = corr_matrix.columns[i] # getting the name of column\n      col_corr.add(colname)\n      #if colname in dataset.columns:\n      #del dataset[colname] # deleting the column from the dataset\n'''","9c1550e0":"# Visualising Head of the Remaining DataFrame\ntrans_Train.head(1)","ee0c7ed4":"# Not Done\n## Anova\n'''\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nanova = ols('SepalLengthCm ~ C(Species) + SepalWidthCm + PetalLengthCm + PetalWidthCm', data = trans_Train).fit()\nsm.stats.anova_lm(anova, typ = 2)\nanova.summary()\n'''","cd6557d1":"# Dividing Trans-Train-OS into Independent and Dependent Features\nx = trans_Train.drop(['is_promoted'], axis = 1)\ny = trans_Train['is_promoted']\nx.shape, y.shape","7d960277":"class_weights = class_weight.compute_class_weight('balanced', np.unique(y), y)\nim_weight = dict(enumerate(class_weights))\nim_weight","a250e55e":"# IGNORE -> Not Done as it takes a huge amount of time.\n'''\nefs = ExhaustiveFeatureSelector(rfc(), \n           min_features = 4,\n           max_features = 10, \n           scoring = 'f1',\n           cv = 5)\n\n# fit the object to the training data.\nefs = efs.fit(x, y)\n\n# print the selected features.\nselected_features = x.columns[list(efs.k_feature_idx_)]\nprint(selected_features)\n'''","34e0e90b":"# Sequential Feature Selector Object and Configuring the Parameters -> Forward Elimination\nsfs_fw = SFS(rfc(class_weight = im_weight, random_state = 0, n_jobs = -1),\n          k_features = 10,\n          forward = True, \n          floating = False,\n          verbose = 2,\n          scoring = 'f1',\n          cv = 5,\n          n_jobs = -1)\n\n# Fit the object to the Training Data.\nsfs_fw.fit(x, y)","ea90842c":"# Print the Selected Features.\nselected_features = x.columns[list(sfs_fw.k_feature_idx_)]\nprint(selected_features)\n\n# Print the Final Prediction Score.\nprint(sfs_fw.k_score_)","d510a6c2":"# IGNORE -> Not now, after all the methods are been evaluated\n'''\n# Transform to the newly Selected Features.\nx_sfs = sfs.transform(x)\n'''","3f4fefe9":"# Sequential Feature Selector Object and Configuring the Parameters -> Backward Elimination\nsfs_bw = SFS(rfc(class_weight = im_weight, random_state = 0, n_jobs = -1),\n          k_features = 10,\n          forward = False, \n          floating = False,\n          verbose = 2,\n          scoring = 'f1',\n          cv = 5,\n          n_jobs = -1)\n\n# Fit the object to the Training Data.\nsfs_bw.fit(x.values, y)","db02f7e9":"# Print the Selected Features.\nselected_features_BW = x.columns[list(sfs_bw.k_feature_idx_)]\nprint(selected_features_BW)\n\n# Print the Final Prediction Score.\nprint(sfs_bw.k_score_)","0cafc697":"# IGNORE -> Not now, after all the methods are been evaluated\n'''\n# Transform to the newly Selected Features.\nx_sfs = sfs.transform(x)\n'''","92335d0d":"# On Feature Engineered Trans-Train\n\n# Spliting into Independent and Dependent Feature Vectors\ny = trans_Train['is_promoted']                                          # dependant feature vector\nx = trans_Train.drop(['is_promoted'], axis = 1)                         # independant feature vector\n\ny = y.values.reshape(-1,1)\n\n'''\nfor i in trans_Train.columns[train.dtypes == 'object']:\n    x[i] = x[i].factorize()[0]\n'''\n\n# Fitting Random Forest Algorithm into our dataset\nrfc_wr_1 = rfc(class_weight = im_weight, random_state = 0, n_jobs = -1)\nrfc_wr_1.fit(x, y)","09ede59c":"# Visualising the Importance of Features by Plot Graph\nfeatures_Imp = pd.Series(rfc_wr_1.feature_importances_, index = x.columns)\nfeatures_Imp.nlargest(10).plot(kind = 'barh')\nplt.show()","fc1ed44f":"# Show the Whole List\nfeatures_Imp * 100","a9aa0370":"# deleting redundant dataframe\ndel dummy_Train","1711b312":"# The \"f1\" scoring is proportional to the number of correct classifications per class\nrf_r = rfc(class_weight = im_weight, random_state = 0, n_jobs = -1) \nrfecv = RFECV(estimator = rf_r, step = 1, cv = 5, scoring = 'f1')   # 5-fold cross-validation\nrfecv = rfecv.fit(x, y)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x.columns[rfecv.support_])","7ad96500":"# Using SelectkBest Method\n\n'''\nfor i in train.columns[train.dtypes == 'object']:\n    x1[i] = x1[i].factorize()[0]\n'''\n'''\nx = trans_Train.drop([\"is_promoted\"], axis = 1) \ny = trans_Train.loc[:, \"is_promoted\"] \n'''\n  \n# Applying SelectKBest to extract top 10 best features\nbest_Features = SelectKBest(score_func = chi2, k = 10)    # using mectric of chi-square\nfit = best_Features.fit(x, y)\ndf_Scores = pd.DataFrame(fit.scores_)\ndf_Columns = pd.DataFrame(x.columns)\n\n# Concat two dataframes for better visualization \nfeature_Scores = pd.concat([df_Columns, df_Scores], axis = 1)\nfeature_Scores.columns = ['Columns', 'Score']\nprint(feature_Scores)","52f42247":"# Displaying the Score of Best 15 Features\nprint(feature_Scores.nlargest(15, 'Score'))","9be8b3b1":"# Fiting Dummy set into Extra Trees\nmodel = ExtraTreesClassifier(class_weight = im_weight, random_state = 0, n_jobs = -1)\nmodel.fit(x, y)","893c67d6":"# Visualising the Importance of Features by Plot Graph\nfeat_importances = pd.Series(model.feature_importances_, index = x.columns)\nfeat_importances.nlargest(10).plot(kind ='barh')\nplt.show()","02b71926":"feat_importances = pd.Series(model.feature_importances_, index = x.columns) * 100\nfeat_importances","8b2c082c":"# Important Features after Feature Selection according to the Algorithms\nprint(f\"*** Results from Filter Methods *** \\n{x.columns}\\n\")                         # Filter Method\nprint(f\"*** Results from Wrapper Methods *** \\n{selected_features_BW}\\n\")             # Backward Selection Method\nprint(f\"*** Results from Embedded Methods*** \\n{x.columns[rfecv.support_]}\\n\")        # Random Forest CV Method","7e3d5bb9":"# Common Features in both Wrapper and Embedded Methods\nset_A = set(selected_features_BW)        # Wrapper \nset_B = set(x.columns[rfecv.support_])   # Embedded\nset_A & set_B                            # Common Entries","ba2d4594":"# Creating a list for the Final List of Features\nf_features = [\"avg_training_score\", \"dept_rating_mean_ratio\", \"tgt_age_bins\", \"KPIs_met >80%\", \"education\",\n              \"previous_year_rating\", \"tgt_recruitment_channel\",\n              \"cummulative_train'_score\"]\n\n# Printing the Final List of Features\nprint(f\"Printing the Final List of Features : {f_features}\")","c17e06cb":"# Taking in Only the Important Independent Features\nx_dim = x.copy()          # Saving the Dataframe for Dimensionality Reducing Techniques.\nx_wt = x[f_features]\nx_wt.head(1)","8318263b":"# Shape of the Independent Features Set\nx_wt.shape","ea8d5d05":"# Shape of the Dependent Feature Set\ny_wt = y\ny_wt.shape","3cdf2879":"# Pickling Over-Sampled Plain DataFrame\nx_wt['is_promoted'] = y_wt\nx_wt.to_pickle(\"WT_Plain.pkl\")","611f80fa":"# creating a copy\nx_scale = x_wt.copy()","ad270d7e":"# Standard Scaling all the Features for Dimensionality Reduction\nsc_x = StandardScaler()\nx_scale = sc_x.fit_transform(x_scale)\nx_scale","78cc67dd":"# Normal - PCA\npca = PCA(n_components = 5, random_state = 0)\npca_wt = pca.fit_transform(x_scale)\nexplained_variance = pca.explained_variance_ratio_\nprint(f\"Variance Explained per Principal Component : {explained_variance}\")","f96243c0":"# Visualising PCA Segregation\nplt.figure(figsize = (10, 5))\nplt.scatter(pca_wt[:,0], pca_wt[:,1], c = y, s = 0.5)","3201ecf9":"# IGNORE -> Takes up More than 10GB+ of RAM and crashes the kernel\n'''\n# Kernel - PCA\nkpca = kp(n_components = 5, kernel = 'rbf')\nx_kpca = kpca.fit_transform(x_scale)\nexplained_variance = kpca.explained_variance_ratio_\n'''","5722dd31":"# t-SNE\ntsne_wt = TSNE(random_state = 0).fit_transform(pca_wt)\n# tsne = TSNE(n_components = 5, init = 'pca', random_state = 0, learning_rate = 150, perplexity)","ed60a414":"# Visualising t-SNE Segregation\nplt.figure(figsize = (10, 5))\nplt.scatter(tsne_wt[:,0], tsne_wt[:,1], c = y, s = 0.5)","d23a9b05":"# Pickling Over-Sampled Plain DataFrame\ntsne_wt = pd.DataFrame(tsne_wt)\ntsne_wt['target'] = y\ntsne_wt.to_pickle(\"WT_tsne.pkl\")","9257a4d1":"# Deleting off redundant objects\/variables - Memory Efficient Techniques\ndel fig, boxc_Age, boxc_LOS, xyz, arr, quantile_1, quantile_3, iqr, lower_Bound, upper_Bound, outliers, discretizer, temp, targetc, cat, cont, dup_cols, train_enc, hm, plot, covMatrix, mask, sfs_fw, selected_features, sfs_bw, selected_features_BW, rfc_wr, rf_r, rfecv, best_Features, fit, df_Scores, df_Columns, feature_Scores_skb, model, feat_importances, set_A, set_B, x_scale, sc_x, pca, explained_variance, x_dim","5473662c":"# Importing Oversampled Pickled DataFrames\ndf_os_plain = pd.read_pickle('\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/OS_Plain.pkl')\ndf_os_tsne = pd.read_pickle('\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/WT_tsne.pkl')","3d99508c":"# Segregating the Features into Independent and Dependent Vectors\nx_os_plain = df_os_plain.drop(['is_promoted'], axis = 1)\ny_os_plain = df_os_plain['is_promoted'].values.reshape(-1, 1)\n\n# Visualising and Storing the Feature Names for Oversampled Plain DataFrame\nos_columns = x_os_plain.columns\nos_columns","9cd16343":"# Creating a Copy of Validation Set\nval_os = val.copy()","d781683d":"# Data Cleaning - Handling NULL Values\n\n## KPIs_met >80% - Filling it up with 0\nval_os[\"KPIs_met >80%\"].fillna(0, inplace = True)\n\n## Age - Filling it up with Train's Median Value\nval_os[\"age\"].fillna(round(train['age'].median()), inplace = True)\n\n## Average Training Score - Filling it up with Train's Median Value\nval_os[\"avg_training_score\"].fillna(round(train['avg_training_score'].median()), inplace = True) \n\n## Awards Won? - Filling it up with 0\nval_os[\"awards_won?\"].fillna(0, inplace = True)\n\n## Education - Filling it up with 'Others'\nval_os[\"education\"].fillna(\"Others\", inplace = True)\n\n## Gender - Filling it up with 'm' as maximum of the employee will be males.\nval_os[\"gender\"].fillna(\"m\", inplace = True)\n\n## No. of Trainings - Filling it up with Train's Mode Value\nval_os[\"no_of_trainings\"].fillna((train['no_of_trainings'].mode()), inplace = True)\n\n## Previous Year Rating - Filling it up with Train's Median Value\nval_os[\"previous_year_rating\"].fillna(round(train['previous_year_rating'].median()), inplace = True)\n\n## Recruitment Channel - Filling it up with Train's Mode Value\nval_os[\"recruitment_channel\"].fillna((train['recruitment_channel'].mode()), inplace = True)\n\n## Region - Filling it up with Train's Mode Value\nval_os[\"region\"].fillna((train['region'].mode()), inplace = True)","488ec6b9":"# Data Handling - Handling Corner Cases\n\n## KPIs_met > 80% - Filling it up with Train's Median Value\nval_os['age'] = val_os.apply(lambda x: round(train.age.median()) if (x['age'] > 100 or x['age'] < 15) else x['age'], axis = 1)\n\n## Age - Filling it up with 0\nval_os['KPIs_met >80%'] = val_os.apply(lambda x: 0 if (x['KPIs_met >80%'] not in [0, 1]) else x['KPIs_met >80%'], axis = 1)\n\n## Average Training Score - Filling it up with Train's Median Value\nval_os['avg_training_score'] = val_os.apply(lambda x: round(train.avg_training_score.median()) if (x['avg_training_score'] > 100) else x['avg_training_score'], axis = 1)\n\n## Awards Won? - Filling it up with 0\nval_os['awards_won?'] = val_os.apply(lambda x: 0 if (x['awards_won?'] not in [0, 1]) else x['awards_won?'], axis = 1)\n\n## Education - Changing it up with Train's Mode Value\nlis = list(jum.education.unique())\nval_os['education'] = val_os.apply(lambda x: train.education.mode() if (x['education'] not in lis) else x['education'], axis = 1)\n\n## Gender - Filling it up with 'm' as maximum of the employee will be males.\nval_os[\"gender\"] = val_os.apply(lambda x: m if (x['gender'] not in ['m', 'f']) else x['gender'], axis = 1)\n\n## No. of Trainings - Filling it up with Train's Mode Value\nval_os['no_of_trainings'] = val_os.apply(lambda x: train['no_of_trainings'].mode() if (x['no_of_trainings'] < 0 or x['no_of_trainings'] > 20) else x['no_of_trainings'], axis = 1)\n\n'''\n## Department - Changing it up with Train's Mode Value\nlis = list(train.department.unique())\nval_os['department'] = val_os.apply(lambda x: train.department.mode() if (x['department'] not in lis) else x['department'], axis = 1)\n'''\n\n## Previous Year Rating - Filling it up with Train's Median Value\nval_os[\"previous_year_rating\"] = val_os.apply(lambda x: round(train.previous_year_rating.median()) if (x['previous_year_rating'] < 0 or x['previous_year_rating'] > 5) else x['previous_year_rating'], axis = 1)\n\n## Recruitment Channel - Changing it up with Train's Mode Value\nlis = list(train.recruitment_channel.unique())\nval_os['recruitment_channel'] = val_os.apply(lambda x: train.recruitment_channel.mode() if (x['recruitment_channel'] not in lis) else x['recruitment_channel'], axis = 1)\n\n## Region - Changing it up with Train's Mode Value\nlis = list(train.region.unique())\nval_os['region'] = val_os.apply(lambda x: train.region.mode() if (x['region'] not in lis) else x['region'], axis = 1)","67528a71":"# Defining K-Fold Target Encoding Class for Validation (K-Fold as for Regularization) [Mapping from Train]\n\nclass KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n    \n    def __init__(self, train, colNames, encodedName):\n        \n        self.train = train\n        self.colNames = colNames\n        self.encodedName = encodedName\n         \n    def fit(self, X, y = None):\n        return self\n\n    def transform(self, X):\n\n        mean = self.train[[self.colNames, self.encodedName]].groupby(self.colNames).mean().reset_index() \n        \n        dd = {}\n        for index, row in mean.iterrows():\n            dd[row[self.colNames]] = row[self.encodedName]\n\n        X[self.encodedName] = X[self.colNames]\n        X = X.replace({self.encodedName: dd})\n\n        return X","32784236":"## Displaying the Top 5 Records of Validation Set\nval_os.head(5)","6966ff35":"# Firstly Target Encoding in Train\n\n## Creating a Copy of Train\ndum = train.copy()\n\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n# Got Removed with New Set of Feature Engineered Set\n'''\n## Target Encoding Department\ntargetc = KFoldTargetEncoderTrain('department', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n'''\n\n## Displaying the 1st 5 records of Dummy Train\ndum.head(5)","0053e057":"# Target Encoding in Validation by Mapping from Train\n\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTest(dum, 'region', 'tgt_region')\nval_os = targetc.fit_transform(val_os)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTest(dum, 'recruitment_channel', 'tgt_recruitment_channel')\nval_os = targetc.fit_transform(val_os)\n\n# Got Removed with New Set of Feature Engineered Set\n'''\n## Target Encoding Department\ntargetc = KFoldTargetEncoderTest(dum, 'department', 'tgt_department')\n'''\n\n# Fit Transforming the Encoding\nval_os = targetc.fit_transform(val_os)","7e113f6e":"# Performing Neccessary Actions on the Features to get our final dataframe for Validation for Model Evaluation and Training\n\n## Boxcox Transformation on 'Age'\nval_os['age_boxcox'] = stats.boxcox(val_os['age'])[0]\n\n# Got Removed with New Set of Feature Engineered Set\n'''\n## Cummalative Training Score for Each Employee\nval_os[\"cummulative_train'_score\"] = val_os.no_of_trainings * val_os.avg_training_score\n\n## Categorise Employees if they're having Good Overall Performance\nval_os['good_overall_performance?'] = np.where((val_os.previous_year_rating >= 3) & (val_os['awards_won?'] == 1) & \n                                                   (val_os.avg_training_score >= val_os.avg_training_score.quantile(0.25)), 1, 0)\n'''\n\n## Encoding Education\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\nval_os['education'] = val_os['education'].map(edu_enc)\n\n## Label Encoding Gender\nl = LabelEncoder()\nval_os.loc[:, 'gender'] = l.fit_transform(val_os.loc[:, 'gender']) ","b32d0744":"# Mapping Features from Train -> Validation\n\ny_val_os = val_os['is_promoted'].values.reshape(-1, 1)\nx_val_os = val_os[os_columns]\n\n## Displaying the 1st 5 records of Validation\nx_val_os.head(5)","9eeebc55":"# Visualising the Shape of Validation Sets\nx_val_os.shape, y_val_os.shape","086d1a46":"# Scaling so as to apply Linear Algorithms and it won't affect Tree Based Algorithms Much - Validation\nsc_x = StandardScaler()\nx_val_os_scale = sc_x.fit_transform(x_val_os)","4e408e24":"# Scaling so as to apply Linear Algorithms and it won't affect Tree Based Algorithms Much - Train\nsc_x = StandardScaler()\nx_train_os_scale = sc_x.fit_transform(x_os_plain)","5c6fb273":"# Importing Target Weighted Pickled DataFrames\ndf_wt_plain = pd.read_pickle('\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/WT_Plain.pkl')\ndf_wt_tsne = pd.read_pickle('\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/WT_tsne.pkl')","39d370aa":"# Segregating the Features into Independent and Dependent Vectors\nx_wt_plain = df_wt_plain.drop(['is_promoted'], axis = 1)\ny_wt_plain = df_wt_plain['is_promoted'].values.reshape(-1, 1)\n\n# Visualising and Storing the Feature Names for Target Weighted Plain DataFrame\nwt_columns = x_wt_plain.columns\nwt_columns","549fc523":"# Creating a Copy of Validation Set\nval_wt = val.copy()","7d5705ba":"# Data Cleaning - Handling NULL Values\n\n## Average Training Score - Filling it up with Train's Median Value\nval_wt[\"avg_training_score\"].fillna(round(train['avg_training_score'].median()), inplace = True)\n\n## Department - Filling it up with Train's Mode Value\nval_wt['department'].fillna(train.department.mode(), inplace = True)\n\n## Age - Filling it up with Train's Median Value\nval_wt[\"age\"].fillna(round(train['age'].median()), inplace = True)\n\n## KPIs_met >80% - Filling it up with 0\nval_wt[\"KPIs_met >80%\"].fillna(0, inplace = True)\n\n## Education - Filling it up with 'Others'\nval_wt[\"education\"].fillna(\"Others\", inplace = True)\n\n## Previous Year Rating - Filling it up with 0\nval_wt[\"previous_year_rating\"].fillna(0, inplace = True)\n\n## Recruitment Channel - Filling it up with Train's Mode Value\nval_wt[\"recruitment_channel\"].fillna((train['recruitment_channel'].mode()), inplace = True)\n\n## No of Trainings - Filling it up with Train's Median Value\nval_wt[\"no_of_trainings\"].fillna(round(train['no_of_trainings'].median()), inplace = True)","2d13b88b":"# Data Handling - Handling Corner Cases\n\n## Average Training Score - Filling it up with Train's Median Value\nval_wt['avg_training_score'] = val_wt.apply(lambda x: round(train.avg_training_score.median()) if (x['avg_training_score'] > 100) else x['avg_training_score'], axis = 1)\n\n## Department - Changing it up with Train's Mode Value\nlis = list(train.department.unique())\nval_wt['department'] = val_wt.apply(lambda x: train.department.mode() if (x['department'] not in lis) else x['department'], axis = 1)\n\n## Previous Year Rating - Filling it up with Train's Median Value\nval_wt[\"previous_year_rating\"] = val_wt.apply(lambda x: round(train.previous_year_rating.median()) if (x['previous_year_rating'] < 0 or x['previous_year_rating'] > 5) else x['previous_year_rating'], axis = 1)\n\n## Age - Filling it up with 0\nval_wt['KPIs_met >80%'] = val_wt.apply(lambda x: 0 if (x['KPIs_met >80%'] not in [0, 1]) else x['KPIs_met >80%'], axis = 1)\n\n## KPIs_met > 80% - Filling it up with Train's Median Value\nval_wt['age'] = val_wt.apply(lambda x: round(train.age.median()) if (x['age'] > 100 or x['age'] < 15) else x['age'], axis = 1)\n\n## Education - Changing it up with Train's Mode Value\nlis = list(train.education.unique())\nval_wt['education'] = val_wt.apply(lambda x: train.education.mode() if (x['education'] not in lis) else x['education'], axis = 1)\n\n## Recruitment Channel - Changing it up with Train's Mode Value\nlis = list(train.recruitment_channel.unique())\nval_wt['recruitment_channel'] = val_wt.apply(lambda x: train.recruitment_channel.mode() if (x['recruitment_channel'] not in lis) else x['recruitment_channel'], axis = 1)\n\n## Awards Won? - Filling it up with 0\nval_wt['awards_won?'] = val_wt.apply(lambda x: 0 if (x['awards_won?'] not in [0, 1]) else x['awards_won?'], axis = 1)\n\n## No. of Trainings - Filling it up with Train's Mode Value\nval_wt['no_of_trainings'] = val_wt.apply(lambda x: train['no_of_trainings'].mode() if (x['no_of_trainings'] < 0 or x['no_of_trainings'] > 20) else x['no_of_trainings'], axis = 1)\n\n'''\n## Gender - Filling it up with 'm' as maximum of the employee will be males.\nval_wt[\"gender\"] = val_wt.apply(lambda x: m if (x['gender'] not in ['m', 'f']) else x['gender'], axis = 1)\n\n## Region - Changing it up with Train's Mode Value\nlis = list(train.region.unique())\nval_wt['region'] = val_wt.apply(lambda x: train.region.mode() if (x['region'] not in lis) else x['region'], axis = 1)\n'''","7b5ce496":"## Displaying the Top 5 Records of Validation Set\nval_wt.head(5)","24a4b3da":"# Binning Age and Length of Service\n\n## Creating a Copy of Train\ndum = train.copy()\n\n## Equal Frequency Discretization\ndiscretizer = EqualFrequencyDiscretiser(q = 10, variables = ['age'], return_object = True, return_boundaries = True)\ndiscretizer\n\n# Fit the Discretization Transformer\ndiscretizer.fit(train)\n\n# Transform the data\ntrans_tr = discretizer.transform(train)\ntrans_f = discretizer.transform(val_wt)\n\n# Adding Binned Categories to the Dataframe - Validation\ndum['age_bins'] = trans_tr.age\n## dum['length_of_service_bins'] = trans_tr.length_of_service\n\n# Adding Binned Categories to the Dataframe - Validation\nval_wt['age_bins'] = trans_f.age\n## val_wt['length_of_service_bins'] = trans_f.length_of_service\n\n## Displaying the Top 5 Records of Validation Set\nval_wt.head(5)","420454f2":"# Firstly Target Encoding in Train\n\n## Target Encoding Age Bins\ntargetc = KFoldTargetEncoderTrain('age_bins', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n# Got Removed with New Set of Feature Engineered Set\n\n'''\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Length of Service Bins\ntargetc = KFoldTargetEncoderTrain('length_of_service_bins', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n'''\n\n## Displaying the 1st Record of Dummy Train\ndum.head(1)","809ef7d8":"# Target Encoding in Validation by Mapping from Train\n\n## Target Encoding Age Bins\ntargetc = KFoldTargetEncoderTest(dum, 'age_bins', 'tgt_age_bins')\nval_wt = targetc.fit_transform(val_wt)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTest(dum, 'recruitment_channel', 'tgt_recruitment_channel')\nval_wt = targetc.fit_transform(val_wt)\n\n# Got Removed with New Set of Feature Engineered Set\n\n'''\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTest(dum, 'region', 'tgt_region')\nval_wt = targetc.fit_transform(val_wt)\n\n## Target Encoding Length of Service Bins\ntargetc = KFoldTargetEncoderTest(dum, 'length_of_service_bins', 'tgt_length_of_service_bins')\nval_wt = targetc.fit_transform(val_wt)\n'''\n\n## Displaying the 1st 3 records of Validation\nval_wt.head(3)","7b4c3c54":"# Performing Neccessary Actions on the Features to get our final dataframe for Validation for Model Evaluation and Training\n\n# Mean Rating by Department -> To Generate 'dept_rating_mean_ratio'\nval_wt['mean_rating_by_dept'] = val_wt['department'].map(val_wt.groupby('department')['previous_year_rating'].mean())\n\n# Prev Years' Rating by Department\nval_wt['dept_rating_mean_ratio'] = val_wt['previous_year_rating'] \/ val_wt['mean_rating_by_dept']\n\n# Cummulative Training Score\nval_wt[\"cummulative_train'_score\"] = val_wt.no_of_trainings * val_wt.avg_training_score\n\n## Encoding Education\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\nval_wt['education'] = val_wt['education'].map(edu_enc)","df9aee52":"# Mapping Features from Train -> Validation\n\ny_val_wt = val_wt['is_promoted'].values.reshape(-1, 1)\nx_val_wt = val_wt[wt_columns]\n\n## Displaying the 1st 5 records of Validation\nx_val_wt.head(5)","c459739d":"# Visualising the Shape of Validation Sets\nx_val_wt.shape, y_val_wt.shape","b9145a1e":"# Scaling so as to apply Linear Algorithms and it won't affect Tree Based Algorithms Much - Validation\nsc_x = StandardScaler()\nx_val_wt_scale = sc_x.fit_transform(x_val_wt)","81c591a4":"# Scaling so as to apply Linear Algorithms and it won't affect Tree Based Algorithms Much - Train\nsc_x = StandardScaler()\nx_train_wt_scale = sc_x.fit_transform(x_wt_plain)","6aa137d5":"# Deleting off redundant objects\/variables - Memory Efficient Techniques\ndel df_os_tsne, dum, targetc, edu_enc, sc_x, df_wt_tsne","50c5a2f6":"# Shapes of the DataFrames we're going to do operations on\nprint(f\"X-Train = {x_train_os_scale.shape}, Y-Train = {y_os_plain.shape}\")\nprint(f\"X-Validation = {x_val_os_scale.shape}, Y-Validation = {y_val_os.shape}\")","e6b426e5":"# Fitting Simple Linear Regression to the Training Set\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","f8f13639":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","41260bd8":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","6296208b":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","57b81a88":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","251533b8":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","d3d057d4":"# Fitting Random Forest Classifier to the Training Set\nclassifier = rfc(n_estimators = 1000, random_state = 0, n_jobs = -1)\nclassifier.fit(x_train_os_scale, y_os_plain)","6ef0f81c":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","00a77e20":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","b62f58ec":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","e2b65811":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","71a59224":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","fe4f75f4":"# Fitting Kernel SVM to the Training Set\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","f8892009":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","768a4b98":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","7e75475f":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","74ea7d7e":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","ca7ae8cf":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","a696ce5b":"# Fitting Kernel SVM to the Training Set\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","d50183ee":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","7b104ce3":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","32701aa8":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","e672a2e8":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","73664ba8":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","2a3f6d0d":"# Fitting KNN to the Training Set\nclassifier = knc(n_neighbors = 10, n_jobs = -1)\nclassifier.fit(x_train_os_scale, y_os_plain)","eae8b804":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","31f424f3":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","be2d336f":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","91fc4c64":"# Checking out the F1-Score \nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","cef77469":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","c1d73e2f":"# Fitting Decision Tree Classifier to the Training Set\nclassifier = dtc(random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","09a11fe1":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","bcf61357":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","3be7a32b":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","09869b7a":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","5ef3e0ae":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","1d773f9d":"# Fitting Naive Bayes to the Training Set\nclassifier = GaussianNB()\nclassifier.fit(x_train_os_scale, y_os_plain)","26c5b2f0":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","c90f4c16":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","42a4f850":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","a6124747":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","57be9708":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","29e16157":"# Fitting XGBoost to the Training Set\nclassifier = XGBClassifier(n_estimators = 1000, n_jobs = -1, random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","9e83c9f0":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","798531b8":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","c26d15f2":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","04d18a1d":"# Checking out the F1-Score (Total)\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","419b32c5":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","1dda111a":"# Fitting Gradient Boosting Classifier to the Training Set\nclassifier = GradientBoostingClassifier(random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","d13eff8b":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","591797d5":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","fdbb5f63":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","98ea01e7":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","3c0d0086":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","0d614f1a":"# Fitting AdaBoost Classifier to the Training Set\nclassifier = AdaBoostClassifier(random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","1e54e904":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","3129f62e":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","af9fd4ef":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","382eb724":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","55d5f23b":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","e68ed3c5":"# Fitting CatBoost Classifier to the Training Set\nclassifier = CatBoostClassifier(random_state = 0, eval_metric = 'F1')\nclassifier.fit(x_train_os_scale, y_os_plain, eval_set = (x_val_os_scale, y_val_os))","8bdffa7c":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","7ecc7fa8":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","858eb331":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","194da4df":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","1c1caf92":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","52fe6a9b":"# Fitting Light GBM Classifier to the Training Set\nmodel_lgb = lgbm.LGBMClassifier(random_state = 0, n_jobs = -1)\nmodel_lgb.fit(x_train_os_scale, y_os_plain)    # model_lgb.score(x_val_wt_scale, y_val_wt)","2edd56dc":"# Predicting the Validation Set Results\ny_pred = model_lgb.predict(x_val_os_scale)","ac591466":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","97ac7ecb":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","b30f1b7c":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, model_lgb.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","00dbab0f":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = model_lgb, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","64e3e468":"# Fitting MLP Classifier to the Training Set\nclassifier = MLPClassifier(activation = \"relu\", random_state = 0)\nclassifier.fit(x_train_os_scale, y_os_plain)","43c99c37":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","6687e462":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","822867f3":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","4e33c353":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","93a522f8":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","c16aa0f7":"# Initialising the Stacking Algorithms\nestimators = [\n        ('decision-tree', dtc(random_state = 0)),\n        ('random-forest', rfc(n_estimators = 1000, random_state = 0, n_jobs = -1)),\n        ('mlp', MLPClassifier(activation = \"relu\", random_state = 0))\n        ]","4b3bd0fa":"# Setting up the Meta-Classifier [Randomly Choosed]\nclassifier = StackingClassifier(\n        estimators = estimators, \n        final_estimator = LogisticRegression(random_state = 0),\n        # final_estimator = XGBClassifier(n_estimators = 1000, n_jobs = -1, random_state = 0)\n        n_jobs = -1, cv = 5\n        )","b2502fc3":"# Fitting my Model\nclassifier.fit(x_train_os_scale, y_os_plain)","b96a18d4":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","8dff5725":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","106de838":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","1882c0dc":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","f8b0c993":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","92d7e31e":"# Determining Class Weights\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(y_wt_plain.reshape(-1, )), y_wt_plain.reshape(-1, ))\nim_weight = dict(enumerate(class_weights))\nim_weight","4a7a8cbe":"# Shapes of the DataFrames we're going to do operations on\nprint(f\"X-Train = {x_train_wt_scale.shape}, Y-Train = {y_wt_plain.shape}\")\nprint(f\"X-Validation = {x_val_wt_scale.shape}, Y-Validation = {y_val_wt.shape}\")","60fe54e6":"# Fitting Simple Linear Regression to the Training Set\nclassifier = LogisticRegression(class_weight = im_weight, random_state = 0)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","3a5dde3f":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","cab2e607":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","12a0df41":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","45ec41be":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","100a2317":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","dd5e4e72":"# Fitting Random Forest Classifier to the Training Set\nclassifier = rfc(random_state = 0, n_jobs = -1, class_weight = im_weight, n_estimators = 1000)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","12a31774":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","d74ed380":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","8ff4a58a":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","4b9c8dcd":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","dc697f5c":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","4d699c1a":"# Fitting Kernel SVM to the Training Set\nclassifier = SVC(kernel = 'rbf', random_state = 0, class_weight = im_weight)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","cd8e5660":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","4934bb7f":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","a6db9407":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","6add6bcc":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","90cc29f6":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","1e35bef2":"# Fitting Kernel SVM to the Training Set\nclassifier = SVC(kernel = 'linear', random_state = 0, class_weight = im_weight)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","e9f5fde0":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","bacff60f":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","1154907c":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","abffa777":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","ad0995f6":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","580d3c3c":"# Fitting KNN to the Training Set\nclassifier = knc(n_neighbors = 10, n_jobs = -1)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","236ebba8":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","c1bd0b5a":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","f7d55d6b":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","c2e3032d":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","570ede57":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","60728650":"# Fitting Decision Tree Classifier to the Training Set\nclassifier = dtc(random_state = 0, class_weight = im_weight)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","3238b107":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","93fd3642":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","2e893340":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","75b2cc25":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","1f03a747":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","d93ea53d":"# Fitting Naive Bayes to the Training Set\nclassifier = GaussianNB()\nclassifier.fit(x_train_wt_scale, y_wt_plain)","62724e2e":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale) ","c47bff7e":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","90e806ff":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","248674e5":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","a061e0b4":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","7bd6a501":"# Count Occurences in Each Class\ncounter = Counter(y_wt_plain.reshape(-1, ))\n\n# Estimate 'scale_pos_weight' value\nestimate = counter[0] \/ counter[1]\nprint('Estimate : %.3f' % estimate)","29898a30":"# Fitting XGBoost to the Training Set\nclassifier = XGBClassifier(n_jobs = -1, random_state = 0, scale_pos_weight = estimate, n_estimators = 1000)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","dfb08728":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","b88c622a":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","39eac5bb":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","2e32de69":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","d66db225":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","07431b69":"# Fitting Gradient Boosting Classifier to the Training Set\nclassifier = GradientBoostingClassifier(random_state = 0, n_estimators = 1000)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","a24af3d7":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","5cab9ed2":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","8013d47a":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","00e58403":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","942fe29d":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","74f98ab3":"# Fitting AdaBoost Classifier to the Training Set\nclassifier = AdaBoostClassifier(random_state = 0, n_estimators = 1000)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","ec633e85":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","16596983":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","d65c91ad":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","8010d045":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","a5e6981e":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","4f493074":"# Fitting CatBoost Classifier to the Training Set\nclassifier = CatBoostClassifier(random_state = 0, eval_metric = 'F1', class_weights = im_weight)\nclassifier.fit(x_train_wt_scale, y_wt_plain, eval_set = (x_val_wt_scale, y_val_wt))","11a0fa3a":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","6a1054b1":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","51065142":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","0015b6c6":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","779ca9be":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","96ce0183":"# Fitting Light GBM Classifier to the Training Set\nmodel_lgb = lgbm.LGBMClassifier(random_state = 0, n_jobs = -1, class_weight = im_weight, n_estimators = 1000)\nmodel_lgb.fit(x_train_wt_scale, y_wt_plain)    # model_lgb.score(x_val_wt_scale, y_val_wt)","f9df9797":"# Predicting the Validation Set Results\ny_pred = model_lgb.predict(x_val_wt_scale)","33e574f6":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","8ead6040":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","28f87ce0":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, model_lgb.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_wt, y_pred)}\")","1891fd1a":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = model_lgb, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","ddad05b1":"# Fitting MLP Classifier to the Training Set\nclassifier = MLPClassifier(activation = \"relu\", random_state = 0)\nclassifier.fit(x_train_wt_scale, y_wt_plain)","ad7609fb":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_wt_scale)","6ad76f17":"# Confusion Matrix\ncm = confusion_matrix(y_val_wt, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","7427f564":"# Classification Report\nprint(classification_report(y_val_wt, y_pred))","3df9256d":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_wt_plain, classifier.predict(x_train_wt_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","81713361":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_wt_scale, y = y_val_wt, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","0fab4bf8":"# Initialising the Stacking Algorithms\nestimators = [\n        ('decision-tree', dtc(random_state = 0, class_weight = im_weight)),\n        ('random-forest', rfc(n_estimators = 1000, random_state = 0, n_jobs = -1, class_weight = im_weight)),\n        ('kernel-svm', SVC(kernel = 'rbf', random_state = 0, class_weight = im_weight))\n        ]","9d130625":"# Setting up the Meta-Classifier [Randomly Choosed]\nclassifier = StackingClassifier(\n        estimators = estimators, \n        final_estimator = LogisticRegression(random_state = 0, class_weight = im_weight)\n        # final_estimator = XGBClassifier(n_estimators = 1000, n_jobs = -1, random_state = 0, scale_pos_weight = estimate)\n        )","2c3925a2":"# Fitting my Model\nclassifier.fit(x_train_os_scale, y_os_plain)","5954b9c0":"# Predicting the Validation Set Results\ny_pred = classifier.predict(x_val_os_scale)","13909524":"# Confusion Matrix\ncm = confusion_matrix(y_val_os, y_pred)\nplt.figure(figsize = (5, 5))\nsns.heatmap(cm, annot = True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')","38692e60":"# Classification Report\nprint(classification_report(y_val_os, y_pred))","6713e3b9":"# Checking out the F1-Score\nprint(f\"F1 for Train : {f1_score(y_os_plain, classifier.predict(x_train_os_scale))}\")\nprint(f\"F1 for Validation : {f1_score(y_val_os, y_pred)}\")","8f385fb9":"# Applying k-fold Cross Validation\nf1_s = cvs(estimator = classifier, X = x_val_os_scale, y = y_val_os, cv = 10, scoring = 'f1')\nprint(f\"Max     : {f1_s.max()}\")\nprint(f\"Mean    : {f1_s.mean()}\")\nprint(f\"Std Dev : {f1_s.std()}\")","5aa2eea1":"# Fitting CatBoost Classifier to the Training Set -> Sample\nclassifier = CatBoostClassifier(random_state = 0, eval_metric = 'F1')\nclassifier.fit(x_train_wt_scale, y_wt_plain, silent = True)","b6acf44c":"# Get all the Paramters of CatBoost Classifier\nclassifier.get_all_params()               # get_all_params() -> to get all the parameters","25579945":"# Setting up the Dictionary of Hyper-Paramters\nhyperparams = {\n    \"eval_metric\" : [\"F1\"],                    # Evaluation Metric\n    #\"random_state\" : [0],                     # Random State to retain the same configuration\n    \"iterations\" : [100, 200, 500, 1000],      # Iterations is an alis for 'n_estimators' -> Maximum no of Trees\n    \"loss_function\" : [\"Logloss\"],             # Loss Function for our Model\n    \"learning_rate\" : [0.03, 0.1, 0.001],      # Learning Rate of our Model\n    \"l2_leaf_reg\" : [3.0, 1.0, 5.0],           # L2 Regularization Parameter of our Cost Function to reduce overfitting\n    # \"subsample\" : [1, 0.66, 0.8],            # Sample rate for bagging. -> Cannot be done with GPU Bayersian Type\n    \"depth\" : [6, 7, 8],                       # Depth of our Trees\n    \"class_weights\" : [im_weight],             # Class Weights\n    \"od_type\" : [\"Iter\"],                      # Type of overfitting detector\n    \"od_wait\" : [50, 100],                     # The No. of Iterations to continue the training after the iteration with the optimal metric value\n    # \"thread_count\" : [-1],                   # No. of threads to use during the training\n    \"task_type\": [\"GPU\"],                      # Processing Unit\n    \"bootstrap_type\" : [\"Poisson\"]             # Method for sampling the weights of Objects, Poisson for GPU [Faster Processing]\n}","e81e786c":"# Using Grid Search CV Method to find out the Best Set of Hyper-Parameters\nclassifier = CatBoostClassifier()\nclass_cv = GridSearchCV(classifier, hyperparams, verbose = 1, scoring = 'f1', n_jobs = -1, cv = 5)\nclass_cv.fit(x_train_wt_scale, y_wt_plain, eval_set = (x_val_wt_scale, y_val_wt))","e9a46ef8":"# Dictionary of the Best Parameters\nclass_cv.best_params_","d0f21754":"# Training Data using Best Paramters\nclassifier = CatBoostClassifier(**class_cv.best_params_)\nclassifier.fit(x_train_wt_scale, y_wt_plain, eval_set = (x_val_wt_scale, y_val_wt), silent = True)","58183ae4":"# Predicting the Results\ny_pred = classifier.predict(x_train_wt_scale)\ny_pred","6753e5fd":"# Classification Report\nprint(classification_report(y_wt_plain, y_pred))","eb1a74b5":"# Predicting the Results\ny_pred = classifier.predict(x_val_wt_scale)  \ny_pred","30e701e5":"print(\"***Classification Report After Hyperparameter Tuning***\")\nprint(\"\\n\")\nprint(classification_report(y_val_wt, y_pred))","9a3dd2e5":"# Checking out the F1-Score\nprint(f\"F1 Hyperparameter Tuning : {f1_score(y_val_wt, y_pred)}\")","1ac2638d":"filename = '11263_HR_Analytics.pkl'\npickle.dump(classifier, open(filename, 'wb'))","ed392b00":"# Visualising the Shape of Test Set\ntest.shape","b3c13135":"# Creating a copy of Test\ntest_c = test.copy()","102bb292":"# Data Cleaning - Handling NULL Values\n\n## Age - Filling it up with Train's Median Value\ntest_c[\"age\"].fillna(round(train['age'].median()), inplace = True)\n\n## Average Training Score - Filling it up with Train's Median Value\ntest_c[\"avg_training_score\"].fillna(round(train['avg_training_score'].median()), inplace = True)\n\n## Education - Filling it up with 'Others'\ntest_c[\"education\"].fillna(\"Others\", inplace = True)\n\n## Previous Year Rating - Filling it up with 0\ntest_c[\"previous_year_rating\"].fillna(0, inplace = True)\n\n## Department - Filling it up with Train's Mode Value\ntest_c[\"department\"].fillna((train['department'].mode()), inplace = True)\n\n## Region - Filling it up with Train's Mode Value\ntest_c[\"region\"].fillna((train['region'].mode()), inplace = True)\n\n## KPIs_met >80% - Filling it up with 0\ntest_c[\"KPIs_met >80%\"].fillna(0, inplace = True)\n\n## Recruitment Channel - Filling it up with Train's Mode Value\ntest_c[\"recruitment_channel\"].fillna((train['recruitment_channel'].mode()), inplace = True)","89f6230e":"# Firstly Target Encoding in Train\n\n## Creating a Copy of Train\ndum = train.copy()\n\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Department\ntargetc = KFoldTargetEncoderTrain('department', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)","e87ad27a":"# Target Encoding in Validation by Mapping from Train\n\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTest(dum, 'region', 'tgt_region')\ntest_c = targetc.fit_transform(test_c)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTest(dum, 'recruitment_channel', 'tgt_recruitment_channel')\ntest_c = targetc.fit_transform(test_c)\n\n## Target Encoding Department\ntargetc = KFoldTargetEncoderTest(dum, 'department', 'tgt_department')\ntest_c = targetc.fit_transform(test_c)","837acfe1":"# Performing Neccessary on the Features to get our final dataframe for Validation for Model Evaluation and Training\n\n## Boxcox Transformation on 'Age'\ntest_c['age_boxcox'] = stats.boxcox(test_c['age'])[0]\n\n## Cummalative Training Score for Each Employee\ntest_c[\"cummulative_train'_score\"] = test_c.no_of_trainings * test_c.avg_training_score\n\n## Categorise Employees if they're having Good Overall Performance\ntest_c['good_overall_performance?'] = np.where((test_c.previous_year_rating >= 3) & (test_c['awards_won?'] == 1) & \n                                                   (test_c.avg_training_score >= test_c.avg_training_score.quantile(0.25)), 1, 0)\n\n## Encoding Education\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\ntest_c['education'] = test_c['education'].map(edu_enc)","95be508f":"# Mapping Features from Train -> Test\n\nx = test_c[os_columns]\ny = test_c['is_promoted'].values.reshape(-1, 1)","20ad1566":"# Scaling so as to apply Linear Algorithms and it won't affect Tree Based Algorithms Much - Validation\nsc_x = StandardScaler()\nx = sc_x.fit_transform(x)","6ad1e485":"# Loading the Model from Pickle File\npath = '\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/11263_HR_Analytics.pkl'\nwith open(path, 'rb') as file:  \n    classifier = pickle.load(file)","aecf1d73":"# Predicting the Results\ny_pred = classifier.predict(x)","ef218b57":"# Classification Report\nprint(\"*** Test Classification Report ***\")\nprint(\"\\n\")\nprint(classification_report(y, y_pred))","d031cdad":"# Checking out the F1-Score (Total)\nprint(f\"F1 Hyperparameter Tuning : {f1_score(y, y_pred)}\")","7585d418":"# Visualising the Shape of Test Set\ntest_set.shape","f9c4243c":"# Creating a copy of Test\ntest_c = test_set.copy()","6e255fcc":"# Data Cleaning - Handling NULL Values\n\n## Age - Filling it up with Train's Median Value\ntest_c[\"age\"].fillna(round(train['age'].median()), inplace = True)\n\n## Average Training Score - Filling it up with Train's Median Value\ntest_c[\"avg_training_score\"].fillna(round(train['avg_training_score'].median()), inplace = True)\n\n## Education - Filling it up with 'Others'\ntest_c[\"education\"].fillna(\"Others\", inplace = True)\n\n## Previous Year Rating - Filling it up with 0\ntest_c[\"previous_year_rating\"].fillna(0, inplace = True)\n\n## Department - Filling it up with Train's Mode Value\ntest_c[\"department\"].fillna((train['department'].mode()), inplace = True)\n\n## Region - Filling it up with Train's Mode Value\ntest_c[\"region\"].fillna((train['region'].mode()), inplace = True)\n\n## KPIs_met >80% - Filling it up with 0\ntest_c[\"KPIs_met >80%\"].fillna(0, inplace = True)\n\n## Recruitment Channel - Filling it up with Train's Mode Value\ntest_c[\"recruitment_channel\"].fillna((train['recruitment_channel'].mode()), inplace = True)","e6040ed9":"# Firstly Target Encoding in Train\n\n## Creating a Copy of Train\ndum = train.copy()\n\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTrain('region', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTrain('recruitment_channel', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)\n\n## Target Encoding Department\ntargetc = KFoldTargetEncoderTrain('department', 'is_promoted', n_fold = 5)\ndum = targetc.fit_transform(dum)","1fd79153":"# Target Encoding in Validation by Mapping from Train\n\n## Target Encoding Region\ntargetc = KFoldTargetEncoderTest(dum, 'region', 'tgt_region')\ntest_c = targetc.fit_transform(test_c)\n\n## Target Encoding Recruitment Channel\ntargetc = KFoldTargetEncoderTest(dum, 'recruitment_channel', 'tgt_recruitment_channel')\ntest_c = targetc.fit_transform(test_c)\n\n## Target Encoding Department\ntargetc = KFoldTargetEncoderTest(dum, 'department', 'tgt_department')\ntest_c = targetc.fit_transform(test_c)","71f8f037":"# Performing Neccessary on the Features to get our final dataframe for Validation for Model Evaluation and Training\n\n## Boxcox Transformation on 'Age'\ntest_c['age_boxcox'] = stats.boxcox(test_c['age'])[0]\n\n## Cummalative Training Score for Each Employee\ntest_c[\"cummulative_train'_score\"] = test_c.no_of_trainings * test_c.avg_training_score\n\n## Categorise Employees if they're having Good Overall Performance\ntest_c['good_overall_performance?'] = np.where((test_c.previous_year_rating >= 3) & (test_c['awards_won?'] == 1) & \n                                                   (test_c.avg_training_score >= test_c.avg_training_score.quantile(0.25)), 1, 0)\n\n## Encoding Education\nedu_enc = {\"Below Secondary\" : 0, \"Others\" : 1, \"Bachelor's\": 2, \"Master's & above\": 3}\ntest_c['education'] = test_c['education'].map(edu_enc)","6f12f890":"# Mapping Features from Train -> Test\nx = test_c[os_columns]","0fa2eaca":"# Scaling so as to apply Linear Algorithms and it won't affect Tree Based Algorithms Much - Validation\nsc_x = StandardScaler()\nx = sc_x.fit_transform(x)","d3780456":"# Loading the Model from Pickle File\npath = '\/content\/drive\/My Drive\/Internship\/Day 6\/HR Analytics\/Checkpoints\/11263_HR_Analytics.pkl'\nwith open(path, 'rb') as file:  \n    classifier = pickle.load(file)","2449aa5d":"# Predicting Results\ny_pred = classifier.predict(x)","c1215c90":"# Creating Submission DataFrame\nSubmission_Test_11263 = pd.DataFrame(test['employee_id'])\nSubmission_Test_11263['is_promoted'] = y_pred","432339e2":"# Converting the DataFrame to csv format\nSubmission_Test_11263.to_csv('Submission_Test_11263.csv', index = False)","bd50c400":"!pip freeze > requirements.txt","a5d00b58":"Inference : No Constant Columns","1450823e":"Inference : <br>\n\n*   There are no completely null features in the dataset.\n*   Though there are two features having null values, namely - <br> \n&nbsp;&nbsp;&nbsp;&nbsp;               1.   education <br>\n&nbsp;&nbsp;&nbsp;&nbsp;               2.   previous_year_rating\n*   The maximum number of null values in a single record is 2.\n\n\n\n","d5b5e10c":"Standard Scaling Independent Feature Set","0f98cbb1":"Reference - To Encode Object Features\n\n```\n# Label Encoder\nfor i in train.columns[train.dtypes == 'object']:\n    x[i] = x[i].factorize()[0]\n```\n\n","71a98073":"Checking Accuracies","c7cfa696":"Equal Frequency Discretization","e8f324be":"Inference : <br>\n*   Increase in Length of Services decreases one's chances of getting promoted even he\/she has won an award.","e84aaf53":"Inference : <br>\n*   Features come out to be different when regularization is applied vs when it's not applied. But, we'll be only considering the cases where regularization is applied so as to reduce overfitting.","e7d04d91":"<h2> Outlier Detection <\/h2>","fbb945f1":"Note : [Works Fine on [Google Colab](https:\/\/colab.research.google.com\/drive\/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)]","19c06af3":"**10. AdaBoost Classifier**","f09eb5a2":"Using Grid Search for searching best set of hyperparameters.   \nModel: CatBoost Classification Algorithm","76120a4e":"Duplicate Features","0b09d35b":"Constant Features","7ba5df7a":"Evaluating its Score","d326d47d":"Quasi Constant Features","f7dacde3":"Inference : No Constant Columns","2b1a5911":"Inference : <br>\n\n*  Length of Service is a continuous feature\n\n\n\n","d5c1c52a":"Inference : <br>\n*   Though the outliers are present in the group of employees having length of service as more than 7 and consists 6.26% of the total population, it may contain some vital information with respect to the dependent feature. So, instead of removing the records directly, it's better to come up with a solution after multivariate analysis.\n*   Any transformation done here should be taken utmost priority as by default the nature is leptokurtic which contains risks.","13c4a50a":"Note : [Works Fine on [Google Colab](https:\/\/colab.research.google.com\/drive\/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)]","6daf04f7":"**Problem Statement -**\n\n*   To Perform Data Analysis on the given dataset and form an insightful report on it.\n*   To devise a ML Algorithm which will help to predict whether a potential promotee at checkpoint in the test set will be promoted or not after the evaluation process.\n\n\n","b6605daa":"Inference : <br>\n*   We need to perform Data Cleaning. \n*   We need to perform 'binning' operation on 'age' and then target encode them.\n*   We need to perform 'target encoding' operations on 'recruitment_channel'.\n*   We need to perform weighted label encoding on 'education'.\n*   We need to engineer features like 'dept_rating_mean_ratio' and 'cummulative_train'_score' out.\n*   Error Handling Must also be done.\n*   Standard Scaling should also be done.","51673d00":"Using SelectKBest","e1343eac":"Checking Accuracies","63356503":"Inference : <br>\nFrom all the Methods Performed, we can draw conclusions that -> <br>\n\n*  KPIs_met >80%\n*  age_boxcox\n*  avg_training_score\n*  awards_won?\n*  education\n*  gender\n*  no_of_trainings\n*  previous_year_rating\n*  tgt_recruitment_channel\n*  tgt_region\n\nare the features that would contribute the most to the outcome of our model. This inference in drawn from the the results of the above algorithms and will be robust to all the algorithms irrespective of its nature.\n\n*  No of Features Selected by Filter Method and Embedded Methods are the Same.\n\nTarget Feature -> `is_promoted`.\n","0a4f9c02":"Inference : <br>\n\n*   There are a total of 9 Departments in the Company namely - <br>\n'Sales & Marketing', 'Operations', 'Technology', 'Analytics', 'R&D', 'Procurement', 'Finance', 'HR', 'Legal'\n*   It is a Nominal Type of Categorical variable.\n\n","7d93f0cf":"Checking Accuracies","9a1ce8f4":"Checking Accuracies","4e1e5225":"Data Cleaning","483bae53":"Therefore, F1-Scorecard of the Models are - <br>\n\n\n\n```\n   Model Name             F1-score (10-Fold Mean for Validation)   Max(F1)\n1. Logistic Regression :             0.125                          0.225\n2. Random Forest :                   0.292                          0.392\n3. Kernel-SVM :                      0.144                          0.222\n4. Liner-SVM :                       0.152                          0.213\n5. K-NN :                            0.175                          0.143\n6. Decision Tree :                   0.285                          0.350\n7. Naive Bayes :                     0.197                          0.283\n8. XGBoost Classifier :              0.290                          0.369\n9. GradientBoosting Classifier :     0.274                          0.373\n10. AdaBoost Classifier :            0.249                          0.340\n11. CatBoost Classifier :            0.292                          0.396\n12. LightGBM :                       0.279                          0.369\n13. MLP :                            0.273                          0.391\n14. Stacking Classifier :            0.292                          0.383\n```\n\n","f02a34d5":"Forward Elimination Method","b23f48bc":"<h2> Previous Year's Rating","084a0e13":"**2. Random Forest**","c8af4d60":"Inference : <br>\n\n*   Statistically, 'Mean' and 'KNN-Imputing' is the best way to impute the NULL Values of Previous Year Rating, but that may cause loss of valuable information.\n*   All the employees who had no previous year's rating are new joinees ie their length of service is 1. So, they've no ratings as of now. \n\n","03882e19":"Mapping Columns From Train to Validation","e27057b2":"Testing on Validation","50b010e9":"**5. K-NN**","b0fb5854":"Main Motive of the Selection of Parameters:\n*   I'm trying to have a respectable 'f1' score along with a good 'recall' as this is a case of imbalanced class prediction. There's always a precision-recall tradeoff and I went with 'recall' as I want to predict the number of promoted cases in actual. As with a promotion a company incurres more expenses and facilities, so it's prediction should be done more accurately. So, I've considered 'f1' and 'recall' as my paramters here.","248fa0c0":"Note : <br>\nThe Accuracy Metrics being used in Model Testing are - \n\n1.   F1 - Score (Primary)\n2.   Classification Report\n3.   K-Fold Cross Validation Score\n4.   Confusion Matrix\n\n","e12bbaa9":"# 3. Data Pre-Processing - I [Data Cleaning]","6b409ab0":"Inference : <br>\n*   It is having Multi-Modal Distribution.","1716b4aa":"Inference : <br>\n*   Here, we can observe an interesting pattern.\n*   A person is most likey to get promoted if he\/she is between '25 to 35'. Here it is independent of the average training score.\n*   A Person is also likely to get promoted if his\/her age is more. Though the probability increases if he\/she acquires more avergae training score.","31b45db3":"Saving Progress in a Pickle File for Further Usage [Pickling]","41f3ce64":"Encoding","d3c4c5e0":"Classification Report Before Hyperparameter Tuning\n\n\n\n```\n               precision    recall  f1-score   support\n\n           0       0.94      0.86      0.90      7522\n           1       0.22      0.41      0.28       699\n\n    accuracy                           0.82      8221\n   macro avg       0.58      0.64      0.59      8221\nweighted avg       0.88      0.82      0.85      8221\n```\n\n","9ca18282":"Inference : <br>\n\n*   So, transformations has helped us a lot in reducing the outliers to a great extend, now let's try out Equal Frequency Discretization and figure out it's impact in Feature Selection.\n\n","2b2b9283":"Since there're less number of features, we can perform EDA on each feature individually!","67857753":"Checking Accuracies","4057974f":"Inference: <br>\n*   Of all the departments, 'Region 2' have the highest strength of employees and 'Region 18' has the lowest.","24400300":"<h2> Regions","e99c0796":"Inference: <br>\n*   The Most common source of recruitment of the company is though 'other' sources which occupies more than half of its total recruitment process and the least through 'reference'.","c5327274":"Inference : <br>\n\n*  'Awards won?' is a binary categorical feature.\n\n\n\n","a8477cf9":"**6. Decision Tree**","e3ad2cd6":"**1. Logistic Regression**","2cc11525":"Data Cleaning","b0bb23ff":"# 10. Saving the Model [Pickling]","ed1e2830":"<h2> <b> Filter Methods","1f89e3fc":"Note : <br>\n\n\n*   `trans_Train_OS` is Feature Engineered Vector which was been oversampled.\n\n","cca1fcad":"Encoding Categorical Features","83e925f1":"NOTE : As we're having an highly imbalanced dataset, oversampling will increase the number of occurence of 'minority' class to match the no. of occurences of 'majority' class.","9556d78c":"Data Cleaning","b85a6ace":"Futher Feature Engineering","d7bc50fe":"Saving Progress in a Pickle File for Further Usage [Pickling]","2a552066":"Standard Scaling Independent Feature Set","45957495":"Inference : <br>\n\n*   There are a total of 34 Regions in which the company operates on.\n*   They are numbered from 'Region_1' to 'Region_34'.\n*   It is a *Nominal* Type of Categorical variable.\n","adeaac87":"#  1. Importing Libraries & Datasets","957b2610":"Data Cleaning","9e779430":"Average Training Score","27d3e87c":"# Checkpoint DataFrames <br>\n\nThe Following DataFrames are been pickled to resume work from here rather than running all the cells all over again. <br>\n<br>\n*  Over-Sampled Plain DataFrame : [OS_Plain.pkl](https:\/\/drive.google.com\/file\/d\/1y5gTHju8oOUHPOJFVnC6NS1NYP8PdD9c\/view?usp=sharing)\n*  Over-Sampled tsne DataFrame : [OS_tsne.pkl](https:\/\/drive.google.com\/file\/d\/10Y5pdk7iz7ADutM7isWDoITFlMX9gz0E\/view?usp=sharing)\n*  Weighted Plain DataFrame : [WT_Plain.pkl](https:\/\/drive.google.com\/file\/d\/1bJ84ZqLy04BXcpHkGA25FOfRebdQIJGu\/view?usp=sharing)\n*  Weighted tsne DataFrame : [WT_tsne.pkl](https:\/\/drive.google.com\/file\/d\/1DQLML6RpmeEyds74bQfvxx4n3HIh0sU2\/view?usp=sharing)","562f7d77":"-----------------","38b9ce23":"Inference : <br>\n\n*   There are no Quasi Constant Features","28041333":"Backward Elimination Method","4e1bcc70":"14. Stacking Classifier","c8c05196":"Mapping Columns From Train to Test","137c7fcf":"Inference : <br>\n\n*   The age group with highest no. of employees is '26-30' with the lowest being '55-60' age group\n*   It's observed that maximum of the workforce comes under the age-group of '26-36'. And there is a decrease in number of employees as the age goes up.\n\n","8f18e7c0":"Note : [Works Fine on [Google Colab](https:\/\/colab.research.google.com\/drive\/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)]","f4d796de":"Checking Accuracies","9a1b5827":"Train data using CatBoost Classifier with best parameters","5dc77abb":"To install the dependencies , download `requirements.txt` and run -\n```\n!pip install -r requirements.txt\n```\n*   RAM of around 8GB is preferred if run on Local.\n\n*   `11263_HR_Analytics.ipynb` can be accessed directly through `colab` or `jupyter notebook` from local.\n\n* Note : Application of GPU is been done in this notebook, but it's not at all neccessary.\n\n","9af69efa":"Inference : <br>\n*   The above plot can be used to find out direct relations amongst the independent features to map to the target variable.\n*   But it cannot be used directly to find out interrelations. It's on a higher level intuition.\n\nNote : [Works Fine on [Google Colab](https:\/\/colab.research.google.com\/drive\/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)] ","e4fb64b5":"Checking Accuracies","4142728d":"<h3> Outlier Detection <\/h3>","c30be6c8":"Checking Accuracies","a7fff539":"Promotion Ratio Per Region","1e693ba7":"Checking Accuracies","685a286b":"Checking Accuracies","828eabcd":"<h2> Trying out <b> Dimentionality Reduction Techniques","9a1e8352":"Predicting the Results.","9c5c9644":"Inference : <br>\n*   It is having Multi-Modal Distribution.","2b07b44d":"**11. CatBoost Classifier**","36c0bfb0":"Inference : <br>\n*   So, we can conclude that if no_of_trainings > 4, then there is a very nullified chance that you'll get selected.","7e291a30":"Inference : <br>\n\n*   The Distribution of Age is Platykurtic as it's kurtosis value is less than 3.\n*   Also, it's Right Skewed with a skewness value of 1.02. (Though we can consider it to be moderately skewed)\n*   The Skewness can me mended by applying box-cox transformation.","ba2ad2aa":"<h2> <b>Embedded Methods","1c5a9b18":"<h2> Length of Service","0dbb29d6":"Inference: <br>\n*   So, according to the distributions given, it'll be better if we consider it as a categorical variable.\n*   Around 81% of the employees are having only 1 training in their profile.\n*   The amount of people drastically goes down as the number of training in their profile decreases.\n*   No. of Training is a *Ordinal* Variable.","3692bcd0":"<h3> Previous Year's Rating <\/h3>","a2b416d1":"**1. Logistic Regression**","fd16512b":"# 12. Generating Submission File","4a43483c":"Defining Class Weight for imbalanced classes.","bdd6748c":"Inference : <br>\nSo, There's much improvement in our model after Hyper-Parameter tuning. \n*   Our minority class, '1' [promoted] has shoot up from 0.47 (Best) to 0.56.\nRecall has been increased by a great amount which was our secondary goal given our dataset was imbalanced. <br>\n<br>\n\nSo, Finally Our Overall Model Accuracy Stands at - 85% <br>\n\nwith, 89% Correct Predictions for '0' Label and <br>\n      84% Correct Predictions for '1' Label, which was our Prime Goal.","991f0da6":"<h1> NULL Values Imputation <\/h1>","9cec7b57":"Checking Accuracies","2fba3810":"# 7. Mapping the Features and Transformations & Scaling of Train to Validation","8620088c":"t-SNE [t-Distributed Stochastic Neighbor Embedding]","82865922":"Equal Frequency Discretization","a55597fb":"Checking Accuracies","3a70d268":"Duplicate Features","b3452703":"Inference: <br>\n*   Only 21.7% of the total employees have 5 Ratings. Majority of the people have an average rating of 3 which is dominated by almost 34% of the total employees.\n*   7.54% have 0 rating or NO Rating as of now. ","e454c8b0":"*  The Models are performing better with weights rather than over-sampling.\n*  `CatBoost` is been selected due to it's most balanced performances in both `f1` and `recall` metrices with respect to the other models.\n<br>\nNote : The Best f1 is in the case of  `GradientBoosting Classifier`. \n<br>\n*  It's F1-Score (Max) is 0.47 without hyperparameter tuning.\n","162240e2":"**11. CatBoost Classifier**","9e344d3a":"Inference : <br>\n\n\n*   Here, 'is_promoted' is the dependent feature vector ie the `target variable`.\n*   The rest features are independent feature vectors.\n\n","a9d116ac":"Using IQR Concept","fec81dad":"<h2> KPIs met > 80%","ceebc1ce":"Inference : <br>\n\n*    There are No Duplicate features.\n*   There are No Constant features.\n*   There are a total of 9 Categorical Features namely - <br>\n'department', 'region', 'education', 'gender', 'recruitment_channel', 'previous_year_rating', 'KPIs_met >80%', 'awards_won?', 'is_promoted'\n*   Out of all the categorical features, 4 of them are binary class categories.\n*   The Rest are continuous features.","2460fd3d":"**13. Multi-Layer Perceptron**","e3b701fa":"**12. Light GBM**","0ac8a5f3":"<h2> Feature Selection","eeb2bb26":"Checking Accuracies","900a2239":"Inference : <br>\n*   So, we can conclude that if no_of_trainings > 4, then there is a very nullified chance that you'll get selected.","aa66efee":"Inference : <br>\n*   Hence proved that there's no outliers. It's having a almost perfect Box-Plot.","7faff070":"Generating Submission File","790490e4":"<h2> <b>Working with Target Weighted Dataframe","13c272a6":"Inference : <br>\n\n*   Most of the Employees are having a below average average test scores in the range '51-60' with 28.3% of the total population.\n*   Though only 19 people have a score of 39 ie the lowest score. It consists of 0.0.6% of the whole population.\n*   A respectable amount of 323 people have scores above 90 out of which 26 people have a score of 99.\n\n","a1439f3f":"Inference : <br>\n*   No. of Training is a continuous feature but can also be considered as a type of *Ordinal* categorical feature if considered within limit as 10.","9eec8ca1":"<h2> Checking for Duplicate Value and Constant Features <h2>","34294d6e":"Checking Accuracies","0d17d262":"<h2> Gender","0a16fb44":"Checking Accuracies","67dd8a19":"**8. XGBoost Classifier**","d0646c6f":"Saving Progress in a Pickle File for Further Usage [Pickling]","0419f850":"# 13. Generating Requirements File [Dependencies]","e55076ba":"Inference : <br>\n*   We need to perform Data Cleaning. \n*   We need to perform 'boxcox' operations on 'age'.\n*   We need to perform weighted label encoding on 'education'.\n*   We need to perform label encoding on 'gender'.\n*   We need to perform 'target encoding' operations on 'region', 'recruitment channel'.\n*   Else, apart from these above all are native features with a few transformations.\n*   Error Handling Must also be done.\n*   Standard Scaling should also be done.","6bf31514":"Checking Accuracies","5efbf39e":"# 2. Data Spliting - Train, Validation and Test","a13754fc":"Note : This Notebook was generated in Google Colab. So, the outputs are being ommited as of now.","1e736a6c":"PCA","29f42c5c":"**2. Random Forest**","9240fc33":"Inferences : <br>\n\nFrom the above Graphs and Matrices -\n\n*   An employee is very likely to get selected if he\/she has a high 'average training score', have 'won an award', has a good 'previous year rating' and has 'KPIs score' more than 80%.\n*   Age of an employee and length of service have a high covariance as expected.\n*   As age of an employee increases, his average training score decreases as they've a negative covariance.\n*   An employee having more number of trainings is more likely NOT to get promoted as they're inversely proportional.\n*   'Education Level' of an employee has a positive correlation with 'Age'.\n*   'Employee_ID' feature had to be dropped down as it is a redundant feature.","3a5f2942":"# <h3> After Model Evaluation, The Model with Best Performance is : `['CatBoost Algorithm']` on Weighted Dataset.","bf4aebe9":"# 8i. Model Testing and Evaluation - Over-Sampled DataFrame","a593dd88":"Checking Accuracies","4627a73f":"<h2> Employee ID <\/h2>","2e3ae936":"CatBoost Parameters Documentation : [catboost.ai](https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html)\n\n\n\n\n","b611f82e":"**Important Links -**\n\n*   Problem Statement - [Google Docs](https:\/\/drive.google.com\/file\/d\/11BHkUYAI302GXwFaNf1B74kkmVZMM1nk\/view?usp=sharing)\n\n*  Colab Notebook Link - [11263_HR_Analytics.ipynb](https:\/\/colab.research.google.com\/drive\/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)\n\n*   Datasets - [Directory Link](https:\/\/drive.google.com\/drive\/folders\/1RyuYY_zHwdu35kxIKdmfIymzQSGIYNbw?usp=sharing)\n\n*   Libraries Pre-requisites -  [requirements.txt](https:\/\/drive.google.com\/file\/d\/1pl9ApbHVcVneEbEZANEl8YNgo-UcUmjY\/view?usp=sharing)        \n\n*   Download Pre-loaded Model -  [Pickle Link](https:\/\/drive.google.com\/file\/d\/1hDE7SdVHxKrSKNKxGeGOb1oE8NZW8cW4\/view?usp=sharing)\n\n*   Download Test Submission File -  [Submission_Test_11263.csv]()\n```\nThe Below Links are not yet updated\n```\n\n\n*  Analytics & Insights - [Document Link]()\n\n*  Procedures Followed - [Document Link]()\n","193b74eb":"NOTE : <br>\n*   Here, we can observe two features describing 'Length of Service' ie 'tgt_length_of_service_bins' and 'length_of_service_boxcox'. So, we'll keep the one with highest correlation wrt the target variable.","f9957ae0":"Note : <br>\nThe Accuracy Metrics being used in Model Testing are - \n\n1.   F1 - Score (Primary)\n2.   Classification Report\n3.   K-Fold Cross Validation Score\n4.   Confusion Matrix\n\n","9c67da7e":"Quasi Constant Features","04a17f52":"Predicting the Results","c11a3fd1":"**5. K-NN**","61d2dc28":"Inference : <br>\n*  Score for Backward Elimination is a bit better than Forward Elimination.","a04a8134":"Correlation and Covariance","098c9e35":"**Technical Goal -** <br>\n\nTo find an Optimal Solution for a Binary Classification Model. <br>\n\n\n*   The goal is to find an optimal team of independent variables so that each independent variable of the team has great impact on the dependent variable profits that each independent variable of the team is a powerful predictor that is highly statistically significant.\n\n*   Perform EDA on the dataset given and take out the insightful analysis for Research.","a8fb3a56":"# 11. Evaluating Test Set (Local) Results","8825e240":"Checking Accuracies","bf005971":"Handling Outside Cases","a69e732c":"**12. Light GBM**","959a2ed4":"<h3> Education <\/h3>","f86ddf7b":"Checking Accuracies","51472f4f":"<h2> Trying out <b> Dimentionality Reduction Techniques","a987cc11":"Note : <br>\n\n\n*   `trans_Train` is Feature Engineered Vector.\n*   `dummy_Train` is not.\n\n","22abd4f2":"Scaling First","0a769165":"<h2> Feature Engineering","f4a66b84":"t-SNE [t-Distributed Stochastic Neighbor Embedding]","a4f1546d":"**8. XGBoost Classifier**","a0f69cf2":"Encoding Categorical Features","581b2f6b":"Note : [Works Fine on [Google Colab](https:\/\/colab.research.google.com\/drive\/1fkRYAUnGrmo3PUGYWv7nwiHibWdR0PFi?usp=sharing)]","25f3a193":"Using IQR Concept","7bb9a691":"<h2> Recruitment Channel","aa7881e4":"Inference : <br>\n\n*   There are actually 6 slabs for previous year's rating amongst the employees starting from 1 to 5.\n*   Here there can be many reasons for one's rating to be \"Not Defined\". They can be if a new employee has joined the company after the rating was been evaluation or even if the employee wasn't present during the rating evaluation, etc.\n*    It is a *Ordinal* Type of Categorical variable.\n\n","82101698":"<h2> Department","a08e0ad9":"<h2> <b> Wrapper Methods","dca3d2ad":"Standard Scaling Independent Feature Set","3b68ad83":"<h2> No of Trainings","7b880182":"Inference : No Duplicate Columns","f3a8921c":"<h2> <b> Wrapper Methods","51831be5":"<h2> Check for Quasi-Constant Features","4492e2ef":"NOTE : <br>\n\n*   We won't be proceeding with Dimentionality Reduced DataFrames as we've a very number of features already, and a heavy strategy like tsne isn't at all neccessary here.","dfc16594":"Inference : <br>\nFrom all the Methods Performed, we can draw conclusions that -> <br>\n\n*  avg_training_score\n*  dept_rating_mean_ratio \n*  tgt_age_bins\n*  KPIs_met >80%\n*  education\n*  previous_year_rating\n*  tgt_age_bins\n*  tgt_recruitment_channel\n*  cummulative_train'_score\n\nare the features that would contribute the most to the outcome of our model. This inference in drawn from the the results of the above algorithms and will be robust to all the algorithms irrespective of its nature.\n\nTarget Feature -> `is_promoted`.\n","c0d7be7f":"Inference: <br>\n*   Of all the departments, The highest strength of employees are having 'Bachelor's Degree' and Employees 'Below Secondary' are the lowest.\n*   The imputed 'Others' slab can include degrees like ITI, Polytechnic, Higher Secondary or any Intermediate Courses between Bachelors' and Secondary. It may also contain employees having Secondary Degrees too.","1e6c3c31":"Evaluating its Score","6fae26b2":"# 5. Exploratory Data Analysis - II [Multivarate Analysis]","b226a0d5":"**9. Gradient Boosting Classifier**","f2b3b8d6":"<h2> Promotion after Training (is_promoted) `[Target Variable]`\n\n","bd0279df":"Inference : <br>\n*   We've observed that Feature Engineered Features have more importance with respect to the algorithm than the normal encoded ones. So, we'll be considering that.","e88afc16":"Correlation and Covariance","97a81f3e":"# 6i. Feature Engineering and Selection `With Sampling` ","d324d028":"Importing the Pickled Files","328c6965":"Mapping Columns From Train to Validation","8ce2399a":"PCA","dedec350":"# End","cf667044":"Exhaustive Feature Search","d30d425b":"Inference : <br>\n\n*   There is no Duplicacy of Employee ID\n*   The feature employee ID would be anyways removed during modeling as it is a redundant feature and won't affect model's performance.\n\n\n\n","38d4ecc8":"RFECV (Random Forest with Regularization)","f568679e":"# 9. Hyperparameter Tuning and Model Optimization","21a9e799":"Inference : <br>\n\n*   So, transformations has helped us a lot in reducing the outliers to a great extend, now let's try out Equal Frequency Discretization and figure out it's impact in Feature Selection.\n\n","1edea80c":"**6. Decision Tree**","7fb9c03c":"Inference : <br>\n\n*   There are actually 4 blocks of educational qualification amongst the employees.\n*   They are \"Master's & above\", \"Bachelor's\", 'Below Secondary', \"Not Defined\" (It might due to error in data collection or the educational qualification of the employee doesn't fit into the slabs described by the company.\n*   It is a *Ordinal* Type of Categorical variable.\n","a49436e8":"**Project Insights -**\n\n*   Imbalanced Dataset.\n*   Trees based algos works better than Linear Models.\n\n\n\n","9e4496a8":"**9. Gradient Boosting Classifier**","28de48f4":"Checking Accuracies","7116a1d4":"Checking Accuracies","15fbd4ce":"<h2> Feature Selection","1d8a4254":"**4. Linear - SVM**","5ed88d0d":"**7. Naive Bayes**","01020c80":"Encoding","6e2316df":"Inference : No Duplicate Columns","3953fca2":"Inference: <br>\n*   Most of the employees have satisfied the KPIs ","01048032":"Calculating Feature Importance during Random Forest (Without Regularization)","bec4f1ce":"Using SelectKBest","fd45cc12":"RFECV (Random Forest with Regularization)","228b633a":"Distribution","48fbd121":"# 6ii. Feature Engineering and Selection `Without Sampling`","ef1b599e":"Inference : <br>\n*   It is having Gaussian Distribtuion.","bf35c59e":"Checking Accuracies","0e88b72b":"Inference : <br>\n\n*   The Distribution of Length of Service is Leptokurtic as it's kurtosis value is more than 3.\n*   Such type of distribution is prone to more outliers and is a very risky feature in case of financial data.\n*   Also, it's Right Skewed with a skewness value of 1.79 which is highly skewed.\n*   The Skewness can me mended by applying box-cox transformation.","d6885f97":"**7. Naive Bayes**","077e2b35":"Inference : <br>\n*   Features come out to be different when regularization is applied vs when it's not applied. But, we'll be only considering the cases where regularization is applied so as to reduce overfitting.","49cc3dfe":"Backward Elimination Method","22342dce":"<h2> Age","4e743efe":"Promotion Ratio Per Region","48b9e195":"Inference : <br>\n\n*   So, we've 54808 records of data in our train set along with 14 features.\n*   In the test set, we're having 23490 records of data which is roughly a bit less than 50% of the train data, along with 13 features which is a feature less than the train set which is supposedly the target variable.\n\n","71276226":"Inference : <br>\n\n*   Average Training Score is a continuous feature\n\n\n\n","e6ecdcbd":"Inference : <br>\n\n*   Therefore it is a Binary Classification Problem.\n\n\n\n","6d5b2e1d":"So, the best way to impute 'Previous Year Rating' would be imputing with 0. Though it'll distort the distribution to some extend, but the actual information would be preserved. And that's our goal, preserving the information and prediction the most accurate results.","d7eb17f7":"# 4. Exploratory Data Analysis - I [Univarate Analysis]","1838dd33":"Inference : <br>\n\n*  'KPIs Met >80%' is a binary categorical feature.\n\n\n\n","c0379c8f":"Scaling First","043c2659":"Inference : <br>\n*   Increase in Number of Trainings decreases one's chances of getting promoted even he\/she has some previous year's rating.","137e361c":"Awards Won?","8898f09d":"Calculating Feature Importance during Random Forest (Without Regularization)","75c24083":"Inference: <br>\n*   Most of the employees haven't won any awards.\n*   'awards_won?' is a highly imbalanced feature. ","e874c69e":"Standard Scaling Independent Feature Set","1a272179":"Saving Progress in a Pickle File for Further Usage [Pickling]","28f228ce":"Constant Features","bc08174c":"Predicting the Results","c03db8dd":"Checking Accuracies","17c1ebd1":"14. Stacking Classifier","3ba7aa48":"Checking Accuracies","01b0a0aa":"Checking Accuracies","92d52b4d":"Mapping Columns From Train to Test","25921a9c":"Inference : <br>\n\n*   Age is a continuous feature\n\n","a730ab41":"Checking Accuracies","dbd9fef9":"Futher Feature Engineering","05e90d06":"Inference: <br>\n\n*   The imputed 'Others' slab can include degrees like ITI, Polytechnic, Higher Secondary or any Intermediate Courses between Bachelors' and Secondary. It may also contain employees having Secondary Degrees too.","3b8c7e4d":"Inference : <br>\n\n*   The Distribution of Length of Service is platykurtic as it's kurtosis value is less than 3.\n*   Also, the data is approximately symmetric according to it's skewness which is 0.45.\n*   Since the feature is symmetric about it's central tendency, so there won't be any chances for outliers.","11fc8a78":"<h2> Feature Engineering","78e34b6d":"Checking Accuracies","7c48d8f6":"<h2> <b>Embedded Methods","755e1104":"Handling Outside Cases","33bac540":"<h2> Education\n","baa117ed":"Forward Elimination Method","0223367f":"# 8ii. Model Testing and Evaluation - Class Weighted DataFrame","1ff74c11":"<h2> <b> Splitting into independent and dependent variable vectors","21884b1f":"Grid Search CV","212cec32":"Inference : <br>\n*  Score for Backward Elimination is a bit better than Forward Elimination.","fce7a5d8":"#  **HR Analytics**\n\n* Author: Rahul Bordoloi \n* Email-ID : <rahul.bordoloi@highradius.com>, <mail@rahulbordoloi.me>                    \n* Emp ID: 11263                                                             \n* Date Created: 20 July, 2020      \n* Language & Version - Python 3.8.4                                      \n\n ","add4fbbf":"Using Extra Trees","551d3a9c":"Exhaustive Feature Search","92d27208":"NOTE : <br>\n*   Here, we can observe two features describing 'Age' ie 'tgt_age_bins' and 'age_boxcox'. So, we'll keep the one with highest correlation wrt the target variable.","ffe88508":"<h2> <b> Filter Methods","b35c7164":"Inference : <br>\n*   Though the outliers are present in the 55-60 age group and consists 2.59% of the total population, it may contain some vital information with respect to the dependent feature. So, instead of removing the records directly, it's better to come up with a solution after multivariate analysis.\n*  Segregating the Age Feature into Bins can also help in retaining the information and removing outliers.","3824f3f5":"Defining Class Weight for imbalanced classes.","a33c1016":"Inference : <br>\n\n*   There are actually 4 blocks of educational qualification amongst the employees.\n*   They are \"Master's & above\", \"Bachelor's\", 'Below Secondary', \"Not Defined\" (It might due to error in data collection or the educational qualification of the employee doesn't fit into the slabs described by the company.\n*   It is a *Ordinal* Type of Categorical variable.\n","244fc640":"Inference : <br>\n\n*   An employee is likely to get promoted if he\/she belongs from 'Technology' Department and the number of promotions in case of 'HR' is the lowest.\n*   An employee from 'region 4' is most likely to get promoted as it's employee to promotion ratio is more than rest of the regions. The least is in the case of 'region_18'.\n*   There is no gender biasness in the case of promotion. Both the genders get equally promoted. Though the employee to promotion rate for the case for females is more than males.\n*   A Person having a Master or Above degree is more likely to get promoted as compared to others.\n*   A Person is very likely to get promoted if he\/she has been referred by some external agent.\n\n","5ca34cae":"**10. AdaBoost Classifier**","9b5414ac":"**4. Linear - SVM**","c3e991ad":"Inference: <br>\n*   Of all the departments, 'Sales & Marketing' have the highest strength of employees and 'R&D' has the lowest.","05967cba":"Inference: <br>\n*   Most of the Employees didn't get promoted.\n*   The Target Feature Vector 'is_promoted' is highly imbalanced.","0594c4f7":"Using Extra Trees","d2541bdc":"Note : <br>\n\n*Continuous Features* -> ['age',\t'length_of_service', 'avg_training_score']\t<br>\n*Categorical Features* -> ['department',\t'region',\t'education','recruitment_channel', 'no_of_trainings', 'previous_year_rating']\t<br>\n*Binary Features* -> ['gender',\t'KPIs_met >80%',\t'awards_won?',\t'is_promoted'] Here, 'is_promoted' is `target`.\t<br>\n\n*   Boxcox Transformations works best with 'age' and 'length_of_service'.  Though Binning can be tried out against for it's performance.\n*   K-Fold Target Encoding Scheme was been applied to 'recruitment_channel', 'region' and 'department'  features.\n*   Manual Weighted Encoding was been applied to 'Education'. Need to check for performance with Target, OHE and Label.\n*   'Gender' is been label encoded.\n","e460e4d5":"<h2> <b> Splitting into independent and dependent variable vectors","a462c98c":"**13. Multi-Layer Perceptron**","b95383a2":"Note : <br>\n\n*Continuous Features* -> ['age',\t'length_of_service', 'avg_training_score']\t<br>\n*Categorical Features* -> ['department',\t'region',\t'education','recruitment_channel', 'no_of_trainings', 'previous_year_rating']\t<br>\n*Binary Features* -> ['gender',\t'KPIs_met >80%',\t'awards_won?',\t'is_promoted'] Here, 'is_promoted' is `target`.\t<br>\n\n\n*   Boxcox Transformations works best with 'age' and 'length_of_service'.  Though Binning can be tried out against for it's performance.\n*   K-Fold Target Encoding Scheme was been applied to 'recruitment_channel', 'region' and 'department'  features.\n*   Manual Weighted Encoding was been applied to 'Education'. Need to check for performance with Target, OHE and Label.\n*   'Gender' is been label encoded.\n","a61cbc4f":"Inference: <br>\n*   'Male' is the dominant workforce and % of 'Female' is very less.\n*    'Gender' is a imbalanced feature.","951fb18e":"Inference : <br>\n\n*   There are 2 categories for Gender amongst the employees.\n*   Maybe there are not provisions for Transgenders or they might not be hired yet according to the dataset given.\n*   It is a Binary *Nominal* Categorical Variable.\n","e3be3c7d":"Inference : <br>\n*   No inference can be drawn from this graph.","c5f0d789":"<h2> <b>Working with Over-Sampled Dataframe","81ef0a45":"Therefore, F1-Scorecard of the Models are - <br>\n\n\n\n```\n   Model Name             F1-score (10-Fold Mean for Validation)   Max(F1)\n1. Logistic Regression :             0.273                          0.291\n2. Random Forest :                   0.330                          0.4\n3. Kernel-SVM :                      0.302                          0.325\n4. Liner-SVM :                       0.265                          0.290\n5. K-NN :                            0.030                          0.081\n6. Decision Tree :                   0.364                          0.389\n7. Naive Bayes :                     0.204                          0.306\n8. XGBoost Classifier :              0.362                          0.468\n9. GradientBoosting Classifier :     0.418                          0.495\n10. AdaBoost Classifier :            0.278                          0.354\n11. CatBoost Classifier :            0.371                          0.471\n12. LightGBM :                       0.383                          0.449\n13. MLP :                            0.197                          0.265\n14. Stacking Classifier :            0.339                          0.374\n```","c34f427a":"Inference : <br>\n\n*   There are 3 categories of Recruitment Channel for this Company.\n*   They're - 'sourcing', 'other', 'referred'\n*   It is a *Nominal* Type of Categorical variable.\n","2aefb928":"**Assumptions Considered -**\n\n\n*    Data Collection is not Synthetic. \n*    The integrity of the data is per the real world.\n\n\n\n","3a7e15a3":"<b> 3. Kernel - SVM","7599bb2b":"<b> 3. Kernel - SVM","9d1934c0":"Checking Accuracies"}}