{"cell_type":{"73995c01":"code","82cf95fe":"code","fc252530":"code","56ffbb7d":"code","aad04a75":"code","11982fa3":"code","e1a34053":"code","6a15ac2e":"code","6a8f632f":"code","22b0efe5":"code","bce35ff2":"code","60a87fb7":"code","13b27233":"code","2c6cfb47":"code","f1e18570":"code","f10c9920":"code","f7ed85b1":"code","6e937fa5":"code","06ad39d1":"code","f3824da5":"code","6cf6c690":"code","f7a2dc6b":"code","f892f0f2":"code","0935dee6":"code","825ead4e":"code","542e9a33":"code","f00735b3":"code","3cb78f28":"code","4a77e489":"code","4be6d895":"code","303e5ae8":"code","2999f132":"code","32c340a3":"code","b41f2cdc":"code","240a421e":"code","7b6769d9":"code","64d0d96e":"code","cc7a2828":"code","4fd67f89":"code","2714851a":"code","5c7532c5":"code","9fcf1ff3":"code","10eed8e5":"code","ea8d07ba":"code","ae999766":"code","06300ffa":"code","09272857":"code","8ecfd3bd":"code","a8634bda":"code","94879ac3":"code","7cb5cb60":"markdown","a90ec681":"markdown","3784befe":"markdown","0d6b9a36":"markdown","05f0cd76":"markdown","2d770638":"markdown","77a8f9f3":"markdown","ec6796aa":"markdown","72f0bef6":"markdown","24292402":"markdown","a2fe012f":"markdown","10b16d83":"markdown","744af558":"markdown","e6b4ca76":"markdown","9b255035":"markdown","662487ce":"markdown","303493c9":"markdown","3d4838b5":"markdown","1c13c59f":"markdown","093527ae":"markdown","6b35d7c2":"markdown","5dbb4f62":"markdown","4333e2de":"markdown","3193bb5b":"markdown","6e3e6bd6":"markdown","dd797e87":"markdown","059b20cd":"markdown"},"source":{"73995c01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","82cf95fe":"import pyarrow.parquet as pq # convert parguet formatted files\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.signal import *\nimport statsmodels.api as sm\nfrom scipy import fftpack # Fast Fourier Transform functions","fc252530":"# plot settings\nrand_seed = 135\nnp.random.seed(rand_seed)\nxsize = 12.0\nysize = 8.0\n\nfrom pylab import plot, show, savefig, xlim, figure, \\\n                hold, ylim, legend, boxplot, setp, axes","56ffbb7d":"%%time\n\ntrain_meta_df = pd.read_csv(\"..\/input\/metadata_train.csv\")\ntrain_df = pq.read_pandas(\"..\/input\/train.parquet\").to_pandas()","aad04a75":"train_meta_df.shape","11982fa3":"train_meta_df.head(n=9)","e1a34053":"train_df.shape","6a15ac2e":"train_df.head()","6a8f632f":"# sampling rate\nnum_samples = train_df.shape[0] # 800,000 samples per signal\nperiod = 0.02 # over a 20ms period\nfs = num_samples \/ period # 40MHz sampling rate\n\n# time array support\nt = np.array([i \/ fs for i in range(num_samples)])\n\n# frequency vector from FFT\nfreqs = fftpack.fftfreq(num_samples, d=1\/fs)","22b0efe5":"%%time\n\nfig, ax = plt.subplots()\nfig.set_size_inches(xsize, ysize)\n\nax =sns.countplot(x=\"phase\", hue=\"target\", data=train_meta_df, ax=ax)\nax.set_title(\"Distributions of `Target` variable for each phase is equal\")\nplt.show()","bce35ff2":"%%time\n\nplt.figure(figsize=(15, 10))\nplt.title(\"ID measurement:0, Target:0\",\n         fontdict={'fontsize':36})\nplt.plot(train_df[\"0\"].values, marker=\"o\", label='Phase 0')\nplt.plot(train_df[\"1\"].values, marker=\"o\", label='Phase 1')\nplt.plot(train_df[\"2\"].values, marker=\"o\", label='Phase 2')\nplt.ylim(-50,50)\nplt.legend()\nplt.show()","60a87fb7":"%%time\n\nplt.figure(figsize=(15, 10))\nplt.title(\"ID measurement:1, Target:1\",\n         fontdict={'fontsize':36})\nplt.plot(train_df[\"3\"].values, marker=\"o\", label='Phase 0')\nplt.plot(train_df[\"4\"].values, marker=\"o\", label='Phase 1')\nplt.plot(train_df[\"5\"].values, marker=\"o\", label='Phase 2')\nplt.ylim(-50,50)\nplt.legend()\nplt.show()","13b27233":"%%time\n\nplt.figure(figsize=(15, 10))\nplt.title(\"ID measurement:2, Target:0\",\n         fontdict={'fontsize':36})\nplt.plot(train_df[\"6\"].values, marker=\"o\", label='Phase 0')\nplt.plot(train_df[\"7\"].values, marker=\"o\", label='Phase 1')\nplt.plot(train_df[\"8\"].values, marker=\"o\", label='Phase 2')\nplt.ylim(-50,50)\nplt.legend()\nplt.show()","2c6cfb47":"# uncomment to subset the data (Note the graphs look really different when you do this)\n#train_subset_df = train_df.iloc[:,range(0,99)]\n#train_subset_meta_df = train_meta_df.iloc[range(0,99),:]\n\n# uncomment to use the full dataset\ntrain_subset_df = train_df\ntrain_subset_meta_df = train_meta_df","f1e18570":"%%time\n\nmean_list = train_subset_df.apply(np.mean)\nmedian_list = train_subset_df.apply(np.median)\nstd_list = train_subset_df.apply(np.std)","f10c9920":"mean_signal_df = mean_list.to_frame()\nmean_signal_df = mean_signal_df.reset_index()\nmean_signal_df = mean_signal_df.drop(\"index\",axis=1)\ntrain_subset_meta_df =train_subset_meta_df.merge(mean_signal_df,\"inner\", \n                    left_index=True,right_index=True)\ntrain_subset_meta_df = train_subset_meta_df.rename(index=str, columns={0:\"mean\"})\n\nmedian_signal_df = median_list.to_frame()\ntrain_subset_meta_df =train_subset_meta_df.merge(median_signal_df,\"inner\", \n                    left_index=True,right_index=True)\ntrain_subset_meta_df = train_subset_meta_df.rename(index=str, columns={0:\"median\"})\n\nstd_signal_df = std_list.to_frame()\ntrain_subset_meta_df =train_subset_meta_df.merge(std_signal_df,\"inner\", \n                    left_index=True,right_index=True)\ntrain_subset_meta_df = train_subset_meta_df.rename(index=str, columns={0:\"std_dev\"})","f7ed85b1":"train_subset_meta_df.head(n=9)","6e937fa5":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Mean Across Target Variables\")\nax = sns.boxplot(x=\"target\", y=\"mean\", data=train_subset_meta_df)","06ad39d1":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Median Across Target Variables\")\nax = sns.boxplot(x=\"target\", y=\"median\", data=train_subset_meta_df)","f3824da5":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Stardard Deviation Across Target Variables\")\nax = sns.boxplot(x=\"target\", y=\"std_dev\", data=train_subset_meta_df)","6cf6c690":"ts1 = train_df[\"0\"]\nts2 = train_df[\"1\"]\nts3 = train_df[\"2\"]\n\nplt.figure(figsize=(16,6))\nplt.title(\"ID measurement:0, Target:0\",\n         fontdict={'fontsize':36})\nplt.plot(ts1.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts1.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.plot(ts2.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts2.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.plot(ts3.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts3.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.legend();","f7a2dc6b":"ts1 = train_df[\"3\"]\nts2 = train_df[\"4\"]\nts3 = train_df[\"5\"]\n\nplt.figure(figsize=(16,6))\nplt.title(\"ID measurement:1, Target:1\",\n         fontdict={'fontsize':36})\nplt.plot(ts1.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts1.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.plot(ts2.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts2.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.plot(ts3.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts3.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.legend();","f892f0f2":"ts1 = train_df[\"6\"]\nts2 = train_df[\"7\"]\nts3 = train_df[\"8\"]\n\nplt.figure(figsize=(16,6))\nplt.title(\"ID measurement:2, Target:0\",\n         fontdict={'fontsize':36})\nplt.plot(ts1.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts1.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.plot(ts2.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts2.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.plot(ts3.rolling(window=100000,center=False).mean(),label='Rolling Mean');\nplt.plot(ts3.rolling(window=100000,center=False).std(),label='Rolling sd');\nplt.legend();","0935dee6":"%%time\n\ndef calc_rolling_amp(row, window=100000):\n    return np.max(row.rolling(window,center=False).mean()) - np.min(row.rolling(window=100000,center=False).mean())\n\nrolling100k_amp = train_subset_df.apply(calc_rolling_amp)\n","825ead4e":"rolling100k_amp_df = rolling100k_amp.to_frame()\ntrain_subset_meta_df =train_subset_meta_df.merge(rolling100k_amp_df,\"inner\", \n                    left_index=True,right_index=True)\ntrain_subset_meta_df = train_subset_meta_df.rename(index=str, columns={0:\"rolling100k_amp\"})","542e9a33":"train_subset_meta_df.head(n=9)","f00735b3":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Amplitude Across Target Variables\")\nax = sns.boxplot(x=\"target\", y=\"rolling100k_amp\", data=train_subset_meta_df)","3cb78f28":"def count1SDfromTheMean(row):\n    max_1sd = np.mean(row) + np.std(row)\n    min_1sd = np.mean(row) - np.std(row)\n    noise_points = [x for x in row if (x > max_1sd) or (x < min_1sd)]\n    return (len(noise_points))\n\n","4a77e489":"%%time\n\ncount1SDfromTheMean_list = train_subset_df.apply(count1SDfromTheMean)","4be6d895":"#%%time\n#count1SDfromTheMean_list = train_df.apply(count1SDfromTheMean)","303e5ae8":"count1SDfromTheMean_df = count1SDfromTheMean_list.to_frame()\ntrain_subset_meta_df =train_subset_meta_df.merge(count1SDfromTheMean_df,\"inner\", \n                    left_index=True,right_index=True)\ntrain_subset_meta_df = train_subset_meta_df.rename(index=str, columns={0:\"count1SDfromTheMean\"})","2999f132":"train_subset_meta_df.head(n=9)","32c340a3":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Noise Count Across Target Variables\")\nax =sns.boxplot(x=\"target\", y=\"count1SDfromTheMean\", data=train_subset_meta_df)","b41f2cdc":"# drop \"count1SDfromTheMean\" from the training set\ntrain_subset_meta_df = train_subset_meta_df.drop([\"count1SDfromTheMean\"], axis=1)","240a421e":"def count2SDfromTheMean(row):\n    max_1sd = np.mean(row) + (2 * np.std(row))\n    min_1sd = np.mean(row) - (2 * np.std(row))\n    noise_points = [x for x in row if (x > max_1sd) or (x < min_1sd)]\n    return (len(noise_points))\n\n","7b6769d9":"%%time\n\ncount2SDfromTheMean_list = train_subset_df.apply(count2SDfromTheMean)","64d0d96e":"count2SDfromTheMean_df = count2SDfromTheMean_list.to_frame()\ntrain_subset_meta_df =train_subset_meta_df.merge(count2SDfromTheMean_df,\"inner\", \n                    left_index=True,right_index=True)\ntrain_subset_meta_df = train_subset_meta_df.rename(index=str, columns={0:\"count2SDfromTheMean\"})","cc7a2828":"train_subset_meta_df.head()","4fd67f89":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Noise Count Across Target Variables\")\nax =sns.boxplot(x=\"target\", y=\"count2SDfromTheMean\", data=train_subset_meta_df)\nplt.ylim(0,250)","2714851a":"n_signals_to_load = 3\nsignals = pq.read_pandas(\n    '..\/input\/train.parquet', \n    columns=[str(i) for i in range(n_signals_to_load)]).to_pandas()\nsignals.columns","5c7532c5":"# get fft coeffs\ndef get_fft_coeffs(sig):\n    return fftpack.fft(sig)\n\n# get coeff with highest norm\ndef get_highest_coeff(fft_coeffs, freqs, verbose=True):\n    coeff_norms = np.abs(fft_coeffs) # get norms (fft coeffs are complex)\n    max_idx = np.argmax(coeff_norms)\n    max_coeff = fft_coeffs[max_idx] # get max coeff\n    max_freq = freqs[max_idx] # assess which is the dominant frequency\n    max_amp = (coeff_norms[max_idx] \/ num_samples) * 2 # times 2 because there are mirrored freqs\n    if verbose:\n        print('Dominant frequency is {:,.1f}Hz with amplitude of {:,.1f}\\n'.format(max_freq, max_amp))\n    \n    return max_coeff, max_amp, max_freq\n\n# get max coeff phase\ndef get_max_coeff_phase(max_coeff):\n    return np.angle(max_coeff)\n\n# construct the instant angular phase vector indexed by pi, i.e. ranges from 0 to 2\ndef get_instant_w(time_vector, f0, phase_shift):\n    w_vector = 2 * np.pi * time_vector * f0 + phase_shift\n    w_vector_norm = np.mod(w_vector \/ (2 * np.pi), 1) * 2 # range between cycle of 0-2 \n    return w_vector, w_vector_norm\n\n# find index of chosen phase to align\ndef get_align_idx(w_vector_norm, align_value=0.5):\n    candidates = np.where(np.isclose(w_vector_norm, align_value))\n    # since we are in discrete time, threre could be many values close to the desired one\n    # so let's take the one in the middle\n    return int(np.median(candidates))","9fcf1ff3":"# align waves with np.roll()\nalign_phase = 0.5 # w_i = pi\/2\n\nfig = plt.figure(figsize=(12, 9))\nplot_number = 0\n\nfor signal_id in signals.columns:\n    # get samples\n    sig = signals[signal_id]\n    \n    # fft\n    fft_coeffs = get_fft_coeffs(sig)\n    \n    # asses dominant frequency\n    max_coeff, amp, f0 = get_highest_coeff(fft_coeffs, freqs, verbose=True)\n    \n    # phase shift\n    ps = get_max_coeff_phase(max_coeff)\n    \n    # get angular phase vector\n    w, w_norm = get_instant_w(t, f0, ps)\n    \n    # generate dominant signal at f0\n    dominant_wave = amp * np.cos(w)\n    \n    # idx to roll\n    origin = get_align_idx(w_norm, align_value=align_phase)\n    \n    # roll signal and dominant wave\n    sig_rolled = np.roll(sig, num_samples - origin)\n    dominant_wave_rolled = np.roll(dominant_wave, num_samples - origin)\n    \n    # plot signals\n    plot_number += 1\n    ax = fig.add_subplot(3, 1, plot_number)\n    \n    ax.plot(t * 1000, sig_rolled, label='Rolled Original') # original signal\n    ax.plot(t * 1000, dominant_wave_rolled, color='red', label='Rolled Wave at {:.0f}Hz'.format(f0)) # wave at f0\n    ax.legend()\n    ax.set_xlabel('time (ms)')\n    ax.set_ylabel('Amplitude')\n    ax.set_title('Signal {} rolled'.format(signal_id))\nfig.tight_layout()","10eed8e5":"# align waves with np.roll()\nalign_phase = 0.5 # w_i = pi\/2\nnbins=125\ntrain_subset_meta_df.index = pd.RangeIndex(start=0, stop=len(train_subset_meta_df), step=1)\n\n\ndiff_df= pd.DataFrame()\nfor signal_id in train_subset_df.columns:\n    # get samples\n    sig = train_subset_df[signal_id]\n    # fft\n    fft_coeffs = get_fft_coeffs(sig)\n    \n    # asses dominant frequency\n    max_coeff, amp, f0 = get_highest_coeff(fft_coeffs, freqs, verbose=False)\n    \n    # phase shift\n    ps = get_max_coeff_phase(max_coeff)\n    \n    # get angular phase vector\n    w, w_norm = get_instant_w(t, f0, ps)\n    \n    # generate dominant signal at f0\n    dominant_wave = amp * np.cos(w)\n    \n    # idx to roll\n    origin = get_align_idx(w_norm, align_value=align_phase)\n    \n    # roll signal and dominant wave\n    sig_rolled = np.roll(sig, num_samples - origin)\n    dominant_wave_rolled = np.roll(dominant_wave, num_samples - origin)\n    \n    diff_bw_signAndDom = np.abs(dominant_wave_rolled-sig_rolled)\n    sum_signals=[]\n    numSignalsInBin=int(num_samples\/nbins)\n    #print(num_samples)\n    #print(numSignalsInBin)\n    for i in range(0,num_samples,numSignalsInBin):\n        bin_sum = np.sum(diff_bw_signAndDom[i:i+numSignalsInBin])\n        sum_signals.append(bin_sum)\n    diff_df = diff_df.append(pd.Series(sum_signals), ignore_index=True)\n","ea8d07ba":"colnamesOfDiffTable=[\"rolldiff\"+str(x) for x in list(diff_df.columns)]\ndiff_df.columns = colnamesOfDiffTable\ntrain_subset_meta_df =train_subset_meta_df.merge(diff_df,\"inner\", \n                    left_index=True,right_index=True)\n","ae999766":"train_subset_meta_df.head()","06300ffa":"# function for setting the colors of the box plots pairs\ndef setBoxColorsOfTargets(bp):\n    setp(bp['boxes'][0], color='blue')\n    setp(bp['caps'][0], color='blue')\n    setp(bp['caps'][1], color='blue')\n    setp(bp['whiskers'][0], color='blue')\n    setp(bp['whiskers'][1], color='blue')\n    setp(bp['fliers'][0], color='blue')\n    setp(bp['fliers'][1], color='blue')\n    setp(bp['medians'][0], color='blue')\n\n    setp(bp['boxes'][1], color='orange')\n    setp(bp['caps'][2], color='orange')\n    setp(bp['caps'][3], color='orange')\n    setp(bp['whiskers'][2], color='orange')\n    setp(bp['whiskers'][3], color='orange')\n    setp(bp['fliers'][2], color='orange')\n    setp(bp['fliers'][3], color='orange')\n    setp(bp['medians'][1], color='orange')","09272857":"np.arange(0, 25, step=1)","8ecfd3bd":"for i in range(0,len(colnamesOfDiffTable),25):\n    rolldiff_train_df = train_subset_meta_df.loc[:,[\"target\"]+colnamesOfDiffTable[i:i+25]].copy()\n    rolldiff_train_df = rolldiff_train_df.set_index(\"target\")\n    rolldiff_train_df = rolldiff_train_df.stack()\n    rolldiff_train_df = rolldiff_train_df.reset_index()\n    rolldiff_train_df.columns=[\"target\",\"rolldiff\",\"value\"]\n\n    plt.figure(figsize=(6,8))\n    sns.set(style=\"whitegrid\")\n    plt.title(\"Sum of Extremeness across Bins in Target Variables\")\n    ax = sns.boxplot(x=\"rolldiff\",y=\"value\", hue=\"target\", data=rolldiff_train_df)\n    plt.xticks(np.arange(0, 25, step=1),np.arange(i, i+25, step=1))\n    plt.xlabel(\"bin\")\n    plt.show()\n    plt.close()","a8634bda":"plt.figure(figsize=(6,8))\nsns.set(style=\"whitegrid\")\nplt.title(\"Sum of Extremeness across Bins in Target Variables\")\nax = sns.boxplot(x=\"target\", y=\"std_dev\", data=train_subset_meta_df)","94879ac3":"train_subset_meta_df.to_csv('metadata_train_V2.csv')","7cb5cb60":"According to the wiki on the 3-phase power scheme:\n> In a symmetric three-phase power supply system, three conductors each carry an alternating current of the same frequency and voltage amplitude relative to a common reference but with a phase difference of one third of a cycle between each.","a90ec681":"Now that we have that figured out, what is a 3-phase power scheme and what does that mean for us in this kaggle competition? To better understand, let's look at the data... ","3784befe":"## Mean, Median, and Standard Deviation of Measurements\nMultiple kernels have done this and found these numbers to be slightly useful so let's see...","0d6b9a36":"We can also note from the plot above that there is a big target class imbalancement issue which could be dealt with via subsampling or loss functions, but I digress.","05f0cd76":"This looks like it could be a really useful feature!","2d770638":"It looks like there is not much of a difference unfortunately...","77a8f9f3":"1. ## Sum \"extremeness\" of each bin\nI want to try and bin the 800,000 data points and see if noise in a specfic location is significantly different in the different targets, but to do this I need to transform the waves so that they can be referenced equally. See kernel for explanation of how this transformation works:  https:\/\/www.kaggle.com\/fernandoramacciotti\/sync-waves-with-fft-coeffs","ec6796aa":"## Amplitude of Rolling Series\nLets smooth the plots of the first 3 power lines to see if we can see any differences between these waves in target=0 verses target=1","72f0bef6":"So based on our tiny sampling of these three powerlines it looks like amplitude could be a significant factor in predicting whether these powerlines are faulty.","24292402":"[](http:\/\/)What if you get the sum of the difference between the rolled original and rolledwave at 50 Hz in 125 bins?","a2fe012f":" Maybe next we can actually try and predict using these features...","10b16d83":"I just have book-keeping variables to help with transforming the voltage signals later...","744af558":"In other words, the 800,000 measurements make up a time-course showing the voltage over time. ","e6b4ca76":"\n### Number of points 2SD from the mean\nThe next feature I am intersted in looking at is the number of data points in each signal that is greater than 2 SD from the mean.","9b255035":"* `train.parquet` which contains the 800,000 rows for the 800,000 measurements for each respective `signal_id`","662487ce":"Therefore, as competitors we need to look into smoothing techniques to measure the amplitude of these waves. We should also measure the variance of our estimated amplitude for each powerline across the 3 phases. ","303493c9":"There is not that much of a difference and spoiler alert the next parameter is probably better and gives basically the same information. I am going to drop this column.","3d4838b5":"It doesn't look like we can see any difference, but this is a small sample size to work with. Let's look at the amplitude across each target group. To calculate the amplitude, I smooth the powerline signals to create a single wave then I subtract the lowest and highest point. ","1c13c59f":"# Resources\n* https:\/\/www.kaggle.com\/timothycwillard\/vsb-power-line-faults-eda-feature-engineering\n* https:\/\/www.kaggle.com\/theoviel\/fast-fourier-transform-denoising","093527ae":"\n## Measuring Amount of Noisy Points\n### Number of points 1SD from the mean\nThe next feature I am intersted in looking at is the number of data points in each signal that is greater than 1 SD from the mean.","6b35d7c2":"We have two training set files:","5dbb4f62":"# Feature Engineering\nSince we don't want to do machine learning on the raw numbers from the parquet files (which could take forever and may not even be useful) I want to create features I can add to the meta tables to train on instead. We will also needs to add these features to the test set.","4333e2de":"(50 cycles\/ 1000 milliseconds) x (20 milliseconds) = 1 cycle","3193bb5b":"* `metadata_train.csv` which contains four columns: \n * `signal_id`: a unique identifier so its meaningless\n  * `id_measurement`: ID code for each powerline. There should be 3 of each number which represents each phase in the 3-phase power scheme (AKA the trio)\n  * `phase`: the phase ID within the trio (0,1,2). \n  * `target`: 0 fixed, 1 broken","6e3e6bd6":"Since each powerline has 3 rows of data (1 for each phase) in the `metadata_train.csv` and the `train.parquet` file that means the `target` is the same for each `signal_id` that has the same `id_measurement` (see plot below which shows the target variable for each phase seperately).  It also means we have 800,000 x 3 measurements per powerline.","dd797e87":"# Introduction\nPowerlines are measured using voltage, but you don't just measure voltage like you measure height. It's not just a single number. It goes up and down making waves. Each wave is a cycle and you can make judgements of each cycle by taking a multitude of measurements over time. For this competition, the underlying electric grid operates at 50 Hz (AKA 50 cycles per 1000 milliseconds). In other words, if we want to measure 50 cycles of voltage on this electric grid we would take measurements for 1000 milliseconds. The measurements in the competition were only performed for 20 milliseconds so we will only see one cycle. See basic math below:","059b20cd":"Note that to make the code run faster I subset the training data for when I am editing this kernel but when I commit the code I do not do this..."}}