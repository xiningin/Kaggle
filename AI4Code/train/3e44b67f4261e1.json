{"cell_type":{"86038816":"code","767d3423":"code","113bafbb":"code","0bcd5842":"code","117ffad5":"code","c9d269db":"code","a7c83558":"code","91927da3":"code","735899e1":"code","11fa4775":"code","e4d3748d":"code","7487f291":"code","c33e6690":"code","13870f10":"code","1bd1e16d":"code","0d3ef87e":"code","1975bd3c":"code","76f40ce2":"code","9621dc84":"code","e2ba566b":"code","2a6782e9":"code","60b970e2":"code","316d072a":"code","0ceaef1b":"code","c330045b":"code","edc06a04":"code","263750fe":"code","e80633f3":"code","05ced720":"code","8377d352":"code","26404b2b":"code","be6ef938":"code","0cddca69":"code","a275c1de":"markdown","8812bfab":"markdown","19d2135b":"markdown","641a696a":"markdown","c824e35f":"markdown","0362c3bc":"markdown","4cf71e53":"markdown","7814dd1a":"markdown","55b5560f":"markdown","d99b1ee7":"markdown","0a7e86d1":"markdown","b3587540":"markdown","e04d89c8":"markdown","5c9a053e":"markdown","78909717":"markdown","b16a554e":"markdown","4c7664de":"markdown","488603ce":"markdown","62c2c72d":"markdown"},"source":{"86038816":"!pip install texthero","767d3423":"!pip install -U spacy","113bafbb":"!pip install lime","0bcd5842":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport texthero as hero  # text processing\nimport seaborn as sns # plotting\nfrom tqdm.auto import tqdm # progress bars\nfrom lime.lime_text import LimeTextExplainer # Model Explanation\nfrom sklearn import metrics # from here and below used for machine learning\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import MaxAbsScaler, FunctionTransformer\nfrom sklearn.model_selection import cross_validate, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer","117ffad5":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport sys\nimport os\n\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9d269db":"# load data\ndf = pd.read_csv(\n    '\/kaggle\/input\/albanian-fake-news-corpus\/alb-fake-news-corpus.csv',\n    parse_dates=['publication_datetime']\n)","a7c83558":"def preprocess_text(s):\n    \"\"\"A text processing pipeline for cleaning up text using the hero package.\"\"\"\n    s = hero.fillna(s)\n    s = hero.lowercase(s)\n    s = hero.remove_digits(s)\n    s = hero.remove_punctuation(s)\n    s = hero.remove_diacritics(s)\n    s = hero.remove_whitespace(s)\n    return s\n\n# A list of stopwords taken from https:\/\/github.com\/arditdine\/albanian-nlp\nSTOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\"]\n\n\ndef make_clf(classifier, scaler=None, feature_extractor=None, use_dense=False):\n    \"\"\"Function for generating pipelines for our experiment.\"\"\"\n    steps = []\n\n    if feature_extractor is not None:\n        steps.append([\"feature_extractor\", feature_extractor])\n    if use_dense:\n        steps.append(\n            [\"to_dense\", FunctionTransformer(lambda x: x.todense(), accept_sparse=True)]\n        )\n    if scaler is not None:\n        steps.append([\"scaler\", scaler])\n    steps.append([\"classifier\", classifier])\n\n    return Pipeline(steps)","91927da3":"data = df","735899e1":"data[\"fake_news\"] = data[\"fake_news\"].astype(bool)","11fa4775":"data[\"preprocessed_content\"] = preprocess_text(data[\"content\"])","e4d3748d":"# Remove stopwords\ndata[\"preprocessed_content_without_stopwords\"] = data[\"preprocessed_content\"].apply(\n    lambda x: \" \".join([word for word in x.split() if word not in (STOPWORDS)])\n)","7487f291":"print(data.loc[data[\"fake_news\"]==1][\"content\"].values[0])","c33e6690":"print(data.loc[data[\"fake_news\"]==0][\"content\"].values[0])","13870f10":"# Defining colors for both sources to be used in our plots\npalette = [\"#d62728\", \"#1f77b4\"]","1bd1e16d":"# Fake News Wordcloud\nhero.wordcloud(\n    data.loc[data[\"fake_news\"] == 1][\"preprocessed_content_without_stopwords\"],\n    max_words=100,\n    width=1900,\n    height=400,\n    background_color=palette[0],\n)","0d3ef87e":"# Non-Fake News Wordcloud\nhero.wordcloud(\n    data.loc[data[\"fake_news\"] == 0][\"preprocessed_content_without_stopwords\"],\n    max_words=100,\n    width=1900,\n    height=400,\n    background_color=palette[1],\n)","1975bd3c":"sns.set(rc={'figure.figsize':(20,10)})\nsns.set_theme(style=\"whitegrid\")","76f40ce2":"# texthero pipeline and plot\ndata[\"pca\"] = (\n    data[\"preprocessed_content_without_stopwords\"].pipe(hero.tfidf).pipe(hero.pca)\n)\n\nplot_values = np.stack(data[\"pca\"], axis=1)\nsns.scatterplot(\n    data=data,\n    x=plot_values[0],\n    y=plot_values[1],\n    hue=\"fake_news\",\n    style=\"fake_news\",\n    markers={True: \"X\", False: \"s\"},\n    alpha=0.5,\n    palette=palette.reverse(),\n)","9621dc84":"data[\"article_word_count\"] = data[\"content\"].apply(lambda x: len(x.split()))\n\nax = sns.histplot(data=data, x=\"article_word_count\", hue=\"fake_news\", palette=palette)\nax.set(xlabel='Article Word Count', ylabel='Frequency')","e2ba566b":"classifiers = [\n    GaussianNB(),\n    LinearSVC(random_state=42),\n    KNeighborsClassifier(),\n    RandomForestClassifier(random_state=42),\n    LogisticRegression(solver=\"liblinear\", random_state=42),\n    DecisionTreeClassifier(random_state=42),\n    MultinomialNB(),\n]\n\nscalers = [MaxAbsScaler(), None]\nprediction_columns = [\"content\", \"preprocessed_content\", \"preprocessed_content_without_stopwords\"]","2a6782e9":"results = []\n\nfor prediction_column in tqdm(prediction_columns):\n    X = data[prediction_column].to_numpy()\n    y = data[\"fake_news\"].astype(int).values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n    \n    for classifier in tqdm(classifiers):\n        for scaler in scalers:\n            clf = make_clf(\n                feature_extractor=TfidfVectorizer(),\n                use_dense=True,\n                scaler=scaler,\n                classifier=classifier,\n            )\n            train_scores = cross_validate(\n                clf,\n                X_train,\n                y_train,\n                cv=5,\n                scoring=[\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"],\n            )\n            clf.fit(X_train, y_train)\n            y_pred = clf.predict(X_test)\n            results.append(\n                {\n                    \"prediction_column\": prediction_column,\n                    \"classifier\": type(classifier).__name__,\n                    \"scaler\": type(scaler).__name__,\n                    \"fit_time\": train_scores[\"fit_time\"].mean(),\n                    \"score_time\": train_scores[\"score_time\"].mean(),\n                    \"train_accuracy\": train_scores[\"test_accuracy\"].mean(),\n                    \"train_precision\": train_scores[\"test_precision_macro\"].mean(),\n                    \"train_recall\": train_scores[\"test_recall_macro\"].mean(),\n                    \"train_f1\": train_scores[\"test_f1_macro\"].mean(),\n                    \"test_accuracy\": metrics.accuracy_score(y_test, y_pred),\n                    \"test_precision\": metrics.precision_score(\n                        y_test, y_pred, average=\"macro\"\n                    ),\n                    \"test_recall\": metrics.recall_score(\n                        y_test, y_pred, average=\"macro\"\n                    ),\n                    \"test_f1\": metrics.f1_score(y_test, y_pred, average=\"macro\"),\n                }\n            )","60b970e2":"results_df = pd.DataFrame(results)","316d072a":"ax = sns.barplot(x=\"test_f1\", y=\"prediction_column\", data=results_df)\nax.set(xlabel='Average Test F1-Score', ylabel='Article Content Preprocessing Approach')","0ceaef1b":"ax = sns.barplot(x=\"test_f1\", y=\"scaler\", data=results_df)\nax.set(xlabel='Average Test F1-Score', ylabel='Scaling Method')","c330045b":"ax = sns.barplot(x=\"test_f1\", y=\"classifier\", data=results_df)\nax.set(xlabel='Average Test F1-Score', ylabel='Classifier')","edc06a04":"ax = sns.barplot(x=\"fit_time\", y=\"classifier\", data=results_df)\nax.set(xlabel='Average Fitting Time (s)', ylabel='Classifier')","263750fe":"best_results_df = results_df[results_df.groupby(['prediction_column'])['test_f1'].transform(max) == results_df['test_f1']]","e80633f3":"best_results_df[\"pipeline\"] = best_results_df[\"prediction_column\"] + \"-\" + best_results_df[\"scaler\"] + \"-\" + best_results_df[\"classifier\"]","05ced720":"ax = sns.barplot(x=\"test_f1\", y=\"pipeline\", data=best_results_df)\nax.set(xlabel='Test F1-Score', ylabel='Pipeline')","8377d352":"X = data[\"preprocessed_content_without_stopwords\"].to_numpy()\ny = data[\"fake_news\"].astype(int).values\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n\nclf = make_clf(\n    feature_extractor=TfidfVectorizer(),\n    use_dense=True,\n    scaler=MaxAbsScaler(),\n    classifier=LogisticRegression(solver=\"liblinear\"),\n)\n\nclf.fit(X_train, y_train)","26404b2b":"explainer = LimeTextExplainer(class_names=[\"Non-Fake News\", \"Fake News\"])","be6ef938":"exp = explainer.explain_instance(X_test[7], clf.predict_proba)\nexp.show_in_notebook(text=True)","0cddca69":"exp = explainer.explain_instance(X_test[5], clf.predict_proba)\nexp.show_in_notebook(text=True)","a275c1de":"After applying PCA on the `preprocessed_content_without_stopwords` column and visualizing the features with a scatter plot, the outputs are shown below. It is noticeable that the feature structures of the two kinds of articles differ significantly.","8812bfab":"## Imports","19d2135b":"Placeholder","641a696a":"Placeholder","c824e35f":"Placeholder","0362c3bc":"Placeholder","4cf71e53":"Furthermore, satirical articles on Kungulli.com tend to be lengthier in terms of word count when compared to non-satirical articles.","7814dd1a":"## Preprocessing\n\nPlaceholder","55b5560f":"Placeholder","d99b1ee7":"## Load data","0a7e86d1":"## Conclusion\n\nPlaceholder","b3587540":"## Results\n\nLets evaluate the results from the experiment.","e04d89c8":"## Utility code","5c9a053e":"### Explaining our predictions\n\nPlaceholder","78909717":"# Fake News Detection and Analysis in Albanian\n\nPlaceholder\n\n## Experiment Setup\n\nThe experiment will be carried out utilizing a Cross-validation (5-fold) experiment setup for internal validation, referred as the training score, and an external testing set for external validation, referred as the testing score, as shown in the Figure below:\n\n![Experiment Setup](https:\/\/i.imgur.com\/E0z2vGN.png)\n\nFor our article contents, we will evaluate a set of classifiers, scaling methods, and three distinct types of preprocessing approaches.\n\nClassifiers used:\n- GaussianNB\n- LinearSVC\n- KNeighborsClassifier\n- RandomForestClassifier\n- LogisticRegression\n- DecisionTreeClassifier\n- MultinomialNB\n\nScaling methods used:\n- Nonetype, alias no scaling at all used\n- MaxAbsScaler\n\nArticle content preprocessing approach:\n- content - Using the article's raw content as found in the original dataset.\n- preprocessed_content - The `preprocess_text` function is used to preprocess the content of the article.\n- preprocessed_content_without_stopwords - Stopwords are also deleted once the content has been preprocessed.\n\n## Getting started\n\nTo begin, we must install the necessary packages.","b16a554e":"## Explorative Data Analysis\n\nPlaceholder","4c7664de":"## Supervised Machine Learning Prediction","488603ce":"Placeholder","62c2c72d":"Placeholder"}}