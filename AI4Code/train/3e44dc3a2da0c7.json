{"cell_type":{"602a8530":"code","982c123a":"code","67ecbf0e":"code","fe0bca5c":"code","52a32d28":"code","09144a43":"code","c9613ca5":"code","f7b42d77":"code","366f18be":"code","5653ef54":"code","9f6dae71":"code","18eba030":"code","51cbe4d1":"code","a77ab2fa":"code","836f2cb2":"code","422aba7c":"code","8ae67e18":"code","9544d7d0":"code","7ee1a7b1":"code","6959bee3":"code","532984d3":"code","7da7b1d2":"code","fd6855c2":"code","1493c01f":"code","9997fdfa":"code","02da3fe2":"code","df7f5575":"code","5375d851":"code","99fd5be1":"code","3941f032":"code","04593e49":"code","eb1191e9":"code","76901103":"code","a2d5d4c9":"code","f03ea76f":"code","6f430402":"markdown","048ffe14":"markdown","455cc5ff":"markdown","18cb4aa2":"markdown","eade7e5d":"markdown","6c890212":"markdown","4121cb81":"markdown","e413de72":"markdown","0322f059":"markdown","3049658a":"markdown","e9dfea1e":"markdown","e19f5eef":"markdown","2bb50755":"markdown","c3794e2a":"markdown","715b5ac9":"markdown","25d1ae0f":"markdown","b223babe":"markdown","20560c6b":"markdown","bc9c51ed":"markdown","8734a852":"markdown","5109278d":"markdown","952f31cf":"markdown"},"source":{"602a8530":"import os\nimport sys\nimport time\nimport glob\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\n# Parallel processing\nfrom joblib import Parallel\nfrom joblib import delayed\n\n# Preprocess\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\n# Evaluation\nfrom sklearn.metrics import r2_score\n\n# Visullize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Modeling\n#import lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\n\n# Others\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","982c123a":"# Dataset path\ndata_path = Path('..\/input\/optiver-realized-volatility-prediction')\n\n# setting display option\npd.options.display.max_columns = 50","67ecbf0e":"# Objective variable\ntarget = 'target'\n\n# submission file setting\nsubmit_file = 'submission.csv'\nId_column = 'row_id'","fe0bca5c":"#\u3000Log Return\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\n# Realized Volatility\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))","52a32d28":"# WAP calculation\ndef wap_calculation1(df):\n    return (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n\ndef wap_calculation2(df):\n    return (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\ndef wap_calculation3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef wap_calculation4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap","09144a43":"# my palams\n# askprice1 - bidprice1\n# askprice2 - bidprice2\n# askprice2 - askprice1\n# bidprice1 - bidprice2\ndef price_ask1_bid1_diff(df):\n    return (df['ask_price1'] - df['bid_price1'])\ndef price_ask2_bid2_diff(df):\n    return (df['ask_price2'] - df['bid_price2'])\ndef price_ask2_bid1_diff(df):\n    return (df['ask_price2'] - df['bid_price1'])\ndef price_ask1_bid2_diff(df):\n    return (df['ask_price1'] - df['bid_price2'])\ndef price_wap1_wap2_diff(df):\n    return (df['wap1'] - df['wap2'])\ndef std_per_mean(df):\n    return np.std(df) \/ np.mean(df)\n","c9613ca5":"# RMSPE\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","f7b42d77":"def book_preprocessing(stock_id : int, data_type = 'train'):\n    # read data\n    df = pd.read_parquet(data_path \/ f'book_{data_type}.parquet\/stock_id={stock_id}\/')\n    \n    # set stock_id\n    df['stock_id'] = stock_id\n    \n    # WAP calculation\n    df['wap1'] = wap_calculation1(df)\n    df['wap2'] = wap_calculation2(df)\n    df['wap3'] = wap_calculation3(df)\n    df['wap4'] = wap_calculation4(df)\n\n    # log return calculation\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return).fillna(0)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return).fillna(0)  \n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return).fillna(0)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return).fillna(0)  \n    \n    # Calculate wap balance\n    df['wap_balance12'] = abs(df['wap1'] - df['wap2'])\n    df['wap_balance34'] = abs(df['wap3'] - df['wap4'])\n    # Calculate spread\n    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n                           \n    # Log_return calculation each stock_id and time_id\n    feat_to_calc_rv = ['log_return1','log_return2','log_return3','log_return4']\n    return_values = pd.DataFrame(\n        df.groupby(\n            ['stock_id','time_id']\n        )[feat_to_calc_rv].agg(realized_volatility)\n    ).reset_index()\n    return_values = return_values.rename(\n        columns={\n            'log_return1': 'realized_volatility1',\n            'log_return2': 'realized_volatility2',\n            'log_return3': 'realized_volatility3',\n            'log_return4': 'realized_volatility4'\n        }\n    )\n\n    # \u96c6\u7d04\u95a2\u6570\u306e\u305f\u3081\u4e0d\u8981\u306a\u5217\u3092\u524a\u9664\n    df = df.drop(['time_id', 'seconds_in_bucket'], axis=1)\n\n#     # skew\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).skew(),\n#         on='stock_id',\n#         suffixes=['', '_skew'],\n#         how='left'\n#     )\n#     # sem\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).sem(),\n#         on='stock_id',\n#         suffixes=['', '_sem'],\n#         how='left'\n#     )    \n#     # kurt\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).apply(pd.Series.kurt).drop('stock_id',axis=1),\n#         on='stock_id',\n#         suffixes=['', '_kurt'],\n#         how='left'\n#     )\n#     # std_per_mean\n#     return_values = return_values.merge(\n#         df.groupby(['stock_id']).agg(std_per_mean),\n#         on='stock_id',\n#         suffixes=['', '_std_per_mean'],\n#         how='left'\n#     )   \n\n    # \u5f8c\u5de5\u7a0b\u3067\u4f7f\u3046\u306e\u3067\u30ea\u30b9\u30c8\u5316\n    features = [\n        'wap1',\n        'wap2',\n        'wap3',\n        'wap4',\n        'ask_price1',\n        'ask_price2',\n        'bid_price1',\n        'bid_price2',\n        'ask_size1',\n        'ask_size2',\n        'bid_size1',\n        'bid_size2',\n        'log_return1',\n        'log_return2',\n        'realized_volatility1',\n        'realized_volatility2',\n        'realized_volatility3',\n        'realized_volatility4',\n        'std_per_mean',\n        'wap_balance12',\n        'wap_balance34',\n        'price_spread1',\n        'price_spread2',\n        'bid_spread',\n        'ask_spread',\n        'bid_ask_spread',\n        'total_volume',\n        'volume_imbalance'\n    ]\n\n    return return_values","366f18be":"df_book = book_preprocessing(97, 'train')\ndf_book","5653ef54":"def trade_preprocessing(stock_id : int, data_type = 'train'):\n    # read data\n    df = pd.read_parquet(data_path \/ f'trade_{data_type}.parquet\/stock_id={stock_id}\/')\n    \n    df = df.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    \n    # set stock_id\n    df['stock_id'] = stock_id\n    \n    # log return calculation\n    df['trade_log_return1'] = df.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    \n    # Log_return calculation each stock_id and time_id\n    df = pd.DataFrame(df.groupby(['stock_id','time_id'])[['trade_log_return1']].agg(realized_volatility).reset_index())\n    \n    # \u305d\u306e\u4ed6\u306e\u30c7\u30fc\u30bf\u3082\u4f7f\u3044\u305f\u3044 size \u3068 order_count\n    \n    return df","9f6dae71":"df_trade = trade_preprocessing(0,'train')\ndf_trade.head()","18eba030":"def get_stock_stat(stock_id : int, data_type = 'train'):\n    \n    # parquet data processing\n    book_stat = book_preprocessing(stock_id, data_type)\n    trade_stat = trade_preprocessing(stock_id, data_type)\n    \n    #Merge book and trade features\n    stock_stat = book_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat","51cbe4d1":"def get_dataSet(stock_ids : list, data_type = 'train'):\n    # Parallel process of get_stock_stat \n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, data_type) \n        for stock_id in stock_ids\n    )\n    # concat several stock_stats in vertical direction, axis=0(default)\n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df","a77ab2fa":"train=pd.read_csv(data_path \/ 'train.csv')\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ndisplay(train.head())\nprint('train data shape:', train.shape)","836f2cb2":"# def miff_max_min(x):\n#     return max(x) - min(x)\n\n# def add_my_param(data):\n\n#     '''\n#     \u30c7\u30fc\u30bf\u3092\u8ffd\u52a0\u3059\u308b\n#     '''\n#     data_ = data.copy()\n#     # target\u306f\u306a\u3044\u5834\u5408\u304c\u3042\u308b\n#     try:\n#         data_ = data_.drop(['target'],axis=1)\n#     except Exception as e:\n#         pass\n\n# #     # \u7279\u5b9a\u306e\u30c7\u30fc\u30bf\u306e\u6700\u5927\u5024\u3092\u8a08\u7b97\n# #     # \u6700\u5927\u5024\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').max(),\n# #         on='stock_id',\n# #         suffixes=['', '_max'],\n# #         how='left'\n# #     )\n\n# #     # \u6700\u5c0f\u5024\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').min(),\n# #         on='stock_id',\n# #         suffixes=['', '_min'],\n# #         how='left'\n# #     )\n\n#     # \u6a19\u6e96\u504f\u5dee\n#     data_ = data_.merge(\n#         data.groupby('stock_id').std(),\n#         on='stock_id',\n#         suffixes=['', '_std'],\n#         how='left'\n#     )\n    \n# #     # \u6700\u5927 - \u6700\u5c0f\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').min() - data.groupby('stock_id').min(),\n# #         on='stock_id',\n# #         suffixes=['', '_diffmaxmin'],\n# #         how='left'\n# #     )\n\n# #     # \u4e2d\u592e\u5024\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').median(),\n# #         on='stock_id',\n# #         suffixes=['', '_median'],\n# #         how='left'\n# #     )\n    \n# #     # \u5e73\u5747\u5024\n# #     data_ = data_.merge(\n# #         data.groupby('stock_id').mean(),\n# #         on='stock_id',\n# #         suffixes=['', '_mean'],\n# #         how='left'\n# #     )\n    \n#     # skew\n#     data_ = data_.merge(\n#         data.groupby('stock_id').skew(),\n#         on='stock_id',\n#         suffixes=['', '_skew'],\n#         how='left'\n#     )\n    \n#     # sem\n#     data_ = data_.merge(\n#         data.groupby('stock_id').sem(),\n#         on='stock_id',\n#         suffixes=['', '_sem'],\n#         how='left'\n#     )    \n#     return data_","422aba7c":"# # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u53d6\u5f97\n# train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), data_type = 'train')\n\n# # \u30d1\u30e9\u30e1\u30fc\u30bf\u8ffd\u52a0\n# # train_stock_stat_df = add_my_param(train_stock_stat_df)\n\n# # Merge train with train_stock_stat_df\n# train = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\n\n# train","8ae67e18":"# pickle\u3092\u4fdd\u5b58\u3001\u8aad\u307f\u8fbc\u3093\u3067\u4f7f\u3046\n# train.to_pickle('train.pkl')\ntrain = pd.read_pickle('..\/input\/from-ver-37\/train.pkl')\ntrain","9544d7d0":"# you need to activate internet connection!\n'''!pip install git+https:\/\/github.com\/aerdem4\/lofo-importance\n\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom lofo import LOFOImportance, Dataset, plot_importance\n%matplotlib inline\n\ntarget=\"target\"\n\nsample_df = train.sample(frac=0.01, random_state=0)\n#sample_df.sort_values(\"AvSigVersion\", inplace=True)\n\n# define the binary target and the features\ncv = KFold(n_splits=4, shuffle=False, random_state=0)\n#target = \"HasDetections\"\nfeatures = [col for col in train.columns if col != target]\n#features = [col for col in train.columns]\n\n\n# define the binary target and the features\ndataset = Dataset(df=sample_df, target=\"target\", features=[col for col in sample_df.columns if col != target])\n\n# get the mean and standard deviation of the importances in pandas format\nlofo = LOFOImportance(dataset, cv=cv, scoring=\"neg_mean_absolute_error\")\nimportance_df = lofo.get_importance()\n\n# plot the means and standard deviations of the importances\nplot_importance(importance_df, figsize=(12, 40))'''","7ee1a7b1":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u3080\ntest = pd.read_csv(data_path \/'test.csv')\n# stock_id\u3068time_id\u3092\u7d44\u307f\u5408\u308f\u305b\u3066\u3001\u7d50\u679c\u3067\u5fc5\u8981\u306b\u306a\u308brow_id\u3092\u4f5c\u308b\ntest['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n# \u78ba\u8a8d\ntest","6959bee3":"# book(\u53d6\u5f15)\u306e\u60c5\u5831\u3092\u5168\u3066\u53d6\u5f97\u3002wap\u3084logreturn\u3082\u8a08\u7b97\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), data_type = 'test')\n\n# min,max,std\u7b49\u8ffd\u52a0\u306e\u7279\u5fb4\u91cf\u8a08\u7b97\u3092\u8a18\u8f09\n# test_stock_stat_df = add_my_param(test_stock_stat_df)\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u63a8\u8ad6\u306b\u4f7f\u3046book\u306e\u60c5\u5831\u3092\u30de\u30fc\u30b8\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\n\n# \u78ba\u8a8d\ntest","532984d3":"# Parameters of Light GBM\n# first try\n# params_lgbm = {\n#         'task': 'train',\n#         'boosting_type': 'gbdt',\n#         'learning_rate': 0.01,\n#         'objective': 'regression',\n#         'metric': 'None',\n#         'max_depth': -1,\n#         'n_jobs': -1,\n#         'feature_fraction': 0.7,\n#         'bagging_fraction': 0.7,\n#         'lambda_l2': 1,\n#         'verbose': -1\n#         #'bagging_freq': 5\n# }","7da7b1d2":"# Define loss function for lightGBM training\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False","fd6855c2":"# training function\ndef light_gbm(X_train, y_train, X_val ,y_val,cats, _pred_name, n_rounds, val_index):\n    \n    print(cats)\n    \n    # Create dataset\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train,2))\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats, weight=1\/np.power(y_val,2))\n    \n    # training\n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 10,\n#                       verbose_eval= 250,\n                     )\n    \n    # Prediction w\/ validation data\n    preds_val = model.predict(train.loc[val_index, features_columns])\n    # train.loc[val_index, _pred_name] = preds_val\n    \n    # RMSPE calculation\n    score = round(rmspe(y_true = y_val, y_pred = preds_val),5)\n\n    # Prediction w\/ validation data\n    test_preds = model.predict(test[features_columns]).clip(0,1e10)\n    \n    # delete dataset\n    del train_data, val_data\n    \n    return score, test_preds, model, preds_val","1493c01f":"# training function\nimport xgboost as xgb\nfrom sklearn import preprocessing\ndef my_xgboost(X_train, y_train, X_val ,y_val,cats, _pred_name, n_rounds, val_index):\n    \n    print(X_train.columns)\n    print(cats)\n    \n    le = preprocessing.LabelEncoder()\n    \n    lookup = {\n        np.int64: 'int',\n        np.float32: 'float',\n        np.float64: 'float',\n        str: 'c'\n    }\n    feature_types = [lookup[type(train.head(1)[t][0])] for t in train.columns]\n    \n    \n    # Create dataset\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_val, label=y_val)\n    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]#\u8a13\u7df4\u30c7\u30fc\u30bf\u306fdtrain\u3001\u8a55\u4fa1\u7528\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306fdvalid\u3068\u8a2d\u5b9a\n    \n    # training\n    model = xgb.train(\n        {\n            'objective': 'reg:squarederror',\n            'silent':1, \n            'random_state':1234, \n            # \u5b66\u7fd2\u7528\u306e\u6307\u6a19 (RMSE)\n            'eval_metric': 'rmse',\n            'max_depth' : '8',\n            'eta' : '0.2'\n        },\n        dtrain,#\u8a13\u7df4\u30c7\u30fc\u30bf\n        n_rounds,#\u8a2d\u5b9a\u3057\u305f\u5b66\u7fd2\u56de\u6570\n        early_stopping_rounds=500,\n        evals=watchlist,\n    )\n    \n    # Prediction w\/ validation data\n    # XGBoost\u306e\u5b66\u7fd2\u3092\u5b9f\u884c\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3067\u4e88\u6e2c\u3001\u8a55\u4fa1\n    dtest = xgb.DMatrix(X_val)\n    preds_val = model.predict(\n        dtest,\n        ntree_limit = model.best_ntree_limit,\n    )\n    #train.loc[val_index, _pred_name] = preds_val\n    \n    # RMSPE calculation\n    score = round(rmspe(y_true = y_val, y_pred = preds_val),5)\n\n    # Prediction w\/ validation data\n    dtest = xgb.DMatrix(test[features_columns])\n    test_preds = model.predict(dtest).clip(0,1e10)\n    # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u30d7\u30ec\u30c7\u30a3\u30af\u30c8\u3092\u304b\u3051\u308b\n    print('pred results:{}'.format(test_preds))\n    \n    # delete dataset\n    del dtrain, dvalid, watchlist, dtest\n    \n    return score, test_preds, model, preds_val","9997fdfa":"test","02da3fe2":"# Categorical data column list\n# cats = []\ncats = ['stock_id']\n\n# \u5b66\u7fd2\u5bfe\u8c61\u7279\u5fb4\u91cf\nfeatures_columns = train.columns.values.tolist()\n\n# drop feat list\n# drop_feat = ['row_id','target','stock_id']\ndrop_feat = ['row_id','target']\nfor i in drop_feat: features_columns.remove(i)\n\nprint(f'Train dataset columns : {len(features_columns)} features')","df7f5575":"\n# 2021\/09\/16 by optuna\n# params_lgbm= {\n#     'objective': 'mean_squared_error',\n#      'metric': 'l1',\n#      'verbosity': -1,\n#      'boosting_type': 'gbdt',\n#      'feature_pre_filter': False,\n#      'lambda_l1': 2.5812494450187865e-05,\n#      'lambda_l2': 0.0005754010268853543,\n#      'num_leaves': 234,\n#      'feature_fraction': 0.42,\n#      'bagging_fraction': 0.9352921232220405,\n#      'bagging_freq': 7,\n#      'min_child_samples': 5,\n#      'num_iterations': 200,\n#      'early_stopping_round': 50\n# }\n\n# # 2021\/09\/17 by optuna more params add_myparam\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 0.0,\n#     'lambda_l2': 0.0,\n#     'num_leaves': 105,\n#     'feature_fraction': 1.0,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 50,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# 2021\/09\/17 by optuna more params add_myparam\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 0.0,\n#     'lambda_l2': 0.0,\n#     'num_leaves': 177,\n#     'feature_fraction': 0.8,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 20,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# 2021\/09\/17 by optuna more params with std_per_mean\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 1.535303758262475e-07,\n#     'lambda_l2': 0.0066570427899383285,\n#     'num_leaves': 256,\n#     'feature_fraction': 0.41600000000000004,\n#     'bagging_fraction': 1.0,\n#     'bagging_freq': 0,\n#     'min_child_samples': 20,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# # Ver24\n# {'objective': 'mean_squared_error',\n#  'metric': 'l1',\n#  'verbosity': -1,\n#  'boosting_type': 'gbdt',\n#  'feature_pre_filter': False,\n#  'lambda_l1': 0.0,\n#  'lambda_l2': 0.0,\n#  'num_leaves': 179,\n#  'feature_fraction': 1.0,\n#  'bagging_fraction': 1.0,\n#  'bagging_freq': 0,\n#  'min_child_samples': 20,\n#  'num_iterations': 200,\n#  'early_stopping_round': 50}\n\n# ver25\n# params_lgbm = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'l1',\n#     'verbosity': -1,\n#     'boosting_type': 'gbdt',\n#     'feature_pre_filter': False,\n#     'lambda_l1': 0.00013301130015106456,\n#     'lambda_l2': 1.083683899969528,\n#     'num_leaves': 203,\n#     'feature_fraction': 0.8999999999999999,\n#     'bagging_fraction': 0.697954364796923,\n#     'bagging_freq': 4,\n#     'min_child_samples': 20,\n#     'num_iterations': 200,\n#     'early_stopping_round': 50\n# }\n\n# ver37\nparams_lgbm = {\n    'objective': 'mean_squared_error',\n    'metric': 'l1',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'feature_pre_filter': False,\n    'lambda_l1': 0.0,\n    'lambda_l2': 0.0,\n    'num_leaves': 19,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 0.6511720136604726,\n    'bagging_freq': 1,\n    'min_child_samples': 20,\n    'num_iterations': 200,\n    'early_stopping_round': 50\n}","5375d851":"'''\noptuna for lightgbm\n'''\n# params = {\n#     'objective': 'mean_squared_error',\n#     'metric': 'mae',\n#     \"verbosity\": -1,\n#     \"boosting_type\": \"gbdt\",\n# }\n\n# best_params, history = {}, []\n\n# lgb_train = lgb.Dataset(train[features_columns], train[target])\n# lgb_eval = lgb.Dataset(test[features_columns], test[target], reference=lgb_train)\n\n# # LightGBM\u5b66\u7fd2\n# gbm = lgb.train(params,\n#                 lgb_train,\n#                 num_boost_round=200,\n#                 valid_sets=[lgb_train, lgb_eval],\n#                 early_stopping_rounds=50\n#                )\n\n# best_params = gbm.params\n# params_lgbm = best_params\n# params_lgbm","99fd5be1":"# '''\n# optuna for xgboost\n# '''\n# import optuna\n# import xgboost as xgb\n\n# def objective_wrap(_train, _labels):\n#     def objective(trial):\n\n#         params = {\n#             \"max_depth\": trial.suggest_int(\"max_depth\", 6, 9),\n#             \"min_child_weight\": 1,\n#             \"eta\": trial.suggest_loguniform(\"eta\", 0.01, 1.0),\n#             \"tree_method\": \"exact\",\n#             \"eval_metric\": \"rmse\",\n#             \"predictor\": \"cpu_predictor\"  \n#         }\n\n#     #     a = train[features_columns]\n#     #     b = train[target].values\n#         dtrain = xgb.DMatrix(_train, _labels)\n\n#         cv_results = xgb.cv(\n#             params,\n#             dtrain,\n#             num_boost_round=1000,\n#             seed=0,\n#             nfold=5, # CV\u306e\u5206\u5272\u6570\n#             metrics={\"rmse\"},\n#             early_stopping_rounds=5\n#         )\n\n#         return cv_results[\"test-rmse-mean\"].min()\n#     return objective\n\n\n# study = optuna.create_study()\n# study.optimize(\n#     objective_wrap(\n#         train[features_columns],\n#         train[target].values\n#     ),\n#     n_trials = 40\n# )","3941f032":"def do_cross_validation_xgb(n_folds, n_rounds, pred_name, reg_alpha=0, reg_lambda=0):\n    \n    scores_folds = []\n    pred_result = []\n\n    # k-flods Ensemble Training\n    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state = 42)\n\n    # Initial value\n    cv_trial = 1\n    \n    # --- Cross Validation ---\n    for train_index, val_index in kf.split(range(len(train))):\n\n        print(f'CV trial : {cv_trial} \/{n_folds}')\n\n        # Divide dataset into train and validation data such as Cross Validation\n        X_train = train.loc[train_index, features_columns]\n        y_train = train.loc[train_index, target].values\n        X_val = train.loc[val_index, features_columns]\n        y_val = train.loc[val_index, target].values\n\n        # train with Light GBM\n        rmspe_score, test_preds, model, preds_val = my_xgboost(X_train, y_train, X_val ,y_val,cats, pred_name, n_rounds, val_index)\n        \n        # record score data at each train in CV\n        scores_folds.append(rmspe_score)\n        pred_result.append(test_preds)\n\n        # Each validation Summary \n        print(f'Fold-{cv_trial} train score. Model-{pred_name} RMSPE: {rmspe_score}')\n        \n        # Prediction w\/ test data\n        dtest = xgb.DMatrix(test[features_columns])\n        test_preds = model.predict(dtest).clip(0,1e10)\n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u30d7\u30ec\u30c7\u30a3\u30af\u30c8\u3092\u304b\u3051\u308b\n        print('test pred results:{}'.format(test_preds))\n        \n        # cv trial \u56de\u6570\u3092\u30a4\u30f3\u30af\u30ea\u30e1\u30f3\u30c8\n        cv_trial += 1\n\n    return pred_result, scores_folds, test_preds","04593e49":"def do_cross_validation_lightgbm(n_folds, n_rounds, pred_name, reg_alpha=0, reg_lambda=0):\n\n    # k-flods Ensemble Training\n    kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state = 42)\n\n    # Initialize scores list\n    scores_folds = []\n    pred_result = []\n\n    # Initial value\n    cv_trial = 1\n\n    params_lgbm['reg_alpha'] = reg_alpha\n    params_lgbm['reg_lambda'] = reg_lambda\n\n    # --- Cross Validation ---\n    for train_index, val_index in kf.split(range(len(train))):\n\n        print(f'CV trial : {cv_trial} \/{n_folds}')\n\n        # Divide dataset into train and validation data such as Cross Validation\n        X_train = train.loc[train_index, features_columns]\n        y_train = train.loc[train_index, target].values\n        X_val = train.loc[val_index, features_columns]\n        y_val = train.loc[val_index, target].values\n\n        # train with Light GBM\n        rmspe_score, test_preds, model, preds_val = light_gbm(X_train, y_train, X_val ,y_val,cats, pred_name, n_rounds, val_index)\n        \n         # record score data at each train in CV\n        scores_folds.append(rmspe_score)\n        pred_result.append(test_preds)\n  \n        # Each validation Summary \n        print(f'Fold-{cv_trial} train score. Model-{pred_name} RMSPE: {rmspe_score}')\n\n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u30d7\u30ec\u30c7\u30a3\u30af\u30c8\u3092\u304b\u3051\u308b\n        test_preds = model.predict(test[features_columns]).clip(0,1e10)\n        print('test pred results:{}'.format(test_preds))\n        \n        # \u30a4\u30f3\u30dd\u30fc\u30bf\u30f3\u30b9\u51fa\u529b\n        train_index = train.head(0)\n        train_index = train_index.drop(drop_feat, axis=1)\n        importance = model.feature_importance()\n        print(importance)\n        print(train_index.columns)\n        display(pd.DataFrame(importance, index=train_index.columns, columns=['importance']))\n\n        # cv trial \u56de\u6570\u3092\u30a4\u30f3\u30af\u30ea\u30e1\u30f3\u30c8\n        cv_trial += 1\n      \n    return pred_result, scores_folds, test_preds","eb1191e9":"pd.set_option(\"max_rows\", None) # \u5168\u3066\u898b\u305f\u3044\n\n# results\n# for xgboost\nscores_folds = {}\ntrain_pred_result = {}\ntest_pred_result = {}\nmodel_name_xgb1 = 'xgb1'\npred_name_xgb1 = f'pred_{model_name_xgb1}'\nscores_folds[model_name_xgb1]=[]\ntrain_pred_result[model_name_xgb1]=[]\ntest_pred_result[model_name_xgb1]=[]\n\n\nmodel_name_lgb1 = 'lgb1'\npred_name_lgb1 = f'pred_{model_name_lgb1}'\nscores_folds[model_name_lgb1]=[]\ntrain_pred_result[model_name_lgb1]=[]\ntest_pred_result[model_name_lgb1]=[]\n\n# \u904e\u5b66\u7fd2\u6291\u5236\u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\u30eb\u30fc\u30d7\nreg_alphas =  [0.1]\nreg_lambdas = [0]\ncv_count = 4\nn_rounds = 10000\nfor reg_alpha in reg_alphas:\n    for reg_lambda in reg_lambdas:\n        # \u4ea4\u5dee\u691c\u5b9a\n        # xgb\n        pr, sf, tr = do_cross_validation_xgb(cv_count, n_rounds, pred_name_xgb1, reg_alpha, reg_lambda)\n        scores_folds[model_name_xgb1].append(sf)\n        train_pred_result[model_name_xgb1].append(pr)\n        test_pred_result[model_name_xgb1].append(tr)\n\n        # lgb\n        pr, sf, tr = do_cross_validation_lightgbm(4, n_rounds, pred_name_lgb1, reg_alpha, reg_lambda)\n        scores_folds[model_name_lgb1].append(sf)\n        train_pred_result[model_name_lgb1].append(pr)\n        test_pred_result[model_name_lgb1].append(tr)\n\npd.options.display.max_columns = 50 # \u8868\u793a\u306e\u6291\u5236\u3092\u521d\u671f\u8a2d\u5b9a\u306b\u623b\u3059","76901103":"# \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3059\u308b\n\n# \u3059\u3079\u3066\u306e\u7d50\u679c\u3092\u51fa\u529b\nprint('='*8+'\\n'+'print all result')\nprint(train_pred_result)\nprint(scores_folds)\n\n# \u63a8\u5b9a\u5024\u3068RMSPE\u306e\u30b9\u30b3\u30a2\u306e\u5e73\u5747\u5024\u3092\u8a08\u7b97\ntrain_last_result = 0\ntest_last_result = 0\nlast_score = 0\ncount = 0\nfor pr in train_pred_result:\n    train_last_result += pd.DataFrame(train_pred_result[pr][0]).mean()\n    test_last_result += pd.DataFrame(test_pred_result[pr][0])\n    last_score += pd.Series(scores_folds[pr][0]).mean()\n    count += 1\ntrain_last_result \/= count\ntest_last_result \/= count\nlast_score \/= count\n\n# \u8868\u793a\nprint('='*8+'\\n'+'print mean result')\nprint('train last result:{}'.format(train_last_result))\nprint('test last result:{}'.format(test_last_result))\nprint('last score:{}'.format(last_score))\n\n# \u7d50\u679c\u3092\u4ee3\u5165\u3059\u308b\ntest[target]=test_last_result\n\ndisplay(test[[Id_column, target]].head(2))","a2d5d4c9":"test","f03ea76f":"test[[Id_column,target]].to_csv(submit_file, index = False)","6f430402":"Check data content of one sample with book_preprocessing function  \ne.g. stock_id = 97","048ffe14":"## 5-2. Cross Validation","455cc5ff":"This notebook shows simple flow to deep dive into the competition.I appreciate community of kaggle.\nI refered to following notebooks.\n\n(Reference)  \n**Introduction to financial concepts and data**  \nhttps:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data  \n\n**LGB Starter**  \nhttps:\/\/www.kaggle.com\/manels\/lgb-starter\/notebook","18cb4aa2":"## 5-1. Training function1 - Light GBM  ","eade7e5d":"# Agenda\n\n1. Import modules  \n2. Common settings\n3. Function Definition\n4. Preprocessing  \n  4-1. Book parquet data processing  \n  4-2. Trade parquet data processing  \n  4-3. Merge book and trade data  \n  4-4. Train data preprocessing  \n  4-5. Test data preprocessing  \n5. Training  \n  5-1. Training function1 - Light GBM  \n  5-2. Cross Validation  \n6. Evaluation\n7. Prediction  \n8. Submission\n","6c890212":"# 3. Functions Definition  ","4121cb81":"# **4-4-1.LOFO importance**","e413de72":"## 5-1-2\n### optuna","0322f059":"# 4. Preprocessing dataset","3049658a":"## 4-5. Test data Preprocessing","e9dfea1e":"Following function is training with Light GBM function.If you would like to try any other function, you could define another function and call it.","e19f5eef":"Check data content of one sample with trade_preprocessing function\ne.g. stock_id = 0","2bb50755":"## 5-1-1\n### prepare data to train","c3794e2a":"## 4-3. Merge book and trade data  \nMerge two data created by preprocessed with book_preprocessing and trade_preprocessing function","715b5ac9":"# 5.Training","25d1ae0f":"## 4-4. Train data preprocessing","b223babe":"## 4-2. Trade parquet data processing","20560c6b":"## 4-1. Book parquet data processing","bc9c51ed":"# 6. Evaluation","8734a852":"# 1. Import modules","5109278d":"# 2. Common Settings","952f31cf":"# 7. Submittion"}}