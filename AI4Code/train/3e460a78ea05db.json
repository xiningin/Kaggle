{"cell_type":{"c03bf605":"code","3abefde8":"code","a7fb1d74":"code","f1d8187c":"code","ea4cf34f":"code","fac9b1d3":"code","fc4e416c":"code","2e913aa8":"code","8d887a1a":"code","314e7cd8":"code","f0199901":"code","d61329e3":"code","453ba0ea":"code","7efde790":"code","cf60d302":"code","96f2ae87":"code","8cd67c2d":"code","9e56722d":"code","123a7743":"code","d1b7c752":"code","7a74b08b":"code","008af284":"code","859933c8":"markdown","e983b9d7":"markdown","de12a93d":"markdown","5e218d73":"markdown","eee90b2f":"markdown","3f13ed71":"markdown","0e811a3e":"markdown","7c16430d":"markdown","77fb9c28":"markdown","979a82b8":"markdown","346f14a0":"markdown","42e1c8bc":"markdown","3646f543":"markdown","f975f889":"markdown","4ee3adbb":"markdown","06052571":"markdown","071b5359":"markdown","82ec65e5":"markdown","50dedaae":"markdown","994c54d4":"markdown","70423cb9":"markdown","a43e2b85":"markdown","d50db5f9":"markdown"},"source":{"c03bf605":"import pandas as pd\nimport numpy as np\n\npd.set_option('display.width', 100)\npd.set_option('precision', 2)\n\ndf = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\").drop(\"customerID\", axis=1)","3abefde8":"print(df.head(10))","a7fb1d74":"print(df.shape)\nprint(df.info())","f1d8187c":"print(df.groupby('Churn').size())","ea4cf34f":"cols =  pd.DataFrame({col_name : sum([int(str(elem).isspace() == True) \\\n                                 for elem in col]) for col_name, col in df.iteritems()}, index=[0])\ncols = cols.rename(index={cols.index[0]: 'blank rows'})\nprint(cols)","fac9b1d3":"# See null values before and after converstion of whitespace-only chars to np.nan\nprint(df.isnull().values.sum())\ndf.replace(r'^\\s+$', np.nan, regex=True, inplace=True)\nprint(df.isnull().values.sum())","fc4e416c":"df['TotalCharges'] = pd.to_numeric(df.TotalCharges)\nprint(df.describe())","2e913aa8":"null_rows = df[df['TotalCharges'].isnull()]\nprint(null_rows[['MonthlyCharges', 'tenure', 'TotalCharges', 'Churn']])","8d887a1a":"# Look at the shape before and after to be sure they were removed\nprint(df.shape)\ndf = df[df['TotalCharges'].notnull()]\nprint(df.shape)","314e7cd8":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor col in df:\n    if df[col].dtype == 'object':\n        if df[col].nunique() == 2:\n            df[col] = le.fit_transform(df[col])\n        elif df[col].nunique() > 2:\n            df[col] = df[col].astype('category').cat.codes","f0199901":"df_maj = df[df['Churn'] == 0]\ndf_min = df[df['Churn'] == 1]\nfrom sklearn.utils import resample\ndf_min_ups = resample(df_min, replace=True, n_samples=5163)\nprint(df_min.shape)\ndf_ups = pd.concat([df_min_ups, df_maj])\nprint(df_ups.Churn.value_counts())","d61329e3":"X = df_ups.iloc[:, :-1].values\ny = df_ups.iloc[:,-1].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","453ba0ea":"from sklearn import metrics\n\ndef print_metrics(y_pred, y_test):\n    '''\n    Prints accuracy, confusion matrix, and classification report for a given set\n    of true and predicted values.\n    '''\n    print(metrics.accuracy_score(y_test, y_pred))\n    print(metrics.confusion_matrix(y_test, y_pred))\n    print(metrics.classification_report(y_test, y_pred))","7efde790":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\n\ndt_clf = DecisionTreeClassifier(max_depth=3)\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\nprint_metrics(dt_pred, y_test)","cf60d302":"%matplotlib inline\nimport pydotplus\nimport matplotlib.pyplot as plt\nimport matplotlib.image as img\n\ncol_names = df.columns.values.tolist()[:-1]\ndot_data = export_graphviz(dt_clf,\n                                feature_names=col_names,\n                                out_file=None,\n                                filled=True,\n                                rounded=True)\n\ngraph = pydotplus.graphviz.graph_from_dot_data(dot_data)\ngraph.write_png('telco_tree.png')\nplt.figure(figsize=(20,20))\nplt.imshow(img.imread(fname='telco_tree.png'))\nplt.show()","96f2ae87":"import plotly.figure_factory as ff\n\ndata = []\nfor i in range(3):\n    trace = {\n        \"type\": 'violin',\n        \"x\": df['Contract'][df['Churn'] == i],\n        \"y\": df['MonthlyCharges'][df['Churn'] == i],\n        \"name\": 'Churn' if i == 1 else 'No Churn',\n        \"box\": {\n            \"visible\": True\n        },\n        \"meanline\": {\n            \"visible\": True\n        }\n    }\n    data.append(trace)\n\nfig1 = {\n    \"data\": data,\n    \"layout\": {\n        \"title\": \"Violin Plot of Contract vs Monthly Charges\",\n        \"yaxis\": {\n            \"zeroline\": False,\n            \"title\": \"Monthly Charges ($)\",\n        },\n        \"xaxis\": {\n            \"title\": \"Contract\",\n            \"tickvals\": [0, 1, 2],\n            \"ticktext\": ['Month-to-month', 'One-year', 'Two-year']\n        }\n    }\n}\n\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\niplot(fig1, filename='violin_contract_monchar')","8cd67c2d":"fig2 = ff.create_facet_grid(\n    df,\n    x='tenure',\n    y='MonthlyCharges',\n    facet_col='Churn',\n)\n\niplot(fig2, filename='scatter_tenure_monchar')","9e56722d":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nXs_train, Xs_test, ys_train, ys_test = train_test_split(X_scaled, y, test_size=0.25)","123a7743":"from sklearn.model_selection import GridSearchCV\n\nclass ClassifierEvaluator:\n    '''\n    Wrapper class for applying GridSearchCV to multiple classifiers,\n    each with their own set of parameters to search through and evaluate.\n    '''\n    def __init__(self, classifiers, params):\n        self.classifiers = classifiers\n        self.params = params\n        self.grid_search_results = {}\n\n    def fit(self, X, y, cv=5, n_jobs=5, scoring=None, refit=True, return_train_score=False):\n        for clf in self.classifiers.keys():\n            print(\"Running grid search for %s.\" % clf)\n            model = self.classifiers[clf]\n            params = self.params[clf]\n            grid_search = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs, scoring=scoring,\n                                       refit=refit, return_train_score=return_train_score)\n            grid_search.fit(X, y)\n            self.grid_search_results[clf] = grid_search\n        print(\"Grid search complete.\")\n\n    def gs_cv_results(self):\n        def iter_results(clf_name, scores, params):\n            stat_dict = {\n                'classifier' : clf_name,\n                'mean_score': np.mean(scores),\n                'min_score': min(scores),\n                'max_score': max(scores),\n                'std_score': np.std(scores),\n            }\n            return pd.Series({**params, **stat_dict})\n\n        rows = []\n        for clf_name, gs_run in self.grid_search_results.items():\n            params = gs_run.cv_results_['params']\n            scores = []\n            for i in range(gs_run.cv):\n                key = \"split{}_test_score\".format(i)\n                result = gs_run.cv_results_[key]\n                scores.append(result.reshape(len(params),1))\n\n            all_scores = np.hstack(scores)\n            for param, score in zip(params, all_scores):\n                rows.append((iter_results(clf_name, score, param)))\n\n        df = pd.concat(rows, axis=1, sort=False).T.sort_values(['mean_score'], ascending=False)\n\n        columns = ['classifier', 'mean_score', 'min_score', 'max_score', 'std_score']\n        columns = columns + [c for c in df.columns if c not in columns]\n\n        return df[columns]","d1b7c752":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nmodels = {\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'RandomForestClassifier' : RandomForestClassifier(),\n    'LogisticRegression': LogisticRegression(),\n}\n\nparams = {\n    'KNeighborsClassifier': {'n_neighbors': [1, 3, 5, 10]},\n    'RandomForestClassifier': {\n                  'n_estimators': [100, 200, 400],\n                  'max_depth': [None, 3, 5, 10],\n                  'min_samples_split': [2, 5, 10],\n                  'max_features': ['sqrt']\n    },\n    'LogisticRegression': {'C': [1, 10, 100]}\n}\n\nevaluator = ClassifierEvaluator(models, params)\nevaluator.fit(Xs_train, ys_train)\nresults = evaluator.gs_cv_results()","7a74b08b":"import plotly.offline as py\nimport plotly.graph_objs as go\n\ntrace = go.Table(\n    header=dict(values=['classifier', 'mean_score', 'C', 'max_depth', 'max_features',\n                        'min_samples_split', 'n_estimators', 'n_neighbors'],\n                fill = dict(color='#C2D4FF'),\n                align = ['left'] * 5),\n    cells=dict(values=[results.classifier, results.mean_score,\n                       results.C, results.max_depth, results.max_features, results.min_samples_split,\n                       results.n_estimators, results.n_neighbors],\n               fill = dict(color='#F5F8FF'),\n               align = ['left'] * 5))\n\ndata = [trace]\niplot(data, filename = 'telco_gridsearchcv_results')","008af284":"rf = RandomForestClassifier(max_depth=None, max_features='sqrt', min_samples_split=2, n_estimators=100)\nrf.fit(Xs_train, ys_train)\ny_pred = rf.predict(Xs_test)\nprint_metrics(y_pred, ys_test)","859933c8":"## Summary data visualizations\n\n### Violinplot for continuous vs discrete\nA good plot for visualizing the relationship between a continuous and discrete variable is the violinplot. It combines the summary statistics of a boxplot with a kernel density estimation to give a better sense of where the most data points are concentrated. This help us to see which combinations of features might be important for distinguishing customers who churn from those who don't. ","e983b9d7":"## Classification using machine learning\n\n### Standardizing the data\nBefore we can use this data with classifiers such as logistic regression and k-nearest neighbors, we need to standardize it to ensure that variables on a larger scale such as TotalCharges do not dominate the feature space. We do this using the `StandardScaler()` from sklearn.","de12a93d":"We can also take a look at the null rows to get an idea of why TotalCharges might be missing, and therefore get a sense of what to do with the data (impute or discard). ","5e218d73":"Another thing to look at is the relative proportion of positive versus negative samples with respect to the target variable. As we can see, we have almost a 3:1 ratio of No to Yes, so our dataset is fairly imbalanced. We'll correct for this later on by upsampling the positive examples using bootstrap resampling.","eee90b2f":"### Important feature exploration using decision trees\nAt this point we can run a decision tree classifier to get a first sense of which attributes might be important, and what sort of accuracy we can expect.","3f13ed71":"### Scatterplot for continuous vs continuous\nWe can use a scatterplot to visualize the relative distribution of two continuous variables. This can be further faceted by the target variable to give us an idea of which data ranges best correspond to churning customers. We can see that customers who churn had a shorter tenure and higher monthly charges. It makes sense intuitively that customers who left had a shorter tenure due to leaving, but interestingly those who stayed with the company longer had a higher monthly bill.","0e811a3e":"## Data Preprocessing\n\n### Identify missing values\nTo get a sense of why TotalCharges is of type `object` despite containing `float64` values, we can iterate over all the columns of the dataframe and a summary of the number of whitespace-only elements in each column.","7c16430d":"With the class defined, we can now import the classifiers we need and create the `models` and `params` dictionaries we'll need for our GridSearchCV. Then we can use the class we've created to perform the grid search using the `fit()` method, then get the results using `gs_cv_results()`.","77fb9c28":"As we can see, there are 11 white-space only (i.e. blank) rows in the TotalCharges column, which explains why pandas treats it as type `object` instead of `float64.` We can replace those with np.nan so that `isnull()` correctly identifies those entries as being null.","979a82b8":"We can visualize the resulting tree to get a sense of which are the optimal splits, and therefore which features seem to be essential to good separation of the classes.","346f14a0":"### Displaying the GridSearchCV results using plotly\nThe `ClassifierEvaluator` class has a method called `gs_cv_results()` that returns a dataframe with a summary of each iteration of the grid search, as well as the mean score across the 5-fold cross-validation. The higher the score, the better the classifier performs in terms of accuracy. The `scoring` parameter can be modified to use different scoring metrics instead of the default.\n\nWe can view the resulting dataframe by using plotly to create a table summarizing the results. From the table we can see that the `RandomForestClassifier` seems to perform best.","42e1c8bc":"### Testing performance on the holdout set\nAs a final step, we can create a `RandomForestClassifier()` with the optimal parameters we have selected and then test it out on the holdout test set to see if we still get a good performance. This is to try to counteract overfitting that so often occurs with all classifiers.","3646f543":"## Feature selection using decision trees\n\n### Splitting the dataset\nNow we split the data into features (X) and target (y), as well as training and test sets for downstream analysis.","f975f889":"### Upsampling the positive \"Churn\" instances to even out the target distribution\nTo even out the target distribution and avoid falling victim to the accuracy paradox, we use bootstrap resampling to upsample the minority class to get an even target distribution between \"Churn\" and \"No churn\" groups. To do this, we split the dataset according to the target and then resample the minority class to be the same size as the majority class, then combine the two to form a new, balanced dataset.","4ee3adbb":"We start off by looking at the first 20 data points to get a sense of what the data looks like.","06052571":"Now we can safely convert TotalCharges to `float64` so our models will correctly treat it as a continuous variable instead of a categorical one. Afterwards, we can run `describe()` on the dataframe to see some summary statistics of the numerical columns. Note that SeniorCitizen is still considered numeric since we have not addressed it yet.","071b5359":"We can also write a function to print out performance metrics for the different classifiers we'll try out to save us from repeating code later on with every classifier.","82ec65e5":"In particular if we look at MonthlyCharges, tenure, TotalCharges, and Churn, we can see that all of these individuals had a tenure of 0, meaning they have not yet been with the company for a whole month, and therefore have not yet been charged any  money. This explains why TotalCharges is missing, and also why none of them have churned yet. At this point we can safely remove these from our model.","50dedaae":"### Encoding categorical variables\nNow we can move on to encoding the categorical variables for use with machine learning techniques, which require that variables be converted to a from that can be mapped in feature space for techniques like kNN and logistic regression.","994c54d4":"We also look at the dimensions and data types of the columns. We can see that there are 7043 customers, as well as 20 different variables that define them. Our target variable is \"Churn\" and right off the bat we can see that SeniorCitizen is of a numerical type despite being a categorical variable (yes\/no). \n\nAlso strange is the fact that TotalCharges is of type \"object\" when it should be \"float64\". We can also see that there are no null objects in any of the columns.","70423cb9":"### Defining a GridSearchCV wrapper class\nIn order to compare different classifiers we can use 5-fold cross-validation in combination with a grid search over several different parameters to simultaneously tune and evaluate several classifiers to get a sense of which one might perform best for this problem, and what performance we can expect. \n\nOne way to do this is to define a wrapper class that takes in a dictionary of classifiers and a dictionary of parameters, with a `fit()` method that does a grid search for each classifier in turn.","a43e2b85":"# Telco Churn Dataset\n\n## Data preview\n\n### Loading in the dataset\n\nThis dataset represents a collection of telecom company customers and whether or not they left the company's services (Churn). It contains information about their account such as how long they've been with the company (tenure), the type of contract they have (Month-to-month, One-year, Two-year), and the amount of money they pay every month on their bill (MonthlyCharges).\n\nHere we load in the dataset from a CSV file and drop the customerID column, since it's irrelevant to our analysis and unlikely to have any relationship to whether a customer churns or not.","d50db5f9":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-preview\" data-toc-modified-id=\"Data-preview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Data preview<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Loading-in-the-dataset\" data-toc-modified-id=\"Loading-in-the-dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Loading in the dataset<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Identify-missing-values\" data-toc-modified-id=\"Identify-missing-values-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Identify missing values<\/a><\/span><\/li><li><span><a href=\"#Encoding-categorical-variables\" data-toc-modified-id=\"Encoding-categorical-variables-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Encoding categorical variables<\/a><\/span><\/li><li><span><a href=\"#Upsampling-the-positive-&quot;Churn&quot;-instances-to-even-out-the-target-distribution\" data-toc-modified-id=\"Upsampling-the-positive-&quot;Churn&quot;-instances-to-even-out-the-target-distribution-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Upsampling the positive \"Churn\" instances to even out the target distribution<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-selection-using-decision-trees\" data-toc-modified-id=\"Feature-selection-using-decision-trees-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Feature selection using decision trees<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Splitting-the-dataset\" data-toc-modified-id=\"Splitting-the-dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Splitting the dataset<\/a><\/span><\/li><li><span><a href=\"#Important-feature-exploration-using-decision-trees\" data-toc-modified-id=\"Important-feature-exploration-using-decision-trees-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Important feature exploration using decision trees<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Summary-data-visualizations\" data-toc-modified-id=\"Summary-data-visualizations-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Summary data visualizations<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Violinplot-for-continuous-vs-discrete\" data-toc-modified-id=\"Violinplot-for-continuous-vs-discrete-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Violinplot for continuous vs discrete<\/a><\/span><\/li><li><span><a href=\"#Scatterplot-for-continuous-vs-continuous\" data-toc-modified-id=\"Scatterplot-for-continuous-vs-continuous-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Scatterplot for continuous vs continuous<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Classification-using-machine-learning\" data-toc-modified-id=\"Classification-using-machine-learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Classification using machine learning<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Standardizing-the-data\" data-toc-modified-id=\"Standardizing-the-data-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Standardizing the data<\/a><\/span><\/li><li><span><a href=\"#Defining-a-GridSearchCV-wrapper-class\" data-toc-modified-id=\"Defining-a-GridSearchCV-wrapper-class-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Defining a GridSearchCV wrapper class<\/a><\/span><\/li><li><span><a href=\"#Displaying-the-GridSearchCV-results-using-plotly\" data-toc-modified-id=\"Displaying-the-GridSearchCV-results-using-plotly-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Displaying the GridSearchCV results using plotly<\/a><\/span><\/li><li><span><a href=\"#Testing-performance-on-the-holdout-set\" data-toc-modified-id=\"Testing-performance-on-the-holdout-set-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Testing performance on the holdout set<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>"}}