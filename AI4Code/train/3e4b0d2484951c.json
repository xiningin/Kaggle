{"cell_type":{"574ed23c":"code","cc1df56f":"code","e5a1e6ca":"code","696761e9":"code","d5221121":"code","7e3e8759":"code","560591a9":"code","29b3002b":"code","955c3594":"code","e4dc6e22":"code","cedb9563":"code","a2728eac":"code","be3984a4":"code","dc683268":"code","805ecbed":"code","69c0ae0c":"code","4514066a":"code","29dda17e":"code","b02db3e2":"code","29cdd035":"code","ef1d87bd":"code","57d8750b":"code","10eb5db4":"code","e54e3f6f":"code","749c4db4":"code","b7f6afe5":"code","6ce145c9":"code","6605e89d":"code","3a01c6e6":"code","1914a010":"code","49f9bbbe":"code","c8c06c43":"code","8f1ecfaf":"code","d25ba582":"code","6fc61372":"code","781d28f2":"code","52633c4d":"code","06e75487":"code","bd928b96":"code","0280b7c9":"code","eb0db64b":"code","6f7c0311":"code","8edbe568":"code","bd9e25b0":"markdown","bd7f97ce":"markdown","92337f8a":"markdown","68e527da":"markdown","8caddfb6":"markdown","5816f7ab":"markdown","4e17a520":"markdown","cc7ed18b":"markdown","56217793":"markdown","c4d8d0fb":"markdown","cd81abab":"markdown","1a1fb6c4":"markdown","8ce29216":"markdown","56ccbec1":"markdown","071f4104":"markdown","fbb7d050":"markdown","c6615404":"markdown","09b894e5":"markdown"},"source":{"574ed23c":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","cc1df56f":"from learntools.core import binder\nbinder.bind(globals())\nfrom learntools.game_ai.ex4 import *","e5a1e6ca":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n!pip install 'tensorflow==1.15.0'\n\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate, utils\nfrom gym import spaces\n\n!apt-get update\n!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n!pip install \"stable-baselines[mpi]==2.9.0\"\n\nfrom stable_baselines.bench import Monitor \nfrom stable_baselines.common.vec_env import DummyVecEnv\nfrom stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\nfrom stable_baselines.a2c.utils import conv, linear, conv_to_fc\nfrom stable_baselines.common.policies import CnnPolicy","696761e9":"class ConnectFourGym:\n    def __init__(self, agent2=\"random\"):\n        ks_env = make(\"connectx\", debug=True)\n        self.env = ks_env.train([None, agent2])\n        self.rows = ks_env.configuration.rows\n        self.columns = ks_env.configuration.columns\n        # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=2, \n                                            shape=(self.rows,self.columns,1), dtype=np.int)\n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n    def reset(self):\n        self.obs = self.env.reset()\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n    def change_reward(self, old_reward, done):\n        if old_reward == 1: # The agent won the game\n            return 1\n        elif done: # The opponent won the game\n            return -1\n        else: # Reward 1\/42\n            return 1\/(self.rows*self.columns)\n    def step(self, action):\n        # Check if agent's move is valid\n        is_valid = (self.obs['board'][int(action)] == 0)\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(int(action))\n            reward = self.change_reward(old_reward, done)\n        else: # End the game and penalize agent\n            reward, done, _ = -10, True, {}\n        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _","d5221121":"# Create ConnectFour environment\nenv = ConnectFourGym(agent2=\"negamax\")\n\n# Create directory for logging training information\nlog_dir = \"log\/\"\nos.makedirs(log_dir, exist_ok=True)\n\n# Logging progress\nmonitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n# Create a vectorized environment\nvec_env = DummyVecEnv([lambda: monitor_env])\n\n# Neural network for predicting action values\ndef modified_cnn(scaled_images, **kwargs):\n    activ = tf.nn.relu\n    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                         init_scale=np.sqrt(2), **kwargs))\n    layer_2 = conv_to_fc(layer_2)\n    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\nclass CustomCnnPolicy(CnnPolicy):\n    def __init__(self, *args, **kwargs):\n        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)","7e3e8759":"# Initialize agent\nmodel = PPO1(CustomCnnPolicy, vec_env, verbose=0, \n             timesteps_per_actorbatch=256,\n             clip_param=0.1,\n             optim_stepsize=0.01,\n             optim_epochs=3,\n#              optim_batchsize=256,\n             adam_epsilon=2.5e-04, \n             n_cpu_tf_sess=None)\n# optim_stepsize=0.001, adam_epsilon=1e-04, timesteps_per_actorbatch=256 => Agent1 0.11, Agent2 0.89\n\nimport time\nstart_time = time.time()\n\n# Train agent\nmodel.learn(total_timesteps=150000)\n\nprint(\"--- %s minutes ---\" % ((time.time() - start_time)\/60))\n\n# Plot cumulative reward\nwith open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n    firstline = fh.readline()\n    assert firstline[0] == '#'\n    df = pd.read_csv(fh, index_col=None)['r']\ndf.rolling(window=1000).mean().plot()\nplt.show()","560591a9":"#os.makedirs(\"trained_1\", exist_ok=True)\n\nmodel.save(\"trained_1\")","29b3002b":"del model # remove to demonstrate saving and loading","955c3594":"def agent1(obs, config):  \n    model = PPO1.load('trained_1')\n#     model = PPO1.load('try1')\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","e4dc6e22":"# Create the game environment\nenv = make(\"connectx\")\n\n# agent1 playing one game round vs another agent (try \"random\" or \"negamax\")\nenv.run([agent1, \"negamax\"])\n\n# Show the game\nenv.render(mode=\"ipython\", width=500, height=450)","cedb9563":"# Play as the first agent against default \"random\" agent.\nenv.run([agent1, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","a2728eac":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = agent1(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\nenv.render(mode=\"ipython\", width=200, height=180, header=False, controls=False)\n#env.render()","be3984a4":"def get_win_percentages(agent1, agent2=\"random\", n_rounds=10):\n    # Use default Connect Four setup\n    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n    # Agent 1 goes first (roughly) half the time          \n    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds\/\/2)\n    # Agent 2 goes first (roughly) half the time      \n    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds\/\/2)]\n    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])\/len(outcomes), 2))\n    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])\/len(outcomes), 2))\n    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))","dc683268":"get_win_percentages(agent1=agent1, agent2=\"random\")","805ecbed":"# \"None\" represents which agent you'll manually play as (first or second player).\nenv.play([None, \"negamax\"], width=500, height=450)","69c0ae0c":"def agent1(obs, config):  \n    \n    import numpy as np\n    from stable_baselines import PPO1\n     \n\n    model = PPO1.load('trained_1')\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])","4514066a":"import inspect\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(agent1, \"submission.py\")","29dda17e":"from kaggle_environments import utils, agent\n\n# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\nagent = agent.get_last_callable(submission)\nsys.stdout = out\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\n\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","b02db3e2":"with open(\".\/trained_1.zip\", 'rb') as f:\n     trained_1 = f.read()\n\ntrained_1[:1000]","29cdd035":"bin_data = trained_1 #Whatever binary data you have store in a variable\nbinary_file_path = 'try1.zip' #Name for new zip file you want to regenerate\nwith open(binary_file_path, 'wb') as f:\n    f.write(bin_data)","ef1d87bd":"# Create the agent\nmy_agent = '''def agent1(obs, config):  \n        \n    from learntools.core import binder\n    binder.bind(globals())\n    \n    import os\n    import random\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n\n    import tensorflow as tf\n    from kaggle_environments import make, evaluate, utils\n    from gym import spaces\n\n    from stable_baselines.bench import Monitor \n    from stable_baselines.common.vec_env import DummyVecEnv\n    from stable_baselines import PPO1, A2C, ACER, ACKTR, TRPO\n    from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n    from stable_baselines.common.policies import CnnPolicy\n    \n    class ConnectFourGym:\n        def __init__(self, agent2=\"random\"):\n            ks_env = make(\"connectx\", debug=True)\n            self.env = ks_env.train([None, agent2])\n            self.rows = ks_env.configuration.rows\n            self.columns = ks_env.configuration.columns\n            # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n            self.action_space = spaces.Discrete(self.columns)\n            self.observation_space = spaces.Box(low=0, high=2, \n                                                shape=(self.rows,self.columns,1), dtype=np.int)\n            # Tuple corresponding to the min and max possible rewards\n            self.reward_range = (-10, 1)\n            # StableBaselines throws error if these are not defined\n            self.spec = None\n            self.metadata = None\n        def reset(self):\n            self.obs = self.env.reset()\n            return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n        def change_reward(self, old_reward, done):\n            if old_reward == 1: # The agent won the game\n                return 1\n            elif done: # The opponent won the game\n                return -1\n            else: # Reward 1\/42\n                return 1\/(self.rows*self.columns)\n        def step(self, action):\n            # Check if agent's move is valid\n            is_valid = (self.obs['board'][int(action)] == 0)\n            if is_valid: # Play the move\n                self.obs, old_reward, done, _ = self.env.step(int(action))\n                reward = self.change_reward(old_reward, done)\n            else: # End the game and penalize agent\n                reward, done, _ = -10, True, {}\n            return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n\n    # Create ConnectFour environment\n    env = ConnectFourGym(agent2=\"random\")\n\n    # Create directory for logging training information\n    log_dir = \"log\/\"\n    os.makedirs(log_dir, exist_ok=True)\n\n    # Logging progress\n    monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n\n    # Create a vectorized environment\n    vec_env = DummyVecEnv([lambda: monitor_env])\n\n    # Neural network for predicting action values\n    def modified_cnn(scaled_images, **kwargs):\n        activ = tf.nn.relu\n        layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n                             init_scale=np.sqrt(2), **kwargs))\n        layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n                             init_scale=np.sqrt(2), **kwargs))\n        layer_2 = conv_to_fc(layer_2)\n        return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n\n    class CustomCnnPolicy(CnnPolicy):\n        def __init__(self, *args, **kwargs):\n            super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)\n\n'''\n\nmy_agent += '''    trained_1 = %s\n\n''' %trained_1\n\nmy_agent += '''    binary_file_path = 'trained_1.zip'\n    with open(binary_file_path, 'wb') as f:\n        f.write(trained_1)\n'''\n\nmy_agent += '''    model = PPO1.load('trained_1')\n    # Use the best model to select a column\n    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n    # Check if selected column is valid\n    is_valid = (obs['board'][int(col)] == 0)\n    # If not valid, select random move. \n    if is_valid:\n        return int(col)\n    else:\n        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])'''","57d8750b":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","10eb5db4":"model = PPO1.load('trained_1')","e54e3f6f":"for key, value in model.get_parameters().items():\n    print(key, value.shape)","749c4db4":"import torch as th\nimport torch.nn as nn","b7f6afe5":"# https:\/\/colab.research.google.com\/drive\/1XwCWeZPnogjz7SLW2kLFXEJGmynQPI-4#scrollTo=FIRaYdm9tCjE\n\nclass PyTorchCnnPolicy(nn.Module):\n    def __init__(self):\n        super(PyTorchCnnPolicy, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(384, 512)\n        self.fc2 = nn.Linear(512, 7)\n        self.relu = nn.ReLU()\n        self.out_activ = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = x.permute(0,2,3,1).contiguous()\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.out_activ(x)\n        return x","6ce145c9":"def copy_cnn_weights(baselines_model):\n    torch_cnn = PyTorchCnnPolicy()\n    model_params = baselines_model.get_parameters()\n    # Get only the policy parameters\n    policy_keys = [key for key in model_params.keys() if \"pi\" in key or \"c\" in key]\n    policy_params = [model_params[key] for key in policy_keys]\n    \n    for (th_key, pytorch_param), key, policy_param in zip(torch_cnn.named_parameters(), policy_keys, policy_params):\n        param = policy_param.copy()\n        # Copy parameters from stable baselines model to pytorch model\n\n        # Conv layer\n        if len(param.shape) == 4:  \n          # https:\/\/gist.github.com\/chirag1992m\/4c1f2cb27d7c138a4dc76aeddfe940c2\n          # Tensorflow 2D Convolutional layer: height * width * input channels * output channels\n          # PyTorch 2D Convolutional layer: output channels * input channels * height * width\n          param = np.transpose(param, (3, 2, 0, 1))\n        \n        # weight of fully connected layer\n        if len(param.shape) == 2:\n            param = param.T\n\n        # bias\n        if 'b' in key:\n            param = param.squeeze()\n\n        param = th.from_numpy(param)\n        pytorch_param.data.copy_(param.data.clone())\n        \n    return torch_cnn","6605e89d":"th_model = copy_cnn_weights(model)","3a01c6e6":"import gym\n\nclass ConnectX(gym.Env):\n    \"\"\"Custom Environment that follows gym interface\"\"\"\n    \n    def __init__(self, opponent_type):\n        self.env = make(\"connectx\", debug=True)\n        self.trainer = self.env.train([None, opponent_type])\n        self.obs = None\n        self.action_space = gym.spaces.Discrete(self.env.configuration.columns)\n        self.observation_space = gym.spaces.Box(0, 2, shape=(self.env.configuration.rows, self.env.configuration.columns), dtype=np.float32)\n\n    def get_kaggle_env(self):\n        return self.env\n\n    def step(self, action):\n        # Wrap kaggle environment.step()\n        if self.obs[0][action] != 0:\n          r = -1 # punish illegal move\n          d = False\n          o = self.obs\n        else:\n          o, r, d, _ = self.trainer.step(int(action))\n          o = np.reshape(np.array(o['board']), (self.env.configuration.rows, self.env.configuration.columns))\n          self.obs = o\n\n        return o, float(r), bool(d), {}\n    \n    def reset(self):        \n        o = self.trainer.reset()\n        self.obs = np.reshape(np.array(o['board']), (self.env.configuration.rows, self.env.configuration.columns))\n        return self.obs\n\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)","1914a010":"gym_env = ConnectX('negamax')\n\nenv = Monitor(gym_env, log_dir, allow_early_resets=True)\n\nenv = DummyVecEnv([lambda: env])","49f9bbbe":"import torch\nfrom torch.autograd import Variable\n\nepisode_reward = 0\ndone = False\nobs = env.reset()\nstep_cnt = 0\nmax_moves = gym_env.get_kaggle_env().configuration.columns * gym_env.get_kaggle_env().configuration.rows\n\nwhile (not done) and step_cnt <= max_moves:\n    step_cnt += 1\n    th_obs = Variable(torch.from_numpy(obs))\n    action = th.argmax(th_model(th_obs.unsqueeze(0))).item()   # th_obs.unsqueeze(0).size() => torch.Size([1, 1, 6, 7])\n\n    print('action:', action)\n    if obs[0][0][action] != 0:\n        print('skipping illegal move')\n    else:\n        obs, reward, done, info = env.step([action])\n        gym_env.render()\n        episode_reward += reward\n        print()","c8c06c43":"torch.save(th_model.state_dict(), 'thmodel')","8f1ecfaf":"import base64\nwith open('thmodel', 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes)","d25ba582":"print(encoded_weights[:1000]) # printing first 1000 characters from encoded weigths for visualisation. Very long string.","6fc61372":"import io\nimport base64\nimport torch\nfrom torch.autograd import Variable\nimport random\n\nagent_th_model = PyTorchCnnPolicy()\n# encoded_weights =b'gAKKCmz8n ..... [long string]\ndecoded = base64.b64decode(encoded_weights)\nbuffer = io.BytesIO(decoded)\nagent_th_model.load_state_dict(torch.load(buffer))","781d28f2":"def my_agent(observation, configuration):\n    obs = np.array(observation['board'])\n    th_obs = Variable(torch.from_numpy(np.array(observation['board']).reshape(1,1,6,7))).float()\n    y = agent_th_model(th_obs)\n    action = th.argmax(agent_th_model(th_obs)).item()\n    if observation.board[action] == 0:\n        return action\n    else:\n        return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])","52633c4d":"kaggle_env = gym_env.get_kaggle_env()\nkaggle_env.reset()\nkaggle_env.run([my_agent, \"negamax\"])\nkaggle_env.render(mode=\"ipython\", width=500, height=450)","06e75487":"get_win_percentages(agent1=my_agent, agent2=\"random\")","bd928b96":"get_win_percentages(agent1=my_agent, agent2=\"negamax\")","0280b7c9":"import inspect\nimport os\n\ndef write_agent_to_file(file):\n    with open(file, \"w\") as f:\n        submission_file = '''\nimport random\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport io\nimport base64\nimport torch\nfrom torch.autograd import Variable\n\nclass PyTorchCnnPolicy(nn.Module):\n    def __init__(self):\n        super(PyTorchCnnPolicy, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(384, 512)\n        self.fc2 = nn.Linear(512, 7)\n        self.relu = nn.ReLU()\n        self.out_activ = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = x.permute(0,2,3,1).contiguous()\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.out_activ(x)\n        return x\n\nagent_th_model = PyTorchCnnPolicy()\nencoded_weights = %s \\n\ndecoded = base64.b64decode(encoded_weights)\nbuffer = io.BytesIO(decoded)\nagent_th_model.load_state_dict(torch.load(buffer))\n\n'''%str(encoded_weights)\n        \n        submission_file += inspect.getsource(my_agent)\n        f.write(submission_file)\n        \nwrite_agent_to_file(\"submission.py\")","eb0db64b":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nfrom kaggle_environments import utils, agent\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\nsubmission_agent = agent.get_last_callable(submission)\nsys.stdout = out\n\nkaggle_env.run([submission_agent, submission_agent])\nprint(\"Success!\" if kaggle_env.state[0].status == kaggle_env.state[1].status == \"DONE\" else \"Failed...\")\n\nkaggle_env.play([submission_agent, None])","6f7c0311":"submission_file = '''\nimport random\nimport numpy as np\nimport torch as th\nimport torch.nn as nn\nimport io\nimport base64\nimport torch\nfrom torch.autograd import Variable\n\nclass PyTorchCnnPolicy(nn.Module):\n    def __init__(self):\n        super(PyTorchCnnPolicy, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=0, bias=True)\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0, bias=True)\n        self.fc1 = nn.Linear(384, 512)\n        self.fc2 = nn.Linear(512, 7)\n        self.relu = nn.ReLU()\n        self.out_activ = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = x.permute(0,2,3,1).contiguous()\n        x = x.view(x.size(0), -1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.out_activ(x)\n        return x\n\nagent_th_model = PyTorchCnnPolicy()\n#encoded_weights = %s\ndecoded = base64.b64decode(encoded_weights)\nbuffer = io.BytesIO(decoded)\nagent_th_model.load_state_dict(torch.load(buffer))\n\n'''#%str(encoded_weights)\n\nsubmission_file += inspect.getsource(my_agent)","8edbe568":"submission_file.splitlines() # For reference only","bd9e25b0":"# Write Submission File\n\n","bd7f97ce":"# Submit to Competition - Alternative\n\nBelow are two notebooks I used as reference.\n\nhttps:\/\/colab.research.google.com\/drive\/1XwCWeZPnogjz7SLW2kLFXEJGmynQPI-4#scrollTo=FIRaYdm9tCjE\n\nhttps:\/\/www.kaggle.com\/nickulus\/connectx-with-stable-baselines\/notebook","92337f8a":"# Self contained submission","68e527da":"# Debug\/Train your Agent","8caddfb6":"# Import necesary packages","5816f7ab":"# Submit to Competition with stable-baselines\n\n### After several attempts, I was unable to successfully submit this to competition.  Once the file is submitted, the stable-baselines module cannot be installed\/imported. \n\n### ModuleNotFoundError: No module named 'stable_baselines'","4e17a520":"# Create ConnectX Environment","cc7ed18b":"# Evaluate your Agent","56217793":"# Validate Submission\nPlay your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n\nWhy validate? This roughly verifies that your submission is fully encapsulated and can be run remotely.","c4d8d0fb":"The subnission file will look similar to the agent, with addition of any packages that migth be needed and modification of the load address. In this case, the load address is pointint go to my directory. Hvae not yet figured out how to have the submission file load the trained_1.zip file once submitted.","cd81abab":"# Install kaggle-environments","1a1fb6c4":"# Test your Agent","8ce29216":"There's a bit of extra work that we need to do to make the environment compatible with Stable Baselines.  For this, we define the `ConnectFourGym` class below.  This class implements ConnectX as an [OpenAI Gym environment](http:\/\/gym.openai.com\/docs\/) and uses several methods:\n- `reset()` will be called at the beginning of every game.  It returns the starting game board as a 2D numpy array with 6 rows and 7 columns.\n- `change_reward()` customizes the rewards that the agent receives.  (_The competition already has its own system for rewards that are used to rank the agents, and this method changes the values to match the rewards system we designed._) \n- `step()` is used to play the agent's choice of action (supplied as `action`), along with the opponent's response.  It returns:\n  - the resulting game board (as a numpy array), \n  - the agent's reward (from the most recent move only: one of `+1`, `-10`, `-1`, or `1\/42`), and\n  - whether or not the game has ended (if the game has ended, `done=True`; otherwise, `done=False`).","56ccbec1":"# Play your Agent\nClick on any column to place a checker there (\"manually select action\").","071f4104":"# Create an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n\n","fbb7d050":"PyTorch serialization adapted from:\n\nhttps:\/\/www.kaggle.com\/c\/connectx\/discussion\/126678","c6615404":"Stable Baselines requires us to work with [\"vectorized\" environments](https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/vec_envs.html).  For this, we can use the `DummyVecEnv` class.  \n\nThe `Monitor` class lets us watch how the agent's performance gradually improves, as it plays more and more games.","09b894e5":"# Define Model Algorithm and Train"}}