{"cell_type":{"ed39405a":"code","cdca0b9b":"code","e11fb874":"code","85113e97":"code","163300ae":"code","46092992":"code","0b8914b6":"code","a464af21":"code","42746402":"code","75c45ad8":"code","332d1c74":"code","1e2a6013":"code","e4ecd53e":"code","9cd9dbd4":"code","c4ebfbe4":"code","a368927e":"code","546d0750":"code","ffd6a7a6":"code","b1a7db01":"code","3fa1a8ac":"code","b324c8ea":"code","871e0673":"code","605d2284":"code","c5e7e52d":"code","dd1caffa":"code","1e8c21e2":"code","9ed63095":"code","bc563b66":"code","afcbf847":"markdown","32879ba9":"markdown","55984b64":"markdown","301f5516":"markdown","e9660d88":"markdown","4fcc164d":"markdown","fdf3d377":"markdown","74546de0":"markdown","a3ec3058":"markdown","c955951c":"markdown","27744fe0":"markdown","3a551996":"markdown","2951ebb2":"markdown","330a49dc":"markdown","05d5ab68":"markdown","63784807":"markdown","4de2a062":"markdown","6f4750ae":"markdown","e3098c13":"markdown","1349117b":"markdown","d8a0fb97":"markdown","3f5bed01":"markdown","b70eca03":"markdown","20d7d528":"markdown"},"source":{"ed39405a":"import numpy as np\nimport pandas as pd \nimport os\nimport sklearn\nimport tensorflow as tf\nimport matplotlib.pyplot as plt","cdca0b9b":"def submit(model, ids, test_features, file_name):\n    result = np.square(model.predict(test_features))\n    submission = pd.DataFrame({\"Id\": list(ids), \"SalePrice\": result.reshape(-1)})\n    submission.to_csv(file_name, index=False)","e11fb874":"train = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")","85113e97":"train.head()","163300ae":"test.head()","46092992":"should_impute = True\nif should_impute: \n    for data in [train, test]:\n        null_features = data.dtypes[data.isnull().sum() > 0]\n        for feature, feature_type in zip(list(null_features.index), null_features):\n            if feature_type == \"float64\":\n                data[feature].replace(np.NAN, data[feature].mean(), inplace=True)\n            else:\n                data[feature].replace(np.NAN, \"Unknown\", inplace=True)","0b8914b6":"train_test = pd.concat([train, test])\ntrain_test = pd.get_dummies(train_test)\ntrain_test.head()","a464af21":"mean_value = train_test.mean()\nstd_value = train_test.std()\n_ = mean_value.pop(\"SalePrice\")\n_ = std_value.pop(\"SalePrice\")","42746402":"train_features = train_test.iloc[: len(train)]\ntest_features = train_test.iloc[len(train):]\n_ = test_features.pop(\"SalePrice\")","75c45ad8":"train_features.shape","332d1c74":"train_features.describe()","1e2a6013":"corr = train_features.corr()\ncorr","e4ecd53e":"sale_price_corr = corr[\"SalePrice\"]\nsale_price_corr = sale_price_corr.sort_values(key = lambda val: abs(val), ascending=False)\nsale_price_corr.head(30)","9cd9dbd4":"threadsold = 0.01","c4ebfbe4":"related_columns = list(sale_price_corr[sale_price_corr.abs() > threadsold].index)\nrelated_columns.remove(\"SalePrice\")\nprint(related_columns, len(related_columns))","a368927e":"train_targets = sklearn.utils.shuffle(train_features)","546d0750":"related_train_features = train_features[related_columns]\nrelated_test_features = test_features[related_columns]","ffd6a7a6":"scale_strategy = [\"standard\", \"sqrt\", \"log\"][0]\nfor column in related_columns:\n    # Don't normlize Categorical Feature\n    if not \"_\" in column:\n        if scale_strategy == \"standard\":\n            related_train_features.loc[:, column] = (related_train_features.loc[:, column] - mean_value[column]) \/ std_value[column]\n            related_test_features.loc[:, column] = (related_test_features.loc[:, column] - mean_value[column]) \/ std_value[column]\n        elif scale_strategy == \"sqrt\":\n            related_train_features.loc[:, column] = np.sqrt(related_train_features.loc[:, column] + 10e-8)\n            related_test_features.loc[:, column] = np.sqrt(related_test_features.loc[:, column] + 10e-8)\n        else:\n            related_train_features.loc[:, column] = np.log(related_train_features.loc[:, column].abs() + 1)\n            related_test_features.loc[:, column] = np.log(related_test_features.loc[:, column].abs() + 1)","b1a7db01":"related_train_features.head()","3fa1a8ac":"train_targets = train_features[\"SalePrice\"]\ntrain_targets.head()","b324c8ea":"train_targets.hist()","871e0673":"train_targets = np.sqrt(train_targets)","605d2284":"from sklearn.model_selection import KFold\n\nshould_train_with_dnn = False\n\ndef train_with_dnn(related_train_features, train_targets):\n    best_model = None\n    best_score = 10e8\n    index = 1\n    should_train_with_dnn = False\n    for train_indices, valid_indices in KFold(n_splits=5).split(related_train_features):\n        X_train = related_train_features.iloc[train_indices]\n        X_val = related_train_features.iloc[valid_indices]\n        y_train = train_targets.iloc[train_indices]\n        y_val = train_targets.iloc[valid_indices]\n        model = tf.keras.Sequential([\n            tf.keras.layers.Input((X_train.shape[-1])),\n            tf.keras.layers.Dense(128, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(64, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(32, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.3),\n            tf.keras.layers.Dense(1)\n        ])\n        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n        model_checkpoint_path = \"model%d.h5\"%(index)\n        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(model_checkpoint_path, save_best_only=True)\n        history = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stop, model_checkpoint], verbose=0)\n        for item in [\"loss\", \"mae\"]:\n            pd.DataFrame(history.history, columns=[item, \"val_\" + item]).plot()\n            plt.title(\"%s in Fold %d\"%(item.capitalize(), index))\n            plt.show()\n        model.load_weights(model_checkpoint_path)\n        _, mae = model.evaluate(X_val, y_val)\n        submit(model, test[\"Id\"], related_test_features, \"submission_dnn_fold_%d.csv\"%(index))\n        if mae < best_score:\n            best_score = mae\n            best_model = model\n        index += 1\n    return best_model\nif should_train_with_dnn:\n    best_model = train_with_dnn(related_train_features, train_targets)\n    ","c5e7e52d":"import catboost\nimport time\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nbegin = time.time()\nX_train, X_val, y_train, y_val = train_test_split(related_train_features, train_targets)\nparameters = {\n    \"depth\": [6, 7, 8, 9, 10],\n    \"learning_rate\": [0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.18, 0.19, 0.2, 0.21, 0.22],\n    \"iterations\": [500, 1000], \n}\ndef train_catboost(hyperparameters, X_train, X_val, y_train, y_val):\n    keys = hyperparameters.keys()\n    best_index = {key:0 for key in keys}\n    best_cat = None\n    best_score = 10e8\n    for (index, key) in enumerate(keys):\n        print(\"Find best %s\" %(key))\n        items = hyperparameters[key]\n        best_parameter = None\n        temp_best = 10e8\n        for (key_index, item) in enumerate(items):\n            iterations = hyperparameters[\"iterations\"][best_index[\"iterations\"]] if key != \"iterations\" else item\n            learning_rate = hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]] if key != \"learning_rate\" else item\n            depth = hyperparameters[\"depth\"][best_index[\"depth\"]] if key != \"depth\" else item\n            print(\"Train with iterations: %d learning_rate: %.2f depth:%d\"%(iterations, learning_rate, depth))\n            cat = catboost.CatBoostRegressor(\n                iterations = iterations, \n                learning_rate = learning_rate,\n                depth = depth\n            )\n            if best_cat == None:\n                best_cat = cat\n            cat.fit(X_train, y_train, verbose=False)\n            y_pred = cat.predict(X_val)\n            score = mean_absolute_error(np.square(y_val), np.square(y_pred))\n            print(\"MAE: %.2f\"%(score))\n            if score < temp_best:\n                temp_best = score\n                best_index[key] = key_index\n                best_parameter = item\n            if score < best_score:\n                best_score = score\n                best_cat = cat\n        print(\"Best %s: \"%(key), best_parameter)\n    best_parameters = {\n        \"iterations\": hyperparameters[\"iterations\"][best_index[\"iterations\"]],\n        \"learning_rate\": hyperparameters[\"learning_rate\"][best_index[\"learning_rate\"]],\n        \"depth\": hyperparameters[\"depth\"][best_index[\"depth\"]]\n    }\n    return best_cat, best_score, best_parameters\nbest_cat, best_score, best_parameters = train_catboost(parameters, X_train, X_val, y_train, y_val)\nprint(\"Best CatBoost Model: \", best_cat)\nprint(\"Best MAE: \", best_score)\nelapsed = time.time() - begin \nprint(\"Elapsed time: \", elapsed)\nsubmit(best_cat, test[\"Id\"], related_test_features, \"submission_cat.csv\")","dd1caffa":"models = []\nfor fold, (train_indices, valid_indices) in enumerate(KFold(n_splits=5, shuffle=True).split(related_train_features)):\n    print(\"Training with Fold %d\" % (fold + 1))\n    X_train = related_train_features.iloc[train_indices]\n    X_val = related_train_features.iloc[valid_indices]\n    y_train = train_targets.iloc[train_indices]\n    y_val = train_targets.iloc[valid_indices]\n    cat = catboost.CatBoostRegressor(\n        iterations = 10000, \n        learning_rate = best_parameters[\"learning_rate\"],\n        depth = best_parameters[\"depth\"],\n        early_stopping_rounds=500\n    )\n    cat.fit(X_train, y_train, verbose=500)\n    y_pred = cat.predict(X_val)\n    score = mean_absolute_error(np.square(y_val), np.square(y_pred))\n    print(\"MAE: %.2f\"%(score))\n    submit(cat, test[\"Id\"], related_test_features, \"submission_cat_fold_%d.csv\"%(fold))\n    models.append(cat)\n    fold += 1","1e8c21e2":"result = np.mean(np.square([model.predict(related_test_features) for model in models]), axis=0)\nsubmission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": result})\nsubmission.to_csv(\"submission.csv\", index=False)","9ed63095":"cat = catboost.CatBoostRegressor(\n    iterations = 10000, \n    learning_rate = best_parameters[\"learning_rate\"],\n    depth = best_parameters[\"depth\"],\n    early_stopping_rounds=300\n)\ncat.fit(related_train_features, train_targets, verbose=100)","bc563b66":"y_pred = cat.predict(related_train_features)\nscore = mean_absolute_error(np.square(train_targets), np.square(y_pred))\nprint(\"MAE: %.2f\"%(score))\nsubmit(cat, test[\"Id\"], related_test_features, \"submission_cat_alldata.csv\")","afcbf847":"## Model Training with all data\nI want the see the performance of training all the data with the Model with Best Parameter. This result has no doubt overfits a lot, however it can get an optimal result that can get top 1% score.","32879ba9":"## Common Functions","55984b64":"## Table of Contents\n- Import Packages\n- Common Funtions\n- Import Dataset\n- EDA & Data Preprocessing\n    - Missing Value Imputation\n    - Convert Categorical Features to Numerical Features\n    - Basic Statistic Info\n    - Find Related Features\n    - Normalization\n- Model Development & Evaluation\n    - Using DNN\n    - Using CatBoost\n        - Hyperparameter Tuning\n        - Model Training with K-Fold algorithm\n        - Model Training with all data\n- Conclusion","301f5516":"### Missing Value Imputation","e9660d88":"### Feature Scaling","4fcc164d":"## Import dataset","fdf3d377":"## Conclusion","74546de0":"## If you find my notebook helpful, please upvote and comment my work.","a3ec3058":"## Using CatBoost","c955951c":"## Model Development & Evaluation\n### Using DNN\nTraining DNN Model Can be time consuming and it's hard to find a good hyperparameter that outperforms other Models. But I like TensorFlow, so I would like to add this solution. besides it's still useful to keep this code for futher study. So I will add a Toggle to control wheter to train with DNN.","27744fe0":"### Hyperparameter Tuning","3a551996":"### Find Related Features","2951ebb2":"### Basic Statistic Info","330a49dc":"### Model Training with K-Fold algorithm\nI will use best parameters to train the Model with K-Fold algorithm. Results can be different and the validation MAE is different from actual result, so I will keep tract of each test result.","05d5ab68":"### Convert Categorical Features to Numerical Features","63784807":"## Import Packages","4de2a062":"## EDA & Data Preprocessing","6f4750ae":"Calcuating avearge house prices of different models is often better than  calculating house prices of best model.","e3098c13":"First I will find best parameters, then I will use K-Fold alogorithm to train the Model, finally I will train the Model with all data to see what difference do they have.","1349117b":"Now calculate its mean and std value for futher normalization, please be noted that SalePrice wouldn't be normalized. In order to see statistic infos, I will normalize them after EDA.","d8a0fb97":"### Log Scaling on House Price","3f5bed01":"Compared to Deep Neural Network, CatBoost Regressor can achive a very good result with simple hyper parameter tuning strategy, and it requires less computing power. It's good for solving regression problem. The result of using K-Fold algorithm is not stable but it can give you surprise.","b70eca03":"## House Price Prediction With TF & CatBoost (Top 1%)","20d7d528":"As we can see Sale Price is related to a lot of factors. I will filter them with a thresold, say 0.05. Then there is 193 features. Actually this thresold is a hyper parameter to filter useful features."}}