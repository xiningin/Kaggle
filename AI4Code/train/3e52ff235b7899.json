{"cell_type":{"6544d6be":"code","a69a263c":"code","b0593660":"code","d62dbae6":"code","0121d981":"code","c5f9c906":"code","507a37c8":"code","75beee2c":"markdown","5781fe19":"markdown","018c2c30":"markdown","ad7aa775":"markdown","4aab25a8":"markdown","9efa6024":"markdown","0d05dffb":"markdown"},"source":{"6544d6be":"#import project libraries\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.preprocessing import StandardScaler\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport time\n\n#import redabillity\n#thanks to Ravi Shah for the tips\nimport sys\nsys.path = [\n    '..\/input\/readability-package',\n] + sys.path\nimport readability\n\n#supress SettingwithCopy warning\npd.options.mode.chained_assignment = None\n\n#set seed\nnp.random.seed(31415)","a69a263c":"#bring in raw data\nraw_train = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\nraw_test = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\n#split into X\/y\nX = pd.DataFrame(raw_train[\"excerpt\"], columns = [\"excerpt\"])\ny = pd.DataFrame(raw_train[\"target\"], columns = [\"target\"])","b0593660":"def add_read_meas(df):\n\n    t1 = time.time()\n    \n    #thanks to Ravi Shah for the tips\n    #create df of readability by row in training data\n    read_meas = [readability.getmeasures(e, lang = \"en\") for e in df[\"excerpt\"]]\n    read_meas = pd.DataFrame(read_meas)\n    \n    #bind readability features to df\n    r_list = [\"readability grades\", \"sentence info\", \"word usage\", \"sentence beginnings\"]\n    for item in r_list:\n        df = df.join(pd.json_normalize(read_meas[item]), lsuffix = \"_tot\")\n    \n    #create word type ratios from nominals\n    w_list = [\"wordtypes\", \"long_words\", \"complex_words\", \"complex_words_dc\", \"tobeverb\", \"auxverb\", \"conjunction_tot\", \"pronoun_tot\", \"preposition_tot\", \"nominalization\"]\n    df[w_list] = df[w_list].div(df.words, axis = 0)\n    \n    #create per sentence ratios from nominals\n    b_list = [\"pronoun\", \"interrogative\", \"article\", \"subordination\", \"conjunction\", \"preposition\"]\n    df[b_list] = df[b_list].div(df.sentences, axis = 0)\n    df = df.drop([\"words\", \"characters\", \"syllables\", \"wordtypes\", \"sentences\", \"paragraphs\"], axis = 1)\n    \n    t2 = time.time()\n    \n    print(f\"readability measures: {t2 - t1}\")\n    \n    #thanks to Ravi Shah for the tips\n    ###################\n    \n    t3 = time.time()\n    \n    #create vectored words array\n    \n    #first step: tokenize, lemmatize, remove stop words\n    \n    #copy text\n    spacy_df = df[\"excerpt\"].copy()\n\n    #set lang\n    nlp = spacy.load('en_core_web_lg')\n\n    #set to lowercase\n    spacy_df = [e.lower() for e in spacy_df]\n\n    #remove punctuation\n    spacy_df = [e.translate(str.maketrans('', '', string.punctuation)) for e in spacy_df]\n\n    #bring text into df\n    spacy_df = pd.DataFrame(spacy_df, columns = [\"excerpt\"])\n\n    #tokenize\n    tokenizer = nlp.tokenizer\n    spacy_df[\"excerpt\"] = spacy_df[\"excerpt\"].apply(lambda row: list(tokenizer(row)))\n\n    #remove stop words\n    stop_words = set(stopwords.words('english')).union(nlp.Defaults.stop_words)\n    spacy_df[\"excerpt\"] = spacy_df[\"excerpt\"].apply(lambda row: [str(token) for token in row if str(token) not in stop_words])\n    \n    #bring rows back to string\n    spacy_df[\"excerpt\"] = spacy_df[\"excerpt\"].str.join(\" \")\n    \n    #second step: vectorize and bind\n    with nlp.disable_pipes():\n        vectors = np.array([nlp(text).vector for text in spacy_df.excerpt])\n    \n    #create column list\n    names = list()\n    for i in range(300):\n        names.append(f\"spacy_{i}\")\n    \n    #create spacy df\n    spacy_cols = pd.DataFrame(vectors, columns = names)\n    \n    #add spacy features\n    df = df.join(spacy_cols)\n    \n    t4 = time.time()\n    \n    print(f\"spacy measures: {t4 - t3}\")\n    \n    #thanks to Ravi Shah for the tips\n    ###################\n    \n    t5 = time.time()\n    \n    #parts of speach tag list and df\n    pos_tags = [\"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"LS\", \"MD\", \"NN\", \"NNS\", \"NNP\", \"NNPS\", \"PDT\", \"POS\", \"PRP\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBZ\", \"WDT\", \"WP\", \"WRB\"]\n    pos = pd.DataFrame(columns = pos_tags)\n    \n    #loop through the rows and add counts for each pos\n    for e in df[\"excerpt\"]:\n        \n        #break excerpt into list of tokens\n        tags = pos_tag(word_tokenize(e))\n        \n        #instantiate\/clear row dict\n        tag_row = dict()\n        \n        #iterate through tags and set to 0\n        for p in pos_tags:\n            tag_row[p] = 0\n\n        #update counts for each word\n        for tag in tags:\n            try:\n                tag_row[tag[1]] += 1\n            except:\n                pass\n        \n        #add row to pos\n        pos = pos.append(tag_row, ignore_index = True)\n    \n    #create ratio of tags\n    pos = pos\/pos.sum(axis=1)[:,None]\n    \n    #add pos features\n    df = df.join(pos)\n    \n    t6 = time.time()\n    \n    print(f\"pos measures: {t6 - t5}\")\n    \n    return df   ","d62dbae6":"#add read meas\nX = add_read_meas(X)\n\n#drop excerpt\nX = X.drop(\"excerpt\", axis = 1)\n\n#show data\nprint(X.head())\n\n#show data shape\nprint(X.shape)\n\n#scale data\nsc = StandardScaler().fit(X)\nX = sc.transform(X)","0121d981":"#create elastic net CV function\ndef ENCV(X, y, folds=10):\n    \n    t1 = time.time()\n    \n    #cv arg\n    cv = RepeatedKFold(n_splits = folds, n_repeats = 3, random_state = 684)\n    \n    #define possible penalty values\n    l_ones = np.arange(.01, 1, .01)\n    \n    #instantiate model\n    model = ElasticNetCV(l1_ratio = l_ones,\n                        alphas = None,\n                        cv = cv,\n                        n_jobs = -1,\n                        normalize = False,\n                        fit_intercept = True,\n                        tol = .1)\n    \n    #fit model\n    fitted_model = model.fit(X, y)\n    \n    #get predictios, calcualte RMSE\n    pred = fitted_model.predict(X)\n    rmse = mean_squared_error(y, pred, squared = False)\n    \n    t2 = time.time()\n    \n    print(f\"elastic net run time: {t2 - t1}\")\n    \n    #return model and RMSE\n    return fitted_model, rmse","c5f9c906":"#run model and save data\nmodel, score = ENCV(X, y)\n\n#show parameters and RMSE score\nprint(model.alpha_, model.l1_ratio_)\nprint(score)","507a37c8":"#prepare test features\ntest = pd.DataFrame(raw_test[\"excerpt\"], columns = [\"excerpt\"])\ntest = add_read_meas(test)\ntest = test.drop(\"excerpt\", axis = 1)\ntest = sc.transform(test)\n\n#build submission\ndata = [raw_test[\"id\"], pd.Series(model.predict(test))]\nheaders = [\"id\", \"target\"]\nsubmission = pd.concat(data, axis=1, keys = headers)\n\n#show submission\nprint(submission)\n\n#save submission\nsubmission.to_csv('submission.csv', index = False)","75beee2c":"Bind features to training data and view.","5781fe19":"Create a function to bind column features to each excerpt.","018c2c30":"Block below starts project up with needed files, libraries, and miscellaneous settings.","ad7aa775":"Model is run and output and RMSE is saved.","4aab25a8":"Create the elastic net model function.","9efa6024":"RMSE is not great, but I am proud of it.\n\n\nTest data is run and submitted below.","0d05dffb":"Raw data files are brought in, and sliced for relevant information. "}}