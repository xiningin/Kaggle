{"cell_type":{"e718a534":"code","96414fb4":"code","ca08a504":"code","672747d2":"code","fc23c9a6":"code","e408bbb1":"code","39064032":"markdown"},"source":{"e718a534":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprefix = \"..\/input\/\"","96414fb4":"df = pd.read_csv(prefix + \"train.csv\", index_col='ID_code')\ntrues = df.loc[df['target'] == 1]\nfalses = df.loc[df['target'] != 1].sample(frac=1)[:len(trues)]\ndata = pd.concat([trues, falses], ignore_index=True).sample(frac=1)\ndata.head()","ca08a504":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ny = data['target']\nX = data.drop('target', axis=1)\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","672747d2":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC, SVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n\nmodels = []\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA', QuadraticDiscriminantAnalysis()))\nmodels.append(('AdaBoost', AdaBoostClassifier()))\nmodels.append(('Bagging', BaggingClassifier()))\nmodels.append(('Extra Trees Ensemble', ExtraTreesClassifier(n_estimators=1000)))\nmodels.append(('Gradient Boosting', GradientBoostingClassifier()))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators=1000)))\nmodels.append(('Ridge', RidgeClassifier()))\nmodels.append(('SGD', SGDClassifier(tol=1e-3, max_iter=10000)))\nmodels.append(('BNB', BernoulliNB()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('MLP', MLPClassifier()))\nmodels.append(('LSVC', LinearSVC(max_iter=100000)))\nmodels.append(('NuSVC', NuSVC(gamma='scale')))\nmodels.append(('SVC', SVC(gamma='scale')))\nmodels.append(('DTC', DecisionTreeClassifier()))\nmodels.append(('ETC', ExtraTreeClassifier()))\n\nDECISION_FUNCTIONS = {\"Ridge\", \"SGD\", \"LSVC\", \"NuSVC\", \"SVC\"}","fc23c9a6":"from sklearn.metrics import roc_auc_score, roc_curve\n%matplotlib inline\n\nbest_model = None\nbest_model_name = \"\"\nbest_valid = 0\nfor name, model in models:\n    model.fit(X_train, y_train)\n    if name in DECISION_FUNCTIONS:\n        proba = model.decision_function(X_valid)\n    else:\n        proba = model.predict_proba(X_valid)[:, 1]\n    score = roc_auc_score(y_valid, proba)\n    fpr, tpr, _  = roc_curve(y_valid, proba)\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', label=f\"ROC curve (auc = {score})\")\n    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n    plt.title(f\"{name} Results\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    if score > best_valid:\n        best_valid = score\n        best_model = model\n        best_model_name = name\n\nprint(f\"Best model is {best_model_name}\")","e408bbb1":"test = pd.read_csv(prefix + \"test.csv\", index_col='ID_code')\nsubmission = pd.read_csv(prefix + \"sample_submission.csv\", index_col='ID_code')\n\ntest_X = scaler.transform(test)\nif best_model_name in DECISION_FUNCTIONS:\n    submission['target'] = best_model.decision_function(test_X)\nelse:\n    submission['target'] = best_model.predict_proba(test_X)[:, 1]\nsubmission.to_csv(f\"{best_model_name}_submission.csv\")","39064032":"Only 10% of the data is positive, so we'll reduce the train size to have an equal numbers of positive and negative samples."}}