{"cell_type":{"d121c410":"code","d1b201a9":"code","24ec755e":"code","307b6a85":"code","29fe70c8":"code","2a902919":"code","52e5ca39":"code","728a9056":"code","2d80a01e":"code","c23d80f8":"code","80ea41b6":"code","842997d5":"code","089c8ee6":"code","207f18f5":"code","35e1931b":"code","197595b6":"code","106da466":"code","4bf4f749":"code","0e71be02":"code","2cf01899":"code","7ca7aa29":"code","2b620ea7":"code","f0df64ea":"code","918c6023":"code","844b8f1c":"code","54fe9626":"code","fa88cfd2":"code","c59e052a":"code","5380beef":"markdown","1c9a63e3":"markdown","e54a885e":"markdown"},"source":{"d121c410":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nimport tensorflow_addons as tfa\nimport os, shutil\nimport PIL\nimport PIL.Image\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1b201a9":"train_labels = pd.read_csv(os.path.join(\"..\",\"input\",\"dont-stop-until-you-drop\", \"train.csv\"))\ntrain_image_dir = os.path.join(\"..\",\"input\",\"dont-stop-until-you-drop\",\"images\", \"train_images\")\ntest_image_dir = os.path.join(\"..\",\"input\",\"dont-stop-until-you-drop\",\"images\", \"test_images\")","24ec755e":"train_labels.set_index(\"image_id\", inplace=True)\ntrain_labels = train_labels.reindex(list(os.walk(train_image_dir))[0][2])","307b6a85":"class_names = list(train_labels['class_6'].unique())\n\nfor i in class_names:\n    os.makedirs(os.path.join('..\/kaggle\/temp\/train_', str(i)))\n    \nfor c in class_names:\n    for i in list(train_labels[train_labels['class_6']==c].index): # Image Id\n        get_image = os.path.join(train_image_dir, i) # Path to Images\n        move_image_to_cat = shutil.copy(get_image, '..\/kaggle\/temp\/train_\/'+str(c))","29fe70c8":"list(os.walk(train_image_dir))[0][2][0:10]","2a902919":"train_labels.head()","52e5ca39":"image_file_names = os.listdir(train_image_dir)","728a9056":"i = 100\nim = PIL.Image.open(os.path.join(train_image_dir, image_file_names[i]))\nprint(str(im.size))\nPIL.Image.open(os.path.join(train_image_dir, image_file_names[i]))","2d80a01e":"heights = []\nwidths = []\nfor i in image_file_names:\n    im = PIL.Image.open(os.path.join(train_image_dir, i))\n    height, width = im.size\n    heights.append(height)\n    widths.append(width)","c23d80f8":"print(\"average image height: {}\".format(np.mean(heights)))\nprint(\"average image width: {}\".format(np.mean(widths)))","80ea41b6":"sns.distplot(heights)","842997d5":"sns.distplot(widths)","089c8ee6":"batch_size = 16\n#img_height = 800\nimg_height =300\n#img_width = 640\nimg_width =300\nnum_classes = 6","207f18f5":"train_ds = tf.keras.utils.image_dataset_from_directory(\n    '..\/kaggle\/temp\/train_\/',\n    labels='inferred',\n    validation_split=0.2,\n    subset=\"training\",\n    seed =781,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)","35e1931b":"class_names = train_ds.class_names\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","197595b6":"img_augmentation = Sequential(\n    [\n        layers.RandomFlip(),\n        layers.RandomContrast(factor=0.1)\n    ],\n    name=\"img_augmentation\",\n)","106da466":"from tensorflow.keras.applications import EfficientNetB3\n\ninputs = layers.Input(shape=(img_height, img_width, 3))\nx = img_augmentation(inputs)\noutputs = EfficientNetB3(include_top=True, weights=None, classes=num_classes)(x)\n\nmodel = tf.keras.Model(inputs, outputs)\nmodel.compile(\n    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)\n\nmodel.summary()\n\nepochs = 5  # @param {type: \"slider\", min:10, max:100}\nhist = model.fit(train_ds, epochs=epochs, verbose=2)","4bf4f749":"results = []\nfor image_name in os.listdir(test_image_dir):\n    img = keras.preprocessing.image.load_img(\n        os.path.join(test_image_dir, image_name), target_size=(img_height, img_width)\n    )\n    img_array = keras.preprocessing.image.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n\n    predictions = model.predict(img_array)\n    score = np.argmax(predictions[0])\n    results.append((image_name, score)) \n","0e71be02":"results_df = pd.DataFrame(results)\nresults_df.to_csv('outputfile.csv', index=False)","2cf01899":"batch_size = 16\nimg_height = 800\nimg_width = 640\nnum_classes = 6","7ca7aa29":"train_ds = tf.keras.utils.image_dataset_from_directory(\n    '..\/kaggle\/temp\/train_\/',\n    labels='inferred',\n    validation_split=0.2,\n    subset=\"training\",\n    seed =781,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    '..\/kaggle\/temp\/train_\/',\n    labels='inferred',\n    validation_split=0.2,\n    subset=\"validation\",\n    seed =781,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)","2b620ea7":"from tensorflow.keras.applications import EfficientNetB3\n\ndef build_model(num_classes):\n    inputs = layers.Input(shape=(img_height, img_width, 3))\n    x = img_augmentation(inputs)\n    model = EfficientNetB3(include_top=False, input_tensor=x, weights=\"imagenet\")\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n\n    top_dropout_rate = 0.2\n    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(inputs, outputs, name=\"EfficientNet\")\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n    model.compile(\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n    return model","f0df64ea":"model = build_model(num_classes=num_classes)\n#model.summary()","918c6023":"epochs = 20  # @param {type: \"slider\", min:8, max:80}\ncallbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                                  patience=2, \n                                                  verbose=1, \n                                                  factor=0.5),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                              verbose=1,\n                                              patience=10),\n             tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5', \n                                                monitor='val_loss',\n                                                verbose=0,\n                                                save_best_only=True)]\n\nhist = model.fit(train_ds, epochs=epochs, validation_data= val_ds, verbose=2, callbacks=callbacks)","844b8f1c":"def unfreeze_model(model):\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-20:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n    model.compile(\n        optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n    )\n\n\nunfreeze_model(model)\n\nepochs = 3  # @param {type: \"slider\", min:8, max:50}\nhist = model.fit(train_ds, validation_data= val_ds, epochs=epochs, callbacks=callbacks)","54fe9626":"# Load the best checkpoint\nmodel = tf.keras.models.load_model('best_model.h5')","fa88cfd2":"results = []\nfor image_name in os.listdir(test_image_dir):\n    img = keras.preprocessing.image.load_img(\n        os.path.join(test_image_dir, image_name), target_size=(img_height, img_width)\n    )\n    img_array = keras.preprocessing.image.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0)  # Create batch axis\n\n    predictions = model.predict(img_array)\n    score = np.argmax(predictions[0])\n    results.append((image_name, score)) \n","c59e052a":"results_df = pd.DataFrame(results)\nresults_df.to_csv('outputfile.csv', index=False)","5380beef":"Now that we have something, lets try this with a fine-tuned model","1c9a63e3":"Note that the images are of different sizes","e54a885e":"Lets begin with trying to transfer learn from  a pre-trained coputer vision model"}}