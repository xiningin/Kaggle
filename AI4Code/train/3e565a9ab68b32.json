{"cell_type":{"4914ba93":"code","982f5b93":"code","7029e227":"code","80cf2c9e":"code","ac10333c":"code","21ba6f39":"code","0e521a57":"code","2f29c50f":"code","5d827adc":"code","16ce5c9c":"code","aea76a6a":"code","6a463ea4":"code","1f7db61e":"code","73664984":"code","6e6e99db":"code","22707e0a":"code","04f82732":"code","975cf10c":"code","783decf9":"code","cb67ab1a":"code","967868f2":"code","623b3853":"code","3bbc4b6c":"code","52db3a6d":"code","6717bf5d":"code","cc1579e9":"code","50444a35":"code","a0bf6ec5":"code","303de4f5":"code","93079735":"code","d4ba4054":"code","fcfef7a1":"code","e3d2ae58":"code","50ef7090":"code","14c10f1d":"code","1e885c88":"code","873bf607":"code","f0b065e6":"code","e8cf0435":"code","fd3302c4":"code","44aa3195":"code","eeb6ce41":"code","6da739ec":"code","5b032839":"code","c6d9b380":"code","1af3cab1":"code","be084cdf":"code","863394d9":"code","c51fd911":"code","9c3e7444":"code","08530ba3":"code","3ef4dd92":"code","326dd50d":"code","8c0c9b65":"code","e615126c":"code","ac424558":"code","b75dd2d9":"code","d3c81c77":"code","d8da549b":"code","f7c3a1fe":"code","f2b8cc31":"code","17653449":"code","5e9af25f":"code","2813ee34":"code","09fcb605":"code","a1081e40":"code","61bcc003":"code","f788103a":"markdown","eea49ef1":"markdown","ab67658e":"markdown","28bbb35d":"markdown","5979163f":"markdown","ebf3afc7":"markdown","0b488e28":"markdown","75872a01":"markdown","46dd0bc8":"markdown","34464078":"markdown","9637b4bc":"markdown","21c5437b":"markdown","d3fe67cf":"markdown","d2b3b3f8":"markdown","a7e86470":"markdown","47668186":"markdown","2989aafd":"markdown","6b5d646d":"markdown","3c64913b":"markdown","41a9fe76":"markdown","87435845":"markdown","84da65a1":"markdown","30a0acc3":"markdown","c4df4943":"markdown","568a4b12":"markdown","48abe9d5":"markdown","8ec635cf":"markdown","dc28b977":"markdown","f4cfd452":"markdown","caf63b01":"markdown","91ed52c8":"markdown","1e0e299d":"markdown","af77b8e7":"markdown","5934df4c":"markdown","f9bf370b":"markdown","b7bef94d":"markdown","72119235":"markdown","6d65cf25":"markdown","e6f1439f":"markdown","0bd743aa":"markdown","8e18184c":"markdown","224ae945":"markdown"},"source":{"4914ba93":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \n\nimport warnings\nwarnings.simplefilter(action='ignore')","982f5b93":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data.head()","7029e227":"test_data.head()","80cf2c9e":"print('Size of the samples is: {}'.format(train_data.shape[0]))\nprint('Size of the features is: {}'.format(train_data.shape[1]))","ac10333c":"print('Size of the survived samples is: {}'.format(len(train_data[train_data.Survived == 1][\"Age\"])))\nprint('Size of the deceased samples is: {}'.format(len(train_data[train_data.Survived == 0][\"Age\"])))\n","21ba6f39":"train_data.describe()","0e521a57":"train_data.dtypes","2f29c50f":"train_data.isnull().sum()","5d827adc":"test_data.isnull().sum()","16ce5c9c":"print('Percent missing from \"Age\" to total dataset is {}%'.format(train_data['Age'].isnull().sum() \/ train_data.shape[0]))","aea76a6a":"plt.hist(train_data[\"Age\"], bins = 15)\nplt.show()\nprint(\"The average age is:\", train_data[\"Age\"].mean())","6a463ea4":"plt.figure(figsize=(50,30))\nsns.barplot(train_data[\"Age\"],train_data[\"Fare\"], palette='rainbow')\nplt.title(\"Relation Between Age and Fare\")","1f7db61e":"corr_matrix = train_data.corr()\nplt.figure(figsize=(13, 10))\nsns.heatmap(data = corr_matrix, annot=True, linewidths=0.2)","73664984":"print (\"Population in Class 1: {}\".format(len(train_data.loc[train_data.Pclass == 1])))\nprint (\"Population in Class 2: {}\".format(len(train_data.loc[train_data.Pclass == 2])))\nprint (\"Population in Class 3: {}\".format(len(train_data.loc[train_data.Pclass == 3])))","6e6e99db":"sns.boxplot(train_data[\"Pclass\"],train_data[\"Age\"], palette='rainbow')\nplt.title(\"Box Plot of Average Age\")","22707e0a":"plt.figure(figsize=(10,8))\nplt.hist(train_data[train_data.Survived == 1][\"Age\"], alpha = 1, bins = 'auto')\nplt.hist(train_data[train_data.Survived == 0][\"Age\"], alpha = 0.5, bins = 'auto')\nplt.title(\"Amount of Surviving Passengers vs Non-Surviving Passengers Per Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Amount Survived\")\nplt.legend([\"Survived\",\"Deceased\"])\n\nplt.show()","04f82732":"cabin_data = train_data['Cabin'].str[0:1].dropna()\nprint(\"The total of cabins: {}\".format(len(train_data['Cabin'].dropna())))\nprint(\"The total of unique cabins: {}\".format(len(cabin_data.unique())))\nnp.sort(cabin_data.unique())\n","975cf10c":"cabin_data.loc[cabin_data == \"T\"]","783decf9":"cabin_data = cabin_data.drop(339)\nnp.sort(cabin_data.unique())","cb67ab1a":"d = {'Cabin': cabin_data, 'Age': train_data[\"Age\"]}\ncabin_df = pd.DataFrame(d).dropna()\nsns.boxplot(cabin_df[\"Cabin\"],cabin_df[\"Age\"], palette='rainbow')\nplt.title(\"Different Ages Between Cabins\")","967868f2":"d = {'Embarked': train_data[\"Embarked\"], 'Age': train_data[\"Age\"]}\ncabin_df = pd.DataFrame(d).dropna()\nsns.boxplot(cabin_df[\"Embarked\"],cabin_df[\"Age\"], palette='rainbow')\nplt.title(\"Different Ages Between Embarked\")","623b3853":"sns.barplot(train_data['Embarked'],train_data['Survived'])\nplt.tight_layout()","3bbc4b6c":"sns.barplot(train_data['SibSp'],train_data['Age'])\nplt.tight_layout()","52db3a6d":"sns.swarmplot(train_data[\"SibSp\"],train_data[\"Age\"], dodge=True, alpha=.8,color='black',s=4)","6717bf5d":"sns.barplot(train_data['Sex'],train_data['Age'])","cc1579e9":"sns.barplot(train_data['SibSp'],train_data['Survived'])","50444a35":"sns.barplot(train_data['Parch'],train_data['Survived'])","a0bf6ec5":"combine = [train_data, test_data]\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['Parch'] + dataset['SibSp'] == 0, 'IsAlone'] = 1\n\ntrain_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","303de4f5":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","93079735":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","d4ba4054":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()","fcfef7a1":"train_data","e3d2ae58":"plt.figure(figsize=(12,8))\nsns.boxplot(train_data[\"Pclass\"],train_data[\"Age\"], palette='rainbow',dodge=True)\nsns.swarmplot(train_data[\"Pclass\"],train_data[\"Age\"], dodge=True, alpha=.8,color='black',s=4)\nplt.title(\"Age by Passenger Class, Separated by Survival\")","50ef7090":"X_df = train_data[train_data['Age'].notna()]\n\nmsk = np.random.rand(len(X_df)) < 0.80\ntrain = X_df[msk]\ntest = X_df[~msk]\ntrain_y = X_df[msk][\"Age\"]\ntest_y = X_df[~msk][\"Age\"]","14c10f1d":"train[\"Survived\"].replace([0,1],[1,0])\ntrain = train[['Survived','Pclass','SibSp']]\n\ntest[\"Survived\"].replace([0,1],[1,0])\ntest = test[['Survived','Pclass','SibSp']]","1e885c88":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\n\nX = np.asanyarray(train)\ny = np.asanyarray(train_y)\n\nregr.fit(X, y)","873bf607":"from sklearn.metrics import r2_score\n\ntest_y = np.asanyarray(test_y)\nAtest_y_ = regr.predict(test)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(Atest_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((Atest_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , Atest_y_) )","f0b065e6":"Btest_y_ = np.full((len(test_y), 1), test_y.mean())\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(Btest_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((Btest_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , Btest_y_) )","e8cf0435":"plt.plot(test_y,  color='blue')\nplt.plot(Btest_y_,  color='purple')","fd3302c4":"X_df = test_data[test_data['Age'].notna()]\n\nmsk = np.random.rand(len(X_df)) < 0.80\ntrain = X_df[msk]\ntest = X_df[~msk]\ntrain_y = X_df[msk][\"Age\"]\ntest_y = X_df[~msk][\"Age\"]","44aa3195":"train = train[['Pclass','SibSp']]\ntest = test[['Pclass','SibSp']]","eeb6ce41":"test_regr = linear_model.LinearRegression()\n\nX = np.asanyarray(train)\ny = np.asanyarray(train_y)\n\ntest_regr.fit(X, y)","6da739ec":"test_y = np.asanyarray(test_y)\nAtest_y_ = test_regr.predict(test)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(Atest_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((Atest_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , Atest_y_) )","5b032839":"Btest_y_ = np.full((len(test_y), 1), test_y.mean())\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(Btest_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((Btest_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , Btest_y_) )","c6d9b380":"Xtrain_df = train_data\nindex = 0\nfor data in Xtrain_df[\"Age\"].isnull(): \n    if data:\n        test_df = [Xtrain_df[[\"Survived\",\"Pclass\",\"SibSp\"]].loc[index]]\n        result_value = regr.predict(test_df)\n        Xtrain_df[\"Age\"].loc[index] = result_value       \n    index += 1\nXtrain_df.head(6)","1af3cab1":"Ytrain_df = Xtrain_df[\"Survived\"]","be084cdf":"Xtrain_df = Xtrain_df.drop([\"Ticket\",\"Cabin\",\"PassengerId\",\"Name\"], axis = 1).replace([\"male\",\"female\"],[1,0]).replace([\"male\",\"female\"],[1,0])\ntest_passenger = test_data[\"PassengerId\"]\ntest_data = test_data.drop([\"Ticket\",\"Cabin\",\"PassengerId\",\"Name\"], axis = 1).replace([\"male\",\"female\"],[1,0]).replace([\"male\",\"female\"],[1,0])","863394d9":"Xtest_df = test_data\nindex = 0\nfor data in Xtest_df[\"Age\"].isnull(): \n    if data:\n        test_df = [Xtest_df[[\"Pclass\",\"SibSp\"]].loc[index]]\n        result_value = test_regr.predict(test_df)\n        Xtest_df[\"Age\"].loc[index] = result_value       \n    index += 1\nXtest_df.head(6)","c51fd911":"train_combine = [Xtrain_df,test_data]\nfor data in train_combine:\n    y = pd.get_dummies(data.Embarked, prefix='Embark')\n    data['Embark_C'] = y['Embark_C']\n    data['Embark_Q'] = y['Embark_Q']\n    data['Embark_S'] = y['Embark_S']","9c3e7444":"Xtrain_df = Xtrain_df.drop([\"Embarked\"], axis = 1)\ntest_data = test_data.drop([\"Embarked\"], axis = 1)","08530ba3":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\nfeatures= ['Age', 'Fare']\nXtrain_df[features]= ss.fit_transform(Xtrain_df[features])\ntest_data[features]= ss.fit_transform(test_data[features])","3ef4dd92":"Xtest_df['Fare'].fillna((Xtest_df['Fare'].mean()), inplace=True)","326dd50d":"Xtest_df.isnull().sum()","8c0c9b65":"Xtrain_df","e615126c":"test_data","ac424558":"Xtest_data = Xtrain_df\nmsk = np.random.rand(len(Xtest_data)) < 0.80\nXtrain = Xtest_data[msk]\nXtest = Xtest_data[~msk]\nXtrain_y = Xtest_data[msk][\"Survived\"]\nXtest_y = Xtest_data[~msk][\"Survived\"]","b75dd2d9":"Xtrain = Xtrain.drop([\"Survived\"], axis = 1)\nXtest = Xtest.drop([\"Survived\"], axis = 1)\nXtrain_df = Xtrain_df.drop([\"Survived\"], axis = 1)\nXtest_df = Xtest_df.drop([\"Embarked\"], axis = 1)","d3c81c77":"# Compare Algorithms\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# prepare configuration for cross validation test harness\nseed = 2\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('TREE', RandomForestClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, Xtrain, Xtrain_y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    model.fit(Xtrain, Xtrain_y)\n    test_y = np.asanyarray(Xtest_y)\n    Atest_y_ = model.predict(Xtest)\n\n    print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(Atest_y_ - test_y)))\n    print(\"Residual sum of squares (MSE): %.2f\" % np.mean((Atest_y_ - test_y) ** 2))\n    print(\"R2-score: %.2f\" % r2_score(test_y , Atest_y_) )\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Model\")\nplt.show()","d8da549b":"index = 0\nc_results = []\nlength = 15\nbest_c = 0\nbest_results = 0\nfor C in range(1,length):\n    clf = RandomForestClassifier(criterion=\"entropy\", max_depth = C)\n    clf.fit(Xtrain, Xtrain_y) \n    yhat = clf.predict(Xtest)\n    score = r2_score(Xtest_y , yhat)\n    c_results.append(score)\n    index += 1\n    print(\"C:\",index,\"Score:\",score)\n    if score > best_results:\n        best_results = score\n        best_c = index","f7c3a1fe":"plt.plot(range(1,length),c_results)","f2b8cc31":"print(\"The best regulation parameter is\",best_c)","17653449":"clf = RandomForestClassifier(n_estimators=10, max_depth=best_c, random_state=1)\nclf.fit(Xtrain_df, Ytrain_df) \nyhat = clf.predict(Xtrain_df)","5e9af25f":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools","2813ee34":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","09fcb605":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(Ytrain_df, yhat, labels=[0,1])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(Ytrain_df, yhat))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Deceased','Survived'],normalize= False,  title='Confusion matrix')","a1081e40":"final_results = model_selection.cross_val_score(clf, Xtrain_df, Ytrain_df, cv=kfold, scoring=scoring)\nmsg = \"%s: %f (%f)\" % (\"Tree\", final_results.mean(), final_results.std())\nplt.boxplot(final_results)\nprint(msg)","61bcc003":"predictions = clf.predict(Xtest_df)\n\noutput = pd.DataFrame({'PassengerId': test_passenger, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f788103a":"Amount of Siblings is seen to have a  difference in the age in the titanic ","eea49ef1":"Time to comapre the Model to using the Average Age","ab67658e":"# Filling in the Missing Data Values using the Regression Model","28bbb35d":"We can see already that the most common age so far is 29-30. In addition, the data of several other features","5979163f":"For the empty fare, since it is a single sample, we will use the median","ebf3afc7":">We will drop T out of the data as it is only a single sample","0b488e28":"# Name and Title","75872a01":"There are slight deviations between the Age of a passenger in correlation to the Pclass. This means that younger people are usually poorer or decide to travel cheaper ","46dd0bc8":"The Train Dataset","34464078":"> Order of the Age to Cabins, ascending: 3,2,1","9637b4bc":"# Finding Data to Fill in the Missing Age ","21c5437b":"# Making a model to predict the Age of a Passenger ","d3fe67cf":"# Cleaning the Database","d2b3b3f8":"the data between deceased and survived is balanced","a7e86470":"# Creating a Prediction Model for the Test Dataset","47668186":"The next step is to find the next variable with high correlation to substitute the values","2989aafd":"# Using different Models on the Improved Dataset","6b5d646d":"The model seems to be getting better acurracy than the use of means. Its now time to fill in the missing Age values with the Model ","3c64913b":"Pclass\tSex\tAge\tSibSp\tParch\tFare\tIsAlone\tPrefix_1\tPrefix_2\tPrefix_3\tPrefix_4\tPrefix_5\tEmbark_C\tEmbark_Q\tEmbark_S","41a9fe76":"As seen above, the most missing data is age. ","87435845":"now the models are ready to be used","84da65a1":"As seen with the small correlation to age with other features, the next goal is to find the best median with other features. Pclass and Survived have low values due to its classification values rather than dynamic values. The next step is to find correlation with classification values ","30a0acc3":">It is odd to find an outlier T when the rest is in alphabetical order ","c4df4943":"There are many titles in the data that are uncommon and can be replaced ","568a4b12":">Next is to drop the catagories that prove no relevance to the data as well as convert some of the string values","48abe9d5":"There seems to be little to no correlation towards the Age and Fare of a Passenger.","8ec635cf":"> The difference in age by sex is too little to be of significance","dc28b977":"# Understanding the Dataset:","f4cfd452":"In regard to age, surivivabilty makes a small different in the start of the graph. Where a young child has the highest chance for survivability ","caf63b01":"As found with all the graphing, the main factors that determine the Age is Pclass, Survived, SibSp","91ed52c8":"# Cleaning up missing Data Values: ","1e0e299d":"Embarked will be used for one hot encoding later on for the prediction model ","af77b8e7":"we can see the most influentical data points for Age is the Cabins, Survived and Pclass. We will focus on the Survived and Pclass as the Cabins have too many missing samples","5934df4c":"Those that travel alone have a lower survivability than those that have family members. Hence, another column will be created based off the amount of siblings ","f9bf370b":"# Creating More Features for the Prediction Model","b7bef94d":"No difference in age from embarked ","72119235":"> Order of the Age to Cabins, G,F,B,C,D,E,A  ","6d65cf25":">The follwing part of the notebook is heavily credited to the works of Manav Sehgal","e6f1439f":"The future of this notebook:\n* increasing organization in both documentation and programming \n* using one hot encoding to increase efficiency of certain models \n* the utilization of deep learning models to create higher accuracy","0bd743aa":"The important datatypes with graphable values is PassengerID, Pclass, Survived, Age, SibSp, Parch, Fare","8e18184c":"confirming that the data for all the Classes are balanced and even","224ae945":"Next is to find if there is a use of the cabins "}}