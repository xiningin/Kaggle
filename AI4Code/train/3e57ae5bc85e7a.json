{"cell_type":{"abdb22ca":"code","3d80304a":"code","e0f17937":"code","dcc4489b":"code","79c6c95e":"code","396df098":"code","01f37e05":"code","3f8fea1d":"code","5ed4aa78":"code","f018c35b":"code","e31d84d3":"code","ff9fe7a2":"code","9ad034b6":"code","fbbd1610":"code","a0e86947":"code","dcc8f13c":"code","7a0e3a3f":"code","e2e7bb03":"code","a06ea86e":"code","9355b423":"code","e20c0670":"code","2ebd221b":"code","c3d11bb5":"code","bb4d53d1":"code","9f6afb8c":"code","2ad0764f":"code","67b56f95":"code","ffe601d4":"code","0a513edd":"code","5024296a":"code","661230c4":"code","99f1c46a":"code","74d59706":"markdown","90a35050":"markdown","a28a5636":"markdown","91ea9ebb":"markdown","54242608":"markdown","a11882cd":"markdown","4f3044f6":"markdown","1a01cd6c":"markdown","b7233d00":"markdown"},"source":{"abdb22ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d80304a":"## visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n## preprocessing\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\n\n## model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, ConfusionMatrixDisplay\n\n## pipeline\nfrom sklearn.pipeline import Pipeline\n\nimport random\n\nseed = random.seed(100)","e0f17937":"path = '..\/input\/drug-classification\/drug200.csv'\ndataLoad = pd.read_csv(path)\ndataLoad.head()","dcc4489b":"dataLoad.info()","79c6c95e":"dataLoad.describe()","396df098":"dataLoad.isnull().sum()","01f37e05":"dataCopy = dataLoad.copy()\ntarget = dataCopy['Drug']\nfeatures = dataCopy.drop('Drug', axis=1)\nfeatures.head()","3f8fea1d":"OH_cols=[]\nfor i in features.columns:\n    if features[i].nunique() < 5 :\n        OH_cols.append(i)\n        \n\nencoder = OneHotEncoder()\nencodedData = encoder.fit_transform(features[OH_cols]).toarray()\nencodedLabel = encoder.categories_\n\nfeatLabel = []\nfor j in range(3):\n    for k in range(len(encodedLabel[j])):\n        label = encodedLabel[j][k]\n        featLabel.append(label)\n\nencoded_df = pd.DataFrame(encodedData, columns=featLabel)\nencoded_df.head()","5ed4aa78":"df_features = pd.concat([features['Age'], features['Na_to_K'], encoded_df], axis=1)\ndf_features.head()","f018c35b":"labelenc = LabelEncoder()\ntargetEncoded = pd.DataFrame(labelenc.fit_transform(target), columns=['Drug'])\ntargetEncoded.head()","e31d84d3":"x_train, x_test, y_train, y_test = train_test_split(df_features, targetEncoded, test_size=0.2)\nX_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=0.2)\nprint(X_train.shape , X_valid.shape)\nprint(Y_train.shape, Y_valid.shape)","ff9fe7a2":"scaler= MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.fit_transform(X_valid)\n\nX_train.shape , X_valid.shape","9ad034b6":"modelBl= KNeighborsClassifier()\nmodelBl.fit(X_train, Y_train.values.ravel())\npredBl = modelBl.predict(X_valid)\nprint(accuracy_score(Y_valid.values.ravel(), predBl))","fbbd1610":"print(classification_report(Y_valid.values.ravel(), predBl))","a0e86947":"cmBl = confusion_matrix(Y_valid.values.ravel(), predBl)\nConfusionMatrixDisplay(cmBl).plot()","dcc8f13c":"Y_valid.value_counts()","7a0e3a3f":"drug = target.value_counts()\nsns.barplot(y=drug, x=drug.index)","e2e7bb03":"cols = ['Age', 'Na_to_K']\n\nfor col in cols:\n    sns.boxplot(y = col, data=dataCopy)\n    plt.show()","a06ea86e":"for u in cols:\n    sns.boxplot(x = 'Drug', y = u, data=dataCopy)\n    plt.show()","9355b423":"## removing outliers on Na_to_K variable\n\nQ1 = dataCopy['Na_to_K'].quantile(0.25)\nQ3 = dataCopy['Na_to_K'].quantile(0.75)\n\nIQR = Q3 - Q1\n\nfor i in range(len(dataCopy['Na_to_K'])):\n    if dataCopy['Na_to_K'][i] < (Q1-IQR*1.5) or dataCopy['Na_to_K'][i] > (Q3 + IQR*1.5):\n        dataCopy['Na_to_K'][i] = np.nan\n    else:\n        continue\n\ndataCopy['Na_to_K'].isnull().sum()","e20c0670":"## Imputing nan value with mean\nimputer = SimpleImputer(strategy='mean')\n\nnaClean = pd.DataFrame(imputer.fit_transform(dataCopy[['Na_to_K']]), columns=['Na_to_K'])\nnaClean.isnull().sum()","2ebd221b":"df_feature= df_features.drop('Na_to_K', axis=1)\ndf_feature.head()","c3d11bb5":"naClean.index = df_feature.index\n\ndf_Feature =pd.concat([df_feature, naClean], axis=1)\ndf_Feature.head()","bb4d53d1":"x_train, x_test, y_train, y_test = train_test_split(df_Feature, targetEncoded, test_size=0.2)\nX_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=0.2)\nprint(X_train.shape , X_valid.shape)\nprint(Y_train.shape, Y_valid.shape)","9f6afb8c":"scaler= MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.fit_transform(X_valid)\n\nX_train.shape , X_valid.shape","2ad0764f":"params ={'n_neighbors':[3, 5, 7, 9]}\n\nmodel = KNeighborsClassifier()\nknnGrid = GridSearchCV(model, params, cv=5)\n\nknnGrid.fit(X_train, Y_train.values.ravel())\n\nprint(knnGrid.best_score_)\nprint(knnGrid.best_params_)","67b56f95":"optmodel = KNeighborsClassifier(n_neighbors=3)\noptmodel.fit(X_train, Y_train.values.ravel())\noptpred = optmodel.predict(X_valid)\nprint(accuracy_score(Y_valid.values.ravel(), optpred))","ffe601d4":"print(classification_report(Y_valid.values.ravel(), optpred))","0a513edd":"cmopt = confusion_matrix(Y_valid.values.ravel(), optpred)\ncmviz = ConfusionMatrixDisplay(cmopt).plot()","5024296a":"my_pipeline = Pipeline(steps=[('scaler', MinMaxScaler()), ('model', optmodel)])\n\nmy_pipeline.fit(x_train, y_train.values.ravel())\npred = my_pipeline.predict(x_test)\n\nprint(accuracy_score(y_test.values.ravel(), pred))","661230c4":"print(classification_report(y_test.values.ravel(), pred))","99f1c46a":"cm = confusion_matrix(y_test.values.ravel(), pred)\ncmviz = ConfusionMatrixDisplay(cm).plot()","74d59706":"## Preprocessing","90a35050":"## Baseline Model","a28a5636":"## Model Optimization","91ea9ebb":"The next step we remove an outliers from the data. It because KNN model is very sensitive with outliers.","54242608":"## Loading Dataset and Basic Analysis","a11882cd":"## Exploratory Data Analysis","4f3044f6":"Then, we are trying to get optimum parameter for our KNN model. In this case I will try to search the most optimum k number. I did it with gridcv method","1a01cd6c":"## Removing Outliers","b7233d00":"## Making pipeline"}}