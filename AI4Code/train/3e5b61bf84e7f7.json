{"cell_type":{"4bf790ce":"code","3e1b9133":"code","2f4a0db4":"code","25310800":"code","4ffa1326":"code","d25153e4":"code","e7baf947":"code","0e8f9544":"code","20ee57c5":"code","5e6b043c":"code","32f84f0f":"code","0ec01f57":"code","b2fb3c97":"code","40570bd8":"code","6f1804ef":"code","0a96846c":"code","edab03ed":"code","6f8b83b7":"code","6d813019":"code","b88f039a":"code","7d7c6db5":"code","df0a3d42":"code","e3c5fc27":"code","d6ed6ae1":"code","6736e93e":"code","88355eee":"code","eb780723":"code","638f7512":"code","84e45b18":"code","8166780c":"code","44e6ab1f":"code","3d8cfe3e":"code","cd9212e1":"code","e9d06cb3":"code","4e6daffa":"code","9ce90f7c":"code","f40ab354":"code","92b2f92d":"code","aed517df":"code","be8d521b":"code","c65c8dd8":"code","9b2054d2":"code","1f8d9ac6":"code","d65c744a":"code","fcafaecd":"code","d93d7ce3":"code","b3f75041":"code","b717870e":"code","fa3807c8":"code","57721a3b":"code","b7ad3455":"code","ef614cb0":"code","edfa81bf":"code","46733ccc":"code","a93029ce":"code","f6652c0e":"code","a4d5c09f":"code","475bc891":"code","b9df5837":"code","0fc34664":"code","f7ac1efa":"code","56026d07":"code","20773f15":"markdown","f24ae906":"markdown","39d9e302":"markdown","28322961":"markdown","2e5978bf":"markdown","3cc04eff":"markdown","e1487e76":"markdown","c3df63c1":"markdown","9c23ad8c":"markdown","43ccc22f":"markdown","28de1665":"markdown","2eb9042c":"markdown","2bcf3988":"markdown","cef73923":"markdown","8c6a71f9":"markdown","62dec3e4":"markdown","cb93e3b6":"markdown","c13c9ba0":"markdown","62b2d639":"markdown","14e0c9f2":"markdown","907ee97f":"markdown","0810e0fa":"markdown","823d6d65":"markdown","e5e112f4":"markdown","365f3343":"markdown","abd811f8":"markdown","6d8758a2":"markdown","84b54062":"markdown","480b46eb":"markdown","b0e82454":"markdown","cb07ebd1":"markdown","8ad1a193":"markdown","16ff74f1":"markdown","458a8c47":"markdown","1dfa597d":"markdown","a7141318":"markdown","ef105f1e":"markdown","cfc02a9c":"markdown","953ddf9d":"markdown","f54861a0":"markdown","fb43d6f8":"markdown","a431e819":"markdown","0df9df9d":"markdown","01f7a53c":"markdown","7d640f00":"markdown","3cc8bed7":"markdown","0644a59e":"markdown","eda4fb66":"markdown","c333feb6":"markdown","8b1d1c6f":"markdown","3725be08":"markdown","4be14401":"markdown","518de34c":"markdown","f711f51e":"markdown","a5de69ad":"markdown"},"source":{"4bf790ce":"# Python version\nimport sys\nprint('Python: {}'.format(sys.version))\n# scipy\nimport scipy\nprint('scipy: {}'.format(scipy.__version__))\n# numpy\nimport numpy\nprint('numpy: {}'.format(numpy.__version__))\n# matplotlib\nimport matplotlib\nprint('matplotlib: {}'.format(matplotlib.__version__))\n# pandas\nimport pandas\nprint('pandas: {}'.format(pandas.__version__))\n# scikit-learn\nimport sklearn\nprint('sklearn: {}'.format(sklearn.__version__))","3e1b9133":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # using for plots\nimport seaborn as sns #using for plots\n%matplotlib inline\nfrom pandas.plotting import scatter_matrix\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split # split train and test sets\n\nfrom sklearn.preprocessing import StandardScaler # for scaling \nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n# Gradient Boosting Machine\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Cross Validation Score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\n\n# import warnings\n# warnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","2f4a0db4":"# Load dataset\npath =\"..\/input\/Iris.csv\"\ndataset = pd.read_csv(path)","25310800":"# shape\nprint(dataset.shape)","4ffa1326":"# head\ndataset.head(10)","d25153e4":"dataset.tail(10)","e7baf947":"dataset.columns","0e8f9544":"dataset.describe()","20ee57c5":"dataset.cov()","5e6b043c":"dataset.corr()","32f84f0f":"#Remove a column from the data\ndataset = dataset.drop('Id',axis=1)","0ec01f57":"dataset.head()","b2fb3c97":"dataset.describe()","40570bd8":"dataset.isnull().sum()","6f1804ef":"dataset.dtypes","0a96846c":"# class distribution\ndataset.groupby('Species').size()\n# dataset.Species.value_counts()","edab03ed":"# box and whisker plots\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False,color=\"green\", figsize=(10,10))\nplt.show()","6f8b83b7":"sns.boxplot(data=dataset)","6d813019":"# histograms\ndataset.hist(color=\"purple\",figsize=(10,10))\nplt.show()","b88f039a":"# scatter plot matrix\n# scatter_matrix(dataset,color=\"green\",figsize =(12,12))\n# # use suptitle to add title to all sublots\n# plt.suptitle(\"Pair Plot\", fontsize=20)\n# plt.show()","7d7c6db5":"sns.pairplot(dataset, hue=\"Species\", height=3, diag_kind=\"kde\")\nplt.show()","df0a3d42":"# print the mean for each column by species\ndataset.groupby(by = \"Species\").mean()\n# plot for mean of each feature for each label class\ndataset.groupby(by = \"Species\").mean().plot(kind=\"bar\")\nplt.title('Class vs Measurements')\nplt.ylabel('mean measurement(cm)')\nplt.xticks(rotation=0)  # manage the xticks rotation\nplt.grid(True)\n# Use bbox_to_anchor option to place the legend outside plot area to be tidy\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))","e3c5fc27":"# create correlation matrix\ncorr = dataset.corr()\nprint(corr)\nimport statsmodels.api as sm\nsm.graphics.plot_corr(corr, xnames=list(corr.columns))\nplt.show()","d6ed6ae1":"dataset.Species = dataset.Species.astype('category')","6736e93e":"dataset.info()","88355eee":"dataset.Species.cat.codes.head()","eb780723":"dataset.Species = dataset.Species.cat.codes","638f7512":"dataset.Species.head()","84e45b18":"dataset.Species.tail()","8166780c":"dataset.columns.values","44e6ab1f":"# Split-out validation dataset\narray = dataset.values\nX = array[:,0:4]\nY = array[:,4]\nX_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=0.2, \nrandom_state=42)","3d8cfe3e":"X_train.shape[0],Y_train.shape[0]","cd9212e1":"X_validation.shape[0],Y_validation.shape[0]","e9d06cb3":"scalar = StandardScaler()\nX_train = scalar.fit_transform(X_train)\nX_validation = scalar.transform(X_validation)","4e6daffa":"X_train[:5]","9ce90f7c":"logis = LogisticRegression()\nlogis.fit(X_train,Y_train)\nprediction=logis.predict(X_validation)\nprint(\"logistic regression::\\n\",confusion_matrix(Y_validation,prediction),\"\\n\")","f40ab354":"svm = SVC()\nsvm.fit(X_train,Y_train)\nprediction=svm.predict(X_validation)\nprint(\"SVM ::\\n\",confusion_matrix(Y_validation,prediction),\"\\n\")","92b2f92d":"    knn = KNeighborsClassifier()\n    knn.fit(X_train,Y_train)\n    prediction=knn.predict(X_validation)\n    print(\"KNN ::\\n\",confusion_matrix(Y_validation,prediction),\"\\n\")","aed517df":"dTmodel = DecisionTreeClassifier()\ndTmodel.fit(X_train,Y_train)\nprediction=dTmodel.predict(X_validation)\nprint(\"DecisionTree ::\\n\",confusion_matrix(Y_validation,prediction),\"\\n\")","be8d521b":"rForest = RandomForestClassifier()\nrForest.fit(X_train,Y_train)\nprediction=rForest.predict(X_validation)\nprint(\"RandomForest ::\\n\",confusion_matrix(Y_validation,prediction),\"\\n\")","c65c8dd8":"grBoosting = GradientBoostingClassifier()\ngrBoosting.fit(X_train,Y_train)\nprediction=grBoosting.predict(X_validation)\nprint(\"GradientBoosting ::\\n\",confusion_matrix(Y_validation,prediction))","9b2054d2":"#using cross_val_score\nlogis = LogisticRegression()\n    \nscores = cross_val_score(logis,X_train,Y_train,cv=5)\nprint(\"Accuracy for logistic regresion: mean: {0:.2f} 2sd: {1:.2f}\".format(scores.mean(),scores.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores2 = cross_val_score(svm,X_train,Y_train,cv=5)\nprint(\"Accuracy for SVM: mean: {0:.2f} 2sd: {1:.2f}\".format(scores2.mean(),scores2.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores3 = cross_val_score(knn,X_train,Y_train,cv=5)\nprint(\"Accuracy for KNN: mean: {0:.2f} 2sd: {1:.2f}\".format(scores3.mean(),scores3.std() * 2))\nprint(\"Scores::\",scores)\nprint(\"\\n\")\n\nscores4 = cross_val_score(dTmodel,X_train,Y_train,cv=5)\nprint(\"Accuracy for Decision Tree: mean: {0:.2f} 2sd: {1:.2f}\".format(scores4.mean(),scores4.std() * 2))\nprint(\"Scores::\",scores4)\nprint(\"\\n\")\n\nscores5 = cross_val_score(rForest,X_train,Y_train,cv=5)\nprint(\"Accuracy for Random Forest: mean: {0:.2f} 2sd: {1:.2f}\".format(scores5.mean(),scores5.std() * 2))\nprint(\"Scores::\",scores5)\nprint(\"\\n\")\n\nscores6 = cross_val_score(grBoosting,X_train,Y_train,cv=5)\nprint(\"Accuracy for Gradient Boosting: mean: {0:.2f} 2sd: {1:.2f}\".format(scores6.mean(),scores6.std() * 2))\nprint(\"Scores::\",scores6)\nprint(\"\\n\")","1f8d9ac6":"#Here we are testing various predictive algorithms from scikit-learn\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('Random Forest', RandomForestClassifier()))\nmodels.append(('Gradient Boosting', GradientBoostingClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=42)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()))","d65c744a":"clf = RandomForestClassifier()\n#Random Forest\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": sp_randint(1, 4),\n              \"min_samples_split\": sp_randint(2, 4),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run randomized search\nn_iter_search = 5\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5)\n\nrandom_search.fit(X_train, Y_train)\nprint(random_search.best_params_)\nprint(random_search.best_estimator_)\nconfusion_matrix(Y_validation,random_search.predict(X_validation))","fcafaecd":"# use a full grid over all parameters\nparam_grid = {\"max_depth\": [3, None],\n              \"max_features\": [1, 3, 4],\n              \"min_samples_split\": [2, 3, 4],\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run grid search\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=5)\n\ngrid_search.fit(X_train, Y_train)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)\nconfusion_matrix(Y_validation,grid_search.predict(X_validation))","d93d7ce3":"# Compare Algorithms\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","b3f75041":"# Make predictions on validation dataset\nknn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\npredictions = knn.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","b717870e":"# Make predictions on validation dataset\nsvn = SVC()\nsvn.fit(X_train, Y_train)\npredictions = svn.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","fa3807c8":"#Input Vector \nX_new = numpy.array([[1, 2.1, 4, 0.2], [  4.7, 3, 1.3, 0.2 ],[  3.1, 1, 2.3, 0.3 ]])\nprint(\"X_new.shape: {}\".format(X_new.shape))","57721a3b":"prediction = svn.predict(X_new)","b7ad3455":"#Prediction of the species from the input vector\nprint(\"Prediction of Species: {}\".format(prediction))","ef614cb0":"# encode string class values as integers\nlabel_encoder = LabelEncoder()\nlabel_encoder_t = label_encoder.fit(Y_train)\nlabel_encoder_v = label_encoder.fit(Y_validation)\nlabel_encoded_yt = label_encoder.transform(Y_train)\nlabel_encoded_yv = label_encoder.transform(Y_validation)","edfa81bf":"dtrain = xgb.DMatrix(X_train, label=label_encoded_yt)\ndtest = xgb.DMatrix(X_validation, label=label_encoded_yv)","46733ccc":"from sklearn.datasets import dump_svmlight_file\n\ndump_svmlight_file(X_train, label_encoded_yt, 'dtrain.svm', zero_based=True)\ndump_svmlight_file(X_validation, label_encoded_yv, 'dtest.svm', zero_based=True)\ndtrain_svm = xgb.DMatrix('dtrain.svm')\ndtest_svm = xgb.DMatrix('dtest.svm')","a93029ce":"param = {\n    'max_depth': 3,  # the maximum depth of each tree\n    'eta': 0.3,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 3}  # the number of classes that exist in this datset\nnum_round = 20  # the number of training iterations","f6652c0e":"bst = xgb.train(param, dtrain, num_round)","a4d5c09f":"# To see how the model looks you can also dump it in human readable form:\n# bst.dump_model('dump.raw.txt')","475bc891":"preds = bst.predict(dtest)\npreds","b9df5837":"import numpy as np\nbest_preds = np.asarray([np.argmax(line) for line in preds])","0fc34664":"best_preds","f7ac1efa":"from sklearn.metrics import precision_score\nprint(precision_score(label_encoded_yv, best_preds, average='macro'))","56026d07":"from sklearn.externals import joblib\njoblib.dump(bst, 'bst_model.pkl', compress=True)\n# bst = joblib.load('bst_model.pkl') # load it later","20773f15":"### 2.6 Breakdown of the data by the class variable.","f24ae906":"GridSearchCV and RandomizedSearchCV are ways to tune hyper parameters.","39d9e302":"####  Histogram of each input variable to get an idea of the distribution.","28322961":"Here each column represents class number 0, 1, or 2. For each line we need to select that column where the probability is the highest:","2e5978bf":"Now we check again data types using info command. now it shows type as category","3cc04eff":"checking column data types using dtypes. Our dataset contains one int, 4 float and 1 object variables. 'Species' is our target varible.","e1487e76":"**Determine the precision of this prediction:**","c3df63c1":"### 2.4 Covariance on data\n`cov()` - Covariance indicates how two variables are related. A positive\ncovariance means the variables are positively related, while a negative\ncovariance means the variables are inversely related. Drawback of covariance\nis that it does not tell you the degree of positive or negative relation","9c23ad8c":"**Perfect! Now save the model for later use:**","43ccc22f":"## 3. Data Visualization\n\nWe now have a basic idea about the data. We need to extend that with some visualizations.\n\nWe are going to look at two types of plots:\n\n   1. Univariate plots to better understand each attribute.\n   2.  Multivariate plots to better understand the relationships between attributes.\n","28de1665":"**Use the model to predict classes for the test set:**","2eb9042c":"#### Pair Plot\nYou can understand the relationship attributes by looking at the distribution of the\ninteractions of each pair of attributes. This uses a built-in function to create a matrix of\nscatter plots of all attributes against all attributes.","2bcf3988":"We can access converted values using codes property.","cef73923":"Using seaboarns package we can show rich plots. I used pairplot function to show scatter plots.","8c6a71f9":"Now I am going to convert target variable as category. we can follow different approaches for this convertion. different approaches as follows\n* LableEncoder()\n* map (We use map for ordinal category columns. You can specify the order)\n* type conversion using astype\n\n","62dec3e4":"## 4. Evaluate Some Algorithms\n\nNow it is time to create some models of the data and estimate their accuracy on unseen data.\nwe are going to cover in this step:\n\n* 1. Separate out a validation dataset.\n* 2. Set-up the test harness to use 10-fold cross validation.\n* 3. Build 5 different models to predict species from flower measurements\n* 4. Select the best model.\n","cb93e3b6":"### Test predictions in data input\n","c13c9ba0":"Check any null values in the datase using isnull and sum. Out data set not have any null\/empty values.","62b2d639":"## 6. Start with XGBoost","14e0c9f2":"**More to come . Please upvote if you find it useful.**","907ee97f":"**XGBoost is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification.**","0810e0fa":"### 2.5 Correlation matrix of data ","823d6d65":"### Train\n \nFinally the training can begin. ","e5e112f4":"### 4.3 Build Models\nLet\u2019s evaluate 6 different algorithms:\n*  Logistic Regression (LR)\n* Linear Discriminant Analysis (LDA)\n* K-Nearest Neighbors (KNN).\n* Classification and Regression Trees (CART).\n* Gaussian Naive Bayes (NB).\n* Support Vector Machines (SVM).\n","365f3343":"**Encode string class values as integers**","abd811f8":"### 3.2 Multivariate Plots","6d8758a2":"### 4.1 Create a Validation Dataset\n**Split the data into train and test sets with 80-20%**","84b54062":"## 2. Summarize the Dataset\n\nNow it is time to take a look at the data.\n\n* 2.1. Dimensions of the dataset.\n* 2.2 Peek at the data itself.\n* 2.3 Statistical summary of data.\n* 2.4 Covariance on data.\n* 2.5 Correlation of data.\n* 2.6 Breakdown of the data by the class variable.\n","480b46eb":"Using StandardScalar function scale all numarical variables. If we have any categorival variables we use OneHotEncoder() for create dummy variables. We can use pandas get_dummies also for to create dummy variables.","b0e82454":"## 1. Loading the dataset","cb07ebd1":"### Test result prediction","8ad1a193":"Box plot for all numarical variables","16ff74f1":"###  2.3 Basic statistics on data","458a8c47":"checking last 10 rows using tail. By default tail shows last 5 rows.","1dfa597d":"Using **GridSearchCV** to tune parameters","a7141318":"### 2.2 Peek at the data itself.","ef105f1e":"### 2.1 Dimensions of the dataset.","cfc02a9c":"### Correlation Matrix\nThe correlation function uses Pearson correlation coefficient, which results in a number\nbetween -1 to 1. A strong negative relationship is indicated by a coefficient closer to -1\nand a strong positive correlation is indicated by a coefficient toward 1","953ddf9d":"### 1.1 Check the versions of libraries","f54861a0":"### 1.2 Import libraries","fb43d6f8":"**Make predictions using KNN**","a431e819":"**Use svmlight for less memory consumption, first dump the numpy array into svmlight format and then just pass the filename to DMatrix:**","0df9df9d":"## 5. Make Predictions","01f7a53c":"**Make predictions using SVM**","7d640f00":"### 4.4 Select Best Model","3cc8bed7":"Placing converted values back to Species column.","0644a59e":"Now we get a nice list with predicted classes:","eda4fb66":"### 4.2 Test Harness\n\nWe will use 10-fold cross validation to estimate accuracy.\nThis will split our dataset into 10 parts, train on 9 and test on 1 and repeat for all combinations of train-test splits.","c333feb6":"**Create the Xgboost specific DMatrix data format from the numpy array. **","8b1d1c6f":"### 1.3 Load the dataset","3725be08":"### 3.1 Univariate Plots","4be14401":"**Set the parameters:**","518de34c":"## Contents\n** 1. Loading the dataset.**\n\n** 2. Summarizing the dataset.**\n\n** 3. Visualizing the dataset.**\n\n** 4. Evaluating algorithms.**\n\n** 5. Making predictions.**\n\n** 6. Start with XGBoost**\n","f711f51e":"Different datasets perform better with different parameters.","a5de69ad":"Displaying columns in the dataset using columns property"}}