{"cell_type":{"9c0e9084":"code","650e53dc":"code","fb72410d":"code","763b299b":"code","b57a4f40":"code","b34716d4":"code","973179cd":"code","7a1ed419":"code","f854e00a":"code","9c801d42":"code","0a2bd43a":"code","7c1607e4":"code","407f6af6":"code","ee2f6690":"code","c3cf8c25":"code","61cbe67d":"code","e90c786e":"code","75b20801":"code","1cff7cab":"code","4914802e":"code","7affeeac":"code","9bd22dfb":"code","51ae3505":"code","fe1560cd":"code","1bf3bbcc":"code","8f2aeb6b":"code","1e4b41ac":"code","d3ed96ae":"code","4e75d21e":"code","f2627ce8":"code","fe8d2515":"code","79c8fe97":"code","85f20f4a":"code","f69906a3":"code","ecf4c6ff":"code","fc365e90":"code","458c3858":"code","150edb81":"code","ad38435d":"code","9f780bca":"code","ce3bc173":"code","1d76da88":"code","353e5e91":"code","b95c3f87":"code","45333b5f":"code","1f36a81d":"code","b48f5080":"code","6bebb541":"code","0a5abccd":"code","ada4179b":"code","22360605":"code","7db76fb3":"code","cb4c22c3":"code","021c5c81":"code","94ed4e31":"code","f9365fec":"code","0978c9b5":"code","56eb05e1":"code","671f380a":"code","b1e4bd59":"code","610496c6":"code","d589a66a":"markdown","e3888d6f":"markdown","f75ebbab":"markdown","5d5820c2":"markdown","92439258":"markdown","979dd318":"markdown","24d74a80":"markdown","8993839f":"markdown","a84a0904":"markdown","de237a72":"markdown","8a70f05a":"markdown"},"source":{"9c0e9084":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt","650e53dc":"#TODO: Statistical checks\n# Conditional Heteroskedasticity occurs when the error terms (the difference between a predicted value by a regression and the real value) are dependent on the data \u2014 for example, the error terms grow when the data point (along the x-axis) grow.\n# Multicollinearity is when error terms (also called residuals) depend on each other.\n# Serial correlation is when one data (feature) is a formula (or completely depends) of another feature.\n\n# Feature importance with XGBoost","fb72410d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","763b299b":"hfmd_data = pd.read_csv('\/kaggle\/input\/hfmd-vietnam\/hfmd_data_analysis\/hfmd_data_analysis\/hfmd_data_province.csv', header=0, names=['PROVINCE', 'MONTH', 'YEAR', 'TOTAL_CASES'])\n\nclimate_data = pd.read_csv('\/kaggle\/input\/hfmd-vietnam\/hfmd_data_analysis\/hfmd_data_analysis\/climate_per_year_per_month.csv')\nclimate_data.drop(climate_data.columns[0], axis=1, inplace=True)\n\nsocial_data = pd.read_csv('\/kaggle\/input\/hfmd-vietnam\/hfmd_data_analysis\/hfmd_data_analysis\/social_data.csv', delimiter=',')","b57a4f40":"social_data","b34716d4":"data = pd.merge(hfmd_data, climate_data, how='inner', on=['PROVINCE', 'MONTH', 'YEAR'])","973179cd":"data = pd.merge(social_data[['PROVINCE', 'YEAR', 'POPULATION_DENSITY']], data, how='inner', on=['PROVINCE', 'YEAR'])","7a1ed419":"data.describe()","f854e00a":"feature_columns = ['PRECTOT',\t'PS',\t'QV2M',\t'RH2M', 'T2M', 'T2MWET', 'T2M_MAX', 'T2M_MIN',\t'T2M_RANGE', 'TS', 'WS10M',\t'WS10M_MAX',\t'WS10M_MIN',\t'WS10M_RANGE',\t'WS50M',\t'WS50M_MAX',\t'WS50M_MIN', 'WS50M_RANGE', 'POPULATION_DENSITY']\ntarget = data['TOTAL_CASES']\nfeature_data = data[feature_columns]","9c801d42":"corr = data.corr()\n\nplt.figure(figsize=(15, 8))\n\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True,\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","0a2bd43a":"data['PROVINCE'].unique()","7c1607e4":"total_monthly_record = []\nfor i in range(1, 13):\n    total_monthly_record.append(data[(data['YEAR'] == 2016) & (data['MONTH'] == i)]['TOTAL_CASES'].sum())\n    \nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(list(range(1, 13)), total_monthly_record)","407f6af6":"total_monthly_record = []\nfor i in range(1, 13):\n    total_monthly_record.append(data[(data['YEAR'] == 2017) & (data['MONTH'] == i)]['TOTAL_CASES'].sum())\n    \nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(list(range(1, 13)), total_monthly_record)","ee2f6690":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error","c3cf8c25":"X_train, X_test, y_train, y_test = train_test_split(feature_data, target, train_size=0.8, stratify=data['PROVINCE'], random_state=42)","61cbe67d":"feature_scaler = StandardScaler()\nfeature_scaler.fit(X_train)\n\ntarget_scaler = StandardScaler()\ntarget_scaler.fit(np.array(y_train).reshape((-1, 1)))\n\nX_train_preprocess = feature_scaler.transform(X_train)\nX_test_preprocess = feature_scaler.transform(X_test)\n\ny_train_preprocess = target_scaler.transform(np.array(y_train).reshape((-1, 1)))\ny_test_preprocess = target_scaler.transform(np.array(y_test).reshape((-1, 1)))","e90c786e":"from sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","75b20801":"def mean_absolute_percentage_error(y_true, y_pred):\n  \n  '''\n    Calculate mean absolute percentage error loss\n  '''\n\n  # y_true, y_pred = check_arrays(y_true, y_pred)\n  return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","1cff7cab":"linear = LinearRegression()\nparameters = {}\nlinear_grid_search_cv = GridSearchCV(linear, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\nlinear_grid_search_cv.fit(X_train_preprocess, y_train_preprocess)","4914802e":"print(f'Best score (MAE): {linear_grid_search_cv.best_score_}')\nprint(f'Best parameters: {linear_grid_search_cv.best_params_}')","7affeeac":"linear_best = linear_grid_search_cv.best_estimator_\ncoef = linear_best.coef_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(coef[0], X_train.columns)","9bd22dfb":"y_pred = linear_best.predict(X_test_preprocess)\n\nmae = mean_absolute_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nmape = mean_absolute_percentage_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nprint(f'MAE loss:{mae}')\nprint(f'MAPE loss:{mape}')","51ae3505":"tree = DecisionTreeRegressor()\nparameters = {'criterion': ['mse', 'mae'], \n              'max_depth': [2, 5, 8, 16, 20],\n              'min_samples_split': [2, 4]}\ntree_grid_search_cv = GridSearchCV(tree, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\ntree_grid_search_cv.fit(X_train_preprocess, y_train_preprocess)","fe1560cd":"print(f'Best score (MAE): {tree_grid_search_cv.best_score_}')\nprint(f'Best parameters: {tree_grid_search_cv.best_params_}')","1bf3bbcc":"tree_best = tree_grid_search_cv.best_estimator_\nfeature_importances = tree_best.feature_importances_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(feature_importances, X_train.columns)","8f2aeb6b":"y_pred = tree_best.predict(X_test_preprocess)\n\nmae = mean_absolute_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nmape = mean_absolute_percentage_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nprint(f'MAE loss:{mae}')\nprint(f'MAPE loss:{mape}')","1e4b41ac":"from sklearn.tree import export_graphviz\nimport graphviz \ndot_data = export_graphviz(tree_best, out_file='no_feature_engineering_tree.dot') ","d3ed96ae":"forest = RandomForestRegressor(n_jobs=-1)\nparameters = {'n_estimators': [10, 30, 50, 100, 500], \n              'max_depth': [2, 5, 8, 10, 15],\n              'min_samples_split': [2, 4]}\nforest_grid_search_cv = GridSearchCV(forest, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\nforest_grid_search_cv.fit(X_train_preprocess, y_train_preprocess)","4e75d21e":"print(f'Best score (MAE): {forest_grid_search_cv.best_score_}')\nprint(f'Best parameters: {forest_grid_search_cv.best_params_}')","f2627ce8":"forest_best = forest_grid_search_cv.best_estimator_\nfeature_importances = forest_best.feature_importances_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(feature_importances, X_train.columns)","fe8d2515":"y_pred = forest_best.predict(X_test_preprocess)\n\nmae = mean_absolute_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nmape = mean_absolute_percentage_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nprint(f'MAE loss:{mae}')\nprint(f'MAPE loss:{mape}')","79c8fe97":"feature_columns = ['PRECTOT',\t'PS',\t'QV2M',\t'RH2M', 'T2M', 'T2MWET', 'T2M_MAX', 'T2M_MIN',\t'T2M_RANGE', 'TS', 'WS10M',\t'WS10M_MAX',\t'WS10M_MIN',\t'WS10M_RANGE',\t'WS50M',\t'WS50M_MAX',\t'WS50M_MIN', 'WS50M_RANGE']\nlinear_feature_columns_2 = ['PS', 'T2M', 'T2MWET', 'T2M_MAX', 'T2M_MIN',\t'T2M_RANGE', 'TS']\nlinear_feature_columns = ['T2M_MAX', 'TS', 'WS10M_MAX']\ntarget = data['TOTAL_CASES']\n\n# target = data['CASE_OVER_POPULATION']\nfeature_data = data[linear_feature_columns_2]","85f20f4a":"X_train, X_test, y_train, y_test = train_test_split(feature_data, target, train_size=0.8, stratify=data['PROVINCE'], random_state=42)","f69906a3":"feature_scaler = StandardScaler()\nfeature_scaler.fit(X_train)\n\ntarget_scaler = StandardScaler()\ntarget_scaler.fit(np.array(y_train).reshape((-1, 1)))\n\nX_train_preprocess = feature_scaler.transform(X_train)\nX_test_preprocess = feature_scaler.transform(X_test)\n\ny_train_preprocess = target_scaler.transform(np.array(y_train).reshape((-1, 1)))\ny_test_preprocess = target_scaler.transform(np.array(y_test).reshape((-1, 1)))","ecf4c6ff":"linear = LinearRegression()\nparameters = {}\nlinear_grid_search_cv = GridSearchCV(linear, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\nlinear_grid_search_cv.fit(X_train_preprocess, y_train_preprocess)","fc365e90":"print(f'Best score (MAE): {linear_grid_search_cv.best_score_}')\nprint(f'Best parameters: {linear_grid_search_cv.best_params_}')","458c3858":"linear_best = linear_grid_search_cv.best_estimator_\ncoef = linear_best.coef_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(coef[0], X_train.columns)","150edb81":"y_pred = linear_best.predict(X_test_preprocess)\n\nmae = mean_absolute_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nmape = mean_absolute_percentage_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nprint(f'MAE loss:{mae}')\nprint(f'MAPE loss:{mape}')","ad38435d":"tree = DecisionTreeRegressor()\nparameters = {'criterion': ['mse', 'mae'], \n              'max_depth': [2, 5, 8, 10],\n              'min_samples_split': [2, 4]}\ntree_grid_search_cv = GridSearchCV(tree, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\ntree_grid_search_cv.fit(X_train_preprocess, y_train_preprocess)","9f780bca":"print(f'Best score (MAE): {tree_grid_search_cv.best_score_}')\nprint(f'Best parameters: {tree_grid_search_cv.best_params_}')","ce3bc173":"tree_best = tree_grid_search_cv.best_estimator_\nfeature_importances = tree_best.feature_importances_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(feature_importances, X_train.columns)","1d76da88":"y_pred = tree_best.predict(X_test_preprocess)\n\nmae = mean_absolute_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nmape = mean_absolute_percentage_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nprint(f'MAE loss:{mae}')\nprint(f'MAPE loss:{mape}')","353e5e91":"from sklearn.tree import export_graphviz\nimport graphviz \ndot_data = export_graphviz(tree_best, out_file='16_17_feature_engineering_tree.dot') ","b95c3f87":"forest = RandomForestRegressor(n_jobs=-1)\nparameters = {'n_estimators': [10, 30, 50, 100, 500, 1000],\n              'max_depth': [2, 5, 8, 10, 15],\n              'min_samples_split': [2, 4]}\nforest_grid_search_cv = GridSearchCV(forest, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\nforest_grid_search_cv.fit(X_train_preprocess, y_train_preprocess)","45333b5f":"print(f'Best score (MAE): {forest_grid_search_cv.best_score_}')\nprint(f'Best parameters: {forest_grid_search_cv.best_params_}')","1f36a81d":"forest_best = forest_grid_search_cv.best_estimator_\nfeature_importances = forest_best.feature_importances_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(feature_importances, X_train.columns)","b48f5080":"y_pred = forest_best.predict(X_test_preprocess)\n\nmae = mean_absolute_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nmape = mean_absolute_percentage_error(target_scaler.inverse_transform(y_test_preprocess), target_scaler.inverse_transform(y_pred))\n\nprint(f'MAE loss:{mae}')\nprint(f'MAPE loss:{mape}')","6bebb541":"data['PROVINCE'].unique()","0a5abccd":"PROVINCE = 'BENTRE'\n\n# feature_columns = ['PRECTOT', 'PS', 'QV2M', 'RH2M', 'T2M', 'T2MWET', 'WS10M', 'WS50M']\nfeature_columns = ['PRECTOT', 'PS', 'QV2M', 'RH2M', 'T2M', 'T2MWET', 'WS10M']\ntarget = data['TOTAL_CASES']\n\n# target = data['CASE_OVER_POPULATION']\nfeature_data = data[feature_columns]","ada4179b":"province_feature = feature_data[data['PROVINCE'] == PROVINCE]\nprovince_target = target[data['PROVINCE'] == PROVINCE]","22360605":"data[data['PROVINCE'] == PROVINCE]","7db76fb3":"feature_scaler = StandardScaler()\nprovince_feature_preprocess = feature_scaler.fit_transform(province_feature)","cb4c22c3":"linear = LinearRegression()\nparameters = {}\nlinear_grid_search_cv = GridSearchCV(linear, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\nlinear_grid_search_cv.fit(province_feature_preprocess, province_target)","021c5c81":"print(f'Best score (MAE): {linear_grid_search_cv.best_score_}')\nprint(f'Best parameters: {linear_grid_search_cv.best_params_}')","94ed4e31":"linear_best = linear_grid_search_cv.best_estimator_\ncoef = linear_best.coef_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(coef, province_feature.columns)","f9365fec":"tree = DecisionTreeRegressor()\nparameters = {'criterion': ['mse', 'mae'], \n              'max_depth': [2, 5, 8, 10],\n              'min_samples_split': [2, 4]}\ntree_grid_search_cv = GridSearchCV(tree, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\ntree_grid_search_cv.fit(province_feature_preprocess, province_target)","0978c9b5":"print(f'Best score (MAE): {tree_grid_search_cv.best_score_}')\nprint(f'Best parameters: {tree_grid_search_cv.best_params_}')","56eb05e1":"tree_best = tree_grid_search_cv.best_estimator_\nfeature_importances = tree_best.feature_importances_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(feature_importances, province_feature.columns)","671f380a":"forest = RandomForestRegressor(n_jobs=-1)\nparameters = {'n_estimators': [10, 30, 50, 100, 500, 1000],\n              'max_depth': [2, 5, 8, 10, 15],\n              'min_samples_split': [2, 4]}\nforest_grid_search_cv = GridSearchCV(forest, parameters, scoring=('neg_mean_absolute_error'), cv=5, verbose=5, n_jobs=-1)\nforest_grid_search_cv.fit(province_feature_preprocess, province_target)","b1e4bd59":"print(f'Best score (MAE): {forest_grid_search_cv.best_score_}')\nprint(f'Best parameters: {forest_grid_search_cv.best_params_}')","610496c6":"forest_best = forest_grid_search_cv.best_estimator_\nfeature_importances = forest_best.feature_importances_\n\nfig, ax = plt.subplots(figsize=(12, 9))\nsns.barplot(feature_importances, province_feature.columns)","d589a66a":"## Decision Tree Regressor\n","e3888d6f":"## Random Forest Regressor","f75ebbab":"# Feature Engineering (2016-2017)","5d5820c2":"## Linear Regression","92439258":"## Random Forest Regressor","979dd318":"## Decision Tree Regressor","24d74a80":"# Train, build model without feature engineering\n","8993839f":"## Linear Regression","a84a0904":"**Climate parameters**\n\n- T2M_RANGE MERRA2 1\/2x1\/2 Temperature Range at 2 Meters (C) \n- PRECTOT MERRA2 1\/2x1\/2 Precipitation (mm day-1) \n- T2MWET MERRA2 1\/2x1\/2 Wet Bulb Temperature at 2 Meters (C) \n- T2M MERRA2 1\/2x1\/2 Temperature at 2 Meters (C) \n- WS50M_MIN MERRA2 1\/2x1\/2 Minimum Wind Speed at 50 Meters (m\/s) \n- PS MERRA2 1\/2x1\/2 Surface Pressure (kPa) \n- T2M_MAX MERRA2 1\/2x1\/2 Maximum Temperature at 2 Meters (C) \n- TS MERRA2 1\/2x1\/2 Earth Skin Temperature (C) \n- WS10M_RANGE MERRA2 1\/2x1\/2 Wind Speed Range at 10 Meters (m\/s) \n- RH2M MERRA2 1\/2x1\/2 Relative Humidity at 2 Meters (%) \n- WS10M_MIN MERRA2 1\/2x1\/2 Minimum Wind Speed at 10 Meters (m\/s) \n- WS10M MERRA2 1\/2x1\/2 Wind Speed at 10 Meters (m\/s) \n- WS50M MERRA2 1\/2x1\/2 Wind Speed at 50 Meters (m\/s) \n- WS50M_MAX MERRA2 1\/2x1\/2 Maximum Wind Speed at 50 Meters (m\/s) \n- T2M_MIN MERRA2 1\/2x1\/2 Minimum Temperature at 2 Meters (C) \n- WS50M_RANGE MERRA2 1\/2x1\/2 Wind Speed Range at 50 Meters (m\/s) \n- WS10M_MAX MERRA2 1\/2x1\/2 Maximum Wind Speed at 10 Meters (m\/s) \n- QV2M MERRA2 1\/2x1\/2 Specific Humidity at 2 Meters (kg kg-1) ","de237a72":"# Per-province","8a70f05a":"# Split and pre-process data"}}