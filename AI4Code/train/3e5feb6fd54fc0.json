{"cell_type":{"2200ae55":"code","e6b6def1":"code","cd2befb9":"code","dfe06015":"code","4cfa615d":"code","48e31422":"code","fa93a979":"code","2d888630":"code","97ce0212":"code","19aa3d09":"code","2bebe70d":"code","f9ae0637":"code","73fe77f0":"code","fecb38b9":"code","d62158ea":"code","34acaa01":"code","6b32994b":"code","16144c63":"code","0fa2a122":"code","f586c93d":"code","ba2fdf78":"code","d91709b5":"code","d608161c":"code","f1021846":"code","7bdc4205":"code","8e3e7b4f":"code","41bbee96":"code","40ef5ea4":"code","22b9b799":"code","3d25ca59":"code","7c7a1879":"code","190002ea":"code","3feba11a":"code","cfe4ae9a":"code","82ef62d9":"code","5ce825e9":"code","f1a28e44":"code","a32a8f52":"code","8c2b2bed":"code","1b72f9a1":"code","15f33420":"code","8b4ceb26":"code","8584b7c8":"code","7c395ae4":"code","74a2defd":"code","71764ff1":"code","343e10b4":"code","47875ffe":"code","70808fb4":"code","ee40812c":"code","7f632703":"code","dcb80039":"code","3403a3cc":"code","62dd14d8":"code","1ccefe49":"code","d46b7d72":"markdown","45c72f57":"markdown","6ee765ac":"markdown","631fc770":"markdown","6a81ef3d":"markdown","3af78741":"markdown","550068f5":"markdown","3e4f4f93":"markdown","a1962cba":"markdown","89da45bf":"markdown","1d24464f":"markdown","06e705d9":"markdown","e057b475":"markdown","687e1021":"markdown","bafcc8b6":"markdown","672dd968":"markdown","ac395116":"markdown","c6b0bbfe":"markdown","aa7f935f":"markdown","504cd5cd":"markdown","a9d6ac76":"markdown","1c629a63":"markdown","1d44d117":"markdown","7f93f3e0":"markdown","9b3c8363":"markdown","e3cfbb0c":"markdown","cd616e09":"markdown","9a7829c6":"markdown","30fdaeea":"markdown","06fdea71":"markdown","a824c0cf":"markdown","23448730":"markdown","fd7afa32":"markdown","79b71c2b":"markdown","f160f6a6":"markdown","7cd892ad":"markdown","08248fa4":"markdown","df03e6e7":"markdown","b8359849":"markdown","5763dde9":"markdown","9411d598":"markdown","4d108d77":"markdown","3c67e132":"markdown","a532f23f":"markdown","fa9e323b":"markdown","e6d3bb86":"markdown","1c76ce5d":"markdown","b9fe1285":"markdown"},"source":{"2200ae55":"import pandas as pd\nimport numpy as np\nimport warnings \nfrom sklearn.preprocessing import Imputer,StandardScaler,LabelEncoder,PolynomialFeatures,MinMaxScaler,OneHotEncoder,RobustScaler\nfrom sklearn.linear_model import LinearRegression,SGDRegressor,Lasso,ElasticNet,Ridge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import TransformerMixin,BaseEstimator\nfrom sklearn_pandas import DataFrameMapper, cross_val_score\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom matplotlib import pyplot\nimport seaborn as sns\nimport pickle\nfrom sklearn.svm import LinearSVR,SVR\nfrom sklearn.model_selection import KFold,GridSearchCV,RandomizedSearchCV,ShuffleSplit,train_test_split,cross_val_predict,cross_validate\nfrom sklearn.feature_selection import RFECV,SelectKBest\nfrom scipy.stats import normaltest,skew,skewtest,boxcox,probplot\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.decomposition import PCA\nfrom mlxtend.regressor import StackingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nimport re\n%pylab inline\npd.set_option('display.max_columns', 500)\nwarnings.filterwarnings('ignore')\n    \ndef make_submission(preds):\n    sample = pd.read_csv('..\/input\/sample_submission.csv')\n    sample['SalePrice'] = preds\n    sample.to_csv('submission.csv',index=False)\n    \ndef rmsle_cv(model):\n    n_folds = 5\n    kf = KFold(n_folds, shuffle=True, random_state=42)\n    rmse= np.sqrt(-cross_val_score(model, train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)\n\n","e6b6def1":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\ntrain_df.drop(\"Id\",1,inplace=True)\ntest_df.drop(\"Id\",1,inplace=True)\n\n#Outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#\ny_train = train_df.SalePrice.values\ntrain_df.drop('SalePrice',axis=1,inplace=True)\n    \nidx_split = train_df.shape[0]\nfull_df = pd.concat((train_df,test_df))\n\n\n","cd2befb9":"#Missing values percentage\nisna_df = (full_df.isnull().sum() \/ full_df.shape[0] * 100).sort_values(ascending=False)\nisna_df = pd.DataFrame(data = isna_df[isna_df.values != 0].values,columns=['Missing Ratio'],index=isna_df[isna_df.values != 0].index)\nisna_df.head()","dfe06015":"#Feature replacement (data description says NA = No such feature in house)\nfull_df['PoolQC'] = full_df['PoolQC'].fillna(\"None\")\nfull_df['MiscFeature'] = full_df['MiscFeature'].fillna(\"None\")\nfull_df['Alley'] = full_df['Alley'].fillna(\"None\")\nfull_df['FireplaceQu'] = full_df['FireplaceQu'].fillna(\"None\")\nfull_df['Fence'] = full_df['Fence'].fillna(\"None\")\n\n#My guess is that Lot frontage could be a median Neighborhood frontage\nfull_df['LotFrontage'] = full_df.groupby(\"Neighborhood\")['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n#No value == no garage\nfor el in ['GarageCond', 'GarageQual','GarageFinish', 'GarageType']:\n    full_df[el].fillna(\"None\",inplace=True)\n\n#Same thing\nfor el in ['GarageYrBlt', 'GarageArea','GarageCars']:\n    full_df[el].fillna(0,inplace=True)\n\n#Just filling this NaNs\nfor el in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    full_df[el].fillna(0,inplace=True)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    full_df[col] = full_df[col].fillna('None')\n    \n\n\nfull_df[\"MasVnrType\"] = full_df[\"MasVnrType\"].fillna(\"None\")\nfull_df[\"MasVnrArea\"] = full_df[\"MasVnrArea\"].fillna(0)\n\nfull_df['MSZoning'] = full_df['MSZoning'].fillna(full_df['MSZoning'].mode()[0])\nfull_df.drop(['Utilities'],1,inplace=True)\n\n#Data descryption says \"Assume typical unless deductions are warranted\"\nfull_df[\"Functional\"] = full_df[\"Functional\"].fillna(\"Typ\")\n\n#Just replace Nan with most typical\nfull_df['Electrical'] = full_df['Electrical'].fillna(full_df['Electrical'].mode()[0])\n\n#Same thing\nfull_df['Exterior1st'] = full_df['Exterior1st'].fillna(full_df['Exterior1st'].mode()[0])\nfull_df['Exterior2nd'] = full_df['Exterior2nd'].fillna(full_df['Exterior2nd'].mode()[0])\n\nfull_df['KitchenQual'] = full_df['KitchenQual'].fillna(full_df['KitchenQual'].mode()[0])\n\nfull_df['MSSubClass'] = full_df['MSSubClass'].fillna(\"None\")\n\nfull_df['SaleType'] = full_df['SaleType'].fillna(full_df['SaleType'].mode()[0])\n","4cfa615d":"#Check are there any NAn values left\n(full_df.isna().sum() \/ full_df.shape[0]).sort_values(ascending=False).head()","48e31422":"#Some numerical values are really categorical\nfull_df['MSSubClass'] = full_df['MSSubClass'].astype(\"str\")\nfull_df['OverallCond'] = full_df['OverallCond'].astype(\"str\")\nfull_df['YrSold'] = full_df['YrSold'].astype(\"str\")\nfull_df['MoSold'] = full_df['MoSold'].astype(\"str\")\n\ncat_cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n#Encode our variables\nfor el in cat_cols:\n    full_df[el] = LabelEncoder().fit_transform(full_df[el])\n    \nprint(full_df.shape)\n        \n","fa93a979":"#Let's add some new variables\nfull_df['TotalSF'] = full_df['TotalBsmtSF'] + full_df['1stFlrSF'] + full_df['2ndFlrSF']\n\n# Numeric relations \nfull_df['Year_built_decades'] = list(map(int,full_df['YearBuilt'].astype('int64') \/ 10))\nfull_df['Year_remodel_decades'] = list(map(int,full_df['YearRemodAdd'].astype('int64') \/ 10))\nfull_df['SF per above room'] = list(map(int,(full_df['GrLivArea'] \/ full_df['TotRmsAbvGrd'])\/10))\n#Binary features\nfull_df['IsNew'] = full_df['YearBuilt'].apply(lambda x: 1 if x >= 1995 else 0)\nfull_df['Remodeled_recently'] = full_df['YearRemodAdd'].apply(lambda x: 1 if x >= 1995  else 0)\nfull_df['Built and remodeled at once'] = (full_df['YearBuilt']  == full_df['YearRemodAdd']).apply(lambda x: 1 if x == True else 0)\n","2d888630":"#Fixing skewness\nnumeric_feats = full_df.dtypes[full_df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = full_df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","97ce0212":"#Box-Cox for some reasons did better job than just log1p\nfrom scipy.special import boxcox1p\nskewed_features = skewness[abs(skewness) > 0.75].index\nlmbda = 0.15\nfor el in skewed_features:\n    full_df[el] = boxcox1p(full_df[el],lmbda)","19aa3d09":"#Save everything so there is no need to run this part of code again\n# full_df.to_csv('clean_full_df.csv',index=False)\n\nfull_df = pd.get_dummies(full_df)\ntrain = full_df.iloc[:idx_split]\ntest = full_df.iloc[idx_split:]\n# full_df.to_csv('full_df_new_features_dummies.csv',index=False)","2bebe70d":"forest = RandomForestRegressor()\nforest.fit(full_df.iloc[:idx_split],y_train)\nimportances = forest.feature_importances_\nimportance_values = sorted(importances,reverse=True)\nimportance_columns = full_df.columns[importances.argsort()[::-1]]\n\nplt.figure(figsize=(20,10))\nplt.title(\"Top 20 features\",fontsize=20)\nplt.bar(range(20),importance_values[:20],color='g',align=\"center\")\nplt.xticks(range(20),importance_columns[:20],rotation=90)\nplt.ylabel(\"Feature importance\")\nplt.xlim([-1,20])","f9ae0637":"# full_df = pd.read_csv('full_df_new_features_dummies.csv')\n# with open('y_train.pickle','rb') as f:\n#     y_train = pickle.load(f)\n# idx_split = 1458\n# train = full_df.iloc[:idx_split]\n# test = full_df.iloc[idx_split:]","73fe77f0":"#Creating a bunch of pipelines. RobustScaler did the better job than the StandardScaler, so I will use it.\npipes = [\n    Pipeline([('scaler',RobustScaler()), ('regressor',Lasso(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',Ridge(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',ElasticNet(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',LinearRegression())]),\n    Pipeline([('regressor',SGDRegressor(random_state = 42))]),\n    Pipeline([('regressor',RandomForestRegressor(random_state = 42))]),\n    Pipeline([('regressor',AdaBoostRegressor(random_state = 42))]),\n    Pipeline([('regressor',GradientBoostingRegressor(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',LinearSVR(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',SVR())]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',KNeighborsRegressor())]),\n    Pipeline([('regressor',XGBRegressor(random_state = 42))]),\n    Pipeline([('regressor',lgb.LGBMRegressor(objective='regression'))]),\n    Pipeline([('scaler',RobustScaler()),('regressor',KernelRidge())])\n]","fecb38b9":"#Looking at the default models performance\narr = np.zeros(shape=(len(pipes),3),dtype='object')\nfor ind,pipe in enumerate(pipes):\n    m = re.match(\"(.+)([.])([A-Za-z0-9-]+)(['>])\",str(type(pipe.get_params()['regressor'])))\n    score = rmsle_cv(pipe)\n    arr[ind] = [m.group(3),score.mean(),score.std()]   \n    print(\"Iter %i out of %i completed\" % (ind + 1,len(pipes)))  \n    ","d62158ea":"res_df = pd.DataFrame(index=arr[:,0],columns=['mean','std'],data=arr[:,1:])\nres_df.sort_values('mean')","34acaa01":"cv_split = KFold(n_splits = 5,random_state = 42,shuffle=True)","6b32994b":"elastic = pipes[2]\npars = {\n    'regressor__alpha': np.linspace(0.001,0.005,40,endpoint=True),\n    'regressor__l1_ratio': np.linspace(0.1,0.2,40,endpoint=True)\n}\nelastic_grid = GridSearchCV(elastic,pars,scoring='neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nelastic_grid.fit(train,y_train)\nprint(elastic_grid.best_params_)","16144c63":"#L1 ratio is on upper bound of our search space, so we have to perform search again\npars = {\n    'regressor__alpha': [0.0020256410256410257],\n    'regressor__l1_ratio': np.linspace(0.18,0.28,40,endpoint=True)\n}\nelastic_grid = GridSearchCV(elastic,pars,scoring='neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nelastic_grid.fit(train,y_train)\nprint(elastic_grid.best_params_)","0fa2a122":"lasso = pipes[0]\npars = {\n    'regressor__alpha': np.linspace(0.0001,0.0008,80,endpoint=True)\n}\nlasso_grid = GridSearchCV(lasso,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nlasso_grid.fit(train,y_train)\nprint(lasso_grid.best_params_)","f586c93d":"ridge = pipes[1]\npars = {\n#     'regressor__alpha': np.logspace(-4,4,20)\n    'regressor__alpha': np.linspace(14,15.5,80)\n}\nridge_grid = GridSearchCV(ridge,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nridge_grid.fit(train,y_train)\nprint(ridge_grid.best_params_)\nprint(np.where(pars['regressor__alpha'] == ridge_grid.best_params_['regressor__alpha']))","ba2fdf78":"gbr = pipes[7]\npars = {\n    'regressor__learning_rate' : [0.001,0.01,0.1],\n    'regressor__n_estimators': np.arange(100,3500,500),\n    'regressor__max_depth': np.linspace(1, 7,7, endpoint=True),\n    'regressor__min_samples_split': np.linspace(0.1, 0.5, 10, endpoint=True),\n    'regressor__min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True),\n    'regressor__max_features': np.linspace(0.1, 0.3, 5, endpoint=True),\n}\ngrid = RandomizedSearchCV(gbr,pars,n_iter=500,scoring='neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\ngrid.fit(train,y_train)\nprint(grid.best_params_)","d91709b5":"svr = pipes[9]\npars = {\n    'regressor__kernel': ['linear'],\n    'regressor__C': np.linspace(0.001,0.02,80,endpoint=True),\n    'regressor__epsilon': [0.01],   \n}\nsvr_grid = GridSearchCV(svr,pars,scoring='neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nsvr_grid.fit(train,y_train)\nprint(svr_grid.best_params_) \nprint(np.where(pars['regressor__C'] == svr_grid.best_params_['regressor__C']))\nprint(rmsle_cv(svr_grid.best_estimator_).mean())","d608161c":"# forest = pipes[5]\n# pars = {\n#     'regressor__learning_rate' : [0.001,0.01,0.1],\n#     'regressor__n_estimators': np.arange(100,3500,500),\n#     'regressor__max_depth': np.linspace(1, 7,7, endpoint=True),\n#     'regressor__min_samples_split': np.linspace(0.1, 0.5, 10, endpoint=True),\n#     'regressor__min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True),\n#     'regressor__max_features': np.linspace(0.1, 0.3, 5, endpoint=True),\n# }\n# grid = GridSearchCV(forest,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\n# grid.fit(train,y_train)\n# print(grid.best_params_)","f1021846":"lin_svr = pipes[8]\npars = {\n    'regressor__C': np.linspace(0.01,15,40,endpoint=True),\n    'regressor__epsilon': np.linspace(0,1,10),\n}\nlin_svr_grid = GridSearchCV(lin_svr,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nlin_svr_grid.fit(train,y_train)\nprint(lin_svr_grid.best_params_)","7bdc4205":"pars = {\n    'regressor__C': np.linspace(0.1,1,80,endpoint=True),\n    'regressor__epsilon': [0.1],\n}\nlin_svr_grid = GridSearchCV(lin_svr,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nlin_svr_grid.fit(train,y_train)\nprint(lin_svr_grid.best_params_)","8e3e7b4f":"ada = pipes[6]\npars = {\n    'regressor__base_estimator': [DecisionTreeRegressor(max_depth=6),DecisionTreeRegressor(max_depth=7),\n                                  DecisionTreeRegressor(max_depth=8),DecisionTreeRegressor(max_depth=9),\n                                 DecisionTreeRegressor(max_depth=10),DecisionTreeRegressor(max_depth=11),\n                                 DecisionTreeRegressor(max_depth=12),DecisionTreeRegressor(max_depth=13),]\n}\nada_grid = GridSearchCV(ada,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nada_grid.fit(train,y_train)\nprint(ada_grid.best_params_)","41bbee96":"pars = {\n    'regressor__base_estimator': [DecisionTreeRegressor(max_depth=12)],\n    'regressor__learning_rate' : [0.01],\n    'regressor__n_estimators': np.arange(100,3500,500),\n}\nada_grid = GridSearchCV(ada,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nada_grid.fit(train,y_train)\nprint(ada_grid.best_params_)","40ef5ea4":"rmsle_cv(ada_grid.best_estimator_).mean()","22b9b799":"knr = pipes[-4]\npars = {\n    'regressor__n_neighbors' : list(map(int,np.linspace(4,7,endpoint=True))),\n    'regressor__weights': ['uniform','distance'],\n    'regressor__leaf_size': np.arange(10,45,10)\n}\ngrid = GridSearchCV(knr,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\ngrid.fit(train,y_train)\nprint(grid.best_params_)\n\nknr = pipes[-4]\npars = {\n    'regressor__n_neighbors' : list(map(int,np.linspace(2,4,endpoint=True))),\n    'regressor__weights': ['distance'],\n    'regressor__leaf_size': np.arange(2,15)\n}\ngrid = GridSearchCV(knr,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\ngrid.fit(train,y_train)\nprint(grid.best_params_)","3d25ca59":"kernelridge = pipes[-1]\npars = {\n    'regressor__alpha': np.linspace(0.00001,0.01,40,endpoint=True),\n}\ngrid = GridSearchCV(kernelridge,pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\ngrid.fit(train,y_train)\nprint(grid.best_params_)","7c7a1879":"lgb_clf = pipes[12]\nlgb_clf.get_params()['regressor']","190002ea":"lgb_pars = {\n    'regressor__lambda': [9],\n    'regressor__max_depth': [30],\n    'regressor__num_leaves': [12],\n    'regressor__n_estimators': [175],\n    'regressor__subsample': [0.1],\n    'regressor__colsample_bytree': [0.21379310344827587],\n    'regressor__reg_alpha': [0.2],\n}\nlgb_grid = GridSearchCV(lgb_clf,lgb_pars,'neg_mean_squared_error',cv=cv_split,n_jobs=-1,verbose=2)\nlgb_grid.fit(train,y_train)\nprint(lgb_grid.best_params_)\nprint(rmsle_cv(lgb_grid.best_estimator_).mean())","3feba11a":"lgb_pars = {\n    'regressor__lambda': 9,\n    'regressor__max_depth': 30,\n    'regressor__num_leaves': 12,\n    'regressor__n_estimators': 175,\n    'regressor__subsample': 0.1,\n    'regressor__colsample_bytree': 0.21379310344827587,\n    'regressor__reg_alpha': 0.2,\n}","cfe4ae9a":"#Helping function\nfrom sklearn import metrics\ndef modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain['target'].values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='rmse', early_stopping_rounds=early_stopping_rounds,stratified=False)\n        alg.set_params(n_estimators=cvresult.shape[0])\n        print( 'N_estimators best = %i' % cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['target'],eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n        \n    #print( model report:\n    print( \"\\nModel Report\")\n    print( \"MSE : %.4g\" % metrics.mean_squared_error(dtrain['target'].values, dtrain_predictions))\n                    \n    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n#     print( feat_imp.shape\n    plt.figure(figsize=(20,20))\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","82ef62d9":"#Set the initial parameters\nxgb_y_train = pd.DataFrame(y_train,columns=['target'])\nxgb_train = pd.concat([train,xgb_y_train],axis=1)\nxgboost = pipes[-3]\nxgboost.set_params(**{\n    'regressor__learning_rate': 0.1,\n    'regressor__n_estimators': 1000, \n    'regressor__max_depth': 5,     \n    'regressor__min_child_weight': 1,     \n    'regressor__gamma': 0,  \n    'regressor__subsample': 0.8,     \n    'regressor__colsample_bytree': 0.8,     \n    'regressor__objective' : 'reg:linear',     \n    'regressor__nthread': 4,\n    'regressor__scale_pos_weight': 1,\n    'regressor__seed': 2,\n})\nxgboost = xgboost.get_params()['regressor']\n# modelfit(xgboost, xgb_train, train.columns)","5ce825e9":"xgboost.set_params(**{'n_estimators': 355})\nparam_test_1 = {\n    'max_depth': list(range(2,5)),\n    'min_child_weight': list(range(1,4))\n}\ngrid_1 = GridSearchCV(xgboost,param_test_1,scoring='neg_mean_squared_error',n_jobs=-1,verbose=2)\ngrid_1.fit(train,y_train)\nprint(grid_1.best_score_,grid_1.best_params_)","f1a28e44":"xgboost.set_params(**grid_1.best_params_)\nparam_test_2 = {\n    'gamma':[i\/10.0 for i in range(0,5)]\n}\ngrid_2 = GridSearchCV(xgboost,param_test_2,scoring='neg_mean_squared_error',n_jobs=-1,verbose=2)\ngrid_2.fit(train,y_train)\nprint(grid_2.best_score_,grid_2.best_params_)","a32a8f52":"xgboost.set_params(**grid_2.best_params_)\nxgboost.set_params(**{'n_estimators': 1000})\n# modelfit(xgboost, xgb_train, train.columns)\n","8c2b2bed":"xgboost.set_params(**{'n_estimators': 419})\nparam_test_3 = {\n 'subsample':[i\/10.0 for i in range(3,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\n\ngrid_3 = GridSearchCV(xgboost,param_test_3,scoring='neg_mean_squared_error',n_jobs=-1,verbose=2)\ngrid_3.fit(train,y_train)\n\n\nprint(grid_3.best_score_,grid_3.best_params_)","1b72f9a1":"param_test_3a = {\n 'subsample':[i\/100.0 for i in range(75,90,5)],\n 'colsample_bytree':[i\/100.0 for i in range(55,70,5)]\n}\ngrid_3a = GridSearchCV(xgboost,param_test_3a,scoring='neg_mean_squared_error',n_jobs=-1,verbose=2)\ngrid_3a.fit(train,y_train)\nprint(grid_3a.best_score_,grid_3a.best_params_)","15f33420":"xgboost.set_params(**grid_3a.best_params_)\nparam_test4 = {\n 'reg_alpha':np.linspace(1e-10,1e-5,10)\n}\ngrid_4 = GridSearchCV(xgboost,param_test4,scoring='neg_mean_squared_error',n_jobs=-1,verbose=2)\ngrid_4.fit(train,y_train)\nprint(grid_4.best_score_,grid_4.best_params_)","8b4ceb26":"xgboost.set_params(**grid_4.best_params_)\nxgboost.set_params(**{'n_estimators': 1000})\n# modelfit(xgboost, xgb_train, train.columns)","8584b7c8":"grid_4.best_estimator_","7c395ae4":"xgboost.set_params(**{'n_estimators': 5000,'learning_rate': 0.01})\n# modelfit(xgboost, xgb_train, train.columns)","74a2defd":"xgboost.set_params(**{'n_estimators': 2818,'learning_rate': 0.01})\nxgboost.set_params(**{'n_jobs': -1})\n","71764ff1":"xgb_pars = xgboost.get_params()\nxgb_pipe_pars = {}\nfor ind,item in enumerate(xgb_pars.items()):\n    xgb_pipe_pars.update({'regressor__' + item[0]: item[1]})\nxgb_pipe_pars","343e10b4":"xgb_pipe_pars = {'regressor__base_score': 0.5,\n 'regressor__booster': 'gbtree',\n 'regressor__colsample_bylevel': 1,\n 'regressor__colsample_bytree': 0.6,\n 'regressor__gamma': 0.0,\n 'regressor__importance_type': 'gain',\n 'regressor__learning_rate': 0.01,\n 'regressor__max_delta_step': 0,\n 'regressor__max_depth': 2,\n 'regressor__min_child_weight': 2,\n 'regressor__missing': None,\n 'regressor__n_estimators': 2818,\n 'regressor__n_jobs': -1,\n 'regressor__nthread': 4,\n 'regressor__objective': 'reg:linear',\n 'regressor__random_state': 42,\n 'regressor__reg_alpha': 1e-05,\n 'regressor__reg_lambda': 1,\n 'regressor__scale_pos_weight': 1,\n 'regressor__seed': 2,\n 'regressor__silent': True,\n 'regressor__subsample': 0.75}","47875ffe":"  estim = [Lasso(alpha=0.0005075949367088608, copy_X=True, fit_intercept=True,\n           max_iter=1000, normalize=False, positive=False, precompute=False,\n           random_state=42, selection='cyclic', tol=0.0001, warm_start=False), \n         ElasticNet(alpha=0.0020256410256410257, copy_X=True, fit_intercept=True,\n          l1_ratio=0.2158974358974359, max_iter=1000, normalize=False,\n          positive=False, precompute=False, random_state=42,\n          selection='cyclic', tol=0.0001, warm_start=False), \n         SVR(C=0.00989873417721519, cache_size=200, coef0=0.0, degree=2, epsilon=0.01,\n          gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True,\n          tol=0.001, verbose=False)\n]\nmeta = Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=42, solver='auto', tol=0.001)\nstacked_regression = StackingRegressor(regressors = estim,meta_regressor = meta)\nstacked_pipeline = Pipeline([\n    ('scaler',RobustScaler()),\n    ('regressor',stacked_regression)\n])\n","70808fb4":"%%time\nalphas = np.linspace(2.5,7.5,80,endpoint=True)\nres_ = []\nfor ind,el in enumerate(alphas):\n    stacked_pipeline.set_params(**{'regressor__meta_regressor__alpha': el})\n    res_.append(rmsle_cv(stacked_pipeline).mean())\n    print('Iter %i out of %i is finished' % (ind+1,len(alphas)))\n    \nplt.plot(alphas,res_)\nprint(alphas[np.argmin(res_)])","ee40812c":"estim = [Lasso(alpha=0.0005075949367088608, copy_X=True, fit_intercept=True,\n           max_iter=1000, normalize=False, positive=False, precompute=False,\n           random_state=42, selection='cyclic', tol=0.0001, warm_start=False), \n         ElasticNet(alpha=0.0020256410256410257, copy_X=True, fit_intercept=True,\n          l1_ratio=0.2158974358974359, max_iter=1000, normalize=False,\n          positive=False, precompute=False, random_state=42,\n          selection='cyclic', tol=0.0001, warm_start=False), \n         SVR(C=0.00989873417721519, cache_size=200, coef0=0.0, degree=2, epsilon=0.01,\n          gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True,\n          tol=0.001, verbose=False)\n]\nmeta = Ridge(alpha=5.2215189873417724, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=42, solver='auto', tol=0.001)","7f632703":"xgb_pipe_pars = {'regressor__base_score': 0.5,\n 'regressor__booster': 'gbtree',\n 'regressor__colsample_bylevel': 1,\n 'regressor__colsample_bytree': 0.6,\n 'regressor__gamma': 0.0,\n 'regressor__importance_type': 'gain',\n 'regressor__learning_rate': 0.01,\n 'regressor__max_delta_step': 0,\n 'regressor__max_depth': 2,\n 'regressor__min_child_weight': 2,\n 'regressor__missing': None,\n 'regressor__n_estimators': 2818,\n 'regressor__n_jobs': -1,\n 'regressor__nthread': 4,\n 'regressor__objective': 'reg:linear',\n 'regressor__random_state': 42,\n 'regressor__reg_alpha': 1e-05,\n 'regressor__reg_lambda': 1,\n 'regressor__scale_pos_weight': 1,\n 'regressor__seed': 2,\n 'regressor__silent': True,\n 'regressor__subsample': 0.75}\n\nlgb_pars = {\n    'regressor__lambda': 9,\n    'regressor__max_depth': 30,\n    'regressor__num_leaves': 12,\n    'regressor__n_estimators': 175,\n    'regressor__subsample': 0.1,\n    'regressor__colsample_bytree': 0.21379310344827587,\n    'regressor__reg_alpha': 0.2,\n}\nestim = [Lasso(alpha=0.0005075949367088608, copy_X=True, fit_intercept=True,\n           max_iter=1000, normalize=False, positive=False, precompute=False,\n           random_state=42, selection='cyclic', tol=0.0001, warm_start=False), \n         ElasticNet(alpha=0.0020256410256410257, copy_X=True, fit_intercept=True,\n          l1_ratio=0.2158974358974359, max_iter=1000, normalize=False,\n          positive=False, precompute=False, random_state=42,\n          selection='cyclic', tol=0.0001, warm_start=False), \n         SVR(C=0.00989873417721519, cache_size=200, coef0=0.0, degree=2, epsilon=0.01,\n          gamma='auto_deprecated', kernel='linear', max_iter=-1, shrinking=True,\n          tol=0.001, verbose=False)\n]\nmeta = Ridge(alpha=5.2215189873417724, copy_X=True, fit_intercept=True, max_iter=None,\n   normalize=False, random_state=42, solver='auto', tol=0.001)\nstacked_regression = StackingRegressor(regressors = estim,meta_regressor = meta)\nstacked_pipeline = Pipeline([\n    ('scaler',RobustScaler()),\n    ('regressor',stacked_regression)\n])\n","dcb80039":"pipes = [\n    Pipeline([('scaler',RobustScaler()), ('regressor',Lasso(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',Ridge(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',ElasticNet(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',LinearRegression())]),\n    Pipeline([('regressor',SGDRegressor(random_state = 42))]),\n    Pipeline([('regressor',RandomForestRegressor(random_state = 42))]),\n    Pipeline([('regressor',AdaBoostRegressor(random_state = 42))]),\n    Pipeline([('regressor',GradientBoostingRegressor(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',LinearSVR(random_state = 42))]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',SVR())]),\n    Pipeline([('scaler',RobustScaler()), ('regressor',KNeighborsRegressor())]),\n    Pipeline([('regressor',XGBRegressor(random_state = 42))]),\n    Pipeline([('regressor',lgb.LGBMRegressor(objective='regression'))]),\n    Pipeline([('scaler',RobustScaler()),('regressor',KernelRidge())]),\n    Pipeline([('scaler',RobustScaler()),('regressor',stacked_regression)])    \n]\ncv_pars = [\n    #lasso\n    {'regressor__alpha': 0.0005075949367088608},\n    #ridge\n    {'regressor__alpha': 14.949367088607595},\n    #elastic\n    {'regressor__alpha': 0.0020256410256410257, 'regressor__l1_ratio': 0.2158974358974359},\n    #LR\n    {},\n    #SGD\\\n    {},\n    #RandomForest\n    {},\n    #AdaBoost\n    {'regressor__learning_rate': 0.01, 'regressor__loss': 'linear', 'regressor__n_estimators': 2100},\n    #GradientBoosting\n    {'regressor__learning_rate': 0.1, \n     'regressor__max_depth': 2.0, \n     'regressor__max_features': 0.25, \n     'regressor__min_samples_leaf': 0.1, \n     'regressor__min_samples_split': 0.4111111111111111,\n     'regressor__n_estimators': 1600},\n    #LinearSVR\n    {'regressor__C': 0.5112658227848101, 'regressor__epsilon': 0},\n    #SVR\n    {'regressor__C': 0.00989873417721519, 'regressor__epsilon': 0.01, 'regressor__kernel': 'linear'},\n    #KNR\n    {'regressor__leaf_size': 2, 'regressor__n_neighbors': 4, 'regressor__weights': 'distance'},\n    #XGB\n    xgb_pipe_pars,\n    #LGB\n    lgb_pars,\n    #KernelRidge\n    {'regressor__alpha': 0.01},\n    #StackedRegression\n    stacked_pipeline.get_params()\n]\n\nfor ind,pipe in enumerate(pipes):\n    pipes[ind] = pipes[ind].set_params(**cv_pars[ind])","3403a3cc":"arr = np.zeros(shape=(len(pipes),3),dtype='object')\nfor ind,pipe in enumerate(pipes):\n    m = re.match(\"(.+)([.])([A-Za-z0-9-]+)(['>])\",str(type(pipe.get_params()['regressor'])))\n    score = rmsle_cv(pipe)\n    arr[ind] = [m.group(3),score.mean(),score.std()]   \n    print(\"Iter %i out of %i completed\" % (ind + 1,len(pipes)))  \n    ","62dd14d8":"compare = lambda x: \"Tuned\" if len(x) > 0 else \"Not tuned\"\nis_tuned = np.array([compare(x) for x in cv_pars])\nres_df = pd.DataFrame(index=arr[:,0],columns=['Mean','Std','Is tuned','Id'],\n                      data=np.hstack((arr[:,1:],is_tuned.reshape(-1,1),np.arange(len(arr)).reshape(-1,1))))\nres_df.sort_values('Mean')","1ccefe49":"pipes[14].fit(train,y_train)\npipes[11].fit(train,y_train)\npipes[12].fit(train,y_train)\npreds = np.expm1(0.7 * pipes[14].predict(test) + 0.15 * pipes[11].predict(test) + 0.15 * pipes[12].predict(test))\nmake_submission(preds)","d46b7d72":"### LightGBM","45c72f57":"AdaBoost is an interesting boosting algorithm. Instead of training new weak learners on residuals, it assign new weights for training instances - larger weights for those, who were predicted incorrectly and smaller weights for others.","6ee765ac":"Final model","631fc770":"## Going to models","6a81ef3d":"Whoooah. That was quite a trip, wasn't it? Thank everybody for reading this kernel. I hope it will help somebody in their work and will be glad to hear some feedback! See ya!","3af78741":"Spoiler: I tried only linear kernel, because all others did significantly worse. So, another argument for linear relationship in this dataset. ","550068f5":"### Random Forest","3e4f4f93":"Poor performance, trains for a while, no need to use it","a1962cba":"Another ensemble technique. Since one Decision Tree is not so good at anything, let's create a lot of possibly overfitted trees, trained on different bootstrapped (sampling with replacement) with random features selected. Combining all that selectively wise predictors we get another kind of strong learner.","89da45bf":"### KernelRidge","1d24464f":"### Gradient Boosting","06e705d9":"Another boosting algorithm. Actually, it is advised not to use it on small datasets, but for some reasons that will be unraveld later, I decided to use it. Main difference from other boosting algorithms - it grows tree leaf-wise, not level-wise. On a large sets it can even outperform XGB.","e057b475":"This I decided not to tune, since AdaBoost and XGBoost are also based on Decision Trees and even before tuning did significantly better than RF. Also, in articles that I read they usually outperformed RF, so again - no tuning. Sorry, Random Forest!","687e1021":"### Ridge","bafcc8b6":"### Elastic Net","672dd968":"## The end!","ac395116":"One of the most popular machine learning algorithms. As ordinary gradient boosting, it learns on the residuals, but here we could do a lot of interesting things - define custom loss function, tweak a lot of parameters, use different types of regularization. I found the guide for tuning XGB here: https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\nFor some reasons modelfit function works with errors here on Kaggle. The only purpose of this function was to find the optimal number of estimators, so I skipped this function calls in this kernel. ","c6b0bbfe":"### Linear SVR (Check)","aa7f935f":"This algorithm calculates the mean value of n nearest neighbours and even after tuning performs poorly. No surprise, actually","504cd5cd":"Actually, as I understood after I did all this - there was no need to use Linear SVR, since there is no big difference between it and SVR (linear kernel!). But sometimes, good thoughts appear in your head too late :)","a9d6ac76":"### Lasso","1c629a63":"### Spoilers from tuning part","1d44d117":"### XGBoost ","7f93f3e0":"Linear model with both L1 and L2 regularization","9b3c8363":"### KNR","e3cfbb0c":"## Looking at the progress","cd616e09":"### Recreating pipes","9a7829c6":"Finally, let's look at top-20 important features due to RandomForest","30fdaeea":"Idea of boosting is to combine a lot of weak learners to strong learner. There are a lot of boosting alghoritms and a lot of ways to implement this general idea. Gradient boosting fits new weak learner on residuals (in case of classification - on pseudo-residuals) of previous ones using the gradient descent of the loss function to improve the strong learner's performance\n\nActually, the idea of heavy grid search was wrong with this algorithm - it would be better to go through every parameter one by one to reduce time spent on search. I tried this approach later on XGBoost and it was much better in terms of computational efficiency and time spent.","06fdea71":"From here and to the end I will follow the guide, so comments are not needed. The main principle is clear - we want to tune parameters one by one with relatively high learning rate and in the end we want to lower learning rate and find an optimal number of estimators","a824c0cf":"Linear model with L2-regularization (penalizes large coefficents, but do not shrink them to zero)","23448730":"### SVR ","fd7afa32":"## Model tuning","79b71c2b":"## Data preparation","f160f6a6":"As we could see, the progress is actually quite significant. But could we do better?","7cd892ad":"### Ada","08248fa4":"Linear models are good, but we want better. In order to achieve this result I decided to use stacking. Stacking is another type of ensemble, where we combine first-level models, train them on our data and pass their predictions to the meta estimator as new input. Of course, there could be more than just two levels, but here we will build relatively simple model.\n\nAfter some experiments I found the best combination of models and the only thing, which has to be done is to fine-tune the meta-estimator","df03e6e7":"Since the best possible kernel for this task is linear, there was no actual need in trying out this algorithm, but anyway, as I asid, some good thoughts appear too late.","b8359849":"### Table","5763dde9":"Before this competition I knew a little bit about SVR from \"Hands on machine learning book\" and just used the algorithm, but after I decided to publish this kernel on Kaggle I felt some motivation to dive deeper :)\n\nThe main difference between SVR and simple regression is that in regression we try to minimize the error rate, while in SVR we want to fit the error within a certain threshold. And while in Regression we have only the hyperplane, in SVR we have also the margins which we are also want to fit (as I understood, the objective for margin is different for classification and regression). What's more important - it allowes us to perform a kernel trick to map a low dimensional data in high dimensinal data with no need to actually create all this polynomial features which will make everything slow and also could make other models performance worse.","9411d598":"## Stacking linear models altogether","4d108d77":"At this stage it is too early to make any conclusions, since situation could change drastically after model tuning. So let's do it!\n\nP.S: If you're not comfortable with lot (no, LOOOOOT) of GridSearch, you could skip this part safely (still, maybe you will be interested in XGBoost tuning, since it's not so obviuos, as it may seem)","3c67e132":"So, the long and boring part of tuning every model is over and now it's time to look on what we've done so far","a532f23f":"The answer is 'yes'! How about little blending on top of our beautiful models, filled with top-class math? Blending is the simple technique of mixing different models predictions. When I tried it, I didn't expect much from it, but suddenly it boosted my performance! My guess is that it worked because of low std of XGB and LGBM, so mixing their predictions with StackedRegression I decreased errors connected to variance. Still, if somebody could explain me why it works like it works - it would be nice.","fa9e323b":"After several tries and with regard to some other kernels I decided to get rid of most significant outliers, fix skewness of some variables, add a little bit of my own and do a heavy grid search on a large set of models to find the best options.\n\nFrom other kernels (especially https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) it is clear, that we are dealing with some kind of linear relationship between independant variables and target (at least, the most important ones are with no doubt in this kind of linearship), but I won't limit myself with linear models only. One of the purposes of this kernel is to get some practical experience with different models and to get a \"feel\" of what could fit and what couldn't. So let's do it!","e6d3bb86":"The moment of truth!","1c76ce5d":"### Fixing some missing values","b9fe1285":"Linear model with L1-regularization (shrinks unnecessary feature's coefficents to zero)"}}