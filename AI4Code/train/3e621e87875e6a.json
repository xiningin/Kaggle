{"cell_type":{"a4c1f436":"code","72f03e7b":"code","f36da7c7":"code","9d312d97":"code","58bc8507":"code","87f80721":"code","44353cfa":"code","fa9493f5":"code","bd24b57d":"code","a233f732":"code","7b156151":"code","3ae64e42":"code","8e9a6c56":"code","1ab6b0fe":"code","16ba9d00":"code","55123426":"code","3448dcdf":"code","e1108373":"code","0d337187":"code","e48f1d79":"code","c533224a":"code","53c3a0bb":"code","78399c19":"code","f26c35ed":"code","ef343c95":"code","707340e0":"code","df34822b":"code","6a8f0253":"code","4c481ba2":"code","dfc7b684":"code","3ff8362d":"code","f9c50721":"code","176f9459":"code","abeeab73":"code","cdaa2646":"code","c7af87a9":"markdown","aff4dd7d":"markdown","8bcbcf08":"markdown","f48504cb":"markdown","ad93a10c":"markdown","1873a43b":"markdown","ef1121a1":"markdown","8783a161":"markdown","5688fba7":"markdown","9d9e5a39":"markdown","bff7a8f0":"markdown","05fea924":"markdown","4858d730":"markdown","64b0ea63":"markdown","78a0b90f":"markdown","e603046e":"markdown","37f57dd6":"markdown","309bf5b4":"markdown","8c03fc43":"markdown","63fdfa15":"markdown"},"source":{"a4c1f436":"import os\nimport glob\nfrom joblib import Parallel, delayed\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns","72f03e7b":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport torch.nn.functional as F\n\nimport random","f36da7c7":"SEED = 42\n\ndef random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    \n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(SEED)\n        torch.cuda.manual_seed_all(SEED)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nrandom_seed(SEED)","9d312d97":"train = pd.read_pickle(\"..\/input\/optiverlgbbase\/train.pkl\")\ntrain","58bc8507":"for col in train.columns.to_list()[4:]:\n    train[col] = train[col].fillna(train[col].mean())","87f80721":"scales = train.drop(['row_id', 'target', 'time_id',\"stock_id\"], axis = 1).columns.to_list()","44353cfa":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(train[scales])","fa9493f5":"import pickle\n\nscaler_name = \"scaler\"\n\n# saving model\npickle.dump(scaler, open(scaler_name, 'wb'))","bd24b57d":"from sklearn.preprocessing import LabelEncoder \nimport pickle","a233f732":"le=LabelEncoder()\nle.fit(train[\"stock_id\"])\ntrain[\"stock_id\"] = le.transform(train[\"stock_id\"])\n\nwith open( 'stock_id_encoder.txt', 'wb') as f:\n    pickle.dump(le, f)","7b156151":"train[\"stock_id\"]","3ae64e42":"from sklearn import datasets\nfrom sklearn import model_selection\n\ndef create_folds(data, num_splits,target):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[target], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data\n\ntrain = create_folds(train, 5,\"target\")","8e9a6c56":"!pip install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","1ab6b0fe":"from pytorch_tabnet.pretraining import TabNetPretrainer\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.metrics import Metric\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(0)\n\n\nimport os\nfrom pathlib import Path","16ba9d00":"def rmspe(y_true, y_pred):\n    '''\n    Compute Root Mean Square Percentage Error between two arrays.\n    '''\n    \n    if (y_true == 0).any():\n        raise ValueError(\"Root Mean Square Percentage Error cannot be used when \"\n                         \"targets contain zero values.\")\n        \n    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) \/ y_true)), axis=0)).item()\n\n    return loss","55123426":"class RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        return rmspe(y_true, y_score)","3448dcdf":"tabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 3,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = 42,\n    #verbose = 5,\n    cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1] # define categorical features\n)","e1108373":"max_epochs = 50","0d337187":"bestscores=[]\n\nfor fold in range(5):\n    \n   \n\n    traindf = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n    validdf = train[train[\"kfold\"]==fold].reset_index(drop=True)\n\n    ## Normalization except stock id ; stock id is used as categoral features\n\n    X_train = traindf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).values\n    \n    X_train = scaler.transform(X_train)\n    X_traindf = pd.DataFrame(X_train)\n\n    X_traindf[\"stock_id\"]=traindf[\"stock_id\"]\n\n    X_train = X_traindf.values\n    y_train = traindf['target'].values.reshape(-1, 1)\n\n    # validation is same\n    X_valid = validdf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).values\n    X_valid = scaler.transform(X_valid)\n\n    X_validdf = pd.DataFrame(X_valid)\n\n    X_validdf[\"stock_id\"]=validdf[\"stock_id\"]\n\n    X_valid = X_validdf.values\n    y_valid = validdf['target'].values.reshape(-1, 1)\n    \n    # calculate weight\n    \n    y_weight = 1\/np.square(traindf[\"target\"])\n    \n   \n    print(\"----Fold:{}--------start----\".format(str(fold)))\n\n    # initialize random seed\n\n    random_seed(SEED)\n\n\n    # tabnet model\n\n    clf = TabNetRegressor(**tabnet_params)\n\n    # tabnet training\n\n    clf.fit(\n        X_train=X_train, y_train=y_train,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        eval_name=['train', 'valid'],\n        eval_metric=[RMSPE],\n        max_epochs=max_epochs,\n        patience=10,\n        batch_size=1024*2, virtual_batch_size=128*2,\n        num_workers=4,\n        drop_last=False,\n        weights = y_weight,\n        loss_fn=nn.L1Loss()\n    )\n\n\n\n    # save tabnet model\n    saving_path_name = \"tabnet_model_test_\" + str(fold)\n    saved_filepath = clf.save_model(saving_path_name)\n\n            \n            \n    bestscores.append(clf.best_cost)\n    \n    ","e48f1d79":"bestscores","c533224a":"print(\"cv average is \",str(np.mean(bestscores)))","53c3a0bb":"Fe = pd.DataFrame()\n\nfeaturecols = traindf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).columns.to_list()\nfeaturecols.append(\"stock_id\")\n\nFe[\"features\"] = featurecols\nFe[\"Importance\"] = clf.feature_importances_\nFe","78399c19":"Fe2 = Fe.sort_values(\"Importance\",ascending=False)\nFe2","f26c35ed":"import seaborn as sns\nimport matplotlib.pyplot as plt","ef343c95":"plt.rcParams[\"font.size\"] = 18\nsns.barplot(y=Fe2[\"features\"][:10],x=Fe2[\"Importance\"][:10])","707340e0":"sns.barplot(y=Fe2[\"features\"][-10:],x=Fe2[\"Importance\"][-10:])","df34822b":"explain_matrix, masks = clf.explain(X_valid)","6a8f0253":"fig, axs = plt.subplots(1, 3, figsize=(20,20))\n\nfor i in range(3):\n    axs[i].imshow(masks[i][:50])\n    axs[i].set_title(f\"mask {i}\")","4c481ba2":"masksum = masks[0]+masks[1]+masks[2]","dfc7b684":"masksumdf = pd.DataFrame(masksum)\nmasksumdf.columns = featurecols\nmasksumdf","3ff8362d":"plt.rcParams[\"font.size\"] = 14\nplt.figure(figsize=(25,15))\nsns.heatmap(masksumdf,cbar=False)\nplt.savefig(\"result.jpg\")","f9c50721":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate(train, test):\n    # Hyperparammeters (just basic)\n    params = {\n      'objective': 'rmse',  \n      'boosting_type': 'gbdt',\n      'num_leaves': 100,\n      'n_jobs': -1,\n      'learning_rate': 0.1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n      'verbose': -1\n    }\n    \n    # Split features and target\n    x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n    y = train['target']\n    x_test = test.drop(['row_id', 'time_id'], axis = 1)\n    # Transform stock id to a numeric value\n    x['stock_id'] = x['stock_id'].astype(int)\n    x_test['stock_id'] = x_test['stock_id'].astype(int)\n    \n    # Create out of folds array\n    oof_predictions = np.zeros(x.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(x_test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 66, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n        val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n        model = lgb.train(params = params, \n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          num_boost_round = 10000, \n                          early_stopping_rounds = 50, \n                          verbose_eval = 50,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val)\n        # Predict the test set\n        test_predictions += model.predict(x_test) \/ 5\n        \n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    # Return test predictions\n    return test_predictions\n","176f9459":"pathB = \".\/\"\n\nmodelpath = [os.path.join(pathB,s) for s in os.listdir(pathB) if (\"zip\" in s)]","abeeab73":"# Read train and test\ntrain2, test = read_train_test()\n\n# Get unique stock ids \n#train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\n#train_ = preprocessor(train_stock_ids, is_train = True)\n#train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\n#train = get_time_stock(train)\ntest = get_time_stock(test)\n\n#train.to_pickle(\"train.pkl\")\n\n# Traing and evaluate\n#test_predictions = train_and_evaluate(train, test)\n\n\n## fillna for test data ##\n\ntrain=train.drop(\"kfold\",axis=1)\n        \nfor col in train.columns.to_list()[4:]:\n    test[col] = test[col].fillna(train[col].mean())\n\n\n### normarize ###    \n\nx_test = test.drop(['row_id', 'time_id',\"stock_id\"], axis = 1).values\n    # Transform stock id to a numeric value\n\nx_test = scaler.transform(x_test)\nX_testdf = pd.DataFrame(x_test)\n\nX_testdf[\"stock_id\"]=test[\"stock_id\"]\n\n# Label encoding\nX_testdf[\"stock_id\"] = le.transform(X_testdf[\"stock_id\"])\n\n\n\n\nx_test = X_testdf.values\n    \n    \npreds=[]\nfor path in modelpath:\n    \n    clf.load_model(path)\n    preds.append(clf.predict(x_test).squeeze(-1))\n    \npreds = np.mean(preds,axis=0)\n\n\ntest['target'] = preds\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)\n","cdaa2646":"test[['row_id', 'target']]","c7af87a9":"# 4. Inference","aff4dd7d":"# 0. Preparation","8bcbcf08":"# This is work in progress. I hope it is helpful for you. Thank you !","f48504cb":"# 2. Feature importance of last model in #1","ad93a10c":"## Notice that there are too many columns to display all.","1873a43b":"Note: If you create a dataset and save the tabnet model, you need to do something to load it. This is because the saved zip file will be unzipped automatically, when you create the dataset. When loading, please refer to the following page for examples.\n\n* https:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/196625#1076025\n* https:\/\/www.kaggle.com\/chumajin\/optiver-realized-ensemble-tabnet-and-lgbm","ef1121a1":"Visualize top 10","8783a161":"## 0.1 Load train data : \n\nI just folk, save and making dataset from public notebooks. This is origined by\n* https:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline \n* https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\n \nThank you very much.\n\nIf you want to see feature engineering, please refer his work (this is very clean code.)","5688fba7":"This metric is from https:\/\/www.kaggle.com\/atamazian\/optiver-tabnet-training\nThank you and please upvote his work.","9d9e5a39":"# About this notebook\n\n## \u30fb I created the models by TabNet instead of LGBM.\n## \u30fb This is training\/inference notebook of Tabnet models.\n## \u30fb In addition, I shared a notebook which is ensembled of this TabNet models and the already public LGBM models at the following site.\n\n## https:\/\/www.kaggle.com\/chumajin\/optiver-realized-ensemble-tabnet-and-lgbm\n\n\n","bff7a8f0":"## 0.4 : Label Encoder \nmaybe stock id shoud be sequencial value when using categorical features.","05fea924":"## 0.2 Fillna","4858d730":"## 1.1 Modeling","64b0ea63":"\n### If this notebook helpful for you, I would be glad if you **upvote** !\n#### \u203b Thank you for those who upvoted my EDA and other notebooks.\n\nmy EDA : https:\/\/www.kaggle.com\/chumajin\/optiver-realized-eda-for-starter-english-version\n","78a0b90f":"Visualize worst 10","e603046e":"# 3. Masks","37f57dd6":"## 0.5 Kfold\nreffered by https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds \n\nThe effect of this function is referred to my topic.\nhttps:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/263321#1461814\n","309bf5b4":"--------------------------------------------------------------------------------------------------------------------------------------------------------------\n## Reference\n\nThe features used are the same as the LGBM notebook below. Thank you for ragnar. please upvote his work.\n\nhttps:\/\/www.kaggle.com\/ragnar123\/optiver-realized-volatility-lgbm-baseline\n\nother reference\n\n\u30fb LGBM\n\nhttps:\/\/www.kaggle.com\/felipefonte99\/optiver-lgb-with-optimized-params\n\n\nhttps:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\n\n\n\u30fb TabNet\n\nhttps:\/\/www.kaggle.com\/atamazian\/optiver-tabnet-training\n\n\nhttps:\/\/github.com\/dreamquark-ai\/tabnet\n\nThank you for reference ! I respect them. please upvote their great work.\n\n---------------------------------------------------------","8c03fc43":"## 0.3 Making Standardscaler for Normalization","63fdfa15":"# 1. TabNet"}}