{"cell_type":{"b177253f":"code","2a559143":"code","541cebe3":"code","8b73b9f0":"code","7df7b520":"code","1aaaf126":"code","cd6bfc81":"code","d47d55e3":"code","d9332499":"code","2c33adb5":"code","b7000e5e":"code","c003f2ce":"code","b6d808e7":"code","bd389c83":"code","42cdd95e":"code","421dd2dd":"code","f63b4f1f":"code","563cde7c":"code","208bcb99":"code","34638e00":"code","ed7d7441":"code","080386dc":"code","e298ecb7":"code","aa3d9ff3":"code","fc1cea45":"code","efbf9c14":"code","49935e8b":"code","5cdd3d4d":"code","edc2a966":"code","231ac336":"code","b2cb227a":"code","5e424e57":"code","f3c4e960":"code","714dba06":"code","f0284a1f":"code","9b6595d2":"code","e9cc3468":"code","a8097f0a":"markdown","5206c70c":"markdown","09b219b6":"markdown","d6911a28":"markdown","7d15fb39":"markdown","c56f180b":"markdown","39c4a7d9":"markdown","43006664":"markdown"},"source":{"b177253f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n##changing directory\nos.chdir('..\/input\/')","2a559143":"all_products = pd.read_csv('products.csv') #49688\nall_orders = pd.read_csv('orders.csv')\nprior_orders = pd.read_csv('order_products__prior.csv')\ntrain_set = pd.read_csv('order_products__train.csv')","541cebe3":"prior_orders_extended = prior_orders.merge(all_orders,on='order_id',how='left')","8b73b9f0":"prior_orders_extended.columns","7df7b520":"prior_orders_extended.head()","1aaaf126":"cust_attribs1 = prior_orders_extended.groupby('user_id').agg({'product_id':['count','nunique'],'order_id':'nunique','reordered':'sum'})\ncust_attribs1.columns = cust_attribs1.columns.map('_'.join)\ncust_attribs1 = cust_attribs1.reset_index().rename(columns=\\\n    {'product_id_count':'total_basket_size','product_id_nunique':'unique_basket_size','order_id_nunique':'user_num_prior_orders','reordered_sum':'reordered_basket_size'})\ncust_attribs1.head()","cd6bfc81":"#cust_attribs2 = prior_orders_extended.groupby('user_id')['product_id'].apply(set).reset_index().rename(columns={'product_id':'products_list'})\n#cust_attribs2.head()","d47d55e3":"cust_attribs3 = all_orders.groupby('user_id').agg({'days_since_prior_order' : 'mean','order_id':'count'})\ncust_attribs3 = cust_attribs3.reset_index().rename(columns=\\\n    {'days_since_prior_order':'user_avg_frequency','order_id':'user_num_orders'})\ncust_attribs3.head()","d9332499":"#cust_attribs = cust_attribs1.merge(cust_attribs2,how='left',on='user_id').\\\n#     merge(cust_attribs3,how='left',on='user_id')\n#del cust_attribs1,cust_attribs2,cust_attribs3\ncust_attribs = cust_attribs1.merge(cust_attribs3,how='left',on='user_id')\ndel cust_attribs1,cust_attribs3\n\ncust_attribs['average_basket_size'] = cust_attribs['total_basket_size']\/cust_attribs['user_num_prior_orders']\ncust_attribs['user_repeatability'] = cust_attribs['reordered_basket_size']\/cust_attribs['total_basket_size']\ncust_attribs.head()","2c33adb5":"t = prior_orders_extended.groupby(['user_id','product_id']).agg({'order_number':['max','count'],'add_to_cart_order':'mean','reordered':'sum'})\nt.columns = t.columns.map('_'.join)\nt = t.reset_index().rename(columns=\\\n    {'order_number_max':'user_prod_last_order_num','order_number_count':'user_prod_num_orders',\\\n     'add_to_cart_order_mean':'avg_prod_basket_position','reordered_sum':'user_prod_num_reorders'})\nt.head()","b7000e5e":"user_prod_attribs = prior_orders_extended[['user_id','product_id','order_id','order_number']].merge(t,how='inner',left_on=['user_id','product_id','order_number'],right_on=['user_id','product_id','user_prod_last_order_num'])\nuser_prod_attribs.head()","c003f2ce":"user_prod_attribs.drop('order_number',axis=1,inplace=True)\nuser_prod_attribs.rename(columns={'order_id':'user_prod_last_order_id'},inplace=True)\nuser_prod_attribs['user_prod_repeatability'] = user_prod_attribs['user_prod_num_reorders']\/ \\\n    user_prod_attribs['user_prod_num_orders']","b6d808e7":"prod_attribs = prior_orders_extended.groupby('product_id').agg({'reordered':['sum','count']})\nprod_attribs.columns = prod_attribs.columns.map('_'.join)\nprod_attribs = prod_attribs.reset_index().rename(columns=\\\n    {'reordered_sum':'prod_num_reorders','reordered_count':'prod_num_orders'})\nprod_attribs['prod_repeatability'] = prod_attribs['prod_num_reorders']\/ \\\n    prod_attribs['prod_num_orders']\nprod_attribs.head()","bd389c83":"print(train_set.columns)\ntest_orders = all_orders[all_orders.eval_set == 'test']\ntrain_orders = all_orders[all_orders.eval_set == 'train']\nall_orders.head()","42cdd95e":"print(\"Number of orders in train_set : \",train_set.order_id.nunique())\nprint(\"Number of orders in train_orders : \",train_orders.order_id.nunique())\nprint(\"Train set: \",train_set.shape)\nprint(\"Train orders: \",train_orders.shape)","421dd2dd":"train_set.reordered.value_counts()","f63b4f1f":"train_df = train_set.merge(train_orders,how='left',on='order_id').\\\n    merge(prod_attribs,how='left',on='product_id').\\\n    merge(cust_attribs,how='left',on='user_id').\\\n    merge(all_products,how='left',on='product_id').\\\n    merge(user_prod_attribs,how='left',on=['user_id','product_id'])\n\nprint(train_orders.shape)\nprint(train_df.shape)\nprint(train_df.columns)\nprint(\"Number of orders in train_df : \",train_df.order_id.nunique())\nprint(\"Number of users in train_df : \",train_df.user_id.nunique())\nprint(\"Any NaNs in target var : \",sum(train_df.reordered != train_df.reordered))\ntrain_df.set_index(['user_id','product_id'],inplace=True)\ntrain_df.head()","563cde7c":"train_df.isnull().sum()","208bcb99":"train_df[train_df['user_prod_last_order_id'].isnull()].head(5)","34638e00":"prior_orders_extended[(prior_orders_extended['user_id']==112108) & (prior_orders_extended['product_id']==10246)]","ed7d7441":"train_df.user_prod_num_orders.fillna(1,inplace=True)\ntrain_df.avg_prod_basket_position = train_df.add_to_cart_order.where(\\\n        train_df.avg_prod_basket_position.isnull(),train_df.avg_prod_basket_position)\ntrain_df.user_prod_num_reorders.fillna(0,inplace=True)\ntrain_df.user_prod_repeatability.fillna(0,inplace=True)\ntrain_df.dropna(subset=['prod_num_orders'],inplace=True,axis=0)\ntrain_df.isnull().sum()","080386dc":"num_features = ['order_hour_of_day', 'days_since_prior_order', 'total_basket_size',\n       'unique_basket_size', 'user_num_prior_orders', 'reordered_basket_size','user_avg_frequency', 'user_num_orders',\n       'average_basket_size', 'user_repeatability',    'user_prod_num_orders', 'avg_prod_basket_position',\n       'user_prod_num_reorders', 'user_prod_repeatability',\n       'prod_num_reorders', 'prod_num_orders', 'prod_repeatability']\ncat_features = ['aisle_id','department_id','order_dow']\ntot_features = list(set(num_features)|set(cat_features))","e298ecb7":"len(num_features)","aa3d9ff3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfig, ax = plt.subplots(nrows = 6, ncols = 3 ,figsize=(30,30))\nflat_ax = [item for sublist in ax for item in sublist]\nfor i,feature in enumerate(num_features):\n     sns.distplot(train_df[feature],ax=flat_ax[i])","fc1cea45":"def get_normalization_parameters(traindf, features):\n    \"\"\"Get the normalization parameters (E.g., mean, std) for traindf for \n    features. We will use these parameters for training, eval, and serving.\"\"\"\n\n    def z_score_params(column):\n        mean = np.float32(traindf[column].mean())\n        std = np.float32(traindf[column].std())\n        min = np.float32(traindf[column].min())\n        max = np.float32(traindf[column].max())\n\n        return {'mean': mean, 'std': std,'min':min,'max':max}\n\n    normalization_parameters = {}\n    for column in features:\n        normalization_parameters[column] = z_score_params(column)\n    return normalization_parameters\n#normalization_parameters = get_normalization_parameters(x_train, num_features)\nnormalization_parameters = get_normalization_parameters(train_df, num_features)\nprint(normalization_parameters)","efbf9c14":"num_features_s = []\nfor feature in num_features:\n    train_df[feature+'_s'] = (train_df[feature]-normalization_parameters[feature]['min'])\/(normalization_parameters[feature]['max']-normalization_parameters[feature]['min'])\n    num_features_s.extend([feature+'_s'])\n    print(feature)\ntot_features_s = list(set(num_features_s)|set(cat_features))    ","49935e8b":"from sklearn.model_selection import train_test_split\nx_train, x_eval, y_train, y_eval = train_test_split(train_df[tot_features_s], train_df['reordered'], test_size=0.2)","5cdd3d4d":"del train_df,all_orders,prior_orders_extended,prior_orders,train_set,all_products","edc2a966":"import tensorflow as tf\n\ntf.enable_eager_execution()\n\ntf.logging.set_verbosity(tf.logging.ERROR)\ntf.set_random_seed(123)","231ac336":"fc = tf.feature_column\nall_features = []\n\ndef std_scaler(feature,mean,std):\n      return (feature - mean)\/std\n\nfor feature_name in cat_features:\n  # Need to one-hot encode categorical features.\n    vocabulary = x_train[feature_name].unique()\n    all_features.append(fc.indicator_column(\n      fc.categorical_column_with_vocabulary_list(feature_name,vocabulary)))\n\nfor feature_name in num_features_s:\n#     fet_mean = normalization_parameters[feature_name]['mean']\n#     fet_std = normalization_parameters[feature_name]['std']\n    all_features.append(fc.numeric_column(feature_name,dtype=tf.float32,\\\n           # normalizer_fn=lambda x:((x-fet_mean)\/fet_std)\\\n                       ))\n#Normalizing is creating a problem - check later","b2cb227a":"print(len(all_features))\nall_features","5e424e57":"example = x_train.head(1)","f3c4e960":"fc.input_layer(dict(example), all_features)","714dba06":"x_train.shape,x_eval.shape","f0284a1f":"NUM_EXAMPLES = len(y_train) #1107693\n\ndef make_input_fn(X, y, n_epochs=None, shuffle=True):\n    def input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n        if shuffle:\n          dataset = dataset.shuffle(NUM_EXAMPLES)\n        # For training, cycle thru dataset as many times as need (n_epochs=None).    \n        dataset = dataset.repeat(n_epochs)  \n        # In memory training doesn't use batching.\n        dataset = dataset.batch(NUM_EXAMPLES)\n        return dataset\n    return input_fn\n\ntrain_input_fn = make_input_fn(x_train.head(800), y_train.head(800))\neval_input_fn = make_input_fn(x_eval.head(1000), y_eval.head(1000), shuffle=False, n_epochs=1)","9b6595d2":"linear_est = tf.estimator.LinearClassifier(all_features)\n\n# Train model.\nlinear_est.train(train_input_fn, max_steps=10)\n\n# Evaluation.\nresults = linear_est.evaluate(eval_input_fn)\nprint('Accuracy : ', results['accuracy'])\nprint('Dummy model: ', results['accuracy_baseline'])","e9cc3468":"def populate_features(df,chosen_features):\n    df_new = df.merge(cust_attribs,how='left',on='user_id').\\\n        merge(user_prod_attribs,how='left',on='user_id').\\\n        merge(prod_attribs,how='left',on='product_id').\\\n        merge(train_set,how='left',on=['order_id','product_id']).\\\n        merge(all_products,how='left',on='product_id')\n    return df_new[chosen_features]","a8097f0a":"User product frequency would make more sense than user average frequency, as quantity of product is not given. Eg. A users usual order quanity can 1 kg of Pulses which he orders once a week, another's might be 2 kg ordered once in two weeks. So this might be a valuable feature. \/\/Work later on bringing this - requires quite a bit of tweaking as order dates are not given as timestamps","5206c70c":"### PREPARING TRAINING DATA","09b219b6":"The NaNs in these variables resulted from user_prod df meaning the user product combination in the training data is new. Verifying it. \nFor imputing these missing values - we can ignore the user_prod_last_order_id,user_prod_last_order_num as we are not using them anywhere now. For rest, \nSince we've a single order per user, we can replace user_prod_num_orders with 1, avg basket position with the current basked position,user_prod_num_reorders and repeatability as 0\n\n9 rows have NaNs on product related columns, which means the product appeared for the first time. Dropping these rows. ","d6911a28":"## FEATURES","7d15fb39":"The training data set given and test data set dervied from orders have different columns. We'll pick order_id, product_id, reordered or not from the training set given, map it to train tagged data from all_orders set and there on add features which we generated. A user-product can be taken as index, rest as features and reordered\/not as label and model can be trained.","c56f180b":"### CUSTOMER","39c4a7d9":"### PRODUCT","43006664":"### CUST-PRODUCT"}}