{"cell_type":{"cc0fac37":"code","18e47ec0":"code","7c02fb55":"code","de9078b9":"code","cd825996":"code","ad238589":"code","417bf602":"code","aeaefe53":"code","a8b3a957":"code","17f02f8d":"code","c56d943a":"code","7785016c":"code","adae6f61":"code","c75d095e":"code","473579ed":"code","34c06b68":"code","04b344fb":"markdown","416c584e":"markdown","f6abed6e":"markdown","5fbcab9e":"markdown","610408c7":"markdown","84f47e5e":"markdown"},"source":{"cc0fac37":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.compose import ColumnTransformer, make_column_selector as selector \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import classification_report, accuracy_score,confusion_matrix, roc_auc_score, roc_curve","18e47ec0":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","7c02fb55":"train_data.sample(10)","de9078b9":"features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare','Embarked', 'Name']\n\nX = train_data[features]\ny = train_data.Survived\nX_test = test_data[features]","cd825996":"#hard coding specific data cleaning for title names, not ideal to include in pipeline.  Change for both training and test holdout.\n\n#Title cleaning for training data\nfor dataset in [X, X_test]:\n    dataset.loc[:,'Title'] = dataset.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    dataset.loc[~dataset.Title.isin(['Mr', 'Miss', 'Mrs', 'Master']), 'Title'] = 'Other'\n    dataset.loc[:,'Title'] = dataset.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    dataset.loc[~dataset.Title.isin(['Mr', 'Miss', 'Mrs', 'Master']), 'Title'] = 'Other'\n\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Other\": 5}\nfor dataset in [X, X_test]:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\nX.drop(['Name'], axis=1, inplace = True)\nX_test.drop(['Name'], axis=1, inplace = True)","ad238589":"cols = ['Sex', 'Embarked']\nX.loc[:,cols] = X.loc[:,cols].astype('category')\nX_test.loc[:,cols] = X.loc[:,cols].astype('category')\nX.info()","417bf602":"numeric_transformer = Pipeline(steps=[\n    ('imp_num', IterativeImputer(max_iter=10, min_value=0)),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imp_cat', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(drop='first'))])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, selector(dtype_exclude=\"category\")),\n    ('cat', categorical_transformer, selector(dtype_include=\"category\"))])\n\n\nclassifier = RandomForestClassifier(\n                      max_depth=5,\n                      n_estimators=500,\n                      max_features='auto')\n\nclass_model = Pipeline([('preprocess', preprocessor),\n                        ('classifier', classifier)])\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\nclass_model.fit(X_train, y_train)\n\nprint(\"model score: %.3f\" % class_model.score(X_val, y_val))","aeaefe53":"#cross validate the test\ny_train_score = cross_val_score(class_model, X_val, y_val, cv=10)\nprint(y_train_score.mean())\nprint(y_train_score.std())","a8b3a957":"X_values = preprocessor.fit_transform(X_train)\nonehot_columns = class_model.named_steps['preprocess'].named_transformers_['cat'].named_steps['onehot'].get_feature_names().tolist()\nnumerical_columns = X.columns[X.dtypes != 'category'].tolist()\n\ndf_pipeline = pd.DataFrame(X_values, columns = numerical_columns + list(onehot_columns) )\ndf_pipeline.head()","17f02f8d":"#feature importance , extract from pipeline\nimport numpy as np\nfeature_importance = pd.Series(data= class_model.named_steps['classifier'].feature_importances_, index = np.array(numerical_columns + list(onehot_columns)))\nfeature_importance.sort_values(ascending=False)","c56d943a":"# verify best hyper parmeters for the random forest\nparam_grid = {\n    'classifier__max_depth': [3, 5, 8],\n    'classifier__n_estimators': [100, 500],\n    'classifier__max_features': [2, 7, 'auto']\n     }\n\ngrid_search = GridSearchCV(class_model, param_grid, cv=8, verbose=0)\ngrid_search.fit(X_train, y_train)\n\nprint((\"best random foreest classification from grid search: %.3f\"\n       % grid_search.score(X_val, y_val)))","7785016c":"grid_search.best_params_","adae6f61":"from yellowbrick.classifier import ConfusionMatrix, ROCAUC\n\nmapping = {0: \"died\", 1: \"survived\"}\nfig, ax = plt.subplots(figsize=(6, 6))\ncm_viz = ConfusionMatrix(\n    class_model,\n    classes=[\"died\", \"survived\"],\n    label_encoder=mapping,\n)\ncm_viz.score(X_val, y_val)\ncm_viz.poof()","c75d095e":"fig, ax = plt.subplots(figsize=(8, 6))\nroc_viz = ROCAUC(class_model)\nroc_viz.score(X_val, y_val)\nroc_viz.poof()","473579ed":"from yellowbrick.model_selection import LearningCurve\nimport numpy as np\n\nfig, ax = plt.subplots(figsize=(8, 6))\ncv = StratifiedKFold(12)\nsizes = np.linspace(0.3, 1.0, 10)\nlc_viz = LearningCurve(\n    class_model,\n    cv=cv,\n    train_sizes=sizes,\n    scoring=\"f1_weighted\",\n    n_jobs=4,\n    ax=ax,\n)\nlc_viz.fit(X, y)\nlc_viz.poof()","34c06b68":"predictions = class_model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput = output.astype(int)\noutput.to_csv('..\/output\/my_submissionKAGGLE.csv', index=False)","04b344fb":"Create pipleine and train data.  The iterative imputer is based on MICE and takes into account other features when imputing , specifically for 'Age'","416c584e":"Here again, using the transformed pipeline to extract feature importance, which is NOT STRAIGHTFORWARD!","f6abed6e":"Even though not that useful for this data set, I found it interesting to be able to inverse tranform the entire pipeline to a data frame to see output of the prepocessing step.  You can use the below to trace back to the pipleine steps above.","5fbcab9e":"Important step to flip a few features to categories for pipeline","610408c7":"Possibly not the best modeling, but fast & modern sklearn approach using pipeline for both preprocessing and model development.  I struggled to find easy ways to extract column features after transforming, if you were struggling to hope this clarifies.  ","84f47e5e":"Below start to use the `yellowbrick` package for ML viz , very easy and effective."}}