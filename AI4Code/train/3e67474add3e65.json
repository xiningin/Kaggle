{"cell_type":{"404026d4":"code","50b90665":"code","498f7073":"code","e3ab47da":"code","280b831c":"code","ada3287b":"code","6a93cc56":"code","72bb7deb":"code","cf63f9a7":"code","65fc54b6":"code","3680c3ca":"code","fefa15d0":"code","cf5f2f6a":"code","56ff3605":"code","944ed21c":"code","fddffd2d":"code","cd72a142":"code","392d5f8b":"code","e5a31e7e":"code","b4fec38e":"code","1c2ee12f":"code","e2cfedb1":"code","a12ba875":"code","7a719a17":"code","df82cd55":"code","439dd79d":"code","1b5d5405":"code","556718f8":"code","cdf62172":"code","bb398006":"code","38471524":"code","73a847e4":"code","dea2b706":"code","0e5cd051":"code","d327c60e":"code","2834d640":"code","c2278b31":"code","03f1e2d0":"code","0fb6645f":"code","3525d2ec":"code","e086c39f":"code","5af1b822":"code","a4736d71":"code","ccf11993":"code","02f934be":"code","f25e96fd":"code","90b37edf":"code","15cd0b28":"code","4d4829d2":"code","db70e10d":"code","3652835b":"code","1f408266":"code","5eb2babb":"code","875b565c":"code","6cbf9de4":"code","7ed6a9f4":"code","bef0ea0b":"code","4edb6df4":"code","3627b9c7":"code","ef4495d4":"code","c9b224fc":"code","1200ebe8":"code","e644f323":"code","b82961b3":"markdown","1f64dbaf":"markdown","87a5e5ce":"markdown","55290dfc":"markdown","0339b670":"markdown","b6160008":"markdown","77db1508":"markdown","c5aeaac1":"markdown","4edf1066":"markdown","b30c4ee9":"markdown","4bda3552":"markdown","40f6a8d1":"markdown","fa0c8dc2":"markdown","4f506195":"markdown","40c0629a":"markdown","57b86928":"markdown","89b67e3e":"markdown","8ac77f56":"markdown","0418c52b":"markdown","121a5111":"markdown","06d1da68":"markdown","ae470964":"markdown","9620d1b0":"markdown","1f6eafa2":"markdown","869c04b1":"markdown","ef4946ef":"markdown","d8cad203":"markdown","9af858dc":"markdown","bea99302":"markdown","865230f4":"markdown","4e3d189b":"markdown","ed0d5576":"markdown","dc77d3b0":"markdown","41be77b4":"markdown","dd6ec53e":"markdown","34371f3a":"markdown","ccaba78e":"markdown","4eb68394":"markdown","446e31a7":"markdown","6281afc2":"markdown","d8ffc479":"markdown","8cf16c0e":"markdown","8fc0b6f7":"markdown","665a3ba9":"markdown","a2d02bee":"markdown","69f181f2":"markdown","3e93bb67":"markdown","be80fbfa":"markdown","982ec0df":"markdown","78d92bde":"markdown","686201d0":"markdown","ef09e542":"markdown","9d145305":"markdown","e6a835f4":"markdown","431214a5":"markdown","d837dfc3":"markdown","bcf373d8":"markdown","9d31a9e4":"markdown","79f511ad":"markdown","c9071043":"markdown","5cde34bd":"markdown","45c694f8":"markdown"},"source":{"404026d4":"from IPython.display import Image\nImage(\"\/kaggle\/input\/images\/textproc.jpg\",  width=400)","50b90665":"Image(\"\/kaggle\/input\/images\/eucldeandistance.png\",  width=400)","498f7073":"Image(\"\/kaggle\/input\/images\/cosineSimilarity.png\",  width=400)","e3ab47da":"# Fetch & clean dataset \n!pip install pyspellchecker\nfrom spellchecker import SpellChecker\nimport pandas as pd\nfrom nltk.corpus import stopwords \nfrom nltk.corpus import wordnet\n\nfrom nltk.stem import WordNetLemmatizer \nimport nltk \nimport re\nimport numpy as np  \nimport pandas as pd \ntrain_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n\n\ndef convert_to_antonym(sentence):\n    words = nltk.word_tokenize(sentence)\n    new_words = []\n    temp_word = ''\n    for word in words:\n        antonyms = []\n        if word == 'not':\n            temp_word = 'not_'\n        elif temp_word == 'not_':\n            for syn in wordnet.synsets(word):\n                for s in syn.lemmas():\n                    for a in s.antonyms():\n                        antonyms.append(a.name())\n            if len(antonyms) >= 1:\n                word = antonyms[0]\n            else:\n                word = temp_word + word # when antonym is not found, it will\n                                    # remain not_happy\n            \n            temp_word = ''\n        if word != 'not':\n            new_words.append(word)\n    return ' '.join(new_words)\n\n\n# def correct_spellings(text):\n#     spell = SpellChecker()\n#     corrected_words = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_words.append(spell.correction(word))\n#         else:\n#             corrected_words.append(word)\n#     return \" \".join(corrected_words)\n\n        \nlemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV }\ndef lemma_words(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word ,wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n \ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n  \"\"\"\n    text = text.lower() # lowercase text\n    text= re.sub(r'[^\\w\\s#]',' ',text) #Removing every thing other than space, word and hash\n    text  = re.sub(r\"https?:\/\/\\S+|www\\.\\S+\", \"\", text )\n    text= re.sub(r'[0-9]',' ',text)\n    #text = correct_spellings(text)\n    text = lemma_words(text)\n    text = convert_to_antonym(text)\n    text = re.sub(' +', ' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text    \n    return text\n\n\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntest_df['text'] = test_df['text'].apply(clean_text)\n\nsentences= pd.DataFrame(columns=['text'])\nsentences['text']= pd.concat([train_df[\"text\"], test_df[\"text\"]])\n\nfrom collections import defaultdict\ntokens_list = [row.split() for row in sentences['text']]\n\n","280b831c":"Image(\"\/kaggle\/input\/images\/word2vec.png\",  width=450)\n","ada3287b":"Image(\"\/kaggle\/input\/images\/skipgram.png\",  width=450)","6a93cc56":"from gensim.models import Word2Vec\nfrom time import time\nt = time()\n# initialize skipgram model\nsg_model = Word2Vec(min_count=2,\n                    window=2,size=300, sg = 1,\n                    sample=5e-5, alpha=0.05, \n                    min_alpha=0.0005,negative=20 )\n\n# build model vocabulary\nsg_model.build_vocab(tokens_list)\n\n# train the model\nsg_model.train(tokens_list, total_examples=sg_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to build Skip gram model vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","72bb7deb":"sg_model.wv.__getitem__('hope')","cf63f9a7":"len(sg_model.wv.__getitem__('hope'))","65fc54b6":"sg_model.wv.similarity('people','saint' )\n","3680c3ca":"sg_model.wv.similarity('people', 'terrorist')","fefa15d0":"# top 5 similar words\nsg_model.wv.most_similar('fire')[:5]","cf5f2f6a":"print(list(sg_model.wv.vocab))\n ","56ff3605":"Image(\"\/kaggle\/input\/images\/cbow.png\", width=450)","944ed21c":"#### Building CBOW wordvectors\nfrom gensim.models import Word2Vec\nfrom time import time\nt = time()\n# initialize\ncbow_model = Word2Vec(min_count=2,window=2,size=300, sg = 0,sample=5e-5, alpha=0.05, min_alpha=0.0005, \n                     negative=20 )\n# build model vocabulary\ncbow_model.build_vocab(tokens_list)\n\n# train the model\ncbow_model.train(tokens_list, total_examples=cbow_model.corpus_count, epochs=30, report_delay=1)\n\nprint('Time to build CBOW model vocab: {} mins'.format(round((time() - t) \/ 60, 2)))\n","fddffd2d":"#fetching  pretrain wordvector\nfrom gensim.models.keyedvectors import KeyedVectors\nt = time()\npretrained_w2vec_embedding = KeyedVectors.load_word2vec_format('..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin', binary=True)\nprint('Time to fetch  pretrain  Word2Vec model vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","cd72a142":"pretrained_w2vec_embedding['people']","392d5f8b":"# shape of pretrain w2vec embedding\npretrained_w2vec_embedding.vectors.shape","e5a31e7e":"#del pretrained_w2vec_embedding","b4fec38e":"!pip install glove_python","1c2ee12f":"#importing the glove library\nfrom glove import Corpus, Glove\n\n# creating a corpus object\ncorpus = Corpus() \n\n#training the corpus to generate the co occurence matrix which is used in GloVe\ncorpus.fit(tokens_list, window=3)\n\n#creating a Glove object which will use the matrix created in the above lines to create embeddings\n#we can set the learning rate as glove uses Gradient Descent\nglove = Glove(no_components=300, learning_rate=0.05)\nglove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\nglove.add_dictionary(corpus.dictionary)\nglove.save('glove.model')","e2cfedb1":"glove.word_vectors[glove.dictionary['people']]","a12ba875":"# # Fetch pretrain glove word vectors \n# import numpy as np \n# pretrained_glove_embedding={}\n# with open('..\/input\/nlpword2vecembeddingspretrained\/glove.6B.300d.txt','r') as f:\n#     for line in f:\n#         values=line.split()\n#         word=values[0]\n#         vectors=np.asarray(values[1:],'float32')\n#         pretrained_glove_embedding[word]=vectors\n# f.close()","7a719a17":"#pretrained_glove_embedding['hello']\n","df82cd55":"# no of  word in pretrained glove embedding\n#len(pretrained_glove_embedding)","439dd79d":"#del pretrained_glove_embedding","1b5d5405":"# dimension =300\n# from gensim.models import FastText\n# fasttext_model = FastText(tokens_list, size=dimension, window=5, min_count=5, workers=4, sg=1)\n","556718f8":"# fasttext_model.wv.__getitem__('people')","cdf62172":"# # fasttext word similarity measure\n# fasttext_model.wv.similarity('evacuation','shelter' )","bb398006":"# # most similar words\n# fasttext_model.wv.most_similar('fire')[:5]","38471524":"# def get_coefs(word, *arr): \n#     return word, np.asarray(arr, dtype='float32')\n\n# EMBEDDING_FILE = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'\n# pretrained_fasttext_embedding = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in (open(EMBEDDING_FILE)))","73a847e4":"## no of words in pretrained fasttext embedding\n#len(pretrained_fasttext_embedding)","dea2b706":"#pretrained_fasttext_embedding['fire']","0e5cd051":"#len(pretrained_fasttext_embedding['fire'])","d327c60e":"# functions for Vector Averaging with word2Vec\nimport numpy as np\ndef w2v_embeddings(text,w2v_model,dimension):\n    if len(text) < 1:\n        return np.zeros(dimension)\n    else:\n        vectorized = [w2v_model.wv[word] if word in w2v_model.wv else np.random.rand(dimension) for word in text] \n    \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum\/len(vectorized)     \n\ndef get_w2v_embeddings(text,w2v_model,dimension):\n        embeddings = text.apply(lambda x: w2v_embeddings(x, w2v_model,dimension))\n        return list(embeddings)","2834d640":"# Text to Numeric Vector Averaging using sgmodel \ntrain_embeddings_sg_model  = get_w2v_embeddings(train_df['text'],sg_model,dimension=300)\n ","c2278b31":"# Validate train set size\nlen(train_embeddings_sg_model)","03f1e2d0":"# validate dimension\nlen(train_embeddings_sg_model[0])","0fb6645f":"# Text to numeric Vector Averaging using cbow model\ntrain_embeddings_cbow_model_  = get_w2v_embeddings(train_df['text'],cbow_model,dimension=300)","3525d2ec":"## functions  for Vector Averaging with GloVe\nimport numpy as np\ndef glove_embeddings(text, glove_model, dim ):\n    dic=glove_model.dictionary\n    if len(text) < 1:\n        return np.zeros(dim)\n    else:\n        vectorized = [glove_model.word_vectors[dic[word]] if word in dic else np.random.rand(dim) for word in text]  \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum\/len(vectorized)     \n\ndef get_glove_embeddings(text,glove_model,dimension):\n        embeddings = text.apply(lambda x: glove_embeddings(x,glove_model, dimension))\n        return list(embeddings)\n","e086c39f":"# Text to numeric Vector Averaging using glove\nimport numpy as np\ntrain_embeddings_glove = get_glove_embeddings(train_df['text'],glove,dimension=300)\ntest_embeddings_glove = get_glove_embeddings(test_df['text'],glove,dimension=300)","5af1b822":"# ## Text to numeric using Averaging with Fasttext\n# import numpy as np\n# fasttext_train_embeddings = w2v_embeddings(train_df['text'], fasttext_model,dimension=300)\n# fasttext_test_embeddings = w2v_embeddings(test_df['text'],  fasttext_model,dimension=300)","a4736d71":"def pretrained_embeddings(text,model,dimension):\n    if len(text) < 1:\n        return np.zeros(dimension)\n    else:\n        vectorized = [model[word] if word in model else np.random.rand(dimension) for word in text] \n    \n    sum = np.sum(vectorized,axis=0)\n    ## return the average\n    return sum\/len(vectorized)     \n\ndef get_pretrained_embeddings(text,model,dimension):\n        embeddings = text.apply(lambda x: pretrained_embeddings(x, model,dimension))\n        return list(embeddings)","ccf11993":"# uncomment section in below code in order to perfrom vector averaging using pretrained word embeeding\n\n#Text to numeric Vector Averaging  using  pretrianed word2vec\ntrain_embeddings_w2vec_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_w2vec_embedding,dimension=300)\n\n## Text to numeric Vector Averaging  using  pretrianed glove\n# train_embeddings_glove_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_glove_embedding,dimension=300)\n\n## Text to numeric Vector Averaging  using  pretrianed fasttext\n# train_embeddings_fasttext_pretrained_  = get_pretrained_embeddings(train_df['text'],pretrained_fasttext_embedding,dimension=300)\n\n","02f934be":"# using keras built in utilities\n# tokenizing using keras  tokenizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.layers import Embedding\ntokenizer_obj=Tokenizer()\n# to builds the word index\ntokenizer_obj.fit_on_texts(tokens_list)\n# to turns strings into lists of integer indices.\nsequences=tokenizer_obj.texts_to_sequences(tokens_list)\n# defining maximum length of sequence \nMAX_LEN= 50\n# pad_sequences is used to ensure that all sequences in a list have the same length\ntweet_pad= pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')\n\n# segregating text & train from corpu\nx_train = tweet_pad[:7613]\nx_test = tweet_pad[7613:]\n\ntargets =  [target for target in train_df['target']]\n\n# set of all word and their sequence no\nword_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))\nvocab_size = len(word_index)+1\n","f25e96fd":"# function to generate embeeding layer weights i.e. embeeding_matrix\n\n\ndef generate_word2vec_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n    embedding_matrix=np.zeros((vocab_size,dimension))\n    for word,i in tqdm(word_index.items()):\n        if i > vocab_size:\n            continue\n        if word in word_vector_model.wv:  \n            emb_vec=word_vector_model.wv.__getitem__(word)\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\ndef generate_pretrained_embeeding_matrix(word_vector_model, dimension, vocab_size= vocab_size, word_index =word_index):\n    embedding_matrix=np.zeros((vocab_size,dimension))\n    for word,i in tqdm(word_index.items()):\n        if i > vocab_size:\n            continue\n        if word in word_vector_model:  \n            emb_vec=word_vector_model[word]\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\n","90b37edf":"from keras.layers import Embedding\n\nembedding_matrix_sg_trained = generate_word2vec_embeeding_matrix(sg_model, dimension = 300)\n\nembedding_layer_sg_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_sg_trained], \n                                     input_length=MAX_LEN, trainable=False)","15cd0b28":"# pre trainde word2vec  \nembedding_matrix_w2v_pretrained = generate_pretrained_embeeding_matrix(pretrained_w2vec_embedding, dimension =300)    \n\nembedding_layer_w2v_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_w2v_pretrained], \n                                     input_length=MAX_LEN, trainable=False)","4d4829d2":"embedding_matrix_cbow_trained = generate_word2vec_embeeding_matrix(cbow_model, dimension = 300)\n\nembedding_layer_cbow_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_cbow_trained], \n                                     input_length=MAX_LEN, trainable=False)","db70e10d":"import numpy as np\nembedding_matrix_glove_trained=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    if i > vocab_size:\n        continue\n    \n    emb_vec=glove.word_vectors[glove.dictionary[word]]\n    if emb_vec is not None:\n        embedding_matrix_glove_trained[i]=emb_vec","3652835b":"embedding_layer_glove_trained = Embedding(vocab_size, 300 , weights=[embedding_matrix_glove_trained], \n                                     input_length=MAX_LEN, trainable=False)","1f408266":"# embedding_matrix_glove_pretrained = generate_pretrained_embeeding_matrix(pretrained_glove_embedding, dimension =300)    \n\n# embedding_layer_glove_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_glove_pretrained], \n#                                      input_length=MAX_LEN, trainable=False)","5eb2babb":" \n# embedding_matrix_fasttext_trained = generate_word2vec_embeeding_matrix(fasttext_model, dimension =300)    \n\n# embedding_layer_fasttext_trained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_trained], \n#                                      input_length=MAX_LEN, trainable=False)","875b565c":"# embedding_matrix_fasttext_pretrained = generate_pretrained_embeeding_matrix(pretrained_fasttext_embedding, dimension =300)    \n\n# embedding_layer_fasttext_pretrained = Embedding(vocab_size, output_dim= 300, weights=[embedding_matrix_fasttext_pretrained], \n#                                      input_length=MAX_LEN, trainable=False)","6cbf9de4":"# Declare embeeding layer of your choics \nembedding_layer = embedding_layer_sg_trained\n\n# try with other embedding layes too\n#embedding_layer_glove_pretrained\n# embedding_layer_fasttext_pretrained\n# embedding_layer_fasttext_trained\n# embedding_layer_cbow_trained\n# embedding_layer_sg_trained\n# embedding_layer_w2vec_pretrained\n# embedding_layer_glove_trained","7ed6a9f4":"import keras\nmy_callbacks = [\n    keras.callbacks.EarlyStopping(patience=5),\n    \n]","bef0ea0b":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense\ndnn_model = Sequential()\ndnn_model.add(embedding_layer)\ndnn_model.add(Flatten())\ndnn_model.add(Dense(1, activation='sigmoid'))\n\ndnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndnn_model.summary()\n\nhistory = dnn_model.fit(x_train,  y = targets,\n                    epochs=10,\n                    batch_size=32,\n                    validation_split=0.2, callbacks= my_callbacks)","4edb6df4":"#CNN is suitable for image processing\n# import keras\n# cnn_model = Sequential()\n# # note : below we add embedding layer\n# cnn_model.add(embedding_layer)\n# cnn_model.add(keras.layers.Dropout(0.2))\n# cnn_model.add(keras.layers.Conv1D(3,3, padding='valid',activation='relu', strides=1))\n# cnn_model.add(keras.layers.GlobalMaxPooling1D())\n# cnn_model.add(keras.layers.Dense(20))\n# cnn_model.add(keras.layers.Dropout(0.2))\n# cnn_model.add(keras.layers.Activation('relu'))\n# cnn_model.add(keras.layers.Dense(1))\n# cnn_model.add(keras.layers.Activation('sigmoid'))\n\n# # Get model summary\n# cnn_model.summary()\n# cnn_model.compile(optimizer='rmsprop',\n#               loss='binary_crossentropy',\n#               metrics=['acc'])\n\n# # compile the model\n# history = cnn_model.fit(x_train,  y = targets,\n#                     epochs=10,\n#                     batch_size=32,\n#                     validation_split=0.2)","3627b9c7":"from keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN\nrnn_model = Sequential()\n# note : below we add embedding layer\nrnn_model.add(embedding_layer)\nrnn_model.add(SimpleRNN(32))\nrnn_model.add(Dense(1, activation='sigmoid'))\nrnn_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = rnn_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","ef4495d4":"from keras.layers import LSTM\nlstm_model = Sequential()\n# note : below we add embedding layer\nlstm_model.add(embedding_layer)\nlstm_model.add(LSTM(32))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nlstm_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = lstm_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","c9b224fc":"from keras.layers import GRU\ngru_model = Sequential()\n# note : below we add embedding layer\ngru_model.add(embedding_layer)\ngru_model.add(GRU(32))\ngru_model.add(Dense(1, activation='sigmoid'))\n\ngru_model.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = gru_model.fit(x_train, y = targets,epochs=10, batch_size=32,validation_split=0.2)","1200ebe8":"# lets predict target values for test set\nmodel =  dnn_model\nraw_preds = model.predict(x_test)\npreds = raw_preds.round().astype(int)\npreds","e644f323":"Image(\"\/kaggle\/input\/images\/too much.png\",  width=450)","b82961b3":"#### SkipGram Model","1f64dbaf":"##### Trained Glove","87a5e5ce":"##### PreTrained Fasttext\n\nFastText developers have also made available pre-computed embeddings for millions of english tokens, obtained from training Wikipedia data and common crawl data. \n\nDisclaimer: Loading the fastText pretrain will consume some serious memory.","55290dfc":"###  4.2 CNN <a class=\"kk\" id=\"4.2\"><\/a>","0339b670":"##### Vector representation of word","b6160008":"## 1.2 Padding <a class=\"kk\" id=\"1.2\"><\/a>\n\nPadding is task of appending string up to given specific length with whitespaces. Padding is used to represent all records as fixed length.","77db1508":"<B>Due to notebook memory constrains, I have commented out Pretrained Glove and Pretrained Fast-text.However, there usage is similar to pre-trained Word2Vec. You can uncomment and run on separate notebook <\/B>","c5aeaac1":"##### Pre-Trained FastText","4edf1066":"#####  Displaying FastText WordVector of given word ","b30c4ee9":"#####  Displaying Glove WordVector of a word ","4bda3552":"##### Fetch most similar words for any given word ","40f6a8d1":"###  2.1.1 Skip-Gram  <a class=\"kk\" id=\"2.1.1\"><\/a>\n \nSkip-Gram is designed to predict the context from base word. From a given word, Skip-gram model tries to predict its neighbouring words.   \n\nSkip-gram is a [(target, context), relevancy] generator. Skip-gram generator gives us pair of words and their relevance (a float value). Let's generate Word2Vec skip-gram embedding for our cleaned-up text corpus using gensim.","fa0c8dc2":"Intrestingly, there's higher similarity b\/w people & terrorists than people & saints :()\n","4f506195":"### 4.5 Recurrent Neural Network \u2013 GRU <a class=\"kk\" id=\"4.5\"><\/a>","40c0629a":"Before applying Word Embedding techniques lets look into into few common NLP vocabulary terms.","57b86928":"### Fasttext  Embedding layers  <a class=\"kk\" id=\"3.2.3\"><\/a>","89b67e3e":"##### Pre-Trained  Word2Vec","8ac77f56":"### 4.3 RNN <a class=\"kk\" id=\"4.3\"><\/a>","0418c52b":"## 2.1 Word2Vec    <a class=\"kk\" id=\"2.1\"><\/a>\n\n Word2Vec is group of related models that are used to produce Word Embeddings. It was created & patented by Tomas Mikolov and a group of a research team from Google in 2013. Each unique word in the corpus is assigned a corresponding vector in the space. Word2Vec relies only on local information of language hence the semantics learnt for a given word is only affected by the surrounding words. Underlying assumption of Word2Vec is that two words sharing similar contexts also share a similar meaning, this at times results into similar vector representation (cosine similarity) of multiple words. One more drawback of word2vec is it's unablity to take care of OutofVocabulary(OOV) word.\n","121a5111":" \n\n\nThanks for reading ! \n\nIf you like this kernel, <b>  Do Upvote<\/b>!!! Its at the top right near Copy&Edit and will not cost you:)\n\n\nIn the next [part-3](https:\/\/www.kaggle.com\/kksienc\/comprehensive-nlp-tutorial-3-bert-others) we will read about state-of-the-art 'BERT Embedding'.\n\n\n\n","06d1da68":"#### CBOW model","ae470964":"### 2.1.2 CBOW (Continuous Bag of Words)  <a class=\"kk\" id=\"2.1.2\"><\/a>\n\nCBOW is designed to predict the base(target) word from context. CBOW is faster to train than the skip-gram and gives slightly better accuracy for frequent words.\n","9620d1b0":"## 3.1 Vector Averaging  <a class=\"kk\" id=\"3.1\"><\/a>\nIn Vector Averaging approach we average all word embeddings (of words) in the text. Final length remains equal to word vector dimension. This is go to technique when we are planning to use machine learning models such a logistic regression, na\u00efve-bayes, svm etc.  \n ","1f6eafa2":"### Vector Averaging With Glove <a class=\"kk\" id=\"3.1.2\"><\/a>","869c04b1":"# 2. Word Embedding Techniques <a class=\"kk\" id=\"2\"><\/a>\n[Back to Contents](#0.1)\n\nNow we will dive deep into word Embedding techniques. But first let's fetch our [Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started) dataset and clean it, as we did in [part 1](https:\/\/www.kaggle.com\/kksienc\/comprehensive-nlp-tutorial-1-ml-perspective).","ef4946ef":"#### vector representation of Word2Vec pretrained word ","d8cad203":"## 1.1 Dimensionality  <a class=\"kk\" id=\"1.1\"><\/a>\n\nDimensionality refers to the length of vectors and equals to number of columns of vector representation.\nFor example, in the above figure each token is encoded into vector of dimesionality 3.","9af858dc":"# 3. Text to Numeric Convertion Using Word Vectors <a class=\"kk\" id=\"3\"><\/a>\n[Back to Contents](#0.1)\n\nWe learned about word embedding techniques and created word vectors for our corpus.  Now we will convert our textual data into numerical using these word vectors. I will explain about two popular texts to numerical conversion techniques using word vectors, \n\n1. Vector Averaging  \n1. Embedding Matrix and Keras Embedding layer\n","bea99302":"#####  Displaying FastText pretrained WordVector of a word ","865230f4":"## 2.2 GloVe  <a class=\"kk\" id=\"2.2\"><\/a>\n\n[GloVe](https:\/\/nlp.stanford.edu\/pubs\/glove.pdf) stands for \"Global Vectors\". It is a Word Embedding [project](https:\/\/nlp.stanford.edu\/projects\/glove\/)  written in C language and developed by Stanford university researchers in 2014. Glove embedding technique is based on \nfirstly, construction of a co-occurrence matrix from a training corpus and then \nsecondly, factorization of co-occurrence matrix in order to yield word vector.\n\nUnlike word2vec, which captures only local statistics of token, Glove captures both global statistics and local statistics of a text tokens. GloVe embeddings relate to the probabilities that two words appear together. [glove_python](https:\/\/github.com\/maciejkula\/glove-python) library provides glove implementation.\n\nFor more details of GloVe implementation of  glove_python refer their [official documentation](https:\/\/pypi.org\/project\/glove\/)\n","4e3d189b":"<a class=\"kk\" id=\"0.1\"><\/a>\n## Contents\n\n1. [Introduction to Word Embedding](#1)\n    1. [Dimensionality](#1.1) \n    1. [Padding](#1.2)\n    1. [Euclidean Distance](#1.3)\n    1. [Cosine Similarity](#1.4)\n1. [Word Embedding Techniques](#2)\n    1. [Word2Vec](#2.1)\n        1. [Skip-Gram](#2.1.1)\n        1. [CBOW (Continuous Bag of Words)](#2.1.2)\n    1. [GloVe](#2.2) \n    1. [FastText](#2.3)\n1. [Text to Numeric Convertion Using Word Vectors](#3)\n    1. [Vector Averaging](#3.1)\n        1. [Vector Averaging With Word2Vec](#3.1.1)\n        1. [Vector Averaging With GloVe](#3.1.2)\n        1. [Vector Averaging With FastText](#3.1.3) \n    1. [Embedding Matrix & Keras Embedding layer](#3.2)\n        1. [Word2Vec Embedding layers](#3.2.1)\n        1. [GloVe Embedding layers](#3.2.2)\n        1. [FastText Embedding layers](#3.2.3)      \n1. [Deep Learning models](#4)\n    1. [Basic-DNN](#4.1)\n    1. [CNN](#4.2)\n    1. [RNN](#4.3)\n    1. [Recurrent Neural Network -LSTM](#4.4)\n    1. [Recurrent Neural Network \u2013 GRU](#4.5)\n ","ed0d5576":"###  Target Prediction  ","dc77d3b0":"## 4. Deep Learning Models <a class=\"kk\" id=\"4\"><\/a>\n[Back to Contents](#0.1)\n\n We have initialized Keras embedding layer for our various word embedding models. Now it\u2019s time to train using deep learning models. I will demonstrate how to train for glove pertained layer. You can test with other six embedding layer also (by just reassigning embedding_layer). One point you will notice that pretrained embedding layers performs much better than their trained counter parts. Again the purpose here is to depict basic Deep Learning model performance and not to obtain high score. ","41be77b4":"##### Validate dimension of our word vector","dd6ec53e":"### 4.1 Basic DNN <a class=\"kk\" id=\"4.1\"><\/a>\n","34371f3a":"##### Trained  CBOW","ccaba78e":"## 1.4 Cosine Similarity  <a class=\"kk\" id=\"1.4\"><\/a>\nCosine similarity is a measure of similarity between two nonzero vectors of an inner product space. It measures the cosine of the angle between them. \n ","4eb68394":"#### Pretrain Glove\n\nGlove developers have also made available pre-computed embeddings for millions of English tokens, obtained from training Wikipedia data and Common crawl data.","446e31a7":"Lets create Keras embedding layers for our word embeddings. As we have created 4 trained word embedding model (skipgram, cbow, glove and fasttext) and 3 pretrained model (one each for word2vec, glove and fasttext) for all these seven we will create a Keras embedding layer.\n\n### Word2Vec Embedding layers  <a class=\"kk\" id=\"3.2.1\"><\/a>","6281afc2":"#####   Glove pretrained WordVector of a word ","d8ffc479":"## 3.2 Embedded Matrix & Keras Embedding layer <a class=\"kk\" id=\"3.2\"><\/a>\n\nAveraging is preferred choice when we intend to use Machine Learning models such as lr, svm, gbm etc. but our purpose here is to utilise Deep-learning algorithms. Deep Learning is a layer bases learning where each layer passes its learning to the next layer.   Few libraries have implement deep learning algorithms. A popular one among them is Keras. We will use Keras for our deep learning modelling purpose.\n\nFor text processing Keras offers an embedding layer. This is the first layer of deep learning algorithm. Weights of the Embedding layer are of the shape (vocabulary_size, embedding_dimension), this weight matrix is also called as Embedding matrix. We will first generate this embedding matrix from our word vectors and then initialize Keras embedding layer for each of our word embeddings. \n\nMoreover, Keras has built-in utilities for doing tokenization and encoding of text. We will use these utilities as they take care of a number of important features such as stripping special characters from strings, padding, fetching N most common words in dataset etc.","8cf16c0e":"##### Trained skipgram","8fc0b6f7":"####  Pretrain Word2Vec\n\nGoogle has made available pretrained word embedding which includes word vectors for a vocabulary of 3 million words and phrases that they have trained on roughly 100 billion words from Google News dataset using Word2Vec.","665a3ba9":"#### Vector Averaging With pretrained Embeddings\n\n ","a2d02bee":"##### Trained FastText","69f181f2":"### 4.4  Recurrent Neural Network -LSTM <a class=\"kk\" id=\"4.4\"><\/a>","3e93bb67":"## References\n- Deep Learning with Python by FRANC\u0327OIS CHOLLET  http:\/\/faculty.neu.edu.cn\/yury\/AAI\/Textbook\/Deep%20Learning%20with%20Python.pdf\n- https:\/\/en.wikipedia.org\/\n- https:\/\/www.oreilly.com\/library\/view\/statistics-for-machine\/9781788295758\/eb9cd609-e44a-40a2-9c3a-f16fc4f5289a.xhtml\n- https:\/\/www.kdnuggets.com\/2018\/04\/implementing-deep-learning-methods-feature-engineering-text-data-skip-gram.html\n- https:\/\/www.kaggle.com\/c\/quora-question-pairs\/discussion\/31257#177483\n- https:\/\/www.kaggle.com\/slatawa\/simple-implementation-of-word2vec\n- https:\/\/becominghuman.ai\/how-does-word2vecs-skip-gram-work-f92e0525def4\n- https:\/\/www.thinkinfi.com\/2019\/06\/single-word-cbow.html(image)\n- https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer\n- https:\/\/medium.com\/@japneet121\/word-vectorization-using-glove-76919685ee0b\n- https:\/\/www.kaggle.com\/christofhenkel\/fasttext-starter-description-only\n","be80fbfa":"##### Measure similarity   b\/w two words ","982ec0df":"We have just build our first word-embedding model and that also with only 3 lines of code. Lets play with this model.","78d92bde":"Did you notice CBOW trained faster than Skipgram ?\n","686201d0":"# 1. Introduction to Word Embedding  <a class=\"kk\" id=\"1\"><\/a>\n[Back to Contents](#0.1)\n\nWord Embedding is also known as Word Vectorization. It means converting word into vector. Vectors are numeric representation of a point in space. Mathematically vectors are 1D array or sequence of numbers.  \n\n\n<B>Why we need Word Embedding? <\/B>\n\nA problem with our previous text to numeric conversion techniques (countvectorizer & tfidf) was that, they ignore synonyms. For example, word 'measure' and \u2018calculate\u2019 were represented differently however, in most sentences they can be used interchangeably. In Word Embedding, similar words are spatially close to each other in vector space. Word Embedding is also capable of preserving semantic and syntactic similarity and relation with other words. The vector representation are such that geometric transformation adopts syntax and semantic. For instance, by adding a \u201cfemale\u201d vector to the vector \"king\", we obtain the vector \"queen\" and by adding a \"plural\" vector to the vector \"king\", we obtain \"kings\". \n\nAnother problem we observe in [part-1](https:\/\/www.kaggle.com\/kksienc\/comprehensive-nlp-tutorial-1-ml-perspective), was production of high dimensionality sparse matrix. Word Embedding solve this also, as it produces low dimensional dense matrix.\n","ef09e542":"## 2.3. Fast-Text <a class=\"kk\" id=\"2.3\"><\/a>\n\n[FastText](https:\/\/fasttext.cc\/) is a library for learning of word embeddings and text classification. The Facebook Research Team created fastText in Nov 2015. Fast-Text is an extension of word2vec library. It builds on Word2Vec by learning vector representations for each word and the n-grams found within each word. FastText assumes a word to be formed by a n-grams of character. For example, sunny is composed of [sun, sunn,sunny],[sunny,unny,nny]... etc, where n could range from 1 to the length of the word. The values of the representations are then averaged into one vector at each training step. While this adds a lot of additional computation to training it enables word embeddings to encode sub-word information. Thus even for previously unseen words, typo errors, and OOV (Out Of Vocabulary) words the model can make an educated guess towards its meaning.Obvious trade off is processing time. Gensim provides the [FastText implementation](https:\/\/radimrehurek.com\/gensim\/models\/fasttext.html).\n\nFor more details of FastText implementation of  Gensim refer their [official documentation](https:\/\/radimrehurek.com\/gensim\/models\/fasttext.html)\n\n","9d145305":"##### Fetch list of word vocabulary ","e6a835f4":"### Vector Averaging With Word2Vec <a class=\"kk\" id=\"3.1.1\"><\/a>","431214a5":"### Vector Averaging With Fasttext  <a class=\"kk\" id=\"3.1.3\"><\/a>\n\nAs Fastext is an extension of word2vec hence the same averaging function of w2vec i.e.`get_ w2v_embeddings` will work for fasttext model too. ","d837dfc3":"### GloVe Embedding Layers   <a class=\"kk\" id=\"3.2.2\"><\/a>","bcf373d8":"## 1.3 Euclidean Distance <a class=\"kk\" id=\"1.3\"><\/a>\n\nEuclidean distance is the shortest distance between two points in (Euclidean) space.\n\n ","9d31a9e4":"This tutorial is part-2 of [Comprehensive tutorial on NLP](https:\/\/www.kaggle.com\/kksienc\/comprehensive-nlp-tutorial-1-ml-perspective) series. In this part we will learn about word embeddings and see how Deep learning has simplified NLP. \n\nPre-requisite: Basic Deep learning understanding would be helpful though not mandatory.","79f511ad":"Word2Vec comes in two flavours,\n - Skip-Gram and \n - Continuous Bag of Words (CBOW)\n\nUnderneath, Word2Vec uses neural network algorithms that can be trained on any type of sequential data. Fortunately we have libraries available that have already implemented these algorithms and we have to just call the method with proper arguments. A popular one among such libraries is <B>gensim<\/B>. It provides the [Word2Vec Class](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.htm) for working with a Word2Vec model.\n\nFor more details of Gensim implementation of Wrod2Vec refer their [official documentation](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)\n","c9071043":"##### PreTrained Glove","5cde34bd":"As mention earlier, due to notebook memory constrains, I have commented out Pretrained-Glove and Pretrained-Fasttext here.The usage is quite similar to pre-trained Word2Vec. ","45c694f8":"#### Building Skipgram  WordVectors using gensim"}}