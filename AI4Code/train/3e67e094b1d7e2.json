{"cell_type":{"92abd87e":"code","8e3d8def":"code","e9e8eb08":"code","91f0363c":"code","1d6df5eb":"code","4c30c72c":"code","af010d52":"code","b65e7f47":"code","bae40f96":"code","0c94af10":"code","db28e7d0":"code","c131d972":"code","fea11892":"code","2d2dc8b9":"code","49bf9a27":"code","2ea64105":"code","75e7e169":"code","a0216c24":"code","a2b26ea3":"code","917e846e":"code","c449d95b":"code","28c5e26b":"code","321b567c":"code","877497dc":"code","561b5546":"code","293718df":"code","2de5f88e":"code","af458dce":"code","d4e3f38c":"code","ae24d46c":"code","39ac4c7f":"code","9b151d86":"code","1617bcd7":"code","53ef325b":"markdown","082a3577":"markdown","facc1f01":"markdown","b4f792f4":"markdown","d56e7180":"markdown","7a319fc2":"markdown","d60092ff":"markdown","f2d360de":"markdown","06edb544":"markdown","8f439c15":"markdown","46df718c":"markdown","06484929":"markdown","862090c1":"markdown","c91c5341":"markdown","66927ae2":"markdown","2eb0f501":"markdown"},"source":{"92abd87e":"!pip install deeppavlov","8e3d8def":"# Import Libraries\n\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\n\n# deeppavlov\nfrom deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\nfrom deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\nfrom deeppavlov.models.preprocessors.str_lower import StrLower\nfrom deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer\nfrom deeppavlov.core.data.simple_vocab import SimpleVocabulary\nfrom deeppavlov.models.embedders.bow_embedder import BoWEmbedder\nfrom deeppavlov.core.data.utils import simple_download\nfrom deeppavlov.models.embedders.glove_embedder import GloVeEmbedder\nfrom deeppavlov.metrics.accuracy import sets_accuracy\nfrom deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel\nfrom deeppavlov.models.preprocessors.one_hotter import OneHotter\nfrom deeppavlov.models.classifiers.proba2labels import Proba2Labels\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e9e8eb08":"data = pd.read_json('..\/input\/News_Category_Dataset_v2.json', lines=True)\ndata.head()","91f0363c":"data.shape","1d6df5eb":"data.isnull().sum()","4c30c72c":"# connect 'headline' and 'short_description'\ndata['text'] = data['headline'] + \" \" + data['short_description']","af010d52":"# export csv\ndata.to_csv('..\/News_Category_Dataset_v2.csv')","b65e7f47":"# read data from particular columns of `.csv` file\ndr = BasicClassificationDatasetReader().read(\n    data_path='..\/',\n    train='News_Category_Dataset_v2.csv',\n    x = 'text',\n    y = 'category'\n)","bae40f96":"# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8\/0.2\ntrain_iterator = BasicClassificationDatasetIterator(\n    data=dr,\n    field_to_split='train',  # field that will be splitted\n    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n    split_proportions=[0.8, 0.2],  #proportions for splitting\n    split_seed=23,  # seed for splitting dataset\n    seed=42)  # seed for iteration over dataset","0c94af10":"# one can get train instances (or any other data type including `all`)\nx_train, y_train = train_iterator.get_instances(data_type='train')\nfor x, y in list(zip(x_train, y_train))[:5]:\n    print('x:', x)\n    print('y:', y)\n    print('=================')","db28e7d0":"str_lower = StrLower()\n# check\nstr_lower(['Kaggle is the best place to study machine learning.'])","c131d972":"tokenizer = NLTKMosesTokenizer()\n# check\ntokenizer(['Kaggle is the best place to study machine learning.'])","fea11892":"train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))","2d2dc8b9":"# initialize simple vocabulary to collect all appeared in the dataset classes\nclasses_vocab = SimpleVocabulary(\n    save_path='.\/tmp\/classes.dict',\n    load_path='.\/tmp\/classes.dict')","49bf9a27":"classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\nclasses_vocab.save()","2ea64105":"# show classes\nlist(classes_vocab.items())","75e7e169":"# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\ntoken_vocab = SimpleVocabulary(\n    save_path='.\/tmp\/tokens.dict',\n    load_path='.\/tmp\/tokens.dict',\n    min_freq=2,\n    special_tokens=('<PAD>', '<UNK>',),\n    unk_token='<UNK>')","a0216c24":"token_vocab.fit(train_x_lower_tokenized)\ntoken_vocab.save()","a2b26ea3":"# number of tokens in dictionary\nlen(token_vocab)","917e846e":"# 10 most common words and number of times their appeared\ntoken_vocab.freqs.most_common()[:10]","c449d95b":"# initialize bag-of-words embedder giving total number of tokens\nbow = BoWEmbedder(depth=token_vocab.len)\n# it assumes indexed tokenized samples\nbow(token_vocab(str_lower(tokenizer(['Kaggle is the best place to study machine learning.']))))","28c5e26b":"# all 10 tokens are in the vocabulary\nsum(bow(token_vocab(str_lower(tokenizer(['Kaggle is the best place to study machine learning.']))))[0])","321b567c":"# Glove : https:\/\/nlp.stanford.edu\/projects\/glove\/\nsimple_download(url=\"http:\/\/files.deeppavlov.ai\/embeddings\/glove.6B.100d.txt\", destination=\".\/glove.6B.100d.txt\")","877497dc":"embedder = GloVeEmbedder(load_path='.\/glove.6B.100d.txt',dim=100, pad_zero=True)","561b5546":"# get all train and valid data from iterator\nx_train, y_train = train_iterator.get_instances(data_type=\"train\")\nx_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")","293718df":"# Intialize `KerasClassificationModel` that composes CNN shallow-and-wide network \n# (name here as`cnn_model`)\ncls = KerasClassificationModel(save_path=\".\/cnn_model_v0\", \n                               load_path=\".\/cnn_model_v0\", \n                               embedding_size=embedder.dim,\n                               n_classes=classes_vocab.len,\n                               model_name=\"cnn_model\",\n                               text_size=15, # number of tokens\n                               kernel_sizes_cnn=[3, 5, 7],\n                               filters_cnn=128,\n                               dense_size=100,\n                               optimizer=\"Adam\",\n                               learning_rate=0.1,\n                               learning_rate_decay=0.01,\n                               loss=\"categorical_crossentropy\")","2de5f88e":"onehotter = OneHotter(depth=classes_vocab.len, single_vector=True)","af458dce":"for ep in range(10):\n    for x, y in tqdm(train_iterator.gen_batches(batch_size=64, \n                                           data_type=\"train\")):\n        x_embed = embedder(tokenizer(str_lower(x)))\n        y_onehot = onehotter(classes_vocab(y))\n        cls.train_on_batch(x_embed, y_onehot)","d4e3f38c":"cls.save()","ae24d46c":"# Infering on validation data we get probability distribution on given data.\ny_valid_pred = cls(embedder(tokenizer(str_lower(x_valid))))","39ac4c7f":"prob2labels = Proba2Labels(max_proba=True)","9b151d86":"# Let's look into obtained result\nprint(\"Text sample: {}\".format(x_valid[0]))\nprint(\"True label: {}\".format(y_valid[0]))\nprint(\"Predicted probability distribution: {}\".format(dict(zip(classes_vocab.keys(), \n                                                               y_valid_pred[0]))))\nprint(\"Predicted label: {}\".format(classes_vocab(prob2labels(y_valid_pred))[0]))","1617bcd7":"# calculate sets accuracy\nsets_accuracy(y_valid, classes_vocab(prob2labels(y_valid_pred)))","53ef325b":"## GloVe Embedder","082a3577":"# Data Load","facc1f01":"## Tokenizer","b4f792f4":"# Importing Libraries","d56e7180":"# Model Build","7a319fc2":"1. [**Importing Libraries**](#Importing-Libraries)   \n2. [**Data Load**](#Data-Load)  \n3. [**Data Preprocessing**](#Data-Preprocessing)  \n    3-1 [**Lowercasing**](#Lowercasing)  \n    3-2 [**Tokenizer**](#Tokenizer)  \n    3-3 [**Vocabulary**](#Vocabulary)  \n    3-4 [**Bag-of-words**](#Bag-of-words)  \n    3-5 [**GloVe Embedder**](#GloVe-Embedder)  \n4. [**Model Build**](#Model-Build)  \n5. [**Train**](#Train)  \n6. [**Check Result**](#Check-Result)  ","d60092ff":"# Data Preprocessing","f2d360de":"## Bag-of-words","06edb544":"## Vocabulary","8f439c15":"# Check Result","46df718c":"<img src=\"http:\/\/static.minne.com\/productimages\/41458659\/large\/5fd6279befc739c4a7b59269397cdeaba3e2f6e0.jpg?1506897964\" width=\"700px\">","06484929":"**If you like it, please upvote ;)**","862090c1":"# News Category Classification","c91c5341":"# Train","66927ae2":"## Lowercasing","2eb0f501":"# Notebook Outline"}}