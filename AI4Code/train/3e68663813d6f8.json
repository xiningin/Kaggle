{"cell_type":{"0acba042":"code","ad3e4efc":"code","45ad5977":"code","2a90acf5":"code","a8ccdd58":"code","d9f2341d":"code","04488b67":"code","89a9f1b3":"code","6612b59e":"code","93d44103":"code","46ef91ba":"code","74ffa145":"code","6ac7adec":"code","6624480b":"code","801356d2":"code","531f003b":"code","5c626bc8":"code","ffca6d5e":"code","1bd63882":"code","a8dd1300":"code","5c79d5db":"code","0ad90949":"code","3729399f":"code","e5b094a1":"code","5bd5cad0":"code","9b5fcffa":"code","f905bcf7":"code","222c8870":"code","dd7f93ce":"code","0cf7d7fb":"code","19b59361":"code","19fa43a7":"code","4a7e82e9":"markdown","bde215f2":"markdown","6f1e58e2":"markdown","f55b7733":"markdown","523609c6":"markdown"},"source":{"0acba042":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ad3e4efc":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom numpy import absolute\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np  # linear algebra\nimport pandas as pd  #\nfrom datetime import datetime\n\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport os\n\n\nprint(os.listdir(\"..\/input\"))","45ad5977":"# Training Data\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","2a90acf5":"# Testing Data\n\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","a8ccdd58":"# Saving ID on a new variable and Dropping ID column\n\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","d9f2341d":"# Plotting to find outliers \n# for items in train.columns:\n#     if len(train[items].unique()) < 20:\n#         sns.catplot(x=train[items], y=train['SalePrice'], data=train)\n#         plt.show()\n#     else:\n#         sns.scatterplot(x=train[items],y=train['SalePrice'])\n#         plt.show()","04488b67":"# Removing Outlier\ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop=True, inplace=True)\nprint(train.shape)","89a9f1b3":"# Plotting output data for Visualization\ntrain['SalePrice'].hist(bins = 40)","6612b59e":"# Adjusting for Skewness\n# train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\ntrain[\"SalePrice\"] = train[\"SalePrice\"]\ny_train = train['SalePrice'].reset_index(drop=True)\n\ntrain['SalePrice'].hist(bins = 40)","93d44103":"# Concat Testing part of both Train and Test and split SalePrice from Train data for Training\ntrain_data = train.drop(['SalePrice'], axis=1)\ntotal_data = pd.concat([train_data, test]).reset_index(drop=True)","46ef91ba":"# Total data is full data to be used for EDA\ntotal_data.shape","74ffa145":"print('columns containing missing values =',total_data.isnull().any().sum())","6ac7adec":"# Finding missing values to plot a Graph\nmissing_counts = pd.DataFrame(total_data.isnull().sum().sort_values(ascending=False))\nmissing_columns = missing_counts[missing_counts.iloc[:,0]>0]\n# Plotting Missing Values\nplt.figure(figsize=(20,10))\nmissing_columns = missing_counts[missing_counts.iloc[:,0]>0]\nsns.barplot(x=missing_columns.index,y=missing_columns.iloc[:,0])\nplt.xticks(rotation=90)\nprint(missing_columns)\nplt.show()","6624480b":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \ntotal_data['MSSubClass'] = total_data['MSSubClass'].apply(str)\ntotal_data['YrSold'] = total_data['YrSold'].astype(str)\ntotal_data['MoSold'] = total_data['MoSold'].astype(str)\n\n# Fixing columns : filling Null values with suitable values in columns\ntotal_data['Functional'] = total_data['Functional'].fillna('Typ')\ntotal_data['Electrical'] = total_data['Electrical'].fillna(\"SBrkr\")\ntotal_data['KitchenQual'] = total_data['KitchenQual'].fillna(\"TA\")\ntotal_data['Exterior1st'] = total_data['Exterior1st'].fillna(total_data['Exterior1st'].mode()[0])\ntotal_data['Exterior2nd'] = total_data['Exterior2nd'].fillna(total_data['Exterior2nd'].mode()[0])\ntotal_data['SaleType'] = total_data['SaleType'].fillna(total_data['SaleType'].mode()[0])\ntotal_data[\"PoolQC\"] = total_data[\"PoolQC\"].fillna(\"None\")","801356d2":"# Filling columns with Mode and Median values\ntotal_data[\"LotFrontage\"].fillna(total_data[\"LotFrontage\"].median(),inplace=True)\ntotal_data[\"MSZoning\"].fillna(total_data[\"MSZoning\"].mode(),inplace=True)","531f003b":"# Filling some columns Nan values with 0's\n\nfor item in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    total_data[item] = total_data[item].fillna(0)\n\n# Filling categorial columns Nan Values with 'None'\n\nfor item in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    total_data[item] = total_data[item].fillna('None')","5c626bc8":"# Filling remaining Object type columns Nan values with None\nobjects = []\nfor i in total_data.columns:\n    if total_data[i].dtype == object:\n        objects.append(i)\ntotal_data.update(total_data[objects].fillna('None'))","ffca6d5e":"# Filling remaining Numerical type columns with 0 values\nnumerical = total_data.select_dtypes(include=np.number).columns.tolist()\nfor i in numerical:\n    total_data.update(total_data[i].fillna(0))","1bd63882":"# Dropping unwanted columns\ntotal_data = total_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","a8dd1300":"# Engineered new columns \ntotal_data['YrBltAndRemod']=total_data['YearBuilt']+total_data['YearRemodAdd']\ntotal_data['TotalSF']=total_data['TotalBsmtSF'] + total_data['1stFlrSF'] + total_data['2ndFlrSF']\n\ntotal_data['Total_sqr_footage'] = (total_data['BsmtFinSF1'] + total_data['BsmtFinSF2'] +\n                                 total_data['1stFlrSF'] + total_data['2ndFlrSF'])\n\ntotal_data['Total_Bathrooms'] = (total_data['FullBath'] + (0.5 * total_data['HalfBath']) +\n                               total_data['BsmtFullBath'] + (0.5 * total_data['BsmtHalfBath']))\n\ntotal_data['Total_porch_sf'] = (total_data['OpenPorchSF'] + total_data['3SsnPorch'] +\n                              total_data['EnclosedPorch'] + total_data['ScreenPorch'] +\n                              total_data['WoodDeckSF'])","5c79d5db":"# Simplifying Features\ntotal_data['haspool'] = total_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['has2ndfloor'] = total_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['hasgarage'] = total_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['hasbsmt'] = total_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_data['hasfireplace'] = total_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","0ad90949":"# Adding pandas dummy values to encode features\nprint(total_data.shape)\nfinal_data = pd.get_dummies(total_data).reset_index(drop=True)\nprint(final_data.shape)","3729399f":"# Splitting Train and test values now with help of variable 'y' \nx_train = final_data.iloc[:len(y_train), :]\nx_test = final_data.iloc[len(x_train):, :]\n\nprint('x_train', x_train.shape)\nprint('y_train', y_train.shape)\nprint('x_test', x_test.shape)","e5b094a1":"# Removing Overfitting features\noverfit = []\nfor i in x_train.columns:\n    counts = x_train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(x_train) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\n# MSZoning_C have an extra field added in data \noverfit.append('MSZoning_C (all)')\n\nX = x_train.drop(overfit, axis=1).copy()\nX_test = x_test.drop(overfit, axis=1).copy()\ny = y_train.copy()\n\nprint('X_train', X.shape)\nprint('Y_train', y.shape)\nprint('X_test', X_test.shape)","5bd5cad0":"# Sequential Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nnn = Sequential()\nnn.add(Dense(118, activation='relu', kernel_initializer='normal', input_dim = 331))\nnn.add(Dense(59, activation='relu'))\nnn.add(Dense(1, kernel_initializer='normal'))","9b5fcffa":"# Complie the model\nfrom keras.optimizers import Adam\nopt = Adam(lr=0.1)\nnn.compile(loss='mean_squared_error', optimizer = opt )","f905bcf7":"# Model summary \nnn.summary()","222c8870":"# Define XG-Boost Model\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006, random_state=42)","dd7f93ce":"# Fitting both the models on test set\n\nnn.fit(X,y, epochs = 400, batch_size = 32,verbose = 0)\nxgb.fit(X,y)","0cf7d7fb":"# Ensembling with the weighted average model\n\n# Predicting test data with neural net and XG-boost models\npred1 = nn.predict(X_test)\npred2 = xgb.predict(X_test)\npred2 = pred2.reshape(1459,1)\n\n# Combining both the predictions with average of 0.5 to both model\n# You can change the average weightage of the individual models \nfinalpred=( pred1 * 0.5 + pred2 * 0.5)\nfinalpred","19b59361":"# Predict submission\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(finalpred)","19fa43a7":"# Submission\nsubmission.to_csv(\"submission.csv\", index=False)","4a7e82e9":"Data is skewed, we will use log1p to remove the skewness in data","bde215f2":"Defining and training models","6f1e58e2":"Data Engineering","f55b7733":"Visualizing output data from train set","523609c6":"Data Analysis"}}