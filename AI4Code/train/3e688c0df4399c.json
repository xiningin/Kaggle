{"cell_type":{"2ebcd766":"code","f80d3071":"code","8ccaa582":"code","2f12dfc0":"code","5403a9ba":"code","f7d00cf5":"code","ac3f9e20":"code","f025f1cf":"code","ce7ee65f":"code","4cad1c2c":"code","67f44684":"code","22a7cef9":"code","a6953f76":"code","25228c93":"code","49408470":"code","95857389":"code","afc97761":"code","1de01971":"code","219a6f79":"code","d6552305":"code","8f989ca8":"code","6c99d895":"code","6c1eff48":"code","3095ae63":"code","58fdf3c4":"code","a4f509cf":"code","f1048c4a":"code","3e95e522":"code","29e606e9":"code","159f62d9":"code","4cc19997":"code","78b48423":"code","8ff19456":"code","29406773":"code","da88743f":"code","b3edaf8b":"code","f8108894":"code","9eddbc34":"code","28a2b906":"code","778869f2":"code","2a90a602":"code","d6b29b56":"code","b2df3a4b":"code","0f9e3e77":"code","c08b6514":"code","7ce79aa9":"code","be8c1446":"code","5a77676d":"code","00865a2b":"code","c0cca0ec":"code","dfbbd1b3":"code","faadad1d":"code","1089f175":"code","a469fa6d":"code","2e2b952b":"code","422ed25c":"code","861513fa":"code","9f865db4":"code","731aac71":"code","081d29f0":"code","bca067dd":"code","a35d881a":"code","d5c6408e":"code","7e07b74e":"code","18da9b45":"code","6fc2d211":"code","fa8740be":"code","372b9f14":"code","e0d9c394":"code","5c06ee4f":"code","a48baa85":"code","167c5ef5":"code","33feb61e":"code","845bf33a":"code","782c5ba8":"code","df242240":"code","c47c5a82":"code","e225acf2":"code","261eb465":"code","4d9fea79":"code","b3f2ef0c":"code","4fef35ee":"code","1937baf2":"markdown","28dfb27d":"markdown","bab34af4":"markdown"},"source":{"2ebcd766":"import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt","f80d3071":"import warnings\nwarnings.filterwarnings('ignore')","8ccaa582":"heart=pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","2f12dfc0":"heart.head()","5403a9ba":"heart.shape","f7d00cf5":"heart.info()","ac3f9e20":"heart.describe()","f025f1cf":"heart.describe(percentiles=[0.25,0.50,0.75,0.90,0.99]).T","ce7ee65f":"from sklearn.model_selection import train_test_split","4cad1c2c":"y=heart.pop('output')","67f44684":"X=heart","22a7cef9":"# train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.3, random_state=100)","a6953f76":"from sklearn.preprocessing import MinMaxScaler","25228c93":"scaler= MinMaxScaler()","49408470":"col= ['age','cp','trtbps','chol','restecg','thalachh','oldpeak','slp','caa','thall']","95857389":"# scaling the col\nX_train[col]= scaler.fit_transform(X_train[col])","afc97761":"X_train.head()","1de01971":"import statsmodels.api as sm","219a6f79":"lr01= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","d6552305":"print(lr01.fit().summary())","8f989ca8":"# dropping 'age' because it is insignificant \nX_train.drop('age',1,inplace=True)","6c99d895":"lr02= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","6c1eff48":"print(lr02.fit().summary())","3095ae63":"# dropping 'restecg'\nX_train.drop('restecg',1, inplace=True)","58fdf3c4":"lr03= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","a4f509cf":"print(lr03.fit().summary())","f1048c4a":"# dropping 'fbs'\nX_train.drop('fbs',1, inplace=True)","3e95e522":"lr04= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","29e606e9":"print(lr04.fit().summary())","159f62d9":"# dropping 'exng'\nX_train.drop('exng',1, inplace=True)","4cc19997":"lr05= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","78b48423":"print(lr05.fit().summary())","8ff19456":"# dropping 'trtbps'\nX_train.drop('trtbps',1, inplace=True)","29406773":"lr06= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","da88743f":"print(lr06.fit().summary())","b3edaf8b":"# dropping 'chol'\nX_train.drop('chol',1, inplace=True)","f8108894":"lr07= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","9eddbc34":"print(lr07.fit().summary())","28a2b906":"# dropping 'slp'\nX_train.drop('slp',1, inplace=True)","778869f2":"lr08= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","2a90a602":"print(lr08.fit().summary())","d6b29b56":"# checking VIFs\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","b2df3a4b":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0f9e3e77":"# dropping 'thall' because VIF>10\nX_train.drop('thall',1, inplace=True)","c08b6514":"lr09= sm.GLM(y_train, sm.add_constant(X_train), family= sm.families.Binomial())","7ce79aa9":"print(lr09.fit().summary())","be8c1446":"vif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5a77676d":"# lr09 is our final model, fitting the model\nlr09= lr09.fit()","00865a2b":"y_train_pred= lr09.predict(sm.add_constant(X_train))","c0cca0ec":"y_train_pred= y_train_pred.values.reshape(-1) ","dfbbd1b3":"y_train_pred_final = pd.DataFrame({'output':y_train.values, 'output_Prob':y_train_pred})\ny_train_pred_final['ID'] = y_train.index\ny_train_pred_final.head()","faadad1d":"numbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.output_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","1089f175":"from sklearn import metrics","a469fa6d":"cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.output, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","2e2b952b":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","422ed25c":"y_train_pred_final['final_predicted'] = y_train_pred_final.output_Prob.map( lambda x: 1 if x > 0.6 else 0)\n\ny_train_pred_final.head()","861513fa":"metrics.accuracy_score(y_train_pred_final.output, y_train_pred_final.final_predicted)","9f865db4":"# confusion matrix\nconfusion1 = metrics.confusion_matrix(y_train_pred_final.output, y_train_pred_final.final_predicted )\nconfusion1","731aac71":"TP = confusion1[1,1] # true positive \nTN = confusion1[0,0] # true negatives\nFP = confusion1[0,1] # false positives\nFN = confusion1[1,0] # false negatives","081d29f0":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","bca067dd":"# Let us calculate specificity\nTN \/ float(TN+FP)","a35d881a":"X_test.head()","d5c6408e":"col= ['age','cp','trtbps','chol','restecg','thalachh','oldpeak','slp','caa','thall']","7e07b74e":"# scaling the col\nX_test[col]= scaler.transform(X_test[col])","18da9b45":"X_test.head()","6fc2d211":"X_train.columns","fa8740be":"# retaing the columns which are present in X_train\nX_test.drop(['age','trtbps','chol','fbs','restecg','exng','slp','thall'],1,inplace=True)","372b9f14":"# Prediction on test dataset using lr09\ny_test_pred=lr09.predict(sm.add_constant(X_test))","e0d9c394":"# converting it to DataFrame\ny_test_pred= pd.DataFrame(y_test_pred)\ny_test= pd.DataFrame(y_test)\ny_test['ID']=y_test.index","5c06ee4f":"y_test_pred.reset_index(drop=True, inplace=True)\ny_test.reset_index(drop=True, inplace=True)","a48baa85":"y_pred_final=pd.concat([y_test,y_test_pred],1)","167c5ef5":"y_pred_final.head()","33feb61e":"y_pred_final=y_pred_final.rename(columns={0:'output_prob'})","845bf33a":"y_pred_final.head()","782c5ba8":"y_pred_final['final_pred']=y_pred_final.output_prob.map(lambda x: 1 if x>0.6 else 0)","df242240":"y_pred_final.head()","c47c5a82":"# calculating the accuracy on test dataset\nmetrics.accuracy_score(y_pred_final.output, y_pred_final.final_pred)","e225acf2":"confusion2 = metrics.confusion_matrix(y_pred_final.output,y_pred_final.final_pred)","261eb465":"confusion2","4d9fea79":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","b3f2ef0c":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","4fef35ee":"# Let us calculate specificity\nTN \/ float(TN+FP)","1937baf2":"### Test Dataset","28dfb27d":"### **About this dataset**\n\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina\nValue 2: atypical angina\nValue 3: non-anginal pain\nValue 4: asymptomatic\ntrtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal\n\nValue 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n\nValue 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\nthalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack","bab34af4":"## Model Building"}}