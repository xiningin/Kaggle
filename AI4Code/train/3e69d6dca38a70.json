{"cell_type":{"4da0e0b4":"code","aa49b752":"code","b9431813":"code","a36c370c":"code","cc88e6f7":"code","f0b6a94f":"code","84e514a6":"code","b7adda3e":"code","28b629bd":"code","ed5d3dac":"code","da6344e3":"code","81650c8f":"code","2d55959c":"code","531038e4":"code","87d358ac":"code","815df2ad":"code","be80a861":"code","44e92368":"code","cce7de43":"code","fdf122c6":"code","94af9206":"code","8d1eb5a8":"code","07fd53aa":"code","066cf134":"code","7af93b45":"code","cf28b797":"code","c3589ee2":"code","6732e865":"code","30b9fc55":"code","0ad430a9":"code","103b7654":"code","dc0ebed2":"code","36cdf1df":"code","46c5f967":"code","bd13dec0":"code","87bdb8e5":"code","0355e8a0":"code","fba6dd4d":"code","661e8c5a":"code","85fdf7e5":"code","f598fc73":"code","9a7e8a72":"code","ebfba6ca":"code","846fa4c8":"code","e8d0053e":"code","f57acd7d":"code","0a51db5d":"code","0dc4d697":"code","2fb26de6":"code","8a71618a":"code","4e04d309":"code","b09da5c8":"code","c9bf6251":"code","855ff0b8":"code","18f71905":"code","1970ef16":"code","a6cb09bf":"code","4958352a":"code","94dc4968":"code","8d941792":"code","0d5bf834":"code","12a4bba2":"code","a971adc3":"code","f8447c87":"code","2fac25e8":"code","a5c79ac5":"code","478cc28a":"code","22211276":"code","ceefa410":"code","6899d6ee":"code","75087f3c":"code","d57b08d9":"code","eaa085b3":"code","2944b5a0":"code","04763f4b":"code","fbf7355b":"code","127d6b16":"code","079152d8":"code","7234099e":"code","bbb27396":"code","1f21944f":"code","eccf3753":"code","66efc5a4":"code","84358463":"code","8382e957":"code","33411cc8":"code","8df74cdb":"code","de1534e3":"code","2161faf5":"code","ae87b406":"code","512db860":"code","c1b4f4bf":"code","065bdde7":"code","ebaa14df":"code","c3a5ab89":"code","c9a2d585":"code","310b422d":"code","6701335a":"code","0e949318":"code","b8817927":"code","0e44eb46":"code","24a1d107":"code","1577b7b4":"code","b1deaa5d":"code","8aa4703d":"code","240ac4fb":"code","f391cf00":"code","55a924ef":"code","e21fe445":"code","4b3ba2a5":"code","288e736a":"code","12b3e166":"code","c44477a5":"code","57aaae34":"code","aef67dea":"code","227a6ce2":"code","32d934e3":"code","573b6364":"code","1b7b8e41":"code","420b045a":"code","9a9790b7":"code","4470e875":"code","beb2ad27":"code","090c8491":"code","fb30a4f0":"code","1a3aaaaa":"code","1698046b":"code","f4a383d5":"code","a278ef64":"code","3c74d32e":"code","deceadad":"code","b446383c":"code","e19b7347":"code","a1657cb8":"code","a8081548":"code","411124e7":"code","8b07e835":"code","2d1e20e3":"code","0ed8e6ce":"code","7f0c11de":"code","e022a506":"code","53c9fdef":"code","a7250ff7":"code","4c797479":"code","44b5c85b":"code","c99c7b0a":"code","b75feb78":"code","7fffa581":"code","138148ed":"code","f9769bb6":"code","418c3217":"code","206c1a79":"code","1d5bdab9":"code","1a4d78b4":"code","a6025d96":"code","dc739f27":"code","439eba69":"markdown","6a7d3c71":"markdown","82bbce3d":"markdown","df50ffe5":"markdown","2e31da21":"markdown","0fcfb0f6":"markdown","9e79094c":"markdown","78f94b45":"markdown","8f46eeca":"markdown","1ba9a0d3":"markdown","447b5db7":"markdown","c6d54ad0":"markdown","5965b631":"markdown","18611723":"markdown","290e9526":"markdown","2a4cb6b7":"markdown","9cf79354":"markdown","69a4534c":"markdown","c85ee3e8":"markdown","b224b163":"markdown","7faac48f":"markdown","33acd9a7":"markdown","76d357e3":"markdown","0346f4fa":"markdown","e3a61c07":"markdown","7c5d4926":"markdown","2cd24ecf":"markdown","bbcbb8ba":"markdown","5fe69f26":"markdown","79854a28":"markdown","9cc47048":"markdown","c10215cb":"markdown","29a07456":"markdown","68f2947d":"markdown","53087c5e":"markdown","b5d673d4":"markdown","de5e0a12":"markdown","6b42f30e":"markdown","f22dd4b3":"markdown","299f3f1a":"markdown","a44e5624":"markdown","28401a0f":"markdown","11567e0f":"markdown","a766fe4d":"markdown","2eb8e133":"markdown","1fe147ed":"markdown","abf100ea":"markdown","376bf278":"markdown","4976d6b3":"markdown","f3ddf2e2":"markdown","55aeeda0":"markdown","7a84d1b7":"markdown","aee6d39c":"markdown","4b1a0e17":"markdown","93670409":"markdown","77c41199":"markdown","f0a02752":"markdown","8c6508e7":"markdown","c6a4f05a":"markdown","ef565266":"markdown","db8acba4":"markdown","8e4f0c21":"markdown","c73e9c60":"markdown","31541148":"markdown","295a56e0":"markdown","954d886f":"markdown"},"source":{"4da0e0b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n#importing ploting libraries\nimport matplotlib.pyplot as plt\n#importing seaborn for statistical plots\nimport seaborn as sns\n#importing ploting libraries\nimport matplotlib.pyplot as plt\n#styling figures\nplt.rc('font',size=14)\nsns.set(style='white')\nsns.set(style='whitegrid',color_codes=True)\n#To enable plotting graphs in Jupyter notebook\n%matplotlib inline\n#importing the feature scaling library\nfrom sklearn.preprocessing import StandardScaler\n#Import Sklearn package's data splitting function which is based on random function\nfrom sklearn.model_selection import train_test_split\n# Import Linear Regression, Ridge and Lasso machine learning library\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\n# Import KNN Regressor machine learning library\nfrom sklearn.neighbors import KNeighborsRegressor\n# Import Decision Tree Regressor machine learning library\nfrom sklearn.tree import DecisionTreeRegressor\n# Import ensemble machine learning library\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor)\n# Import support vector regressor machine learning library\nfrom sklearn.svm import SVR\n#Import the metrics\nfrom sklearn import metrics\n#Import the Voting regressor for Ensemble\nfrom sklearn.ensemble import VotingRegressor\n# Import stats from scipy\nfrom scipy import stats\n# Import zscore for scaling\nfrom scipy.stats import zscore\n#importing the metrics\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\n#importing the K fold\nfrom sklearn.model_selection import KFold\n#importing the cross validation score\nfrom sklearn.model_selection import cross_val_score\n#importing the preprocessing library\nfrom sklearn import preprocessing\n# importing the Polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\n#importing kmeans clustering library\nfrom sklearn.cluster import KMeans\nfrom sklearn.utils import resample\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aa49b752":"#Read the Dataset\n\nconcrete_df=pd.read_csv('\/kaggle\/input\/concrete-compressive-strength-data-set\/compresive_strength_concrete.csv')\n","b9431813":"#Check the first five records \n\nconcrete_df.head()\n","a36c370c":"#Check the last few records \n\nconcrete_df.tail()","cc88e6f7":"#renaming columns\nconcrete_df = concrete_df.rename(columns={'Cement (component 1)(kg in a m^3 mixture)':\"cement\",\n       'Blast Furnace Slag (component 2)(kg in a m^3 mixture)':\"slag\",\n       'Fly Ash (component 3)(kg in a m^3 mixture)':\"ash\",\n       'Water  (component 4)(kg in a m^3 mixture)':\"water\",\n       'Superplasticizer (component 5)(kg in a m^3 mixture)':\"superplastic\",\n       'Coarse Aggregate  (component 6)(kg in a m^3 mixture)':\"coarseagg\",\n       'Fine Aggregate (component 7)(kg in a m^3 mixture)':\"fineagg\", 'Age (day)':\"age\",\n       'Concrete compressive strength(MPa, megapascals) ':\"strength\"})","f0b6a94f":"#Info of the dataset\n\nconcrete_df.info()\n","84e514a6":"# Data type of the columns \n\nconcrete_df.dtypes\n","b7adda3e":"#To get the shape \nconcrete_df.shape","28b629bd":"#To get the columns name\nconcrete_df.columns","ed5d3dac":"# Five point summary\nconcrete_df.describe().T\n","da6344e3":"print('Range of values: ', concrete_df['cement'].max()-concrete_df['cement'].min())\n","81650c8f":"#Central values \nprint('Minimum age: ', concrete_df['cement'].min())\nprint('Maximum age: ',concrete_df['cement'].max())\nprint('Mean value: ', concrete_df['cement'].mean())\nprint('Median value: ',concrete_df['cement'].median())\nprint('Standard deviation: ', concrete_df['cement'].std())","2d55959c":"#Quartiles\n\nQ1=concrete_df['cement'].quantile(q=0.25)\nQ3=concrete_df['cement'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['cement']))","531038e4":"#Outlier detection from Interquartile range (IQR) in original data\n\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in cement: ', L_outliers)\nprint('Upper outliers in cement: ', U_outliers)","87d358ac":"print('Number of outliers in cement upper : ', concrete_df[concrete_df['cement']>586.4375]['cement'].count())\nprint('Number of outliers in cement lower : ', concrete_df[concrete_df['cement']<-44.0625]['cement'].count())\nprint('% of Outlier in cement upper: ',round(concrete_df[concrete_df['cement']>586.4375]['cement'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in cement lower: ',round(concrete_df[concrete_df['cement']<-44.0625]['cement'].count()*100\/len(concrete_df)), '%')","815df2ad":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='cement',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Cement', fontsize=15)\nax1.set_title('Distribution of cement', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['cement'],ax=ax2)\nax2.set_xlabel('Cement', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Cement vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['cement'])\nax3.set_xlabel('Cement', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Cement vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","be80a861":"#Range of values \nprint('Range of values: ', concrete_df['slag'].max()-concrete_df['slag'].min())\n","44e92368":"#Central values\n\nprint('Minimum slag: ', concrete_df['slag'].min())\nprint('Maximum slag: ',concrete_df['slag'].max())\nprint('Mean value: ', concrete_df['slag'].mean())\nprint('Median value: ',concrete_df['slag'].median())\nprint('Standard deviation: ', concrete_df['slag'].std())\nprint('Null values: ',concrete_df['slag'].isnull().any())","cce7de43":"#Quartiles\n\nQ1=concrete_df['slag'].quantile(q=0.25)\nQ3=concrete_df['slag'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['slag']))","fdf122c6":"# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in slag: ', L_outliers)\nprint('Upper outliers in slag: ', U_outliers)","94af9206":"print('Number of outliers in slag upper : ', concrete_df[concrete_df['slag']>357.375]['slag'].count())\nprint('Number of outliers in slag lower : ', concrete_df[concrete_df['slag']<-214.425]['slag'].count())\nprint('% of Outlier in slag upper: ',round(concrete_df[concrete_df['slag']>357.375]['slag'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in slag lower: ',round(concrete_df[concrete_df['slag']<-214.425]['slag'].count()*100\/len(concrete_df)), '%')","8d1eb5a8":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='slag',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Slag', fontsize=15)\nax1.set_title('Distribution of slag', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['slag'],ax=ax2)\nax2.set_xlabel('Slag', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Slag vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['slag'])\nax3.set_xlabel('Slag', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Slag vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","07fd53aa":"#Range of values observed\n\nprint('Range of values: ', concrete_df['ash'].max()-concrete_df['ash'].min())\n","066cf134":"#Central values\n\nprint('Minimum ash: ', concrete_df['ash'].min())\nprint('Maximum ash: ',concrete_df['ash'].max())\nprint('Mean value: ', concrete_df['ash'].mean())\nprint('Median value: ',concrete_df['ash'].median())\nprint('Standard deviation: ', concrete_df['ash'].std())","7af93b45":"#Quartiles \n\nQ1=concrete_df['ash'].quantile(q=0.25)\nQ3=concrete_df['ash'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['ash']))","cf28b797":"#Outlier detection from Interquartile range (IQR) in original data\n\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in ash: ', L_outliers)\nprint('Upper outliers in ash: ', U_outliers)","c3589ee2":"print('Number of outliers in ash upper : ', concrete_df[concrete_df['ash']>295.75]['ash'].count())\nprint('Number of outliers in ash lower : ', concrete_df[concrete_df['ash']<-177.45]['ash'].count())\nprint('% of Outlier in ash upper: ',round(concrete_df[concrete_df['ash']>295.75]['ash'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in ash lower: ',round(concrete_df[concrete_df['ash']<-177.45]['ash'].count()*100\/len(concrete_df)), '%')","6732e865":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='ash',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Ash', fontsize=15)\nax1.set_title('Distribution of ash', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['ash'],ax=ax2)\nax2.set_xlabel('Ash', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Ash vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['ash'])\nax3.set_xlabel('Ash', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Ash vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","30b9fc55":"#Range of values observed\n\nprint('Range of values: ', concrete_df['water'].max()-concrete_df['water'].min())\n","0ad430a9":"#Central values\nprint('Minimum water: ', concrete_df['water'].min())\nprint('Maximum water: ',concrete_df['water'].max())\nprint('Mean value: ', concrete_df['water'].mean())\nprint('Median value: ',concrete_df['water'].median())\nprint('Standard deviation: ', concrete_df['water'].std())\nprint('Null values: ',concrete_df['water'].isnull().any())","103b7654":"#Quartiles\nQ1=concrete_df['water'].quantile(q=0.25)\nQ3=concrete_df['water'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['water']))","dc0ebed2":"# Outlier detection from Interquartile range (IQR) in original data\n\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in water: ', L_outliers)\nprint('Upper outliers in water: ', U_outliers)","36cdf1df":"print('Number of outliers in water upper : ', concrete_df[concrete_df['water']>232.65]['water'].count())\nprint('Number of outliers in water lower : ', concrete_df[concrete_df['water']<124.25]['water'].count())\nprint('% of Outlier in water upper: ',round(concrete_df[concrete_df['water']>232.65]['water'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in water lower: ',round(concrete_df[concrete_df['water']<124.25]['water'].count()*100\/len(concrete_df)), '%')","46c5f967":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='water',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Water', fontsize=15)\nax1.set_title('Distribution of water', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['water'],ax=ax2)\nax2.set_xlabel('Water', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Water vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['water'])\nax3.set_xlabel('Water', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Water vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","bd13dec0":"#Range of values observed\n\nprint('Range of values: ', concrete_df['superplastic'].max()-concrete_df['superplastic'].min())\n","87bdb8e5":"#Central values\n\nprint('Minimum superplastic: ', concrete_df['superplastic'].min())\nprint('Maximum superplastic: ',concrete_df['superplastic'].max())\nprint('Mean value: ', concrete_df['superplastic'].mean())\nprint('Median value: ',concrete_df['superplastic'].median())\nprint('Standard deviation: ', concrete_df['superplastic'].std())\nprint('Null values: ',concrete_df['superplastic'].isnull().any())","0355e8a0":"#Quartiles\n\nQ1=concrete_df['superplastic'].quantile(q=0.25)\nQ3=concrete_df['superplastic'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['superplastic']))","fba6dd4d":"#Outlier detection from Interquartile range (IQR) in original data\n\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in superplastic: ', L_outliers)\nprint('Upper outliers in superplastic: ', U_outliers)","661e8c5a":"print('Number of outliers in superplastic upper : ', concrete_df[concrete_df['superplastic']>25.5]['superplastic'].count())\nprint('Number of outliers in superplastic lower : ', concrete_df[concrete_df['superplastic']<-15.3]['superplastic'].count())\nprint('% of Outlier in superplastic upper: ',round(concrete_df[concrete_df['superplastic']>25.5]['superplastic'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in superplastic lower: ',round(concrete_df[concrete_df['superplastic']<-15.3]['superplastic'].count()*100\/len(concrete_df)), '%')","85fdf7e5":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='superplastic',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Superplastic', fontsize=15)\nax1.set_title('Distribution of superplastic', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['superplastic'],ax=ax2)\nax2.set_xlabel('Superplastic', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Superplastic vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['superplastic'])\nax3.set_xlabel('Superplastic', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Superplastic vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","f598fc73":"#Range of values observed\n\nprint('Range of values: ', concrete_df['coarseagg'].max()-concrete_df['coarseagg'].min())\n","9a7e8a72":"#Central values\n\nprint('Minimum value: ', concrete_df['coarseagg'].min())\nprint('Maximum value: ',concrete_df['coarseagg'].max())\nprint('Mean value: ', concrete_df['coarseagg'].mean())\nprint('Median value: ',concrete_df['coarseagg'].median())\nprint('Standard deviation: ', concrete_df['coarseagg'].std())\nprint('Null values: ',concrete_df['coarseagg'].isnull().any())","ebfba6ca":"#Quartiles\n\nQ1=concrete_df['coarseagg'].quantile(q=0.25)\nQ3=concrete_df['coarseagg'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['coarseagg']))","846fa4c8":"#Outlier detection from Interquartile range (IQR) in original data\n\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in coarseagg: ', L_outliers)\nprint('Upper outliers in coarseagg: ', U_outliers)","e8d0053e":"print('Number of outliers in coarseagg upper : ', concrete_df[concrete_df['coarseagg']>1175.5]['coarseagg'].count())\nprint('Number of outliers in coarseagg lower : ', concrete_df[concrete_df['coarseagg']<785.9]['coarseagg'].count())\nprint('% of Outlier in coarseagg upper: ',round(concrete_df[concrete_df['coarseagg']>1175.5]['coarseagg'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in coarseagg lower: ',round(concrete_df[concrete_df['coarseagg']<785.9]['coarseagg'].count()*100\/len(concrete_df)), '%')","f57acd7d":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='coarseagg',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Coarseagg', fontsize=15)\nax1.set_title('Distribution of coarseagg', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['coarseagg'],ax=ax2)\nax2.set_xlabel('Coarseagg', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Coarseagg vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['coarseagg'])\nax3.set_xlabel('Coarseagg', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Coarseagg vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","0a51db5d":"#Range of values observed\n\nprint('Range of values: ', concrete_df['fineagg'].max()-concrete_df['fineagg'].min())\n","0dc4d697":"#Central Values \n\nprint('Minimum value: ', concrete_df['fineagg'].min())\nprint('Maximum value: ',concrete_df['fineagg'].max())\nprint('Mean value: ', concrete_df['fineagg'].mean())\nprint('Median value: ',concrete_df['fineagg'].median())\nprint('Standard deviation: ', concrete_df['fineagg'].std())\nprint('Null values: ',concrete_df['fineagg'].isnull().any())","2fb26de6":"#Quartiles\n\nQ1=concrete_df['fineagg'].quantile(q=0.25)\nQ3=concrete_df['fineagg'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['fineagg']))","8a71618a":"#Outlier detection from Interquartile range (IQR) in original data\n\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in fineagg: ', L_outliers)\nprint('Upper outliers in fineagg: ', U_outliers)","4e04d309":"print('Number of outliers in fineagg upper : ', concrete_df[concrete_df['fineagg']>963.575]['fineagg'].count())\nprint('Number of outliers in fineagg lower : ', concrete_df[concrete_df['fineagg']<591.37]['fineagg'].count())\nprint('% of Outlier in fineagg upper: ',round(concrete_df[concrete_df['fineagg']>963.575]['fineagg'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in fineagg lower: ',round(concrete_df[concrete_df['fineagg']<591.37]['fineagg'].count()*100\/len(concrete_df)), '%')","b09da5c8":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='fineagg',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Fineagg', fontsize=15)\nax1.set_title('Distribution of fineagg', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['fineagg'],ax=ax2)\nax2.set_xlabel('Fineagg', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Fineagg vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['fineagg'])\nax3.set_xlabel('Fineagg', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Fineagg vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","c9bf6251":"#Range of values observed\n\nprint('Range of values: ', concrete_df['age'].max()-concrete_df['age'].min())\n","855ff0b8":"#Central values\n\nprint('Minimum age: ', concrete_df['age'].min())\nprint('Maximum age: ',concrete_df['age'].max())\nprint('Mean value: ', concrete_df['age'].mean())\nprint('Median value: ',concrete_df['age'].median())\nprint('Standard deviation: ', concrete_df['age'].std())\nprint('Null values: ',concrete_df['age'].isnull().any())","18f71905":"#Quartiles\n\nQ1=concrete_df['age'].quantile(q=0.25)\nQ3=concrete_df['age'].quantile(q=0.75)\nprint('1st Quartile (Q1) is: ', Q1)\nprint('3st Quartile (Q3) is: ', Q3)\nprint('Interquartile range (IQR) is ', stats.iqr(concrete_df['age']))","1970ef16":"#Outlier detection from Interquartile range (IQR) in original data\n# IQR=Q3-Q1\n#lower 1.5*IQR whisker i.e Q1-1.5*IQR\n#upper 1.5*IQR whisker i.e Q3+1.5*IQR\nL_outliers=Q1-1.5*(Q3-Q1)\nU_outliers=Q3+1.5*(Q3-Q1)\nprint('Lower outliers in age: ', L_outliers)\nprint('Upper outliers in age: ', U_outliers)","a6cb09bf":"print('Number of outliers in age upper : ', concrete_df[concrete_df['age']>129.5]['age'].count())\nprint('Number of outliers in age lower : ', concrete_df[concrete_df['age']<-66.5]['age'].count())\nprint('% of Outlier in age upper: ',round(concrete_df[concrete_df['age']>129.5]['age'].count()*100\/len(concrete_df)), '%')\nprint('% of Outlier in age lower: ',round(concrete_df[concrete_df['age']<-66.5]['age'].count()*100\/len(concrete_df)), '%')","4958352a":"fig, (ax1,ax2,ax3)=plt.subplots(1,3,figsize=(13,5))\n\n#boxplot\nsns.boxplot(x='age',data=concrete_df,orient='v',ax=ax1)\nax1.set_ylabel('Age', fontsize=15)\nax1.set_title('Distribution of age', fontsize=15)\nax1.tick_params(labelsize=15)\n\n#distplot\nsns.distplot(concrete_df['age'],ax=ax2)\nax2.set_xlabel('Age', fontsize=15)\nax2.set_ylabel('Strength', fontsize=15)\nax2.set_title('Age vs Strength', fontsize=15)\nax2.tick_params(labelsize=15)\n\n#histogram\nax3.hist(concrete_df['age'])\nax3.set_xlabel('Age', fontsize=15)\nax3.set_ylabel('Strength', fontsize=15)\nax3.set_title('Age vs Strength', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.5)\nplt.tight_layout()","94dc4968":"# Distplot\nfig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(concrete_df['cement'],ax=ax2[0][0])\nsns.distplot(concrete_df['slag'],ax=ax2[0][1])\nsns.distplot(concrete_df['ash'],ax=ax2[0][2])\nsns.distplot(concrete_df['water'],ax=ax2[1][0])\nsns.distplot(concrete_df['superplastic'],ax=ax2[1][1])\nsns.distplot(concrete_df['coarseagg'],ax=ax2[1][2])\nsns.distplot(concrete_df['fineagg'],ax=ax2[2][0])\nsns.distplot(concrete_df['age'],ax=ax2[2][1])\nsns.distplot(concrete_df['strength'],ax=ax2[2][2])","8d941792":"# Histogram\nconcrete_df.hist(figsize=(15,15))\n","0d5bf834":"## pairplot- plot density curve instead of histogram in diagonal\n\nsns.pairplot(concrete_df, diag_kind='kde')","12a4bba2":"# corrlation matrix \ncor=concrete_df.corr()\ncor","a971adc3":"#heatmap\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\n\nsns.heatmap(cor, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap=\"BuPu\",linecolor=\"black\")\nplt.title('Correlation between features');","f8447c87":"# water vs cement\nsns.lmplot(x=\"cement\",y=\"water\",data=concrete_df)\nplt.show()","2fac25e8":"#Check for the missing values \nconcrete_df.isnull().sum()","a5c79ac5":"#Checking for outliers\nconcrete_df1=concrete_df.copy()\nconcrete_df1.boxplot(figsize=(35,15))\n","478cc28a":"#Number of outliers present in the dataset\nprint('Number of outliers in cement: ',concrete_df1[((concrete_df1.cement - concrete_df1.cement.mean()) \/ concrete_df1.cement.std()).abs() >3]['cement'].count())\nprint('Number of outliers in slag: ',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) \/ concrete_df1.slag.std()).abs() >3]['slag'].count())\nprint('Number of outliers in ash: ',concrete_df1[((concrete_df1.ash - concrete_df1.ash.mean()) \/ concrete_df1.ash.std()).abs() >3]['ash'].count())\nprint('Number of outliers in water: ',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) \/ concrete_df1.water.std()).abs() >3]['water'].count())\nprint('Number of outliers in superplastic: ',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) \/ concrete_df1.superplastic.std()).abs() >3]['superplastic'].count())\nprint('Number of outliers in coarseagg: ',concrete_df1[((concrete_df1.coarseagg - concrete_df1.coarseagg.mean()) \/ concrete_df1.coarseagg.std()).abs() >3]['coarseagg'].count())\nprint('Number of outliers in fineagg: ',concrete_df1[((concrete_df1.fineagg - concrete_df1.fineagg.mean()) \/ concrete_df1.fineagg.std()).abs() >3]['fineagg'].count())\nprint('Number of outliers in age: ',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) \/ concrete_df1.age.std()).abs() >3]['age'].count())","22211276":"print('Records containing outliers in slag: \\n',concrete_df1[((concrete_df1.slag - concrete_df1.slag.mean()) \/ concrete_df1.slag.std()).abs() >3]['slag'])\n","ceefa410":"print('Records containing outliers in water: \\n',concrete_df1[((concrete_df1.water - concrete_df1.water.mean()) \/ concrete_df1.water.std()).abs() >3]['water'])\n","6899d6ee":"print('Records containing outliers in superplastic: \\n',concrete_df1[((concrete_df1.superplastic - concrete_df1.superplastic.mean()) \/ concrete_df1.superplastic.std()).abs() >3]['superplastic'])\n","75087f3c":"print('Records containing outliers in age: \\n',concrete_df1[((concrete_df1.age - concrete_df1.age.mean()) \/ concrete_df1.age.std()).abs() >3]['age'])\n","d57b08d9":"#Handling the outliers\n\n#Replacing the outliers by median\nfor col_name in concrete_df1.columns[:-1]:\n    q1 = concrete_df1[col_name].quantile(0.25)\n    q3 = concrete_df1[col_name].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    concrete_df1.loc[(concrete_df1[col_name] < low) | (concrete_df1[col_name] > high), col_name] = concrete_df1[col_name].median()","eaa085b3":"concrete_df1.boxplot(figsize=(35,15))\n","2944b5a0":"#Scaling the features\n\nconcrete_df_z = concrete_df1.apply(zscore)\nconcrete_df_z=pd.DataFrame(concrete_df_z,columns=concrete_df.columns)","04763f4b":"#Splitting the data into independent and dependent attributes\n\n#independent and dependent variables\nX=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]","fbf7355b":"# Split X and y into training and test set in 70:30 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)\n","127d6b16":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","079152d8":"#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))","7234099e":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","bbb27396":"from scipy.stats import pearsonr\nsns.set(style=\"darkgrid\", color_codes=True)   \nwith sns.axes_style(\"white\"):\n    sns.jointplot(x=y_test, y=y_pred, stat_func=pearsonr,kind=\"reg\", color=\"k\");","1f21944f":"#Store the accuracy results for each model in a dataframe for final comparison\nresults = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'1'})\nresults = results[['Method', 'accuracy']]\nresults","eccf3753":"num_folds = 18\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","66efc5a4":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Decision Tree k fold'], 'accuracy': [accuracy]},index={'2'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","84358463":"concrete_df_z.info()\n","8382e957":"#Create a copy of the dataset\nconcrete_df2=concrete_df_z.copy()\n","33411cc8":"#independent and dependent variable\nX = concrete_df2.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df2['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","8df74cdb":"dt_model = DecisionTreeRegressor()\ndt_model.fit(X_train , y_train)","de1534e3":"#printing the feature importance\nprint('Feature importances: \\n',pd.DataFrame(dt_model.feature_importances_,columns=['Imp'],index=X_train.columns))","2161faf5":"y_pred = dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_DT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_DT)","ae87b406":"from scipy.stats import pearsonr\nsns.set(style=\"darkgrid\", color_codes=True)   \nwith sns.axes_style(\"white\"):\n    sns.jointplot(x=y_test, y=y_pred, stat_func=pearsonr,kind=\"reg\", color=\"k\");","512db860":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Decision Tree2'], 'accuracy': [acc_DT]},index={'3'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","c1b4f4bf":"#independent and dependent variables\nX=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","065bdde7":"# Regularizing the Decision tree classifier and fitting the model\nreg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","ebaa14df":"print (pd.DataFrame(reg_dt_model.feature_importances_, columns = [\"Imp\"], index = X_train.columns))\n","c3a5ab89":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport graphviz\nimport pydot\nbank_df=concrete_df_z\nxvar = bank_df.drop('strength', axis=1)\nfeature_cols = xvar.columns","c9a2d585":"dot_data = StringIO()\nexport_graphviz(reg_dt_model, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n(graph,) = pydot.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('concrete_pruned.png')\nImage(graph.create_png())","310b422d":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","6701335a":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree'], 'accuracy': [acc_RDT]},index={'4'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","0e949318":"num_folds = 18\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(reg_dt_model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","b8817927":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree k fold'], 'accuracy': [accuracy]},index={'5'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","0e44eb46":"#Create a copy of the dataset\nconcrete_df3=concrete_df_z.copy()","24a1d107":"#independent and dependent variable\nX = concrete_df3.drop( ['strength','ash','coarseagg','fineagg'] , axis=1)\ny = concrete_df3['strength']\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","1577b7b4":"# Regularizing the Decision tree classifier and fitting the model\nreg_dt_model = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\nreg_dt_model.fit(X_train, y_train)","b1deaa5d":"y_pred = reg_dt_model.predict(X_test)\n# performance on train data\nprint('Performance on training data using DT:',reg_dt_model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using DT:',reg_dt_model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RDT=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","8aa4703d":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Pruned Decision Tree2'], 'accuracy': [acc_RDT]},index={'6'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","240ac4fb":"cluster_range = range( 1, 15 )  \ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(concrete_df1)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","f391cf00":"# Elbow plot\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","55a924ef":"# k=6\ncluster = KMeans( n_clusters = 6, random_state = 2354 )\ncluster.fit(concrete_df_z)","e21fe445":"# Creating a new column \"GROUP\" which will hold the cluster id of each record\nprediction=cluster.predict(concrete_df_z)\nconcrete_df_z[\"GROUP\"] = prediction     \n# Creating a mirror copy for later re-use instead of building repeatedly\nconcrete_df_z_copy = concrete_df_z.copy(deep = True)","4b3ba2a5":"centroids = cluster.cluster_centers_\ncentroids","288e736a":"centroid_df = pd.DataFrame(centroids, columns = list(concrete_df1) )\ncentroid_df","12b3e166":"## Instead of interpreting the neumerical values of the centroids, let us do a visual analysis by converting the \n## centroids and the data in the cluster into box plots.\nimport matplotlib.pylab as plt\nconcrete_df_z.boxplot(by = 'GROUP',  layout=(3,3), figsize=(15, 10))","c44477a5":"#independent and dependent variables\nX=concrete_df_z.iloc[:,0:8]\ny = concrete_df_z.iloc[:,8]\n# Split X and y into training and test set in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","57aaae34":"model=RandomForestRegressor()\nmodel.fit(X_train, y_train)","aef67dea":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using RFR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using RFR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_RFR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_RFR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","227a6ce2":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor'], 'accuracy': [acc_RFR]},index={'7'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","32d934e3":"num_folds = 20\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","573b6364":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Random Forest Regressor k fold'], 'accuracy': [accuracy]},index={'8'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","1b7b8e41":"model=GradientBoostingRegressor()\nmodel.fit(X_train, y_train)","420b045a":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_GBR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_GBR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","9a9790b7":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor'], 'accuracy': [acc_GBR]},index={'9'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","4470e875":"num_folds = 20\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","beb2ad27":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost Regressor k fold'], 'accuracy': [accuracy]},index={'10'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","090c8491":"model=AdaBoostRegressor()\nmodel.fit(X_train, y_train)","fb30a4f0":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_ABR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_ABR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","1a3aaaaa":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ada Boosting Regressor'], 'accuracy': [acc_ABR]},index={'11'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","1698046b":"num_folds = 18\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","f4a383d5":"tempResultsDf = pd.DataFrame({'Method':['Ada Boosting Regressor k fold'], 'accuracy': [accuracy]},index={'12'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","a278ef64":"model=BaggingRegressor()\nmodel.fit(X_train, y_train)","3c74d32e":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using GBR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using GBR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_BR=metrics.r2_score(y_test, y_pred)\nprint('Accuracy DT: ',acc_BR)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","deceadad":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Bagging Regressor'], 'accuracy': [acc_BR]},index={'13'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","b446383c":"num_folds = 20\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","e19b7347":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Bagging Regressor k fold'], 'accuracy': [accuracy]},index={'14'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","a1657cb8":"error=[]\nfor i in range(1,30):\n    knn = KNeighborsRegressor(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i!=y_test))","a8081548":"plt.figure(figsize=(12,6))\nplt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","411124e7":"#k=3\nmodel = KNeighborsRegressor(n_neighbors=3)\nmodel.fit(X_train, y_train)","8b07e835":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using KNNR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using KNNR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_K=metrics.r2_score(y_test, y_pred)\nprint('Accuracy KNNR: ',acc_K)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","2d1e20e3":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['KNN Regressor'], 'accuracy': [acc_K]},index={'15'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","0ed8e6ce":"num_folds = 30\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","7f0c11de":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['KNN Regressor k fold'], 'accuracy': [accuracy]},index={'16'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","e022a506":"model = SVR(kernel='linear')\nmodel.fit(X_train, y_train)","53c9fdef":"y_pred = model.predict(X_test)\n# performance on train data\nprint('Performance on training data using SVR:',model.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using SVR:',model.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_S=metrics.r2_score(y_test, y_pred)\nprint('Accuracy SVR: ',acc_S)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","a7250ff7":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Support Vector Regressor'], 'accuracy': [acc_S]},index={'17'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","4c797479":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(model,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","44b5c85b":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['SVR k fold'], 'accuracy': [accuracy]},index={'18'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","c99c7b0a":"#Multiple model Ensemble\nfrom sklearn import svm\nLR=LinearRegression()\nKN=KNeighborsRegressor(n_neighbors=3)\nSVM=svm.SVR(kernel='linear')","b75feb78":"\nevc=VotingRegressor(estimators=[('LR',LR),('KN',KN),('SVM',SVM)])\nevc.fit(X_train, y_train)","7fffa581":"y_pred = evc.predict(X_test)\n# performance on train data\nprint('Performance on training data using ensemble:',evc.score(X_train,y_train))\n# performance on test data\nprint('Performance on testing data using ensemble:',evc.score(X_test,y_test))\n#Evaluate the model using accuracy\nacc_E=metrics.r2_score(y_test, y_pred)\nprint('Accuracy ensemble: ',acc_E)\nprint('MSE: ',metrics.mean_squared_error(y_test, y_pred))","138148ed":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ensemble'], 'accuracy': [acc_E]},index={'19'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults\n","f9769bb6":"num_folds = 10\nseed = 77\nkfold = KFold(n_splits=num_folds, random_state=seed)\nresults1 = cross_val_score(evc,X, y, cv=kfold)\naccuracy=np.mean(abs(results1))\nprint('Average accuracy: ',accuracy)\nprint('Standard Deviation: ',results1.std())","418c3217":"#Store the accuracy results for each model in a dataframe for final comparison\ntempResultsDf = pd.DataFrame({'Method':['Ensemble k fold'], 'accuracy': [accuracy]},index={'20'})\nresults = pd.concat([results, tempResultsDf])\nresults = results[['Method', 'accuracy']]\nresults","206c1a79":"concrete_XY = X.join(y)\n","1d5bdab9":"values = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbmTree = GradientBoostingRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbmTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbmTree.score(test[:, :-1] , y_test)\n    predictions = gbmTree.predict(test[:, :-1])  \n\n    stats.append(score)","1a4d78b4":"# plot scores\n\nfrom matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","a6025d96":"values = concrete_XY.values\n# Number of bootstrap samples to create\nn_iterations = 1000        \n# size of a bootstrap sample\nn_size = int(len(concrete_df_z) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nstats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    rfTree = RandomForestRegressor(n_estimators=100)\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_test)\n    predictions = rfTree.predict(test[:, :-1])  \n\n    stats.append(score)","dc739f27":"# plot scores\n\nfrom matplotlib import pyplot\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","439eba69":"##  K fold cross validation\n","6a7d3c71":"**Visualizing the Regularized Tree**\n","82bbce3d":"## Ensemeble KNN Regressor, SVR, LR\n","df50ffe5":"Observation\n\n1. It shows that there are eight independent variables ( cement, slag, ash, water, superplastic, coarseagg, fineagg, age) and one dependent variable (strength).\n2. All the records are numeric.","2e31da21":"# Fineagg","0fcfb0f6":"1. Here, None of the dimensions are good predictor of target variable.\n2. For all the dimensions (variables) every cluster have a similar range of values except in one case.\n3. We can see that the body of the cluster are overlapping.\n4. So in k means, though, there are clusters in datasets on different dimensions. But we can not see any distinct characteristics of these clusters which tell us to break data into different clusters and build separate models for them.","9e79094c":"## K fold cross validation\n","78f94b45":"1. After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold, Bagging Regressor are giving better results as compared to other models.\n2. Now as the dataset have different gaussians, we can apply k means clustering and then we can apply the models and compare the accuracy.","8f46eeca":"# 4. Feature Engineering, Model Building and Model Tuning\n","1ba9a0d3":"## Using Random Forest Regressor\n","447b5db7":"It gives the column names of the dataset.\n","c6d54ad0":"## Age","5965b631":"It gives the details about the number of rows (1030), number of columns (9), data types information i.e. except age which is integer type all other columns are float type. Memory usage is 72.5 KB. Also,there are no null values in the data.\n","18611723":"## Superplastic","290e9526":"This model is also overfit.\n","2a4cb6b7":"## K fold cross validation\n","9cf79354":"## K Means Clustering\n","69a4534c":"### Iteration 2\n","c85ee3e8":"## Support Vector Regressor\n","b224b163":"1. Here, we have used Standard deviation method to detect the outliers.If we have any data point that is more than 3 times the standard deviation, then those points are very likely to be outliers.\n2. We can see that slag, water, superplastic and age contain outliers.","7faac48f":"Here, we can see that ash,coarseagg and fineagg are least significant variable.\n","33acd9a7":"The reason why we are doing all this analysis is if we find any kind of dimensions which are very strongly correlated i.e. r value close to 1 or -1 such dimensions are giving same information to your algorithms, its a redundant dimension. So in such cases we may want to keep one and drop the other which we should keep and which we should drop depends on again your domain expertise, which one of the dimension is more prone to errors.I would like to drop that dimension. Or we have a choice to combine these dimensions and create a composite dimension out of it.","76d357e3":"**Drop the least significant variable**\n","0346f4fa":"## K fold cross validation\n","e3a61c07":"## K fold cross validation\n","7c5d4926":"## K fold cross validation\n","2cd24ecf":"# DecisionTree Regression\n","bbcbb8ba":"## Bootstrap Sampling\n","5fe69f26":"1. It is also giving the same information we observed in pairplot analysis.\n2. water shows significant negative relationship with superplastic and fineagg. It also shows some kind of positive relationship with slag and age.","79854a28":"## Bagging Regressor\n","9cc47048":"## Using Gradient Boosting Regressor\n","c10215cb":"## Random Forest","29a07456":"## Slag","68f2947d":"## Iteration2\n","53087c5e":"# 1. Import Libraries ","b5d673d4":"**The acuracy on testing dataset is not improved, still it is an overfit model.**\n","de5e0a12":"The bootstrap random forest classification model performance is between 84%-90.8% which is better than other classification algorithms.","6b42f30e":"**There is a overfitting in the model as the dataset is performing 99% accurately in trainnig data. However, the accuracy on test data drops.**\n","f22dd4b3":"# 3. Exploratory Data Analysis","299f3f1a":"### Cement","a44e5624":"Here, all the attributes in the same scale(unit) except the age attribute. Hence, we are scaling the attributes. We are using zscore for scaling.\n","28401a0f":"## 3.2 MultiVariate Analysis","11567e0f":"## K fold cross validation\n","a766fe4d":"## KNN Regressor\n","2eb8e133":"Here, we can see the correlation value between the attributes.\n","1fe147ed":"1. It gives the descriptive statistics (mean, median, mode, percentiles, min, max, standard deviation) and count of the columns of the dataset.\n2. We can see that cement,slag,ash are left skewed","abf100ea":"It gives the data types of each column of the dataset.\n","376bf278":"## Gradient Boosting Regressor\n","4976d6b3":"1. So, cement, age and water are significant attributes.\n2. Here, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column. This we have seen in pairplot also.","f3ddf2e2":"It gives the details of the number of rows and columns present in the dataset.There are 1030 rows and 9 columns.\n","55aeeda0":"## 3.1 Univariate Analysis  ","7a84d1b7":"## 3.3 Handling missing values and Outliers","aee6d39c":"## Ada Boosting Regressor\n","4b1a0e17":"## Ash","93670409":"strength attribute : Relationship between dependent and independent attributes\n\nstrength: Now its comparing the target column with all other independent attributes and its showing us very vital information.\n\nstrength vs cement: It is linearly related to the cement. The relationship is positive and we can see that for a given value of cement we have a multiple values of strength. Which one should we pick we don't know. Hence Cement though it has poditive relationship with the strength, it is not a very good predictor. It is a weak predictor.\nstrength vs slag: There is no particular trend.\nstrength vs slag: There is also no particular trend.\nstrength vs age: For a given value of age, we have different values of strength. Hence, It is not a good predictor.\nstrength vs superplastic:For a given value of age, we have different values of strength. Hence, It is not a good predictor.\nOther attributes does not give any strong relationship with strength.\nHence, we can see that none of the independent attributes are a good predictors of the strength attribute. There is a no linear relationship between them.\n\nSo, we will not use Linear model","77c41199":"### Description of indepedant variables","f0a02752":"It also shows that slag, ash, water superplastic, and age contains outliers.\n","8c6508e7":"**Off Diagonal Analysis: Relationship between indpendent attributes**\n\nScatter plots\n\n1. cement vs other independent attributes: This attribute does not have any significant relation with slag, ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n2. slag vs other independent attributes: This attribute also does not have any significant relation with ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n3. ash vs other independent attributes: This attribute also does not have any significant relation with water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n4. water vs other independent attributes: This attribute have negative linear relationship with superplastic and fineagg. It does not have any significant relationship with other independent atributes. This is true as Superplasticizers allows the reduction of water in the concrete upto the extent of 30% without reducing the workability.\n5. superplastic vs other independent attributes:This attribute have negative linear relationship with water only. It does not have any significant relationship with other independent attributes.\n6. coarseagg vs other independent attributes:This attribute also does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n7. fineagg vs other independent attributes:It has negative linear relationship with water. It does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.\n","c6a4f05a":"**Diagonals Analysis**\nThe diagonal gives the same information, we got using distplot.\n\n1. cement attribute have almost normal curve.\n2. slag has two gausssians and rightly skewed.It shows the presence of outlies.\n3. ash has two gaussians and rightly skewed.It shows the presence of outlies.\n4. water has atleast guassians and slighly left skewed.It shows the presence of outlies.\n5. superplastic has multiple gaussians and rightly skewed.It shows the presence of outlies.\n6. coarseagg has three guassians and almost normal.\n7. fineagg has almost two guassians and looks like normal.\n8. age has multiple guassians and rightly skewed. It shows the presence of outlies.\n9. strength is close to a normal curve.\n10. We not only have missing values problem but also outliers problem in the dataset.","ef565266":"**Observation**\n\nWe can see observe that :\n\n1. cement is almost normal.\n2. slag has three gausssians and rightly skewed.\n3. ash has two gaussians and rightly skewed.\n4. water has three guassians and slighly left skewed.\n5. superplastic has two gaussians and rightly skewed.\n6. coarseagg has three guassians and almost normal.\n7. fineagg has almost two guassians and looks like normal.\n8. age has multiple guassians and rightly skewed.","db8acba4":"## K fold cross validation\n","8e4f0c21":"## Coarseagg","c73e9c60":"## Water","31541148":"## K fold cross validation\n","295a56e0":"## Regularising\/Pruning of Decision Tree\n","954d886f":"# 2. Meet and Greet the Data"}}