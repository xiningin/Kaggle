{"cell_type":{"862205d5":"code","797d3d4e":"code","389e59a7":"code","b12075a5":"code","91b3bcaa":"code","e25f4f06":"code","ca7195f0":"code","1eb65056":"code","f186e787":"code","aaa6c3ed":"code","fe1befbd":"code","aa1dbc99":"code","1a089bfd":"code","6d93a282":"code","7718b67c":"code","e503cd83":"code","8618d7a5":"code","aa2b73e9":"code","ab39e119":"code","51e02702":"code","3948784b":"code","3ef1c32f":"code","24c942c7":"code","817be9ac":"markdown","520b8f30":"markdown","3a22dab8":"markdown","6215a8e3":"markdown","cb66cf99":"markdown","2f960831":"markdown","35e56ff0":"markdown","8468d60b":"markdown","d9ef2658":"markdown","61f50678":"markdown","ef4e34a1":"markdown","e3831496":"markdown","0442d1ec":"markdown","a2afabc2":"markdown","7c7ffc74":"markdown"},"source":{"862205d5":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt    \nimport seaborn as sns\nimport os\nfrom scipy.stats import norm\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n\n%matplotlib inline\npd.set_option('display.max_columns', None)   # for showing all columns of dataset\n#pd.set_option('display.max_rows', None)     # for showing all rows of dataset","797d3d4e":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")","389e59a7":"# shape of dataset(rows,columns)\nprint(train.shape)              \nprint(test.shape)","b12075a5":"# Lets check our train dataset\ntrain.head()     ","91b3bcaa":"train.info()    # Information about train dataset","e25f4f06":"# id column is not useful so we drop it\ntrain.drop('id',axis=1,inplace=True)   \ntest.drop('id',axis=1,inplace=True)","ca7195f0":"# describe the Statistics of dataset \ntrain.describe().T.style.bar(subset=['mean'])\\\n                            .background_gradient(subset=['std'])\\\n                            .background_gradient(subset=['50%'])\\\n                            .background_gradient(subset=['max'])","1eb65056":"test.describe().T.style.bar(subset=['mean'])\\\n                            .background_gradient(subset=['std'])\\\n                            .background_gradient(subset=['50%'])\\\n                            .background_gradient(subset=['max'])","f186e787":"# compare between train and test dataset\n\ndef diff_color(x):\n    color = 'red' if x<0 else ('green' if x > 0 else 'black')\n    return f'color: {color}'\n\n(train.describe() - test.describe())[test.columns].T.iloc[:,1:].style\\\n        .bar(subset=['mean', 'std'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n        .applymap(diff_color, subset=['min', 'max'])","aaa6c3ed":"# Null values\ntrain.isnull().sum()","fe1befbd":"# check target variable\nsns.countplot(train['target'])\ntrain.target.value_counts()","aa1dbc99":"# Distribution of features in respect of target variable\nfeature_columns = train.columns.drop('target')\nnum_rows, num_cols = 10,5\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(20, 30))\n\n\nfor index, column in enumerate(feature_columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n\n    sns.kdeplot(train.loc[train['target'] == 'Class_1', column], shade=True, ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_2', column], shade=True, ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_3', column], shade=True, ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_4', column], shade=True, ax=axes[i,j])\n\n\nplt.tight_layout()\nplt.show()","1a089bfd":"fig, ax = plt.subplots(figsize=(10 , 10))\n\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\nsns.heatmap(train.corr(),\n        square=True, center=0, linewidth=0.2,\n        cmap='Reds',\n        mask=mask, ax=ax) \n\nax.set_title('Feature Correlation', loc='center', fontweight='bold')\nplt.show()","6d93a282":"# Label Encoder\nfrom sklearn.preprocessing import LabelEncoder\nle =LabelEncoder()\nle.fit(train['target'])\ntrain['target'] = le.transform(train['target'])\n","7718b67c":"X = train.drop('target',axis=1)\ny = train['target']","e503cd83":"X","8618d7a5":"folds = 5\nSEED = 24\nbasic_model =[]","aa2b73e9":"y_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_xgb = np.zeros((test.shape[0], 4))\nsf = StratifiedKFold(n_splits = folds, shuffle=True, random_state=SEED)\nfor fold,(train_idx,val_idx) in enumerate(sf.split(X,y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    xgb =XGBClassifier(objective ='multi:softprob',random_state=SEED)\n    xgb.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 50)\n    y_val_pred = xgb.predict_proba(X_val)\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_xgb += xgb.predict_proba(test)\n\n\ny_test_pred_xgb = y_test_pred_xgb \/ folds\n\nprint(f\"Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\n\nbasic_model.append({'model': 'xgboost', 'logloss': log_loss(y, y_oof_pred)})\n    \n    ","ab39e119":"y_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_catb = np.zeros((test.shape[0], 4))\n\nkf = StratifiedKFold(n_splits = folds, shuffle=True, random_state=SEED)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx] \n        \n    catb = CatBoostClassifier(random_state=SEED)\n    catb.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 200)\n\n    y_val_pred = catb.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_catb += catb.predict_proba(test)\n\n\ny_test_pred_catb = y_test_pred_catb \/ folds\n\nprint(f\"Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\nbasic_model.append({'model': 'catboost', 'logloss': log_loss(y, y_oof_pred)})","51e02702":"y_oof_pred = np.zeros((train.shape[0], 4))\ny_test_pred_lgbm = np.zeros((test.shape[0], 4))\n\nkf = StratifiedKFold(n_splits = folds, shuffle= True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    \n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n    lgbm = LGBMClassifier(random_state=SEED)\n\n    lgbm.fit(X_train, y_train,\n                 eval_set = [(X_train, y_train),(X_val, y_val)],\n                 verbose = 200, early_stopping_rounds=150)\n\n    y_val_pred = lgbm.predict_proba(X_val)\n\n    print(f\"Fold {fold + 1} Logloss: {log_loss(y_val, y_val_pred)}\")\n\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred_lgbm += lgbm.predict_proba(test)\n\n\ny_test_pred_lgbm= y_test_pred_lgbm\/ folds\n\nprint(f\"-- Overall OOF Logloss: {log_loss(y, y_oof_pred)}\")\nbasic_model.append({'model': 'lgbm', 'logloss': log_loss(y, y_oof_pred)})","3948784b":"df = pd.DataFrame(basic_model, index=None)\ndf","3ef1c32f":"\nsns.catplot(y=\"model\", x=\"logloss\", data=df,kind='violin')\nplt.show()","24c942c7":"sample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n\n\nsubmission = pd.DataFrame(y_test_pred_catb)\nsubmission.columns = ['Class_1', 'Class_2','Class_3','Class_4']\n\n\n\n\nsubmission['id'] = sample_submission['id']\nsubm = submission[['id','Class_1', 'Class_2','Class_3','Class_4']]\n\nsubm.to_csv(\"submission.csv\", index=True)","817be9ac":"# Models\n- XGBOOST\n- Catboost\n- LGBM\n","520b8f30":"As we can see the data is not internally correlated thus all variables can be used in features selecton. Internal correlation may leads to strong correlation and covariation signals making other relations underated","3a22dab8":"**Correlation matrix**","6215a8e3":"**LGBM**","cb66cf99":"As plot shows us CatBoost is better than lgbm and xgboost","2f960831":"- **Count** : Number of rows in data set.\n- **mean**  : mean is the average value of particular feature.\n- **std.**  : std stands for Standard Deviation.It measures the spread of a data distribution. The more spread out a data distribution is, the greater its standard deviation.\n- **min**   : Minimum value of Feature.\n- **25%**   : It shows the 25% value of that feature.\n- **50%**   : It shows the 50% value of that feature.\n- **75%**   : It shows the 75% value of that feature.\n- **max**   : Maximum value of that feature\n\nSome of you think why we need them.Above all gives us the basic statistical information which will be very helpful in our EDA.\n","35e56ff0":"As we can see there are 51 columns and 100000 rows in train dataset and most of them having \"int64\" datatype only target variable's data type is 'object' .\n\n\n","8468d60b":"As we can see there no null value\n\n\n\n\n**Lets See Distribution of Target.**","d9ef2658":"# EDA","61f50678":"**XGBOOST**","ef4e34a1":"# Load Dataset","e3831496":"**CatBoost**","0442d1ec":"Future vision Update for this notebook: \n1. Hyperparameter optimization for the above models\n2. Use other ML Algorithms also\n3. create ANN and then hypertune the Ann model","a2afabc2":"# Feature Engineering","7c7ffc74":"# Load Packages"}}