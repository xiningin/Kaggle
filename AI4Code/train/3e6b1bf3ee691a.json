{"cell_type":{"bf1f03f2":"code","4ddbe5b8":"code","5722283c":"code","cd7ff706":"code","730f9db2":"code","adf2ec8f":"code","fbb272e6":"code","3d5d7f99":"code","b39d17e4":"code","d2c0c881":"code","b457891c":"code","36710927":"code","d63b7478":"code","79dd9087":"code","26ff5473":"code","247c4845":"code","fc0ecdf1":"code","7315284b":"code","b6c311ef":"code","7ba12789":"code","399c63a2":"code","09124e48":"markdown","4fbc109c":"markdown","54234e40":"markdown","7d17eca8":"markdown","d344e0d0":"markdown","feaf6ce0":"markdown","e8c81c0e":"markdown","dcbc12c1":"markdown","8c2d5be9":"markdown","feeecf9f":"markdown","3289acd2":"markdown","6b5caa7d":"markdown","8fd82780":"markdown","d136975c":"markdown","d00292f7":"markdown","ec41c1fa":"markdown","a700bfc0":"markdown","9a106abe":"markdown","224834b9":"markdown","d5d742c3":"markdown","8c3390f4":"markdown"},"source":{"bf1f03f2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\njfk2=pd.read_csv(\"..\/input\/flight-take-off-data-jfk-airport\/M1_final.csv\")\njfk1=jfk2.iloc[:,:].values\njfk=pd.DataFrame(jfk1)\nerr_lab=[]\nerr_ohe=[]","4ddbe5b8":"#label encoding the data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlab_enc=LabelEncoder()\none_hot=OneHotEncoder()\njfk.iloc[:,3]= lab_enc.fit_transform(jfk.iloc[:,3])\njfk.iloc[:,4]= lab_enc.fit_transform(jfk.iloc[:,4])\njfk.iloc[:,5]= lab_enc.fit_transform(jfk.iloc[:,5])\njfk.iloc[:,15]= lab_enc.fit_transform(jfk.iloc[:,15])\njfk.iloc[:,19]= lab_enc.fit_transform(jfk.iloc[:,19])\njfk = jfk.astype('int')\njfk.head(10)","5722283c":"#splitting the dataset in the ration of 90:10\nfrom sklearn.model_selection import train_test_split\nX=jfk[[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]]\ny=jfk[[22]]\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1, random_state=20)","cd7ff706":"import math\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nlr0 = LinearRegression()\nlr0.fit(X_train, y_train)\nx_lr0=lr0.predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,x_lr0))\nerr_lab.append(temp)","730f9db2":"from sklearn.linear_model import Ridge\nlr1 = Ridge(alpha=0.5, normalize = True)\nlr1.fit(X_train, y_train)\nx_lr1=lr1.predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,x_lr1))\nerr_lab.append(temp)","adf2ec8f":"from sklearn.linear_model import Lasso\nlr2 = Lasso(alpha=0.1)\nlr2.fit(X_train, y_train)\nx_lr2=lr2.predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,x_lr2))\nerr_lab.append(temp)","fbb272e6":"from sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=10)\nknn_pred= knn.fit(X_train,y_train.values.ravel()).predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,knn_pred))\nerr_lab.append(temp)","3d5d7f99":"from sklearn.svm import SVR\nreg= SVR(kernel='rbf')\nreg.fit(X_train,y_train.values.ravel())\nsvr_pred= reg.predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,svr_pred))\nerr_lab.append(temp)","b39d17e4":"from sklearn.naive_bayes import GaussianNB\ngnb= GaussianNB()\ngnb_pred= gnb.fit(X_train,y_train.values.ravel()).predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,gnb_pred))\nerr_lab.append(temp)","d2c0c881":"from sklearn.ensemble import RandomForestClassifier\nrfc= RandomForestClassifier()\nrfc_pred= rfc.fit(X_train,y_train.values.ravel()).predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,rfc_pred))\nerr_lab.append(temp)","b457891c":"import lightgbm as lgb\nlgb_train= lgb.Dataset(X_train, label = y_train)\nlgb_eval= lgb.Dataset(X_train,y_train, reference= lgb_train)\nparams = {'boosting_type': 'gbdt', 'num_leaves': 60, 'learning_rate': 0.2}\ngbm_pred = lgb.train(params, lgb_train, 100).predict(X_test)\ntemp= math.sqrt(mean_squared_error(y_test,gbm_pred))\nerr_lab.append(temp)","36710927":"#hot encoding whole datasets\nfrom sklearn.preprocessing import OneHotEncoder\nohe=OneHotEncoder()\nX_ohe = pd.get_dummies(X, columns=[3, 5, 13, 15, 19])\n\nX1_train, X1_test, y1_train, y1_test = train_test_split(X_ohe,y, test_size = 0.1, random_state =10)","d63b7478":"lr0.fit(X1_train, y1_train)\nx1_lr0=lr0.predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,x1_lr0))\nerr_ohe.append(temp)","79dd9087":"lr1.fit(X1_train, y1_train)\nx1_lr1=lr1.predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,x1_lr1))\nerr_ohe.append(temp)","26ff5473":"lr2 = Lasso(alpha=0.1)\nlr2.fit(X1_train, y1_train)\nx1_lr2=lr2.predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,x1_lr2))\nerr_ohe.append(temp)","247c4845":"knn= KNeighborsClassifier(n_neighbors=10)\nknn_pred1= knn.fit(X1_train,y1_train.values.ravel()).predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,knn_pred1))\nerr_ohe.append(temp)","fc0ecdf1":"reg= SVR()\nreg.fit(X1_train,y1_train.values.ravel())\nsvr_pred1= reg.predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,svr_pred1))\nerr_ohe.append(temp)","7315284b":"gnb= GaussianNB()\ngnb_pred= gnb.fit(X1_train,y1_train.values.ravel()).predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,gnb_pred))\nerr_ohe.append(temp)","b6c311ef":"rfc= RandomForestClassifier()\nrfc_pred1= rfc.fit(X1_train,y1_train.values.ravel()).predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,rfc_pred1))\nerr_ohe.append(temp)","7ba12789":"lgb_train= lgb.Dataset(X1_train, label = y1_train)\nlgb_eval= lgb.Dataset(X1_train,y1_train, reference= lgb_train)\nparams = {'boosting_type': 'gbdt', 'num_leaves': 60, 'learning_rate': 0.2}\ngbm_pred1 = lgb.train(params, lgb_train, 100).predict(X1_test)\ntemp= math.sqrt(mean_squared_error(y1_test,gbm_pred1))\nerr_ohe.append(temp)","399c63a2":"labels=['Linear Regression','Ridge Regression','Lasso Regression','KNN','SVR','Naive Bayes','Random Forest','LightGBM']\ndf = pd.DataFrame({'Label encoding':err_lab,'One Hot encoding':err_ohe}, index=labels)\nax = df.plot.bar(rot=0,figsize=(20,10))","09124e48":"<img src= \"https:\/\/images.unsplash.com\/photo-1586710025696-b142c39e7355?ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8amZrJTIwYWlycG9ydHxlbnwwfHwwfHw%3D&ixlib=rb-1.2.1&auto=format&fit=crop&w=800&q=60\" alt =\"jfk\" style='width: 2000px;'>\n\n# Purpose of this project:\nTo test several models to predict Taxi_out time for JFK Airport.\n\nTaxi_out is the time aircrafts spend on the surface. The period of time when the aircraft leave the tereminal and actually takes off.\n\nReducing the taxi_out time will help in cutting the consumption of fuel.","4fbc109c":"2.7 Random Forest Classifier","54234e40":"2.5 Support Vector Regression","7d17eca8":"1.5 Support Vector Regression","d344e0d0":"1.4 KNeighbors Classifier","feaf6ce0":"# Compairing models on the basis of RMSE","e8c81c0e":"2.1 Linear Regression","dcbc12c1":"2.6 Gaussian Naive Bayes","8c2d5be9":"1.3 Lasso Regression","feeecf9f":"# Conclusions:\n1. Label encoding is better in predicting taxi_out time in almost all algorithms.\n2. One hot encoding produces exceptinally big RMSE in Naive Bayes algorithm.\n3. On the basis of RMSE we can say that LightGBM model is best model in both Label encoded as well as One Hot Encoded data.","3289acd2":"1.2 Ridge Regression","6b5caa7d":"2.8 Light GBM","8fd82780":"2.4 KNeighbors Classifier","d136975c":"# 1. Label encoding","d00292f7":"1.1 Linear Regression","ec41c1fa":"# 2. One Hot Encoding","a700bfc0":"2.2 Ridge Regression","9a106abe":"1.6 Gaussian Naive Bayes","224834b9":"1.7 Random Forest Classifier","d5d742c3":"2.3 Lasso Regression","8c3390f4":"1.8 Light GBM"}}