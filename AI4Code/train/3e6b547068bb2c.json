{"cell_type":{"93494451":"code","b6f8a761":"code","69c0ec28":"code","94685361":"code","1d7c31a0":"code","8d2c2e2b":"code","e43af162":"code","729d8172":"code","20788cec":"code","f2f62ecd":"code","b342ac0b":"code","8f129ba6":"code","9ce3bcac":"code","5dcf740f":"code","1bb5704d":"code","dd4b4e0b":"code","5d1d7d0c":"code","fe224d0b":"markdown","d77307ec":"markdown","980f0a47":"markdown","9f95143d":"markdown","5fcc1bd6":"markdown","eba7a557":"markdown","ac597ad9":"markdown","06a3d1da":"markdown","2eab9dd9":"markdown"},"source":{"93494451":"from __future__ import absolute_import, division, print_function, unicode_literals # legacy compatibility\n\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\nimport tensorflow_datasets as tfds\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","b6f8a761":"# helper functions\n\n# select from from_list elements with index in index_list\ndef select_from_list(from_list, index_list):\n  filtered_list= [from_list[i] for i in index_list]\n  return(filtered_list)\n\n# append in filtered_list the index of each element of unfilterd_list if it exists in in target_list\ndef get_ds_index(unfiliterd_list, target_list):\n  index = 0\n  filtered_list=[]\n  for i_ in unfiliterd_list:\n    if i_[0] in target_list:\n      filtered_list.append(index)\n    index += 1\n  return(filtered_list)\n\n# select a url for a unique subset of CIFAR-100 with 20, 40, 60, or 80 classes\ndef select_classes_number(classes_number = 20):\n  cifar100_20_classes_url = \"https:\/\/pastebin.com\/raw\/nzE1n98V\"\n  cifar100_40_classes_url = \"https:\/\/pastebin.com\/raw\/zGX4mCNP\"\n  cifar100_60_classes_url = \"https:\/\/pastebin.com\/raw\/nsDTd3Qn\"\n  cifar100_80_classes_url = \"https:\/\/pastebin.com\/raw\/SNbXz700\"\n  if classes_number == 20:\n    return cifar100_20_classes_url\n  elif classes_number == 40:\n    return cifar100_40_classes_url\n  elif classes_number == 60:\n    return cifar100_60_classes_url\n  elif classes_number == 80:\n    return cifar100_80_classes_url\n  else:\n    return -1\n","69c0ec28":"# load the entire dataset\n(x_train_all, y_train_all), (x_test_all, y_test_all) = tf.keras.datasets.cifar100.load_data(label_mode='fine')","94685361":"print(x_train_all.shape)","1d7c31a0":"# REPLACE WITH YOUR TEAM NUMBER\nteam_seed = 27\n\n# select the number of classes\ncifar100_classes_url = select_classes_number(80)\n\nteam_classes = pd.read_csv(cifar100_classes_url, sep=',', header=None)\nCIFAR100_LABELS_LIST = pd.read_csv('https:\/\/pastebin.com\/raw\/qgDaNggt', sep=',', header=None).astype(str).values.tolist()[0]\n\nour_index = team_classes.iloc[team_seed,:].values.tolist()\nour_classes = select_from_list(CIFAR100_LABELS_LIST, our_index)\ntrain_index = get_ds_index(y_train_all, our_index)\ntest_index = get_ds_index(y_test_all, our_index)\n\nx_train_ds = np.asarray(select_from_list(x_train_all, train_index))\ny_train_ds = np.asarray(select_from_list(y_train_all, train_index))\nx_test_ds = np.asarray(select_from_list(x_test_all, test_index))\ny_test_ds = np.asarray(select_from_list(y_test_all, test_index))\n\n# print our classes\nprint(our_classes)\n\nCLASSES_NUM=len(our_classes)\n\nprint(x_train_ds[1].shape)\n\n# get (train) dataset dimensions\ndata_size, img_rows, img_cols, img_channels = x_train_ds.shape\n\n# set validation set percentage (wrt the training set size)\nvalidation_percentage = 0.15\nval_size = round(validation_percentage * data_size)\n\n# Reserve val_size samples for validation and normalize all values\nx_val = x_train_ds[-val_size:]\/255\ny_val = y_train_ds[-val_size:]\nx_train = x_train_ds[:-val_size]\/255\ny_train = y_train_ds[:-val_size]\nx_test = x_test_ds\/255\ny_test = y_test_ds\n\nprint(len(x_val))\n\n# summarize loaded dataset\nprint('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\nprint('Validation: X=%s, y=%s' % (x_val.shape, y_val.shape))\nprint('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))\n\n# get class label from class index\ndef class_label_from_index(fine_category):\n  return(CIFAR100_LABELS_LIST[fine_category.item(0)])\n\n# plot first few images\nplt.figure(figsize=(6, 6))\nfor i in range(9):\n\t# define subplot\n  plt.subplot(330 + 1 + i).set_title(class_label_from_index(y_train[i]))\n\t# plot raw pixel data\n  plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n  #show the figure\nplt.show()","8d2c2e2b":"def save_tfrecords(x_train, y_train, record_file):\n    \"\"\"\n    Saving x_train to proper tfrecord file with name record_file\n    \"\"\"    \n    n_samples = x_train.shape[0]\n    dimension = x_train.shape[1]\n    with tf.io.TFRecordWriter(record_file) as writer:\n       for i in range(n_samples):\n          image = x_train[i]\n          label = y_train[i]\n          tf_example = image_example(image, label, dimension)\n          writer.write(tf_example.SerializeToString())\n\n    ","e43af162":"BATCH_SIZE = 128\nAUTOTUNE = tf.data.experimental.AUTOTUNE # https:\/\/www.tensorflow.org\/guide\/data_performance\n\n# Track the data type\ndataType = x_train.dtype\n\n# Decoding function\ndef decode_record(record, labeled=True):\n    image = tf.io.decode_raw(\n        record['image_raw'], out_type=dataType, little_endian=True, fixed_length=None, name=None\n    )\n    label = record['label']\n    dimension = 32\n    image = tf.reshape(image, (dimension, dimension, 3))\n    if (labeled) : \n        return (image, label)\n    return image\n\ndef parse_record(record):\n    name_to_features = {\n        'dimension': tf.io.FixedLenFeature([], tf.int64),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n        'image_raw': tf.io.FixedLenFeature([], tf.string),\n    }\n    return tf.io.parse_single_example(record, name_to_features)\n\n#Our dataset is not ordered in any meaningful way, so the order can be ignored when loading our dataset. \n#By ignoring the order and reading files as soon as they come in, it will take a shorter time to load the data.\ndef load_dataset(filenames, labeled=True):\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False  # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames)  # automatically interleaves reads from multiple files\n    dataset = dataset.with_options( ignore_order )  # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(parse_record )\n    dataset = dataset.map(decode_record )\n    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n    return dataset\n\n#We define the following function to get our different datasets.\ndef get_dataset(filenames, labeled=True, BATCH_SIZE=BATCH_SIZE):\n    dataset = load_dataset(filenames, labeled=labeled)\n    dataset= dataset.repeat()\n    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n    dataset = dataset.batch(BATCH_SIZE)\n    return dataset\n\n# Convert values to compatible tf.Example types.\ndef _bytes_feature(value):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy()  # BytesList won't unpack a string from an EagerTensor.\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\ndef _float_feature(value):\n    \"\"\"Returns a float_list from a float \/ double.\"\"\"\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\ndef _int64_feature(value):\n    \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n# Create the features dictionary.\ndef image_example(image, label, dimension):\n    feature = {\n        'dimension': _int64_feature(dimension),\n        'label': _int64_feature(label),\n        'image_raw': _bytes_feature(image.tobytes()),\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n","729d8172":"# we user prefetch https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#prefetch \n# see also AUTOTUNE\n# the dataset is now \"infinite\"\n\ndef _input_fn(x,y, BATCH_SIZE):\n  ds = tf.data.Dataset.from_tensor_slices((x,y))\n  ds = ds.shuffle(buffer_size=data_size)\n  ds = ds.repeat()\n  ds = ds.batch(BATCH_SIZE)\n  ds = ds.prefetch(buffer_size=AUTOTUNE)\n  return ds\n\ntrain_ds =_input_fn(x_train,y_train, BATCH_SIZE) #PrefetchDataset object\nvalidation_ds =_input_fn(x_val,y_val, BATCH_SIZE) #PrefetchDataset object\ntest_ds =_input_fn(x_test,y_test, BATCH_SIZE) #PrefetchDataset object\n\n# steps_per_epoch and validation_steps for training and validation: https:\/\/www.tensorflow.org\/guide\/keras\/train_and_evaluate\n\ndef train_model(model,callbacks=[], epochs = 10, steps_per_epoch = 2, validation_steps = 1):\n\n  history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_ds, validation_steps=validation_steps, callbacks=callbacks)\n  return(history)","20788cec":"# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n\tplt.figure(figsize=(8, 8))\n\tplt.suptitle('Training Curves')\n\t# plot loss\n\tplt.subplot(211)\n\tplt.title('Cross Entropy Loss')\n\tplt.plot(history.history['loss'], color='blue', label='train')\n\tplt.plot(history.history['val_loss'], color='orange', label='val')\n\tplt.legend(loc='upper right')\n\t# plot accuracy\n\tplt.subplot(212)\n\tplt.title('Classification Accuracy')\n\tplt.plot(history.history['accuracy'], color='blue', label='train')\n\tplt.plot(history.history['val_accuracy'], color='orange', label='val')\n\tplt.legend(loc='lower right')\n\treturn plt\n \n# print test set evaluation metrics\ndef model_evaluation(model, evaluation_steps):\n\tprint('\\nTest set evaluation metrics')\n\tloss0,accuracy0 = model.evaluate(test_ds, steps = evaluation_steps)\n\tprint(\"loss: {:.2f}\".format(loss0))\n\tprint(\"accuracy: {:.2f}\".format(accuracy0))\n\ndef model_report(model, history, evaluation_steps = 10):\n\tplt = summarize_diagnostics(history)\n\tplt.show()\n\tmodel_evaluation(model, evaluation_steps)\n    \n    \nfrom datetime import datetime\nfrom packaging import version\nimport time\n%load_ext memory_profiler\n\ndef run (OPTIMAL_MODEL,callback_list,best_filepath):\n  steps_per_epoch=50\n  start= time.time()\n  OPTIMAL_MODEL_history = train_model(OPTIMAL_MODEL,callback_list, 400, 20, 4)\n  end=time.time()\n  total_training_time=end-start\n  \n  OPTIMAL_MODEL.load_weights(best_filepath) \n  _, accuracy_in_test =OPTIMAL_MODEL.evaluate(test_ds,steps = steps_per_epoch)\n  _, accuracy_in_train =OPTIMAL_MODEL.evaluate(train_ds,steps = steps_per_epoch)\n  _, accuracy_in_val =OPTIMAL_MODEL.evaluate(validation_ds,steps = steps_per_epoch)\n  model_report(OPTIMAL_MODEL,OPTIMAL_MODEL_history,5)\n  print(OPTIMAL_MODEL.summary())\n  \n  print(f\"\"\"\n  Total time of training {total_training_time}\n  Accuracy in Test: {accuracy_in_test}  (sec)\n  Accuracy in Train: {accuracy_in_train}  (sec)\n  Accuracy in Validation: {accuracy_in_val}  (sec)\n  \"\"\")","f2f62ecd":"import IPython\nimport kerastuner as kt\n# saving first sets to tfrecords format for better memory perf\nsave_tfrecords(x_train, y_train, \"dataTrain.tfrecords\")\nsave_tfrecords(x_test, y_test, \"dataTest.tfrecords\")\nsave_tfrecords(x_val, y_val, \"dataVal.tfrecords\")\n\n\n# and loading them\nBatch_size=64\ntrain_ds =get_dataset(\"dataTrain.tfrecords\",True,Batch_size)\nvalidation_ds =get_dataset(\"dataVal.tfrecords\",True,Batch_size)\ntest_ds =get_dataset(\"dataTest.tfrecords\",True,Batch_size)","b342ac0b":"# Optimal simple CNN network\ndef optimal_simple_model(summary):\n    dropoutRate=0.5\n    model = models.Sequential()\n    model.add(layers.experimental.preprocessing.Resizing(32, 32 ,input_shape=(32,32,3)))\n    model.add(layers.experimental.preprocessing.RandomRotation(0.2))\n    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32,32,3)))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.MaxPooling2D((2, 2)))\n    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n    model.add(layers.Flatten())\n    model.add(layers.Dropout(dropoutRate))\n    model.add(layers.Dense(200, activation='relu'))\n    model.add(layers.Dense(100, activation='softmax'))\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=[\"accuracy\"])\n    return model","8f129ba6":"best_filepath=\"weightsBest.simpleOptimal1.hdf5\"\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=18)\ncheckpoint =tf.keras.callbacks.ModelCheckpoint(best_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallback_list=[checkpoint,early_stopping]\nOPTIMAL_MODEL = optimal_simple_model(summary = True)\n\n%memit run (OPTIMAL_MODEL,callback_list,best_filepath)","9ce3bcac":"# and loading them\nBatch_size=4*64\ntrain_ds =get_dataset(\"dataTrain.tfrecords\",True,Batch_size)\nvalidation_ds =get_dataset(\"dataVal.tfrecords\",True,Batch_size)\ntest_ds =get_dataset(\"dataTest.tfrecords\",True,Batch_size)","5dcf740f":"def init_VGG16_optim_model(summary):\n    vgg_model=tf.keras.applications.VGG16(input_shape=(101,101,3), include_top=False, weights='imagenet')\n    \n    VGG16_MODEL=vgg_model.layers[0](vgg_model)\n    \n\n    dropout_layer = tf.keras.layers.Dropout(rate = 0.7)\n    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n\n  # add top layer for CIFAR100 classification\n    dense = tf.keras.layers.Dense(300,activation = \"relu\")\n    # add top layer for CIFAR100 classification\n    resize=layers.experimental.preprocessing.Resizing(101,101 ,input_shape=(32,32,3))\n    prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')\n    model = tf.keras.Sequential([layers.experimental.preprocessing.RandomRotation(0.2,input_shape=(32,32,3)),resize,VGG16_MODEL,dropout_layer,  global_average_layer,prediction_layer])\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.000005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=[\"accuracy\"])\n    if summary: \n        model.summary()\n    return model","1bb5704d":"best_filepath=\"weightsBest.transferLearning2.hdf5\"\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\ncheckpoint =tf.keras.callbacks.ModelCheckpoint(best_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallback_list=[checkpoint,early_stopping]\nTransfer_MODEL = init_VGG16_optim_model(summary = True)\n%memit run (Transfer_MODEL,callback_list,best_filepath)","dd4b4e0b":"def init_xceptional_optim_model(summary):\n    A=tf.keras.applications.Xception(input_shape=(401,401,3), include_top=False, weights='imagenet')\n    B=A.layers[0](A)\n\n    dropout_layer = tf.keras.layers.Dropout(rate = 0.6)\n    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n    for layer in B.layers[:120]:\n        layer.trainable=False\n  # add top layer for CIFAR100 classification\n    dense = tf.keras.layers.Dense(300,activation = \"relu\")\n    # add top layer for CIFAR100 classification\n    prediction_layer = tf.keras.layers.Dense(len(CIFAR100_LABELS_LIST),activation='softmax')\n    rotate=layers.experimental.preprocessing.RandomRotation(0.1,input_shape=(32,32,3))\n    resize=layers.experimental.preprocessing.Resizing(401,401 ,input_shape=(32,32,3))\n    model = tf.keras.Sequential([rotate,resize,B, global_average_layer,dropout_layer ,dense,prediction_layer])\n    model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.00005), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=[\"accuracy\"])\n    if summary: \n        model.summary()\n    return model","5d1d7d0c":"best_filepath=\"weightsBest.transferLearning3.hdf5\"\nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\ncheckpoint =tf.keras.callbacks.ModelCheckpoint(best_filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\ncallback_list=[checkpoint,early_stopping]\nTransfer_MODEL =init_xceptional_optim_model(summary = True)\n%memit run (Transfer_MODEL,callback_list,best_filepath)","fe224d0b":"## 1. Simple CNN optimization","d77307ec":"## Load data","980f0a47":"## Train and Evaluation","9f95143d":"## Saving TFRecords","5fcc1bd6":"# Net Architectures","eba7a557":"# Preparing and Helper functions","ac597ad9":"## 2. Transfer Learning: VGG16","06a3d1da":"# \u039d\u03b5\u03c5\u03c1\u03c9\u03bd\u03b9\u03ba\u03ac \u0394\u03af\u03ba\u03c4\u03c5\u03b1 \u03ba\u03b1\u03b9 \u0395\u03c5\u03c6\u03c5\u03ae \u03a5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac \u03a3\u03c5\u03c3\u03c4\u03ae\u03bc\u03b1\u03c4\u03b1\n\n\n## 3\u03b7 \u0395\u03c1\u03b3\u03b1\u03c3\u03af\u03b1\n\n\u0391\u03c5\u03b3\u03b5\u03c1\u03b9\u03bd\u03bf\u03cd \u0391\u03bd\u03b4\u03c1\u03b9\u03ac\u03bd\u03b1  el16192\n\n\u0392\u03b1\u03c3\u03b9\u03bb\u03b5\u03af\u03bf\u03c5 \u039c\u03b1\u03c1\u03af\u03b1    el16167\n\n\u0393\u03ba\u03ad\u03b3\u03ba\u03b1\u03c2 \u0394\u03b7\u03bc\u03ae\u03c4\u03c1\u03b9\u03bf\u03c2   el16004","2eab9dd9":"## 3. Trasnfer Learning: Xceptional"}}