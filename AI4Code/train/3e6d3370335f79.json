{"cell_type":{"6ef2a2db":"code","787dcf9d":"code","a598c3af":"code","29c14e9b":"code","8164d151":"code","9e0f9cd1":"code","0e444b99":"code","3c945f72":"code","255c28b1":"code","d03776f3":"code","8bebf4df":"code","2c1d6297":"code","a85dc709":"code","f7ac0cf9":"code","9035b37f":"code","f1429aa0":"code","817a9035":"code","3bc56a6c":"code","c7f371a4":"code","cf90dc99":"code","03266a9a":"code","bd558616":"code","7d6cd307":"code","bf2d5b14":"code","d1987e61":"code","4c2b3383":"code","bfb3eb70":"code","526b607c":"code","d98d2f55":"code","0dc1ab35":"code","98c01ff8":"code","a51b21ff":"code","55062415":"code","e40d3eed":"code","ad9acb1b":"code","cef01226":"code","eecb1073":"code","ef901f7a":"code","9ac8b775":"code","0d3e76e5":"code","a903cff3":"code","7f14cc0f":"code","6669e977":"code","f8c79c60":"code","4e2530ad":"code","cd188f0c":"code","5f46bdc0":"code","3d7787bb":"code","d7359d63":"code","381c6aee":"code","39a099b8":"code","ec81e67e":"code","3101ec84":"code","9e323591":"code","bb78dbc5":"code","3401dcb8":"code","76732cac":"code","0991e47c":"code","e9641fbf":"code","e57625eb":"code","4d0843ea":"code","05a73118":"code","df4d1164":"code","4c480196":"code","1f095058":"code","01324830":"code","d9ff30df":"code","27a5d767":"code","1cde4ed9":"code","5d965829":"code","2383052f":"code","042d3298":"code","1b981362":"code","4b3939f2":"code","8a02f966":"code","76b0ebd1":"code","20de3361":"code","21bc0726":"code","6b1b44f9":"code","85b0d442":"code","d017eb34":"code","156245ac":"code","e4914f12":"code","13efa584":"code","0426a448":"code","3f876cb1":"code","ed3b5120":"code","60febdac":"code","6bfb5e92":"code","aed13020":"code","90824725":"code","9f33a279":"markdown","bbc8dbe3":"markdown","edfba42a":"markdown","629edfb3":"markdown","da620f2b":"markdown","b09a8bd8":"markdown","2b796fbc":"markdown","a32b8c03":"markdown","d5820fb8":"markdown","d4986001":"markdown","4d36bd13":"markdown","32c1d139":"markdown","69678bdc":"markdown","fd740863":"markdown","43e1c108":"markdown","8a7a33d3":"markdown","6ceb18f5":"markdown","347b3e3c":"markdown","7fcaa894":"markdown","6264e25b":"markdown","35d1dca7":"markdown","55f4d34f":"markdown","8bbab87a":"markdown","25830610":"markdown","bfd9bc09":"markdown","d9c0a7bb":"markdown","d5bd1d83":"markdown","96831454":"markdown","ea9358fa":"markdown","e182818d":"markdown","4ebc25a7":"markdown","f0a22856":"markdown","5df4d15c":"markdown","cba3d40c":"markdown","5aa5b5ae":"markdown","e6703876":"markdown","172a8cec":"markdown","41d32205":"markdown","a29a8f80":"markdown","1bdacfc9":"markdown","48440764":"markdown","86602939":"markdown","b72cda5d":"markdown","1fccdcf8":"markdown","eb0ea1cb":"markdown","a56995d9":"markdown","43f175f1":"markdown","98ff1200":"markdown","19928004":"markdown","17bf6b61":"markdown","f2c26fe4":"markdown","4f69a639":"markdown","c4c7c60e":"markdown","34ac0ce5":"markdown","c29b82b3":"markdown","df7c7a60":"markdown","2f853505":"markdown","dff4a8af":"markdown","c4350848":"markdown","afa38514":"markdown","6f258efd":"markdown","8760a5db":"markdown","b65ebaea":"markdown","946eb69a":"markdown","13e9ad93":"markdown","2b2e4b57":"markdown","e0e34fbb":"markdown","b83f4231":"markdown","df781782":"markdown","1f81fb7d":"markdown","1916aeb9":"markdown","e1e4a2b5":"markdown","bd44e9b0":"markdown","5aa7e406":"markdown","fc99035e":"markdown","98142a86":"markdown","7daf1281":"markdown","bf7737d4":"markdown","6cc3ed2f":"markdown","d889f6c7":"markdown","8a8388de":"markdown","41c62b4f":"markdown","27ca3344":"markdown","9fc2fd47":"markdown","d6aef052":"markdown","b29cde6c":"markdown","8c909308":"markdown","7c36123a":"markdown","40b7092d":"markdown","db676482":"markdown","7d620075":"markdown","a6cbb5ef":"markdown","2ad47908":"markdown","28a7ae7f":"markdown","3560d3b7":"markdown","72303e65":"markdown","2baa509e":"markdown","caef6ca7":"markdown","66d8f0be":"markdown","ca22262e":"markdown","bd5a14bb":"markdown","8f5fbcc7":"markdown","a5665f3e":"markdown","940a8ff0":"markdown","48b5adc2":"markdown","10e393b6":"markdown","b5dc670b":"markdown","280842e4":"markdown","f50d9b67":"markdown","2df4eaeb":"markdown","7c8d2c6b":"markdown","c5679eab":"markdown","48d43762":"markdown","f164f00f":"markdown","c9be7b38":"markdown","9702b592":"markdown","4cf40e99":"markdown","b5eb9cd5":"markdown","91198b92":"markdown","aa0d53c1":"markdown","8c488cd3":"markdown","5ebb80bf":"markdown","7752e5d1":"markdown","38ef88e4":"markdown","e3393948":"markdown","a5ed191e":"markdown","4890c8b4":"markdown","6c09c41f":"markdown","4c80d480":"markdown","2d13df37":"markdown","06df0e76":"markdown","ecb104e6":"markdown","9a1bca82":"markdown","9ed2f7e4":"markdown","4ee08527":"markdown","fba49d35":"markdown","fd477f6a":"markdown","480cbe34":"markdown","0c9efd78":"markdown","af308b99":"markdown","fe401254":"markdown","cf011e0d":"markdown","5a65620e":"markdown","3fbb082f":"markdown","6f4b56b8":"markdown","d27f75b2":"markdown","002f286c":"markdown","56bd4283":"markdown","22f6da16":"markdown","122d1899":"markdown","c2535fdf":"markdown","4aabc057":"markdown","9c0e80ec":"markdown","c1bf0ca5":"markdown","cfc92937":"markdown","07ff73b1":"markdown","29a82af3":"markdown","18ccb6b9":"markdown","b2ab76d4":"markdown"},"source":{"6ef2a2db":"# Importing data analysis packages\nimport pandas as pd\nimport numpy as np\n\n# Importing data visualization packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Importing feature selection packages\nfrom sklearn.feature_selection import RFECV\n\n# Importing model selection packages\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_validate, GridSearchCV, cross_val_score\n\n# Importing machine learning packages\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, RidgeClassifier, SGDClassifier, PassiveAggressiveClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC \nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\n# Miscellaneous\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_colwidth',80)","787dcf9d":"# Importing datasets\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_combine = df_train.append(df_test, sort=False)","a598c3af":"df_train.head()","29c14e9b":"df_train.tail()","8164d151":"df_train.shape","9e0f9cd1":"df_test.shape","0e444b99":"feat_desc = pd.DataFrame({'Description': ['Passenger ID',\n                                          'Whether the passenger was survived or not',\n                                          'The ticket class that the passenger bought',\n                                          'The passenger name',\n                                          'The gender of the passenger',\n                                          'The age of the passenger',\n                                          'The number of siblings\/spouses that the passenger has aboard the Titanic',\n                                          'The number of parents\/children that the passenger has aboard the Titanic',\n                                          'The ticket number of the passenger',\n                                          'The ticket fare that the passenger paid',\n                                          'The cabin number that the passenger boarded',\n                                          'The passenger port of embarkation'], \n                          'Values': [df_train[i].unique() for i in df_train.columns],\n                          'Number of unique values': [len(df_train[i].unique()) for i in df_train.columns]}, \n                          index = df_train.columns)\n\nfeat_desc","3c945f72":"# Setting 'PassengerId' as the index of training and test dataset\ndf_train.set_index('PassengerId', inplace=True)\ndf_test.set_index('PassengerId', inplace=True)\ndf_combine = df_train.append(df_test, sort=False)","255c28b1":"plt.figure(figsize=(11,9))\n\n# Creating univariate distribution of Survived feature\nplt.subplot(221)\nsns.countplot(df_combine['Survived'], color='sandybrown')\nplt.ylabel('Number of passenger')\nplt.xlabel('Survived')\nplt.title('Number of passenger per survival (Survived)', size=13)\n\n# Creating univariate distribution of Sex feature\nplt.subplot(222)\nsns.countplot(df_combine['Sex'], color='sandybrown')\nplt.ylabel('Number of passenger')\nplt.xlabel('Sex')\nplt.title('Number of passenger per gender class (Sex)', size=13)\n\n# Creating univariate distribution of Pclass feature\nplt.subplot(223)\nsns.countplot(df_combine['Pclass'], color='sandybrown')\nplt.ylabel('Number of passenger')\nplt.xlabel('Ticket class (Pclass)')\nplt.title('Number of passenger per ticket class (Pclass)', size=13)\n\n# Creating univariate distribution of Embarked feature\nplt.subplot(224)\nsns.countplot(df_combine['Embarked'], color='sandybrown')\nplt.ylabel('Number of passenger')\nplt.xlabel('Port of embarkation (Embarked)')\nplt.title('Number of passenger per port of embarkation (Embarked)', size=13)\n\n# Adjusting the spaces between graphs\nplt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n\nplt.show()","d03776f3":"plt.figure(figsize=(12,10))\n\n# Creating univariate distribution of 'SibSp' feature\nplt.subplot(221)\nsns.countplot(df_combine['SibSp'], color='lightcoral')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Number of Siblings\/Spouses (SibSp)')\nplt.title('Number of Passengers per\\nNumber of Siblings\/Spouses (SibSp)', size=13)\n\n# Creating univariate distribution of 'Parch' feature\nplt.subplot(222)\nsns.countplot(df_combine['Parch'], color='lightcoral')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Number of Parents\/Childrens (Parch)')\nplt.title('Number of Passengers per\\nNumber of Parents\/Childrens (Parch)', size=13)\n\n# Creating univariate distribution of 'Age' feature\nplt.subplot(223)\nsns.distplot(df_combine['Age'].dropna(), color='lightcoral', kde=False, norm_hist=False)\nplt.ylabel('Number of Passengers')\nplt.xlabel('Age')\nplt.title('Distribution of Passengers\\' Age', size=13)\n\n# Creating univariate distribution of 'Fare' feature\nplt.subplot(224)\nsns.distplot(df_combine['Fare'].dropna(), color='lightcoral', kde=False, norm_hist=False)\nplt.ylabel('Number of Passengers')\nplt.xlabel('Fare')\nplt.title('Distribution of Fare', size=13)\n\nplt.subplots_adjust(hspace = 0.3, wspace = 0.3)\n\nplt.show()","8bebf4df":"plt.figure(figsize=(7,10))\n\n# Creating boxplot of 'SibSp' feature\nplt.subplot(221)\nsns.boxplot(x='SibSp', data=df_train, color='powderblue', orient='v')\nplt.ylabel('Number of siblings\/spouses (SibSp)')\n\n# Creating boxplot of 'Parch' feature\nplt.subplot(222)\nsns.boxplot(x='Parch', data=df_train, color='powderblue', orient='v')\nplt.ylabel('Number of parents\/children (Parch)')\n\n# Creating boxplot of 'Age' feature\nplt.subplot(223)\nsns.boxplot(x='Age', data=df_train, color='powderblue', orient='v')\n\n# Creating boxplot of 'Fare' feature\nplt.subplot(224)\nsns.boxplot(x='Fare', data=df_train, color='powderblue', orient='v')\n\nplt.subplots_adjust(hspace = 0.3, wspace = 0.5)\n\nplt.show()","2c1d6297":"# Identifying missing data in the training and test dataset\npd.DataFrame({'Number of Missing Values (Training)': df_train.isna().sum(), \n              '% of Missing Values (Training)': (df_train.isna().sum()\/df_train.shape[0] * 100).round(2),\n              'Number of Missing Values (Test)': df_test.isna().sum().round(0), \n              '% of Missing Values (Test)': (df_test.isna().sum()\/df_test.shape[0] * 100).round(2)})","a85dc709":"# Imputing 'Cabin' feature\ndf_train['Cabin'].fillna('Z', inplace=True)\ndf_test['Cabin'].fillna('Z', inplace=True)","f7ac0cf9":"df_combine['Cabin'] = df_train['Cabin'].str.get(0)\ndf_combine.groupby('Cabin')['Pclass'].value_counts().to_frame('Count')","9035b37f":"# Viewing rows that contain missing values in 'Embarked' feature\ndf_train.loc[df_train['Embarked'].isna()]","f1429aa0":"# Imputing 'Embarked' feature\ndf_train.loc[df_train['Embarked'].isna(), 'Embarked'] = 'S'","817a9035":"# Viewing rows that contain missing values in 'Fare' feature\ndf_test.loc[df_test['Fare'].isna()]","3bc56a6c":"# Imputing 'Fare' feature\ndf_test.loc[df_test['Fare'].isna()] = df_train['Fare'].mean()","c7f371a4":"plt.figure(figsize=(12,10))\n\n# Creating correlation matrix\nsns.heatmap(df_train.corr(), annot=True, cmap='coolwarm')\n\nplt.show()","cf90dc99":"# Creating 'df_age', which contains instances with complete data \n# and will be the training data for the linear regression model\ndf_age = df_train.loc[~df_train['Age'].isna()]\n\n# Initiating linear regression model\nreg = LinearRegression()\n\n# Training linear regression model\nreg.fit(df_age[['SibSp', 'Pclass']], df_age['Age'])\n\n# Predicting 'Age' feature by using linear regression model\npred_age_train = pd.Series(reg.predict(df_train[['SibSp', 'Pclass']]), index=df_train.index)\npred_age_test = pd.Series(reg.predict(df_test[['SibSp', 'Pclass']]), index=df_test.index)\n\n# Filling missing values based on the predicted 'Age' values\ndf_train['Age'].fillna(pred_age_train, inplace=True)\ndf_test['Age'].fillna(pred_age_test, inplace=True)","03266a9a":"plt.figure(figsize=(12,6))\n\n# Creating histogram of 'Age' feature from the training set\nplt.subplot(121)\nsns.distplot(df_train['Age'])\nplt.title('Age Distribution of Passengers in the Training Dataset After Imputation', size=13)\n\n# Creating histogram of 'Age' feature from the test set\nplt.subplot(122)\nsns.distplot(df_test['Age'])\nplt.title('Age Distribution of Passengers in the Test Dataset After Imputation', size=13)\n\nplt.show()","bd558616":"# Checking instances that have negative value in the training dataset\ndf_train.loc[df_train['Age'] < 0]","7d6cd307":"# Checking instances that have negative value in the test dataset\ndf_test.loc[df_test['Age'] < 0]","bf2d5b14":"# Replacing instances that have negative value with the mean of the 'Age' feature\ndf_train.loc[df_train['Age'] < 0, 'Age'] = df_train['Age'].mean()\ndf_test.loc[df_test['Age'] < 0, 'Age'] = df_train['Age'].mean()","d1987e61":"# Rounding values in the 'Age' feature\ndf_train['Age'] = df_train['Age'].round().astype('int')\ndf_test['Age'] = df_test['Age'].round().astype('int')","4c2b3383":"# Creating 'Title' feature\ndf_train['Title'] = df_train['Name'].str.split(',', expand=True)[1].str.split('.').str.get(0)\ndf_test['Title'] = df_test['Name'].str.split(',', expand=True)[1].str.split('.').str.get(0)","bfb3eb70":"# Viewing the distribution of 'Title' feature\ndf_train['Title'].value_counts().to_frame('Number of Passengers').T","526b607c":"# Creating 'SibSp+Parch' feature\ndf_train['SibSp+Parch'] = df_train['SibSp'] + df_train['Parch'] \ndf_test['SibSp+Parch'] = df_test['SibSp'] + df_test['Parch'] ","d98d2f55":"# Creating 'IsAlone' feature\ndf_train['IsAlone'] = df_train['SibSp+Parch'].map(lambda x: 1 if x == 0 else 0)\ndf_test['IsAlone'] = df_test['SibSp+Parch'].map(lambda x: 1 if x == 0 else 0)","0dc1ab35":"train_size = df_train.shape[0]\ntest_size = df_test.shape[0]\ndf_combine = df_train.append(df_test, sort=False)\ndf_combine['Last_Name'] = df_combine['Name'].str.split(',', expand=True)[0]","98c01ff8":"fare_df = df_combine.loc[df_combine['SibSp+Parch'] > 0, ['Last_Name', 'Fare', 'SibSp+Parch']]\nfare_diff = (fare_df.groupby(['Last_Name', 'SibSp+Parch'])['Fare'].aggregate('max') - fare_df.groupby(['Last_Name', 'SibSp+Parch'])['Fare'].aggregate('min')).value_counts()\nprint('Percentage of families with the same fare: {:.2f}%'.format(fare_diff[0]\/fare_diff.sum()*100))","a51b21ff":"train_temp_df = df_combine.iloc[:train_size]\nfamily_group_df = train_temp_df.loc[train_temp_df['SibSp+Parch']>0, \n                                    ['Last_Name', 'Fare', 'SibSp+Parch', 'Survived']].groupby(['Last_Name', 'Fare'])\nfamily_df = pd.DataFrame(data=family_group_df.size(), columns=['Size_in_training_dataset'])\nfamily_df['Survived_Total'] = family_group_df['Survived'].sum().astype('int')\nfamily_df['SibSp+Parch'] = family_group_df['SibSp+Parch'].mean().astype('int')\nall_survived = (family_df['Size_in_training_dataset'] == family_df['Survived_Total']).sum()\/len(family_df)*100\nprint('Families with the whole members survived: {:.1f}%'.format(all_survived))\nall_not_survived = (family_df['Survived_Total']==0).sum()\/len(family_df)*100\nprint('Families with the whole members not survived: {:.1f}%'.format(all_not_survived))","55062415":"df_combine['FamilySurvival'] = 0.5\n\nfor _, grp_df in df_combine[['Survived', 'Last_Name', 'Fare']].groupby(['Last_Name', 'Fare']):\n    if len(grp_df) > 1:\n        for ind, row in grp_df.iterrows():\n            ## Finding out if any family members survived or not\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            ## If any family members survived, put this feature as 1\n            if smax == 1: \n                df_combine.loc[ind, 'FamilySurvival'] = 1\n            ## Otherwise if any family members perished, put this feature as 0\n            elif smin == 0: \n                df_combine.loc[ind, 'FamilySurvival'] = 0","e40d3eed":"train_temp_df = df_combine.iloc[:train_size]\nticket_group_df = train_temp_df.groupby('Ticket')\nticket_df = pd.DataFrame(data=ticket_group_df.size(), columns=['Size_in_training_dataset'])\nticket_df['Survived_Total'] = ticket_group_df['Survived'].sum().astype('int')\nticket_df['Not_Family'] = ticket_group_df['Last_Name'].unique().apply(len)\nticket_df = ticket_df.loc[(ticket_df['Size_in_training_dataset'] > 1) & (ticket_df['Not_Family'] > 1)]\nprint('Number of groups in training set that is not family: {}'.format(len(ticket_df)))\nall_survived = (ticket_df['Size_in_training_dataset'] == ticket_df['Survived_Total']).sum()\/len(ticket_df)*100\nprint('Families with the whole members survived: {:.1f}%'.format(all_survived))\nall_not_survived = (ticket_df['Survived_Total'] == 0).sum()\/len(ticket_df)*100\nprint('Families with the whole members not survived: {:.1f}%'.format(all_not_survived))","ad9acb1b":"for grp, grp_df in df_combine.groupby('Ticket'):\n    if len(grp_df) > 1:\n        for ind, row in grp_df.iterrows():\n            if (row['FamilySurvival']) == 0 or (row['FamilySurvival'] == 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                if smax == 1:\n                    df_combine.loc[ind, 'FamilySurvival'] = 1\n                elif smin == 0:\n                    df_combine.loc[ind, 'FamilySurvival'] = 0\n\ndf_train['FamilySurvival'] = df_combine.iloc[:train_size]['FamilySurvival']\ndf_test['FamilySurvival'] = df_combine.iloc[train_size:]['FamilySurvival']","cef01226":"df_combine['RealFare'] = 0\n\nfor _, grp_df in df_combine.groupby(['Ticket']):\n    grp_size = len(grp_df)\n    for ind, row in grp_df.iterrows():\n        real_fare = row['Fare']\/grp_size\n        df_combine.loc[ind, 'RealFare'] = real_fare\n\ndf_train['Fare'] = df_combine.iloc[:train_size]['RealFare']\ndf_test['Fare'] = df_combine.iloc[train_size:]['RealFare']","eecb1073":"# Viewing data type of each feature in the dataset\ndf_train.dtypes.to_frame(name='Data type')","ef901f7a":"# Converting 'Sex' feature data type\ndf_train.replace({'male': 1, 'female': 0}, inplace=True)\ndf_test.replace({'male': 1, 'female': 0}, inplace=True)","9ac8b775":"# Converting 'Embarked' feature data type\ndf_train.replace({'S': 0, 'C': 1, 'Q': 2}, inplace=True)\ndf_test.replace({'S': 0, 'C': 1, 'Q': 2}, inplace=True)","0d3e76e5":"# Dropping 'Name' and 'Ticket' feature\ndf_train.drop(columns=['Name', 'Ticket'], inplace=True)\ndf_test.drop(columns=['Name', 'Ticket'], inplace=True)","a903cff3":"plt.figure(figsize=(13,10))\n\n# Creating a bar chart of ticket class (Pclass) vs probability of survival (Survived)\nax1 = plt.subplot(221)\ng1 = sns.barplot(x='Pclass', y='Survived', data=df_train, color='seagreen')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Ticket Class (Pclass)')\nax1.set_xticklabels(['1st Class', '2nd Class', '3rd Class'])\nplt.title('Ticket Class (Pclass) Survival Comparison', size=13)\n\n# Creating a bar chart of ticket class (Pclass) and gender (Sex) vs probability of survival (Survived)\nax2 = plt.subplot(222)\ng2 = sns.barplot(x='Pclass', y='Survived', hue='Sex', data=df_train, palette='BuGn_r')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Ticket Class (Pclass)')\nax2.set_xticklabels(['1st Class', '2nd Class', '3rd Class'])\nhandles, _ = g2.get_legend_handles_labels()\nax2.legend(handles, ['Female', 'Male'], title='Gender')\nplt.title('Ticket Class (Pclass) | Gender (Sex) Survival Comparison', size=13)\n\n# Creating a bar chart of ticket class (Pclass) and port of embarkation (Embarked) vs probability of survival (Survived)\nax3 = plt.subplot(223)\ng3 = sns.barplot(x='Pclass', y='Survived', hue='Embarked', data=df_train, palette='BuGn_r')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Ticket Class (Pclass)')\nax3.set_xticklabels(['1st Class', '2nd Class', '3rd Class'])\nhandles, _ = g3.get_legend_handles_labels()\nax3.legend(handles, ['Southampton', 'Cherbourg', 'Queenstown'], title='Port of Embarkation')\nplt.title('Ticket Class (Pclass) | Port of Embarkation (Embarked) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of ticket class (Pclass) and passenger is alone (IsAlone) vs probability of survival (Survived)\nax4 = plt.subplot(224)\ng4 = sns.barplot(x='Pclass', y='Survived', hue='IsAlone', data=df_train, palette='BuGn_r')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Ticket Class (Pclass)')\nax4.set_xticklabels(['1st Class', '2nd Class', '3rd Class'])\nhandles, _ = g4.get_legend_handles_labels()\nax4.legend(handles, ['No', 'Yes'], title='Is Alone?')\nplt.title('Ticket Class (Pclass) | Passenger Is Alone (IsAlone) \\n Survival Comparison', size=13)\n\nplt.subplots_adjust(hspace = 0.4, wspace = 0.3)\n\nplt.show()","7f14cc0f":"plt.figure(figsize=(13,10))\n\n# Creating a bar chart of gender (Sex) vs probability of survival (Survived)\nax1 = plt.subplot(221)\ng1 = sns.barplot(x='Sex', y='Survived', data=df_train, color='dodgerblue')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Gender (Sex)')\nax1.set_xticklabels(['Female', 'Male'])\nplt.title('Gender (Sex) Survival Comparison', size=13)\n\n# Creating a bar chart of gender (Sex) and ticket class (Pclass) vs probability of survival (Survived)\nax2 = plt.subplot(222)\ng2 = sns.barplot(x='Sex', y='Survived', hue='Pclass', data=df_train, palette='GnBu_d')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Gender (Sex)')\nax2.set_xticklabels(['Female', 'Male'])\nhandles, _ = g2.get_legend_handles_labels()\nax2.legend(handles, ['1st Class', '2nd Class', '3rd Class'], title='Ticket Class')\nplt.title('Gender (Sex) | Ticket Class (Pclass) Survival Comparison', size=13)\n\n# Creating a bar chart of gender (Sex) and port of embarkation (Embarked) vs probability of survival (Survived)\nax3 = plt.subplot(223)\ng3 = sns.barplot(x='Sex', y='Survived', hue='Embarked', data=df_train, palette='GnBu_d')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Gender (Sex)')\nax3.set_xticklabels(['Female', 'Male'])\nhandles, _ = g3.get_legend_handles_labels()\nax3.legend(handles, ['Southampton', 'Cherbourg', 'Queenstown'], title='Port of Embarkation')\nplt.title('Gender (Sex) | Port of Embarkation (Embarked) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of gender (Sex) and passenger is alone (IsAlone) vs probability of survival (Survived)\nax4 = plt.subplot(224)\ng4 = sns.barplot(x='Sex', y='Survived', hue='IsAlone', data=df_train, palette='GnBu_d')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Gender (Sex)')\nax4.set_xticklabels(['Female', 'Male'])\nhandles, _ = g4.get_legend_handles_labels()\nax4.legend(handles, ['No', 'Yes'], title='Is Alone?')\nplt.title('Gender (Sex) | Passenger Is Alone (IsAlone) \\n Survival Comparison', size=13)\n\nplt.subplots_adjust(hspace = 0.4, wspace = 0.3)\n\nplt.show()","6669e977":"plt.figure(figsize=(13,10))\n\n# Creating a bar chart of port of embarkation (Embarked) vs probability of survival (Survived)\nax1 = plt.subplot(221)\ng1 = sns.barplot(x='Embarked', y='Survived', data=df_train, color='steelblue')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Port of Embarkation (Embarked)')\nax1.set_xticklabels(['Southampton', 'Cherbourg', 'Queenstown'])\nplt.title('Port of Embarkation (Embarked) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of port of embarkation (Embarked) and ticket class (Pclass) \n# vs probability of survival (Survived)\nax2 = plt.subplot(222)\ng2 = sns.barplot(x='Embarked', y='Survived', hue='Pclass', data=df_train, palette='ocean_r')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Port of Embarkation (Embarked)')\nax2.set_xticklabels(['Southampton', 'Cherbourg', 'Queenstown'])\nhandles, _ = g2.get_legend_handles_labels()\nax2.legend(handles, ['1st Class', '2nd Class', '3rd Class'], title='Ticket Class')\nplt.title('Port of Embarkation (Embarked) | Ticket Class (Pclass) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of port of embarkation (Embarked) and gender (Sex) vs probability of survival (Survived)\nax3 = plt.subplot(223)\ng3 = sns.barplot(x='Embarked', y='Survived', hue='Sex', data=df_train, palette='ocean_r')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Port of Embarkation (Embarked)')\nax3.set_xticklabels(['Southampton', 'Cherbourg', 'Queenstown'])\nhandles, _ = g3.get_legend_handles_labels()\nax3.legend(handles, ['Female', 'Male'], title='Gender')\nplt.title('Port of Embarkation (Embarked) | Gender (Sex) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of port of embarkation (Embarked) and passenger is alone (IsAlone) \n# vs probability of survival (Survived)\nax4 = plt.subplot(224)\ng4 = sns.barplot(x='Embarked', y='Survived', hue='IsAlone', data=df_train, palette='ocean_r')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Port of Embarkation (Embarked)')\nax4.set_xticklabels(['Southampton', 'Cherbourg', 'Queenstown'])\nhandles, _ = g4.get_legend_handles_labels()\nax4.legend(handles, ['No', 'Yes'], title='Is Alone?')\nplt.title('Port of Embarkation (Embarked) | Passenger Is Alone (IsAlone) \\n Survival Comparison', size=13)\n\nplt.subplots_adjust(hspace = 0.4, wspace = 0.3)\n\nplt.show()","f8c79c60":"plt.figure(figsize=(13,10))\n\n# Creating a bar chart of passenger is alone (IsAlone) vs probability of survival (Survived)\nax1 = plt.subplot(221)\ng1 = sns.barplot(x='IsAlone', y='Survived', data=df_train, color='steelblue', ci = None)\nplt.ylabel('Probability of Survival')\nplt.xlabel('Passenger Is Alone (IsAlone)')\nax1.set_xticklabels(['No', 'Yes'])\nplt.title('Passenger Is Alone (IsAlone) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of passenger is alone (IsAlone) and ticket class (Pclass) \n# vs probability of survival (Survived)\nax2 = plt.subplot(222)\ng2 = sns.barplot(x='IsAlone', y='Survived', hue='Pclass', data=df_train, palette='ocean_r', ci = None)\nplt.ylabel('Probability of Survival')\nplt.xlabel('Passenger Is Alone (IsAlone)')\nax2.set_xticklabels(['No', 'Yes'])\nhandles, _ = g2.get_legend_handles_labels()\nax2.legend(handles, ['1st Class', '2nd Class', '3rd Class'], title='Ticket Class')\nplt.title('Passenger Is Alone (IsAlone) | Ticket Class (Pclass) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of passenger is alone (IsAlone) and gender (Sex) vs probability of survival (Survived)\nax3 = plt.subplot(223)\ng3 = sns.barplot(x='IsAlone', y='Survived', hue='Sex', data=df_train, palette='ocean_r', ci = None)\nplt.ylabel('Probability of Survival')\nplt.xlabel('Passenger Is Alone (IsAlone)')\nax3.set_xticklabels(['No', 'Yes'])\nhandles, _ = g3.get_legend_handles_labels()\nax3.legend(handles, ['Female', 'Male'], title='Gender')\nplt.title('Passenger Is Alone (IsAlone) | Gender (Sex) \\n Survival Comparison', size=13)\n\n# Creating a bar chart of passenger is alone (IsAlone) and port of embarkation (Embarked) \n# vs probability of survival (Survived)\nax4 = plt.subplot(224)\ng4 = sns.barplot(x='IsAlone', y='Survived', hue='Embarked', data=df_train, palette='ocean_r', ci = None)\nplt.ylabel('Probability of Survival')\nplt.xlabel('Passenger Is Alone (IsAlone)')\nax4.set_xticklabels(['No', 'Yes'])\nhandles, _ = g4.get_legend_handles_labels()\nax4.legend(handles, ['Southampton', 'Cherbourg', 'Queenstown'], title='Port of Embarkation')\nplt.title('Passenger Is Alone (IsAlone) | Port of Embarkation (Embarked) \\n Survival Comparison', size=13)\n\nplt.subplots_adjust(hspace = 0.4, wspace = 0.3)\n\nplt.show()","4e2530ad":"plt.figure(figsize=(18,3))\n\n# Creating a bar chart of passenger's title (Title) vs probability of survival (Survived)\nsns.barplot(x='Title', y='Survived', data=df_train, color='cadetblue', ci = None)\nplt.ylabel('Probability of Survival')\nplt.xlabel('Passenger Title')\nplt.title('Passenger Title Survival Comparison', size=13)\n\nplt.show()","cd188f0c":"# Viewing the distribution of passenger's title ('Title') in the training dataset\ndf_train['Title'].value_counts().to_frame('Number of Passengers').T","5f46bdc0":"# Viewing the distribution of passenger's title ('Title') in the test dataset\ndf_test['Title'].value_counts().to_frame('Number of Passengers').T","3d7787bb":"# Binning titles with count less than 10 into a new category named 'Other'\ndf_train['Title'] = df_train['Title'].str.strip().map(lambda x: x if x == 'Mr' or x == 'Miss' or x == 'Mrs' or x == 'Master' else 'Other')\ndf_test['Title'] = df_test['Title'].str.strip().map(lambda x: x if x == 'Mr' or x == 'Miss' or x == 'Mrs' or x == 'Master' else 'Other')","d7359d63":"plt.figure(figsize=(15,10))\n\n# Creating a bar chart of passenger's title (Title) vs probability of survival (Survived)\nax1 = plt.subplot(221)\ng1 = sns.barplot(x='Title', y='Survived', data=df_train, color='cadetblue')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Title')\nplt.title('Passenger\\'s Title Survival Comparison', size=13)\n\n# Creating a bar chart of passenger's title (Title) and ticket class (Pclass) vs \n# probability of survival (Survived)\nax2 = plt.subplot(222)\ng2 = sns.barplot(x='Title', y='Survived', hue='Pclass', data=df_train, palette='GnBu_d')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Title')\nhandles, _ = g2.get_legend_handles_labels()\nax2.legend(handles, ['1st Class', '2nd Class', '3rd Class'], title='Ticket Class')\nplt.title('Passenger\\'s Title | Ticket Class (Pclass) Survival Comparison', size=13)\n\n# Creating a bar chart of passenger's title (Title) and passenger is alone (IsAlone) vs \n# probability of survival (Survived)\nax3 = plt.subplot(223)\ng3 = sns.barplot(x='Title', y='Survived', hue='IsAlone', data=df_train, palette='GnBu_d')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Title')\nhandles, _ = g3.get_legend_handles_labels()\nax3.legend(handles, ['No', 'Yes'], title='Is Alone?')\nplt.title('Passenger\\'s Title | Passenger Is Alone (IsAlone) Survival Comparison', size=13)\n\n# Creating a bar chart of passenger's title (Title) and port of embarkation (Embarked) vs \n# probability of survival (Survived)\nax4 = plt.subplot(224)\ng4 = sns.barplot(x='Title', y='Survived', hue='Embarked', data=df_train, palette='GnBu_d')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Title')\nhandles, _ = g4.get_legend_handles_labels()\nax4.legend(handles, ['Southampton', 'Cherbourg', 'Queenstown'], title='Port of Embarkation')\nplt.title('Passenger\\'s Title | Port of Embarkation (Embarked) Survival Comparison', size=13)\n\nplt.subplots_adjust(hspace = 0.4, wspace = 0.3)\n\nplt.show()","381c6aee":"# Checking the data type of 'Title' feature\ndf_train['Title'].dtype","39a099b8":"# Performing dummy coding scheme to 'Title' feature\ndf_train = df_train.join(pd.get_dummies(df_train['Title'], prefix='Title'), how='outer')\ndf_test = df_test.join(pd.get_dummies(df_test['Title'], prefix='Title'), how='outer')","ec81e67e":"# Dropping 'Title' feature\ndf_train.drop(columns=['Title'], inplace=True)\ndf_test.drop(columns=['Title'], inplace=True)","3101ec84":"plt.figure(figsize=(20,5))\n\n# Creating a bar chart of passenger's age (Age) vs probability of survival (Survived)\nsns.barplot(x='Age', y='Survived', data=df_train, ci=True, color='cadetblue')\nplt.ylabel('Probability of Survival')\nplt.title('Passenger\\'s Age Survival Comparison', size=15)\n\nplt.show()","9e323591":"# Binning 'Age' feature\ndf_train['Age_binned'] = pd.cut(df_train['Age'], np.arange(0, 85, 5), include_lowest=True)\ndf_test['Age_binned'] = pd.cut(df_test['Age'], np.arange(0, 85, 5), include_lowest=True)","bb78dbc5":"# Creating a bar chart of passenger's age group (Age_binned) vs probability of survival (Survived)\nplt.figure(figsize=(20,5))\nsns.barplot(x='Age_binned', y='Survived', data=df_train, ci=False, color='cadetblue')\nplt.xlabel('Age Group')\nplt.ylabel('Probability of Survival')\nplt.title('Passenger\\'s Age Survival Comparison', size=13)\n\nplt.show()","3401dcb8":"# Binning 'Age' feature\ndf_train['Age_binned'] = pd.cut(df_train['Age'], [0, 5, 30, 60, 80], include_lowest=True)\ndf_test['Age_binned'] = pd.cut(df_test['Age'], [0, 5, 30, 60, 80], include_lowest=True)","76732cac":"#plt.figure(figsize=(8,6))\n\n# Creating a bar chart of passenger's age group (Age_binned) vs probability of survival (Survived)\nsns.barplot(x='Age_binned', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Age Group')\nplt.ylabel('Probability of Survival')\nplt.title('Passenger\\'s Age Survival Comparison', size=13)\n\nplt.show()","0991e47c":"# Checking the data type of 'Age_binned' feature\ndf_train['Age_binned'].dtype","e9641fbf":"# Converting 'Age_binned' feature data type\ndf_train['Age_binned'] = pd.cut(df_train['Age'], [0, 5, 30, 60, 80], labels=[0, 1, 2, 3], retbins=False, include_lowest=True)\ndf_train['Age_binned'] = df_train['Age_binned'].astype('int')\ndf_test['Age_binned'] = pd.cut(df_test['Age'], [0, 5, 30, 60, 80], labels=[0, 1, 2, 3], retbins=False, include_lowest=True)\ndf_test['Age_binned'] = df_test['Age_binned'].astype('int')","e57625eb":"# Dropping 'Age' feature\ndf_train.drop(columns='Age', inplace=True)\ndf_test.drop(columns='Age', inplace=True)","4d0843ea":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of number of siblings\/spouses (SibSp) vs probability of survival (Survived)\nsns.barplot(x='SibSp', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Number of Siblings\/Spouses (SibSp)')\nplt.ylabel('Probability of Survival')\nplt.title('Number of Siblings\/Spouses (SibSp) Survival Comparison', size=13)\n\nplt.show()","05a73118":"plt.figure(figsize=(14,5))\n\n# Creating boxplot of 'SibSp' feature\nplt.subplot(121)\nsns.boxplot(x='SibSp', data=df_train, color='cadetblue', orient='v')\nplt.ylabel('Number of Siblings\/Spouses (SibSp)')\n\n# Creating univariate distribution of 'SibSp' feature\nplt.subplot(122)\nsns.countplot(x='SibSp', data=df_train, color='cadetblue', orient='v')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Number of Siblings\/Spouses (SibSp)')\nplt.title('Number of Passengers per Number of Siblings\/Spouses (SibSp)', size=13)\n\nplt.show()","df4d1164":"# In SibSp feature, binning values 3 to 8 together into value 3 \ndf_train['SibSp'] = df_train['SibSp'].map(lambda x: 3 if x == 4 or x == 5 or x == 8 else x)\ndf_test['SibSp'] = df_test['SibSp'].map(lambda x: 3 if x == 4 or x == 5 or x == 8 else x)","4c480196":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of number of siblings\/spouses (SibSp) vs probability of survival (Survived)\nsns.barplot(x='SibSp', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Number of Siblings\/Spouses (SibSp)')\nplt.ylabel('Probability of Survival')\nplt.title('Number of Siblings\/Spouses (SibSp) Survival Comparison', size=13)\n\nplt.show()","1f095058":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of number parents\/children (Parch) vs probability of survival (Survived)\nsns.barplot(x='Parch', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Number of Parents\/Children (Parch)')\nplt.ylabel('Probability of Survival')\nplt.title('Number of Parents\/Children (Parch) Survival Comparison', size=13)\n\nplt.show()","01324830":"plt.figure(figsize=(14,5))\n\n# Creating boxplot of 'Parch' feature\nplt.subplot(121)\nsns.boxplot(x='Parch', data=df_train, color='cadetblue', orient='v')\nplt.ylabel('Number of Parents\/Children (Parch)')\n\n# Creating univariate distribution of 'Parch' feature\nplt.subplot(122)\nsns.countplot(x='Parch', data=df_train, color='cadetblue', orient='v')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Number of Parents\/Children (Parch)')\nplt.title('Number of Passengers per Parents\/Children (Parch)', size=13)\n\nplt.show()","d9ff30df":"# In Parch feature, binning value 1 to 6 together into value 1 \ndf_train['Parch'] = df_train['Parch'].map(lambda x: x if x == 0 else 1)\ndf_test['Parch'] = df_test['Parch'].map(lambda x: x if x == 0 else 1)","27a5d767":"plt.figure(figsize=(6,5))\n\n# Creating a bar chart of number of parents\/children (Parch) vs probability of survival (Survived)\nsns.barplot(x='Parch', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Number of Parents\/Children (Parch)')\nplt.ylabel('Probability of Survival')\nplt.title('Number of Parents\/Children (Parch) Survival Comparison', size=13)\n\nplt.show()","1cde4ed9":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of number siblings\/spouses\/parents\/children (SibSp+Parch) vs \n# probability of survival (Survived)\nsns.barplot(x='SibSp+Parch', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)')\nplt.ylabel('Probability of Survival')\nplt.title('Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)\\n Survival Comparison', size=13)\n\nplt.show()","5d965829":"plt.figure(figsize=(14,5))\n\n# Creating boxplot of 'SibSp+Parch' feature\nplt.subplot(121)\nsns.boxplot(x='SibSp+Parch', data=df_train, color='cadetblue', orient='v')\nplt.ylabel('Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)')\n\n# Creating univariate distribution of 'SibSp+Parch' feature\nplt.subplot(122)\nsns.countplot(x='SibSp+Parch', data=df_train, color='cadetblue', orient='v')\nplt.ylabel('Number of Passengers')\nplt.xlabel('Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)')\nplt.title('Number of Passengers per\\n Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)', size=13)\n\nplt.show()","2383052f":"# In SibSp+Parch feature, binning values 1, 2, and 3 together into value 1\ndf_train['SibSp+Parch'] = df_train['SibSp+Parch'].map(lambda x: 1 if x == 1 or x == 2 or x == 3 else x)\ndf_test['SibSp+Parch'] = df_test['SibSp+Parch'].map(lambda x: 1 if x == 1 or x == 2 or x == 3 else x)\n\n# In SibSp+Parch feature, binning values 4 to 10 together into value 2\ndf_train['SibSp+Parch'] = df_train['SibSp+Parch'].map(lambda x: 2 if x == 4 or x == 5 or x == 6 or x == 7 or x == 10 else x)\ndf_test['SibSp+Parch'] = df_test['SibSp+Parch'].map(lambda x: 2 if x == 4 or x == 5 or x == 6 or x == 7 or x == 10 else x)","042d3298":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of number siblings\/spouses\/parents\/children (SibSp+Parch) vs \n# probability of survival (Survived)\nsns.barplot(x='SibSp+Parch', y='Survived', data=df_train, color='cadetblue')\nplt.xlabel('Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)')\nplt.ylabel('Probability of Survival')\nplt.title('Number Siblings\/Spouses\/Parents\/Children (SibSp+Parch)\\n Survival Comparison', size=13)\n\nplt.show()","1b981362":"plt.figure(figsize=(14,5))\n\nplt.hist([df_train.loc[df_train['Survived']==0,'Fare'], \n          df_train.loc[df_train['Survived']==1,'Fare']], stacked=True, bins=20, label=['Not Survived', 'Survived'])\nplt.ylabel('Number of Passengers')\nplt.xlabel('Ticket Price (Fare)')\nplt.title('Histogram of Ticket Price (Fare) with Survived\/Not-Survived Stacked', size=13)\nplt.legend()\n\nplt.show()","4b3939f2":"# Binning 'Fare' feature\ndf_train['Fare_binned'] = pd.cut(df_train['Fare'], bins=[0,25,75,513], include_lowest=True)\ndf_test['Fare_binned'] = pd.cut(df_test['Fare'], bins=[0,25,75,513], include_lowest=True)","8a02f966":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of ticket price (Fare_binned) vs probability of survival (Survived)\nsns.barplot(x='Fare_binned', y='Survived', data=df_train, color='cadetblue')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Fare Group')\n\nplt.show()","76b0ebd1":"df_train['Fare_binned'].dtype","20de3361":"# Converting 'Fare' feature data type\ndf_train['Fare_binned'] = pd.cut(df_train['Fare'], bins=[0,25,75,513], labels=[0, 1, 2], retbins=False, include_lowest=True)\ndf_train['Fare_binned'] = df_train['Fare_binned'].astype('int')\ndf_test['Fare_binned'] = pd.cut(df_test['Fare'], bins=[0,25,75,513], labels=[0, 1, 2], retbins=False, include_lowest=True)\ndf_test['Fare_binned'] = df_test['Fare_binned'].astype('int')","21bc0726":"# Dropping 'Fare' feature\ndf_train.drop(columns='Fare', inplace=True)\ndf_test.drop(columns='Fare', inplace=True)","6b1b44f9":"# Getting the first letter of 'Cabin' feature\ndf_train['Cabin'] = df_train['Cabin'].str.get(0)","85b0d442":"plt.figure(figsize=(8,5))\n\n# Creating a bar chart of Cabin vs probability of survival (Survived)\nsns.barplot(x='Cabin', y='Survived', data=df_train, color='cadetblue')\nplt.ylabel('Probability of Survival')\nplt.xlabel('Cabin')\nplt.title('Cabin Survival Comparison', size=13)\n\nplt.show()","d017eb34":"# Creating 'HaveCabin' feature\n#df_train['HaveCabin'] = df_train['Cabin'].str.get(0)\ndf_test['HaveCabin'] = df_test['Cabin'].str.get(0)\ndf_train['HaveCabin'] = df_train['Cabin'].map(lambda x: 0 if x == 'Z' else 1)\ndf_test['HaveCabin'] = df_test['HaveCabin'].map(lambda x: 0 if x == 'Z' else 1)","156245ac":"plt.figure(figsize=(8,5))\n\nsns.barplot(x='HaveCabin', y='Survived', data=df_train, color='cadetblue')\nplt.ylabel('Probability of Survival')\nplt.xlabel('HaveCabin')\nplt.title('Cabin Survival Comparison', size=13)\n\nplt.show()","e4914f12":"# Dropping 'Cabin' feature\ndf_train.drop(columns=['Cabin'], inplace=True)\ndf_test.drop(columns=['Cabin'], inplace=True)","13efa584":"plt.figure(figsize=(14,10))\n\n# Creating a heatmap of correlation among features\nsns.heatmap(df_train.corr(), cmap='RdYlGn', annot=True)\nplt.title('Correlation Among Features', size=15)\n\nplt.show()","0426a448":"# List of machine learning algorithms that will be used for predictions\nestimator = [('Logistic Regression', LogisticRegression), ('Ridge Classifier', RidgeClassifier), \n             ('SGD Classifier', SGDClassifier), ('Passive Aggressive Classifier', PassiveAggressiveClassifier), \n             ('SVC', SVC), ('Linear SVC', LinearSVC), ('Nu SVC', NuSVC), \n             ('K-Neighbors Classifier', KNeighborsClassifier),\n             ('Gaussian Naive Bayes', GaussianNB), ('Multinomial Naive Bayes', MultinomialNB), \n             ('Bernoulli Naive Bayes', BernoulliNB), ('Complement Naive Bayes', ComplementNB), \n             ('Decision Tree Classifier', DecisionTreeClassifier), \n             ('Random Forest Classifier', RandomForestClassifier), ('AdaBoost Classifier', AdaBoostClassifier), \n             ('Gradient Boosting Classifier', GradientBoostingClassifier), ('Bagging Classifier', BaggingClassifier), \n             ('Extra Trees Classifier', ExtraTreesClassifier), ('XGBoost', XGBClassifier)]\n\n# Separating independent features and dependent feature from the dataset\nX_train = df_train.drop(columns='Survived')\ny_train = df_train['Survived']\n\n# Creating a dataframe to compare the performance of the machine learning models\ncomparison_cols = ['Algorithm', 'Training Time (Avg)', 'Accuracy (Avg)', 'Accuracy (3xSTD)']\ncomparison_df = pd.DataFrame(columns=comparison_cols)\n\n# Generating training\/validation dataset splits for cross validation\ncv_split = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n\n# Performing cross-validation to estimate the performance of the models\nfor idx, est in enumerate(estimator):\n    \n    cv_results = cross_validate(est[1](), X_train, y_train, cv=cv_split)\n    \n    comparison_df.loc[idx, 'Algorithm'] = est[0]\n    comparison_df.loc[idx, 'Training Time (Avg)'] = cv_results['fit_time'].mean()\n    comparison_df.loc[idx, 'Accuracy (Avg)'] = cv_results['test_score'].mean()\n    comparison_df.loc[idx, 'Accuracy (3xSTD)'] = cv_results['test_score'].std() * 3\n\ncomparison_df.set_index(keys='Algorithm', inplace=True)\ncomparison_df.sort_values(by='Accuracy (Avg)', ascending=False, inplace=True)\n\n#Visualizing the performance of the models\nfig, ax = plt.subplots(figsize=(12,10))\n\ny_pos = np.arange(len(comparison_df))\nax.barh(y_pos, comparison_df['Accuracy (Avg)'], xerr=comparison_df['Accuracy (3xSTD)'], color='skyblue')\nax.set_yticks(y_pos)\nax.set_yticklabels(comparison_df.index)\nax.set_xlabel('Accuracy Score (Average)')\nax.set_title('Performance Comparison After Simple Modelling', size=13)\nax.set_xlim(0, 1)\n\nplt.show()","3f876cb1":"# A list of machine learning algorithms that will be optimized\nestimator = [('Logistic Regression', LogisticRegression), ('Ridge Classifier', RidgeClassifier), ('SVC', SVC), \n             ('Linear SVC', LinearSVC), ('Nu SVC', NuSVC), ('Random Forest Classifier', RandomForestClassifier), \n             ('AdaBoost Classifier', AdaBoostClassifier), \n             ('Gradient Boosting Classifier', GradientBoostingClassifier), \n             ('Bagging Classifier', BaggingClassifier), ('XGBoost', XGBClassifier)\n            ]\n\nindex = [est[0] for est in estimator]\n\n# A dictionary containing hyperparameters that are to be optimized for each machine learning algorithm\ngrid_params = {'SVC': {'C': np.arange(1,21,1), 'gamma': [0.005, 0.01, 0.015, 0.02], 'random_state': [0]},\n               'Ridge Classifier': {'alpha': [0.001, 0.0025, 0.005], 'random_state': [0]},\n               'Nu SVC': {'nu': [0.5], 'gamma': [0.001, 0.01, 0.1, 1], 'random_state': [0]},\n               'Gradient Boosting Classifier': {'learning_rate': [0.001, 0.005, 0.01, 0.015], 'random_state': [0],\n                                                'max_depth': [1,2,3,4,5], 'n_estimators': [300, 350, 400, 450, 500]},\n               'Linear SVC': {'C': [1, 5, 10], 'random_state': [0]},\n               'Logistic Regression': {'C': np.arange(2,7.5,0.25), \n                                       'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], \n                                       'random_state': [0]},\n               'AdaBoost Classifier': {'learning_rate': np.arange(0.05, 0.21, 0.01), 'n_estimators': [50, 75, 100, 125, 150], \n                                       'random_state': [0]},\n               'Random Forest Classifier': {'n_estimators': [200, 250, 300, 350], 'max_depth': [1,2,3,4,5,6], \n                                            'criterion': ['gini', 'entropy'], 'random_state': [0]},\n               'Bagging Classifier': {'n_estimators': np.arange(200, 300, 10), 'random_state': [0]},\n               'XGBoost': {'learning_rate': [0.001, 0.005, 0.01, 0.015], 'random_state': [0],\n                           'max_depth': [1,2,3,4,5], 'n_estimators': [300, 350, 400, 450, 500]}\n              }\n\n# Creating a dataframe to compare the performance of the machine learning models after hyperparameter optimization \nbest_params_df = pd.DataFrame(columns=['Optimized Hyperparameters', 'Accuracy'], index=index)\n\n# start_total = time.perf_counter()\n\n# Performing grid-search cross-validation to optimize hyperparameters and estimate the performance of the models\nfor idx, est in enumerate(estimator):\n    \n    # start = time.perf_counter()\n    \n    best_clf = GridSearchCV(est[1](), param_grid=grid_params[est[0]], cv=cv_split, scoring='accuracy', n_jobs=12)\n    best_clf.fit(X_train, y_train)\n    \n    # run = time.perf_counter() - start\n    \n    best_params_df.loc[est[0], 'Optimized Hyperparameters'] = [best_clf.best_params_]\n    best_params_df.loc[est[0], 'Accuracy'] = best_clf.best_score_\n    \n    #print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(est[0], best_clf.best_params_, run))\n\n    \n#run_total = time.perf_counter() - start_total\n#print('Total optimization time was {:.2f} minutes.'.format(run_total\/60))","ed3b5120":"# Showing the results of grid-search cross-validation\nbest_params_df.sort_values('Accuracy')","60febdac":"# Performing feature selection using RFECV\nxg = GradientBoostingClassifier(learning_rate=0.005, max_depth=2, n_estimators=450, random_state=0)\nselector = RFECV(xg, step=1, cv=cv_split, scoring='accuracy', n_jobs=8)\nselector = selector.fit(X_train, y_train)","6bfb5e92":"# Showing the result of RFECV\npd.DataFrame([X_train.columns, selector.ranking_], index=['Features', 'Ranking']).T.sort_values(by='Ranking')","aed13020":"# Listing the selected features based on RFECV\nselected_features = ['Pclass', 'Sex', 'SibSp+Parch', 'FamilySurvival', 'Title_Mr', 'Fare_binned']\n\n# Training the gradient boosting classifier model\ngb = GradientBoostingClassifier(learning_rate=0.005, max_depth=2, n_estimators=450, random_state=0)\ngb.fit(X_train[selected_features], y_train)\n\n# Estimating the performance of the model by using cross-validation\ngb_acc_score = cross_val_score(gb, X_train[selected_features], y_train, cv=cv_split, scoring='accuracy')\n\nprint('The performance of the model using the selected features: {:.2f}%'.format(gb_acc_score.mean()*100))","90824725":"# Training the model\ngb = GradientBoostingClassifier(learning_rate=0.005, max_depth=2, n_estimators=450, random_state=0)\ngb.fit(X_train[selected_features], y_train)\n\n# Creating a submission file\ntest_Survived = pd.DataFrame(gb.predict(df_test[selected_features]), columns=['Survived'], index=np.arange(892,1310,1))\ntest_Survived = test_Survived.reset_index()\ntest_Survived.rename(columns={'index': 'PassengerID'}, inplace=True)\ntest_Survived.to_csv(\"gb.csv\",index=False)","9f33a279":"### Viewing data shape","bbc8dbe3":"## 4.4. Passenger Is Alone (*IsAlone*) EDA\nFor passenger is alone feature, let's visualize the following:\n- Passenger is alone vs probability of survival (***Survived***)\n- Passenger is alone and ticket class (***Pclass***) vs probability of survival\n- Passenger is alone and gender (***Sex***) vs probability of survival\n- Passenger is alone and port of embarkation (***Embarked***) vs probability of survival","edfba42a":"<a id='section 4.6.'><\/a>","629edfb3":"If we closely examine the data, members of a family paid the same fare for the tickets. To show this, we call the following:","da620f2b":"Let's re-visualize number of parents\/children vs probability of survival bar chart after binning:","b09a8bd8":"# References","2b796fbc":"# 3.0. Data Wrangling\nData wrangling is the process of cleaning, structuring, and enriching raw data into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes, such as analytics. To prepare the dataset, we use the 4C's framework, i.e., **Correcting**, **Completing**, **Creating**, and **Converting**:\n- Correcting includes:\n  * [3.1. Rectifying any abnormal, inaccurate, or non-acceptable values](#section 3.1.)\n  * [3.2. Handling outliers](#section 3.2.)\n  * [3.6. Removing features that do not contribute to the analysis](#section 3.6.)\n- Completing includes:\n  * [3.3. Handling missing data](#section 3.3.)\n- Creating includes:\n  * [3.4. Creating new features from existing features (feature engineering)](#section 3.4.)\n- Converting includes:\n  * [3.5. Converting data types](#section 3.5.)","a32b8c03":"We can observe that passengers in cabin *E*, *D*, and *B* had a high chance of survival. Meanwhile, passengers with no cabin (*Z*) had lower chance of survival than passengers with a cabin (please see this [section](#Imputing missing values in Cabin feature) to understand why *Z* value represents passenger with no cabin). \n\nBased on this observation, we will create a feature named ***HaveCabin***, which determines whether a passenger had a cabin or not. If a passenger had a cabin, ***HaveCabin*** = 1. And if a passenger had no cabin, ***HaveCabin*** = 0. ","d5820fb8":"# 1.0. Problem Definition\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we are to perform analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\nBefore moving to the next section, we need to import all packages required to do the analysis by calling the following:","d4986001":"The results of grid-search cross-validation is shown below:","4d36bd13":"<a id='section 3.5.'><\/a>","32c1d139":"We can observe that ***Pclass***, ***Sex***, ***FamilySurvival***, and ***HaveCabin*** are some features that have a pretty high correlation with ***Survived***. Some multicollinearities (i.e., collinearity among independent features) are detected, such as ***Title_Mr*** and ***Sex***. Multicollinear features cause redundancies as they contain almost the same information. This increase training time. We should try to eliminate redundant features as it reduces training time. But for now, we will permit multicollinearities in our dataset. ","69678bdc":"We do not need ***Age*** feature anymore, so we can drop it.","fd740863":"<a id='section 5.2.'><\/a>","43e1c108":"Based on the charts above, we can infer that only a small number of passengers has siblings\/spouses\/parents\/children more than two. We can also see that values higher than two are considered as outliers. \n\nBased on the above observations, we decide to bin values *1*, *2*, and *3* together into value *1*. In other words, value *1* represents number of siblings\/spouses\/parents\/children between one and three. Meanwhile, we will bin values *4* to *10* together into value *2*. In other words, value *2* represents number of siblings\/spouses\/parents\/children between four and 10.","8a7a33d3":"Let's re-visualize number of siblings\/spouses vs probability of survival bar chart after binning:","6ceb18f5":"We can observe that it is difficult to see any patterns from the above bar plot. At a glimpse, there is no relationships\/correlations between the ***Age*** feature and the probability of survival. Thus, we will segment ***Age*** feature into some segments to find meaningful patterns in the ***Age*** feature. \n\nIn the first trial, we will segment ***Age*** feature into equal-width bins of width 5. We will name this segmentation as ***Age_binned*** feature. Let's try to visualize ***Age_binned*** vs probability of survival (***Survived***):","347b3e3c":"The performance of the model with feature selection (85.34% accuracy) is slightly lower than without feature selection (85.63% accuracy). But, with only six independent features, we can build a model that has an accuracy resembling of model with 16 features. This is the power of feature selection, where we can select features that contribute the most to our model.","7fcaa894":"This is my first kernel at Kaggle. The objective of this kernel is to show the comprehensive workflow of building predictive models by using machine learning tools. The workflow includes:\n1. **Problem Definition**: Without a correct understanding about the problem we face, we will not be able to apply the appropriate solution to the problem. A clear definition of the problem enables us to identify the appropriate data to gather and technique(s) to use in order to solve the problem. \n2. **Data Gathering**: Data is the lifeblood of predictive analytics. We have to know which data to use, where to gather them, and how to make them useful to solve our problem. \n3. **Data Wrangling**: Raw data are generally incomplete, inconsistent, and contain many errors. Thus, we need to prepare the data for further processing. Data wrangling is the process of cleaning, structuring, and enriching raw data into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes, such as analytics.\n4. **Exploratory Data Analysis**: Exploratory data analysis (EDA) is an approach of performing initial investigations on our data. EDA normally has descriptive nature and uses graphical statistics to discover patterns, to identify anomalies, to test hypothesis, and to check assumptions regarding our data. \n5. **Data Modelling**: There is no machine learning algorithm that works best in all situations. So, the best approach is to generate a high performance model is by trying multiple algorithms, optimizing them, and comparing them for your specific scenario. Data modelling involves selecting and optiming the machine learning models that generate the best predictive performance based on the data we have. \n6. **Prediction**: Once we have developed the best predictive model, we can deploy it to make predictions. \n\nI have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it. If you think this notebook gives something valuable to your journey to data science, PLEASE UPVOTE. It will keep me motivated.","6264e25b":"***Age_binned*** feature is still in a categorical data type. We need to tranform it to numerical data type. Because this feature is ordinal, the following transformations are made:\n- Age group *0-5* is represented by value 0\n- Age group *6-30* is represented by value 1\n- Age group *31-60* is represented by value 2\n- Age group *61-80* is represented by value 3","35d1dca7":"<a id='section 4.1.'><\/a>","55f4d34f":">*\u201cIt was the ship of dreams to everyone else. To me, it was a slave ship, taking me back to America in chains. Outwardly, I was everything a well brought up girl should be. Inside, I was screaming.\u201d \u2014Rose*","8bbab87a":"From the chart above, we can observe the following:\n- There is a decreasing trend of survival probability when the number of siblings\/spouses is one and onward. \n- All passengers who had more than four siblings\/spouses were not survived. ","25830610":"## 3.2. Handling outliers\nAn outlier is an observation point that is distant from other observations. Outliers in data can distort predictions and affect the accuracy, if we do not detect and handle them. There are several techniques to detect outliers. This [article](https:\/\/medium.com\/@mehulved1503\/effective-outlier-detection-techniques-in-machine-learning-ef609b6ade72) discusses some common techniques to detect outliers. \n\nIn this tutorial, we use **the box plot rule**. The box plot rule states that for a given numerical variable, outliers are those observations that lie outside 1.5 * IQR, where IQR, the \u2018Inter Quartile Range\u2019 is the difference between 75th and 25th quartiles. To perform outliers identification using the box plot rule, we plot all of the numerical features using box plot. ","bfd9bc09":"## 4.5. Passenger's Title (*Title*) EDA\nWe visualize passenger's title vs probability of survival (***Survived***):","d9c0a7bb":"<a id='section 3.3.'><\/a>","d5bd1d83":"# 5.0. Data Modelling\nThere is no machine learning algorithm that works best in all situations. So, the best approach is to generate a high performance model is by trying multiple algorithms, optimizing them, and comparing them for your specific scenario. In this section, we perform the following:\n- [**5.1. Simple Modelling**](#section 5.1.): We will handpick several machine learning algorithms and create models based on them and the dataset we have.\n- [**5.2. Hyperparameter Optimization for the Best Models**](#section 5.2.): We will pick the best models based on the simple modelling, and then tune their hyperparameters to improve their performance. \n- [**5.3. Model Optimization with Feature Selection**](#section 5.3.): More independent features do not make a better model, but the right independent features do. We will perform feature selection to automatically select features that contribute the most to output feature we are interested in.  \n\nIn this problem, we use **accuracy** as the measure of model's performance. Accuracy is the fraction of predictions our model got right. ","96831454":"The ***Title*** feature is still in a object (string) data type. We need to tranform it to numerical data type. This feature has no intrinsic ordering to the categories (i.e., nominal categorical feature). \n\nThere are several methods to transform nominal categorical features. Dipanjan Sarkar writes comprehensive discussions about these methods in this [article](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63). For now, we will use **dummy coding scheme** to transform ***Title*** feature. The coding scheme encodes or transforms the attribute into **m** binary features which can only contain a value of 1 or 0. Each observation in the categorical feature is thus converted into a vector of size m with only one of the values as 1 (indicating it as active). We perform dummy coding scheme to ***Title*** feature by calling the following:  ","ea9358fa":"Let's try to visualize ***Age_binned*** vs probability of survival (***Survived***):","e182818d":"# 4.0. Exploratory Data Analysis\nExploratory data analysis (EDA) is an approach of performing initial investigations on our data. EDA normally has descriptive nature and uses graphical statistics to discover patterns, to identify anomalies, to test hypothesis, and to check assumptions regarding our data. In this section, we will perform EDA for each feature in our dataset. The focus is to find patterns and correlations between the independent features and the output feature (i.e., ***Survived***). Some feature engineering are performed based on EDA. \n- [4.1. Ticket Class (***Pclass***) EDA](#section 4.1.)\n- [4.2. Gender (***Sex***) EDA](#section 4.2.)\n- [4.3. Port of Embarkation (***Embarked***) EDA](#section 4.3.)\n- [4.4. Passenger Is Alone (***IsAlone***) EDA](#section 4.4.)\n- [4.5. Passenger's Title (***Title***) EDA](#section 4.5.)\n- [4.6. Passenger's Age (***Age***) EDA](#section 4.6.)\n- [4.7. Number of Siblings\/Spouses (***SibSp***) EDA](#section 4.7.)\n- [4.8. Number of Parents\/Children (***Parch***) EDA](#section 4.8.)\n- [4.9. Number of  Siblings\/Spouses\/Parents\/Children (***SibSp+Parch***) EDA](#section 4.9.)\n- [4.10.Ticket Price (***Fare***) EDA](#section 4.10.)\n- [4.11.Passenger's Cabin (***Cabin***) EDA](#section 4.11.)\n- [4.12.Correlation Among Features](#section 4.12.)","4ebc25a7":"Let's visualize number of ticket price vs probability of survival bar chart after binning:","f0a22856":"There seems to a clear correlation after we perform binning in ***Fare*** feature. \n\nNow, let's check the data type of ***Fare_binned*** feature.","5df4d15c":"## 4.6. Passenger's Age (*Age*) EDA\nWe visualize passenger's age vs probability of survival (***Survived***) by calling the following:","cba3d40c":"From the table above, we can observe the following:\n- These features can be divided into four categories:\n  * Nominal categorical feature: feature that has two or more categories, but there is no intrinsic ordering to the categories. In the dataset, this includes ***Survived***, ***Name***, ***Sex***, ***Ticket***, ***Cabin***. \n  * Ordinal categorical feature: this feature has similar characteristics with nominal categorical feature. The difference is it has a clear ordering of the categories. In the dataset, this includes ***Pclass*** and ***Embarked***. \n  * Integer numerical feature: feature that must take an integer value. In the dataset, this includes ***SibSp*** and ***Parch***.\n  * Continuous numerical feature: feature that can take on infinitely many, uncountable values. In the dataset, this includes ***Age*** and ***Fare***.\n- ***Survived*** is the dependent\/outcome feature of this problem set. It contains binary data with value 0 (i.e., not survived) and 1 (i.e., survived). Other features are the potential independent\/predictor features.\n- ***Pclass*** contains three values: *1*, *2*, and *3*. *1* indicates 1st class ticket, *2* indicates 2nd class ticket, and *3* indicates 3rd class ticket.\n- ***Embarked*** feature contains three values: *S*, *C*, and *Q*. *S* indicates Southampton, *C* indicates Cherbourg, and *Q* indicates Queenstown. ***Embarked*** is considered as a ordinal categorical feature because Titanic embarked these three ports in a sequence. It went to Southampton, then Cherbourg, and finally Queenstown. \n- ***PassengerId*** feature contains unique identifier for each passenger. This feature does not have an impact to the output feature. However, it will be very useful to give each passenger a unique identifier.   ","5aa5b5ae":"Again, let's try to visualize ***Age_binned*** vs probability of survival (***Survived***):","e6703876":"<a id='References'><\/a>","172a8cec":"<a id='section 4.2.'><\/a>","41d32205":"To handle these negative values, we replace these values with the mean of the ***Age*** feature.","a29a8f80":"## 4.2. Gender (*Sex*) EDA\nFor gender feature, let's visualize the following:\n- Gender vs probability of survival (***Survived***)\n- Gender and ticket class (***Pclass***) vs probability of survival\n- Gender and port of embarkation (***Embarked***) vs probability of survival\n- Gender and passenger is alone (***IsAlone***) vs probability of survival","1bdacfc9":"### Imputing missing values in *Age* feature\n\nTo impute missing values in ***Age***, we use linear regression. Linear regression \u201ctheoretically\u201d provides good estimates for missing values. The feature with missing data, which is ***Age***, becomes the dependent variable. To identify the appropriate independent variables\/predictors, we use features that have high correlation with ***Age***. To do that, we use correlation matrix. ","48440764":"## 5.2. Hyperparameter Optimization for the Best Models\nHyperparameters are parameters whose value are set before the learning process begins. In other words, we cannot optimize hyperparameters through learning process. Hyperparameter optimization is the process of selecting the values for a model\u2019s hyperparameters that maximize the accuracy of the model. In this section, we pick 10 top models to be optimized. We use [grid-search cross-validation](https:\/\/chrisalbon.com\/machine_learning\/model_evaluation\/cross_validation_parameter_tuning_grid_search\/) method to optimize the hyperparameters. ","86602939":"<a id='section 4.12.'><\/a>","b72cda5d":"<a id='section 5.3.'><\/a>","1fccdcf8":"## 3.1. Rectifying any abnormal, inaccurate, or non-acceptable values\nWe do not want to find any abnormal, inaccurate, or non-acceptable values in our dataset because they can distort predictions and affect the accuracy, if you don\u2019t detect and handle them. An example of abnormal value is to find a passenger to be 1000 years old, which is not possible. We can use univariate distribution of each feature to identify any abnormal, inaccurate, or non-acceptable values. ","eb0ea1cb":"We do not need the original ***Title*** feature, so we will drop it.","a56995d9":"To test our assumption, let' see the ticket class (***Pclass***) distribution of *Z* (i.e., no cabin) compared to the other cabins.","43f175f1":"<a id='section 4.10.'><\/a>","98ff1200":"**Converting *Sex* feature data type**","19928004":"<a id='section 3.0.'><\/a>","17bf6b61":"## 2.1. Gathering and Importing Data\nThe train and test dataset are given in this [link](https:\/\/www.kaggle.com\/c\/titanic\/data). We import the datasets by calling the following:","f2c26fe4":"### Creating *IsAlone* feature\n***IsAlone*** feature determines whether a passenger had siblings\/spouses and\/or parents\/children. If a passenger did not have any siblings\/spouses and\/or parents\/children, ***IsAlone*** = 1. And if a passenger had any siblings\/spouses and\/or parents\/children, ***IsAlone*** = 0. ","4f69a639":"<a id='section 5.0.'><\/a>","c4c7c60e":"<a id='FamilySurvival'><\/a>","34ac0ce5":"The best performance is achieved by gradient boosting classifier with 85.63% accuracy. This is a slight improvement from 84% accuracy achieved by simple modelling. ","c29b82b3":"From the charts above, we can observe the following:\n- Passengers who were travelling alone were less likely to survive than passengers who were travelling with someone else. \n- 1st class passengers were more likely to survive than 2nd class passengers, and 2nd class passengers were more likely to survive than 3rd class passengers. This applies for both passengers who were travelling alone and with someone else. \n- Female passengers were more likely to survive than male passengers. This applies for both passengers who were travelling alone and with someone else. \n- Passengers embarked from Southampton had higher survival chance than passengers embarked from Cherbourg and Queenstown. This applies for both passengers who were travelling alone and with someone else. ","df7c7a60":"To check the result, we can check the distribution of ***Age***.","2f853505":"We visualize ***Cabin*** vs probability of survival (***Survived***) by calling the following:","dff4a8af":"## 5.3. Model Optimization with Feature Selection\nFeature selection is the process to select features that contribute the most to the output feature we are interested in. Having irrelevant features in our data can decrease the accuracy of the model and make your model learn based on irrelevant features. There are some techniques that can be used to perform feature selection. This [article](https:\/\/towardsdatascience.com\/why-how-and-when-to-apply-feature-selection-e9c69adfabf2) by Sudharsan Asaithambi discuss various types of feature selection. \n\nIn this problem, we use recursive feature elimination with cross validation (RFECV) for selecting features. A detail explanation about [RFECV](https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html) can be read here. We can see from the previous analysis that gradient boosting classifier is the best performing algorithms. Therefore, we use it for our model. ","c4350848":"The result seems a little bit odd because there are some negative values in the distribution. To check which instances have negative values, we call the following:","afa38514":"## 4.7. Number of Siblings\/Spouses (*SibSp*) EDA\nWe visualize number of siblings\/spouses vs probability of survival (***Survived***) by calling the following:","6f258efd":"Based on the chart above, we can achieve ~84% accuracy by performing simple modelling. In the next section, we will try to improve the models by tuning their hyperparameters.  ","8760a5db":"## 5.1. Simple Modelling\nWe will handpick several machine learning algorithms and create models based on them and the dataset we have. Then, cross-validation is performed to estimate the accuracy of the machine learning models. You can read more about cross-validation in this [article](https:\/\/towardsdatascience.com\/cross-validation-in-machine-learning-72924a69872f).","b65ebaea":"# Titanic Survival Prediction: End-to-End ML Workflow (Top 7%)","946eb69a":"<a id='section 4.3.'><\/a>","13e9ad93":"Let's re-visualize number of siblings\/spouses\/parents\/children vs probability of survival bar chart after binning:","2b2e4b57":"### Imputing missing values in *Cabin* feature\nPassengers who have missing values in ***Cabin*** feature are assumed to not have Cabin in the ship. So, we impute the missing values with *Z*, which indicates passengers with no cabin.  ","e0e34fbb":"From the charts above, we can observe the following:\n- Passengers embarked from Cherbourg were more likely to survive than passengers embarked from Southampton and Queenstown. \n- 1st class passengers had higher probability of survival than 2nd class passengers. This applies to passengers embarked from Southampton and Cherbourg. But, 2nd class passengers were more likely to survive than 1st class passengers for people embarked from Queenstown. \n- Female passengers were more likely to survive than male passengers. This applies to all ports of embarkation. \n- Passengers embarked from Southampton and Cherbourg were more likely to survive if they boarded the ship with someone else. ","b83f4231":"From the chart above, we can observe the following:\n- Passengers who had siblings\/spouses\/parents\/children more than four had low probability of survival","df781782":"Based on the above observations, we decide to bin values *1* to *6* together into value *1*. In other words, value *1* represents passengers that had parents\/chidren on board. ","1f81fb7d":"<a id='section 2.0.'><\/a>","1916aeb9":"In addition to family, if we examine the data closely, we will see there are groups of people with same ticket number, and they pay the same fare. This suggests group of friends are travelling together. One will think these friends will help each other and will survive or perish at the same time. We will explore this information here.","e1e4a2b5":"**Converting *Embarked* feature data type**","bd44e9b0":"<a id='section 3.1.'><\/a>","5aa7e406":"<a id='section 4.9.'><\/a>","fc99035e":"Based on the charts above, we can observe the following:\n- The number of passengers that were not survived is more than the number of passengers that were survived.\n- There were more males than females in the ship. \n- Most passengers were in the third class. \n- Most passengers embarked from Southampton. \n\nThere seems to be no abnormal values from these categorical features. Therefore, no value correction is necessary. ","98142a86":"We also round values in the ***Age*** feature because we do not need values with high amount of precision.","7daf1281":"Let's check the data type of ***Title*** feature:","bf7737d4":"From the histogram, we can observe the following:\n- The fraction of survived passengers increases as the ticket price increases. \n- The number of passengers that paid fare more than \u00a3100 are very small. \n\nBased on the above observations, we will bin the ticket price feature and store it in ***Fare_binned*** by calling the following:","6cc3ed2f":"<a id='section 4.4.'><\/a>","d889f6c7":"We can infer that majority family either all perished or all survived.\n\nTo create ***FamilySurvival*** feature, we call the following:","8a8388de":"<a id='section 4.8.'><\/a>","41c62b4f":"## 4.11. Passenger's Cabin (*Cabin*) EDA\nBefore we perform EDA on ***Cabin*** feature, we will transform it so the values represent only the first letter of the cabin name. ","27ca3344":"The following kernels are my main references in creating this notebook:\n- [A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n- [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)\n- [Titanic Top 4% with ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n- [Blood is thicker than water & friendship forever](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever)","9fc2fd47":"After we bin some title categories together, let's visualize the following:\n- Passenger's title vs probability of survival (***Survived***)\n- Passenger's title and ticket class (***Pclass***) vs probability of survival\n- Passenger's title and passenger is alone (***IsAlone***) vs probability of survival\n- Passenger's title and port of embarkation (***Embarked***) vs probability of survival","d6aef052":"## 4.1. Ticket Class (*Pclass*) EDA\nFor ticket class feature, let's visualize the following:\n- Ticket class vs probability of survival (***Survived***)\n- Ticket class and gender (***Sex***) vs probability of survival\n- Ticket class and port of embarkation (***Embarked***) vs probability of survival\n- Ticket class and passenger is alone (***IsAlone***) vs probability of survival","b29cde6c":"<a id='IsAlone'><\/a>","8c909308":"Based on the observation, ***Age***, ***Cabin***, ***Embarked***, and ***Fare*** contain some missing values.\n\nAfter we identfy features with missing values, we need to do something to handle these missing values. Alvira Swalin writes a comprehensive post about how to handle missing data in this [link](https:\/\/towardsdatascience.com\/how-to-handle-missing-data-8646b18db0d4). ","7c36123a":"We do not need ***Cabin*** feature anymore, so we can drop it.","40b7092d":"Our model yields **81.339%** accuracy and is on the **top 7%** of the total entries. Not bad for our first attempt. Any suggestions to improve our score are most welcome.","db676482":"To impute this missing value, we use the mean of ***Fare*** feature.","7d620075":"## 3.3. Handling missing data\nMost machine learning algorithms cannot handle missing values. Therefore, it is important to identify feature(s) that contain missing values in order for us to fix them before performing data modelling. ","a6cbb5ef":"From the charts above, we can observe the following:\n- Passengers with higher ticket class had higher probability of survival than passengers with lower ticket class. \n- Female passengers in the 1st and 2nd class had a considerably higher chance of survival (more than 85%) than female passengers in the 3rd class (only about 50%). \n- Male passengers in the 1st and 2nd class had a considerably higher chance of survival (~35%) than male passengers in the 2nd and 3rd class. Male passengers in the 2nd and 3rd class had only probability of survival less than 20%. \n- Passengers who embarked from Cherbourg and Queenstown were more likely to survive than passengers embarked from Southampton. \n- Passengers who boarded the ship with someone else were more likely to survive. This applies to all ticket classes. \n\nBased on these observations, it seems that the rescue was prioritized for passengers who had higher ticket class. We can infer that ***Pclass*** is an important feature to predict survival of passengers. ","2ad47908":"## 4.10. Ticket Price (*Fare*) EDA\nWe visualize histogram of ticket price with survived\/not-survived stacked for each plot by calling the following:","28a7ae7f":"## 4.8. Number of Parents\/Children (*Parch*) EDA\nWe visualize number of parents\/children vs probability of survival (***Survived***) by calling the following:","3560d3b7":"From the charts above, we can observe the following:\n- Female passengers had significantly higher chance of survival (more than 70%) than male passengers (~20%). \n- Female passengers in the 1st and 2nd class had a considerably higher chance of survival (more than 85%) than female passengers in the 3rd class (only about 50%). Meanwhile, male passengers in the 1st class had considerably higher chance of survival (about 36%) than male passengers in the 2nd and 3rd class. \n- Passengers who boarded the ship from Cherbourg had higher probability of survival than passengers who boarded the ship from Southampton and Queenstown. This applies for both male and female passengers.\n- Female passengers who boarded the ship alone had a slightly better chance of survival than female passengers who boarded the ship with someone else. But, vice versa, male passengers who boarded the ship with someone else had higher probability of survival than male passengers who boarded the ship alone. \n\nBased on these observations, it seems that the rescue was prioritized for female passengers. We can infer that ***Sex*** is an important feature to predict survival of passengers.","72303e65":"<a id='section 2.2.'><\/a>","2baa509e":"### Identifying missing data in the training and test dataset\n\nWe call the following to identify missing data in the training and test dataset:","caef6ca7":"<a id='Describing features'><\/a>","66d8f0be":"It can be observed that ***Pclass***, ***Sex***, ***SibSp+Parch***, ***FamilySurvival***, ***Title_Mr***, and ***Fare_binned*** are the most important features for our gradient boosting classifier model. Let's see how our model perform by using the selected features.","ca22262e":"## 4.12. Correlation Among Features\nLet's visualize the correlation among features by using a heatmap.","bd5a14bb":"***Fare_binned*** feature is still in a categorical data type. We need to tranform it to numerical data type. Because this feature is ordinal, the following transformations are made:\n- Fare group 0-25 is represented by value 0\n- Fare group 25-75 is represented by value 1\n- Fare group 75-513 is represented by value 2","8f5fbcc7":"## 3.5. Converting data types\nMost machine learning algorithms require all features to be converted into some numeric representations that can be understood by these algorithms. Categorical features cannot be processed directly by the algorithms and conversions on these features shall be done before subsequent data processing. Therefore, knowing the data type of all features helps us in deciding which feature(s) need conversion to other data type. We call the following to see the data type pf each feature in the dataset:","a5665f3e":"### Imputing missing values in *Embarked* feature\n\nBefore imputing missing values in ***Embarked***, we should see the missing values in the dataset.","940a8ff0":"The result of RFECV can be seen below:","48b5adc2":"Let's see the distribution of passenger's title:","10e393b6":"<a id='section 4.5.'><\/a>","b5dc670b":"<a id='section 3.2.'><\/a>","280842e4":"### Viewing a small part of the dataset ","f50d9b67":"We do not need ***Fare*** feature anymore, so we can drop it.","2df4eaeb":"<a id='section 4.11.'><\/a>","7c8d2c6b":"## 4.3. Port of Embarkation (*Embarked*) EDA\nFor port of embarkation feature, let's visualize the following:\n- Port of embarkation vs probability of survival (***Survived***)\n- Port of embarkation and ticket class (***Pclass***) vs probability of survival\n- Port of embarkation and gender (***Sex***) vs probability of survival\n- Port of embarkation and passenger is alone (***IsAlone***) vs probability of survival","c5679eab":"# 6.0. Prediction\nWe use gradient boosting classifier with hyperparameter optimization and RFECV feature selection to predict the survival of passengers in out test dataset.","48d43762":"<a id='Title'><\/a>","f164f00f":"From the chart above, we can observe the following:\n- Age group *0-5* had a high probability of survival (around 70%).\n- Age group *6-10*, *11-15*, *16-20*, *21-25*, and *26-30* had similar probability of failure.\n- Age group *31-35*, *36-40*, *41-45*, *46-50*, *51-55*, and *56-60* had similar probability of failure.\n\nBased on the observation, we will try again to segment ***Age*** feature into the following groups: *0-5*, *6-30*, *31-60*, and *61-80*. ","c9be7b38":"This suggests the fare was for a whole family, not per individual. We can use both last name and fare to group passengers into families in case different families with the same last name.\n\nTo see if the assumption that members of a family would survive or perish together, we call the following:","9702b592":"<a id='section 1.0.'><\/a>","4cf40e99":"### Univariate distributions of numerical features","b5eb9cd5":"We can observe that the training dataset has 891 entries and 12 columns while the test dataset has 418 entries and 11 columns.","91198b92":"As mentioned previously, in ***SibSp*** feature, values bigger than 2 are considered as outliers. Let's re-visualize this: ","aa0d53c1":"<a id='section 3.4.'><\/a>","8c488cd3":"## 3.4. Creating new features from existing features\n\nWe can use existing features to create new features to determine if they contribute to predict our outcome. For this dataset, we create the following features from the existing features:\n- [***Title***](#Title)\n- [***SibSp+Parch***](#SibSp+Parch)\n- [***IsAlone***](#IsAlone)\n- [***FamilySurvival***](#FamilySurvival)\n\nEach is discussed in the following:","5ebb80bf":"From the chart above, we can observe the following:\n- Passengers with title *Mme*, *Ms*, *Lady*, *Sir*, *Mlle*, *Col*, or *the Countess* had 100% chance of survival.\n- No passengers with title *Don*, *Rev*, *Capt* or *Jonkheer* had survived. \n- Passengers with title *Mrs*, *Miss*, or *Master* had higher chance of survival than passengers with title *Mr*.\n\nWe have not checked the distribution of passenger's title, so let's check it by calling the following:","7752e5d1":"## 2.2. Exploring Data Structure and Features\nBefore performing data analysis, we often need to know the structure of our data. Therefore, we perform the following:\n- [Viewing a small part of our datasets](#Viewing a small part of the dataset)\n- [Viewing data shape](#Viewing data shape)\n- [Describing the features contained in the datasets](#Describing features)","38ef88e4":"<a id='Viewing a small part of the dataset'><\/a>","e3393948":"### Univariate distributions of categorical features","a5ed191e":"It can be seen that most passengers with *Z* value were in the 3rd class.  This strengthens our previous assumption that the missing values in the ***Cabin*** feature indicates passengers with no cabin. ","4890c8b4":"Based on the correlation matrix, we can see that the ***Age*** feature has some correlations with ***SibSp*** and ***Pclass*** feature. Thus, we make ***SibSp*** and ***Pclass*** as the predictors to predict the missing values in ***Age*** feature.\n\nInstances with complete data are used for training the linear regression model. The generated model is then used to predict missing values for incomplete instances.","6c09c41f":"## 4.9. Number of  Siblings\/Spouses\/Parents\/Children (*SibSp+Parch*) EDA\nWe visualize number of siblings\/spouses\/parents\/children vs probability of survival (***Survived***) by calling the following:","4c80d480":"There seems to be a recognizable pattern and correlation between the age group and the probability of survival. Thus, we are going to use this grouping for the subsequent analysis. \n\nLet's check the data type of ***Age_binned*** feature:","2d13df37":"<a id='section 3.6.'><\/a>","06df0e76":"*Mr*, *Miss*, and *Mrs* were the most common passenger's title while other titles have a very rare occurrence. Directly using these features can cause a lot of issues and adversely affect the analysis and machine learning models. \n\nBased on the above observations, we decide to bin title categories with count less than 10 into a new category named *Other*.  ","ecb104e6":"Based on the charts above, we can observe the following:\n- In ***SibSp*** feature, values bigger than 2 are considered as outliers.\n- In ***Parch*** feature, values bigger than 0 are considered as outliers.\n- In ***Age*** feature, values bigger than 64 are considered as outliers.\n- In ***Fare*** feature, values bigger than 70 are considered as outliers.\n\nWe are still not sure what to do with these outliers. Thus, we will perform [exploratory analysis](#section 4.0.) before deciding what to do with the identified outliers.  ","9a1bca82":"### Creating *FamilySurvival* feature\n\nNote: this part is taken from a kernel created by [S.Xu](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever). \n\n***FamilySurvival*** feature indicates if the passenger had any family members that are survived\/not-survived. The assumption is that members of a family would survive or perish together. \n\nFirst, we use passenger's last name to divide the passengers into families. Thus, we create a feature named ***Last_Name*** by calling the following:","9ed2f7e4":"<a id='section 2.1.'><\/a>","4ee08527":"Only a small number of passengers has more than two siblings\/spouses. Directly using these features can cause a lot of issues and adversely affect the analysis and machine learning models. \n\nBased on the above observations, we decide to bin values *3*, *4*, *5*, and *8* together into value *3*. In other words, value *3* represents number of siblings\/spouses of more than two. ","fba49d35":"We will also fix the ***Fare*** feature, so it will reflect the price paid by a passenger. We call the following:","fd477f6a":"<a id='Imputing missing values in Cabin feature'><\/a>","480cbe34":"We decide to impute missing entries in ***Cabin***, ***Age*** and ***Embarked*** feature. ","0c9efd78":"Based on the charts above, we can observe the following:\n- The distribution of ***SibSp*** feature is skewed with most passengers did not have any siblings or spouses aboard the Titanic. Only a few passengers had siblings\/spouses more than one. The value range is still acceptable and there seemes to be no abnormal, inaccurate, or non-acceptable values. \n- The distribution of ***Parch*** feature is skewed with most passengers did not have any parents or childrens aboard the Titanic. Only a few passengers had parents\/children more than one. The value range is still acceptable and there seemes to be no abnormal, inaccurate, or non-acceptable values. \n- The distribution of ***Age*** resembles a normal distribution. Most passengers were between 20 and 40 years old. The maximum age of a passenger is 80 years old. The value range is still acceptable and there seemes to be no abnormal, inaccurate, or non-acceptable values. \n- The distribution of ***Fare*** feature is skewed with most passengers paid fare between \u00a30 to \u00a3100. The maximum fare paid by a passenger was \u00a3512. It is stated in this [article](https:\/\/www.quora.com\/What-were-the-ticket-prices-to-board-the-Titanic) that the cost of tickets to board the Titanic were:\n    - First Class (parlor suite) \u2014 \u00a3870\n    - First Class (berth)\u2014 \u00a330\n    - Second Class \u2014 \u00a312\n    - Third Class \u2014 \u00a33 to \u00a38\n- Based on the above information, the value range of ***Fare*** feature is still acceptable.\n\nBased on these observations, there seems to be no abnormal values from these numerical features. Therefore, no value correction is necessary.","af308b99":"<a id='section 6.0.'><\/a>","fe401254":"<a id='section 4.7.'><\/a>","cf011e0d":"# Table of Contents\n- [**1.0. Problem Definition**](#section 1.0.)\n- [**2.0. Data Gathering and Import**](#section 2.0.)\n    - [2.1. Gathering and Importing Data](#section 2.1.)\n    - [2.2. Exploring Data Structure and Features](#section 2.2.) \n\n\n- [**3.0. Data Wrangling**](#section 3.0.)\n  - [3.1. Rectifying any abnormal, inaccurate, or non-acceptable values](#section 3.1.)\n  - [3.2. Handling outliers](#section 3.2.)\n  - [3.3. Handling missing data](#section 3.3.)\n  - [3.4. Creating new features from existing features (feature engineering)](#section 3.4.)\n  - [3.5. Converting data types](#section 3.5.)\n  - [3.6. Removing features that do not contribute to the analysis](#section 3.6.)\n\n\n- [**4.0. Exploratory Data Analysis**](#section 4.0.)\n  - [4.1. Ticket Class (***Pclass***) EDA](#section 4.1.)\n  - [4.2. Gender (***Sex***) EDA](#section 4.2.)\n  - [4.3. Port of Embarkation (***Embarked***) EDA](#section 4.3.)\n  - [4.4. Passenger Is Alone (***IsAlone***) EDA](#section 4.4.)\n  - [4.5. Passenger's Title (***Title***) EDA](#section 4.5.)\n  - [4.6. Passenger's Age (***Age***) EDA](#section 4.6.)\n  - [4.7. Number of Siblings\/Spouses (***SibSp***) EDA](#section 4.7.)\n  - [4.8. Number of Parents\/Children (***Parch***) EDA](#section 4.8.)\n  - [4.9. Number of  Siblings\/Spouses\/Parents\/Children (***SibSp+Parch***) EDA](#section 4.9.)\n  - [4.10.Ticket Price (***Fare***) EDA](#section 4.10.)\n  - [4.11.Passenger's Cabin (***Cabin***) EDA](#section 4.11.)\n\n\n- [**5.0. Data Modelling**](#section 5.0.)\n\n\n- [**6.0. Prediction**](#section 6.0.)\n\n\n- [**References**](#References)","5a65620e":"Out of curiousity, I searched about Mrs. George Nelson (Martha Evelyn) and found this in this [link](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html):\n\n*\"Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard.\"*\n\nSo, we can impute the missing values as *S*.","3fbb082f":"<a id='section 4.0.'><\/a>","6f4b56b8":"<a id='SibSp+Parch'><\/a>","d27f75b2":"Let's visualize ***HaveCabin*** vs probability of survival:","002f286c":"<a id='section 5.1.'><\/a>","56bd4283":"We will overload the ***Family survival*** feature instead of creating a separate feature.","22f6da16":"We can observe that *Mr*, *Miss*, and *Mrs* were the most common passenger's title while other titles have a very rare occurrence. Directly using these features can cause a lot of issues and adversely affect the analysis and machine learning models. Thus, titles with rare occurrences, such as *Capt*, *Lady*, and *Sir*, should be binned together. We will perform [exploratory data analysis](#section 4.0.) before deciding whether to bin rare titles together or not.","122d1899":"<a id='Viewing data shape'><\/a>","c2535fdf":"### Imputing missing values in ***Fare*** feature\nBefore imputing missing values in ***Fare*** feature, we should see the missing value in the dataset.","4aabc057":"We can observe that seven features have integer data type, i.e., ***Survived***, ***Pclass***, ***Age***, ***SibSp***, ***Parch***, ***SibSp+Parch***, and ***IsAlone***. Two features have float data type, i.e., ***FamilySurvival*** and ***Fare***. And six features have object (string) data type, i.e., ***Name***, ***Sex***, ***Ticket***, ***Cabin***, ***Embarked***, and ***Title***.\n\nBased on these observations, we need to convert features that have object (string) data type into numerical data type. \n- ***Name*** and ***Ticket*** feature will be removed because they will not be used anymore in the subsequent analysis.\n- For ***Sex*** feature, we will replace the values with binary numbers. *male* is replaced by 1 and *female* by 0. \n- ***Embarked*** feature is considered as a ordinal categorical feature because Titanic embarked these three ports in a sequence. It went to Southampton, then Cherbourg, and finally Queenstown. Thus, *S* (representing Southampton) is replaced by 0, *C* (representing Cherbourg) by 1, and *Q* (representing Queenstown) by 2. \n- For ***Title*** and ***Cabin*** feature, the conversion is a bit complex because they comprise of several categories and have no clear ordering of categories. We will think about how to convert these features after performing [exploratory data analysis](#section 4.0.).","9c0e80ec":"From the chart above, we can observe the following:\n- All passengers who had more four and five parents\/children were not survived.\n- There is no apparent trend in the bar plot above. \n\nAs mentioned previously, in ***Parch*** feature, values bigger than 0 are considered as outliers. Let's re-visualize this: ","c1bf0ca5":"### Creating *SibSp+Parch* feature\nWe add the number of siblings\/spouses (***SibSp***) and the number of parents\/children (***Parch***) and make it a feature named ***SibSp+Parch***.  ","cfc92937":"## 3.6. Removing features that do not contribute to the analysis\n***Name*** and ***Ticket*** feature will be removed because they will not be used anymore in the subsequent analysis. Thus, we drop them for our datasets by calling the following:","07ff73b1":"### Describing features\nEach column gives certain information about a Titanic passenger feature. The description about each feature is given in this [link](https:\/\/www.kaggle.com\/c\/titanic\/data). ","29a82af3":"### Creating ***Title*** feature\nWe can extract the title of passengers (e.g., Mr., Miss, Mrs., etc.) from the ***Name*** features and make it a feature named ***Title***. To do that, we call the following:","18ccb6b9":"# 2.0. Data Gathering and Import\nIn this section, we perform the following:\n- [2.1. Gathering and Importing Data](#section 2.1.)\n- [2.2. Exploring Data Structure and Features](#section 2.2.) ","b2ab76d4":"From the charts above, we can observe the following:\n- Passengers with title *Mrs* had the highest survival chance while passengers with title *Mr* had the lowest survival chance. \n- Among passengers with title *Mr*, 1st class passengers had higher chance of survival than 2nd and 3rd class passengers. Among passengers with title *Mrs*, *Miss*, and *Master*, 1st class and 2nd class passengers had similar chance of survival. They were more likely to survive than 3rd class passengers. Among passenger with *Other* title, 1st class passengers had significantly higher likelihood to survive than 2nd class passengers. \n- Passengers with title *Master* had 100% chance of survival if they boarded 1st and 2nd class. "}}