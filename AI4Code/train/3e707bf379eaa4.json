{"cell_type":{"ac52064d":"code","a07eb31c":"code","bd13c16e":"code","7117183e":"code","a40f3d02":"code","5da8fb47":"code","45899ebe":"code","f7855317":"code","4484d058":"code","ea5bef77":"code","2c7dcc38":"code","436a93af":"code","0f7a9d17":"code","11d8c996":"code","f67a7eed":"code","a2df4758":"code","bc4ac267":"code","9628e6ea":"code","764db551":"code","6745fe13":"code","f3469149":"code","c55f6e54":"code","8bab8441":"code","b7584792":"code","67824b8a":"code","852bb6ab":"code","4928505e":"code","bed5074d":"code","247f81dd":"code","fb5c41c2":"code","a70707a4":"code","7e22edc6":"code","36254b6e":"code","65b4fe5d":"code","5df96c00":"code","75f0914b":"code","f8d375b6":"code","b0c5be57":"markdown","8bcd0e8b":"markdown","9dde913b":"markdown","29b309f2":"markdown","8d27dd53":"markdown","46318f0f":"markdown","50ea4468":"markdown","60774541":"markdown","d59b592e":"markdown","fbd536d7":"markdown","a962638e":"markdown","99e51820":"markdown","179583ed":"markdown","9a287c85":"markdown","3cf9c2df":"markdown","1cc524f9":"markdown","01a4f991":"markdown"},"source":{"ac52064d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a07eb31c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","bd13c16e":"house = pd.read_csv('\/kaggle\/input\/delhi-house-price-prediction\/MagicBricks.csv')\nhouse.head()","7117183e":"house.shape","a40f3d02":"house.info()","5da8fb47":"house.isnull().sum()","45899ebe":"house.describe()","f7855317":"# area count\nplt.ylabel('Count of records')\nsns.distplot(house['Area'], bins=25, kde=False)\nplt.show()","4484d058":"# plotting price column as log\nsns.distplot(np.log(house['Price']), bins=30)\nplt.show()","ea5bef77":"# plotting frequency graphs for categorical variables\nplt.figure(figsize=(18,12))\nplt.subplot(3,3,1)\nsns.countplot(house['BHK'])\nplt.subplot(3,3,2)\nsns.countplot(house['Bathroom'])\nplt.subplot(3,3,3)\nsns.countplot(house['Parking'])\nplt.subplot(3,3,4)\nsns.countplot(house['Status'])\nplt.subplot(3,3,5)\nsns.countplot(house['Type'])\nplt.subplot(3,3,6)\nsns.countplot(house['Furnishing'])\nplt.subplot(3,3,7)\nsns.countplot(house['Transaction'])\nplt.show()","2c7dcc38":"# plotting barplot for all variables against target variable\nplt.figure(figsize=(15,12))\nplt.subplot(3,3,1)\nsns.barplot(x='BHK', y='Price', data=house, estimator=np.median)\nplt.subplot(3,3,2)\nsns.barplot(x='Bathroom', y='Price', data=house, estimator=np.median)\nplt.subplot(3,3,3)\nsns.barplot(x='Parking', y='Price', data=house, estimator=np.median)\nplt.subplot(3,3,4)\nsns.barplot(x='Furnishing',y='Price', data=house, estimator=np.median)\nplt.subplot(3,3,5)\nsns.barplot(x='Transaction',y='Price', data=house, estimator=np.median)\nplt.subplot(3,3,6)\nsns.barplot(x='Status',y='Price', data=house, estimator=np.median)\nplt.subplot(3,3,7)\nsns.barplot(x='Type',y='Price', data=house, estimator=np.median)\nplt.show()","436a93af":"# plotting area against target variable\nsns.scatterplot(x='Area',y='Price',data=house)\nplt.show()","0f7a9d17":"# plotting boxplot for each variables\nplt.figure(figsize=(18,12))\nplt.subplot(3,3,1)\nsns.boxplot(house['BHK'])\nplt.subplot(3,3,2)\nsns.boxplot(house['Bathroom'])\nplt.subplot(3,3,3)\nsns.boxplot(house['Parking'])\nplt.subplot(3,3,4)\nsns.boxplot(house['Area'])\nplt.subplot(3,3,5)\nsns.boxplot(house['Per_Sqft'])\nplt.subplot(3,3,6)\nsns.boxplot(house['Price'])\nplt.show()","11d8c996":"# reduced dataset by removing insignificant observations to get rid of outliers\ndf = house[(house['BHK']<7)&(house['Bathroom']<6)&(house['Parking']<9)&(house['Area']<10000)]\nprint('Previous dataset shape:', house.shape)\nprint('New dataset shape:',df.shape)","f67a7eed":"df.isnull().sum()","a2df4758":"df['Per_Sqft'] = df['Per_Sqft'].fillna(df['Price']\/df['Area'])","bc4ac267":"df = df[~(df['Furnishing'].isnull())]","9628e6ea":"df.isnull().sum()","764db551":"# dropping locality variable\ndf = df.drop('Locality', axis=1)\n\n# checking shape of final dataset\ndf.shape","6745fe13":"df.info()","f3469149":"cat_cols = ['BHK','Bathroom','Furnishing','Parking','Status','Transaction','Type']\nnum_cols = ['Area','Price','Per_Sqft']","c55f6e54":"# convert data types of categorical variable as string\nfor cols in cat_cols:\n    df[cols] = df[cols].astype(str)","8bab8441":"# creating dummy variables\ndf = pd.get_dummies(data=df, drop_first=True)\ndf.head()","b7584792":"sns.distplot(df['Price'])","67824b8a":"# converting target variable into log\ndf['Price'] = np.log(df['Price'])\n\n# plotting target variable dist plot post log transformation\nsns.distplot(df['Price'])\nplt.show()","852bb6ab":"# separating dataset into X and y\nX = df.drop('Price', axis=1)\ny = df['Price']","4928505e":"# dividing data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","bed5074d":"# scaling data\nfrom sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\nX_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)\n\nprint('Shape of X_train:', X_train.shape)\nprint('Shape of X_test:',X_test.shape)","247f81dd":"# importing linear regression package\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n# fitting train data\nlr.fit(X_train, y_train)\n\n#predicting train data\npred_train_lr = lr.predict(X_train)\n\n# predicting test data\npred_test_lr = lr.predict(X_test)\n\n# importing evaluation metrices\nfrom sklearn.metrics import r2_score, mean_squared_error\nprint('Train data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_train_lr, y_train)))\nprint('R-squared:',r2_score(pred_train_lr,y_train))\nprint('\\n')\nprint('Test data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_test_lr, y_test)))\nprint('R-squared:',r2_score(pred_test_lr,y_test))","fb5c41c2":"# plotting result for visual\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nplt.title('Train data prediction')\nsns.scatterplot(x=y_train,y=pred_train_lr)\nplt.subplot(2,2,2)\nplt.title('Test data prediction')\nsns.scatterplot(x=y_test,y=pred_test_lr)\nplt.show()","a70707a4":"# importing random forest regressor package\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\n\n# fitting training data\nrf.fit(X_train,y_train)\n\n# predicting train data\npred_train_rf = rf.predict(X_train)\n\n#predicting test data\npred_test_rf = rf.predict(X_test)\n\n# evaluating the model\nprint('Train data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_train_rf, y_train)))\nprint('R-squared:',r2_score(pred_train_rf,y_train))\nprint('\\n')\nprint('Test data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_test_rf, y_test)))\nprint('R-squared:',r2_score(pred_test_rf,y_test))","7e22edc6":"# plotting results for visual\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nplt.title('Train data prediction')\nsns.scatterplot(x=y_train,y=pred_train_rf)\nplt.subplot(2,2,2)\nplt.title('Test data prediction')\nsns.scatterplot(x=y_test,y=pred_test_rf)\nplt.show()","36254b6e":"# import gradient boosting regressor package\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbr = GradientBoostingRegressor()\n\n# fitting training data\ngbr.fit(X_train, y_train)\n\n# predicting train data\npred_train_gbr = gbr.predict(X_train)\n\n# predicting test data \npred_test_gbr = gbr.predict(X_test)\n\n# evaluating the model\nprint('Train data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_train_gbr, y_train)))\nprint('R-squared:',r2_score(pred_train_gbr,y_train))\nprint('\\n')\nprint('Test data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_test_gbr, y_test)))\nprint('R-squared:',r2_score(pred_test_gbr,y_test))","65b4fe5d":"# plotting results for visual\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nplt.title('Train data prediction')\nsns.scatterplot(x=y_train,y=pred_train_gbr)\nplt.subplot(2,2,2)\nplt.title('Test data prediction')\nsns.scatterplot(x=y_test,y=pred_test_gbr)\nplt.show()","5df96c00":"# importing xtreme boosting regressor package\nfrom xgboost import XGBRegressor\nxgb = XGBRegressor()\n\n# fitting training data\nxgb.fit(X_train,y_train)\n\n# predicting train data\npred_train_xgb = xgb.predict(X_train)\n\n# predicting test data\npred_test_xgb = xgb.predict(X_test)\n\n# evaluating the model \nprint('Train data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_train_xgb, y_train)))\nprint('R-squared:',r2_score(pred_train_xgb,y_train))\nprint('\\n')\nprint('Test data results')\nprint('RMSE:', np.sqrt(mean_squared_error(pred_test_xgb, y_test)))\nprint('R-squared:',r2_score(pred_test_xgb,y_test))","75f0914b":"# plotting results for visual\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nplt.title('Train data prediction')\nsns.scatterplot(x=y_train,y=pred_train_xgb)\nplt.subplot(2,2,2)\nplt.title('Test data prediction')\nsns.scatterplot(x=y_test,y=pred_test_xgb)\nplt.show()","f8d375b6":"data = [['Linear_Regression',r2_score(pred_train_lr,y_train),r2_score(pred_test_lr,y_test)],\n        ['Random_Forest',r2_score(pred_train_rf,y_train),r2_score(pred_test_rf,y_test)],\n        ['Gradient_Boosting_Regressor',r2_score(pred_train_gbr,y_train),r2_score(pred_test_gbr,y_test)],\n        ['XGBoosting_Regressor',r2_score(pred_train_xgb,y_train),r2_score(pred_test_xgb,y_test)]]\nresult = pd.DataFrame(data, columns=['algo','train_score','test_score'])\nresult","b0c5be57":"### *Xtreme Gradient Boosting Regressor* ","8bcd0e8b":"### *Linear Regression*","9dde913b":"**Univariate Analysis**","29b309f2":"### *Gradient Boosting Regressor*","8d27dd53":"**Understanding data**","46318f0f":"> ### Feature Engineering ","50ea4468":"***Handling missing values***","60774541":"### Model Building","d59b592e":"### EDA","fbd536d7":"**Bi-variate Analysis**","a962638e":"Now there is no missing value in the dataset.","99e51820":"### *Random Forest Regressor* ","179583ed":"**Data Pre-processing**","9a287c85":"****Seems Gradient Boosting Regressor is the best algo out of the all algo used without parameter tuning****","3cf9c2df":"Importing basic libraries","1cc524f9":"Seems there are outliers","01a4f991":"**Reading dataset**"}}