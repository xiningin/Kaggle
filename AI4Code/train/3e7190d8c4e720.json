{"cell_type":{"173fff5c":"code","e34f3b79":"code","56e88bdc":"code","1c022c8f":"code","46313349":"code","ae4deb63":"code","4589822e":"code","5b177aa1":"code","f3c7fb1c":"code","cb1fe0ba":"code","aeea2225":"code","490fce49":"code","700b5994":"code","3afc46a7":"code","f46fcd96":"code","2b735fd4":"code","2567ca41":"code","f9225084":"code","e79041fb":"code","2e6785c4":"code","69bd26c8":"code","dcc23f06":"code","b8924bc8":"code","6db6cf71":"code","d336de9e":"code","67eb80f2":"code","f0a37dc4":"code","a3644392":"markdown","7eb0a30c":"markdown","724c43b0":"markdown","113539a9":"markdown","7349dcc9":"markdown","10bde2e8":"markdown","fb6dacd8":"markdown","aa3ba4ab":"markdown","433bc374":"markdown","63aff7dc":"markdown","8a7fe989":"markdown","9fad6e52":"markdown"},"source":{"173fff5c":"import numpy as np \nimport pandas as pd \nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nimport category_encoders as ce\nimport matplotlib.pyplot as plt\n\nSEED = 19\nnp.random.seed(SEED)","e34f3b79":"train = pd.read_csv(\"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train.csv\")\ntrain.columns = [col.lower() for col in train.columns]\ntrain.info()","56e88bdc":"test = pd.read_csv(\"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/test.csv\")\ntest.columns = [col.lower() for col in test.columns]\ntest.info()","1c022c8f":"submission = pd.read_csv(\"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv\")\nsubmission.columns = [col.lower() for col in submission.columns]\nsubmission.info()","46313349":"train.sort_values([\"patient\",\"weeks\"], inplace=True)\ntrain[\"base_week\"] = train.groupby(\"patient\")[\"weeks\"].transform(lambda x: x.iloc[0])\ntrain[\"base_fvc\"] = train.groupby(\"patient\")[\"fvc\"].transform(lambda x: x.iloc[0])\ntrain[\"base_percent\"] = train.groupby(\"patient\")[\"percent\"].transform(lambda x: x.iloc[0])\ntrain","ae4deb63":"train.info()","4589822e":"input_features = [\"weeks\", \"age\", \"sex\", \"smokingstatus\", \"base_week\", \"base_fvc\", \"base_percent\"]\ncategorical_features = [\"sex\",\"smokingstatus\"]\n\ngroup_col = \"patient\"\nn_folds = 10\ntarget = \"fvc\"\n\n# left and right quantiles used to estimate confidence\nalpha_qleft = 0.2\nalpha_qright = 0.8","5b177aa1":"encoder = ce.OrdinalEncoder(cols=categorical_features, handle_unknown='impute')\nencoder.fit(train.loc[:, categorical_features])\ntrain.loc[:, categorical_features] = encoder.transform(train.loc[:, categorical_features])","f3c7fb1c":"# model hyperparams\nmodel_params = {\n    'objective':'quantile',\n    'metric':'quantile',\n    'max_bin': 127,\n    'num_leaves': 7,\n    'min_data_in_leaf': 15,\n    'learning_rate': 0.025,\n    'feature_fraction':0.8,\n    'bagging_fraction':0.8,\n    'bagging_freq':1,\n    'seed':SEED,\n}","cb1fe0ba":"# columns where oof predictions will be saved\ntrain[\"pred_qleft\"] = None\ntrain[\"pred_expected\"] = None\ntrain[\"pred_qright\"] = None","aeea2225":"model_params[\"alpha\"] = alpha_qleft\ngkf = GroupKFold(n_splits=n_folds)\nall_models_qleft = list()\n\nfor fold,(train_idx, valid_idx) in enumerate(gkf.split(train, train[target], train[group_col])):\n    \n    train_data = train.loc[train_idx, :]\n    valid_data = train.loc[valid_idx, :]\n    \n    train_df_kwargs = {\n        \"data\":train_data.loc[:, input_features],\n        \"categorical_feature\":categorical_features,\n        \"free_raw_data\":False,\n        \"label\":train_data.loc[:, target].values\n    }\n    _train_data = lgb.Dataset(**train_df_kwargs)\n\n    valid_df_kwargs = {\n        \"data\":valid_data.loc[:, input_features],\n        \"categorical_feature\":categorical_features,\n        \"free_raw_data\":False,\n        \"label\":valid_data.loc[:, target].values\n    }\n    _valid_data = lgb.Dataset(**valid_df_kwargs)\n    \n    training_kwargs = {\n        \"train_set\": _train_data,\n        \"valid_sets\": _valid_data,\n        \"early_stopping_rounds\": 250,\n        \"num_boost_round\": 1000,\n        \"params\": model_params,\n        \"verbose_eval\":50,\n    }\n    model = lgb.train(**training_kwargs)\n    all_models_qleft.append(model)\n    \n    ## oof predictions\n    train.loc[valid_idx,\"pred_qleft\"] = model.predict(train.loc[valid_idx, input_features])","490fce49":"lgb.plot_importance(all_models_qleft[0], importance_type='gain', figsize=(10,8))\nplt.show()\n\nlgb.plot_importance(all_models_qleft[0], importance_type='split', figsize=(10,8))\nplt.show()","700b5994":"model_params[\"alpha\"] = 0.5\ngkf = GroupKFold(n_splits=n_folds)\nall_models_expected = list()\n\nfor fold,(train_idx, valid_idx) in enumerate(gkf.split(train, train[target], train[group_col])):\n    \n    train_data = train.loc[train_idx, :]\n    valid_data = train.loc[valid_idx, :]\n    \n    train_df_kwargs = {\n        \"data\":train_data.loc[:, input_features],\n        \"categorical_feature\":categorical_features,\n        \"free_raw_data\":False,\n        \"label\":train_data.loc[:, target].values\n    }\n    _train_data = lgb.Dataset(**train_df_kwargs)\n\n    valid_df_kwargs = {\n        \"data\":valid_data.loc[:, input_features],\n        \"categorical_feature\":categorical_features,\n        \"free_raw_data\":False,\n        \"label\":valid_data.loc[:, target].values\n    }\n    _valid_data = lgb.Dataset(**valid_df_kwargs)\n    \n    training_kwargs = {\n        \"train_set\": _train_data,\n        \"valid_sets\": _valid_data,\n        \"early_stopping_rounds\": 250,\n        \"num_boost_round\": 1000,\n        \"params\": model_params,\n        \"verbose_eval\":50,\n    }\n    model = lgb.train(**training_kwargs)\n    all_models_expected.append(model)\n\n    ## oof predictions\n    train.loc[valid_idx,\"pred_expected\"] = model.predict(train.loc[valid_idx, input_features])","3afc46a7":"lgb.plot_importance(all_models_expected[0], importance_type='gain', figsize=(10,8))\nplt.show()\n\nlgb.plot_importance(all_models_expected[0], importance_type='split', figsize=(10,8))\nplt.show()","f46fcd96":"model_params[\"alpha\"] = alpha_qright\ngkf = GroupKFold(n_splits=n_folds)\nall_models_qright = list()\n\nfor fold,(train_idx, valid_idx) in enumerate(gkf.split(train, train[target], train[group_col])):\n    \n    train_data = train.loc[train_idx, :]\n    valid_data = train.loc[valid_idx, :]\n    \n    train_df_kwargs = {\n        \"data\":train_data.loc[:, input_features],\n        \"categorical_feature\":categorical_features,\n        \"free_raw_data\":False,\n        \"label\":train_data.loc[:, target].values\n    }\n    _train_data = lgb.Dataset(**train_df_kwargs)\n\n    valid_df_kwargs = {\n        \"data\":valid_data.loc[:, input_features],\n        \"categorical_feature\":categorical_features,\n        \"free_raw_data\":False,\n        \"label\":valid_data.loc[:, target].values\n    }\n    _valid_data = lgb.Dataset(**valid_df_kwargs)\n    \n    training_kwargs = {\n        \"train_set\": _train_data,\n        \"valid_sets\": _valid_data,\n        \"early_stopping_rounds\": 250,\n        \"num_boost_round\": 1000,\n        \"params\": model_params,\n        \"verbose_eval\":50,\n    }\n    model = lgb.train(**training_kwargs)\n    all_models_qright.append(model)\n\n    ## oof predictions\n    train.loc[valid_idx,\"pred_qright\"] = model.predict(train.loc[valid_idx, input_features])","2b735fd4":"lgb.plot_importance(all_models_qright[0], importance_type='gain', figsize=(10,8))\nplt.show()\n\nlgb.plot_importance(all_models_qright[0], importance_type='split', figsize=(10,8))\nplt.show()","2567ca41":"def error_metric(ytrue, ypred, confidence):\n    sig_clipped = (np.clip(confidence, a_min=70., a_max=None)).astype(float)\n    delta = (np.clip(np.abs(ytrue - ypred), a_min=None, a_max=1000)).astype(float)\n    metric = -(np.sqrt(2)*delta \/ sig_clipped) - np.log(np.sqrt(2)*sig_clipped)\n    return np.mean(metric)","f9225084":"ytrue = train.fvc.values\nypred = train.pred_expected.values\nconfidence = train.pred_qright.values - train.pred_qleft.values\nprint(f\"CV error: {error_metric(ytrue, ypred, confidence)}\")","e79041fb":"test.rename({\"weeks\":\"base_week\", \"fvc\":\"base_fvc\", \"percent\":\"base_percent\"}, axis=1, inplace=True)\ntest.loc[:, categorical_features] = encoder.transform(test.loc[:, categorical_features])\ntest","2e6785c4":"submission[\"patient\"] = submission.patient_week.apply(lambda x: x.split(\"_\")[0])\nsubmission[\"weeks\"] = submission.patient_week.apply(lambda x: int(x.split(\"_\")[1]))\nsubmission","69bd26c8":"predict_dataframe = (submission\n                     .merge(test, how=\"left\", on=[\"patient\"])\n                     .loc[:, input_features])\npredict_dataframe","dcc23f06":"predictions_qleft = list()\nfor model in all_models_qleft:\n    _pred = model.predict(predict_dataframe)\n    predictions_qleft.append(_pred)\n    \npred_qleft = np.mean(predictions_qleft, axis=0)","b8924bc8":"predictions_expected = list()\nfor model in all_models_expected:\n    _pred = model.predict(predict_dataframe)\n    predictions_expected.append(_pred)\n    \npred_expected = np.mean(predictions_expected, axis=0)","6db6cf71":"predictions_qright = list()\nfor model in all_models_qright:\n    _pred = model.predict(predict_dataframe)\n    predictions_qright.append(_pred)\n    \npred_qright = np.mean(predictions_qright, axis=0)","d336de9e":"submission = pd.read_csv(\"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/sample_submission.csv\")\nsubmission.loc[:, \"FVC\"] = pred_expected\nsubmission.loc[:, \"Confidence\"] = pred_qright - pred_qleft\nsubmission.to_csv(\"submission.csv\", index=False)","67eb80f2":"submission.FVC.describe()","f0a37dc4":"submission.Confidence.describe()","a3644392":"***\n## Loading data","7eb0a30c":"***","724c43b0":"***\n## Minimal features","113539a9":"### Model training: right quantile","7349dcc9":"***\n## Inference: prepare predict data","10bde2e8":"***\n## Summary\n\nThis notebook implements quantile regression with LightGBM using only tabular data (no images). \n\n* Because LightGBM is not able to predict more than a value per model, three different models are trained for each quantile.\n* The 0.5 quantile is used for the expected value.\n* The difference between the quantiles 0.8 and 0.2 are used to estimate the confidence. \n* Training is done with group 10-fold cross validation.","fb6dacd8":"***\n### Model training: expected value","aa3ba4ab":"***\n## Inference: predict","433bc374":"***\n## submission","63aff7dc":"***\n### Model training: cross-validation error","8a7fe989":"***\n## Model training: for each quantile","9fad6e52":"### Model training: left quantile"}}