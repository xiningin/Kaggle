{"cell_type":{"564174a6":"code","f78f1641":"code","f9e4223c":"code","a569d44d":"code","09668089":"code","a3f0f838":"code","405334c3":"code","3be688db":"code","1e860864":"code","15c69d85":"code","f3bb33f4":"code","5bd357a4":"code","a7ffcb33":"code","bc796410":"code","97070c2c":"code","f300c34b":"code","31bda496":"code","6571b65d":"code","14702492":"code","dbdaa419":"code","b717557a":"code","8c362f86":"code","dbb471d6":"code","31eaa7a9":"code","f00a2baf":"code","89aaf0bd":"code","68bce4b7":"markdown","b6f59f93":"markdown","cc3a43fc":"markdown","d9f56b92":"markdown","1a512bc0":"markdown","0655cdd6":"markdown","50b928f9":"markdown","d6efacf0":"markdown","69c31e9c":"markdown","e23e6ed6":"markdown","0c71ead0":"markdown","ccf1c70d":"markdown","05b4f06a":"markdown","445e7912":"markdown","e4810b18":"markdown","b69afea2":"markdown","4afa1035":"markdown"},"source":{"564174a6":"! pip install --quiet chart-studio","f78f1641":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport re\nfrom tqdm.notebook import tqdm\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\nimport torch\nimport fastai\nfrom fastai import *\nfrom fastai.text import *\n\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nfrom wordcloud import WordCloud\nfrom plotly.offline import iplot\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nplt.style.use('fivethirtyeight')\ntorch.device(0)","f9e4223c":"train_data = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding='latin-1')\ntest_data = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\", encoding='latin-1')","a569d44d":"train_data.head()","09668089":"test_data.head()","a3f0f838":"data = pd.concat([train_data, test_data])\ndata.shape","405334c3":"data['Sentiment'] = data['Sentiment'].map({'Extremely Positive':'Positive', 'Extremely Negative':'Negative', 'Negative':'Negative', 'Positive':'Positive', 'Neutral':'Neutral'})\ntrain_data = data[['OriginalTweet', 'Sentiment']]","3be688db":"train_data.describe()","1e860864":"vals = [len(train_data[train_data['Sentiment']=='Negative']['Sentiment']), len(train_data[train_data['Sentiment']=='Positive']['Sentiment']), len(train_data[train_data['Sentiment']=='Neutral']['Sentiment'])]\nidx = ['Negative', 'Positive', 'Neutral']\nfig = px.pie(\n    train_data,\n    names='Sentiment',\n    title='Target Value Distribution Chart',\n    color_discrete_sequence=px.colors.sequential.Agsunset\n)\niplot(fig)","15c69d85":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.len()\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.len()\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.len()\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\n\nfig.update_layout(title_text=\"Character Count\")\niplot(fig)","f3bb33f4":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.split().map(lambda x: len(x))\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.split().map(lambda x: len(x))\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.split().map(lambda x: len(x))\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.update_layout(title_text=\"Word Count\")\niplot(fig)","5bd357a4":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].apply(lambda x: len(set(str(x).split()))).to_list()\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].apply(lambda x: len(set(str(x).split()))).to_list()\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].apply(lambda x: len(set(str(x).split()))).to_list()\n\nfig = ff.create_distplot([neg, pos, neu], ['Negative', 'Positive', 'Neutral'])\nfig.update_layout(title_text=\"Unique Word Count Distribution\")\niplot(fig)","a7ffcb33":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.split().map(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w or 'ftp' in w]))\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.update_layout(title_text=\"URL Count\")\niplot(fig)","bc796410":"negative = \" \".join(train_data[train_data['Sentiment'] == 'Negative']['OriginalTweet'].to_list())\npositive = \" \".join(train_data[train_data['Sentiment'] == 'Positive']['OriginalTweet'].to_list())\nneutral = \" \".join(train_data[train_data['Sentiment'] == 'Neutral']['OriginalTweet'].to_list())\n\nfig, ax = plt.subplots(1, 3, figsize=(15,15))\nng_wlc = WordCloud(width=256, height=256, collocations=False).generate(negative)\nps_wlc = WordCloud(width=256, height=256, collocations=False).generate(positive)\nne_wlc = WordCloud(width=256, height=256, collocations=False).generate(neutral)\nwcs = [ng_wlc, ps_wlc, ne_wlc]\ntitls = [\"Negative Tweets\", \"Positive Tweets\", \"Neutral Tweets\"]\n\nfor num, el in enumerate(wcs):\n    ax[num].imshow(el)\n    ax[num].axis('off')\n    ax[num].set_title(titls[num])","97070c2c":"# Remove everything except basic text characters\ntrain_data['OriginalTweet'] = train_data['OriginalTweet'].str.replace(\"[^a-zA-Z]\", \" \").str.lower()\ntrain_data.sample(5)","f300c34b":"# Change the column name and encode the labels\ntrain_data = train_data.rename(columns={'Sentiment':'label'})\ntrain_data['label'] = train_data['label'].apply(lambda x: 0 if x=='Negative' else (1 if x=='Positive' else 2))","31bda496":"# Let us now split the dataset into training and validation sets\nsplit_pcent = 0.20  # How much percent of data should go into testing set\nsplit = int(split_pcent * len(train_data))\n\nshuffled_set = train_data.sample(frac=1).reset_index(drop=True)   # Shuffle the data\nvalid_set = shuffled_set[:split]   # Get everything till split number\ntrain_set = shuffled_set[split:]   # Get everything after split number\n\ntrain_set = train_set[['label', 'OriginalTweet']]\nvalid_set = valid_set[['label', 'OriginalTweet']]","6571b65d":"# Make a Language Model Data Bunch from our train set\ndata_bunch = TextLMDataBunch.from_df(train_df=train_set, valid_df=valid_set, path=\"\")\n\n# Make the data classifier\ndata_clf = TextClasDataBunch.from_df(path=\"\", train_df=train_set, valid_df=valid_set, vocab=data_bunch.train_ds.vocab, bs=16)","14702492":"# Define the language learner model and fit for one epoch\nlearner = language_model_learner(data_bunch, arch=AWD_LSTM, drop_mult=0.5)\n\nlearner.fit_one_cycle(1, 1e-2)","dbdaa419":"# Try unfreezing last 3 layers first\nlayers_to_unfreeze = [1, 2, 3]\nfor i in layers_to_unfreeze:\n    learner.freeze_to(-i)\n    learner.fit_one_cycle(1, 1e-2)","b717557a":"learner.unfreeze()\nlearner.fit_one_cycle(1, 1e-2)","8c362f86":"# Save the encoder\nlearner.save_encoder('learn_encoder')","dbb471d6":"clf = text_classifier_learner(data_clf, arch=AWD_LSTM, drop_mult=0.5)\nclf.load_encoder('learn_encoder')","31eaa7a9":"clf.fit_one_cycle(5, 1e-2)","f00a2baf":"# Let's unfreeze all it's layers and train it.\nclf.unfreeze()\nclf.fit_one_cycle(5)","89aaf0bd":"clf.predict(\"The COVID is harming our lives and destroying job opportunities\")","68bce4b7":"<h3 style=\"color:green;text-align:center\">Character Frequency Count<\/h3>","b6f59f93":"<h2 style=\"color:blue;text-align:center\">Testing and Classification<\/h2>\n<hr>","cc3a43fc":"<h2 style=\"color:blue;text-align:center\">Text Cleaning<\/h2>\n<hr>","d9f56b92":"<h3 style=\"color:green;text-align:center\">Word Cloud<\/h3>","1a512bc0":"<h3 style=\"color:green;text-align:center\">URL Count<\/h3>","0655cdd6":"Just train the learner as-is.","50b928f9":"Now unfreeze the hidden layers and train the learner.","d6efacf0":"Now we train the classifier using the encoder above.","69c31e9c":"We'll join both Datasets, shuffle them and them divide them.","e23e6ed6":"<h2 style=\"color:blue;text-align:center\">Modelling<\/h2>\n<hr>","0c71ead0":"<h3 style=\"color:green;text-align:center\">Target Value Distribution<\/h3>","ccf1c70d":"<h3 style=\"color:green;text-align:center\">Unique Word Count<\/h3>","05b4f06a":"<h1 style=\"color:aqua;text-align:center\">COVID-19 Tweet EDA + Fast.ai Classification<\/h1>\n\n\n<strong style=\"color:red\">If you like my notebook, please leave an upvote!<\/strong>\n<hr>","445e7912":"Let's now unfreeze all layers and then train them.","e4810b18":"<h2 style=\"color:blue;text-align:center\">Exploratory Data Analysis<\/h2>\n<hr>","b69afea2":"We want this classification to be 3-way so changing `Extremely Positive` to `Positive` and `Extremely Negative` to `Negative`.\nFor the moment, we only need the tweet text and the sentiment of it.","4afa1035":"<h3 style=\"color:green;text-align:center\">Word Count Distribution<\/h3>"}}