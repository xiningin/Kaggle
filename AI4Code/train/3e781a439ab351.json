{"cell_type":{"9e98bf21":"code","f8f886a3":"code","d6d73842":"code","cf2516b3":"code","5d514d3e":"code","8611ba80":"code","4c8b72ae":"code","7dd7223a":"code","adc3adec":"code","94101173":"code","8ec4148c":"code","64d282ae":"code","1b37ea29":"code","feb43482":"code","1bb34dc5":"code","edcfbbb5":"code","29011582":"code","55a704e3":"code","bc8af5d4":"code","d620d155":"code","52e24c8b":"code","9962f61e":"code","bec4024d":"code","30708c63":"code","9e1b0eb8":"code","3a747264":"code","f8fd439a":"code","5eed92ae":"code","640b15be":"code","7298a82c":"code","690c223d":"code","5024b1d1":"code","da3d331f":"code","a9edfcd7":"code","0cc30ca2":"code","87f8ea2e":"code","57a50c4c":"code","ad120fa1":"code","06d0758c":"code","c2726191":"code","1ad09d32":"code","bfdcbc4a":"code","ca5efb02":"code","9d695c46":"code","ad2cb9cb":"code","6d741994":"code","791f352c":"markdown","b93829e5":"markdown","164046db":"markdown","8660f675":"markdown","f1d58941":"markdown","c7d124c0":"markdown","63b6fa74":"markdown","d0adc925":"markdown","426c1087":"markdown","4f0d24ac":"markdown","77481f4f":"markdown","9aee43d4":"markdown","1ae30854":"markdown","4193a7ed":"markdown","e187e154":"markdown","d585f5e1":"markdown","533c2f37":"markdown","e544b1fc":"markdown","348ed35b":"markdown","9202f0fc":"markdown","1d27af36":"markdown","0b698fe7":"markdown","116f29b8":"markdown","f2743036":"markdown","4a54a9ce":"markdown","f9783975":"markdown","b6ef56a2":"markdown","1c9d8a90":"markdown","25c0b16e":"markdown","c0d0969e":"markdown","8ce96562":"markdown","0509266a":"markdown","b3df7ab4":"markdown","43d16166":"markdown","af84b60c":"markdown","2a188384":"markdown","987aa83e":"markdown"},"source":{"9e98bf21":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","f8f886a3":"df = pd.read_csv('..\/input\/turnover.csv')\ndf.info()","d6d73842":"df.describe().T","cf2516b3":"df[['sales', 'salary']].describe().T","5d514d3e":"(df.salary.value_counts()\/len(df.salary))*100","8611ba80":"((df.sales.value_counts()\/len(df.sales))*100).plot(kind='bar')","4c8b72ae":"df.satisfaction_level.hist()","7dd7223a":"plt.figure(figsize=(35,7))\n((df.satisfaction_level.value_counts().sort_index()\/len(df.satisfaction_level))*100).plot(kind='bar')","adc3adec":"df.satisfaction_level.plot(kind='box')","94101173":"df.left.value_counts()\/len(df.left)","8ec4148c":"df.satisfaction_level = df.satisfaction_level.astype('category')\nax = pd.Series((df[df.left==0].satisfaction_level.value_counts()\/len(df.left))*100).sort_index().plot(kind='bar',color='g',figsize=(35,10))\npd.Series((df[df.left==1].satisfaction_level.value_counts()\/len(df.left))*100).sort_index().plot(kind='bar',color='r',alpha= 0.7,figsize=(35,10), ax=ax)\nax.legend([\"Stayed\", \"Left\"])","64d282ae":"plt.figure(figsize=(5,5))\nax = ((df[df.left==0].salary.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g')\n((df[df.left==1].salary.value_counts().sort_index()\/len(df.left)*100)).plot(kind='bar',color='r',alpha= 0.7, ax= ax)\nax.legend([\"Stayed\", \"Left\"])","1b37ea29":"plt.figure(figsize=(35,7))\n((df.last_evaluation.value_counts().sort_index()\/len(df.last_evaluation))*100).plot(kind='bar')","feb43482":"plt.figure(figsize=(35,5))\n((df[df.left==1].last_evaluation.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='r')\n((df[df.left==0].last_evaluation.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g',alpha=0.4)","1bb34dc5":"plt.figure(figsize=(5,2))\n((df[df.left==0].number_project.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g')\n((df[df.left==1].number_project.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='r',alpha=0.7)","edcfbbb5":"plt.figure(figsize=(40,5))\n((df[df.left==1].average_montly_hours.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='r')\n((df[df.left==0].average_montly_hours.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g',alpha=0.4)","29011582":"plt.figure(figsize=(10,5))\n((df[df.left==1].time_spend_company.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='r')\n((df[df.left==0].time_spend_company.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g',alpha=0.4)","55a704e3":"plt.figure(figsize=(5,3))\n((df[df.left==1].Work_accident.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='r')\n((df[df.left==0].Work_accident.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g',alpha=0.4)","bc8af5d4":"plt.figure(figsize=(35,7))\n((df[df.Work_accident ==1].satisfaction_level.value_counts().sort_index()\/len(df[df.Work_accident ==1]))*100).plot(kind='bar')","d620d155":"plt.figure(figsize=(5,3))\n((df[df.left==1].promotion_last_5years.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='r')\n((df[df.left==0].promotion_last_5years.value_counts().sort_index()\/len(df.left))*100).plot(kind='bar',color='g',alpha=0.4)","52e24c8b":"plt.figure(figsize=(5,3))\n((df[df.left==1].sales.value_counts()\/len(df.left))*100).plot(kind='bar',color='r')\n((df[df.left==0].sales.value_counts()\/len(df.left))*100).plot(kind='bar',color='g',alpha=0.4)","9962f61e":"df.salary.value_counts()","bec4024d":"lkup = {\"low\": 0, \"medium\": 1, \"high\": 2}\ndf['sal_num'] = df['salary'].map(lkup)","30708c63":"df.drop('salary', inplace=True, axis=1)","9e1b0eb8":"df.sal_num.value_counts()","3a747264":"plt.figure(figsize=(10,5))\nsns.heatmap(df.corr(),cbar=True,fmt =' .2f', annot=True, cmap='coolwarm')","f8fd439a":"df = pd.concat([df, pd.get_dummies(df['sales'],prefix='sl', prefix_sep='_')], axis=1)\ndf.drop('sales', inplace=True, axis=1)","5eed92ae":"y = df.left.values\ndf.drop('left', inplace=True, axis=1)","640b15be":"df_NK = df.copy()\n\nbins = [0, 0.11, 0.35, 0.46, 0.71, 0.92,1.0]\ndf_NK['satisfaction_level_bin'] = pd.cut(df_NK.satisfaction_level,bins)\n\nbins = [0, 0.47, 0.48, 0.65, 0.88, 0.89,1.0]\ndf_NK['last_evaluation_bin'] = pd.cut(df_NK.last_evaluation,bins)\n\nlkup = { 3: \"low\", 4 : \"medium\", 5 : \"medium\",  2: \"high\", 6: \"high\", 7: \"Very high\"}\ndf_NK['number_project_cat'] = df_NK['number_project'].map(lkup)\n\nbins = [96, 131, 165, 178, 179, 259, 287]\ndf_NK['average_montly_hours_bin'] = pd.cut(df_NK.average_montly_hours,bins)\n\nlkup = { 2: \"low\", 3 : \"medium\", 4 : \"medium\", 6 : \"medium\", 5: \"high\", 7: \"very low\", 8: \"very low\", 10: \"very low\"}\ndf_NK['time_spend_company_cat'] = df_NK['time_spend_company'].map(lkup)\n\ndf_NK = pd.concat([df_NK, pd.get_dummies(df_NK['satisfaction_level_bin'],prefix='sts', prefix_sep='_')], axis=1)\ndf_NK.drop('satisfaction_level', inplace=True, axis=1)\ndf_NK.drop('satisfaction_level_bin', inplace=True, axis=1)\n\ndf_NK = pd.concat([df_NK, pd.get_dummies(df_NK['last_evaluation_bin'],prefix='le', prefix_sep='_')], axis=1)\ndf_NK.drop('last_evaluation_bin', inplace=True, axis=1)\ndf_NK.drop('last_evaluation', inplace=True, axis=1)\n\ndf_NK = pd.concat([df_NK, pd.get_dummies(df_NK['number_project_cat'],prefix='np', prefix_sep='_')], axis=1)\ndf_NK.drop('number_project_cat', inplace=True, axis=1)\ndf_NK.drop('number_project', inplace=True, axis=1)\n\ndf_NK = pd.concat([df_NK, pd.get_dummies(df_NK['average_montly_hours_bin'],prefix='am', prefix_sep='_')], axis=1)\ndf_NK.drop('average_montly_hours_bin', inplace=True, axis=1)\ndf_NK.drop('average_montly_hours', inplace=True, axis=1)\n\ndf_NK = pd.concat([df_NK, pd.get_dummies(df_NK['time_spend_company_cat'],prefix='tsc', prefix_sep='_')], axis=1)\ndf_NK.drop('time_spend_company_cat', inplace=True, axis=1)\ndf_NK.drop('time_spend_company', inplace=True, axis=1)\n\ndf_NK.info()","7298a82c":"# Split into train and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.30, random_state=101)","690c223d":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l2',C=0.50)\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","5024b1d1":"# Split into train and test dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_NK, y, test_size=0.30, random_state=101)","da3d331f":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l2',C=0.50)\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","a9edfcd7":"from sklearn.preprocessing import PolynomialFeatures\npol = PolynomialFeatures(2)","0cc30ca2":"# Split into train and test dataset\nX_train, X_test, y_train, y_test = train_test_split(pol.fit_transform(df_NK.as_matrix()), y, test_size=0.30, random_state=101)","87f8ea2e":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l2',C=0.5)\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","57a50c4c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ndf_PCA= sc.fit_transform(df_NK)","ad120fa1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_PCA, y, test_size=0.30, random_state=101)","06d0758c":"from sklearn.decomposition import PCA\npca_comp = 2\npca = PCA(n_components=pca_comp)\n#pca = PCA(n_components=df_NK.shape[1])\npca.fit(X_train)","c2726191":"#pca_mat = pd.DataFrame(pca.components_.T, columns=['PC' + str(i) for i in range(1,df_NK.shape[1]+1) ], index=df_NK.columns)\npca_mat = pd.DataFrame(pca.components_.T, columns=['PC' + str(i) for i in range(1,pca_comp+1) ], index=df_NK.columns)","1ad09d32":"#pca_mat = pca_mat*pca.explained_variance_ratio_","bfdcbc4a":"pca_mat= pca_mat.abs()","ca5efb02":"pca_mat['lin_influ'] = pca_mat.sum(axis=1)","9d695c46":"from sklearn.preprocessing import PolynomialFeatures\npol = PolynomialFeatures(2)","ad2cb9cb":"X_train, X_test, y_train, y_test = train_test_split(pol.fit_transform(df_NK[pca_mat.lin_influ.nlargest(29).index].as_matrix()), y, test_size=0.30, random_state=101)","6d741994":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(penalty='l2',C=0.4)\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","791f352c":"Only around 8% of employees are in high income bracket. Majority are in low and medium income brackets","b93829e5":"Satisfaction level of employees who had a work accident","164046db":"Based on what we visualized above, we will bin some of the fields to remove non-linearity ","8660f675":"Department","f1d58941":"There are no highly correlated variables, so we don\u2019t have to remove any","c7d124c0":"Distribution of employees based on number of projects they have worked","63b6fa74":"Definite improvement! Ideally we should not include the test data in the visualization","d0adc925":"Only 24% of the data represents the employees who have left the company, the data is not balanced so we should be careful while interpreting the accuracy of the predictive models","426c1087":"* Satisfaction level distribution is some what consistant from 0.5 onwards.\n* Nearly 2.4% of employees gave a score of 0.1 and are the leading group","4f0d24ac":"We will transform the data using the polynomialfeatures and see whether the accuracy improves","77481f4f":"Distribution of employees based on average monthly hours spent at company","9aee43d4":"Promotion Last 5 years","1ae30854":"Time Spend","4193a7ed":"Distribution of scores across different satisfaction levels","e187e154":"Majority of the employees who were evaluated 0.47 or below have left. \nFrom 0.49 to 0.65, the percentage is consistently low, it then picks up from 0.66 onwards reaching to a maximum at 0.89 but fluctuates a lot in between then drops to 0 at 0.9 and stays at 0 there onwards","d585f5e1":"There is a considerable improvement in the F1 score, so the feature engineering really helped","533c2f37":"Employees who have worked on less than three or more than 5 projects have a high tendency to leave","e544b1fc":"Distribution of the dependent variable \"*left*\"","348ed35b":"Logistic Regression","9202f0fc":"Distribution of employees based on performance","1d27af36":"First we will train the model on the original data","0b698fe7":"Now we will check the satisfaction level of employees, this feature is probably the most important one that we need to look at. If someone is not satisfied, chances of that person leaving the company is very high","116f29b8":"Since the data is non-linear, algorithms like random forest and KNN would outperform logistic regression here. Our feature engineering reduced the non-linearity to an extend but not enough to outperform a non-linear algorithm.","f2743036":"Majority are in Sales, Technical and Support","4a54a9ce":"We will plot a few more distributions","f9783975":"We will now transform the salary field to numeric","b6ef56a2":"There is no improvement, but with ten less dimensions, the model produces an overall F1 score same as that of the previous model, which had 39 features.","1c9d8a90":"There is no surprise here, low and medium income employees have a tendency to leave compared to high earners","25c0b16e":"All the employees who voted below 0.12 have left the company, no surprise there, but majority of employees who voted 0.12 to 0.35 have stayed on while who have voted 0.36 - 0.46 have left. \nLeaving percentage is consistently low afterward, till 0.71. \nNone of the employees who voted beyond 0.93 have left the company. The relation is obviously not linear; Logistic Regression might perform poorly here unless we include some polynomial degrees.","c0d0969e":"Distribution of employees across the departments","8ce96562":"We will plot the distribution of employee satisfaction level again but this time will colour code the bars based on whether they have left  the company or not","0509266a":"Now let's see whether we can reduce the input dimension using PCA","b3df7ab4":"Observations\n* There are 10 columns and no missing data;\n* Following categorical variables are already encoded as numeric. \n        Work_accident\n        left\n        promotion_last_5years\n* There are 10 departments\n* Salary is populated as categorical and is already binned ","43d16166":"Heatmap","af84b60c":"Distribution of employees based on salary","2a188384":"Logistic regression performs poorly on the original data, now we will train a logistic model on the feature engineered data.","987aa83e":"Work Accident"}}