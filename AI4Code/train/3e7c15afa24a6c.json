{"cell_type":{"8d8ba21f":"code","35c75da1":"code","5522a0d0":"code","980fb649":"code","5d1a5519":"code","cbfb8bef":"code","fad17c16":"code","7d85a01c":"code","3a2b4fe3":"code","0632e03b":"code","3228c9d1":"code","6da810b9":"code","92a7055f":"code","ce8398bd":"code","5ff6605d":"markdown","397c7049":"markdown","e82f7f8b":"markdown","c76f3734":"markdown","516bead2":"markdown","e39a10ef":"markdown","4a6d9e7d":"markdown","3c835532":"markdown","065ad57d":"markdown"},"source":{"8d8ba21f":"#essential libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy.cluster.vq import *\nimport operator\nfrom matplotlib import pyplot as plt   \nimport pickle as pkl\nimport shelve\nimport re\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport scipy\nfrom sklearn.feature_extraction import DictVectorizer","35c75da1":"#Import shelve dictionary containing all bacnet devices\nsensors_dict = shelve.open('..\/input\/zodiac-dataset\/bacnet_devices.shelve')","5522a0d0":"#device_list filters the NAE for a particular building. This is currently manual. It can be automated \n#if building names are known.\n\n#bonner hall\ndevice_list = [\n                \"557\",\n                \"607\",\n                \"608\",\n                \"609\",\n                \"610\",\n]","980fb649":"#Parse the data in the dictionary as filtered by device_list\n#Gives us a sensor_list with sensor information of a building\nsensor_list = []\nnames_list = []\nnames_listWithDigits = [] \nsensor_type_namez=[]\ndesc_list = []\nunit_list = []\ntype_str_list = []\ntype_list = []\njci_names_list = []\nsource_id_set = set([])\nfor nae in device_list:\n    device = sensors_dict[nae]\n    h_dev = device['props']\n    for sensor in device['objs']:\n        h_obj = sensor['props']\n        source_id = str(h_dev['device_id']) + '_' + str(h_obj['type']) + '_' + str(h_obj['instance'])\n        \n        if h_obj['type'] not in (0,1,2,3,4,5,13,14,19):\n            continue\n        \n        if source_id in source_id_set:\n            continue\n        else:\n            source_id_set.add(source_id)\n        \n        #create individual lists\n        #remove numbers from names because they do not indicate type of sensor\n        names_listWithDigits.append(sensor['jci_name']) \n        sensor_type_namez.append(sensor['sensor_type'])\n        names_list.append(''.join([c for c in sensor['name'] if not c.isdigit()]))\n        desc_list.append(''.join([c for c in sensor['desc'] if not c.isdigit()]))\n        jci_names_list.append(''.join([c for c in sensor['jci_name'] if not c.isdigit()]))\n        #convert string to dictionary for categorical vectorization\n        unit_list.append({str(sensor['unit']):1})\n        type_str_list.append({str(h_obj['type_str']):1})\n        type_list.append({str(h_obj['type']):1})\n        \n        #create a flat list of dictionary to avoid using json file\n        sensor_list.append({'source_id': source_id, \n                            'name': sensor['name'], \n                            'description': sensor['desc'],\n                            'unit': sensor['unit'],\n                            'type_string': h_obj['type_str'],\n                            'type': h_obj['type'],\n                            #'device_id': h_obj['device_id'],\n                            'jci_name': sensor['jci_name'],\n                            #add data related characteristics here\n                        })\nsensor_df = pd.DataFrame(sensor_list)\nsensor_df = sensor_df.set_index('source_id')\nsensor_df = sensor_df.groupby(sensor_df.index).first()\nprint len(sensor_list)","5d1a5519":"#Create a bag of words from sensor string metadata. Vectorize so that it can be used in ML algorithms.\nnamevect = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')\nnamebow = scipy.sparse.coo_matrix(namevect.fit_transform(names_list))\n\ndescvect = CountVectorizer() \ndescbow = scipy.sparse.coo_matrix(descvect.fit_transform(desc_list))\n\nunitvect = DictVectorizer() \nunitbow = scipy.sparse.coo_matrix(unitvect.fit_transform(unit_list))\n\ntype_str_vect = DictVectorizer() \ntype_str_bow = scipy.sparse.coo_matrix(type_str_vect.fit_transform(type_str_list))\n\ntypevect = DictVectorizer() \ntypebow = scipy.sparse.coo_matrix(typevect.fit_transform(type_list))\n\njcivect = CountVectorizer() \njcibow = scipy.sparse.coo_matrix(jcivect.fit_transform(jci_names_list))\n\nfeature_set = jcivect.get_feature_names()+ \\\n              descvect.get_feature_names()+ \\\n              unitvect.get_feature_names()+ \\\n              type_str_vect.get_feature_names()+ \\\n              typevect.get_feature_names()\n              \n\nfinal_bow = scipy.sparse.hstack([\n                                 #namebow,\n                                 descbow,\n                                 unitbow,\n                                 type_str_bow,\n                                 typebow,\n                                 jcibow\n                                ]) \nbow_array = final_bow.toarray() # this is the bow for each sensor. ","cbfb8bef":"# Hierarchical agglomerative clustering \nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport scipy.cluster.hierarchy as hier\n\nnum_of_sensors = len(bow_array)\na = np.array(bow_array[:num_of_sensors])\nz = linkage(a,metric='cityblock',method='complete')","fad17c16":"#Apply threshold to hierarchical tree to obtain individual clusters. Results stored in equip_map\ndists = list(set(z[:,2]))\nthresh = (dists[2] + dists[3]) \/2 \nprint \"Threshold: \", thresh\nb = hier.fcluster(z,thresh, criterion='distance')\ncluster_map = {}\nequip_map = {}\nfor i in range(len(b)):\n    cluster_map[names_list[i]] = b[i]\n    print i, names_list[i], b[i]\n    if b[i] in equip_map:\n        equip_map[b[i]][\"sensors\"].append(sensor_list[i])\n        equip_map[b[i]][\"sensor_ids\"].append(i)\n    else:\n        equip_map[b[i]] = {\"sensors\":[sensor_list[i]]}\n        equip_map[b[i]][\"sensor_ids\"] = [i]\n    sensor_list[i]['equip_cluster_id'] = b[i]\nsorted_map = sorted(cluster_map.items(), key=operator.itemgetter(1))","7d85a01c":"#read ground truth sensor types\nimport csv\nbuilding = 'bonner'\nground_truth_list = []\nwith open('metadata\/'+building+'_sensor_types.csv') as ground_truth_file:\n    csv_reader = csv.DictReader(ground_truth_file)\n    for row in csv_reader:\n        ground_truth_list.append(row)\nsensor_type_map = {s['source_id']:s['sensor_type'] for s in ground_truth_list}","3a2b4fe3":"# Merges the clusters formed by hierarchical clustering based on \"description\" tag. \nequip_desc_map = {}\nsensor_abbrvs = [s['jci_name'].split('.')[-1].lower() if '.' in s['jci_name'] else s['jci_name'] for s in ground_truth_list]\n#sensor_abbrvs = [re.sub('[^a-z ]', '', s) for s in sensor_abbrvs]\n\nfor k,v in equip_map.iteritems():\n    #print v\n    desc_list = [s['description'].lower() for s in v['sensors']]\n    desc_list = [re.sub('[^a-z ]', '', d) for d in desc_list]\n    desc_list = [sensor_abbrvs[i] if d == '' else d for i,d in enumerate(desc_list)]\n    if len(set(desc_list)) == 1:\n        if desc_list[0] in equip_desc_map and desc_list[0] != '':\n            equip_desc_map[desc_list[0]]['sensors'] += v['sensors']\n            equip_desc_map[desc_list[0]]['sensor_ids'] += v['sensor_ids']\n        elif desc_list[0] == '':\n            equip_desc_map[k] = v\n        else:\n            equip_desc_map[desc_list[0]] = v\n    else:\n        equip_desc_map[k] = v\n    \n#print \"merged cluster:\", len(equip_desc_map)","0632e03b":"#get ground truth set\n#equip_map = equip_desc_map #Uncomment for using merged clusters\n# Manually label say 10 clusters and hence multiple sensors. \nimport random\nnum_manual_labels = 10\nsensor_labels = []\nsensor_bow = []\nlabeled_equip_keys = []\nequip_cluster_lens = {k:len(v['sensors']) for k,v in equip_map.iteritems()}\nsorted_equip_keys = sorted(equip_cluster_lens.items(), key=operator.itemgetter(1), reverse=True)\nfor i in range(num_manual_labels):\n#for c_id in equip_map.keys()[:num_manual_labels]:\n    c_id = random.choice(equip_map.keys())\n    #c_id = sorted_equip_keys[i][0]\n    labeled_equip_keys.append(c_id)\n    for ix,i in enumerate(equip_map[c_id]['sensor_ids']):\n            sensor_bow.append(bow_array[i])\n            source_id = sensor_list[i]['source_id']\n            sensor_labels.append(sensor_type_map[source_id])\nsensor_bow = np.array(sensor_bow)\nsensor_labels = np.array(sensor_labels)","3228c9d1":"#learn a model\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.mixture import GMM\nfrom sklearn.mixture import DPGMM\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nle = LabelEncoder()\nle.fit(sensor_labels)\n#print list(le.classes_)\ntrain_labels = le.transform(sensor_labels)\nmodel = RandomForestClassifier(n_estimators=400, random_state=0)\nmodel.fit(sensor_bow,sensor_labels)\n#model.fit(sensor_bow)","6da810b9":"def apply_model_on_all_clusters( ): # model, T_low, T_high, ....\n    # This method uses global variables including model, T_low, T_high and several others. \n    # Goal: apply model on all clusters and determine correctness of \"confident predictions\" \n    # and manually label \"very low confidence\" ones \n\n    global sensor_labels\n    global sensor_bow \n    global labeled_equip_keys \n    \n    # Iteratively apply Random Forest to label new sensors \n    change_thresholds = True \n    n_wrong_confident_sensor_pred = 0\n    sensor_bow = list(sensor_bow)\n    sensor_labels = list(sensor_labels) \n    n_high_confidence_sensors = 0\n    n_manually_labeled_thisepoch = 0 # epoch = whatever happens after (re-) training RF models\n\n    for p in equip_map.keys(): # for each cluster \n    #for p in sorted_equip_keys:\n        #p = p[0]\n\n        # Escape if already labeled. \n        if p in labeled_equip_keys:\n            continue\n\n\n        # Get sensors from this cluster. \n        sample_bow = []\n        for k in equip_map[p]['sensor_ids']:\n            sample_bow.append(bow_array[k])\n        sample_bow = np.array(sample_bow)\n\n\n        # Apply trained model: \n        confidence = model.predict_proba(sample_bow)\n        prediction_label = model.predict(sample_bow)\n        # Get overall max confidence for any sensor in cluster: \n        max_c = 0\n        for c in confidence:\n            max_c = max(np.append(c,[max_c]))\n\n\n        # Compare with Thresholds. \n        flag = 0    \n        if max_c < T_low:\n            flag = 1\n        if max_c > T_high:\n            flag = 2        \n\n        if flag==1: \n            n_manually_labeled_thisepoch+=1 \n\n        \n        # Handle the cluster beyond threshold: \n        if flag>0: \n            change_thresholds = False \n            labeled_equip_keys.append(p)  \n\n            # For each sensor in this cluster: \n            for k in range(len(equip_map[p]['sensors'])):  \n                sourceid = equip_map[p]['sensors'][k]['source_id']\n                true_type = sensor_type_map[sourceid] \n                pred_type = prediction_label[k]              \n\n                if flag==2: \n                    n_high_confidence_sensors+=1\n                    if pred_type != true_type: \n                        n_wrong_confident_sensor_pred+=1 \n\n                    # append these sensors into labeled ones (with possibly wrong labels): \n                    sensor_bow.append(bow_array[equip_map[p]['sensor_ids'][k]]) \n                    sensor_labels.append(pred_type) \n\n                if flag==1:                 \n                    \n                    # append these sensors into labeled ones (with ground truth): \n                    sensor_bow.append(bow_array[equip_map[p]['sensor_ids'][k]]) \n                    sensor_labels.append(true_type) \n\n            break\n        #sensor_bow = np.array(sensor_bow)\n        #sensor_labels = np.array(sensor_labels)\n        #model.fit(sensor_bow, sensor_labels)\n        \n    return n_manually_labeled_thisepoch, n_wrong_confident_sensor_pred, n_high_confidence_sensors, len(sensor_labels), 100.0*len(sensor_labels)\/len(bow_array),change_thresholds,len(bow_array)-len(sensor_labels)   \n    # return __  , __ , __ , num labeled sensors , % labeled sensors, change_thresholds, num_sensors_in_gray ","92a7055f":"# Iteratively train RF model and call the method to apply it on all clusters. \n# When method asks us to change thresholds, then we do so. \n# Otherwise we re-train RF and try catch more sensors. \n# We also record the number of correct sensors in each iteration, the manual effort in each iteration etc. \n\n#print equip_map.keys() \n#print labeled_equip_keys \nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(n_estimators=400, random_state=0)\nmodel.fit(sensor_bow,sensor_labels) \n\nnum_sensors_in_gray=100\n\nT_low = 0.1\nT_high = 0.95 \nthresholds = [ (0.1,0.95), (0.1,0.9) , (0.15,0.9), (0.15,0.85), (0.2,0.85), (0.25,0.85), (0.3,0.85), (0.35,0.85), (0.4,0.85), (0.45,0.85), (0.5,0.85), (0.55,0.85), (0.6,0.85), (0.65,0.85), (0.7,0.85), (0.75,0.85), (0.8,0.85), (0.849999999,0.85) ] \n#thresholds = [ (0.1,0.9) , (0.15,0.9), (0.15,0.85), (0.2,0.85), (0.25,0.85), (0.3,0.85), (0.35,0.85), (0.4,0.85), (0.45,0.85), (0.45,0.8),(0.5,0.8), (0.55,0.8), (0.6,0.8), (0.65,0.8), (0.7,0.8), (0.75,0.8), (0.7999999,0.8) ] \n#thresholds = [ (0.1,0.7) , (0.25,0.7), (0.3,0.7), (0.3,0.7), (0.35,0.7), (0.4,0.7), (0.4,0.65),(0.45,0.65), (0.5,0.65), (0.5,0.6), (0.55,0.6), (0.5999999,0.6)] \n\nthresh_count=0 \n\n# Start iterations: \nn_manual_lab_clusters_iter = [10 ]\nn_sensors_covered_iter = [len(sensor_labels) ] \n\nwhile num_sensors_in_gray>0: \n    T_low,T_high = thresholds[thresh_count] \n    \n    # Re-train model: \n    model.fit(sensor_bow,sensor_labels) \n    \n    # Use model to label clusters\/sensors: \n    n_manually_labeled_thisepoch, n_wrong_confident_sensor_pred, n_high_confidence_sensors, n_sens_covered, perc_coverage,change_thresholds,num_sensors_in_gray = apply_model_on_all_clusters()        \n    print n_manually_labeled_thisepoch, n_wrong_confident_sensor_pred, n_high_confidence_sensors, n_sens_covered, perc_coverage,change_thresholds,num_sensors_in_gray  \n    n_manual_lab_clusters_iter.append(n_manual_lab_clusters_iter[-1] + n_manually_labeled_thisepoch) \n    n_sensors_covered_iter.append(n_sens_covered) \n    \n    if change_thresholds: \n        thresh_count+=1 \n        print T_low, T_high","ce8398bd":"# This code uses regular expressions to map descriptions (and if needed, jci_name) to ground truth \n# Goal: Get a manual effort (in mapping either of above to ground truth) to coverage \n\ndesc_list=[] \njc_names_list=[] \nsensor_info = {} \nfor s in sensor_list: \n    sid = s['source_id'] \n    sensor_info[sid]={} \n    d = s['description'].lower() \n    d = ''.join([i for i in d if not i.isdigit()]) #remove digits \n    d = re.sub(r\"[^\\w' ]\", \"\",  d ) # remove special chars \n    d = ' '.join(d.split()) #remove extra spaces \n    sensor_info[sid]['desc'] = d \n    desc_list.append(d)     \n    \n    j = s['jci_name'].split('.')[-1] \n    sensor_info[sid]['jci'] = j \n    jc_names_list.append(j)  \n    sensor_info[sid]['figuredout'] = False \n    \nmanualeffort=[0] \ncoveredsensors=[0] \ndesc_map = {}\njci_map = {} \n\nfor s in sensor_list: \n    sid = s['source_id'] \n    if sensor_info[sid]['figuredout']==True: continue # If label known, skip. \n        \n    # Info about this sensor: \n    gt = sensor_type_map[ s['source_id'] ]  # ground truth \n    d = s['description'].lower() \n    d = ''.join([i for i in d if not i.isdigit()]) #remove digits \n    d = re.sub(r\"[^\\w' ]\", \"\",  d ) # remove special chars \n    d = ' '.join(d.split()) #remove extra spaces \n    j = s['jci_name'].split('.')[-1] \n\n    \n    if not d ==\"\": \n        if not d in desc_map: \n            manualeffort.append(manualeffort[-1]+1) \n            desc_map[d] = gt \n            jci_map[j] = gt \n            sensor_info[sid]['figuredout']=True \n            # Check how many it catches: \n            numcatches=0\n            for s2 in sensor_list: \n                sid2 = s2['source_id'] \n                if sensor_info[sid2]['figuredout']==False and (sensor_info[sid2]['desc']==d   or sensor_info[sid2]['jci']==j): \n                    sensor_info[sid2]['figuredout']=True\n                    numcatches+=1 \n            coveredsensors.append(coveredsensors[-1] + numcatches) \n            \n    else: \n        if not j in jci_map: \n            manualeffort.append(manualeffort[-1]+1) \n            jci_map[j] = gt \n            sensor_info[sid]['figuredout']=True \n            # Check how many it catches: \n            numcatches=0\n            for s2 in sensor_list: \n                sid2 = s2['source_id'] \n                if sensor_info[sid2]['figuredout']==False and sensor_info[sid2]['jci']==j: \n                    sensor_info[sid2]['figuredout']=True\n                    numcatches+=1 \n            coveredsensors.append(coveredsensors[-1] + numcatches) \n            \n            \n            \n    \n# Just checking. \nfor s in sensor_list: \n    sid = s['source_id'] \n    if sensor_info[sid]['figuredout']==False: \n        print sid \n\n\n        \n\n# Plot the manual effort vs coverage for Regex based approach. \nplt.plot(manualeffort, coveredsensors, 'ro')\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=15)\nplt.ylabel('# points covered', fontsize=20)\nplt.xlabel('Manual inputs', fontsize=20)\nplt.tight_layout()\n\nplt.xticks(np.arange(0, max(manualeffort)+1, 75.))\n\nplt.savefig(\"BonnersensorsREGEXManualVsCoverage.pdf\",bbox_inches='tight',dpi=150)\n\n\nlen(set(jc_names_list))","5ff6605d":"# Zodiac: Source Code and Documentation","397c7049":"Modern buildings consist of many different types of infrastructure such as lighting, air conditioning, power, water. For operation and maintenance of these systems with minimal manual supervision, networked sensors are deployed across the building so that it can be monitored remotely. <a href=\"https:\/\/en.wikipedia.org\/wiki\/Building_management_system\"> Building Management Systems (BMS)<\/a> are software systems that collect data from installed sensors, allow remote control of equipment and provide visualizations for the maintenance personnel. \n\nModern BMSes have thousands of data points per building, and these data points correspond to installed sensors, actuators of equipment as well as configuration parameters. With modern data processing and control mechanisms it is possible to exploit these data points to create useful and innovative building applications such as personalized control, fault detection, demand response management, model predictive control, power grid stability and many more. However, a major impediment to deployment of these applications is that the data points are organized for building domain experts and not computer algorithms. As a result, the metadata that describes the context of the data points in the BMS has errors, extraneous notes, vendor specific notations and other inconsistencies which make it difficult for a machine to interpret the data. Our project Zodiac exploits machine learning techniques to map the raw building metadata to a consistent format so that applications can be developed on top of a common interface and reused across multiple buildings. \n\nOur full research paper which describes the building metadata problem and the Zodiac algorithm can be found <a href=\"http:\/\/dl.acm.org\/citation.cfm?id=2821674\">here<\/a>. The Zodiac project home page where we share our raw building metadata, manually labelled ground truth data point types and this source code page can be found <a href=\"http:\/\/www.synergylabs.org\/bharath\/zodiac.html\">here<\/a>.","e82f7f8b":"We are making the source code available using the Jupyter notebook with explanations alongside so it is easy to follow and replicate our results. \n\n#### Input File Paths\nThe notebook code assumes that the raw metadata and manually labelled ground truth files are available in \"metadata\" directory at the same level as the notebook.\n\n#### Python Libraries\nWe heavily use the <a href=\"http:\/\/scikit-learn.org\/stable\/\">Python Scikit Learn<\/a> library, which contains implementations of popular machine learning algorithms such as hierarchical clustering and random forest classifier used in our algorithm. We use Python <a href=\"http:\/\/www.numpy.org\/\">numpy<\/a> and <a href=\"http:\/\/pandas.pydata.org\/\">pandas<\/a> libraries to store and manipulate large records of metadata. These data structures work well with Scikit Learn modules.\n\nWe use Python <a href=\"https:\/\/docs.python.org\/2\/library\/re.html\">re<\/a> for regular expressions and <a href=\"http:\/\/matplotlib.org\/\">matplotlib<\/a> for plotting. We also use <a href=\"https:\/\/docs.python.org\/2\/library\/shelve.html\">shelve<\/a> and <a href=\"https:\/\/docs.python.org\/2\/library\/pickle.html\">pickle<\/a> for object serialization and non-volatile storage.","c76f3734":"The \"bacnet_device_id\" corresponds to a middlebox that hosts up to 4000 building data points. One building can be assigned several middleboxes based on its requiments and the network design. All of the 55 buildings we obtain our metadata from are managed by the vendor <a href=\"http:\/\/www.johnsoncontrols.com\/\">Johnson Control Inc.<\/a>, and some of the metadata organizing may be specific to this vendor.\n\nEach point is identified by its \"instance\" within the middlebox, and we define a \"source_id\" as a university wide unique identifier for the point. There are two names associated with the point. The \"name\" corresponds to BACnet name property, and the name is based on network architecture. The \"jci_name\" is a proprietary BACnet field used by Johnson Controls, and the name is assigned relative to building location hierarchy. The last part of the name (\"ZN T\" above) encodes an abbreviation of the data point type. The human readable data point type is given in \"description\" (e.g. \"Zone Temperature\"). \n\nBACnet also encodes the data type and input\/output in one field referred to as \"type\" above. For example, \"analog input\" means the data type is float and the point is of input type (e.g. sensor).","516bead2":"The file \"bacnet_devices.shelve\" contains the building points metadata for about 55 buildings at <a href=\"http:\/\/ucsd.edu\/\">University of California, San Diego<\/a> (UCSD). We obtain this metadata using <a href=\"http:\/\/www.bacnet.org\/\">BACnet<\/a>, a standard building automation network communication protocol. \n\nWhen loaded, the shelve file becomes a Python dictionary. The format of the metadata is as follows:","e39a10ef":"### Source Code","4a6d9e7d":"### This is not part of the source code ####\n[\"bacnet_device_id\": {\n           \"props\":{ \"device_id\": 557 },\n           \"objs\":{\n               \"sensor_1\":{\n                   \"props\":{\n                       \"type\": 0,\n                       \"type_str\": \"analog input\", #BACnet encodes type as enumerator. 0 stands for analog input\n                       \"instance\": 230523\n                   },\n                   \"name\":\"NAE 57 N2 1 VAV 5 ZN T\",\n                   \"source_id\":\"557_0_3001187\", #Concatenation of device id, type and object instance\n                   \"description\":\"Zone Temperature\", #Human readable point description\n                   \"unit\": 64, #BACnet encoded units. 64 stands for Fahrenheit\n                   \"jci_name\": \"BH.1STFLR.RM-1511.VAV-5.ZN-T\" \n               }\n           }\n },\n \"bacnet_device_id_2\": {\n   ...\n },\n ...\n]","3c835532":"## The following code is adapted from Dr Bharathan Balaji and the team who has worked on the Zodaic project\n\nhttps:\/\/github.com\/synergylabs\/Zodiac","065ad57d":"###### Date: May 30, 2016"}}