{"cell_type":{"ece16755":"code","906cf1f3":"code","acdd869d":"code","d2f8533a":"code","7fe00608":"code","42df9c08":"code","9404e68d":"code","94c12ab5":"code","aa20d05d":"code","f921343e":"code","0702fd50":"code","f8632fc8":"code","63bf6d26":"code","a76c888d":"code","b3c416f5":"code","fd9e3f9b":"code","b9d59283":"code","8a466c4e":"code","4d2053b1":"code","901be427":"code","08af4453":"code","b4b4dd00":"code","7258c2c1":"code","54f4766e":"code","47695783":"code","c0e706ac":"code","cdd0e64d":"code","a8daf157":"code","66038f61":"code","705d3fb0":"code","34964755":"code","456a0c1f":"code","bba8ef20":"code","88940493":"markdown","f42f3f1e":"markdown","516fe108":"markdown","545f8d1b":"markdown","d170e1dd":"markdown","11571756":"markdown","8c51b0a9":"markdown","af22d345":"markdown","3e348aff":"markdown","5069623a":"markdown","eef57000":"markdown","7dd59e73":"markdown","e0f49924":"markdown","f72d8017":"markdown","53cd8df6":"markdown","8fd82372":"markdown","c5a18e82":"markdown","9b924a1f":"markdown","0b5483cb":"markdown","3ba69e67":"markdown","4f4146d6":"markdown","fb2ee3de":"markdown","8120bf6b":"markdown","db2c0d74":"markdown","9e4fff50":"markdown"},"source":{"ece16755":"#codes from Rodrigo Lima  @rodrigolima82\nfrom IPython.display import Image\nImage(url = 'data:image\/png;base64,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',width=400,height=400)","906cf1f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as py\nimport plotly.express as px\nimport seaborn\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acdd869d":"df = pd.read_csv('\/kaggle\/input\/ai4all-project\/results\/classifier\/lasso_min\/lassoRandomForest_probs.csv')\ndf.head()","d2f8533a":"print(f\"data shape: {df.shape}\")","7fe00608":"# Numerical features\nNumerical_feat = [feature for feature in df.columns if df[feature].dtypes != 'O']\nprint('Total numerical features: ', len(Numerical_feat))\nprint('\\nNumerical Features: ', Numerical_feat)","42df9c08":"from sklearn import preprocessing\nencoder = preprocessing.LabelEncoder()\ndf[\"CZB_ID\"] = encoder.fit_transform(df[\"CZB_ID\"].fillna('Nan'))\n#df[\"Unnamed: 0\"] = encoder.fit_transform(df[\"Unnamed: 0\"].fillna('Nan'))\ndf.head()","9404e68d":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","94c12ab5":"y_train = df.COVID19.values","aa20d05d":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df.values)\n    rmse= np.sqrt(-cross_val_score(model, df.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","f921343e":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","0702fd50":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","f8632fc8":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","63bf6d26":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","a76c888d":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","b3c416f5":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","fd9e3f9b":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b9d59283":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8a466c4e":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4d2053b1":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","901be427":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","08af4453":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","b4b4dd00":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","7258c2c1":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","54f4766e":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","47695783":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","c0e706ac":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","cdd0e64d":"stacked_averaged_models.fit(df.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(df.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(df.values))\nprint(rmsle(y_train, stacked_train_pred))","a8daf157":"model_xgb.fit(df, y_train)\nxgb_train_pred = model_xgb.predict(df)\nxgb_pred = np.expm1(model_xgb.predict(df))\nprint(rmsle(y_train, xgb_train_pred))","66038f61":"model_lgb.fit(df, y_train)\nlgb_train_pred = model_lgb.predict(df)\nlgb_pred = np.expm1(model_lgb.predict(df.values))\nprint(rmsle(y_train, lgb_train_pred))","705d3fb0":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","34964755":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","456a0c1f":"test_ID = df['CZB_ID']","bba8ef20":"sub = pd.DataFrame()\nsub['CZB_ID'] = test_ID\nsub['COVID19'] = ensemble\nsub.to_csv('submission.csv',index=False)","88940493":"#Stacking models\n\nSimplest Stacking approach : Averaging base models","f42f3f1e":"#Conclusion: Very bad Scores (Scores bem ruinzinhos quando n\u00e3o se sabe trabalhar.)","516fe108":"Das War's, Kaggle Notebook Runner: Mar\u00edlia Prata   @mpwolke ","545f8d1b":"#Averaged base models score","d170e1dd":"#codes from Serigne  https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook","11571756":"#Ensembling StackedRegressor, XGBoost and LightGBM","8c51b0a9":"mlfromscratch.com","af22d345":"#Define a cross validation strategy\n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","3e348aff":"#Base models scores\n\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","5069623a":"#Label Encoding","eef57000":"#Kernel Ridge Regression","7dd59e73":"#Base models\n\n#LASSO Regression :\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","e0f49924":"#Final Training and Prediction StackedRegressor","f72d8017":"#Elastic Net Regression : again made robust to outliers","53cd8df6":"#Ensemble prediction","8fd82372":"#LightGBM ","c5a18e82":"#Stacking Averaged models Score","9b924a1f":"#Gradient Boosting Regression : With huber loss that makes it robust to outliers","0b5483cb":"#Stacking averaged Models Class","3ba69e67":"#Submission","4f4146d6":"#XGBoost","fb2ee3de":"#Averaged base models class","8120bf6b":"#XGBoost ","db2c0d74":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQdDaIg1fBF3cIZn9xojKZzNSWmlzcLQ0oI5w&usqp=CAU)youtube.com","9e4fff50":"#Import librairies  Continuous Data Regressions instead of Classifiers."}}