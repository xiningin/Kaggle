{"cell_type":{"c4fca12d":"code","1da95686":"code","345738d6":"code","57e7cd9b":"code","2e23bb16":"code","e39b0b81":"code","2733674d":"code","18509486":"code","3e0e02f8":"code","2f5d4fda":"code","02172164":"code","dffca57b":"code","caa8c7e7":"code","79ba41fb":"code","5285cec3":"code","e24e0b8c":"code","548cc33a":"code","dbaf783e":"code","a99e1731":"code","45a20ef0":"code","cd611a36":"code","397c768a":"code","26e1b52e":"code","abc34262":"code","e87a5db7":"code","d456fcac":"code","02e0399a":"code","58c91818":"code","0de13b60":"code","f9ae872f":"code","6f805a39":"code","7726cd2f":"code","5f8bdc8b":"code","fa361e88":"code","5ce3e0e3":"code","651d410e":"code","9bc5aaba":"code","8c730370":"code","c00bf109":"code","b7d8c78f":"code","bfa25db7":"code","73977027":"markdown","96bd39ba":"markdown","74693718":"markdown","0062e5f4":"markdown","8050e7ac":"markdown","d83d2b6f":"markdown","ac582838":"markdown","97abbee6":"markdown","562163a6":"markdown","ad4a785f":"markdown","b1d6f64f":"markdown","a28fa67f":"markdown","5f352c0b":"markdown","f03b0eab":"markdown","c98d329b":"markdown","476cd543":"markdown"},"source":{"c4fca12d":"!pip install object-detection-fastai","1da95686":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nimport os\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom sklearn.model_selection import StratifiedKFold,KFold\n\nfrom object_detection_fastai.helper.object_detection_helper import *\nfrom object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\nfrom object_detection_fastai.models.RetinaNet import RetinaNet\nfrom object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric","345738d6":"sns.set_style('darkgrid')","57e7cd9b":"path = Path('\/kaggle\/input\/global-wheat-detection');path.ls()","2e23bb16":"train = pd.read_csv(path\/'train.csv')","e39b0b81":"train.head()","2733674d":"tr = train.image_id.value_counts()\ntr = pd.DataFrame({'image_id':tr.index,'wheat_count':tr.values})\ntr = tr.sample(frac=1.,random_state=2020).reset_index(drop=True)","18509486":"tr.head()","3e0e02f8":"fnames = get_files(path\/'train')","2f5d4fda":"coco_source = untar_data(URLs.COCO_TINY)\nimages, lbl_bbox = get_annotations(coco_source\/'train.json')\nimg2bbox = dict(zip(images, lbl_bbox))","02172164":"images[0],lbl_bbox[0]","dffca57b":"def get_lbl_img(train):\n    wheat2bbox = {}\n    train['label'] = 'wheat'\n    grp = train.image_id.unique()\n    tr_gr = train.groupby(['image_id'])\n    from tqdm.notebook import tqdm\n    for i in tqdm(range(len(grp))):\n        name = str(grp[i]) + '.jpg'\n        bbox = []\n        temp_b = []\n        temp = tr_gr.get_group(grp[i])\n        tt = temp.loc[:,'bbox'].values\n        for j in range(len(temp)):\n            t = tt[j][1:-1].split(',')\n            t = [float(x) for x in t]  # x,y, width, height\n            # Currently our coordinates are x,y,w,h and we want x1,y1,x2,y2\n            # To convert it, we need to add our width and height to the respective x and y.\n            t[2],t[3] = t[0]+t[2],t[1]+t[3]  \n            # To achieve x2,y2 those we simply add width to x and height to y :\n            # x2 = x + w and y2 = y + h\n            t1 = [t[1],t[0],t[3],t[2]]   # inverse in fromat w,h for fastai\n            temp_b.append(t1)\n        bbox.append(temp_b)\n        bbox.append(['wheat']*len(temp))\n        wheat2bbox[name] = bbox\n    return wheat2bbox","caa8c7e7":"wheat2bbox = get_lbl_img(train)","79ba41fb":"a = wheat2bbox['d1f73158a.jpg'];a","5285cec3":"get_y_func = lambda o: wheat2bbox[Path(o).name] ","e24e0b8c":"sample = pd.read_csv('..\/input\/global-wheat-detection\/sample_submission.csv')","548cc33a":"sample.head()","dbaf783e":"image_id = sample.image_id.values","a99e1731":"ts = (ObjectItemList.from_df(sample,path, folder = 'test' , suffix = '.jpg',cols='image_id'))","45a20ef0":"data = (ObjectItemList.from_df(tr,path, folder = 'train' , suffix = '.jpg',cols='image_id')\n        #Where are the images? ->\n        .split_by_rand_pct(0.2)                          \n        #How to split in train\/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func on the file name of the data\n        .transform(size=512)\n        .add_test(ts)\n        #Data augmentation? -> Standard transforms; also transform the label images\n        .databunch(bs=2, collate_fn=bb_pad_collate))   ","cd611a36":"data.show_batch(1 , figsize = (8,8) ,ds_type=DatasetType.Valid)","397c768a":"len(data.train_ds),len(data.valid_ds),len(data.test_ds)","26e1b52e":"data.classes","abc34262":"size = 512","e87a5db7":"anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1, 2], scales=[0.35, 0.55, 0.75, 1, 1.25, 1.45])","d456fcac":"fig,ax = plt.subplots(figsize=(10,10))\nax.imshow(image2np(data.valid_ds[0][0].data))\n\nfor i, bbox in enumerate(anchors[:18]):\n    bb = bbox.numpy()\n    x = (bb[0] + 1) * size \/ 2 \n    y = (bb[1] + 1) * size \/ 2 \n    w = bb[2] * size \/ 2\n    h = bb[3] * size \/ 2\n    \n    rect = [x,y,w,h]\n    draw_rect(ax,rect)","02e0399a":"len(anchors)","58c91818":"n_classes = data.train_ds.c\n\ncrit = RetinaNetFocalLoss(anchors)\n\nencoder = create_body(models.resnet18, True, -2)\n\nmodel = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=18, sizes=[32,16,8,4], chs=32, final_bias = -4., n_conv = 2)","0de13b60":"voc = PascalVOCMetric(anchors, size, [i for i in data.train_ds.y.classes[1:]])\nlearn = Learner(data,\n                model, \n                loss_func=crit,\n                callback_fns=[BBMetrics],\n                metrics=[voc],\n                model_dir = '\/kaggle\/working\/')","f9ae872f":"learn.split([model.encoder[6], model.c5top5]);\nlearn.freeze_to(-2)\n#learn = learn.to_fp16()","6f805a39":"#learn.lr_find()\n#learn.recorder.plot()","7726cd2f":"gc.collect()","5f8bdc8b":"#learn.unfreeze()\nlearn.fit_one_cycle(4, 1e-3 ,callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='AP-wheat', name ='best_wheat')])","fa361e88":"learn.load('best_wheat');\nlearn.export('\/kaggle\/working\/gwheat.pkl')","5ce3e0e3":"learn.recorder.plot_losses()","651d410e":"show_results_side_by_side(learn, anchors, detect_thresh=0.5, nms_thresh=0.1, image_count=5)","9bc5aaba":"def show_output(item,bboxs_tot,scores_tot):\n    fig,ax = plt.subplots(figsize=(10,10))\n    ax.imshow(image2np(item.data))\n    plt.axis('off')\n    area_max = 512**2\/5 \n    for bbox, c in zip(bboxs_tot[0], scores_tot[0].numpy()):\n        txt = 'wheat, {0:.4f}'.format(c)\n        if bbox[2]*bbox[3] <= area_max:\n            draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=txt,text_size=12,color='red')","8c730370":"def process_preds_show(item,clas,bboxs,show_img,cnt,i):\n    detect_thresh=0.4   # set your own detection threshold\n    nms_thresh=0.1\n    pred_string = []\n    scores_tot = []\n    bboxs_tot = []\n    show_img = True if i<cnt else False\n    for clas_pred, bbox_pred in list(zip(clas, bboxs)):\n        bbox_pred, scores, preds = process_output(clas_pred, bbox_pred, anchors, detect_thresh)\n        if bbox_pred is not None:\n            to_keep = nms(bbox_pred, scores, nms_thresh)\n            bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu()\n        t_sz = torch.Tensor([size])[None].cpu()\n        if bbox_pred is not None:\n            bbox_pred = to_np(rescale_boxes(bbox_pred, t_sz))\n                # change from center to top left\n            bbox_pred[:, :2] = bbox_pred[:, :2] - bbox_pred[:, 2:] \/ 2\n            bboxs_tot.append(bbox_pred)\n            scores_tot.append(scores)\n    if show_img:\n        show_output(item,bboxs_tot,scores_tot)\n    area_max = (1024**2)\/5\n    for s,bbx in zip(scores_tot[0].numpy(),bboxs_tot[0]):\n        bbx = [int(round(x)) for x in bbx*2]\n        if bbx[2]*bbx[3] <= area_max :\n            res = \"{0:.4f} {1} {2} {3} {4}\".format(s,bbx[1],bbx[0],bbx[3],bbx[2])\n            pred_string.append(res)\n    return pred_string","c00bf109":"def get_prediction(show_img=True,cnt=10): \n    # Set show img True to see img or else false for bboxs only, cnt for number of images to show\n    preds_str = {}\n    for i in range(len(data.test_ds)):\n        item = learn.data.test_ds[i][0]  #Pick one image\n        batch = learn.data.one_item(item)\n        clas,bboxs,xtr = learn.pred_batch(batch=batch)\n        prd = process_preds_show(item,clas,bboxs,show_img,cnt,i) \n        preds_str[image_id[i]] = \" \".join(prd)\n    return preds_str","b7d8c78f":"prediction = get_prediction()\n# Set False to not show images\n# Regardless of that it will give prediction string ","bfa25db7":"submit = pd.DataFrame.from_dict(prediction,orient='index').reset_index()\nsubmit.columns = ['image_id','PredictionString']\nsubmit.head(10)","73977027":"### Now lets create a Object detection data block","96bd39ba":"* A function needs to return the coordinates then the labels. \n* Let's look at an example quickly; from fastai before we preprocess the data","74693718":"### What Is an Anchor Box?\n* **Anchor boxes are a set of predefined bounding boxes of a certain height and width. These boxes are defined to capture the scale and aspect ratio of specific object classes you want to detect** and are typically chosen based on object sizes in your training datasets. During detection, the predefined anchor boxes are tiled across the image. The network predicts the probability and other attributes, such as background, intersection over union (IoU) and offsets for every tiled anchor box. The predictions are used to refine each individual anchor box. You can define several anchor boxes, each for a different object size. Anchor boxes are fixed initial boundary box guesses.\n\n* **The network does not directly predict bounding boxes, but rather predicts the probabilities and refinements that correspond to the tiled anchor boxes.** The network returns a unique set of predictions for every anchor box defined. The final feature map represents object detections for each class. The use of anchor boxes enables a network to detect multiple objects, objects of different scales, and overlapping objects.","0062e5f4":"### Let's look at the dict of images and labels","8050e7ac":"## DataFrame Format:","d83d2b6f":"#### If you like, please don't forget to upvote","ac582838":"* we have **images** and **labels**\n* Then both get **ziped in a dictionary**","97abbee6":"* **n_anchors = len(ratios) x len(scales)**","562163a6":"## If you reached till here please don't forget to upvote.","ad4a785f":"## DataBunch","b1d6f64f":"## fastai images label format for object detection\n\n* Now we have our actual data frame above, we need to make some adjustments. \n\n* Currently our coordinates are **x, y , w , h** and we want **x1, y1, x2, y2**\n\n* To convert it, we need to add our width and height to the respective x and y. \n\n* To achieve x2, y2 those we simply add **width to x** and **height to y** :---- **x2 = x + w** and **y2 = y + h**\n\n\n\n* **dict contains image_names, bbox, label**\n\n**format**\n\n**{ 'dgdsf244.jpg' : [ [ [ bbox1 ][ bbox2] ] , \n                        [ 'label1' ,'label2' ] ]**\n","a28fa67f":"### Advantage of Using Anchor Boxes\n* When using anchor boxes, you can evaluate all object predictions at once. Anchor boxes eliminate the need to scan an image with a sliding window that computes a separate prediction at every potential position. ","5f352c0b":"## Prediction Test images helpful functions","f03b0eab":"# Fastai v1 : Object detection Tutorial","c98d329b":"## Model Training","476cd543":"* Given a path below function will simply return its bbox and labels"}}