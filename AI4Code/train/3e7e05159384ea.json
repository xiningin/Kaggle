{"cell_type":{"f363945f":"code","2b467bb0":"code","f7960ba0":"code","faab96cb":"code","02daf80d":"code","efb6f5ba":"code","e8f2064e":"code","25105518":"code","0abcd47f":"code","9f21bb02":"code","dcfb53ff":"code","0ab63b7e":"code","44a7bbea":"code","00828623":"code","e4a40f96":"code","6fca82bc":"code","df1e8552":"code","cbe1601e":"code","6a4adc79":"code","264424fb":"code","647cb0e7":"code","ddef9bdf":"code","cb953acd":"code","20e7df13":"code","ca506ff6":"code","90361392":"code","e71db54d":"code","2d14e065":"code","925aeedb":"code","d09ee587":"code","dcb46436":"code","83bfb52d":"code","3ed17a9e":"code","920ad92d":"code","72f7bcb6":"code","3f528867":"code","ee07e0f8":"code","c258cb32":"code","0a480262":"code","8f36fb83":"code","e46d7f9c":"code","0d2cd5ed":"markdown","da134c1c":"markdown","9856efff":"markdown","4d00f813":"markdown","e7fb4a13":"markdown","15e1d90c":"markdown","95da09ff":"markdown","833cba4f":"markdown","a2a96cbb":"markdown","0a0a7b62":"markdown","2ba28ddb":"markdown","d14fe172":"markdown","dc6acad8":"markdown","31dd9f86":"markdown","85ada26f":"markdown","801fd858":"markdown","5e4b4786":"markdown","1cefbbb9":"markdown","6dea1423":"markdown","6c6db8f4":"markdown","96918831":"markdown","2d1dabb4":"markdown","30b5661d":"markdown","2e84e4a8":"markdown","71a252ae":"markdown","2540751b":"markdown","8b488728":"markdown","25523a55":"markdown","c18eb6a1":"markdown","35b0cd1b":"markdown","29c368d5":"markdown","96588107":"markdown","92ee2251":"markdown","fddd33b9":"markdown","bfc46fc5":"markdown","1a839654":"markdown","90531174":"markdown"},"source":{"f363945f":"import pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport numpy as np\n\nfrom plotly.subplots import make_subplots\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.feature_selection import VarianceThreshold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain = train_df.copy()\nfamily_column = train['SibSp'] + train['Parch']\ntrain['Family'] = family_column\ntrain = train[['Survived', 'Pclass', 'Name', 'Sex', 'Age', 'Family', 'Embarked', 'Fare']]\n\n# Account for missingness\ntrain['Age'] = train['Age'].interpolate()\ntrain['Fare'] = train['Fare'].interpolate()\n\ntrain.head(5)","2b467bb0":"train.describe()","f7960ba0":"print(str(round(np.mean(train['Survived']) * 100)) + \"% of the passengers on the RMS Titanic survived.\\n\")\nprint(str(round((sum((train[train['Sex'] == 'female'])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were female.\\n\")\nprint(str(round((sum((train[train['Pclass'] == 1])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were first class.\")\nprint(str(round((sum((train[train['Pclass'] == 2])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were second class.\")\nprint(str(round((sum((train[train['Pclass'] == 3])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were third class.\\n\")\nprint(str(round((sum((train[train['Age'] <= 20])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were 20 or younger.\")\nprint(str(round((sum((train[(train['Age'] > 20) & (train['Age'] < 50)])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were between 20 and 50.\")\nprint(str(round((sum((train[train['Age'] >= 50])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors were 50 or older.\\n\")\nprint(str(round((sum((train[train['Family'] == 0])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors had no family members aboard.\")\nprint(str(round((sum((train[train['Family'] >= 3])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors had three or more family members aboard.\\n\")\nprint(str(round((sum((train[train['Embarked'] == 'S'])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors embarked from Southampton.\")\nprint(str(round((sum((train[train['Embarked'] == 'C'])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors embarked from Cherbourg.\")\nprint(str(round((sum((train[train['Embarked'] == 'Q'])['Survived']) \/ sum(train['Survived'])) * 100)) + \"% of the survivors embarked from Queenstown.\")","faab96cb":"def get_title(name):\n    if '.' in name:\n        title = name.split(',')[1].split('.')[0].strip()\n    else:\n        title = 'None'\n        \n    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n        title = 'Mr'\n    elif title in ['the Countess', 'Mme', 'Lady']:\n        title = 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        title = 'Miss'\n    elif title == 'Dr':\n        row = train.loc[train['Name'] == name].index[0]\n        if train.iloc[row]['Sex'] == 'male':\n            title = 'Mr'\n        else:\n            title = 'Mrs'\n    return title\n\ntitles = train['Name'].map(lambda x: get_title(x))\ntrain['Title'] = titles\ntrain.head(5)","02daf80d":"survivors = train[train['Survived'] == 1]\nfemale_survivors = survivors[survivors['Sex'] == 'female']\nmale_survivors = survivors[survivors['Sex'] == 'male']\nclasses = ['First Class', 'Second Class', 'Third Class']\nfemale_classes = female_survivors['Pclass'].value_counts(sort=False, normalize=True).to_list()\nmale_classes = male_survivors['Pclass'].value_counts(sort=False, normalize=True).to_list()\nfig = go.Figure(data=[\n    go.Bar(name='Female', x=classes, y=female_classes),\n    go.Bar(name='Male', x=classes, y=male_classes)])\nfig.update_layout(barmode='stack', width=400, height=400, title=\"Class and Sex of Survivors Ratios\")\nfig.show()","efb6f5ba":"s_port = survivors[survivors['Embarked'] == 'S']\nc_port = survivors[survivors['Embarked'] == 'C']\nq_port = survivors[survivors['Embarked'] == 'Q']\n\ns_classes = s_port['Pclass'].value_counts(sort=False, normalize=True).to_list()\nc_classes = c_port['Pclass'].value_counts(sort=False, normalize=True).to_list()\nq_classes = q_port['Pclass'].value_counts(sort=False, normalize=True).to_list()\n\nfig = go.Figure(data=[\n    go.Bar(name='Southampton', x=classes, y=s_classes),\n    go.Bar(name='Cherbourg', x=classes, y=c_classes),\n    go.Bar(name='Queenstown', x=classes, y=q_classes)])\nfig.update_layout(barmode='stack', width=450, height=400, title=\"Class and Embarking Port of Survivors Ratios\")\nfig.show()","e8f2064e":"title_counts_survived = train[train['Survived'] == 1]['Title'].value_counts()\ntitle_counts_dead = train[train['Survived'] == 0]['Title'].value_counts()\ntitles = list(title_counts_survived.index)\n\ndel train['Name']\n\nfig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}]])\n\nfig.add_trace(\n    go.Pie(labels=titles, values=title_counts_survived, title='Survivor Titles'),\n    row=1, col=1,\n)\nfig.add_trace(\n    go.Pie(labels=titles, values=title_counts_dead, title='Non-Survivor Titles'),\n    row=1, col=2\n)\n\nfig.update_layout(width=600, height=400, title_text=\"Titles of Survivors vs. Non-Survivors\")\nfig.show()\n","25105518":"fig = px.histogram(train, x='Age', y='Survived', color='Survived', marginal='box', opacity=0.75, \n                   hover_data=train.columns, title='Ages of Survived and Dead Groups')\nfig.update_layout(width=700, height=400)\nfig.show()","0abcd47f":"fig = px.histogram(train, x='Survived', y='Family', color='Survived', marginal='box', opacity=0.75, \n                   hover_data=train.columns, orientation='h', title='Number of Family Members Aboard for Survived and Dead Groups')\nfig.update_layout(width=700, height=400)\nfig.show()","9f21bb02":"fig = px.histogram(train, x='Fare', y='Survived', color='Survived', marginal='box', opacity=0.75,\n                  hover_data=train.columns, title='Fare Distribution Among Survivors and Non-Survivors')\nfig.update_layout(width=700, height=400)\nfig.show()","dcfb53ff":"titanic_dummies = pd.get_dummies(train, columns=['Pclass', 'Sex', 'Embarked', 'Title'], prefix=['Class', 'Sex', 'Port', 'Title'])\ntitanic_dummies.head(5)","0ab63b7e":"titanic_dummies[['Age', 'Family', 'Fare']] = titanic_dummies[['Age', 'Family', 'Fare']].apply(lambda x: (x - np.mean(x)) \/ (np.max(x) - np.min(x)))\ntitanic_dummies","44a7bbea":"sel = VarianceThreshold(threshold=0.8 * (1 - 0.8))\nsel.fit_transform(titanic_dummies)\nfitted = titanic_dummies[titanic_dummies.columns[sel.get_support(indices=True)]]\nfitted.head(5)","00828623":"print('Original DF shape vs feature-selected DF shape: ' + str(titanic_dummies.shape) + ', ' + str(fitted.shape))","e4a40f96":"SVC_classifier = SVC(kernel='linear')\nfeatures = fitted[fitted.columns[1:]]\nlabel = fitted[fitted.columns[0]]\n\nX_train, X_test, Y_train, Y_test = train_test_split(features, label, test_size=0.2)\nSVC_classifier.fit(X_train, Y_train)","6fca82bc":"y_pred = SVC_classifier.predict(X_test)\ny_pred","df1e8552":"def cross_val(model, X_test, Y_test, cv):\n    cross_val_scores = cross_val_score(model, X_test, Y_test, cv=cv)\n    print(\"10-Fold Cross Validation Scores: \" + str(list(cross_val_scores)))\n    print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (cross_val_scores.mean(), cross_val_scores.std() * 2))\n    \ncross_val(SVC_classifier, X_test, Y_test, 10)","cbe1601e":"tn, fp, fn, tp = confusion_matrix(Y_test, y_pred).ravel()\nprint((tn, fp, fn, tp))\n\ndef plot_confusion_matrix(Y_true, Y_pred):\n    cm = list(confusion_matrix(Y_true, Y_pred))\n    x = ['Pred. Not Survived', 'Pred. Survived']\n    y = ['Not Survived', 'Survived']\n    cm_text = [['TN', 'FP'], ['FN', 'TP']]\n    fig = ff.create_annotated_heatmap(cm, x=x, y=y, annotation_text=cm_text, colorscale=\"aggrnyl\")\n    fig.update_layout(title=\"Confusion Matrix\", width=400, height=400)\n    fig.show()\n\nplot_confusion_matrix(Y_test, y_pred)","6a4adc79":"print(\"Model accuracy score from Conf. Matrix: \" + str((tp + tn) \/ float(tp + tn + fp + fn)))\nprint(\"True accuracy score: \" + str(accuracy_score(Y_test, y_pred)))","264424fb":"sensitivity = tp \/ float(fn + tp) # These are all positive.\n\nprint(\"Recall score: \" + str(sensitivity))","647cb0e7":"specificity = tn \/ float(tn + fp) # These are all negative.\n\nprint(\"Specificty score: \" + str(specificity))","ddef9bdf":"optimal_k = int(round(np.sqrt(len(X_train))))\n\nneigh = KNeighborsClassifier(n_neighbors=optimal_k)\nneigh.fit(X_train, Y_train)\n\ny_pred_knn = neigh.predict(X_test)\ny_pred_knn","cb953acd":"cross_val(neigh, X_test, Y_test, 10)","20e7df13":"plot_confusion_matrix(Y_test, y_pred_knn)","ca506ff6":"def get_confusion_metrics(Y_true, Y_pred):\n    tn, fp, fn, tp = confusion_matrix(Y_true, Y_pred).ravel()\n    print(\"Model accuracy score from Conf. Matrix: \" + str((tp + tn) \/ float(tp + tn + fp + fn)))\n    print(\"True accuracy score: \" + str(accuracy_score(Y_true, Y_pred)))\n    \n    sensitivity = tp \/ float(fn + tp)\n    print(\"Recall score: \" + str(sensitivity))\n    \n    specificity = tn \/ float(tn + fp)\n    print(\"Specificty score: \" + str(specificity))\n    \nget_confusion_metrics(Y_test, y_pred_knn)","90361392":"neigh3 = KNeighborsClassifier(n_neighbors=3)\nneigh3.fit(X_train, Y_train)\n\ny_pred_knn3 = neigh3.predict(X_test)\ny_pred_knn3","e71db54d":"cross_val(neigh3, X_test, Y_test, 10)","2d14e065":"get_confusion_metrics(Y_test, y_pred_knn3)","925aeedb":"features_list = list(fitted.columns[1:])\nvals = fitted.loc[:, features_list].values\nvals = StandardScaler().fit_transform(vals)\n\n# Now each value in the feature dataset is standardized!\nvals","d09ee587":"standardized_fitted = pd.DataFrame(vals, columns=features_list)\nstandardized_fitted.head(5)","dcb46436":"pca = PCA().fit(standardized_fitted)\n\nxi = np.arange(1, 9, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nfig = px.line(x=xi, y=y,)\nfig.add_trace(\n    go.Scatter(\n        mode='markers',\n        x=[5],\n        y=[0.985374],\n        marker=dict(\n            color='red',\n            size=10,\n            opacity=0.5\n        ),\n        showlegend=False\n    )\n)\nfig.update_layout(width=600, height=400, xaxis_title='# Components', yaxis_title='PC Variance for Whole Dataset', title='PCA Explained Variance Ratio for Fitted Data')\nfig.show()","83bfb52d":"pca_titanic = PCA(n_components=7)\nprincipal_components_titanic = pca_titanic.fit_transform(standardized_fitted)\n\npc_columns = ['PC' + str(i + 1) for i in range(7)]\n\nprincipal_components_df = pd.DataFrame(data=principal_components_titanic,\n                                      columns=pc_columns)\nprincipal_components_df.insert(0, 'Survived', label)\nprincipal_components_df","3ed17a9e":"print('Explained variation per principal component: {}'.format(pca_titanic.explained_variance_ratio_))","920ad92d":"X_train_pca = pca_titanic.fit_transform(X_train)\nX_test_pca = pca_titanic.fit_transform(X_test)\n\nneigh_pca = KNeighborsClassifier(n_neighbors=optimal_k)\nneigh_pca.fit(X_train_pca, Y_train)\n\ny_pred_knn_pca = neigh_pca.predict(X_test_pca)\ny_pred_knn_pca\n","72f7bcb6":"cross_val(neigh_pca, X_test_pca, Y_test, 10)","3f528867":"get_confusion_metrics(Y_test, y_pred_knn_pca)","ee07e0f8":"param_grid = [{'kernel': ['poly'], 'C': [0.01, 0.1, 1, 10, 100, 1000], 'degree': [2, 3, 4]},\n             {'kernel': ['rbf'], 'C': [0.01, 0.1, 1, 10, 100, 1000], 'gamma': [1e-3, 1e-4, 1e-6, 1e-8]},\n             {'kernel': ['sigmoid'], 'C': [0.01, 0.1, 1, 10, 100, 1000], 'gamma': [1e-3, 1e-4]}]\n\nclf = GridSearchCV(SVC(), param_grid, scoring='recall')\nclf.fit(X_train, Y_train)\nprint(clf.best_params_)","c258cb32":"# These were the best params from our grid search.\nsvc_nl = SVC(kernel='poly', degree=3, C=0.01)\nsvc_nl.fit(X_train, Y_train)","0a480262":"y_pred_svc_nl = svc_nl.predict(X_test)\ny_pred_svc_nl","8f36fb83":"plot_confusion_matrix(Y_test, y_pred_svc_nl)","e46d7f9c":"get_confusion_metrics(Y_test, y_pred_svc_nl)","0d2cd5ed":"Our KNN model is moderately sensitive and highly specific. Let's try decreasing our number of neighbors. We'll use 5.","da134c1c":"##### True Positive Rate\n\nThis is a straightfoward metric. Now, we'll look at something a little more interesting: **sensitivity**, which we want to maximize. When the true value is positive, how often is the prediction correct? This is also known as the \"true positive rate\" or \"recall\". It is calculated as follows:","9856efff":"There is indeeed a stark contrast: you were much less likely to survive if your title was \"Miss.\"\n\nPlotly's overlaid histograms don't seem to be supported on Kaggle, but the shape of the age distribution for non-survivors is lower but pretty similar to the one for survivors.","4d00f813":"### Evaluate Again\nThe results barely change!","e7fb4a13":"# Model 3\n## Non-Linear Support Vector Classifer\nLet's see if we can get our SVM to perform even better by changing its kernel function, out of which there are three other possibilities: polynomial, Radial Basis Function (rbf), and sigmoid.\n\n![SVC](https:\/\/miro.medium.com\/max\/1922\/1*Ha7EfcfB5mY2RIKsXaTRkA.png)\nSince training all of three of these kernels to get the best accuracy possible will wind up to be cumbersome, I'll grid search through sets of possible values for *C*, *gamma*, and the kernel function, using `GridSearchCV()` from `sklearn`. Here's a brief breakdown of the parameters passed to the `SVC()` instance.\n* **C** - The larger the value for C, the smaller the margin, and the smaller the value, the larger the margin. Recall that we want to maximize the margin in order to better generalize to our testing data. Therefore, this value controls the trade-off between a low training error and a low testing error.\n* **gamma** - How far the influence of a single training example reaches. Higher value for gamma => points closer to the decision boundary carry more weight, so the decision boundary is more of a wiggly line.\n* **degree** - Degree of the polynomial kernel function\n\nHopefully this gives an intuitive overview; grid searching is the most common approach to finding the best hyperparameters, as there are no established rules to picking *C*, for example. The code took a very long time to run so I commented it out.","15e1d90c":"Let's look at some summary statistics for our data.","95da09ff":"# Some Essential Info About the Survivors\nThis may help discover relationships and will familiarize us with the data.","833cba4f":"# Model 1\n## Linear Support Vector Classifier\nNow we're ready to pass our data into classification models. The first one we will employ is the linear SVC, since we know that it is effective for high-dimensional data. It's also memory-efficient! We will be going with a simple binary linear classifier.\n\nThe underlying idea of the SVM is to find the hyperplane that best differentiates two classes. Support vectors are the coordinates of each observation in the *n*-dimensional space. We also want the distance between the hyperplane and the nearest data point (called the margin) to be maximized (note that the SVM selects the hyperplane that **best segregates the two classes** prior to maximizing the margin). \n### Example SVC Visual\n![scikit-learn.org\/stable\/_images\/sphx_glr_plot_iris_svc_0011.png](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_iris_svc_0011.png)","a2a96cbb":"### Evaluating the Model\nWe will be utilizing model evaluation metrics to quantify the performance of our linear SVC on the testing dataset we created.\n#### K-Fold Cross Validation to Get Accuracy\nSystemically create *k* train\/test splits and average the results. This is similar to bootstrapping. ","0a0a7b62":"Since this number of dimensions is still pretty high for KNN, I'm going to predict that we won't see a marked increase in the accuracy, but let's test it out! We'll have to go through the process of train-test splitting again.","2ba28ddb":"Predict the label for the testing features. This `predict` function will return an array.","d14fe172":"# \ud83d\udef3 Titanic EDA + Elementary Machine Learning Models\n## [Work in Progress]\nThis is my first proper notebook uploaded to this site. Any insight and feedback is appreciated. All visualizations will be produced with the Plotly library. I am a beginner at this and this is my first comprehensive entry to a Kaggle competition, but I hope it helps somewhat.\n\nFirst, as a preprocessing measure, I decided to combine the `SibSp` and `Parch` columns into one that describes the number of family members an individual had aboard. Missing\/`NaN` values were also taken care of with the `interpolate()` function provided by pandas, and the method used is `linear`.","dc6acad8":"We achieved a pretty decent accuracy. More notably, our sensitivity score increased a little more compared to that of other models. ","31dd9f86":"# Visualizing this Info\nFirst, I'll create a stacked bar graph displaying the class and sex of those who survived. It is no surprise that the majority of survivors were first class, and that there is an even distribution between males and females. ","85ada26f":"#### Compute Metrics from this Confusion Matrix","801fd858":"### It was that easy. Now let's do our evaluations.","5e4b4786":"As the accuracy doesn't significantly change with a lower value for *k*, we might not always achieve peak accuracy by picking *k* to the the square root of the length of our data.\n \nSince we know now that KNN doesn't work with high-dimensional data, let's try reducing the dimensions of our training dataset. We will accomplish this through PCA. Standardization will be accomplished within that step, so we don't have to worry about that.\n\n### Dimensionality Reduction\n#### Standardization\nWe must begin by **standardizing** the data, since standardization will project our original data onto directions which maximize the variance. ","1cefbbb9":"There also don't seem to be any key discrepancies between the number of family members on board for survivors and non-survivors, which is mildly surprising.","6dea1423":"We can expect that the one guy who paid way more for this than everybody else did survive.","6c6db8f4":"Now, we have a 6-dimensional DataFrame, and our KNN classifier should perform better. For the sake of curiosity, I'm going to get the explained variance per principal component.","96918831":"Running PCA wasn't a **necessity** since we already applied feature selection before and it doesn't have an effect on accuracy and only improves computing time. PCA is lossy because it has the possibly of changing our data space drastically. With calculation time in mind, we can proceed with our 6-dimensional data, but we're looking for accuracy. We've already cut the number of dimensions by more than half since we started.\n\n##### Let's recap...\nOn average, it's fair to say that our linear SVC is doing around the same with this dataset as the KNN classifier.","2d1dabb4":"# Model 2\n## K-Nearest-Neighbors Classifier\nLike the SVC, K-Nearest-Neighbors is more commonly used in classification, and it's quite simple to understand. It makes its estimation for a the class a data point belongs to based on the classes of the data points that surround it. When all the closest data points to an observation belong to a certain class, we can say with a good confidence level that observation also belongs to that class.\n\nHowever, a drawback of this model is that its calculation time increases significantly with a greater number of dimensions. Linear SVC, on the other hand, has the benefit of working well with high-dimensional data.\n\n![KNN](https:\/\/www.analyticsvidhya.com\/wp-content\/uploads\/2014\/10\/K-judgement.png)\n\nAs far as choosing *k* goes, a common practice is to either choose an odd number or *sqrt(n)*, where *n* is the total number of observations.\n","30b5661d":"# Preparing the Data for a Machine Learning Model and Feature Selection\n## Dummy Data\nNow, we want to get our data in a proper format before we feed it into a model. Clearly this will be cumbersome to tackle with all the categorical features we have. One approach to address this is generating **dummy data**. The pandas `get_dummies()` function will use one-hot encoding to convert the categorical data, so new columns will be generated. We will add prefixes to make our columns more readable.","2e84e4a8":"## Feature Selection\nWe will use this feature selector to remove all low-variance features, i.e. features whose variance doesn't meet a given threshold. This is good because we don't want unwanted noise prior to our prediction. By default, VarianceThreshold removes all features that have 0 variance, or have the same values in all samples. Since we have all boolean features, we can remove all features that are either one or zero in 80% or more in all of the samples. The variance of boolean features is given by: *Var(x) = p(1 - p)*, and this is the threshold we will use for our selector. \n`get_support(indices=True)` will maintain the column names in our new generated data.","71a252ae":"There we have it. Our ideal number of components for PCA will be 6.","2540751b":"# Adding a New Important Feature: Titles\nWe want to see if the title of an individual had an effect on their survivability, so we want a column with that information. I'll define a function to get the title of a passenger and generalize it to a more common title.","8b488728":"### Evaluating Our RBF Model","25523a55":"Now, we have our standardized features in a tabular format. We now need to project our 9-dimensional data to *n*-dimensional **principal components**. Before we do that, however, we need to pick *n*, which should be the value at which the inflection point occurs between the principal components accounting for a large amount of the variance and 100% of the variance. We will use `explained_variance_ratio_` to do this, and we'll plot it.","c18eb6a1":"Next, let's look at the distribution of titles for survivors vs. non-survivors. I predict that we may see a discrepancy.","35b0cd1b":"##### True Negative Rate\nNext, calculate the **specificity score**. This is the opposite of the sensitivity score. When the true value is negative, how often is the prediction correct? How \"specific\" is the classifier in predicting for positive true values?","29c368d5":"Below, we can see that our feature selector removed seven columns, namely `Port_Q`, `Port_C`, `Title_Master`, and `Title_Mrs`, `Age`, `Family`, and `Fare`.","96588107":"This one is a bit more interesting. Queenstown embarkers make up an overwhelming amount of the third class. Queenstown was one of the major transatlantic ports. Most of the first class members are from Cherbourg, located in France. The distribution of classes among Southampton embarkers is relatively uniform.","92ee2251":"Our classifier is highly moderately sensitive and highly specific. ","fddd33b9":"### Best Parameters for Recall Score: RBF Kernel, C = 10, Gamma = 0.001","bfc46fc5":"## Normalize the Data","1a839654":"#### Plot Confusion Matrix\nFinally, let's plot the confusion matrix for our model. \n* Every observation in our dataset is presented in exactly one box.\n* It is a 2x2 matrix because there are two response classes. **","90531174":"Finally, we can split our training and testing data and fit our model."}}