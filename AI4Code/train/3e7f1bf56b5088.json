{"cell_type":{"51d0f8c2":"code","7585986b":"code","88924758":"code","ba47ffcc":"code","107d049b":"code","fa7454d8":"code","946a9a38":"code","021e315f":"code","43d1b4cb":"code","044f57a5":"code","2fccb2e7":"code","a3eafe14":"code","db58f45e":"markdown","a6fe9e29":"markdown","f96ad8d1":"markdown","cfa2edfb":"markdown","879fffa8":"markdown","90ac2e24":"markdown","6cf82218":"markdown","89128582":"markdown","b4598a05":"markdown","ae0491a2":"markdown","25e817ed":"markdown"},"source":{"51d0f8c2":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# for preprocessing the data:\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom sklearn.model_selection import train_test_split\n\n# importing the neural network libraries:\nfrom keras.optimizers import *\nfrom keras.losses import binary_crossentropy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# importing a classifier from xgboost:\nfrom xgboost import XGBClassifier\n\n# importing metrics to measure our accuracy:\nfrom sklearn.metrics import accuracy_score","7585986b":"data = pd.read_csv(\"..\/input\/tictactoe\/tic-tac-toe.csv\")\ndata.head()","88924758":"data.isnull().sum()","ba47ffcc":"y = data['class']\ndata.drop(['class'], inplace=True, axis=1)","107d049b":"label = LabelEncoder()\n\ny = label.fit_transform(y)","fa7454d8":"cbe = CatBoostEncoder()\ndata = cbe.fit_transform(data, y)","946a9a38":"train, test, ytrain, ytest = train_test_split(data, y,\n                                              test_size=0.4, train_size=0.6)","021e315f":"model = Sequential([\n    Dense(256, activation='relu', input_shape=(9,)),\n    Dense(128, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(8, activation='relu'),\n    Dense(1, activation='sigmoid')\n])","43d1b4cb":"model.compile(metrics=['accuracy'], loss='binary_crossentropy', optimizer='Adam')","044f57a5":"model.fit(train, ytrain, epochs=40,\n          validation_data=(test, ytest))","2fccb2e7":"xg = XGBClassifier(n_estimators=350)\n\nxg.fit(train, ytrain)\n\nxgPreds = xg.predict(test)","a3eafe14":"accuracy_score(xgPreds, ytest)","db58f45e":"Defining the labels to predict on:","a6fe9e29":"Encoding the labels or converting them to numerical form since categorical values cannot be used in algorithms:","f96ad8d1":"Lets try predicting with XGBoost Classifier now:","cfa2edfb":"# Importing Libraries:","879fffa8":"I think that 98% accuracy on a classifier is descent enough! (pun intended)","90ac2e24":"Dividing the dataset into training and test sets:","6cf82218":"Concluding with fitting the neural network, I can say that it is good but we can do better since in my opinion a 1000 examples aren't exactly enough to train a plain artificial neural network with and get tinkerable results (pardon me if I am wrong I am new to this too!). The accuracy we got is almost 95% on the training set while the accuracy on the test set is 96% . ","89128582":"Reading the data:","b4598a05":"# Preprocessing the Data:","ae0491a2":"# Training and Predicting:","25e817ed":"Checking if there are any NaN values:"}}