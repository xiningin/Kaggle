{"cell_type":{"159604e4":"code","bcdf3d72":"code","d20dc584":"code","623a8f20":"code","f354bb1c":"code","908b9ffe":"code","6105432c":"code","c47c35ae":"code","6f9faacb":"code","5b96bcdb":"code","66589c2b":"code","e4530656":"code","ec22869d":"code","258600fe":"code","d5a30ab7":"code","49c2ea5c":"code","48a79600":"code","b8d38d44":"code","1fd597ba":"code","8fd953b3":"code","5edad847":"code","19a34181":"code","1224a4d3":"code","eb2f5d4a":"code","dd8e37d4":"code","4ca1ebcc":"code","a2ee62dd":"code","99015d01":"code","e1220131":"code","1c3c8fea":"code","67481095":"code","a9ea868b":"code","2b01ab86":"code","401d231e":"code","0707659b":"code","61cd1b2d":"code","cd36192e":"code","bb3a1c1b":"code","54f3714c":"code","efcf1914":"code","6adac95f":"code","6c0930d1":"code","c4ac558f":"code","c93be0fc":"code","b14c9990":"code","eafe54d1":"code","aad1710e":"code","eadca074":"code","09f4dfe2":"code","991dcc8b":"code","19956201":"code","feef7a08":"code","78cbc94f":"code","e3d48a58":"code","d831c373":"code","e750ea09":"code","e011b1a0":"code","3803ec57":"code","2616b3c0":"code","c91a774b":"code","4c8309f9":"code","c9f53865":"code","1096033e":"code","aeb70183":"code","49afa527":"code","bc1f3fb6":"code","cd3acfe9":"code","665fa330":"code","e7dc4d5c":"code","581e9c8a":"code","dadc36c8":"code","078ae393":"code","8863009a":"code","422e4295":"code","f23d4d68":"code","bab358ce":"code","05fc6257":"code","e0b5da4f":"code","9d26682a":"code","51c3bd49":"code","d42e5441":"code","5aaeda65":"code","0da24a0d":"code","927f4e1a":"code","4b892b29":"code","3929fcb4":"code","118d63d2":"code","f2d1c0de":"code","ba0bd937":"code","b0ba8ccd":"code","368be4dd":"code","a1704506":"code","263afacf":"code","9068d1a8":"code","e201d82e":"code","ef70ddec":"code","88df388a":"code","e11ace1d":"code","d73073da":"code","01e9f561":"code","99ae32e5":"markdown","5877f1af":"markdown","d1befc7b":"markdown","f3a57616":"markdown","104c3652":"markdown","8cf62b86":"markdown","444b4f87":"markdown","9ca4ca71":"markdown","ec723c34":"markdown","a75f9ca0":"markdown","32d59f95":"markdown","61945baa":"markdown","481b791a":"markdown","a2793633":"markdown","7d8cdd5b":"markdown","c9567bd2":"markdown","b14e63d9":"markdown","dfcc10c4":"markdown","2236b0b2":"markdown","d83dadce":"markdown","382e0539":"markdown","d828ada1":"markdown","080f7035":"markdown","b539a3a8":"markdown","d25a55b5":"markdown","6bd3c8f5":"markdown","4db643e3":"markdown","af9a229f":"markdown"},"source":{"159604e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bcdf3d72":"df = pd.read_csv('\/kaggle\/input\/health-insurance-prediction\/train_Df64byy.csv')","d20dc584":"\nprint(df.head(5))\n","623a8f20":"df.columns","f354bb1c":"df.info()","908b9ffe":"print(\"Percentage of missing data in column Holding_Policy_Type is {}\".format((df['Holding_Policy_Type'].isnull().sum()\/df.shape[0]) * 100))\nprint(\"Percentage of missing data in column Holding_Policy_Duration is {}\".format((df['Holding_Policy_Duration'].isnull().sum()\/df.shape[0]) * 100))\nprint(\"Percentage of missing data in column Health Indicator is {}\".format((df['Health Indicator'].isnull().sum()\/df.shape[0]) * 100))","6105432c":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputed = imputer.fit_transform(df)\ndf_imputed = pd.DataFrame(imputed)\ndf_imputed.isnull().sum()\ndf_imputed.columns = ['ID', 'City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Upper_Age', 'Lower_Age', 'Is_Spouse',\n       'Health Indicator', 'Holding_Policy_Duration', 'Holding_Policy_Type',\n       'Reco_Policy_Cat', 'Reco_Policy_Premium', 'Response']\ndf_imputed","c47c35ae":"df_imputed.describe()","6f9faacb":"import seaborn as sns\nsns.displot(df_imputed['Region_Code'])","5b96bcdb":"df_imputed.groupby('City_Code').count()\n","66589c2b":"sns.displot(df_imputed['Accomodation_Type'],color='red')","e4530656":"\nsns.displot(df_imputed['Reco_Insurance_Type'],color='green')","ec22869d":"sns.displot(df_imputed['Upper_Age'],color='blue')","258600fe":"sns.displot(df_imputed['Upper_Age'],color='yellow')","d5a30ab7":"df_joint = df_imputed[df_imputed['Reco_Insurance_Type'] == 'Joint']\ndf_joint.groupby(by='Is_Spouse').agg('count')","49c2ea5c":"df_imputed.groupby(by='Response').agg('count')","48a79600":"df_imputed['Health Indicator'].unique()","b8d38d44":"df_grouped_by_health = df_imputed.groupby(by='Health Indicator').agg('count')","1fd597ba":"df_grouped_by_health['ID'].plot()","8fd953b3":"\ndf_imputed['Holding_Policy_Duration'].unique()","5edad847":"df_grouped_by_health = df_imputed.groupby(by='Holding_Policy_Duration').agg('count')\nprint(df_grouped_by_health['ID'])","19a34181":"\ndf_imputed['Reco_Policy_Cat'].unique()","1224a4d3":"df_grouped_by_holding = df_imputed.groupby(by='Reco_Policy_Cat').agg('count')\ndf_grouped_by_holding['ID'].plot()","eb2f5d4a":"\ndf_imputed['Reco_Policy_Premium'].unique()","dd8e37d4":"df_grouped_by_premium = df_imputed.groupby(by='Reco_Policy_Premium').agg('count')\ndf_grouped_by_premium['ID'].plot()","4ca1ebcc":"\nX = df_imputed.drop(columns= ['Response'],axis = 1)\nY = df_imputed['Response']\nY=Y.astype('int')","a2ee62dd":"from imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nundersample = RandomUnderSampler(sampling_strategy='majority')\nX_under, y_under = undersample.fit_resample(X, Y)\nprint(Counter(y_under))","99015d01":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(X_under,y_under,test_size=0.1)","e1220131":"# Labelencoding\nle = LabelEncoder()\nvar_mod = X_train.select_dtypes(include='object').columns\nfor i in var_mod:\n    X_train[i] = le.fit_transform(X_train[i])\n    \nfor i in var_mod:\n    X_test[i] = le.fit_transform(X_test[i])","1c3c8fea":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel = LogisticRegression(random_state = 0, max_iter = 1500)\nparameters = {'C':[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\nclf = RandomizedSearchCV(model, parameters, cv=3, scoring='roc_auc')\nclf.fit(X_train, Y_train)","67481095":"clf.best_params_","a9ea868b":"model = LogisticRegression(random_state = 0, max_iter = 1500, C = 5)\nmodel.fit(X_train, Y_train)","2b01ab86":"y_test_pred = model.predict_proba(X_test)[:,1]","401d231e":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","0707659b":"df_test = pd.read_csv('\/kaggle\/input\/test-data\/test_YCcRUnU.csv')\ndf_test.head(5)\n","61cd1b2d":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputed = imputer.fit_transform(df_test)\ndf_imputed_test = pd.DataFrame(imputed)\ndf_imputed_test.isnull().sum()\ndf_imputed_test.columns = ['ID', 'City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Upper_Age', 'Lower_Age', 'Is_Spouse',\n       'Health Indicator', 'Holding_Policy_Duration', 'Holding_Policy_Type',\n       'Reco_Policy_Cat', 'Reco_Policy_Premium']\ndf_imputed_test","cd36192e":"# Labelencoding\nle = LabelEncoder()\nvar_mod = df_imputed_test.select_dtypes(include='object').columns\nfor i in var_mod:\n    df_imputed_test[i] = le.fit_transform(df_imputed_test[i])","bb3a1c1b":"y_test = model.predict_proba(df_imputed_test)[:,1]\n","54f3714c":"print(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test","efcf1914":"print(output.head(5))\n","6adac95f":"output.to_csv('SubmissionFile_LR.csv',index=False)","6c0930d1":"from imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\noversample = RandomOverSampler(sampling_strategy='minority')\nX_over, y_over = oversample.fit_resample(X, Y)\nprint(Counter(y_over))","c4ac558f":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(X_over,y_over,test_size=0.1)","c93be0fc":"# Labelencoding\nle = LabelEncoder()\nvar_mod = X_train.select_dtypes(include='object').columns\nfor i in var_mod:\n    X_train[i] = le.fit_transform(X_train[i])\n    \nfor i in var_mod:\n    X_test[i] = le.fit_transform(X_test[i])","b14c9990":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel = LogisticRegression(random_state = 0, max_iter = 1500)\nparameters = {'C':[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\nclf = RandomizedSearchCV(model, parameters, cv=3, scoring='roc_auc')\nclf.fit(X_train, Y_train)","eafe54d1":"clf.best_params_","aad1710e":"model = LogisticRegression(random_state = 0, max_iter = 1500, C = 0.01)\nmodel.fit(X_train, Y_train)","eadca074":"y_test_pred = model.predict_proba(X_test)[:,1]","09f4dfe2":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","991dcc8b":"y_test = model.predict_proba(df_imputed_test)[:,1]","19956201":"\nprint(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test\n","feef7a08":"output.to_csv('SubmissionFile_LR_with_oversampling.csv',index=False)","78cbc94f":"# Labelencoding\nle = LabelEncoder()\nvar_mod = df.select_dtypes(include='object').columns\nfor i in var_mod:\n    df[i] = le.fit_transform(df[i])","e3d48a58":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='constant')\nimputed = imputer.fit_transform(df)\ndf_imputed = pd.DataFrame(imputed)\ndf_imputed.isnull().sum()\ndf_imputed.columns = ['ID', 'City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Upper_Age', 'Lower_Age', 'Is_Spouse',\n       'Health Indicator', 'Holding_Policy_Duration', 'Holding_Policy_Type',\n       'Reco_Policy_Cat', 'Reco_Policy_Premium', 'Response']\ndf_imputed","d831c373":"\nX = df_imputed.drop(columns= ['Response'],axis = 1)\nY = df_imputed['Response']\nY=Y.astype('int')","e750ea09":"from imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\noversample = RandomOverSampler(sampling_strategy='minority')\nX_over, y_over = oversample.fit_resample(X, Y)\nprint(Counter(y_over))","e011b1a0":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(X_over,y_over,test_size=0.1)","3803ec57":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\nmodel = LogisticRegression(random_state = 0, max_iter = 1500)\nparameters = {'C':[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\nclf = RandomizedSearchCV(model, parameters, cv=3, scoring='roc_auc')\nclf.fit(X_train, Y_train)","2616b3c0":"clf.best_params_","c91a774b":"model = LogisticRegression(random_state = 0, max_iter = 1500, C = 5)\nmodel.fit(X_train, Y_train)","4c8309f9":"y_test_pred = model.predict_proba(X_test)[:,1]","c9f53865":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","1096033e":"y_test = model.predict_proba(df_imputed_test)[:,1]\nprint(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test\noutput.to_csv('SubmissionFile_LR_with_oversampling_constant.csv',index=False)","aeb70183":"from sklearn.ensemble import RandomForestClassifier\nrandom_f = RandomForestClassifier(n_estimators=100,max_features = \"log2\")\nrandom_f.fit(X_train, Y_train)","49afa527":"random_f.feature_importances_","bc1f3fb6":"sorted_idx = random_f.feature_importances_.argsort()\nplt.barh(X_train.columns[sorted_idx], random_f.feature_importances_[sorted_idx])\nplt.xlabel(\"Random Forest Feature Importance\")","cd3acfe9":"feature_selected = pd.DataFrame(X_train[['Region_Code','Reco_Policy_Premium','Reco_Policy_Cat','Lower_Age','Upper_Age','City_Code']])","665fa330":"random_f_select = RandomForestClassifier(n_estimators=300,max_features = \"log2\")\nrandom_f_select.fit(feature_selected, Y_train)","e7dc4d5c":"log_regression_select = LogisticRegression(random_state = 0, max_iter = 1500, C = 0.001)\nlog_regression_select.fit(feature_selected, Y_train)","581e9c8a":"test_cv = pd.DataFrame(X_test[['Region_Code','Reco_Policy_Premium','Reco_Policy_Cat','Lower_Age','Upper_Age','City_Code']])","dadc36c8":"y_test_pred = random_f_select.predict_proba(test_cv)[:,1]","078ae393":"y_test_pred = log_regression_select.predict_proba(test_cv)[:,1]","8863009a":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","422e4295":"final_test = pd.DataFrame(df_imputed_test[['Region_Code','Reco_Policy_Premium','Reco_Policy_Cat','Lower_Age','Upper_Age','City_Code']])","f23d4d68":"y_test = random_f_select.predict_proba(final_test)[:,1]\nprint(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test\noutput.to_csv('SubmissionFile.csv',index=False)","bab358ce":"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import StackingClassifier\n\nestimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),('svr', make_pipeline(StandardScaler(),LinearSVC(random_state=42)))]\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n\nclf.fit(feature_selected, Y_train)","05fc6257":"y_test_pred = clf.predict_proba(test_cv)[:,1]\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","e0b5da4f":"y_test = clf.predict_proba(final_test)[:,1]\nprint(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test\noutput.to_csv('SubmissionFile.csv',index=False)","9d26682a":"df_imputed_X = df_imputed.drop(columns= ['Response'],axis = 1)\nY_train_ft = df_imputed['Response']\nY_train_ft=Y_train_ft.astype('int')","51c3bd49":"import featuretools as ft\n# creating and entity set 'es'\nes = ft.EntitySet(id = 'entity')\n\n# adding a dataframe \nes.entity_from_dataframe(entity_id = 'policy', dataframe = df_imputed_X, index = 'ID')","d42e5441":"es = ft.EntitySet(id = 'entity')\n\n# adding a dataframe \nes.entity_from_dataframe(entity_id = 'test', dataframe = df_imputed_test, index = 'ID')","5aaeda65":"es.normalize_entity(base_entity_id='policy', new_entity_id='predict', index = 'Reco_Policy_Cat')","0da24a0d":"es.normalize_entity(base_entity_id='test', new_entity_id='predict', index = 'Reco_Policy_Cat')","927f4e1a":"feature_matrix_test, feature_names_test = ft.dfs(entityset=es, \ntarget_entity = 'test', \nmax_depth = 2, \nverbose = 1, \nn_jobs = 3)","4b892b29":"feature_matrix, feature_names = ft.dfs(entityset=es, \ntarget_entity = 'policy', \nmax_depth = 2, \nverbose = 1, \nn_jobs = 3)","3929fcb4":"feature_matrix.columns","118d63d2":"feature_matrix.head()","f2d1c0de":"feature_matrix = feature_matrix.reindex(index=df_imputed['ID'])\nfeature_matrix = feature_matrix.reset_index()","ba0bd937":"X_train_ft = feature_matrix","b0ba8ccd":"X_train,X_test,Y_train,Y_test = train_test_split(X_train_ft,Y_train_ft,test_size=0.1)","368be4dd":"X_test","a1704506":"X_train = X_train.drop(columns= ['ID'],axis = 1)\nX_test = X_test.drop(columns= ['ID'],axis = 1)","263afacf":"print(X_train.shape,feature_matrix_test.shape)","9068d1a8":"random_f_select_ft = RandomForestClassifier(n_estimators=100,max_features = \"log2\")\nrandom_f_select_ft.fit(X_train, Y_train)","e201d82e":"y_test_pred = random_f_select_ft.predict_proba(X_test)[:,1]\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","ef70ddec":"y_test = random_f_select_ft.predict_proba(feature_matrix_test)[:,1]\nprint(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test\noutput.to_csv('SubmissionFile.csv',index=False)","88df388a":"Y_train.shape","e11ace1d":"# GradientBoost Classification\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.ensemble import GradientBoostingClassifier\nseed = 7\nnum_trees = 30\nkfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\nmodel = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\nresults = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold)\nprint(results.mean())\nmodel.fit(X_train, Y_train)","d73073da":"y_test_pred = model.predict_proba(X_test)[:,1]\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\ntest_fpr, test_tpr, te_thresholds = roc_curve(Y_test, y_test_pred)\n#print(test_fpr, test_tpr, te_thresholds)\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"alpha: hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","01e9f561":"y_test = model.predict_proba(df_imputed_test)[:,1]\nprint(y_test)\nprint(type(y_test))\noutput = pd.DataFrame()\noutput['ID'] = df_test['ID']\noutput['Response'] = y_test\noutput.to_csv('SubmissionFile_Gradient_Boost.csv',index=False)","99ae32e5":"We have the above different types of health indicators.","5877f1af":"As the Is_Spouse column is to be considered only for 'Joint' policies, hence we have done a filter on the Reco_Insurance_Type column.\n\nFurther on we have 8422 people who are spouses in joint policy type and 1924 who are not.","d1befc7b":"Using Oversampling for balancing the dataset","f3a57616":"Using the constant imputing strategy","104c3652":"Imputing the missing values in test data as well","8cf62b86":"The general trend which can be observed from the above table is that as the number of years for holding the policy increases the number of people holding the policy decreases.\n\nWe do have a good number of people who have policy holding duration of 14+ years","444b4f87":"The data is highly imbalanced. As we have 38,673 people who did not show interest in the policy and 12209 who did.","9ca4ca71":"Using label encoder to encode categorical data","ec723c34":"Out of the total almost 4000 policies recommended are of the Individual type","a75f9ca0":"Let's check if we have any missing values","32d59f95":"The above is the distribution of field 'Reco_Policy_Cat'","61945baa":"This dataset is a part of AnalyticsVidhya Job-A-Thon event.\n\nAbout the data(https:\/\/datahack.analyticsvidhya.com\/contest\/job-a-thon)\n\nYour Client FinMan is a financial services company that provides various financial services like loan, investment funds, insurance etc. to its customers. FinMan wishes to cross-sell health insurance to the existing customers who may or may not hold insurance policies with the company. The company recommend health insurance to it's customers based on their profile once these customers land on the website. Customers might browse the recommended health insurance policy and consequently fill up a form to apply. When these customers fill-up the form, their Response towards the policy is considered positive and they are classified as a lead.\n\nOnce these leads are acquired, the sales advisors approach them to convert and thus the company can sell proposed health insurance to these leads in a more efficient manner.\n\nNow the company needs your help in building a model to predict whether the person will be interested in their proposed Health plan\/policy given the information about:\n\nDemographics (city, age, region etc.)\nInformation regarding holding policies of the customer\nRecommended Policy Information\n\n\n\n","481b791a":"We now perform a basic level of analysis on the data features to better understand our dataset.","a2793633":"We have a lot of people from the age group of 24-26 and 73-75","7d8cdd5b":"We see that we have most people with X1 indicator which dips in an order such that X1>X2>X3....X9","c9567bd2":"Creating X and Y","b14e63d9":"Hyperparameter Tuning for Logistic Regression using RandomizedSearchCV(due to memory constraints)","dfcc10c4":"We have missing values in Holding_Policy_Type, Holding_Policy_Duration, Health Indicator","2236b0b2":"The feature holding policy type can be NULL when either the customer did not take a policy or if the data has not been captured correctly. We impute these Null values with the majority value(mode) present in the feature. \n\n\nWe do not altogether drop the NULL values as they are a good percentage of our data.","d83dadce":"The data contains most people from the region codes between 0-1000 and least between 5000-6000 region codes","382e0539":"Checking balance of data","d828ada1":"Splitting into train and test datasets(to check AUC of our model)","080f7035":"Using Undersampling technique to balance the dataset.","b539a3a8":"We have maximum policies with premium less than 20000 ","d25a55b5":"For less than 14 years we have the year mentioned after which it is generalized as 14+ years of holding.\n\n","6bd3c8f5":"We have a lot of people from the age group of 24-26 and 73-75","4db643e3":"We have 36 different city codes in our training dataset, with most of the population coming from C1 followed by C2 and C3","af9a229f":"The data contains more people having a owned home than people with a rented home."}}