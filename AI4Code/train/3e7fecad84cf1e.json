{"cell_type":{"a8765f8a":"code","c3cde1ce":"code","ccbc9688":"code","9f083da0":"code","024e57d3":"code","19a1e295":"code","94371310":"code","f2b91ec2":"code","1724c1eb":"code","8c08b5f4":"code","3c9c2d4b":"code","94fbb387":"code","b34980a0":"code","77b6bb93":"code","9d8bbd3b":"code","3c0af165":"code","a4768c21":"code","7d772d55":"code","33d6dca4":"code","2d6e3e55":"markdown","621fefd6":"markdown","fcc29916":"markdown","8fc0c0b4":"markdown","5e83e941":"markdown","5cc00764":"markdown","e9365a09":"markdown","61ec1a6b":"markdown","0a91ac66":"markdown","5a60eb5a":"markdown","bf92e678":"markdown","dac3b048":"markdown","44babb25":"markdown","2ceb67ba":"markdown","2e8bad8c":"markdown","d81ae065":"markdown","deff022b":"markdown"},"source":{"a8765f8a":"import os\nimport tensorflow                as tf\nimport pandas                    as pd\nimport numpy                     as np\nimport seaborn                   as sns\nimport cv2                       as cv\nimport matplotlib.pyplot         as plt\nfrom tqdm.notebook               import tqdm\nfrom sklearn.cluster             import DBSCAN\nfrom sklearn.manifold            import TSNE\nfrom tensorflow.keras            import layers\nfrom sklearn.model_selection     import train_test_split\nfrom tensorflow.keras.models     import Sequential\nfrom tensorflow.keras.models     import Model\nfrom tensorflow.keras.layers     import Dense, Reshape, Flatten, Dropout, Input\nfrom tensorflow.keras.layers     import Conv2D, Conv2DTranspose, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\n\n\nplt.rc('figure',figsize=(16,8))\nsns.set_context('paper',font_scale=1.5)\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\nprint(tf.__version__)","c3cde1ce":"#GLOBAL VARS\nFOLDER_PATH     = '\/kaggle\/input\/6000-children-and-teen-book-covers\/BookCovers\/'\nN_IMAGES        = 6232\nLATENT_DIM      = 1024\nN_BATCHES       = 128\nN_EPOCHS        = 350\nBATCH_PER_EPOCH = N_IMAGES \/\/ N_BATCHES\n\n# detect and init the TPU\ntpu          = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n#Loading Images and Normalizing between 0 and 1\nimages       = np.array([cv.resize(plt.imread(FOLDER_PATH+i),(128,128),interpolation = cv.INTER_AREA)\/ 255.0 for i in os.listdir(FOLDER_PATH)])","ccbc9688":"_, axs  = plt.subplots(4, 4, figsize=(12, 12))\naxs     = axs.flatten()\n\nfor img, ax in zip(np.random.choice(np.arange(1,len(images)),4*4), axs):\n    ax.imshow(images[img])\n    \nplt.tight_layout()\nplt.show()","9f083da0":"with tpu_strategy.scope() as tpu:\n\n    Encoder = Sequential(name='Encoder')\n\n    Encoder.add( tf.keras.applications.VGG16(input_shape=(128,128,3),\n                                               include_top=False,\n                                               weights='imagenet'))\n    Encoder.layers[0].trainable = False\n\n    Encoder.add(Flatten())\n    Encoder.add(Dense(LATENT_DIM, activation='sigmoid'))","024e57d3":"with tpu_strategy.scope() as tpu:\n\n    Decoder = Sequential(name='Decoder')\n\n    Decoder.add(Dense(256*16*16, input_dim=LATENT_DIM)) \n    Decoder.add(LeakyReLU(alpha=0.2))\n    Decoder.add(Reshape((16, 16, 256)))\n\n    Decoder.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    Decoder.add(LeakyReLU(alpha=0.2))\n\n    Decoder.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    Decoder.add(LeakyReLU(alpha=0.2))\n\n    Decoder.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    Decoder.add(LeakyReLU(alpha=0.2))\n\n\n    Decoder.add(Conv2D(3, (8,8), activation='sigmoid', padding='same'))","19a1e295":"with tpu_strategy.scope() as tpu:\n\n    autoencoder = Sequential(name='autoencoder')\n    autoencoder.add(Encoder) \n    autoencoder.add(Decoder)\n    \n    # Loss functtion\n    def ssim_loss(y_true, y_pred):\n        SSIM = tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n        return (1-SSIM)\/2\n    \n    #Optimizer\n    opt = Adam(lr=0.0002, beta_1=0.8)\n\n    \n    autoencoder.compile(loss=ssim_loss, optimizer=opt)","94371310":"autoencoder.summary()","f2b91ec2":"autoencoder.fit(images, images,\n                epochs=150,\n                batch_size=16,\n                shuffle=True,verbose=0)","1724c1eb":"predictions = autoencoder.predict(images)\nR=np.random.choice(np.arange(1,len(images)),4*4)\npredictions = autoencoder.predict(images)\n\n_, axs = plt.subplots(4, 4, figsize=(12, 12))\naxs = axs.flatten()\nfor img, ax in zip(R, axs):\n    ax.imshow(images[img])\nplt.tight_layout()\nplt.show()","8c08b5f4":"_, axs = plt.subplots(4, 4, figsize=(12, 12))\naxs = axs.flatten()\nfor img, ax in zip(R, axs):\n    ax.imshow(predictions[img])\nplt.tight_layout()\nplt.show()","3c9c2d4b":"#Extracting Both Parts of Our Autoencoder\ndecoder = autoencoder.get_layer('Decoder')\nencoder = autoencoder.get_layer('Encoder')\n\n#Converting Images into a Latent Representation\nlatent_representation = encoder.predict(images,verbose=1)","94fbb387":"tsne = TSNE(n_components= 2)\nr2 = tsne.fit_transform(latent_representation)","b34980a0":"dbs = DBSCAN(eps=0.05,min_samples = 2)\nr2_df = pd.DataFrame({'d1':r2[:,0],'d2':r2[:,1]})\ndbs.fit(r2)\nr2_df['cluster'] = dbs.labels_","77b6bb93":"import plotly.express as ex\n\nex.scatter(r2_df,x='d1',y='d2',color='cluster',title='Distribution of Images in Reduced Latent Dimension')","9d8bbd3b":"indecies = r2_df[r2_df.cluster == r2_df.cluster.value_counts().index[1]].index.to_list()","3c0af165":"_, axs = plt.subplots(5, 4, figsize=(12, 12))\naxs = axs.flatten()\nfor img, ax in zip(indecies, axs):\n    ax.imshow(images[img])\nplt.tight_layout()\nplt.show()","a4768c21":"R2=np.random.choice(np.arange(1,len(images)),2)\nimg1 = images[R2[0]]\nimg2 = images[R2[1]]\nplt.subplot(1,2,1)\nplt.title('img1')\nplt.imshow(img1)\nplt.subplot(1,2,2)\nplt.title('img2')\nplt.imshow(img2)","7d772d55":"img1 = latent_representation[R2[0]]\nimg2 = latent_representation[R2[1]]\n\ninterpolated = np.array([alpha*img1 + (1-alpha)*img2 for alpha in np.arange(0,1,0.1)])\nexamples = decoder.predict(interpolated)","33d6dca4":"\n\nplt.figure(figsize=(20,11))\nfor i in range(10):\n    plt.title(f'Transition {i+1}')\n    plt.subplot(2, 5, 1 + i)\n    plt.axis('off')\n    plt.imshow(examples[i])\nplt.show()","2d6e3e55":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Original Sample of Images<\/h3>\n","621fefd6":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Extracting Image Latent Representation Via Our Encoder<\/h3>\n","fcc29916":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">\"Walking\" in The Latent Dimension<\/h3>\n\n\n**Note**: In this part, as a small fun experiment, we will select two random pictures and take a \"walk\" starting at the position of the first image in the latent dimension and going towards the second image, such a \"walk\" is achieved via linear interpolation.","8fc0c0b4":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">The Encoder<\/h3>\n","5e83e941":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Training and Evaluating the Autoencoder<\/h3>\n","5cc00764":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Assembled Autoencoder<\/h3>\n","e9365a09":"\n\n<img src=https:\/\/i.ibb.co\/WF3qZmH\/generator.png alt=\"AEA\" class=\"center\">","61ec1a6b":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Resulting Image From Sampled Cluster<\/h3>\n","0a91ac66":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>\n","5a60eb5a":"<img src=https:\/\/i.ibb.co\/2d2PQNn\/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png alt=\"AEA\" class=\"center\">","bf92e678":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Reconstructed Sample of Images<\/h3>\n","dac3b048":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Exploring the Latent Dimension<\/h3>\n","44babb25":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Creating an Autoencoder<\/h3>\n","2ceb67ba":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading and Global Variables<\/h3>\n","2e8bad8c":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">The Decoder<\/h3>\n","d81ae065":"**Note**: After reducing the dimensionality of our latent representation, we can clearly see clusters within our data that we can interpret as similar images.\nDBSCAN clustering was applied in order to extract the cluster of highest density with the hypothesis that those will be books from the same series or simply of the same theme.","deff022b":"**What is an autoencoder** - An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning).The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (\u201cnoise\u201d).\n\nVariants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models.Autoencoders are applied to many problems, from facial recognition to acquiring the meaning of words. (Wikipedia.com)\n\n\n<img src=https:\/\/i.ibb.co\/WWStX76\/enbocedrdecoderarc.png alt=\"AEA\" class=\"center\">"}}