{"cell_type":{"23aa9cdd":"code","d398ac90":"code","80d214cb":"code","62c2aa3d":"code","132ed2e4":"code","4c758d6a":"code","b8d9963d":"code","cf4ef8d9":"code","8ea4a94d":"code","8e024b08":"code","b8458f9b":"code","06324e90":"code","d7a827c1":"code","dbafb18e":"code","99f6d9a1":"code","92e1b320":"code","4bb4bce8":"code","e663a912":"code","a3478221":"code","fff864e8":"code","a86e297c":"code","c91984a1":"code","81398b64":"code","085335b5":"code","633cad1c":"code","8d719cb5":"code","fb6eb681":"code","21cbeab7":"code","fe1fe4cb":"code","a84750f2":"code","424c7f86":"code","f0f070fe":"code","3534f98d":"code","af111022":"code","3f8f3f78":"code","4763592a":"code","bef37c6e":"code","e5148cff":"code","1ed67b5a":"code","49213d2c":"code","f9f5041d":"code","927b94a4":"code","9b1efb91":"code","3010c79b":"code","a1574ff4":"code","d14f47f4":"code","be365bbb":"code","89ef4c81":"code","a63c5c5c":"code","12dc8d88":"code","12017750":"code","e1bcd13b":"code","dc33dd15":"code","375de241":"code","fe46ec1a":"code","5ff9eccd":"code","e8746c49":"code","6bb7d341":"code","0b4a27c8":"code","78e2ae85":"code","5af8cc82":"code","18861a27":"markdown","61af718b":"markdown","0e6b9e38":"markdown","a0444048":"markdown","f44301d0":"markdown","d2c7875c":"markdown","7a33566f":"markdown","663e7970":"markdown","45b3fb57":"markdown","0905466e":"markdown","4c9ed20b":"markdown","27e1d1a9":"markdown","33b14f8c":"markdown","0d1f384b":"markdown","e251e498":"markdown","f759be6a":"markdown","4732347c":"markdown","ce00ee3c":"markdown","52d04d61":"markdown","e59f2fc3":"markdown"},"source":{"23aa9cdd":"#importing of the data structure library\nimport pandas as pd","d398ac90":"#importing and reading the dataset into our main code\ndf = pd.read_csv('..\/input\/heart-disease\/heart_disease.csv')","80d214cb":"# attribute information\ndf.columns","62c2aa3d":"#data type of our attributes\ndf.dtypes","132ed2e4":"#data size; number of rows and columns\ndf.shape","4c758d6a":"#the first 10 records in the dataset\ndf.head(10)","b8d9963d":"#importing of the visualization libraries\nimport matplotlib.pyplot as plt \nimport seaborn as sns   \n%matplotlib inline\nplt.rcParams['figure.figsize'] = 10, 5; ","cf4ef8d9":"sns.countplot(x='target', data=df) ","8ea4a94d":"#histogram\nplt.figure(figsize=(5,5)) \nplt.xlabel('age')\ndf.age.hist()  #an alternative way o","8e024b08":"sns.countplot(x='sex', data=df)  # male = 0 ; female = 1","b8458f9b":" #relationship between age and heart disease\n sns.boxplot(x='target', y ='age', data=df)","06324e90":"#relationship between gender and heart disease\n# 0 = male ; 1 = female\nsns.countplot(x='sex', hue='target', data=df)","d7a827c1":"#relationship between chest pain and heart disease\nsns.countplot(x='cp', hue='target', data=df)","dbafb18e":"#level of correlation between attributes in the dataset \ndf.corr()","99f6d9a1":"# tells me the sum of missing values i have for each column (if any)\ndf.isnull().sum()","92e1b320":"# statistical summary\ndf.describe()","4bb4bce8":"print(df.skew())","e663a912":"df.cp.value_counts()  # number of people for each category","a3478221":"# number of people for each category\ndf.thal.value_counts()","fff864e8":"# number of people for each category\ndf.restecg.value_counts()","a86e297c":"# number of people for each category\ndf.slope.value_counts()","c91984a1":"# dummy coding all categorical predictors and naming each dummy coded column\ncp_dummy = pd.get_dummies(df.cp, drop_first= True)\ncp_dummy.columns = ['cp_' + str(col) for col in cp_dummy.columns]\nrestecg_dummy = pd.get_dummies(df.restecg, drop_first= True)\nrestecg_dummy.columns = ['restecg_' + str(col) for col in restecg_dummy.columns]\nslope_dummy = pd.get_dummies(df.slope, drop_first= True)\nslope_dummy.columns = ['slope_' + str(col) for col in slope_dummy.columns]","81398b64":"# dummy coding and naming the 'thal' columns\nthal_dummy = pd.get_dummies(df.thal, drop_first= True)\nthal_dummy.columns = ['thal_' + str(col) for col in thal_dummy.columns]\nthal_dummy.head()","085335b5":"#dropping the 'thal' column with no category\n#thal_dummy.drop(['thal_0'], axis=1, inplace=True)\n#thal_dummy.head()","633cad1c":"# dummy coding our target\ntarget_dummy = pd.get_dummies(df.target, drop_first= True)\ntarget_dummy.columns","8d719cb5":"# dropping the '0' column of target\n#target_dummy.drop([0], axis=1, inplace=True)\n#target_dummy.columns","fb6eb681":"target_dummy.columns = ['target_1']\ntarget_dummy.columns","21cbeab7":"df_dummy = pd.concat([df, cp_dummy, thal_dummy, restecg_dummy, slope_dummy, target_dummy,], axis=1)","fe1fe4cb":"df_dummy.drop(['cp', 'thal', 'restecg', 'slope', 'target'], axis=1, inplace=True)\ndf_dummy.columns","a84750f2":"df_dummy.head()","424c7f86":"# dummmy coded csv file\ndf_dummy.to_csv('encoded_heart_disease_dummy.csv')","f0f070fe":"df_dummy.shape","3534f98d":"# Predictors\nX = df_dummy.drop('target_1', axis=1)\nX.shape","af111022":"# Target\ny = df_dummy.target_1\ny.shape","3f8f3f78":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)","4763592a":"# train_test split results\nX_train.shape","bef37c6e":"X_test.shape","e5148cff":"y_train.shape","1ed67b5a":"y_test.shape","49213d2c":"from sklearn import tree\ndtree_clf = tree.DecisionTreeClassifier(random_state=0) #(max_leaf_nodes=3;default)  #decision tree created","f9f5041d":"#training my model\na = dtree_clf.fit(X_train, y_train)","927b94a4":"# decision tree predict responses from x_test\ndtree_clf.predict(X_test)","9b1efb91":"# to store the values of my predicted responses\ndtree_pred = dtree_clf.predict(X_test)","3010c79b":"# testing the accuracy of my decision tree by comparing my predicted respnses against my actual values (y_test)\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, dtree_pred)","a1574ff4":"# Misclassification\n1 - accuracy_score(y_test, dtree_pred)","d14f47f4":"#using random forest for prediction (bagging)\nfrom sklearn import ensemble\nrf_clf = ensemble.RandomForestClassifier(random_state=0)\nb = rf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nrf_acc = accuracy_score(y_test, rf_pred)\n\nrf_acc","be365bbb":"# gradient boosting classifier (boosting)\ngb_clf = ensemble.GradientBoostingClassifier(random_state=0)\n_ = gb_clf.fit(X_train, y_train)\ngb_pred = gb_clf.predict(X_test)\ngb_acc = accuracy_score(y_test, gb_pred)\n\ngb_acc","89ef4c81":"# Evaluation Metrics\nfrom sklearn.metrics import classification_report\nprint('.....Decision Tree Classification Report......')\nprint(classification_report(y_test, dtree_pred))\nprint('.....Random Forest Classification Report......')\nprint(classification_report(y_test, rf_pred))\nprint('.....Gradient Boosting Classification Report......')\nprint(classification_report(y_test, gb_pred))","a63c5c5c":"# Finetuned Decision Tree                                                           \ndtree2_clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=12, max_leaf_nodes=10, random_state= 0)\n_ = dtree2_clf.fit(X_train, y_train)\ndtree_pred = dtree2_clf.predict(X_test)\ndtree_acc = accuracy_score(y_test, dtree_pred)\n\ndtree_acc","12dc8d88":"#Fune Tuned Random Forest\nrf2 = ensemble.RandomForestClassifier(n_estimators=1000, max_depth=6, max_leaf_nodes=10, random_state= 0, n_jobs=-1)\n_ = rf2.fit(X_train,y_train)\nrf_pred = rf2.predict(X_test)\nrf_acc = accuracy_score(y_test, rf_pred)\n\nrf_acc","12017750":"# Fine Tuned gradient boosting classifier \ngb_clf2 = ensemble.GradientBoostingClassifier(loss= 'deviance',learning_rate= 0.001, n_estimators=1000, random_state= 0, max_depth= 7, max_leaf_nodes= 10)\n_ = gb_clf2.fit(X_train, y_train)\ngb_pred = gb_clf2.predict(X_test)\ngb_acc = accuracy_score(y_test, gb_pred)\n\ngb_acc","e1bcd13b":"# finetuned Evaluation Metrics\nfrom sklearn.metrics import classification_report\nprint('.....Decision Tree Classification Report......')\nprint(classification_report(y_test, dtree_pred))\nprint('.....Random Forest Classification Report......')\nprint(classification_report(y_test, rf_pred))\nprint('.....Gradient Boosting Classification Report......')\nprint(classification_report(y_test, gb_pred))","dc33dd15":"# Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable\n# feature importance for random forest (best performing model out of the finetuned models)\nf_imp = []\nfor feature in zip(list(X_train), rf2.feature_importances_):\n    f_imp.append(feature)\n\nfeature =[]\nimp_score = []\nfor i in f_imp:\n    feature.append(i[0])\n    imp_score.append(i[1])\n\nfeature = pd.DataFrame(feature)\nimp_score = pd.DataFrame(imp_score)\n\nfeature_imp = pd.concat([feature,imp_score],axis=1)\nfeature_imp.columns = ['feature', 'importance_score']\nfeature_imp.sort_values(by=['importance_score'], inplace=True, ascending=False)\nfeature_imp = round(feature_imp,5)\nfeature_imp.head()","375de241":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","fe46ec1a":"y_pred_prob = rf2.predict_proba(X_test)[:,1]\nprint('AUC_score', roc_auc_score(y_test, y_pred_prob))","5ff9eccd":"df.target.value_counts()  # shows our dataset is slightly oversampled (1's), so how do we create a balance for our model?","e8746c49":"import imblearn\nfrom imblearn.combine import SMOTEENN","6bb7d341":"# Combination method (oversampling + undersampling)\n\nsampler = SMOTEENN(sampling_strategy='auto', random_state= 0)\nX_us, y_us = sampler.fit_sample(X, y)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_us, y_us, test_size=0.3, random_state=50)\n\n# decision tree\ndtree3_clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=12, max_leaf_nodes=10, random_state= 0)\n_ = dtree3_clf.fit(X_train, y_train)\ndtree_pred = dtree3_clf.predict(X_test)\ndtree_acc = accuracy_score(y_test, dtree_pred)\n\nprint(dtree_acc)\n\n# random forest\nrf3 = ensemble.RandomForestClassifier( n_estimators=1000, max_depth=6, max_leaf_nodes=10, random_state= 0, n_jobs=-1)\n_ = rf3.fit(X_train, y_train)\nrf_pred = rf3.predict(X_test)\nrf_acc = accuracy_score(y_test, rf_pred)\n\nprint(rf_acc)\n\n# gradient boosting method\ngb_clf3 = ensemble.GradientBoostingClassifier(loss= 'deviance',learning_rate= 0.001, n_estimators=1000, random_state= 0, max_depth= 7, max_leaf_nodes= 10)\n_ = gb_clf3.fit(X_train, y_train)\ngb_pred = gb_clf3.predict(X_test)\ngb_acc = accuracy_score(y_test, gb_pred)\n\nprint(gb_acc)\n\nfrom sklearn.metrics import classification_report\nprint('.....Decision Tree Classification Report......')\nprint(classification_report(y_test, dtree_pred))\nprint('.....Random Forest Classification Report......')\nprint(classification_report(y_test, rf_pred))\nprint('.....Gradient Boosting Classification Report......')\nprint(classification_report(y_test, gb_pred))\n","0b4a27c8":"from imblearn.ensemble import BalancedRandomForestClassifier","78e2ae85":"# ensemble method; A balanced random forest classifier.\nsampler_2 = BalancedRandomForestClassifier(sampling_strategy= 'auto', random_state=0)\nX_us, y_us = sampler.fit_sample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_us, y_us, test_size=0.3, random_state=50)\n\n# random forest\nrf4 = ensemble.RandomForestClassifier( n_estimators=1000, max_depth=6, max_leaf_nodes=10, random_state= 0, n_jobs=-1)\n_ = rf4.fit(X_train, y_train)\nrf_pred = rf4.predict(X_test)\nrf_acc = accuracy_score(y_test, rf_pred)\n\nprint(rf_acc)\n\nfrom sklearn.metrics import classification_report\nprint('.....Random Forest Classification Report......')\nprint(classification_report(y_test, rf_pred))\n","5af8cc82":"# Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable\n# feature importance for random forest (best performing model out of the data sampling models)\nf_imp = []\nfor feature in zip(list(X_train), rf3.feature_importances_):\n    f_imp.append(feature)\n\nfeature =[]\nimp_score = []\nfor i in f_imp:\n    feature.append(i[0])\n    imp_score.append(i[1])\n\nfeature = pd.DataFrame(feature)\nimp_score = pd.DataFrame(imp_score)\n\nfeature_imp = pd.concat([feature,imp_score],axis=1)\nfeature_imp.columns = ['feature', 'importance_score']\nfeature_imp.sort_values(by=['importance_score'], inplace=True, ascending=False)\nfeature_imp = round(feature_imp,5)\nfeature_imp.head()","18861a27":"**I will be focusing on the tree models for my prediction model. Ideally, my goal is to pick the model with the best recall score. Recall because it is a test of the 'true positives' actually captured by the model i eventually will pick.\n\nAlso, i have decided to use the default parameters of each tree model for the first round of model testing. This is generally to see how the models will perform without finetunning the parameters. **","61af718b":"## Conclusion: My best model is the model with the balanced random classifier.","0e6b9e38":"**Several machine learning algorithms make the assumption that the data follow a normal (or Gaussian) distribution. This is easy to check with the skewness value, which explains the extent to which the data is normally distributed. Ideally, the skewness value should be between -1 and +1, and any major deviation from this range indicates the presence of extreme values.**","a0444048":"# Heart Diease Prediction\n\nBY OLOMO RACHAEL AYOBAMI","f44301d0":"**auc-roc score for random forest (best performing model out of the finetuned models)\nused for classification problems. \nIt tells how well the model is able to distinguish between classes.**","d2c7875c":"## Single Variable Visualizations","7a33566f":"## Model Finetunning ","663e7970":"**The goal of this project is to create a machine learning model that will detect if a patient has heart disease or not.\n**Age, sex, cp, trestbps, chol, fbs, restecg, thalach, exang, oldpeak, slope, ca, thal= independent variables\ntarget = dependent variable**\n\n**Objectives:\na.\tData Exploration\nb.\tMissing Data\nc.\tOutliers\nd.\tTrain and Test with 3 models\ne.\t3 Models with final fine-tuned parameters\nf.\tData Sampling**\n\n****The dataset used for this project was obtained from : https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n**\n**Here are the meanings of the abbreviations used for the independent variables:\n**\nAge = age\nSex = sex (0= Male; 1= Female) \ncp = chest pain type (4 values)\ntrestbps = resting blood pressure\nchol = serum cholestoral in mg\/dl\nfbs = fasting blood sugar > 120 mg\/dl\nrestecg = resting electrocardiographic results (values 0,1,2)\nthalach = maximum heart rate achieved\nexang = exercise induced angina\noldpeak = ST depression induced by exercise relative to rest\nslope = the slope of the peak exercise ST segment\nca = number of major vessels (0-3) colored by flourosopy\nthal: 3 = normal; 6 = fixed defect; 7 = reversable defect****\n\n**dependent variable: target (0 = no heart disease; 1 = heart disease)**\n\n**This heart disease project is solves a classification problem.**","45b3fb57":"![image.png](attachment:image.png)","0905466e":"## Data Exploration","4c9ed20b":"## Dummy Coding categorical variables","27e1d1a9":"## Data Sampling","33b14f8c":"**From my metrics classification report as shown above, random forest performed the best overall. Especially, for the \"1's\" target. Not forgetting that my focus here is on the recall score. Random forest has the highest recall score out of the 3 tree models i used.**","0d1f384b":"## Predictive Model Training","e251e498":"For model finetunning, i started modifying the parameters for each tree model. This happened to improve on the performance score for our initially trained tree models.","f759be6a":"## Missing Columns","4732347c":"## Multiple Variable Correlations","ce00ee3c":"**from my evaluation metrics report, all the models have improved in terms of their accuracy score except for gradient boosting method which maintained it's overall accuracy score from the last training.**","52d04d61":"## Outliers","e59f2fc3":"****The goal of data sampling is still finding a way to improve on my ML models and to reduce the level of biasness for my models. This time,i decided to look at my target values. It shows my models will be better at predicting that a patient has heart disease (the 1's are slightly higher than the 0's for the target values), but i want my model to be balanced and not slightly oversampled. Therefore, i carry out data sampling to create a balnced model that will predict when a person has heart disease and vice versa.Also, i want my accuracy scores to be close to 1. **\n\n**NB: Accuracy scores range between 0-1; 0= no accuracy, 1= maximum accuracy.****"}}