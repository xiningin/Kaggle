{"cell_type":{"be899577":"code","f475c1df":"code","91b0e397":"code","54d48abd":"code","4fb83544":"code","f3f076e9":"code","3d373b65":"code","611ec0e3":"code","da9676b1":"code","b21b0053":"code","bfd94d21":"code","6006ba50":"code","fbf305e2":"code","82d281c3":"code","e1a94b69":"code","48d162fe":"code","df2e49c2":"code","f7499b22":"code","1134d62c":"code","26b2194e":"code","00ea5a36":"code","799a85f9":"code","eabea972":"code","403f211e":"code","eb4d85a5":"code","ca1c0632":"code","83c7792b":"code","0c0be3ac":"markdown","01449bf2":"markdown","11cd9af3":"markdown","a2caefa5":"markdown","2e13d3ba":"markdown","b2ecbeca":"markdown","f10b46d2":"markdown","b2f50a70":"markdown"},"source":{"be899577":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f475c1df":"import xgboost as xgb\nfrom sklearn.metrics import matthews_corrcoef\nimport datetime as dt\nimport pickle","91b0e397":"df_train = pd.read_csv('..\/input\/student-shopee-code-league-marketing-analytics\/train.csv')\ndf_users = pd.read_csv('..\/input\/student-shopee-code-league-marketing-analytics\/users.csv')\ndf_test = pd.read_csv('..\/input\/student-shopee-code-league-marketing-analytics\/test.csv')\ndf_sub = pd.read_csv('..\/input\/student-shopee-code-league-marketing-analytics\/sample_submission_0_1.csv')","54d48abd":"print('The train shape:',df_train.shape)\nprint('Users shape:', df_users.shape)","4fb83544":"df_merge = df_train.merge(df_users,on = 'user_id',how = 'left')\ndf_test = df_test.merge(df_users,on = 'user_id',how = 'left')\n\nprint('The train shape:',df_merge.shape)\nprint('Users shape:', df_test.shape)","f3f076e9":"#attr_1 and attr_2 fillwith \ndf_merge.isna().mean()","3d373b65":"intersect_userid = set(df_merge['user_id']).intersection(df_test['user_id'])","611ec0e3":"df_merge['grass_date'] = df_merge['grass_date'].astype('datetime64')\ndf_merge['dayofweek'] = df_merge['grass_date'].dt.dayofweek\n# df_merge['is_weekend'] = df_merge['dayofweek'].apply(lambda x: 1 if x>=5 else 0)\n# df_merge['grass_day'] = df_merge['grass_date'].dt.day\n\ndf_test['grass_date'] = df_test['grass_date'].astype('datetime64')\ndf_test['dayofweek'] = df_test['grass_date'].dt.dayofweek\n# df_test['is_weekend'] = df_test['dayofweek'].apply(lambda x: 1 if x>=5 else 0)\n# df_test['grass_day'] = df_test['grass_date'].dt.day\n\n\ndf_merge = df_merge.drop(columns = ['grass_date'])\ndf_test = df_test.drop(columns = ['grass_date'])","da9676b1":"df_merge['last_open_day'] = df_merge['last_open_day'].replace(to_replace =\"Never open\", value =-999) \ndf_merge['last_checkout_day'] = df_merge['last_checkout_day'].replace(to_replace =\"Never checkout\", value = -999) \ndf_merge['last_login_day'] = df_merge['last_login_day'].replace(to_replace =\"Never login\", value = -999) \n\ndf_merge[['last_open_day','last_checkout_day','last_login_day']] = df_merge[['last_open_day','last_checkout_day','last_login_day']].astype(int)\n\n\n\n\ndf_test['last_open_day'] = df_test['last_open_day'].replace(to_replace =\"Never open\", value =-999) \ndf_test['last_checkout_day'] = df_test['last_checkout_day'].replace(to_replace =\"Never checkout\", value =-999) \ndf_test['last_login_day'] = df_test['last_login_day'].replace(to_replace =\"Never login\", value =-999) \n\ndf_test[['last_open_day','last_checkout_day','last_login_day']] = df_merge[['last_open_day','last_checkout_day','last_login_day']].astype(int)","b21b0053":"df_merge[['country_code','domain']] = df_merge[['country_code','domain']].astype(str)\ndf_test[['country_code','domain']] = df_test[['country_code','domain']].astype(str)","bfd94d21":"df_merge = pd.get_dummies(df_merge)\ndf_test = pd.get_dummies(df_test)","6006ba50":"features_train = df_merge.drop(columns = ['open_flag','row_id']).copy()\ntarget_train = df_merge['open_flag'].copy()\n\nprint('Features train shape', features_train.shape)","fbf305e2":"\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.under_sampling import TomekLinks\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.under_sampling import ClusterCentroids\nfrom sklearn import preprocessing\n\nscaler = MinMaxScaler()\ntemp = df_merge.drop(columns = ['open_flag','row_id','user_id']).fillna(df_merge.drop(columns = ['open_flag','row_id','user_id']).median())\n\nfeatures_train = pd.DataFrame(scaler.fit_transform(temp),columns = temp.columns)\ntarget_train = df_merge['open_flag']\n\n\n\n# # Run this to use tomeklinks\ntl = TomekLinks()\nX_tl, y_tl = tl.fit_resample(features_train, df_merge['open_flag'])\nfeatures_train = X_tl\ntarget_train = y_tl\n\n\n# print(features_train.shape)\n\n","82d281c3":"data_dmatrix = xgb.DMatrix(data=features_train,label=target_train)","e1a94b69":"THRESHOLD = 0.4\n\n\ndef evalmcc(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'MCC', matthews_corrcoef(labels, preds > THRESHOLD)\n\n\ndef evalmcc_min(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'MCC', matthews_corrcoef(labels, preds > THRESHOLD)\n\n\nxgb_params = {\n    'seed': 0,\n    'colsample_bytree': 0.5, \n    'subsample': 0.5,  \n    'learning_rate': 0.1,\n    'objective': 'binary:logistic',\n    'max_depth': 7, \n    'min_child_weight': 5,\n    'scale_pos_weight': 1,\n}\n\nres = xgb.cv(xgb_params, data_dmatrix, num_boost_round=1000, nfold=5, seed=0, stratified=True,\n            early_stopping_rounds=25, verbose_eval=1, show_stdv=True, feval=evalmcc, maximize=True)\n\n","48d162fe":"xg_cls = xgb.train(params=xgb_params, dtrain=data_dmatrix,num_boost_round=103,maximize=True)","df2e49c2":"import matplotlib.pyplot as plt\nxgb.plot_importance(xg_cls)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()","f7499b22":"temp = df_test.drop(columns = ['row_id','user_id']).fillna(df_test.drop(columns = ['row_id','user_id']).median()) \ntest_X = pd.DataFrame(scaler.fit_transform(temp),columns = temp.columns) #Case scale","1134d62c":"pred_xgboost = xg_cls.predict(xgb.DMatrix(test_X))\npred_xgboost","26b2194e":"pred_xgboost[39]","00ea5a36":"df_submission = pd.DataFrame({'row_id':[i for i in range(len(df_test))],'open_flag':pred_xgboost})\ndf_submission['open_flag'] = df_submission['open_flag'].apply(lambda x : 1 if x>= 0.4 else 0)\ndf_submission.to_csv('submission_fillage_(0.4)_tomek_median.csv',index = False)\ndf_submission['open_flag'].value_counts(normalize = True)","799a85f9":"from numba import jit\n\n# @jit\ndef mcc(tp, tn, fp, fn):\n    sup = tp * tn - fp * fn\n    inf = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n    if inf==0:\n        return 0\n    else:\n        return sup \/ np.sqrt(inf)\n\n# @jit\ndef eval_mcc(y_true, y_prob, show=False):\n    idx = np.argsort(y_prob)\n    print(idx)\n    y_true_sort = y_true[idx]\n    n = y_true.shape[0]\n    nump = 1.0 * np.sum(y_true) # number of positive\n    numn = n - nump # number of negative\n    tp = nump\n    tn = 0.0\n    fp = numn\n    fn = 0.0\n    best_mcc = 0.0\n    best_id = -1\n    prev_proba = -1\n    best_proba = -1\n    mccs = np.zeros(n)\n    for i in range(n):\n        # all items with idx < i are predicted negative while others are predicted positive\n        # only evaluate mcc when probability changes\n        proba = y_prob[idx[i]]\n        if proba != prev_proba:\n            prev_proba = proba\n            new_mcc = mcc(tp, tn, fp, fn)\n            if new_mcc >= best_mcc:\n                best_mcc = new_mcc\n                best_id = i\n                best_proba = proba\n        mccs[i] = new_mcc\n        if y_true_sort[i] == 1:\n            tp -= 1.0\n            fn += 1.0\n        else:\n            fp -= 1.0\n            tn += 1.0\n    if show:\n        y_pred = (y_prob >= best_proba).astype(int)\n        score = matthews_corrcoef(y_true, y_pred)\n        print(score, best_mcc)\n        plt.plot(mccs)\n        return best_proba, best_mcc, y_pred\n    else:\n        return best_mcc\n","eabea972":"## 5 fold CV\nfrom catboost import CatBoostClassifier, cv, Pool\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, KFold\nimport matplotlib.pyplot as plt\nskf = StratifiedKFold(n_splits=10, random_state=2021, shuffle=True)\n\n\n\ndef evalerror_mat_coeff(y_true, p_pred_1):\n    # convert probability (y=1) to label with threshold 0.5\n    y_pred = [1 if pred>=THRESHOLD else 0 for pred in p_pred_1]\n    return 'MCC', matthews_corrcoef(y_true, y_pred), True\n\n#Run this for no polynomial\ntemp = df_test.drop(columns = ['row_id','user_id']).fillna(df_test.drop(columns = ['row_id','user_id']).median()) \nX_test = pd.DataFrame(scaler.fit_transform(temp),columns = temp.columns)\n\n\n\nbest_epochs = []\nfold_scores = []\ntest_pred_cv = []\nbest_cutoff = []\nvote_predictions = []\nfor i,(train_index, val_index) in enumerate(skf.split(features_train, target_train)):\n    print(f\"<------------------------------------- Fold: {i+1} ------------------------------------->\")\n    print(\"TRAIN:\", train_index, \"TEST:\", val_index)\n    X_train, X_val = features_train.iloc[train_index], features_train.iloc[val_index]\n    y_train, y_val = target_train.iloc[train_index], target_train.iloc[val_index]\n    \n    # Best tuned\n    params = {'loss_function':'Logloss', # objective function\n          'eval_metric':'MCC:hints=skip_train~false', \n          'verbose': 200, # output to stdout info about training process every 200 iterations\n          'random_seed': 2020,\n          'iterations': 3000,\n          'depth': 8, \n          'l2_leaf_reg': 3,\n          'learning_rate': 0.05,\n         }  \n    \n    \n    cbc_1 = CatBoostClassifier(**params,class_weights=[1, 1.75])\n    \n    \n    cbc_1.fit(X_train, y_train, \n          eval_set=(X_val, y_val), \n          use_best_model=True, \n          early_stopping_rounds=50,\n          plot=True)\n    val_pred = cbc_1.predict(X_val)\n    \n    test_predictions = cbc_1.predict_proba(X_test)\n    test_pred_cv.append(test_predictions)\n    \n\n    \n    val_pred_proba = cbc_1.predict_proba(X_val)\n    \n    best_proba, best_mcc, y_pred = eval_mcc(np.array(y_val), np.array([pred[1] for pred in val_pred_proba]), True)\n    \n    print('Best Thres:',best_proba)\n    best_cutoff.append(best_proba)\n    print('Best MCC',best_mcc)\n    THRESHOLD = best_proba\n    \n    \n    fold_score = matthews_corrcoef(y_val, val_pred)\n    print(f\"MCC score of fold no adjust threshold {i+1}:{fold_score}\")\n    fold_scores.append(fold_score)\n    \n    cv_vote_pred = [1 if p[1]>=best_proba else 0 for p in test_predictions]\n    vote_predictions.append(cv_vote_pred)\n \n","403f211e":"np.mean(fold_scores)","eb4d85a5":"np.mean(best_cutoff)","ca1c0632":"sum(test_pred_cv)\/10\nfinal_preds_cat = []\npreds_cat = sum(test_pred_cv)\/10\nfor pred in preds_cat:\n    final_preds_cat.append(pred[1])\nfinal_preds_cat","83c7792b":"df_submission = pd.DataFrame({'row_id':[i for i in range(len(df_test))],'open_flag':final_preds_cat})\ndf_submission['open_flag'] = df_submission['open_flag'].apply(lambda x : 1 if x>= 0.53 else 0)\ndf_submission.to_csv('submission_fillage_(0.53)_catboost.csv',index = False)\ndf_submission['open_flag'].value_counts(normalize = True)","0c0be3ac":"### MCC Optimization (Finding the best threshold)","01449bf2":"### Get dummies","11cd9af3":"### Fill \"Never\" (Because we use tree-based model we could replace it with a value unrelated to the original data)**","a2caefa5":"### Tomeklinks : This undersampling method takes out the datapoint of class 0 that is the closest to class 1 for generalizatio as shown.\n\n![image.png](attachment:image.png)","2e13d3ba":"### Experiment with XGBoost","b2ecbeca":"### Prepare for training","f10b46d2":"# CatBoost","b2f50a70":"### Clean date"}}