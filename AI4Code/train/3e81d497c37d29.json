{"cell_type":{"c823c460":"code","51681a65":"code","e98bb3ec":"code","379934b2":"code","b2f2ec13":"code","d573fd28":"code","dd5b0c90":"code","9bf10cf0":"code","70f54905":"code","e576c360":"code","a72aca2c":"code","82b85951":"code","f11f119b":"code","965d3308":"code","10e73ae3":"code","898aec63":"code","659a2b57":"code","d2ed0397":"code","af8fa0e3":"code","a35418be":"code","91675f10":"code","b8c3078e":"code","ed440ddd":"code","c28204db":"code","86fec4e3":"code","69029660":"code","f2d9689d":"code","41883904":"code","f1f99f25":"code","8bd8912a":"code","5aba01a6":"markdown","565be6e2":"markdown","9354e859":"markdown","4e0af0c2":"markdown","631bb6cf":"markdown","f1170015":"markdown","55207320":"markdown","03a9f14a":"markdown","82fad553":"markdown","3f8b0b07":"markdown","c10ef82b":"markdown","700dfac6":"markdown","99d55f4b":"markdown","5c5d4bb6":"markdown","c807f04c":"markdown","e641f1aa":"markdown","7ca1be7c":"markdown","74a18b81":"markdown","6f5e0d71":"markdown","37687645":"markdown","741e8e16":"markdown","2a79c6d2":"markdown","238309ff":"markdown"},"source":{"c823c460":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n!pip install mglearn\nimport mglearn\nfrom sklearn.impute import KNNImputer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nimport matplotlib.pyplot as plt","51681a65":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")","e98bb3ec":"data.isnull().sum().sort_values()","379934b2":"\nsns.heatmap(data.isnull(), yticklabels=False,cbar=False)","b2f2ec13":"data.dtypes","d573fd28":"data.Embarked.value_counts()","dd5b0c90":"data.Cabin.value_counts(ascending =False)\n","9bf10cf0":"#Solution_1\n# data['Sex'] = pd.factorize(data['Sex'])[0] \n#Solution_2\ndata['Sex'] = np.where(data['Sex'] == \"male\",0,1)\ndata.Sex.value_counts()","70f54905":"fig,ax=plt.subplots(figsize = (10,8))\nsns.heatmap(data.isnull(), yticklabels=False,cbar=False)","e576c360":"data= data.drop(['Name'],axis = 1)\ndata.head()","a72aca2c":"X_train_1 = data.drop(['Survived'], axis = 1)\ny_train_1= data['Survived']","82b85951":"categorical = [column for column in X_train_1.columns \\\n    if column in [ 'Cabin', 'Embarked','Ticket','Name']]\n\ncategorical\n\nnumeric = [column for column in X_train_1.columns if column not in categorical]\nlist(numeric)","f11f119b":"imputer = KNNImputer(n_neighbors=2)\nX_train_1[numeric] = pd.DataFrame(imputer.fit_transform(X_train_1[numeric]))\nX_train_1[numeric] ","965d3308":"X_train_1['Embarked'] = X_train_1['Embarked'].fillna(X_train_1['Embarked'].mode()[0])\nX_train_1['Cabin'] = X_train_1['Cabin'].fillna(X_train_1['Cabin'].mode()[0])\nX_train_1['Ticket'] = X_train_1['Ticket'].fillna(X_train_1['Ticket'].mode()[0])\n\nX_train_1['Embarked'] = pd.factorize(X_train_1['Embarked'])[0]+1\nX_train_1['Cabin'] = pd.factorize(X_train_1['Cabin'])[0]+1\nX_train_1['Ticket'] = pd.factorize(X_train_1['Ticket'])[0]+1","10e73ae3":"X_train_1.head()","898aec63":"X_train_2 = data.drop(['Survived'], axis = 1)\ny_train_2= data['Survived']","659a2b57":"fig,ax=plt.subplots(figsize = (10,8))\n\nsns.boxplot(X_train_2.Age)","d2ed0397":"# Distribution plot\nfig,ax=plt.subplots(figsize = (10,8))\nsns.distplot(X_train_2.Age)","af8fa0e3":"x= X_train_2['Age'].mean()\nX_train_2['Age'] = X_train_2['Age'].fillna(x)\nX_train_2['Embarked'] = X_train_2['Embarked'].fillna(X_train_2['Embarked'].mode()[0])\nX_train_2['Cabin'] = X_train_2['Cabin'].fillna(X_train_2['Cabin'].mode()[0])","a35418be":"X_train_2.isnull().sum()","91675f10":"X_train_2['Embarked'] = pd.factorize(X_train_2['Embarked'])[0] +1\nX_train_2['Cabin'] = pd.factorize(X_train_2['Cabin'])[0] +1\nX_train_2['Ticket'] = pd.factorize(X_train_2['Ticket'])[0] +1\n\n\n","b8c3078e":"X_train_2.Embarked.value_counts()","ed440ddd":"from sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(X_train_1, y_train_1, test_size=0.25, random_state=0)\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_train_2, y_train_2, test_size=0.25, random_state=0)\n","c28204db":"pipe = make_pipeline(StandardScaler(), LogisticRegression())\npipe.fit(X_train, y_train)  # apply scaling on training data\npreds = pipe.predict(X_test)\n","86fec4e3":"pipe_1 = make_pipeline(StandardScaler(), LogisticRegression())\npipe_1.fit(X_train_2, y_train_2)  # apply scaling on training data\npreds_1 = pipe.predict(X_test_2)\n","69029660":"print(\"The Score of the Method I with LrModel : {:.2f}\".format(pipe.score(X_test,y_test)))\nprint(\"The Score of the Method II with LrModel : {:.2f}\".format(pipe_1.score(X_test_2,y_test_2)))","f2d9689d":"print(\"The Corelation by using Method I with LrModel: {:.3f}\".format(np.corrcoef(y_test, preds)[0][1]))\nprint(\"The Correlation of the Method II with LrModel {:.3f}\".format(np.corrcoef(y_test_2, preds_1)[0][1]))","41883904":"pipe_2 = make_pipeline(StandardScaler(), SVC())\npipe_2.fit(X_train, y_train)  # apply scaling on training data\npreds_2 = pipe_2.predict(X_test)","f1f99f25":"pipe_3 = make_pipeline(StandardScaler(), SVC())\npipe_3.fit(X_train_2, y_train_2)  # apply scaling on training data\npreds_3 = pipe_3.predict(X_test_2)","8bd8912a":"print(\"The Score of the Method I with SV_Model : {:.3f}\".format(pipe_2.score(X_test,y_test)))\nprint(\"The Score of the Method II with SV_Model: {:.3f}\".format(pipe_3.score(X_test_2,y_test_2)))","5aba01a6":"## Support Vector_Model with Method I","565be6e2":"# This work compares the influence of Handling Missing Values Methods on the accuracy of a model\n\n# Table of Contents\n## 1.Import Dataset Titanic\n## 2.Missvalues Checking\n## 3.Method\n  - #### Predict with KNNImpute\n  - #### Replace Missing Value using strategy (Mean, Median or Mode) \n  \n## 4.Methods Comparing and Evaluation\n  - #### Using the LogisticReggression Model\n\n","9354e859":"### Ploting Missvalues_Number as a Heatmap","4e0af0c2":"# 1. Import Titanic_Data","631bb6cf":"## Split Categorie and Numeric Values","f1170015":"## LogisticRegression_Model with Method I","55207320":"## Drop Name Columns","03a9f14a":"## Factorize the Sex_Column ","82fad553":"## Factorize Categories  Values\n","3f8b0b07":"## Use KNNImpute to predict numeric Missvalues","c10ef82b":"## LogisticRegression_Model with Method II","700dfac6":"## Replacing and Factorizing\n- ###  To replace Categories missing data points, i used the mode (` most_frequent`) Strategy\n- ###  Factorize Categories_Columns","99d55f4b":"### Number of Cabin categories","5c5d4bb6":"# 3.2 Method II\n## Use` Boxplot` and ` Distribution ` Plot for deciding whether to use mean, mode or median for imputation\n- We can see (Centring Distribution with low number  of outliers), so that the mean is the best Way to replace missing Values \n-  imputing missing data with mode values can be done with numerical and categorical data\n-  imputing missing data with mean and median values can be done only with numerical data.","c807f04c":"## Checking if there is other Missvalues","e641f1aa":"# 3.1 Methode I\n## Imputing with Prediction_Model\n- Is a good strategy to predict the missvalues\n- This strategie is only for numeric values\n- We have the option to provide the strategy parameter with a method other than the mean, it can be median , most_frequent, constant or a Prediction_Model like KNN","7ca1be7c":"## Missvalues ploting","74a18b81":"## Support Vector_Model with Method II","6f5e0d71":"### Number of Embarked_categories","37687645":"# 2. Missvalues checking","741e8e16":"## Split the dataset into Features and Target","2a79c6d2":"### I have the option to provide the strategy parameter with  `Mean` and `Mode` method","238309ff":"# 4. Methods evaluation\n- The Imputation with KNNImput has a better Accuracy with Lr_Model\n- The Imputation with KNNImput has a better Accuracy with SV_Model"}}