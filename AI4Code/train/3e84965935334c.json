{"cell_type":{"f93a965e":"code","c8c26fed":"code","be27373f":"code","9275663c":"code","4f030bdf":"code","789fb3a5":"code","2b23742c":"code","32d8b308":"code","7aa91fe2":"code","afb49b63":"code","7bad9437":"code","a501d5e7":"code","bcd8aab8":"code","0df12d8b":"code","0abca677":"code","4ee50fb4":"code","2237ccc8":"code","2b68e73f":"code","6161c4a4":"code","75b10580":"code","bfcd53db":"code","dd494aba":"code","ddae6c50":"code","2e230e6d":"code","940472ab":"markdown","c51a9e0e":"markdown","c149a6f6":"markdown","8c7a0e2d":"markdown"},"source":{"f93a965e":"# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\nimport numpy as np\nimport urllib\nimport tarfile\nimport os\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom imageio import imread, imsave, mimsave\nfrom PIL import Image\nimport glob\nimport shutil","c8c26fed":"import numpy as np # linear algebra\nimport xml.etree.ElementTree as ET # for parsing XML\nimport matplotlib.pyplot as plt # to show images\nfrom PIL import Image # to read images\nimport os\nimport glob\n\nroot_images=\"..\/input\/all-dogs\/all-dogs\/\"\nroot_annots=\"..\/input\/annotation\/Annotation\/\"","be27373f":"all_images=os.listdir(\"..\/input\/all-dogs\/all-dogs\/\")\nprint(f\"Total images : {len(all_images)}\")\n\nbreeds = glob.glob('..\/input\/annotation\/Annotation\/*')\nannotation=[]\nfor b in breeds:\n    annotation+=glob.glob(b+\"\/*\")\nprint(f\"Total annotation : {len(annotation)}\")\n\nbreed_map={}\nfor annot in annotation:\n    breed=annot.split(\"\/\")[-2]\n    index=breed.split(\"-\")[0]\n    breed_map.setdefault(index,breed)\n    \nprint(f\"Total Breeds : {len(breed_map)}\")","9275663c":"def bounding_box(image):\n    bpath=root_annots+str(breed_map[image.split(\"_\")[0]])+\"\/\"+str(image.split(\".\")[0])\n    tree = ET.parse(bpath)\n    root = tree.getroot()\n    objects = root.findall('object')\n    for o in objects:\n        bndbox = o.find('bndbox') # reading bound box\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        \n    return (xmin,ymin,xmax,ymax)\n\ndef get_crop_image(image):\n    bbox=bounding_box(image)\n    im=Image.open(os.path.join(root_images,image))\n    im=im.crop(bbox)\n    return im","4f030bdf":"%%time\nplt.figure(figsize=(10,10))\nfor i,image in enumerate(all_images):\n    im=get_crop_image(image)\n    \n    plt.subplot(3,3,i+1)\n    plt.axis(\"off\")\n    plt.imshow(im)    \n    if(i==8):\n        break","789fb3a5":"%%time\npath = '..\/input\/all-dogs'\ndataset = 'all-dogs'\ndata_path = os.path.join(path, dataset)\nimages = glob.glob(os.path.join(data_path, '*.*')) \nprint(len(images))","2b23742c":"images[0]","32d8b308":"batch_size = 16\nz_dim = 100\nWIDTH = 64\nHEIGHT = 64\n\nOUTPUT_DIR = 'samples_dogs'\nif not os.path.exists(OUTPUT_DIR):\n    os.mkdir(OUTPUT_DIR)\n\nGEN_DIR = 'generated_dogs'\nif not os.path.exists(GEN_DIR):\n    os.mkdir(GEN_DIR)\n    \nX = tf.placeholder(dtype=tf.float32, shape=[None, HEIGHT, WIDTH, 3], name='X')\nnoise = tf.placeholder(dtype=tf.float32, shape=[None, z_dim], name='noise')\nis_training = tf.placeholder(dtype=tf.bool, name='is_training')\n\ndef lrelu(x, leak=0.2):\n    return tf.maximum(x, leak * x)\n\ndef sigmoid_cross_entropy_with_logits(x, y):\n    return tf.nn.sigmoid_cross_entropy_with_logits(logits=x, labels=y)","7aa91fe2":"def discriminator(image, reuse=None, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('discriminator', reuse=reuse):\n        h0 = lrelu(tf.layers.conv2d(image, kernel_size=5, filters=64, strides=2, padding='same'))\n        \n        h1 = tf.layers.conv2d(h0, kernel_size=5, filters=128, strides=2, padding='same')\n        h1 = lrelu(tf.contrib.layers.batch_norm(h1, is_training=is_training, decay=momentum))\n        \n        h2 = tf.layers.conv2d(h1, kernel_size=5, filters=256, strides=2, padding='same')\n        h2 = lrelu(tf.contrib.layers.batch_norm(h2, is_training=is_training, decay=momentum))\n        \n        h3 = tf.layers.conv2d(h2, kernel_size=5, filters=512, strides=2, padding='same')\n        h3 = lrelu(tf.contrib.layers.batch_norm(h3, is_training=is_training, decay=momentum))\n        \n        h4 = tf.contrib.layers.flatten(h3)\n        h4 = tf.layers.dense(h4, units=1)\n        return tf.nn.sigmoid(h4), h4","afb49b63":"def generator(z, is_training=is_training):\n    momentum = 0.9\n    with tf.variable_scope('generator', reuse=None):\n        d = 4\n        h0 = tf.layers.dense(z, units=d * d * 512)\n        h0 = tf.reshape(h0, shape=[-1, d, d, 512])\n        h0 = tf.nn.relu(tf.contrib.layers.batch_norm(h0, is_training=is_training, decay=momentum))\n        \n        h1 = tf.layers.conv2d_transpose(h0, kernel_size=5, filters=256, strides=2, padding='same')\n        h1 = tf.nn.relu(tf.contrib.layers.batch_norm(h1, is_training=is_training, decay=momentum))\n        \n        h2 = tf.layers.conv2d_transpose(h1, kernel_size=5, filters=128, strides=2, padding='same')\n        h2 = tf.nn.relu(tf.contrib.layers.batch_norm(h2, is_training=is_training, decay=momentum))\n        \n        h3 = tf.layers.conv2d_transpose(h2, kernel_size=5, filters=64, strides=2, padding='same')\n        h3 = tf.nn.relu(tf.contrib.layers.batch_norm(h3, is_training=is_training, decay=momentum))\n        \n        h4 = tf.layers.conv2d_transpose(h3, kernel_size=5, filters=3, strides=2, padding='same', activation=tf.nn.tanh, name='g')\n        return h4","7bad9437":"g = generator(noise)\nd_real, d_real_logits = discriminator(X)\nd_fake, d_fake_logits = discriminator(g, reuse=True)\n\nvars_g = [var for var in tf.trainable_variables() if var.name.startswith('generator')]\nvars_d = [var for var in tf.trainable_variables() if var.name.startswith('discriminator')]\n\nloss_d_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_real_logits, tf.ones_like(d_real)))\nloss_d_fake = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_fake_logits, tf.zeros_like(d_fake)))\nloss_g = tf.reduce_mean(sigmoid_cross_entropy_with_logits(d_fake_logits, tf.ones_like(d_fake)))\nloss_d = loss_d_real + loss_d_fake","a501d5e7":"update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer_d = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_d, var_list=vars_d)\n    optimizer_g = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(loss_g, var_list=vars_g)","bcd8aab8":"def read_image(image_name, height, width):\n    image = get_crop_image(image_name)\n    \n    h = image.size[0]\n    w = image.size[1]\n    \n    image = np.array(image.resize((height, width)))\n    return image \/ 255.","0df12d8b":"def montage(images):    \n    if isinstance(images, list):\n        images = np.array(images)\n    img_h = images.shape[1]\n    img_w = images.shape[2]\n    n_plots = int(np.ceil(np.sqrt(images.shape[0])))\n    if len(images.shape) == 4 and images.shape[3] == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 3)) * 0.5\n    elif len(images.shape) == 4 and images.shape[3] == 1:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1, 1)) * 0.5\n    elif len(images.shape) == 3:\n        m = np.ones(\n            (images.shape[1] * n_plots + n_plots + 1,\n             images.shape[2] * n_plots + n_plots + 1)) * 0.5\n    else:\n        raise ValueError('Could not parse image shape of {}'.format(images.shape))\n    for i in range(n_plots):\n        for j in range(n_plots):\n            this_filter = i * n_plots + j\n            if this_filter < images.shape[0]:\n                this_img = images[this_filter]\n                m[1 + i + i * img_h:1 + i + (i + 1) * img_h,\n                  1 + j + j * img_w:1 + j + (j + 1) * img_w] = this_img\n    return m","0abca677":"sess = tf.Session()\nsess.run(tf.global_variables_initializer())\nz_samples = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\nloss = {'d': [], 'g': []}\n\noffset = 0\nfor i in tqdm_notebook(range(10000)):\n    n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    \n    offset = (offset + batch_size) % len(images)\n    batch = np.array([read_image(img, HEIGHT, WIDTH) for img in all_images[offset: offset + batch_size]])\n    batch = (batch - 0.5) * 2\n    \n    d_ls, g_ls = sess.run([loss_d, loss_g], feed_dict={X: batch, noise: n, is_training: True})\n    loss['d'].append(d_ls)\n    loss['g'].append(g_ls)\n    \n    sess.run(optimizer_d, feed_dict={X: batch, noise: n, is_training: True})\n    sess.run(optimizer_g, feed_dict={X: batch, noise: n, is_training: True})\n    sess.run(optimizer_g, feed_dict={X: batch, noise: n, is_training: True})\n        \n    if i % 1000 == 0:\n        print(i, d_ls, g_ls)\n        gen_imgs = sess.run(g, feed_dict={noise: z_samples, is_training: False})\n        gen_imgs = (gen_imgs + 1) \/ 2\n        imgs = [img[:, :, :] for img in gen_imgs]\n        gen_imgs = montage(imgs)\n        plt.axis('off')\n        plt.imshow(gen_imgs)\n        plt.show()\n\nplt.plot(loss['d'], label='Discriminator')\nplt.plot(loss['g'], label='Generator')\nplt.legend(loc='upper right')\nplt.show()","4ee50fb4":"# saver = tf.train.Saver()\n# saver.save(sess, os.path.join(OUTPUT_DIR, 'dcgan_' + dataset), global_step=60000)","2237ccc8":"%%time\n# sess = tf.Session()\n# sess.run(tf.global_variables_initializer())\n\n# saver = tf.train.import_meta_graph(os.path.join('samples_dogs', 'dcgan_' + dataset + '-60000.meta'))\n# saver.restore(sess, tf.train.latest_checkpoint('samples_dogs'))\ngraph = tf.get_default_graph()\ng = graph.get_tensor_by_name('generator\/g\/Tanh:0')\nnoise = graph.get_tensor_by_name('noise:0')\nis_training = graph.get_tensor_by_name('is_training:0')\n\nn = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\ngen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\ngen_imgs = (gen_imgs + 1) \/ 2\nimgs = [img[:, :, :] for img in gen_imgs]\ngen_imgs = montage(imgs)\ngen_imgs = np.clip(gen_imgs, 0, 1)\nplt.figure(figsize=(8, 8))\nplt.axis('off')\nplt.imshow(gen_imgs)\nplt.show()","2b68e73f":"n_batches = 10000 \/\/ batch_size\nlast_batch_size = 10000 % batch_size\n\nfor i in tqdm_notebook(range(n_batches)):\n    n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n    gen_imgs = (gen_imgs + 1) \/ 2\n    for j in range(batch_size):\n        imsave(os.path.join(GEN_DIR, f'sample_{i}_{j}.png'), gen_imgs[j])","6161c4a4":"%%time\nfor i in range(last_batch_size):\n    n = np.random.uniform(-1.0, 1.0, [batch_size, z_dim]).astype(np.float32)\n    gen_imgs = sess.run(g, feed_dict={noise: n, is_training: False})\n    gen_imgs = (gen_imgs + 1) \/ 2\n    imsave(os.path.join(GEN_DIR, f'sample_{n_batches}_{i}.png'), gen_imgs[i])","75b10580":"%%time\nshutil.make_archive('images', 'zip', GEN_DIR)","bfcd53db":"len(os.listdir(GEN_DIR))","dd494aba":"!ls -l .","ddae6c50":"!rm -rf generated_dogs\/","2e230e6d":"!rm -rf samples_dogs\/","940472ab":"The script is based on the book **deep interesing**","c51a9e0e":"# 1 Crop image","c149a6f6":"The code below is based on [the amazing script.](https:\/\/www.kaggle.com\/whizzkid\/crop-images-using-bounding-box)","8c7a0e2d":"# 2 DCGAN"}}