{"cell_type":{"58911757":"code","f288169a":"code","fb05c6ef":"code","4886b113":"code","6940dd92":"code","86da29c0":"code","17a28910":"code","b05a2504":"code","34a057e2":"code","def8641d":"code","5890c9b2":"code","a3f68957":"code","308cf361":"code","5928c37d":"code","5f29f4fc":"code","7f91e2f1":"code","d6aed6b3":"markdown","40f04f4c":"markdown","dadd246f":"markdown","1d42abf6":"markdown","9fb8789a":"markdown","22eccedf":"markdown","331a44ce":"markdown","66526197":"markdown","ea151e8e":"markdown","17a4dae6":"markdown","d30cacbc":"markdown","d90a184e":"markdown","cd35eeed":"markdown"},"source":{"58911757":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f288169a":"# import BERT tokenization\n\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","fb05c6ef":"import tokenization\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint","4886b113":"train_data=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","6940dd92":"train_data.head()","86da29c0":"test_data.head()","17a28910":"label = preprocessing.LabelEncoder()\ny = label.fit_transform(train_data['target'])\ny = to_categorical(y)\nprint(y[:5])","b05a2504":"module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","34a057e2":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\ndef bert_encode(texts, tokenizer, max_len=60):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len-len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","def8641d":"def build_model(bert_layer, max_len=60):\n    input_word_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = layers.Dense(1, activation='sigmoid')(clf_output)\n    model = models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","5890c9b2":"train = train_data.fillna(' ')\ntest = test_data.fillna(' ')\nmax_len = 60\ntrain_input = bert_encode(train['location']+' '+train['keyword']+' '+train['text'], tokenizer, max_len=max_len)\ntest_input = bert_encode(test['location']+' '+test['keyword']+' '+test['text'], tokenizer, max_len=max_len)\ntrain_labels = train.target.values","a3f68957":"labels = label.classes_\nprint(labels)","308cf361":"model = build_model(bert_layer, max_len=max_len)\nmodel.summary()","5928c37d":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=16\n)","5f29f4fc":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","7f91e2f1":"submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","d6aed6b3":"**4. How to use BERT?**\n\nBERT can be used for a wide variety of language tasks, while only adding a small layer to the core model: \n1. Classification tasks such as sentiment analysis are done similarly to Next Sentence classification, by adding a classification layer on top of the Transformer output for the [CLS] token. \n2. In Question Answering tasks (e.g. SQuAD v1.1), the software receives a question regarding a text sequence and is required to mark the answer in the sequence. Using BERT, a Q&A model can be trained by learning two extra vectors that mark the beginning and the end of the answer.\n3. In Named Entity Recognition (NER), the software receives a text sequence and is required to mark the various types of entities (Person, Organization, Date, etc) that appear in the text. Using BERT, a NER model can be trained by feeding the output vector of each token into a classification layer that predicts the NER label.","40f04f4c":"Now we are all set to create our model. To do so, we will create a function named build_model that having tf.keras.models.Model class. Inside the function we will define our model layers. Our model will consist of three Dense neural network layers and also dropout layer. We have chosen a learning rate to 1e-5.\n\nRELU function :- With default values, this returns max(x, 0), the element-wise maximum of 0 and the input tensor. Modifying default parameters allows you to use non-zero thresholds, change the max value of the activation, and to use a non-zero multiple of the input for values below the threshold.\n\nSoftmax function :- Softmax converts a real vector to a vector of categorical probabilities. The elements of the output vector are in range (0, 1) and sum to 1. Each vector is handled independently. The axis argument sets which axis of the input the function is applied along. Softmax is often used as the activation for the last layer of a classification network because the result could be interpreted as a probability distribution. The softmax of each vector x is computed as exp(x) \/ tf.reduce_sum(exp(x)).\n\nBinary corssentropy:- Computes the cross-entropy loss between true labels and predicted labels. We can use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1). For each example, there should be a single floating-point value per prediction.","dadd246f":"**Build a BERT layer**","1d42abf6":"**2. Why BERT?**\n\nBERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.\nBERT gives it incredible accuracy and performance on smaller data sets which solves a huge problem in natural language processing.\n\n\n","9fb8789a":"**Build The Model**","22eccedf":"**Import Libraries and Data**","331a44ce":"**Encoding the text**","66526197":"we create a BERT vocab_file in the form a numpy array. We then set the text to lowercase and finally we pass our vocab_file and do_lower_case variables to the Tokenizer object.","ea151e8e":"**3. How does it work?**\n\nBERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT\u2019s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:\n\nToken embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\nSegment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.\nPositional embeddings: A positional embedding is added to each token to indicate its position in the sentence.\n\n","17a4dae6":"**Run the model**","d30cacbc":"**Label encoding of labels**","d90a184e":"**1. What is BERT?**\n\n\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection.\n\n","cd35eeed":"**we create a BERT embedding layer by importing the BERT model from hub.KerasLayer**"}}