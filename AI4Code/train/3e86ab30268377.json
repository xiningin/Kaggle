{"cell_type":{"d04879a1":"code","9a835f46":"code","cbd2b255":"code","94de7da2":"code","b38625d5":"code","333736da":"code","632e4b9b":"code","5cf6f1dd":"code","671689d0":"code","5399d839":"code","1e850376":"code","73770a9c":"code","3f512608":"code","b53a3862":"code","466d5306":"code","abefa0cc":"code","dfa13242":"code","44484828":"markdown","7ab6bbe1":"markdown","a6939471":"markdown","1d10d16e":"markdown","d5f84676":"markdown","8152056a":"markdown","8e9d5e66":"markdown","32e64ac7":"markdown","80d60b32":"markdown","78807159":"markdown","7721f595":"markdown","98811b7c":"markdown","3c344a87":"markdown","a6be79e6":"markdown","a731d718":"markdown"},"source":{"d04879a1":"from tqdm.notebook import tqdm\nfrom collections import Counter\nimport requests\nimport gzip\nimport pandas as pd\nimport json\nimport networkx as nx\nimport numpy as np\nimport shutil\nimport os\nimport random\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('punkt')                  # \u4e0b\u8f7d\u5206\u8bcd\u6240\u9700\u8981\u7684\u8d44\u6e90\nnltk.download('stopwords')              # \u4e0b\u8f7d\u505c\u7528\u8bcd\nstop = set(stopwords.words('english'))  # \u52a0\u8f7d\u505c\u7528\u8bcd","9a835f46":"output_notebook()  # \u52a0\u8f7dbokeh","cbd2b255":"n = 40\nbase_url = \"https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research-public\/open-corpus\/2021-11-01\/\"\nos.mkdir(\"tmp\")\n\n\ndef download(filename):\n    \"\"\"\n    \u4e0b\u8f7d\u51fd\u6570\n    \"\"\"\n    url = base_url+filename\n    response = requests.get(url, stream=True)\n    total_size_in_bytes = int(response.headers.get('content-length', 0))\n    block_size = 1024\n    progress_bar = tqdm(total=total_size_in_bytes, unit='B', unit_scale=True)\n    with open(\"tmp\/\"+filename, 'wb') as file:\n        for data in response.iter_content(block_size):\n            progress_bar.update(len(data))\n            file.write(data)\n    progress_bar.close()\n    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n        print(\"ERROR, something went wrong\")\n\n\nwith open(\"tmp.json\", \"wb\") as f:\n    for i in range(n):\n        filename = f\"s2-corpus-{i:03d}.gz\"\n        download(filename)                  # \u4e0b\u8f7d\n        print(f\"decompressing {filename}\")\n        with gzip.GzipFile(\"tmp\/\"+filename) as g:  # \u89e3\u538b\n            f.write(g.read())\n\nshutil.rmtree(\"tmp\")","94de7da2":"ids = set()\nwith open(\"tmp.json\") as f:\n    for i in tqdm(f, desc=\"Analysing corpus\"):\n        item = json.loads(i)\n        ids.add(item[\"id\"])\n\nnewids = set()\ntotal_citation = 0\nwith open(\"citation.json\", \"w\") as outf:\n    with open(\"tmp.json\") as f:\n        for i in tqdm(f, desc=\"Generating citations\", total=len(ids)):\n            item = json.loads(i)\n            for j in item[\"inCitations\"]:\n                if j in ids:\n                    newids.add(item[\"id\"])\n                    newids.add(j)\n                    outf.write(json.dumps(\n                        {\"reference\": item[\"id\"],\n                         \"citation\": j}))\n                    outf.write(\"\\n\")\n                    total_citation += 1\n\nwith open(\"corpus.json\", \"w\") as outf:\n    with open(\"tmp.json\") as f:\n        for i in tqdm(f, desc=\"Cropping corpus\", total=len(ids)):\n            item = json.loads(i)\n            if item[\"id\"] in newids:\n                outf.write(json.dumps(item, ensure_ascii=False))\n                outf.write(\"\\n\")\n\nos.remove(\"tmp.json\")\nprint(f\"Paper: {len(newids)} Citation:{total_citation}\")","b38625d5":"def load_corpus() -> pd.DataFrame:\n    '''\n    \u8fd4\u56de \u4e00\u4e2a pd.DataFrame \u5bf9\u8c61\n        \u81f3\u5c11\u9700\u8981\u5305\u542b\u4e24\u4e2a\u5217\uff0c\u5217text\u4e3a\u539f\u59cb\u6587\u672c\uff1b\u5217filename\u4e3a\u6587\u4ef6\u540d\n    '''\n    colums = [\"id\", \"title\", \"paperAbstract\", \"fieldsOfStudy\", \"year\"]\n    data = (pd.read_json(\"corpus.json\", lines=True)[colums])\n    data = data.set_index(\"id\")\n    return data\n\n\ndata = load_corpus()\ndata","333736da":"def compute_link_metrics(link_file, max_iter=1000) -> pd.DataFrame:\n    '''\n    \u4f20\u5165 link_file\uff0c\u8be5\u6587\u4ef6\u5305\u542b\u4e86\u8fb9\u7684\u5173\u7cfb\n    \u8fd4\u56de \u4e00\u4e2a pd.DataFrame \u5bf9\u8c61\n        \u5fc5\u987b\u5305\u542bfilename\u5217\uff0c\u540cload_corpus\u7684\u7ed3\u679c\u4e00\u81f4\n        \u53e6\u5916\u81f3\u5c11\u5305\u542bPR, A, H\u4e09\u5217\u4e2d\u7684\u4e00\u5217\uff0c\u5206\u522b\u4ee3\u8868PageRank\u503c\uff0cAuthority\u503c\u548cHub\u503c\n    '''\n    citation = pd.read_json(link_file, lines=True)\n    G = nx.DiGraph()\n    for _, i in citation.iterrows():\n        G.add_edge(i[\"citation\"], i[\"reference\"])\n\n    # \u8ba1\u7b97PageRank\n    pr = pd.Series(nx.pagerank(G, alpha=0.85, max_iter=max_iter), name=\"PR\")\n\n    # \u8ba1\u7b97HTIS\n    h, a = nx.hits(G, max_iter=max_iter)\n    h = pd.Series(h, name=\"H\")\n    a = pd.Series(a, name=\"A\")\n\n    return pd.concat([pr, a, h], axis=1)\n\n\nlink_metrics = compute_link_metrics(\"citation.json\")\nlink_metrics","632e4b9b":"def is_en(x):\n    return all(ord(c) <= 122 and ord(c) >= 65 for c in x)\n\n\ndef generate_tokens(col=\"title\"):\n    tokens = {k: [i for i in nltk.word_tokenize(v) if is_en(i) and i not in stop] for k,v in tqdm(data[col].to_dict().items())}\n    return tokens\n\n\ntokens = generate_tokens(\"paperAbstract\")\n\navgdl = np.mean(list(map(lambda x: len(x), tokens.values())))\navgdl","5cf6f1dd":"vocabulary = []\nfor k, v in tokens.items():\n    vocabulary += list(set(tokens[k]))\ndf = nltk.FreqDist(vocabulary)\nvocabulary = sorted(list(set(vocabulary)))\nrandom.sample(vocabulary, 10)","671689d0":"def search(query, topk=10, k1=1.6, b=0.75) -> pd.DataFrame:\n    '''\n    query\u662f\u4e00\u4e2a\u7eaf\u6587\u672c\n    \u8fd4\u56de\u6700\u76f8\u5173\u7684\u5341\u4e2a\u6587\u6863\u7684DataFrame\uff0c\u6309\u76f8\u5173\u6027\u964d\u5e8f\u6392\u5217\n    \u81f3\u5c11\u5305\u542b\u4e09\u4e2a\u5b57\u6bb5\n        filename: \u539f\u59cb\u6587\u4ef6\u540d\n        text: \u539f\u59cb\u6587\u672c\n        score: \u76f8\u5173\u5f97\u5206\n    '''\n    query = [i for i in nltk.word_tokenize(query) if i.isalpha() and i not in stop]\n    result = {}\n    for k, d in tokens.items():\n        if len(d) == 0:\n            continue\n        score = 0\n        for q in query:\n            idf = np.log(1+(len(tokens) - df[q] + 0.5) \/ (df[q] + 0.5))\n            fqd = Counter(d)[q]\/len(d)\n            right_part = (fqd*(k1+1))\/(fqd+k1*(1-b+b*len(d)\/avgdl))\n            score += idf*right_part\n        result[k] = score\n    result = pd.Series(result, name=\"score\").sort_values(ascending=False).iloc[:topk].to_frame()\n    return result.join(data).join(link_metrics)\n\n\nsearch(\"computer networks\")","5399d839":"from bokeh.layouts import column,row\nfrom bokeh.transform import linear_cmap\nfrom bokeh.models import OpenURL, TapTool,Circle, Select, Slider,CustomJS,ColumnDataSource\n\n\ndef visualization(query, result, decomposition_data, n_clusters):\n    \"\"\"\n    \u53ef\u89c6\u5316\u51fd\u6570\n    \"\"\"\n    TOOLTIPS = [\n        (\"Title\", \"@title\"),\n        (\"Score\", \"@score\"),\n        (\"Year\", \"@year\"),\n        (\"Fields\", \"@fieldsOfStudy\"),\n        (\"PageRank\", \"@PR\"),\n        (\"Hubs\", \"@H\"),\n        (\"Authorities\", \"@A\")\n    ]\n\n    source = pd.DataFrame(decomposition_data, columns=[\"x\", \"y\"], index=result.index).join(result)\n    source = source.join(pd.Series([10 if i < 10 else 5 for i in range(100)], index=result.index, name=\"size\"))\n    colsource = ColumnDataSource(source)\n\n    tools = \"pan, wheel_zoom, zoom_in, zoom_out, reset, tap, save\"\n    scatter = figure(height=600, width=600, tooltips=TOOLTIPS, tools=tools, title=f\"\u5173\u952e\u8bcd {query} Top100\u67e5\u8be2\u7ed3\u679c\u805a\u7c7b\u5e76\u53ef\u89c6\u5316\")\n    r = scatter.circle('x', 'y', source=colsource, color=linear_cmap('label', 'Viridis256', 0, n_clusters),size=\"size\")\n    r.selection_glyph = Circle(fill_color=linear_cmap('label', 'Viridis256', 0, n_clusters), line_color=None)\n    r.nonselection_glyph = Circle(fill_color=linear_cmap('label', 'Viridis256', 0, n_clusters), line_color=None)\n\n    # \u5b9e\u73b0score\u7b5b\u9009\n    score = Slider(title=\"min score\", value=0, start=0, end=source[\"score\"].max(), step=0.01)\n    source_copy = ColumnDataSource(source.copy())\n    score_callback = CustomJS(args=dict(source=colsource, s2=source_copy),\n                    code=\"\"\"\n    const data = s2.data;\n    const score = cb_obj.value;\n    const s1 = source.data\n    const colums = Object.keys(s1)\n    for (let i = 0; i < colums.length; i++) {\n        s1[colums[i]]=[]\n    }\n    for (let i = 0; i < data['score'].length; i++) {\n            if(data['score'][i]>=score){\n                for (let j = 0; j < colums.length; j++) {\n                    s1[colums[j]].push(data[colums[j]][i])\n                }\n            }\n    }\n    source.change.emit();\n    \"\"\")\n    score.js_on_change('value', score_callback)\n\n    # \u5b9e\u73b0select\u7b5b\u9009\n    select = Select(title=\"Select\", options=[\"All\",\"Top 10\"], value=\"All\")\n    select_callback = CustomJS(args=dict(source=colsource, s2=source_copy),\n                    code=\"\"\"\n    const data = s2.data;\n    const opt = cb_obj.value;\n    const s1 = source.data\n    const colums = Object.keys(s1)\n    for (let i = 0; i < colums.length; i++) {\n        s1[colums[i]]=[]\n    }\n    const n = (opt===\"All\")?data['score'].length:10;\n    for (let i = 0; i < n; i++) {\n                for (let j = 0; j < colums.length; j++) {\n                    s1[colums[j]].push(data[colums[j]][i])\n                }\n    }\n    source.change.emit();\n    \"\"\")\n    select.js_on_change('value', select_callback)\n\n    # \u5b9e\u73b0\u70b9\u51fb\u6548\u679c\n    url = \"https:\/\/semanticscholar.org\/paper\/@index\"\n    taptool = scatter.select(type=TapTool)\n    taptool.callback = OpenURL(url=url)\n\n    show(row(column(select, score), scatter))","1e850376":"from sklearn.feature_extraction.text import TfidfTransformer,CountVectorizer \nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\n\n\ndef search_with_cluster(query,n_clusters=10) -> pd.DataFrame:\n    '''\n    query: \u6240\u9700\u8981\u67e5\u8be2\u7684\u5b57\u7b26\u4e32\n    n_clusters: \u805a\u7c7b\u7684\u7c07\u6570\n    \u8fd4\u56de\u6700\u76f8\u5173\u7684100\u4e2a\u6587\u6863\u7684DataFrame\uff0c\u6309\u76f8\u5173\u6027\u964d\u5e8f\u6392\u5217\n    \u81f3\u5c11\u5305\u542b\u56db\u4e2a\u5b57\u6bb5\n        filename: \u539f\u59cb\u6587\u4ef6\u540d\n        text: \u539f\u59cb\u6587\u672c\n        score: \u76f8\u5173\u5f97\u5206\n        cluster: \u805a\u7c7b\u7684\u7c7b\u522b\n    '''\n    # \u8ba1\u7b97TF-IDF\n    result = search(query, 100)\n    vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x)\n    transformer = TfidfTransformer()\n    result_corpus = [tokens[i] for i in result.index]\n    tfidf = transformer.fit_transform(vectorizer.fit_transform(result_corpus))\n    weight = tfidf.toarray()\n\n    # \u805a\u7c7b\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(weight)\n\n    # \u964d\u7ef4\n    tsne = TSNE(n_components=2)\n    decomposition_data = tsne.fit_transform(weight)\n\n    # \u5c01\u88c5\u7ed3\u679c\n    kmeans_result = pd.Series(kmeans.labels_, index=result.index,name=\"label\")\n    result = result.join(kmeans_result)\n\n    # \u53ef\u89c6\u5316\n    visualization(query, result, decomposition_data, n_clusters)\n\n    return result\n\n\nresult = search_with_cluster(\"computer networks\")\nresult","73770a9c":"search(\"deep learning\")","3f512608":"search_with_cluster('deep learning')","b53a3862":"search(\"neural networks\")","466d5306":"search_with_cluster(\"neural networks\")","abefa0cc":"search(\"natural language processing\")","dfa13242":"search_with_cluster(\"natural language processing\")","44484828":"## \u6848\u4f8b\uff1aneural networks","7ab6bbe1":"## \u68c0\u7d22\u7b97\u6cd5\u5b9e\u73b0\n\n\u68c0\u7d22\u7b97\u6cd5\u4e3aBM25","a6939471":"# \u805a\u7c7b\u5206\u6790\n* \u6539\u9020`search`\u51fd\u6570\uff0c\u5bf9`search`\u7684 top 100 \u7684\u7ed3\u679c \u8fdb\u884cTF-IDF\u5411\u91cf\u5316\uff0c\u5e76\u805a\u7c7b\u3002\n* \u6bcf\u6b21\u67e5\u8be2\u53ef\u89c6\u5316100\u7bc7\u6587\u6863\uff0c\u53ca\u5176\u7c7b\u522b\uff0c\u540c\u65f6\u5728\u56fe\u4e2d\u6807\u6ce8\u51fatop 10\u7684\u6587\u6863\n\n> TIPS:\n> \n> \u4e0d\u540c\u989c\u8272\u7684\u70b9\u4ee3\u8868\u4e0d\u540c\u7684\u7c7b\n> \u5f53\u9f20\u6807\u653e\u5728\u6570\u636e\u70b9\u4e0a\u662f\u53ef\u4ee5\u5c55\u793a\u5bf9\u5e94\u8bba\u6587\u7684\u76f8\u5e94\u4fe1\u606f\uff0c\u70b9\u51fb\u5b83\u8fd8\u80fd\u8f6c\u8df3\u81f3\u5bf9\u5e94\u7684\u8bba\u6587\u754c\u9762\uff0cTOP10\u7684\u5927\u5c0f\u4f1a\u66f4\u5927\n","1d10d16e":"## \u5206\u8bcd\n\n\u4f7f\u7528nltk\u5206\u6790\uff0c\u5e76\u5220\u53bb\u505c\u7528\u8bcd\u548c\u975e\u82f1\u8bed\u5355\u8bcd","d5f84676":"# \u6848\u4f8b\u6d4b\u8bd5\n* \u8bf7\u5728\u8fd9\u91cc\u7ed9\u51fa\u81f3\u5c113\u4e2aquery\uff0c\u5e76\u6253\u5370`search`\u548c`search_with_cluster`\u7684\u7ed3\u679c","8152056a":"## \u722c\u53d6\u6570\u636e","8e9d5e66":"# \u68c0\u7d22\u6a21\u578b\n* \u8bf7\u5b8c\u6210`search`\u51fd\u6570\n* \u8bf7\u8be6\u7ec6\u53d9\u8ff0\u6a21\u578b\u7684\u601d\u8def\u3001\u7ec6\u8282\n","32e64ac7":"## \u6848\u4f8b\uff1anatural language processing","80d60b32":"# \u8bed\u6599\n\n- \u6570\u636e\u6e90\uff1a[Semantic Scholar](https:\/\/www.semanticscholar.org\/)\n- \u6536\u96c6\u65b9\u5f0f\uff1a\u901a\u8fc7\u5176[Open Corpus](https:\/\/api.semanticscholar.org\/corpus)\u670d\u52a1\u722c\u53d6\u5176\u50a8\u5b58\u7684\u6587\u732e\u6570\u636e\uff0c\u6587\u732e\u6570\u636e\u5728[\u5f00\u653e\u6570\u636e ODC-BY](https:\/\/opendatacommons.org\/licenses\/by\/1-0\/)\u8bb8\u53ef\u4e0b\u4f7f\u7528\u548c\u4f20\u64ad\n- \u6587\u732e\u6570\u636e\u88ab\u5206\u6210\u4e866k\u5757\u4ee5Json\u683c\u5f0f\u5e76\u4f7f\u7528bzip\u7b97\u6cd5\u538b\u7f29\u50a8\u5b58\u5728AWS\u50a8\u5b58\u670d\u52a1\u4e2d\uff0c\u5f53\u6211\u4eec\u53ea\u9009\u53d6\u524d40\u5757\u65f6\uff0c\u6587\u732e\u6570\u91cf\u51711321518\u7bc7\uff0c\u901a\u8fc7\u6e05\u6d17\u540e\u83b7\u53d6\u5230\u4e00\u5171119588\u7bc7\u6587\u732e\u548c85356\u5f15\u7528\u8bb0\u5f55\u3002","78807159":"## \u7edf\u8ba1\u8bcd\u5178\u548cDF","7721f595":"# \u94fe\u63a5\u5206\u6790\n* \u4f8b\u5982\u8bba\u6587\u7684\u5f15\u7528\u5173\u7cfb\u3001\u5fae\u535a\u7684\u8f6c\u53d1\u5173\u7cfb\uff1b\n\n\n* \u5982\u679c\u6709\u94fe\u63a5\u5173\u7cfb\uff0c\u8bf7\u5b8c\u6210\u8be5\u90e8\u5206\n* \u5b9e\u73b0`compute_link_metrics`\u51fd\u6570","98811b7c":"# \u5bfc\u5165\u9700\u8981\u7684\u5e93","3c344a87":"# \u52a0\u8f7d\u8bed\u6599\n* \u5b9e\u73b0 `load_corpus` \u51fd\u6570\n","a6be79e6":"## \u6848\u4f8b\uff1aDeep Learning","a731d718":"## \u6e05\u6d17\u6570\u636e\n\n\u53ea\u9009\u53d6\u6570\u636e\u96c6\u4e2d\u6709\u5f15\u7528\u6587\u732e\u548c\u88ab\u5f15\u6587\u732e\u90fd\u5728\u5b50\u96c6\u7684\u4e2d\u7684\u6570\u636e\u90e8\u5206\n\n\u5e76\u6784\u5efa\u5f15\u7528\u4e0e\u88ab\u5f15\u7528\u5173\u7cfb"}}