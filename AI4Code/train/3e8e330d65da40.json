{"cell_type":{"b05751c2":"code","f751eed4":"code","e81e6fc5":"code","63eb826e":"code","8af37bb5":"code","6dfc1027":"code","5331827d":"code","7c1bf10c":"code","3f6ce80c":"code","7aca0f86":"code","af979d8c":"code","68a1a816":"code","4b55a158":"code","bb137006":"code","0526a401":"code","0eed2b5f":"code","790f4f1c":"code","8133d8d2":"code","25f2a101":"code","3686c161":"code","e3c63699":"code","e3b32587":"code","ab3a472d":"code","847e68f1":"code","e57981e5":"code","2b17cd9e":"code","4efeb95b":"code","f3758d7e":"code","f7a336db":"code","ea2602c7":"code","fba608fb":"code","74ac6783":"code","dfabc0eb":"code","7f88c011":"code","ec301794":"code","9ab1d670":"code","b5e2e7ad":"code","5946c88c":"code","afa781aa":"code","eb9d9401":"code","f1d2d761":"code","e1ca2747":"code","dcf9aab3":"code","497087f3":"code","7ac635f4":"markdown","f99b0aec":"markdown","c3d744b1":"markdown","b4e0920f":"markdown","1d439f45":"markdown","31843871":"markdown","6a7ec08a":"markdown","707c01ac":"markdown","46056862":"markdown","13dfe1b8":"markdown","74b8fb20":"markdown","9cace44f":"markdown","4b4585bc":"markdown","b2c76b65":"markdown"},"source":{"b05751c2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge, LogisticRegression\nimport time\nfrom sklearn import preprocessing\nimport warnings\nimport datetime\nwarnings.filterwarnings(\"ignore\")\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom scipy.stats import describe, rankdata\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nimport xgboost as xgb\n","f751eed4":"\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e81e6fc5":"#Loading Train and Test Data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"{} observations and {} features in train set.\".format(train.shape[0],train.shape[1]))\nprint(\"{} observations and {} features in test set.\".format(test.shape[0],test.shape[1]))","63eb826e":"train.head()","8af37bb5":"test.head()","6dfc1027":"train.target.describe()","5331827d":"train[train.columns[2:]].describe()","7c1bf10c":"test[test.columns[1:]].describe()","3f6ce80c":"plt.figure(figsize=(12, 5))\nplt.hist(train['0'].values, bins=20)\nplt.title('Histogram 0 train counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","7aca0f86":"plt.figure(figsize=(12, 5))\nplt.hist(train['1'].values, bins=20)\nplt.title('Histogram 1 train counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","af979d8c":"plt.figure(figsize=(12, 5))\nplt.hist(train['123'].values, bins=20)\nplt.title('Histogram 123 counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","68a1a816":"plt.figure(figsize=(12, 5))\nplt.hist(test['0'].values, bins=20)\nplt.title('Histogram 0 test counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","4b55a158":"plt.figure(figsize=(12, 5))\nplt.hist(test['1'].values, bins=20)\nplt.title('Histogram 1 test counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","bb137006":"plt.figure(figsize=(12, 5))\nplt.hist(test['123'].values, bins=20)\nplt.title('Histogram 123 test counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","0526a401":"train_test = pd.concat([train[train.columns[2:]], test[test.columns[1:]]])","0eed2b5f":"train_test.shape","790f4f1c":"corr = train_test.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","8133d8d2":"AUCs = []\nGinis = []\n\n\nfor i in range(300):\n    AUC = roc_auc_score(train.target.values, train[str(i)].values)\n    AUCs.append(AUC)\n    Gini = 2*AUC - 1\n    Ginis.append(Gini)\n","25f2a101":"np.sort(np.abs(Ginis))[::-1]","3686c161":"np.argsort(np.abs(Ginis))[::-1]","e3c63699":"[Ginis[k] for k in np.argsort(np.abs(Ginis))[::-1][:13]]","e3b32587":"roc_auc_score(train.target.values,train['33'].values)","ab3a472d":"roc_auc_score(train.target.values,train['65'].values)","847e68f1":"roc_auc_score(train.target.values,-train['217'].values)","e57981e5":"roc_auc_score(train.target.values,-train['117'].values)","2b17cd9e":"roc_auc_score(train.target.values,-train['91'].values)","4efeb95b":"roc_auc_score(train.target.values,-train['295'].values)","f3758d7e":"roc_auc_score(train.target.values,train['24'].values)","f7a336db":"roc_auc_score(train.target.values,train['199'].values)","ea2602c7":"roc_auc_score(train.target.values,-train['80'].values)","fba608fb":"roc_auc_score(train.target.values,-train['73'].values)","74ac6783":"roc_auc_score(train.target.values,-train['194'].values)","dfabc0eb":"roc_auc_score(train.target.values,0.146*train['33'].values + 0.12*train['65'].values-0.06*train['217'].values-0.05*train['117'].values\n             -0.05*train['91'].values-0.05*train['295'].values+0.05*train['24'].values+0.05*train['199'].values-\n             0.05*train['80'].values- 0.05*train['73'].values-0.05*train['194'].values)","7f88c011":"preds = (0.146*test['33'].values + 0.12*test['65'].values-0.06*test['217'].values-0.05*test['117'].values\n             -0.05*test['91'].values-0.05*test['295'].values+0.05*test['24'].values+0.05*test['199'].values-\n             0.05*test['80'].values- 0.05*test['73'].values-0.05*test['194'].values)\npreds = rankdata(preds)\/preds.shape[0]\npreds","ec301794":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = preds\nsample_submission.to_csv('submission.csv', index=False)","9ab1d670":"pca = PCA(n_components=0.99)\npca.fit(train_test.values)","b5e2e7ad":"pca.n_components_","5946c88c":"pca = PCA(n_components=0.9)\npca.fit(train_test.values)\npca.n_components_","afa781aa":"Sum_of_squared_distances = []\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(train_test)\n    Sum_of_squared_distances.append(km.inertia_)","eb9d9401":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","f1d2d761":"NN = 80\n\ntrain_pred = 0\n\ngini_list = [Ginis[k] for k in np.argsort(np.abs(Ginis))[::-1][:NN]]\n\nfor i in range(NN):\n    if gini_list[i] > 0:\n        train_pred += train[str(np.argsort(np.abs(Ginis))[::-1][i])].values\n    else:\n        train_pred -= train[str(np.argsort(np.abs(Ginis))[::-1][i])].values\n        \nroc_auc_score(train.target.values, train_pred)","e1ca2747":"test_pred = 0\n\ngini_list = [Ginis[k] for k in np.argsort(np.abs(Ginis))[::-1][:NN]]\n\nfor i in range(NN):\n    if gini_list[i] > 0:\n        test_pred += test[str(np.argsort(np.abs(Ginis))[::-1][i])].values\n    else:\n        test_pred -= test[str(np.argsort(np.abs(Ginis))[::-1][i])].values","dcf9aab3":"test_pred = rankdata(test_pred)\/test_pred.shape[0]\ntest_pred","497087f3":"sample_submission['target'] = test_pred\nsample_submission.to_csv('submission_80.csv', index=False)","7ac635f4":"Now let's look at the data","f99b0aec":"There really doesn't seem to be an optimal number of clusters. Sems that this dataset is as scale-free adn uncorrelated as they come!","c3d744b1":"Seems that **all** of the features are numerical, with approximately 0 mean and 1.0 standard deviation. That's very interesting. \n\nLet's now look at the distributions of various features in the train set:","b4e0920f":"Based on this **very** limited subsample, seems that all of the features are approaximately normally distributed.","1d439f45":"Now we'll join train and test numerical featues into a single dataset, and explore how features are correlated with each other. ","31843871":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","6a7ec08a":"Based on these values, we'll create a **very** simple blend and see how well it does:","707c01ac":"Now we'll take a look at the top most-correlated featues and see how good of an AUC they oir their opposites get with the target. ","46056862":"So as expected, it **does** overfit - we \"only\" get 0.814 on public LB. Still, this is not bad - it beats several \"benchmark\" algorithms, while using **NO** machine learning!!!","13dfe1b8":"So PCA doesn't seem to help much here.\n\nLet's take a look at clustering. We'll try to fit the KMeans clustering on the entire dataset, and try to see what the optimal number of clusters is.","74b8fb20":"Since the target is binary, we see that almost 2\/3 of the target belongs to the positive class, 0r 0.64 x 250 = 160.","9cace44f":"Not bad! AUC of 0.903 is pretty good for any predictive model. But is it overfitting? There is only one way to find out! Let's submit it and see how it performs on public LB.","4b4585bc":"Next, we'll do some unviariate estimations of the target. We'll try to find the features that best predict the target by themselves. ","b2c76b65":"Huh! That looks like all the features are almost completely uncorrelated!"}}