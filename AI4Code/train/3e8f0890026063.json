{"cell_type":{"53d66c67":"code","2c65eda1":"code","ad6852c8":"code","03785852":"code","530d949d":"code","cb537a85":"code","9095a268":"code","0be809ce":"code","9c2e4812":"code","74df5301":"code","d2c7ddde":"code","e3f61672":"code","b4893ddf":"code","3c2ca3f5":"code","0e8895ae":"code","c3682046":"code","c21614ff":"code","57de9c21":"code","68e3a8af":"code","17ee8231":"code","0fb8c425":"code","6a2965a6":"code","358cea53":"code","b472ee9d":"code","e12a52d0":"code","dc2453d5":"code","8864b597":"code","2d6a9504":"code","61e5d0cb":"code","95add991":"code","d1acd605":"code","f973a3aa":"code","14371318":"code","6ddd81f9":"code","19d44f34":"code","fe4ac3a3":"code","294cd15b":"code","e2ce470a":"code","487fab8a":"code","fa0796ae":"code","35df9f42":"code","e84e7260":"code","1adfbf78":"code","d2ba629b":"code","92441d91":"code","74e49566":"code","0556dcd6":"code","94cb1d72":"code","8baca440":"code","1440e872":"code","508e043f":"code","5872a33e":"code","daf98145":"code","00186421":"code","359d6493":"code","177b2491":"code","d9939e9f":"code","93a6510e":"code","57a51637":"code","2b5f2b7a":"code","9f940782":"code","8c7a54f2":"code","80d9a252":"code","8f76c875":"code","7684a295":"code","a0d09cb9":"code","998d0863":"code","58370509":"code","5d444384":"code","68a4c240":"markdown","c96871df":"markdown","c08464d4":"markdown","d26be413":"markdown","4b574eae":"markdown","8fea2aac":"markdown","84908bef":"markdown","0d65734d":"markdown","64bd5d09":"markdown","22ab8d97":"markdown","d99b5f9a":"markdown","b6ce9c48":"markdown"},"source":{"53d66c67":"import matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd","2c65eda1":"from scipy.io import loadmat\nmnist = loadmat(\"..\/input\/mnist-original\")\n","ad6852c8":"dict.keys(mnist)","03785852":"y = mnist[\"label\"][0]\nx = mnist[\"data\"].T","530d949d":"print(x.shape)\nprint(y.shape)","cb537a85":"def image(num1, num2, S):\n    plt.gray()\n    plt.subplot(3,4,num1)\n    plt.imshow((16-S[num2]\/255*16).reshape(28,28))\n    return\nJ = {(i+1):(i*6650) for i in list(range(10))}\nfor i in list(range(11))[1:11]: image(i,J[i],x)","9095a268":"from sklearn.model_selection import train_test_split","0be809ce":"test_x, train_x, test_y, train_y = train_test_split(x, y, test_size=1\/7.0, random_state=0)\nprint(train_x.shape)\nprint(test_x.shape)","9c2e4812":"from sklearn.decomposition import PCA\n","74df5301":"pca64 = PCA(n_components=64)\nX_reduced = pca64.fit_transform(train_x)\n\nprint('Projecting %d-dimensional data to 2D' % train_x.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection')","d2c7ddde":"pca_2 = PCA(n_components=2)\nX_reduced = pca_2.fit_transform(train_x)\n\nprint('Projecting %d-dimensional data to 2D' % train_x.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection')","e3f61672":"pca = PCA().fit(train_x)\n\nplt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.xlim(0, 784)\nplt.yticks(np.arange(0, 1.1, 0.1))\n\nplt.axhline(0.9, c='c')\nplt.show();","b4893ddf":"pca1 = PCA(n_components=0.9)\nX_reduced1 = pca1.fit_transform(train_x)\n\nprint('Projecting %d-dimensional data to 2D' % train_x.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced1[:, 0], X_reduced1[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection')","3c2ca3f5":"pca1.n_components_","0e8895ae":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, Normalizer","c3682046":"scaler1 = MinMaxScaler()\nscaler2 = StandardScaler()\nscaler3 = RobustScaler()\nscaler4 = Normalizer()\nscaler1.fit(train_x)\nscaler2.fit(train_x)\nscaler3.fit(train_x)\nscaler4.fit(train_x)","c21614ff":"train_img01 = scaler1.transform(train_x)\ntest_img01 = scaler1.transform(train_x)\ntrain_img02 = scaler2.transform(train_x)\ntest_img02 = scaler2.transform(train_x)\ntrain_img03 = scaler3.transform(train_x)\ntest_img03 = scaler3.transform(train_x)\ntrain_img04 = scaler4.transform(train_x)\ntest_img04 = scaler4.transform(train_x)","57de9c21":"pca_minmax = PCA(n_components=0.9)\nX_reduced2 = pca_minmax.fit_transform(train_img01)\n\nprint('Projecting %d-dimensional data to 2D' % train_img01.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced2[:, 0], X_reduced2[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection. MinMaxScaler. n_components = %d' % pca_minmax.n_components_)","68e3a8af":"pca_stsc = PCA(n_components=0.9)\nX_reduced2 = pca_stsc.fit_transform(train_img02)\n\nprint('Projecting %d-dimensional data to 2D' % train_img02.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced2[:, 0], X_reduced2[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection. StandardScaler.  n_components = %d' % pca_stsc.n_components_)","17ee8231":"pca_rsc = PCA(n_components=0.9)\nX_reduced2 = pca_rsc.fit_transform(train_img03)\n\nprint('Projecting %d-dimensional data to 2D' % train_img03.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced2[:, 0], X_reduced2[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection. RobustScaler. n_components = %d' % pca_rsc.n_components_)","0fb8c425":"pca_norm = PCA(n_components=0.9)\nX_reduced2 = pca_norm.fit_transform(train_img04)\n\nprint('Projecting %d-dimensional data to 2D' % train_img04.shape[1])\n\nplt.figure(figsize=(12,10))\nplt.scatter(X_reduced2[:, 0], X_reduced2[:, 1], c=train_y, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. PCA projection. Normalizer.  n_components = %d' % pca_norm.n_components_)","6a2965a6":"pca01 = PCA().fit(train_img01)\npca02 = PCA().fit(train_img02)\npca03 = PCA().fit(train_img03)\npca04 = PCA().fit(train_img04)\n\nplt.figure(figsize=(10,7))\nplt.plot(np.cumsum(pca01.explained_variance_ratio_), color='m', lw=2, label='MinMaxScaler')\nplt.plot(np.cumsum(pca02.explained_variance_ratio_), color='r', lw=2, label='StandardScaler')\nplt.plot(np.cumsum(pca03.explained_variance_ratio_), color='g', lw=2, label='RobustScaler')\nplt.plot(np.cumsum(pca04.explained_variance_ratio_), color='b', lw=2, label='Normalizer')\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\nplt.xlim(0, 784)\nplt.yticks(np.arange(0, 1.1, 0.1))\nplt.legend();\nplt.axhline(0.9, c='c')\nplt.show()","358cea53":"train_img64 = pca64.transform(train_x)\ntest_img64 = pca64.transform(train_x)\ntrain_img_2 = pca_2.transform(train_x)\ntest_img_2 = pca_2.transform(train_x)\ntrain_img1 = pca1.transform(train_x)\ntest_img1 = pca1.transform(train_x)\ntrain_minmax = pca_minmax.transform(train_x)\ntest_minmax = pca_minmax.transform(train_x)\ntrain_stsc = pca_stsc.transform(train_x)\ntest_stsc = pca_stsc.transform(train_x)\ntrain_rsc = pca_rsc.transform(train_x)\ntest_rsc = pca_rsc.transform(train_x)\ntrain_norm = pca_norm.transform(train_x)\ntest_norm = pca_norm.transform(train_x)","b472ee9d":"approximation1 = pca_minmax.inverse_transform(train_minmax)\napproximation2 = pca_stsc.inverse_transform(train_stsc)\napproximation3 = pca_rsc.inverse_transform(train_rsc)\napproximation4 = pca_norm.inverse_transform(train_norm)","e12a52d0":"#approximation1 = scaler1.inverse_transform(approximation1)\n#approximation2 = scaler2.inverse_transform(approximation2)\napproximation3 = scaler3.inverse_transform(approximation3)\n","dc2453d5":"plt.figure(figsize=(16,8));\n\n# Original Image\nplt.subplot(1, 5, 1);\nplt.imshow(train_x[1].reshape(28,28), \n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('784 components', fontsize = 14)\nplt.title('Original Image', fontsize = 10);\n\n# MimMax\nplt.subplot(1, 5, 2);\nplt.imshow(approximation1[1].reshape(28, 28), \n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('%d components' % pca_minmax.n_components_, fontsize = 14)\nplt.title('90% of Explained Variance', fontsize = 10);\n\n#Standart\nplt.subplot(1, 5, 3);\nplt.imshow(approximation2[1].reshape(28, 28), \n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('%d components' % pca_stsc.n_components_, fontsize = 14)\nplt.title('90% StandartScaler', fontsize = 10);\n\n#Robust\nplt.subplot(1, 5, 4);\nplt.imshow(approximation3[1].reshape(28, 28), \n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('%d components' % pca_rsc.n_components_, fontsize = 14)\nplt.title('90% RobustScaler', fontsize = 10);\n\n# Normal\nplt.subplot(1, 5, 5);\nplt.imshow(approximation4[1].reshape(28, 28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('%d components' % pca_norm.n_components_, fontsize = 14)\nplt.title('90% Normalizer', fontsize = 10);","8864b597":"from sklearn.cluster import KMeans, AgglomerativeClustering, Birch\nfrom sklearn.metrics import v_measure_score, silhouette_score","2d6a9504":"def solver(Clust_met, train_x, train_y, n_cl):\n    Sil = np.zeros((1,n_cl))\n    Vsc = np.zeros((1,n_cl))\n    for k in list(range(n_cl))[2:n_cl]:\n        X = Clust_met(n_clusters = k).fit_predict(train_x)\n        Sil[0][k] = silhouette_score(train_x, X, metric='euclidean')\n        Vsc[0][k] = v_measure_score(train_y, X)\n        #print('silhouette_score', Sil[0][k])\n        #print('v_measure_score', Vsc[0][k])\n        #print('-'*10)\n    return Sil, Vsc","61e5d0cb":"Sil1, Vsc1 = solver(KMeans, train_x, train_y, 30)","95add991":"Sil2, Vsc2 = solver(AgglomerativeClustering, train_x, train_y, 30)","d1acd605":"fig = plt.figure()\nax = plt.axes()\nK = np.arange(0, 30)\nax.plot(K, Sil1[0], label = \"KMeans\")\nax.plot(K, Sil2[0], label = \"AgglomerativeClustering\")\nplt.legend()\nplt.xlabel(\"K\")\nplt.ylabel(\"Silhouette\")\nplt.title('Original Data', fontsize = 14)","f973a3aa":"fig = plt.figure()\nax = plt.axes()\nK = np.arange(0, 30)\nax.plot(K, Vsc1[0], label = \"KMeans\")\nax.plot(K, Vsc2[0], label = \"AgglomerativeClustering\")\nplt.legend()\nplt.xlabel(\"K\")\nplt.ylabel(\"v_measure_score\")\nplt.title('Original Data', fontsize = 14)","14371318":"Sil1_norm, Vsc1_norm, = solver(KMeans, train_norm, train_y, 30)","6ddd81f9":"Sil2_norm, Vsc2_norm = solver(AgglomerativeClustering, train_norm, train_y, 30)","19d44f34":"Sil1_rsc, Vsc1_rsc, = solver(KMeans, train_rsc, train_y, 30)","fe4ac3a3":"Sil2_rsc, Vsc2_rsc = solver(AgglomerativeClustering, train_rsc, train_y, 30)","294cd15b":"fig = plt.figure()\nax = plt.axes()\nK = np.arange(0, 30)\nax.plot(K, Sil1_norm[0], label = \"KMeans_norm\")\nax.plot(K, Sil1_rsc[0], label = \"KMeans_rsc\")\nax.plot(K, Sil2_norm[0], label = \"AgglomerativeClustering_norm\")\nax.plot(K, Sil2_rsc[0], label = \"AgglomerativeClustering_rsc\")\nplt.legend()\nplt.xlabel(\"K\")\nplt.ylabel(\"Silhouette\")\nplt.title('PCA Data', fontsize = 14)","e2ce470a":"fig = plt.figure()\nax = plt.axes()\nK = np.arange(0, 30)\nax.plot(K, Vsc1_norm[0], label = \"KMeans_norm\")\nax.plot(K, Vsc1_rsc[0], label = \"KMeans_rsc\")\nax.plot(K, Vsc2_norm[0], label = \"AgglomerativeClustering_norm\")\nax.plot(K, Vsc2_rsc[0], label = \"AgglomerativeClustering_rsc\")\nplt.legend()\nplt.xlabel(\"K\")\nplt.ylabel(\"v_measure_score\")\nplt.title('PCA Data', fontsize = 14)","487fab8a":"from sklearn.manifold import TSNE ","fa0796ae":"tsne = TSNE().fit_transform(train_x)","35df9f42":"x_tsne = (tsne.T[0]).T\ny_tsne = (tsne.T[1]).T","e84e7260":"plt.figure(figsize=(12,10))\nplt.scatter(x_tsne, y_tsne, c = train_y, edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. tSNE' )","1adfbf78":"XAGG_tsne = AgglomerativeClustering(n_clusters = 10).fit_predict(tsne)","d2ba629b":"XKM_tsne = KMeans(n_clusters = 10, n_jobs=-1).fit_predict(tsne)","92441d91":"XBir_tsne = Birch(n_clusters=10).fit_predict(tsne)","74e49566":"x11 = pd.DataFrame(XAGG_tsne)\nx12 = pd.DataFrame(XKM_tsne)\nx13 = pd.DataFrame(XBir_tsne)","0556dcd6":"X122 = pd.concat([x11, x12, x13], axis=1)","94cb1d72":"X122.head()","8baca440":"silhouette_score(tsne, XAGG_tsne, metric='euclidean')","1440e872":"silhouette_score(tsne, XKM_tsne, metric='euclidean')","508e043f":"silhouette_score(tsne, XBir_tsne, metric='euclidean')","5872a33e":"v_measure_score(train_y, XAGG_tsne)","daf98145":"v_measure_score(train_y, XKM_tsne)","00186421":"v_measure_score(train_y, XBir_tsne)","359d6493":"plt.figure(figsize=(24,6));\n\nplt.subplot(1,3,1);\nplt.scatter(x_tsne, y_tsne, c=XAGG_tsne, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. tSNE ---> AgglomerativeClustering')\n\nplt.subplot(1,3,2)\nplt.scatter(x_tsne, y_tsne, c=XKM_tsne, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. tSNE ---> KMeans')\n\nplt.subplot(1,3,3)\nplt.scatter(x_tsne, y_tsne, c=XBir_tsne, \n            edgecolor='none', alpha=0.7, s=40,\n            cmap=plt.cm.get_cmap('nipy_spectral', 10))\nplt.colorbar()\nplt.title('MNIST. tSNE ---> Birch')","177b2491":"# function for solve\ndef clust_solve(model, datay, datas, name):\n    X_ = pd.DataFrame()\n    inf = pd.DataFrame()\n    tmp = {}\n    n = 1\n    for data in datas:\n        X_1 = model_agg.fit_predict(data)\n        X_[str(name)+str(n)] = X_1\n        #print('data ', n)\n        tmp['data'] = 'data' + str(n)\n        tmp['silhouette_score'] = silhouette_score(data, X_1, metric='euclidean', sample_size=10000)\n        tmp['v_measure_score'] = v_measure_score(datay, X_1)\n        inf = inf.append([tmp])\n        n += 1\n    return X_, inf","d9939e9f":"model_agg = AgglomerativeClustering(n_clusters=10)","93a6510e":"trains = [train_x, train_img64, train_img_2, train_img1, \n          train_minmax, train_stsc, train_rsc, train_norm]","57a51637":"tests = [test_x[:10000], test_img64[:10000], test_img_2[:10000], test_img1[:10000], \n          test_minmax[:10000], test_stsc[:10000], test_rsc[:10000], test_norm[:10000]]","2b5f2b7a":"X_AGG, inf_AGG = clust_solve(model_agg, train_y, trains, name='agg')\ninf_AGG","9f940782":"X_AGG.head(10)","8c7a54f2":"X_AGGt, inf_AGGt = clust_solve(model_agg, test_y[:10000], tests, name='agg')\ninf_AGGt","80d9a252":"X_AGGt.head(10)","8f76c875":"#x1 = pd.DataFrame(X_AGG)\n#x1.to_csv('X_AGG.csv', sep=',', encoding='utf-8', index=False)","7684a295":"model_Kmeans = KMeans(n_clusters = 10, n_jobs=-1)","a0d09cb9":"X_KM, inf_KM = clust_solve(model_Kmeans, train_y, trains, name='km')\ninf_KM","998d0863":"X_KM.head(10)","58370509":"X_KMt, inf_KMt = clust_solve(model_Kmeans, test_y[:10000], tests, name='km')\ninf_KMt","5d444384":"X_KMt.head(10)","68a4c240":"### KMeans","c96871df":"### Principal component analysis (PCA)","c08464d4":"### Import libs\n","d26be413":"**Restore data after PCA**","4b574eae":" Let's look at the data","8fea2aac":"### AgglomerativeClustering","84908bef":"Create some data sets for model learning.:  \n    1. train_img64 -  64 components\n    2. train_img_2 - 2 components\n    3. train_img1 -  90% info(87 components)\n    4. train_minmax, train_stsc, train_rsc, train_norm - 90% after preprocessing\n  ","0d65734d":"As you can see, in a set of 70,000 handwritten numbers. Each has 784 signs that correspond to the picture 28x28.","64bd5d09":"Try to use **MinMaxScaler, StandardScaler, RobustScaler, Normalizer** for **PCA ","22ab8d97":"In practice, as a rule, so many principal components are chosen to leave 90% of the variance of the original data. To do this, you can specify the value n_components = 0.9","d99b5f9a":"## Clusterization","b6ce9c48":"### t-SNE\n"}}