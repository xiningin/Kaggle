{"cell_type":{"567d6b97":"code","148c2dca":"code","ff443d82":"code","2f77040f":"code","3f0f6da3":"code","22605800":"code","fe829389":"code","191c2cd5":"code","6cac9e09":"code","e4518948":"code","e4effc14":"code","faba6d48":"code","10e82142":"code","05f1743e":"code","00e34a6c":"code","20eabc6d":"code","b7a29127":"code","962380c5":"code","024fb91b":"code","2ee5079f":"code","5ef9b485":"code","9cff8ecb":"code","fbbad796":"code","90f50546":"code","38b48b11":"code","0e856f99":"code","82481d8e":"code","d4551032":"code","1e845602":"code","40f5b696":"code","9c37ee49":"code","703119ff":"code","d9533e2f":"markdown","95417190":"markdown","c6626c08":"markdown","1f96d0e9":"markdown","702a8fcb":"markdown","df77fb41":"markdown"},"source":{"567d6b97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","148c2dca":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub_sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","ff443d82":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\n\nplt.rcParams.update({'font.size': 14})\n\n\n","2f77040f":"train.head()","3f0f6da3":"test.tail()","22605800":"train.shape","fe829389":"test.shape","191c2cd5":"# NA data\ntrain.isnull().sum()","6cac9e09":"test.isnull().sum()","e4518948":"# Check number of unique keywords, and whether they are the same for train and test sets\nprint (train.keyword.nunique(), test.keyword.nunique())\nprint (set(train.keyword.unique()) - set(test.keyword.unique()))","e4effc14":"sns.countplot(y=train.target);","faba6d48":"# Most common keywords\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.keyword, order = train.keyword.value_counts().iloc[:15].index)\nplt.title('Top 15 keywords')\nplt.show()","10e82142":"kw_d = train[train.target==1].keyword.value_counts().head(10)\nkw_nd = train[train.target==0].keyword.value_counts().head(10)\n\nplt.figure(figsize=(22,5))\nplt.subplot(121)\nsns.barplot(kw_d, kw_d.index, color='c')\nplt.title('Top keywords for disaster tweets')\nplt.subplot(122)\nsns.barplot(kw_nd, kw_nd.index, color='y')\nplt.title('Top keywords for non-disaster tweets')\nplt.show()","05f1743e":"top_d = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\ntop_nd = train.groupby('keyword').mean()['target'].sort_values().head(10)\n\nplt.figure(figsize=(22,5))\nplt.subplot(121)\nsns.barplot(top_d, top_d.index, color='pink')\nplt.title('Keywords with highest % of disaster tweets')\nplt.subplot(122)\nsns.barplot(top_nd, top_nd.index, color='yellow')\nplt.title('Keywords with lowest % of disaster tweets')\nplt.show()","00e34a6c":"# Most common locations\nplt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top 15 locations')\nplt.show()","20eabc6d":"# Cleaning the texts\nimport re\nimport nltk\n\nnltk.download('stopwords')\n","b7a29127":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 7613):\n    review = re.sub('[^a-zA-Z]', ' ', train['text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","962380c5":"corpus","024fb91b":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\nX = cv.fit_transform(corpus).toarray()\ny = train.iloc[:, 4].values","2ee5079f":"X.shape","5ef9b485":"y.shape","9cff8ecb":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)","fbbad796":"# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","90f50546":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)","38b48b11":"y_pred","0e856f99":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","82481d8e":"cm","d4551032":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 3263):\n    review = re.sub('[^a-zA-Z]', ' ', test['text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","1e845602":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1500)\ntest_data= cv.fit_transform(corpus).toarray()\n","40f5b696":"test_prediction=classifier.predict(test_data)","9c37ee49":"test_prediction","703119ff":"submit = sub_sample.copy()\nsubmit.target = test_prediction\nsubmit.to_csv('submission.csv', index=False)","d9533e2f":"**Now we will do Prediction for test data**","95417190":"**CLEANING OF TEXT**\n\n**Natural Language Processing(nltk)**: with Python provides a practical introduction to programming for language processing. Written by the creators of NLTK, it guides the reader through the fundamentals of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure, and more.\n\n**Stop Words**: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\nWe would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages.\n\n**Stemming**: is the process of producing morphological variants of a root\/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words \u201cchocolates\u201d, \u201cchocolatey\u201d, \u201cchoco\u201d to the root word, \u201cchocolate\u201d and \u201cretrieval\u201d, \u201cretrieved\u201d, \u201cretrieves\u201d reduce to the stem \u201cretrieve\u201d. So basically we have use** PorterStemmer** of nltk library.\n\n **Corpus**:The nltk.corpus package defines a collection of corpus reader classes, which can be used to access the contents of a diverse set of corpora.After removing the stopwords from text it will be store on the corpus.\n \n As now the cleaning data is text form we have to convert into numerical form for the prediction purpose\n \n**CountVectorizer**:Convert a collection of text documents to a matrix of token counts.\nwe can represent text data numerically: one-hot encoding (or count vectorization).We will be creating vectors that have a dimensionality equal to the size of our vocabulary, and if the text data features that vocab word, we will put a one in that dimension. Every time we encounter that word again, we will increase the count, leaving 0s everywhere we did not find the word even once.The result of this will be very large vectors.\n\nBy splitting the train data into raining and testing part the model is prepared and then the test data has been predicted using the Naive Bayes.","c6626c08":"**VISUALIZATION**","1f96d0e9":"**Cleaning of test data for prediction purpose in the same manner as train data**","702a8fcb":"About the competition:\n\nTwitter has become an important communication channel in times of emergency. The ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\nFiles:\n\ntrain.csv - the training set\n\ntest.csv - the test set\n\nsample_submission.csv - a sample submission file in the correct format\n\n\nColumns:\nid - a unique identifier for each tweet\n\ntext - the text of the tweet\n\nlocation - the location the tweet was sent from (may be blank)\n\nkeyword - a particular keyword from the tweet (may be blank)\n\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","df77fb41":"**IMPORTING ALL THE NECESSARY LIBRARIES**"}}