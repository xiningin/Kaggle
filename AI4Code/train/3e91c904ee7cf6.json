{"cell_type":{"2a4a9800":"code","06c81a3b":"code","2f14c76c":"code","86c83ba9":"code","2855a1af":"code","2018213f":"code","be8d7f10":"code","036cf8cc":"code","6c768f6c":"code","88f212a7":"code","05946987":"code","d03c6d67":"code","698ccd57":"code","50f52dd2":"code","7a945e72":"code","c4098405":"code","2f4761f8":"code","03c1a7a8":"code","ec244134":"code","b7c4f07b":"code","b1001fc6":"code","ce75e3fe":"code","02768d78":"code","10b0d053":"code","57a8c600":"code","31858adb":"code","5829f840":"code","8a46399b":"code","a71856a4":"code","dc8dfc75":"code","424ad711":"code","fd4c4983":"code","bb07008b":"code","a53185e6":"code","02fae06e":"code","d499c61b":"code","4ac911f4":"code","e5e244ca":"code","4a3e7e23":"markdown","c7e7b501":"markdown","a5e75931":"markdown","0b694e56":"markdown","191b7dc0":"markdown","19b7e02b":"markdown","918b26aa":"markdown","0af7e3ca":"markdown","a6035935":"markdown","0b1b364e":"markdown","2379a577":"markdown","de29e902":"markdown","cdaba3d6":"markdown"},"source":{"2a4a9800":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","06c81a3b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVC\nfrom scipy.stats import randint\nimport xgboost as xgb\nfrom sklearn import linear_model\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score\nfrom sklearn.metrics import r2_score, mean_absolute_error\n\nimport warnings \nwarnings.filterwarnings('ignore')","2f14c76c":"train=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","86c83ba9":"train.describe()","2855a1af":"train.info()","2018213f":"sns.set_theme(style='darkgrid')\nsns.scatterplot(x='LotArea',y='SalePrice',data=train)","be8d7f10":"sns.scatterplot(x='GrLivArea',y='SalePrice',data=train)","036cf8cc":"#Remove Outliers\ntrain[train.LotArea <150000]\ntrain[train.GrLivArea < 4000]","6c768f6c":"sns.countplot(x='MSZoning',data=train,palette='viridis')","88f212a7":"sns.boxplot(y='SalePrice',x='SaleCondition',data=train)","05946987":"sns.countplot(x='YrSold',data=train,palette='ocean_r')","d03c6d67":"sns.countplot(x='OverallQual',data=train,palette='inferno_r')","698ccd57":"fig = sns.barplot(x = 'GarageCars',y = 'SalePrice', data = train)\nfig.set_xticklabels(labels=['No car', '1 car', '2 cars', '3 cars', '4 cars'], rotation=90)\nplt.xlabel(\"Number of cars in Garage\");","50f52dd2":"sns.kdeplot(x='SalePrice',data=train)","7a945e72":"sns.kdeplot(x='GrLivArea',data=train)","c4098405":"sns.kdeplot(x='LotArea',data=train)","2f4761f8":"num_cols=train.select_dtypes(exclude=['object'])\ncat_cols = train.select_dtypes(include=['object'])\nnum_cols_t=test.select_dtypes(exclude=['object'])\ncat_cols_t=test.select_dtypes(include=['object'])\ncat_cols.isnull().sum()","03c1a7a8":"var=num_cols.corr()\n\nplt.figure(figsize=(30,15))\nsns.heatmap(var, center=0, annot=True)\nplt.title(\"Correlation among numerical Attributes\")\nplt.show()","ec244134":"# Dropping features\ndrop=['Id','MSSubClass','OverallCond','BsmtFinSF2','LowQualFinSF','BsmtHalfBath','KitchenAbvGr','EnclosedPorch','3SsnPorch',\n      'PoolArea','MiscVal','MoSold','YrSold','Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Street','Utilities','CentralAir',\n      'Condition2','MSZoning','LandSlope','Heating',\n       'HeatingQC','KitchenQual','GarageType','GarageFinish','SaleType','BsmtExposure','LandSlope']\ntrain.drop(columns=drop,axis=1,inplace=True)\ntest.drop(columns=drop,axis=1,inplace=True)","b7c4f07b":"train.isnull().sum()","b1001fc6":"train['LotFrontage'].fillna(train['LotFrontage'].mean(),inplace=True)\ntrain['GarageYrBlt'].fillna(train['GarageYrBlt'].median(),inplace=True)\ntrain['MasVnrArea'].fillna(train['MasVnrArea'].median(),inplace=True)\ntrain['BsmtQual'].fillna('TA',inplace=True)\ntrain['BsmtCond'].fillna('NA',inplace=True)\ntrain['BsmtFinType1'].fillna('Unf',inplace=True)\ntrain['BsmtFinType2'].fillna('NA',inplace=True)\ntrain['Electrical'].fillna('SBrkr',inplace=True)\ntrain['GarageQual'].fillna('NA',inplace=True)\ntrain['GarageCond'].fillna('NA',inplace=True)\ntrain['MasVnrType'].fillna('None',inplace=True)\ntrain.isnull().sum() #Great! Now we have zero null values in our training data","ce75e3fe":"# Log tranformation on sale price, gr liv area and lot area\ntrain['LotArea']=np.log(train['LotArea'])\ntrain['SalePrice']=np.log(train['SalePrice'])\ntrain['GrLivArea']=np.log(train['GrLivArea'])\nsns.kdeplot(x='SalePrice',data=train)\nplt.title('After Log Tranformation')","02768d78":"x=train.drop(columns='SalePrice',axis=1)\ny=train['SalePrice']","10b0d053":"from sklearn.preprocessing import LabelEncoder\nx=x.apply(LabelEncoder().fit_transform)","57a8c600":"x.head()","31858adb":"# splitting the dataset into train and test data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state = 42)\nx_train.shape,y_train.shape","5829f840":"from sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\n#Gradient Boosting\ngbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.1, max_depth=1, random_state=31).fit(x_train, y_train)\ncv = cross_val_score(gbr, x_train, y_train, cv = 10)\ny_pred = gbr.predict(x_test)\ncv = np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Erro: \" + str(mac))","8a46399b":"#Random Forest Model\nrf_model=RandomForestRegressor(bootstrap=False, max_depth=500, max_features='auto',\n                       min_samples_leaf=15,criterion='mse', n_jobs=-1, random_state=18).fit(x_train,y_train)\ny_pred=rf_model.predict(x_test)\ncv = cross_val_score(rf_model, x_train, y_train, cv = 10)\ncv=np.mean(cv)\nr2=r2_score(y_pred,y_test)\nme=mean_absolute_error(y_pred,y_test)\ncv=np.mean(cv)\nprint(\"Cross val score: \" + str(cv))\nprint('r2 score :' +str(r2))\nprint('Mean_absolute_error:'+str(me))","a71856a4":"# XGBoost Regressor\nxgb = xgb.XGBRegressor(learning_rate=0.01, n_estimators=1000, objective='reg:squarederror', random_state = 31).fit(x_train, y_train)\ncv = cross_val_score(xgb, x_train, y_train, cv = 10)\ny_pred = xgb.predict(x_test)\ncv = np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Error: \" + str(mac))","dc8dfc75":"#Ada Boost\nada=AdaBoostRegressor(n_estimators=1000,learning_rate=1,random_state=40).fit(x_train,y_train)\ncv=cross_val_score(ada,x_train,y_train,cv=10)\ny_pred=ada.predict(x_test)\ncv=np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Error: \" + str(mac))","424ad711":"#Linear Regression\nlr=LinearRegression(fit_intercept=True,n_jobs=-1).fit(x_train,y_train)\ncv=cross_val_score(lr,x_train,y_train,cv=10)\ny_pred=ada.predict(x_test)\ncv=np.mean(cv)\nr2=r2_score(y_pred,y_test)\nmac=mean_absolute_error(y_pred,y_test)\nprint(\"Cross val score: \" + str(cv))\nprint(\"R^2 score: \" + str(r2))\nprint(\"Mean Absolute Error: \" + str(mac))","fd4c4983":"test.head()","bb07008b":"#Let's look for null values in our test dataset\ntest.isnull().sum()","a53185e6":"test['LotFrontage'].fillna(test['LotFrontage'].mean(),inplace=True)\ntest['GarageYrBlt'].fillna(test['GarageYrBlt'].median(),inplace=True)\ntest['MasVnrArea'].fillna(test['MasVnrArea'].median(),inplace=True)\ntest['Exterior1st'].fillna('VinylSd',inplace=True)\ntest['Exterior2nd'].fillna('VinylSd',inplace=True)\ntest['BsmtQual'].fillna('TA',inplace=True)\ntest['BsmtCond'].fillna('TA',inplace=True)\ntest['BsmtFinType1'].fillna('GLQ',inplace=True)\ntest['BsmtFinType2'].fillna('Unf',inplace=True)\ntest['BsmtFinSF1'].fillna(test['BsmtFinSF1'].median(),inplace=True)\ntest['GarageQual'].fillna('TA',inplace=True)\ntest['GarageCond'].fillna('TA',inplace=True)\ntest['GarageCars'].fillna(test['GarageCars'].median(),inplace=True)\ntest['GarageArea'].fillna(test['GarageArea'].mean(),inplace=True)\ntest['Functional'].fillna('Typ',inplace=True)\ntest['BsmtFullBath'].fillna(test['BsmtFullBath'].median(),inplace=True)\ntest['TotalBsmtSF'].fillna(test['TotalBsmtSF'].mean(),inplace=True)\ntest['BsmtUnfSF'].fillna(test['BsmtUnfSF'].mean(),inplace=True)\ntest['MasVnrType'].fillna('None',inplace=True)\ntest.isnull().sum() # Great! Now we have zero null values in our test dataset","02fae06e":"# Log tranformation on Gr liv area and lot area\ntest['LotArea']=np.log(test['LotArea'])\ntest['GrLivArea']=np.log(test['GrLivArea'])\nsns.kdeplot(x='LotArea',data=test)","d499c61b":"#labelling test data\ntest = test.apply(LabelEncoder().fit_transform)","4ac911f4":"#making predictions using gradient boosting regresssor\nprediction=gbr.predict(test)","e5e244ca":"sample_submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nfinal_data = {'Id': sample_submission.Id, 'SalePrice': prediction}\nfinal_submission = pd.DataFrame(data=final_data)\nfinal_submission.to_csv('submission_file_.csv',index =False)","4a3e7e23":"The house with garage of 3 cars has the highest sale price.","c7e7b501":"**Exploratory Data Analysis**","a5e75931":"We will use gradient boosting technique as it gives the highest r^2 value","0b694e56":"Model Evaluation","191b7dc0":"We will remove columns containing more than 40% missing values.","19b7e02b":"**Test Data**","918b26aa":"Overall Quality has a very high impact on sale price and after looking at the correlation matrix we can drop features with very less correlation to sale price.","0af7e3ca":"We can see outliers in Gr liv area and lot area","a6035935":"Import necessary libraries","0b1b364e":"Sale Price, Gr liv area and lot area are skewed distribution. Hence, we need to scale it for better model accuracy. We will use log transformation.","2379a577":"Let's try to fix the null values","de29e902":"**Feature Engineering**","cdaba3d6":"80% of the houses are Residential low density houses and no commercial property is there. A,C and I have zero values in train and test data"}}