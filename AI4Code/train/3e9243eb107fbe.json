{"cell_type":{"fac0ec0d":"code","09e2ff2e":"code","a98f8649":"code","77b3f576":"code","3a4394fa":"code","29c095f3":"code","5145e512":"code","3ba8b1a4":"code","0bf918f5":"code","9719088f":"code","5424a712":"code","dd99262f":"code","5a02c352":"code","55b0dc60":"code","07105568":"code","d03bd7d4":"code","e3264fd1":"markdown","352090c6":"markdown","054b3bb4":"markdown","ac9fa4f4":"markdown","8e5ff5df":"markdown","d9d3ab02":"markdown","7236cc91":"markdown","8fba3a8f":"markdown","16e8fe6d":"markdown","d5ba74e3":"markdown"},"source":{"fac0ec0d":"import numpy as np\nimport pandas as pd\nimport os\nprint(os.listdir(\"..\/input\"))","09e2ff2e":"x_train_data = pd.read_csv('..\/input\/X_train.csv')\ny_train_data = pd.read_csv('..\/input\/y_train.csv')\nx_test_data = pd.read_csv('..\/input\/X_test.csv')","a98f8649":"x_train_data.head()","77b3f576":"x_train_data.drop(['series_id', 'measurement_number'], axis=1).describe()","3a4394fa":"def quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.degrees(math.atan2(t0, t1))\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.degrees(math.asin(t2))\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.degrees(math.atan2(t3, t4))\n\n    return X, Y, Z","29c095f3":"def feature_engineering(data):\n    eulers = np.array([quaternion_to_euler(*x) for x in data[['orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']].values])\n    data['euler_orientation_x'] = eulers[:, 0]\n    data['euler_orientation_y'] = eulers[:, 1]\n    data['euler_orientation_z'] = eulers[:, 2]\n\n    accCols = ['linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z']\n    gyroCols = ['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z']\n    quatCols = ['orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']\n    gyroEulerCols = ['euler_orientation_x', 'euler_orientation_y', 'euler_orientation_z']\n\n    data['angular_velocity_norm'] = np.sqrt(np.sum(np.square(data[gyroCols]),axis=1))\n    data['orientation_norm'] = np.sqrt(np.sum(np.square(data[quatCols]),axis=1))\n    data['linear_acceleration_norm'] = np.sqrt(np.sum(np.square(data[accCols]),axis=1))\n    data['euler_orientation_norm'] = np.sqrt(np.sum(np.square(data[gyroEulerCols]),axis=1))\n    data['acc_vs_vel'] = data['linear_acceleration_norm'] \/ data['angular_velocity_norm']\n    \n    return data\n\nx_train_data = feature_engineering(x_train_data)\nx_test_data = feature_engineering(x_test_data)","5145e512":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\noriginal_series_id = x_train_data['series_id']\ndf_x_train = pd.DataFrame(sc.fit_transform(x_train_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1)),\n                       columns=x_train_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1).columns)\ndf_x_train['series_id'] = original_series_id\ndf_x_train.describe()","3ba8b1a4":"original_series_id = x_test_data['series_id']\ndf_x_test = pd.DataFrame(sc.transform(x_test_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1)),\n                       columns=x_test_data.drop(['row_id', 'series_id', 'measurement_number'], axis=1).columns)\ndf_x_test['series_id'] = original_series_id","0bf918f5":"series_id = x_train_data.series_id.unique()\nset([len(df_x_train[df_x_train['series_id'] == row_id]) for row_id in series_id])","9719088f":"def get_sliding_window(x, y, seq_length, shift):\n    data_grouped_by_classes = {key: [] for key in set(y['surface'].values)}\n    for class_name in set(y['surface'].values):\n        for y_row in y.iterrows():\n            y_row = y_row[1]\n            if y_row['surface'] == class_name:\n                data_grouped_by_classes[class_name].append([x[x['series_id'] == y_row['series_id']], y_row['surface']])       \n    \n    x_ret = []\n    y_ret = []\n    for class_name in data_grouped_by_classes.keys():\n        for arr in data_grouped_by_classes[class_name]:\n            arr_y = arr[1]\n            arr_x = arr[0].values\n            batch_size_total = seq_length\n            n_batches = len(arr_x)\/\/batch_size_total\n            arr_x = arr_x[:n_batches * batch_size_total]\n            for n in range(0, arr_x.shape[0], shift):  \n                if n + seq_length > arr_x.shape[0]:\n                    break\n                x_batch = arr_x[n:n+seq_length]\n                y_batch = arr_y\n                x_ret.append(x_batch)\n                y_ret.append(y_batch)\n    return x_ret, y_ret\n\ndef get_data(x, y, batch_size, seq_length, shift):\n    import random\n    data_x = []\n    data_y = []\n    for x_, y_ in zip(*get_sliding_window(x, y, seq_length, shift)):\n        data_x.append(x_)\n        data_y.append(y_)\n    \n    c = list(zip(data_x, data_y))\n    random.shuffle(c)\n    data_x, data_y = zip(*c)\n    ret_x = []\n    ret_y = []\n    \n    for i in range(0, len(data_x), batch_size):\n        if len(data_x) <= i+batch_size:\n            break\n        ret_x.append(np.array(data_x[i:i+batch_size])[0])\n        ret_y.append(np.array(data_y[i:i+batch_size])[0])\n    return np.array(ret_x), np.array(ret_y)","5424a712":"x_train, y_train = get_data(df_x_train, y_train_data, 1, 16, 2)\nx_train = x_train[:, :, :-1]\nx_train = x_train.reshape(-1, 4, 4, 18)","dd99262f":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nenc_l = LabelEncoder()\nenc = OneHotEncoder(handle_unknown='ignore')\nnr_y_train = enc_l.fit_transform(np.array(y_train))\none_hot_y_train = enc.fit_transform(nr_y_train.reshape(-1, 1))","5a02c352":"import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, LSTM, GRU, SimpleRNN, Conv1D, TimeDistributed, MaxPooling1D, Flatten, Dropout\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import EarlyStopping","55b0dc60":"def init_model():\n    model = Sequential()\n    model.add(TimeDistributed(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same'), batch_input_shape=(None, None, 4, 18)))\n    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n    model.add(TimeDistributed(Flatten()))\n    model.add(LSTM(16))\n    model.add(Dense(9, activation='softmax'))\n    \n    adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n\n    model.compile(optimizer=adam,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","07105568":"model = init_model()\ncallbacks = [EarlyStopping('val_loss', patience=3)]\nmodel.fit(x_train, one_hot_y_train, epochs=10, batch_size=128, validation_split=0.2, callbacks=callbacks)","d03bd7d4":"series_id = df_x_test.series_id.unique()\nour_predictions = []\nfor row_id in series_id:\n    batch_data = df_x_test[df_x_test['series_id'] == row_id].drop(['series_id'], axis=1).values[:128, :].reshape(8, 16, 18).reshape(8, 4, 4, 18)\n    predictions = model.predict(batch_data)\n    final_predictions = list(sum(predictions)\/len(predictions))\n    our_predictions.append(final_predictions)\n\nlabel_rez = enc_l.inverse_transform(enc.inverse_transform(our_predictions))\n\ndf = pd.DataFrame({'series_id': series_id.tolist(),'surface': label_rez.tolist()})\ndf.to_csv('submission.csv', index=False)","e3264fd1":"As we see not all data columns have zero mean and variance of 1. To help the training process we going to normalize the data with standard scaller.","352090c6":"We need to use the same scaller parameters for the test data.","054b3bb4":"We need to get as much data as we can, so we add sliding window.","ac9fa4f4":"Lets add some features :)","8e5ff5df":"Time for training","d9d3ab02":"So this will be our network:\n<img src=\"https:\/\/imgur.com\/C5YptiK.png\">","7236cc91":"For this kernel I descided not to repeat what others did, so there are no visualizations of the data, and class analysis, you can go to other notebooks for that. I descided that I will go straight to the point :)","8fba3a8f":"### Data preparation","16e8fe6d":"### Deep Learning Model","d5ba74e3":"### Loading the data"}}