{"cell_type":{"7af43670":"code","2c04163c":"code","0857a1d9":"code","4596c95c":"code","cac44090":"code","ac0ba918":"code","1c9dc331":"code","d41a7d8c":"code","2efd8438":"code","024e9ea5":"code","ffb96d6f":"code","95a03858":"code","031dbcaa":"code","d30d1589":"code","bf329851":"code","838df418":"code","3e56e40d":"code","ec0822f3":"code","558584a7":"code","3c47e98b":"code","46ea5ea3":"code","97b69636":"code","fea01a68":"code","60334d08":"code","2bee8512":"code","32f5b38d":"code","c674f684":"code","6ddadd92":"code","bbeceb0d":"code","7e6cf93a":"code","d147e601":"code","6d081e21":"code","c589e46f":"code","cdcbccf4":"code","e1c1b2e8":"code","629b199b":"code","1e5e3dd6":"code","17abc2fc":"code","a587f9ce":"code","f19dcc05":"code","5ec89ee8":"code","80bfe706":"code","a6de83d1":"code","2beaf7bb":"code","5b7aab4f":"code","b74796be":"code","648a71dd":"code","3e93a933":"code","017e608c":"code","1ad1558f":"code","411c100e":"code","2a473c15":"code","3c24c322":"code","d7c4cce5":"code","10659f5b":"code","65a98fcf":"code","363e02ac":"code","bba0b86d":"code","15942a0c":"code","b4fcc82c":"code","b7dec50c":"code","4012af0d":"code","a1624e14":"code","6aa60fbc":"code","e275146a":"code","6e9529da":"code","4a80bcd6":"code","aab363d9":"code","c23b8ae7":"code","ce359ed8":"code","c9287ef6":"code","6979ff28":"code","1545124f":"code","8de1462d":"code","6076791f":"code","9981c6c5":"code","3aac3998":"code","89384490":"code","a566ff0a":"code","a61c4ef0":"code","d14f3add":"code","2c2924eb":"code","8ff6de6a":"code","aafc7169":"code","e7a145e3":"code","f5bcd5f3":"code","9334be41":"code","613801a0":"code","f9f76b67":"code","df37c2ce":"code","a12c55ec":"code","8c2e50c3":"code","73bbc165":"code","d55a9ac3":"code","a361742b":"code","0b495645":"code","9016198d":"code","b702854b":"code","7b7d441e":"code","43e42449":"code","5fa4025c":"code","b237a148":"code","cefcef19":"code","6743ae37":"code","45a60826":"code","b5b3888d":"code","2a7f21fe":"code","d0a3e400":"code","78b9b1cc":"code","82510573":"code","c15a5f83":"code","c1af9963":"code","d94b6688":"code","1f229969":"code","b1a166eb":"code","5c831a0f":"code","7646be0a":"code","9a6c5834":"code","a3800535":"code","2d7e9230":"code","f83fdb70":"code","f59393af":"code","b2179481":"code","4fab2f76":"code","123fac92":"code","dcb49ea5":"code","16561ce3":"code","80b88e16":"code","ae265921":"code","a7499b3b":"code","a477dfd9":"code","9ca6b486":"code","16185d8a":"code","1b34d9de":"code","46fc43b4":"code","2d57c9c1":"code","8c321f7e":"code","246e8dac":"code","c910f54e":"code","122eb4fa":"markdown","ee56a187":"markdown","3e44a5fb":"markdown","6603dc63":"markdown","81e3d298":"markdown","7104125e":"markdown","18caa8ff":"markdown","4f8d4e6e":"markdown","98aa80c2":"markdown","133cb693":"markdown","49369434":"markdown","c4572117":"markdown","6cedb853":"markdown","69eef16d":"markdown","4e5b2e71":"markdown","80385bc7":"markdown","bd3575e0":"markdown","76e432a4":"markdown","e1419617":"markdown","f806ab2c":"markdown","9033a7cf":"markdown","5d10e76f":"markdown","d88822ad":"markdown","67edeefd":"markdown","13f20851":"markdown"},"source":{"7af43670":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2c04163c":"#Importing the necessary Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","0857a1d9":"# Importing the dataset\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n# For easy identification of rows from the train and test, we create a feature called label and set 0 for rows from train and 1 for rows from test\ntrain['Label']='0'\ntest['Label']='1'","4596c95c":"# Concatenating train and test data.\ndf=pd.concat([train,test])","cac44090":"df.shape","ac0ba918":"# As we have the label feature, we can reset the index values.\ndf.index=np.arange(0,1309)","1c9dc331":"# Checking for null values\ndf.isna().sum()","d41a7d8c":"# Cabin feature has the most null values (77%)\ndf['Cabin'].isna().sum()*100\/len(df['Cabin'])","2efd8438":"# Converting the feature to string type as the nan values also get converted to string values.\ndf['Cabin']=df['Cabin'].astype(str)","024e9ea5":"df['Cabin'].value_counts()","ffb96d6f":"# Extracting the first letter of each value as it as it provides the different cabin classes in which the passengers have travelled.\nlist_cabin=[]\nfor i in df['Cabin']:\n    if i!='nan':\n        i=i[0]\n        list_cabin.append(i)\n    else:\n        list_cabin.append(i)\nlist_cabin","95a03858":"df['Cabin']=list_cabin","031dbcaa":"# Converting nan to a seperate class called NA\ndf['Cabin']=df['Cabin'].apply(lambda x:'NA' if x=='nan' else x)","d30d1589":"df['Cabin'].value_counts()","bf329851":"df['With_without_cabin']=df['Cabin'].apply(lambda x: 0 if x=='NA' else 1)","838df418":"sns.countplot(df['With_without_cabin'],hue=df['Survived'])","3e56e40d":"# Imputing the missing values with the most recurring value ('S')\ndf['Embarked']=df['Embarked'].apply(lambda x: x if x=='S' or x=='C' or x=='Q' else 'S')","ec0822f3":"# To impute the missing values of age, we consider the most corelated features with age.\ndf.corr()['Age']","558584a7":"# Creating a feature called married from the name feature\nname=list(df['Name'].values)","3c47e98b":"title=[]\nfor i in name:\n    i=i.split(' ')[1]\n    title.append(i)","46ea5ea3":"\nmarried=[]\nfor i in title:\n    if 'Mr.' in i or 'Mrs.' in i:\n        married.append(1)\n    else:\n        married.append(0)\n        ","97b69636":"df['Married']=married","fea01a68":"df=df.drop(['Name','Cabin','Ticket'],1)","60334d08":"df.head()","2bee8512":"# Married and Pclass have the highest corelation with Age and hence we use the information from those features to impute\n# the missing values.\nabs(df.corr()['Age'])","32f5b38d":"age=df[['Age','Married','Pclass']]","c674f684":"# We have different combinations of information in the married and pclass features and we take all the combinations \n# and impute the missing values with those median values.\nm0_cl1=age[(age['Married']==0) & (age['Pclass']==1)]\nm0_cl2=age[(age['Married']==0) & (age['Pclass']==2)]\nm0_cl3=age[(age['Married']==0) & (age['Pclass']==3)]\nm1_cl1=age[(age['Married']==1) & (age['Pclass']==1)]\nm1_cl2=age[(age['Married']==1) & (age['Pclass']==2)]\nm1_cl3=age[(age['Married']==1) & (age['Pclass']==3)]","6ddadd92":"m0_cl1.fillna(m0_cl1['Age'].median(),inplace=True)\nm0_cl2.fillna(m0_cl2['Age'].median(),inplace=True)\nm0_cl3.fillna(m0_cl3['Age'].median(),inplace=True)\nm1_cl1.fillna(m1_cl1['Age'].median(),inplace=True)\nm1_cl2.fillna(m1_cl2['Age'].median(),inplace=True)\nm1_cl3.fillna(m1_cl3['Age'].median(),inplace=True)","bbeceb0d":"age_df=pd.concat([m0_cl1,m0_cl2,m0_cl3,m1_cl1,m1_cl2,m1_cl3],0)","7e6cf93a":"age_df['index']=age_df.index","d147e601":"age_df.sort_values('index',inplace=True)","6d081e21":"df['Age']=age_df['Age']","c589e46f":"# As we have only 1 missing value, we can impute it with the mean value.\ndf['Fare'].fillna(df['Fare'].mean(),inplace=True)","cdcbccf4":"# Only Survived feature has missing values and the 418 values are from the test data which we have to predict.\ndf.isna().sum()","e1c1b2e8":"# We can combine the parch and SibSp to calculate the number of family members they had on board.\ndf['Dependents']=df['Parch']+df['SibSp']","629b199b":"# Mapping the different values obtained to categorize it.\ndf['Dependents']=df['Dependents'].map({0:'No',1:'Few',2:'Few',3:'Few',4:'Few',5:'Few',6:'Many',7:'Many',10:'Many'})","1e5e3dd6":"# Dropping the Parch and SibSp features as the information has been obtained from it.\ndf=df.drop(['Parch','SibSp'],1)","17abc2fc":"df.head()","a587f9ce":"# As the location in which a passenger boarded does not affect whether they survived or not, we can drop it.\ndf=df.drop('Embarked',1)","f19dcc05":"# As the passengerID is also a feature not required for the model, we can drop it \ndf=df.drop('PassengerId',1)","5ec89ee8":"# Convering class to string type in order to create dummies for this feature too.\ndf['Pclass']=df['Pclass'].astype(str)","80bfe706":"df=pd.get_dummies(df,drop_first=True)","a6de83d1":"# Splitting indepenndent and dependent features\nX=df.drop('Survived',1)\ny=df['Survived']","2beaf7bb":"# Scaling the features to bring them all to one scale.\nfrom sklearn.preprocessing import StandardScaler\nX=pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)","5b7aab4f":"X.head()","b74796be":"#Dropping the Label feature as we can split the data into train and test based on the index values. \nX=X.drop('Label_1',1)","648a71dd":"X_train=X.iloc[:891]\nX_test=X.iloc[891:]\ny_train=df['Survived'].iloc[:891]\ny_test=df['Survived'].iloc[891:]","3e93a933":"# Splitting the train data into train and validation to check the performance of different models.\nfrom sklearn.model_selection import train_test_split\ntrain_X,X_val,train_y,y_val=train_test_split(X_train,y_train,test_size=0.25,random_state=42)","017e608c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,f1_score","1ad1558f":"# We tune the value of C as it gives penalty to different features.\nfor i in [0.0001,0.001,0.1,1,10,100,1000]:\n    lr=LogisticRegression(C=i).fit(train_X,train_y)\n    y_pred_lr=lr.predict(X_val)\n    print('For C value',i,'f1 score is: ',f1_score(y_val,y_pred_lr))","411c100e":"lr=LogisticRegression(C=10).fit(train_X,train_y)\ny_pred_lr=lr.predict(X_val)\nprint('For C value 10 f1 score is: ',f1_score(y_val,y_pred_lr))","2a473c15":"from sklearn.preprocessing import binarize","3c24c322":"from sklearn.metrics import accuracy_score","d7c4cce5":"# Calcutating the threshold value of where to convert the probable values as 0 and 1.\nfor i in range(1,11):\n    y_pred2=lr.predict_proba(X_val)\n    bina=binarize(y_pred2,threshold=i\/10)[:,1]\n    cm2=confusion_matrix(y_val,bina)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    print('f1 score: ',f1_score(y_val,bina))\n    print('accuracy score: ',accuracy_score(y_val,bina))\n    print('\\n')","10659f5b":"#0.4 is where we have the least misclassified values.\ny_pred2=lr.predict_proba(X_val)\nbina=binarize(y_pred2,threshold=0.4)[:,1]\nprint(confusion_matrix(y_val,bina))\nprint('f1_score: ',f1_score(y_val,bina))\nprint('accuracy_score: ',accuracy_score(y_val,bina))","65a98fcf":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier().fit(train_X,train_y)\ny_pred_dt=dt.predict(X_val)\nf1_score(y_val,y_pred_dt)","363e02ac":"# Hyper Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\ndt=DecisionTreeClassifier()\nparam_grid = {\n\n    'criterion': ['gini','entropy'],\n    'max_depth': [10,15,20,25],\n    'min_samples_split' : [5,10,15,20],\n    'min_samples_leaf': [2,5,7],\n    'random_state': [42,135,777],\n}\n\nrf_grid=GridSearchCV(estimator=dt,param_grid=param_grid,n_jobs=-1,return_train_score=True)\n\nrf_grid.fit(train_X,train_y)\n","bba0b86d":"rf_grid.best_params_","15942a0c":"cv_res_df=pd.DataFrame(rf_grid.cv_results_)","b4fcc82c":"cv_res_df.head()","b7dec50c":"# We take the point where the test score is high and also where the difference between the train and test score is minimal\nplt.figure(figsize=(20,5))\nplt.plot(cv_res_df['mean_train_score'])\nplt.plot(cv_res_df['mean_test_score'])\nplt.xticks(np.arange(0,250,5),rotation=90)\nplt.show()","4012af0d":"cv_res_df[['mean_train_score','mean_test_score']].iloc[240:246]","a1624e14":"pd.DataFrame(cv_res_df.iloc[240]).T","6aa60fbc":"# Creating a decision tree model with the optimal hyperparameters\ndt=DecisionTreeClassifier(max_depth=20,min_samples_leaf=7,min_samples_split=5,criterion='entropy',random_state=42).fit(train_X,train_y)","e275146a":"y_pred_dtc=dt.predict(X_val)","6e9529da":"accuracy_score(y_val,y_pred_dtc)","4a80bcd6":"f1_score(y_val,y_pred_dtc)","aab363d9":"confusion_matrix(y_val,y_pred_dtc)","c23b8ae7":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier().fit(train_X,train_y)\ny_pred_rf=rf.predict(X_val)\nprint(accuracy_score(y_val,y_pred_rf),'\\t',f1_score(y_val,y_pred_rf))","ce359ed8":"# Hyper parameter tuning\nfrom sklearn.model_selection import GridSearchCV\nrf=RandomForestClassifier()\nparam_grid = {\n    \n    'n_estimators':[10,20,30],\n    'criterion': ['gini','entropy'],\n    'max_depth': [10,15,20,25],\n    'min_samples_split' : [5,10,15],\n    'min_samples_leaf': [2,5,7],\n    'random_state': [42,135,777],\n    'class_weight': ['balanced' ,'balanced_subsample']\n}\n\nrf_grid=GridSearchCV(estimator=rf,param_grid=param_grid,n_jobs=-1,return_train_score=True)\n\nrf_grid.fit(train_X,train_y)","c9287ef6":"cv_res_df=pd.DataFrame(rf_grid.cv_results_)","6979ff28":"plt.figure(figsize=(20,5))\nplt.plot(cv_res_df['mean_train_score'])\nplt.plot(cv_res_df['mean_test_score'])\nplt.xticks(np.arange(0,1200,50),rotation=90)\nplt.show()","1545124f":"pd.DataFrame(cv_res_df.iloc[330])","8de1462d":"rfc=RandomForestClassifier(class_weight='balanced',criterion='gini',max_depth=10,min_samples_leaf=2,min_samples_split=5,n_estimators=30,random_state=42).fit(train_X,train_y)","6076791f":"y_pred_rfc=rfc.predict(X_val)\nprint(accuracy_score(y_val,y_pred_rfc),'\\t',f1_score(y_val,y_pred_rfc))","9981c6c5":"confusion_matrix(y_val,y_pred_rfc)","3aac3998":"#Converting the dataset into matrix.\nimport xgboost as xgb\ndtrain=xgb.DMatrix(train_X,train_y)\ndval=xgb.DMatrix(X_val,y_val)","89384490":"param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\nnum_round = 2\nbst = xgb.train(param, dtrain, num_round)\n# make prediction\npreds = bst.predict(dval)","a566ff0a":"for i in range(1,11):\n    bina=binarize(preds.reshape(-1,1),threshold=i\/10)\n    cm2=confusion_matrix(y_val,bina)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    print('f1 score: ',f1_score(y_val,bina))\n    print('accuracy score: ',accuracy_score(y_val,bina))\n    print('\\n')","a61c4ef0":"from sklearn.ensemble import GradientBoostingClassifier\ngr_boost=GradientBoostingClassifier().fit(train_X,train_y)\ny_pred_gr=gr_boost.predict(X_val)\nprint(accuracy_score(y_val,y_pred_gr),'\\t',f1_score(y_val,y_pred_gr))","d14f3add":"GBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4,6,8,10],\n              'min_samples_leaf': [20,50,100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=5, scoring=\"accuracy\", n_jobs= -1, verbose = 1)\n\ngsGBC.fit(train_X,train_y)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","2c2924eb":"gsGBC.best_params_","8ff6de6a":"gr_boost1=GradientBoostingClassifier(**gsGBC.best_params_).fit(train_X,train_y)\ny_pred_gr1=gr_boost1.predict(X_val)\nprint(f1_score(y_val,y_pred_gr1),accuracy_score(y_val,y_pred_gr1))","aafc7169":"from sklearn.ensemble import AdaBoostClassifier\nada_boost=AdaBoostClassifier().fit(train_X,train_y)\ny_pred_ada=ada_boost.predict(X_val)\nprint(accuracy_score(y_val,y_pred_ada),'\\t',f1_score(y_val,y_pred_ada))","e7a145e3":"y_pred_test_lr=lr.predict(X_test)\ny_pred_test2=lr.predict_proba(X_test)","f5bcd5f3":"y_pred_test_lr=binarize(y_pred_test2,threshold=0.4)[:,1]","9334be41":"log_pred=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nlog_pred['Survived']=y_pred_test_lr\nlog_pred.to_csv('Logistic pred.csv',index=False)","613801a0":"y_pred_test_dt=dt.predict(X_test)\ndt_pred=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\ndt_pred['Survived']=y_pred_test_dt\ndt_pred.to_csv('Decision_tree_pred.csv',index=False)","f9f76b67":"y_pred_test_rfc=rfc.predict(X_test)\nrfc_pred=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nrfc_pred['Survived']=y_pred_test_rfc\nrfc_pred.to_csv('Random_forest_pred.csv',index=False)","df37c2ce":"dtest=xgb.DMatrix(X_test)","a12c55ec":"pred_xgb=bst.predict(dtest)","8c2e50c3":"y_pred_xgb=binarize(pred_xgb.reshape(-1,1),threshold=0.4)","73bbc165":"pred_test_xgb=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\npred_test_xgb['Survived']=y_pred_xgb","d55a9ac3":"pred_test_xgb.to_csv('XGBoost_pred.csv',index=False)","a361742b":"y_pred_test_gr=gr_boost.predict(X_test)\npred_gr=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\npred_gr['Survived']=y_pred_test_gr\npred_gr.to_csv('GradientBoost_pred.csv',index=False)","0b495645":"y_pred_test_ada=ada_boost.predict(X_test)\npred_ada=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\npred_ada['Survived']=y_pred_test_ada\npred_ada.to_csv('AdaBoost_pred.csv',index=False)","9016198d":"from sklearn.decomposition import PCA","b702854b":"pca=PCA().fit(X)","7b7d441e":"pca.explained_variance_ratio_","43e42449":"plt.figure(figsize=(12,6))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.grid()","5fa4025c":"# From the graph, we see that 95% of the variance is explained by 6 principle components and hence we can take 6 Principle\n# components.\nX1=PCA(n_components=6).fit_transform(X)\nX1","b237a148":"# Splitting train and test independent features.\nX_test_pca,X_train_pca=X1[891:],X1[:891]","cefcef19":"# We repeat the same steps for the converted data.\ntrai_x,val_x,trai_y,val_y=train_test_split(X_train_pca,y_train,test_size=0.3,random_state=42)","6743ae37":"for i in [0.00001,0.0001,0.001,0.1,1,10,100,1000]:\n    lr=LogisticRegression(C=i).fit(trai_x,trai_y)\n    y_pred_pca_lr=lr.predict(val_x)\n    print('For C value',i,'f1 score is: ',f1_score(val_y,y_pred_pca_lr))","45a60826":"lr=LogisticRegression(C=1).fit(trai_x,trai_y)\ny_pred_pca_lr=lr.predict(val_x)\nprint(f1_score(val_y,y_pred_pca_lr),'\\t',accuracy_score(val_y,y_pred_pca_lr))","b5b3888d":"for i in range(1,11):\n    y_pred2=lr.predict_proba(val_x)\n    bina=binarize(y_pred2,threshold=i\/10)[:,1]\n    cm2=confusion_matrix(val_y,bina)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    print('f1 score: ',f1_score(val_y,bina))\n    print('accuracy score: ',accuracy_score(val_y,bina))\n    print('\\n')","2a7f21fe":"y_pred2=lr.predict_proba(X_test_pca)\ny_pred_pca_lr=binarize(y_pred2,threshold=0.4)[:,1]","d0a3e400":"sub_lr=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nsub_lr['Survived']=y_pred_pca_lr\nsub_lr.to_csv('Logistic_Regression_PCA.csv',index=False)\n","78b9b1cc":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier().fit(trai_x,trai_y)\ny_pred_dt=dt.predict(val_x)\nf1_score(val_y,y_pred_dt)","82510573":"\nfrom sklearn.model_selection import GridSearchCV\ndt=DecisionTreeClassifier()\nparam_grid = {\n\n    'criterion': ['gini','entropy'],\n    'max_depth': [5,10,15,20,25],\n    'min_samples_split' : [5,10,15,20],\n    'min_samples_leaf': [2,5,7,10],\n    'random_state': [42,135,777],\n}\n\nrf_grid=GridSearchCV(estimator=dt,param_grid=param_grid,n_jobs=-1,return_train_score=True)\n\nrf_grid.fit(trai_x,trai_y)","c15a5f83":"rf_grid.best_params_","c1af9963":"cv_res_df=pd.DataFrame(rf_grid.cv_results_)","d94b6688":"plt.figure(figsize=(12,6))\ncv_res_df['mean_train_score'].plot()\ncv_res_df['mean_test_score'].plot()\nplt.xticks(np.arange(0,500,20),rotation=90)\nplt.show()","1f229969":"cv_res_df.iloc[267]","b1a166eb":"dt=DecisionTreeClassifier(max_depth=5,min_samples_leaf=7,min_samples_split=10,criterion='entropy',random_state=42).fit(trai_x,trai_y)","5c831a0f":"y_pred_val_pca_dt=dt.predict(val_x)\nprint(f1_score(val_y,y_pred_val_pca_dt),accuracy_score(val_y,y_pred_val_pca_dt))","7646be0a":"y_pred_pca_dt=dt.predict(X_test_pca)","9a6c5834":"sub_dt=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nsub_dt['Survived']=y_pred_pca_dt\nsub_dt.to_csv('Decision_Tree_PCA.csv',index=False)","a3800535":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier().fit(trai_x,trai_y)\ny_pred_rf=rf.predict(val_x)\nprint(accuracy_score(val_y,y_pred_rf),'\\t',f1_score(val_y,y_pred_rf))","2d7e9230":"from sklearn.model_selection import GridSearchCV\nrf=RandomForestClassifier()\nparam_grid = {\n    \n    'n_estimators':[10,20,30],\n    'criterion': ['gini','entropy'],\n    'max_depth': [10,15,20,25],\n    'min_samples_split' : [5,10,15],\n    'min_samples_leaf': [2,5,7],\n    'random_state': [42,135,777],\n    'class_weight': ['balanced' ,'balanced_subsample']\n}\n\nrf_grid=GridSearchCV(estimator=rf,param_grid=param_grid,n_jobs=-1,return_train_score=True)\n\nrf_grid.fit(trai_x,trai_y)","f83fdb70":"rf_grid.best_params_","f59393af":"cv_res_df=pd.DataFrame(rf_grid.cv_results_)\ncv_res_df","b2179481":"plt.figure(figsize=(12,6))\ncv_res_df['mean_train_score'].plot()\ncv_res_df['mean_test_score'].plot()","4fab2f76":"cv_res_df['diff']=cv_res_df['mean_train_score']-cv_res_df['mean_test_score']","123fac92":"cv_res_df[['mean_train_score','mean_test_score']][(cv_res_df['mean_test_score']>0.80) & (cv_res_df['mean_train_score']<0.90)]","dcb49ea5":"cv_res_df.iloc[11]","16561ce3":"rf=RandomForestClassifier(n_estimators=10,max_depth=10,min_samples_leaf=2,min_samples_split=10,random_state=777,criterion='gini').fit(trai_x,trai_y)\ny_pred_rf=rf.predict(val_x)\nprint(accuracy_score(val_y,y_pred_rf),'\\t',f1_score(val_y,y_pred_rf))","80b88e16":"y_pred_pca_rf=rf.predict(X_test_pca)","ae265921":"sub_rf=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nsub_rf['Survived']=y_pred_pca_rf\nsub_rf.to_csv('Random_Forest_PCA.csv',index=False)","a7499b3b":"dtrain_pca=xgb.DMatrix(trai_x,trai_y)\ndval_pca=xgb.DMatrix(val_x)\nparam = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\nnum_round = 2\nbst = xgb.train(param, dtrain_pca, num_round)\n# make prediction\npreds = bst.predict(dval_pca)","a477dfd9":"for i in range(1,11):\n    bina=binarize(preds.reshape(-1,1),threshold=i\/10)\n    cm2=confusion_matrix(val_y,bina)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')\n    print('f1 score: ',f1_score(val_y,bina))\n    print('accuracy score: ',accuracy_score(val_y,bina))\n    print('\\n')","9ca6b486":"dtest_pca=xgb.DMatrix(X_test_pca)\npreds=bst.predict(dtest_pca)\nbina=binarize(preds.reshape(-1,1),threshold=0.5)","16185d8a":"sub_xgb=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nsub_xgb['Survived']=bina\nsub_xgb.to_csv('XGBoost_PCA.csv',index=False)","1b34d9de":"from sklearn.ensemble import GradientBoostingClassifier\ngr_boost=GradientBoostingClassifier().fit(trai_x,trai_y)\ny_pred_gr=gr_boost.predict(val_x)\nprint(accuracy_score(val_y,y_pred_gr),'\\t',f1_score(val_y,y_pred_gr))","46fc43b4":"y_pred_pca_gr=gr_boost.predict(X_test_pca)","2d57c9c1":"sub_gr=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nsub_gr['Survived']=y_pred_pca_gr\nsub_gr.set_index('PassengerId')\nsub_gr.to_csv('GradientBoost_PCA.csv',index=False)","8c321f7e":"ada_boost=AdaBoostClassifier().fit(trai_x,trai_y)\ny_pred_val=ada_boost.predict(val_x)\nprint(accuracy_score(val_y,y_pred_val),'\\t',f1_score(val_y,y_pred_val))","246e8dac":"y_pred_pca_ab=ada_boost.predict(X_test_pca)","c910f54e":"sub_ab=pd.DataFrame(np.arange(892,1310),columns=['PassengerId'])\nsub_ab['Survived']=y_pred_pca_ab\nsub_ab.to_csv('Adaboost_PCA.csv',index=False)","122eb4fa":"### PREDICTING THE SURVIVAL OF PASSENGERS IN THE TITANIC\nThe famous dataset to start learning the aspects of Exploratory Data Analysis and Machine Learning Models.\n\nFeatures of the dataset\n- Survival: Whether the passenger has survived or not\n- pclass: Class in which a passenger travelled.\n- sex: \tGender of the passenger\t\n- Age: \tAge of the passengers in years\t\n- sibsp: number of siblings \/ spouses in the Titanic of a passenger\n- parch: number of parents \/ children in the Titanic of a passenger\n- ticket: Ticket number\t\n- fare: ticket fare\t\n- cabin: Cabin in which passengers travelled\t\n- embarked: \tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton","ee56a187":"### Upon submission, I got the highest score for the Gradient Boosting Model without PCA. For different data pre processing techniques, we get different results. My best result was from the above mentioned model.","3e44a5fb":"### Decision Tree","6603dc63":"## Random Forest","81e3d298":"#### Logistic Regression","7104125e":"### PCA\n- We try to perform PCA on the data to check if it improves the performance.","18caa8ff":"### XGBoost","4f8d4e6e":"### AdaBoost","98aa80c2":"#### Ada Boost","133cb693":"#### Decision Tree","49369434":"### Gradient Boost","c4572117":"### Ada Boost","6cedb853":"### Logistic Regression","69eef16d":"#### Gradient Boost","4e5b2e71":"- Out of the 1300 passengers, around 700 have travelled without a cabin and only 200 out of them have survived.","80385bc7":"### Decision Tree","bd3575e0":"### XG Boost","76e432a4":"### Train and validation","e1419617":"- With the categories of NA and others, we can segregate the passengers as with cabin if cabin has values other than NA and without cabin if it is NA ","f806ab2c":"### Random Forest","9033a7cf":"#### Random Forest","5d10e76f":"#### Logistic Regression","d88822ad":"#### XGBoost","67edeefd":"### Submission:","13f20851":"### GradientBoost"}}