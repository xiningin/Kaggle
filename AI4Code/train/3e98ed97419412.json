{"cell_type":{"093cda14":"code","a20a33bc":"code","2342f4ac":"code","9c15aab8":"code","d9afbca8":"code","8beb5d43":"code","ef3b59b9":"code","4341da97":"code","795ca5f3":"code","3b8cc609":"code","4e76d4df":"code","3232c990":"code","eb4e70d9":"code","ecc079ec":"code","ec7ef54d":"code","881c91c6":"code","75ff6f4d":"code","8653a0fc":"code","b4cda468":"code","62ae8451":"code","d593ccb2":"code","7f31632d":"code","cdc35d4e":"code","62ef42f3":"code","88bb4144":"code","00311b06":"code","43dcb3a1":"code","d9509371":"code","717e285f":"code","e618528a":"code","c2704d5a":"code","a40860d7":"code","2791f1dc":"code","7d42319e":"code","24d65b15":"code","441f169e":"code","e36ee162":"code","471e411b":"code","c6ae1615":"code","19fafc2a":"code","7c127105":"code","9b5fd96d":"code","3a0f2ecc":"code","549b5f75":"code","ebf64259":"code","daed4b05":"code","5a8f3ab2":"code","e27bd607":"code","7758ee09":"code","77127dc5":"code","6caeb9be":"code","cf8319bb":"code","a39d9ef0":"code","a7e56a2b":"code","bab90e4c":"markdown","8aed9236":"markdown","ebbbf1f6":"markdown","6c7dafd1":"markdown","a1b2f375":"markdown","9978bb1e":"markdown","d49eb68f":"markdown","35437f7b":"markdown","2f544234":"markdown","6311acda":"markdown","3dfbded9":"markdown","657931a7":"markdown","257c4426":"markdown","efa88539":"markdown","3c85bab5":"markdown","cff9530e":"markdown","489963f1":"markdown","9b4754fb":"markdown","f155749d":"markdown","607e15dd":"markdown","beaf2109":"markdown","8aeba26b":"markdown","dea73c8b":"markdown","8a892ce8":"markdown","24b5ea98":"markdown","77efbfb2":"markdown"},"source":{"093cda14":"#importing required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom scipy.stats import skew\nimport warnings\nwarnings.filterwarnings('ignore')","a20a33bc":"# loading data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain.head()","2342f4ac":"# checking the dimension of datasets\ntrain.shape, test.shape","9c15aab8":"# ckeck the columns\ntrain.columns","d9afbca8":"# missing data from train dataset\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data.head(30)","8beb5d43":"# Dealing with missing data\ntrain = train.drop((missing_data[missing_data['Total']>1]).index,1)\ntrain['Electrical'] = train['Electrical'].fillna(train['Electrical'].mode()[0])\ntrain.isnull().sum().max()","ef3b59b9":"# After deleting the columns \ntrain.columns","4341da97":"# missing data from test dataset\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data1.head(40)","795ca5f3":"test = test.drop((missing_data1[missing_data1['Total']>4]).index,1)","3b8cc609":"# missing data from test dataset\ntotal = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\nmissing_data1 = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data1.head(40)","4e76d4df":"null_features = (missing_data1[missing_data1['Total']>0]).index\nnull_features","3232c990":"for feature in null_features:\n    test[feature] = test[feature].fillna(test[feature].mode()[0])","eb4e70d9":"# check again if there is any null values\ntest.isnull().sum().max()","ecc079ec":"#Descriptive statistics summary\ntrain['SalePrice'].describe()","ec7ef54d":"#histogram\nsns.distplot(train['SalePrice']);","881c91c6":"# correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat,vmax=.8,square = True);","75ff6f4d":"# most correlated features \ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice'])>0.5]\nplt.figure(figsize=(10,10))\nsns.heatmap(train[top_corr_features].corr(),annot = True);\ntop_corr_features","8653a0fc":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","b4cda468":"# differentiate between numerical and categorical varibles\ncategorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns","62ae8451":"# taking numerical dataset and categorical datasets separately \ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","d593ccb2":"train_cat.shape,train_num.shape","7f31632d":"# checkin skewmess of all features\nskewness = train_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","cdc35d4e":"skewness = skewness[abs(skewness)>0.5]\nskewness.index","62ef42f3":"# applying log tranform\n#train_num[skewness.index] = np.log1p(train_num[skewness.index])","88bb4144":"train_cat.head()","00311b06":"# using get_dummies here it is used for data manipulation. It converts categorical data into dummy or indicator variables\ntrain_cat = pd.get_dummies(train_cat)","43dcb3a1":"train_cat.shape","d9509371":"# concatenating train_num (numerical variable) and train_cat (categorical variable)\ntrain1 = pd.concat([train_cat,train_num],axis=1)\ntrain1.shape","717e285f":"# differentiate between numerical and categorical varibles\ncategorical_features = test.select_dtypes(include = [\"object\"]).columns\nnumerical_features = test.select_dtypes(exclude = [\"object\"]).columns\n\ntest_num = test[numerical_features]\ntest_cat = test[categorical_features]","e618528a":"test_num.shape,test_cat.shape","c2704d5a":"# finding skewness of all features\nskewness = test_num.apply(lambda x: skew(x))\nskewness.sort_values(ascending=False)","a40860d7":"# we are selecting features where skewness is greater than 0.5 to fix their skewness\n#skewness = skewness[abs(skewness)>0.5]\n#len(skewness.index)","2791f1dc":"# applying log tranform\n#test_num[skewness.index] = np.log1p(test_num[skewness.index])","7d42319e":"test_cat.head()","24d65b15":"test_cat = pd.get_dummies(test_cat)","441f169e":"test_cat.head()","e36ee162":"test1 = pd.concat([test_cat,test_num],axis=1)\n","471e411b":"test1.shape","c6ae1615":"# set minimum and maximum threshold values to detect ouliers using standard deviation\nmin_threshold = train1.SalePrice.mean() - 3*train1.SalePrice.std()\nmax_threshold = train1.SalePrice.mean() + 3*train1.SalePrice.std()","19fafc2a":"min_threshold,max_threshold","7c127105":"# removing the outlier's from dataset\ntrain1 = train1[(train1.SalePrice<max_threshold) & (train1.SalePrice)>min_threshold]","9b5fd96d":"train1.shape,test1.shape","3a0f2ecc":"# importing all the required library for modeling here we are going to use statsmodels \nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split","549b5f75":"cols = [col for col in train1.columns if col not in test1.columns]\ncols.remove('SalePrice')\ntrain1 = train1.drop(cols,axis=1)","ebf64259":"# assining the required data to the respective variables  \nX = train1.drop(['SalePrice'],axis=1)\ny = train1['SalePrice']","daed4b05":"#X1 = sm.add_constant(X)\n#test2 = sm.add_constant(test1)","5a8f3ab2":"# checking shapes\ntest1.shape,train1.shape","e27bd607":"train1.shape","7758ee09":"#model = sm.OLS(y, X).fit()\n#predictions = model.predict(test2)\n#print(\"ROOT MEAN SQUARED ERROR : \",math.sqrt(sum((y-predictions)**2)\/len(y)))\n","77127dc5":"# parity plot for statsmodels OLS\n''''plt.scatter(predictions,y,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([10.5,13.5],[10.5,13.5],c='red')\nplt.show()'''","6caeb9be":"from xgboost import XGBRegressor\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,random_state=0)\nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmodel.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\npredictions1 = model.predict(X_valid)\nprint(\"ROOT MEAN SQUARED ERROR : \",math.sqrt(sum((y_valid-predictions1)**2)\/len(y_valid)))","cf8319bb":"# parity plot \nplt.scatter(predictions1,y_valid,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([50000,700000],[50000,700000],c='red')\nplt.show()","a39d9ef0":"# lets now test for the test set\npredictions = model.predict(test1)","a7e56a2b":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nfinal_submission = pd.DataFrame({'Id':submission['Id'],'SalePrice':predictions})\nfinal_submission.to_csv('submission1.csv', index=False)","bab90e4c":"linear regression model makes a good amount of **assumptions** for the data you provide\n\n1. Linearity \n2. No noise \n3. No collinearity\n4. Normal distribution\n5. Scale","8aed9236":"## Scatter plot between 'SalePrice' and its correlated Variables ","ebbbf1f6":"**`Standard Deviation `** means simply it shows you how far away data point from the mean.<br>\nIf our dataset is normally distributed then most of the data fall under `1 standard deviation nearly 68% of data`.<br>\nHere we will use **`3 standard deviation` to `remove outliers`**. \nNearly **`99.7%`** of values are within **`3 standard deviation`** of the mean and the data `outside the 3 standard deviation` we consider them as `outlier's`. ","6c7dafd1":"trying to optimize model with **XGBoost** stands for **extreme gradient boosting**","a1b2f375":"Hello everyone, this is the first thing that I'm putting on the kaggle and from now I will continously updating it.<br>\nSo as a beginner it is difficult to get this done I have referred to others expert's kernel's which is [here](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) and [here](https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing) and there are so many other great kernels to look into.<br>\nThe main aim behind this kernel to make understand basics to beginner and give little understanding about exploratory data analysis approach.<br>\n\n","9978bb1e":"##### Lets focus on 4th assumption, and that is that predictors and target variable should follow a gaussian distribution. ","d49eb68f":"As we did before for **train** data same logic applies here for **test** data.","35437f7b":"## Missing data","2f544234":"Here we are using **Log Transform**","6311acda":"### Using 3 standard deviation to remove outliers ","3dfbded9":"We used **heatmap** here, so we can get the overview of all the features relationship ","657931a7":"## Outliers","257c4426":"_Handling missing values is an essential part of data cleaning and preparation process_.<br>\n***np.nan, None and NaT (for datetime64 types)*** _are standard missing value for Pandas_.<br>\n_Not all missing values come in nice and clean **np.nan or None format** instead there may be some characters like `??`or `--`,etc_.","efa88539":"The above scatterplot comparing the actual values against predicted values in an easy understandable way.","3c85bab5":"## Skewness of test data","cff9530e":"## Fixing Skewness","489963f1":"### Handling missing data from both test and train datasets","9b4754fb":"For `categorical variables` and `numerical variable` we are just filling the **Null** values with most frequent(**mode**) from specified columns.","f155749d":"This are the features most **correlated** with the 'SalePrice'.\n\n**OverallQual, GrLivArea, GarageCars and TotalBsmtSF** are strongly correlated with the 'SalePrice'.\n\n**GarageCars** and **GarageArea** are also some of the most strongly correlated variables.\n\nSame goes for **TotalBsmtSF** and **1stFloor**.","607e15dd":"## Modeling ","beaf2109":"We will delete the variable when there is more then **15% missing data**, according to this 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu','LotFrontage' will get deleted.\n\n'GarageType','GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond' have the same number of missing data so this missing data refers to the same set of observation so we delete this variables and we have considered 'GarageCars' which expressed the most of the information about the garage.\n\nSame logic goes for 'BsmtExposure', 'BsmtFinType1','BsmtQual','BsmtCond','BsmtFinType2'.\n'MasVnrArea' and 'MasVnrType' haave strong corelation with 'YearBuilt' and 'OverallQual' which we have already considered.\n\nFinally 'Electrical' have only one null value so we replace it with its mode.\n","8aeba26b":"##### we are selecting features where skewness is greater than 0.5 to fix their skewness","dea73c8b":"##### Let's focus on 1st asumption","8a892ce8":"## Analysing SalePrice ","24b5ea98":"Now our task will be training and testing the data, so we need to drop the `SalePrice` from the training dataset and will assign it to the `y`","77efbfb2":"#### same applies here as we did for train data"}}