{"cell_type":{"67822ec9":"code","46194034":"code","395db709":"code","375055ff":"code","cd3e7dfd":"code","516091ff":"code","441d2c26":"code","160d068e":"code","0572867b":"code","81c79c06":"code","c1f4df7e":"code","33d5d684":"code","2417aea4":"code","37503bd3":"code","21f9dad7":"code","d5e51c4e":"code","5f98cc69":"code","7bc5283e":"code","39a5286c":"code","5a61712b":"code","743ff16f":"code","33decbdf":"code","b5f7cc83":"code","74069995":"code","56c386ca":"code","8640b50b":"code","6cb9a580":"code","357c28d7":"code","072dfe20":"code","85892044":"code","6f3527d1":"code","86eb319d":"code","37288978":"code","a78175d9":"code","30e591bc":"code","b4d3cafa":"code","0d9cc3c5":"code","2e0eec51":"code","67bf2bbc":"code","134700e3":"code","c7883c4a":"code","c4e047d3":"code","e7e2a6af":"code","c1e2e0ad":"code","bfbd511b":"code","2a29df70":"code","a9d79415":"code","822549aa":"code","69332219":"code","6a02363e":"code","6b9aabfd":"code","d04cd579":"code","16aed908":"code","6c7c8384":"markdown","6beefb39":"markdown","fd04a033":"markdown","e2038d12":"markdown","2677d301":"markdown","f7788e4d":"markdown","eb174fa7":"markdown","c0042770":"markdown","6c766e80":"markdown","66650d29":"markdown","d9d62143":"markdown","b252108c":"markdown","a2f9cc62":"markdown","2e6cf203":"markdown","e968c3c0":"markdown","137e66a1":"markdown","6d4457cf":"markdown","ee1f69c7":"markdown","8e2e3bdc":"markdown","e798b0ea":"markdown","16d6cff8":"markdown","b22ca811":"markdown","a0e542f3":"markdown","246fd072":"markdown","b6a9bcc3":"markdown","4c67e6cb":"markdown","60f32002":"markdown","d70e626a":"markdown","4fa52355":"markdown","f72524bb":"markdown","8ce6018a":"markdown","4600aa52":"markdown","f04b404b":"markdown","f1561d18":"markdown","c5fc54ea":"markdown","ae712604":"markdown","81f7f6d6":"markdown","f1bb90dc":"markdown","96208533":"markdown","27d65ac9":"markdown","c5cd0688":"markdown","a488a62b":"markdown","63ceb7fb":"markdown","ce7e9668":"markdown","14a32ff1":"markdown","d59e61c4":"markdown","cab02d2a":"markdown","34c82586":"markdown","20275383":"markdown","452220a5":"markdown","d4519975":"markdown","1c683f62":"markdown","abf80f11":"markdown","3c1ad25a":"markdown","1a5fa9a1":"markdown","301668ed":"markdown","1c8ac786":"markdown","9da12653":"markdown","85b6f238":"markdown","ee8f2909":"markdown","51ffe97e":"markdown","18f2da84":"markdown","fccaea91":"markdown","479503a5":"markdown","77030f8d":"markdown","73b39813":"markdown","422ba6b9":"markdown","67e64368":"markdown","1d5c8dd0":"markdown","8a1dea3d":"markdown"},"source":{"67822ec9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","46194034":"import numpy as np  \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict\nfrom surprise import KNNWithMeans\nfrom surprise import SVD, SVDpp\nfrom surprise import KNNBaseline\nfrom surprise import KNNBasic\nfrom surprise import KNNWithZScore\nfrom surprise import BaselineOnly\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise import accuracy\nfrom surprise.model_selection import train_test_split\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import KFold\nfrom surprise.model_selection import GridSearchCV\n\nimport time","395db709":"start_time = time.time()\n\ndf = pd.read_csv(\"\/kaggle\/input\/ratings-electronics\/ratings_Electronics.csv\", names=[\"userId\", \"productId\", \"rating\", \"timestamp\"])  \ndf.head() \n\ncomputational_time = time.time() - start_time\nprint('Done in %0.3fs' %(computational_time))","375055ff":"rows_count, columns_count = df.shape\nprint('Total Number of rows :', rows_count)\nprint('Total Number of columns :', columns_count)","cd3e7dfd":"df.dtypes","516091ff":"unique_userId = df['userId'].nunique()\nunique_productId = df['productId'].nunique()\nprint('Total number of unique Users    : ', unique_userId)\nprint('Total number of unique Products : ', unique_productId)","441d2c26":"sns.heatmap(df.isna(), yticklabels=False, cbar=False, cmap='viridis')","160d068e":"df.apply(lambda x : sum(x.isnull()))","0572867b":"df.isnull().sum()","81c79c06":"df.isna().any()","c1f4df7e":"df_transpose = df.describe().T\ndf_transpose","33d5d684":"df_transpose[['min', '25%', '50%', '75%', 'max']]","2417aea4":"plt.figure(figsize=(20,5))\nsns.boxplot(data=df, orient='h', palette='Set2', dodge=False)","37503bd3":"start_time = time.time()\n\nsns.pairplot(df, diag_kind= 'kde')\n\ncomputational_time = time.time() - start_time\nprint('Done in %0.3fs' %(computational_time))","21f9dad7":"df['rating'].value_counts()","d5e51c4e":"rating_counts = pd.DataFrame(df['rating'].value_counts()).reset_index()\nrating_counts.columns = ['Labels', 'Ratings']\nrating_counts","5f98cc69":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15,7))\nsns.countplot(df['rating'], ax=ax1)\nax1.set_xlabel('Rating Distribution', fontsize=10)\nax1.set_ylabel('Count', fontsize=10)\n\n\nexplode = (0.1, 0, 0.1, 0, 0)\nax2.pie(rating_counts[\"Ratings\"], explode=explode, labels=rating_counts.Labels, autopct='%1.2f%%',\n        shadow=True, startangle=70)\nax2.axis('equal')\nplt.title(\"Rating Ratio\")\nplt.legend(rating_counts.Labels, loc=3)\nplt.show()","7bc5283e":"df.corr()","39a5286c":"mask = np.zeros_like(df.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(5,2))\nplt.title('Correlation of Attributes', y=1.05, size=10)\nsns.heatmap(df.corr(),vmin=-1, cmap='plasma',annot=True,  mask=mask, fmt='.2f')","5a61712b":"df = df.drop(['timestamp'], axis=1)","743ff16f":"df1 = df.copy()","33decbdf":"df1.head()","b5f7cc83":"users_counts = df1['userId'].value_counts().rename('users_counts')\nusers_data   = df1.merge(users_counts.to_frame(),\n                                left_on='userId',\n                                right_index=True)","74069995":"subset_df = users_data[users_data.users_counts >= 50]\nsubset_df.head()","56c386ca":"product_rating_counts = subset_df['productId'].value_counts().rename('product_rating_counts')\nproduct_rating_data   = subset_df.merge(product_rating_counts.to_frame(),\n                                left_on='productId',\n                                right_index=True)","8640b50b":"product_rating_data = product_rating_data[product_rating_data.product_rating_counts >= 10]\nproduct_rating_data.head()","6cb9a580":"amazon_df = product_rating_data.copy()","357c28d7":"panda_data = amazon_df.drop(['users_counts', 'product_rating_counts'], axis=1)","072dfe20":"panda_data.head()","85892044":"k = 5","6f3527d1":"reader = Reader(rating_scale=(1, 5))","86eb319d":"surprise_data = Dataset.load_from_df(panda_data[['userId', 'productId', 'rating']], reader)","37288978":"trainset, testset = train_test_split(surprise_data, test_size=.30, random_state=7)","a78175d9":"panda_data.groupby('productId')['rating'].mean().head()","30e591bc":"panda_data.groupby('productId')['rating'].mean().sort_values(ascending=False).head()","b4d3cafa":"prod_rating_count = pd.DataFrame(panda_data.groupby('productId')['rating'].mean().sort_values(ascending=False))\nprod_rating_count['prod_rating_count'] = pd.DataFrame(panda_data.groupby('productId')['rating'].count())\nprod_rating_count.head(k)","0d9cc3c5":"basic_poplurity_model = prod_rating_count.sort_values(by=['prod_rating_count'], ascending=False)\nbasic_poplurity_model.head(k)","2e0eec51":"#Count of user_id for each unique song as recommendation score \npanda_data_grouped = panda_data.groupby('productId').agg({'userId': 'count'}).reset_index()\npanda_data_grouped.rename(columns = {'userId': 'score'},inplace=True)\npanda_data_grouped.head()\n","67bf2bbc":"#Sort the songs on recommendation score \npanda_data_sort = panda_data_grouped.sort_values(['score', 'productId'], ascending = [0,1]) \n      \n#Generate a recommendation rank based upon score \npanda_data_sort['Rank'] = panda_data_sort['score'].rank(ascending=0, method='first') \n          \n#Get the top 5 recommendations \npopularity_recommendations = panda_data_sort.head(k) \npopularity_recommendations ","134700e3":"# UsINNG popularity based recommender model to make predictions\nimport warnings\nwarnings.filterwarnings('ignore')\ndef recommend(userId):     \n    user_recommendations = popularity_recommendations \n          \n    #Adding user_id column for which the recommendations are being generated \n    user_recommendations['userID'] = userId \n      \n    #Bringing user_id column to the front \n    cols = user_recommendations.columns.tolist() \n    cols = cols[-1:] + cols[:-1] \n    user_recommendations = user_recommendations[cols] \n          \n    return user_recommendations ","c7883c4a":"find_recom = [15,121,55,230,344]   # This list is user choice.\nfor i in find_recom:\n    print(\"Here is the recommendation for the userId: %d\\n\" %(i))\n    print(recommend(i))    \n    print(\"\\n\") ","c4e047d3":"cv_results = []  # to store cross validation result ","e7e2a6af":"svd_param_grid = {'n_epochs': [20, 25], 'lr_all': [0.007, 0.009, 0.01], 'reg_all': [0.4, 0.6]}\n\nsvd_gs = GridSearchCV(SVD, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nsvdpp_gs = GridSearchCV(SVDpp, svd_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\n\nsvd_gs.fit(surprise_data)\nsvdpp_gs.fit(surprise_data)\n\n# best RMSE score\nprint(svd_gs.best_score['rmse'])\nprint(svdpp_gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(svd_gs.best_params['rmse'])\nprint(svdpp_gs.best_params['rmse'])","c1e2e0ad":"start_time = time.time()\n\n# Creating Model using best parameters\nsvd_model = SVD(n_epochs=20, lr_all=0.005, reg_all=0.2)\n\n# Training the algorithm on the trainset\nsvd_model.fit(trainset)\n\n\n# Predicting for test set\npredictions_svd = svd_model.test(testset)\n\n# Evaluating RMSE, MAE of algorithm SVD on 5 split(s) by cross validation\nsvd_cv = cross_validate(svd_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n# Storing Crossvalidation Results in dataframe\nsvd_df = pd.DataFrame.from_dict(svd_cv)\nsvd_described = svd_df.describe()\ncv_results = pd.DataFrame([['SVD', svd_described['test_rmse']['mean'], svd_described['test_mae']['mean'], \n                           svd_described['fit_time']['mean'], svd_described['test_time']['mean']]],\n                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])\n\n\n# get RMSE\nprint(\"\\n\\n==================== Model Evaluation ===============================\")\naccuracy.rmse(predictions_svd, verbose=True)\nprint(\"=====================================================================\")\ncomputational_time = time.time() - start_time\nprint('\\n Computational Time : %0.3fs' %(computational_time))\ncv_results","bfbd511b":"start_time = time.time()\n\n# Creating Model using best parameters\nsvdpp_model = SVDpp(n_epochs=25, lr_all=0.01, reg_all=0.4)\n\n# Training the algorithm on the trainset\nsvdpp_model.fit(trainset)\n\n\n# Predicting for test set\npredictions_svdpp = svdpp_model.test(testset)\n\n# Evaluating RMSE, MAE of algorithm SVDpp on 5 split(s) by cross validation\nsvdpp_cv = cross_validate(svdpp_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n# Storing Crossvalidation Results in dataframe\nsvdpp_df = pd.DataFrame.from_dict(svdpp_cv)\nsvdpp_described = svdpp_df.describe()\nsvdpp_cv_results = pd.DataFrame([['SVDpp', svdpp_described['test_rmse']['mean'], svdpp_described['test_mae']['mean'], \n                           svdpp_described['fit_time']['mean'], svdpp_described['test_time']['mean']]],\n                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])\n\ncv_results = cv_results.append(svdpp_cv_results, ignore_index=True)\n\n# get RMSE\nprint(\"\\n\\n==================== Model Evaluation ===============================\")\naccuracy.rmse(predictions_svdpp, verbose=True)\nprint(\"=====================================================================\")\ncomputational_time = time.time() - start_time\nprint('\\n Computational Time : %0.3fs' %(computational_time))\ncv_results","2a29df70":"start_time = time.time()\n\nknn_param_grid = {'bsl_options': {'method': ['als', 'sgd'],\n                              'reg': [1, 2]},\n              'k': [15, 20, 25, 30, 40, 50, 60],\n              'sim_options': {'name': ['msd', 'cosine', 'pearson_baseline']}\n              }\n\nknnbasic_gs = GridSearchCV(KNNBasic, knn_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nknnmeans_gs = GridSearchCV(KNNWithMeans, knn_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\nknnz_gs     = GridSearchCV(KNNWithZScore, knn_param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=5)\n\n\nknnbasic_gs.fit(surprise_data)\nknnmeans_gs.fit(surprise_data)\nknnz_gs.fit(surprise_data)\n\n# best RMSE score\nprint(knnbasic_gs.best_score['rmse'])\nprint(knnmeans_gs.best_score['rmse'])\nprint(knnz_gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(knnbasic_gs.best_params['rmse'])\nprint(knnmeans_gs.best_params['rmse'])\nprint(knnz_gs.best_params['rmse'])\n\ncomputational_time = time.time() - start_time\nprint('\\nComputational Time : %0.3fs' %(computational_time))","a9d79415":"start_time = time.time()\n\n# Creating Model using best parameters\nknnBasic_model = KNNBasic(k=50, sim_options={'name': 'cosine', 'user_based': False})\n\n# Training the algorithm on the trainset\nknnBasic_model.fit(trainset)\n\n# Predicting for test set\nprediction_knnBasic = knnBasic_model.test(testset)\n\n# Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s)\nknnBasic_cv = cross_validate(knnBasic_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n# Storing Crossvalidation Results in dataframe\nknnBasic_df = pd.DataFrame.from_dict(knnBasic_cv)\nknnBasic_described = knnBasic_df.describe()\nknnBasic_cv_results = pd.DataFrame([['KNNBasic', knnBasic_described['test_rmse']['mean'], knnBasic_described['test_mae']['mean'], \n                           knnBasic_described['fit_time']['mean'], knnBasic_described['test_time']['mean']]],\n                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])\n\ncv_results = cv_results.append(knnBasic_cv_results, ignore_index=True)\n\n# get RMSE\nprint(\"\\n\\n==================== Model Evaluation ===============================\")\naccuracy.rmse(prediction_knnBasic, verbose=True)\nprint(\"=====================================================================\")\n\ncomputational_time = time.time() - start_time\nprint('\\n Computational Time : %0.3fs' %(computational_time))\ncv_results\n","822549aa":"start_time = time.time()\n\n# Creating Model using best parameters\nknnZscore_model = KNNWithZScore(k=60, sim_options={'name': 'cosine', 'user_based': False})\n\n# Training the algorithm on the trainset\nknnZscore_model.fit(trainset)\n\n# Predicting for testset\nprediction_knnZscore = knnZscore_model.test(testset)\n\n# Evaluating RMSE, MAE of algorithm KNNWithZScore on 5 split(s)\nknnZscore_cv = cross_validate(knnZscore_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n# Storing Crossvalidation Results in dataframe\nknnZscore_df = pd.DataFrame.from_dict(knnZscore_cv)\nknnZscore_described = knnZscore_df.describe()\nknnZscore_cv_results = pd.DataFrame([['KNNWithZScore', knnZscore_described['test_rmse']['mean'], knnZscore_described['test_mae']['mean'], \n                           knnZscore_described['fit_time']['mean'], knnZscore_described['test_time']['mean']]],\n                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])\n\ncv_results = cv_results.append(knnZscore_cv_results, ignore_index=True)\n\n# get RMSE\nprint(\"\\n\\n==================== Model Evaluation ===============================\")\naccuracy.rmse(prediction_knnZscore, verbose=True)\nprint(\"=====================================================================\")\n\ncomputational_time = time.time() - start_time\nprint('\\n Computational Time : %0.3fs' %(computational_time))\ncv_results\n","69332219":"start_time = time.time()\n\n# Creating Model using best parameters\nknnMeansUU_model = KNNWithMeans(k=60, sim_options={'name': 'cosine', 'user_based': True})\n\n# Training the algorithm on the trainset\nknnMeansUU_model.fit(trainset)\n\n# Predicting for testset\nprediction_knnMeansUU = knnMeansUU_model.test(testset)\n\n# Evaluating RMSE, MAE of algorithm KNNWithMeans User-User on 5 split(s)\nknnMeansUU_cv = cross_validate(knnMeansUU_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n# Storing Crossvalidation Results in dataframe\nknnMeansUU_df = pd.DataFrame.from_dict(knnMeansUU_cv)\nknnMeansUU_described = knnMeansUU_df.describe()\nknnMeansUU_cv_results = pd.DataFrame([['KNNWithMeans User-User', knnMeansUU_described['test_rmse']['mean'], knnMeansUU_described['test_mae']['mean'], \n                           knnMeansUU_described['fit_time']['mean'], knnMeansUU_described['test_time']['mean']]],\n                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])\n\ncv_results = cv_results.append(knnMeansUU_cv_results, ignore_index=True)\n\n# get RMSE\nprint(\"\\n\\n==================== Model Evaluation ===============================\")\naccuracy.rmse(prediction_knnMeansUU, verbose=True)\nprint(\"=====================================================================\")\n\ncomputational_time = time.time() - start_time\nprint('\\n Computational Time : %0.3fs' %(computational_time))\ncv_results\n","6a02363e":"start_time = time.time()\n\n# Creating Model using best parameters\nknnMeansII_model = KNNWithMeans(k=60, sim_options={'name': 'cosine', 'user_based': False})\n\n# Training the algorithm on the trainset\nknnMeansII_model.fit(trainset)\n\n# Predicting for testset\nprediction_knnMeansII = knnMeansII_model.test(testset)\n\n# Evaluating RMSE, MAE of algorithm KNNWithMeans Item-Item on 5 split(s)\nknnMeansII_cv = cross_validate(knnMeansII_model, surprise_data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\n# Storing Crossvalidation Results in dataframe\nknnMeansII_df = pd.DataFrame.from_dict(knnMeansII_cv)\nknnMeansII_described = knnMeansII_df.describe()\nknnMeansII_cv_results = pd.DataFrame([['KNNWithMeans Item-Item', knnMeansII_described['test_rmse']['mean'], knnMeansII_described['test_mae']['mean'], \n                           knnMeansII_described['fit_time']['mean'], knnMeansII_described['test_time']['mean']]],\n                            columns = ['Model', 'RMSE', 'MAE', 'Fit Time', 'Test Time'])\n\ncv_results = cv_results.append(knnMeansII_cv_results, ignore_index=True)\n\n# get RMSE\nprint(\"\\n\\n==================== Model Evaluation ===============================\")\naccuracy.rmse(prediction_knnMeansII, verbose=True)\nprint(\"=====================================================================\")\n\ncomputational_time = time.time() - start_time\nprint('\\n Computational Time : %0.3fs' %(computational_time))\ncv_results\n","6b9aabfd":"x_algo = ['KNN Basic', 'KNNWithMeans-User-User', 'KNNWithMeans-Item-Item', 'KNN ZScore', 'SVD', 'SVDpp']\nall_algos_cv = [knnBasic_cv, knnMeansUU_cv, knnMeansII_cv, knnZscore_cv, svd_cv, svdpp_cv]\n\nrmse_cv = [round(res['test_rmse'].mean(), 4) for res in all_algos_cv]\nmae_cv  = [round(res['test_mae'].mean(), 4) for res in all_algos_cv]\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2, 1, 1)\nplt.title('Comparison of Algorithms on RMSE', loc='center', fontsize=15)\nplt.plot(x_algo, rmse_cv, label='RMSE', color='darkgreen', marker='o')\nplt.xlabel('Algorithms', fontsize=15)\nplt.ylabel('RMSE Value', fontsize=15)\nplt.legend()\nplt.grid(ls='dashed')\n\nplt.subplot(2, 1, 2)\nplt.title('Comparison of Algorithms on MAE', loc='center', fontsize=15)\nplt.plot(x_algo, mae_cv, label='MAE', color='navy', marker='o')\nplt.xlabel('Algorithms', fontsize=15)\nplt.ylabel('MAE Value', fontsize=15)\nplt.legend()\nplt.grid(ls='dashed')\n\nplt.show()\n\ncv_results","d04cd579":"top_n = defaultdict(list)\ndef get_top_n(predictions, n=k):\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\ntop_n = get_top_n(predictions_svd, n=k)\ntop_n","16aed908":"def precision_recall_at_k(predictions, k=5, threshold=3.5):\n    '''Return precision and recall at k metrics for each user.'''\n\n    # First map the predictions to each user.\n    user_est_true = defaultdict(list)\n    for uid, _, true_r, est, _ in predictions:\n        user_est_true[uid].append((est, true_r))\n\n    precisions = dict()\n    recalls = dict()\n    for uid, user_ratings in user_est_true.items():\n\n        # Sort user ratings by estimated value\n        user_ratings.sort(key=lambda x: x[0], reverse=True)\n\n        # Number of relevant items\n        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n\n        # Number of recommended items in top k\n        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n\n        # Number of relevant and recommended items in top k\n        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n                              for (est, true_r) in user_ratings[:k])\n\n        # Precision@K: Proportion of recommended items that are relevant\n        precisions[uid] = n_rel_and_rec_k \/ n_rec_k if n_rec_k != 0 else 1\n\n        # Recall@K: Proportion of relevant items that are recommended\n        recalls[uid] = n_rel_and_rec_k \/ n_rel if n_rel != 0 else 1\n\n    return precisions, recalls\n\n\nkf = KFold(n_splits=5)\nsvd_model = SVD(n_epochs=20, lr_all=0.005, reg_all=0.2)\nprecs = []\nrecalls = []\n\nfor trainset, testset in kf.split(surprise_data):\n    svd_model.fit(trainset)\n    predictions = svd_model.test(testset)\n    precisions, recalls = precision_recall_at_k(predictions, k=5, threshold=3.5)\n\n    # Precision and recall can then be averaged over all users\n    print('Precision : ', sum(prec for prec in precisions.values()) \/ len(precisions))\n    print('recalls : ',sum(rec for rec in recalls.values()) \/ len(recalls))\n","6c7c8384":"# ------------------------Matrix Factorization Based Algorithms------------------------------","6beefb39":"<b>Comment:<\/b> By displaying the datatypes of each variable we can see the following:\n\n   -  int type           :  rating, timestamp\n   -  object type(string):  userId, productId\n\n","fd04a033":"##  Read and explore the given dataset. (Rename column\/add headers, plot histograms, find data characteristics)","e2038d12":"## Summary ::-\n\n<b>Insight : <\/b>\n- I Have done EDA to understand the data in precise way and found that most of the custermors have given 5 rating. Which gives us an important information that Amazon is performing good in tems of Electronics Products sell.  \n- Taken the subset of data based on users who have given 50 or more rating and the products which recieved 10 or more number of ratings to overcome the Grey ship problem.\n- In <b>Popularity Model<\/b> I have shown the top 5 recommended products irrespective of users. This means same top 5 products will be recommended to each user.\n- I have used 'Matrix Factorization Based Algorithms' & 'k-NN Based Algorithms' to build <b>Collaborative Filtering model<\/b>.\n- We have also seen that KNNWithMeans is performing well compare to other k-NN Based Algorithms.\n- I found that SVD++ has given lowest RMSE which is slightly better than SVD but computational time of SVD++ is 12 times greater than SVD. Hence I have consider SVD to get the recommended products.\n    - SVD with parameters   => Number of Epochs = 20, Learning Rate= 0.005, Regularization Term = 0.2\n    - SVD++ with parameters => Number of Epochs = 25, Learning Rate= 0.01, Regularization Term = 0.4\n- I have computed the precision which is almost 87%. Here we can interpret that 87% of my recommendations are actually relevant to the user.\n- In recall we can interpret that 83% percent of the relevant items were recommended in the top-k items.","2677d301":"# Taking a subset of the dataset to make it less sparse\/ denser.( For example, keep the users only who has given 50 or more number of ratings ) ::-","f7788e4d":"### => Grid Search :-","eb174fa7":"## Spliting the data randomly into train and test dataset. (Split it in 70\/30 ratio) ::-","c0042770":"### => KNNWithZScore :-","6c766e80":"### => Shape of the data :- ","66650d29":"#### Dropping timestamp :-","d9d62143":"### =>  Five point summary of  numerical attributes  :-","b252108c":"<b>Comment : <\/b> As we do not have many attributes and if we see the correlation between rating and timestamp then we won't find any high correlation between them.","a2f9cc62":"### =>  SVD :-","2e6cf203":"# :::::::::::::::::::::::::::::::::::::Steps and tasks::::::::::::::::::::::::::::::::::::::::::::","e968c3c0":"<b>Comment:<\/b> Shape of the dataframe is (7824482, 4).\nThere are 7824482 rows and 4 columns in the dataset.","137e66a1":"### => Data type of each attribute :-","6d4457cf":"### => Import the necessary libraries :","ee1f69c7":"# Building Popularity Recommender model ::- ","8e2e3bdc":"- Here I am using SVD algorithm to get the top 5 recommendations of new products for each user.","e798b0ea":"# ::--------------------------- Exploratory Data Analysis -------------------------------- ::","16d6cff8":"### => Checking the presence of outliers :-","b22ca811":"Taking copy of dataframe df to df1 before doing any manupulation with dataset so to save loading time I have copied.","a0e542f3":"### => KNNWithMeans User-User ","246fd072":"### =>  Data Characteristics :-","b6a9bcc3":"<b> Comment :<\/b> Here we can see that the RMSE of testset and complete dataset found from cross_validation is amost same it seems our model is performing well on trainset and testset.","4c67e6cb":"<b>Observation :<\/b> From the barplot and pia chart we can clearly see that approx 55% of data have 5 rating followed by 4(approx 19%). Least number of people have given 2 rating. One important insight is coming from here that most of the products are liked by the customers.   ","60f32002":"Here I am Using grid search to find out the best hyper parameters for <b>KNNBasic<\/b>, <b>KNNWithMeans<\/b> and <b>KNNWithZScore<b\/> Algorithm.","d70e626a":"<b>Comment : <\/b>  From above we can see that \n- Mean of rating is less than median which stats that the distribution is negatively skewed.\n- Mean of timestamp is almost near to median which stats the distribution is symmetric.","4fa52355":"### => SVD++ :-","f72524bb":"### => Checking the presence of missing values :-","8ce6018a":"<b> Comment : <\/b> From above we can see there is tall tower of 5 rating which stats that most of the customers have given 5 rating. Very less customers have given 2 rating. From here we can infer an important thing that most of the electronics products are liked by the customers.","4600aa52":"### => Read the data as a dataframe :- ","f04b404b":"<b>Comment : <\/b> Top 5 popular products(B0088CJT4U, B003ES5ZUU, B000N99BBC, B007WTAJTO, B00829TIEK).\n- Since this is a popularity-based recommender model, recommendations remain the same for all users. We predict the products based on the popularity. It is not personalized to particular user.","f1561d18":"To Get top - K ( K = 5) recommendations I am initalizing k below.","c5fc54ea":"# ::-------------------------------------- Data Visualization ------------------------------------::","ae712604":"<b>Comment:<\/b> Here I have read the Ratings Data using read_csv() function of pandas. df is a dataframe. I have used head() funtion to display first 5 records of the dataset.","81f7f6d6":"<b>Comment:<\/b> From above missing value graph we can see that there is no missing values which I have also checked bu using inull() and isna() function of dataframe.","f1bb90dc":"## =>  Using mean of products rating :-","96208533":"#### => Taking Subset of products which have recieved 10 or more number of ratings to overcome the Grey Ship problem :-","27d65ac9":"To load the dataset from a pandas dataframe, we will need the load_from_df() method. we will also need a reader obeject which I have I have already decleared. ","c5cd0688":"#### Using popularity based recommender model to make predictions and find recommendations for random list of users with inferences","a488a62b":"<b>Comment:<\/b> Number of 1 ratings in our dataset is higher than other rating. There are most number of people who have given 1 rating to the products. \nBelow code shows the ration among them.","63ceb7fb":"From above algorithm comparisons plots we can infer the followings:\n- RMSE : we can see that SVD++ is giving the best RMSE value with parameters {'n_epochs': 25, 'lr_all': 0.01, 'reg_all': 0.4} and SVD is giving the second best RMSE with parameters {'n_epochs': 20, 'lr_all': 0.005, 'reg_all': 0.2}\n- MAE : Here SVD++ and KNNWithMeans both are giving the best MAE value.\n- Svd++ is having the best RMSE in Matrix Factorization Based Algorithms.\n- KNNWithMeans is giving the best RMSE in Collaborative Filtering Algorithms.\n- <b> Important : <\/b> If compare SVD and SVD++ then can notice that RMSE and MAE value of SVD is slightly differs from the SVD++ but the Fit Time and Test Time taken by SVD is significant less(12 times) than SVD++. So, we will proceed with SVD got get top-k recommendations\n","ce7e9668":"<b>Important : <\/b> Here I am considering only those products which recieved as least 10 ratings. Because there may be some cases where product's number of rating will be 1 or 2 but rating value will be 5, in this case these kind of products will be appear at top for the recommendation which would not be a good recommendation technique.    ","14a32ff1":"<b>Comment : <\/b> Above we have found the best parameters for the SVD an SVDpp algorithm and we will be pass these parameters to while creating model.","d59e61c4":"<b><u>Data Description<\/u>:<\/b>  Amazon Reviews data (Electronics Dataset) The repository has several \ndatasets. For this case study, we are using the Electronics \ndataset. \n\n<b><u>Domain<\/u>:<\/b>  E-commerce \n\n<b><u>Context<\/u>:<\/b> Online E-commerce websites like Amazon, Flipkart uses \ndifferent recommendation models to provide different \nsuggestions to different users. Amazon currently uses \nitem-to-item collaborative filtering, which scales to massive \ndata sets and produces high-quality recommendations in \nreal-time.   \n\n<b><u>Objective<\/u>:<\/b>  Build a recommendation system to recommend products to \ncustomers based on the their previous ratings for other \nproducts.\n\n<b><u>Attribute Information <\/u> :<\/b>\n\nInput variables:\n##### Amazon Electronics Rating data:\n1. UserId : Every user identified with a unique id \n2. ProductId : Every product identified with a unique id \n3. Rating : Rating of the corresponding product by the corresponding user\n4. Timestamp : Time of the rating ( ignore this column for this exercise)\n    \n<b> <u>Learning Outcomes <\/u> :<\/b>\n- Exploratory Data Analysis\n- Creating a Recommendation system using real data \n- Collaborative filtering","cab02d2a":"## ----------------------------- Comparison of all algorithms on RMSE and MAE ------------------------","34c82586":"## top - K ( K = 5) recommendations ::-","20275383":"### =>  Pair plot that includes all the columns of the data frame :-","452220a5":"## => Ranking-Based Algorithms  :-\n\nCreating a Product recommender :-\n\nBuilding Popularity Recommender model(Non-personalised) :-","d4519975":"<b> Comment : <\/b> From the above list we can see that model is recommending top 5 products to each user. There are some cases which  it recommends less than 5 products. It happend becaus model is not able to find appropriate number of neighbours.","1c683f62":"## Evaluation Results :-","abf80f11":"In two way I will be creating the Recommender model.\n- Using mean of product rating\n- Using Ranking Based Algorithm","3c1ad25a":"<b>Observation : <\/b> From the above boxplot we can see that there are outliers in timestamp column. But I will not fixed the outliers as I will be dropping timestamp which is mentioned in the problem statement.","1a5fa9a1":"### => Checking the ratio of all 5 ratings","301668ed":"# ---------------------------------- k-NN Based Algorithms ----------------------------------------","1c8ac786":"<b>Comment : <\/b> I have calculated Precision and recall at k=5. As we know that Precision and recall are binary metrics used to evaluate models with binary output. Thus we need a way to translate our numerical problem (ratings usually from 1 to 5) into a binary problem (relevant and not relevant items). To do the translation I have assumed that any true rating above 3.5 corresponds to a <b>relevant<\/b> item and any true rating below 3.5 is <b>irrelevant<\/b>. \n- My precision at 5 in a top-5 recommendation problem is alomost 87%. This means that 87% of the recommendation are relevent to the users.\n- My recall at 5 in a top-5 recommendation problem is almost 83%. This means that 83% of the total number of the relevent products appear in the top-k result.","9da12653":"### => Grid Search :-","85b6f238":"### => KNNBasic :-","ee8f2909":"### => Unique UserId and ProductID :-","51ffe97e":"## Building Collaborative Filtering model ::-\n\nFor the Collaborative Filtering Model I am going to use <b>SVD, KNNWithMeans <\/b> and I will also test with other algorithm","18f2da84":"<b>Comment : <\/b> Above we can see only those users who have given 50 or more number of ratings.","fccaea91":"#### => Taking Subset of users who have given 50 or more number of ratings :-","479503a5":"Here I am Using grid search to find out the best hyper parameters for SVD and SVDpp Algorithm.\n- n_epochs values : [20, 25] \n- lr_all          : [0.007, 0.009, 0.01]\n- reg_all         : [0.4, 0.6]","77030f8d":"<b>Comment :<\/b> \n- Here I have used numpy, pandas, matplotlib, seaborn for EDA and Data Visualization. \n- Also used Surprise library for data spliting, model building and for accuracy.\n- GridSearchCV to find the best parameters.","73b39813":"<b>Comment : <\/b> Above is the list of top 5 popular products for the recommendation.","422ba6b9":"### => Creating and view the correlation matrix :-","67e64368":"### => KNNWithMeans Item-Item ","1d5c8dd0":"### => Precision and recall at k=5","8a1dea3d":"We need to define a Reader object for Surprise to be able to parse the dataframe."}}