{"cell_type":{"e9c59c83":"code","0f80fadb":"code","1c240e02":"code","a4775ee6":"code","625db35e":"code","33102591":"code","96b77e4b":"code","950e4236":"code","da80c404":"code","c10ff657":"code","c1d71d51":"code","5ef5b6b5":"code","fc8a0378":"code","a4cff3a1":"code","1e756358":"code","22304ad8":"code","f297fb41":"code","f0225186":"code","88d7937b":"code","9244dbef":"code","c1dba7d0":"code","f8c343b6":"code","a854733b":"code","e6ddab31":"code","4e6816a1":"code","e7b231a6":"code","1ec07597":"code","631aef99":"code","1d90b7c7":"code","3d7d4e22":"code","12643af9":"code","c5848f22":"code","5b529112":"code","c74a5fa4":"code","5398c88b":"code","0b64a9f1":"code","d951f681":"code","60b60b6a":"code","6c26082a":"code","4a227246":"code","56b48162":"code","01234905":"code","104879d2":"code","627e847c":"code","e42b9e16":"code","ede89391":"code","339007a5":"code","39603ede":"code","327dbcd4":"code","650f07c5":"code","a1fe82d0":"code","23bfb396":"code","045b6d2d":"code","6259f908":"code","ab189e44":"code","be5f46ad":"code","64fd0dff":"code","ca655f39":"code","c3585b39":"code","3197f8bb":"code","a64c6c45":"code","1b8263d6":"code","6e7821f8":"code","24786a74":"code","44a2cace":"code","c46c6605":"code","7d29f4ec":"code","320a6dd3":"code","0897b5ac":"code","b3bf43e7":"code","687652f5":"code","9b88710f":"code","b4baa53f":"code","66a79d3e":"code","94c801db":"code","bea51e26":"code","6bb5be3a":"code","edaa1e2e":"code","118c609d":"code","2c5ddd0a":"code","495b75b1":"code","d623a9ed":"code","d07d57c8":"code","a6d8fb43":"code","305b3814":"code","83acd95d":"code","12c653da":"code","2837f84c":"code","43e25d76":"code","13d5dd24":"code","a1b3f767":"code","9bf6abf9":"code","e2b35cc1":"code","3e057e19":"code","c407986c":"code","161c6871":"code","ed4e1c06":"code","d7822cdd":"code","e853a931":"code","f6f3a05d":"code","4fc569f0":"code","34c73dfc":"code","cc3ec4dc":"code","760a1681":"code","eb4a0432":"code","1aee3225":"code","5b297cc2":"code","bd2e76ca":"code","f5774e47":"code","fad57c81":"code","cbdac78c":"code","f1549b7f":"code","6b807346":"code","729db78a":"code","5ea36422":"code","f30c3f40":"code","8fb91ba0":"code","c3e22ae6":"code","004f77df":"code","86d2156b":"code","00ad3db2":"code","8a73af34":"code","a2c85d9b":"code","b5e829fa":"code","c566accd":"code","dbe23b6b":"code","b0c722be":"code","9a9b123d":"code","d88d0aad":"code","a0f7c626":"code","a410df1d":"code","c78a1444":"code","ac79c657":"code","f6552051":"code","087373c8":"code","a9c275e0":"code","681cf585":"code","613dff02":"code","d2e13a73":"code","78d93c43":"code","388e3a8d":"code","1fab8a76":"code","043d2b55":"code","9c42c484":"code","a665a80d":"code","be7c9c20":"code","30d16879":"code","eb353239":"code","96ce4a3a":"code","a164fec1":"code","3ebad479":"code","51a412da":"code","8921041c":"code","41d60391":"code","1588e1d3":"code","797cad8a":"code","cc15e8ab":"code","dbf0e404":"code","dfd0ecba":"code","800cca7b":"code","50eb882a":"code","c12ac3a8":"code","120bb5c1":"code","4441f93f":"code","0f14e579":"code","8cac947e":"code","d231f016":"code","a64d41ed":"markdown","dbb02139":"markdown","d852ea9a":"markdown","28cfe329":"markdown","2a083d08":"markdown","85daf18e":"markdown","d7878a5e":"markdown","bd18cd0c":"markdown","6c204e3f":"markdown","f741f3bf":"markdown","33cf4bfa":"markdown","595d6964":"markdown","e5ed1143":"markdown","2405a0fb":"markdown","b5470502":"markdown","90788723":"markdown","ef3f9d07":"markdown","b1f58c25":"markdown","aed46b4b":"markdown","be42e9f5":"markdown","62fa44bf":"markdown","42a5e958":"markdown","8dff59d5":"markdown","fb0fad94":"markdown","de9a0abd":"markdown","426ebe42":"markdown","ccb40016":"markdown","df50cd53":"markdown","5fc0433c":"markdown","64352f44":"markdown","7fd621db":"markdown","2fdfeb5f":"markdown","d206d928":"markdown","937ba06d":"markdown","6697ea7c":"markdown","d3747824":"markdown","7a601910":"markdown","638cef67":"markdown","34278d13":"markdown","67df4e3f":"markdown","6a4f4658":"markdown","a2586d90":"markdown","54ffdb0c":"markdown","aeadf193":"markdown","4817381d":"markdown","416c85c2":"markdown","de75e4de":"markdown","3985102c":"markdown","119a3bdf":"markdown","e53ceea3":"markdown","4dda832a":"markdown","b507b68a":"markdown","7585341e":"markdown","97ac6ac9":"markdown","ea9ec914":"markdown","8fd3670c":"markdown","717d9650":"markdown","504e23a5":"markdown","31acf717":"markdown","244bad52":"markdown","0f3cbbf7":"markdown","93950335":"markdown","9f56a4d8":"markdown","66492448":"markdown","4a6cd5a4":"markdown","50ab2dae":"markdown","be78fd6e":"markdown","9216feb7":"markdown","72b97898":"markdown","09cfaf0d":"markdown","5c7fee8d":"markdown","f58034a1":"markdown","f20bc53b":"markdown","018203ae":"markdown","b977436d":"markdown","8c37fdd6":"markdown","08588906":"markdown","05e2b53f":"markdown","0e3aa3ad":"markdown","24b051f7":"markdown","d5009789":"markdown","a54f794c":"markdown","692cd45d":"markdown","1f43948f":"markdown","68394fef":"markdown","2bb32d87":"markdown","cf93c641":"markdown","23d21f8b":"markdown","44e62d80":"markdown","e6eb389b":"markdown","a5f6cd20":"markdown","61e62028":"markdown","7fb2a72d":"markdown","3aa0e246":"markdown","ab1f078a":"markdown","1c21190e":"markdown","caa8b1dc":"markdown","bb14e05f":"markdown","de0a143b":"markdown","33b14590":"markdown","4b56300e":"markdown","f51a1b70":"markdown","3edc1136":"markdown","1f27ca5e":"markdown","dc406e47":"markdown","2e4b8d46":"markdown","c548eed1":"markdown","ec73cc22":"markdown","9470862c":"markdown","2a341434":"markdown","7c7a186a":"markdown","45fe01bb":"markdown","ca02beaa":"markdown","3852af8c":"markdown","37640b9a":"markdown","04d77a0f":"markdown","2fdb2538":"markdown","62e79cce":"markdown","7c1ebfe1":"markdown","00c7dd3b":"markdown","146de678":"markdown","2c1f7781":"markdown","8df68adb":"markdown","024966e0":"markdown","a814f0c3":"markdown","82ba2f56":"markdown","0b545d8d":"markdown","c3683755":"markdown","9814d2d2":"markdown","cf08133e":"markdown","06ab0c46":"markdown","00b1dcd8":"markdown","65416456":"markdown","3890f645":"markdown","9822681c":"markdown","52c0a2e2":"markdown","c95b5906":"markdown","e05cc995":"markdown"},"source":{"e9c59c83":"# hide warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Import Libraries\nimport sys,joblib\nimport six\nsys.modules['sklearn.externals.six'] = six\nsys.modules['sklearn.externals.joblib'] = joblib\nimport numpy as np \nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set_context(\"talk\", font_scale = 0.65, rc={\"grid.linewidth\": 5})\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows', 400)\nfrom sklearn.linear_model import LogisticRegression,LinearRegression,LassoCV,Lasso,Ridge,LogisticRegressionCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA,IncrementalPCA\nfrom sklearn.model_selection import GridSearchCV,cross_val_score,KFold,StratifiedKFold,RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score,precision_score,recall_score\nfrom sklearn.metrics import precision_recall_curve,roc_auc_score,roc_curve\nfrom imblearn.over_sampling import SMOTE,RandomOverSampler,ADASYN\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,QuantileTransformer\nfrom scipy.stats import skew\nfrom fancyimpute import IterativeImputer,KNN\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import RFE\nimport statsmodels.api as sm\n","0f80fadb":"telecom = pd.read_csv('..\/input\/telecom-churn-dataset\/telecom_churn_data.csv')\ntelecom.head()","1c240e02":"#Checking the shape\nprint(telecom.shape)\nprint('\\n')\n# Checking Dataset Info \nprint(telecom.info(verbose=True,null_counts=True))","a4775ee6":"#Summary and checking outliers\ntelecom.describe(percentiles=[0.25,0.5,0.75,0.99])","625db35e":"#Function to check percentage of null values present in dataset \ndef calnullpercentage(df):\n    missing_num= df[df.columns].isna().sum().sort_values(ascending=False)\n    missing_perc= (df[df.columns].isna().sum()\/len(df)*100).sort_values(ascending=False)\n    missing= pd.concat([missing_num,missing_perc],keys=['Total','Percentage'],axis=1)\n    missing= missing[missing['Percentage']>0]\n    return missing","33102591":"# check the %age of null values\ncalnullpercentage(telecom)\n","96b77e4b":"# Number of columns having null values\nprint(len(calnullpercentage(telecom)))","950e4236":"#Checking categorical and numerical columns \ntelecom.select_dtypes(include='object').head(3)","da80c404":"# Deriving new columns for total recharge amount data for 6 and 7th month\ntelecom['tot_rech_amt_data_6'] = telecom['total_rech_data_6'] * telecom['av_rech_amt_data_6']\ntelecom['tot_rech_amt_data_7'] = telecom['total_rech_data_7'] * telecom['av_rech_amt_data_7']\n# Deriving new columns for total amount spent during  6 and 7th month\ntelecom['tot_amt_6'] = telecom[['total_rech_amt_6','tot_rech_amt_data_6']].sum(axis=1)\ntelecom['tot_amt_7'] = telecom[['total_rech_amt_7','tot_rech_amt_data_7']].sum(axis=1)\n#first two months average\ntelecom['avg_amt_6_7'] = telecom[['tot_amt_6','tot_amt_7']].mean(axis=1)\n# Filtering customers based on percentile having goodphase_avg more than or equal to cutoff of 70th percentile\ntelecom=telecom.loc[(telecom['avg_amt_6_7'] >= np.percentile(telecom['avg_amt_6_7'], 70))]\ntelecom.shape","c10ff657":"# Deriving new columns for total recharge amount data for 8 and 9th month\ntelecom['tot_rech_amt_data_8'] = telecom['total_rech_data_8'] * telecom['av_rech_amt_data_8']\ntelecom['tot_rech_amt_data_9'] = telecom['total_rech_data_9'] * telecom['av_rech_amt_data_9']\n# Deriving new columns for total amount spent during  8 and 9th month\ntelecom['tot_amt_8'] = telecom[['total_rech_amt_8','tot_rech_amt_data_8']].sum(axis=1)\ntelecom['tot_amt_9'] = telecom[['total_rech_amt_9','tot_rech_amt_data_9']].sum(axis=1)","c1d71d51":"#Finding categorical columns where dtype is float but those columns are having 0 or 1 values only\ncatg= []\nfor col in telecom.columns:\n    if len(telecom[col].unique())== 2 | 3:\n        catg.append(col)\n# COnverting into categorical or object type\ntelecom[catg]=telecom[catg].apply(lambda x:x.astype('object'))\ncol_tmp=['total_rech_num_6','total_rech_num_7','total_rech_num_8','total_rech_num_9','total_rech_data_6',\\\n        'total_rech_data_7','total_rech_data_8','total_rech_data_9']\ntelecom[col_tmp]=telecom[col_tmp].apply(lambda x:x.astype('object'))","5ef5b6b5":"x=['tot_amt_8','total_rech_amt_8','tot_rech_amt_data_8','total_rech_data_8','av_rech_amt_data_8']\nplt.figure(figsize=(8,5))\nsns.heatmap(telecom[x].corr(),annot=True,cmap='viridis_r')","fc8a0378":"telecom.drop(['tot_rech_amt_data_6','tot_rech_amt_data_7','tot_rech_amt_data_8',\\\n              'tot_rech_amt_data_9'],inplace=True,axis=1)\n","a4cff3a1":"# Where summation of columns = 0 then churn =1  else 0\ntelecom['churn']= np.where(telecom[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9',\\\n                                    'vol_3g_mb_9']].sum(axis=1)== 0,1,0)","1e756358":"# Removing all features having \u2018 _9\u2019, etc. in their names\ntelecom.drop(telecom.filter(regex='_9|sep', axis = 1).columns, axis=1,inplace=True)","22304ad8":"## Churn Percentage\npd.DataFrame(round(telecom['churn'].value_counts(normalize=True)*100,2))","f297fb41":"telecom.shape","f0225186":"def redundant_feature(df):\n    redundant = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        count_max = counts.iloc[0]\n        if count_max \/ len(df) * 100 > 95:\n            redundant.append(i)\n    redundant = list(redundant)\n    return redundant\n\n","88d7937b":"print('Before dropping Redundant features: ',telecom.shape)\nredundant_features = redundant_feature(telecom)\ntelecom = telecom.drop(redundant_features, axis=1)\nprint('After dropping Redundant features: ',telecom.shape)","9244dbef":"#Function to impute NaN with 0\ndef imputeNaN(df,col_name):\n    for col in col_name:\n        df[col].fillna(0,inplace=True)\ncol_40= calnullpercentage(telecom)[calnullpercentage(telecom)['Percentage']>40].index\n#call function\nimputeNaN(telecom,col_40)","c1dba7d0":"# checking %age of null values\ncalnullpercentage(telecom)","f8c343b6":"pd.DataFrame((telecom[calnullpercentage(telecom).index]==0).sum()).head(10)","a854733b":"imput_col= list(set(calnullpercentage(telecom).index)-set(('date_of_last_rech_6',\\\n                                                      'date_of_last_rech_7','date_of_last_rech_8')))\nknn_imp =KNNImputer()\ntelecom[imput_col] = knn_imp.fit_transform(telecom[imput_col])","e6ddab31":"# checking %age of null values\ncalnullpercentage(telecom)","4e6816a1":"telecom.fillna(0,inplace=True)","e7b231a6":"# checking %age of null values\ncalnullpercentage(telecom)","1ec07597":"telecom.shape","631aef99":"# Checking missing value percentage if any\ncalnullpercentage(telecom) #no missing value","1d90b7c7":"telecom.head(3)","3d7d4e22":"# no duplicate mobile number\nlen(telecom['mobile_number'].unique())","12643af9":"#Dropping mobile number since it doesn't help in modelling and prediction \ntelecom.drop('mobile_number',inplace=True,axis=1)","c5848f22":"def showvalues(ax,m=None):\n    for p in ax.patches:\n        ax.annotate(\"%.1f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\\\n                    ha='center', va='center', fontsize=14, color='k', rotation=0, xytext=(0, 7),\\\n                    textcoords='offset points',fontweight='light',alpha=0.9) ","5b529112":"#### Correlation between numerical variables\nplt.figure(figsize=(20,18))\nax=sns.heatmap(telecom.corr())\nplt.setp(ax.get_xticklabels(), visible=False)\nplt.setp(ax.get_yticklabels(), visible=False)\nax.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False, labelbottom=False)","c74a5fa4":"# Function to plot distribution plot for months(6,7 and 8) for churn and non churn customers. \n# Also, plotting box plot and strip plot for the same.\ndef dist_box_plot(df,col1,col2,col3):    \n    fig, axes = plt.subplots(nrows=1, ncols=4,figsize=(20, 4))\n    ax = sns.distplot(df[df['churn']==1][col1], bins = 40, ax = axes[0], kde = False,\\\n                      color='#2980B9',hist_kws={\"alpha\": 1})\n    ax.set_title('Churn',fontweight='bold',size=20)\n    ax = sns.distplot(df[df['churn']==0][col1], bins = 40, ax = axes[1], kde = False,\\\n                     color='#E67E22',hist_kws={\"alpha\": 1})\n    ax.set_title('Non-Churn',fontweight='bold',size=20)\n    ax = sns.distplot(df[df['churn']==1][col2], bins = 40, ax = axes[2], kde = False,\\\n                     color='#2980B9',hist_kws={\"alpha\": 1})\n    ax.set_title('Churn',fontweight='bold',size=20)\n    ax = sns.distplot(df[df['churn']==0][col2], bins = 40, ax = axes[3], kde = False,\\\n                     color='#E67E22',hist_kws={\"alpha\": 1})\n    ax.set_title('Non-Churn',fontweight='bold',size=20)\n    fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(20, 4))\n    ax=sns.boxplot(x='churn', y=col1, data=df,ax=axes[0])\n    ax=sns.stripplot(x='churn', y=col1, data=df, jitter=True, edgecolor=\"gray\",ax=axes[0])\n    ax.yaxis.label.set_visible(False)\n    ax.set_title(col1,fontweight='bold',size=20)\n    ax=sns.boxplot(x='churn', y=col2, data=df,ax=axes[1])\n    ax=sns.stripplot(x='churn', y=col2, data=df, jitter=True, edgecolor=\"gray\",ax=axes[1])\n    ax.yaxis.label.set_visible(False)\n    ax.set_title(col2,fontweight='bold',size=20)\n    fig, axes = plt.subplots(nrows=1, ncols=3,figsize=(20, 4))\n    ax = sns.distplot(df[df['churn']==1][col3], bins = 40, ax = axes[0], kde = False,\\\n                     color='#2980B9',hist_kws={\"alpha\": 1})\n    ax.set_title('Churn',fontweight='bold',size=20)\n    ax = sns.distplot(df[df['churn']==0][col3],bins=40, ax = axes[1], kde = False,\\\n                     color='#E67E22',hist_kws={\"alpha\": 1})\n    ax.set_title('Non-Churn',fontweight='bold',size=20)\n    ax=sns.boxplot(x='churn', y=col3, data=df,ax=axes[2])\n    ax=sns.stripplot(x='churn', y=col3, data=df, jitter=True, edgecolor=\"gray\",ax=axes[2])\n    ax.yaxis.label.set_visible(False)\n    ax.set_title(col3,fontweight='bold',size=20)\n    plt.show()","5398c88b":"dist_box_plot(telecom,'arpu_6','arpu_7','arpu_8')","0b64a9f1":"dist_box_plot(telecom,'onnet_mou_6','onnet_mou_7','onnet_mou_8')","d951f681":"dist_box_plot(telecom,'offnet_mou_6','offnet_mou_7','offnet_mou_8')","60b60b6a":"dist_box_plot(telecom,'roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8')","6c26082a":"dist_box_plot(telecom,'loc_og_t2t_mou_6','loc_og_t2t_mou_7','loc_og_t2t_mou_8')","4a227246":"dist_box_plot(telecom,'loc_og_t2f_mou_6','loc_og_t2f_mou_7','loc_og_t2f_mou_8')","56b48162":"dist_box_plot(telecom,'std_og_t2t_mou_6','std_og_t2t_mou_7','std_og_t2t_mou_8')","01234905":"dist_box_plot(telecom,'std_og_mou_6','std_og_mou_7','std_og_mou_8')","104879d2":"dist_box_plot(telecom,'spl_og_mou_6','spl_og_mou_7','spl_og_mou_8')","627e847c":"dist_box_plot(telecom,'loc_ic_t2t_mou_6','loc_ic_t2t_mou_7','loc_ic_t2t_mou_8')","e42b9e16":"dist_box_plot(telecom,'loc_ic_t2f_mou_6','loc_ic_t2f_mou_7','loc_ic_t2f_mou_8')","ede89391":"dist_box_plot(telecom,'total_ic_mou_6','total_ic_mou_7','total_ic_mou_8')","339007a5":"dist_box_plot(telecom,'total_rech_amt_6','total_rech_amt_7','total_rech_amt_8')","39603ede":"dist_box_plot(telecom,'max_rech_amt_6','max_rech_amt_7','max_rech_amt_8')","327dbcd4":"dist_box_plot(telecom,'last_day_rch_amt_6','last_day_rch_amt_7','last_day_rch_amt_8')","650f07c5":"dist_box_plot(telecom,'max_rech_data_6','max_rech_data_7','max_rech_data_8')","a1fe82d0":"dist_box_plot(telecom,'av_rech_amt_data_6','av_rech_amt_data_7','av_rech_amt_data_8')","23bfb396":"dist_box_plot(telecom,'vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8')","045b6d2d":"dist_box_plot(telecom,'vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8')","6259f908":"dist_box_plot(telecom,'arpu_3g_6','arpu_3g_7','arpu_3g_8')","ab189e44":"dist_box_plot(telecom,'arpu_2g_6','arpu_2g_7','arpu_2g_8')","be5f46ad":"dist_box_plot(telecom,'jun_vbc_3g','jul_vbc_3g','aug_vbc_3g')","64fd0dff":"dist_box_plot(telecom,'tot_amt_6','tot_amt_7','tot_amt_8')","ca655f39":"dist_box_plot(telecom,'total_rech_num_6','total_rech_num_7','total_rech_num_8')","c3585b39":"plt.figure(figsize=(8,6))\nax=sns.distplot(telecom['aon'],color='#FFC30F',hist_kws={\"alpha\": 0.7})\nax.set_title('Age on Network',fontweight='bold',size=20)\nplt.show()","3197f8bb":"plt.figure(figsize=(10,6))\ncolor=['#1ABC9C','#E74C3C']\nfig, axes = plt.subplots(nrows=1, ncols=3,figsize=(20, 8))\ndf_temp= pd.DataFrame(telecom.groupby(['night_pck_user_6','churn']).count()['arpu_6']).reset_index()\nax= sns.barplot(x=df_temp['night_pck_user_6'],y=df_temp['arpu_6'],hue=df_temp['churn'],palette=color,ax=axes[0])\nshowvalues(ax)\nax.set_title('night_pck_user_6',fontweight='bold',size=20)\ndf_temp= pd.DataFrame(telecom.groupby(['night_pck_user_7','churn']).count()['arpu_6']).reset_index()\nax= sns.barplot(x=df_temp['night_pck_user_7'],y=df_temp['arpu_6'],hue=df_temp['churn'],palette=color,ax=axes[1])\nshowvalues(ax)\nax.set_title('night_pck_user_6',fontweight='bold',size=20)\ndf_temp= pd.DataFrame(telecom.groupby(['night_pck_user_8','churn']).count()['arpu_6']).reset_index()\nax= sns.barplot(x=df_temp['night_pck_user_8'],y=df_temp['arpu_6'],hue=df_temp['churn'],palette=color,ax=axes[2])\nshowvalues(ax)\nax.set_title('night_pck_user_8',fontweight='bold',size=20)\nplt.show()","a64c6c45":"plt.figure(figsize=(10,6))\ncolor=['#1ABC9C','#E74C3C']\nfig, axes = plt.subplots(nrows=1, ncols=3,figsize=(20, 8))\ndf_temp= pd.DataFrame(telecom.groupby(['fb_user_6','churn']).count()['arpu_6']).reset_index()\nax= sns.barplot(x=df_temp['fb_user_6'],y=df_temp['arpu_6'],hue=df_temp['churn'],palette=color,ax=axes[0])\nshowvalues(ax)\nax.set_title('fb_user_6',fontweight='bold',size=20)\ndf_temp= pd.DataFrame(telecom.groupby(['fb_user_7','churn']).count()['arpu_6']).reset_index()\nax= sns.barplot(x=df_temp['fb_user_7'],y=df_temp['arpu_6'],hue=df_temp['churn'],palette=color,ax=axes[1])\nshowvalues(ax)\nax.set_title('fb_user_7',fontweight='bold',size=20)\ndf_temp= pd.DataFrame(telecom.groupby(['fb_user_8','churn']).count()['arpu_6']).reset_index()\nax= sns.barplot(x=df_temp['fb_user_8'],y=df_temp['arpu_6'],hue=df_temp['churn'],palette=color,ax=axes[2])\nshowvalues(ax)\nax.set_title('fb_user_8',fontweight='bold',size=20)\nplt.show()","1b8263d6":"# Function to plot columns related to minutes of usage with month\ndef plot_Churn_Mou(colList,calltype):\n    fig, ax = plt.subplots(figsize=(7,4))\n    df=telecom.groupby(['churn'])[colList].mean().T\n    plt.plot(df)\n    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n    plt.legend(['Non-Churn', 'Churn'])\n    plt.title(\"Avg. \"+calltype+\" MOU  V\/S Month\", loc='left', fontsize=12, fontweight=0, color='#F1948A')\n    plt.xlabel(\"Month\")\n    plt.ylabel(\"Avg. \"+calltype+\" MOU\")","6e7821f8":"# Function to plot columns related to minutes of usage with month\ndef plot_Churn(data,col):\n    # per month churn vs Non-Churn\n    fig, ax = plt.subplots(figsize=(7,4))\n    colList=list(data.filter(regex=(col)).columns)\n    colList = colList[:3]\n    plt.plot(telecom.groupby('churn')[colList].mean().T)\n    ax.set_xticklabels(['Jun','Jul','Aug','Sep'])\n    plt.legend(['Non-Churn', 'Churn'])\n    plt.title( str(col) +\" V\/S Month\", loc='left', fontsize=12, fontweight=0, color='#F1948A')\n    plt.xlabel(\"Month\")\n    plt.ylabel(col)\n    plt.show()","24786a74":"ic_mou = ['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8']\nog_mou = ['total_og_mou_6','total_og_mou_7','total_og_mou_8']\nplot_Churn_Mou(ic_mou,'Incoming')\nplot_Churn_Mou(og_mou,'Outgoing')","44a2cace":"# Creating new features which are ratio of total outgoing call with total incoming calls minutes of usage.\nfor i in range(6,9):\n    telecom['tot_og_to_ic_mou_'+str(i)] = (telecom['total_og_mou_'+str(i)])\/(telecom['total_ic_mou_'+str(i)]+1)","c46c6605":"# Creating new features which are ratio of local outgoing call with local incoming calls minutes of usage\nfor i in range(6,9):\n    telecom['loc_og_to_ic_mou_'+str(i)] = (telecom['loc_og_mou_'+str(i)])\/(telecom['loc_ic_mou_'+str(i)]+1)","7d29f4ec":"# Creating new features which are ratio of roaming outgoing call with local incoming calls minutes of usage\nfor i in range(6,9):\n    telecom['roam_og_to_ic_mou_'+str(i)] = (telecom['roam_og_mou_'+str(i)])\/(telecom['roam_ic_mou_'+str(i)]+1)","320a6dd3":"# Creating new features which are ratio of Special outgoing call with local incoming calls minutes of usage\nfor i in range(6,9):\n    telecom['spl_og_to_ic_mou_'+str(i)] = (telecom['spl_og_mou_'+str(i)])\/(telecom['spl_ic_mou_'+str(i)]+1)","0897b5ac":"# Creating new features which are ratio of std outgoing call with local incoming calls minutes of usage\nfor i in range(6,9):\n    telecom['std_og_to_ic_mou_'+str(i)] = (telecom['std_og_mou_'+str(i)])\/(telecom['std_ic_mou_'+str(i)]+1)","b3bf43e7":"# Creating new features which are ratio of isd outgoing call with local incoming calls minutes of usage\nfor i in range(6,9):\n    telecom['isd_og_to_ic_mou_'+str(i)] = (telecom['isd_og_mou_'+str(i)])\/(telecom['isd_ic_mou_'+str(i)]+1)","687652f5":"plot_Churn(telecom,'tot_og_to_ic_mou')","9b88710f":"plot_Churn(telecom,'loc_og_to_ic_mou')","b4baa53f":"plot_Churn(telecom,'std_og_to_ic_mou')","66a79d3e":"plot_Churn(telecom,'tot_amt')","94c801db":"# Creating new features combining 2g and 3g\nfor i in range(6,9):\n    telecom['total_vol_'+str(i)] = (telecom['vol_2g_mb_'+str(i)])+(telecom['vol_3g_mb_'+str(i)])","bea51e26":"plot_Churn(telecom,'total_vol')","6bb5be3a":"# Creating new features combining average revenue per user from 2g and 3g\nfor i in range(6,9):\n    telecom['total_arpu_'+str(i)] = (telecom['arpu_3g_'+str(i)])+(telecom['arpu_3g_'+str(i)])","edaa1e2e":"date_cols=['date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8','date_of_last_rech_6','date_of_last_rech_7',\\\n         'date_of_last_rech_8']\nfor col in date_cols:\n    telecom[col] = pd.to_datetime(telecom[col],format='%m\/%d\/%Y',errors='coerce')","118c609d":"telecom['date_of_last_rech_dow_6'] = telecom['date_of_last_rech_6'].dt.dayofweek.astype(str)\ntelecom['date_of_last_rech_dow_7'] = telecom['date_of_last_rech_7'].dt.dayofweek.astype(str)\ntelecom['date_of_last_rech_dow_8'] = telecom['date_of_last_rech_8'].dt.dayofweek.astype(str)\ntelecom['date_of_last_rech_data_dow_6'] = telecom['date_of_last_rech_data_6'].dt.dayofweek.fillna(7).astype(int).astype(str)\ntelecom['date_of_last_rech_data_dow_7'] = telecom['date_of_last_rech_data_7'].dt.dayofweek.fillna(7).astype(int).astype(str)\ntelecom['date_of_last_rech_data_dow_8'] = telecom['date_of_last_rech_data_8'].dt.dayofweek.fillna(7).astype(int).astype(str)","2c5ddd0a":"# recent recharge date \ncols = ['date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8']\ntelecom['last_rech_date'] = telecom[cols].max(axis=1)\n# number of days from  the recent charge date till last date of Aug\/8th month\ntelecom['days_since_last_rech'] = np.floor(( pd.to_datetime('2014-08-31',\\\n                                                         format='%Y-%m-%d') - telecom['last_rech_date'] ).astype('timedelta64[D]'))\n# subtract it from 3 to add higher weightage for values present in all the columns. \n# len(cols) = 3,  means present in all columns, 0 means not present in any column\ntelecom['rech_weightage'] = len(cols) - (telecom[cols].isnull().sum(axis=1))\ntelecom.drop(['last_rech_date','date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8'], axis=1, inplace=True)\n\n# recent recharge date \ncols = ['date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8']\ntelecom['last_rech_data_date'] = telecom[cols].max(axis=1)\n# number of days from  the recent charge date till last date of Aug\/8th month\ntelecom['days_since_last_data_rech'] = np.floor(( pd.to_datetime('2014-08-31',\\\n                                                         format='%Y-%m-%d') - telecom['last_rech_data_date'] ).astype('timedelta64[D]'))\ntelecom['days_since_last_data_rech'] = telecom['days_since_last_data_rech'].fillna(0)\n# subtract it from 3 to add higher weightage for values present in all the columns. \n# len(cols) = 3,  means present in all columns, 0 means not present in any column\ntelecom['rech_data_weightage'] = len(cols) - (telecom[cols].isnull().sum(axis=1))\ntelecom.drop(['last_rech_data_date','date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8'], axis=1, inplace=True)","495b75b1":"catg =['night_pck_user_6','monthly_2g_6','sachet_2g_6','monthly_3g_6','sachet_3g_6','fb_user_6',\\\n       'night_pck_user_7','monthly_2g_7','sachet_2g_7','monthly_3g_7','sachet_3g_7','fb_user_7',\\\n       'date_of_last_rech_dow_6','date_of_last_rech_dow_7','date_of_last_rech_data_dow_6','date_of_last_rech_data_dow_7',\\\n       'date_of_last_rech_dow_8','date_of_last_rech_data_dow_8','night_pck_user_8','monthly_2g_8','sachet_2g_8',\\\n       'monthly_3g_8','sachet_3g_8','fb_user_8']\ncatg1 =['night_pck_user_6','fb_user_6','night_pck_user_7','fb_user_7',\\\n       'date_of_last_rech_dow_6','date_of_last_rech_dow_7','date_of_last_rech_data_dow_6','date_of_last_rech_data_dow_7',\\\n       'date_of_last_rech_dow_8','date_of_last_rech_data_dow_8','night_pck_user_8','monthly_2g_8','sachet_2g_8',\\\n       'monthly_3g_8','sachet_3g_8','fb_user_8']\nnum_col = list(set(telecom.columns).difference(set(catg)))","d623a9ed":"#filtering columns name by removing only last charcter of column name\ncol_list = telecom.select_dtypes(include=['float64',\\\n                                          'int64']).filter(regex='_6|_7').drop(catg[:12],axis=1).drop(['avg_amt_6_7','og_others_6'],axis=1).columns.str[:-2]\nfor idx, col in enumerate(col_list.unique()):\n    col_name = 'avg67_'+col # lets create the column name dynamically\n    col6 = col+'_6'\n    col7 = col+'_7'\n    telecom[col_name] = round((telecom[col6]  + telecom[col7])\/ 2,2)","d07d57c8":"## Deriving columns difference between 8 month column and avg of 6th and 7th column calculated above\ncol_list1 = telecom.select_dtypes(include=['float64',\\\n                                          'int64']).filter(regex='avg67_').columns\ncol_list2 = telecom.select_dtypes(include=['float64',\\\n                                          'int64']).filter(regex='_8').drop(['fb_user_8','night_pck_user_8'],axis=1).columns\nfor col1,col2 in zip(col_list1,col_list2):\n    col_name=col2[:-2]+'_avgdiff8'\n    telecom[col_name]=telecom[col2]-telecom[col1]\n    ","a6d8fb43":"#Dropping those columns since we already created features from it(dropping because of multicollinearity)\ncol_list = telecom.select_dtypes(include=['float64',\\\n                'int64']).filter(regex='_6|_7').drop(catg[:12],axis=1).drop(['avg_amt_6_7','og_others_6'],axis=1).columns\ntelecom.drop(col_list,axis=1,inplace=True)\nnum_col = list(set(telecom.columns).difference(set(catg)))","305b3814":"dummy_df=pd.get_dummies(telecom[catg],drop_first=True)\ntelecom=pd.concat([telecom,dummy_df],axis=1)\ntelecom= telecom.drop(catg,axis=1)","83acd95d":"# Lets find out if numerical predictor variables are largely skewed or not\ntelecom_numerical=telecom[num_col]\nskew_features = telecom_numerical.apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\nskewness = pd.DataFrame({'Skew' :high_skew})\npd.DataFrame(skew_features,columns=['Skewness']).head(10)","12c653da":"#Removing Skewness\nnum_col.remove('churn')\nqntle_trnsfrm=QuantileTransformer()\ntelecom[num_col]= qntle_trnsfrm.fit_transform(telecom[num_col])","2837f84c":"X=telecom.drop('churn',axis=1) #Independent\/predictor variable\ny=telecom['churn'] #output variables\nrandm_state= 42 # fixing random state to use same state wherever applicable","43e25d76":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randm_state)\nprint(X_train.shape)\nprint(X_test.shape)","13d5dd24":"smt = SMOTE()\nX_train,y_train = smt.fit_sample(X_train,y_train)\n","a1b3f767":"# scaling using StandardScaler scaler\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\nX=scaler.fit_transform(X)","9bf6abf9":"# We are going with PCA 0.9 variance or 90% information\npca=PCA(0.9)\ndf_train=pca.fit_transform(X_train)\ndf_train.shape","e2b35cc1":"# Lets see correlation between these new PCA  features\nplt.figure(figsize=(10,8))\nsns.heatmap(pd.DataFrame(df_train).corr())","3e057e19":"#intializing PCA with n_components=61\npca=PCA(n_components=70,random_state=randm_state,svd_solver='randomized')\nX_train_pca=pca.fit_transform(X_train)\nX_test_pca=pca.transform(X_test)","c407986c":"#transforming X also using PCA, since we will use it during cross validation\npca=PCA(n_components=70,random_state=42,svd_solver='randomized')\nX_pca=pca.fit_transform(X)","161c6871":"#Function to plot accuracy metrics\ndef model_metrics(actual, predicted):\n    confusion = confusion_matrix(actual, predicted)\n    TP=confusion[1,1] #True positives\n    TN= confusion[0,0] #True Negatives\n    FP=confusion[0,1] #False Positives\n    FN= confusion[1,0] #False Negatives\n    acc_score = round(accuracy_score(actual, predicted),2) #accuracy score\n    rcl_score=round(recall_score(actual, predicted),2) #recall score\n    roc_score = round(roc_auc_score(actual, predicted),2) # roc_auc score\n    fpr = round(FP\/float(TN+FP),2) #False Positive Ration\n    specificity = round(TN\/float(TN+FP),2) #False Positive Ration\n    metrics_df = pd.DataFrame(data=[[acc_score,roc_score,fpr,\\\n                                     specificity,rcl_score,TP,\\\n                                     TN,FP,FN,]],columns=['accuracy','roc_auc','fpr','specificity','recall_score',\\\n                                                          'true_positive','true_negative',\\\n                                                          'false_positive','false_negative'],index=['score'])\n    return metrics_df","ed4e1c06":"# Function to predict class labels based on model predicted probabilty and cutoff\/threshold for assigning labels\ndef predictChurnlabeloncutoff(model,X,y,threshold=0.5):\n    pred_probs = model.predict_proba(X)[:,1]\n    pred_df= pd.DataFrame({'churn':y, 'churn_Prob':pred_probs})\n    # Creating new column 'predicted' with 1 if Churn_Prob>threshold else 0\n    pred_df['predicted'] = pred_df.churn_Prob.map( lambda x: 1 if x > threshold else 0)\n    return pred_df","d7822cdd":"def optimal_cutoff(df):\n    # Let's create columns with different probability cutoffs \n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        df[i] = df.churn_Prob.map( lambda x: 1 if x > i else 0)\n    # Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','recall','specificity'])\n    from sklearn.metrics import confusion_matrix\n\n    # TP = confusion[1,1] # true positive \n    # TN = confusion[0,0] # true negatives\n    # FP = confusion[0,1] # false positives\n    # FN = confusion[1,0] # false negatives\n\n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        \n        cm1 = confusion_matrix(df.churn, df[i] )\n        total1=sum(sum(cm1))\n        accuracy = (cm1[0,0]+cm1[1,1])\/total1\n\n        speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    cutoff_df.plot.line(x='prob', y=['accuracy','recall','specificity'],figsize=(8,6))\n    plt.show()","e853a931":"gnb=GaussianNB()\ngnb.fit(X_train_pca,y_train)\ny_test_pred=gnb.predict(X_test_pca)\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","f6f3a05d":"log_reg= LogisticRegression(random_state=randm_state,class_weight='balanced')\nlog_reg.fit(X_train_pca,y_train) #fitting model\ny_test_pred=log_reg.predict(X_test_pca) #model prediction\nprint(classification_report(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","4fc569f0":"fold=StratifiedKFold(random_state=randm_state,shuffle=True,n_splits=5) #stratified kfold for cross validation","34c73dfc":"params={'penalty':['l1'],'C':list(np.power(10.0, np.arange(-2, 3))),'solver':('saga','liblinear'),\\\n                                          'class_weight':['balanced']}\n#we are using scoring metrics Recall\nlog_regcv=GridSearchCV(LogisticRegression(random_state=randm_state,max_iter=1000),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nlog_regcv.fit(X_train_pca,y_train)","cc3ec4dc":"print(log_regcv.best_params_)\nprint(log_regcv.best_score_)\nprint(log_regcv.best_estimator_)\n# test set prediction using tuned model\nlog_regcv=log_regcv.best_estimator_\n","760a1681":"log_regcv.fit(X_train_pca,y_train) #fitting best models\ndf_cutoff=predictChurnlabeloncutoff(log_regcv,X_test_pca,y_test)\noptimal_cutoff(df_cutoff)","eb4a0432":"threshold=0.25 # from the above graph(recall, accuracy and specificity have good score)\ny_test_pred= predictChurnlabeloncutoff(log_regcv,X_test_pca,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","1aee3225":"knn=KNeighborsClassifier()\nknn.fit(X_train_pca,y_train)\ny_test_pred = knn.predict(X_test_pca)\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","5b297cc2":"fold=StratifiedKFold(random_state=randm_state,shuffle=True,n_splits=3) #stratified kfold for cross validation\n#params={'n_neighbors': list(range(1,6)), 'p':[1,2],'weights':['uniform', 'distance']}\n# Setting params to tuned values, since it takes around 3hrs for 30 fits with knn\nparams={'n_neighbors':[3], 'p':[2]} #I ran gridsearchcv on above paramgerids with 30 fits and found these tuned params\n#we are using scoring metrics Recall\nknn_cv=GridSearchCV(KNeighborsClassifier(),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nknn_cv.fit(X_train_pca,y_train)","bd2e76ca":"print(knn_cv.best_params_)\nprint(knn_cv.best_score_)\nprint(knn_cv.best_estimator_)\n# test set prediction using tuned model\nknn_cv=knn_cv.best_estimator_ # best parameters value for knn","f5774e47":"knn_cv.fit(X_train_pca,y_train) #fitting best models\ndf_cutoff=predictChurnlabeloncutoff(knn_cv,X_test_pca,y_test)\noptimal_cutoff(df_cutoff)\n","fad57c81":"threshold=0.45 #from graph, at 0.45 we can have good score for recall(objective to maximize recall) with decent score for accuracy and specificity.\ny_test_pred= predictChurnlabeloncutoff(knn_cv,X_test_pca,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","cbdac78c":"rfc=RandomForestClassifier(random_state=randm_state)\nrfc.fit(X_train_pca,y_train)\ny_test_pred= rfc.predict(X_test_pca)\nmodel_metrics(y_test,y_test_pred)","f1549b7f":"fold=StratifiedKFold(random_state=randm_state,shuffle=True,n_splits=3)\nparams={'n_estimators': range(50,150,30)}\n#we are using scoring metrics Recall\nrfc_cv=GridSearchCV(RandomForestClassifier(random_state=randm_state,class_weight='balanced'),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nrfc_cv.fit(X_train_pca,y_train)","6b807346":"cv_results = pd.DataFrame(rfc_cv.cv_results_)\nplt.plot(cv_results['param_n_estimators'],cv_results['mean_test_score'],label='test')\nplt.plot(cv_results['param_n_estimators'],cv_results['mean_train_score'],label='train')\nplt.legend()\n","729db78a":"params={'criterion': [\"gini\", \"entropy\"]}\n#we are using scoring metrics Recall\nrfc_cv=GridSearchCV(RandomForestClassifier(random_state=randm_state,class_weight='balanced',n_estimators=80),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nrfc_cv.fit(X_train_pca,y_train)","5ea36422":"rfc_cv.best_params_","f30c3f40":"fold=StratifiedKFold(random_state=randm_state,shuffle=True,n_splits=3)\nparams={'max_features': [5, 10, 15, 20, 25]}\n#we are using scoring metrics Recall\nrfc_cv=GridSearchCV(RandomForestClassifier(random_state=randm_state,class_weight='balanced',\\\n                                           n_estimators=80,criterion='entropy'),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nrfc_cv.fit(X_train_pca,y_train)","8fb91ba0":"cv_results = pd.DataFrame(rfc_cv.cv_results_)\nplt.plot(cv_results['param_max_features'],cv_results['mean_test_score'],label='test')\nplt.plot(cv_results['param_max_features'],cv_results['mean_train_score'],label='train')\nplt.legend()\n","c3e22ae6":"params={'min_samples_leaf': range(50, 200, 50)}\n#we are using scoring metrics Recall\nrfc_cv=GridSearchCV(RandomForestClassifier(random_state=randm_state,class_weight='balanced',\\\n                                           n_estimators=80,criterion='entropy',max_features=15),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nrfc_cv.fit(X_train_pca,y_train)","004f77df":"print(rfc_cv.best_params_)\nprint(rfc_cv.best_score_)","86d2156b":"### Tunning minimum sample split","00ad3db2":"params={'min_samples_split': range(50, 200, 50)}\n#we are using scoring metrics Recall\nrfc_cv=GridSearchCV(RandomForestClassifier(random_state=randm_state,class_weight='balanced',\\\n                                           n_estimators=80,criterion='entropy',max_features=15,min_samples_leaf=50,\\\n                                          ),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nrfc_cv.fit(X_train_pca,y_train)","8a73af34":"cv_results = pd.DataFrame(rfc_cv.cv_results_)\nplt.plot(cv_results['param_min_samples_split'],cv_results['mean_test_score'],label='test')\nplt.plot(cv_results['param_min_samples_split'],cv_results['mean_train_score'],label='train')\nplt.legend()\n","a2c85d9b":"rfc=RandomForestClassifier(random_state=randm_state,class_weight='balanced',\\\n                                           n_estimators=80,criterion='entropy',max_features=15,min_samples_leaf=50,\\\n                                          min_samples_split=100)\nrfc.fit(X_train_pca,y_train)","b5e829fa":"#rfc.fit(X_train_pca,y_train) #fitting best models\ndf_cutoff=predictChurnlabeloncutoff(rfc,X_train_pca,y_train)\noptimal_cutoff(df_cutoff)\n","c566accd":"# We want recall to be better so going with 0.34, as you can see above from the at0.34 recall is slightly better than accuracy and specificty\nthreshold=0.34 #from graph, at 0.34 we can have good score for recall, accuracy and specificity.\ny_test_pred= predictChurnlabeloncutoff(rfc,X_test_pca,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","dbe23b6b":"svc= SVC(random_state=randm_state,class_weight='balanced')\nsvc.fit(X_train_pca,y_train) #fitting model\ny_test_pred=svc.predict(X_test_pca) #model prediction\nprint(classification_report(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","b0c722be":"fold=StratifiedKFold(random_state=randm_state,shuffle=True,n_splits=3)\nparams={'C':list(np.power(10.0, np.arange(-1, 2)))}\n#we are using scoring metrics Recall\nsvc_cv=GridSearchCV(SVC(random_state=randm_state,class_weight='balanced'),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nsvc_cv.fit(X_train_pca,y_train)","9a9b123d":"print(svc_cv.best_params_)\nprint(svc_cv.best_score_)","d88d0aad":"params={'kernel' :['poly', 'rbf']}\n#we are using scoring metrics Recall\nsvc_cv=GridSearchCV(SVC(random_state=randm_state,class_weight='balanced',C=10),param_grid=params,cv=fold,scoring='recall',\\\n                       verbose=1,return_train_score=True)\nsvc_cv.fit(X_train_pca,y_train)","a0f7c626":"svc= SVC(random_state=randm_state,class_weight='balanced',C=0.1,kernel='rbf',probability=True)\nsvc.fit(X_train_pca,y_train) #fitting model\ny_train_pred=svc.predict_proba(X_train_pca) #model prediction","a410df1d":"# We are going slightly towards recall score, reason mentioned at starting of data modelling.\nthreshold=0.4 #from graph, at 0.4 we can have good score for recall, accuracy and specificity.\ny_test_pred= predictChurnlabeloncutoff(svc,X_test_pca,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","c78a1444":"gbc=GradientBoostingClassifier()\ngbc.fit(X_train_pca,y_train)\ny_train_pred = gbc.predict_proba(X_train_pca)","ac79c657":"df_cutoff=predictChurnlabeloncutoff(gbc,X_train_pca,y_train)\noptimal_cutoff(df_cutoff)\n","f6552051":"# We are going slightly towards recall score, reason mentioned at starting of data modelling.\nthreshold=0.3 #from graph, at 0.3 we can have good score for recall, accuracy and specificity.\ny_test_pred= predictChurnlabeloncutoff(gbc,X_test_pca,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","087373c8":"xgb=XGBClassifier(random_state=randm_state ,max_depth = 3 , learning_rate=0.01,\\\n                  n_estimators=100,objective='binary:logistic')\nxgb.fit(X_train_pca,y_train)\ny_train_pred = xgb.predict_proba(X_train_pca)\n","a9c275e0":"df_cutoff=predictChurnlabeloncutoff(xgb,X_train_pca,y_train) #optimal cutoff\noptimal_cutoff(df_cutoff)\n","681cf585":"# We are going slightly towards recall score, reason mentioned at starting of data modelling.\nthreshold=0.42 #from graph, at 0.42 we can have good score for recall, accuracy and specificity\ny_test_pred= predictChurnlabeloncutoff(xgb,X_test_pca,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","613dff02":"X=telecom.drop('churn',axis=1) # Feature variables for new interpretable models\ny=telecom['churn'] \nprint(X.shape)","d2e13a73":"df_train, df_test = train_test_split(telecom, train_size = 0.7, test_size = 0.3, random_state = randm_state)\nprint(df_train.shape)\nprint(df_train.shape)","78d93c43":"y_train = df_train['churn']\nX_train = df_train.drop('churn',axis=1)\ny_test = df_test['churn']\nX_test = df_test.drop('churn',axis=1)\n","388e3a8d":"# Scaling\nscaler = StandardScaler()\ncolums = list(X_train.columns)\nX_train[colums] = scaler.fit_transform(X_train[colums])\nX_test[colums] = scaler.transform(X_test[colums])","1fab8a76":"log_reg=LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, penalty='l1',\n                   random_state=42, solver='liblinear')\n# Running RFE with the output number of the variable equal to 30\nrfe = RFE(log_reg,20)             # running RFE\nrfe = rfe.fit(X_train, y_train)\n","043d2b55":"pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)),columns=[['column_name','Include','feature_rank']])\n","9c42c484":"col = X_train.columns[rfe.support_]\nX_train_rfe = X_train[col]","a665a80d":"#statsmodel to find significant variables\nX_train_rfe_new = sm.add_constant(X_train_rfe)\nlog_reg_sm = sm.OLS(y_train,X_train_rfe_new).fit() \nprint(log_reg_sm.summary())","be7c9c20":"# lets drop avg67_loc_og_t2f_mou since it has high p value\nX_train_rfe_new=X_train_rfe.drop('avg67_loc_og_t2f_mou',axis=1)","30d16879":"vif = pd.DataFrame()\nX = X_train_rfe_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","eb353239":"#statsmodel to find significant variables\nX_train_rfe_new = sm.add_constant(X_train_rfe_new)\nlog_reg_sm = sm.OLS(y_train,X_train_rfe_new).fit() \nprint(log_reg_sm.summary())","96ce4a3a":"# lets drop vol_2g_mb_avgdiff8 since it has high p value\nX_train_rfe_new = X_train_rfe_new.drop('vol_2g_mb_avgdiff8',axis=1)\n ","a164fec1":"vif = pd.DataFrame()\nX = X_train_rfe_new.drop(['const'], axis=1)\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3ebad479":"# lets drop avg67_total_rech_amt since it has high VIF value\nX_train_rfe_new=X_train_rfe_new.drop('avg67_total_rech_amt',axis=1)","51a412da":"vif = pd.DataFrame()\nX = X_train_rfe_new.drop(['const'], axis=1)\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8921041c":"# lets drop av_rech_amt_data_8 since it has high VIF value\nX_train_rfe_new=X_train_rfe_new.drop('av_rech_amt_data_8',axis=1)","41d60391":"vif = pd.DataFrame()\nX = X_train_rfe_new.drop(['const'], axis=1)\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1588e1d3":"X_train=X_train[X.columns]\nX_test=X_test[X.columns]","797cad8a":"log_reg=LogisticRegression(C=0.01, class_weight='balanced', max_iter=1000, penalty='l1',\n                   random_state=42, solver='liblinear')\nlog_reg.fit(X_train,y_train)","cc15e8ab":"#optimal cutoff\ndf_cutoff=predictChurnlabeloncutoff(log_reg,X_train,y_train)\noptimal_cutoff(df_cutoff)\n","dbf0e404":"#As we can see cutoff=0.45 given good recall score as well as accuracy and specificity.\nthreshold=0.45 #from graph, at 0.45 we can have good score for recall(objective to maximize recall) with decent score for accuracy and specificity.\ny_test_pred= predictChurnlabeloncutoff(log_reg,X_test,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","dfd0ecba":"feat_imp=log_reg.coef_\nfeat_imp=np.abs(feat_imp)\nlen(X_train.columns)","800cca7b":"cmap = plt.get_cmap('Spectral')\ncolors = [cmap(i) for i in np.linspace(0, 1, 16)]\nplt.figure(figsize=(16,16))\nwedges, labels, autopct = plt.pie(feat_imp, labeldistance=1.02,labels=X_train.columns, autopct='%1.1f%%', shadow=False, colors=colors)\nfor lab in labels:\n    lab.set_fontsize(15)\nplt.rcParams['font.size'] = 12\nplt.rcParams['font.weight'] = 'bold'\ncentre_circle = plt.Circle((0,0),0.20,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis('equal')  \nplt.tight_layout()\nplt.title('Logistic Regression Feature Importance',fontweight='bold',size=20)\nplt.show()","50eb882a":"y_train = df_train['churn']\nX_train = df_train.drop('churn',axis=1)\ny_test = df_test['churn']\nX_test = df_test.drop('churn',axis=1)\n# Scaling\nscaler = StandardScaler()\ncolums = list(X_train.columns)\nX_train[colums] = scaler.fit_transform(X_train[colums])\nX_test[colums] = scaler.transform(X_test[colums])","c12ac3a8":"rfc=RandomForestClassifier(random_state=randm_state,class_weight='balanced',\\\n                                           n_estimators=80,criterion='entropy',max_features=15,min_samples_leaf=50,\\\n                                          min_samples_split=100)\nrfc.fit(X_train,y_train)","120bb5c1":"#optimal cutoff\ndf_cutoff=predictChurnlabeloncutoff(rfc,X_train,y_train)\noptimal_cutoff(df_cutoff)\n","4441f93f":"#As we can see cutoff=0.38 given good recall score as well as accuracy and specificity.\nthreshold=0.38 #from graph, at 0.38 we can have good score for recall(objective to maximize recall) with decent score for accuracy and specificity.\ny_test_pred= predictChurnlabeloncutoff(rfc,X_test,y_test,threshold).predicted\nprint('Classification Report:\\n')\nprint(classification_report(y_test,y_test_pred))\nprint('Confusion Matrix:\\n')\nprint(confusion_matrix(y_test,y_test_pred))\nmodel_metrics(y_test,y_test_pred)","0f14e579":"# Check the feature importance score for each feature\nfeat_imp_df = pd.DataFrame({'Feature':X_train.columns, 'Score':rfc.feature_importances_})\nfeat_imp_df = feat_imp_df.sort_values('Score', ascending=False).reset_index() # Order features by score\nfeat_imp_df.head(20)","8cac947e":"m=list(feat_imp_df.Feature[:16])\nn=list(feat_imp_df.Score[:16])","d231f016":"cmap = plt.get_cmap('Spectral')\ncolors = [cmap(i) for i in np.linspace(0, 1, 16)]\nplt.figure(figsize=(16,16))\nwedges1, labels1, autopct1 = plt.pie(list(feat_imp_df.Score[:15]), labeldistance=1.02,\\\n                                  labels=list(feat_imp_df.Feature[:15]), autopct='%1.1f%%', shadow=False, colors=colors)\nfor lab in labels:\n    lab.set_fontsize(15)\nplt.rcParams['font.size'] = 12\nplt.rcParams['font.weight'] = 'bold'\ncentre_circle = plt.Circle((0,0),0.20,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.axis('equal')  \nplt.tight_layout()\nplt.title('Random Forest Feature Importance',fontweight='bold',size=20)\nplt.show()","a64d41ed":"<b>`For each feature, it counts the values of that feature. If the most recurrent value of the feature is repeated almost in all the instances (*zeros \/ len(X) * 100 > 95*). Then it drops these features because their values are almost the same for all instances and will not help in learning process and those features are not useful in our prediction.`","dbb02139":"### Tuning n_estimators and criterion","d852ea9a":"### Scaling","28cfe329":"`As we can see VIF value is less than 2, we have taken care of multicollinearity issue between feature variables, Lets do prediction and modelling using these variables`","2a083d08":"### tot_amt (Total amount)","85daf18e":"### vol_3g_mb (3g data volume in Mb)","d7878a5e":"`Significant drop in total incoming calls and total outgoing calls for churn customers , however for non churn customer its increasing.`","bd18cd0c":"### vol_2g_mb (2g data volume in Mb)","6c204e3f":"# Random Forest(Default parameters)","f741f3bf":"`Similalry Offnet minutes of usage is also decreasing for churn customers in 8th month.As compared to 6th and 7th month , in 8th month there is no high minutes of usage as the gitter graph is condensed.`","33cf4bfa":"#### Function to show values on bar plot","595d6964":"### total_amount (Total amount)","e5ed1143":"# Data Visualisation and EDA","2405a0fb":"### total_rech_num (Total no of recharges)","b5470502":"## BaseLine Models (Without tuning hyperparameters, Running with default parameters)","90788723":"`Creating derived features: total_vol_data - Combining vol_2g_mb and vol_3g_mb. These features will combine the 2g data usage and 3g data usage and should be a better predictor of churn.`","ef3f9d07":"### month_vbc (Volume based cost - when no specific scheme is not purchased and paid as per usage)","b1f58c25":"### isd_og_mou  (Minutes of usage  on ISD outgoing calls)","aed46b4b":"### std_og_to_ic_mou( Std outgoing mou to incoming mou)","be42e9f5":"## arpu (Average Revenue per user)","62fa44bf":"`Function to impute NaN values where %age of missing values > 40%, Reason for taking cutoff 40% is beacuse for these columns we can replace NaN with 0(for example, fb_user_7, not used facebook(NaN),av_rech_amt_data_8, not done recharge(NaN) similarly for other columns.`","42a5e958":"`Top 10 feature variables for business which will be very helpful in business making decision and give additional benefits\/discount to customers who are likely to churn`\n- total_og_mou_8 (Total outgoing minutes of usage for 8th month)\n- total_ic_mou_8 (Total incoming minutes of usage for 8th month)\n- tot_amt_avgdiff8 (Total amount 6th and 7th month average difference from 8th month)\n- ot_og_to_ic_mou_8 (outgoing to incoming minutes of usage ratio for 8th month\n- last_day_rch_amt_8 (last dat recharge amount for 8th month)\n- tot_amt_8 (total amount for 8th month)\n- roam_og_to_ic_mou_8 (roaming outgoing to incoming minutes of usage for 8th month)\n- arpu_avgdiff8\t( avergae revenure per user 6th and 7th month average from 8th month)\n- arpu_8 ( avergae revenure per user for 8th month)\n- days_since_last_rech (days since last recharge)\n\n`From logistic regression using 16 feature variables I got:`\n- Recall Score= 86%\n- Accuracy = 86%\n- ROC AUC= 86%\n- Specificity= 86%","8dff59d5":"`Lets fill these data missing values with 0 since date is not available for these columns. We will handle these rows later in an efficient way`","fb0fad94":"`Ratio is getting dropped for churn customers`","de9a0abd":"`Looks like Max Recharge Amount is decreased in 8th month for Churn customers.`","426ebe42":"### loc_ic_t2f_mou  (Minutes of usage from Operator T to fixed lines of T on local incoming calls)","ccb40016":"`After dropping missing values will result in dropping 1838 or 6% rows`","df50cd53":"# Data Preparation","5fc0433c":"### Tuning max_features","64352f44":"`Conclusion of different tuned models:`\n- Naive Bayes baseline model with PCA and SMOTE\n    - Recall : 40%\n- Logistic Regression PCA and SMOTE\n    - Recall : 83%\n    - Accuracy : 82%\n- KNN with PCA and SMOTE   \n    - Recall : 86%\n    - Accuracy : 71%\n- Random Forest with PCA and SMOTE   \n    - Recall : 81%\n    - Accuracy : 81%\n- SVC with PCA and SMOTE   \n    - Recall : 68%\n- Gradient Boosting with PCA and SMOTE   \n    - Recall : 82%\n    - Accuracy : 82%\n- XGBoost with PCA and SMOTE   \n    - Recall : 88%\n    - Accuracy : 73%\n\n`XGboost classifie and K Nearest neighbours classifier has maximum recall respectively. We will impelement interpretable model later in this notebook. Since, PCA does transformation of original raw attrinutes to find new variables or Principal components which are orthogonal and uncorrelated to each other. Hence, these model features cannot be used for business.`","7fd621db":"`Tuning all hyperparametrs will take large amount of time beacuse of possible combination will for gridsearchcv will be very large. Hence, tuning 1 or 2 hyperparamters at a time.`","2fdfeb5f":"## *Business Problem Overview* \u00b6\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\nSo we need to analyse telecom industry data and predict high value customers who are at high risk of churn and identify main indicators of churn.\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.","d206d928":"`Significant drop in Total Minutes of usage on incoming calls for 8th month for churn customers. High outliers  present for  non churn customers.`","937ba06d":"# Naive Bayes Classifier","6697ea7c":"`Decrease in Volume based cost - when no specific scheme is not purchased and paid as per usage in 8th month for churn customers`","d3747824":"# SVM(default parameters)","7a601910":"# Logistic Regression (Default parameters)","638cef67":"# Random Forest(Tuned Hyperparameters)","34278d13":"## Feature Engineering","67df4e3f":"### arpu_3g (Avg Revenue per user from 3g data)","6a4f4658":"# Interpretable model for Business understanding: Logistic Regression","a2586d90":"`Local Incoming calls minutes of usage are reduced in 8th month. Huge outliers for Non churn customers`","54ffdb0c":"`ARPU is increasing from 6th to 8th month for churn customers who hasn't taken Night Pack service. And ARPU is getting decreased from 6th to 8th month for night pack users. Means moslty churn customers have stopped their night pack facility`","aeadf193":"#### Creating dummy variables for categorical variables","4817381d":"### last_day_rch_amt (Last day Reacharge Amount)","416c85c2":"## *Business objective*\u00b6\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the features\/data from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n### *Understanding Customer Behaviour During Churn*\u00b6\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n\nThe \u2018good\u2019 phase: In this phase, the customer is happy with the service and behaves as usual.\n\nThe \u2018action\u2019 phase: The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\nThe \u2018churn\u2019 phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1\/0 based on this phase, you discard all data corresponding to this phase.\n\nIn this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.","de75e4de":"# KNN (Tuned Hyperameters)","3985102c":"`Creating derived features: total_arpu - Combining arpu_2g and arpu_3g. These features will combine the average revenue per user from 2g and 3g data and should be a better predictor of churn.`","119a3bdf":"`For most of the customers Age on Network is around 800-900 days. There are outliers too as the maximum value is 4321.`","e53ceea3":"Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\n- total_ic_mou_9\n- total_og_mou_9\n- vol_2g_mb_9\n- vol_3g_mb_9\n\nAfter tagging churners, remove all the attributes corresponding to the churn phase (all attributes having \u2018 _9\u2019, etc. in their names).","4dda832a":"### loc_og_t2m_mou  (Minutes of usage from operator T to fixed line T on local outgoing calls)","b507b68a":"` I have taken  recharge amount `more than or equal to X`, where X is the 70th percentile of the average recharge amount in the first two months and getting 30k rows. If I use `more than(>)sign`, will get 29.9k rows, but going with problem statement.`","7585341e":"`going with 100 samples to take care of overfitting.`","97ac6ac9":"# KNN (Using Default paramters)","ea9ec914":"#### Dropping Redundant columns, since we have already created derived features from them and derived features reflects the same information.","8fd3670c":"`Creating derived features: roam_og_to_ic_mou_6, roam_og_to_ic_mou_7, roam_og_to_ic_mou_8 These features will hold the ratio (=roam_og_mou \/ roam_ic_mou) for each month. These features will combine the roaming calls, both incoming and outgoing informations and should be a better predictor of churn.`","717d9650":"` Significant decrease in minutes of usage within same operator on STD outgoing calls for the churn customers in 8th month .`","504e23a5":"`All principal components are uncorrelated with each other`","31acf717":"# <font color='black'>\n![telecom](https:\/\/www.pinclipart.com\/picdir\/big\/171-1716993_since-b2c-companies-will-at-times-provide-a.png)","244bad52":"# Gradient Boosting","0f3cbbf7":"### max_rech_amt (Max Reacharge Amount)","93950335":"` Less Minutes of usage on Special calls for churn customers in 8th month. There are some customers who are having high minutes of usage.`","9f56a4d8":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)<\/center><\/h2>\n","66492448":"`There are many techniques available. I am using ADASYN(Adaptive Synthetic) which is an improved version of SMOTE.\nWhat it does is same as SMOTE just with a minor improvement. After creating those sample it adds a random small values to the points thus making it more realistic. In other words instead of all the sample being linearly correlated to the parent they have a little more variance in them i.e they are bit scattered.`","4a6cd5a4":"`Average recharge amount data is decreased in 8th month for churn customers.`","50ab2dae":"`Building another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. A good choice to identify important variables is a logistic regression model and Random Forest classifier. In case of logistic regression, make sure to handle multi-collinearity.`","be78fd6e":"`Creating derived features: isd_og_to_ic_mou_6, isd_og_to_ic_mou_7, isd_og_to_ic_mou_8 These features will hold the ratio (=isd_og_mou \/ isd_ic_mou) for each month. These features will combine the isd calls, both incoming and outgoing informations and should be a better predictor of churn.`","9216feb7":"### Fb User","72b97898":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>Thank You:)<\/center><\/h2>\n","09cfaf0d":"`we can clearly see that Minutes of usage for all kind of calls within the same operator network is decreasing for churn customers.Also it looks like some of the customers are having high minutes of usage( outlier present)`","5c7fee8d":"`Only few customer are calling to Fixed line. Also mou for fixed lines is decreased for churn customers in 8th month.`","f58034a1":"`Significant drop in Avg Revenue per user from 3g data in 8th month for churn customers`","f20bc53b":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>If you found this notebook helpful , some upvotes would be very much appreciated - That will keep me motivated :)<\/center><\/h2>\n","018203ae":"### aon(Age on network - number of days the customer is using the operator T network)","b977436d":"`Total data volume decreases for churn customer decreases as we go from jun to Aug.`","8c37fdd6":"`Significant drop in average revenue in 8th month (churn customers)  for the users who are avaialing fb_user facility.However ARPU is increasing for churn users who arent avaialing fb_users facility.`","08588906":"# Importing Libraries","05e2b53f":"`As we can see from skewness values, predictor variables are highly skewed. We need to take care of skewness.`","0e3aa3ad":"` As we can see average revenue per user is decreasing for churn customers in 8th month.Also there are lots of outlier exists in revenue as some customers might using higher data and recharging frequently.`","24b051f7":"### SVM(Tuned Hyperparameters)","d5009789":"`Last day Reacharge decreased in in 8th month for churn customers.`","a54f794c":"### av_rech_amt_data (Average recharge amount data)","692cd45d":"### Applying PCA","1f43948f":"`Creating derived features: loc_og_to_ic_mou_6, loc_og_to_ic_mou_7, loc_og_to_ic_mou_8 These features will hold the ratio (=loc_ogmou \/ loc_icmou) for each month. These features will combine the local calls, both incoming and outgoing informations and should be a better predictor of churn.`","68394fef":"## *About dataset:*\nDataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively.","2bb32d87":"`Creating derived features: std_og_to_ic_mou_6, std_og_to_ic_mou_7, std_og_to_ic_mou_8 These features will hold the ratio (=std_og_mou \/ std_ic_mou) for each month. These features will combine the std calls, both incoming and outgoing informations and should be a better predictor of churn.`","cf93c641":"### loc_ic_t2t_mou  (Minutes of usage within same operator on local incoming calls)","23d21f8b":"`Similalry significant drop in Avg Revenue per user for 2g data for churn customers.`","44e62d80":"`STD outgoing calls has been decreased for 8th month churn customers.`","e6eb389b":"### Note: We want to predict customers who are likely to churn(positive class). Hence, we want to maximize(Recall\/Senstivity). Also, we don't want to drop accuracy or roc_auc value much while maximising recall.","a5f6cd20":"# Tuned SVC","61e62028":"# Random Forest Interpretable model ","7fb2a72d":"### roam_ic_mou (Minutes of usage on roaming incoming calls)","3aa0e246":"##  Handling Skewness","ab1f078a":"`As we can see clearly very less usage of Minutes of usage from operator T to fixed line T on local outgoing calls, which means customers prefer calling mobile.Also there is no outliers present for 8th month churn customer graph which shows decrease in minutes of usage.`","1c21190e":"` No of features =15 based on above plot`","caa8b1dc":"### Tunning minimum sample leaf","bb14e05f":"### arpu_2g (Avg Revenue per user from 2g data)","de0a143b":"### loc_og_t2t_mou (Minutes of usage within same operator on local outgoing calls)","33b14590":"# Data Modelling ","4b56300e":"`There are many techniques available. I am using SMOTE.`","f51a1b70":"### Night pack user","3edc1136":"`As we can see both logistic regression and random forest classifier provides us similar predictor variables for making business decision to decrease or stop customer from churning who are likely to churn. Telcom company can introduce promotional offers to those customers who are likely to churn. Also, we have implemented all our models for high value customers. Now, Business people can make decision on those most important preidctor variables. Since, we have use different models for predicting these feature variable and difference techniques for feature selection we are getting similar predicting variable but order is slightly different. Hence we recommend  Telecom company to consider these feature variables which are strong indicators to manage customer churn.`","1f27ca5e":"`As compard to other parameters it looks like customers uses less services during roaming. Also gitter graph for 8th month (churn customers) shows slight decrease in mou.`","dc406e47":"# Tuned XGBoost Model (Tuned)","2e4b8d46":"### max_rech_data (Max Reachrge Data)","c548eed1":"#### Creating new features which will be average  of all columns for 6th, 7th ","ec73cc22":"`Total recharge amount distribution is getting increased  from 6 to 7th month and then getting decrease in 8th month for Churn customers.`","9470862c":"`2g data volume in Mb decreased in 8th month for churn customers.`","2a341434":"`Huge decrease in total amount for churn customers .There are lots of outliers in Total amount ,churn customers count increased from 6th to 8th month and amount is decreased.`","7c7a186a":"` approximately 92% customers not churned and 8% customers got churned. Also, we can see class imbalance is there and we will deal with it later` ","45fe01bb":"### onnnet_mou (Minutes of usage for all kind of calls within the same operator network)","ca02beaa":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:tomato' role=\"tab\" aria-controls=\"home\"><center>Telecom Churn Prediction Indian and Southeast Asian market<\/center><\/h2>\n","3852af8c":"`Out of 226 columns 166 have null values`","37640b9a":"`As we can see from the graph, correlation is present between features. We will take care of correlated features later using techniques like PCA, t-SNE or any  other suitable technique for this problem`","04d77a0f":"` As we can see from above missing value dataframe and value count == 0, large percentage of values are zero in missing value columns. I can impute most missing value column having NaN value with 0 if I assume that they have not use local incoming service, special outgoing service that is why these columns have NaN values. But this assumption doesn't helping much beacuse most values in these columns have 0 and it infers the same thing. So Imputing missing values for columns mentioned above.`","2fdb2538":"### std_og_t2t_mou  (Minutes of usage within same operator on STD outgoing calls)","62e79cce":"### total_ic_mou  (Total Minutes of usage on incoming calls)","7c1ebfe1":"`Top 10 features for Business should consider while predicting customers who are likely to churn and can make business decision on those. Below are the features importances in their decreasing order:`\n- total_ic_mou_8 (total incoming minutes of usage for 8th month)\n- days_since_last_rech (days since last recharge)\n- roam_og_to_ic_mou_8(roaming outgoing to incoming ratio of minute of usage for 8th month)\n- total_vol_8 (total volume of data usage (2g + 3g) for 8th month\n- last_day_rch_amt_8 (last day recharge amount for 8th month)\n- avg67_arpu( avergae revenure per user average for 6th and 7th month)\n- tot_og_to_ic_mou_8 (total outgoing to incoming minutes of usage for 8th month)\n- total_ic_mou_avgdiff8 (total incoming minutes of usage average 6th and 7th month difference from 8th month)\n- aon (age on network)\n- avg67_max_rech_amt(average of 6th and 7th month maximum recaherge amount)\n\n`From logistic regression using 16 feature variables I got:`\n- Recall Score= 87%\n- Accuracy = 85%\n- ROC AUC= 85%\n- Specificity= 84%","00c7dd3b":"`Creating derived features: og_to_ic_mou_6, og_to_ic_mou_7, og_to_ic_mou_8 These features will hold the ratio (=total_ogmou \/ total_icmou) for each month. These features will combine both incoming and outgoing informations and should be a better predictor of churn.`","146de678":"## Categorical columns","2c1f7781":"# Logistic Regression( Tuned Hyperameters)","8df68adb":"`Creating derived features: spl_og_to_ic_mou_6, spl_og_to_ic_mou_7, spl_og_to_ic_mou_8 These features will hold the ratio (=spl_og_mou \/ spl_ic_mou) for each month. These features will combine the Special calls, both incoming and outgoing informations and should be a better predictor of churn.`","024966e0":"# Tuned Random Forest ","a814f0c3":"### loc_og_to_ic_mou( Local outgoing mou to incoming mou)","82ba2f56":"## Identifying CHURN CUSTOMERS ","0b545d8d":"### spl_og_mou  (Minutes of usage  on Special calls)","c3683755":"`Taking 80 as n_estimators.`","9814d2d2":"`As the ratio of outgoing to incoming seesm to be getting dropped for churn customer , we can say incoming calls were less in Jun and singificantly increases which cause the ratio to drop.`","cf08133e":"`Max recharge data reduced for 8th month churn customers.It looks like many customers are using high data as their huge outliers.`","06ab0c46":"`local outgoing call with in same operator has decreased in 8th month for churn customers.`","00b1dcd8":"`We can see same trend here as 3g data volume in Mb usage decreased in 8th month (for churn customers)`","65416456":"## Filter high-value customers\nWe need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).","3890f645":"### tot_og_to_ic_mou( Total outgoing mou to incoming mou)","9822681c":"### total_rech_amt (Total Reacharge Amount)","52c0a2e2":"`Total amount spent tends to decrease for the churn customers.`","c95b5906":"# offnet_mou (Minutes of usage for All kind of calls outside the operator T network)","e05cc995":"## Handling Imbalance Dataset"}}