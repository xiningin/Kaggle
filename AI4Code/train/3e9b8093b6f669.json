{"cell_type":{"f7ae981b":"code","e21ea456":"code","f00d7487":"code","93e975bf":"code","f26d7ec5":"code","ac6bb46e":"code","40ab8135":"code","984edb3b":"code","2b465700":"code","d5bb9100":"code","daafb92f":"code","680c3b21":"code","33d532ae":"code","5195cfae":"code","dc3d138c":"code","d67c51f2":"code","32989023":"code","eeb12bf0":"markdown","aa7a2c70":"markdown","c3094c6d":"markdown","9559ae5f":"markdown","ed932102":"markdown","ae35cb82":"markdown","90c3ac16":"markdown","53da6aa4":"markdown","9626965b":"markdown","9e8cc055":"markdown","50db6a14":"markdown","e7696f99":"markdown","ef98d287":"markdown","f105ec13":"markdown","02dc87c4":"markdown","9899c260":"markdown","5404d15b":"markdown"},"source":{"f7ae981b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler","e21ea456":"data = pd.read_csv(\"..\/input\/weight-height\/weight-height.csv\")\ndata.head()","f00d7487":"data.shape","93e975bf":"#considering only 100 points for easy analysis\ndata_2 = data.drop('Gender', axis = 1)[:100]","f26d7ec5":"data_2.head()","ac6bb46e":"sns.scatterplot(x = data_2['Weight'], y = data_2['Height']);","40ab8135":"scaler = StandardScaler()\nscaled_data = scaler.fit_transform(data_2)","984edb3b":"y_ = scaled_data.T[0]\nx_ = scaled_data.T[1]\nX = np.linspace(-2, 3, 100)\nfig, ax = plt.subplots(3, 3, figsize=(20, 20))\nfor i in range (0, 3):\n    for j in range (0, 3):\n        Y = j*X+i\n        sns.scatterplot(ax=ax[i][j], x=x_, y=y_);\n        sns.lineplot(ax=ax[i][j], x=X, y=Y, color='red');\n        ax[i][j].set_title(f\"solpe:{j}  Interscept:{i}\")\n        ax[i][0].set_ylabel( \"Height\")\n        ax[2][j].set_xlabel( \"Weight\")\n    ","2b465700":"Y = 2*X+0\nsns.scatterplot(x=x_, y=y_);\nsns.lineplot(x=X, y=Y, color='red');\nplt.xlabel(\"Weight\", fontsize=16);\nplt.ylabel(\"Height\", fontsize=16);","d5bb9100":"fig = plt.subplots(figsize=(15, 10))\nsns.scatterplot(x=x_, y=y_);\nsns.lineplot(x=X, y=Y, color='red');\nx1 = -0.1989432\ny1 = 2 \nx2 = 0.50000164\ny2 = -1.02679815\nplt.plot(x1, y1, c='black',  marker='.', markersize=20);\nplt.plot(x2, y2, c='black',  marker='.', markersize=20);\n\nplt.vlines(x2, -4, y2, linestyle=\"dashed\", colors='black');\nplt.hlines(y2, -2, x2, linestyle=\"dashed\", colors='black');\n\nplt.vlines(x1, -4, y1, linestyle=\"dashed\", colors='black');\nplt.hlines(y1, -2, x1, linestyle=\"dashed\", colors='black');\n\ny_1hat = 2*x1+0\ny_2hat = 2*x2+0\n\nplt.vlines(x2, y2, y_2hat, linestyle=\"dashed\", colors='green');\nplt.hlines(y_2hat, -2, x2, linestyle=\"dashed\", colors='green');\nplt.plot(x2, y_2hat,  marker='.', markersize=20, c='green');\nplt.annotate(\"   Actual value $\\ y_2$\", (x2, y2), weight=\"bold\")\nplt.annotate(\"   Predicted value $ \\hat y_2$\", (x2, y_2hat), weight=\"bold\")\nplt.annotate(\"Error\", (x2,y_2hat), textcoords=\"offset points\", xytext=(20,-30), ha='center', weight=\"bold\")\n\nplt.hlines(y_1hat, -2, x1, linestyle=\"dashed\", colors='green');\nplt.plot(x1, y_1hat, c='green', marker='.', markersize=20);\nplt.xlabel(\"Weight\", fontsize=16);\nplt.ylabel(\"Height\", fontsize=16);\nplt.annotate(\"   Actual value$\\ y_1$\", (x1, y1),weight=\"bold\")\nplt.annotate(\"   Predicted value $ \\hat y_1$\", (x1, y_1hat), weight=\"bold\")\nplt.annotate(\"Error\", (x1,y_1hat), textcoords=\"offset points\", xytext=(-20,40), ha='center', weight=\"bold\")","daafb92f":"w = np.linspace(-10, 10, 100)\nw_o = np.linspace(-10, 10, 100)\nw, w_o = np.meshgrid(w, w_o)\nerror = (y_ - (np.dot(w.T,x_) + w_o))**2\nerror = scaler.fit_transform(error)","680c3b21":"fig = plt.figure(figsize=(8, 8))\nax = fig.add_subplot(111, projection='3d')\nax.scatter3D(w, w_o, error)\nplt.xlabel(\"slope $ \\omega$\", fontsize=14);\nplt.ylabel(\"Intercept $ \\omega_o$\", fontsize=14);\nplt.show()","33d532ae":"w, w_o = 2, 3","5195cfae":"x, y = x_, y_ \nLr = 0.00001\nslope = []\nintercept = []\nerror = []\nj = []\ni = 0\nwhile i < 4000:\n    j.append(i)\n    slope.append(w);\n    intercept.append(w_o)\n    e = np.sum((y - (w*x + w_o))**2)\n    error.append(e)\n    dw = -2*np.sum(x*(y - (w*x+w_o)))\n    dw_o = -2*np.sum((y - (w*x+w_o)))\n    w = w - dw*Lr\n    w_o = w_o - dw_o*Lr\n    i = i+1","dc3d138c":"sns.lineplot(x = np.array(j), y = np.array(error));\nplt.xlabel(\"Iterations\",fontsize=15);\nplt.ylabel(\"Error\", fontsize=15);","d67c51f2":"w, w_o","32989023":"X = np.linspace(-3, 3, 100)\nY = w*X+w_o\nsns.scatterplot(x=x, y=y);\nsns.lineplot(x=X, y=Y, color='red');\nplt.xlabel(\"Weight\", fontsize=16);\nplt.ylabel(\"Height\", fontsize=16);","eeb12bf0":"now of task is to somehow reduce the total error \n\n$ E_T = \\sum_{n=1}^{N}  (y_n - \\hat y_n) ^2 $\n\nwe know that the equation of a line is\n\n$ \\hat y =  \\omega * x +  \\omega_o $\n\ntherefore our error becomes\n\n$ E_T = \\sum_{n=1}^{N}  (y_n -  (\\omega * x_n +  \\omega_o)) ^2 $\n\nnow our optimization problem reduces to selecting $ \\omega $ and $ \\omega_o $ such that our total error becomes minimum","aa7a2c70":"as we can see from the above plot that the error in our prediction is \n\n$ E_1 = y_1 - \\hat y_2$  and $E_2 = y_1 - \\hat y_2 $  \n\nto avoid dealing with Negative values we can square the error without any change in the outcome, therefore error becomes\n\n$ E_1 = (y_1 - \\hat y_2) ^2$  and $E_2 = (y_1 - \\hat y_2) ^2 $  \n\nthe same arugument can be extended to all the points which don't lie on our best fit line\n\n$ E_3 = (y_3 - \\hat y_3) ^2$ , $E_4 = (y_4 - \\hat y_4) ^2 .....$  \n\nwe can obtain total error by adding all the errors\n\n$ E_T = \\sum_{n=1}^{N}  (y_n - \\hat y_n) ^2 $\n","c3094c6d":"as we can see there are a lot of point which does not lie on the line, these are errors. for simplicity let's take any two such point $ x_1 $ and $ x_2 $ for these point according to our best fit line the output should have been $ \\hat y_1 $ and $ \\hat y_2  $ but as we can see actual height value is $ y_1 $ and $ y_2 $ ","9559ae5f":"Gradient Descent is an optimization algorithm. we will use it to find optimal slope and intercept\n\nOptization problem : $ E_t = \\sum_{n=1}^{N}  (y_n -  (\\omega * x_n +  \\omega_o)) ^2 $ \n\nTo find its minima we that the partial derivate of the error function wrt $\\omega $ and $ \\omega_o$\n\n$ \\Large\\frac{\\partial E_t}{\\partial \\omega}$ = $ -2 *  \\sum_{n=1}^{N} x_n * (y_n -  (\\omega * x_n +  \\omega_o)) $\n\n$ \\Large\\frac{\\partial E_t}{\\partial \\omega_o}$ = $ -2 *  \\sum_{n=1}^{N} (y_n -  (\\omega * x_n +  \\omega_o)) $\n\nthis is called **Gradient**\n\nNow we try to find the value of $\\omega $ and $ \\omega_o$ for which this gradient becomes zero or tends to zero and for this we will start with a random initial value of $\\omega $ and $ \\omega_o$ and then try to descend to the lowest point of this gradient","ed932102":"# Error\n\nnow lets choose the line with $ \\omega = 2 $ and $ \\omega_o = 1 $","ae35cb82":"# Optimization Problem","90c3ac16":"# Gradient Descent ","53da6aa4":"Let's try to plot the line by choosing few values of $\\omega $ and $\\omega_o$ ","9626965b":"to descent the gradient we try changing $ \\omega $ and $ \\omega_o$ simultaneously to arrive at a value for which our error is minimum. For this we choosing a learning rate $Lr$ which will change the values of $ \\omega $ and $ \\omega_o$.\n\nFor each iteration we calculate the $ \\large\\frac{\\partial E_t}{\\partial \\omega}$ and $ \\large\\frac{\\partial E_t}{\\partial \\omega_o}$, then to get new\n$ \\omega $ and $ \\omega_o$ we use the formula\n\n$ \\omega' $  = $ \\omega $  - $ \\large\\frac{\\partial E_t}{\\partial \\omega}$ * $Lr$\n\n$ \\omega_o' $ = $ \\omega_o$ - $ \\large\\frac{\\partial E_t}{\\partial \\omega_o}$ * $Lr$","9e8cc055":"Linear regression is used for finding linear relationship between target and one or more predictors.","50db6a14":"let's consider height as our target and weight as out feature for now.","e7696f99":"Let's us plot and see how the error changes with different values of slope $ \\omega$ and intercept $ \\omega_o$","ef98d287":"let's look at our data first","f105ec13":"# Result\nOur best Fit Line has $ \\omega $ = 0.7887090849755773 and $ \\omega_o$ =  0.0009983582341356702","02dc87c4":"# Linear Regression Using Gradient Descent\n","9899c260":"# Relation Between Features\n\nour object here is to find a linear relationship between our target - Height and out feature - weight. \nThe relation ship can be expressed as\n\n$height$ = $\\omega \\ * weight $ + $\\omega_o$ \n\nor\n\n$y$ = $\\omega * x$ + $\\omega_o$ \n\nwhich is same as the equation of a line, there we need to find a line that best fits the data by selecting different values of $\\omega $ (slope) and $\\omega_o$ (interscept)\n","5404d15b":"now let's see how many iteration we need to minimize the error"}}