{"cell_type":{"a3d8714b":"code","0da7f710":"code","aad0961f":"code","9d8a6478":"code","5f3acdb6":"code","3c4e94de":"code","7e2feee7":"code","bce439e7":"code","c01ee9d6":"code","74f21b56":"code","94d9cbed":"code","b3203623":"code","a69633b1":"code","606b6655":"code","1a508d1b":"code","da67388b":"code","6d5baa8e":"code","b94637a4":"code","64c07178":"code","b5c31e3f":"code","6e914d96":"code","dc315fd6":"markdown","665ebdc4":"markdown","0ec1e78a":"markdown"},"source":{"a3d8714b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom tensorflow.keras import preprocessing\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing","0da7f710":"# Import the data\ndf = pd.read_csv('..\/input\/language-detection-dataset\/languages.csv')\ndf.head()","aad0961f":"# Get all unique languages values\nprint(df.groupby('language').nunique())\nprint(f\"Total Length of dataset: {len(df)}\")","9d8a6478":"# Encode language column\n# le = preprocessing.LabelEncoder()\n# le.fit(df['language'])\n# df['language'] = le.transform(df['language'])\n# le.classes_\n\ndef onehot_encode(df, columns, prefixes):\n    df = df.copy()\n    for column, prefix in zip(columns, prefixes):\n        dummies = pd.get_dummies(df[column], prefix=prefix)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n\ndf = onehot_encode(\n    df,\n    columns=['language'],\n    prefixes=['lan']\n)","5f3acdb6":"df.head()","3c4e94de":"y = df.drop('text', axis=1)\nX = df['text']","7e2feee7":"# Split data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=17)","bce439e7":"# Turn pandas dataframe into TensorFlow Dataset\nraw_train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\nraw_test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))","c01ee9d6":"# Create batches\nbatch_size = 32\nraw_train_dataset = raw_train_dataset.batch(batch_size)\nraw_test_dataset = raw_test_dataset.batch(batch_size)","74f21b56":"# Print three labels as example\nfor text_batch, label_batch in raw_train_dataset.take(1):\n    for i in range(3): \n        print(\"Review:\", text_batch.numpy()[i])\n        print(\"Label:\", label_batch.numpy()[i])","94d9cbed":"# Get number of unique words in entire dataset\nfrom collections import Counter\nresults = Counter()\ndf['text'].str.split().apply(results.update) # Very computer intensive method\nlen(results) # 104611","b3203623":"max_features = 60000 # Total words to vectorize\nsequence_length = 20 # The length of a sentence\n\nvectorize_layer = TextVectorization(\n    max_tokens=max_features,\n    output_mode='int',\n    output_sequence_length=sequence_length)\n\n# Adapt to our text\nvectorize_layer.adapt([t for t in df['text']])","a69633b1":"# Function to vectorize text\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return tf.dtypes.cast(vectorize_layer(text), tf.float32)","606b6655":"# Retrieve a batch from the dataset\ntext_batch, label_batch = next(iter(raw_train_dataset))\nfirst_review, first_label = text_batch[0], label_batch[0]\nprint(\"Review:\", first_review)\nprint(\"Label:\", first_label)\nprint(\"Vectorized review:\", vectorize_text(first_review))","1a508d1b":"# Apply the TextVectorization step to the train and test dataset\ntrain_ds = raw_train_dataset.map(lambda x,y: (vectorize_text(x), y))\ntest_ds = raw_test_dataset.map(lambda x ,y: (vectorize_text(x) , y))","da67388b":"# Performance measures\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)","6d5baa8e":"tf.keras.backend.clear_session()\n\nembedding_dim = 16\n\nmodel = tf.keras.Sequential([\n    layers.Embedding(max_features + 1, embedding_dim, input_length=4),   \n    layers.GlobalAveragePooling1D(),\n    layers.Dense(128, activation=tf.nn.relu), \n    layers.Dense(64, activation=tf.nn.relu), \n    layers.Dense(32, activation=tf.nn.relu),\n    layers.Dense(4, activation=tf.nn.softmax),    \n])\n\nmodel.summary()","b94637a4":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","64c07178":"epochs = 1\nhistory = model.fit(\n    train_ds,\n    epochs=epochs\n)","b5c31e3f":"loss, accuracy = model.evaluate(test_ds)\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","6e914d96":"to_predict = [\"Servus was hei\u00dft du?\"] # [0,0,1,0] (German)\n# Vectorize text before giving it to the model\nto_predict = vectorize_layer(to_predict)\nprediction = model.predict(to_predict)\n\nclasses = [\"English\", \"French\", \"German\", \"Spanish\"]\n\nhighest_prediction = tf.math.argmax(prediction, 1).numpy()\n\nprint(classes[highest_prediction[0]])\nprint(f\"Certainty: {prediction[0][highest_prediction][0] * 100}%\")\n","dc315fd6":"## Predict","665ebdc4":"## Prepare the dataset for training","0ec1e78a":"## Create the actual model"}}