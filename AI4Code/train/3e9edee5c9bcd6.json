{"cell_type":{"b241de91":"code","70257405":"code","e2163d0b":"code","f75a25cc":"code","fb71e6a6":"code","e7df1778":"code","8b2429c9":"code","0dc885c9":"code","bbd8112a":"code","57e0428d":"markdown","d71f8ea5":"markdown","ba7db6b8":"markdown","1e5fe27c":"markdown","5133e3c7":"markdown","f2fb95b1":"markdown","84c5b616":"markdown","e4168a98":"markdown","02c17006":"markdown"},"source":{"b241de91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntargets = train['target']\ntrain = train.drop(columns=['id', 'target'])\nids = test['id']\ntest = test.drop(columns=['id'])","70257405":"from sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(test.values)\ntrans_data = transformer.transform(train.values)\ntrans_test = transformer.transform(test.values)\n\ntrans_data_df = pd.DataFrame(trans_data, columns=train.columns)\ntrans_test_df = pd.DataFrame(trans_test, columns=test.columns)","e2163d0b":"from sklearn.cluster import KMeans, DBSCAN, MeanShift\nfrom sklearn.mixture import GaussianMixture\ncenter_point_0 = trans_data_df[targets == 0].mean()\n#print(center_point_0)\ncenter_point_1 = trans_data_df[targets == 1].mean()\nstart = np.array([center_point_0, center_point_1])\nkmeans = KMeans(n_clusters=2, init=start).fit(test.values)\ng_mix = GaussianMixture(n_components=2).fit(test.values)\n#db_scan = DBSCAN(eps=75, min_samples=10).fit(test.values)\n","f75a25cc":"predicts = kmeans.predict(train.values)\ng_predicts= g_mix.predict(train.values)\ncount= 0\nfor i in range(len(predicts)):\n    if targets[i] == predicts[i]:\n        count+=1\nprint(\"K-Means Cluster Matches Target : \", str(count), \" times out of 250\")\ndata = pd.DataFrame(trans_data.copy(), columns=train.columns)\ndata['k_cluster'] = predicts\ndata['g_mix'] = g_predicts\nt_data = pd.DataFrame(trans_test.copy(), columns=test.columns)\nt_data['k_cluster'] = kmeans.predict(test.values)\nt_data['g_mix'] = g_mix.predict(test.values)","fb71e6a6":"count=0\nfor i in range(len(g_predicts)):\n    if targets[i] == g_predicts[i]:\n        count+=1\nprint(\"For Gaussian-Mixture : \", count)","e7df1778":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport heapq\nprint(data.head())\nrf = RandomForestClassifier(n_estimators=300, max_depth=5,random_state=0)\nrf.fit(data.values, targets)\nmy_heap = []\nfor i in range(len(rf.feature_importances_)):\n    heapq.heappush(my_heap, (-rf.feature_importances_[i], data.columns[i]))\nheights = []\nlabels = []\nfor i in range(10):\n    height, label = heapq.heappop(my_heap)\n    heights.append(-height)\n    labels.append(label)\nplt.bar(range(10), height=heights, tick_label=labels)\n\nplt.show()","8b2429c9":"param_grid = {\"C\": [.01, .1, 1, 10, 100, 1000],\n                \"penalty\": ('l1','l2')}\ntop_data = data.drop(columns=['k_cluster', 'g_mix'])\nclf = LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced', max_iter = 1000)\ngs = GridSearchCV(clf, param_grid, cv=5)\ngs.fit(top_data.values, targets)\nprint(gs.best_score_)\nprint(gs.best_estimator_)\npredictions = gs.predict_proba(t_data.drop(columns=['k_cluster', 'g_mix']).values)[:,1]\noutput = pd.DataFrame({'id' : ids, 'target' : predictions})\noutput.to_csv('scale_only_output.csv',index=None)","0dc885c9":"param_grid = {\"C\": [.01, .1, 1, 10, 100, 1000],\n                \"penalty\": ('l1','l2')}\ntop_data = data\nclf = LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced', max_iter = 1000)\ngs = GridSearchCV(clf, param_grid, cv=5)\ngs.fit(top_data.values, targets)\nprint(gs.best_score_)\nprint(gs.best_estimator_)\npredictions = gs.predict_proba(t_data.values)[:,1]\noutput = pd.DataFrame({'id' : ids, 'target' : predictions})\noutput.to_csv('scale_and_clustering_output.csv',index=None)","bbd8112a":"param_grid = {\"C\": [.01, .1, 1, 10, 100, 1000],\n                \"penalty\": ('l1','l2')}\ntop_data = data[labels]\nclf = LogisticRegression(random_state=0, solver='liblinear', class_weight='balanced', max_iter = 1000)\ngs = GridSearchCV(clf, param_grid, cv=5)\ngs.fit(top_data.values, targets)\nprint(gs.best_score_)\nprint(gs.best_estimator_)\npredictions = gs.predict_proba(t_data[labels].values)[:,1]\noutput = pd.DataFrame({'id' : ids, 'target' : predictions})\noutput.to_csv('reduced_scale_and_clustering_output.csv',index=None)","57e0428d":"**With RFE+Clustering**","d71f8ea5":"**Add Cluster Information To The Dataset**","ba7db6b8":"**With Clustering**","1e5fe27c":"**Scale The Data**","5133e3c7":"**Without Clustering**","f2fb95b1":"We can see from this graph that k_cluster has a high importance","84c5b616":"**Clustering Test Data**","e4168a98":"**Check Variable Importances With RandomForests**","02c17006":"I have tried submitting all of these, and Clustering without reducing the variables generalizes the best.\n(Clustering for these scores was done before transforming the data)\n\nThe original 300 scaled score .816\n\nAdding clustering increases to .837\n\nThe top 9 scaled variables and k_means scores .801"}}