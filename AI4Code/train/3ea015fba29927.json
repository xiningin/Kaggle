{"cell_type":{"c1e7659b":"code","982ed0cd":"code","65f95ed8":"code","978b9d7d":"code","80447336":"code","23f423f6":"code","4e7f816f":"code","45981ccf":"code","643f61bc":"code","ba260d93":"code","7a5477b9":"code","4ed44107":"code","5c8bca16":"code","853c8903":"code","38caaea5":"code","05b5408a":"code","3f61f35c":"markdown","544d3f64":"markdown","5ac32faf":"markdown","d60be7f3":"markdown","7ecd2945":"markdown","6abf8728":"markdown","5f512106":"markdown","03493834":"markdown","3cabff47":"markdown","b5ecb171":"markdown","bc00d12d":"markdown","ed7fbf28":"markdown","19216d7b":"markdown","e62ca9cd":"markdown"},"source":{"c1e7659b":"import plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","982ed0cd":"prices = np.loadtxt('..\/input\/binance-bitcoin-futures-price-10s-intervals\/prices_btc_Jan_11_2020_to_May_22_2020.txt', dtype=float)\nprint('Number of prices:', len(prices))","65f95ed8":"fig = go.Figure(data=go.Scatter(y=prices[-10000:]))\nfig.show()","978b9d7d":"def buy(btc_price, btc, money):\n    if(money != 0):\n        btc = (1 \/ btc_price ) * money\n        money = 0\n    return btc, money\n\n\ndef sell(btc_price, btc, money):\n    if(btc != 0):\n        money = btc_price * btc\n        btc = 0\n    return btc, money\n\n\ndef wait(btc_price, btc, money):\n    # do nothing\n    return btc, money","80447336":"np.random.seed(1)\n\n# set of actions that the user could do\nactions = { 'buy' : buy, 'sell': sell, 'wait' : wait}\n\nactions_to_nr = { 'buy' : 0, 'sell' : 1, 'wait' : 2 }\nnr_to_actions = { k:v for (k,v) in enumerate(actions_to_nr) }\n\nnr_actions = len(actions_to_nr.keys())\nnr_states = len(prices)\n\n# q-table = reference table for our agent to select the best action based on the q-value\nq_table = np.random.rand(nr_states, nr_actions)","23f423f6":"def get_reward(before_btc, btc, before_money, money):\n    reward = 0\n    if(btc != 0):\n        if(before_btc < btc):\n            reward = 1\n    if(money != 0):\n        if(before_money < money):\n            reward = 1\n            \n    return reward","4e7f816f":"def choose_action(state):\n    if np.random.uniform(0, 1) < eps:\n        return np.random.randint(0, 2)\n    else:\n        return np.argmax(q_table[state])","45981ccf":"def take_action(state, action):\n    return actions[nr_to_actions[action]](prices[state], btc, money)","643f61bc":"def act(state, action, theta):\n    btc, money = theta\n    \n    done = False\n    new_state = state + 1\n    \n    before_btc, before_money = btc, money\n    btc, money = take_action(state, action)\n    theta = btc, money\n    \n    reward = get_reward(before_btc, btc, before_money, money)\n    \n    if(new_state == nr_states):\n        done = True\n    \n    return new_state, reward, theta, done","ba260d93":"reward = 0\nbtc = 0\nmoney = 100\n\ntheta = btc, money","7a5477b9":"# exploratory\neps = 0.3\n\nn_episodes = 20\nmin_alpha = 0.02\n\n# learning rate for Q learning\nalphas = np.linspace(1.0, min_alpha, n_episodes)\n\n# discount factor, used to balance immediate and future reward\ngamma = 1.0","4ed44107":"rewards = {}\n\nfor e in range(n_episodes):\n    \n    total_reward = 0\n    \n    state = 0\n    done = False\n    alpha = alphas[e]\n    \n    while(done != True):\n\n        action = choose_action(state)\n        next_state, reward, theta, done = act(state, action, theta)\n        \n        total_reward += reward\n        \n        if(done):\n            rewards[e] = total_reward\n            print(f\"Episode {e + 1}: total reward -> {total_reward}\")\n            break\n        \n        q_table[state][action] = q_table[state][action] + alpha * (reward + gamma *  np.max(q_table[next_state]) - q_table[state][action])\n\n        state = next_state","5c8bca16":"plt.ylabel('Total Reward')\nplt.xlabel('Episode')\nplt.plot([rewards[e] for e in rewards.keys()])","853c8903":"state = 0\nacts = np.zeros(nr_states)\ndone = False\n\nwhile(done != True):\n\n        action = choose_action(state)\n        next_state, reward, theta, done = act(state, action, theta)\n        \n        acts[state] = action\n        \n        total_reward += reward\n        \n        if(done):\n            break\n            \n        state = next_state","38caaea5":"buys_idx = np.where(acts == 0)\nwait_idx = np.where(acts == 2)\nsell_idx = np.where(acts == 1)","05b5408a":"plt.figure(figsize=(15,15))\nplt.plot(buys_idx[0], prices[buys_idx], 'bo', markersize=2)\nplt.plot(sell_idx[0], prices[sell_idx], 'ro', markersize=2)\nplt.plot(wait_idx[0], prices[wait_idx], 'yo', markersize=2)","3f61f35c":"### Plot the Results","544d3f64":"### Functions to get rewards and act upon action","5ac32faf":"<h1 id=\"dataset\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <left> ( \u20bf )  Datasets\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>\n\nQ-learning is an off policy reinforcement learning algorithm that seeks to find the best action to take given the current state. It\u2019s considered off-policy because the q-learning function learns from actions that are outside the current policy, like taking random actions, and therefore a policy isn\u2019t needed. More specifically, q-learning seeks to learn a policy that maximizes the total reward.","d60be7f3":"<h1 id=\"data\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <left> ( \u20bf )  Retrieve data\n        <a class=\"anchor-link\" href=\"#data\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","7ecd2945":"### Create actions, states tables","6abf8728":"<h1 id=\"analysis\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <left> ( \u20bf ) Analysis\n        <a class=\"anchor-link\" href=\"#analysis\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","5f512106":"### Load price data","03493834":"#### Steps for Q-network learning\n\nHere are the 3 basic steps:\n\n- Agent starts in a state=0 takes an action and receives a reward\n- Agent selects action by referencing Q-table with highest value (max) OR by random (epsilon, \u03b5)\n- Update q-values","3cabff47":"<h1 id=\"goal\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <left> ( \u20bf )  Goal\n        <a class=\"anchor-link\" href=\"#goal\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>\n\nThe goal of this notebook is to showcase the use of Q-learning in order to buy and sell in order to maximize the money\/btc amount.\n\nThis is an controlled environment - **DO NOT USE ON REAL BITCOIN**\n\n**Don't forget to up-vote in order to support me from developing new kernels (notebooks)**","b5ecb171":"<h1 id=\"implementation\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <left> ( \u20bf )  Q Learning Implementation\n        <a class=\"anchor-link\" href=\"#implementation\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","bc00d12d":"<pre style=\"border:0.5px dotted;\">\n\u2566\u2569\u2569\u2550\u2569\u2569\u2550\u2550\u2557\u2500\n\u2563\u2500\u2554\u2550\u2550\u2550\u2557\u2500\u2551\u2500\n\u2551\u2500\u255a\u2550\u2550\u2550\u255d\u2500\u255a\u2557\n\u2551\u2500\u2554\u2550\u2550\u2550\u2550\u2557\u2500\u2551\n\u2563\u2500\u255a\u2550\u2550\u2550\u2550\u255d\u2500\u2551\n\u2569\u2566\u2566\u2550\u2566\u2566\u2550\u2550\u2550\u255d\n<\/pre>\n\n### ( \u20bf ) - Q learning - Reinforcement Learning\n#### by Alin CIjov","ed7fbf28":"<h1 id=\"training\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <left> ( \u20bf ) Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/left>\n<\/h1>","19216d7b":"### Functions to buy, sell and wait","e62ca9cd":"### Conclusion\n\nEven if we trained for only 20 episodes, we can see clear how the Q learning works in reinforcement learning.<br>\nWe got a Q network 2D array that holds the best action for each state plus we also use exploratory using (eps) in order to try new actions, if the actions get better rewards, we add them to the Q table by increasing the q table actions using the formula :<br>\n> q_table[state][action] + alpha * (reward + gamma *  np.max(q_table[next_state]) - q_table[state][action])"}}