{"cell_type":{"e2dd9df9":"code","9c21dc8b":"code","b30f6993":"code","46481b7c":"code","8b9bd65f":"code","74752061":"code","4e29e966":"code","9526adc5":"code","647b685d":"code","d63ffaa2":"code","a7904449":"code","355edae6":"code","94e67dfa":"code","ccfb75ce":"code","b1d1ff73":"code","dbea875c":"code","3242cc7d":"code","39d5c33b":"code","9523ea59":"code","653962c2":"code","2b14fa98":"code","105ca20c":"code","b1c713d3":"code","6f1fd6b5":"code","b62ce652":"code","8870e175":"code","b9f14e66":"code","f304a13d":"code","076792e5":"code","fff2e8ed":"code","a6d34eac":"code","851744e4":"code","e72fc5ed":"code","47bbc4ae":"code","71f30d6e":"code","69797b44":"code","45df1bf1":"code","908188e4":"code","b8d1a691":"code","7af190be":"markdown","ed96f97b":"markdown","58ba6402":"markdown","f133c5d0":"markdown","a5fd47de":"markdown","a81dcaef":"markdown","df390a9b":"markdown","84bf779c":"markdown","ee4e4815":"markdown","842f32d4":"markdown","d346d6a1":"markdown","6c2ea262":"markdown","67fea911":"markdown","efa950a8":"markdown","56152947":"markdown","deacb95e":"markdown","f6a50343":"markdown","9216f622":"markdown","b322bb00":"markdown","49dbd32e":"markdown","7c4e9f04":"markdown","f86890df":"markdown","5cfafb54":"markdown","bbb52272":"markdown","b5f4a624":"markdown","dfe7bb57":"markdown","00cb75b8":"markdown","a678f531":"markdown","9cade07a":"markdown","4d1a0c8f":"markdown","de1a9637":"markdown"},"source":{"e2dd9df9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c21dc8b":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\",)  #Ignore certain system-wide alerts\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n# Import Machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import MinMaxScaler\nimport lightgbm as lgb\n\n#Import metric for performance evaluation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay","b30f6993":"train_df = pd.read_csv('\/kaggle\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Test.csv')\n\n#Using the read.csv code of the pandas library, \n#we read the train and test csv files and assign them to two variables called train_df and test_df.","46481b7c":"print(train_df.shape)\nprint(test_df.shape)\nprint(train_df.size)\nprint(test_df.size)\n\n# Seeing size, row and column counts in data with shape code and size code","8b9bd65f":"train_df.head()\n\n# Seeing first five row in train_df data","74752061":"test_df.head()\n\n# Seeing first five row in test_df data","4e29e966":"train_df.rename(columns={\"labels\": \"churn\"}, inplace=True)\n#we change label column name  with \"chrun\"","9526adc5":"train_df.describe().T\n\n#Check mean,avarage,median,min,max and std ","647b685d":"def summary(df):\n    Types = df.dtypes\n    Counts = df.apply(lambda x: x.count())\n    Uniques = df.apply(lambda x: x.unique().shape[0]) # If shape[0] is not written, it returns a list of unique values.\n    cols = ['Types', 'Counts', 'Uniques']\n    str = pd.concat([Types, Counts, Uniques], axis = 1, sort=True)\n    str.columns = cols\n    display(str.sort_values(by='Uniques', ascending=False))\n    print('__________Data Types__________\\n')\n    print(str.Types.value_counts())\nsummary(df=train_df)\n\n# check types,counts,unique of the data","d63ffaa2":"import missingno as msno\n\nmsno.matrix(train_df);\n\n#Visualization of nan values","a7904449":"dataset = train_df[\"churn\"].value_counts()\ndataset","355edae6":"sizes = [29941,3967]\nlabels='NO','YES'\nexplode = (0, 0.1)  \nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode,autopct='%1.1f%%',shadow=True, startangle=75 )\nax1.axis('equal') \nax1.set_title(\"Client Churn Distribution\")\n\nax1.legend(labels)\n\nplt.show()\n        \n #ratio of those who churn and those who don't\n","94e67dfa":"def show_correlations(df, show_chart = True):\n    fig = plt.figure(figsize = (20,10))\n    corr = df.corr()\n    if show_chart == True:\n        sns.heatmap(corr, \n                    xticklabels=corr.columns.values,\n                    yticklabels=corr.columns.values,\n                    annot=True)\n    return corr\n\ncorrelation_df = show_correlations(train_df,show_chart=True)\n\n#Get Correlation of \"churn\" with other variables:","ccfb75ce":"# Get Correlation of \"churn\" with other variables with sorted:\nplt.figure(figsize=(15,8))\ntrain_df.corr()['churn'].sort_values(ascending = False).plot(kind='bar')","b1d1ff73":"features = list(train_df.columns)\nfeatures.remove('churn')\nfeatures\n\n#getting the column names and removing the churn column\n","dbea875c":"float_features = [i for i in train_df.columns if train_df[i].dtype == 'float64']\nfloat_features","3242cc7d":"int_features=[i for i in train_df.columns if train_df[i].dtype == 'int64']\nint_features.remove('churn')\nint_features","39d5c33b":"fig, ax = plt.subplots(4, 2, figsize = (15, 10))\nax = ax.flatten()\nfor i, c in enumerate(float_features):\n    sns.boxplot(x = train_df[c], ax = ax[i], palette = 'Set3')\nplt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()\n\n#Box plot of float features","9523ea59":"def plot_hist(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable], bins = 50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","653962c2":"for n in float_features:\n    plot_hist(n)","2b14fa98":"fig, ax = plt.subplots(5, 2, figsize = (15, 10))\nax = ax.flatten()\nfor i, c in enumerate(int_features):\n    sns.boxplot(x = train_df[c], ax = ax[i], palette = 'Set3')\nplt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()\n\n#Box plot of integer features","105ca20c":"def bar_plot(variable):\n    \"\"\"\n        input: variable \n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_df[variable]\n    # count number of float variable(value\/sample)\n    varValue = var.value_counts()\n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))\n    \n\n","b1c713d3":"for c in int_features:\n    bar_plot(c)","6f1fd6b5":"#In basic data analysis, we see the relationship between columns and Churn.\nfig, ax = plt.subplots(9, 1, figsize = (10, 30))\nax = ax.flatten()\nfor i, c in enumerate(int_features):\n    a = sns.countplot(x = train_df[c], ax = ax[i], hue = train_df['churn'])\n    for p in a.patches:\n        a.annotate('{:.1f}%'.format(100 * p.get_height() \/ len(train_df)), (p.get_x() + 0.1, p.get_height() + 5))\nplt.suptitle('Count Plot of Integer Features', fontsize = 20)\nfig.tight_layout()","b62ce652":"#Split data into train and test sets\nX = train_df.drop('churn', axis=1)\ny = train_df['churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=50)  ","8870e175":"#Defining the modelling function\ndef modeling(alg, alg_name, params={}):\n    model = alg(**params) #Instantiating the algorithm class and unpacking parameters if any\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    #Performance evaluation\n    def print_scores(alg, y_true, y_pred):\n        print(alg_name)\n        acc_score = accuracy_score(y_true, y_pred)\n        print(\"accuracy: \",acc_score)\n        pre_score = precision_score(y_true, y_pred)\n        print(\"precision: \",pre_score)\n        rec_score = recall_score(y_true, y_pred)                            \n        print(\"recall: \",rec_score)\n        f_score = f1_score(y_true, y_pred, average='weighted')\n        print(\"f1_score: \",f_score)\n\n    print_scores(alg, y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    #Create the Confusion Matrix Display Object(cmd_obj). \n    cmd_obj = ConfusionMatrixDisplay(cm, display_labels=['Churn', 'notChurn'])\n\n    #The plot() function has to be called for the sklearn visualization\n    cmd_obj.plot()\n\n    #Use the Axes attribute 'ax_' to get to the underlying Axes object.\n    #The Axes object controls the labels for the X and the Y axes. It also controls the title.\n    cmd_obj.ax_.set(\n                    title='Confusion Matrix with labels!!', \n                    xlabel='Predicted Churn', \n                    ylabel='Actual Churn')\n    #Finally, call the matplotlib show() function to display the visualization of the Confusion Matrix.\n    plt.show()\n    \n    return model","b9f14e66":"# Running logistic regression model\nlog_model = modeling(LogisticRegression, 'Logistic Regression')","f304a13d":"# Trying other machine learning algorithms: SVC\nsvc_model = modeling(SVC, 'SVC Classification')","076792e5":"#Decision tree\ndt_model = modeling(DecisionTreeClassifier, \"Decision Tree Classification\")","fff2e8ed":"#Naive bayes \nnb_model = modeling(GaussianNB, \"Naive Bayes Classification\")","a6d34eac":"#Ada Boost Classifier\nad_model=modeling(AdaBoostClassifier, \"Ada Boost Classifier\")","851744e4":"#Gradient Boosting Classifier\ngbm_model=modeling(GradientBoostingClassifier, \"Gradient Boosting Classifier\")","e72fc5ed":"# LightGBM model\nLGBM_model = modeling(lgb.LGBMClassifier, 'Light GBM')","47bbc4ae":"\n# First i will try for Smote if it was not good mean we will apply for the undersampling\nover = SMOTE()\nX, y = over.fit_resample(X,y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7) \nscaler= MinMaxScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","71f30d6e":"clf = RandomForestClassifier(n_estimators=100, random_state=7)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n","69797b44":"cm = confusion_matrix(y_test, y_pred)\ncmd_obj = ConfusionMatrixDisplay(cm, display_labels=['1', '0'])\ncmd_obj.plot()\ncmd_obj.ax_.set(\n                    title='Confusion Matrix with labels!!', \n                    xlabel='Predicted Churn', \n                    ylabel='Actual Churn'\n                    )\nplt.show()","45df1bf1":"#Showing the accuracy of our models in the form of bar plots\ndf = pd.DataFrame({'Models':['Naive','DecTree','LogReg', 'SVC','ADA','GBM','LGBM','RandFor'], 'Prediction':[0.836,0.873, 0.892, 0.893 ,0.898,0.906,0.913,0.949]})\nax = df.plot.bar(x='Models', y='Prediction', rot=45)","908188e4":"#Saving best model \nimport joblib\n#Sava the model to disk\nfilename = 'model.sav'\njoblib.dump(log_model, filename)","b8d1a691":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","7af190be":"<a id = \"21\"><\/a><br>\n# Random Forest Classifier","ed96f97b":"<a id = \"19\"><\/a><br>\n# Gradient Boosting Classifier","58ba6402":" \n<a id = \"1\"><\/a><br>\n# 1-Importing The Necessary Libraries","f133c5d0":"\n<a id = \"9\"><\/a><br>\n## Integer Variables","a5fd47de":"You can check my github page for streamlit codes\n\nhttps:\/\/github.com\/caliskanbulent\/web_insc\/tree\/master","a81dcaef":"<a id = \"12\"><\/a><br>\n# Train and Test Split","df390a9b":"If you want to read my Medium article, you can view the medium link below.\n\nhttps:\/\/medium.com\/@bulentcaliskan283\/how-to-implement-customer-churn-prediction-with-machine-learning-97ceaecba364","84bf779c":"# Explanation \n    Given are 16 distinguishing factors that can help in understanding the customer churn, your objective as a data scientist is to build a Machine Learning model that can predict whether the insurance company will lose a customer or not using these factors.\n\n    You are provided with 16 anonymized factors (feature_0 to feature 15) that influence the churn of customers in the insurance industry","ee4e4815":"You can view and run the web-deployed version from this site.\n\nhttp:\/\/churnproject.herokuapp.com\/","842f32d4":"<a id = \"16\"><\/a><br>\n# Decision Tree Classification","d346d6a1":"<a id = \"14\"><\/a><br>\n# Logistic Regression","6c2ea262":"<a id = \"4\"><\/a><br>\n# 4-Missing Value","67fea911":"<a id = \"22\"><\/a><br>\n# Trial and Conclusion ","efa950a8":"<a id = \"11\"><\/a><br>\n# 9-Modelling","56152947":"<a id = \"17\"><\/a><br>\n# Naive Bayes Classification","deacb95e":"\n<a id = \"10\"><\/a><br>\n# 8-Basic Data Analysis","f6a50343":"\n<a id = \"6\"><\/a><br>\n# 6-Visualization\n","9216f622":"# Insurance Churn Prediction\n## Introduction\n    Insurance companies around the world operate in a very competitive environment. With various aspects of data collected from millions of customers, it is painstakingly hard to analyze and understand the reason for a customer\u2019s decision to switch to a different insurance provider.\n\n    For an industry where customer acquisition and retention are equally important, and the former being a more expensive process, insurance companies rely on data to understand customer behavior to prevent retention. Thus knowing whether a customer is possibly going to switch beforehand gives Insurance companies an opportunity to come up with strategies to prevent it from actually happening.","b322bb00":"<a id = \"15\"><\/a><br>\n# SVC Classification","49dbd32e":"# <a id = \"23\"><\/a><br>\n# 10- Medium Article","7c4e9f04":"\n\n<font color = 'green'>\nContent: \n\n1. [Import Libraries](#1)\n1. [Load and Check Data](#2)\n    \n1. [Check Uniques, Data's Types And Counts](#3)\n1. [Missing Value](#4)\n1. [Check The Churn](#5)\n1. [Visualization](#6)\n1. [Variable Analysis](#7)\n    * [Float Variable](#8)\n    * [Integer Variable](#9)\n1. [Basic Data Analysis](#10)\n1. [Modelling](#11)\n    * [Train-Test Split](#12)\n    * [Confusion Matrix](#13)\n    * [Logistic Regression](#14)\n    * [SVC Classification](#15)\n    * [Decision Tree Classification](#16)\n    * [Naive Bayes Classification](#17)\n    * [Ada Boost Classifier](#18)\n    * [Gradient Boosting Classifier](#19)\n    * [LGBM Classifier](#20)\n    * [Random Forest Classifier](#21)\n    * [Trial and Conclusion](#22)\n1. [Medium Article](#23)\n1. [Web Deploy](#24)    ","f86890df":"<a id = \"18\"><\/a><br>\n# Ada Boost Classifier","5cfafb54":"<a id = \"20\"><\/a><br>\n# LGBM Classifier","bbb52272":"<a id = \"3\"><\/a><br>\n# 3-Check Uniques,Data's Types And Counts\n","b5f4a624":"\n<a id = \"7\"><\/a><br>\n# 7- Variable Analysis\n* float Variable: 'feature_0','feature_1','feature_2','feature_3','feature_4','feature_5','feature_6'\n* integer  Variable: 'feature_7','feature_8','feature_9','feature_10','feature_11','feature_12','feature_13','feature_14','feature_15\n","dfe7bb57":"\n<a id = \"5\"><\/a><br>\n# 5-Check The Churn","00cb75b8":"<a id = \"13\"><\/a><br>\n# Confusion Matrix","a678f531":"<a id = \"8\"><\/a><br>\n## Float Variables","9cade07a":"\n<a id = \"2\"><\/a><br>\n# 2-Load and Check Data","4d1a0c8f":"# <a id = \"25\"><\/a><br>\n# 12-Github Page","de1a9637":"# <a id = \"24\"><\/a><br>\n# 11-Web Deploy"}}