{"cell_type":{"085a9b87":"code","a0261e3f":"code","c02d5c11":"code","8fad8299":"code","f40038da":"code","a193afd9":"code","3793f9e6":"markdown","cae2e7d9":"markdown","191b4b08":"markdown","86ecf23e":"markdown","2c13f042":"markdown","f569ae76":"markdown","ceb1ca78":"markdown","e603c17f":"markdown","904e6d10":"markdown","4ec67635":"markdown","aae66a75":"markdown"},"source":{"085a9b87":"from nltk.tokenize import sent_tokenize\n\ntext = \"Hello everyone! Welcome to Xsaras AI. Hope you all have a great experience throughout this internship.\"\ntokenized_text = sent_tokenize(text)\n\nprint(tokenized_text)","a0261e3f":"from nltk.tokenize import word_tokenize\n\ntokenized_word = word_tokenize(text)\n\nprint(tokenized_word)","c02d5c11":"from nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\"))\n\nprint(stop_words)","8fad8299":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nstemmed_words = []\n\nexample = ['labelling', 'interning', 'presentation', 'happiness', 'happy']\n\nfor w in example:\n    stemmed_words.append(ps.stem(w))\n    \nprint(stemmed_words)","f40038da":"from nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nlemmas = []\n\nexample = ['happiness', 'happy']\n\nfor w in example:\n    lemmas.append(lem.lemmatize(w))\n    \nprint(lemmas)","a193afd9":"import pandas as pd\nimport nltk\n# nltk.download('stopwords') # Uncomment when running for the first time to download, comment it out after\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ncol = [2] # Index of the column to read\ndf = pd.read_excel('\/kaggle\/input\/word-frequency\/set_1_4.xlsx', usecols=col) # Enter filename\ndf.columns = [\"tweet\"] # Name the column for simplicity\n\ndf[\"tweet\"] = df[\"tweet\"].str.lower()\ndf[\"cleaned\"] = df[\"tweet\"].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)])) # Stop words removal\ndf[\"cleaned\"] = df[\"cleaned\"].str.replace('[#,@,&,?,!,.,\\,,-]', '') # Special characters removal\nfreq = pd.Series(' '.join(df[\"cleaned\"]).split()).value_counts()[:75] # Change the value of 75 to see the desired number of words\n\nprint(freq)\n\nfreq.to_csv(\"Word_Freq.csv\")","3793f9e6":"**Stopwords**\n\nStopwords are considered as noise in the text. The text may contain stop words such as is, am, are, this, a, an, the, etc. To remove the stopwords, you need to create a list of stopwords and filter out your list of tokens from these words.","cae2e7d9":"Here, I have written a simple Python program to read a Twitter Hate Speech dataset file in an Excel format, clean the data of stop words and special characters and show the top 75 most recurrent words in the Tweets column.","191b4b08":"# **Natural Language Toolkit**","86ecf23e":"The Natural Language Toolkit, or more commonly referred to as NLTK, is a suite of libraries used in the natural language processing of English text. \n\nIt contains several helpful methods for the identification, classification, cleaning and processing of the text. Some of the basic tasks that can be performed with the help of NLTK are:\n1. Tokenization:\n        i. Sentence\n        ii. Word\n    \n2. Stopwords elimination\n\n3. Normalization:\n        i. Stemming\n        ii. Lemmatization","2c13f042":"**Normalization**\n\nLexicon normalization considers another type of noise in the text. It reduces derivationally related forms of a word to a common root word.","f569ae76":"**Sentence Tokenization**","ceb1ca78":"**Word Tokenization**","e603c17f":"**Tokenization**\n\nTokenization is the first step in text analysis. The process of breaking down a text paragraph into smaller chunks such as words or sentence is called Tokenization. A token is a single entity i.e., the building block of a sentence or paragraph.","904e6d10":"**Stemming**\n\nStemming is a process of linguistic normalization, which reduces words to their root word, or truncates the derivational affixes. For example, connection, connected, connecting reduce to a common word \"connect\".","4ec67635":"# **Application - Word Frequency**","aae66a75":"**Lemmatization**\n\nLemmatization reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context."}}