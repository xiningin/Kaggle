{"cell_type":{"2f64fb32":"code","4fa279cc":"code","224ced80":"code","23b95cde":"code","c530db9d":"code","a23bc27c":"code","e8c86dfc":"code","3e9e4d07":"code","505ee98d":"code","d230f1f1":"code","04076066":"code","36cf7841":"code","95017d93":"code","ca3dbc5a":"code","8b0a4b50":"code","a1e5749c":"code","ac88ea74":"code","7d00900d":"code","39ebb0c6":"code","faf3485a":"code","27c8c5e6":"code","45bb7634":"code","6c41c6a0":"code","5a34b5ba":"code","43bf20cf":"code","745d12da":"code","796f7f37":"code","b5c9b6c0":"code","8cc95044":"code","d1dcef92":"code","0b99f491":"code","b8c8b097":"code","9ec592ed":"code","af28a524":"code","6766cc42":"code","974d75fa":"code","fb506763":"code","cea5a101":"code","6f8b8e2b":"code","0c2beef1":"code","90e19175":"code","40f56ff4":"code","855d4362":"code","5ad06b5c":"code","021a6b43":"code","5b14f808":"code","9f526cce":"markdown","d2ba0b27":"markdown","f03478dd":"markdown","a8e6c231":"markdown","d83f8a02":"markdown","ed82f144":"markdown","f674f813":"markdown","a0b51ae1":"markdown","252f313b":"markdown","887b2920":"markdown","92b95414":"markdown","60b7b800":"markdown","57536380":"markdown","5fcaac8a":"markdown","73178888":"markdown","7f57ce0d":"markdown","d97d072b":"markdown","4fb77c19":"markdown","267ec5bd":"markdown","ed83f2da":"markdown","299dbb33":"markdown","3d0e44ff":"markdown","aeaaab00":"markdown","5adea7c3":"markdown","f08ffd02":"markdown","77ff6cbf":"markdown","756319df":"markdown","edf9bd5d":"markdown","41ad8058":"markdown","4b2b0116":"markdown","95d1ac96":"markdown","dda5cc66":"markdown","3929078d":"markdown","092f957c":"markdown","eb0e36b7":"markdown","049e95dc":"markdown","dc0ca0ae":"markdown","76b87c37":"markdown","313f6c2b":"markdown","e7699f92":"markdown","aa9704d6":"markdown","80e1cbc5":"markdown","eb5c19e9":"markdown","f3f05975":"markdown","406b84b5":"markdown"},"source":{"2f64fb32":"import pandas as pd\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","4fa279cc":"import pandas_profiling\n\ntrain.profile_report()","224ced80":"train.info()","23b95cde":"test.info()","c530db9d":"train.head()","a23bc27c":"num_features = ['Age', 'SibSp', 'Parch', 'Fare']\ncat_features = ['Survived', 'Pclass', 'Sex', 'Embarked']\ncat_other = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n\ntrain[num_features].describe()","e8c86dfc":"%matplotlib inline\nimport matplotlib as plt\n\ntrain[num_features].hist(bins=8, figsize=(10,10))","3e9e4d07":"num_passengers_in_train_set = 891\n\nnum_pass_without_child_or_parent = train.loc[train.Parch == 0]['Parch'].count()\npercent_pass_without_child_or_parent = 100 * num_pass_without_child_or_parent \/ num_passengers_in_train_set\nprint(\"%2d passengers (%2d%%) travelled without a parent or child\" %(num_pass_without_child_or_parent, percent_pass_without_child_or_parent) )\n\nnum_pass_without_sib_or_spouse = train.loc[train.SibSp == 0]['SibSp'].count()\npercent_pass_without_sib_or_spouse = 100 * num_pass_without_sib_or_spouse \/ num_passengers_in_train_set\nprint(\"%2d passengers (%2d%%) travelled without a sibling or spouse\" %(num_pass_without_sib_or_spouse, percent_pass_without_sib_or_spouse) )","505ee98d":"train[cat_features].describe(include=['O']) # include strings in the white list of features","d230f1f1":"train[cat_other].describe(include=['O']) # include strings in the white list of features","04076066":"# who resided in cabins C23, C25 and C27?\ntrain.loc[train.Cabin == 'C23 C25 C27'][['Name', 'Ticket']].values","36cf7841":"# who had ticket 347082? \ntrain.loc[train.Ticket == '347082'][['Name', 'Ticket', 'Cabin', 'Fare', 'Pclass']].values","95017d93":"correlation_matrix = train.corr() # Compute pairwise correlation of columns, excluding NA\/null values.\ncorrelation_matrix['Survived']","ca3dbc5a":"train_children = train.loc[train.Age <= 12].copy()\nnum_children = train_children.shape[0]\nnum_children_survived = train_children[train_children.Survived == 1].shape[0]\npercent_children_survived = 100 * num_children_survived \/ num_children\n\nnum_children_in_pc1 = train_children.loc[train_children.Pclass == 1].shape[0]\nnum_children_in_pc2 = train_children.loc[train_children.Pclass == 2].shape[0]\nnum_children_in_pc3 = train_children.loc[train_children.Pclass == 3].shape[0]\n\nprint('%d (%d%%) of the %d children (<=12 years) survived' %(num_children_survived, percent_children_survived, num_children ))\nprint('%d, %d and %d children travelled in 1st, 2nd and 3rd class respectively' %(num_children_in_pc1,num_children_in_pc2, num_children_in_pc3 ))\n\n\ntrain_children[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Pclass', ascending=True)","8b0a4b50":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False) # return object with group labels as the index, sorted by Survived.","a1e5749c":"# Determine which passengers have missing Embarkation data\ntrain.loc[pd.isnull(train.Embarked)]","ac88ea74":"train.loc[train.Cabin == 'B28']","7d00900d":"train.loc[train.Ticket == '113572']","39ebb0c6":"# calculate median values for train and test sets\n#\n# Using a SimpleImputer is overkill in this circumstance as it calculates the median\/mode values for all features. We only need two in total in each dataset\nfrom sklearn.impute import SimpleImputer\n\n# Store PassengerIds for the test data as we'll need them for submission\ntest_passenger_ids = test['PassengerId']\n\n# Drop the Cabin column\nfor data in [train, test]: \n    # Note: this drop must be performed inplace else the contents of train and test will not be updated.\n    data.drop(['Ticket', 'Cabin', 'PassengerId'], axis=1, inplace=True)\n\n# print(test.loc[pd.isnull(test.Fare)])       \nfor data in [train, test]: \n    imputer = SimpleImputer(strategy='median')\n    data_trans = pd.DataFrame(imputer.fit_transform(data[['Fare']]), columns=['Fare'], index=data.index) #DataFrame\n    data.update(data_trans) # update the original DataFrame with imputed values\n\n# Replace missing Embarked data with most frequent (mode) imputed values\nfor data in [train, test]:\n    imputer = SimpleImputer(strategy='most_frequent') # most_frequent == mode\n    data_trans = pd.DataFrame(imputer.fit_transform(data[['Embarked']]), columns=['Embarked'], index=data.index) #DataFrame\n    data.update(data_trans) # update the original DataFrame with imputed values","faf3485a":"# create dummy variables for Embarked\ntrain_dummies = pd.get_dummies(train, columns=['Embarked'], prefix=['Embarked'])\ntrain = train_dummies\n\ntest_dummies = pd.get_dummies(test, columns=['Embarked'], prefix=['Embarked'])\ntest = test_dummies\n\n# map male and female values to 1 and 0 respectively    \ntrain['Sex'] = train['Sex'].apply(lambda x: 1 if x == 'male' else 0)  \ntest['Sex'] = test['Sex'].apply(lambda x: 1 if x == 'male' else 0)  \n\n","27c8c5e6":"train.info()","45bb7634":"# The following code is based on notebook https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n#\nfor data in [train, test]:\n    data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \npd.crosstab(train['Title'], train['Sex'])","6c41c6a0":"for data in [train, test]:\n    data['Title'].replace('Mlle', 'Miss', inplace=True)\n    data['Title'].replace('Ms', 'Miss', inplace=True)\n    data['Title'].replace('Mme', 'Mrs', inplace=True)   \n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).count()","5a34b5ba":"for data in [train, test]:\n    data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n            'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other', inplace=True)\n\nprint(train[['Title', 'Survived']].groupby(['Title'], as_index=False).count())","43bf20cf":"train.head()","745d12da":"train_dummies = pd.get_dummies(train, columns=['Title'], prefix=['Title'])\ntrain = train_dummies\n\ntest_dummies = pd.get_dummies(test, columns=['Title'], prefix=['Title'])\ntest = test_dummies\n\n\nfor data in [test, train]:\n    data.drop(['Name'], axis=1, inplace=True)","796f7f37":"train.head()","b5c9b6c0":"#\n# Replace missing Age data with iteratively imputed values\n#\n#\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfor data in [test, train]:\n    \n    #iterative_regressor = IterativeImputer(random_state=0, max_iter=100, estimator=DecisionTreeRegressor(max_features=None, random_state=0))\n    iterative_regressor = IterativeImputer(random_state=0, max_iter=100, estimator=KNeighborsRegressor(n_neighbors=15))\n\n    iterative_regressor.fit(data[['Age', 'Sex', 'Pclass', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Other']])\n\n    data_imp = iterative_regressor.transform(data[['Age', 'Sex', 'Pclass', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Other']])\n    data_temp = pd.DataFrame(data_imp, columns=['Age', 'Sex', 'Pclass', 'Title_Master', 'Title_Miss', 'Title_Mr', 'Title_Other'], index=data.index) #DataFrame\n    \n    # replace Age with imputed values\n    data['Age'] = data_temp['Age']\n","8cc95044":"train.info()\ntest.info()","d1dcef92":"train['FamilySize'] = 0\ntest['FamilySize'] = 0\n\n\nfor data in [train, test]:\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    data['IsAlone'] = data['FamilySize'].apply(lambda x: 1 if x == 1 else 0)\n    data['PassengerFare'] = data.Fare\/data.FamilySize","0b99f491":"train.head()","b8c8b097":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","9ec592ed":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))","af28a524":"# Even though PassengerFare is a better measure of what each passenger paid, It's Pearson correlation with Survived is less than Fare. \n# Fare and Passenger and of course highly correlated so the new Feature PasengerFare should be dropped.\n#\nfor data in [train, test]:\n    data.drop(['PassengerFare','Embarked_Q'], axis=1, inplace=True)","6766cc42":"from sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(random_state=1, n_estimators=100)\n\n#dtc_rfe = RFECV(model, step = 1, scoring = 'accuracy', cv = cv_split)\nrfc_rfe = RFECV(model, step = 1, scoring = 'accuracy', cv = 10)\nrfc_rfe.fit(train.drop(['Survived'], axis=1), train['Survived'].copy()) # remove the target 'Survived' before performing RFE\n\nprint('The optimal number of features :', rfc_rfe.n_features_)\nprint('The optimal features are:', (train.drop(['Survived'], axis=1)).columns[rfc_rfe.support_])","974d75fa":"#reduce train and test dataframes. Select only the optimal features.\ntrain_survived = train['Survived'].copy()\ntrain.drop(['Survived'], axis=1, inplace=True)\ntrain = pd.DataFrame(train.values[:,rfc_rfe.support_], columns=train.columns[rfc_rfe.support_], index=train.index)\ntrain['Survived'] = train_survived # re-add Survived\n\ntest = pd.DataFrame(test.values[:,rfc_rfe.support_], columns=test.columns[rfc_rfe.support_], index=test.index)\n\ntrain.head()","fb506763":"train_X = train.drop(['Survived'], axis=1)\ntrain_Y = train['Survived'].copy()\ntest_X = test # there are no labels for test","cea5a101":"# Enable when required by a new model.\n#\nfrom sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\n\n# note we perform a fit and transform on train_X. We then use the same fit to transform test_X.\ntrain_X = standard_scaler.fit_transform(train_X)\n\ntest_X = standard_scaler.transform(test_X)","6f8b8e2b":"# import modules required by each model building code segment.\nimport numpy as np \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import ShuffleSplit\n\nimport pprint\n\npp = pprint.PrettyPrinter(indent=4)\n\n# store scores in a Dict for later comparison\nmodels = {}\n\n# an alternative to simply specifying the number of folds for cv in each model fit operation\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n","0c2beef1":"from sklearn.linear_model import LogisticRegressionCV\n\nmodel_name = 'lrc_model'\nmodels[model_name] = {}\n\n# create an instance of lrc model with untuned parameters\nmodel = LogisticRegressionCV(random_state=1, cv=10, max_iter=1000)\n\n# fit model to training data with cross-validation\nmodel.fit(train_X, train_Y)\n\n# store model\nmodels[model_name]['model'] = model\n\n# store score\nmodels[model_name]['crossval_score'] = model.score(train_X, train_Y, sample_weight=None)\n\npp.pprint(models[model_name])","90e19175":"from sklearn.tree import DecisionTreeClassifier\n\nmodel_name = 'dtc_model'\nmodels[model_name] = {}\n\n# create an instance of dtc model with untuned parameters\nmodel = DecisionTreeClassifier(random_state=1)\n\n# fit model using all training data\nmodel.fit(train_X, train_Y)\n\n# store model\nmodels[model_name]['model'] = model\n\n# store non-cross validation score. This represents the mean accuracy with overfitting.\nmodels[model_name]['score'] = model.score(train_X, train_Y, sample_weight=None)\n\n# store cross-val_score\nmodels[model_name]['cross_val_score'] = cross_val_score(model, train_X, \\\n                                                             train_Y, scoring=\"accuracy\", cv=10).mean()\n\npp.pprint(models[model_name])","40f56ff4":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_name = 'rfc_model'\nmodels[model_name] = {}\n\n# create an instance of rfc model with untuned parameters\nmodel = RandomForestClassifier(random_state=1)\n\n# fit model using all training data\nmodel.fit(train_X, train_Y)\n\n# store model\nmodels[model_name]['model'] = model\n\n# store non-cross validation score. This represents the mean accuracy with overfitting.\nmodels[model_name]['score'] = model.score(train_X, train_Y, sample_weight=None)\n\n# store cross-val_score\nmodels[model_name]['cross_val_score'] = cross_val_score(model, train_X, \\\n                                                             train_Y, scoring=\"accuracy\", cv=10).mean()\n\npp.pprint(models[model_name])","855d4362":"#\n# TODO This needs to be revised to reflext lrcCV and not LRC. The scoring needs to be revised too.\n#\nmodel_name = 'lrc_model'\n\nmodels[model_name]['param_grid'] = [\n        {'max_iter':[1000]}]\n\ngrid_search = GridSearchCV(models[model_name]['model'], models[model_name]['param_grid'], cv=10,\n                           scoring='accuracy', return_train_score=True)\n\ngrid_search.fit(train_X, train_Y)\n\n\n# store best model and params\nmodels[model_name]['best_params'] = grid_search.best_params_\nmodels[model_name]['best_estimator'] = grid_search.best_estimator_\n#\n# Modify this!!!\n#\nmodels[model_name]['best_score'] = grid_search.best_score_\n\n\npp.pprint(models)","5ad06b5c":"model_name = 'dtc_model'\n\nmodels[model_name]['param_grid'] = [\n        {'max_features': ['auto'], 'max_depth': [7,8,9], 'criterion': ['gini', 'entropy']}\n]\n\n#\n#grid_search = GridSearchCV(models[model_name]['model'], models[model_name]['param_grid'], cv=10,\n#                           scoring='accuracy', return_train_score=True)\n    \ngrid_search = GridSearchCV(models[model_name]['model'], models[model_name]['param_grid'], cv=cv_split,\n                           scoring='accuracy', return_train_score=True)    \n\ngrid_search.fit(train_X, train_Y)\n\n\n# store best model and params\nmodels[model_name]['best_params'] = grid_search.best_params_\nmodels[model_name]['best_estimator'] = grid_search.best_estimator_\nmodels[model_name]['best_score'] = grid_search.best_score_\n\npp.pprint(models[model_name])","021a6b43":"model_name = 'rfc_model'\n\nmodels[model_name]['param_grid'] = [\n        {'n_estimators': [100], 'oob_score': [True], 'max_features': ['auto'], 'max_depth': [7,8,9], 'criterion': ['gini', 'entropy']}\n]\n\n#\n#grid_search = GridSearchCV(models[model_name]['model'], models[model_name]['param_grid'], cv=10,\n#                           scoring='roc_auc', return_train_score=True)\n\ngrid_search = GridSearchCV(models[model_name]['model'], models[model_name]['param_grid'], cv=cv_split,\n                           scoring='roc_auc', return_train_score=True)\n\n\n\ngrid_search.fit(train_X, train_Y)\n\n\n# store best model and params\nmodels[model_name]['best_params'] = grid_search.best_params_\nmodels[model_name]['best_estimator'] = grid_search.best_estimator_\nmodels[model_name]['best_score'] = grid_search.best_score_\n\npp.pprint(models[model_name])","5b14f808":"import datetime\n\ntoday = datetime.datetime.now()\nfile_name = today.strftime(\"%Y%m%d%H%M%S\")\n\nbest_score = 0\n# Determine the best model\nfor key, value in models.items():\n    if (value['best_score'] > best_score):\n        best_score = value['best_score']\n        best_model = key\n        best_estimator = value['best_estimator']\n\nprint(\"The best model is\",best_model, \"with a score of\", best_score)        \npredictions = best_estimator.predict(test_X)\n\noutput_predictions = pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': predictions})\noutput_predictions.to_csv(file_name+\"prediction.csv\", index=False)\n\nf = open(file_name+\"models.txt\",\"w\")    \npp = pprint.PrettyPrinter(indent=4, stream=f)\npp.pprint(models)\nf.close()","9f526cce":"# Introduction\n\nWelcome. This notebook focuses on exploring and developing models to predict which passengers survived the Titanic shipwreck.\n\n## Purpose\n\nThis notebook was created to help consolidate my understanding of using an end-to-end data science framework to solve a classification problem. While the content presented within the notebook could be considered useful reference material for a beginner-level audience, I would encourage any reader to check-out the notebooks listed in the _References_ section of this notebook first.\n\nThe section on model building is not intended to be an exhaustive exploration of all possible classification machine learning algorithms. Instead, it covers a few promosing models influenced by the notebooks mentioned in the _References_ section and my own study of this topic. Future versions of the notebook will incorporate other models after understanding how they work, their strengths and limitations. \n\n## Framework\n\nThe data science framework used within this notebook consists of the following stages:\n\n1. Problem Statement - what problem are we solving, why is it important, to whom, what are the constraints and baseline solution.\n2. Data Gathering - what data sources are available and how might they be enriched\n3. Data Exploration - what is the structure, completeness and relationship between data.\n4. Data Preparation - missing data, outliers, feature engineering and normalisation\n5. Model Building - using a selection of machine learning algorithms to build models\n6. Model Tuning & Validation - hyper parameter tuning with cross-validation of the most promosing models \n7. Model Implementation - submission and scoring\n\nThe process isn't linear and sequential, but recursive in nature, with results, or lack there of leading to revisting earlier stages.\n\n## References\n\nThere are some incredibly helpful public notebooks associated with this competition. I found the following particularly valuable so include them as both attribution and so that others may read and learn from them.\n\n* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n* https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/\n* https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n* https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n* https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\/comments\n* https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling","d2ba0b27":"#### Feature Selection\nWhat's the optimal number of features to use in the model? Let's use sklearn.feature_selection.RFECV() to automate the process.","f03478dd":"We are now going to create a new feature to represent a passenger's direct (spouse, parent, child, sibling) family size and use it to calculate an adjusted passenger fare (ticket cost \/ no. people associated with a ticket). We'll also use it to create a binary feature IsAlone. We may choose to drop one or more of these features in the tuning phase, but for now they seem like sensible features to be considered. Families will probably help each other survive (vs being alone) and passengers travelling on the same ticket will skew Fares.","a8e6c231":"## Random Forest Classifier","d83f8a02":"### Dropping Features\nI initially started by dropping all features with less than 0.05 (abs) correlation with Survived. This turned out to be too aggressive and produced a lower submission score, so I settled on:\n\n* Embarked_Q\n* PassengerFare as Fare had both a higher correlation with Survived and provided a higher submission score.","ed82f144":"## Decision Tree","f674f813":"Let's now represent Sex and Embarked in a form suitable for modelling. Sex can be converted using an ordinal encoder (as there are only two categories) but we need a different approach for Embarked. Assuming closeness for each of the Embarkation locations is most probably incorrect.","a0b51ae1":"## Missing Data\n\nLet us revisit train and test data sets and decide what to do with missing data.\n\nIn the combined data sets, we have missing data for:\n\n* Embarked\n* Cabin\n* Age\n* Fare\n\nCabin contains significant amounts of missing data, so let's drop the entire column for this interation of the notebook. We may revisit this in later revisions as logically passengers located on the upper decks would have had easier access to escape.\n\nThere's only one missing data point for Fare, so our approach should have minimal impact on the final model. The histogram of Fare is heavily skewed towards the bottom end so let's choose median rather than mean.\n\nFor the missing Age data, we'll use a different approach. We'll use sklearn.impute.IterativeImputer() to estimate the age based on similar groups of passengers. We'll need to delay this step until after we've engineered a new feature 'Title' from 'Name.\n\nAs Embarked is a categorical feature and only two data points are missing, we'll use the mode value. However, as the number of missing data points is small, let's explore the two passengers and see whether we can increase our confidence in our decision to use mode.","252f313b":"## Data Structures and Variables\nOne of the challenges when reading a notebook, especially for a beginner-level audience is keeping track of the myriad of different variables  during the Data Preparation stage. When possible, I've performed transforms 'in-place' thereby reducing intermediate data structures and variable names. When this simply wasn't possible, I felt the additional step of copying intermediate results back to _train_ and _test_ helped readability at the expensive of some compute cycles. ","887b2920":"The data consists of:\n\n* 891 (train) and 418 (test) data points\n* 12 columns, 11 contain feature data, while column 'Survived' contains labels.\n\n**Missing Data**\n* Age 714 of 891\n* Cabin 204 of 891\n* Embarked 889 of 891\n* Fare 417 of 418 (test data only)\n\nWe'll decide how to handle missing data later.\n\nMost of the column names are self explanatory such as Age, Sex and Name, but SibSp and Parch in particular are not. Let's view the first few data points to better understand each feature's structure.","92b95414":"Viewing the data and referring to the accommpanying competition [data dictionary](https:\/\/www.kaggle.com\/c\/titanic\/data) helps complete our understanding.\n\n* PassengerId - a unique id for each passenger.\n* Survived - whether a passenger survived. 0 = No, 1 = Yes.\n* Pclass - the class of travel. 1 = 1st, 2 = 2nd, 3 = 3rd.\n* Name - name and title of passenger\n* Sex - male or female.\n* Age - age in years. Note: age is represented by a float. The [data dictionary](https:\/\/www.kaggle.com\/c\/titanic\/data) states that age is fractional if less than 1. If the age is estimated, is it in the form of xx.5.\n* Sibsp - defines the number of sibling or spousal members onboard.\n* Parch - defines the number of parents or children onboard.\n* Ticket - ticket number. There does not appear to be a consistent format for ticket numbers.\n* Fare - cost of the ticket. There is no mention of currency so given there are three points of embarkation, each in different countries (France, England and Ireland), we should explore this further.\n* Embarked - designates the town of embarkation.\n\nBefore we can train a model and subsequently make a prediction, we need to prepare the data. One of the main preparatory steps is to represent categories of data numerically. It's therefore helpful to group the features:\n\n**Categorical Features** { Survived, Pclass, Sex, Embarked }\n\n**Numerical Features** { Age, Sibsp, Parch, Fare }\n\n**Other Features** { PassengerId, Name, Ticket, Cabin }\n\nLet us now explore the data, feature by feature, starting with those expresses as numerical values.","60b7b800":"This is a significant result and explains why the example submission file accommpanying the competition uses only gender. Gender alone is a strong predictor of passenger survival.","57536380":"Immediately we can see the numerical features most strongly correlating with 'Survived' are:\n\n* Pclass (-ve)\n* Fare (+ve)\n* Parch (+ve)\n\nWhile we are yet to decide how we will preprocess Fare - if at all - we can see that the class of travel (1st, 2nd and 3rd - in that order) have the greatest correlation to a passenger surving.\n\nWe then have 'Parch', which is a measure of the number of parents or children the passenger has onboard.\n\nIntuitively, age should have a strong correlation with survival rate, but he correlation coefficients are marginal. Let's explore survial rates for children.","5fcaac8a":"Does anyone else share the same ticket number?","73178888":"## Logistic Regression","7f57ce0d":"we can see that:\n\n* 65% of the passengers in the training set are male.\n* 72% embarked at Southampton.\n\nand finally those features belonging to 'other'.","d97d072b":"Thankfully, the child survival rate is significantly greater than the overall survival rate (57% vs 32%), however, sadly survival rates were significantly higher in 1st and second class than third class.\n\nNow let us explore survival rates between genders to see if it's consistent with the code of conduct https:\/\/en.wikipedia.org\/wiki\/Women_and_children_first.","4fb77c19":"## Separating the data into features and labels\nLet us split the training data into features and labels ready for modelling.","267ec5bd":"As expected, we see a high degree of correlation between the engineered features and the source features e.g. FamilySide:SibSp\/Parch and Title_* and Sex. As we computed FamilySize to determine PassengerFare as well as combining SibSp and Parch in a meaningful way, we may be able to drop one or more of SibSp and Parch during the modelling phase. For now, let's leave them in. \n\nAn alternative to using a heatmap is to list the absolute correlation values in order. Depending on the modelling or prediction compute\/time constraints, it may be necessary to drop some features. I've read that features with a low correlation should be removed as they just introduce more noise to the model. I've not seen a really good explaination of this yet or what's a suitable cut off point. More investigation required.","ed83f2da":"we can now drop column Name as we've extracted the important Title information from it.\n\nAs mentioned earlier, we'll need to convert each of the new title categores into features. pandas.get_dummies() can do this for us. ","299dbb33":"# Problem Statement\n\nThe goal of this competition is to:\n\n> Use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nAs this competition was designed as a tutorial to beginners, it's value and purpose is just that.\n\nThere are few real constraints to solving this problem. The biggest contraint appears to be time and whatever compute resources are available. \n\n## Background\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n## Baseline\n\nBefore starting any data modelling exercise, it's valuable to determine a baseline. We are told that 1502 (67.5%) of the 2224 passengers and crew did not survive. A simple model that classifies all passengers as \"Not Survived\" will have a corresponding precision of: \n\n**Accuracy** = (TP + TN) \/ (TP + TN + FN + FP) = 2224 \/ 2946 = 75.5%\n\n**Precision** = TP\/(TP+FP) = 1502 \/ 2224 = 67.5%\n\nwhere:\n* True Positives (TP) = 1502\n* True Negatives (TN) = 722\n* False Positives (FP) = 2224 - 1502 = 722\n* False Negatives (FN) = 0 as we never predict a negative outcome i.e. Survived.\n\nSo these are the scores we need to beat.\n\n## Future Work\n\nFurther exploration and development of the following areas will almost certainly improve our final model.\n\n* additional feature enrichment and engineering\n* additional machine learning models\n* ensemble modeling","3d0e44ff":"# Model Building\n\nWith data pre-processing or preparation complete, we are now ready to train our model(s) on the data with a view to choosing a small number of models for tuning (via hyper-parameters).","aeaaab00":"# Data Exploration\nWe'll start by viewing the structure of the data. This includes the number of columns (features and labels), their names, data types and how complete the data set is.\n\nPandas.DataFrame.profile_report() is useful to quickly summarise data once loaded into a DataFrame.","5adea7c3":"## Decision Tree Classifier","f08ffd02":"# Model Tuning\nWe now improve upon the scores we achieved earlier by tuning each model's hyper-parameters. This can be done manually, sklearn.model_selection.GridSearchCV() automates much of this.\n\n## Logistic Regression\n","77ff6cbf":"# Data Gathering\n\nThree data sets are provided by the competition organiser.\n\n* train.csv - intended for the purpose of model training.\n* test.csv - intended for the purpose of model testing.\n* gender_submission.csv - an example submission data set based on a model that predicts survival based on gender only.\n\nIt doesn't appear to be a violation (constraint) of competition rules to augment this data with additional data sources. Data enrichement can certainly improve model accuracy. However, with exception to a small of cases (called out in each section), I have restricted the modelling and tuning stages to using data provided by the competition organiser. I wanted to exhaust the data provided and explore what's possible with the data provided and good science.\n\nWith a good understanding of the problem and the available data, let's get a copy of the data and start exploring.","756319df":"Data wrangling requires developing solid foundational skills working with DataFrames, so aligned with the purpose of this notebook, let's compliment Pandas.DataFrame.profile_report() and explore the data manually.","edf9bd5d":"# Model Implementation","41ad8058":"Let's reduce the number of title by grouping together minority titles. What's a minority? Let's choose groups with < 1% of total passengers i.e groups < 9 people.","4b2b0116":"### Title Information\nWhile I believe that most of the information contained within Name isn't particularly useful, different titles most probably will correlate strongly with Survived. Let's extract the title information.","95d1ac96":"* 23% of passengers have an assigned cabin.\n* 72% of cabins listed are unique. This may be due to multiple occupancy rooms.\n* some passengers are associated with multiple cabins. For example, the Fortune family resided in C23, C25 and C27.\n* 24% of Ticket numbers are not unique. Investigation shows that family members commonly had the same ticket number. They also have the same Fare ... can we reliably use this to determine an accurate Fare for passengers with the same ticket number? Let's explore this further later.\n* Everyone's recorded name is unique.\n\nNow let's look at which of the features linearly correlates most strongly with survival.","dda5cc66":"The passengers could be a gradmother\/grand daughter or possibly Miss Amelie is a maid\/helper.\n\n**Note:** A Google search for Icard, Miss Amelie tells us that she was the maid to Mrs George Nelson Stone. They boarded at Southampton (mode Embarkation port) so our decision to use mode turned out well.","3929078d":"## Random Forest Classifier","092f957c":"> ## Feature Engineering","eb0e36b7":"# Data Preparation\nFollowing a cursory exploration of the data, let us now prepare the data for model building. This means:\n\n* outliers - we may revist in a later version of the notebook.\n* address missing data\n* preparing category data\n* feature engineering\n* scaling\/normalisation (pending models that require it)","049e95dc":"## Feature Scaling\nWhile not necessary for some modelling algorithms e.g Decision Tree, we need to scale the data to avoid particular features dominating our model.","dc0ca0ae":"With our revised set of features, let's use a heatmap to view not only the linear correlation between each feature and target but also between features.","76b87c37":"We can see that:\n\n* passenger ages range from less than six months to eighty years.\n* there's an age spread of 42 years in the fourth quartile indicating relatively few older passengers.\n* there's a large spread in fares, ranging from zero (does this correspond to crew members?) to > 512. The histogram of 'Fare' data and the quartile data show that fares are heavily skewed towards the bottom end.\n* 76% of passengers travelled without either a parent or child\n* 68% of passengers travelled without a sibling or spouse\n* It is stated in the competition details that that total no. of passengers and crew was 2224. Our training data therefore represents 40% of those onboard.\n\nLet us now explore the category data.","313f6c2b":"Let's combine similar titles such as Ms:Miss, Mlle:Miss and Mme:Mrs","e7699f92":"We are now ready to revist imputing the missing age values. Let's start with using KNeighborsRegressor as the estimator then potentially explore alternatives later.","aa9704d6":"The passengers are travelling together (same ticket) and reside in the same cabin. They are not siblings\/parent\/child\/spouse. Let's see if anyone else is in their cabin.","80e1cbc5":"Confirm that all the missing Age data has been replaced.","eb5c19e9":"## Outliers\n\nThe three fares > $500 may be outliers. Before we simply exclude them, we need to base this on them being a statsitical outlier. We'll look into this in a later notebook; for now, we will leave them alone.","f3f05975":"# Table of Contents\n <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Purpose\" data-toc-modified-id=\"Purpose-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Purpose<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Framework\" data-toc-modified-id=\"Framework-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Framework<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#References\" data-toc-modified-id=\"References-13\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>References<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Problem Statement<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Background\" data-toc-modified-id=\"Background-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Background<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Baseline<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Future-Work\" data-toc-modified-id=\"Future-Work-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Future Work<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Data-Gathering\" data-toc-modified-id=\"Data-Gathering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data Gathering<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Data-Structures-and-Variables\" data-toc-modified-id=\"Data-Structures-and-Variables-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Data Structures and Variables<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Data Exploration<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Data Preparation<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Outliers\" data-toc-modified-id=\"Outliers-51\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Outliers<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Missing-Data\" data-toc-modified-id=\"Missing-Data-52\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Missing Data<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-53\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Feature Engineering<\/a><\/div><div class=\"lev3 toc-item\"><a href=\"#Title-Information\" data-toc-modified-id=\"Title-Information-531\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;<\/span>Title Information<\/a><\/div><div class=\"lev3 toc-item\"><a href=\"#Dropping-Features\" data-toc-modified-id=\"Dropping-Features-532\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;<\/span>Dropping Features<\/a><\/div><div class=\"lev4 toc-item\"><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-5321\"><span class=\"toc-item-num\">5.3.2.1&nbsp;&nbsp;<\/span>Feature Selection<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Separating-the-data-into-features-and-labels\" data-toc-modified-id=\"Separating-the-data-into-features-and-labels-54\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Separating the data into features and labels<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-55\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>Feature Scaling<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Model-Building\" data-toc-modified-id=\"Model-Building-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Model Building<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Decision Tree Classifier<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-63\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Random Forest Classifier<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Model-Tuning\" data-toc-modified-id=\"Model-Tuning-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Model Tuning<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-71\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-72\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Decision Tree<\/a><\/div><div class=\"lev2 toc-item\"><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-73\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Random Forest Classifier<\/a><\/div><div class=\"lev1 toc-item\"><a href=\"#Model-Implementation\" data-toc-modified-id=\"Model-Implementation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Model Implementation<\/a><\/div>","406b84b5":"We now choose the model with the best grid search cross-validation score and use it to predict whether each passenger in the test data set survived or not.\n\nIn addition to generating the final prediction submissions, let's create a record of each model's best parameters, estimators and score."}}