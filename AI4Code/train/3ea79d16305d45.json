{"cell_type":{"ea39f9f9":"code","b09981b2":"code","91746216":"code","02dd5ef2":"code","ac1d539f":"code","f1f4ed8a":"code","053afb02":"code","dbed33d0":"code","6604ce3a":"code","0ea28e8b":"code","62938dda":"code","25b3e2a8":"code","b2110d90":"code","31e74b79":"code","8bba28b2":"code","4592d950":"code","99233d83":"code","ef9bd874":"code","a459dbb6":"code","bdbfe1ae":"code","1e9d40bd":"code","b5b0356a":"code","d76ea497":"code","096c5832":"code","d468b646":"code","381588d9":"code","30cb447a":"code","c96b2669":"code","9c26eef3":"code","3ab20d2d":"code","6b72dd04":"code","2110c75e":"code","57758dc2":"markdown","19768612":"markdown","f40aef62":"markdown","56b269aa":"markdown","2d566304":"markdown","fa1a6307":"markdown","0052a18f":"markdown","843408c0":"markdown","23ec2bc6":"markdown","baf38fe6":"markdown","0ac8925d":"markdown","2918374b":"markdown","2dd7f47f":"markdown","d2b87bbc":"markdown","67c773e1":"markdown","1bdef9d9":"markdown","abb23d0f":"markdown","4c844356":"markdown","f3d523bd":"markdown","f5d2377c":"markdown","51859682":"markdown","ab89a11d":"markdown","07603956":"markdown","c92d366a":"markdown","27096997":"markdown","f39cab70":"markdown","095cfd0c":"markdown","733f4fde":"markdown","4b25c615":"markdown"},"source":{"ea39f9f9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport glob\nimport cv2\nimport os\n\nfrom colorama import Fore, Back, Style\n\n# Setting color palette.\nplt.rcdefaults()\nplt.style.use('dark_background')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b09981b2":"# Assigning paths to variables\nINPUT_PATH = os.path.join('..', 'input')\nDATASET_PATH = os.path.join(INPUT_PATH, 'landmark-recognition-2020')\nTRAIN_IMAGE_PATH = os.path.join(DATASET_PATH, 'train')\nTEST_IMAGE_PATH = os.path.join(DATASET_PATH, 'test')\nTRAIN_CSV_PATH = os.path.join(DATASET_PATH, 'train.csv')\nSUBMISSION_CSV_PATH = os.path.join(DATASET_PATH, 'sample_submission.csv')","91746216":"train = pd.read_csv(TRAIN_CSV_PATH)\nprint(\"training dataset has {} rows and {} columns\".format(train.shape[0],train.shape[1]))\n\nsubmission = pd.read_csv(SUBMISSION_CSV_PATH)\nprint(\"submission dataset has {} rows and {} columns \\n\".format(submission.shape[0],submission.shape[1]))","02dd5ef2":"# understand folder structure\nprint(Fore.YELLOW + \"If you want to access image a40d00dc4fcc3a10, you should traverse as shown below:\\n\",Style.RESET_ALL)\n\nprint(Fore.GREEN + f\"Image name: {train['id'].iloc[9]}\\n\",Style.RESET_ALL)\n\nprint(Fore.BLUE + f\"First folder to look inside: {train['id'][9][0]}\")\nprint(Fore.BLUE + f\"Second folder to look inside: {train['id'][9][1]}\")\nprint(Fore.BLUE + f\"Second folder to look inside: {train['id'][9][2]}\",Style.RESET_ALL)","ac1d539f":"print(Fore.BLUE + f\"{'---'*20} \\n Mapping for Training Data \\n {'---'*20}\")\ndata_label_dict = {'image': [], 'target': []}\nfor i in tqdm(range(train.shape[0])):\n    data_label_dict['image'].append(\n        TRAIN_IMAGE_PATH + '\/' +\n        train['id'][i][0] + '\/' + \n        train['id'][i][1]+ '\/' +\n        train['id'][i][2]+ '\/' +\n        train['id'][i] + \".jpg\")\n    data_label_dict['target'].append(\n        train['landmark_id'][i])\n\n#Convert to dataframe\ntrain_pathlabel = pd.DataFrame(data_label_dict)\nprint(train_pathlabel.head())\n    \nprint(Fore.BLUE + f\"{'---'*20} \\n Mapping for Test Data \\n {'---'*20}\",Style.RESET_ALL)\ndata_label_dict = {'image': []}\nfor i in tqdm(range(submission.shape[0])):\n    data_label_dict['image'].append(\n        TEST_IMAGE_PATH + '\/' +\n        submission['id'][i][0] + '\/' + \n        submission['id'][i][1]+ '\/' +\n        submission['id'][i][2]+ '\/' +\n        submission['id'][i] + \".jpg\")\n\ntest_pathlabel = pd.DataFrame(data_label_dict)\nprint(test_pathlabel.head())","f1f4ed8a":"# list of unique landmark ids\ntrain.landmark_id.unique()","053afb02":"# count of unique landmark_ids\nprint(\"There are\", train.landmark_id.nunique(), \"landmarks in the training dataset\")","dbed33d0":"# each class count-wise\ntrain.landmark_id.value_counts()","6604ce3a":"files = train_pathlabel.image[:10]\nprint(Fore.BLUE + \"Shape of files from training dataset\",Style.RESET_ALL)\nfor i in range(10):\n    im = cv2.imread(files[i])\n    print(im.shape)\n\n\nprint(\"------------------------------------\")    \nprint(\"------------------------------------\")    \nprint(\"------------------------------------\")    \n\nfiles = test_pathlabel.image[:10]\nprint(Fore.BLUE + \"Shape of files from test dataset\",Style.RESET_ALL)\nfor i in range(10):\n    im = cv2.imread(files[i])\n    print(im.shape)","0ea28e8b":"plt.figure(figsize = (12, 8))\n\nsns.kdeplot(train['landmark_id'], color=\"yellow\",shade=True)\nplt.xlabel(\"LandMark IDs\")\nplt.ylabel(\"Probability Density\")\nplt.title('Class Distribution - Density plot')\n\nplt.show()","62938dda":"fig = plt.figure(figsize = (12,8))\n\ncount = train.landmark_id.value_counts().sort_values(ascending=False)[:6]\n\nsns.countplot(x=train.landmark_id,\n             order = train.landmark_id.value_counts().sort_values(ascending=False).iloc[:6].index)\n\nplt.xlabel(\"LandMark Id\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Top 6 Classes in the Dataset\")\n\nplt.show()","25b3e2a8":"top6 = train.landmark_id.value_counts().sort_values(ascending=False)[:6].index\n\nimages = []\n\nfor i in range(6):\n    img=cv2.imread(train_pathlabel[train_pathlabel.target == top6[i]]['image'].values[1])   \n    images.append(img)\n\nf, ax = plt.subplots(3,2, figsize=(20,15))\nfor i, img in enumerate(images):        \n        ax[i\/\/2, i%2].imshow(img)\n        ax[i\/\/2, i%2].axis('off')\n       ","b2110d90":"fig = plt.figure(figsize = (12,8))\n\ncount = train.landmark_id.value_counts().sort_values(ascending=False)[:50]\n\nsns.countplot(x=train.landmark_id,\n             order = train.landmark_id.value_counts().sort_values(ascending=False).iloc[:50].index)\n\nplt.xticks(rotation = 90)\n\nplt.xlabel(\"LandMark Id\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Top 50 Classes in the Dataset\")\n\nplt.show()","31e74b79":"top50 = train.landmark_id.value_counts().sort_values(ascending=False).index[:50]\n\nimages = []\n\nfor i in range(50):\n    img=cv2.imread(train_pathlabel[train_pathlabel.target == top50[i]]['image'].values[1])   \n    images.append(img)\n\nf, ax = plt.subplots(10,5, figsize=(20,15))\nfor i, img in enumerate(images):        \n        ax[i\/\/5, i%5].imshow(img)\n        ax[i\/\/5, i%5].axis('off')\n       ","8bba28b2":"fig = plt.figure(figsize = (12,8))\n\ncount = train.landmark_id.value_counts()[-6:]\n\nsns.countplot(x=train.landmark_id,\n             order = train_pathlabel.target.value_counts().iloc[-6:].index)\n\nplt.xlabel(\"LandMark Id\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Bottom 6 Classes in the Dataset\")\n\nplt.show()","4592d950":"bottom6 = train.landmark_id.value_counts()[-6:].index\n\nimages = []\n\nfor i in range(6):\n    img=cv2.imread(train_pathlabel[train_pathlabel.target == bottom6[i]]['image'].values[1])   \n    images.append(img)\n\nf, ax = plt.subplots(3,2, figsize=(20,15))\nfor i, img in enumerate(images):        \n        ax[i\/\/2, i%2].imshow(img)\n        ax[i\/\/2, i%2].axis('off')\n       ","99233d83":"top5 = train.landmark_id.value_counts().sort_values(ascending=False).index[:5]\nfor i in range(5):\n    images = []      \n    for j in range(12):\n        img=cv2.imread(train_pathlabel[train_pathlabel.target == top5[i]]['image'].values[j])   \n        images.append(img)           \n    f, ax = plt.subplots(3,4,figsize=(20,15))\n    for k, img in enumerate(images):        \n        ax[k\/\/4, k%4].imshow(img)\n        ax[k\/\/4, k%4].axis('off')\nplt.show()","ef9bd874":"files = train_pathlabel.image[11:23]\n\nimages = []\n\nfor i in range(11,23):    \n    img=cv2.imread(files[i])   \n    images.append(img)\nf, ax = plt.subplots(3,4, figsize=(20,15))\nfor i, img in enumerate(images):\n        ax[i\/\/4, i%4].imshow(img)\n        ax[i\/\/4, i%4].axis('off')","a459dbb6":"files = test_pathlabel.image[11:23]\nimages = []\n\nfor i in range(11,23):\n    img=cv2.imread(files[i])   \n    images.append(img)\nf, ax = plt.subplots(3,4, figsize=(20,15))\nfor i, img in enumerate(images):\n        ax[i\/\/4, i%4].imshow(img)\n        ax[i\/\/4, i%4].axis('off')","bdbfe1ae":"files = train_pathlabel.image[:4]\n\nfig = plt.figure(figsize = (20,9))\n\nfor i in range(4):\n    img=cv2.imread(files[i])   \n    plt.subplot(2,2,i+1)\n    plt.hist(img.ravel(), bins = 256,color = 'gold')\n    \nplt.suptitle(\"Histogram for Grayscale Images\",fontsize = 25)    \nplt.show()","1e9d40bd":"fig = plt.figure(figsize = (20,9))\n\nfor i in range(4):\n    img=cv2.imread(files[i])   \n    plt.subplot(2,2,i+1)\n    plt.hist(img.ravel(), bins = 256,color = 'magenta',cumulative = True)\n\nplt.suptitle(\"Cumulative Histogram for Grayscale Images\",fontsize = 25)    \nplt.show()","b5b0356a":"fig = plt.figure(figsize = (20,9))\n\nfor i in range(4):\n    img=cv2.imread(files[i])   \n    plt.subplot(2,2,i+1)\n    plt.hist(img.ravel(), bins = 8, color = \"coral\")\n\nplt.suptitle(\"Cumulative Histogram for Grayscale Images - Bin Size = 8\",fontsize = 25)    \nplt.show()","d76ea497":"fig = plt.figure(figsize = (20,9))\n\nfor i in range(4):\n    img=cv2.imread(files[i])   \n    plt.subplot(2,2,i+1)\n    plt.hist(img.ravel(), bins = 256, color = 'orange', )\n    plt.hist(img[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n    plt.hist(img[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n    plt.hist(img[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n    plt.xlabel('Intensity Value')\n    plt.ylabel('Count')\n    plt.legend(['Total', 'Red_Channel', 'Green_Channel', 'Blue_Channel'])\n\nplt.suptitle(\"Color Histograms\",fontsize = 25)    \nplt.show()","096c5832":"import copy\nimport csv\nimport gc\nimport operator\nimport os\nimport pathlib\nimport shutil\n\nimport numpy as np\nimport PIL\nimport pydegensac\nfrom scipy import spatial\nimport tensorflow as tf","d468b646":"# Dataset parameters:\nINPUT_DIR = os.path.join('..', 'input')\n\nDATASET_DIR = os.path.join(INPUT_DIR, 'landmark-recognition-2020')\nTEST_IMAGE_DIR = os.path.join(DATASET_DIR, 'test')\nTRAIN_IMAGE_DIR = os.path.join(DATASET_DIR, 'train')\nTRAIN_LABELMAP_PATH = os.path.join(DATASET_DIR, 'train.csv')","381588d9":"# DEBUGGING PARAMS:\nNUM_PUBLIC_TRAIN_IMAGES = 1580470 # Used to detect if in session or re-run.\nMAX_NUM_EMBEDDINGS = -1  # Set to > 1 to subsample dataset while debugging.","30cb447a":"\n# Retrieval & re-ranking parameters:\nNUM_TO_RERANK = 3\nTOP_K = 3 # Number of retrieved images used to make prediction for a test image.\n","c96b2669":"# RANSAC parameters:\nMAX_INLIER_SCORE = 35\nMAX_REPROJECTION_ERROR = 7.0\nMAX_RANSAC_ITERATIONS = 8500000\nHOMOGRAPHY_CONFIDENCE = 0.99","9c26eef3":"# DELG model:\nSAVED_MODEL_DIR = '..\/input\/delg-saved-models\/local_and_global'\nDELG_MODEL = tf.saved_model.load(SAVED_MODEL_DIR)\nDELG_IMAGE_SCALES_TENSOR = tf.convert_to_tensor([0.70710677, 1.0, 1.4142135])\nDELG_SCORE_THRESHOLD_TENSOR = tf.constant(175.)\nDELG_INPUT_TENSOR_NAMES = [\n    'input_image:0', 'input_scales:0', 'input_abs_thres:0'\n]","3ab20d2d":"# Global feature extraction:\nNUM_EMBEDDING_DIMENSIONS = 2048\nGLOBAL_FEATURE_EXTRACTION_FN = DELG_MODEL.prune(DELG_INPUT_TENSOR_NAMES,\n                                                ['global_descriptors:0'])\n\n# Local feature extraction:\nLOCAL_FEATURE_NUM_TENSOR = tf.constant(1000)\nLOCAL_FEATURE_EXTRACTION_FN = DELG_MODEL.prune(\n    DELG_INPUT_TENSOR_NAMES + ['input_max_feature_num:0'],\n    ['boxes:0', 'features:0'])","6b72dd04":"def to_hex(image_id) -> str:\n  return '{0:0{1}x}'.format(image_id, 16)\n\n\ndef get_image_path(subset, image_id):\n  name = to_hex(image_id)\n  return os.path.join(DATASET_DIR, subset, name[0], name[1], name[2],\n                      '{}.jpg'.format(name))\n\n\ndef load_image_tensor(image_path):\n  return tf.convert_to_tensor(\n      np.array(PIL.Image.open(image_path).convert('RGB')))\n\n\ndef extract_global_features(image_root_dir):\n  \"\"\"Extracts embeddings for all the images in given `image_root_dir`.\"\"\"\n\n  image_paths = [x for x in pathlib.Path(image_root_dir).rglob('*.jpg')]\n\n  num_embeddings = len(image_paths)\n  if MAX_NUM_EMBEDDINGS > 0:\n    num_embeddings = min(MAX_NUM_EMBEDDINGS, num_embeddings)\n\n  ids = num_embeddings * [None]\n  embeddings = np.empty((num_embeddings, NUM_EMBEDDING_DIMENSIONS))\n\n  for i, image_path in enumerate(image_paths):\n    if i >= num_embeddings:\n      break\n\n    ids[i] = int(image_path.name.split('.')[0], 16)\n    image_tensor = load_image_tensor(image_path)\n    features = GLOBAL_FEATURE_EXTRACTION_FN(image_tensor,\n                                            DELG_IMAGE_SCALES_TENSOR,\n                                            DELG_SCORE_THRESHOLD_TENSOR)\n    embeddings[i, :] = tf.nn.l2_normalize(\n        tf.reduce_sum(features[0], axis=0, name='sum_pooling'),\n        axis=0,\n        name='final_l2_normalization').numpy()\n\n  return ids, embeddings\n\n\ndef extract_local_features(image_path):\n  \"\"\"Extracts local features for the given `image_path`.\"\"\"\n\n  image_tensor = load_image_tensor(image_path)\n\n  features = LOCAL_FEATURE_EXTRACTION_FN(image_tensor, DELG_IMAGE_SCALES_TENSOR,\n                                         DELG_SCORE_THRESHOLD_TENSOR,\n                                         LOCAL_FEATURE_NUM_TENSOR)\n\n  # Shape: (N, 2)\n  keypoints = tf.divide(\n      tf.add(\n          tf.gather(features[0], [0, 1], axis=1),\n          tf.gather(features[0], [2, 3], axis=1)), 2.0).numpy()\n\n  # Shape: (N, 128)\n  descriptors = tf.nn.l2_normalize(\n      features[1], axis=1, name='l2_normalization').numpy()\n\n  return keypoints, descriptors\n\n\ndef get_putative_matching_keypoints(test_keypoints,\n                                    test_descriptors,\n                                    train_keypoints,\n                                    train_descriptors,\n                                    max_distance=0.9):\n  \"\"\"Finds matches from `test_descriptors` to KD-tree of `train_descriptors`.\"\"\"\n\n  train_descriptor_tree = spatial.cKDTree(train_descriptors)\n  _, matches = train_descriptor_tree.query(\n      test_descriptors, distance_upper_bound=max_distance)\n\n  test_kp_count = test_keypoints.shape[0]\n  train_kp_count = train_keypoints.shape[0]\n\n  test_matching_keypoints = np.array([\n      test_keypoints[i,]\n      for i in range(test_kp_count)\n      if matches[i] != train_kp_count\n  ])\n  train_matching_keypoints = np.array([\n      train_keypoints[matches[i],]\n      for i in range(test_kp_count)\n      if matches[i] != train_kp_count\n  ])\n\n  return test_matching_keypoints, train_matching_keypoints\n\n\ndef get_num_inliers(test_keypoints, test_descriptors, train_keypoints,\n                    train_descriptors):\n  \"\"\"Returns the number of RANSAC inliers.\"\"\"\n\n  test_match_kp, train_match_kp = get_putative_matching_keypoints(\n      test_keypoints, test_descriptors, train_keypoints, train_descriptors)\n\n  if test_match_kp.shape[\n      0] <= 4:  # Min keypoints supported by `pydegensac.findHomography()`\n    return 0\n\n  try:\n    _, mask = pydegensac.findHomography(test_match_kp, train_match_kp,\n                                        MAX_REPROJECTION_ERROR,\n                                        HOMOGRAPHY_CONFIDENCE,\n                                        MAX_RANSAC_ITERATIONS)\n  except np.linalg.LinAlgError:  # When det(H)=0, can't invert matrix.\n    return 0\n\n  return int(copy.deepcopy(mask).astype(np.float32).sum())\n\n\ndef get_total_score(num_inliers, global_score):\n  local_score = min(num_inliers, MAX_INLIER_SCORE) \/ MAX_INLIER_SCORE\n  return local_score + global_score\n\n\ndef rescore_and_rerank_by_num_inliers(test_image_id,\n                                      train_ids_labels_and_scores):\n  \"\"\"Returns rescored and sorted training images by local feature extraction.\"\"\"\n\n  test_image_path = get_image_path('test', test_image_id)\n  test_keypoints, test_descriptors = extract_local_features(test_image_path)\n\n  for i in range(len(train_ids_labels_and_scores)):\n    train_image_id, label, global_score = train_ids_labels_and_scores[i]\n\n    train_image_path = get_image_path('train', train_image_id)\n    train_keypoints, train_descriptors = extract_local_features(\n        train_image_path)\n\n    num_inliers = get_num_inliers(test_keypoints, test_descriptors,\n                                  train_keypoints, train_descriptors)\n    total_score = get_total_score(num_inliers, global_score)\n    train_ids_labels_and_scores[i] = (train_image_id, label, total_score)\n\n  train_ids_labels_and_scores.sort(key=lambda x: x[2], reverse=True)\n\n  return train_ids_labels_and_scores\n\n\ndef load_labelmap():\n  with open(TRAIN_LABELMAP_PATH, mode='r') as csv_file:\n    csv_reader = csv.DictReader(csv_file)\n    labelmap = {row['id']: row['landmark_id'] for row in csv_reader}\n\n  return labelmap\n\n\ndef get_prediction_map(test_ids, train_ids_labels_and_scores):\n  \"\"\"Makes dict from test ids and ranked training ids, labels, scores.\"\"\"\n\n  prediction_map = dict()\n\n  for test_index, test_id in enumerate(test_ids):\n    hex_test_id = to_hex(test_id)\n\n    aggregate_scores = {}\n    for _, label, score in train_ids_labels_and_scores[test_index][:TOP_K]:\n      if label not in aggregate_scores:\n        aggregate_scores[label] = 0\n      aggregate_scores[label] += score\n\n    label, score = max(aggregate_scores.items(), key=operator.itemgetter(1))\n\n    prediction_map[hex_test_id] = {'score': score, 'class': label}\n\n  return prediction_map\n\n\ndef get_predictions(labelmap):\n  \"\"\"Gets predictions using embedding similarity and local feature reranking.\"\"\"\n\n  test_ids, test_embeddings = extract_global_features(TEST_IMAGE_DIR)\n\n  train_ids, train_embeddings = extract_global_features(TRAIN_IMAGE_DIR)\n\n  train_ids_labels_and_scores = [None] * test_embeddings.shape[0]\n\n  # Using (slow) for-loop, as distance matrix doesn't fit in memory.\n  for test_index in range(test_embeddings.shape[0]):\n    distances = spatial.distance.cdist(\n        test_embeddings[np.newaxis, test_index, :], train_embeddings,\n        'cosine')[0]\n    partition = np.argpartition(distances, NUM_TO_RERANK)[:NUM_TO_RERANK]\n\n    nearest = sorted([(train_ids[p], distances[p]) for p in partition],\n                     key=lambda x: x[1])\n\n    train_ids_labels_and_scores[test_index] = [\n        (train_id, labelmap[to_hex(train_id)], 1. - cosine_distance)\n        for train_id, cosine_distance in nearest\n    ]\n\n  del test_embeddings\n  del train_embeddings\n  del labelmap\n  gc.collect()\n\n  pre_verification_predictions = get_prediction_map(\n      test_ids, train_ids_labels_and_scores)\n\n#  return None, pre_verification_predictions\n\n  for test_index, test_id in enumerate(test_ids):\n    train_ids_labels_and_scores[test_index] = rescore_and_rerank_by_num_inliers(\n        test_id, train_ids_labels_and_scores[test_index])\n\n  post_verification_predictions = get_prediction_map(\n      test_ids, train_ids_labels_and_scores)\n\n  return pre_verification_predictions, post_verification_predictions\n\n\ndef save_submission_csv(predictions=None):\n  \"\"\"Saves optional `predictions` as submission.csv.\n\n  The csv has columns {id, landmarks}. The landmarks column is a string\n  containing the label and score for the id, separated by a ws delimeter.\n\n  If `predictions` is `None` (default), submission.csv is copied from\n  sample_submission.csv in `IMAGE_DIR`.\n\n  Args:\n    predictions: Optional dict of image ids to dicts with keys {class, score}.\n  \"\"\"\n\n  if predictions is None:\n    # Dummy submission!\n    shutil.copyfile(\n        os.path.join(DATASET_DIR, 'sample_submission.csv'), 'submission.csv')\n    return\n\n  with open('submission.csv', 'w') as submission_csv:\n    csv_writer = csv.DictWriter(submission_csv, fieldnames=['id', 'landmarks'])\n    csv_writer.writeheader()\n    for image_id, prediction in predictions.items():\n      label = prediction['class']\n      score = prediction['score']\n      csv_writer.writerow({'id': image_id, 'landmarks': f'{label} {score}'})","2110c75e":"def main():\n  labelmap = load_labelmap()\n  num_training_images = len(labelmap.keys())\n  print(f'Found {num_training_images} training images.')\n\n  if num_training_images == NUM_PUBLIC_TRAIN_IMAGES:\n    print(\n        f'Found {NUM_PUBLIC_TRAIN_IMAGES} training images. Copying sample submission.'\n    )\n    save_submission_csv()\n    return\n\n  _, post_verification_predictions = get_predictions(labelmap)\n  save_submission_csv(post_verification_predictions)\n\nmain()","57758dc2":"### 1. Grayscale Image\n\n* We will loaded the grayscale images here & generated its histogram \n\n* Since the images are stored in the form of a 2D ordered matrix we converted it to a 1D array using the ravel() method","19768612":"# Class Distribution Analysis","f40aef62":"<div class=\"alert alert-block alert-info\">    \nJust 2 images per class for the bottom 6 classes\n<\/div>","56b269aa":"### Bottom 6 Class Categories","2d566304":"<div class=\"alert alert-block alert-info\">\n    \n* Landmark id '138982' has more than 6000 images. \n* Next Top 5 clasess in this table has less than 2500 images\n* Rest of the classes has less than 1000 samples in the training dataset\n\n<\/div>","fa1a6307":"### Top 50 Class Categories","0052a18f":"### Let's declare PATH variables","843408c0":"# Check File Sizes of first 10 files","23ec2bc6":"### Let's check out images from the bottom 6 classes","baf38fe6":"# Introduction\n\nThis is the third Landmark Recognition competition with a new set of test images.\n\nThis technology (predicting landmarks labels) directly from image pixels, will help people better understand and organize their photo collections. \n\n<div class=\"alert alert-block alert-info\">\n<b>Biggest challenge in this competition:<\/b> \n\nThis seems to be an extremely challenging competition because it contains a much larger number of classes (there are more than 81K classes in this challenge), and the number of training examples per class may not be very large. <\/div>\n\n\n<div class=\"alert alert-block alert-info\">\n<b>Another Challenge:<\/b> \n\nFor quite a lot of classes, there are only 2 images provided in the training set and for most of the classes training samples are less than 100 for that particular class, This means training dataset is highly imbalanced.<\/div>","0ac8925d":"# Few Images from test dataset","2918374b":"### 2. Cumulative histogram\nThe cumulative histogram is a special histogram that can be derived from the normal histogram. \nWe find the counts of each intensity value from 0\u2013255 and then add each subsequent counts","2dd7f47f":"### Let's check out images from the top 6 classes","d2b87bbc":"# Some more Images from training set","67c773e1":"# Build dictionary to store image paths & labels","1bdef9d9":"# Load CSV files \ud83d\udcc3","abb23d0f":"### Density plot for class distribution","4c844356":"<h1><center>Google Landmark Recognition 2020<\/center><\/h1>","f3d523bd":"<div class=\"alert alert-block alert-info\">\n    \n* Landmark id '138982' has more than 6000 images\n* Next Top 5 clasess in this table have less than 2500 images\n<\/div>","f5d2377c":"# About the Kernel\n\n1. First version      - EDA\n2. 3rd to 5th Version - Model Building\n3. 6th Version - Some Bug fixing related to saving image paths to the training and test datasets","51859682":"### Top 6 Class Categories","ab89a11d":"### Let's check out images from the top 50 classes","07603956":"# Model Building","c92d366a":"### 4. Color Image\n\n* In color images, we have 3 color channels representing RGB. In Combined Color Histogram the intensity count is the sum of all three color channels.","27096997":"# Histogram of images from training dataset","f39cab70":"### 3. Grayscale Image - With bins = 8\nUsually, the range of intensity values of images is from [0\u2013255] in 8bits representation(2\u2078). \n\nBut images can be also represented using 2\u00b9\u2076, 2\u00b3\u00b2 bits and so on. In such cases the intensity range is high and it is hard to represent each intensity value in a histogram.\n\nWe use binning to overcome the above problem. Here we quantize the range into several buckets. For example,\n\nIf we quantize 0-255 into 8 bins, here our bins will be: 0-31, 32-63, 64-95, 96-127, 128-159, 160-191, 192-223, 224-255","095cfd0c":"<div class=\"alert alert-block alert-info\">\n    \n<b> Observations from the whole analysis done above:<\/b>\n* There are 81313 unique landmark_ids\n* There is only one landmark which has more than 6000 images\n* Number of images per landmark_id ranges from 2 to 6272.\n\n<\/div>","733f4fde":"# FOLDER STRUCTURE\n\n<div class=\"alert alert-block alert-info\">\nWe have Training Data saved in folder ranging `0 to 9` and `a to f`, We will see one example below to understand the folder structure\n<\/div>","4b25c615":"### We will now check few images from the top 5 classes"}}