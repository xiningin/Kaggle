{"cell_type":{"75f94fe2":"code","8b1b208b":"code","0d32315b":"code","9d73a93a":"code","7147bae6":"code","96502993":"code","93dcba64":"code","27243a41":"code","a1c6dafd":"code","24de75f9":"code","840d37f6":"code","122fb7f6":"code","7e0bbe91":"code","b1f9088b":"code","859ec149":"code","a1850cab":"code","118569be":"code","48341a42":"code","8453744c":"code","8c087bfe":"code","a9136d36":"code","8bc93e1d":"code","5ef591d7":"code","7260c56e":"code","8c8f4d6f":"code","6418350d":"code","80b35d4c":"code","95c3f6c2":"code","b097d2cb":"code","eeac3bba":"code","4e954a1d":"code","e1557000":"code","426d9cec":"code","fa25b7f5":"code","51154cb0":"code","a35d9627":"code","a61b7d59":"code","44a2a001":"code","71d44f54":"code","bc94bb8a":"code","1d9af2a9":"code","a69998e2":"code","bd45614b":"code","bc98cedb":"code","4c673d10":"code","deff1e6a":"code","efbf9abc":"code","6dce9183":"code","9760db39":"code","c90dea97":"code","c0a57a03":"code","c8cdd2ad":"code","c17f51bc":"code","24c9f5bd":"code","c11c3b7f":"code","68438b5e":"code","107781aa":"code","d1a5f248":"code","8fcaf016":"code","d0e40cc1":"code","e6575ddb":"code","fc5a57f2":"code","ce610095":"code","a810025f":"code","3882662d":"code","808bddd2":"code","a42112c3":"code","f429c0b3":"code","a94bcc78":"code","acf40d7e":"code","aea36d37":"code","f85d03e4":"code","586317eb":"code","4b21b7d6":"code","d456d177":"code","666791d4":"code","992ea728":"markdown","74125dfb":"markdown","8bfd0e18":"markdown","af86b106":"markdown","946f53f7":"markdown","441b67cc":"markdown","2a1a1222":"markdown","10758639":"markdown","3d620343":"markdown","8a9c4abb":"markdown","1f1f0577":"markdown","5642db4a":"markdown","2dee1ab8":"markdown","1221d0ab":"markdown","0350066e":"markdown","44b5417c":"markdown","35be35b8":"markdown","cea64fba":"markdown","df792078":"markdown","efbad94f":"markdown","74e9a05f":"markdown","44d8137f":"markdown","f47eacab":"markdown","6b349161":"markdown","e5845c43":"markdown","f404fcb9":"markdown","19a881f8":"markdown","b0ca338d":"markdown","4064ed8c":"markdown","32c61af3":"markdown","56374eee":"markdown","221116a5":"markdown","307fb9e1":"markdown","5531f132":"markdown","fc3a7ad8":"markdown","ad4d6cfb":"markdown","464bed92":"markdown","85beb1bb":"markdown","7d5f42c9":"markdown","b532f457":"markdown","5e42ea44":"markdown","399fe243":"markdown","3db889aa":"markdown","288eab06":"markdown","29944cc1":"markdown","90898c1b":"markdown","4c88c819":"markdown","463c2847":"markdown","ac656fe7":"markdown","ca28f86d":"markdown","521cf503":"markdown","f9f7d3a1":"markdown","ce53599c":"markdown","469b7542":"markdown","1d3fbd8e":"markdown","127a4517":"markdown","2774fc87":"markdown","1f3d8ba7":"markdown","48385093":"markdown","28cc7010":"markdown","daad8dce":"markdown","ffd77bbb":"markdown","9f12fe82":"markdown","db7f2e8f":"markdown","e038797b":"markdown","adf34066":"markdown","5e304175":"markdown","b397c379":"markdown","8cb0e65d":"markdown","e7249ca4":"markdown","2ef5b13f":"markdown","3ccc26ce":"markdown","23896fb1":"markdown"},"source":{"75f94fe2":"%matplotlib inline\n\nimport cv2\nimport random\nimport time\nfrom datetime import timedelta\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\n\ntqdm.pandas() # for using pandas.Series.progress_apply()\nimport plotly.express as px\nimport plotly.figure_factory as ff # for the distplot\nimport plotly.graph_objects as go # for the box plot without named columns\n\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.models import clone_model\nfrom keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization,Activation\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nfrom IPython.display import display\n\ntf.compat.v1.disable_eager_execution() # necessary since the last tf update","8b1b208b":"tf.__version__","0d32315b":"tf.compat.v1.reset_default_graph()","9d73a93a":"# Use this method to walk through a directory and display all the contents\n\n#import os\n#for dirname, _, filenames in os.walk('..\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","7147bae6":"path = '..\/input\/plant-pathology-2020-fgvc7\/'\nSAMPLE_LEN = 100\nIMG_PATH = path + 'images\/'\n\ndf_train = pd.read_csv(path + 'train.csv')\ndf_test = pd.read_csv(path + 'test.csv')\nsample_sub = pd.read_csv(path + 'sample_submission.csv')","96502993":"display(df_train.head())\ndisplay(df_test.head())\ndisplay(sample_sub.head())\n\nprint('df_train shape: ', df_train.shape)\nprint('df_test shape: ', df_test.shape)","93dcba64":"def load_image(image_id):\n    file_path = image_id + \".jpg\"\n    img = cv2.imread(IMG_PATH + file_path)\n    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\ntrain_images = df_train[\"image_id\"][:SAMPLE_LEN].progress_apply(load_image)","27243a41":"fig = px.imshow(cv2.resize(train_images[0], (205, 136)))\nfig.show()","a1c6dafd":"red_values = [np.mean(train_images[idx][:, :, 0]) for idx in range(len(train_images))]\ngreen_values = [np.mean(train_images[idx][:, :, 1]) for idx in range(len(train_images))]\nblue_values = [np.mean(train_images[idx][:, :, 2]) for idx in range(len(train_images))]\nvalues = [np.mean(train_images[idx]) for idx in range(len(train_images))]","24de75f9":"plt.figure(figsize=[16, 14])\n\nplt.subplot(411)\nsns.distplot(values, hist=True, rug=True, bins=50, color='grey')\nplt.title('Distribution of channel values.')\n\nplt.subplot(412)\nsns.distplot(red_values, hist=True, rug=True, bins=50, color='red')\nplt.title('Distribution of red channel values.')\n\nplt.subplot(413)\nsns.distplot(green_values, hist=True, rug=True, bins=50, color='green')\nplt.title('Distribution of green channel values.')\n\nplt.subplot(414)\nsns.distplot(blue_values, hist=True, rug=True, bins=50, color='blue')\nplt.title('Distribution of blue channel values.');","840d37f6":"comb = [red_values, green_values, blue_values]\n\nfig = plt.figure(figsize=[8, 6])\nplt.boxplot(comb, labels=['Red', 'Green', 'Blue'])\nplt.title('Boxplots for different color channels')\nplt.ylabel('Color value')\nplt.xlabel('Color channel');","122fb7f6":"fig_all = ff.create_distplot([values], group_labels=[\"All Channels\"], colors=[\"purple\"])\nfig_all.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig_all.data[0].marker.line.width = 0.5\ndisplay(fig_all)\n\nfig_r = ff.create_distplot([red_values], group_labels=[\"Red Channel\"], colors=[\"red\"])\nfig_r.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig_r.data[0].marker.line.width = 0.5\ndisplay(fig_r)\n\nfig_g = ff.create_distplot([green_values], group_labels=[\"Green Channel\"], colors=[\"green\"])\nfig_g.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig_g.data[0].marker.line.width = 0.5\ndisplay(fig_g)\n\nfig_b = ff.create_distplot([blue_values], group_labels=[\"Blue Channel\"], colors=[\"blue\"])\nfig_b.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig_b.data[0].marker.line.width = 0.5\ndisplay(fig_b)","7e0bbe91":"fig = ff.create_distplot([red_values, green_values, blue_values], \n                         group_labels=['Red Channel', 'Green Channel', 'Blue Channel'], \n                         colors=['red', 'green', 'blue'])\n\nfig.update_layout(title_text='Distribution of all channel values')\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.data[1].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[1].marker.line.width = 0.5\nfig.data[2].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[2].marker.line.width = 0.5\n\nfig","b1f9088b":"fig = go.Figure()\nfig.add_trace(go.Box(y=red_values, name=\"Red Channel\", marker_color='red'))\nfig.add_trace(go.Box(y=green_values, name=\"Green Channel\", marker_color='green'))\nfig.add_trace(go.Box(y=blue_values, name=\"Blue Channel\", marker_color='blue'))\nfig.update_layout(yaxis_title=\"RGB value\", xaxis_title=\"Color channel\",\n                  title=\"RGB value vs. Color channel\")","859ec149":"# Notice: df_train is cut off at SAMPLE_LEN\n\ndef plot_sample_leafs(symp, rows=2, cols=3):\n    \n    last_row_cols = None\n    \n    # get indexes of images with specified symptom\n    df = df_train.iloc[:SAMPLE_LEN, :]\n    indexes = df.index[df[symp] == 1].tolist()\n    \n    # check if the df contains rows*cols number of images with specified symptom\n    if len(indexes) >= rows*cols:\n        sample_indexes = random.sample(indexes, rows*cols)\n    else:\n        sample_indexes = indexes\n        # check if the indexes can be divided into full rows\n        if len(indexes)%cols == 0:\n            rows = int(len(indexes)\/cols)\n        # otherwise define the number of pictures in the last row\n        else:\n            rows = int(len(indexes)\/cols) + 1\n            last_row_cols = len(indexes)%cols\n            \n    # create axes\n    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(30, (20\/3)*rows))\n    \n    # show images\n    for r in range(rows):\n        for c in range(cols):\n            sample_index = sample_indexes.pop()\n            ax[r, c].imshow(cv2.resize(train_images[sample_index], (205, 136)))\n            \n            # break the loop if no more axes can be filled with images\n            if last_row_cols is not None and (r==rows-1 and c==last_row_cols-1):\n                print('Not enough images with specified symp to fill all axis.')\n                break\n            \n    plt.show()","a1850cab":"plot_sample_leafs('healthy')","118569be":"plot_sample_leafs('scab')","48341a42":"plot_sample_leafs('rust')","8453744c":"plot_sample_leafs('multiple_diseases')","8c087bfe":"fig = px.parallel_categories(df_train[[\"healthy\", \"scab\", \"rust\", \"multiple_diseases\"]], \n                             color=\"healthy\", color_continuous_scale=\"sunset\",\n                             title=\"Parallel categories plot of targets\")\nfig","a9136d36":"fig = go.Figure([go.Pie(labels=df_train.columns[1:],\n           values=df_train.iloc[:, 1:].sum().values)])\nfig.update_layout(title_text=\"Pie chart of targets\", template=\"simple_white\")\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.5\nfig.show()","8bc93e1d":"def plotly_hist(df, col, title, is_wanted=True):\n    true_color = 'green'\n    false_color = 'red'\n    if is_wanted is not True:\n        true_color = 'red'\n        false_color = 'green'\n    df = df[col].apply(bool).apply(str)\n    fig = px.histogram(df, x=col, title=title, color=col, color_discrete_map={\n                                                                    'True': true_color,\n                                                                    'False': false_color})\n    fig.update_layout(template=\"simple_white\")\n    fig.data[0].marker.line.color = 'rgb(0, 0, 0)'\n    fig.data[0].marker.line.width = 0.5\n    fig.data[1].marker.line.color = 'rgb(0, 0, 0)'\n    fig.data[1].marker.line.width = 0.5\n    display(fig)\n    \nplotly_hist(df_train, 'healthy', 'Healthy distribution')\nplotly_hist(df_train, 'scab', 'Scab distribution', is_wanted=False)\nplotly_hist(df_train, 'rust', 'Rust distribution', is_wanted=False)\nplotly_hist(df_train, 'multiple_diseases', 'Multiple diseases distribution', is_wanted=False)","5ef591d7":"# This method processes the canny edges for a given image and calculates the bounding box of the object\n# from the part of the image which contains edge pixels.\n# Using Canny edge detection to identify a bounding box works well for images with a blurred background.\ndef show_edges_and_bounding_box(img):\n    emb_img = img.copy()\n    edges = cv2.Canny(img, 100, 200)\n    edge_coors = []\n    for i in range(edges.shape[0]):\n        for j in range(edges.shape[1]):\n            if edges[i][j] != 0:\n                edge_coors.append((i, j))\n    \n    row_min = edge_coors[np.argsort([coor[0] for coor in edge_coors])[0]][0]\n    row_max = edge_coors[np.argsort([coor[0] for coor in edge_coors])[-1]][0]\n    col_min = edge_coors[np.argsort([coor[1] for coor in edge_coors])[0]][1]\n    col_max = edge_coors[np.argsort([coor[1] for coor in edge_coors])[-1]][1]\n    new_img = img[row_min:row_max, col_min:col_max]\n    \n    emb_img[row_min-10:row_min+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_max-10:row_max+10, col_min:col_max] = [255, 0, 0]\n    emb_img[row_min:row_max, col_min-10:col_min+10] = [255, 0, 0]\n    emb_img[row_min:row_max, col_max-10:col_max+10] = [255, 0, 0]\n    \n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('Canny Edges', fontsize=24)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=24)\n    plt.show()","7260c56e":"show_edges_and_bounding_box(train_images[0])\nshow_edges_and_bounding_box(train_images[1])\nshow_edges_and_bounding_box(train_images[2])","8c8f4d6f":"def show_flips(img):\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(cv2.flip(img, 0))\n    ax[1].set_title('Vertical Flip', fontsize=24)\n    ax[2].imshow(cv2.flip(img, 1))\n    ax[2].set_title('Horizontal Flip', fontsize=24)\n    plt.show()","6418350d":"show_flips(train_images[0])\nshow_flips(train_images[1])\nshow_flips(train_images[2])","80b35d4c":"def show_blurred_image(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(cv2.blur(img, (100, 100)))\n    ax[1].set_title('Blurred Image', fontsize=24)\n    plt.show()","95c3f6c2":"show_blurred_image(train_images[0])","b097d2cb":"# this method visualizes the effect of a convolution with a 7x7 filter where every value is 1\/25\n\ndef show_conv(img):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 20))\n    kernel = np.ones((7, 7), np.float32)\/25\n    conv = cv2.filter2D(img, -1, kernel) # ddepth=-1 ensure output.shape is equal to input.shape\n    ax[0].imshow(img)\n    ax[0].set_title('Original Image', fontsize=24)\n    ax[1].imshow(conv)\n    ax[1].set_title('Convolved Image', fontsize=24)\n    plt.show()","eeac3bba":"show_conv(train_images[0])","4e954a1d":"# Number of classes\nnum_classes = 4\n\n# Number of colour channels for the images\nnum_channels = 3\n\n# Tuple with height and width and channels of images used to reshape arrays.\nimg_shape = train_images[0].shape\n\nprint('img_shape: ', img_shape, '\\n'\n     'img_width: ', train_images[0].shape[1], '\\n'\n     'img_height: ', train_images[0].shape[0], '\\n'\n     'num_classes: ', num_classes, '\\n'\n     'num_channels: ', num_channels)","e1557000":"img_width = 128\nimg_height = 128\nimg_shape = (img_height, img_width, num_channels)\n\n# TensorFlow functions require numpy arrays\nlabels = df_train.drop('image_id', axis=1).to_numpy()\n\nimage_ids = df_train['image_id'].to_numpy()\n\n# encode as \n# healthy = 0\n# multiple_diseases = 1\n# rust = 2\n# scab = 3\nlabels_cls = df_train['healthy']*1 + df_train['multiple_diseases']*2 + df_train['rust']*3 + df_train['scab']*4 - 1 \n\n# set to 0 to train on all available data\nVALIDATION_SIZE = 181\n\n# split data into training & validation\n# we do not specify x_train here because the images will be loaded in by batch lateron so save memory\n#### DELETE x_test = train_images[:VALIDATION_SIZE]\ny_test = labels[:VALIDATION_SIZE]\ntest_ids = image_ids[:VALIDATION_SIZE]\n# keep a single target vector for print_test_accuracy() function\ny_test_cls = labels_cls[:VALIDATION_SIZE]\n\n###### DELETE x_train = train_images[VALIDATION_SIZE:]\ny_train = labels[VALIDATION_SIZE:]\ntrain_ids = image_ids[VALIDATION_SIZE:]\ny_train_cls = labels_cls[VALIDATION_SIZE:]\n\n\nprint('train ids shape: {}'.format(train_ids.shape))\nprint('validation ids shape: {}'.format(test_ids.shape))","426d9cec":"def new_weights(shape):\n    return tf.Variable(tf.random.truncated_normal(shape, stddev=0.05))\n\ndef new_biases(length):\n    return tf.Variable(tf.constant(0.05, shape=[length]))\n\ndef new_conv_layer(input,              # The previous layer.\n                   num_input_channels, # Num. channels in prev. layer.\n                   filter_size,        # Width and height of each filter.\n                   num_filters,        # Number of filters.\n                   use_pooling=True):  # Use 2x2 max-pooling.\n\n    # Shape of the filter-weights for the convolution.\n    # This format is determined by the TensorFlow API.\n    shape = [filter_size, filter_size, num_input_channels, num_filters]\n\n    # Create new weights aka. filters with the given shape.\n    weights = new_weights(shape=shape)\n\n    # Create new biases, one for each filter.\n    biases = new_biases(length=num_filters)\n\n    # Create the TensorFlow operation for convolution.\n    # Note the strides are set to 1 in all dimensions.\n    # The first and last stride must always be 1,\n    # because the first is for the image-number and\n    # the last is for the input-channel.\n    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n    # is moved 2 pixels across the x- and y-axis of the image.\n    # The padding is set to 'SAME' which means the input image\n    # is padded with zeroes so the size of the output is the same.\n    layer = tf.nn.conv2d(input=input,\n                         filters=weights,\n                         strides=[1, 1, 1, 1],\n                         padding='SAME')\n\n    # Add the biases to the results of the convolution.\n    # A bias-value is added to each filter-channel.\n    layer += biases\n\n    # Use pooling to down-sample the image resolution?\n    if use_pooling:\n        # This is 2x2 max-pooling, which means that we\n        # consider 2x2 windows and select the largest value\n        # in each window. Then we move 2 pixels to the next window.\n        layer = tf.nn.max_pool(input=layer,\n                               ksize=[1, 2, 2, 1],\n                               strides=[1, 2, 2, 1],\n                               padding='SAME')\n\n    # Rectified Linear Unit (ReLU).\n    # It calculates max(x, 0) for each input pixel x.\n    # This adds some non-linearity to the formula and allows us\n    # to learn more complicated functions.\n    layer = tf.nn.relu(layer)\n\n    # Note that ReLU is normally executed before the pooling,\n    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n    # save 75% of the relu-operations by max-pooling first.\n\n    # We return both the resulting layer and the filter-weights\n    # because we will plot the weights later.\n    return layer, weights\n\ndef flatten_layer(layer):\n    # Get the shape of the input layer.\n    layer_shape = layer.get_shape()\n\n    # The shape of the input layer is assumed to be:\n    # layer_shape == [num_images, img_height, img_width, num_channels]\n\n    # The number of features is: img_height * img_width * num_channels\n    # We can use a function from TensorFlow to calculate this.\n    num_features = layer_shape[1:4].num_elements()\n    \n    # Reshape the layer to [num_images, num_features].\n    # Note that we just set the size of the second dimension\n    # to num_features and the size of the first dimension to -1\n    # which means the size in that dimension is calculated\n    # so the total size of the tensor is unchanged from the reshaping.\n    layer_flat = tf.reshape(layer, [-1, num_features])\n\n    # The shape of the flattened layer is now:\n    # [num_images, img_height * img_width * num_channels]\n\n    # Return both the flattened layer and the number of features.\n    return layer_flat, num_features\n\ndef new_fc_layer(input,          # The previous layer.\n                 num_inputs,     # Num. inputs from prev. layer.\n                 num_outputs,    # Num. outputs.\n                 use_relu=True): # Use Rectified Linear Unit (ReLU)?\n\n    # Create new weights and biases.\n    weights = new_weights(shape=[num_inputs, num_outputs])\n    biases = new_biases(length=num_outputs)\n\n    # Calculate the layer as the matrix multiplication of\n    # the input and weights, and then add the bias-values.\n    layer = tf.matmul(input, weights) + biases\n\n    # Use ReLU?\n    if use_relu:\n        layer = tf.nn.relu(layer)\n\n    return layer","fa25b7f5":"# Convolutional Layer 1.\nfilter_size1 = 8          # Convolution filters are 8x8 pixels.\nnum_filters1 = 16         # There are 16 of these filters.\n\n# Convolutional Layer 2.\nfilter_size2 = 8          # Convolution filters are 8x8 pixels.\nnum_filters2 = 36         # There are 36 of these filters.\n\n# Convolutional Layer 3.\nfilter_size3 = 4\nnum_filters3 = 72\n\n# Fully-connected layer.\nfc_size = 128             # Number of neurons in fully-connected layer.","51154cb0":"x = tf.compat.v1.placeholder(tf.float32, shape=[None, img_height, img_width, num_channels], name='x')\nx_image = tf.reshape(x, [-1, img_height, img_width, num_channels])\ny_true = tf.compat.v1.placeholder(tf.float32, shape=[None, num_classes], name='y_true')\ny_true_cls = tf.argmax(y_true, axis=1)","a35d9627":"layer_conv1, weights_conv1 = new_conv_layer(input=x_image,\n                                           num_input_channels=num_channels,\n                                           filter_size=filter_size1,\n                                           num_filters=num_filters1,\n                                           use_pooling=True)","a61b7d59":"layer_conv1","44a2a001":"layer_conv2, weights_conv2 = new_conv_layer(input=layer_conv1,\n                                           num_input_channels=num_filters1,\n                                           filter_size=filter_size2,\n                                           num_filters=num_filters2,\n                                           use_pooling=True)","71d44f54":"layer_conv2","bc94bb8a":"layer_conv3, weights_conv3 = new_conv_layer(input=layer_conv2,\n                                           num_input_channels=num_filters2,\n                                           filter_size=filter_size3,\n                                           num_filters=num_filters3,\n                                           use_pooling=True)","1d9af2a9":"layer_conv3","a69998e2":"layer_flat, num_features = flatten_layer(layer_conv3)","bd45614b":"layer_flat, num_features","bc98cedb":"layer_fc1 = new_fc_layer(input=layer_flat,\n                         num_inputs=num_features,\n                         num_outputs=fc_size,\n                         use_relu=True)","4c673d10":"layer_fc1","deff1e6a":"layer_fc2 = new_fc_layer(input=layer_fc1,\n                         num_inputs=fc_size,\n                         num_outputs=num_classes,\n                         use_relu=False)","efbf9abc":"layer_fc2","6dce9183":"y_pred = tf.nn.softmax(layer_fc2)","9760db39":"y_pred_cls = tf.argmax(y_pred, axis=1)","c90dea97":"cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer_fc2, labels=y_true)","c0a57a03":"cost = tf.reduce_mean(cross_entropy)","c8cdd2ad":"optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4).minimize(cost)","c17f51bc":"correct_prediction = tf.equal(y_pred_cls, y_true_cls)","24c9f5bd":"accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))","c11c3b7f":"def load_image_batch(image_ids, for_tf=True, img_shape=(128, 128)):\n    \"\"\"\n    params:\n    image_ids: list of image_ids to load from preset path.\n    for_tf: boolean signaling wether the ouput will be reshaped to a 4D Tensor or be a list of 3D arrays.\n    If reshape=False then the returned list can be used for plotting images.\n    Return: 4D Tensor or list of 3D arrays.\n    The Output has the shape (image_num, image_height, image_width, num_channels) which is the required\n    shape of a Tensor for TensorFlow.\n    \"\"\"\n    imgs = []\n    for image_id in image_ids:\n        file_path = image_id + \".jpg\"\n        img = cv2.imread(IMG_PATH + file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, img_shape)\n        imgs.append(img)\n    \n    # reshape output array to 4D Tensor and normalize values\n    if for_tf == True:\n        image_batch = np.ndarray(shape=(len(imgs), img_shape[0], img_shape[1], 3),dtype = np.float32)\n        for i in range(len(imgs)):\n            image_batch[i] = imgs[i] \/ 255.0\n    else:\n        image_batch = imgs\n        \n    return image_batch\n    \n\ndef random_batch(image_ids, y, batch_size):\n    \n    assert (len(image_ids) == len(y))\n    \n    random_indexes = np.random.permutation(len(image_ids))\n    batch_indexes = random_indexes[:batch_size]\n    \n    image_ids = image_ids[batch_indexes]\n    x_batch = load_image_batch(image_ids)\n    y_batch = y[batch_indexes]\n    \n    return x_batch, y_batch\n\ntrain_batch_size = 64\n\n# Counter for total number of iterations performed so far.\ntotal_iterations = 0\n\ndef optimize(num_iterations):\n    # Ensure we update the global variable rather than a local copy.\n    global total_iterations\n\n    # Start-time used for printing time-usage below.\n    start_time = time.time()\n\n    for i in range(total_iterations,\n                   total_iterations + num_iterations):\n\n        # Get a batch of training examples.\n        # x_batch now holds a batch of images and\n        # y_true_batch are the true labels for those images.\n        x_batch, y_true_batch = random_batch(train_ids, y_train, batch_size=train_batch_size)\n\n        # Put the batch into a dict with the proper names\n        # for placeholder variables in the TensorFlow graph.\n        feed_dict_train = {x: x_batch, y_true: y_true_batch}\n\n        # Run the optimizer using this batch of training data.\n        # TensorFlow assigns the variables in feed_dict_train\n        # to the placeholder variables and then runs the optimizer.\n        session.run(optimizer, feed_dict=feed_dict_train)\n\n        # Print status every 100 iterations.\n        if i % 100 == 0:\n            # Calculate the accuracy on the training-set.\n            acc = session.run(accuracy, feed_dict=feed_dict_train)\n\n            # Message for printing.\n            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n\n            # Print it.\n            print(msg.format(i + 1, acc))\n\n    # Update the total number of iterations performed.\n    total_iterations += num_iterations\n\n    # Ending time.\n    end_time = time.time()\n\n    # Difference between start and end-times.\n    time_dif = end_time - start_time\n\n    # Print the time-usage.\n    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n    \ndef plot_images(images, cls_true, cls_pred=None):\n    assert len(images) == len(cls_true) == 9\n    \n    if isinstance(images, pd.DataFrame):\n        images = images.to_numpy()\n        \n    if isinstance(cls_true, pd.Series):\n        cls_true = cls_true.to_numpy()\n        \n    if isinstance(cls_pred, pd.Series):\n        cls_pred = cls_pred.to_numpy()\n            \n    # Create figure with 3x3 sub-plots.\n    fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n\n    for i, ax in enumerate(axes.flat):\n        # Plot image.\n        ax.imshow(images[i])\n\n        # Show true and predicted classes.\n        if cls_pred is None:\n            xlabel = \"True: {0}\".format(cls_true[i])\n        else:\n            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n\n        # Show the classes as the label on the x-axis.\n        ax.set_xlabel(xlabel)\n        \n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()\n    \ndef plot_example_errors(cls_pred, correct):\n    # This function is called from print_test_accuracy() below.\n\n    # cls_pred is an array of the predicted class-number for\n    # all images in the test-set.\n\n    # correct is a boolean array whether the predicted class\n    # is equal to the true class for each image in the test-set.\n\n    # Negate the boolean array.\n    incorrect = (correct == False)\n    \n    # Get the images from the test-set that have been\n    # incorrectly classified.\n    image_ids = test_ids[incorrect]\n    image_ids = image_ids[0:9]\n    # pass reshape=False so it returns a list of images instead of a 4D Tensor\n    images = load_image_batch(image_ids, for_tf=False) \n    \n    ###########images.shape\n    ##########plt.imshow(images[0])\n    \n    # Get the predicted classes for those images.\n    cls_pred = cls_pred[incorrect]\n\n    # Get the true classes for those images.\n    cls_true = y_test_cls[incorrect] \n    \n    # Plot the first 9 images.\n    plot_images(images=images,\n                cls_true=cls_true[0:9],\n                cls_pred=cls_pred[0:9])\n    \ndef plot_confusion_matrix(cls_pred):\n    # This is called from print_test_accuracy() below.\n\n    # cls_pred is an array of the predicted class-number for\n    # all images in the test-set.\n\n    # Get the true classifications for the test-set.\n    cls_true = y_test_cls \n    \n    # Get the confusion matrix using sklearn.\n    cm = confusion_matrix(y_true=cls_true,\n                          y_pred=cls_pred)\n\n    # Print the confusion matrix as text.\n    print(cm)\n\n    # Plot the confusion matrix as an image.\n    plt.matshow(cm)\n\n    # Make various adjustments to the plot.\n    plt.colorbar()\n    tick_marks = np.arange(num_classes)\n    plt.xticks(tick_marks, range(num_classes))\n    plt.yticks(tick_marks, range(num_classes))\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n\n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()\n    \n# Split the test-set into smaller batches of this size.\ntest_batch_size = 64\n\ndef print_test_accuracy(show_example_errors=False,\n                        show_confusion_matrix=False):\n\n    # Number of images in the test-set.\n    num_test = test_ids.shape[0]\n\n    # Allocate an array for the predicted classes which\n    # will be calculated in batches and filled into this array.\n    cls_pred = np.zeros(shape=num_test, dtype=np.int)\n\n    # Now calculate the predicted classes for the batches.\n    # We will just iterate through all the batches.\n    # There might be a more clever and Pythonic way of doing this.\n\n    # The starting index for the next batch is denoted i.\n    i = 0\n\n    while i < num_test:\n        # The ending index for the next batch is denoted j.\n        j = min(i + test_batch_size, num_test)\n\n        # Get the images from the test-set between index i and j.\n        image_ids = test_ids[i:j]\n        images = load_image_batch(image_ids)\n\n        # Get the associated labels.\n        labels = y_test[i:j]\n\n        # Create a feed-dict with these images and labels.\n        feed_dict = {x: images, y_true: labels}\n\n        # Calculate the predicted class using TensorFlow.\n        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)\n\n        # Set the start-index for the next batch to the\n        # end-index of the current batch.\n        i = j\n\n    # Convenience variable for the true class-numbers of the test-set.\n    cls_true = y_test_cls \n\n    # Create a boolean array whether each image is correctly classified.\n    correct = (cls_true == cls_pred)\n\n    # Calculate the number of correctly classified images.\n    # When summing a boolean array, False means 0 and True means 1.\n    # correct_sum = correct.sum()\n    correct_sum = sum(1 for boo in correct if boo == True)\n\n    # Classification accuracy is the number of correctly classified\n    # images divided by the total number of images in the test-set.\n    acc = float(correct_sum) \/ num_test\n\n    # Print the accuracy.\n    msg = \"Accuracy on Test-Set: {0:.1%} ({1} \/ {2})\"\n    print(msg.format(acc, correct_sum, num_test))\n\n    # Plot some examples of mis-classifications, if desired.\n    if show_example_errors:\n        print(\"Example errors:\")\n        plot_example_errors(cls_pred=cls_pred, correct=correct)\n\n    # Plot the confusion matrix, if desired.\n    if show_confusion_matrix:\n        print(\"Confusion Matrix:\")\n        plot_confusion_matrix(cls_pred=cls_pred)","68438b5e":"session = tf.compat.v1.Session()","107781aa":"session.run(tf.compat.v1.global_variables_initializer())","d1a5f248":"print_test_accuracy()","8fcaf016":"optimize(num_iterations=1)","d0e40cc1":"print_test_accuracy()","e6575ddb":"optimize(num_iterations=99) # We already performed 1 iteration above.","fc5a57f2":"print_test_accuracy(show_example_errors=True)","ce610095":"optimize(num_iterations=900) # We performed 100 iterations above.","a810025f":"print_test_accuracy(show_example_errors=True, show_confusion_matrix=True)","3882662d":"def get_pred_by_batch(test_ids):\n    \n    full_pred = np.zeros(shape=(len(test_ids), 4))\n    \n    num_test = test_ids.shape[0]\n    \n    # The starting index for the next batch is denoted i.\n    i = 0\n    while i < num_test:\n        # The ending index for the next batch is denoted j.\n        j = min(i + test_batch_size, num_test)\n\n        # Get the images from the test-set between index i and j.\n        image_ids = test_ids[i:j]\n        images = load_image_batch(image_ids)\n        \n        # pass to CNN and get predictions\n        feed_dict = {x: images}\n        pred = session.run(y_pred, feed_dict=feed_dict)\n        full_pred[i:j] = pred\n        \n        i = j\n    \n    return full_pred","808bddd2":"pred = get_pred_by_batch(df_test['image_id'])\npred = pd.DataFrame(pred, columns=['healthy', 'multiple_diseases', 'rust', 'scab'])\n\npred = pd.concat([df_test['image_id'], pred], axis=1)","a42112c3":"pred.head()","f429c0b3":"pred.to_csv('plant_pathology_2020_sol_1000its', index=False)","a94bcc78":"tf.compat.v1.reset_default_graph()\ntf.compat.v1.enable_eager_execution()","acf40d7e":"img_size = 100","aea36d37":"path = '..\/input\/plant-pathology-2020-fgvc7\/'\nIMG_PATH = path + 'images\/'\n\ndf_train = pd.read_csv(path + 'train.csv')\ndf_test = pd.read_csv(path + 'test.csv')\n\ndef fetch_imgs_np(names, img_size=100, num_channels=3, normalize=True):\n    imgs_np = np.ndarray(shape=(len(names), img_size, img_size, num_channels), dtype=np.float32)\n    for i, name in enumerate(names):\n        img_path = IMG_PATH + name + '.jpg'\n        img=cv2.imread(img_path)\n        img=cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_AREA)\n        if normalize is True:\n            img = img \/ 255\n        imgs_np[i] = img\n    return imgs_np\n        \ntrain_imgs = fetch_imgs_np(df_train['image_id'], img_size=img_size)\ntest_imgs = fetch_imgs_np(df_test['image_id'], img_size=img_size)\n\n# label_columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\nx_train, x_val, y_train, y_val = train_test_split(train_imgs, df_train.drop('image_id', axis=1), test_size=0.2, random_state=42)\n\nx_train.shape, x_val.shape, y_train.shape, y_val.shape","f85d03e4":"def construct_model():\n    model = Sequential()\n    \n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', input_shape=(img_size, img_size, 3),\n                activation='relu'))\n    model.add(BatchNormalization()) # Normalize the activations of the previous layer at each batch\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.2))\n    \n    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n   \n    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n    \n    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2)))\n    model.add(Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.3))\n\n    model.add(Flatten()) # Flatten the input\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(4, activation='softmax'))\n    # Configure the learning process\n    # The loss function is the objective that the model will try to minimize\n    # For any classification problem, use accuracy metric\n    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=0.0)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    model.summary()\n    return model","586317eb":"model = construct_model()\n\nannealer = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, verbose=1, min_lr=1e-3)\ncheckpoint = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n\n# Generates batches of image data with data augmentation\ndatagen = ImageDataGenerator(rotation_range=360, # Degree range for random rotations\n                        width_shift_range=0.2, # Range for random horizontal shifts\n                        height_shift_range=0.2, # Range for random vertical shifts\n                        zoom_range=0.2, # Range for random zoom\n                        horizontal_flip=True, # Randomly flip inputs horizontally\n                        vertical_flip=True) # Randomly flip inputs vertically\n\ndatagen.fit(x_train)\n\n# Fits the model on batches with real-time data augmentation\nhist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n               steps_per_epoch=x_train.shape[0] \/\/ 32,\n               epochs=120,\n               verbose=1,\n               callbacks=[annealer, checkpoint],\n               validation_data=(x_val, y_val))","4b21b7d6":"predict = model.predict(test_imgs)\nall_predict = np.ndarray(shape = (test_imgs.shape[0],4),dtype = np.float32)\nfor i in range(0,test_imgs.shape[0]):\n    for j in range(0,4):\n        if predict[i][j]==max(predict[i]):\n            all_predict[i][j] = 1\n        else:\n            all_predict[i][j] = 0 ","d456d177":"healthy = [pred[0] for pred in all_predict]\nmultiple_diseases = [pred[1] for pred in all_predict]\nrust = [pred[2] for pred in all_predict]\nscab = [pred[3] for pred in all_predict]\n\ndf_dict = {'image_id':df_test['image_id'],'healthy':healthy,'multiple_diseases':multiple_diseases,'rust':rust,'scab':scab}\npred = pd.DataFrame(df_dict)\n\npred.head()","666791d4":"pred.to_csv('plant_pathology_2020_sub_xCNN', index=False)","992ea728":"Workflow\n--\n\n1. Look at the big picture\n2. Get the data\n3. Discover and visualize the data to gain insights\n4. Prepare the data for machine learning algorithms\n5. Select a model and train it\n6. Fine-tune your model.\n7. Present your solution.\n8. Launch, monitor, and maintain your system. ","74125dfb":"Third conolutional layer.","8bfd0e18":"### Distributions of target variables","af86b106":"The convolutional layers output 4-dim tensors. We now wish to use these as input in a fully-connected network, which requires for the tensors to be reshaped or flattened to 2-dim tensors.","946f53f7":"## Visualizations of target variables","441b67cc":"## Visualize different leafs","2a1a1222":"Create the second convolutional layer, which takes as input the output from the first convolutional layer. The number of input channels corresponds to the number of filters in the first convolutional layer.","10758639":"Observations:\n- The data is labelled so that only ever one symptom is true for every sample (e.g. either 'healthy', 'scab', 'rust' or 'multiple_diseases').","3d620343":"The Process of Canny edge detection algorithm can be broken down to 5 different steps (Wikipedia):\n\n1. Apply Gaussian filter to smooth the image in order to remove the noise\n2. Find the intensity gradients of the image\n3. Apply non-maximum suppression to get rid of spurious response to edge detection\n4. Apply double threshold to determine potential edges\n5. Track edge by hysteresis: Finalize the detection of edges by suppressing all the other edges that are weak and not connected to strong edges.\n\nGood explanation of individual steps on https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_imgproc\/py_canny\/py_canny.html.\n\ncv2.Canny(image, minVal, maxVal) returns a binary image with the highlighted edges.","8a9c4abb":"Whilst the first algorithm achieved a score around 0.68 on the kaggle test set, the second one got up to 0.84.","1f1f0577":"# 5. Select a model and train it - Build custom CNN","5642db4a":"The images are very large right now. Resize them to shape(256, 256) to save on memory. If I do not resize the images I run into an out-of-RAM-issue lateron.","2dee1ab8":"### Convolution","1221d0ab":"### Creating TensorFlow session","0350066e":"#### Performance before any optimization\n\nThe accuracy on the test-set is very low because the model variables have only been initialized and not optimized at all, so it just classifies the images randomly.","44b5417c":"# 6. Refine model","35be35b8":"# 7. Present solution","cea64fba":"The convolution algorithm transformes an input image of a certain shape into an output image with a certain shape by moving filters\/kernels\/weights across the input image and calculating the weighed sums (dot product of filter values and image values). \n\nHere the ouput image is smaller than the input image after the convolution.\n\n![image.png](attachment:image.png)","df792078":"Imports\n--","efbad94f":"Create the first convolutional layer. It takes x_image as input and creates num_filters1 different filters, each having width and height equal to filter_size1. Finally we wish to down-sample the image so it is half the size by using 2x2 max-pooling.","74e9a05f":"The output image can also be of the same shape as the input image or larger by adding a zero padding around the outside of the input image.\n\n![image.png](attachment:image.png)","44d8137f":"### Max Pooling","f47eacab":"Prepare data.","6b349161":"#### combined plots","e5845c43":"# 1. Look at the big picture","f404fcb9":"Observations:\n- Healthy leafs show green color across the entire leafs.\n- Leafs with scab have large stretches of brown across the leaf. Scab is defined as \"any of various plant diseases caused by fungi or bacteria and resulting in crustlike spots on fruit, leaves, or roots\".\n- Leafs with rust show smaller yellow to reddish spots. Rust is defined as \"a disease, especially of cereals and other grasses, characterized by rust-colored pustules of spores on the affected leaf blades and sheaths and caused by any of several rust fungi\".\n- Leafs with multiple diseases show symptoms of scab and rust with especially reddish spots.","19a881f8":"#### Performance after 1000 optimization iterations\n\nAfter 1000 optimization iterations, the model has greatly increased its accuracy on the test-set to more than 90%.","b0ca338d":"The second fully-connected layer estimates how likely it is that the input image belongs to each of the 10 classes. However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each element is limited between zero and one and the 10 elements sum to one. This is calculated using the so-called softmax function and the result is stored in y_pred.","4064ed8c":"Max pooling is very similar to convolution, except it involves finding the maximum value in a window instead of finding the dot product of the window with a kernel. Max pooling does not require a kernel and it is very useful in reducing the dimensionality of convolutional feature maps in CNNs. The image below demonstrates the working of MaxPool:\n\n![image.png](attachment:image.png)","32c61af3":"## TensorFlow run","56374eee":"### placeholder variables","221116a5":"### Cost function to be optimized","307fb9e1":"## Create training and validation subsets","5531f132":"### CNN configuration","fc3a7ad8":"### Canny edge detection","ad4d6cfb":"### with matplotlib.pyplot","464bed92":"#### individual plots","85beb1bb":"Over-fitting usually happens when your neural network tries to learn high frequency features (patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially has data points in all frequencies, effectively distorting the high frequency (high level) features. This also means that lower frequency (low level) components (usually, your intended data) are also distorted, but your neural network can learn to look past that. Adding just the right amount of noise can enhance the learning capability.\n\ncv2.blur() method is used to blur an image using the normalized box filter (This is NOT gaussian noise). The function smooths an image using the kernel. (https:\/\/www.geeksforgeeks.org\/python-opencv-cv2-blur-method\/)\n\ndst\t= cv.blur(src, ksize[, dst[, anchor[, borderType]]])\n\nParameters:\n- src: Input array\/ image\n- ksize: tuple with the size of the kernel used for the blurring algorithm","7d5f42c9":"## Channel Distributions","b532f457":"Try using a more complex CNN and enlarge dataset with previously mentioned data augmentation techniques.","5e42ea44":"OpenCV has a builtin function cv2.filter2D() to convolve a kernel with an image. It\u2019s arguments are\n\n- src: input image\n- ddepth: desired depth of the output image. If it is negative, it will be the same as that of the input image (-> adds zero padding).\n- borderType: pixel extrapolation method.\n\n(https:\/\/theailearner.com\/tag\/cv2-filter2d\/)","399fe243":"(Competition explanation from Kaggle)\nProblem Statement\n--\nMisdiagnosis of the many diseases impacting agricultural crops can lead to misuse of chemicals leading to the emergence of resistant pathogen strains, increased input costs, and more outbreaks with significant economic loss and environmental impacts. Current disease diagnosis based on human scouting is time-consuming and expensive, and although computer-vision based models have the promise to increase efficiency, the great variance in symptoms due to age of infected tissues, genetic variations, and light conditions within trees decreases the accuracy of detection.\n\nSpecific Objectives\n--\nObjectives of \u2018Plant Pathology Challenge\u2019 are to train a model using images of training dataset to 1) Accurately classify a given image from testing dataset into different diseased category or a healthy leaf; 2) Accurately distinguish between many diseases, sometimes more than one on a single leaf; 3) Deal with rare classes and novel symptoms; 4) Address depth perception\u2014angle, light, shade, physiological age of the leaf; and 5) Incorporate expert knowledge in identification, annotation, quantification, and guiding computer vision to search for relevant features during learning.","3db889aa":"## Introduction to image processing techniques with OpenCV","288eab06":"### Performance measures","29944cc1":"Observations:\n- When plotting all three color channel distributions in one plot the distribution curves look very similar. Green has the highest values, followed by red and blue.\n\nUsing plotly.graph_objects.Figure().add_trave create a boxplot from image data (which is not represented in a pandas DataFrame with column names).","90898c1b":"### creating helper functions","4c88c819":"### ReLu","463c2847":"# 4. Prepare the data for machine learning algorithms","ac656fe7":"Flipping is a useful method in data augmentation. Because a flipped image looks entirely different to a computer algorithm, it helps increasing the amount of training images. Even with lots of data, augmentation of this data can make it more robust, for example by flipping certain areas of images which only ever pointed in one direction. This could have been a false feature the algorithm would have picked up on.\n\ncv2.flip(src, flipCode[, dst]) returns the flipped array (here image) dst.\nParameters:\t\n- src \u2013 input array.\n- dst \u2013 output array of the same size and type as src. If dst is specified the function will return None.\n- flipCode \u2013 a flag to specify how to flip the array; 0 means flipping around the x-axis and positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means flipping around both axes.","ca28f86d":"#### Save solution for kaggle","521cf503":"## Building the computational graph (layering the CNN) ","f9f7d3a1":"Sources\n--\n- https:\/\/www.kaggle.com\/tarunpaparaju\/plant-pathology-2020-eda-models\n- The book: Hand on Machine Learning with sklearn and Tensorflow by Aur\u00e9lien G\u00e9ron\n- helper functions for building the custom CNN from https:\/\/github.com\/Hvass-Labs\/TensorFlow-Tutorials\/blob\/master\/02_Convolutional_Neural_Network.ipynb\n- complex CNN architecture from https:\/\/www.kaggle.com\/shawon10\/plant-pathology-eda-and-deep-cnn","ce53599c":"#### Performance after 100 optimization iterations\n\nAfter 100 optimization iterations, the model has significantly improved its classification accuracy.","469b7542":"Reduce predicted values from floating point numbers to 0 and 1. With only ever one 1 per row\/ sample.","1d3fbd8e":"# 2. Get the data","127a4517":"### Blurring","2774fc87":"Observation: \n- While green parts of the image show low blue values (on the RGB scale) the brown parts show high blue values. This suggest that the blue channel may be important to detect diseases on leaves.","1f3d8ba7":"ReLU stands for rectified linear unit, and is a type of activation function. Mathematically, it is defined as y = max(0, x). Visually, it looks like the following:\n![image.png](attachment:image.png)\n\nRead here about different types of ReLUs: https:\/\/medium.com\/@danqing\/a-practical-guide-to-relu-b83ca804f1f7\n\nRead here about why ReLUs work: https:\/\/www.kaggle.com\/dansbecker\/rectified-linear-units-relu-in-deep-learning","48385093":"Add another fully-connected layer that outputs vectors of length 4 for determining which of the 4 classes the input image belongs to. Note that ReLU is not used in this layer.","28cc7010":"### defining helper functions","daad8dce":"# Discover and visualize the data to gain insights","ffd77bbb":"Add a fully-connected layer to the network. The input is the flattened layer from the previous convolution. The number of neurons or nodes in the fully-connected layer is fc_size. ReLU is used so we can learn non-linear relations.","9f12fe82":"Run this command to reset the computational graph when rerunning the notebook.","db7f2e8f":"Observations:\n- A majority of leafs in the dataset are unhealthy and show either symptoms of scab or rust. Only very few images show multiple diseases.","e038797b":"### layering components","adf34066":"### Optimization method","5e304175":"## Introduction to essential layers of convolutional neural networks (CNNs)","b397c379":"#### Performance after 1 optimization iteration\nThe classification accuracy does not improve much from just 1 optimization iteration, because the learning-rate for the optimizer is set very low.","8cb0e65d":"Observations:\n- The distribution of the values of the combined channels resemble a normal distribution centered around 105.\n- The distribution of the red channel values also show a normal distribution which is a little tail heavy (rightward skew).\n- The green channel shows a more uniform like distribution of values with a smaller peak and a leftward skew.\n- The blue channel has the most uniform distribution.","e7249ca4":"### Training","2ef5b13f":"### with plotly.figure_factory","3ccc26ce":"### Flipping","23896fb1":"In this case the filter used was specified to be a 7x7 array with all values equal to 1\/25.\nBecause the filter values are multiplied with the image RGB values, increasing the filter values will brighten the output image up to a point where it becomes all white, when (r,g,b)=(255,255,255). And decreasing the filter values will darken the output image. Of course in this case the filter is uniformly filled with values. In an actual convolutional layer of a CNN the filters are generated so that they highlight certain (informative) aspect of the image. \n\nHowever using such filters for convolution can again be used for augmenting image data.\n\nIt seems like this filter has a 'increasing sunshine' effect on the image."}}