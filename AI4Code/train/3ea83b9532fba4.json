{"cell_type":{"d8393f5d":"code","b1717cd6":"code","b5f6eb66":"code","b3c1f634":"code","02299469":"code","721312e5":"code","38a264cc":"code","09a96bd6":"code","f51339db":"code","ece12602":"code","d2eefe8b":"code","8e040d9c":"code","540c3bc5":"code","52ce7199":"code","e52ee48e":"code","0eecfe0b":"code","cf992e44":"code","a717d32c":"code","40b2b1a8":"code","cf71850c":"code","385c5ce6":"code","e2ff686e":"code","2ea96aea":"code","7a153957":"code","e1ab79f0":"code","e1db4c51":"code","00ce828e":"code","e4575caf":"code","f5d432ff":"code","59b2ee64":"markdown","1587dc85":"markdown"},"source":{"d8393f5d":"import os \nimport cv2\nimport numpy as np \nimport pandas as pd \nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom os.path import isfile, join\n\n# For CNN \nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.utils import to_categorical, plot_model\nfrom keras import layers, regularizers, optimizers, callbacks\nfrom sklearn.model_selection import train_test_split \nfrom collections import defaultdict\n\n# For Autoencoder\nimport keras\nimport gzip\nfrom keras.models import Model\nfrom keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.optimizers import RMSprop\nfrom keras.layers.normalization import BatchNormalization\n\n# setting paths\npath = '..\/input\/socofing\/SOCOFing'\nreal_dir = '..\/input\/socofing\/SOCOFing\/Real'\naltered_dir = '..\/input\/socofing\/SOCOFing\/Altered'","b1717cd6":"# Forming a dataframe to \ndata = []\ndf=pd.DataFrame(data,columns=['img_id','gender', 'hand', 'finger', 'alteration'])\n\nd = defaultdict(lambda: \"Not Present\")\nd[\"Right\"] = 1\nd[\"Left\"] = 0\nd[\"M\"] = 0\nd[\"F\"] = 1\nd[\"thumb\"] = 0\nd[\"index\"] = 1\nd[\"middle\"] = 2\nd[\"ring\"] = 3\nd[\"little\"] = 4\nd[\"No\"] = 0\nd[\"Obl\"] = 1\nd[\"CR\"] = 2\nd[\"Zcut\"] = 3\n\nfor dirname, dirnames, filenames in os.walk(path ):\n    print(dirname + \":\")\n    alteration='No'\n    for filename in filenames:#print(filename)\n        #if isfile(filename)\n        img, ext = os.path.splitext(filename)\n        img_id, name = img.split('__')\n        if(dirname != real_dir):\n            gender, hand, finger, name, alteration = name.split('_')\n        else:\n            #print(\"ss\")\n            gender, hand, finger, name = name.split('_')\n        a0 = int(img_id)\n        a1 = d[gender]\n        a2 = d[hand]\n        a3 = d[finger]\n        a4 = d[alteration]\n        data = [[a0, a1, a2, a3, a4]]\n        df1=pd.DataFrame(data,columns=['img_id','gender', 'hand', 'finger', 'alteration'])\n        df = df.append(df1)\n    print(\"image count: \\t\",len(filenames), \"\\n\")\n\nprint(\"dataframe shape: \\t\",df.shape, \"\\n\\n\")\nprint(df.dtypes)","b5f6eb66":"# Histogram \nnHistogramShown = 5\nnHistogramPerRow = 5\n\nnunique = df.nunique()\ndf = df[[col for col in df[0:4]]]\nnRow, nCol = df.shape\ncolumnNames = list(df)\nnHistRow = (nCol + nHistogramPerRow - 1) \/ nHistogramPerRow\nplt.figure(num=None, figsize=(6*nHistogramPerRow, 8*nHistRow), dpi=80, facecolor='w', edgecolor='k')\nfor i in range(min(nCol, nHistogramShown)):\n    plt.subplot(nHistRow, nHistogramPerRow, i+1)\n    df.iloc[:,i].hist()\n    plt.ylabel('counts')\n    plt.xticks(rotation=90)\n    plt.title(f'{columnNames[i]} (column {i})')\nplt.tight_layout(pad=1.0, w_pad=1.0, h_pad=1.0)\nplt.show()","b3c1f634":"# Ploting different types of images from the dataset provided\nreal = matplotlib.image.imread(join(real_dir, \"1__M_Left_index_finger.BMP\"))\nalt_cr = matplotlib.image.imread(join(altered_dir, \"Altered-Hard\", \"1__M_Left_index_finger_CR.BMP\"))\nalt_obl = matplotlib.image.imread(join(altered_dir, \"Altered-Hard\", \"1__M_Left_index_finger_Obl.BMP\"))\nalt_zcut = matplotlib.image.imread(join(altered_dir, \"Altered-Hard\", \"1__M_Left_index_finger_Zcut.BMP\"))\n\nplt.figure()\nplt.figure(figsize=(24, 25))\nplt.subplot(1, 5, 1, facecolor='w')\nplt.imshow(real, cmap='gray')\nplt.subplot(1, 5, 2, facecolor='w')\nplt.imshow(alt_cr, cmap='gray')\nplt.subplot(1, 5, 3, facecolor='w')\nplt.imshow(alt_obl, cmap='gray')\nplt.subplot(1, 5, 4, facecolor='w')\nplt.imshow(alt_zcut, cmap='gray')","02299469":"def load_images(path, train=True):# Data loading function \n    print(\"Loading data from: \", path)\n    data = []\n    for img in os.listdir(path):\n        name, ext = os.path.splitext(img)\n        ID, etc = name.split('__')\n        ID = int(ID) - 1\n        img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n        img_resize = cv2.resize(img_array, (96, 96))\n        data.append([ID, img_resize])\n    return data","721312e5":"# importing images data using loading function\nEasy_images = load_images(altered_dir+\"\/Altered-Easy\", train=True)\nMedium_images = load_images(altered_dir+\"\/Altered-Medium\", train=True)\nHard_images = load_images(altered_dir+\"\/Altered-Hard\", train=True)\nReal_images = load_images(real_dir, train=False)\n#merging altered data\nAltered_images = np.concatenate([Easy_images, Medium_images, Hard_images], axis=0)\n\ndel Easy_images, Medium_images, Hard_images","38a264cc":"# Data extracting and preprocessing\nx_altd, y_ID_altd = [], []\n\nfor ID, img in Altered_images:\n    x_altd.append(img)\n    y_ID_altd.append(ID)    \n\nx_altd = np.array(x_altd).reshape(-1, 96, 96, 1)\nx_altd = x_altd \/ 255.0\ny_ID_altd = to_categorical(y_ID_altd, num_classes=600) # 600 persons in total\n","09a96bd6":"# Data spliting \nImgs_train, Imgs_val, ID_train, ID_val = train_test_split(\n    x_altd, y_ID_altd, test_size=0.2, random_state=2)\ndel x_altd, y_ID_altd","f51339db":"# Setting up Convolutional Neural Network Architecture \nmodel = [0]\nmodel_name = 'Fingerprint_Model'\nmodel = Sequential(name= model_name)\n\nmodel.add(layers.Conv2D(64, (5, 5), activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape = (96, 96, 1)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(layers.Conv2D(128,(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool2D((2, 2)))\nmodel.add(layers.Dropout(0.3))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dropout(0.4))\nmodel.add(layers.Dense(600, activation='softmax'))\n\n# Complete with Adam optimizer and entropy cost\nmodel.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.summary()","ece12602":"#Plot model graph in layers\n#plot_model(model, show_shapes=True, to_file='.\/model.png')","d2eefe8b":"# Hyperparameters \nhistory = [0] \nReduceLR_minlr = 1e-9\nepochs = 15\nbatch_size = 128\nCallBack = [\n    callbacks.EarlyStopping(monitor='val_accuracy', patience=7, mode='max', verbose=1),\n    callbacks.ReduceLROnPlateau(factor=0.1, patience=0.9, min_lr=ReduceLR_minlr, verbose=1),\n    callbacks.TensorBoard(log_dir=\".\/log_dir\/\"+model_name)]\n\n# Training the fingerprint model \nhistory = model.fit(Imgs_train, ID_train,\n                    batch_size = batch_size,\n                    epochs = epochs, \n                    validation_data = (Imgs_val, ID_val),\n                    verbose = 1, callbacks= CallBack)\n# Saving fingerprint model\nmodel.save('model.saved')","8e040d9c":"del Imgs_train, Imgs_val, ID_train, ID_val, Altered_images","540c3bc5":"# Ploting accuracy and loss values\npd.DataFrame(history.history).plot(figsize = (8,5))\nplt.grid(True)\nplt.gca().set_ylim(0,1)","52ce7199":"# Loading saved fingerprint model \nmodel = load_model('model.saved')\ncount = 0\n\n# Prediction and evaluation trained model\n\nfor ID, feature in Real_images:\n    img = np.array(feature).reshape( -1, 96, 96, 1)\n    pred_id = model.predict_classes(img)\n    error = ID-pred_id\n    if error != 0:\n        #print(\"wrong prediction\", pred_id)\n        count = count +1\n        \n# Count of wrongly predicted images\nprint(\"Error count : \", count)\n# Percentage error for real images\nprint(\"Percentage error : \",count\/60, \" % \")\n\ndel Real_images","e52ee48e":"input_img = Input(shape = (96, 96, 1))\n# Implementing convolutional autoencoder\ndef autoencoder(input_img):\n    #encoder\n    Econv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_img) #96 x 96 x 1 (wide and thin)\n    Epool1 = MaxPooling2D(pool_size=(2, 2))(Econv1) \n    Econv2 = Conv2D(32, (3, 3), activation='relu', padding='same')(Epool1) \n    Epool2 = MaxPooling2D(pool_size=(2, 2))(Econv2) \n    Econv3 = Conv2D(8, (3, 3), activation='relu', padding='same')(Epool2) \n\n    #decoder\n    Dconv1 = Conv2D(8, (3, 3), activation='relu', padding='same')(Econv3) #7 x 7 x 128 (small and thick)\n    Dup1 = UpSampling2D((2,2))(Dconv1) \n    Dconv2 = Conv2D(32, (3, 3), activation='relu', padding='same')(Dup1)\n    Dup2 = UpSampling2D((2,2))(Dconv2) \n    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(Dup2) \n    return decoded","0eecfe0b":"# Compilation of autoencoder model\nauto_encoder = Model(input_img, autoencoder(input_img))\nauto_encoder.compile(loss='mean_squared_error', optimizer = RMSprop())\nauto_encoder.summary()\n","cf992e44":"# Plot finger model grapy in layers\n#plot_model(auto_encoder, show_shapes=True, to_file='.\/auto_encoder.png')","a717d32c":"Real_images = load_images(real_dir, train=False)","40b2b1a8":"x_img, y_ID = [], []\n\n# Data extracting and preprocessing\nfor ID, img in Real_images:\n    x_img.append(img)\n\nx_img = np.array(x_img)\nx_img = x_img \/ 255.0\ny_ID = to_categorical(y_ID, num_classes=600)\n","cf71850c":"# Splitting dataset into training and validation subsets\nImgs_train, Imgs_val, ID_train, ID_val = train_test_split(\n    x_img, x_img, test_size=0.2, random_state=15)\n\ndel x_img, y_ID, Real_images","385c5ce6":"# Training autoencoder\nencoder_train = auto_encoder.fit(Imgs_train ,\n                            ID_train , \n                            batch_size = 128 , \n                            epochs = 60 ,\n                            verbose = 1,\n                            validation_data =(Imgs_val, ID_val))","e2ff686e":"# Plot of loss and validation loss\nloss = encoder_train.history['loss']\nval_loss = encoder_train.history['val_loss']\nepochs = range(60)\nplt.figure()\nplt.plot(epochs, loss, '-', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n","2ea96aea":"pred = auto_encoder.predict(Imgs_val)\n\n# Visualizing Predictions\nplt.figure(figsize=(10, 5))\nprint(\"Test Images\")\nfor i in range(3): # ploting actual images\n    plt.subplot(1, 3, i+1)\n    img = np.array(ID_val).reshape( -1, 96, 96, 1)\n    plt.imshow( img[i+5, ..., 0], cmap='gray')\nplt.show()    \n    \nplt.figure(figsize=(10, 5))\nprint(\"Reconstruction of Test Images\")\nfor i in range(3): # ploting generated images\n    plt.subplot(1, 3, i+1)\n    plt.imshow(pred[i+5, ..., 0], cmap='gray')  \nplt.show()","7a153957":"# Adding noice to images \nnoise_factor = 0.25\nx_train_noisy = Imgs_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=Imgs_train.shape)\nx_test_noisy = Imgs_val + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=Imgs_val.shape)\n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)","e1ab79f0":"# Noicy images visualization\nn = 5\nplt.figure(figsize=(20, 2))\nfor i in range(n):\n    ax = plt.subplot(1, n, i + 1)\n    plt.imshow(x_test_noisy[i].reshape( 96, 96 ))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","e1db4c51":"# Compilation of autoencoder for noicy images\nauto_encoder1 = Model(input_img, autoencoder(input_img))\nauto_encoder1.compile(loss='mean_squared_error', optimizer = RMSprop())\nauto_encoder1.summary()","00ce828e":"# Training autoencoder for noicy images\nencoder_train1 = auto_encoder1.fit(x_train_noisy ,\n                            x_train_noisy , \n                            batch_size = 128 , \n                            epochs = 80 ,\n                            verbose = 1,\n                            validation_data =(x_test_noisy, x_test_noisy))","e4575caf":"# loss and validation loss plot for noicy images model\nloss = encoder_train1.history['loss']\nval_loss = encoder_train1.history['val_loss']\nepochs = range(80)\nplt.figure()\nplt.plot(epochs, loss, '-', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","f5d432ff":"dec_pred = auto_encoder1.predict(x_test_noisy)\n\n# Visualizing Predictions for noicy images \nplt.figure(figsize=(10, 5))\nprint(\"Test Images\")\nfor i in range(3):# images from training data\n    plt.subplot(1, 3, i+1)\n    img = np.array(x_test_noisy).reshape( -1, 96, 96, 1)\n    plt.imshow( img[i+5, ..., 0], cmap='gray')\nplt.show()    \n    \nplt.figure(figsize=(10, 5))\nprint(\"Reconstruction of Test Images\")\nfor i in range(3):# genetated images results\n    plt.subplot(1, 3, i+1)\n    plt.imshow(dec_pred[i+5, ..., 0], cmap='gray')  \nplt.show()","59b2ee64":"Implementation of Convolutional Autoencoder","1587dc85":"Implementation of Convolutional Neural Network"}}