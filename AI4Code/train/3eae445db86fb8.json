{"cell_type":{"830f12f2":"code","c3a2680d":"code","3a0b9a2b":"code","2e7b7a33":"code","b6218180":"code","6945d45f":"code","f0d73908":"code","281c8879":"code","2387c81e":"code","9e78bc88":"code","00f8bdb0":"code","5cc80209":"code","e64a1589":"code","15980e58":"code","4843d44e":"code","8ef972ef":"code","48dc47bd":"code","05c6b118":"code","c2011b48":"code","09636a02":"code","24bf176c":"code","93772648":"code","bf381bd1":"markdown","48224f27":"markdown","9ed84dfd":"markdown","1709bb43":"markdown","591a1702":"markdown","de60cf53":"markdown","9d335b8d":"markdown"},"source":{"830f12f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3a2680d":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold \nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import log_loss\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport","3a0b9a2b":"sample = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n\ntest_f = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_f = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ndrug = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\ntarg_nscore = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntarg_score = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')","2e7b7a33":"train = train_f.merge(targ_score,on = 'sig_id',how = 'left')\ntrain = train.merge(targ_nscore,on = 'sig_id',how = 'left')","b6218180":"stargs_name = list(targ_score.columns[1:])\nscored_targets = train[list(targ_score.columns[1:])].sum(axis = 1)\nnscored_targets = train[list(targ_nscore.columns[1:])].sum(axis = 1)\n\nfig,axes = plt.subplots(figsize = (32,8),ncols = 2)\nsns.countplot(scored_targets,ax = axes[0])\nsns.countplot(nscored_targets,ax = axes[1])\n# scored_targets\n\nfor i in range(2):\n    axes[i].tick_params(axis = 'x',labelsize =20)\n    axes[i].tick_params(axis = 'y', labelsize = 20)\n\naxes[0].set_title(f'Training set unique scored per sample',size = 22 , pad = 22)  \naxes[1].set_title(f'Training set unique not scored per sample',size = 22 , pad = 22)   \nplt.show()","6945d45f":"fig, axes = plt.subplots(figsize = (24,24),nrows = 3, ncols = 2)\n\nsns.countplot(train_f['cp_type'],ax = axes[0][0])\nsns.countplot(test_f['cp_type'],ax = axes[0][1])\n\nsns.countplot(train_f['cp_time'],ax = axes[1][0])\nsns.countplot(test_f['cp_time'],ax = axes[1][1])\n\nsns.countplot(train_f['cp_dose'],ax = axes[2][0])\nsns.countplot(test_f['cp_dose'],ax = axes[2][1])\n\nfor i, f in enumerate(['cp_type','cp_time','cp_dose']):\n    for j , d in enumerate(['training','test']):\n        axes[i][j].set_title(f'{d} Set {f} Distribution',size = 20,pad = 15)","f0d73908":"len(train_f) - len(test_f)","281c8879":"test_f.shape","2387c81e":"train_f.shape","9e78bc88":"train_f.columns","00f8bdb0":"len(targ_score)","5cc80209":"train.head()","e64a1589":"drug.head()","15980e58":"def preprocess(df):\n    df = df.copy() \n    # df.loc[:,'something'] = in df locate all(:) rows and take col with 'something' key\n    df.loc[:,'cp_type'] = df.loc[:,'cp_type'].map({'trt_cp':0,'ctl_vehicle':1})\n    df.loc[:,'cp_dose'] = df.loc[:,'cp_dose'].map({'D1':0,'D2':1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_f)\ntest = preprocess(test_f)\ndel targ_score['sig_id']","4843d44e":"targ_score.shape","8ef972ef":"def metric(y_true,y_pred):\n    metrics = []\n    metrics.append(log_loss(y_true,y_pred.astype(float),labels = [0,1])) #loss algortithm\n    return np.mean(metrics)","48dc47bd":"test_f.head()","05c6b118":"cols = targ_score.columns\nsubmission = sample.copy()\nsubmission.loc[:,cols] = 0\nsubmission\n\n\nN_splits = 5\noff_loss = 0\nfor c, columns in enumerate(cols,1):\n    y = targ_score[columns]\n    total_loss = 0\n    \n    for fn,(trn_idx,val_idx) in enumerate(KFold(n_splits = N_splits, shuffle = True).split(train)):\n    # trn_idx , val_idx are the shuffled indexes for train and validation  \n        print('Fold :',fn+1)\n        X_train,X_val = train.iloc[trn_idx],train.iloc[val_idx] # locate data based on random index generated using KFold for training and testing\n        y_train,y_val = y.iloc[trn_idx],y.iloc[val_idx]\n        \n        model = XGBRegressor(tree_method = 'gpu_hist',\n                           min_child_weight = 1,\n                           learning_rate = 0.015,\n                           colsample_bytree = 0.65,\n                           gamma = 3.69,\n                           max_delta_step = 2.07,\n                           max_depth = 10,\n                           n_estimators = 207,\n                           subsample = 1)\n        \n        model.fit(X_train,y_train)\n        pred = model.predict(X_val)\n        loss = metric(y_val,pred)\n        total_loss += loss\n        predictions = model.predict(test)\n        submission[columns] += predictions\/N_splits\n        \n    off_loss += total_loss\/N_splits #average loss\n    print('Model '+str(c)+\":Loss = \"+str(total_loss\/N_splits))","c2011b48":"off_loss\/100","09636a02":"submission","24bf176c":"submission.loc[test['cp_type']== 1,targ_score.columns] = 0","93772648":"submission.to_csv('submission.csv',index = False)","bf381bd1":"## **train_features.csv** - \n  ### Features for the training set. \n  \n  Features **g-** signify **gene expression** data, and **c-** signify **cell viability** data.\n  \n  **cp_type** indicates samples treated with a compound *(cp_vehicle)* or with a control perturbation *(ctrl_vehicle)*; control perturbations have no MoAs; \n    \n  **cp_time and cp_dose** indicate treatment *duration* (24, 48, 72 hours) and *dose *(high or low).","48224f27":"## Encode cp_type and cp_dose","9ed84dfd":"# Model","1709bb43":"## Inhibitors are molecules that binds an enzyme and decreases its activity \n\n## Agonist are chemicals that binds a receptor and activates the receptor to produce biological response.\n\n## Antagonist block the action of agonist ","591a1702":"### for trp_cp - Dose 1 \n### for ctl_vehicle - Dose 2\n\n","de60cf53":"# EDA","9d335b8d":"## Gene Expression - \n\nGene expression is the process by which information from a gene is used in the synthesis of a functional gene product. These products are often proteins, but in non-protein-coding genes such as transfer RNA or small nuclear RNA genes, the product is a functional RNA.\n\n## Cell viability -\n\nCell viability assays use a variety of markers as indicators of metabolically active (living) cells"}}