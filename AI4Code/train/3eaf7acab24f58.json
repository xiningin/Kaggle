{"cell_type":{"d3a78d4c":"code","561e25b4":"code","207b5be7":"code","11ec415f":"code","80c4d0e9":"code","8dca6ade":"code","0f71ae7c":"code","6fba570b":"code","0ded0f14":"code","cc78a807":"code","d00ede29":"code","27525610":"code","6cfb340a":"code","44135bff":"code","97b5cd94":"code","e91b964c":"code","1cc38786":"code","afd091c8":"code","19da69d1":"code","8de76cb7":"code","9a9cfcfa":"code","e34fb417":"code","202e9c8c":"code","c68b84d4":"code","0fb6c72d":"code","7a02e507":"code","be9f51f9":"code","e0fb72aa":"code","0bf79369":"code","0946b528":"code","ddee34fb":"code","b741cca1":"code","eff104c5":"code","3e77ae1e":"code","4e13eee5":"code","4e3db066":"code","d136ee32":"code","72330100":"code","626d53ff":"code","d5428a29":"code","13c9da04":"code","49fc9ab5":"code","6f15ac38":"code","ee38a570":"code","e5952124":"code","ebc93d37":"code","2e11e530":"code","5923940d":"code","0c204bcc":"code","a9dd8ac1":"code","5fdda045":"code","69270f6f":"markdown","3eb419d1":"markdown","73330325":"markdown","5eff21f2":"markdown","2b2eea78":"markdown","46beb8ae":"markdown","94a300ad":"markdown","66510741":"markdown","fadb3c09":"markdown","efd0ca84":"markdown","bab3316d":"markdown","5d728299":"markdown"},"source":{"d3a78d4c":"%matplotlib inline\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn import preprocessing\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor, plot_importance\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","561e25b4":"df = pd.read_csv('\/kaggle\/input\/restaurant-revenue-prediction\/train.csv.zip')\ndf.shape","207b5be7":"test_df = pd.read_csv('\/kaggle\/input\/restaurant-revenue-prediction\/test.csv.zip')\ntest_df.shape","11ec415f":"df.head()","80c4d0e9":"df.describe()","8dca6ade":"def display_all(df):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(df)\n\ndisplay_all(df.head().transpose())","0f71ae7c":"df.isnull().sum().sort_index()\/len(df)","6fba570b":"fig, ax = plt.subplots(1,2, figsize=(19, 5))\ng1 = sns.countplot(df['Type'],palette=\"Set2\", ax=ax[0]);\ng2 = sns.countplot(test_df['Type'],palette=\"Set2\", ax=ax[1]);\nfig.show()","0ded0f14":"fig, ax = plt.subplots(1,2, figsize=(19, 5))\ng1 = sns.countplot(df['City Group'],palette=\"Set2\", ax=ax[0]);\ng2 = sns.countplot(test_df['City Group'],palette=\"Set2\", ax=ax[1]);\nfig.show()","cc78a807":"(df['City'].nunique(), test_df['City'].nunique())","d00ede29":"test_df.loc[test_df['Type']=='MB', 'Type'] = 'DT'","27525610":"df.drop('City', axis=1, inplace=True)\ntest_df.drop('City', axis=1, inplace=True)","6cfb340a":"import datetime\ndf.drop('Id',axis=1,inplace=True)\ndf['Open Date']  = pd.to_datetime(df['Open Date'])\ntest_df['Open Date']  = pd.to_datetime(test_df['Open Date'])\nlaunch_date = datetime.datetime(2015, 3, 23)\n# scale days open\ndf['Days Open'] = (launch_date - df['Open Date']).dt.days \/ 1000\ntest_df['Days Open'] = (launch_date - test_df['Open Date']).dt.days \/ 1000\ndf.drop('Open Date', axis=1, inplace=True)\ntest_df.drop('Open Date', axis=1, inplace=True)","44135bff":"plt.rc('figure', max_open_warning = 0)\nfor i in range(1,38):\n    fig, ax = plt.subplots(1,2, figsize=(19, 5))\n    g1 = sns.distplot(df['P{}'.format(i)], ax=ax[0], kde=False);\n    g2 = sns.distplot(test_df['P{}'.format(i)], ax=ax[1], kde=False);\n    fig.show()","97b5cd94":"df.dtypes","e91b964c":"(mu, sigma) = norm.fit(df['revenue'])\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(19, 5))\nax1 = sns.distplot(df['revenue'] , fit=norm, ax=ax1)\nax1.legend([f'Normal distribution ($\\mu=$ {mu:.3f} and $\\sigma=$ {sigma:.3f})'], loc='best')\nax1.set_ylabel('Frequency')\nax1.set_title('Revenue Distribution')\nax2 = stats.probplot(df['revenue'], plot=plt)\nf.show();","1cc38786":"# Revenue is right skewed, taking the log will make it more normally distributed for the linear models\n# Remember to use expm1 on predictions to transform back to dollar amount\n(mu, sigma) = norm.fit(np.log1p(df['revenue']))\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(19, 5))\nax1 = sns.distplot(np.log1p(df['revenue']) , fit=norm, ax=ax1)\nax1.legend([f'Normal distribution ($\\mu=$ {mu:.3f} and $\\sigma=$ {sigma:.3f})'], loc='best')\nax1.set_ylabel('Frequency')\nax1.set_title('Log(1+Revenue) Distribution')\nax2 = stats.probplot(np.log(df['revenue']), plot=plt)\nf.show();","afd091c8":"# cap revenue at 10,000,000 for outliers\n# df.loc[df['revenue'] > 10000000, 'revenue'] = 10000000","19da69d1":"# Correlation between numeric features with revenue\nplt.figure(figsize=(10, 8))\nsns.heatmap(df.drop(['revenue','City Group','Type'], axis=1).corr(), square=True)\nplt.suptitle('Pearson Correlation Heatmap')\nplt.show();","8de76cb7":"corr_with_revenue = df.drop(['City Group','Type'],axis=1).corr()['revenue'].sort_values(ascending=False)\nplt.figure(figsize=(10,7))\ncorr_with_revenue.drop('revenue').plot.bar()\nplt.show();","9a9cfcfa":"sns.pairplot(df[df.corr()['revenue'].sort_values(ascending=False).index[:5]])\nplt.show();","e34fb417":"# copy_df = df.copy()\n# copy_test_df = test_df.copy()\n# numeric_features = df.dtypes[df.dtypes != \"object\"].index\n# skewed_features = df[numeric_features].apply(lambda x: skew(x))\n# skewed_features = skewed_features[skewed_features > 0.5].index\n# df[skewed_features] = np.log1p(df[skewed_features])\n# test_df[skewed_features.drop('revenue')] = np.log1p(test_df[skewed_features.drop('revenue')])\n# Above handles skewed features using log transformation\n# Below uses multiple imputation for P1-P37, since they are actually categorical\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp_train = IterativeImputer(max_iter=30, missing_values=0, sample_posterior=True, min_value=1, random_state=37)\nimp_test = IterativeImputer(max_iter=30, missing_values=0, sample_posterior=True, min_value=1, random_state=23)\n\np_data = ['P'+str(i) for i in range(1,38)]\ndf[p_data] = np.round(imp_train.fit_transform(df[p_data]))\ntest_df[p_data] = np.round(imp_test.fit_transform(test_df[p_data]))","202e9c8c":"# drop_first=True for Dummy Encoding for object types, and drop_first=False for OHE\ncolumnsToEncode = df.select_dtypes(include=[object]).columns\ndf = pd.get_dummies(df, columns=columnsToEncode, drop_first=False)\ntest_df = pd.get_dummies(test_df, columns=columnsToEncode, drop_first=False)","c68b84d4":"df['revenue'] = np.log1p(df['revenue'])\nX, y = df.drop('revenue', axis=1), df['revenue']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=118)","0fb6c72d":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso","7a02e507":"params_ridge = {\n    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    'fit_intercept' : [True, False],\n    'normalize' : [True,False],\n    'solver' : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n}\n\nridge_model = Ridge()\nridge_regressor = GridSearchCV(ridge_model, params_ridge, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\nridge_regressor.fit(X_train, y_train)\nprint(f'Optimal alpha: {ridge_regressor.best_params_[\"alpha\"]:.2f}')\nprint(f'Optimal fit_intercept: {ridge_regressor.best_params_[\"fit_intercept\"]}')\nprint(f'Optimal normalize: {ridge_regressor.best_params_[\"normalize\"]}')\nprint(f'Optimal solver: {ridge_regressor.best_params_[\"solver\"]}')\nprint(f'Best score: {ridge_regressor.best_score_}')","be9f51f9":"ridge_model = Ridge(alpha=ridge_regressor.best_params_[\"alpha\"], fit_intercept=ridge_regressor.best_params_[\"fit_intercept\"], \n                    normalize=ridge_regressor.best_params_[\"normalize\"], solver=ridge_regressor.best_params_[\"solver\"])\nridge_model.fit(X_train, y_train)\ny_train_pred = ridge_model.predict(X_train)\ny_pred = ridge_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","e0fb72aa":"# Ridge Model Feature Importance\nridge_feature_coef = pd.Series(index = X_train.columns, data = np.abs(ridge_model.coef_))\nridge_feature_coef.sort_values().plot(kind = 'bar', figsize = (13,5));","0bf79369":"params_lasso = {\n    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    'fit_intercept' : [True, False],\n    'normalize' : [True,False],\n}\n\nlasso_model = Lasso()\nlasso_regressor = GridSearchCV(lasso_model, params_lasso, scoring='neg_root_mean_squared_error', cv=5, n_jobs=-1)\nlasso_regressor.fit(X_train, y_train)\nprint(f'Optimal alpha: {lasso_regressor.best_params_[\"alpha\"]:.2f}')\nprint(f'Optimal fit_intercept: {lasso_regressor.best_params_[\"fit_intercept\"]}')\nprint(f'Optimal normalize: {lasso_regressor.best_params_[\"normalize\"]}')\nprint(f'Best score: {lasso_regressor.best_score_}')","0946b528":"lasso_model = Lasso(alpha=lasso_regressor.best_params_[\"alpha\"], fit_intercept=lasso_regressor.best_params_[\"fit_intercept\"], \n                    normalize=lasso_regressor.best_params_[\"normalize\"])\nlasso_model.fit(X_train, y_train)\ny_train_pred = lasso_model.predict(X_train)\ny_pred = lasso_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","ddee34fb":"# Lasso Model Feature Importance\nlasso_feature_coef = pd.Series(index = X_train.columns, data = np.abs(lasso_model.coef_))\nlasso_feature_coef.sort_values().plot(kind = 'bar', figsize = (13,5));","b741cca1":"from sklearn.linear_model import ElasticNetCV, ElasticNet\n\n# Use ElasticNetCV to tune alpha automatically instead of redundantly using ElasticNet and GridSearchCV\nel_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps=5e-2, cv=10, n_jobs=-1)         \nel_model.fit(X_train, y_train)\nprint(f'Optimal alpha: {el_model.alpha_:.6f}')\nprint(f'Optimal l1_ratio: {el_model.l1_ratio_:.3f}')\nprint(f'Number of iterations {el_model.n_iter_}')","eff104c5":"y_train_pred = el_model.predict(X_train)\ny_pred = el_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","3e77ae1e":"# ElasticNet Model Feature Importance\nel_feature_coef = pd.Series(index = X_train.columns, data = np.abs(el_model.coef_))\nn_features = (el_feature_coef>0).sum()\nprint(f'{n_features} features with reduction of {(1-n_features\/len(el_feature_coef))*100:2.2f}%')\nel_feature_coef.sort_values().plot(kind = 'bar', figsize = (13,5));","4e13eee5":"from sklearn.neighbors import KNeighborsRegressor\n\nparams_knn = {\n    'n_neighbors' : [3, 5, 7, 9, 11],\n}\n\nknn_model = KNeighborsRegressor()\nknn_regressor = GridSearchCV(knn_model, params_knn, scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\nknn_regressor.fit(X_train, y_train)\nprint(f'Optimal neighbors: {knn_regressor.best_params_[\"n_neighbors\"]}')\nprint(f'Best score: {knn_regressor.best_score_}')","4e3db066":"knn_model = KNeighborsRegressor(n_neighbors=knn_regressor.best_params_[\"n_neighbors\"])\nknn_model.fit(X_train, y_train)\ny_train_pred = knn_model.predict(X_train)\ny_pred = knn_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","d136ee32":"from sklearn.ensemble import RandomForestRegressor\n\nparams_rf = {\n    'max_depth': [10, 30, 35, 50, 65, 75, 100],\n    'max_features': [.3, .4, .5, .6],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [30, 50, 100, 200]\n}\n\nrf = RandomForestRegressor()\nrf_regressor = GridSearchCV(rf, params_rf, scoring='neg_root_mean_squared_error', cv = 10, n_jobs = -1)\nrf_regressor.fit(X_train, y_train)\nprint(f'Optimal depth: {rf_regressor.best_params_[\"max_depth\"]}')\nprint(f'Optimal max_features: {rf_regressor.best_params_[\"max_features\"]}')\nprint(f'Optimal min_sample_leaf: {rf_regressor.best_params_[\"min_samples_leaf\"]}')\nprint(f'Optimal min_samples_split: {rf_regressor.best_params_[\"min_samples_split\"]}')\nprint(f'Optimal n_estimators: {rf_regressor.best_params_[\"n_estimators\"]}')\nprint(f'Best score: {rf_regressor.best_score_}')","72330100":"rf_model = RandomForestRegressor(max_depth=rf_regressor.best_params_[\"max_depth\"], \n                                 max_features=rf_regressor.best_params_[\"max_features\"], \n                                 min_samples_leaf=rf_regressor.best_params_[\"min_samples_leaf\"], \n                                 min_samples_split=rf_regressor.best_params_[\"min_samples_split\"], \n                                 n_estimators=rf_regressor.best_params_[\"n_estimators\"], \n                                 n_jobs=-1, oob_score=True)\nrf_model.fit(X_train, y_train)\ny_train_pred = rf_model.predict(X_train)\ny_pred = rf_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","626d53ff":"# Random Forest Model Feature Importance\nrf_feature_importance = pd.Series(index = X_train.columns, data = np.abs(rf_model.feature_importances_))\nn_features = (rf_feature_importance>0).sum()\nprint(f'{n_features} features with reduction of {(1-n_features\/len(rf_feature_importance))*100:2.2f}%')\nrf_feature_importance.sort_values().plot(kind = 'bar', figsize = (13,5));","d5428a29":"import lightgbm as lgbm\n\nparams_lgbm = {\n    'learning_rate': [.01, .1, .5, .7, .9, .95, .99, 1],\n    'boosting': ['gbdt'],\n    'metric': ['l1'],\n    'feature_fraction': [.3, .4, .5, 1],\n    'num_leaves': [20],\n    'min_data': [10],\n    'max_depth': [10],\n    'n_estimators': [10, 30, 50, 100]\n}\n\nlgb = lgbm.LGBMRegressor()\nlgb_regressor = GridSearchCV(lgb, params_lgbm, scoring='neg_root_mean_squared_error', cv = 10, n_jobs = -1)\nlgb_regressor.fit(X_train, y_train)\nprint(f'Optimal lr: {lgb_regressor.best_params_[\"learning_rate\"]}')\nprint(f'Optimal feature_fraction: {lgb_regressor.best_params_[\"feature_fraction\"]}')\nprint(f'Optimal n_estimators: {lgb_regressor.best_params_[\"n_estimators\"]}')\nprint(f'Best score: {lgb_regressor.best_score_}')","13c9da04":"lgb_model = lgbm.LGBMRegressor(learning_rate=lgb_regressor.best_params_[\"learning_rate\"], boosting='gbdt', \n                               metric='l1', feature_fraction=lgb_regressor.best_params_[\"feature_fraction\"], \n                               num_leaves=20, min_data=10, max_depth=10, \n                               n_estimators=lgb_regressor.best_params_[\"n_estimators\"], n_jobs=-1)\nlgb_model.fit(X_train, y_train)\ny_train_pred = lgb_model.predict(X_train)\ny_pred = lgb_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","49fc9ab5":"# LightGBM Feature Importance\nlgb_feature_importance = pd.Series(index = X_train.columns, data = np.abs(lgb_model.feature_importances_))\nn_features = (lgb_feature_importance>0).sum()\nprint(f'{n_features} features with reduction of {(1-n_features\/len(lgb_feature_importance))*100:2.2f}%')\nlgb_feature_importance.sort_values().plot(kind = 'bar', figsize = (13,5));","6f15ac38":"params_xgb = {\n    'learning_rate': [.1, .5, .7, .9, .95, .99, 1],\n    'colsample_bytree': [.3, .4, .5, .6],\n    'max_depth': [4],\n    'alpha': [3],\n    'subsample': [.5],\n    'n_estimators': [30, 70, 100, 200]\n}\n\nxgb_model = XGBRegressor()\nxgb_regressor = GridSearchCV(xgb_model, params_xgb, scoring='neg_root_mean_squared_error', cv = 10, n_jobs = -1)\nxgb_regressor.fit(X_train, y_train)\nprint(f'Optimal lr: {xgb_regressor.best_params_[\"learning_rate\"]}')\nprint(f'Optimal colsample_bytree: {xgb_regressor.best_params_[\"colsample_bytree\"]}')\nprint(f'Optimal n_estimators: {xgb_regressor.best_params_[\"n_estimators\"]}')\nprint(f'Best score: {xgb_regressor.best_score_}')","ee38a570":"xgb_model = XGBRegressor(learning_rate=xgb_regressor.best_params_[\"learning_rate\"], \n                         colsample_bytree=xgb_regressor.best_params_[\"colsample_bytree\"], \n                         max_depth=4, alpha=3, subsample=.5, \n                         n_estimators=xgb_regressor.best_params_[\"n_estimators\"], n_jobs=-1)\nxgb_model.fit(X_train, y_train)\ny_train_pred = xgb_model.predict(X_train)\ny_pred = xgb_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","e5952124":"# XGB with early stopping\nxgb_model.fit(X_train, y_train, early_stopping_rounds=4,\n             eval_set=[(X_test, y_test)], verbose=False)\ny_train_pred = xgb_model.predict(X_train)\ny_pred = xgb_model.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","ebc93d37":"# XGB Feature Importance, relevant features can be selected based on its score\nfeature_important = xgb_model.get_booster().get_fscore()\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=['score']).sort_values(by = 'score', ascending=True)\ndata.plot(kind='bar', figsize = (13,5))\nplt.show()","2e11e530":"rf_model_en = RandomForestRegressor(max_depth=200, max_features=0.4, min_samples_leaf=3, \n                                 min_samples_split=6, n_estimators=30, n_jobs=-1, oob_score=True)\nrf_model_en.fit(X_train, y_train)\ny_train_pred = rf_model_en.predict(X_train)\ny_pred = rf_model_en.predict(X_test)\nprint('Train r2 score: ', r2_score(y_train_pred, y_train))\nprint('Test r2 score: ', r2_score(y_test, y_pred))\ntrain_rmse = np.sqrt(mean_squared_error(y_train_pred, y_train))\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(f'Train RMSE: {train_rmse:.4f}')\nprint(f'Test RMSE: {test_rmse:.4f}')","5923940d":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import StackingRegressor\nfrom matplotlib import pyplot\n\n# get a stacking ensemble of models\ndef get_stacking():\n    # define the base models\n    base_models = list()\n    base_models.append(('ridge', ridge_model))\n    base_models.append(('lasso', lasso_model))\n    base_models.append(('rf', rf_model_en))\n    # define meta learner model\n    learner = LinearRegression()\n    # define the stacking ensemble\n    model = StackingRegressor(estimators=base_models, final_estimator=learner, cv=10)\n    return model\n \n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['ridge'] = ridge_model\n    models['lasso'] = lasso_model\n    models['rf_en'] = rf_model_en\n    models['stacking'] = get_stacking()\n    return models\n \n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=19)\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X_train, y_train)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} {mean(scores):.3f} {std(scores):.3f}')\n# plot model performance for comparison\npyplot.boxplot(results, labels=names, showmeans=True)\npyplot.show()","0c204bcc":"# define the base models\nbase_models = list()\nbase_models.append(('ridge', ridge_model))\nbase_models.append(('lasso', lasso_model))\nbase_models.append(('rf', rf_model_en))\n# define meta learner model\nlearner = LinearRegression()\n# define the stacking ensemble\nstack1 = StackingRegressor(estimators=base_models, final_estimator=learner, cv=10)\n# fit the model on all available data\nstack1.fit(X, y)","a9dd8ac1":"# define the base models\nbase_model = list()\nbase_model.append(('rf1', rf_model))\nbase_model.append(('rf2', rf_model_en))\nbase_model.append(('rf3', RandomForestRegressor(max_depth=8, max_features=0.1, min_samples_leaf=3, \n                                                min_samples_split=2, n_estimators=250, n_jobs=-1, oob_score=False)))\n# define meta learner model\nlearner = LinearRegression()\n# define the stacking ensemble\nstack2 = StackingRegressor(estimators=base_model, final_estimator=learner, cv=10)\n# fit the model on all available data\nstack2.fit(X, y)","5fdda045":"submission = pd.DataFrame(columns=['Id','Prediction'])\nsubmission['Id'] = test_df['Id']\n\nridge_pred = ridge_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(ridge_pred)\nsubmission.to_csv('submission_ridge.csv',index=False)\n\nlasso_pred = lasso_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(lasso_pred)\nsubmission.to_csv('submission_lasso.csv',index=False)\n\nelastic_pred = el_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(elastic_pred)\nsubmission.to_csv('submission_elastic.csv',index=False)\n\nknn_pred = knn_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(knn_pred)\nsubmission.to_csv('submission_knn.csv',index=False)\n\nrf_pred = rf_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(rf_pred)\nsubmission.to_csv('submission_rf.csv',index=False)\n\nlgb_pred = lgb_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(lgb_pred)\nsubmission.to_csv('submission_lgb.csv',index=False)\n\nxgb_pred = xgb_model.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(xgb_pred)\nsubmission.to_csv('submission_xgb.csv',index=False)\n\nstack_pred1 = stack1.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(stack_pred1)\nsubmission.to_csv('submission_stack1.csv',index=False)\n\nstack_pred2 = stack2.predict(test_df.drop('Id', axis=1))\nsubmission['Prediction'] = np.expm1(stack_pred2)\nsubmission.to_csv('submission_stack2.csv',index=False)","69270f6f":"# Light GBM","3eb419d1":"# Restaurant Revenue Prediction\nModels used:\n* Lasso & Ridge Regression\n* ElasticNet\n* KNN Regressor\n* Random Forest\n* Light GBM\n* XGBoost\n* Ensembling","73330325":"# Ridge and Lasso Regression","5eff21f2":"The **MB** Type will be replaced with the **DT** Type in the test set since it's not available in our training set. The **City** feature is useless since our training set contains **34** unique cities but the test set contains **57** unique cities.","2b2eea78":"# Feature Engineering\n\nIf the distribution of the data is left skewed, the skewness values will be negative. If the distribution of the data is right skewed, the skewness values will be positive.","46beb8ae":"# ElasticNet (combination of Ridge & Lasso)","94a300ad":"# Submissions","66510741":"# K-Nearest Neighbors","fadb3c09":"# Random Forest","efd0ca84":"The dataset is quite small so complex models with many parameters should be avoided. Using a complex model for this dataset will cause the model to overfit to the dataset. Regularization techniques will definitely need to be used to prevent the possibility of overfitting.","bab3316d":"# XGBoost","5d728299":"# Regressor Ensembling"}}