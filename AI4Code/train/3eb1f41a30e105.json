{"cell_type":{"47ad6ac7":"code","95c30f98":"code","ca7f037a":"code","f8998d83":"code","10974d58":"markdown","e2e37db0":"markdown","9b3b7fab":"markdown"},"source":{"47ad6ac7":"# uninstall\n!pip uninstall -y wandb\n\n# download\n!pip install transformers==4.8.2\n\n# import\nimport json\nimport torch\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","95c30f98":"# Dataset class\nclass SentimentDataset(Dataset):\n    def __init__(self, txt_list, label_list, tokenizer, max_length):\n        # define variables    \n        self.input_ids = []\n        self.attn_masks = []\n        self.labels = []\n        map_label = {0:'negative', 4: 'positive'}\n        # iterate through the dataset\n        for txt, label in zip(txt_list, label_list):\n            # prepare the text\n            prep_txt = f'<|startoftext|>Review: {txt}\\nSentiment: {map_label[label]}<|endoftext|>'\n            # tokenize\n            encodings_dict = tokenizer(prep_txt, truncation=True,\n                                       max_length=max_length, padding=\"max_length\")\n            # append to list\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n            self.labels.append(map_label[label])\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx], self.labels[idx]\n\n# Data load function\ndef load_sentiment_dataset(tokenizer, random_seed = 1, file_path=\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\"):\n    # load dataset and sample 10k reviews.\n    df = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)\n    df = df[[0, 5]]\n    df.columns = ['label', 'text']\n    df = df.sample(10000, random_state=1)\n    \n    def pick_first_n_words(string, max_words=250): # tried a few max_words, kept 250 as max tokens was < 512\n        split_str = string.split()\n        return \" \".join(split_str[:min(len(split_str), max_words)])\n\n    df['text'] = df['text'].apply(lambda x: pick_first_n_words(x))\n    \n    # divide into test and train\n    X_train, X_test, y_train, y_test = \\\n              train_test_split(df['text'].tolist(), df['label'].tolist(),\n              shuffle=True, test_size=0.05, random_state=random_seed, stratify=df['label'])\n\n    # get max length\n    max_length_train = max([len(tokenizer.encode(text)) for text in X_train])\n    max_length_test = max([len(tokenizer.encode(text)) for text in X_test])\n    max_length = max([max_length_train, max_length_test]) + 10  #for special tokens (sos and eos) and fillers\n    max_length = max(max_length, 300)\n    print(f\"Setting max length as {max_length}\")\n\n    # format into SentimentDataset class\n    train_dataset = SentimentDataset(X_train, y_train, tokenizer, max_length=max_length)\n\n    # return\n    return train_dataset, (X_test, y_test)","ca7f037a":"# import \nfrom torch.utils.data import Dataset, random_split\nfrom transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPTNeoForCausalLM\n\n# device\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# model\nmodel_name = \"EleutherAI\/gpt-neo-125M\"\nseed = 42\n\n# seed\ntorch.manual_seed(seed)","f8998d83":"# iterate for N trials\nfor trial_no in range(3):\n    \n    print(\"Loading model...\")\n    # load tokenizer and model\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<|startoftext|>',\n                                              eos_token='<|endoftext|>', pad_token='<|pad|>')\n    model = GPTNeoForCausalLM.from_pretrained(model_name).cuda()\n    model.resize_token_embeddings(len(tokenizer))\n\n    print(\"Loading dataset...\")\n    train_dataset, test_dataset = load_sentiment_dataset(tokenizer, trial_no)\n    \n    print(\"Start training...\")\n    training_args = TrainingArguments(output_dir='results', num_train_epochs=2, \n                                    logging_steps=10, load_best_model_at_end=True,\n                                      save_strategy=\"epoch\", per_device_train_batch_size=2, per_device_eval_batch_size=2,\n                                    warmup_steps=100, weight_decay=0.01, logging_dir='logs')\n\n    Trainer(model=model, args=training_args, train_dataset=train_dataset,\n            eval_dataset=test_dataset, data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n                                                                  'attention_mask': torch.stack([f[1] for f in data]),\n                                                                  'labels': torch.stack([f[0] for f in data])}).train()\n    \n    # test\n    print(\"Start testing...\")\n    # eval mode on model\n    _ = model.eval()\n\n    # compute prediction on test data\n    original, predicted, all_text, predicted_text = [], [], [], []\n    map_label = {0:'negative', 4: 'positive'}\n    for text, label in tqdm(zip(test_dataset[0], test_dataset[1])):\n        # predict sentiment on test data\n        prompt = f'<|startoftext|>Review: {text}\\nSentiment:'\n        generated = tokenizer(f\"<|startoftext|> {prompt}\", return_tensors=\"pt\").input_ids.cuda()\n        sample_outputs = model.generate(generated, do_sample=False, top_k=50, max_length=512, top_p=0.90, \n                temperature=0, num_return_sequences=0)\n        pred_text = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n        # extract the predicted sentiment\n        try:\n            pred_sentiment = re.findall(\"\\nSentiment: (.*)\", pred_text)[-1]\n        except:\n            pred_sentiment = \"None\"\n        original.append(map_label[label])\n        predicted.append(pred_sentiment)\n        all_text.append(text)\n        predicted_text.append(pred_text)\n    #transform into dataframe\n    df = pd.DataFrame({'text': all_text, 'predicted': predicted, 'original': original, 'predicted_text': predicted_text})\n    df.to_csv(f\"result_run_{trial_no}.csv\", index=False)\n    # compute f1 score\n    print(f1_score(original, predicted, average='macro'))","10974d58":"## GPT-Neo Finetuning on Sentiment Classification\n\n### Overview\n\n- Compare performance of different text generation model on a sentiment detection task.\n- For this, we will fine the text generation model GPT-2 on train data and report performance on the test data.\n- Hence, we will also learn how to fine tune the TG models along wth how to apply these model to an example NLP task.\n\n### Model\n\n- Huggingface\n\n### Dataset\n\n- Tweet Sentiment","e2e37db0":"### Dataset load and prep functions","9b3b7fab":"### Load model and tokenizer; Call data Prep"}}