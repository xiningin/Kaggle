{"cell_type":{"a792eca0":"code","0efdc871":"code","27d4df93":"code","e7ca6cff":"code","c9bfb99f":"code","83a599d9":"code","ac071c28":"code","c1646f7d":"code","59a803bf":"code","207db023":"code","d31d57f4":"code","3ebac565":"code","0a30409f":"code","924960a0":"code","36f889ee":"code","52d2c755":"code","3de66a95":"code","1763f96e":"code","c8741ed4":"code","042d94aa":"code","ff7ff47d":"code","04d89d38":"markdown","decc125b":"markdown","f6c07003":"markdown"},"source":{"a792eca0":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","0efdc871":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","27d4df93":"config = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':5,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","e7ca6cff":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","c9bfb99f":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","83a599d9":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('..\/input\/roberta-base')    \n        self.head = AttentionHead(768,768,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x","ac071c28":"def get_embeddings(df,path,plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/roberta-base')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","c1646f7d":"train_embeddings1 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model0\/model0.bin')\ntest_embeddings1 = get_embeddings(test_data,'..\/input\/clr-roberta\/model0\/model0.bin')\n'''\ntrain_embeddings2 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model1\/model1.bin')\ntest_embeddings2 = get_embeddings(test_data,'..\/input\/clr-roberta\/model1\/model1.bin')\n\ntrain_embeddings3 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model2\/model2.bin')\ntest_embeddings3 = get_embeddings(test_data,'..\/input\/clr-roberta\/model2\/model2.bin')\n\ntrain_embeddings4 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model3\/model3.bin')\ntest_embeddings4 = get_embeddings(test_data,'..\/input\/clr-roberta\/model3\/model3.bin')\n\ntrain_embeddings5 =  get_embeddings(train_data,'..\/input\/clr-roberta\/model4\/model4.bin')\ntest_embeddings5 = get_embeddings(test_data,'..\/input\/clr-roberta\/model4\/model4.bin')\n'''","59a803bf":"def synthesize_excerpt():\n    pass","207db023":"train_X=train_embeddings1\ntrain_Y=train_data.target.values\ntest_X=test_embeddings1\n\nprint('train_X: ',train_embeddings1.shape)\nprint('train_Y: ',train_data.target.shape)\nprint('test_X: ',test_embeddings1.shape)\n","d31d57f4":"from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.svm import SVR\nfrom sklearn.decomposition import PCA","3ebac565":"def Search_Model(train_X=train_embeddings1,train_Y=train_data.target.values):\n    '''\n    find and return best model\n    '''\n    pipe = Pipeline([\n        ('scaler',None),\n        ('reduce_dim',PCA()),\n        ('regressor',None)\n        ])\n    \n    \n    scalers_to_test = [StandardScaler(), RobustScaler(), QuantileTransformer()]\n    regressors_to_test=[Ridge(),Lasso()]\n    alpha_to_test = 2.0**np.arange(-4, +4)\n    n_features_to_test = np.arange(5, 10)\n    params = [\n        {'scaler': scalers_to_test,\n         \n         'reduce_dim__n_components': n_features_to_test,\n         \n         'regressor': regressors_to_test,\n         'regressor__alpha': alpha_to_test}\n    ]\n    \n    gridsearch = GridSearchCV(pipe, params,'neg_mean_squared_error',cv=3, verbose=3).fit(train_X, train_Y)\n    return gridsearch","0a30409f":"def Try_Model(train_X=train_embeddings1,train_Y=train_data.target.values):\n    '''\n    find and return best model\n    '''\n    pipe = Pipeline([\n        ('scaler',None),\n        ('reduce_dim',PCA()),\n        ('regressor',SVR())\n        ])\n    \n    \n    scalers_to_test = [StandardScaler()]#, RobustScaler(), QuantileTransformer()]\n    alpha_to_test = 2.0**np.arange(3, +4)\n    kernel_to_test=['linear']#,'rbf']\n    n_features_to_test = np.array([25,75,100,125])#np.arange(16, 25)\n    params = [\n        {'scaler': scalers_to_test,\n         \n         'reduce_dim__n_components': n_features_to_test,\n\n         'regressor__C': alpha_to_test,\n         'regressor__kernel': kernel_to_test\n        }\n    ]\n    \n    gridsearch = GridSearchCV(pipe, params,'neg_mean_squared_error',cv=3, verbose=2).fit(train_X, train_Y)\n    return gridsearch","924960a0":"#grd=Try_Model()","36f889ee":"#grd=Search_Model()\n#pd.DataFrame(grd.cv_results_)\n#best_model=grd.best_estimator_\n#best_model","52d2c755":"selected=Pipeline(steps=[('scaler', StandardScaler()),\n                ('reduce_dim', PCA(n_components=25)),\n                ('regressor', SVR(C=8.0, kernel='linear'))])","3de66a95":"'''\nselected=Pipeline(steps=[('scaler', StandardScaler()),\n                ('reduce_dim', PCA(n_components=9)),\n                ('regressor', Ridge(alpha=0.5))])\n'''","1763f96e":"selected.fit(train_X,train_Y)\nY_pred=selected.predict(test_X)","c8741ed4":"#import pickle\n#with open('best_model.pickle', 'wb') as handle:\n#    pickle.dump(best_model, handle, protocol=pickle.HIGHEST_PROTOCOL)","042d94aa":"ret=pd.DataFrame(test_data['id'])\nret['target']=Y_pred","ff7ff47d":"ret.to_csv('submission.csv',index=False)","04d89d38":"This notebook uses below given notebooks to make predictions.\n\n1. LB 0.468 https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-3\n2. LB 0.474 https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm","decc125b":"## From Embedding to Target","f6c07003":"## Submission"}}