{"cell_type":{"f7444821":"code","582bfcb4":"code","c1ab9614":"code","bce7c86a":"code","e4667401":"code","95e4af5c":"code","383fdf7c":"code","c4afac58":"code","4f9f4343":"code","ac54aa24":"code","31b81c54":"code","bcf4de70":"code","fc4833f0":"code","b6a34f2c":"code","f5dfb41e":"code","c19dae0e":"code","63365a74":"code","c7856e0d":"code","c7436cd4":"code","ba13f96e":"code","8e78cf92":"code","6528cc8c":"code","0258013d":"code","0317eb4e":"code","8c5d6d2a":"code","1a5192da":"code","531a19bf":"code","5df17747":"code","9a0a7d23":"code","76414771":"code","ba4943e8":"code","410a39b1":"code","5bbed4c4":"code","f0f0e57f":"code","f2e44f10":"code","d4ea42c6":"code","1cc2ec22":"code","ad9f3f01":"code","d947b15a":"code","00e3078e":"code","a23fe9a9":"markdown","48b5fed2":"markdown","c57d5bb4":"markdown","a8736b2d":"markdown","5ced374f":"markdown","b5db8481":"markdown","0ed232d9":"markdown","f5bf66f7":"markdown","0af6d549":"markdown","0c65ed11":"markdown","dfee5de8":"markdown"},"source":{"f7444821":"__author__ = 'prokaggler: https:\/\/www.kaggle.com\/prokaggler' \nfrom contextlib import contextmanager\nimport time \nfrom pathlib import Path\n\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\nimport tensorflow_addons as tfa\nfrom keras.layers import Dense, LSTM, BatchNormalization, Dropout\nfrom tensorflow_addons.layers import WeightNormalization\nimport tensorflow as tf\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\ntf.random.set_seed(0)\n\nimport xgboost\nfrom sklearn.multiclass import OneVsRestClassifier\nimport lightgbm","582bfcb4":"global train, test, target, submission, n_comp, tx, sc, pca, test_pca, X, le, X_pca, clf, model","c1ab9614":"@contextmanager\ndef timer(name):\n    start = time.time()\n    yield\n    print(f\"[{name}] done in --> {time.time() - start:0f} s \")","bce7c86a":"import os\ninput_dir = '\/kaggle\/input'\n\ndef read_csv(input_dir:str):\n    print(\"list files...\")\n    for dirname, _, filenames in os.walk(input_dir):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))","e4667401":"def load_dataset():\n    print(\"read csv ...\")\n    global train, test, target, submission\n    train = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_features.csv\")\n    test = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\n    target = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_scored.csv\")\n    submission = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")","95e4af5c":"with timer(\"load dataset\"):\n    read_csv(input_dir=input_dir)\n    load_dataset()","383fdf7c":"train.head()","c4afac58":"test.head()","4f9f4343":"target.head()","ac54aa24":"print(f\"train: {train.shape}\")\nprint(f\"test: {test.shape}\")\nprint(f\"target: {target.shape}\")","31b81c54":"cat_cols = train.columns[1:4].values \nnumeric_cols = train.columns[4:].values\nprint(\"categorical columns:\", len(cat_cols))\nprint(\"numeric columns:\", len(numeric_cols))","bcf4de70":"cells = [c for c in train.columns.values if c.startswith(\"c-\")]\ngenes = [c for c in train.columns.values if c.startswith(\"g-\")]","fc4833f0":"def derive_features(df):\n    df[\"sum\"] = np.sum(df.iloc[:, 4:876], axis=1)\n    df[\"mean\"] = np.mean(df.iloc[:, 4:876], axis=1)\n    df[\"std\"] = np.std(df.iloc[:, 4:876], axis=1)\n    df[\"var\"] = np.var(df.iloc[:, 4:876 ], axis=1)","b6a34f2c":"def label_enc(df):\n    le = LabelEncoder()\n    for cols in df[cat_cols]:\n        df[cols] = le.fit_transform(df[cols])","f5dfb41e":"tx = None\ndef min_max(df):\n    sc = MinMaxScaler(feature_range=(0,1))\n    tx = sc.fit_transform(df.values)\n    return sc, tx","c19dae0e":"label_enc(train)\nderive_features(train)\ntrain = train.merge(target, on = \"sig_id\")\n# train = train[train[\"cp_type\"] != \"ctl_vehicle\"]\ntrain.drop(\"sig_id\", axis=1, inplace = True)\nsc, tx = min_max(train)\ntrain.iloc[:, :879].head()","63365a74":"# target label -> 879 onwards\ntrain.iloc[:, 879:].head()","c7856e0d":"print('\\nRunning PCA ...')\nn_comp = 463\npca = PCA(n_components=n_comp, svd_solver='full', random_state=123)\nX_pca = pca.fit_transform(train.iloc[:, :879])\nprint('Explained variance: %.4f' % (pca.explained_variance_ratio_.sum() * 100))\n","c7436cd4":"# X = pd.DataFrame(X_pca, columns=[\"feature_\"+str(c) for c in range(X_pca.shape[1])])\n# X.head()","ba13f96e":"X_train, X_val, y_train, y_val = train_test_split(X, train.iloc[:, 879: ], test_size=0.1, random_state=123)\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","8e78cf92":"label_enc(test)\nderive_features(test)\n# test = test[test[\"cp_type\"] != \"ctl_vehicle\"]\ntest.drop(\"sig_id\", axis=1, inplace = True)\n# sc, tx = min_max(test)\ntest.head()","6528cc8c":"# print('\\nRunning PCA ...')\n# n_comp = 463\n# pca = PCA(n_components=n_comp, svd_solver='full', random_state=123)\n# test_pca = pca.fit_transform(test.iloc[:, :880])\n# print('Explained variance: %.4f' % (pca.explained_variance_ratio_.sum() * 100))","0258013d":"# X_test = pd.DataFrame(test_pca, columns=[\"feature_\"+str(c) for c in range(test_pca.shape[1])])\n# X_test.head()","0317eb4e":"test.head()","8c5d6d2a":"from  sklearn.preprocessing import MinMaxScaler","1a5192da":"mn = MinMaxScaler(feature_range=(-1,1))\nX = mn.fit_transform(train.iloc[:, :879])\nmn_test = MinMaxScaler() \nmntest = MinMaxScaler(feature_range=(-1,1))\nX_test = mntest.fit_transform(test.values)","531a19bf":"X_train, X_val, y_train, y_val = train_test_split(X, train.iloc[:, 879: ], test_size=0.1, random_state=123)","5df17747":"!rm  -rf  model* *.csv *.png","9a0a7d23":"input_dim = X.shape[1]\noutput_dim = 206\ny =  train.iloc[:, 879:]\ndef mlp_classifier(X_train, y_train, X_val, y_val, input_dim, output_dim):\n    \n    input_features = tf.keras.Input(shape=input_dim, dtype=\"float32\")\n    x = Dense(8, activation= 'relu')(input_features)\n    x = Dense(8, activation = 'relu') (x)\n    x = Dense(8, activation = 'relu') (x)\n    x = Dense(8, activation = \"relu\")(x)\n\n    out = Dense(output_dim, activation = \"sigmoid\")(x)\n    model = tf.keras.Model(input_features, out)\n    \n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(.0011), metrics=['categorical_accuracy'])\n    print(\"Model Summary:\\n \", model.summary())\n    \n    cbs = [\n    tf.keras.callbacks.EarlyStopping(patience=2),\n    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.6f}_.h5', save_best_only=True, monitor='val_loss', mode='min'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n\n]\n    \n    for i in range(1):\n        # X.values, y.values\n        model.fit(X, y, batch_size=2**(11 + i), epochs= 3000, callbacks=cbs , verbose=1,  validation_data=(X_val, y_val))\n        \n    print(\"model trained and saved...\")\n    return model\n\nmlp_classifier(X_train, y_train, X_val, y_val, input_dim, output_dim)","76414771":"input_dim = X.shape[1]\noutput_dim = 206\ny =  train.iloc[:, 879:]\ndef lstm_classifier(X_train, y_train, X_val, y_val, input_dim, output_dim):\n    \n    input_features = tf.keras.Input(shape=input_dim, dtype=\"float32\")\n    x = tf.keras.layers.Reshape((1, input_dim))(input_features)\n    x = LSTM(32, return_sequences=True)(x)\n    x = LSTM(8, return_sequences=True)(x)\n    x = LSTM(4)(x)\n    x = Dense(8, activation=\"relu\")(x)\n    out = Dense(output_dim, activation = \"sigmoid\")(x)\n    model = tf.keras.Model(input_features, out)\n    \n    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.RMSprop(.0011), metrics=['categorical_accuracy'])\n    print(\"Model Summary:\\n \", model.summary())\n    \n    cbs = [\n    tf.keras.callbacks.EarlyStopping(patience=12),\n    tf.keras.callbacks.ModelCheckpoint(filepath='model2.{epoch:02d}-{val_loss:.6f}_.h5', save_best_only=True, monitor='val_loss', mode='min'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n\n]\n    \n    for i in range(1):\n        model.fit(X, y, batch_size=2**(11 + i), epochs= 3000, callbacks=cbs , verbose=1,  validation_data=(X_val, y_val))\n    tf.keras.utils.plot_model(model, to_file='lstm.png')\n    print(\"model trained and saved...\")\n\nlstm_classifier(X_train, y_train, X_val, y_val, input_dim, output_dim)","ba4943e8":"input_dim = X.shape[1]\noutput_dim = 206\ny =  train.iloc[:, 879:]\ndef mlp_classifier2(X_train, y_train, X_val, y_val, input_dim, output_dim):\n    input_features = tf.keras.Input(shape=(input_dim), dtype=\"float32\")\n#     x = tf.keras.layers.Reshape((input_dim))(input_features)\n    x = tf.keras.layers.BatchNormalization()(input_features)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation='relu'))(x)\n    \n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation='relu'))(x)\n    \n    x = tf.keras.layers.BatchNormalization()(x)\n    out = tfa.layers.WeightNormalization(tf.keras.layers.Dense(output_dim, activation = \"sigmoid\"))(x)\n    model = tf.keras.Model(input_features, out)\n    \n    model.compile(loss=BinaryCrossentropy(label_smoothing=0.1), optimizer=tf.keras.optimizers.RMSprop(.0011 ), metrics=['categorical_accuracy'])\n    print(\"Model Summary:\\n \", model.summary())\n    \n    cbs = [\n    tf.keras.callbacks.EarlyStopping(patience=2),\n    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.6f}_.h5', save_best_only=True, monitor='val_loss', mode='min'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n\n]\n    \n    for i in range(1):\n        model.fit(X, y, batch_size=2**(11 + i), epochs= 3000, callbacks=cbs , verbose=1,  validation_data=(X_val, y_val))\n    tf.keras.utils.plot_model(model, to_file='mlp2.png')\n    print(\"model trained and saved...\")\n\nmlp_classifier2(X_train, y_train, X_val, y_val, input_dim, output_dim)","410a39b1":"input_dim = X.shape[1]\noutput_dim = 206\ny =  train.iloc[:, 879:]\ndef mlp_classifier2(X_train, y_train, X_val, y_val, input_dim, output_dim):\n    model = tf.keras.Sequential([tf.keras.layers.Input(input_dim)])\n                \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation='relu')))\n    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation='relu'))) \n    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(output_dim, activation=\"sigmoid\")))\n    \n        \n    print(\"Model Summary:\\n \", model.summary())\n    \n    model.compile(optimizer=tf.optimizers.RMSprop(lr = 1e-3), \n                  loss=BinaryCrossentropy(label_smoothing=0.1),\n                  )\n    \n    cbs = [\n    tf.keras.callbacks.EarlyStopping(patience=2),\n    tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.6f}_.h5', save_best_only=True, monitor='val_loss', mode='min'),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n\n]\n    \n    for i in range(1):\n        model.fit(X, y, batch_size=2**(11 + i), epochs= 3000, callbacks=cbs , verbose=1,  validation_data=(X_val, y_val))\n    tf.keras.utils.plot_model(model, to_file='mlp2.png')\n    print(\"model trained and saved...\")\n\nmlp_classifier2(X_train, y_train, X_val, y_val, input_dim, output_dim)","5bbed4c4":"# WGT = df.iloc[0,1:].values\n# oof = np.zeros((len(train),206))\n# preds = np.zeros((len(test),206))\n\n# df_oof = targets.copy()\n# df_oof.iloc[:,1:-1] = oof\n# df_oof.to_csv('oof.csv',index=False)\n\n# oof = np.clip(oof,0.0005,0.999)\n# oof[train.cp_type==1,] = 0\n\n# cv_score = log_loss( targets.iloc[:,1:-1].values.flatten(), oof.flatten() )\n# print('CV SCORE with CLIPPING = %.5f'%cv_score)","f0f0e57f":"def load_best_model():\n    \"\"\"  loads the best trained Model \"\"\"\n    path=Path(os.getcwd())\n    models = (list(path.glob(\"*.h5\")))\n    val_loss_model = []\n    for files  in  models:\n        models_val_loss = files.as_posix().split(\"-\")[1].split(\"_\")[0]\n        val_loss_model.append(models_val_loss)\n    min_val_loss = min(val_loss_model)\n    print(\"min_val_loss: \", min_val_loss)\n    best_model_path = next(path.glob(f\"*-{min_val_loss}_*\")).as_posix()\n    print(\"best saved model found at: \",best_model_path )\n    model = tf.keras.models.load_model( best_model_path )\n    return model","f2e44f10":"def evaluate_model(X_val, y_val, X_test, model):\n    print(\"Evaluate on test data\")\n    results = model.evaluate(X_val, y_val, batch_size=128)\n    print(\"val loss, val acc:\", results)\n    predictions = model.predict(X_test)\n    print(\"predictions shape:\", predictions.shape)\nmodel = load_best_model()\nevaluate_model(X_val, y_val, X_test, model)","d4ea42c6":"def predict(X_test):\n    model = load_best_model()\n    out = model.predict(X_test)\n    return out\npredictions = predict(X_test)","1cc2ec22":"predictions = np.clip(predictions,0.0005,0.999)\n# oof[train.cp_type==1,] = 0\npredictions = predictions.round(4)","ad9f3f01":"#sample file\nsubmission.head()","d947b15a":"def create_submission(predictions=np.array):\n    out = pd.DataFrame(data=predictions, columns=submission.columns[1:].tolist(), index=submission.sig_id)\n    sub_file_name = \"submission.csv\"\n    out.to_csv(sub_file_name, index=True)\n    print(\"Submission file saved: \", sub_file_name)\ncreate_submission(predictions)","00e3078e":"pd.read_csv(\"submission.csv\")","a23fe9a9":"# Preprocessing Enhancements\n* 1. Train and Test data Normalization\n* 2. Train\/Test\/Target vectorization\n","48b5fed2":"# Predict","c57d5bb4":"# 3.Feature Engineering","a8736b2d":"# Prepare","5ced374f":"## 3.1 dimentionality reduction","b5db8481":"### Enhancements\n- [ ] Multilable cross validation\n- [ ] Model recconstruction\n- [ ] Hyperparameter optimiztion\n- [x] Create submission file module\n- [ ] Handle class imbalance and preprocess \n","0ed232d9":"# 2.Load Dataset","f5bf66f7":"# !!! Please **UPVOTE** if you like the kernel","0af6d549":"# Build","0c65ed11":"# Create Submission File","dfee5de8":"# 1.Load lib"}}