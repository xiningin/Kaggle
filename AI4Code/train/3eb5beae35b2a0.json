{"cell_type":{"aa176261":"code","bd297e7e":"code","67534887":"code","0ae75358":"code","a86e761c":"code","775ef5b3":"code","fd8875d2":"code","1da1d598":"code","41249791":"code","6f726d41":"code","ae03c450":"code","8cc81deb":"code","334ef9a1":"code","d9e2fded":"code","27cf763e":"markdown","caa7b593":"markdown","f1f1befd":"markdown","86407780":"markdown","25edbf74":"markdown","b12e854f":"markdown","830c0660":"markdown","28252e28":"markdown","a1f44085":"markdown"},"source":{"aa176261":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","bd297e7e":"from sklearn.datasets import make_classification\n\nfeat = 5 # specify the number of features\nX, y = make_classification(n_samples = 100, n_features=feat,\n                                n_redundant=0, n_informative=2,\n                                n_clusters_per_class=1, flip_y = 0.1,\n                                class_sep = 0.5, random_state=0)","67534887":"dataset = pd.DataFrame(X, columns=[ 'Feature_'+str(i) for i in range(1,feat+1) ])\ndataset['Label'] = y\ndataset","0ae75358":"def sig(x):\n    return 1 \/ (1 + np.exp(-x))","a86e761c":"values = np.linspace(-10, 10, 100)\nplt.figure()\nplt.plot(values, sig(values))\nplt.axhline(y=0.5, color=\"black\", linestyle='--', alpha=0.8, lw=0.5)\nplt.axvline(color = 'black', linestyle ='--', alpha=0.8, lw=0.5)\nplt.title('Sigmoid Function')\nplt.ylabel(' sig(x) --> ')\nplt.xlabel(' x --> ')\nplt.show()","775ef5b3":"def costl(x, y, th, l=1, loc=None):\n    m = len(x)\n    if loc != 'in':\n        x = sig(np.hstack((np.array([1]*m).reshape(-1,1), x)))\n        \n    h = sig(x.dot(th)) # new hypothesis squished b\/w 0 & 1 using sigmoid function\n    \n    c1 = (y.ravel()).dot(np.log(h)) # cost calculated when target is 1\n    \n    c2 = ((1-y).ravel()).dot(np.log(1-h)) # cost calculated when target is 0\n    \n    penalty = np.sum(th[1:]**2)\/(2*m)*l # l2 regularization to prevent over-fitting\n    return ((c1 + c2) \/ (-m))[0] + penalty","fd8875d2":"def gdl(x, y, a=0.1, itr=100,l2=1, graph=0):\n    m, n = x.shape\n    # m -> no of datapoints\n    # n -> no of features\n    \n    th = np.array([0]*(n+1)).reshape(-1,1)\n    # th -> set intercept and coefficient values to 0 initially\n    \n    x = np.hstack((np.ones((m,1)), x))\n    # add a row of 1s to the dataset to mulitply with the intercept term in th\n    \n    y = y.reshape(-1,1)\n    cst = [costl(x, y, th, l=l2, loc='in')]\n    # list to store the cost to check for convergence\n    \n    pen = np.array([0] + [(a*l2\/m)]*n).reshape(-1,1)\n    # penalty term need not penalize the intercept term\n    \n    for i in range(itr):\n        der = (x.T).dot(sig(x.dot(th)) - y) \/ m\n        # gradient of the cost function\n        \n        th = (th-(pen*th)) - (a * der) #updated th\n        cst.append(costl(x, y, th, l=l2, loc='in')) #cost for updated th\n    if graph == 1:\n        plt.figure()\n        plt.plot(cst) #plot the cost wrt iterations\n        plt.title('Convergence curve for alpha = {} and max_iteration = {}'.format(a, itr))\n        plt.ylabel('Cost function')\n        plt.xlabel('Iteration')\n        plt.show()\n    return th","1da1d598":"out = gdl(X, y, a=9, itr=500, graph=1)","41249791":"print('Intercept: ', out[0,0])\nprint('Coefficients: ', out[1:].ravel()) # bad parameters estimated","6f726d41":"out = gdl(X, y, a=0.0001, itr=500, graph=1)","ae03c450":"print('Intercept: ', out[0,0])\nprint('Coefficients: ', out[1:].ravel()) # bad parameters estimated","8cc81deb":"out = gdl(X, y, a=0.5, itr=500, graph=1)","334ef9a1":"print('Intercept: ', out[0,0])\nprint('Coefficients: ', out[1:].ravel()) # bad parameters estimated","d9e2fded":"from sklearn.linear_model import LogisticRegression\nlor = LogisticRegression(penalty='l2', C=1).fit(X, y) # Note: C = 1\/lambda\nprint('Intercept: ', lor.intercept_[0])\nprint('Coefficients: ', lor.coef_[0])","27cf763e":"# Logistic Regression\n\n  It is one of the most widely used algorithm in machine learning for classification. In this notebook let us understand how to implement it with numpy. We will also be using pandas and matplotlib for getting a good intiution of it's working. \n  \n  Gradient Descent has been used here as a solver, the main job of a solver is to find the minimum of the cost function. The cost function gives us the error value while comparing our predicted value with the actual value. A logarithmic cost function has been used here since we are building a binary classifier whose labels are 0 or 1.","caa7b593":"### Sigmoid Function\n\n A sigmoid is a special function which converts all its input values in the range of 0 and 1. It is used here because all our prediction should be in the range of 0 and 1. If our prediction < 0.5 then it belongs to class 0. If our prediction >=0.5 then it belongs to class 1.","f1f1befd":"### Intercept and Coefficient calculating Function for logistic regression using Gradient Descent\n\nx -> the training dataset, a 2D array.\n\ny -> target values, 1D array.\n\na -> learning rate.\n\nitr -> maximum number of iterations to be performed.\n\ngraph -> To check if the model has coverged or not.\n         Set 1, to see the plot. \n         Set 0, to get the intercept and coefficients alone, without the plot.\n         \nl2 -> lambda term, specifies the degree of regularization","86407780":"### Logarithmic Cost Function for logistic regression\n\nL2 regularization penalty has been used here, Regularizations are used in order to prevent over-fitting. We try to penalize the model for fitting very well to the training set because the training set might contain noise, outliers, mis-labelled points, etc whose effect can be minimized by regularization.","25edbf74":"## Creating synthetic datasets for testing","b12e854f":"### Model takes more number of iterations to converge when learning rate (a) is very small","830c0660":"### Verifying Results with Scikit-learn","28252e28":"### A non-converging model for high learning rate(a)","a1f44085":"### A perfectly converging model for appropriate learning rate(a)"}}