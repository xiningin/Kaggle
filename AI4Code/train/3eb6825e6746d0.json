{"cell_type":{"91209b44":"code","15d57bbb":"code","7327493d":"code","d66cd540":"code","3f2b2f71":"code","76dde96c":"code","6bbc213f":"code","dfb6507f":"code","7a0e1c7a":"code","2c465cae":"code","52dcfcab":"code","2512e35b":"code","c81c3b54":"code","1325bdd2":"code","06112dd1":"code","1befb130":"code","83b0a046":"code","5f3d37ce":"code","9f4ab2ee":"code","bece33c9":"code","dfecd212":"code","8e8e6fd1":"code","97341814":"markdown","bf0d349b":"markdown","a48e70db":"markdown","46ae0e25":"markdown","d5d60985":"markdown","884d0d9b":"markdown","bd51f313":"markdown","9e521c6c":"markdown","5c084a60":"markdown","49c4749e":"markdown","ccb65566":"markdown","e5e8ac0c":"markdown","772add1f":"markdown","c37ef81e":"markdown","9a64123c":"markdown","28c0a26d":"markdown","783ea48b":"markdown","c88ee22e":"markdown","6e969e4e":"markdown","01430ddd":"markdown","e1690202":"markdown","5603d87e":"markdown","012d9d40":"markdown","9473ef94":"markdown","547eaafa":"markdown"},"source":{"91209b44":"import numpy as np \nimport pandas as pd\nfrom scipy.stats import kurtosis, skew\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nimport xgboost as xgb\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n","15d57bbb":"data = pd.read_csv('..\/input\/car-prediction-using-regression\/Car details v3.csv')","7327493d":"data","d66cd540":"# start by converting ruppees to dollars\ndata['selling_price'] = data['selling_price']\/74.93\n\n# remove torque- almost same as power (related mathematically- and linearly)\ndata.drop('torque', axis='columns', inplace=True)\n\n# show unique values in class columns\nprint('\\n unique values in transmission column \\n',data['transmission'].unique())\nprint('\\n unique values in seller type column \\n', data['seller_type'].unique())\nprint('\\n unique values in owner column \\n', data['owner'].unique())\nprint('\\n unique values in seats column\\n', data['seats'].unique())","3f2b2f71":"# change names for convenience\ndata.rename(columns = {'selling_price':'price', 'km_driven':'miles', 'mileage':'consumption', 'max_power':'power', 'seller_type':'seller', 'name':'manuf.'}, inplace=True )\ndata = data[['manuf.', 'fuel','seller','transmission','owner','year','miles', 'engine','consumption','power','seats','price']]\n\n# convert mileage to floats\ndata['consumption'] = data['consumption'].apply(lambda x: float(str(x).split()[0]) if pd.notnull(x) else x)\n\n# get car manufactur   data.rename(columns = {'selling_price':'price', 'km_driven':'mileage', 'mileage':'consumption', 'max_power':'power', 'seller_type':'seller', 'name':'manuf.'}, inplace=True )\ndata = data[['manuf.', 'fuel','seller','transmission','owner','year','miles', 'engine','consumption','power','seats','price']]\ndata['manuf.'] = data['manuf.'].apply(lambda x: (str(x).split()[0]) if pd.notnull(x) else x)\n\n# convert max_power to floats\ndata['engine'] = data['engine'].apply(lambda x: float(str(x).split()[0]) if pd.notnull(x) else x)\n\n# deal with problematic cell (containing 'bhp' only)\ndata['power'].replace(' bhp',np.nan, inplace=True)\ndata['power'] = data['power'].apply(lambda x: str(x) if pd.notnull(x) else x)\n\n# convert engine volume to float\ndata['power'] = data['power'].apply(lambda x: float(x.split()[0]) if pd.notnull(x) else x)\n\n# convert owner type\ndata['owner'] = data['owner'].apply(lambda x: (str(x).split()[0]) if pd.notnull(x) else x)\n\n# transform years from year issued to years old for convenience\ndata['year'] = 2021.0 - data['year']","76dde96c":"\"\"\" group together all instances with manuf. that has 15 and less occurences in data\nI am assuming that 15 and less occurences in this dataset is not enough for a decent evaluation\nTherefore I am aknowledging thess manuf. as \"Other\" in my evaluation.\"\"\"\n\nmanuf_occurences_in_data = data['manuf.'].value_counts().sort_values()\nprint(\"\\nmanuf. occurences in data: \\n\\n\", manuf_occurences_in_data)\nprint(\"\\n\")\n\n# manufactureres with 15 and less occurunces are merged as others\nmanuf_single_occurence = manuf_occurences_in_data[manuf_occurences_in_data <= 15].index\nfor manuf in manuf_single_occurence:\n    data = data.replace(to_replace=manuf, value='Other')\n\n# show that \"Other\" field in manuf. has been formed\nmew_manuf_occurences_in_data = data['manuf.'].value_counts().sort_values()\nprint(\"processed manuf. occurences in data \\n\\n\", mew_manuf_occurences_in_data)\nprint(\"\\n\")\n\n# show column with nan values\nna_cols = data.isna().any()\nna_cols = na_cols[na_cols == True] \nprint(\"these are nan cols \\n\")\nprint(na_cols)\nprint(\"\\n\")\n\n#find columns with zero cells\nprint(\"columns with zero cells \\n\")\nprint((data==0).sum(axis=0))\nprint(\"\\n\")\n\n# we can now see that consumption here is 0 which is not posiible at all, same for power\ncols_with_zero = list(((data==0).sum(axis=0)!=0).index[((data==0).sum(axis=0)!=0).values])\nfor zero_col in cols_with_zero:    \n    col_median = data[zero_col].median()\n    data[zero_col].replace(to_replace=0, value=col_median, inplace=True)\n\nprint(\"columns with zero cells after processing \\n\")\nprint((data==0).sum(axis=0))\nprint(\"\\n\")\n\n# input median into nan values\nfor col in na_cols.index:\n    median = data[col].median(skipna=True)\n    data[col] = data[col].fillna(median)\n    \nprint(\"show that there are no more nan values- is null: \\n\")\nprint(data.isnull().values.any())\n\n# show column with nan values\nna_cols = data.isna().any()\nna_cols = na_cols[na_cols == True] \nprint(\"these are nan cols \\n\")\nprint(na_cols)\nprint(\"\\n\")","6bbc213f":"data","dfb6507f":"numeric_data = data.iloc[:,5:]\nnum_of_cols = (len(numeric_data.columns))\n# generate skewness vectors\nskew_vec = []\nkurt_vec = []\nmean_vec = []\n# iterate over numeric columns and for each one calculate skewness and kurtosis    \nfor col in range(num_of_cols):\n\n    a_skew = skew(numeric_data.iloc[:,col])\n    a_kurt = kurtosis(numeric_data.iloc[:,col])\n    a_mean = np.mean(numeric_data.iloc[:,col])\n    skew_vec.append(a_skew)\n    kurt_vec.append(a_kurt)\n    mean_vec.append(a_mean)\n    \n# cast to array   \nskew_vec = np.array(skew_vec)\nkurt_vec = np.array(kurt_vec)\nmean_vec = np.array(mean_vec)\n\n# show histograms for each columns\nfor col in range(num_of_cols):\n\n    plt.figure()\n    plt.grid()\n    fig = sns.histplot(data = numeric_data.iloc[:,col])\n    plt.text(0.8, 0.95,'skew. = '+\"{:.2f}\".format(skew_vec[col]), fontsize=9,transform=fig.transAxes)\n    plt.text(0.8, 0.9,'kurt.  = '+\"{:.2f}\".format(kurt_vec[col]), fontsize=9,transform=fig.transAxes)\n    plt.text(0.8, 0.85,'mean  = '+\"{:.2f}\".format(mean_vec[col]), fontsize=9,transform=fig.transAxes)\n\n    plt.show()","7a0e1c7a":"def get_feature_rating(train_data_X_Y, feature):\n\n    # get all manuf list    \n    feature_list = data[str(feature)].unique()\n    feature_length = len(feature_list)\n    \n    # how many times each manuf appeared\n    feature_appearences_in_data = data[str(feature)].value_counts().sort_values().sort_index()\n    feature_ratio_in_data = feature_appearences_in_data\/len(data)\n    \n    # show which manuf. are priciest\n    feature_price_mean = data.groupby([str(feature),]).mean()['price']\n    \n    # set to sort\n    feature_appearence_in_data_sorted = feature_appearences_in_data.sort_values(ascending=True) \n    feature_appearence_in_data_sorted = feature_appearence_in_data_sorted.index\n    \n    # set weights in accordance to appearnces\n    order = np.arange(1,feature_length+1, dtype='float64')\n    weights = order\/feature_length\n    \n    # calculate rating\n    weights = pd.DataFrame(data=weights, index=feature_appearence_in_data_sorted)\n    feature_rating = weights.sort_index()\n    feature_rating = feature_price_mean.multiply(feature_rating[0])\n    feature_order = feature_rating.sort_values().index\n    feature_rating_final = pd.DataFrame(data=order, index=feature_order)\n    \n    return feature_rating_final","2c465cae":"def replace_features_with_ratings(data, feature_rating):\n            \n    feature = feature_rating.index\n    order = feature_rating.iloc[:,0]\n    data = data.replace(to_replace=feature, value=order)\n\n    return data","52dcfcab":"def print_scores(model_name, model):\n    print(\"\\n\"+model_name+\" train rmse score\\n\", model['train_neg_root_mean_squared_error'])\n    print(model_name+\" val RMSE score\\n\",model['test_neg_root_mean_squared_error'])\n    print(\"\\n\"+model_name+\"  train r2 score\\n\",model['train_r2'])\n    print(model_name+\" val R2 score\\n\",model['test_r2'])\n    print(\"\")\n","2512e35b":"def one_hot(data):\n    data = pd.get_dummies(data, drop_first=True)\n    return data","c81c3b54":"def standardize(data):\n    stand_scaler = StandardScaler()\n    data = stand_scaler.fit_transform(data)\n    return data, stand_scaler","1325bdd2":"def compare_w_wo_feature_scores(cv_all_models, model_names):\n    \n    model_train_wo_ratings = []\n    model_train_w_ratings = []\n    model_test_wo_ratings = []\n    model_test_w_ratings = []\n    \n    number_of_models = len(cv_all_models)\n    kfolds = np.arange(1,11,1)\n    for model_index, model_name in zip(range(int(number_of_models\/2)), model_names):\n        for error_type,train_label, test_label in zip(['Negative RMSE', 'R2 Score'],['train_neg_root_mean_squared_error','train_r2'],['test_neg_root_mean_squared_error','test_r2']):\n            plt.figure()\n            plt.grid()\n            \n            # plot train w vs wo ratings\n            plt.scatter(x=kfolds, y=cv_all_models[model_index][train_label], marker='^',c='b', label='Without Ratings')\n            plt.scatter(x=kfolds, y=cv_all_models[model_index+6][train_label], marker='s',c='r',label='With Ratings')\n            plt.xlabel(xlabel='kfold')\n            plt.ylabel(ylabel=error_type)\n            plt.title(model_name+\" Train \" +error_type)\n            plt.legend()\n            plt.show()\n            model_wo_rating_train_mean =  np.mean(cv_all_models[model_index][train_label])\n            model_train_wo_ratings.append(model_wo_rating_train_mean)\n            print(model_name + \" Train WO ratings \"+ str(model_wo_rating_train_mean) )\n            model_w_rating_train_mean = np.mean(cv_all_models[model_index+6][train_label])\n            model_train_w_ratings.append(model_w_rating_train_mean)\n            print(model_name + \" Train W ratings \" +str(model_w_rating_train_mean))\n            print(\"\\n\")\n\n            # plot test w vs wo ratings\n            plt.figure()\n            plt.grid()\n            plt.scatter(x=kfolds, y=cv_all_models[model_index][test_label], marker='^',c='b',label='Without Ratings')\n            plt.scatter(x=kfolds, y=cv_all_models[model_index+6][test_label], marker='s',c='r',label='With Ratings')\n            plt.xlabel(xlabel='kfold')\n            plt.ylabel(ylabel=error_type)\n            plt.title(model_name +\" Test \" +error_type)\n            plt.legend()\n            plt.show()\n            model_wo_rating_train_mean =  np.mean(cv_all_models[model_index][train_label])\n            model_test_wo_ratings.append(model_wo_rating_train_mean)\n            print(model_name + \" Test WO ratings \"+ str(model_wo_rating_train_mean) )\n            model_w_rating_train_mean = np.mean(cv_all_models[model_index+6][train_label])\n            model_test_w_ratings.append(model_w_rating_train_mean)\n            print(model_name + \" Test W ratings \" +str(model_w_rating_train_mean))\n            print(\"\\n\")\n\n    \n    return model_train_wo_ratings, model_train_w_ratings,model_test_wo_ratings,model_test_w_ratings","06112dd1":"data_x = data.iloc[:,0:10] \ndata_y = data.iloc[:,11]","1befb130":"X_train, X_test, Y_train, Y_test = train_test_split(data_x, data_y, test_size = 0.3, random_state = 2)","83b0a046":"train_data_X_Y = pd.concat([X_train,Y_train], axis=1) \n","5f3d37ce":"    # set correlation matrix\n    correlation_mat = train_data_X_Y.corr()\n    # setup of triangle matrix\n    mask = np.triu(np.ones_like(correlation_mat, dtype=bool))\n    # setup the matplotlib figure\n    plt.figure(figsize=(10,10))\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(correlation_mat, mask=mask, cmap=cmap, vmax=1,vmin=-1, center=0,\n                square=True, linewidths=1, cbar_kws={\"shrink\": .5})","9f4ab2ee":"# store all models for future comparison\ncv_all_models = []\nmodel_names = [\"Linear Regression\", \"Regression Tree\",\"Random Forest\",\"KNN\",\"ADABoost\", \"Gradient Boosting\"]\nfeature_rating_dict = {}\n\n# copy the data for using the two options stated above\ntrain_data_X_Y_copy = train_data_X_Y.copy()\nX_test_copy = X_test.copy()\n\n# firstly with no ratings - one-hot. second iteration with the ratings.\nfor get_rating in [False, True]:\n    \n    # Predict using rating for features, \n    # This will replace classes with the rating leaving one hot encoding to the rest of the features - transmission and fuel\n    # When set to false, one hot encoding will replace ALL features with one hot encoding.\n    \n    if get_rating == True:\n        print(\"\\n\\n   Fit using rating for manuf. owner and seller \\n\\n\")\n        for feature in ['manuf.','owner','seller']:\n            feature_rating = get_feature_rating(train_data_X_Y_copy, feature)\n            train_data_X_Y_copy = replace_features_with_ratings(train_data_X_Y_copy, feature_rating)\n            feature_rating_dict[feature] = feature_rating\n    else:\n        print(\"\\n\\n   Fit using One-hot for manuf. owner and seller \\n\\n\")\n      \n    # in order to keep the original data\n    train_data_X_Y = train_data_X_Y_copy\n    \n    # needed price to get ratings - no need after getting the ratings\n    X_train= train_data_X_Y.drop('price', axis='columns')\n    X_train= one_hot(X_train)\n    X_train_cols = X_train.columns\n    \n    # scaling the train data- later scaling the test data according to same scaler\n    X_train, stand_scaler = standardize(X_train)\n    X_train_df = pd.DataFrame(data=X_train, columns=X_train_cols)\n    \n    # casting to numpy arrays\n    X_train = np.array(X_train_df)\n    Y_train = np.array(Y_train)\n    Y_test = np.array(Y_test)    \n    \n    # toggle rating and no rating for test (same as before)\n    if get_rating:\n        for feature in feature_rating_dict.keys():\n            X_test_copy = replace_features_with_ratings(X_test_copy, feature_rating_dict[feature])\n    X_test = X_test_copy\n    \n    # one hot for test \n    X_test = one_hot(X_test)\n    \n    # standardize test \n    X_test = stand_scaler.fit_transform(X_test)\n\n    # 5. FITTING ALGORIGHMS USING CV TO CHECK FOR BEST FIT- WITH AND WITHOUT THE RATINGS.\n    \n    \n    \"\"\"LINEAR REGRESSION\"\"\"\n        \n    print(\"\\n\\n---LINEAR REGRESSION---\")\n    # fit linear regression\n\n    lin_reg = LinearRegression()\n    cv_lin_reg = cross_validate(lin_reg, X_train, Y_train, cv=10, \n                                scoring=('r2','neg_root_mean_squared_error'),\n                                return_train_score=True, return_estimator=True)\n    print_scores('Linear Regression', cv_lin_reg)\n\n    # append to result list\n    cv_all_models.append(cv_lin_reg)\n\n    \n    \"\"\" REGRESSION TREE\"\"\"\n\n    print(\"\\n\\n---REGRESSION TREE---\")\n    dec_tree_reg = DecisionTreeRegressor(splitter='best', max_depth = 10)\n    cv_dec_tree_reg = cross_validate(dec_tree_reg, X_train, Y_train, cv=10, \n                                scoring=('r2','neg_root_mean_squared_error'),\n                                return_train_score=True, return_estimator=True)\n    print_scores('Regression Tree', cv_dec_tree_reg)\n    \n    # append to result list\n    cv_all_models.append(cv_dec_tree_reg)\n    \n    \"\"\"RANDOM FOREST\"\"\"\n\n    print(\"\\n\\n---RANDOM FOREST---\")\n    random_forest_regressor = RandomForestRegressor(max_depth=32, n_estimators = 400)\n    cv_random_forest_regressor = cross_validate(random_forest_regressor, X_train, Y_train, cv=10, \n                                scoring=('r2','neg_root_mean_squared_error'),\n                                return_train_score=True, return_estimator=True)\n    print_scores('Random Forest', cv_random_forest_regressor)\n    \n    # append to result list\n    cv_all_models.append(cv_random_forest_regressor)\n    \n    \"\"\"KNN\"\"\"\n\n    print(\"\\n\\n---KNN---\")\n    knn_regressor = KNeighborsRegressor(n_neighbors=5)\n    cv_knn_regressor = cross_validate(knn_regressor, X_train, Y_train, cv=10, \n                                scoring=('r2','neg_root_mean_squared_error'),\n                                return_train_score=True, return_estimator=True)\n    print_scores('KNN', cv_knn_regressor)\n    \n    # append to result list\n    cv_all_models.append(cv_knn_regressor)\n    \n    \"\"\"ADABOOST\"\"\"\n    \n    print(\"\\n\\n---ADABOOST---\")\n    adaboost_regressor = AdaBoostRegressor(n_estimators=100)\n    cv_adaboost_regressor = cross_validate(adaboost_regressor, X_train, Y_train, cv=10, \n                                scoring=('r2','neg_root_mean_squared_error'),\n                                return_train_score=True, return_estimator=True)\n    print_scores('AdaBoost Regressor', cv_knn_regressor)\n    \n    # append to result list\n    cv_all_models.append(cv_adaboost_regressor)\n    \n    \"\"\"XGBOOST\"\"\"\n    \n    print(\"\\n\\n---XGBOOST---\")\n    xgb_regressor= xgb.XGBRegressor (objective=\"reg:squarederror\", random_state=5, eval_metric=\"rmse\")\n    cv_xgboost_regressor = cross_validate(xgb_regressor, X_train, Y_train, cv=10, \n                                          scoring=('r2','neg_root_mean_squared_error'), return_train_score=True, return_estimator=True)\n    print_scores('XGBoost', cv_xgboost_regressor)\n    \n    # append to result list\n    cv_all_models.append(cv_xgboost_regressor)\n    \n    ","bece33c9":"    # plot differences\n    model_train_wo_ratings, model_train_w_ratings,model_test_wo_ratings,model_test_w_ratings = compare_w_wo_feature_scores(cv_all_models, model_names)\n    ","dfecd212":"    # two best models- Random Forest and XGBoost both with rating\n    \n    # retrain Random Forest Model from scratch on all data\n    final_rf_regressor = RandomForestRegressor(max_depth=32, n_estimators = 400)\n    final_rf_regressor.fit(X_train, Y_train)\n    final_rf_score = final_rf_regressor.score(X_test, Y_test)\n    final_rf_predictions = final_rf_regressor.predict(X_test)\n    \n    # retrain XGBoost Model from scratch on all data\n    final_xgb_regressor = xgb.XGBRegressor (objective=\"reg:squarederror\", random_state=5, eval_metric=\"rmse\") \n    final_xgb_regressor.fit(X_train, Y_train)\n    final_xgb_predictions = final_xgb_regressor.predict(X_test)\n    final_xgb_score = final_xgb_regressor.score(X_test, Y_test)\n    \n    # create final predictions \n    predictions_df = pd.DataFrame(data = {'Random Forest Predictions' :final_rf_predictions,'XGBoost Predictions': final_xgb_predictions,'Targets': Y_test})\n    ","8e8e6fd1":"    predictions_df","97341814":"Processed data:","bf0d349b":"Import libraries","a48e70db":"The next function prints the score for each model - along with the kfold scores using R2 and RMSE\n","46ae0e25":"From here on forward I will use only the train data for analysis\/scaling etc. and later make sure that the test data will undergo the same process.\nTo do so I will concat together X_train and Y_train together and continue to investigate the data.","d5d60985":"**----END OF ADDITIONL FUNCTIONS----**","884d0d9b":"Split into train and test.","bd51f313":"**3. Preprocessing and cleaning the train data.**","9e521c6c":"set data and labels.","5c084a60":"**1. Get data function- reads the downloaded csv file.**","49c4749e":"**ADDITIONAL FUNCTIONS NEEDED**","ccb65566":"Key points:\n1. We can see that \"power\" is heavily positive correlated, the same can be said to engine (which generates the power).\n2. \"year\" feature is negatively correlated which could be very intuitively explained- the newer the car the more expensive it is.  \n3. The same as line item no. 2 can be said for the \"miles\" feature, less miles means less wear and tear.","e5e8ac0c":"The next function plots the scores with and without ratings for each model used in the predictions. ","772add1f":"Moving forward.","c37ef81e":"**2. Understanding the data and initial cleaning.**","9a64123c":"**6. Choosing to predict with or without the ratings.**\n\n**7. Choosing best model for predictions.**\n","28c0a26d":"create correlation matrix to see mot important features","783ea48b":"**5. Fitting algorithms using CV to check best fit- with and without the ratings**\n","c88ee22e":"The next function executes one hot encoding ","6e969e4e":"This function calculates the rating for the features: manuf., owner, and seller\nThe rating is based on occurnces in data as well as price:\n1. If a car manuf. occures once and the price is high for instance, it might be misleading for it can be the top prices model for that particular manuf. therefore the mean price and the occurences in the data play a role in calculating the rating.\n2. The formula for the rating is  **manuf_weight * mean_price_of_manuf**\n* manuf_weight : There are 36 manuf., most prevalent manuf. will get a a weight of  1.0 and least prevalent will get a rating of 1\/36.","01430ddd":"Unprocessed data:","e1690202":"The next function replaces manuf. with the rating calculated beforehand. ","5603d87e":"**4. Generating rating for class features.**\n\n\nAt this stage I will check whether class features should get numeric rating rather than One-Hot encoding. This is under the notion that:\n1. The manufacturer of the car affects the price of the car e.g. a Mercedes is usually more luxurious than a fiat and therefore is more expensive.\n2. The owner also affects the price, first hand is preferable to second which is better than third etc.\n3. In addition, seller is also a key feature. who sells the car? Individual, dealer or trustmark dealer. \n\nI will rate them all-according to the data, then fit the models along with the ratings.\nLater I will use One-Hot encoding and again, fit all the models from scratch.\nFinally I will compare the two options and concure which results in better outcomes. \n\nAll next predictions will be done twice due to this idea. The test stage will include only the better option. ","012d9d40":"The next function standatdizes the input data and returns the scaler object. ","9473ef94":"**In this notebook I will predict car prices using machine learning algorithms.**\nThe scope of this notebook:\n1. Reading the data.\n2. Understanding the data & initial cleaning.\n3. Preprocessing and and more cleaning of the data.\n4. Generating rating for class features. (explained later in the chapter)\n5. Fitting algorithms using CV to check best fit- with and without the ratings from line item no. 4. \n6. Decide to predict with or without the ratings.\n7. Choosing best model for predictions.\n8. Predicting on test data after it has been processed in the same manner as the train data.\n\nLet's put the pedal to the metal...\n","547eaafa":"**8. Predicting on test data after it has been processed in the same manner as the train data.**"}}