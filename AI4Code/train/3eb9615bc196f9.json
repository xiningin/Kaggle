{"cell_type":{"e08766b2":"code","6bda133a":"code","d7fb49a0":"code","46bd8ad3":"code","0604823e":"code","1a5b9956":"code","6b6ad8a1":"code","738ce779":"code","d092cb93":"code","fae37a9a":"code","8ed11452":"code","b106744d":"code","d336632c":"code","d868eed8":"code","75c9aca5":"code","43947cc6":"code","048f729b":"code","f41bd6bb":"code","aa381e60":"code","925ae687":"code","decc24ed":"code","4a683fd0":"code","5d4224bb":"code","0409ea80":"code","78f63744":"code","5f67b9a1":"code","80b5de0f":"code","6f17968c":"code","b5cb3370":"code","0b260fa1":"code","f6504a35":"code","362585af":"code","bd29ca45":"code","868545ac":"code","6a5e429f":"code","8e175901":"code","fa0bb980":"code","10b88519":"markdown","f3f4dd45":"markdown","8f6895d5":"markdown","1a3dbe2b":"markdown","6a16753b":"markdown","0adbf144":"markdown","fa021397":"markdown","895cf8cf":"markdown","1992e6e2":"markdown","2f4939b9":"markdown","cdd88210":"markdown","8c306ed2":"markdown"},"source":{"e08766b2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nimport os\nfrom tqdm import tqdm\nimport json\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')","6bda133a":"train_json = '..\/input\/region-proposals-of-crop-weed-dataset\/train.json'\ntest_json = '..\/input\/region-proposals-of-crop-weed-dataset\/test.json'\nimages_path = '..\/input\/crop-and-weed-detection-data-with-bounding-boxes\/agri_data\/data\/'\nmodel_path  = '..\/input\/rcnn-training-part-1-finetuning\/RCNN_crop_weed_classification_model.h5'\nlabel_csv = '..\/input\/convert-yolo-labels-to-pascalvoc-format\/pascal_voc_format.csv'\nnegative_ex_path = '..\/input\/rcnn-data-preprocessing-part-2\/Train\/background\/'","d7fb49a0":"with open(train_json,'r') as train:\n    train_region = json.load(train)","46bd8ad3":"with open(test_json,'r') as test:\n    test_region = json.load(test)","0604823e":"train_images_list = list(train_region.keys())\ntest_images_list = list(test_region.keys())","1a5b9956":"print(len(train_images_list))\nprint(len(test_images_list))","6b6ad8a1":"labels = pd.read_csv(label_csv)\nlabels.head()","738ce779":"model = tf.keras.models.load_model(model_path)","d092cb93":"model.summary()","fae37a9a":"model_without_last_2FC = tf.keras.models.Model(model.inputs,model.layers[-5].output)","8ed11452":"model_without_last_2FC.summary()","b106744d":"train_features = []\n\ntest_features = []\n\n\nfor index in tqdm(range(len(labels))):\n    id = labels.loc[index,'filename']\n    img = cv2.imread(images_path + id)\n    rgb_img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    xmin,ymin,xmax,ymax = int(labels.loc[index,'xmin']) ,int(labels.loc[index,'ymin']),int(labels.loc[index,'xmax']),int(labels.loc[index,'ymax'])\n\n    resized = cv2.resize(rgb_img[ymin:ymax,xmin:xmax,:],(224,224))\n\n    feature_of_img = model_without_last_2FC.predict(resized.reshape(1,224,224,3)\/255)\n    \n    if id in train_images_list:\n        \n        train_features.append([feature_of_img,labels.loc[index,'class']])\n        \n    else:\n        test_features.append([feature_of_img,labels.loc[index,'class']])\n      ","d336632c":"print(len(train_features))\n\nprint(len(test_features))\n","d868eed8":"for index,img in tqdm(enumerate(os.listdir(negative_ex_path)[:5000])):  #only extracting for 10,000 images\n    img = cv2.imread(negative_ex_path + img )\n    rgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    #images already in (224,224,3)\n    feature_of_img = model_without_last_2FC.predict(rgb.reshape(1,224,224,3)\/255)\n    if index<3500:\n        train_features.append([feature_of_img,'background'])\n    else:\n        test_features.append([feature_of_img,'background'])","75c9aca5":"import random\nrandom.shuffle(train_features)","43947cc6":"X_train = np.array([x[0] for x in train_features])\nX_train = X_train.reshape(-1,4096)","048f729b":"X_train.shape","f41bd6bb":"y_train = [x[1] for x in train_features]\ny_train = np.array(y_train).reshape(-1,1)","aa381e60":"y_train.shape","925ae687":"X_test = np.array([x[0] for x in test_features])\nX_test = X_test.reshape(-1,4096)","decc24ed":"y_test = [x[1] for x in test_features]\ny_test = np.array(y_test).reshape(-1,1)","4a683fd0":"from sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix","5d4224bb":"svm_model_linear = SVC(kernel = 'linear', C = 1,probability=True).fit(X_train, y_train) \nsvm_predictions = svm_model_linear.predict(X_test)","0409ea80":"accuracy = svm_model_linear.score(X_test, y_test)","78f63744":"accuracy","5f67b9a1":"cm = confusion_matrix(y_test, svm_predictions) ","80b5de0f":"sns.heatmap(cm,annot=True)","6f17968c":"img = cv2.imread(negative_ex_path + os.listdir(negative_ex_path)[45] )\nrgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(rgb)","b5cb3370":"feature_of_img = model_without_last_2FC.predict(rgb.reshape(1,224,224,3)\/255)","0b260fa1":"svm_model_linear.predict(feature_of_img)","f6504a35":"svm_model_linear.predict_proba(feature_of_img)","362585af":"svm_model_linear.classes_","bd29ca45":"img = cv2.imread(images_path+'agri_0_1024.jpeg')\nrgb = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(rgb)","868545ac":"resized = cv2.resize(rgb,(224,224))","6a5e429f":"plt.imshow(resized)","8e175901":"svm_model_linear.predict_proba(model_without_last_2FC.predict(resized.reshape(1,224,224,3)\/255))","fa0bb980":"import pickle\n\nwith open('svm_classifier.pkl','wb') as svm_model:\n    pickle.dump(svm_model_linear , svm_model)","10b88519":"# Extracting features from ground truth labeled images","f3f4dd45":"## Loading pre generated region proposals and Negative examples","8f6895d5":"# Extracting features from Negative examples","1a3dbe2b":"# Check on some images","6a16753b":"# Saving SVM model","0adbf144":"# Loading data","fa021397":"## loading model without last two Fully connected layers","895cf8cf":"When we pass image from model it will return (1,4096) size feature vector","1992e6e2":"## Loading object annotation","2f4939b9":"# SVM Training","cdd88210":"# Preparing data for SVM","8c306ed2":"## Loading pretrained CNN model"}}