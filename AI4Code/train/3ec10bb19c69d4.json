{"cell_type":{"5fa42cba":"code","f9943b49":"code","55883249":"code","6ed0077a":"code","c40bed64":"code","870b8ed3":"code","94941da4":"code","a4b5e1fc":"code","d2a3a1b5":"code","60030ed8":"code","f2b2a886":"code","ade5af06":"code","e4e31102":"code","38289583":"code","5a4994ad":"code","db8c8cec":"code","36290163":"code","15cf2118":"code","ee5297fe":"code","1d07750c":"markdown","2e26b5ba":"markdown","c86c3bc1":"markdown","e80e1eaa":"markdown","91f9320e":"markdown","ade90649":"markdown","697e345c":"markdown"},"source":{"5fa42cba":"import plotly.graph_objects as go\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nfrom datetime import datetime\nfrom pathlib import Path\nfrom sklearn import preprocessing\nimport keras.backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, RNN, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split","f9943b49":"train = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv', parse_dates=['Date'])\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv',parse_dates=['Date'])\nsubmission = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/submission.csv')\ntrain.tail()","55883249":"mask = train['Date'].max()\nworld_cum_confirmed = sum(train[train['Date'] == mask].ConfirmedCases)\nworld_cum_fatal = sum(train[train['Date'] == mask].Fatalities)","6ed0077a":"print('Number of Countires are: ', len(train['Country_Region'].unique()))\nprint('Training dataset ends at: ', mask)\nprint('Number of cumulative confirmed cases worldwide are: ', world_cum_confirmed)\nprint('Number of cumulative fatal cases worldwide are: ', world_cum_fatal)","c40bed64":"cum_per_country = train[train['Date'] == mask].groupby(['Date','Country_Region']).sum().sort_values(['ConfirmedCases'], ascending=False)\ncum_per_country[:10]","870b8ed3":"date = train['Date'].unique()\ncc_us = train[train['Country_Region'] == 'US'].groupby(['Date']).sum().ConfirmedCases\nft_us = train[train['Country_Region'] == 'US'].groupby(['Date']).sum().Fatalities\ncc_ity = train[train['Country_Region'] == 'Italy'].groupby(['Date']).sum().ConfirmedCases\nft_ity = train[train['Country_Region'] == 'Italy'].groupby(['Date']).sum().Fatalities\ncc_spn = train[train['Country_Region'] == 'Spain'].groupby(['Date']).sum().ConfirmedCases\nft_spn = train[train['Country_Region'] == 'Spain'].groupby(['Date']).sum().Fatalities\ncc_gmn = train[train['Country_Region'] == 'Germany'].groupby(['Date']).sum().ConfirmedCases\nft_gmn = train[train['Country_Region'] == 'Germany'].groupby(['Date']).sum().Fatalities\ncc_frc = train[train['Country_Region'] == 'France'].groupby(['Date']).sum().ConfirmedCases\nft_frc = train[train['Country_Region'] == 'France'].groupby(['Date']).sum().Fatalities\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=date, y=cc_us, name='US'))\nfig.add_trace(go.Scatter(x=date, y=cc_ity, name='Italy'))\nfig.add_trace(go.Scatter(x=date, y=cc_spn, name='Spain'))\nfig.add_trace(go.Scatter(x=date, y=cc_gmn, name='Germany'))\nfig.add_trace(go.Scatter(x=date, y=cc_frc, name='France'))\nfig.update_layout(title=\"Plot of Cumulative Cases for Top 5 countires (except China)\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Cases\")\nfig.update_xaxes(nticks=30)\n\nfig.show()","94941da4":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=date, y=ft_us, name='US'))\nfig.add_trace(go.Scatter(x=date, y=ft_ity, name='Italy'))\nfig.add_trace(go.Scatter(x=date, y=ft_spn, name='Spain'))\nfig.add_trace(go.Scatter(x=date, y=ft_gmn, name='Germany'))\nfig.add_trace(go.Scatter(x=date, y=ft_frc, name='France'))\nfig.update_layout(title=\"Plot of Fatal Cases for Top 5 countires (except China)\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"Cases\")\nfig.update_xaxes(nticks=30)\n\nfig.show()","a4b5e1fc":"train.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","d2a3a1b5":"train.fillna(' ',inplace=True)\ntest.fillna(' ', inplace=True)\ntrain_id = train.pop('id')\ntest_id = test.pop('forecastid')\n\ntrain['cp'] = train['country_region'] + train['province_state']\ntest['cp'] = test['country_region'] + test['province_state']\n\ntrain.drop(['province_state','country_region'], axis=1, inplace=True)\ntest.drop(['province_state','country_region'], axis =1, inplace=True)","60030ed8":"df = pd.DataFrame()\ndef create_time_feat(data):\n    df['date']= data['date']\n    df['hour']=df['date'].dt.hour\n    df['weekofyear']=df['date'].dt.weekofyear\n    df['quarter'] =df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['dayofyear']=df['date'].dt.dayofyear\n    \n    x=df[['hour','weekofyear','quarter','month','dayofyear']]\n    \n    return x\n\ncr_tr = create_time_feat(train)\ncr_te = create_time_feat(test)","f2b2a886":"train_df = pd.concat([train,cr_tr], axis=1)\ntest_df = pd.concat([test, cr_te], axis =1)\ntest_df.dropna(inplace=True)\n","ade5af06":"le=LabelEncoder()\ntrain_df['cp_le']=le.fit_transform(train_df['cp'])\ntest_df['cp_le']=le.transform(test_df['cp'])\n\ntrain_df.drop(['cp'], axis=1, inplace=True)\ntest_df.drop(['cp'], axis=1, inplace=True)","e4e31102":"def create_date_feat(data, cf, ft):\n    for d in data['date'].drop_duplicates():\n        for i in data['cp_le'].drop_duplicates():\n            org_mask = (data['date']==d) & (data['cp_le']==i)\n            for lag in range(1,15):\n                mask_loc = (data['date']==(d-pd.Timedelta(days=lag))) & (data['cp_le']==i)\n                \n                try:\n                    data.loc[org_mask, 'cf_' + str(lag)]=data.loc[mask_loc, cf].values\n                    data.loc[org_mask, 'ft_' + str(lag)]=data.loc[mask_loc, ft].values\n                \n                except:\n                    data.loc[org_mask, 'cf_' + str(lag)]=0.0\n                    data.loc[org_mask, 'ft_' + str(lag)]=0.0\n\ncreate_date_feat(train_df,'confirmedcases','fatalities')","38289583":"cf_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','cf_1', 'cf_2', 'cf_3', \n          'cf_4', 'cf_5', 'cf_6', 'cf_7', 'cf_8', 'cf_9','cf_10', 'cf_11', 'cf_12', \n          'cf_13', 'cf_14']\nft_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','ft_1', 'ft_2', 'ft_3', \n          'ft_4', 'ft_5', 'ft_6', 'ft_7', 'ft_8', 'ft_9','ft_10', 'ft_11', 'ft_12', \n          'ft_13', 'ft_14']\n\ntrain_x_cf = train_df[cf_feat]\nprint(train_x_cf.shape)\ntrain_x_ft = train_df[ft_feat]\nprint(train_x_ft.shape)\ntrain_x_cf_reshape = train_x_cf.values.reshape(train_x_cf.shape[0],1,train_x_cf.shape[1])\ntrain_x_ft_reshape = train_x_ft.values.reshape(train_x_ft.shape[0],1,train_x_ft.shape[1])\n\ntrain_y_cf = train_df['confirmedcases']\ntrain_y_ft = train_df['fatalities']\n\ntrain_y_cf_reshape = train_y_cf.values.reshape(-1,1)\ntrain_y_ft_reshape = train_y_ft.values.reshape(-1,1)\n\ntr_x_cf, val_x_cf, tr_y_cf, val_y_cf = train_test_split(train_x_cf_reshape, train_y_cf_reshape, test_size=0.2, random_state=0)\ntr_x_ft, val_x_ft, tr_y_ft, val_y_ft = train_test_split(train_x_ft_reshape, train_y_ft_reshape, test_size=0.2, random_state=0)","5a4994ad":"def rmsle(pred,true):\n    assert pred.shape[0]==true.shape[0]\n    return K.sqrt(K.mean(K.square(K.log(pred+1) - K.log(true+1))))\n\nes = EarlyStopping(monitor='val_loss', min_delta = 0, verbose=0, patience=10, mode='auto')\nmc_cf = ModelCheckpoint('model_cf.h5', monitor='val_loss', verbose=0, save_best_only=True)\nmc_ft = ModelCheckpoint('model_ft.h5', monitor='val_loss', verbose=0, save_best_only=True)\n\ndef lstm_model(hidden_nodes, second_dim, third_dim):\n    model = Sequential([LSTM(hidden_nodes, input_shape=(second_dim, third_dim), activation='relu'),\n                        Dense(64, activation='relu'),\n                        Dense(32, activation='relu'),\n                        Dense(1, activation='relu')])\n    model.compile(loss=rmsle, optimizer = 'adam')\n    \n    return model\n\nmodel_cf = lstm_model(10, tr_x_cf.shape[1], tr_x_cf.shape[2])\nmodel_ft = lstm_model(10, tr_x_ft.shape[1], tr_x_ft.shape[2])\n\nhistory_cf = model_cf.fit(tr_x_cf, tr_y_cf, epochs=200, batch_size=512, validation_data=(val_x_cf,val_y_cf), callbacks=[es,mc_cf])\nhistory_ft = model_ft.fit(tr_x_ft, tr_y_ft, epochs=200, batch_size=512, validation_data=(val_x_ft,val_y_ft), callbacks=[es,mc_ft])","db8c8cec":"plt.figure(figsize=(8,6))\nplt.plot(history_cf.history['loss'], label='Train')\nplt.plot(history_cf.history['val_loss'], label='Test')\nplt.title(\"CF Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc=\"upper left\")\nplt.show()","36290163":"# formatting Test data & predicting\n\nfeat = ['confirmedcases','fatalities','cf_1', 'ft_1', 'cf_2', 'ft_2', 'cf_3', 'ft_3', \n        'cf_4', 'ft_4', 'cf_5', 'ft_5', 'cf_6', 'ft_6', 'cf_7', 'ft_7', 'cf_8', 'ft_8',\n        'cf_9', 'ft_9', 'cf_10', 'ft_10', 'cf_11', 'ft_11', 'cf_12', 'ft_12', 'cf_13', 'ft_13',\n        'cf_14', 'ft_14']\nc_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','cf_1', 'cf_2', 'cf_3', \n          'cf_4', 'cf_5', 'cf_6', 'cf_7', 'cf_8', 'cf_9','cf_10', 'cf_11', 'cf_12', \n          'cf_13', 'cf_14']\nf_feat =  ['cp_le', 'weekofyear','quarter','month','dayofyear','ft_1', 'ft_2', 'ft_3', \n          'ft_4', 'ft_5', 'ft_6', 'ft_7', 'ft_8', 'ft_9','ft_10', 'ft_11', 'ft_12', \n          'ft_13', 'ft_14']\ntot_feat = ['cp_le', 'weekofyear','quarter','month','dayofyear','cf_1', 'ft_1', 'cf_2', 'ft_2', 'cf_3', 'ft_3', \n        'cf_4', 'ft_4', 'cf_5', 'ft_5', 'cf_6', 'ft_6', 'cf_7', 'ft_7', 'cf_8', 'ft_8',\n        'cf_9', 'ft_9', 'cf_10', 'ft_10', 'cf_11', 'ft_11', 'cf_12', 'ft_12', 'cf_13', 'ft_13',\n        'cf_14', 'ft_14']\n\ntest_new = test_df.copy().join(pd.DataFrame(columns=feat))\ntest_mask = (test_df['date'] <= train_df['date'].max())\ntrain_mask = (train_df['date'] >= test_df['date'].min())\ntest_new.loc[test_mask,feat] = train_df.loc[train_mask, feat].values\nfuture_df = pd.date_range(start = train_df['date'].max()+pd.Timedelta(days=1),end=test_df['date'].max(), freq='1D')\n\ndef create_add_trend_pred(data, cf, ft):\n    for d in future_df:\n        for i in data['cp_le'].drop_duplicates():\n            org_mask = (data['date']==d) & (data['cp_le']==i)\n            for lag in range(1,15):\n                mask_loc = (data['date']==(d-pd.Timedelta(days=lag))) & (data['cp_le']==i)\n                \n                try:\n                    data.loc[org_mask, 'cf_' + str(lag)]=data.loc[mask_loc,cf].values\n                    data.loc[org_mask, 'ft_' + str(lag)]=data.loc[mask_loc,ft].values\n                    \n                except:\n                    data.loc[org_mask, 'cf_' + str(lag)]=0.0\n                    data.loc[org_mask, 'ft_' + str(lag)]=0.0\n            \n            test_x = data.loc[org_mask,tot_feat]\n            \n            test_x_cf = test_x[c_feat]\n            test_x_cf = test_x_cf.to_numpy().reshape(1,-1)\n            test_x_cf_reshape = test_x_cf.reshape(test_x_cf.shape[0],1,test_x_cf.shape[1])\n            \n            test_x_ft = test_x[f_feat]\n            test_x_ft = test_x_ft.to_numpy().reshape(1,-1)\n            test_x_ft_reshape = test_x_ft.reshape(test_x_ft.shape[0],1,test_x_ft.shape[1])\n            data.loc[org_mask, cf] = model_cf.predict(test_x_cf_reshape)\n            data.loc[org_mask, ft] = model_ft.predict(test_x_ft_reshape)\n\ncreate_add_trend_pred(test_new, 'confirmedcases', 'fatalities')","15cf2118":"sub_pred = pd.DataFrame({'ForecastId': test_id, 'ConfirmedCases':test_new['confirmedcases'],'Fatalities':test_new['fatalities']})\nsub_pred.to_csv('submission.csv', index=False)","ee5297fe":"submission[:20]","1d07750c":"### LSTM Modelling","2e26b5ba":"### Make Prediction","c86c3bc1":"## Basic Statistics & Visualization\n\nThere will be more graphics and less code descriptions. If you want more explanation, then go through the first kernel. More EDA in https:\/\/www.kaggle.com\/mrmorj\/covid-19-eda-xgboost","e80e1eaa":"### Build Features","91f9320e":"### Thoughts Next\n\na) add last week variance, days that since 1st case occurs\n\nb) Add country hosptical beds\n\nc) Add weather data, temp + humidity\n\n*Thank you https:\/\/www.kaggle.com\/debanga\/covid-19-week2-eda for the good material for the development of the topic.*\n","ade90649":"**Do leave an upvote if you like the work:) Constructive feedbacks are welcome!**","697e345c":"# COVID-19 Prediction using LSTM\n \nIn this project, I will use data from the last three months to predict confirmed cases and deaths for the month of April. The work has just begun, good results have been obtained.\n\nThis is my second kernel on this topic. I also used a different approach to solve the incidence issue, it is available here: https:\/\/www.kaggle.com\/mrmorj\/covid-19-eda-xgboost\n\n**I will post all updates here. I ask you to support the project like if it seemed useful to you.**"}}