{"cell_type":{"23302a52":"code","0291ff38":"code","3d7ef486":"code","070c53fc":"code","983fc35f":"code","4373ebb9":"code","954a555f":"code","bfefb401":"code","6a5072f8":"code","1376183b":"code","d91e6f1d":"code","ee5f5528":"code","f05e7dce":"code","ff84efe8":"code","f17614fa":"code","0d363dac":"code","ab554e2d":"code","4220734c":"code","9957ad20":"code","a542baf4":"code","bd00e223":"code","a08bf189":"code","2c809c8b":"code","eaee61b1":"code","0aa5d140":"code","b463b5a8":"code","52c5d7db":"code","b3999303":"code","9916e4aa":"code","2df8d450":"code","d011fb52":"code","67e8b654":"markdown","bc1585a4":"markdown","ab33d96c":"markdown","fe63f36f":"markdown","63851a7b":"markdown","f8b0af0b":"markdown","77f62bbd":"markdown","e514d873":"markdown","a16c78de":"markdown","d9872e1e":"markdown","d059a2e3":"markdown","d4e4ac25":"markdown","e12a1648":"markdown","2d209a9e":"markdown","a5cd6485":"markdown","f35a6eda":"markdown","abcf404b":"markdown","769f0907":"markdown","8033f22e":"markdown","91a1c0cb":"markdown","69d412a4":"markdown","ef723899":"markdown","5179c166":"markdown","0095a79f":"markdown","ffe5d523":"markdown","f0213d20":"markdown"},"source":{"23302a52":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0291ff38":"import pandas as pd\nTrumpTweets = pd.read_csv(\"..\/input\/trumptweets\/TrumpTweets.csv\",encoding ='latin1',nrows = 20000)\nTrumpTweets.head()","3d7ef486":"TrumpTweets['created_at']= pd.to_datetime(TrumpTweets['created_at']) \nTrumpTweets['created_at'] = TrumpTweets['created_at'] - + pd.Timedelta(hours=1)\nTrumpTweets['created_at'] = TrumpTweets['created_at'].dt.date","070c53fc":"# TrumpTweets['favorite_line'] = TrumpTweets['created_at']-TrumpTweets['created_at'][19999]\n# TrumpTweets['favorite_line']=TrumpTweets['favorite_line'].dt.days\n# from sklearn.linear_model import LinearRegression\n# reg = LinearRegression().fit(TrumpTweets['favorite_line'].values.reshape(-1, 1),TrumpTweets['favorite_count'] )\n# print(reg.score(TrumpTweets['favorite_line'].values.reshape(-1, 1),TrumpTweets['favorite_count']))\n# TrumpTweets['expected_favorites'] = reg.predict(TrumpTweets['favorite_line'].values.reshape(-1, 1))\n# TrumpTweets['above_average'] = TrumpTweets['favorite_count']>=TrumpTweets['expected_favorites']\n","983fc35f":"TrumpTweets['Average_past_20_tweets'] = np.maximum(TrumpTweets['favorite_count'].iloc[::-1].shift().rolling(min_periods=1, window=21).mean().iloc[::-1],1)\nTrumpTweets['Difference_over_average'] = (TrumpTweets['favorite_count']-TrumpTweets['Average_past_20_tweets'])\/TrumpTweets['Average_past_20_tweets']\nTrumpTweets['above_average'] = TrumpTweets['favorite_count']>=TrumpTweets['Average_past_20_tweets']","4373ebb9":"TrumpTweets['Difference_over_average']=pd.cut(TrumpTweets['Difference_over_average'], bins=[-float('inf'), -0.5, 0.5, float('inf')], labels=['low', 'mid', 'high'])","954a555f":"wordDict = {}\nfor i in TrumpTweets['text']:\n    if i is not None:\n        for word in i.split():\n            if word.lower() in wordDict:\n                wordDict[word.lower()] = wordDict[word.lower()]+1\n            else:\n                wordDict[word.lower()]=1\n\nimport operator, collections\nwordDictCounts = sorted(wordDict.items(), key=operator.itemgetter(1),reverse=True)\nwordDict = sorted_dict = collections.OrderedDict(wordDictCounts)\nwordDict = {k:v for k,v in wordDict.items() if not v == 1}\n    \nwords = list(wordDict.keys())\n","bfefb401":"import numpy as np\nAllWords = np.zeros((len(TrumpTweets['text']),len(wordDict)), dtype=int)\n\n   \ntweet_index=0    \nfor tweet in TrumpTweets['text']:\n    for word in tweet.split():\n        if word.lower() in words:\n            AllWords[tweet_index][words.index(word.lower())]=AllWords[tweet_index][words.index(word.lower())]+1\n    tweet_index+=1","6a5072f8":"import gc\nWordCountsOfTweets = pd.DataFrame(AllWords, columns = words) \ndel AllWords\ngc.collect()#Collect garbage to allocate memory","1376183b":"print(WordCountsOfTweets.columns[63])","d91e6f1d":"WordCountsOfTweets=WordCountsOfTweets.drop(columns=WordCountsOfTweets.columns[0:63])","ee5f5528":"WordCountsOfTweets","f05e7dce":"from sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n# X, X_test, y, y_test = train_test_split(WordCountsOfTweets[WordCountsOfTweets.columns[0:20000]], TrumpTweets.above_average, test_size=0.3, random_state=1)\nX=WordCountsOfTweets[1000:19999][WordCountsOfTweets.columns]\ny=TrumpTweets.Difference_over_average[1000:19999]\nX_test = WordCountsOfTweets[:1000][WordCountsOfTweets.columns]\ny_test = TrumpTweets.Difference_over_average[:1000]","ff84efe8":"print(X.shape)\nprint(y.shape)\nprint(X_test.shape)\nprint(y_test.shape)\n","f17614fa":"y_test.value_counts().plot(kind='bar')\n#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Accuracy by always guessing most occured category:\",len(y_test[y_test=='low'])\/len(y_test))","0d363dac":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import tree\nimport graphviz\n\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X,y)\n\ntree.plot_tree(clf, max_depth = 2)\ndot_data = tree.export_graphviz(clf, out_file=None, filled=True, rounded=True, special_characters=True, max_depth = 2, feature_names=WordCountsOfTweets.columns, class_names=[\"low\", \"mid\", \"high\"])  \ngraph = graphviz.Source(dot_data)  \n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\ngraph\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","ab554e2d":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=5, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=91, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X,y)\n\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# print(\"Accuracy by always guessing above:\",len(y_test[y_test])\/len(y_test))\n#print(\"Accuracy by always guessing below:\",1-len(y_test[y_test])\/len(y_test))","4220734c":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X,y)\n\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","9957ad20":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='adam', warm_start = True,alpha=1e-5,hidden_layer_sizes=(144,12,3), random_state=1)\nclf.fit(X, y)\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","a542baf4":"from sklearn.naive_bayes import GaussianNB\n\n#Create a Gaussian Classifier\ngnb = GaussianNB()\n\n#Train the model using the training sets\ngnb.fit(X, y)\n\n#Predict the response for test dataset\ny_pred = gnb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","bd00e223":"# from sklearn import svm\n# clf = svm.SVC()\n# clf.fit(X, y)\n# y_pred = clf.predict(X_test)\n# print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# print(len(y_test[y_test])\/len(y_test))","a08bf189":"from sklearn.decomposition import PCA\npca = PCA(n_components=400)\nprincipalComponents = pca.fit_transform(WordCountsOfTweets[WordCountsOfTweets.columns[0:30000]])\nprincipalDf = pd.DataFrame(data = principalComponents)","2c809c8b":"from sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nX, X_test, y, y_test = train_test_split(principalDf, TrumpTweets.above_average, test_size=0.3, random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X,y)\n\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","eaee61b1":"import pandas as pd\nSP500 = pd.read_csv(\"..\/input\/sp500\/SP500.csv\")\nSP500['Buy'] = SP500['Open']<SP500['Close']\nSP500 = SP500.reindex(index=SP500.index[::-1])\nSP500['Date'] = pd.to_datetime(SP500['Date']) \nSP500['Date'] = SP500['Date'].dt.date\nSP500","0aa5d140":"import numpy as np\nAllWords = np.zeros((len(TrumpTweets.created_at.unique()),len(wordDict)), dtype=int)\n\n   \ntweet_index=0    \nfor date in TrumpTweets.created_at.unique():\n    for tweet in TrumpTweets.text[TrumpTweets['created_at']==date]:\n        for word in tweet.split():\n            if word in words:\n                AllWords[tweet_index][words.index(word)]=AllWords[tweet_index][words.index(word)]+1\n    tweet_index+=1","b463b5a8":"import gc\nWordCountsOfTweets = pd.DataFrame(AllWords, columns = words) \ndel AllWords\ngc.collect()","52c5d7db":"validDates = TrumpTweets.created_at.unique()[np.isin(TrumpTweets.created_at.unique(),SP500['Date'].values)]\nWordCountsOfTweets = WordCountsOfTweets[np.isin(TrumpTweets.created_at.unique(),SP500['Date'].values)]\nSP500 = SP500[np.isin(SP500['Date'].values,validDates)]","b3999303":"from sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n# X, X_test, y, y_test = train_test_split(WordCountsOfTweets[WordCountsOfTweets.columns[0:20000]], TrumpTweets.above_average, test_size=0.3, random_state=1)\nX=WordCountsOfTweets[600:1200]\ny=SP500.Buy[600:1200]\nX_test = WordCountsOfTweets[1200:]\ny_test = SP500.Buy[1200:]","9916e4aa":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf= RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=5, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=201, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nclf.fit(X,y)\n\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(len(y_test[y_test])\/len(y_test))","2df8d450":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf.fit(X,y)\n\ny_pred = clf.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(len(y_test[y_test])\/len(y_test))","d011fb52":"print(\"Accuracy:\",metrics.accuracy_score(y_test, ~y_pred))\nprint(len(y_test[y_test])\/len(y_test))","67e8b654":"**Test Data Distribution**","bc1585a4":"**Filter out all prices and tweets that didnt have a corresponding tweet or price that day**","ab33d96c":"**Decision Tree Model**","fe63f36f":"**Naive Bayes Model**","63851a7b":"**Run AdaBoost model**","f8b0af0b":"**Random Forest Model**","77f62bbd":"![](http:\/\/)![](http:\/\/)**Predicing insights on the Favorite Count of Trump's tweet, using the words he tweeted**","e514d873":"**Neural Networks**","a16c78de":"**Use PCA to reduce dimensions (does worse than without reduction)**","d9872e1e":"**We remove all most common words before a word we thought was important (democrats).**","d059a2e3":"**Split data into training\/test data by date.\n**\n**Commented out version splits data completely randomly instead**","d4e4ac25":"**Not used anymore, attempted to do linreg, predicting number of tweets by days since start**","e12a1648":"**Split data chronologically**","2d209a9e":"**Create data frame with words of all tweets said that day as predictors**","a5cd6485":"**Shape of all sets**","f35a6eda":"**Correctly format trump tweet dates**","abcf404b":"**Create Dictionary of all words said by trump**","769f0907":"**Import SP500**","8033f22e":"**SVM model (is very slow and not great results)**","91a1c0cb":"**Then we seperate this value into three cateogries, low, mid and high. **","69d412a4":"**Run Random Forest Model**","ef723899":"**AdaBoost**","5179c166":"**Here we take the average of the last 20 tweets that come before the current tweet. We then subtract the average from the current tweets number of favoites and then divide by the average. What we are left with is how much more or how much less the real favorite count is compared to the average**","0095a79f":"**Transform tweets into predictors by word using dictionary**","ffe5d523":"**Make into Pandas DF**","f0213d20":"**Remove some common stop words**"}}