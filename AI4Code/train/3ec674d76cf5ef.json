{"cell_type":{"2d6fea56":"code","943e69b2":"code","5d766fbf":"code","b39f6478":"code","f4303ab8":"code","cc3759d2":"code","db399cdc":"code","5b3793fe":"code","d712e63f":"code","c7be7ff6":"code","2decd0f0":"code","96afc4a2":"code","cd2cce8a":"code","ada98739":"code","b06a7120":"code","5b13e236":"code","7b0fac44":"code","019eb3ee":"code","326046d2":"markdown"},"source":{"2d6fea56":"import os\nimport gc\nimport random\nimport time\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import kurtosis, skew\n\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 300)","943e69b2":"def convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df\n\n\ntrain = pd.read_csv(\"..\/input\/ml-competition-203-2021\/input_training.csv\")\ntrain = convert_to_32bit(train)\ntest = pd.read_csv(\"..\/input\/ml-competition-203-2021\/input_test.csv\")\ntest = convert_to_32bit(test)\ntarget = pd.read_csv(\"..\/input\/ml-competition-203-2021\/output_training_IxKGwDV.csv\")\ntarget = convert_to_32bit(target)\ntrain['target'] = target['target'].values\ndel target\n_ = gc.collect()","5d766fbf":"print(f\"train features shape : {train.shape}\")\nprint(f\"test features shape : {test.shape}\")","b39f6478":"train.head()","f4303ab8":"test.head()","cc3759d2":"# row wise NANs values train set\ntrain.isnull().sum(axis=1).sort_values()","db399cdc":"# row wise NANs values train set\ntest.isnull().sum(axis=1).sort_values()","5b3793fe":"# value counts of row wise NANs for abs_rtn columns train set\nrel_cols = [col for col in train.columns if \"rel\" in col]\ntrain_rel_features = train[rel_cols].isnull().sum(axis=1)\ntrain_rel_features.sort_values()","d712e63f":"# value counts of row wise NANs for abs_rtn columns train set\ntest_rel_features = test[rel_cols].isnull().sum(axis=1)\ntest_rel_features.sort_values()","c7be7ff6":"# value counts of row wise NANs for abs_rtn columns train set\nabs_cols = [col for col in train.columns if \"abs\" in col]\ntrain_abs_features = train[abs_cols].isnull().sum(axis=1)\ntrain_abs_features.sort_values()","2decd0f0":"# value counts of row wise NANs for abs_rtn columns test set\ntest_abs_features = test[abs_cols].isnull().sum(axis=1)\ntest_abs_features.sort_values()","96afc4a2":"# We can use 'abs_cols' or 'rel_cols' because the nan_values are correlated between them\n# If for a specific row we found 50 nan_values for 'abs_cols' implies 50 nan_values\ntrain['nan_per_row'] = train[abs_cols].isna().sum(axis=1)\ntest['nan_per_row'] = test[abs_cols].isna().sum(axis=1)","cd2cce8a":"#replace nans by interpolation\nfor df in [train,test]:\n    for x in [abs_cols,rel_cols]:\n        df[x] = df[x].interpolate(axis=1, limit_direction=\"both\", inplace=False)","ada98739":"def get_stats_groupby(data):\n    \n    groupby_cols = ['day', 'pid']\n    for group_col in groupby_cols:\n        \n        create_feature_dict = {feature:[np.mean, np.median, kurtosis, skew, np.std] for feature in abs_cols+rel_cols}\n        df_tmp = data.groupby(group_col).agg(create_feature_dict).reset_index()\n        df_tmp.columns = ['_'.join(col) for col in df_tmp.columns]\n        \n        for col in df_tmp.columns:\n            data[col] = data[group_col].map(df_tmp[col])\n        \n        # Statistical features for abs_cols\n    data[\"fe_abs_mean\"]                = data[abs_cols].mean(axis=1)\n    data[\"fe_abs_std\"]                 = data[abs_cols].std(axis=1)\n    data[\"fe_abs_skew\"]                = data[abs_cols].skew(axis=1)\n    data[\"fe_abs_kurtosis\"]            = data[abs_cols].kurtosis(axis=1)\n    data[\"fe_abs_extent\"]              = data[abs_cols].max(axis=1) - data[abs_cols].min(axis=1)\n    data[\"fe_abs_mad\"]                 = data[abs_cols].mad(axis=1)\n    data[\"fe_abs_median\"]              = data[abs_cols].median(axis=1)\n    data[\"fe_abs_inter_q\"]             = data[abs_cols].quantile(0.75, axis=1) - data[abs_cols].quantile(0.25, axis=1)\n    data[\"fe_abs_AV_a\"]                = data[abs_cols].quantile(0.25, axis=1) - 1.5*(data[abs_cols].quantile(0.75, axis=1) - data[abs_cols].quantile(0.25, axis=1))\n    data[\"fe_abs_AV_b\"]                = data[abs_cols].quantile(0.75, axis=1) + 1.5*(data[abs_cols].quantile(0.75, axis=1) - data[abs_cols].quantile(0.25, axis=1))\n    data[\"fe_abs_realized_vol\"]        = np.sqrt(np.sum(data[abs_cols]**2, axis=1))\n    data[\"fe_abs_realized_quarticity\"] = (data[abs_cols].count(axis=1)\/3)*np.sum(data[abs_cols]**4, axis=1)\n    \n    # Statistical for rel_cols\n    data[\"fe_rel_mean\"]        = data[rel_cols].mean(axis=1)\n    data[\"fe_rel_std\"]         = data[rel_cols].std(axis=1)\n    data[\"fe_rel_skew\"]        = data[rel_cols].skew(axis=1)\n    data[\"fe_rel_kurtosis\"]    = data[rel_cols].kurtosis(axis=1)\n    data[\"fe_rel_extent\"]      = data[rel_cols].max(axis=1) - data[rel_cols].min(axis=1)\n    data[\"fe_rel_mad\"]         = data[rel_cols].mad(axis=1)\n    data[\"fe_rel_median\"]      = data[rel_cols].median(axis=1)\n    data[\"fe_rel_inter_q\"]     = data[rel_cols].quantile(0.75, axis=1) - data[rel_cols].quantile(0.25, axis=1)\n    data[\"fe_rel_AV_a\"]        = data[rel_cols].quantile(0.25, axis=1) - 1.5*(data[rel_cols].quantile(0.75, axis=1) - data[rel_cols].quantile(0.25, axis=1))\n    data[\"fe_rel_AV_b\"]        = data[rel_cols].quantile(0.75, axis=1) + 1.5*(data[rel_cols].quantile(0.75, axis=1) - data[rel_cols].quantile(0.25, axis=1))\n    \n    # Statistical features for special variables\n    data[\"fe_LS_mean\"]          = data['day'].map(data.groupby(\"day\")[\"LS\"].mean())\n    data[\"fe_LS_std\"]           = data['day'].map(data.groupby(\"day\")[\"LS\"].std())\n    data[\"fe_LS_median\"]        = data['day'].map(data.groupby(\"day\")[\"LS\"].median())\n    data[\"fe_LS_extent\"]        = data['day'].map(data.groupby(\"day\")[\"LS\"].max() - data.groupby(\"day\")[\"LS\"].min())\n    data[\"fe_LS_mad\"]           = data['day'].map(data.groupby(\"day\")[\"LS\"].mad())\n    data[\"fe_NLV_mean\"]         = data['day'].map(data.groupby(\"day\")[\"NLV\"].mean())\n    data[\"fe_NLV_std\"]          = data['day'].map(data.groupby(\"day\")[\"NLV\"].std())\n    data[\"fe_NLV_median\"]       = data['day'].map(data.groupby(\"day\")[\"NLV\"].median())\n    data[\"fe_NLV_extent\"]       = data['day'].map(data.groupby(\"day\")[\"NLV\"].max() - data.groupby(\"day\")[\"NLV\"].min())\n    data[\"fe_NLV_mad\"]          = data['day'].map(data.groupby(\"day\")[\"NLV\"].mad())\n    \n    # Group stock by date\n    if group_col == 'day':\n        \n        data['count_stock'] = data[group_col].map(data.groupby(group_col)['pid'].nunique())\n    # Group date by stock  \n    else:\n        data['count_day'] = data[group_col].map(data.groupby(group_col)['day'].nunique())\n    \n    return data","b06a7120":"%%time\ntrain = get_stats_groupby(train)\ntest  = get_stats_groupby(test)","5b13e236":"train.isna().sum().sort_values()","7b0fac44":"#replace nan per 0\nrep_values= 0\ntrain = train.replace([np.inf, -np.inf], rep_values)\ntrain.fillna(rep_values, inplace=True)\ntest = train.replace([np.inf, -np.inf], rep_values)\ntest.fillna(rep_values, inplace=True)","019eb3ee":"# Store the cleaned data into csv files\ntrain.to_csv(\".\/train.csv\", index=False)\ntest.to_csv(\".\/test.csv\", index=False)","326046d2":"# Feature Engineering"}}