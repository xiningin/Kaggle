{"cell_type":{"57a0532a":"code","26559955":"code","b40d8569":"code","76bb2127":"code","9c07ebf2":"code","06284980":"code","1e647379":"code","2d4e0068":"code","c9b576bd":"code","ebdfda6e":"markdown","8d01d462":"markdown","878559ba":"markdown","26c1f46b":"markdown","599f66ac":"markdown","527846c9":"markdown","15b77983":"markdown","da85666c":"markdown","12667ec1":"markdown","b371d987":"markdown"},"source":{"57a0532a":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n\n\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# For our models spaghetti\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error","26559955":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()\ntrain.describe()","b40d8569":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","76bb2127":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\ncat_cols=object_cols\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","9c07ebf2":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n","06284980":"class EnsembleModel:\n    def __init__(self, params):\n        \"\"\"\n        LGB + XGB + CatBoost model\n        \"\"\"\n        self.lgb_params = params['lgb']\n        self.xgb_params = params['xgb']\n        self.cat_params = params['cat']\n\n        self.lgb_model = LGBMRegressor(**self.lgb_params)\n        self.xgb_model = XGBRegressor(**self.xgb_params)\n        self.cat_model = CatBoostRegressor(**self.cat_params)\n\n    def fit(self, x, y, *args, **kwargs):\n        return (self.lgb_model.fit(x, y, *args, **kwargs),\n                self.xgb_model.fit(x, y, *args, **kwargs),\n               self.cat_model.fit(x, y, *args, **kwargs))\n\n    def predict(self, x, weights=[1.0, 1.0, 1.0]):\n        \"\"\"\n        Generate model predictions\n        :param x: data\n        :param weights: weights on model prediction, first one is the weight on lgb model\n        :return: array with predictions\n        \"\"\"\n        return (weights[0] * self.lgb_model.predict(x) +\n                weights[1] * self.xgb_model.predict(x) +\n                weights[2] * self.cat_model.predict(x)) \/ 3","1e647379":"# ------------------------------------------------------------------------------\n# Parameters\n# ------------------------------------------------------------------------------\nN_ESTIMATORS = 10000\nSEED = 2021\nBAGGING_SEED = 42\n\n# ------------------------------------------------------------------------------\n# LightGBM: training and inference\n# ------------------------------------------------------------------------------\nlgb_params = {'random_state': SEED,\n          'metric': 'rmse',\n          'n_estimators': N_ESTIMATORS,\n          'n_jobs': -1,\n          'cat_feature': [x for x in range(len(cat_cols))],\n          'bagging_seed': SEED,\n          'feature_fraction_seed': SEED,\n          'learning_rate': 0.009867383057779643,\n          'max_depth': 16,\n          'num_leaves': 66,\n          'reg_alpha': 17.335285595031994,\n          'reg_lambda': 10.987474846877767,\n          'colsample_bytree': 0.2256038826485174,\n          'min_child_samples': 31,\n          'subsample_freq': 1,\n          'subsample': 0.8032697250789377,\n          'max_bin': 522,\n#           'min_data_per_group': 127,\n          'cat_smooth': 81,\n          'cat_l2': 0.029690334194270022\n          }\n\nensemble_params = {\n    \"lgb\" : lgb_params,\n    'xgb': {\n        'random_state': SEED,\n        'max_depth': 13,\n        'learning_rate': 0.020206705089028228,\n        'gamma': 3.5746731812451156,\n        'min_child_weight': 564,\n        'n_estimators': N_ESTIMATORS,\n        'colsample_bytree': 0.5015940592112956,\n        'subsample': 0.6839489639112909,\n        'reg_lambda': 18.085502002853246,\n        'reg_alpha': 0.17532087359570606,\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'tree_method':'gpu_hist',\n        'n_jobs': -1\n    },\n    'cat': {\n        'random_state': SEED,\n        'depth': 3.0,\n        'fold_len_multiplier': 1.1425259013471902,\n        'l2_leaf_reg': 7.567589781752637,\n        'leaf_estimation_backtracking': 'AnyImprovement',\n        'learning_rate': 0.25121635918496565,\n        'max_bin': 107.0,\n        'min_data_in_leaf': 220.0,\n        'random_strength': 3.2658690042589726,\n        'n_estimators': N_ESTIMATORS,\n        'eval_metric': 'RMSE',\n         'task_type': 'GPU'\n    }\n}","2d4e0068":"%%time\nspaghetti_model = EnsembleModel(ensemble_params)\nspaghetti_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=500, verbose=False)\n#spaghetti_prediction=spaghetti_model.predict(X_valid, weights=[2.8, 0.1, 0.1])\n#mse=mean_squared_error(y_valid, spaghetti_prediction, squared=False)\n#print(\"MSE IS \",mse)","c9b576bd":"#Use the model to generate predictions\npredictions = spaghetti_model.predict(X_test, weights=[2.8, 0.1, 0.1])\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","ebdfda6e":"# Imports","8d01d462":"# Splitting training data","878559ba":"# Transforming Data","26c1f46b":"# Creating our spaghetti model\n\nCreating a class that holds our spaghetti 3 models","599f66ac":"# Getting the reqired files","527846c9":"# Predict and submit!\n\nIt seems that lgbm gets you the most accurate scores vs the others (with the selected parameters at least), so I gave it a heavier weight. I realize that I could just run lgbm alone and get a very similar score but I wanted to implement this to see how it works. I believe if best hyperparameters are selected for each model, a very accurate score can be generated.","15b77983":"# Training time!\n\nIf you want to play with the parameters above uncomment the commented code to see how it effects the overall MSE, I commented the code here to save some running time (I checked the MSE, I dont need to check it while saving and running for submission)","da85666c":"Mixing 3 prediction models and getting their weighted average predictions might sound like a good idea on paper, but after trying it, it seems that accuracy isnt that much different from using 1 well tuned model (on this dataset atleast), accuracy might improve if parameters are tuned, you are welcome to try and get better score using this notebook!\n\nI got the inspiration to do it from this entry: https:\/\/www.kaggle.com\/gvyshnya\/ensemble-lgb-xgb-catboost-optimized\/comments\n\nBelow I try to build a spaghetti \ud83c\udf5d of 3 regressor models:\n- LightGBM \ud83d\udca1\n- XGBoost \ud83d\ude80\n- CatBoost \ud83d\ude3a\ud83d\ude80","12667ec1":"# It's your turn now!\n\nTry to play with the parameters of the models and see what it gets you, good luck!","b371d987":"# Defining the parameters for each model\n\nI am still not very versed on the best way to optimize the parameters, I am sure someone with more experience with this can get a very good score if the right hyperparameters are plugged in into each model"}}