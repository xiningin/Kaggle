{"cell_type":{"07dc5a1a":"code","effe68b6":"code","6bb49c44":"code","b508b277":"code","65e3b9f1":"code","6230401f":"code","aa8b530b":"code","86840625":"code","979aaea8":"code","73e295d9":"code","86d82dea":"code","bf8d1f10":"code","49b5e0ea":"code","d0532b93":"code","354a03e2":"code","43715574":"code","7f1a4ee8":"code","37ba9abb":"code","3fadf3c2":"code","33f977c6":"code","9e579e9f":"code","a25e27b6":"markdown","4eff208c":"markdown","ce53bfca":"markdown","7daa80a9":"markdown","e4e8c4d9":"markdown","4081ffba":"markdown","9d0e5a3e":"markdown","fc8a76e8":"markdown","31998004":"markdown","32420bb5":"markdown","8d4362f8":"markdown","1ac59a55":"markdown","3e4f4b63":"markdown","8dc0f595":"markdown"},"source":{"07dc5a1a":"# load packages\nimport numpy as np \nimport pandas as pd\nimport os\nimport json\nimport gensim\nimport nltk\nimport unidecode\nimport string\nimport pyLDAvis.gensim, pyLDAvis.sklearn\nimport pickle\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom gensim import corpora, models\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk.corpus import stopwords\n\nnp.random.seed(2020)","effe68b6":"# files\/folders available\npath = '\/kaggle\/input\/CORD-19-research-challenge\/'\nos.listdir(path)","6bb49c44":"os.chdir(os.path.join(path,'comm_use_subset\/comm_use_subset\/pdf_json'))\nfile_list = os.listdir(os.getcwd())\n## Read files from the folder and store in a df\narticle_df = pd.DataFrame()\nfor file in file_list:\n    article_data = pd.read_json(file, orient = 'index')\n    article_df = article_df.append(article_data.T, ignore_index = True)","b508b277":"## Extract titles\/abstracts\ntitles = [x['title'] for x in article_df.metadata]\nabstracts = [x[0]['text'] for x in article_df.abstract if x!=[]] \nlen(abstracts)","65e3b9f1":"stop_words = set(stopwords.words('english'))\n\ndef text_preprocess(text):\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    #Convert to lower\n    text = text.lower()\n    #Remove accented characters\n    text = unidecode.unidecode(text)\n    #Remove punctuation\n    table = str.maketrans('', '', string.punctuation)\n    text = [w.translate(table) for w in text.split()]\n    lemmatized = []\n    #Lemmatize non-stop words and save\n    other_words = ['virus','study','viral','human','infection']\n    for word in text:\n        if word not in stop_words:\n            x = lemma.lemmatize(word)\n            if x not in other_words:\n                lemmatized.append(x)\n    return lemmatized","6230401f":"%%time\n#Preprocess each abstract and store in a list\npreprocessed_docs = []\nfor abstract in abstracts:\n    pre_processed_abstract = text_preprocess(abstract)\n    preprocessed_docs.append(pre_processed_abstract)","aa8b530b":"%%time\n#Create dictionary from set of abstracts\ndictionary = gensim.corpora.Dictionary(preprocessed_docs)\n\n#Filter extremes - remove all words that occur less than 15 times and more than 50% of document; only keep 100,000 of the remaining\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n\n# Create Bag of Words and TF-IDF (LDA on TF-IDF is not recommended but some practitioners still do it) \nbow_corpus = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]","86840625":"%%time\n###Takes around 12 minutes for 100 passes\n#Create the Gensum LDA model on BoW;\nlda_model_bow = gensim.models.LdaMulticore(bow_corpus, num_topics=3, id2word=dictionary, passes=50, workers=2)\nfor idx, topic in lda_model_bow.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","979aaea8":"%%time\n## Using PyLDAVis to visualize the topic model\n## The lambda value for the viz should be set between 0.2 and 0.4 according to \n# When lambda = 1, the relevance score = P(word | topic k), in other words their frequency count.\npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary)","73e295d9":"%%time\n###Takes around 20 minutes for 100 passes\n# Create LDA topic-model; 3 topics for now based on trial and error\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=3, id2word=dictionary, passes=50, workers=4, random_state = 0)\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","86d82dea":"%%time\n## Using PyLDAVis to visualize the topic model\npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, dictionary)","bf8d1f10":"### Add the estimated topic\/cluster for each abstract in the df (subset to remove empty abstracts)\n\ntopic_list = []\nscore_list = []\n\nfor doc in corpus_tfidf:\n    topic,score = max(lda_model_tfidf[doc], key=lambda x: x[1]) #find max value of the topic probabilities for doc\n    topic_list.append(topic)\n    score_list.append(score)\n    \ntopic_list_bow = []\nscore_list_bow = []\n\nfor doc in bow_corpus:\n    topic,score = max(lda_model_bow[doc], key=lambda x: x[1])\n    topic_list_bow.append(topic)\n    score_list_bow.append(score)\n\n#Subset df to remove empty abstracts\narticle_df_subset = article_df[article_df.abstract.str.len() != 0]\n\narticle_df_subset.loc[:,'lda_tfidf_topic_number'] = topic_list\narticle_df_subset.loc[:,'lda_tfidf_topic_score'] = score_list\narticle_df_subset.loc[:,'lda_bow_topic_number'] = topic_list_bow\narticle_df_subset.loc[:,'lda_bow_topic_score'] = score_list_bow","49b5e0ea":"%%time\n\n## Non-Negative Matrix Factorization (NMF) topic model using sklearn\nno_features = 1000\ndocs = [' '.join(x) for x in preprocessed_docs]\n# NMF is able to use tf-idf\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(docs)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\nno_topics = 4\n\n# Run NMF\nnmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)","d0532b93":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print (\"Topic %d:\" % (topic_idx))\n        print (\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\ndisplay_topics(nmf, tfidf_feature_names, 20)","354a03e2":"%%time\n#LDA using Scikit-Learn\nno_features = 1000\n\ndocs = [' '.join(x) for x in preprocessed_docs]\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n\ntfidf = tfidf_vectorizer.fit_transform(docs)\ntfidf_feature_names = tfidf_vectorizer.get_feature_names()\n\n#Set number of topics\nno_topics = 3\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=3, max_iter=100, learning_method='online', learning_offset=50.,random_state=0).fit(tfidf)","43715574":"import pyLDAvis.sklearn\npyLDAvis.enable_notebook()\npyLDAvis.sklearn.prepare(lda, tfidf, tfidf_vectorizer)","7f1a4ee8":"#Extracting topic for each document, using sk-learn LDA\ntopic_sk_lda = []\nfor i in range(0,tfidf.shape[0]):\n    topic_sk_lda.append(np.argmax(lda.transform(tfidf[i])))\n    \n## Add to subset DF\narticle_df_subset['topic_sk_lda'] = topic_sk_lda","37ba9abb":"article_df_subset['topic_sk_lda'].value_counts()","3fadf3c2":"article_df_subset['lda_tfidf_topic_number'].value_counts()","33f977c6":"article_df_subset['lda_bow_topic_number'].value_counts()","9e579e9f":"path = '\/kaggle\/working\/'\nos.chdir(path)\nPkl_Filename = \"Pickle_LDA_tfidf.pkl\"  \nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(lda_model_tfidf, file)    \n    \nPkl_Filename = \"Pickle_BOW_tfidf.pkl\"  \nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(lda_model_bow, file)\n    \nPkl_Filename = \"Pickle_LDA_sklearn_.pkl\"  \nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(lda, file)\n    \narticle_df_subset.to_csv('article_df.csv')","a25e27b6":"**4.4 LDA - Sklearn**","4eff208c":"### 4. Run the models","ce53bfca":"**4.1 LDA model - BoW**","7daa80a9":"# Clustering the papers using Topic Models \n## Latent Dirichlet Allocation (LDA) using Gensim and Scikit-Learn Implementations; Non-Negative Matrix Factorization using Scikit-Learn implementation\n\nThe idea is to first cluster the papers based on latent topics and then use the relevant topic for each task.\n\nHigh level overview of steps:\n* Read files\n* Extract abstracts\n* Pre-process\n* Create Bag of Words\/TF-IDF\n* LDA using Gensim, Sklearn, NMF using Sklearn\n* Visualize using pyLDAvis, update number of topics\n","e4e8c4d9":"**4.2 LDA model - TFIDF**","4081ffba":"**Create dictionary, bag of words corpus and tf-idf mx**","9d0e5a3e":"Scikit-Learn LDA is much much quicker than gensim-LDA and gives similar results. Going ahead with the Scikit-Learn model.","fc8a76e8":"### 1. **Load packages and set preliminaries**","31998004":"### 2. **Read files from commercial use subfolder**","32420bb5":"**Pre-processing the abstracts**\n\nPre-processing using following steps:\n\n* Convert to lower case\n* Remove accented characters\n* Remove punctuation\n* Remove stop words\n* Remove some extra words that are quite common -- virus, study, human, infection\n* Lemmatization","8d4362f8":"### 3. **Extract abstracts from the dataset and pre-process**","1ac59a55":"### 5. Save models and DF","3e4f4b63":"**4.3 NMF**","8dc0f595":"An overview of topics observed : \n\n![image.png](attachment:image.png)\n\nThis is my interpretation of the topics based on the word relevance; it is up for discussion.\nOn subsequent runs, the top words may change but the overall topics seem to remain the same."}}