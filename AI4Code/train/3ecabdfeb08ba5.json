{"cell_type":{"b87a48c1":"code","fba30f60":"code","cafceacf":"code","eec7caeb":"code","c26bdb1e":"code","3a9dd38f":"code","d958ca4f":"code","89bf61a8":"code","93d162f4":"code","ef58047b":"code","d926ad27":"code","d2121517":"code","5c94fd26":"code","17f618e7":"code","0a0a02cf":"code","3d7e6b95":"code","e584434d":"code","9105e7b9":"code","2357efbf":"code","a28fa3d5":"code","0604cfab":"code","9b7be335":"code","8426450f":"code","70d51cf9":"code","6eace86c":"code","b4b88862":"code","5da2d0f4":"code","ce265d52":"code","f32a4f49":"code","acf8b2cc":"code","23f03205":"code","2c020388":"code","92450a8e":"code","420115fc":"code","69c0e09e":"code","bfe768f3":"code","c22870d6":"code","183412a7":"code","b477b81b":"code","2502ef3d":"code","632009d2":"code","e02f1bbd":"code","543ff32e":"code","a857f4b8":"markdown","89755447":"markdown","545bed33":"markdown","9316ed29":"markdown","1fa570e5":"markdown","bbe75658":"markdown","d6ef8d1d":"markdown","ff06084f":"markdown","0ab870a9":"markdown","99341eab":"markdown","038cdf29":"markdown","406db5b2":"markdown","242bc6bb":"markdown","4fd62b83":"markdown","7ec7357f":"markdown","67e7f372":"markdown","f1ac3bf4":"markdown","3dbc05d2":"markdown"},"source":{"b87a48c1":"tez_path = '..\/input\/tez-lib\/'\neffnet_path = '..\/input\/efficientnet-pytorch\/'\nimport sys\nsys.path.append(tez_path)\nsys.path.append(effnet_path)","fba30f60":"import os\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport tez\nfrom tez.datasets import ImageDataset\nfrom tez.callbacks import EarlyStopping\n\nimport torch\nimport torch.nn as nn\n\nimport torchvision\n\nfrom sklearn import metrics, model_selection\n\n%matplotlib inline","cafceacf":"dfx = pd.read_csv(\"..\/input\/plant-pathology-2021-fgvc8\/train.csv\")\ndfx.head()","eec7caeb":"from sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\ndfx[\"encoded_labels\"] = labelencoder.fit_transform(dfx[\"labels\"])\ndfx.head()","c26bdb1e":"dfx.encoded_labels.value_counts()","3a9dd38f":"df_train, df_valid = model_selection.train_test_split(dfx, test_size=0.2, random_state=42, stratify=dfx.encoded_labels.values)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)","d958ca4f":"df_train.shape","89bf61a8":"df_valid.shape","93d162f4":"image_path = \"..\/input\/resized-plant2021\/img_sz_512\"\n\ntrain_image_paths = [\n    os.path.join(image_path, x) for x in df_train.image.values\n]\n\ntrain_image_paths = [\n    os.path.join(image_path, x) for x in df_train.image.values\n]","ef58047b":"train_image_paths[:5]","d926ad27":"train_target = df_train.encoded_labels.values\nvalid_target = df_valid.encoded_labels.values","d2121517":"train_target","5c94fd26":"valid_target","17f618e7":"train_dataset = ImageDataset(\n    image_paths = train_image_paths,\n    targets = train_target,\n    augmentations = None\n)","0a0a02cf":"def plot_img(img_dict):\n    img_tensor = img_dict['image']\n    target = img_dict['targets']\n    print(target)\n    plt.figure(figsize=(5,5))\n    image = img_tensor.permute(1,2,0)\/255\n    plt.imshow(image)","3d7e6b95":"plot_img(train_dataset[10])","e584434d":"train_aug = A.Compose(\n    [\n        A.RandomResizedCrop(256, 256),\n        A.Transpose(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.HueSaturationValue(\n            hue_shift_limit=0.2, \n            sat_shift_limit=0.2,\n            val_shift_limit=0.2, \n            p=0.5\n        ),\n        A.RandomBrightnessContrast(\n            brightness_limit=(-0.1,0.1), \n            contrast_limit=(-0.1, 0.1), \n            p=0.5\n        ),\n#         A.Normalize(\n#             mean=[0.485, 0.456, 0.406], \n#             std=[0.229, 0.224, 0.225], \n#             max_pixel_value=255.0, \n#             p=1.0\n#         )\n    ]\n)\n\nvalid_aug = A.Compose(\n    [\n        A.CenterCrop(256, 256, p=1.0),\n        A.Transpose(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.HueSaturationValue(\n            hue_shift_limit=0.2, \n            sat_shift_limit=0.2,\n            val_shift_limit=0.2, \n            p=0.5\n        ),\n        A.RandomBrightnessContrast(\n            brightness_limit=(-0.1,0.1), \n            contrast_limit=(-0.1, 0.1), \n            p=0.5\n        ),\n#         A.Normalize(\n#             mean=[0.485, 0.456, 0.406], \n#             std=[0.229, 0.224, 0.225], \n#             max_pixel_value=255.0, \n#             p=1.0\n#         )\n    ]\n)","9105e7b9":"train_dataset = ImageDataset(\n    image_paths = train_image_paths,\n    targets = train_target,\n    augmentations = train_aug\n)\n\nvalid_dataset = ImageDataset(\n    image_paths = train_image_paths,\n    targets = train_target,\n    augmentations = valid_aug\n)","2357efbf":"plot_img(train_dataset[10])","a28fa3d5":"import pickle\n\npretrained=True\npretrained_model = torchvision.models.resnet18(pretrained=pretrained)\n\nPkl_Filename = \"pretrained_resnet18.pkl\"\n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(pretrained_model, file)","0604cfab":"# Pkl_Filename = \"..\/input\/resnet18-tez\/pretrained_resnet18.pkl\"\n\n# with open(Pkl_Filename, 'rb') as file:  \n#     pretrained_model = pickle.load(file)","9b7be335":"class PlantModel(tez.Model):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.convnet = pretrained_model\n        self.convnet.fc = nn.Linear(512, num_classes)\n        self.step_scheduler_after = \"epoch\"\n        \n    def loss(self, outputs, targets):\n        if targets is None: \n            return None\n        return nn.CrossEntropyLoss()(outputs, targets)\n    \n    def monitor_metrics(self, outputs, targets):\n        outputs = torch.argmax(outputs, dim = 1).cpu().detach().numpy()\n        targets = targets.cpu().detach().numpy()\n        acc = metrics.accuracy_score(targets, outputs)\n        f1_score = metrics.f1_score(outputs, targets, average='weighted')\n        return{\n            \"accuracy\" : acc,\n            \"f1_score\" : f1_score\n        }\n    \n    def fetch_optimizer(self):\n        opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return opt\n\n    def fetch_scheduler(self):\n        sch = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=0.7)\n        return sch\n        \n    def forward(self, image, targets=None):\n        outputs = self.convnet(image)\n        if targets is not None: \n            loss = self.loss(outputs, targets)\n            mon_metrics = self.monitor_metrics(outputs, targets)\n            return outputs, loss, mon_metrics\n        return outputs, None, None","8426450f":"# torchvision.models.resnet152(pretrained=False)","70d51cf9":"# dfx.encoded_labels.nunique()","6eace86c":"model = PlantModel(num_classes = dfx.encoded_labels.nunique())","b4b88862":"img = train_dataset[0][\"image\"]\ny = train_dataset[0][\"targets\"]\nmodel(img.unsqueeze(0), y.unsqueeze(0))","5da2d0f4":"es = EarlyStopping(\n    monitor = \"train_accuracy\", \n    model_path = \"model.bin\", \n    patience = 2,\n    mode='max'\n)\n\nmodel.fit(\n    train_dataset,\n    valid_dataset = valid_dataset,\n    train_bs = 32,\n    valid_bs = 64,\n    device = \"cuda\",\n    callbacks = [es],\n    fp16 = True,\n    epochs = 1\n)","ce265d52":"# model.save(\"model.bin\")\ntorch.save(model.state_dict(), \".\/trainedmodel.bin\")","f32a4f49":"# import pickle\n\n# # Pkl_Filename = \"resnet18_trained_model.pkl\"  \n\n# # with open(Pkl_Filename, 'wb') as file:  \n# #     pickle.dump(model, file)","acf8b2cc":"test_dfx = pd.read_csv(\"..\/input\/plant-pathology-2021-fgvc8\/sample_submission.csv\")\nimage_path = \"..\/input\/plant-pathology-2021-fgvc8\/test_images\/\" \n\n# model.load(\"..\/input\/resnet18-tez\/model.bin\")\n\n# model\n\n# Pkl_Filename = \"..\/input\/resnet18-tez\/resnet18_trained_model.pkl\"\n# with open(Pkl_Filename, 'rb') as file:  \n#     model = pickle.load(file)\n\n# test_dfx.head()","23f03205":"from sklearn.preprocessing import LabelEncoder\n\nlabelencoder = LabelEncoder()\ntest_dfx[\"encoded_labels\"] = labelencoder.fit_transform(test_dfx[\"labels\"])\ntest_dfx.head()","2c020388":"test_image_paths = [\n    os.path.join(image_path, x) for x in test_dfx.image.values\n]\n\ntest_target = test_dfx.encoded_labels","92450a8e":"test_aug = A.Compose(\n    [\n        A.RandomResizedCrop(256, 256),\n        A.Transpose(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.HueSaturationValue(\n            hue_shift_limit=0.2, \n            sat_shift_limit=0.2,\n            val_shift_limit=0.2, \n            p=0.5\n        ),\n        A.RandomBrightnessContrast(\n            brightness_limit=(-0.1,0.1), \n            contrast_limit=(-0.1, 0.1), \n            p=0.5\n        ),\n#         A.Normalize(\n#             mean=[0.485, 0.456, 0.406], \n#             std=[0.229, 0.224, 0.225], \n#             max_pixel_value=255.0, \n#             p=1.0\n#         )\n    ]\n)","420115fc":"test_dataset = ImageDataset(\n    image_paths = test_image_paths,\n    targets = test_target,\n    augmentations = test_aug\n)\n\ntest_dataset[0]","69c0e09e":"final_preds = None\nfor j in range(5):\n    preds = model.predict(test_dataset, batch_size=32, n_jobs=-1)\n    temp_preds = None\n    for p in preds:\n        if temp_preds is None:\n            temp_preds = p\n        else:\n            temp_preds = np.vstack((temp_preds, p))\n    if final_preds is None:\n        final_preds = temp_preds\n    else:\n        final_preds += temp_preds\nfinal_preds \/= 5","bfe768f3":"final_preds = final_preds.argmax(axis=1)\nfinal_preds","c22870d6":"test_dfx.encoded_labels = final_preds\ntest_dfx.head()","183412a7":"lblist = df_train.drop_duplicates(subset=['labels'])\nlblist = lblist.set_index(\"encoded_labels\")\nlblist","b477b81b":"# lblist.at[5, \"labels\"]","2502ef3d":"def get_labels(val):\n    return lblist.at[val, \"labels\"]","632009d2":"pred_lists = []\nfor i, pred in enumerate(final_preds):\n    label = get_labels(pred)\n    pred_lists.append(label)\n    \npred_lists","e02f1bbd":"test_dfx[\"labels\"] = pred_lists\ntest_dfx = test_dfx.drop(columns=['encoded_labels'])\ntest_dfx.reset_index()\ntest_dfx","543ff32e":"test_dfx.to_csv(\"submission.csv\", index=False)","a857f4b8":"# **Predict testset**","89755447":"# **Create Model**","545bed33":"**Resnet152 structure**","9316ed29":"# **Import image**","1fa570e5":"# **Load data**","bbe75658":"**Load test data**","d6ef8d1d":"# **Create train_dataset**","ff06084f":"**Save trained model**","0ab870a9":"**Apply augmentation**","99341eab":"**Encode label**","038cdf29":"# **Augmentation**","406db5b2":"**Train model**","242bc6bb":"**Our custom model**","4fd62b83":"# **Encoded labels**","7ec7357f":"# **Split train, valid & Reset index**","67e7f372":"**Set train, valid target**","f1ac3bf4":"**Predicting**","3dbc05d2":"**Plot image**"}}