{"cell_type":{"a50bc878":"code","5d8ce063":"markdown"},"source":{"a50bc878":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5d8ce063":"Marketing Analytics in R : statistical modeling\n\nINWT statistics (company in Germany)\nCustomer lifetime value \u2013 linear regression\n\u2022\tPredicted future net-profit\n\u2022\tIdentify promising customers\n\u2022\tPrioritize customers according to future margins\n\u2022\tNo further customer segmentation\n\nR code:\nStr(clvdata1,give.attr=False)\nLibrary(corrplot)\nClvdata1 %>% select(nOrders,nitems,\u2026.,margin, futuremargin) %>% corr() %>% corrplot()\nSimple linear regression\nsimpleLM <- lm(futuremargin ~ margin, data=clvData1)\nsummary(simpleLM)\n\n\nOne threat to accuracy of linear regression is \u2018omitted variable bias\u2019\n-\tWhen a variable not included in regression is correlated to both explanatory variable and response variable.\nHence multiple regression\n\nR code:\nmultipleLM <- lm(futuremargin ~ arg1+arg2\u2026, data = clvData)\n\nProblem with multiple regression:\nmulticollinearity -> \n-\tTo remove multicollinearity we analyse through \u2018Variance Inflation factors\u2019.\n-\tVIF higher than 5 problematic higher than 10 poor regression estimates\n\nR code: \nlibrary(rms)\nvif(multipleLM) \nsalesModel1 <- lm(salesThisMon ~ . - id, data = salesData)  #code to run multiple regression except id column\nsalesModel2 <- lm(salesThisMon ~ . - id - preferredBrand - nBrands, data = salesData) #to remove multiple columns \n\nModel validation, model fit & prediction:\n-\tCoefficient of determination R2: proportion of the dependent variable variance that is explained by regression model, 1 explains 100% dependent variable variation. (0.9 are rarely reached)\n-\tThe Multiple R-squared of 0.8236 tells you that 82.36% of the dependent variable's variation is explained by the explanatory variables.\n-\tF test: is the test for overall fit of the model. Whether or not R2 is equal to zero, that is to say at least one regressor or set of regressor have significant explanatory power.\nIn our model p value of F test is smaller than 0.05 hence the hypothesis of R2 0 is rejected.\n\nOver fitting:\nSeveral ways to avoid over fitting-\no\tKeep your model lean\no\tOut of sample validation\no\tCross validation\no\tLess AIC () score model is preferred in case to multiple models\n\nPredicting values-\nR code:\npredMargin <- predict(model, newdata = newdataset)\nhead(predMargin)\nmean(predMargin, na.rm=True)\n\nChurn prevention in online marketing\nBinary Logistic regression\nR code: \nLogitmodelfull <- glm(returncustomer ~ title + newsletter+ \u2026., family= binomial, data=churndata)\nSummty(logitmodelfull)\n\nCoeffcient interpretation\ncoefExp <-  coef(logitmodelfull) %>% exp() %>% round(2)\n\nWhen developing a model you need to figure out which variables to include and which one to exclude.\nR code:\nlibrary(MASS)\nlogitmodelnew <- stepAIC(logicmodelfull,trace=0)\nsummary(logitmodelnew)\n\nglm - generalized linear model\nR code:\n# Build logistic regression model\nlogitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage + age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, family = binomial, data = defaultData)\n# Take a look at the model\nsummary(logitModelFull)\n# Take a look at the odds\ncoefsexp <- coef(logitModelFull) %>% exp() %>% round(2)\ncoefsexp\n\nEveryone is talking about statistical significance, but do you know the exact meaning of it? What is the correct interpretation of a p value equal to 0.05 for a variable's coefficient, when we have the following null hypothesis:\nH0: The influence of this variable on the payment default of a customer is equal to zero.\nAns : The probability of finding this coefficient's value is only 5%, given that our null hypothesis (the respective coefficient is equal to zero) is true\nThe stepAIC() function gives back a reduced model (library(MASS))\nR code:\nlibrary(MASS)\n# The old (full) model\nlogitModelFull <- glm(PaymentDefault ~ limitBal + sex + education + marriage + age + pay1 + pay2 + pay3 + pay4 + pay5 + pay6 + billAmt1 + billAmt2 + billAmt3 + billAmt4 + billAmt5 + billAmt6 + payAmt1 + payAmt2 + payAmt3 + payAmt4 + payAmt5 + payAmt6, family = binomial, defaultData)\n#Build the new model\nlogitModelNew <- stepAIC(logitModelFull,trace=0) \n#Look at the model\nsummary(logitModelNew) \n# Save the formula of the new model (it will be needed for the out-of-sample part) \nformulaLogit <- as.formula(summary(logitModelNew)$call)\nformulaLogit\n\nmodel fit and thresholding\npseudo R2 statistics \n\tMcfadden\n\tCox & Snell\n\tNagelkerke \n Model > 0.2 reasonable \nModel > 0.4 good\nModel > 0.5 very good\n\nLibrary(descr)\nLogRegR2(logitModelNew)\n\nAccuracy: \nConfusion matrix\nPrediction \\ Observation\tnegative\tPositive\nNegative\ttrue negative\tfalse negative\nPositive\tFalse positive\tTrue positive\nR code:\nLibrary(SDMTools)\nPredict(logitModelnew,type=\u201dresponse\u201d,na.action=na.exclude)\nConfusion.matrix(churndata$returnCustomer,churnData$predNew,threshold=0.5)\nAccuracynew <- sum(diag(confMatrixNew))\/sum(confMatrixNew)\nWhen comparing the values of the different models with each other: In case different models have the same accuracy values, always choose the model with less explanatory variables.\nR code:\nlibrary(SDMTools)\n# Prepare data frame with threshold values and empty payoff column\npayoffMatrix <- data.frame(threshold = seq(from = 0.1, to = 0.5, by = 0.1),payoff = NA) \npayoffMatrix \n \nfor(i in 1:length(payoffMatrix$threshold)) {\n  # Calculate confusion matrix with varying threshold\n  confMatrix <- confusion.matrix(defaultData$PaymentDefault, defaultData$predNew, threshold = payoffMatrix$threshold[i])\n  # Calculate payoff and save it to the corresponding row\n  payoffMatrix$payoff[i] <- confMatrix[1,1]*250 + confMatrix[1,2]*(-1000)\n}\npayoffMatrix\n\nOut of sample validation and cross validation\nOut of sample:\nRandomly split data in 2 parts\nGenerally 2\/3rd in train set, 1\/3rd in test set\nR code:\nSet.seed(23432)\nchurnData$isTrain <- rbinom(nrow(churnData),1,0.66)\ntrain <- subset(churnData, churnData$isTrain == 1)\ntest <- subset(churnData, churnData$isTrain ==0)\n\nCrossValidation:\nR code\nLibrary(boot)\n# Accuracy function with threshold=0.3\nAcc03 <- function(r,pi=0) {\nCm <- confusion.matrix(r,pi,threshold=0.3)\nAcc<-sum(diag(cm))\/sum(cm)\nReturn(acc)\n}\n#Accuracy\nSet.seed(23424)\ncv.glm(churnData,logitModelNew,cost=Acc03,K=6)$delta\n\n\nSurvival Analysis in Customer Relationship Management:\n-\tLess aggregation\n-\tAllow us to model when an event takes place\n-\tNo arbitrarily set timeframe\n-\tDeeper insights into customer relations\n\nQ. Which of the following is a question that can be answered with survival analysis?\nA. After ordering for the first time in an online shop, when do customers place their second order?\n\nIn survival analysis, each observation has one of two states: either an event occured, or it didn't occur. But you don't know if it occurs tomorrow or in three years.\n\nSurvival analysis is suited for situations where for some observations an event has not yet happened, but may happen at some point in time.\n\nSurvival curve analysis by Kaplan Meier:\n-\tCreate survival object : in a new column \nR code: \ncbind(dataSurv %>% select(tenure,churn),surv=Surv(dataSurv$tenure,dataSurv$churn)) %>% head(10)\n\n-\tSurvival function: gives the probability that customer would not churn in the period leading to the time t.\n-\tCumulative hazard function: cumulative risk \/probability the customer churned up until time t\n-\tHazard Rate\/force of mortality\/instantaneous event rate: describe the risk that an event will occur in a small interval around time t given that event has not yet happened \nEstimation of Survival function\nRCode:\nfitKM <- survfit(Surv(dataSurv$tenure,dataSurv$churn) ~ 1, type=\u201dkapaln-meier\u201d)\nfitKM$surv\nprint(fitKM)\nKaplan meier with Categorial Covariate\nfitKMstr <- survfit(Surv(tenure,churn) ~ Partner, data = dataSurv)\n\nSurvival function, hazard function and hazard rate\n\u2022\tThe hazard rate can go up and down. For example, customers could be very likely to churn at the beginning of their customer relationship, then become less likely for some months, and then become more likely again due to a saturation effect.\n\u2022\tThe survival function describes the proportion of observations who are still alive (or, for example, in a customer relationship, the proportion of customers who haven't churned yet), depending on their time under observation.\n\u2022\tThe hazard rate describes the risk that an event occurs within a very small period of time (provided that it hasn't occurred yet).\n\u2022\tThe cumulative hazard function describes the cumulative risk until time t.\n\nRcode:\n# Create survival object\nsurvObj <- Surv(dataNextOrder$daysSinceFirstPurch, dataNextOrder$boughtAgain)\n\n# Look at structure\nstr(survObj)\n# Compute and print fit\nfitKMSimple <- survfit(survObj ~ 1)\nprint(fitKMSimple)\n\n# Plot fit\nplot(fitKMSimple,\n     conf.int = FALSE, xlab = \"Time since first purchase\", ylab = \"Survival function\", main = \"Survival function\")\n\n# Compute fit with categorical covariate\nfitKMCov <- survfit(survObj ~ voucher, data = dataNextOrder)\n\n# Plot fit with covariate and add labels\nplot(fitKMCov, lty = 2:3,\n     xlab = \"Time since first purchase\", ylab = \"Survival function\", main = \"Survival function\")\nlegend(90, .9, c(\"No\", \"Yes\"), lty = 2:3)\n\nCox PH model with constant covariates (cox propotional hazard)\n-\tRelative hazard function remains constant over time\nRCode:\nLibrary(rms)\nUnits(dataSurv$tenure) <- \u201cMonth\u201d\ndd<- datadist(dataSurv)\noptions(datadist=\u201ddd\u201d)\nfitCPH1<-cph(Surv(tenure,churn)~ gender+\u2026..,data=dataSurv,x=TRUE,y=TRUE,surv=TRUE,time.inc=1)\nexp(fitCPH1$coeffcients)\nsurvplot(fitCPH1,Monthlycharges,label.curves=list(keys=1:5))\nplot(summary(fitCHP1),log=TRUE)\n\nQ. What does the proportional hazard assumption mean?\nA. The influence of the predictors does not change over time.\n\nexp(fitCPH$coefficients)\nshoppingCartValue           voucher          returned       gender=male \n 0.9978601         0.7449362         0.7301667         1.1140891\n You can see that a shopping cart value increase of 1 dollar decreases the hazard to buy again by a factor of only slightly below 1 - but the coefficient is significant, as are all coefficients. For customers who used a voucher, the hazard is 0.74 times lower, and for customers who returned any of the items, the hazard is 0.73 times lower. Being a man compared to a women increases the hazard of buying again by the factor 1.11.\n\nInterpretation of coefficients\nQ. You computed a Cox PH model and got a coefficient of 0.8 for your continuous predictor X. What is the correct interpretation?\nA. A one-unit increase in X increases the hazard by a factor of about 2.23.\nThe effect is multiplicative and you took the exponential.\n\nChecking model assumptions and making predictions\ntestCHP1 <- cox.zph(fitCPH1)\nprint(testCPH1)\nsd\nrewe\n\nif the p value of the test is less than 0.5 . we can reject the hypothesis that the given variable meet proportional hazards assumption.\nAccording to the test several predictors, violate proportional changes assumption hence there affect changes over time.\n\nIf the proportional assumption holds it is should be a horizontal line.\n\nThe test provided by cox.zph() is conservative and sensitive to number of observations.\n\nIf PH assumption is violated for certain variable?\n-\tStratified cox model makes sense, this model allows shape of underlying hazard to vary for the different level of variable.\n-\tCategorical variable are added to argument stratum \n-\tContinuous variables are classed first\n-\tThe regression coefficient are modelled across strata.\nAnother solution is to model time dependent coefficients by dividing the time under observation into different period for which we assume coefficients to be constant.\n\nIn order to make sure model is not overfitted.\nRcode:\n# Check proportional hazard assumption and print result\ntestCPH <- cox.zph(fitCPH)\nprint(testCPH)\n# Plot time-dependent beta\nplot(testCPH, var = \"gender=male\")\n# Load rms package\nlibrary(rms)\n# Validate model\nvalidate(fitCPH, method = \"crossvalidation\",\n         B = 10, dxy = TRUE, pr = FALSE)\n# Create data with new customer\nnewCustomer <- data.frame(daysSinceFirstPurch = 21, shoppingCartValue = 99.90, gender = \"female\", voucher = 1, returned = 0, stringsAsFactors = FALSE)\n# Make predictions\npred <- survfit(fitCPH, newdata = newCustomer)\nprint(pred)\nplot(pred)\n# Correct the customer's gender\nnewCustomer2 <- newCustomer\nnewCustomer2$gender <- \"male\"\n# Redo prediction\npred2 <- survfit(fitCPH, newdata = newCustomer2)\nprint(pred2)\n\nPCA for CRM data\n-\tHandle Multicollinearity (because of high dimensional data)\n-\tCreate indices (weighted average where weight is derived from data)\n-\tVisualize and understand high dimensional data \nReduce no of variables in your analysis.\nReduces large number of correlated variables\n\nServe as an exploratory tool, it is not meant to test hypothesis about structure of your data.\nAll data must be continuous or binary.\n\nPurposes of PCA\nQ. PCA is a method you can use for a wide range of purposes. Which of the following is something PCA can NOT do?\nA. Developing data-driven indices\nB. Visualizing high-dimensional data\nC. Testing hypotheses about the underlying structure of the data\nD. Solving the problem of highly intercorrelated variables in a multiple regression (\"multicollinearity\")\n\nPCA uses the correlations to determine the components that cover as much as possible of the original variance. You may already be able to see groups in the correlation matrix which form a component later on.\n\nPCA computation\nFocus lies on the variances of the respective variable.\nVariables with high variances are over represented in the resulting principal component\nDifferences in units can cause issues.\nHence scale() function is used to standardize the variables with  mean 0 variance 1\n\nRCode:\ndataCustomers <- dataCustomers %>% scale() %>% as.data.frame()\n pcaCust <- prcomp(dataCustomers)\npcaCust$sdev\nEigenvalues \u2013 higher the eigenvalue the more important the component is\npcaCust$sdev ^ 2\nproporation of explained variance\n(pcaCust$sdev^2 \/ length(pcaCust$sdev)) %>% round(2)\nRound(pcaCust$rotation[,1:6],2)\n\n-\tPrincipal components are extracted such that they cover as much of the original variance as possible. If some variables had larger variance than others, they would be overrepresented in the components.\n\n-\tIt's the loadings and not the standard deviations that help to interpret the components. The standard deviations can be used to compute the importance of the components.\nChoosing right number of Principal Components\nGeneral rule \nEigenvalues >1 (kasier-gutmann criterion)\nScreeplot \u2013 neglect components right to the elbow \nGeneral cumulative proportion of variance should be <= .7\nRcode:\nScreeplot(pcaCust,type=\u201dlines)\nBox()\nAbline(h-1,lty=2)\n#Biplot\nbiplot(pcaCust,choices=1:2,cex=0.7)\nPCA in regression analysis\nMod1 <- lm(customerSatis ~ ., dataCustomers)\nVif(mod1)\ndataCustComponents <- cbind(dataCustomers[,\u201dcustomerSatis\u201d],pcaCust$x[,1:6]) %>% as.data.frame\nmod2 <- lm(customerSatis ~ ., dataCustComponents)\nvif(mod2)\nWhat is the danger of having too many variables in a regression?\nDanger of multicollinearity: The regression estimates become very unstable\nIf there are many predictors, it is likely that some of them can be explained to a large extent by others. This leads to lower precision in terms of higher standard errors.\n# Predict log shares with all original variables\nmod1 <- lm(logShares ~ ., data = newsData)\n# Create dataframe with log shares and first 6 components\ndataNewsComponents <- cbind(logShares = newsData[, \"logShares\"],\n                            pcaNews$x[, 1:6]) %>%\n  as.data.frame()\n# Predict log shares with first six components\nmod2 <- lm(logShares ~ ., data = dataNewsComponents)\n# Print adjusted R squared for both models\nsummary(mod1)$adj.r.squared\nsummary(mod2)$adj.r.squared\n"}}