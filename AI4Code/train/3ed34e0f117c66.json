{"cell_type":{"7e901918":"code","7ea5c677":"code","cdc3550f":"code","ed077bec":"code","43e689d9":"code","1e776002":"code","d59b8c1d":"code","395a5145":"code","c073a167":"code","6803cf77":"code","276e6f1d":"code","3ef29c4c":"code","60b495fb":"code","f6fb69a9":"code","c2dc4c79":"code","66929278":"code","8e07a84f":"code","1a937d91":"code","41a476f6":"code","e8348e97":"code","50e4eff6":"code","8d384a97":"code","3bbd7165":"code","d671b707":"code","0641622e":"code","822d21d9":"code","ec958a78":"code","9ec48a60":"code","1f27b67e":"markdown","20736199":"markdown","00ba5c59":"markdown","acc9ab59":"markdown","b8aab3fd":"markdown","1a5eb138":"markdown","ea86236e":"markdown","c2b86825":"markdown","05034100":"markdown","7e2343c7":"markdown"},"source":{"7e901918":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7ea5c677":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import QuantileTransformer , PowerTransformer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\n\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import TimeDistributed, Flatten\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers.recurrent import LSTM\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\ncmap = cm.get_cmap('Spectral') # Colour map (there are many others)\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import r2_score","cdc3550f":"train_file = \"..\/input\/nasa-cmaps\/CMaps\/test_FD001.txt\" \ntest_file = \"..\/input\/nasa-cmaps\/CMaps\/test_FD001.txt\"\nRUL_file = \"..\/input\/nasa-cmaps\/CMaps\/RUL_FD001.txt\"\n\ndata = pd.read_csv(train_file,sep=\" \",header=None)\ndata.head()","ed077bec":"\ndata.drop(columns=[26,27],inplace=True)\ncolumns = [\"Section-{}\".format(i)  for i in range(26)]\ndata.columns = columns\n\ndata.head()","43e689d9":"data.describe()","1e776002":"# Names \nMachineID_name = [\"Section-0\"]\nRUL_name = [\"Section-1\"]\nOS_name = [\"Section-{}\".format(i) for i in range(2,5)]\nSensor_name = [\"Section-{}\".format(i) for i in range(5,26)]\n\n# Data in pandas DataFrame\nMachineID_data = data[MachineID_name]\nRUL_data = data[RUL_name]\nOS_data = data[OS_name]\nSensor_data = data[Sensor_name]\n\n# Data in pandas Series\nMachineID_series = data[\"Section-0\"]\nRUL_series = data[\"Section-1\"]","d59b8c1d":"grp = RUL_data.groupby(MachineID_series)\nmax_cycles = np.array([max(grp.get_group(i)[\"Section-1\"]) for i in MachineID_series.unique()])\nprint(\"Max Life >> \",max(max_cycles))\nprint(\"Mean Life >> \",np.mean(max_cycles))\nprint(\"Min Life >> \",min(max_cycles))","395a5145":"data.drop(columns=[\"Section-0\",\"Section-4\", \"Section-5\", \"Section-9\", \"Section-10\",  \"Section-14\",\"Section-20\",\"Section-22\", \"Section-23\"] , inplace=True)\ndata.head()","c073a167":"\ngen = MinMaxScaler(feature_range=(0, 1))\ndata = gen.fit_transform(data)\ndata = pd.DataFrame(data)\ndata=np.nan_to_num(data)\n\npt = PowerTransformer()\ndata = pt.fit_transform(data)\ndata","6803cf77":"def RUL_df():\n    rul_lst = [j  for i in MachineID_series.unique() for j in np.array(grp.get_group(i)[::-1][\"Section-1\"])]\n    rul_col = pd.DataFrame({\"rul\":rul_lst})\n    return rul_col\n\n\nX_train = np.array(data)\n\ny_train = np.array(RUL_df()).reshape(-1,1)\n\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) \n\n\nprint(X_train.shape,y_train.shape)\nX_train","276e6f1d":"#print(max_cycles)\nprint(sum(max_cycles))\ncount = 0\nfor cycle_len in max_cycles:\n    for i in range(6):\n        y_train[count+i] = 0\n    count = count + cycle_len\nprint(count)","3ef29c4c":"def create_dataset(X, AR=10):\n    data = []\n    for i in range(len(X)-AR-1):\n        data.append(X[i:(i+AR)])\n    return np.array(data)\n\n\nX_train = create_dataset(X_train)\ny_train = y_train[11:]","60b495fb":"def predict_model(layers):\n        #d = 0.2\n        model = Sequential()\n        model.add(LSTM(128, input_shape=(layers[1], layers[0]), return_sequences=True))\n        #model.add(Dropout(d))\n        model.add(LSTM(64, input_shape=(layers[1], layers[0]), return_sequences=False))\n        #model.add(Dropout(d))\n        model.add(Dense(16,kernel_initializer='uniform',activation='relu'))        \n        model.add(Dense(1,kernel_initializer='uniform',activation='relu'))\n        model.compile(loss='mean_squared_error',optimizer='adam')\n        return model","f6fb69a9":"model = predict_model([17,5])\nprint(model.summary())","c2dc4c79":"history = model.fit(\n    X_train,\n    y_train,\n    batch_size=512,\n    epochs=100,\n    validation_split=0.15,\n    verbose=1)","66929278":"y_train_pred = model.predict(X_train)\nprint(\"R-Squared Score \", r2_score(y_train,y_train_pred))\n","8e07a84f":"df_test = pd.read_csv(test_file, sep=\" \",header=None)\ndf_test.drop(columns=[26,27],inplace=True)\ndf_test.columns = columns\ndf_test.head()","1a937d91":"df_rul = pd.read_csv(RUL_file, names=['rul'])\ndf_rul.head()","41a476f6":"RUL_name = [\"Section-1\"]\nRUL_data = df_test[RUL_name]\nMachineID_series = df_test[\"Section-0\"]\ngrp = RUL_data.groupby(MachineID_series)\nmax_cycles = np.array([max(grp.get_group(i)[\"Section-1\"]) for i in MachineID_series.unique()])\nmax_cycles[0] = max_cycles[0] - 6","e8348e97":"df_test.drop(df_test[[\"Section-0\", \"Section-4\", \"Section-5\",  \"Section-9\", \"Section-10\",  \"Section-14\", \"Section-20\", \"Section-22\",\"Section-23\"]], axis=1 , inplace=True)\ndf_test.head()            ","50e4eff6":"gen = MinMaxScaler(feature_range=(0, 1))\ndf_test = gen.fit_transform(df_test)\ndf_test = pd.DataFrame(df_test)\n#df_test = df_test.rolling(20).mean()\npt = PowerTransformer()\ndf_test = pt.fit_transform(df_test)\ndf_test=np.nan_to_num(df_test)","8d384a97":"X_test = np.array(df_test)\n\ny_test = np.array(df_rul)\n\nprint(X_test.shape,y_test.shape)\nprint(max_cycles)","3bbd7165":"def create_dataset(X, look_back=5):\n    data = []\n    for i in range(len(X)-look_back-1):\n        data.append(X[i:(i+look_back)])\n    return np.array(data)\nX_test = create_dataset(X_test)\nprint(X_test.shape,y_test.shape)","d671b707":"pred = model.predict(X_test)\npred.shape","0641622e":"final_pred = []\ncount = 0\nfor i in range(100):\n    temp = 0\n    j = max_cycles[i] \n    while j>0:\n        temp = temp + pred[count]\n        j=j-1\n        count=count+1\n    final_pred.append(int(temp\/max_cycles[i]))","822d21d9":"print(final_pred)","ec958a78":"fig = plt.figure(figsize=(18,10))\nplt.plot(final_pred,color='red', label='prediction')\nplt.plot(y_test,color='blue', label='y_test')\nplt.legend(loc='upper left')\nplt.grid()\nplt.show()","9ec48a60":"print(\"mean_squared_error >> \", mean_squared_error(y_test,final_pred))\nprint(\"root_mean_squared_error >> \", math.sqrt(mean_squared_error(y_test,final_pred)))\nprint(\"mean_absolute_error >>\",mean_absolute_error(y_test,final_pred))","1f27b67e":"#### Dataset statistics  for each parameter","20736199":"## Normalized data","00ba5c59":"## Conduct Dataframe","acc9ab59":"**R_square score of the predictive model using LTSM is 0.795**","b8aab3fd":"## AR model : using( t(i)... t(i+5) predict t(i+6)","1a5eb138":"## Test Dataset","ea86236e":"## Count the number of cycles","c2b86825":"## AR model : using( t(i)... t(1+10) predict t(i+11)","05034100":"## Train data using LTSM","7e2343c7":"About this Dataset\nDescription\nPrognostics and health management is an important topic in industry for predicting state of assets to avoid downtime and failures. This data set is the Kaggle version of the very well known public data set for asset degradation modeling from NASA. It includes Run-to-Failure simulated data from turbo fan jet engines.\n\nEngine degradation simulation was carried out using C-MAPSS. Four different were sets simulated under different combinations of operational conditions and fault modes. Records several sensor channels to characterize fault evolution. The data set was provided by the Prognostics CoE at NASA Ames.\n\nPrediction Goal\nIn this dataset the goal is to predict the remaining useful life (RUL) of each engine in the test dataset. RUL is equivalent of number of flights remained for the engine after the last datapoint in the test dataset.\n\nData Set Organization\nData Set: FD001\n\nTrain trjectories: 100\n\nTest trajectories: 100\n\nConditions: ONE (Sea Level)\n\nFault Modes: ONE (HPC Degradation)\n\nData Set: FD002\n\nTrain trjectories: 260\n\nTest trajectories: 259\n\nConditions: SIX\n\nFault Modes: ONE (HPC Degradation)\n\nData Set: FD003\n\nTrain trjectories: 100\n\nTest trajectories: 100\n\nConditions: ONE (Sea Level)\n\nFault Modes: TWO (HPC Degradation, Fan Degradation)\n\nData Set: FD004\n\nTrain trjectories: 248\n\nTest trajectories: 249\n\nConditions: SIX\n\nFault Modes: TWO (HPC Degradation, Fan Degradation)"}}