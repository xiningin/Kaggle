{"cell_type":{"809d91c5":"code","24eeaae0":"code","dc78ebc3":"code","9f1c4e46":"code","333649d2":"code","379be43b":"code","b9d8140b":"code","b4b0f8ae":"code","57151283":"code","d741ea22":"code","a387c4b5":"code","5459c483":"code","a999e761":"code","7cf1ec9e":"code","99a7c031":"code","1e48b890":"code","33d8090f":"code","4242c6f7":"code","e49c5dab":"code","26bcb0d1":"code","0b9c3c38":"code","08fabd60":"code","ecbe61e4":"code","6962cfc9":"code","42a78f7d":"code","fd517d46":"code","eff71c45":"code","b1325919":"code","5158eeaa":"code","49bc09f9":"code","20618398":"code","2458f1d0":"code","e304d6c0":"code","f5684e64":"code","1a34df8c":"code","655b66b2":"code","8dde74d6":"code","a25ae92d":"code","9686c777":"code","fe13ddcc":"code","a5ffc89d":"code","4d2f81f8":"code","2d602589":"code","30433132":"code","0c1ff0d2":"code","baaf48cd":"code","d4215c98":"code","b99cd764":"code","931dba46":"code","b27d149c":"code","584e513e":"code","fb61837e":"code","d4a8d9e9":"code","39f77753":"code","4764d1f7":"code","ac082c38":"code","b8e0a81c":"code","0cd43aca":"code","3385626b":"code","138c3719":"code","3e22889d":"code","53d74253":"code","3a2e77a9":"code","d87715b7":"code","a99936e9":"code","2103b2f6":"code","24724e81":"code","328ea20f":"code","fac65c4e":"code","bf6453e6":"code","6adfeb1e":"code","ece29d11":"code","54f835ae":"code","4d15dd0a":"code","1f79c43f":"code","be179651":"code","dc324fd3":"code","83cc0d2d":"code","c91013f2":"code","8be4314d":"markdown","2d288209":"markdown","77737f53":"markdown","ad008ad0":"markdown","b7309510":"markdown"},"source":{"809d91c5":"# Last amended: 15th April, 2021\n# Objectives:\n#                i) Data exploration\n#               ii) Data Visualization\n#              iii) Feature Engineering\n#               iv) Modeling\n#                v) Model optimization\n#\n#https:\/\/colab.research.google.com\/drive\/1lHArmyzqCy31EfrC7kM3r-qzB9xwP7LX\n#https:\/\/www.kaggle.com\/c\/liberty-mutual-fire-peril\n\n#           i) Liberty Mutual Group - Fire Peril Loss Cost \n#          ii) Predict a transformed ratio of loss to total insured value\n# Project by V. Siva Sundara Prasad, Chief Manager - IT","24eeaae0":"# 1.3 Call libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os","dc78ebc3":"# 1.4 Display output of multiple commands from a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","9f1c4e46":"# Read all seven files using pandas\n#train = pd.read_csv(\"train.csv.zip\")\ntrain = pd.read_csv(\"..\/input\/liberty-mutual-fire-peril\/train.csv.zip\")\n\ntest = pd.read_csv(\"..\/input\/liberty-mutual-fire-peril\/test.csv.zip\")\n# \n#test = pd.read_csv(\"test.csv.zip\")\n","333649d2":"# Also set options to display all rows\/all columns\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', None)  # or 199","379be43b":"# Check if read\n#       ((452061, 302), (450728, 301))\ntrain.shape \ntest.shape","b9d8140b":"# Reduce the size of Test and Train data frames to 25 % as the memory is being fully exhausted\n# The rows are selected randomly\n\ntrain = train.sample(frac = .25, replace = False)\n\ntest = test.sample(frac = .25, replace = False)","b4b0f8ae":"# Check the shape of the reduced data set\n\ntrain.shape # (113015, 302)\ntest.shape # (112682, 301)","57151283":"# Import Garbage Collector\nimport gc \n\n# Invoke gc\ngc.collect()","d741ea22":"# 2.1 Look at train data\nprint(\"\\n---train----\\n\")\ntrain.shape         # (452061, 302)\nprint(\"\\n------train------\\n\")\ntrain.head()\nprint(\"\\n-----Summary------\\n\")\ntrain.describe(include=\"all\")\nprint(\"\\n-----dtypes------\\n\")\ntrain.dtypes","a387c4b5":"#Replace NaN with some random values\n#Each set columns are replaced with a different values\nfor df in (train, test):\n\n  field_names = df.head()\n  for x in field_names:\n    if x[:3] == 'var':\n      df[x] = df[x].fillna(0.1)  #.astype('float64')\n    else:\n       if x[:8] == 'crimeVar':\n         df[x] = df[x].fillna(1.1).astype('float64')\n       else:\n          if x[:9] == 'geodemVar':\n            df[x] = df[x].fillna(2.1).astype('float64')\n          else:\n             if x[:10] == 'weatherVar':\n               df[x] = df[x].fillna(3.1).astype('float64')","5459c483":"# Encode the Categorical values in the train data\nfrom sklearn.preprocessing import OrdinalEncoder\n\nord_enc = OrdinalEncoder()\ntrain[\"var1_enc\"] = ord_enc.fit_transform(train[[\"var1\"]])\n#train[[\"var1\", \"var1_enc\"]].head(11)\ntrain[\"var2_enc\"] = ord_enc.fit_transform(train[[\"var2\"]])\ntrain[\"var3_enc\"] = ord_enc.fit_transform(train[[\"var3\"]])\ntrain[\"var4_enc\"] = ord_enc.fit_transform(train[[\"var4\"]])\ntrain[\"var5_enc\"] = ord_enc.fit_transform(train[[\"var5\"]])\ntrain[\"var6_enc\"] = ord_enc.fit_transform(train[[\"var6\"]])\ntrain[\"var8_enc\"] = ord_enc.fit_transform(train[[\"var8\"]])\ntrain[\"var9_enc\"] = ord_enc.fit_transform(train[[\"var9\"]])\ntrain[\"var10_enc\"] = ord_enc.fit_transform(train[[\"var10\"]])\ntrain[\"var11_enc\"] = ord_enc.fit_transform(train[[\"var11\"]])\ntrain[\"var12_enc\"] = ord_enc.fit_transform(train[[\"var12\"]])\ntrain[\"var13_enc\"] = ord_enc.fit_transform(train[[\"var13\"]])\ntrain[\"var14_enc\"] = ord_enc.fit_transform(train[[\"var14\"]])\ntrain[\"var15_enc\"] = ord_enc.fit_transform(train[[\"var15\"]])\ntrain[\"var16_enc\"] = ord_enc.fit_transform(train[[\"var16\"]])\ntrain[\"var17_enc\"] = ord_enc.fit_transform(train[[\"var17\"]])","a999e761":"# Encode the Categorical values in the actual test data\ntest[\"var1_enc\"] = ord_enc.fit_transform(test[[\"var1\"]])\ntest[\"var2_enc\"] = ord_enc.fit_transform(test[[\"var2\"]])\ntest[\"var3_enc\"] = ord_enc.fit_transform(test[[\"var3\"]])\ntest[\"var4_enc\"] = ord_enc.fit_transform(test[[\"var4\"]])\ntest[\"var5_enc\"] = ord_enc.fit_transform(test[[\"var5\"]])\ntest[\"var6_enc\"] = ord_enc.fit_transform(test[[\"var6\"]])\ntest[\"var8_enc\"] = ord_enc.fit_transform(test[[\"var8\"]])\ntest[\"var9_enc\"] = ord_enc.fit_transform(test[[\"var9\"]])\ntest[\"var10_enc\"] = ord_enc.fit_transform(test[[\"var10\"]])\ntest[\"var11_enc\"] = ord_enc.fit_transform(test[[\"var11\"]])\ntest[\"var12_enc\"] = ord_enc.fit_transform(test[[\"var12\"]])\ntest[\"var13_enc\"] = ord_enc.fit_transform(test[[\"var13\"]])\ntest[\"var14_enc\"] = ord_enc.fit_transform(test[[\"var14\"]])\ntest[\"var15_enc\"] = ord_enc.fit_transform(test[[\"var15\"]])\ntest[\"var16_enc\"] = ord_enc.fit_transform(test[[\"var16\"]])\ntest[\"var17_enc\"] = ord_enc.fit_transform(test[[\"var17\"]])","7cf1ec9e":"# Encode the column dummy in both the train and test data sets \ntrain[\"dummy_enc\"] = ord_enc.fit_transform(train[[\"dummy\"]])\ntest[\"dummy_enc\"] = ord_enc.fit_transform(test[[\"dummy\"]])","99a7c031":"# View the dataframe \ntrain.head()","1e48b890":"# Add a column each for the 4 categories of columns which consist of the std of all the respective columns\nfor df in (train, test):\n  var_enc_cols = list(df.columns)\n  a = var_enc_cols.index(\"var1_enc\") \n  b = var_enc_cols.index(\"var17_enc\") + 1\n  a\n  print(\"\")\n  b\n  var_enc_cols = list(df.columns)[a:b]\n  var_enc_cols  \n  #df[\"var_enc_cols_mean\"] = df[var_enc_cols].mean(axis=1)\n  df[\"var_enc_cols_mean\"] = df[var_enc_cols].std(axis=1)\n  #df.head() \n\n  crime_var_cols = list(df.columns)\n  a = crime_var_cols.index(\"crimeVar1\") \n  b = crime_var_cols.index(\"crimeVar9\") + 1\n  a\n  print(\"\")\n  b\n  crime_var_cols = list(df.columns)[a:b]\n  crime_var_cols\n  #df[\"crime_var_cols_mean\"] = df[crime_var_cols].mean(axis=1)\n  df[\"crime_var_cols_mean\"] = df[crime_var_cols].std(axis=1)\n  #df.head()\n\n  geodem_var_cols = list(df.columns)\n  a = geodem_var_cols.index(\"geodemVar1\") \n  b = geodem_var_cols.index(\"geodemVar37\") + 1\n  a\n  print(\"\")\n  b\n  geodem_var_cols = list(df.columns)[a:b]\n  geodem_var_cols\n  #df[\"geodem_var_cols_mean\"] = df[geodem_var_cols].mean(axis=1)\n  df[\"geodem_var_cols_mean\"] = df[geodem_var_cols].std(axis=1)\n  #df.head()\n\n  weather_var_cols = list(df.columns)\n  a = weather_var_cols.index(\"weatherVar1\") \n  b = weather_var_cols.index(\"weatherVar236\") + 1\n  a\n  print(\"\")\n  b\n  weather_var_cols = list(df.columns)[a:b]\n  weather_var_cols\n  #df[\"weather_var_cols_mean\"] = df[weather_var_cols].mean(axis=1)\n  df[\"weather_var_cols_mean\"] = df[weather_var_cols].std(axis=1)\n  df.head()","33d8090f":"# Copy Target column to another variable and drop the column\ny = train['target'].values\ntrain.drop(columns = ['target'], inplace = True)","4242c6f7":"# Check the shape of the train after deleting the column\ntrain.shape\ntrain.head()\n","e49c5dab":"# drop unnecessary columns in train data set\ntrain.drop(columns = ['id', 'var1', 'var2', 'var3', 'var4', 'var5', 'var6', 'var7', 'var8'], inplace = True)\ntrain.drop(columns = ['var9', 'var10', 'var11', 'var12', 'var13', 'var14', 'var15', 'var16', 'var17', 'dummy'], inplace = True)\ntrain.head()","26bcb0d1":"# Import matplot lib to draw plots for data vizualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","0b9c3c38":"# Distribution Plot\nsns.distplot(train.var_enc_cols_mean)\nsns.despine()               # Plot with and without it\n","08fabd60":"# Distribution Plot - on multiple variables from Train dataset\ncolumns = ['dummy_enc', 'var_enc_cols_mean', 'crime_var_cols_mean',\t'geodem_var_cols_mean',\t'weather_var_cols_mean']\nfig = plt.figure(figsize = (10,10))\nfor i in range(len(columns)):\n    plt.subplot(2,3,i+1)\n    sns.distplot(train[columns[i]])\n","ecbe61e4":"# More such relationships through for-loop\ncolumns = ['geodem_var_cols_mean',\t'weather_var_cols_mean', 'var_enc_cols_mean', 'crime_var_cols_mean']\ncatVar = ['dummy_enc'\t]\n\n# Now for loop. First create pairs of cont and cat variables\nmylist = [(cont,cat)  for cont in columns  for cat in catVar]\nmylist\n\n# 6.4 Now run-through for-loop\nfig = plt.figure(figsize = (10,10))\nfor i, k in enumerate(mylist):\n    #print(i, k[0], k[1])\n    plt.subplot(4,2,i+1)\n    sns.boxplot(x = k[1], y = k[0], data = train)","6962cfc9":"sns.jointplot(train.geodem_var_cols_mean,\ttrain.weather_var_cols_mean)","42a78f7d":"# Joint plot between var_enc_cols_mean & train.crime_var_cols_mean\n\nsns.jointplot(train.var_enc_cols_mean, train.crime_var_cols_mean,\n              kind = \"hex\"\n              )\n","fd517d46":"# Joint plot between geodem_var_cols_mean &\ttrain.weather_var_cols_mean\nsns.jointplot(train.geodem_var_cols_mean,\ttrain.weather_var_cols_mean,\n              kind = \"hex\"\n              )\n","eff71c45":"# See the power of t-sne\n#      (t-distributed Stochastic Neighbor Embedding)\n\nfrom sklearn.manifold import TSNE\n\n# 11.5.1 Project all data but 'Clicked_on_ad' on two axis\n#        Also just replace nc with nc_rand and try again\n\n#dummy_enc\tvar_enc_mean\tcrime_var_cols_mean\tgeodem_var_cols_mean\tweather_var_cols_mean\n\nenc_mean_cols = list(train.columns)\na = enc_mean_cols.index(\"dummy_enc\") \nb = enc_mean_cols.index(\"weather_var_cols_mean\") + 1\n\n# X_embedded = TSNE(n_components=2).fit_transform(train.iloc[a:a+3, a+3:b])\nX_embedded = TSNE(n_components=2).fit_transform(train.iloc[2:20, 21:30])\nX_embedded.shape    # (1000,2), numpy array\ndf = pd.DataFrame(X_embedded, columns=['X','Y'])\n\n# No two plots will be the same\nsns.relplot(x = \"X\",\n            y = \"Y\",\n            hue = train.dummy_enc,    # Colur each point as per 1 or 0\n            data = df\n            )\n","b1325919":"# Import Standard Scaler \nfrom sklearn.preprocessing import StandardScaler as ss","5158eeaa":"# Scale data using StandardScaler\nscale = ss()     # Create an instance of class\nscale.fit(train)                # Train object on the data\nX = scale.transform(train)      # Transform data\nX[:5, :]                  # See first 5 rows","49bc09f9":"# Import Train Test Split class\nfrom sklearn.model_selection import train_test_split\n# Class to develop kmeans model\nfrom sklearn.cluster import KMeans\n# Plotting library\nimport seaborn as sns\n# How good is clustering?\nfrom sklearn.metrics import silhouette_score\nfrom yellowbrick.cluster import SilhouetteVisualizer\n","20618398":"# Split train dataset into train\/test\nX_train, X_test, _, y_test = train_test_split( X,               # np array without target\n                                               y,               # Target\n                                               test_size = 0.25 # test_size proportion\n                                               )\n# 4.1 Examine the results\nX_train.shape              # (339045, 303)\nX_test.shape               # (113016, 303)\n","2458f1d0":"# Use sklearn's StandardScaler() to scale dataset\nclf = KMeans(n_clusters = 2)\n# Train the object over data\nclf.fit(X_train)\n\n# So what are our clusters?\nclf.cluster_centers_\nclf.cluster_centers_.shape         # (2, 303)\nclf.labels_                        # Cluster labels for every observation\nclf.labels_.size                   # 339045\nclf.inertia_                       # 91021988.53256002 Sum of squared distance to respective centriods, SSE\n","e304d6c0":"#iv)  Use sklearn's StandardScaler() to scale dataset\n# Make prediction over splitted test data and check accuracy\ny_pred = clf.predict(X_test)\ny_pred\n# 6.1 How good is prediction\nnp.sum(y_pred == y_test)\/y_test.size # 0.7837474339916471\n","f5684e64":"#iv)  Use sklearn's StandardScaler() to scale dataset\ndx = pd.Series(X_test[:, 0])\ndy = pd.Series(X_test[:,1])\nsns.scatterplot(dx,dy, hue = y_pred)\n","1a34df8c":"# Show the sample test dataframe\ntest.shape\ntest.head() # (112682, 322)","655b66b2":"# Drop unwanted columns\ntest.drop(columns = ['id', 'var1', 'var2', 'var3', 'var4', 'var5', 'var6'], inplace = True)\ntest.drop(columns = ['var7', 'var8', 'var9', 'var10', 'var11', 'var12', 'var13', 'var14', 'var15', 'var16', 'var17', 'dummy'], inplace = True)\ntest.head()","8dde74d6":"# Check the shape of train & actual test \ntrain.shape # (113015, 303)\ntest.shape # (112682, 303)","a25ae92d":"# fit the actual test data using StandardScaler\nscale.fit(test)                # Train object on the data\nX = scale.transform(test)      # Transform data\n","9686c777":"# Show first 5 rows\nX[:5, :]     ","fe13ddcc":"#iv)  Use sklearn's StandardScaler() to scale dataset\nclf = KMeans(n_clusters = 2)\n# Train the object over data\nclf.fit(X)\n\n# So what are our clusters?\nclf.cluster_centers_\nclf.cluster_centers_.shape         # (2, 303)\nclf.labels_                        # Cluster labels for every observation\nclf.labels_.size                   # 450728\nclf.inertia_                       # 120773499.68828635 Sum of squared distance to respective centriods, SSE\n","a5ffc89d":"#iv)  Use sklearn's StandardScaler() to scale dataset\n# Make prediction over actual test data and check accuracy\ny_pred = clf.predict(X)\ny_pred\n# How good is prediction\nnp.sum(y_pred == 1)\/450728  # 0.803629 (1 - 0.196371)","4d2f81f8":"#iv)  Use sklearn's StandardScaler() to scale dataset and plot \n#  Are clusters distiguisable?\n#     We plot 1st and 2nd columns of X\n#     Each point is coloured as per the\n#     cluster to which it is assigned (y_pred)\ndx = pd.Series(X[:, 0])\ndy = pd.Series(X[:,1])\nsns.scatterplot(dx,dy, hue = y_pred)","2d602589":"# Scree plot: X_train\nsse = []\nfor i,j in enumerate(range(3)):\n    # How many clusters?\n    n_clusters = i+1\n    # Create an instance of class\n    clf1 = KMeans(n_clusters = n_clusters)\n    # Train the kmeans object over data\n    clf1.fit(X_train)\n    # Store the value of inertia in sse\n    sse.append(clf1.inertia_ )\n\n# Plot the line now\nsns.lineplot(range(1, 4), sse)","30433132":"# Scree plot: X - actual test data\nsse = []\nfor i,j in enumerate(range(3)):\n    #  How many clusters?\n    n_clusters = i+1\n    #  Create an instance of class\n    clf1 = KMeans(n_clusters = n_clusters)\n    #  Train the kmeans object over data\n    clf1.fit(X)\n    #  Store the value of inertia in sse\n    sse.append(clf1.inertia_ )\n\n# Plot the line now\nsns.lineplot(range(1, 4), sse)","0c1ff0d2":"gc.collect()","baaf48cd":"# Import GaussianMixture class\nfrom sklearn.mixture import GaussianMixture\n\nimport time","d4215c98":"# Perform clustering using Gaussian Mixture Modeling.\ngm_liberty = GaussianMixture(\n                           n_components = 3,   # More the clusters, more the time\n                           n_init = 10,\n                           max_iter = 100\n                         )","b99cd764":"#  Perform clustering using Gaussian Mixture Modeling.\nstart = time.time()\ngm_liberty.fit(X)\nend = time.time()\n(end - start)\/60     # 6.76 minutes\n","931dba46":"#  Perform clustering using Gaussian Mixture Modeling.\n# Did algorithm(s) converge?\ngm_liberty.converged_     # True\n","b27d149c":"#   Perform clustering using Gaussian Mixture Modeling.\n# Clusters labels\ngm_liberty.predict(X)\n\n","584e513e":"#v)   Perform clustering using Gaussian Mixture Modeling.\n# How many iterations did they perform?\ngm_liberty.n_iter_      #  9","fb61837e":"#   Perform clustering using Gaussian Mixture Modeling.\n#  What is the frequency of data-points\n#       for the three clusters. (np.unique()\n#       ouputs a tuple with counts at index 1)\n\nnp.unique(gm_liberty.predict(X), return_counts = True)[1]\/len(X)","d4a8d9e9":"#   Perform clustering using Gaussian Mixture Modeling.\n# GMM is a generative model.\n#     Generate a sample from each cluster\n#     ToDo: Generate digits using MNIST\n\ngm_liberty.sample()","39f77753":"#   Perform clustering using Gaussian Mixture Modeling.\n# Plot cluster and cluster centers\n#     both from kmeans and from gmm\n\nfig = plt.figure()\n\nplt.scatter(X[:, 0], X[:, 1],\n            c=gm_liberty.predict(X),\n            s=2)\n\nplt.scatter(gm_liberty.means_[:, 0], gm_liberty.means_[:, 1],\n            marker='v',\n            s=5,               # marker size\n            linewidths=5,      # linewidth of marker edges\n            color='red'\n            )\nplt.show()","4764d1f7":"# Lookup anomalous customers and try to understand their behavior.\n#     Anomaly detection\n#     Anomalous points are those that\n#     are in low-density region\n#     Or where density is in low-percentile\n#     of 4%\n#     score_samples() method gives score or\n#     density of a point at any location.\n#     Higher the value, higher its density\n\ndensities = gm_liberty.score_samples(X)\ndensities","ac082c38":"# Lookup anomalous customers and try to understand their behavior.\ndensity_threshold = np.percentile(densities,4)\ndensity_threshold # 5.4128688273335195","b8e0a81c":"# Lookup anomalous customers and try to understand their behavior.\nanomalies = X[densities < density_threshold]\nanomalies\nanomalies.shape","0cd43aca":"# Lookup anomalous customers and try to understand their behavior.\n# Show anomalous points\nfig = plt.figure()\nplt.scatter(X[:, 0], X[:, 1], c = gm_liberty.predict(X))\nplt.scatter(anomalies[:, 0], anomalies[:, 1],\n            marker='x',\n            s=50,               # marker size\n            linewidths=5,      # linewidth of marker edges\n            color='red'\n            )\nplt.show()","3385626b":"# Lookup anomalous customers and try to understand their behavior.\n# Get first unanomalous data\nunanomalies = X[densities >= density_threshold]\nunanomalies.shape    # (108174, 303)","138c3719":"# Lookup anomalous customers and try to understand their behavior.\n# Transform both anomalous and unanomalous data\n#     to pandas DataFrame\n#df_anomalies = pd.DataFrame(anomalies, columns = ['x', 'y', 'p'])\ndf_anomalies = pd.DataFrame(anomalies)\ndf_anomalies['z'] = 'anomalous'   # Create a IIIrd constant columna\n#df_normal = pd.DataFrame(unanomalies, columns = ['x','y', 'p'])\ndf_normal = pd.DataFrame(unanomalies)\ndf_normal['z'] = 'unanomalous'    # Create a IIIrd constant column\n","3e22889d":"df_anomalies.columns\n\ndf_normal.head()","53d74253":"# Lookup anomalous customers and try to understand their behavior.\n# Let us see density plots\nsns.distplot(df_anomalies[0])\nsns.distplot(df_normal[0])","3a2e77a9":"# Lookup anomalous customers and try to understand their behavior.\n# Let us see density plots\nsns.distplot(df_anomalies[301])\nsns.distplot(df_normal[301])","d87715b7":"# Lookup anomalous customers and try to understand their behavior.\n# Let us see density plots\nsns.distplot(df_anomalies[100])\nsns.distplot(df_normal[100])","a99936e9":"# Lookup anomalous customers and try to understand their behavior.\n# Draw side-by-side boxplots\n# Ist stack two dataframes\ndf = pd.concat([df_anomalies,df_normal])\n# Draw featurewise boxplots\nsns.boxplot(x = df['z'], y = df[0])\nsns.boxplot(x = df['z'], y = df[301])\nsns.boxplot(x = df['z'], y = df[100])","2103b2f6":"#vi)  Use aic and bic measures to draw a scree plot and discover ideal number of clusters\nstart = time.time()\n\nbic = []\naic = []\nfor i in range(3):\n    gm2 = GaussianMixture(\n                     n_components = i+1,\n                     n_init = 10,\n                     max_iter = 100)\n    gm2.fit(X)\n    bic.append(gm2.bic(X))\n    aic.append(gm2.aic(X))\n    \nend = time.time()\n(end - start)\/60     # 17.740078067779542 minutes","24724e81":"#vi)  Use aic and bic measures to draw a scree plot and discover ideal number of clusters\nfig = plt.figure()\nplt.plot([1,2,3], aic)\nplt.plot([1,2,3], bic)\nplt.show()","328ea20f":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt","fac65c4e":"# t-stochaistic neighbourhood embedding\n#     Even though data is already in 2-dimension,\n#     for the sake of completion, \n#     darwing a 2-D t-sne plot and colour\n#     points by gmm-cluster labels\nstart = time.time()\ntsne = TSNE(n_components = 3, perplexity = 30)\ntsne_out = tsne.fit_transform(X)\nplt.scatter(tsne_out[:, 0], tsne_out[:, 1],\n            marker='o',\n            s=50,              # marker size\n            linewidths=5,      # linewidth of marker edges\n            c=gm2.predict(X)   # Colour as per gmm\n            )\nplt.title('t-SNE visualization');\nend = time.time()\n(end - start)\/60     # 14.636476087570191 minutes\n\n#### This block is taking more than 2 hours to complete","bf6453e6":"from skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb","6adfeb1e":"test.head()","ece29d11":"#  Classification\n\nITERATIONS=10   # Decides how many param-combinations can be tested\n\n# Define parameter range\nparams ={\n        'dummy_enc': Integer(0,1),\n        'var_enc_cols_mean': Real(0, 1000000, 'log-uniform'),\n        'crime_var_cols_mean': Real(1e-9, 1.0, 'log-uniform'),\n        'geodem_var_cols_mean': Real(1e-9, 10, 'log-uniform'),      \n        'weather_var_cols_mean': Real(1e-9, 10, 'log-uniform')\n    }\n    ","54f835ae":"\n# This code is used to over come the error \n# TypeError: __init__() got an unexpected keyword argument 'iid'\ndef bayes_search_CV_init(self, estimator, search_spaces, optimizer_kwargs=None,\n                         n_iter=50, scoring=None, fit_params=None, n_jobs=1,\n                         n_points=1, iid=True, refit=True, cv=None, verbose=0,\n                         pre_dispatch='2*n_jobs', random_state=None,\n                         error_score='raise', return_train_score=False):\n\n        self.search_spaces = search_spaces\n        self.n_iter = n_iter\n        self.n_points = n_points\n        self.random_state = random_state\n        self.optimizer_kwargs = optimizer_kwargs\n        self._check_search_space(self.search_spaces)\n        self.fit_params = fit_params\n\n        super(BayesSearchCV, self).__init__(\n             estimator=estimator, scoring=scoring,\n             n_jobs=n_jobs, refit=refit, cv=cv, verbose=verbose,\n             pre_dispatch=pre_dispatch, error_score=error_score,\n             return_train_score=return_train_score)\n        \nBayesSearchCV.__init__ = bayes_search_CV_init","4d15dd0a":"# Drop in replacement for GridSearchCV\n# https:\/\/scikit-optimize.github.io\/stable\/modules\/generated\/skopt.BayesSearchCV.html\nbayes_cv_tuner = BayesSearchCV(\n                               # https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn\n                               # lso specify 'fixed' parameter values\n                               estimator = xgb.XGBClassifier(\n                                                             n_jobs = 3,\n                                                             objective = 'binary:logistic',\n                                                             eval_metric = 'auc', # Not relevant here\n                                                                                  # See comments below\n                                                             tree_method='approx' \n                                                                     # 'hist' is one fast approx method\n                                                            ),\n                              search_spaces = params,    \n                                  scoring = 'roc_auc',\n                              cv = StratifiedKFold(\n                                                    n_splits=3,\n                                                    shuffle=True,\n                                                   ),\n                              n_jobs = 3,\n                              n_iter = ITERATIONS,   \n                              verbose = 1,\n                              refit = True\n                             )\n","1f79c43f":"#Remove BayesSearchCV(iid=) parameter \n#  Define a callback function\ndef status_print(optim_result):\n    \"\"\"Status callback during bayesian hyperparameter search\"\"\"\n    \n    # Get all the models tested so far in DataFrame format\n    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)\n    #print(optim_result)\n    # print (all_models)\n    \n    # Get current parameters and the best parameters    \n    best_params = pd.Series(bayes_cv_tuner.best_params_)\n    print('Model: {}\\n            \\\n           Best ROC-AUC: {}\\n     \\\n           Best params: {}\\n      \\\n           '.format(            \n                     len(all_models), # no of rows\n                     np.round(bayes_cv_tuner.best_score_, 4),\n                     bayes_cv_tuner.best_params_\n                    )\n          \n         )\n    \n    # Save all model results\n    all_models.to_csv(\"myresults_cv_results.csv\")","be179651":"X.shape\ntest.shape\ntest.head()","dc324fd3":"X[:5, 298:303]","83cc0d2d":"\n# Finally fit the model\nstart = time.time()\nresult = bayes_cv_tuner.fit(X[:, : ],\n                            y_pred,\n                            callback=[status_print] # callback = [list_of_callables] is called after each \n                                                    #   parameter combination tested.\n                           )\nend = time.time()\n(end-start)\/60","c91013f2":"######### Done ##################","8be4314d":"### Field descriptions\n\nMost of the fields are self-explanatory. The following are descriptions for those that aren't.\n>  **id :** A unique identifier of the data set\n\n>  **target :** The transformed ratio of loss to total insured value\n\n>  **dummy :** Nuisance variable used to control the model, but not working as a predictor\n\n>  **var1 \u2013 var17 :** A set of normalized variables representing policy characteristics (note: var11 is the weight used in the weighted gini score calculation)\n\n>  **crimeVar1 \u2013 crimeVar9:** A set of normalized Crime Rate variables\n\n>  **geodemVar1 \u2013 geodemVar37 :** A set of normalized geodemographic variables\n\n>  **weatherVar1 \u2013 weatherVar236 :** A set of normalized weather station variables   \n","2d288209":"### Libraries and data files","77737f53":"### Read all data","ad008ad0":"### Explore train data","b7309510":"# The problem\nA Fortune 100 company, Liberty Mutual Insurance has provided a wide range of insurance products and services designed to meet their customers' ever-changing needs for over 100 years.\n\nWithin the business insurance industry, fire losses account for a significant portion of total property losses. High severity and low frequency, fire losses are inherently volatile, which makes modeling them difficult. In this problem, the task is to predict the target, a transformed ratio of loss to total insured value, using the provided information. This will enable more accurate identification of each policyholder\u2019s risk exposure and the ability to tailor the insurance coverage for their specific operation.\n\nThe data provided represents almost a million insurance records and the task is to predict a transformed ratio of loss to total insured value (called \"target\" within the data set). The provided features contain policy characteristics, information on crime rate, geodemographics, and weather.\n\nThe train and test sets are split randomly. For each id in the test set, you must predict the target using the provided features."}}