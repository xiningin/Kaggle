{"cell_type":{"ff6338a0":"code","ab98b2f5":"code","8cd448e7":"code","c069c8f8":"code","00d7ad49":"code","c909f986":"code","9db22460":"code","7a8ec560":"code","a6b63c30":"code","60ec9a85":"code","b775f0dd":"code","69abf881":"code","e7f86aa7":"code","7efec2fe":"code","fc0de3ac":"markdown","9d5bbaa6":"markdown","17b7312d":"markdown","72db0ef3":"markdown","2eaa4c02":"markdown","bf00811f":"markdown","5ff4fc79":"markdown","d3177a61":"markdown","dd968585":"markdown","3a1fbc82":"markdown","39d4ea71":"markdown","5eb6146b":"markdown"},"source":{"ff6338a0":"import numpy as np \nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nimport glob\nimport os\nimport math\nimport csv\nimport gc\nimport re\n\nfrom multiprocessing import Process, Manager\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer \n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom ast import literal_eval\n\nfrom gensim.models import Word2Vec\n\nplt.style.use('ggplot')\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n#metafile = open('..\/input\/CORD-19-research-challenge\/metadata.csv')\n\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\ntokenizer = RegexpTokenizer(r'\\w+')\n\nw2v = Word2Vec.load('..\/input\/models\/word2vec\/w2v.model')","ab98b2f5":"def save_prep_data():\n    \"\"\"\n    preprocesses all relevant json file papers and stores them in a pandas data frame\n    \"\"\"\n    all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\n\n    df_covid = pd.DataFrame(columns=['paper_id', 'title', 'abstract', 'body_text'])\n    \n    batch_size = 20000\n    batch_idx = 0\n    for idx, entry in enumerate(all_json):\n        print(f'Processing index: {idx} of {len(all_json)}')\n        \n        with open(entry) as json_file:\n            json_text = json.load(json_file)\n\n            paper_id = json_text['paper_id']\n\n            body_text_stem = []\n            abb_lexicon = {}\n            for body_elem in json_text['body_text']:\n                abb_lexicon.update(get_abbreviation_lexicon(body_elem['text']))\n\n                for w in tokenizer.tokenize(body_elem['text'].lower()):\n                    if w in abb_lexicon:\n                        abb = abb_lexicon[w]\n                        for abb_w in tokenizer.tokenize(abb):\n                            if abb_w not in stop_words:\n                                body_text_stem.append(stemmer.stem(abb_w)) \n                    else:\n                        if w not in stop_words:\n                            body_text_stem.append(stemmer.stem(w)) \n\n            df_covid.loc[idx] = [paper_id, title, abstract, body_text_stem]\n            \n        \n        if idx % batch_size == 0 and idx != 0:\n            print('Save batch #', batch_idx)\n            df_covid.to_pickle('prep_data_'+str(batch_idx)+'.pkl')\n            df_covid = pd.DataFrame(columns=['paper_id', 'title', 'abstract', 'body_text'])\n            batch_idx += 1\n\n    df_covid.to_pickle('prep_data_'+str(batch_idx)+'.pkl')\n\n    \ndef get_abbreviation_lexicon(text):\n    \"\"\"\n    text: full text to create the abbreviation lexicon from\n    \n    returns a dictionary [k: abbreviation (string); v: specification(list<string>)]\n    \"\"\"\n    sentences = sent_tokenize(text)\n    abbreviation_lexicon = {}\n    \n    for sent in sentences:\n        bracket_strings = re.findall(r'\\((.*?)\\)', sent)\n        \n        p_symbol = re.compile('[-,@_!#$%^&*()<>?\/\\|}{~:]|\\d|[\\u0370-\\u03FF]') # Check for symbols and numbers\n        p_uppercase = re.compile('\\w+[A-Z]') # Only use uppercase letters\n        \n        if bracket_strings:\n            for string in bracket_strings:\n                if len(string) > 1 and len(string.split()) == 1 and re.search(p_symbol, string) == None:\n                    uc_string = re.search(p_uppercase, string)\n                    if uc_string:\n                        string = uc_string.group(0)\n                        \n                        re_spec = None\n                        pattern = '\\((?='+ string + ')'\n                        i = 0\n                        while True:\n                            letter = string[len(string)-(i+1)]\n                            \n                            if i == 0: \n                                pattern = '\\w+ ' + pattern \n                            else: \n                                pattern = '\\w+( |-)' + pattern   \n                            \n                            re_result = re.search(pattern, sent)\n                            if not re_result:\n                                pattern = '\\((?='+ string + ')'\n                                for i in range(len(string)):\n                                    if i == 0: \n                                        pattern = '\\w+ ' + pattern \n                                    else: \n                                        pattern = '\\w+( |-)' + pattern  \n                                    re_spec = re.search(pattern, sent)   \n                                break\n                                \n                            first_letter = re_result.group(0)[0]\n                            \n                            if first_letter.lower() == letter.lower():\n                                i += 1\n                                if i == len(string):\n                                    re_spec = re.search(pattern, sent)  \n                                    break\n                        if re_spec:\n                            specification = re_spec.group(0)[:-2]\n                            abbreviation_lexicon.update({string.lower(): specification.lower()})\n    return abbreviation_lexicon","8cd448e7":"def tf_idf(x, D):\n    \"\"\"\n    x: word to calculate the tf-idf value (string)\n    D: corpus with all documents (list<list<string>>)\n    \n    returns the tfidf-value for x in D (double)\n    \"\"\"\n    x = PorterStemmer().stem(x)\n    \n    # calculate the idf value\n    texts = D['body_text']\n    N = len(texts)\n    d_count = len(texts[texts.str.contains(x)])\n    idf_val = math.log10(N \/ (1 + d_count)) + 1\n    \n    result = []\n    \n    for _, row in D.iterrows():\n        if len(row['body_text']) > 0:\n            # calculate the tf value\n            tf_val = math.log10(1 + (row['body_text'].count(x) \/ len(row['body_text'])))\n            # combine tf- and idf-value\n            tf_idf = tf_val * idf_val\n            result.append({'paper_id': row['paper_id'], 'tf-idf': tf_idf})\n            \n    return result\n\ndef sum_tf_idf(X, D, topN, sum_results):\n    \"\"\"\n    X: Wordlist to calculate the tf-idf values (list<string>)\n    D: corpus with all documents (list<list<string>>)\n    topN: number of the top N documents to return based on the tfidf \n    \n    returns a result dictionary. [paper_id (string), tf-idf (double)]\n    \"\"\"\n    tfidf_results = []\n    \n    \n    for x in X:\n        tfidf = tf_idf(x, D)\n        tfidf = sorted(tfidf, key=lambda k: k['paper_id'], reverse=True) \n        tfidf_results.append(tfidf)\n\n    for i in range(len(tfidf_results[0])):\n        n = 0\n        value = 0\n        for res in tfidf_results:\n            if res[i]['tf-idf'] > 0:\n                n += 1\n            value += res[i]['tf-idf']\n        value *= n\n\n        if len(sum_results) < topN:\n            sum_results.append({'paper_id': tfidf_results[0][i]['paper_id'], 'tf-idf': value})\n            sum_results = sorted(sum_results, key=lambda k: k['tf-idf'], reverse=True) \n        elif value > sum_results[topN-1]['tf-idf']:\n            sum_results[topN-1] = {'paper_id': tfidf_results[0][i]['paper_id'], 'tf-idf': value}\n            sum_results = sorted(sum_results, key=lambda k: k['tf-idf'], reverse=True) \n    return sum_results","c069c8f8":"def get_most_relevant(X, topN=20):\n    \"\"\"\n    X: Wordlist to calculate the sum-tf-idf values (list<string>)\n    topN: number of the top N documents to return based on the tfidf (integer)\n    \n    returns a result dictionary for the N-most relevant papers. [paper_id (string), tf-idf (double)]\n    \"\"\"\n    sum_results = []\n    \n    for i in range(3):\n        data = pd.read_pickle('..\/input\/preprocessed-data\/prep_data\/prep_data_' + str(i) + '.pkl')\n        data = data[data['paper_id'].str[0:3] != 'PMC']\n        sum_results = sum_tf_idf(X, data, topN, sum_results)\n        del data\n        gc.collect()\n    return sum_results","00d7ad49":"def get_body_text(paper_id):\n    \"\"\"\n    paper_id: ID of the paper (string)\n    \n    returns the body_text (string)\n    \"\"\"\n    file = glob.glob('\/kaggle\/input\/CORD-19-research-challenge\/**\/' + paper_id + '.json', recursive=True)\n    \n    body_text = ''\n    if len(file) > 0:\n        file = json.load(open(file[0]))\n        \n        for body_elem in file['body_text']:\n            body_text += body_elem['text']\n    \n    return body_text\n\ndef get_abstract(paper_id):\n    \"\"\"\n    paper_id: ID of the paper (string)\n    \n    returns the abstract (string)\n    \"\"\"\n    file = glob.glob('\/kaggle\/input\/CORD-19-research-challenge\/**\/' + paper_id + '.json', recursive=True)\n    \n    abstract = ''\n    if len(file) > 0:\n        file = json.load(open(file[0]))\n        \n        if 'abstract' in file:\n            for body_elem in file['abstract']:\n                abstract += body_elem['text'] \n    return abstract\n\ndef get_title(paper_id):\n    \"\"\"\n    paper_id: ID of the paper (string)\n    \n    returns the title (string)\n    \"\"\"\n    file = glob.glob(f'{root_path}\/**\/' + paper_id + '.json', recursive=True)\n    \n    title = '**No title found**'\n    if len(file) > 0:\n        file = json.load(open(file[0]))\n        title = file['metadata']['title']\n    return title","c909f986":"from heapq import nlargest\nfrom gensim.summarization.summarizer import summarize\n\n\ndef get_gensim_summary(text, ratio=0.2):\n    \"\"\"\n    text: The original text to be summarized (string)\n    ratio: defines how huge the summary should be based on percentage of the original text (double)\n    \n    returns a text summary based on the gensim summary function (string)\n    \"\"\"\n    return summarize(text, ratio=ratio)\n\ndef get_sif_summary(text, model, compare_text=None, num_sentences=10, as_array=False):\n    \"\"\"\n    text: the text to summarize (string)\n    model: gensim model to get the word vectors (BaseWordEmbeddingsModel e.g. word2vec, fasttext) \n    compare_text: text to compute the similarity of the sentences (e.g. paper's abstract)\n    num_sentences: length of the summary (int)\n    as_array: True -> return summary as array; False: return as text\n    \n    returns the summary (list<string> or string)\n    \"\"\"\n    if text == '' or text == None:\n        return 'No text found. Summarization failed!'\n    \n    sentences = sent_tokenize(text)  #split text into sentences\n\n    if compare_text == None:\n        compare_text = text\n    text_sif = sif_embeddings([compare_text], model)  #Calculate sif-vector for the text\n\n    similarity = []\n\n    for sent in sentences:\n        sent_sif = sif_embeddings([sent], model)  #Calculate sif-vector for the sentence\n\n        sif = np.concatenate((text_sif, sent_sif), axis=0)\n        cos = cosine_similarity(sif)[0][1]\n        \n        similarity.append({'sent': sent, 'cos': cos})\n\n    similarity = sorted(similarity, key=lambda k: k['cos'], reverse=True)\n\n    summary = ''\n    if as_array:\n        summary = []\n    for i in range(num_sentences):\n        if as_array:\n            summary.append(similarity[i]['sent'])\n        else:\n            summary += similarity[i]['sent'] + '\\n'\n    return summary\n\ndef get_sentence_similarity(text1, text2, model):\n    \"\"\"\n    text1, text2: texts to calculate the cosine similarity, list of sentences in text (string)\n    model: gensim model to get the word vectors (BaseWordEmbeddingsModel e.g. word2vec, fasttext) \n    \n    returns a result dictionary. [sent1 (string), sent2 (string), cos (double)]\n    \"\"\"\n    sent_similarity = []\n    \n    sentences1 = sent_tokenize(text1)\n    sentences2 = sent_tokenize(text2)\n    \n    manager = Manager()\n    emb_dict1 = manager.dict()\n    emb_dict2 = manager.dict()\n    \n    def calc_sif1(sentences):\n        for s in sentences:\n            emb_dict1[s] = sif_embeddings([s], model)\n    def calc_sif2(sentences):\n        for s in sentences:\n            emb_dict2[s] = sif_embeddings([s], model)    \n    p1 = Process(target=calc_sif1, args=(sentences1,))\n    p2 = Process(target=calc_sif2, args=(sentences2,))\n    p1.start()\n    p2.start()\n    p1.join()\n    p2.join()\n    \n    for k1, v1 in emb_dict1.items():\n        for k2, v2 in emb_dict2.items():\n            sif = np.concatenate((v1, v2))\n            cos = cosine_similarity(sif)[0][1]\n            sent_similarity.append({'sent1': k1, 'sent2': k2, 'cos': cos})\n    sent_similarity = sorted(sent_similarity, key=lambda k: k['cos'], reverse=True) \n    return sent_similarity\n\ndef sif_embeddings(texts, model, alpha=1e-3):\n    \"\"\"\n    Calculates sentence embeddings based on the avg sif-word-vectors in the sentence\n    \n    texts: list of texts to perform sentence embedding (list<string>)\n    model: gensim model to get the word vectors (BaseWordEmbeddingsModel e.g. word2vec, fasttext)\n    alpha: alpha value for the sif calculation, standard=0,003 (double)\n    \n    returns a vector for each sentence (numpy.ndarray)\n    \"\"\"\n    tok_texts = []\n    for i in range(len(texts)):\n        tok_texts.append(tokenizer.tokenize(texts[i]))\n    \n    vlookup = model.wv.vocab\n    vectors = model.wv \n    size = model.vector_size\n\n    # Compute the normalization constant Z\n    #Z = sum(len(t) for t in tok_texts)\n    Z = sum(vlookup[k].count for k in vlookup)\n    \n    output = []\n    \n    for t in tok_texts:\n        count = 0\n        v = np.zeros(size, dtype=np.float32)  # Summary vector\n        \n        for w in t:\n            if w in vlookup:\n                v += (alpha \/ (alpha + (vlookup[w].count \/ Z))) * vectors[w]\n                count += 1\n\n        if count > 0:\n            for i in range(size):\n                v[i] *= 1 \/ count\n        output.append(v)\n    \n    return np.vstack(output).astype(np.float32)","9db22460":"results = get_most_relevant(['social', 'distancing'], topN=10)\ncorpus = []\nfor i in range(len(results)):\n    text = get_body_text(results[i]['paper_id'])\n    corpus.append(text)","7a8ec560":"w2v_sif = sif_embeddings(corpus, w2v)\nmatrix = cosine_similarity(w2v_sif)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nmask = np.zeros_like(matrix)\nmask[np.triu_indices_from(mask)] = True\n\nsns.set()\nsns.set_style('white')\na4_dims = (20, 12)\nfig, ax = plt.subplots(figsize=a4_dims)\nax = sns.heatmap(ax=ax, data=matrix, mask=mask, annot=True, fmt=\".2f\")\n\ntitles = []\nfor result in results:\n    titles.append(get_title(result['paper_id']))\nprint('Paper titles:')\nfor idx in range(len(titles)):\n    print(str(idx) + ': ' + titles[idx])","a6b63c30":"paper_id = 3","60ec9a85":"# Step 3\nabstract = get_abstract(results[paper_id]['paper_id'])\nif abstract == '':\n    abstract = None\n    \nsif_summary = get_sif_summary(corpus[paper_id], w2v, compare_text=abstract, num_sentences=5, as_array=False)\ngensim_summary = get_gensim_summary(corpus[paper_id], ratio=0.05)","b775f0dd":"print(sif_summary)","69abf881":"print(gensim_summary)","e7f86aa7":"paper_id1 = 2\npaper_id2 = 7","7efec2fe":"# Step 4\nsent_similarity = get_sentence_similarity(corpus[paper_id1], corpus[paper_id2], w2v)\n\ntopN = 5\nfor item in sent_similarity:\n    topN -= 1\n    print(str(item['cos']) + ':')\n    print()\n    print(item['sent1'])\n    print()\n    print(item['sent2'])\n    print()\n    print()\n    if topN == 0:\n        break","fc0de3ac":"Hurray! You made it to the end. We hope that you could find some interesting ideas in this kernel and that our tool can help you to filter interesting insight from papers and to foster rapid research sharing among scholars. ","9d5bbaa6":"# Step 4 : Monitor if papers have similar opinions on the investigated topic\n\nWhat is actually important to know is how many scholar came to the same findings or support certain findings. Thus one can see whether a statements tends to be true or false. \n\nIn order to help users to easily find those overlaps in documents we build a sentence similarity function that highlights similar sentences within documents. \n\nPlease select two papers you want to compare.","17b7312d":"# Functions to generate summaries and detect similar sentences\n\nIn order to give the reader a short executive summary for the investigated paper we created three different summary functions. \n\nThereby we tried to satisfy three different focus points for summaries: \n\n1. General overview of the text                                        (satisfied by the gensim summary function)\n2. Keywords specific insight to narrow focuspoints within the document (satisfied by our keyword focused summary)\n3. A good scientific overview of the paper                             (satisfied by the SIF-embedding summary)","72db0ef3":"# Basic idea of the kernel\n \nWe joined the competition to contribute an advanced search engine help search for research papers. The idea was to develop a tool that is able to search relevant for a given word vector, very much alike a traditional search engine tool. The paper is then ranked in relevance to the established word vector. Afterwards our tool generates a heatmap cluster for the N most relevant papers, helping to identify similar papers. The goal for generating this heatmap was to quickly gain insight into a specific topic and find corresponding papers, so that a comprehensive insight is possible.\n\nUltimately, the tool can then produce a summary for each of the papers either using a SIF-like, TF-IDF based approach, or genomic summary function. In our tests, the generic gensim summary function generated the most general, but also most easy to read summaries. If a comprehensive insight was needed, the TF-IDF approach performed the best. It allows to give priority to certain keywords, so that the reader can quickly identify important statements on specific sub-topics. With this research tool we hope to help researchers navigate more easily through the ever-growing number of papers regarding the SARS-COVID 19 pandemic so that they may find adequate countermeasures to combat the virus.\n\nIn the second submission period we will focus our efforts on improving the usability of the kernel through a more sophisticated visualisation, and an improved search algorithm. We believe PageRank can be used to find the most relevant papers and would be able to improve the quality of searches but are concerned that papers that are still in the process of being published could screw the results.\n\nFor now, we are happy to showcase our engine and guide you through our implementation process step-by-step on the next section. If you are only interested in using the tool, you can use the following code block to operate the system. (Please make sure to initialize all functions first and import all necessary libraries)\n","2eaa4c02":"# Functions to access N-most relevant documents based on their TF-IDF value\n\nHere we created a simply function that returns the n-most relevant documents for the given search vector, based on their TF-IDF values. ","bf00811f":"# Step 1 : Get the most relevant Papers based on a given search vector","5ff4fc79":"# TF-IDF Calculation:\n\nIn order to find useful documents based on our word vector we leverage the tf-idf formular. Thereby, we considered one paper as a bag of words. The TF-IDF Formular it self is sensitive to occurencies of specific words within this bag. By leveraging an abbreviation lexicon we tried to compensate the difference between the information input from the user and the term specific knowledge of scientists. \n\nWithin the formular we decided to go with the log-normalized TF value and the smooth inverse document frequency. By using this slight modification of the formular we were able to standardize our metric among the highly fluctuative length of documents. \n\nIn a mathmatical way the formular looks as following: \n<img src=\"https:\/\/dxlcdg.db.files.1drv.com\/y4mGJgWxDwCygv1uY1RcE8DXA_p6eI4FwAcKywLIYNVj_u7YvDeIWebCJpDLw8wNfLAZfbY3D9i5fWNX_VXrE0DxOJZmN3Kz_7ifGhyXW7P04_qmfENVv3WxYmdJ-3Gs5Msq6wNc2RqlA8dv5mPENk9fWW7xpwO8WDHAJyaIRv9P9mGOnbCs9xFV6FUDJneSM1YTmp3jElPgbHaeUFh8-dGlw?width=1992&height=1125&cropmode=none\" width=\"1992\" height=\"1125\" \/>","d3177a61":"# Step 2 : Show similar papers for the given search vector\n\nThis steps helps you to identify similar papers using a heatmap visualization. \n\nFor identifying papers that are similar we use cosine similarity, a direction measure for vectors. For this the text has to be embedded as a vector. The words are embedded using the word2vec model by gensim. For identifying the similarity between whole papers we combine the word-embeddings with the Smooth Inverse Frequency(SIF) approach, by averaging the sum of all word vectors of a text. \n\nWord embeeding allows us to span dimensional feature spaces for the documents and thus we are able to use linear algebra techniques from machine learning to measure the distance of documents within these dimensional spaces.\n\nPapers that cover the same topics are likely to have a high cosine similarity, while documents from diffent topics have a low correlation or even negative.\n\nFor comprehensive visual explanation:\n<img src=\"https:\/\/ehlkdg.db.files.1drv.com\/y4mIcBzGGmA-S_Cq2YQ0djO-LiIliShqTK9bRhgdShsrg1GPYcVuinTlwQC7KZZ253Qbhp1nvGrzTkhXRx-cnAL6D2aw6NByNyYfaIm1BSgZP_P7f6Z5ep5hid4BN4ZxSj7uZsD8p3BjP6JuNdJHn2QGvgABcgMYTwTRKIDeM7rM-raAV1bRqLawkJTMtzyeSynXLEiMqyd8_ypycbGIBEF_Q?width=2538&height=1305&cropmode=none\" width=\"2538\" height=\"1305\" \/>","dd968585":"# Step 3 : Generate summaries for the papers you are interested in\n\nHere you can generate your summary for the text you are interested in. If you use our normal summary function you can indicate specific keywords in which you are interested in. For example you could specifically skim through the text searching for \"effecive measurement techniques\" in papers about \"social distancing\". ","3a1fbc82":"# Preprocessing of the Data\n\nBefore we started working with the given data we went through the common steps of natural language preprocessing:\n1. We lowered and tokenized our sentences. (we used a RegEx Tokenizer)\n2. We checked if a word is an abbreviation and replaced it with its full description for consistency among all documents (get_abbreviation_lexicon()).\n3. We checked if a word is a \"stopword\" and erased it from the corpus. (we used the NLTK corpus)\n4. We stemmed our words. (we used the PorterStemmer)\n\nAfter preprocessing we broke down the documents in four buckets, since one big bucket would max out the given RAM and saved it as a pickle file (gave us the possibility to save data as binary). ","39d4ea71":"# Functions to access specific information within data files\n\nFor verification and further investigation of the relevant papers we created a few simple functions that let us acces various data points in the documents. For example we get_abstract() gives you the abstract of the paper, if there exists any.","5eb6146b":"Here you can select which paper you want to summarize. Select the id from the heatmap and title-list above."}}