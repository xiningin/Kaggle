{"cell_type":{"14a84622":"code","0684910e":"code","07baee16":"code","5a9a1bad":"code","7e048101":"code","eb75113d":"code","60fedfd9":"code","7fbe1fa8":"code","ea71bdd8":"code","9fc78d16":"code","81ed9a5e":"code","75c87a7d":"code","cdc5ba51":"code","4909b497":"code","da7e2d53":"code","40e56869":"code","66ff5de4":"code","45a8a7a7":"code","6c9947aa":"code","3cf474d5":"code","27ce7785":"code","fa7c1602":"code","d5849371":"markdown","a95b60e8":"markdown","f6f04db7":"markdown","fca79e1e":"markdown","f7beb0f5":"markdown","81aa5ab7":"markdown","2c456749":"markdown","b55ae7d8":"markdown","2b9e0745":"markdown","65af8745":"markdown","cc8196ff":"markdown","d4192fc5":"markdown","d5a99699":"markdown","b3fcb9c4":"markdown","5994f085":"markdown","0a60e70a":"markdown","56b03e49":"markdown","0365c0cd":"markdown","387a2bc3":"markdown"},"source":{"14a84622":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0684910e":"!pip install beautifulsoup4 ","07baee16":"# Basic imports\nimport numpy as np\nimport pandas as pd \nimport re\nfrom bs4 import BeautifulSoup\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom collections import Counter, defaultdict\nimport altair as alt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import RepeatedStratifiedKFold,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.metrics import f1_score\nimport lime\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer","5a9a1bad":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","7e048101":"stop_words = set(stopwords.words('english')) \n\ndef text_cleaner(text,num):\n    '''\n        Text cleaner does the following\n        1. Lowercase text\n        2. Removes non text from raw reviews\n        3. Substitutes not alphanumeric characters\n        4. Correct words using the contractions mapping dictionary\n        5. Removes Junk characters generated after cleaning\n        6. Remove stop words if num=0 that means for review only not for summary\n        \n        Parameters: String, Number\n        Returns: String\n    '''\n    newString = text.lower() \n    newString = BeautifulSoup(newString, \"lxml\").text\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    newString = re.sub(r\"http\", \"\", newString)\n    newString = re.sub('\"','', newString)    \n    newString = re.sub(r\"'s\\b\",\"\",newString)\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n    newString = re.sub('[m]{2,}', 'mm', newString)\n    if(num==0):\n        tokens = [w for w in newString.split() if not w in stop_words]\n    else:\n        tokens=newString.split()\n    long_words=[]\n    for i in tokens:\n        if len(i)>1:                                                \n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()\n\n# Cleaning raw reviews\ncleaned_text_train = []\nfor t in train['text']:\n    cleaned_text_train.append(text_cleaner(t,1)) \n\ncleaned_text_test = []\nfor t in test['text']:\n    cleaned_text_test.append(text_cleaner(t,1))\n    \ntrain['text']=cleaned_text_train\ntest['text']=cleaned_text_test\n\n# Cleaning keywords and locations\ntrain['keyword'] = train['keyword'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))\ntrain['keyword'] = train['keyword'].apply(lambda x: lemmatizer.lemmatize(x.lower()))\ntest['keyword'] = test['keyword'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))\ntest['keyword'] = test['keyword'].apply(lambda x: lemmatizer.lemmatize(x.lower()))\ntrain['location'] = train['location'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))\ntest['location'] = test['location'].apply(lambda x: re.sub(\"[^a-zA-Z]\", \" \", str(x)))\ntrain['full_text'] = train['keyword'] + ' ' + train['location'] + ' ' + train['text']\ntest['full_text'] = test['keyword'] + ' ' + test['location'] + ' '  + test['text']\ntrain['full_text'] = train['full_text'].apply(lambda x: re.sub('nan', ' ', str(x)))\ntest['full_text'] = test['full_text'].apply(lambda x: re.sub('nan', ' ', str(x)))","eb75113d":"temp0 = train.target.value_counts().reset_index(name='count')\nalt.Chart(temp0, title='Non Disaster vs Disaster tweet count').mark_bar().encode(\n    x='index:O',\n    y='count',\n    color='index:O'\n).properties(width=400)","60fedfd9":"no_missing = train[(train['keyword']!='nan') & (train['location']!='nan')]","7fbe1fa8":"temp1 = no_missing['keyword'].value_counts().head(20).reset_index(name='count')\nalt.Chart(temp1, title='Top 20 keywords in tweets').mark_bar().encode(\n    alt.X('index', axis=alt.Axis(labelAngle=-45)),\n    y='count'\n).properties(width=700).configure_axis(\n    labelFontSize=15,\n    titleFontSize=15\n)","ea71bdd8":"temp2 = no_missing['location'].value_counts().head(20).reset_index(name='count')\nalt.Chart(temp2, title='Top 20 locations of tweets').mark_bar().encode(\n    alt.X('index', axis=alt.Axis(labelAngle=-45)),\n    y='count'\n).properties(width=700).configure_axis(\n    labelFontSize=15,\n    titleFontSize=15\n)","9fc78d16":"temp3 = no_missing.groupby(['keyword', 'location']).size().nlargest(20).reset_index(name='count')\nalt.Chart(temp3, title='Top location and keyword combinations').mark_circle(size=60).encode(\n    alt.X('location', axis=alt.Axis(labelAngle=-45)),\n    y='count',\n    color='keyword',\n    tooltip=['location', 'keyword', 'count']\n).properties(width=700).interactive().configure_axis(\n    labelFontSize=15,\n    titleFontSize=15\n)","81ed9a5e":"temp4 = no_missing.groupby(['keyword', 'target']).size().nlargest(40).reset_index(name='count')\nalt.Chart(temp4, title='How good a keyword is indicator of disaster?').mark_bar().encode(\n    x='target:O',\n    y='count',\n    color='target:N',\n    column='keyword'\n).properties(width=40)","75c87a7d":"temp5 = no_missing.groupby(['location', 'target']).size().nlargest(30).reset_index(name='count')\nalt.Chart(temp5, title='How good a location is indicator of disaster?').mark_bar().encode(\n    x='target:O',\n    y='count',\n    color='target:N',\n    column='location'\n).properties(width=40)","cdc5ba51":"temp6 = no_missing[no_missing['target']==1]['keyword'].value_counts().head(20).reset_index(name='count')\nalt.Chart(temp6, title='Top keywords present in Disaster tweets').mark_bar().encode(\n    alt.X('index', axis=alt.Axis(labelAngle=-45)),\n    y='count'\n).properties(width=700).configure_axis(\n    labelFontSize=15,\n    titleFontSize=15\n)","4909b497":"list_corpus = train[\"full_text\"].tolist()\nlist_labels = train[\"target\"].tolist()\nX_train, X_val, y_train, y_val = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=40)\nvectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), stop_words = 'english', binary=True, lowercase=True)\ntrain_vectors = vectorizer.fit_transform(X_train)\nval_vectors = vectorizer.transform(X_val)","da7e2d53":"logreg = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', n_jobs=-1, random_state=40)\nlogreg.fit(train_vectors, y_train)\npred = logreg.predict(val_vectors)\nf1 = f1_score(y_val, pred, average='weighted')\nprint(\"f1 score = %.3f\" % (f1))","40e56869":"X_val[205]","66ff5de4":"c = make_pipeline(vectorizer, logreg)\nclass_names=list(train.target.unique()[::-1])\nexplainer = LimeTextExplainer(class_names=class_names)\n\nidx = 205\nexp = explainer.explain_instance(X_val[idx], c.predict_proba, num_features=6)\nprint('Tweet id: %d' % idx)\nprint('Predicted class =', class_names[logreg.predict(val_vectors[idx]).reshape(1,-1)[0,0]])\nprint('True class: %s' % class_names[y_val[idx]])","45a8a7a7":"exp.show_in_notebook(text = X_val[idx])","6c9947aa":"X_val[23]","3cf474d5":"idx = 23\nexp = explainer.explain_instance(X_val[idx], c.predict_proba, num_features=6)\nprint('Tweet id: %d' % idx)\nprint('Predicted class =', class_names[logreg.predict(val_vectors[idx]).reshape(1,-1)[0,0]])\nprint('True class: %s' % class_names[y_val[idx]])","27ce7785":"exp.show_in_notebook(text = X_val[idx])","fa7c1602":"test_corpus = test[\"full_text\"].tolist()\ntest_vectors = vectorizer.transform(test_corpus)\npred = logreg.predict(test_vectors)\nsample_submission[\"target\"] = pred\nsample_submission.to_csv(\".\/submission.csv\", index=False)","d5849371":"## Modeling\nWe will not be building complex model or use neural nets for this dataset. A simple model should be good enough for decent outcome on f1 metrics. We will try to understand predictions of simple model with LIME explainer","a95b60e8":"***Top keyowrds related to diasters***","f6f04db7":"**Example 1**","fca79e1e":"#### Basic Cleaning pipeline","f7beb0f5":"**Comparison of Disaster and Non Disaster tweet counts**","81aa5ab7":"Keyword and location combination is good indicator of disaster. Like USA has lot of sandstorms, india having rail disasters, mumbai battling with floods. Damn! There is such a thing as pedophile hunting ground. WTF","2c456749":"#### New Dataframe to analyze importance of keywords and locations ","b55ae7d8":"Some locations like australia, india, mumbai have most of their tweets related to disaster","2b9e0745":"Fatality, weapon, siren, death are top keywords bot not neccesarily disaster indicators. But, keywords like flood and wildfire indicated disaster. We will compare keyword occurance in disaster and non disaster tweets later on.","65af8745":"**How good a location is indicator of disaster**","cc8196ff":"#### Datasets","d4192fc5":"**Submission**","d5a99699":"**Most occuring keyword and location combinations**","b3fcb9c4":"**Top 20 keywords in the dataset**","5994f085":"Lot of tweets come from unknown places. USA tops the location reference in tweets. We can correct some of the locations as unknown location can be classified as worldwide. And locations with different spelling and abbreviations can be clubbed.","0a60e70a":"**Example 2**","56b03e49":"**Top 20 locations of tweets**","0365c0cd":"**How good a keyword is indicator of disaster**","387a2bc3":"Some keywords are strong indicators of disasters such as draught, earthquake, flood, airplaneaccident, buildingsonfire etc"}}