{"cell_type":{"2805c5c9":"code","e4439880":"code","f9c7a24e":"code","e0814446":"code","e9daef67":"code","828aaacb":"code","b203bb54":"code","4132c385":"code","dcffa6da":"code","653c4654":"code","44e1d301":"code","7c956a62":"code","e0905383":"code","24a81e0a":"code","7bf43048":"code","8426be8e":"code","8a197b98":"code","4ad808e1":"code","8b44ac7c":"code","2d67d85a":"code","f9a5b355":"code","73b22d84":"code","e155db83":"code","c1c28c8a":"code","33c567cb":"code","bb6e01f7":"code","d47c6626":"code","b8c4176e":"code","202202d0":"code","eab8636f":"code","b5309440":"code","8b59d6c3":"code","8b3d7e90":"code","a2f68eb1":"code","9c22aa06":"code","af2e0bcc":"code","5ee53afc":"code","28ea6422":"code","2f91b15c":"code","b133d8b3":"code","04f60406":"code","edee5495":"code","42e44d89":"code","cf0e603e":"code","6f1ff71b":"code","d133ff78":"markdown","481ad6eb":"markdown","cfd8ed7b":"markdown","df1bfa1c":"markdown","2f3b7d2e":"markdown","c2f5d301":"markdown","9188ba86":"markdown","763ec6c5":"markdown","7f15ad5b":"markdown","e1442af7":"markdown","0b5ef241":"markdown","14aa2edc":"markdown","72c702f8":"markdown","978b6b15":"markdown","1efc80d6":"markdown","39ac0e74":"markdown","c3618bc5":"markdown","79902d4c":"markdown","3d7a1272":"markdown","aa9fa1dc":"markdown","6021a4d6":"markdown","166c9725":"markdown","da3dc79f":"markdown","ba96924e":"markdown","37cb839b":"markdown","081cd1a0":"markdown","94f9c353":"markdown","23be4a86":"markdown","024facc6":"markdown","963dbe60":"markdown","14309f94":"markdown"},"source":{"2805c5c9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e4439880":"data = pd.read_csv('..\/input\/indian-liver-patient-records\/indian_liver_patient.csv')\ndata.head()","f9c7a24e":"data.isnull().sum()","e0814446":"data[data['Albumin_and_Globulin_Ratio'].isnull()]","e9daef67":"data['Albumin_and_Globulin_Ratio'].describe()","828aaacb":"data['Albumin_and_Globulin_Ratio'].plot.hist()","b203bb54":"data['Albumin_and_Globulin_Ratio'].median()","4132c385":"#Creating checkpoint\ndf = data.copy()","dcffa6da":"df['Albumin_and_Globulin_Ratio'].fillna(data['Albumin_and_Globulin_Ratio'].median(), inplace=True)\ndf.isnull().sum()","653c4654":"df.describe(percentiles=[0.3,.5,.8]).round(2)","44e1d301":"print(f'Total number of Rows {df.shape[0]}\\nTotal number of Columns {df.shape[1]}')","7c956a62":"sns.pairplot(df, hue='Dataset', palette='viridis')","e0905383":"fig = plt.figure(figsize=(5,7))\nax = sns.boxplot(data = df['Total_Bilirubin'],orient='v')\nax.set_ylabel(\"Total Bilirubin\")","24a81e0a":"fig = plt.figure(figsize=(5,7))\nax = sns.boxplot(data = df['Direct_Bilirubin'],color='Red',orient='v')\nax.set_ylabel(\"Direct Bilirubin\")","7bf43048":"df['Alkaline_Phosphotase'].hist()","8426be8e":"df['Aspartate_Aminotransferase'].hist()","8a197b98":"df['Total_Protiens'].hist()","4ad808e1":"df['Albumin'].hist()","8b44ac7c":"df['Albumin_and_Globulin_Ratio'].hist()","2d67d85a":"plt.figure(figsize=(6,6))\nax = sns.countplot(x = df['Dataset'].apply(lambda x:'Liver Disease' if x == 1 else 'Non-Liver Disease'), hue=df['Gender'])\nax.set_xlabel('Patient Condition')\nfor p in ax.patches:\n  ax.annotate(f'{p.get_height()}',(p.get_x()+0.15, p.get_height()+3))","f9a5b355":"pd.Series(map(lambda x: 'Old_Age' if x>=90 else 'Adult_Age' if x > 21 else \"Young_Age\",df['Age'])).value_counts(normalize=True)","73b22d84":"df.head()","e155db83":"df.groupby('Gender').sum()['Total_Protiens'].plot.bar(color='#253660').set_ylabel('Total_Proteins')","c1c28c8a":"#Creating Checkpoint\ndf2 = df.copy()","33c567cb":"df2.head()","bb6e01f7":"df2 = pd.get_dummies(data=df2,columns=['Gender','Dataset'], drop_first=True)\ndf2.head()","d47c6626":"df2.rename(columns={'Dataset_2':'Have_Disease'},inplace=True)\ndf2 = df2[['Gender_Male', 'Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase',\n       'Alamine_Aminotransferase', 'Aspartate_Aminotransferase',\n       'Total_Protiens', 'Albumin', 'Albumin_and_Globulin_Ratio',\n       'Have_Disease']]","b8c4176e":"df2.head()","202202d0":"plt.figure(figsize=(15,6))\nsns.heatmap(df2.corr(),cmap='GnBu',annot=True)","eab8636f":"#Creating the check point after removing the correlated data\ndf3 = df2.drop(['Direct_Bilirubin','Aspartate_Aminotransferase','Albumin'], axis=1)","b5309440":"df3['Have_Disease'].value_counts()","8b59d6c3":"df3.describe(percentiles=[0.30,0.60,0.90])","8b3d7e90":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n#checkpoint\ndf4 = pd.concat([df3['Gender_Male'],pd.DataFrame(sc.fit_transform(df3.iloc[:,1:7])),df3['Have_Disease']], axis=1)\ndf4.columns = df3.columns\ndf4.head()","a2f68eb1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import train_test_split","9c22aa06":"from sklearn.metrics import confusion_matrix, precision_score, accuracy_score, recall_score","af2e0bcc":"def ml_algorthims(data):\n    fig, axes = plt.subplots(3,2, figsize=(10,10))\n    print(\"::::::::::::::::::: Splitting the dataset into train and test ::::::::::\")\n    x = data.drop('Have_Disease',axis=1).values\n    y = data.iloc[:,-1].values\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n    print(f\"X_Train : {x_train.shape}\\t X_Test : {x_test.shape}\\nY_Train : {y_train.shape}\\t Y_Test : {y_test.shape}\\n\")\n    print()\n    print(\" :::::::::::::::Logistic Regression::::::::::::: \")\n    lg = LogisticRegression().fit(x_train,y_train)\n    lg_pred = lg.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,lg_pred),annot=True, ax=axes[0,0])\n    ax.set_title(\"Logistic Confusion Matrix\")\n    # print(classification_report(lg_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,lg_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,lg_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,lg_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Decision Tree Classifier::::::::::::: \")\n    dtree = DecisionTreeClassifier().fit(x_train,y_train)\n    dtree_pred = dtree.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,dtree_pred),annot=True, ax=axes[0,1])\n    ax.set_title(\"Decision Tree Confusion Matrix\")\n    # print(classification_report(dtree_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,dtree_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,dtree_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,dtree_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Random Forest Classifier::::::::::::: \")\n    rftree = RandomForestClassifier().fit(x_train,y_train)\n    rftree_pred = rftree.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,rftree_pred),annot=True, ax=axes[1,0])\n    ax.set_title(\"Random Forest Confusion Matrix\")\n    # print(classification_report(rftree_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,rftree_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,rftree_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,rftree_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Xgboost::::::::::::: \")\n    xgb_model = XGBClassifier().fit(x_train,y_train)\n    xgb_model_pred = xgb_model.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,xgb_model_pred),annot=True, ax=axes[1,1])\n    ax.set_title(\"Xgb Confusion Matrix\")\n    # print(classification_report(xgb_model_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,xgb_model_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,xgb_model_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,xgb_model_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::K Nearest Neighbour::::::::::::: \")\n    knn = KNeighborsClassifier().fit(x_train,y_train)\n    knn_pred = knn.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,knn_pred),annot=True, ax=axes[2,0])\n    ax.set_title(\"KNN Confusion Matrix\")\n    # print(classification_report(knn_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,knn_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,knn_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,knn_pred):0.2f}')\n    print()\n    print(\" :::::::::::::::Support Vector Machine (SVM)::::::::::::: \")\n    svm = SVC().fit(x_train,y_train)\n    svm_pred = svm.predict(x_test)\n    ax = sns.heatmap(confusion_matrix(y_test,svm_pred),annot=True, ax=axes[2,1])\n    ax.set_title(\"SVM Confusion Matrix\")\n    # print(classification_report(svm_pred,y_test))\n    print(f'Accuracy : {accuracy_score(y_test,svm_pred):0.2f}')\n    print(f'Precision : {precision_score(y_test,svm_pred):0.2f}')\n    print(f'Recall : {recall_score(y_test,svm_pred):0.2f}')","5ee53afc":"import warnings  \nwarnings.filterwarnings('ignore')","28ea6422":"ml_algorthims(df4)","2f91b15c":"df_class_0 = df4[df4['Have_Disease'] == 0].copy()\ndf_class_1 = df4[df4['Have_Disease'] == 1].copy()\nundersample = df_class_0.sample(df_class_1.shape[0]).reset_index(drop=True)","b133d8b3":"ml_algorthims(pd.concat([undersample,df_class_1],axis=0))","04f60406":"oversample = df_class_1.sample(df_class_0.shape[0], replace=True)","edee5495":"ml_algorthims(pd.concat([oversample,df_class_0],axis=0))","42e44d89":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(sampling_strategy = 'minority')\nx_sm, y_sm = smote.fit_sample(df4.drop('Have_Disease',axis=1),df4['Have_Disease'])","cf0e603e":"nd = pd.concat([pd.DataFrame(x_sm),pd.DataFrame(y_sm)],axis=1)\nnd.columns = df4.columns","6f1ff71b":"ml_algorthims(nd)","d133ff78":"#### Looking into univairate features","481ad6eb":"* 91.5% of data are from 21-89 Age.\n* 8% of data are from below 22 Age.\n* 0.1% of data are from above 89 Age","cfd8ed7b":"#### 1. Undersampling the majority class","df1bfa1c":"## Balancing the imbalanced Dataset","2f3b7d2e":"#### Lets Encode the nominal features","c2f5d301":"### Unbalanced(Original) Dataset","9188ba86":"So,here we will try to focus on recall score value.","763ec6c5":"### Here datasets conatins unbalanced Classes so we will try to resample our data to reduce the incorrectness.","7f15ad5b":"Looks for Correct metrics to replace the Null value","e1442af7":"We will look for metrics","0b5ef241":"#### 2. Oversampling minority class","14aa2edc":"Dataset Column<br>\n* 1 - Patient with liver disease\n* 2 - Patient with no disease","72c702f8":"Here data are normally distrubuted but sightly right skewed.So, we can can go with Mean or Mediam. Here i am going with median.","978b6b15":"#### Checking for Null Values","1efc80d6":"<img src=\"https:\/\/static.packt-cdn.com\/products\/9781838555078\/graphics\/C13314_06_05.jpg\"\/>&nbsp;&nbsp;&nbsp;&nbsp;\n\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/04\/Equation_Accuracy.png\"\/>&nbsp;&nbsp;&nbsp;&nbsp;\n* Accuracy is defined as the ratio of correctly predicted examples by the total examples.\n* Remember, accuracy is a very useful metric when all the classes are equally important. But this might not be the case if we are predicting if a patient has Liver Cancer. In this example, we can probably tolerate FPs but not FNs.\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/04\/Confusion-matrix_Precision.png\"\/>&nbsp;&nbsp;&nbsp;&nbsp;\n* Precision tells us how many of the correctly predicted cases actually turned out to be positive.\n* Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.\n* Precision is important in music or video recommendation systems, e-commerce websites, etc. Wrong results could lead to customer churn and be harmful to the business.\n\n<img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/04\/Confusion-matrix_Recall.png\"\/>&nbsp;&nbsp;&nbsp;&nbsp;\n* Recall tells us how many of the actual positive cases we were able to predict correctly with our model.\n* Recall is a useful metric in cases where False Negative trumps False Positive.\n* Recall is important in medical cases where it doesn\u2019t matter whether we raise a false alarm but the actual positive cases should not go undetected!","39ac0e74":"Looking for model accuracy when model is treated with unbalanced dataset","c3618bc5":"Male consuming more protients then Female. And also there low number of Female in dataset","79902d4c":"## Importing data","3d7a1272":"Lets look for Total Bilirubin and Direct Bilirubin\n* Normal results for a total bilirubin test are 1.2 milligrams per deciliter (mg\/dL) for adults and usually 1 mg\/dL for those under 18.\n* Normal results for direct bilirubin are generally 0.3 mg\/dL.\n\n*https:\/\/www.mayoclinic.org\/tests-procedures\/bilirubin\/about\/pac-20393041#:~:text=Normal%20results%20for%20a%20total,are%20generally%200.3%20mg%2FdL.*\n","aa9fa1dc":"Here we can conclude that,\n* Total_Bilirubin and Direct_Bilirubin are highly colorelated also, Alamine_Aminotransferase and Aspartate_Aminotransferase and Total_Protiens and Albumin.\n* We can delete one features to increase the model training speed and accuracy\n*But this not always true.For reference look into this\nhttps:\/\/datascience.stackexchange.com\/questions\/24452\/in-supervised-learning-why-is-it-bad-to-have-correlated-features","6021a4d6":"#### 3.SMOTE(Synthetic Minority Oversampling Techinque)","166c9725":"Looks for column containing the Null Values","da3dc79f":"Lets see by Age Category in datset","ba96924e":"#### Looking for data description","37cb839b":"Here also we could see that the datasets contains huge outliers as Normal Direct Bilirubin must contains data around 0.3 but here data shows max upto 20, which are False or huge outliers.","081cd1a0":"### Buliding our models","94f9c353":"Upvote if you like, Feedback and suggestions are always welcome\ud83d\ude0a","23be4a86":"## Importing the Relevant Library","024facc6":"#### Looking for Correlation between features","963dbe60":"Here we could see that the datasets contains huge outliers as Normal Total Bilirubin must contains data around 1.2 but here data shows max upto 75, which are false or huge outliers","14309f94":"#### Scaling the data"}}