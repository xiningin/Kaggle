{"cell_type":{"59ae94b1":"code","c691c944":"code","a7fd32aa":"code","a8770afa":"code","95b63714":"code","3bd0f84c":"code","edb9d71b":"code","edc96d74":"code","24da8fdb":"code","6f90978c":"code","80970f5d":"code","20d80231":"code","31d77ddd":"code","44776d56":"code","c60b3c6e":"code","4d7cebcd":"code","c6298a78":"code","4f29b145":"code","45b081ce":"code","3d282a90":"code","21eee9e6":"code","4f290013":"code","222a7191":"code","15017272":"code","03015201":"code","fb7bf808":"markdown","1969e282":"markdown","acfac275":"markdown","fb883b25":"markdown","42c70d35":"markdown","4b9b6227":"markdown","1be04cd7":"markdown","0ebfa681":"markdown","1591f55f":"markdown","0df9d165":"markdown","18f0d772":"markdown","3256ec9e":"markdown","75aade0e":"markdown","c4b861e8":"markdown","b715fef4":"markdown","ee477b91":"markdown","b2800cf1":"markdown","a37b1203":"markdown","b29b104e":"markdown","fe5c4d3c":"markdown","6136dd72":"markdown","b251f141":"markdown","40588698":"markdown","523620e0":"markdown"},"source":{"59ae94b1":"import pandas as pd\nimport numpy as np\nimport pickle\nimport datetime\nimport math\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom keras.layers import Input,Dense\nfrom keras.models import Model\nfrom keras.optimizers import Nadam\nfrom keras.callbacks import ModelCheckpoint","c691c944":"df = pd.read_csv('..\/input\/renfe.csv')\ndf.head()","a7fd32aa":"df.isna().sum() #Number of null values in different columns","a8770afa":"df.dropna(inplace=True) #Remove all the rows containing nan","95b63714":"df.drop([\"Unnamed: 0\"], axis = 1, inplace = True)\ndf.drop([\"insert_date\"], axis = 1, inplace = True) #These values doesn't effect\ndf.head()","3bd0f84c":"df[\"origin\"].value_counts().plot(kind = \"bar\") #Origin places distribution","edb9d71b":"df[\"destination\"].value_counts().plot(kind = \"bar\")","edc96d74":"df['destination'].replace([\"MADRID\",\"BARCELONA\",'SEVILLA','VALENCIA','PONFERRADA'],[0,1,2,3,4], inplace = True) #Maps to numerical values\ndf['origin'].replace([\"MADRID\",\"BARCELONA\",'SEVILLA','VALENCIA','PONFERRADA'],[0,1,2,3,4], inplace = True)","24da8fdb":"df[\"train_type\"].value_counts().plot(kind = \"bar\")\nk = df[\"train_type\"].unique()\nl = [x for x in range(len(k))]\nprint(\"Numbers used to encode different train types\",l) #numbers used to encode different trains\ndf['train_type'].replace(k,l, inplace = True)","6f90978c":"df[\"train_class\"].value_counts().plot(kind = \"bar\")#Plotting for train classes\nk = df[\"train_class\"].unique()\nl = [x for x in range(len(k))]\nprint(\"Numbers used to encode different train classes:\",l)\ndf['train_class'].replace(k,l, inplace = True)","80970f5d":"df[\"fare\"].value_counts().plot(kind = \"bar\") #Plotting for fare types\nk = df[\"fare\"].unique()\nl = [x for x in range(len(k))]\nprint(\"Numbers used to encode different fare classes:\",l)\ndf['fare'].replace(k,l, inplace = True)","20d80231":"f,ax = plt.subplots(figsize=(6, 6))\nsns.heatmap(df.corr(), annot=True, cmap = \"Blues\", linewidths=.5, fmt= '.2f',ax = ax)\nplt.show()","31d77ddd":"start = df['start_date'].values\nend = df['end_date'].values\ndatetimeFormat = '%Y-%m-%d %H:%M:%S'\nduration = []\nfor i in range(len(start)):\n    diff = datetime.datetime.strptime(end[i], datetimeFormat)- datetime.datetime.strptime(start[i], datetimeFormat)\n    duration.append(diff.seconds)\ndf['duration'] = duration","44776d56":"start_weekdays = []\nfor i in start:\n    start_weekdays.append(datetime.datetime.strptime(i,datetimeFormat).weekday())\nend_weekdays = []\nfor i in end:\n    end_weekdays.append(datetime.datetime.strptime(i,datetimeFormat).weekday())\ndf['start_weekday'] = start_weekdays\ndf['end_weekday'] = end_weekdays","c60b3c6e":"#Converting datetime to cyclic features for departure times\nhr_cos = [] #hr_cos,hr_sin,min_cos,min_sin\nhr_sin = []\nmin_cos = []\nmin_sin = []\ndata = df['start_date'].values\nfor i in range(len(data)):\n    time_obj = datetime.datetime.strptime(data[i],'%Y-%m-%d %H:%M:%S')\n    hr = time_obj.hour\n    minute = time_obj.minute\n    sample_hr_sin = math.sin(hr*(2.*math.pi\/24))\n    sample_hr_cos = math.cos(hr*(2.*math.pi\/24))\n    sample_min_sin = math.sin(minute*(2.*math.pi\/60))\n    sample_min_cos = math.cos(minute*(2.*math.pi\/60))\n    hr_cos.append(sample_hr_cos)\n    hr_sin.append(sample_hr_sin)\n    min_cos.append(sample_min_cos)\n    min_sin.append(sample_min_sin)\ndf['depart_time_hr_sin'] = hr_sin\ndf['depart_time_hr_cos'] = hr_cos\ndf['depart_time_min_sin'] = min_sin\ndf['depart_time_min_cos'] = min_cos\n#Converting datetime to cyclic features for arrival times\nhr_cos = [] #hr_cos,hr_sin,min_cos,min_sin\nhr_sin = []\nmin_cos = []\nmin_sin = []\ndata = df['end_date'].values\nfor i in range(len(data)):\n    time_obj = datetime.datetime.strptime(data[i],'%Y-%m-%d %H:%M:%S')\n    hr = time_obj.hour\n    minute = time_obj.minute\n    sample_hr_sin = math.sin(hr*(2.*math.pi\/24))\n    sample_hr_cos = math.cos(hr*(2.*math.pi\/24))\n    sample_min_sin = math.sin(minute*(2.*math.pi\/60))\n    sample_min_cos = math.cos(minute*(2.*math.pi\/60))\n    hr_cos.append(sample_hr_cos)\n    hr_sin.append(sample_hr_sin)\n    min_cos.append(sample_min_cos)\n    min_sin.append(sample_min_sin)\ndf['arrival_time_hr_sin'] = hr_sin\ndf['arrival_time_hr_cos'] = hr_cos\ndf['arrival_time_min_sin'] = min_sin\ndf['arrival_time_min_cos'] = min_cos\n","4d7cebcd":"df.drop([\"start_date\"], axis = 1, inplace = True)\ndf.drop([\"end_date\"], axis = 1, inplace = True)","c6298a78":"f,ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(df.corr(), annot=True, cmap = \"Greens\", linewidths=.5, fmt= '.2f',ax = ax)\nplt.show()","4f29b145":"df.head()#Just checking to see if all worked as per our logic","45b081ce":"places_sc = MinMaxScaler(copy=False)\ntrain_type_sc = MinMaxScaler(copy=False)\ntrain_class_sc = MinMaxScaler(copy=False)\nfare_sc = MinMaxScaler(copy=False)\nweekday_sc = MinMaxScaler(copy=False)\nduration_sc = MinMaxScaler(copy=False)\nprice_sc = MinMaxScaler(copy=False)\ndf['origin'] = places_sc.fit_transform(df['origin'].values.reshape(-1,1))\ndf['destination'] = places_sc.fit_transform(df['destination'].values.reshape(-1,1))\ndf['train_type'] = train_type_sc.fit_transform(df['train_type'].values.reshape(-1,1))\ndf['train_class'] = train_class_sc.fit_transform(df['train_class'].values.reshape(-1,1))\ndf['fare'] = fare_sc.fit_transform(df['fare'].values.reshape(-1,1))\ndf['start_weekday'] = weekday_sc.fit_transform(df['start_weekday'].values.reshape(-1,1))\ndf['end_weekday'] = weekday_sc.fit_transform(df['end_weekday'].values.reshape(-1,1))\ndf['duration'] = duration_sc.fit_transform(df['duration'].values.reshape(-1,1))\ndf['price'] = price_sc.fit_transform(df['price'].values.reshape(-1,1))","3d282a90":"df.head()","21eee9e6":"data = df.values\nY = data[:,3]\nX = np.delete(data,3,1)\n#Creating different data splits for training,validation and testing!\nx_train = X[:2223708]\ny_train = Y[:2223708]\nx_validation = X[2223708:2246398]\ny_validation = Y[2223708:2246398]\nx_test = X[2246398:]\ny_test = Y[2246398:]","4f290013":"input_layer = Input((X.shape[1],))\ny = Dense(64,kernel_initializer='he_normal',activation='tanh')(input_layer)\ny = Dense(8,kernel_initializer='he_normal',activation='sigmoid')(y)\ny = Dense(1,kernel_initializer='he_normal',activation='sigmoid')(y)\ny = Dense(1,kernel_initializer='he_normal',activation='tanh')(y)\nmodel = Model(inputs=input_layer,outputs=y)\nmodel.compile(Nadam(),loss='mse')\nmodel.summary()","222a7191":"history = model.fit(x_train,y_train,validation_data=(x_validation,y_validation),epochs = 100,batch_size=2048,callbacks=[ModelCheckpoint('best_model.hdf5',monitor='val_loss',mode='min')])","15017272":"plt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend()\nplt.show()","03015201":"model.load_weights('best_model.hdf5') #this will load the best model that we saved earlier\nscores = model.evaluate(x_test,y_test)\nprint(\"Test Set  RMSE(before scaling ):\",scores)\npred = model.predict(x_test)\ny_test = y_test.reshape(22692,1)\nk = y_test-pred\nk = price_sc.inverse_transform(k)\nrmse = np.sqrt(np.mean(np.square((k))))\nprint('Test Set RMSE(after scaling) :',rmse)","fb7bf808":"As I conclude our story ,I should say from that little model to get this much low error is much of an awe to me! \nThis is the best I am able to come up with for now! but who knows maybe you guys can get inspirations and ideas from my work and create a better model or better features and beat my score!!! I am happy to help if you have any doubts in my code !\n\n\nPlease do comment your thoughts and ideas and if you feel this was upto the mark then do Up vote this kernel on top right corner and let other's know about this !\n\nThanks for your time! :-)","1969e282":"### Conclusion and Results\nWe have come to the final part of this story!  \n\n\nNow we will load the best model that we achieved during training that is the one with lowest error on validation set of our data and use that to predict the prices for test test then we see how much root mean squared error we make on price after we inverse scale the prediction to euros.  ","acfac275":"#### 2.Extracting Weekdays information\nThe python datetime module also gives you support to get which day of the week in the following way!","fb883b25":"### Plot the training results","42c70d35":"### Let the training begins!","4b9b6227":"### Feature Engineering \nWe have start_date and end_date. Think about it ,If we are able to find the duration of the journey then it should have better relation to price that we are trying to predict! Also the tickets booked from Monday to Friday (working days) and Tickets booked in (Saturdays and Sundays) can have different influence on the price too. Now the departure and arrival times are given in string and if we convert it to cyclic features that have the knowledge that hour 23 is nearer to hour 1 and similary with minutes.\n\nLet's create all these extract features.","1be04cd7":"### Encoding Categorical Values ","0ebfa681":"It seems the heatmap is not as dense as we require we can infact squeeze out more information out of the dataset to create new features and refine existing features!","1591f55f":"#### Structure\nMy model has 3 hidden layers and 1 output layer, all layers are initialized Xavier initialization.\nI have tried out different layer structures and activation functions as a result I find the below configuration to be the best!\nI am using Nestrov Adam Optimizer and loss function is Mean Squared Error  \n\n* [Nestrov + Adam](http:\/\/cs229.stanford.edu\/proj2015\/054_report.pdf)\n* [Xavier and He Normal (He-et-al) Initialization](https:\/\/medium.com\/@prateekvishnu\/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528) \n\n","0df9d165":"#### 3.Extracting hour and minutes in cyclic forms \nWe have date and time given here in the string format but before passing it into Neural Network(NN) we have to encode it , also we need to factor in the idea that 23rd hour is closer to 1 hour (the cyclic nature of time).I have used a unit cirle of radius 1 unit to encode the time in it's points on the circumference!\n\nSo a particular time is encoded as the x and y coordinates of that time in the circle.  \n***\nThe formulas used to convert are\n\n  \n  y = sin(hr*pi\/24)  \n  \n  x = cos(hr*pi\/24)\n***\n\nThe features will look similar to this picture\n![Cyclic Time Representation](https:\/\/i.stack.imgur.com\/MkVNg.png)\nSimilary we can convert for day,date and month and even year.\n\nWe do it for both departure and arrival times.","18f0d772":"### Scaling features \nBefore sending in the input if we scale it then it will be easier for the model and simpler model can capture the pattern in the data quickly,I am using MinMaxScaler provided by sklearn here (It showed more faster and accurate convergence than StandardScaler)","3256ec9e":"Let's do the same with train_class,train_type and fare","75aade0e":"For prediciting the price of a ticket it doesn't depend on the insert_date (time of scrapping) and that unamed column at first,so let's remove unwanted noise from our data","c4b861e8":"### Model\nNow let's convert the pandas datafram into numpy arrays and let's create the model using keras and train it !","b715fef4":"### Remove Redundant Features\nAs we already extracted required information from the start_date and end_date, which currently only has the month and year of the journey which is actually doesn't matter for the price so we remove these features from data.Also no need in having information about year and month in start_date and end_date as the dataset doesn't contain all the months and different years.","ee477b91":"### Visualise Again!\nNow we have structured and squeezed out the important features from the data,Let's see how does it correlate now","b2800cf1":"### Let's load and see the data\nFirst of all we need to need the data and get an understanding about it.","a37b1203":"As we can see now the heatmap show better correlations and it's comparetively more dense than the previous one,it's also crowded towards the top left corner.","b29b104e":"We need to remove the Nan (None or data not available entries) from the data, before that we need to see if there is any ?!\n","fe5c4d3c":"### Visualization\nOkay, enough of graphs! now let's see how are different features related to each other using the pandas inbuilt function corr() and seaborn to plot it as a heatmap","6136dd72":"#### 1.Extracting Duration of Journey\nWe have start_date and end_date so just subtracting them after converting into python datetime format will just do it!","b251f141":"It seems like both origin and destination of trains are pretty much distributed the same and trains along MADRID is highest in our dataset.\n\nhmm... Okay let's encode these columns to numerical values","40588698":"### Plotting Time!!!","523620e0":"Nice!!!! now the data is cleaned of unwanted values and nan values, but still we can't pass it to our neural network model as it still has lots of categorical strings like origin,destination,train_type etc which must be converted to numerical values.\n\nBefore that let's plot some graphs and understand how the data is distributed and which category is having more data and as such.\n\nLet's do that!"}}