{"cell_type":{"96d74d2a":"code","82353270":"code","a25b9377":"code","ae2b8a69":"code","114b2645":"code","a9f98a38":"code","01beedf1":"code","5d5fa24d":"code","3db8e899":"code","a0066c44":"code","721940af":"code","fbac3965":"code","f6135d10":"code","ad5476fa":"code","395122d4":"code","b8bdcc76":"code","9b3bc399":"code","92450b5d":"code","94d67ae1":"code","0aa939bd":"code","64207d2d":"code","862b26b6":"code","26a5ae7f":"code","63ac4c7f":"code","980f6db5":"code","ac774c53":"code","8d4a9326":"code","cb79d69e":"code","113021b4":"code","43b58b1c":"code","4d214d03":"markdown","7f6d250c":"markdown","2f4bb74b":"markdown","0c225c45":"markdown","f18daf98":"markdown"},"source":{"96d74d2a":"import requests\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sequences\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","82353270":"response = requests.get('https:\/\/ocw.mit.edu\/ans7870\/6\/6.006\/s08\/lecturenotes\/files\/t8.shakespeare.txt')","a25b9377":"response.text[:1500]","ae2b8a69":"data=response.text.lower().split(\"\\n\")\ndata[0]","114b2645":"len(data)","a9f98a38":"data = \" \".join(data)\ndata[:1000]","01beedf1":"print(string.punctuation)","5d5fa24d":"def clean_text(file):\n  tokens = file.split()#Split by Whitespace\n  table = str.maketrans(' ', ' ', string.punctuation)#We can use the function maketrans() to create a mapping table\n  tokens = [w.translate(table) for w in tokens]#Python offers a function called translate() that will map one set of characters to another.\n  tokens = [word for word in tokens if word.isalpha()]#The isalpha() method returns True if all the characters are alphabet letters (a-z).\n  return tokens\n\ntokens = clean_text(data)\nprint(tokens[:50])","3db8e899":"len(tokens)","a0066c44":"len(set(tokens))#total no words","721940af":"#here we predict the word 51th postion using 50th position\n#divide our data in chunks of 51 words and at the last we will separate the last word from every line\nlength_sentence = 50 + 1\nlines = []\nfor i in range(length_sentence, len(tokens)):\n  seq = tokens[i-length_sentence:i]\n  line = ' '.join(seq)\n  lines.append(line)\n  if i > 200000:#limit our dataset to 200000 words.\n    break\n\nprint(len(lines))","fbac3965":"lines[0]","f6135d10":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(lines)\nsequences = tokenizer.texts_to_sequences(lines)#texts_to_sequences() transforms each text in texts to a sequence of integers.","ad5476fa":"sequences = np.array(sequences)","395122d4":"sequences [:100]","b8bdcc76":"sequences = np.array(sequences)\nx, y = sequences[:, :-1], sequences[:,-1]","9b3bc399":"x[0]","92450b5d":"#tokenizer.word_index gives the mapping of each unique word to its numerical equivalent.\n#tokenizer.word_index gives the vocab_size.\nvocab_size = len(tokenizer.word_index) + 1","94d67ae1":"vocab_size","0aa939bd":"y = to_categorical(y, num_classes=vocab_size)","64207d2d":"y","862b26b6":"seq_length = x.shape[1]\nseq_length","26a5ae7f":"model = Sequential()\n# Add Input Embedding Layer\nmodel.add(Embedding(vocab_size, 50, input_length=seq_length))\n# Add Hidden Layer 1 LSTM Layer\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(LSTM(100))\n#Add output layers\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(vocab_size, activation='softmax'))","63ac4c7f":"model.summary()","980f6db5":"model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","ac774c53":"history=model.fit(x, y, batch_size = 256, epochs = 5)","8d4a9326":"#plotting the training accuracy of the model\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'])\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel('Training accuracy per epochs')\nplt.show()","cb79d69e":"seed_text=lines[100]\nseed_text","113021b4":"def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n  text = []\n\n  for _ in range(n_words):\n    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n    encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n\n    y_predict = model.predict_classes(encoded)\n\n    predicted_word = ''\n    for word, index in tokenizer.word_index.items():\n      if index == y_predict:\n        predicted_word = word\n        break\n    seed_text = seed_text + ' ' + predicted_word\n    text.append(predicted_word)\n  return ' '.join(text)","43b58b1c":"generate_text_seq(model, tokenizer, seq_length, seed_text, 10)","4d214d03":"# # Now we will split each line such that the first 50 words are in X and the last word is in y.","7f6d250c":"# if we increase epochs the accuracy increase","2f4bb74b":"#  Generating the text\nGreat, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.","0c225c45":"# import library","f18daf98":"# Dataset preparation\n\nDataset cleaning\n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words.\n\nConvert the text to the lower case and the remove the data from \\n"}}