{"cell_type":{"8c7547d0":"code","61cd6b25":"code","49c02c35":"code","8acfc801":"code","bbdeac49":"code","db3975e9":"code","b05fed1a":"code","01c29abf":"code","841908dc":"code","a8cdc400":"code","ff4c3ed9":"code","195f79b8":"code","33cfe61e":"code","3b171519":"code","c194d1b9":"code","2ec7cd4b":"code","0f050d9c":"code","5e5c6005":"code","f0b3a960":"code","cd2844cf":"code","a90d10e4":"code","2ca010dd":"code","7e690e96":"code","52dd51ac":"code","f958fabd":"code","035ebcbf":"code","e5a75315":"code","bb59b557":"code","7c957037":"code","88279456":"code","f38b792a":"code","95c9094f":"code","717e6240":"code","dc81e442":"code","aa1137ca":"code","5891201f":"code","37d4d629":"code","16430947":"code","b08fddac":"code","e42ecf0a":"code","bf00836f":"code","34c34c2d":"code","f7b4a60c":"code","3728682a":"code","0d894f65":"code","cefa0d26":"code","27a999b7":"code","5f27e84d":"code","893c2385":"code","e86b0dc4":"code","8a6448d1":"code","86346e98":"code","018ba158":"code","47caedd4":"code","87512f62":"code","7fe2a81c":"code","34ab9963":"code","ec9a848f":"code","0d151a89":"code","ad7fdeec":"code","ec630420":"code","50c69832":"code","e4bb980d":"code","da521bf5":"code","e0644aa0":"code","dcfb3fa1":"code","76505251":"code","db909483":"code","6d219179":"code","97c5688f":"code","c143db7c":"code","2822169d":"code","8e581d16":"code","29b027a1":"code","552cd516":"code","338b3912":"code","4e2b5a1d":"code","d7b1dc16":"code","ce406aa7":"code","21115f0d":"code","2db44c1c":"code","f4c8c431":"code","c0031204":"code","bedc1c22":"code","5b04767e":"code","22b9301a":"code","3f92c645":"code","fa2cfb5d":"code","cf873bb1":"code","0f98cc54":"code","a1a10ad4":"code","19d6f2a4":"code","a1d61d4b":"code","575ac1ee":"code","9e9d770b":"code","3229a1d1":"code","7c695d82":"code","afe9291c":"code","e97b3223":"code","e8e2bdb0":"code","9473e0d0":"code","a0293beb":"code","1ffde6ae":"code","1aebec45":"code","90b07a4c":"code","0c4d4e83":"code","d4ec8077":"code","e84bcc21":"code","11683b06":"code","89965887":"code","9a63fcbe":"code","e005caee":"code","02db373e":"code","9773e9de":"code","d7be2071":"code","43173b38":"code","c71d5bb0":"code","af99a8af":"code","d70b58e3":"code","5a593859":"code","b59c4b8b":"code","6a9040fe":"code","0e0f7069":"code","b27783e6":"code","0325af1e":"code","de86e48e":"code","93d869b6":"code","263d4a81":"code","2009083c":"code","ba7c9c1f":"code","0514729c":"code","993d0147":"code","e9f0ac07":"code","68cb683c":"code","2fed5715":"code","c23a17c1":"code","fdb50cba":"code","38718664":"code","3f90b6b4":"code","6a0f6eda":"code","dfedfc22":"code","74502b1b":"code","75424e4a":"code","31bb90e2":"code","7f3a4fb1":"code","86611bc2":"code","db5ebe13":"code","314ce648":"code","cea9f985":"code","376018ec":"code","448be9bb":"code","9bf3348d":"code","39fb2877":"code","64035917":"code","3c2e09b0":"code","4512d444":"code","e1ff4c04":"code","78382563":"code","4bb89a7a":"code","f444cb66":"code","5b9c0e37":"code","220aa337":"code","10ef7ca0":"code","a479bedc":"code","2bf2c013":"code","9f5d1898":"code","c245dd74":"code","6b0ce5a1":"code","e6132944":"code","0a8cdf37":"code","c7c87af4":"code","ac7243b8":"code","d6057b06":"code","34d64d54":"code","38a0e22c":"code","35c9a56a":"code","9c8fc842":"code","26f36023":"code","b2514b64":"code","31742146":"code","962ef386":"code","c2dff426":"code","446c9f41":"code","2d9f12c9":"code","82bfbc0e":"code","0c76876a":"code","caa6cca1":"code","192a0a9f":"markdown","a002f9e2":"markdown","28701ae2":"markdown","9bdf0805":"markdown","4164390a":"markdown","bf06ef46":"markdown","2724793c":"markdown","0d6a4e41":"markdown","eff7081b":"markdown","7b793f29":"markdown","7ec5c07e":"markdown","7eeb0df6":"markdown","ede1c31c":"markdown","ad8e03b4":"markdown","6b180a8f":"markdown","b0a72f1e":"markdown","3b98fcb2":"markdown","ef1a77d5":"markdown","c6434833":"markdown","5ee5138d":"markdown","e66b048c":"markdown","8ed5b3f7":"markdown","137ac7ad":"markdown","e0317eeb":"markdown","7771933e":"markdown","54c23b5e":"markdown","541e3dbe":"markdown","dd2a5c26":"markdown","17a3c7b2":"markdown","df2fea01":"markdown","aee73b89":"markdown","e51559d6":"markdown","cbdfa655":"markdown","1c42ba91":"markdown","46068030":"markdown","9f8b064c":"markdown","b16b4d8e":"markdown","3f8787e8":"markdown","f51541db":"markdown","1b432d0b":"markdown","b1cfd5e9":"markdown","9377a9c8":"markdown","5bbc2dde":"markdown","e1fba12b":"markdown","86b2c62c":"markdown","a4c1db1c":"markdown","96a91d2d":"markdown","f949adda":"markdown","3c34bd27":"markdown","741ef209":"markdown","c15125ea":"markdown","20528ae7":"markdown","0efaf93d":"markdown","ac381de5":"markdown","bfc5e34d":"markdown","4d9aab87":"markdown","a327ab5b":"markdown","04e2ca7f":"markdown","1cbf2589":"markdown","042f5bfc":"markdown","5891e155":"markdown","04db690e":"markdown","104d3fea":"markdown","fec6eae8":"markdown","3637490c":"markdown","aec8f12e":"markdown"},"source":{"8c7547d0":"!pip install nlpaug","61cd6b25":"# tool box\n\nimport numpy as np\nimport pandas as pd\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport matplotlib.gridspec as gridspec\nfrom mpl_toolkits.basemap import Basemap\nimport plotly.express as px\n\nimport IPython.display as ipd  # To play sound in the notebook\nimport librosa\nimport librosa.display\nimport sklearn\nimport librosa.display as librosa_display\nimport nlpaug\nimport nlpaug.augmenter.audio as naa\n\nimport os\nfrom PIL import Image\nimport pathlib\nimport csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler\nimport keras\nfrom keras import layers\nimport random\nfrom keras.models import Sequential\nfrom tqdm import tqdm\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\nimport warnings\nwarnings.filterwarnings('ignore')","49c02c35":"# General Settings\n\n# display all the columns in the dataset\npd.pandas.set_option('display.max_columns', None)\n\n# Setting color palette.\npurple_black = [\n\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"\n]\n\n# Setting plot styling.\n#plt.style.use('ggplot')\nplt.style.use('fivethirtyeight')","8acfc801":"base_path='..\/input\/birdsong-recognition\/'\naudio_path=base_path+'train_audio\/'","bbdeac49":"# training dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/birdsong-recognition\/train.csv\")\ntrain.shape","db3975e9":"# lets inspect first few rows of the dataset\ntrain.head()","b05fed1a":"# check null values\ntrain.isnull().sum().sort_values(ascending = False)[train.isnull().sum()!=0]","01c29abf":"# visualize missing values:\nplt.figure(constrained_layout=True, figsize=(12, 8))\npercent = (train.isnull().sum().sort_values(ascending=False) \/ len(train) *\n           100)[(train.isnull().sum().sort_values(ascending=False) \/ len(train) *\n                 100) != 0]\n\nmissing = pd.DataFrame({\"missing%\":percent})\n\nsns.barplot(x=missing.index,\n            y='missing%',\n            data=missing,\n            palette=purple_black)\nplt.title('Train Data Missing Values')","841908dc":"test = pd.read_csv(\"\/kaggle\/input\/birdsong-recognition\/test.csv\")\ntest.shape","a8cdc400":"test","ff4c3ed9":"# no of unique classes(birds) in the dataset\nprint(\"dataset has\",train.species.nunique(),\"unique bird's species\")","195f79b8":"# count wise distribution of bird's species\ncount = train.species.value_counts().sort_values(ascending = False)\ncount","33cfe61e":"# lets visualize class distribution in the dataset\nfig = px.pie(count,\n             values=count.values,\n             names=count.index,\n             color_discrete_sequence=purple_black,\n             hole=.4)\nfig.update_traces(textinfo='percent', pull=0.05)\nfig.show()","3b171519":"# country\nprint(\"training dataset has data from\",train.country.nunique(),\"unique countries\")","c194d1b9":"# lets visualize top 10 countries\nplt.figure(constrained_layout=True, figsize=(16, 8))\nsns.countplot(train.country,\n              alpha=0.9,              \n              palette=purple_black,\n              order = train.country.value_counts().sort_values(ascending=False).iloc[:10].index,)\nplt.xlabel(\"Country\")\nplt.ylabel(\"Count\")\nplt.title(\"Country wise Distribution\")\nplt.show()","2ec7cd4b":"# world shape file\nworld_map = gpd.read_file(\"..\/input\/worldshapefile\/world_shapefile.shp\")\n\n# Coordinate reference system\ncrs = {\"init\" : \"epsg:4326\"}","0f050d9c":"# let's filter out \"not specified\" values\ndf = train[train[\"latitude\"] != \"Not specified\"]\n\n# convert latitude and longitute to float variables\ndf[\"latitude\"] = df[\"latitude\"].astype(float)\ndf[\"longitude\"] = df[\"longitude\"].astype(float)","5e5c6005":"# create geometric list\ngeometry = [Point(xy) for xy in zip(df[\"longitude\"], df[\"latitude\"])]\n\n# create geography dataframe\ngeo = gpd.GeoDataFrame(df, crs=crs, geometry=geometry)\n\n# Create ID for species\nspecies = geo[\"species\"].value_counts().reset_index()\nspecies.insert(0, 'ID', range(0, 0 + len(species)))\n\nspecies.columns = [\"ID\", \"species\", \"count\"]\n\n# merge the dataframes\ngeo = pd.merge(geo, species, how=\"left\", on=\"species\")","f0b3a960":"# visualize bird's on the world map!\nfig, ax = plt.subplots(figsize = (20, 9))\nworld_map.plot(ax=ax, alpha=0.4, color=\"blue\")\n\npalette = iter(sns.hls_palette(len(species)))\n\nfor i in range(264):\n    geo[geo[\"ID\"] == i].plot(ax=ax, markersize=30, color=next(palette), marker=\"o\");\n    \nplt.title(\"These colorful small circles are our birds :-)\")","cd2844cf":"# check the date format\ntrain.date.head()","a90d10e4":"# lets pull year from the given date\n\ntrain['year'] = train['date'].apply(lambda x: x.split('-')[0])\n\n# lets visualize year wise distribution\n\nfig = plt.figure(constrained_layout=True, figsize=(20,8))\n\nsns.countplot(train.year,             \n              alpha=0.9,              \n              palette=purple_black,           \n              order = train.year.value_counts().sort_values(ascending=False).iloc[:15].index   \n             )\nplt.xlabel(\"Year\")\nplt.ylabel(\"Count\")\nplt.title('Year-Wise Distribution')\n\nplt.show()","2ca010dd":"# lets pull month from the date\ntrain['month'] = train['date'].apply(lambda x: x.split('-')[1])\n\n# lets visualize month wise distribution\n\nfig = plt.figure(constrained_layout=True, figsize=(20,8))\n\nsns.countplot(train.month,             \n              alpha=0.9,              \n              palette=purple_black,           \n              order = train.month.value_counts().sort_values(ascending=False).index   \n             )\nplt.xlabel(\"Month\")\nplt.ylabel(\"Count\")\nplt.title('Month-Wise Distribution')\nplt.show()","7e690e96":"print(\"There are\",train.ebird_code.nunique(),\"ebird codes in the dataset\")\nprint(\"training dataset has\",train.sci_name.nunique(),\"unique sci_names\")","52dd51ac":"# lets visualize ebird code & sci_name\n\nfig = plt.figure(constrained_layout=True, figsize=(20,8))\n\ngrid = gridspec.GridSpec(ncols=4, nrows=1, figure=fig)\n\nax1 = fig.add_subplot(grid[0, :2])\nsns.countplot(train.ebird_code,             \n              alpha=0.9,\n              ax=ax1,\n              palette=purple_black,           \n              order = train.ebird_code.value_counts().sort_values(ascending = False).iloc[:15].index   \n             )\nplt.xlabel(\"Ebird Code\")\nplt.ylabel(\"Count\")\nplt.title('Ebird Code Distribution')\nplt.xticks(rotation=30)\n\nax2 = fig.add_subplot(grid[0, 2:4])\nsns.countplot(train.sci_name,             \n              alpha=0.9,\n              ax=ax2,\n              palette=purple_black,           \n              order = train.sci_name.value_counts().sort_values(ascending = False).iloc[:15].index   \n             )\nplt.xlabel(\"Scientific Name\")\nplt.ylabel(\"Count\")\nplt.title('Scientific Name Distribution')\nplt.xticks(rotation=30)\nplt.show()","f958fabd":"# lets check the no. of unique values for the field filename\nprint(\"training dataset has\",train.filename.nunique(),\"unique filenames\")\nprint(\"training dataset has\",train.title.nunique(),\"unique titles\")\nprint(\"training dataset has\",train.description.nunique(),\"unique descriptions\")\nprint(\"training dataset has\",train.xc_id.nunique(),\"unique xc_id\")\nprint(\"training dataset has\",train.url.nunique(),\"unique urls\")","035ebcbf":"# lets check top 3 descriptions and see how it looks like\nfig = plt.figure(constrained_layout=True, figsize=(20, 12))\nsns.countplot(train.description,\n              alpha=0.9,              \n              palette=purple_black,\n              order= train.description.value_counts().sort_values(ascending = False).iloc[:3].index)\n\nplt.xlabel(\"Description\")\nplt.ylabel(\"Count\")\nplt.title('Description Distribution')\n\nfig.show()","e5a75315":"# lets visualize top Playback used, channel & Ratings fields\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 9))\n\n# Creating a grid:\ngrid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n# playback used\nax1 = fig.add_subplot(grid[0, :2])\n\nsns.countplot(train.playback_used,\n              alpha=0.9,\n              ax=ax1,\n              order= train.playback_used.value_counts().sort_values(ascending = False).index,\n              palette=purple_black)\n\nplt.xlabel(\"Playback_Used\")\nplt.ylabel(\"Count\")\nax1.set_title('PlayBack Used Distribution')\n\n# channels.\nax2 = fig.add_subplot(grid[0, 2:])\n\n# Plot the countplot.\nsns.countplot(train.channels,\n              alpha=0.9,\n              ax=ax2,\n              order= train.channels.value_counts().sort_values(ascending = False).index,\n              palette=purple_black)\n\nplt.xlabel(\"Channels\")\nplt.ylabel(\"Count\")\nax2.set_title('Channels Distribution')\n\n# Ratings\nax3 = fig.add_subplot(grid[1, :])\n\nsns.countplot(train.rating,\n              alpha=0.9,\n              ax = ax3,\n              palette=purple_black,              \n              order= train.rating.value_counts().sort_values(ascending = False).index)\n\nplt.xlabel(\"Ratings\")\nplt.ylabel(\"Count\")\nax3.set_title('Ratings Distribution')\n\nplt.show()","bb59b557":"# lets visualize pitch, speed & no. of notes\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 9))\n# Creating a grid:\ngrid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n# pitch\nax1 = fig.add_subplot(grid[0, :2])\n\nsns.countplot(train.pitch,\n              alpha=0.9,\n              ax=ax1,\n              palette=purple_black,\n              order= train.pitch.value_counts().sort_values(ascending = False).index)\nplt.xlabel(\"Pitch\")\nplt.ylabel(\"Count\")\nax1.set_title('Pitch Distribution')\n\n\n\n# speed\nax2 = fig.add_subplot(grid[0, 2:])\n\n# Plot the countplot.\nsns.countplot(train.speed,\n              alpha=0.9,\n              ax=ax2,\n              palette=purple_black,\n              order= train.speed.value_counts().sort_values(ascending = False).index)\n\nplt.xlabel(\"Speed\")\nplt.ylabel(\"Count\")\nax2.set_title('Speed Distribution')\n\n# number_of_notes\nax3 = fig.add_subplot(grid[1, :])\n\nsns.countplot(train.number_of_notes,\n              alpha=0.9,\n              ax=ax3,\n              palette=purple_black,\n              order= train.number_of_notes.value_counts().sort_values(ascending = False).index)\nplt.xlabel(\"Number Of Notes Distribution\")\nplt.ylabel(\"Count\")\nax3.set_title('Number Of Notes')\n\nplt.show()","7c957037":"plt.figure(constrained_layout=True, figsize=(12, 8))\nsns.distplot(train.duration,\n            color='coral')\n\nplt.xlabel(\"Duration\")\nplt.ylabel(\"Count\")\nplt.title('Duration Distribution')\n","88279456":"# lets check the no. of unique values for the field primary & secondary labels\nprint(\"training dataset has\",train.primary_label.nunique(),\"unique primary labels\")\n\nprint(\"training dataset has\",train.secondary_labels.nunique(),\"unique secondary labels\")","f38b792a":"# lets visualize primary labels\nplt.figure(constrained_layout=True, figsize=(12, 8))\n\ncount = train.primary_label.value_counts().sort_values(ascending = False)[:50]\n\nfig = px.pie(count,\n             values=count.values,\n             names=count.index,\n             color_discrete_sequence=purple_black,\n             hole=.4)\nfig.update_traces(textinfo='percent', pull=0.05)\n\nfig.show()","95c9094f":"# lets visualize seconary labels\nplt.figure(constrained_layout=True, figsize=(12, 8))\n\ncount = train.secondary_labels.value_counts().sort_values(ascending = False)[:20]\n\nfig = px.pie(count,\n             values=count.values,\n             names=count.index,\n             color_discrete_sequence=purple_black,\n             hole=.4)\nfig.update_traces(textinfo='percent', pull=0.05)\n\nfig.show()","717e6240":"# lets visualize bird_seen, sampling rate and type fields\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 9))\n\n# Creating a grid:\ngrid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n# playback used\nax1 = fig.add_subplot(grid[0, :2])\n\nsns.countplot(train.bird_seen,\n              alpha=0.9,\n              ax=ax1,\n              palette=purple_black,              \n              order = train.bird_seen.value_counts().sort_values(ascending = False).index)\nplt.xlabel(\"Bird Seen Distribution\")\nplt.ylabel(\"Count\")\nax1.set_title('Bird Seen')\n\n\n# sampling_rate.\nax2 = fig.add_subplot(grid[0, 2:])\n\n# Plot the countplot.\nsns.countplot(train.sampling_rate,\n              alpha=0.9,\n              ax=ax2,\n              palette=purple_black,\n              order = train.sampling_rate.value_counts().sort_values(ascending = False).index)\n\nplt.xlabel(\"Sampling Rate Distribution\")\nplt.ylabel(\"Count\")\nax2.set_title('Sampling Rate')\n\n# type              \nax3 = fig.add_subplot(grid[1, :])\n\nsns.countplot(train.type              ,\n              alpha=0.9,\n              ax = ax3,\n              palette=purple_black,           \n              order = train.type.value_counts().sort_values(ascending = False).iloc[:10].index)\n            \nplt.xlabel(\"Type Distribution\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation = 30)\nax3.set_title('Type')\n\nplt.show()","dc81e442":"# lets visualize elevation,volume,length\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 9))\n\n# Creating a grid:\ngrid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n\n# sampling_rate.\nax1 = fig.add_subplot(grid[0, :2])\n\n# Plot the countplot.\nsns.countplot(train.length,\n              alpha=0.9,\n              ax=ax1,\n              palette=purple_black,\n              order = train.length.value_counts().sort_values(ascending = False).index)\n\nplt.xlabel(\"Length Distribution\")\nplt.ylabel(\"Count\")\nax1.set_title('Length')\n\n# volume              \nax2 = fig.add_subplot(grid[0, 2:])\n\nsns.countplot(train.volume,\n              alpha=0.9,\n              ax = ax2,\n              palette=purple_black,           \n              order = train.volume.value_counts().sort_values(ascending = False).index,   \n             )\nplt.xlabel(\"Volume Distribution\")\nplt.ylabel(\"Count\")\nax2.set_title('Volume')\n\n# elevation              \nax3 = fig.add_subplot(grid[1, :])\nsns.countplot(train.elevation,\n              alpha=0.9,\n              ax = ax3,\n              palette=purple_black,           \n              order = train.elevation.value_counts().sort_values(ascending = False).iloc[:15].index,   \n             )\nplt.xlabel(\"Elevation Distribution\")\nplt.ylabel(\"Count\")\nax3.set_title('Elevation')\n\nplt.show()","aa1137ca":"# lets visualize file type, license, bitrate_of_mp3\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 9))\n\n# Creating a grid:\ngrid = gridspec.GridSpec(ncols=4, nrows=2, figure=fig)\n\n# playback used\nax1 = fig.add_subplot(grid[0, :2])\n\nsns.countplot(train.file_type,\n              alpha=0.9,\n              ax=ax1,\n              palette=purple_black,\n              order = train.file_type.value_counts().sort_values(ascending = False).index)\nplt.xlabel(\"File Type\")\nplt.ylabel(\"Count\")\nax1.set_title('File Type Distribution')\n\n\n# sampling_rate.\nax2 = fig.add_subplot(grid[0, 2:])\n\n# using short forms for liecense values\ntrain['license'] = train['license'].replace([\"Creative Commons Attribution-NonCommercial-ShareAlike 4.0\"],[\"CCA-NCSA4.0\"])\ntrain['license'] = train['license'].replace([\"Creative Commons Attribution-NonCommercial-ShareAlike 3.0\"],[\"CCA-NCSA3.0\"])\ntrain['license'] = train['license'].replace([\"Creative Commons Attribution-ShareAlike 3.0\"],[\"CCA-SA3.0\"])\ntrain['license'] = train['license'].replace([\"Creative Commons Attribution-ShareAlike 4.0\"],[\"CCA-SA4.0\"])\n                                          \n\n# Plot the countplot.\nsns.countplot(train.license,\n              alpha=0.9,\n              ax=ax2,\n              palette=purple_black,\n              order = train.license.value_counts().sort_values(ascending = False).index)\n\nplt.xlabel(\"License\")\nplt.ylabel(\"Count\")\nax2.set_title('License Distribution')\n\n# type              \nax3 = fig.add_subplot(grid[1, :])\n\nsns.countplot(train.bitrate_of_mp3,              \n              alpha=0.9,\n              ax = ax3,\n              palette=purple_black,           \n              order = train.bitrate_of_mp3.value_counts().sort_values(ascending = False).iloc[:10].index)\nplt.xlabel(\"Bitrate Of Mp3\")\nplt.ylabel(\"Count\")\nax3.set_title('Bitrate Of Mp3  Distribution')\n\nplt.show()","5891201f":"# lets visualize background\nplt.figure(constrained_layout=True, figsize=(12, 8))\n\ncount = train.background.value_counts().sort_values(ascending = False)[:20]\n\nfig = px.pie(count,\n             values=count.values,\n             names=count.index,\n             color_discrete_sequence=purple_black,\n             hole=.4)\nfig.update_traces(textinfo='percent', pull=0.05)\n\nfig.show()","37d4d629":"# lets visualize author\nplt.figure(constrained_layout=True, figsize=(12, 8))\n\ncount = train.author.value_counts().sort_values(ascending = False)[:20]\n\nfig = px.pie(count,\n             values=count.values,\n             names=count.index,\n             color_discrete_sequence=purple_black,\n             hole=.4)\nfig.update_traces(textinfo='percent', pull=0.05)\n\nfig.show()","16430947":"# lets visualize recordist\nplt.figure(constrained_layout=True, figsize=(12, 8))\n\ncount = train.recordist.value_counts().sort_values(ascending = False)[:20]\n\nfig = px.pie(count,\n             values=count.values,\n             names=count.index,\n             color_discrete_sequence=purple_black,\n             hole=.4)\nfig.update_traces(textinfo='percent', pull=0.05)\n\nfig.show()","b08fddac":"print('Minimum samples per category = ', min(train.ebird_code.value_counts()))\nprint('Maximum samples per category = ', max(train.ebird_code.value_counts()))","e42ecf0a":"perfal = '\/kaggle\/input\/birdsong-recognition\/train_audio\/perfal\/XC463087.mp3'   # Hi-hat\nipd.Audio(perfal)","bf00836f":"lotduc = '\/kaggle\/input\/birdsong-recognition\/train_audio\/lotduc\/XC121426.mp3'   # Hi-hat\nipd.Audio(lotduc)\n","34c34c2d":"rewbla = '\/kaggle\/input\/birdsong-recognition\/train_audio\/rewbla\/XC135672.mp3'   # Hi-hat\nipd.Audio(rewbla)","f7b4a60c":"warvir = '\/kaggle\/input\/birdsong-recognition\/train_audio\/warvir\/XC192521.mp3'   # Hi-hat\nipd.Audio(warvir)","3728682a":"lecthr = '\/kaggle\/input\/birdsong-recognition\/train_audio\/lecthr\/XC141435.mp3'   # Hi-hat\nipd.Audio(lecthr)","0d894f65":"def audioinfo(filename, species):   \n    # The load functions loads the audio file and converts it into an array of values which represent the amplitude if a sample at a \n    # given point of time.\n\n    data,sample_rate1 = librosa.load(filename, res_type='kaiser_best')\n\n    print(\"data:\",data,\"\\n\")\n    print(\"Sample Rate (KHz):\",sample_rate1)\n\n    # lenth of the audio\n    print('Audio Length:', np.shape(data)[0]\/sample_rate1)\n    \n    # ----------------------------------------------------------WAVE PLOT-----------------------------------------------------------\n    plt.figure(figsize=(30,20))\n    plt.subplot(3,1,1)\n    \n    # Amplitude and frequency are important parameters of the sound and are unique for each audio. \n\n    # librosa.display.waveplot is used to plot waveform of amplitude vs time where the first axis is an amplitude and second axis is time\n   \n    librosa.display.waveplot(data,sr=sample_rate1,color = 'darkblue')\n    plt.xlabel(\"Time (seconds) -->\")\n    plt.ylabel(\"Amplitude\")\n    plt.title(\"Waveplot for - \" + species)\n    \n    # --------------------------------------------------------SPECTOGRAM------------------------------------------------------------\n    plt.subplot(3,1,2)\n     # .stft converts data into short term Fourier transform. STFT converts signal such that we can know the amplitude of given \n     # frequency at a given time. Using STFT we can determine the amplitude of various frequencies playing at a given time of an audio\n     # signal. \n    X = librosa.stft(data)\n\n    Xdb = librosa.amplitude_to_db(abs(X))\n\n    #.specshow is used to display spectogram.\n    librosa.display.specshow(Xdb, sr=sample_rate1, x_axis='time', y_axis='hz',cmap = 'winter') \n\n    plt.colorbar()\n    plt.xlabel(\"Time (seconds) -->\")\n    plt.ylabel(\"Amplitude\")\n    plt.title(\"Spectogram for - \" + species)\n    \n    # ----------------------------------------------------MEL SPECTOGRAM----------------------------------------------------------\n    plt.subplot(3,1,3)\n    librosa.feature.melspectrogram(y=data, sr=sample_rate1)\n\n    D = np.abs(librosa.stft(data))**2\n    S = librosa.feature.melspectrogram(S=D)\n    S = librosa.feature.melspectrogram(y=data, sr=sample_rate1)\n\n    librosa.display.specshow(librosa.power_to_db(S,ref=np.max),x_axis='time',cmap = 'rainbow')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(\"Mel spectrogram for species - \" + species)\n    plt.xlabel(\"Time (seconds) -->\")\n    plt.ylabel(\"Amplitude\")   \n     \n    plt.show()","cefa0d26":"audioinfo('\/kaggle\/input\/birdsong-recognition\/train_audio\/perfal\/XC463087.mp3',\"perfal\")","27a999b7":"audioinfo('\/kaggle\/input\/birdsong-recognition\/train_audio\/lotduc\/XC121426.mp3',\"lotduc\")","5f27e84d":"audioinfo(\"\/kaggle\/input\/birdsong-recognition\/train_audio\/rewbla\/XC135672.mp3\",\"rewbla\")","893c2385":"audioinfo( '\/kaggle\/input\/birdsong-recognition\/train_audio\/warvir\/XC192521.mp3',\"warvir\")","e86b0dc4":"audioinfo(\"\/kaggle\/input\/birdsong-recognition\/train_audio\/lecthr\/XC141435.mp3\",\"lecthr\")","8a6448d1":"def zero_cross(filename):\n    data,sample_rate1 = librosa.load(filename)\n    # Zooming in\n    n0 = 9000\n    n1 = 9100\n    plt.figure(figsize=(20, 5))\n    plt.plot(data[n0:n1],color = \"gold\")\n    plt.grid()\n    \n    zero_crossings = librosa.zero_crossings(data, pad=False)\n    print(\"Zero Crossing Shape:\",zero_crossings.shape)\n    \n    print(\"Total Zero Crossings:\",sum(zero_crossings))","86346e98":"zero_cross(perfal)","018ba158":"zero_cross(lotduc)","47caedd4":"zero_cross(rewbla)","87512f62":"zero_cross(warvir)","7fe2a81c":"zero_cross(lecthr)","34ab9963":"def spectral_centroid(filename):\n    data,sample_rate1 = librosa.load(filename)\n    \n    spectral_centroids = librosa.feature.spectral_centroid(data, sr=sample_rate1)[0]\n    spectral_centroids.shape\n\n    # Computing the time variable for visualization\n    plt.figure(figsize=(20,5))\n    frames = range(len(spectral_centroids))\n    t = librosa.frames_to_time(frames)\n\n    # Normalising the spectral centroid for visualisation\n    def normalize(data, axis=0):\n        return sklearn.preprocessing.minmax_scale(data, axis=axis)\n\n    #Plotting the Spectral Centroid along the waveform\n    librosa.display.waveplot(data, sr=sample_rate1, alpha=0.4)\n    plt.plot(t, normalize(spectral_centroids), color='r')","ec9a848f":"spectral_centroid(perfal)","0d151a89":"spectral_centroid(lotduc)","ad7fdeec":"spectral_centroid(rewbla)","ec630420":"spectral_centroid(warvir)","50c69832":"spectral_centroid(lecthr)","e4bb980d":"def rolloff(filename):\n    data,sample_rate1 = librosa.load(filename)\n    \n    spectral_centroids = librosa.feature.spectral_centroid(data, sr=sample_rate1)[0]\n    frames = range(len(spectral_centroids))\n    t = librosa.frames_to_time(frames)\n    \n    def normalize(data, axis=0):\n        return sklearn.preprocessing.minmax_scale(data, axis=axis)\n\n    plt.figure(figsize=(20,5))\n    spectral_rolloff = librosa.feature.spectral_rolloff(data+0.01, sr=sample_rate1)[0]\n    librosa.display.waveplot(data, sr=sample_rate1, alpha=0.4)\n    plt.plot(t, normalize(spectral_rolloff), color='g')\n    plt.grid()","da521bf5":"rolloff(perfal)","e0644aa0":"rolloff(lotduc)","dcfb3fa1":"rolloff(rewbla)","76505251":"rolloff(warvir)","db909483":"rolloff(lecthr)","6d219179":"# MFCC\ndef mfcc(filename):\n    data,sample_rate1 = librosa.load(filename)\n    plt.figure(figsize=(20,5))\n    mfccs = librosa.feature.mfcc(data, sr=sample_rate1)\n    print(mfccs.shape)\n\n    librosa.display.specshow(mfccs, sr=sample_rate1, x_axis='time')","97c5688f":"mfcc(perfal)","c143db7c":"mfcc(lotduc)","2822169d":"mfcc(rewbla)","8e581d16":"mfcc(warvir)","29b027a1":"mfcc(lecthr)","552cd516":"def chrom_freq(filename):\n    data,sample_rate1 = librosa.load(filename)\n    \n    hop_length = 512\n    chromagram = librosa.feature.chroma_cqt(data, sr=sample_rate1, hop_length=hop_length)\n    plt.figure(figsize=(15, 5))\n    librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length)","338b3912":"chrom_freq(perfal)","4e2b5a1d":"chrom_freq(lotduc)","d7b1dc16":"chrom_freq(rewbla)","ce406aa7":"chrom_freq(warvir)","21115f0d":"chrom_freq(lecthr)","2db44c1c":"def fundamental_frequency(filename):\n    y, sr = librosa.load(filename)\n    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n    times = librosa.times_like(f0)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n    fig, ax = plt.subplots()\n    img = librosa.display.specshow(D, x_axis='time', y_axis='log', ax=ax)\n    ax.set(title='pYIN fundamental frequency estimation')\n    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n    ax.plot(times, f0, label='f0', color='cyan', linewidth=3)\n    ax.legend(loc='upper right')","f4c8c431":"fundamental_frequency(perfal)","c0031204":"fundamental_frequency(warvir)","bedc1c22":"fundamental_frequency(lotduc)","5b04767e":"fundamental_frequency(lecthr)","22b9301a":"fundamental_frequency(rewbla)","3f92c645":"def compute_tempogram(filename):\n    # computing local onset autocorrelation\n    y,sr = librosa.load(filename)\n    hop_length = 512\n    oenv = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length)\n    tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sr,\n                                      hop_length=hop_length)\n    \n    # Computing global onset autocorrelation\n    ac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\n    ac_global = librosa.util.normalize(ac_global)\n    \n    # Estimating global tempo\n    tempo = librosa.beat.tempo(onset_envelope=oenv, sr=sr,\n                           hop_length=hop_length)[0]\n    \n    # plotting\n    \n    fig, ax = plt.subplots(nrows=4, figsize=(10, 10))\n    times = librosa.times_like(oenv, sr=sr, hop_length=hop_length)\n    ax[0].plot(times, oenv, label='Onset strength')\n    ax[0].label_outer()\n    ax[0].legend(frameon=True)\n    librosa.display.specshow(tempogram, sr=sr, hop_length=hop_length,\n                             x_axis='time', y_axis='tempo', cmap='magma',\n                             ax=ax[1])\n    ax[1].axhline(tempo, color='w', linestyle='--', alpha=1,\n                label='Estimated tempo={:g}'.format(tempo))\n    ax[1].legend(loc='upper right')\n    ax[1].set(title='Tempogram')\n    x = np.linspace(0, tempogram.shape[0] * float(hop_length) \/ sr,\n                    num=tempogram.shape[0])\n    ax[2].plot(x, np.mean(tempogram, axis=1), label='Mean local autocorrelation')\n    ax[2].plot(x, ac_global, '--', alpha=0.75, label='Global autocorrelation')\n    ax[2].set(xlabel='Lag (seconds)')\n    ax[2].legend(frameon=True)\n    freqs = librosa.tempo_frequencies(tempogram.shape[0], hop_length=hop_length, sr=sr)\n    ax[3].semilogx(freqs[1:], np.mean(tempogram[1:], axis=1),\n                 label='Mean local autocorrelation', basex=2)\n    ax[3].semilogx(freqs[1:], ac_global[1:], '--', alpha=0.75,\n                 label='Global autocorrelation', basex=2)\n    ax[3].axvline(tempo, color='black', linestyle='--', alpha=.8,\n                label='Estimated tempo={:g}'.format(tempo))\n    ax[3].legend(frameon=True)\n    ax[3].set(xlabel='BPM')\n    ax[3].grid(True)","fa2cfb5d":"compute_tempogram(lecthr)","cf873bb1":"compute_tempogram(warvir)","0f98cc54":"compute_tempogram(perfal)","a1a10ad4":"compute_tempogram(lotduc)","19d6f2a4":"def decompose_audio(filename):\n    y,sr = librosa.load(filename)\n    D = librosa.stft(y)\n    #y_harmonic, y_percussive = librosa.effects.hpss(D, margin=(1.0,5.0)) # we will get more isolated percussive component by increasing margin \n    D_harmonic, D_percussive = librosa.decompose.hpss(D)\n    # Pre-compute a global reference power from the input spectrum\n    rp = np.max(np.abs(D))\n\n    plt.figure(figsize=(12, 8))\n\n    plt.subplot(3, 1, 1)\n    librosa.display.specshow(librosa.amplitude_to_db(D, ref=rp), y_axis='log')\n    plt.colorbar()\n    plt.title('Full spectrogram')\n\n    plt.subplot(3, 1, 2)\n    librosa.display.specshow(librosa.amplitude_to_db(D_harmonic, ref=rp), y_axis='log')\n    plt.colorbar()\n    plt.title('Harmonic spectrogram')\n\n    plt.subplot(3, 1, 3)\n    librosa.display.specshow(librosa.amplitude_to_db(D_percussive, ref=rp), y_axis='log', x_axis='time')\n    plt.colorbar()\n    plt.title('Percussive spectrogram')\n    plt.tight_layout()","a1d61d4b":"decompose_audio(perfal)","575ac1ee":"decompose_audio(lotduc)","9e9d770b":"decompose_audio(warvir)","3229a1d1":"decompose_audio(lecthr)","7c695d82":"def pitch_speed(filename):\n    data, sr = librosa.load(filename)\n    pitch_speed = data.copy()\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.0  \/ length_change\n    print(\"resample length_change = \",length_change)\n    tmp = np.interp(np.arange(0,len(pitch_speed),speed_fac),np.arange(0,len(pitch_speed)),pitch_speed)\n    minlen = min(pitch_speed.shape[0], tmp.shape[0])\n    pitch_speed *= 0\n    pitch_speed[0:minlen] = tmp[0:minlen]\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(pitch_speed, sr=sr, color='r', alpha=0.25)\n    plt.title('augmented pitch and speed')\n    return ipd.Audio(data, rate=sr)","afe9291c":"pitch_speed(perfal)","e97b3223":"pitch_speed(lotduc)","e8e2bdb0":"pitch_speed(rewbla)","9473e0d0":"pitch_speed(warvir)","a0293beb":"pitch_speed(lecthr)","1ffde6ae":"def pitch(filename):\n    data, sr = librosa.load(filename)\n    y_pitch = data.copy()\n    bins_per_octave = 12\n    pitch_pm = 2\n    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n    print(\"pitch_change = \",pitch_change)\n    y_pitch = librosa.effects.pitch_shift(y_pitch.astype('float64'), \n                                          sr, n_steps=pitch_change, \n                                          bins_per_octave=bins_per_octave)\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(y_pitch, sr=sr, color='r', alpha=0.25)\n    plt.title('augmented pitch only')\n    plt.tight_layout()\n    plt.show()\n    return ipd.Audio(data, rate=sr)","1aebec45":"pitch(perfal)","90b07a4c":"pitch(lotduc)","0c4d4e83":"pitch(rewbla)","d4ec8077":"pitch(warvir)","e84bcc21":"pitch(lecthr)","11683b06":"def speed(filename):\n    data, sr = librosa.load(filename)\n    aug = naa.SpeedAug()\n    augmented_data = aug.augment(data)\n\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(augmented_data, sr=sr, color='r', alpha=0.25)\n    plt.title('augmented speed only')\n    plt.tight_layout()\n    plt.show()\n    return ipd.Audio(augmented_data, rate=sr)","89965887":"speed(perfal)","9a63fcbe":"speed(lotduc)","e005caee":"speed(rewbla)","02db373e":"speed(warvir)","9773e9de":"speed(lecthr)","d7be2071":"def augmentation(filename):\n    data, sr = librosa.load(filename)\n    y_aug = data.copy()\n    dyn_change = np.random.uniform(low=1.5,high=3)\n    print(\"dyn_change = \",dyn_change)\n    y_aug = y_aug * dyn_change\n    print(y_aug[:50])\n    print(data[:50])\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(y_aug, sr=sr, color='r', alpha=0.25)\n    plt.title('amplify value')\n    return ipd.Audio(y_aug, rate=sr)","43173b38":"augmentation(perfal)","c71d5bb0":"augmentation(lotduc)","af99a8af":"augmentation(rewbla)","d70b58e3":"augmentation(warvir)","5a593859":"augmentation(lecthr)","b59c4b8b":"def add_noise(filename):\n    data, sr = librosa.load(filename)\n    y_noise = data.copy()\n    # you can take any distribution from https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/routines.random.html\n    noise_amp = 0.005*np.random.uniform()*np.amax(y_noise)\n    y_noise = y_noise.astype('float64') + noise_amp * np.random.normal(size=y_noise.shape[0])\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(y_noise, sr=sr, color='r', alpha=0.25)\n    return ipd.Audio(y_noise, rate=sr)","6a9040fe":"add_noise(perfal)","0e0f7069":"add_noise(lotduc)","b27783e6":"add_noise(rewbla)","0325af1e":"add_noise(warvir)","de86e48e":"add_noise(lecthr)","93d869b6":"def random_shift(filename):\n    data, sr = librosa.load(filename)\n    y_shift = data.copy()\n    timeshift_fac = 0.2 *2*(np.random.uniform()-0.5)  # up to 20% of length\n    print(\"timeshift_fac = \",timeshift_fac)\n    start = int(y_shift.shape[0] * timeshift_fac)\n    print(start)\n    if (start > 0):\n        y_shift = np.pad(y_shift,(start,0),mode='constant')[0:y_shift.shape[0]]\n    else:\n        y_shift = np.pad(y_shift,(0,-start),mode='constant')[0:y_shift.shape[0]]\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(y_shift, sr=sr, color='r', alpha=0.25)\n    return ipd.Audio(y_shift, rate=sr)","263d4a81":"random_shift(perfal)","2009083c":"random_shift(lotduc)","ba7c9c1f":"random_shift(rewbla)","0514729c":"random_shift(warvir)","993d0147":"random_shift(lecthr)","e9f0ac07":"def hpss(filename):\n    data, sr = librosa.load(filename)\n    y_hpss = librosa.effects.hpss(data.astype('float64'))\n    print(y_hpss[1][:10])\n    print(data[:10])\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(y_hpss[1], sr=sr, color='r', alpha=0.25)\n    plt.title('apply hpss')\n    return ipd.Audio(y_hpss[1], rate=sr)","68cb683c":"hpss(perfal)","2fed5715":"hpss(lotduc)","c23a17c1":"hpss(rewbla)","fdb50cba":"hpss(warvir)","38718664":"hpss(lecthr)","3f90b6b4":"def streching(filename):\n    data, sr = librosa.load(filename)\n    input_length = len(data)\n    streching = data.copy()\n    streching = librosa.effects.time_stretch(streching.astype('float'), 1.1)\n    if len(streching) > input_length:\n        streching = streching[:input_length]\n    else:\n        streching = np.pad(streching, (0, max(0, input_length - len(streching))), \"constant\")\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(streching, sr=sr, color='r', alpha=0.25)\n    \n    plt.title('stretching')\n    return ipd.Audio(streching, rate=sr)","6a0f6eda":"streching(perfal)","dfedfc22":"streching(lotduc)","74502b1b":" streching(rewbla)","75424e4a":" streching(warvir)","31bb90e2":"streching(lecthr)","7f3a4fb1":"def crop(filename):\n    data, sr = librosa.load(filename)\n    aug = naa.CropAug(sampling_rate=sr)\n    augmented_data = aug.augment(data)\n\n    librosa_display.waveplot(augmented_data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(data, sr=sr, color='r', alpha=0.25)\n\n    plt.tight_layout()\n    plt.show()\n\n    return ipd.Audio(augmented_data, rate=sr)","86611bc2":"crop(perfal) ","db5ebe13":"crop(lotduc)","314ce648":"crop(rewbla)","cea9f985":"crop(warvir)","376018ec":"crop(lecthr)","448be9bb":"def loudnessaug(filename):\n    data, sr = librosa.load(filename)\n    aug = naa.LoudnessAug(loudness_factor=(2, 5))\n    augmented_data = aug.augment(data)\n\n    librosa_display.waveplot(augmented_data, sr=sr, alpha=0.25)\n    librosa_display.waveplot(data, sr=sr, color='r', alpha=0.5)\n\n    plt.tight_layout()\n    plt.show()\n\n    return ipd.Audio(augmented_data,rate=sr)","9bf3348d":"loudnessaug(perfal) ","39fb2877":"loudnessaug(lotduc)","64035917":"loudnessaug(rewbla)","3c2e09b0":"loudnessaug(warvir)","4512d444":"loudnessaug(lecthr)","e1ff4c04":"def mask(filename):\n    data, sr = librosa.load(filename)\n    aug = naa.MaskAug(sampling_rate=sr, mask_with_noise=False)\n    augmented_data = aug.augment(data)\n\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(augmented_data, sr=sr, color='r', alpha=0.25)\n\n    plt.tight_layout()\n    plt.show()\n    \n    return ipd.Audio(augmented_data, rate=sr)","78382563":"mask(perfal)","4bb89a7a":"mask(lotduc)","f444cb66":"mask(rewbla)","5b9c0e37":"mask(warvir)","220aa337":"mask(lecthr)","10ef7ca0":"def shift(filename):\n    data, sr = librosa.load(filename)\n    aug = naa.ShiftAug(sampling_rate=sr)\n    augmented_data = aug.augment(data)\n\n    librosa_display.waveplot(data, sr=sr, alpha=0.5)\n    librosa_display.waveplot(augmented_data, sr=sr, color='r', alpha=0.25)\n\n    plt.tight_layout()\n    plt.show()\n    \n    return ipd.Audio(augmented_data, rate=sr)","a479bedc":"shift(perfal)","2bf2c013":"shift(lotduc)","9f5d1898":"shift(rewbla)","c245dd74":"shift(warvir)","6b0ce5a1":"shift(lecthr)","e6132944":"train_set= train.copy()\nbirds_key=train[\"ebird_code\"].unique()\nbirds_key","0a8cdf37":"random.shuffle(birds_key)\ntrain_set = train_set.query(\"ebird_code in @birds_key\")\n\nidBirdDict = {}\nebirdDict = {}\nebirdDict[\"nocall\"] = 0\nidBirdDict[0] = \"nocall\"\nfor idx, unique_ebird_code in enumerate(train_set.ebird_code.unique()):\n    ebirdDict[unique_ebird_code] = str(idx+1)\n    idBirdDict[idx+1] = str(unique_ebird_code)","c7c87af4":"ebirdDict","ac7243b8":"idBirdDict","d6057b06":"#Let create a Sample Set as Whote data set will run for long hours\nsample_set=pd.DataFrame(columns=['ebird_code','audio_File_path',\"song_sample\",\"bird\"])","34d64d54":"#Using Francois's code to extract the data\/ run model\n\ndef get_sample(filename, bird, sample_set):\n    min_max_Scaler=MinMaxScaler()\n    wave_data, wave_rate = librosa.load(filename)\n    data_point_per_second = 10\n    \n    #Take 10 data points every second\n    prepared_sample = wave_data[0::int(wave_rate\/data_point_per_second)]\n    #We normalize each sample before extracting 5s samples from it\n    normalized_sample = min_max_Scaler.fit_transform(prepared_sample.reshape(-1, 1))\n    normalized_sample = normalized_sample.flatten()\n    \n    #only take 5s samples and add them to the dataframe\n    song_sample = []\n    sample_length = 5*data_point_per_second\n    for idx in range(0,len(normalized_sample),sample_length): \n        song_sample = normalized_sample[idx:idx+sample_length]\n        if len(song_sample)>=sample_length:\n            sample_set = sample_set.append({\"song_sample\":np.asarray(song_sample).astype(np.float32),\n                                            \"bird\":ebirdDict[bird],\n                                           \"audio_File_path\":filename,\n                                           \"ebird_code\":bird}, \n                                           ignore_index=True)\n                     \n    return sample_set","38a0e22c":"# we will run for 5000 records for total Trains set to prepare for Model \nwith tqdm(total=5000) as pbar:\n    for idx, row in train_set[:5000].iterrows():\n        pbar.update(1)\n        #print(idx)\n        sample_set = get_sample(row.audio_File_path, row.ebird_code, sample_set)","35c9a56a":"#Now out of the complete sequence length we will choose with the fixed 50 sequence length for the above input array on Sample Set\n# also divide the sample set into train and val set on the basis of 80:20\nsequence_length = 50\nsplit_per = 0.80\ntrain_item_count = int(len(sample_set)*split_per)\nval_item_count = len(sample_set)-int(len(sample_set)*split_per)\ntraining_set = sample_set[:train_item_count]\nvalidation_set = sample_set[train_item_count:]","9c8fc842":"# we will have Sequential LSTM with dropout and 3 layer as SOftMax and Optimizer is ADAM\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, recurrent_dropout=0.2,input_shape=(None, sequence_length)))\nmodel.add(LSTM(32,recurrent_dropout=0.2))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(len(ebirdDict.keys()), activation=\"softmax\"))\n\nmodel.summary()\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1, factor=0.7),\n             EarlyStopping(monitor='val_loss', patience=10),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\nmodel.compile(loss=\"categorical_crossentropy\", optimizer='adam')","26f36023":"# Take the Xtrain and Y train from train Set from Sample Set data frame to be feed into LSTM Model\nX_train = np.asarray(np.reshape(np.asarray([np.asarray(x) for x in training_set[\"song_sample\"]]),(train_item_count,1,sequence_length))).astype(np.float32)\ntrain_gd = np.asarray([np.asarray(x) for x in training_set[\"bird\"]]).astype(np.float32)\nY_train = to_categorical(\n                train_gd, num_classes=len(ebirdDict.keys()), dtype='float32'\n            )\n\n\nX_val = np.asarray(np.reshape(np.asarray([np.asarray(x) for x in validation_set[\"song_sample\"]]),(val_item_count,1,sequence_length))).astype(np.float32)\nval_gd = np.asarray([np.asarray(x) for x in validation_set[\"bird\"]]).astype(np.float32)\nY_val = to_categorical(\n                val_gd, num_classes=len(ebirdDict.keys()), dtype='float32'\n            )","b2514b64":"# Fit the LSTM model and plot the Train and validation Loss for 100 Epochs and batch Size of 32\nmodel_his1 = model.fit(X_train, Y_train, \n          epochs = 100, \n          batch_size = 32, \n          validation_data=(X_val, Y_val), \n          callbacks=callbacks)\n\nplt.plot(model_his1.history['loss'])\nplt.plot(model_his1.history['val_loss'])\nplt.title('Loss over epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='best')\nplt.show()","31742146":"# make the predictions function to predict on Unsenn data from the Model trained\nmodel.load_weights(\"best_model.h5\")\n\ndef make_prediction(df, audio_file_path):\n        \n    loaded_audio_sample = []\n    previous_filename = \"\"\n    data_point_per_second = 10\n    sample_length = 5*data_point_per_second\n    wave_data = []\n    wave_rate = None\n    \n    for idx,row in df.iterrows():\n        if previous_filename == \"\" or previous_filename!=row.filename:\n            filename = '{}\/{}.mp3'.format(audio_file_path, row.filename)\n            wave_data, wave_rate = librosa.load(filename)\n            sample = wave_data[0::int(wave_rate\/data_point_per_second)]\n        previous_filename = row.filename\n        \n        #basically allows to check if we are running the examples or the test set.\n        if \"site\" in df.columns:\n            if row.site==\"site_1\" or row.site==\"site_2\":\n                song_sample = np.array(sample[int(row.seconds-5)*data_point_per_second:int(row.seconds)*data_point_per_second])\n            elif row.site==\"site_3\":\n                #for now, I only take the first 5s of the samples from site_3 as they are groundtruthed at file level\n                song_sample = np.array(sample[0:sample_length])\n        else:\n            #same as the first condition but I isolated it for later and it is for the example file\n            song_sample = np.array(sample[int(row.seconds-5)*data_point_per_second:int(row.seconds)*data_point_per_second])\n\n        input_data = np.reshape(np.asarray([song_sample]),(1,sequence_length)).astype(np.float32)\n        prediction = model.predict(np.array([input_data]))\n        predicted_bird = idBirdDict[np.argmax(prediction)]\n\n        df.at[idx,\"birds\"] = predicted_bird\n    return df","962ef386":"#Let see how our model performs on example set given\nexample_set = pd.read_csv(base_path+\"example_test_audio_summary.csv\")\nexample_set[\"filename\"] = [ \"BLKFR-10-CPL_20190611_093000.pt540\" if filename==\"BLKFR-10-CPL\" else \"ORANGE-7-CAP_20190606_093000.pt623\" for filename in example_set[\"filename\"]]\nexample_set\n","c2dff426":"example_audio_file_path = base_path +\"example_test_audio\"\nif os.path.exists(example_audio_file_path):\n    example_set = make_prediction(example_set, example_audio_file_path)\nexample_set","446c9f41":"# Now lets predict on the test Set and prepare the Submission File\ntest_audio_file_path = base_path+\"test_audio\/\"\nsubmission_set = pd.read_csv(base_path+\"sample_submission.csv\")\nsubmission_set.head()","2d9f12c9":"if os.path.exists(test_audio_file_path):\n    submission_set = make_prediction(test, test_audio_file_path)","82bfbc0e":"submission_set[:20]","0c76876a":"submission_set.to_csv(\"submission.csv\", index=False)","caa6cca1":"submission_set","192a0a9f":"### 3.Zero Crossing Rate\n\n* The zero crossing rate indicates the number of times that a signal crosses the horizontal axis.","a002f9e2":"## Adding Noise","28701ae2":"\ud83d\udccc Key Observations:\n* We have 264 unique primary labels, which is equal to the no. of unique species\n* Primary labels are equally distributed in the dataset\n* We have 5385 unique secondary labels, even though for majority of the audio clips, this field is left blank \"[]\" ","9bdf0805":"# Exploratory Data Analysis\n\n![image.png](attachment:image.png)","4164390a":"### 4. lets explore \"date\" feature","bf06ef46":"### 8. Next set of fields that we will examine are pitch, no. of notes & speed\n\n* pitch: Was the Pitch of the Bird Call increasing \/ decreasing or constant\n* number_of_notes: No: of Syllables\n* speed: whether speed is constant (level), decreasing (decelerating), increasing (accelerating), or both (in either order","2724793c":"## Shift Augmentation","0d6a4e41":"\ud83d\udccc Key Observations:\n* We have 264 unique bird species\n* Blackpoll Warbler, American Crow,Veery & Lesser Goldfinch are few of the most frequently present in the dataset ","eff7081b":"\ud83d\udccc <b>Key Observations:<\/b>\n\n* Intersting point to note here is that, each recording has multiple labels associated with it, `species`, `primary label` and `secondary label`. This means that recordings may contain voice of more than one bird, which is quite natural because birds normally sing in groups!","7b793f29":"\ud83d\udccc Key Observations:\n    Only the first three rows are available for download; the full test.csv is in the hidden test set. \n   \nTest dataset has following columns:\n\n`site`: Site ID.\n\n`row_id`: ID code for the row.\n\n`seconds`: the second ending the time window, if any. Site 3 time windows cover the entire audio file and have null entries for seconds.\n\n`audio_id`: ID code for the audio file.","7ec5c07e":"**LSTM Model**","7eeb0df6":"### 9. Rhythm Fetaure\nTempogram - local autocorrelation of the onset strength envelope","ede1c31c":"\ud83d\udccc Key Observations:\n\n* pitch\/speed - most of the values for these field are not specified\n* number of notes is not specified for majority of the records","ad8e03b4":"## Change pitch only","6b180a8f":"\ud83d\udccc <b>Key Observations:<\/b>\n* Training dataset `train.csv` has 21375 rows and 35 columns","b0a72f1e":"### 1. Let's listen to some music!","3b98fcb2":"librosa.display is used to display the audio files in different formats such as `wave plot`, `spectrogram`, or `colormap` ","ef1a77d5":"\ud83d\udccc Key Observations:\n* Max recording are in the month of May(End of Spring) & June(Start of Summer)","c6434833":"### 3.Let's explore latitude and longitude, we will plot our birds on the world map\n\n* latitude: latitude co-ordinate of the earth\n* longitude: longitude co-ordinate of the earth","5ee5138d":"### 14. Lets inspect background, author & recordist\n\nrecordist: Name of the recordist\nbackground: Background Birds Identified\nauthor: Person who recorded the audio","e66b048c":"## Mask Augmentation","8ed5b3f7":"## Applying hpss","137ac7ad":"\ud83d\udccc Key Observations:\n\nThere are as many unique ebird codes as the no. of species,also the distribution is same as species, which means each ebird code represnts one particular species\n\nThere are 264 unique scientific names,equal to the no. of species, seems to have 1:1 mapping with species","e0317eeb":"## value augmentation","7771933e":"# **Model Pre-Processing**","54c23b5e":"# Let's check the metadata (.csv files)","541e3dbe":"# Introduction\n\nOur challenge in this competition is to identify which birds are calling in long recordings, given training data generated in meaningfully different contexts. This is the exact problem facing scientists trying to automate the remote monitoring of bird populations.\n\n\n### Files\n\n1. `train_audio` -  The train data consists of short recordings of individual bird calls\n\n2. `test_audio`  -  The hidden test_audio directory contains approximately 150 recordings in mp3 format, each roughly 10 minutes long. \nThe recordings were taken at three separate remote locations in North America. `Sites 1` and `2` were labeled in `5 second increments` and need matching predictions, but due to the time consuming nature of the labeling process the `site 3` files are only labeled at the file level.\n\n3. `test.csv` Only the first three rows are available for download; the full test.csv is in the hidden test set.\n\n4.`train.csv` - A wide range of metadata is provided for the training data. The most directly relevant fields are:\n\n* `ebird_code`: a code for the bird species. You can review detailed information about the bird codes by appending the code to https:\/\/ebird.org\/species\/, such as https:\/\/ebird.org\/species\/amecro for the American Crow.\n    \n* `recodist`: the user who provided the recording.\n\n* `location`: where the recording was taken. Some bird species may have local call 'dialects', so you may want to seek geographic diversity in your training data.\n\n* `date`: while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.\n\n* `filename`: the name of the associated audio file.","dd2a5c26":"### LIBROSA\n\nWe will use `LibROSA` package to analyse audio files , it provides the building blocks necessary to create audio information retrieval systems.\n\nWe will also use `IPython.display` package to listen to audio files in the notebook.","17a3c7b2":"### 2. Let's check recording location - Country\n\n* country: Species recorded location","df2fea01":"### 10. Lets inspect primary and secondary labels\n\n* primary_label: Meta-Data for Labeling Birds in Xeno Catalog\n* secondary_labels : Background Birds Identified","aee73b89":"### 5. Let's check ebird code & sci_name        \n\nebird_code - a code for the bird species. You can review detailed information about the bird codes by appending the code to https:\/\/ebird.org\/species\/, such as https:\/\/ebird.org\/species\/amecro for the American Crow.**\n\n* sci_name: Scientific Name of the Bird         ","e51559d6":"## random shifting","cbdfa655":"### 1. Let's check the class distribution (no. of unique birds in the dataset)","1c42ba91":"## Change pitch and speed","46068030":"### 2. Waveplots,Spectogram, Mel-Spectorgram\n\n\n* `Waveplots` let us know the loudness of the audio at a given time. Waveplot is the time-domain representation of a given signal. \n    This shows us the loudness (amplitude) of sound wave changing with time. Here amplitude = 0 represents silence. \n\n\n* `Spectogram` is a visual representation of the spectrum of frequencies of sound or other signals as they vary with time. It\u2019s a representation of frequencies changing with respect to time for given music signals (shows different frequencies playing at a particular time along with it\u2019s amplitude)\n\n* `Mel-Spectogtram` it represents an acoustic time-frequency representation of a sound, it is a normal Spectrogram, but with a Mel Scale on the y axis","9f8b064c":"## Change speed only","b16b4d8e":"<h1><center>BirdSound Recognition<\/center><\/h1>\n\n![](https:\/\/m.media-amazon.com\/images\/I\/81g3oOHeYZL._SS500_.jpg)","3f8787e8":"### 7. Chrome Frequencies\n","f51541db":"### 12. Lets inspect volume,length and elevation\n\n* elevation: Height from Sea Level\n* length: This is the length of the Bird Call, not the length of the recording.\n* volume:","1b432d0b":"\ud83d\udccc Key Observations:\n* Length & Volumne is not specified for majority of the recordings\n\n* Elevation is 0-10 for majortity of the records","b1cfd5e9":"\ud83d\udccc Key Observations:\n\n* In most of the cases birds were seen while recording\n* Sampling rate is mostly 44100 and 48000 Hz\n* In most if the cases birds were found either actually singing or calling when they were recorded","9377a9c8":"### 6. MFCC\n* Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"","5bbc2dde":"\ud83d\udccc Key Observations:\n* Majority of the recordings are done in USA, followed by CANADA and MEXICO","e1fba12b":"### 6. Lets inspect filename,title,description,xc_id & url\n\n* title : Ebird_Code with Species Name\n* filename: name of the associated audio file present in the train_audio directory.\n* description: Description about the recording provided by the recordist\n* xc_id: xeno-canto bird Id\n* url: xeno-canto Bird Link","86b2c62c":"## cropping","a4c1db1c":"### 5. Spectral Rolloff\n\nSpectral rolloff is the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies.","96a91d2d":"\ud83d\udccc Key Observations:\n\nThere are 21375 unique filenames & titles in the dataset, which is equal to the no. of rows in the dataset, which means each row has a unique filename and a unique title corresponding to a recording\n\nThere are 12694 unique descriptions and 6199 missing values, this field seems to be like a remark field\n\nThere are 21375 unique xc_id & urls in the dataset, which is equal to the no. of rows in the dataset, which means each row has a unique xc_id & unique url corresponding to a recording","f949adda":"\ud83d\udccc Key Observations:\n* No. of recording started increasing from 2012, max recordings in 2014","3c34bd27":"\ud83d\udccc Key Observations:\n\n* Majority of the recordings are in mp3 format\n* First 2 category forms the majority for license type\n* 128000 bps is the most frequently used bitrate for recoding","741ef209":"\ud83d\udccc Key Observations:\n* The number of audio samples per category is non-nform. The minimum number of audio samples in a category is 9 while the maximum is 100","c15125ea":"### Test Dataset","20528ae7":"1. Now we will prepare adict with unique birds and Key into the dict","0efaf93d":"### 4. Spectral Centroid\n\n* The spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located.","ac381de5":"\ud83d\udccc Key Observations:\n* duration of most of the audio files are between 0 to 300 seconds","bfc5e34d":"## Loudness Augmentation","4d9aab87":"### 9. Lets take a look at duration\n\n* duration: Total Recording in Seconds","a327ab5b":"### 8. Fundamental Frequency Estimation using probabilistic YIN algo**\n\nUsed to calculate the fundamental frequency curve from given audio input","04e2ca7f":"# Getting our tools ready","1cbf2589":"# Audio Augmentation","042f5bfc":"### 10. Harmonic-percussive source separaton from audio input\n\nhpss: It will decompose an audio time series into harmonic and percussive components.","5891e155":"# Audio Files Analysis\n\n`Audio Signal`\n\n* The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\n  It is a complex signal composed of multiple \u2018single-frequency sound waves\u2019 which travel together as a disturbance(pressure-change) in the medium. \n\n  When sound is recorded we only capture the resultant amplitudes of those multiple waves. \n\n\n\n![image.png](attachment:image.png)\n\n`Sampling`\n\n* Sound is a continuous wave. We can digitise sound by breaking the continuous wave into discrete signals. This process is called sampling. Sampling converts a sound wave into a sequence of samples or a discrete-time signal.\n\n`Sampling Rate (sr)`\n* The sampling rate is the number of samples per second. Hz or Hertz is the unit of the sampling rate. 20 kHz is the audible range for human beings.\n\n`Amplitudes`\n* From the definition of sound waves \u2014 This amplitude is actually the amplitude of air particles which are oscillating because of the pressure change in the atmosphere due to sound.\n\n* These amplitudes are not very informative, as they only talk about the loudness of audio recording. \n \n`Fourier Transform`\n\n* To better understand the audio signal, it is necessary to transform it into the frequency-domain. The frequency-domain representation of a signal tells us what different frequencies are present in the signal. \n\n* Fourier Transform is a mathematical concept that can convert a continuous signal from time-domain to frequency-domain. ","04db690e":"### 13. Lets inspect file type, license, bitrate_of_mp3\n\n* file_type - Audio File type and mostly every file is a mp3\n* license - License of the recording\n* birate_of_mp3 - Number of Bits used for encoding per second","104d3fea":"### 11. Lets inspect bird seen, sampling rate and type\n\n* bird_seen: Was the Bird Seen during the recording\n* sampling_rate: Digital Samples recorded per second, most of the samples are 44.1kHz \/ 48kHz\n* Type of Bird Sound Recorded. Wing Noise \/ Song \/ Call \/ Flight Call","fec6eae8":"## Streching","3637490c":"### 7. Next set of fields that we will examine are playback_used, channel & rating\n\n* rating: rates the audio quality from 0-5\n* playback_used: Was playback used to lure the bird ?\n* channels: stereo or mono","aec8f12e":"\ud83d\udccc Key Observations:\n\n* For most of the recordings present in the dataset, playback was not used\n* Channels seems to have almost equal distribution\n* Most of the audio have good ratings, kind of an indication that people usually loves bird's voice"}}