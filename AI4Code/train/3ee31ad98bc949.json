{"cell_type":{"b6f11e64":"code","40ccdfa5":"code","4eeb8c71":"code","ee5db284":"code","f3c9153c":"code","08bcfb8e":"code","8041cc72":"code","069938ef":"code","2e7c6372":"code","b41f548f":"code","38ad070f":"code","9264af6d":"code","840a68e9":"code","2e2c37a3":"code","2b2b27e4":"code","f8f51e8c":"code","c5b3c7a6":"code","8e66e62c":"code","ac0fc8bd":"code","2820b38a":"code","26c3a731":"code","d2cefa9f":"code","3d1c32a0":"code","d2888d0e":"code","659a2278":"markdown","4afea386":"markdown","93acecb8":"markdown","5d170195":"markdown","21417bc5":"markdown","4fae292f":"markdown","70f6be7a":"markdown","d886d6f9":"markdown"},"source":{"b6f11e64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40ccdfa5":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport os","4eeb8c71":"df = pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\ndf = df.drop(\"Id\", axis = 1)\ndf.head()","ee5db284":"df.isnull().sum()","f3c9153c":"print(df[\"Married\/Single\"].unique())\nprint('\\n')\nprint(df['House_Ownership'].unique())\nprint('\\n')\nprint(df['Car_Ownership'].unique())\nprint('\\n')\nprint(df[\"Profession\"].unique())\nprint('\\n')\nprint(df[\"CITY\"].unique())\nprint('\\n')\nprint(df['STATE'].unique())","08bcfb8e":"df = df.drop('CITY', axis = 1)\ndf.head()","8041cc72":"en = LabelEncoder()\ncolumns = ['Married\/Single',\n           'House_Ownership',\n           'Car_Ownership',\n           'Profession',\n           'STATE']\n\nfor col in columns:\n    df[col] = en.fit_transform(df[col])\n\ndf.head()","069938ef":"Y = df[\"Risk_Flag\"]\nX = df.drop(\"Risk_Flag\", axis = 1)\n\nX_train, X_valid, Y_train, Y_valid = train_test_split(X,Y, train_size = 0.8, test_size = 0.2, random_state = 0)","2e7c6372":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_auc_score","b41f548f":"clf = XGBClassifier(learning_rate = 0.1,\n                    n_estimators = 1000,\n                    use_label_encoder = False, \n                    random_state = 42)\n\nclf.fit(X_train, Y_train, eval_metric = \"logloss\")\npredictions = clf.predict(X_valid)\nprint(\"accuracy_score: \" + str(accuracy_score(Y_valid, predictions)))","38ad070f":"print(\"f1_score: \" + str(f1_score(Y_valid, predictions)))","9264af6d":"print(\"ROC AUC score: \" + str(roc_auc_score(Y_valid, predictions)))","840a68e9":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 400, criterion = 'entropy')\nrf.fit(X_train, Y_train)\nrf.score(X_train, Y_train)","2e2c37a3":"from sklearn.metrics import classification_report, roc_curve\n\n\npredictions_rf = rf.predict(X_valid)\n\nprint(\"accuracy_score: \" + str(accuracy_score(Y_valid, predictions_rf)))\n\nprint(classification_report(Y_valid, predictions_rf))\n\nprint(\"ROC AUC score: \"+ str(roc_auc_score(Y_valid, predictions_rf)) )","2b2b27e4":"fpr1, tpr1, thresh1 = roc_curve(Y_valid, predictions_rf, pos_label =1)\n\n#roc curve for tpr = fpr\nrandom_probs = [0 for i in range(len(Y_valid))]\np_fpr, p_tpr, _ = roc_curve(Y_valid, random_probs, pos_label=1)\n\nplt.figure(figsize = (12,8))\nplt.plot(fpr1, tpr1, linestyle = ':', color = \"orange\", label = \"Random Forest Classifier\")\nplt.plot(p_fpr, p_tpr, linestyle = ':', color = \"blue\")\n\nplt.title(\"ROC curve\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\n\nplt.legend(loc = \"best\")\n\nplt.show()","f8f51e8c":"cm = confusion_matrix(Y_valid, predictions_rf)\ncmDisplay = ConfusionMatrixDisplay(cm).plot()","c5b3c7a6":"test_df = pd.read_csv(\"\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv\")\n\ntest_df = test_df.drop(['ID','CITY'], axis = 1)\n\nen = LabelEncoder()\ncolumns = ['Married\/Single',\n           'House_Ownership',\n           'Car_Ownership',\n           'Profession',\n           'STATE']\n\nfor col in columns:\n    test_df[col] = en.fit_transform(test_df[col])\n\ntest_df.head()","8e66e62c":"from sklearn.preprocessing import StandardScaler, MinMaxScaler","ac0fc8bd":"sc_X = StandardScaler()\ntest = sc_X.fit_transform(test_df)\npred = rf.predict(test)","2820b38a":"import optuna","26c3a731":"def rf_objective(trial):\n    params = {\n        \"n_estimators\": trial.suggest_int('n_estimators', 10, 100),\n        \"max_depth\": trial.suggest_categorical(\"max_depth\", [7, 8, 9, 10, 11, 12, None]),\n        \"criterion\": trial.suggest_categorical('criterion', [\"gini\", \"entropy\"]),\n        \"min_samples_split\": trial.suggest_int('min_samples_split', 2, 5),\n        \"min_samples_leaf\": trial.suggest_categorical('min_samples_leaf', [1, 2]),\n        \"max_features\": trial.suggest_categorical('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n        \"class_weight\": trial.suggest_categorical('class_weight', [\"balanced\"]),\n        \"random_state\": trial.suggest_categorical('random_state', [0]),\n        \"n_jobs\": trial.suggest_categorical('n_jobs', [-1]),\n    }\n    model = RandomForestClassifier(**params)\n    model.fit(X_train, Y_train)\n    return -roc_auc_score(Y_valid, model.predict(X_valid))","d2cefa9f":"# Best RandomForest ROC-AUC: 0.8419 with parameters {'n_estimators': 10, 'max_depth': None, 'criterion': 'entropy', 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'class_weight': 'balanced', 'random_state': 0, 'n_jobs': -1}\n#study = optuna.create_study()\n#study.optimize(rf_objective, n_trials=200, timeout=3600 * 2)\n#print(f\"Best RandomForest ROC-AUC: {-round(study.best_value, 4)} with parameters {study.best_params}\\n\\n\")","3d1c32a0":"rf_best_score = 0.8419\nrf_best_params = {'n_estimators': 10, \n                  'max_depth': None, \n                  'criterion': 'entropy', \n                  'min_samples_split': 5, \n                  'min_samples_leaf': 2, \n                  'max_features': 'sqrt', \n                  'class_weight': 'balanced', 'random_state': 0, 'n_jobs': -1}\nrf = RandomForestClassifier(**rf_best_params)\nrf.fit(X_train, Y_train);","d2888d0e":"print(f\"Random Forest Best: {rf_best_score}\")\n\ncm = confusion_matrix(Y_valid, rf.predict(X_valid))\ncmDisplay = ConfusionMatrixDisplay(cm).plot()","659a2278":"### XGB Classifier","4afea386":"### Training the Random Forest Classifier","93acecb8":"### Encode Categorical Data","5d170195":"### Train & validation data split by 80% 20%","21417bc5":"### Apply prediction on test data","4fae292f":"It doesn't have any missing data","70f6be7a":"### Import necessary package and training data","d886d6f9":"### Hyperparameters tuning"}}