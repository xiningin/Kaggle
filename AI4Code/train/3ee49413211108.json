{"cell_type":{"2bb624a3":"code","ff04d433":"code","98e6a404":"code","09d6b1c8":"code","30ce08dc":"code","9f547036":"code","e0629590":"code","2e059a13":"code","b791ddbc":"code","a2a3c246":"code","6c6dd54a":"code","4a569d3c":"code","629730e3":"code","c2106b42":"code","33cfbaef":"code","c33c5904":"code","5b5e8e15":"code","db9455e6":"code","290e7afe":"code","30828aca":"code","e8408cfa":"code","806d4ef8":"code","2f61e231":"code","96b91004":"code","2b6e7590":"code","3a43d2bc":"code","25b84707":"code","4f9b2de1":"code","a31d136f":"code","9d94a8e8":"code","6654ef38":"code","b0d8f1b2":"code","16de87bb":"code","1581b533":"code","3ac745cd":"code","3b804086":"code","f3a9d849":"code","914c7506":"code","64059350":"code","b187b67e":"code","b635733f":"code","38c253e0":"code","a1e5d7b7":"code","d5e82379":"code","da046f60":"code","101a46ed":"code","708d542b":"code","c57bfe23":"code","10bc8cc6":"code","c255f2c2":"code","eeb5c852":"code","7eb80a81":"code","90b42f01":"code","de52e478":"code","f5669b51":"code","ab8a45eb":"code","0f4e815e":"code","82ab8c4d":"code","e0b0bf12":"code","65415f31":"code","74f2eb25":"code","1794307f":"code","6205ec7c":"code","56b8d34b":"code","19ab5710":"code","91043527":"code","5bf217a3":"code","c21da432":"markdown","c993e32d":"markdown","365c3583":"markdown","8d553a19":"markdown","a3e1ba8a":"markdown","0e93394b":"markdown","f0311419":"markdown","3e828bea":"markdown","9b41b64d":"markdown","a95dcc58":"markdown","03570a59":"markdown","3631dcc7":"markdown","7ce2e2d0":"markdown","a95e33d5":"markdown","d998e2bb":"markdown","a86aed6c":"markdown","1bdc1fdf":"markdown","cd654bbf":"markdown","e3514fcf":"markdown","e901c210":"markdown","bd8782ac":"markdown","81fcea96":"markdown","61f6d5bb":"markdown","b5d195b4":"markdown","26c4f644":"markdown","87994799":"markdown"},"source":{"2bb624a3":"import pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import chi2_contingency\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC  \nfrom sklearn.naive_bayes import GaussianNB\n\n\n","ff04d433":"dataset = pd.read_csv(\"diabetes.csv\")","98e6a404":"dataset.head()","09d6b1c8":"dataset.shape","30ce08dc":"dataset.iloc[:,:-1].info()","9f547036":"dataset.describe()","e0629590":"dataset.isna().sum()","2e059a13":"discrete_feature = ['Pregnancies','Age','Outcome']\n\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","b791ddbc":"overall_diabetes_rate = dataset['Outcome'].mean()\noverall_diabetes_rate","a2a3c246":"group_by_pregnancy = dataset.groupby(\"Pregnancies\").agg({'Outcome': np.mean})\ngroup_by_pregnancy","6c6dd54a":"axes = plt.axes()\naxes.axhline(overall_diabetes_rate, color = 'red')\ngroup_by_pregnancy.plot(marker='x', legend= False, ax = axes)\naxes.set_ylabel('Proportion of  Diabetic people')\naxes.legend(['Enite dataset', 'Numbers of pregancnies'])","4a569d3c":"feature = 'Age'\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (20,5),constrained_layout=True)\nbin_x = range(25,80,2)\n\nax1.hist(dataset[feature],bins=bin_x,rwidth=0.9)\nax1.set_xticks(range(25,80,2))\nax1.set_xlabel('Age',fontsize=15)\nax1.set_ylabel('Count',fontsize=15)\nax1.set_title('Age Distribution',fontsize=20)\n\nax2.hist(dataset[dataset['Outcome']==1][feature], label = 'Positive',bins=bin_x,rwidth=0.9)\nax2.hist(dataset[dataset['Outcome']==0][feature], label = 'Negative',bins=bin_x,rwidth=0.5)\nax2.legend()\nax2.set_xticks(range(25,80,2))\nax2.set_xlabel('Age',fontsize=15)\nax2.set_ylabel('Count',fontsize=15)\nax2.set_title('Diabetes: Positive vs Negative',fontsize=20)\n\nplt.show()","629730e3":"continuous_feature=[feature for feature in dataset.columns if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","c2106b42":"for feature in continuous_feature:\n    dataset[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","33cfbaef":"columns_incorrect = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n\ndef replace_(x):\n    if x == 0:\n        return np.nan\n    return x\n\nfor column in columns_incorrect:\n    dataset[column] = dataset[column].map(replace_).values","c33c5904":"dataset.isna().sum()","5b5e8e15":"dataset.isnull().sum(axis=1).value_counts()\n#Number of rows by number of missing values","db9455e6":"def check_mcar(data, incorrect_columns, dependent_categorical_variable):\n    \"\"\" To check whether the missingness of data is dependent on the outcome variable. A significant relationship can bias our results.\"\"\"\n    new_columns = []\n    \n    for column in incorrect_columns:\n        data[column+\"_missing\"]  = False\n        data.loc[data[data[column].isnull()].index, column+\"_missing\"] = True\n        new_columns.append(column+\"_missing\")\n        \n    for column in incorrect_columns:\n        grouped_true =data[data[column+\"_missing\"]==True].groupby(dependent_categorical_variable)[column+\"_missing\"].count()\n        grouped_false =data[data[column+\"_missing\"]==False].groupby(dependent_categorical_variable)[column+\"_missing\"].count()\n        table = [[grouped_true[0], grouped_false[0]],[grouped_true[1],grouped_false[1]]]\n        chi2, p, dof, ex = chi2_contingency(table, correction=True)\n        print(\"The p-value of chi-square test between\", column +\" and \"+dependent_categorical_variable,  \"is equal to {}\".format(p))\n","290e7afe":"check_mcar(dataset, columns_incorrect, \"Outcome\")","30828aca":"dataset.shape","e8408cfa":"#Dropping columns to check missingness\nprint(\"Shape before dropping\", dataset.shape)\nfor column in columns_incorrect:\n    dataset.drop((column+\"_missing\"), inplace=True,axis=1)\nprint(\"Shape after dropping\", dataset.shape)","806d4ef8":"dataset.head()","2f61e231":"imputer1 = SimpleImputer(strategy=\"mean\")\nimputer2 = SimpleImputer(strategy=\"mean\")\nimputer3 = SimpleImputer(strategy=\"median\")\nimputer4 = SimpleImputer(strategy=\"median\")\nimputer5 = SimpleImputer(strategy=\"median\")\n\n\ndataset[\"Glucose\"] = imputer1.fit_transform(dataset[\"Glucose\"].values.reshape(-1, 1)).copy()\ndataset[\"BloodPressure\"] = imputer2.fit_transform(dataset[\"BloodPressure\"].values.reshape(-1, 1)).copy()\ndataset[\"SkinThickness\"] = imputer3.fit_transform(dataset[\"SkinThickness\"].values.reshape(-1, 1)).copy()\ndataset[\"Insulin\"] = imputer4.fit_transform(dataset[\"Insulin\"].values.reshape(-1, 1)).copy()\ndataset[\"BMI\"] = imputer5.fit_transform(dataset[\"BMI\"].values.reshape(-1, 1)).copy()\n","96b91004":"dataset.isna().sum()","2b6e7590":"for feature in continuous_feature:\n    dataset[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","3a43d2bc":"sns.pairplot(dataset, hue=\"Outcome\")","25b84707":"fig=plt.figure(figsize=(10,7))\nbackgroundcolor='#f6f5f7'\nfig.patch.set_facecolor(backgroundcolor)\nsns.heatmap(data=dataset.corr(),annot=True,cmap='OrRd')","4f9b2de1":"def get_redundant_pairs(df):\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(dataset, 20))","a31d136f":"sns.set(font_scale=1)\nLABELS = [\"Negative\", \"Positive\"]\ncount_classes = pd.value_counts(dataset['Outcome'], sort = True)\ncount_classes.plot(kind = 'pie', rot=0)\nplt.title(\"Visualization of Value of Label\")\nplt.xticks(range(2), LABELS)\nplt.ylabel(\"Frequency\")","9d94a8e8":"dataset.groupby('Outcome').mean()","6654ef38":"y = dataset['Outcome']\ndataset.drop(columns=['Outcome'],inplace=True)\nX= dataset\n","b0d8f1b2":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=54,stratify=y)","16de87bb":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","1581b533":"features_response = dataset.columns.tolist()","3ac745cd":"[f_stat,f_p_value] = f_classif(X_train, y_train)","3b804086":"f_test_df = pd.DataFrame({'Feature': features_response,'F-statistic': f_stat, 'p-value': f_p_value})\nf_test_df.sort_values('p-value')\n","f3a9d849":"lr = LogisticRegression(random_state=42)","914c7506":"param_lr = dict()\nparam_lr['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nparam_lr['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nparam_lr['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100,1e+3]","64059350":"lr_search = GridSearchCV(lr, param_lr, scoring='accuracy', n_jobs=-1, cv=5)\nlr_search.fit(X_train, y_train)","b187b67e":"lr_search.best_params_\n","b635733f":"lr = LogisticRegression(penalty = \"none\", solver= \"newton-cg\",C=1e-05,random_state=42)\nlr.fit(X_train, y_train)","38c253e0":"y_pred = lr.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)","a1e5d7b7":"accuracy","d5e82379":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","da046f60":"knn = KNeighborsClassifier()\nparam_knn = {'n_neighbors':np.arange(2, 50)}  \ngrid_knn = GridSearchCV(knn, param_grid=param_knn,scoring='accuracy', cv=5)\n\ngrid_knn.fit(X_train, y_train)\n\n","101a46ed":"grid_knn.best_params_","708d542b":"knn = KNeighborsClassifier(n_neighbors= 13)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)","c57bfe23":"accuracy","10bc8cc6":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","c255f2c2":"dt = DecisionTreeClassifier()\nparam_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=param_dt, cv=5)\ngrid_dt.fit(X_train, y_train)","eeb5c852":"grid_dt.best_params_","7eb80a81":"dt = DecisionTreeClassifier(criterion= 'entropy', max_depth= 5, min_samples_leaf= 20)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy\n","90b42f01":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","de52e478":"rf = RandomForestClassifier(random_state=42)\nparam_rf = {'n_estimators':[100, 350, 500], 'min_samples_leaf':[2, 10, 30]}\ngrid_rf = GridSearchCV(rf, param_grid=param_rf, cv=5)\ngrid_rf.fit(X_train, y_train)","f5669b51":"grid_rf.best_params_","ab8a45eb":"rf = RandomForestClassifier(min_samples_leaf= 10, n_estimators= 100,random_state=42)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","0f4e815e":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","82ab8c4d":"svc = SVC(random_state=42)","e0b0bf12":"param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}","65415f31":"grid_svc = GridSearchCV(svc, param_grid=param_grid, cv=5)\ngrid_svc.fit(X_train, y_train)","74f2eb25":"grid_svc.best_params_","1794307f":"svc = SVC(random_state=42,C= 100, gamma= 0.01, kernel= 'sigmoid')","6205ec7c":"svc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy","56b8d34b":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","19ab5710":"from sklearn.naive_bayes import GaussianNB\n","91043527":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\ny_pred = gnb.predict(X_test)\naccuracy_score(y_test,y_pred)","5bf217a3":"sns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","c21da432":"#### Using a threshold of 0.7, we conclude that there is no multicollinearity between any independent variables. As such, we do not drop any features to resolve this. I will get back to this later during feature selection.","c993e32d":"### There are no null values. Hence, we do not require filling or dropping of any rows.","365c3583":"### The data has been obtained from the National Institute of Diabetes and Digestive and Kidney Diseases in India.","8d553a19":"## Random Forest Classifier","a3e1ba8a":"#### There are a lot of instances with a positive outcome for Diabetes (Taking a threshold of 1:10). Hence, due to it not being rare, balancing the dataset is not necessary","0e93394b":"## SVM","f0311419":"#### Skin thickness and BMI seem to be correlated. No other pair of independent variables seems to be highly correlated","3e828bea":"# 4. Modelling","9b41b64d":"There are no independent Categorical variables. Hence, none need to be one hot encoded, and are apt in their current float\/integer form","a95dcc58":"#### Some instances are observed to have Glucose\/ Skin Thickness\/ BMI\/ Blood Pressure equal to 0. This is biologically impossible for a living human. The simple answer would be to drop the rows with any of these features equal to 0. However, this can lead to a major loss of information or bias. As such, we must devise a way to deal with the missing values whilst abstaining from dropping the respective rows. We also avoid removing columns with a high number of null values as they are all crucial to our analysis\n\n\n#### I advoacte for imputation of data rather than removal of data","03570a59":"#### The  \ud835\udf122 could be used to test goodness of fit, homogenity test and independence test. The latter will be used in this case to figure out if the missigness of data in the Height column is dependent (or not) on the other variables (columns).\n\n#### The test starts by stating a first hypothesis called (the null hypothesis) and calculates a measure of closness between the observed data and the expected data (in the case where the null hypothesis is satisfied).\n\n#### The null hypothesis in this case is the following: There is no association between the missingness in the Height column and the dependent variable\n\n#### Alpha = 0.05","3631dcc7":"#### Groups of people with higher number of pregnancies tend to have a higher diabetes rate.\n\n#### Younger cohorts have a lower diabetes rate as oppoed to older people","7ce2e2d0":"There are 9 columns and 768 rows","a95e33d5":"# 2. Data Exploration","d998e2bb":"## Naive Bayes","a86aed6c":"#### Glucose and BMI seem to be the most useful determiners for predicting diabetes. All predictors seem to be related with the response variable, and thus will be useful in our model ","1bdc1fdf":"## Decision Trees","cd654bbf":"# SVM is the best model with an accuracy of 78%","e3514fcf":"### F-test","e901c210":"# 3. Scaling the features","bd8782ac":"# 1. Importing relevant libraries and dataset","81fcea96":"### Train-test split","61f6d5bb":"## Logistic Regression","b5d195b4":"#### Since all p-values are > than 0.05, we do not reject the null hypothesis, and thus there is no relationship between the missingness of any data and whether the person is diabetic. Although it is hard to tell with certitude whether the data is missing at random or not, the above test tells us that there is no evidence to tell that the data is not missing at random baleful for the outcome of interest.\n\n#### Hence, safely assuming that the data is missing at random, we can imputate the missing values using the mean and median.\n\n#### Since Glucose and BloodPressure follow a normal distribution and are without outliers, missing data for these variables can be imputated using the mean. Since the other three columns \u2013 SkinThickness, BMI and Insulin have presence of outliers, the missing values will be imputated with the mean for these.","26c4f644":"#### We notice that at an average people diabetes tend to have a higher number of pregnancies, higher glucose levels, higher blood pressure, thicker skin, a higher score on the insulin test, higher BMI levels, and a higher age.","87994799":"## K-nearest Neighbours"}}