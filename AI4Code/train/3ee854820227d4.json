{"cell_type":{"511ea2b5":"code","ba6a0b48":"code","6337ecc5":"code","4d516fe5":"code","d7de2c40":"code","6f57f8e5":"code","b81f675e":"code","08ef5259":"code","727cf32d":"code","2a827752":"code","ddc5a86d":"code","d76a7082":"code","253a9175":"code","beb0a5f6":"code","84ae01e8":"code","978a5612":"code","f5b1996f":"code","ea79a08f":"code","d9c35436":"code","975795c5":"code","8e7d6f35":"markdown","7397c50b":"markdown","1d77200c":"markdown","774cf0b1":"markdown","a592fba1":"markdown","c9a790c1":"markdown"},"source":{"511ea2b5":"# clone git repo\n!git clone https:\/\/github.com\/luopeixiang\/im2latex.git","ba6a0b48":"# change directory to the cloned repo\n%cd im2latex","6337ecc5":"# install dependencies -> this will conflict with local dependencies and some warning will be thrown\n!pip install Distance==0.1.3\n!pip install nltk==3.4.5\n!pip install numpy==1.17.2\n!pip install Pillow==6.1.0\n!pip install six==1.12.0\n!pip install torch==1.2.0\n!pip install torchvision==0.4.0\n!pip install tqdm==4.36.1","4d516fe5":"# download data\n%cd data\n!wget http:\/\/lstm.seas.harvard.edu\/latex\/data\/im2latex_validate_filter.lst\n!wget http:\/\/lstm.seas.harvard.edu\/latex\/data\/im2latex_train_filter.lst\n!wget http:\/\/lstm.seas.harvard.edu\/latex\/data\/im2latex_test_filter.lst\n!wget http:\/\/lstm.seas.harvard.edu\/latex\/data\/formula_images_processed.tar.gz\n!wget http:\/\/lstm.seas.harvard.edu\/latex\/data\/im2latex_formulas.norm.lst\n!tar -zxvf formula_images_processed.tar.gz","d7de2c40":"# preprocess data\n%cd ..\n!python preprocess.py","6f57f8e5":"from os.path import join\nimport pickle as pkl\nfrom collections import Counter\nimport argparse\n\n\nSTART_TOKEN = 0\nPAD_TOKEN = 1\nEND_TOKEN = 2\nUNK_TOKEN = 3\n\n# buid sign2id\nclass Vocab(object):\n    def __init__(self):\n        self.sign2id = {\"<s>\": START_TOKEN, \"<\/s>\": END_TOKEN,\n                        \"<pad>\": PAD_TOKEN, \"<unk>\": UNK_TOKEN}\n        self.id2sign = dict((idx, token)\n                            for token, idx in self.sign2id.items())\n        self.length = 4\n\n    def add_sign(self, sign):\n        if sign not in self.sign2id:\n            self.sign2id[sign] = self.length\n            self.id2sign[self.length] = sign\n            self.length += 1\n\n    def __len__(self):\n        return self.length\n    \n    def __repr__(self):\n        return \"Vocabulary (n = {})\".format(len(self.sign2id))","b81f675e":"min_count = 10\nvocab = Vocab()\ncounter = Counter()\n\ndata_dir = \"data\"\nformulas_file = join(data_dir, 'im2latex_formulas.norm.lst')\nwith open(formulas_file, 'r') as f:\n    formulas = [formula.strip('\\n') for formula in f.readlines()]\n\nwith open(join(data_dir, 'im2latex_train_filter.lst'), 'r') as f:\n    for line in f:\n        _, idx = line.strip('\\n').split()\n        idx = int(idx)\n        formula = formulas[idx].split()\n        counter.update(formula)\n\nfor word, count in counter.most_common():\n    if count >= min_count:\n        vocab.add_sign(word)","08ef5259":"from functools import partial\n\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom model import Im2LatexModel, Trainer\nfrom utils import collate_fn, get_checkpoint\nfrom data import Im2LatexDataset","727cf32d":"batch_size = 32\nmax_len = 150\nuse_cuda = False\ndata_path = \"data\/\"\ntrain_loader = DataLoader(\n    Im2LatexDataset(data_path, 'train', max_len),\n    batch_size=batch_size,\n    collate_fn=partial(collate_fn, vocab.sign2id),\n    pin_memory=True if use_cuda else False,\n    num_workers=4)","2a827752":"sample = next(iter(train_loader))","ddc5a86d":"sample[0].shape, sample[1].shape","d76a7082":"import matplotlib.pyplot as plt\n%matplotlib inline","253a9175":"plt.imshow(sample[0][1, 0, :, :], cmap=\"gray\")","beb0a5f6":"sample[1][1]","84ae01e8":"print([vocab.id2sign[int(e)] for e in sample[1][1]])","978a5612":"val_loader = DataLoader(\n    Im2LatexDataset(data_path, 'validate', max_len),\n    batch_size=batch_size,\n    collate_fn=partial(collate_fn, vocab.sign2id),\n    pin_memory=True if use_cuda else False,\n    num_workers=4)","f5b1996f":"vocab_size = len(vocab)\ndropout = 0.2\nemb_dim = 80\ndec_rnn_h = 512\nadd_position_features = False\nmodel = Im2LatexModel(\n    vocab_size, emb_dim, dec_rnn_h,\n    add_pos_feat=add_position_features,\n    dropout=dropout\n)\nmodel = model.to(\"cpu\")","ea79a08f":"lr_decay = 0.5\nlr = 3e-4\nlr_patience = 3\nmin_lr = 3e-5\noptimizer = optim.Adam(model.parameters(), lr=lr)\nlr_scheduler = ReduceLROnPlateau(\n    optimizer,\n    \"min\",\n    factor=lr_decay,\n    patience=lr_patience,\n    verbose=True,\n    min_lr=min_lr\n)","d9c35436":"class Arguments:\n    def __init__(self, decay_k=1.0, clip=2.0,\n                 sample_method='teacher_forcing', save_dir='.\/ckpts',\n                 print_freq=100):\n        self.decay_k = decay_k\n        self.clip = clip\n        self.sample_method = sample_method\n        self.save_dir = save_dir\n        self.print_freq = print_freq\nargs = Arguments()","975795c5":"epoches = 10\ntrainer = Trainer(optimizer, model, lr_scheduler,\n                  train_loader, train_loader, args,\n                  use_cuda=use_cuda,\n                  init_epoch=1, last_epoch=epoches)\n# begin training\ntrainer.train()","8e7d6f35":"## **Train**","7397c50b":"## **Build vocab**","1d77200c":"## **Create model and optimizer**","774cf0b1":"## **Beginning**\n\nload data and run `python preprocess.py` before running this notebook","a592fba1":"## **Train model**","c9a790c1":"## **Load data**"}}