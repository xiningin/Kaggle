{"cell_type":{"d55f2907":"code","78b9ea46":"code","141a5fb8":"code","e2b44dd5":"code","d65248e0":"code","95cdc5cf":"code","07e71e39":"code","d044e84a":"code","4d3286ea":"code","6b8a721d":"code","1a50bf76":"code","4273ab4c":"code","ddccd983":"code","eba1033e":"code","7d7391a4":"code","bc1f0797":"code","65dfb6b4":"code","49c54674":"code","20ab5762":"code","8b255fc2":"code","dc562d79":"code","4924641e":"code","872f6af2":"code","79f07c14":"code","aa8bdecf":"code","c604dd1e":"code","f906147b":"code","ac523dff":"code","0a87f697":"code","7e16be23":"code","f891d061":"code","93d1c687":"code","440cda17":"code","68b9ad56":"code","0c206ce8":"code","81791330":"code","89bd06ae":"code","c9401f69":"code","198ced99":"code","b579d869":"code","c4773c26":"code","5997eccc":"code","4eb0be99":"code","4d014dc7":"code","19011155":"code","23163de7":"code","384fdddc":"code","986011e9":"code","08f28f90":"code","f0d9854e":"code","dbab3e88":"code","926c48f2":"code","f913c704":"code","2076c0a5":"code","0b79f093":"code","971e33c7":"code","287ae51d":"code","2949f7fc":"code","bd692c19":"code","f14f85f2":"code","50f5c294":"code","f080a58c":"code","e38f8eff":"code","002c651f":"code","a0b54095":"code","1a5d2204":"code","f025a1d6":"code","ad737f9d":"code","aa5b7662":"code","db0999bf":"code","421b830a":"code","3f2fbcfb":"code","21da9ad2":"code","0cdae8ba":"code","14538c6f":"code","645463f2":"code","1183d8b8":"code","00e4a338":"code","bcd309c5":"code","3b68dea1":"code","6a8756c9":"code","965793c0":"code","621dd4c6":"code","0c508ca2":"code","9ed13f60":"code","b21e16f1":"code","7b72eb15":"markdown","0dfbcb00":"markdown","b600a5d4":"markdown","d72130d6":"markdown","e4a1bfd1":"markdown","9cb76e14":"markdown","817c1dfb":"markdown","8b018822":"markdown","6ad34767":"markdown","7e7b3736":"markdown","a2959fd9":"markdown","82297e75":"markdown","3ddd9a81":"markdown","ca0a1539":"markdown","149af084":"markdown","2354cb77":"markdown","c0f680da":"markdown","e4b56f02":"markdown","10fb00c1":"markdown","a21f483a":"markdown","0af95db9":"markdown","2d4bb1a8":"markdown","39396866":"markdown","74e00ccd":"markdown","c2c4ad8d":"markdown","6ebcbca5":"markdown","99100d30":"markdown","1bb60511":"markdown","4e7920f3":"markdown","225a2fc7":"markdown","412a321a":"markdown","32aa8610":"markdown","322d8e50":"markdown","56a21b72":"markdown","35416c0f":"markdown","552cdb9c":"markdown","21367ad8":"markdown","358fb037":"markdown","b1f0bfcc":"markdown","3e2f4925":"markdown","547d95b1":"markdown","f3e81d8d":"markdown","53940b80":"markdown","cf58b53a":"markdown","de37a79c":"markdown","ef292691":"markdown","58f9225f":"markdown","227fe3aa":"markdown","f290ffa7":"markdown"},"source":{"d55f2907":"from sklearn.linear_model import LogisticRegression\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n!pip install seaborn --upgrade\nimport seaborn as sns\nimport random\nimport math\nfrom sklearn.model_selection import train_test_split\n\nSEED = 299458792","78b9ea46":"train_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', low_memory=False, nrows=10**5, \n                       dtype={'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', 'content_id': 'int16', 'content_type_id': 'int8',\n                              'task_container_id': 'int16', 'user_answer': 'int8', 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n                             'prior_question_had_explanation': 'boolean',\n                             }\n                      )\n\n#train_df = dt.fread(\"..\/input\/riiid-test-answer-prediction\/train.csv\").to_pandas()","141a5fb8":"print(\"Train size:\", train_df.shape)","e2b44dd5":"train_df.head()","d65248e0":"questions = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\nquestions['tags'] = questions['tags'].astype('category') \nquestions[['question_id', 'bundle_id','correct_answer','part']] = questions[['question_id', 'bundle_id','correct_answer','part']].apply(pd.to_numeric, downcast='unsigned')","95cdc5cf":"print(\"Questions:\", questions.shape)","07e71e39":"questions.head()","d044e84a":"lectures = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')\nlectures['type_of'] = lectures['type_of'].astype('category') \nlectures[['lecture_id', 'tag','part']] = lectures[['lecture_id', 'tag','part']].apply(pd.to_numeric, downcast='unsigned')","4d3286ea":"print(\"Lectures:\", lectures.shape)","6b8a721d":"lectures.head()","1a50bf76":"train_df.describe()","4273ab4c":"train_df.isnull().sum()","ddccd983":"print(train_df.dtypes)","eba1033e":"for col in train_df.columns:\n  print('Number of Unique variables in ' + col+':',len(train_df[col].unique()))","7d7391a4":"train_df.head()","bc1f0797":"train_user_grouped=train_df.groupby(['user_id']).apply(lambda x: pd.Series({\n      'total_answered_correctly'       : x['answered_correctly'].sum(),\n      'total_answered'       : x['answered_correctly'].count(),\n      'fraction_answered_correctly'      : x['answered_correctly'].sum()\/x['answered_correctly'].count(),\n      'total_lectures' : x['content_type_id'].sum(),\n      'median_time_to_answer' : x['prior_question_elapsed_time'].median(),\n      'std_time_to_answer' : x['prior_question_elapsed_time'].std()}))","65dfb6b4":"train_user_grouped=train_user_grouped.loc[train_user_grouped.loc[:,'total_answered']>=100,:]","49c54674":"train_user_grouped.sort_values(by=['fraction_answered_correctly'],ascending=False).head(5)","20ab5762":"train_user_grouped.sort_values(by=['fraction_answered_correctly'],ascending=True).head(5)","8b255fc2":"train_user_grouped.sort_values(by=['median_time_to_answer'],ascending=True).head(5)","dc562d79":"train_user_grouped.sort_values(by=['median_time_to_answer'],ascending=False).head(5)","4924641e":"def boxplot_sorted(df, by, column, ax, rot=90):\n    # use dict comprehension to create new dataframe from the iterable groupby object\n    # each group name becomes a column in the new dataframe\n    df2 = pd.DataFrame({col:vals[column] for col, vals in df.groupby(by)})\n    # find and sort the median values in this new dataframe\n    meds = df2.median().sort_values()\n    # use the columns in the dataframe, ordered sorted by median value\n    # return axes so changes can be made outside the function\n    return df2[meds.index].boxplot(rot=rot,ax=ax,return_type=\"axes\")\n\nfig, ax = plt.subplots(figsize=(50,8))\nboxplot_sorted(train_df, by = ['user_id'], column = 'prior_question_elapsed_time', ax=ax)\nplt.xlabel('user_id')\nplt.ylabel('prior_question_elapsed_time')\nplt.title('Boxplots')\nplt.suptitle('')\nplt.show()","872f6af2":"sns.histplot(train_user_grouped['fraction_answered_correctly'])","79f07c14":"sns.histplot(train_user_grouped['median_time_to_answer'])","aa8bdecf":"questions.describe()","c604dd1e":"questions.isnull().sum()","f906147b":"print(questions.dtypes)","ac523dff":"for col in questions.columns:\n  print('Number of Unique variables in ' + col+':',len(questions[col].unique()))","0a87f697":"questions.tail(100)","7e16be23":"sns.pairplot(questions.iloc[:,:-2])","f891d061":"questions_bundle_grouped=questions.groupby(['bundle_id']).apply(lambda x: pd.Series({\n      'total_questions'       : x['question_id'].count()}))","93d1c687":"sns.histplot(questions_bundle_grouped['total_questions'])","440cda17":"questions_part_grouped=questions.groupby(['part']).apply(lambda x: pd.Series({\n      'total_questions'       : x['question_id'].count()}))","68b9ad56":"questions_part_grouped","0c206ce8":"questions_tags_grouped=questions.groupby(['tags']).apply(lambda x: pd.Series({\n      'total_questions'       : x['question_id'].count()}))","81791330":"sns.histplot(questions_tags_grouped['total_questions'])","89bd06ae":"questions_tags_grouped.sort_values(by=['total_questions'],ascending=False).head(15)","c9401f69":"lectures.describe()","198ced99":"lectures.isnull().sum()","b579d869":"print(lectures.dtypes)","c4773c26":"for col in lectures.columns:\n  print('Number of Unique variables in ' + col+':',len(lectures[col].unique()))","5997eccc":"lectures_tag_grouped=lectures.groupby(['tag']).apply(lambda x: pd.Series({\n      'total_lectures'       : x['lecture_id'].count()}))","4eb0be99":"sns.histplot(lectures_tag_grouped['total_lectures'])","4d014dc7":"lectures_tag_grouped.sort_values(by=['total_lectures'],ascending=False).head(15)","19011155":"lectures_part_grouped=lectures.groupby(['part']).apply(lambda x: pd.Series({\n      'total_lectures'       : x['lecture_id'].count()}))","23163de7":"lectures_part_grouped","384fdddc":"lectures_type_grouped=lectures.groupby(['type_of']).apply(lambda x: pd.Series({\n      'total_lectures'       : x['lecture_id'].count()}))","986011e9":"sns.histplot(lectures_type_grouped['total_lectures'])","08f28f90":"lectures_type_grouped.sort_values(by=['total_lectures'],ascending=False).head(15)","f0d9854e":"#columns for analysis, only select columns needed to prevent out of memory\ncols_2_read = ['timestamp','content_id','user_id','content_type_id','answered_correctly','prior_question_elapsed_time','prior_question_had_explanation']#,'row_id','task_container_id','user_answer','prior_question_had_explanation']\n\n#column type to minimize memory usage\nall_dtype_dic = {'row_id': 'uint64', 'timestamp': 'uint64', 'user_id': 'uint32', 'content_id': 'uint16', 'content_type_id': 'uint8',\n                              'task_container_id': 'uint16', 'user_answer': pd.CategoricalDtype([-1,0,1,2,3]), 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n                             'prior_question_had_explanation': 'boolean',\n                             }\n#dictionary to look up column type based on column selected\ncols_2_read_dtype_dic = {k: all_dtype_dic[k] for k in all_dtype_dic.keys() & cols_2_read}\ntrain_df = pd.DataFrame()\n\n#read dataset in chunks to prevent out-of-memory\nfor chunk in pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', low_memory=False,nrows=10**7,\n                       dtype=cols_2_read_dtype_dic,usecols = cols_2_read,chunksize=10**5 \n                      ):\n    train_df = pd.concat([train_df, chunk], ignore_index=True)","dbab3e88":"train_df['user_id'].describe()","926c48f2":"train_df.describe()","f913c704":"# data split to have 70% of train, 30% of tune + test\ntrain_df, validation_test_df = train_test_split(train_df,\n                                          test_size=0.3,\n                                          random_state=SEED,\n                                          shuffle=True,\n                                          stratify=None)\n","2076c0a5":"# further evenly split between tune and test\nvalidation_df, test_df = train_test_split(validation_test_df,\n                                    test_size=0.5,\n                                    random_state=SEED,\n                                    shuffle=True,\n                                    stratify=None)","0b79f093":"#reset index of created dataframes\ntrain_df=train_df.reset_index(drop=True)\ntest_df=test_df.reset_index(drop=True)\nvalidation_df=validation_df.reset_index(drop=True)","971e33c7":"#check shape\nprint(train_df.shape)\nprint(validation_df.shape)\nprint(test_df.shape)","287ae51d":"questions = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\nquestions['tags'] = questions['tags'].astype('category') \nquestions[['question_id', 'bundle_id','correct_answer','part']] = questions[['question_id', 'bundle_id','correct_answer','part']].apply(pd.to_numeric, downcast='unsigned')\n\nlectures = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')\nlectures['type_of'] = lectures['type_of'].astype('category') \nlectures[['lecture_id', 'tag','part']] = lectures[['lecture_id', 'tag','part']].apply(pd.to_numeric, downcast='unsigned')","2949f7fc":"def concat_questions_lectures(train_df,questions,lectures):\n   ##questions\n   train_questions = train_df.loc[train_df.loc[:,'content_type_id']==0,:]\n   train_questions=train_questions.set_index('content_id')\n   questions = questions.rename(columns={\"question_id\": \"content_id\",'tags':'question_tags'})\n   questions = questions.set_index('content_id') \n   train_questions=train_questions.join(questions, how='left')\n   #lectures\n   train_lectures = train_df.loc[train_df.loc[:,'content_type_id']==1,:]\n   train_lectures=train_lectures.set_index('content_id')\n   lectures = lectures.rename(columns={\"lecture_id\": \"content_id\",'tag':'lecture_tag'})\n   lectures = lectures.set_index('content_id') \n   train_lectures=train_lectures.join(lectures, how='left')\n   #lectures and questions\n   train_questions_lectures= pd.concat([train_questions,train_lectures], axis=0)\n   train_questions_lectures.reset_index(inplace=True)\n   train_questions_lectures = train_questions_lectures.rename(columns = {'index':'content_id'})\n   return train_questions_lectures\n\ntrain_df = concat_questions_lectures(train_df,questions,lectures)\nvalidation_df = concat_questions_lectures(validation_df,questions,lectures)\ntest_df = concat_questions_lectures(test_df,questions,lectures)","bd692c19":"train_df.isnull().sum()","f14f85f2":"train_df.dtypes","50f5c294":"train_df[['type_of','question_tags','lecture_tag']] = train_df[['type_of','question_tags','lecture_tag']].astype('category') \ntrain_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']] = train_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']].apply(pd.to_numeric, downcast='unsigned')\n\nvalidation_df[['type_of','question_tags','lecture_tag']] = validation_df[['type_of','question_tags','lecture_tag']].astype('category') \nvalidation_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']] = validation_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']].apply(pd.to_numeric, downcast='unsigned')\n\ntest_df[['type_of','question_tags','lecture_tag']] = train_df[['type_of','question_tags','lecture_tag']].astype('category') \ntest_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']] = test_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']].apply(pd.to_numeric, downcast='unsigned')","f080a58c":"##fill prior_question_elapsed_time\ntrain_df['prior_question_elapsed_time'] = train_df['prior_question_elapsed_time'].fillna(0)\nvalidation_df['prior_question_elapsed_time'] = validation_df['prior_question_elapsed_time'].fillna(0)\ntest_df['prior_question_elapsed_time'] = test_df['prior_question_elapsed_time'].fillna(0)\n\n##prior_question_had_explanation\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].fillna(False)\nvalidation_df['prior_question_had_explanation'] = validation_df['prior_question_had_explanation'].fillna(False)\ntest_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(False)\n\n##fill rows for questions which were lectures\ntrain_df['question_tags']=train_df['question_tags'].cat.add_categories(-1)\ntrain_df[['bundle_id','correct_answer','question_tags']] = train_df[['bundle_id','correct_answer','question_tags']].fillna(-1)\nvalidation_df['question_tags']=validation_df['question_tags'].cat.add_categories(-1)\nvalidation_df[['bundle_id','correct_answer','question_tags']] = validation_df[['bundle_id','correct_answer','question_tags']].fillna(-1)\ntest_df['question_tags']=test_df['question_tags'].cat.add_categories(-1)\ntest_df[['bundle_id','correct_answer','question_tags']] = test_df[['bundle_id','correct_answer','question_tags']].fillna(-1)\n\n##fill rows for lectures which were questions\ntrain_df['lecture_tag']=train_df['lecture_tag'].cat.add_categories(-1)\ntrain_df[['lecture_tag']] = train_df[['lecture_tag']].fillna(-1)\nvalidation_df['lecture_tag']=validation_df['lecture_tag'].cat.add_categories(-1)\nvalidation_df[['lecture_tag']] = validation_df[['lecture_tag']].fillna(-1)\ntest_df['lecture_tag']=test_df['lecture_tag'].cat.add_categories(-1)\ntest_df[['lecture_tag']] = test_df[['lecture_tag']].fillna(-1)\n\n##fill type_of as questions which questions\ntrain_df['type_of']=train_df['lecture_tag'].cat.add_categories('question')\ntrain_df[['type_of']] = train_df[['type_of']].fillna('question')\nvalidation_df['type_of']=validation_df['lecture_tag'].cat.add_categories('question')\nvalidation_df[['type_of']] = validation_df[['type_of']].fillna('question')\ntest_df['type_of']=test_df['lecture_tag'].cat.add_categories('question')\ntest_df[['type_of']] = test_df[['type_of']].fillna('question')","e38f8eff":"train_df.isnull().sum()","002c651f":"train_df.dtypes","a0b54095":"train_df['user_interaction_lag'] = train_df.sort_values(['user_id','timestamp']).groupby('user_id')['timestamp'].diff()\nvalidation_df['user_interaction_lag'] = validation_df.sort_values(['user_id','timestamp']).groupby('user_id')['timestamp'].diff()\ntest_df['user_interaction_lag'] = test_df.sort_values(['user_id','timestamp']).groupby('user_id')['timestamp'].diff()","1a5d2204":"def user_answer_stats(df):\n  questions_only_df = df[df['answered_correctly']!=-1]\n  questions_only_df = questions_only_df[questions_only_df['user_id'].notna()]\n  grouped_by_user_df = questions_only_df.groupby('user_id')\n  user_answers_df = grouped_by_user_df.agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'skew']}).copy()\n  user_answers_df.columns = ['mean_user_accuracy', 'questions_answered', 'std_user_accuracy', 'median_user_accuracy', 'skew_user_accuracy']\n  user_answers_df.reset_index(inplace=True)\n  user_answers_df = user_answers_df.rename(columns = {'index':'user_id'})\n  return user_answers_df\n\ntrain_df = train_df.merge(user_answer_stats(train_df), how='left', on='user_id')\nvalidation_df = validation_df.merge(user_answer_stats(validation_df), how='left', on='user_id')\ntest_df = test_df.merge(user_answer_stats(test_df), how='left', on='user_id')","f025a1d6":"def content_answer_stats(df):\n  questions_only_df = df[df['answered_correctly']!=-1]\n  questions_only_df = questions_only_df[questions_only_df['content_id'].notna()]\n  grouped_by_content_df = questions_only_df.groupby('content_id')\n  content_answers_df = grouped_by_content_df.agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'skew']}).copy()\n  content_answers_df.columns = ['mean_accuracy', 'questions_asked', 'std_accuracy', 'median_accuracy', 'skew_accuracy']\n  content_answers_df.reset_index(inplace=True)\n  content_answers_df = content_answers_df.rename(columns = {'index':'content_id'})\n  return content_answers_df\n\ntrain_df = train_df.merge(content_answer_stats(train_df), how='left', on='content_id')\nvalidation_df = validation_df.merge(content_answer_stats(validation_df), how='left', on='content_id')\ntest_df = test_df.merge(content_answer_stats(test_df), how='left', on='content_id')","ad737f9d":"train_df.info()","aa5b7662":"features = [\n    'mean_user_accuracy', \n    'questions_answered',\n    'std_user_accuracy', \n    'median_user_accuracy',\n    'skew_user_accuracy',\n    'mean_accuracy', \n    'questions_asked',\n    'std_accuracy', \n    'median_accuracy',\n    'prior_question_elapsed_time', \n    'prior_question_had_explanation',\n    'skew_accuracy',\n]\n\ntarget = 'answered_correctly'","db0999bf":"train_df = train_df[train_df['answered_correctly']!=-1][features + [target]]\nvalidation_df = validation_df[validation_df['answered_correctly']!=-1][features + [target]]\ntest_df = test_df[test_df['answered_correctly']!=-1][features + [target]]","421b830a":"train_df.isnull().sum()","3f2fbcfb":"train_df = train_df.replace([np.inf, -np.inf], np.nan)\ntrain_df = train_df.fillna(0.5)\n\nvalidation_df = validation_df.replace([np.inf, -np.inf], np.nan)\nvalidation_df = validation_df.fillna(0.5)\n\ntest_df = test_df.replace([np.inf, -np.inf], np.nan)\ntest_df = test_df.fillna(0.5)\n\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype('bool')\nvalidation_df['prior_question_had_explanation'] = validation_df['prior_question_had_explanation'].astype('bool')\ntest_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype('bool')","21da9ad2":"train_df.dtypes","0cdae8ba":"import lightgbm as lgb\nimport optuna","14538c6f":"def create_model(trial):\n    num_leaves = trial.suggest_int(\"num_leaves\", 2, 31)\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n    max_depth = trial.suggest_int('max_depth', 3, 8)\n    min_child_samples = trial.suggest_int('min_child_samples', 100, 1200)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0001, 0.99)\n    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 90)\n    bagging_fraction = trial.suggest_uniform('bagging_fraction', 0.0001, 1.0)\n    feature_fraction = trial.suggest_uniform('feature_fraction', 0.0001, 1.0)\n    model = lgb.LGBMClassifier(\n        num_leaves=num_leaves,\n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        min_child_samples=min_child_samples, \n        min_data_in_leaf=min_data_in_leaf,\n        learning_rate=learning_rate,\n        feature_fraction=feature_fraction,\n        random_state=666\n)\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(train_df[features], train_df[target])\n    score = roc_auc_score(test_df[target].values, model.predict_proba(test_df[features])[:,1])\n    return score\n\n# # uncomment to use optuna\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=70)\n# params = study.best_params\n# params['random_state'] = 666\n\nparams = {'num_leaves': 6, 'n_estimators': 242, 'max_depth': 3, \n          'min_child_samples': 352, 'learning_rate': 0.1954705382751018, 'min_data_in_leaf': 72, \n          'bagging_fraction': 0.2654709578619099, 'feature_fraction': 0.4901470199766588} #0.7714113571421728 0.7717271921771559  0.7711878055353522\n\nmodel = lgb.LGBMClassifier(**params)\nmodel.fit(train_df[features], train_df[target])","645463f2":"print('LGB score: ', roc_auc_score(validation_df[target].values, model.predict_proba(validation_df[features])[:,1]))","1183d8b8":"print('LGB score: ', roc_auc_score(test_df[target].values, model.predict_proba(test_df[features])[:,1]))","00e4a338":"cols_2_read = ['timestamp','content_id','user_id','content_type_id','answered_correctly','prior_question_elapsed_time','prior_question_had_explanation']#,'row_id','task_container_id','user_answer','prior_question_had_explanation']\nall_dtype_dic = {'row_id': 'uint64', 'timestamp': 'uint64', 'user_id': 'uint32', 'content_id': 'uint16', 'content_type_id': 'uint8',\n                              'task_container_id': 'uint16', 'user_answer': pd.CategoricalDtype([-1,0,1,2,3]), 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n                             'prior_question_had_explanation': 'boolean',\n                             }\ncols_2_read_dtype_dic = {k: all_dtype_dic[k] for k in all_dtype_dic.keys() & cols_2_read}\ntrain_df = pd.DataFrame()\nfor chunk in pd.read_csv('train.csv', low_memory=False,nrows=10**7,\n                       dtype=cols_2_read_dtype_dic,usecols = cols_2_read,chunksize=10**5 \n                      ):\n    train_df = pd.concat([train_df, chunk], ignore_index=True)","bcd309c5":"#add lectures and questions\ntrain_df = concat_questions_lectures(train_df,questions,lectures)","3b68dea1":"#change to numeric\ntrain_df[['type_of','question_tags','lecture_tag']] = train_df[['type_of','question_tags','lecture_tag']].astype('category') \ntrain_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']] = train_df[['timestamp', 'user_id','content_id','content_type_id','answered_correctly','bundle_id','correct_answer','part']].apply(pd.to_numeric, downcast='unsigned')","6a8756c9":"#Feature engineering\n\n#user_interaction_lag\ntrain_df['user_interaction_lag'] = train_df.sort_values(['user_id','timestamp']).groupby('user_id')['timestamp'].diff()\n\n#content_answer_stats\ntrain_content_answer_stats_df = content_answer_stats(train_df)\n#train_content_answer_stats_df.to_csv('train_content_answer_stats_df.csv') # uncomment to save output to use for feature engineering of test dataset for competition submissions\ntrain_df = train_df.merge(train_content_answer_stats_df, how='left', on='content_id')\n\n#user answer stats\ntrain_user_answer_stats_df = user_answer_stats(train_df)\n#train_user_answer_stats_df.to_csv('train_user_answer_stats_df.csv') # uncomment to save output to use for feature engineering of test dataset for competition submissions\ntrain_df = train_df.merge(train_user_answer_stats_df, how='left', on='user_id')","965793c0":"train_df = train_df[train_df['answered_correctly']!=-1][features + [target]]","621dd4c6":"train_df.isnull().sum()","0c508ca2":"train_df.dtypes","9ed13f60":"#deal with the nulls\ntrain_df = train_df.replace([np.inf, -np.inf], np.nan)\ntrain_df = train_df.fillna(0.5)\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype('bool')","b21e16f1":"params = {'num_leaves': 6, 'n_estimators': 242, 'max_depth': 3, \n          'min_child_samples': 352, 'learning_rate': 0.1954705382751018, 'min_data_in_leaf': 72, \n          'bagging_fraction': 0.2654709578619099, 'feature_fraction': 0.4901470199766588}\n\nmodel = lgb.LGBMClassifier(**params)\nmodel.fit(train_df[features], train_df[target])\n#model.booster_.save_model('model.txt') # uncomment to save output to use for feature engineering of test dataset for competition submissions","7b72eb15":"## Data preparation","0dfbcb00":"### 1. What is the distribution of lectures per tag? \n","b600a5d4":"### 3. What is the distribution of questions per type_of? Which type has the most number of lectures? \n","d72130d6":"### 1. Who are the top 5 students?","e4a1bfd1":"## Final Train, Validation, and Test data","9cb76e14":"## Data split","817c1dfb":"<a id=\"top\"><\/a>\n# Table of contents\n\n*  [Load Libraries and Data](#1)\n*  [EDA using questions - train.csv](#2)\n*  [EDA using questions - questions.csv](#3)\n*  [EDA using questions - lectures.csv](#4)\n*  [Model Building - Data Split, Preparation and Hyperparameter tuning](#5)\n*  [Model Building -Training with full data](#6)","8b018822":"<a id=\"6\"><\/a>\n# Model Building -Training with full data","6ad34767":"questions.csv: metadata for the questions posed to users.\n\nquestion_id: foreign key for the train\/test content_id column, when the content type is question (0).\n\nbundle_id: code for which questions are served together.\n\ncorrect_answer: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n\npart: top level category code for the question.\n\ntags: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.","7e7b3736":"row_id: (int64) ID code for the row.\n\ntimestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n\nuser_id: (int32) ID code for the user.\n\ncontent_id: (int16) ID code for the user interaction\n\ncontent_type_id: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\ntask_container_id: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n\nuser_answer: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\nanswered_correctly: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\nprior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\nprior_question_had_explanation: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback","a2959fd9":"In this section the the model tuned in the previous step is trained using the full dataset. The relevant feature engineering tables are saved to csv. The model is saved to txt. The output will be loaded into a seperate notebook in preparation for Kaggle submission.","82297e75":"## Fill NaN values","3ddd9a81":"## Convert all to Numeric","ca0a1539":"The above code allows you to add only the columns needed for analysis.","149af084":"## Feature Engineering","2354cb77":"### 3. Who are the top 5 fastest students to answer question?","c0f680da":"### 2. What is the distribution of lectures per part? Which part has the most number of questions? Which part has the least number of questions?","e4b56f02":"<a id=\"5\"><\/a>\n# Model Building - Data Split, Preparation and Hyperparameter tuning","10fb00c1":"## Lets ask some simple questions about lectures.csv dataset\n\n1. What is the distribution of lectures per tag? \n2. What is the distribution of lectures per part? Which part has the most number of questions? Which part has the least number of questions?\n3. What is the distribution of questions per type_of? Which type has the most number of lectures? \n","a21f483a":"<a id=\"2\"><\/a>\n# EDA using questions - train.csv","0af95db9":"<a id=\"3\"><\/a>\n# EDA using questions - questions.csv","2d4bb1a8":"<a id=\"4\"><\/a>\n# EDA using questions - lectures.csv","39396866":"### 2. Who are the bottom 5 worst student?","74e00ccd":"<a id=\"1\"><\/a>\n# Load Libraries and Data","c2c4ad8d":"Yes, the distribution does have an signifhcant effect on the answer because the standard deviation is comparable to the median across all the students.","6ebcbca5":"row_id: (int64) ID code for the row.\n\ntimestamp: (int64) the time in milliseconds between this user interaction and the first event completion from that user.\n\nuser_id: (int32) ID code for the user.\n\ncontent_id: (int16) ID code for the user interaction\n\ncontent_type_id: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n\ntask_container_id: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n\nuser_answer: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n\nanswered_correctly: (int8) if the user responded correctly. Read -1 as null, for lectures.\n\nprior_question_elapsed_time: (float32) The average time in milliseconds it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n\nprior_question_had_explanation: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback","99100d30":"### Content Statistics","1bb60511":"## Features Selection","4e7920f3":"### 3. What is the distribution of questions per tags? ","225a2fc7":"## Load dataset for Model building","412a321a":"## Lets ask some simple questions for students who have answered atleast a 100 questions\n1. Who are the top 5 students?\n2. Who are the bottom 5 worst student?\n3. Who are the top 5 fastest students to answer question?\n4. Who are the slowest 5 students to answer questions? What metric was used? Does the distribution have an effect on the answer?\n5. What is the performance distirbution of the class based on answer correctness?\n6.What is the performance distirbution of the class based on speed correctness?\n","32aa8610":"## question.csv","322d8e50":"## Lets ask some simple questions about questions.csv dataset\n\n1. What is the distribution of questions per bundle? \n2. What is the distribution of questions per part? Which part has the most number of questions? Which part has the least number of questions?\n3. What is the distribution of questions per tags? Which tag has the most number of questions? \n","56a21b72":"## Perform a full join of train, questions, and lectures","35416c0f":"## Train full model and save","552cdb9c":"## lectures.csv","21367ad8":"### 5. What is the performance distirbution of the class based on answer correctness?","358fb037":"lectures.csv: metadata for the lectures watched by users as they progress in their education.\n\nlecture_id: foreign key for the train\/test content_id column, when the content type is lecture (1).\n\npart: top level category code for the lecture.\n\ntag: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n\ntype_of: brief description of the core purpose of the lecture","b1f0bfcc":"# Riiid! Answer Correctness Prediction\n## Track knowledge states of 1M+ students in the wild\n\nIn this competition, your challenge is to create algorithms for \"Knowledge Tracing,\" the modeling of student knowledge over time. The goal is to accurately predict how students will perform on future interactions. You will pair your machine learning skills using Riiid\u2019s EdNet data.\n\n**Note: Sections of the code are intended to be run seperately as such there are multiple load data cell at the appropriate breakpoints **","3e2f4925":"### Calculate Lag (difference in time between two consecutive interactions for each user)","547d95b1":"### 6. What is the performance distirbution of the class based on speed correctness?","f3e81d8d":"### 2. What is the distribution of questions per part? Which part has the most number of questions? Which part has the least number of questions?","53940b80":"## Load data","cf58b53a":"**This time we are going to split data into 3 parts: train (70%), tune (15%), and test (15%). The tune set is used to perform model selection and feature selection (more in latter lessons).**","de37a79c":"### User statistics","ef292691":"## LightGBM model Creation and Hyperparamter tuning","58f9225f":"## train.csv","227fe3aa":"### 4. Who are the slowest 5 students to answer questions? What metric was used? Does the distribution have an effect on the answer?","f290ffa7":"### 1.  What is the distribution of questions per bundle?"}}