{"cell_type":{"276b7c8c":"code","e8997506":"code","58435da9":"code","9841361b":"code","9e88926f":"code","1eb549f5":"code","b0752de7":"code","31d76df7":"code","aa568805":"code","342f8d5c":"code","8e65adc8":"code","0edf26fa":"code","3de95bf4":"code","f8dd65cf":"code","8a79b565":"code","ec882889":"code","0af2dbf9":"code","07e830b6":"code","7d89cb90":"code","9ac3a3f9":"code","5870d52d":"code","d7abc4a7":"code","f9f4bedd":"code","6b675a87":"code","6d0f0cef":"code","ddd9dd14":"code","d75e0431":"code","0f20c382":"code","6a20341a":"code","d68d7fd7":"code","cfaee4f8":"code","c91ae067":"code","8cfadcb9":"code","603553d1":"code","101aaecc":"code","fea66f21":"code","0ee983dc":"code","257cd2b6":"code","367d0f64":"code","616a6de3":"code","3064abba":"code","cdca5700":"code","12bde5fb":"code","99975791":"code","d89eecec":"code","0234f5d1":"code","594db644":"markdown","4e9c84a6":"markdown","e6420eda":"markdown","bbc73f17":"markdown","7589eb10":"markdown","26fe5f00":"markdown","28000d23":"markdown","1fd5cbab":"markdown","e7bec381":"markdown","f2ac51a0":"markdown","463ea1b2":"markdown","f664402c":"markdown","5607a585":"markdown","d8be1ffe":"markdown","228952a2":"markdown","a3f47166":"markdown","6fed8be7":"markdown","ff202038":"markdown","ded8599b":"markdown","8488d623":"markdown","1b5df335":"markdown","9a7fbfc8":"markdown","239c52cf":"markdown","b9cb4870":"markdown","47f8b497":"markdown","5c9de131":"markdown","5768c095":"markdown","73fa265f":"markdown","b9060d89":"markdown","f724acdb":"markdown","66cdf9cc":"markdown","3c1b2692":"markdown","364378d6":"markdown","7ba4f8fd":"markdown","6cc8e256":"markdown","22a094a2":"markdown","728d6be6":"markdown","0b5e2476":"markdown","ac1f1273":"markdown","67cf5790":"markdown","b09dfd23":"markdown","c56c4a68":"markdown"},"source":{"276b7c8c":"import os\nimport gc\nimport time\nimport math\nimport datetime\nfrom math import log, floor\nfrom sklearn.neighbors import KDTree\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nfrom tqdm.notebook import tqdm as tqdm\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport pywt\nfrom statsmodels.robust import mad\n\nimport scipy\nimport statsmodels\nfrom scipy import signal\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\nfrom scipy.signal import butter, deconvolve\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\n\nimport joblib\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn import preprocessing\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e8997506":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n    ","58435da9":"def read_data(PATH):\n    print('Reading files...')\n    calendar = pd.read_csv(f'{PATH}\/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    sell_prices = pd.read_csv(f'{PATH}\/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    sales_train_validation = pd.read_csv(f'{PATH}\/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    submission = pd.read_csv(f'{PATH}\/sample_submission.csv')\n    return calendar, sell_prices, sales_train_validation, submission\n\ncalendar, sell_prices, sales_train_validation, submission = read_data(\"..\/input\/m5-forecasting-accuracy\")","9841361b":"sales_train_validation_melt = pd.melt(sales_train_validation, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='day', value_name='demand')","9e88926f":"sales_CA_1 = sales_train_validation_melt[sales_train_validation_melt.store_id == \"CA_1\"]\nnew_CA_1 = pd.merge(sales_CA_1, calendar, left_on=\"day\", right_on=\"d\", how=\"left\")\nnew_CA_1 = pd.merge(new_CA_1, sell_prices, left_on=[\"store_id\", \"item_id\", \"wm_yr_wk\"],right_on=[\"store_id\", \"item_id\", \"wm_yr_wk\"], how=\"left\")\nnew_CA_1[\"day_int\"] = new_CA_1.day.apply(lambda x: int(x.split(\"_\")[-1]))","1eb549f5":"new_CA_1.head()","b0752de7":"day_sum = new_CA_1.groupby(\"day_int\")[[\"sell_price\", \"demand\"]].agg(\"sum\").reset_index()","31d76df7":"\nfig = make_subplots(rows=2, cols=1)\n\nfig.add_trace(go.Scatter(x=day_sum.day_int, \n                         y=day_sum.demand,\n                         #showlegend=False,\n                         mode=\"lines\",\n                         name=\"demand\",\n                         #marker=dict(color=\"mediumseagreen\"),\n                         ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=day_sum.day_int, \n                         y=day_sum.sell_price,\n                         #showlegend=False,\n                         mode=\"lines\",\n                         name=\"sell_price\",\n                         #marker=dict(color=\"mediumseagreen\")\n                         ),\n             \n              row=2,col=1           \n              )\n\nfig.update_layout(height=1000, title_text=\"SUM -> Demand  and Sell_price\")\nfig.show()","aa568805":"# For each day we count_nonzeros over products sell_price and demand\n\nday_sum = new_CA_1.groupby(\"day_int\")[[\"demand\",\"event_name_1\" ]].agg({\"demand\": np.count_nonzero, \"event_name_1\": \"first\"}).reset_index()\ndef count_nulls(series):\n    return len(series) - series.count()\n\ncout_null = new_CA_1.groupby(\"day_int\")[\"sell_price\"].agg(count_nulls).reset_index()","342f8d5c":"\nfig = make_subplots(rows=2, cols=1)\n\nfig.add_trace(go.Scatter(x=cout_null.day_int, \n                         y=cout_null.sell_price,\n                         #showlegend=False,\n                         mode=\"lines\",\n                         name=\"sell_price\",\n                         #marker=dict(color=\"mediumseagreen\")\n                        ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=day_sum.day_int, \n                         y=day_sum.demand,\n                         #showlegend=False,\n                         mode=\"lines\",\n                         name=\"demand\",\n                         #marker=dict(color=\"mediumseagreen\")\n                        ),\n             \n              row=2,col=1           \n              )\n\nfig.update_layout(height=1000, title_text=\"Count_Nonzero -> Sell_price  and Demand\")\nfig.show()","8e65adc8":"item_id = new_CA_1.groupby(\"item_id\")[[\"sell_price\", \"demand\"]].agg({\n    \"sell_price\": [\"max\", \"mean\", \"min\"],\n    \"demand\" : [\"max\", \"mean\", \"min\"]\n}).reset_index()","0edf26fa":"fig = make_subplots(rows=1, cols=1)\n\nitem_id = item_id.sort_values((\"sell_price\", \"max\"))\nfig.add_trace(go.Scatter(x=item_id[\"item_id\"], \n                         y=item_id[\"sell_price\", \"max\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"max\",\n                         #marker=dict(color=\"mediumseagreen\")\n                         ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=item_id[\"item_id\"], \n                         y=item_id[\"sell_price\", \"mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"mean\",\n                         #marker=dict(color=\"yellow\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.add_trace(go.Scatter(x=item_id[\"item_id\"], \n                         y=item_id[\"sell_price\", \"min\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"min\",\n                         #marker=dict(color=\"blue\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.update_layout(height=500, title_text=\"Sell_price\")\nfig.show()","3de95bf4":"\nfig = make_subplots(rows=1, cols=1)\n\nitem_id = item_id.sort_values((\"demand\", \"max\"))\nfig.add_trace(go.Scatter(x=item_id[\"item_id\"], \n                         y=item_id[\"demand\", \"max\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"max\",\n                         #marker=dict(color=\"mediumseagreen\")\n                         ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=item_id[\"item_id\"], \n                         y=item_id[\"demand\", \"mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"mean\",\n                         #marker=dict(color=\"yellow\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.add_trace(go.Scatter(x=item_id[\"item_id\"], \n                         y=item_id[\"demand\", \"min\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"min\",\n                         #marker=dict(color=\"blue\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.update_layout(height=500, title_text=\"Demand\")\nfig.show()","f8dd65cf":"# For each item week_days vs week_ends over days sell_price and demand\n\nweek_end = new_CA_1[new_CA_1.weekday == \"Sunday\"]\nweek_day = new_CA_1[new_CA_1.weekday != \"Sunday\"]\n\nweek_end = week_end.groupby(\"item_id\")[[\"demand\", \"sell_price\"]].agg([\"mean\", \"max\"]).reset_index()\nweek_end.columns = ['_'.join(col).strip() for col in week_end.columns.values]\n\nweek_day = week_day.groupby(\"item_id\")[[\"demand\", \"sell_price\"]].agg([\"mean\", \"max\"]).reset_index()\nweek_day.columns = ['_'.join(col).strip() for col in week_day.columns.values]","8a79b565":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=week_end[\"item_id_\"],\n                         y=week_end[\"demand_mean\"],\n                         mode=\"lines\",\n                         name=\"week_day\"\n\n))\n\nfig.add_trace(go.Scatter(x=week_end[\"item_id_\"],\n                         y=week_day[\"demand_mean\"],\n                         mode=\"lines\",\n                         name=\"normal_day\"\n\n))\n\nfig.update_layout(height=500, title_text=\"Demand\")\nfig.show()\n","ec882889":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=week_end[\"item_id_\"],\n                         y=week_end[\"sell_price_mean\"],\n                         mode=\"lines\",\n                         name=\"week_day\"\n\n))\n\nfig.add_trace(go.Scatter(x=week_end[\"item_id_\"],\n                         y=week_day[\"sell_price_mean\"],\n                         mode=\"lines\",\n                         name=\"normal_day\"\n\n))\n\nfig.update_layout(height=500,title_text=\"Sell_price\")\nfig.show()\n","0af2dbf9":"events = new_CA_1[~new_CA_1.event_name_1.isna()]\nevents = events.groupby(\"event_name_1\")[[\"demand\", \"sell_price\"]].agg([\"mean\", \"max\"]).reset_index()\nevents.columns = ['_'.join(col).strip() for col in events.columns.values]","07e830b6":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=events[\"event_name_1_\"],\n                         y=events[\"demand_mean\"],\n                         mode=\"lines\",\n                         name=\"week_day\"\n))\n\nfig.update_layout(height=500, title_text=\"Demand\")\nfig.show()\n","7d89cb90":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=events[\"event_name_1_\"],\n                         y=events[\"sell_price_mean\"],\n                         mode=\"lines\",\n                         name=\"week_day\"\n\n))\n\nfig.update_layout(height=500, title_text=\"Sell_price\")\nfig.show()\n","9ac3a3f9":"## Number of items contain each Category\n\ndef n_unique(series):\n    return series.nunique()\n\nCategory_count = new_CA_1.groupby(\"cat_id\")[\"item_id\"].agg(n_unique).reset_index()","5870d52d":"fig = px.bar(Category_count, y=\"item_id\", x=\"cat_id\", color=\"cat_id\", title=\"Category Item Count\")\n\nfig.update_layout(height=500, width=600)\nfig.show()","d7abc4a7":"## For each category mean of deman and sell_price\n\nCategory = new_CA_1.groupby([\"day_int\",\"cat_id\"])[[\"demand\", \"sell_price\"]].agg([\"mean\", \"max\"]).reset_index()\nCategory.columns = ['_'.join(col).strip() for col in Category.columns.values]\n\nFOODS = Category[Category.cat_id_ == \"FOODS\"]\nHOBBIES = Category[Category.cat_id_ == \"HOBBIES\"]\nHOUSEHOLD = Category[Category.cat_id_ == \"HOUSEHOLD\"]","f9f4bedd":"\nfig = make_subplots(rows=1, cols=1)\n\n\nfig.add_trace(go.Scatter(x=FOODS[\"day_int_\"], \n                         y=FOODS[\"demand_mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"FOODS\",\n                         #marker=dict(color=\"mediumseagreen\")\n                         ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=HOBBIES[\"day_int_\"], \n                         y=HOBBIES[\"demand_mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"HOBBIES\",\n                         #marker=dict(color=\"yellow\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.add_trace(go.Scatter(x=HOUSEHOLD[\"day_int_\"], \n                         y=HOUSEHOLD[\"demand_mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"HOUSEHOLD\",\n                         #marker=dict(color=\"blue\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.update_layout(height=500, title_text=\"Demand Mean Over Category by day-by-day\")\nfig.show()","6b675a87":"\nfig = make_subplots(rows=1, cols=1)\n\n\nfig.add_trace(go.Scatter(x=FOODS[\"day_int_\"], \n                         y=FOODS[\"demand_max\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"FOODS\",\n                         #marker=dict(color=\"mediumseagreen\")\n                         ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=HOBBIES[\"day_int_\"], \n                         y=HOBBIES[\"demand_max\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"HOBBIES\",\n                         #marker=dict(color=\"yellow\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.add_trace(go.Scatter(x=HOUSEHOLD[\"day_int_\"], \n                         y=HOUSEHOLD[\"demand_max\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"HOUSEHOLD\",\n                         #marker=dict(color=\"blue\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.update_layout(height=500, title_text=\"Demand Max Over Category by day-by-day\")\nfig.show()","6d0f0cef":"\nfig = make_subplots(rows=1, cols=1)\n\n\nfig.add_trace(go.Scatter(x=FOODS[\"day_int_\"], \n                         y=FOODS[\"sell_price_mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"FOODS\",\n                         #marker=dict(color=\"mediumseagreen\")\n                         ),\n\n              row=1,col=1         \n              )\n\nfig.add_trace(go.Scatter(x=HOBBIES[\"day_int_\"], \n                         y=HOBBIES[\"sell_price_mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"HOBBIES\",\n                         #marker=dict(color=\"yellow\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.add_trace(go.Scatter(x=HOUSEHOLD[\"day_int_\"], \n                         y=HOUSEHOLD[\"sell_price_mean\"],\n                         #showlegend=Ture,\n                         mode=\"lines\",\n                         name=\"HOUSEHOLD\",\n                         #marker=dict(color=\"blue\")\n                         ),\n             \n              row=1,col=1           \n              )\n\nfig.update_layout(height=500, title_text=\"Sell_price Mean Over Category by day-by-day\")\nfig.show()","ddd9dd14":"fig = go.Figure()\n\nfig.add_trace(go.Box(x=FOODS.cat_id_, y=FOODS.demand_mean, name=\"FOODS\"))\n\nfig.add_trace(go.Box(x=HOUSEHOLD.cat_id_, y=HOUSEHOLD.demand_mean, name=\"HOUSEHOLD\"))\n\nfig.add_trace(go.Box(x=HOBBIES.cat_id_, y=HOBBIES.demand_mean, name=\"HOBBIES\"))\n\n\nfig.update_layout(yaxis_title=\"Demand\", xaxis_title=\"Time\", title=\"Demand Mean vs. Category\")","d75e0431":"fig = go.Figure()\n\nfig.add_trace(go.Box(x=FOODS.cat_id_, y=FOODS.sell_price_mean, name=\"FOODS\"))\n\nfig.add_trace(go.Box(x=HOUSEHOLD.cat_id_, y=HOUSEHOLD.sell_price_mean, name=\"HOUSEHOLD\"))\n\nfig.add_trace(go.Box(x=HOBBIES.cat_id_, y=HOBBIES.sell_price_mean, name=\"HOBBIES\"))\n\n\nfig.update_layout(yaxis_title=\"Sell Price\", xaxis_title=\"Time\", title=\"Sell Price Mean vs. Category\")","0f20c382":"## Number of items contain each Deportments\n\ndef n_unique(series):\n    return series.nunique()\n\ndep_count = new_CA_1.groupby(\"dept_id\")[\"item_id\"].agg(n_unique).reset_index()","6a20341a":"px.bar(dep_count, y=\"item_id\", x=\"dept_id\", color=\"dept_id\", title=\"Deportment Item Count\")","d68d7fd7":"## For each Deportment mean of deman and sell_price\n\ndep = new_CA_1.groupby([\"day_int\",\"dept_id\"])[[\"demand\", \"sell_price\"]].agg([\"mean\", \"max\"]).reset_index()\ndep.columns = ['_'.join(col).strip() for col in dep.columns.values]","cfaee4f8":"\nfig = make_subplots(rows=1, cols=1)\n\nfor each_dep in dep.dept_id_.unique():\n    dep_df = dep[dep.dept_id_ == each_dep]\n    fig.add_trace(go.Scatter(x=dep_df[\"day_int_\"], \n                             y=dep_df[\"demand_mean\"],\n                             #showlegend=Ture,\n                             mode=\"lines\",\n                             name=each_dep,\n                             #marker=dict(color=\"mediumseagreen\")\n                             ),\n\n                  row=1,col=1         \n                  )\n    \nfig.update_layout(title_text=\"Demand Mean Over Deportments by day-by-day\")\nfig.show()","c91ae067":"\nfig = make_subplots(rows=1, cols=1)\n\nfor each_dep in dep.dept_id_.unique():\n    dep_df = dep[dep.dept_id_ == each_dep]\n    fig.add_trace(go.Scatter(x=dep_df[\"day_int_\"], \n                             y=dep_df[\"sell_price_mean\"],\n                             #showlegend=Ture,\n                             mode=\"lines\",\n                             name=each_dep,\n                             #marker=dict(color=\"mediumseagreen\")\n                             ),\n\n                  row=1,col=1         \n                  )\n    \nfig.update_layout(title_text=\"Sell Prices Mean Over Deportments by day-by-day\")\nfig.show()","8cfadcb9":"fig = go.Figure()\n\nfor each_dep in dep.dept_id_.unique():\n    dep_df = dep[dep.dept_id_ == each_dep]\n\n    fig.add_trace(go.Box(x=dep_df.dept_id_, y=dep_df.demand_mean, name=each_dep))\n    \n\n\nfig.update_layout(yaxis_title=\"Demand\", xaxis_title=\"Time\", title=\"Demand Mean vs. Deportment\")","603553d1":"fig = go.Figure()\n\nfor each_dep in dep.dept_id_.unique():\n    dep_df = dep[dep.dept_id_ == each_dep]\n\n    fig.add_trace(go.Box(x=dep_df.dept_id_, y=dep_df.sell_price_mean, name=each_dep))\n    \n    \nfig.update_layout(yaxis_title=\"Sell Price\", xaxis_title=\"Time\", title=\"Sell Price Mean vs. Deportment\")","101aaecc":"# take only one stor for demo\n\nCA1 = new_CA_1\nCA1.head()","fea66f21":"CA1 = CA1[[\"item_id\",\"day_int\", \"demand\", \"sell_price\", \"date\"]]\nCA1.fillna(0, inplace=True)\nprint(CA1.shape)\nCA1.head()","0ee983dc":"def date_features(df):\n    \n    df[\"date\"] = pd.to_datetime(df[\"date\"])\n    df[\"day\"] = df.date.dt.day\n    df[\"month\"] = df.date.dt.month\n    df[\"week_day\"] = df.date.dt.weekday\n\n    df.drop(columns=\"date\", inplace=True)\n\n    return df\n\ndef sales_features(df):\n\n    df.sell_price.fillna(0, inplace=True)\n\n    return df\n\ndef demand_features(df):\n\n    df[\"lag_t28\"] = df[\"demand\"].transform(lambda x: x.shift(28))\n    df[\"rolling_mean_t7\"] = df[\"demand\"].transform(lambda x:x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df['rolling_mean_t60'] = df['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    df.fillna(0, inplace=True)\n\n    return df","257cd2b6":"!mkdir \"something_spl\"\n# Saving each item with there item name.npy\n\nfor item in tqdm(CA1.item_id.unique()):\n    one_item = CA1[CA1.item_id == item][[\"demand\", \"sell_price\", \"date\"]]\n    item_df = date_features(one_item)\n    item_df = sales_features(item_df)\n    item_df = demand_features(item_df)\n    joblib.dump(item_df.values, f\"something_spl\/{item}.npy\")","367d0f64":"# create dataframe for loading npy files and  train valid split\n\ndata_info = CA1[[\"item_id\", \"day_int\"]]\n\n# total number of days -> 1913\n# for training we are taking data between 1800 < train <- 1913-28-28 = 1857\n\ntrain_df = data_info[(1800 < data_info.day_int) &( data_info.day_int < 1857)]\n\n# valid data is given last day -> 1885 we need to predict next 28days\n\nvalid_df = data_info[data_info.day_int == 1885]","616a6de3":"label = preprocessing.LabelEncoder()\nlabel.fit(train_df.item_id)\nlabel.transform([\"FOODS_3_827\"])","3064abba":"class DataLoading:\n    def __init__(self, df, train_window = 28, predicting_window=28):\n        self.df = df.values\n        self.train_window = train_window\n        self.predicting_window = predicting_window\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, item):\n        df_item = self.df[item]\n        item_id = df_item[0]\n        day_int = df_item[1]\n        \n        item_npy = joblib.load(f\"something_spl\/{item_id}.npy\")\n        item_npy_demand = item_npy[:,0]\n        features = item_npy[day_int-self.train_window:day_int]\n    \n\n        predicted_demand = item_npy_demand[day_int:day_int+self.predicting_window]\n\n        item_label = label.transform([item_id])\n        item_onehot = [0] * 3049\n        item_onehot[item_label[0]] = 1\n\n        list_features = []\n        for f in features:\n            one_f = []\n            one_f.extend(item_onehot)\n            one_f.extend(f)\n            list_features.append(one_f)\n\n        return {\n            \"features\" : torch.Tensor(list_features),\n            \"label\" : torch.Tensor(predicted_demand)\n        }","cdca5700":"## for exaple one item\n\ndatac = DataLoading(train_df)\nn = datac.__getitem__(100)\nn[\"features\"].shape, n[\"label\"].shape","12bde5fb":"class LSTM(nn.Module):\n    def __init__(self, input_size=3062, hidden_layer_size=100, output_size=28):\n        super().__init__()\n        self.hidden_layer_size = hidden_layer_size\n        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n        self.linear = nn.Linear(hidden_layer_size, output_size)\n\n        self.hidden_cell = (torch.zeros(1,1,self.hidden_layer_size),\n                            torch.zeros(1,1,self.hidden_layer_size))\n        \n    def forward(self, input_seq):\n\n        lstm_out, self.hidden_cell = self.lstm(input_seq)\n\n        lstm_out = lstm_out[:, -1]\n\n        predictions = self.linear(lstm_out)\n\n        return predictions\n","99975791":"# loss function\ndef criterion1(pred1, targets):\n    l1 = nn.MSELoss()(pred1, targets)\n    return l1","d89eecec":"def train_model(model,train_loader, epoch, optimizer, scheduler=None, history=None):\n    model.train()\n    total_loss = 0\n    \n    t = tqdm(train_loader)\n    \n    for i, d in enumerate(t):\n        \n        item = d[\"features\"].cuda().float()\n        y_batch = d[\"label\"].cuda().float()\n\n        optimizer.zero_grad()\n\n        out = model(item)\n        loss = criterion1(out, y_batch)\n\n        total_loss += loss\n        \n        t.set_description(f'Epoch {epoch+1} : , LR: %6f, Loss: %.4f'%(optimizer.state_dict()['param_groups'][0]['lr'],total_loss\/(i+1)))\n\n        if history is not None:\n            history.loc[epoch + i \/ len(X), 'train_loss'] = loss.data.cpu().numpy()\n            history.loc[epoch + i \/ len(X), 'lr'] = optimizer.state_dict()['param_groups'][0]['lr']\n\n        loss.backward()\n        optimizer.step()\n        \n\ndef evaluate_model(model, val_loader, epoch, scheduler=None, history=None):\n    model.eval()\n    loss = 0\n    pred_list = []\n    real_list = []\n    RMSE_list = []\n    with torch.no_grad():\n        for i,d in enumerate(tqdm(val_loader)):\n            item = d[\"features\"].cuda().float()\n            y_batch = d[\"label\"].cuda().float()\n\n            o1 = model(item)\n            l1 = criterion1(o1, y_batch)\n            loss += l1\n            \n            o1 = o1.cpu().numpy()\n            y_batch = y_batch.cpu().numpy()\n            \n            for pred, real in zip(o1, y_batch):\n                rmse = np.sqrt(sklearn.metrics.mean_squared_error(real, pred))\n                RMSE_list.append(rmse)\n                pred_list.append(pred)\n                real_list.append(real)\n\n    loss \/= len(val_loader)\n    \n    if scheduler is not None:\n        scheduler.step(loss)\n\n    print(f'\\n Dev loss: %.4f RMSE : %.4f'%(loss, np.mean(RMSE_list)))\n    ","0234f5d1":"DEVICE = \"cuda\"\nTRAIN_BATCH_SIZE = 512\nTEST_BATCH_SIZE = 128\nEPOCHS = 1\nstart_e = 1\n\n\nmodel = LSTM()\nmodel.to(DEVICE)\n\ntrain_dataset = DataLoading(train_df)\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset=train_dataset,\n    batch_size= TRAIN_BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    drop_last=True\n)\n\n\nvalid_dataset = DataLoading(valid_df)\n\nvalid_loader = torch.utils.data.DataLoader(\n    dataset=valid_dataset,\n    batch_size= TEST_BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    drop_last=True\n)\n\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, mode='min', factor=0.7, verbose=True, min_lr=1e-5)\n\nfor epoch in range(start_e, EPOCHS+1):\n    train_model(model, train_loader, epoch, optimizer, scheduler=scheduler, history=None)\n    evaluate_model(model, valid_loader, epoch, scheduler=scheduler, history=None)\n","594db644":"**Observation :** From the above plots we can observe `FOOD3` have hight demand","4e9c84a6":"- we observe that at one item -> one day step we are taking last 28days of features to predict next 28days demand.\n- targets are 28days demand from that particular day","e6420eda":"**Observation :** From the above bar plot we observe thet each category `FOOD`: 1437 , `HOBBIES` : 565 , `HOUSEHOLD` : 1047 items","bbc73f17":"### Preparing Train & Validation df <a id=\"5.3\"><\/a>","7589eb10":"### Load Data <a id=\"3.1\"><\/a>","26fe5f00":"## Data Overview <a id=\"2\"><\/a>\n\nThe dataset consists of five .csv files.\n\n### File 1: calendar.csv\n- Contains the dates on which products are sold. The dates are in a <code>yyyy\/dd\/mm<\/code> format.\n\n- `date`: The date in a \u201cy-m-d\u201d format.\n- `wm_yr_wk`: The id of the week the date belongs to.\n- `weekday`: The type of the day (Saturday, Sunday, ..., Friday).\n- `wday`: The id of the weekday, starting from Saturday.\n- `month`: The month of the date.\n- `year`: The year of the date.\n- `event_name_1`: If the date includes an event, the name of this event.\n- `event_type_1`: If the date includes an event, the type of this event.\n- `event_name_2`: If the date includes a second event, the name of this event.\n- `event_type_2`: If the date includes a second event, the type of this event.\n- snap_CA, snap_TX, and snap_WI: A binary variable (0 or 1) indicating whether the stores of CA, TX or WI allow SNAP 3 purchases on the examined date. 1 indicates that SNAP purchases are allowed.\n\n\n\n### File 2: sales_train_validation.csv\n- Contains the historical daily unit sales data per product and store <code>[d_1 - d_1913]<\/code>.\n\n- `item_id`: The id of the product.\n- `dept_id`: The id of the department the product belongs to.\n- `cat_id`: The id of the category the product belongs to.\n- `store_id`: The id of the store where the product is sold.\n- `state_id`: The State where the store is located.\n- `d_1, d_2, ..., d_i, ... d_1941`: The number of units sold at day i, starting from 2011-01-29.\n\n### File 3: sell_prices.csv\n- Contains information about the price of the products sold per store and date.\n\n- `store_id`: The id of the store where the product is sold.\n- `item_id`: The id of the product.\n- `wm_yr_wk`: The id of the week.\n- `sell_price`: The price of the product for the given week\/store. The price is provided per week (average across seven days). If not available, this means that the product was not sold during the examined week. Note that although prices are constant at weekly basis, they may change through time (both training and test set).\n\n\n### File 4: submission.csv\n- Demonstrates the correct format for submission to the competition.\n\n- Each row contains an `id` that is a concatenation of an `item_id` and a `store_id`, which is either `validation` (corresponding to the Public leaderboard), or `evaluation` (corresponding to the Private leaderboard). You are predicting 28 forecast days `(F1-F28)` of items sold for each row. For the `validation` rows, this corresponds to `d_1914 - d_1941`, and for the `evaluation` rows, this corresponds to `d_1942 - d_1969`. (Note: a month before the competition close, the ground truth for the `validation` rows will be provided.)\n\n### File 5: sales_train_evaluation.csv\n\n- Available one month before the competition deadline. It will include sales for <code>[d_1 - d_1941]<\/code>.\n\nIn this competition, we need to forecast the sales for <code>[d_1942 - d_1969]<\/code>. These rows form the evaluation set. The rows <code>[d_1914 - d_1941]<\/code> form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset.","28000d23":"### Model Formation <a id=\"5.1\"><\/a>\n\n- ###### State -> Store -> Category -> Depoartment -> Item\n- ###### Total Stores -> 10\n- ###### For one Store -> 3049\n- ###### For one Item -> 1913 days\n- ###### For one Item -> Category -> Deportment\n\n- ###### We need to predict 28d \"Demand\" for every Item in Every Store\n\n### Pytorch DataLoader\n\n- ###### In data we have 10 Stores * 3049 Items => 30490 Store Items\n- ###### For each Item we need to create Features\n- ###### Because of its lots of data we need to create dataloders to load\n- ###### For that assume items are independet and for each item we have 1914 days\n- ###### For each item we need to create one npy file\n- ###### Each npy file contain 1913 days\n- ###### Each npy file contain 1913 days","1fd5cbab":"### For each Department <a id=\"4.6\"><\/a>\n\n- For each Category we have some Deportments\n\n- There are total 7 Deportments","e7bec381":"### For week_days vs week_ends <a id=\"4.3\"><\/a>\n\n- For each item week_days vs week_ends over days sell_price and demand","f2ac51a0":"**Observation :** \n* From the above plot we observe that some products demend high in some days the min is zero because of Christmas day","463ea1b2":"**Observation :** From the above plots we can observe `HOBBIES` have hight sell Price","f664402c":"## Introduction","5607a585":"### Run Function <a id=\"5.7\"><\/a>","d8be1ffe":"**Observation :** From the above plots we observe that `FOOD` category have more demand","228952a2":"**Observation :** \n* From the above cout_nonzeros over product `demand` we observe that non_zeros count increasing day-by-day\n* From the above cout_null_values over product `sell_price` we observe that day-by-day null values decreasing and at the end we observe that count close to 0","a3f47166":"### Store -> CA_1","6fed8be7":"**Observation :** From the above plot we observe `FOODS_1` and `HOUSEHOLD_2` have high sell_prices","ff202038":"**Observation :** \n* From the above plot we observe that in weekdays and weekends are probably same prices","ded8599b":"### For each item <a id=\"4.2\"><\/a>\n\n- For each item we `[max, mean, min]` over days `sell_price` and `demand`\n","8488d623":"## EDA <a id=\"4\"><\/a>","1b5df335":"## stay tuned compitation metric and custom loss parts coming soon :)\n\n<h2 style=\"color:red;\"> Please upvote if you like it. It motivates me. Thank you \u263a\ufe0f .<\/h2>","9a7fbfc8":"**Observation :** \n* From the above plot we observe that for every product the prices change over time","239c52cf":"### LSTM+NN Model <a id=\"5.5\"><\/a>","b9cb4870":"### For each Category <a id=\"4.5\"><\/a>\n\n- In the data there are three categorys","47f8b497":"- In this data, The products are sold across `ten stores`, located in `three States` (CA, TX and WI)\n- For each `State` we have some `Stores`\n- In each `Store` we have `3,049 products`\n- For each `Product` belongs to one of three `Category` are `[Hobbies, Foods, Household]`\n- For each `Category` we have some `Department` \n- For each `Department` we have some `Products`\n\n**So we need to extract features for each `Product`**\n\n- For EDA we are going to use one of `Store` \n- We are going to use `CA_1`","5c9de131":"### Train and Eval functions<a id=\"5.6\"><\/a>","5768c095":"### For each day <a id=\"4.1\"><\/a>\n\n- For each day we `sum` over products `sell_price` and `demand`\n- For each day we `count_nonzeros` over products `sell_price` and `demand`\n","73fa265f":"**Observation :** \n* From the above sum over product `demand` we observe that some days are \"Zeros\" because those are `Christmas` days, I think in chirstmas day the store was closed. and we observe some patterns over the years. \n* From the above sum over product `sell_price` we observe that day-by-day the sells are increasing. at the end its becaming constant.","b9060d89":"## Understand Business Problem <a id=\"1\"><\/a>\n\nIn the challenge, you are predicting item sales at stores in various locations for two 28-day time periods. Information about the data is found in the [M5 Participants Guide.](https:\/\/mofc.unic.ac.cy\/m5-competition\/)\n","f724acdb":"**Observation :** \n* From the above plot we observe that in some event days the demand increase and decrease ","66cdf9cc":"### Preprocessing Data\n\n- We need to merge all dataframes as a single dataframe then it's eassy to do some EDA and Modeling\n- Using `pd.melt()` function convert `sales_train_validation` columns `[d_1 - d_1941]` to rows as `demand` column \n- then we mege `calendar` , `sell_prices`, `sales_train_validation`","3c1b2692":"**Observation :** \n* From the above plot we observe that in week days some more demand on some items","364378d6":"### Pytorch Data Loader <a id=\"5.4\"><\/a>\n\n- we are taking last 28 days data features to predict next 28days of demand for each item\n- `train_window = 28` and `predicting_window=28`","7ba4f8fd":"**Observation :** From the above plot we observe that `FOOD_3` have more demand and it have some picks ","6cc8e256":"### Feature Engineering <a id=\"5.2\"><\/a>\n\n- For every item we need to extract features","22a094a2":"<img src=\"https:\/\/i.imgur.com\/bOhLgbV.jpg\" height=\"100px\" width=\"400px\">","728d6be6":"# Contents\n\n* [<font size=4>Understand Business Problem<\/font>](#1)\n\n* [<font size=4>Data Overview<\/font>](#2)\n* [<font size=4>Data Preprocessing<\/font>](#3)\n  * [Loading Data](#3.1)\n  * [Preparing Data](#3.2)\n* [<font size=4>EDA<\/font>](#4)\n  * [For each day](#4.1)\n  * [For each item](#4.2)\n  * [For week_days vs week_ends](#4.3)\n  * [For events days](#4.4)\n  * [For each Category](#4.5)\n  * [ For each Department](#4.6)\n* [<font size=4>Modeling<\/font>](#5)\n  * [Model Formation](#5.1)\n  * [Feature Engineering](#5.2)\n  * [Preparing Train & Validation df](#5.3)\n  * [Pytorch Data Loader](#5.4)\n  * [LSTM+NN Model](#5.5)\n  * [Train and Eval functions](#5.6)\n  * [Run Function](#5.7)\n","0b5e2476":"**Observation :** From the above plot we observe `HOBBIES` and `HOUSEHOLD` have high sell_prices","ac1f1273":"**Observation :** \n- From the above plot we observe that `FOODS` Category have 3Deportments, `HOBBIES` have 2Deportments, `HOUSEHOLD` have 2Deportments total 7Deportments\n- Deportment `FOODS_3` have more items","67cf5790":"## For events days <a id=\"4.4\"><\/a>","b09dfd23":"## Data Preprocessing <a id=\"3\"><\/a>","c56c4a68":"## Modeling"}}