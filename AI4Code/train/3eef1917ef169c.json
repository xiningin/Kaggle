{"cell_type":{"dfcff5c6":"code","097c1a43":"code","c569767e":"code","23281ed6":"code","83225ab5":"code","e4a2c634":"code","1997c402":"code","896dd238":"code","4c5eac8b":"code","cda46e55":"code","318a3b1c":"code","96d7be0e":"code","f4a17464":"code","e38ec99d":"code","37b11358":"code","398333d0":"code","e44c2496":"code","014c6f70":"code","d98de5f4":"code","be4b511c":"code","e94aac6e":"code","e1cb775e":"markdown","f77fc5ce":"markdown","34ecc7a3":"markdown","0ea73669":"markdown","be0a7f1e":"markdown","ae9c7777":"markdown","09bb930c":"markdown","ae655846":"markdown","39c546f3":"markdown","bb069138":"markdown","41636eb5":"markdown","ef42c63a":"markdown","d0dffa79":"markdown"},"source":{"dfcff5c6":"import os\nimport numpy as np\nimport pandas as pd\nimport gc\nimport math\nimport itertools \nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport logging\n# no extensive logging \nlogging.getLogger().setLevel(logging.NOTSET)\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer, TFRobertaModel\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors","097c1a43":"BATCH_SIZE = 16\nMAX_LEN = 192\nLR_TRANSFORMER = 5e-6\nLR_HEAD = 1e-3\nMODEL = 'jplu\/tf-xlm-roberta-large'\nAUTO = tf.data.experimental.AUTOTUNE\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","c569767e":"def connect_to_TPU():\n    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy()\n\n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n\n    return tpu, strategy, global_batch_size\n\n\ntpu, strategy, global_batch_size = connect_to_TPU()\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","23281ed6":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","83225ab5":"def create_dist_dataset(X, y=None, training=False):\n    dataset = tf.data.Dataset.from_tensor_slices(X)\n\n    if y is not None:\n        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n\n    if training:\n        dataset = dataset.shuffle(len(X)).repeat()\n        \n    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n        \n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n    return dist_dataset","e4a2c634":"train_df = pd.read_csv('..\/input\/multilingual\/train.csv')\nprint(len(train_df))\ntrain_df.head()","1997c402":"X_train = regular_encode(train_df.comment_text.values, tokenizer, MAX_LEN)\ny_train = train_df.toxic.values.reshape(-1,1)","896dd238":"train_dist_dataset = create_dist_dataset(X_train.astype(np.int32), y_train.astype(np.float32), training= True)","4c5eac8b":"val_df = pd.read_csv('..\/input\/multilingual\/val.csv')\nprint(len(val_df))\nval_df.head()","cda46e55":"X_val = regular_encode(val_df.comment_text.values, tokenizer, MAX_LEN)\ny_val = val_df.toxic.values.reshape(-1,1)","318a3b1c":"val_dist_dataset = create_dist_dataset(X_val.astype(np.int32))","96d7be0e":"test_df = pd.read_csv('..\/input\/multilingual\/test.csv')\nprint(len(test_df))\ntest_df.head()","f4a17464":"X_test = regular_encode(test_df.comment_text.values, tokenizer, MAX_LEN)","e38ec99d":"test_dist_dataset = create_dist_dataset(X_test.astype(np.int32))","37b11358":"def get_model():\n    \n    transformer = TFRobertaModel.from_pretrained(MODEL)\n    \n    inp = Input(shape=(192,),dtype = 'int32', name = 'input_word_ids')\n    x = transformer(inp)[0]\n    cls_token = Dropout(0.2)(x[:, 0, :])\n    \n    x = Dense(256, activation = 'relu', input_shape = (1024,), name = 'custom')(cls_token)\n    x = Dropout(0.2)(x)\n    x = Dense(1, activation='sigmoid',input_shape = (256,), name = 'custom_1')(x)\n    \n    model = Model(inp, x)\n    return model","398333d0":"def create_model_and_optimizer():\n    with strategy.scope():                \n        model = get_model()\n        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n        optimizer_head = Adam(learning_rate=LR_HEAD)\n    return model, optimizer_transformer, optimizer_head\n\nmodel, optimizer_transformer, optimizer_head = create_model_and_optimizer()\nmodel.summary()","e44c2496":"def define_losses_and_metrics():\n    with strategy.scope():\n        loss_object = tf.keras.losses.BinaryCrossentropy(\n                          from_logits=False,\n                          reduction=tf.keras.losses.Reduction.NONE,\n                          label_smoothing = 0.1)\n\n        def compute_loss(labels, predictions):\n            per_example_loss = loss_object(labels, predictions)\n            loss = tf.nn.compute_average_loss(\n                per_example_loss, global_batch_size = global_batch_size)\n            return loss\n\n        train_accuracy_metric = tf.keras.metrics.AUC(name='training_AUC')\n\n    return compute_loss, train_accuracy_metric\n\ncompute_loss, train_accuracy_metric = define_losses_and_metrics()","014c6f70":"@tf.function\ndef distributed_train_step(data):\n    strategy.experimental_run_v2(train_step, args=(data,))\n\ndef train_step(inputs):\n    features, labels = inputs\n    \n    ### get transformer and head separate vars\n    # get rid of pooler head with None gradients\n    transformer_trainable_variables = [ v for v in model.trainable_variables \n                                       if (('pooler' not in v.name)  and \n                                           ('custom' not in v.name))]\n    head_trainable_variables = [ v for v in model.trainable_variables \n                                if 'custom'  in v.name]\n\n    # calculate the 2 gradients ( note persistent, and del)\n    with tf.GradientTape(persistent=True) as tape:\n        predictions = model(features, training=True)\n#         print(labels, predictions)\n#         labels = tf.reshape(labels, (16,1))\n#         tf.expand_dims(labels, -1)\n        loss = compute_loss(labels, predictions)\n    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n    gradients_head = tape.gradient(loss, head_trainable_variables)\n    del tape\n        \n    ### make the 2 gradients steps\n    optimizer_transformer.apply_gradients(zip(gradients_transformer, \n                                              transformer_trainable_variables))\n    optimizer_head.apply_gradients(zip(gradients_head, \n                                       head_trainable_variables))\n\n    train_accuracy_metric.update_state(tf.round(labels), predictions)\n\ndef predict(dataset):  \n    predictions = []\n    for tensor in dataset:\n        predictions.append(distributed_prediction_step(tensor))\n    predictions = np.vstack(list(map(np.vstack,predictions)))\n    return predictions\n\n@tf.function\ndef distributed_prediction_step(data):\n    predictions = strategy.experimental_run_v2(prediction_step, args=(data,))\n    return strategy.experimental_local_results(predictions)\n\ndef prediction_step(inputs):\n    features = inputs  # note datasets used in prediction do not have labels\n    predictions = model(features, training=False)\n    return predictions","d98de5f4":"def train(train_dist_dataset, val_dist_dataset=None, y_val=None, total_steps=2000, validate_every=200):\n    train_accuracy_metric.reset_states()\n    best_weights, history = None, []\n    step = 0\n    ### Training loop ###\n    for tensor in train_dist_dataset:\n        distributed_train_step(tensor) \n        step+=1\n\n        if (step % validate_every == 0):   \n            ### Print train metrics ###  \n            train_metric = train_accuracy_metric.result().numpy()\n            print(\"Step %d, train AUC: %.5f\" % (step, train_metric))   \n            \n            ### Test loop with exact AUC ###\n            if val_dist_dataset:\n                val_metric = roc_auc_score(np.round(y_val), predict(val_dist_dataset))\n                print(\"Step %d,   val AUC: %.5f\" %  (step,val_metric))   \n                \n                # save weights if it is the best yet\n                history.append(val_metric)\n                if history[-1] == max(history):\n                    best_weights = model.get_weights()\n\n            ### Reset (train) metrics ###\n            train_accuracy_metric.reset_states()\n            \n        if step  == total_steps:\n            break\n    \n    ### Restore best weighths ###\n    if(best_weights):\n        model.set_weights(best_weights)","be4b511c":"train(train_dist_dataset, val_dist_dataset, y_val, total_steps = 1000, validate_every = 100)","e94aac6e":"preds = predict(test_dist_dataset)[:,0]","e1cb775e":"# Custom Training Steps","f77fc5ce":"## Train Dataset","34ecc7a3":"## Val Dataset","0ea73669":"# LOSS AND METRICS","be0a7f1e":"# Configuring TPU","ae9c7777":"## Test Dataset","09bb930c":"# Model","ae655846":"# Importing necessary Libraries","39c546f3":"# Training Loop","bb069138":"# Training","41636eb5":"# Important Variables","ef42c63a":"# Helper Functions","d0dffa79":"# Preparing Dataset"}}