{"cell_type":{"f967e614":"code","9082fdcc":"code","679b2933":"code","a5139fc3":"code","99f6804a":"code","9ee21d73":"code","594f4490":"code","fa294959":"code","e0c942f1":"code","96a99fb3":"code","76f8c9e0":"code","7ef10f39":"code","b5d0c3d2":"code","6de11bf3":"code","f636ce60":"code","cb166b59":"code","5e7899e2":"code","c7f19b35":"code","f49930a1":"code","a868e0c8":"code","23f857b1":"code","bd309410":"code","6347c18a":"code","e38f8491":"code","1ec3e22d":"code","06a9e98e":"code","c2f7e8c0":"code","efb572e0":"code","4f0aec58":"code","377d5d7c":"code","5f7e9304":"code","24c4fcc1":"code","ca4db7ee":"code","817d5a7d":"code","9cbdd5d9":"code","738c58c4":"code","dcc5beeb":"code","3f3f1459":"code","f3117b3b":"code","7df5d17c":"code","eb62d5fe":"code","589f3aa4":"code","f140929b":"code","e6c1cddd":"code","6efa4623":"markdown","3e6e6870":"markdown","15047b47":"markdown","2715d8bf":"markdown","cdb2c6a0":"markdown","681f53c1":"markdown","d3f6369e":"markdown","fcb820e4":"markdown","b5f59cd4":"markdown","19a99eba":"markdown","3972f341":"markdown","bb605a18":"markdown"},"source":{"f967e614":"import pandas as pd\nimport numpy as np\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","9082fdcc":"# Read the data\ndata = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n\n# Look at a snapshot of data\ndata.head()","679b2933":"data.shape","a5139fc3":"# See the summary stats and frequency distribution of features\ndata.describe()","99f6804a":"# Lets' see if there are any missing values\n\nprint (data.apply(lambda x: sum(x.isnull()),axis=0))\nprint (np.where(data.applymap(lambda x: x == ' ')))","9ee21d73":"# Let's see if there is class imbalance in the target variable\nprint (data['Churn'].value_counts(ascending=True))","594f4490":"data['TotalCharges'].replace(to_replace = ' ', value= np.nan, inplace = True)\ndata['TotalCharges'] = data['TotalCharges'].astype(float)\ndata.dropna(axis=0, inplace=True)","fa294959":"from sklearn.model_selection import train_test_split\n\ny = data['Churn']\nX = data.loc[:, data.columns != 'Churn']\n\nX_train, X_test, y_train, y_test =   train_test_split(X, y, test_size=0.20, random_state=111)\n\nprint(X_train.shape, X_test.shape)","e0c942f1":"### Remove customerID\nX_train.drop(['customerID'], axis = 1, inplace=True)\nX_test.drop(['customerID'], axis = 1, inplace=True)","96a99fb3":"# Impute missing values, if any!. Check number of missing values\nprint(\"Num missing values before imputation:\")\nprint(pd.DataFrame(X_train['TotalCharges']).isnull().sum())\n\nprint(\"Num missing values before imputation:\")\nprint(pd.DataFrame(X_test['TotalCharges']).isnull().sum())","76f8c9e0":"# Convert 'SeniorCitizen' column into categorical\nX_train['SeniorCitizen']=pd.Categorical(X_train['SeniorCitizen'])\nX_test['SeniorCitizen']=pd.Categorical(X_test['SeniorCitizen'])","7ef10f39":"# Encode target variables to 0, 1\ny_train = y_train.map(dict(Yes=1, No=0))\ny_test = y_test.map(dict(Yes = 1, No=0))\nprint(y_train.shape, y_test.shape)","b5d0c3d2":"# Divide the columns into 3 categories, one ofor standardisation, one for label encoding and one for one hot encoding\nnum_cols = [\"tenure\", 'MonthlyCharges', 'TotalCharges']\ncat_cols_ohe =['PaymentMethod', 'Contract', 'InternetService'] # those that need one-hot encoding\ncat_cols_le = list(set(X_train.columns)- set(num_cols) - set(cat_cols_ohe)) #those that need label encoding","6de11bf3":"from sklearn.preprocessing import StandardScaler\n\nscaler= StandardScaler()\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])\n","f636ce60":"X_train = pd.DataFrame(X_train)\nX_test= pd.DataFrame(X_test)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nle = LabelEncoder()\nohe = OneHotEncoder()\n\nfor col in cat_cols_le:\n    le.fit(X_train[col])\n    X_train[col] = le.transform(X_train[col])\n    X_test[col] = le.transform(X_test[col])\n\n","cb166b59":"print(X_train.shape, X_test.shape)","5e7899e2":"ohe.fit(X_train[cat_cols_ohe])\ntr_cols= ohe.transform(X_train[cat_cols_ohe])\nte_cols = ohe.transform(X_test[cat_cols_ohe])\n\nX_train.drop(columns=cat_cols_ohe, inplace=True)\nX_test.drop(columns=cat_cols_ohe, inplace=True)\n\nX_train = np.hstack((X_train,tr_cols.toarray()))\nX_test = np.hstack((X_test, te_cols.toarray()))\nprint(X_train.shape, X_test.shape)\n\n\n","c7f19b35":"print(y_train.shape, y_test.shape)","f49930a1":"print(y_train.value_counts(), '\\n', y_test.value_counts())","a868e0c8":"#Importing necessary modules\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom sklearn.model_selection import train_test_split","23f857b1":"seed = 7\nnp.random.seed(seed)","bd309410":"\nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\ninput_shape = X_train.shape[1]","6347c18a":"model = Sequential()\nmodel.add(Dense(32, input_dim=input_shape, kernel_initializer='uniform', activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.7))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n\nmodel.fit(X_train, y_train,\n              epochs=25,\n          batch_size=24, class_weight={0:0.2, 1:0.8})\n\n","e38f8491":"\nscore = model.evaluate(X_test, y_test, batch_size=20)","1ec3e22d":"print(score)\nprint (\"Accuracy : %s\" % \"{0:.3%}\".format(score[1]))\n","06a9e98e":"\ntrain_pred_dl=model.predict_classes(X_train)\ntest_pred_dl=model.predict_classes(X_test)","c2f7e8c0":"from sklearn import metrics\nmlp_conf_matrix = metrics.confusion_matrix(y_test, test_pred_dl)\nprint (mlp_conf_matrix)","efb572e0":"\naccuracy = metrics.accuracy_score(y_test,test_pred_dl)\n    \nprint (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n#Print Recall\nrecall = metrics.recall_score(y_test,test_pred_dl)\n    \nprint (\"Recall : %s\" % \"{0:.3%}\".format(recall))","4f0aec58":"encoding_dim  = 32\n# this is our input placeholder\ninput_img = Input(shape=(input_shape,))\n\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_img)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_shape, activation='sigmoid')(encoded)\n\n# this model maps an input to its reconstruction\nautoencoder = Model(inputs=input_img, outputs=decoded)","377d5d7c":"autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy') #optimizer = adam --- can also be used ","5f7e9304":"autoencoder.fit(X_train, X_train,\n                epochs=50,\n                batch_size=24,\n                shuffle=True,\n                validation_data=(X_test, X_test))","24c4fcc1":"# this model maps an input to its encoded representation\nencoder = Model(inputs=input_img, outputs=encoded)\n","ca4db7ee":"x_train_encoded = encoder.predict(X_train)\nx_test_encoded = encoder.predict(X_test)","817d5a7d":"x_test_encoded","9cbdd5d9":"x_train_encoded.shape","738c58c4":"x_test_encoded.shape","dcc5beeb":"model2 = Sequential()\n\nmodel2.add(Dense(64, input_dim = 32, kernel_initializer='uniform', activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(16, activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))","3f3f1459":"model2.compile(loss='binary_crossentropy',\n              optimizer='Adam',\n              metrics=['accuracy'])","f3117b3b":"x_train_encoded.shape","7df5d17c":"model2.fit(x_train_encoded, y_train, batch_size=32, epochs=25, class_weight={0:0.2, 1:0.8})\n","eb62d5fe":"score2 = model2.evaluate(x_test_encoded, y_test)\nprint (score2)","589f3aa4":"\ntrain_pred_dlac=model2.predict_classes(x_train_encoded)\ntest_pred_dlac=model2.predict_classes(x_test_encoded)","f140929b":"dlac_conf_matrix = metrics.confusion_matrix(y_test, test_pred_dlac)\nprint (dlac_conf_matrix)","e6c1cddd":"\naccuracy = metrics.accuracy_score(y_test,test_pred_dlac)\n    \nprint (\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n#Print Recall\nrecall = metrics.recall_score(y_test,test_pred_dlac)\n    \nprint (\"Recall : %s\" % \"{0:.3%}\".format(recall))","6efa4623":"### Split the data into train and test sets\n","3e6e6870":"### Import required libraries","15047b47":"## MLP using features from AutoEncoders","2715d8bf":"## Develop a Deep Learning Based Churn Prediction Engine\n","cdb2c6a0":"#### Load & understand the data ","681f53c1":"### Type Conversions","d3f6369e":"## Building the ANN Model","fcb820e4":"## Data Preprocessing","b5f59cd4":"### Standardizing numeric attributes","19a99eba":"### Encoding attributes","3972f341":"### Missing values","bb605a18":"### Observations:\n\n**On Type conversions:**\n\n- Columns like CustomerID can be removed from the analysis\n- We see that 'Tenure' and 'MonthlyCharges' are numeric columns present in the data, with the data close to normal distribution. \n- Along with them, 'TotalCharges' is also a numeric column but contains some info missing, but still is not a nan.\n- The column 'SeniorCitizen' is a categorical column by its nature with 'Yes' as 1, and No as 0. So it shuold be converted into Categorical type\n- All the categorical attribtues are strings. Hence there is need to convert them into numbers, by a way of encoding.\n- Among the categorical attribtues, majority of them have binary classes(2 levels). Label encoding would help assign labels 0,1 for the levels as appropriate.\n- But attributes like 'PaymentMethod', 'Contract', 'InternetService' are nominal and have more than 2 levels. So along with label encoding, we need to convert them into equidistant levels.\n\n**On Missingness of data:**\n The data is clean and there are no missing values in the data\n \n**On the class imbalance in the target attribute**\nThere are more instances where the customers din't churn than those that have custoemrs churned out. Class imbalance is clearly seen."}}