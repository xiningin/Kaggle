{"cell_type":{"0e68ea5b":"code","08d8b550":"code","e7b2a8cf":"code","78f2e42e":"code","48df1649":"code","5d14208e":"code","b87ab55d":"code","6cab63c1":"code","37ee4002":"code","d839364a":"code","65e2f800":"code","c7182963":"code","22cb7fb7":"code","24d4ee15":"code","da072618":"code","00b69639":"code","8b05a682":"code","662a53e2":"code","49d3ad4b":"code","4e1a18b9":"code","83a72de5":"code","a178851f":"code","4a279e86":"code","0cc0bc56":"code","dc12fd3d":"code","d1ae8cb8":"code","40dd902b":"code","c40f96e2":"code","a51bf6a9":"code","23902b13":"code","155c8b0d":"code","36ca0ed9":"code","2fd9610c":"code","1073ac61":"code","c7780714":"code","6f438768":"code","23346c14":"code","7a4d7b43":"code","6a193a81":"code","dc4ea845":"code","ed96b3cd":"code","31df9fe7":"code","868d9818":"code","75468ed5":"code","1c5c5687":"code","883c2036":"code","243564cf":"code","346389b0":"code","306a0bcf":"code","7461a2e0":"code","1ceaf9c1":"code","8b8f597c":"code","d3af1823":"code","bef20933":"code","126386fa":"code","6c3a4963":"code","bc5a9277":"code","754e72d2":"code","1fedb847":"code","0f7e3c32":"code","6ecff1cd":"code","3b67381e":"code","ec4c500e":"code","792af981":"code","c69203bf":"markdown","bdb566a1":"markdown","46851ae3":"markdown","11e67ea6":"markdown","157c6a14":"markdown","87cf7176":"markdown","cb896334":"markdown","4fc517ca":"markdown","b1b4848b":"markdown","00e96f93":"markdown","efc82059":"markdown","75e8aed3":"markdown","5c9c86a1":"markdown","6ff91f2d":"markdown","70536a2b":"markdown","85a6e1ff":"markdown","02b78102":"markdown","9edc4431":"markdown","d4f5a2bf":"markdown","b764386a":"markdown","3cd75177":"markdown","b6fe3102":"markdown","7bee263a":"markdown","bcad6f53":"markdown","5d4b2d61":"markdown","bbcb3ed3":"markdown","9c53f113":"markdown","653ca779":"markdown","5a4dabf7":"markdown","2956b33f":"markdown","6fee75b5":"markdown","8632e30b":"markdown","7671d725":"markdown","441ccc7e":"markdown","17c172fc":"markdown","2dc1c89a":"markdown","719db542":"markdown","dd04233a":"markdown","2d4eb8ef":"markdown","51237d75":"markdown","c21bd974":"markdown","893362ca":"markdown","0b765f97":"markdown","231ed205":"markdown","5253cd57":"markdown","2972a027":"markdown","3bc98295":"markdown","693ff5af":"markdown","0a248755":"markdown","9baa73bd":"markdown","cf2b1e49":"markdown","8ac493fa":"markdown","cfede874":"markdown","5971cf1d":"markdown","256d206e":"markdown","aebe5cdf":"markdown","4238f1fe":"markdown","57459984":"markdown","3505f04c":"markdown","f3124241":"markdown","cbab2a43":"markdown","695acfb1":"markdown","50335174":"markdown","c7f8d8ab":"markdown","99413b51":"markdown","5d1ae4f7":"markdown","f1406a9d":"markdown","a8fbbc8b":"markdown","6c2c3346":"markdown","ae11d12d":"markdown","812b0cc3":"markdown","ff948e37":"markdown","d0ba30cb":"markdown","96b2fd66":"markdown","6a820d32":"markdown","844a97cf":"markdown","a3528fb6":"markdown","36c67fa3":"markdown","a3218e71":"markdown"},"source":{"0e68ea5b":"import os\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.under_sampling import TomekLinks\nfrom itertools import permutations, combinations\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, ShuffleSplit, GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, recall_score, plot_roc_curve, matthews_corrcoef, classification_report, confusion_matrix\n\n\nwarnings.simplefilter('ignore')\n\nRandom_seed = 1\nsns.set_theme()","08d8b550":"files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\nfiles","e7b2a8cf":"df = pd.read_csv(files[0])","78f2e42e":"df.rename(columns=df.iloc[0], inplace = True)\ndf.drop(df.index[0], inplace = True)\ndf.head()","48df1649":"cl = df.columns[2:4]\nfor i, j in enumerate(cl):\n    print('Unique value of the ' + str(j) + ' features', df[j].unique())","5d14208e":"print('Unique index:', (df['EDUCATION'] == 'X3').argmax())\nprint('Unique index:', (df['EDUCATION'] == 'EDUCATION').argmax())","b87ab55d":"unique_i = []\nfor i in df.keys():\n    unique_i.append(df[i].loc[202])\nprint(unique_i, end=' ')","6cab63c1":"data = df.drop(index=[202, 203], inplace=False)","37ee4002":"plt.figure(figsize=(23, 3))\nsns.heatmap(data.isnull(), yticklabels=False, cbar=True, cmap='gray')\nplt.title(\"Missing values\")\nprint(\"\\033[1m\" + \" There is no missing value in this dataset\" + \"\\033[0m\")","d839364a":"dtype = []\nfor i in df.keys():\n    dtype.append((df[i]).dtypes)\nprint(dtype[0], end=' ')","65e2f800":"cat = ['SEX', 'EDUCATION', 'default payment next month']\nfor i, j in enumerate(cat):\n    data[j] = data[j].astype('string')\n    data[j] = data[j].astype('str')","c7182963":"for i in range(data.shape[0]):\n    if   data['SEX'].iloc[i] == 'female':\n        data['SEX'].iloc[i] = 2\n    elif data['SEX'].iloc[i] == 'male':\n        data['SEX'].iloc[i] = 1","22cb7fb7":"for i in range(data.shape[0]):\n    if data['EDUCATION'].iloc[i] == 'university':\n        data['EDUCATION'].iloc[i] = 2\n    elif data['EDUCATION'].iloc[i] == 'graduate school':\n        data['EDUCATION'].iloc[i] = 1\n    elif data['EDUCATION'].iloc[i] == 'high school':\n        data['EDUCATION'].iloc[i] = 3\n    else:\n        data['EDUCATION'].iloc[i] = 0","24d4ee15":"for i in range(data.shape[0]):\n    if  data['default payment next month'].iloc[i] == 'default':\n        data['default payment next month'].iloc[i] = 1\n    else: data['default payment next month'].iloc[i] = 0","da072618":"for i, j in enumerate(data.columns[:12]):\n    data[j] = data[j].astype('int32')\nfor i, j in enumerate(data.columns[12:24]):\n    data[j] = data[j].astype('float64')\ndata['default payment next month'] = data['default payment next month'].astype('int32')","00b69639":"data.info()","8b05a682":"plt.figure(figsize=(10, 10))\nfor i, j in enumerate(data.columns):\n    plt.subplot(5, 5, i+1)\n    plt.boxplot(data[j], 0, 'gray', showmeans=True,\n                meanline=True, autorange=True)\n    plt.title('Box plt - ' + str(data.columns[i]))\nplt.tight_layout()","662a53e2":"def delete_outliers(cl):\n\n    Q1 = np.percentile(cl, 25, interpolation='midpoint')\n    Q3 = np.percentile(cl, 65, interpolation='midpoint')\n    IQR = Q3 - Q1\n\n    upper = np.where(cl >= (Q3+1.5*IQR))\n    lower = np.where(cl <= (Q1-1.5*IQR))\n    new_cl = pd.DataFrame(cl)\n    new_cl.drop(upper[0], inplace=True)\n    new_cl.drop(lower[0], inplace=True)\n\n    return new_cl","49d3ad4b":"PAY_ = data.columns[6:24]\nfor i in PAY_:\n    data[i] = delete_outliers((data[i]).values)","4e1a18b9":"data_n = data.dropna(axis=0, how='any', inplace=False)\nprint('\\n', 'main data shape:', data.shape, '\\n',\n      'data shape after removing the outliers:', data_n.shape, '\\n',\n      'number of removed outliers:', data.shape[0] - data_n.shape[0])","83a72de5":"figure, axes = plt.subplots(figsize=(15, 10))\nfor i, j in enumerate(data_n.columns[1:]):\n    plt.subplot(5, 5, i+1)\n    plt.hist(data_n[j], color='black', histtype='bar', alpha=0.7)\n    plt.title(str(data_n.columns[i+1]), pad=2)\n    plt.ylabel(\"Values\")\nfigure.tight_layout()","a178851f":"payments = data_n.iloc[:, 6:12]\nrepayment_mode = np.zeros((data_n.shape[0]))\nfor i in range(data_n.shape[0]):\n    repayment_mode[i] = np.mean(payments.iloc[i, :], axis=0)\ndata_n['repayment_mode'] = np.ceil(repayment_mode)\nnp.unique(data_n['repayment_mode'])","4a279e86":"sns.heatmap(data_n[['repayment_mode', 'LIMIT_BAL','default payment next month']].corr(), annot=True, cmap = 'Greys')\nplt.xticks(rotation=15)\nplt.title('correlation')\nplt.savefig('correlation.png')","0cc0bc56":"cat = ['SEX', 'EDUCATION', 'MARRIAGE', 'default payment next month']\n\nplt.figure(figsize=(20, 5))\nfor i, j in enumerate(cat):\n    plt.subplot(1, 4, i+1)\n    sns.countplot(x=j, data=data_n, color='gray')\n    plt.title('Count plot of ' + j)\nplt.tight_layout()\n","dc12fd3d":"bill_amount = data_n.columns[12:18]\npay_amount = data_n.columns[18:24]","d1ae8cb8":"plt.figure(figsize=(10, 5))\nplt.subplot(2, 2, 1)\nsns.heatmap(data_n[bill_amount].corr(), annot=True, cmap='Greys')\nplt.title(\"bill_amount\")\nplt.subplot(2, 2, 2)\nsns.heatmap(data_n[pay_amount].corr(), annot=True, cmap='Greys')\nplt.title(\"pay_amount\")\nplt.tight_layout()","40dd902b":"df_pca = data_n[bill_amount]\nX = (df_pca).values\nscale = StandardScaler()\nscale.fit(X)\nX = scale.transform(X)\n\n\npca = PCA(n_components=2)\npcaScale = pca.fit_transform(X)\npcadf = pd.DataFrame(data=pcaScale, columns=[\n                     'PCA_1', 'PCA_2'])\npcadf.head(5)","c40f96e2":"def dist(data):\n    sns.set_theme()\n    sns.distplot(data, color='gray')","a51bf6a9":"plt.figure(figsize=(10, 5))\nfor i, j in enumerate(bill_amount):\n    plt.subplot(2, 3, i+1)\n    dist(data_n[j])\n    plt.tight_layout()\nprint(\"Bill amount distribution\")","23902b13":"plt.figure(figsize=(10, 5))\nfor i, j in enumerate(pay_amount):\n    plt.subplot(2, 3, i+1)\n    dist(data_n[j])\n    plt.tight_layout()\nprint(\"paid amount distribution\")","155c8b0d":"data_n['SUM_BILL_AMT'] = np.sum(data_n[bill_amount], axis=1)\ndata_n['SUM_PAY_AMT'] = np.sum(data_n[pay_amount], axis=1)","36ca0ed9":"sns.heatmap(data_n[['SUM_BILL_AMT', 'SUM_PAY_AMT', 'SEX', 'EDUCATION', 'MARRIAGE', 'default payment next month', \n            'LIMIT_BAL', 'repayment_mode']].corr(), annot=True, cmap='Greys')\nplt.title('correlation')\nplt.savefig('correlation.png')","2fd9610c":"dist(data_n['LIMIT_BAL'])","1073ac61":"plt.figure(figsize=(10, 7))\nfor i, j in enumerate(cat):\n    plt.subplot(2, 2, i+1)\n    sns.boxplot(x=data_n[j], y=data_n['LIMIT_BAL'])\nplt.tight_layout()","c7780714":"plt.figure(figsize=(10, 7))\nfor i, j in enumerate(cat):\n    plt.subplot(2, 2, i+1)\n    sns.violinplot(x=data_n[j], y=data_n['LIMIT_BAL'])\nplt.tight_layout()","6f438768":"plt.figure(figsize=(10, 7))\nfor i, j in enumerate(cat):\n    plt.subplot(1, 4, i+1)\n    colors = sns.color_palette('pastel')[0:5]\n    plt.pie(data_n[cat[i]].value_counts(), autopct='%1.1f%%',\n            labels=np.unique(data_n[cat[i]]), colors=colors)\n    plt.title((cat[i]))\nplt.tight_layout()","23346c14":"def scatter(X, y, title_x, title_y):\n    plt.scatter(X, y, marker='o', c='black')\n    plt.title(\"Scatter plot\")\n    plt.xlabel(title_x)\n    plt.ylabel(title_y)\n    plt.tight_layout()\n    m, b = np.polyfit(X, y, 1)\n    plt.plot(X, m*X + b, 'r--', label='linear regression')\n    plt.legend()","7a4d7b43":"sample = data_n.sample(frac=0.007, random_state=Random_seed)","6a193a81":"cl = ['LIMIT_BAL', 'SUM_BILL_AMT', 'SUM_PAY_AMT']\n\np = combinations(cl, 2)\nplt.figure(figsize=(20, 7))\nfor i, j in enumerate(p):\n    plt.subplot(2, 5, i+1)\n    scatter(sample[j[0]], sample[j[1]], j[0], j[1])","dc4ea845":"def line_plot(X: np.array, y: np.array,\n              title_x: str, title_y: str,\n              xlabel: str, ylabel: str,\n              title: str) -> 'Plot':\n    plt.plot(X, label=title_x, color='black', alpha=0.5)\n    if y is not None:\n        plt.plot(y, label=title_y, color='black')\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.grid(True)\n    plt.legend()\n\n\nline_plot.__annotations__","ed96b3cd":"line_plot(X=sample['SUM_BILL_AMT'].values,\n          y=sample['SUM_PAY_AMT'].values,\n          title_x='Sum of the bill amount',\n          title_y='Sum of the paid bills',\n          xlabel='clients',\n          ylabel='Amount',\n          title='The trend of the bill summation')","31df9fe7":"line_plot(X=(sample['LIMIT_BAL']).values, y=None, title_x='Sum of the bill amount',\n          title_y=None, xlabel='clients',\n          ylabel='Amount',\n          title='The trend of credit')","868d9818":"new_data = data_n[['ID', 'LIMIT_BAL', 'SEX',\n                 'EDUCATION', 'MARRIAGE',\n                 'AGE', 'default payment next month',\n                 'repayment_mode', 'SUM_BILL_AMT',\n                 'SUM_PAY_AMT']]\n\nnew_data.head(5).style.background_gradient(cmap='gist_gray_r').set_precision(0)","75468ed5":"fig = plt.figure(figsize=(20, 7))\n\nplt.subplot(2, 2, 1)\nsns.kdeplot(new_data[new_data['default payment next month']==1]['SUM_PAY_AMT'])\nsns.kdeplot(new_data[new_data['default payment next month']==0]['SUM_PAY_AMT'])\n\nfig.legend(labels=['Defaulted', 'Not Defaulted'])\nplt.title('Based on summation of the payment')\n\nplt.subplot(2, 2, 2)\nsns.kdeplot(new_data[new_data['default payment next month']==1]['repayment_mode'])\nsns.kdeplot(new_data[new_data['default payment next month']==0]['repayment_mode'])\nfig.legend(labels=['Defaulted', 'Not Defaulted'])\nplt.title('Based on repayment_mode')\nplt.tight_layout()","1c5c5687":"# Showing the progress Bar\n\ndef ProgressBar(percent, barLen=20):\n    sys.stdout.write(\"\\r\")\n    progress = \"\"\n    for i in range(barLen):\n        if i < int(barLen * percent):\n            progress += \"=\"\n        else:\n            progress += \" \"\n    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n    sys.stdout.flush()","883c2036":"X = (new_data.drop(columns=['ID', 'default payment next month'], axis=0)).values\ny = (new_data['default payment next month']).values\n\nX = np.ascontiguousarray(X, dtype=np.float64)\ny = y.astype('int32')","243564cf":"cv = 5\nerr = np.zeros((cv,))\npred = np.zeros_like(y)\nproba = np.zeros((y.shape[0], int(len(np.unique(y)))))\nbest_params = []\n\nk = ShuffleSplit(n_splits=cv, test_size=0.3, random_state=Random_seed)\n\nfor cv_i, (train_index, test_index) in enumerate(k.split(X, y)):\n    x_train, x_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model = MLPClassifier(solver='adam', hidden_layer_sizes=(100,), random_state=Random_seed)\n\n    pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n\n    param_grid = {'clf__hidden_layer_sizes': [10, 20, 50, 100]}\n\n    clf = RandomizedSearchCV(estimator=pipe,\n                             param_distributions=param_grid,\n                             scoring='accuracy',\n                             refit=True,\n                             cv=2,\n                             verbose=False,\n                             n_jobs=-1)\n\n\n    clf.fit(x_train, y_train)\n\n    pred[test_index] = clf.predict(x_test)\n\n    proba[test_index, :] = clf.predict_proba(x_test)\n\n    err[cv_i, ] = clf.score(x_test, y_test)\n\n    best_params.append(clf.best_params_)\n\n    ProgressBar(cv_i\/abs((cv)-1), barLen=cv+1)\n","346389b0":"plt.plot(err)\nplt.xlabel('cross-validated Fold')\nplt.ylabel('Accuracy')\nplt.title('The accuracy of different folds ')","306a0bcf":"print(classification_report(y, pred))","7461a2e0":"plt.figure(figsize=(20, 5))\nplot_roc_curve(clf, x_test, y_test)\nplt.title('ROC plot')\nprint('accuracy:', np.mean(err))","1ceaf9c1":"sns.heatmap(confusion_matrix(y, pred), annot=True, cmap='PuBu')","8b8f597c":"nn = (pd.DataFrame(best_params).mode()).values[0][0]\nprint('optimized number of hidden neuron:', nn)","d3af1823":"# This method, returns the probability of the `Default payment` \n\ndef proba(X: np.array, y: np.array, bin: int):\n    model = MLPClassifier(solver='adam', hidden_layer_sizes=(100,))\n    model.fit(X, y)\n    proba = model.predict_proba(X)[:, bin]\n\n    plt.hist(proba[y == 0], bins=50, label='Negatives')\n    plt.hist(proba[y == 1], bins=50, label='Positives', alpha=0.7, color='r')\n    if bin == 1:\n        plt.title('Default payment for probability')\n        plt.xlabel('Probability of having default payment for next month')\n    if bin == 0:\n        plt.title('No default payment for probability')\n        plt.xlabel('Probability of not having default payment for next month')\n    plt.ylabel('default payment next month')\n    plt.legend()\n    return (classification_report(y, model.predict(X), target_names=['0', '1']))","bef20933":"plt.figure(figsize=(15, 5))\nfor i in range(2):\n    plt.subplot(1, 2, i+1)\n    print(proba(X, y, i))\nplt.tight_layout()","126386fa":"print(\"class 1 size, Before undersampling: {}\".format(sum(y==1)))\nprint(\"class 0 size, Before undersampling: {} \\n\".format(sum(y==0)))\n\nplt.figure(figsize=(10, 3))\nplt.subplot(1, 2, 1)\nsns.kdeplot(y)\nplt.title('Distribution of the class labels')\nplt.xlabel('class label')\n\nplt.subplot(1, 2, 2)\nfor class_value in range(2):\n    row_ix = np.where(y == class_value)\n    plt.scatter(X[row_ix, 0], X[row_ix, 1])\nplt.title('samples from each class')","6c3a4963":"undersample = TomekLinks(sampling_strategy = 'majority')\nX, y  = undersample.fit_resample(X, y)","bc5a9277":"print(\"class 1 size, after undersampling: {}\".format(sum(y==1)))\nprint(\"class 0 size, after undersampling: {} \\n\".format(sum(y==0)))","754e72d2":"cv = 2\nerr = np.zeros((cv,))\npred = np.zeros_like(y)\nproba = np.zeros((y.shape[0], int(len(np.unique(y)))))\nbest_params = []\n\nk = StratifiedKFold(n_splits=cv, shuffle=True, random_state=Random_seed)\n\nfor cv_i, (train_index, test_index) in enumerate(k.split(X, y)):\n    x_train, x_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model = MLPClassifier(solver='adam', hidden_layer_sizes=(\n        100,), random_state=Random_seed)\n\n    pipe = Pipeline([('scaler', StandardScaler()), ('clf', model)])\n\n    param_grid = {'clf__hidden_layer_sizes': [10, 20, 50, 100]}\n\n    pipe.fit(x_train, y_train)\n\n    pred[test_index] = pipe.predict(x_test)\n\n    proba[test_index, :] = pipe.predict_proba(x_test)\n\n    err[cv_i, ] = pipe.score(x_test, y_test)\n\n    ProgressBar(cv_i\/abs((cv)-1), barLen=cv+1)","1fedb847":"sns.heatmap(confusion_matrix(y, pred, normalize='true'), annot=True, cmap='PuBu')","0f7e3c32":"print(classification_report(y, pred))","6ecff1cd":"# A method to train a regressor neural network for the regression problem\n\ndef model_nn(cv, X, y):\n\n    err = np.zeros((cv,))\n    pred = np.zeros_like(y)\n\n    k = ShuffleSplit(n_splits=cv, test_size=0.3, random_state=Random_seed)\n\n    for cv_i, (train_index, test_index) in enumerate(k.split(X, y)):\n        x_train, x_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        model = MLPRegressor(solver='adam', hidden_layer_sizes=(100,), random_state=Random_seed)\n\n        pipe = Pipeline([('scaler', StandardScaler()), ('reg', model)])\n\n        param_grid = {'reg__hidden_layer_sizes': bernoulli(0.3)}\n\n        clf = RandomizedSearchCV(estimator=pipe,\n                                 param_distributions=param_grid,\n                                 scoring='neg_root_mean_squared_error',\n                                 refit=True,\n                                 cv=2,\n                                 verbose=True)\n\n        clf.fit(x_train, y_train)\n\n        pred[test_index] = clf.predict(x_test)\n\n        err[cv_i, ] = clf.score(x_test, y_test)\n\n        ProgressBar(cv_i\/abs((cv)-1), barLen=cv+1)\n    \n    print('RMSE:', np.mean(err, axis=0))\n\n    return pred","3b67381e":"bill_amt = data_n.columns[12:18]\nsum_pay_amt = data_n.columns[27]\n\nX = (data_n[bill_amt]).values\ny = (data_n[sum_pay_amt]).values\n\nX = np.ascontiguousarray(X, dtype=np.float64)\ny = y.astype('float64')\n\npred_bill_amt = model_nn(2, X, y)","ec4c500e":"pd.DataFrame(pred_bill_amt).T","792af981":"sns.kdeplot(pred_bill_amt, color ='red')\nplt.title('Predicted amount of the next bill')\nplt.xlabel('amount of the bill $')\nwarnings.simplefilter('ignore')","c69203bf":"### Check the distribution of credit for the categorical features","bdb566a1":"### Aggregating the features of the bill (Amount and payed)","46851ae3":"##### Define the method to plot the line plot","11e67ea6":">The credit limit has almost the same distribution as paid and amount of bills.","157c6a14":"#### Check correlation between the sum of paid and amount of bills and other categorical features","87cf7176":"### Check the distribution of the bills","cb896334":"Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)","4fc517ca":"Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)","b1b4848b":"The above plot, give us the histogram of the `default payment next month` in terms of the probability. \n\nAt a glance, we can see the `default payment next month` has a normal distribution over `repayment_mode` which is an extracted feature based on the average of the repayments. So, we can be sure about the performance of the model over this feature.","00e96f93":"<a id='Questions'><\/a>\n\n# Investigative Questions:\n\n## 1. How do you ensure that customers can\/will pay their loans?\n\nBy applying the classification model over the response variable, we would have the probability of default payment of the next month, so this is one of the tens indexes that one can decide lending money as a loan.\n\nMoreover, by utilizing the regression model over different variables and extracting different features, we can check the feature bills and payment of the customers as well.\n\n## 2. Can we approve customers with high certainty?\n\nNo. The evaluations of the models are not satisfying. From my point of view, we need more data to build a more robust model and rely n that with better generalization test scores on the unseen dataset.\n\nFor instance, the `ROC` metric shows that the result does not tend to `True Positive` labels. Or, we can see the almost same low result from other metrics.\n\n## 3. Which attributes in the data can we deem to be statistically significant to the problem at hand?\n\nThe payment, repayment status, and bill amount have a significant effect on the response variable.\n\n## 4. What concrete information can we derive from the data we have?\n\n\nExtracting different features and finding that there is a relation ship between them.\nWe can observe that the `bills payments` are correlated to the next month's value. This situation is the same for the `bill amount`.\nMoreover, we understood that dataset distribution is not balanced.\n\nOther information is as follows\n<ul>\n    <li> Probability of the different features <\/li>\n    <li> distribution of the features concerning their self and other features<\/li>\n    <li> Repayment status <\/li>\n    <li> distribution of the class labels <\/li>\n    <li> correlation between pairwise features <\/li>\n    <li> relationship between categorical features <\/li>\n    <li> Train the supervised machine learning models <\/li>\n    <li> Return the optimized hyperparameters of the models <\/li>\n<\/ul>\n   \n\n","efc82059":"# Check the outliers","75e8aed3":"### Let's summarize some of my findings from the above plot;\n<ul>\n    <li> The lower credit limit increase the delay in payment, but we can not be sure as the negative correlation is insignificant <\/li>\n<\/ul>\n\nAuthor: [Seyedsaman Emami](https:\/\/github.com\/samanemami)","5c9c86a1":"## ROC and Confusion Matrix\n<h4> Receiver Operator Characteristic <\/h4>\n\nPrint out the confusion matrix for two classes","6ff91f2d":"## Check a simple method\nNow, let's review a simple model without pipeline and Grid search method to see the advantage of the probability.\n\nHere, we are measuring the probability of one class.\n\nNote that in the following I did not split the data, as I only wanted to show the probability of the `default payment` and the distribution of them","70536a2b":"## EDA on Repayment history","85a6e1ff":"### PCA convert dimension of six to two","02b78102":"#### [Back to top](#index)","9edc4431":"### <font color='red'>Scatter time!<\/font> \n\n##### Define the method","d4f5a2bf":"# Conslusion","b764386a":"If you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.\n\nAuthor: [Seyedsaman Emami](https:\/\/github.com\/samanemami)","3cd75177":"# Author: Seyedsaman Emami\n\n| **Author** | **GitHub** | \n| :-- | :-- | \n| Seyedsaman Emami | [GitHub profile](https:\/\/github.com\/samanemami) |\n\n---\n<hr>\n\n## Abouth this NoteBook\n\n### EDA\nIn the following Notebook, I implemented a comprehensive EDA over a downloaded SQL dataset after converting it to the Data frame. For the EDA part, I considered the following;\n\n* Cleaning the dataset\n* Feature engineering\n* modifying the data types\n* Outlier treatment\n* Statistical analysis\n* Categorical analysis\n* PCA\n\nMoreover, I considered different features by extracting them from the dataset.\nFor EDA, I also considered each feature group separately.\n\n### Model Selection\n\nFor this part, I considered two different problems two deal with them, binary classification and regression.\nFor the regression problem, I considered the bill amount as the independent variable and the summation of the paid amount as the dependant variable.\n\nFor the classification, I used the default payment as my dependant variable.\n\nI explain each problem in detail in the following.\n\n\n#### Binary classification\n\nCheck out the independent variable for the classification;\n<ul>\n    <li> LIMIT_BAL <\/li>\n    <li> SEX <\/li>\n    <li> EDUCATION<\/li>\n    <li> MARRIAGE <\/li>\n    <li> AGE <\/li>\n    <li> repayment_mode<\/li>\n    <li> SUM_BILL_AMT <\/li>\n    <li> SUM_PAY_AMT <\/li>\n<\/ul>\n\n` y = data['default payment next month']`\n\nMoreover, I will address the imbalanced data distribution and the proper approaches to detect and treat them.\n\nYou can find different metrics for model evaluation as well.\n\n#### Regression\n\nFor the regression, I considered the following features as the input and tried to predict the summation of the payment considering the 7th month\n<ul>\n    <li> BILL_AMT1 <\/li>\n    <li> BILL_AMT2 <\/li>\n    <li> BILL_AMT3 <\/li>\n    <li> BILL_AMT4 <\/li>\n    <li> BILL_AMT5 <\/li>\n    <li> BILL_AMT6 <\/li>  \n<\/ul>\n\nAuthor: [Seyedsaman Emami](https:\/\/github.com\/samanemami)","b6fe3102":"<h4> To work smoothly with the dataset, I again changed the response variable's position in the data frame. <\/h4>","7bee263a":"# Check the distribution of the default payment ","bcad6f53":"# Handling Imbalanced Data\n## SMOTE to under sample bigger class\n\nDue to the class label distribution and the reports (classification report, confusion Matrix), we found that the class labels are imbalanced. So, I am going to implement the SMOTE to improve the performance of the model, in terms of selecting both classes to learn.","5d4b2d61":"## Classification report\n\nShowing the main classification report.\nFor this matter, I take advantage of the Sklearn Library.\n\nAs for the `y_tets` I used the `y` because I already implemented the cross-validation, so the pred contains all the test_indices.","bbcb3ed3":"# Train the model with the under sampled data","9c53f113":"### What is telling us this plot?\n<ul>\n    <li> An increasing amount of bills causes a better situation in terms of repayment. <\/li>\n    <li> It is evident that by increasing the bill amount, the bill payment will increase too <\/li>\n    <li> There is no meaningful relationship between Education level and bill amount <\/li>\n    <li> The credit limit has a high correlation between paid and amount of bills <\/li>\n    <li> There is a significant relationship between default payment next month and repayment status <\/li>\n<\/ul>","653ca779":"> Aaaaaaand yes, of course, we have .....Outliers.\n\nNow, time to deal with them!\n> First, let me define a method to delete the outliers for us","5a4dabf7":"Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)\n\nIf you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.","2956b33f":"> Bills are higher than the paid amount on the trend, so the company does not get back its credit","6fee75b5":"> At the first glance, the plots show that the credit limit correlates with bills (both paid and amount)\n\nAuthor: Seyedsaman Emami","8632e30b":"#### Dropping the unique value and define a new data frame","7671d725":"# Evaluation the classifier\n### Accuracy","441ccc7e":"<h4> So, I plotted the scatter plots between different pair variables alongside the regression line by calculating the slope and intercept between X and y. <\/h4>","17c172fc":"## Optimized hyperparameters\n##### Now, let's check the optimized hyperparameters and build our model based on them","2dc1c89a":"# Default payment\n## Classification\n\nIn the following, I am going to predict the `Default payment` as a binary classification and return the probabilities to have a great `KPI` for decision making.\n\nMoreover, I will check the distribution of the `Default payment` in terms of the Density base of the `repayment mode` and `summation of the payments`.","719db542":"<a id='Statistical'><\/a>\n# Statistical report\n\nIn this part, I investigate the statistical features of the attributes to find different relationships\n\n##  Histogram","dd04233a":"### 1.1.3. Missing values","2d4eb8ef":"### Create a new feature that indicates the customers' repayemnt mode by taking the simple average","51237d75":"## Regression\n\nTraining the shallow Neural Network\nTraining the shallow Neural Network for regression.\n\nIn the following, I used shuffle split as my cross-validation method with 10-folds. The reason which I used this method is the matter of training speed.\n\nThe model is MLPRegressor with 100 Neurons and one hidden layer. I used Adam as the optimizer due to its coverage rates.\n\nMLPRegressor(solver='adam', hidden_layer_sizes=(100,))\nAs I was using the NN, I had to normalize my data, so I preferred a pipeline to push my data through the pipeline.\n\nFor tunning the hyperparameters, I rathered to use the RandomizedSearchCV this will let me use the statistical distribution for my grid instead of using the constant values, which I believe is better due to the training speed and performance.\n\nThe scoring function for ranking the folds is `neg_root_mean_squared_error`.\n\nAuthor: [Seyedsaman (Sam) Emami](https:\/\/github.com\/samanemami)","c21bd974":"##### Define new dataset with customized features","893362ca":"### Check and change\/define the data type","0b765f97":"<h4> So I find out that our categorical features have another value that is not related to the columns <\/h4>\n\n#### Find the related index of the unique 'X' Value","231ed205":"## EDA on the categorical features","5253cd57":"<a id='index'><\/a>\n# Table of contents\n\n* [Dataset](#Dataset)\n* [Statistical report](#Statistical)\n* [Model selection](#model)\n* [Answer to the investigative Questions](#Questions)\n\n### Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)\n\nIf you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.","2972a027":"#### Define data types","3bc98295":"#### [Back to top](#index)\nAuthor Seyedsaman Emami","693ff5af":"### 1.1.2. Feature engineering","0a248755":"# Predict the summation of the payment considering the 7th month","9baa73bd":"### Checking the relationship between categorical features and credit limit","cf2b1e49":"# Import libraries","8ac493fa":"####  [back to top](#index)","cfede874":"**Describe the above plot briefly**\n<ul>\n    <li>We can see almost a normal distribution for clients who have hot academic degrees. <\/li>\n    <li>The payment next month if is default then has more skewed distribution and with the lower amount of credit.<\/li>\n<\/ul>\n\nAuthor: [Seyedsaman Emami](https:\/\/github.com\/samanemami)","5971cf1d":"From the classification report we can see that the class labels are imbalance","256d206e":"#### I was thinking it would be better to check the repayment of our customers and define a risk level for each of them\n\nNow let's look at the correlation between the old features and repayment status to see if we are lucky to catch something from this data set ;)","aebe5cdf":"#### Data sampling","4238f1fe":"#### define the data types","57459984":"# Optimizing the hyperparameters","3505f04c":"### 1.1.1. Define the header \n<h4> Define the header of the dataset by replacing the index <\/h4>","f3124241":"Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)\n\nIf you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.","cbab2a43":"<a id='Q1'><\/a>","695acfb1":"<h4> Check the relation with the scatter and regression line <\/h4>\n\nAs in the columns, there are too many <font color='red'> **combinations** <\/font>, I considered all of the possible combinations in my list of features to plot","50335174":"The `pred_bill_amt` value, identify the estimated amount of the future bill for each customer\n\n# Kernel Density Estimation\n\nWe can check the probability of the future pay summation for the customers, as we can see the probable amounts are less ten 500 $.\n\nSo, one strategy would identifying the customers with lower pay_summation amount to give them a loan","c7f8d8ab":"##### [Back to top](#index)\n<a id='model' ><\/a>\n# Modeling\n\n## Classification\n\nThe classifier is a shallow Neural Network with 100 hidden neurons.\nThis model has been tested over and over with different statistical modifications over the dataset.\n\nIn the following, I trained the classifier model over the response binary attribute which is the `Default payment`. \n\nI used the Classification report, ROC, confusion matrix, and Accuracy to assess the model performance.\n\nI found that the dataset is imbalanced so I tried to handle the data distribution by using Tomek Link and selecting the sub-sample of the dataset.\n\n","99413b51":"<h4> Let's make sure that this situation is the same for the rest of the features <\/h4>","5d1ae4f7":"**Describe the above plot briefly**\n<ul>\n    <li>We still have outliers. <\/li>\n    <li>25th percentile observations of both men and women are the same. <\/li>\n    <li>The median (Q2) of customers with high school degrees is more leading<\/li>\n<\/ul>","f1406a9d":"#### Change the data type for each column\n\n\n#### one-hot encoding","a8fbbc8b":"<h4> The above plots show us lots of information; <\/h4>\n<ul>\n    <li> The Sex, Education, and default payment binarized perfectly. <\/li>\n    <li> Most of the customers are young. <\/li>\n    <li> we can observe the distribution of each payment.<\/li>\n<\/ul>\n\n\nAuthor: Seyedsaman Emami","6c2c3346":"## Train the regression model","ae11d12d":"We can see the under-sampling effect on the model performance, as the model is now considering both classes.","812b0cc3":"## EDA on Credit limit","ff948e37":"We can observe that the CF Matrix returns the same result as the classification report, the reason goes back to the data distribution.","d0ba30cb":"<a id='Dataset'><\/a>\n\nDataset\n##  Import dataset from sql ","96b2fd66":"## 1.1. Data cleaning","6a820d32":"## EDA on payment bills","844a97cf":"<h5>We are looking to a ROC curve that is more close to the True positive<\/h5>\n\n> I personally, want a better curve than this\n\nThis plot, maintain the result of the classification report","a3528fb6":"The above model shows that different class distribution without cross-validation","36c67fa3":"## undersampling strategy\nT-Link is a pair of data points from different classes (nearest-neighbors)","a3218e71":"### Summary of the above plot;\n<ul>\n    <li> There is a high correlation between each bill amount. <\/li>\n    <li> The bills (paid and amount) are not normally distributed, it will reduce some ML performances <\/li>\n<\/ul>"}}