{"cell_type":{"8156ede9":"code","15712b0e":"code","e5ce8362":"code","f8d19c8e":"code","dff269cc":"code","203f16e4":"code","78d4f95d":"code","2576ee39":"code","5411d5f5":"code","97d587eb":"code","75f58da9":"code","2ba3d369":"code","74116bbf":"code","8050e85b":"code","61916bd8":"code","d186caae":"markdown","4904460f":"markdown","18f4bf09":"markdown","95514166":"markdown","9d810f91":"markdown","b0c2fd08":"markdown","d6f451ca":"markdown","ab95ca9e":"markdown","bd6dd2a5":"markdown","e6d53f04":"markdown","28d775f5":"markdown","d3c50a27":"markdown","ef92302c":"markdown","f1a2ca68":"markdown","8696541f":"markdown"},"source":{"8156ede9":"# Importing libraries and loading the files\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Load files\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# Checking for NaNs for each column\nprint(train.isna().sum())\nprint(test.isna().sum())","15712b0e":"# To test the importance of Age, we can calculate the correlation matrix a see how it correlates with the other variables\nprint(train.corr()) \n\n# It seems pretty uncorrelated with the survived column. There is a weak correlation both with Pclass and SibSp.\n# Age could be dropped and Pclass and SibSp used instead in thqe model (especially the first one).\n# Fare is als correlated with the survived column. Embarked should be filled with the most frequent value of the column\ntrain['Embarked'] = train['Embarked'].fillna('S')\ntrain['Family_size'] = train['SibSp']+train['Parch']+1 #summing up the two columns to obtain the family size per each passenger\ntrain['Fare_class'] = pd.qcut(train.Fare, 5, labels=False) #Fare divided into 5 classes\n\n# We look also at the test set to check for eventual NaN and to fill what is deemed important\ntest['Fare'] = test['Fare'].fillna(test.Fare.mean())\ntest['Family_size'] = test['SibSp']+test['Parch']+1 #summing up the two columns to obtain the family size per each passenger\ntest['Fare_class'] = pd.qcut(test.Fare, 5, labels=False) #Fare divided into 5 classes","e5ce8362":"# Calculating the percentage of survived passengers depending on the class, sex, siblings\/spouse, and parents\/children\n# Pclass vs Survived\nprint(train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\n# sex vs Survived\nprint(train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\n# SibSp vs Survived\nprint(train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\n# Parch vs Survived\nprint(train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\n# Family_size vs Survived\nprint(train[[\"Family_size\", \"Survived\"]].groupby(['Family_size'], as_index=False).mean().sort_values(by='Survived', ascending=False))\n\n# Fare_class vs Survived\nprint(train[[\"Fare_class\", \"Survived\"]].groupby(['Fare_class'], as_index=False).mean().sort_values(by='Survived', ascending=False))","f8d19c8e":"# Data visualization: number of people per class, sex, and family size, plus how many survived per feature\nf, axes = plt.subplots(4, 3, figsize=(15, 15))\nsns.catplot('Pclass',data=train,kind='count',ax=axes[0,0]) #Number of people per ticket class\nsns.catplot('Sex',data=train,hue='Pclass',kind='count',ax=axes[0,1]) #Number of people per sex, further divided per ticket class\nsns.catplot('Family_size',data=train,hue='Pclass',kind='count',ax=axes[0,2]) #Number of people per family size, further divided per ticket class\nsns.catplot('Family_size',data=train,hue='Sex',kind='count',ax=axes[1,0]) #Number of people per family size, further divided per sex\nsns.catplot('Fare_class',data=train,hue='Pclass',kind='count',ax=axes[1,1]) #Number of people per fare class, further divided per ticket class\nsns.catplot('Fare_class',data=train,hue='Sex',kind='count',ax=axes[1,2]) #Number of people per fare class, further divided per sex\nsns.catplot(x='Pclass',y='Survived',data=train,kind='bar',ci=None,ax=axes[2,0]) #Number of survived people per ticket class\nsns.catplot(x='Sex',y='Survived',data=train,hue='Pclass',kind='bar',ci=None,ax=axes[2,1]) #Number of survived people per sex, further divided per ticket class\nsns.catplot(x='Family_size',y='Survived',data=train,hue='Pclass',kind='bar',ci=None,ax=axes[2,2]) #Number of survived people per family size, further divided per ticket class\nsns.catplot(x='Family_size',y='Survived',data=train,hue='Sex',kind='bar',ci=None,ax=axes[3,0]) #Number of survived people per family size, further divided per sex\nsns.catplot(x='Fare_class',y='Survived',data=train,hue='Pclass',kind='bar',ci=None,ax=axes[3,1]) #Number of survived people per fare class, further divided per ticket class\nsns.catplot(x='Fare_class',y='Survived',data=train,hue='Sex',kind='bar',ci=None,ax=axes[3,2]) #Number of survived people per fare class, further divided per sex\nfor abc in range(2,14,1):\n    plt.close(abc)","dff269cc":"# Converting sex and embarked to dummy variables\nsex_type = {\"male\":0,\"female\":1}\ntrain[\"Sex\"] = train[\"Sex\"].map(sex_type)\ntest[\"Sex\"] = test[\"Sex\"].map(sex_type)\nembarked_loc = {\"S\":0,\"C\":1,\"Q\":2}\ntrain[\"Embarked\"] = train[\"Embarked\"].map(embarked_loc)\ntest[\"Embarked\"] = test[\"Embarked\"].map(embarked_loc)","203f16e4":"# Creating input train and test structures\nX_train = train.drop([\"Survived\",\"Fare\",\"Cabin\",\"Age\",\"PassengerId\",\"Name\",\"Ticket\"], axis=1)\nY_train = train[\"Survived\"]\nX_test = test.drop([\"PassengerId\",\"Name\",\"Age\",\"Ticket\",\"Fare\",\"Cabin\"],axis=1)","78d4f95d":"# Random forest algorigthm with 100 estimations\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)","2576ee39":"# Can age really improve the random forest classification?\nf, axes = plt.subplots(1, 2, figsize=(10, 5))\nsns.distplot(train[\"Age\"],hist=True,bins=20,ax=axes[0]) #approximately normal distribution, it can be used to fill the NaNs\naxes[0].set_title('Age distribution train set')\nax2 = sns.distplot(test[\"Age\"],hist=True,bins=20,ax=axes[1], color=\"r\") #approximately normal distribution, it can be used to fill the NaNs\naxes[1].set_title('Age distribution test set')","5411d5f5":"# Filling the Age NaNs by sampling from the two normal distributions\nmeanAge_train, stdAge_train = train[\"Age\"].mean(), train[\"Age\"].std() #mean and standard deviation for the column Age, train set\nnullAge_train = train[\"Age\"].isnull() #get the indeces where NaNs are present in the Age column\nsampAge_norm_dist_train = np.random.normal(meanAge_train, stdAge_train, nullAge_train.sum())\nfor ii in range(0,len(sampAge_norm_dist_train)): #checking for eventual age values below zero\n    if sampAge_norm_dist_train[ii] < 0:\n        sampAge_norm_dist_train[ii] = np.random.normal(meanAge_train, stdAge_train, 1)\ntrain[\"Age\"].loc[nullAge_train] = sampAge_norm_dist_train #fill in the NaNs with the samples values\n        \n\nmeanAge_test, stdAge_test = test[\"Age\"].mean(), test[\"Age\"].std() #same procedure as for the train set\nnullAge_test = test[\"Age\"].isnull()\nsampAge_norm_dist_test = np.random.normal(meanAge_test, stdAge_test, nullAge_test.sum())\nfor jj in range(0,len(sampAge_norm_dist_test)):\n    if sampAge_norm_dist_test[jj] < 0:\n        sampAge_norm_dist_test[jj] = np.random.normal(meanAge_test, stdAge_test, 1)\ntest[\"Age\"].loc[nullAge_test] = sampAge_norm_dist_test\n\n# Now both train and test set should have no NaNs in the Age column\nprint(train.isna().sum()) \nprint(test.isna().sum())","97d587eb":"# Plotting Age distribution after NaNs filling\nf, axes = plt.subplots(1, 2, figsize=(10, 5))\nsns.distplot(train[\"Age\"],hist=True,bins=20,ax=axes[0]) #approximately normal distribution, it can be used to fill the NaNs\naxes[0].set_title('Age distribution train set after NaNs filling')\nax2 = sns.distplot(test[\"Age\"],hist=True,bins=20,ax=axes[1], color=\"r\") #approximately normal distribution, it can be used to fill the NaNs\naxes[1].set_title('Age distribution test set after NaNs filling')","75f58da9":"# Binning Age into discrete intervals\nage_threshold = [0,18,35,50,65,train[\"Age\"].max()] #should make sense to divide into minors, young adults, adults, old adults, elderly\nage_label = [1,2,3,4,5] #new label for the age bins\ntrain[\"Age_discrete\"] = pd.cut(train[\"Age\"], age_threshold, labels=age_label, include_lowest = True) #new column that will be used in the classification\ntest[\"Age_discrete\"] = pd.cut(test[\"Age\"], age_threshold, labels=age_label, include_lowest = True) #same procedure for the test dataset","2ba3d369":"# Creating input train and test structures with Age\nX_train_w_age = train.drop([\"Survived\",\"Fare\",\"Cabin\",\"Age\",\"PassengerId\",\"Name\",\"Ticket\"], axis=1)\nX_test_w_age = test.drop([\"PassengerId\",\"Name\",\"Age\",\"Ticket\",\"Fare\",\"Cabin\"],axis=1)\n\n# Random forest algorigthm with 100 estimations, including Age as variable\nrandom_forest_w_age = RandomForestClassifier(n_estimators=100)\nrandom_forest_w_age.fit(X_train_w_age, Y_train)\n\nY_prediction_w_age = random_forest_w_age.predict(X_test_w_age)\n\nrandom_forest_w_age.score(X_train_w_age, Y_train)","74116bbf":"# Calculating the importance of each variable in both models\nfeatures= X_train.columns\nimportances = random_forest.feature_importances_\nindices = np.argsort(importances)\nplt.figure(1)\nplt.title('Feature Importances without Age')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')","8050e85b":"# Same procedure as before, but including the random forest with the Age\nfeatures_w_age = X_train_w_age.columns\nimportances_w_age = random_forest_w_age.feature_importances_\nindices_w_age = np.argsort(importances_w_age)\nplt.figure(1)\nplt.title('Feature Importances with Age')\nplt.barh(range(len(indices_w_age)), importances_w_age[indices_w_age], color='b', align='center')\nplt.yticks(range(len(indices_w_age)), [features_w_age[i] for i in indices_w_age])\nplt.xlabel('Relative Importance')","61916bd8":"# Save result to CSV\nsubmission_titanic = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':Y_prediction_w_age})\nsubmission_titanic.to_csv('Titanic_submission_1.csv',index=False)\n\nsubmission_titanic_without_age = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':Y_prediction})\nsubmission_titanic_without_age.to_csv('Titanic_submission_2.csv',index=False)","d186caae":"The score of the random forest on the train set is now higher: about 89%. We can now understand how important is Age in the new model by investigating the relative weigth of each variable in the two trained random forest classifiers.","4904460f":"Now with the last two commands we should see no missing values in the Age columns in the two datasets. We can plot again the two distributions to check if they are visually similar than before the NaNs filling:","18f4bf09":"The score of the random forest on the train set is: 85.63%. By now we haven't considered the Age as an important variable in the classification. Therefore, we could try to further explore the effect of Age in the classification. As Age data are missing from both datasets, we start by plotting the two Age distributions.","95514166":"Although the train random forest classifier considering the Age as an input variable gives a better training score, once I submitted both files I got a better classification using the test in the model without using Age. In particular, I obtained the following scores:\n1) Without age: 0.76555\n2) With age: 0.73205\n\nSo going back to the starting question: is Age really important? Did I use the wrong approach to fill the NaNs? Does it give better classifications when combined with other variables?\n\nThanks for reading through!\n\nI am happy to accept suggestions, comments, and questions.","9d810f91":"After we have filled the missing values in the two dataset, we could have a look at some basic statistic of each column. In particular, how many survived passengers are related to each variable.","b0c2fd08":"We then run a random forest classifier with 100 estimators and predict the survivals on the test set using the model we have trained:","d6f451ca":"Now both results can be saved and are ready for submission.","ab95ca9e":"The next step is to convert Sex and Embarked into dummy variables, as below:","bd6dd2a5":"We can finally prepare the input training and testing sets from the dataset we have cleaned and worked on:","e6d53f04":"We can now again prepare the inputs for the random forest classifier, as we have done above:","28d775f5":"Both have similar distributions, which can be assumed to be normal. Therefore we can try to fill the NaNs in the Age by random sampling from these distributions:","d3c50a27":"It can be seen how in both train and test sets cabin information is mostly missing. Age is also partially without entries. Moreover, Embarked in the train set and Fare in the test set have couples of NaNs.","ef92302c":"We are now ready to divide Age into discrete intervals. While searching for a logical division, I stumbled upon many suggestions. I then deciced to divide into minors (<18), young adults (<35), adults (<50), old adults (<65, is there even such a thing? :P), and elderly (>65).","f1a2ca68":"Furthermore, we could use seaborn to visualize some of the intervariable relationships and their link with the survival rate.","8696541f":"Hey there! After some years in research, I would like to change my career perspectives into data science\/analysis. For this reason I decided to start using kaggle. I guess almost everybody started with the Titanic challenge. As there is already a lot of accessible material to tackle this dataset, I decided to focus only on the use of a random forest classifier. In particular, I analyzed whether the Age variable is really important for the final prediction."}}