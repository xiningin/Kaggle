{"cell_type":{"5b71ca77":"code","71ade0e0":"code","1c2dbfd8":"code","82d2b4de":"code","0d7085a3":"code","1efdffb8":"code","e1e6a4d5":"code","2cdc446c":"code","f516e7f3":"code","510f270c":"code","6311ce5a":"code","f7488c7c":"code","37869e8d":"code","358b1d1a":"code","46239a50":"code","3c0836fc":"code","86d0fac1":"code","0ac0959d":"code","db91cc31":"code","38f6ab0f":"code","678ea548":"code","56fe028c":"code","3de889a8":"code","9491762f":"code","79ddb492":"code","16be280e":"code","c4eefd74":"code","a7f50c58":"code","2e2aa603":"code","9097182d":"code","26e24f2b":"code","f67fec7c":"code","eb7393e4":"code","07fa12d5":"code","fe5a8a04":"code","52cea299":"code","3c9a2166":"code","e497862a":"code","320d0ae8":"code","7286334d":"code","55a033f2":"code","0fed0726":"code","6b6a971e":"code","dadc555b":"code","ee00c15a":"code","6ab22005":"code","287d971a":"code","82e71799":"code","082e0b5f":"code","4375f573":"code","8a0b8d56":"code","b3040a31":"code","2b1524d5":"code","576beea1":"code","ca77ba67":"code","8555ebb0":"markdown","14555471":"markdown","88f28bf4":"markdown","203847d3":"markdown","7b04753d":"markdown","bb72dfe8":"markdown","ce1fc3bc":"markdown","92168eb5":"markdown","3d0613fe":"markdown","d50e2a5d":"markdown","6dc88339":"markdown"},"source":{"5b71ca77":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import randrange, uniform\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","71ade0e0":"df = pd.read_csv(\"..\/input\/Day - Dataset.csv\",index_col = 0)","1c2dbfd8":"df.head(10)","82d2b4de":"df.columns","0d7085a3":"df.dtypes","1efdffb8":"#Converting variables datatype to required datatypes\n#Categorical variables\ndf['dteday'] = pd.to_datetime(df['dteday'],yearfirst = True)\ndf['season'] = df['season'].astype('category')\ndf['yr']     = df['yr'].astype('category')\ndf['mnth']   = df['mnth'].astype('category')\ndf['holiday']= df['holiday'].astype('category')\ndf['weekday']= df['weekday'].astype('category')\ndf['workingday']= df['workingday'].astype('category')\ndf['weathersit']= df['weathersit'].astype('category')\n\n#Continuous variables\ndf['temp'] = df['temp'].astype('float')\ndf['atemp']= df['atemp'].astype('float')\ndf['hum']  = df['hum'].astype('float')\ndf['windspeed'] = df['windspeed'].astype('float')\ndf['casual'] = df['casual'].astype('float')\ndf['registered'] = df['registered'].astype('float')\ndf['cnt'] = df['cnt'].astype('float')","e1e6a4d5":"df.dtypes","2cdc446c":"ordered_data = df.copy()","f516e7f3":"missing_val = pd.DataFrame(df.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val = missing_val.rename(columns={'index':'variable',0:'Missing_values'})","510f270c":"missing_val","6311ce5a":"#Craeting new variables from existing variables for visualizations (Future Engineering)\ndf['actual_temp'] = df['temp'] * 39\ndf['actual_atemp'] = df['atemp'] * 50\ndf['actual_windspeed'] = df['windspeed'] * 67\ndf['actual_hum'] = df['hum'] * 100","f7488c7c":"df.columns","37869e8d":"df.head()","358b1d1a":"#Cheking the Distribution of data by using Histograms\nsns.set_style(\"whitegrid\")\nsns.distplot(df['actual_temp'],rug=True)","46239a50":"sns.distplot(df['actual_atemp'], rug=True)","3c0836fc":"sns.distplot(df['actual_hum'], rug=True)","86d0fac1":"sns.distplot(df['actual_windspeed'],rug=True)","0ac0959d":"sns.distplot(df['cnt'],rug=True)","db91cc31":"continuous_variables = ['actual_temp','actual_atemp','actual_windspeed','actual_hum','cnt']\nfor i in continuous_variables:\n    plt.hist(df[i],bins='auto')\n    plt.title(\"Checking Distribution for Variable \"+str(i))\n    plt.ylabel(\"Density\")\n    plt.xlabel(i)\n    plt.show()","38f6ab0f":"#Bike Rentals Per Monthly\nmonthly_sales = df.groupby('mnth').size()\nprint(monthly_sales)\n#Plotting the Graph\nplot = monthly_sales.plot(title='Monthly Sales',xticks=(1,2,3,4,5,6,7,8,9,10,11,12))\nplot.set_xlabel('Months')\nplot.set_ylabel('Total Number of Bikes')","678ea548":"#Checking the distribution categorical Data using factorplot\nsns.set_style(\"whitegrid\")\nsns.factorplot(data=df, x='dteday', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='yr', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='mnth', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='season', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='weathersit', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='workingday', kind= 'count',size=4,aspect=2)\n","56fe028c":"#Scatter plot for temprature against bike rentals \nsns.scatterplot(data=df,x='actual_temp',y='cnt')\n","3de889a8":"#Scatter plot for humidity against bike rentals \nsns.scatterplot(data=df,x='actual_hum',y='cnt')\n","9491762f":"#Scatter plot for atemp(feeled_temparature) against bike rentals \nsns.scatterplot(data=df,x='actual_atemp',y='cnt')","79ddb492":"#Scatter plot for windspeed against bike rentals \nsns.scatterplot(data=df,x='actual_windspeed',y='cnt')","16be280e":"df.columns","c4eefd74":"#Checking Outliers in  data using boxplot\nsns.boxplot(data=df[['actual_temp','actual_atemp','actual_windspeed','actual_hum']])\nfig=plt.gcf()\nfig.set_size_inches(8,8)","a7f50c58":"sns.boxplot(data=df[['season','mnth','holiday','weekday']])\nfig=plt.gcf()\nfig.set_size_inches(8,8)","2e2aa603":"sns.boxplot(data=df[['workingday','weathersit','casual','registered']])\nfig=plt.gcf()\nfig.set_size_inches(8,8)","9097182d":"#Variables that are used to remove outliers\n#Not considering casual because this is not predictor variable\n#Not considering holiday because workingday variable includes holiday, so therte is no useful of considering holiday variables.\nout_names = ['actual_windspeed','actual_hum']","26e24f2b":"#Detecting and Removing Outliers\nfor i in out_names :\n    print (i)\n    q75,q25 = np.percentile(df.loc[:,i],[75,25])\n    iqr = q75-q25\n    \n    min = q25 - (iqr*1.5)\n    max = q75 + (iqr*1.5)\n    print (min)\n    print (max)\n    \n    df = df.drop(df[df.loc[:,i] < min].index)\n    df = df.drop(df[df.loc[:,i] > max].index)   ","f67fec7c":"#Checking Outliers in data after outliers removel using boxplot\nsns.boxplot(data=df[['actual_temp','actual_atemp','actual_windspeed','actual_hum']])\nfig=plt.gcf()\nfig.set_size_inches(8,8)","eb7393e4":"df.head()","07fa12d5":"continuous_variables = [ 'temp','atemp', 'hum', 'windspeed', 'casual',\n                        'registered', 'cnt', 'actual_temp', 'actual_atemp', 'actual_windspeed', 'actual_hum']","fe5a8a04":"#Future selection on the basis of Correlation, multcollinearity and variable importance\n#cnames = [\"actual_temp\",\"actual_atemp\",\"actual_hum\",\"acttual_windspeed\"]\n#cnames = [\"temp\",\"atemp\",\"hum\",\"windspeed\"]\n\ndf_cor = df.loc[:,continuous_variables]\nf, ax = plt.subplots(figsize=(10,10))\n\n#Generate correlation matrix\ncor_mat = df_cor.corr()\n\n#Plot using seaborn library\nsns.heatmap(cor_mat, mask=np.zeros_like(cor_mat, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax)\nplt.plot()","52cea299":"cat_columns = ['season', 'yr', 'mnth', 'holiday', 'weekday','workingday', 'weathersit']\n# making every combinationfrom cat_columns\nfactors_paired = [(i,j) for i in cat_columns for j in cat_columns]\nfactors_paired\np_values = [] \nfrom scipy.stats import chi2_contingency \nfor factor in factors_paired:\n    if factor[0] != factor[1]:\n        chi2, p, dof, ex = chi2_contingency(pd.crosstab(df[factor[0]], df[factor[1]]))\n        p_values.append(p.round(3))\n    else:\n        p_values.append('-') \np_values = np.array(p_values).reshape((7,7))\np_values = pd.DataFrame(p_values, index=cat_columns, columns=cat_columns)\nprint(p_values)","3c9a2166":"# checking vif of numerical column without dropping multicollinear column \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vf \nfrom statsmodels.tools.tools import add_constant\ncontinuous = add_constant(df[['temp', 'atemp', 'hum', 'windspeed']])\nvif = pd.Series([vf(continuous.values, i) for i in range(continuous.shape[1])], index = continuous.columns) \nprint(vif.round(1))\n\n# Checking VIF values of numeric columns after dropping column atemp \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vf \nfrom statsmodels.tools.tools import add_constant\ncontinuous = add_constant(df[['temp', 'hum', 'windspeed']]) \nvif = pd.Series([vf(continuous.values, i)  for i in range(continuous.shape[1])], index = continuous.columns) \nvif.round(1)","e497862a":"#Removing variables atemp beacuse it is highly correlated with temp,\n#Removing weekday,holiday because they don't contribute much to the independent cariable\n#Removing Causal and registered becuase that's what we need to predict.\ndf = df.drop(columns=['holiday','dteday','atemp','casual','registered','actual_temp','actual_atemp',\n                      'actual_windspeed','actual_hum'])","320d0ae8":"df.head(10)","7286334d":"df.columns","55a033f2":"df2 = df.copy()","0fed0726":"categorical_var = ['season', 'yr', 'mnth', 'weekday', 'workingday', 'weathersit']","6b6a971e":"df2.columns","dadc555b":"#Dummy Variable creation for categorical variables\ndf2 = pd.get_dummies(data = df2,columns=categorical_var)","ee00c15a":"df2.columns","6ab22005":"df2['count'] = df2['cnt'] \ndf2 =  df2.drop('cnt',axis=1)\ndf2.columns","287d971a":"df_plt_tree = df2.drop('count',axis=1) \ndf2.shape","82e71799":"#Import Libraries for decision tree\nfrom sklearn.tree import DecisionTreeRegressor,export_graphviz\nfrom sklearn.metrics import accuracy_score,r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree","082e0b5f":"#Splitting data into train and test data\ntrain,test = train_test_split(df2,test_size = 0.2, random_state = 123)","4375f573":"#Function for Performing all the tasks such as Error metrix rmse,mape,r-squared,accuracy,predictions\ndef evaluate(model, test_features, test_actual):\n    predictions = model.predict(test_features)\n    #Creating new data frame with comparing actual and predicted values\n    df_Dt = pd.DataFrame({'actual':test_actual,'predicted':predictions})\n    errors = abs(predictions - test_actual)\n    mape = 100 * np.mean(errors \/ test_actual)\n    accuracy = 100 - mape\n    rmse = np.sqrt(mean_squared_error(test_actual,predictions))\n    rsquared = r2_score(test_actual, predictions)\n    print('<---Model Performance--->')\n    print('R-Squared Value = {:0.2f}'.format(rsquared))\n    print('RMSE = {:0.2f}'.format(rmse))\n    print('MAPE = {:0.2f}'.format(mape))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    return","8a0b8d56":"#Decision Tree model development\n#Training the model with train data\nmodel = DecisionTreeRegressor(random_state = 123).fit(train.iloc[:,0:33],train.iloc[:,33])\n\n#Function for predictions, Error metrix rmse,mape,r-squared,accuracy\nevaluate(model, test.iloc[:,0:33], test.iloc[:,33])\n\ndotfile = open(\"pt.dot\",'w')\ndf = tree.export_graphviz(model,out_file=dotfile,feature_names = df_plt_tree.columns)","b3040a31":"#import libraries for Linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#Create model Linear Regression using LinearRegression\nmodel = LinearRegression().fit(train.iloc[:,0:33],train.iloc[:,33])\n\n#Function for predictions, Error metrix rmse,mape,r-squared,accuracy\nevaluate(model, test.iloc[:,0:33], test.iloc[:,33])","2b1524d5":"#Import the libraries for Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\n#Train the model\nRf_model = RandomForestRegressor(n_estimators=500,random_state=123).fit(train.iloc[:,0:33], train.iloc[:,33])\n\n#Function for predictions, Error metrix rmse,mape,r-squared,accuracy\nevaluate(Rf_model, test.iloc[:,0:33], test.iloc[:,33])","576beea1":"#Hyperparameter tuning using GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [12,14,16],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2,3],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [900,1000,1200]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 5, n_jobs = -1, verbose = 2)","ca77ba67":"grid_search.fit(test.iloc[:,0:33], test.iloc[:,33])\ngrid_search.best_params_\nbest_grid = grid_search.best_estimator_\n#Applying gridsearchcsv to test data\ngrid_accuracy = evaluate(best_grid,test.iloc[:,0:33],test.iloc[:,33])","8555ebb0":"**Exploratory Data Analysis**","14555471":"**Outlier Analysis**","88f28bf4":"**Hypothesis Testing**                                                                                                         \n**Null Hypothesis**                                                                                                           \n     Two variables are independant                                                                                             \n     \n**Alternate Hypothesis**                                                                                                       \n     Two variables are not independant**\n \n> If p-value is less than 0.05 then reject null hypothesis, that means two are variables are dependant(not independant)\n> but in our case most of the p-value are greater than 0.05,hence we need to accept taht we failed to reject null hypothesis \n \n\n","203847d3":"**Random forest**","7b04753d":"**Future Selection**","bb72dfe8":"****Distribution of Data by Visualizations**","ce1fc3bc":"**Model Development**","92168eb5":"**Hyperparameter Tunnig**","3d0613fe":"**Linear Regression**","d50e2a5d":"**Decision Tree**","6dc88339":"**Missing Value Analysis**"}}