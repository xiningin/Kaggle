{"cell_type":{"52bdca9f":"code","41b069b3":"code","b1f004d9":"code","3a0ec057":"code","5b32f3b3":"code","515b8231":"code","f41a84f9":"code","fb56f7df":"code","f33132fb":"code","8928fd62":"code","26e859e7":"code","a40f7906":"code","39c1620b":"code","41adefde":"code","a28961e3":"code","5c0b7d15":"code","6e2d7584":"code","8c6b012d":"code","8fabd0c2":"code","4b2575be":"code","dcf5ecaf":"code","7bc39c62":"code","5101e2e2":"code","4f56e565":"code","59187299":"code","e39729ec":"code","79b87949":"code","62f77c85":"code","6b8acf56":"code","28604abc":"markdown","8cd553e7":"markdown","cb69f1bc":"markdown","94f11306":"markdown","3df2e8c6":"markdown","2606ec8a":"markdown","b4116960":"markdown","ed59b640":"markdown","98e8e5df":"markdown","2ba14cec":"markdown","4870afd5":"markdown","5a5680ae":"markdown","7c1fd041":"markdown","dc64e1e7":"markdown","dd95b1f2":"markdown","d068d679":"markdown"},"source":{"52bdca9f":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n%matplotlib inline","41b069b3":"df = pd.read_csv('..\/input\/gene-expression\/actual.csv')","b1f004d9":"df_independent = pd.read_csv('..\/input\/gene-expression\/data_set_ALL_AML_independent.csv')","3a0ec057":"df_train = pd.read_csv('..\/input\/gene-expression\/data_set_ALL_AML_train.csv')","5b32f3b3":"df.head()","515b8231":"df_independent.head()","f41a84f9":"df_train.head()","fb56f7df":"df_train.shape","f33132fb":"df_independent.shape","8928fd62":"df_Train = df_train.drop(['Gene Description','Gene Accession Number'], axis=1)","26e859e7":"# we don't need call columns so let's remove them\n\ntrainreq = [col for col in df_Train.columns if \"call\" not in col]\ntrain = df_Train[trainreq]\ntrain.head()\n","a40f7906":"train.shape","39c1620b":"from sklearn.preprocessing import StandardScaler","41adefde":"scaler = StandardScaler()","a28961e3":"scaled_X = scaler.fit_transform(train)","5c0b7d15":"scaled_X","6e2d7584":"from sklearn.decomposition import PCA","8c6b012d":"model = PCA(n_components=2)","8fabd0c2":"principal_components = model.fit_transform(scaled_X)","4b2575be":"plt.figure(figsize=(8,6))\nplt.scatter(principal_components[:,0],principal_components[:,1])\nplt.xlabel('First principal component')\nplt.ylabel('Second Principal Component')","dcf5ecaf":"model.n_components","7bc39c62":"model.components_","5101e2e2":"df_comp = pd.DataFrame(model.components_,index=['PC1','PC2'],columns=train.columns)","4f56e565":"df_comp","59187299":"plt.figure(figsize=(20,3),dpi=150)\nsns.heatmap(df_comp,annot=True)","e39729ec":"model.explained_variance_ratio_","79b87949":"np.sum(model.explained_variance_ratio_)","62f77c85":"explained_variance = []\n\nfor n in range(1,38):\n    pca = PCA(n_components=n)\n    pca.fit(scaled_X)\n    \n    explained_variance.append(np.sum(pca.explained_variance_ratio_))","6b8acf56":"plt.plot(range(1,38),explained_variance)\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Variance Explained\");","28604abc":"**In this numpy matrix array, each row represents a principal component, Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\nWe can visualize this relationship with a heatmap:**","8cd553e7":"### what is principal componet?\n\n* **it is a linear combination of original features.**\n\n* **the more variance the original feature accounts for, the more influence it has over the principal components.**\n\n* **it the data sets with all the features we have 100% of the information but sometimes with less features with high variance we can have 90% of the information so we preffer to have less feature to solve the problem because the more feature doesn't make the significant difference in the pecentage of the information.**","cb69f1bc":"# how to choose n_component?","94f11306":"# Principal Component Analysis(PCA)\n\n* **let's use dimensionality reduction technique on features.**\n\n* **the purpose of this algorithm is to find the key features of the data set that have lots of features in its historical data and also it helps us for visualization and data analysis because it is hard in high dimensions(so many features).**\n\n### Dimensionality Reduction outcomes: \n\n* **Understand which features describe the most variance in the data set.**\n\n*  **helps us to understand large features data set, especially through visualization because visualization helps human to have a better understanding of data set to solve the problem.**\n\n* **Can also act as a simpler data set for training data for ML algorithms**\n\n* **Reduce dimensions then train ML algorithm on smaller data set.**\n\n* **Helps reduce N features to a desired smaller set of components through a transformation.**\n\n### variance Explained:\n\n*  **We've seen that ceratain features are more important or have more explanatory power that other features. for example, size of the house is much more important that the color of a house. this is what we are  talking about in Variance. Here variance means that how can we describe important features about the problem from the data set.(note that here we don't have label.)**\n\n* **for measuring this we have to find the features that has the most dispersion in the data set.(features with highest variance has the most information in it.)with this trick we can reduce the dimension and also have important features.**\n\n![pca_algorithm_3d.jpg](attachment:aa3de293-6e15-4a62-96f2-cc147bb79b55.jpg)\n","3df2e8c6":"### some important notes:\n\n* **algorithms like PCA do not simply choose a subset of the existing features.**\n\n* **they create new dimensional components that are combinations of proportions of the existing features.**","2606ec8a":"## Context\n\n**This dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).**\n\n**microarray: a grid of DNA segments of known sequence that is used to test and map DNA fragments, antibodies, or proteins.**\n\n**AML: A type of cancer of the blood and bone marrow with excess immature white blood cells.**\n\n**ALL: A type of cancer of the blood and bone marrow that affects white blood cells.Acute lymphoblastic leukemia is the most common childhood cancer.**\n\n## Content\n\n**Golub et al \"Molecular Classification of Cancer: Class Discovery and Class\nPrediction by Gene Expression Monitoring\"\nThere are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.**\n\n## Acknowledgements\n\n**These datasets have been converted to a comma separated value files (CSV).**\n\n## Inspiration\n\n**These datasets are great for classification problems. The original authors used the data to classify the type of cancer in each patient by their gene expressions.**","b4116960":"**n_component = 10 gives us more that 96% of information.**","ed59b640":"# import the data set","98e8e5df":"### how it works?\n\n* **Principal Component Analysis operates by creating a new dimensions(the principal components) that are normalized linear combinations of the original features.**\n\n#### PCA steps:\n\n1. Get original data\n\n1. Standardize the data : the average of each feature. then we have new origin of coordinates. \n\n1. Caculate covariance matrix for data.\n\n1. Apply Linear Transformation.\n\n1. Calculate EigenVectors by EigenValues.\n\n1. Choose N largest EigenVlaues.\n\n1. project original data onto EigenVectors.\n\n![image.png](attachment:b931ea1d-6853-48c7-b787-878ed7f9d5c1.png)","2ba14cec":"#  Calculating the Explained Variance by the Component","4870afd5":"**now we have two components instead of 38 features.so we can visualize it.**","5a5680ae":"# about the data set","7c1fd041":"# Component","dc64e1e7":"# Scaling the Data (Standardize the dataset)","dd95b1f2":"# import libraries","d068d679":"# PCA"}}