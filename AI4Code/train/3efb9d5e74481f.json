{"cell_type":{"1335143b":"code","68015bb2":"code","5fc01d6a":"code","2cc97bbc":"code","e5961585":"code","da5d24ce":"code","0f65070e":"code","716bff48":"code","9bea0200":"code","65443e2d":"code","9da87917":"code","130b4321":"code","e11381ff":"code","ee5be3fe":"code","1d5a1e68":"code","f2544198":"code","c3ce84c0":"code","1e6ca5f3":"code","b4e2b086":"code","f206d79a":"code","a2ccf689":"code","73ef4458":"code","6b2fb0b8":"code","bb88a9c9":"code","6c5a5e1c":"code","470b6b65":"code","c2f8bbef":"code","b73f50a3":"code","2b264a1f":"code","92918259":"code","ac470b6d":"code","0b069cc7":"code","3c2b584d":"code","7aabec03":"code","5356a454":"code","459bf33f":"code","848473f5":"code","333d2cfc":"code","77e0e28b":"code","de0b0e04":"code","3cc172a1":"code","1eed3e5f":"code","400f1933":"code","cfba74f8":"code","072ef49d":"code","86d568f0":"code","c42719fc":"code","826ec41d":"code","914cadf9":"code","279d4bc0":"code","d0037ee4":"code","e4149c45":"code","0617c6a3":"code","9970c2ca":"markdown","ed154064":"markdown","52f0e9c6":"markdown","6327b060":"markdown","4d46436f":"markdown","7918591a":"markdown","6ef08656":"markdown","317074b6":"markdown","6d47fe94":"markdown","43e5557e":"markdown","e022365a":"markdown","b3f9380c":"markdown","3bf7c82b":"markdown","94413bf0":"markdown","a6d88f9a":"markdown","c9237a66":"markdown","089736eb":"markdown","7ef13cd7":"markdown","3b1cbf7f":"markdown","cb6b8129":"markdown","20b280e8":"markdown","1d698833":"markdown"},"source":{"1335143b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68015bb2":"train_data = pd.read_csv(\"..\/input\/hackerearth-how-not-to-lose-a-customer-in-10-days\/train.csv\")\nx_test = pd.read_csv(\"..\/input\/hackerearth-how-not-to-lose-a-customer-in-10-days\/test.csv\")\nds_subm = pd.DataFrame(x_test['customer_id'])\nprint( \"Shape of train:\", train_data.shape)\ntrain_data = train_data.drop_duplicates()\nprint( \"Shape of train after droping duplicates:\", train_data.shape)\nprint(\"Shape of test data:\", x_test.shape)\n\n#printing the data\ntrain_data.head()","5fc01d6a":"x_test.head()","2cc97bbc":"train_data['churn_risk_score'].hist()\n## We are good to go. There are sufficient number of rows for each type of values to learn from.","e5961585":"print(train_data.info())\nprint(\"\\n\")\nprint(x_test.info())","da5d24ce":"print(train_data.nunique())\nprint('\\n')\nprint(x_test.nunique())","0f65070e":"train_data.describe()","716bff48":"x_test.describe()","9bea0200":"def to_num(data,feature):\n    data[feature] = pd.to_numeric(data[feature],errors='coerce')","65443e2d":"to_num(train_data,'avg_frequency_login_days')\nto_num(x_test,'avg_frequency_login_days')","9da87917":"useless = ['Name','security_no','referral_id','customer_id']\ntrain_data = train_data.drop(useless,axis = 1)\nx_test = x_test.drop(useless,axis = 1)\ntrain_data.columns","130b4321":"x_test.columns","e11381ff":"train_data.hist(['age','days_since_last_login','avg_time_spent','avg_transaction_value'\n                 ,'avg_frequency_login_days','points_in_wallet'],figsize=(18,10),)\n\n# avg time spent  - more skewed","ee5be3fe":"x_test.hist(['age','days_since_last_login','avg_time_spent','avg_transaction_value'\n                 ,'avg_frequency_login_days','points_in_wallet'],figsize=(18,10),)","1d5a1e68":"def PlotBarCharts(inpData, colsToPlot):\n    \n    # Generating multiple subplots\n    fig, subPlot=plt.subplots(nrows=3, ncols=4, figsize=(28,15))\n    \n    subPlot = subPlot.ravel()\n\n    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])","f2544198":"# Calling the function\nPlotBarCharts(inpData=train_data, colsToPlot=['gender', 'region_category', 'membership_category',\n        'joined_through_referral', 'preferred_offer_types',\n       'medium_of_operation', 'internet_option' ,\n        'used_special_discount',\n       'offer_application_preference', 'past_complaint', 'complaint_status',\n       'feedback'])\n\n#by loooking at graph we can say that each value in each features has enough data for model to learn.","c3ce84c0":"#days_since_last_login\ntrain_data['days_since_last_login'][train_data['days_since_last_login']>0].sort_values(ascending=True)","1e6ca5f3":"x_test['days_since_last_login'][x_test['days_since_last_login']>0].sort_values(ascending=True)","b4e2b086":"train_data['days_since_last_login'][train_data['days_since_last_login']<0]=1\nx_test['days_since_last_login'][x_test['days_since_last_login']<0]=1","f206d79a":"#avg_time_spent\ntrain_data['avg_time_spent'][train_data['avg_time_spent']>0].sort_values(ascending=True)","a2ccf689":"train_data['avg_time_spent'][train_data['avg_time_spent']<3000].sort_values(ascending=False)","73ef4458":"x_test['avg_time_spent'][x_test['avg_time_spent']>0].sort_values(ascending=True)","6b2fb0b8":"train_data['avg_time_spent'][train_data['avg_time_spent']<0]=1.837399\ntrain_data['avg_time_spent'][train_data['avg_time_spent']>3000]=2899.660000\nx_test['avg_time_spent'][x_test['avg_time_spent']<0]=13.111290\nx_test['avg_time_spent'][x_test['avg_time_spent']>3000]=2830.522122\n\n","bb88a9c9":"train_data['avg_frequency_login_days'][train_data['avg_frequency_login_days']>0].sort_values(ascending=True)","6c5a5e1c":"train_data['avg_frequency_login_days'][train_data['avg_frequency_login_days']<60].sort_values(ascending=True)","470b6b65":"x_test['avg_frequency_login_days'][x_test['avg_frequency_login_days']>0].sort_values(ascending=True)","c2f8bbef":"train_data['avg_frequency_login_days'][train_data['avg_frequency_login_days']<0]=0.009208\ntrain_data['avg_frequency_login_days'][train_data['avg_frequency_login_days']>60]=56.538478\nx_test['avg_frequency_login_days'][x_test['avg_frequency_login_days']<0]=0.127616\n#x_test['avg_frequency_login_days'][x_test['avg_frequency_login_days']>60]=58.445839\n","b73f50a3":"train_data['points_in_wallet'][train_data['points_in_wallet']>0].sort_values(ascending=True)","2b264a1f":"x_test['points_in_wallet'][x_test['points_in_wallet']>0].sort_values(ascending=True)","92918259":"train_data['points_in_wallet'][train_data['points_in_wallet']<0]= 6.432208\nx_test['points_in_wallet'][x_test['points_in_wallet']<0]= 6.034666","ac470b6d":"#lets create a dataframe which shows us the missing value and percentage-\n#-missing values.\n\ndef missing_per(data):\n    #missing_data.columns = [\"Feature\",\"missing_num\"]\n    missing_data = data.isna().sum().reset_index()\n    missing_data.columns = [\"Feature\",\"Missing_Value\"]\n    missing_data['Missing_Percent'] = round(missing_data.Missing_Value \/ len(data) * 100,2) \n    return  missing_data[missing_data.Missing_Value > 0]","0b069cc7":"#train data\nto_be_imputed = missing_per(train_data)\nto_be_imputed","3c2b584d":"#test data\nto_be_imputed = missing_per(x_test)\nto_be_imputed","7aabec03":"#handling the missing values in categorical features\ndef impute_cat(data,feature):\n    #data[feature]= np.where(data[feature].isnull(),\"Unknown\",data[feature])\n    data[feature] = data[feature].replace([np.nan,'?'],'Unknown')","5356a454":"for features in ['region_category','preferred_offer_types','joined_through_referral','medium_of_operation']:\n    impute_cat(train_data,features)\n    impute_cat(x_test,features)","459bf33f":"#handling the missing values in numerical features\ndef impute_num(data,feature):\n    data[feature] = data[feature].fillna(data[feature].mean())","848473f5":"for features in ['points_in_wallet','avg_frequency_login_days']:\n    impute_num(train_data,features)\n    impute_num(x_test,features)","333d2cfc":"#dropping rows with -1 as value i churn risk score\ntrain_data['churn_risk_score'] = train_data['churn_risk_score'].replace(-1,np.nan)\ntrain_data = train_data.dropna()","77e0e28b":"x_test.isna().sum()","de0b0e04":"#correlation to select features:\n#threshold > 0.2 in magnitude\ntrain_data.corr()","3cc172a1":"train_data.corr()","1eed3e5f":"def FunctionAnova(inpData, TargetVariable, CategoricalPredictorList):\n    from scipy.stats import f_oneway\n\n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n    \n    print('##### ANOVA Results ##### \\n')\n    for predictor in CategoricalPredictorList:\n        CategoryGroupLists=inpData.groupby(predictor)[TargetVariable].apply(list)\n        AnovaResults = f_oneway(*CategoryGroupLists)\n        \n        # If the ANOVA P-Value is <0.05, that means we reject H0\n        if (AnovaResults[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n            SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n    \n    return(SelectedPredictors)","400f1933":"CategoricalPredictorList=['gender', 'region_category', 'membership_category',\n        'joined_through_referral', 'preferred_offer_types',\n       'medium_of_operation', 'internet_option' ,\n        'used_special_discount',\n       'offer_application_preference', 'past_complaint', 'complaint_status',\n       'feedback']\nFunctionAnova(inpData=train_data, \n              TargetVariable='churn_risk_score', \n              CategoricalPredictorList=CategoricalPredictorList)","cfba74f8":"Selected_columns = ['avg_transaction_value','avg_frequency_login_days','points_in_wallet','region_category','membership_category',\n                    'joined_through_referral','preferred_offer_types','medium_of_operation',\n                    'used_special_discount','offer_application_preference','past_complaint','feedback','churn_risk_score']\nfor_test = ['avg_transaction_value','avg_frequency_login_days','points_in_wallet','region_category','membership_category',\n                    'joined_through_referral','preferred_offer_types','medium_of_operation',\n                    'used_special_discount','offer_application_preference','past_complaint','feedback']\ndataforml = train_data[Selected_columns]\ndataforml['churn_risk_score'] = dataforml['churn_risk_score'].astype('int')\nX_test = x_test[for_test]\nX_test.columns","072ef49d":"#define x_train and y_train\nx_train = dataforml.iloc[:,:12]   \ny_train = dataforml.iloc[:,-1:]              ","86d568f0":"!pip install feature-engine\nfrom feature_engine.encoding import CountFrequencyEncoder\n\nencoder = CountFrequencyEncoder(encoding_method='count',variables=None)\nX_train = encoder.fit_transform(x_train)\nX_train.head()","c42719fc":"X_test = encoder.transform(X_test)\nX_test","826ec41d":"#score = 100 x metrics.f1_score(actual, predicted, average=\"macro\")\nfrom sklearn import tree , metrics , ensemble , linear_model\nfrom sklearn.metrics import f1_score\n#ds = ensemble.RandomForestClassifier(max_depth=9)      #9,800\nds = tree.DecisionTreeClassifier(max_depth=9)\nds.fit(X_train,y_train)\n","914cadf9":"train_pred = ds.predict(X_train)","279d4bc0":"score =  100 * metrics.f1_score(y_train, train_pred, average=\"macro\")\nscore","d0037ee4":"test_pred = ds.predict(X_test)","e4149c45":"ds_subm.index = X_test.index\nds_subm['churn_risk_score'] = test_pred\nds_subm.to_csv('dectre_sub_dep9.csv',index = False)","0617c6a3":"ds_subm.head()","9970c2ca":"### Visualization:","ed154064":"## Model:","52f0e9c6":"Points In Wallet:","6327b060":"### Imputing the values:","4d46436f":"### Looking at the distribution of Target variable:\n","7918591a":"#### Imputing numerical features for both train and test:","6ef08656":"## Outlier treatment: \n\n**Replacing outliers: Have to treat them one by one.**\n","317074b6":"### Getting percentage value of missing values:","6d47fe94":"# Encoding:","43e5557e":"# so far max_depth 9 gives best result that is 76.31","e022365a":"## Feature Selection:","b3f9380c":"### Selected Features:","3bf7c82b":"# Missing Values:\n\n### Look out for missing values and treat them.","94413bf0":"Days Since Last Login:","a6d88f9a":"### Removing useless columns from the data:\n","c9237a66":"Avg Time Spent:","089736eb":"### Reading data:","7ef13cd7":"Avg Freq Login Days:","3b1cbf7f":"### Basic Data Exploration:","cb6b8129":"#### Imputing categorical features for both train and test:","20b280e8":"## Feature Engineering:","1d698833":"# Separation of data:"}}