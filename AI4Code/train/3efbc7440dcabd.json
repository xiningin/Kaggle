{"cell_type":{"a3fa343c":"code","d965169e":"code","16e18b50":"code","3903ce58":"code","a4c376a2":"code","b326d244":"code","7e913d8e":"code","b0c9dc3f":"code","130ccdad":"code","226e8f72":"code","7d3fdb29":"code","44ad2c9e":"code","5c39b2cb":"code","360e1dc8":"code","3f696571":"code","e1cc8513":"code","7419f656":"code","dbed17f6":"code","243be605":"code","ea1c1c6e":"code","e6cc5baa":"code","7bee54da":"code","f0ed15d1":"code","3680e662":"code","422b0ca2":"markdown","b6c9d6c4":"markdown","3d4335f3":"markdown","d4af4ae7":"markdown","45c3e39b":"markdown","9d2a63a4":"markdown","dd92e0cf":"markdown","e93fac04":"markdown","692f969c":"markdown","1dbaa931":"markdown","b353b2fc":"markdown","07398056":"markdown","6788f862":"markdown","15daad12":"markdown","11424b66":"markdown","4dd22b28":"markdown","7c8f0626":"markdown","b17a1f02":"markdown","41da3c83":"markdown","c8f191ce":"markdown","b0e72f1d":"markdown","31e424a1":"markdown","e461782e":"markdown","6b794153":"markdown","a7b12194":"markdown","5a26de7e":"markdown","9d1be9db":"markdown"},"source":{"a3fa343c":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom pathlib import Path\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\n# Set matplotlib default style\nmpl.style.use('seaborn-darkgrid')","d965169e":"df_train = pd.read_csv('..\/input\/facial-keypoints-detection\/training.zip')\ndf_train.head(1)","16e18b50":"df_train.info()","3903ce58":"df_train.describe()","a4c376a2":"feature_col, target_cols = 'Image', list(df_train.drop('Image', axis=1).columns)","b326d244":"df_train[target_cols] = df_train[target_cols].fillna(df_train[target_cols].mean())\ndf_train.info()","7e913d8e":"# Image characteristics\nIMG_WIDTH  = 96\nIMG_HEIGHT = 96\nIMG_CHANNELS = 1\n\n# Split the `Image` column around delimiter `space` and \n# create a numpy array with `dtype=int`, finally reshape\n# it according the defined height width, and channels\nimages = np.array(df_train[feature_col].str.split().tolist(), dtype='float').reshape(-1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\nlabels = df_train[target_cols].to_numpy()","b0c9dc3f":"normalized_images = images \/ 255.","130ccdad":"def show_examples(images, landmarks):\n    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(16, 16))\n    \n    for img, marks, ax in zip(images, landmarks, axes.ravel()):\n        # Keypoints\n        x_points = marks[:: 2]\n        y_points = marks[1::2]\n        \n        ax.imshow(img.squeeze(), cmap='gray')\n        ax.scatter(x_points, y_points, s=10, color='red')\n    \n    plt.show()\n    \n\nidx = np.random.choice(16, 16)\nshow_examples(images[idx], labels[idx])","226e8f72":"train_images, valid_images, train_labels, valid_labels = train_test_split(normalized_images, labels, test_size=0.1, random_state=7)","7d3fdb29":"from tensorflow.keras.layers import (\n    Input,\n    Conv2D, \n    MaxPool2D, \n    Dense, \n    BatchNormalization, \n    ReLU, \n    Dropout, \n    Flatten,\n    Dropout,\n    Concatenate,\n    GlobalAvgPool2D\n)\n\nfrom tensorflow.keras.regularizers import L2\n\ndef inception_module(inputs, f1, f2):\n    x1 = Conv2D(f1, 3, padding='same')(inputs)\n    x1 = BatchNormalization()(x1)\n    x1 = ReLU()(x1)\n    \n    x2 = Conv2D(f2, 5, padding='same')(inputs)\n    x2 = BatchNormalization()(x2)\n    x2 = ReLU()(x2)\n    \n    return Concatenate()([x1, x2])","44ad2c9e":"def build_model():\n    inputs = Input((96, 96, 1))\n\n    x = inception_module(inputs, 64,  32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 64,  32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 128, 32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 128, 32)\n    x = MaxPool2D()(x)\n    \n    x = inception_module(x, 256, 64)\n    x = MaxPool2D()(x)\n    \n    x = Flatten()(x)\n    x = Dense(1024, kernel_regularizer=L2(l2=0.05))(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    x = Dense(512, kernel_regularizer=L2(l2=0.02))(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    x = Dense(128, kernel_regularizer=L2(l2=0.01))(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n    \n    x = Dense(30)(x)\n    \n    model = tf.keras.Model(inputs, outputs=x)\n    return model\n\nmodel = build_model()\nmodel.summary()","5c39b2cb":"tf.keras.utils.plot_model(model, show_shapes=True)","360e1dc8":"model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])","3f696571":"ckp_filepath = 'trained_models\/model'\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=ckp_filepath, \n                                                      monitor='val_mae', \n                                                      mode='auto',\n                                                      save_best_only=True, \n                                                      save_weights_only=True)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9, monitor='val_mae', \n                                                 mode='auto', cooldown=0, patience=5, verbose=1, min_lr=1e-5)","e1cc8513":"EPOCHS = 300\nBATCH_SIZE = 256\n\n# if Path(ckp_filepath).exists():\n#     model.load_weights(ckp_filepath)\n    \nhistory = model.fit(train_images, \n                    train_labels, \n                    validation_data=(valid_images, valid_labels), \n                    batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=[model_checkpoint, reduce_lr])","7419f656":"fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 12))\n\nlogs = history.history\n\nn_epochs = range(len(logs['loss']))\ntrain_loss, valid_loss = logs['loss'], logs['val_loss']\ntrain_mae, valid_mae = logs['mae'], logs['val_mae']\n\n\nax1.plot(n_epochs, train_loss, label='train loss', linestyle='solid')\nax1.plot(n_epochs, valid_loss, label='validation loss', linestyle='dashed')\nax1.set(xlabel='Number of iterations', ylabel='Mean squared error', title='Training and Validation loss vs Number of epochs', yscale='log')\n_ = ax1.legend()\n\nax2.plot(n_epochs, train_mae, label='train MAE', linestyle='solid')\nax2.plot(n_epochs, valid_mae, label='validation MAE', linestyle='dashed')\nax2.set(xlabel='Number of iterations', ylabel='Mean absolute error', title='Training and Validation MAE vs Number of epochs', yscale='log')\n_ = ax2.legend()","dbed17f6":"df_test = pd.read_csv('..\/input\/facial-keypoints-detection\/test.zip')\ndf_test.head(1)","243be605":"test_images = np.array(df_test['Image'].str.split().tolist(), dtype='int').reshape(-1, 96, 96, 1)","ea1c1c6e":"normalized_test_images = test_images \/ 255.","e6cc5baa":"model.load_weights(ckp_filepath) # load the best weights\nkeypoints_predictions = model.predict(normalized_test_images, batch_size=BATCH_SIZE)","7bee54da":"idx = np.random.choice(16, 16)\nshow_examples(test_images[idx], keypoints_predictions[idx])","f0ed15d1":"lookup = pd.read_csv('..\/input\/facial-keypoints-detection\/IdLookupTable.csv')\nlookup.head()","3680e662":"df_test_predictions = pd.DataFrame(keypoints_predictions, columns=target_cols)\ndf_test_predictions.index += 1\nlookup['Location'] = lookup.set_index(['ImageId', 'FeatureName']).index.map(df_test_predictions.stack()).values\n\n\ntimestamp = pd.to_datetime('today').floor('15min')\nlookup[['RowId', 'Location']].to_csv(f'predictions_{timestamp}.csv', index=False)","422b0ca2":"### Visualize the sample images","b6c9d6c4":"### Training data statistics","3d4335f3":"## Imports","d4af4ae7":"#### Descriptive statistics of training data","45c3e39b":"## Data preprocessing","9d2a63a4":"## Visualize training statistics","dd92e0cf":"## Train the model","e93fac04":"## Load the training dataset","692f969c":"### Predict the keypoints for test images","1dbaa931":"## Visualize the model","b353b2fc":"### Normalize Images","07398056":"## Submission","6788f862":"### Prepare test dataset","15daad12":"### Load test dataset","11424b66":"#### Concise summary of training data","4dd22b28":"##### As it can be seen from the above summary of `df_train` there are lots of columns which contains `NaN` values, we can `fill` these `NaN` values by `mean` of the respective columns.","7c8f0626":"## Build the model","b17a1f02":"### Normalize test images","41da3c83":"### Data cleaning","c8f191ce":"## Model evaluation","b0e72f1d":"## Model callbacks","31e424a1":"### Dataset preparation","e461782e":"## Facial keypoints detection using *CNN model* inspired by *Inception architecture*","6b794153":"### Prepare `train` and `dev` sets","a7b12194":"## Compile the model","5a26de7e":"### Visualize sample test predictions","9d1be9db":"#### Define `feature` and `target` columns"}}