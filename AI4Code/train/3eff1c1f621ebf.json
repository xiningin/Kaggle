{"cell_type":{"a586ac82":"code","b750ca1f":"code","be8a31ab":"code","af9d8b4b":"code","8c829a11":"code","f50cd133":"code","693966e2":"code","33e1cc74":"code","8f65070f":"code","213703cf":"code","a4d05562":"code","7a89590e":"code","4ff6be45":"code","648d9d6a":"code","2590e745":"code","11334b08":"code","473a71b8":"code","d6c75582":"code","b74d2ef9":"code","ff30e92e":"code","90b50ba0":"code","fc511f61":"code","1dd9d713":"code","5f3e2b8e":"code","b0327a70":"code","917f6296":"markdown","5f16939d":"markdown","a4e8e69c":"markdown","60b19ec2":"markdown","2152e93f":"markdown","f62c177a":"markdown","80113bb9":"markdown","dde65cc6":"markdown","db31a25c":"markdown","b287b12d":"markdown"},"source":{"a586ac82":"# let's install Feature-engine\n\n!pip install feature-engine","b750ca1f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.pipeline import Pipeline\n\n\n# import classes from Feature-engine\nfrom feature_engine.creation import MathematicalCombination, CombineWithReferenceFeature","be8a31ab":"# Load dataset\n\ndata = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n\nprint(data.shape)\n\ndata.head()","af9d8b4b":"# check how many wines of different qualities there are\n\n# percentage of wines of each quality\n(data['quality'].value_counts() \/ len(data)).sort_index().plot.bar()\n\n# plot\nplt.title('Wine Quality')\nplt.ylabel('Percentage of wines in the data')\nplt.xlabel('Wine Quality')\nplt.show()","8c829a11":"# let's transform the target into binary\n\n# wines with quality below 6 will be considered low quality (0)\ndata['quality'] = np.where(data['quality'] <= 6, 0, 1)\n\n(data['quality'].value_counts() \/ len(data)).plot.bar()\n\nplt.title('Wine Quality')\nplt.ylabel('Percentage of wines in the data')\nplt.xlabel('Wine Quality')\nplt.show()","f50cd133":"# let's explore variable distributions with histograms\n\ndata.hist(bins=50, figsize=(10,10))\n\nplt.show()","693966e2":"# let's evaluate the mean variable value per wine quality\n\ng = sns.PairGrid(data, x_vars=[\"quality\"], y_vars=data.columns[0:-1])\ng.map(sns.barplot)\nplt.show()","33e1cc74":"# now let's explore the data with boxplots\n\n# reorganise for plotting\ndf = data.melt(id_vars=['quality'])\n\n# capture variables\ncols = df.variable.unique()\n\n# plot first 6 columns\ng = sns.axisgrid.FacetGrid(df[df.variable.isin(cols[0:6])], col='variable', sharey=False)\ng.map(sns.boxplot, 'quality','value')\nplt.show()","8f65070f":"# plot remaining columns\ng = sns.axisgrid.FacetGrid(df[df.variable.isin(cols[6:])], col='variable', sharey=False)\ng.map(sns.boxplot, 'quality','value')\nplt.show()","213703cf":"data.head()","a4d05562":"# the citric acid affects the pH of the wine\n\nplt.scatter(data['citric acid'], data['pH'], c=data['quality'])\nplt.xlabel('Citric acid')\nplt.ylabel('pH')\nplt.show()","7a89590e":"# the sulphates may affect the pH of the wine\n\nplt.scatter(data['sulphates'], data['pH'], c=data['quality'])\nplt.xlabel('sulphates')\nplt.ylabel('pH')\nplt.show()","4ff6be45":"plt.scatter(data['sulphates'], data['citric acid'], c=data['quality'])\nplt.xlabel('sulphates')\nplt.ylabel('citric acid')\nplt.show()","648d9d6a":"# let's evaluate the relationship between some molecules and the density of the wine\n\ng = sns.PairGrid(data, y_vars=[\"density\"], x_vars=['chlorides','sulphates', 'residual sugar', 'alcohol'])\ng.map(sns.regplot)\nplt.show()","2590e745":"# combine fixed and volatile acidity to create total acidity\n# and mean acidity\n\ncombinator = MathematicalCombination(\n    variables_to_combine=['fixed acidity', 'volatile acidity'],\n    math_operations = ['sum', 'mean'],\n    new_variables_names = ['total_acidity', 'average_acidity']\n)\n\ndata = combinator.fit_transform(data)\n\n# note the new variables at the end of the dataframe\ndata.head()","11334b08":"# let's combine salts into total minerals and average minerals\n\ncombinator = MathematicalCombination(\n    variables_to_combine=['chlorides', 'sulphates'],\n    math_operations = ['sum', 'mean'],\n    new_variables_names = ['total_minerals', 'average_minerals']\n)\n\ndata = combinator.fit_transform(data)\n\n# note the new variable at the end of the dataframe\ndata.head()","473a71b8":"# let's determine the sulfur that is not free\n\ncombinator = CombineWithReferenceFeature(\n    variables_to_combine=['total sulfur dioxide'],\n    reference_variables=['free sulfur dioxide'],\n    operations=['sub'],\n    new_variables_names=['non_free_sulfur_dioxide']\n)\n\ndata = combinator.fit_transform(data)\n\n# note the new variable at the end of the dataframe\ndata.head()","d6c75582":"# let's calculate the % of free sulfur\n\ncombinator = CombineWithReferenceFeature(\n    variables_to_combine=['free sulfur dioxide'],\n    reference_variables=['total sulfur dioxide'],\n    operations=['div'],\n    new_variables_names=['percentage_free_sulfur']\n)\n\ndata = combinator.fit_transform(data)\n\n# note the new variable at the end of the dataframe\ndata.head()","b74d2ef9":"# let's determine from all free sulfur how much is as salt\n\ncombinator = CombineWithReferenceFeature(\n    variables_to_combine=['sulphates'],\n    reference_variables=['free sulfur dioxide'],\n    operations=['div'],\n    new_variables_names=['percentage_salt_sulfur']\n)\n\ndata = combinator.fit_transform(data)\n\n# note the new variable at the end of the dataframe\ndata.head()","ff30e92e":"# now let's explore the new variables with boxplots\n\nnew_vars = [\n    'total_acidity',\n    'average_acidity',\n    'total_minerals',\n    'average_minerals',\n    'non_free_sulfur_dioxide',\n    'percentage_free_sulfur',\n    'percentage_salt_sulfur']\n\n# reorganise for plotting\ndf = data[new_vars+['quality']].melt(id_vars=['quality'])\n\n# capture variables\ncols = df.variable.unique()\n\n# plot first 6 columns\ng = sns.axisgrid.FacetGrid(df[df.variable.isin(cols)], col='variable', sharey=False)\ng.map(sns.boxplot, 'quality','value')\nplt.show()","90b50ba0":"data = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\n\n# make binary target\ndata['quality'] = np.where(data['quality'] <= 6, 0, 1)\n\n# separate dataset into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    data.drop(labels=['quality'], axis=1),\n    data['quality'],\n    test_size=0.2,\n    random_state=0)\n\nX_train.shape, X_test.shape","fc511f61":"pipe = Pipeline([\n    # variable creation\n    ('acidity', MathematicalCombination(\n        variables_to_combine=['fixed acidity', 'volatile acidity'],\n        math_operations = ['sum', 'mean'],\n        new_variables_names = ['total_acidity', 'average_acidity']\n        )\n    ),\n    \n    ('total_minerals', MathematicalCombination(\n        variables_to_combine=['chlorides', 'sulphates'],\n        math_operations = ['sum', 'mean'],\n        new_variables_names = ['total_minerals', 'average_minearals'],\n        )\n    ),\n    \n    ('non_free_sulfur', CombineWithReferenceFeature(\n        variables_to_combine=['total sulfur dioxide'],\n        reference_variables=['free sulfur dioxide'],\n        operations=['sub'],\n        new_variables_names=['non_free_sulfur_dioxide'],\n        )\n    ),\n    \n    ('perc_free_sulfur', CombineWithReferenceFeature(\n        variables_to_combine=['free sulfur dioxide'],\n        reference_variables=['total sulfur dioxide'],\n        operations=['div'],\n        new_variables_names=['percentage_free_sulfur'],\n        )\n    ),\n    \n    ('perc_salt_sulfur', CombineWithReferenceFeature(\n        variables_to_combine=['sulphates'],\n        reference_variables=['free sulfur dioxide'],\n        operations=['div'],\n        new_variables_names=['percentage_salt_sulfur'],\n        )\n    ),\n    \n    # =====  the machine learning model ====\n    \n    ('gbm', GradientBoostingClassifier(n_estimators=10, max_depth=2, random_state=1)),\n])\n\n# create new variables, and then train gradient boosting machine\n# uses only the training dataset\n\npipe.fit(X_train, y_train)","1dd9d713":"# make predictions and determine model performance\n\n# the pipeline takes in the raw data, creates all the new features and then\n# makes the prediction with the model trained on the final subset of variables\n\n# obtain predictions and determine model performance\n\npred = pipe.predict_proba(X_train)\nprint('Train roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n\npred = pipe.predict_proba(X_test)\nprint('Test roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))","5f3e2b8e":"new_vars = ['total_acidity', 'average_acidity', 'total_minerals', 'average_minearals',\n           'non_free_sulfur_dioxide', 'percentage_free_sulfur','percentage_salt_sulfur']","b0327a70":"importance = pd.Series(pipe.named_steps['gbm'].feature_importances_)\nimportance.index = list(X_train.columns) + new_vars\n\nimportance.sort_values(ascending=False).plot.bar(figsize=(15,5))\nplt.ylabel('Feature importance')\nplt.show()","917f6296":"Hello everyone,\n\nIn this notebook, I will first explore the main characteristics of the variables, and then their relationships with each other. Next, I will create new variables by combining the existing ones with mathematical operations. Finally, I will train a model to predict wine quality on the final variable set.\n\nI will create the new features utilizing a new Python open-source library called [Feature-engine](https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html)\n\nFeature-engine classes preserve Scikit-learn functionality with the methods fit and transform to first learn the parameters from the data, and then transform the data utilizing those parameters.\n\nThe beauty of using Feature-engine is that we can accomodate all transformations within a Scikit-learn pipeline, so that we can in a few lines of code, create all the new variables and then train a model on the final dataset. And, when scoring the test set, we only need to feed the raw data to the pipeline to obtain the final predictions.\n\n## I hope you find this kernel useful and if you do, your **UPVOTES** will be highly appreciated.\n","5f16939d":"## Feature importance","a4e8e69c":"All variables are continuous.","60b19ec2":"Most wines are medium to low quality. Only a few of high quality (>6)","2152e93f":"## Create additional variables\n\nLet's combine variables into new ones to capture additional information.","f62c177a":"## Exploratory Data Analysis\n\nLet's have a look at the variables and their relationships.","80113bb9":"## Machine Learning Pipeline\n\nNow we are going to carry out all variable creation within a Scikit-learn Pipeline and add a classifier at the end.","dde65cc6":"Good quality wine tend to have more citric acid and more sulphate, thus similar pH.","db31a25c":"We see that some of the variables that we created are somewhat important for the prediction, like average_minerals, total_minerals, and total and average acidity.\n\nThat is all folks!\n\n\n## References and further reading\n\n- [Feature-engine](https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html), Python open-source library\n- [Python Feature Engineering Cookbook](https:\/\/www.packtpub.com\/data\/python-feature-engineering-cookbook)\n\n## Other Kaggle kernels featuring Feature-engine\n\n- [Feature selection for bank customer satisfaction prediction](https:\/\/www.kaggle.com\/solegalli\/feature-selection-with-feature-engine)\n- [Feature engineering and selection for house price prediction](https:\/\/www.kaggle.com\/solegalli\/predict-house-price-with-feature-engine)\n\n","b287b12d":"There doesn't seem to be a difference in pH between wines of low and high quality, but high quality wines tend to have more alcohol, for example.\n\nSimilarly, good quality wines tend to have more sulphates but less free and total sulfur, a molecule that is part of the sulphates.\n\nGood quality wines tend to have more citric acid, yet surprisingly, the pH in good quality wines is not lower. So the pH must be equilibrated through something else, for example the sulphates."}}