{"cell_type":{"403b5067":"code","d98b6b77":"code","a804a805":"code","89223254":"code","05059113":"code","a9fc6bbb":"code","1ff1c4e2":"code","bc5d5dfe":"code","7052e20b":"code","a83d6271":"code","2bfa4e35":"code","c1a008af":"code","addcdc52":"code","6dc00bd6":"code","49bb3714":"code","698c3e49":"code","9c1a1fca":"code","d4c56bd2":"code","b256563f":"code","c2370bfc":"code","e166d26a":"code","676c3ec6":"code","cd3c97a0":"markdown","b340bc96":"markdown","5b8df2c2":"markdown","e1185451":"markdown","bed9d447":"markdown","3a746730":"markdown","0bedf381":"markdown","bec88821":"markdown","4a96713e":"markdown","8883930d":"markdown","7c0331fd":"markdown","f0977ee6":"markdown","9f1e345c":"markdown","3caa9359":"markdown","5a6d97f8":"markdown","340ad949":"markdown","553cc29c":"markdown","cddec699":"markdown","0247fbb7":"markdown","f86141f6":"markdown","c6ca0786":"markdown","cd99d6a4":"markdown","a9d4557d":"markdown"},"source":{"403b5067":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndata = pd.read_csv('..\/input\/fifa-2018-match-statistics\/FIFA 2018 Statistics.csv')\ny = (data['Man of the Match'] == \"Yes\")  # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in data.columns if data[i].dtype in [np.int64]]\nX = data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=0, max_depth=5, min_samples_split=5).fit(train_X, train_y)","d98b6b77":"from sklearn import tree\nimport graphviz\n\ntree_graph = tree.export_graphviz(tree_model, out_file=None, feature_names=feature_names)\ngraphviz.Source(tree_graph)","a804a805":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature='Goal Scored')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Goal Scored')\nplt.show()","89223254":"feature_to_plot = 'Distance Covered (Kms)'\npdp_dist = pdp.pdp_isolate(model=tree_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\n\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","05059113":"# Build Random Forest model\nrf_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\npdp_dist = pdp.pdp_isolate(model=rf_model, dataset=val_X, model_features=feature_names, feature=feature_to_plot)\npdp.pdp_plot(pdp_dist, feature_to_plot)\nplt.show()","a9fc6bbb":"# Similar to previous PDP plot except we use pdp_interact instead of pdp_isolate and pdp_interact_plot instead of pdp_isolate_plot\nfeatures_to_plot = ['Goal Scored', 'Distance Covered (Kms)']\ninter1  =  pdp.pdp_interact(model=tree_model, dataset=val_X, model_features=feature_names, features=features_to_plot)\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","1ff1c4e2":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Environment Set-Up for feedback system.\nimport sys\nsys.path.append('..\/input\/ml-insights-tools')\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom ex3 import *\nprint(\"Setup Complete\")\n\n# Data manipulation code below here\ndata = pd.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', nrows=50000)\n\n# Remove data with extreme outlier coordinates or negative fares\ndata = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +\n                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +\n                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +\n                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +\n                  'fare_amount > 0'\n                  )\n\ny = data.fare_amount\n\nbase_features = ['pickup_longitude',\n                 'pickup_latitude',\n                 'dropoff_longitude',\n                 'dropoff_latitude']\n\nX = data[base_features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)\nprint(\"Data sample:\")\ndata.head()","bc5d5dfe":"data.describe()","7052e20b":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\nfeat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","a83d6271":"for feat_name in base_features:\n    pdp_dist =  pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()","2bfa4e35":"q_1.solution()","c1a008af":"# Add your code here\ninter2  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=['pickup_longitude', 'dropoff_longitude'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter2, feature_names=['pickup_longitude', 'dropoff_longitude'], plot_type='contour')\nplt.show()","addcdc52":"q_2.solution()","6dc00bd6":"savings_from_shorter_trip = 15\n\nq_3.check()","49bb3714":"q_3.hint()\nq_3.solution()","698c3e49":"# This is the PDP for pickup_longitude without the absolute difference features. Included here to help compare it to the new PDP you create\nfeat_name = 'pickup_longitude'\npdp_dist_original = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist_original, feat_name)\nplt.show()\n\n\n\n# create new features\ndata['abs_lon_change'] = abs(data.dropoff_longitude - data.pickup_longitude)\ndata['abs_lat_change'] = abs(data.dropoff_latitude - data.pickup_latitude)\n\nfeatures_2  = ['pickup_longitude',\n               'pickup_latitude',\n               'dropoff_longitude',\n               'dropoff_latitude',\n               'abs_lat_change',\n               'abs_lon_change']\n\nX = data[features_2]\nnew_train_X, new_val_X, new_train_y, new_val_y = train_test_split(X, y, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_train_X, new_train_y)\n\nfeat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=new_val_X, model_features=features_2, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()\n\nq_4.check()","9c1a1fca":"q_4.hint()\nq_4.solution()","d4c56bd2":"q_5.solution()","b256563f":"from numpy.random import rand\n\nn_samples = 20000\n\n# Create array holding predictive feature\nX1 = 4 * rand(n_samples) - 2\nX2 = 4 * rand(n_samples) - 2\n# Create y. you should have X1 and X2 in the expression for y\n#y = np.ones(n_samples)\ny = -2 * X1 * (X1<-1) + X1 - 2 * X1 * (X1>1) - X2\n\n# create dataframe because pdp_isolate expects a dataFrame as an argument\nmy_df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\npredictors_df = my_df.drop(['y'], axis=1)\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\n\n# visualize your results\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()\n\nq_6.check()","c2370bfc":"q_6.hint()\nq_6.solution()","e166d26a":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nn_samples = 20000\n\n# Create array holding predictive feature\nX1 = 4 * rand(n_samples) - 2\nX2 = 4 * rand(n_samples) - 2\n# Create y. you should have X in the expression for y\ny = X1 * X2\n\n# create dataframe because pdp_isolate expects a dataFrame as an argument\nmy_df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\npredictors_df = my_df.drop(['y'], axis=1)\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()\nperm = PermutationImportance(my_model).fit(predictors_df, my_df.y)\n\nq_7.check()\n\n# show the weights for the permutation importance you just calculated\neli5.show_weights(perm, feature_names = ['X1', 'X2'])","676c3ec6":"# Uncomment the following lines for the hint or solution\nq_7.hint()\nq_7.solution()","cd3c97a0":"## Question 4\nIn the PDP's you've seen so far, location features have primarily served as a proxy to capture distance traveled. In the permutation importance lessons, you added the features `abs_lon_change` and `abs_lat_change` as a more direct measure of distance.\n\nCreate these features again here. You only need to fill in the top two lines.  Then run the following cell.  \n\n**After you run it, identify the most important difference between this partial dependence plot and the one you got without absolute value features. The code to generate the PDP without absolute value features is at the top of this code cell.**\n\n---","b340bc96":"## Question 7\nCreate a dataset with 2 features and a target, such that the pdp of the first feature is flat, but its permutation importance is high.  We will use a RandomForest for the model.\n\n*Note: You only need to supply the lines that create the variables X1, X2 and y. The code to build the model and calculate insights is provided*.","5b8df2c2":"## Q6\nThe code cell below does the following:\n\n1. Creates two features, `X1` and `X2`, having random values in the range [-2, 2].\n2. Creates a target variable `y`, which is always 1.\n3. Trains a `RandomForestRegressor` model to predict `y` given `X1` and `X2`.\n4. Creates a PDP plot for `X1` and a scatter plot of `X1` vs. `y`.\n\nDo you have a prediction about what the PDP plot will look like? Run the cell to find out.\n\nModify the initialization of `y` so that our PDP plot has a positive slope in the range [-1,1], and a negative slope everywhere else. (Note: *you should only modify the creation of `y`, leaving `X1`, `X2`, and `my_model` unchanged.*)","e1185451":"## Question 3\nConsider a ride starting at longitude -73.92 and ending at longitude -74. Using the graph from the last question, estimate how much money the rider would have saved if they'd started the ride at longitude -73.98 instead?","bed9d447":"Uncomment the lines below to see a hint or the solution (including an explanation of the important differences between the plots).","3a746730":"For the sake of explanation, our first example uses a Decision Tree which you can see below. In practice, you'll use more sophistated models for real-world applications.","0bedf381":"Why does the partial dependence plot have this U-shape?\n\nDoes your explanation suggest what shape to expect in the partial dependence plots for the other features?\n\nCreate all other partial plots in a for-loop below (copying the appropriate lines from the code above).","bec88821":"Uncomment the lines below for a hint or solution","4a96713e":"## Q2\n\nNow you will run a 2D partial dependence plot.  As a reminder, here is the code from the tutorial.  \n\n```\ninter1  =  pdp.pdp_interact(model=my_model, dataset=val_X, model_features=feature_names, features=['Goal Scored', 'Distance Covered (Kms)'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['Goal Scored', 'Distance Covered (Kms)'], plot_type='contour')\nplt.show()\n```\n\nCreate a 2D plot for the features `pickup_longitude` and `dropoff_longitude`.  Plot it appropriately?\n\nWhat do you expect it to look like?","8883930d":"As guidance to read the tree:\n- Leaves with children show their splitting criterion on the top\n- The pair of values at the bottom show the count of True values and False values for the target respectively, of data points in that node of the tree.","7c0331fd":"This model thinks you are more likely to win *Player of The Game* if your players run a total of 100km over the course of the game. Though running much more causes lower predictions. In general, the smooth shape of this curve seems more plausible than the step function from the Decision Tree model.  Though this dataset is small enough that we would be careful in how we interpret any model.\n\n# 2D Partial Dependence Plots\nIf you are curious about interactions between features, 2D partial dependence plots are also useful. We will again use the Decision Tree model for this graph.  It will create an extremely simple plot, but you should be able to match what you see in the plot to the tree itself.\n","f0977ee6":"# Partial Dependence Plots\n\nWhile [feature importance](https:\/\/www.kaggle.com\/pratjain\/permutation-importance) shows what variables most affect predictions, partial dependence plots show **how** a feature affects predictions. This is useful to answer questions like:\n\n* How would similarly sized houses be priced in different areas?..\n* Are predicted health differences between two groups due to differences in their diets, or some other factor?..\n\nIf you are familiar with linear or logistic regression models, partial dependence plots can be interepreted similarly to the coefficients in those models.  Though, partial dependence plots on sophisticated models can capture more complex patterns than coefficients from simple models. \n\n# How it Works\n\nLike permutation importance, **partial dependence plots are calculated after a model has been fit.** \n\nIn our soccer example, teams may differ in many ways. How many passes they made, shots they took, goals they scored, etc. At first glance, it seems difficult to disentangle the effect of these features. To see how partial plots separate out the effect of each feature, we start by considering a single row of data. For example, that row of data might represent a team that had the ball 50% of the time, made 100 passes, took 10 shots and scored 1 goal.\n\nWe will use the fitted model to predict our outcome (probability their player won \"man of the game\"). But we **repeatedly alter the value for one variable** to make a series of predictions.  We could predict the outcome if the team had the ball only 40% of the time. We then predict with them having the ball 50% of the time.  Then predict again for 60%.  And so on.  We trace out predicted outcomes (on the vertical axis) as we move from small values of ball possession to large values (on the horizontal axis).\n\nIn this description, we used only a single row of data.  Interactions between features may cause the plot for a single row to be atypical.  So, we repeat that mental experiment with multiple rows from the original dataset, and we plot the average predicted outcome on the vertical axis.  \n\n# Code Example","9f1e345c":"A few items are worth pointing out as you interpret this plot\n- The y axis is interpreted as **change in the prediction** from what it would be predicted at the baseline or leftmost value.\n- A blue shaded area indicates level of confidence\n\nFrom this particular graph, we see that scoring a goal substantially increases your chances of winning \"Player of The Game.\"  But extra goals beyond that appear to have little impact on predictions.\n\nHere is another example plot:","3caa9359":"Here is the code to create the Partial Dependence Plot using the [PDPBox library](https:\/\/pdpbox.readthedocs.io\/en\/latest\/).","5a6d97f8":"This graph seems too simple to represent reality. But that's because the model is so simple. You should be able to see from the decision tree above that this is representing exactly the model's structure. You can easily compare the structure or implications of different models. Here is the same plot with a Random Forest model.","340ad949":"## Question 1\n\nHere is the code to plot the partial dependence plot for pickup_longitude.  Run the following cell.","553cc29c":"Uncomment the line below to see the solution and explanation for how one might reason about the plot shape.","cddec699":"For a solution or hint, uncomment the appropriate line below.","0247fbb7":"Do the shapes match your expectations for what shapes they would have? Can you explain the shape now that you've seen them? \n\nUncomment the following line to check your intuition.","f86141f6":"# Exercises\n\nWe will create partial dependence plots and practice building insights with data from the [Taxi Fare Prediction](https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction) competition.","c6ca0786":"Partial dependence plots can be really interesting. We have a [discussion thread](https:\/\/www.kaggle.com\/learn-forum\/65782) to talk about what real-world topics or questions you'd be curious to see addressed with partial dependence plots.","cd99d6a4":"## Question 5\nConsider a scenario where you have only 2 predictive features, which we will call `feat_A` and `feat_B`. Both features have minimum values of -1 and maximum values of 1.  The partial dependence plot for `feat_A` increases steeply over its whole range, whereas the partial dependence plot for feature B increases at a slower rate (less steeply) over its whole range. Does this guarantee that `feat_A` will have a higher permutation importance than `feat_B`.  Why or why not?\n\nAfter you've thought about it, uncomment the line below for the solution.","a9d4557d":"This graph shows predictions for any combination of Goals Scored and Distance covered. For example, we see the highest predictions when a team scores at least 1 goal and they run a total distance close to 100km.  If they score 0 goals, distance covered doesn't matter. Can you see this by tracing through the decision tree with 0 goals? But distance can impact predictions if they score goals. Make sure you can see this from the 2D partial dependence plot. Can you see this pattern in the decision tree too?"}}