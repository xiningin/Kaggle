{"cell_type":{"44609422":"code","1c6f7fa6":"code","cd692315":"code","05595b8d":"code","6a31fcd0":"code","55b6ec37":"code","034e52eb":"code","80e2c5cf":"code","d9390e36":"code","ffab6510":"code","9e3393e7":"code","87eb1161":"code","4553b89d":"code","ec889122":"code","2b4d7bd8":"code","c309f78b":"code","0f0d9cf4":"code","45e1d271":"code","2796beeb":"code","71916067":"code","849ad662":"code","b1a29891":"code","64aa5353":"code","4f51299a":"code","5760889a":"code","fc4a2e03":"code","7316aad6":"code","a401dbf8":"code","2c9608fd":"code","ce686859":"code","f0193bcf":"code","cf59b28d":"code","90a8a884":"code","1fdab99d":"code","5c4e5a92":"code","07fd6fee":"code","3ed16b69":"code","59875d50":"code","03066f54":"code","2802d451":"code","ef561ac3":"code","d543a75b":"code","a48e7c1a":"code","008caf93":"code","d15ef79e":"code","46ef51af":"markdown","c81a0a6a":"markdown","810b366e":"markdown","1166f41a":"markdown","be81fd35":"markdown","2e89e0cb":"markdown","e19d36b0":"markdown","3be88a05":"markdown"},"source":{"44609422":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c6f7fa6":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","cd692315":"import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nimport scipy.io\n\ndef sigmoid(x):\n    \"\"\"\n    Compute the sigmoid of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size.\n\n    Return:\n    s -- sigmoid(x)\n    \"\"\"\n    s = 1\/(1+np.exp(-x))\n    return s\n\ndef relu(x):\n    \"\"\"\n    Compute the relu of x\n\n    Arguments:\n    x -- A scalar or numpy array of any size.\n\n    Return:\n    s -- relu(x)\n    \"\"\"\n    s = np.maximum(0,x)\n    \n    return s\n\ndef load_planar_dataset(seed):\n    \n    np.random.seed(seed)\n    \n    m = 400 # number of examples\n    N = int(m\/2) # number of points per class\n    D = 2 # dimensionality\n    X = np.zeros((m,D)) # data matrix where each row is a single example\n    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n    a = 4 # maximum ray of the flower\n\n    for j in range(2):\n        ix = range(N*j,N*(j+1))\n        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n        Y[ix] = j\n        \n    X = X.T\n    Y = Y.T\n\n    return X, Y\n\ndef initialize_parameters(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    b1 -- bias vector of shape (layer_dims[l], 1)\n                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n                    bl -- bias vector of shape (1, layer_dims[l])\n                    \n    Tips:\n    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims) # number of layers in the network\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) \/ np.sqrt(layer_dims[l-1])\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\ndef forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation (and computes the loss) presented in Figure 2.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape ()\n                    b1 -- bias vector of shape ()\n                    W2 -- weight matrix of shape ()\n                    b2 -- bias vector of shape ()\n                    W3 -- weight matrix of shape ()\n                    b3 -- bias vector of shape ()\n    \n    Returns:\n    loss -- the loss function (vanilla logistic loss)\n    \"\"\"\n        \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache\n\ndef backward_propagation(X, Y, cache):\n    \"\"\"\n    Implement the backward propagation presented in figure 2.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n    cache -- cache output from forward_propagation()\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1.\/m * np.dot(dZ3, A2.T)\n    db3 = 1.\/m * np.sum(dZ3, axis=1, keepdims = True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1.\/m * np.dot(dZ2, A1.T)\n    db2 = 1.\/m * np.sum(dZ2, axis=1, keepdims = True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1.\/m * np.dot(dZ1, X.T)\n    db1 = 1.\/m * np.sum(dZ1, axis=1, keepdims = True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(i)] = Wi\n                    parameters['b' + str(i)] = bi\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(i)] = dWi\n                    grads['db' + str(i)] = dbi\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    \n    n = len(parameters) \/\/ 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for k in range(n):\n        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n        \n    return parameters\n\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  n-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0\/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    # print results\n\n    #print (\"predictions: \" + str(p[0,:]))\n    #print (\"true labels: \" + str(y[0,:]))\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p\n\ndef compute_cost(a3, Y):\n    \"\"\"\n    Implement the cost function\n    \n    Arguments:\n    a3 -- post-activation, output of forward propagation\n    Y -- \"true\" labels vector, same shape as a3\n    \n    Returns:\n    cost - value of the cost function\n    \"\"\"\n    m = Y.shape[1]\n    \n    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n    cost = 1.\/m * np.nansum(logprobs)\n    \n    return cost\n\ndef load_dataset():\n    train_dataset = h5py.File('datasets\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('datasets\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    train_set_x_orig = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n    test_set_x_orig = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n    \n    train_set_x = train_set_x_orig\/255\n    test_set_x = test_set_x_orig\/255\n\n    return train_set_x, train_set_y, test_set_x, test_set_y, classes\n\n\ndef predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3>0.5)\n    return predictions\n\ndef load_planar_dataset(randomness, seed):\n    \n    np.random.seed(seed)\n    \n    m = 50\n    N = int(m\/2) # number of points per class\n    D = 2 # dimensionality\n    X = np.zeros((m,D)) # data matrix where each row is a single example\n    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n    a = 2 # maximum ray of the flower\n\n    for j in range(2):\n        \n        ix = range(N*j,N*(j+1))\n        if j == 0:\n            t = np.linspace(j, 4*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta\n            r = 0.3*np.square(t) + np.random.randn(N)*randomness # radius\n        if j == 1:\n            t = np.linspace(j, 2*3.1415*(j+1),N) #+ np.random.randn(N)*randomness # theta\n            r = 0.2*np.square(t) + np.random.randn(N)*randomness # radius\n            \n        X[ix] = np.c_[r*np.cos(t), r*np.sin(t)]\n        Y[ix] = j\n        \n    X = X.T\n    Y = Y.T\n\n    return X, Y\n\ndef plot_decision_boundary(model, X, y):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole grid\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n    plt.show()\n    \ndef load_2D_dataset():\n    data = scipy.io.loadmat('datasets\/data.mat')\n    train_X = data['X'].T\n    train_Y = data['y'].T\n    test_X = data['Xval'].T\n    test_Y = data['yval'].T\n\n    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n    \n    return train_X, train_Y, test_X, test_Y\ndef compute_loss(a3, Y):\n    \n    \"\"\"\n    Implement the loss function\n    \n    Arguments:\n    a3 -- post-activation, output of forward propagation\n    Y -- \"true\" labels vector, same shape as a3\n    \n    Returns:\n    loss - value of the loss function\n    \"\"\"\n    \n    m = Y.shape[1]\n    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n    loss = 1.\/m * np.nansum(logprobs)\n    \n    return loss","05595b8d":"data=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","6a31fcd0":"data.head()","55b6ec37":"X=data.drop(columns=['target'],axis=1)\ny=data['target'].values","034e52eb":"X=np.array(X)","80e2c5cf":"sc=StandardScaler()","d9390e36":"X=sc.fit_transform(X)","ffab6510":"def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n    learning_rate -- learning rate for gradient descent \n    num_iterations -- number of iterations to run gradient descent\n    print_cost -- if True, print the cost every 1000 iterations\n    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n    \n    Returns:\n    parameters -- parameters learnt by the model\n    \"\"\"\n        \n    grads = {}\n    costs = [] # to keep track of the loss\n    m = X.shape[1] # number of examples\n    layers_dims = [X.shape[0], 10, 5, 1]\n    \n    # Initialize parameters dictionary.\n    if initialization == \"zeros\":\n        parameters = initialize_parameters_zeros(layers_dims)\n    elif initialization == \"random\":\n        parameters = initialize_parameters_random(layers_dims)\n    elif initialization == \"he\":\n        parameters = initialize_parameters_he(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        a3, cache = forward_propagation(X, parameters)\n        \n        # Loss\n        cost = compute_loss(a3, Y)\n\n        # Backward propagation.\n        grads = backward_propagation(X, Y, cache)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 1000 iterations\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n            costs.append(cost)\n            \n    # plot the loss\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","9e3393e7":"train_X,test_X,train_Y,test_Y=train_test_split( X, y, test_size=0.10, random_state=8)","87eb1161":"train_X = train_X.T\ntrain_Y = train_Y.reshape((1, train_Y.shape[0]))\ntest_X = test_X.T\ntest_Y = test_Y.reshape((1, test_Y.shape[0]))","4553b89d":"# GRADED FUNCTION: initialize_parameters_zeros \n\ndef initialize_parameters_zeros(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    parameters = {}\n    L = len(layers_dims)            # number of layers in the network\n    \n    for l in range(1, L):\n        ### START CODE HERE ### (\u2248 2 lines of code)\n        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        ### END CODE HERE ###\n    return parameters","ec889122":"parameters = initialize_parameters_zeros([13,10,1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","2b4d7bd8":"parameters = model(train_X, train_Y, initialization = \"zeros\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","c309f78b":"# GRADED FUNCTION: initialize_parameters_random\n\ndef initialize_parameters_random(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n    parameters = {}\n    L = len(layers_dims)            # integer representing the number of layers\n    \n    for l in range(1, L):\n        ### START CODE HERE ### (\u2248 2 lines of code)\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        ### END CODE HERE ###\n\n    return parameters\n","0f0d9cf4":"parameters = initialize_parameters_random([13, 10, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","45e1d271":"parameters = model(train_X, train_Y, initialization = \"random\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","2796beeb":"# GRADED FUNCTION: initialize_parameters_he\n\ndef initialize_parameters_he(layers_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layers_dims) - 1 # integer representing the number of layers\n     \n    for l in range(1, L + 1):\n        ### START CODE HERE ### (\u2248 2 lines of code)\n        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 \/ layers_dims[l - 1])\n        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n        ### END CODE HERE ###\n        \n    return parameters","71916067":"parameters = initialize_parameters_he([2, 4, 1])\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","849ad662":"parameters = model(train_X, train_Y, initialization = \"he\")\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","b1a29891":"# 2nd part\ndef model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    parameters = initialize_parameters_he(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob < 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        \n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n                                            # but this assignment will only explore one at a time\n        if lambd == 0 and keep_prob == 1:\n            grads = backward_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob < 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","64aa5353":"parameters = model(train_X, train_Y)\nprint (\"On the training set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","4f51299a":"# GRADED FUNCTION: compute_cost_with_regularization\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    \n    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n    \n    ### START CODE HERE ### (approx. 1 line)\n    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) \/ (2 * m)\n    ### END CO\n    ### END CODER HERE ###\n    \n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost","5760889a":"A3,c=forward_propagation(train_X,parameters)","fc4a2e03":"\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    \n    ### START CODE HERE ### (approx. 1 line)\n    dW3 = 1. \/ m * np.dot(dZ3, A2.T) + (lambd * W3) \/ m\n    ### END CODE HERE ###\n    db3 = 1. \/ m * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    ### START CODE HERE ### (approx. 1 line)\n    dW2 = 1. \/ m * np.dot(dZ2, A1.T) + (lambd * W2) \/ m\n    ### END CODE HERE ###\n    db2 = 1. \/ m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    ### START CODE HERE ### (approx. 1 line)\n    dW1 = 1. \/ m * np.dot(dZ1, X.T) + (lambd * W1) \/ m\n    ### END CODE HERE ###\n    db1 = 1. \/ m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3, \"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients","7316aad6":"grads = backward_propagation_with_regularization(train_X, train_Y, c, lambd = 0.7)\nprint (\"dW1 = \\n\"+ str(grads[\"dW1\"]))\nprint (\"dW2 = \\n\"+ str(grads[\"dW2\"]))\nprint (\"dW3 = \\n\"+ str(grads[\"dW3\"]))","a401dbf8":"parameters = model(train_X, train_Y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","2c9608fd":"# GRADED FUNCTION: forward_propagation_with_dropout\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob=0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. \n    D1 = np.random.rand(A1.shape[0], A1.shape[1])     # Step 1: initialize matrix D1 = np.random.rand(..., ...)\n    D1 = D1 < keep_prob                            # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)\n    A1 = A1 * D1                                      # Step 3: shut down some neurons of A1\n    A1 = A1 \/ keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    ### START CODE HERE ### (approx. 4 lines)\n    D2 = np.random.rand(A2.shape[0], A2.shape[1])     # Step 1: initialize matrix D2 = np.random.rand(..., ...)\n    D2 = D2 < keep_prob                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)                           \n    A2 = A2 * D2                                      # Step 3: shut down some neurons of A2\n    A2 = A2 \/ keep_prob                               # Step 4: scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache","ce686859":"A3, cache = forward_propagation_with_dropout(train_X, parameters, keep_prob = 0.7)\nprint (\"A3 = \" + str(A3))","f0193bcf":"# GRADED FUNCTION: backward_propagation_with_dropout\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1. \/ m * np.dot(dZ3, A2.T)\n    db3 = 1. \/ m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    ### START CODE HERE ### (\u2248 2 lines of code)\n    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation\n    dA2 = dA2 \/ keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1. \/ m * np.dot(dZ2, A1.T)\n    db2 = 1. \/ m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    ### START CODE HERE ### (\u2248 2 lines of code)\n    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation\n    dA1 = dA1 \/ keep_prob              # Step 2: Scale the value of neurons that haven't been shut down\n    ### END CODE HERE ###\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1. \/ m * np.dot(dZ1, X.T)\n    db1 = 1. \/ m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients","cf59b28d":"parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_X, train_Y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_X, test_Y, parameters)","90a8a884":"# we see that the accuracy on the train set and test set came closer and the accuracy on the test score ncreased making our model away from the overfittin model it was earlier.\n","1fdab99d":"# GRADED FUNCTION: update_parameters_with_gd\n\ndef update_parameters_with_gd(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using one step of gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters to be updated:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients to update each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(parameters) \/\/ 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n        ### END CODE HERE ###\n        \n    return parameters","5c4e5a92":"\nparameters = update_parameters_with_gd(parameters, grads, learning_rate=0.01)\nprint(\"W1 =\\n\" + str(parameters[\"W1\"]))\nprint(\"b1 =\\n\" + str(parameters[\"b1\"]))\nprint(\"W2 =\\n\" + str(parameters[\"W2\"]))\nprint(\"b2 =\\n\" + str(parameters[\"b2\"]))","07fd6fee":"# mini batch gradient\n# GRADED FUNCTION: random_mini_batches\n\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        ### START CODE HERE ### (approx. 2 lines)\n        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        ### START CODE HERE ### (approx. 2 lines)\n        end = m - mini_batch_size * math.floor(m \/ mini_batch_size)\n        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n        ### END CODE HERE ###\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","3ed16b69":"# momentum\n# GRADED FUNCTION: initialize_velocity\n\ndef initialize_velocity(parameters):\n    \"\"\"\n    Initializes the velocity as a python dictionary with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients\/parameters.\n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    \n    Returns:\n    v -- python dictionary containing the current velocity.\n                    v['dW' + str(l)] = velocity of dWl\n                    v['db' + str(l)] = velocity of dbl\n    \"\"\"\n    \n    L = len(parameters) \/\/ 2 # number of layers in the neural networks\n    v = {}\n    \n    # Initialize velocity\n    for l in range(L):\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\n        ### END CODE HERE ###\n        \n    return v\n","59875d50":"# GRADED FUNCTION: update_parameters_with_momentum\n\ndef update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    \"\"\"\n    Update parameters using Momentum\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- python dictionary containing the current velocity:\n                    v['dW' + str(l)] = ...\n                    v['db' + str(l)] = ...\n    beta -- the momentum hyperparameter, scalar\n    learning_rate -- the learning rate, scalar\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- python dictionary containing your updated velocities\n    \"\"\"\n\n    L = len(parameters) \/\/ 2 # number of layers in the neural networks\n    \n    # Momentum update for each parameter\n    for l in range(L):\n        \n        ### START CODE HERE ### (approx. 4 lines)\n        # compute velocities\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        # update parameters\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n        ### END CODE HERE ###\n        \n    return parameters, v","03066f54":"# GRADED FUNCTION: initialize_adam\n\ndef initialize_adam(parameters) :\n    \"\"\"\n    Initializes v and s as two python dictionaries with:\n                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n                - values: numpy arrays of zeros of the same shape as the corresponding gradients\/parameters.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters.\n                    parameters[\"W\" + str(l)] = Wl\n                    parameters[\"b\" + str(l)] = bl\n    \n    Returns: \n    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n                    v[\"dW\" + str(l)] = ...\n                    v[\"db\" + str(l)] = ...\n    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n                    s[\"dW\" + str(l)] = ...\n                    s[\"db\" + str(l)] = ...\n\n    \"\"\"\n    \n    L = len(parameters) \/\/ 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n\n        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n    ### END CODE HERE ###\n    \n    return v, s","2802d451":"# GRADED FUNCTION: update_parameters_with_adam\n\ndef update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n    \"\"\"\n    Update parameters using Adam\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(l)] = Wl\n                    parameters['b' + str(l)] = bl\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(l)] = dWl\n                    grads['db' + str(l)] = dbl\n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    learning_rate -- the learning rate, scalar.\n    beta1 -- Exponential decay hyperparameter for the first moment estimates \n    beta2 -- Exponential decay hyperparameter for the second moment estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    v -- Adam variable, moving average of the first gradient, python dictionary\n    s -- Adam variable, moving average of the squared gradient, python dictionary\n    \"\"\"\n    \n    L = len(parameters) \/\/ 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n        ### START CODE HERE ### (approx. 2 lines)\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n        ### END CODE HERE ###\n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        ### START CODE HERE ### (approx. 2 lines)\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] \/ (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] \/ (1 - np.power(beta1, t))\n        ### END CODE HERE ###\n\n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        ### START CODE HERE ### (approx. 2 lines)\n        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n        ### END CODE HERE ###\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n        ### START CODE HERE ### (approx. 2 lines)\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] \/ (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] \/ (1 - np.power(beta2, t))\n        ### END CODE HERE ###\n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n        ### START CODE HERE ### (approx. 2 lines)\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] \/ np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] \/ np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n        ### END CODE HERE ###\n\n    return parameters, v, s","ef561ac3":"def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,\n          beta1=0.9, beta2=0.999, epsilon=1e-8, num_epochs=10000, print_cost=True):\n    \"\"\"\n    3-layer neural network model which can be run in different optimizer modes.\n    \n    Arguments:\n    X -- input data, of shape (2, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (1, number of examples)\n    layers_dims -- python list, containing the size of each layer\n    learning_rate -- the learning rate, scalar.\n    mini_batch_size -- the size of a mini batch\n    beta -- Momentum hyperparameter\n    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n    epsilon -- hyperparameter preventing division by zero in Adam updates\n    num_epochs -- number of epochs\n    print_cost -- True to print the cost every 1000 epochs\n\n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0                            # initializing the counter required for Adam update\n    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n    \n    # Initialize parameters\n    parameters = initialize_parameters(layers_dims)\n\n    # Initialize the optimizer\n    if optimizer == \"gd\":\n        pass # no initialization required for gradient descent\n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n    \n    # Optimization loop\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n\n        for minibatch in minibatches:\n\n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n\n            # Forward propagation\n            a3, caches = forward_propagation(minibatch_X, parameters)\n\n            # Compute cost\n            cost = compute_cost(a3, minibatch_Y)\n\n            # Backward propagation\n            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n\n            # Update parameters\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1 # Adam counter\n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n                                                               t, learning_rate, beta1, beta2,  epsilon)\n        \n        # Print the cost every 1000 epoch\n        if print_cost and i % 1000 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n                \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters","d543a75b":"# train 3-layer model\nimport math\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"gd\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n","a48e7c1a":"# Using momentum optimization method\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = \"momentum\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n\n","008caf93":"#mini batch using the adam method\n# train 3-layer model\nlayers_dims = [train_X.shape[0], 5, 2, 1]\nparameters = model(train_X, train_Y, layers_dims, optimizer = \"adam\")\n\n# Predict\npredictions = predict(train_X, train_Y, parameters)\n","d15ef79e":"# as we can see that the adam optimizer gave out an accuracy of about 98 percent which is pretty good right now\n","46ef51af":"Optimization","c81a0a6a":"L2 regularization","810b366e":"Random Initialization","1166f41a":"**Train accuracy of the different initializations**\n\n**Zero initialization:**\nAccuracy: 0.5330882352941176\n\n**Random Initialization :**\nAccuracy: 0.7536764705882353\n\n**He\/Xavier Initialization:**\nAccuracy: 0.9779411764705882\n","be81fd35":"He initialization","2e89e0cb":"Zero initialization","e19d36b0":"Using Dropout Function","3be88a05":"From this we can see that the adam optimizer was the best as it gave an accuracy of about 97 percent but mmetum would be great too but in this situation it wasn't because of small and simple dataset and low learning rate ...."}}