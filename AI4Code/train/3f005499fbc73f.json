{"cell_type":{"70b0dfb5":"code","c1f4f718":"code","952c7f39":"code","134fc82d":"code","ca86ddfd":"code","a4644cd8":"code","53ce1734":"code","e0b55472":"code","5ddefea9":"code","e4c18875":"code","0c34e423":"code","6dbf7bf8":"code","d26094a6":"code","d1e757be":"code","0359981b":"code","841c13f4":"code","d9945068":"code","03cdaeab":"code","d8a77e69":"code","51f56e1b":"code","5fd32a49":"code","11691de4":"code","ff3e44fc":"code","35f2910c":"code","8315eb28":"code","42ca561f":"code","be47213e":"code","a0c52529":"code","096b8074":"code","3816f5ae":"code","89f98601":"code","0582bd7d":"code","7af33194":"code","3eea7921":"code","ab0a9292":"code","27ea79b1":"code","697a2998":"code","5c421a90":"code","1baba1e7":"code","76e168d9":"code","10a266e7":"code","58612988":"code","45e8bd3e":"code","7864fc31":"code","207f5fcf":"code","e2edc7fe":"code","aa8f687b":"code","94db9670":"markdown","af7b6fe3":"markdown","2b4f0446":"markdown","b30cdc6d":"markdown","30e9989e":"markdown","c5d6449a":"markdown","3e92e647":"markdown","3bddefee":"markdown","d5e8a568":"markdown","6a21f89f":"markdown","53e227fd":"markdown","6f23a396":"markdown","cea2db14":"markdown","29e2aea4":"markdown","71c7536f":"markdown","40729df9":"markdown","75c30680":"markdown","9a057fff":"markdown","fe0ba95f":"markdown","9f48880b":"markdown","615134c3":"markdown","82c0a19a":"markdown","96244a3d":"markdown","89b299e0":"markdown","124af1cc":"markdown","aa97310c":"markdown","5098aec8":"markdown","7b4bc180":"markdown","29954ed9":"markdown","4d07898a":"markdown","82dce142":"markdown"},"source":{"70b0dfb5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport math\nimport csv\nfrom tqdm import tqdm\n\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.decomposition import LatentDirichletAllocation\nimport spacy\nfrom wordcloud import WordCloud\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1f4f718":"df = pd.read_csv(\"\/kaggle\/input\/govtech-text-indon\/indonesia_news_jan_may_2020.csv\", parse_dates=['NormalisedDate'])\nprint('Shape of dataset',df.shape)\ndf.head()","952c7f39":"df.isna().sum()","134fc82d":"print('In fact, there are {} empty author entries.'.format(df.Author[df['Author'] == '[]'].count()))\ndf.Author = df.Author.replace({'[]': np.NaN})","ca86ddfd":"try: \n    import tldextract\nexcept ImportError:\n    !pip install tldextract\n    import tldextract\ndf['domain'] = df['SOURCEURL'].apply(lambda x: tldextract.extract(x).domain)\ndf['domain'].value_counts(ascending=True)[-20:].plot(kind='barh',figsize=[6, 5],title='Top 20 Domain Names')\nplt.show()\nnum_articles = sum(df['domain'].value_counts()[:20])\nprint('The top {} domains published {} out of {} articles ({:.2f}%).'.format(20,num_articles,len(df),num_articles\/len(df)*100))\nprint('There are {} unique domains.'.format(df['domain'].nunique()))","a4644cd8":"print(df['Text'].iloc[0])","53ce1734":"df.groupby('NormalisedDate').size().plot.line(figsize=[10, 3],title='Number of articles across time')\nplt.ylabel('Number of articles')\nplt.xlabel('Date')\nplt.show()","e0b55472":"print('News articles dates range from {} to {}.'.format(df.NormalisedDate.min().strftime('%A %d %B %Y'),df.NormalisedDate.max().strftime('%A %d %B %Y')))\nprint('Max number of articles: {} published on {}.'.format(df.groupby('NormalisedDate').size().max(),df.groupby('NormalisedDate').size().idxmax().strftime('%A %d %B %Y')))\nprint('Statistics of articles published per day:')\nprint(df.groupby('NormalisedDate').size().describe())","5ddefea9":"df['NormalisedDate'].dt.day_name().value_counts().plot(kind='barh',figsize=[5, 3],title='Histogram of day of week published')\nplt.xlabel('Number of articles')\nplt.ylabel('Day of week')\nplt.show()","e4c18875":"df['week'] = df['NormalisedDate'].apply(lambda x : x.isocalendar()[1])\ndf.groupby('week').size().plot.line(figsize=[10, 3],title='Number of articles across time')\n\nplt.xticks(list(i for i in range(df['week'].max()+1) if i%4 == 0))\nplt.xlabel('Week number')\nplt.ylabel('Number of articles')\nplt.show()","0c34e423":"df['NormalisedDate'].dt.month_name().value_counts().plot(kind='barh',figsize=[4, 3],title='Histogram of month published')\nplt.xlabel('Number of articles')\nplt.ylabel('Month')\nplt.show()","6dbf7bf8":"df['text'] = df['Text'].str.strip().str.lower().str.replace('[^\\w\\s]','') # regex matching on non letters, numbers, underscores or spaces","d26094a6":"nlp = spacy.load(\"en_core_web_lg\",disable=[\"tagger\", \"parser\"])\ntqdm.pandas()","d1e757be":"stopwords = spacy.lang.en.stop_words.STOP_WORDS\ncustom_stopwords = {'say'}\nfor word in custom_stopwords:\n    stopwords.add(word)","0359981b":"def spacy_tokenizer(sentence):\n    return [word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1 or word.lemma_ in custom_stopwords)]","841c13f4":"print('Example cleaned text')\nprint(' '.join(spacy_tokenizer(df.iloc[0].text)))","d9945068":"texts = df.text\ncount_vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, min_df=3)\ncount_vector = count_vectorizer.fit_transform(tqdm(texts))","03cdaeab":"word_count = pd.DataFrame({'word': count_vectorizer.get_feature_names(), 'count': np.asarray(count_vector.sum(axis=0))[0]})\nword_count.sort_values('count', ascending=False).set_index('word')[:20].sort_values('count', ascending=True).plot(kind='barh')\nplt.title('Word Count in Combined Corpus')\nplt.xlabel('Count')\nplt.show()","d8a77e69":"num_topics = 30","51f56e1b":"lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\nlda.fit(count_vector)","5fd32a49":"def print_top_words(model, vectorizer, n_top_words):\n    feature_names = vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n        \n        print(message)\nprint_top_words(lda, count_vectorizer, n_top_words=25)","11691de4":"def get_top_words(model, vectorizer, n_top_words):\n    feature_names = vectorizer.get_feature_names()\n    topics = []\n    for topic_idx, topic in enumerate(model.components_):\n        topics.append([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n    return topics","ff3e44fc":"topics = pd.Series(get_top_words(lda, count_vectorizer, n_top_words=25))\ntopics","35f2910c":"df['topic'].value_counts(ascending=True)[-20:].plot(kind='barh',figsize=[10, 5],title='Top 20 Topics')\nplt.title('Topic Count in Combined Corpus')\nplt.xlabel('Count')\nplt.show()","8315eb28":"tfidf = TfidfTransformer()\ntfidf.fit_transform(count_vector)\n\nidf_weights = pd.DataFrame(tfidf.idf_, index=count_vectorizer.get_feature_names(),columns=[\"idf_weights\"]).sort_values(by=['idf_weights'])\nidf_weights","42ca561f":"doc_topic_dist = pd.DataFrame(lda.transform(count_vector))\ndf['topic'] = doc_topic_dist.idxmax(axis=1)\ndf['probability'] = doc_topic_dist.max(axis=1)","be47213e":"prob = 0.1\nplt.bar(range(20),[doc_topic_dist[doc_topic_dist>prob][i].count() for i in range(20)])\nplt.title('Number of articles by topic with >{}% in content'.format(prob*100))\nplt.xlabel('Topic')\nplt.ylabel('Count')\nplt.show()","a0c52529":"def get_topic_wordclouds(n_topics,idf=False):\n    \"\"\"\n    Generates top n topics by number of articles in dataset.\n    Wordclouds of each topic is printed as output.\n    Size of words are scaled by count if idf is false. Else, it is scaled by tf-idf.\n    \"\"\"\n    for topic in topics[df['topic'].value_counts()[:n_topics].index]:\n        if idf:\n            weights = dict(((word, len(word_count[word_count['word']==word]['count'].values[0] * idf_weights.loc[word])) for word in topic))\n        else:\n            weights = dict(((word, word_count[word_count['word']==word]['count'].values[0]) for word in topic))\n\n        wordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue',width=2400, height=1500)\n        wordcloud.generate_from_frequencies(weights)\n        plt.imshow(wordcloud)\n        plt.axis(\"off\")\n        plt.show()\n        \nget_topic_wordclouds(4,idf=False)","096b8074":"get_topic_wordclouds(4,idf=True)","3816f5ae":"n = 8\n\nplt.figure(figsize=(20,8))\nfor i in range(n):\n    plt.plot(df[df['topic'] == df['topic'].value_counts().index[i]].groupby('week').size(),marker='x', label='Topic '+str(df['topic'].value_counts().index[i])+': '+' '.join(topics.iloc[df['topic'].value_counts().index[i]][:5]))\nplt.legend(loc=\"upper right\")\nplt.title('Top '+str(n)+' topics over time')\nplt.xticks(list(i for i in range(df['week'].max()+1) if i%4 == 0))\nplt.xlabel('Week number')\nplt.ylabel('Number of articles')\nplt.show()","89f98601":"def plot_difference_plotly(mdiff, title=\"\", annotation=None):\n    \"\"\"Plot the difference between models.\n\n    Uses plotly as the backend.\"\"\"\n    import plotly.graph_objs as go\n    import plotly.offline as py\n\n    annotation_html = None\n    if annotation is not None:\n        annotation_html = [\n            [\n                \"similarity {}\".format(\", \".join(diff_tokens))\n                for diff_tokens in annotation\n            ]\n            for row in annotation\n        ]\n\n    data = go.Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n    layout = go.Layout(width=950, height=950, title=title, xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n    py.iplot(dict(data=[data], layout=layout))\n\n\ndef plot_difference_matplotlib(mdiff, title=\"\", annotation=None):\n    \"\"\"Helper function to plot difference between models.\n\n    Uses matplotlib as the backend.\"\"\"\n    import matplotlib.pyplot as plt\n    fig, ax = plt.subplots(figsize=(18, 14))\n    data = ax.imshow(mdiff, cmap='RdBu_r', origin='lower')\n    plt.title(title)\n    plt.colorbar(data)\n\n\ntry:\n    get_ipython()\n    import plotly.offline as py\nexcept Exception:\n    #\n    # Fall back to matplotlib if we're not in a notebook, or if plotly is\n    # unavailable for whatever reason.\n    #\n    plot_difference = plot_difference_matplotlib\nelse:\n    py.init_notebook_mode()\n    plot_difference = plot_difference_plotly\n","0582bd7d":"top_n = num_topics\ndef jaccard_similarity(list1, list2):\n    intersection = len(list(set(list1).intersection(list2)))\n    union = (len(list1) + len(list2)) - intersection\n    return float(intersection) \/ union, list(set(list1).intersection(list2))\n\nprint('Example similarity:')\nprint(jaccard_similarity(topics.iloc[1][:top_n],topics.iloc[2][:top_n]))","7af33194":"closest = []\nannotation = []\nmdiff = np.ones((num_topics, num_topics))\nfor i in range(num_topics):\n    to_add = []\n    for j in range(num_topics):\n        score, words = jaccard_similarity(topics.iloc[i][:top_n],topics.iloc[j][:top_n])\n        to_add.append(words)\n        mdiff[i,j] = score\n        if 0.2 <score<1: closest.append((i+1,j+1,score,words))\n    annotation.append(to_add)\n\nplot_difference(mdiff, title=\"Topic difference by Jaccard similarity\")","3eea7921":"from scipy.stats.stats import pearsonr\n\ntopic_time_series = []\nfor j in range(num_topics):\n    week = df[df['topic'] == j].groupby('week').size()\n    topic_time_series.append([week[i] if i in week.index else 0 for i in range(1, df['week'].max()+1)])\n    \nmdiff = np.ones((num_topics, num_topics))\ninsignificant = []\n\nfor i in range(num_topics):\n    for j in range(num_topics):\n        rho, p = pearsonr(topic_time_series[i],topic_time_series[j])\n        mdiff[i,j] = rho\n        if not p < 0.05: insignificant.append((i,j,rho,p))\n\nprint('There are {} out of {} values with p >= 0.05, i.e. not statistically significant correlation.'.format(len(insignificant),30**2))\nplot_difference(mdiff, title=\"Topic time series by Pearson Correlation\")","ab0a9292":"import pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()\npanel = pyLDAvis.sklearn.prepare(lda, count_vector, count_vectorizer, mds='tsne', sort_topics=False)\npyLDAvis.display(panel)","27ea79b1":"domain_topic_count = df.groupby(['topic','domain']).size().reset_index(name='count')\ndomain_topic_count","697a2998":"rarity_range = (domain_topic_count['count'] \/ df['topic'].value_counts()[domain_topic_count['topic']]).value_counts()\nrarity_range","5c421a90":"(df['domain'].value_counts().value_counts()\/df['domain'].nunique())[:10]","1baba1e7":"n = 20\ntop_domains = {domain for domain in df['domain'].value_counts().index[:n]}\nprint(top_domains)","76e168d9":"domain_topic_max = domain_topic_count.iloc[domain_topic_count.groupby(['domain']).idxmax()['count']]\ndomain_topic_max","10a266e7":"domain_published_sum = domain_topic_count.groupby('domain').sum()['count']\ndomain_published_sum = domain_published_sum[domain_published_sum > 2] # sources with at least 3 articles overall\ndomain_published_sum","58612988":"domain_topic_max = domain_topic_max[domain_topic_max['domain'].isin(domain_published_sum.index)]\ndomain_topic_max = domain_topic_max[domain_topic_max['count'] > 2] # sources with at least 3 articles in top topic\ndomain_topic_max","45e8bd3e":"top_topics = domain_topic_max['topic'].value_counts()\ntop_topics.plot(kind='bar',figsize=[8, 5])\n\nplt.title('Histogram of top topics per publisher with >2 articles on the topic')\nplt.ylabel('Number of publishers')\nplt.xlabel('Topic number')\nplt.show()\n\nfor i in top_topics[:3].index:\n    print('Topic {}: {}.'.format(i,' '.join(topics[i])))","7864fc31":"domain_topic_max['proportion'] = [(domain_topic_max[domain_topic_max['domain']==domain]['count'].values[0] \/ domain_published_sum[domain]) for domain in domain_topic_max['domain']]\ndomain_topic_max","207f5fcf":"domain_topic_max_filtered = domain_topic_max[domain_topic_max['proportion']>.5]\ndomain_topic_max_filtered['topic'].value_counts().plot(kind='bar',figsize=[8, 5])\n\nfor i in domain_topic_max_filtered['topic'].value_counts()[:3].index:\n    print('Topic {}: {}.'.format(i,' '.join(topics[i])))\n\nplt.title('Histogram of top topics per publisher with >50% of and >3 articles on topic')\nplt.ylabel('Number of publishers')\nplt.xlabel('Topic number')\nplt.show()","e2edc7fe":"topics_docs = topics.apply(lambda x : nlp(' '.join(x)))\n\nfrom collections import Counter\nner_count = {}\nfor doc in topics_docs:\n    for ent in doc.ents:\n        if ent.label_ not in ner_count: ner_count[ent.label_] = Counter()\n        ner_count[ent.label_][ent.text]+=1\n        \nfor entity, counter in ner_count.items():\n#     print(entity)\n    text = []\n    for word, count in counter.most_common(50):\n        [text.append(word) for _ in range(count)]\n#         print(word,count)\n        \n    if len(text) > 10:\n        wordcloud = WordCloud(background_color=\"white\",contour_width=3, contour_color='steelblue') # , max_words=5000, ,width=2400, height=1500\n        wordcloud.generate(' '.join(text))\n        plt.imshow(wordcloud)\n        plt.axis(\"off\")\n        plt.title(entity+': '+spacy.explain(entity))\n        plt.show()","aa8f687b":"from spacy import displacy\ndisplacy.render(topics_docs, style=\"ent\")","94db9670":"Getting the most popular topic per publisher with at least 3 publications in the topic.","af7b6fe3":"Plot top n topics against time","2b4f0446":"Example text","b30cdc6d":"## NER on topics","30e9989e":"Looking at top topics published by top n domains, for n = 20","c5d6449a":"Visualizing similarity of topics by Multi-dimensional scaling using T-SNE.","3e92e647":"Plotting the number of articles per date, we see a cyclic pattern of roughly 4 peaks per month.","3bddefee":"### EDA","d5e8a568":"Get statistics of number of articles published per day.","6a21f89f":"An attempt to compare correlation in publication time with topics failed, due to statistical insignificance.","53e227fd":"## Visualising topics","6f23a396":"## Topics over time","cea2db14":"To account for this shortfall of information from author, we can use the domain name from SOURCEURL.","29e2aea4":"# Identifying Key Issues and Trends Related to a Indonesia from News Reports","71c7536f":"Compare the similarity between topics.","40729df9":"The Monday to Friday work week explains the cyclic nature of publications, with dips representing weekends.","75c30680":"## Comparing topic similarity","9a057fff":"## Mapping topics to sources","fe0ba95f":"Get top topics per domain within top 20 domains.","9f48880b":"Checking for missing values","615134c3":"## LDA Topic Modeling","82c0a19a":"Looking at the range of rarity values calculated (max = 0.7%), we need to try another measure.","96244a3d":"## Text preprocessing\nCleaning text: Removing punctuation and mapping text to lowercase","89b299e0":"Aggregating by week, we see the number of articles being rather constant","124af1cc":"### Import dependencies","aa97310c":"Finding domains that publish more than 50% of its publications on its top topic.","5098aec8":"Plotting wordclouds of topics to visualize","7b4bc180":"We see that many domains appear only once or twice (55.9% and 15.4% respectively).","29954ed9":"### Initial idea\nTopics exclusive to certain news outlets:\nCalculate # of articles \/ # topics for each domain and topic. If rarity > threshold, topic is exclusive to these domains (i.e. x% of articles published by domain).","4d07898a":"Using spaCy, we classify named entity from each topic by types, and generate wordclouds to visualize the main entities by types.","82dce142":"This does not corroborate with the output from df.head(), where we see some Author values registered as an empty list."}}