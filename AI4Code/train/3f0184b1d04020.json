{"cell_type":{"24d653f9":"code","706ccc3b":"code","d3c36128":"code","f39325e3":"code","8b2e5d8d":"code","f689719a":"code","c9b1e8e3":"code","7ca020ca":"code","5d0511f3":"code","34aa00ab":"code","8ab8ee2e":"code","2ac43774":"code","7cd77b4b":"code","2bd3b4d9":"code","6444db2b":"code","b99d2cb0":"code","450a18fb":"code","712333de":"code","3951e4b3":"code","e2dbaae0":"markdown","7855073b":"markdown","3e3a2d38":"markdown","a2d817b0":"markdown","02574ac9":"markdown","3d2af695":"markdown","d21c6d4a":"markdown","066aa659":"markdown","ccf0be3d":"markdown","48a924a9":"markdown","3e4be22b":"markdown","908e850a":"markdown","38ce36fc":"markdown","81df559a":"markdown","f672ebfc":"markdown","d915ec20":"markdown","4ac06d0b":"markdown","0057ce92":"markdown"},"source":{"24d653f9":"from fastai.vision import DataBunch, Learner, accuracy, top_k_accuracy, callbacks, ClassificationInterpretation, DatasetType\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom torchvision import transforms\nimport torch\nimport numpy as np\n\nfrom torch import nn\n\nimport matplotlib.pyplot as plt\n\nfrom functools import partial\n\nplt.style.use('seaborn')","706ccc3b":"torch.manual_seed(0)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(0)","d3c36128":"train_data = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(train_data.shape, test_data.shape)\ntrain_data.head()","f39325e3":"y = train_data.values[:, 0]\nX = train_data.values[:, 1:]","8b2e5d8d":"X_test = test_data.values\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_val.shape","f689719a":"class MNIST(torch.utils.data.Dataset):\n\n    def __init__(self, X, y=None, transform=None):\n        self.X = X.astype(np.uint8).reshape(-1, 28, 28)\n        \n        if y is None:\n            y = torch.zeros(len(X_test))\n        \n        class dummy:\n            def __getitem__(self, idx):\n                return y[idx]            \n            classes = np.unique(y).tolist()\n            \n        self.y = dummy()\n        \n        self.transform = transform\n        \n        self.c = len(self.y.classes)\n\n    def __len__(self):\n        return len(self.X)\n        \n    def __getitem__(self, idx):        \n        X = self.X[idx]\n        if self.transform:\n            X = self.transform(X)\n\n        return X, self.y[idx]\n    ","c9b1e8e3":"train_dataset = MNIST(X_train, y_train, transform=transforms.Compose([\n                                               transforms.ToPILImage(mode='L'),\n                                               transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n                                               transforms.RandomResizedCrop(28, scale=(0.98, 1.02), ratio=(1, 1)),\n                                               transforms.ToTensor(),\n                                               transforms.Normalize(X_train.mean()[None], X_train.std()[None]),\n                                           ]))\n\n\nval_dataset = MNIST(X_val, y_val, transform=transforms.Compose([\n                                               transforms.ToPILImage(mode='L'),                                           \n                                               transforms.ToTensor(),\n                                               transforms.Normalize(X_val.mean()[None], X_val.std()[None]),  \n                                           ]))\ntest_dataset = MNIST(X_test, transform=transforms.Compose([\n                                               transforms.ToPILImage(mode='L'), \n                                               transforms.ToTensor(),\n                                               transforms.Normalize(X_test.mean()[None], X_test.std()[None]),\n                                           ]))","7ca020ca":"def get_loader(dataset, batch_size, shuffle=True,):\n    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, pin_memory=True)\n\ndef get_Databunch(bs=128):\n    return DataBunch(\n            train_dl=get_loader(train_dataset, batch_size=bs),  \n            valid_dl=get_loader(val_dataset, batch_size=bs),  \n            test_dl=get_loader(test_dataset, batch_size=bs, shuffle=False)\n        )","5d0511f3":"fig, axs = plt.subplots(10, 10, figsize=(25, 25))\ndb = get_Databunch(bs=128)\nb = db.one_batch()\nfor i, ax in enumerate(axs.flatten()):\n    ax.set_title(b[1][i].item())\n    ax.imshow(b[0][i][0].numpy())\n    ax.axis('off')","34aa00ab":"number_of_classes = 10\ninput_features = 784\nclass MNISClassifier(nn.Module):\n    def __init__(self, do = 0.25):\n        super(MNISClassifier, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.PReLU(),\n            \n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), \n            nn.BatchNorm2d(64),\n            nn.PReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),            \n            nn.Dropout(do)    \n        )\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.PReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1), \n            nn.BatchNorm2d(128),\n            nn.PReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),            \n            nn.Dropout(do)    \n        )\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(256),\n            nn.PReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1), \n            nn.BatchNorm2d(256),\n            nn.PReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),            \n            nn.Dropout(do),\n            \n            nn.Conv2d(256, 512, kernel_size=1, stride=1, padding=1),\n            nn.PReLU(),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size=2, stride=2), \n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=2, stride=1, padding=1), \n            nn.PReLU(),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(kernel_size=2, stride=2),            \n            nn.Dropout(do)    \n        )        \n        \n        self.layer4 = nn.Sequential(\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.PReLU(),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.PReLU(),\n            nn.Linear(256, number_of_classes)\n        )\n        \n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.layer1(x.contiguous())        \n        x = self.layer2(x)\n        x = self.layer3(x)        \n        x = x.view(batch_size, -1)\n        x = self.layer4(x)        \n        \n        return x\n","8ab8ee2e":"label_value_counts = train_data.label.value_counts().reset_index()\nlabel_value_counts.columns = ['Class', 'Count']\nlabel_value_counts.set_index('Class').plot.barh()","2ac43774":"weights = (train_data.label.value_counts().mean()\/train_data.label.value_counts())\n\nw = weights.reset_index()\nw.columns = ['Class', 'Weight']\nw.set_index('Class').plot.barh()","7cd77b4b":"weights_tensor = torch.from_numpy(weights.values).float().cuda()\nlearn = Learner(    \n    get_Databunch(bs=512), \n    MNISClassifier(0.2), \n    loss_func=nn.CrossEntropyLoss(weight=weights_tensor),\n    metrics=accuracy\n)\nlearn.fit(\n    epochs=5, \n    lr=0.0005, \n    callbacks=callbacks.SaveModelCallback(learn, every='improvement', monitor='accuracy')\n)","2bd3b4d9":"def top_2_acc(*args):\n    return top_k_accuracy(*args, k=2)\n\n\nlearn = Learner(    \n    get_Databunch(bs=1024),\n    learn.model, \n    loss_func=nn.CrossEntropyLoss(weight=weights_tensor),\n    metrics=[accuracy, top_2_acc]\n)\ndef current_lr(out, true, learn):\n    return torch.tensor(learn.recorder.lrs[-1])\n\ndef current_mom(out, true, learn):\n    return torch.tensor(learn.recorder.moms[-1])\n\nlearn.metrics.append(partial(current_lr, learn=learn))\nlearn.metrics.append(partial(current_mom, learn=learn))\nepochs=30\nlearn.fit_one_cycle(\n    cyc_len=epochs, \n    max_lr=0.0005,\n    div_factor=40,\n    callbacks=callbacks.SaveModelCallback(learn, every='improvement', monitor='accuracy')\n)\n","6444db2b":"learn.recorder.plot_losses()","b99d2cb0":"ci = ClassificationInterpretation.from_learner(learn)\nci.plot_confusion_matrix(figsize=(6,6), normalize=False)","450a18fb":"preds, _ = learn.get_preds(DatasetType.Test)\nlabels = preds.cpu().numpy().argmax(1)\n\nsub =  pd.DataFrame(data=dict(ImageId=range(1, len(labels)+1), Label=labels))\nsub.head()","712333de":"sub.Label.value_counts().plot.bar()","3951e4b3":"sub.to_csv('sub.csv', index=False)","e2dbaae0":"We notice a slight imbalance in the distribution, so we use weights to compensate for it. We use them later in the CrossEntropyLoss.","7855073b":"# FastNumbers \ud83d\udd22 \ud83d\udca8\n\nWe use a simple CNN implemented in PyTorch and use [fast.ai](https:\/\/www.fast.ai\/) to get a state-of-the-art result for the MNIST dataset.","3e3a2d38":"Now we use the sklearn's *train_test_split* function splitting the training data into a training record and a validation record. We choose the popular 80\/20 split ratio.","a2d817b0":"\nNow we'll start training. First we do a little pre-training to move the weights in the right direction and later use a [1cyclic Policy Learning Rate](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) to get state-of-the-art accuracy. \nWe use a callback that always stores the best model and loads the best model after training. So we can be sure that we are always working with the best model.","02574ac9":"Now let's use fastai's built-in confusion matrix to take a look at our results.","3d2af695":"Now let's see what our transformed numbers look like.","d21c6d4a":"Finally, we can check the distribution of our predicted values and see if it makes sense. ","066aa659":"Now we need to predict the data from our test set. We can do this simply by using *get_preds* and passing *DatasetType.Test* as type to indicate that we want the preds for the test set.","ccf0be3d":"We want the results to be reproducible, so we set the following options:","48a924a9":"To reduce overfitting, we use PyTorch's transformation functions. This function set scales, rotates and moves images in the train data set randomly. You can play around with the values to change the appearance, but remember that the numbers must still be recognizable.  We also use transformations to normalize all datasets.","3e4be22b":"### Load the Data\nIn order to load the data we use pandas' build-in function *read_csv* resulting in a *DataFrame*.","908e850a":"### Creating the Dataset\nWe want to use fastai's build-in features later, so we have to create a PyTorch Dataset. The Dataset needs two functions:\n* \\__len__ which returns the length of the data set\n* \\__getitem__ which returns an item from the dataset given an index\n\n\nFastai also needs the property *c*, which represents the number of classes. The *y* attribute also requires a *classes* attribute that stores all classes contained in y, so we create a dummy class that meets this requirement. In our data set we first transform the data type of the images X to np.uint8 and transform them into their native ratio 28x28. \n\n","38ce36fc":"## Model training\n","81df559a":"We observe that the first columns represent the label and the other columns the pixel. Therefore we have to slice the training set into a data part (*X*) and a label part (*y*). We access the numpy array representation of the DataFrame via the *values* property. To split the numpy array into X and y, we have to retrieve first column using \\[:, 0\\] and all but the first column using \\[:, 1:\\].","f672ebfc":"All right, the numbers look good so we can go on with our CNN. We used a standard CNN architecture with multiple convolution layers with max pooling.","d915ec20":"The distribution is fine, so we write the results to a csv file.","4ac06d0b":"Let's check the distribution of the labels before training.","0057ce92":"Now we pass everything to fastai's Databunch, which we will use later. First, we have to create a dataloader from the dataset. The Databunch takes a train, validation and test dataloader (train_dl, valid_dl, test_dl)."}}