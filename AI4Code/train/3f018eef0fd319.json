{"cell_type":{"63c5ce7e":"code","0bcdd8e4":"code","9afd9f19":"code","9f96361b":"code","a2d332a7":"code","22bb2016":"code","6178fc79":"code","72154b5c":"code","4ab53191":"code","e9d84301":"code","696b8c0f":"code","6f3af12a":"code","e2a21809":"code","5a9600a4":"code","6c969d08":"code","11a05242":"code","43aefcc5":"code","726b283f":"code","5f6e8017":"code","cec9ff00":"code","80b7bafd":"code","ea2a5871":"code","867fa0c3":"code","4c88126c":"code","fe949581":"code","dc8eb056":"code","d1cda230":"code","bb7eb79c":"code","0a737780":"code","e1c0142a":"code","a09eb591":"code","f86fe005":"code","1afc9fed":"code","dcede542":"code","84e5f74e":"code","7c51b63a":"code","c8d0dc30":"code","91af9544":"code","6c6121c9":"code","c99b5d8a":"code","48cfc6e2":"code","8f479ba4":"code","dd225266":"code","b20a3d7e":"code","2d86c11d":"code","682aeb2c":"code","cb4fc705":"code","a6268a19":"code","fda2fce5":"code","83487f92":"code","6bdd7c84":"code","c5815b3f":"code","4d136f24":"code","f5cd421e":"code","00ab3531":"code","ec22d733":"code","183eb692":"code","86e3ca12":"code","0790e479":"code","6f2231a7":"code","7b55b6ef":"code","199093a3":"code","8f5d97c1":"code","e4beb322":"code","5d2da253":"code","d250e463":"code","6025fefb":"code","16da8a68":"code","681adb06":"code","acd60b10":"code","76d7bee6":"code","307c1442":"code","74f4e895":"code","1e838349":"code","4a8b4f4d":"code","2b7126d2":"code","cb1fbab3":"markdown","89819310":"markdown","05827e6a":"markdown","0147484a":"markdown","8c2ce661":"markdown","394e9cc3":"markdown","4517626b":"markdown","3d24428e":"markdown","243f351c":"markdown","5347d9d2":"markdown","131fa589":"markdown","234537dd":"markdown","093b3903":"markdown","5af3a62e":"markdown","dd173c4f":"markdown","7210fdd2":"markdown","1603732d":"markdown","17667e7d":"markdown","d879b593":"markdown","9895b4fc":"markdown","e728609f":"markdown","5f736e22":"markdown","746cfb12":"markdown","ab580ba9":"markdown","e01f2670":"markdown","73fe41a2":"markdown","1c486fbd":"markdown","a39e4d2c":"markdown","a488a175":"markdown","a5afc145":"markdown","8d752b55":"markdown","7ddb903e":"markdown","1d32d259":"markdown","4f58b231":"markdown","c08734ac":"markdown","a8498ab2":"markdown","4b7cb6ce":"markdown","e2c4a214":"markdown","7ac1ccbc":"markdown","921bea38":"markdown","018ac5cb":"markdown","0cc0c4e8":"markdown","346f9132":"markdown","af8e7103":"markdown","9be39f43":"markdown","7d07d2f9":"markdown","18d1d3d8":"markdown","8614bbb3":"markdown","ddc1031e":"markdown"},"source":{"63c5ce7e":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\nimport warnings\nwarnings.simplefilter('ignore')","0bcdd8e4":"data = pd.read_csv('..\/input\/nlp-reports-news-classification\/water_problem_nlp_en_for_Kaggle_100.csv', delimiter=';', header=0)\ndata = data.fillna(0)\n\nconvert_dict = {'text': str, \n                'env_problems': int,\n                'pollution': int, \n                'treatment': int,\n                'climate': int,\n                'biomonitoring': int} \n  \ndata = data.astype(convert_dict)\ndata","9afd9f19":"# Take \"env_problems\" only\ntarget_name = 'env_problems'\nname_for_plot = 'environmental problems'\ndata = data[['text', target_name]]\ndata.columns = ['text', 'target']\ndata","9f96361b":"# The TASK for this dataset requires that the test set contain at least 40% of the data \ntrain, test = train_test_split(data, test_size=0.4, shuffle=True, random_state=0)\ntrain_base = train.copy()\ntrain_target = train_base['target']\ntest_base = test.copy()\ntest_target = test_base['target']","a2d332a7":"print('There are {} rows and {} columns in train'.format(data.shape[0],data.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","22bb2016":"data.head(10)","6178fc79":"# extracting the number of examples of each class\nReal_len = data[data['target'] == 1].shape[0]\nNot_len = data[data['target'] == 0].shape[0]","72154b5c":"# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,Real_len,3, label=\"Real\", color='blue')\nplt.bar(15,Not_len,3, label=\"Not\", color='red')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","4ab53191":"def length(text):    \n    '''a function which returns the length of text'''\n    return len(text)","e9d84301":"data['length'] = data['text'].apply(length)","696b8c0f":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nbins = 150\nplt.hist(data[data['target'] == 0]['length'], alpha = 0.6, bins=bins, label='Not')\nplt.hist(data[data['target'] == 1]['length'], alpha = 0.8, bins=bins, label='Real')\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nplt.grid()\nplt.show()","6f3af12a":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ndata_len=data[data['target']==1]['text'].str.len()\nax1.hist(data_len,color='blue')\nax1.set_title(f'{name_for_plot} datas')\ndata_len=data[data['target']==0]['text'].str.len()\nax2.hist(data_len,color='red')\nax2.set_title(f'Not {name_for_plot} datas')\nfig.suptitle('Characters in datas')\nplt.show()","e2a21809":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ndata_len=data[data['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(data_len,color='blue')\nax1.set_title(f'{name_for_plot} datas')\ndata_len=data[data['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(data_len,color='red')\nax2.set_title(f'Not {name_for_plot} datas')\nfig.suptitle('Words in a data')\nplt.show()\n","5a9600a4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword=data[data['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='blue')\nax1.set_title(name_for_plot)\nword=data[data['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title(f'Not {name_for_plot}')\nfig.suptitle('Average word length in each data')","6c969d08":"def create_corpus(target):\n    corpus=[]\n    \n    for x in data[data['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","11a05242":"def create_corpus_df(data, target):\n    corpus=[]\n    \n    for x in data[data['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","43aefcc5":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]","726b283f":"# displaying the stopwords\nnp.array(stop)","5f6e8017":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","cec9ff00":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\nplt.rcParams['figure.figsize'] = (18.0, 6.0)\nx,y=zip(*top)\nplt.bar(x,y)","80b7bafd":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","ea2a5871":"plt.figure(figsize=(16,5))\ncorpus=create_corpus(0)\ndic=defaultdict(int)\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","867fa0c3":"plt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","4c88126c":"sns.barplot(x=y,y=x)","fe949581":"def get_top_data_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","dc8eb056":"plt.figure(figsize=(16,5))\ntop_data_bigrams=get_top_data_bigrams(data['text'])[:10]\nx,y=map(list,zip(*top_data_bigrams))\nsns.barplot(x=y,y=x)","d1cda230":"df = pd.concat([train,test])\ndf.shape","bb7eb79c":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","0a737780":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","e1c0142a":"df['text']=df['text'].apply(lambda x : remove_URL(x))","a09eb591":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","f86fe005":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","1afc9fed":"df['text']=df['text'].apply(lambda x : remove_html(x))","dcede542":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","84e5f74e":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","7c51b63a":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","c8d0dc30":"df['text']=df['text'].apply(lambda x : remove_punct(x))","91af9544":"target_big_corrected = True","6c6121c9":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef clean_datas(data):\n    \"\"\"Removes links and non-ASCII characters\"\"\"\n    \n    data = ''.join([x for x in data if x in string.printable])\n    \n    # Removing URLs\n    data = re.sub(r\"http\\S+\", \"\", data)\n    \n    return data","c99b5d8a":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","48cfc6e2":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef remove_punctuations(text):\n    punctuations = '@#!?+&*[]-%.:\/();$=><|{}^' + \"'`\"\n    \n    for p in punctuations:\n        text = text.replace(p, f' {p} ')\n\n    text = text.replace('...', ' ... ')\n    \n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n    \n    return text","8f479ba4":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life redata\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"redata\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"data me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","dd225266":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","b20a3d7e":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\ndef convert_abbrev_in_text(text):\n    tokens = word_tokenize(text)\n    tokens = [convert_abbrev(word) for word in tokens]\n    text = ' '.join(tokens)\n    return text","2d86c11d":"def data_clearning(df):\n    # Data clearning\n    df[\"text\"] = df[\"text\"].apply(lambda x: clean_datas(x))\n    df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))\n    df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuations(x))\n    df[\"text\"] = df[\"text\"].apply(lambda x: convert_abbrev_in_text(x))\n    return df","682aeb2c":"# Thanks to https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\nif target_big_corrected:\n    train = data_clearning(train)\n    test = data_clearning(test)\n    data = data_clearning(data)","cb4fc705":"corpus_new1=create_corpus_df(df,1)\nlen(corpus_new1)","a6268a19":"corpus_new1[:10]","fda2fce5":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","83487f92":"corpus_new0=create_corpus_df(df,0)\nlen(corpus_new0)","6bdd7c84":"corpus_new0[:10]","c5815b3f":"# Generating the wordcloud with the values under the category dataframe\nplt.figure(figsize=(12,8))\nword_cloud = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.show()","4d136f24":"df.head(10)","f5cd421e":"def cv(data):\n    count_vectorizer = CountVectorizer()\n\n    emb = count_vectorizer.fit_transform(data)\n\n    return emb, count_vectorizer","00ab3531":"# Preparation data for processing\nX_train = train[\"text\"].tolist()\ny_train = train[\"target\"].tolist()\nX_train_counts, count_vectorizer = cv(X_train)\n\nX_test = test[\"text\"].tolist()\ny_test = test[\"target\"].tolist()\nX_test_counts = count_vectorizer.transform(X_test)","ec22d733":"def plot_LSA(test_data, test_labels, savepath=\"PCA_demo.csv\", plot=True):\n        lsa = TruncatedSVD(n_components=2)\n        lsa.fit(test_data)\n        lsa_scores = lsa.transform(test_data)\n        color_mapper = {label:idx for idx,label in enumerate(set(test_labels))}\n        color_column = [color_mapper[label] for label in test_labels]\n        colors = ['orange','blue']\n        if plot:\n            plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=150, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\n            orange_patch = mpatches.Patch(color='orange', label='Not')\n            blue_patch = mpatches.Patch(color='blue', label='Real')\n            plt.legend(handles=[orange_patch, blue_patch], prop={'size': 20})\n\nfig = plt.figure(figsize=(14, 8))          \nplot_LSA(X_train_counts, y_train)\nplt.show()","183eb692":"def tfidf(data):\n    tfidf_vectorizer = TfidfVectorizer()\n\n    train = tfidf_vectorizer.fit_transform(data)\n\n    return train, tfidf_vectorizer\n\nX_train_tfidf, tfidf_vectorizer = tfidf(X_train)\nX_test_tfidf = tfidf_vectorizer.transform(X_test)","86e3ca12":"fig = plt.figure(figsize=(14, 8))\nplot_LSA(X_train_tfidf, y_train)\nplt.show()","0790e479":"def create_corpus_new(df):\n    corpus=[]\n    for data in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(data)]\n        corpus.append(words)\n    return corpus   ","6f2231a7":"corpus=create_corpus_new(df)","7b55b6ef":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","199093a3":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ndata_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","8f5d97c1":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","e4beb322":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec           ","5d2da253":"data_pad[0][0:]","d250e463":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=4e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","6025fefb":"model.summary()","16da8a68":"data_pad.shape","681adb06":"train=data_pad[:train.shape[0]]\ntest=data_pad[train.shape[0]:]","acd60b10":"# Training data\nfig = plt.figure(figsize=(14, 8))          \nplot_LSA(train,train_target)\nplt.show()","76d7bee6":"# Test data\nfig = plt.figure(figsize=(14, 8))          \nplot_LSA(test,test_target)\nplt.show()","307c1442":"# Model tuning\nhistory=model.fit(train,train_target,batch_size=12,epochs=20,validation_data=(test,test_target),verbose=2)","74f4e895":"train_pred_GloVe = model.predict(train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')","1e838349":"# Showing Confusion Matrix\n# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\ndef plot_cm(y_true, y_pred, title, figsize=(5,4)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","4a8b4f4d":"# Showing Confusion Matrix for GloVe model\nplot_cm(y_train, train_pred_GloVe_int, 'Confusion matrix for GloVe model', figsize=(7,7))","2b7126d2":"# Test prediction\ntest_pred = model.predict(test).round().astype('int')\nprint('Accuracy score of the test prediction by the GloVe model -', accuracy_score(test_target, test_pred))","cb1fbab3":"## 5. Data Cleaning <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","89819310":"we will do a bigram (n=2) analysis over the datas. Let's check the most common bigrams in datas.","05827e6a":"First let's check datas indicating clas 1.","0147484a":"### Number of words in a data","8c2ce661":"Before we begin with anything else, let's check the class distribution.","394e9cc3":"Thanks to:\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n* https:\/\/www.kaggle.com\/itratrahman\/nlp-tutorial-using-python","4517626b":"Now,we will move on to class 0.","3d24428e":"Thanks to https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","243f351c":"## 1. Acknowledgements <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)\n\nThis notebook is based on the my notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert) for the Kaggle competition [Natural Language Processing with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)\n\nThis kernel uses such good notebooks: \n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n* https:\/\/www.kaggle.com\/itratrahman\/nlp-tutorial-using-python\n* https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud\n* https:\/\/www.kaggle.com\/vbmokin\/disaster-nlp-keras-bert-using-tfhub-tuning\n* https:\/\/www.kaggle.com\/rftexas\/text-only-kfold-bert\n\nand other resources:\n* https:\/\/github.com\/hundredblocks\/concrete_NLP_tutorial\/blob\/master\/NLP_notebook.ipynb","5347d9d2":"[Go to Top](#0)","131fa589":"Now,we will analyze datas with class 1.","234537dd":"Thanks to https:\/\/github.com\/hundredblocks\/concrete_NLP_tutorial\/blob\/master\/NLP_notebook.ipynb","093b3903":"### Removing urls","5af3a62e":"### Class distribution","dd173c4f":"### Number of characters in datas","7210fdd2":"### Removing HTML tags","1603732d":"Thanks to https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","17667e7d":"In both of them,\"of\" and \"the\" dominates which are followed by \"in\" in class 0 and \"and\" in class 1.","d879b593":"## 9. GloVe <a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","9895b4fc":"### Analyzing punctuations","e728609f":"Here we will use GloVe pretrained corpus model to represent our words. It is available in 3 varieties : 50D, 100D and 200 Dimentional. We will try 100D here.","5f736e22":"These embeddings are not badly separated.","746cfb12":"## 8. TF IDF <a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","ab580ba9":"## 6. WordCloud <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","e01f2670":"### Removing Emojis","73fe41a2":"## 4. EDA <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","1c486fbd":"## Baseline Model with GloVe results","a39e4d2c":"First we  will analyze datas with class 0.","a488a175":"Lot of cleaning needed !","a5afc145":"###  Average word length in a data","8d752b55":"### Removing punctuations","7ddb903e":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [Acknowledgements](#1)\n1. [Import libraries](#2)\n1. [Download data](#3)\n1. [EDA](#4)\n1. [Data Cleaning](#5)\n1. [WordCloud](#6)\n1. [Bag of Words Counts](#7)\n1. [TF IDF](#8)\n1. [GloVe](#9)\n1. [Showing Confusion Matrix for GloVe](#10)","1d32d259":"These embeddings don't look very cleanly separated. Let's see if we can still fit a useful model on them.","4f58b231":"## 2. Import libraries <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","c08734ac":"### Visualizing the embeddings","a8498ab2":"### Common stopwords in datas","4b7cb6ce":"### Target = 0","e2c4a214":"Your comments and feedback are most welcome.","7ac1ccbc":"The distributions are different. 150 characters in a data are the one of the most common among both.","921bea38":"### Target = 1","018ac5cb":"## 10. Showing Confusion Matrix for GloVe <a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","0cc0c4e8":"Thanks to https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial","346f9132":"### Common words","af8e7103":"### N-gram analysis","9be39f43":"### Big target correction","7d07d2f9":"<a class=\"anchor\" id=\"0\"><\/a>\n# Dataset [\"NLP : Reports & News Classification\"](https:\/\/www.kaggle.com\/vbmokin\/nlp-reports-news-classification)\n## *Automatic Environmental Reports & News Classification (English)*\n\n# Target = \"environmental problems\" - Yes\/No or Real\/Not\n\n# NLP:\n* EDA (with WordCloud) \n* Bag of Words \n* TF IDF\n* GloVe\n* PCA visualization\n* Confusion Matrix\n\nBERT see in a separate notebook","18d1d3d8":"I hope you find this notebook and [dataset](https:\/\/www.kaggle.com\/vbmokin\/nlp-reports-news-classification) useful and enjoyable.","8614bbb3":"## 3. Download data <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","ddc1031e":"## 7. Bag of Words Counts <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}