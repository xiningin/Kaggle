{"cell_type":{"33f09ace":"code","230b825d":"code","70a25992":"code","a95ad4ef":"code","9a0e659e":"code","8819f2fe":"code","67887731":"code","803607c9":"code","b3136f53":"code","e2254699":"code","9f1152be":"code","7bfc67be":"code","fc557fe9":"code","a462df50":"code","cf2add73":"code","58418282":"code","dbf065ab":"code","a4ea2ead":"code","b2ff4636":"code","184ca133":"code","bab316fc":"code","556864ee":"code","14dd49a7":"code","0c117b30":"code","2545f1e2":"code","31d24c17":"code","85095b2b":"code","d3427e69":"code","cff09c4f":"code","d46693f1":"code","d718b3c1":"code","0bd60597":"markdown","e59e4b23":"markdown","84ea2c57":"markdown","abf90547":"markdown","688056b4":"markdown","54320c83":"markdown","e15c38c4":"markdown","c616a6a1":"markdown","039e9872":"markdown","4e6cdd80":"markdown","f4bffb0d":"markdown","aa784816":"markdown","95290439":"markdown","f632ba41":"markdown","3fd2be03":"markdown","58273f02":"markdown","ec9207e8":"markdown","e9869973":"markdown","4f8c173a":"markdown","46c03c4c":"markdown","d151684e":"markdown"},"source":{"33f09ace":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Initialize the random number generator\nimport random\nrandom.seed(0)\n\n# Ignore the warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","230b825d":"tweets = pd.read_csv(\"..\/input\/farmers-protest-tweets-dataset-csv\/tweets-2021-03-3.csv\")","70a25992":"tweets.head()","a95ad4ef":"tweets.columns","9a0e659e":"#df2 = df.head(3000)","8819f2fe":"users = pd.read_csv(\"..\/input\/farmers-protest-tweets-dataset-csv\/users-2021-03-3.csv\")","67887731":"users.head()","803607c9":"final =pd.merge(tweets, users, on=\"userId\")","b3136f53":"final.head()","e2254699":"import seaborn as sns\nsns.countplot(x=\"verified\",data=final);","9f1152be":"final.columns","7bfc67be":"final_verified = final[final[\"verified\"]==True]","fc557fe9":"final_verified.shape","a462df50":"check = final_verified.groupby('username')['tweetId'].count()","cf2add73":"print(\"Maximum tweets from verified account\", check.idxmax(), check.max())","58418282":"final_unverified = final[final[\"verified\"]==False]","dbf065ab":"check2 = final_unverified.groupby('username')['tweetId'].count()","a4ea2ead":"print(\"Maximum tweets from unverified account\", check2.idxmax(), check2.max())","b2ff4636":"check3 = final_verified.groupby('location')['tweetId'].count()","184ca133":"print(\"Maximum verified tweets from location\", check3.idxmax(), check3.max())","bab316fc":"check4 = final_unverified.groupby('location')['tweetId'].count()","556864ee":"print(\"Maximum unverified tweets from location\", check4.idxmax(), check4.max())","14dd49a7":"data = pd.read_csv( \"..\/input\/twitter-sentiment\/Sentiment.csv\")\n# Keeping only the neccessary columns\ndata = data[['text','sentiment']]\ndata.head()","0c117b30":"#sns.countplot(x=\"id\",data=data);","2545f1e2":"import re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndata = data[data.sentiment != \"Neutral\"]\ndata['text'] = data['text'].apply(lambda x: x.lower())\ndata['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n\nprint(data[ data['sentiment'] == 'Positive'].size)\nprint(data[ data['sentiment'] == 'Negative'].size)\n\nfor idx,row in data.iterrows():\n    row[0] = row[0].replace('rt',' ')\n    \nvocabSize = 2000\ntokenizer = Tokenizer(num_words=vocabSize, split=' ')\ntokenizer.fit_on_texts(data['text'].values)\nX = tokenizer.texts_to_sequences(data['text'].values)\nX = pad_sequences(X)","31d24c17":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n\nembed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","85095b2b":"from sklearn.model_selection import train_test_split\n\nY = pd.get_dummies(data['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","d3427e69":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 2)","cff09c4f":"score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","d46693f1":"import numpy as np\n\npos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n\nfor x in range(len(X_test)):\n    \n    result = model.predict(X_test[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y_test[x]):\n        if np.argmax(Y_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_test[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\nprint(\"pos_acc\", pos_correct\/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_cnt*100, \"%\")","d718b3c1":"twt = ['Punjab\u2019s lions and Bengals tigers fighting together again. History repeats itself.']\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\nprint(twt)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","0bd60597":"## 0. Setup the environment","e59e4b23":"![image.png](attachment:image.png)","84ea2c57":"## 3. Get the column names","abf90547":"## 2. Print sample data","688056b4":"## 8.2 Data preprocessing for model","54320c83":"## 7. Print sample data","e15c38c4":"## 4. Read the users data","c616a6a1":"## 8.5 Fit the model","039e9872":"## 8.4 Train the LSTM model","4e6cdd80":"## 8.6 Analyse sentiment of a sample tweet","f4bffb0d":"## 6. Merge the datasets","aa784816":"## 5. Print sample data","95290439":"## 8. Sentiment analysis using LSTM","f632ba41":"## We see that a very small percentage of the twitter accounts are verified, let us restrict our analysis to the verified accounts only","3fd2be03":"# We have almost 13 times more tweets from unverified account compared to verified accounts","58273f02":"## 8.3 Define the model","ec9207e8":"## 1. Read the data","e9869973":"## Next steps - Run the analysis on all the tweets","4f8c173a":"# We have almost 6 times more tweets from unverified account in India compared to verified accounts","46c03c4c":"## Subset the data for faster processing, we can then run on the entire data once everything looks fine","d151684e":"### 8.1 Read in sample data with sentiment marked"}}