{"cell_type":{"6cbe8afe":"code","32c1fdf2":"code","cbfa8bc4":"code","c0efddb0":"code","1b7d960b":"code","cafd6876":"code","0e112cd0":"code","59edbabf":"code","169d5e82":"code","d61dee76":"code","fd604eb4":"code","60f7ae17":"code","d5848935":"code","0fe6f6d0":"code","0406275e":"code","4650816d":"code","499e1024":"code","c92a9a2d":"code","ae5123b0":"code","9da46ecf":"code","d5aac4aa":"markdown"},"source":{"6cbe8afe":"!pip install pytorch-msssim","32c1fdf2":"import numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport glob\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.datasets as dset\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nimport pandas as pd\nfrom IPython.display import HTML\nfrom tqdm.auto import tqdm\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.nn.utils import spectral_norm as snorm\nfrom pytorch_msssim import MS_SSIM\n\ndef is_cuda():\n    if torch.cuda.is_available():\n        print(\"CUDA available\")\n        return \"cuda\"\n    else:\n        print(\"No CUDA. Working on CPU.\")\n        return \"cpu\"\n        \ndevice = is_cuda()","cbfa8bc4":"root = \"..\/input\/lionkingscreenshots\/\"\n\nbatch_size = 8\nimage_size = 256\nlr = 1e-4","c0efddb0":"def show_tensor_images(image_tensor, num_images=8, size=(3, 64, 64), nrow=4, figsize=8):\n\n    image_tensor = (image_tensor + 1) \/ 2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n    plt.figure(figsize=(figsize, figsize))\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()\n    \ndef to_rgb(img):\n    rgb_img = Image.new(\"RGB\", img.size)\n    rgb_img.paste(img)\n    return rgb_img\n\nclass ImageSet(Dataset):\n    def __init__(self, root, transform):\n        self.transform = transform\n        self.imgs = sorted(glob.glob(os.path.join(root, \"*.*\"))) # Only LionKing\n        \n    def __getitem__(self, index):\n        img = Image.open(self.imgs[index % len(self.imgs)])\n        img = to_rgb(img)\n        img = self.transform(img)\n        return img\n    \n    def __len__(self):\n        return len(self.imgs)\n\ntransform = transforms.Compose([\n    transforms.Resize(image_size),\n    transforms.CenterCrop(image_size),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n\ndataset = ImageSet(transform=transform, root=\"..\/input\/lionkingscreenshots\/\")\ndataloader = DataLoader(dataset, batch_size=8, shuffle=False)","1b7d960b":"class ResBlock(nn.Module):\n    def __init__(self, channel):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(channel, channel, 3, 1, 1)),\n            nn.BatchNorm2d(channel),\n            nn.ReLU(),\n            nn.utils.spectral_norm(nn.Conv2d(channel, channel, 3, 1, 1)),\n            nn.BatchNorm2d(channel)\n        )\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        res = x\n        x = self.block(x)\n        out = self.relu(res+x)\n        return out\n    \nclass SLE(nn.Module):\n    def __init__(self, in_channel):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=4),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, in_channel, 4, 1, 0)),\n            nn.LeakyReLU(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, in_channel\/\/8, 1, 1, 0)),\n            nn.Sigmoid()\n        )\n\n    def forward(self, high, low):\n        x = self.block(low)\n        return high * x\n\nclass Decoder(nn.Module):\n    def __init__(self, z_dim=128, out_res=256):\n        super().__init__()\n        assert out_res == 256, \"Only Output Resolution of 256x256 Implemented, got {}\".format(out_res)\n        \n        self.res1 = ResBlock(z_dim)\n        \n        self.block1 = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.Conv2d(z_dim, z_dim\/\/2, 3, 1, 1),\n            nn.BatchNorm2d(z_dim\/\/2),\n            nn.ReLU()\n        )\n        \n        self.res2 = ResBlock(z_dim\/\/2)\n        \n        self.block2 = self.make_block(z_dim\/\/2, z_dim\/\/4)\n        \n        self.res3 = ResBlock(z_dim\/\/4)\n        \n        self.block3 = self.make_block(z_dim\/\/4, z_dim\/\/8)\n        \n        self.sle = SLE(z_dim)\n        \n        self.out = nn.Sequential(\n            nn.Conv2d(z_dim\/\/8, 3, 3, 1, 1),\n            nn.Tanh()\n        )\n#         self.skip1 = self.upsample(z_dim, z_dim\/\/2)\n#         self.skip2 = self.upsample(z_dim\/\/2, z_dim\/\/4)\n        \n\n    def make_block(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 3, 1, 1)),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU()\n        )\n        return block\n        \n#     def upsample(self, in_channel, out_channel):\n#         block = nn.Sequential(\n#             nn.Upsample(scale_factor=2, mode='nearest'),\n#             nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 1, 1, 0)),\n#             nn.BatchNorm2d(out_channel),\n#             nn.LeakyReLU(0.1)\n#         )\n#         return block\n    \n    def forward(self, x):      # 128 x 32 x 32\n        h = self.res1(x)\n        x = self.block1(x)\n        x = self.res2(x)\n        x = self.block2(x)\n        x = self.res3(x)\n        x = self.block3(x)\n        x = self.sle(x, h)\n        x = self.out(x)\n        return x\n    \nclass Encoder(nn.Module):\n    def __init__(self, hidden_dim=128, in_res=256):\n        super().__init__()\n        assert in_res == 256, \"Only Output Resolution of 256x256 Implemented, got {}\".format(in_res)\n                \n        self.block1 = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(3, hidden_dim\/\/4, 4, 2, 1)),\n            nn.LeakyReLU(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(hidden_dim\/\/4, hidden_dim\/\/4, 3, 1, 1)),\n            nn.BatchNorm2d(hidden_dim\/\/4),\n            nn.LeakyReLU(0.1)\n        )\n        \n        self.res1 = ResBlock(hidden_dim\/\/4)\n        \n        self.block2 = self.make_block(hidden_dim\/\/4, hidden_dim\/\/2)\n        \n        self.res2 = ResBlock(hidden_dim\/\/2)\n        \n        self.block3 = self.make_block(hidden_dim\/\/2, hidden_dim)\n        \n        self.res3 = ResBlock(hidden_dim)\n        \n#         self.skip1 = self.down_sample(hidden_dim\/\/2, hidden_dim)\n\n\n    def forward(self, x):\n        x = self.block1(x) # 32 x 128 x 128\n        x = self.res1(x)\n        x = self.block2(x) # 64 x 64 x 64\n        x = self.res2(x)\n        x = self.block3(x) # 128 x 32 x 32\n        x = self.res3(x)\n        \n        return x\n\n    def make_block(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 4, 2, 1)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1),\n            nn.utils.spectral_norm(nn.Conv2d(out_channel, out_channel, 3, 1, 1)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1)\n        )\n        return block\n\n    def down_sample(self, in_channel, out_channel):\n        block = nn.Sequential(\n            nn.AvgPool2d(2, 2),\n            nn.utils.spectral_norm(nn.Conv2d(in_channel, out_channel, 1, 1, 0)),\n            nn.BatchNorm2d(out_channel),\n            nn.LeakyReLU(0.1)\n        )\n        return block\n    \nclass AutoEncoder(nn.Module):\n    def __init__(self, res=256, z_dim=128):\n        super().__init__()\n        self.encoder = Encoder(in_res=res, hidden_dim=z_dim) # Naming Mistake.........\n        self.decoder = Decoder(out_res=res, z_dim=z_dim)     # Naming Mistake.........\n        \n    def forward(self, x):\n        x = self.encoder(x) # 64 x 64 x 64\n        x = self.decoder(x) # 3 x 256 x 256\n        return x\n    \n    def encode(self, x):\n        return self.encoder(x)\n    \n    def decode(self, x):\n        return self.decoder(x)","cafd6876":"class ResBlock3d(nn.Module):\n    def __init__(self, channel):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.utils.spectral_norm(nn.Conv3d(channel, channel, 3, 1, 1)),\n            nn.BatchNorm3d(channel),\n            nn.ReLU(),\n            nn.utils.spectral_norm(nn.Conv3d(channel, channel, 3, 1, 1)),\n            nn.BatchNorm3d(channel)\n        )\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        res = x\n        x = self.block(x)\n        out = self.relu(res+x)\n        return out","0e112cd0":"class Discriminator_V(nn.Module):\n    def __init__(self, ndf):\n        super().__init__()\n\n        self.block1 = nn.Sequential(\n            snorm(nn.Conv3d(3, ndf, 3, 1, 0)),\n            nn.BatchNorm3d(ndf),\n            nn.LeakyReLU(0.1)\n        )\n        self.res1 = ResBlock3d(ndf)\n        self.block2 = nn.Sequential(\n            snorm(nn.Conv3d(ndf, 2*ndf, 3, 1, 0)),\n            nn.BatchNorm3d(2*ndf),\n            nn.LeakyReLU(0.1)\n        )\n        self.res2 = ResBlock3d(2*ndf)\n        self.out = nn.Conv3d(2*ndf, 1, 4, 1, 0)\n        \n        \n    def forward(self, x):      # 8 x 3 x 256 x 256\n        x = x.permute(1,0,2,3) # 3 x 8 x 256 x 256\n        x = x.unsqueeze(0)     # 1 x 3 x 8 x 256 x 256\n        x = self.block1(x)\n        x = self.res1(x)\n        x = self.block2(x)\n        x = self.res2(x)\n        x = self.out(x)       # 1 x 1 x 1 x 250 x 250\n        return x","59edbabf":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)","169d5e82":"class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [snorm(nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False))]\n        if normalize:\n            layers.append(nn.InstanceNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            snorm(nn.ConvTranspose2d(in_size, out_size, 4, 2, 1, bias=False)),\n            nn.InstanceNorm2d(out_size),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n\n        return x\n\n\nclass GeneratorUNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3):\n        super(GeneratorUNet, self).__init__()\n\n        self.down1 = UNetDown(in_channels, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 256)\n        self.up4 = UNetUp(512, 128)\n        self.up5 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.Upsample(scale_factor=2),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(128, out_channels, 4, padding=1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        # U-Net generator with skip connections from encoder to decoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        u1 = self.up1(d6, d5)\n        u2 = self.up2(u1, d4)\n        u3 = self.up3(u2, d3)\n        u4 = self.up4(u3, d2)\n        u5 = self.up5(u4, d1)\n\n        return self.final(u5)\n\n\n##############################\n#        Discriminator\n##############################\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3):\n        super(Discriminator, self).__init__()\n\n        def discriminator_block(in_filters, out_filters, normalization=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [snorm(nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1))]\n            if normalization:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(in_channels * 2, 64, normalization=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1, bias=False)\n        )\n\n    def forward(self, img_A, img_B):\n        # Concatenate image and condition image by channels to produce input\n        img_input = torch.cat((img_A, img_B), 1)\n        return self.model(img_input)","d61dee76":"# End-to-End Generator Model\n\nclass GrandGenerator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.autoencoder = AutoEncoder()\n        self.unet = GeneratorUNet()\n    \n    def forward(self, x):\n        z1 = x[0].unsqueeze(0)\n        z2 = x[-1].unsqueeze(0)\n        z1 = self.autoencoder.encode(z1)\n        z2 = self.autoencoder.encode(z2)\n        zs = torch.cat([torch.lerp(z1, z2, v) for v in np.linspace(0, 1, 8)]).cuda()\n        decoded = self.autoencoder.decode(zs)\n        recon = self.autoencoder(x)\n        refined = self.unet(decoded)\n        \n        return decoded, recon, refined","fd604eb4":"G = GrandGenerator()\n# D_V = Discriminator_V(ndf=32)\nD = Discriminator()\nG.apply(weights_init_normal)\nD.apply(weights_init_normal)\n# D_V.to(device)\nD.to(device)\nG.to(device)\n# G.load_state_dict(torch.load(\"..\/input\/project-ae\/model.pt\"))","60f7ae17":"g_optim = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n# dv_optim = optim.Adam(D_V.parameters(), lr=5*lr, betas=(0.5, 0.999))\nd_optim = optim.Adam(D.parameters(), lr=5*lr, betas=(0.5, 0.999))","d5848935":"adv_loss = nn.BCEWithLogitsLoss()\nrecon_loss = nn.L1Loss()\nssim_loss = MS_SSIM(data_range=1, size_average=True)\n\ndv_losses = []\nd_losses = []\ng_losses = []\n\nidx = np.load(\"..\/input\/capture-io-2\/lion-indices.npy\")","0fe6f6d0":"!nvidia-smi","0406275e":"num_iters = 18000\nshow_iters= 300","4650816d":"cur_iter = 0\n\nwhile True:\n    t = 0\n    for data in tqdm(dataloader):\n        t += 1\n        cur_iter += 1\n        if t-1 in idx:\n            continue\n\n        data = data.to(device)\n#         dv_optim.zero_grad()\n\n        decoded, recon, refined = G(data)\n\n#         d_pred_real = D_V(data)\n#         real_labels = torch.ones_like(d_pred_real)\n#         d_pred_fake = D_V(decoded)\n#         fake_labels = torch.zeros_like(d_pred_fake)\n#         d_real_loss = adv_loss(d_pred_real, real_labels)\n#         d_fake_loss = adv_loss(d_pred_fake, fake_labels)\n#         dv_loss = 0.5 * (d_real_loss + d_fake_loss)\n#         dv_losses.append(dv_loss.item())\n#         dv_loss.backward(retain_graph=True)\n        \n#         dv_optim.step()\n\n        d_optim.zero_grad()\n        \n        d_pred_real = D(data, decoded)\n        real_labels = torch.ones_like(d_pred_real)\n        d_pred_fake = D(refined, decoded)\n        fake_labels = torch.zeros_like(d_pred_fake)\n        d_real_loss = adv_loss(d_pred_real, real_labels)\n        d_fake_loss = adv_loss(d_pred_fake, fake_labels)\n        d_loss = 0.5 * (d_real_loss + d_fake_loss)\n        d_losses.append(d_loss.item())\n        d_loss.backward(retain_graph=True)\n        \n        d_optim.step()\n        \n        g_optim.zero_grad()\n        \n#         d_pred_fake_v = D_V(decoded)\n#         real_labels = torch.ones_like(d_pred_fake_v)\n#         g_adv_loss = adv_loss(d_pred_fake_v, real_labels)\n        g_ssim_loss = ssim_loss(decoded, data)\n        g_recon_loss = recon_loss(recon, data)\n        \n        d_pred_fake = D(refined, decoded)\n        real_labels = torch.ones_like(d_pred_real)\n        r_adv_loss = adv_loss(d_pred_fake, real_labels)\n        r_recon_loss = recon_loss(refined, data)\n        \n        g_loss = 0.1*g_ssim_loss + 10*g_recon_loss + 0.5*(r_adv_loss + 100*r_recon_loss)\n        g_losses.append(g_loss.item())\n        g_loss.backward()\n        \n        g_optim.step()\n        \n\n        if (cur_iter) % show_iters == 0:\n            print(\"{} \/ {}, D_loss: {:.4f}, G_loss: {:.4f}, \".format(cur_iter, num_iters, \n                                                                        d_loss.item(), g_loss.item()))\n            \n            output = torch.cat([data.float(), decoded.float(), refined.float()], dim=0)\n            \n            show_tensor_images(output, num_images=24, figsize=12, nrow=8)\n#             show_tensor_images(data.float())\n#             show_tensor_images(decoded.float())\n#             show_tensor_images(refined.float())\n#                 imgs_list.append(model(fixed).detach().cpu())\n\n            torch.save(G.state_dict(), \"G.pt\")\n            torch.save(D.state_dict(), \"D,pt\")\n#             torch.save(D_V.state_dict(), \"D_V.pt\")\n\n\n        del g_loss, d_pred_real, d_pred_fake, recon, refined, decoded\n        torch.cuda.empty_cache()\n        \n        if cur_iter >= num_iters:\n            break\n\n    if cur_iter >= num_iters:\n        break","499e1024":"G.eval()\nR.eval()\n\nimgs = []\n\nidx = np.load(\"..\/input\/capture-io-2\/lion-indices.npy\")\n\ndataloader = DataLoader(dataset, batch_size=8, shuffle=False)\n\nt = 0\nfor data in dataloader:\n    t += 1\n    if t-1 in idx:\n        continue\n    z1 = data[0].unsqueeze(0).cuda()\n    z2 = data[-1].unsqueeze(0).cuda()\n    z1 = G.encode(z1)\n    z2 = G.encode(z2)\n    zs = torch.cat([torch.lerp(z1, z2, v) for v in np.linspace(0.1, 1, 16)]).cuda()\n    zs = G.decode(zs)\n    out = R(zs)\n#     show_tensor_images(model(zs), num_images=9, nrow=9, figsize=16)\n    for i in range(len(out)):\n        imgs.append(out[i].detach().cpu())\n    \n    if t >= 200:\n        break\n        \nlen(imgs)","c92a9a2d":"fig = plt.figure(figsize=(5,5))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose((i+1)\/2,(1,2,0)), animated=True)] for i in imgs]\nani = animation.ArtistAnimation(fig, ims, interval=100, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","ae5123b0":"# imgs = []\n# n_epochs = 5\n# idx = np.load(\"..\/input\/capture-io-2\/lion-indices.npy\")\n# recon_loss = nn.L1Loss()\n\n# t = 0\n# for _ in range(n_epochs):\n#     for data in tqdm(dataloader):\n#         data = data.cuda()\n#         optimizer.zero_grad()\n#         t += 1\n#         if t-1 in idx:\n#             continue\n#         z1 = data[0].unsqueeze(0).cuda()\n#         z2 = data[-1].unsqueeze(0).cuda()\n#         z1 = model.encode(z1)\n#         z2 = model.encode(z2)\n#         zs = torch.cat([torch.lerp(z1, z2, v) for v in np.linspace(0, 1, 8)]).cuda()\n#         out = model.decode(zs)\n#         loss = recon_loss(out, data)\n#         loss.backward()\n#         optimizer.step()\n\n#         if t % 100 == 0:\n#             print(\"Current Iter: {} \/ {}\".format(t, len(dataloader)))\n#             print(\"Current Loss: {:.4f}\".format(loss.item()))\n#             show_tensor_images(out)\n#             torch.save(model, \"model.pt\")\n#             with torch.no_grad():\n#                 sample = torch.cat([torch.lerp(z1, z2, v) for v in np.linspace(0, 1, 16)]).cuda()\n#                 sample_out = model.decode(sample)\n#                 for i in range(len(sample_out)):\n#                     imgs.append(sample_out[i].detach().cpu())","9da46ecf":"# fig = plt.figure(figsize=(5,5))\n# plt.axis(\"off\")\n# ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in imgs]\n# ani = animation.ArtistAnimation(fig, ims, interval=120, repeat_delay=1000, blit=True)\n\n# HTML(ani.to_jshtml())","d5aac4aa":"# Pix2Pix"}}