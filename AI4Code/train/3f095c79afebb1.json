{"cell_type":{"2cca3d2d":"code","f1322e2b":"code","ed5e9521":"code","d1ff3b98":"code","bd72989d":"code","700497c3":"code","0879bbdf":"code","3635bbf5":"code","8f0da45b":"code","b8ec4f11":"code","7805c1be":"code","653472b9":"code","59763281":"code","5d38db7c":"code","64c867a8":"code","85d60929":"code","42055159":"code","de0c36a2":"code","8cfcb133":"code","d8c0d35f":"code","67257cc7":"code","6ce94762":"code","43927d43":"code","a3ad6841":"code","b3c917cf":"code","236283b5":"code","47aa2627":"code","6cd1f734":"code","6fa29868":"code","c2448561":"code","c2e4c7e7":"code","447cc019":"code","2fbb78b5":"code","bd6b885d":"code","1fd3e4de":"code","559397e3":"code","e78ebfc7":"code","7b409b17":"code","f1d47442":"code","50c7768f":"code","c941a7fa":"code","641a5484":"code","f1290e11":"code","c9beccb3":"markdown","aae42c01":"markdown","6d716430":"markdown","477a73d4":"markdown","d60ed967":"markdown","bd792837":"markdown","f365c2c0":"markdown","81769a01":"markdown","5640c6e1":"markdown","1eb11be7":"markdown","d9e6b053":"markdown","6441d82d":"markdown","da2bcff5":"markdown","e32fce86":"markdown"},"source":{"2cca3d2d":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set() \n#import missingno\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-apr-2021\/')\n\n#from matplotlib import pyplot as plt","f1322e2b":"train = pd.read_csv(input_path \/ 'train.csv', index_col='PassengerId')\ndisplay(train.head())","ed5e9521":"train.info()","d1ff3b98":"test = pd.read_csv(input_path \/ 'test.csv', index_col='PassengerId')\ndisplay(test.head())","bd72989d":"submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='PassengerId')\ndisplay(submission.head())","700497c3":"#Split numerical and categorical variables\ndf_num = train[['Age','SibSp', 'Parch', 'Fare']]\ndf_cat = train[['Survived', 'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']]","0879bbdf":"train.isnull().sum().sort_values(ascending=False)","3635bbf5":"test.isnull().sum().sort_values(ascending=False)","8f0da45b":"#histograms of numberical variables\nfig, axes = plt.subplots(1, 4, figsize=(22,6))\nfig.suptitle('Distribution of numerical variables')\n\nsns.histplot(x = train['Age'],kde=True, ax=axes[0])\nsns.histplot(x = train['SibSp'],kde=True, ax=axes[1])\nsns.histplot(x = train['Parch'],kde=True, ax=axes[2])\nsns.histplot(x = train['Fare'],kde=True, ax=axes[3])\n","b8ec4f11":"#Correlations\nprint(df_num.corr())\nsns.heatmap(df_num.corr())","7805c1be":"#Average values for survivors vs deceased\npd.pivot_table(train, index = 'Survived', values = df_num.columns)","653472b9":"sns.displot(data = train, x = 'Age',kde=True, hue = 'Survived', col= 'Pclass')","59763281":"sns.displot(data = train, x = 'Age',kde=True, hue = 'Survived', col= 'Sex')","5d38db7c":"fig, axes = plt.subplots(1, 3, figsize=(22,6))\nfig.suptitle('Distribution of categorical variables')\n#'Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']]\nsns.barplot(x = 'Sex', y = 'Survived', data = train, ax=axes[0])\nsns.barplot(x = 'Pclass', y = 'Survived', data = train, ax=axes[1])\nsns.barplot(x = 'Embarked', y = 'Survived', data = train, ax=axes[2])","64c867a8":"#Embarked by class - greater proportion of 3rd class in Southampton, could be linked with lower surviaval rate\nsns.catplot(x='Pclass', col = 'Embarked', data = train, kind = 'count')","85d60929":"#Cabin letter, n = nan\ntrain['cabin_lett'] = train.Cabin.apply(lambda x: str(x)[0])\ntest['cabin_lett'] = test.Cabin.apply(lambda x: str(x)[0])\n#print(train.cabin_lett.value_counts())\npd.pivot_table(train, index = 'Survived', columns = 'cabin_lett', values = 'Ticket', aggfunc = 'count')\n\n#Cabin number\ntrain['cabin_num'] = train.Cabin.str.extract('(\\d+)',expand=True,)\ntrain.cabin_num = pd.to_numeric(train.cabin_num)\ntest['cabin_num'] = test.Cabin.str.extract('(\\d+)',expand=True,)\ntest.cabin_num = pd.to_numeric(test.cabin_num)","42055159":"sns.barplot(x = 'cabin_lett', y = 'Survived', data = train)","de0c36a2":"#log fare\ntrain['log_Fare'] = np.log(train.Fare +1)\ntest['log_Fare'] = np.log(test.Fare +1)\nsns.displot(data = train, x = 'log_Fare',kde=True, hue = 'Survived')","8cfcb133":"train['family'] = train.SibSp + train.Parch\ntest['family'] = test.SibSp + test.Parch\n#train.head()\nsns.kdeplot(data = train, x = 'family',shade=True, hue = 'Survived')","d8c0d35f":"train['log_Fam'] = np.log(train.family+1)\ntest['log_Fam'] = np.log(test.family+1)\nsns.kdeplot(data = train, x = 'log_Fam',shade=True, hue = 'Survived')","67257cc7":"#Sex and class\ntrain_Pclass_str = train.Pclass.apply(str)\ntrain['Who'] = train.Sex + train_Pclass_str\n\ntest_Pclass_str = test.Pclass.apply(str)\ntest['Who'] = test.Sex + test_Pclass_str","6ce94762":"#Name split\ntrain[['last_name','first_name']] = train.Name.str.split(\", \",expand=True,)\ntest[['last_name','first_name']] = test.Name.str.split(\", \",expand=True,)","43927d43":"#Ticket split\n\ntrain['ticket_num'] = train.Ticket.str.extract('(\\d+)',expand=True,)\ntrain.ticket_num = pd.to_numeric(train.ticket_num)\ntrain['log_ticket_num'] = np.log(train.ticket_num+1)\ntrain['ticket_lett'] = train.Ticket.replace('(\\d)', '', regex=True)\n\ntest['ticket_num'] = test.Ticket.str.extract('(\\d+)',expand=True,)\ntest.ticket_num = pd.to_numeric(test.ticket_num)\ntest['log_ticket_num'] = np.log(test.ticket_num+1)\ntest['ticket_lett'] = test.Ticket.replace('(\\d)', '', regex=True)\n\n#train.ticket_lett.unique()","a3ad6841":"train.info()","b3c917cf":"train.head()","236283b5":"#null values\n#impute nulls - age & fare\ntrain.Embarked = train.Embarked.fillna(value = 'N')\ntrain.Age = train.Age.fillna(train.Age.mean())\n#train.Fare = train.Fare.fillna(train.Fare.median())\ntrain.log_Fare = train.log_Fare.fillna(train.log_Fare.median())\n#train.Cabin = train.Cabin.fillna(0)\ntrain.Ticket = train.Ticket.fillna(0)\ntrain.ticket_lett = train.ticket_lett.fillna('')\ntrain.ticket_num = train.ticket_num.fillna(train.ticket_num.mean())\ntrain.cabin_num = train.cabin_num.fillna(train.cabin_num.mean())\ntrain.log_ticket_num = train.log_ticket_num.fillna(train.log_ticket_num.median())\n\ntest.Embarked = test.Embarked.fillna(value = 'N')\ntest.Age = test.Age.fillna(test.Age.mean())\n#test.Fare = test.Fare.fillna(test.Fare.median())\n#test.Cabin = test.Cabin.fillna(0)\ntest.Ticket = test.Ticket.fillna(0)\ntest.log_Fare = test.log_Fare.fillna(test.log_Fare.median())\ntest.ticket_lett = test.ticket_lett.fillna('')\ntest.ticket_num = test.ticket_num.fillna(test.ticket_num.mean())\ntest.log_ticket_num = test.log_ticket_num.fillna(test.log_ticket_num.median())\ntest.cabin_num = test.cabin_num.fillna(test.cabin_num.mean())","47aa2627":"train.pop('Cabin')\ntrain.pop('Ticket')\ntrain.pop('Sex')\ntrain.pop('Fare')\ntrain.pop('Name')\ntrain.pop('log_ticket_num')","6cd1f734":"test.pop('Cabin')\ntest.pop('Ticket')\ntest.pop('Fare')\ntest.pop('Sex')\ntest.pop('Name')\ntest.pop('log_ticket_num')","6fa29868":"#label encoding catergoricals\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","c2448561":"train.head()","c2e4c7e7":"test.head()","447cc019":"target = train.pop('Survived')\nX_train, X_valid, y_train, y_valid = train_test_split(train, target, train_size=0.75, shuffle=False)","2fbb78b5":"XGBoost","bd6b885d":"#Import libaries, run a grid search to find best paramters for model\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nxgb_model = XGBClassifier(use_label_encoder=False,eval_metric = \"logloss\")\nparams_xgb = [\n    {'n_estimators':[10,100,250,500],\n     'max_depth':[2,4,6,8],\n     'learning_rate':[0.1,0.05,0.01],\n     'min_child_weight':[1,2,4,6,8]}]\n\ngrid_search = GridSearchCV(xgb_model, params_xgb, cv=5, scoring='accuracy', n_jobs=1)\ngrid_search.fit(train,target)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","1fd3e4de":"#gridsearch again but with narrower parameters\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nxgb_model = XGBClassifier(learning_rate=0.05,use_label_encoder=False,eval_metric = \"logloss\")\nparams_xgb = [\n    {'n_estimators':[200,225,250,275,300,325,350,375],\n     'max_depth':[3,4,5],\n     'min_child_weight':[7,8,9,10]}]\n\ngrid_search = GridSearchCV(xgb_model, params_xgb, cv=5, scoring='accuracy', n_jobs=1)\ngrid_search.fit(train,target)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","559397e3":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nlgbm = LGBMClassifier(learning_rate=0.05, num_leaves=35, max_depth=7, n_estimators=160, feature_fraction=0.7, reg_alpha=0.6)\nparams_lgb = [\n    { 'reg_lambda': [0.5,0.6,0.7]}]\n    #reg_alpha = 0.2,\n    #reg_lambda = 0.4)}]\n\ngrid_search = GridSearchCV(lgbm, params_lgb, cv=5, scoring='accuracy', n_jobs=1)\ngrid_search.fit(train,target)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","e78ebfc7":"from catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nclf_model = CatBoostClassifier(iterations =575, depth=6, learning_rate = 0.05, l2_leaf_reg=0.4, eval_metric = \"Logloss\")\nparams_clf = [\n    {'':[100,200,500]}]\n    # 'iterations':[10,100,250,500],\n     #'depth':[2,4,6,8]}]\n\ngrid_search = GridSearchCV(clf_model, params_clf, cv=5, scoring='accuracy', n_jobs=1)\ngrid_search.fit(train,target)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","7b409b17":"from catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nclf_model = CatBoostClassifier(learning_rate = 0.05, iterations=575,depth=6, l2_leaf_reg = 10, border_count = 32, eval_metric = \"Logloss\")\nparams_clf = [\n    {\n     'border_count':[32,5,10,20,50,100,200]}]\n     \n\ngrid_search = GridSearchCV(clf_model, params_clf, cv=5, scoring='accuracy', n_jobs=1)\ngrid_search.fit(train,target)\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)","f1d47442":"from catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nclf = CatBoostClassifier(iterations =575, depth=6, learning_rate = 0.05, l2_leaf_reg=0.4, eval_metric = \"Logloss\")\nclf.fit(X_train, y_train,  \n        eval_set=(X_valid, y_valid), \n        verbose=False\n)\npredictions = clf.predict(X_valid)\nprint(predictions)\nprint(accuracy_score(predictions, y_valid))","50c7768f":"from xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.metrics import accuracy_score\n\nmodel = XGBClassifier(n_estimators=275,learning_rate=0.05,max_depth=4,min_child_weight=8,use_label_encoder=False,eval_metric = \"logloss\")\nmodel.fit(train,target)\npredictions = model.predict(test)\nprint(predictions)\n\nplot_importance(model)","c941a7fa":"clf = CatBoostClassifier(iterations =575, depth=6, learning_rate = 0.05, l2_leaf_reg=0.4, eval_metric = \"Logloss\")\nclf.fit(train,target)\npredictions = clf.predict(test)\nprint(predictions)","641a5484":"lgbm = LGBMClassifier(learning_rate=0.05, num_leaves=35, max_depth=7, n_estimators=160, feature_fraction=0.7, reg_alpha=0.6)\nlgbm.fit(train,target)\npredictions = lgbm.predict(test)\nprint(predictions)","f1290e11":"#Create submission file\nsub = pd.DataFrame({'PassengerId':test.index, 'Survived':predictions})\nsubmission = sub.set_index('PassengerId')\nsubmission.to_csv('cat-a.csv')","c9beccb3":"0.7844300000000001\n{0.05 learning rate, 'depth': 6, 'iterations': 500}","aae42c01":"Numerical variables","6d716430":"# CatBoost","477a73d4":"Categorical variables","d60ed967":"# Model tuning","bd792837":"# Exploratory Analysis","f365c2c0":"# Preprocessing","81769a01":"# Feature Engineering","5640c6e1":"0.7845799999999999\n{'depth': 6, 'iterations': 575","1eb11be7":"# LightGBM ","d9e6b053":"# **Split data into train and validation**","6441d82d":"Here are my final accuracy scores for my predictions made via my three models. \nXGBClassifier:      private score = 0.79520 (best score)\n                    public score  = 0.79724 \nCatBoostClassifier: private score = 0.79362 (top 43%)\n                    public score  = 0.79757\nLGBMClassifier:     private score = 0.79515\n                    public score  = 0.79700\n\nXGBClassifier outperformed the other two models with the private dataset with 79.520% accuracy. The CatBoostClassifier model performed the best in the public score, and was automatically used as the final submission for the Kaggle rankings, placing me in the top 43% of entrants.  ","da2bcff5":"Base model 0.77925 {'learning_rate': 0.05, 'max_depth': 4, 'min_child_weight': 8, 'n_estimators': 250}\nThen start adding in new features from feauture engineering section to find if they improve model, and tune parameters again.","e32fce86":"# Model fitting"}}