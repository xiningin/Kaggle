{"cell_type":{"d024abf6":"code","9e217bde":"code","2d4db1cb":"code","a6b574f2":"code","e2cc8e89":"code","df8de64f":"code","bbe86959":"code","a162cb63":"code","3a2ab42b":"code","9e615d42":"code","8e9edcbb":"code","7ef923d3":"code","3860ee91":"code","5958dd59":"code","765c1463":"code","6ff4e011":"code","687046dd":"code","5451d3f6":"code","25fb42ad":"code","4970a2da":"code","dc6090b7":"code","864b98f7":"code","9c58ffa6":"code","bf61ca55":"code","fd8a5168":"code","bd8068ae":"code","ebaf738c":"markdown","14f5f353":"markdown","85469c73":"markdown","56a94939":"markdown"},"source":{"d024abf6":"import os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom pandasql import sqldf\n\nimport matplotlib.pyplot as plt\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Model, Sequential, model_from_json\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.layers import Input, Dense, Dropout, Flatten, Lambda, Embedding\nfrom keras.initializers import RandomNormal, Constant\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras import regularizers\nfrom keras import backend as K\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport seaborn as sns\nimport warnings\n\nfrom math import sqrt\n\nimport itertools\nfrom tqdm import tqdm\n\nnp.random.seed(42)  # for reproducibility\n\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set(font_scale=1)\n\npd.set_option('display.max_columns', 60)\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","9e217bde":"def concat_data():\n    df_train = pd.read_csv('..\/input\/train.csv')\n    df_test = pd.read_csv('..\/input\/test.csv')\n    df_extra = pd.read_csv('..\/input\/store.csv')\n    df_test['Sales'] = -1\n    df_full = pd.concat([df_train, df_test]).reset_index(drop=True)\n\n    #Merge extra information about stores\n    df_full = df_full.merge(df_extra, left_on=['Store'], right_on=['Store'], how='left')\n    \n    df_full['Year'] = pd.DatetimeIndex(df_full['Date']).year\n    df_full['Month'] = pd.DatetimeIndex(df_full['Date']).month\n    df_full['Day'] = pd.DatetimeIndex(df_full['Date']).day\n    df_full['WeekOfYear'] = pd.DatetimeIndex(df_full['Date']).weekofyear\n    \n    # Calculate competition open in months\n    df_full['CompetitionOpen'] = 12 * (df_full.Year - df_full.CompetitionOpenSinceYear) + \\\n        (df_full.Month - df_full.CompetitionOpenSinceMonth)\n\n    # Calculate promo open time in months\n    df_full['PromoOpen'] = 12 * (df_full.Year - df_full.Promo2SinceYear) + \\\n        (df_full.WeekOfYear - df_full.Promo2SinceWeek) \/ 4.0\n    df_full['PromoOpen'] = df_full.PromoOpen.apply(lambda x: x if x > 0 else 0)\n    df_full.loc[df_full.Promo2SinceYear == 0, 'PromoOpen'] = 0\n\n    # Transform month interval in a boolean column \n    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',\n                 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n    df_full['monthStr'] = df_full.Month.map(month2str)\n    df_full.loc[df_full.PromoInterval == 0, 'PromoInterval'] = ''\n    df_full['IsPromoMonth'] = 0\n    for interval in df_full.PromoInterval.unique():\n        interval = str(interval)\n        if interval != '':\n            for month in interval.split(','):\n                df_full.loc[(df_full.monthStr == month) & (df_full.PromoInterval == interval), 'IsPromoMonth'] = 1\n\n\n    return df_full\n\ndf_full = concat_data()","2d4db1cb":"def extrat_test_data(df_full):\n    df_train = df_full.loc[df_full['Sales'] != -1]\n    df_test = df_full.loc[df_full['Sales'] == -1]\n\n    return df_train, df_test\n\ndf_train, df_test = extrat_test_data(df_full)","a6b574f2":"df_full.head()","e2cc8e89":"df_full.info()","df8de64f":"# Function to calculate missing values by column (By DSA)\ndef missing_values_table(df):\n    # Total missing values\n    mis_val = df.isnull().sum()\n\n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n\n    # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(\n    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n\n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[\n        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n    '% of Total Values', ascending=False).round(1)\n\n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n          \" columns that have missing values.\")\n\n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns\n\nmissing_values_table(df_full)","bbe86959":"df_full.groupby('StoreType')['Sales'].describe()","a162cb63":"df_full.groupby('StoreType')['Customers', 'Sales'].sum()","3a2ab42b":"# Plotting correlations\nnum_feat=df_full.columns[df_full.dtypes!=object]\nnum_feat=num_feat[1:-1] \nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(df_full[col].values, df_full['Sales'].values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(10,15))\nrects = ax.barh(ind, np.array(values), color='red')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t Sales\")","9e615d42":"# Heatmap of correlations features\ncorrMatrix=df_full[[\"Sales\", \"DayOfWeek\", \"Open\", \"Promo\", \"SchoolHoliday\", \"CompetitionDistance\",\n                    \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Promo2\",\n                    \"Promo2SinceWeek\", \"Promo2SinceYear\", \"Year\", \"Month\", \"Day\",\n                    \"CompetitionOpen\", \"PromoOpen\", \"IsPromoMonth\", \"Store\"]].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(30, 30))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features')","8e9edcbb":"def clean_data(use_text_columns = True):\n    '''\n    Function that clean data and create a new features to enrich the model\n    '''\n    cols_num = [\"Sales\", \"DayOfWeek\", \"Open\", \"Promo\", \"SchoolHoliday\", \"CompetitionDistance\",\n                \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Promo2\",\n                \"Promo2SinceWeek\", \"Promo2SinceYear\", \"Wapp\", \"Avg_Customers\", \"Year\", \"Month\", \"Day\",\n                \"CompetitionOpen\", \"PromoOpen\", \"IsPromoMonth\", \"Store\"]\n\n    cols_text = [\"StateHoliday\", \"StoreType\", \"Assortment\"]\n\n    df_train = pd.read_csv('..\/input\/train.csv')    \n    len_train_data = len(df_train)\n\n    df_test = pd.read_csv('..\/input\/test.csv')\n\n    # Setting null values of column Open in test dataset\n    df_test.loc[df_test['DayOfWeek'] != 7, 'Open'] = 1\n    df_test.loc[df_test['DayOfWeek'] == 7, 'Open'] = 0\n\n    avg_customer = sqldf(\n      \"\"\"\n      SELECT\n      Store,\n      DayOfWeek,\n      sum(case when Customers is not null then Sales\/Customers else 0 end) as Wapp,\n      round(avg(Customers)) Avg_Customers\n      from df_train\n      group by Store,DayOfWeek\n      \"\"\"\n    )\n    \n    df_test = sqldf(\n      \"\"\"\n      SELECT\n      t.*,\n      ac.Wapp,\n      ac.Avg_Customers\n      from df_test t\n      left join avg_customer ac on t.Store = ac.Store and t.DayOfWeek = ac.DayOfWeek\n      \"\"\"\n    )\n    \n    df_train = sqldf(\n      \"\"\"\n      SELECT\n      t.*,\n      ac.Wapp,\n      ac.Avg_Customers\n      from df_train t\n      left join avg_customer ac on t.Store = ac.Store and t.DayOfWeek = ac.DayOfWeek\n      \"\"\"\n    )\n\n    # Merge train and test dataset\n    all_data = pd.concat([df_train, df_test], ignore_index=True)\n\n    df_extra = pd.read_csv('..\/input\/store.csv')\n    df_full = pd.concat([df_train, df_test]).reset_index(drop=True)\n\n    # Merge extra information about stores\n    all_data = df_full.merge(df_extra, left_on=['Store'], right_on=['Store'], how='left')\n\n    # Separate date in Year, Month and Day\n    all_data.loc[all_data['StateHoliday'] == 0, 'StateHoliday'] = 'd'\n    all_data['Year'] = pd.DatetimeIndex(all_data['Date']).year\n    all_data['Month'] = pd.DatetimeIndex(all_data['Date']).month\n    all_data['Day'] = pd.DatetimeIndex(all_data['Date']).day\n    all_data['WeekOfYear'] = pd.DatetimeIndex(all_data['Date']).weekofyear\n\n    # Calculate competition open in months\n    all_data['CompetitionOpen'] = 12 * (all_data.Year - all_data.CompetitionOpenSinceYear) + \\\n        (all_data.Month - all_data.CompetitionOpenSinceMonth)\n\n    # Calculate promo open time in months\n    all_data['PromoOpen'] = 12 * (all_data.Year - all_data.Promo2SinceYear) + \\\n        (all_data.WeekOfYear - all_data.Promo2SinceWeek) \/ 4.0\n    all_data['PromoOpen'] = all_data.PromoOpen.apply(lambda x: x if x > 0 else 0)\n    all_data.loc[all_data.Promo2SinceYear == 0, 'PromoOpen'] = 0\n    \n    # Transform month interval in a boolean column \n    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun',\n                 7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n    all_data['monthStr'] = all_data.Month.map(month2str)\n    all_data.loc[all_data.PromoInterval == 0, 'PromoInterval'] = ''\n    all_data['IsPromoMonth'] = 0\n    for interval in all_data.PromoInterval.unique():\n        interval = str(interval)\n        if interval != '':\n            for month in interval.split(','):\n                all_data.loc[(all_data.monthStr == month) & (all_data.PromoInterval == interval), 'IsPromoMonth'] = 1\n\n    data_numeric = all_data[cols_num]\n    \n    # Fill NAN values\n    # Only column CompetitionDistance is fill NaN with a median value\n    data_numeric['CompetitionDistance'].fillna(data_numeric['CompetitionDistance'].median(), inplace = True)\n\n    # Other values is fill with zero\n    data_numeric.fillna(0, inplace = True)\n\n    if (use_text_columns):\n        data_text = all_data[cols_text]\n        data_text = pd.get_dummies(data_text, dummy_na=False)\n\n        complete_data = pd.concat([data_numeric, data_text], axis = 1)\n\n        df_train = complete_data.iloc[:len_train_data,:]\n        df_test = complete_data.iloc[len_train_data:,:]\n    else:\n        df_train = data_numeric.iloc[:len_train_data,:]\n        df_test = data_numeric.iloc[len_train_data:,:]\n\n    return df_train, df_test","7ef923d3":"def load_train_data(scaler_x, scaler_y):\n    '''\n    Transform train data set and separate a test dataset to validate the model in the end of training and normalize data\n    '''\n    X_train = train.drop([\"Sales\"], axis=1) # Features\n    y_train = np.array(train[\"Sales\"]).reshape((len(X_train), 1)) # Targets\n    X_train = scaler_x.fit_transform(X_train)\n    y_train = scaler_y.fit_transform(y_train)\n\n    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n\n    return (X_train, y_train), (X_test, y_test)","3860ee91":"def load_test_data():\n    '''\n    Remove column of predictions and normalize data of submission test data set.\n    '''\n    X_test = test.drop([\"Sales\"], axis=1) # Features\n    X_test = StandardScaler().fit_transform(X_test)\n\n    return X_test","5958dd59":"# Show info of model\ndef show_info(model, X, y, log, weights = None):\n    '''\n    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n    '''\n    if (log != None):\n        # summarize history for loss\n        plt.figure(figsize=(14,10))\n        plt.plot(log.history['loss'])\n        plt.plot(log.history['val_loss'])\n        plt.title('Model Loss')\n        plt.ylabel('loss')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n        print('\\n')\n        \n        # summarize history for rmse\n        plt.figure(figsize=(14,10))\n        plt.plot(log.history['rmse'])\n        plt.plot(log.history['val_rmse'])\n        plt.title('Model RMSE')\n        plt.ylabel('rmse')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n        print('\\n')\n        \n        # summarize history for rmspe\n        plt.figure(figsize=(14,10))\n        plt.plot(log.history['rmspe'])\n        plt.plot(log.history['val_rmspe'])\n        plt.title('Model RMSPE')\n        plt.ylabel('rmspe')\n        plt.xlabel('epoch')\n        plt.legend(['train', 'test'], loc='upper left')\n        plt.show()\n\n    if (weights != None):\n        model.load_weights(weights)\n\n    predictions = model.predict(X, verbose=1)\n\n    mse = mean_squared_error(y, predictions)\n    rmse = sqrt(mse)\n    rmspe = rmspe_val(y, predictions)\n\n    print('MSE: %.3f' % mse)\n    print('RMSE: %.3f' % rmse)\n    print('RMSPE: %.3f' % rmspe)","765c1463":"def rmspe_val(y_true, y_pred):\n    '''\n    RMSPE calculus to validate evaluation metric about the model\n    '''\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true), axis=0))[0]","6ff4e011":"def rmspe(y_true, y_pred):\n    '''\n    RMSPE calculus to use during training phase\n    '''\n    return K.sqrt(K.mean(K.square((y_true - y_pred) \/ y_true), axis=-1))","687046dd":"def rmse(y_true, y_pred):\n    '''\n    RMSE calculus to use during training phase\n    '''\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","5451d3f6":"def create_model():\n    '''\n    Create a neural network\n    '''\n    initializer = RandomNormal(mean=0.0, stddev=0.05, seed=None)\n\n    model = Sequential()\n    model.add(Dense(512, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer=initializer))\n    model.add(Dropout(0.4))\n    model.add(Dense(512, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer=initializer))\n    model.add(Dropout(0.4))\n    model.add(Dense(512, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer=initializer))\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation=\"linear\", kernel_initializer=initializer))\n    adam = Adam(lr=1e-3, decay=1e-3)\n\n    # Compile model\n    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[rmse, rmspe])\n\n    return model","25fb42ad":"train, test = clean_data(use_text_columns = True)","4970a2da":"train.head()","dc6090b7":"# Hyperparameters and load data to train the model\nbatch_size = 512\nnb_epoch = 300\n\nscaler_x = StandardScaler()\nscaler_y = StandardScaler()\n\nprint('Loading data...')\n(X_train, y_train), (X_test, y_test) = load_train_data(scaler_x, scaler_y)\n\nprint('Build model...')\nmodel = create_model()\nmodel.summary()","864b98f7":"print('Fit model...')\nfilepath=\"weights_rossmann.best.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\ncallbacks_list = [checkpoint, early_stopping]\n\nlog = model.fit(X_train, y_train,\n          validation_split=0.20, batch_size=batch_size, epochs=nb_epoch, shuffle=True, callbacks=callbacks_list)","9c58ffa6":"show_info(model, X_test, y_test, log, weights='weights_rossmann.best.hdf5')","bf61ca55":"test_data = load_test_data()\n\ndf_teste = pd.read_csv('..\/input\/test.csv')","fd8a5168":"predict = model.predict(test_data)\npredict = scaler_y.inverse_transform(predict)","bd8068ae":"submission = pd.DataFrame()\nsubmission['Id'] = df_teste[\"Id\"]\nsubmission['Sales'] = predict\n\nsubmission.to_csv('submission.csv', index=False)","ebaf738c":"**Missing Values**","14f5f353":"## Analisys Exploratory","85469c73":"## Functions to create and train the model","56a94939":"## RMSPE Formula\n$\\textrm{RMSPE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left(\\frac{y_i - \\hat{y}_i}{y_i}\\right)^2}$"}}