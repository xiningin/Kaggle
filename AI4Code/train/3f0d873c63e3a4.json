{"cell_type":{"76d4aa23":"code","2da6c63b":"code","1db340f2":"code","921d3e39":"code","97fa82d5":"code","173fc432":"code","71a536be":"code","a305a9b6":"code","6caa7323":"code","73024fbe":"code","7c3c2117":"code","36a0f7be":"code","36f0ad37":"code","6b3f523a":"code","4e3b57b3":"code","b8084590":"code","e87f8f94":"code","c59b9e5b":"code","065b1a2c":"code","0a0d0562":"code","5f6b4452":"code","4e311a2c":"code","b6b86e21":"markdown","05529f6e":"markdown","92b52d95":"markdown","190afad0":"markdown","49a5594e":"markdown","6e3e36c7":"markdown","7672c855":"markdown","1ef6a624":"markdown","69cf1226":"markdown","04ecea31":"markdown","9bed14ce":"markdown","9f1ff4aa":"markdown"},"source":{"76d4aa23":"import numpy as np\nimport torch, os\nimport random\n# fix seed for reproducible results\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    #torch.use_deterministic_algorithms(True)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nset_seed(42)","2da6c63b":"SIZE_X = (572, 572) # size of input images\nSIZE_Y = (388, 388) # size of input segmented images\n\nBATCH_SIZE = 8 # batch size\n\n\nTRAIN_SHARE = 100 # size of train set\nVAL_SHARE = 50# size of val set\nTEST_SHARE = 50# size of test  set\n\nMAX_EPOCHS = 150 # number of epochs \nLEARNING_RATE = 5e-3 # learning rate\n\nSCHEDULER_STEP = 50 # scheduler step\nSCHEDULER_GAMMA = 0.1\n","1db340f2":"images = []\nlesions = []\nfrom skimage.io import imread\nimport os\nroot = '\/kaggle\/input\/ph2databaseaddi\/PH2Dataset'\n\nfor root, dirs, files in os.walk(os.path.join(root, 'PH2 Dataset images')):\n    if root.endswith('_Dermoscopic_Image'):\n        images.append(imread(os.path.join(root, files[0])))\n    if root.endswith('_lesion'):\n        lesions.append(imread(os.path.join(root, files[0])))","921d3e39":"# resize images as required by UNet architecture, resize() automatically normalizes to (0,1)\n# X = image to be segemnted\n# Y = segmented image\n\nfrom skimage.transform import resize\nsize_X = SIZE_X\nsize_Y = SIZE_Y\n\nimport cv2 \nX = [cv2.resize(x, size_X, interpolation=cv2.INTER_NEAREST) for x in images] \nY = [cv2.resize(y, size_Y, interpolation=cv2.INTER_NEAREST)>0.5 for y in lesions] \nX = X \/ np.max(X)\nY = Y \/ np.max(Y)\n\n\n# convert to float32\nimport numpy as np\nX = np.array(X, np.float32)\nY = np.array(Y, np.float32)\n\n\nprint(f'Loaded {len(X)} images')","97fa82d5":"# generate len(X) random indices\n# from len(X) as it were np.arange(len(X))\n# False is to generate without replacement (no repetitions)\n\nix = np.random.choice(len(X), len(X), False)\n#\n# split generated indices to train-val-test sets as following: 100 test-50 val-50 test\n# [100, 150] entries indicate where along axis the ix array is split. \ntr, val, ts = np.split(ix, [TRAIN_SHARE, TRAIN_SHARE+VAL_SHARE])\nassert (len(tr), len(val), len(ts))==(TRAIN_SHARE, VAL_SHARE, TEST_SHARE)","173fc432":"# load data using dataloader\nfrom torch.utils.data import DataLoader\n\n# set the batch size\nbatch_size = BATCH_SIZE\n\n# set the dataloaders\n# set drop_last to skip the batches with the # elements < batch size\ndata_tr = DataLoader(list(zip(np.rollaxis(X[tr], 3, 1), Y[tr, np.newaxis])), \n                     batch_size=batch_size, shuffle=True, drop_last=True)\ndata_val = DataLoader(list(zip(np.rollaxis(X[val], 3, 1), Y[val, np.newaxis])),\n                      batch_size=batch_size, shuffle=True, drop_last=True)\ndata_ts = DataLoader(list(zip(np.rollaxis(X[ts], 3, 1), Y[ts, np.newaxis])),\n                     batch_size=batch_size, shuffle=True, drop_last=True)","71a536be":"# use cuda if available\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","a305a9b6":"import torch\nfrom torchvision import transforms\n\nclass AugmentImageAndMask(torch.utils.data.Dataset):\n    \"\"\"\n    class to augment images for image segmentation problem\n    applies the same transformations to both the input image and its segmented counterpart (mask)\n    ---\n    input requirements\n    ---\n    all images are rectangular, i.e. have the same size in both dimensions\n    all mask images have lower linear number of pixels (as expected by Unet)\n    only even linear number of pixels are accepted\n    ---\n    return\n    ---\n    tuple of (augmented original image, its augmented mask)\n    i.e. ((augmented image [0], augmented mask [0]), \n          (augmented image [1], augmented mask [1]),...\n          )\n    \n    \"\"\"\n    def __init__(self, tensors, transform = None):\n        # check that all tensors have the same size as the first one\n        assert (tensor.size(0) == tensors[0].size(0) for tensor in tensors)\n        assert (tensor.size(1) == tensors[0].size(1) for tensor in tensors)\n        self.tensors = tensors\n        self.transform = transform\n        \n    def __getitem__(self,index):\n        #\n        # get images and masks one by one\n        x = self.tensors[0][index]  \n        y = self.tensors[1][index]\n        #\n        # assert both images are rectangular\n        assert x.shape[-1]==x.shape[-2], \"Dimensions of input images are assumed to be equal. Shape of a current input image is ({},{})\".format(x.shape[-2],x.shape[-1])\n        assert y.shape[-1]==y.shape[-2], \"Dimensions of input masks are assumed to be equal. Shape of a current input mask is ({},{})\".format(y.shape[-2],y.shape[-1])\n        #\n        # zero-pad mask images to the shape of input images:\n        #\n        # set padding's width and height\n        # for the -2 and -1 dimensions (width and height of individual images)\n        n_pad_12 = int(np.subtract(x.shape[-1], y.shape[-1])\/\/2)\n        # for -3 dimension (number of channels)\n        # if the input images are RGB:\n        if len(x)==3 and x.shape[0]==3:\n            n_pad_3 = 1\n        # if the input images are grayscale\n        elif x.shape[0]==1:\n            n_pad_3 = 0\n        else:\n            raise ValueError(\"Input images must be either RGB or gray. Expected input dimensions are [3,x,x] for RGB or [1,x,x] or [x,x] for grayscale\")\n        # zero-pad\n        y_pad = torch.from_numpy(np.pad(y, ((n_pad_3,n_pad_3), (n_pad_12, n_pad_12), (n_pad_12, n_pad_12)), 'constant', constant_values=0))\n        #\n        # concatenate to apply the same transform to both image(x) and its mask(y)\n        xy = torch.cat((x,y_pad), dim=0)\n        #\n        # transform\n        if self.transform:\n            xy = self.transform(xy)\n            #\n            # split back into image(x) and mask(y)\n            x,y_pad_aug = torch.chunk(xy, chunks=2, dim=0)\n            #\n            # unpad trasformed mask back to its original size  \n            # if RGB input images \n            if len(x)==3 and x.shape[0]==3:\n                # if images and masks have different sizes\n                if n_pad_12 != 0:\n                    y = y_pad_aug[1, n_pad_12:-n_pad_12, n_pad_12:-n_pad_12]\n                # if images and masks have the same sizes\n                else: \n                    y = y_pad_aug[1, :, :]\n            # if grayscale input images \n            elif x.shape[0]==1:\n                # if images and masks have different sizes\n                if n_pad_12 != 0:\n                    y = y_pad_aug[:, n_pad_12:-n_pad_12, n_pad_12:-n_pad_12]\n                # if images and masks have the same sizes\n                else: \n                    y = y_pad_aug[:, :, :]\n            # add dimension 1 to augmented and unpadded mask images\n            y = y[np.newaxis, : , : ]\n        # return augmented images and masks\n        return x,y\n\n\ndef random_transforms(prob = 0.5):\n    \"\"\"\n    Defines transforms for image augmentation\n    ---\n    Parameters\n    ---\n    prob: float between 0 and 1\n        probability of applying random transformations (the same for all transformations)\n        Default: 0.5\n    ---\n    Return\n    ---\n    transform:\n        an instance of Compose class\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(p=prob),\n        transforms.RandomVerticalFlip(p=prob),\n        ])\n    \n    return transform\n\ndef get_augmented_tensors(*augmented_xy_pairs):\n    \"\"\"\n    unzips (reorders) the ouput of AugmentImageAndMask class augmented_xy_pairs\n    so that the augmented images and augmented masks are in two separate tensors\n    ---\n    input\n    ---\n    augmented pairs of (image,mask) tensors \n    ---\n    return\n    ---\n    tuple of augmented images, tuple of augmented masks\n    each tuple element is a tensor\n    the length of each tuple corresponds to the length of the 0th dimension \n    of the images' and masks' tensors parsed into AugmentImageAndMask class\n    \"\"\"\n    #\n    # note:\n    # the star in *augmented_xy_pairs is needed to unpack the list so that all elements of it can be passed as different parameters.\n    augmented_xy = list(zip(*augmented_xy_pairs))\n    return augmented_xy[0], augmented_xy[1]\n\n\n\n\n# Example: \n\n\nimport numpy as np\nx = torch.rand(2, 3, 4, 4)\ny = torch.rand(2, 1, 2, 2)\nprint(\"x \", x)\nprint(\" \")\nprint(\"y \", y)\n\naugmented_xy_pairs = AugmentImageAndMask(tensors=(x,y), transform=random_transforms(prob = 0.5))\n#\n# note:  the star * in *augmented_xy_pairs is needed to unpack the tuple of (x,y) tuples \n# into individual (x,y) tuples \naugmented_x, augmented_y = get_augmented_tensors(*augmented_xy_pairs)\nX_batch = torch.stack(augmented_x, dim=0)\nY_batch = torch.stack(augmented_y, dim=0)\nprint(\" \")\nprint(\"x augm \", X_batch.shape)\nprint(\" \")\nprint(\"y augm \", Y_batch.shape)","6caa7323":"import matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\n\n# plot some augmented images \nfor X,Y in data_tr:\n    augmented_xy_pairs = AugmentImageAndMask(tensors=(X,Y), transform=random_transforms(prob = 0.5))\n    #\n    # note:  the star * in *augmented_xy_pairs is needed to unpack the tuple of (x,y) tuples \n    # into individual (x,y) tuples \n    augmented_x, augmented_y = get_augmented_tensors(*augmented_xy_pairs)\n    X_batch = torch.stack(augmented_x, dim=0)\n    Y_batch = torch.stack(augmented_y, dim=0)\n    n_images = X_batch.shape[0]\n\n    plt.figure(figsize=(18, 6))\n    for i in range(n_images):\n        plt.subplot(2, n_images, i+1)\n        plt.axis(\"off\")\n        plt.imshow(np.rollaxis(X_batch[i].numpy(), 0, 3), cmap='gray')\n        #print(X_batch[i].shape)\n\n        plt.subplot(2, n_images, i+n_images+1)\n        plt.axis(\"off\")\n        plt.imshow(Y_batch[i, 0], cmap='gray')\n        #print(Y_batch[i].shape)\n    plt.show();\n    # plot only the 1st batch\n    break","73024fbe":"# install torchvision\n!pip install torchvision","7c3c2117":"import torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport torch.optim as optim\nfrom time import time\n\nfrom matplotlib import rcParams","36a0f7be":"class UNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # encoder (downsampling)\n        # Each enc_conv\/dec_conv block should look like this:\n        # nn.Sequential(\n        #     nn.Conv2d(...),\n        #     ... (2 or 3 conv layers with relu and batchnorm),\n        # )\n        ##################\n        # encoder layer 0 \n        #################\n        # 3, 572, 572\n        self.e0_conv = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3),\n            # 64, 570, 570\n            nn.Conv2d(64, 64, kernel_size=3),\n            # 64, 568, 568\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n            )\n        # 64, 568, 568\n        self.e0_pool =   nn.MaxPool2d(2, stride=2, return_indices=False) \n        # 64, 284, 284 \n        #\n        ##################\n        # encoder layer 0 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n        #################\n        self.e0_crop = nn.Sequential(\n            # 64, 568, 568\n            torchvision.transforms.CenterCrop(392)\n            # 64, 392,392\n            )\n        self.e0_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n        # 64, 196,196\n        #\n        #################\n        # encoder layer1\n        ################\n        self.e1_conv = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3),\n            # 128, 282, 282\n            nn.Conv2d(128, 128, kernel_size=3),\n            # 128, 280, 280\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n            )\n        # 128, 280, 280\n        self.e1_pool =   nn.MaxPool2d(2, stride=2, return_indices=False)\n        # 128, 140, 140\n        #\n        ###################\n        # encoder layer 1 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n        ##################\n        self.e1_crop = nn.Sequential(\n            # 128, 280, 280\n            torchvision.transforms.CenterCrop(200)\n            # 128, 200, 200\n            )\n        self.e1_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n        # 128, 100, 100\n        #\n        ###################\n        # encoder layer 2\n        ###################\n        self.e2_conv = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3),\n            # 256, 138, 138\n            nn.Conv2d(256, 256, kernel_size=3),\n            # 256, 136, 136\n            nn.BatchNorm2d(256),\n            nn.ReLU()\n            )\n        # 256, 136, 136\n        self.e2_pool =  nn.MaxPool2d(2, stride=2, return_indices=False)\n        # 256, 68, 68\n        #\n        #################\n        # encoder layer 2 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n        #################\n        self.e2_crop = nn.Sequential(\n            # 256, 136, 136\n            torchvision.transforms.CenterCrop(104)\n            # 256, 104, 104\n            )\n        self.e2_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n        # 256, 52, 52\n        #\n        ##################\n        # encoder layer 3\n        #################\n        self.e3_conv = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3),\n            # 512, 66, 66\n            nn.Conv2d(512, 512, kernel_size=3),\n            # 512, 64, 64\n            nn.BatchNorm2d(512),\n            nn.ReLU()\n            )\n        self.e3_pool =  nn.MaxPool2d(2, stride=2, return_indices=False)\n        # 512, 32, 32\n        #\n        #################\n        # encoder layer 3 - cropping for decoder layer and generating a fake maxpool image tensor to get indices for maxunpool\n        #################\n        self.e3_crop = nn.Sequential(\n            # 512, 64, 64\n            torchvision.transforms.CenterCrop(56)\n            # 512, 56, 56\n            )\n        self.e3_pool_idx =  nn.MaxPool2d(2, stride=2, return_indices=True)\n        # 512, 28, 28\n        #\n        ###\n        # bottleneck\n        ###\n        # 512, 32, 32\n        self.bottleneck_conv = nn.Sequential(\n            nn.Conv2d(512, 1024, kernel_size=3),\n            # 1024, 30, 30\n            nn.Conv2d(1024, 1024, kernel_size=3),\n            # 1024, 28, 28\n            nn.BatchNorm2d(1024),\n            nn.ReLU()\n            )\n            # 1024, 28, 28\n\n        # decoder (upsampling)\n        ###################\n        # decoder layer 3\n        ###################\n        # 1024, 28, 28--> 255         \n        self.d3_upsample = nn.MaxUnpool2d(kernel_size=2, stride=2)\n        #self.d3_upsample =  nn.Upsample(scale_factor=2)\n        # 1024, 56, 56\n        self.d3_upconv = nn.Sequential(\n          # H_out=stride*(H_in\u22121)\u22122\u00d7padding+kernel_size+output_padding \n          # 56 = 1*(56-1)-2*1+2+1 \n          #nn.ConvTranspose2d(1024, 512, kernel_size=2, padding=1, output_padding = 1),\n          # 56 = 1*(56-1)-2*1+3\n          nn.ConvTranspose2d(1024, 512, kernel_size=3, padding=1),\n          # 512, 56, 56\n          nn.BatchNorm2d(512),\n          nn.ReLU()\n          # 512, 56, 56\n          # 1024, 56, 56 after concatenation w\/ corresponding cropped encoder map\n          )\n        # 1024, 56, 56 \n        #\n        self.d3_conv = nn.Sequential(\n            nn.Conv2d(1024, 512, kernel_size=3),\n            ## 56- 3 + 1\n            # 512, 54 ,54\n            nn.Conv2d(512, 512, kernel_size=3),\n            # 512, 52, 52\n            nn.BatchNorm2d(512),\n            nn.ReLU()\n            # 512, 52, 52\n            )\n        #\n        ##################\n        # decoder layer 2 \n        ##################\n        # 512, 52, 52\n        self.d2_upsample =  nn.MaxUnpool2d(kernel_size=2, stride=2)\n        #self.d2_upsample =  nn.Upsample(scale_factor=2)\n        # 512, 104, 104\n        self.d2_upconv = nn.Sequential(\n        #nn.ConvTranspose2d(512, 256, kernel_size=2, padding=1, output_padding = 1),\n        nn.ConvTranspose2d(512, 256, kernel_size=3, padding=1),\n        # 256, 104, 104\n        nn.BatchNorm2d(256),\n        nn.ReLU()\n        # 256, 104, 104\n        # 512, 104, 104 after concatenation w\/ corresponding cropped encoder map\n        )\n        #\n        self.d2_conv = nn.Sequential(\n            nn.Conv2d(512, 256, kernel_size=3),\n            # 256, 102 ,102\n            nn.Conv2d(256, 256, kernel_size=3),\n            # 256, 100, 100\n            nn.BatchNorm2d(256),\n            nn.ReLU()\n            # 256, 100, 100\n            )\n        ##################\n        # decoder layer 1\n        ##################\n        # 256, 100, 100\n        self.d1_upsample =   nn.MaxUnpool2d(kernel_size=2, stride=2)\n        #self.d1_upsample =  nn.Upsample(scale_factor=2)\n        # 256, 200, 200\n        self.d1_upconv = nn.Sequential(\n            #nn.ConvTranspose2d(256, 128, kernel_size=2, padding=1, output_padding = 1),\n            nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1),\n            # 128, 200, 200\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n            # 128, 200, 200\n            # 256, 200, 200 after concatenation w\/ corresponding cropped encoder map\n            )\n        #\n        self.d1_conv = nn.Sequential(\n            # 256, 200, 200\n            nn.Conv2d(256, 128, kernel_size=3),\n            # 128, 198, 198\n            nn.Conv2d(128, 128, kernel_size=3),\n            # 128, 196, 196\n            nn.BatchNorm2d(128),\n            nn.ReLU()\n            # 128, 196, 196\n            )\n        ###\n        # decoder layer 0\n        ###\n        # 128, 196, 196\n        self.d0_upsample =   nn.MaxUnpool2d(kernel_size=2, stride=2)\n        #self.d0_upsample =  nn.Upsample(scale_factor=2)\n        # 128, 392, 392\n        self.d0_upconv = nn.Sequential(\n          #nn.ConvTranspose2d(128, 64, kernel_size=2, padding=1, output_padding = 1),\n          nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1),\n          # 64, 392, 392\n          nn.BatchNorm2d(64),\n          nn.ReLU()\n          # 64, 392, 392\n          # 128, 392, 392 after concatenation w\/ corresponding cropped encoder map\n          )\n        #\n        self.d0_conv = nn.Sequential(\n            # 128, 392, 392\n            nn.Conv2d(128, 64, kernel_size=3),\n            # 64, 390, 390\n            nn.Conv2d(64, 64, kernel_size=3),\n            # 64, 388, 388\n            nn.Conv2d(64, 1, kernel_size=1),\n            # 1, 388, 388\n            nn.BatchNorm2d(1),\n            # 1, 388, 388\n            )\n\n    def forward(self, x):\n        # encoder\n        ###################\n        # encoder layer 0\n        ##################\n        #\n        # convolutions\n        #--> # 1, 572, 572\n        e0 = self.e0_conv(x)\n        # --> # 64, 568, 568 \n        assert (e0.shape[1],e0.shape[2],e0.shape[3])==(64, 568, 568), \"encoder layer e0 expected shape {}, got{}\".format(\"(64, 568, 568)\",(e0.shape[1],e0.shape[2],e0.shape[3]))\n        #\n        # pooling\n        e0_pool = self.e0_pool(e0)\n        # --> # 64, 284, 284 \n        assert (e0_pool.shape[1],e0_pool.shape[2],e0_pool.shape[3])==(64, 284, 284), \"encoder layer e0 after pooling expected shape{}\".format(\"(64, 284, 284)\")\n        #\n        # cropping and cropped indices for skip connections\n        e0_crop = self.e0_crop(e0)\n        # --> # 64, 392,392\n        _, idx0_crop = self.e0_pool_idx(e0_crop) # --> pooling indices from 64, 392,392 cropped map \n        # --> # 64, 196,196\n        assert (e0_crop.shape[1],e0_crop.shape[2],e0_crop.shape[3])==(64, 392, 392), \"encoder layer e0 after cropping expected shape{}\".format(\"(64, 392, 392)\")\n        assert (idx0_crop.shape[1],idx0_crop.shape[2],idx0_crop.shape[3])==(64, 196, 196), \"encoder indices idx0 after cropping expected shape {}, got {}\".format(\"(64, 196, 196)\",(idx0_crop.shape[1],idx0_crop.shape[2],idx0_crop.shape[3]))\n        #\n        ##################\n        # encoder layer 1\n        ##################\n        #  convolutions\n        # --> # 64, 284, 284\n        e1 = self.e1_conv(e0_pool)\n        # --> # 128, 280, 280\n        assert (e1.shape[1],e1.shape[2],e1.shape[3])==(128, 280, 280), \"encoder layer e1 expected shape {}\".format(\"(128, 280, 280)\")\n        #\n        # pooling\n        e1_pool = self.e1_pool(e1)\n        # --> # 128, 140, 140\n        assert (e1_pool.shape[1],e1_pool.shape[2],e1_pool.shape[3])==(128, 140, 140), \"encoder layer e1 after pooling expected shape{}\".format(\"(128, 140, 140)\")\n        #\n        # cropping and cropped indices for skip connections\n        e1_crop = self.e1_crop(e1)\n        # --> # 128, 200, 200\n        _, idx1_crop = self.e1_pool_idx(e1_crop) # --> pooling indices from 128, 200, 200 cropped map\n        # --> # 128, 100, 100\n        assert (e1_crop.shape[1],e1_crop.shape[2],e1_crop.shape[3])==(128, 200, 200), \"encoder layer e1 after cropping expected shape{}\".format(\"(128, 200, 200)\")\n        assert (idx1_crop.shape[1],idx1_crop.shape[2],idx1_crop.shape[3])==(128, 100, 100), \"encoder indices idx1 after cropping expected shape{}\".format(\"(128, 100, 100)\")\n        #\n        ##################\n        # encoder layer 2\n        ##################\n        #  convolutions\n        # --> # 128, 140, 140\n        e2 = self.e2_conv(e1_pool)\n        # --> # 256, 136, 136\n        assert (e2.shape[1],e2.shape[2],e2.shape[3])==(256, 136, 136), \"encoder layer e2 expected shape {}\".format(\"(256, 136, 136)\")\n        #\n        # pooling\n        e2_pool = self.e2_pool(e2) \n        # --> # 256, 68, 68\n        assert (e2_pool.shape[1],e2_pool.shape[2],e2_pool.shape[3])==(256, 68, 68), \"encoder layer e2 after pooling expected shape{}\".format(\"(256, 68, 68)\")\n        #\n        # cropping and cropped indices for skip connections\n        e2_crop = self.e2_crop(e2) \n        # --> # 256, 104, 104\n        _, idx2_crop = self.e2_pool_idx(e2_crop) # --> pooling indices from 256, 104, 104 cropped map\n        # --> # 256, 52, 52\n        assert (e2_crop.shape[1],e2_crop.shape[2],e2_crop.shape[3])==(256, 104, 104), \"encoder layer e2 after cropping expected shape{}\".format(\"(256, 104, 104)\")\n        assert (idx2_crop.shape[1],idx2_crop.shape[2],idx2_crop.shape[3])==(256, 52, 52), \"encoder indices idx2 after cropping expected shape{}\".format(\"(256, 52, 52)\")\n        #\n        ##################\n        # encoder layer 3\n        ##################\n        #  convolutions\n        # --> # 256, 68, 68\n        e3 = self.e3_conv(e2_pool)\n        # --> # 512, 64, 64\n        assert (e3.shape[1],e3.shape[2],e3.shape[3])==(512, 64, 64), \"encoder layer e3 expected shape {}\".format(\"(512, 64, 64)\")\n        #\n        # pooling\n        e3_pool = self.e3_pool(e3) \n        # --> # 512, 32, 32\n        assert (e3_pool.shape[1],e3_pool.shape[2],e3_pool.shape[3])==(512, 32, 32), \"encoder layer e3 after pooling expected shape{}\".format(\"(512, 32, 32)\")\n        #\n        # cropping and cropped indices for skip connections\n        e3_crop = self.e3_crop(e3) \n        # --> # 512, 56, 56\n        _, idx3_crop = self.e3_pool_idx(e3_crop) # --> pooling indices from 512, 56, 56 cropped map\n        # --> # 512, 28, 28\n        assert (e3_crop.shape[1],e3_crop.shape[2],e3_crop.shape[3])==(512, 56, 56), \"encoder layer e3 after cropping expected shape{}\".format(\"(512, 56, 56)\")\n        assert (idx3_crop.shape[1],idx3_crop.shape[2],idx3_crop.shape[3])==(512, 28, 28), \"encoder indices idx3 after cropping expected shape{}\".format(\"(512, 28, 28)\")\n        #\n        # bottleneck\n        # --> # 512, 32, 32\n        b = self.bottleneck_conv(e3_pool) \n        # --> # 1024, 28, 28\n        assert (b.shape[1],b.shape[2],b.shape[3])==(1024, 28, 28), \"bottleneck expected shape{}\".format(\"(1024, 28, 28)\")\n        #\n        # decoder\n        ##################\n        # decoder layer 3 (reverse counting order)\n        ##################\n        #\n        # upconvolution\n        d3_upconv = self.d3_upconv(b) \n        # --> # 512, 28, 28\n        assert (d3_upconv.shape[1],d3_upconv.shape[2],d3_upconv.shape[3])==(512, 28, 28), \"decoder layer d3 after upconvolution expected shape{}\".format(\"(512, 28, 28)\")\n        #\n        # upsampling  idx3 - 512, 28, 28\n        # --> # 512, 28, 28 \n        d3_upsample = self.d3_upsample(d3_upconv,idx3_crop) \n        #d3_upsample = self.d3_upsample(b) \n        # --> # 512, 56, 56     \n        assert (d3_upsample.shape[1],d3_upsample.shape[2],d3_upsample.shape[3])==(512, 56, 56), \"decoder layer d3 after upsampling expected shape{}\".format(\"(512, 56, 56)\")\n        #\n        # concatenation\n        d3_concat = torch.cat((e3_crop,d3_upsample),dim=1) \n        # -->  512,56,56 + 512,56,56 = 1024,56,56\n        assert (d3_concat.shape[1],d3_concat.shape[2],d3_concat.shape[3])==(1024,56,56), \"decoder layer d3 after concatenation expected shape{}\".format(\"(1024,56,56)\")\n        #\n        # convolution\n        d3 = self.d3_conv(d3_concat) \n        # -->    # 512, 52, 52 \n        assert (d3.shape[1],d3.shape[2],d3.shape[3])==(512, 52, 52), \"decoder layer d3 final expected shape{}\".format(\"(512, 52, 52)\")\n        #\n        ##################\n        # decoder layer 2\n        ##################\n        #\n        # upconvolution\n        d2_upconv = self.d2_upconv(d3) \n        # --> # 256, 52, 52\n        assert (d2_upconv.shape[1],d2_upconv.shape[2],d2_upconv.shape[3])==(256, 52, 52), \"decoder layer d2 after upconvolution expected shape{}\".format(\"(256, 52, 52)\")\n        #\n        # upsampling - idx2 - # 256, 52, 52\n        d2_upsample = self.d2_upsample(d2_upconv,idx2_crop) \n        #d2_upsample = self.d2_upsample(d3) \n        # --> 256, 104, 104\n        assert (d2_upsample.shape[1],d2_upsample.shape[2],d2_upsample.shape[3])==(256, 104, 104), \"decoder layer d2 after upsampling expected shape{}\".format(\"(256, 104, 104)\")\n        #\n        # concatenation\n        d2_concat = torch.cat((e2_crop,d2_upsample),dim=1) \n        # -->  256, 104, 104 + 256, 104, 104 = 512, 104, 104\n        assert (d2_concat.shape[1],d2_concat.shape[2],d2_concat.shape[3])==(512, 104, 104), \"decoder layer d2 after concatenation expected shape{}\".format(\"(512, 104, 104)\")\n        #\n        # convolution\n        d2 = self.d2_conv(d2_concat)   \n        # 256, 100, 100 \n        assert (d2.shape[1],d2.shape[2],d2.shape[3])==(256, 100, 100 ), \"decoder layer d2 final expected shape{}\".format(\"(256, 100, 100 )\")\n        #\n        ##################\n        # decoder layer 1\n        ##################\n        #\n        # upconvolution\n        d1_upconv = self.d1_upconv(d2) \n        # --> # 128, 100, 100\n        assert (d1_upconv.shape[1],d1_upconv.shape[2],d1_upconv.shape[3])==(128, 100, 100), \"decoder layer d1 after upconvolution expected shape{}\".format(\"(128,100,100)\")\n        #\n        # upsampling\n        d1_upsample = self.d1_upsample(d1_upconv,idx1_crop) \n        #d1_upsample = self.d1_upsample(d2) \n        # --> 128, 200, 200\n        assert (d1_upsample.shape[1],d1_upsample.shape[2],d1_upsample.shape[3])==(128, 200, 200), \"decoder layer d1 after upsampling expected shape{}\".format(\"(128, 200, 200)\")\n        #\n        # concatenation\n        d1_concat = torch.cat((e1_crop,d1_upsample),dim=1) \n        # -->  128, 200, 200 + 128, 200, 200 = 256, 200, 200\n        assert (d1_concat.shape[1],d1_concat.shape[2],d1_concat.shape[3])==(256, 200, 200), \"decoder layer d1 after concatenation expected shape{}\".format(\"(256, 200, 200)\")\n        #\n        # convolution\n        d1 = self.d1_conv(d1_concat)   \n        # -->    # 128, 196, 196 \n        assert (d1.shape[1],d1.shape[2],d1.shape[3])==(128, 196, 196), \"decoder layer d1 final expected shape{}\".format(\"(128, 196, 196)\")\n        #\n        ##################\n        # decoder layer 0\n        ##################\n        #\n        # upconvolution\n        d0_upconv = self.d0_upconv(d1) \n        # --> # 64, 196, 196\n        assert (d0_upconv.shape[1],d0_upconv.shape[2],d0_upconv.shape[3])==(64, 196, 196), \"decoder layer d0 after upconvolution expected shape{}\".format(\"(64, 196, 196)\")\n        #\n        # upsampling\n        d0_upsample = self.d0_upsample(d0_upconv,idx0_crop) \n        #d0_upsample = self.d0_upsample(d1) \n        # --> 64, 392, 392\n        assert (d0_upsample.shape[1],d0_upsample.shape[2],d0_upsample.shape[3])==(64, 392, 392), \"decoder layer d0 after upsampling expected shape{}\".format(\"(64, 392, 392)\")\n#\n        # concatenation\n        d0_concat = torch.cat((e0_crop,d0_upsample),dim=1) \n        # -->  64, 392, 392 + 64, 392, 392 = 128, 392, 392\n        assert (d0_concat.shape[1],d0_concat.shape[2],d0_concat.shape[3])==(128, 392, 392), \"decoder layer d0 after concatenation expected shape{}\".format(\"(128, 392, 392)\")\n        #\n        # convolution\n        d0 = self.d0_conv(d0_concat) \n        # -->    # 1,388,388\n        assert (d0.shape[1],d0.shape[2],d0.shape[3])==(1,388,388), \"decoder layer d0 final expected shape{}\".format(\"(1,388,388)\")\n        \n        # return d0 output\n        return d0","36f0ad37":"def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n    \"\"\"\n    IOU metrics taken from \n    https:\/\/www.kaggle.com\/iezepov\/fast-iou-scoring-metric-in-pytorch-and-numpy\n    \"\"\"\n    # You can comment out this line if you are passing tensors of equal shape\n    # But if you are passing output from UNet or something it will most probably\n    # be with the BATCH x 1 x H x W shape\n    outputs = outputs.squeeze(1).byte()  # BATCH x 1 x H x W => BATCH x H x W\n    labels = labels.squeeze(1).byte()\n    SMOOTH = 1e-8\n    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n    \n    iou = (intersection + SMOOTH) \/ (union + SMOOTH)  # We smooth our devision to avoid 0\/0\n    \n    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() \/ 10  # This is equal to comparing with thresolds\n    \n    return thresholded  # ","6b3f523a":"bce_loss = nn.BCEWithLogitsLoss()","4e3b57b3":"def train(model, opt, loss_fn, epochs, batch_size, data_tr, data_val, metric, lr_sched=None):\n    \"\"\"\n    - trains the model\n    - computes training loss, validation loss and validation score\n    - plots the progress\n    ---\n    input\n    ---\n    model: \n        pytorch  model to train\n    opt: \n        optimizer\n    loss_fn : \n        loss function\n    epochs: int\n        number of epochs\n    data_tr: dataloader\n        training data\n    data_val: dataloader\n        validation data\n    metric: \n        segmentation metric\n    lr_sched: \n        scheduler\n        default: None\n    ---\n    return\n    ---\n    training loss, validation loss and validation score\n    \"\"\"\n    # loss and score declaration\n    train_loss_values = []\n    val_loss_values = []\n    avg_score_values = []\n    #\n    # iterate through epochs\n    for epoch in range(epochs):\n        #\n        # epochs counter\n        ii = 0\n        print('* Epoch %d\/%d' % (epoch+1, epochs))\n        # \n        # average loss declaration\n        avg_loss = 0\n        val_avg_loss = 0\n        #\n        ##############\n        # train model \n        ##############\n        #\n        model.train() \n        #\n\n        for X_batch, Y_batch in data_tr:\n   \n            # augment images\n            augmented_xy_pairs = AugmentImageAndMask(tensors=(X_batch, Y_batch), transform=random_transforms(prob = 0.5))\n            #\n            # note:  the star * in *augmented_xy_pairs is needed to unpack the tuple of (x,y) tuples \n            # into individual (x,y) tuples \n            augmented_x, augmented_y = get_augmented_tensors(*augmented_xy_pairs)\n            X_batch = torch.stack(augmented_x, dim=0)\n            Y_batch = torch.stack(augmented_y, dim=0)\n            #print(\"augmented_x \", X_batch.shape) \n            #print(\"augmented_y \", Y_batch.shape) \n\n            #print(\"batch \", ii, \" out of \", len(data_tr) )\n            # epochs counter\n            ii+=1\n            #print(\"X_batch.shape from data_tr\",X_batch.shape)\n            #print(\"Y_batch.shape from data_tr\",Y_batch.shape)\n          #\n            # data to device\n            X_batch = X_batch.to(device)\n            #print(\"X_batch.shape to device\",X_batch.shape)\n            Y_batch = Y_batch.to(device)\n            #print(\"Y_batch.shape to device\",Y_batch.shape)\n            #\n            # set parameter gradients to zero\n            opt.zero_grad()\n            #\n            # forward propagation\n            #\n            # get logits\n            Y_pred = model(X_batch)\n            #print(\"Y_pred.shape\",Y_pred.shape)\n            #\n            # compute train loss\n            loss =  loss_fn(Y_pred,Y_batch) # forward-pass - BCEWithLogitsLoss (pred,prob)\n            #\n            # backward-pass\n            #\n            loss.backward()  \n            # update weights\n            opt.step()  \n            #\n            # calculate loss to show the user\n            avg_loss += loss \n        avg_loss = avg_loss \/ len(data_tr)\n        #\n        print('loss: %f' % avg_loss)\n        # append train loss\n        train_loss_values.append(avg_loss.detach().cpu().numpy())\n\n        #\n        # validate model\n        #\n        with torch.no_grad():\n            # \n            # set dropout and batch normalization layers to evaluation mode before running inference\n            model.eval()  \n            score = 0\n            avg_score = 0\n          #\n            for X_val, Y_val in data_val:\n                # get logits for val set\n                Y_hat =  model(X_val.to(device)).detach().to('cpu')\n                #\n                # only for plotting purposes: apply sigmoid and round to the nearest integer (0,1)\n                # to obtain binary image\n                Y_hat_2plot = torch.round(torch.sigmoid(Y_hat))\n                #Y_hat = torch.round(torch.sigmoid(Y_hat))\n\n                #print(\"Y hat shape\", Y_hat.shape)\n                #\n                # compute val loss and append it\n                val_loss =  loss_fn(Y_hat, Y_val)\n                val_avg_loss += val_loss \n                #\n                # compute score for the current batch\n                #score += metric(Y_hat_2plot.to(device), Y_val.to(device)).mean().item()\n                # temporarily replace by metric().mean without .item() because this leads to float has no attribute detach error \n                # see https:\/\/github.com\/horovod\/horovod\/issues\/852\n                score += metric(Y_hat_2plot.to(device), Y_val.to(device)).mean()\n            #\n            # compute and append average val loss at current epoch\n            val_avg_loss = val_avg_loss \/ len(data_val)\n            val_loss_values.append(val_avg_loss.detach().cpu().numpy())\n            #\n            # compute and append average score at current epoch\n            avg_score = score\/len(data_val)\n            avg_score_values.append(avg_score.detach().cpu().numpy())\n\n\n\n        clear_output(wait=True)\n        \n        # plotting\n        num_images_to_plot = 5 * (batch_size > 5) + batch_size * (batch_size <= 5)\n        rcParams['figure.figsize'] = (2*num_images_to_plot,2*4)\n        #\n        for k in range(num_images_to_plot):\n            # subplot (height, width, absolute image position)\n            plt.subplot(4, num_images_to_plot, k+1)\n            plt.imshow(np.rollaxis(X_val[k].numpy(), 0, 3), cmap='gray')\n            plt.title('Input image')\n            plt.axis('off')\n\n\n            plt.subplot(4, num_images_to_plot, k+num_images_to_plot+1)\n            plt.imshow(Y_hat[k, 0], cmap='gray')\n            plt.title('Output')\n            plt.axis('off')\n\n\n            plt.subplot(4, num_images_to_plot, k+num_images_to_plot*2+1)\n            plt.imshow(Y_hat_2plot[k, 0], cmap='gray')\n            plt.title('Binary output')\n            plt.axis('off')\n\n\n            plt.subplot(4, num_images_to_plot, k+num_images_to_plot*3+1)\n            plt.imshow(Y_val[k, 0], cmap='gray')\n            plt.title('Ground truth')\n            plt.axis('off')\n            \n            plt.tight_layout()\n\n        plt.suptitle('%d \/ %d - train loss: %f' % (epoch+1, epochs, avg_loss))\n        plt.suptitle('%d \/ %d - val. loss: %f' % (epoch+1, epochs, val_avg_loss))\n        plt.show()\n\n\n        # CHANGES HERE\n        # make a scheduler step if required\n        if lr_sched != None:\n            lr_sched.step()\n        # CHANGES END\n\n    plt.plot(train_loss_values)\n    plt.plot(val_loss_values)\n    plt.plot(avg_score_values)\n    plt.legend([\"train_loss\", \"val_loss\", \"val_score\"], loc =\"lower right\")\n    plt.show\n\n    return train_loss_values, val_loss_values, avg_score_values","b8084590":"def score_model(model, metric, data):\n    \"\"\"\n    computes model's score using provided metric and data\n    ---\n    return\n    ---\n    scores\/len(data): float\n        model's score\n    \"\"\"\n    # set dropout and batch normalization layers to evaluation mode before running inference\n    model.eval()\n    scores = 0\n    # iterate thru data\n    for X_batch, Y_label in data:\n      # no gradient for validation\n        with torch.no_grad():\n          #\n          # predict\n            Y_pred = model(X_batch.to(device))\n          #\n          # compute sigmoid and round to the nearest integer (0,1) \n          # to be able to compare with the binary ground truth images\n            Y_pred = torch.round(torch.sigmoid(Y_pred))\n            \n        scores += metric(Y_pred, Y_label.to(device)).mean().item()\n\n    return scores\/len(data)","e87f8f94":"# send model to device \nunet_model = UNet().to(device)\n\n# define \nmax_epochs = MAX_EPOCHS\nbatch_size = BATCH_SIZE\nbce_loss = nn.BCEWithLogitsLoss()\nunet_optimizer = optim.AdamW(unet_model.parameters(), lr=LEARNING_RATE)\n# scheduler\nscheduler = optim.lr_scheduler.StepLR(optimizer=unet_optimizer, step_size=SCHEDULER_STEP, gamma=SCHEDULER_GAMMA)","c59b9e5b":"train_loss_values, val_loss_values, avg_score_values = train(unet_model, unet_optimizer, bce_loss, max_epochs, batch_size, data_tr, data_val, iou_pytorch, lr_sched=scheduler)","065b1a2c":"score_model(unet_model, iou_pytorch, data_val)","0a0d0562":"import pandas as pd\nfrom IPython.display import FileLink\n\n# map individual array elements to floats and then the map to a list\ntrain_loss_values_2save = (list(map(float,train_loss_values)))\nval_loss_values_2save = (list(map(float,val_loss_values)))\navg_score_values_2save = (list(map(float,avg_score_values)))\n\n# save loss and score as csv\npd.DataFrame(train_loss_values_2save).to_csv('train_loss_values.csv', index = False)\nFileLink(r'train_loss_values.csv')\npd.DataFrame(val_loss_values_2save).to_csv('val_loss_values.csv', index = False)\nFileLink(r'val_loss_values.csv')\npd.DataFrame(avg_score_values_2save).to_csv('avg_score_values.csv', index = False)\nFileLink(r'avg_score_values.csv')\n\n\n\n\n# save model\ntorch.save(unet_model.state_dict(), 'my-unet-model.pt')\nFileLink(r'my-unet-model.pt')","5f6b4452":"states = {\n        'number of epochs': MAX_EPOCHS,\n        'state_dict': unet_model.state_dict(),\n        'optimizer': unet_optimizer.state_dict()\n         }\ntorch.save(states, 'my-unet-model-states.pt')\nFileLink(r'my-unet-model-states.pt')","4e311a2c":"import pandas as pd\nfrom matplotlib import pyplot as plt\ntrain_loss_values = pd.read_csv(\"..\/input\/16-hw-semantic-segmentation-unet\/train_loss_values.csv\")\nval_loss_values = pd.read_csv(\"..\/input\/16-hw-semantic-segmentation-unet\/val_loss_values.csv\")\navg_score_values = pd.read_csv(\"..\/input\/16-hw-semantic-segmentation-unet\/avg_score_values.csv\")\n\nfirst_epoch = 1\nplt.plot(train_loss_values[first_epoch:])\nplt.plot(val_loss_values[first_epoch:])\nplt.plot(avg_score_values[first_epoch:])\nplt.legend([\"train_loss\", \"val_loss\", \"avg_score\"], loc =\"lower right\")\nplt.show","b6b86e21":"## Setup training pipeline and train the model","05529f6e":"## Implement UNet model","92b52d95":"## Augment images and their masks","190afad0":"## Save loss, score and model","49a5594e":"## Split into train-val-test","6e3e36c7":"## Read  and resize images\n> Read images and lesions (segmented images) from the root","7672c855":"## Plot resulting train-val loss and score","1ef6a624":"> Resize images to have the same size as the net expects\n\n> Use **nearest neighboour interpolation**. This is important because using any other interpolation \"may result in tampering with the ground truth labels\" [ https:\/\/ai.stackexchange.com\/questions\/6274\/how-can-i-deal-with-images-of-variable-dimensions-when-doing-image-segmentation ]","69cf1226":"## Fix seed for reproducibility and import packages\n* #torch.use_deterministic_algorithms(True) yields error in kaggle, no error in colab","04ecea31":"## Define segmentation metric and loss","9bed14ce":"# Semantic segmentation of moles' images by UNet\n\n* This is a pytorch implementation of the UNet network for semantic segmentation of moles' images\n* Dataset: PD2 ADDI database https:\/\/www.fc.up.pt\/addi\/ph2%20database.html \n* Homework of Unit 16, Deep learning course (part 1, spring 2021) offered by Moscow Institute of Physics and Technology https:\/\/stepik.org\/course\/91157\/info","9f1ff4aa":"## Set parameters"}}