{"cell_type":{"988610b9":"code","fbb7c9e0":"code","7790c85c":"code","3a7d15e5":"code","4cffca78":"code","cea8c9fd":"code","52b9c030":"code","0da138c8":"code","28aee487":"code","f9a6fc15":"code","36bacc94":"code","46c917b9":"code","7fdbb46a":"code","dfacbbde":"code","15d1b8f8":"code","87563765":"code","99a318d8":"code","9942fd7b":"code","623d9baa":"code","a2fcd602":"code","a7e616e5":"code","bf5e3a08":"code","c80813a0":"code","6eaa1790":"code","b9392c73":"code","bda30a5e":"code","e5b5cc05":"code","d5aec82a":"code","0beb2d35":"code","fd244301":"code","eacead27":"code","a8ce3dfb":"markdown","7c05e07e":"markdown","4f9a5fc3":"markdown","6e0753f1":"markdown","75b78ce9":"markdown","efd18dc4":"markdown","e0b6b983":"markdown","68c78239":"markdown","2f033b98":"markdown","52dcc176":"markdown","078d0f07":"markdown","cfc55ece":"markdown","3a68bfcb":"markdown","0084bab8":"markdown","fb47f380":"markdown","98229e80":"markdown","f9540af7":"markdown","140e4920":"markdown","88705073":"markdown","4c64ad5d":"markdown","90c00040":"markdown","ba3ece9d":"markdown","a42d7ee8":"markdown","4b79d67b":"markdown"},"source":{"988610b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbb7c9e0":"# Plotting Libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\n%matplotlib inline\n\n# Metrics for Classification technique\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# Scaler\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\n\n# Cross Validation\n\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split\n\n# Keras\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.activations import relu, sigmoid\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# others\n\n!pip install -U dtale","7790c85c":"# Importing Data\n\ndf = pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\")\ndata = df.copy()\ndf.head(6) # Mention no of rows to be displayed from the top in the argument","3a7d15e5":"df.info()","4cffca78":"df.describe().transpose()","cea8c9fd":"import dtale\nimport dtale.app as dtale_app\n\ndtale_app.USE_NGROK = True\nd = dtale.show(df)\nd.main_url()","52b9c030":"# Correlation between the features.\n\nplt.figure(figsize=(20,12))\nsns.set_context('notebook',font_scale = 1.3)\nsns.heatmap(df.corr(),annot=True,cmap='coolwarm',linewidth = 4,linecolor='black')\nplt.tight_layout()","0da138c8":"X = df.iloc[:, 3:13]\ny = df.iloc[:, 13]","28aee487":"#Create dummy variables\ngeography=pd.get_dummies(X[\"Geography\"],drop_first=True)\ngender=pd.get_dummies(X['Gender'],drop_first=True)\n","f9a6fc15":"## Concatenate the Data Frames\n\nX=pd.concat([X,geography,gender],axis=1)","36bacc94":"## Drop Unnecessary columns\nX=X.drop(['Geography','Gender'],axis=1)","46c917b9":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","7fdbb46a":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","dfacbbde":"# Initialising the ANN\nclassifier = Sequential()","15d1b8f8":"# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 15, kernel_initializer = 'he_uniform',activation='relu',input_dim = 11))\n\n# Adding the Dropout layer\nclassifier.add(Dropout(0.3))","87563765":"# Adding the second hidden layer\nclassifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))\n\n# Adding the Dropout layer\nclassifier.add(Dropout(0.2))","99a318d8":"# Adding the output layer\n\nclassifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))","9942fd7b":"# Compiling the ANN\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","623d9baa":"# summary of model\n\nclassifier.summary()","a2fcd602":"# Fitting the ANN to the Training set\n\nclassifier.fit(X_train, y_train,validation_split=0.33, batch_size = 10, epochs = 100)","a7e616e5":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","bf5e3a08":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))","c80813a0":"# Accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred,y_test))","6eaa1790":"# Function\n\ndef create_model(layers, activation):\n    model = Sequential()\n    for i, nodes in enumerate(layers):\n        if i==0:\n            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n        else:\n            model.add(Dense(nodes))\n            model.add(Activation(activation))\n            model.add(Dropout(0.3))\n            \n    model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid')) # Note: no activation beyond this point\n    \n    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n    return model\n    \nmodel = KerasClassifier(build_fn=create_model, verbose=0)","b9392c73":"layers = [(20,),(10,),(30,),(15,10),(30,15),(30,20),(25,10), (40, 20), (45, 30, 15),(50,20,10)]\nactivations = ['sigmoid', 'relu']\nparams = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[100])\n","bda30a5e":"grid = RandomizedSearchCV(estimator=model, param_distributions =params,cv=5,n_iter=20,verbose=3)","e5b5cc05":"grid.fit(X_train,y_train)","d5aec82a":"grid.best_params_","0beb2d35":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","fd244301":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(y_test, y_pred))","eacead27":"# Accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_pred,y_test))","a8ce3dfb":"**Now we will use One-hot Encoding for categorical features.**","7c05e07e":"*  As we are having binary classification problem, we will be using sigmoid function as activation function in our last layer.\n* Here we have glorot_uniform technique as weight initialization technique.","4f9a5fc3":"# Churn Modelling Prediction With ANN\n![image.png](attachment:image.png)\n\n* Lot of work is done on Modelling using ANN with hyperparameter optimization.\n* I have achieved accuracy of 86% using ANN.\n* If you find my work interesting, do upvote it.\n\n## Problem Statement\n\nThe dataset is the details of the customers in a company. The column are about it's estimated salary, age, sex, etc. Aiming to provide all details about an employee. We have to predict whether customer will exit the bank in future or not.\n\n## Features\n\n* CustomerId\n* Surname\n* CreditScore\n* Geography(Country)\n* Gender\n* Age\n* Tenure\n* Balance\n* NumOfProducts\n* HasCrCard\n* IsActiveMember\n* EstimatedSalary\n* Exited(Dependent Feature)\n\n## Introduction\n\nThis notebook serves as tutorial for beginners to give them intuition about the practical implementation of artificial neural network. I have eplained about how to select best hyperparameters with the help of RandomizedSearchCV and KerasClassifier. I have also used D-Tale to perform EDA. I suggest you to use D-Tale only if you are familiar with basics about EDA.","6e0753f1":"### Hyperparameter Optimization","75b78ce9":"**Click on the above link which opens another tab. Now you can analyze dataset without writing codes and you can also export code for the plots.**","efd18dc4":"## Dropout Layer\n\nThis layer is introduced to implement the concept of Inverted Dropout which is a regularization technique to prevent high variance. Accoding to the value of keep_prob(Research Paper Term) the layer randomly select neuron and deactivate them. While working for test set we multiply the weight with this keep_prob value. \n![image.png](attachment:image.png)","e0b6b983":"**After applying Hyperparameter optimization, the accuracy is somewhat same.**","68c78239":"# EDA\n\nA brief work is done on EDA. Our main focus will be on implementation of ANN.","2f033b98":"## Weight Initialization\n\n* It is important to initialize wight correctly as it prevents exploding gradient problem.\n* Do not initialize same value to each weight.\n* There must be variance in weight initialization.\n* In case of **ReLu** activation function, we prefer **he initialization** method.\n* In case of **sigmoid** activation function, we prefer **glorot technique**.\n\n### he_uniform\n\nDraws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 \/ fan_in) (fan_in is the number of input units in the weight tensor).\n\n### he_normal\n\nIt draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ fan_in) where fan_in is the number of input units in the weight tensor.\n\n### glorot_uniform\n\nDraws samples from a uniform distribution within [-limit, limit], where limit = sqrt(6 \/ (fan_in+fan_out)) (fan_in is the number of input units in the weight tensor).\n\n### glorot_normal\n\nIt draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ (fan_in+fan_out)) where fan_in is the number of input units in the weight tensor.","52dcc176":"### Leaky ReLu Function\n![image.png](attachment:image.png)","078d0f07":"### ReLu Function\n![image.png](attachment:image.png)\n\n* While performing backpropagation, if derivative gets zero then old weight and new weight will be same.\n* To overcome above problem we use Leaky ReLu activation function.","cfc55ece":"## Activation Function\n\n### Sigmoid Function\n![image.png](attachment:image.png)\n\n* The derivative of sigmoid function ranges from 0 to 0.25 which is root cause of vanishing gradient problem.\n* Due to the above reason we prefer ReLu in hidden layers.\n\n","3a68bfcb":"**We will use D-Tale to perform EDA.**","0084bab8":"**There are two type of feature scaling - **\n\n* Standarization - StandardScaler()\n* Normalization - MinMaxScaler()\n\n**We will use standarization for our case.**","fb47f380":"# THANK YOU!!\n![image.png](attachment:image.png)","98229e80":"* Adam optimizer is used to reduce the cost function to its lowest. It has advantages of both RMSProp and Adagrad optimizer method.\n![image.png](attachment:image.png)\n\n* 'binary_crossentropy' is used as loss as it is binary classification problem.","f9540af7":"## Importing Necessary Libraries","140e4920":"**We have achieved accuracy of 86.7% without Hyperparameter Optimization.**","88705073":"**Woah!! we need not to worry about missing values as there are no missing values.**","4c64ad5d":"* input_dim will be used only for first layer as it will take input 11 features of our dataset.\n* No of layers =6\n* Activation function = ReLu\n* Whenever we use reLu as activation function, we use 'he_uniform' for our weight initialization technique.","90c00040":"# Modelling [ANN with Hyperparameter Optimization]","ba3ece9d":"# Feature Engineering","a42d7ee8":"## Data Loading\n\nOur first step is to extract train and test data. We will be extracting data using pandas function read_csv. Specify the location to the dataset and import them.","4b79d67b":"**This is default first cell in any kaggle kernel. They import NumPy and Pandas libraries and it also lists the available Kernel files. NumPy is the fundamental package for scientific computing with Python. Pandas is the most popular python library that is used for data analysis.**"}}