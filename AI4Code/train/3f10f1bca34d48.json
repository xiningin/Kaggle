{"cell_type":{"40ca82e6":"code","58a7241f":"code","76694601":"code","ce4ebc45":"code","0cf1f90b":"code","6e89da58":"code","32453907":"code","39d87051":"code","ebf8a11f":"code","99ac3218":"code","c07e0832":"code","4789aed1":"code","28bc6424":"code","ea0a17bd":"code","c9b04d37":"code","803d38c9":"code","a2f9fca0":"code","0893d220":"code","287c0c13":"code","2a9c90d0":"code","1379d828":"code","a4a46d21":"code","5ef81ebe":"code","6ae34316":"code","0f65f25a":"code","3dbb9a2b":"code","edb46fc7":"code","c9d1c0f2":"code","829de782":"code","53aca0f0":"code","e5e530e5":"code","95d87f8a":"code","063d9ec3":"code","a6a57380":"code","0723b071":"code","e258f433":"code","d33e4443":"code","9aa6b5df":"code","c1166b99":"code","e6a09312":"markdown","af62595e":"markdown","512f3ef5":"markdown","94213eba":"markdown","d0847c7a":"markdown","5680e867":"markdown","54aa9f38":"markdown","d04c2e84":"markdown"},"source":{"40ca82e6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","58a7241f":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","76694601":"df.head()","ce4ebc45":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Class'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Class')\nax[0].set_ylabel('')\nsns.countplot('Class',data=df,ax=ax[1])\nax[1].set_title('Class')\nplt.show()","0cf1f90b":"X = df.drop('Class', axis=1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","6e89da58":"params_xGB = {\n    'nthread':16, \n    'gamma': 0, \n    'max_depth': 6, \n    'min_child_weight': 1, \n    'max_delta_step': 0, \n    'subsample': 1.0,\n        \n    'colsample_bytree': 1.0, \n       \n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2020,\n    'tree_method' : 'gpu_hist',\n}","32453907":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                    index=y_train.index,columns=['prediction'])\nk_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2020)\nstart = time.time() \nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n    dCV = xgb.DMatrix(data=X_cv_fold)\n    \n    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000, \n                 nfold=5, early_stopping_rounds=200, verbose_eval=100)\n    \n    best_rounds = np.argmin(np.array(bst['test-logloss-mean']))\n    bst = xgb.train(params_xGB, dtrain, best_rounds)\n    \n    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        bst.predict(dCV)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nxgb_runtime = time.time() - start    \nloglossXGBoostGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\n\nprint( 'XGBoost Gradient Boosting Log Loss : {0:.4f} ,  XGBoost Runtime : {1:.4f}'.format(loglossXGBoostGradientBoosting ,xgb_runtime ))","39d87051":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","ebf8a11f":"n_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2018\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators, \n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, \n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n        class_weight=class_weight)","99ac3218":"##RandomForest with stratified 5 Fold\n\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\nstart = time.time() \nclf = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, n_jobs=-1, random_state = 2020, oob_score=True)\ncv = StratifiedKFold(n_splits=5,random_state = 2020)\ny_preds_rf = np.zeros(X_test.shape[0])\nn_iter = 0 \nfor train_index,test_index in cv.split(X_train,y_train):\n    trx , tsx = X_train.iloc[train_index] , X_train.iloc[test_index]\n    vly , vlt = y_train.iloc[train_index] , y_train.iloc[test_index]\n    RFC = RFC.fit(trx,vly)   \n    loglossTraining = log_loss(vly, \\\n                                RFC.predict_proba(trx))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[tsx.index,:] = \\\n        RFC.predict_proba(tsx)  \n    loglossCV = log_loss(vlt, \\\n        predictionsBasedOnKFolds.loc[tsx.index,1])\n    cvScores.append(loglossCV)\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \n    n_iter += 1\n    cv_roc_score = roc_auc_score(y_test, RFC.predict_proba(X_test)[:,1], average = 'macro')\n    cv_precision, cv_recall, _ = precision_recall_curve(y_test,RFC.predict_proba(X_test)[:,1])\n    cv_pr_auc = auc(cv_recall, cv_precision)\n    print( '\\n#{0}, CV_ROC_AUC : {1} , RF_CV_PR_AUC : {2} '.format(n_iter ,cv_roc_score, cv_pr_auc))\n    y_preds_rf += RFC.predict_proba(X_test)[:,1]\/ cv.n_splits\nrf_runtime = time.time() - start \nrf_cv_roc_score = roc_auc_score(y_test, y_preds_rf, average = 'macro')\nrf_cv_precision, rf_cv_recall, _ = precision_recall_curve(y_test,y_preds_rf)\nrf_cv_pr_auc = auc(rf_cv_recall, rf_cv_precision)    \nloglossRandomForestsClassifier = log_loss(y_train, \n                                          predictionsBasedOnKFolds.loc[:,1])\nprint( 'Random Forest Log Loss : {0:.4f} ,  Random Forest Runtime : {1:.4f}'.format(loglossRandomForestsClassifier ,rf_runtime ))\n    ","c07e0832":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsRandomForests = preds.copy()\n\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],\n                                                       preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],\n                                            preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n          Area under the curve = {0:0.2f}'.format(\n          areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","4789aed1":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n    'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2020,\n    'verbose': 50,\n    'num_threads':16,\n    'random_state ' : 2020\n}","28bc6424":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\nstart = time.time() \nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 , verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\nlgbm_runtime = time.time() - start     \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint( 'LightGBM Log Loss : {0:.4f} ,  LightGBM Runtime : {1:.4f}'.format(loglossLightGBMGradientBoosting ,lgbm_runtime ))","ea0a17bd":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","c9b04d37":"predictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)","803d38c9":"predictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)","a2f9fca0":"predictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)","0893d220":"print(\"Log Loss of Random Forests on Test Set: \", \\\n          logLossTestSetRandomForests)\nprint(\"Log Loss of XGBoost Gradient Boosting on Test Set: \", \\\n          logLossTestSetXGBoostGradientBoosting)\nprint(\"Log Loss of LightGBM Gradient Boosting on Test Set: \", \\\n          logLossTestSetLightGBMGradientBoosting)","287c0c13":"#RF\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetRandomForests)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetRandomForests)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetRandomForests)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","2a9c90d0":"#XGB\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetXGBoostGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","1379d828":"#LGB\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLightGBMGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","a4a46d21":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')","5ef81ebe":"thresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)","6ae34316":"### Using CPU only\n\nstart = time.time()\n\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'binary_logloss',\n            'metric_freq' : 50,\n            'max_depth' :4, \n            'num_leaves' : 31,\n            'learning_rate' : 0.01,\n            'feature_fraction' : 1.0,\n            'bagging_fraction' : 1.0,\n            'bagging_freq' : 0,\n            'bagging_seed' : 2020,\n            'num_threads' : 16\n           }\n\n\nlgbm_clf = LGBMClassifier(boosting_type = 'gbdt',\n           objective= 'binary',\n           metric='auc',\n#             metric_freq = 50,\n#             max_depth =4, \n#             num_leaves = 31,\n#             learning_rate = 0.01,\n#             feature_fraction = 1.0,\n#             bagging_fraction = 1.0,\n#             bagging_freq = 0,\n# #             bagging_seed = 2020,\n#             num_threads = 16,\n                          random_state = 2020)\n\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train,  verbose = 50)\n\n\nlgbm_cpu_runtime = time.time() - start\n\nget_eval_by_threshold(y_test, lgbm_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\nlgbm_precision, lgbm_recall, _ = precision_recall_curve(y_test,lgbm_clf.predict_proba(X_test)[:,1])\nlgbm_pr_auc = auc(lgbm_recall, lgbm_precision)\n\n\n\nprint( 'LightGBM_ROC_AUC : {0:.4f} , LightGBM_PR_AUC : {1:.4f} ,Runtime : {2:.4f}'.format(lgbm_roc_score ,lgbm_pr_auc, lgbm_cpu_runtime))","0f65f25a":"start = time.time()\n\nxgb_clf = XGBClassifier(random_state = 2020)\nxgb_clf.fit(X_train, y_train, verbose = 50)\n\nxgb_gpu_runtime = time.time() - start\n\npred = xgb_clf.predict(X_test)\n\nget_eval_by_threshold(y_test, xgb_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\n\nxgb_gpu_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test,xgb_clf.predict_proba(X_test)[:,1])\nxgb_gpu_pr_auc = auc(xgb_recall, xgb_precision)\n\n\n\nprint( 'XGboost_gpu_ROC_AUC : {0:.4f} , XGboost_gpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_gpu_roc_score ,xgb_gpu_pr_auc, xgb_gpu_runtime ))","3dbb9a2b":"import eli5\nfrom eli5.sklearn import PermutationImportance\nwarnings.filterwarnings('ignore')","edb46fc7":"perm_xgb = PermutationImportance(xgb_clf, random_state=2020).fit(X_test, y_test)\neli5.show_weights(perm_xgb, feature_names = X_test.columns.tolist())","c9d1c0f2":"import shap","829de782":"X = df.drop('Class', axis=1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","53aca0f0":"y_trainin = y_train.to_frame()\nfor_sample_train_df = pd.concat([X_train, y_trainin], axis=1)\ny_testet = y_test.to_frame()\nfor_sample_test_df = pd.concat([X_test, y_testet], axis=1)","e5e530e5":"X_ = for_sample_train_df.drop('Class', axis=1)\ny_ = for_sample_train_df['Class']\n\nsample_train_x, sample_test_x, sample_train_y, sample_test_y = train_test_split(X_, y_, test_size = 0.8 , random_state = 2020, stratify = y_)\n\ndel X_train, X_test, y_train, y_test , y_trainin, y_testet,","95d87f8a":"gc.collect()","063d9ec3":"## Make sample for faster computation\n\nX_ = for_sample_train_df.drop('Class', axis=1)\ny_ = for_sample_train_df['Class']\n\nsample_train_x, sample_test_x, sample_train_y, sample_test_y = train_test_split(X_, y_, test_size = 0.80 , random_state = 2020, stratify = y_)","a6a57380":"X_sampled = sample_train_x.copy()","0723b071":"#LightGBM\nimport shap\nshap.initjs()\n\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n\nexplainer = shap.TreeExplainer(lgbm_clf)\nshap_values = explainer.shap_values(X_sampled)","e258f433":"shap.force_plot(explainer.expected_value[1], shap_values[1][2,:], X_sampled.iloc[2,:])","d33e4443":"shap.force_plot(explainer.expected_value[1], shap_values[1][3,:], X_sampled.iloc[3,:])","9aa6b5df":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X_sampled, plot_type=\"bar\")","c1166b99":"# shap.force_plot(base_value=explainer.expected_value[1], shap_values=shap_values[1], features=X_sampled.columns)","e6a09312":"* This book contains wonderful explanation about importance of \"Unsupervised Learning\" in the field of Anomaly Detection, and handling imbalanced data","af62595e":"## LightGBM","512f3ef5":"## Random Forest","94213eba":"## Permutation Importance","d0847c7a":"### This notebook is based on published book 'Hands-on Unsupervised Learing'\n","5680e867":"Apply to Test Set","54aa9f38":"## SHAP","d04c2e84":"## Xgboost"}}