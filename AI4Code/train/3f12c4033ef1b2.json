{"cell_type":{"ff47ccb2":"code","f50a8efe":"code","b0fef3d6":"code","52e554bc":"code","ad769ca4":"code","20429fee":"code","7886d5b6":"code","e34d8469":"code","40d7b8b5":"code","33a56528":"code","9f3164d8":"code","4d13ca07":"code","c48d6634":"code","f3c535f0":"code","95ad00ad":"code","3d185df7":"code","916d3900":"code","02e9199a":"code","4d8e013e":"code","30abad18":"code","11de3da7":"code","1b44072c":"code","080455c0":"code","94fa0ac1":"code","3cf68663":"code","7fd40147":"markdown","0ba9b40e":"markdown","7068a625":"markdown"},"source":{"ff47ccb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f50a8efe":"df1 = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\ndf1.head()","b0fef3d6":"df1['salary'].isnull().sum()","52e554bc":"df1[df1.status=='Not Placed'].shape","ad769ca4":"df2 = df1.drop(['salary', 'sl_no'], axis=1)\ndf2.shape","20429fee":"df2.dtypes","7886d5b6":"import seaborn as sns\nsns.pairplot(df2)","e34d8469":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf2['gender'] = le.fit_transform(df2['gender'])\ndf2['ssc_b'] = le.fit_transform(df2['ssc_b'])\ndf2['hsc_b'] = le.fit_transform(df2['hsc_b'])\ndf2['hsc_s'] = le.fit_transform(df2['hsc_s'])\ndf2['degree_t'] = le.fit_transform(df2['degree_t'])\ndf2['workex'] = le.fit_transform(df2['workex'])\ndf2['specialisation'] = le.fit_transform(df2['specialisation'])\ndf2['status'] = le.fit_transform(df2['status'])","40d7b8b5":"df2.head()","33a56528":"df2.shape","9f3164d8":"df2.dtypes","4d13ca07":"x = df2.iloc[:,:12]\ny = df2['status']\nx.shape, y.shape","c48d6634":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(x,y)","f3c535f0":"print(model.)","95ad00ad":"import matplotlib.pyplot as plt\n%matplotlib inline\nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(5).plot(kind='barh')\nplt.show()","3d185df7":"X = x[['etest_p', 'mba_p', 'hsc_p', 'degree_p', 'ssc_p']]","916d3900":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","02e9199a":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","4d8e013e":"import optuna\nimport sklearn.svm\ndef objective(trial):\n\n    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC'])\n    \n    if classifier == 'RandomForest':\n        n_estimators = trial.suggest_int('n_estimators', 200, 2000,10)\n        max_depth = int(trial.suggest_float('max_depth', 10, 100, log=True))\n\n        clf = sklearn.ensemble.RandomForestClassifier(\n            n_estimators=n_estimators, max_depth=max_depth)\n    else:\n        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\n        \n        clf = sklearn.svm.SVC(C=c, gamma='auto')\n\n    return sklearn.model_selection.cross_val_score(\n        clf,x_train,y_train, n_jobs=-1, cv=3).mean()","30abad18":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\n\ntrial = study.best_trial\n\nprint('Accuracy: {}'.format(trial.value))\nprint(\"Best hyperparameters: {}\".format(trial.params))","11de3da7":"trial","1b44072c":"study.best_params","080455c0":"rf=sklearn.ensemble.RandomForestClassifier(n_estimators=490,max_depth=91.71293127875516)\nrf.fit(x_train,y_train)","94fa0ac1":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\ny_pred=rf.predict(x_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","3cf68663":"optuna.visualization.plot_contour(study, params=['n_estimators', 'max_depth'])","7fd40147":"Hyper Parameter Tuning using OPTUNA","0ba9b40e":"This Means the data is Logically Fine","7068a625":"Thus Let us take the best 5 Features"}}