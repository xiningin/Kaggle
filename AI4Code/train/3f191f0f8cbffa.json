{"cell_type":{"e205620c":"code","a9c086d7":"code","09ae8207":"code","5537b7a2":"code","325445b6":"code","eb987098":"code","321fca9a":"code","6138ce25":"code","c3b80e1d":"code","0272e235":"code","244494f0":"code","3f4f2392":"code","aae7ef97":"code","0185c10b":"code","f612d42f":"code","ed855189":"code","ec783fcf":"code","ebab242d":"code","491810e2":"code","f881d360":"code","a495a5a7":"code","0e445db8":"code","eac25543":"code","5a5887aa":"code","0a0112f5":"code","1dd7b75b":"code","d19bf9f2":"code","7e0dcf8c":"code","81432713":"code","05500f0a":"code","03ee583d":"code","bfcdd325":"code","37190c16":"code","4c1c2bdc":"code","c466d8d6":"code","c26d6413":"code","ac64a831":"code","bc737a38":"code","ad15c261":"code","39acf53a":"code","2afbb5dd":"code","2e388660":"code","0bc10650":"code","6b8e7fa5":"code","f6810edf":"code","04dce20a":"code","6e91c192":"code","1462eaf9":"code","0e050aca":"markdown","50c27f0b":"markdown","f975562e":"markdown","02b56cc8":"markdown","eb411f45":"markdown","7d2ead4c":"markdown","d7bafdfa":"markdown","f59c838e":"markdown","dd46c981":"markdown","67e70fd9":"markdown","c94d180d":"markdown","f63ede53":"markdown","181c82a1":"markdown","25e52b14":"markdown","c8690704":"markdown","b553f2d8":"markdown","1cb7c526":"markdown","c3cbbf01":"markdown","48d1fee0":"markdown","31379256":"markdown","f30e280f":"markdown","a0f19156":"markdown","88f0a997":"markdown","8b1f2bcf":"markdown","e5b8dc72":"markdown","8dc76c19":"markdown","6b43d48f":"markdown","ff11d6c3":"markdown","ea9b90e2":"markdown","c2383bca":"markdown","0da6c6df":"markdown","164539bd":"markdown","2ba79822":"markdown"},"source":{"e205620c":"import pandas as pd\nfrom urllib.request import urlopen  \nimport os.path as osp\nimport os\nimport logging\nimport zipfile\nfrom glob import glob\nlogging.getLogger().setLevel('INFO')\n\nimport pickle\nimport random\nimport numpy as np\nimport datetime\nimport time\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sn\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()","a9c086d7":"def dump(obj, name):\n    pickle.dump(obj, open(name+'.p', \"wb\")) \n\ndef load(name):\n    obj = pickle.load(open(name+\".p\", \"rb\")) \n    return obj","09ae8207":"ATP_DIR = \"..\/input\/\"\nATP_FILES = sorted(glob(\"%s\/*.xls*\" % ATP_DIR))\ndf_atp = pd.concat([pd.read_excel(f) for f in ATP_FILES], sort=False, ignore_index=True)","5537b7a2":"# look at the size of the data\ndf_atp.head()","325445b6":"# numbers of rows and columns\ndf_atp.shape","eb987098":"# look at what are the columns\ndf_atp.columns","321fca9a":"# look at the type of the data in each columns\ndf_atp.info()","6138ce25":"df_atp.describe()","c3b80e1d":"# columns of which the data type is 'object' (string)\nprint([c for c in df_atp.columns if df_atp[str(c)].dtypes == 'object'])","0272e235":"# check and find out the erros\nfor c in ['EXW', 'Lsets', 'WRank', 'LRank', 'L2', 'W2', 'L3','W3']:\n    print([x for x in df_atp[str(c)] if type(x)==str])","244494f0":"# make a copy of the original data\ndf_atp2 = df_atp.copy()\n\n# cleaning the data\ndf_atp2[\"EXW\"] = df_atp2[\"EXW\"].replace(\"2.,3\", 2.3)\ndf_atp2[\"Lsets\"] = df_atp2[\"Lsets\"].replace(\"`1\", 1)\n\ndf_atp2[\"LRank\"] = df_atp2[\"LRank\"].replace(\"NR\", np.nan).astype('float')\ndf_atp2[\"WRank\"] = df_atp2[\"WRank\"].replace(\"NR\", np.nan).astype('float')\n\ndf_atp2[\"L2\"] = df_atp2[\"L2\"].replace(' ', np.nan)\ndf_atp2[\"W2\"] = df_atp2[\"W2\"].replace(' ', np.nan)\ndf_atp2[\"L3\"] = df_atp2[\"L3\"].replace(' ', np.nan)\ndf_atp2[\"W3\"] = df_atp2[\"W3\"].replace(' ', np.nan)","3f4f2392":"df_atp2.describe()","aae7ef97":"# checking out missing values\ndf_atp2.isnull().sum().sort_values(ascending=False)","0185c10b":"# columns without missing values\ncol_0miss = [i for i, x in df_atp2.isnull().sum().iteritems() if x == 0]\nprint(col_0miss)","f612d42f":"# columns with missing values\ncol_miss = [x for x in df_atp2.columns if x not in col_0miss]\nprint(col_miss)","ed855189":"# check the missing values by year\ndf_atp2['year'] = df_atp2.Date.apply(lambda x: x.year)","ec783fcf":"col_odds = ['AvgL', 'AvgW', 'B&WL', 'B&WW', 'B365L', 'B365W', 'CBL', 'CBW', 'EXL', 'EXW',\n             'GBL', 'GBW', 'IWL', 'IWW', 'LBL', 'LBW', 'MaxL', 'MaxW', 'PSL', 'PSW',\n             'SBL', 'SBW', 'SJL', 'SJW', 'UBL', 'UBW']\ndf_atp2[col_odds + ['year']].groupby('year').count()","ebab242d":"df_atp3 = df_atp.copy()\n# replace NR by the largest ranking value\ndf_atp3['WRank'] = df_atp3.WRank.replace('NR', df_atp3.WRank.replace('NR', np.nan).max())\ndf_atp3['LRank'] = df_atp3.LRank.replace('NR', df_atp3.LRank.replace('NR', np.nan).max())\n\n# select a subset of data\nbeg = datetime.datetime(2010,1,1)\nend = datetime.datetime(2019,1,1)\nindices = df_atp3[(df_atp3.Date>=beg)&(df_atp3.Date<=end)].index\ntest = df_atp3.iloc[indices,:]\n\n# classical ATP ranking\natp_rank = 100*(test.WRank<test.LRank).sum()\/len(indices)\n\n# ATP Entry points\natp_pts = 100*(test.WPts>test.LPts).sum()\/len(indices)\n\n# Bookmakers\nbook_pi = 100*(test.PSW<test.PSL).sum()\/len(indices)\nbook_365 = 100*(test.B365W<test.B365L).sum()\/len(indices)\n\n\n# Plot\n\nlabels = [\"ATP entry ranking\", \"ATP entry points\", \"Pinnacle\", \"Bet365\"]\nvalues = [atp_rank, atp_pts, book_pi, book_365]\ny_pos = np.arange(len(labels))\n\nfig = plt.figure(figsize=(13,9))\nwith plt.style.context('ggplot'):\n    plt.barh(y_pos, values)\n\nplt.yticks(y_pos, labels, fontsize=16)\nplt.xticks(fontsize=14)\nplt.xlabel('% of matches correctly predicted', fontsize=16)\nplt.title(\"Prediction of all matches since \"+beg.strftime(\"%Y-%m-%d\")+\" (\"+str(len(indices))+\" matches)\",\n         fontsize=20)\nplt.xlim([65,70])\nplt.tight_layout()\nplt.show()","491810e2":"def past_victories(df, player):\n    \"\"\"\n    For each match, return the percentage of victories of the player in the past.\n    \"\"\"\n\n    vic_pers = []\n    for i in df[1::].index:\n        name = df.loc[i, player]\n        df_past = df.iloc[0:i]\n        vic_per = df_past[df_past.Winner == name].count().ATP\/len(df_past)*100\n        vic_pers.append(vic_per)\n\n    vic_pers = [0.0] + vic_pers\n\n    return vic_pers","f881d360":"# Feature of past victories percentage for winner and loser \nwin_pers = past_victories(df_atp2, player='Winner')\nlos_pers = past_victories(df_atp2, player='Loser')\ndic_pers = {\"vicW\": win_pers, \"vicL\": los_pers}\ndf_vic = pd.DataFrame(data=dic_pers, columns=dic_pers.keys())","a495a5a7":"def historical_mean_score(df, col_pair, num_days):\n    \"\"\"\n    For each match, return the mean value of past x days (num_days) for column \n    in question of each player. For examle, if col_pair is ('WRank', 'LRank'), and\n    num_days is 180, this function returns a dataframe with two columns (WMRank, LMRank)\n    of which each row indicates the mean values of ATP Rank of the player (Winner) \n    in the past 180 days and that of the player (Loser).\n    \"\"\"\n\n    col_w, col_l = col_pair\n    if col_w.startswith('W'):\n        colname_w = 'WM'+col_w[1::]\n    elif col_w.endswith('W'):\n        colname_w = 'WM'+col_w[0:-1]\n    if col_l.startswith('L'):\n        colname_l = 'LM'+col_l[1::]\n    elif col_l.endswith('L'):\n        colname_l = 'LM'+col_l[0:-1]\n\n    mscoresW = []\n    mscoresL = []\n    for i, row in df.iterrows():\n        winner = row.Winner\n        loser = row.Loser\n        date_cur = row.Date\n        date_hist = date_cur - datetime.timedelta(days=num_days)\n        df_hist = df.iloc[df[(df.Date>=date_hist)&(df.Date<date_cur)].index, :]\n        mscoreW = (df_hist[df_hist.Winner == winner][str(col_w)].mean() + \n                  df_hist[df_hist.Loser == winner][str(col_l)].mean())\/2\n        mscoreL = (df_hist[df_hist.Winner == loser][str(col_w)].mean() + \n                  df_hist[df_hist.Loser == loser][str(col_l)].mean())\/2\n        mscoresW.append(mscoreW)\n        mscoresL.append(mscoreL)\n\n    dict_mscore = {colname_w: mscoresW, colname_l: mscoresL}\n    df_mscore = pd.DataFrame(data=dict_mscore, columns=dict_mscore.keys())\n\n    return df_mscore ","0e445db8":"# create new features with recent historical data\nnum_days = 360 # historical data of 180 days before the match\ndf_mrank = historical_mean_score(df_atp2, ('WRank', 'LRank'), num_days=num_days)\ndf_mpts = historical_mean_score(df_atp2, ('WPts', 'LPts'), num_days=num_days)\ndf_msets = historical_mean_score(df_atp2, ('Wsets', 'Lsets'), num_days=num_days)\ndf_mPS = historical_mean_score(df_atp2, ('PSW', 'PSL'), num_days=num_days)\ndf_mB365 = historical_mean_score(df_atp2, ('B365W', 'B365L'), num_days=num_days)\n\n# concat recent score features\n\ndf_mscores = pd.concat([df_mrank, df_mpts, df_msets, df_mPS, df_mB365],1)","eac25543":"def cal_diff(df, col_pairs):\n    \"\"\"\n    Return the numerical difference between the two columns in col_pairs, \n    and store the values in a dataframe.\n    \"\"\"\n\n    dic = {}\n    for col_pair in col_pairs:\n        col_win, col_los = col_pair\n\n    if col_win.endswith(\"W\"):\n        col_new = col_win[0:-1] + \"_diff\"\n    else:\n        col_new = col_win[1::] + \"_diff\"\n\n    dic[col_new] = abs(df[str(col_win)] - df[str(col_los)])\n    df_diff = pd.DataFrame(data=dic, columns=dic.keys())\n\n    return df_diff","5a5887aa":"cols_win = ['B365W', 'PSW',  'WPts', 'WRank']\ncols_los = ['B365L', 'PSL',  'LPts', 'LRank'] \ncol_pairs = list(zip(cols_win, cols_los))\n\nodds_diff = cal_diff(df_atp2, col_pairs)\nvic_diff = cal_diff(df_vic, [('vicW', 'vicL')])\n\ndf_diff = pd.concat([odds_diff, vic_diff], 1)","0a0112f5":"def randomize_data(df, col_pairs):\n\n    \"\"\"\n    shuffle the original data, so that we have two balanced class, of which\n    50% of label \"1\" and 50% of label \"0\".\n    \"\"\"\n    # random index for player1\n    idx1 = random.sample(range(1, len(df)), len(df)\/\/2)\n    # index for player2, the rest of the index after substracing the index for player1\n    idx2 = list(set(np.array(range(0, len(df)))) - set(idx1))\n\n    cols = []\n    col_names = []\n    for x in col_pairs:\n        col_win, col_los = x\n        col1 = list(df[str(col_win)][idx1].append(df[str(col_los)][idx2])\n        .sort_index())\n        col2 = list(df[str(col_los)][idx1].append(df[str(col_win)][idx2])\n        .sort_index())\n        cols.append([col1, col2])\n    # rename the columns\n    if col_win == \"Winner\":\n        col1_name = \"player1\"\n    elif col_win.endswith(\"W\"):\n        col1_name = col_win[0:-1] + \"_1\"\n    elif col_win.startswith(\"W\"):\n        col1_name = col_win[1::] + \"_1\" \n\n    if col_los == \"Loser\":\n        col2_name = \"player2\"\n    elif col_los.endswith('L'):\n        col2_name = col_los[0:-1] + \"_2\"\n    elif col_los.startswith('L'):\n        col2_name = col_los[1::] + \"_2\"\n\n    col_names.append([col1_name, col2_name])\n\n    player_data = [v for l in cols for v in l]\n    columns = [v for c in col_names for v in c]\n    player_data = pd.DataFrame(list(map(list, zip(*player_data))), columns=columns)\n\n    return player_data, idx1","1dd7b75b":"# data to 'shuffle'\nplayer_data = pd.concat([df_atp2[cols_win + cols_los + ['Winner', 'Loser']], \n                         df_vic, df_mscores],1)\ncol_pairs = col_pairs + [('Winner', 'Loser'), ('vicW', 'vicL'), ('WMRank', 'LMRank'),\n                         ('WMPts', 'LMPts'), ('WMsets', 'LMsets'), ('WMPS', 'LMPS'), \n                         ('WMB365', 'LMB365')]\n# randomize the winner, loser data to player1, player2 data\nplayer_data, idx1 = randomize_data(player_data, col_pairs)","d19bf9f2":"def categorical_features_encoding(cat_features):\n    \"\"\"\n    Categorical features encoding.\n    Simple one-hot encoding.\n    \"\"\"\n\n    ohe = OneHotEncoder()\n    cat_features_encoded = ohe.fit_transform(cat_features)\n    columns = ohe.get_feature_names(list(cat_features.columns))\n    cat_features = pd.DataFrame(cat_features_encoded.todense(), columns=columns)\n\n    return cat_features","7e0dcf8c":"def features_players_encoding(data):\n    \"\"\"\n    Encoding of the players . \n    The players are not encoded like the other categorical features because for \n    each match we encode both players at the same time (we put a 1 in each row \n    corresponding to the players playing the match for each match).\n    \"\"\"\n    player1 = data.player1\n    player2 = data.player2\n    le = LabelEncoder()\n    le.fit(list(player1)+list(player2))\n    player1 = le.transform(player1)\n    player2 = le.transform(player2)\n    encod = np.zeros([len(data), len(le.classes_)])\n    for i in range(len(data)):\n        encod[i,player1[i]] += 1\n    for i in range(len(data)):\n        encod[i,player2[i]] += 1\n    columns = [\"player_\"+el for el in le.classes_]\n    players_encoded = pd.DataFrame(encod, columns=columns)\n    \n    return players_encoded","81432713":"def num_features_transform(features_numerical):\n    \"\"\"\n    Handling missing values and then standardization.\n    \"\"\"\n\n    features_numerical = features_numerical.fillna(0)\n    scaler = StandardScaler()\n    features_numerical_scaled = scaler.fit_transform(features_numerical)\n    columns = features_numerical.columns\n    features_numerical = pd.DataFrame(features_numerical_scaled, columns=columns)\n\n    return features_numerical","05500f0a":"# categorical features except the players\ncol_cat = ['Best of', 'Court', 'Round', 'Series', 'Surface', 'Tournament']\ndf_cat = df_atp2[col_cat]\n# onehot encode categorical features\nfeatures_categorical_encoded = categorical_features_encoding(df_cat)\n# onehot encode players\nplayers_encoded = features_players_encoding(player_data)\n# concat all the categorical features\nfeatures_onehot = pd.concat([features_categorical_encoded, players_encoded],1)\n\n# numerical features\nfeatures_numerical = pd.concat([player_data.drop(['player1', 'player2'],axis=1), df_diff],1)","03ee583d":"# concat all the features (categorical and numerical)\nX = pd.concat([features_onehot, features_numerical], 1)\n\n# create \"label\" data, which is what we want to predict\ny = pd.Series(np.ones(len(df_atp)), name='label')\ny.iloc[idx1] = 0 # if player1 wins, the label is 0","bfcdd325":"def get_indices(df, begin_date, end_date):\n    \"\"\"\n    Calculate the index of observations between beginning date and ending date.\n    \"\"\"\n\n    indices = df[(df.Date >= begin_date) & (df.Date < end_date)].index\n\n    return indices\n\ndef crop_data(data, indices):\n    \"\"\"\n    Return the data for corresponding indices.\n    \"\"\"\n\n    if data.ndim > 1:\n        cropped_data = data.iloc[indices,:].reset_index(drop=True)\n    else:\n        cropped_data = data.iloc[indices].reset_index(drop=True)\n\n    return cropped_data","37190c16":"# train set, data from 2010 to 2017 \n#begin_train = df_atp2.Date.iloc[0]\nbegin_train = datetime.datetime(2010,1,1) \nend_train = datetime.datetime(2017,1,1) \nindices_train = get_indices(df_atp2, begin_train, end_train)\n\nX_train = crop_data(X, indices_train)\ny_train = crop_data(y, indices_train)\n\n# test set, data from 2017 to 2018\nbegin_test = datetime.datetime(2017,1,1)\nend_test = datetime.datetime(2018,1,1)\nindices_test = get_indices(df_atp2, begin_test, end_test)\n\nX_test = crop_data(X, indices_test)\ny_test = crop_data(y, indices_test)","4c1c2bdc":"# XGB classifier with default parameters\nt = time.time()\nxgb_clf = xgb.XGBClassifier()\nxgb_clf.fit(X_train, y_train)\nprint('Elapsed: %s' % (time.time() - t))","c466d8d6":"# use the model to make predictions with the test data\ny_pred = xgb_clf.predict(X_test)","c26d6413":"# check model performance\ncount_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}'.format(count_misclassified))\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: {:.2f}%'.format(100*accuracy))\n\nprint(classification_report(y_test, y_pred))","ac64a831":"score = 'accuracy'\nt = time.time()\n\n\nprint(\"# Tuning hyper-parameters for %s\" % score)\nprint()\n\nxgb_gs = xgb.XGBClassifier()\nparam_grid = {'max_depth': [2, 3], \n              'learning_rate': [0.1, 0.3], \n              'n_estimators': [50, 100]}\ntscv = TimeSeriesSplit(n_splits=3).split(X_train)\nclf = GridSearchCV(xgb_gs, param_grid, scoring=score, cv=tscv)\nclf.fit(X_train, y_train)\n\nprint(\"Best parameters set found on development set:\")\nprint()\nprint(clf.best_params_)\nprint()\nprint(\"Grid scores on development set:\")\nprint()\nmeans = clf.cv_results_['mean_test_score']\nstds = clf.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, clf.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\nprint()\n\nprint(\"Detailed classification report:\")\nprint()\nprint(\"The model is trained on the full development set.\")\nprint(\"The scores are computed on the full evaluation set.\")\nprint()\ny_true, y_pred = y_test, clf.predict(X_test)\nprint(classification_report(y_true, y_pred))\nprint()\n\nprint('Elapsed: %s' % (time.time() - t))","bc737a38":"# save the best parameters\nclf_bestP = clf.best_params_","ad15c261":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0)\ndtrain=xgb.DMatrix(X_train, label=y_train)\ndval=xgb.DMatrix(X_val, label=y_val)\n\neval_set = [(dtrain, 'train'), (dval, 'val')]\nprogress = {}\n\nparams = {'eval_metric':['error', 'auc', 'logloss'], 'objective':'binary:logistic'}\nparams.update(clf_bestP)\n\nt = time.time()\n\nmodel=xgb.train(params, dtrain, num_boost_round=params['n_estimators'], \n                evals=eval_set, evals_result=progress,\n                verbose_eval=False, early_stopping_rounds=7)\n\nprint('Elapsed: %s' % (time.time() - t))","39acf53a":"train_loss = progress['train']['logloss']\nval_loss = progress['val']['logloss']\n\nplt.figure(figsize=(9,6))\nplt.plot(np.array(range(1,len(train_loss)+1)), train_loss, label='train_loss')\nplt.plot(np.array(range(1,len(val_loss)+1)), val_loss, label='val_loss')\nplt.legend()\nplt.xlabel('number of iteration')\nplt.ylabel('logloss')\n#plt.ylim([0,1])\n\n\ntrain_error = progress['train']['error']\nval_error = progress['val']['error']\n\nplt.figure(figsize=(9,6))\nplt.plot(np.array(range(1,len(train_loss)+1)), train_error, label='train_error')\nplt.plot(np.array(range(1,len(val_loss)+1)), val_error, label='val_error')\nplt.legend()\nplt.xlabel('number of iteration')\nplt.ylabel('error')\n#plt.ylim([0,1])\n\n\ntrain_auc = progress['train']['auc']\nval_auc = progress['val']['auc']\n\nplt.figure(figsize=(9,6))\nplt.plot(np.array(range(1,len(train_loss)+1)), train_auc, label='train_auc')\nplt.plot(np.array(range(1,len(val_loss)+1)), val_auc, label='val_auc')\nplt.legend()\nplt.xlabel('number of iteration')\nplt.ylabel('auc')\n#plt.ylim([0,1])\n\nplt.show()\n\nprint(\"Best iteration number: %d\" % (len(train_loss)+1))","2afbb5dd":"# The probability given by the model to each outcome of each match :\npred_test = model.predict(xgb.DMatrix(X_test)) \npred_test = pred_test  > 0.5  \npred_test = pred_test.astype(int)\n\nprint(classification_report(y_test, pred_test))\nprint('Accuracy: {:.2f}%'.format(100*accuracy_score(y_test, pred_test)))","2e388660":"cm = confusion_matrix(y_test, pred_test)\ndf_cm = pd.DataFrame(cm, index = [1, 0], columns = [1, 0])\n\n# plot\nfig = plt.figure(figsize=(9, 6))\nsn.set(font_scale=1.4) # label size\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt=\"d\", cmap=\"YlGnBu\") # font size\nplt.xlabel('actual')\nplt.ylabel('prediction')\nplt.title('Confusion Matrix')\n\n#fig.savefig(fig_dir+'cm.png')\nplt.show()","0bc10650":"# plotting decision trees within our trained XGBoost classifier\n# to get insight into the gradient boosting process\nax = xgb.plot_tree(model,num_trees=0)\nax.figure.set_size_inches(13, 9)\nax.set(title='XGB classifier boosting process')\n\n#ax.figure.savefig(fig_dir+'tree.png')\nplt.show()","6b8e7fa5":"# use XGBoost library's built-in function to plot features ordered by their importance\nax = xgb.plot_importance(model)\nax.figure.set_size_inches(13, 9)\nax.figure.tight_layout()\n\n#ax.figure.savefig(fig_dir+'fi.png')\nplt.show()","f6810edf":"def rankings(df, player):\n    \"\"\"\n    For a given player, return his rank and date for each match.\n    \"\"\"\n    \n    idx_w = df_atp[(df_atp.Winner == player)].index\n    idx_l = df_atp[(df_atp.Loser == player)].index\n    dates = df_atp.Date.iloc[idx_w.append(idx_l)].sort_index()\n    rankings = df_atp['WRank'].iloc[idx_w].append(df_atp['LRank'].iloc[idx_l]).sort_index()\n\n    return dates, rankings\n\n\n\nfig = plt.figure(figsize=(13, 9))\n\nx, y = rankings(df_atp, 'Federer R.')\nplt.plot_date(x, y, '-.')\n\nx, y = rankings(df_atp, 'Nadal R.')\nplt.plot_date(x, y, '-+')\n\nx, y = rankings(df_atp, 'Djokovic N.')\nplt.plot_date(x, y, '-v')\n\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('ATP Entry Rank', fontsize=18)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.legend(['Federer R.', 'Nadal R.', 'Djokovic N.'], fontsize=14)\nplt.title('ATP entry rank evolution of the three biggest players')\n#plt.ylim([0, 50])\n\n#fig.savefig(fig_dir+'ranking.png')\nplt.show()","04dce20a":"# number of Grand Slam victories by player (first 15 players)\ndf_gs = df_atp[df_atp.Series == 'Grand Slam'].groupby(['Winner']).agg({\n    'ATP':'count'}).sort_values(by='ATP', ascending=False).iloc[0:15]\n\nfig, ax = plt.subplots(figsize=(13,9))\nax = sn.barplot(x=df_gs.index, y=df_gs.ATP)\nax.set_xticklabels(labels=df_gs.index, rotation=90)\nax.set(title='Number of Grand Slam victories since 2000')\nax.figure.tight_layout()\n\n#fig.savefig(fig_dir+\"grand_slam.png\")\nplt.show()","6e91c192":"df_gs2 = df_atp[df_atp.Series == 'Grand Slam'].groupby(['Winner', 'Tournament']).agg({\n    'ATP':'count'}).sort_values(by='ATP', ascending=False).iloc[0:35]\ndf_gs2 = df_gs2.reset_index()\n\nax = sn.catplot(x='Winner', y='ATP', hue='Tournament', data=df_gs2, kind=\"bar\",\n                height=6, aspect=2)\nax.set_xticklabels(labels=df_gs2.Winner.unique(), rotation=90)\nax.set(title='Number of Grand Slam victories by tournament since 2000')\nmpl.rcParams['figure.figsize'] = (13,9)\n\n#ax.savefig(fig_dir+\"grand_slam_tournament.png\")\nplt.show()","1462eaf9":"df_sur = df_atp.groupby(['Winner', 'Surface']).agg({'ATP':'count'})\ndf_FR = df_sur.loc['Federer R.']\ndf_NR = df_sur.loc['Nadal R.']\ndf_DN = df_sur.loc['Djokovic N.']\n\n\nlabels = df_FR.index\nangles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\nangles = np.concatenate((angles,[angles[0]]))\nFR = np.concatenate((df_FR.ATP,[df_FR.ATP[0]]))\nNR = np.concatenate((df_NR.ATP,[df_NR.ATP[0]]))\nDN = np.concatenate((df_DN.ATP,[df_DN.ATP[0]]))\n\nfig = plt.figure(figsize=(13,9))\nax = fig.add_subplot(111, polar=True)\nax.plot(angles, FR, 'o-', linewidth=2, label='Federer R.')\nax.plot(angles, NR, 'o-', linewidth=2, label='Nadal R.')\nax.plot(angles, DN, 'o-', linewidth=2, label='Djokovic N.')\nax.fill(angles, FR, alpha=0.25)\nax.set_thetagrids(angles * 180\/np.pi, labels)\nax.grid(True)\nplt.legend(loc='upper right', bbox_to_anchor=(1.2,1))\n\n#fig.savefig(fig_dir+\"surface_effect.png\")\nplt.show()","0e050aca":"### 2.3 Feature engineering\n\n1.   Selecting features: \n  * Be careful that only the information that can be available before match can be used directly as features. So the features like match score (\"L1, L2, L3, L4, L5, W1, W2, W3, W4, W5, Lsets, Wsets), match 'Comment', cannot be directly used as features. But the historical information before the match can be used to reflect the performance of the players.\n  * Match 'Location' is not used for training, since this is not an important feature. The players' performace isn't impacted much by the location of the match. This is prouved by trying taking into account or not this feature. The prediction accuracy almost doesn't change. \n  * For odds information, since generally it is not known before the match, it is not used directly as features. Except for Bet365 and Pinnacle, odds are at the beginning of the match. So they can be used as features directly.\n2.   Adding features: \n  * For some features, such as the odds (only PS and B365), the ranking, and the points, the difference between the winner and the loser can be further considered as additional features. \n  * Historical match performance can be calculated for each player and each match and used as features.\n  * Recent match information is more important for the prediction. So ranking, points, and numbers of sets won in the last 6 month before each match are calculated and used as additional features.\n\n","50c27f0b":"### 3.1 Define features(X) and target(y) variable ","f975562e":"### 8.1 Three biggest players' rank evolutions","02b56cc8":"## Workflow\n1.   Defining the problem\n\n> * Input: ATP match information between the year 2000 and 2018\n> * Output: predict match result of 2017\n> * Type of the problem : Predicting the outcomes of sport matches is known as a classification problem. If a match is played between player1 and player2, then the problem can be formulated as classifying player1 as a winner or a loser. So it's a binary classification problem. \n \n2.   Choosing the performance evaluation metric\n\n> * Accuracy\n\n3.   Deciding on an evaluation protocol\n\n> * K-fold cross validation: be careful that since the data is chronological, and we use historical information to predict future outcomes, we should split the data so that the test set is always after the train set chronologically. To do this, we can use sklearn's TimeSeriesSplit to split the train set from the test set.\n\n4.   Preparing the data\n\n> * Cleaning the data\n> * Features engineering \n> * Handling missing values\n> * Transformation (standardization\/normalization and encoing)\n\n5.   Developing a model that performs better than a baseline\n\n> * We know that for binary classfication problem, the baselien is 50%.\n> * We can try XGBoost classifier (decision tree)\n\n6.   Tuning the model (overfit first then regularize)","eb411f45":"### 6.1 Evaluation metrics ","7d2ead4c":"#### Generating new features\nThe answer to question4 can be a good feature which reflects the historical performance of each player.","d7bafdfa":"## Helpers","f59c838e":"We can notice that half the odds columns (14 out of 26) do not have data after 2010. ","dd46c981":"### 2.2 Missing values\n\n* Identify missing values\n* Decide whether or not we want to keep entries with missing data --> depend on how random missing values are\n* Note on imputing missing values: Most learning algorithms perform poorly when missing values are expressed as not a number (np.NaN) and need some form of missing value imputation. Be aware that some libraries and algorithms, such as XGBoost, can handle missing values and impute these values automatically by learning. For filling up missing values with common strategies, sklearn provides a SimpleImputer. The four main strategies are mean, most_frequent, median and constant. Other popular ways to impute missing data are clustering the data with the k-nearest neighbor (KNN) algorithm or interpolating the values using a wide range of interpolation methods. Both techniques are not implemented in sklearn\u2019s preprocessing library.","67e70fd9":"### 3.2 Split train-test set","c94d180d":"## 5. Optimization","f63ede53":"### 5.2 Training with best parameters and Hold-out validation","181c82a1":"### 6.2 Confusion matrix","25e52b14":"## 2. Preprocessing\n* Clean up the data\n* Handling missing values\n\n> * xgboost: not necessary to impute\n\n* Feature engineering\n* Randomize (shuffle)\n* Transformation \n  * numerical data (no needs for scaling)\n  * categorical data (encoding)","c8690704":"## 4. Building model","b553f2d8":"### 8.4 Surface effect on three biggest players\n","1cb7c526":"### 5.1 Grid search for the best hyperparameters","c3cbbf01":"### 8.3 Number of Grand Slam victories by tournament by player ","48d1fee0":"### 8.2 Number of Grand Slam victories by player","31379256":"## 1. Quick look at the data","f30e280f":"We can notice that for the columns \"EXW, L2, L3, LRank, Lsets, W2, W3, WRank\", it is not normal that they have object(string) elements, as they should be numerical numbers. So there might be some errors. It explains also why after executing df_atp.describe(), we did not see these comlumns.","a0f19156":"From the figure above we can see that Pinnacle odds can be a good predictor.","88f0a997":"### 2.5 Transformation\n\nFor categorical features, encoding is necessary for all the machine learning algorithms. But for numerical features, standardization(scaling) is not necessary for tree-based algorithms. ","8b1f2bcf":"We can notice that most of the missing values are in the columns concerning odds. Let's check them out by grouping by 'year'.","e5b8dc72":"## Before working on new features\n\nIt would be interesting to see how some elements, such as odds, atp entry ranking, and entry points can help to do the prediction alone.","8dc76c19":"### 2.4 Randomize (shuffle) data\nI randomly assigned \u201cPlayer 1\u201d to be either the winner or loser, and \u201cPlayer 2\u201d to be the other person. The random index is used to balance all the features (odds, points, ranks, players) concernant player1 and player2.\n\n\nFor the labels: I added a label for each match as to whether Player 1 won the match. If player 1 wins, the label is 0, and if the player 2 wins, the label is 1. This would be the label that the model will try to predict.","6b43d48f":"## 3. Preparing the data and defining the training set and test set","ff11d6c3":"## Download the dataset","ea9b90e2":"We can see that, generally, winners have smaller odds, smaller ranks and larger points. So we can build some features according to these patterns. Notably, the odds difference, point difference and rank difference between the winner and the loser.","c2383bca":"## 7. Understanding the model with feature importance","0da6c6df":"## 8. Some other data explorations","164539bd":"## 6. Checking performance","2ba79822":"### 2.1 Clean up the data"}}