{"cell_type":{"6d76711a":"code","913edb62":"code","15849997":"code","60469b75":"code","eb161df5":"code","405a1faa":"code","c5f3902b":"code","d839237b":"code","c49a8940":"code","0f233d56":"code","350c5967":"code","a2d65424":"code","7e774fea":"code","ab9453cc":"code","b027c1ce":"code","23167e6f":"code","42c9bf21":"code","b176dcbb":"code","7752a5ca":"code","55f1ecd5":"code","0d5ef622":"code","709934b1":"code","3903a44e":"code","5ee8169f":"code","91fb96bb":"code","2a15a19b":"code","9e0c7fda":"code","bdd62011":"code","f5e046ce":"code","dea4ae25":"code","581a2ded":"code","cab62341":"code","df9e8fc7":"code","3e4bfe2c":"code","b98a31c6":"code","71608fbd":"code","578bb271":"code","082ec7bf":"code","9ce99a79":"code","095ffa5e":"code","afa92dfc":"code","bed4c27d":"code","f070b76b":"code","51946c01":"code","320dcf89":"code","14dc99d6":"code","f88248a3":"code","4787e1bf":"code","cb8e03e4":"code","e912a5e2":"code","e839f1e4":"code","e1c899cc":"code","a3a84df4":"code","f1c0d9b0":"code","4b9cfefa":"code","5c4c447d":"code","1bda03ab":"code","a1bee5af":"code","f81e7559":"code","d15df391":"code","b87026ab":"code","215b2deb":"code","2fa8d732":"markdown","6ea9bd97":"markdown","c200dbac":"markdown","535a2b2e":"markdown","00ff51ac":"markdown","39007089":"markdown","dab316b3":"markdown","bddaf530":"markdown","ad6bbddf":"markdown","f880e4ad":"markdown","72d3d8f3":"markdown","359a2c00":"markdown","7acb7774":"markdown","734106b5":"markdown","c078321b":"markdown","354c51d4":"markdown","04dabd63":"markdown","cca8d752":"markdown","e7b03048":"markdown","2638ea90":"markdown","5717845c":"markdown","e7cabf08":"markdown","da8a7a16":"markdown","06969a94":"markdown","f9a45993":"markdown","99571482":"markdown","608228d4":"markdown","eacc06ea":"markdown","40511218":"markdown","f976f7b0":"markdown","a61ad991":"markdown","d1f05e1f":"markdown","1471ae2f":"markdown","e3b5d532":"markdown","f63dafe5":"markdown","379d8c28":"markdown","05beee11":"markdown","eac20826":"markdown","79424061":"markdown","6b070443":"markdown","be2fbbe8":"markdown","8eeb9a0e":"markdown","5ed65328":"markdown","e85b4f14":"markdown","086bb144":"markdown","239ddb25":"markdown","6e3a5e66":"markdown","c9f2ba79":"markdown","a81859a3":"markdown","2eb670dd":"markdown","f39d71b5":"markdown","212bc3dc":"markdown","c32325bf":"markdown","c5adcc09":"markdown","2bcff3f3":"markdown","6227d7b0":"markdown","230960ad":"markdown","f7ce1cb4":"markdown","e4ceb0bd":"markdown","4bc4c284":"markdown"},"source":{"6d76711a":"# Data Analysis\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import boxcox\n\n# Data Visualisation\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\nplt.style.use('dark_background')\n%matplotlib inline\nimport seaborn as sns\n\n# Data Modeling\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score,RandomizedSearchCV\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('Setup Complete')","913edb62":"train_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv',index_col='Id')\ntest_data = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv',index_col='Id')","15849997":"train_data.info()","60469b75":"train_data.describe()","eb161df5":"train_data.describe(include='O')","405a1faa":"missing_col = train_data.columns[train_data.isnull().sum()\/train_data.shape[0]*100>0]\nmissing_col = pd.DataFrame({'Columns':missing_col,\n              'Percentage':train_data[missing_col].isnull().sum()\/train_data.shape[0]*100})\nmissing_col.reset_index(drop=True,inplace=True)\nmissing_col.sort_values(by='Percentage',ascending=False,inplace=True)\nmissing_col","c5f3902b":"for col in missing_col.Columns:\n    data = train_data.copy()\n    data[col] = np.where(data[col].isnull() == True,1,0)\n    data.groupby(col)['SalePrice'].mean().plot.bar()\n    plt.title(col)\n    plt.show()","d839237b":"numerical_feature = [col for col in train_data.select_dtypes(exclude='O')]\nnumerical_feature","c49a8940":"train_data[numerical_feature].head(5)","0f233d56":"year_col = [col for col in numerical_feature if 'Yr' in col or 'Year'in col or 'Mo' in col]\nyear_col","350c5967":"for col in year_col:\n    data = train_data.copy()\n    if col != 'MoSold':\n        data[col] = 2021 - data[col]\n    sns.regplot(x=data[col],y=data.SalePrice)\n    plt.title(col)\n    plt.show()","a2d65424":"area_col = [col for col in numerical_feature if 'SF' in col or 'Area' in col]\narea_col","7e774fea":"for col in area_col:\n    data = train_data.copy()\n    sns.regplot(x=data[col],y=data['SalePrice'])\n    plt.title(col)\n    plt.show()\n   ","ab9453cc":"discrete_col = [col for col in numerical_feature if train_data[col].nunique() < 20 and \n                (col not in year_col and col not in area_col)]\ndiscrete_col","b027c1ce":"for col in discrete_col:\n    data = train_data.copy()\n    data.groupby(col)['SalePrice'].mean().plot.bar()\n    plt.title(col)\n    plt.show()","23167e6f":"categorical_col = [col for col in train_data.select_dtypes(include=['object','category'])]\ncategorical_col","42c9bf21":"for col in categorical_col:\n    data = train_data.copy()\n    data.groupby(col)['SalePrice'].mean().plot.bar()\n    plt.title(col)\n    plt.show()","b176dcbb":"continuous_col = [col for col in numerical_feature \n                 if (col not in area_col) and (col not in discrete_col) and (col not in year_col)]\ncontinuous_col","7752a5ca":"for col in continuous_col:\n    data = train_data.copy()\n    sns.regplot(x=data[col],y=data.SalePrice)\n    plt.title(col)\n    plt.show()\n    print(data[col].skew(),'||',data[col].nunique())","55f1ecd5":"corr = train_data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr,cmap='CMRmap')","0d5ef622":"# Printing shape of train data for future refference\ntrain_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv',index_col='Id')\nprint(train_data.shape)\n\n# Printing shape of test data for future refference\ntest_data = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv',index_col='Id')\nprint(test_data.shape)\n\n# Combining the two dataset\ncombine_data = pd.concat([train_data.drop('SalePrice',axis=1),test_data],axis=0)\n\n# printing the new dataset\nprint(combine_data.shape)\n\n# Renaming some columns for better understanding\ncombine_data.rename(columns={'EnclosedPorch':'EnclosedPorchSF','3SsnPorch':'3SsnPorchSF',\n                             'ScreenPorch':'ScreenPorchSF'},inplace=True)","709934b1":"# Getting Numerical Features\nnumerical_feature = [col for col in combine_data.select_dtypes(exclude='O')]\n\n# Getting Categorical Features\ncategorical_col = [col for col in combine_data.select_dtypes(include=['object','category'])]","3903a44e":"# Getting name of the missing values\nmissing_col = combine_data.columns[combine_data.isnull().sum()\/combine_data.shape[0]*100>0]\n\n# Create a DataFrame of those missing columns with the missing percentage\nmissing_col = pd.DataFrame({'Columns':missing_col,\n              'Percentage':combine_data[missing_col].isnull().sum()\/combine_data.shape[0]*100})\n\n# Drop default index\nmissing_col.reset_index(drop=True,inplace=True)\n\n# Create a list holding columns dtypes\nx = []\nfor col in missing_col.Columns:\n    x.append(combine_data[col].dtype)\n\n# Assigning the list to the new column Type\nmissing_col['Type'] = x\nmissing_col.sort_values(by='Percentage',ascending=False,inplace=True)\nmissing_col","5ee8169f":"categorical_missing = missing_col.loc[missing_col['Type'] == 'O']\nfor col in categorical_missing.Columns:\n    combine_data[col].fillna('NA',inplace=True)","91fb96bb":"combine_data.drop(missing_col.loc[missing_col.Percentage>40,'Columns'],inplace=True,axis=1)","2a15a19b":"combine_data.fillna(0,inplace=True)\nnumerical_missing = combine_data[numerical_feature].count() - combine_data[numerical_feature].astype(bool).sum()\nnumerical_missing = numerical_missing\/combine_data.shape[0]*100\nnumerical_missing = numerical_missing[numerical_missing>0]\nfor col in numerical_missing.index.to_list():\n    combine_data[col+'missing'] = np.where(combine_data[col] == 0,1,0)\n    combine_data[col] = np.where(combine_data[col] == 0,combine_data[col].median(),combine_data[col])\nnumerical_missing","9e0c7fda":"combine_data.drop(numerical_missing[numerical_missing>40].index.to_list(),axis=1,inplace=True)","bdd62011":"combine_data.isnull().sum().sum()","f5e046ce":"combine_data['BsmtFinUnfSF_r'] = combine_data['BsmtUnfSF']\/combine_data['TotalBsmtSF']","dea4ae25":"combine_data['GrLivLot_r'] = combine_data['GrLivArea']\/combine_data['LotArea']","581a2ded":"combine_data['1stFlrLot_r'] = combine_data['1stFlrSF']\/combine_data['LotArea']","cab62341":"combine_data['OverallGrade'] = combine_data['OverallCond'] + combine_data['OverallQual']","df9e8fc7":"combine_data.fillna(0,inplace=True)","3e4bfe2c":"# getting updated numerical features\nnumerical_feature = [col for col in combine_data.select_dtypes(exclude='O')\n if 'missing' not in col and '_r' not in col]\n\n# getting updated catagorical features\ncategorical_feature = [col for col in combine_data.select_dtypes(include='O')] \n\n# Getting DateTime Features\nyear_col = [col for col in numerical_feature if 'Yr' in col or 'Year'in col]\n\n# Getting Area Features\narea_col = [col for col in numerical_feature if 'SF' in col or 'Area' in col]\n# Getting Discrete Features\n\ndiscrete_col = [col for col in numerical_feature if combine_data[col].nunique() < 20 and \n                (col not in year_col and col not in area_col)]\n# Getting Continunous Features\n\ncontinuous_col = [col for col in numerical_feature \n                 if (col not in area_col) and (col not in discrete_col) and\n                  (col not in year_col)]","b98a31c6":"for col in discrete_col:\n    # checking null values\n    print('print null Values:',combine_data[col].isnull().sum())\n\n    # checking for if anything is abnormal\n    print('Value Count')\n    print(combine_data[col].value_counts())\n    print('='*30)","71608fbd":"for col in categorical_feature:\n    temp = combine_data[col].value_counts()\n    temp = temp[temp <= combine_data.shape[0]*0.01].index\n    for x in temp.to_list():\n            combine_data[col].replace({x:'Rare'},inplace=True)","578bb271":"for col in categorical_feature:\n    print(col)\n    print('-'*30)\n    # checking null values\n    print('print null Values:',combine_data[col].isnull().sum())\n\n    # checking for if anything is abnormal\n    print('Value Count')\n    print(combine_data[col].value_counts())\n    print('='*30)\n    combine_data = combine_data.join(pd.get_dummies(combine_data[col],\n                                                    prefix=col,drop_first=True))","082ec7bf":"for col in year_col:\n    # create difference\n    combine_data[col] = 2021 - combine_data[col]\n    print(col)\n    print('-'*30)\n    # checking null values\n    print('print null Values:',combine_data[col].isnull().sum())","9ce99a79":"for col in area_col:\n    ''' # Changing <=0 values to 1 for boxcox operation\n    combine_data.loc[combine_data[col]<=0,col] = 1\n    \n    # Applying boxcox\n    combine_data[col] = pd.Series(boxcox(combine_data[col])[0])\n    \n    # Fillna with mean value\n    combine_data[col].fillna(combine_data[col].mean(),inplace=True)\n    \n    # Calculate IOR\n    ior = combine_data[col].quantile(.75) - combine_data[col].quantile(.25)\n    \n    # Setting lower and upper bound\n    lower_b = combine_data[col].quantile(.25) - ior*1.5\n    upper_b = combine_data[col].quantile(.75) + ior*1.5\n    \n    # Detecting outliers and remove it \n    combine_data[col] = np.where(combine_data[col] < lower_b,lower_b,combine_data[col])\n    combine_data[col] = np.where(combine_data[col] > upper_b,upper_b,combine_data[col])\n    '''\n\n    print(col)\n    print('-'*30)\n    # checking null values\n    print('print null Values:',combine_data[col].isnull().sum())\n","095ffa5e":"for col in continuous_col:\n    '''# Changing <=0 values to 1 for boxcox operation\n    combine_data.loc[combine_data[col]<=0,col] = 1\n    \n    # Applying boxcox\n    combine_data[col] = pd.Series(boxcox(combine_data[col])[0])\n    \n    # Fillna with mean value\n    combine_data[col].fillna(combine_data[col].mean(),inplace=True)\n    \n    # Calculate IOR\n    ior = combine_data[col].quantile(.75) - combine_data[col].quantile(.25)\n    \n    # Setting lower and upper bound\n    lower_b = combine_data[col].quantile(.25) - ior*1.5\n    upper_b = combine_data[col].quantile(.75) + ior*1.5\n    \n    # Detecting outliers and remove it \n    combine_data[col] = np.where(combine_data[col] < lower_b,lower_b,combine_data[col])\n    combine_data[col] = np.where(combine_data[col] > upper_b,upper_b,combine_data[col])'''\n\n    print(col)\n    print('-'*30)\n    # checking null values\n    print('print null Values:',combine_data[col].isnull().sum())\n    ","afa92dfc":"# train data\nprep_train_data = combine_data.loc[:train_data.shape[0]]\nprep_train_data = prep_train_data.join(train_data.SalePrice)\n\n# test data\nprep_test_data = combine_data.loc[train_data.shape[0]+1:]\n# Saving in a csv file\nprep_train_data.to_csv('Preprocessed_trained_data.csv')\nprep_test_data.to_csv('Preprocessed_test_data.csv')","bed4c27d":"train_data = pd.read_csv('Preprocessed_trained_data.csv',index_col='Id')\ntest_data = pd.read_csv('Preprocessed_test_data.csv',index_col='Id')","f070b76b":"X = train_data.drop('SalePrice',axis=1)\ny = train_data.SalePrice\n\n# drop categorical types\nX.drop(categorical_feature,axis=1,inplace=True)\ntest_data.drop(categorical_feature,axis=1,inplace=True)\n","51946c01":"train_data.shape","320dcf89":"corr = X.corrwith(y)\ncorr = abs(corr)\ncorr = corr.sort_values(ascending=False)\nf_list2 = corr[corr>0.06]","14dc99d6":"# Selecting features\nselected_cols = f_list2.index.to_list()\n\n# copy the selected feaures\nX_sel = X[selected_cols]\ntest_sel = test_data[selected_cols]","f88248a3":"len(selected_cols)","4787e1bf":"model = XGBRegressor(n_estimators=1000,eta=0.05,verbosity=0)\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nxg_mae = sc.mean()\nprint(xg_mae)","cb8e03e4":"model = LinearRegression()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nlr_mae = sc.mean()\nprint(lr_mae)","e912a5e2":"model = Lasso()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nl_mae = sc.mean()\nprint(l_mae)","e839f1e4":"model = Ridge()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nr_mae = sc.mean()\nprint(r_mae)","e1c899cc":"model = ElasticNet()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nel_mae = sc.mean()\nprint(el_mae)","a3a84df4":"model = RandomForestRegressor()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nrf_mae = sc.mean()\nprint(rf_mae)","f1c0d9b0":"model = SVR()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nsvr_mae = sc.mean()\nprint(svr_mae)","4b9cfefa":"model = KNeighborsRegressor(n_neighbors=3)\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\nknn_mae = sc.mean()\nprint(knn_mae)","5c4c447d":"model = DecisionTreeRegressor()\nsc = -1*cross_val_score(model,X_sel,y,cv=10,scoring='neg_mean_absolute_error')\ndt_mae = sc.mean()\nprint(dt_mae)","1bda03ab":"rank = pd.DataFrame({'Model':['XGBoost','LinearRegression','Lasso','Ridge','ElasticNet',\n                              'RandomForestRegressor','SVR','KNeighborsRegressor','DecisionTree']\n                    ,'Score':[xg_mae,lr_mae,l_mae,r_mae,el_mae,rf_mae,svr_mae,knn_mae,dt_mae]})\nrank.sort_values(by='Score')","a1bee5af":"model = XGBRegressor()\nparam = [{'n_estimators':[100,200,300,400,500,1000,1500],\n          'max_depth':[3,4,5,6,7,8,9],\n          'eta':[0.02,0.03,0.04,0.05,0.06,0.07],\n          'gamma':[0.1,0.2,0.3,0.5,0.7,0.9],\n          'min_child_weight':[1,2,3,4,5,6],\n          'subsample':[0.3,0.5,0.7,0.9,1],\n          'verbosity':[0]\n         }]\ngrd = RandomizedSearchCV(estimator=model,\n                        param_distributions=param,\n                        cv=10,\n                        scoring='neg_mean_absolute_error')\n#grd.fit(X_sel,y)","f81e7559":"'''print(grd.best_score_)\nprint(grd.best_estimator_)\nXGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, eta=0.04, gamma=0.1,\n             gpu_id=-1, importance_type='gain', interaction_constraints='',\n             learning_rate=0.0399999991, max_delta_step=0, max_depth=6,\n             min_child_weight=4, missing=nan, monotone_constraints='()',\n             n_estimators=400, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n             tree_method='exact', validate_parameters=1, verbosity=0)'''","d15df391":"model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, eta=0.04, gamma=0.1,\n             gpu_id=-1, importance_type='gain', interaction_constraints='',\n             learning_rate=0.0399999991, max_delta_step=0, max_depth=6,\n             min_child_weight=4, monotone_constraints='()',\n             n_estimators=400, n_jobs=8, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=0.5,\n             tree_method='exact', validate_parameters=1, verbosity=0)\nmodel.fit(X_sel,y)","b87026ab":"pred = model.predict(test_sel)","215b2deb":"output = pd.DataFrame({'Id':prep_test_data.index,'SalePrice':pred})\noutput.to_csv('demo.csv',index=False)","2fa8d732":"### Finding Missing Values\nprinting missing value percentage","6ea9bd97":"## LinearRegression","c200dbac":"###### Observtaion :\n* Alley, MasVnrType, MasVnrArea, Fence, MiscFeature has slighlty high price  and \n* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageQual, GarageCond, PoolQC has less price.","535a2b2e":"#### Selecting data with final features","00ff51ac":"#### Merging test and train data for feature engineering","39007089":"### Saving the preprocessed data ","dab316b3":"## **Model Ranking**","bddaf530":"#### Categorical Features","ad6bbddf":"###### Observation :\n* As year difference increase the sale price decrease\n* MoSold and YrSold don't have any effect on Sale Price ","f880e4ad":"### Finding Categorical Features","72d3d8f3":"#### Imputing numerical missing values","359a2c00":"#### Load the preprocessed data","7acb7774":"#### Dropping some missing values","734106b5":"## ElasticNet","c078321b":"## Lasso","354c51d4":"### **Imputing Missing values**","04dabd63":"## **Data Cleaning**","cca8d752":"#### Create ground floor and lot area ratio","e7b03048":"# **Feature Engineering**\n* Imputing Missing values \n* Create Temporal Features\n\n## **Data Cleaning**\n\n* Detect Outliers and Remove Them\n* Encode Categorical Variable\n* standardise The Values of the variables to the same range (if required)","2638ea90":"#### Create basment unfinished and total basement ratio","5717845c":"### Finding Area Features","e7cabf08":"####  Area Features","da8a7a16":"# **Features Selection**","06969a94":"#### Discreate Features ","f9a45993":"#### Finding the correlation with target feature","99571482":"### XGBOOST","608228d4":"#### Overall Grade","eacc06ea":"#### Continuous Features","40511218":"## SVR","f976f7b0":"#### let see the the data","a61ad991":"### **Final Data Fitting**","d1f05e1f":"###### Observation :\n* All features related to porch and Misc are not shown any effect on SalePrice","1471ae2f":"### Finding All Numerical Values","e3b5d532":"### Finding Discrete Features","f63dafe5":"#### Dropping some numerical features","379d8c28":"### Lets See train_data columns and other information","05beee11":"#### DateTime Features","eac20826":"## KNeighborsRegressor\n","79424061":"#### See missing value types","6b070443":"#### Create X and y","be2fbbe8":"### Finding Continuous Feature","8eeb9a0e":"#### Some of the above has formed 0 \/ 0 as resulting nan","5ed65328":"# **Housing Price Assignment**\n\nHousing Prices Competition for Kaggle Learn Users","e85b4f14":"# **Data Modeling**","086bb144":"### Loading the data","239ddb25":"### Finding DateTime Feature ( Year )","6e3a5e66":"#### Importing Important Libaries","c9f2ba79":"    major missing value will be fill with separate value and minor missing value will be fill with mode value","a81859a3":"## DecisionTreeRegressor","2eb670dd":"###### Observation :\n* LowQualFinSf, PoolArea, BsmtFinSF2 dont much effect on the target feature  \n* There are many outliers we tackle that in feature Engineering part","f39d71b5":"## Ridge","212bc3dc":"### **Create Temporal Features**","c32325bf":"#### Imputing categorical missing values","c5adcc09":"#### Let see are these missing value correlation with target feature","2bcff3f3":"# **Exploratory Data Analysis** \n* Finding missing value \n* Correlation with missing value with target value\n* All numerical values\n* Distribution of continous values\n* Finding categorical values\n* Cardinality of Categorical Variable\n* Finding Outliers\n* Relationship between independent and dependent feature","6227d7b0":"#### Replace rare categories ","230960ad":"## RandomForestRegressor","f7ce1cb4":"### Correlation Plot ","e4ceb0bd":"#### Create 1st floor and lot area ratio","4bc4c284":"##### lets check if there any missing columns left"}}