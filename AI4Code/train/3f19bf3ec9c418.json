{"cell_type":{"2be0262d":"code","7fe98422":"code","3056dc46":"code","d184d71a":"code","d0aa31c2":"code","623ec207":"code","a457eea8":"code","5ba954cd":"code","580e7e1c":"code","44f921c0":"code","39b35c9c":"code","bf1e22c4":"code","7146ae76":"code","657aaae2":"code","c0bc3243":"code","f5ee7f33":"code","5d4d9f8c":"code","007b6a85":"code","2e199d73":"code","60dac644":"code","a9ce98a6":"code","18bf72ff":"code","1bfe5f55":"code","e0bfdbfc":"code","e1838156":"code","6097fce6":"code","ac07a615":"code","c0ac33b8":"code","8f4e1479":"code","76719ce3":"code","36dbbdec":"code","93bcc431":"code","b11215dd":"code","ce2063cb":"code","520945b3":"code","6d77a22c":"code","1ba66bb8":"code","df051198":"code","654c84d0":"code","08f2f70f":"code","54e38d97":"code","40f4dfad":"code","f190cd3b":"code","ca5711cb":"code","0a83ac77":"code","63175578":"code","91029eda":"code","550d12c5":"code","84ee02d3":"code","37333597":"markdown","1bc04445":"markdown","d1ee8994":"markdown","14a7fadd":"markdown","70c9eedd":"markdown","7d7617b8":"markdown","90051e55":"markdown","ef4f6728":"markdown","1a94acb9":"markdown","032a5631":"markdown","d9fb4b7e":"markdown","cd130616":"markdown","b8216a4b":"markdown","009fdfe3":"markdown"},"source":{"2be0262d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom collections import Counter\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom scipy import stats\nimport optuna\nimport sys\nimport os\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\npd.options.mode.chained_assignment = None \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fe98422":"import warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)","3056dc46":"train_df=pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/train.csv\")\ntest_df=pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/test.csv\")\ntrain_df.head()","d184d71a":"train_df=train_df.drop(['id'], 1)\ntrain_df.head()","d0aa31c2":"test_df.head()","623ec207":"train_df.info()","a457eea8":"cate_features=['audio_mode','key','time_signature']\nnominal_features=['song_duration_ms','instrumentalness','acousticness','danceability','audio_valence','liveness','loudness','speechiness','tempo','energy']\ntarget_feature=['song_popularity']\nfeatures=nominal_features+cate_features+target_feature","5ba954cd":"plt.figure(figsize =(25,12))\nfor j,i in enumerate(nominal_features):\n    plt.subplot(5,3,j+1)\n    sns.set_theme('notebook')\n    sns.boxplot(data = train_df ,x = i)\nplt.tight_layout()\nplt.show()","580e7e1c":"fig, axes = plt.subplots(5, 3, figsize=(20, 15))\nsns.histplot(train_df[\"song_duration_ms\"],kde=True, ax=axes[0, 0])\nsns.histplot(train_df[\"acousticness\"],kde=True, ax=axes[0, 1])\nsns.histplot(train_df[\"danceability\"], kde=True,ax=axes[0, 2])\nsns.histplot(train_df[\"audio_valence\"],kde=True, ax=axes[1, 0])\nsns.histplot(train_df[\"liveness\"],kde=True, ax=axes[1,1])\nsns.histplot(train_df[\"loudness\"],kde=True, ax=axes[1,2])\nsns.histplot(train_df[\"speechiness\"],kde=True, ax=axes[2, 0])\nsns.histplot(train_df[\"tempo\"],kde=True, ax=axes[2,1])\nsns.histplot(train_df[\"audio_valence\"],kde=True, ax=axes[2, 2])\nsns.histplot(train_df[\"audio_mode\"],kde=False, ax=axes[3, 0])\nsns.histplot(train_df[\"key\"],kde=False, ax=axes[3,1])\nsns.histplot(train_df[\"time_signature\"],kde=False, ax=axes[3, 2])\nsns.histplot(train_df[\"instrumentalness\"],kde=True, ax=axes[4, 0])\nsns.histplot(train_df[\"song_popularity\"],kde=True, ax=axes[4, 1])\nsns.histplot(train_df[\"energy\"],kde=True, ax=axes[4, 1])\nfig.show()","44f921c0":"sns.histplot(train_df[\"loudness\"],kde=True)","39b35c9c":"for x in nominal_features:\n    print(x,\" : \",train_df[x].skew())","bf1e22c4":"skewed_features=['acousticness','liveness','instrumentalness','loudness','speechiness']","7146ae76":"#log transformation\nfor x in skewed_features:\n    log_inst=np.log(train_df[x])\n    print(x,\"  \",log_inst.skew())\n    if not np.isnan(log_inst.skew()):\n        train_df[x]=log_inst","657aaae2":"data= np.array(train_df[\"loudness\"])\ndata=data.reshape(-1, 1)\nfrom sklearn.preprocessing import PowerTransformer\npower = PowerTransformer(method='yeo-johnson', standardize=True)\ndata_trans = power.fit_transform(data)\ndata_trans=data_trans.reshape(1, -1)\ni=0\nfor num in data_trans[0]:\n    train_df[\"loudness\"][i]=num\n    i=i+1","c0bc3243":"print(train_df[\"loudness\"].skew())\nsns.histplot(train_df[\"loudness\"],kde=True)","f5ee7f33":"train_df.isnull().sum().sort_values(ascending = False)","5d4d9f8c":"msno.bar(train_df)","007b6a85":"msno.matrix(train_df)","2e199d73":"train_df.shape","60dac644":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","a9ce98a6":"it_imputer  = IterativeImputer(max_iter=1000)\ntrain_imp = it_imputer.fit_transform(train_df[features])\ntrain_df = pd.DataFrame(train_imp, columns=features)","18bf72ff":"train_df.shape","1bfe5f55":"msno.matrix(train_df)","e0bfdbfc":"train_df['song_duration_ms'].fillna(train_df['song_duration_ms'].mean(), inplace=True)\ntrain_df['liveness'].fillna(train_df['liveness'].mean(), inplace=True)\ntrain_df['danceability'].fillna(train_df['danceability'].mean(), inplace=True)\ntrain_df['acousticness'].fillna(train_df['acousticness'].mean(), inplace=True)\ntrain_df['instrumentalness'].fillna(train_df['instrumentalness'].mean(), inplace=True)\ntrain_df['energy'].fillna(train_df['energy'].mean(), inplace=True)\ntrain_df['loudness'].fillna(train_df['loudness'].mean(), inplace=True)\ntrain_df['key'].fillna(train_df['key'].median(), inplace=True)","e1838156":"f, ax = plt.subplots(figsize=(20, 20))\ncorr = train_df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool),annot=True, cmap=sns.diverging_palette(220, 10, as_cmap=True),square=True, ax=ax)","6097fce6":"cor_matrix = train_df.corr().abs()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.7)]\nprint(to_drop)","ac07a615":"data=train_df.drop(['song_popularity'],axis=1)\ntarget=train_df['song_popularity']","c0ac33b8":"stand_scaler = preprocessing.StandardScaler()\nscaler = stand_scaler.fit(data)\ntrain_scaled = scaler.transform(data)\ndata = pd.DataFrame(train_scaled, columns = data.columns)\ndata.shape","8f4e1479":"data.head()","76719ce3":"skewed_features=['acousticness','liveness','instrumentalness','speechiness']\n#log transformation\nfor x in skewed_features:\n    log_inst=np.log(test_df[x])\n    print(x,\"  \",log_inst.skew())\n    if not np.isnan(log_inst.skew()):\n        test_df[x]=log_inst\n        \ndata_l= np.array(test_df[\"loudness\"])\ndata_l=data_l.reshape(-1, 1)\nfrom sklearn.preprocessing import PowerTransformer\npower = PowerTransformer(method='yeo-johnson', standardize=True)\ndata_trans = power.fit_transform(data_l)\ndata_trans=data_trans.reshape(1, -1)\ni=0\nfor num in data_trans[0]:\n    test_df[\"loudness\"][i]=num\n    i=i+1\n","36dbbdec":"features=test_df.columns.to_list()","93bcc431":"it_imputer  = IterativeImputer(max_iter=1000)\ntest_imp = it_imputer.fit_transform(test_df[features])\ntest_df = pd.DataFrame(test_imp, columns=features)","b11215dd":"id=test_df['id']\ntest_df=test_df.drop(['id'],1)\ntest_df.head()","ce2063cb":"test_scaled = scaler.transform(test_df)\ntest_scaled_df = pd.DataFrame(test_scaled, columns = test_df.columns)\ntest_scaled_df.describe()","520945b3":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n#data, x_test, target, y_test  = train_test_split(data, target, test_size=0.25, random_state=0,shuffle=True, stratify=target)\n","6d77a22c":"import optuna \nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\nfrom xgboost import XGBClassifier\nimport joblib","1ba66bb8":"def objective(trial: Trial,X,y) -> float:\n    \n    joblib.dump(study, 'study.pkl')\n\n    param = {\n                \"n_estimators\" : trial.suggest_int('n_estimators', 0, 1000),\n                'max_depth':trial.suggest_int('max_depth', 2, 25),\n                'reg_alpha':trial.suggest_int('reg_alpha', 0, 5),\n                'reg_lambda':trial.suggest_int('reg_lambda', 0, 5),\n                'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n                'gamma':trial.suggest_int('gamma', 0, 5),\n                'learning_rate':trial.suggest_loguniform('learning_rate',0.005,0.5),\n                'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n                'nthread' : -1\n            }\n    \n    model = XGBClassifier(**param)\n    \n    return cross_val_score(model, X, y, cv=3).mean()","df051198":"study = optuna.create_study(direction='maximize',sampler=TPESampler())\nstudy.optimize(lambda trial : objective(trial,data,target),n_trials= 50)","654c84d0":"print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))\nparams=study.best_trial.params","08f2f70f":"hist = study.trials_dataframe()\nhist.head()","54e38d97":"optuna.visualization.plot_slice(study)","40f4dfad":"optuna.visualization.plot_parallel_coordinate(study)","f190cd3b":"## Compute `class_weights` using sklearn\ncls_weight = (target.shape[0] - np.sum(target)) \/ np.sum(target)","ca5711cb":"xgb_tuned = XGBClassifier(**params, scale_pos_weight=cls_weight,random_state=0, n_jobs=-1)\nxgb_tuned.fit(data, target)\n","0a83ac77":"pred=xgb_tuned.predict (test_scaled_df)","63175578":"sub_df=pd.read_csv(\"\/kaggle\/input\/song-popularity-prediction\/test.csv\")","91029eda":"sub_df.head()","550d12c5":"newdf=pd.DataFrame(zip(sub_df['id'], pred),columns = ['id','song_popularity'])","84ee02d3":"newdf.to_csv(\"submission.csv\",encoding='utf-8', index=False)","37333597":"<li>song_duration_ms, liveness, key, danceability, acousticness, instrumentalness, energy, loudness columns have Null values <\/li>\n<li>Replacing key with median as it is categorical variable <\/li>\n<li>Remaining columns will be replaced with Mean value <\/li>","1bc04445":"### 1.4 Normalization","d1ee8994":"Below Features have outliers:\n* song_duration_ms  \n* instrumentalness \n* liveness \n* loudness \n* speechiness \n* tempo","14a7fadd":"* !pip install missingno\n* !pip install outlier_utils\n* !pip install optuna","70c9eedd":"## 2. Model Building","7d7617b8":"#### Handling Highly Skewed Data\nSkewed Features\n* acousticness  \n* liveness  \n* instrumentalness  \n* loudness  \n* speechiness  \n<br\/><br\/>\nLog Transformation doesn't work on negatively skewed data","90051e55":"### 1.1 Skew Data in Train","ef4f6728":"## 1. Data Preprocessing ","1a94acb9":"https:\/\/www.kaggle.com\/para24\/xgboost-stepwise-tuning-using-optuna","032a5631":"### 1.3 Data Corelation","d9fb4b7e":"1. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean.  <\/br>\n1. If the skewness is between -0.5 and 0.5, the data are fairly symmetrical. If the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed. If the skewness is less than -1 or greater than 1, the data are highly skewed. <\/br>\n1. Data needs Transformation <\/br>\n1. For More info <\/br>\n* https:\/\/opendatascience.com\/transforming-skewed-data-for-machine-learning\/\n* https:\/\/towardsdatascience.com\/top-3-methods-for-handling-skewed-data-1334e0debf45","cd130616":"### 1.2 Null value handling","b8216a4b":"# Test Data preparation","009fdfe3":"# 1. Exploratory Data Analysis"}}