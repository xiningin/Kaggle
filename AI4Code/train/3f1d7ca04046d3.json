{"cell_type":{"0c4ca7bd":"code","646096de":"code","58f1e9c0":"code","ea7c8894":"code","7b1b4c58":"code","bd67355a":"code","cf8e643a":"code","d8c78c99":"code","d1d3d09c":"code","8b3e462b":"code","5638e336":"code","b8f42977":"code","80047ac6":"code","cb1a490b":"code","99250e50":"code","cb831f0a":"code","f4c76a88":"code","afa24623":"code","28dafa81":"code","fb2e280f":"code","73786e72":"code","4d2a177b":"code","e5826f06":"code","aac6d2fe":"code","7287bc60":"code","83a9347f":"code","f6d30f2b":"code","b8c019b3":"code","2f33aa23":"code","9f3518fc":"code","8c76a25a":"code","86d7034c":"code","9686e54b":"code","12c6bdf7":"code","81741583":"markdown","3dcb1800":"markdown","e13b6fed":"markdown","8ee11b15":"markdown","0ca4d247":"markdown","69753df0":"markdown","c8bd08eb":"markdown","d9d1090d":"markdown","c0a298d2":"markdown","4618600f":"markdown","df187480":"markdown","8f1ec151":"markdown","73caa0f0":"markdown","d5ebbbf3":"markdown","39c01f88":"markdown","f3e6d025":"markdown","b12a4eb5":"markdown","6001bcf7":"markdown","343591f4":"markdown","f65beeee":"markdown"},"source":{"0c4ca7bd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style='white', context='notebook', palette='deep')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","646096de":"from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\nfrom collections import Counter\n\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","58f1e9c0":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","ea7c8894":"print(data.shape)","7b1b4c58":"data.describe().round(decimals=2)","bd67355a":"features = data.columns.values","cf8e643a":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    \n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers ","d8c78c99":"# Potential outlier\n# Select observations containing more than 4 outliers\ndata.loc[detect_outliers(data, 4, features)]","d1d3d09c":"data = data.fillna(np.nan)\ndata.isnull().sum()","8b3e462b":"# Correlation matrix\nplt.figure(figsize=(20, 15))\nsns.heatmap(data.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")","5638e336":"data.corr()['Class'].sort_values(ascending=False)","b8f42977":"def most_corr(param, n):\n    class_corr = data.corr()[param].sort_values(ascending=False)\n    list_class = []\n    for i in features:\n        if(np.abs(class_corr[i]) >= n):\n           list_class.append(i)\n    return list_class","80047ac6":"most_corr('Class', 0.1) # Select features with correlation higher than 0.1 (positive correlation) or lower than -0.1 (negative correlation)","cb1a490b":"# Plot skew\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(features)):\n    fig.add_subplot(8,4,i+1)\n    sns.distplot(data[features[i]], kde_kws={'bw': 0.1})\n    plt.title('Skew : %.2f' % data[features[i]].skew())\n    \nplt.tight_layout()\nplt.show()","99250e50":"# Univariate analysis - boxplot\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(features)):\n    fig.add_subplot(8,4,i+1)\n    sns.boxplot(y=data[features[i]])\n    \nplt.tight_layout()\nplt.show()","cb831f0a":"# Bivariate analysis - boxplot\nfig = plt.figure(figsize=(12,18))\nfor i in range(len(features)):\n    fig.add_subplot(8,4,i+1)\n    sns.boxplot(y=data[features[i]], x=data['Class'])\n    \nplt.tight_layout()\nplt.show()","f4c76a88":"attributes_select = ['V1', 'V3', 'V4', 'V7', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18']\ndata.loc[detect_outliers(data, 4, attributes_select)]['Class'].value_counts()","afa24623":"# Count Class values\ndata['Class'].value_counts()","28dafa81":"attributes = ['V1', 'V3', 'V4', 'V7', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18', 'Class']\ndataset = data[attributes]","fb2e280f":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(dataset, test_size=0.2)","73786e72":"train['Class'].value_counts()","4d2a177b":"test['Class'].value_counts()","e5826f06":"X_train = train.drop(['Class'], axis = 1)\nY_train = train['Class']\n\nX_test = test.drop(['Class'], axis = 1)\nY_test = test['Class']","aac6d2fe":"kfold = StratifiedKFold(n_splits=10)\n\n# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nalgorithm = [\"DecisionTree\",\"AdaBoost\",\n             \"ExtraTrees\", \"LogisticRegression\"]\n\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(LogisticRegression(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n    \n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\n        \"CrossValMeans\":cv_means,\n        \"CrossValerrors\": cv_std,\n        \"Algorithm\": algorithm\n        })","7287bc60":"# Plot score of cross validation models\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\", data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","83a9347f":"cv_res","f6d30f2b":"from sklearn.metrics import plot_confusion_matrix","b8c019b3":"# Decision Tree\nDTC = DecisionTreeClassifier(random_state=random_state)\n\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'class_weight': ['balanced', {0:1,1:10}, {0:1,1:100}, {0:0.5, 1:289}]\n}\n\n\ngsDTC = GridSearchCV(DTC, param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsDTC.fit(X_train,Y_train)\n\nDTC_best = gsDTC.best_estimator_\n\n# Best score\ngsDTC.best_score_\n\n# Plot Confusion Matrix\nplot_confusion_matrix(DTC_best, X_test, Y_test)","2f33aa23":"DTC_best","9f3518fc":"# Extra Trees Classifier\nETC = ExtraTreesClassifier(random_state=random_state)\n\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'class_weight': ['balanced', {0:1,1:10}, {0:1,1:100}, {0:0.5, 1:289}]\n}\n\n\ngsETC = GridSearchCV(ETC, param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsETC.fit(X_train,Y_train)\n\nETC_best = gsETC.best_estimator_\n\n# Best score\ngsETC.best_score_\n\n# Plot Confusion Matrix\nplot_confusion_matrix(ETC_best, X_test, Y_test)","8c76a25a":"ETC_best","86d7034c":"# LogisticRegression\nLRC = LogisticRegression()\n\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'C': [0.001, 0.01, 0.1, 1],\n    'class_weight': ['balanced', {0:1,1:10}, {0:1,1:100}, {0:0.5, 1:289}]\n}\n\n\ngsLRC = GridSearchCV(LRC, param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsLRC.fit(X_train,Y_train)\n\nLRC_best = gsLRC.best_estimator_\n\n# Best score\ngsLRC.best_score_\n\n# Plot Confusion Matrix\nplot_confusion_matrix(LRC_best, X_test, Y_test)","9686e54b":"LRC_best","12c6bdf7":"# Ada Boost Classifier\nADB = AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state, class_weight={0: 1, 1: 10}),random_state=random_state)\n\n\n# Search grid for optimal parameters\nex_param_grid = {\n    'learning_rate': [0.001, 0.01, 0.1]\n}\n\n\ngsADB = GridSearchCV(ADB, param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsADB.fit(X_train,Y_train)\n\nADB_best = gsADB.best_estimator_\n\n# Best score\ngsADB.best_score_\n\n# Plot Confusion Matrix\nplot_confusion_matrix(ADB_best, X_test, Y_test)","81741583":"# 2. Load and Check Data","3dcb1800":"## 5.3 Hyperparameter Tuning","e13b6fed":"# Credit Card Fraud Prediction (Imbalanced Class)","8ee11b15":"## 2.3 Check for null and missing values","0ca4d247":"# 4. Feature Selection","69753df0":"## 5.1 Preparation","c8bd08eb":"> ## Notes :\n- Likely have correlation 'V1', 'V3', 'V4', 'V7', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18'\n- No need to reduce skewness\n- Imbalaced 'Class' data, must be handled\n- No need to drop outliers due to most of value 1 Class are in outlier","d9d1090d":"# Final","c0a298d2":"## 2.1 Load Data","4618600f":"> Notes\n- It seems the accuracy of the model is quite similar, but let's look at hyperparameter tuning with class weight and confusion matrix","df187480":"### The best model is Extratress classifier with lowest false negative and positive","8f1ec151":"## 5.2 Cross Validation Model","73caa0f0":"## 3.2 Categorical Analysis","d5ebbbf3":"> ## Notes :\n- Outlier is detected\n- No missing value","39c01f88":"## 2.2 Outlier Detection","f3e6d025":"# 5. Build Model","b12a4eb5":"# 3. Exploratory Data Analysis","6001bcf7":"## 3.1 Numerical Analysis","343591f4":"# 1. Import Libraries","f65beeee":"> Notes\n- Decision Tree and Ada Boost Classifier give similar result due to Ada Boost uses Decision Tree as Base estimators"}}