{"cell_type":{"611282e0":"code","c82d0ee3":"code","67317aac":"code","ced42914":"code","0bf1f758":"code","ca83b014":"code","cc68ec8c":"code","43264dac":"code","016652e7":"code","17da3126":"code","b571c953":"code","30579588":"code","a97b1492":"code","3715e565":"markdown","9f2a28e9":"markdown","0255d9ee":"markdown","ee9703b4":"markdown","c968857a":"markdown","3fcf3ccc":"markdown","9250b01e":"markdown","7d07b7a1":"markdown","0de45965":"markdown","2d41095a":"markdown"},"source":{"611282e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c82d0ee3":"!pip install tensorflow-datasets\n\n# Import TensorFlow Datasets\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport logging\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)\n\n\n# Helper libraries\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#enable eager execution for image.numpy() method\ntf.enable_eager_execution()\ntf.executing_eagerly()","67317aac":"dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']","ced42914":"train_dataset","0bf1f758":"class_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']","ca83b014":"num_train_examples = metadata.splits['train'].num_examples\nnum_test_examples = metadata.splits['test'].num_examples\nprint(\"Number of training examples: {}\".format(num_train_examples))\nprint(\"Number of test examples:     {}\".format(num_test_examples))","cc68ec8c":"print(metadata.supervised_keys)\n\n#Normalize function is getting Images, Label from the mnist metadata.supervised_keys\ndef normalize(images, labels):\n  images = tf.cast(images, tf.float32)\n  images \/= 255\n  return images, labels\n\n# The map function applies the normalize function to each element in the train\n# and test datasets. It is similar to df.apply()\ntrain_dataset =  train_dataset.map(normalize)\ntest_dataset  =  test_dataset.map(normalize)\n\n# The first time you use the dataset, the images will be loaded from disk\n# Caching will keep them in memory, making training faster\ntrain_dataset =  train_dataset.cache()\ntest_dataset  =  test_dataset.cache()","43264dac":"tf.executing_eagerly()","016652e7":"# Take a single image, and remove the color dimension by reshaping\nfor image, label in test_dataset.take(1):\n  break\nimage = image.numpy().reshape((28,28))\n\n# Plot the image - voila a piece of fashion clothing\nplt.figure()\nplt.imshow(image, cmap=plt.cm.binary)\n\n#Note the reason why we created class_names[] is because the label is a number in Dataset\nplt.xlabel(label)\nplt.colorbar()\nplt.grid(False)\nplt.show()","17da3126":"model = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n                           input_shape=(28, 28, 1)),\n    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n    tf.keras.layers.MaxPooling2D((2, 2), strides=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10,  activation=tf.nn.softmax)\n])","b571c953":"model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","30579588":"BATCH_SIZE = 32\nmodel_1 = model.fit(train_dataset.batch(BATCH_SIZE), epochs=5)","a97b1492":"model.save('Fashion_Classifier_CNN_Model.h5') ","3715e565":"Since the class names are not included with the dataset, store them here to use later when plotting the images:","9f2a28e9":"Also try out what google did in thier colab!!! \nhttps:\/\/colab.research.google.com\/github\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l03c01_classifying_images_of_clothing.ipynb#scrollTo=o_Dp8971McQ1","0255d9ee":"Assemble the CNN Model architecture with the no. of Nuerions and the activation function to be used!!!!","ee9703b4":"1) Naive training\n\nNOTE---\nValueError: Error when checking input: expected flatten_2_input to have 4 dimensions, but got array with shape (28, 28, 1)\n\nThe above error comes when you do - model_1 = model.fit(train_dataset, epochs=5)\n\nTo avoid this make use of BATCH_SIZE\n\nSo .... the 4 dimentions are - \na)BATCH_SIZE\nb)28\nc)28\nd)1\n","c968857a":"Refer https:\/\/github.com\/tensorflow\/tensorflow\/issues\/27519 if you are unable to run below code\n\n\nThe below code is taking 1 image as a 28x28 pixel value and then plotting it unsing plty","3fcf3ccc":"RESOURCE ==========\n* (CNN DETAILED EXPLAINATION) https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\n* https:\/\/github.com\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l03c01_classifying_images_of_clothing.ipynb\n* https:\/\/colab.research.google.com\/github\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l04c01_image_classification_with_cnns.ipynb#scrollTo=gut8A_7rCaW6","9250b01e":"You get details of DATA from METADATA variable when the mnist is loaded","7d07b7a1":"Load the dataset and fetch its training and testing data","0de45965":"We will now normalize the Pixel data (28x28 are there) . This will help correct any skew in data","2d41095a":"Compile the NN with a Loss Function and a Gradient Descent function and also metrics "}}