{"cell_type":{"23e3ab20":"code","bd9ac199":"code","ebca0fc1":"code","33a9aea9":"code","7179304c":"code","ed04f649":"code","ca431922":"code","b6fc3306":"code","5ebcce3d":"code","77792ceb":"code","23b7906d":"code","81d46bab":"code","1c4375a7":"code","9cfd341f":"code","bafe1e74":"code","bba087e5":"code","7b796503":"code","3c1f2633":"code","a2ce2bc9":"code","c23e77ec":"code","d18012de":"code","3e9e52ce":"code","66b8c9bb":"code","ae100e44":"code","20ffec4c":"code","ab6e0d8d":"code","f3475295":"code","f55fb370":"code","5f2dd7ea":"code","615c327a":"code","223fd014":"code","c185c533":"code","96b06da3":"code","fb4367df":"code","49258001":"code","1ef7b68a":"code","05ef477c":"code","bfed7f4c":"code","155758bb":"code","6b0fddaa":"code","d2d8f114":"code","8a892cdd":"code","02e456bb":"code","96e1a5be":"code","40ed6db0":"markdown","0bdf018f":"markdown","73ddade0":"markdown","81c80dde":"markdown","5180d1ea":"markdown","22d2d589":"markdown","2383e009":"markdown","32d860b0":"markdown","2cf9f35c":"markdown","c3142dfa":"markdown","8b41c350":"markdown","56a23289":"markdown","a5c4d5c2":"markdown"},"source":{"23e3ab20":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd9ac199":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n%matplotlib inline","ebca0fc1":"df=pd.read_csv(\"\/kaggle\/input\/adult-income-dataset\/adult.csv\")","33a9aea9":"df.head()","7179304c":"df.info()","ed04f649":"df.shape","ca431922":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","b6fc3306":"print('Workclass, ? numbers: ',df['workclass'][df['workclass']=='?'].count())\nprint('Percentage: {0:.2f}%'.format(df['workclass'][df['workclass']=='?'].count()\/(48842)*100))","5ebcce3d":"print('Occupation, ? numbers: ',df['occupation'][df['occupation']=='?'].count())\nprint('Percentage: {0:.2f}%'.format(df['occupation'][df['occupation']=='?'].count()\/(48842)*100))","77792ceb":"print('Native Country, ? numbers: ',df['native-country'][df['native-country']=='?'].count())\nprint('Percentage: {0:.2f}%'.format(df['native-country'][df['native-country']=='?'].count()\/(48842)*100))","23b7906d":"import statistics \nfrom statistics import mode \n  \ndef most_common(List): \n    return(mode(List))","81d46bab":"print(most_common(df['workclass']))\nprint(most_common(df['occupation']))\nprint(most_common(df['native-country']))","1c4375a7":"df['workclass']=df['workclass'].replace('?','Private')\ndf['occupation']=df['occupation'].replace('?','Prof-specialty')\ndf['native-country']=df['native-country'].replace('?','United-States')","9cfd341f":"df.head()","bafe1e74":"cols =['workclass', 'education','marital-status', 'occupation',\n               'relationship','race', 'gender', 'native-country', 'income'] \nfor i in cols:\n    print(i,':')\n    print('')\n    print(df[i].value_counts())\n    print('')","bba087e5":"#education category\ndf.education=df.education.replace(['Preschool','1st-4th','5th-6th','7th-8th','9th','10th','11th','12th'],'left')\ndf.education=df.education.replace('HS-grad','school')\ndf.education=df.education.replace(['Assoc-voc','Assoc-acdm','Prof-school','Some-college'],'higher')\ndf.education=df.education.replace('Bachelors','undergrad')\ndf.education=df.education.replace('Masters','grad')\ndf.education=df.education.replace('Doctorate','doc')","7b796503":"#marital status\ndf['marital-status']=df['marital-status'].replace(['Married-civ-spouse','Married-AF-spouse'],'married')\ndf['marital-status']=df['marital-status'].replace(['Never-married','Divorced','Separated','Widowed',\n                                                   'Married-spouse-absent'], 'not-married')","3c1f2633":"#income\ndf.income=df.income.replace('<=50K', 0)\ndf.income=df.income.replace('>50K', 1)","a2ce2bc9":"df.head()","c23e77ec":"df.describe()","d18012de":"plt.figure(figsize=(20,7))\nsns.countplot(x='age',data=df)\nsns.despine()\nplt.title('Age Distribution')","3e9e52ce":"px.histogram(df,x='age',color='gender',nbins=40)","66b8c9bb":"px.pie(df,values='educational-num',names='education',title='Percentage of Education',\n      color_discrete_sequence=px.colors.qualitative.G10)","ae100e44":"plt.figure(figsize=(15,7))\nsns.countplot(x='workclass',data=df)\nsns.despine()","20ffec4c":"plt.figure(figsize=(20,7))\nsns.countplot(x='occupation',data=df)\nsns.despine()","ab6e0d8d":"# fig=px.bar(df,x='occupation')\n# fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n# fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n# fig.show()","f3475295":"# fig=px.bar(df,x='relationship')\n# fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n# fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n# fig.show()","f55fb370":"# fig=px.bar(df,x='race')\n# fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n# fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n# fig.show()","5f2dd7ea":"sns.countplot(y=\"gender\",data=df)","615c327a":"# fig=px.bar(df,x='native-country')\n# fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n# fig.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n# fig.show()","223fd014":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","c185c533":"df1=df.copy()\ndf1=df1.apply(LabelEncoder().fit_transform)\ndf1.head()","96b06da3":"std_sclr=StandardScaler().fit(df1.drop('income',axis=1))","fb4367df":"X=std_sclr.transform(df1.drop('income',axis=1))\ny=df['income']","49258001":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","1ef7b68a":"forest=RandomForestClassifier(n_estimators=5,random_state=0)","05ef477c":"forest.fit(X_train,y_train)","bfed7f4c":"print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))","155758bb":"importance=forest.feature_importances_","6b0fddaa":"for i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,5))\nplt.bar([x for x in range(len(importance))], importance)\nplt.title('Feature Importance')\nplt.show()","d2d8f114":"gbrt=GradientBoostingClassifier(max_depth=1,learning_rate=1,random_state=0)\ngbrt.fit(X_train,y_train)","8a892cdd":"print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))","02e456bb":"importance=gbrt.feature_importances_","96e1a5be":"for i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.figure(figsize=(10,5))\nplt.bar([x for x in range(len(importance))], importance)\nplt.title('Gradient Boosting Classifier Feature Importances')\nplt.show()","40ed6db0":"## Random Forest Classifier","0bdf018f":"Our attributes have a lot of detailed category, let's generallize some of them neatly.","73ddade0":"# Basic Operations","81c80dde":"### Although there are no NaN cells, we can see that some attributes have '?' insted of string.\n\n...Actually these attributes are: Workclass, Occupation and Native Country. Let's see how many corrupted data are there.","5180d1ea":"# Visualization ","22d2d589":"We can see that Private, Prof-Speciality and United States are reasonable to replace '?' marks.","2383e009":"# Data Wrangling, Cleaning ","32d860b0":"# Conclusion\n\nTo sum up, we used two types of Decision Tress classifiers. Both, in my opinion, performed very well taking into consideration the relative massive quantity of data. By switching to dafault parameters in these algorithms, we have:\n\n* Random tree forest classifier: 0.844 accuracy on a test set, and\n* Gradient boosting machines: 0.864 accuracy on a test set.\n\nIt is also worth to mention some key points. \n\nFisrt, I have played with parameters of both algorithms in order to increase accuracy score of a test set. However, I could achieve only 1-2% of accuracy of previous scores. It is not limited for you to make a detailed anaylsis of those paremters in order to achieve at least 0.90 accuracy on a test set. \n\nSecond, many people claim that it is meaningless to indicate score of a training set. They say that it obvious that traing set will perform maximum, nearly 0.98-1.00. While, in my opinion, I think that we need to show it in order to understand how our test set works: whether it underfits or outfits our data. Having notable accuracy on a training set and minimum distinction from a test set, indicates us that our algorithms performed their best. \n\nFinally, there is a question of which algorithm is better. Let's have look to numbers we have. Obviously, Gradient boosting machines are better for 2.31% compating to Random forest trees classfier. However, as I mentioned playing with some variables and paramaters such as n_estimators or max_depth we can slightly improve our outputs. All in all, it is left as a will of oneslef's choice to use either of them. As Andreas C. M\u00fcller & Sarah Guido (authors of 'Introduction to Machine Learning' textbook) say, 'gradient boosted decision trees are among the most powerful and widely used models for supervised learning. Their main drawback is that they require careful tuning of the parameters and may take a long time to train. Similarly to other tree-based models, the algorithm works well without scaling and on a mixture of binary and continuous features. As with other tree-based models, it also often does not work well on high-dimensional sparse data.'\n\nP.S. if you have any suggestions please feel free to comment.\n","2cf9f35c":"# Decision Trees Application ","c3142dfa":"Percentage of these columns can be neglected but for me it's too sad to drop them. Therefore, I will replace these values with frequently occured value.","8b41c350":"## Gradient Boosting Classifier ","56a23289":"### Below is the outline of columns. Let me describe some of them in general way.\n\n### Categorical Columns:\n\n* workclass: categorical (a sector where person belongs);\n* education: categorical (highest educational degree);\n* marital-status: categorical\n* occupation: categorical (a field of work);\n* relationship: categorical (children, wife, etc);\n* race: categorical;\n* sex: categorical;\n* native-country: categorical (a person's country of origin).\n\n### Numerical Columns:\n\n* age: int;\n* education-num: int (how many year a person did study);\n* fnlwgt: int (final weight);\n* capital-gain: int (net worth);\n* capital-loss: int (money are spent: loans, mortgage, taxes, etc.);\n* hours-per-week: int.","a5c4d5c2":"# Introduction\n\n### Our key aim in this dataset is to predict income of a person whether he got low (less than 50,000 a year) or high (more than 50,000 a year) salary using Ensembles of Decision Trees, namely, Random forest trees and Gradient boosting machines for classification. Our objective is to correctly follow to a friend process in order to achieve good and correct model performance."}}