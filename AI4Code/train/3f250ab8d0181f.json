{"cell_type":{"091292ee":"code","55e067b4":"code","44d19a04":"code","69a973e2":"code","8f507a7e":"code","88f75176":"code","07a197fd":"code","6b81417e":"code","816c202e":"code","667bcb6d":"code","f2712ac3":"code","7cb5946f":"code","402152e5":"code","0e13e3e7":"code","74e9b41e":"code","5de91913":"code","c9aa9544":"markdown"},"source":{"091292ee":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55e067b4":"from efficientnet.keras import EfficientNetB4\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport pandas as pd\nimport numpy as np","44d19a04":"# index csv\ntrain_dex = pd.read_csv(\"train.csv\")\ntest_dex = pd.read_csv(\"test.csv\")\n\n#paths\ntrain_dir = os.path.join(os.path.abspath(''), 'copyDataset', 'train', 'train')\nvalid_dir = os.path.join(os.path.abspath(''), 'copyDataset', 'train', 'validation')\ntest_dir = os.path.join(os.path.abspath(''), 'dataset', 'test', 'test')","69a973e2":"#model attributes\nbatch_size = 4\nepochs = 20\nIMG_HEIGHT = 300\nIMG_WIDTH = 300","8f507a7e":"#dataset length\ntotal_train = 0\nfor i in os.listdir(train_dir):\n    total_train += len(os.listdir(os.path.join(train_dir, i)))\ntotal_val = 0\nfor i in os.listdir(valid_dir):\n    total_val += len(os.listdir(os.path.join(valid_dir, i)))","88f75176":"#data augmentation for training to prevent overfitting\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\ntrain_image_generator = ImageDataGenerator(rescale=1.\/255, \n                                           shear_range=0.2,\n                                           zoom_range=0.2,\n                                           horizontal_flip=True,\n                                           rotation_range=40,\n                                           preprocessing_function=preprocess_input\n                                           )\n\ntest_image_generator = ImageDataGenerator(rescale=1.\/255, preprocessing_function=preprocess_input)","07a197fd":"#flow from directory\ntrain_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=train_dir,\n                                                           shuffle=True,\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                           class_mode='categorical')\n\nvalid_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n                                                           directory=valid_dir,\n                                                           shuffle=True,\n                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                                           class_mode='categorical')","6b81417e":"#generating a callback to save training progress\ncheckpoint_path = \"ic_chkpt\/eff_netb3.ckpt\"\ncheckpoint_dir = os.path.dirname(checkpoint_path)\n\n# Create a callback that saves the model's weights\ncp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n                                                 save_weights_only=True,\n                                                 verbose=1,\n                                                 save_freq=10000)","816c202e":"#transfer learning with imagenet\neff_net = EfficientNetB4(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))","667bcb6d":"#freeze bottom layers\neff_net.trainable = False","f2712ac3":"#train these layers\nx = eff_net.output\nx = Flatten()(x)\nx = Dense(512, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\npredictions = Dense(42, activation=\"softmax\")(x)\nmodel = tf.keras.Model(inputs = eff_net.input, outputs = predictions)\n\nmodel.compile(tf.keras.optimizers.SGD(lr=0.0001, momentum=0.9), \n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n              metrics=['accuracy'])","7cb5946f":"# Train model\nhistory = model.fit(train_data_gen,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=valid_data_gen,\n              callbacks=[cp_callback],\n              initial_epoch=11\n              )","402152e5":"#PREDICTIONS\n#use validation image generator since its the same as test\ntest_generator = test_image_generator.flow_from_directory(directory=test_dir,\n                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n                                        batch_size=1,\n                                        class_mode=None,  # this means our generator will only yield batches of data, no labels\n                                        shuffle=False)","0e13e3e7":"test_step_size = test_generator.n\/\/1\ntest_generator.reset()\npred = model.predict(test_generator,\n                            steps = test_step_size,\n                            verbose=1)","74e9b41e":"#padding zeroes for single digit class labels (\"1\" -> \"01\")\ndef zero_inserter(x):\n    if len(str(x)) < 2:\n        return (\"0\" + str(x))\n    else:\n        return (str(x))","5de91913":"predicted_class_indices=np.argmax(pred,axis=1)\nlabels = (train_data_gen.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nfilenames=test_generator.filenames\n\nresult_df=pd.DataFrame({\"filename\":filenames,\n                      \"category\":predictions})\n\nresult_df['filename'] = result_df['filename'].apply(lambda x: x.split(\"\\\\\")[1])\n\nactual_test = pd.read_csv(\"test.csv\")\nactual_test.head()\nactual_test_list = list(actual_test['filename'])\n\nresult_df[\"category\"] = result_df.category.apply(zero_inserter)\n\nresult_df[result_df['filename'].isin(actual_test_list)].to_csv(\"results_effnetb4.csv\", index=False)","c9aa9544":"Simple implementation of efficientnetb4 (73% accuracy) <br>\nNo ensemble learning. <br>\nEach epoc takes 40 minutes+ on a gtx 1060"}}