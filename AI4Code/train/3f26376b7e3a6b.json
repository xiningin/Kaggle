{"cell_type":{"f1356ce2":"code","f79d29f4":"code","7a4e3d92":"code","eb7019b9":"code","20af3a14":"code","fa9891fc":"code","d97d2153":"code","db1f4b63":"code","6ed0561b":"code","e7b139a8":"code","ecec1d48":"code","07f2a722":"code","baa5415b":"code","7ac1c76c":"code","a5938321":"code","71ce32b8":"code","85c24f48":"code","b7c9d0e1":"code","e6a3a2bd":"code","df5f0481":"code","fd8fba56":"code","6e0f92b2":"code","8e46a9c4":"code","23973f2b":"code","9f374fa5":"code","8fb715a3":"code","dfe84590":"code","0fc9676e":"code","8508254b":"code","be2b885d":"code","2b941a55":"code","a1cee79e":"code","aa0de0f9":"code","fd656474":"code","d9d6db87":"code","0f4b9e90":"code","9172cf98":"code","27a239c3":"code","5eaef29c":"code","8da39e6b":"code","5fc311a0":"code","445b5e9f":"code","ce838ed0":"code","2c0bd0cc":"markdown","df4290ba":"markdown","6a1ebc38":"markdown","ed3faa8f":"markdown","82f818a5":"markdown","1376cb2e":"markdown","f45a568c":"markdown","43a9a0b7":"markdown","293c19d5":"markdown","f09819fa":"markdown","83d12c08":"markdown","f1668dda":"markdown","8379feef":"markdown","bb4cf39e":"markdown","82420011":"markdown","6bee95d1":"markdown","e2f44a3e":"markdown","8f156b6c":"markdown","df669f9a":"markdown","13fb4833":"markdown","6feee6f2":"markdown","8fbc1cb1":"markdown","3bfec74d":"markdown","3a361412":"markdown","e0994e5f":"markdown","42e01dbc":"markdown","8ef26e5d":"markdown","d5298eae":"markdown","59a55d3e":"markdown","1b505df3":"markdown","d06b52a1":"markdown","5ea43a4f":"markdown","245125cd":"markdown","fbfb4d51":"markdown","be96017b":"markdown","8ecb058c":"markdown","004f4e91":"markdown","cfafecec":"markdown","412711aa":"markdown"},"source":{"f1356ce2":"#here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # plottbing \nimport seaborn as sns\nfrom seaborn import countplot\nfrom matplotlib.pyplot import suptitle\n%matplotlib inline\n\nfrom PIL import Image # image library\nimport urllib\nimport requests\n\nimport re # regex\nimport string\n\n\nimport nltk\nfrom nltk.corpus import stopwords # stop words\nstop_words = stopwords.words('english')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer,PorterStemmer\nstemmer = SnowballStemmer(\"english\",ignore_stopwords=False)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score, cross_val_predict\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nimport xgboost\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nfrom sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier","f79d29f4":"#load train data\ntraining_data = pd.read_csv('..\/input\/traindata\/train.csv')\ntraining_data.info()","7a4e3d92":"%%time\n\n#Converts uppercase letters to lowercase\n\ndef convert2_lowercase(data):\n    data =[string.lower() for string in data if string.isupper]\n    return ''.join(data)\n\n#Removes Punctuations\n\ndef remove_punct(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r' ',data)\n    return data\n\n#Remove Whitespace\n\ndef remove_whitespace(data):\n    tag=re.compile(r'\\s+')\n    data=tag.sub(r' ',data)\n    return data\n\n#Removes Roman words\n\ndef remove_roman(data):\n    en_tag =re.compile(r'^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$')\n    data=en_tag.sub(r' ',data)\n    return data\n\n#Remove redundant words\n\ndef remove_redun(data):\n    red_tag=re.compile(r'[?<=(  )\\\\]|[&&|\\|\\|-]')\n    data=red_tag.sub(r' ',data)\n    return data\n\n#Removes Numbers\n\ndef remove_num(data):\n    tag=re.compile(r'[0-9]+')\n    data=tag.sub(r' ',data)\n    return data\n\ntraining_data.loc[:,('question_text')] = training_data['question_text'].apply(lambda z: convert2_lowercase(z))\ntraining_data.loc[:,('question_text')] = training_data['question_text'].apply(lambda z: remove_num(z))\ntraining_data.loc[:,('question_text')] = training_data['question_text'].apply(lambda z: remove_roman(z))\ntraining_data.loc[:,('question_text')] = training_data['question_text'].apply(lambda z: remove_redun(z))\ntraining_data.loc[:,('question_text')] = training_data['question_text'].apply(lambda z: remove_punct(z))\ntraining_data.loc[:,('question_text')] = training_data['question_text'].apply(lambda z: remove_whitespace(z))","eb7019b9":"%%time\n\n#tokenization and stemmimng the text using the NLTK library of Python\n##imported the snowball stemmer to stem the tokenized text\nimport nltk\nfrom nltk.corpus import stopwords # stop words\nstop_words = stopwords.words('english')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\",ignore_stopwords=False)\n\n\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words= [word for word in token_words if len(word)>3] \n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(stemmer.stem(word))\n    return \" \".join(stem_sentence)\n\n\ntraining_data['questions'] = training_data['question_text'].apply(lambda z: stemSentence(z))","20af3a14":"import time\nt1=time.time()\n#Lemmatize the dataset\nfrom nltk.stem import WordNetLemmatizer\n\ndef lemma_traincorpus(data):\n    out_data=\"\"\n    for words in data:\n        lemmatizer=WordNetLemmatizer()\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ntraining_data.loc[:,('questions')]=training_data['questions'].apply(lambda z: lemma_traincorpus(z))\n\nt2=time.time()\nprint(\"Total elapsed time:\",round(t2-t1),\"sec\")","fa9891fc":"pd.set_option('display.max_colwidth',None)\ntrain_data= training_data.drop(columns=['qid','question_text'])\ntrain_data.head()","d97d2153":"train_df= pd.read_csv(\"..\/input\/cn-quora-train-data\/clean_train_data_100k_dropna.csv\") #clean & normalised subset of train data \ntrain_df.info()","db1f4b63":"pd.set_option('max_colwidth',None)\ntrain_df.head(10)","6ed0561b":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer=TfidfVectorizer(stop_words=None,use_idf=True,max_df =0.7,max_features=11000,lowercase=False,ngram_range=(1,3))\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df['questions'].values.tolist())","e7b139a8":"# Create dataFrame\ntfidf_df = pd.DataFrame(train_tfidf.toarray().T,index=tfidf_vectorizer.get_feature_names())\n \n# Change column headers\ntfidf_df.columns = train_df.questions\ntfidf_df.head() #Term-Document matrix","ecec1d48":"# split a dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\n# create dataset\ny_train = train_df['target']\n# split into train test sets\nX_train, X_test, y_train, y_test = train_test_split(train_tfidf, y_train, test_size=0.20,random_state=14)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","07f2a722":"del training_data, train_df\nimport gc\ngc.collect()","baa5415b":"Image.open('..\/input\/logreg\/logit.png')","7ac1c76c":"#Applying Logistic Regression on split tfidf baseline\nfrom sklearn.linear_model import LogisticRegression\n\nlogit_model=LogisticRegression(max_iter=250,random_state=14)\nlogit_model.fit(X_train,y_train)\npred_model_logit = logit_model.predict(X_test)\nprint(\"Coefficients of LR Model:\",logit_model.coef_)","a5938321":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndef conf_matrix(actual, prediction, model_name):\n    cm_array=metrics.confusion_matrix(actual,prediction,labels=[0,1])\n    sns.heatmap(cm_array,annot=True, fmt='.0f',xticklabels=['Sincere','Insincere'],yticklabels=['Sincere','Insincere'])\n    plt.ylabel('True\\n')\n    plt.xlabel('Predicted\\n')\n    plt.title(model_name)\n    plt.show()\n    \nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_model_logit,'Logistic Regression Model\\n')\nplt.show()","71ce32b8":"#generic function to plot ROC curve\n\ndef draw_roc(model,test_X,test_Y,name):\n    predict_prob = pd.DataFrame(model.predict_proba(test_X))\n    test_results=pd.DataFrame({'actual': test_Y})\n    test_results =test_results.reset_index()\n    test_results['class1']= predict_prob.iloc[:,1:2]\n    fpr,tpr,thresh = metrics.roc_curve(test_results.actual,test_results.class1,drop_intermediate=False)\n    auc = metrics.roc_auc_score(test_results.actual,test_results.class1)\n    plt.figure(figsize=(8,5))\n    plt.plot(fpr,tpr,'-',label='ROC CURVE (area=%0.2f)' %auc)\n    plt.ylabel(\"sensitiivity - True Positive\")\n    plt.xlabel(\"(1-specifivity) - False Positive\")\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0,1.01])\n    plt.plot([0,1],[0,1],'k--')\n    plt.legend(loc='lower right')\n    plt.title(name)\n    plt.show()\n    \ndraw_roc(logit_model,X_test,y_test,'logistic model')","85c24f48":"%%time\n\nfrom sklearn.neighbors import KNeighborsClassifier\n#train and fit the classifier\nknn_model= KNeighborsClassifier().fit(X_train,y_train)\npred_model_knn = knn_model.predict(X_test)\n\n#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_model_knn,'KNN Model\\n')\nplt.show()","b7c9d0e1":"from sklearn.ensemble import RandomForestClassifier\n#train and fit the classifier\nrf_model = RandomForestClassifier(n_estimators = 250, criterion='gini',random_state=14).fit(X_train, y_train)\npred_model_rf = rf_model.predict(X_test)\n\n#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_model_rf,'Random Forest Model\\n')\nplt.show()","e6a3a2bd":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\ncl_tree=DecisionTreeClassifier(criterion ='entropy',max_depth=10)\ncl_tree.fit(X_train, y_train)\n#Accuracy\nfrom sklearn import metrics\ntree_pred=cl_tree.predict(X_test)\nnp.round(metrics.roc_auc_score(y_test, tree_pred),2)\n#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,tree_pred,'Decision Tree Classifier\\n')\nplt.show()","df5f0481":"from sklearn import tree\nfrom sklearn.tree import export_graphviz\nfrom sklearn.tree import DecisionTreeClassifier\n#print(tree.export_text(cl_tree))","fd8fba56":"#Applying MultiNomial Naive Bayes on split tfidf baseline\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel_nb = MultinomialNB()\nmodel_nb.fit(X_train,y_train)\npred_model_nb = model_nb.predict(X_test)\n\n#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_model_nb,'Naive Bayes Model\\n')\nplt.show()","6e0f92b2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#Base model logistic regression\nlogit_model=LogisticRegression(max_iter=250,random_state=14)\n\n#Ada Boost Classifier\nada_model = AdaBoostClassifier(logit_model,n_estimators=100)\nada_model.fit(X_train,y_train)\n\n#prediction model\npred_model_ada = ada_model.predict(X_test)\n\n#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_model_ada,'AdaBoost Model\\n')\nplt.show()","8e46a9c4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n#Gradient Boosting Classifier\ngb_model = GradientBoostingClassifier(n_estimators=150,max_depth=10)\ngb_model.fit(X_train,y_train)\n\n#prediction model\npred_model_gb = gb_model.predict(X_test)\n\n#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_model_gb,'Gradient Boosting Model\\n')\nplt.show()","23973f2b":"%%time\nfrom xgboost import XGBClassifier\nmodel_xgb = XGBClassifier(scale_pos_weight=15,use_label_encoder=False)\nmodel_xgb.fit(X_train,y_train)\nprint(model_xgb)\nfrom sklearn.metrics import accuracy_score\npred_xgb = model_xgb.predict(X_test)\npredictions = [round(value) for value in pred_xgb]\naccuracy =accuracy_score(y_test,predictions)\nprint(accuracy)","9f374fa5":"#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_xgb,'XG Boost Model\\n')\nplt.show()","8fb715a3":"import lightgbm as lgb\nmodel_clf = lgb.LGBMClassifier()\nmodel_clf.fit(X_train, y_train)\n\n#predict the results\npred_lgbm=model_clf.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(pred_lgbm, y_test)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, pred_lgbm)))","dfe84590":"#confusion matrix\nsns.set_context(\"notebook\", font_scale=1.1)\nplt.figure(figsize=(5,5))\nconf_matrix(y_test,pred_lgbm,'Light GBMModel\\n')\nplt.show()","0fc9676e":"print(\"Logistic Regression Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_model_logit))\nprint(\"KNN Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_model_knn))\nprint(\"Random Forest Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_model_rf))\nprint(\"Decision Tree Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,tree_pred))\nprint(\"Naive-Bayes-MultinomialNB Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_model_nb))\nprint(\"AdaBoost Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_model_rf))\nprint(\"Gradient Boost Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_model_gb))\nprint(\"XG Boost Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_xgb))\nprint(\"Light GBM Model\")\nprint(\"------------------------------------------------------\")\nprint(metrics.classification_report(y_test,pred_lgbm))","8508254b":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\npred_model_list = [pred_model_logit,pred_model_knn,pred_model_rf,tree_pred,pred_model_nb,pred_model_ada,pred_model_gb,pred_xgb,pred_lgbm]\n\n#calculate the scores of models\ndef cal_metric(score):\n    metric_list=[]\n    for predmodel in  pred_model_list:\n        metric_list.append(score(y_test,predmodel))\n    return metric_list\n        \nmodel_training_metrics = pd.DataFrame(index=[\"Logistic\",\"KNN\",\"RandomForest\",\"DecisionTree\",\"MultinomialNB\",\"AdaBoost\",\"GradientBoost\",\"XGBoost\",\"LightGBM\"])\nmodel_training_metrics['Accuracy']=cal_metric(accuracy_score)\n#model_training_metrics['Precision']=cal_metric(precision_score) - avoiding zero division warning\nmodel_training_metrics['Recall']=cal_metric(recall_score)\nmodel_training_metrics['F1_Score']=cal_metric(f1_score)\nmodel_training_metrics = model_training_metrics.sort_values('F1_Score', ascending=False)\nmodel_training_metrics.to_csv(\"StatModels_Unbal_TFIDF.csv\",sep=\",\",index=False)\nprint(model_training_metrics)","be2b885d":"import gc\ngc.collect()","2b941a55":"import imblearn\nfrom sklearn import metrics\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\ntrain_df= pd.read_csv(\"..\/input\/cn-quora-train-data\/clean_train_data_100k_dropna.csv\") #clean & normalised subset of train data\n\ntfidf_vectorizer=TfidfVectorizer(stop_words=None,use_idf=True,max_df =0.7,max_features=10000,lowercase=False,ngram_range=(1,3))\ntrain_tfidf = tfidf_vectorizer.fit_transform(train_df['questions'].values.tolist())\n\n# create dataset\ny_train = train_df['target']\n# split into train test sets\nX_train, X_test, y_train, y_test = train_test_split(train_tfidf, y_train, test_size=0.20,random_state=14)\n\n#Balancing The Sampple for TFIDF Baseline\n#SMOTE TOMEK Link is used over sample the data with under sampling\n#Source:https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/combine.html#combine\n\nmethod = imblearn.combine.SMOTETomek()\nsmotet_train_X,smotet_train_y=method.fit_sample(X_train,y_train)\nprint(smotet_train_X.shape,smotet_train_y.shape)","a1cee79e":"%%time\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import StratifiedKFold,KFold,cross_val_score,cross_val_predict\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\n# Models\nmodel_logit = LogisticRegression(solver='lbfgs',class_weight='balanced', max_iter=10000)\nmodel_gb = GradientBoostingClassifier(n_estimators=200,max_depth=10)\nmodel_xgb = XGBClassifier(scale_pos_weight=15,use_label_encoder=False)\nmodel_lgbm = lgb.LGBMClassifier()\n\n# Predictions\nmodel_logit.fit(smotet_train_X, smotet_train_y)\npred_model_logit = model_logit.predict(X_test)\n\nmodel_gb.fit(smotet_train_X, smotet_train_y)\npred_model_gb = model_gb.predict(X_test)\n\nmodel_xgb.fit(smotet_train_X, smotet_train_y)\npred_model_xgb = model_xgb.predict(X_test)\n\nmodel_lgbm.fit(smotet_train_X, smotet_train_y)\npred_model_lgbm = model_lgbm.predict(X_test)\n\n\npred_model_list = [pred_model_logit,pred_model_gb,pred_model_xgb,pred_model_lgbm]","aa0de0f9":"#calculate the scores of models\n\ndef cal_metric(score):\n    metric_list=[]\n    for predmodel in  pred_model_list:\n        metric_list.append(score(y_test,predmodel))\n    return metric_list\n        \nsmodel_training_metrics = pd.DataFrame(index=[\"Logistic\",\"GradientBoost\",\"XGBoost\",\"LightGBM\"])\nsmodel_training_metrics['Accuracy']=cal_metric(accuracy_score)\nsmodel_training_metrics['Precision']=cal_metric(precision_score) \nsmodel_training_metrics['Recall']=cal_metric(recall_score)\nsmodel_training_metrics['F1_Score']=cal_metric(f1_score)\nsmodel_training_metrics = smodel_training_metrics.sort_values('F1_Score', ascending=False)\nsmodel_training_metrics.to_csv(\"StatModels_SMOTETomek_TFIDF.csv\",sep=\",\",index=False)\nprint(smodel_training_metrics)","fd656474":"del smodel_training_metrics\nimport gc\ngc.collect()","d9d6db87":"train_df= pd.read_csv(\"..\/input\/cn-quora-train-data\/clean_train_data_100k_dropna.csv\") #clean & normalised subset of train data\ntrain_df.info()","0f4b9e90":"#Load word2vec algorithm from gensim\nimport gensim\nfrom gensim.models import Word2Vec,KeyedVectors\n\n#Convert Input DataFrame to a List\ncheck_df=list(train_df['questions'].str.split())\nmodel=Word2Vec(check_df,min_count=1,iter=20)","9172cf98":"def word2_sentence(data):\n    #w = [word for word in data if word in model.wv.vocab]\n    text_vector = np.mean([model.wv[word] for word in data.split()], axis=0)\n    return text_vector","27a239c3":"#Convert word vectors to sentence vectors\/sentence vectors and apply mean pooling\ntrain_df['question_vectors'] = train_df['questions'].apply(word2_sentence)","5eaef29c":"train_df.question_vectors[0].shape","8da39e6b":"#Split the dataset into training and testing sets\ny_train =train_df['target']\nX_train,X_test,y_train,y_test= train_test_split(train_df['question_vectors'],y_train,test_size=0.2,random_state=14)\nprint(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","5fc311a0":"X_train_l=list(X_train)\nX_test_l=list(X_test)","445b5e9f":"from sklearn.model_selection import KFold,cross_val_score,cross_val_predict\n\nmodels=[]\nmodels.append(('LightGBM',lgb.LGBMClassifier()))\n#models.append(('XGBoost',XGBClassifier(use_label_encoder=False)))\nmodels.append(('GradientBoost',GradientBoostingClassifier(learning_rate=1e-2, loss='deviance',n_estimators=250)))\n              \nwvmodel_training_result, wvmodel_validation_result=[],[]\nscoring='accuracy'\n\nprint(\"Statistical Models Word2Vec - Baseline Evaluation\\n\")\nfor name,model in models:\n    kfold   = KFold(n_splits=5,shuffle=False)\n    results = cross_val_score(model,X_train_l,y_train,cv=kfold)\n    print(\"Classifier: \",name, \"has a training score of\", round(results.mean(), 2) * 100, \"% accuracy\")\n    print(\"------------------------------------------------------------------------------------------\")\n    predictions=cross_val_predict(model,X_test_l,y_test)\n    accuracy = accuracy_score(predictions,y_test)\n    wvmodel_training_result.append(results.mean())\n    wvmodel_validation_result.append(accuracy)","ce838ed0":"wvfinal_outcomes=pd.DataFrame(columns=['Model','Training Accuracy','Validation Accuracy'])\nwvfinal_outcomes['Model']=models\nwvfinal_outcomes['Training Accuracy']=wvmodel_training_result\nwvfinal_outcomes['Validation Accuracy']=wvmodel_validation_result\nwvfinal_outcomes.to_csv('Word2Vec-Baseline.csv')\nprint(wvfinal_outcomes)","2c0bd0cc":"### Display Decision Tree","df4290ba":"### TF-IDF Vectorization","6a1ebc38":"## Analysing TFIDF Baseline with simple split \n\n**I am evaluating the performance of \"statistical classifiers\" on the TFIDF vectorized data sampled without balanced train_test_split.**","ed3faa8f":"# \u26a1END-to-END-Natural Language Processing-2\u26a1on Quora Insincere Questions\n\nIn this notebook, I am building statistical machine learning models with Non-Semantic TFIDF vectorization and Semantic Embeddings as well.\n\nSo, let's get started.\n\nAs always, I hope you find this kernel useful and your **UPVOTES** would be highly appreciated.","82f818a5":"## KNN model","1376cb2e":"## Logistic Regression Model","f45a568c":"If ROC >0.7 for the model, it's better classifier. However, it's important to check the recall scores to avoid overfitting the majority class of unbalanced train data set.","43a9a0b7":"### (optional) START from here without running above codes","293c19d5":"- Bernoulli NB which relies on multivariate bernoulli distributions of the input features and also expects the data to be in binary format.\n\n\n<img src=\"https:\/\/www.astroml.org\/_images\/fig_simple_naivebayes_1.png\">\n\nOther variants include:\n- Categorical NB\n- Partial Fit of NB models\n\nResources:\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/naive-bayes-explained\/)\n- [Kernel](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india#Vectorization-and-Benchmarking)\n- [Jason's Blog](https:\/\/machinelearningmastery.com\/classification-as-conditional-probability-and-the-naive-bayes-algorithm\/)","f09819fa":"### Imbalanced data\n\nImbalanced data sets are a special case for classification problem where the class distribution is not uniform among the classes. Typically, they are composed by two classes: The majority (negative) class and the minority (positive) class.\n\nIn our case, positive class is where target=1 (Insincere) and negative class is where target=0 (Sincere).","83d12c08":"## Light GBM Classifier\nLightGBM is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It has helped Kagglers win data science competitions.\n\n- Kagglers start to use LightGBM more than XGBoost. LightGBM is 6 times faster than XGBoost\n- Light GBM is a relatively new algorithm and have long list of parameters given in the [LightGBM documentation](http:\/\/https:\/\/github.com\/microsoft\/LightGBM)\n- The size of dataset is increasing rapidly. It is become very difficult for traditional data science algorithms to give accurate results. Light GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.\n- It is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data.\n\n### Important points about tree-growth\nIf we grow the full tree, best-first (leaf-wise) and depth-first (level-wise) will result in the same tree. The difference is in the order in which the tree is expanded. Since we don't normally grow trees to their full depth, order matters.\n\nApplication of early stopping criteria and pruning methods can result in very different trees. Because leaf-wise chooses splits based on their contribution to the global loss and not just the loss along a particular branch, it often (not always) will learn lower-error trees \"faster\" than level-wise.\n\nFor a small number of nodes, leaf-wise will probably out-perform level-wise. As we add more nodes, without stopping or pruning they will converge to the same performance because they will literally build the same tree eventually.\n\n### Core Parameters \n**Task** : It specifies the task you want to perform on data. It may be either train or predict.\n\n**application** : This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.\n\n**regression** : for regression\n**binary** :for binary classification\n**multiclass** : for multiclass classification problem\n**boosting** : defines the type of algorithm you want to run, default=gdbt.\n\n**gbdt** : traditional Gradient Boosting Decision Tree\n**rf** : random forest\n**dart**: Dropouts meet Multiple Additive Regression Trees\n**goss** : Gradient-based One-Side Sampling\n**num_boost_round** : Number of boosting iterations, typically 100+\n\n**learning_rate** : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003\u2026\n\n**num_leaves** : number of leaves in full tree, default: 31\n\n**device** : default: cpu, can also pass gpu","f1668dda":"## Random Forest Classifier with Linear Discriminent Analysis(LDA)\nIn case of uniformly distributed data, LDA almost always performs better than PCA. However if the data is highly skewed (irregularly distributed) then it is advised to use PCA since LDA can be biased towards the majority class.\n\nHence,I am not performing LDA.","8379feef":"### Statistical Training Without Balancing\n\nThe initial model training benchmark can be used for further improvement by balancing the dataset. \n\n#### Splitting the sets\n\n[Jason's blog](https:\/\/machinelearningmastery.com\/train-test-split-for-evaluating-machine-learning-algorithms\/) provides a good idea about splitting techniques for generic classification problems.\n\nIn generic supervised learning, models are trained after splitting the tfidf vectorized data. This transformation is linear and just splits depending on the ratio\/test size provided. \n\nWe can split the data in different ways:\n\n1) Test Train Split\n2) Stratified Split\n3) Stratified Shuffle Split\n4) Shuffle Split\n5) Stratified K Fold\n\nI am using the SMOTE to balance the train and test sets with KFold CV later on.","bb4cf39e":"### Interpreting the coefficients of the model - Logistic regression with multiple predictor variables and no interaction terms\n\n\nlogit(p) = log(p\/(1-p))= \u03b20 + \u03b21*x1 + \u2026 + \u03b2k*xk\n\nApplying such a model to our dataset,each estimated coefficient is the expected change in the log odds of being in +ve class (target=1) for a unit increase in the corresponding predictor variable holding the other predictor variables constant at certain value.  Each exponentiated coefficient is the ratio of two odds, or the change in odds in the multiplicative scale for a unit increase in the corresponding predictor variable holding other variables at certain value.","82420011":"p = e^z\/(1+e^z) ,where z = \u03b20 + \u03b21*x1 + \u2026 + \u03b2k*xk. z is linear. Sigmoid function of p show a return value (y axis) in the range 0 to 1","6bee95d1":"<img src=\"https:\/\/media.giphy.com\/media\/LrKqe8TdKqxaTT58PY\/giphy.gif\" align='left' width ='400' height='300'>","e2f44a3e":"## Decision Tree Classifier","8f156b6c":"## Applying Word2Vec Baseline with KFold\nI am building 2 Classifiers namely, Light GBM and Gradient Boost with Word2Vec embeddings.","df669f9a":"## Applying SMOTE balancing on TFIDF vectors\n\nKeep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. \n\n> ### That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. \n\nOnly by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won\u2019t be an overfitting problem.","13fb4833":"\n# Conclusion of Non-Semantic Baseline Techniques\n\nWe have seen overview of 'statistical classifiers' with non-semantic TFIDF vectorized data and also attained a parallel analysis of the accuracy of the different algorithms. \n\nNow, Let's explore the models using **'semantic embeddings'** (vectors) with these traditional\/statistical classifiers.","6feee6f2":"## Interpretation of Metrics\n\n- Accuracy of the models is 94% approx.\n\n- F1 Scores are terribly bad.\n\n- Gradient Boost, XG Boost,Light GBM, and Logistic Regression Classifiers better than that of others in terms of overfitting as evident from F1.\n\nThese metrics also shows that all the models are **overfitting** because imbalanced data.\n\nHence, it's necessary to **balance** the classes and do the cross validation with Kfold to avoid the **\"overfitting\"**. I am using SMOTE for balancing the train and test data sets.\n\n**F1 Score is better metric for our case as it's unbalanced data classification.**","8fbc1cb1":"## XG Boost Classifier","3bfec74d":"## Brief Introduction of Statistical Models\n\n**Logistic Regression**\n\nIn a supervised learning mode , Logistic Regression is one of the standardised models under generalized linear models which tries a convex optimization by passing the cost function through the sigmoid function. \n\nThe effective loss function for logistic regression is, E = (|y_predicted -y_actual|^2). \n\nTo optimize the cost function (in this case a linear sum of weights & biases passed through sigmoid kernel), by applying stochastic gradient descent. Since by gradient descent, the steepest slope is considered, the change in derivatives (gradients) at each stage is computed and the weights of the cost function are updated. \n\nBlog on [Gradient Descent for ML](https:\/\/machinelearningmastery.com\/gradient-descent-for-machine-learning\/) provides an idea. \n\n**Decision Trees**\n\nDecision Trees is a supervised model for classification\/regression. This works on creating decision branches which evolves a criteria and is often acknowledged as a simplistic classification (white box) model as the stages of decision can be easily derived. \n\n**Random Forests**\n\nRandom Forests is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree. When splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size max_features. (See the parameter tuning guidelines for more details).The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.\n\nIn general, the major logics involved in Decision Trees involves computation of **Entropy or Gini Index.**\n\n**Gradient Boosting**\n\nGradient Boosting is a central part of ensemble modelling in sklearn. The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability \/ robustness over a single estimator.\n\nTwo families of ensemble methods are usually distinguished:\n\nIn averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\nExamples: Bagging methods, Forests of randomized trees\n\nBy contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\nExamples: AdaBoost, Gradient Tree Boosting\n\nSeveral Boosting Models can be found under this criteria.\n\n**AdaBoosting**\n\nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Initially, those weights are all set to (1\/N), so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence\n\n**LightGBM**\n\nLight GBM is another gradient boosting strategy which relies on trees.It has the following advantages:\n- Faster training speed and higher efficiency.\n\n- Lower memory usage.\n\n- Better accuracy.\n\n- Support of parallel and GPU learning.\n\n- Capable of handling large-scale data.\n\nLightGBM grows leaf-best wise and will choose the leaf with maximum max delta loss to grow.","3a361412":"## Random Forest Model","e0994e5f":"## AdaBoost Classifier","42e01dbc":"### Cleaning and Normalising text data\n\nThese are 3 steps to create TF-IDF matrix\n\n**1. Cleaning:**\n\n* Converted text to lower case\n* Removed puncutations\n* Removed roman words and redundant words\n* Removed numbers\n* Removed whitespaces\n\n**2. Lemmatized** the 'words' to 'root words' with WordNetLemmatizer of NLTK ([WordNetLemmatizer](http:\/\/https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/))\n\n**3. TfidfVectorizer method of NLTK**\n\n* applied snowball stemmer to 'stem' the words\n* removed stop words\n* max_df = 0.7 is applied to select those words with maximum occurence in the corpus is less than 75% (highly frequent words are good to ignore)\n* ngrams =(1,3) to select up to Bigrams\n* words with length more 3 is selected to ignore unimportant words\n\nResultant,TF-IDF vectorised corpus\/matrix is visualised as a Dataframe","8ef26e5d":"\n> ## Word2Vec","d5298eae":"\n## Applying SMOTE TFIDF Balanced Baseline with KFold\nI am building 4 Classifiers namely, Light GBM, XG Boost, Logistic Regression and Gradient Boost based on the performance on non-semantic vectors.","59a55d3e":"## Accuracy,Recall and F1 Scores of Stat Models on TFIDF Vectors","1b505df3":"## MultiNomial Naive Bayes\n\n[MultiNomial NB](https:\/\/ogrisel.github.io\/scikit-learn.org\/sklearn-tutorial\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html) is a probabilistic statistical classification model which uses conditional probability to segregate or classify samples. This works well with discrete integer valued features (such as count vectorization) but can also be used with TFIDF vectors. Particularly, this uses the Bayes Theorem which tries to determine conditional probability using prior and posterior probabilities as shown in the figure:\n\n<img src=\"https:\/\/storage.googleapis.com\/coderzcolumn\/static\/tutorials\/machine_learning\/article_image\/Scikit-Learn%20-%20Naive%20Bayes.jpg\">\n\nThe major concept under this category is statistics of [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html). There are many variants:\n\n- Gaussian NB which relies on Gaussian distribution of the input features\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Yune_Lee\/publication\/255695722\/figure\/fig1\/AS:297967207632900@1448052327024\/Illustration-of-how-a-Gaussian-Naive-Bayes-GNB-classifier-works-For-each-data-point.png\">\n\n\n- Complement NB which is suited for unbalanced classes and relies on the statistics of complement of each class to generate the weights. It is better than Multinomial NB for textual classification as it has a normalization factor (and a smoothing hyperparameter alpha) which tends to capture information from longer sequences of text.","d06b52a1":"## Creating Sentence Vectors from Word2Vec\n\nSince Word2Vec creates vector embeddings for individual words in a corpus by transforming them to a manifold, we need effective document \/sentence vectors from these individual vectorized words. The concept of pooling is derived from Neural Networks particularly Convolution Neural Network Architectures where MaxPooling signifies taking the maximum from a range (particularly a kernel\/filter or a window of input features).","5ea43a4f":"## Gradient Boosting Classifier","245125cd":"## Metrics of Statistical Models trained on Non-semantic vectors ##","fbfb4d51":"<img src =\"https:\/\/media.giphy.com\/media\/10LrGOCDuhJODu\/giphy.gif\" height='500' width='500' align='right'>\n\n\n> ## Visit next part to explore the Neural Networks on these embeddings","be96017b":"<img src=\"https:\/\/media.giphy.com\/media\/GxnJO5weiJ2uY\/giphy.gif\" align='left' width ='400' height='300'>","8ecb058c":"# Static Semantic Embedding Baseline\n\nIn this context, I am applying the statistical (traditional) ML classifiers on Word2Vec and Fasttext data to create classification models.\n\nI am already demonstrated these embeddings in the previous notebook at [END-to-END-Natural Language Processing-1\u26a1](https:\/\/www.kaggle.com\/rizdelhi\/end-to-end-natural-language-processing-1). \n\n**Semantic embeddings** include word embeddings which can either by ***static and dynamic***.\n\nStatic: Word2Vec, Glove, Fasttext, Google News,etc\n\nDynamic: Elmo, etc\n\n\n> ## Statistical Models on Static Embeddings\n\nNow, Let's move forward to build the statsitical models on the Static embedding\/ Compressed Sentence Vectors. This allows to apply gradient boosting algorithms on the static embeddings computed by taking the mean of the word vectors. \n\nSteps:\n- Apply Word2Vec on the Corpus\n- Create Sentence Vectors by Mean Pooling\n- Run the input sentence vectors with Kfold Cross Validation on Traditional and gradient boosting classifiers.\n\nPrimarily I am starting with **Word2Vec algorithm**.","004f4e91":"# Statistical Models with TFIDF Vectors\n- Logistic Regression\n- K Nearest Neighbours(KNN)\n- Naive-Bayes\n- Decision Trees\n- Random Forest\n- AdaBoost\n- Gradient Boosting\n- Support Vector Machine\n- XGboost\n- Light GBM\n------------------------------------------------------\n***With Non Semantic TfIdf vectors***\n\nand\n\n***Semantic Static Embeddings\/ Semantic Dynamic Embeddings\/ Transformer Embeddings***\n_____________________________\n\nLater, Let's see how to improvise on these models with standard neural models like Bidirectional LSTMs, Convolution Networks, Gated Recurrecnt Units in Part 3.","cfafecec":"### Convert the sentence vectors to List\n\nThis is done to ensure the dimensionality of the input sentence vectors is that of an array (list). This can be easily fed into any statistical classifier for our use case.","412711aa":"# Multiple Model Training using KFold Cross Validation\n\nIn this concept, I am training multiple statistical models based on \"KFold Cross Validation Technique\". \n\n- KFold cross validators provide train\/test split indices for splitting the dataset into 'k' folds without shuffling. \n\nThe general methodology:\n* Shuffle the dataset randomly.\n* Split the dataset into k groups\n* For each unique group:\n* Take the group as a hold out or test data set\n* Take the remaining groups as a training data set\n* Fit a model on the training set and evaluate it on the test set\n* Retain the evaluation score and discard the model\n* Summarize the skill of the model using the sample of model evaluation scores\n\nThis technique has the simple rule: \n\n**The first n_samples % n_splits folds have size n_samples \/\/ n_splits + 1, other folds have size n_samples \/\/ n_splits, where n_samples is the number of samples.**"}}