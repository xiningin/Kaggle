{"cell_type":{"77b12e95":"code","7f125662":"code","19e6c625":"code","12355838":"code","c4df8914":"code","69d5cc28":"code","63a24d1a":"code","b658ec78":"code","b09da7a3":"code","35441779":"code","65bdf951":"markdown","264ebcd0":"markdown","5b8c8ec6":"markdown","5cf0a042":"markdown"},"source":{"77b12e95":"from argparse import ArgumentParser\nfrom pytorch_lightning.metrics import functional as FM\n\nimport pytorch_lightning as pl\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef auc_multi(y_true, y_pred, exclude_index=None):\n    M = y_true.shape[1]\n    results = torch.zeros(M)\n    sample_weight = None\n    if exclude_index is not None:\n        sample_weight = torch.ones(y_true.shape[0]).type_as(y_pred)\n        sample_weight[exclude_index] = 0.0\n    for i in range(M):\n        try:\n            results[i] = FM.auroc(\n                y_pred[:, i], y_true[:, i], sample_weight=sample_weight\n            )\n        except:\n            pass\n    return results.mean()\n\n\ndef get_activation(activation):\n    if activation == \"relu\":\n        return nn.ReLU()\n    elif activation == \"leaky_relu\":\n        return nn.LeakyReLU()\n    else:\n        raise ValueError\n\n\nclass InvaseModel(pl.LightningModule):\n    enable_print_log = True\n    \n    @staticmethod\n    def add_model_specific_args(parent_parser):\n        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n        parser.add_argument(\"--learning_rate\", type=float, default=0.0002)\n        parser.add_argument(\"--max_learning_rate\", type=float, default=None)\n        parser.add_argument(\"--vehicle_weight\", type=float, default=0.1)\n        parser.add_argument(\"--num_layers\", type=int, default=3)\n        parser.add_argument(\"--actor_h_dim\", type=int, default=100)\n        parser.add_argument(\"--critic_h_dim\", type=int, default=200)\n        parser.add_argument(\"--weight_decay\", type=float, default=1e-5)\n        parser.add_argument(\"--lambda_\", type=float, default=0.1)\n        parser.add_argument(\"--minus\", action=\"store_true\")\n        parser.add_argument(\"--scheduler_patience\", type=int, default=10)\n        parser.add_argument(\"--activation\", type=str, default=\"relu\")\n        parser.add_argument(\"--dropout\", type=float, default=0.2)\n        return parser\n\n    def __init__(self,\n             learning_rate,\n             input_dim: int = 874,\n             output_dim: int = 206,\n             include_vehicle: bool = False,\n             vehicle_weight: float = 1.0,\n             minus: bool = False,\n             scheduler_patience: int = 10,\n             class_weight=None,\n             weight_decay: float = 1e-5,\n             actor_h_dim: int = 100,\n             critic_h_dim: int = 200,\n             num_layers: int = 3,\n             activation: str = \"relu\",\n             dropout: float = 0.2,\n             lambda_: float = 0.1,\n            **kwargs,\n        ):\n        super(InvaseModel, self).__init__()\n        self.class_weight = class_weight\n        self.output_dim = output_dim\n        self.learning_rate = learning_rate\n        self.include_vehicle = include_vehicle\n        self.vehicle_weight = vehicle_weight\n        self.weight_decay = weight_decay\n        self.minus = minus\n        self.scheduler_patience = scheduler_patience\n        self.lambda_ = lambda_\n        self._last_train_loss = None\n        self.actor = self.build_layers(input_dim, actor_h_dim, input_dim,\n                                       num_layers - 2, activation, dropout)\n        self.critic = self.build_layers(input_dim, critic_h_dim, output_dim,\n                                        num_layers - 2, activation, dropout)\n        self._cache = None\n        if self.minus:\n            self.baseline = None\n        else:\n            self.baseline = self.build_layers(input_dim, critic_h_dim, output_dim,\n                                              num_layers - 2, activation, dropout)\n        self.save_hyperparameters()\n\n    def build_layers(self, input_dim: int, hidden_dim: int, output_dim: int, n_layer: int,\n                     activation: str,\n                     dropout: float):\n        layers = [\n            nn.BatchNorm1d(input_dim),\n            nn.Dropout(dropout),\n            nn.utils.weight_norm(nn.Linear(input_dim, hidden_dim)),\n            get_activation(activation),\n        ]\n        for i in range(n_layer):\n            layers += [\n                nn.BatchNorm1d(hidden_dim),\n                nn.Dropout(dropout),\n                nn.utils.weight_norm(nn.Linear(hidden_dim, hidden_dim)),\n                get_activation(activation),\n            ]\n        layers += [\n            nn.BatchNorm1d(hidden_dim),\n            nn.Dropout(dropout),\n            nn.utils.weight_norm(nn.Linear(hidden_dim, output_dim))\n        ]\n        return nn.Sequential(*layers)\n\n    def actor_loss(self, selection, actor_out, baseline_loss, critic_loss, y):\n        if self.minus:\n            reward = - critic_loss\n        else:\n            reward = -(critic_loss - baseline_loss)\n        y_pred = actor_out.sigmoid()\n        actor_loss = selection * torch.log(y_pred + 1e-8) + (1 - selection) * torch.log(1 - y_pred + 1e-8)\n        actor_loss = reward * actor_loss.sum(1) - self.lambda_ * y_pred.mean(1)\n        return (-actor_loss).mean()\n\n    def _train_loss(self, out, x, y):\n        return F.binary_cross_entropy_with_logits(out, y)\n    \n    def training_step(self, batch, batch_idx, optimizer_idx):\n        x = batch[\"x\"].float()\n        y = batch[\"y\"].float()\n        selection, actor_out, baseline_out, critic_out = self(x)\n        if optimizer_idx == 0:\n            critic_loss = self._train_loss(critic_out, x, y)\n            self.log(\"train_critic_loss\", critic_loss, logger=True, prog_bar=True)\n            return critic_loss\n        elif self.minus:\n            if optimizer_idx == 1:\n                critic_loss = self._train_loss(critic_out, x, y)\n                actor_loss = self.actor_loss(selection, actor_out,\n                                             None, critic_loss.detach(), y)\n                self.log(\"train_actor_loss\", actor_loss, logger=True, prog_bar=True)\n                return actor_loss\n        else:\n            if optimizer_idx == 1:\n                baseline_loss = self._train_loss(baseline_out, x, y)\n                self.log(\"train_baseline_loss\", baseline_loss, logger=True, prog_bar=True)\n                return baseline_loss\n            elif optimizer_idx == 2:\n                critic_loss = self._train_loss(critic_out, x, y)\n                baseline_loss = self._train_loss(baseline_out, x, y)\n                actor_loss = self.actor_loss(selection, actor_out,\n                                             baseline_loss.detach(), critic_loss.detach(), y)\n                self.log(\"train_actor_loss\", actor_loss, logger=True, prog_bar=True)\n                return actor_loss\n\n    def validation_step(self, batch, batch_idx):\n        x = batch[\"x\"].float()\n        y = batch[\"y\"].float()\n        selection, actor_out, baseline_out, critic_out = self(x)\n        loss = F.binary_cross_entropy_with_logits(critic_out, y)\n        auc_score = auc_multi(y, critic_out.sigmoid())\n        metrics = {\"val_loss\": loss, \"val_auc\": auc_score}\n        self.log_dict(metrics, logger=True, prog_bar=True)\n        return loss\n\n    def validation_epoch_end(self, validation_step_outputs):\n        val_loss = np.mean([t.item() for t in validation_step_outputs])\n        if self.enable_print_log:\n            print(\n                f\"epoch: {self.current_epoch} val_loss: {val_loss:.4f}\"\n            )\n    \n    def predict(self, x):\n        with torch.no_grad():\n            _, _, _, out = self(x)\n            pred = out.sigmoid().detach().cpu()\n        return pred\n\n    def forward(self, x):\n        actor_out = self.actor(x)\n        prob = actor_out.detach().sigmoid()\n        prob = torch.where(torch.isnan(prob), torch.zeros_like(prob).to(prob.device), prob)\n        selection = torch.bernoulli(prob)\n        critic_out = self.critic(x * selection)\n        if self.minus:\n            baseline_out = None\n        else:\n            baseline_out = self.baseline(x)\n        return selection, actor_out, baseline_out, critic_out\n\n    def configure_optimizers(self):\n        optimizers = []\n        lr_schedulers = []\n        optimizer0 = torch.optim.Adam(\n            self.critic.parameters(), lr=self.learning_rate,\n            weight_decay=self.weight_decay\n        )\n        lr_scheduler0 = {\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer0, mode=\"min\", factor=0.1, patience=self.scheduler_patience,\n                eps=1e-4, verbose=False\n            ),\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n        optimizers.append(optimizer0)\n        lr_schedulers.append(lr_scheduler0)\n        if not self.minus:\n            optimizer1 = torch.optim.Adam(\n                self.baseline.parameters(), lr=self.learning_rate,\n                weight_decay=self.weight_decay\n            )\n            lr_scheduler1 = {\n                \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer1, mode=\"min\", factor=0.1, patience=self.scheduler_patience,\n                    eps=1e-4, verbose=False\n                ),\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1,\n            }\n            optimizers.append(optimizer1)\n            lr_schedulers.append(lr_scheduler1)\n\n        optimizer2 = torch.optim.Adam(\n            self.actor.parameters(), lr=self.learning_rate,\n            weight_decay=self.weight_decay\n        )\n        lr_scheduler2 = {\n            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer2, mode=\"min\", factor=0.1, patience=self.scheduler_patience,\n                eps=1e-4, verbose=False\n            ),\n            \"monitor\": \"val_loss\",\n            \"interval\": \"epoch\",\n            \"frequency\": 1,\n        }\n        optimizers.append(optimizer2)\n        lr_schedulers.append(lr_scheduler2)\n        return optimizers, lr_schedulers\n","7f125662":"import numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer\n\n\ndef get_pca(all_features, col_prefix, n_comp):\n    cols = [col for col in all_features.columns if col.startswith(col_prefix)]\n    data2 = PCA(n_components=n_comp, random_state=42).fit_transform(all_features[cols])\n    data2 = pd.DataFrame(data2, columns=[f\"pca_{col_prefix}-{i}\" for i in range(n_comp)])\n    return data2\n\n\ndef handle_category(all_features, include_vehicle=False):\n    cols = list(all_features.columns)[1:]\n    mapping = {\n        \"cp_type\": {\"trt_cp\": 0, \"ctl_vehicle\": 1},\n        \"cp_time\": {48: 0, 72: 1, 24: 2},\n        \"cp_dose\": {\"D1\": 0, \"D2\": 1},\n    }\n    start_col = 0 if include_vehicle else 1\n    categorical_cols = cols[start_col:3]\n    numerical_cols = cols[3:]\n    all_features.loc[:, categorical_cols] = np.stack(\n        [all_features[c].apply(lambda x: mapping[c][x]).values for c in categorical_cols],\n        axis=1,\n    )\n    return all_features, categorical_cols, numerical_cols\n\n\ndef prepare_data(base_dir, add_pca=False, include_vehicle=False,\n                 stat: bool = False,\n                 return_dataframe: bool = False,\n                 pca_g: int = 600,\n                 pca_c: int = 81,\n                 ):\n    base_train = pd.read_csv(str(Path(base_dir) \/ \"train_features.csv\"))\n    train_targets_scored = pd.read_csv(str(Path(base_dir) \/ \"train_targets_scored.csv\"))\n    test_features = pd.read_csv(str(Path(base_dir) \/ \"test_features.csv\"))\n    train_features = base_train\n    # train_features = base_train\n    if not include_vehicle:\n        train_rows = train_features.shape[0]\n        test_rows = test_features.shape[0]\n        train_features = train_features.loc[\n            base_train[\"cp_type\"] == \"trt_cp\"\n        ].reset_index(drop=True)\n        train_targets_scored = train_targets_scored.loc[\n            base_train[\"cp_type\"] == \"trt_cp\"\n        ].reset_index(drop=True)\n        test_features = test_features.loc[\n            test_features[\"cp_type\"] == \"trt_cp\"\n        ].reset_index(drop=True)\n        print(f\"exclude vehicle train rows {train_rows} -> {train_features.shape[0]}\"\n              f\" test rows {test_rows} -> {test_features.shape[0]}\")\n    all_features = pd.concat([train_features, test_features], ignore_index=True)\n    all_features, categorical_cols, numerical_cols = handle_category(\n        all_features,\n        include_vehicle=include_vehicle,\n    )\n    if add_pca:\n        scaler = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n        all_features.loc[:, numerical_cols] = scaler.fit_transform(all_features[numerical_cols])\n\n    pca_start_col = -1\n    if add_pca:\n        old_cols = all_features.shape[1]\n        pca0 = get_pca(all_features, col_prefix=\"g-\", n_comp=pca_g)\n        pca1 = get_pca(all_features, col_prefix=\"c-\", n_comp=pca_c)\n        all_features = pd.concat([all_features, pca0, pca1], axis=1)\n        new_cols = all_features.shape[1]\n        print(f\"pca add {new_cols - old_cols} {old_cols} -> {new_cols}\")\n        pca_start_col = old_cols\n\n    if stat:\n        g_cols = [col for col in all_features.columns if col.startswith(\"g-\")]\n        c_cols = [col for col in all_features.columns if col.startswith(\"c-\")]\n        old_cols = all_features.shape[1]\n        for s in [\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]:\n            all_features[f\"g_{s}\"] = getattr(all_features[g_cols], s)(axis=1)\n            all_features[f\"c_{s}\"] = getattr(all_features[c_cols], s)(axis=1)\n            all_features[f\"gc_{s}\"] = getattr(all_features[g_cols + c_cols], s)(axis=1)\n        new_cols = all_features.shape[1]\n        print(f\"stat add {new_cols - old_cols} {old_cols} -> {new_cols}\")\n    assert all_features.shape[0] == train_features.shape[0] + test_features.shape[0]\n    train_features = all_features.iloc[:train_features.shape[0], :]\n    test_features = all_features.iloc[train_features.shape[0]:, :]\n    if return_dataframe:\n        return train_features, train_targets_scored, test_features\n    start_col = 1 if include_vehicle else 2\n    base_train_np = train_features.iloc[:, start_col:].values.astype(np.float)\n    X_test = test_features.iloc[:, start_col:].values.astype(np.float)\n    targets_tr = train_targets_scored.iloc[:, 1:].values.astype(\n        np.float32\n    )\n    assert base_train_np.shape[0] == targets_tr.shape[0]\n    assert base_train_np.shape[1] == X_test.shape[1]\n    return base_train_np, targets_tr, X_test\n","19e6c625":"base_train, targets_tr, test_X = prepare_data(\"..\/input\/lish-moa\/\", add_pca=True, pca_g=375, pca_c=45)\nprint(base_train.shape, targets_tr.shape, test_X.shape)","12355838":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        dct = {\n            \"x\": torch.tensor(self.features[idx, :], dtype=torch.float),\n            \"y\": torch.tensor(self.targets[idx, :], dtype=torch.float),\n        }\n        return dct\n\n\nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, idx):\n        dct = {\"x\": torch.tensor(self.features[idx, :], dtype=torch.float)}\n        return dct\n","c4df8914":"import pytorch_lightning as pl\nimport numpy as np\nimport shutil\nimport torch\nimport torch.nn as nn\nfrom pathlib import Path\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n\ndef predict(test_features, model_path: Path, model=None):\n    test_dataset = TestDataset(test_features)\n    dataloader = torch.utils.data.DataLoader(\n        test_dataset, 128, shuffle=False, drop_last=False\n    )\n    if model is None:\n        model = InvaseModel.load_from_checkpoint(str(model_path))\n    preds = []\n    for data in dataloader:\n        pred = model.predict(data[\"x\"].to(model.device)).cpu().numpy()\n        preds.append(pred)\n    return np.concatenate(preds, axis=0)\n\n\ndef train_(train_X, train_Y, valid_X, valid_Y, test_X, fold, seed, epochs=100):\n    train_dataset = MoADataset(train_X, train_Y)\n    valid_dataset = MoADataset(valid_X, valid_Y)\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, 256,\n                                                   shuffle=True, drop_last=False, num_workers=10,\n                                                   pin_memory=True)\n    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, 256, shuffle=False, drop_last=False,\n                                                   num_workers=10,\n                                                   pin_memory=True)\n    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=10)\n    trainer_root = Path(\"\/kaggle\/log\")\n    if not trainer_root.exists():\n        trainer_root.mkdir(parents=True)\n    trainer = pl.Trainer(gpus=1, callbacks=[early_stop_callback],\n                         progress_bar_refresh_rate=0,\n                         default_root_dir=str(trainer_root),\n                         max_epochs=epochs,\n                         logger=False)\n    model = InvaseModel(input_dim=train_X.shape[1], output_dim=train_Y.shape[1], learning_rate=1e-3,\n                        actor_h_dim=1024, critic_h_dim=1500, num_layers=3, lambda_=0.01)\n    trainer.fit(model, train_dataloader, valid_dataloader)\n    print(f\"val loss best: {trainer.checkpoint_callback.best_model_score}\")    \n    save_path = Path(\"\/kaggle\/model\") \/ f\"ckpt\/{seed}\"\n    if not save_path.exists():\n        save_path.mkdir(parents=True)\n    model_save_path = save_path \/ f\"model_{fold}.ckpt\"\n    shutil.copy(str(Path(trainer.checkpoint_callback.best_model_path)),\n                str(model_save_path))\n    pred = predict(test_X, model_save_path)\n    return pred","69d5cc28":"import sys\n\nsys.path.append('..\/input\/iterativestratification')","63a24d1a":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n\ndef create_fold(base_dir, n_split, seed, include_vehicle):\n    target_df = pd.read_csv(str(Path(base_dir) \/ \"train_targets_scored.csv\"))\n    columns = target_df.columns[1:]\n    drug_df = pd.read_csv(str(Path(base_dir) \/ \"train_drug.csv\"))\n    if not include_vehicle:\n        train_df = pd.read_csv(str(Path(base_dir) \/ \"train_features.csv\"))\n        target_df = target_df[train_df[\"cp_type\"] == \"trt_cp\"].reset_index(drop=True)\n    target_df = target_df.merge(drug_df, on='sig_id', how='left')\n    vc = target_df.drug_id.value_counts()\n    vc1 = vc.loc[vc <= 18].index.sort_values()\n    vc2 = vc.loc[vc > 18].index.sort_values()\n    if n_split >= 2:\n        splits = MultilabelStratifiedKFold(n_splits=n_split, random_state=1 + seed, shuffle=True)\n    else:\n        splits = MultilabelStratifiedKFold(n_splits=10, random_state=1 + seed, shuffle=True)\n    drug_id2fold = {}\n    sig_id2fold = {}\n    tmp = target_df.groupby('drug_id')[columns].mean().loc[vc1]\n    for fold, (idx_t, idx_v) in enumerate(splits.split(tmp, tmp[columns])):\n        drug_id2fold.update({k: fold for k in tmp.index[idx_v].values})\n    if n_split >= 2:\n        splits = MultilabelStratifiedKFold(n_splits=n_split, random_state=1 + seed, shuffle=True)\n    else:\n        splits = MultilabelStratifiedKFold(n_splits=10, random_state=1 + seed, shuffle=True)\n    tmp = target_df.loc[target_df.drug_id.isin(vc2)].reset_index(drop=True)\n    for fold, (idx_t, idx_v) in enumerate(splits.split(tmp, tmp[columns])):\n        sig_id2fold.update({k:fold for k in tmp.sig_id[idx_v].values})\n    target_df['fold'] = target_df.drug_id.map(drug_id2fold)\n    target_df.loc[target_df.fold.isna(), 'fold'] = \\\n        target_df.loc[target_df.fold.isna(), 'sig_id'].map(sig_id2fold)\n    target_df.fold = target_df.fold.astype('int8')\n    folds = []\n    for fold in range(n_split):\n        train_index = target_df[target_df.fold != fold].index\n        valid_index = target_df[target_df.fold == fold].index\n        folds.append((fold, (train_index, valid_index)))\n    return folds\n","b658ec78":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nn_seeds = 2\nn_splits = 7\nepochs = 25\n\n\nBASE_DIR = \"\/kaggle\/input\/lish-moa\/\"\npred_list = []\nfor seed in range(n_seeds):\n    splits = create_fold(BASE_DIR, n_splits, seed, include_vehicle=False)\n    for fold, (index_train, index_valid) in splits:\n        train_X = base_train[index_train]\n        train_Y = targets_tr[index_train]\n        valid_X = base_train[index_valid]\n        valid_Y = targets_tr[index_valid]\n        pred = train_(train_X, train_Y, valid_X, valid_Y, test_X, fold, seed, epochs)\n        pred_list.append(pred)\n\nall_pred = np.stack(pred_list)\nprint(all_pred.shape)","b09da7a3":"submission = pd.read_csv(str(Path(BASE_DIR) \/ \"sample_submission.csv\"))\ntest_features = pd.read_csv(str(Path(BASE_DIR) \/ \"test_features.csv\"))\nprediction = all_pred.mean(0)\nprint(prediction.shape)\nprint(submission.shape)\nsubmission.head()","35441779":"submission.loc[test_features['cp_type']!='ctl_vehicle', submission.columns[1:]] = prediction\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0\nsubmission.to_csv(\"submission.csv\", index=False)","65bdf951":"### Model","264ebcd0":"kfold with train_drug","5b8c8ec6":"### Preprocessing","5cf0a042":"### INVASE\n\nPytorch Lightning implementation of INVASE. I think it has potential, but I don't have time to try it, so I'll share the notebook.\n\n- [Original Paper](https:\/\/openreview.net\/forum?id=BJg_roAcK7)\n- [github](https:\/\/github.com\/jsyoon0823\/INVASE)\n\n![%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202020-11-09%2012.08.36.png](attachment:%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202020-11-09%2012.08.36.png)"}}