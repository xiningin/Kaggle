{"cell_type":{"aafdace3":"code","01e95637":"code","987b5920":"code","2ce74bef":"code","2701d485":"code","8ba2cc84":"code","4862d33f":"code","658fe3e2":"code","850676fd":"code","46113bbe":"code","e89113af":"code","19394414":"code","655c5158":"code","efc266ad":"code","cd03b350":"code","2428b30f":"code","af639a1b":"code","add0d5b5":"code","ca55d53d":"code","75b843e0":"code","0a54fd3c":"code","7560154b":"code","dc333081":"code","d875e741":"code","34723e1f":"code","edeab923":"code","c698b8eb":"code","e3b761e7":"code","8987d66d":"code","a046942c":"code","8dcc6419":"code","7bc6654c":"code","587a5119":"code","7641796d":"code","b0869c22":"code","b17ef105":"code","d880b276":"code","aea1c842":"code","7965abf2":"code","e993549f":"code","dcae44bc":"code","e1eaf37c":"code","123722df":"code","71b25325":"code","18ea1da4":"code","5b2483cb":"code","105cfcc6":"code","5b1d86ea":"code","4ba2c649":"markdown","2dda280a":"markdown","e05078f2":"markdown","dee356bc":"markdown","1dc979e9":"markdown","f03c3f85":"markdown","c237a5ed":"markdown","5fdead15":"markdown","de25d573":"markdown","69a976c0":"markdown","07fd3acf":"markdown","becb832e":"markdown","f799845c":"markdown","5cb4b02f":"markdown","581960af":"markdown","2d2cdfd4":"markdown","71199ea4":"markdown","06f449bd":"markdown","28cb1dfb":"markdown","aa9538f6":"markdown","35ceb868":"markdown","4855db65":"markdown","6f98489e":"markdown","3f529a5d":"markdown","0c3c3599":"markdown","40fff971":"markdown","0f237656":"markdown","5f89688f":"markdown","27d63657":"markdown","83a2d58c":"markdown","c0d806ca":"markdown","e56ba8e5":"markdown","56c1621c":"markdown","3d1ae3d0":"markdown","49f7638a":"markdown","383bbfad":"markdown","abd34c92":"markdown","6e3a12ad":"markdown","5d8693da":"markdown","ec081874":"markdown","646fc626":"markdown","394906d9":"markdown","0fc79d76":"markdown","35e42a76":"markdown","32320c48":"markdown","058f272d":"markdown","accae206":"markdown"},"source":{"aafdace3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n\nfrom scipy.stats import stats\nfrom scipy.stats import zscore\nfrom scipy.stats.mstats import winsorize\nfrom scipy.stats import jarque_bera\nfrom scipy.stats import normaltest\n\nimport sklearn.preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport time\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nfrom sklearn.cluster import DBSCAN\n\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport umap\n\ntitle_font={\"family\": \"arial\", \"weight\": \"bold\", \"color\": \"darkred\", \"size\": 12}\nlabel_font={\"family\": \"arial\", \"weight\": \"bold\", \"color\": \"darkblue\", \"size\": 10}","01e95637":"df= pd.read_csv(\"..\/input\/creating-customer-segments\/customers.csv\")\ndf.head()","987b5920":"df.head().T","2ce74bef":"df.info() #There is 440 rows and 8 columns on data. However 'Channel' and 'Region' columns must be nominal. ","2701d485":"df[\"Channel\"].replace({1:\"h_r_c\", 2:\"Retail\"}, inplace=True)    #Channel:{Hotel\/Restaurant\/Cafe - 1, Retail - 2}\ndf[\"Region\"].replace({1:\"Lisbon\", 2:\"Porto\", 3:\"Other\"}, inplace=True)  #Region:{Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)","8ba2cc84":"df.head() #Looks nice!","4862d33f":"df.isnull().sum()*100\/df.shape[0] #No null value appears.","658fe3e2":"plt.figure(figsize=(12,8))\ncol_names= [\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicatessen\"]\nfor i in range(6):\n    plt.subplot(2,3,i+1)\n    ax=sns.boxplot(x=df[col_names[i]], linewidth=2.5)\n    plt.title(col_names[i], fontdict=title_font)\nplt.show()\n\n#There is appear some outliers.","850676fd":"plt.figure(figsize=(16,8))\ncol_names= [\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicatessen\"]\nfor i in range(6):\n    plt.subplot(3,2,i+1)\n    sns.distplot(df[col_names[i]], kde=False)\n    plt.title(col_names[i], fontdict=title_font)\nplt.show()","46113bbe":"df[\"log_Fresh\"]= np.log(df[\"Fresh\"])\ndf[\"log_Milk\"]= np.log(df[\"Milk\"])\ndf[\"log_Grocery\"]= np.log(df[\"Grocery\"])\ndf[\"log_Frozen\"]= np.log(df[\"Frozen\"])\ndf[\"log_Detergents_Paper\"]= np.log(df[\"Detergents_Paper\"])\ndf[\"log_Delicatessen\"]= np.log(df[\"Delicatessen\"])\n\nplt.figure(figsize=(14,8))\ncol_names_log= [\"Fresh\", \"log_Fresh\", \"Milk\", \"log_Milk\", \"Grocery\", \"log_Grocery\",\n                \"Frozen\", \"log_Frozen\", \"Detergents_Paper\", \"log_Detergents_Paper\",\n                \"Delicatessen\", \"log_Delicatessen\"]\nfor i in range(12):\n    plt.subplot(6,2,i+1)\n    sns.distplot(df[col_names_log[i]], kde=False)\n    plt.title(col_names_log[i], fontdict=title_font)\nplt.show()\n    \n#logarithmic transformation may have worked for get rid of outliers","e89113af":"col_names= [\"Fresh\", \"Milk\", \"Grocery\", \"Frozen\", \"Detergents_Paper\", \"Delicatessen\"]\ncol_names_log= [\"log_Fresh\", \"log_Milk\", \"log_Grocery\", \"log_Frozen\", \"log_Detergents_Paper\", \"log_Delicatessen\"]\nfor col in col_names:\n    comparison= pd.DataFrame(columns={\"col_name\", \"threshold\", \"outliers\", \"outliers_log\"})\n    q75, q25= np.percentile(df[col],[75,25])\n    q75_log, q25_log= np.percentile(np.log(df[col]), [75,25])\n    caa= q75-q25\n    caa_log= q75_log,q25_log \n    for threshold in np.arange(0,5,0.5):\n        min_value= q25-(caa*threshold)\n        max_value= q75+(caa*threshold)\n        min_value_log= q25_log-(caa*threshold)\n        max_value_log= q75_log+(caa*threshold)\n        outliers= len(np.where((df[col]>max_value) | (df[col]<min_value))[0])\n        outliers_log= len(np.where((np.log(df[col])>max_value) | (np.log(df[col])<min_value))[0])\n        comparison= comparison.append({\"col_name\":col, \"threshold\":threshold, \"outliers\":outliers, \"outliers_log\":outliers_log},\n                                           ignore_index=True)\n    display(comparison)\n    \n    #As we see all outliers are running out with logarithmic transformation. \n    #After that we keep going with logarithmic transformation of columns","19394414":"for col in col_names:\n    df[col]= np.log(df[col])\nfor col_log in col_names_log:\n    del df[col_log]\n    \ndf.columns","655c5158":"df.describe()","efc266ad":"plt.figure(figsize=(18,6))\nsns.countplot(data=df, x=df[\"Region\"], hue=df[\"Channel\"], palette=\"Blues_d\", order=[\"Lisbon\", \"Porto\", \"Other\"])\nplt.title(\"REG\u0130ON\", fontdict=title_font)\nplt.xlabel(\"Region\", fontdict=label_font)\nplt.ylabel(\"Count\", fontdict=label_font)\nplt.tick_params(colors=\"black\")\nplt.show()\n","cd03b350":"df.corr()","2428b30f":"correlation_matrix=df.corr()\nplt.figure(figsize=(15,8))\nsns.heatmap(correlation_matrix, square=True, annot=True, vmin=0, vmax=1, linewidth=0.5)\nplt.title(\"Correlation Matrix Of Continuous Features\", fontdict=title_font)\nplt.show()\n\n#Only a relationship appears between Fresh and Milk columns.","af639a1b":"df= pd.concat([df, pd.get_dummies(df[\"Channel\"], drop_first=True), pd.get_dummies(df[\"Region\"], drop_first=True)], axis=1)\ndf.drop(columns=[\"Channel\", \"Region\"], axis=1, inplace=True)\ndf.head()  #We have transformed categorical columns to dummy.","add0d5b5":"# Before clustering, we transform features from original version to standardize version\nscaler= StandardScaler()\ndf_std= scaler.fit_transform(df)","ca55d53d":"#Creating model\nk_means= KMeans(n_clusters=3, random_state=123) \n%timeit k_means.fit(df_std) # \"\"%timeit\" shows the running time of the model\nclustering= k_means.predict(df_std)\n\n#plot cluster size\nplt.hist(clustering)\nplt.title(\"Sales Per Cluster\", fontdict=title_font)\nplt.xlabel(\"Clusters\", fontdict=label_font)\nplt.ylabel(\"Sales\", fontdict=label_font)\nplt.show()","75b843e0":"#Trying model with different n_clusters and compare the silhouette scores \nsilhouette= pd.DataFrame(columns={\"n_clusters\", \"silhouette_score\"})\nfor n_cluster in range(2,20):\n    k_means= KMeans(n_clusters=n_cluster, random_state=123).fit_predict(df_std)\n    s_scores= metrics.silhouette_score(df_std, k_means, metric=\"euclidean\")\n    silhouette= silhouette.append({\"n_clusters\": n_cluster, \"silhouette_score\": s_scores}, ignore_index=True)\nsilhouette.sort_values(by=\"silhouette_score\", ascending=False)","0a54fd3c":"#plot scores\ns_scores=[]\nfor n_cluster in range(2,15):\n    s_scores.append(metrics.silhouette_score(df_std, KMeans(n_clusters=n_cluster).fit_predict(df_std), metric=\"euclidean\"))\n    \nn= [2,3,4,5,6,7,8,9,10,11,12,13,14]\nsns.barplot(x=n, y=s_scores, palette=\"Blues_d\")\nplt.title(\"Comparison The Silhouette Scores\", fontdict=title_font)\nplt.xlabel(\"Number of Clusters\", fontdict=label_font)\nplt.ylabel(\"Silhouette Scores\", fontdict=label_font)\nplt.show()","7560154b":"agg_cluster= AgglomerativeClustering(n_clusters=3, affinity=\"euclidean\").fit_predict(df_std)\n\nplt.hist(agg_cluster)\nplt.title(\"Sales Per Cluster\", fontdict=title_font)\nplt.xlabel(\"Clusters\", fontdict=label_font)\nplt.ylabel(\"Sales\", fontdict=label_font)\nplt.show()","dc333081":"#Trying model with different parameters\nlink_df= pd.DataFrame(columns={\"n_clusters\", \"linkage\", \"silhouette_score\"})\nfor n_cluster in range(2,15):\n    for link in [\"ward\",\"complete\",\"average\"]:\n        agg_cluster= AgglomerativeClustering(n_clusters=n_cluster, affinity=\"euclidean\", linkage=link).fit_predict(df_std)\n        s_scores= metrics.silhouette_score(df_std, agg_cluster, metric=\"euclidean\")\n        link_df= link_df.append({\"n_clusters\": n_cluster, \"linkage\":link, \"silhouette_score\":s_scores}, ignore_index=True)\nlink_df= link_df.sort_values(by=\"silhouette_score\", ascending=False)\ndisplay(link_df)","d875e741":"plt.figure(figsize=(10,7))\nsns.barplot(x=link_df.iloc[0:10,2], y=link_df.iloc[0:10,1], palette=\"Blues_d\", hue=link_df[\"linkage\"])\nplt.title(\"Sales Per Cluster\", fontdict=title_font)\nplt.xlabel(\"Clusters\", fontdict=label_font)\nplt.ylabel(\"Silhouette Score\", fontdict=label_font)\nplt.show()","34723e1f":"# Creating model with different parameters\ndbscan_df= pd.DataFrame(columns={\"eps\", \"min_samples\", \"silhouette_score\", \"n_clusters\"})\nfor eps in np.arange(0.8,2,0.1):\n    for min_sample in range(1,10): \n        dbscan_clusters= DBSCAN(eps=eps, min_samples=min_sample).fit(df_std)\n        s_scores=  metrics.silhouette_score(df_std, dbscan_clusters.labels_, metric='euclidean')\n        dbscan_df= dbscan_df.append({\"eps\":eps, \"min_samples\":min_sample, \"silhouette_score\":s_scores,\n                                     \"n_clusters\":len(set(dbscan_clusters.labels_))}, ignore_index=True)\ndbscan_df= dbscan_df.sort_values(by=\"silhouette_score\", ascending=False)\ndisplay(dbscan_df.head())","edeab923":"gmm_cluster= GaussianMixture(n_components=2, random_state=123).fit_predict(df_std)\n\nplt.hist(gmm_cluster)\nplt.title(\"Sales Per Cluster\", fontdict=title_font)\nplt.xlabel(\"Clusters\", fontdict=label_font)\nplt.ylabel(\"Sales\", fontdict=label_font)\nplt.show()","c698b8eb":"# Trying model with different parameters\ngmm_df= pd.DataFrame(columns={\"n_components\", \"silhouette_score\"})\nfor n in range(2,10):\n    gmm_cluster= GaussianMixture(n_components=n, random_state=123).fit_predict(df_std)\n    s_scores=metrics.silhouette_score(df_std, gmm_cluster, metric=\"euclidean\")\n    gmm_df= gmm_df.append({\"n_components\":n, \"silhouette_score\":s_scores}, ignore_index=True)\ngmm_df= gmm_df.sort_values(by=\"silhouette_score\", ascending=False)\ndisplay(gmm_df)","e3b761e7":"k_means_cluster= KMeans(n_clusters=3, random_state=123).fit_predict(df_std)\nhierarchy_cluster= AgglomerativeClustering(n_clusters=None, affinity=\"euclidean\", distance_threshold=4.2, linkage=\"average\").fit_predict(df_std)\ndbscan_cluster= DBSCAN(eps=1.9, min_samples=3).fit(df_std)\ngmm_cluster= GaussianMixture(n_components=2).fit_predict(df_std)\n\nk_means_s= metrics.silhouette_score(df_std, k_means_cluster, metric=\"euclidean\")\nhierarchy_s= metrics.silhouette_score(df_std, hierarchy_cluster, metric=\"euclidean\")\ndbscan_s= metrics.silhouette_score(df_std, dbscan_cluster.labels_, metric=\"euclidean\")\ngmm_s= metrics.silhouette_score(df_std, gmm_cluster, metric=\"euclidean\")\n\ncompare= [[\"KMeans\", k_means_s], [\"Hierarchical\", hierarchy_s], [\"DBSCAN\", dbscan_s], [\"GMM\", gmm_s]]\ncompare= pd.DataFrame(compare, columns=[\"Clustering Method\", \"Silhouette Score\"])\ncompare= compare.sort_values(by=\"Silhouette Score\", ascending=False)\ndisplay(compare)\n","8987d66d":"df[\"cluster\"]=hierarchy_cluster\ndf[\"cluster\"].value_counts() #If we use n_clusters=2, as seems clusters are not suitable. \n                             #Thats why we did not use n_clusters=2. We used distance_threshold=4.2 parameter. ","a046942c":"plt.figure(figsize=(20,12))\nthreshold_list= [3.6, 3.9, 4.2, 4.5, 4.8, 5.1]\nfor i in range(6):\n        plt.subplot(3,2,i+1)\n        dendrogram(linkage(df_std, method=\"average\"), color_threshold=threshold_list[i])\n        plt.title(\"Dendrogram for threshold:= {}\".format(threshold_list[i]), fontdict=title_font)\n        plt.xlabel(\"Sample Index or Cluster Size\", fontdict=label_font)\n        plt.ylabel(\"Distance\", fontdict=label_font)\nplt.show()","8dcc6419":"hierarchy_cluster= AgglomerativeClustering(n_clusters=None, affinity=\"euclidean\", distance_threshold=4.2,\n                                            linkage=\"average\").fit_predict(df_std)\ndf[\"cluster\"]=hierarchy_cluster\ndf[\"cluster\"].value_counts()  #We can use 3 cluster that names \"1\", \"2\", and \"other\" cluster","7bc6654c":"df[\"cluster\"]= k_means_cluster  #It looks good.\ndf[\"cluster\"].value_counts()","587a5119":"df_norm= sklearn.preprocessing.normalize(df_std)\ndf_norm.shape","7641796d":"pca= PCA(n_components=7).fit(df_norm)\npca_all= pca.fit_transform(df_norm)\n\nprint(\"Original sahpe of data: {}\".format(df_norm.shape))\nprint(\"Transformed shape of data: {}\".format(pca_all.shape))","b0869c22":"np.cumsum(pca.explained_variance_ratio_)  #Explained variance scores.","b17ef105":"plt.plot(np.cumsum(pca.explained_variance_ratio_))","d880b276":"tsne= TSNE(n_components=2, perplexity=40, n_iter=300).fit(df_std)\ntsne2= tsne.fit_transform(df_std)\nprint(\"Original shape of data: {}\".format(df_std.shape))\nprint(\"Transformed shape of data: {}\".format(tsne2.shape))","aea1c842":"df_tsne= pd.DataFrame(tsne2, columns={\"D1\", \"D2\"})\ndf_tsne.head()","7965abf2":"plt.figure(figsize=(12,6))\nplt.scatter(df_tsne[\"D1\"], df_tsne[\"D2\"], c=KMeans(n_clusters=3, random_state=123).fit_predict(tsne2), cmap=\"viridis\" )\nplt.show()","e993549f":"tsne= TSNE(n_components=3, perplexity=40, n_iter=300).fit(df_std)\ntsne3= tsne.fit_transform(df_std)\nprint(\"Original shape of data: {}\".format(df_std.shape))\nprint(\"Transformed shape of data: {}\".format(tsne3.shape))","dcae44bc":"df_tsne3= pd.DataFrame(tsne3, columns={\"D1\", \"D2\", \"D3\"})\ndf_tsne3.head()","e1eaf37c":"fig= px.scatter_3d(df_tsne3, x=df_tsne3[\"D1\"], y=df_tsne3[\"D2\"], z=df_tsne3[\"D3\"],\n                   color=KMeans(n_clusters=3, random_state=123).fit_predict(tsne3))\nfig.show()","123722df":"umap_fit= umap.UMAP(n_neighbors=5, min_dist=0.3,metric=\"correlation\")\nresult_umap= umap_fit.fit_transform(df_norm)\nprint(\"Original shape of data: {}\".format(df_norm.shape))\nprint(\"Transformed shape of data: {}\".format(result_umap.shape))","71b25325":"df_umap= pd.DataFrame(result_umap, columns={\"D1\", \"D2\"})\ndf_umap.head()","18ea1da4":"plt.figure(figsize=(12,6))\nplt.scatter(df_umap[\"D1\"], df_umap[\"D2\"], c=KMeans(n_clusters=3, random_state=123).fit_predict(result_umap), cmap=\"viridis\")\nplt.show()","5b2483cb":"kmeans3= KMeans(n_clusters=3, random_state=123).fit(df_std)\ndf[\"cluster\"]=kmeans3.labels_","105cfcc6":"columns= ['Fresh', 'Milk', 'Grocery', 'Frozen', 'Detergents_Paper',\n           'Delicatessen', 'h_r_c', 'Other', 'Porto']\nfor col in columns:\n    plt.figure(figsize=(14,3))\n    for i in range(0,3):\n        plt.subplot(1,3,i+1)\n        cluster= df[df[\"cluster\"]==i]\n        cluster[col].hist()\n        plt.title(\"{} \\n {}\".format(col, i), fontdict=title_font)\n    plt.tight_layout()\n    plt.show()\n        \n        \n\n","5b1d86ea":"fig= px.scatter_3d(df, x=df[\"Fresh\"], y=df[\"Milk\"], z=df[\"Grocery\"], color=df[\"cluster\"], title=\"Fresh - Milk - Grocery\")\nfig1= px.scatter_3d(df, x=df[\"Frozen\"], y=df[\"Detergents_Paper\"], z=df[\"Delicatessen\"], color=df[\"cluster\"],\n                    title=\"Frozen - Detergents And Paper - Delicatessen\")\nfig2= px.scatter_3d(df, x=df[\"h_r_c\"], y=df[\"Other\"], z=df[\"Porto\"], color=df[\"cluster\"], title=\"h_r_c - Porto - Other\")\n\nfig.show()\nfig1.show()\nfig2.show()","4ba2c649":"Some statistical significance about the data.","2dda280a":"### 3-Overview About Outliers ","e05078f2":"Correlation Matrix","dee356bc":"We don't use any dimesnsion reduction methods.","1dc979e9":"#### 5.2- Hierarchy Clustering","f03c3f85":"Apply PCA by all dimensions.","c237a5ed":"Review outliers with histogram.","5fdead15":"#### 5.1- K-Means","de25d573":"### 2-Checking For Null Values","69a976c0":"### 5-Clustering Methods","07fd3acf":"### 4-Exploratory Data Analaysis","becb832e":"TSNE in 3 Dimensions.","f799845c":"### 1- General Information Of Data","5cb4b02f":"Visulization of data","581960af":"- Best parameters: \n* eps=0.5\n* min_samples=1\n- Best silhouette score:0.238782","2d2cdfd4":"General \u0131nformation. ","71199ea4":"It doesn't seem we need PCA transform.","06f449bd":"TSNE in 2 Dimensions: (We will use the standardize data while applying TSNE to the data.)","28cb1dfb":"#### Examination of the correlation between continuous features.","aa9538f6":"As we see the most reasonable threshold value is 4.2. Let's check again hierarchical clustering.","35ceb868":"The customer segments data is included as a selection of 440 data points collected on data found from clients of a wholesale distributor in Lisbon, Portugal. \nFeatures\n- Fresh: annual spending (m.u.) on fresh products (Continuous);\n- Milk: annual spending (m.u.) on milk products (Continuous);\n- Grocery: annual spending (m.u.) on grocery products (Continuous);\n- Frozen: annual spending (m.u.) on frozen products (Continuous);\n- Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous);\n- Delicatessen: annual spending (m.u.) on and delicatessen products (Continuous);\n- Channel: {Hotel\/Restaurant\/Cafe - 1, Retail - 2} (Nominal)\n- Region: {Lisbon - 1, Oporto - 2, or Other - 3} (Nominal)","4855db65":"### 8- Understanding The Clusters","6f98489e":"#### 7.3- UMAP","3f529a5d":"#### 7.1- PCA","0c3c3599":"# CUSTOMER SEGMENTATION RESEARCH PROJECT\n#### 1-General Information of the Data\n#### 2-Checking for Null Values\n#### 3-Overview About Outliers\n#### 4-Exploratory Data Analaysis\n#### 5-Clustering Methods\n#### 6-Comparing Results for the Optimal Number of Cluster Model\n#### 7-Dimension Reduction\n#### 8-Understanding The Clusters\n#### 9-Results","40fff971":"We will check also KMeans clustering.","0f237656":"#### 5.4- GMM","5f89688f":"#### We will see the results again after dimension reduction methods. After this, we will continue with our KMeans model. We will look at the effects of PCA, UMAP and T-SNE on our Kmeans model.","27d63657":"General view of first five row the data.","83a2d58c":"### 9- Results","c0d806ca":"- Best parameters: n_clusters=3\n- Best silhouette score: 0.308643","e56ba8e5":"Data must be normalized to apply PCA.","56c1621c":"#### 7.2 T-SNE","3d1ae3d0":"- Best paameters: n_clusters=2 linkage=average\n- Silhouette Score:0.488268","49f7638a":"#### We will review with Tukey's Method.","383bbfad":"Importing useful libraries.","abd34c92":"### 7- Dimension Reduction","6e3a12ad":"Let's check again the outliers with logarithmic transformation.","5d8693da":"Visualization of the data","ec081874":"UMAP with 2 dimensions.","646fc626":"Visualization of the data","394906d9":"After trying different methods, I decided on Kmeans with 3 clusters. We havn't use any dimension reduction methods. Some of the results about cluster.\n1. Cluster 0: This cluster mostly includes customers it is in Lisbon and other cities. Than, these customers also mostly buy \n   milk, grocery, detergent and paper products. And sales in this cluster are mostly retail.\n2. Cluster 1: This cluster mostly includes customers it is in Porto. Than, these customers also mostly buy milk and grocery. \n3. Cluster 2: This cluster mostly includes customers it is in Lisbon and other cities. Sales in this cluster are mostly hotel,\n   restourant and cafe. ","0fc79d76":"The Hieararchical method seems has the highest silhouette score. However, we have to control the number of members of the cluster.","35e42a76":"### 6-Comparing Results for the Optimal Number of Cluster Model","32320c48":"Best Parameters: \n- n_components=2\n- silhouette_score=0.284682","058f272d":"#### 5.3- DBSCAN","accae206":"#### Feature Engineering"}}