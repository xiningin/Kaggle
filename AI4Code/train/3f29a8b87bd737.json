{"cell_type":{"ceb4e296":"code","0910b902":"code","d79ab47f":"code","0dffce36":"code","54d122d4":"code","35024fed":"code","13f7b16a":"code","eb629782":"code","90d31b57":"code","58883e65":"code","dede89f2":"code","0f8cf196":"code","e544e7c8":"code","44cc9534":"code","1426b55d":"code","1b5596d5":"code","7e6910be":"code","c5cf754a":"code","6f1da99b":"code","44b86b20":"code","fa2667d2":"code","9e011a7d":"code","d88088c4":"code","a4cf4216":"code","cc949f18":"markdown","d9f0b92d":"markdown","afd9bb4b":"markdown","34698bae":"markdown","a6b31788":"markdown","f2fa822b":"markdown","9a367abc":"markdown","9c3de47b":"markdown","e5f1af1f":"markdown","220e1d14":"markdown","eda3204c":"markdown","26dbf525":"markdown","7abacdbe":"markdown","ad8ee0cc":"markdown"},"source":{"ceb4e296":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","0910b902":"df = pd.read_csv('\/kaggle\/input\/democratic-debate-transcripts-2020\/debate_transcripts_v3_2020-02-26.csv', encoding='cp1252')\ndf['date'] = pd.to_datetime(df.date)\ndf.head()","d79ab47f":"df = df.loc[df.speaker.isin({'Joe Biden', 'Elizabeth Warren', 'Bernie Sanders', 'Pete Buttigieg', 'Amy Klobuchar', 'Michael Bloomberg', 'Tom Steyer', 'Tulsi Gabbard'})]\ndf.speaker.value_counts()","0dffce36":"df.groupby(by='speaker').speaking_time_seconds.sum().plot.bar()\nplt.show()","54d122d4":"# Multi-Index on debate, candidate\ndebate_candidate_time = df.groupby(by=['date', 'speaker']).speaking_time_seconds.sum()\n# Most recent debate\ndebate_candidate_time['2020-02-25']","35024fed":"# Multi-Index on candidate, debate\ncandidate_debate_time = df.groupby(by=['speaker', 'date']).speaking_time_seconds.sum()\ncandidate_debate_time","13f7b16a":"# Print Median Speaking Times\nfor candidate in df.speaker.unique():\n    med = round(candidate_debate_time[candidate].median()\/60)\n    print(f'{candidate}: {med} minutes (median)')","eb629782":"# Plot Speaking Times Line Graph\nplt.figure(figsize=(20,10))\nfor candidate in df.speaker.unique():\n    candidate_debate_time[candidate].plot(label=candidate)\nplt.legend()\nplt.xlabel('date')\nplt.ylabel('num seconds')\nplt.title('Candidate Speaking Time over time')\nplt.show()","90d31b57":"import spacy\n\nnlp = spacy.load('en_core_web_sm')","58883e65":"# I add additional stops that lead to lower quality topics\nadditional_stops = {'things', 'way', 'sure', 'thing', 'question', 'able', 'point', 'lot', 'time'}","dede89f2":"from spacy.util import filter_spans\n\n\ndef _remove_stops(span):\n    while span and span[0].pos_ not in {'ADJ', 'NOUN', 'PROPN'}:\n        span = span.doc[span.start+1:span.end]\n    return span\n\n\n# Resource: \n# https:\/\/github.com\/explosion\/spacy\/blob\/master\/examples\/information_extraction\/entity_relations.py\ndef merge_ents_and_nc(doc):\n    spans = list(doc.ents) + list(doc.noun_chunks)\n    spans = list(map(_remove_stops, spans))\n    spans = filter_spans(spans)\n    with doc.retokenize() as retokenizer:\n        for span in spans:\n            if span:\n                # Added this in from the code - need to lemmatize better and keep ent types\n                root = span.root\n                attrs = {'LEMMA': span.text.lower(), 'POS': root.pos_, 'ENT_TYPE': root.ent_type_}\n                retokenizer.merge(span, attrs=attrs)\n\n\ndef to_terms_list(doc):\n    merge_ents_and_nc(doc)\n    return [term.lemma_ for term in doc if len(term.lemma_) > 2 and\\\n             not term.is_stop and\\\n             term.pos_ in {'NOUN', 'PROPN', 'ADJ'} and\\\n             term.lemma_ not in additional_stops and\\\n             \"crosstalk\" not in term.lower_]","0f8cf196":"from spacy.tokens.doc import Doc\n\ncorpus = list(nlp.pipe(df.speech.values, n_threads=4))","e544e7c8":"terms = [to_terms_list(doc) for doc in corpus]\ndf['terms'] = terms","44cc9534":"import itertools\nfrom collections import Counter\n\ndef most_frequent(terms, k):\n    flat_terms = list(itertools.chain(*terms))\n    return Counter(flat_terms).most_common(k)","1426b55d":"mf = df.groupby(by='speaker').terms.apply(lambda terms: most_frequent(terms, 10))\nspeakers = mf.index\ntop_terms = mf.values\nfor s, tt in zip(speakers, top_terms):\n    print(f\"{s}: {tt}\\n\")","1b5596d5":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import NMF\n\n# Useful hack: this dummy allows us to use our extracted terms from the methods above, overriding sklearn's tokenizer.\ndummy = lambda x: x\n\nvectorizer = TfidfVectorizer(max_df=0.5, min_df=10, preprocessor=dummy, tokenizer=dummy, ngram_range=(1,2))\nfeatures = vectorizer.fit_transform(terms)","7e6910be":"N=8\ntm = NMF(n_components=N)\ndoc_topic_matrix = tm.fit_transform(features)\ntopic_term_matrix = tm.components_\nterms = vectorizer.get_feature_names()\n\n# Array where rows are docs, columns are topics\nprint(doc_topic_matrix.shape)","c5cf754a":"def top_topic_terms(topic_term_matrix, i, n_terms=10):\n    topic = topic_term_matrix[i]\n    return [(terms[idx], topic[idx]) for idx in np.argsort(topic)[::-1][:n_terms]]","6f1da99b":"for i in range(N):\n    print(i)\n    print(top_topic_terms(topic_term_matrix, i))","44b86b20":"names = {0: 'The American People',\n         1: 'The President of the United States',\n         2: 'Biden Telling us what he is saying is a matter of fact',\n         3: 'The American Country',\n         4: 'Global Diplomatic Issues',\n         5: 'Social\/Constitutional Rights',\n         6: 'Healthcare and Medicare',\n         7: 'Donald Trump'}","fa2667d2":"for i in range(N):\n    col = f\"topic_{i}\"\n    df[col] = doc_topic_matrix[:,i]\ndf.head()","9e011a7d":"def percent_comp(x):\n    return x\/x.sum()","d88088c4":"df_speaker_topics = df.groupby(by=['speaker'])[df.columns[-N:]].sum()\ndf_speaker_topics","a4cf4216":"import matplotlib.pyplot as plt\n\ntopic_speaker_dist = df_speaker_topics.apply(percent_comp, axis=1)\n\nfor i, topic in enumerate(topic_term_matrix):\n    print(names[i])\n    print(' | '.join(terms[idx] for idx in np.argsort(topic)[::-1][:10]))\n    topic_speaker_dist[f'topic_{i}'].plot.bar()\n    plt.title(f'Topic: {names[i]}')\n    plt.xlabel('Candidate')\n    plt.ylabel('Percent of Topic')\n    plt.show()","cc949f18":"## Simple is Better than Complex. My Simple Idea with Topic Modeling\n1. Create a Corpus of Documents from `df.speech.values`\n2. Most Frequent Terms by Speaker\n3. Vectorize using `sklearn.feature_extraction.text.TfidfVectorizer`, using parsed documents as output from `to_terms_list` as input\n4. Use NMF Topic Model, `sklearn.decomposition.NMF`, to break our corpus into `N` topics.","d9f0b92d":"Notice how the most successful candidate, Bernie Sanders, speaks less about Donald Trump and focuses much more on healthcare, medicare, and the american people.\n\nCompare this to second tier candidates like Buttigieg and Klobuchar who have \"president\" or \"donald trump\" surface higher in their most frequent terms.","afd9bb4b":"## Load the data - It is encoded in cp1252","34698bae":"## Speakers who are candidates still in the race:\n- Joe Biden\n- Elizabeth Warren\n- Bernie Sanders\n- Pete Buttigieg\n- Amy Klobuchar\n- Michael Bloomberg\n- Tom Steyer\n- Tulsi Gabbard","a6b31788":"We can see that Elizabeth Warren is given the most time to speak, followed by Bernie Sanders and Joe Biden, then Buttigieg, Klobuchar, and Bloomberg.\n\nWe can see that Sanders has had the majority of speaking time recently, and despite his just recent entry into the race, Michael Bloomberg spoke for the second longest time in the recent debate.\n\nSurprisingly, Amy Klobuchar has quite a bit of speaking time despite her low performance in the polls.","f2fa822b":"## Topic Modeling\nI take a heuristic approach to topic modeling, and have executed the below cells a few times to find a suitable value for `N`. I look for a mix of granularity and qualitative meaning.\n\nAfter iterative attempts, I chose **8** topics, and I do a human pass through to summarize them. One particularly great approach for helping decide the number of topics is *Stability*, as seen in the paper: *[How Many Topics? Stability Analysis for Topic Models*](https:\/\/arxiv.org\/pdf\/1404.4606.pdf). I need to clean up my code for that, and will implement it here in the coming days, stay tuned...","9a367abc":"## About NMF:\n**NMF** stands for **Non-negative Matrix Factorization** and it is a form of dimensionality reduction. \n\nThe idea is:\n\nGiven some non-negative matrix $X$, find two matrices, $W$ and $H$, such that $W\\cdot H \\approx X$   \n\nWithout getting too deep into the math, here is a high level leyman's overview of what happens:\n\n1. Initialize matrices $W$ and $H$ with random values.\n2. Perform $W \\cdot H$ and compare the values to the original matrix X\n3. Now, adjust these values incrementally through a number of iterations to optimize.\n\n\nThe output of this is the matrices $W$ and $H$. In the case of text processing where $X$ is some matrix containing text feature weights, these represent the 'document-topic matrix' and 'topic-term matrix' respectively.\n\nThe 'document-topic matrix', or `doc_topic_matrix` in our code, has the shape `len(corpus), n_topics` and represents the topic distribution for each document.\n\nThe 'topic-term matrix', `topic_term_matrix` in our code, has the shape `n_topics, len(terms)`, and represents the term distribtuion for each topic, i.e. topic 0 consists dominantly of terms A, B, C, ...","9c3de47b":"### Candidate Speaking Time over time","e5f1af1f":"## Speaker-Topic DataFrame\nI create a `pandas.DataFrame` to display: for each speaker, their total weight in each topic.\nThis can help answer two things:\n1. If we apply `percent_comp`, the percent composition on each row, we get the percentage each candidate talks about each topic - which topics they focus on.\n2. If we apply `percent_comp` on each column, we'll see who dominates a topic.","220e1d14":"### Candidate Speaking Time","eda3204c":"## Filter to only the above mentioned candidates","26dbf525":"## Speaking Time EDA","7abacdbe":"## Define Methods for Parsing the texts\n- I will be using `spaCy` as I have found their parsing to be quick and accurate.\n\nOur goal will be to turn a document into a list of terms. I will not be using a DNN like *Bert*, and therefore will be doing a good deal of preprocessing, including stop word removal, lemmatization, and noun chunk\/entity merging.","ad8ee0cc":"## Percent of Topic, by Candidate\nShow, for each topic, which candidates talk about it the most, by the above `df_speaker_topics`"}}