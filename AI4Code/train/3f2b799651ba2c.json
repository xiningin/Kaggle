{"cell_type":{"fc42e9a6":"code","90a2d335":"code","d5976195":"code","22de9c5a":"code","de01257f":"code","31232503":"code","12d5cf5e":"code","5252e966":"code","966d8bfc":"code","a7278f5e":"code","3827cc70":"code","83ada3ba":"code","46895e12":"code","25e422ba":"code","cd171855":"code","157c3a62":"code","8b394a74":"code","09d82820":"code","42bb7768":"markdown"},"source":{"fc42e9a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","90a2d335":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col=[\"PassengerId\"])\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", index_col=[\"PassengerId\"])","d5976195":"X_train = train.drop(columns=\"Survived\")\nY = train[\"Survived\"]\nY = Y.drop([62,830])","22de9c5a":"unq = X_train.Ticket.unique().tolist()\nprint(len(unq))","de01257f":"X_train.Ticket.value_counts()","31232503":"len(X_train.Cabin.unique())","12d5cf5e":"X_train.isnull().sum()","5252e966":"X = X_train.drop(columns=[\"Name\",\"Cabin\"])\n# X.Embarked = X.drop.Embarked.dropna(axis=0)\nX = X.drop([62,830])\n","966d8bfc":"test = test.drop(columns=[\"Name\",\"Cabin\"])\ntest.isnull().sum()","a7278f5e":"# X.info()\nnumerical_cols = [\"Age\"]\ncategorical_cols = [\"Sex\",\"Ticket\",\"Embarked\"]\nX.isnull().sum()","3827cc70":"# One hot encoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X[categorical_cols]))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(test[categorical_cols]))\n\nOH_cols_train.index = X.index\nOH_cols_valid.index = test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X.drop(categorical_cols, axis=1)\nnum_X_valid = test.drop(categorical_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n","83ada3ba":"# imputation\ntrain_X_plus = OH_X_train.copy()\ntest_X_plus = OH_X_valid.copy()\n\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(train_X_plus))\nimputed_X_valid_plus = pd.DataFrame(my_imputer.transform(test_X_plus))\n\n# Imputation removed column names; put them back\nimputed_X_train_plus.columns = train_X_plus.columns\nimputed_X_valid_plus.columns = test_X_plus.columns\n\nX_train = imputed_X_train_plus\nX_valid = imputed_X_valid_plus","46895e12":"# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# scaler.fit(X_train)\n# scaled_df = scaler.transform(X_train)\n# X_train = pd.DataFrame(scaled_df, columns=X_train.columns)","25e422ba":"X_train","cd171855":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_Y, test_Y = train_test_split(X_train,Y)","157c3a62":"import xgboost as xgb\n\nlearning_rate = [10,20,30,40,50,60]\nfor rate in learning_rate:\n    model = xgb.XGBClassifier(learning_rate=0.16, n_estimators=10,\n                              silent=True, objective='binary:logistic', booster='gbtree')\n\n    model.fit(train_X,train_Y)\n    preds = model.predict(test_X)\n    score = mean_absolute_error(test_Y, preds)\n\n    print('MAE: ', score)","8b394a74":"import xgboost as xgb\nmodel = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\n\nmodel.fit(X_train,Y)\npreds = model.predict(X_valid)\n# score = mean_absolute_error(test_Y, preds)\n\n# print('MAE: ', score)","09d82820":"output = pd.DataFrame({\"PassengerId\": test.index, \"Survived\": preds})\noutput.to_csv(\"XGB_approach_tuned2.csv\", index=False)\n\n# TO DO:\n# cross-validation, hyperparameters optimization, check knn model, check ensemble learning models, do standardization or normalization","42bb7768":"STILL WORSE THAN VERSION 2- BEST RESULTS OBTAINED WITH RANDOM FOREST CLASSIFIER"}}