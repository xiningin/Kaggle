{"cell_type":{"9cae03bd":"code","a0ae29c0":"code","a4c3d49e":"code","4970c83f":"code","8cdebb4c":"code","fd4be7b4":"code","8660b4e6":"code","9d371a52":"code","9256e54f":"code","3764da47":"code","72116bbd":"code","92e31489":"code","105d155a":"code","5f0cb8cc":"code","b34dac3d":"code","4ffd8aa5":"code","09f574a5":"code","c9aa2b4a":"code","7926b03d":"code","36825018":"code","72bffef0":"code","bb90a925":"code","7885946d":"code","7220773b":"code","51f77cee":"code","94286c9e":"code","877d27bb":"code","f898e12d":"code","c715e7f6":"code","681af376":"code","8af88f4f":"code","b3552eca":"code","4f19c881":"code","17da2f5c":"code","58a4849b":"code","1297e93b":"code","680d4ded":"code","a9a9773f":"code","79937cf2":"code","973fbb0f":"code","75a63b2a":"code","ae42fa23":"code","88aa6b78":"code","84ec2a1c":"code","8568950c":"code","ada0dbf0":"code","7ffeb658":"code","a7f8c2f7":"code","08b26da1":"markdown","ecbd4ece":"markdown","0e303dca":"markdown","247e33ab":"markdown","39a02896":"markdown","8689fb15":"markdown","d4e80fc6":"markdown","2f7dc21c":"markdown","f617b594":"markdown","09fa5085":"markdown","20a143c4":"markdown","9ce69a7a":"markdown","dcc61af1":"markdown","80594537":"markdown","7b5bb8e6":"markdown","abf59f33":"markdown","8a866859":"markdown","517d47c5":"markdown","cb3d21d7":"markdown","daf27b9f":"markdown","46184ce8":"markdown","ac634a72":"markdown","655b94bc":"markdown","ebd05aa6":"markdown","58c92302":"markdown","e83e5fc4":"markdown","fcb8f218":"markdown","17ade8b1":"markdown","3e9c1d62":"markdown","908241c0":"markdown","cdf62ed2":"markdown","4e04a05f":"markdown","cfd478a4":"markdown","a2ccd4ee":"markdown","27056477":"markdown"},"source":{"9cae03bd":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntrain.shape, test.shape","a0ae29c0":"train.head()","a4c3d49e":"train.isnull().sum()","4970c83f":"len(train['url_legal'].unique())","8cdebb4c":"len(train['license'].unique())","fd4be7b4":"sns.displot(train['target'])\nplt.show()","8660b4e6":"max_len_train = max(map(lambda x: len(x), train['excerpt']))\nmax_len_test = max(map(lambda x: len(x), test['excerpt']))\n\nprint('MAX EXCERPT LENGTH | TRAIN: ', max_len_train)\nprint('MAX EXCERPT LENGTH | TEST: ', max_len_test)","9d371a52":"import torch\n\nimport random\nimport numpy as np\nfrom tqdm import tqdm\n\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntorch.__version__","9256e54f":"if torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\nprint(f\"# Using device: {device}\")","3764da47":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train, test_size=0.2,random_state=0)\ntrain.shape, val.shape, test.shape","72116bbd":"train.to_csv('train_data.csv',index=False)\nval.to_csv('val_data.csv',index=False)\ntest.to_csv('test_data.csv',index=False)","92e31489":"from transformers import AutoTokenizer\n\nROBERTA_BASE_PATH = r'..\/input\/roberta-base'\n\ntokenizer = AutoTokenizer.from_pretrained(ROBERTA_BASE_PATH, do_lower_case=True)","105d155a":"len(tokenizer.vocab)","5f0cb8cc":"tokens = tokenizer.tokenize('Hello WORLD how ARE yoU?')\n\nprint(tokens)","b34dac3d":"indexes = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(indexes)","4ffd8aa5":"init_token = tokenizer.cls_token\neos_token = tokenizer.sep_token\npad_token = tokenizer.pad_token\nunk_token = tokenizer.unk_token\n\nprint(init_token, eos_token, pad_token, unk_token)","09f574a5":"init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\neos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\npad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\nunk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n\nprint(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)","c9aa2b4a":"init_token_idx = tokenizer.cls_token_id\neos_token_idx = tokenizer.sep_token_id\npad_token_idx = tokenizer.pad_token_id\nunk_token_idx = tokenizer.unk_token_id\n\nprint(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)","7926b03d":"tokenizer.max_model_input_sizes","36825018":"max_input_length = tokenizer.max_model_input_sizes['roberta-base']\n\nprint(max_input_length)","72bffef0":"def tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence) \n    tokens = tokens[:max_input_length-2]\n    return tokens","bb90a925":"from torchtext import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = tokenize_and_cut,\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  init_token = init_token_idx,\n                  eos_token = eos_token_idx,\n                  pad_token = pad_token_idx,\n                  unk_token = unk_token_idx)\n\nTARGET = data.LabelField(dtype = torch.float, use_vocab=False)","7885946d":"fields = [('id',None), ('url_legal',None), ('license', None), \n          ('excerpt', TEXT), ('target', TARGET), ('standard_error', None)]","7220773b":"%%time\n\ntrain_data, valid_data, test_data = data.TabularDataset.splits(\n                                        path = '',\n                                        train = '.\/train_data.csv',\n                                        validation = '.\/val_data.csv',\n                                        test = '.\/test_data.csv',\n                                        format = 'csv',\n                                        fields = fields,\n                                        skip_header = True\n)","51f77cee":"print(f\"Number of training examples: {len(train_data)}\")\nprint(f\"Number of validation examples: {len(valid_data)}\")\nprint(f\"Number of testing examples: {len(test_data)}\")","94286c9e":"print(vars(train_data.examples[6]))","877d27bb":"max_len_train = max(map(lambda x: len(vars(x)['excerpt']), train_data.examples))\nmax_len_val = max(map(lambda x: len(vars(x)['excerpt']), valid_data.examples))\nmax_len_test = max(map(lambda x: len(vars(x)['excerpt']), test_data.examples))\n\n\nprint('MAX TOKENIZED LENGTH | TRAIN: ', max_len_train)\nprint('MAX TOKENIZED LENGTH | VAL: ', max_len_val)\nprint('MAX TOKENIZED LENGTH | TEST: ', max_len_test)","f898e12d":"tokens = tokenizer.convert_ids_to_tokens(vars(train_data.examples[6])['excerpt'])\n\nprint(tokens)","c715e7f6":"BATCH_SIZE = 64\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE, \n    device = device,\n    sort=False)","681af376":"from transformers import AutoModel\n\nbert = AutoModel.from_pretrained(ROBERTA_BASE_PATH)","8af88f4f":"import torch.nn as nn\n\nclass BERTGRUSentiment(nn.Module):\n    def __init__(self,\n                 bert,\n                 hidden_dim,\n                 output_dim,\n                 n_layers,\n                 bidirectional,\n                 dropout):\n        \n        super().__init__()\n        \n        self.bert = bert\n        \n        embedding_dim = bert.config.to_dict()['hidden_size']\n        \n        self.rnn = nn.GRU(embedding_dim,\n                          hidden_dim,\n                          num_layers = n_layers,\n                          bidirectional = bidirectional,\n                          batch_first = True,\n                          dropout = 0 if n_layers < 2 else dropout)\n        \n        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n        \n        #text = [batch size, sent len]\n                \n        with torch.no_grad():\n            embedded = self.bert(text)[0]\n                \n        #embedded = [batch size, sent len, emb dim]\n        \n        _, hidden = self.rnn(embedded)\n        \n        #hidden = [n layers * n directions, batch size, emb dim]\n        \n        if self.rnn.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        else:\n            hidden = self.dropout(hidden[-1,:,:])\n                \n        #hidden = [batch size, hid dim]\n        \n        output = self.out(hidden)\n        \n        #output = [batch size, out dim]\n        \n        return output","b3552eca":"HIDDEN_DIM = 256\nOUTPUT_DIM = 1\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.25\n\nmodel = BERTGRUSentiment(bert,\n                         HIDDEN_DIM,\n                         OUTPUT_DIM,\n                         N_LAYERS,\n                         BIDIRECTIONAL,\n                         DROPOUT)","4f19c881":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","17da2f5c":"for name, param in model.named_parameters():                \n    if name.startswith('bert'):\n        param.requires_grad = False","58a4849b":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","1297e93b":"for name, param in model.named_parameters():                \n    if param.requires_grad:\n        print(name)","680d4ded":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())","a9a9773f":"criterion = nn.MSELoss()","79937cf2":"model = model.to(device)\ncriterion = criterion.to(device)","973fbb0f":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        predictions = model(batch.excerpt).squeeze(1)\n        \n        loss = criterion(predictions, batch.target)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","75a63b2a":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            predictions = model(batch.excerpt).squeeze(1)\n            \n            loss = criterion(predictions, batch.target)\n\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","ae42fa23":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","88aa6b78":"N_EPOCHS = 10\n\nbest_valid_loss = float('inf')\n\nfor epoch in tqdm(range(N_EPOCHS)):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_iterator, optimizer, criterion)\n    valid_loss = evaluate(model, valid_iterator, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f}')","84ec2a1c":"model.load_state_dict(torch.load('model.pt'))","8568950c":"def make_prediction(model, tokenizer, sentence):\n    model.eval()\n    tokens = tokenizer.tokenize(sentence)\n    tokens = tokens[:max_input_length-2]\n    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n    tensor = torch.LongTensor(indexed).to(device)\n    tensor = tensor.unsqueeze(0)\n    prediction = torch.sigmoid(model(tensor))\n    return prediction.item()","ada0dbf0":"submit = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsubmit.head()","7ffeb658":"for i in range(len(test)):\n    text = test.loc[i,'excerpt']\n    excerpt_id = test.loc[i,'id']\n    \n    target = make_prediction(model, tokenizer, text)\n    \n    idx = submit[submit['id']==excerpt_id].index\n    submit.loc[idx,'target']=target\n    \nsubmit","a7f8c2f7":"submit.to_csv('submission.csv',index=False)","08b26da1":"...or by explicitly getting them from the tokenizer.","ecbd4ece":"We can use the `convert_ids_to_tokens` to transform these indexes back into readable tokens.","0e303dca":"### NEXT STEPS\n\n1. Use url_legal and license as features and stack them alongside text features.\n2. Perform hyperparameter tuning","247e33ab":"We see that the tokenized lengths are lesser than maximum sequence length for our pretrained model. \n\n**Hence, we can try experiementing with the maximum sequence length parameter defined above.**","39a02896":"## Inference\n\nWe'll now use the model to make predictions on our test data. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model.","8689fb15":"## Train the Model\n\nAs is standard, we define our optimizer and criterion (loss function). \n\nThe loss function that we will be using is **Mean Squared Error** loss function, since we are predicting a continuous variable.","d4e80fc6":"We can check how many parameters the model has. Our standard models have under 5M, but this one has 112M! Luckily, 110M of these parameters are from the transformer and we will not be training those.","2f7dc21c":"Another thing we need to handle is that the model was trained on sequences with a defined maximum length - it does not know how to handle sequences longer than it has been trained on. We can get the maximum length of these input sizes by checking the `max_model_input_sizes` for the version of the transformer we want to use. In this case, it is 512 tokens.\n\n**You can experiment with changing this value.**","f617b594":"Using the tokenizer is as simple as calling `tokenizer.tokenize` on a string. This will tokenize and lower case the data in a way that is consistent with the pre-trained transformer model.","09fa5085":"Next, we create an instance of our model using standard hyperparameters. \n\n**We can experiment with these parameters in order to obtain better results**","20a143c4":"Let's calculate **maximum tokenized length** for excerpts among splits.","9ce69a7a":"## CommonLit Readability Prize\n\n### Using Transformers for predicting complexity of passages - Regression Problem","dcc61af1":"### Basic EDA","80594537":"**The distribution is indicative of normal distribution**","7b5bb8e6":"We can double check the names of the trainable parameters, ensuring they make sense. As we can see, they are all the parameters of the GRU (`rnn`) and the linear layer (`out`).","abf59f33":"We load the data and create the validation splits as before.","8a866859":"We can numericalize tokens using our vocabulary using `tokenizer.convert_tokens_to_ids`.","517d47c5":"Next, we'll define functions for: performing a training epoch, performing an evaluation epoch and calculating how long a training\/evaluation epoch takes.","cb3d21d7":"Place the model and criterion onto the GPU (if available)","daf27b9f":"We can check an example and ensure that the text has already been numericalized.","46184ce8":"Finally, we'll train our model. Even though we are not training any of the transformer's parameters we still need to pass the data through the model which takes a considerable amount of time on a standard GPU.","ac634a72":"As before, we create the iterators. Ideally we want to use the largest batch size that we can as I've found this gives the best results for transformers.","655b94bc":"We now need to define a function that we will pass to our `TEXT` field that will handle all the tokenization for us. It will also cut down the number of tokens to a maximum length. Note that our maximum length is 2 less than the actual maximum length. This is because we need to append two tokens to each sequence, one to the start and one to the end.","ebd05aa6":"Next, we'll define our actual model. \n\nInstead of using an embedding layer to get embeddings for our text, we'll be using the pre-trained transformer model. These embeddings will then be fed into a GRU to produce a prediction for the complexity of excerpt. We get the embedding dimension size (called the `hidden_size`) from the transformer via its config attribute. The rest of the initialization is standard.\n\nWithin the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. The transformer actually returns the embeddings for the whole sequence as well as a *pooled* output. The [documentation](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertModel) states that the pooled output is \"usually not a good summary of the semantic content of the input, you\u2019re often better with averaging or pooling the sequence of hidden-states for the whole input sequence\", hence we will not be using it. The rest of the forward pass is the standard implementation of a recurrent model, where we take the hidden state over the final time-step, and pass it through a linear layer to get our predictions.","58c92302":"We can now see that our model has under 3M trainable parameters, making it almost comparable to the `FastText` model. However, the text still has to propagate through the transformer which causes training to take considerably longer.","e83e5fc4":"The `tokenizer` has a `vocab` attribute which contains the actual vocabulary we will be using. We can check how many tokens are in it by checking its length.","fcb8f218":"## Preparing Data\n\nFirst, as always, let's set the random seeds for deterministic results.","17ade8b1":"The transformer has already been trained with a specific vocabulary, which means we need to train with the exact same vocabulary and also tokenize our data in the same way that the transformer did when it was initially trained.\n\nLuckily, the transformers library has tokenizers for each of the transformer models provided. In this case we are using the BERT model which ignores casing (i.e. will lower case every word).","3e9c1d62":"## Build the Model\n\nNext, we'll load the pre-trained model, making sure to load the same model as we did for the tokenizer.","908241c0":"The transformer was also trained with special tokens to mark the beginning and end of the sentence, detailed [here](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertModel). As well as a standard padding and unknown token. We can also get these from the tokenizer.\n\n**Note**: the tokenizer does have a beginning of sequence and end of sequence attributes (`bos_token` and `eos_token`) but these are not set and should not be used for this transformer.","cdf62ed2":"We'll load up the parameters that gave us the best validation loss and make predictions on the test set.","4e04a05f":"Now we define our fields. The transformer expects the batch dimension to be first, so we set `batch_first = True`. As we already have the vocabulary for our text, provided by the transformer we set `use_vocab = False` to tell torchtext that we'll be handling the vocabulary side of things. We pass our `tokenize_and_cut` function as the tokenizer. The `preprocessing` argument is a function that takes in the example after it has been tokenized, this is where we will convert the tokens to their indexes. Finally, we define the special tokens - making note that we are defining them to be their index value and not their string value, i.e. `100` instead of `[UNK]` This is because the sequences will already be converted into indexes.\n\nWe also define the target field which is our target variable. We won't be using any vocabulary there since it's a continuous variable.","cfd478a4":"We can get the indexes of the special tokens by converting them using the vocabulary...","a2ccd4ee":"In order to freeze paramers (not train them) we need to set their `requires_grad` attribute to `False`. To do this, we simply loop through all of the `named_parameters` in our model and if they're a part of the `bert` transformer model, we set `requires_grad = False`. ","27056477":"**Finding maximum sequence length, this can help us later defining the maximum sequence length for our transformers model**"}}