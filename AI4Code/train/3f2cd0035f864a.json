{"cell_type":{"32d29c73":"code","1048693c":"code","825bbb44":"code","ee0818c9":"code","18cb81e4":"code","2515fd6a":"code","51aec6c6":"code","9a0b9afe":"code","409f251b":"code","7376f701":"code","58dc59cf":"code","a99c3b38":"code","0d0dc6c1":"code","660747ba":"code","4fe1610e":"code","4e35d472":"code","bcca7b7b":"code","e7e4170b":"code","1da5362a":"code","77058e22":"code","b1324397":"markdown","3b3b1df4":"markdown","ed843335":"markdown","d4b50df3":"markdown","aca49261":"markdown","1ed19f09":"markdown","bde65ea5":"markdown","bfcaac72":"markdown","5e6f9248":"markdown","978b74a9":"markdown","f3cb9d9d":"markdown","98cefbe5":"markdown"},"source":{"32d29c73":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport pylab as plt\nimport seaborn as sns; sns.set()\n\nimport random # Utile pour creer un jeu de validation\nrandom.seed(0)\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport sys","1048693c":"print(\"python\", sys.version)\nfor module in np, pd, tf, keras:\n    print(module.__name__, module.__version__)","825bbb44":"assert sys.version_info >= (3, 5) # Python \u22653.5 required\nassert tf.__version__ >= \"2.0\"    # TensorFlow \u22652.0 required","ee0818c9":"train_dir = \"..\/input\/cactus-dataset\/cactus\/train.csv\"\ntrain_data = pd.read_csv(train_dir)\ntrain_data.has_cactus = train_data.has_cactus.astype(str) # Pour appliquer le preprocessing il faut transformer les variables has_cactus en str","18cb81e4":"train_data.has_cactus.value_counts()","2515fd6a":"print(train_data.shape)","51aec6c6":"import matplotlib.pyplot as mlp\nimport matplotlib.image as mpimg\n\nimg = mpimg.imread(\"..\/input\/cactus-dataset\/cactus\/train\/000c8a36845c0208e833c79c1bffedd1.jpg\")\nplt.axis(\"off\")\nimgplot = mlp.imshow(img)","9a0b9afe":"#nb_train = np.random.rand(len(train_data)) < 0.8\n#train_set = train_data[nb_train]\n#valid_set = train_data[~nb_train]\n\n#datagen = keras.preprocessing.image.ImageDataGenerator(\n#    rescale=1.\/255,\n#    shear_range = 0.2,\n#    zoom_range = 0.2,\n#    horizontal_flip = True,\n#    vertical_flip = True,\n#    preprocessing_function = keras.applications.xception.preprocess_input)\n\n#train_generator = datagen.flow_from_dataframe(\n#    dataframe = train_set,\n#    directory = train_dir,\n#    x_col = 'id',\n#    y_col = 'has_cactus',\n#    target_size = (32,32),\n#    batch_size = 64,\n#    class_mode = 'binary')\n\n#valid_generator = datagen.flow_from_dataframe(\n#    dataframe = valid_set,\n#    directory = train_dir,\n#    x_col = 'id',\n#    y_col = 'has_cactus',\n#    target_size = (32,32),\n#    batch_size = 64,\n#    class_mode = 'binary')","409f251b":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndef generator(train_data, directory, batch_size, target_size, class_mode):\n    \n    x_col = 'id'\n    y_col = 'has_cactus'\n    \n    train_datagen = ImageDataGenerator(\n        rescale = 1.\/255, \n        horizontal_flip = True, \n        vertical_flip = True, \n        validation_split = 0.2)\n\n    train_generator = train_datagen.flow_from_dataframe(\n        train_data, \n        directory = directory, \n        x_col = x_col, \n        y_col = y_col, \n        target_size = target_size, \n        class_mode = class_mode, \n        batch_size = batch_size, \n        shuffle = True, \n        subset = 'training')\n\n    valid_generator = train_datagen.flow_from_dataframe(\n        train_data, \n        directory = directory, \n        x_col = x_col, \n        y_col = y_col, \n        target_size = target_size, \n        class_mode = class_mode, \n        batch_size = batch_size, \n        shuffle = True, \n        subset = 'validation')\n    \n    return train_generator, valid_generator","7376f701":"directory = \"..\/input\/cactus-dataset\/cactus\/train\"\nbatch_size = 64\ntarget_size = (32,32) # On a des images 32x32\nclass_mode = 'binary' # Binary puisque qu'on a un vecteur contenant des 0 ou des 1\n\ntrain_generator, valid_generator = generator(train_data, directory, batch_size, target_size, class_mode)","58dc59cf":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\nfrom tensorflow.keras.layers import BatchNormalization, MaxPooling2D, GlobalAveragePooling2D","a99c3b38":"model = Sequential([\n    Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (32,32,3)),\n    BatchNormalization(),\n    Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (32,32,3)),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    BatchNormalization(),\n    MaxPooling2D(),\n    \n    GlobalAveragePooling2D(),\n    \n    Dense(256, activation = 'relu'),\n    Dropout(0.5),\n    \n    Dense(1, activation = 'sigmoid')\n]) # padding = 'same' permet de ne pas diminuer la taille des images\n   # sigmoid au lieu de softmax puisqu'il est utilis\u00e9 pour une regression logistique a 2 classes (classification), de plus la somme des proba ne doit pas etre \u00e9gale a 1\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy']) # On utilise une crossentropy binaire et non pas une sparse_categorical_crossentropy\n                                      # On utilise l'optimizer Adam, plus rapide que SGD : en ayant effectuer plusieurs test on peut supposer qu'il converge correctement\nmodel.summary()","0d0dc6c1":"#model = keras.models.Sequential([\n    \n#    keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.SeparableConv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.MaxPool2D(pool_size=2),\n#    keras.layers.Dropout(rate=0.4),\n    \n#    keras.layers.SeparableConv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.SeparableConv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n#    keras.layers.BatchNormalization(),\n#    keras.layers.MaxPool2D(pool_size=2),\n#    keras.layers.Dropout(rate=0.4),\n    \n#    keras.layers.Flatten(),\n#    keras.layers.Dense(128, activation=\"relu\"),\n#    keras.layers.Dense(1, activation=\"sigmoid\")\n#])\n\n#model.compile(optimizer= keras.optimizers.SGD(lr=1e-4, momentum=0.9), loss='binary_crossentropy', \n#              metrics=['accuracy'])\n\n#epochs = 40\n#history = model.fit_generator(train_generator,\n#          validation_data=valid_generator,\n#          epochs=epochs,\n#          callbacks = [EarlyStopping(patience=10)])","660747ba":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n\ncallbacks = [EarlyStopping(monitor = 'val_loss', patience = 20),\n             ReduceLROnPlateau(patience = 10, verbose = 1),\n             ModelCheckpoint(filepath = 'best_model.h5', monitor = 'val_loss', verbose = 0, save_best_only = True)]\n\nhistory = model.fit_generator(train_generator,\n          validation_data = valid_generator,\n          epochs = 100,\n          verbose = 1,\n          shuffle = True,\n          callbacks = callbacks,\n          class_weight = class_weights)","4fe1610e":"pd.DataFrame(history.history).plot()\nplt.show()","4e35d472":"model.load_weights(\"best_model.h5\")","bcca7b7b":"def test_gen(test_dir, target_size, batch_size, class_mode):\n    test_datagen = ImageDataGenerator(\n        rescale = 1.\/255)\n\n    test_generator = test_datagen.flow_from_directory(\n        directory = test_dir,\n        target_size = target_size, \n        batch_size = batch_size,\n        class_mode = class_mode,\n        shuffle = False)  \n    return test_generator","e7e4170b":"test_dir = \"..\/input\/\/cactus-dataset\/cactus\/test\/\"\ntarget_size = (32,32)\nbatch_size = 1\nclass_mode = None\n\ntest_generator = test_gen(test_dir, target_size, batch_size, class_mode)","1da5362a":"#pred = model.predict_generator(test_generator,verbose=1)\n#pred_binary = [0 if value < 0.50 else 1 for value in pred]  ","77058e22":"def submission():\n    sample_submission = pd.read_csv(\"..\/input\/cactus-dataset\/cactus\/sample_submission.csv\")\n\n    filenames = [path.split('\/')[-1] for path in test_generator.filenames] # On r\u00e9cupere les noms des images pour en faire une colonne sur notre csv final\n    proba = list(model.predict_generator(test_generator)[:,0]) # On recupere les probabilit\u00e9s pr\u00e9dites par notre modele sur le jeu test (sur lequel on a fait de la data augmentation)\n\n    sample_submission.id = filenames\n    sample_submission.has_cactus = proba\n\n    sample_submission.to_csv('submission.csv', index=False)\n    return sample_submission\n\nsample_submission = submission()\nsample_submission.head()","b1324397":"**1. Import des packages de base**","3b3b1df4":"On peut voir qu'il y a 13136 images avec cactus et 4364 images sans cactus. Le jeu de donn\u00e9es n'est pas \u00e9quilibr\u00e9.","ed843335":"Dans notre programme pr\u00e9c\u00e9dent nous n'avions pas utiliser de callbacks : <br>\n- Early stopping pour eviter un surajustement du jeu d'entrainement\n- ReduceLROnPlateau nous permet de r\u00e9duire le learning rate lorsque la val_accuracy ne s'ameliore plus\n- ModelCheckpoint va nous permettre de sauvegarder le modele apres chaque epoch, notamment en observant les val_loss <br> <br>\n\nNotre jeu d'image n'est pas \u00e9quilibr\u00e9, on utlise la fonction compute_class_weight pour estimer le poids des classes du jeu d'image : <br>\n- 'balanced' va permettre de \"r\u00e9pliquer\" la classe inf\u00e9rieure (0 : pas de cactus) jusqu'a obtenir autant d'echantillions que la classe sup\u00e9rieure (1 : cactus) et donc de r\u00e9equilibrer les poids","d4b50df3":"**2. Import du csv d'apprentissage et observations des donn\u00e9es**","aca49261":"**5. Apprentissage sur le jeu test et soumisson**","1ed19f09":"**4. Construction et entrainement du reseau de neurones**","bde65ea5":"On peut directement normaliser les images dans cette fonctions (rescale) et \u00e9galement separer nos donn\u00e9es en deux pour en faire un jeu d'apprentissage et de validation.","bfcaac72":"Avant de pr\u00e9dire des probabilit\u00e9s, nous voulions afficher uniquement des 0 ou des 1, mais les resultats n'etaient pas concluant.","5e6f9248":"> Dans une pr\u00e9c\u00e9dente version du programme nous n'avions pas fait de data augmentation sur les images. Cela nous donnait un score beaucoup plus faible. <br>\nDe plus nous n'utilisions pas le parametre pour separer les donn\u00e9es dans la fonction ImageDataGenerator, nous le faisions a la main. <br>\nNous avons ensuite tent\u00e9 de faire de la data augmentation en tatonnant pour ajuster les parametres.","978b74a9":"Dans un premier temps nous avions fait le reseau de neurone suivant mais les resultats n'etaient pas concluant, nous avions tenter de remplacer les Conv2D par des SeparableConv2D :","f3cb9d9d":"**3. Data augmentation des images train**","98cefbe5":"Par manque de temps nous n'avons pas pu tester le preprocess avec xception (en supposant que les images soient pr\u00e9entrain\u00e9es)."}}