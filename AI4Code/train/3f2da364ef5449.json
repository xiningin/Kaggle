{"cell_type":{"1cd62d3a":"code","ab046e81":"code","06be57a5":"code","31a32d06":"code","c6936180":"code","d4372893":"code","48850faf":"code","632dcb87":"code","53b3149a":"markdown","cc5fc792":"markdown","e7749697":"markdown","ef83870b":"markdown","e2e136cd":"markdown"},"source":{"1cd62d3a":"from fastai.tabular.all import *","ab046e81":"%%time\nSPLIT_IDX = 2493988 # train\/val split at 80% of time_ids\ndata_df = pd.read_feather('..\/input\/ubiquant-trainfeather-32-bit\/train32.feather')\nval_df = data_df.iloc[SPLIT_IDX:].copy()\n\nftrs = [f'f_{i}' for i in range(300)]\nfeature_tensor = torch.tensor(data_df[ftrs].to_numpy()).cuda()\ntarget_tensor = torch.tensor(data_df.target.to_numpy()).cuda()\n\ndel data_df","06be57a5":"class UbiquantDataset:\n    def __init__(self, feature_tensor, targets):\n        store_attr()\n        self.n_inp = 2\n    def __getitem__(self, idx):\n        return torch.empty(0),self.feature_tensor[idx], self.targets[idx, None]\n    \n    def __len__(self):\n        return len(self.feature_tensor)\n    \nclass UbiDL(DataLoader):\n    def __iter__(self):\n        if self.shuffle:\n            self.__idxs = torch.tensor(range(0,self.n))\n        else:\n            self.__idxs = torch.tensor(range(0,self.n))\n        for batch_start in range(0, self.n, self.bs):\n            if batch_start + self.bs > self.n and self.drop_last:\n                return \n            indices = self.__idxs[batch_start:batch_start+self.bs]\n            yield self.dataset[indices]","31a32d06":"def pearson_coef(data):\n    return data.corr()['target']['preds']\n\nclass CompMetric(AccumMetric):\n    def __init__(self, val_df):\n        super().__init__(None)\n        self.val_df = val_df\n        \n    @property\n    def name(self):\n        return 'Pears'\n        \n    @property\n    def value(self):\n        preds = torch.cat(self.preds)\n        val_df['preds'] = preds.cpu().numpy()\n        return np.mean(self.val_df[['time_id', 'target', 'preds']].groupby('time_id').apply(pearson_coef))","c6936180":"def pearson_loss(x, y):\n    xd = x - x.mean()\n    yd = y - y.mean()\n    nom = (xd*yd).sum()\n    denom = ((xd**2).sum() * (yd**2).sum()).sqrt()\n    return 1 - nom \/ denom","d4372893":"ds_train = UbiquantDataset(feature_tensor[:SPLIT_IDX], target_tensor[:SPLIT_IDX])\nds_val = UbiquantDataset(feature_tensor[SPLIT_IDX:], target_tensor[SPLIT_IDX:])\n\ndls = DataLoaders.from_dsets(ds_train, ds_val , bs = 4096,dl_type=UbiDL, num_workers=0)","48850faf":"model = TabularModel(emb_szs={}, n_cont=300, out_sz=1, layers = [128,64, 32,16]).cuda()\n\nlearn = Learner(dls, model, loss_func=pearson_loss, metrics = CompMetric(val_df))","632dcb87":"%%time\nlearn.fit(5, 1e-3)","53b3149a":"The goal of this notebook is to provide a lightweight and fast starting point for training MLP models. It uses a very simple model and is optimized for speed so the whole thing including dataloading and training takes around a minute and half to run on kaggle. Still using the same parameters I got a respectable single model score: `0.143` when trained on the whole dataset.\n\nThings to note:\n- All the data is preloaded into GPU and then directly passed to model without any copying. \n- I'm using a custom version of pearson coefficient for loss.\n- There is no regularization, just a small model and small number of epochs to prevent overfitting.\n- A custom callback allows me to track the competition metric while training.\n- A flat constant learning rate, I found that LR scheduling didn't help with just five epochs.\n- A large batch size makes training fast.","cc5fc792":"### Load the data from feather and stick on GPU","e7749697":"### Use fast.ai Learner for trainig","ef83870b":"### Barebone lightweight dataloading","e2e136cd":"### A custom metric and loss function for training"}}