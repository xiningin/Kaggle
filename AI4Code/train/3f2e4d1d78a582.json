{"cell_type":{"810aa2ea":"code","993ebb84":"code","b28fda42":"code","bcc4dadf":"code","52ea6c5b":"code","6af67a40":"code","dc3fdcd8":"code","44aec569":"code","b10370b8":"code","e53937a8":"code","deb422e6":"code","ee3c4b44":"code","271168e3":"code","28a157aa":"code","b9b1335f":"code","0580795f":"code","b776a26f":"code","7e00823d":"code","5ccb18e8":"code","1b112343":"code","6149d03c":"code","eda73570":"markdown","928a6821":"markdown","ece13529":"markdown","de8e67d5":"markdown","ff675dd6":"markdown","7130ba44":"markdown","78839cd8":"markdown","1dac2d0b":"markdown","b8c8e595":"markdown","5d7f60b3":"markdown","7915cb2d":"markdown","30d3ee59":"markdown","a4e03355":"markdown","68472ef1":"markdown","ed4b7375":"markdown","d590cb10":"markdown","f39aa193":"markdown","38b1ea33":"markdown","00e826a0":"markdown","329ee4eb":"markdown","1a851adf":"markdown","76e91d59":"markdown"},"source":{"810aa2ea":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.utils import shuffle\nfrom keras.utils import to_categorical\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader","993ebb84":"np.random.seed(27)\ntorch.manual_seed(27)","b28fda42":"test_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntrain_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","bcc4dadf":"test_df.head()","52ea6c5b":"train_df.head()","6af67a40":"def to_tensor(data):\n    return [torch.FloatTensor(point) for point in data]\n\nclass MNISTDataset(Dataset):\n    def __init__(self, df, X_col, y_col):\n        self.features = df[X_col].values\/255\n        self.targets = df[y_col].values.reshape((-1, 1))\n\n    def __len__(self):\n        return len(self.targets)\n    \n    def __getitem__(self, idx):\n        return to_tensor([self.features[idx], self.targets[idx]])","dc3fdcd8":"y_col = \"label\"\ntest_df[y_col] = [-1]*len(test_df)\n\nsplit = int(0.8*len(train_df))\nvalid_df = train_df[split:].reset_index(drop=True)\ntrain_df = train_df[:split].reset_index(drop=True)","44aec569":"X_col = list(train_df.columns[1:])\n\ntrain_set = MNISTDataset(train_df, X_col, y_col)\nvalid_set = MNISTDataset(valid_df, X_col, y_col)\n\nvalid_loader = DataLoader(valid_set, batch_size=1024, shuffle=True)\ntrain_loader = DataLoader(train_set, batch_size=1024, shuffle=True)","b10370b8":"class MLP(nn.Module):\n    def __init__(self, i, u, v, o):\n        super(MLP, self).__init__()\n        self.relu_layer = nn.ReLU()\n        self.dense_1 = nn.Linear(i, u)\n        self.dense_2 = nn.Linear(u, v)\n        self.dense_output = nn.Linear(v, o)\n        \n    def forward(self, x):\n        x = self.relu_layer(self.dense_1(x))\n        x = self.relu_layer(self.dense_2(x))\n        logits = self.dense_output(x); return logits","e53937a8":"device = torch.device('cuda')\nnetwork = MLP(i=784, u=20, v=15, o=10).to(device)\noptimizer = Adam(params=network.parameters(), lr=0.01)","deb422e6":"print(network)","ee3c4b44":"def cel(y_true, y_pred):\n    y_true = y_true.long().squeeze()\n    return nn.CrossEntropyLoss()(y_pred, y_true)\n\ndef acc(y_true, y_pred):\n    y_true = y_true.long().squeeze()\n    y_pred = torch.argmax(y_pred, axis=1)\n    return (y_true == y_pred).float().sum()\/len(y_true)","271168e3":"start = time.time()\nprint(\"STARTING TRAINING ...\\n\")\n\ntrain_losses, valid_losses = [], []\ntrain_accuracies, valid_accuracies = [], []\n\nfor epoch in range(20):\n    network = network.train()\n    print(\"Epoch: {}\".format(epoch + 1))\n    batch_train_losses, batch_train_accuracies = [], []\n    \n    batch = 0\n    for train_batch in train_loader:\n        train_X, train_y = train_batch\n\n        train_X = train_X.to(device)\n        train_y = train_y.to(device)\n        train_preds = network.forward(train_X)\n        train_loss = cel(train_y, train_preds)\n        train_accuracy = acc(train_y, train_preds)\n        \n        optimizer.zero_grad()\n        train_loss.backward()\n        \n        optimizer.step()\n        train_loss = np.round(train_loss.item(), 3)\n        train_accuracy = np.round(train_accuracy.item(), 3)\n\n        end = time.time()\n        batch = batch + 1\n        log = batch % 10 == 0\n        time_delta = np.round(end - start, 3)\n        \n        batch_train_losses.append(train_loss)\n        batch_train_accuracies.append(train_accuracy)\n        logs = \"Batch: {} || Train Loss: {} || Train Acc: {} || Time: {} s\"\n        if log: print(logs.format(batch, train_loss, train_accuracy, time_delta))\n        \n    train_losses.append(np.mean(batch_train_losses))\n    train_accuracies.append(np.mean(batch_train_accuracies))\n    \n    total_valid_loss = 0\n    total_valid_points = 0\n    total_valid_accuracy = 0\n    \n    with torch.no_grad():\n        for valid_batch in valid_loader:\n            valid_X, valid_y = valid_batch\n            \n            valid_X = valid_X.to(device)\n            valid_y = valid_y.to(device)\n            valid_preds = network.forward(valid_X)\n            valid_loss = cel(valid_y, valid_preds)\n            valid_accuracy = acc(valid_y, valid_preds)\n            \n            total_valid_points += 1\n            total_valid_loss += valid_loss.item()\n            total_valid_accuracy += valid_accuracy.item()\n            \n    valid_loss = np.round(total_valid_loss\/total_valid_points, 3)\n    valid_accuracy = np.round(total_valid_accuracy\/total_valid_points, 3)\n    \n    valid_losses.append(valid_loss)\n    valid_accuracies.append(valid_accuracy)\n    \n    end = time.time()\n    time_delta = np.round(end - start, 3)\n    logs = \"Epoch: {} || Valid Loss: {} || Valid Acc: {} || Time: {} s\"\n    print(\"\\n\" + logs.format(epoch + 1, valid_loss, valid_accuracy, time_delta) + \"\\n\")\n    \nprint(\"ENDING TRAINING ...\")","28a157aa":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(valid_losses)),\n                         y=valid_losses, mode=\"lines+markers\", name=\"valid\",\n                         marker=dict(color=\"indianred\", line=dict(width=.5,\n                                                                  color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_losses)),\n                         y=train_losses, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"darkorange\", line=dict(width=.5,\n                                                                   color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Cross Entropy\",\n                  title_text=\"Cross Entropy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","b9b1335f":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(valid_accuracies)),\n                         y=valid_accuracies, mode=\"lines+markers\", name=\"valid\",\n                         marker=dict(color=\"indianred\", line=dict(width=.5,\n                                                                  color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_accuracies)),\n                         y=train_accuracies, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"darkorange\", line=dict(width=.5,\n                                                                   color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Accuracy\",\n                  title_text=\"Accuracy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","0580795f":"def softmax(x):\n    return np.exp(x)\/np.sum(np.exp(x), axis=1)[:, None]","b776a26f":"test_set = MNISTDataset(test_df, X_col, y_col)\ntest_loader = tqdm(DataLoader(test_set, batch_size=1024, shuffle=False))\n\ntest_preds = []\nwith torch.no_grad():\n    for test_X, _ in test_loader:\n        test_X = test_X.to(device)\n        test_pred = network.forward(test_X)\n        test_preds.append(softmax(test_pred.detach().cpu().numpy()))","7e00823d":"submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")\nsubmission[\"Label\"] = np.argmax(np.concatenate(test_preds, axis=0), axis=1)","5ccb18e8":"test_batch = next(iter(test_loader))[0]\ntest_X = test_batch.reshape(-1, 28, 28)[:36]\nfig, ax = plt.subplots(nrows=6, ncols=6, figsize=(15, 15))\n\nfor i, image in enumerate(test_X):\n    ax[i\/\/6][i%6].axis('off'); ax[i\/\/6][i%6].imshow(image, cmap='gray')\n    ax[i\/\/6][i%6].set_title(np.argmax(test_preds[0][i], axis=0), fontsize=20, color=\"red\")","1b112343":"submission.head(10)","6149d03c":"submission.to_csv(\"submission.csv\", index=False)","eda73570":"# Preparing the ground","928a6821":"## Prepare submission\n\n* Next, we find the indices of maximum probability using <code>np.argmax<\/code>.\n* These indices represent our final prediction from 0 to 9 (for the digit).","ece13529":"The loss seems to converge over time towards a good value (0.18).","de8e67d5":"## Define the PyTorch dataloaders\n\n* Now we create train and valid dataloaders to get data batches for the training and validation loops.","ff675dd6":"## Run inference on the test data\n\n* The final step is to infer the model on the test data.\n* We create a test dataloader and iterate through it using a loop with <code>torch.no_grad()<\/code> enabled.\n* The generated predictions are passed through the <code>softmax<\/code> function to convert logits to probabilities.","7130ba44":"## Submit submission file\n\n* Finally, we convert our submission dataframe to a .csv file for submission.","78839cd8":"## Define binary cross entropy and accuracy\n* Next, we define our loss function (binary cross entropy) and evaluation metric (accuracy).","1dac2d0b":"## Visualize metrics over time\n\n* Now, we visualize how the metrics (loss and accuracy) change over time.","b8c8e595":"The model is able to classify almost all images. But, it is getting confused in certain cases.","5d7f60b3":"## Split train\/valid (80\/20)\n\n* We now split the data into training and validation sets (train: 80%, valid: 20%).","7915cb2d":"## Visualize predictions\n\n* Now, we visualize some sample test predictions.\n* The red label above each image is the prediction made by the model.","30d3ee59":"## Load the training and testing data\n\n* Now we need to load the training and testing data.\n* These dataframes contain the pixel data required to make predictions.","a4e03355":"<center><img src=\"https:\/\/i.imgur.com\/1qejfmD.png\" width=\"750px\"><\/center>\n\n\n# Introduction\n\nIn this tutorial, I will demonstrate how to build a simple MLP (Vanilla Neural Network) to recognize handwritten digits using PyTorch. These handwritten digit images are taken from the famous **MNIST database** which contains several thousands of (28 x 28) grayscale images representing the ten digits from 0 to 9. MNIST is the classic \"Hello World!\" of machine learning. It is a great place to get started with Deep Learning. The task is to take an image as input and predict which digit it is from 0 to 9 (classification).","68472ef1":"## Import necessary libraries\n\n* Now, we import all the libraries we need.\n* **matplotlib, tqdm, and plotly** for visualization.\n* **numpy, pandas, keras, sklearn, and torch** for modelling.","ed4b7375":"## Define MLP model\n\n* Next, we define the actual model which we are going to train.\n* The MLP model includes an input layer (**784** neurons), and output layer (**10** neurons).\n* There are two hidden layers of **20** and **15** neurons each (between the input and output layers).","d590cb10":"# Takeaways\n\n* The MLP model is able to predict handwritten digits with an accuracy of 94.7 %.\n* The performance can be improved by using Convolutional Neural Networks (CNNs).\n* CNNs are specialized architectures made for computer vision (built to understand spatial features).\n* Using a flattened 784D vector makes it difficult to capture spatial relationships. But, an MLP is a great start.","f39aa193":"### Define model and optimizer","38b1ea33":"## Define PyTorch dataset\n\n* The first step is to build a PyTorch dataset to generate data for our model.\n* A PyTorch dataset has three fundamental functions: <code>init<\/code>, <code>len<\/code>, and <code>getitem<\/code>.\n* The <code>init<\/code> function initializes all the components required for data loading (image data and labels data)\n* The <code>len<\/code> function simply returns the length of the dataset. This indicates the number of retrievable samples.\n* The <code>getitem<\/code> function returns a data point at a given index <code>idx<\/code>. The actual logic is written in this function.\n* The <code>getitem<\/code> function does 2 things: gets the target and retrieves the 28 x 28 image at an <code>idx<\/code> (as a vector).","00e826a0":"The accuracy seems to converge over time towards a good value (0.95).","329ee4eb":"## Train the model on GPU\n\n* Now, we train the model on the NVIDIA Tesla P100 GPU provided by Kaggle.\n* First, we do a training loop, where we train the model with back-prop to adjust the parameters.\n* Second, we do a validation loop, to check the model's performance on unseen data after each epoch.\n* The training loop uses forward-prop and back-prop, while the validation loop uses only forward-prop.\n* We use the <code>torch.no_grad()<\/code> flag for the validation loop as no gradients are needed for forward-prop.","1a851adf":"## Set random seeds\n\n* The next step is to set the random seed for NumPy and PyTorch.\n* Setting the random seed helps us keep training determinstic and ensure reproducible results.","76e91d59":"# Modeling\n\n* Next, we need to build a machine learning pipeline to recognize these handwritten digits.\n* I will use a classic Vanilla Neural Network (VNN) also known as a Multi-layer Perceptron (MLP)."}}