{"cell_type":{"0da8420d":"code","7d5a12ba":"code","e14fe24b":"code","0f264f7f":"code","77604170":"code","b1ff8916":"code","9965898b":"code","6f8e5bd6":"code","fc141e1e":"code","6dafe712":"code","a39c18cf":"code","593dd695":"code","1a9a080f":"code","c8dad607":"code","1dbb987e":"code","0e315c3a":"code","e4fe4917":"code","26ad539a":"code","3c9a9236":"code","2642ce8e":"markdown","2b7c6be5":"markdown","f878d97a":"markdown","b691fc89":"markdown","e93b999c":"markdown","9018d07b":"markdown","1e5d649d":"markdown","364639ce":"markdown","081b1826":"markdown","b8199b71":"markdown","6fa21290":"markdown","588bb9d0":"markdown","81415cb4":"markdown","c99d1eb9":"markdown","2ba29743":"markdown","67a7b0f2":"markdown","da1fb470":"markdown","59b5e7c8":"markdown","2176e5af":"markdown","a9989a32":"markdown"},"source":{"0da8420d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# install external library for text augmentation\n!pip install nlpaug\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# plotting libraries\nimport seaborn as sn \nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go # Plotly for interactive data visualizatoins\n\nimport re # regex for text cleaning\nimport nltk # natural language processing tool kit\nimport pickle # for saving\/serializing machine learning models\nimport nlpaug.augmenter.word as naw # text augmentor\n# neural networks and embeddings stuff\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Dropout, Activation\nfrom keras.layers.embeddings import Embedding\n\nfrom nltk.stem import WordNetLemmatizer # lemmatizing tool\nfrom sklearn.feature_extraction.text import TfidfVectorizer # tfidf for text representation\nfrom sklearn.model_selection import train_test_split # for data splitting\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score # for classifier evaluation\nfrom sklearn.ensemble import RandomForestClassifier # random forest machine learning model\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7d5a12ba":"# read csv file into a dataframe\nraw_tweets=pd.read_csv('\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv')\n# show sample of data\nraw_tweets.sample(5)","e14fe24b":"crosstab_sentiments=pd.crosstab(raw_tweets.airline, raw_tweets.airline_sentiment)\ncompanies=list(crosstab_sentiments.index)\n\nfig = go.Figure(data=[\n    go.Bar(name=col_name, x=companies, y=list(crosstab_sentiments[col_name]))\nfor col_name in list(crosstab_sentiments.columns)])\n# Change the bar mode\nfig.update_layout(barmode='stack',\n                  title='Sentiment distribution per company',\n                  yaxis=dict(title='Sentiment distribution'),\n                 xaxis=dict(title='Companies'))\nfig.show()","0f264f7f":"crosstab_neg_reasons=pd.crosstab(raw_tweets.airline,raw_tweets.negativereason)\ncompanies=list(crosstab_neg_reasons.index)\n\nfig = go.Figure(data=[\n    go.Bar(name=col_name, x=companies, y=list(crosstab_neg_reasons[col_name]))\nfor col_name in list(crosstab_neg_reasons.columns)])\n# Change the bar mode\nfig.update_layout(barmode='stack',\n                  title='Negative reasons distribution per company',\n                  yaxis=dict(title='Negative reasons distribution'),\n                 xaxis=dict(title='Companies'))\nfig.show()","77604170":"labels = list(crosstab_neg_reasons.columns)\nvalues = [crosstab_neg_reasons[col_name].sum() for col_name in labels]\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_layout(title='Overall distribution for negative reasons')\nfig.show()","b1ff8916":"fig = go.Figure(\n    [go.Bar(x=list(raw_tweets['airline_sentiment'].unique()),\n            y=raw_tweets['airline_sentiment'].value_counts())]\n)\n# Change the bar mode\nfig.update_layout(\n                  title='Sentiment distribution',\n                  yaxis=dict(title='Count'),\n                 xaxis=dict(title='Sentiment'))\nfig.show()","9965898b":"def clean_text(dataframe):\n    '''\n    function to clean tweets dataframe text , eg : remove white spaces , special chars and website urls\n    '''\n    # select nedded columns\n    dataframe=dataframe[['airline_sentiment','text']]\n    # drop nans\n    dataframe=dataframe.dropna()\n    # convert review into lower case\n    dataframe['text']=dataframe['text'].apply(lambda row : row.lower())\n    # remove numbers\n    dataframe['text']=dataframe['text'].apply(lambda row : re.sub('\\d+','',row))\n    # remove tweet account name\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r'@\\w+', '',row))\n    # remove website urls\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r'http\\S+', '',row))\n    # rwmove special characters\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r\"[^A-Za-z0-9']+\", ' ',row))\n    # remove white spaces\n    dataframe['text']=dataframe['text'].apply(lambda row:re.sub(r'\\s+', ' ',row))\n    \n    return dataframe\n\ntraindf=clean_text(raw_tweets)","6f8e5bd6":"lem = WordNetLemmatizer()\ndef stem_review(review):\n    return ' '.join([lem.lemmatize(word) for word in review.split(' ')])\ndef normalize_text(dataframe):\n    '''\n    function used for rooting \/ stemming tokens\n    '''\n    dataframe['text']=dataframe['text'].apply(lambda x : stem_review(x))\n\nnormalize_text(traindf)","fc141e1e":"def augment_text(dataframe):\n    '''\n    function used to augment minor classes in tweets dataset\n    '''\n    augmented_df = pd.DataFrame(columns=['text', 'airline_sentiment'])\n    augmentor=naw.WordNetAug(aug_p=0.5)\n    aug_sent={'neutral':3,\n             'positive':4}\n    for i in range(len(dataframe)):\n        current_label=dataframe['airline_sentiment'].iloc[i]\n        if current_label in list(aug_sent.keys()):\n            current_text=dataframe['text'].iloc[i]\n            for j in range(aug_sent[current_label]):\n                text = augmentor.augment(current_text)\n                label = current_label\n                tempdf = pd.DataFrame(list(zip([text], [label])), columns=['text', 'airline_sentiment'])\n                augmented_df = augmented_df.append(tempdf)\n        else :\n                text = dataframe['text'].iloc[i]\n                label = current_label\n                tempdf = pd.DataFrame(list(zip([text], [label])), columns=['text', 'airline_sentiment'])\n                augmented_df = augmented_df.append(tempdf)\n    return augmented_df\naugmented_traindf=augment_text(traindf)","6dafe712":"# convert text into numeric values using tf-idf vectorizer\ntfidfconverter = TfidfVectorizer(\n                                 min_df=5, \n                                 max_df=0.7,\n                                 ngram_range=(1,2),\n                                 stop_words='english'\n                                )  \nX = tfidfconverter.fit_transform(augmented_traindf['text']).toarray()\ny = augmented_traindf['airline_sentiment'].values","a39c18cf":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\ntext_classifier = RandomForestClassifier(n_estimators=400,n_jobs=-1,verbose=True)\ntext_classifier.fit(X_train, y_train)","593dd695":"# model testing\npredictions = text_classifier.predict(X_test)\nprint(\"Classification Report :\\n {} \\n Model Acurracy = {}\".format(classification_report(y_test,predictions),\n                                                                 accuracy_score(y_test, predictions)))\n# confusion matrix\nprint(\"\\n Confusion Matrix\")\ndf_cm = pd.DataFrame(confusion_matrix(y_test, predictions),['negative','neutral','positive'],['negative','neutral','positive'])\nsn.set(font_scale=1.2)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 20})\n","1a9a080f":"pkl_filename = \"classifier_random_forest.pkl\"  \nwith open(pkl_filename, 'wb') as file:  \n    pickle.dump(text_classifier, file)\nprint(\"Model Saved\")","c8dad607":"ycat=pd.get_dummies(augmented_traindf['airline_sentiment']).values\nX=augmented_traindf['text'].values\ntk = Tokenizer()\ntk.fit_on_texts(X)\nX_seq = tk.texts_to_sequences(X)\nX_pad = pad_sequences(X_seq, maxlen=100, padding='post')\nX_train, X_test, y_train, y_test = train_test_split(X_pad, ycat, test_size = 0.25, random_state = 1)","1dbb987e":"vocabulary_size = len(tk.word_counts.keys())+1\nmax_words = 100\nembedding_size = 32\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\nmodel.add(Flatten())\nmodel.add(Dense(3, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","0e315c3a":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),batch_size=32,epochs=4,verbose=True)","e4fe4917":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","26ad539a":"# model testing\npredictions = [np.argmax(i) for i in model.predict(X_test)]\ny_test=[np.argmax(i) for i in y_test]\nprint(\"Classification Report :\\n {} \\n Model Acurracy = {}\".format(classification_report(y_test,predictions,\n                                                                                         target_names=['negative','neutral','positive']),\n                                                                 accuracy_score(y_test, predictions)))\n# confusion matrix\nprint(\"\\n Confusion Matrix\")\ndf_cm = pd.DataFrame(confusion_matrix(y_test, predictions),['negative','neutral','positive'],['negative','neutral','positive'])\nsn.set(font_scale=1.2)#for label size\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 20})","3c9a9236":"model.save('text_classifier_neural_network.h5')\nprint('model_saved')","2642ce8e":"When we notice the nature of our dataset , that most of features are categorical features , \nso i think that par and pie plots should be a very good visualization techinques for our dataset EDA\n","2b7c6be5":"**Exploratory Analysis**","f878d97a":"the model shows good performance in the evaluation stage , so we may save it to a use as a base model for future enhancements","b691fc89":"**Par plot to show sentiment class distributions**","e93b999c":"from previous plot , we notice that the dataset may suffer from class imbalance problem , we may try to oversample the minority classes using text augmentation technique , nlpaug is very helpful tool , I will use it to oversample neutral and positive classes , note : nlpaug does not blindly copies data , but it apply random word variations with the same meaninig (high cosine similarity)\ncheck text augmentation section below after data cleaning and stemming","9018d07b":"the model shows good performance in the evaluation stage , so we may save it to a use as a base model for future enhancements","1e5d649d":"visualizing model behavior during training","364639ce":"**Classification using Deep learning techniques**\n\n\nwe will train an **embedding layer followed by dense layer of 3 neurons with batch size =32 for 4 epochs** , i choosed this arhitecture to keep the network simple as i tried adding more dense layers and the model was training for 10 epochs but the model has symptoms of overfitting also the train and validation error curves were increasing after 4 epochs .","081b1826":"train an embedding layer with Fully connected neural network","b8199b71":"we see that most of reviews are negative for most of the companies , to help these companies take better decesions we need to focus on negative reviews\n\n\n**Stacked par chart to show negative reasons distributions per company**","6fa21290":"now we will build a classifier using classical machine learning algorithms , then we will use advanced deep learning techniques , from my experience with text classifiers the combination between Tfidfvectorizer with Random forest algorithm has never failed me :)","588bb9d0":"**Text augmentation**\n\nto solve class imbalance problem we shall augment\/oversample minor classes","81415cb4":"finally as it was predicted , training an embedding layer with fully connected neural network (deep learning methods) shows better performance than conventional text vectorization techniques and machine learning algorithms","c99d1eb9":"**Model training**","2ba29743":"**Stacked par chart to show the distribution of reviews per company**","67a7b0f2":"Now we shall start second part of the problem , we need to build an efficient text \/ sentiment classifier to predict customer review sentiment","da1fb470":"**Model evaluation**","59b5e7c8":"**Text cleaning and preprocessing**","2176e5af":"**Text Representation**\n\ni will use TfidfVectorizer for sentiment vectorization , from many experiments i have done before with sentiment\/reviews datasets , below configuration gives good results ","a9989a32":"**Pie plot to check the overall distribution for negative reasons**"}}