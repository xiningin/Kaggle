{"cell_type":{"136a8035":"code","ac8d6e96":"code","bf6e15cc":"code","2b4e9749":"code","14637e44":"code","db124678":"code","8435b90e":"code","d53abd58":"code","bc986190":"code","0f0866fb":"code","261168f4":"code","df3df99c":"code","c38f2105":"code","20c5b865":"code","c301aa03":"code","483dcd92":"code","934a84a2":"code","07f6727d":"code","11f8f226":"code","ad5b8f37":"code","07b2821a":"code","00cff6bf":"code","27759aeb":"code","5a27d689":"code","aab0ccc2":"code","70560ff5":"code","d7885454":"markdown","c0712249":"markdown","f968681f":"markdown","436ead1e":"markdown","c151ffac":"markdown","e904b23b":"markdown","e5b1e825":"markdown","bc24df51":"markdown","bad2698c":"markdown","6f53948c":"markdown","aaeef51f":"markdown","6cd1f61f":"markdown","b7e14efc":"markdown","8d8f508e":"markdown","2c8bb89a":"markdown","e87c3841":"markdown","17db380d":"markdown","00a68584":"markdown","ff4ca4b0":"markdown","b28a3a55":"markdown","306edb7e":"markdown","1e3bc7c8":"markdown"},"source":{"136a8035":"import pandas as pd\nimport numpy as np \nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n\nwarnings.filterwarnings('ignore')\n\n# imported the file which contains top 25 headlines, stock went up or down(label) and date\ndata1 = pd.read_csv('..\/input\/stocknews\/Combined_News_DJIA.csv')\ndata1.head()","ac8d6e96":"data1.isnull().sum()","bf6e15cc":"# filling the null values with median \n\ndata1['Top23'].fillna(data1['Top23'].median,inplace=True)\ndata1['Top24'].fillna(data1['Top24'].median,inplace=True)\ndata1['Top25'].fillna(data1['Top25'].median,inplace=True)","2b4e9749":"# seperating the data into train and test\n\ntrain = data1[data1['Date'] < '20150101']\ntest = data1[data1['Date'] > '20141231']","14637e44":"# removing punctuations and changing all the letters to lowercase for both train and test\n\nall_data = [train,test]\n\nfor df in all_data:\n    df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n    for i in df.columns:\n        if i=='Date':\n            continue\n        if i=='Label':\n            continue\n        df[i] = df[i].str.lower()\n\ntrain.head()","db124678":"# combining all the headlines in train data into one and appending them into a list \n\nheadlines = []\nfor row in range(0,len(train.index)):\n    headlines.append(' '.join(str(x) for x in train.iloc[row,2:]))\nheadlines[0]","8435b90e":"# combining all the headlines in test data into one and appending them into a list \n\ntest_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))","d53abd58":"# Applying countvectorizer on headlines list that we created before and max features is set to 100009\n\ncountvector=CountVectorizer(ngram_range=(2,2),max_features=100009)\ntraindataset=countvector.fit_transform(headlines)\n\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(traindataset,train['Label'])","bc986190":"# Applying countvectorizer on test_transform list that we created before \n\ntest_dataset = countvector.transform(test_transform)\npredictions = randomclassifier.predict(test_dataset)","0f0866fb":"# confusion matrix for \n\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)","261168f4":"# accuracy score (compared test daset original output values with predictions)\n\nscore=accuracy_score(test['Label'],predictions)\nprint(score)","df3df99c":"max_features_num = [500,600,700,800,900,1000]\nngram = [1,2,3,4,5]\nfor i in max_features_num:\n    for j in ngram:\n        countvector=CountVectorizer(ngram_range=(j,j),max_features=i)\n        traindataset=countvector.fit_transform(headlines)\n        test_dataset = countvector.transform(test_transform)\n\n        xgb = XGBClassifier(random_state =1)\n        xgb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\n        predictions = xgb.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))\n        score=accuracy_score(test['Label'],predictions)\n        print('max number of features used : {}'.format(i))\n        print('ngram_range ({},{})'.format(j,j))\n        print(score)\n        matrix=confusion_matrix(test['Label'],predictions)\n        print('confusion matrix : {}'.format(matrix))\n        print('===============================')","c38f2105":"countvector=CountVectorizer(ngram_range=(1,1),max_features=800)\ntraindataset=countvector.fit_transform(headlines)\ntest_dataset = countvector.transform(test_transform)\n\n\nxgb = XGBClassifier(random_state =1)\nxgb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\npredictions = xgb.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))","20c5b865":"predictions","c301aa03":"cb=CatBoostClassifier(random_state=1)\ncb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\npredictions = xgb.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))\nmatrix=confusion_matrix(test['Label'],predictions)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nprint('===============')\nprint(matrix)","483dcd92":"def performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))\n\n\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,300,400],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True,False],\n                                  'max_depth': [None,15, 20],\n                                  'max_features': ['auto', 10],\n                                  'min_samples_leaf': [1,2,5],\n                                  'min_samples_split': [2,3,5]}\n\nclf_rf = GridSearchCV(rf,param_grid = param_grid, cv=5 , verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(traindataset,train['Label'])\nperformance(best_clf_rf,'Random Forest')","934a84a2":"best_rf = best_clf_rf.best_estimator_","07f6727d":"countvector=CountVectorizer(ngram_range=(2,2))\ntraindataset=countvector.fit_transform(headlines)\ntest_dataset = countvector.transform(test_transform)\n\nbest_rf.fit(traindataset,train['Label'])\npredictions = best_rf.predict(test_dataset)\npredictions","11f8f226":"score=accuracy_score(test['Label'],predictions)\nprint(score)\nprint('========================')\nprint('confusion matrix :')\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)","ad5b8f37":"countvector=CountVectorizer(ngram_range=(1,1),max_features=800)\ntraindataset=countvector.fit_transform(headlines)\ntest_dataset = countvector.transform(test_transform)\n\nxgb = XGBClassifier(random_state =1)\nparam_grid = {\n    'n_estimators': [500,550,600,650],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65,0.9],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = RandomizedSearchCV(xgb, param_distributions = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(pd.DataFrame(traindataset.todense(), columns=countvector.get_feature_names()),train['Label'])\nperformance(best_clf_xgb,'XGB')","07b2821a":"best_clf_xgb = best_clf_xgb.best_estimator_\n\nbest_rf.fit(traindataset,train['Label'])\npredictions = best_rf.predict(pd.DataFrame(test_dataset.todense(), columns=countvector.get_feature_names()))\npredictions","00cff6bf":"score=accuracy_score(test['Label'],predictions)\nprint('score :')\nprint(score)\nprint('==================================')\nprint('confusion matrix :')\nmatrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)","27759aeb":"fin_score = {'randomforest (without hp)':0.859788 , 'randomforest (with hp)':0.851851,\n             'XGBoost (without hp)':0.8650793,'XGBoost (with hp)':0.806878,'CATBoost(without hpt)':0.83597}\nimport plotly\nplotly.offline.init_notebook_mode (connected = True)","5a27d689":"px.bar(x = list(fin_score.keys()),y = list(fin_score.values()),title='ACCURACY SCORE FOR RF AND XGB (WITH AND WITHOUR HYPERPARAMETERS)',labels={'x':'Algorithms','y':'Score'})","aab0ccc2":"x1 = ['randomforest(without hpt)','randomforest(with hpt)','XGBoost(without hpt)','XGBoost(with hpt)','CATBoost(without hpt)']\nx1_TP = [135,131,161,139,154]\nX1_FN = [5,1,26,26,30]","70560ff5":"fig = go.Figure(data=[\n    go.Bar(name='TRUE POSITIVE', x=x1, y=x1_TP),\n    go.Bar(name='FALSE NEGATIVE', x=x1, y=X1_FN)\n])\n\nfig.update_layout(barmode='group')\nfig.show()","d7885454":"#### <font color='darkblue'>Thank you :)<\/font>","c0712249":"# <font color='green'>Predicting Stocks (Goes up or down) using News Headlines<\/font>","f968681f":"## <font color='darkred'>Whole process in detail :<\/font>\n### 1)  Filling null values in the dataset with median\n\n### 2)  Combining all the headlines into one news \n\n### 3)  Cleaning the text by removing punctuations and changing all the letters to lowercase\n\n### 4)  Applying countvectorizer to all the headlines\n\n### 5)  Visualizing the results and choosing the best algorithm based on requirements","436ead1e":"## <font color='darkred'> Conclusion<\/font>","c151ffac":"### <font color='darkred'>Random forest without hyperparameter tuning<\/font>","e904b23b":"\n\n<font color='darkblue'>After all this analysis we can conclude that the best algorithm which gave good accuracy and less false negetive values is randomforest using hyperparameter tuning\n\nIf you care about more true positive values and less on false negetive values then the best algorithm for you is XGBOOST without hyperparameter tuning<\/font>","e5b1e825":"<font color='darkblue'>As you can see that performing hyperparameter tuning on randomforest made the model good predictions and also decreased the false negative value.\n<\/font>","bc24df51":"## <font color='darkred'>Data Cleaning<\/font>","bad2698c":"## <font color='darkred'>Applying Machine Learning Algorithms (Random forest , XGBOOST and CATBoost)<\/font>","6f53948c":" ###  <font color='darkred'>XGBoost with hyperparameter tuning <\/font>","aaeef51f":"###  <font color='darkred'>Random forest with hyperparameter tuning <\/font>","6cd1f61f":"#### <font color='darkblue'>If you like my work please do upvote my kernel and if you have any suggestions please do comment below<\/font>","b7e14efc":"\n<font color='darkblue'>Lets use hyperparameter tuning for XGBOOST and see if the accuracy is improving or not<\/font>","8d8f508e":"### * The kernel is all about creating a model to predict the stocks whether they go up or down based on the top 25 headlines \n \n### * The first column is \"Date\", the second is \"Label\", and the following ones are news headlines ranging from \"Top1\" to \"Top25\".\n \n### * In Label column the value is \"1\" when DJIA Adj Close value rose or stayed as the same\n \n### * In Label column the value is \"0\" when DJIA Adj Close value decreased.","2c8bb89a":"<font color='darkblue'>Lets apply XGBoost , and will also try different numbers of max features for countvectorizer and see which number gives us the maximum accuracy<\/font>\n\n\n\n\n### <font color='darkred'>XGBoost without hyperparameter tuning<\/font>","e87c3841":"\n\n<font color='darkblue'>The maximum features for countvectorizer is set to 100009 because, i tried many other numbers for maximum features and for 100009 i got the best accuracy, with lowest False positive values ( you can see below in the confusion matrix you can try other values and check it yourself, if you find the best accuracy with other maximum features then comment below<\/font>","17db380d":"## <font color='darkred'>Objective :<\/font>\n### The goal is to create a machine learning model that predicts whether the stock goes up or down based on top 25 headlines ","00a68584":"<font color='darkblue'>Catboost is giving the same results as xgboost\n\nNow lets use hyperparameters and see whether the model is improving or not \n    \nAt first we will perform hyperparameter tuning for random forest<\/font>","ff4ca4b0":"#### <font color='darkblue'>If you want to have a clear pictures of which model performed well and which model got more true positives and false negatives ,you can see the above visualizations and decide which model you need according to your requirements<\/font>","b28a3a55":"<font color='darkblue'>As you can see above , after using hyperparameters for XGBoost , the accuracy didn't improve <\/font>","306edb7e":"### <font color='darkred'>CATBoost without hyperparameter tuning<\/font>","1e3bc7c8":"<font color='darkblue'>Maximum accuracy :<\/font>\n\nmax number of features used : 800\n\nngram_range (2,2)\n\n0.8650793650793651\n\nconfusion matrix : [[161  25]\n [ 26 166]]"}}