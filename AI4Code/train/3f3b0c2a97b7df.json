{"cell_type":{"25ae3eff":"code","c3cb43d4":"code","7d0b687a":"code","24fabbc5":"code","e5a956fc":"code","647e3ea4":"code","2e5916ea":"code","d35754b9":"code","13ef317f":"code","242bc798":"code","c9b7da3c":"code","ab1abfa1":"code","dd61ca78":"code","e68d9ecd":"code","b57aec61":"code","ffcdeebf":"code","14dcd7bd":"code","5fa70496":"code","bebda59b":"code","018097a2":"code","49a83109":"code","e35b2f42":"code","d107bdf7":"code","7e9822cf":"code","cb155705":"code","82acd935":"code","3883a5a9":"code","690331bb":"code","0699ca78":"code","f4bc7acd":"code","4b6e2286":"code","9f6311f9":"code","4c8106a3":"code","72303066":"code","ada11e5e":"code","6ba1c771":"code","8fb3388a":"code","84eaf90f":"code","5bf2e62b":"code","3fe46da0":"code","76c0a64e":"code","a9a86a42":"code","e0e2772d":"code","1f244568":"code","9a91715b":"code","740f69ce":"code","2691c926":"code","6da092de":"code","e42816b0":"code","18a30fc7":"code","bdcf925d":"code","8515fd10":"code","2ab43945":"code","689721d6":"code","9b4f2b01":"code","df7a3920":"code","1501d7f2":"code","29d244de":"code","f8927f04":"code","10f30a63":"code","9e2397dc":"code","5756cc42":"code","d5ef62c7":"code","0567d354":"code","910fc9b6":"code","2e87d1c1":"code","5265b8cc":"code","9d778694":"code","e2fe6c52":"code","ee4bf2dd":"code","4444a146":"code","0a5da5a6":"code","05e35df9":"code","5df66afe":"code","8fea4c8a":"code","4291dd4e":"code","51657e26":"code","baa8fc1f":"code","7bd16c9c":"code","c275c976":"code","7605548a":"code","4e614a53":"code","fb4b21eb":"code","066444a2":"code","48ecd5f8":"code","886bc1a4":"code","9d768d1f":"code","50bf36fd":"code","48cf2bc0":"code","eca1427d":"code","ce90f2ea":"markdown","bd5d05e5":"markdown","a6cc7991":"markdown","8871c589":"markdown","611a21e2":"markdown","d3067062":"markdown","b6b4cd31":"markdown","1108d7f1":"markdown","fe7075ad":"markdown","9467ffda":"markdown","1dc0ca66":"markdown","e965481f":"markdown","eaaaf5ff":"markdown","6607ddaa":"markdown","35db7a0d":"markdown","65a8fad3":"markdown","c8b38807":"markdown","c8df6f9a":"markdown","48a4f40d":"markdown","ffa0876f":"markdown","8d8c3aea":"markdown","d4e92687":"markdown","cbfbe327":"markdown","4dff2f7b":"markdown","ce432703":"markdown","b2f694d1":"markdown","373c503d":"markdown","4b2ec2b4":"markdown","49a4135b":"markdown","41361ba0":"markdown","51e2e233":"markdown","186a7e6b":"markdown","5007268f":"markdown","916189eb":"markdown","16d63c2e":"markdown","c12225b6":"markdown","a1a2312b":"markdown","ea1f7625":"markdown","500d9c16":"markdown","07c714e2":"markdown","9cef4132":"markdown","6022b3a3":"markdown","c9cf3be2":"markdown","f37dd871":"markdown","a61a11b9":"markdown","ae670d4b":"markdown","939c4082":"markdown","049950a6":"markdown","98883a3a":"markdown","ba77f4db":"markdown","1679e818":"markdown","fe659b87":"markdown","1ab08e83":"markdown","48106e5a":"markdown","a6bba14d":"markdown","73c5c6de":"markdown","6247a7b9":"markdown","445a4489":"markdown","7e7c0101":"markdown","d60cd6b6":"markdown","b095c678":"markdown","67c83ffe":"markdown","e24b6924":"markdown","19b1748c":"markdown","c34302ba":"markdown","47ce9cc4":"markdown"},"source":{"25ae3eff":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\nfrom collections import Counter\nfrom wordcloud import WordCloud\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV","c3cb43d4":"# read in dataset\nkick = pd.read_csv('..\/input\/ks-projects-201801.csv', encoding='latin1')","7d0b687a":"# basic statistics\nprint('Dimension of the dataset:', kick.shape)\n\n# show first 5 observations of dataframe for exploration purposes\nkick.head()","24fabbc5":"# name of the columns\nkick.columns","e5a956fc":"# data type of each column\nkick.get_dtype_counts()","647e3ea4":"# number of missing values per column\nkick.isnull().sum().sort_values(ascending = False)","2e5916ea":"# remove rows with missing values\nkick.dropna(inplace=True)","d35754b9":"# number of unique values per column, sorted by descending order\nkick.T.apply(lambda x: x.nunique(), axis=1).sort_values(ascending=False)","13ef317f":"def categorical_with_per_count(kick, feature):\n    '''\n    Calculate frequency of the categorical feature with % and count base.\n    Sorted on the descending order.\n    '''\n    \n    # calculate frequency on % and value\n    freq_merged = pd.concat([kick[feature].value_counts(normalize=True) * 100,\n                             kick[feature].value_counts(normalize=False)], axis=1)\n    # rename columns\n    freq_merged.columns = [feature + '_%', feature + '_count']\n    return freq_merged","242bc798":"categorical_with_per_count(kick, 'state')","c9b7da3c":"# keep `failed` and `successful` states\nkick.query(\"state in ['failed', 'successful']\", inplace=True)","ab1abfa1":"# class balance of the dataframe\ncategorical_with_per_count(kick, 'state')","dd61ca78":"# select features for further analysis\nkick = kick.loc[:, ['name', 'category', 'main_category', 'deadline',\n                    'usd_goal_real', 'launched', 'state', 'country']]","e68d9ecd":"# rename `usd_goal_real` to `goal`\nkick.rename(columns={'usd_goal_real':'goal'}, inplace=True)","b57aec61":"# frequency of the main category\ncategorical_with_per_count(kick, 'main_category')","ffcdeebf":"# change dimension of the plot\ndims = (10, 8)\nfig, ax = plt.subplots(figsize = dims)\n\n# barplot of the main categories by descending order\nsns.countplot(\n    y=kick.main_category,\n    order = kick['main_category'].value_counts().index\n)","14dcd7bd":"# top 10 the most frequent country\ncategorical_with_per_count(kick, 'country').head(n=10)","5fa70496":"kick.goal.describe()","bebda59b":"# calculate frequency of the goal: the most popular goal\ncategorical_with_per_count(kick, 'goal').head(n=10)","018097a2":"# combine different plots into one: goal and log(goal)\ndims = (14, 8)\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize=dims)\nsns.distplot(kick.goal, ax=ax1)\nsns.distplot(np.log1p(kick.goal), ax=ax2)","49a83109":"# convert strings to `datetime`\nkick['launced'] = pd.to_datetime(kick.launched)\nkick['deadline'] = pd.to_datetime(kick.deadline)\nkick = kick.sort_values('launced')","e35b2f42":"def show_wordcloud(data, title = None):\n    '''Split names by space and generate word counts.'''\n    wordcloud = WordCloud(\n        background_color='white',\n        max_words=100,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","d107bdf7":"# successful Projects\nshow_wordcloud(kick[kick.state == 'successful']['name'])","7e9822cf":"# failed Projects\nshow_wordcloud(kick[kick.state == 'failed']['name'])","cb155705":"# initialize new data frame\nkk = pd.DataFrame()","82acd935":"# length of the name\nkk['name_len'] = kick.name.str.len()","3883a5a9":"# if name contains a question mark\nkk['name_is_question'] = (kick.name.str[-1] == '?').astype(int)","690331bb":"# if name contains an exclamation mark\nkk['name_is_exclamation'] = (kick.name.str[-1] == '!').astype(int)","0699ca78":"# if name is uppercase\nkk['name_is_upper'] = kick.name.str.isupper().astype(float)","f4bc7acd":"def count_non_character(row):\n    '''Number of non character in the sentence'''\n    return sum((0 if c.isalpha() else 1 for c in str(row)))","4b6e2286":"# number of non character in the name\nkk['name_non_character'] = kick.name.apply(count_non_character)","9f6311f9":"# number of words in the name\nkk['name_number_of_word'] = kick.name.apply(lambda x: len(str(x).split(' ')))","4c8106a3":"# We generate new feature based on ratio between vowels and other alpha characters\ndef countVowelstoLettersRatio(s):\n    '''Count ratio between vowels and letters'''\n    s = str(s)\n    count = 1  \n    vowels = 0\n    for i in s:\n        if i.isalpha():\n            count = count + 1\n            if i in 'aeiou':\n                vowels = vowels + 1\n    return ((vowels * 1.0) \/ count)\n\n# for each name calculate vowels ratio\nkk['name_vowel_ratio'] = kick.name.apply(countVowelstoLettersRatio)","72303066":"# create indicator variable for `country` variable\nkk['country_is_us'] = (kick.country == 'US').astype(int)","ada11e5e":"kk['Goal_1000'] = kick.goal.apply(lambda x: x \/\/ 1000)","6ba1c771":"kk['Goal_500'] = kick.goal.apply(lambda x: x \/\/ 500)","8fb3388a":"kk['Goal_10'] = kick.goal.apply(lambda x: x \/\/ 10)","84eaf90f":"# log transformation of `goal` to reduce skewness \nkick['goal'] = np.log1p(kick.goal)\nkk['goal'] = kick['goal']","5bf2e62b":"from datetime import datetime\nimport time\n\ndef to(dt):\n    '''Add timestamp as a value'''\n    return time.mktime(dt.timetuple())\n\nkick['timestamp'] = kick['launced'].apply(to)    ","3fe46da0":"# We will create data frames containing only single main category\ncategories = set(kick.main_category)\nframes = {}\nfor ct in categories:\n    frames[ct] = kick[kick['main_category'] == ct]","76c0a64e":"# We will use Progressbar to track progress as it istime consuming operation\nimport pyprind\npbar = pyprind.ProgBar(331675)\n\n\ndef getElementsInRange(cat,end,week):\n    '''Get number of launched projects in given range from (end - week) to end'''\n    global pbar\n    pob = frames[cat]\n    start = end - pd.DateOffset(weeks = week)\n    # as we sorted our projects by launch date earlier geting number of projects in given date range is easy\n    value = pob['launced'].searchsorted(end)[0] - pob['launced'].searchsorted(start)[0]\n    pbar.update()\n    return value\n# Number of projects in same category for last week    \nkk['Last_Week'] = kick.apply(lambda x: getElementsInRange(x['main_category'],x['launced'],1),axis = 1) ","a9a86a42":"pbar = pyprind.ProgBar(331675)\n# Number of projects in same category for last month    \nkk['Last_Month'] = kick.apply(lambda x: getElementsInRange(x['main_category'],x['launced'],4),axis = 1) ","e0e2772d":"pbar = pyprind.ProgBar(331675)\n# Number of projects in same category for last year    \nkk['Last_Year'] = kick.apply(lambda x: getElementsInRange(x['main_category'],x['launced'],52),axis = 1) ","1f244568":"pbar = pyprind.ProgBar(331675)\n# Number of projects in same category for last 3 months  \nkk['Last_3_Month'] = kick.apply(lambda x: getElementsInRange(x['main_category'],x['launced'],13),axis = 1)","9a91715b":"pbar = pyprind.ProgBar(331675)\n# Number of projects in same category for last 6 months  \nkk['Last_6_Month'] = kick.apply(lambda x: getElementsInRange(x['main_category'],x['launced'],26),axis = 1)","740f69ce":"def getDelta(a,b):\n    '''Get diffence in days between launch and deadline'''\n    return (a - b).days\n\n# Duration of the project   \nkk['Duration'] = kick.apply(lambda x: getDelta(x['deadline'],x['launced']),axis = 1)","2691c926":"## Month of launch\nkk['Month'] = kick['launced'].apply(lambda x : x.month)","6da092de":"# Hour at which project was launched\nkk['Hour'] = kick['launced'].apply(lambda x : x.hour)","e42816b0":"## Month of deadline\nkk['deadline_month'] = kick['deadline'].apply(lambda x : x.month)","18a30fc7":"# indicator feature for weekend\nkk['isLaunchWeekend'] = kick['launced'].apply(lambda x : int(x.weekday() > 5))","bdcf925d":"kk['Category'] = kick['category']\nkk['main_category'] = kick['main_category']","8515fd10":"def getRangeMean(cat,end,week):\n    global pbar\n    pob = frames[cat]\n    start = end - pd.DateOffset(weeks = week)\n    value = pob.iloc[pob['launced'].searchsorted(start)[0]:pob['launced'].searchsorted(end)[0]]['goal'].mean()\n    pbar.update()\n    return value\npbar = pyprind.ProgBar(331675)\n# Mean goal for category last month\nkk['mean_goal_in_category_last_month'] = kick.apply(lambda x: getRangeMean(x['main_category'],x['launced'],4),axis = 1) ","2ab43945":"def getRangeMedian(cat,end,week):\n    global pbar\n    pob = frames[cat]\n    start = end - pd.DateOffset(weeks = week)\n    value = pob.iloc[pob['launced'].searchsorted(start)[0]:pob['launced'].searchsorted(end)[0]]['goal'].median()\n    pbar.update()\n    return value\npbar = pyprind.ProgBar(331675)\n# Median goal for category last month\nkk['median_goal_in_category_last_month'] = kick.apply(lambda x: getRangeMedian(x['main_category'],x['launced'],4),axis = 1) ","689721d6":"pbar = pyprind.ProgBar(331675)\n# Mean goal for category last month\nkk['mean_goal_in_category_last_year'] = kick.apply(lambda x: getRangeMean(x['main_category'],x['launced'],52),axis = 1) ","9b4f2b01":"pbar = pyprind.ProgBar(331675)\n# Median goal in category last month\nkk['median_goal_in_category_last_year'] = kick.apply(lambda x: getRangeMedian(x['main_category'],x['launced'],52),axis = 1) ","df7a3920":"kk['median_goal_Last_6_Month'] = kick.apply(lambda x: getRangeMedian(x['main_category'],x['launced'],26),axis = 1)","1501d7f2":"kk['mean_goal_Last_6_Month'] = kick.apply(lambda x: getRangeMean(x['main_category'],x['launced'],26),axis = 1)","29d244de":"kk['mean_goal_Last_Week'] = kick.apply(lambda x: getRangeMean(x['main_category'],x['launced'],1),axis = 1)","f8927f04":"kk['median_goal_Last_Week'] = kick.apply(lambda x: getRangeMedian(x['main_category'],x['launced'],1),axis = 1)","10f30a63":"kk = kk.fillna(0)   # fill created NAs with 0s","9e2397dc":"# include state of project\nkk['state'] = kick.state","5756cc42":"dims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.violinplot(x=\"main_category\", y=\"Duration\", hue= 'state', data=kk, split=True)","d5ef62c7":"dims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.countplot(x='Category', palette=\"pastel\",data= kk[kk['state'] == 'successful'].groupby(\"Category\")\n              .filter(lambda x: len(x) > 3000), ax=ax)","0567d354":"dims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.countplot(x='Category', palette=\"pastel\",data= kk[kk['state'] == 'failed']\n              .groupby(\"Category\").filter(lambda x: len(x) > 4500), ax=ax)","910fc9b6":"frame = pd.DataFrame()\ncounts = kk.Category.value_counts()\nsucc = kk[kk.state == 'successful'].Category.value_counts()\n\n\nrows_list = []\nfor i in counts.keys():\n    dict1 = {}\n    # get input row in dictionary format\n    # key = col_name\n    dict1['Category'] = i\n    dict1['Percent'] = succ[i] \/ counts[i] * 100\n    rows_list.append(dict1)\n\nframe = pd.DataFrame(rows_list)   \nso = frame.sort_values('Percent').tail(10)\nhigh = frame.sort_values('Percent').head(10)","2e87d1c1":"dims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.barplot(x='Category',y = 'Percent', palette=\"pastel\",data = so, ax=ax)","5265b8cc":"dims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.barplot(x='Category',y = 'Percent', palette=\"pastel\",data = high, ax=ax)","9d778694":"dims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.distplot(kk.name_vowel_ratio[kk.state == 'successful'], kde=True,color='g', rug=False, ax = ax);\nsns.distplot(kk.name_vowel_ratio[kk.state == 'failed'], kde=True,color='r', rug=False, ax = ax);","e2fe6c52":"corr = kk.corr()\ndims = (16, 10)\nfig, ax = plt.subplots(figsize = dims)\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values,ax = ax)","ee4bf2dd":"# seperate dependent and independent part into seperate variables\ny = (kk.state == 'successful').astype(int)\nx = kk.drop(['state'], axis = 1)","4444a146":"# transform target variable into categorical value\nclass_le = LabelEncoder()\ny = class_le.fit_transform(y.values)","0a5da5a6":"# create dummy variable for main category and category variables\nx_pca = pd.get_dummies(x, columns = ['Category','main_category'], drop_first=True)","05e35df9":"# explained variance ratio with cumulative sum\npca = PCA(n_components=10, random_state=1)\nX_pca = pca.fit_transform(x_pca.values)","5df66afe":"# plot cumulative variance\nplt.figure(figsize=(10, 6))\nplt.bar(range(1, 11), pca.explained_variance_ratio_, alpha=0.5, align='center')\nplt.title('Explained variance ratio by Principal components-PCA', fontsize=16)\nplt.step(range(1, 11), np.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\n\nplt.axvline(2, linestyle=':', label='n_components chosen', c='red')\nplt.legend(prop=dict(size=12))\n\nplt.show()","8fea4c8a":"plt.figure(figsize=(10, 6))\nplt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], label='0 - successful')\nplt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], label='1 - failed')\n\nplt.title('Variation of the classes based on first two components', fontsize=16)\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.legend(loc='best', prop=dict(size=12))\n\nplt.show()","4291dd4e":"# perform One-hot-encoding\nx = pd.get_dummies(x, columns = ['main_category','Category'], drop_first=True)","51657e26":"# initialize balanced class indeces\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n\n# initialize values into different variables\nfor train_index, test_index in sss.split(x, y):\n    X_train, X_test = x.iloc[train_index, :].reset_index(drop=True), x.iloc[test_index, :].reset_index(drop=True)\n    y_train, y_test = y[train_index], y[test_index]","baa8fc1f":"print('Train dataset class distribution:')\nprint(np.bincount(y_train))\n\nprint('\\nTest dataset class distribution:')\nprint(np.bincount(y_test))","7bd16c9c":"# initialize list of the transformations\npipe_rf = Pipeline(steps=[\n    ('std', StandardScaler()),\n    ('rf', RandomForestClassifier(n_estimators=100, max_depth=4, min_samples_leaf=3))\n])\n\n# list of the parameters to test\nparam_grid = [\n    {\n        'rf__max_depth': [3, 4, 5],\n        'rf__min_samples_leaf': [4, 5]\n    }\n]\n\n# initialize grid search\nestimator = GridSearchCV(\n    pipe_rf,\n    cv=StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=1), # preserve class balance\n    param_grid=param_grid,\n    scoring='roc_auc'\n)\n\n# fit train data\nestimator.fit(X_train, y_train)","c275c976":"print('Best Grid Search result :', estimator.best_score_)\nprint('Best parameter :', estimator.best_params_)","7605548a":"# get best classifier\nclf_rf = estimator.best_estimator_\n\n# predict test data set\ny_pred_rf = estimator.predict(X_test)","4e614a53":"# test data set auc error\nprint('Train data ROC\/AUC :', )\nprint('Test data ROC\/AUC :', roc_auc_score(y_true=y_test, y_score=y_pred_rf))\n\n# confusion matrix\nprint('\\nConfusion matrix')\nprint(confusion_matrix(y_true=y_test, y_pred=y_pred_rf))\n\n# classification matrix\nprint('\\nClassification matrix')\nprint(classification_report(y_true=y_test, y_pred=y_pred_rf))","fb4b21eb":"# initialize list of the transformations\npipe_lr = Pipeline(steps=[\n    ('std', StandardScaler()),\n    ('lr', LogisticRegression(penalty='l1', C=0.001, random_state=0))\n])\n\nparam_grid = [\n    {\n        'lr__C': [0.001, 0.01, 0.1, 1],\n        'lr__penalty': ['l1', 'l2']  # regularizatian parameter\n    }\n]\n\nestimator = GridSearchCV(\n    pipe_lr,\n    cv=StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=1), # preserve class balance\n    param_grid=param_grid,\n    scoring='roc_auc',\n)\n\n# fit training data\nestimator.fit(X_train, y_train)","066444a2":"print('Best Grid Search result :', estimator.best_score_)\nprint('Best parameter :', estimator.best_params_)","48ecd5f8":"# get best estimator\nclf_lr = estimator.best_estimator_\n\n# predict test data set\ny_pred_lr = estimator.predict(X_test)","886bc1a4":"# test data set auc error\nprint('Train data ROC\/AUC :', )\nprint('Test data ROC\/AUC :', roc_auc_score(y_true=y_test, y_score=y_pred_lr))\n\n# confusion matrix\nprint('\\nConfusion matrix')\nprint(confusion_matrix(y_true=y_test, y_pred=y_pred_lr))\n\n# classification matrix\nprint('\\nClassification matrix')\nprint(classification_report(y_true=y_test, y_pred=y_pred_lr))","9d768d1f":"# initialize list of the transformations\n\n# We will use parameters that were found based on experiments made on local machine\n\nclf_gbm = lgb.LGBMClassifier(\n        n_estimators=1000,\n        num_leaves=25,\n        colsample_bytree=.8,\n        subsample=.9,\n        max_depth=9,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01\n)\n\n\n# fit training data\nclf_gbm.fit(X_train,\n              y_train,\n              eval_metric='auc', \n              verbose=0)","50bf36fd":"# predict test data set\ny_pred_gbm = clf_gbm.predict(X_test)","48cf2bc0":"# test data set auc error\nprint('Test data ROC\/AUC :', roc_auc_score(y_true=y_test, y_score=y_pred_gbm))\n\n# confusion matrix\nprint('\\nConfusion matrix')\nprint(confusion_matrix(y_true=y_test, y_pred=y_pred_gbm))\n\n# classification matrix\nprint('\\nClassification matrix')\nprint(classification_report(y_true=y_test, y_pred=y_pred_gbm))","eca1427d":"# feature importance for Light GBM\npredictor_columns = X_train.columns\nfeat_import = list(zip(predictor_columns, list(clf_gbm.feature_importances_)))\nns_df = pd.DataFrame(data = feat_import, columns=['Feat_names', 'Importance'])\nns_df_sorted = ns_df.sort_values(['Importance', 'Feat_names'], ascending = [False, True])\n\nns_df_sorted","ce90f2ea":"Distribution of the `goal` variable. We can observe goal is highly skeweed to the right. **`log`** transformation can solve the outlier problem.","bd5d05e5":"#### Mean and median values of goals in date ranges","a6cc7991":"### Principal Component Analysis - PCA\n\nOn the data set we have two categorical variables: `main category` and `category` with 15 and 159 distinct values respectively. `One-Hot-Encoding` of these categorical variables will create total `174` indicator variables. Now, we will explore Principal Component Analysis to reduce the dimensionality.\n\nCreate **dummy** variables for categorical features.","8871c589":"`StratifiedShuffleSplit` provides train\/test indices to split data in train\/test sets, by respecting class balance.","611a21e2":"#### Launched and deadline\nConvert `launced`, `deadline` variables into `date` type.","d3067062":"From the dataset we can observe that `Film & Video` and `Music` together contain **30%** of all the observation.","b6b4cd31":"Observe that failed projects have more observations in higher end","1108d7f1":"Counts of data types in the dataframe. We have `int`, `float` and `text` features. Date (time) features (launched and deadline) are represented as text data type.","fe7075ad":"`78.8%` of the projects come from `USA`. So, we can group countries into two groupes, USA and Others.","9467ffda":"Now, in order to take into account other currencies we will take modulo 10.","1dc0ca66":"Perform `PCA` on the original data.","e965481f":"<h6> Least successful categories based on counts<\/h6>","eaaaf5ff":"# Visualization","6607ddaa":"\n    Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowdfunding platform focused on creativity and merchandising.\n\nMission of the company is **help bring creative projects to life**.\n\n*More:* [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Kickstarter)\n","35db7a0d":"## Explorative Data Analysis (EDA)\n\n   Exploratory Data Analysis is the first step in any data analysis process. It tries to make sense of the data to figure out what questions can be asked from the data and also how to manipulate the data to get the answers to those questions. It is usually done by looking at **patterns**, **outliers**, and **anomalies** in order to reveal structural secrets, test assumptions and getting insights into the data. \n\n### General statistics\nLet's look at some basic statistics about `kickstarter` project. Dataframe consists of `378,661` observations and `15` features.","65a8fad3":"Surprisingly, Chiptune is most successful category with more than 80 percent success rate","c8b38807":"## Load Modules and Data\n\n### Import packages","c8df6f9a":"#### Logistic regression\n\n`Logistics Regression` (or logit model) is another technique from Machine Learning for classfication. Logistic regression is named for the function used at the core of the method, the logistic function.\n\nThe logistic function, also called the `sigmoid function` was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It\u2019s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\n> 1 \/ (1 + e^-value)\n\nWhere e is the base of the natural logarithms and value is the actual numerical value that you want to transform. (source: *machinelearningmastery.com*)","48a4f40d":"Based on the above table we can observe non-uniform distribution of the states. There are `6` states, where `failed` and `successful` in total contain `88%` of all projects. For the further analysis we will keep only these two classes. We end up with `binary classification` problem.","ffa0876f":"### goal\n\nFrom EDA we know that the most popular `goal` for the projects is dividable to 1000. In order to take into account this information we will create `3` indicator variables.\n\n* modulo 1000\n* modulo 500\n* modulo 10","8d8c3aea":"Three type of classification algorithms have been used for this analysis: `Random Forest`, `Logistic Regression` and `Light GBM`. Before performing any transformation and modelling, data has been scaled to **zero mean** and **unit variance**.\n\nWe tried to understand which dataset and classification model perform better on **Kickstarter** project.\n\nApart from `ROC AUC` metric, `confusion matrix`, `precision and recall` measures are used to assess the classification performance.\n\nNow we will perform three different classification models:\n* Random Forest\n* Logistic Regression\n* Light GBM\n\nFor example, here are the steps, for tuning parameters on K Neirest Neighbors:","d4e92687":"## Partition dataset into train \/ test sets\n\nIn order to test the performance of the model and evaluate the model, original dataset was spitted into train and test sets.  `Stratified Shuffle Split` is used, which shuffles the data each time before splitting (randomized folds). The same **class balance** for test and train datasets is preserved.\n\nData set contains `60%` failed and `40%` successful projects.\n\nWe will keep `20%` of original data as test set.","cbfbe327":"### Read in dataset","4dff2f7b":"**Plot of variability of the classes**\n\nFollowing figure shows successfulness of the project based on the first two components. On the x-axes first principal component, on the y-axes second principal component were used. We can observe from the figure that there is highly variance for each class.","ce432703":"Based on the descriptive statistics we can observe that `goal` is highly skeweed i.e. difference between `75%` percentile and `max` value is very high.\n\nFrom the frequency of the `goal`, we can observe that the most popular amount of money for projects are 5K or 10K, otherwise projects which are divisible by **1000, 500**.","b2f694d1":"**Feature Importance**\n\nFeature importances allows to see which of the features contribute to result of the model more ","373c503d":"### name\n\nBased on the name of the the project the following features will be generated such as:\n\n* length of the name\n* if the name contains question mark at the end\n* if the name contains exclamation mark at the end\n* if the name is upper case\n* number of non characters in name\n* number of words in the name\n* ratio between vowels and alphanumeric length of name","4b2ec2b4":"#### Main category\nLet's look at the frequency of the `main category`.","49a4135b":"Keep only `failed` and `successful` states from dataframe. In this case, class distribution will be more or less balanced and will make further analysis easy.","41361ba0":"### Launch date","51e2e233":"As a basic general statistics,  number of missing values for each features is calculated, and sorted on the descending order. `usd pledged` contains `3797` missing values. Whereas `name` has only `4` missing values.\nRows with missing values will be removed.","186a7e6b":"## Modelling and Hyperparameter Optimization","5007268f":"The purpose of this work is to analyze data and build a **classifier** to predict *successfulness* of the Kickstarter campaign based on historical observations and trends. \n\nIn this kernel, you will discover a suite of techniques for data transformation (*pandas*), visualization (*matplotlib*, *seaborn*) and classification (*sklearn*, *Light GBM*), which could be used as a guidelines to give enough information on each method to get started with a working code example.\n\nDataset consists of 300K projects with following features:\n* `ID` internal kickstarter id\n* `name` name of project - A project is a finite work with a clear goal that you\u2019d like to bring to life. Think albums, books, or films.\n* `category` category\n* `main_category` category of campaign\n* `currency` currency used to support\n* `deadline` deadline for crowdfunding\n* `goal` fundraising goal - The funding goal is the amount of money that a creator needs to complete their project.\n* `launched` date launched\n* `pledged` amount pledged by \"crowd\"\n* `state` Current condition the project is in\n* `backers` number of backers\n* `country` country pledged from\n* `usd pledged` amount of money pledged\n* `Goal` amount in USD\n","916189eb":"### Table of Contents\n\n<ul>\n<li>Load Data and Modules<\/li>\n<li>Explorative Data Analysis<\/li>\n<li>Feature extraction<\/li>\n<li>Data visualization<\/li>\n<li>Partition dataset into train \/ test sets<\/li>\n<li>Modelling and Hyperparameter Optimization<\/li>\n<li>Conclusion<\/li>\n<\/ul>","16d63c2e":"\n<img src=\"https:\/\/nomadicdivision.org\/wp-content\/uploads\/2015\/01\/kickstarter-logo-light-e1426121148157.jpg\" alt=\" Kickstater Logo\" style=\"width: 95%\"\/>","c12225b6":"## Univariate feature analysis\n\nUnivariate analysis is one of the simplest forms of statistical analysis. As it is suggested from the name it looks only at one variable and is often used to describe population, the things that can be looked at include things like frequencies and distrubution of the data\n","a1a2312b":"<h6> Least successful categories based on percent <\/h6>","ea1f7625":"The most popular words on the **successful** project name.","500d9c16":"Pipeline from `sklearn` package is used to specify each step of the execution. Pipeline allows us to specify preprocessing and modelling functions, which will execute consequtively.\n\n```python\npipe_knn = Pipeline(steps=[\n    ('scl', StandardScaler()), # scale the matrix\n    ('pca', PCA(n_components=2, random_state=1)), # reduce the dimensionality\n    ('knn', KNeighborsClassifier(n_neighbors=4, p=2, metric='minkowski', n_jobs=-1)) # perform KNN\n])\n```\n\n\nAfterwards we  create a dictionary of possible values to explore for each step of the Pipeline. On the following example, we want to explore as a principal components ```[3, 5, 7]``` and number of neighbors for KNN.  ``` [3, 4, 5]```\n\n```python\nparam_grid = [\n    {\n        'pca__n_components': [3, 5, 7],\n        'knn__n_neighbors': [3, 4, 5]\n    }\n]\n```\n\n`GridSearchCV` is used in order to explore all the combination of the values by evaluating 3-folds `Cross-validation` each time. **ROC AUC** is used as a metric.\n\n```python\nestimator = GridSearchCV(\n        pipe_knn,\n        cv=StratifiedShuffleSplit(n_splits=3, test_size=0.3, random_state=1), # preserve class balance\n        param_grid=param_grid,\n        scoring='roc_auc',\n        n_jobs=-1\n    )\n    ```","07c714e2":"### Name","9cef4132":"After removing two `state` we end up with balanced binary classification problem, with failed (60%) and successful (40%).","6022b3a3":"<h6> Most successful categories based on counts <\/h6>","c9cf3be2":"The most popular words on the **failed** project name.","f37dd871":">From the figure we can observe that, by using only two components we can seperate classes easily.","a61a11b9":"Compute confusion matrix and classification report to evaluate the accuracy of a classification.","ae670d4b":"### country\n\nAs most of the projects come from `USA` we will create indicator variable.","939c4082":"#### State of the project - binary classification\n`state` shows how successful was your project.","049950a6":"List of the column names, which indicate parameters of the project.","98883a3a":"#### Random Forest\n\nA random forest classifier is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\n\nDecision trees can suffer from high variance which makes their results fragile to the specific training data used.\n\nBuilding multiple models from samples of your training data, called bagging, can reduce this variance, but the trees are highly correlated.\n\nRandom Forest is an extension of bagging that in addition to building trees based on multiple samples of your training data, it also constrains the features that can be used to build the trees, forcing trees to be different. This, in turn, can give a lift in performance. (source: *machinelearningmastery.com*)","ba77f4db":"Compute confusion matrix and classification report to evaluate the accuracy of a classification.","1679e818":"#### Include main_category and category ","fe659b87":"Compute confusion matrix and classification report to evaluate the accuracy of a classification.","1ab08e83":"Number of unique values per feature is calculated and sorted on the descending order.","48106e5a":"`main category` distribution is more or less equal, so we don't need to group them. We have enough observation for each `main category`.","a6bba14d":"Apps, Web and Mobile Games are the least succesful categories","73c5c6de":"Observe that longer campaign have more failure rates prety much for all categories","6247a7b9":"<h6> Most successful categories based on percent <\/h6>","445a4489":"#### Country\nTop 10 the most frequent countries in the dataset.","7e7c0101":"#### Features which will be used for further analysis\nWe will remove features that are leaking our label so that our classifier will be able to predict success of project right after it's launch. We end up with follwing features:\n* name\n* main category\n* category\n* launched date\n* deadline\n* country\n* goal in dollar\n* state\n\n`currency` can be explained by `country` i.e. Euro is used by European countries, Pounds for Great Britain, Dollar in USA, etc.","d60cd6b6":"## Feature generation","b095c678":"### Wordcloud\n\nWord clouds are the simplest way of communicating word frequencies in easier mannner. The more a word appears in a source, the bigger and bolder it appears in the word cloud. It allows easy allows seeing frequent words at first glance.","67c83ffe":"#### Light GBM\n\nLight GBM is a gradient boosting framework that uses tree based learning algorithm.\n\nMore: [Light GBM](https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc)","e24b6924":" Based on lauch date and goals of the project we will create additional features that will contain information about number of launched projects, mean and median of goals for:\n \n<ul> \n\n<li>Last Week<\/li>\n<li>Last Month<\/li>\n<li>Last 3 Month<\/li>\n<li>Last 6 Month<\/li>\n<li>Last Year<\/li>\n\n<\/ul>","19b1748c":"#### Goal\n\nFirst of all, we will use `use_goal_real` which is a dollar equivalent of the goal. As the dataset contains different currencies for project. In order to make the same currency, `use_goal_real` will be used.\n\nFor a continuous variable we can generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding missing values.\nFor numeric data, the result's index will include `count`, `mean`, `std`, `min`, `max` as well as lower, `50` and upper percentiles. By default the lower percentile is `25` and the upper percentile is ``75``. The ``50`` percentile is the same as the median.","c34302ba":"## Conclusion","47ce9cc4":"\nOn the above figure:\n* *Histogram*: explained variance of each component\n* *Blue line*: cumulative explained variance ratio\n* *Red dotted* line: optimal number of components\n\nFrom the above plot we can observe that, first component explains almost `100%` of the variance on the original data set."}}