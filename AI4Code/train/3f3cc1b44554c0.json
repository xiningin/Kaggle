{"cell_type":{"cb9a430a":"code","d5664463":"code","038d9dbc":"code","cfb8b55e":"code","33cfb8ff":"code","ca3f4164":"code","277896bc":"code","16f614c3":"code","ca2e3e21":"code","568bf1a2":"code","ce2a78bc":"code","933be3db":"code","43b8c40a":"code","0cbf7c80":"code","d033333f":"code","0bb26630":"code","7dd69a72":"code","21332a9b":"code","09be5976":"code","882b10b0":"code","9998d9ef":"code","615a8464":"code","89ef9986":"code","0a6877bb":"code","ab517263":"code","4e0b0954":"code","5a4f8373":"code","9223eba0":"code","dcd4505d":"code","116c7ba5":"code","f4fd0216":"code","6bf0f6c4":"code","dbf67078":"code","9bd6d652":"code","38ebc378":"code","b823ed1c":"code","f317c1aa":"code","49e88ff7":"code","23722b5d":"code","fb39fa8e":"code","ca0ba7ee":"code","d6ebd579":"code","55567a43":"code","41669953":"code","e50ce253":"code","f706467e":"code","afe4c09e":"code","f90d4d3a":"code","0536e80b":"code","9808ecb3":"code","f7f4e0bc":"code","63223b02":"code","f2844dfd":"code","78434fa5":"code","dc944dd1":"code","7b137746":"code","dfbd6581":"code","cb0b2580":"code","44074e85":"code","305d8c24":"code","608e45b5":"code","d7e8f288":"code","80ef88fa":"code","7fcfc456":"code","46d4dbce":"code","5cfa5ee8":"markdown","07551685":"markdown","9148ffc6":"markdown","8d324d19":"markdown","3ade293e":"markdown","e8bd55d3":"markdown","ebd50a92":"markdown","85d841a4":"markdown","416b1455":"markdown","cd01112d":"markdown","6e2788fd":"markdown","ef1b531f":"markdown","1806c90b":"markdown","be6caa11":"markdown","e39cddc0":"markdown","448a3bca":"markdown","ba34b3aa":"markdown","65c76cc5":"markdown","1409c05b":"markdown"},"source":{"cb9a430a":"## data processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom zipfile import ZipFile # read zip files directly\nimport gc # garbage collection\nimport pickle # save python objects\nimport random \n\n# parallel processing\nimport multiprocessing\nfrom joblib import delayed, Parallel\n\n# cool progress bar\nimport tqdm\nimport time\n\n# modeling and evaluation\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import TSNE\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d5664463":"### tensorflow related packages and functions\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import Input, Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import make_sampling_table, skipgrams\nfrom tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.callbacks import History\nfrom tensorflow.keras import optimizers","038d9dbc":"#===============================================\n# global parameters\n#===============================================\n\n# show entire value of cell in pandas\npd.set_option('display.max_colwidth', 1000)\npd.set_option('display.max_columns', 500)\n\n# number of cpus\ncpus = multiprocessing.cpu_count()\nf\"Number of CPUs: {cpus}\"","cfb8b55e":"# test for gpu\nprint(tf.test.is_gpu_available(\n    cuda_only=False, min_cuda_compute_capability=None\n))\nprint(tf.test.gpu_device_name())","33cfb8ff":"# Input data files are available in the read-only \"..\/input\/\" directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca3f4164":"# set data directory\ndata_dir =  \"\/kaggle\/input\/instacart-market-basket-analysis\/\"","277896bc":"# product file\nwith ZipFile(data_dir + \"products.csv.zip\") as z:\n    with z.open(\"products.csv\") as f:\n        products = pd.read_csv(f)\nprint(products.shape)\nproducts.head()","16f614c3":"# department file\nwith ZipFile(data_dir + \"departments.csv.zip\") as z:\n    with z.open(\"departments.csv\") as f:\n        dept = pd.read_csv(f)\nprint(dept.shape)\ndept","ca2e3e21":"# aisle file\nwith ZipFile(data_dir + \"aisles.csv.zip\") as z:\n    with z.open(\"aisles.csv\") as f:\n        aisle = pd.read_csv(f)\nprint(aisle.shape)\naisle.head()","568bf1a2":"# merge all files to a single product file\nproducts = pd.merge(products, aisle, on = \"aisle_id\", how = \"left\")\nproducts = pd.merge(products, dept, on = \"department_id\", how = \"left\")\nprint(products.shape)","ce2a78bc":"# departments with the most products\nproducts[\"department\"].value_counts()","933be3db":"# aisles with the most products\nproducts[\"aisle\"].value_counts()","43b8c40a":"# orders file\nwith ZipFile(data_dir + \"orders.csv.zip\") as z:\n    with z.open(\"orders.csv\") as f:\n        orders = pd.read_csv(f)\nprint(orders.shape)\norders","0cbf7c80":"# remove test data\norders = orders.loc[orders[\"eval_set\"] != \"test\", :]\nprint(orders.shape)","d033333f":"# aggregate to user level\nusers = orders.groupby(\"user_id\").agg({\"order_number\": \"max\"})\nusers = users.reset_index()\nprint(users.shape)\nusers.head()","0bb26630":"# =========================================================\n# split train-val-test\n# =========================================================\n\n# training set: 60%\n# validation set: 20%\n# test set: 20%\nusers[\"eval\"] = np.random.choice([\"train\", \"test\", \"val\"], size = users.shape[0], p = [0.6, 0.2, 0.2])\nusers[\"eval\"].value_counts()","7dd69a72":"# =========================================================\n# merge with orders data\n# =========================================================\norders = pd.merge(orders, users, on = [\"user_id\", \"order_number\"], how = \"left\")\nprint(orders.shape)","21332a9b":"# set missing eval to prior and delete eval_set column\norders.loc[pd.isnull(orders[\"eval\"]), \"eval\"] = \"prior\"\norders.drop(labels = \"eval_set\", axis = 1, inplace = True)\nprint(orders[\"eval\"].value_counts())\norders.head()","09be5976":"# orders file\nwith ZipFile(data_dir + \"order_products__prior.csv.zip\") as z:\n    with z.open(\"order_products__prior.csv\") as f:\n        prior = pd.read_csv(f)\nprint(prior.shape)\nprior","882b10b0":"# product frequency to check top-products\nprod_freq = prior[\"product_id\"].value_counts()\nprint(prod_freq.shape)\nprod_freq","9998d9ef":"# keep products bought at least 200 times\nmin_freq = 200\nprod_freq = prod_freq[prod_freq >= min_freq]\nprint(prod_freq.shape)","615a8464":"# subset prior data\nprior = prior.loc[prior[\"product_id\"].isin(list(prod_freq.index)), :]\nprint(prior.shape)","89ef9986":"# function to aggregate\nf = {\"product_id\": lambda g: \" \".join(g),\n    \"add_to_cart_order\": [\"count\"]}\n\n# format product-ids to string\nprior[\"product_id\"] = prior[\"product_id\"].astype(str)\n\n# roll-up\nprior_orders = prior.groupby([\"order_id\"]).agg(f)\nprint(prior_orders.shape)\nprior_orders.head()","0a6877bb":"# =================================================\n# reset column levels and rename\n# =================================================\n\nprior_orders.columns = prior_orders.columns.droplevel(1)\nprior_orders.rename(columns = {\"add_to_cart_order\" : \"num_products\"}, inplace = True)\nprior_orders = prior_orders.loc[prior_orders[\"num_products\"] > 1, :]\nprior_orders.reset_index(inplace = True)\nprint(prior_orders.shape)\nprior_orders.head()","ab517263":"# Note:\n## - We have made our own training and test data\n## - This dataset will eventually be appended to the larger prior orders","4e0b0954":"# train orders file\nwith ZipFile(data_dir + \"order_products__train.csv.zip\") as z:\n    with z.open(\"order_products__train.csv\") as f:\n        train = pd.read_csv(f)\nprint(train.shape)\ntrain.head()","5a4f8373":"# retain only frequently sold products\ntrain = train.loc[train[\"product_id\"].isin(list(prod_freq.index)), :]\nprint(train.shape)","9223eba0":"# function to aggregate\nf = {\"product_id\": lambda g: \" \".join(g),\n    \"add_to_cart_order\": [\"count\"]}\ntrain[\"product_id\"] = train[\"product_id\"].astype(str)\ntrain_orders = train.groupby([\"order_id\"]).agg(f)\ntrain_orders.columns = train_orders.columns.droplevel(1)\ntrain_orders.reset_index(inplace = True)\ntrain_orders.rename(columns = {\"add_to_cart_order\" : \"num_products\"}, inplace = True)\ntrain_orders = train_orders.loc[train_orders[\"num_products\"] > 1, :] # retain baskets with more than one product\nprint(train_orders.shape)\ntrain_orders.head()","dcd4505d":"# =========================================================\n# Combine training and prior orders\n# =========================================================\n\n\n# add identifier column to both data sets to append\n# orders-level data\nprior_orders[\"eval\"] = \"prior\"\ntrain_orders[\"eval\"] = \"train\"\n\n# order-product level data\nprior[\"eval\"] = \"prior\"\ntrain[\"eval\"] = \"train\"\n\n\n# append wide orders\nall_orders_wide = prior_orders.append(train_orders)\nall_orders_wide.reset_index(drop = True, inplace = True)\nprint(all_orders_wide.shape)\n\n# append long orders\nall_orders_long = prior.append(train)\nall_orders_long.reset_index(drop = True, inplace = True)\nprint(all_orders_long.shape)","116c7ba5":"all_orders_wide.head()","f4fd0216":"# =========================================================\n# merge orders-wide and orders to get train-test split\n# =========================================================\n\nall_orders_wide.drop(labels = \"eval\", axis = 1, inplace = True)\n\n# merge\norders_wide = pd.merge(all_orders_wide, orders[[\"order_id\", \"user_id\", \"eval\"]],\n                       on = \"order_id\", how = \"left\")\nprint(orders_wide.shape)","6bf0f6c4":"#===============================================\n# split train-val-test\n#===============================================\n\ntrain = orders_wide.loc[orders_wide[\"eval\"].isin([\"prior\", \"train\"]), :]\nval = orders_wide.loc[orders_wide[\"eval\"] == \"val\", :]\ntest = orders_wide.loc[orders_wide[\"eval\"] == \"test\", :]\nprint(\"train size:\", train.shape)\nprint(\"val size:\", val.shape)\nprint(\"test size:\", test.shape)","dbf67078":"# clear up memory\ndel all_orders_wide\ndel orders_wide\ngc.collect()\ngc.collect()","9bd6d652":"#===============================================\n# randomly sample training data\n#===============================================\n\nsample_size = 1000000\ntrain = train.sample(n = sample_size)\ntrain = train.reset_index(drop = True)\nprint(train.shape)\ntrain.head()","38ebc378":"# use keras tokenizer to split baskets into individual products\nvocab_size = 15000\ntokenizer = Tokenizer(num_words = vocab_size, lower = False)\n\n# fit the tokenizer orders\ntokenizer.fit_on_texts(list(train[\"product_id\"].values))\nprint(tokenizer.document_count)\nprint(len(tokenizer.word_counts))","b823ed1c":"# map orders from product-ids to contiguous integers\ntrain_orders = tokenizer.texts_to_sequences(list(train[\"product_id\"].values))\nval_orders = tokenizer.texts_to_sequences(list(val[\"product_id\"].values))\ntest_orders = tokenizer.texts_to_sequences(list(test[\"product_id\"].values))\nprint(len(train_orders))\nprint(len(val_orders))\nprint(len(test_orders))","f317c1aa":"# manually inspect some data\nprint(train_orders[:3])\ntrain[\"product_id\"][0:3]\n","49e88ff7":"# save pickle files\nwith open('\/kaggle\/working\/train_orders.pkl', 'wb') as f:\n    pickle.dump(train_orders, f)\nf.close() \n    \n\nwith open('\/kaggle\/working\/val_orders.pkl', 'wb') as f:\n    pickle.dump(val_orders, f)\nf.close()     \n\n    \nwith open('\/kaggle\/working\/test_orders.pkl', 'wb') as f:\n    pickle.dump(test_orders, f)  \nf.close()     ","23722b5d":"# define vocab size - this is total number of unique products \nvocab_size = len(tokenizer.word_index) + 1 # we add one to account for products in test\/val that were not in train. They are treated as \"UNK\". ","fb39fa8e":"# build the sampling table for negative sampling\nsampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size, sampling_factor = 0.001)","ca0ba7ee":"class DataGenerator(Sequence):\n    'Generates data for Keras'\n    def __init__(self, order_data, batch_size = 64, cs = 15, ns = 20, shuffle = True):\n        'Initialization'\n        self.batch_size = batch_size\n        self.order_data = order_data\n        self.shuffle = shuffle\n        self.cs = cs\n        self.ns = ns\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.order_data) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.order_data[k] for k in indexes]\n\n        # Generate data\n        X, dv = self.__data_generation(list_IDs_temp)\n\n        return X, dv\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.order_data))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        iv = []\n        dv =[]\n\n        # Generate data\n        for i, d in enumerate(list_IDs_temp):\n            # Store sample\n            couples, labels = skipgrams(d, vocabulary_size = vocab_size, window_size = self.cs,\n                                        negative_samples = self.ns, sampling_table = sampling_table)\n            iv = iv + couples\n            dv = dv + labels\n            \n        X = np.array(iv, dtype = \"int32\")\n        X = [X[:, 0], X[:, 1]]\n        return X, np.array(dv)","d6ebd579":"# randomly sample orders for quick training\ntrain_orders = random.sample(train_orders, 300000)\nprint(len(train_orders))","55567a43":"# initialize train and validation data generators with default setting\ntrain_gen = DataGenerator(train_orders)\nval_gen = DataGenerator(val_orders)\n\n# test data\ntest_gen = DataGenerator(test_orders, cs = 10, ns = 0, shuffle = False)","41669953":"# model input parameters\nemb_size = 32\nbatch_size = 32","e50ce253":"# define input layers\ninput_target = Input((1, ))\ninput_context = Input((1, ))\n\n# target\ntarget = Embedding(input_dim = vocab_size, output_dim= emb_size, name = \"rho\")(input_target)\ntarget = Reshape(target_shape = (emb_size, 1))(target)\n\n# context\ncontext = Embedding(input_dim = vocab_size, output_dim= emb_size, name = \"alpha\")(input_context)\ncontext = Reshape(target_shape = (emb_size, 1))(context)\n\n# concatenate model inputs and outputs\ninput_model = [input_target, input_context]\noutput_embeddings = Dot(axes = 1)([target, context])\noutput_model = Flatten()(output_embeddings)\n\n# complete model\noutput_model = Dense(1, activation = \"sigmoid\")(output_model)\n\n# define as keras model\nemb_model = Model(inputs = input_model, outputs = output_model)","f706467e":"emb_model.summary()","afe4c09e":"# model optimizer and compile\nadam = optimizers.Adam()\nemb_model.compile(optimizer = adam, loss = \"binary_crossentropy\", metrics = [\"acc\"])","f90d4d3a":"# initial weights - rho\ninit_alpha = emb_model.get_layer(\"alpha\").get_weights()[0]\nprint(init_alpha.shape)","0536e80b":"# fit model\nhistory = History()\nt0 = time.time()\nemb_model.fit(x = train_gen, \n               epochs = 1,\n               validation_data = val_gen,\n               use_multiprocessing = True,\n               callbacks = [history])\nt1 = time.time()","9808ecb3":"# save keras model\n# emb_model.save('\/kaggle\/working\/emb_model\/')\nemb_model = tf.keras.models.load_model('\/kaggle\/input\/product-embeddings\/emb_model\/')","f7f4e0bc":"# initial weights\ninit_alpha","63223b02":"# final weights - target words\nfinal_alpa = emb_model.get_layer(\"alpha\").get_weights()[0]\nprint(final_alpa.shape)\nfinal_alpa","f2844dfd":"#===============================================\n# extract embeddings to data frame\n#===============================================\n\ndef EmbToDataFrame(ix_word, emb_mat, col_prefix = \"rho\"):\n    emb_df = {ix_word[i]: list(emb_mat[i-1]) for i in list(ix_word.keys())[:vocab_size]}\n    emb_df = pd.DataFrame.from_dict(emb_df, orient = \"columns\")\n    emb_df = emb_df.transpose().reset_index(drop = False)\n    emb_df.columns = [\"product_id\"] + [col_prefix + str(i + 1) for i in range(emb_df.shape[1] - 1)]\n    return emb_df","78434fa5":"# reversed mapping of words to index\nix_word = tokenizer.index_word","dc944dd1":"# get data frame from alpha matrix\nalpha_df = EmbToDataFrame(ix_word, emb_mat = emb_model.get_layer(\"alpha\").get_weights()[0], col_prefix = \"alpha\")\ndisplay(alpha_df.head())","7b137746":"#===============================================\n# similarity in alpha matrix\n#===============================================\n\nalpha_sim = cosine_similarity(alpha_df.iloc[:, 1:])\nalpha_sim = pd.DataFrame(alpha_sim)\nalpha_sim.reset_index(inplace = True, drop = True)\nalpha_sim.index = list(alpha_df[\"product_id\"].values)\nalpha_sim.columns = list(alpha_df[\"product_id\"].values)\ndisplay(alpha_sim.head())","dfbd6581":"# compute product similarity given the embeddings\ndef ComputeProductSimilarity(prod_id, alpha_sim, top = 5, include_prod_info = True):\n    sim = alpha_sim.loc[:, prod_id]\n    sim = sim.sort_values(ascending = False)\n    sim = sim[1:][0:top]\n    sim = pd.DataFrame({\"product_id\" : list(sim.index), \"score\": sim}, index = None)\n    if include_prod_info:\n        sim = products.loc[products[\"product_id\"].isin(sim[\"product_id\"]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]]\n    return sim","cb0b2580":"prod_id = \"100\"\nprint(products.loc[products[\"product_id\"].isin([prod_id]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]])\nComputeProductSimilarity(prod_id = prod_id, alpha_sim = alpha_sim, top = 10, include_prod_info = True)","44074e85":"# Find most similar products to the following product\nprod_id = \"49332\"\nprint(products.loc[products[\"product_id\"].isin([prod_id]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]])\nComputeProductSimilarity(prod_id = prod_id, alpha_sim = alpha_sim, top = 10, include_prod_info = True)","305d8c24":"# Find most similar products to the following product\nprod_id = \"3151\"\nprint(products.loc[products[\"product_id\"].isin([prod_id]), [\"product_id\", \"product_name\", \"aisle\", \"department\"]])\nComputeProductSimilarity(prod_id = prod_id, alpha_sim = alpha_sim, top = 10, include_prod_info = True)","608e45b5":"#===============================================\n# prep data for t-sne\n#===============================================\n\nalpha_df[\"product_id\"] = alpha_df[\"product_id\"].astype(str)\nproducts[\"product_id\"] = products[\"product_id\"].astype(str)\n\n# relevant columns from product info\nprod_info_cols = [\"product_id\", \"product_name\", \"department\", \"aisle\"]\n\n# merge product information and product embeddings into a single data frame \nprod_vec_df = pd.merge(products[prod_info_cols], alpha_df, on = \"product_id\", how = \"inner\")\nprint(prod_vec_df.shape)","d7e8f288":"#===============================================\n# fit t-sne\n#===============================================\n\n# define model\ntsne = TSNE(n_components = 2, verbose = 1, perplexity = 35, n_iter = 400)\n\n# columns to fit on\nprod_vec_names = list(prod_vec_df.columns)[4:]\n\n# fit\nt0 = time.time()\ntsne_fit = tsne.fit_transform(prod_vec_df[prod_vec_names])\nt1 = time.time()","80ef88fa":"#===============================================\n# create t-sne data frame for plotting\n#===============================================\n\ntsne_df = prod_vec_df[[\"product_name\", \"department\", \"aisle\"]]\n\n# extract t-sne dimensions\ntsne_df[\"x_tsne\"] = tsne_fit[:,0]\ntsne_df[\"y_tsne\"] = tsne_fit[:,1]\nprint(tsne_df.describe())","7fcfc456":"#===============================================\n# subset data for plot\n#===============================================\n\n# select only top departments\nselect_dept = [\"produce\", \"babies\", \"beverages\"]\ntsne_plot_df = tsne_df.loc[tsne_df[\"department\"].isin(select_dept), :]\nprint(tsne_plot_df.shape)","46d4dbce":"plt.figure(figsize = (12, 12))\ng = sns.scatterplot(x = \"x_tsne\", y = \"y_tsne\",\n              hue=\"department\",\n              data = tsne_plot_df)","5cfa5ee8":"### Aggregate to order-level from order-product level","07551685":"### 4.5. Visualize embeddings using T-SNE","9148ffc6":"### 4.1. Data generator - this is typically the tricky part in setting up a data pipeline for deep learning","8d324d19":"## **1. Import Libraries**","3ade293e":"### 4.3 Inspect model results","e8bd55d3":"## 2.2 **Orders data**","ebd50a92":"----","85d841a4":"# Product Emebddings for Recommendation Systems\n\n-----\n\n## Instacart Grocery Dataset\n\nSource: https:\/\/www.kaggle.com\/c\/instacart-market-basket-analysis\n\n- Instacart is an online grocery delivery service\n- They have made available 3M grocery orders for over 200K users\n- They provide between 4 to 100 orders for each user and each order contains the sequence of products purchased\n- We also have a brief description of the products\n\nGoals:\n- We will use this data to generate product embeddings - dense continuous representations of discrete tokens\n- We will apply methods from Natural Language Processing to analyze product baskets\n----","416b1455":"## 3. Prepare data for modeling","cd01112d":"## 2. Raw data","6e2788fd":"## 2.3 **Prior orders**","ef1b531f":"### 4.4. Extract results for downstream tasks","1806c90b":"-----","be6caa11":"## **2.1 Product info**","e39cddc0":"----","448a3bca":"-------","ba34b3aa":"## 2.4 **Train orders**","65c76cc5":"## **4. Setup Tensorflow**","1409c05b":"### 4.2. Set-up a simple model"}}