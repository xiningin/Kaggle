{"cell_type":{"51745095":"code","796830ed":"code","d1fa14d3":"code","2c7af8d7":"code","41eaadda":"code","9b1257a0":"code","efae6b19":"code","27063f89":"code","b686d258":"code","c7f72059":"code","40be587d":"code","3d3ba02a":"code","8f5e78de":"code","cfe5812e":"code","fc424443":"code","4ce17492":"code","cb2a3ac4":"code","1baae7b6":"code","ac27b0f5":"code","c405dbec":"code","5e057e0a":"code","cf0d58c9":"code","1c8fbedb":"code","3496ec5f":"code","ac4063ca":"code","0a65fed4":"code","a750f9ec":"code","e9b411fc":"code","2ab6d40e":"code","d4560ee9":"code","693304fa":"code","e4a9374c":"code","bb3cfab3":"code","62af3741":"code","0d6faf17":"code","e95244ef":"markdown","4a6e68d2":"markdown","1b4b96ab":"markdown","fc6428ac":"markdown","d39b0534":"markdown","5e0d31d6":"markdown","5fe9e1e2":"markdown","fbd90f69":"markdown","e57b6999":"markdown","45c7134d":"markdown","1f9ed26a":"markdown","853978f7":"markdown","17e9b275":"markdown","c8e6f716":"markdown","df1661dc":"markdown","29a8cf8a":"markdown","755a16c6":"markdown","fafbd447":"markdown","83ca530a":"markdown","04c30486":"markdown","d729bd50":"markdown","68322860":"markdown","d275aa91":"markdown","8d3c927b":"markdown","1a75f436":"markdown","517caec8":"markdown","fe48f77f":"markdown","848b2ac6":"markdown","0f2a8579":"markdown","a57785ca":"markdown","7087bcc1":"markdown","3952c648":"markdown"},"source":{"51745095":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","796830ed":"#uploading libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report,plot_roc_curve, roc_auc_score, roc_curve\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn import metrics\n\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom yellowbrick.datasets import load_nfl\nfrom yellowbrick.classifier import ClassPredictionError\nfrom yellowbrick.cluster import InterclusterDistance\nfrom itertools import cycle\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=ConvergenceWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None  # default='warn'","d1fa14d3":"df = pd.read_csv('\/kaggle\/input\/nutrition-facts\/menu.csv')","2c7af8d7":"df.head()","41eaadda":"df.columns","9b1257a0":"#using the Sugars column we will compute the 'Sugar (% Daily Value)': it is about 30g of Sugar a day\ndf['Sugar (% Daily Value)'] = df['Sugars']\/30*100\n\n#lets round the columns value\npd.set_option('precision', 0)\ndf['Sugar (% Daily Value)'] = df['Sugar (% Daily Value)'].round()","efae6b19":"df.isnull().sum()","27063f89":"columns = ['Saturated Fat (% Daily Value)', 'Sugar (% Daily Value)', 'Cholesterol (% Daily Value)']\n\nfor column in columns:\n    df[column].plot(kind = 'hist',bins = 30,figsize = (14,5))\n    plt.axvline(x=100, color='r', label='axvline - full height',linewidth=6) #max allowed daily value (%)\n    plt.title(\"Distplot of {}\".format(column), fontsize = 14)\n    plt.show()\n    ","b686d258":"columns = ['Saturated Fat (% Daily Value)','Sugar (% Daily Value)', 'Cholesterol (% Daily Value)']\n\nfor column in columns:\n    #plotting Saturated Fat (% Daily Value), Sugar (% Daily Value), Cholesterol (% Daily Value)\n    plt.figure(figsize=(12,4))\n    sns.barplot(x=df.groupby('Category')[column].mean().index,\n                y=df.groupby('Category')[column].mean().values)\n    plt.title(\"Distplot of food categories by {}\".format(column), fontsize = 18)\n    plt.ylabel(\"%\")\n    plt.xlabel(\"menu\",fontsize = 16)\n    plt.xticks(rotation=45, fontsize = 16)\n    plt.axhline(y=100, color='red', linestyle='-', linewidth=4)\n    plt.show()","c7f72059":"df1 = df[['Category', 'Item', 'Saturated Fat (% Daily Value)', 'Sugar (% Daily Value)' ]]","40be587d":"df1.head()","3d3ba02a":"X = df1.iloc[:,[2,3]].values","8f5e78de":"\nwcss=[]\nfor i in range(1,10):\n    kmeans=KMeans(n_clusters=i, init='k-means++',random_state=0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(1,10),wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","cfe5812e":"#Fitting K-MEans to the dataset\nkmeans=KMeans(n_clusters=3,init='k-means++',random_state=0)\ny_kmeans=kmeans.fit_predict(X)\nplt.figure(figsize=(12,10))\n#Visualize the clusters\n\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='black',label='Centroids')\n\nplt.scatter(X[y_kmeans==0,0],X[y_kmeans==0,1],s=100,c='green',label='Cluster1')\nplt.scatter(X[y_kmeans==1,0],X[y_kmeans==1,1],s=100,c='yellow',label='Cluster2')\nplt.scatter(X[y_kmeans==2,0],X[y_kmeans==2,1],s=100,c='blue',label='Cluster3')\n\n\n\nplt.title('Clusters of food items', fontsize = 18)\nplt.xlabel('Saturated Fat (% Daily Value)', fontsize = 16)\nplt.ylabel('Sugar (% Daily Value)',fontsize = 16)\nplt.legend(fontsize = 20)\nplt.axhline(y=100, color='red', linestyle='-', linewidth=6)\n\nplt.show()","fc424443":"#Verify the number of clusters by the Calinski metrics\n# Instantiate the clustering model and visualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(\n    model, k=(2,12), metric='calinski_harabasz', timings=False\n)\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","4ce17492":"#Double check by the Silhouette score\n# Instantiate the clustering model and visualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(\n    model, k=(2,12), metric='silhouette', timings=False\n)\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure\n\n","cb2a3ac4":"# Instantiate the clustering model and visualizer\nmodel = KMeans(3, random_state=42)\nvisualizer = SilhouetteVisualizer(model, colors='yellowbrick')\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()","1baae7b6":"#Instantiate the clustering model and visualizer\nmodel = KMeans(3)\nvisualizer = InterclusterDistance(model)\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show() ","ac27b0f5":"df1.head()","c405dbec":"#assigning food items to corresponding cluster 1\ndf_1 = df1.loc[df1['Sugar (% Daily Value)'] < 101]\ndf_1['Clusters'] = 1\n\n#assigning food items to corresponding cluster 2\nCriteria_1 = df1['Sugar (% Daily Value)'] > 101\nCriteria_2 = df1['Saturated Fat (% Daily Value)'] < 50\nCriteria_all = Criteria_1 & Criteria_2\ndf_2 = df1[Criteria_all]\ndf_2['Clusters'] = 2\n\n#assigning food items to corresponding cluster 3\nCriteria_3 = df1['Sugar (% Daily Value)'] > 101\nCriteria_4 = df1['Saturated Fat (% Daily Value)'] > 50\nCriteria_all = Criteria_3 & Criteria_4\ndf_3 = df1[Criteria_all]\ndf_3['Clusters'] = 3\n\ndf_ready = df_1.append([df_2, df_3])","5e057e0a":"df_ready.head()","cf0d58c9":"df_ready.columns","1c8fbedb":"#extracting the critical features for the classification\ndf_ready_final = df_ready[['Saturated Fat (% Daily Value)', 'Sugar (% Daily Value)', 'Clusters']]","3496ec5f":"df_ready_final.head()","ac4063ca":"plt.figure(figsize=(12,4))\nax = sns.barplot(x='Category',\n                 y='Saturated Fat (% Daily Value)',\n                 hue=\"Clusters\", data=df_ready)\nplt.xticks(rotation=90, fontsize=16)\nplt.title('Category clustered by Saturated Fat (% Daily Value)', fontsize = 14)\nax.legend(loc=10, fontsize = 14)\n\nplt.figure(figsize=(12,4))\nax = sns.barplot(x='Category',\n                 y='Sugar (% Daily Value)',\n                 hue=\"Clusters\", data=df_ready)\nplt.xticks(rotation=90, fontsize=16)\nplt.title('Category clustered by Sugar (% Daily Value)', fontsize = 14)\nax.legend(loc=10, fontsize = 14)","0a65fed4":"#splitting the dataset, so no information leakage occurs\nX_train, X_test, y_train, y_test = train_test_split(df_ready_final.drop(labels=['Clusters'], axis=1),\n    df_ready_final['Clusters'], test_size=0.3, random_state=0)\n\n#using SMOTE to balance the dataset\nos = SMOTE(random_state=0, k_neighbors=2)\nX_train_os,y_train_os=os.fit_sample(X_train,y_train)\nX_test_os,y_test_os=os.fit_sample(X_test,y_test)\nprint(\"The number of y_train classes before fit {}\".format(Counter(y_train)))\nprint(\"The number of y_train classes after fit {}\".format(Counter(y_train_os)))\nprint(\"The number of y_test classes before fit {}\".format(Counter(y_test)))\nprint(\"The number of y_test classes after fit {}\".format(Counter(y_test_os)))","a750f9ec":"fig, axes = plt.subplots(1, 2, figsize=(12, 7))\n\n# before balancing\nsns.countplot(y_train, ax=axes[0])\naxes[0].set_title(\"Before balancing\",  fontsize = 20)\n\n# after balancing\nsns.countplot(y_train_os, ax=axes[1])\naxes[1].set_title('After balancing', fontsize = 20)\n\nplt.show()","e9b411fc":"estimators = {\n    'KNeighborsClassifier' :[KNeighborsClassifier()],\n    'Random Forest' :[RandomForestClassifier()],\n    'Neural Network' :[MLPClassifier()],\n    'Deep Neural Network': [MLPClassifier(hidden_layer_sizes = [100]*5)],\n}\n\n\ndef mfit(estimators, X_train_os, y_train_os):\n    for m in estimators:\n        estimators[m][0].fit(X_train_os, y_train_os)\n        print(m+' fitted')\n\nmfit(estimators, X_train_os, y_train_os)","2ab6d40e":"def mpredict(estimators, X_test_os, y_test_os):\n    outcome = dict()\n    r_a_score = dict()\n    for m in estimators:\n        y_pred = estimators[m][0].predict(X_test_os)\n        #r_a_score[m] = roc_auc_score(y_test, y_pred)\n        outcome[m] = [y_pred, confusion_matrix(y_pred,y_test_os), classification_report(y_pred,y_test_os)]\n    return outcome, r_a_score\n\noutcome, r_a_score = mpredict(estimators, X_test_os, y_test_os)\nfor m in outcome:\n    print('------------------------'+m+'------------------------')\n    print(outcome[m][1])\n    print(outcome[m][2])","d4560ee9":"rf_classifier = RandomForestClassifier()\nrf_classifier.fit(X_train_os, y_train_os)\nrf_predictions = rf_classifier.predict(X_test_os)","693304fa":"confusion_matrix(rf_predictions,y_test_os)","e4a9374c":"clf = RandomForestClassifier()\nvisualizer = ClassificationReport(clf,support=True)\n\nvisualizer.fit(X_train_os, y_train_os)        # Fit the visualizer and the model\nvisualizer.score(X_test_os, y_test_os)        # Evaluate the model on the test data\nvisualizer.show() ","bb3cfab3":"cm = ConfusionMatrix(clf)\n\n# Fit fits the passed model. This is unnecessary if you pass the visualizer a pre-fitted model\ncm.fit(X_train_os, y_train_os)\n\n# To create the ConfusionMatrix, we need some test data. Score runs predict() on the data\n# and then creates the confusion_matrix from scikit-learn.\ncm.score(X_test_os, y_test_os)\n\n# How did we do?\ncm.show()\n","62af3741":"#We define the model as an SVC in OneVsRestClassifier setting.\n#this means that the model will be used for class 1 vs class 2, class 2vs class 3 and\n#class 1 vs class 3. So, we have 3 cases at #the end and within each case, \n#the bias will be varied in order to get the ROC curve of the given case - 3 ROC curves as output.\n\nclassifier = OneVsRestClassifier(RandomForestClassifier())\nX = df_ready_final.drop('Clusters', axis = 1).to_numpy()\nnumber_of_classes = df_ready_final['Clusters'].to_numpy()\ny_bin = label_binarize(number_of_classes, classes=[1, 2, 3])\nn_classes = y_bin.shape[1]\n\n# We split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size= 0.5, random_state=0)\n\n\ny_score = classifier.fit(X_train, y_train).predict_proba(X_test)\n# Plotting and estimation of FPR, TPR\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nplt.figure(figsize=(12,10))\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\ncolors = cycle(['blue', 'red', 'green'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=1.5, label='ROC curve of class {0} (area = {1:0.2f})' ''.\n             format(i+1, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k-', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize = 20)\nplt.ylabel('True Positive Rate', fontsize = 20)\nplt.title('Receiver operating characteristic for multi-class data', fontsize = 20)\nplt.legend(loc=\"lower right\", fontsize = 20)\nplt.show()","0d6faf17":"#Class Prediction Error\nvisualizer = ClassPredictionError(\n    RandomForestClassifier(random_state=42, n_estimators=10)\n)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train_os, y_train_os)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test_os, y_test_os)\n\n# Draw visualization\nvisualizer.show()","e95244ef":"In this section we will compare the following multiclassification models:\n\n1. KNeighborsClassifier\n2. Random Forest\n3. Neural Network\n4. Deep Neural Network","4a6e68d2":"Model_Evaluation\u00b6\nIn this section we will examine the following classification visualizers:\n\n1. Classification Report and Confusion Matrix\n2. ROC\/AUC curve\n3. Class Prediction Error\n\nFor this section I will use the wonderful [yellowbrick library](https:\/\/www.scikit-yb.org\/en\/stable\/index.html): these guys specialize onto the model evaluations of regression, classification, clustering, text cases and many more!","1b4b96ab":"**Interestingly, there is not a column of the 'Sugar (% Daily Value)' in this dataframe. This just has 'Carbohydrates (% Daily Value)' which is misleading in a way because it gives us only the total number of the carbohydrates which include sugars, starch and other compounds related to the carbohydrates. What we want is to calculate 'Sugar (% Daily Value)', so we can estimate 'TRUE' sugarish content of menu.**","fc6428ac":"<a class = 'anchor' id = 'Quick-glance-at-the-Data'><\/a>","d39b0534":"# Exploratory Data Analysis","5e0d31d6":"# Quick glance at Data","5fe9e1e2":"No null values here","fbd90f69":"<a class = 'anchor' id = 'Deployment'><\/a>","e57b6999":"[Cluster Mac food!](https:\/\/mcdonalds20.herokuapp.com\/)","45c7134d":"In this dection we will continue with the data preprocessing by clustering fat and sugar containing items together and assigning the cluster category for each food item","1f9ed26a":" <a><\/a>\n# Table of contents\n\n1. [Quick glance at Data](#Quick-glance-at-the-Data)\n\n2. [Exploratory Data Analysis](#EDA)\n\n3. Data Preprocessing 1: [Data Clustering](#Data_Clustering)\n\n4. Data Preprocessing 2: [Clustering model validation](#Clustering_model_validation)\n\n7. [Model Building](#Model_Building)\n\n8. [Model Evaluation](#Model_Evaluation)\n\n9. [Deployment](#Deployment)","853978f7":"# Data Clustering","17e9b275":"Our major concern are columns that display the 'Saturated Fat (% Daily Value)' and 'Sugar (% Daily Value)'. We will take a look at the Cholesterol as well even though based on the cited [research paper](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0033062015300256), Cholesterol would not be as dangerous as the combination of Sugars and Fats. For simplicity I call Saturated Fat as fat","c8e6f716":"Because two clusters overlap in the 2D space, it does not imply that they overlap in the original [feature space](https:\/\/www.scikit-yb.org\/en\/stable\/api\/cluster\/icdm.html)","df1661dc":"<a class = 'anchor' id = 'Data_Clustering'><\/a>","29a8cf8a":"<a class = 'anchor' id = 'Model_Building'><\/a>","755a16c6":"<a class = 'anchor' id = 'Model_Evaluation'><\/a>","fafbd447":"Now lets examen food categories considering fat, sugar and cholesterol features","83ca530a":"The [KElbowVisualizer](https:\/\/www.scikit-yb.org\/en\/latest\/api\/cluster\/elbow.html) implements the \u201celbow\u201d method to allow data scientists choose the optimal number of clusters by fitting the model with a range of values for K. If the line chart resembles an arm, then the \u201celbow\u201d (the point of inflection on the curve) is a good indication that the underlying model fits best at that point. In the visualizer \u201celbow\u201d will be annotated with a dashed line.","04c30486":"# Clustering model validation","d729bd50":"Coffee & Tea, Smoothies & Shakes exceed the daily uptake of sugars posing a relative health concern. \n\n**Breakfast, Chicken & Fish, Beef & Pork items appear to be healthy though!**","68322860":"It looks like we are within the fat limit, however how is about the sugar?!\nHere we obtained three clusters , which clearly show us that food related to the cluster 1 and 3 are among the least healthy. The cluster 2 however does not exceed the daily limit neither by sugars nor by fats, thus food is safe to eat.","d275aa91":"<a class = 'anchor' id = 'EDA'><\/a>","8d3c927b":"We will use the three most commonly used models to verify the number of cluster we have chosen.\n\n1. [The Calinski-Harabasz Metric](https:\/\/www.mathworks.com\/help\/stats\/clustering.evaluation.calinskiharabaszevaluation-class.html). \n\nThis evaluation is an object consisting of sample data, clustering data, and Calinski-Harabasz criterion values used to evaluate the optimal number of clusters. \n\n2. [The Silhouette Coefficient](https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html)\n\nThis evaluation is used by computing the density of clusters. The score is calculated by averaging the silhouette coefficient for each sample, obtained as the difference between the average intra-cluster distance and the mean nearest-cluster distance for each sample, \n\n3. [Intercluster Distance Maps](https:\/\/www.scikit-yb.org\/en\/stable\/api\/cluster\/icdm.html)\n\nThis evaluation display the cluster centers in 2 dimensions with the distance to other centers preserved. E.g. the closer to centers are in the visualization, the closer they are in the original feature space.","1a75f436":"So, now we will cluster our data into 3 clusters and we will assing the separate column for the clusters","517caec8":"# You can eat healthy at McDonald's.\n\n**Introduction**\n\nCoronary Heart Disease and Diabetes are the major beasts of modern society occured due to an unbalanced diet filled up with the sugarish and fatty food. **Thus, we will explore not just the individual effects of Sugar and Fat, but the combination of those nutrients that poses the greater danger.**\n\n[The Evidence for Saturated Fat and for Sugar Related to Coronary Heart Disease](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0033062015300256)\n\n\nDietary guidelines suggest reducing the intake of saturated fats. Those can raise levels of cholesterol , therefore increasing the risk of atherosclerotic coronary heart disease (CHD). **However, cholesterol is only modestly associated with CHD. Regarding saturated fats, these fats may have different effects on CHD risk based on the specific saturated fatty acids (SFAs) they contain**. When saturated fats are replaced with refined carbohydrates, and specifically with added sugars (like sucrose or high fructose corn syrup), the end result is not favorable for heart health. Such replacement leads to increase the risk of CHD. A diet high in added sugars has been found to cause a **3-fold increased risk** of  cardiovascular disease. \n\nThus, in this kernel I will try to assess the overall quality of McDonald's food regarding saturated fats and sugar content. During the data preprocessing I have grouped the food items into three clusters based on their sugar and fat makeup. Using K-Means clustering I have found that foods with exceeded Daily values % of Sugar and Fat were found at the clusters 1 and 3. The Cluster 2 however displays foods which do not exceed the daily uptake of Fat and Sugar, providing an opportunity to eat healthy at McDonald's. Using my deployed model we can easily classify the individual food items!\n\n**Please do UpVote this Notebook if you find it helpful!**","fe48f77f":"<a class = 'anchor' id = 'Clustering_model_validation'><\/a>","848b2ac6":"**Based on the confusion matrix the best model is Random Forest Classifier!**","0f2a8579":"# Model Building","a57785ca":"# Model Evaluation","7087bcc1":"# Deployment","3952c648":"Summary of plots distributions:\n\n1. Saturated fat: Looks like (Saturated) fats are within the range of the daily value (0 - 100%).\n2. Sugars: Sugars however significantly exceed the daily value! Some of foods have overwhelming 400% sugars of daily dose!\n3. Cholesterol: most of foods have low cholesterol index\n\n**The key takeaway:\nEven though fats appear to be in acceptable range, the major concern for us is the combinaton of the fats with the considerably high levels of sugars. Later we will combine the sugars and fats together and cluster food items based on this combined fat-sugar value.**\n"}}