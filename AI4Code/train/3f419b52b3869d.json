{"cell_type":{"bd52e904":"code","04094417":"code","56991e46":"code","80bd5b17":"code","c20e2697":"code","55fd9b98":"code","bdd46ccb":"code","a7a82be7":"code","72d0d36f":"code","fe1e1808":"code","8588bf0e":"code","e5dcb89a":"code","1f9f0e99":"code","d1f58ad6":"code","6a9ea2b6":"code","d0a0874d":"code","88a67160":"code","2dcbd49f":"code","038232a4":"code","6a9cc6c3":"code","3b9840b6":"code","d27618c5":"code","1e5690bc":"code","ad4cf914":"code","283d8756":"code","47b21283":"code","69273098":"code","82a9d992":"markdown","49e304e3":"markdown","7418025e":"markdown","dd7b07a6":"markdown","5c6a21a8":"markdown","3ddd7855":"markdown","019f790a":"markdown","298bea12":"markdown","757d40ea":"markdown","a73cd593":"markdown","701d915c":"markdown","4ede9f84":"markdown","9dff1f1b":"markdown"},"source":{"bd52e904":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# various dask tasks\nimport dask                                          # distributed parallel processing\nimport dask.dataframe as dd                          # data processing, CSV file I\/O (e.g. pd.read_csv), dask\nfrom dask.distributed import Client, progress        # task distribution\nfrom dask_ml.model_selection import RandomizedSearchCV # for use with keras tuner, which is imported below\n\n# joblib and dumping models to disc\nimport joblib\nfrom joblib import dump\n\n#progress bar library\nfrom tqdm import tqdm\n\n#sci-kit learn inclusion\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# alternative tokenizer\nimport spacy\nnlp = spacy.load(\"en\")\n\n# HuggingFace\nfrom transformers import BertTokenizer, TFBertModel, BertConfig\n\n# TF \/ Keras imports\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import activations, layers, initializers, Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\n#Keras tuner\nfrom kerastuner.tuners import RandomSearch\n\n\n# For opening files\nimport codecs\nimport os\nimport pathlib\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","04094417":"# set here just one mode, so we don't waste time considering the others\nEMBEDDING_SOURCE = \"GLOVE\"\n# do we wish to use keras tune to do our hyperparameter tuning\nTUNER_ON = True\n# dask status\nDASK_ON = False","56991e46":"if DASK_ON is True:\n    client = Client()","80bd5b17":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.shape","c20e2697":"train_df.head()","55fd9b98":"train_df.hist('target', bins=2)","bdd46ccb":"train_df.keyword.value_counts()","a7a82be7":"train_df.keyword.unique()","72d0d36f":"train_df.location.unique()","fe1e1808":"train_df.location.value_counts()","8588bf0e":"def keyword_cleaner(x):\n    if type(x) is float:\n        return 'UNKNOWN'\n    else:\n        x = x.replace('%20', ' ')\n        doc = nlp(x)\n        full_lemma = ' '.join(y.lemma_ for y in doc)\n        return full_lemma\n\ntrain_df['keyword_cleaned'] = train_df['keyword'].map(keyword_cleaner)\n    \nkeyword_enc = OneHotEncoder(handle_unknown='ignore')\ntransformed = keyword_enc.fit_transform(train_df['keyword_cleaned'].to_numpy().reshape(-1, 1))\nohe_df = pd.DataFrame(transformed.todense(), columns=keyword_enc.get_feature_names())\nmeta_features = len(ohe_df.columns)\n    \nohe_df.to_csv('ohe_train_df.csv', index=False)\ndump(keyword_enc, 'keyword_encoder.joblib')\n    ","e5dcb89a":"def location_cleaner(x):\n    if type(x) is float:\n        return 'unknown'\n    else:\n        x = x.lower().strip()\n        x = x.replace('?', '')\n        if len(x) == 0:\n            return 'unknown'\n        x = x.replace('united states', 'usa')\n        x = x.replace('usa of america', 'usa')\n        x = x.replace('norf carolina', 'north carolina')\n        if x == 'tn':\n            x = 'tennessee'\n        return x\n\ntrain_df['location_cleaned'] = train_df['location'].map(location_cleaner)\n    \nlocation_enc = OneHotEncoder(handle_unknown='ignore')\ntransformed = location_enc.fit_transform(train_df['location_cleaned'].to_numpy().reshape(-1, 1))\nohe_location_df = pd.DataFrame(transformed.todense(), columns=location_enc.get_feature_names())\nmeta_location_features = len(ohe_location_df.columns)\n    \nohe_location_df.to_csv('ohe_location_train_df.csv', index=False)\ndump(location_enc, 'location_encoder.joblib')","1f9f0e99":"train_x = train_df[['text']] \ntrain_y = train_df[['target']]\ninput_shape = train_x.shape[0]\n\nvocab = set()\nfor desc in train_df['text'].values:\n    vocab = vocab.union(set(desc.split(' ')))\nvocab_size = len(vocab) + 1\n\nprint(vocab_size)\nprint(train_x.shape)\n\nword_index_cnt = 1\nword_index = {}\nseq_of_seqs = []\nrows = train_x['text'].tolist()\nprint(len(rows))\niters = 0\nfor snippet in train_x['text'].tolist():\n    if iters % 1000 == 0:\n        print(iters)\n    doc = nlp(snippet)\n    sequences = []\n    for token in doc:\n        lower_token = token.text.lower()\n        if(lower_token not in word_index):\n            word_index[lower_token] = word_index_cnt\n            word_index_cnt += 1    \n        sequences.append(word_index[lower_token])\n    seq_of_seqs.append(sequences)\n    iters += 1\n\n# OOV marker - for prediction use only\nword_index['<OOV>'] = word_index_cnt\nword_index_cnt += 1    \n    \nmax_len = 25        \ntrain_x_new = pad_sequences(seq_of_seqs, maxlen=max_len)\n\nprint('Done tokenization')","d1f58ad6":"if EMBEDDING_SOURCE == 'FASTTEXT':\n    #load embeddings\n    print('loading word embeddings...')\n    embeddings_index = {}\n    with codecs.open('\/kaggle\/input\/fasttext\/wiki.simple.vec', encoding='utf-8') as fs:\n        for line in tqdm(fs):\n            values = line.rstrip().rsplit(' ')\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    #embedding matrix\n    print('preparing embedding matrix...')\n\n    MAX_NB_WORDS = 100000\n    # the pre-trained weights are 300, so this set for us\n    FASTTEXT_EMBEDDING_DIM = 300\n\n    words_not_found = []\n    nb_words = min(MAX_NB_WORDS, len(word_index) + 1)\n    embedding_matrix = np.zeros((nb_words, FASTTEXT_EMBEDDING_DIM))\n    for word, i in word_index.items():\n        if i >= nb_words:\n            continue\n        embedding_vector = embeddings_index.get(word)\n        if (embedding_vector is not None) and len(embedding_vector) > 0:\n            # words not found in embedding index will be all-zeros.\n            embedding_matrix[i] = embedding_vector\n        else:\n            words_not_found.append(word)\n    print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n    fasttext_embedding_layer = layers.Embedding(nb_words, FASTTEXT_EMBEDDING_DIM, weights=[embedding_matrix], input_length=input_shape, trainable=False)","6a9ea2b6":"if EMBEDDING_SOURCE == 'GLOVE':\n    print('Creating Glove Embedding Layer')\n\n    embeddings_index = {}\n    GLOVE_DIR = '\/kaggle\/input\/glove-global-vectors-for-word-representation'\n    GLOVE_EMBEDDING_DIM = 200\n\n    with open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt')) as fs:\n        lines = fs.readlines()\n    for line in lines:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\n    print('Found %s word vectors.' % len(embeddings_index))\n\n    hits = 0\n    misses = 0\n    missed_words = []\n\n    # Prepare embedding matrix\n    embedding_matrix = np.zeros((len(word_index) + 1, GLOVE_EMBEDDING_DIM))\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            # Words not found in embedding index will be all-zeros.\n            # This includes the representation for \"padding\" and \"OOV\"\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n            missed_words.append(word)\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n    print(missed_words[0:100])\n\n    glove_embedding_layer = layers.Embedding(\n        len(word_index) + 1,\n        GLOVE_EMBEDDING_DIM,\n        embeddings_initializer=initializers.Constant(embedding_matrix),\n        input_length=input_shape,\n        trainable=False,\n    )","d0a0874d":"if EMBEDDING_SOURCE == 'BERT':\n    print('Creating BERT Embedding Layer')\n\n    # Highly dubious, update as is necessary\n    BERT_EMBEDDING_DIM = 256\n    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    \n    iters = 0    \n    ii_list = []\n    am_list = []\n    tti_list = []\n    \n    for snippet in train_x['text'].tolist():\n        if iters % 1000 == 0:\n            print(iters)\n\n        encoding = tokenizer(snippet)\n        \n        # Create inputs\n        input_ids = encoding['input_ids']\n        token_type_ids = [0] * len(encoding['input_ids'])\n        attention_mask = [1] * len(input_ids)\n\n        # Pad and create attention masks.\n        # Skip if truncation is needed\n        padding_length = max_len - len(input_ids)\n        if padding_length > 0:  # pad\n            input_ids = input_ids + ([0] * padding_length)\n            attention_mask = attention_mask + ([0] * padding_length)\n            token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        ii_list.append(input_ids)\n        am_list.append(attention_mask)\n        tti_list.append(token_type_ids)\n        \n        iters += 1\n\n    encoder = TFBertModel.from_pretrained(\"bert-base-uncased\")\n    print(encoder)\n\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n    \n    #bert_embedding_layer = encoder(\n    #    input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n    #)[0]\n    \n    sequence_output, pooled_output = encoder({\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"attention_mask\": attention_mask\n    })    \n    \n    print(type(seq_output))\n    print(type(pooled_output))\n    print(type(encoder.bert))\n    \n    bert_embedding_layer = encoder.bert\n    \n    print('Done creating BERT Embedding layer')","88a67160":"if EMBEDDING_SOURCE == 'CUSTOM':\n    CUSTOM_EMBEDDING_DIM = 512\n    training_data_embedding_layer = layers.Embedding(input_dim = vocab_size, output_dim = CUSTOM_EMBEDDING_DIM, input_length = input_shape)","2dcbd49f":"def build_model(hp):\n\n    # this is treated as free text\n    nlp_input = layers.Input(shape=(input_shape,), name='nlp_input')\n    # these are treated as categorical features\n    meta_input = layers.Input(shape=(meta_features + meta_location_features,), name='meta_input')\n\n    if EMBEDDING_SOURCE == 'GLOVE':\n        print('GLOVE EMBARKED')\n        FINAL_EMBEDDING_DIM = GLOVE_EMBEDDING_DIM\n        embedding_layer = glove_embedding_layer(nlp_input)\n    elif EMBEDDING_SOURCE == 'BERT':\n        print('BERT EMBARKED')\n        FINAL_EMBEDDING_DIM = BERT_EMBEDDING_DIM\n        embedding_layer = bert_embedding_layer(nlp_input)\n    elif EMBEDDING_SOURCE == 'FASTTEXT':\n        print('FASTTEXT EMBARKED')\n        FINAL_EMBEDDING_DIM = FASTTEXT_EMBEDDING_DIM\n        embedding_layer = fasttext_embedding_layer(nlp_input)  \n    else:\n        # default is create embeddings from the training data\n        FINAL_EMBEDDING_DIM = CUSTOM_EMBEDDING_DIM\n        embedding_layer = training_data_embedding_layer(nlp_input)\n        \n    lstm_layer = layers.LSTM(FINAL_EMBEDDING_DIM, dropout=0.1)(embedding_layer)\n\n    merged = layers.Concatenate(axis=1)([lstm_layer, meta_input])\n    hp_dense_units = hp.Int('units', min_value = 32, max_value = 512, step = 32)\n    dense_layer = layers.Dense(hp_dense_units, activation='relu')(merged)\n    output_layer = layers.Dense(1, activation='sigmoid')(dense_layer)\n\n    model = Model(inputs=[nlp_input, meta_input], outputs=[output_layer])\n\n    hp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3, 1e-4]) \n    \n    model.compile(loss='binary_crossentropy', optimizer = Adam(learning_rate=hp_learning_rate), metrics=['accuracy'], )\n    \n    return model","038232a4":"def build_model_wrapper(optimizer=\"adam\", embedding_size = 512, dense_size=32, dropout_value = 0.1):\n    \n    # this is treated as free text\n    nlp_input = layers.Input(shape=(input_shape,), name='nlp_input')\n    # these are treated as categorical features\n    meta_input = layers.Input(shape=(meta_features + meta_location_features,), name='meta_input')\n\n    if EMBEDDING_SOURCE == 'GLOVE':\n        print('GLOVE EMBARKED')\n        FINAL_EMBEDDING_DIM = GLOVE_EMBEDDING_DIM\n        embedding_layer = glove_embedding_layer(nlp_input)\n    elif EMBEDDING_SOURCE == 'BERT':\n        print('BERT EMBARKED')\n        FINAL_EMBEDDING_DIM = BERT_EMBEDDING_DIM\n        embedding_layer = bert_embedding_layer(nlp_input)\n    elif EMBEDDING_SOURCE == 'FASTTEXT':\n        print('FASTTEXT EMBARKED')\n        FINAL_EMBEDDING_DIM = FASTTEXT_EMBEDDING_DIM\n        embedding_layer = fasttext_embedding_layer(nlp_input)  \n    else:\n        # default is create embeddings from the training data\n        FINAL_EMBEDDING_DIM = CUSTOM_EMBEDDING_DIM\n        embedding_layer = training_data_embedding_layer(nlp_input)\n        \n    lstm_layer = layers.LSTM(FINAL_EMBEDDING_DIM, dropout=dropout_value)(embedding_layer)\n\n    merged = layers.Concatenate(axis=1)([lstm_layer, meta_input])\n    dense_layer = layers.Dense(dense_size, activation='relu')(merged)\n    output_layer = layers.Dense(1, activation='sigmoid')(dense_layer)\n\n    model = Model(inputs=[nlp_input, meta_input], outputs=output_layer)\n\n    model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['accuracy'], )\n    \n    print(model.summary())    \n    \n    return model","6a9cc6c3":"if TUNER_ON is True:\n    tuner = RandomSearch(build_model,\n        objective='val_loss',\n        max_trials=18,\n        executions_per_trial=1,\n    )","3b9840b6":"tf.config.run_functions_eagerly(True)\n\nepochs = 5\nbatch_size = 64\nes_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# splitting training into train and validation\nX_train_pure, X_valid, y_train_pure, y_valid = train_test_split(train_x_new, train_y, test_size = 0.15, random_state=0)\nohe_train, ohe_valid = train_test_split(ohe_df, test_size = 0.15, random_state=0)\nohe_location_train, ohe_location_valid = train_test_split(ohe_location_df, test_size = 0.15, random_state=0)\n\nprint(ohe_train.shape)\nprint(ohe_location_train.shape)\n\nmeta_train = pd.concat([ohe_train, ohe_location_train], axis=1, sort=False)\nmeta_valid = pd.concat([ohe_valid, ohe_location_valid], axis=1, sort=False)\n\nif TUNER_ON is True:\n    tuner.search([X_train_pure, meta_train], y_train_pure, epochs=epochs, callbacks=[es_callback], validation_data = ([X_valid, meta_valid], y_valid))\n    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n    model_real = tuner.hypermodel.build(best_hps)\nelse:\n    print('Using KerasClassifier method to select best hyperparameters for model')\n    \n    clf = KerasClassifier(build_fn=build_model_wrapper)\n    \n    # define the grid search parameters\n    #parameters = {\n    #    'epochs': [10, 100, ],\n    #    'batch_size':[2, 16, 32],\n    #    'dense_size':[8, 16, 32, 64],\n    #    #'embedding_size': [32, 64, 128, 256, 512],\n    #    'optimizer':['RMSprop', 'Adam', 'Adamax', 'sgd'],\n    #    'dropout_value': [0.5, 0.4, 0.3, 0.2, 0.1, 0]\n    #}\n    parameters = {\n        'epochs': [10],\n        'batch_size':[32],\n        'dense_size':[32],\n        #'embedding_size': [32, 64, 128, 256, 512],\n        'optimizer':['Adam'],\n        'dropout_value': [0.1]\n    }\n    \n    kfold_splits = 2\n    grid_search = GridSearchCV(estimator=clf,  \n                    n_jobs=-1, \n                    verbose=1,\n                    return_train_score=True,\n                    cv=kfold_splits,  \n                    #StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n                    param_grid=parameters,)    \n\n    # I don't think GridSearch for multiple outputs really does work, proceed with caution\n    combi_input = np.concatenate([X_train_pure, meta_train], axis=1)\n    \n    if DASK_ON is True:\n        with joblib.parallel_backend('dask'):\n            grid_search.fit(combi_input, y_train_pure)\n    else:\n        grid_search.fit(combi_input, y_train_pure)\n\n    model_real = grid_search.best_estimator_\n    print(grid_search.best_parameters_)\n    \nmodel_real.summary()","d27618c5":"if DASK_ON is True:\n    with joblib.parallel_backend('dask'):\n       fit_history = model_real.fit([X_train_pure, meta_train], y_train_pure, batch_size = batch_size, epochs=epochs, callbacks=[es_callback], validation_data = ([X_valid, meta_valid], y_valid), verbose=1)\nelse:\n    fit_history = model_real.fit([X_train_pure, meta_train], y_train_pure, batch_size = batch_size, epochs=epochs, callbacks=[es_callback], validation_data = ([X_valid, meta_valid], y_valid), verbose=1)","1e5690bc":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_df.shape","ad4cf914":"test_df['keyword_cleaned'] = test_df['keyword'].map(keyword_cleaner)\n    \ntransformed = keyword_enc.transform(test_df['keyword_cleaned'].to_numpy().reshape(-1, 1))\nohe_df = pd.DataFrame(transformed.todense(), columns=keyword_enc.get_feature_names())\nohe_df.to_csv('ohe_test_df.csv', index=False)","283d8756":"test_df['location_cleaned'] = test_df['location'].map(location_cleaner)\n    \ntransformed = location_enc.transform(test_df['location_cleaned'].to_numpy().reshape(-1, 1))\nohe_location_df = pd.DataFrame(transformed.todense(), columns=location_enc.get_feature_names())\nohe_location_df.to_csv('ohe_location_test_df.csv', index=False)","47b21283":"X_test = test_df[['text']]\n\n#test_sequences = tokenizer.texts_to_sequences(X_test['text'])\n\ntest_sequences = []\nfor snippet in X_test['text'].tolist():\n    sequences = []\n    doc = nlp(snippet)\n    for token in doc:\n        lower_token = token.text.lower()\n        if(lower_token not in word_index):\n            sequences.append(word_index['<OOV>'])\n        else:\n            sequences.append(word_index[lower_token])\n    test_sequences.append(sequences)\n        \nX_test_new = pad_sequences(test_sequences, maxlen=max_len)\n\nmeta_test = pd.concat([ohe_df, ohe_location_df], axis=1, sort=False)\n\npreds = model_real.predict([X_test_new, meta_test])\npreds = np.ndarray.round(preds).astype(int)\npreds","69273098":"# you could use any filename. We choose submission here\nsub_df = pd.DataFrame({'id': test_df.id, 'target': preds.flatten()})\nsub_df.to_csv('submission21.csv', index=False)","82a9d992":"If we want to solely use the input text to create embeddings, this is what we need.\nIt is good to have this as he last option in case we want to start with pretrained weights and then\nfine tune with the competition's text (if it makes sense and not already taken care of already).","49e304e3":"Let us perform manipulation of keyword column to reduce the number of unique values","7418025e":"Here we will create an embedding layer based upon FastText","dd7b07a6":"Any globals needed throughout the notebook","5c6a21a8":"Create a tuner to search for the best hyperparameters","3ddd7855":"Search for the model with the best hyperparameters","019f790a":"Tokenization performed with the help of SpaCy","298bea12":"Start dask Client","757d40ea":"Here is where we create the GLOVE embedding layer, if necessary","a73cd593":"If we wish to use pretrained embeddings, this is what our Embedding layer should look like.  Each layer is created only if used for the notebook (i.e.  if we use GLOVE, don't create the embedding layer for FastText)","701d915c":"Now build the model with the best hyperparameters","4ede9f84":"This is where we create embedding layer for BERT, if appropriate","9dff1f1b":"Similar mainpulation \/ cleaning of the location column"}}