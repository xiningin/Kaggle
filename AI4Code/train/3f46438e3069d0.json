{"cell_type":{"558ae615":"code","e06e9961":"code","8115a68b":"code","eb6ca3f7":"code","c7b8f07e":"code","e9482853":"code","9bb7692f":"code","8027cd06":"code","3b67196c":"code","02d380ba":"code","04328df4":"code","a45b9f35":"code","76649a54":"code","155e84d1":"code","4eece3ee":"code","d7a110b9":"code","2895cf0b":"code","24b130ce":"code","b6e88aa1":"code","d381eb4a":"code","f620813c":"code","a694fd34":"markdown","a51c0fec":"markdown","80693b39":"markdown","9f7f2ba4":"markdown","71b57898":"markdown","79478b5a":"markdown","60773d77":"markdown","7673ff56":"markdown","0c6c8a3b":"markdown","5735b2e0":"markdown"},"source":{"558ae615":"import os\nos.listdir('..\/input\/competitive-data-science-predict-future-sales')","e06e9961":"import numpy as np \nimport pandas as pd\nimport random as rd \nimport math\nimport datetime \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8115a68b":"shops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nsales = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nitem_cat = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\n\nsub = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\n\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","eb6ca3f7":"items.head()","c7b8f07e":"topItemCats = items[\"item_category_id\"].value_counts().index[:20]\ntopItemCatsValues = items[\"item_category_id\"].value_counts().values[:20]\n\nplt.figure(figsize=(18, 6))\nax = sns.barplot(topItemCats, topItemCatsValues, alpha=0.8)\nplt.title(\"Top 20 item catgories sold\")\nplt.ylabel('Items values', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","e9482853":"sales.info()","9bb7692f":"sales[\"date\"] = pd.to_datetime(sales[\"date\"])\n\nsales.head()","8027cd06":"price_idx = sales[\"item_price\"].value_counts().index[:10]\nunitsSold = sales[\"item_price\"].value_counts().values[:10]\n\nplt.figure(figsize=(18, 6))\nax = sns.barplot(price_idx, unitsSold, order=price_idx, alpha=0.8)\nplt.title(\"Top 10 most sold amount\")\nplt.ylabel('Units Sold', fontsize=12)\nplt.xlabel('Price', fontsize=12)\nplt.show()","3b67196c":"shop_id_idx = sales[\"shop_id\"].value_counts().index[:10]\nunitsSold = sales[\"shop_id\"].value_counts().values[:10]\n\nplt.figure(figsize=(18, 6))\nax = sns.barplot(shop_id_idx, unitsSold, order=shop_id_idx, alpha=0.8)\nplt.title(\"Top 10 shops with most sales\")\nplt.ylabel('Units Sold', fontsize=12)\nplt.xlabel('Shop ID', fontsize=12)\nplt.show()","02d380ba":"sales[\"year\"] = sales[\"date\"].dt.year\nsales[\"month\"] = sales[\"date\"].dt.month\nsales[\"day\"] = sales[\"date\"].dt.day","04328df4":"plt.figure(figsize=(8,4))\nsns.countplot(y=\"year\", data=sales)\nplt.title(\"Total sales yearly\")","a45b9f35":"# We want to predict total products sold, so we are grouping no. of products sold on each \"date_block_num\"\nts = sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts","76649a54":"plt.figure(figsize=(16, 6))\nplt.title('Total sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)","155e84d1":"plt.figure(figsize=(16, 6))\nplt.plot(ts.rolling(12).mean().values, label='Rolling mean')\nplt.plot(ts.rolling(12).std().values, label='Rolling std')\nplt.legend()","4eece3ee":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nres = sm.tsa.seasonal_decompose(ts.values, freq=12, model=\"multiplicative\")\nfig = res.plot()","d7a110b9":"res = sm.tsa.seasonal_decompose(ts.values, freq=12, model=\"additive\")\nfig = res.plot()","2895cf0b":"def test_stationarity(data):\n    dftest = adfuller(data, autolag=\"AIC\")\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of observation Used'])\n    for key, val in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = val\n    print(dfoutput)\n    \ntest_stationarity(ts)","24b130ce":"log_transform = ts\nlog_transform = log_transform.apply(lambda x: math.log(1 + x))\nlog_transform = pd.DataFrame(log_transform)\ndiff = log_transform - log_transform.shift(1)\ndiff = diff.fillna(0) \ntest_stationarity(diff)","b6e88aa1":"plt.figure(figsize=(16,16))\n\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\n\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(diff)\nplt.show()","d381eb4a":"#ACF and PACF plots:\nfrom statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(diff, nlags=20)\nlag_pacf = pacf(diff, nlags=20, method='ols')\n\nplt.figure(figsize=(16, 7))\n#Plot ACF: \nplt.subplot(121) \nplt.plot(lag_acf, marker=\"o\")\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n\n#Plot PACF:\nplt.subplot(122)\nplt.plot(lag_pacf, marker=\"o\")\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","f620813c":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(ts, order=(1, 1, 1))\nresults_AR = model.fit(disp=-1)","a694fd34":"Sales are decreasing each year","a51c0fec":"There is a trend in our data\n\nResiduals =>\n\nWhen you remove the trend, Seasonality and other observable patterns from the data, white noise is left and this    termed as residuals","80693b39":"We have to reduce the p-value by taking log","9f7f2ba4":"# Sales","71b57898":"* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* date - date in format dd\/mm\/yyyy\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category","79478b5a":"# Items","60773d77":"q - The lag value where the ACF chart crosses the upper confidence interval for the first time, \n    q=1 has +ve correlation\n    \np - The lag value where the PACF chart crosses the upper confidence interval for the first time, \n     p=1 has +ve correlation and p=11 has high -ve correlation","7673ff56":"ADF test for stationarity\n\n* Null Hypothesis: data is non-stationary\n* large p-values are indicative of non-stationarity, and small p-values suggest stationarity. Using the usual 5% threshold, differencing is required if the p-value is greater than 0.05.","0c6c8a3b":"The p-value is reduced","5735b2e0":"Now by calculating ACF and PACF we can get the values of p and q which will be used to in ARIMA model"}}