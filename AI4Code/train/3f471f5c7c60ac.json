{"cell_type":{"f6fc85f5":"code","0309e325":"code","e738a6ab":"code","68ccb31c":"code","1a026bd7":"code","0b1cb383":"code","14f7c0ae":"code","8b5b3e64":"code","1c4fbd08":"code","9d2d95b8":"code","3d4482e6":"code","c3d60291":"code","ef8e4a52":"code","1c9a57a9":"code","d9663f4b":"code","b01be254":"code","72e92ec7":"code","8a14cafd":"code","f33b29db":"code","62a2b846":"code","3657f17e":"code","8b00bc2b":"code","f1480704":"code","5fb5a79a":"code","0868a332":"code","f192fed8":"code","5863f3c7":"code","7232edd9":"code","17d59ad8":"code","5d53effa":"code","00ea991f":"code","995110d6":"code","4dd23a17":"code","fbffe16d":"code","9175ac31":"code","1649220d":"code","1169aed2":"code","f5e9c4fc":"code","79c0fd44":"code","6115520d":"code","7290a6f6":"code","0c15df6d":"code","cc12984f":"code","03b2fa75":"code","e35e125b":"code","fb67e677":"code","08a04f03":"code","d4a8070c":"code","be993baa":"code","1bd025b7":"code","9d295294":"code","ce34ae2e":"code","497b8f6e":"code","8f5de78e":"code","6472b5d6":"code","f4bf74c5":"code","da1c00cb":"code","8e6dd18d":"code","b811f852":"code","70a092ff":"code","9c336540":"code","174d5433":"code","49edab18":"code","f37f4576":"code","34e05174":"code","54c5aeb2":"code","c959ffa6":"markdown","38156ce1":"markdown","e99a903f":"markdown","a7a9ca32":"markdown","4c1542d0":"markdown","3b6a0f9d":"markdown","921089c7":"markdown","7ac4f685":"markdown","6956bb3a":"markdown","5be345de":"markdown","a9787da1":"markdown","5587dea6":"markdown","b300ea23":"markdown","18170d8a":"markdown","b4e7fa68":"markdown","67298f40":"markdown","f420c439":"markdown","adb256a4":"markdown"},"source":{"f6fc85f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0309e325":"train = pd.read_csv('..\/input\/train_V2.csv')\ntest = pd.read_csv('..\/input\/test_V2.csv')","e738a6ab":"train.head()","68ccb31c":"train.shape, test.shape","1a026bd7":"# train = train[:500000]","0b1cb383":"test.head()","14f7c0ae":"train.columns","8b5b3e64":"target_col = 'winPlacePerc'","1c4fbd08":"train.drop(2744604, inplace=True, errors='ignore')","9d2d95b8":"train.dtypes","3d4482e6":"train['Id'].describe()","c3d60291":"train['groupId'].describe()","ef8e4a52":"train['matchId'].describe()","1c9a57a9":"train['matchType'].describe()","d9663f4b":"train['matchType'].unique()","b01be254":"pd.get_dummies(train['matchType']).head() # getting one hot encoded dataframe","72e92ec7":"# Get one hot encoding of columns matchType\none_hot_train = pd.get_dummies(train['matchType'])\n# Drop column B as it is now encoded\ntrain = train.drop('matchType', axis = 1)\n# Join the encoded df\ntrain = train.join(one_hot_train)\ntrain.head()","8a14cafd":"# Get one hot encoding of columns matchType\none_hot_test = pd.get_dummies(test['matchType'])\n# Drop column B as it is now encoded\ntest = test.drop('matchType', axis = 1)\n# Join the encoded df\ntest = test.join(one_hot_test)\ntest.head()","f33b29db":"print(\"Adding Features\")\ndef add_feature(df):\n    df['headshotrate'] = df['kills']\/df['headshotKills']\n    df['killStreakrate'] = df['killStreaks']\/df['kills']\n    df['healthitems'] = df['heals'] + df['boosts']\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    df['killPlace_over_maxPlace'] = df['killPlace'] \/ df['maxPlace']\n    df['headshotKills_over_kills'] = df['headshotKills'] \/ df['kills']\n    df['distance_over_weapons'] = df['totalDistance'] \/ df['weaponsAcquired']\n    df['walkDistance_over_heals'] = df['walkDistance'] \/ df['heals']\n    df['walkDistance_over_kills'] = df['walkDistance'] \/ df['kills']\n    df['killsPerWalkDistance'] = df['kills'] \/ df['walkDistance']\n    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"]\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    print(\"Removing Na's From DF\")\n    df.fillna(0, inplace=True)","62a2b846":"#head = train.head()\n# add_feature(train)\n# add_feature(test)\n#head","3657f17e":"train.shape, test.shape","8b00bc2b":"from sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train, test_size=0.2)","f1480704":"trainIds = train['Id']\ntestIds = test['Id']","5fb5a79a":"#??train.drop","0868a332":"x_train = train.drop(['Id', 'groupId', 'matchId', target_col], axis=1)\nx_train.head(1)","f192fed8":"y_train = train[target_col]\ny_train.head()","5863f3c7":"x_val = val.drop(['Id', 'groupId', 'matchId', target_col], axis=1)\nx_val.head(1)","7232edd9":"y_val = val[target_col]\ny_val.head()","17d59ad8":"x_test = test.drop(['Id', 'groupId', 'matchId'], axis=1)\nx_test.head(1)","5d53effa":"x_train.shape, x_test.shape","00ea991f":"from sklearn.tree import DecisionTreeRegressor","995110d6":"#%timeit \ndtree=DecisionTreeRegressor(max_depth=3)\ndtree.fit(x_train, y_train)","4dd23a17":"dtree.score(x_train, y_train)","fbffe16d":"dtree.score(x_val, y_val)","9175ac31":"from IPython.display import SVG\nfrom graphviz import Source\nfrom sklearn import tree\ngraph = Source( tree.export_graphviz(dtree, out_file=None, feature_names=x_train.columns))\nSVG(graph.pipe(format='svg'))","1649220d":"p_val = dtree.predict(x_val)","1169aed2":"def mae(p, t):\n    return np.sum(np.abs(p - t)) \/ len(p)","f5e9c4fc":"mae(p_val, y_val)","79c0fd44":"dtree=DecisionTreeRegressor(max_depth=15)\ndtree.fit(x_train, y_train)","6115520d":"def print_score(mm):\n    print(\"train r^2 \" + str(mm.score(x_train, y_train)))\n    print(\"validation r^2 \" + str(mm.score(x_val, y_val)))\n    p_val = mm.predict(x_val)\n    p_train = mm.predict(x_train)\n    print(\"mean absolute error(Train): \" + str(mae(p_train, y_train)))\n    print(\"mean absolute error(Validation): \" + str(mae(p_val, y_val)))\nprint_score(dtree)","7290a6f6":"p_test = dtree.predict(x_test)","0c15df6d":"p_test","cc12984f":"submission = pd.DataFrame()\nsubmission['Id'] = testIds\nsubmission[target_col] = p_test","03b2fa75":"submission.head()","e35e125b":"submission.to_csv(\"submission.csv\", index=False)","fb67e677":"def train_and_get_score(max_depth):\n    dtree=DecisionTreeRegressor(max_depth=max_depth)\n    dtree.fit(x_train[:10000], y_train[:10000])\n    p_val = dtree.predict(x_val[:1000])\n    p_train = dtree.predict(x_train[:10000])\n    return mae(p_train, y_train[:10000]), mae(p_val, y_val[:1000])","08a04f03":"train_and_get_score(5)","d4a8070c":"train_score = []\nvalid_score = []\nx_axis = []\nfor max_depth in range(1, 40, 5):\n    ts, vs = train_and_get_score(max_depth)\n    train_score.append(ts)\n    valid_score.append(vs)\n    x_axis.append(max_depth)","be993baa":"import matplotlib.pyplot as plt\nplt.plot(x_axis, train_score,)\nplt.plot(x_axis, valid_score)\nplt.ylabel('Score')\nplt.xlabel('max_depth')\nplt.show()","1bd025b7":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_jobs=-1, max_depth=15, max_features='sqrt', n_estimators=100)\n%time model.fit(x_train, y_train)","9d295294":"print_score(model)","ce34ae2e":"model.estimators_[0]","497b8f6e":"print_score(model.estimators_[0])","8f5de78e":"#for dtree_model in model.estimators_:\n#    print_score(dtree_model)\n#    print(\"------------------------------------\")","6472b5d6":"p_test = model.predict(x_test)\nsubmission = pd.DataFrame()\nsubmission['Id'] = testIds\nsubmission[target_col] = p_test\nsubmission.to_csv(\"submission_random_forest_30.csv\", index=False)","f4bf74c5":"feature_imp = pd.DataFrame(model.feature_importances_,\n                                   index = x_train.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)","da1c00cb":"feature_imp","8e6dd18d":"feature_imp['importance'].nlargest(10).plot(kind='barh')","b811f852":"#import xgboost","70a092ff":"# xgb = xgboost.XGBRegressor(n_jobs=-1, n_estimators=100, learning_rate=0.2, gamma=0, subsample=0.75,\n                           #colsample_bytree=1, max_depth=3)\n# %time xgb.fit(x_train, y_train)","9c336540":"#print_score(xgb)","174d5433":"from lightgbm import LGBMRegressor\nparams = {\n    'n_estimators': 100,\n    'learning_rate': 0.3, \n    'num_leaves': 30,\n    'objective': 'regression_l2', \n    'metric': 'mae',\n    'verbose': -1,\n}\n\nmodel = LGBMRegressor(**params)\nmodel.fit(\n   x_train, y_train,\n    eval_metric='mae',\n    verbose=20,\n)","49edab18":"print_score(model)","f37f4576":"p_test = model.predict(x_test)\nsubmission = pd.DataFrame()\nsubmission['Id'] = testIds\nsubmission[target_col] = p_test\nsubmission.to_csv(\"submission_lightgdm.csv\", index=False)","34e05174":"feature_imp = pd.DataFrame(model.feature_importances_,\n                                   index = x_train.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)","54c5aeb2":"feature_imp['importance'].nlargest(10).plot(kind='barh')","c959ffa6":"Let's now increase the depth to get our base score.","38156ce1":"So lets check the shape of dataframes to see everything is alright.","e99a903f":"# Decision Trees\nLets build a decision tree regrassor,\nRegrasor is a meachine learning model which is used to predict continous variable.\nSince We expect to have a continous functional relationship between input and final output. It's better to use Regrassor for this problem.","a7a9ca32":"* walkDistance - Total distance traveled on foot measured in meters.\n* killPlace - Ranking in match of number of enemy players killed.","4c1542d0":"So let's explore this...\n\n**How do you find a single best split?**\n- So the  idea is to try out every variable and split on every posible value of that variable.\n\n**Okay, this sounds simple but how do you measure the quality of the split?**\n\n- You could take a weighted average of sample and mse to get the total mse and then just pick the best split.\nNow you just have to recurse it for every posbile split till you hit max_depth and thats it. You have your decision tree.","3b6a0f9d":"# Random forest","921089c7":"matchType is another string field. We have to convert that to numerical variable. ","7ac4f685":"There are basically 2 ways to convert string column to  numerical. \n* Assign a unique number to each of the category.\n* use one hot encoding \nWe have 16 unique value so it's a good candidate for one hot encoding.","6956bb3a":"So Most of our variables are int or float64. Thats good :)\nLet's explore some variables that are strings.","5be345de":"As you can see training error is going down as we increase max_depth in a decision tree. This is called overfitting. Which means model is specifically learning based on training data but not generalizing well. This is huge problem in decision tree.  \n**How do we solve this?**","a9787da1":"# Prepare data\n","5587dea6":"# Decision tree model\nwe have training data, validation data and test data now. So let try our first model. ","b300ea23":"So Id is unique as we expected. ","18170d8a":"Okay so now we have all the columns that we want to use as numeric. Lets split the data to validation and training.\n\nPossibly the most important idea in machine learning is that of having separate training & validation data sets. As motivation, suppose you don't divide up your data, but instead use all of it. And suppose you have lots of parameters:\n\n![image.png](attachment:image.png)\n\nThe error for the pictured data points is lowest for the model on the far right (the blue curve passes through the red points almost perfectly), yet it's not the best choice. Why is that? If you were to gather some new data points, they most likely would not be on that curve in the graph on the right, but would be closer to the curve in the middle graph.\n\nThis illustrates how using all our data can lead to overfitting. A validation set helps diagnose this problem.","b4e7fa68":"Lets see how change depth of the tree changes training and validation score","67298f40":"# PUBG\nBattle Royale-style video games have taken the world by storm. 100 players are dropped onto an island empty-handed and must explore, scavenge, and eliminate other players until only one is left standing, all while the play zone continues to shrink.\n\nPlayerUnknown's BattleGrounds (PUBG) has enjoyed massive popularity. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.\n\nThe team at PUBG has made official game data available for the public to explore and scavenge outside of \"The Blue Circle.\" This competition is not an official or affiliated PUBG site - Kaggle collected data made possible through the PUBG Developer API.\n\nYou are given over 65,000 games' worth of anonymized player data, split into training and testing sets, and asked to predict final placement from final in-game stats and initial player ratings.\n\nWhat's the best strategy to win in PUBG? Should you sit in one spot and hide your way into victory, or do you need to be the top shot? Let's let the data do the talking!","f420c439":"So groupId looks like a identifier for a team and match ID is a identifier for a match. Remember in pubG a team can have 4 people and match can have 100 people.\nobviously who is on your team and who are other players in your match is going to matter a lot but let's build some basic model without these 2 features.\nIt's always good idea to build something working and simple first to understand the complexity of the problem we are dealing with and get some base score.","adb256a4":"# Feature Engineering"}}