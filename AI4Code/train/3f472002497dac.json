{"cell_type":{"a534f294":"code","d60b09d7":"code","8725661f":"code","436dc50b":"code","1d85f178":"code","db54a6ec":"code","0298ece6":"code","54b00544":"code","a5f8cde3":"code","7dd52031":"code","c599ce65":"code","bf27f084":"code","5ee92f94":"code","16368c49":"code","bbf1b2bb":"code","28fd9515":"code","816b90b5":"code","e2a18d9e":"code","62d90753":"code","818a97d5":"code","e36aa293":"code","a05e8c15":"code","6aaeb6a2":"code","d2b701bf":"code","6ecbd2fe":"code","bec660bf":"code","19568680":"code","3edfe2a7":"code","424240a8":"code","1f7aba2a":"markdown","a12eef08":"markdown","60bf7e52":"markdown","e628d664":"markdown","c807d71b":"markdown"},"source":{"a534f294":"import pandas as pd\nimport numpy as np \n\nimport time\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"stopwords\")\n\nimport torch\nimport torch.nn.utils.rnn as rnn_utils\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","d60b09d7":"train_file_path = \"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\"\n# train_file_path = \".\/archive.zip\"","8725661f":"df = pd.read_csv(train_file_path, encoding='ISO-8859-1', header=None, engine='python')\n# df = pd.read_csv(train_file_path, compression='zip', encoding='ISO-8859-1', header=None)\n\ndf.columns = ['sentiment','id','date','flag','user','tweet']\ndf.sentiment = df.sentiment.map({4:1,0:0})","436dc50b":"# Remove duplicate tweets\ninit_count = len(df)\ntweet_unique = df.tweet.drop_duplicates(keep=False)\nrecords_with_same_tweet = df[True ^ df.tweet.isin(tweet_unique)]\nrecords_with_same_tweet[['sentiment', 'user', 'tweet']]\ndf = df[True ^ df.tweet.isin(records_with_same_tweet.tweet)]\ncurr_count = len(df)\nprint(f\"{init_count - curr_count} duplicate rows dropped!\\nCurrent row count: {curr_count}\\n\\n\")\n\ndf.sentiment.map({1:\"Positive(1)\",0:\"Negative(0)\"}).value_counts().plot(kind='bar', color=['green', 'red'])\nplt.title(\"Data distribution\", fontdict={\"fontsize\": 20})\nplt.show()","1d85f178":"stop_words = set(stopwords.words(\"english\"))","db54a6ec":"def word_count(df, title):\n    word_cloud = WordCloud(max_words=200,\n                           background_color='white',\n                           stopwords=stop_words,\n                           colormap='bone',\n                           height=1000,\n                           width=1800)\n    tweets = df.tweet.values\n    word_cloud.generate(str(tweets).lower())\n    fig = plt.figure()\n    plt.imshow(word_cloud)\n    fig.set_figwidth(10)\n    fig.set_figheight(10)\n    plt.title(title, fontdict={\"fontsize\": 20})\n    plt.show()\n\nword_count(df, \"Original tweets\")","0298ece6":"# use the lowercase \ndf[\"tweet\"] = df[\"tweet\"].str.lower()\n\n# abbreviation check list\nabbreviations = {\n        \"$\" : \" dollar \",\n        \"\u20ac\" : \" euro \",\n        \"4ao\" : \"for adults only\",\n        \"a.m\" : \"before midday\",\n        \"a3\" : \"anytime anywhere anyplace\",\n        \"aamof\" : \"as a matter of fact\",\n        \"acct\" : \"account\",\n        \"adih\" : \"another day in hell\",\n        \"afaic\" : \"as far as i am concerned\",\n        \"afaict\" : \"as far as i can tell\",\n        \"afaik\" : \"as far as i know\",\n        \"afair\" : \"as far as i remember\",\n        \"afk\" : \"away from keyboard\",\n        \"app\" : \"application\",\n        \"approx\" : \"approximately\",\n        \"apps\" : \"applications\",\n        \"asap\" : \"as soon as possible\",\n        \"asl\" : \"age, sex, location\",\n        \"atk\" : \"at the keyboard\",\n        \"ave.\" : \"avenue\",\n        \"aymm\" : \"are you my mother\",\n        \"ayor\" : \"at your own risk\", \n        \"b&b\" : \"bed and breakfast\",\n        \"b+b\" : \"bed and breakfast\",\n        \"b.c\" : \"before christ\",\n        \"b2b\" : \"business to business\",\n        \"b2c\" : \"business to customer\",\n        \"b4\" : \"before\",\n        \"b4n\" : \"bye for now\",\n        \"b@u\" : \"back at you\",\n        \"bae\" : \"before anyone else\",\n        \"bak\" : \"back at keyboard\",\n        \"bbbg\" : \"bye bye be good\",\n        \"bbc\" : \"british broadcasting corporation\",\n        \"bbias\" : \"be back in a second\",\n        \"bbl\" : \"be back later\",\n        \"bbs\" : \"be back soon\",\n        \"be4\" : \"before\",\n        \"bfn\" : \"bye for now\",\n        \"blvd\" : \"boulevard\",\n        \"bout\" : \"about\",\n        \"brb\" : \"be right back\",\n        \"bros\" : \"brothers\",\n        \"brt\" : \"be right there\",\n        \"bsaaw\" : \"big smile and a wink\",\n        \"btw\" : \"by the way\",\n        \"bwl\" : \"bursting with laughter\",\n        \"c\/o\" : \"care of\",\n        \"cet\" : \"central european time\",\n        \"cf\" : \"compare\",\n        \"cia\" : \"central intelligence agency\",\n        \"csl\" : \"can not stop laughing\",\n        \"cu\" : \"see you\",\n        \"cul8r\" : \"see you later\",\n        \"cv\" : \"curriculum vitae\",\n        \"cwot\" : \"complete waste of time\",\n        \"cya\" : \"see you\",\n        \"cyt\" : \"see you tomorrow\",\n        \"dae\" : \"does anyone else\",\n        \"dbmib\" : \"do not bother me i am busy\",\n        \"diy\" : \"do it yourself\",\n        \"dm\" : \"direct message\",\n        \"dwh\" : \"during work hours\",\n        \"e123\" : \"easy as one two three\",\n        \"eet\" : \"eastern european time\",\n        \"eg\" : \"example\",\n        \"embm\" : \"early morning business meeting\",\n        \"encl\" : \"enclosed\",\n        \"encl.\" : \"enclosed\",\n        \"etc\" : \"and so on\",\n        \"faq\" : \"frequently asked questions\",\n        \"fawc\" : \"for anyone who cares\",\n        \"fb\" : \"facebook\",\n        \"fc\" : \"fingers crossed\",\n        \"fig\" : \"figure\",\n        \"fimh\" : \"forever in my heart\", \n        \"ft.\" : \"feet\",\n        \"ft\" : \"featuring\",\n        \"ftl\" : \"for the loss\",\n        \"ftw\" : \"for the win\",\n        \"fwiw\" : \"for what it is worth\",\n        \"fyi\" : \"for your information\",\n        \"g9\" : \"genius\",\n        \"gahoy\" : \"get a hold of yourself\",\n        \"gal\" : \"get a life\",\n        \"gcse\" : \"general certificate of secondary education\",\n        \"gfn\" : \"gone for now\",\n        \"gg\" : \"good game\",\n        \"gl\" : \"good luck\",\n        \"glhf\" : \"good luck have fun\",\n        \"gmt\" : \"greenwich mean time\",\n        \"gmta\" : \"great minds think alike\",\n        \"gn\" : \"good night\",\n        \"g.o.a.t\" : \"greatest of all time\",\n        \"goat\" : \"greatest of all time\",\n        \"goi\" : \"get over it\",\n        \"gps\" : \"global positioning system\",\n        \"gr8\" : \"great\",\n        \"gratz\" : \"congratulations\",\n        \"gyal\" : \"girl\",\n        \"h&c\" : \"hot and cold\",\n        \"hp\" : \"horsepower\",\n        \"hr\" : \"hour\",\n        \"hrh\" : \"his royal highness\",\n        \"ht\" : \"height\",\n        \"ibrb\" : \"i will be right back\",\n        \"ic\" : \"i see\",\n        \"icq\" : \"i seek you\",\n        \"icymi\" : \"in case you missed it\",\n        \"idc\" : \"i do not care\",\n        \"idgadf\" : \"i do not give a damn fuck\",\n        \"idgaf\" : \"i do not give a fuck\",\n        \"idk\" : \"i do not know\",\n        \"ie\" : \"that is\",\n        \"i.e\" : \"that is\",\n        \"ifyp\" : \"i feel your pain\",\n        \"ig\" : \"instagram\",\n        \"iirc\" : \"if i remember correctly\",\n        \"ilu\" : \"i love you\",\n        \"ily\" : \"i love you\",\n        \"imho\" : \"in my humble opinion\",\n        \"imo\" : \"in my opinion\",\n        \"imu\" : \"i miss you\",\n        \"iow\" : \"in other words\",\n        \"irl\" : \"in real life\",\n        \"j4f\" : \"just for fun\",\n        \"jic\" : \"just in case\",\n        \"jk\" : \"just kidding\",\n        \"jsyk\" : \"just so you know\",\n        \"l8r\" : \"later\",\n        \"lb\" : \"pound\",\n        \"lbs\" : \"pounds\",\n        \"ldr\" : \"long distance relationship\",\n        \"lmao\" : \"laugh my ass off\",\n        \"lmfao\" : \"laugh my fucking ass off\",\n        \"lol\" : \"laughing out loud\",\n        \"ltd\" : \"limited\",\n        \"ltns\" : \"long time no see\",\n        \"m8\" : \"mate\",\n        \"mf\" : \"motherfucker\",\n        \"mfs\" : \"motherfuckers\",\n        \"mfw\" : \"my face when\",\n        \"mofo\" : \"motherfucker\",\n        \"mph\" : \"miles per hour\",\n        \"mr\" : \"mister\",\n        \"mrw\" : \"my reaction when\",\n        \"ms\" : \"miss\",\n        \"mte\" : \"my thoughts exactly\",\n        \"nagi\" : \"not a good idea\",\n        \"nbc\" : \"national broadcasting company\",\n        \"nbd\" : \"not big deal\",\n        \"nfs\" : \"not for sale\",\n        \"ngl\" : \"not going to lie\",\n        \"nhs\" : \"national health service\",\n        \"nrn\" : \"no reply necessary\",\n        \"nsfl\" : \"not safe for life\",\n        \"nsfw\" : \"not safe for work\",\n        \"nth\" : \"nice to have\",\n        \"nvr\" : \"never\",\n        \"nyc\" : \"new york city\",\n        \"oc\" : \"original content\",\n        \"og\" : \"original\",\n        \"ohp\" : \"overhead projector\",\n        \"oic\" : \"oh i see\",\n        \"omdb\" : \"over my dead body\",\n        \"omg\" : \"oh my god\",\n        \"omw\" : \"on my way\",\n        \"p.a\" : \"per annum\",\n        \"p.m\" : \"after midday\",\n        \"pm\" : \"prime minister\",\n        \"poc\" : \"people of color\",\n        \"pov\" : \"point of view\",\n        \"pp\" : \"pages\",\n        \"ppl\" : \"people\",\n        \"prw\" : \"parents are watching\",\n        \"ps\" : \"postscript\",\n        \"pt\" : \"point\",\n        \"ptb\" : \"please text back\",\n        \"pto\" : \"please turn over\",\n        \"qpsa\" : \"what happens\", \n        \"ratchet\" : \"rude\",\n        \"rbtl\" : \"read between the lines\",\n        \"rlrt\" : \"real life retweet\", \n        \"rofl\" : \"rolling on the floor laughing\",\n        \"roflol\" : \"rolling on the floor laughing out loud\",\n        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n        \"rt\" : \"retweet\",\n        \"ruok\" : \"are you ok\",\n        \"sfw\" : \"safe for work\",\n        \"sk8\" : \"skate\",\n        \"smh\" : \"shake my head\",\n        \"sq\" : \"square\",\n        \"srsly\" : \"seriously\", \n        \"ssdd\" : \"same stuff different day\",\n        \"tbh\" : \"to be honest\",\n        \"tbs\" : \"tablespooful\",\n        \"tbsp\" : \"tablespooful\",\n        \"tfw\" : \"that feeling when\",\n        \"thks\" : \"thank you\",\n        \"tho\" : \"though\",\n        \"thx\" : \"thank you\",\n        \"tia\" : \"thanks in advance\",\n        \"til\" : \"today i learned\",\n        \"tl;dr\" : \"too long i did not read\",\n        \"tldr\" : \"too long i did not read\",\n        \"tmb\" : \"tweet me back\",\n        \"tntl\" : \"trying not to laugh\",\n        \"ttyl\" : \"talk to you later\",\n        \"u\" : \"you\",\n        \"u2\" : \"you too\",\n        \"u4e\" : \"yours for ever\",\n        \"utc\" : \"coordinated universal time\",\n        \"w\/\" : \"with\",\n        \"w\/o\" : \"without\",\n        \"w8\" : \"wait\",\n        \"wassup\" : \"what is up\",\n        \"wb\" : \"welcome back\",\n        \"wtf\" : \"what the fuck\",\n        \"wtg\" : \"way to go\",\n        \"wtpa\" : \"where the party at\",\n        \"wuf\" : \"where are you from\",\n        \"wuzup\" : \"what is up\",\n        \"wywh\" : \"wish you were here\",\n        \"yd\" : \"yard\",\n        \"ygtr\" : \"you got that right\",\n        \"ynk\" : \"you never know\",\n        \"zzz\" : \"sleeping bored and tired\"\n    }\n\ndef decontract(text):\n    # remove special chars\n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n    # remove contractions\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let us\", text)\n    text = re.sub(r\"let's\", \"let us\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"we're\", \"we are\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"that\\x89\u00db\u00aas\", \"that is\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"wont\", \"will not\", text)\n    text = re.sub(r\"they're\", \"they are\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"cant\", \"cannot\", text)\n    text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n    text = re.sub(r\"wasn't\", \"was not\", text)\n    text = re.sub(r\"wasnt\", \"was not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"dont\", \"do not\", text)\n    text = re.sub(r\"don\u00e5\u00abt\", \"do not\", text)  \n    text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"didnt\", \"did not\", text)\n    text = re.sub(r\"aren't\", \"are not\", text)\n    text = re.sub(r\"isn't\", \"is not\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"haven't\", \"have not\", text)\n    text = re.sub(r\"hasn't\", \"has not\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n    text = re.sub(r\"you're\", \"you are\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aare\", \"you are\", text)\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"i\\x89\u00db\u00aam\", \"i am\", text)\n    text = re.sub(r\"shoulda\", \"should have\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"wouldn't\", \"would not\", text)\n    text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n    text = re.sub(r\"here's\", \"here is\", text)\n    text = re.sub(r\"here\\x89\u00db\u00aas\", \"here is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"you've\", \"you have\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n    text = re.sub(r\"youve\", \"you have\", text)\n    text = re.sub(r\"couldn't\", \"could not\", text)\n    text = re.sub(r\"we've\", \"we have\", text)\n    text = re.sub(r\"doesn't\", \"does not\", text)\n    text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"i've\", \"i have\", text)\n    text = re.sub(r\"i\\x89\u00db\u00aave\", \"i have\", text)\n    text = re.sub(r\"y'all\", \"you all\", text)\n    text = re.sub(r\"would've\", \"would have\", text)\n    text = re.sub(r\"it'll\", \"it will\", text)\n    text = re.sub(r\"we'll\", \"we will\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"weren't\", \"were not\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"they'll\", \"they will\", text)\n    text = re.sub(r\"they'd\", \"they would\", text)\n    text = re.sub(r\"they've\", \"they have\", text)\n    text = re.sub(r\"i'd\", \"i would\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", text)\n    text = re.sub(r\"should've\", \"should have\", text)\n    text = re.sub(r\"we'd\", \"we would\", text)\n    text = re.sub(r\"i'll\", \"i will\", text)\n    text = re.sub(r\"^ill$\", \"i will\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", text)    \n    text = re.sub(r\"ain't\", \"am not\", text)    \n    text = re.sub(r\"you'd\", \"you would\", text)\n    text = re.sub(r\"could've\", \"could have\", text)\n    text = re.sub(r\"m\u00cc\u00bcsica\", \"music\", text)\n    text = re.sub(r\"some1\", \"someone\", text)\n    text = re.sub(r\"yrs\", \"years\", text)\n    text = re.sub(r\"hrs\", \"hours\", text)\n    text = re.sub(r\"2morow|2moro\", \"tomorrow\", text)\n    text = re.sub(r\"2day\", \"today\", text)\n    text = re.sub(r\"4got|4gotten\", \"forget\", text)\n    text = re.sub(r\"b-day|bday\", \"b-day\", text)\n    text = re.sub(r\"mother's\", \"mother\", text)\n    text = re.sub(r\"mom's\", \"mom\", text)\n    text = re.sub(r\"dad's\", \"dad\", text)\n    text = re.sub(r\"^[h|a]+$\", \"haha\", text)\n    text = re.sub(r\"lmao|lolz|rofl\", \"lol\", text)\n    text = re.sub(r\"thanx|thnx|thx\", \"thanks\", text)\n    text = re.sub(r'all[l]+', \"all\", text)\n    text = re.sub(r'so[o]+', \"so\", text)\n    text = re.sub(r'a[w]+', \"awww\", text)\n    text = re.sub(r'why[y]+', \"why\", text)\n    text = re.sub(r'way[y]+', \"way\", text)\n    text = re.sub(r'will[l]+', \"will\", text)\n    text = re.sub(r'oo[o]+h', \"ooh\", text)\n    text = re.sub(r'hey[y]+', \"hey\", text)\n    text = re.sub(r\"boo[o]+m\", \"boom\", text)\n    text = re.sub(r\"co[o]+ld\", \"cold\", text)\n    text = re.sub(r\"goo[o]+d\", \"good\", text)\n    text = re.sub(r\"luckigrrl\", \"lucky girl\", text)\n    text = re.sub(r\"evolvin\", \"evolving\", text)\n\n    # specific\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"@\", \"\" , text)         # removal of @\n    text = re.sub(r\"http\\S+\", \"\", text)   # removal of URLs\n    text = re.sub(r\"#\", \"\", text)          # hashtag processing\n\n    # general\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n\n    # deal with some abbreviations\n    words = text.split()\n    text = ' '.join([abbreviations[word] if word in abbreviations.keys() else word.strip(string.punctuation) for word in words])\n\n    # character entity references\n    text = re.sub(r\"&gt;\", \">\", text)\n    text = re.sub(r\"&lt;\", \"<\", text)\n    text = re.sub(r\"&amp;\", \"&\", text)\n\n    # typos, slang and informal abbreviations\n    text = re.sub(r\"w\/e\", \"whatever\", text)\n    text = re.sub(r\"usagov\", \"usa government\", text)\n    text = re.sub(r\"<3\", \"love\", text)\n    text = re.sub(r\"trfc\", \"traffic\", text)\n    # remove urls\n    # text = re.sub(r\"http\\S+\", \"\", text)\n\n    # remove mentions\n    text = re.sub(r'^@[0-9a-zA-Z_]+', \"\", text)\n\n    # words with punctuations and special characters\n    for punc in string.punctuation:\n        text = text.replace(punc, '')\n\n    # ... and ..\n    text = text.replace('...', ' ... ')\n    if '...' not in text:\n        text = text.replace('..', ' ... ')\n\n    return text\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stop_words])\n\ndef remove_repeating_char(text):\n    return re.sub(r'(.)\\1+', r'\\1', text)\n\ndef remove_email(text):\n    return re.sub('@[^\\s]+', ' ', text)\n\ndef remove_URLs(text):\n    return re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))',' ',text)\n\ndef remove_numbers(text):\n    return re.sub('[0-9]+', '', text)\n\ndef clean_tweets(text):\n    text = decontract(text)\n    text = remove_stopwords(text)\n#     text = remove_repeating_char(text)\n    text = remove_email(text)\n    text = remove_URLs(text)\n    # text = remove_numbers(text)\n    return text\nprint(\"functions created!\")\n\ninit_rowcount = len(df)\ndf['tokens'] = df.tweet.apply(lambda tweet: clean_tweets(tweet))\ndf['length'] = df.tokens.str.len()\ndf = df[df.length != 0]\ncurr_rowcount = len(df)\navg_tweet_length = df.length.mode().iat[0]\nmin_tweet_length = df.length.min()\nprint(f\"functions applied!\\n\\n{init_rowcount - curr_rowcount} rows with zero-length tweets dropped!\\nMin tweet length observed: {min_tweet_length} words\\nAvg tweet length observed: {avg_tweet_length} words\")\n","54b00544":"df = df[['sentiment', 'tokens']]\ndf.rename(columns={'tokens': 'tweet'}, inplace=True)\ndf = pd.concat([df[df.sentiment == 0][:200000], df[df.sentiment == 1][:200000]])\ndf.to_csv(\".\/noemoticon-cleaned.csv\")\n\nword_count(df, \"After data cleansing\")\n","a5f8cde3":"word_count(df[df.sentiment == 1], \"POSITIVE tweets\")\nprint(\"\")\nword_count(df[df.sentiment == 0], \"NEGATIVE tweets\")\n","7dd52031":"tweet_source = pd.read_csv(\".\/noemoticon-cleaned.csv\")\nprint(f\"{len(tweet_source)} rows loaded to memory!\\nColumns: {tweet_source.columns}\\n\")\ntweet_source.reset_index()\ntweet_source = tweet_source.sample(frac=1.)\nprint(f\"{len(tweet_source)} rows sampled!\\nColumns: {tweet_source.columns}\")\n","c599ce65":"train_set, test_set = train_test_split(tweet_source, test_size=0.2, random_state=7)\nprint(f\"Train Set:\\nSize: {train_set.shape}\\nTest Set:\\nSize: {test_set.shape}\")\n","bf27f084":"!pip install -q scikit-fuzzy\n!pip install -q vaderSentiment\n!pip install -q NRCLex\n!pip install -q afinn","5ee92f94":"import nltk\nnltk.download('punkt')","16368c49":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nfrom nrclex import NRCLex\nfrom afinn import Afinn\n\nafinn = Afinn()\nvader = SentimentIntensityAnalyzer()\n\ndef getValenceAndEmotionsScore(text):\n    valence_scores = vader.polarity_scores(text)\n    emotion = NRCLex(text)\n    emotion_scores = emotion.affect_frequencies\n    afinn_score = afinn.score(text)\n    all_scores = dict(valence_scores, **emotion_scores)\n    all_scores['afinn_score'] = afinn_score\n    return all_scores\n    \ndef covertToLexiconScores(df):\n    polarity = []\n    labels = []\n    for tweet in df[df.sentiment == 1].tweet:\n        score = getValenceAndEmotionsScore(tweet)\n        polarity.append(score)\n        labels.append(1)\n    for tweet in df[df.sentiment == 0].tweet:\n        score = getValenceAndEmotionsScore(tweet)\n        polarity.append(score)\n        labels.append(0)\n    return pd.DataFrame(polarity), np.array(labels)\n\ntrain_tweets, train_labels = covertToLexiconScores(train_set)\ntest_tweets, test_labels = covertToLexiconScores(test_set)\nprint(f\"Train dataset:\\nShape: {train_tweets.shape}\\n\\nTest dataset:\\nShape: {test_tweets.shape}\")\n","bbf1b2bb":"import re\nimport skfuzzy as fuzz\nimport time\n\nclass lnfsnet(object):\n    def __init__(self):\n        # 1. Define valence polarity score ranges\n        self.posx = np.linspace(0, 1, 11)\n        self.negx = np.linspace(0, 1, 11)\n        self.outcomex = np.linspace(0, 10, 11)\n\n        # 2. Define MFs (Gauss2MF: Parameter: Mean1\/Centroid1, Sigma1, Mean2\/Centroid2, Sigma2)\n        self.pos_low_mf  = fuzz.gauss2mf(self.posx, 0,    0.2, 0.2,  0.2)\n        self.pos_med_mf  = fuzz.gauss2mf(self.posx, 0.45, 0.2, 0.45, 0.2)\n        self.pos_high_mf = fuzz.gauss2mf(self.posx, 0.7,  0.2, 1,    0.2)\n        self.neg_low_mf  = fuzz.gauss2mf(self.negx, 0,    0.2, 0.2,  0.2)\n        self.neg_med_mf  = fuzz.gauss2mf(self.negx, 0.45, 0.2, 0.45, 0.2)\n        self.neg_high_mf = fuzz.gauss2mf(self.negx, 0.7,  0.2, 1,    0.2)\n        # 2-class classification - Scale : Neg Pos\n        self.outcome_neg_mf = fuzz.gauss2mf(self.outcomex, 0, 2, 5,  2)\n        self.outcome_pos_mf = fuzz.gauss2mf(self.outcomex, 5, 2, 10, 2)\n\n        # 3. Plot MFs\n        self.plot_mfs()\n\n    def plot_mfs(self):\n        fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, figsize=(8, 9))\n\n        ax0.plot(self.posx, self.pos_low_mf, 'b', linewidth=1.5, label='Low')\n        ax0.plot(self.posx, self.pos_med_mf, 'g', linewidth=1.5, label='Medium')\n        ax0.plot(self.posx, self.pos_high_mf, 'r', linewidth=1.5, label='High')\n        ax0.set_title('Membership plot for linguistic variable = positive valence polarity', fontdict={\"fontsize\": 20})\n        ax0.legend()\n\n        ax1.plot(self.negx, self.neg_low_mf, 'b', linewidth=1.5, label='Low')\n        ax1.plot(self.negx, self.neg_med_mf, 'g', linewidth=1.5, label='Medium')\n        ax1.plot(self.negx, self.neg_high_mf, 'r', linewidth=1.5, label='High')\n        ax1.set_title('Membership plot for linguistic variable = negative valence polarity', fontdict={\"fontsize\": 20})\n        ax1.legend()\n\n        ax2.plot(self.outcomex, self.outcome_pos_mf, 'b', linewidth=1.5, label='Negative')\n        ax2.plot(self.outcomex, self.outcome_neg_mf, 'r', linewidth=1.5, label='Positive')\n        ax2.set_title('Membership plot for sentiment outcome \/ consequent', fontdict={\"fontsize\": 20})\n        ax2.legend()\n\n        fig.tight_layout(pad=2.0)\n        plt.show()\n\n    def normalize_scores(self, df):\n        def clean_score(score):\n            return 0.9 if score >= 1 else round(score, 1)\n\n        def compute_pos(row):\n            trust        = row['trust']\n            surprise     = row['surprise']\n            positive     = row['positive']\n            negative     = row['negative']\n            joy          = row['joy']\n            pos          = row['pos']\n            neu          = row['neu']\n            neg          = row['neg']\n            afinn_score  = row['afinn_score']\n            compound     = row['compound']\n            improved_pos = clean_score(max(positive, pos, (neu if positive > negative else 0), trust, surprise, joy))\n            improved_pos = improved_pos if abs(improved_pos - neg) > 0.2 else (clean_score(max(afinn_score, compound)) if afinn_score > 0 and compound > 0 else improved_pos)\n            return improved_pos\n\n        def compute_neg(row):\n            fear         = row['fear']\n            anger        = row['anger']\n            positive     = row['positive']\n            negative     = row['negative']\n            sadness      = row['sadness']\n            disgust      = row['disgust']\n            neg          = row['neg']\n            neu          = row['neu']\n            pos          = row['pos']\n            afinn_score  = row['afinn_score']\n            compound     = row['compound']\n            improved_neg = clean_score(max(negative, neg, (neu if positive < negative else 0), fear, anger, sadness, disgust))\n            improved_neg = improved_neg if abs(improved_neg - pos) > 0.2 else (clean_score(max(afinn_score*-1, compound*-1)) if afinn_score < 0 and compound < 0 else improved_neg)\n            return improved_neg\n        \n        df['pos'] = df.apply(lambda row: compute_pos(row), axis=1)\n        df['neg'] = df.apply(lambda row: compute_neg(row), axis=1)\n        return df[['pos', 'neg']]\n\n    def fuzzify(self, df):\n        df['pos_low_mf_score']  = df.pos.apply(lambda score: fuzz.interp_membership(self.posx, self.pos_low_mf,  score))\n        df['pos_med_mf_score']  = df.pos.apply(lambda score: fuzz.interp_membership(self.posx, self.pos_med_mf,  score))\n        df['pos_high_mf_score'] = df.pos.apply(lambda score: fuzz.interp_membership(self.posx, self.pos_high_mf, score))\n        df['neg_low_mf_score']  = df.neg.apply(lambda score: fuzz.interp_membership(self.negx, self.neg_low_mf,  score))\n        df['neg_med_mf_score']  = df.neg.apply(lambda score: fuzz.interp_membership(self.negx, self.neg_med_mf,  score))\n        df['neg_high_mf_score'] = df.neg.apply(lambda score: fuzz.interp_membership(self.negx, self.neg_high_mf, score))\n        return df[['pos_low_mf_score', 'pos_med_mf_score', 'pos_high_mf_score', 'neg_low_mf_score', 'neg_med_mf_score', 'neg_high_mf_score']]\n\n    def apply_rules(self, df):\n        def find_significant_rule(row):\n            pos_low_mf_score = row['pos_low_mf_score']\n            pos_med_mf_score = row['pos_med_mf_score']\n            pos_high_mf_score = row['pos_high_mf_score']\n            neg_low_mf_score = row['neg_low_mf_score']\n            neg_med_mf_score = row['neg_med_mf_score']\n            neg_high_mf_score = row['neg_high_mf_score']\n\n            active_rule1 = np.fmin(pos_low_mf_score,  neg_med_mf_score)\n            active_rule2 = np.fmin(pos_low_mf_score,  neg_high_mf_score)\n            active_rule3 = np.fmin(pos_med_mf_score,  neg_high_mf_score)\n\n            active_rule4 = np.fmin(pos_low_mf_score,  neg_low_mf_score)\n            active_rule5 = np.fmin(pos_med_mf_score,  neg_med_mf_score)\n            active_rule6 = np.fmin(pos_high_mf_score, neg_high_mf_score)\n\n            active_rule7 = np.fmin(pos_med_mf_score,  neg_low_mf_score)\n            active_rule8 = np.fmin(pos_high_mf_score, neg_low_mf_score)\n            active_rule9 = np.fmin(pos_high_mf_score, neg_med_mf_score)\n\n            # 2-class classification\n            neg_firing_strength = np.fmax(np.fmax(active_rule1, active_rule2), active_rule3)\n            outcome_neg_consequent = np.fmin(neg_firing_strength, self.outcome_neg_mf)\n            \n            neu_firing_strength = np.fmax(np.fmax(active_rule4, active_rule5), active_rule6)\n            pos_firing_strength = np.fmax(np.fmax(active_rule7, active_rule8), active_rule9)\n            non_neg_firing_strength = np.fmax(neu_firing_strength, pos_firing_strength)\n            outcome_pos_consequent = np.fmin(non_neg_firing_strength, self.outcome_pos_mf)\n            \n            # Aggregate all three output membership functions together\n            consequent = np.fmax(outcome_neg_consequent, outcome_pos_consequent)\n            return consequent\n\n        df['fuzzy_pred'] = df.apply(lambda row: find_significant_rule(row), axis=1)\n        return df[['fuzzy_pred']]\n\n    def defuzzify(self, df):\n        def compute_crisp(fuzzy_pred):\n            try:\n              output = round(fuzz.defuzz(self.outcomex, fuzzy_pred, 'centroid'), 2)\n              return 0 if output < 5 else 1\n            except:\n              return 1\n\n        df['output'] = df.fuzzy_pred.apply(lambda fuzzy_pred: compute_crisp(fuzzy_pred))\n        return df[['output']]\n\n    def fit_and_predict(self, df):\n        y = self.normalize_scores(df)\n        y = self.fuzzify(y)\n        y = self.apply_rules(y)\n        y = self.defuzzify(y)\n        return y\n","28fd9515":"start = time.time()\n\nnet = lnfsnet()\ntrain_pred = net.fit_and_predict(train_tweets)\ntest_pred = net.fit_and_predict(test_tweets)\n\nprint(f\"Execution time: {int((time.time() - start)\/60):>3d} min(s) {((time.time() - start)%60):>4.2f} sec(s)\")","816b90b5":"actuals = train_labels.squeeze()\npredicted = []\nfor pred in train_pred.squeeze():\n    if pred > 0.5:\n        predicted.append(1)\n    else:\n        predicted.append(0)\nf1 = f1_score(actuals, predicted)\nacc = accuracy_score(actuals, predicted)\nprint(f\"Training:\\nf1-score: {f1:>6.4f}\\naccuracy: {acc:>6.4f}\")\n","e2a18d9e":"actuals = test_labels.squeeze()\npredicted = []\nfor pred in test_pred.squeeze():\n    if pred > 0.5:\n        predicted.append(1)\n    else:\n        predicted.append(0)\nf1 = f1_score(actuals, predicted)\nacc = accuracy_score(actuals, predicted)\nprint(f\"Test:\\nf1-score: {f1:>6.4f}\\naccuracy: {acc:>6.4f}\\n\\n\")\n\ncm = confusion_matrix(actuals, predicted)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[\"positive\\nsentiment\", \"negative\\nsentiment\"])\ndisp.plot()\nplt.title(\"Confusion Matrix\", fontdict={\"fontsize\": 20})\nplt.show()","62d90753":"# GRU Hyper params\nepoch_num = 70\nbatch_size = 128\nlr = .001\ninput_dim = 50\noutput_dim = 1\ngru_num_layers = 1","818a97d5":"punctuations = set(string.punctuation)\nvocab_size = 0\nword_to_idx = None\ntweets = tweet_source.tweet.values\nall_tokens = ['<pad>']\nfor tweet in tweets:\n    tokens = str(tweet).split()\n    for token in tokens:\n        if not token in stop_words and not token in punctuations:\n            all_tokens.append(token)\ndiff_tokens = set(all_tokens)\nvocab_size = len(diff_tokens)\nword_to_idx = {word:idx for idx, word in enumerate(diff_tokens)}\nall_values = word_to_idx.values()\nprint(f\"Vocab generated!\\nSize: {vocab_size}\\n\\nWord index map generated!\\n# of keys: {len(word_to_idx.keys())}\\nMin index: {min(all_values)}\\nMax index: {max(all_values)}\")\n","e36aa293":"def covertTweetToTokenids(df):\n    tweets = df.tweet.values\n    token_list = []\n    for tweet in tweets:\n        tokens = str(tweet).split()\n        token_ids = []\n        for token in tokens:\n            if not token in stop_words and not token in punctuations:\n                token_ids.append(word_to_idx[token])\n        token_list.append(torch.LongTensor(token_ids))\n    return token_list","a05e8c15":"train_tweets = covertTweetToTokenids(train_set)\ntrain_labels = train_set.sentiment.values\n\n# get tokens\ntest_tweets = covertTweetToTokenids(test_set)\ntest_labels = test_set.sentiment.values\n\n# pad\ntest_length = [len(data) for data in test_tweets]\ntest_data = rnn_utils.pad_sequence(test_tweets, batch_first=True, padding_value=0)\n\nprint(f\"Train Set:\\nSize: {len(train_tweets)}\\nTest Set:\\nSize: {len(test_tweets)}\")\n","6aaeb6a2":"class TweetDataset(Dataset):\n    def __init__(self, train_x, train_y):\n        self.train_x = train_x\n        self.train_y = train_y\n    def __len__(self):\n        return len(self.train_y)\n    def __getitem__(self, item):\n        return self.train_x[item - 1], self.train_y[item - 1]\n\ndef collate_fn(train_data):\n    (train_data, train_label) = zip(*train_data)\n    data_length = [len(data) for data in train_data]\n    train_data = rnn_utils.pad_sequence(train_data, batch_first=True, padding_value=0)\n    train_label = torch.Tensor(train_label)\n    return train_data, train_label, data_length","d2b701bf":"train_data = TweetDataset(train_tweets,\n                          train_labels)\ntrain_dataloader = DataLoader(train_data,\n                              batch_size=batch_size,\n                              collate_fn=collate_fn)","6ecbd2fe":"class TweetNet(torch.nn.Module):\n    def __init__(self, input_dim, output_dim, vocab_size, gru_num_layers=1, bidirectional=False, dropout=.2, hidden_layers = [128, 64, 128]):\n        super(TweetNet, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = input_dim \/\/ 2 if bidirectional else input_dim\n        self.output_dim = output_dim\n        self.gru_num_layers = gru_num_layers\n        # Embedding\n        if not vocab_size == 0:\n            self.embed = torch.nn.Embedding(vocab_size, input_dim)\n        # GRUs\n        self.gru_layer = torch.nn.GRU(\n            input_size=self.input_dim, \n            hidden_size=self.hidden_dim, \n            num_layers=self.gru_num_layers, \n            bidirectional=bidirectional, \n            batch_first=True\n        )\n        # The FFN to adjust the outputs\n        if hidden_layers and not len(hidden_layers) == 0:\n            # the dim is not changed through the two GRU layer\n            hidden_list = [torch.nn.Linear(self.input_dim, hidden_layers[0])]\n            for idx in range(len(hidden_layers) - 1):\n                hidden_list.append(torch.nn.Linear(hidden_layers[idx], hidden_layers[idx + 1]))\n            self.hidden_layer_list = torch.nn.ModuleList(hidden_list)\n            self.hidden_out_dim = hidden_layers[-1]\n        else:\n            self.hidden_layer_list = []\n            self.hidden_out_dim = self.input_dim\n        # Output layer\n        self.output = torch.nn.Linear(self.hidden_out_dim, self.output_dim)\n        # Other functions\n        self.activate = torch.nn.ReLU()\n        self.dropout = torch.nn.Dropout(dropout)\n    \n    def forward(self, x, x_len, pretrained_embed=False):\n        if not pretrained_embed:\n            x = self.embed(x)\n        # pack padded seq\n        x = rnn_utils.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n        # GRU layer\n        output, hidden_info = self.gru_layer(x)\n        # pad packed seq\n        output, length = rnn_utils.pad_packed_sequence(output, batch_first=True)\n        # gather the output from the last unpad token\n        fin_cell_outputs = []\n        for idx in range(len(length)):\n            fin_cell_outputs.append(output[idx][length[idx]-1])\n        # stack the output\n        output = torch.stack(fin_cell_outputs)\n        # ffn process\n        for layer in self.hidden_layer_list:\n            output = layer(output)\n            output = self.activate(output)\n            output = self.dropout(output)\n        # output layer, get logits\n        output = self.output(output)\n        if self.output_dim == 1:\n            output = torch.sigmoid(output)\n        else:\n            output = torch.softmax(output, dim=1)\n        return output","bec660bf":"net = TweetNet(input_dim=input_dim, output_dim=output_dim, vocab_size=vocab_size, gru_num_layers=gru_num_layers)\nnet = net.to(device)\n\noptimiser = torch.optim.Adam(net.parameters(), lr=lr)\nloss_func = torch.nn.BCELoss()\n\nnet.train()\n","19568680":"bad_records = 0\nprint(\"Mini batch epoch training started....\\n\\n\")\nprint(\"Epoch   | Total loss | Execution time\")\nprint(\"--------+------------+--------------------------\")\nstart_time = time.time()\nfor epoch in range(epoch_num):\n    epoch_loss = 0\n    for data, label, length in train_dataloader:\n        try:\n            input_vec = data.to(device)\n            label = label.to(device)\n            pred = net(input_vec, length)\n            loss = loss_func(pred.squeeze(), label.squeeze())\n            optimiser.zero_grad()\n            loss.backward()\n            optimiser.step()\n            epoch_loss += loss.cpu().data.numpy()\n        except:\n            bad_records += 1\n    if (epoch + 1) % 5 == 0 or (epoch + 1) == epoch_num:\n        print(f\"{epoch + 1:>5d}   | {epoch_loss:>10.4f} | {int((time.time() - start_time)\/60):>3d} min(s) {((time.time() - start_time)%60):>4.2f} sec(s)\")\n        start_time = time.time()\n\nprint(\"--------+------------+--------------------------\")\nprint(f\"{bad_records} bad records skipped!!!\")","3edfe2a7":"net.eval()\nnet.to('cpu')\n\n# test\nexec_start_time = time.time()\nif len(test_data) <= 50000:\n    print(\"Running batch test execution...\")\n    test_pred = net(test_data, test_length)\nelse:\n    print(\"High test data volume observed.\\nRunning mini batch test execution...\")\n    chunks = [test_data[x:x+50000] for x in range(0, len(test_data), 50000)]\n    chunks_len = [test_length[x:x+50000] for x in range(0, len(test_length), 50000)]\n\n    chunk_pred = []\n    for idx, chunk_len in enumerate(chunks_len):\n        chunk = chunks[idx]\n        chunk_pred.append(net(chunk, chunk_len))\n    test_pred = torch.cat(chunk_pred)\n\n    if len(test_data) != len(test_pred):\n        print(f\"{len(test_data) - len(test_pred)} records skipped!\")\nprint(f\"Execution time: {int((time.time() - exec_start_time)\/60):>3d} min(s) {((time.time() - exec_start_time)%60):>4.2f} sec(s)\")\n","424240a8":"pactuals = test_labels.squeeze()\npredicted = []\nfor pred in test_pred.squeeze():\n    if pred > 0.5:\n        predicted.append(1)\n    else:\n        predicted.append(0)\nf1 = f1_score(actuals, predicted)\nacc = accuracy_score(actuals, predicted)\nprint(f\"f1-score: {f1:>6.4f}\\naccuracy: {acc:>6.4f}\\n\\n\")\n\ncm = confusion_matrix(actuals, predicted)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=[\"positive\\nsentiment\", \"negative\\nsentiment\"])\ndisp.plot()\nplt.title(\"Confusion Matrix\", fontdict={\"fontsize\": 20})\nplt.show()","1f7aba2a":"<div class=\"alert alert-block alert-success\">\n<h2><center><strong>Data Preparation <\/strong><\/center><\/h2>\n\n\n---\n\n\n<center><img src=\"https:\/\/developer.qualcomm.com\/sites\/default\/files\/attachments\/learning_resources_01-01.png\" width=\"400\" height=\"200\"><\/center>\n\n---\n\n  <p>The Sentiment140 dataset, unlike others is a semi-structured dataset where although it contains 5 columns (id, data, flag, user, tweet), the only fields that are useful for our analysis is primarily tweet. Hence the dimensionality of this dataset cannot be easily determined. This raw data comprises of sentences which ranges between 1 and 250 characters resulting in an average of 30 word tokens.<\/p>\n  <p>The following set of steps are performed as part of data preparation:<\/p>\n  <ol>\n    <li>Load Dataset<\/li>\n    <li>Normalize target labels - <i>Map value 4 as 1 (i.e. Positive) and 0 as 0 (i.e. Negative)<\/i><\/li>\n    <li>Remove duplicates - <i>duplicates identified by a combination of user, tweet and sentiment label<\/i><\/li>\n    <li>Remove special and non ASCII characters<\/li>\n    <li>Replace contractions with complete phrases <i>(e.g.: \"I'll\" with \"I will\")<\/i><\/li>\n    <li>Expand Abbreviations and other urban lingos<\/li>\n    <li>Remove stopwords - <i>Use NLTK library's English stopword dictionary<\/i><\/li>\n    <li>Remove email addresses, URLs, twitter user handles, etc.<\/li>\n  <\/ol>\n  <p>Once the data is cleaned then we remove any records that ends up with a zero-length tweet.<\/p>\n\n---\n\n<\/div>","a12eef08":"<div class=\"alert alert-block alert-success\">  \n<h2><center><strong>Model Analysis & Benchmarking<\/strong><\/center><\/h2>\n\n---\n\n<h3><strong>Scoring & Metrics<\/strong><\/h3>\n  <p>The metrics used for measuring the performance of both models are:<\/p>\n  <ul>\n    <li>F1 score<\/li>\n    <li>Accuracy<\/li>\n  <\/ul>\n\n---\n<p>Below scores are captured from running both models for 500k records in a train-test split of 80-20.<\/p>\n\n|       | Macro-F1 | Accuracy |\n|:-----:|----------|:--------:|\n| L-NFS | 0.7021   | 0.6266   |\n|   GRU | 0.4984   | 0.4995   |\n\n<p>Running the training and test on the complete data (1.6M tweets) as well as in a k-fold cross validation was not possible given the time and resource constraint. I tried running the models on both local (CPU only) and Google Colab (GPU) but given the resource limitation, both executions were either timing out or running out of memory resulting in recurrent kernal restarts.<\/p>\n\n---\n\n<h3><strong>SOTA<\/strong><\/h3>\n  <p>Although no state-of-the-art (SOTA) is established for this problem statement, a study published by <i>Ribeiro, F.N., Ara\u00fajo, M., Gon\u00e7alves, P. et al. 2016<\/i> shows that Vader lexicon as well as Sentiment140 lexicon both give a Macro-F1 and Accuracy score in the range of 0.5 to 0.6 for a 3-class sentiment classification and even lower for a 2-class sentiment classification.<\/p>\n\n\n  ---\n\n  <h3><strong>Reference<\/strong><\/h3>\n  <ul>\n  <li>Ribeiro, F.N., Ara\u00fajo, M., Gon\u00e7alves, P. et al. <b>SentiBench - a benchmark comparison of state-of-the-practice sentiment analysis methods.<\/b> EPJ Data Sci. 5, 23 (2016). <code><a href=\"url\">https:\/\/doi.org\/10.1140\/epjds\/s13688-016-0085-1<\/a><\/code><\/li>\n  <\/ul>\n<\/div>","60bf7e52":"<div class=\"alert alert-block alert-success\">  \n<h2><center><strong>AI Model Implementation: Linguistic Neuro Fuzzy System (L-NFS) <\/strong><\/center><\/h2>\n\n---\n\n<center><img src=\"https:\/\/www.edureka.co\/blog\/wp-content\/uploads\/2019\/12\/Picture2.png\" width=\"400\" height=\"200\"><\/center>\n\n---\n\n<h3><strong>Architecture<\/strong><\/h3>\n  <p>The Linguistic Neuro Fuzzy System (L-NFS) Model is a custom model with 4 layers<\/p>\n  <ul>\n    <li>Input layer<\/li>\n    <li>Input fuzzification layer<\/li>\n    <li>Rule Inference layer<\/li>\n    <li>Defuzzification \/ output layer<\/li>\n  <\/ul>\n\n\n<h4><strong>Detailed Description<\/strong><\/h4>\n  <p>The cleaned tweet field is taken and convert into polarity scores, like valence scores, afinn score, NRCLex emotion scores. The <b>VaderSentiment<\/b>, NLTK's <b>NRCLex<\/b> and <b>AFINN<\/b> lexicon packages are used for these scores conversion. The polarity scores are then combined together to generate a consolidated pos and neg polarity scores using if-then rules and rounded to 1 decimal place and is then passed through a fuzzification layer. Guassian Membership functions <code>skfuzzy.gauss2mf()<\/code> are used to fuzzify the inputs to generate 3 linguistic variables (low, medium, high) each for each input respectively. The <code>skfuzzy.gauss2mf()<\/code> allows us to define MFs that allows us to define a tail due to 2 centroids \/ mean and sigma params, thus ensuring coverage throughout the universe of discouse of both the pos and neg linguistic variables.<\/p>\n\n  <p>9 set of fuzzy if-then rules are generated (3 x 3 linguistic variables) using the <b>Mamdani rules system<\/b>of the following form:<\/p>\n\n<center>\n  <div class=\"alert alert-block alert-warning\">  \n  Rule $R_i \\rightarrow$ IF $A$ is $A_1^i$ and $B$ is $B_1^i$ THEN $C$ is $C_1^i \\forall i$\n  <\/div>\n\n<\/center>\n\n<p>The resultant consequent is then passed through a defuzzification layer built on the <b>centroid<\/b> mode to generate the crisp outcome - $y_{pred}$ <\/p>\n\n---\n\n<\/div>","e628d664":"<div class=\"alert alert-block alert-info\">  \n<h1><center><strong>Sentiment Analysis using a Linguistic Neuro Fuzzy System (L-NFS)<\/strong><\/center><\/h1>\n\n---\n    \n<center><img src=\"https:\/\/d3caycb064h6u1.cloudfront.net\/wp-content\/uploads\/2021\/06\/sentimentanalysishotelgeneric-2048x803-1.jpg\" width=\"600\" height=\"320\"><\/center>\n\n---\n\n<h2><strong>Introduction<\/strong><\/h2>\n    <p><b>Sentiment Analysis<\/b> also called as opinion mining is a standard classification problem in the area of Natural Language processing. The idea behind this is to determine the polarity associated with the text as either positive or negative or sometime neutral (two-class or three-class classifications) for a specific topic. In this project, I plan to use the <i>Sentiment140 Twitter datasets<\/i> to learn and predict the sentiments of 1.6M tweets. A typical approach taken for such a problem relies on techniques in Deep Learning and Artificial Neural Networks. What I propose here is a more explainable and hybrid approach that delves into both Fuzzy Logic and Artificial Neural Networks. I then compare the results obtained from the said model with a classical NLP technique built using a linear embedding layer and GRU based RNN Cell.<p>\n\n---\n<h2><strong>Implementation Details<\/strong><\/h2>\n   <p>The entire exercise has 3 main tasks<\/p>\n   \n   - Data preparation \\& exploratory data analysis\n   - Implementing L-NFS AI model\n   - Implementing Benchmark (RNN) AI Model\n\n---\n\n<\/div>","c807d71b":"<div class=\"alert alert-block alert-success\">  \n<h2><center><strong>Benchmark Model Implementation: GRU <\/strong><\/center><\/h2>\n\n---\n\n<center><img src=\"https:\/\/blog.floydhub.com\/content\/images\/2019\/07\/image14.jpg\" width=\"400\" height=\"200\"><\/center>\n\n---\n\n<h3><strong>Architecture<\/strong><\/h3>\n  <p>The GRU based RNN model built here consists of 4 layers:<\/p>\n  <ul>\n    <li>Input layer<\/li>\n    <li>Embedding layer<\/li>\n    <li>GRU layer, followed by a ReLU activation<\/li>\n    <li>Dropout layer<\/li>\n    <li>Sigmoid output layer<\/li>\n  <\/ul>\n\n  <p>The raw input tweet vectors are first used to generate a vocabulary set which is then used to convert the word tokens into numeric token ids. This forms the input layer. The input is then right padded to ensure that the sentence vectors are all of the same size and then passed to a word embedding layer which vectorises the token ids. The word vectors are then passed to the GRU RNN-cell, followed by a non-linear ReLU activation. Lastly, the hidden GRU layer output is passed to a Dropout layer depending on the context vectors. Sigmoid activation is then performed on top of this output to get the binary labels - $y_{pred}$.<\/p>\n\n  <p>We run mini-batch SGD training for 70 epochs to train the model.<\/p>\n\n---\n\n<\/div>"}}