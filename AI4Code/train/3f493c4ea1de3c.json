{"cell_type":{"9040e9c9":"code","65a82059":"code","04cd8a3c":"code","7e480e50":"code","bcf57208":"code","afd65f8e":"code","93323e62":"code","47ad0045":"code","8d9b6ee0":"code","74ab8da5":"code","ccde576b":"code","e504b9f2":"code","7d334fd2":"markdown","a636d387":"markdown","e0ab229c":"markdown"},"source":{"9040e9c9":"import numpy as np\nimport pandas as pd\nimport re\nimport string\nimport pickle\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","65a82059":"# Preprocessing tools\n\nps = PorterStemmer()\nwnl = WordNetLemmatizer()\nstr_punc = string.punctuation\n\nengstopwords = stopwords.words(\"english\")\nengstopwordsV2 = re.sub('[' + re.escape(string.punctuation) + ']', '',\n                        ' '.join(engstopwords)).split()\n\nengstopwords = set(engstopwords).union(set(engstopwordsV2))","04cd8a3c":"# Function to lemmatize a word using the three types: adjective, verb, noun\ndef lemmatize_all_types(word):\n    word = wnl.lemmatize(word, 'a')\n    word = wnl.lemmatize(word, 'v')\n    word = wnl.lemmatize(word, 'n')\n    return word\n\n# Function to clean text\ndef clean(text):\n    # Remove URLs from text\n    text = re.sub(\"http.*?([ ]|\\|\\|\\||$)\", \"\", text).lower()\n    url_regex = r\"\"\"(?i)\\b((?:https?:(?:\/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b\/?(?!@)))\"\"\"\n    text = re.sub(url_regex, \"\", text)\n\n    # Remove specific punctuation (usually associated with a word)\n    text = re.sub(r'(:|;).', \" \", text)\n    \n    # Remove punctuations\n    text = re.sub('['+re.escape(str_punc)+']',\" \",  text)\n    \n    # Remove parantheses, brackets\n    text = re.sub('(\\[|\\()*\\d+(\\]|\\))*', ' ', text)\n    \n    # Remove string marks\n    text = re.sub('[\u2019\u2018\u201c\\.\u201d\u2026\u2013]', '', text)\n    text = re.sub('[^(\\w|\\s)]', '', text)\n    text = re.sub('(gt|lt)', '', text)\n    \n    #Check that each word is not stopword, and lemmatize it\n    text = list(map(lemmatize_all_types, text.split()))\n    text = [word for word in text if (word not in engstopwords)]\n    text = \" \".join(text)\n    return text\n\n# Convert personality type into a dominant cognitive function\ndef letters_to_functions(personality_type):\n    translator = {\n            'ENxP': 'Ne',\n            'INxJ': 'Ni',\n            'ESxP': 'Se',\n            'ISxJ': 'Si',\n            'ExTJ': 'Te',\n            'IxTP': 'Ti',\n            'IxFP': 'Fi',\n            'ExFJ': 'Fe',\n            \n            'xNxx': 'N',\n            'xSxx': 'S',\n            'xxTx' : 'T',\n            'xxFx': 'F',\n            'Ixxx':'I',\n            'Exxx':'E',\n            }\n    return translator[personality_type]\n\n# Convert a Cognitive Functions Stack into Personality Type\ndef functions_to_letters(functions):\n    translator = {\n            'NiTe': 'INTJ',\n            'NiFe': 'INFJ',\n            'NeTi': 'ENTP',\n            'NeFi': 'ENFP',\n            'SiTe': 'ISTJ',\n            'SiFe': 'ISFJ',\n            'SeTi': 'ESTP',\n            'SeFi': 'ESFP',\n            'TeNi': 'ENTJ',\n            'FeNi': 'ENFJ',\n            'TiNe': 'INTP',\n            'FiNe': 'INFP',\n            'TeSi': 'ESTJ',\n            'FeSi': 'ESFJ',\n            'TiSe': 'ISTP',\n            'FiSe': 'ISFP',\n            }\n    return translator[functions]\n","7e480e50":"# Cognitive Functions meta-information\ncf_info = {\n        \"Ni\": {\n                'name':'intuition',\n                'role':'input',\n                'direction':'internal'\n                },\n        \"Ne\": {\n                'name':'intuition',\n                'role':'input',\n                'direction':'external'\n                },\n        \"Si\": {\n                'name':'sensing',\n                'role':'input',\n                'direction':'internal'\n                },\n        \"Se\": {\n                'name':'sensing',\n                'role':'input',\n                'direction':'external'\n                },\n        'Ti': {\n                'name':'thinking',\n                'role':'output',\n                'direction':'internal'\n                },\n        'Te': {\n                'name':'thinking',\n                'role':'output',\n                'direction':'external'\n                },\n        'Fi': {\n                'name':'feeling',\n                'role':'output',\n                'direction':'internal',\n                },\n        'Fe': {\n                'name':'feeling',\n                'role':'output',\n                'direction':'external'\n                }\n        }\n\n# Dictionary of pretrained models\nmodels = {\n        # Phase 1 models\n        'NiNe':None,\n        'NiSi':None,\n        'NiSe':None,\n        'NeSi':None,\n        'NeSe':None,\n        'SiSe':None,\n        \n        'TiTe':None,\n        'TiFi':None,\n        'TiFe':None,\n        'TeFi':None,\n        'TeFe':None,\n        'FiFe':None,\n            \n        # Phase 2 models\n    \n        # Scenario 1 (detect dominant cognitive function)\n        'NiTe':None,\n        'NiFe':None,\n        'SiTe':None,\n        'SiFe':None,\n        \n        'NeFi':None,\n        'NeTi':None,\n        'SeFi':None,\n        'SeTi':None,\n        \n        # Secenario 2 (fixing phase 1 result corruption)\n        'NiTi':None,\n        'NiFi':None,\n        'NeTe':None,\n        'NeFe':None,\n        \n        'SiTi':None,\n        'SiFi':None,\n        'SeTe':None,\n        'SeFe':None,\n        }\n\n# Supporter pretrained models, increases accuracy ~3%\nsupporters = {\n        'NvS':None,\n        'FvT':None,\n        'IvE':None,\n        }","bcf57208":"# Read models\npath = \"..\/input\/mbti-pretrained-models\/\"\n\nfor key in models:\n    with open(path + key + '.pickle', 'rb') as file:\n        models[key] = pickle.load(file)\n\nfor _key in supporters:\n    with open(path + _key + '.pickle', 'rb') as file:\n        supporters[_key] = pickle.load(file)","afd65f8e":"# Convert the direction of a cognitive function (i => e | e => i)\ndef flip_cf_direction(cognitiveFunction):\n    direction = cognitiveFunction[1]\n    new_direction = 'i' if direction == 'e' else 'e'\n    return cognitiveFunction[0] + new_direction\n\n# Pipeline for using a pre-trained model to predict sample\ndef process_classify_sample(modelObject, sample):\n    vectorizer = modelObject['cv']\n    label_encoder = modelObject['labelEncoder']\n    model = modelObject['model']\n    \n    # Preprocessing\n    clean_sample = clean(sample)\n    x = vectorizer.transform([clean_sample]).toarray()\n    \n    # Classification\n    y = model.predict(x)\n    y_probability = max(model.predict_proba(x)[0])\n    classified_cf = label_encoder.inverse_transform(y)[0]\n    return letters_to_functions(classified_cf), y_probability\n\n    \n# Phase 1 of classifying a personality type\n# Recognize the top perceiving cognitive function\n# and the top judging cognitive function\ndef phase1(sample):\n    # INPUT => sample: string\n    # OUTPUT => input_cf_acc, output_cf_acc: Series \n\n    # Keep track of every input (perceiving) cognitive function likelihood\n    input_cf_acc = pd.Series({\"Ni\":0, \"Ne\":0, \"Si\":0, \"Se\":0}, dtype=float)\n    # same for output (judging) cognitive functions\n    output_cf_acc = pd.Series({\"Ti\":0, \"Te\":0, \"Fi\":0, \"Fe\":0}, dtype=float)\n    \n    input_cf = np.array(['Ni', 'Ne', 'Si', 'Se'])\n    output_cf = np.array(['Ti', 'Te', 'Fi', 'Fe'])\n    \n    # nested loop for input (perceiving) cognitive functions\n    # so models are: NiNe, NiSi, NiSe,  ...\n    for i in range(3):\n        for j in range(i+1, 4):\n            model_name = input_cf[i] + input_cf[j]\n            modelObject = models[model_name]\n            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n            \n            # Incrase likelihood of the prediction class\n            input_cf_acc[cognitive_fn] += probability\n            other_cf = input_cf[i] if input_cf[j] == cognitive_fn else input_cf[j]\n            \n            # Increase likelihood (smaller value) of other classes\n            input_cf_acc[other_cf] += 1 - probability\n            \n    # Another nested loop for output (judging) cognitive functions\n    # so models are: TiTe, TiFi, TiFe, ...\n    for i in range(3):\n        for j in range(i+1, 4):\n            model_name = output_cf[i] + output_cf[j]\n            modelObject = models[model_name]\n            cognitive_fn, probability = process_classify_sample(modelObject, sample)\n            \n            # Incrase likelihood of the prediction class\n            output_cf_acc[cognitive_fn] += probability\n            other_cf = output_cf[i] if output_cf[j] == cognitive_fn else output_cf[j]\n            \n            # Increase likelihood (smaller value) of other classes\n            output_cf_acc[other_cf] += 1 - probability\n    \n    # Use supporter model (iNtuition vs Sensing)\n    modelObject = supporters[\"NvS\"]\n    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n    \n    # Increase iNtuitive functions\n    if cognitive_fn == \"N\":\n        input_cf_acc[['Ni', 'Ne']] += probability\n        input_cf_acc[['Si', 'Se']] += 1 - probability\n    # Increase Sensing functions\n    else:\n        input_cf_acc[['Si', 'Se']] += probability\n        input_cf_acc[['Ni', 'Ne']] += 1 - probability\n    \n    # Use supporter model (Feeling vs Thinking)\n    modelObject = supporters[\"FvT\"]\n    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n    \n    # Increase Feeling functions likelihood\n    if cognitive_fn == \"F\":\n        output_cf_acc[['Fi', 'Fe']] += probability\n        output_cf_acc[['Ti', 'Te']] += 1 - probability\n    # Increase Thinking Functions likelihood\n    else:\n        output_cf_acc[['Ti', 'Te']] += probability\n        output_cf_acc[['Fi', 'Fe']] += 1 - probability   \n    \n    # Use supporter model (Introvert vs Extrovert)\n    modelObject = supporters[\"IvE\"]\n    cognitive_fn, probability = process_classify_sample(modelObject, sample)\n    \n    # Increase Introverted functions likelihood\n    if cognitive_fn == \"I\":\n        input_cf_acc[['Ni', 'Si']] += probability\n        input_cf_acc[['Ne', 'Se']] += 1 - probability\n        \n        output_cf_acc[['Fi', 'Ti']] += probability\n        output_cf_acc[['Fe', 'Te']] += 1 - probability\n        \n    # Increase  Extroverted functions likelihood\n    else:\n        input_cf_acc[['Ne', 'Se']] += probability\n        input_cf_acc[['Ni', 'Si']] += 1 - probability\n        \n        output_cf_acc[['Fe', 'Te']] += probability\n        output_cf_acc[['Fi', 'Ti']] += 1 - probability\n    \n    # Return: likelihoods of perceiving (input) cognitive functions\n    # and judging (output) cogitive function\n    return input_cf_acc, output_cf_acc\n\n    \n    \n# Phase 2 of classification algorithm\n# Determine which cognitive function is the dominant\n# and which is the auxiliary.\n# and Fix the classification if necessary\n# Necessary: if phase 1 results a two cognitive functions\n# of the same direction (ex: Ni-Ti), this is not acceptable\n# since there's no personality with these Dom-Aux functions\ndef phase2(sample, input_acc, output_acc):\n    # Number of classifications done on every cognitive functions\n    # (i.e. we ran 5 models having 'Ni' as one of it's classes)\n    counter_models_ran = 5\n    \n    # Get max-likelihood data (probability & className)\n    # maxInput is the perceiving function which got the maximum likelihood\n    # maxOutput is the judgning function ~~~~~~\n    maxInput = {'name':input_acc.idxmax(), 'proba':input_acc.max()}\n    maxOutput = {'name':output_acc.idxmax(), 'proba':output_acc.max()}\n    \n    # Get direction of each cognitive function\n    maxInputDirection = cf_info[maxInput['name']]['direction']\n    maxOutputDirection = cf_info[maxOutput['name']]['direction']\n    \n    # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n    cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n    phase2_model_name = \"\".join(cf_stack)\n    \n    # if both perceiving & judging classes (functions) are opposite direction\n    if maxInputDirection != maxOutputDirection:\n        # We know the top two cognitive functions (ie. Ni, Te)\n        # this path will run the appropriate models to\n        # determine which of these functions is Dominant (primary)\n        # and which is Auxiliary (secondary)\n\n        # determine which cognitive_function is the dominant one\n        modelObject = models[phase2_model_name]\n        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n        counter_models_ran += 1\n        \n    # both perceiving & judging functions have same direction (they must NOT)\n    else:\n        # There is an ambiguity, the top 2 cognitive_functions cannot be of\n        # the same direction (ie. Ni, Ti) (they're both 'i')\n        # One of them is correct, this function will detect which one is\n        \n        # Detect which of these functions is more accurate\n        modelObject = models[phase2_model_name]\n        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n        counter_models_ran += 1\n        \n        \n        # if dominant cognitive function is an input (perceiving) \n        # (ie: Ni, Ne, Si, Se)\n        # flip the direction of the other -output- function\n        # example: Ni-Ti  => Ni-Te\n        if dominant_cf_name == input_acc.idxmax():\n            problematic_cf = output_acc.idxmax()\n            input_acc[input_acc.idxmax()] += probability\n            fixed_cf_name = flip_cf_direction(problematic_cf)\n            temp_acc = output_acc[fixed_cf_name]\n            output_acc[fixed_cf_name] = output_acc[problematic_cf] + (1 - probability)\n            output_acc[problematic_cf] = temp_acc\n        \n        # else if the dominant cognitive function is an output (judging)\n        # (ie: Ti, Te, Fi, Fe)\n        # flip the direction of the other -input- function\n        # example: Ni-Ti => Ne-Ti\n        else:\n            problematic_cf = input_acc.idxmax()\n            output_acc[output_acc.idxmax()] += probability\n            fixed_cf_name = flip_cf_direction(problematic_cf)\n            temp_acc = input_acc[fixed_cf_name]\n            input_acc[fixed_cf_name] = input_acc[problematic_cf] + (1 - probability)\n            input_acc[problematic_cf] = temp_acc\n        \n        # Now we've fixed the direction of the (less) accurate cognitive function\n        # We need to determine which function is dominant\n        \n        # Get the next models ready by concatenating cognitive functions (ie. 'NiTe')\n        cf_stack = np.array([input_acc.idxmax(), output_acc.idxmax()])\n        phase2_final_model_name = \"\".join(cf_stack)\n\n        # Run model\n        modelObject = models[phase2_final_model_name]\n        dominant_cf_name, probability = process_classify_sample(modelObject, sample)\n        counter_models_ran += 1\n    \n    # Now we know which cognitive function is dominant and which is auxilary.\n    # handle their likelihoods\n    \n    # If dominant function is input (perceiving)\n    # increase its likelihood, decrease the other\n    if dominant_cf_name == input_acc.idxmax():\n        input_acc[input_acc.idxmax()] += probability\n        output_acc[output_acc.idxmax()] += 1 - probability\n\n    # else, dominant function is output (judging)\n    # increase its likelihood, decrease the other\n    else:\n        output_acc[output_acc.idxmax()] += probability\n        input_acc[input_acc.idxmax()] += 1 - probability\n\n    # Select the winners in phase 1\n    cognitive_functions_stack = pd.Series({\n            input_acc.idxmax(): input_acc[input_acc.idxmax()],\n            output_acc.idxmax(): output_acc[output_acc.idxmax()]\n            })\n    \n    # Select the dominant function as the 1st winner in phase 2\n    dominant_function =  dominant_cf_name\n    \n    # Select the auxiliary function as the 2nd winner in phase 2\n    auxiliary_function = cognitive_functions_stack.drop(dominant_cf_name).index[0]\n\n    # Convert cognitive functions to personality types\n    personality_type = functions_to_letters(dominant_function + auxiliary_function)\n    \n    # Calculate probability\n    probability = (cognitive_functions_stack[dominant_function] \/ counter_models_ran \n                   + cognitive_functions_stack[auxiliary_function] \/ counter_models_ran) \/ 2\n    \n    return personality_type, probability\n\n# Run classification algorithm phases\ndef run(sample):\n    input_acc, output_acc = phase1(sample)\n    personality, probability = phase2(sample, input_acc, output_acc)\n    return personality, probability","93323e62":"# VALIDATION\nvalidation_set = pd.read_csv(path + 'validation_set.csv')\n\ndef classify(sentence):\n    personality, probability = run(sentence)\n    return personality\n\n# Results as 16 personality types\ny_pred = validation_set['posts'].apply(classify) # it takes ~5 minutes to finish\ny_true = validation_set['type']\n\n# Results as 4 categories (NT, NF, ST, SF)\ny_pred_soft = y_pred.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')\ny_true_soft = y_true.str.replace('I', '').str.replace('E','').str.replace('J','').str.replace('P','')","47ad0045":"# Classification reports\nprint(\"4 Classes\")\nprint(classification_report(y_true_soft, y_pred_soft))\n\nprint(\"\\n16 Classes\\n\")\nprint(classification_report(y_true, y_pred))\n","8d9b6ee0":"sentence = \"\"\"\nHey, they call me the Commander, I create plans and strategies for everything,\nI solve problems using highly optimized solutions, and I use my intuition\nto predict possible scenarios in the future, some people consider me as bossy,\nbut I'm not, recognized me?\n\"\"\"\npersonality, probability = run(sentence) # ENTJ\nprint(f\"Personality: {personality}\\nLikelihood: {probability}\")","74ab8da5":"sentence = \"\"\"\nHey, they call me the Adventurer, I do artworks and sometimes play guitar,\nI enjoy spending time alone listening to music, I appreciate authenticity\nand honesty, I'm always connected to my feelings and alert of it,\nsome people consider as an artist, but I'm not, and I won't let them\ndefine what I am, regonized me?\n\"\"\"\npersonality, probability = run(sentence) # ISFP\nprint(f\"Personality: {personality}\\nLikelihood: {probability}\")","ccde576b":"sentence = \"\"\"\nHey guys, wanna hang out ? come on ..\nhiking out, flirting with girls, going to parties ..\nwhat's life without fun!\n\"\"\"\npersonality, probability = run(sentence) # ESxP\nprint(f\"Personality: {personality}\\nLikelihood: {probability}\")","e504b9f2":"sentence = \"\"\"\nHey. .. .. .. what? .. .. ..\nokay I forgot. Hi, I'm the Logician, or at least that's what they call me besides 'the robot',\nI use my critical thinking skills to logically analyze everything,\nI spend a lot of my time thinking about solutions for problems that will never occur,\nreading articles, investigating theories, understanding concepts, it's all about my mind.\nI know I'm in a computer program being tested, I recognized you ..\n\"\"\"\npersonality, probability = run(sentence) # INTP\nprint(f\"Personality: {personality}\\nLikelihood: {probability}\")","7d334fd2":"# Important Update 1 (31-12-2021)\n**The dataset that was used to train the models, is now available at [MBTI 500 Dataset](https:\/\/www.kaggle.com\/zeyadkhalid\/mbti-personality-types-500-dataset)**","a636d387":"# Visual Explanation of the algorithm\nChoose one of the following:<br>\n[PowerPoint slide show (Recommended)](https:\/\/docs.google.com\/presentation\/d\/1CNQgWNvMKb862dwAmO5c-pP5g3kMtSRJ\/edit?usp=sharing&ouid=109931366324041339501&rtpof=true&sd=true)<br>\n[PDF](https:\/\/drive.google.com\/file\/d\/1qXSAlIorIGyhkQM0oDFHeGn4ejUjNv8g\/view?usp=sharing)<br>","e0ab229c":"The algorithm classifies **Cognitive Functions**, and based on the top two cognitive functions we can tell which personality type it is\n\nPre-trained models are **Logistic Regression binary classifiers**, trained on [MBTI 500 Dataset](https:\/\/www.kaggle.com\/zeyadkhalid\/mbti-personality-types-500-dataset), to classify (each) two cognitive functions. \nThe **dataset** is significantly **imbalanced**, so I used only 6K samples for training, and 2K samples for validation (*validation data is uploaded with the pre-trained models*).\n\n**Note**: I\u2019m sure that the pre-trained models have **high variance** (poor accuracy with different datasets), the top reason is that the data (texts) are all about MBTI, personality theories and how each type sees and copes with other personality types (because data were gathered from MBTI communities - they talk about a single subject). So you can test the algorithm on posts and comments from Reddit MBTI communities, you\u2019ll see better results."}}