{"cell_type":{"b5d29703":"code","b1f1b3f5":"code","aeb75239":"code","cdcc1faa":"code","cab7b072":"code","b833a1bd":"code","c49af467":"code","60178388":"code","461c1895":"code","b8941809":"code","2b678d0d":"code","de85dc60":"code","d5410001":"code","26e0bf83":"code","583757f2":"code","0a53847f":"code","2706cb48":"code","cd422d40":"code","6feea496":"code","06f7a9a6":"code","23efb311":"code","62831264":"code","cd6f3f13":"code","d93ff6ff":"code","3f9c11ed":"code","333ea10f":"code","e94fa060":"code","c74dcc60":"code","9a1c046f":"code","1847a677":"code","ade92093":"code","d013912c":"code","8bf081fc":"code","9e0cf89b":"code","f24c4f25":"code","fb72dd18":"code","0465417e":"code","16f70068":"code","9704f733":"code","46986513":"code","462746f0":"code","47d356f4":"code","6ac69711":"code","88417b8f":"code","26096fa7":"code","13f0ad13":"code","9bf0fafe":"code","bd9e69eb":"code","b4817db6":"code","7d8e9817":"code","37f97cd0":"code","02de324b":"code","faabbf82":"code","90f2d680":"markdown","a8fa7f01":"markdown","aa156d48":"markdown","59feb7cb":"markdown","86d9da7e":"markdown","50dfdb81":"markdown","dd1e7c35":"markdown","45346c1b":"markdown","17e755af":"markdown","048d4541":"markdown","7084e638":"markdown","68d7834c":"markdown","dc7ba96b":"markdown","0dad276c":"markdown","4b8206cc":"markdown","63d24d09":"markdown","07544497":"markdown","3aa41817":"markdown","c58daeb1":"markdown","1e44c5bd":"markdown","b28e8b82":"markdown","9d7a790d":"markdown","edf6a2b8":"markdown","39f43455":"markdown","3bc99ff9":"markdown","8c918737":"markdown","bbc37384":"markdown","a922822a":"markdown","89777950":"markdown","325bc4ff":"markdown","a33ce3f1":"markdown","11fdbd16":"markdown","f11eba84":"markdown","e79c2848":"markdown","ee94769c":"markdown","a53f30ba":"markdown","fd62dda5":"markdown","d11cd269":"markdown","5e2673a2":"markdown","878ec744":"markdown","d2c197f8":"markdown","1164b1e4":"markdown","e5aeb412":"markdown","e47cf04a":"markdown","551e6a24":"markdown","f7b09f21":"markdown","996e8c33":"markdown","a8f95cbb":"markdown","4a043bb6":"markdown","a556a56c":"markdown","4e7453fa":"markdown"},"source":{"b5d29703":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","b1f1b3f5":"df=pd.read_csv('..\/input\/voice-gender\/voice_gender.csv')\ndf # Just a quick look at our dataframe","aeb75239":"df.shape  ","cdcc1faa":"df.corr()","cab7b072":"df.isnull().sum()","b833a1bd":"print ('Unique values in label column: ', df.label.unique())\nprint ('How many non null values we have in our label column: ', len(df.label))\nprint ('Value counts for each class in our label: ') \ndf.label.value_counts()","c49af467":"X=df.iloc[:,:-1]\nX.head()","60178388":"from sklearn.preprocessing import LabelEncoder\ny=df.iloc[:,-1]\n\n# Encode label category\n# male -> 1\n# female -> 0\n\ngender_encoder=LabelEncoder()\ny=gender_encoder.fit_transform(y)\ny","461c1895":"test=['male','female', 'male']\ngender_encoder.transform(test)","b8941809":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit(X)\nX=scaler.transform(X)","2b678d0d":"df.loc[:,'meanfreq'].hist(bins=30)   # Original distribution in column 'meanfreq'","de85dc60":"XX=pd.DataFrame(X)\nXX[0].hist(bins=30)  # Column 'meanfreq' stardardized","d5410001":"sns.boxplot(XX[0])","26e0bf83":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","583757f2":"from sklearn.svm import SVC\nfrom sklearn import metrics\n\nsvc=SVC() #Default hyperparameters\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","0a53847f":"svc=SVC(kernel='linear')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","2706cb48":"svc=SVC(kernel='rbf')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","cd422d40":"svc=SVC(kernel='poly')\nsvc.fit(X_train,y_train)\ny_pred=svc.predict(X_test)\nprint('Accuracy Score:')\nprint(metrics.accuracy_score(y_test,y_pred))","6feea496":"from sklearn.model_selection import cross_val_score\nsvc=SVC(kernel='linear')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') \nprint(scores)","06f7a9a6":"print(scores.mean())","23efb311":"svc=SVC(kernel='rbf')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') \nprint(scores)","62831264":"print(scores.mean())","cd6f3f13":"svc=SVC(kernel='poly')\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy') \nprint(scores)","d93ff6ff":"print(scores.mean())","3f9c11ed":"C_range=list(range(1,26))\nacc_score=[]\nfor c in C_range:\n    svc = SVC(kernel='linear', C=c)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","333ea10f":"C_values=list(range(1,26))\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(C_values,acc_score)\nplt.xticks(np.arange(0,27,2))\nplt.xlabel('Value of C for SVC')\nplt.ylabel('Cross-Validated Accuracy')","e94fa060":"C_range=list(np.arange(0.1,2,0.1))\nacc_score=[]\nfor c in C_range:\n    svc = SVC(kernel='linear', C=c)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","c74dcc60":"C_values=list(np.arange(0.1,2,0.1))\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(C_values,acc_score)\nplt.xticks(np.arange(0.0,2,0.2))\nplt.xlabel('Value of C for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","9a1c046f":"gamma_range=[0.0001,0.001,0.01,0.1,1,10,100]\nacc_score=[]\nfor g in gamma_range:\n    svc = SVC(kernel='rbf', gamma=g)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","1847a677":"gamma_range=[0.0001,0.001,0.01,0.1,1,10,100]\n\n# plot the value of gamma for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(gamma_range,acc_score)\nplt.xlabel('Value of gamma for SVC ')\nplt.xticks(np.arange(0.0001,100,5))\nplt.ylabel('Cross-Validated Accuracy')","ade92093":"gamma_range=[0.0001,0.001,0.01,0.1,1]\nacc_score=[]\nfor g in gamma_range:\n    svc = SVC(kernel='rbf', gamma=g)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","d013912c":"gamma_range=[0.0001,0.001,0.01,0.1,1]\n\n# plot the value of gamma for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(gamma_range,acc_score)\nplt.xlabel('Value of gamma for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","8bf081fc":"gamma_range=[0.01,0.02,0.03,0.04,0.05]\nacc_score=[]\nfor g in gamma_range:\n    svc = SVC(kernel='rbf', gamma=g)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)    ","9e0cf89b":"gamma_range=[0.01,0.02,0.03,0.04,0.05]\n\n# plot the value of gamma for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(gamma_range,acc_score)\nplt.xlabel('Value of gamma for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","f24c4f25":"degree=[2,3,4,5,6]\nacc_score=[]\nfor d in degree:\n    svc = SVC(kernel='poly', degree=d)\n    scores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\n    acc_score.append(scores.mean())\nprint(acc_score)","fb72dd18":"degree=[2,3,4,5,6]\n\n# plot the value of C for SVM (x-axis) versus the cross-validated accuracy (y-axis)\nplt.plot(degree,acc_score,color='r')\nplt.xlabel('degrees for SVC ')\nplt.ylabel('Cross-Validated Accuracy')","0465417e":"from sklearn.svm import SVC\nsvc= SVC(kernel='linear',C=0.1)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\naccuracy_score= metrics.accuracy_score(y_test,y_predict)\nprint(accuracy_score)","16f70068":"from sklearn.model_selection import cross_val_score\nsvc=SVC(kernel='linear',C=0.1)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores.mean())","9704f733":"from sklearn.svm import SVC\nsvc= SVC(kernel='rbf',gamma=0.01)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\nmetrics.accuracy_score(y_test,y_predict)","46986513":"svc=SVC(kernel='rbf',gamma=0.01)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores.mean())","462746f0":"from sklearn.svm import SVC\nsvc= SVC(kernel='poly',degree=3)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\naccuracy_score= metrics.accuracy_score(y_test,y_predict)\nprint(accuracy_score)","47d356f4":"svc=SVC(kernel='poly',degree=3)\nscores = cross_val_score(svc, X, y, cv=10, scoring='accuracy')\nprint(scores.mean())","6ac69711":"from sklearn.svm import SVC\nsvm_model = SVC()","88417b8f":"tuned_parameters = {\n 'degree': [2,3,4],'gamma': [0.01,0.02,0.03],'C':(np.arange(0.1,0.5,0.1)) , 'kernel':['linear','rbf','poly']\n                    }","26096fa7":"from sklearn.model_selection import GridSearchCV\n\nmodel_svm = GridSearchCV(svm_model, tuned_parameters,cv=10,scoring='accuracy')","13f0ad13":"model_svm.fit(X_train, y_train)","9bf0fafe":"print(model_svm.best_score_)","bd9e69eb":"print(model_svm.best_estimator_)","b4817db6":"print(model_svm.best_params_)","7d8e9817":"y_predict=model_svm.predict(X_test)\nprint(metrics.accuracy_score(y_predict,y_test))","37f97cd0":"from sklearn.svm import SVC\nsvc= SVC(kernel='linear',C=0.4,gamma=0.01,degree=2)\nsvc.fit(X_train,y_train)\ny_predict=svc.predict(X_test)\naccuracy_score= metrics.accuracy_score(y_test,y_predict)\nprint(accuracy_score)","02de324b":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","faabbf82":"cm=confusion_matrix(y_test,y_predict, labels=model_svm.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_svm.classes_)\ndisp.plot(cmap='Blues')","90f2d680":"### CV on Linear kernel","a8fa7f01":"### Default RBF kernel","aa156d48":"#### With K-fold cross validation (where K=10):","59feb7cb":"As you know, machine learning models expects our label to be numeric, so in our case we have to convert our label to a machine-readable form, for this we use LabelEncoder from sklearn which offers us do exactly this transformation.","86d9da7e":"**We can see there is constant decrease in the accuracy score as gamma value is greater than 0.03. Thus gamma=0.01 is the best parameter.**","50dfdb81":"### Taking polynomial kernel with different degrees:","dd1e7c35":"**Accuracy score is highest for C=0,1.**","45346c1b":"We are going to use the following code to see the best parameters found by the function:","17e755af":"The accuracy is slightly better without K-fold cross validation but it may fail to generalise the unseen data. Hence it is advisable to perform K-fold cross validation where all the data is covered so it may predict unseen data well.","048d4541":"### CV on Polynomial kernel","7084e638":"### CV on rbf kernel","68d7834c":"### Splitting dataset into training set and testing set for better generalisation","dc7ba96b":"## Let's perform Grid search technique to find the best hyperparameters:","0dad276c":"## Separating features and labels","4b8206cc":"## Reading the comma separated values file as a dataframe","63d24d09":"### Performing SVM taking hyperparameter C=0.1 and kernel as linear:","07544497":"### Let's look into more detail of what is the exact value of C which is giving us the best accuracy score","3aa41817":"We can see that for gamma=10 to 100 the kernel is performing poorly. We can also see a slight dip in accuracy score when gamma is around 1. Let's look into more details for the range 0.0001 to 1.","c58daeb1":"**Accuracy score is highest for third degree polynomial and then there is a drop as degree of polynomial increases. Therefore increase in polynomial degree results in high complexity of the model and as a result overfitting.**","1e44c5bd":"Technically, the gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as similarity measure between two points. Intuitively, a small gamma value define a Gaussian function with a large variance. In this case, two points can be considered similar even if are far from each other. In the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.","b28e8b82":"### Let's see distribution of the first column before and after stardardized:","9d7a790d":"### Just to check the encoding, let's use the following array and see its corresponding transformation","edf6a2b8":"### Performing SVM taking hyperparameter degree=3 and kernel as poly:","39f43455":"Above we could see the result of our best model match with the outcome of the GridSeachCV, but this is only one error metric, it would be better if we could see the performance of the model in only one plot, for this we will use the confusion matrix. ","3bc99ff9":"#### With K-fold cross validation (where K=10):","8c918737":"In K-fold cross validation we generally take the mean of all the scores.","bbc37384":"Polynomial kernel is performing poorly.The reason behind this maybe it is overfitting the training dataset","a922822a":"We can conclude from above that svm by default uses rbf kernel as a parameter for kernel","89777950":"The score increases steadily and reaches its peak before 0.1 and then decreases till gamma=1.Thus Gamma should be around 0.01.","325bc4ff":"### Performing SVM taking hyperparameter gamma=0.01 and kernel as rbf:","a33ce3f1":"**When K-fold cross validation is applied we can obtain different scores in each iteration. This happens because when we use train_test_split method,the dataset get split in random manner into testing and training dataset. Thus it depends on how the dataset got split and which samples are training set and which samples are in testing set. Due to the fact that it's random selecting, we could face an hypothetical case where an specific group in our label have a skew or certain characteristics in particular and surprisingly these values are taken as training data, which would make our model to give us a wrong prediction.**\n\n**With K-fold cross validation we can see that the dataset got split into 10 equal parts thus covering all the data into training as well into testing set. This is the reason why we got 10 different accuracy scores.**","11fdbd16":"## Converting our categorical labels to int type values using label encoding","f11eba84":"### Taking kernel as rbf and evaluating accurary with different values of gamma:","e79c2848":"## Performing K-fold cross validation with different kernels","ee94769c":"## Importing all the necessary libraries","a53f30ba":"## Checking the correlation between each feature","fd62dda5":"**From the above plot we can see that accuracy has been close to 97% for values of C around 1 and then it drops a bit and remains constant.**","d11cd269":"Thus, we can say there are equal number of male and female in our label column","5e2673a2":"### Default Polynomial kernel","878ec744":"#### Hi, welcome to my project! Today I m going to run Support Vector Machine algorithms with different kernels (linear, gaussian, polynomial) and also tune the various parameters such as C ,gamma and degree to find out the best performing model to recongnize voice gender.","d2c197f8":"We can see above how the accuracy score is different everytime. This shows that accuracy score depends upon how the datasets got split.","1164b1e4":"### Let's plot a confusion matrix for this best model:","e5aeb412":"#### With K-fold cross validation (where K=10):","e47cf04a":"### Default Linear kernel","551e6a24":"## Data Standardisation","f7b09f21":"Check the accuracy above by running the best model and computing its corresponding accuracy:","996e8c33":"Standardization refers to shifting the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance). It is useful to standardize attributes for a model. Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data.","a8f95cbb":"Our dataframe has 21 features and 3168 instances.","4a043bb6":"The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points.\n\nThus for very large values we can cause overfitting of the model and for a very small value of C we can cause underfitting. Thus the value of C must be chosen in such a manner that it generalised the unseen data well.","a556a56c":"### Taking all the values of C and checking out the accuracy score with kernel as linear:","4e7453fa":"## Running SVM with default hyperparameter."}}