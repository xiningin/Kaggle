{"cell_type":{"1eff8eef":"code","bc171dc4":"code","9e79393e":"code","72fcced0":"code","a0d6039c":"code","574c7108":"code","358b5432":"code","6dadcb5a":"code","7f5b5d0e":"code","b897e3aa":"code","9ae09671":"code","5e0bd36c":"code","b512c619":"code","6946eb33":"code","882e1d16":"code","25e3ae69":"code","74ea51ce":"code","d73ad081":"code","4ff52272":"code","a47dc5f3":"code","39e0a9bf":"code","084c0b56":"code","b25134b8":"code","46392572":"code","d43b1866":"code","58ff36ca":"code","a6782735":"code","a130ac6e":"code","6f102e4c":"code","b2cc10ec":"code","30bd2e4f":"code","81454eb8":"code","49fcd7b1":"code","a699c32f":"code","776d3273":"code","f5c062e5":"code","f5fb49f8":"code","16b68737":"code","cf4927d8":"code","77ccf719":"code","0746935c":"code","cf7997a8":"code","03fadcc4":"code","6dc07b7f":"code","163ad31f":"code","e6537776":"code","ebccaea9":"code","c2993f50":"code","fba744df":"code","2e8404e7":"code","30f43028":"code","d0e21269":"code","4c386c04":"code","cae118fc":"code","b9a9922b":"code","b4e431ef":"code","e065730b":"code","bbd0b7c2":"code","65e7fda8":"code","aab8c3ba":"code","efe0a8fe":"code","d43cac9e":"code","e54f3e6b":"code","b747063a":"code","d1e27beb":"code","e29aa932":"markdown","2a91c04b":"markdown","d61c0baf":"markdown","107b9be1":"markdown","3853f849":"markdown","60ac548b":"markdown","3e944fe1":"markdown","8c131de3":"markdown"},"source":{"1eff8eef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nimport json\nimport time\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom statistics import mean\nfrom tqdm import tqdm_notebook\nfrom uuid import uuid4\n\n## sklearn\nfrom sklearn import svm\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.naive_bayes import MultinomialNB as MB\nfrom sklearn.model_selection import StratifiedKFold\n\n## Keras Modules\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, LSTM, GRU, Bidirectional, Dropout, Reshape, Flatten, concatenate, Input, Conv1D, GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.regularizers import l1, l2, l1_l2\n\n## Torch Modules\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\nimport spacy\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\n# !python -m spacy download en\nnlp = spacy.load('en')","bc171dc4":"!pip install transformers","9e79393e":"## PyTorch Transformer\nfrom transformers import RobertaModel, RobertaTokenizer\nfrom transformers import RobertaForSequenceClassification, RobertaConfig, AdamW, get_linear_schedule_with_warmup","72fcced0":"## Check if Cuda is Available\nprint(torch.cuda.is_available())","a0d6039c":"RANDOM_STATE = 1234\n\ntqdm_notebook().pandas()","574c7108":"data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")[[\"text\", \"target\"]]\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndata.head()","358b5432":"def clean_text(text):\n    \n    # Special characters\n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", text)\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"Jap\u00cc_n\", \"Japan\", text)    \n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a33million\", \"3 million\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n    \n    # remove url link\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    # remove html tag\n    text = re.sub(r'<.*?>', '', text)\n    \n    # remove numbers\n    text = re.sub(r'[\\d]+', ' ', text)\n    \n    return text","6dadcb5a":"# remove controversial tweets\nunique_targets = data.groupby('text').agg(unique_target=('target', pd.Series.nunique))\ncontroversial_tweets = unique_targets[unique_targets['unique_target'] > 1].index\n\ndata = data[~data['text'].isin(controversial_tweets)]\n\n# remove duplicates rows\ndata = data.drop_duplicates(subset='text', keep='first')\n\n# remove special characters, url, and html tags\ndata['text'] = data['text'].apply(clean_text) \ntest['text'] = test['text'].apply(clean_text)","7f5b5d0e":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base') \nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base') ","b897e3aa":"def prepare_features(data_set, labels=None, max_seq_length = 100, \n                     zero_pad = True, include_special_tokens = True): \n    \n    ## Tokenzine Input\n    input_ids = []\n    attention_masks = []\n    \n    for sent in data_set:\n        encoded_dict = tokenizer.encode_plus(\n                    sent,                      # Sentence to encode.\n                    add_special_tokens = include_special_tokens, # Add '[CLS]' and '[SEP]'\n                    max_length = max_seq_length,           # Max length according to our text data.\n                    pad_to_max_length = zero_pad, # Pad & truncate all sentences.\n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',     # Return pytorch tensors.\n               )\n    \n        # Add the encoded sentence to the id list. \n\n        input_ids.append(encoded_dict['input_ids'])\n\n        # And its attention mask (simply differentiates padding from non-padding).\n\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    # convert the lists into tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    \n    if labels is not None: \n        labels = torch.tensor(labels)\n        return input_ids, attention_masks, labels\n    else: \n        return input_ids, attention_masks","9ae09671":"train_val = 0.8\ntrain = data.sample(frac=train_val, random_state=RANDOM_STATE)\nval = data.drop(train.index).reset_index(drop=True)\ntrain = train.reset_index(drop=True)","5e0bd36c":"train_input_ids, train_attention_masks, train_labels = prepare_features(\n    train['text'], train['target'])\nval_input_ids, val_attention_masks, val_labels = prepare_features(\n    val['text'], val['target'])\ntest_input_ids, test_attention_masks = prepare_features(\n    test['text'])","b512c619":"training_set = TensorDataset(train_input_ids, train_attention_masks, train_labels)\nvalidation_set = TensorDataset(val_input_ids, val_attention_masks, val_labels)\ntest_set = TensorDataset(test_input_ids, test_attention_masks)","6946eb33":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","882e1d16":"torch.cuda.is_available()","25e3ae69":"BATCH_SIZE = 32\nLEARNING_RATE = 1e-05\nEPSILON = 1e-8\nMAX_EPOCHS = 10","74ea51ce":"loading_params = {'batch_size': BATCH_SIZE,\n          'shuffle': True,\n          'drop_last': False,\n          'num_workers': 1}\n\n\ntraining_loader = DataLoader(training_set, **loading_params)\nvalidation_loader = DataLoader(validation_set, **loading_params)\n\ntest_loading_params = {'batch_size': BATCH_SIZE,\n          'shuffle': False,\n          'drop_last': False,\n          'num_workers': 1}\n\ntesting_loader = DataLoader(test_set, **test_loading_params)","d73ad081":"#https:\/\/www.kaggle.com\/datafan07\/disaster-tweets-nlp-eda-bert-with-transformers\n\nloss_function = nn.CrossEntropyLoss()\n\noptimizer = AdamW(model.parameters(),\n                  lr = LEARNING_RATE, # args.learning_rate\n                  eps = EPSILON # args.adam_epsilon\n                )\n\n# number of training steps\ntotal_steps = len(training_loader) * MAX_EPOCHS\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","4ff52272":"def format_time(elapsed):    \n    \"\"\"A function that takes a time in seconds and returns a string hh:mm:ss\"\"\"\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","a47dc5f3":"def flat_accuracy(preds, labels):\n    \"\"\"A function for calculating accuracy scores\"\"\"\n    pred_flat = np.argmax(preds, axis=1)\n    labels_flat = labels\n    return accuracy_score(labels_flat, pred_flat)","39e0a9bf":"# model = model.train()\n\nfor epoch in tqdm_notebook(range(MAX_EPOCHS)):\n    # start time for each epoch\n    t0 = time.time()\n    \n    total_train_loss = 0\n    \n    model.train()\n    \n    print(\"EPOCH -- {} \/ {}\".format(epoch, MAX_EPOCHS))\n    for step, batch in enumerate(training_loader):\n        if step % 30 == 0 and not step == 0: \n            elapsed = format_time(time.time() - t0)\n            print(' Batch {} of {}. Elapsed: {:}'.format(step, len(training_loader), elapsed))\n            \n        input_ids = batch[0].to(device).to(torch.int64)\n        input_masks = batch[1].to(device).to(torch.int64)\n        labels = batch[2].to(device).to(torch.int64)          \n                  \n        # Always clear any previously calculated gradients before performing a backward pass. PyTorch doesn't do this automatically because accumulating the gradients is 'convenient while training RNNs'. \n        model.zero_grad()\n                  \n#         optimizer.zero_grad()\n#         sent = sent.squeeze(0)\n#         if torch.cuda.is_available():\n#             sent = sent.cuda()\n#             label = label.cuda()\n                  \n        loss, logits = model(input_ids, \n                           token_type_ids=None,\n                           attention_mask=input_masks, \n                           labels=labels)\n                  \n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n        \n        total_train_loss += loss.item()\n        loss.backward()\n                  \n        # Clip the norm of the gradients to 1.0. This is to help \n        # prevent the 'exploding gradients' problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # update parameters and move a step forward using the computed gradients          \n        optimizer.step()\n        scheduler.step()\n        \n        #         output = model.forward(sent)[0]\n        #         _, predicted = torch.max(output, 1)\n\n        #         loss = loss_function(output, label)\n        #         loss.backward()\n        #         optimizer.step()\n    avg_train_loss = total_train_loss \/ len(training_loader)\n    training_time = format_time(time.time() - t0)\n            \n    print('')\n    print(' Average training loss: {0:.4f}'.format(avg_train_loss))\n    print(' Training epoch took: {:}'.format(training_time))\n    \n    print('Running Validation')\n                  \n    model.eval()\n        \n    val_predictions = []\n    val_labels = []\n    for batch in validation_loader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        with torch.no_grad():\n            loss, logits = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask,\n                       labels=b_labels)\n            \n        val_predictions.append(logits.detach().cpu().numpy())\n        val_labels.append(b_labels.to('cpu').numpy())\n\n    val_predictions = [item for sublist in val_predictions for item in sublist]\n    val_labels = [item for sublist in val_labels for item in sublist]\n        \n    val_accuracy = flat_accuracy(val_predictions, val_labels)\n    print('  Accuracy: {0:.4f}'.format(val_accuracy))","084c0b56":"model.eval()\npredictions = []\nfor batch in testing_loader:\n    batch = tuple(t.to(device) for t in batch)\n    \n    input_ids, input_masks = batch\n    \n    with torch.no_grad():\n        logits = model(input_ids, \n                        token_type_ids=None,\n                        attention_mask=input_masks)[0]\n        \n        logits = logits.detach().cpu().numpy()\n        \n        predictions.append(logits)","b25134b8":"flat_predictions = [item for sublist in predictions for item in sublist]\ntargets = np.argmax(flat_predictions, axis=1).flatten() ","46392572":"test['target'] = targets","d43b1866":"tokenizer.convert_tokens_to_ids(tokenizer.tokenize('dick'))","58ff36ca":"submission = test[['id', 'target']].to_csv(\"submission_roberta.csv\", index=False)","a6782735":"# function to preprocess the tweets.\ndef preprocess(texts, allowed_postags=['NOUN', \"ADJ\", \"VERB\", \"ADV\", \"DET\"]): \n    texts_out = []\n    for text in texts: \n        # lower case the sentences\n        lowered_text = text.lower()\n    \n        # parse the text with nlp() from spacy. It treats emoticons as words.\n        doc = nlp(lowered_text)\n\n        # remove space, numericals, and punctuation.\n        tokens = [token for token in doc if not (token.is_punct | \n                                               token.is_space |  \n                                               token.is_digit)]\n\n        # filter only words is alpha\n        tokens = [token for token in tokens if token.is_alpha]\n        \n        # lemmatization, filter words by pos tag.\n        lemmas = [token.lemma_ for token in tokens if token.pos_ in allowed_postags]\n      \n        # remove stop words\n        words = [lemma for lemma in lemmas if lemma not in stop_words]\n\n        texts_out.append(words)\n    return texts_out","a130ac6e":"data_pruned = data.copy(deep=True)\ndata_pruned['text'] = data_pruned['text'].progress_apply(clean_text)\ndata_pruned['text'] = data_pruned['text'].progress_apply(preprocess)","6f102e4c":"test_pruned = test.copy(deep=True)\ntest_pruned['text'] = test_pruned['text'].apply(clean_text)\ntest_pruned['text'] = test_pruned['text'].apply(preprocess)","b2cc10ec":"all_text = pd.concat([data_pruned[['text']], test_pruned[['text']]], ignore_index=True)","30bd2e4f":"all_text","81454eb8":"cv = CountVectorizer(ngram_range=(1,2))\ntfidf_transformer = TfidfTransformer()\nx = cv.fit_transform(all_text['text'])\nx_all_tfidf = tfidf_transformer.fit_transform(x)","49fcd7b1":"data_pruned.shape","a699c32f":"training_samples = data_pruned.shape[0]\nX_train = x_all_tfidf[:training_samples,:]\nX_test = x_all_tfidf[training_samples:,:]","776d3273":"y_train = data_pruned['target']","f5c062e5":"mb_classifier = MB().fit(X_train, y_train)","f5fb49f8":"pred = mb_classifier.predict(X_train)\n\nc = classification_report(y_train,pred)","16b68737":"skf = StratifiedKFold(n_splits=5, random_state=RANDOM_STATE)\ntotal_accuracy = []\ntotal_precision = []\ntotal_recall = []\nfor train_index, val_index in skf.split(X_train, y_train):\n    current_X_train = X_train[train_index]\n    current_y_train = y_train.iloc[train_index]\n    current_X_val = X_train[val_index]\n    current_y_val = y_train.iloc[val_index]\n    \n    clf = clf_svm\n    clf.fit(current_X_train, current_y_train)\n    \n    current_predictions = clf.predict(current_X_val)\n    total_accuracy.append(accuracy_score(current_y_val, current_predictions))\n    total_precision.append(precision_score(current_y_val, current_predictions))\n    total_recall.append(recall_score(current_y_val, current_predictions))\n    \nave_accuracy = mean(total_accuracy)\nave_precision = mean(total_precision)\nave_recall = mean(total_recall)\nprint(\"Average Accuracy: {:.4f}\".format(ave_accuracy))\nprint(\"Average Precision: {:.4f}\".format(ave_precision))\nprint(\"Average Recall: {:.4f}\".format(ave_recall))\n","cf4927d8":"y_train.shape","77ccf719":"test_pruned['target'] = mb_classifier.predict(x_test)","0746935c":"test_pruned[['id', 'target']].to_csv(\"submission_2.csv\", index=False)","cf7997a8":"clf_svm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')","03fadcc4":"clf_svm.fit(x_train, y_train)\npred = clf_svm.predict(x_train)\nprint(classification_report(y_train,pred))","6dc07b7f":"test_pruned['target'] = clf_svm.predict(x_test)\ntest_pruned[['id', 'target']].to_csv(\"submission_svm.csv\", index=False)","163ad31f":"TRAIN_VAL_SPLIT = 0.8\n\ntrain = data.sample(frac=TRAIN_VAL_SPLIT, random_state=RANDOM_STATE)\nval = data[~data.index.isin(train.index)]\n\nX_train = train['text'].values\ny_train = train['target'].values\n\nX_val = val['text'].values\ny_val = val['target'].values\n\nX_test = test['text'].values","e6537776":"# function to preprocess the tweets.\ndef preprocess(texts, allowed_postags=['NOUN', \"ADJ\", \"VERB\", \"ADV\", \"PROPN\", \"DET\"]): \n    texts_out = []\n    for text in texts: \n        # lower case the sentences\n        lowered_text = text.lower()\n        \n        # remove url link\n        lowered_text = re.sub(r'https?:\/\/\\S+|www\\.\\S+', '', lowered_text)\n\n        # remove html tags\n        lowered_text = re.sub(r'<.*?>', '', lowered_text)\n    \n        # parse the text with nlp() from spacy. It treats emoticons as words.\n        doc = nlp(lowered_text)\n\n        # remove space, numericals, and punctuation.\n        tokens = [token for token in doc if not (token.is_punct | \n                                               token.is_space |  \n                                               token.is_digit)]\n\n        # lemmatization, filter words by pos tag.\n        lemmas = [token.lemma_ for token in tokens if token.pos_ in allowed_postags]\n      \n        # remove stop words\n        words = [lemma for lemma in lemmas if lemma not in stop_words]\n\n        texts_out.append(words)\n    return texts_out","ebccaea9":"train_corpus = preprocess(X_train)\nval_corpus = preprocess(X_val)\ntest_corpus = preprocess(X_test)","c2993f50":"corpus = train_corpus + val_corpus + test_corpus","fba744df":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\ntrain_sequences = tokenizer.texts_to_sequences(train_corpus)\nval_sequences = tokenizer.texts_to_sequences(val_corpus)\ntest_sequences = tokenizer.texts_to_sequences(test_corpus)\n\n# the dictionary of word occurrences.\nword_index = tokenizer.word_index\n\ntrain_max_length = max([len(x) for x in train_sequences])\nval_max_length = max([len(x) for x in val_sequences])\ntest_max_length = max([len(x) for x in test_sequences])\n\nmax_length = max(train_max_length, val_max_length, test_max_length)\n\nX_train_pad = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\")\nX_val_pad = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\")\nX_test_pad = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\")\n\nvocab = np.array(list(tokenizer.word_index.keys()))\nvocab_size = len(tokenizer.word_index) + 1","2e8404e7":"EMBEDDING_DIM = 30","30f43028":"model_tuned = Sequential()\nmodel_tuned.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\nmodel_tuned.add(GRU(units=30, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\nmodel_tuned.add(GRU(units=30, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\nmodel_tuned.add(GRU(units=30, dropout=0.2, recurrent_dropout=0.2, \n                               kernel_regularizer=l1_l2(0.01, 0.01), recurrent_regularizer=l1_l2(0.01, 0.01)))\nmodel_tuned.add(Dense(1, activation='sigmoid'))\n\nmodel_tuned.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","d0e21269":"history = model_tuned.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_val_pad, y_val), verbose=2)","4c386c04":"y_pred_tuned = model_tuned.predict(X_test_pad)","cae118fc":"y_pred_binary = list(map(lambda x: 1 if x >= 0.5 else 0, y_pred_tuned))","b9a9922b":"y_pred_binary","b4e431ef":"test['target'] = y_pred_binary","e065730b":"test[['id', 'target']].to_csv(\"submission_rnn.csv\", index=False)","bbd0b7c2":"def ConvNet(max_sequence_length, num_words, embedding_dim, labels_index):\n \n    embedding_layer = Embedding(num_words,\n                            embedding_dim,\n                            input_length=max_sequence_length)\n    \n    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n    embedded_sequences = embedding_layer(sequence_input)\n    convs = []\n    filter_sizes = [3,4,5,6]\n    for filter_size in filter_sizes:\n        l_conv = Conv1D(filters=50, \n                        kernel_size=filter_size, \n                        activation='relu')(embedded_sequences)\n        l_conv = Dropout(0.2)(l_conv)\n        l_pool = GlobalMaxPooling1D()(l_conv)\n        convs.append(l_pool)\n    l_merge = concatenate(convs, axis=1)\n#     x = Dropout(0.2)(l_merge)  \n    x = Dense(32, activation='relu')(l_merge)\n    x = Dropout(0.2)(x)\n    preds = Dense(labels_index, activation='sigmoid')(x)\n    model = Model(sequence_input, preds)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])\n    model.summary()\n    return model","65e7fda8":"model = ConvNet(max_length, vocab_size, EMBEDDING_DIM, 1)","aab8c3ba":"y_val","efe0a8fe":"y_train","d43cac9e":"hist = model.fit(X_train_pad, \n                 y_train, \n                 epochs=25, \n                 batch_size=128, \n                 validation_data=(X_val_pad, y_val), \n                 verbose=2) ","e54f3e6b":"y_pred = model.predict(X_test_pad)\ny_pred_binary = list(map(lambda x: 1 if x >= 0.5 else 0, y_pred))","b747063a":"test['target'] = y_pred_binary\ntest[['id', 'target']].to_csv(\"submission_cnn.csv\", index=False)","d1e27beb":"data[data['target'] == 0].values","e29aa932":"# 2. TF-IDF ","2a91c04b":"## 2.1 Naive Bayse","d61c0baf":"# 3. CNN","107b9be1":"## 2.2 SVM","3853f849":"This notebook applies RoBerta pretraining and naive base to ","60ac548b":"# 0. Data Cleaning","3e944fe1":"# 0. Utility Functions","8c131de3":"# 1. Pretraining Models"}}