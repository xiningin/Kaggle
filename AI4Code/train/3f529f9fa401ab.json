{"cell_type":{"095ef991":"code","14f558e5":"code","1a8eba8c":"code","d660e89a":"code","c5d4b041":"code","3d8251ce":"code","2e93e9ec":"code","1e9061aa":"code","0f903d92":"code","faf53a93":"code","658d6837":"code","ac3bb334":"code","8a397097":"code","62822420":"markdown","99e7e997":"markdown"},"source":{"095ef991":"pip install pyswarms","14f558e5":"# Import modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\n\n\n# Import PySwarms\nimport pyswarms as ps\n\n# Some more magic so that the notebook will reload external python modules;\n# see http:\/\/stackoverflow.com\/questions\/1907993\/autoreload-of-modules-in-ipython\n%load_ext autoreload\n%autoreload 2","1a8eba8c":"import sklearn\nfrom sklearn.datasets import load_breast_cancer\n# sklearn.datasets.load_breast_cancer","d660e89a":"# Load the iris dataset\ndata = load_iris()\n\n# Store the features as X and the labels as y\nX = data.data\ny = data.target","c5d4b041":"# Forward propagation\ndef forward_prop(params):\n    \"\"\"Forward propagation as objective function\n\n    This computes for the forward propagation of the neural network, as\n    well as the loss. It receives a set of parameters that must be\n    rolled-back into the corresponding weights and biases.\n\n    Inputs\n    ------\n    params: np.ndarray\n        The dimensions should include an unrolled version of the\n        weights and biases.\n\n    Returns\n    -------\n    float\n        The computed negative log-likelihood loss given the parameters\n    \"\"\"\n    # Neural network architecture\n    n_inputs = 4\n    n_hidden = 20\n    n_classes = 3\n\n    # Roll-back the weights and biases\n    W1 = params[0:80].reshape((n_inputs,n_hidden))\n    b1 = params[80:100].reshape((n_hidden,))\n    W2 = params[100:160].reshape((n_hidden,n_classes))\n    b2 = params[160:163].reshape((n_classes,))\n\n    # Perform forward propagation\n    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n    a1 = np.tanh(z1)     # Activation in Layer 1\n    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2\n    logits = z2          # Logits for Layer 2\n\n    # Compute for the softmax of the logits\n    exp_scores = np.exp(logits)\n    probs = exp_scores \/ np.sum(exp_scores, axis=1, keepdims=True)\n\n    # Compute for the negative log likelihood\n    N = 150 # Number of samples\n    corect_logprobs = -np.log(probs[range(N), y])\n    loss = np.sum(corect_logprobs) \/ N\n\n    return loss","3d8251ce":"def f(x):\n    \"\"\"Higher-level method to do forward_prop in the\n    whole swarm.\n\n    Inputs\n    ------\n    x: numpy.ndarray of shape (n_particles, dimensions)\n        The swarm that will perform the search\n\n    Returns\n    -------\n    numpy.ndarray of shape (n_particles, )\n        The computed loss for each particle\n    \"\"\"\n    n_particles = x.shape[0]\n    j = [forward_prop(x[i]) for i in range(n_particles)]\n    return np.array(j)","2e93e9ec":"# Initialize swarm\noptions = {'c1': 0.5, 'c2': 0.3, 'w':0.9}\n\n# Call instance of PSO\ndimensions = (4 * 20) + (20 * 3) + 20 + 3\noptimizer = ps.single.GlobalBestPSO(n_particles=100, dimensions=dimensions, options=options)\n\n# Perform optimization\ncost, pos = optimizer.optimize(f, iters=1000, verbose=True)#, print_step=100","1e9061aa":"from pyswarms.utils.functions import single_obj as fx\nfrom pyswarms.utils.plotters import (plot_cost_history, plot_contour, plot_surface)","0f903d92":"plot_cost_history(cost_history=optimizer.cost_history)#optimizer.cost_history\nplt.show()","faf53a93":"\nplot_cost_history(cost_history=pos)#optimizer.cost_history\nplt.show()","658d6837":"from matplotlib import animation, rc\nfrom IPython.display import HTML\n# equivalent to rcParams['animation.html'] = 'html5'\n# See http:\/\/louistiao.me\/posts\/notebooks\/save-matplotlib-animations-as-gifs\/\nrc('animation', html='html5')","ac3bb334":"def predict(X, pos):\n    \"\"\"\n    Use the trained weights to perform class predictions.\n\n    Inputs\n    ------\n    X: numpy.ndarray\n        Input Iris dataset\n    pos: numpy.ndarray\n        Position matrix found by the swarm. Will be rolled\n        into weights and biases.\n    \"\"\"\n    # Neural network architecture\n    n_inputs = 4\n    n_hidden = 20\n    n_classes = 3\n\n    # Roll-back the weights and biases\n    W1 = pos[0:80].reshape((n_inputs,n_hidden))\n    b1 = pos[80:100].reshape((n_hidden,))\n    W2 = pos[100:160].reshape((n_hidden,n_classes))\n    b2 = pos[160:163].reshape((n_classes,))\n\n    # Perform forward propagation\n    z1 = X.dot(W1) + b1  # Pre-activation in Layer 1\n    a1 = np.tanh(z1)     # Activation in Layer 1\n    z2 = a1.dot(W2) + b2 # Pre-activation in Layer 2\n    logits = z2          # Logits for Layer 2\n\n    y_pred = np.argmax(logits, axis=1)\n    return y_pred","8a397097":"(predict(X, pos) == y).mean()","62822420":"# Error Function","99e7e997":"# Harmonics"}}