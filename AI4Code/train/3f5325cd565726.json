{"cell_type":{"01493bc2":"code","750d1836":"code","2c05f7a1":"code","81ef0731":"code","cf4e66e0":"code","a8f8a39c":"code","a7a87ae6":"code","3ebc15b7":"code","129ad8db":"code","265e2899":"code","02d9d706":"code","555ff8cf":"code","e02d0e4d":"code","04b2a496":"code","51d40af0":"code","79491933":"code","adfaae39":"code","d7e1cea8":"code","4af4f045":"code","cdbb34fe":"code","40ea45c7":"code","77417e37":"code","1874ab31":"code","4d67c1a2":"code","06eb7dc9":"markdown","f33c78e7":"markdown","20c6f353":"markdown","d26b0d7f":"markdown","d6ed3c34":"markdown","26b2468e":"markdown","abd0c8ea":"markdown","2f7483c6":"markdown","cbecda10":"markdown","039799db":"markdown","911659f3":"markdown","98a1fa67":"markdown","90fa5ed8":"markdown","9e50adfe":"markdown","a8d8dce6":"markdown","7f75f51c":"markdown"},"source":{"01493bc2":"!pip install gokinjo scikit-learn\n!pip install autogluon\n!pip install --upgrade ipykernel","750d1836":"# general packages\nimport pandas as pd\nimport numpy as np\nimport time\n\n# knn features\nfrom gokinjo import knn_kfold_extract\nfrom gokinjo import knn_extract\n\n# ml tools\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import f1_score, log_loss, roc_auc_score\n\n# models\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# optimization\nimport optuna\n\n# interpretable ml\nimport shap\n\n# automl\nfrom autogluon.tabular import TabularPredictor\n\n# ignore specific warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"ntree_limit is deprecated, use `iteration_range` or model slicing instead.\")","2c05f7a1":"# aux functions\n\ndef get_threshold(y_true, y_pred):\n    thresholds = np.arange(0.0, 1.0, 0.01)\n    f1_scores = []\n    for thresh in thresholds:\n        f1_scores.append(\n            f1_score(y_true, [1 if m>thresh else 0 for m in y_pred]))\n    f1s = np.array(f1_scores)\n    return thresholds[f1s.argmax()]\n    \ndef custom_f1(y_true, y_pred):\n    max_f1_threshold =  get_threshold(y_true, y_pred)\n    y_pred = np.where(y_pred>max_f1_threshold, 1, 0)\n    return f1_score(y_true, y_pred) ","81ef0731":"# load data\ntrain = pd.read_csv('..\/input\/porto-seguro-data-challenge\/train.csv').drop('id', axis=1)\ntest = pd.read_csv('..\/input\/porto-seguro-data-challenge\/test.csv').drop('id', axis=1)\nsample_submission = pd.read_csv('..\/input\/porto-seguro-data-challenge\/submission_sample.csv')\nmeta = pd.read_csv('..\/input\/porto-seguro-data-challenge\/metadata.csv')\n\n# get data types\ncat_nom = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Qualitativo nominal\")].iloc[:,0]] \ncat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Qualitativo ordinal\")].iloc[:,0]] \nnum_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Quantitativo discreto\")].iloc[:,0]] \nnum_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Quantitativo continua\")].iloc[:,0]] ","cf4e66e0":"# convert to numpy because gokinjo expects np arrays\nX = train[cat_nom+cat_ord+num_dis+num_con].to_numpy()\ny = train.y.to_numpy()\nX_test = test[cat_nom+cat_ord+num_dis+num_con].to_numpy()\n\n# extract on train data\nKNN_feat_train = knn_kfold_extract(X, y, k=1, normalize='standard')\nprint(\"KNN features for training set, shape: \", np.shape(KNN_feat_train))\n\n# extract on test data\nKNN_feat_test = knn_extract(X, y, X_test, k=1, normalize='standard')\nprint(\"KNN features for test set, shape: \", np.shape(KNN_feat_test))\n\n# convert to dataframe\nKNN_feat_train = pd.DataFrame(KNN_feat_train, columns=[\"knn\"+str(x) for x in range(KNN_feat_train.shape[1])])\nKNN_feat_test = pd.DataFrame(KNN_feat_test, columns=[\"knn\"+str(x) for x in range(KNN_feat_test.shape[1])])\n\n# store KNN features, they are computationally expensive\nKNN_feat_train.to_csv('knn_feat_train.csv',index=False)\nKNN_feat_test.to_csv('knn_feat_test.csv',index=False)","a8f8a39c":"knn_feat_train = pd.read_csv('..\/input\/porto-seguro-knn-feature-extraction-k-1\/knn_feat_train.csv')\nknn_feat_test = pd.read_csv('..\/input\/porto-seguro-knn-feature-extraction-k-1\/knn_feat_test.csv')","a7a87ae6":"X_test = test[cat_nom+cat_ord+num_dis+num_con]\nX = train[cat_nom+cat_ord+num_dis+num_con]\ny = train.y\n\nK=10\nSEED=314\nkf = KFold(n_splits=K, random_state=SEED, shuffle=True)","3ebc15b7":"fixed_params = {\n    'random_state': 9,\n    \"objective\": \"binary:logistic\",\n    \"eval_metric\": 'logloss',\n    'use_label_encoder':False,\n    'n_estimators':10000,\n}\n\ndef objective(trial):\n    \n    hyperparams = {\n        'clf':{\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 5.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 5.0, log=True)\n        }\n    }\n    \n    if hyperparams['clf'][\"booster\"] == \"gbtree\" or hyperparams['clf'][\"booster\"] == \"dart\":\n        hyperparams['clf'][\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n        hyperparams['clf'][\"eta\"] = trial.suggest_float(\"eta\", 0.01, 0.1, log=True)\n        hyperparams['clf'][\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n        hyperparams['clf'][\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n        hyperparams['clf']['min_child_weight'] = trial.suggest_int('min_child_weight', 5, 20)\n        hyperparams['clf'][\"subsample\"] = trial.suggest_float(\"subsample\", 0.03, 1)\n        hyperparams['clf'][\"colsample_bytree\"] = trial.suggest_float(\"colsample_bytree\", 0.03, 1)\n        hyperparams['clf']['max_delta_step'] = trial.suggest_float('max_delta_step', 0, 10)\n        \n    if hyperparams['clf'][\"booster\"] == \"dart\":\n        hyperparams['clf'][\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n        hyperparams['clf'][\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n        hyperparams['clf'][\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n        hyperparams['clf'][\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n    \n    params = dict(**fixed_params, **hyperparams['clf'])\n    \n    xgb_oof = np.zeros(X.shape[0])\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n        \n        model = XGBClassifier(**params)\n        \n        model.fit(X_train, y_train,\n                  eval_set=[(X_val, y_val)],\n                  early_stopping_rounds=150,\n                  verbose=False)\n    \n        xgb_oof[val_idx] = model.predict_proba(X_val)[:,1]\n\n        del model\n\n    return log_loss(y, xgb_oof)","129ad8db":"study_xgb = optuna.create_study(direction='minimize')\n\nstudy_xgb.optimize(objective, \n               timeout=60*5, # original time: 60*60*7.5\n               gc_after_trial=True)","265e2899":"# After 7.5 hours...\nstudy_xgb = {'booster': 'gbtree',\n 'lambda': 9.012384508756378e-07,\n 'alpha': 0.7472040331088792,\n 'max_depth': 5,\n 'eta': 0.01507605562231303,\n 'gamma': 1.0214961302342215e-08,\n 'grow_policy': 'lossguide',\n 'min_child_weight': 5,\n 'subsample': 0.9331005225916879,\n 'colsample_bytree': 0.25392142363325004,\n 'max_delta_step': 5.685109389498008}","02d9d706":"final_params_xgb = dict()\nfinal_params_xgb['clf']=dict(**fixed_params, **study_xgb)","555ff8cf":"X_test = test[cat_nom+cat_ord+num_dis+num_con]\nX = train[cat_nom+cat_ord+num_dis+num_con]\ny = train.y\n\nK=15 # number of bins with Sturge\u2019s rule\nSEED=123\nkf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)","e02d0e4d":"shap1_oof = np.zeros((X.shape[0], X.shape[1]))\nshap1_test = np.zeros((X_test.shape[0], X_test.shape[1]))\nmodel_shap1_oof = np.zeros(X.shape[0])\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n    print(f\"\u279c FOLD :{fold}\")\n    X_train = X.iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_val = X.iloc[val_idx]\n    y_val = y.iloc[val_idx]\n    \n    start = time.time()\n    \n    model = XGBClassifier(**final_params_xgb['clf'])\n    \n    model.fit(X_train, y_train,\n              eval_set=[(X_val, y_val)],\n              early_stopping_rounds=150,\n              verbose=False)\n    \n    model_shap1_oof[val_idx] += model.predict_proba(X_val)[:,1]\n    \n    print(\"Final F1     :\", custom_f1(y_val, model_shap1_oof[val_idx]))\n    print(\"Final AUC    :\", roc_auc_score(y_val, model_shap1_oof[val_idx]))\n    print(\"Final LogLoss:\", log_loss(y_val, model_shap1_oof[val_idx]))\n\n    explainer = shap.TreeExplainer(model)\n    \n    shap1_oof[val_idx] = explainer.shap_values(X_val)\n\n    shap1_test += explainer.shap_values(X_test) \/ K\n\n    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n    \nshap1_oof = pd.DataFrame(shap1_oof, columns = [x+\"_shap1\" for x in X.columns])\nshap1_test = pd.DataFrame(shap1_test, columns = [x+\"_shap1\" for x in X_test.columns])\n\nprint(\"Final F1     :\", custom_f1(y, model_shap1_oof))\nprint(\"Final AUC    :\", roc_auc_score(y, model_shap1_oof))\nprint(\"Final LogLoss:\", log_loss(y, model_shap1_oof))","04b2a496":"X = pd.concat([X, knn_feat_train], axis=1)\nX_test = pd.concat([X_test, knn_feat_test], axis=1)","51d40af0":"shap2_oof = np.zeros((X.shape[0], X.shape[1]))\nshap2_test = np.zeros((X_test.shape[0], X_test.shape[1]))\nmodel_shap2_oof = np.zeros(X.shape[0])\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n    print(f\"\u279c FOLD :{fold}\")\n    X_train = X.iloc[train_idx]\n    y_train = y.iloc[train_idx]\n    X_val = X.iloc[val_idx]\n    y_val = y.iloc[val_idx]\n    \n    start = time.time()\n    \n    model = CatBoostClassifier(random_seed=SEED,\n                               verbose = 0,\n                               n_estimators=10000,\n                               loss_function= 'Logloss',\n                               use_best_model=True,\n                               eval_metric= 'Logloss')\n    \n    model.fit(X_train, y_train, \n              eval_set = [(X_val,y_val)], \n              early_stopping_rounds = 100,\n              verbose = False)\n    \n    model_shap2_oof[val_idx] += model.predict_proba(X_val)[:,1]\n    \n    print(\"Final F1     :\", custom_f1(y_val, model_shap2_oof[val_idx]))\n    print(\"Final AUC    :\", roc_auc_score(y_val, model_shap2_oof[val_idx]))\n    print(\"Final LogLoss:\", log_loss(y_val, model_shap2_oof[val_idx]))\n\n    explainer = shap.TreeExplainer(model)\n    shap2_oof[val_idx] = explainer.shap_values(X_val)\n    shap2_test += explainer.shap_values(X_test) \/ K\n\n    print(f\"elapsed: {time.time()-start:.2f} sec\\n\")\n    \nshap2_oof = pd.DataFrame(shap2_oof, columns = [x+\"_shap\" for x in X.columns])\nshap2_test = pd.DataFrame(shap2_test, columns = [x+\"_shap\" for x in X_test.columns])\n\nprint(\"Final F1     :\", custom_f1(y, model_shap2_oof))\nprint(\"Final AUC    :\", roc_auc_score(y, model_shap2_oof))\nprint(\"Final LogLoss:\", log_loss(y, model_shap2_oof))","79491933":"train = pd.concat([train, shap1_oof], axis=1)\ntest = pd.concat([test, shap1_test], axis=1)\n\ntrain = pd.concat([train, shap2_oof], axis=1)\ntest = pd.concat([test, shap2_test], axis=1)","adfaae39":"predictor = TabularPredictor(label=\"y\",\n                             problem_type='binary',\n                             eval_metric=\"log_loss\",\n                             path='.\/AutoGlon\/',\n                             verbosity=1)\n\npredictor.fit(train, presets='best_quality', time_limit=60*5) # original: 60*60*7.5\n\nresults = predictor.fit_summary()","d7e1cea8":"# get final predictions\ny_oof = predictor.get_oof_pred_proba().iloc[:,1]\ny_pred = predictor.predict_proba(test).iloc[:,1]","4af4f045":"final_threshold = get_threshold(train.y, y_oof)\nfinal_threshold","cdbb34fe":"print(\"Final F1     :\", custom_f1(y, y_oof))\nprint(\"Final AUC    :\", roc_auc_score(y, y_oof))\nprint(\"Final LogLoss:\", log_loss(y, y_oof))","40ea45c7":"# Write predictions to sub\nsample_submission['predicted'] = np.where(y_pred>final_threshold, 1, 0).astype('int64')\nsample_submission.to_csv('autogluon_shap_sub.csv',index=False)","77417e37":"# Write predictions to stack\nsample_submission['predicted'] = y_pred\n\nsample_submission.to_csv('autogluon_shap_sub_probs.csv',index=False)\npd.DataFrame({'id':train.index, 'autogluon_shap_oof':y_oof}).to_csv('autogluon_oof.csv',index=False)","1874ab31":"pd.read_csv('..\/input\/porto-seguro-catboost-xgboost-shap-autogluon\/autogluon_shap_sub.csv')\\\n    .to_csv('sub.csv',index=False)","4d67c1a2":"# Remove log files\nimport shutil\nshutil.rmtree('.\/AutoGlon\/')","06eb7dc9":"**Note**: The results may vary given the stochastic nature of the algorithm or evaluation procedure.","f33c78e7":"## XGBoost","20c6f353":"# Sub final prediction\n--- ","d26b0d7f":"# Stage 3: Final Model (Autogluon)\n---","d6ed3c34":"# Import\n---","26b2468e":"# Exploratory Data Analysis\n\nsee more in <https:\/\/www.kaggle.com\/gomes555\/porto-seguro-r-an-lise-explorat-ria-dos-dados> \n\nTODO: add slide","abd0c8ea":"## Concatenate shap values with train\/test data","2f7483c6":"# Stage 1: Tune XGBoost\n---","cbecda10":"# Stage 2: Calcule OOF SHAP values\n---","039799db":"# Stage 0: Extract KNN features (k=1)\n---","911659f3":"<head>\n<title>Font Awesome Icons<\/title>\n<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n<link rel=\"stylesheet\" href=\"https:\/\/cdnjs.cloudflare.com\/ajax\/libs\/font-awesome\/4.7.0\/css\/font-awesome.min.css\">\n<\/head>\n\n<p style=\"color:red\">If you liked it, don't forget to upvote! \ud83e\udd18<\/p>\n\n- See portuguese (\ud83c\udde7\ud83c\uddf7 ) version with comments in my blog: <https:\/\/gomesfellipe.github.io\/post\/2021-11-01-solucao-final-porto-seguro-data-challenge\/>\n- See full solution on github (<i class=\"fa fa-github\"><\/i>): <https:\/\/github.com\/gomesfellipe\/porto_seguro_data_challenge>","98a1fa67":"# Porto Seguro Data Challenge\n---\n\n## Problem Definition\n\n**Objective**: \"In this competition you will be challenged to build a model that predicts the probability of purchasing a product.\"\n\n<div class=\"alert alert-warning\"> \n<big><strong>\u26a0\ufe0f Warning! <br><\/strong> <\/big>\n<div style=\"color: rgb(0, 0, 0);\">I called this notebook \"simplified solution\" because my final submission involves blending models and pseudolabels but, when the competition ended, I realized that this solution was simpler to implement and had a higher private result than the notebook I selected. \ud83d\ude05  \n<br> \n<br> \nHope you enjoy it!\n<\/div>\n<\/div>","90fa5ed8":"# References\n---\n\n- https:\/\/github.com\/momijiame\/gokinjo\n- https:\/\/www.kaggle.com\/melanie7744\/tps6-boost-your-score-with-knn-features\n- https:\/\/www.kaggle.com\/c\/otto-group-product-classification-challenge\/discussion\/14335\n- https:\/\/www.kaggle.com\/pavelvod\/gbm-supervised-pretraining\n","9e50adfe":"# Sub\n---","a8d8dce6":"\n# Load dependencies\n---","7f75f51c":"## CatBoost"}}