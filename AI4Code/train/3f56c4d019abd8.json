{"cell_type":{"7a7c66a3":"code","1812cfe1":"code","e13ee5d0":"code","a3a418cc":"code","d737f6fb":"code","167a9c11":"code","eb55de2d":"code","cffa2951":"code","8c40cbf9":"code","c62f616e":"code","e3650efc":"code","408a1390":"code","5e74d277":"code","77350c6d":"code","c101ab05":"code","4d12033e":"code","1d5001a1":"code","91f9f688":"markdown","d3e7e6c5":"markdown"},"source":{"7a7c66a3":"import numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\nfrom keras.optimizers import SGD\nimport math\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","1812cfe1":"def plot_predictions(test,predicted):\n    plt.plot(test, color='red',label='Real IBM Stock Price')\n    plt.plot(predicted, color='blue',label='Predicted IBM Stock Price')\n    plt.title('IBM Stock Price Prediction')\n    plt.xlabel('Time')\n    plt.ylabel('IBM Stock Price')\n    plt.legend()\n    plt.show()\n\ndef return_rmse(test,predicted):\n    rmse = math.sqrt(mean_squared_error(test, predicted))\n    print(\"The root mean squared error is {}.\".format(rmse))","e13ee5d0":"dataset = pd.read_csv('..\/input\/IBM_2006-01-01_to_2018-01-01.csv', index_col='Date', parse_dates=['Date'])\ndataset.head()","a3a418cc":"# Checking for missing values\ntraining_set = dataset[:'2016'].iloc[:,1:2].values\ntest_set = dataset['2017':].iloc[:,1:2].values","d737f6fb":"dataset[\"High\"][:'2016'].plot(figsize=(16,4),legend=True)\ndataset[\"High\"]['2017':].plot(figsize=(16,4),legend=True)\nplt.legend(['Training set (Before 2017)','Test set (2017 and beyond)'])\nplt.title('IBM stock price')\nplt.show()","167a9c11":"sc = MinMaxScaler(feature_range=(0,1))\ntraining_set_scaled = sc.fit_transform(training_set)","eb55de2d":"# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \nX_train = []\ny_train = []\nfor i in range(60,2769):\n    X_train.append(training_set_scaled[i-60:i,0])\n    y_train.append(training_set_scaled[i,0])\nX_train, y_train = np.array(X_train), np.array(y_train)","cffa2951":"print(y_train)","8c40cbf9":"# Reshaping X_train for efficient modelling\n\nX_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\nX_train.shape","c62f616e":"# LSTM architecture \n\nregressor = Sequential()\n\nregressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units=50, return_sequences=True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units=50))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units=1))\n\nregressor.compile(optimizer='rmsprop',loss='mean_squared_error')\n# Fitting to the training set\nregressor.fit(X_train,y_train,epochs=50,batch_size=32)","e3650efc":"# Now to get the test set ready in a similar way as the training set.\n# The following has been done so forst 60 entires of test set have 60 previous values which is impossible to get unless we take the whole \n# 'High' attribute data for processing\ndataset_total = pd.concat((dataset[\"High\"][:'2016'],dataset[\"High\"]['2017':]),axis=0)\ninputs = dataset_total[len(dataset_total)-len(test_set) - 60:].values\ninputs = inputs.reshape(-1,1)\ninputs  = sc.transform(inputs)","408a1390":"# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(60,311):\n    X_test.append(inputs[i-60:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\npredicted_stock_price = regressor.predict(X_test)\npredicted_stock_price = sc.inverse_transform(predicted_stock_price)","5e74d277":"plot_predictions(test_set,predicted_stock_price)","77350c6d":"# Evaluating our model\nreturn_rmse(test_set,predicted_stock_price)","c101ab05":"regressorGRU = Sequential()\n\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n\n\nregressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(X_train.shape[1],1), activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n\nregressorGRU.add(GRU(units=50, activation='tanh'))\nregressorGRU.add(Dropout(0.2))\n\nregressorGRU.add(Dense(units=1))\n\nregressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n# Fitting to the training set\nregressorGRU.fit(X_train,y_train,epochs=50,batch_size=150)\n","4d12033e":"# Preparing X_test and predicting the prices\nX_test = []\nfor i in range(60,311):\n    X_test.append(inputs[i-60:i,0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\nGRU_predicted_stock_price = regressorGRU.predict(X_test)\nGRU_predicted_stock_price = sc.inverse_transform(GRU_predicted_stock_price)","1d5001a1":"plot_predictions(test_set,GRU_predicted_stock_price)","91f9f688":"## RNN is used to deal with Sequential Data","d3e7e6c5":"# Gated Recurrent Units GRU "}}