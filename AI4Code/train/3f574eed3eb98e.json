{"cell_type":{"e7e71df8":"code","84c022e4":"code","785bc427":"code","2088f7e0":"code","44f909aa":"code","6de1e8a7":"code","d7ea6025":"code","1f1f1a66":"code","0d955041":"code","14cde3fe":"code","0a5dbe53":"code","e22e7878":"code","49928800":"code","d4fe6a2d":"code","4234b632":"code","02bd79a1":"code","cf4ef0fd":"code","c2083589":"code","6b7fe158":"markdown","77e7491b":"markdown","f7f94815":"markdown","63d67a3c":"markdown","5e667c7c":"markdown","9fb0e911":"markdown","943d41b2":"markdown","28e64e66":"markdown","5eb0fc2b":"markdown","01e22b94":"markdown","65040c37":"markdown","b0c818fc":"markdown","6cf7abde":"markdown","985488af":"markdown","090ec653":"markdown","35d96357":"markdown","d9310c32":"markdown","e8c8e817":"markdown","3e957d48":"markdown","ab60a9bf":"markdown","629192fe":"markdown","bf73046f":"markdown","b5789a28":"markdown","6e1853b5":"markdown","420f5dd3":"markdown","a00e4498":"markdown","d8182cc4":"markdown","6987fb5d":"markdown","217de083":"markdown"},"source":{"e7e71df8":"#import Python libraries used in this lecture\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import metrics\nimport numpy as np","84c022e4":"df = pd.read_csv(\"..\/input\/lecture8data1\/data13.1.csv\", usecols =['Age(x)','Disease(y)']) # use selected columns\ndf","785bc427":"import seaborn as sns\nsns.scatterplot(data=df, x='Age(x)', y='Disease(y)')","2088f7e0":"sns.regplot(data=df, x='Age(x)', y='Disease(y)', logistic=True,ci=None)","44f909aa":"df2 = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf2.head(5)","6de1e8a7":"df2 = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf2.head(5)","d7ea6025":"import seaborn as sns\nsns.scatterplot(data=df2, x=\"BMI\", y=\"Age\", hue=\"Outcome\")","1f1f1a66":"X = df2.iloc[:,0:8] #stop is excluded\ny = df2.iloc[:,8] ","0d955041":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=4)","14cde3fe":"#normalisation\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \nscaler.fit(X_train)\nX_train = scaler.transform(X_train) \nX_test = scaler.transform(X_test)","0a5dbe53":"# Create LogisticRegression object\nclf = LogisticRegression(max_iter =2000, solver ='liblinear',penalty='l1')\n# Train LogisticRegression Classifer\nclf = clf.fit(X_train,y_train)\nclf.get_params()","e22e7878":"\n#Predict the response on test data\ny_pred = clf.predict(X_test) \nprint(\"predict\\n\", y_pred[0:3])\nprint(\"actual\\n\", y_test[0:3]) #output includes the index from the original data\n# Model Accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","49928800":"prob = clf.predict_proba(X_test)\nprint(prob[0:10])","d4fe6a2d":"df3 = pd.DataFrame(zip(X.columns, np.transpose(clf.coef_.tolist()[0])), columns=['features', 'coefficient']) # create a dataframe\ndf3 = df3.append({'features':'intercept','coefficient' : clf.intercept_.tolist()[0]}, ignore_index=True) # append a new row\ndf3","4234b632":"a = (\"A\", \"B\", \"C\")\nb = (\"1\", \"2\", \"3\")\nc = zip(a, b)\nprint(tuple(c))","02bd79a1":"import numpy as np\nimport matplotlib.pyplot as plt\nx = np.linspace(-10, 10, 100) \ny = np.exp(x)\/(1 + np.exp(x)) \nplt.plot(x, y) \nplt.xlabel(\"x\") \nplt.ylabel(\"Sigmoid\") \nplt.grid(True) \nplt.show()","cf4ef0fd":"# code for calculate the above value\nimport math\nlogbit = math.exp(0.29*50-18.17)\nP = logbit\/(1+logbit)\nprint(P) ","c2083589":"# calculate odds for probability =0.80\nprint(\"probability:\", 0.80)\nprint(\"odds:\", 0.80\/(1-0.80))\nprint(\"log-odds:\", np.log(0.80\/(1-0.80)))","6b7fe158":"### Build a logistic regression model\n* Create a logistic regression object (classifier)\n* Train the classifer on training data `fit(X_train,y_train)`\n* Print parameters used in the classifier","77e7491b":"* Split dataset into training  data and test data\n    * 70% records for training\n    * 30% records for test","f7f94815":"## Part 1 What is Logistic Regression?\n### Example: Classification \n* Disease presence or not \n    * A data set includes 20 patients\n    * A binary classification problem: decide the presence\/absence of disease  based on patients\u2019 age\n* Target (response) variable: `disease(y)` \n    * 0 standing for no\n    * 1 for yes\n* Predicator: `Age(x)` with **numerical values** in the range [25, 84] ","63d67a3c":"* Print the probability of the first ten patietns' absence or presense of diabeties","5e667c7c":"### Parameters in Logistic Regression\n* `max_iter`: the maximum number of iterations for a solver  to iterate\n* `penalty`:  Used to penalize large coefficients $\\beta_0, \\cdots, \\beta_n$ and improve the performance of a model \n    $$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x_1+ \\cdots + \\beta_n x_n}}{1+e^{\\beta_0 +\\beta_1 x+\\cdots+\\beta_n x_n}}$$\n    * **L1 regularization** penalizes large coefficients with the scaled sum of the absolute values of the weights: $|\\beta_0|+\\cdots+|\\beta_n|$\n    * **L2 regularization** penalizes large coefficients with the scaled sum of the squares of the weights: $|\\beta_0|^2+\\cdots+|\\beta_n|^2$\n    * **Elastic-net regularization** is a linear combination of L1 and L2 regularization.\n* `solver`: used for fitting the model in logistic regression\n    * For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones.\n    * \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018saga\u2019 handle L2 or no penalty\n    * \u2018liblinear\u2019 and \u2018saga\u2019 also handle L1 penalty\n    * \u2018saga\u2019 also supports \u2018elasticnet\u2019 penalty\n    * \u2018liblinear\u2019 does not support setting penalty='none'\n* `n_job`: Number of CPU cores used when parallelizing over classes\n    * -1 means using all processors ","9fb0e911":"### Print coefficients of the logistic regression Curve\n$$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x_1+ \\cdots + \\beta_n x_n}}{1+e^{\\beta_0 +\\beta_1 x+\\cdots+\\beta_n x_n}}$$\n* Coefficients are $\\beta_1, \\cdots, \\beta_n$\n    * weights on input variables (features) `Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age`\t\n* Intercept: $\\beta_0$","943d41b2":"## Part 3 Explanation of Logistic Regression\n### Review: Sigmoid Function\n* Explanation of Logistic Regression needs some mathematical knowledge. Part 3 aims at providing a bit more complex mathematical understanding of logistic regression\n* **Sigmoid function** is an important function in machine learning, with S-shaped curve or sigmoid curve.\n$$\n\\mbox{Sigmoid}(x) =  \\frac{e^{x}}{1+e^{x}}\n$$\n* Properties of sigmoid function:\n    * S-shape with values between 0 and 1.\n    * When $x \\to -\\infty$, $\\mbox{Sigmoid}(x) \\to 0$.\n    * When $x \\to +\\infty$, $\\mbox{Sigmoid}(x) \\to 1$ ","28e64e66":"### Maximum Likelihood Function for Logistic Regression\n* What is the likelihood function for logistic regression?\n ![image.png](attachment:image.png)\n* Observation $H$ has  a probability of positive response $y=1$:\t\n$$\\pi (H) = Prob(y=1 | H)$$\n* Observation $G$ has a probability of negative response $y=0$: \n$$1 \u2013 \\pi(G) = Prob(Y=0 | G)$$\n* An observation  with positive response $y =1$ contributes $\\pi(x) = [\\pi(x)]^1$ to likelihood\n* An observations with negative response $y =0$ contributes $1-\\pi(x)=[1-\\pi(x)]^{1-0}$ to likelihood\n* As $y  =0$ or $1$, the contribution to the likelihood of the observation  $x$ is \n$$\n[\\pi(x )]^{y } [1-\\pi(x )]^{(1-y )}\n$$\n* The likelihood function over $m$ observations is a product\n$$\nL(\\beta \\mid X) =   [\\pi(x_1)]^{y_1} [1-\\pi(x_1)]^{(1-y_1)} \\times \\cdots \\times [\\pi(x_m)]^{y_m} [1-\\pi(x_m)]^{(1-y_m)}\n$$\n","5eb0fc2b":"### Logistic Regression Curve\n* What is the logistic regression curve?\n* Logistic regression curve is  a sigmoid function\n$$\n\\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x}}{1+e^{\\beta_0 +\\beta_1 x}}\n$$\n    * $x$ is the observed value of predictor \n    * $e$ is  a famous mathematical constant  = $2.71828182845904523536\\cdots $\n    * coeffcients $\\beta_0, \\beta_1$ controls the curve shape ","01e22b94":"## Part 2 Case Study: Implement Logistic Regression for Classification with Python\n### Data set\n* [Pima Indians Diabetes Database](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database?select=diabetes.csv):   to  predict whether or not a patient has diabetes\n* In particular, all patients are females at least 21 years old of Pima Indian heritage.\n* One target variable: \n    * `Outcome` \n    * 0 for no\n    * 1 for yes \n* Predictor variables: \n    * `Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin,\tBMI, DiabetesPedigreeFunction, Age`\t\n* All data take numerical values\n","65040c37":"## Summary\n* Logistic regression is to find a sigmoid function which describes the relationship between predictor variables and target variable (binary) \n* The logistic regression function can be explained as the probability of positive response: `Outcome=1` \n* Training in logistic regression is to find a logistic regression curve best fitting observations  \n* Sklean provides high-level implementation of logistic regression\n* Logistic regression uses maximum likelihood estimation  ","b0c818fc":"# Logistic Regression\nCOMP20121 Machine Learning for Data Analytics\n\nAuthor: [Jun He](https:\/\/www.ntu.ac.uk\/staff-profiles\/science-technology\/jun-he) ","6cf7abde":"### Evaluate the model\n* Predict the label of patients in test data set\n* Calculate the accuracy of prediction","985488af":"### Logistic Regression Model\n* Model: \n    * Logistic regression is to find a S-shape curve between predictor `Age(x)` and target variable `Disease(y)` based on observations\n* Logistic regression curve is represented by a **mathematical function**\n    $$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x}}{1+e^{\\beta_0 +\\beta_1 x}}$$ \n    * $\\beta_0, \\beta_1$ are two parameters which determines the shape of the curve. They values are automatically learned by the model (not hyperparameter)\n    * $\\beta_0$ is an intercept\n    * $\\beta_1$ is a weight on the input: explained in Part 3\n    * $e$ is Euler's constant = $2.71828182845904523536\\cdots$\n  \n* The value $\\pi(x)$ is explained as the probability with the disease for record $x$\n    * The higher $\\pi(x)$, the more likely disease, while the smaller $\\pi(x)$, the less likely disease\n    * For example, given a patient with $age =50$, $\\pi(50) = 0.025$ means the probability of the patient with the disease presence = 0.025 or 2.5% ","090ec653":"### Interpreting Logistic Regression Curve\n![image.png](attachment:image.png)\n* $\\pi(x)$ is interpreted as  the  **probability  of positive  `outcome = 1`**\n    * probability disease (positive outcome) present for records $x$ \n* $1 \u2013 \\pi(x)$ is interpreted as the **probability of negative `outcome = 0`**\n    * probability disease (positive outcome) not present  \n* For the example, from \n$$\n\\pi (x) =\\frac{e^{0.29*age-18.17}}{1+ e^{0.29*age-18.17}}$$\n* we know the probability of 50-year old patient having disease is = 2.5%\n$$\n\\pi (x) =\\frac{e^{0.29*50-18.17}}{1+ e^{0.29*50-18.17}}=0.025$$","35d96357":"### Interpret Logistic Regression Model: Odds of Success\n* The probability of 70-year old having disease  is 80%, and the probaility of not having disease is 1-80% = 20%\n* **Odds of success**  is the ratio between the probabilities of positive and negative events\n* Odds for 50-year old patient with disease is\n$$\n \\frac{\\pi(x)}{1\u2212\\pi(x)}=  \\frac{0.80}{1-0.80}=4\n$$\n    * Odds = 1 when an event just as likely to occur 50%  to 50%\n    * Odds > 1, when an event more likely to occur than not\n    * Odds < 1, when an event less likely to occur than not\n* Probability ranges from $0$ to $1$, however odds ranges from $0$ to  $+\\infty$\n* Odds indicates how much more likely event occurs versus not occurring\n* **Log-odds** is a log-scale value of odds\n$$\n \\log( \\frac{\\pi(x)}{1\u2212\\pi(\ud835\udc65)})\n$$","d9310c32":"### Exploratory data analysis  \n* Understand potential relationship between patient age and presence\/absence of disease\n* Use Seaborn scatterplot `Age(x)` and `Disease(y)`\n* Finding:\n    * the older, the more likely presence of disease\n    * the younger, the more likely absence of disease \n","e8c8e817":"### Review: Probability\n|<img src =\"https:\/\/mrnussbaum.com\/uploads\/\/ed416974dd9ab7cc138f9baae013f541.jpg\" width =300>|\n|:--:|\n|[Probability](https:\/\/mrnussbaum.com\/uploads\/\/ed416974dd9ab7cc138f9baae013f541.jpg)||\n\n\n* **Probability** describes how likely an event is to occur\n    * For example, $P(red)$ denotes the probability of the event $red$ occurs\n* The probability of an event takes a value between 0 and 1\n    * 0 means the event will not happen \n    * 1 means the event always happen\n    * The higher the probability of an event, the more likely it is that the event will occur \n* Conditional probability\n    * refers to the probability of one event occurring with some relationship to one or more other events\n    * For example, $P(red \\mid blue)$ refers to the probability of event $red$ following  event $blue$","3e957d48":"### Training (= called fitting in Sklearn)\n* The process of finding the best logistic regression curve fitting observations is called model training or fitting \n* Question: How to derive the curve best fitting observations? Discussion will be given in Part 3\n* **Solver**: it is an optimization algorithm used for fitting the model (i.e., find the best logistic regression curve).  There are many solvers available.\n    * The training is an iterative process in logistic regression\n    * A solver stops search when the maximal number of iterations is reached\n![image.png](attachment:image.png)","ab60a9bf":"### Maximum Likelihood Estimation for Logistic Regression\n* Maximum Likelihood Estimation aims at finding the parameters $\\beta=(\\beta_0, \\beta_1)$ or $(\\beta_0, \\beta_1,\\cdots, \\beta_n)$ which maximizes the likelihood function based on $m$ observations   \n$$\n\\max L(\\beta \\mid X) \n$$ \n    * $X$ are observations\n    * $\\beta$ are parameters to be optimised\n\n\n|<img src=\"https:\/\/www.weibull.com\/hotwire\/issue148\/ht148-13.png\" width=300>|\n|:--:|\n|[A likelihood surface. The task is to find the location where the likelihood takes the maximum value.](https:\/\/www.weibull.com\/hotwire\/issue148\/ht148-13.png)|\n\n* Different algorithms (called **solvers**) have been proposed for searching the optimal values of $\\beta$\n* The search process is an iteration. So the **number of maximum iteration** is needed to stop the iteration.","629192fe":"### Multi-Variate Logistic Regression\n* Multi-variate logistic regression: multi-input variables $x_1, \\cdots, x_n$. \n* Example: Diabetes data set\n    * Several input variables: `Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction , Age`\n    * One output variable: `Outcome` with two labels 0 (no diabetes) and 1 (yes diabetes)\n* Multi-Variate Logistic Regression is to find the best logistic regression function between the output $y$ and input $x_1, \\cdots, x_n$\n$$ \\pi(x) = \\frac{e^{\\beta_0 +\\beta_1 x_1+ \\cdots + \\beta_n x_n}}{1+e^{\\beta_0 +\\beta_1 x+\\cdots+\\beta_n x_n}}$$\n    * $\\pi(x)$ is the probability of outcome being 1\n    * $\\beta_0$ is an intercept\n    * $\\beta_1, \\cdots, \\beta_n$ are weights on the input $x_1, \\cdots, x_n$ respectively","bf73046f":"#### Normalize training data then test data\n* Normalize training data `X_Train` to the range $[0,1]$\n* Create `MinMaxScaler` and fit it with training data `X_Train`\n* Apply the **same scaler** to `X_test`\n* **No need to normalize target variable `y` because they represent class labels**","b5789a28":"### Maximum Likelihood Estimation\n![image.png](attachment:image.png)\n* Given some observations, logistic regression is to find the  curve with parameters $\\beta$     best fitting observations $X$\n* **Maximum Likelihood Estimation** (MLE) is used to implement this task\n* MLE is a machine learning technique for estimating parameters of a model  \n* The goal of maximum likelihood estimation is to maximize  a **likelihood** function of observing the data given the model parameters $\\beta$\n$$ L(  \\beta \\mid X)$$\n* We cannot discuss maximum likelihood estimation in depth in this module","6e1853b5":"### Prepare data\n* Split dataset in predictor and target variables","420f5dd3":"### Exploratory data analysis\n* EDA can help understand data\n* Understand the relationship between two predictor variables and the target variables\n* For example, scatter plot `BMI`, `Age`, and `Outcome`\n* Finding 1: if  a patient is older and has a higher BMI, the more likely the disease presence\n* Finding 2: Outliers: BMI value is 0 for some patients. How to handle outliers? In this case study, nothing is done to handle this issue for the sake of simplicity","a00e4498":"### Review: Mathematical Functions\n* A mathematical function is written in the form $y=f(x_1, \\cdots, x_n)$\n    * For example, $ y =x_1 + x_2+\\cdots+ x_n$\n    * $x_1, \\cdots, x_n$ are  **independent variables** (input) with numerical values\n    * $y$ is a **dependent variable** (output) with numerical values.\n* A machine learning model can be regarded as a function\n    * input values of $n$ predictors (features) $x_1, \\cdots, x_n$\n    * output a class label $y$\n    \n    ![image.png](attachment:image.png)","d8182cc4":"### Interpret Coefficients in Logistic Regression Curve\n* **Odds of success**\n$$\nOdds =   \\frac{\\pi(x)}{1\u2212\\pi(\ud835\udc65)}\n$$\n* **Log-odds** for one independent variable $x$\n$$\ng(x) =\\ln (\\frac{\\pi(x)}{1-\\pi(x)}) = \\beta_0 +\\beta_1 x\n$$\n* **Coefficient** $\\beta_1$ represents how much the log-odds of success changes when the input variable $x$ increases by a unit\n$$\n\\beta_1 =g(x+1) -g(x)\n$$\n* Coefficient $\\beta_1$ can be regarded as a **weight** on the input variable $x$\n    * The larger $\\beta_1$, the bigger weight\n","6987fb5d":"### Review: Zip Function in Numpy\n* Join two tuples together\n* Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables","217de083":"## Learning objectives\n* understand the logistic regression model\n* implement logistic regression for classification with Sklearn"}}