{"cell_type":{"3042a745":"code","654f02c1":"code","1700c714":"code","6e8cda68":"code","7a88f76d":"code","c31dce1b":"code","392f33cb":"code","4f77f912":"code","6b1e9ef1":"code","c866e03b":"code","797ea9d1":"code","858edd79":"code","b1154acd":"markdown","cfc21498":"markdown","ade23dcd":"markdown","32c38148":"markdown","a2044ba1":"markdown","665dadc8":"markdown","57e752de":"markdown","f8e31520":"markdown","7ad795f8":"markdown","db297627":"markdown","ae45ef65":"markdown","fff53819":"markdown"},"source":{"3042a745":"import sys\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")\n!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n\nfrom transformers import *\n\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.notebook import tqdm\nfrom math import floor, ceil\n\nimport tensorflow as tf\nprint(tf.__version__)","654f02c1":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nsub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","1700c714":"albert_path = '..\/input\/albertlargev2huggingface\/'\ntokenizer = AlbertTokenizer.from_pretrained(albert_path, do_lower_case=True)\nalbert_model = TFAlbertModel.from_pretrained(albert_path)\n\n#bert_path = '..\/input\/bert-base-uncased-huggingface\/'\n#tokenizer = BertTokenizer.from_pretrained(bert_path+'vocab.txt', do_lower_case=True)\n#bert_model = TFBertModel.from_pretrained(bert_path)","6e8cda68":"print(\"Data cleaning started........\")\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    #text = text.lower().split()\n    #stops = set(stopwords.words(\"english\"))\n    #text = [w for w in text if not w in stops]    \n    #text = \" \".join(text)\n    return(text)\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","7a88f76d":"columns = ['question_title','question_body','answer']\ntrain= clean_data(train, columns)\ntest = clean_data(test, columns)\nprint(\"Data cleaning Done........\")","c31dce1b":"def _get_segments(tokens,_maxlen):\n    sentences_segments = []\n    i=0\n    for token in tokens:\n        sentences_segments.append(i)\n        if token == \"[SEP]\":\n            i += 1\n    return sentences_segments + [0] * (_maxlen - len(tokens))\n\ndef _get_inputs(df,_maxlen,tokenizer,use_keras_pad=False):\n    \n    #generate input ids  \n    maxqnans = np.int((_maxlen-34)\/2) #approx token size : qn = 230, ans = 230, title =30, special_words = 4\n    pattern = '[^\\w\\s]+|\\n' # remove everything including newline (|\\n) other than words (\\w) or spaces (\\s)\n    \n    sentences = [[\"[CLS] \"] + tokenizer.tokenize(title)[:20]+ [\"[SEP]\"]\n                  + tokenizer.tokenize(qn)[:maxqnans] + [\"[SEP]\"]\n                  + tokenizer.tokenize(ans)[:maxqnans] + [\"[SEP]\"]\n                  for (title,qn,ans) \n                in \n              zip(df['question_title'].str.replace(pattern, '').values.tolist(),\n              df['question_body'].str.replace(pattern, '').values.tolist(),\n              df['answer'].str.replace(pattern, '').values.tolist())]\n    \n    \n    # if less than max length provided then the words are padded\n    if use_keras_pad:\n        sentences_padded = pad_sequences(sentences, dtype=object, maxlen=_maxlen, value=['[PAD]'],padding='post')\n    else:\n        sentences_padded = [tokens + [\"[PAD]\"]*(_maxlen-len(tokens)) if len(tokens)!=_maxlen else tokens for tokens in sentences ]\n\n    sentences_converted = [tokenizer.convert_tokens_to_ids(s) for s in sentences_padded]\n    \n    \n    #generate masks\n    # bert requires a mask for the words which are padded. \n    # Say for example, maxlen is 100, sentence size is 90. then, [1]*90 + [0]*[100-90]\n    sentences_mask = [[1]*len(tokens)+[0]*(_maxlen - len(tokens)) for tokens in sentences]\n \n    \n    #generate segments\n    # for each separation [SEP], a new segment is converted\n    sentences_segments = [_get_segments(tokens,_maxlen=_maxlen) for tokens in sentences]\n\n    genLength = set([len(tokens) for tokens in sentences_padded])\n    if _maxlen < 20:\n        raise Exception(\"max length cannot be less than 20\")\n    elif len(genLength)!=1: \n        print(genLength)\n        raise Exception(\"sentences are not of same size \\n {}\".format(genLength))\n\n    #convert list into tensor integer arrays and return it\n    #return sentences_converted,sentences_segment, sentences_mask\n    #return sentences\n    return [np.asarray(sentences_converted, dtype=np.int32), \n            np.asarray(sentences_mask, dtype=np.int32), \n            np.asarray(sentences_segments, dtype=np.int32)]\n    #return [tf.cast(sentences_converted,tf.int32), tf.cast(sentences_segment,tf.int32), tf.cast(sentences_mask,tf.int32)]","392f33cb":"MAX_SEQUENCE_LENGTH = 512\nXtr = _get_inputs(train,_maxlen=MAX_SEQUENCE_LENGTH,tokenizer=tokenizer,use_keras_pad=False)\nytr = np.asarray(train.iloc[:,11:])","4f77f912":"Xte = _get_inputs(test,_maxlen=MAX_SEQUENCE_LENGTH,tokenizer=tokenizer,use_keras_pad=False)","6b1e9ef1":"from tensorflow.keras.layers import Dense, Dropout,Embedding, LSTM, Bidirectional, Input, Dropout, GlobalAveragePooling1D\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K\n\nfrom sklearn.model_selection import KFold, GroupKFold\nfrom scipy.stats import spearmanr\n\nimport warnings; warnings.simplefilter('ignore')","c866e03b":"def build_model():\n    \n    token_inputs = Input((MAX_SEQUENCE_LENGTH), dtype=tf.int32, name='input_word_ids')\n    #mask_inputs = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    #seg_inputs = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n\n    # going with pooled output since seq_output results in ResourceExhausted Error even with GPU\n    _,pooled_output = albert_model(token_inputs)#[token_inputs, mask_inputs, seg_inputs])\n    #X = GlobalAveragePooling1D()(pooled_output)\n    X = Dropout(0.2)(pooled_output)\n    output_= Dense(30, activation='sigmoid', name='output')(X)\n\n    #bert_model2 = Model([token_inputs, mask_inputs, seg_inputs],output_)\n    bert_model2 = Model(token_inputs,output_)\n    \n    print(bert_model2.summary())\n    \n    bert_model2.compile(optimizer= Adam(learning_rate=0.0001), loss='binary_crossentropy')\n    \n    return bert_model2","797ea9d1":"i=0\nnum_folds = 3\n#kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\ngkf = GroupKFold(n_splits=3).split(X=train.question_body, groups=train.question_body)\nfold_score = []\n#test_preds = np.zeros((Xte[0].shape[0],ytr.shape[1])) # mimic rows shape of test data, columns shape from train since, test will have any column for outputs\ntest_preds = []\n\n#for train_index,val_index in kfold.split(ytr):\nfor fold, (train_index, val_index) in enumerate(gkf):\n\n    i= i+1\n    print('executing fold no: {}'.format(i))\n    \n    K.clear_session()\n    # train_index gets a random sample of rows for training\n    # Xtr is a list contains 3 np arrays - ids, masks, segments so, using list comprehension to get the splits\n    Xtr_fold = [arr[train_index] for arr in Xtr]\n    ytr_fold = ytr[train_index]\n    \n    Xtr_val = [arr[val_index] for arr in Xtr]\n    ytr_val = ytr[val_index]\n    \n    model = build_model()\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    #model.fit(Xtr_fold,ytr_fold,epochs=5,batch_size = 8,validation_data = (Xtr_val,ytr_val),callbacks=[es])\n    model.fit(Xtr_fold[0],ytr_fold,epochs=5,batch_size = 4 ,validation_split = 0.2,callbacks=[es])\n    \n    #calculate spearman score\n    pred_ = pd.DataFrame(model.predict(Xtr_val[0]))\n    val_ = pd.DataFrame(ytr_val)\n    spearman_score = np.nanmean([spearmanr(pred_.iloc[:,i].values,val_.iloc[:,i].values).correlation for i in np.arange(len(pred_.columns))])\n    print(\"Spearman Score on validation data : {}\".format(spearman_score))\n    fold_score.append(spearman_score)\n    test_preds.append(model.predict(Xte))\n\nprint(\"Spearman Score on validation data : {}\".format(np.mean(fold_score)))\n","858edd79":"#average of across arrays by row\nsub.iloc[:, 1:] = np.average(test_preds,axis=0)\nsub.to_csv('submission.csv', index=False)","b1154acd":"### Text Cleaning:  Click on Expand to see the full cleanup code","cfc21498":"****Actual Colab Notebook with an extended version on how to implement using tfhub too: https:\/\/colab.research.google.com\/drive\/1IubZ3T7gqD09ZIVmJapiB5MXUnVGlzwH#scrollTo=MFMsIbmadulU\n\n***Colab version implementing DistilBert***\nhttps:\/\/colab.research.google.com\/drive\/16QC5JftxtJTR6HiLll9pcnJSmZqc5HhZ\n\n***Kaggle Internet Off Version using Huggingface for Albert\/DistilRoberta\/Robertalarge***  \nhttps:\/\/www.kaggle.com\/stitch\/albert-huggingface-internet-off-i  \nhttps:\/\/www.kaggle.com\/stitch\/distilroberta-robertalarge-internet-off-i  \n\nThanks to @returnofsputnik for pointing out huggingface.\n\nArticle explaining my understanding BERT and the common issues with using tf2.0:\nhttps:\/\/medium.com\/@muralim1585\/bert-in-keras-tensorflow-2-0-using-tfhub-huggingface-81c08c5f81d8\n\nPlease consider upvoting if you find this useful. This notebook to some extent is an adaptation of \n[akensert](https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-minimalistic). Plus, [urvishp80](https:\/\/www.kaggle.com\/urvishp80\/quest-encoding-ensemble).  Please consider upvoting that post too. Thanks to @abhishek for figuring how to use transformers offline.","ade23dcd":"## Add Pre-trained model using Transformer\nMuch has been said about these pretrained models but, the important concepts around transfer learning meaning,  how we can use it required some bit of digging. For example, if you had used tfhub version of bert. One might wonder, what its the \"trainable\" in the layer?\n\nIt turns out there are three ways that we can do transfer learning of Pretrained models:  \n- Feature Extraction : where the pretrained layer is used to only extract features like using batchnormalization to convert the weights into a range between 0 to 1 with mean being 0. In this method, the weights are not updated during back propagation. This what is marked as non-trainable in the model summary and in tfhub as trainable=False. If you would like to set it manually. then, you select the layer like this: model.add(dense(100, trainable=False)) \n- Fine Tuning: Sort of what this entire competition is about. BERT is ideal for this task because it trained for question answering. So, we just have to fine tune the model to suit our purpose. What it means - the layer has been trained for a general dataset. we need to retrain to optimize for our specific task. This again, in TFHUB is trainable = True. In Keras Model Summary, its called out as trainable parameters. Usually this is huge. Albert which is the light version contains 11 million parameters. \n- Extract Layers: this is another way where in we extract only those layers required for task for example, we might extract just the lower levels layers in BERT to carry out tasks like POS, sentiment analysis, etc where only extracting word level features would be enough and too much of context or sequence matching is not required.\n\n***Important Note***  \nGiven the crazy about of parameters amount to be tuned in less than 2 hrs + the BERT paper recommendation is adopt:  \n- Epochs - range between 3,4  \n- Batch_size - 4,8,16 (if you training for specific groups like stackoverflow then might be 32)  \n- Layers - You may not have add any additional layers apart from the output & averaging one. Since BERT or equivalent has already has optimized the layres & hidden units for us. I did try adding adding a layer for vanishing gradients - these deep belief networks can dwindle the weights to almost zero so, i had added a leaky relu layer to slow weight deterioration. The model run time went up crazily.  \n- Custom_Callback (from BERT minimalistic)- beware that this function is doing model.predict on validation and test data for each epoch which, will considerably slow down your model completion. although its nice to see the score improvement at each epoch. My suggestion is try to calculate at the end of every fold and not epoch.  ","32c38148":"1. Go to the github page of huggingface transformers - source - link -https:\/\/github.com\/huggingface\/transformers\/tree\/master\/src\/transformers\n2. Open the relevant config, modeling or tokenization. In this case, for example it will be configuration_albert.py\n3. Under Config_Archive_Map list, you will find aws source its downloading the required files\n4. You will require, one config.json, spiece.model (if sentence piece is being used) or vocab.txt\/vocab and tf_model.h5 (for keras)\n5. Voila ! download file and add it as a dataset to your kernel.\nYou can use the dataset - https:\/\/www.kaggle.com\/stitch\/albertlargev2huggingface","a2044ba1":"I am new to this class of Pre-trained models - BERT, ROBERTA, ELMO, etc. I have tried my best to understand and implement things my own way. It's been fun to work on this problem. But, it is hard to hold on to the learning after kaggle so, i am documenting the learnings for as a reference. I hope this notebook will benefit the ones for who are new to this and everything sounds like excellent gibberish. Hopefully,it sheds light on few things for them.\n\nOtherwise too, I was able to get an easy way of implementing models from huggingface with internet off. If you are interested in only this. Please refer to the below link - under kaggle internet off version.","665dadc8":"Xtr_holdout = [arr[:4800] for arr in Xtr]\nytr_holdout = ytr[:4800]\nXtr_val = [arr[1200:] for arr in Xtr]\nytr_val = ytr[1200:]\n\nprint(Xtr_holdout[1].shape,\" \", ytr_holdout.shape)\n\nmodel = build_model(\nes = EarlyStopping(monitor='val_loss', min_delta=0, patience=1, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\nmodel.fit(Xtr_holdout,ytr_holdout,epochs=5,batch_size = 8,validation_split = 0.2,callbacks=[es])\n    \npred_ = pd.DataFrame(model.predict(Xtr_val,batch_size=8))\nval_ = pd.DataFrame(ytr_val)\nspearman_score = np.nanmean([spearmanr(pred_.iloc[:,i].values,val_.iloc[:,i].values).correlation for i in np.arange(len(pred_.columns))])\nprint(spearman_score)","57e752de":"To avoid overfitting and promote generalization - Implementing KFold Validation & Early Stopping & Dropout & L2 Regualizer (Ridge: pushes weights closer to zero not zero)\n\nI am not going expand on the concepts here but, will write a separate article and share the link at later point","f8e31520":"### Convert text into Bert required format of input_ids, masks, segments","7ad795f8":"## Create train and test data","db297627":"# KFold & Group Fold Validation","ae45ef65":"## Preprocessing:  \n\nFrom a little EDA, it looked like the columns question_title, question_body and answer are the important ones. If I equate it to squad dataset then, question_title is the question, question_body is the context and answer is answer. The average length of title is 30, body is 200 or less and answer is 200 or less. So, the max_sequence_length of 512 works nicely as we can acommodate the above representation very well.\n\nThis involves two steps:\n\n- The text contains quite bit of punctuations, intent notes (eg: \"read:\"), math formulas and new line spaces. This needs to be cleaned. For this I have used : https:\/\/www.kaggle.com\/urvishp80\/quest-encoding-ensemble\n- The question and answer needs to be converted into bert tokens. More on this has been briefed in the medium article. The code that has been used though has been broken into functions and for a new person its hard to follow. To make it easy, I had created another version using only list comprehensions in a sequence of generate inputs, masks and segments. But, its not helping performance. So, its better to adopt this function as it is - from BERT Minmalistic. Here is a map of how its connected:  \n    -  Trim_inputs is used to make sure that the token counts do not cross the threshold\n    - convert_to_* is fairly self explanatory\n\n![image.png](attachment:image.png)","fff53819":"## Leave One Out Validation - about 20% of data"}}