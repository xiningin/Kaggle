{"cell_type":{"93ca95c1":"code","704dfdf8":"code","9255c7d5":"code","14b2efca":"code","aeec74d5":"code","c3202083":"code","d9f72481":"code","391086f6":"code","184c1992":"code","157b4ccc":"code","fcb10648":"code","3837b917":"code","db0fbca9":"code","696333c9":"code","e77322e7":"code","71fd243c":"code","8603f70c":"code","8695c1fb":"code","4d8ed17a":"code","ee5f6cd7":"code","4a4fdad7":"code","e0528a3d":"code","cf27dac7":"code","c6efb542":"markdown","d877ff30":"markdown","0e9db786":"markdown","dbb6f530":"markdown","d2f7eede":"markdown","e426bf7e":"markdown","926cc8ac":"markdown","e57f6e4e":"markdown","3d51eef8":"markdown","831ead4d":"markdown"},"source":{"93ca95c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df = pd.read_csv(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","704dfdf8":"df.head()","9255c7d5":"df.isnull().sum()","14b2efca":"df.quality.unique()","aeec74d5":"df['quality'].dtypes","c3202083":"df[['quality']] = df['quality'].apply(lambda x : (x - x) if int(x) < 6 else ((x - x) + 1))","d9f72481":"df","391086f6":"#Splitting y and X\ny = df['quality']\nX = df.drop('quality', axis = 1)","184c1992":"y","157b4ccc":"#Splitting into test and train data\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","fcb10648":"from sklearn.tree import DecisionTreeClassifier\nd_model = DecisionTreeClassifier()\ncriterion = ['gini', 'entropy']\nmax_depth = [1, 3, 5, None]\nsplitter = ['best', 'random']","3837b917":"grid = GridSearchCV(estimator = d_model, cv= 3, param_grid=dict(criterion=criterion, max_depth=max_depth, splitter= splitter))","db0fbca9":"grid.fit(X_train, y_train)","696333c9":"print (grid.best_score_, grid.score(X_test, y_test))","e77322e7":"from sklearn.linear_model import LogisticRegression\nl_model = LogisticRegression(max_iter=10000)\nl_model.fit(X_train, y_train)","71fd243c":"l_model.score(X_test, y_test)","8603f70c":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\n\ndef get_score(n_estimators):\n    my_pipeline = Pipeline(steps=[\n        ('preprocessor', SimpleImputer()),\n        ('model', RandomForestClassifier(n_estimators, random_state=42))\n    ])\n    scores = -1 * cross_val_score(my_pipeline, X_train, y_train,\n                                  cv=3,\n                                  scoring='neg_mean_absolute_error')\n    return scores.mean()","8695c1fb":"results = {}\nfor i in range(50,1000, 50):\n    results[i] = get_score(i)","4d8ed17a":"plt.plot(list(results.keys()), list(results.values()))\nplt.show()","ee5f6cd7":"r_forest = RandomForestClassifier(n_estimators=200)\nr_forest.fit(X_train, y_train)\nr_forest.score(X_test, y_test)","4a4fdad7":"y_valid = r_forest.predict(X_test)\ny_valid","e0528a3d":"from sklearn.metrics import mean_absolute_error\n\nmean_absolute_error(y_valid, y_test)","cf27dac7":"rfc_eval = cross_val_score(estimator = r_forest, X = X_train, y = y_train, cv = 10)\nrfc_eval.mean()*100","c6efb542":"No null values in the dataset.","d877ff30":"Classifying the data into: \n* 3,4,5 -> Bad (0)\n* 6,7,8 -> Good (1)","0e9db786":"**LogisticRegression**","dbb6f530":"This model performed better than Decision Tree. Let's try RandomForrestClassifier now","d2f7eede":"From the above graph, we learn that for n_estimators = 400, the best score can be achieved","e426bf7e":"Thank you for going through the  notebook. I am a beginner in data science, and would love to get some comments on my work above.","926cc8ac":"**DecisionTreeClassifier and GridSearchCV**","e57f6e4e":"# A beginner's guide of using multiple ML algorithms to find out the best one, based on accuracy score.\n\nThe Algorithms used in this notebook are:\n1. DecisionTreeClassifier and GridSearchCV\n2. LogisticRegression\n3. RandomForestClassfier","3d51eef8":"**RandomForestClassfier**","831ead4d":"Calculatind MAE for RandomForestClassifier"}}