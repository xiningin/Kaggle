{"cell_type":{"e3c1bd7e":"code","4633c048":"code","9ec1ac64":"code","e1c15380":"code","a8d48d3c":"code","b4b6e697":"code","f2ecfae6":"code","59c32068":"code","461b8b39":"code","26e7f917":"code","88b30d18":"code","097f9792":"code","8a706289":"code","ae103d18":"code","0cabadec":"code","1906c3da":"code","7871223e":"code","679bddd4":"code","8a1d0a6b":"code","36d0c8fb":"code","6d37d010":"code","e2f1907d":"code","8f1bcdbd":"code","29ecbbad":"code","b5ecdf31":"code","94a1e494":"code","e2815ab6":"code","2572cd63":"code","a6f1be25":"markdown","19648180":"markdown","e94460be":"markdown","0c49be33":"markdown","5a5f6bfe":"markdown","e6931d3d":"markdown","1092f1aa":"markdown","25a361e3":"markdown"},"source":{"e3c1bd7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport math\nimport random\nimport pickle\nimport itertools\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error \n\nfrom sklearn.utils import shuffle\n\nfrom scipy.signal import resample\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, label_ranking_average_precision_score, label_ranking_loss, coverage_error\n\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Dropout, Input, Flatten, SeparableConv1D\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.merge import Concatenate\nfrom keras.models import Model\n\nfrom keras import backend as K\nfrom keras.optimizers import Adam\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nnp.random.seed(4)\n\nimport pickle\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4633c048":"df = pd.read_csv(\"..\/input\/mitbih_train.csv\", header=None)\ndf2 = pd.read_csv(\"..\/input\/mitbih_test.csv\", header=None)\ndf3= pd.read_csv(\"..\/input\/ptbdb_abnormal.csv\", header=None)\ndf = pd.concat([df, df2,df3], axis=0)","9ec1ac64":"df.head()","e1c15380":"df.info()","a8d48d3c":"df[187].value_counts()","b4b6e697":"M = df.values\nX = M[:, :-1]\ny = M[:, -1].astype(int)","f2ecfae6":"del df\ndel df2\ndel M","59c32068":"C0 = np.argwhere(y == 0).flatten()\nC1 = np.argwhere(y == 1).flatten()\nC2 = np.argwhere(y == 2).flatten()\nC3 = np.argwhere(y == 3).flatten()\nC4 = np.argwhere(y == 4).flatten()","461b8b39":"x = np.arange(0, 187)*8\/1000\n\nplt.figure(figsize=(12,12))\nplt.subplot(5, 1, 1)\n\nplt.plot(x, X[C0, :][0], label=\"Cat. N\")\nplt.title(\"1-beat ECG for category N\", fontsize=20)\nplt.ylabel(\"Amplitude\", fontsize=15)\nplt.xlabel(\"Time (ms)\", fontsize=15)\n\nplt.subplot(5,1, 2)\nplt.plot(x, X[C1, :][0], label=\"Cat. S\")\nplt.title(\"1-beat ECG for category S\", fontsize=20)\nplt.ylabel(\"Amplitude\", fontsize=15)\nplt.xlabel(\"Time (ms)\", fontsize=15)\n\nplt.subplot(5,1,3)\nplt.plot(x, X[C2, :][0], label=\"Cat. V\")\nplt.title(\"1-beat ECG for category V\", fontsize=20)\nplt.ylabel(\"Amplitude\", fontsize=15)\nplt.xlabel(\"Time (ms)\", fontsize=15)\n\nplt.subplot(5,1, 4)\nplt.plot(x, X[C3, :][0], label=\"Cat. F\")\nplt.title(\"1-beat ECG for category F\", fontsize=20)\nplt.ylabel(\"Amplitude\", fontsize=15)\nplt.xlabel(\"Time (ms)\", fontsize=15)\n\nplt.subplot(5,1,5)\nplt.plot(x, X[C4, :][0], label=\"Cat. Q\")\nplt.title(\"1-beat ECG for category Q\", fontsize=20)\nplt.ylabel(\"Amplitude\", fontsize=15)\nplt.xlabel(\"Time (ms)\", fontsize=15)\nplt.tight_layout()\nplt.show()","26e7f917":"def stretch(x):\n    l = int(187 * (1 + (random.random()-0.5)\/3))\n    y = resample(x, l)\n    if l < 187:\n        y_ = np.zeros(shape=(187, ))\n        y_[:l] = y\n    else:\n        y_ = y[:187]\n    return y_\n\ndef amplify(x):\n    alpha = (random.random()-0.5)\n    factor = -alpha*x + (1+alpha)\n    return x*factor\n\ndef augment(x):\n    result = np.zeros(shape= (4, 187))\n    for i in range(3):\n        if random.random() < 0.33:\n            new_y = stretch(x)\n        elif random.random() < 0.66:\n            new_y = amplify(x)\n        else:\n            new_y = stretch(x)\n            new_y = amplify(new_y)\n        result[i, :] = new_y\n    return result","88b30d18":"plt.plot(X[0, :])\nplt.plot(amplify(X[0, :]))\nplt.plot(stretch(X[0, :]))\nplt.show()","097f9792":"result = np.apply_along_axis(augment, axis=1, arr=X[C3]).reshape(-1, 187)\nclasse = np.ones(shape=(result.shape[0],), dtype=int)*3\nX = np.vstack([X, result])\ny = np.hstack([y, classe])","8a706289":"subC0 = np.random.choice(C0, 800)\nsubC1 = np.random.choice(C1, 800)\nsubC2 = np.random.choice(C2, 800)\nsubC3 = np.random.choice(C3, 800)\nsubC4 = np.random.choice(C4, 800)","ae103d18":"X_test = np.vstack([X[subC0], X[subC1], X[subC2], X[subC3], X[subC4]])\ny_test = np.hstack([y[subC0], y[subC1], y[subC2], y[subC3], y[subC4]])\n\nX_train = np.delete(X, [subC0, subC1, subC2, subC3, subC4], axis=0)\ny_train = np.delete(y, [subC0, subC1, subC2, subC3, subC4], axis=0)\n\nX_train, y_train = shuffle(X_train, y_train, random_state=0)\nX_test, y_test = shuffle(X_test, y_test, random_state=0)\n\ndel X\ndel y","0cabadec":"X_train = np.expand_dims(X_train, 2)\nX_test = np.expand_dims(X_test, 2)","1906c3da":"print(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","7871223e":"ohe = OneHotEncoder()\ny_train = ohe.fit_transform(y_train.reshape(-1,1))\ny_test = ohe.transform(y_test.reshape(-1,1))","679bddd4":"print(\"X_train\", X_train.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_test\", y_test.shape)","8a1d0a6b":"n_obs, feature, depth = X_train.shape\nbatch_size = 500","36d0c8fb":"def build_model():\n    input_img = Input(shape=(feature, depth), name='ImageInput')\n    x = Conv1D(32, 3, activation='relu', padding='same', name='Conv1_1')(input_img)\n    x = Conv1D(32, 3, activation='relu', padding='same', name='Conv1_2')(x)\n    x = MaxPooling1D(2, name='pool1')(x)\n    \n    x = SeparableConv1D(32, 3, activation='relu', padding='same', name='Conv2_1')(x)\n    x = SeparableConv1D(32, 3, activation='relu', padding='same', name='Conv2_2')(x)\n    x = MaxPooling1D(2, name='pool2')(x)\n    \n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv3_1')(x)\n    x = BatchNormalization(name='bn1')(x)\n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv3_2')(x)\n    x = BatchNormalization(name='bn2')(x)\n    \n    x = SeparableConv1D(64, 3, activation='relu', padding='same', name='Conv3_3')(x)\n    x = MaxPooling1D(2, name='pool3')(x)\n    \n    x = Flatten(name='flatten')(x)\n    x = Dense(128, activation='relu', name='fc1')(x)\n    x = Dropout(0.6, name='dropout1')(x)\n    x = Dense(128, activation='relu', name='fc2')(x)\n    x = Dropout(0.5, name='dropout2')(x)\n    x = Dense(5, activation='softmax', name='fc3')(x)\n    \n    model = Model(inputs=input_img, outputs=x)\n    return model\n\n","6d37d010":"model =  build_model()\nmodel.summary()","e2f1907d":"def exp_decay(epoch):\n    initial_lrate = 0.001\n    k = 0.75\n    t = n_obs\/\/(10000 * batch_size)  # every epoch we do n_obs\/batch_size iteration\n    lrate = initial_lrate * math.exp(-k*t)\n    return lrate\n\nlrate = LearningRateScheduler(exp_decay)","8f1bcdbd":"adam = Adam(lr = 0.001, beta_1 = 0.9, beta_2 = 0.999)","29ecbbad":"model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","b5ecdf31":"history = model.fit(X_train, y_train, \n                    epochs=75, \n                    batch_size=batch_size, \n                    verbose=2, \n                    validation_data=(X_test, y_test), \n                    callbacks=[lrate])","94a1e494":"# Get predictions\npreds = model.predict(X_test, batch_size=1000)\npreds = np.argmax(preds, axis=-1)\n\n# Original labels\norig_test_labels = np.argmax(y_test, axis=-1)\n\nprint(orig_test_labels.shape)\nprint(preds.shape)","e2815ab6":"# Get the confusion matrix\ncm  = confusion_matrix(orig_test_labels, preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","2572cd63":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","a6f1be25":"**ECG Heartbeat Categorization Dataset**\n\nI tried to use custom CNN with Conv1D and SeparableConv1D layers on ECG Dataset.","19648180":"**Custom 1D CNN Model**","e94460be":"# Split","0c49be33":"Lets look at the number of data for each labels","5a5f6bfe":"Lets look at the confusion matrix ","e6931d3d":"Lets plot the Training and test accuracy & Loss graph!!","1092f1aa":"# Visual Input","25a361e3":"# Data augmentation\n\nData Augmentation on the smallest class 3 to the same level as class 1. With that we will be able to have a test set of around 5x800 observations."}}