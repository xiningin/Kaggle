{"cell_type":{"941fc0b6":"code","2c0b7cb3":"code","2f2efed3":"code","8ff9c5ca":"code","30294d26":"code","39b00a2e":"code","741160a9":"code","c1a1a262":"code","86e1d547":"code","393517a2":"code","6c11f670":"code","e04f2bee":"code","72f5096f":"code","ef0500d0":"code","fa97437b":"code","d8e52c19":"code","6ef65bf3":"code","e28e81af":"code","e8332fce":"code","754b09ac":"code","393e57f8":"code","bf298526":"code","a82ec426":"code","eec96784":"code","20db46e8":"code","b7497c23":"code","2a58d447":"code","cc48a5d3":"code","3c9971d2":"code","724adfe8":"code","6973bd3d":"code","659d6615":"code","07696363":"code","7994d438":"code","2b64e749":"code","c0f6f4f2":"code","54a1ffc5":"code","b4d326b5":"code","6d0136be":"code","46863cd5":"code","470b14d5":"code","8e27620d":"code","76a1109d":"code","8e144958":"code","3763101a":"code","095411f2":"code","2627820a":"code","ff93143e":"code","961fb117":"code","5346105e":"code","e4ec9094":"code","e4426589":"code","c00e2257":"code","fb474184":"code","6b2cccc1":"code","d5ec4e0e":"code","f9ae6e55":"code","f00e6cad":"code","bad29e28":"code","dfd072fc":"code","a50473e2":"code","2bdf4240":"code","028c1d58":"markdown","a426d028":"markdown","d2d6d739":"markdown","dc8c7105":"markdown","5ee2cf58":"markdown","a8287e64":"markdown","1c38fdf2":"markdown","e0bc5173":"markdown"},"source":{"941fc0b6":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c0b7cb3":"train = pd.read_csv('..\/input\/analytics-vidhya-jobathon-september-2021\/train.csv')\ntest = pd.read_csv('..\/input\/analytics-vidhya-jobathon-september-2021\/test.csv')","2f2efed3":"train.duplicated().sum(), test.duplicated().sum()","8ff9c5ca":"train.isnull().sum().sum(), test.isnull().sum().sum()","30294d26":"print(\"Training data starts from: {}\".format(train.Date.min()))\nprint(\"Training data end on: {}\".format(train.Date.max()))\nprint()\nprint(\"Testing data starts from: {}\".format(test.Date.min()))\nprint(\"Testing data end on: {}\".format(test.Date.max()))","39b00a2e":"df = train.copy()","741160a9":"df.Date = pd.to_datetime(df.Date)\ndf['Day'] = df.Date.dt.day\ndf['Month'] = df.Date.dt.month\ndf['Year'] = df.Date.dt.year\ndf['DayOfWeek'] = df.Date.dt.dayofweek","c1a1a262":"plt.figure(figsize=(18,8))\nplt.plot(df.groupby(df.Day).sum().Sales)\nplt.title(\"Sale vs Day\")\nplt.xlabel('Day')\nplt.ylabel('Sales')\nplt.show()","86e1d547":"plt.figure(figsize=(18,8))\nplt.plot(df.groupby(df.DayOfWeek).sum().Sales)\nplt.title(\"Sale vs DayOfWeek\")\nplt.xlabel('DayofWeek')\nplt.ylabel('Sales')\nplt.show()","393517a2":"plt.figure(figsize=(18,8))\nplt.plot(df.groupby(df.Month).sum().Sales)\nplt.title(\"Sale vs Month\")\nplt.xlabel('Month')\nplt.ylabel('Sales')\nplt.show()","6c11f670":"plt.figure(figsize=(18,8))\ntemp_df = df.groupby(df.Year).sum()\nsns.barplot(temp_df.index, temp_df.Sales, palette='Blues')\nplt.title(\"Total SALE in Each Year\")\nplt.xlabel('Year')\nplt.ylabel('Sales')\nplt.show()","e04f2bee":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(train.corr(), mask=mask, cmap=cmap, annot=True, ax=ax1)\nax1.set_title('Train')\n\nsns.scatterplot(train['#Order'], train.Sales, ax=ax2)","72f5096f":"plt.figure(figsize=(18,8))\ntemp_df = df.groupby(df.Store_Type).sum()\nsns.barplot(temp_df.index, temp_df.Sales, palette='Blues')\nplt.title(\"Store Type vs Sales\")\nplt.xlabel('Store Type')\nplt.ylabel('Sales')\nplt.show()","ef0500d0":"df.head()","fa97437b":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\ntemp_df = df.groupby(df.Store_Type).sum()\nsns.barplot(temp_df.index, temp_df['#Order'], ax=ax1, palette='Blues')\ntemp_df = df.groupby(df.Store_Type).count()\nsns.barplot(temp_df.index, temp_df['Discount'], ax=ax2, palette='Blues')\nplt.show()","d8e52c19":"from statsmodels.tsa.seasonal import seasonal_decompose\ntemp_df = train.copy()\ntemp_df.Date = pd.to_datetime(temp_df.Date)\ntemp_df.index = temp_df.Date\ntemp_df.Sales = temp_df.Sales.apply(lambda x: None if x == 0 else x)\ntemp_df.Sales = temp_df.Sales.fillna(method='ffill').fillna(method='bfill')\ntemp_df = temp_df[['Sales']]\ntemp_df = temp_df.groupby(temp_df.index).sum()\nresult = seasonal_decompose(temp_df, model='additive', freq=52)\n\nfig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(18,8))\nax1.plot(result.trend)\nax1.axhline(y = temp_df.Sales.mean(), color = 'r', linestyle = '-', label='Sales Mean')\nax1.set_title(\"Trend\")\nax2.plot(result.resid)\nax2.set_title(\"Error\")\nax1.legend()\nplt.show()","6ef65bf3":"train['is_train'] = 1\ntest['is_train'] = 0\n\ndf = pd.concat([train, test])","e28e81af":"df.loc[df['is_train'] == 1, 'SalesLog'] = np.log(1+df.loc[df['is_train'] == 1]['Sales']) #Transforming Sales to 1+log","e8332fce":"df.Date = pd.to_datetime(df.Date)\ndf['Day'] = df.Date.dt.day\ndf['Month'] = df.Date.dt.month\ndf['Year'] = df.Date.dt.year\ndf['DayOfWeek'] = df.Date.dt.dayofweek\ndf['DayOfYear'] = df.Date.dt.dayofyear\n#df.drop('Date', axis=1, inplace=True)","754b09ac":"features_x = ['Store_id', 'Day', 'Month', 'Year', 'DayOfWeek', 'DayOfYear', 'Store_Type', 'Location_Type', 'Region_Code', 'Holiday', 'Discount']\nfeatures_y = ['SalesLog']","393e57f8":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor feature in features_x:\n  print(feature)\n  if(df[feature].dtype==np.object):\n    df[feature] = le.fit_transform(df[feature])","bf298526":"df['Zscore'] = (df.Sales - df.Sales.mean())\/df.Sales.std()","a82ec426":"thresh=4.0\ndef check_outlier(value):\n    if(value>=thresh):\n        return True\n    else:\n        return False\n\ndf['Outlier'] = df.Zscore.apply(check_outlier)","eec96784":"store_data_sales = df.groupby([df['Store_Type']])['Sales'].sum()\nstore_data_customers = df.groupby([df['Store_Type']])['#Order'].sum()\nstore_data_open = df.groupby([df['Store_Type']])['Holiday'].count()\n\nstore_data_sales_per_day = store_data_sales \/ store_data_open\nstore_data_customers_per_day = store_data_customers \/ store_data_open\nstore_data_sales_per_customer_per_day = store_data_sales_per_day \/ store_data_customers_per_day","20db46e8":"df  = pd.merge(df, store_data_sales_per_day.reset_index(name='SalesPerDay'), how='left', on=['Store_Type'])\ndf = pd.merge(df, store_data_customers_per_day.reset_index(name='CustomersPerDay'), how='left', on=['Store_Type'])\ndf = pd.merge(df, store_data_sales_per_customer_per_day.reset_index(name='SalesPerCustomersPerDay'), how='left', on=['Store_Type'])","b7497c23":"store_features = ['Store_Type', 'SalesPerDay', 'CustomersPerDay', 'SalesPerCustomersPerDay']\nfeatures_x = list(set(features_x + store_features))","2a58d447":"holidays_each_day_of_week = df.groupby(df.DayOfWeek).sum().Holiday\ndf = pd.merge(df, holidays_each_day_of_week.reset_index(name='HolidaysPerDayOfWeek'), on=['DayOfWeek'])","cc48a5d3":"discount_each_day_of_week = df.groupby(df.DayOfWeek).sum().Discount\ndf = pd.merge(df, discount_each_day_of_week.reset_index(name='DiscountsPerDayOfWeek'), on=['DayOfWeek'])","3c9971d2":"orders_each_day_of_week = df.groupby(df.DayOfWeek).sum()['#Order']\ndf = pd.merge(df, orders_each_day_of_week.reset_index(name='OrdersPerDayOfWeek'), on=['DayOfWeek'])","724adfe8":"orders_per_holiday = df.groupby(df.Holiday).sum()['#Order']\ndf = pd.merge(df, orders_per_holiday.reset_index(name='OrdersPerHoliday'), on=['Holiday'])","6973bd3d":"orders_per_region = df.groupby(df.Region_Code).sum()['#Order']\ndf = pd.merge(df, orders_per_region.reset_index(name='OrdersPerRegion'), on=['Region_Code'])","659d6615":"orders_per_location = df.groupby(df.Location_Type).sum()['#Order']\ndf = pd.merge(df, orders_per_location.reset_index(name='OrdersPerLocation'), on=['Location_Type'])","07696363":"orders_per_store = df.groupby(df.Store_Type).sum()['#Order']\ndf = pd.merge(df, orders_per_store.reset_index(name='OrdersPerStore'), on=['Store_Type'])","7994d438":"discount_per_store_id = df.groupby(df.Store_id).sum()['Discount']\ndf = pd.merge(df, discount_per_store_id.reset_index(name='DiscountPerStoreID'), on=['Store_id'])","2b64e749":"orders_per_store_id = df.groupby(df.Store_id).sum()['#Order']\ndf = pd.merge(df, orders_per_store_id.reset_index(name='OrderPerStoreID'), on=['Store_id'])","c0f6f4f2":"orders_per_store_id = df.groupby(df.Discount).sum()['#Order']\ndf = pd.merge(df, orders_per_store_id.reset_index(name='OrderPerDiscount'), on=['Discount'])","54a1ffc5":"orders_per_location = df.groupby(df.Location_Type).sum()['Discount']\ndf = pd.merge(df, orders_per_location.reset_index(name='DiscountPerLocation'), on=['Location_Type'])","b4d326b5":"import datetime\nholidays_next_week=[]\nholidays_next_week_index=[]\nfor index, value in df.groupby(df.Date).sum().iterrows():\n    start_range = index + datetime.timedelta(days=7)\n    end_range = index + datetime.timedelta(days=15)\n    holidays = sum((df.groupby(df.Date).sum()[start_range:end_range]).Holiday)\n    holidays_next_week.append(holidays)\n    holidays_next_week_index.append(index)\n    \nholidays_next_week = pd.Series(holidays_next_week)\nholidays_next_week.shape","6d0136be":"holidays_this_week=[]\nindex_list = []\nfor index, value in df.groupby(df.Date).sum().iterrows():\n    start_range = index \n    end_range = index + datetime.timedelta(days=7)\n    holidays = sum((df.groupby(df.Date).sum()[start_range:end_range]).Holiday)\n    holidays_this_week.append(holidays)\n    index_list.append(index)\n    \nholidays_this_week = pd.Series(holidays_this_week)\nholidays_this_week.shape","46863cd5":"holidays_last_week=[]\nholidays_last_week_index=[]\nfor index, value in df.groupby(df.Date).sum().iterrows():\n    start_range = index - datetime.timedelta(days=7)\n    end_range = index + datetime.timedelta(days=1)\n    holidays = sum((df.groupby(df.Date).sum()[start_range:end_range]).Holiday)\n    holidays_last_week.append(holidays)\n    holidays_last_week_index.append(index)\n    \nholidays_last_week = pd.Series(holidays_next_week)\nholidays_last_week.shape","470b14d5":"temp_df = pd.DataFrame({'HolidaysNextWeek':holidays_next_week, 'Date': holidays_next_week_index})\ndf = pd.merge(df, temp_df, on=['Date'])","8e27620d":"temp_df = pd.DataFrame({'HolidaysThisWeek':holidays_this_week, 'Date': index_list})\ndf = pd.merge(df, temp_df, on=['Date'])","76a1109d":"temp_df = pd.DataFrame({'HolidaysLastWeek':holidays_last_week, 'Date': holidays_last_week_index})\ndf = pd.merge(df, temp_df, on=['Date'])","8e144958":"df['DateInt'] = df.Date.dt.strftime('%Y%m%d').map(int)\ndf.drop(\"Date\", axis=1, inplace=True)","3763101a":"temp = df.copy()\nfeatures_x = [feature for feature in df.columns if feature not in ('#Order', 'Sales', 'SalesLog', 'Zscore', 'is_train', 'Outlier', 'ID')]","095411f2":"len(features_x)","2627820a":"from sklearn.preprocessing import PolynomialFeatures\ndef get_cross_feature(features):\n  poly = PolynomialFeatures(interaction_only=True)\n  cross_feature = pd.DataFrame(poly.fit_transform(df[features]), columns=poly.get_feature_names(features)).iloc[:, -1]\n  return cross_feature\n\nfrom itertools import permutations\npermutated_features = list(permutations(features_x[15:], 2))\npermutated_features = [[x, y] for x, y in permutated_features]\nfor features in permutated_features:\n  print(features)\n  temp = pd.concat([temp, get_cross_feature(features)], axis=1)","ff93143e":"df = temp.copy()","961fb117":"features_x = [feature for feature in df.columns if feature not in ('#Order', 'Sales', 'SalesLog', 'Zscore', 'is_train', 'Outlier', 'ID')]","5346105e":"df[features_x].tail()","e4ec9094":"print(len(features_x))\nprint(df.shape)\nprint(df.isnull().sum().sum())","e4426589":"df.loc[(df['is_train'] == 1)][features_x].nunique()[-20:]","c00e2257":"from sklearn.inspection import permutation_importance\nimport lightgbm as lgb\n\nlgbm = lgb.LGBMRegressor()\ndata = df.loc[(df['is_train'] == 1) & (df['Outlier'] == False)]\nlgbm.fit(data[features_x], data[features_y])\nresult = permutation_importance(lgbm, data[features_x], data[features_y], n_repeats=5, n_jobs=-1)","fb474184":"scores = pd.DataFrame({'importances': result.importances_mean, 'features': features_x})\nscores = scores.sort_values(by='importances', ascending=False)\nscores = scores[scores.importances > 0].reset_index(drop=True)\n\nfeatures_x = scores.features","6b2cccc1":"scores","d5ec4e0e":"import xgboost as xgb","f9ae6e55":"#copied from https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/custom_metric_obj.html\nfrom typing import Tuple\n\ndef gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n    '''Compute the gradient squared log error.'''\n    y = dtrain.get_label()\n    return (np.log1p(predt) - np.log1p(y)) \/ (predt + 1)\n\ndef hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n    '''Compute the hessian for squared log error.'''\n    y = dtrain.get_label()\n    return ((-np.log1p(predt) + np.log1p(y) + 1) \/\n            np.power(predt + 1, 2))\n\ndef squared_log(predt: np.ndarray,\n                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n    '''Squared Log Error objective. A simplified version for RMSLE used as\n    objective function.\n    '''\n    predt[predt < -1] = -1 + 1e-6\n    grad = gradient(predt, dtrain)\n    hess = hessian(predt, dtrain)\n    return grad, hess\n\ndef rmsle(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n    ''' Root mean squared log error metric.'''\n    y = dtrain.get_label()\n    predt[predt < -1] = -1 + 1e-6\n    elements = np.power(np.log1p(y) - np.log1p(predt), 2)\n    return 'PyRMSLE', float(np.sqrt(np.sum(elements) \/ len(y)))","f00e6cad":"data = df.loc[(df['is_train'] == 1) & (df['Outlier'] == False)]\ntrain_len = int(len(data) * .8)\nx_train, y_train, x_test, y_test = data[features_x].iloc[:train_len, :], data[features_y].iloc[:train_len], data[features_x].iloc[train_len:, :], data[features_y].iloc[train_len:]\nprint(x_train.shape, y_train.shape, x_test.shape, y_test.shape)","bad29e28":"dtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test, y_test)\n\nnum_round = 20000\nevallist = [(dtrain, 'train'), (dtest, 'test')]\n\nparam = {'max_depth': 9,\n         'eta': 0.001,\n         'subsample': 0.5,\n         'colsample_bytree': 0.5,\n         'colsample_bylevel': 0.5,\n         'min_child_weight': 5,\n         'tree_method': 'gpu_hist'\n         }\n\nplst = list(param.items())\n\nmodel = xgb.train(plst, dtrain, num_round, evallist, obj=squared_log, feval=rmsle, verbose_eval=250, early_stopping_rounds=250)","dfd072fc":"submit = df.loc[df['is_train'] == 0]\ndsubmit = xgb.DMatrix(submit[features_x])\npredictions = model.predict(dsubmit)\n\ndf_predictions = submit['ID'].reset_index()\ndf_predictions['ID'] = df_predictions['ID']\ndf_predictions['Sales'] = (np.exp(predictions) - 1) * 0.985 #Scale Back\n\ndf_predictions.sort_values('ID', inplace=True)\n#df_predictions[['ID', 'Sales']].to_csv('solution.csv', index=False)","a50473e2":"df_predictions[['ID', 'Sales']]","2bdf4240":"#df_predictions[['ID', 'Sales']].to_csv('solution.csv', index=False)","028c1d58":"# Feature Engineering","a426d028":"### Problem Statement:\n\nYour Client WOMart is a leading nutrition and supplement retail chain that offers a comprehensive range of products for all your wellness and fitness needs. \n\nWOMart follows a multi-channel distribution strategy with 350+ retail stores spread across 100+ cities. \n\nEffective forecasting for store sales gives essential insight into upcoming cash flow, meaning WOMart can more accurately plan the cashflow at the store level.\n\nSales data for 18 months from 365 stores of WOMart is available along with information on Store Type, Location Type for each store, Region Code for every store, Discount provided by the store on every day, Number of Orders everyday etc.\nYour task is to predict the store sales for each store in the test set for the next two months.\n\n### Evaluation Metric:\n\nThe evaluation metric for this competition is MSLE * 1000 across all entries in the test set.\n\nPublic and Private Split\n\nTest data is further divided into Public (First 20 Days) and Private (Last 41 Days). You will make the prediction for two months (61 days).\n\nYour initial responses will be checked and scored on the Public data.\nThe final rankings would be based on your private score which will be published once the competition is over.","d2d6d739":"# Understanding the Data","dc8c7105":"# Reading Data","5ee2cf58":"# Exploratory Data Analysis","a8287e64":"Modelling","1c38fdf2":"# Thank you! \n\nSocial:<br>\nhttps:\/\/www.linkedin.com\/in\/shivam017arora\/<br>\nhttps:\/\/medium.com\/@arora_shivam","e0bc5173":"Feature Selection using Permutation Feature Importance"}}