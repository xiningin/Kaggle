{"cell_type":{"b2d6816d":"code","44ab11ea":"code","993525df":"code","341e78dc":"code","ab0fb216":"code","bb68c491":"code","73e62f57":"code","f0ff3af9":"code","de052e06":"code","b4316b7f":"code","04dbd93c":"code","14e8f2bf":"code","2decd3e1":"code","6c4f7aed":"code","c7cdc385":"code","b6202d2f":"code","7444f90a":"code","56988393":"code","98158356":"code","4b9edfd8":"code","797d8a4e":"code","01917548":"code","6d4704e1":"code","4e866ec8":"code","979db7ee":"code","942da0c0":"code","ee32d21b":"code","1086ae80":"code","6e2c819a":"code","c465cb39":"code","989a5567":"code","fc91ae75":"code","05a1ffc3":"code","96f9b9b1":"code","52dd8132":"code","446a297c":"code","9600abcd":"code","760c6a09":"code","efe798f0":"code","683d7fdd":"code","2cfc7613":"code","7b42cdd0":"code","f01a999c":"markdown"},"source":{"b2d6816d":"# From Brendon Hall, Enthought post on seg\n# This notebook demonstrates how to train a machine learning algorithm to predict facies from well log data. The dataset we will use comes from a class excercise from The University of Kansas on Neural Networks and Fuzzy Systems. This exercise is based on a consortium project to use machine learning techniques to create a reservoir model of the largest gas fields in North America, the Hugoton and Panoma Fields. For more info on the origin of the data, see Bohling and Dubois (2003) and Dubois et al. (2007).\n\n# The dataset we will use is log data from nine wells that have been labeled with a facies type based on oberservation of core. We will use this log data to train a support vector machine to classify facies types. Support vector machines (or SVMs) are a type of supervised learning model that can be trained on data to perform classification and regression tasks. The SVM algorithm uses the training data to fit an optimal hyperplane between the different classes (or facies, in our case). We will use the SVM implementation in scikit-learn.\n\n# First we will explore the dataset. We will load the training data from 9 wells, and take a look at what we have to work with. We will plot the data from a couple wells, and create cross plots to look at the variation within the data.\n\n# Next we will condition the data set. We will remove the entries that have incomplete data. The data will be scaled to have zero mean and unit variance. We will also split the data into training and test sets.\n\n# We will then be ready to build the SVM classifier. We will demonstrate how to use the cross validation set to do model parameter selection.\n\n# Finally, once we have a built and tuned the classifier, we can apply the trained model to classify facies in wells which do not already have labels. We will apply the classifier to two wells, but in principle you could apply the classifier to any number of wells that had the same log data.","44ab11ea":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as colors\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfrom pandas import set_option\nset_option(\"display.max_rows\", 10)\npd.options.mode.chained_assignment = None\n\ntraining_data = pd.read_csv('..\/input\/raw-data\/facies_vectors.csv')\ntraining_data","993525df":"# To evaluate the accuracy of the classifier,\n# we will remove one well from the training set so that we can compare \n# the predicted and actual facies labels.\nblind = training_data[training_data['Well Name'] == 'SHANKLE']\ntraining_data = training_data[training_data['Well Name'] != 'SHANKLE']\nblind","341e78dc":"# Let's clean up this dataset. The 'Well Name' and 'Formation' columns \n# can be turned into a categorical data type.\ntraining_data['Well Name'] = training_data['Well Name'].astype('category')\ntraining_data['Formation'] = training_data['Formation'].astype('category')\ntraining_data['Well Name'].unique()","ab0fb216":"# 1=sandstone  2=c_siltstone   3=f_siltstone \n# 4=marine_silt_shale 5=mudstone 6=wackestone 7=dolomite\n# 8=packstone 9=bafflestone\nfacies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00',\n       '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n\nfacies_labels = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS',\n                 'WS', 'D','PS', 'BS']\n#facies_color_map is a dictionary that maps facies labels\n#to their respective colors\nfacies_color_map = {}\nfor ind, label in enumerate(facies_labels):\n    facies_color_map[label] = facies_colors[ind]\n\ndef label_facies(row, labels):\n    return labels[ row['Facies'] -1]\n    \ntraining_data.loc[:,'FaciesLabels'] = training_data.apply(lambda row: label_facies(row, facies_labels), axis=1)\ntraining_data.describe()","bb68c491":"# Looking at the count values, most values have 4149 valid values except for PE, which has 3232.\n# In this tutorial we will drop the feature vectors that don't have a valid PE entry.\nPE_mask = training_data['PE'].notnull().values\ntraining_data = training_data[PE_mask]","73e62f57":"# Let's take a look at the data from individual wells in a more familiar log plot form. \n# We will create plots for the five well log variables, as well as a log for facies labels.\ndef make_facies_log_plot(logs, facies_colors):\n    #make sure logs are sorted by depth\n    logs = logs.sort_values(by='Depth')\n    cmap_facies = colors.ListedColormap(\n            facies_colors[0:len(facies_colors)], 'indexed')\n    \n    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n    \n    cluster=np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n    \n    f, ax = plt.subplots(nrows=1, ncols=6, figsize=(8, 12))\n    ax[0].plot(logs.GR, logs.Depth, '-g')\n    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')\n    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n    ax[4].plot(logs.PE, logs.Depth, '-', color='black')\n    im=ax[5].imshow(cluster, interpolation='none', aspect='auto',\n                    cmap=cmap_facies,vmin=1,vmax=9)\n    \n    divider = make_axes_locatable(ax[5])\n    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n    cbar=plt.colorbar(im, cax=cax)\n    cbar.set_label((17*' ').join([' SS ', 'CSiS', 'FSiS', \n                                'SiSh', ' MS ', ' WS ', ' D  ', \n                                ' PS ', ' BS ']))\n    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n    \n    for i in range(len(ax)-1):\n        ax[i].set_ylim(ztop,zbot)\n        ax[i].invert_yaxis()\n        ax[i].grid()\n        ax[i].locator_params(axis='x', nbins=3)\n    \n    ax[0].set_xlabel(\"GR\")\n    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n    ax[1].set_xlabel(\"ILD_log10\")\n    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n    ax[2].set_xlabel(\"DeltaPHI\")\n    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n    ax[3].set_xlabel(\"PHIND\")\n    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n    ax[4].set_xlabel(\"PE\")\n    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n    ax[5].set_xlabel('Facies')\n    \n    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])\n    ax[5].set_xticklabels([])\n    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)","f0ff3af9":"# We then show log plots for wells SHRIMPLIN.\nmake_facies_log_plot(\n    training_data[training_data['Well Name'] == 'SHRIMPLIN'],\n    facies_colors)","de052e06":"# Again for NOLAN wells\nmake_facies_log_plot(\n    training_data[training_data['Well Name'] == 'NOLAN'],\n    facies_colors)","b4316b7f":"#count the number of unique entries for each facies, sort them by\n#facies number (instead of by number of entries)\nfacies_counts = training_data['Facies'].value_counts().sort_index()\n#use facies labels to index each count\nfacies_counts.index = facies_labels\n\nfacies_counts.plot(kind='bar',color=facies_colors, \n                   title='Distribution of Training Data by Facies')\nfacies_counts","04dbd93c":"#save plot display settings to change back to when done plotting with seaborn\ninline_rc = dict(mpl.rcParams)\n\nimport seaborn as sns\nsns.set()\nsns.pairplot(training_data.drop(['Well Name','Facies','Formation','Depth','NM_M','RELPOS'],axis=1),\n             hue='FaciesLabels', palette=facies_color_map,\n             hue_order=list(reversed(facies_labels)))\n\n#switch back to default matplotlib plot style\nmpl.rcParams.update(inline_rc)","14e8f2bf":"# Now we extract just the feature variables we need to perform the classification. \n# The predictor variables are the five wireline values and two geologic constraining variables. \n# We also get a vector of the facies labels that correspond to each feature vector.\ncorrect_facies_labels = training_data['Facies'].values\n\nfeature_vectors = training_data.drop(['Formation', 'Well Name', 'Depth','Facies','FaciesLabels'], axis=1)\nfeature_vectors.describe()","2decd3e1":"from sklearn import preprocessing\n\nscaler = preprocessing.StandardScaler().fit(feature_vectors)\nscaled_features = scaler.transform(feature_vectors)","6c4f7aed":"feature_vectors","c7cdc385":"# Split to test and training data, test will be used to compare the accuracy of the model, since we the facies of the model\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n        scaled_features, correct_facies_labels, test_size=0.2, random_state=42)","b6202d2f":"#Training the classifier\nfrom sklearn import svm\n\nclf = svm.SVC()\nclf.fit(X_train,y_train)","7444f90a":"#Predict\npredicted_labels = clf.predict(X_test)","56988393":"# To simplify reading the confusion matrix, \n# a function has been written to display the matrix along with facies labels and various error metrics.\ndef display_cm(cm, labels, hide_zeros=False,\n                             display_metrics=False):\n    \"\"\"Display confusion matrix with labels, along with\n       metrics such as Recall, Precision and F1 score.\n       Based on Zach Guo's print_cm gist at\n       https:\/\/gist.github.com\/zachguo\/10296432\n    \"\"\"\n\n    precision = np.diagonal(cm)\/cm.sum(axis=0).astype('float')\n    recall = np.diagonal(cm)\/cm.sum(axis=1).astype('float')\n    F1 = 2 * (precision * recall) \/ (precision + recall)\n    \n    precision[np.isnan(precision)] = 0\n    recall[np.isnan(recall)] = 0\n    F1[np.isnan(F1)] = 0\n    \n    total_precision = np.sum(precision * cm.sum(axis=1)) \/ cm.sum(axis=(0,1))\n    total_recall = np.sum(recall * cm.sum(axis=1)) \/ cm.sum(axis=(0,1))\n    total_F1 = np.sum(F1 * cm.sum(axis=1)) \/ cm.sum(axis=(0,1))\n    #print total_precision\n    \n    columnwidth = max([len(x) for x in labels]+[5]) # 5 is value length\n    empty_cell = \" \" * columnwidth\n    # Print header\n    print(\"    \" + \" Pred\", end=' ')\n    for label in labels: \n        print(\"%{0}s\".format(columnwidth) % label, end=' ')\n    print(\"%{0}s\".format(columnwidth) % 'Total')\n    print(\"    \" + \" True\")\n    # Print rows\n    for i, label1 in enumerate(labels):\n        print(\"    %{0}s\".format(columnwidth) % label1, end=' ')\n        for j in range(len(labels)): \n            cell = \"%{0}d\".format(columnwidth) % cm[i, j]\n            if hide_zeros:\n                cell = cell if float(cm[i, j]) != 0 else empty_cell\n            print(cell, end=' ')\n        print(\"%{0}d\".format(columnwidth) % sum(cm[i,:]))\n        \n    if display_metrics:\n        print()\n        print(\"Precision\", end=' ')\n        for j in range(len(labels)):\n            cell = \"%{0}.2f\".format(columnwidth) % precision[j]\n            print(cell, end=' ')\n        print(\"%{0}.2f\".format(columnwidth) % total_precision)\n        print(\"   Recall\", end=' ')\n        for j in range(len(labels)):\n            cell = \"%{0}.2f\".format(columnwidth) % recall[j]\n            print(cell, end=' ')\n        print(\"%{0}.2f\".format(columnwidth) % total_recall)\n        print(\"       F1\", end=' ')\n        for j in range(len(labels)):\n            cell = \"%{0}.2f\".format(columnwidth) % F1[j]\n            print(cell, end=' ')\n        print(\"%{0}.2f\".format(columnwidth) % total_F1)\n    \n                  \ndef display_adj_cm(\n        cm, labels, adjacent_facies, hide_zeros=False, \n        display_metrics=False):\n    \"\"\"This function displays a confusion matrix that counts \n       adjacent facies as correct.\n    \"\"\"\n    adj_cm = np.copy(cm)\n    \n    for i in np.arange(0,cm.shape[0]):\n        for j in adjacent_facies[i]:\n            adj_cm[i][i] += adj_cm[i][j]\n            adj_cm[i][j] = 0.0\n        \n    display_cm(adj_cm, labels, hide_zeros, \n                             display_metrics)","98158356":"\nfrom sklearn.metrics import confusion_matrix\n\nconf = confusion_matrix(y_test, predicted_labels)\ndisplay_cm(conf, facies_labels, hide_zeros=True)","4b9edfd8":"def accuracy(conf):\n    total_correct = 0.\n    nb_classes = conf.shape[0]\n    for i in np.arange(0,nb_classes):\n        total_correct += conf[i][i]\n    acc = total_correct\/sum(sum(conf))\n    return acc","797d8a4e":"# The error within these 'adjacent facies' can also be calculated. \nadjacent_facies = np.array([[1], [0,2], [1], [4], [3,5], [4,6,7], [5,7], [5,6,8], [6,7]])\n\ndef accuracy_adjacent(conf, adjacent_facies):\n    nb_classes = conf.shape[0]\n    total_correct = 0.\n    for i in np.arange(0,nb_classes):\n        total_correct += conf[i][i]\n        for j in adjacent_facies[i]:\n            total_correct += conf[i][j]\n    return total_correct \/ sum(sum(conf))","01917548":"print('Facies classification accuracy = %f' % accuracy(conf))\nprint('Adjacent facies classification accuracy = %f' % accuracy_adjacent(conf, adjacent_facies))","6d4704e1":"#model selection takes a few minutes, change this variable\n#to true to run the parameter loop\ndo_model_selection = True\n\nif do_model_selection:\n    C_range = np.array([.01, 1, 5, 10, 20, 50, 100, 1000, 5000, 10000])\n    gamma_range = np.array([0.0001, 0.001, 0.01, 0.1, 1, 10])\n    \n    fig, axes = plt.subplots(3, 2, \n                        sharex='col', sharey='row',figsize=(10,10))\n    plot_number = 0\n    for outer_ind, gamma_value in enumerate(gamma_range):\n        row = int(plot_number \/ 2)\n        column = int(plot_number % 2)\n        cv_errors = np.zeros(C_range.shape)\n        train_errors = np.zeros(C_range.shape)\n        for index, c_value in enumerate(C_range):\n            \n            clf = svm.SVC(C=c_value, gamma=gamma_value)\n            clf.fit(X_train,y_train)\n            \n            train_conf = confusion_matrix(y_train, clf.predict(X_train))\n            cv_conf = confusion_matrix(y_test, clf.predict(X_test))\n        \n            cv_errors[index] = accuracy(cv_conf)\n            train_errors[index] = accuracy(train_conf)\n\n        ax = axes[row, column]\n        ax.set_title('Gamma = %g'%gamma_value)\n        ax.semilogx(C_range, cv_errors, label='CV error')\n        ax.semilogx(C_range, train_errors, label='Train error')\n        plot_number += 1\n        ax.set_ylim([0.2,1])\n        \n    ax.legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0.)\n    fig.text(0.5, 0.03, 'C value', ha='center',\n             fontsize=14)\n             \n    fig.text(0.04, 0.5, 'Classification Accuracy', va='center', \n             rotation='vertical', fontsize=14)","4e866ec8":"\nclf = svm.SVC(C=10, gamma=1)        \nclf.fit(X_train, y_train)\n\ncv_conf = confusion_matrix(y_test, clf.predict(X_test))\n\nprint('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\nprint('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))","979db7ee":"display_cm(cv_conf, facies_labels, \n           display_metrics=True, hide_zeros=True)","942da0c0":"display_adj_cm(cv_conf, facies_labels, adjacent_facies, \n           display_metrics=True, hide_zeros=True)","ee32d21b":"# Applying the classification model to the blind data\n# We held a well back from the training, and stored it in a dataframe called blind:\nblind","1086ae80":"y_blind = blind['Facies'].values","6e2c819a":"well_features = blind.drop(['Facies', 'Formation', 'Well Name', 'Depth'], axis=1)\nwell_features.describe()","c465cb39":"# Now we can transform this with the scaler we made before:\nX_blind = scaler.transform(well_features)","989a5567":"# Now it's a simple matter of making a prediction and storing it back in the dataframe:\ny_pred = clf.predict(X_blind)\nblind['Prediction'] = y_pred","fc91ae75":"# Let's see how did with the confusion matrix\ncv_conf = confusion_matrix(y_blind, y_pred)\n\nprint('Optimized facies classification accuracy = %.2f' % accuracy(cv_conf))\nprint('Optimized adjacent facies classification accuracy = %.2f' % accuracy_adjacent(cv_conf, adjacent_facies))","05a1ffc3":"display_cm(cv_conf, facies_labels,\n           display_metrics=True, hide_zeros=True)","96f9b9b1":"# but does remarkably well on the adjacent facies predictions.\ndisplay_adj_cm(cv_conf, facies_labels, adjacent_facies,\n               display_metrics=True, hide_zeros=True)","52dd8132":"def compare_facies_plot(logs, compadre, facies_colors):\n    #make sure logs are sorted by depth\n    logs = logs.sort_values(by='Depth')\n    cmap_facies = colors.ListedColormap(\n            facies_colors[0:len(facies_colors)], 'indexed')\n    \n    ztop=logs.Depth.min(); zbot=logs.Depth.max()\n    \n    cluster1 = np.repeat(np.expand_dims(logs['Facies'].values,1), 100, 1)\n    cluster2 = np.repeat(np.expand_dims(logs[compadre].values,1), 100, 1)\n    \n    f, ax = plt.subplots(nrows=1, ncols=7, figsize=(9, 12))\n    ax[0].plot(logs.GR, logs.Depth, '-g')\n    ax[1].plot(logs.ILD_log10, logs.Depth, '-')\n    ax[2].plot(logs.DeltaPHI, logs.Depth, '-', color='0.5')\n    ax[3].plot(logs.PHIND, logs.Depth, '-', color='r')\n    ax[4].plot(logs.PE, logs.Depth, '-', color='black')\n    im1 = ax[5].imshow(cluster1, interpolation='none', aspect='auto',\n                    cmap=cmap_facies,vmin=1,vmax=9)\n    im2 = ax[6].imshow(cluster2, interpolation='none', aspect='auto',\n                    cmap=cmap_facies,vmin=1,vmax=9)\n    \n    divider = make_axes_locatable(ax[6])\n    cax = divider.append_axes(\"right\", size=\"20%\", pad=0.05)\n    cbar=plt.colorbar(im2, cax=cax)\n    cbar.set_label((17*' ').join([' SS ', 'CSiS', 'FSiS', \n                                'SiSh', ' MS ', ' WS ', ' D  ', \n                                ' PS ', ' BS ']))\n    cbar.set_ticks(range(0,1)); cbar.set_ticklabels('')\n    \n    for i in range(len(ax)-2):\n        ax[i].set_ylim(ztop,zbot)\n        ax[i].invert_yaxis()\n        ax[i].grid()\n        ax[i].locator_params(axis='x', nbins=3)\n    \n    ax[0].set_xlabel(\"GR\")\n    ax[0].set_xlim(logs.GR.min(),logs.GR.max())\n    ax[1].set_xlabel(\"ILD_log10\")\n    ax[1].set_xlim(logs.ILD_log10.min(),logs.ILD_log10.max())\n    ax[2].set_xlabel(\"DeltaPHI\")\n    ax[2].set_xlim(logs.DeltaPHI.min(),logs.DeltaPHI.max())\n    ax[3].set_xlabel(\"PHIND\")\n    ax[3].set_xlim(logs.PHIND.min(),logs.PHIND.max())\n    ax[4].set_xlabel(\"PE\")\n    ax[4].set_xlim(logs.PE.min(),logs.PE.max())\n    ax[5].set_xlabel('Facies')\n    ax[6].set_xlabel(compadre)\n    \n    ax[1].set_yticklabels([]); ax[2].set_yticklabels([]); ax[3].set_yticklabels([])\n    ax[4].set_yticklabels([]); ax[5].set_yticklabels([])\n    ax[5].set_xticklabels([])\n    ax[6].set_xticklabels([])\n    f.suptitle('Well: %s'%logs.iloc[0]['Well Name'], fontsize=14,y=0.94)","446a297c":"compare_facies_plot(blind, 'Prediction', facies_colors)","9600abcd":"# Applying the classification model to new data\n# Now that we have a trained facies classification model we can use it to\n# identify facies in wells that do not have core data. \n# In this case, we will apply the classifier to two wells, \n# but we could use it on any number of wells for which we have the same set of well logs for input.\n\n# This dataset is similar to the training data except it does not have facies labels. \n# It is loaded into a dataframe called test_data.\nwell_data = pd.read_csv('..\/input\/validation-data\/validation_data_nofacies.csv')\nwell_data['Well Name'] = well_data['Well Name'].astype('category')\nwell_features = well_data.drop(['Formation', 'Well Name', 'Depth'], axis=1)","760c6a09":"# The data needs to be scaled using the same constants we used for the training data.\nX_unknown = scaler.transform(well_features)","efe798f0":"#predict facies of unclassified data\ny_unknown = clf.predict(X_unknown)\nwell_data['Facies'] = y_unknown\nwell_data","683d7fdd":"well_data['Well Name'].unique()","2cfc7613":"# We can use the well log plot to view the classification results along with the well logs.\nmake_facies_log_plot(\n    well_data[well_data['Well Name'] == 'STUART'],\n    facies_colors=facies_colors)\n\nmake_facies_log_plot(\n    well_data[well_data['Well Name'] == 'CRAWFORD'],\n    facies_colors=facies_colors)","7b42cdd0":"# Finally we can write out a csv file with the well data along with the facies classification results.\nwell_data.to_csv('well_data_with_facies.csv')","f01a999c":"**Conditioning the data set**"}}