{"cell_type":{"80d26cf4":"code","ba0286f6":"code","d78cd180":"code","84eb02c1":"code","708df371":"code","8c68ac1a":"code","386f6d91":"code","fc6df997":"code","ea487aba":"code","8420709c":"code","15ad43e5":"code","b7329a6e":"code","676082b7":"code","792e6c68":"code","236688d4":"code","321eb28e":"code","7fd68186":"code","c2dd4cce":"code","f701f7a7":"code","3cca90d9":"code","c5746bab":"code","0b3bd1ea":"code","76917769":"code","978f7299":"code","46cf69b4":"code","ed1637e1":"code","3f6190e5":"code","dee8aada":"code","f3faf220":"code","1761836a":"code","53936e36":"code","a81d187a":"code","a9317027":"code","6ee12db7":"code","91eea98d":"code","5de7d2d2":"code","8824bcc7":"code","57dd69d9":"code","0e9cd34d":"code","0b48d3ce":"code","f4d13c82":"code","81d863e0":"code","047597a6":"code","a185c1b7":"code","313789bb":"code","a9322763":"code","14b305bf":"code","4199040e":"code","b89b0ed5":"code","7292a8b2":"code","4375c680":"code","297ecf7c":"code","2fc7fccc":"code","bf055eb5":"code","2fd978bd":"code","60aa512e":"code","dfe33397":"code","2ccb879d":"code","f3035772":"code","522ddf50":"code","faad5016":"code","aea5ceb9":"code","6edd5345":"code","8c09fa09":"code","30ceb3f8":"code","506540af":"code","53c0b428":"code","dd620a88":"code","75e63590":"code","3d137b74":"code","34e4c90e":"code","97726c1d":"code","cbbb9e67":"code","e347741f":"code","95b29e9f":"code","ac71784c":"code","ce5f559e":"code","a077eb84":"code","004715a0":"code","aced2f2f":"code","e7d15050":"markdown","932290d8":"markdown","b993294c":"markdown","1c61dd15":"markdown","aaba419b":"markdown","754db773":"markdown","5874a360":"markdown","e0b032c0":"markdown","f4955ee2":"markdown","353f1933":"markdown","ded2d41d":"markdown","447ca858":"markdown","f0c9b869":"markdown","74a837a8":"markdown","af6ae3c8":"markdown","bf23ef7e":"markdown","485db4e3":"markdown","0a8d8fea":"markdown","67ea5062":"markdown","67cc3f9f":"markdown","05bfa8ab":"markdown","d17ecadf":"markdown","2c9630cd":"markdown","fe526d0e":"markdown","11fe7c67":"markdown","40b5c946":"markdown","8e664f42":"markdown","fd880b3b":"markdown","2db13a0e":"markdown","b63eebe5":"markdown","d8f4743b":"markdown","659334b8":"markdown","3086fddf":"markdown","80c98e72":"markdown","fa22b21d":"markdown","438e48e1":"markdown","d99a8994":"markdown","1cafda39":"markdown","c2109ede":"markdown","c8734abd":"markdown","faf447c4":"markdown","227d7bb8":"markdown","8b273297":"markdown","0479ee4d":"markdown","59134da2":"markdown","d01fab2d":"markdown","d19a99f9":"markdown","90946d59":"markdown","cefb02a0":"markdown","542ced48":"markdown","d7a076de":"markdown","86227f03":"markdown","5ed5008e":"markdown","60819f33":"markdown","72d673ad":"markdown","df2f694b":"markdown","e23d04dd":"markdown","43ad9324":"markdown","43d994a2":"markdown","17ad37ef":"markdown","48091e95":"markdown","d4f7f12a":"markdown","71f5531c":"markdown"},"source":{"80d26cf4":"import numpy as np\nimport pandas as pd\nnew_data=pd.read_csv('..\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')#import data csv file\nnew_data.info() #information about our data type","ba0286f6":"new_data","d78cd180":"print(new_data['age'].isnull().sum().sum())","84eb02c1":"import matplotlib.pyplot as plt\nplt.xlabel(\"target\")\nplt.ylabel(\"target value count\")\nplt.title(\"bar plot of target\")\nvc=new_data[\"target\"].value_counts()\nvc.plot.bar(rot=0)","708df371":"#une autre fa\u00e7on de faire barplot\nimport seaborn as sns\nsns.countplot(x='sex',data=new_data)\nplt.xticks()","8c68ac1a":" \n\ng=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'age',bins=5)","386f6d91":"\n\ng=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'oldpeak',bins=5)","fc6df997":"new_data[['restecg','target']].groupby(['restecg'],as_index=True).mean()","ea487aba":"new_data[['thal','target']].groupby(['thal'],as_index=True).mean()","8420709c":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'cp',bins=10)","15ad43e5":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'trestbps',bins=5)","b7329a6e":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'chol',bins=5)","676082b7":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'fbs',bins=7)\n","792e6c68":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'restecg',bins=7)","236688d4":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'thalach',bins=5)\n","321eb28e":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'exang',bins=5)","7fd68186":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'oldpeak',bins=5)","c2dd4cce":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'slope',bins=5)","f701f7a7":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'ca',bins=7)\n\n\n\n","3cca90d9":"g=sns.FacetGrid(new_data,col='target')\n\ng.map(plt.hist,'thal',bins=7)","c5746bab":"from sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import metrics","0b3bd1ea":"x=new_data[[\"sex\",\"oldpeak\",\"exang\",\"ca\",\"cp\"]]\ny=new_data[\"target\"]\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.35,random_state=40) #splitting data with test size of 35%\n\nlogreg = LogisticRegression()   #build our logistic model\nlogreg.fit(x_train, y_train)  #fitting training data\ny_pred  = logreg.predict(x_test)    #testing model\u2019s performance\nprint(\"Accuracy={:.2f}\".format(logreg.score(x_test, y_test)))","76917769":"confusion_matrix= pd.crosstab(y_test,y_pred, rownames=['actual'],colnames=['predicted'])\nprint(confusion_matrix)","978f7299":"#Importing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","46cf69b4":"from sklearn.metrics import roc_curve\n\n# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_pred, pos_label=1)\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","ed1637e1":"from sklearn.metrics import roc_auc_score\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, y_pred)","3f6190e5":"print(auc_score1)","dee8aada":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","f3faf220":"features = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']\nx = new_data.loc[:, features].values","1761836a":"y = new_data.loc[:,['target']].values","53936e36":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nx = StandardScaler().fit_transform(x)","a81d187a":"pd.DataFrame(data = x, columns = features).head()\n","a9317027":"pca = PCA(n_components=2)","6ee12db7":"principalComponents = pca.fit_transform(x)","91eea98d":"principalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\n","5de7d2d2":"principalDf.head(5)","8824bcc7":"new_data[['target']]","57dd69d9":"finalDf = pd.concat([principalDf, new_data[['target']]], axis = 1)\nfinalDf.head(5)","0e9cd34d":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 Component PCA', fontsize = 20)\n\n\ntargets = [1, 0]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","0b48d3ce":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nx=principalDf\ny=new_data[\"target\"]\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.24,random_state=40) #splitting data with test size of 35%\n\nlogreg = LogisticRegression()   #build our logistic model\nlogreg.fit(x_train, y_train)  #fitting training data\ny_pred  = logreg.predict(x_test)    #testing model\u2019s performance\nprint(\"Accuracy={:.2f}\".format(logreg.score(x_test, y_test)))","f4d13c82":"confusion_matrix= pd.crosstab(y_test,y_pred, rownames=['actual'],colnames=['predicted'])\nprint(confusion_matrix)","81d863e0":"#Importing the classification report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","047597a6":"from sklearn.metrics import roc_curve\n\n# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, y_pred, pos_label=1)\nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","a185c1b7":"from sklearn.metrics import roc_auc_score\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, y_pred)\nprint(auc_score1)","313789bb":"#import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","a9322763":"#Choosing the features as x and the target as y\nx=finalDf[['principal component 1','principal component 2']]\ny=finalDf['target']","14b305bf":"#import the relevant libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=30) #split our data with test size of 20% \n\n","4199040e":"n_neighbors=30\nscores=[]\nfor k in range(1,30):\n    knn=KNeighborsClassifier(n_neighbors-k)\n    knn.fit(x_train,y_train)\n    y_pred=knn.predict(x_test)\n    print('Accuracy for k=',k,'is:',round(accuracy_score(y_pred,y_test),2))\n    scores.append(round(accuracy_score(y_pred,y_test),2))","b89b0ed5":"import matplotlib.pyplot as plt\nplt.plot(range(1,30),scores)\nplt.xlabel('Value K for KNN')\nplt.ylabel('Testing Accuracy')","7292a8b2":"from sklearn import tree   \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n","4375c680":"#features extraction\nx=finalDf[['principal component 1','principal component 2']]\ny=finalDf['target']\n#splitting data\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.20,random_state=10)\n\n#applying tree algorithm\ntr = tree.DecisionTreeClassifier()  \ntr.fit(x_train, y_train)   #fitting our model\ny_pred=tr.predict(x_test)   # evaluating our model\nprint(\"score:{}\".format(accuracy_score(y_test, y_pred)))","297ecf7c":"y_test","2fc7fccc":"len(y_pred)","bf055eb5":"len(finalDf)","2fd978bd":"\ncn=finalDf.target\n\n#conversion from list of numpy.int64 to list of string\ncn=[str(x) for x in cn]\n","60aa512e":"import graphviz\ndot_data= tree.export_graphviz(tr, out_file=None, class_names= cn) #our model\ngraph= graphviz.Source(dot_data)#code to visualize our tree\n#graph.render(\"finalDf\")\n\ngraph.render(\"finalDf\",view=True)\ngraph","dfe33397":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier #Importing Random Forest Classifier\nfrom sklearn import metrics  # Importing metrics to test accuracy\n","2ccb879d":"X=finalDf[['principal component 1','principal component 2']]\ny=finalDf['target']\nx_train, x_test, y_train, y_test= train_test_split(X, y, test_size=0.3) #splitting data with test size of 30%\n#Random Forest prediction\nclf=RandomForestClassifier(n_estimators=10)  #Creating a random forest with 100 decision trees\nclf.fit(x_train, y_train)  #Training our model\ny_pred=clf.predict(x_test)  #testing our model\nprint(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))  #Measuring the accuracy of our modelx=finalDf[['principal component 1','principal component 2']]\n","f3035772":"from sklearn.cluster import AgglomerativeClustering #Importing our clustering algorithm : Agglomerative\nmodel=AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\nclust_labels=model.fit_predict(finalDf)  #Applying agglomerative algorithm with 2 clusters, using euclidean distance as a metric\n","522ddf50":"clust_labels","faad5016":"len(clust_labels)","aea5ceb9":"y=finalDf['target']\ny","6edd5345":"print(\"Accuracy:\", metrics.accuracy_score(y, clust_labels))","8c09fa09":"#print each point\u2019s corresponding cluster\n#The number of clusters for each point:\nagglomerative=pd.DataFrame(clust_labels)\nagglomerative","30ceb3f8":"#import matplotlib.pyplot as plt\nfig =plt.figure()\nax = fig.add_subplot(111)\nscatter = ax.scatter (finalDf ['principal component 1'] , finalDf [\"principal component 2\"] , c= agglomerative[0], s=50)#c : color,s : scalar or array_like, shape (n, )\nax.set_title(\"Agglomerative Clutering\")\nax.set_xlabel(\"principal component 1\")\nax.set_ylabel(\"Sprincipal component 2\")\nplt.colorbar(scatter)","506540af":"#import matplotlib.pyplot as plt\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize=(10,7))\nplt.title(\"Customer Dendrograms\")\ndend=shc.dendrogram(shc.linkage(finalDf, method=\"complete\"))\n#To visualize how the groups are made we can use a dendrogram.","53c0b428":"from sklearn.cluster import AgglomerativeClustering #Importing our clustering algorithm : Agglomerative\nmodel=AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='complete')\nclust_labels=model.fit_predict(new_data)  #Applying agglomerative algorithm with 2 clusters, using euclidean distance as a metric\ny=new_data['target']\nprint(\"Accuracy:\", metrics.accuracy_score(y, clust_labels))\n#with pca we have a better accuracy","dd620a88":"from sklearn.cluster import KMeans  #Importing our clustering algorithm: KMeans\nkmeans=KMeans(n_clusters=2, random_state=0)  #Cluster our data by choosing 2 as number of clusters, ??rabdom_state:Determines random number generation for centroid initialization\nkmeans.fit(finalDf)","75e63590":"#This instruction enables us to print the label of each point in our data after the clustering is done.\nlabels=pd.DataFrame(kmeans.labels_)\nlabels","3d137b74":"kmeans.predict(finalDf)","34e4c90e":"print(\"Accuracy:\", metrics.accuracy_score(y, kmeans.predict(finalDf)))","97726c1d":"#Each inner list represents the coordinates of a cluster center. qst: winta n7abso l iterations\nkmeans.predict(finalDf)\nprint(kmeans.cluster_centers_)   #Printing the coordinates of cluster centers.","cbbb9e67":"plt.scatter(finalDf[\"principal component 1\"][kmeans.labels_ == 0],          \n            finalDf[\"principal component 2\"][kmeans.labels_ == 0],s=80,c='magenta',label='heartattack') #label?\n\nplt.scatter(finalDf[\"principal component 1\"][kmeans.labels_ == 1],\n           finalDf[\"principal component 2\"][kmeans.labels_ == 1],s=80,c='yellow',label='no_heartattack')\n\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label = 'Centroids')#??\nplt.title('Clusters of Customers')\nplt.xlabel('principal component 1')\nplt.ylabel('principal component 2')\nplt.legend()\nplt.show()","e347741f":"kmeans=KMeans(n_clusters=2, random_state=0)  #Cluster our data by choosing 2 as number of clusters, ??rabdom_state:Determines random number generation for centroid initialization\nkmeans.fit(new_data)\n#This instruction enables us to print the label of each point in our data after the clustering is done.\nlabels=pd.DataFrame(kmeans.labels_)\nprint(\"Accuracy:\", metrics.accuracy_score(y, kmeans.predict(new_data)))\n","95b29e9f":"#k best value\n#import numpy as np\nfrom sklearn.cluster import KMeans  #Importing our clustering algorithm: KMeans\nsum_of_squared_distances=[]\n\nk=range(1,15)\nfor k in k:\n    km= KMeans(n_clusters=k)\n    km= km.fit(new_data)\n    \n    \n    sum_of_squared_distances.append(km.inertia_)\n    \nnrow = len(sum_of_squared_distances)\nprint(sum_of_squared_distances)\n\n\nprint (nrow)","ac71784c":"#import numpy as np\n#import matplotlib.pyplot as plt\n%matplotlib inline\nk=range(1,15)\nplt.plot(k, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","ce5f559e":"from sklearn.model_selection import train_test_split\nX=finalDf[['principal component 1','principal component 2']]\ny=finalDf['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)","a077eb84":"#import pandas as pd\n#import numpy as np\n#import matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(X_train, y_train)","004715a0":"y_pred = svclassifier.predict(X_test)","aced2f2f":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","e7d15050":"The accuracy , precision,, recall and support with pca are better","932290d8":"Optimal K Value Plot\nThanks to this plot we can easily find out the best value of K , we can notice that the best accuracy is 0.85 when k= 13 or k=11","b993294c":"A decision tree is a set of hierarchical decisions which eventually give us a final result.\n","1c61dd15":"# DATA PREPROCESSING","aaba419b":"The accuracy of a machine learning classification algorithm is one way to measure how often the algorithm classifies a data point correctly. Accuracy is the number of correctly predicted data points out of all the data points. More formally, it is defined as the number of true positives and true negatives divided by the number of true positives, true negatives, false positives, and false negatives","754db773":"# UNSUPERVISED LEARNING","5874a360":"the elbow method: to determine the best k value , based on sse : the sum of squared diffrence between the cluster center and each obsarvation related to that cluster\nthe optimal number of clusers corresponds to the optimal minimum value of sse (elbow joint) ","e0b032c0":"# Conclusion","f4955ee2":"In this graph : we can see that the amount of people having a heart attack in this data set is more than people who didn't have it.\nmost people who have heart attack in our dataset","353f1933":"# Agglomerative","ded2d41d":"Random Forest we take random selection of features rather than using all features to grow the trees.","447ca858":"k of the optimal minimum value is 2","f0c9b869":"to view the correlation between target  and age:\nData correlation: In ML,  how our features correspond with our output.","74a837a8":"most people with exang 0 have heart attack","af6ae3c8":"most people with 0 old peak have heart attack","bf23ef7e":"# Health-care-data-set-on-heart-attack-possibility","485db4e3":"Restrecg 1: there are more people that have heart attacks\nRestreg 0 and 2: there are more people that dont have heart attack","0a8d8fea":"dimension reduction techniques to transform the larger dataset into a smaller dataset by identifying the correlations and patterns with preserving most of the valuable information.","67ea5062":"# # k-means algo","67cc3f9f":"In thal 2 : there are more people who had heart attacks","05bfa8ab":"restrecg=1: most of people have a heart attack","d17ecadf":"With PCA","2c9630cd":"About data set:\nThis database : 14 attributes, .The \"target\" field refers to the presence of heart disease in the patient. It is integer valued 0 = no\/less chance of heart attack and 1 = more chance of heart attack\nAttribute Information\n1) age\n2) sex (1 man, 0 woman)\n3) chest pain type (4 values)\n4) resting blood pressure\n5) serum cholestoral in mg\/dl\n6)fasting blood sugar > 120 mg\/dl\n7) resting electrocardiographic results (values 0,1,2)\n8) maximum heart rate achieved\n9) exercise induced angina\n10) oldpeak = ST depression induced by exercise relative to rest\n11)the slope of the peak exercise ST segment\n12) number of major vessels (0-3) colored by flourosopy\n13) thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n14) target: 0= less chance of heart attack 1= more chance of heart attack","fe526d0e":"recall: Out of all the positive classes, how much we predicted correctly. It should be high as possible. (tp\/tp+fp)\n precision :how many are correctly classified among that class tp\/tp+fn\n support: is the number of occurence of the given class in your dataset","11fe7c67":"A true positive or true negative is a data point that the algorithm correctly classified as true or false, respectively. A false positive or false negative, on the other hand, is a data point that the algorithm incorrectly classified","40b5c946":"Performing standardization is a crucial step because the original variables may have different scales. We need to bring them to a similar range to get reasonable covariance analysis.","8e664f42":"The fit method of SVC class is called to train the algorithm on the training data,","fd880b3b":"We have more women than men ","2db13a0e":"most people with ca = 0 have heart attack, -most people with ca=2 dont have heart attack\n","b63eebe5":"\nTrestbs 140: more people had a heart attack\n-between 180 and 200: less people who have a heart attack ","d8f4743b":"There are more people with heart attack in fbs 0 ","659334b8":"we have better results with PCA  , The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\n","3086fddf":"most people with thal =2 have heart attack, - most people with thal = 3 don't have heart attack","80c98e72":"# AUC-ROC Curve","fa22b21d":"# LOGISTIC REGRESSION","438e48e1":"between 100 and 125: pmost of people dont have heart attack \n, -between 150 and 175: most people have heart attack","d99a8994":"People between 40 and 50 have heart attacks\n\nin 30 and  70: more people have heart attacks compared to those they don't ","1cafda39":"Suppervised learning is better in our case with better accuracy scores; \nSVM algorithm,knn and logistic regression gave us the best accuracy score with 0.85 and 0.89 .","c2109ede":"Between 200 and 300 : more people who had a heart attack","c8734abd":"True positive Rate : tp\/tp+fn , Specificity tells us what proportion of the positive class got correctly classified.\nFalse Positive Rate: fp\/tn+fp, PR tells us what proportion of the positive class got incorrectly classified by the classifier.\n\nThe higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\ncourbe trace le taux de vrais positifs en fonction du taux de faux positifs \nis a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:\n\nTrue Positive Rate\nFalse Positive Rate","faf447c4":"# DATA VISUALIZATION ","227d7bb8":"The first principal component will capture most of the variance from the original variables and the second principal component captures the second highest variance and so on","8b273297":"K-MEANS algorithm is sensitive to scaling (might change results) ,so we try without pca:","0479ee4d":"# svm","59134da2":"To know the number of missing values in the data set:","d01fab2d":"Between 0 and 1 oldpeak: there are more people having heart attacks ","d19a99f9":"# DECESION TREE","90946d59":"cp 0: more people who don't have HA\ncp 2: more people have heart attack\n\n","cefb02a0":"BLACK: people who had a heart attack\nWHITE: people who didnt have a heart attack","542ced48":"find hyperplane in n dimential space that distinctivly classify the data point.\nobj: find a plane that has the max margin","d7a076de":"we have no missing data in our data set","86227f03":"# RANDOM FOREST","5ed5008e":"Accuracy Without pca is better so kmeans is sensitive to scaling","60819f33":"Capture the hierarchical relationship between the clusters.","72d673ad":"# ROC CURVE","df2f694b":"Knn Definition\nK-Nearest-Neighbor algorithm, often abbreviated K-NN, is used to classify new data based on the assumption that it\u2019s similar to its K nearest neighbors","e23d04dd":"# the elbow method","43ad9324":"Predict the probability that an input x belongs to class b as an output.\nFor binary classification","43d994a2":"Trying without PCA \n","17ad37ef":"# KNN","48091e95":"The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes.\nThe higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n","d4f7f12a":"most people with slope =2 have heart attack, -most people with slope =0 don't have heart attack","71f5531c":"data preprocessing:  Because we can have dirty data caused by : \nhuman error, equipement malfunction...\nIn our case the data is clean.\n"}}