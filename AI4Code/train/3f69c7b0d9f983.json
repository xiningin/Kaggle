{"cell_type":{"55c62be8":"code","1028fb5c":"code","8ae47f39":"code","6cceaee0":"code","fe156b7f":"code","e569da76":"code","f4caceea":"code","ac4f8977":"code","7418322f":"code","25781a1d":"code","657e683b":"code","9cc5c7bf":"code","4cdaf4cc":"code","5cfd4a85":"code","9bd26836":"code","49470fdd":"code","4540a42d":"code","9282b1ae":"code","9b892013":"code","0edc0a65":"code","8dc28532":"code","b7490cdb":"code","5e2dc42b":"code","ccb9cd0b":"code","0260deaa":"code","5f81bef2":"code","aeaa684f":"code","35616f9a":"code","1497daac":"code","95d5fb80":"code","84c4fbd1":"code","5fd178f7":"code","8abe7479":"code","1c3617ec":"code","eb4be6c5":"code","555b70c4":"code","57702f69":"code","83dd8edf":"code","b9f7114a":"code","22f4d258":"code","81ad13d5":"code","bcde2a12":"code","fe0ae1fc":"code","6b2d217b":"code","50a189b5":"code","0572427d":"code","1e4c3083":"code","354e2269":"code","16582d03":"code","f0615b40":"code","bdb6ede1":"code","7b93dbba":"code","9c6c33d8":"code","9df6ce94":"code","20ca42c5":"code","10a4a711":"code","383229e2":"code","b6d5fc04":"code","bee32cf2":"code","36ce9343":"code","e9b415b6":"code","c54b1baf":"code","33b0f2c7":"code","3cfaac73":"code","107fe8e1":"code","5dcc0cf6":"code","65f222c4":"code","b51a71f3":"markdown","d1ddaff5":"markdown","918bf975":"markdown","d23d2c3d":"markdown","f791cb80":"markdown","50de3e46":"markdown","a357632f":"markdown","909140b7":"markdown","212c319e":"markdown","19ce13cf":"markdown","e2611c65":"markdown","9433885d":"markdown","3b0a4c33":"markdown","6466f183":"markdown","1e8fe4d8":"markdown","90d7a88d":"markdown","c210c8ce":"markdown","d5574f86":"markdown","43190712":"markdown","2a5c8808":"markdown","6e121c23":"markdown","12c5eb83":"markdown","0cf63a73":"markdown","dbb25e20":"markdown","7a237b8b":"markdown","f8ac046a":"markdown","b048e48d":"markdown","854a17e1":"markdown","e8b2a309":"markdown","c1c8f347":"markdown"},"source":{"55c62be8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model, metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","1028fb5c":"house = pd.read_csv('..\/input\/ushouseprices\/train.csv')","8ae47f39":"house.head()","6cceaee0":"house.info()","fe156b7f":"# listing all missing values percentage wise\n(house.isnull().sum() * 100 \/ len(house)).sort_values(ascending=False).head()","e569da76":"# Counting unique value in each categorical column\ncat_col =house.select_dtypes(include=['object'])\n\nfor i in list(cat_col.columns):\n    print(\"We have {} unique value in {} column: {}.\".format(len(cat_col[i].unique()), \n                                                     i, cat_col[i].unique()))\n    print('*'*140)","f4caceea":"house['SalePrice'].describe()","ac4f8977":"#histogram\nsns.distplot(house['SalePrice']);\n","7418322f":"#skewness and kurtosis\nprint(\"Skewness: %f\" % house['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % house['SalePrice'].kurt())","25781a1d":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([house['SalePrice'], house[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n","657e683b":"# Scatter plot between target feature(SalePrice) and the important features\nfor col in ('GrLivArea','PoolArea','GarageArea','OpenPorchSF','LotArea','TotalBsmtSF','MasVnrArea'):\n    data = pd.concat([house['SalePrice'], house[col]], axis=1)\n    data.plot.scatter(x=col, y='SalePrice', ylim=(0,800000),title = \"SalePrice vs {}\".format(col));\n","9cc5c7bf":"#check missing ratios\ntotal_data_na = ((house.isnull().sum() \/ len(house)) * 100).sort_values(ascending=False)","4cdaf4cc":"#check missing ratios top 30\ntotal_data_na = total_data_na.drop(total_data_na[total_data_na == 0].index).sort_values(ascending=False)[:30]\ntotal_data_na\n","5cfd4a85":"# plotting missing values\nplt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=total_data_na.index, y=total_data_na)\n\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing value', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=25)","9bd26836":"# dropping columns having Nan values more than 75%\nhouse = house.loc[:, house.isnull().mean() < .75]\n","49470fdd":"# FireplaceQu - NA means no fireplace, so replacing with None\nhouse[\"FireplaceQu\"] = house[\"FireplaceQu\"].fillna(\"None\")","4540a42d":"# for LotFrontage, replacing missing value with median\nhouse[\"LotFrontage\"]=house.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# replacing Na values to None because Na means there is no facility for Garage\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    house[col] =house[col].fillna('None')\n    \n# Replacing missing with 0 (No garage = no cars in such garage)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    house[col] = house[col].fillna(0)\n\n# Missing value in these feature means no basement (replace with 0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    house[col] = house[col].fillna(0)\n    \n# NaN means in these column is no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    house[col] = house[col].fillna('None')\n    \n# NA means no masonry veneer for houses (Fill zero for area and None for type)\nhouse[\"MasVnrType\"] = house[\"MasVnrType\"].fillna(\"None\")\nhouse[\"MasVnrArea\"] = house[\"MasVnrArea\"].fillna(0)\n\n\n# Filling MSZoning with most common value 'RL'\nhouse['MSZoning'] = house['MSZoning'].fillna(house['MSZoning'].mode()[0])\n\n\n# For Utilitites, most of the records are 'AllPub'. It won't help in prediction\nhouse = house.drop(['Utilities'], axis=1)\n\n\n# data description says- Assume typical(Typ) unless deductions are warranted\nhouse[\"Functional\"] = house[\"Functional\"].fillna(\"Typ\")\n\n\n# replacing with most repeated \"SBrkr\" value\nhouse['Electrical'] = house['Electrical'].fillna(house['Electrical'].mode()[0])\n\n# set \"TA\" most frequent in place of missing value\nhouse['KitchenQual'] = house['KitchenQual'].fillna(house['KitchenQual'].mode()[0])\n\n\n# replacing with most common value \nhouse['Exterior1st'] = house['Exterior1st'].fillna(house['Exterior1st'].mode()[0])\nhouse['Exterior2nd'] = house['Exterior2nd'].fillna(house['Exterior2nd'].mode()[0])\n\n\n# replacing with most common value again\nhouse['SaleType'] = house['SaleType'].fillna(house['SaleType'].mode()[0])\n\n# MSSubClass-type of dwelling, NA means no building class. (fill with None)\nhouse['MSSubClass'] = house['MSSubClass'].fillna(\"None\")\n","9282b1ae":"((house.isnull().sum()\/len(house))*100).sort_values(ascending=False).head()","9b892013":"#MSSubClass=The building class\nhouse['MSSubClass'] = house['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nhouse['OverallCond'] = house['OverallCond'].astype(str)\n","0edc0a65":"df_cat = house.select_dtypes(include = 'object')\n#analysis of categorical variables \nplt.figure(figsize=(30,100))\nfor i in enumerate(df_cat.columns.values):\n    sns.set_style(\"whitegrid\")\n    plt.subplot(12,4,i[0]+1)\n    sns.countplot(i[1],data = df_cat)\n    plt.xticks(rotation = 45,fontsize=13)\n    plt.xlabel(i[1],fontsize = 15,fontweight = 'bold')\n    plt.title('Sale_Price'+' vs '+i[1],fontweight = 'bold',fontsize=15)","8dc28532":"house.head()","b7490cdb":"num_vars = house.select_dtypes(exclude=['object']).columns.tolist()","5e2dc42b":"cat_cols = house.select_dtypes(include=['object']).columns.tolist()","ccb9cd0b":"# get dummy variables for season, weekday, mnth and weathersit\ndummy_vars = pd.get_dummies(house[cat_cols],drop_first=True)\n\n# concat the dummy df with original df\nhouse = pd.concat([house,dummy_vars], axis = 1)\n\n# drop season column\nhouse.drop(cat_cols, axis=1, inplace=True)","0260deaa":"house.shape","5f81bef2":"from sklearn.model_selection import train_test_split\n# We specify this so that the train and test data set always have the same rows, respectively\n# Split train test dataset\ndf_train, df_test = train_test_split(house, train_size = 0.7, random_state = 10 )\nprint(df_train.shape)\nprint(df_test.shape)\n","aeaa684f":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","35616f9a":"# Scaling of train set\n# instantiate an object\nscaler = MinMaxScaler()\n# fit and transform on training data\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","1497daac":"# transform test dataset \ndf_test[num_vars] = scaler.transform(df_test[num_vars])\ndf_test.head()","95d5fb80":"# distribution after scaling\nplt.figure(figsize=(16,6))\nplt.subplot(121)\nsns.distplot(df_train.SalePrice)\nplt.subplot(122)\nsns.distplot(df_test.SalePrice)","84c4fbd1":"# Creating X and y data dataframe for train set\ny_train = df_train.pop('SalePrice')\nX_train = df_train\nX_train.head()","5fd178f7":"# Creating X and y data dataframe for test set\ny_test = df_test.pop('SalePrice')\nX_test = df_test\n\nX_test.head()\n","8abe7479":"# Define dictionary to store our rankings\nranks = {}\n# Create our function which stores the feature rankings to the ranks dictionary\ndef ranking(ranks, names, order=1):\n    minmax = MinMaxScaler()\n    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n    ranks = map(lambda x: round(x,5), ranks)\n    return dict(zip(names, ranks))","1c3617ec":"imp_col = X_train.columns.values","eb4be6c5":"# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n# it will not handle the overfitting\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","555b70c4":"cv_result_r = pd.DataFrame(model_cv.cv_results_)\ncv_result_r['param_alpha'] = cv_result_r['param_alpha'].astype('float32')\ncv_result_r.head()\n","57702f69":"plt.figure(figsize=(16,8))\nplt.plot(cv_result_r['param_alpha'],cv_result_r['mean_train_score'])\nplt.plot(cv_result_r['param_alpha'],cv_result_r['mean_test_score'])\nplt.xlabel('Alpha')\nplt.xscale('log')\nplt.ylabel('R2 Score')\nplt.show()","83dd8edf":"# Printing the best hyperparameter alpha\nprint(model_cv.best_params_)","b9f7114a":"#Fitting Ridge model for alpha = 10 and printing coefficients which have been penalised\nalpha = 10\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\n# print(ridge.coef_)","22f4d258":"# Lets calculate some metrics such as R2 score, RSS and RMSE\ny_pred_train = ridge.predict(X_train)\ny_pred_test = ridge.predict(X_test)\n\nmetric2 = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(r2_train_lr)\nmetric2.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(r2_test_lr)\nmetric2.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(rss1_lr)\nmetric2.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(rss2_lr)\nmetric2.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(mse_train_lr)\nmetric2.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(mse_test_lr)\nmetric2.append(mse_test_lr**0.5)","81ad13d5":"rank_ridge = ranking(np.abs(ridge.coef_), imp_col)\nsorted(rank_ridge.items(), key=lambda item: item[1] , reverse=True)[:20]","bcde2a12":"#Fitting Ridge model for alpha = 20 and printing coefficients which have been penalised\nalpha2x = 20\nridge = Ridge(alpha=alpha2x)\n\nridge.fit(X_train, y_train)\n# print(ridge.coef_)","fe0ae1fc":"# Lets calculate some metrics such as R2 score, RSS and RMSE\ny_pred_train = ridge.predict(X_train)\ny_pred_test = ridge.predict(X_test)\n\nmetric2 = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(r2_train_lr)\nmetric2.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(r2_test_lr)\nmetric2.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(rss1_lr)\nmetric2.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(rss2_lr)\nmetric2.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(mse_train_lr)\nmetric2.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(mse_test_lr)\nmetric2.append(mse_test_lr**0.5)","6b2d217b":"rank_ridge = ranking(np.abs(ridge.coef_), imp_col)\nsorted(rank_ridge.items(), key=lambda item: item[1] , reverse=True)[:20]","50a189b5":"lasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","0572427d":"cv_result_l = pd.DataFrame(model_cv.cv_results_)\ncv_result_l['param_alpha'] = cv_result_l['param_alpha'].astype('float32')\ncv_result_l.head()\n","1e4c3083":"plt.figure(figsize=(16,8))\nplt.plot(cv_result_l['param_alpha'],cv_result_l['mean_train_score'])\nplt.plot(cv_result_l['param_alpha'],cv_result_l['mean_test_score'])\nplt.xlabel('Alpha')\nplt.xscale('log')\nplt.ylabel('R2 Score')\nplt.show()","354e2269":"# Printing the best hyperparameter alpha\nprint(model_cv.best_params_)","16582d03":"#Fitting Lasso model for alpha = 0.0001 and printing coefficients which have been penalised\n\nalpha =0.0001\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train, y_train) ","f0615b40":"# Lets calculate some metrics such as R2 score, RSS and RMSE\n\ny_pred_train = lasso.predict(X_train)\ny_pred_test = lasso.predict(X_test)\n\nmetric3 = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(r2_train_lr)\nmetric3.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(r2_test_lr)\nmetric3.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(rss1_lr)\nmetric3.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(rss2_lr)\nmetric3.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(mse_train_lr)\nmetric3.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(mse_test_lr)\nmetric3.append(mse_test_lr**0.5)","bdb6ede1":"rank_lasso = ranking(np.abs(lasso.coef_), imp_col)\ntop_lasso_features = sorted(rank_lasso.items(), key=lambda item: item[1] , reverse=True)[:20]\ntop_lasso_features","7b93dbba":"#Fitting lasso model for alpha = 0.0002 and printing coefficients which have been penalised\n\nalpha2x =0.0002\n\nlasso = Lasso(alpha=alpha2x)\n        \nlasso.fit(X_train, y_train) ","9c6c33d8":"# Lets calculate some metrics such as R2 score, RSS and RMSE\n\ny_pred_train = lasso.predict(X_train)\ny_pred_test = lasso.predict(X_test)\n\nmetric3 = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(r2_train_lr)\nmetric3.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(r2_test_lr)\nmetric3.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(rss1_lr)\nmetric3.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(rss2_lr)\nmetric3.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(mse_train_lr)\nmetric3.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(mse_test_lr)\nmetric3.append(mse_test_lr**0.5)","9df6ce94":"rank_lasso = ranking(np.abs(lasso.coef_), imp_col)\nsorted(rank_lasso.items(), key=lambda item: item[1] , reverse=True)[:20]","20ca42c5":"# Creating a table which contain all the metrics\n\nlr_table = {'Metric': ['R2 Score (Train)','R2 Score (Test)','RSS (Train)','RSS (Test)',\n                       'MSE (Train)','MSE (Test)']\n        }\n\nlr_metric = pd.DataFrame(lr_table ,columns = ['Metric'] )\n\nrg_metric = pd.Series(metric2, name = 'Ridge Regression')\nls_metric = pd.Series(metric3, name = 'Lasso Regression')\n\nfinal_metric = pd.concat([lr_metric, rg_metric, ls_metric], axis = 1)\n\nfinal_metric","10a4a711":"betas = pd.DataFrame(index=X_train.columns)","383229e2":"betas.rows = X_train.columns","b6d5fc04":"\nbetas['Ridge'] = ridge.coef_\nbetas['Lasso'] = lasso.coef_","bee32cf2":"pd.set_option('display.max_rows', None)\nbetas.head(20)","36ce9343":"top_lasso_features","e9b415b6":"X_train_q3 = X_train.drop(['GrLivArea','Condition2_PosN','RoofMatl_WdShngl','OverallQual','Neighborhood_NoRidge'], axis = 1)\nX_test_q3 = X_test.drop(['GrLivArea','Condition2_PosN','RoofMatl_WdShngl','OverallQual','Neighborhood_NoRidge'], axis = 1)","c54b1baf":"X_test_q3.shape","33b0f2c7":"lasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train_q3, y_train) ","3cfaac73":"# Printing the best hyperparameter alpha\nprint(model_cv.best_params_)","107fe8e1":"#Fitting Ridge model for alpha = 1000 and printing coefficients which have been penalised\n\nalpha =0.0001\n\nlasso = Lasso(alpha=alpha)\n        \nlasso.fit(X_train_q3, y_train) ","5dcc0cf6":"# Lets calculate some metrics such as R2 score, RSS and RMSE\n\ny_pred_train = lasso.predict(X_train_q3)\ny_pred_test = lasso.predict(X_test_q3)\n\nmetric3 = []\nr2_train_lr = r2_score(y_train, y_pred_train)\nprint(r2_train_lr)\nmetric3.append(r2_train_lr)\n\nr2_test_lr = r2_score(y_test, y_pred_test)\nprint(r2_test_lr)\nmetric3.append(r2_test_lr)\n\nrss1_lr = np.sum(np.square(y_train - y_pred_train))\nprint(rss1_lr)\nmetric3.append(rss1_lr)\n\nrss2_lr = np.sum(np.square(y_test - y_pred_test))\nprint(rss2_lr)\nmetric3.append(rss2_lr)\n\nmse_train_lr = mean_squared_error(y_train, y_pred_train)\nprint(mse_train_lr)\nmetric3.append(mse_train_lr**0.5)\n\nmse_test_lr = mean_squared_error(y_test, y_pred_test)\nprint(mse_test_lr)\nmetric3.append(mse_test_lr**0.5)","65f222c4":"rank_lasso = ranking(np.abs(lasso.coef_), imp_col)\ntop_lasso_q3 = sorted(rank_lasso.items(), key=lambda item: item[1] , reverse=True)[:20]\ntop_lasso_q3","b51a71f3":"# Ridge Regression","d1ddaff5":"## Analysis Ridge and Lasso","918bf975":"### converting data types of some columns whcih are int but it should be str","d23d2c3d":"## using dummies to convert categorical columns to numerical columns","f791cb80":"## According to Problem Statement - Part II What if we choose double value of alpha","50de3e46":"## Data Preparation","a357632f":"## Ridge and Lasso Regression\n\nLet's now try predicting house prices, a dataset used in simple linear regression, to perform ridge and lasso regression.","909140b7":"### Top 20 features from ridge after 2x alpha","212c319e":"# Subjective Question \n## Question 3\n#### After building the model, you realised that the five most important predictor variables in the lasso model are not available in the incoming data. You will now have to create another model excluding the five most important predictor variables. Which are the five most important predictor variables now?\n","19ce13cf":"    A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n\n\n\n    The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n\n\n    The company wants to know:\n\n    Which variables are significant in predicting the price of a house, and\n\n    How well those variables describe the price of a house.","e2611c65":"### Rescaling the Features\nWe will use MinMax scaling.\n\nDuring EDA we could observe that there is different range of data in the data set. So it becomes important to scale the data. Here we will be using Min-Max scaling (normalisation) to scale both training and tesing dataset.","9433885d":"## Lets observe the changes in the coefficients after regularization","3b0a4c33":"# Lasso","6466f183":"## Top 20 important features from lasso after 2x alpha","1e8fe4d8":"## Relationship with numerical variables","90d7a88d":"## Data Cleaning and feature engineering","c210c8ce":"## Splitting data into train and test ","d5574f86":"## Top 20 features from ridge ","43190712":"#### some important numerical feature which can affect the house price significantly based on business requirements\n    GrLivArea,PoolArea,GarageArea,OpenPorchSF,LotArea,TotalBsmtSF,MasVnrArea","2a5c8808":"# Problem Statement","6e121c23":"## According to Problem Statement - Part II What if we choose double value of alpha","12c5eb83":"## Now checking the missing values\n","0cf63a73":"## analysing the target feature , i.e SalePrice","dbb25e20":"## Imputation according to the required DataType and Data_description text file","7a237b8b":"    Let's drop the top 5 features from the model and build model again ","f8ac046a":"## Top 20 important features from lasso","b048e48d":"### After dropping top 5 important features and then building the model again \n## top 5 features are\n\n    LowQualFinSF\n    1stFlrSF\n    Fireplaces\n    LotArea\n    Exterior2nd_Brk_Cmn","854a17e1":"# Final model : Lasso \n### Lasso regression would be a better option it would help in feature elimination and the model will be more robust.","e8b2a309":"### Visualizing the cleaned dataset","c1c8f347":"## Loading important libraries"}}