{"cell_type":{"92ceac2d":"code","b005b244":"code","6f593727":"code","514a3353":"code","ca18b292":"code","6a8562db":"code","c678ccf4":"code","1d67d5c9":"code","f9cd958c":"code","4bd5d841":"code","c7dc04b9":"code","9e8ff3aa":"code","9f5cb1a2":"code","b7a56c77":"code","1a29dc1c":"code","94214242":"code","c8e482c5":"code","8a357701":"code","3f818e57":"code","2fadf16e":"code","627833ed":"code","fd64aa9e":"code","f4c134b2":"code","20f59853":"code","86390d90":"code","4c8eb409":"code","c13c0c21":"code","f5fb14e8":"code","f93d057d":"code","3980439b":"code","f433fc0b":"code","bd915dd7":"code","86890417":"code","d62f2aa4":"code","d91bb497":"code","b0ec86d5":"markdown","415c9f85":"markdown","ff3d1917":"markdown","6f308229":"markdown","22ab0a05":"markdown","04070488":"markdown","8cbe1582":"markdown","cfd2ab92":"markdown","059d523e":"markdown","06ebe72f":"markdown","3d4f6458":"markdown","38994cb6":"markdown","b04d0af4":"markdown","166c150b":"markdown","4e77da62":"markdown","d7e1dc5a":"markdown","962236e2":"markdown","b42c5e34":"markdown","5375aa34":"markdown","9ff89be3":"markdown"},"source":{"92ceac2d":"import plotly.figure_factory as ff\nimport numpy as np\n\n\n#background = text\nbg_text = [['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n           ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n           ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n           ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n           ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n           ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e'],\n           ['t', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', 'n', 'd', 's', 'e', 'r', 'i', 'e', 's'],        \n        ['2', '0', '2', '1' , 't', 'a', 'b', 'u', 'l', 'a', 'r', 'p', 'l', 'a', 'y', 'g', 'r', 'o', 'u', '@', 'd', 'e', 's'],]\n          \n    \ntext_1 = text_2 = bg_text\n\nz = [[.0, .0, 0, 0, 0, 0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, .8, .8, .8, .0, .8, .8, .8, .0, .8, .8, .8, .0, .0, .0, .8, .8, .8, .0, .8, .0, .0],\n     [.0, .0, .8, .0, .8, .0, .8, .0, .0, .0, .0, .8, .0, .0, .0, .0, .0, .0, .8, .0, .8 ,.0, .0],\n     [.0, .0, .8, .0, .8, .0, .8, .0, .0, .0, .0, .8, .0, .8, .8, .0, .8, .8, .8, .0, .8, .0, .0],\n     [.0, .0, .8, .0, .8, .0, .8, .0, .0, .0, .0, .8, .0, .0, .0, .0, .8, .0, .0, .0, .8, .0, .0],\n     [.0, .0, .8, .8, .8, .0, .8, .8, .8, .0, .0, .8, .0, .0, .0, .0, .8, .8, .8, .0, .8, .0, .0],\n     [.0, .0, 0, 0, 0,.0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0 ],\n     [.0, .0, 0, 0, 0,.0, .5, .5, .5, .0,  .5, .5, .5, .0, .5, .5, .5, .0, .0, .0, .0, .0, .0],\n     [.0, .0,.0,.0,.0,.0, .0, .5, .0, .0,  .5, .0, .5, .0, .5, .0, .0, .0, .0, .0, .0, .0, .0],\n     [.0, .0, 0, 0, 0, 0, .0, .5, .0, .0,  .5, .5, .5, .0, .5, .5, .5, .0, .0, .0, .0, .0, .0],\n     [.0, .0, 0,.0,.0,.0, .0, .5, .0, .0,  .5, .0, .0, .0, .0, .0, .5, .0, .0, .0, .0, .0, .0],\n     [.0, .0, 0, 0, 0, 0, .0, .5, .0,  0,  .5, .0, .0, .0, .5, .5, .5, .0, .0, .0, .0, .0, .0],\n     [.0, .0, 0, 0, 0, 0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0, .0 ],\n     ]\n     \n    \n# Display something on hover\nhover=[]\nfor x in range(len(bg_text)):\n    hover.append([i + '<br>' + 'TPS: October 2021' + str(j)\n                      for i, j in zip(text_1[x], text_2[x])])\n\n# Invert Matrices\nbg_text = bg_text[::-1]\nhover =hover[::-1]\nz = z[::-1]\n\n# # Set Colorscale\n# colorscale=[[0.0, '#d4d4d4'], [.2, '#996515'],\n#             [.4, '#D4AF37'], [.6, 'gold'],\n#             [.8, 'seagreen'],[1, '#996515']]\n\n\n# Make Annotated Heatmap\nfig = ff.create_annotated_heatmap(z, annotation_text=bg_text, text=hover,\n                                 colorscale='Greens', font_colors=['white'], hoverinfo='text')\nfig.update_layout(width=750,\n                  height=450,\n                 )                \n\nfig.show()","b005b244":"import os\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f593727":"train = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv', index_col='id', nrows=1000000)\ntest = pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv', index_col='id', nrows=500000)\nsubmission= pd.read_csv(r'\/kaggle\/input\/tabular-playground-series-oct-2021\/sample_submission.csv', index_col='id', nrows=500000)","514a3353":"print(train.shape)\nprint(test.shape)","ca18b292":"train.head()","6a8562db":"test.head()","c678ccf4":"train.info()","1d67d5c9":"test.info()","f9cd958c":"display(train.isna().sum().sum())\ndisplay(test.isna().sum().sum())","4bd5d841":"train.describe().T","c7dc04b9":"target = train['target']","9e8ff3aa":"pal = ['#6495ED','#ff355d']\nplt.figure(figsize=(8, 6))\nax = sns.countplot(x=target, palette=pal)\nax.set_title('Target variable distribution', fontsize=20, y=1.05)\n\nsns.despine(right=True)\nsns.despine(offset=10, trim=True)\n","9f5cb1a2":"cat_features =[]\nnum_features =[]\n\nfor col in train.columns:\n    if train[col].dtype=='float64':\n        num_features.append(col)\n    elif col != 'target':\n        cat_features.append(col)\n#print('Catagoric features: ', cat_features)\ndisplay(len(cat_features))\n#print('Numerical features: ', num_features)\ndisplay(len(num_features))","b7a56c77":"train_ = train.sample(10000, random_state=2021)\ntest_ = test.sample(5000, random_state=2021)","1a29dc1c":"def count_plot_testTrain(data1, data2, features, titleText):\n    L = len(features)\n    nrow= int(np.ceil(L\/9))\n    ncol= 9\n\n    fig, ax = plt.subplots(nrow, ncol,figsize=(22, 12), sharey=True, facecolor='#dddddd')\n    ax.flatten()\n    fig.subplots_adjust(top=0.92)\n    i = 1\n    for feature in features:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, color='#6495ED', data=data1, label='train')\n        ax = sns.countplot(x=feature, color='#ff355d', data=data2, label='test')\n        ax.set_xlabel(feature)\n        ax.set_ylabel('')\n        ax.set_yticks([]) \n        ax.xaxis.set_label_position('top') \n        \n        i += 1\n        \n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper right',borderaxespad= 3.0) \n    plt.suptitle(titleText ,fontsize = 20)\n    plt.show()","94214242":"count_plot_testTrain(train_, test_, cat_features, titleText='Train & test data categorical features count plots ')","c8e482c5":"def count_plot(data, features, titleText, hue=None):\n    \n    L = len(features)\n    nrow= int(np.ceil(L\/9))\n    ncol= 9\n    \n    fig, ax = plt.subplots(nrow, ncol,figsize=(22, 12), \n                           sharey=True, facecolor='#dddddd')\n    fig.subplots_adjust(top=0.92)\n    i = 1\n    for feature in features:\n        total = float(len(data)) \n        plt.subplot(nrow, ncol, i)\n        ax = sns.countplot(x=feature, palette=pal, data=data, hue=hue)\n        ax.set_xlabel(feature)\n        ax.set_ylabel('')\n        ax.xaxis.set_label_position('top')\n        ax.set_yticks([]) \n        ax.get_legend().remove()\n        i += 1\n        \n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper right',borderaxespad= 3.0) \n    \n    plt.suptitle(titleText ,fontsize = 20)\n    plt.show()    ","8a357701":"count_plot(train, cat_features, 'Train data cat_feats: target distribution (count plot)', hue=target)","3f818e57":"def density_plotter(a, b, title):    \n    L = len(num_features[a:b])\n    nrow= int(np.ceil(L\/10))\n    ncol= 10\n    fig, ax = plt.subplots(nrow, ncol,figsize=(24, 12), sharey=False, facecolor='#dddddd')\n\n    fig.subplots_adjust(top=0.90)\n    i = 1\n    for feature in num_features[a:b]:\n        plt.subplot(nrow, ncol, i)\n        ax = sns.kdeplot(train_[feature], shade=True,  color='#6495ED',  alpha=0.85, label='train')\n        ax = sns.kdeplot(test_[feature], shade=True, color='#ff355d',  alpha=0.85, label='test')\n        ax.yaxis.set_major_formatter(FormatStrFormatter('%.0f'))\n        ax.xaxis.set_label_position('top')\n        ax.set_ylabel('')\n        ax.set_yticks([])        \n        ax.set_xticks([])\n        i += 1\n\n    lines, labels = fig.axes[-1].get_legend_handles_labels()    \n    fig.legend(lines, labels, loc = 'upper center',borderaxespad= 4.0) \n\n    plt.suptitle(title, fontsize=20)\n    plt.show()","2fadf16e":"density_plotter(a=0, b=60, title='Density plot of numerical features: train & test data (first 60 feats)')","627833ed":"density_plotter(a=60, b=120, title='Density plot of numerical features: train & test data (second 60 feats)')","fd64aa9e":"density_plotter(a=120, b=180, title='Density plot of numerical features: train & test data (third 60 feats)')","f4c134b2":"density_plotter(a=180, b=len(train.columns), title='Density plot of numerical features: train & test data (last 60 feats)')","20f59853":"df_num = pd.concat([train[num_features], train['target']], axis=1)","86390d90":"fig, ax = plt.subplots(1, 1, figsize=(16 , 16))\ncorr = train.sample(10000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1, vmax=0.1, vmin=-0.1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .85}, mask=mask ) \n\nax.set_title('Correlation heatmap: Numerical features', fontsize=24, y= 1.05);","4c8eb409":"df_cat = pd.concat([train[cat_features], train['target']], axis=1)\nfig, ax = plt.subplots(1, 1, figsize=(16 , 16))\ncorr = df_cat.sample(10000, random_state=2021).corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax, square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),vmax=0.5, vmin=-0.5,\n        cbar_kws={\"shrink\": .85}, mask=mask )\nax.set_title('Correlation heatmap: Categorical features', fontsize=24, y= 1.05)\nplt.show()","c13c0c21":"from sklearn.decomposition import PCA\nimport plotly.express as px\n\ndf = train.sample(10000, random_state=2021)\npca = PCA(n_components=2)\ncomponents = pca.fit_transform(df)\n\nfig = px.scatter(components, x=0, y=1,color=df['target'])\nfig.update_layout(title_text='PCA (2-component): All features ', \n                  paper_bgcolor='rgb(230, 230, 230)',\n                  plot_bgcolor='rgb(230, 230, 230)',\n                  xaxis = {'showgrid': False},\n                  yaxis = {'showgrid': False},\n                  width=550, height=400,\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'}, \n                  )\nfig.layout.coloraxis.colorbar.title = 'target'\nfig.show()","f5fb14e8":"pca = PCA(n_components=2)\ncomponents = pca.fit_transform(df_num.sample(10000, random_state=2021))\n\nfig = px.scatter(components, x=0, y=1,color=df['target'])\nfig.update_layout(title_text='PCA (2-component): Numerical features', \n                  paper_bgcolor='rgb(230, 230, 230)',\n                  plot_bgcolor='rgb(230, 230, 230)',\n                  xaxis = {'showgrid': False},\n                  yaxis = {'showgrid': False},\n                  width=550, height=400,\n                  titlefont={'color':'black', 'size': 24, 'family': 'San-Serif'}, \n                  )\nfig.layout.coloraxis.colorbar.title = 'target'\nfig.show()","f93d057d":"pca = PCA(n_components=50, random_state=2021)\npca.fit_transform(df)\nprint('Explained variance: %.4f' % pca.explained_variance_ratio_.sum())","3980439b":"n_comp = np.arange(25, 286, 25)\nexp_variance =[]\n\nfor n in n_comp:\n    pca = PCA(n, random_state=2021)\n    pca.fit_transform(df)\n    exp_variance.append(pca.explained_variance_ratio_.sum())\n\nzip(n_comp, exp_variance)\n\nplt.figure(figsize=(8, 6))\nplt.scatter(n_comp, exp_variance)\nplt.xlabel('n_compenents')\nplt.ylabel('% explainde variance')\nplt.grid()\nplt.title('Explained variance by n-features', fontsize=20, y=1.05);","f433fc0b":"import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import cross_validate\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndata_dir = Path('..\/input\/tabular-playground-series-oct-2021\/')\n\ndf_train = pd.read_csv(\n    data_dir \/ \"train.csv\",\n    index_col='id',\n    nrows=25000, \n)\n\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')\n\nFEATURES = df_train.columns[:-1]\nTARGET = df_train.columns[-1]\n\nX = df_train.loc[:, FEATURES]\ny = df_train.loc[:, TARGET]\n\nseed = 0\nfold = 5","bd915dd7":"model_xgb = XGBClassifier(max_depth=3,\n    subsample=.85,\n    colsample_bytree=.1,\n    n_jobs=-1,\n    tree_method='gpu_hist',\n    sampling_method='gradient_based', \n    random_state= seed,\n)\ndef score(X, y, model_xgb, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_xgb, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_xgb, cv=fold)\ndisplay(scores)","86890417":"model_lgbm = LGBMClassifier(\n    num_iterations=100,\n    objective = \"binary\",\n    feature_pre_filter = False,  \n    device_type = 'gpu',\n    )\ndef score(X, y, model_lgbm, cv):\n    scoring = [\"roc_auc\"]\n    scores = cross_validate(\n        model_lgbm, X, y, scoring=scoring, cv=cv, return_train_score=True\n    )\n    scores = pd.DataFrame(scores).T\n    return scores.assign(\n        mean = lambda x: x.mean(axis=1),\n        std = lambda x: x.std(axis=1),\n    )\n\nscores = score(X, y, model_lgbm, cv=fold)\ndisplay(scores)","d62f2aa4":"model_lgbm.fit(X, y, eval_metric='auc')\nX_test = pd.read_csv(data_dir \/ \"test.csv\", index_col='id')\n\ny_pred_lgbm = pd.Series(\n    model_lgbm.predict_proba(X_test)[:, 1],\n    index=X_test.index,\n    name=TARGET,\n)\ny_pred_lgbm.to_csv(\"submission_xgb.csv\")","d91bb497":"y_pred_lgbm","b0ec86d5":"#### The second 60 features","415c9f85":"<a id=\"top\"><\/a>    \n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n\n* [Introduction](#1)\n* [EDA](#2)\n    * [Data Overview](#2.1)\n    * [Correlation heatmap](#2.2)\n    * [Dimension reduction using PCA](#2.3)\n    * [EDA summary](#2.4)\n* [Modeling](#3)\n    * [xgboost](#3.1)\n    * [lgbm](#3.2)\n    * [Submission](#3.3)\n\n\n<a id=\"1\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>1. Introduction<\/b><\/font>\n\nStarting from January this year, the kaggle competition team is offering a month-long tabulary playground competitions. This series aims to bridge between inclass competition and featured competitions with a friendly and approachable datasets.\n\nFor this competition, you will be predicting a binary target based on a number of feature columns given in the data. The columns are a mix of scaled continuous features and binary features.\n\nThe data is synthetically generated by a GAN that was trained on **real-world molecular response data**.\n\nFiles to work with:\n\n* train.csv - the training data with the target column\n* test.csv - the test set; you will be predicting the target for each row in this file (the probability of the binary target)\n* sample_submission.csv - a sample submission file in the correct format\n\n**Evaluation**: \n\nSubmissions are evaluated on area under the **ROC curve** between the predicted probability and the observed target.","ff3d1917":"<a id=\"2.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.2 Correlation heatmap<\/b><\/font>","6f308229":"### Categorical features","22ab0a05":"### Target\n> Target distribution is balanced.","04070488":"<a id=\"3\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>3. Modeling<\/b><\/font>\n\n* Using the start code from last month by [kaggle competition team](https:\/\/www.kaggle.com\/ryanholbrook\/getting-started-september-2021-tabular-playground)\n* 5-fold average","8cbe1582":"#### The third 60 features","cfd2ab92":"\n<a id=\"2.3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.3  Dimension reduction using PCA<\/b><\/font>\n\n","059d523e":"<a id=\"3.2\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>3.2 LGBM<\/b><\/font>\n","06ebe72f":"<a id=\"2\"><\/a>\n<font color=\"lightseagreen\" size=+2.5><b>2. EDA<\/b><\/font>\n### Set-up","3d4f6458":"### Explained variance \n- Reducing the features space to 50 components could 'only' explain 79% of the variance. \n- 100 features could explain around 91% of the total variance.","38994cb6":"### Features\n* There are 45  categorical features. All binary.\n* And 240 numerical features","b04d0af4":"#### The last 60 features","166c150b":"#### Numerical features\n#### The first 60 features","4e77da62":"### In progress...","d7e1dc5a":"### Target distribution (categorical features)\n* Note f22! The only categorical feature that seem to show some correlattion with the taget variable. (This was also highlighted in the discussion by Mikolaj and Craig Thomas)","962236e2":"<a id=\"3.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>3.1 XGBoost<\/b><\/font>\n","b42c5e34":"<a id=\"3.3\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>3.3 Submission<\/b><\/font>\n","5375aa34":"<a id=\"2.4\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.4 EDA summary<\/b><\/font>\n\n- There are no missing values in both train and test dataset.\n- The distribution of target variable is balanced, almost 50%-50%.\n- The distribution of features in both train and test dataset is similar.\n- Correlation of most features with target variables is week. Only f22 (cat_feature) is visible on the heatmap with correlation coeficient of around |0.5|.\n","9ff89be3":"<a id=\"2.1\"><\/a>\n<font color=\"lightseagreen\" size=+1.5><b>2.1 Data overview<\/b><\/font>\n\n#### Data size\n- Train data has 1000000 rows and 286 features including the target variable\n- Test dataset has 500000 rows and 285 features.\n\n#### Missing Values\n- No missing values in both train and test datasets!\n\n#### Features\n- There are 45  categorical features. All binary.\n- And 240 numerical features\n\n#### Target\n- Binary target (1, 0)\n- Target distribution is balanced."}}