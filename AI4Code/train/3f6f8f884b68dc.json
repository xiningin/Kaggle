{"cell_type":{"f5efad83":"code","3ef874e4":"code","85331e27":"code","26a15a0d":"code","f855adfc":"code","ca691cec":"code","c5703dcd":"code","e03fc505":"code","9e8d27d8":"code","0ac77e83":"code","1824f0db":"code","d854b267":"code","9c9ad2e0":"code","9c7a0a1d":"code","c1ace53c":"code","75534e9e":"code","fe0ac646":"code","b2e5f22b":"code","611dae96":"code","ed5da1ee":"code","54f82206":"markdown","60a55a06":"markdown","1c475eb7":"markdown","5960939d":"markdown","cf8e905a":"markdown","9e276a41":"markdown","954f335d":"markdown","556b9b31":"markdown","7f2cadc8":"markdown","f905481d":"markdown","e447e313":"markdown"},"source":{"f5efad83":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\npd.reset_option('^display.', silent=True)\n\n# Load data\nX_train = pd.read_csv('\/kaggle\/input\/employee-attrition\/employee_attrition_train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/employee-attrition\/employee_attrition_test.csv')\n\n# Make target numerical\nX_train.Attrition = X_train.Attrition.apply(lambda x: 0 if x == 'No' else 1)\n\n# Split target and predictors\ny_train = X_train['Attrition']\nnum_train = len(X_train)\nX_train.drop(['Attrition'], axis=1, inplace=True)\n\ndf = pd.concat([X_train, X_test], ignore_index=True)","3ef874e4":"df.info()","85331e27":"df.describe()","26a15a0d":"# Detect if data is imbalanced\nprint(y_train.value_counts())","f855adfc":"pd.set_option('mode.chained_assignment', None)\n\n# Fill missing values for DailyRate with median\ndaily_rates = df.groupby(['Gender', 'Education', 'JobLevel']).DailyRate\nf = lambda x: x.fillna(x.median())\ndf.DailyRate = daily_rates.transform(f)\n\n# Fill missing values for age with median\nages = df.groupby(['Gender', 'Education']).Age\nf = lambda x: x.fillna(x.median())\ndf.Age = ages.transform(f)\n\n# Set missing values for travel to Non-Travel\ndf.BusinessTravel[df.BusinessTravel.isnull()] = 'Non-Travel'\n\n# Set missing values for DistanceFromHome to median\ndf.DistanceFromHome[df.DistanceFromHome.isnull()] = np.around(df.DistanceFromHome.mean())\n\n# Set missing values for MaritalStatus to Married\ndf.MaritalStatus[df.MaritalStatus.isnull()] = 'Married'\n\n# Save indices of categorial features\ncategorical_features_indices = np.where(df.dtypes == 'object')[0]","ca691cec":"# Split the df into train and test set\nX_train = df.iloc[:num_train,:]\nX_test = df.iloc[num_train:,:]\n\n# Make a training and validation set\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.75, stratify=y_train, random_state=0)\n\n# Imbalanced data, so set weight\npos_weight = sum(y_train.values == 0)\/sum(y_train.values == 1)","c5703dcd":"import catboost\nparams = {\"iterations\": 1000,\n          \"learning_rate\": 0.1,\n          \"scale_pos_weight\": pos_weight,\n          \"eval_metric\": \"AUC\",\n          \"custom_loss\": \"Accuracy\",\n          \"loss_function\": \"Logloss\",\n          \"od_type\": \"Iter\",\n          \"od_wait\": 30,\n          \"logging_level\": \"Verbose\",\n          \"random_seed\": 0\n}\n\ntrain_pool = catboost.Pool(X_train, y_train, cat_features=categorical_features_indices)\nvalid_pool = catboost.Pool(X_valid, y_valid, cat_features=categorical_features_indices)\n\nmodel = catboost.CatBoostClassifier(**params)\nmodel.fit(train_pool, eval_set=valid_pool, plot=False)","e03fc505":"from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\nfrom sklearn.metrics import precision_score, recall_score\nfrom catboost.utils import get_roc_curve, select_threshold\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n\ny_pred = model.predict(X_valid)\nprint(f\"Confusion Matrix:\\n {confusion_matrix(y_valid, y_pred)}\\n\")\nprint(f\"Classification Report:\\n {classification_report(y_valid, y_pred)}\\n\")\n\ny_pred = model.predict(X_valid)\nprint(f\"Accuracy on validation set: {accuracy_score(y_valid, y_pred)}\")\nprint(f\"Precision on validation set: {precision_score(y_valid, y_pred)}\")\nprint(f\"Recall on validation set: {recall_score(y_valid, y_pred)}\")\n\nfpr_train, tpr_train, _ = get_roc_curve(model, train_pool)\nfpr_valid, tpr_valid, _ = get_roc_curve(model, valid_pool)\n\nplt.figure(figsize=(8,6))\nplot_roc_curve(fpr_train, tpr_train, \"Training ROC\")\nplot_roc_curve(fpr_valid, tpr_valid, \"Validation ROC\")\nplt.legend(loc=\"lower right\")\nplt.title(\"ROC plot\")\nplt.ylabel(\"TPR\")\nplt.xlabel(\"FPR\")\nplt.show()","9e8d27d8":"# Get feature importances\nmodel.get_feature_importance(train_pool, fstr_type=catboost.EFstrType.FeatureImportance, prettified=True)","0ac77e83":"# Plot feature importances\nimportances = model.get_feature_importance(train_pool, fstr_type=catboost.EFstrType.FeatureImportance)\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12,12))\nplt.title('Feature importance for CatBoost classifier')\nplt.barh(X_train.columns[indices][::-1], importances[indices][::-1])","1824f0db":"interactions = model.get_feature_importance(train_pool, fstr_type=catboost.EFstrType.Interaction)\nfeature_interaction = [[X_train.columns[interaction[0]], X_train.columns[interaction[1]], interaction[2]] for interaction in interactions]\nfeature_interaction_df = pd.DataFrame(feature_interaction, columns=['feature1', 'feature2', 'interaction_strength'])\nfeature_interaction_df.head(10)","d854b267":"pd.Series(index=zip(feature_interaction_df['feature1'], feature_interaction_df['feature2']), data=feature_interaction_df['interaction_strength'].values, name='interaction_strength').head(10)[::-1].plot(kind='barh', figsize=(12,12))","9c9ad2e0":"import shap\nshap_values = model.get_feature_importance(train_pool, fstr_type=catboost.EFstrType.ShapValues)\nshap.initjs()\nshap.summary_plot(shap_values[:, :-1], X_train, feature_names=X_train.columns.tolist())","9c7a0a1d":"shap.summary_plot(shap_values[:, :-1], X_train, feature_names=X_train.columns.tolist(), plot_type=\"bar\")","c1ace53c":"# Helper function to plot shap values\ndef shap_plot(j):\n    explainerModel = shap.TreeExplainer(model)\n    shap_values_Model = explainerModel.shap_values(X_test)\n    p = shap.force_plot(explainerModel.expected_value, shap_values_Model[j], X_test.iloc[[j]])\n    return(p)\n\nshap_plot(0)","75534e9e":"shap_plot(10)","fe0ac646":"shap_plot(45)","b2e5f22b":"shap_plot(49)","611dae96":"shap_plot(50)","ed5da1ee":"y_test_preds = model.predict(X_test)\ny_test_probas = model.predict_proba(X_test)\n\nprint(f\"First 20 predictions on test set: {y_test_preds[:10]}\")\nprint(f\"First 20 dropout probabilities: {y_test_probas[:10]}\")\nprint(f\"Number of predicated dropouts: {np.sum(y_test_preds == 1)}\")\nprint(f\"Number of predicated non-dropouts: {np.sum(y_test_preds == 0)}\")","54f82206":"![](https:\/\/harver.com\/wp-content\/uploads\/2019\/02\/Employee-Attrition-Turnover-1024x437.jpg)\n\n# Introduction\n\nThe key to success in any organization is attracting and retaining top talent. As an HR analyst one of the key task is to determine which factors keep employees at the company and which prompt others to leave. Given in the data is a set of data points on the employees who are either currently working within the company or have resigned. The objective is to identify and improve these factors to prevent loss of good people.\n\nThis notebook predicts whether or not employee's resign their position at work.\n\nKey takeaways:\n\n* Uses CatBoosts, so no manual encoding of categorial values.\n* Find the best features (which is partly how much overtime work you put on).\n* Plots SHAP values for a few predictions to show what speaks for and what speaks against resignation.\n","60a55a06":"# Short EDA\n\nThe data contains 1029 training 441 test samples with information about an employee's current position, their background and whether or not they have resigned their position at any point. There are a few NaN values we'll deal with next, but overall no really surprises in the data. We note howver that the dataset is imbalanced, as apparently a lot more people choose to keep their jobs rather than resigning.\n\n","1c475eb7":"# Feature importance\n\nOne of the cool things about using decision tree algorithms for machine-learning is that you get a feature ranking for free when you're done training. So what we'll do next is extract the features ordered by importance from the model and show them in a nice table, and plot them as well.","5960939d":"# Train\/test split\n\nNow that we're done encoding, we can make a proper train and validation split and save the test set for later evaluation. The data is imbalanced, so we record a weight for the positive 1's used next for classification. This is simply the ratio between negative and positive values.","cf8e905a":"# Feature interactions\n\nFeature interactions show the interaction strength for each pair of features that are used in the model. Formally, it reflects the sum of absolute differences (after being summed across all trees) between the leaves of the tree containing the interaction and the leaves of the tree not containing the interaction. In other words, how correlated two features are. We see that there is a strong link between Department and JobRole in this case. Also the size of your paycheck seems to depend on your seniority, not surprisingly.","9e276a41":"# SHAP Values\n\nSHAP values help explain how our model makes predictions. Below we've used the training data to identify which features makes you more likely to stay in your job role and which features makes you likely to resign. The features are on the left vertical axis ranked in descending order and the SHAP value strengths are on the horizontal axis. The horizontal location of the dots shows whether the effect of that value is associated with a higher or lower prediction and the color shows whether that variable is high (in red) or low (in blue) for that observation.\n\nFor example, a *low* value of **StockOptionLevel** has a *positive* impact on the predictions, whereas a *high* value of *YearsInCurrentRole* has a *negative* impact on the predictions. A positive impact pulls us towards classifying a employee as a 1 (they will likely resign) and a negative impact pulls us towards classifying the employee's as a 0 (they will likely be staying). Categorical values are shown in gray. \n\nSee more here: https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d\n\nThe next plots show the SHAP values for the training data and a few examples on the test set as well.","954f335d":"# Data loading\n\nWe'll do some simple data loading of the train and test set, encode the targets as numerical values since it's more simplier to work with and put all the data in the dataframe at the end.","556b9b31":"# Feature encoding\n\nHere we just fill the NaN values with some good medians based on employee's gender, education and job level. Since we're using CatBoost, we simply save the indices of the categorial features instead of doing any manual encoding of them. CatBoost does this by itself and very effectively too, so better let it have it.","7f2cadc8":"# Model predictions\n\nLastly we'll make a few predictions on unseen test data. It turns out, that most employee's are happy with their current role and most likely to stay, however 2 out of 10 do want to resign their position.","f905481d":"After 20 iterations, CatBoost got a 83% accuracy on the validation set, which didn't take more than a quarter of a second. Next we'll make a classification report, show precision\/recall scores and plot the ROC curves.","e447e313":"# Modelling\n\nCatBoost is an algorithm for gradient boosting on decision trees. It is developed by Yandex researchers and engineers, and is used for search, recommendation systems, personal assistant, self-driving cars, weather prediction and many other tasks at Yandex and in other companies, including CERN, Cloudflare, Careem taxi. It is in open-source and can be used by anyone. More here: https:\/\/catboost.ai\/\n\nIt has categorical features support and usually gives excellent results without hyperparameter optimization, so we can pretty much train right away. We set a high number of iterations, in order to train as long as we keep seeing improvements, and so that early stopping (Overfit detector in CatBoost lingo) will kick in should it start to overfit. We optimize for Logloss as well, not surprisingly.\n\nKaggle does not support this, but if you run this yourself, make sure to enable *plot* so you'll get nice plots :)"}}