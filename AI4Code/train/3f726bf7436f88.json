{"cell_type":{"75f0b921":"code","d628733a":"code","90bb5335":"code","3d96089d":"code","05cdc434":"code","c1872ab1":"code","83ae3ea7":"code","29fcc72b":"code","8edb3eca":"code","9a71ade2":"code","b48db3b1":"code","2258057f":"code","0be31e29":"code","16f1f2ee":"code","e669e44e":"code","2e47e74e":"code","f7da96ff":"code","294314f2":"code","3e0f38b1":"code","5d0fc147":"code","62e3a1c2":"code","6032cbc7":"code","5885c102":"code","f13a8d4d":"code","10ac7258":"code","77573761":"code","f21c7ce5":"code","dafaa056":"code","13f725ee":"code","d2a1b02c":"code","f548fea2":"code","9616c47c":"code","a987c83f":"code","e71ece63":"code","e4668b9e":"code","dc360a64":"code","d95b8c17":"code","14df11b8":"code","d2636a7f":"markdown","1bd2cdd1":"markdown","e169f7d3":"markdown","0414b06f":"markdown","655f9e84":"markdown","b61ecb7c":"markdown","69c6e0fe":"markdown","f7e3826b":"markdown","69de4602":"markdown","570e97b7":"markdown","2a321699":"markdown","8422214a":"markdown","c7502dd5":"markdown","84e8b8e1":"markdown","5ef552d9":"markdown","3fa4a21d":"markdown","93811b1f":"markdown","01faa6fd":"markdown","32b4cc00":"markdown","13874449":"markdown","5a1ac4de":"markdown","4f7c9604":"markdown","7729c48d":"markdown"},"source":{"75f0b921":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d628733a":"df_gender = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ndf=pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')","90bb5335":"df.head()","3d96089d":"df.info()","05cdc434":"df.describe()","c1872ab1":"df.var()","83ae3ea7":"df.corr()","29fcc72b":"df.shape","8edb3eca":"df.isna().sum()","9a71ade2":"import matplotlib.pyplot as plt\nimport seaborn as sns\ncolors = ['#DB1C18','#DBDB3B','#51A2DB']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})","b48db3b1":"df['Cabin'].value_counts()","2258057f":"#lets take the cabin bype from Cabin feature\ndf['Cabin']=df['Cabin'].replace('(\\d)','',regex=True)\ndf['Cabin']=df['Cabin'].str[0]\n","0be31e29":"df['Cabin']","16f1f2ee":"l=len(df)\nfig, ax = plt.subplots(nrows=2,ncols=5, figsize=(15,6), constrained_layout=True)\nsns.despine(top=True, right=True, left=True, bottom=True, offset=None, trim=False)\nax=ax.flatten()\n\n#PassengerId\nsns.histplot(df['PassengerId'], ax=ax[0])\n\n#Survived\nsns.countplot(x=df['Survived'], ax=ax[1])\nfor i,j in enumerate(ax[1].patches):\n    ax[1].text(x=i, y=j.get_height()\/2, s=f'{np.round(j.get_height()\/l,3)*100} %')\n\n#PClass\nsns.countplot(x=df['Pclass'],ax=ax[2])\nfor i,j in enumerate(ax[2].patches):\n    ax[2].text(x=i, y=j.get_height(), s=f'{np.round(j.get_height()\/l,3)*100} %')\n    \n#Sex\nsns.countplot(x=df['Sex'], ax=ax[3])\nfor i,j in enumerate(ax[3].patches):\n    ax[3].text(x=i, y=j.get_height(), s=f'{np.round(j.get_height()\/l,2)*100} %')\n\n#Age\nsns.histplot(df['Age'],bins=10, ax=ax[4],log_scale=10)\n\n#Sibsb\nsns.countplot(x=df['SibSp'], ax=ax[5])\nfor i,j in enumerate(ax[5].patches):\n    ax[5].text(x=i, y=j.get_height(), s=j.get_height())\n\n#Sibsb\nsns.countplot(x=df['Parch'], ax=ax[6])\nfor i,j in enumerate(ax[6].patches):\n    ax[6].text(x=i, y=j.get_height(), s=j.get_height())\n\n#Fare\nsns.histplot(df['Fare'],bins=50, ax=ax[7])\n\n#Embarked\nsns.countplot(x=df['Embarked'], ax=ax[8])\nfor i,j in enumerate(ax[8].patches):\n    ax[8].text(x=i, y=j.get_height(), s=j.get_height())\n\n#Embarked\nsns.countplot(x=df['Cabin'], ax=ax[9])\nfor i,j in enumerate(ax[9].patches):\n    ax[9].text(x=i, y=j.get_height(), s=j.get_height())","e669e44e":"#drop PassengerId and Name feature as it is unique values\ndf=df.drop(['PassengerId','Name'], axis=1)","2e47e74e":"print(\"Percentage of missng values\")\ndf.isna().sum().sort_values(ascending=False)\/len(df)","f7da96ff":"from scipy.stats import chi2_contingency, chisquare\ndf1 = df[~df['Cabin'].isnull()]\nctab=pd.crosstab(index=df1['Survived'], columns=df1['Cabin'])\nprint(ctab)\nchi, p_value, d_free, _ = chi2_contingency(ctab)\nif p_value <=0.05:\n    print(f\"P_value is : {p_value}, Reject the Null Hypothesis and there is significant differnce \")\nelse:\n    print(f\"P_value is : {p_value}, Accept the Null Hypothesis and there no is significant differnce in the cabin group and surivor \")","294314f2":"sns.boxplot(x=df['Pclass'], y=df['Age'])","3e0f38b1":"df.groupby(by=['Pclass']).mean()['Age']","5d0fc147":"#Fill Age based on PClass category and mean Age.\ndef age_fill(col):\n    age=col[0]\n    pclass=col[1]\n    if pd.isnull(age):\n        if pclass ==1:\n            return 38\n        elif pclass ==2 :\n            return 29\n        else:\n            return 25\n    else:\n        return age\n\n","62e3a1c2":"df['Age']=df[['Age','Pclass']].apply(age_fill, axis=1)","6032cbc7":"#Embarked feature has only 2 null value, we can drop it for modeling purpose\ndf.dropna(axis=0, subset=['Embarked'], inplace=True)","5885c102":"#We can drop the Tickets & Cabin column as well. \ndf.drop(['Ticket','Cabin'], axis=1, inplace=True)","f13a8d4d":"df.head()","10ac7258":"#Since our Target feature is Survived, let us do the bivariated analysis with Survived feature. \n\nfig, ax = plt.subplots(nrows=3,ncols=3, figsize=(15,6), constrained_layout=True)\nsns.despine(top=True, right=True, left=True, bottom=True, offset=None, trim=False)\nax=ax.flatten()\n\n\n#PClass\nsns.barplot(y=df['Survived'], x=df['Pclass'],ax=ax[0])\nfor i,j in enumerate(ax[0].patches):\n    ax[0].text(x=i, y=j.get_height(), s=np.round(j.get_height(),2))\n    \n#Sex\nsns.barplot(y=df['Survived'], x=df['Sex'],ax=ax[1])\nfor i,j in enumerate(ax[1].patches):\n    ax[1].text(x=i, y=j.get_height(), s=np.round(j.get_height(),2))\n\n#Age\nsns.histplot(data=df, x='Age',bins=10, ax=ax[2], hue='Survived', kde=True, stat='count')\n\n#Sibsp\nsns.barplot(y=df['Survived'], x=df['SibSp'],ax=ax[3])\nfor i,j in enumerate(ax[3].patches):\n    ax[3].text(x=i, y=j.get_height(), s=np.round(j.get_height(),2))\n\n#Parch\nsns.barplot(y=df['Survived'], x=df['Parch'],ax=ax[4])\nfor i,j in enumerate(ax[4].patches):\n    ax[4].text(x=i, y=j.get_height(), s=np.round(j.get_height(),2))\n\n#Fare\nsns.histplot(data=df ,x='Fare',bins=50, ax=ax[5], hue='Survived')\n\n#Embarked\nsns.barplot(y=df['Survived'], x=df['Embarked'],ax=ax[7])\nfor i,j in enumerate(ax[7].patches):\n    ax[7].text(x=i, y=j.get_height(), s=np.round(j.get_height(),2))\n\nax[6].set_xticks([])\nax[6].set_yticks([])\n\nax[8].set_xticks([])\nax[8].set_yticks([])\n","77573761":"fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(15,6), constrained_layout=True)\nax=ax.flatten()\nsns.boxplot(data=df, y='Age', x='Sex', ax=ax[0])\nsns.boxplot(data=df, y='Age', x='Sex', ax=ax[1], hue='Survived')","f21c7ce5":"fig, ax = plt.subplots(nrows=1,ncols=2, figsize=(15,6), constrained_layout=True)\nax=ax.flatten()\nsns.boxplot(data=df, y='Fare', x='Sex', ax=ax[0])\nsns.boxplot(data=df, y='Fare', x='Sex', ax=ax[1], hue='Survived')","dafaa056":"fig, ax= plt.subplots(nrows=1, ncols=2, figsize=(15,6))\nax=ax.flatten()\nsns.heatmap(df[df['Survived']==1].corr(),annot=True, ax=ax[0])\nsns.heatmap(df[df['Survived']==0].corr(),annot=True, ax=ax[1])","13f725ee":"sns.pairplot(df, hue='Survived')","d2a1b02c":"df['Pclass']=df['Pclass'].astype('category')\ndf['Sex']=df['Sex'].astype('category')\ndf['SibSp']=df['SibSp'].astype('category')\ndf['Parch']=df['Parch'].astype('category')\ndf['Embarked']=df['Embarked'].astype('category')\n","f548fea2":"df.info()","9616c47c":"df_dummy=pd.get_dummies(df, columns=['Pclass','Sex','SibSp','Parch','Embarked'], drop_first=True)","a987c83f":"df_dummy.info()","e71ece63":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX=df_dummy.drop(['Survived'], axis=1)\ny=df_dummy['Survived']\nscale = StandardScaler()\n\n\nX_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=1)\n#X_train = scale.fit_transform(X_train)\n#X_test =scale.transform(X_test)","e4668b9e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom yellowbrick.classifier import ROCAUC\nmodel = LogisticRegression(solver='liblinear')\nmodel.fit(X_train, y_train)\nprint(f\"Accuracy Score from Training data: {model.score(X_train, y_train)}\")\npred=model.predict(X_test)\nprint(f\"Accuracy Score from Training data: {accuracy_score(y_test, pred)}\")\nvisual = ROCAUC(model, classes=['Survived', 'Not-Survived'])\nvisual.fit(X_train, y_train)\nvisual.score(X_test, y_test) \nvisual.show()\n\nprint(\"Classification Report \\n\")\nprint(classification_report(pred, y_test))\n\nprint(\"Confusion Matrix \\n\")\nsns.heatmap(confusion_matrix(pred, y_test), annot=True)\n\n","dc360a64":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom yellowbrick.classifier import ROCAUC\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\nprint(f\"Accuracy Score from Training data: {model.score(X_train, y_train)}\")\npred=model.predict(X_test)\nprint(f\"Accuracy Score from Training data: {accuracy_score(y_test, pred)}\")\nvisual = ROCAUC(model, classes=['Survived', 'Not-Survived'])\nvisual.fit(X_train, y_train)\nvisual.score(X_test, y_test) \nvisual.show()\n\nprint(\"Classification Report \\n\")\nprint(classification_report(pred, y_test))\n\nprint(\"Confusion Matrix \\n\")\nsns.heatmap(confusion_matrix(pred, y_test), annot=True)\n\nfeat=pd.DataFrame()\nfeat['columns']=df_dummy.drop(['Survived'], axis=1).columns\nfeat['score']=model.feature_importances_\nfeat=feat.sort_values(by=['score','columns'], ascending=False)\nfig=plt.figure(figsize=(18,6))\nsns.barplot(data=feat, x='columns', y='score')","d95b8c17":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom yellowbrick.classifier import ROCAUC\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\nprint(f\"Accuracy Score from Training data: {model.score(X_train, y_train)}\")\npred=model.predict(X_test)\nprint(f\"Accuracy Score from Training data: {accuracy_score(y_test, pred)}\")\nvisual = ROCAUC(model, classes=['Survived', 'Not-Survived'])\nvisual.fit(X_train, y_train)\nvisual.score(X_test, y_test) \nvisual.show()\n\nprint(\"Classification Report \\n\")\nprint(classification_report(pred, y_test))\n\nprint(\"Confusion Matrix \\n\")\nsns.heatmap(confusion_matrix(pred, y_test), annot=True)\n\nfeat=pd.DataFrame()\nfeat['columns']=df_dummy.drop(['Survived'], axis=1).columns\nfeat['score']=model.feature_importances_\nfeat=feat.sort_values(by=['score','columns'], ascending=False)\nfig=plt.figure(figsize=(18,6))\nsns.barplot(data=feat, x='columns', y='score')","14df11b8":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom yellowbrick.classifier import ROCAUC\nmodel = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\nprint(f\"Accuracy Score from Training data: {model.score(X_train, y_train)}\")\npred=model.predict(X_test)\nprint(f\"Accuracy Score from Training data: {accuracy_score(y_test, pred)}\")\nvisual = ROCAUC(model, classes=['Survived', 'Not-Survived'])\nvisual.fit(X_train, y_train)\nvisual.score(X_test, y_test) \nvisual.show()\n\nprint(\"Classification Report \\n\")\nprint(classification_report(pred, y_test))\n\nprint(\"Confusion Matrix \\n\")\nsns.heatmap(confusion_matrix(pred, y_test), annot=True)\n","d2636a7f":"**Feature details:**  \n**PassengerId** - As mentioned in the above analysis, PassengerId has no information for analysis. so, lets drop this feature.  \n**Survival** -Around 62% of passengers & Ship crew are not survived and only 38% of passenger are survived in the disaster.  \n**Pclass** - 24% of people travelled in 1st class, 20% people travelled in 2nd class & 55% people travelled in 3rd class.  \n**Sex** - Out of the Passengers travelled, 65% are male and 35% are Female.  \n**Age** - Age of the passenger travelled in the ship are ranging from 5 years to 70 year. and mostly between 20 to 50 years old.  \n**Embarked** - Most passenger are embarked from S = Southampton. and followed by C = Cherbourg and Q = Queenstown. \n","1bd2cdd1":"### Lets try to fix the missing values","e169f7d3":"Age - has 177 NA values.  \nCabin - has 687 NA values.  \nEmbarked - has 2 NA values","0414b06f":"### Ensemble Technique","655f9e84":"* most female passenger travelled in high fare compared to male passengers.\n* passengers survival rate is comparitively high based on fares. ","b61ecb7c":"Using Ensemble techinque give high accuracy in Training data, where as the accuracy score reduced in Test data. when we see the feature importance, we can see the Sex, Fare, Age, PClass, Embarck give high score. Let us try with Tree method. ","69c6e0fe":"**Cabin** has 77% missing values in the dataset, we can impute the missing values. but i dont think this will make sense by imputing 77% of values. so, it is better to drop the feature. only 27% of data has the cabin details. before dropping the feature, let us do Hypothesis testing to confirm the same.\n\n**Null Hypothesis** - Cabin has no significant difference in Survior count.  \n**Alternate Hypothesis** - Cabin has significatn difference in Surivor count.\n\ncritical value (Alpha)- 5% significance","f7e3826b":"***Highlevel note on correlation***  \nSince most feature are categorical values, the correlation doesn't make much sense on the outcome. however, on the highlevel we will see the analysis.  \n1. PassengerId - negative and less influence with Survived feature\n2. Pclass - though the relationship is negative, it has some consliderable co-relation values. \n3. Age - also can be considered for analysis as it has negative correlation. \n4. SibSp, Parch, Fare - also have some considerable influence with Survived feature.\n\n***for the categorical features, we need to use Hypothesis testing to find the relationship***","69de4602":"### Bivriated and Multivariated Analysis","570e97b7":"* 25-75% Quantile of the male and female passenger are more or less the same. it is betwen 20 to 40. and most passenger are between 0 to 60 years.  \n* when it comes of survival rate. Male surival age is between 0 to 60. same as not survived rate.\n* for female passengers, age for not survived and survived has few differences. ","2a321699":"### Survived","8422214a":"## Univariated Analysis","c7502dd5":"## Data Engineering","84e8b8e1":"### Basic Logistic regression","5ef552d9":"From basic modeling, we can understand that the model works well. we get Training data accuracy of about 80% and test data accuracy of 84%. which has already addressed the variance and bias in prediciton. ROC cure and other metrics looks good. classification report - both Precision and Recall gives us more or less similar score. ","3fa4a21d":"**Problem Statement**  \nProblem statement is to create a model to indentify whether the passenger is survived or not based on the given features. \n\nTarget Feature: Survived.","93811b1f":"**Feature details**  \nPassengerId - just like Serial No. which may not be required for modeling and analysis.  \nSurvived - Tells about the passenger is survived or not. categorical feature with 0 or 1 values.  \nPclass - Ticket class - Categorical feature whether the passenger travelled in 1st, 2nd or 3rd class.  \nName - Name of the Passenger, may not be needed for modeling.  \nSex - Gender of the passenger, categorical feature - male or female.  \nAge - Age in Years on the particular year when the incident happend. \nSibSp - Number of sibiling\/Spouse of the passenger on boarded in the ship.  \nParch - No of parents\/children on boarded on the ship.  \nTicket - Travelled ticket number - May not be required for modeling, as it is random number or sequeuntial number generatet for the tickets.  \nFare - Fare paid for the ticket.\nCabin - Cabin in which the passenger travelled. \nEmbarked - Port of Embarkation - C = Cherbourg, Q = Queenstown, S = Southampton.  \n\n***Here most features are categorical values, which means the decision making involved in the analysis rather than the linear relationshiop among the data. Lets us analyse further to understand more***\n\n","01faa6fd":"**From the above test, we conclude that we can drop the Cabin feature**","32b4cc00":"## Hypothesis Testing","13874449":"## Data Modeling","5a1ac4de":"**Analysis**  \n*below analysis are based on 95% confidence interval.*\n1. Passenger from class 1 have survived around 63%, class 2 has 47% and class 3 has 24% of survivors. which means that Pclass is highgly related for the survival of the passenger in Titanic.\n2. Female passenger have more sruvival rate of about 74% (male 19%).  \n3. Children below 0-10 have high survival rate. apart from this bin, rest other seems similar. further analysing with sex and pclass would give more insight.  \n4. Passengers onboarded from C = Cherbourg has high survival rate. why?","4f7c9604":"### Test train split","7729c48d":"## Statistical Analysis"}}