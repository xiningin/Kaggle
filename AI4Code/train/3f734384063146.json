{"cell_type":{"1cce37c5":"code","2f217915":"code","c1ea33c0":"code","61871c63":"code","e06d50c7":"code","799fbdfc":"code","ff5c43fe":"code","92aa15c5":"code","c13247f5":"code","d479d9b4":"code","5b2d1b29":"code","24b4ffeb":"code","274654b5":"code","87b1d012":"code","6fc5c459":"code","54c63278":"code","7a3eb04f":"markdown","628f10e9":"markdown","711ceec1":"markdown","563ecb74":"markdown"},"source":{"1cce37c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f217915":"# Setup plotting\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n# Set Matplotlib defaults\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('animation', html='html5')","c1ea33c0":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import GroupShuffleSplit\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/train.csv')\n\nX = train.copy().dropna()\ny = X.pop('target')\nX.drop(columns=['row_id'], axis=1, inplace=True)\n\ntargets = y.unique()\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(targets)\ny = le.transform(y)\n\nfeatures_num = X.columns\nfeatures_cat = []\n\npreprocessor = make_column_transformer(\n    (StandardScaler(), features_num),\n    (OneHotEncoder(), features_cat),\n)\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n\nX_train.head()","61871c63":"X_train = preprocessor.fit_transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\ninput_shape = [X_train.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","e06d50c7":"model = keras.Sequential([\n    layers.Dense(1, input_shape=input_shape),\n    layers.Dense(10, activation=\"softmax\"),\n])\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    verbose=0, # suppress output since we'll plot the curves\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[0:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","799fbdfc":"# Start the plot at epoch 10\nhistory_df.loc[10:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","ff5c43fe":"model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation=\"softmax\"),\n])\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","92aa15c5":"from tensorflow.keras import callbacks\n\n# define an early stopping callback\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","c13247f5":"model = keras.Sequential([\n    layers.Dense(128, activation='relu', input_shape=input_shape),\n    layers.Dense(64, activation='relu'),    \n    layers.Dense(10, activation=\"softmax\"),\n])\nmodel.compile(optimizer=\"rmsprop\",\n             loss=\"sparse_categorical_crossentropy\",\n             metrics=[\"accuracy\"])\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=512,\n    epochs=50,\n    callbacks=[early_stopping]\n)\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()));","d479d9b4":"test = pd.read_csv('..\/input\/tabular-playground-series-feb-2022\/test.csv')\n\nx_test = test.copy()\nx_test = x_test.drop(['row_id'], axis=1)\nx_test = preprocessor.fit_transform(x_test)","5b2d1b29":"predictions = model.predict(x_test)","24b4ffeb":"max_predictions = [np.argmax(predictions[i]) for i in range(len(predictions))]","274654b5":"bacteria = le.inverse_transform(max_predictions)","87b1d012":"sample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2022\/sample_submission.csv')","6fc5c459":"sample_submission['target'] = bacteria","54c63278":"sample_submission.to_csv('submission.csv', index=False)","7a3eb04f":"# 1) Evaluate Baseline","628f10e9":"# 2) Add Capacity","711ceec1":"# 4) Train and Interpret","563ecb74":"# 3) Define Early Stopping Callback"}}