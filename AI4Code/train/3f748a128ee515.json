{"cell_type":{"e3c650f7":"code","18a65502":"code","740ad8bf":"code","1b006eb9":"code","cdd28bb7":"code","ac4b95ec":"code","940c9c50":"code","ea645a0a":"code","8289bb5e":"code","67d57f11":"code","a0b24d84":"code","1a98d829":"code","bceb617f":"code","f76c44eb":"code","4bd657d4":"code","d6ca5d78":"code","409fde11":"code","9dd4ba4c":"code","1b9ab2e7":"code","8fb5ba5d":"code","dbe973e0":"code","7897b517":"code","f4e9c481":"code","022888a2":"code","c10c5245":"code","a4fc7cfb":"code","ba3a4ab5":"code","07d2c0f1":"code","f970fd5f":"code","82202afa":"code","49e4d63a":"code","4d53b4d4":"code","d858a5c4":"code","e27c69e4":"code","a81e6c35":"code","24669c41":"code","ebdae842":"code","9d3de58c":"code","985e31eb":"code","ef270423":"code","c751ca48":"code","e232ebc6":"code","3beb298e":"code","4d622580":"code","d32b4e2c":"code","7d09cb0d":"code","b56e4970":"code","c756023a":"code","708451cb":"markdown","f7ddfcb3":"markdown","4c404b8c":"markdown","5b009bb8":"markdown","40a27da7":"markdown"},"source":{"e3c650f7":"#importing reading & nummerical operation library i.e. pandas & numpy respectively\n\nimport pandas as pd\nimport numpy as np\n","18a65502":"train=pd.read_csv(\"..\/input\/deltax-data\/data.csv\")\ntest=pd.read_csv(\"..\/input\/deltax-data\/predict.csv\")\nss=pd.read_csv(\"..\/input\/deltax-data\/sample_submission.csv\")","740ad8bf":"#checking overall top 5 row from train file\n\ntrain.head()","1b006eb9":"#checking overall top 5 row from test file\n\ntest.head()","cdd28bb7":"#checking the submission structure\n\nss.head()","ac4b95ec":"#checking the data type of columns of train file\n#this is to check categorical & contionous value columns or any other like 'date-time' data type\n\ntrain.info()","940c9c50":"#checking the data type of columns of test file\n\ntest.info()","ea645a0a":"#checking number of columns present in train file\n\ntrain.columns","8289bb5e":"#checking number of columns present in test file\n\ntest.columns","67d57f11":"#checking the dimension of train file which has 4571 rows & 9 columns\n\ntrain.shape","a0b24d84":"#checking the dimension of test file which has 318 rows & 6 columns\n\ntest.shape","1a98d829":"#checking \"number of unique values\" & \"respective values\" present in each column mentioned i.e.['campaign','adgroup','ad']\n\nprint(train['campaign'].nunique())\nprint(\"\\n\")\nprint(train['campaign'].unique())\nprint(\"\\n\")\nprint(train['adgroup'].nunique())\nprint(\"\\n\")\nprint(train['adgroup'].unique())\nprint(\"\\n\")\nprint(train['ad'].nunique())\nprint(\"\\n\")\nprint(train['ad'].unique())\n\n","bceb617f":"#finding the overall features of column with continous values present in train file \n\ntrain.describe()","f76c44eb":"#finding the overall features of column with continous values present in test file\n\ntest.describe()","4bd657d4":"#checking the null values present in train file\n\ntrain.isnull().sum().sort_values(ascending=False)","d6ca5d78":"#checking the null values present in test file\n\ntest.isnull().sum().sort_values(ascending=False)","409fde11":"#checking the count of values of each category in 'adgroup' column \n\ntrain.adgroup.value_counts()","9dd4ba4c":"#checking the count of values of each category in 'adgroup' column \n\ntrain.ad.value_counts()","1b9ab2e7":"#converting date object feature to datetime feature for train file\n#this is done inorder extract date time features from date column\n\ntrain['date'] = pd.to_datetime(train['date'], errors='coerce')","8fb5ba5d":"#now we can see data type of date column has changed to datetime64 \n\ntrain.info()","dbe973e0":"#converting date object feature to datetime feature for test file\n#this is done inorder extract date time features from date column\n\ntest['date'] = pd.to_datetime(test['date'], errors='coerce')","7897b517":"#now we can see data type of date column has changed to datetime64\n\ntest.info()","f4e9c481":"#extrating week day feature & creating a new column called 'weekday' & checking the top 5 rows of train file\n\ntrain['weekday'] = train['date'].dt.dayofweek\ntrain.head()","022888a2":"#extrating week day feature & creating a new column called 'weekday' & checking the top 5 rows of test file\n\ntest['weekday'] = test['date'].dt.dayofweek\ntest.head()","c10c5245":"#creating a function to find outlier\ndef find_outliers(x):\n    q1=x.quantile(.25)\n    q3=x.quantile(.75)\n    iqr=q3-q1\n    floor=q1-1.5*iqr\n    ceiling=q3+1.5*iqr\n    outlier_indices=list(x.index[(x<floor)|(x>ceiling)])\n    outlier_values=list(x[outlier_indices])\n    return outlier_indices,outlier_values","a4fc7cfb":"#now we had to check outliers for continous columns i.e. 'impressions','clicks','conversions','revenue'\n\nprint(\"Outliers for impressions\")\nimpressions_indices, impressions_values=find_outliers(train['impressions'])\nprint(np.sort(impressions_values))\nprint(\"\\n\")\n\nprint(\"Outliers for clicks\")\nclicks_indices, clicks_values=find_outliers(train['clicks'])\nprint(np.sort(clicks_values))\nprint(\"\\n\")\n\nprint(\"Outliers for conversions\")\nconversions_indices, conversions_values=find_outliers(train['conversions'])\nprint(np.sort(conversions_values))\nprint(\"\\n\")\n\nprint(\"Outliers for revenue\")\nrevenue_indices, revenue_values=find_outliers(train['revenue'])\nprint(np.sort(revenue_values))","ba3a4ab5":"#ploted the outliers uning box plot method to see the distribution pattern of the values impressions in train file\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline  \n\nplt.boxplot(train.impressions)","07d2c0f1":"#ploted the outliers uning box plot method to see the distribution pattern of the values for'clicks' column\n\nplt.boxplot(train.clicks)","f970fd5f":"#ploted the outliers uning box plot method to see the distribution pattern of the values for'conversions' column\n\nplt.boxplot(train.conversions)","82202afa":"#ploted the outliers uning box plot method to see the distribution pattern of the values for 'revenue' column\n\nplt.boxplot(train.revenue)","49e4d63a":"#assigned categorical vaules to specific number for modeling for both train & test file\n\nfrom sklearn.preprocessing import LabelEncoder\ncolumns_to_Encode = ['adgroup','ad']\nle = LabelEncoder()\nfor each in columns_to_Encode:\n    train[each] = le.fit_transform(train[each])\nfor column in columns_to_Encode:\n    test[column] = le.fit_transform(test[column])","4d53b4d4":"#checking encoded categorical values from top 5 rows in train file \ntrain.head()","d858a5c4":"#checking encoded categorical values from top 5 rows in test file\ntest.head()","e27c69e4":"#storing the index values for test file refernce for submission file\ntest_index=test.index","a81e6c35":"#droping 'date' & 'campaign' column. Firstly, we already extracted possible feature from 'date' column. Secondly, 'campaign' column had all same so it was of no use of taking it as feature column from train file\n\ntrain=train.drop(['date','campaign'], axis = 1)","24669c41":"#checking top 5 row of train file\ntrain.head()","ebdae842":"#droping 'index','date' & 'campaign' column from test file.\n\ntest=test.drop(['index','date','campaign'], axis = 1)","9d3de58c":"#checking top 5 row of train file\ntest.head()","985e31eb":"#creating input variables in 'X' & output variables in Y(which has 4 different outputs to be determined)\n\nX = train.drop(['impressions','clicks','conversions','revenue'], axis=1)\nY_imp = np.log(train['impressions']+1)\nY_cli = np.log(train['clicks']+1)\nY_con = np.log(train['conversions']+1)\nY_rev = np.log(train['revenue']+1)","ef270423":"#combining all four outputs so that model could understand\n\nfour_Y = np.zeros((len(Y_rev),4))\nfour_Y[:,0] = Y_imp\nfour_Y[:,1] = Y_cli\nfour_Y[:,2] = Y_con\nfour_Y[:,3] = Y_rev","c751ca48":"from sklearn.multioutput import MultiOutputRegressor\nimport lightgbm as lgb\nfrom tqdm import tqdm","e232ebc6":"#overall model with parameters\ny_pred = 0\nN = 4 \nfor i in tqdm(range(N)):\n    model = MultiOutputRegressor(lgb.LGBMRegressor(random_state=i*101), n_jobs=-1)\n    model.fit(X, four_Y)\n    y_pred += model.predict(test)\ny_pred \/= N","3beb298e":"#creating the submission file\n\nsub=pd.DataFrame()\nsub[\"index\"]=test_index\nsub[\"impressions\"]=np.clip(np.exp(y_pred[:,0])-1, 0, None)\nsub[\"clicks\"]=np.exp(y_pred[:,1])-1\nsub[\"conversions\"]=np.exp(y_pred[:,2])-1\nsub[\"revenue\"]=np.exp(y_pred[:,3])-1","4d622580":"#overview of top 5 rows submission file\n\nsub.head()","d32b4e2c":"#rounding off the values in columns which could not have decical values  \n\nsub['impressions']=sub['impressions'].round(decimals=0)\nsub['clicks']=sub['clicks'].round(decimals=0)\nsub['conversions']=sub['conversions'].round(decimals=0)\n","7d09cb0d":"#converting the columns to interger to avoid decimal\n\nsub['impressions']=sub['impressions'].astype(int)\nsub['clicks']=sub['clicks'].astype(int)\nsub['conversions']=sub['conversions'].astype(int)","b56e4970":"#overview of top 5 rows submission file\nsub.head()","c756023a":"#exporting submission file to csv format \nsub.to_csv('submission_file_main.csv',index=False)","708451cb":"# Thank you","f7ddfcb3":"# Now, from the four graphs we have plotted, we can see that large number of values fall as outliers & treating them would result to feature loss from train file. Hence, we should not treat the outliers","4c404b8c":"# **As I have checked the time interval for train file which followed from \"1st of 8 month of the year 2020\" to \"28th day of 2nd month of the year 2021\". Mainly, the time duration was \"7 months\". So, one of the meaningful feature that can be extracted from date column could be the 'weekday - which day in a week'**","5b009bb8":"# Here the algorithm which I used was Multi-Output Regressor technique with LightGBM as boosting technique","40a27da7":"# The problem is about different Ad Campaigns followed by different ad groups & ads with some cost incured as a result of different campaigns, which lead to various outcomes such as impressions due to campaign, clicks on the ad,conversions happended due to campaign & revenue generated from the campaign. \n\n# And now we are to predict these outcomes mentioned above from another dataset with same parameters"}}