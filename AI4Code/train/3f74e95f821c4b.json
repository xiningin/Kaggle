{"cell_type":{"2b1ee338":"code","6309167d":"code","1316321b":"code","16ac76b3":"code","4869a222":"code","214109da":"code","d4dec71e":"code","e22deb60":"code","7da54555":"code","984fc465":"code","a3966253":"code","df2efa09":"code","a579fa13":"code","ca9e8efe":"code","531bea5b":"code","a2ab99ef":"code","0ff806f3":"code","0e7cde7a":"code","b1e5c41d":"code","59efaae2":"markdown","bab10033":"markdown","5a0dc73d":"markdown","df87bd76":"markdown","f86eaeb5":"markdown","db61feb1":"markdown","9972e4e2":"markdown"},"source":{"2b1ee338":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport zipfile\nfrom tqdm import tqdm\nimport os","6309167d":"local_zip = \"..\/input\/facial-keypoints-detection\/training.zip\"\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/train')\nzip_ref.close()","1316321b":"local_zip = \"..\/input\/facial-keypoints-detection\/test.zip\"\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp\/test')\nzip_ref.close()","16ac76b3":"training_directory = \"\/tmp\/train\/\"\ntesting_directory = \"\/tmp\/test\/\"\n\nprint(os.listdir(training_directory))\nprint(os.listdir(testing_directory))","4869a222":"train = pd.read_csv(\"\/tmp\/train\/training.csv\")\ntest = pd.read_csv(\"\/tmp\/test\/test.csv\")\n","214109da":"train.head(5)","d4dec71e":"train.info()","e22deb60":"train.isnull().any().value_counts()","7da54555":"train.fillna(method = 'ffill',inplace = True)","984fc465":"X = train.Image.values\ndel train['Image']\nY = train.values","a3966253":"x = []\nfor i in tqdm(X):\n    q = [int(j) for j in i.split()]\n    x.append(q)\nlen(x)","df2efa09":"x = np.array(x)\nx = x.reshape(7049, 96,96,1)\nx  = x\/255.0\nx.shape","a579fa13":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,Y,random_state = 42,test_size = 0.3)","ca9e8efe":"x_train.shape,x_test.shape","531bea5b":"y_train.shape,y_test.shape","a2ab99ef":"from keras.layers.advanced_activations import LeakyReLU\nfrom keras.models import Sequential, Model\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam","0ff806f3":"model = Sequential()\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","0e7cde7a":"model.compile(optimizer = 'adam',loss = 'mean_squared_error', metrics = ['mae','acc'])\nmodel.fit(x_train,y_train,batch_size=256, epochs=100,validation_data=(x_test,y_test))","b1e5c41d":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","59efaae2":"**Importing the libraries for the Model**","bab10033":"**Import the Libraries**","5a0dc73d":"**Convolutional Neural Network**","df87bd76":"**Access the train and test csv present in the zipfiles**","f86eaeb5":"**Preprocessing and preparing the data**","db61feb1":"**Splitting the dataset**","9972e4e2":"**Reading the train and test csv**"}}