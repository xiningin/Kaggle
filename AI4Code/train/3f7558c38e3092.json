{"cell_type":{"5b0664f7":"code","19b5d8e5":"code","af5874de":"code","2a31a25d":"code","08c682a4":"code","6e45f71b":"code","29c3eb4e":"code","620ed095":"code","228db4de":"code","89990447":"code","47b738d0":"code","bf9046e5":"code","c195972f":"code","63d1ca2e":"code","af0f7484":"code","9040e2bb":"code","2c668a65":"code","7a5579f3":"code","950cbb2e":"code","058aadd6":"code","e62baa24":"code","c0f6ba51":"code","97ae8e7f":"code","d5dd3fdf":"code","bc69708d":"code","eefc151c":"code","5cef56f3":"code","e21b611d":"markdown","b8ac5b70":"markdown","49234165":"markdown","660ff3b7":"markdown","ffd90ac5":"markdown","c904a6c6":"markdown","14d09721":"markdown","0c42b1a5":"markdown"},"source":{"5b0664f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19b5d8e5":"# %% [code]\n# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Sat Jul 14 13:13:34 2018\n\n@author: kcy\n\"\"\"\nimport pandas as pd\nimport numpy as np\n\n#import twitter data\ndata = pd.read_csv(r\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\",encoding = \"latin1\")\n\ndata_drop = data[data.label==0].index                                    # dropping 0 labelled tweets to balance our dataset\ndata = data.drop(data_drop[0:22000]).reset_index().drop(\"index\",axis=1)\n\n\nimport re                                                                # Regular Expression Library\nimport nltk as nlp  \nnlp.download(\"stopwords\")                                                # Download stopwords (Irrelevant Words) to folder named \"Corpus\"\nfrom nltk.corpus import stopwords                                        # Import Stopwords we downloaded\n\nnlp.download('punkt')\n\ndescription_list = []                                                    # We will put all words' last version into this list.\nfor description in data.tweet:\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)                    # Drop all characters excluded letters(a-z) and replace them whith \" \" (space)\n    description = description.lower()                                    # Turn all letters to lowercase\n    description = nlp.word_tokenize(description)                         # Advance version of Split Funtion\n    lemma = nlp.WordNetLemmatizer()                                      # Sort words by grouping inflected or variant forms of the same word.\n    description = [ lemma.lemmatize(word) for word in description]\n    description = \" \".join(description)\n    description_list.append(description)\n\n#bag of words\n\nfrom sklearn.feature_extraction.text import CountVectorizer             \nmax_features = 3000                                                     # We will choose most frequent 3000 words\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray() # Sparce Matrix\n\nprint(\"en sik kullanilan {} kelimeler: {}\".format(max_features,count_vectorizer.get_feature_names()))\n\n# %%\ny = data.label                                                          # Hate or normal tweet classes\nx = pd.DataFrame(sparce_matrix)\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\n\nx_train = pd.DataFrame(x_train).reset_index().drop(\"index\",axis=1)\nx_test = pd.DataFrame(x_test).reset_index().drop(\"index\",axis=1)\ny__train = pd.DataFrame(y_train).reset_index().drop(\"index\",axis=1)\ny_test = pd.DataFrame(y_test).reset_index().drop(\"index\",axis=1)","af5874de":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV ,train_test_split\nRFC = RandomForestClassifier()\nparameters = { \"max_depth\" : [100,1000,2000], \"min_samples_split\" : [20,50,100]}\n#creating our grid to find best parameters\ntree_grid_search = GridSearchCV(RFC,param_grid=parameters,scoring=\"accuracy\",cv = 3)  \ntree_grid_search.fit(x_train,y_train) # adding data to grid search\nprint(\" best parameters :\", tree_grid_search.best_params_,\"\\n best score : \" ,tree_grid_search.best_score_)","2a31a25d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nRFC = RandomForestClassifier(max_depth=1000,min_samples_split=100)         # Use Best Parameters to train model\nRFC.fit(x_train,y_train)","08c682a4":"RFC_pred = RFC.predict(x_train)                                            # Predictions of train set\nRFC_pred_test = RFC.predict(x_test)                                        # Predictions of test set\nRFC_test_report = classification_report(y_test, RFC_pred_test)             # Test Report\nRFC_test_confusion = confusion_matrix(y_test, RFC_pred_test)               # COnfusion matrix of predictions of test set\n\nRFC_train_report = classification_report(y_train,RFC_pred)                 # Train Report\nRFC_train_confusion = confusion_matrix(y_train,RFC_pred)                   # COnfusion matrix of predictions of train set\nprint(\"RFC Confusion MAtrix :\\n  \",RFC_test_confusion)","6e45f71b":"print(\"Test Reports For RFC: \\n\",RFC_test_report)","29c3eb4e":"from sklearn.metrics import plot_precision_recall_curve,plot_roc_curve\nplot_precision_recall_curve(RFC,x_test,y_test)","620ed095":"import pickle\n# save the model to disk\nRVC_filename = 'finalized_RFC_model.sav'\npickle.dump(RFC, open(RVC_filename, 'wb'))","228db4de":"# load the model from disk\nimport pickle\n\nRVC_filename = 'finalized_RFC_model.sav'\nfrom sklearn.metrics import classification_report, confusion_matrix\nloaded_RFC_model = pickle.load(open(RVC_filename, 'rb'))\nRVC = loaded_RFC_model","89990447":"train_svm_x, test_svm_x, train_svm_y, test_svm_y = train_test_split(x,y, test_size = 0.92, random_state = 42) # USE SMALL DATA FOR SVC!","47b738d0":"from sklearn.model_selection import GridSearchCV ,train_test_split\nfrom sklearn.svm import SVC\nimport numpy as np\nsvc = SVC()\n#creating paramater dictionary\nparameters = {\"kernel\" : [\"linear\", \"rbf\", \"poly\"] , \"C\" : [0.1, 0.5, 1, 5], \"tol\" : [0.001, 0.1 ]}\n\ngrid_search = GridSearchCV(svc,param_grid=parameters,scoring=\"recall\",cv = 5) # creating our grid to find best parameters\ngrid_search.fit(train_svm_x,train_svm_y) # adding data to grid search\nprint(\" best parameters :\", grid_search.best_params_,\"\\n best score : \" ,grid_search.best_score_)","bf9046e5":"from sklearn.svm import SVC\nsvc = SVC(C=1, kernel=\"linear\",tol=0.1)                                  # Use Best Parameters to train model\nsvc.fit(x_train,y_train)\n","c195972f":"svc_y_pred = svc.predict(x_train)                                        # Predictions of train set\nsvc_y_pred_test = svc.predict(x_test)                                    # Predictions of test set\nsvc_test_report = classification_report(y_test, svc_y_pred_test)         # Test Report\nsvc_test_confusion = confusion_matrix(y_test, svc_y_pred_test)           # COnfusion matrix of predictions of test set\n\nsvc_train_report = classification_report(y_train,svc_y_pred)             # Train Report\nsvc_train_confusion = confusion_matrix(y_train,svc_y_pred)               # COnfusion matrix of predictions of train set\nprint(\"SVC Confusion MAtrix \\n: \",svc_test_confusion)","63d1ca2e":"print(\"Test Reports For SVC: \\n\",svc_test_report)","af0f7484":"plot_precision_recall_curve(svc,x_test,y_test)","9040e2bb":"import pickle\n# save the model to disk\nSVC_filename = 'finalized_SVC_model.sav'\npickle.dump(svc, open(SVC_filename, 'wb'))","2c668a65":"# load the model from disk\nSVC_filename = 'finalized_SVC_model.sav'\nloaded_SVC_model = pickle.load(open(SVC_filename, 'rb'))\nsvc = loaded_SVC_model\n","7a5579f3":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library\nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 1500, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1]))\n    classifier.add(Dense(units = 750, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 250, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 100, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 25, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 15)\nclassifier.fit(x_train,y_train)\n","950cbb2e":"ANN_pred = classifier.predict(x_train)                                  # Predictions of train set\nANN_pred_test = classifier.predict(x_test)                              # Predictions of test set\nclassifier.score(x_test,y_test)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nANN_test_report = classification_report(y_test, ANN_pred_test)          # Test Report\nANN_test_confusion = confusion_matrix(y_test, ANN_pred_test)            # COnfusion matrix of predictions of test set\n\nANN_train_report = classification_report(y_train,ANN_pred)              # Train Report\nANN_train_confusion = confusion_matrix(y_train,ANN_pred)                # COnfusion matrix of predictions of train set\n\nprint(\"ANN Confusion MAtrix: \\n \",ANN_test_confusion)","058aadd6":"print(\"Test Report For ANN \\n\" ,ANN_test_report)","e62baa24":"ensemble_test_predictions = ((pd.DataFrame(RFC_pred_test)+pd.DataFrame(svc_y_pred_test)+pd.DataFrame(ANN_pred_test))\/3).round(0)  # taking average of all models' answers\nensemble_test_predictions.head()","c0f6ba51":"ensemble_confsuion= confusion_matrix(y_test, ensemble_test_predictions)\n\nprint(\"Ensemble Model Confusion MAtrix: \\n \",ensemble_confsuion)","97ae8e7f":"ensemble_test_report = classification_report(y_test, ensemble_test_predictions)\nprint(\"Test Report For Ensemble Learning \\n\" ,ensemble_test_report)","d5dd3fdf":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n\nRandomFC = accuracy_score(y_test,RFC_pred_test)*100\nSVCC = accuracy_score(y_test,svc_y_pred_test)*100\nANN = accuracy_score(y_test,ANN_pred_test)*100\nEnsemble = accuracy_score(y_test,ensemble_test_predictions)*100\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfig = sns.barplot([\"RFC\",\"SVC\",\"ANN\",\"ENSEMBLE\"],[RandomFC,SVCC,ANN,Ensemble])\nfig.set_title('Accurcy Scores of Models')\nfig.set(xlabel=\"Models\",ylabel=\"Acuuracy (%)\")\nfig.set(ylim=(80,100))","bc69708d":"RandomFC = precision_score(y_test,RFC_pred_test)*100\nSVCC = precision_score(y_test,svc_y_pred_test)*100\nANN = precision_score(y_test,ANN_pred_test)*100\nEnsemble = precision_score(y_test,ensemble_test_predictions)*100\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfig = sns.barplot([\"RFC\",\"SVC\",\"ANN\",\"ENSEMBLE\"],[RandomFC,SVCC,ANN,Ensemble])\nfig.set_title('Precision Scores of Models')\nfig.set(xlabel=\"Models\",ylabel=\"Precision (%)\")\nfig.set(ylim=(80,100))","eefc151c":"RandomFC = recall_score(y_test,RFC_pred_test)*100\nSVCC = recall_score(y_test,svc_y_pred_test)*100\nANN = recall_score(y_test,ANN_pred_test)*100\nEnsemble = recall_score(y_test,ensemble_test_predictions)*100\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfig = sns.barplot([\"RFC\",\"SVC\",\"ANN\",\"ENSEMBLE\"],[RandomFC,SVCC,ANN,Ensemble])\nfig.set_title('Recall Scores of Models')\nfig.set(xlabel=\"Models\",ylabel=\"Recall (%)\")\nfig.set(ylim=(60,100))","5cef56f3":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\nRandomFC = f1_score(y_test,RFC_pred_test)*100\nSVCC = f1_score(y_test,svc_y_pred_test)*100\nANN = f1_score(y_test,ANN_pred_test)*100\nEnsemble = f1_score(y_test,ensemble_test_predictions)*100\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfig = sns.barplot([\"RFC\",\"SVC\",\"ANN\",\"ENSEMBLE\"],[RandomFC,SVCC,ANN,Ensemble])\nfig.set_title('F1 Scores of Models')\nfig.set(xlabel=\"Models\",ylabel=\"F1 (%)\")\nfig.set(ylim=(70,90))","e21b611d":"# ENSEMBLE LEARNING","b8ac5b70":"# TRAINING A SVC","49234165":"# SAVE RFC","660ff3b7":"# COMPARISIONS OF 4 MODELS' PERFORMANCE","ffd90ac5":"# TRAINING AN ARTIFICIAL NEURAL NETWORK","c904a6c6":"# PREPARING DATA","14d09721":"# SAVE SVC","0c42b1a5":"### We will use 3 model's average answer to obtain maximum performance"}}