{"cell_type":{"7eea9391":"code","32d7530a":"code","9f906266":"code","1187e211":"code","10107d7f":"code","c8f54adf":"code","f0e22837":"code","f08b434b":"code","4c8df07a":"code","a002535f":"code","41081896":"code","8c8d80d6":"code","9e1ff790":"code","4c7682d3":"code","81a11e69":"code","1c218d33":"code","051e05a9":"code","bb91cbe0":"code","1d29b9d9":"code","9993ef6c":"code","373ca5a0":"code","e22a3d27":"code","ca8a8f64":"code","3d728905":"code","feb214fd":"code","9aac98bd":"code","45a1945b":"code","e6d5f0fa":"code","e3320643":"code","fde64675":"code","58789adb":"code","a1528e8c":"code","dfecc287":"code","124ebffe":"code","a73863b4":"code","3e2ff175":"code","19eddac0":"markdown","d30e674a":"markdown","d54bcb9b":"markdown","caecf636":"markdown","3d01b63a":"markdown","f21683ad":"markdown","6bc04068":"markdown","60b3cf7d":"markdown","90566792":"markdown","3d78cebf":"markdown","b84d2b8b":"markdown","75dcf4f7":"markdown","0cfd2d42":"markdown","26768eee":"markdown","3b965b76":"markdown","a923d474":"markdown"},"source":{"7eea9391":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32d7530a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle","9f906266":"with open(\"..\/input\/traffic-sign-classification\/train.p\", mode='rb') as training_data:\n    train = pickle.load(training_data)\nwith open(\"..\/input\/traffic-sign-classification\/valid.p\", mode='rb') as validation_data:\n    valid = pickle.load(validation_data)\nwith open(\"..\/input\/traffic-sign-classification\/test.p\", mode='rb') as testing_data:\n    test = pickle.load(testing_data)","1187e211":"X_train,y_train = train['features'],train['labels']\nX_validation,y_validation = valid['features'],valid['labels']\nX_test,y_test = test['features'],test['labels']","10107d7f":"X_train.shape","c8f54adf":"y_train.shape","f0e22837":"X_test.shape","f08b434b":"y_test.shape","4c8df07a":"X_validation.shape","a002535f":"y_validation.shape","41081896":"i = 234\nplt.imshow(X_train[i]) #will show the image\ny_train[i] #this will show to which class does the image belongs","8c8d80d6":"i = 12232\nplt.imshow(X_train[i])\ny_train[i]","9e1ff790":"from sklearn.utils import shuffle","4c7682d3":"X_train,y_train = shuffle(X_train,y_train)","81a11e69":"X_train_gray = np.sum(X_train\/3,axis=3,keepdims=True) #While keeping the dimensions same, we are averaging the 3 colours RGB into one(gray).","1c218d33":"X_train_gray.shape #now the dimension is 32 by 32 by 1.\n","051e05a9":"X_test_gray = np.sum(X_test\/3,axis=3,keepdims=True)\nX_validation_gray = np.sum(X_validation\/3,axis=3,keepdims=True)","bb91cbe0":"X_test_gray.shape","1d29b9d9":"X_validation_gray.shape","9993ef6c":"X_train_gray_norm = (X_train_gray - 128)\/128\nX_test_gray_norm = (X_test_gray - 128)\/128\nX_validation_gray_norm = (X_validation_gray - 128)\/128","373ca5a0":"X_train_gray_norm","e22a3d27":"i = 300\nplt.imshow(X_train_gray[i].squeeze(),cmap='gray')#we use squeeze becuase we dont want the dimns to be 32 by 32 by 1 but 32 by 32.\ny_train[i]\nplt.figure() #create new image\nplt.imshow(X_train[i]) #actual image\n","ca8a8f64":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,AveragePooling2D,Dense,Flatten,Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import TensorBoard","3d728905":"cnn_model = Sequential()","feb214fd":"cnn_model.add(Conv2D(filters = 6,kernel_size = (5,5),activation = 'relu',input_shape =(32,32,1))) #filters represent the depth,kernal size is size of filter layer,input shape is dimns of the input image\ncnn_model.add(AveragePooling2D())\n\ncnn_model.add(Conv2D(filters = 16,kernel_size = (5,5),activation = 'relu')) #filters represent the depth,kernal size is size of filer layer,input shape is dimns of the input image\ncnn_model.add(AveragePooling2D())\n\ncnn_model.add(Flatten())\n#creating an artificial neural network\ncnn_model.add(Dense(120,activation='relu'))\ncnn_model.add(Dense(84,activation='relu'))\ncnn_model.add(Dense(43,activation='softmax')) #output - hence 43 neurons corresponding to 43 classes\n\n","9aac98bd":"cnn_model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])","45a1945b":"history = cnn_model.fit(X_train_gray_norm,y_train,batch_size=500,epochs = 20,verbose=1,validation_data=(X_validation_gray_norm,y_validation))","e6d5f0fa":"score = cnn_model.evaluate(X_test_gray_norm,y_test)\nprint('Test Accuracy:{}'.format(score[1]))","e3320643":"plott = pd.DataFrame(history.history)","fde64675":"plott.plot()","58789adb":"prediction = cnn_model.predict_classes(X_test_gray_norm)","a1528e8c":"from sklearn.metrics import confusion_matrix","dfecc287":"cm = confusion_matrix(y_test,prediction)","124ebffe":"plt.figure(figsize=(25,25))\nsns.heatmap(cm,annot=True)","a73863b4":"from sklearn.metrics import accuracy_score","3e2ff175":"accuracy_score(y_test,prediction)","19eddac0":"Lets see if we have preserved the actual features of image or not.","d30e674a":"# Normalisation","d54bcb9b":"Training data has 34799 images . Each image with its repective label.","caecf636":"So the dimns have been reduced to 32 by 32 by 1.","3d01b63a":"# Model Training","f21683ad":"So we have normalised the pixels such that they range from -1 to 1.","6bc04068":"**Great! We have achived a quite high accuracy for thousands of images of traffic signals with few basic steps!**","60b3cf7d":" We have been provided with images of traffic signs and the goal is to train a Deep Network to classify them\n- The dataset contains 43 different classes of images. \n- Classes are as listed below: \n\n    - ( 0, b'Speed limit (20km\/h)') ( 1, b'Speed limit (30km\/h)')\n    - ( 2, b'Speed limit (50km\/h)') ( 3, b'Speed limit (60km\/h)')\n    - ( 4, b'Speed limit (70km\/h)') ( 5, b'Speed limit (80km\/h)')\n    - ( 6, b'End of speed limit (80km\/h)') ( 7, b'Speed limit (100km\/h)')\n    - ( 8, b'Speed limit (120km\/h)') ( 9, b'No passing')\n    - (10, b'No passing for vehicles over 3.5 metric tons')\n    - (11, b'Right-of-way at the next intersection') (12, b'Priority road')\n    - (13, b'Yield') (14, b'Stop') (15, b'No vehicles')\n    - (16, b'Vehicles over 3.5 metric tons prohibited') (17, b'No entry')\n    - (18, b'General caution') (19, b'Dangerous curve to the left')\n    - (20, b'Dangerous curve to the right') (21, b'Double curve')\n    - (22, b'Bumpy road') (23, b'Slippery road')\n    - (24, b'Road narrows on the right') (25, b'Road work')\n    - (26, b'Traffic signals') (27, b'Pedestrians') (28, b'Children crossing')\n    - (29, b'Bicycles crossing') (30, b'Beware of ice\/snow')\n    - (31, b'Wild animals crossing')\n    - (32, b'End of all speed and passing limits') (33, b'Turn right ahead')\n    - (34, b'Turn left ahead') (35, b'Ahead only') (36, b'Go straight or right')\n    - (37, b'Go straight or left') (38, b'Keep right') (39, b'Keep left')\n    - (40, b'Roundabout mandatory') (41, b'End of no passing')\n    - (42, b'End of no passing by vehicles over 3.5 metric tons')\n\n- The network used is called Le-Net that was presented by Yann LeCun\nhttp:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-01a.pdf","90566792":"# Data Preperation","3d78cebf":"The model consists of the following layers: \n\n- STEP 1: THE FIRST CONVOLUTIONAL LAYER #1\n    - Input = 32x32x1\n    - Output = 28x28x6\n    - Output = (Input-filter+1)\/Stride* => (32-5+1)\/1=28\n    - Used a 5x5 Filter with input depth of 3 and output depth of 6\n    - Apply a RELU Activation function to the output\n    - pooling for input, Input = 28x28x6 and Output = 14x14x6\n\n\n    * Stride is the amount by which the kernel is shifted when the kernel is passed over the image.\n\n- STEP 2: THE SECOND CONVOLUTIONAL LAYER #2\n    - Input = 14x14x6\n    - Output = 10x10x16\n    - Layer 2: Convolutional layer with Output = 10x10x16\n    - Output = (Input-filter+1)\/strides => 10 = 14-5+1\/1\n    - Apply a RELU Activation function to the output\n    - Pooling with Input = 10x10x16 and Output = 5x5x16\n\n- STEP 3: FLATTENING THE NETWORK\n    - Flatten the network with Input = 5x5x16 and Output = 400\n\n- STEP 4: FULLY CONNECTED LAYER\n    - Layer 3: Fully Connected layer with Input = 400 and Output = 120\n    - Apply a RELU Activation function to the output\n\n- STEP 5: ANOTHER FULLY CONNECTED LAYER\n    - Layer 4: Fully Connected Layer with Input = 120 and Output = 84\n    - Apply a RELU Activation function to the output\n\n- STEP 6: FULLY CONNECTED LAYER\n    \n   - Note - Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. The below figure shows convolution would work with a stride of 2.","b84d2b8b":"- Wild Animals Crossing\n","75dcf4f7":"**Data Exploration**","0cfd2d42":"The actual features are preserved!","26768eee":"- No passing","3b965b76":"So we have 34799 images each consisting of 32 by 32 pixel dimension with RGB mode.","a923d474":"The values in the heatmap shows the number of correclty identified signals corresponding to each class."}}