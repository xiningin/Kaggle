{"cell_type":{"830e3c12":"code","9edc1d31":"code","95a1c975":"code","ec6411b1":"code","d5595bc3":"code","cfdecd06":"code","afe48eed":"code","280ca453":"code","4353c19c":"code","05940089":"code","1ca26c49":"code","0c99f382":"code","9301e560":"code","6957a4c1":"code","1e923e2f":"code","4dee770a":"code","a65fefa3":"code","0714345b":"code","9f2594e2":"code","1ec43e70":"code","3346963b":"code","a20a2ee4":"code","aef83b56":"code","817e8e1e":"code","0cef92f3":"markdown","1cc83dce":"markdown","8e38b7af":"markdown","9e58761c":"markdown","938613b0":"markdown","3ded1a96":"markdown","df62bdf3":"markdown","f98fb8b4":"markdown","22d603dd":"markdown","5bad799d":"markdown","b9cb7347":"markdown","1efd59a4":"markdown"},"source":{"830e3c12":"import numpy as np","9edc1d31":"# data\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1)\nX_new = np.array([[0], [2]])\nX_b = np.c_[np.ones((100, 1)), X]","95a1c975":"# Linear Regrtession in Scikit-Learn\n\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg.intercept_, lin_reg.coef_","ec6411b1":"print(lin_reg.predict(X_new))\n\n# OR\n\ntheta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\nprint(theta_best_svd)","d5595bc3":"from sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nsgd_reg.fit(X, y.ravel())\nsgd_reg.intercept_, sgd_reg.coef_","cfdecd06":"import random\n\nrandom.seed(10)\n\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)","afe48eed":"from sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly_features.fit_transform(X)","280ca453":"X[0]","4353c19c":"X_poly[0]","05940089":"# This gives the intercept, the coeff of X^2 and X\nlin_reg = LinearRegression()\nlin_reg.fit(X_poly, y)\nlin_reg.intercept_, lin_reg.coef_","1ca26c49":"sgd_reg = SGDRegressor(penalty=\"l2\")\nsgd_reg.fit(X, y.ravel())\nsgd_reg.predict([[4.5]])","0c99f382":"sgd_reg = SGDRegressor(penalty=\"l1\")\nsgd_reg.fit(X, y.ravel())\nsgd_reg.predict([[4.5]])","9301e560":"import pandas as pd","6957a4c1":"import statsmodels.api as sm\ndf_adv = pd.read_csv('data\/Advertising.csv', index_col=0)\nX = df_adv[['TV', 'Radio','Newspaper']]\ny = df_adv['Sales']\ndf_adv.head()","1e923e2f":"X = sm.add_constant(X)","4dee770a":"X.head()","a65fefa3":"## fit a OLS model with intercept on TV and Radio\n\nmodel= sm.OLS(y, X).fit()","0714345b":"model.summary()\n### The std err value for every feature is less which states that there is no collinearity","9f2594e2":"import matplotlib.pyplot as plt\nX.iloc[:,1:].corr()\n\n# This shows that there is no multicoolinearity amongst the features","1ec43e70":"df_salary = pd.read_csv('https:\/\/raw.githubusercontent.com\/krishnaik06\/Multicollinearity\/master\/data\/Salary_Data.csv')\ndf_salart.head()","3346963b":"X = df_salary[['YearsExperience', 'Age']]\ny = df_salary['Salary']","a20a2ee4":"X = sm.add_constant(X)\nmodel = sm.OLS(y,X).fit()","aef83b56":"model.summary()","817e8e1e":"X.iloc[:,1:].corr()","0cef92f3":"### Multicollinearity in Linear Regression","1cc83dce":"### Linear Regression","8e38b7af":"### Stochastic Gradient Descent\n- When the cost function is very irregular, this can actually help the algorithm jump out of local minima, so Stochastic Gradient Descent has a better chance of finding the global minimum than Batch Gradient Descent does.\n- Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum.\n- One solution to this dilemma is to gradually reduce the learning rate.\n\nIn Scikit-Learn we have SGDRegressor","9e58761c":"### Learning Curves\n- If your model is underfitting the training data, adding more training examples **will not help**. You need to use a more complex model or come up with better features.\n- One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error.","938613b0":"### Batch Gradient Descent\n- In this a whole set of Training data is considered for finding the optimal value, this makes it slower than others. \n- \u03b8(next step) = \u03b8 \u2212 \u03b7\u2207\u03b8 MSE(\u03b8). \n- The learning rate playes a vital role.","3ded1a96":"### Gradient Descent\n- It measures the local gradient of the error function with regards to the parameter vector \u03b8, and it goes in the direction of descending gradient. Once the gra\u2010dient is zero, you have reached a minimum.\n- For a Linear Regression model happens to be a convex function. This implies that there are no local minima, just one global minimum. ","df62bdf3":"### Polynomial Regression\n","f98fb8b4":"### Logistic Regression\n\n- If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled \u201c1\u201d), or else it predicts that it does not (i.e., it belongs to the negative class, labeled \u201c0\u201d).\n- It uses the Sigmoid Function as its boundary. \n- ","22d603dd":"### Mini-batch Gradient Descen\n- Instead of computing the gradients based on the full train\u2010\ning set (as in Batch GD) or based on just one instance (as in Stochastic GD), Minibatch GD computes the gradients on small random sets of instances called minibatches.","5bad799d":"### The Normal Equation\n- \u03b8 = (X^TX)^-1 X^T y\n- This equation would give the optimal value of thita for any equation.\n- It is not preferable for large dataset as it will increase the complexity by about O(n^3). \n- It is better than Gradient descent when having samller dataset. ","b9cb7347":"### The Bias\/Variance Tradeoff\nModel\u2019s generalization error can be expressed as the sum of three very different\nerrors:\n#### Bias\n- This part of the generalization error is due to wrong assumptions.\n- High bias means when train and test set neither fit the model, both having high error.\n#### Variance\n- This part is due to the model\u2019s excessive sensitivity to small variations in the training data.\n- When test error is very high than train error.","1efd59a4":"### Regularized Linear Models\n- Regularization is performed to decrease rge coefficients of the linear model to make model more efficient.\n\n#### Ridge regression\n- \u00bd(\u2225 w \u22252)<sup>2<\/sup> is added to the cost function, it is the l2-norm.\n- **J \u03b8 = MSE \u03b8 + \u03b1\u00bd\u2211\u03b8<sup>2<\/sup>**\n\n#### Lasso regression\n- It tends to completely eliminate the weights of the least important features (i.e., set them to zero).\n- **J(\u03b8) = MSE(\u03b8) + \u03b1\u2211i|\u03b8|**\n\n#### Elastic Net\n- Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio r. \n- When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression"}}