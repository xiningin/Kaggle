{"cell_type":{"1bb9a2f6":"code","6a265bdf":"code","2679a5fe":"code","49e94ef9":"code","75bf455c":"code","3a75d77b":"code","8fea436d":"code","136b4cc5":"code","d07fca11":"code","4df598f2":"code","369498db":"code","ac4f0cca":"code","dcb8bf14":"code","10bd1471":"code","33b8fb71":"markdown","aaa7d217":"markdown","005c1691":"markdown","9319e112":"markdown","0da47a09":"markdown","ec2c1907":"markdown"},"source":{"1bb9a2f6":"# Starter code to load data\nimport pandas as pd\n# Training dataset\ndata=pd.read_csv('..\/input\/bgse-svm-death\/mimic_train.csv')\ndata.head()","6a265bdf":"# Test dataset (to produce predictions)\ndata_test=pd.read_csv('..\/input\/bgse-svm-death\/mimic_test_death.csv')\ndata_test.sort_values('icustay_id').head()","2679a5fe":"# Sample output prediction file\npred_sample=pd.read_csv('..\/input\/bgse-svm-death\/mimic_kaggle_death_sample_submission.csv')\npred_sample.sort_values('icustay_id').head()","49e94ef9":"#your code here","75bf455c":"#!pip install ipython-autotime","3a75d77b":"import pandas as pd\nimport numpy as np\nimport time\n\nfrom sklearn.svm import SVC, LinearSVC\n\nnp.random.seed(3123) # impose random seed for reproducibility\n\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n# kernel approximators\nfrom sklearn.kernel_approximation import Nystroem, RBFSampler\n\nfrom imblearn.under_sampling import  RandomUnderSampler\nfrom imblearn.over_sampling import  RandomOverSampler\n\nimport gc","8fea436d":"%%time\n\nexp_flag = data['HOSPITAL_EXPIRE_FLAG']\nnumbers = ['HeartRate_Min','HeartRate_Max','HeartRate_Mean','SysBP_Min',\n       'SysBP_Max', 'SysBP_Mean', 'DiasBP_Min', 'DiasBP_Max', 'DiasBP_Mean',\n       'MeanBP_Min', 'MeanBP_Max', 'MeanBP_Mean', 'RespRate_Min',\n       'RespRate_Max', 'RespRate_Mean', 'TempC_Min', 'TempC_Max', 'TempC_Mean',\n       'SpO2_Min', 'SpO2_Max', 'SpO2_Mean', 'Glucose_Min', 'Glucose_Max',\n       'Glucose_Mean']\ncateg = ['ADMISSION_TYPE', 'FIRST_CAREUNIT']\n\nX=data.loc[:,numbers+categ]\nX_test=data_test.loc[:,numbers+categ]\n\n# Split preprocessing depending on type\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numbers),\n        ('cat', categorical_transformer, categ)],\n        remainder='drop')\n\npreprocessor.fit(X)\nX=preprocessor.transform(X)\nX_test=preprocessor.transform(X_test)\n\nX=pd.DataFrame(X, \n              columns= numbers+\n               list(preprocessor.transformers_[1][1]['onehot'].get_feature_names(categ)))\nX_test=pd.DataFrame(X_test, \n              columns= numbers+\n               list(preprocessor.transformers_[1][1]['onehot'].get_feature_names(categ)))\n\nX_colnames=X.columns\n","136b4cc5":"X.head()","d07fca11":"%%time\n# Undersample\nsampler = RandomUnderSampler(random_state=0)\nX, exp_flag = sampler.fit_resample(X, exp_flag)","4df598f2":"%%time\n# Try linear kernel using LinearSVC\nMySvc = LinearSVC( max_iter=10000)\ngrid_values = {'C':[0.1, 1, 10]}\ngrid_svc_acc1 = GridSearchCV(MySvc, \n                    param_grid = grid_values, scoring='roc_auc',\n                    cv=StratifiedKFold(n_splits=5, shuffle=True),\n                    n_jobs=5)\ngrid_svc_acc1.fit(X, exp_flag)\ngrid_svc_acc1.best_score_\n","369498db":"%%time\n# Try linear kernel\ngc.collect()\n\nMySvc = SVC(kernel='linear')\ngrid_values = {'C':[0.1, 1, 10]}\ngrid_svc_acc2 = GridSearchCV(MySvc, \n                    param_grid = grid_values, scoring='roc_auc',\n                    cv=StratifiedKFold(n_splits=5, shuffle=True),\n                    n_jobs=5)\ngrid_svc_acc2.fit(X, exp_flag)\ngrid_svc_acc2.best_score_","ac4f0cca":"%%time\n# Try  rbf\ngc.collect()\n\nMySvc = SVC(kernel='rbf')\ngrid_values = {'C':[0.1, 1, 10]}\ngrid_svc_acc3 = GridSearchCV(MySvc, \n                    param_grid = grid_values, scoring='roc_auc',\n                    cv=StratifiedKFold(n_splits=5, shuffle=True),\n                    n_jobs=5)\ngrid_svc_acc3.fit(X, exp_flag)\ngrid_svc_acc3.best_score_","dcb8bf14":"%%time\ngc.collect()\n\n# Try kernel approximation with Nystroem\nGAMMA=0.2\nN_Comp=200\nfeature_map_nystroem = Nystroem(gamma=GAMMA,\n                                 random_state=1,\n                                 n_components=N_Comp)\nfeature_map_nystroem.fit(X)\nX_transformed = feature_map_nystroem.transform(X)\nMySvc = LinearSVC( max_iter=10000)\ngrid_values = {'C':[0.1, 1, 10]}\ngrid_svc_acc4 = GridSearchCV(MySvc, \n                    param_grid = grid_values, scoring='roc_auc',\n                    cv=StratifiedKFold(n_splits=5, shuffle=True),\n                    n_jobs=5)\ngrid_svc_acc4.fit(X, exp_flag)\ngrid_svc_acc4.best_score_","10bd1471":"%%time\n# Try kernel approximation with RBFSampler\ngc.collect()\n\nfeature_map_sampler = RBFSampler(gamma=GAMMA,\n                                 random_state=1,\n                                 n_components=N_Comp)\nfeature_map_sampler.fit(X)\nX_transformed = feature_map_sampler.transform(X)\nMySvc = LinearSVC( max_iter=10000)\ngrid_values = {'C':[0.1, 1, 10]}\ngrid_svc_acc5 = GridSearchCV(MySvc, \n                    param_grid = grid_values, scoring='roc_auc',\n                    cv=StratifiedKFold(n_splits=5, shuffle=True),\n                    n_jobs=5)\ngrid_svc_acc5.fit(X, exp_flag)\ngrid_svc_acc5.best_score_","33b8fb71":"Parameter tuning and Cross-validation accuracies using different methods","aaa7d217":"If you want to scale to more data or features, you can try kernel approximation methods + linear SVC... (check the documentation!)","005c1691":"## Programming project: probability of death\n\nIn this project, you have to predict the probability of death of a patient that is entering an ICU (Intensive Care Unit).\n\nThe dataset comes from MIMIC project (https:\/\/mimic.physionet.org\/). MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n\nEach row of *mimic_train.csv* correponds to one ICU stay (*hadm_id*+*icustay_id*) of one patient (*subject_id*). Column HOSPITAL_EXPIRE_FLAG is the indicator of death (=1) as a result of the current hospital stay; this is the outcome to predict in our modelling exercise.\nThe remaining columns correspond to vitals of each patient (when entering the ICU), plus some general characteristics (age, gender, etc.), and their explanation can be found at *mimic_patient_metadata.csv*. \n\nNote that the main cause\/disease of patient contidition is embedded as a code at *ICD9_diagnosis* column. The meaning of this code can be found at *MIMIC_metadata_diagnose.csv*. **But** this is only the main one; a patient can have co-occurrent diseases (comorbidities). These secondary codes can be found at *extra_data\/MIMIC_diagnoses.csv*.\n\nDon't use features that you don't know the first day a patient enters the ICU, such as LOS.\n\nAs performance metric, you can use *AUC* for the binary classification case, but feel free to report as well any other metric if you can justify that is particularly suitable for this case.\n\nMain tasks are:\n+ Using *mimic_train.csv* file build a predictive model for *HOSPITAL_EXPIRE_FLAG* .\n+ For this analysis there is an extra test dataset, *mimic_test.csv*. Apply your final model to this extra dataset and submit to Kaggle competition to obtain accuracy of prediction (follow the requested format).\n\nTry to optimize hyperparameters of your SVM model.\n\nYou can follow those **steps** in your first implementation:\n1. *Explore* and understand the dataset. \n2. Manage missing data.\n2. Manage categorial features. E.g. create *dummy variables* for relevant categorical features, or build an ad hoc distance function.\n3. Build a prediction model. Try to improve it using methods to tackle class imbalance.\n5. Assess expected accuracy  of previous models using *cross-validation*. \n6. Test the performance on the test file by submitting to Kaggle, following same preparation steps (missing data, dummies, etc). Remember that you should be able to yield a prediction for all the rows of the test dataset.\n\nFor the in-class version, feel free to reduce the training dataset if you experience computational constraints.\n\n## Sample usage of SVM...","9319e112":"# Project: Support Vector Machines (SVM)\n","0da47a09":"Preprocessing...","ec2c1907":"<img src = \"..\/..\/Data\/bgsedsc_0.jpg\">"}}