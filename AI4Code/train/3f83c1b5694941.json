{"cell_type":{"649ad169":"code","9f3f6f29":"code","97ff1519":"code","0af5934a":"code","1b4c7136":"code","7679bc15":"code","4c0ea6af":"code","0f1a5b3f":"code","1a221759":"code","5c4944a9":"code","664b48d9":"code","4a6c1c1e":"code","b7fd4515":"code","5e7b2986":"code","af409daf":"code","5464559c":"code","0259811c":"code","4caa9bd9":"code","2796609c":"code","04de57bd":"code","8e6b8804":"code","dff246cb":"code","7638e0fb":"code","aeaff98c":"code","a3da2183":"code","1f733b9b":"code","df5cb067":"code","22921c59":"code","2673596c":"code","3c34fafd":"code","79d232a9":"code","94e99a7b":"code","3c64c13e":"code","0203055e":"code","8d43b7d0":"code","746bc37e":"code","b0d536b7":"code","b757e01a":"code","5ed4420a":"code","427754e8":"code","6be563e0":"code","4343ee98":"code","6e743c95":"code","b5189ad5":"code","3104e1d4":"code","6b1e06db":"code","e4b90789":"markdown","f19c1aa6":"markdown","a2ecde65":"markdown","876b32f3":"markdown"},"source":{"649ad169":"\nimport numpy as np \nimport pandas as pd \nimport cv2\n\n","9f3f6f29":"df = pd.read_csv('..\/input\/facial-expression\/fer2013.csv')","97ff1519":"df.head()","0af5934a":"df.iloc[0]['pixels']\nprint(df)\nlen(df.iloc[0]['pixels'].split())\n# 48 * 48","1b4c7136":"label_map = ['Anger', 'Neutral', 'Fear', 'Happy', 'Sad', 'Surprise']","7679bc15":"import matplotlib.pyplot as plt","4c0ea6af":"img = df.iloc[0]['pixels'].split()\n# img                                    img is in string","0f1a5b3f":"img = [int(i) for i in img]  #Conversion into integer","1a221759":"type(img[0])","5c4944a9":"len(img)","664b48d9":"img = np.array(img)","4a6c1c1e":"img = img.reshape(48,48)","b7fd4515":"img.shape","5e7b2986":"plt.imshow(img, cmap='gray')\nplt.xlabel(df.iloc[0]['emotion'])","af409daf":"X = []     #List of images\ny = []     #Emotions","5464559c":"def getData(path):\n    anger = 0\n    fear = 0\n    sad = 0\n    happy = 0\n    surprise = 0\n    neutral = 0\n    df = pd.read_csv(path)\n    \n    X = []\n    y = []    \n    \n    for i in range(len(df)):\n        if df.iloc[i]['emotion'] != 1:     #removal of disgust images\n            if df.iloc[i]['emotion'] == 0:  #We will take 4000 images from each dataset [0] is anger emotion\n                if anger <= 4000:            \n                    y.append(df.iloc[i]['emotion'])\n                    im = df.iloc[i]['pixels']\n                    im = [int(x) for x in im.split()]\n                    X.append(im)\n                    anger += 1\n                else:\n                    pass\n                \n            if df.iloc[i]['emotion'] == 2:\n                if fear <= 4000:            \n                    y.append(df.iloc[i]['emotion'])# Fear\n                    im = df.iloc[i]['pixels']\n                    im = [int(x) for x in im.split()]\n                    X.append(im)\n                    fear += 1\n                else:\n                    pass\n                \n            if df.iloc[i]['emotion'] == 3:\n                if happy <= 4000:                  #fear\n                    y.append(df.iloc[i]['emotion'])\n                    im = df.iloc[i]['pixels']\n                    im = [int(x) for x in im.split()]\n                    X.append(im)\n                    happy += 1\n                else:\n                    pass\n                \n            if df.iloc[i]['emotion'] == 4:\n                if sad <= 4000:            \n                    y.append(df.iloc[i]['emotion'])#Sad\n                    im = df.iloc[i]['pixels']\n                    im = [int(x) for x in im.split()]\n                    X.append(im)\n                    sad += 1\n                else:\n                    pass\n                \n            if df.iloc[i]['emotion'] == 5:\n                if surprise <= 4000:            #Surprise\n                    y.append(df.iloc[i]['emotion'])\n                    im = df.iloc[i]['pixels']\n                    im = [int(x) for x in im.split()]\n                    X.append(im)\n                    surprise += 1\n                else:\n                    pass\n                \n            if df.iloc[i]['emotion'] == 6:\n                if neutral <= 4000:            #neutral\n                    y.append(df.iloc[i]['emotion'])\n                    im = df.iloc[i]['pixels']\n                    im = [int(x) for x in im.split()]\n                    X.append(im)\n                    neutral += 1\n                else:\n                    pass\n\n            \n            \n    return X, y  \n    ","0259811c":"X, y = getData('..\/input\/facial-expression\/fer2013.csv')","4caa9bd9":"np.unique(y, return_counts=True)","2796609c":"X = np.array(X)\/255.0\ny = np.array(y)","04de57bd":"X.shape, y.shape","8e6b8804":"y_o = []\nfor i in y:\n    if i != 6:\n        y_o.append(i)\n                                            #Moving 6th to another position \n    else:\n        y_o.append(1)","dff246cb":"np.unique(y_o, return_counts=True)","7638e0fb":"for i in range(5):\n    r = np.random.randint((1), 24000, 1)[0]\n    plt.figure()\n    plt.imshow(X[r].reshape(48,48), cmap='gray')\n    plt.xlabel(label_map[y_o[r]])","aeaff98c":"X = X.reshape(len(X), 48, 48, 1)        #If color image 3 instead of 1 ","a3da2183":"# no_of_images, height, width, coloar_map  In Cnn data should be in 4 dimensions ","1f733b9b":"X.shape","df5cb067":"from keras.utils import to_categorical\ny_new = to_categorical(y_o, num_classes=6)","22921c59":"len(y_o), y_new.shape","2673596c":"y_o[150], y_new[150]","3c34fafd":"from keras.models import Sequential\nfrom keras.layers import Dense , Activation , Dropout ,Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.metrics import categorical_accuracy\nfrom keras.models import model_from_json\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import *\nfrom keras.layers.normalization import BatchNormalization","79d232a9":"model = Sequential()              #Exactly One input and one output (layer by layer modelling)\n\n\ninput_shape = (48,48,1)\n\n\nmodel.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\nmodel.add(Conv2D(64, (5, 5), padding='same'))\nmodel.add(BatchNormalization())          #This has the effect of stabilizing the learning process and dramatically reducing\n                                        #the number of training epochs required to train deep networks.\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n\nmodel.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\nmodel.add(Conv2D(128, (5, 5),padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\n## (15, 15) --->  30\nmodel.add(Flatten())\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n\n# fer_json = model.to_json()\n# with open(\"fer.json\", \"w\") as json_file:\n#     json_file.write(fer_json)\n# model.save_weights(\"fer.h5\")","94e99a7b":"history = model.fit(X, y_new, epochs=200, batch_size=64, shuffle=True, validation_split=0.2)\nprint(history.history.keys())\n\n\n\n","3c64c13e":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\nplt.show()","0203055e":"# summarize history for accuracy\n# plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\nplt.show()","8d43b7d0":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","746bc37e":"# model.save('model.h5')","b0d536b7":"# fer_json = model.to_json()  \n# with open(\"fer.json\", \"w\") as json_file:  \n#     json_file.write(fer_json)  \n# model.save_weights(\"fer.h5\")  ","b757e01a":"# import cv2","5ed4420a":"# test_img = cv2.imread('..\/input\/fer2013\/train\/sad\/Training_10022789.jpg', 0)","427754e8":"# print (test_img)","6be563e0":"# test_img.shape","4343ee98":"# test_img = cv2.resize(test_img, (48,48))\n# test_img.shape","6e743c95":"# test_img = test_img.reshape(1,48,48,1)","b5189ad5":"# model.predict(test_img)","3104e1d4":"# label_map = ['Anger', 'Neutral', 'Fear', 'Happy', 'Sad', 'Surprise']","6b1e06db":"# import os  \n# import cv2  \n# import numpy as np  \n# from keras.models import model_from_json  \n# from keras.preprocessing import image  \n  \n# #load model  \n# model = model_from_json(open(\".\/fer.json\", \"r\").read())  \n# #load weights  \n# model.load_weights('.\/fer.h5')  \n  \n  \n# face_haar_cascade = cv2.CascadeClassifier('..\/input\/harrcascade\/haarcascade_frontalcatface.xml')  \n  \n  \n# cap=cv2.VideoCapture(0)  \n  \n# while True:  \n#     ret,test_img=cap.read()# captures frame and returns boolean value and captured image  \n#     if not ret:  \n#         continue  \n#     gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)  \n  \n#     faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)  \n  \n  \n#     for (x,y,w,h) in faces_detected:  \n#         cv2.rectangle(test_img,(x,y),(x+w,y+h),(255,0,0),thickness=7)  \n#         roi_gray=gray_img[y:y+w,x:x+h]#cropping region of interest i.e. face area from  image  \n#         roi_gray=cv2.resize(roi_gray,(48,48))  \n#         img_pixels = image.img_to_array(roi_gray)  \n#         img_pixels = np.expand_dims(img_pixels, axis = 0)  \n#         img_pixels \/= 255  \n  \n#         predictions = model.predict(img_pixels)  \n  \n#         #find max indexed array  \n#         max_index = np.argmax(predictions[0])  \n  \n#         emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')  \n#         predicted_emotion = emotions[max_index]  \n  \n#         cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)  \n  \n#     resized_img = cv2.resize(test_img, (1000, 700))  \n#     cv2.imshow('Facial emotion analysis ',resized_img)  \n  \n  \n  \n#     if cv2.waitKey(10) == ord('q'):#wait until 'q' key is pressed  \n#         break  \n  \n# cap.release()  \n# cv2.destroyAllWindows  ","e4b90789":"# **label_map = ['Anger', 'Neutral', 'Fear', 'Happy', 'Sad', 'Surprise']**","f19c1aa6":"# **Data Set import**","a2ecde65":"# **Preprocessing **","876b32f3":"# **Library Import**"}}