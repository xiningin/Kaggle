{"cell_type":{"f385608b":"code","3031fe60":"code","9abcb335":"code","52093b76":"code","d9864f1a":"code","6eb106c5":"code","1777af85":"code","9bb07801":"code","d55c504d":"code","55024e53":"code","a87dce1f":"code","2ceebcba":"code","4d6cc4ae":"code","1b9f94dc":"code","1b343f59":"code","0ee6e010":"code","1a5f3d59":"code","2eb1782a":"code","ceff0690":"code","aaa75964":"code","3e65de9e":"markdown","1fa9fd66":"markdown","6c2c5139":"markdown","c32644e1":"markdown","201c496c":"markdown","67994a30":"markdown","c539492f":"markdown","0cf6c5ef":"markdown","4a8db29b":"markdown","0bbfe013":"markdown","05977a5f":"markdown","daa279b5":"markdown","f929c6b7":"markdown","6aefe4ad":"markdown","c971a272":"markdown","ab36c436":"markdown","e1168283":"markdown","cb5b9305":"markdown"},"source":{"f385608b":"import numpy as np \nimport pandas as pd \nimport sklearn.utils as skutils\nimport sklearn.model_selection as skmodelsel\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","3031fe60":"df = pd.read_csv('..\/input\/HousePriceData.csv')   #Training Dataset\ndf = skutils.shuffle(df)\ndfTrain, dfValid = skmodelsel.train_test_split(df, test_size=0.2)\ndfTrain.head()","9abcb335":"pd.plotting.scatter_matrix(dfTrain, alpha=1, diagonal='kde',color='r')\nplt.show()","52093b76":"#%matplotlib notebook\nplt3D = plt.figure().gca(projection='3d')\nplt3D.scatter(dfTrain['FloorArea'], dfTrain['BedRooms'], dfTrain['Price'],color=\"r\")\nplt3D.set_xlabel('FloorArea')\nplt3D.set_ylabel('BedRooms')\nplt3D.set_zlabel('Price')\nplt.show()","d9864f1a":"def extractFeatures(df):\n    df_Features=df.loc[:,['FloorArea','BedRooms']]\n    df_Label=df.loc[:,['Price']]\n    X=df_Features.values\n    Y=df_Label.values\n    return X,Y","6eb106c5":"X,Y=extractFeatures(dfTrain)","1777af85":"SMean=np.mean(X,axis=0)   \nSDev=np.std(X,axis=0)\ndef NormalizeInput(X,SMean,SDev):   \n    XNorm=(X-SMean)\/SDev\n    return XNorm","9bb07801":"XNorm=NormalizeInput(X,SMean,SDev)","d55c504d":"def mapFeature(X,degree):\n    \n    sz=X.shape[1]\n    if (sz==2):\n        sz=(degree+1)*(degree+2)\/2\n        sz=int(sz)\n    else:\n         sz=degree+1\n    out=np.ones((X.shape[0],sz))     #Adding Bias W0\n\n    sz=X.shape[1]\n    if (sz==2):\n        X1=X[:, 0:1]\n        X2=X[:, 1:2]\n        col=1\n        for i in range(1,degree+1):        \n            for j in range(0,i+1):\n                out[:,col:col+1]= np.multiply(np.power(X1,i-j),np.power(X2,j))    \n                col+=1\n        return out\n    else:\n        for i in range(1,degree+1):        \n            out[:,i:i+1]= np.power(X,i)\n    \n    return out","55024e53":"degree=3\ninputX=mapFeature(XNorm,degree)  ","a87dce1f":"batchSize=len(Y)         #no of Examples\niterations = 5000\nalpha =100000000\nbeta1=0.9\nbeta2=0.999\nlearningDecayRate=0.998\nepsilon=0.0000000001\nfeatureCount=inputX.shape[1] \nweights=np.zeros((featureCount, 1)) #initialize Weight Paramters\nvDW=np.zeros((featureCount, 1))\nsDW=np.zeros((featureCount, 1))\nlossList=np.zeros((iterations,1),dtype=float)  #for plotting loss curve","2ceebcba":"\nfor k in range(iterations):\n    #nth iteration\n    t=k+1\n    \n    #Hypothesis\n    hypothesis=np.matmul( inputX,weights)           \n    \n    #Loss\n    loss=hypothesis-Y  \n    \n    \n    #derivative\n    dW=np.matmul(inputX.T,loss)  #Derivative\n   \n    #learning Rate decrease as training progresses \n    alpha=alpha*learningDecayRate\n    \n    #gradient Update\n    vDW = (beta1) *vDW+ (1-beta1) *dW        #Momentum  \n    sDW = (beta2) *sDW+ (1-beta2) *(dW**2)   #RMSProp\n    \n    vDWc =vDW\/(1-beta1**t)\n    sDWc =sDW\/(1-beta2**t)\n    #weights=weights - (alpha\/batchSize)*dW       #normal\n    #weights=weights - (alpha\/batchSize)*vDW     #Momentum   \n    #weights=weights - (alpha\/batchSize)*dW\/np.sqrt(csDW+epsilon)     #RMSProp \n    weights=weights - (alpha\/batchSize)*(vDWc\/(np.sqrt(sDWc)+epsilon)) #Adam          \n    \n    \n    #Compute Loss for Plotting\n    newLoss=np.matmul( inputX,weights)-Y\n    newLossSqr=np.multiply(newLoss,newLoss)\n    lossList[k]=(1.0\/(2.0*batchSize))* np.sum(newLossSqr)\n\nprint(\"{0:.15f}\".format(lossList[iterations-1][0]))","4d6cc4ae":"plt.subplot(111)\nplt.plot(lossList,color='r')\nplt.xlabel('Full Plot')\nplt.show()\n\nplt.subplot(121)\nplt.plot(lossList[0:500],color='b')\nplt.xlabel('First 500 Values')\n\n\nplt.subplot(122)\nplt.plot(lossList[len(lossList)-500:len(lossList)],color='b')\nplt.xlabel('Last 500 Values')\nplt.show()","1b9f94dc":"def predict(X,weights,SMean,SDev,degree):\n    XNorm=NormalizeInput(X,SMean,SDev)\n    inputX=mapFeature(XNorm,degree)\n    PY=np.matmul(inputX, weights)\n    return PY\n","1b343f59":"def getRMSE(aY,pY):\n    Error=aY- pY\n    ErrorSqr=Error**2\n    MSE=ErrorSqr.mean()\n    RMSE=np.sqrt(MSE)\n    return RMSE","0ee6e010":"pY=predict(X, weights,SMean,SDev,degree)  # Predict with bias feature added\nprint(\"{0:.15f}\".format(getRMSE(Y, pY)))","1a5f3d59":"vX,vY=extractFeatures(dfValid)\npY=predict(vX, weights,SMean,SDev,degree)  # Predict with bias feature added\nprint(\"{0:.15f}\".format(getRMSE(vY, pY)))","2eb1782a":"pY=predict(X, weights,SMean,SDev,degree)  # Predict with bias feature added\nplt.scatter(X[:,0],Y,color=\"r\")\nplt.scatter(X[:,0],pY[:,0],color=\"b\")\nplt.xlabel(\"FloorArea\")\nplt.ylabel(\"Price\")\nplt.legend([\"Actual\",\"Predicted\"])\nplt.show()","ceff0690":"plt.close()\npY=predict(X, weights,SMean,SDev,degree)  # Predict with bias feature added\nplt.scatter(X[:,1],pY[:,0],color=\"b\")\nplt.scatter(X[:,1],Y,color=\"r\")\nplt.xlabel(\"BedRooms\")\nplt.ylabel(\"Price\")\nplt.legend([\"Actual\",\"Predicted\"])\nplt.show()","aaa75964":"%matplotlib notebook\nfig = plt.figure()\nplt3D = fig.add_subplot(111, projection='3d')   \nplt3D.scatter(X[:,0],X[:,1],Y,marker=\"o\",color=\"r\")\n\nx_min, x_max = X[:, 0].min() , X[:, 0].max() \ny_min, y_max = X[:, 1].min() , X[:, 1].max() \nu = np.linspace(x_min, x_max,20) \nv = np.linspace(y_min, y_max, 20) \nz = np.zeros(( len(u), len(v) )) \nU,V=np.meshgrid(u,v)\nfor i in range(len(u)): \n    for j in range(len(v)): \n        uv= np.column_stack((np.array([[u[i]]]),np.array([[v[j]]])))\n        pv =predict(uv, weights,SMean,SDev,degree)\n        z[i,j] =pv[0][0]\nz = np.transpose(z) \nplt3D.plot_surface(U,V,z,alpha=0.5,color='b')\nplt.show()","3e65de9e":"<h5> Training Data Prediction visualization in 2D (Floor Area vs Volume)","1fa9fd66":"<h5>RMSE on Training Data","6c2c5139":"<h5> Visualize Data","c32644e1":"<h5> Training Data Prediction visualization in 2D (BedRooms Area vs Volume)","201c496c":"<h5> RMSE on Validation Data","67994a30":"<h1> Prediction\/RMSE Evaluation","c539492f":"<h1> Training","0cf6c5ef":"<h5> Training Data Prediction in 3D","4a8db29b":"<h5> Gradient Descent Updates","0bbfe013":"<H1>Read Data from CSV","05977a5f":"<h5>Add Polynomial Features","daa279b5":"<h1>Extract Input Feature to <b>X <\/b>and Label to <b>y<\/b>","f929c6b7":"<h1>Regression with Feature Mapping","6aefe4ad":"<h5>Using Polynomial Regresion of degree 3 in two variables $x_1$,$x_2$ and y<\/h5>\n<p>\n$h(x) = w_0 + w_1 x_1+ w_2 x_2 + w_3 x_1^2 + w_4 x_1 x_2 + w_5 x_2^2 + w_6 x_2^3  + w_7 x_1 x_2^2 + w_8 x_1^2 x_2 + w_9 x_1^3 $\n  \n<\/p> ","c971a272":"<h5> Initialization","ab36c436":"<h5>Normalize Input    ","e1168283":"<h1>Plot Loss Curve","cb5b9305":"<h2> Model Final Verification"}}