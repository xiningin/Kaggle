{"cell_type":{"c2b03b79":"code","1f3f11af":"code","26eb18e3":"code","e2298aa9":"code","920bbb72":"code","77ffcfc9":"code","a6f22896":"code","09eb6022":"code","0a02248c":"code","97e2e8c4":"code","8b7eec5f":"code","27134142":"code","85dd3064":"code","7ce3e30b":"code","8ed8a354":"code","ddaba087":"code","1f9b667b":"code","1c9ac318":"code","453141d2":"code","5c5d22c5":"code","09e294a0":"code","61007519":"code","39ea477e":"code","99d34aa2":"code","9611e04b":"code","435eacb9":"code","93003311":"code","11b13be4":"code","ce1e5fc0":"code","e6148cce":"code","3b951a25":"code","6bcade2b":"code","ac027b54":"code","1f2b4693":"code","0289ffc3":"code","86b6c346":"code","99075eab":"code","7f1ef528":"code","d1a6f9db":"code","36f8d1f4":"code","87429b0f":"code","bcfddee1":"code","ac879d4b":"code","b82532cd":"code","192d0a73":"code","21c7ecad":"code","c2fdf885":"code","138688bc":"code","9c3a4f9a":"code","335b02e8":"code","5825bbe3":"code","38a2af13":"code","016e5981":"code","b705821e":"code","4986219a":"code","732678f3":"code","8dd3c185":"code","1e002b5a":"code","222ae430":"code","d4735308":"code","82ba7bf5":"code","9f0aaddb":"code","30df57bb":"code","96eeaeb0":"code","10f90153":"code","c80969ce":"code","a6ca6548":"code","576294bd":"code","7da8c10e":"code","686f5597":"code","aae43445":"code","26b9900e":"code","b4ea2caf":"code","1abf0b88":"code","0571cef9":"code","4eaba476":"code","92e03844":"code","80171926":"code","a948b04e":"code","61b5a107":"markdown","d4a933b6":"markdown","ec8a393b":"markdown","792a9c31":"markdown","8448a86c":"markdown","e524109d":"markdown","68aadd3c":"markdown","01029bff":"markdown","ba5a0ddc":"markdown","47637636":"markdown","32f3e50c":"markdown","317f121c":"markdown","cf7ede79":"markdown","b0f035cb":"markdown","f44b4b7d":"markdown","71a4d21c":"markdown","4fd012e0":"markdown","a4f297d4":"markdown","a6626795":"markdown","88194bc4":"markdown","6b719477":"markdown","6a2e7716":"markdown","e0476a56":"markdown","0e796b53":"markdown","03824344":"markdown","14428d3a":"markdown","9237b982":"markdown","5fe3cc37":"markdown","49f0fdab":"markdown","4142cf39":"markdown","5144bbf0":"markdown","59314890":"markdown","6ed7f60c":"markdown","99ee4758":"markdown","c19ad2d1":"markdown","1382cc6f":"markdown","094259dc":"markdown","43651df4":"markdown","3d6ff099":"markdown","66af509c":"markdown","988153ee":"markdown","b818a06b":"markdown","bbc7f953":"markdown","73a073e6":"markdown"},"source":{"c2b03b79":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"ticks\", rc={'figure.figsize':(9,8)})\nsns.set_context(rc = {\"font.size\":15, \"axes.labelsize\":15}, font_scale=2)\nsns.set_palette('colorblind');\nfrom pandas.api.types import CategoricalDtype\n\nfrom scipy import stats\n\nfrom scipy.stats import pearsonr,spearmanr, boxcox_normmax, chi2_contingency, chi2, f, shapiro, probplot\nfrom scipy.special import boxcox1p\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols \n\n# pandas defaults\npd.options.display.max_columns = 500\npd.options.display.max_rows = 500\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom time import time","1f3f11af":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint(\"Training data shape: \", train.shape)\nprint(\"Testing data shape: \", test.shape)","26eb18e3":"train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace = True)\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace = True)\ntrain.reset_index(drop=True, inplace=True)\ntrain.head()","e2298aa9":"train.shape","920bbb72":"sale_price_df = train[['SalePrice']].copy()","77ffcfc9":"data = pd.concat([train.drop(columns='SalePrice'), test], axis = 0)\ndata.shape","a6f22896":"round((data.isnull().sum()[data.isnull().sum()!=0]\/data.shape[0])*100,2)","09eb6022":"data.drop(columns = 'Id', inplace = True)","0a02248c":"data.drop(columns = ['LowQualFinSF',  'BsmtHalfBath', 'ScreenPorch',  'Street', 'Alley', 'Utilities', 'LandSlope', \n                     'Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond', 'BsmtCond',  'BsmtFinType2', \n                     'Heating', 'CentralAir', 'Electrical', 'Functional', 'GarageQual', 'GarageCond', 'PavedDrive', \n                     'PoolQC', 'Fence', 'MiscFeature', 'SaleType'], inplace = True) \ndata.shape","97e2e8c4":"for col in ['BsmtFullBath']:\n    data[col] = data[col].fillna(data[col].value_counts().idxmax())\n    \ndata['GarageYrBlt'] = data['GarageYrBlt'].fillna(0)\n    \nfor col in ['BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF','GarageCars','GarageArea', 'MasVnrArea', 'LotFrontage']:\n    data[col] = data[col].fillna(data[col].median())","8b7eec5f":"for col in ['MasVnrType']:\n    data[col] = data[col].fillna(data[col].value_counts().idxmax())\n    \nfor col in ['GarageType', 'GarageFinish', 'BsmtQual', 'BsmtExposure',  'BsmtFinType1', 'FireplaceQu']:\n    data[col] = data[col].fillna('None')\n    \ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].value_counts().index[0])\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].value_counts().index[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].value_counts().index[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].value_counts().index[0])","27134142":"data.isnull().sum()[data.isnull().sum()!=0]","85dd3064":"data['House_Qual'] = data['OverallQual'] + data['OverallCond']\ndata['Total_bathrooms'] = data['BsmtFullBath'] + data['FullBath'] + data['HalfBath']\ndata['Total_basement_SF'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['TotalBsmtSF']\ndata['Total_sqr_footage'] = data['BsmtFinSF1'] + data['BsmtFinSF2'] + data['1stFlrSF'] + data['2ndFlrSF']","7ce3e30b":"data['MSSubClass'] = data['MSSubClass'].astype(str)\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","8ed8a354":"data.drop(columns = ['MiscVal', 'PoolArea', '3SsnPorch', 'EnclosedPorch', 'BsmtFinSF2', 'KitchenQual'], inplace = True)","ddaba087":"cat_type = CategoricalDtype(['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered=True)\nfor col in ['BsmtQual', 'FireplaceQu']:\n    data[col] = data[col].astype(cat_type)\n    \ncat_type = CategoricalDtype(['IR3', 'IR2', 'IR1', 'Reg'], ordered=True)\ndata['LotShape'] = data['LotShape'].astype(cat_type)\n\ncat_type = CategoricalDtype(['None', 'No', 'Mn', 'Av', 'Gd'], ordered=True)\ndata['BsmtExposure'] = data['BsmtExposure'].astype(cat_type)\n\ncat_type = CategoricalDtype(['None', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'], ordered=True)\nfor col in ['BsmtFinType1']:\n    data[col] = data[col].astype(cat_type)","1f9b667b":"cat_type = CategoricalDtype(['Low', 'HLS', 'Bnk', 'Lvl'], ordered=True)\ndata['LandContour'] = data['LandContour'].astype(cat_type)\n\ncat_type = CategoricalDtype(['Po', 'Fa', 'TA', 'Gd', 'Ex'], ordered=True)\nfor col in ['ExterQual']:\n    data[col] = data[col].astype(cat_type)\n    \ncat_type = CategoricalDtype(['None', 'Detchd', 'CarPort', 'BuiltIn', 'Basment', 'Attchd', '2Types'],ordered=True)\ndata['GarageType'] = data['GarageType'].astype(cat_type)\n\ncat_type = CategoricalDtype(['None', 'Unf', 'RFn', 'Fin'],ordered=True)\ndata['GarageFinish'] = data['GarageFinish'].astype(cat_type)\n","1c9ac318":"data['Has_Garage'] = np.where(data['GarageArea']>0,1,0)\ndata['Has_basement'] = np.where(data['Total_basement_SF'] > 0, 1, 0)\ndata['Has_fireplace'] = np.where(data['Fireplaces']>0,1,0)","453141d2":"for col in data.filter(regex = '^Has', axis = 'columns').columns:\n    print(data[col].value_counts(normalize = True))","5c5d22c5":"data.drop(columns = ['Has_Garage', 'Has_basement'], inplace = True)\ndata.shape","09e294a0":"sns.distplot(sale_price_df['SalePrice']);\nplt.title(\"Skew: {} | Kurtosis: {}\".format(sale_price_df['SalePrice'].skew(), sale_price_df['SalePrice'].kurt()));","61007519":"numeric_columns = data.select_dtypes('number').columns\nnumeric_columns = numeric_columns[~numeric_columns.str.contains('^Has')]\nnumeric_columns","39ea477e":"X_train = data.iloc[:len(train),:].copy()\nX_test = data.iloc[len(train):,:].copy()\nX_train = pd.concat([X_train,sale_price_df['SalePrice']], axis = 1)","99d34aa2":"def generate_heatmap(df):\n    # Generate a heatmap with the upper triangular matrix masked\n    # Compute the correlation matrix\n    corr = df.corr(method=\"spearman\")\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n    plt.figure(figsize = (15,9));\n    # Draw the heatmap with the mask \n    sns.heatmap(corr, mask=mask, cmap='coolwarm', fmt = '.2f', linewidths=.5, annot = True);\n    plt.title(\"Correlation heatmap\");\n    return","9611e04b":"import math\ncorrelation_results_list = []\nncols = 3\nnrows = math.ceil(len(numeric_columns)\/ncols)\nfig, axes = plt.subplots(nrows,ncols, figsize=(ncols*4.5,nrows*3))\naxes_list = [item for sublist in axes for item in sublist] \nfor col in numeric_columns:\n    ax = axes_list.pop(0) # Take the first axes of the axes_list\n    sns.regplot(X_train[col], X_train['SalePrice'], ax = ax)\n    stp = spearmanr(X_train[col], X_train['SalePrice'])\n    str_title = \"r = \" + \"{0:.2f}\".format(stp[0])\n    ax.set_title(str_title,fontsize=11)\n    correlation_results_list.append((col, abs(stp[0])))\n    \nplt.tight_layout(); \nplt.show();","435eacb9":"correlation_df = pd.DataFrame(correlation_results_list, columns = ['column_name', 'correlation'])\ncorrelation_df.sort_values(by = 'correlation', ascending = False, inplace = True)\ncorrelation_df.head()","93003311":"columns_with_high_corr = correlation_df.loc[correlation_df['correlation']>=0.6, 'column_name'].to_list()\ncolumns_with_high_corr","11b13be4":"columns_with_low_corr = correlation_df[correlation_df['correlation']<0.6]['column_name'].to_list()","ce1e5fc0":"data.drop(columns = columns_with_low_corr, inplace = True)\nX_train.drop(columns = columns_with_low_corr, inplace = True)","e6148cce":"generate_heatmap(X_train[columns_with_high_corr+['SalePrice']])","3b951a25":"data.drop(columns = ['House_Qual', 'GarageArea', 'GarageYrBlt', 'Total_sqr_footage', 'YearBuilt'], inplace = True)\nX_train.drop(columns = ['House_Qual', 'GarageArea', 'GarageYrBlt', 'Total_sqr_footage', 'YearBuilt'], inplace = True)","6bcade2b":"category_cols = X_train.select_dtypes(['object', 'category', 'int32']).columns\ncategory_cols","ac027b54":"X_train['SalePrice'] = np.log1p(X_train['SalePrice'])","1f2b4693":"categ_columns_with_high_association = []\ncateg_columns_with_low_association = []\ndef perform_anova_and_its_results(categ_col, num_col='SalePrice', df = X_train):\n    df_sst = len(df[num_col])-1\n    df_ssb = df[categ_col].nunique() - 1\n    df_ssw = df_sst - df_ssb\n    F_critical = f.ppf(0.95, df_ssb, df_ssw)\n#     print(\"F_Critical: {0:.3f}\".format(F_critical))\n    results = ols('{} ~{}'.format(num_col, categ_col), data = df).fit()\n    aov_table = sm.stats.anova_lm(results, typ = 1)  \n    F_stat = aov_table.loc[categ_col, 'F']\n#     print(\"F_statistic: {0:.3f}\".format(F_stat))\n    if (F_stat > F_critical):\n#         print(\"F-statistic is more than F-critical\")\n#         print(\"There is an association between {} and {}\".format(categ_col,num_col))\n        categ_columns_with_high_association.append(categ_col)\n    else:\n#         print(\"F-statistic is less than F-critical\")\n#         print(\"There is no association between {} and {}\".format(categ_col,num_col))\n        categ_columns_with_low_association.append(categ_col)\n#     print('-'*30)","0289ffc3":"for col in category_cols:\n    perform_anova_and_its_results(col)","86b6c346":"categ_columns_with_low_association","99075eab":"data.drop(columns = categ_columns_with_low_association, inplace = True)\nX_train.drop(columns = categ_columns_with_low_association, inplace = True)","7f1ef528":"def phi_coefficient(a,b):\n    temp = pd.crosstab(a,b)\n    nr = (temp.iloc[1,1] * temp.iloc[0,0]) - (temp.iloc[0,1]*temp.iloc[1,0])\n    dr = np.sqrt(np.product(temp.apply(sum, axis = 'index')) * np.prod(temp.apply(sum, axis = 'columns')))\n    return(nr\/dr)","d1a6f9db":"cat_binary_cols = []\nfor col in X_train.select_dtypes(['int32', 'object']).columns:\n    if (X_train[col].nunique()==2):\n        cat_binary_cols.append(col)\n        \ncat_binary_cols","36f8d1f4":"def cramers_v(a,b):\n    crosstab = pd.crosstab(a,b)\n    chi2 = chi2_contingency(crosstab)[0]  # chi-squared value\n    n = crosstab.sum().sum()\n    phi2 = chi2\/n\n    r, k = crosstab.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return(np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1))))\n\n\nfrom collections import Counter\ndef conditional_entropy(x, y):\n    \"\"\"\n    Calculates the conditional entropy of x given y: S(x|y)\n    Wikipedia: https:\/\/en.wikipedia.org\/wiki\/Conditional_entropy\n    :param x: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of measurements\n    :param y: list \/ NumPy ndarray \/ Pandas Series\n        A sequence of measurements\n    :return: float\n    \"\"\"\n    # entropy of x given y\n    y_counter = Counter(y)\n    xy_counter = Counter(list(zip(x,y)))\n    total_occurrences = sum(y_counter.values())\n    entropy = 0.0\n    for xy in xy_counter.keys():\n        p_xy = xy_counter[xy] \/ total_occurrences\n        p_y = y_counter[xy[1]] \/ total_occurrences\n        entropy += p_xy * math.log(p_y\/p_xy)\n    return entropy\n\n\ndef theils_u(x, y):\n    s_xy = conditional_entropy(x,y)\n    x_counter = Counter(x)\n    total_occurrences = sum(x_counter.values())\n    p_x = list(map(lambda n: n\/total_occurrences, x_counter.values()))\n    s_x = stats.entropy(p_x)\n    if s_x == 0:\n        return(1)\n    else:\n        return((s_x - s_xy)\/s_x)","87429b0f":"category_cols = X_train.select_dtypes(['object', 'category', 'int32']).columns.to_list()\ncategory_cols","bcfddee1":"temp = pd.DataFrame(columns=category_cols, index=category_cols).fillna(0)\ncat_col_correlation = []\nfor row in category_cols:\n    a = row\n    for col in category_cols:\n        b = col\n        temp.loc[a,b] = theils_u(X_train[a],X_train[b])\n        temp.loc[b,a] = temp.loc[a,b]\n        cat_col_correlation.append((a,b,temp.loc[a,b]))","ac879d4b":"cat_cols_corr_df = pd.DataFrame(cat_col_correlation, columns = ['col1', 'col2', 'correlation'])\ncat_cols_corr_df.head()","b82532cd":"cat_cols_corr_df.shape","192d0a73":"high_corr_cat=cat_cols_corr_df[(cat_cols_corr_df['correlation']>=0.6) & (cat_cols_corr_df['col1']!=cat_cols_corr_df['col2'])]\nhigh_corr_cat.head()","21c7ecad":"high_corr_cat","c2fdf885":"data.drop(columns = ['Has_fireplace', 'Exterior1st', 'MSSubClass', 'MSZoning'], inplace = True)\nX_train.drop(columns = ['Has_fireplace', 'Exterior1st', 'MSSubClass', 'MSZoning'], inplace = True)","138688bc":"t = X_train.select_dtypes(['int64', 'float64']).columns.tolist()\nt.remove('SalePrice')\nt","9c3a4f9a":"X_train.select_dtypes(['int64', 'float64']).columns","335b02e8":"nrows = 2\nncols = 3\nfig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4.5,nrows*3))\naxes_list = [item for sublist in axes for item in sublist]\nfor col in t:\n    ax = axes_list.pop(0) # Take the first axes of the axes_list\n    probplot(X_train[col], dist = 'norm', plot = ax)\n    str_title = \"QQ-plot of {}\".format(col)\n    ax.set_title(str_title,fontsize=11)\n\n    \nplt.tight_layout(); \nplt.show();\n\n# Now use the matplotlib .remove() method to \n# delete anything we didn't use\nfor ax in axes_list:\n    ax.remove()","5825bbe3":"for col in t:\n    X_train[col] = boxcox1p(X_train[col], boxcox_normmax(X_train[col]+1))\n    data[col] = boxcox1p(data[col], boxcox_normmax(data[col]+1))","38a2af13":"nrows = 2\nncols = 3\nfig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4.5,nrows*3))\naxes_list = [item for sublist in axes for item in sublist]\nfor col in t:\n    ax = axes_list.pop(0) # Take the first axes of the axes_list\n    probplot(X_train[col], dist = 'norm', plot = ax)\n    str_title = \"QQ-plot of {}\".format(col)\n    ax.set_title(str_title,fontsize=11)\n\n    \nplt.tight_layout(); \nplt.show();\n\n# Now use the matplotlib .remove() method to \n# delete anything we didn't use\nfor ax in axes_list:\n    ax.remove()","016e5981":"nominal_cols = X_train.select_dtypes('object').columns.to_list()\nnominal_cols","b705821e":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer, OneHotEncoder,OrdinalEncoder, RobustScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, KFold\nfrom sklearn.pipeline import Pipeline","4986219a":"train = data.iloc[:len(X_train),:].copy()\ntrain['SalePrice'] = X_train['SalePrice'].copy()\ntest = data.iloc[len(X_train):,:].copy()","732678f3":"print(\"Train shape: \", train.shape)\nprint(\"Test shape: \", test.shape)","8dd3c185":"X = train.loc[:,train.columns!='SalePrice']\ny = train['SalePrice'].copy()\nX.shape","1e002b5a":"ohe_dict_cols = {}\nfor col in X.select_dtypes(include=['object']).dtypes.index:\n    ohe_dict_cols[col] = pd.Series(X[col].unique()).to_list()\n    \n# For one-hot encoder\nt_k = []  # nominal column names\nt_v = []  # values of nominal columns\nfor k,v in ohe_dict_cols.items():\n    t_k.append(k)\n    t_v.append(v)","222ae430":"ord_encod_dict = {}\nfor col in X.select_dtypes(include='category').columns:\n    ord_encod_dict[col] = pd.Series(data[col].unique().sort_values()).to_list()\n\n# For ordinal encoder\nordinal_cols = []\nordinal_vals = []\nfor k,v in ord_encod_dict.items():\n    ordinal_cols.append(k)\n    ordinal_vals.append(v)","d4735308":"number_cols = X.select_dtypes('number').columns.tolist()\nnumber_cols","82ba7bf5":"# Setup cross validation folds\nkf = KFold(n_splits = 10, random_state=1, shuffle=True)","9f0aaddb":"import xgboost as xgb\n\nxg_reg = xgb.XGBRegressor(objective ='reg:squarederror', random_state=1, n_jobs = -1, max_depth = 3, min_child_weight = 5,\n                       alpha = 1e-5, gamma = 0, subsample=0.8, colsample_bytree=0.7, learning_rate =0.1, n_estimators = 400)\n\ncolT = ColumnTransformer([\n    ('dummy_col', OneHotEncoder(drop = 'first', categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\n\nxg_pipeline = Pipeline(steps = [('colt', colT), (\"xg\", xg_reg)])\nxg_pipeline.fit(X, y)\n\n# prediction_XG_1 = xg_pipeline.predict(test)\n\nscores = -cross_val_score(xg_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","30df57bb":"from sklearn.ensemble import RandomForestRegressor\nrfreg = RandomForestRegressor(random_state=1, n_jobs = -1, max_depth =  8, max_samples = None, min_samples_leaf =  2, \n                              n_estimators = 120)\n\ncolT = ColumnTransformer([\n    ('dummy_col', OneHotEncoder(drop = 'first', categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\nrf_pipeline = Pipeline(steps = [('colt', colT), (\"rf\", rfreg)])\nrf_pipeline.fit(X, y)\n\nscores = -cross_val_score(rf_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","96eeaeb0":"from sklearn.linear_model import Ridge\nridge_reg = Ridge(random_state = 1, alpha = 25)\n\nrb_scaler = RobustScaler(with_centering=False)\n\ncolT = ColumnTransformer([\n    ('dummy_col', OneHotEncoder(drop = 'first', categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\n# skb = SelectKBest(f_regression)\n\nridge_pipeline = Pipeline(steps = [('colt', colT), ('rb', rb_scaler), (\"ridge\", ridge_reg)])\nridge_pipeline.fit(X, y)\n\nscores = -cross_val_score(ridge_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","10f90153":"from sklearn.linear_model import Lasso\nlasso_reg = Lasso(random_state = 1, alpha = 0.0005)\n\nrb_scaler = RobustScaler(with_centering=False)\n\ncolT = ColumnTransformer([\n    ('dummy_col', OneHotEncoder(drop = 'first', categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\n# Bigger the alpha values the lesser the features that will be selected\n\nlasso_pipeline = Pipeline(steps = [('colt', colT),  ('rb', rb_scaler), (\"lasso\", lasso_reg)])\nlasso_pipeline.fit(X, y)\n\nscores = -cross_val_score(lasso_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","c80969ce":"from sklearn.linear_model import ElasticNet\nelastic_reg = ElasticNet(random_state = 1, l1_ratio = 0.095, alpha = 0.0005)\n\nrb_scaler = RobustScaler(with_centering=False)\n\ncolT = ColumnTransformer([\n    ('dummy_col', OneHotEncoder(drop = 'first', categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\n# Bigger the alpha values the lesser the features that will be selected\n\nelastic_pipeline = Pipeline(steps = [('colt', colT),  ('rb', rb_scaler), (\"elastic\", elastic_reg)])\nelastic_pipeline.fit(X, y)\n\nscores = -cross_val_score(elastic_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","a6ca6548":"from catboost import CatBoostRegressor\ncat_obj_cols = X.select_dtypes(['category', 'object']).columns.to_list()","576294bd":"from sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1)\n\ncat = CatBoostRegressor(iterations=100, depth=6, learning_rate=0.1, eval_metric = 'RMSE', loss_function='RMSE',\n                       l2_leaf_reg = 1)","7da8c10e":"cat.fit(X_train, y_train, cat_features = cat_obj_cols, eval_set= (X_validation, y_validation), plot=True);","686f5597":"import lightgbm as lgb\nfrom lightgbm import LGBMRegressor\n\nlgbm = LGBMRegressor(boosting_type = 'gbdt' ,random_state=1, n_jobs=-1, learning_rate = 0.1, max_depth = 3, \n                     min_child_weight = 0.001, min_split_gain = 0, n_estimators = 250, reg_alpha = 0.01)\n\n\ncolT = ColumnTransformer([\n    ('dummy_col',OneHotEncoder(drop = 'first',categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\n\nlgbm_pipeline = Pipeline(steps = [('colt', colT), (\"lgbm\", lgbm)])\nlgbm_pipeline.fit(X, y)\n\nscores = -cross_val_score(lgbm_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","aae43445":"from mlxtend.regressor import StackingCVRegressor\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(lasso_pipeline['lasso'], ridge_pipeline['ridge'], xg_pipeline['xg'],\n                                            lgbm_pipeline['lgbm'], elastic_pipeline['elastic']),\n                                meta_regressor = xg_pipeline['xg'], use_features_in_secondary=True)\n\n\ncolT = ColumnTransformer([\n    ('dummy_col',OneHotEncoder(drop = 'first',categories = t_v), t_k),\n    ('ordinal_cols', OrdinalEncoder(categories = ordinal_vals), ordinal_cols)\n], remainder = 'passthrough')\n\n\nstack_gen_pipeline = Pipeline(steps = [('colt', colT), (\"stack_gen\", stack_gen)])\nstack_gen_pipeline.fit(X, y)\n\nscores = -cross_val_score(stack_gen_pipeline, X, y, cv = kf, n_jobs = -1,scoring = 'neg_root_mean_squared_error')\nprint('RMSE scores: ', scores)\nprint('RMSE scores mean: {:.3f}'.format(scores.mean()))\nprint('RMSE scores std deviation: {:.3f}'.format(scores.std()))","26b9900e":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(X):\n    return ((0.05 * lasso_pipeline.predict(X)) + \\\n            (0.05 * ridge_pipeline.predict(X)) + \\\n            (0.5 * xg_pipeline.predict(X)) + \\\n            (0.1 * lgbm_pipeline.predict(X)) + (0.3 * stack_gen_pipeline.predict(X)))","b4ea2caf":"# Define error metrics\nfrom sklearn.metrics import mean_squared_error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1abf0b88":"# Get final precitions from the blended model\nblended_score = rmsle(y, blended_predictions(X))\nprint('RMSE blended : {:.3f}'.format(blended_score))","0571cef9":"predictions_blended = blended_predictions(test)","4eaba476":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","92e03844":"sub_pred = np.expm1(predictions_blended)","80171926":"df_output = pd.DataFrame()\ndf_output['Id'] = test['Id']\ndf_output['SalePrice'] = sub_pred\ndf_output.head()","a948b04e":"df_output.to_csv('submission.csv', index = False)","61b5a107":"Converting other numerical columns to normal","d4a933b6":"### Dropping ID column","ec8a393b":"### Use heatmap to check for multicorrelation","792a9c31":"## Modelling","8448a86c":"#### Columns on which Ordinal Encoder will be applied","e524109d":"## Removing outliers","68aadd3c":"Filling NA values of numerical columns","01029bff":"To get our original SalePrice values back, we will apply `np.expm1` at the end of the study to cancel the `log1p` transformation after training and testing the models.","ba5a0ddc":"> Eliminating features with low variance and zero variance. Zero variance features are comprised of the same values. Low variance features arise from features with most values the same and with few unique values. One way low variance features can arise, is from dummy variables for categories with very few members. The dummy variable will be mostly 0s with very few 1s.\n\nFirst thing to do is get rid of the features with more than 80% missing values (figure below). For example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 80%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are not informative for machine learning models. So we drop the features with more than 80% missing values or 80% same object\/category value.\n\n### Dropping columns\n\nColumns to be dropped because of very low varaiance:","47637636":"### rbinding train and test data","32f3e50c":"## 1. XGBoost model","317f121c":"Checking whether the columns are normally distributed","cf7ede79":"Majority of the columns starting with __Has__ has a low variance so dropping those columns.","b0f035cb":"#### Columns on which one hot encoder will be applied\n\n","f44b4b7d":"### Check correlation of predictor variables with target variables","71a4d21c":"### Dealing with all object\/category type columns\nConverting object columns to ordinal category type","4fd012e0":"Dropping columns which show multicolinearity","a4f297d4":"## Loading Dataset","a6626795":"## 9. StackingCV Regressor","88194bc4":"### Submission","6b719477":"#### Association between 2 nominal binary variables\nCan be used to avoid multicolinearity","6a2e7716":"### Dropping columns having low correlation with SalePrice","e0476a56":"Filling NA values of object\/categorical columns","0e796b53":"Dropping columns to avoid multicolinearity\n- OverallQual with __House_Qual__ \n- YearBuilt with __GarageYrBlt__\n- GrLivArea with __Total_sqr_footage__ \n- GarageCars with __GarageArea__\n\n- GarageCars with __GarageArea and GarageYrBlt__\n\n### Dropping columns to avoid multicolinearity","03824344":"### 4. Lasso Regression","14428d3a":"### Create new features","9237b982":"Dropping numeric columns having low correlation","5fe3cc37":"### checking for presence of null","49f0fdab":"#### Nominal columns","4142cf39":"#### Dropping columns","5144bbf0":"#### Applying log(1+x) transform on SalePrice","59314890":"So no object columns having 2 unique values so no need of finding phi_coefficent.","6ed7f60c":"### 3. Ridge regression\n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","99ee4758":"#### EDA - Category and Category analysis","c19ad2d1":"## 10. Blend models and get predictions","1382cc6f":"Here given more weightage to the model which has a lower(better) RMSE value.","094259dc":"## 8. light GBM","43651df4":"So NA values of all columns are filled.\n\n### Creating new features","3d6ff099":"> ### 7. Catboost","66af509c":"### Nominal Categorical and continuous (target) variable","988153ee":"Checking the normality of the columns again","b818a06b":"## 2. Random Forest Regressor ","bbc7f953":"#### Some of the non-numeric predictors are stored as numbers; convert them into strings ","73a073e6":"### 5. Elastic net regression"}}