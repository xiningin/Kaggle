{"cell_type":{"70034096":"code","a1bd2cb8":"code","caff7aad":"code","26b05ddc":"code","3749a1e4":"code","c0a49fc8":"code","f9933ed0":"code","9557919b":"code","125a0a86":"code","a449c19c":"code","216ffa32":"code","23d7b898":"code","3f443422":"code","ad142fa4":"code","b662ae65":"code","4da98aa1":"code","b950fa6b":"code","8c9494ce":"code","c0c558d4":"code","691b09df":"code","276e5c83":"code","53550d0c":"code","f95ef0c3":"code","a946b519":"code","1923483e":"code","51c67693":"code","0f5ec717":"markdown","65e5bec5":"markdown","b5c56292":"markdown","4b9d577e":"markdown","dcda3ead":"markdown","73d03323":"markdown","9a7927fd":"markdown","22f6f24b":"markdown","126933bc":"markdown"},"source":{"70034096":"%config Completer.use_jedi = False","a1bd2cb8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","caff7aad":"filepath =\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\nheart_features = pd.read_csv(filepath)\nheart_features.head(3)","26b05ddc":"heart_features.describe()","3749a1e4":"heart_features.info()","c0a49fc8":"heart_features['time'].plot()","f9933ed0":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nplt.style.use('ggplot') # default plot style.\n\nfrom scipy import stats\nfrom scipy.stats import norm\nnumeric_columns = ['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure',\n                  'platelets','serum_creatinine','serum_sodium','sex',\n                  'smoking','time','DEATH_EVENT']","9557919b":"corr_data = heart_features.loc[:, numeric_columns].corr()\n\nplt.figure(figsize=(20,12))\nsns.heatmap(corr_data, annot=True, fmt='.3f',cmap='coolwarm',square=True)\nplt.show()","125a0a86":"heart_features['vulnerability'] = heart_features['age']\/heart_features['time']\n","a449c19c":"heart_features['stress'] = (heart_features['smoking']+\n                            heart_features['serum_creatinine']+\n                            heart_features['high_blood_pressure'])\/heart_features['time']\n","216ffa32":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nmy_pipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('reducer', PCA(n_components=2)),\n])\n\nhf_red = my_pipe.fit_transform(heart_features.drop(['DEATH_EVENT'],axis=1))","23d7b898":"hf_full = pd.concat([pd.DataFrame(hf_red),heart_features], axis=1)","3f443422":"X = hf_full.drop(['DEATH_EVENT'], axis=1)\ny = hf_full[[\"DEATH_EVENT\"]]","ad142fa4":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, shuffle=True, random_state=42)","b662ae65":"unique, counts = np.unique(y_train, return_counts=True)\nprint (np.asarray((unique, counts)).T)","4da98aa1":"prop=counts[0]\/counts[1] #sin oversample\nprop","b950fa6b":"from sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nclf = HistGradientBoostingClassifier().fit(X_train, y_train)\nclf.score(X_test, y_test)","8c9494ce":"from xgboost import XGBClassifier\nimport xgboost as xgb\ndtrain = xgb.DMatrix(\n        X_train,\n        y_train    )\n\ndtest = xgb.DMatrix(\n        X_test,\n        y_test    )","c0c558d4":"\ntrained_model = xgb.train(\n                        {\n                          'eta': 0.1,#0.1 learning rate 0.01 clasico               \n                          'colsample_bytree' : 1, #? 0.1 best 0.8 previus, 0.5 better than 0.8, 0.1 worst roc,1 best ROC\n                          'sample_type': 'weighted',\n                          'min_child_weight':1,#1 is the default\n                          'max_delta_step':1,#0 for imbalanced data, [1,10], 1 is the best\n                          'max_depth': 20,#10-precavido, 30 m\u00e1s auc con 5 tambien fue bueno, 0 FN y 32409, a mayor profundidad mejor va siendo el modelo 50 max\n                          'subsample': 0.8,#0.8\n                          'objective': 'binary:logistic',#classificator\n                          'n_estimators':1,#10,100*,1000 es lo mismo\n                          'scale_pos_weight':174.85470085470087,#prop entre label:1 y label:0\n                          'num_parallel_tree':2,#1 lo traje de los 2 en paralelo, 2 fue mejor FP,15 ha sido el mejor, ya 30 empeora\n                          'gamma': 10,#20\n                          'alpha' : 20,#10 velocity\n                          'lambda': 50,#50 overfitting L2 regularization\n                          'silent': True,\n                          'verbose_eval': False,\n                          'tree_method':'hist',#auto? hist is very good  \n                          'grow_policy':'depthwise',#default depthwise, only aviable with hist tree\n                          'max_bin':200   #default 256,more, better splits, worst computing time             \n                        },\n                        dtrain,\n                        num_boost_round=100, evals=[(dtrain, 'train'),(dtest,'test')])#,early_stopping_rounds=120)#100-140   early stopping = 10% total epochs(50\/500)\n                          #1000","691b09df":"prediction = trained_model.predict(dtest)","276e5c83":"from xgboost import plot_importance, plot_tree\n\nplot_importance(trained_model, max_num_features=12)","53550d0c":"from sklearn.metrics import roc_curve\nfrom matplotlib import pyplot\n\nfpr, tpr, thresholds = roc_curve(y_test, prediction)\n# calculate the g-mean for each threshold\ngmeans = np.sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = np.argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n# plot the roc curve for the model\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker='.', label='XGBoost')\npyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n# show the plot\npyplot.show()","f95ef0c3":"def threshold(predictions,th):\n  \"\"\"\n  predictions: array con los valores de las predicciones de clases, no booleano\n  th: threshold desde el cual decimos si la predicci\u00f3n es 1 o es 0\n  Esta funci\u00f3n nos permite ajustar el threshold de las predicciones para hacer el modelo m\u00e1s relajado o precavido\n  \"\"\"\n  pred =np.zeros(len(predictions))\n  for i in range(len(predictions)):\n    if (predictions[i]<=th):\n      pred[i]=0\n    else:\n      pred[i]=1\n  return pred\npredictions = threshold(prediction,thresholds[ix])","a946b519":"from sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_test, predictions)\nprint(\"Auc en el test : \", auc) #0.719, con pca2 y f 0.805","1923483e":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, predictions)\nprint(\"Accuracy on the test: \", acc)#0.727, con pca 2 y feature 0.808","51c67693":"from sklearn.metrics import confusion_matrix \nconfusion_matrix(y_test, predictions)","0f5ec717":"### XGBoost\n","65e5bec5":"Features, statistics and correlations.","b5c56292":"### EDA","4b9d577e":"### Modelling","dcda3ead":"### Feature Engineering","73d03323":"### Metrics","9a7927fd":"### Threshold selection","22f6f24b":"Find the best threshold.","126933bc":"This is the most important part, here we use the PCA, create the *vulnerability* and the *stress* feature. With this the model improves from 74% to 81%."}}