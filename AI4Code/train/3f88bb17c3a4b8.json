{"cell_type":{"95a81efe":"code","ab33e49a":"code","e9864863":"code","efbcf4b5":"code","02435b37":"code","28ddad17":"code","b5ea973e":"code","3b77ace5":"code","a3753fb8":"code","c1654989":"code","4327cd79":"markdown"},"source":{"95a81efe":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport string\nimport collections\n \nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.manifold import MDS\n\nimport sklearn\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.semi_supervised import LabelSpreading\nfrom sklearn.metrics import classification_report\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import confusion_matrix\n\nimport scipy as sc\nfrom scipy.cluster.hierarchy import fcluster\nfrom scipy.cluster.hierarchy import ward, dendrogram\n\nimport gensim\nfrom gensim import corpora\nfrom gensim.models.word2vec import Word2Vec\nfrom gensim.models import KeyedVectors\n\n# libraries for visualization\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n\nfrom itertools import chain\n\nimport json\n\n%matplotlib inline","ab33e49a":"df = pd.read_csv('..\/input\/tales_data_set.csv')","e9864863":"df","efbcf4b5":"df_train = df.copy()","02435b37":"for i in range(len(df_train)):\n    # leave approximately 80% for training\n    if np.random.random() > 0.8:\n        df_train.loc[i, 'Label'] = None","28ddad17":"punctuation = string.punctuation\n\ndef process_text(text):\n    \"\"\"Remove punctuation, lower text, tokenize text and stem tokens (words).\n    Args:\n      text: a string.\n    Returns:\n      Tokenized text i.e. list of stemmed words. \n    \"\"\"\n    \n    # replace all punctuation by blanc spaces\n    trantab = str.maketrans(punctuation, ' '*len(punctuation))\n    text = text.translate(trantab)\n    \n    # lower text\n    text = text.lower()\n    \n    # tokenize text\n    tokens = word_tokenize(text) \n  \n    # remove stop words\n    # filtered_text = [w for w in word_tokens if not w in stopwords] \n        \n    # stemm text\n    # stemmer = PorterStemmer()\n    # tokens = [stemmer.stem(t) for t in filtered_text]\n\n    return tokens","b5ea973e":"def define_vectorizer(add_stop_words = [], max_df=0.5, min_df=0.1, ngram_range=(1, 1)):\n    \"\"\"Transform texts to Tf-Idf coordinates.\n    Args:\n      add_stop_words: addititional stop_words, list of strings.\n      ngram_range: tuple (min_n, max_n) (default=(1, 1))\n        The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n        All values of n such that min_n <= n <= max_n will be used.\n      max_df: float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency strictly higher than\n        the given threshold (corpus-specific stop words).\n        If float, the parameter represents a proportion of documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n      min_df: float in range [0.0, 1.0] or int\n        When building the vocabulary ignore terms that have a document frequency strictly lower than\n        the given threshold. This value is also called cut-off in the literature.\n        If float, the parameter represents a proportion of documents, integer absolute counts.\n        This parameter is ignored if vocabulary is not None.\n    Returns:\n      Vectorizer. \n    \"\"\"\n    vectorizer = TfidfVectorizer(tokenizer=process_text,\n                             #stop_words=stopwords.words('english') + add_stop_words,\n                             max_df=max_df,\n                             min_df=min_df,\n                             ngram_range=ngram_range)#,\n                             #lowercase=True)\n    return vectorizer\n\ndef compute_tfidf_matrix(corpus, vectorizer):\n    \"\"\"Transform texts to Tf-Idf coordinates.\n    Args:\n      corpus: list of strings.\n      vectorizer: sklearn TfidfVectorizer.\n    Returns:\n      A sparse matrix in Compressed Sparse Row format with tf-idf scores \n    Raises:\n      ValueError: If `corpus` generates empty vocabulary or after pruning no terms remain.\n    \"\"\"\n    tfidf_matrix = vectorizer.fit_transform(corpus) # raises ValueError if bad corpus\n    return tfidf_matrix\n\ndef corpus_features(df, field):\n    corpus = df[field].values\n    vectorizer = define_vectorizer()\n    tfidf_matrix = compute_tfidf_matrix(corpus = corpus, vectorizer = vectorizer)\n    dist = 1 - cosine_similarity(tfidf_matrix)\n    return vectorizer, tfidf_matrix, dist","3b77ace5":"def auto_fill_in(df, field = 'Label'):\n    \"\"\"\n    df - data frame\n    \"\"\"\n    assert field in df.columns, field+' is not in df columns'\n    df_ri = df.reset_index(drop=True)\n    train_ind = df_ri.loc[~df_ri[field].isna()].index\n    target_ind = df_ri.loc[df_ri[field].isna()].index\n    assert len(train_ind) > 0, 'There is no labeled data in df'\n    assert len(target_ind) > 0, 'There is no unlabeled data in df'\n    train = df_ri.iloc[train_ind]\n    target = df_ri.iloc[target_ind]\n    \n    texts = df_ri['Tale'].apply(lambda x: x.lower().replace('\\n', ''))\n    vectorizer = define_vectorizer()\n    X = compute_tfidf_matrix(texts.values, vectorizer)\n    \n    X_train = X[train_ind]\n    X_target = X[target_ind]\n    \n    #y_train = train['cluster'].values\n    #y_train = y_train.astype('int')\n    le = LabelEncoder()\n    y_train = le.fit_transform(train[field].values)\n        \n    KNN = KNeighborsClassifier(n_neighbors=7) # any n_neighbors is fine\n    KNN.fit(X_train, y_train) \n    \n    y_target = KNN.predict(X_target)\n    y_cluster = le.inverse_transform(y_target)\n    \n    df_ri.loc[target_ind, field] = y_cluster\n    \n    return df_ri","a3753fb8":"df_auto_fill_in = auto_fill_in(df_train)","c1654989":"confusion_matrix(df['Label'], df_auto_fill_in['Label'])","4327cd79":"Please check out my tutorial on clustering on another Kaggle dataset https:\/\/www.kaggle.com\/zigger\/clustering-songs-by-artists.  \nHere I use a part of it to build a classifier of adult and child tales (with almost no preprocessing)."}}