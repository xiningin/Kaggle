{"cell_type":{"1b6e4a16":"code","2709f1bc":"code","f4b85ca0":"code","aea2bc31":"code","075f4067":"code","d87aaa36":"code","2f7794ea":"code","4a507fbb":"code","f96ff012":"code","ab2d7939":"code","e7292cac":"code","28403b69":"code","3541682b":"code","e8e2e4b2":"code","312c456f":"code","42d98a69":"code","a3ff7627":"code","5d77fece":"code","cf392561":"code","fd467c25":"code","5497a3fb":"code","51937528":"code","a4a17ca9":"code","d252a035":"code","19750256":"code","8ae85f2b":"code","0bbc7bb6":"code","cee7bd35":"code","c4b37ee4":"code","4f3d3728":"code","cd68513f":"code","2e68edd5":"code","34f28820":"code","bd703e56":"code","8840f99c":"code","7b7cedcd":"code","ce1ff8cd":"code","be3edd86":"code","882e68d0":"code","de3fe694":"code","62075acf":"code","16103615":"code","3b45fae1":"code","79c80dfa":"code","5cc851ba":"code","b1fe3dbb":"code","bda7d94d":"code","06056cfa":"code","c685cda3":"code","fa85ca6e":"markdown","00be9dbc":"markdown","283738f7":"markdown","9becc050":"markdown","4fc68426":"markdown","73812696":"markdown","4b992dde":"markdown","0687f123":"markdown","eac6b1c3":"markdown","c995cbfe":"markdown","3e0b1172":"markdown","61f0f2e6":"markdown","2d5e7990":"markdown","6fd2f2ca":"markdown","f6fdfb89":"markdown"},"source":{"1b6e4a16":"import pandas as pd\nfrom pandas_profiling import ProfileReport\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold,train_test_split,GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom IPython.display import Image\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_tree, plot_importance\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","2709f1bc":"df_train = pd.read_csv(\"..\/input\/mobile-price-classification\/train.csv\", header=0)\ndf_test = pd.read_csv(\"..\/input\/mobile-price-classification\/train.csv\", header=0)","f4b85ca0":"df_test.head()","aea2bc31":"df_train.info()","075f4067":"df_train.describe()","d87aaa36":"df_train.isna().sum()","2f7794ea":"print('train data duplicates = {}'.format(df_train.duplicated().sum()))","4a507fbb":"matrix = np.triu(df_train.corr())\nsns.set_style(\"white\")\nf,ax=plt.subplots(figsize = (16,16))\nsns.heatmap(df_train.corr(),annot= True,fmt = \".2f\",ax=ax,\n            vmin = -1,\n            vmax = 1, mask = matrix,cmap = \"coolwarm\",\n            linewidth = 0.2,linecolor = \"white\")\nplt.xticks(rotation=70)\nplt.yticks(rotation=0)\nplt.title('Correlation Map', size = 14)\nplt.show()","f96ff012":"mydf_train = df_train.copy()\nmydf_test = df_test.copy()","ab2d7939":"mydf_train['diag_px'] = np.sqrt(mydf_train.px_height**2 + mydf_train.px_width**2)\nmydf_train['diag_sc'] = np.sqrt(mydf_train.sc_h**2 +mydf_train.sc_w**2)\n\nmydf_test['diag_px'] = np.sqrt(mydf_test.px_height**2 + mydf_test.px_width**2)\nmydf_test['diag_sc'] = np.sqrt(mydf_test.sc_h**2 + mydf_test.sc_w**2)\n","e7292cac":"mydf_train.drop(['px_height', 'px_width','sc_h', 'sc_w'],axis=1, inplace=True)\nmydf_test.drop(['px_height', 'px_width','sc_h', 'sc_w'],axis=1, inplace=True)","28403b69":"matrix = np.triu(mydf_train.corr())\nsns.set_style(\"white\")\nf,ax=plt.subplots(figsize = (16,16))\nsns.heatmap(mydf_train.corr(),annot= True,fmt = \".2f\",ax=ax,\n            vmin = -1,\n            vmax = 1, mask = matrix,cmap = \"coolwarm\",\n            linewidth = 0.2,linecolor = \"white\")\nplt.xticks(rotation=70)\nplt.yticks(rotation=0)\nplt.title('Correlation Map', size = 14)\nplt.show()","3541682b":"#data_train.profile_report()","e8e2e4b2":"ax = sns.boxplot(x=\"price_range\",\n            y=\"ram\",\n            data=df_train,\n            palette=\"Set3\",\n            fliersize=5)","312c456f":"df_train['lnram'] = np.log(df_train.ram)\ndf_train['regularized'] = (df_train.ram - np.mean(df_train.ram))\/ (max(df_train.ram)-min(df_train.ram))\ndf_train['standard'] = (df_train.ram - np.mean(df_train.ram))\/ np.std(df_train.ram)","42d98a69":"fig, (ax1, ax2, ax3,ax4) = plt.subplots(4,1,figsize=(10,10))\nsns.boxplot(x=\"price_range\",\n            y=\"ram\",\n            data=df_train,\n            palette=\"Set3\",\n            fliersize=5,\n            ax=ax1)\nsns.boxplot(x=\"price_range\",\n            y=\"lnram\",\n            data=df_train,\n            palette=\"Set3\",\n            fliersize=5,\n            ax=ax2)\nsns.boxplot(x=\"price_range\",\n            y=\"regularized\",\n            data=df_train,\n            palette=\"Set3\",\n            fliersize=5,\n            ax=ax3)\nsns.boxplot(x=\"price_range\",\n            y=\"standard\",\n            data=df_train,\n            palette=\"Set3\",\n            fliersize=5,\n            ax=ax4)","a3ff7627":"df_train.drop(labels=[\"lnram\", \"regularized\", 'standard'],axis=1, inplace=True)","5d77fece":"mydf_train['ram_stand'] = (mydf_train.ram - np.mean(mydf_train.ram))\/ np.std(mydf_train.ram)\nmydf_test['ram_stand'] =  (mydf_test.ram - np.mean(mydf_test.ram))\/ np.std(mydf_test.ram)","cf392561":"sns.displot(df_train, x=\"ram\",\n            hue=\"price_range\",\n            kind=\"kde\", \n            fill=True,\n            palette=\"tab10\",\n            ax=ax);","fd467c25":"fig, ax = plt.subplots()\nsns.boxplot(x=\"price_range\",\n                 y=\"battery_power\",\n                 data=df_train,\n                 palette=\"Set3\",\n                 fliersize=5,\n            ax=ax);\nfig.set_size_inches(10,10)","5497a3fb":"fig, ax = plt.subplots()\nsns.boxplot(x=\"price_range\",\n                 y=np.log(df_train.battery_power),\n                 data=df_train,\n                 palette=\"Set3\",\n                 fliersize=5);\nfig.set_size_inches(10,10)","51937528":"sns.displot(df_train, x='battery_power',\n            hue=\"price_range\",\n            kind=\"kde\", \n            fill=True,\n            palette=\"tab10\");\n","a4a17ca9":"fig, ax = plt.subplots()\nsns.scatterplot(data=df_train,x='battery_power', y='ram',\n                hue='price_range',\n                palette=\"dark\",\n                alpha=0.65,\n                ax=ax);\nfig.set_size_inches(8,8)","d252a035":"corr = mydf_train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 15))\n    ax = sns.heatmap(corr,\n                     fmt = \".2f\",\n                     mask=mask,\n                     cmap=\"YlGnBu\",\n                     ax=ax,\n                     annot=True,\n                     vmin=-1,vmax=1,\n                     linecolor = \"white\",\n                     linewidth = 0.2,\n                     #center=0\n                     )","19750256":"corr = df_train.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 15))\n    ax = sns.heatmap(corr,\n                     fmt = \".2f\",\n                     mask=mask,\n                     cmap=\"YlGnBu\",\n                     ax=ax,\n                     annot=True,\n                     vmin=-1,vmax=1,\n                     linecolor = \"white\",\n                     linewidth = 0.2,\n                     #center=0\n                     )","8ae85f2b":"ax = sns.barplot(x=\"price_range\", y=\"three_g\", data=df_train)","0bbc7bb6":"ax = sns.barplot(x=\"price_range\", y=\"four_g\", data=df_train)","cee7bd35":"sns.set_theme(style=\"whitegrid\")\nax = sns.barplot(x=\"price_range\", y=\"ram\",hue='n_cores', data=df_train,\n            palette=\"dark\",\n            alpha=.7)","c4b37ee4":"df_train.columns","4f3d3728":"x = df_train.drop(\"price_range\", axis=1)\ny = df_train.price_range\n\nX_train, X_test_1, Y_train, Y_test_1 = train_test_split(x, y, test_size=0.2, random_state=42) \nX_valid, X_test, Y_valid, Y_test = train_test_split(X_test_1, Y_test_1, test_size=0.25, random_state=42)","cd68513f":"random_state = 42\n\n#X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2, random_state = random_state)","2e68edd5":"print(X_train.shape,X_valid.shape,X_test.shape)\nprint(\"\\n\")\nprint(Y_train.shape,Y_valid.shape,Y_test.shape)","34f28820":"Strander =StandardScaler() \nX_train = Strander.fit_transform(X_train)\nX_valid = Strander.fit_transform(X_valid)\nX_test = Strander.transform(X_test)","bd703e56":"models = {\n    'GaussianNB': GaussianNB(),\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'RandomForestClassifier': RandomForestClassifier(random_state=42),\n    'SupportVectorMachineLinear': SVC(kernel='linear',gamma='auto',random_state=42),\n    'SupportVectorMachineRbf': SVC(kernel='rbf',gamma='auto',random_state=42),\n    'MultiLinearPrecptron': MLPClassifier(random_state=42), \n    'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'GradientBoostingClassifier': GradientBoostingClassifier(random_state=42)}","8840f99c":"modelNames = ['GaussianNB',\n              'LogisticRegression',\n              'RandomForestClassifier',\n              'SupportVectorMachineLinear',\n              'SupportVectorMachineRbf',\n              'MultiLinearPrecptron',\n              'DecisionTreeClassifier',\n              'KNeighborsClassifier',\n              'GradientBoostingClassifier'\n              ]\ncv_results_acc = []\ntrainScores = []\nvalidationScores = []\ntestScores = []\nbest_estimators = []\n\nfor each in models:\n      '''\n      model = GridSearchCV(models[each],\n                       param_grid=classifier_param[each],\n                       cv = StratifiedKFold(n_splits = 10),\n                       scoring = \"accuracy\",\n                       n_jobs = -1,verbose = 2,\n                       );\n                       '''\n      model = models[each]\n      model.fit(X_train, Y_train)\n      print(\"Model: {}\".format(each))\n      #print(\"Best Estimator: {}\".format(model.best_estimator_))\n      #print('{}'.format(modelNames[each])) \n      train_score = model.score(X_train,Y_train)\n      print('Train score of trained model: {}'.format(train_score*100))\n      trainScores.append(train_score*100)\n\n      validation_score = model.score(X_valid, Y_valid)\n      print('Validation score of trained model: {}'.format(validation_score*100))\n      validationScores.append(validation_score*100)\n\n      test_score = model.score(X_test, Y_test)\n      print('Test score of trained model: {}'.format(test_score*100))\n      testScores.append(test_score*100)\n      print(\" \")\n        \n      y_predictions = model.predict(X_test)\n      conf_matrix = confusion_matrix(y_predictions, Y_test)\n\n      print('Confussion Matrix: \\n{}\\n'.format(conf_matrix))\n\n      predictions = model.predict(X_test)\n      cm = confusion_matrix(predictions, Y_test)\n\n      \n\n      print(\"\") \n      print('Classification Report: \\n{}\\n'.format(classification_report(predictions, Y_test)))\n      print(\"\")\n\n      for i in range(1):\n        current = modelNames[i]\n        modelNames.remove(modelNames[i])\n        cv_score = cross_val_score(model, X_train, Y_train,scoring=\"accuracy\", cv=10)\n        cv_results_acc.append(cv_score.mean()*100)\n        print(\"Cross Validation Accuracy: {}:{}\".format(current, cv_score.mean()))\n\n        preds = model.predict(X_test)\n        confusion_matr = confusion_matrix(Y_test, preds) #normalize = 'true'\n        print(\"===================================================================================\")\n        print(\"\")\n        print(\"\")\n        print(\"\")","7b7cedcd":"models_results = {\"Test_Accuracy\":testScores,\n                               \"Cross_Validation_Accuracy\": cv_results_acc,\n                               \"Models\":['GaussianNB',\n                                          'LogisticRegression',\n                                          'RandomForestClassifier',\n                                          'SupportVectorMachineLinear',\n                                          'SupportVectorMachineRbf',\n                                          'MultiLinearPrecptron',\n                                          'DecisionTreeClassifier',\n                                          'KNeighborsClassifier',\n                                          'GradientBoostingClassifier'\n                                          ]}\nmodels_results = pd.DataFrame(models_results, columns= ['Models','Test_Accuracy', 'Cross_Validation_Accuracy'])\n","ce1ff8cd":"models_results = models_results.sort_values(\"Cross_Validation_Accuracy\",ascending=False)\nfig, ax = plt.subplots()                \nsns.barplot(y=models_results.Models, x= models_results.Cross_Validation_Accuracy,\n            ax = ax,\n            palette=\"dark\",\n            alpha=.8)\nplt.xlabel(\"Mean Accuracy\")\nplt.title(\"Cross Validation Scores\")\nfig.set_size_inches(8,8)","be3edd86":"X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42) \n\nStrander =StandardScaler() \nX_train = Strander.fit_transform(X_train)\nX_test = Strander.transform(X_test)","882e68d0":"max_itr = 200\nmodels_best = [LogisticRegression(random_state=42,max_iter= max_itr),\n              RandomForestClassifier(random_state=4),\n              SVC(random_state=42,probability=True),\n              MLPClassifier(random_state=42),\n              GradientBoostingClassifier(random_state=42)]","de3fe694":"svc_param_grid = {\"kernel\" : [\"rbf\", \"linear\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [0.1, 1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-4, 4, 20),\n                    \"penalty\": [\"l1\",\"l2\",\"none\"]}\n\n\n\ngbc_param_grid = {\"learning_rate\": [0.05, 0.1, 0.2],\n                  \"min_samples_split\": [2,3,10],\n                  \"min_samples_leaf\": [1,3,10]}\n\nMLP_pram_grid = {\"activation\":[\"logistic\",\"relu\"],\n              \"solver\":[\"adam\",\"sgd\"]}\n\n\nclassifier_param = [logreg_param_grid,\n                    rf_param_grid,\n                    svc_param_grid,\n                    MLP_pram_grid,\n                    gbc_param_grid]","62075acf":"models_best_Names = [\n                     'LogisticRegression',\n                     'RandomForestClassifier',\n                     'SupportVectorMachine',\n                     'MultiLinearPrecptron',\n                     'GradientBoostingClassifier'\n                      ]","16103615":"models_best_Names = [\n                     'LogisticRegression',\n                     'RandomForestClassifier',\n                     'SupportVectorMachine',\n                     'MultiLinearPrecptron',\n                     'GradientBoostingClassifier'\n                      ]\n\ncv_result = []\nbest_estimators = []\nmean_squared_errors = []\nroc_auc_scores = []\nrecall_scores = []\nprecision_scores = []\nf1_scores = []\n\nfor i in range(len(models_best)):\n    print(\"---------------------------------------------------------------------------\")\n    model = GridSearchCV(models_best[i],\n                       param_grid=classifier_param[i],\n                       cv = StratifiedKFold(n_splits = 10),\n                       scoring = \"accuracy\",\n                       n_jobs = -1,verbose = 2)\n    \n    model.fit(X_train,Y_train)\n    \n    cv_result.append(model.best_score_)\n    \n    mean_squared_errors.append(mean_squared_error(Y_test,model.predict(X_test)))\n\n    recall_scores.append(recall_score(Y_test, model.predict(X_test), average='weighted'))\n    \n    precision_scores.append(precision_score(Y_test, model.predict(X_test), average='weighted'))\n    f1_scores.append(f1_score(Y_test, model.predict(X_test), average='weighted'))\n    \n    best_estimators.append(model.best_estimator_)\n    print(\"\")\n    print(\"\")\n    print(\"Model: {} \\n\".format(models_best_Names[i]))\n    print(\"Accuracy: %{} \".format(round(cv_result[i]*100,2)))\n    print(\"MSE: {} \".format(mean_squared_errors[i]))\n    print(\"Recall: {} \".format(recall_scores[i]))\n    print(\"Precision: {} \".format(precision_scores[i]))\n    print(\"F1-Score: {} \\n\".format(f1_scores[i]))\n    #print(\"Best Estimator: {} \".format(model.best_estimator_))\n\n    test_score = model.score(X_test, Y_test)\n    print('Test score of trained model: {}'.format(test_score*100))\n    testScores.append(test_score*100)\n    print(\" \")\n      \n    y_predictions = model.predict(X_test)\n    conf_matrix = confusion_matrix(y_predictions, Y_test)\n\n    print('Confussion Matrix: \\n{}\\n'.format(conf_matrix))\n\n    predictions = model.predict(X_test)\n    cm = confusion_matrix(predictions, Y_test)\n\n\n\n    \n\n    print(\"\") \n    print('Classification Report: \\n{}\\n'.format(classification_report(predictions, Y_test)))\n    print(\"\")\n\n    print(\"Best Estimator: {} \\n\".format(model.best_estimator_))    \nprint(\"---------------------------------------------------------------------------\")","3b45fae1":"models_results = {\"Test_Accuracy\":test_score,\n                  \"Cross_Validation_Accuracy\": cv_result,\n                  \"Models\":[\n                          'LogisticRegression',\n                          'RandomForestClassifier',\n                          'SupportVectorMachine',\n                          'MultiLinearPrecptron',\n                          'GradientBoostingClassifier'\n                            ]}\nmodels_results = pd.DataFrame(models_results, columns= ['Models','Test_Accuracy', 'Cross_Validation_Accuracy'])\n","79c80dfa":"models_results = models_results.sort_values(\"Cross_Validation_Accuracy\",ascending=False)\nfig, ax = plt.subplots()                \nsns.barplot(y=models_results.Models, x= models_results.Cross_Validation_Accuracy,\n            ax = ax,\n            palette=\"dark\",\n            alpha=.8)\nplt.xlabel(\"Mean Accuracy\")\nplt.title(\"Cross Validation Scores\")\nfig.set_size_inches(8,8)","5cc851ba":"best_alg = best_estimators[0]\nprint(\"Best Algorithem: {} \\n\".format(best_alg)) \nprint(\"Accuracy: %{} \".format(round(cv_result[0]*100,2)))\ny_predictions = best_alg.predict(X_test)\nconf_matrix = confusion_matrix(y_predictions, Y_test)\n\nprint('Confussion Matrix: \\n{}\\n'.format(conf_matrix))\n\npredictions = model.predict(X_test)\ncm = confusion_matrix(predictions, Y_test)\n\n\n\n\n\nprint(\"\") \nprint('Classification Report: \\n{}\\n'.format(classification_report(predictions, Y_test)))","b1fe3dbb":"x = mydf_train.drop(\"price_range\", axis=1)\ny = mydf_train.price_range\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state=42) \n\nStrander =StandardScaler() \nX_train = Strander.fit_transform(X_train)\nX_test = Strander.transform(X_test)","bda7d94d":"models_best_Names = [\n                     'LogisticRegression',\n                     'RandomForestClassifier',\n                     'SupportVectorMachine',\n                     'MultiLinearPrecptron',\n                     'GradientBoostingClassifier'\n                      ]\n\ncv_result = []\nbest_estimators = []\ntestScores = []\n\nfor i in range(len(models_best)):\n    print(\"---------------------------------------------------------------------------\")\n    model = GridSearchCV(models_best[i],\n                       param_grid=classifier_param[i],\n                       cv = StratifiedKFold(n_splits = 10),\n                       scoring = \"accuracy\",\n                       n_jobs = -1,verbose = 2)\n    \n    model.fit(X_train,Y_train)\n    \n    cv_result.append(model.best_score_)\n    \n    mean_squared_errors.append(mean_squared_error(Y_test,model.predict(X_test)))\n\n    best_estimators.append(model.best_estimator_)\n    print(\"\")\n    print(\"\")\n    print(\"Model: {} \\n\".format(models_best_Names[i]))\n    print(\"Accuracy: %{} \".format(round(cv_result[i]*100,2)))\n    print(\"MSE: {}\\n \".format(mean_squared_errors[i]))\n    #print(\"Best Estimator: {} \".format(model.best_estimator_))\n\n    test_score = model.score(X_test, Y_test)\n    print('Test score of trained model: {}'.format(test_score*100))\n    testScores.append(test_score*100)\n    print(\" \")\n      \n    y_predictions = model.predict(X_test)\n    conf_matrix = confusion_matrix(y_predictions, Y_test)\n\n    print('Confussion Matrix: \\n{}\\n'.format(conf_matrix))\n\n    predictions = model.predict(X_test)\n    cm = confusion_matrix(predictions, Y_test)\n\n\n\n    \n\n    print(\"\") \n    print('Classification Report: \\n{}\\n'.format(classification_report(predictions, Y_test)))\n    print(\"\")\n\n    print(\"Best Estimator: {} \\n\".format(model.best_estimator_))    \nprint(\"---------------------------------------------------------------------------\")","06056cfa":"models_results = {\"Test_Accuracy\":testScores,\n                  \"Cross_Validation_Accuracy\": cv_result,\n                  \"Models\":[\n                          'LogisticRegression',\n                          'RandomForestClassifier',\n                          'SupportVectorMachine',\n                          'MultiLinearPrecptron',\n                          'GradientBoostingClassifier'\n                            ]}\nmodels_results = pd.DataFrame(models_results, columns= ['Models','Test_Accuracy', 'Cross_Validation_Accuracy'])\nmodels_results = models_results.sort_values(\"Cross_Validation_Accuracy\",ascending=False)\nfig, ax = plt.subplots()                \nsns.barplot(y=models_results.Models, x= models_results.Cross_Validation_Accuracy,\n            ax = ax,\n            palette=\"dark\",\n            alpha=.8)\nplt.xlabel(\"Mean Accuracy\")\nplt.title(\"Cross Validation Scores\")\nfig.set_size_inches(8,8)","c685cda3":"best_alg = best_estimators[0]\nprint(\"Best Algorithem: {} \\n\".format(best_alg)) \nprint(\"Accuracy: %{} \".format(round(cv_result[0]*100,2)))\ny_predictions = best_alg.predict(X_test)\nconf_matrix = confusion_matrix(y_predictions, Y_test)\n\nprint('Confussion Matrix: \\n{}\\n'.format(conf_matrix))\n\npredictions = model.predict(X_test)\ncm = confusion_matrix(predictions, Y_test)\n\n\n\n\n\nprint(\"\") \nprint('Classification Report: \\n{}\\n'.format(classification_report(predictions, Y_test)))","fa85ca6e":"# colclusion\n>* Logistic Regression is the best model for the data and its best estimator is\n` LogisticRegression(C=545.5594781168514, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=200, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False) `\n>* Best split of the data is to take 80% from the data to train set and 20% of the data for testing set without taking validation set and make it inside the training\n>* Reducing features by replace mobile's hieght and wiedth with diameter and also for screen is not effective and reduce the accuracy of the model but reducing also the variation between models in cllassification.  \n>* Making standardization scaling to the data is significantly effective for increasing model classification","00be9dbc":"transformaton didn't handeled the data but, I will choose standarradization. seems good enough for me to reduce variation","283738f7":"# visualization","9becc050":"found here that there are outliers in ram with price ranges so, i will try handel them by transformation.","4fc68426":"*corelation with targets:*\n>* `ram` is highly corelated with `price_range`\n* there is corelation between `price_range` and `battery_power`, `px_height` and `px_width` \n\n*corelation between features:*\n>* `three_g` corelated with `four_g`\n* `sc_width` corelated with `sc_height`\n* `px_width` corelated with `px_height`\n* *front camera* corelated with *back camera*\n\n","73812696":"## See my solution in feature reduction","4b992dde":"## The data is very good. Has no null values or duplicated values","0687f123":"# Corelation","eac6b1c3":"## **`diag_px` and `diag_sc` added to the data**\nAccording to [relation](https:\/\/www.omnicalculator.com\/math\/diagonal-of-rectangle)\n\n","c995cbfe":"# Cleaning the data","3e0b1172":"## Results for the best Algorithem","61f0f2e6":"# Loadind data and wrangling\n ","2d5e7990":"# Reducing dependances","6fd2f2ca":"##I will choose te first 5 algorithmes to get the best models of all with different hyperparameters","f6fdfb89":"# Importting neccessary liberaries "}}