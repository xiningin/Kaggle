{"cell_type":{"ee693bf9":"code","ef447aea":"code","e81ad862":"code","9b1cc7d6":"code","a1e21b50":"code","54cfdf52":"code","7cedf893":"code","48313e7f":"code","5d71ac1c":"code","73b0fcde":"code","d8dcd912":"code","a17fcbc4":"code","87b42410":"code","6615c4e9":"code","625333dd":"code","971fc7c1":"code","a1433909":"code","abfee7ae":"code","2f838ea9":"code","131f1afe":"code","b2c1a420":"code","7852b79e":"code","a3bc69a9":"code","8bb78bd3":"code","8934800c":"code","f569c116":"code","5fec022b":"code","57bc70e5":"code","702da084":"code","aa679f96":"code","584d6e32":"code","3a4b3900":"code","5c777c7f":"code","a7d877c4":"code","f4795f7b":"code","b598f87e":"code","c09464a7":"code","c59e78cc":"code","0c0b2040":"code","2ca94cba":"code","715d8865":"code","6c11d89c":"code","51d7e8a2":"code","8f7a36ca":"code","5fa1ce8b":"code","dcfe59d9":"code","a1295acd":"code","5054572f":"code","9f1badb1":"code","9090454f":"code","6680c51c":"code","2e3cbbf3":"code","9d2e24d9":"code","5816f3eb":"code","e51ea192":"code","ba0469d3":"code","bc902020":"code","d21cd390":"code","0e0555ed":"code","94907294":"code","ab763003":"markdown","41add4bb":"markdown","eea6c18b":"markdown","22c3bc03":"markdown","8eac877c":"markdown","95fafe11":"markdown","495fac92":"markdown","d45b02e2":"markdown","dc7d52d6":"markdown","b48ac9c1":"markdown","aa64ffcf":"markdown","dff53a43":"markdown","d3547eb7":"markdown","0bf2d267":"markdown","3ee98731":"markdown","53905df5":"markdown","25640b1b":"markdown"},"source":{"ee693bf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef447aea":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None \n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","e81ad862":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\", index_col=\"PassengerId\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\", index_col=\"PassengerId\")","9b1cc7d6":"print(train_data.info())","a1e21b50":"print(train_data.isna().sum())","54cfdf52":"print(test_data.info())","7cedf893":"print(test_data.isna().sum())","48313e7f":"print(train_data[\"Pclass\"].unique())\ntrain_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5d71ac1c":"print(train_data[\"Name\"])","73b0fcde":"train_data.Name[1].split()","d8dcd912":"train_data = train_data.assign(fname = train_data.Name.str.split(\",\").str[0])\ntrain_data[\"title\"] = pd.Series([i.split(\",\")[1].split(\".\")[0].strip() for i in train_data.Name], index=train_data.index)","a17fcbc4":"test_data = test_data.assign(fname = test_data.Name.str.split(\",\").str[0])\ntest_data[\"title\"] = pd.Series([i.split(\",\")[1].split(\".\")[0].strip() for i in test_data.Name], index=test_data.index)\ntrain_data.drop(\"Name\", axis=1, inplace=True)\ntest_data.drop(\"Name\", axis=1, inplace=True)","87b42410":"print(test_data.fname.nunique())\nprint(test_data.title.nunique())","6615c4e9":"ts = sns.countplot(x=\"title\",data=train_data)\nts = plt.setp(ts.get_xticklabels(), rotation=90)\nprint(train_data[\"title\"].unique())\nprint(test_data[\"title\"].unique())\nother_titles = [title\n                for title in train_data[\"title\"]\n                if title not in [\"Mr\", \"Miss\", \"Mme\", \"Mlle\", \"Mrs\", \"Ms\"]]\nother_titles.append(\"Dona\")","625333dd":"train_data[\"title\"] = train_data['title'].replace(other_titles, 'Other')\ntrain_data[\"title\"] = train_data[\"title\"].map({\"Mr\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Master\":2, \"Other\":3})\ntest_data[\"title\"] = test_data['title'].replace(other_titles, 'Other')\ntest_data[\"title\"] = test_data[\"title\"].map({\"Mr\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Master\":2, \"Other\":3})","971fc7c1":"print(train_data.title)\nprint(test_data.title.isna().sum()) # No NaNs left","a1433909":"from sklearn.preprocessing import OneHotEncoder\noh = OneHotEncoder(handle_unknown=\"ignore\", sparse = False)\n\ntrain_data = train_data.join(pd.DataFrame(oh.fit_transform(train_data[[\"fname\", \"title\"]]), index = train_data.index))\ntest_data = test_data.join(pd.DataFrame(oh.transform(test_data[[\"fname\", \"title\"]]), index = test_data.index))\ntrain_data.drop(\"fname\", axis = 1, inplace = True)\ntest_data.drop(\"fname\", axis = 1, inplace = True)","abfee7ae":"print(train_data[\"Sex\"].unique())\ntrain_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2f838ea9":"interactions = train_data.assign(sex_class = train_data['Sex'] + \"_\" + train_data['Pclass'].astype(\"str\"))\ninteractions[['sex_class', 'Survived']].groupby(['sex_class'], as_index=False).mean().sort_values(by='Survived', ascending=False)","131f1afe":"train_data = train_data.assign(sex_class = train_data['Sex'] + \"_\" + train_data['Pclass'].astype(\"str\"))\ntest_data = test_data.assign(sex_class = test_data['Sex'] + \"_\" + test_data['Pclass'].astype(\"str\"))","b2c1a420":"train_data[\"Sex\"] = train_data[\"Sex\"].map({\"female\":0, \"male\":1})\ntest_data[\"Sex\"] = test_data[\"Sex\"].map({\"female\":0, \"male\":1})","7852b79e":"train_data[\"sex_class\"] = train_data[\"sex_class\"].map({\"female_1\":0, \"female_2\":1, \"female_3\":2, \"male_1\":4, \"male_2\":5, \"male_3\":6})\ntest_data[\"sex_class\"] = test_data[\"sex_class\"].map({\"female_1\":0, \"female_2\":1, \"female_3\":2, \"male_1\":4, \"male_2\":5, \"male_3\":6})","a3bc69a9":"g = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Age\")","8bb78bd3":"def find_similar_passengers(id, dataset):\n    subset = dataset[(dataset.title == dataset.title[id]) &\n                    (dataset.Pclass == dataset.Pclass[id])]\n\n    if subset[\"Age\"].mean() == \"NaN\":\n        subset = dataset[(dataset[\"sex_class\"] == dataset.iloc[id][\"sex_class\"])]\n\n    if subset[\"Age\"].mean() == \"NaN\":\n        subset = dataset[(dataset[\"sex\"] == dataset.iloc[id][\"sex\"])]\n\n    age = subset[\"Age\"].mean()\n    return age","8934800c":"no_ages = train_data[train_data[\"Age\"].isna()].index\nfor pid in no_ages:\n    train_data.Age[pid] = find_similar_passengers(pid, train_data)\n\nno_ages_test = test_data[test_data[\"Age\"].isna()].index\nfor pid2 in no_ages_test:\n    test_data.Age[pid2] = find_similar_passengers(pid2, test_data)","f569c116":"train_data[\"age_group\"] =  pd.cut(train_data[\"Age\"], bins=[0,5,65,100], labels=[0,1,2]).astype(\"int64\")\ntest_data[\"age_group\"] = pd.cut(test_data[\"Age\"], bins=[0,5,65,100], labels=[0,1,2]).astype(\"int64\")","5fec022b":"train_data[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","57bc70e5":"train_data[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","702da084":"train_data[\"fsize\"] = train_data[\"SibSp\"] + train_data[\"Parch\"] + 1\ntest_data[\"fsize\"] = test_data[\"SibSp\"] + test_data[\"Parch\"] + 1","aa679f96":"train_data[['fsize', 'Survived']].groupby(['fsize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","584d6e32":"print(train_data.Ticket.nunique())\nprint(train_data.Ticket.tail())","3a4b3900":"train_data[\"ticket_prefix\"] = pd.Series([len(i.split()) > 1 for i in train_data.Ticket], index=train_data.index)\n","5c777c7f":"train_data[['ticket_prefix', 'Survived']].groupby(['ticket_prefix'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a7d877c4":"train_data.drop(\"ticket_prefix\", axis=1, inplace=True)\ntrain_data.drop(\"Ticket\", axis=1, inplace=True)\ntest_data.drop(\"Ticket\", axis=1, inplace=True)","f4795f7b":"g = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","b598f87e":"import numpy as np\ntrain_data[\"Fare\"] = train_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ntest_data[\"Fare\"] = test_data[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ng = sns.FacetGrid(train_data, col='Survived')\ng = g.map(sns.distplot, \"Fare\")","c09464a7":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"S\")\ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c59e78cc":"train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"S\")\nprint(train_data.Embarked.isna().sum())","0c0b2040":"train_data = train_data.join(pd.get_dummies(train_data['Embarked'], prefix=\"Embarked_\"))\ntest_data = test_data.join(pd.get_dummies(test_data['Embarked'], prefix=\"Embarked_\"))","2ca94cba":"train_data.drop(\"Embarked\", axis=1, inplace=True)\ntest_data.drop(\"Embarked\", axis=1, inplace=True)","715d8865":"train_data.drop(\"Cabin\", axis=1, inplace=True)\ntest_data.drop(\"Cabin\", axis=1, inplace=True)","6c11d89c":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\ntrain_y = train_data[\"Survived\"]\ntrain_data.drop(\"Survived\", axis=1, inplace=True)\n\nscoring_method = \"f1\"\n\ntrain_scaled = ss.fit_transform(train_data)\ntest_scaled = ss.transform(test_data)","51d7e8a2":"print(train_data.isna().sum())\nprint(test_data.isna().sum())","8f7a36ca":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nmodel = LogisticRegression(random_state=10, max_iter = 1000)\nlogit_params = {\n    \"C\": [1, 3, 10, 20, 30, 40],\n    \"solver\": [\"lbfgs\", \"liblinear\"]\n    \n}\nlogit_gs = GridSearchCV(model, logit_params, scoring=\"f1\", cv = 5, n_jobs=4)\n","5fa1ce8b":"logit_gs.fit(train_data, train_y)","dcfe59d9":"print(logit_gs.best_params_)\nprint(logit_gs.best_score_)","a1295acd":"from sklearn.svm import SVC\nsvc_model = SVC()\n\ntest_parameters = {\n    \"C\": [1, 3, 10, 30, 100],\n    \"kernel\": [\"linear\", \"poly\", \"rbf\" , \"sigmoid\"],\n}\nsvc_gs = GridSearchCV(svc_model, test_parameters, scoring=\"f1\", cv=5, n_jobs=4)","5054572f":"svc_gs.fit(train_scaled, train_y)","9f1badb1":"print(svc_gs.best_params_)\nprint(svc_gs.best_score_)","9090454f":"from lightgbm import LGBMClassifier\nlgb_model = LGBMClassifier()\ntest_parameters = {\n    \"n_estimators\": [int(x) for x in np.linspace(5, 30, 6)],\n    \"reg_alpha\": [0, 0.75, 1, 1.25],\n    \"learning_rate\": [0.5, 0.4, 0.35, 0.3, 0.25, 0.2],\n    \"subsample\": [0.5, 0.75, 1]\n}\nlgb_gs = GridSearchCV(lgb_model, test_parameters, scoring=scoring_method, cv=8, n_jobs=4)","6680c51c":"lgb_gs.fit(train_data, train_y)","2e3cbbf3":"print(lgb_gs.best_params_)\nprint(lgb_gs.best_score_)","9d2e24d9":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier()\n\nrf_params ={\n    'bootstrap': [True, False],\n    'max_depth': [10, None],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [1, 2, 4],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [5, 10, 15, 20, 25, 30]}\n\nrf_gs = GridSearchCV(rf_model, rf_params, scoring=scoring_method, cv=8, n_jobs=4)","5816f3eb":"rf_gs.fit(train_data, train_y)","e51ea192":"print(rf_gs.best_params_)\nprint(rf_gs.best_score_)","ba0469d3":"from sklearn.ensemble import VotingClassifier\n\nensemble_model = VotingClassifier(estimators=[\n    (\"logit\", logit_gs.best_estimator_),\n    (\"rf\", rf_gs.best_estimator_),\n    (\"svc\", svc_gs.best_estimator_),\n    (\"lgb\", lgb_gs.best_estimator_),\n], voting = \"hard\")","bc902020":"ensemble_model.fit(train_data, train_y)","d21cd390":"ensemble_model.score(train_data, train_y)","0e0555ed":"preds = ensemble_model.predict(test_data)","94907294":"output = pd.DataFrame({'PassengerId': test_data.index,\n                       'Survived': preds})\n\noutput.to_csv('submission.csv', index=False)","ab763003":"# BASIC INFORMATION ABOUT TRAINING AND TESTING DATA SET","41add4bb":"MODELLING","eea6c18b":"VOTING CLASSIFIER \n","22c3bc03":"SIBLINGS OR SPOUSES","8eac877c":"SEX","95fafe11":"LOGISTIC REGRESSION","495fac92":"TICKET","d45b02e2":"# DATA VISULIZATION","dc7d52d6":"NAME","b48ac9c1":"# DATA LOADING","aa64ffcf":"LIGHT GRADIENT BOOSTING","dff53a43":"PARCH","d3547eb7":"FARE","0bf2d267":"MODEL COMPARISON","3ee98731":"RANDOM FOREST","53905df5":"P CLASS","25640b1b":"SVM"}}