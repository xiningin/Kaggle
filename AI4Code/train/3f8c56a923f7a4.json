{"cell_type":{"0542a9ae":"code","39cb5fed":"code","b641dc63":"code","dba1dd7f":"code","a9078d9e":"code","6d95f0c4":"code","39809089":"code","3aa7f6b4":"code","ef87511a":"code","d91f27e9":"code","58662432":"code","5ae3c53e":"code","4bfc8ae9":"code","4a1f9553":"code","3657a443":"code","765697cb":"code","507071ef":"code","6417a825":"code","b610459a":"code","58c24a96":"code","c3cf6414":"code","8ac37bab":"code","65cb31bd":"code","814b1620":"code","438a8f87":"code","d1413441":"code","cdac638d":"code","86eaaabc":"code","e6856692":"code","72be2d71":"code","ceb16ec2":"code","f4a0d05c":"code","ca3f9144":"code","7ed17dc2":"markdown","a9725ab4":"markdown","8bd6274c":"markdown","26a659a3":"markdown","8736fb0f":"markdown","4687c5b4":"markdown","995499af":"markdown"},"source":{"0542a9ae":"import os\nimport gc\nimport sys\nimport cv2\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold,StratifiedKFold\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification)\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL","39cb5fed":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\n#\u5bfe\u6570\u5206\u3060\u3051num_bins\u306b\u5206\u5272\n#\u30d3\u30cb\u30f3\u30b0\u51e6\u7406,\u30c7\u30fc\u30bf\u3092bin\u6570\u3067\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3059\u308b\nnum_bins = int(np.floor(1 + np.log2(len(train_data))))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n\ntarget = train_data['target'].to_numpy()\nbins = train_data.bins.to_numpy()\n\n#\u5e73\u5747\u4e8c\u6761\u8aa4\u5dee\ndef rmse_score(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true,y_pred))","b641dc63":"config = {\n    'batch_size':128,\n    'max_len':256,\n    'nfolds':10,\n    'seed':42,\n}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])","dba1dd7f":"class CLRPDataset(Dataset):\n    def __init__(self,df,tokenizer):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],return_tensors='pt',\n                                max_length=config['max_len'],\n                                padding='max_length',truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)","a9078d9e":"class AttentionHead(nn.Module):\n    def __init__(self, in_features, hidden_dim, num_targets):\n        super().__init__()\n        self.in_features = in_features\n        self.middle_features = hidden_dim\n\n        self.W = nn.Linear(in_features, hidden_dim)\n        self.V = nn.Linear(hidden_dim, 1)\n        self.out_features = hidden_dim\n\n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n\n        score = self.V(att)\n\n        attention_weights = torch.softmax(score, dim=1)\n\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector","6d95f0c4":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        self.roberta = AutoModel.from_pretrained('..\/input\/clrp-pytorch-roberta-pretrain-roberta-large\/clrp_roberta_large\/')    \n        #changed attentionHead Dimension from 768 to 1024 by changing model from roberta-base to roberta-large\n        self.head = AttentionHead(1024,1024,1)\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(self.head.out_features,1)\n\n    def forward(self,**xb):\n        x = self.roberta(**xb)[0]\n        x = self.head(x)\n        return x","39809089":"#\u3053\u3053\u304c\u65b0\u898f\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n    #cuda\u4f7f\u3048\u305f\u3089\u4f7f\u3046\u69cb\u6587\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    model = Model()\n    model.load_state_dict(torch.load(path))\n    model.to(device)\n    model.eval()\n    \n    tokenizer = AutoTokenizer.from_pretrained('..\/input\/clrp-pytorch-roberta-pretrain-roberta-large\/clrp_roberta_large\/')\n    \n    ds = CLRPDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config[\"batch_size\"],\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False\n                 )\n        \n    #\u4ee5\u4e0b\u3067predictions\u3092\u62bd\u51fa\u3059\u308b\u305f\u3081\u306b\u4f7f\u3063\u305f\u69cb\u6587\u3092\u4f7f\u3063\u3066embeddings\u3092return\u3057\u3066\u3044\u308b.\n    #SVM\u306e\u624b\u6cd5\u3068\u306f\u3001embeddings\u306e\u610f\u5473\u306f\uff1f\n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs.detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","3aa7f6b4":"#SVM\u3092\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u51e6\u7406\u3057\u3066\u3044\u308b\ndef get_preds_svm(X,y,X_test,bins=bins,nfolds=10,C=10,kernel='rbf'):\n    scores = list()\n    train_preds = np.zeros((X.shape[0]))\n    preds = np.zeros((X_test.shape[0]))\n    \n    kfold = StratifiedKFold(n_splits=config['nfolds'],shuffle=True,random_state=config['seed'])\n    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n        model = SVR(C=C,kernel=kernel,gamma='auto')\n        X_train,y_train = X[train_idx], y[train_idx]\n        X_valid,y_valid = X[valid_idx], y[valid_idx]\n        \n        model.fit(X_train,y_train)\n        prediction = model.predict(X_valid)\n        train_preds[valid_idx] = prediction\n        score = rmse_score(prediction,y_valid)\n        print(f'Fold {k} , rmse score: {score}')\n        scores.append(score)\n        preds += model.predict(X_test)\n        \n    print(\"mean rmse\",np.mean(scores))\n    return train_preds,np.array(preds)\/nfolds","ef87511a":"def get_svm(fold):\n    model_path = f'..\/input\/clrp-pytorch-roberta-finetune-roberta-large\/model{fold}\/model{fold}.bin'\n    train_embeddings =  get_embeddings(train_data,model_path)\n    test_embeddings = get_embeddings(test_data,model_path)\n    train_svm,svm_preds = get_preds_svm(train_embeddings,target,test_embeddings)\n    return train_svm,svm_preds","d91f27e9":"train_svm_list = []\ntest_svm_list = []\nfor fold in range(5):\n    train_svm,svm_preds = get_svm(fold)\n    train_svm_list.append(train_svm)\n    test_svm_list.append(svm_preds)","58662432":"train_svm = np.array(train_svm_list).mean(axis=0)\ntest_svm = np.array(test_svm_list).mean(axis=0)","5ae3c53e":"from sklearn import model_selection\ntrain_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'kfold'] = f\n    return data\ntrain = create_folds(train_df, num_splits=5)","4bfc8ae9":"i = 3\ntrain_idx, val_idx = train.index[train['kfold'] != i].tolist(), train.index[train['kfold'] == i].tolist()","4a1f9553":"from sklearn.metrics import mean_squared_error\ndef rmse(targets, preds):\n    return round(np.sqrt(mean_squared_error(targets, preds)), 4)\nprint('CV\u2019s RMSE:{}'.format(rmse(np.array(train_data.target.values), train_svm)))","3657a443":"from sklearn.metrics import mean_squared_error\ndef rmse(targets, preds):\n    return round(np.sqrt(mean_squared_error(targets, preds)), 4)\nprint('CV\u2019s RMSE:{}'.format(rmse(np.array(train_data.target.values)[val_idx], train_svm[val_idx])))","765697cb":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport gc\ngc.enable()","507071ef":"BATCH_SIZE = 32\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n# ROBERTA_PATH = \"\/kaggle\/input\/roberta-base\"\n# TOKENIZER_PATH = \"\/kaggle\/input\/roberta-base\"\nROBERTA_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nTOKENIZER_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n\ntrain_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\nprint(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index)\n# Remove incomplete entries if any.\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")","6417a825":"# DATASET\nclass LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)\n# MODEL\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.Tanh(),                       \n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )        \n\n        self.regressor = nn.Sequential(                        \n            nn.Linear(768, 1)                        \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.regressor(context_vector)","b610459a":"def rmse(targets, preds):\n    return round(np.sqrt(mean_squared_error(targets, preds)), 4)\n\ndef set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True\n        \ndef predict(model, data_loader,is_test=False):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    if is_test:\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n\n                pred = model(input_ids, attention_mask)                        \n\n                result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n                index += pred.shape[0]\n    else:\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n\n                pred = model(input_ids, attention_mask)                        \n\n                result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n                index += pred.shape[0]\n            \n\n    return result","58c24a96":"gc.collect()\nNUM_FOLDS = 5\nSEED = 1000\nkfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\nvalid_prediction = np.zeros(len(train_df))\n\nfor fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{fold + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n        \n    set_random_seed(SEED + fold)\n    \n#     train_dataset = LitDataset(train_df.loc[train_indices])    \n    val_dataset = LitDataset(train_df.loc[val_indices])    \n        \n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n#                               drop_last=True, shuffle=True, num_workers=2)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False, num_workers=2)    \n    \n    set_random_seed(SEED + fold)   \n    \n    pred = predict(model,val_loader)\n    \n    valid_prediction[val_indices] = pred\n        \n    del model\n    gc.collect()\nprint('CV\u2019s RMSE:{}'.format(rmse(train_df.target.values, valid_prediction)))","c3cf6414":"# In the training process, the author deleted the 106\n# To match the dim in the training set of meta model\n# we put the real target in the training set of meta model\nimport copy\nfor_meta = copy.deepcopy(valid_prediction)\nfor_meta = np.insert(for_meta,106,0)  ","8ac37bab":"NUM_MODELS = 5\n\nall_predictions = np.zeros((NUM_MODELS, len(test_df)))\n\ntest_dataset = LitDataset(test_df, inference_only=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\n\nfor model_index in range(NUM_MODELS):            \n    model_path = f\"..\/input\/commonlit-roberta-0467\/model_{model_index + 1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n                        \n    model = LitModel()\n    model.load_state_dict(torch.load(model_path, map_location=DEVICE))    \n    model.to(DEVICE)\n    if (model_index + 1) != 4:\n        all_predictions[model_index] = predict(model, test_loader,is_test=True)\n    else:\n        pred = (predict(model, test_loader,is_test=True) * 0.7 + test_svm * 0.3)\n        all_predictions[model_index] = pred\n                \n    del model\n    gc.collect()","65cb31bd":"import os\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn import model_selection\ntrain_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\ndef create_folds(data, num_splits):\n    data[\"fold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'fold'] = f\n    return data\nkfold_df = create_folds(train_df, num_splits=5)\n\nin_folder_path = Path('..\/input\/roberta-large-5fold-aux')\nscripts_dir = Path(in_folder_path \/ 'scripts')\nos.chdir(scripts_dir)\nexec(Path(\"imports.py\").read_text())\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\nos.chdir('\/kaggle\/working')","814b1620":"def make_dataloader(data, tokenizer, is_train=True):\n    if is_train:\n        dataset = CLRPDataset(data, tokenizer=tokenizer)\n        sampler = RandomSampler(dataset)\n    else:\n        dataset = CLRPDataset(data, tokenizer=tokenizer,max_len=Config.max_len,is_test=True)\n        sampler = SequentialSampler(dataset)\n    batch_dataloader = DataLoader(dataset, sampler=sampler, batch_size=Config.batch_size)\n    return batch_dataloader\n\ndef get_preds_k(dl,model_num,models_folder_path):\n    model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to(Config.device)\n    model.eval()\n          \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(Config.device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n#             outputs = outputs.detach().cpu().numpy()\n            outputs = outputs.cpu().detach().numpy().ravel().tolist()\n            embeddings.extend(outputs)\n    return np.array(embeddings)\n\ndef stacking_get_pred_k(models_folder_path):\n    tokenizer = torch.load('..\/input\/tokenizers\/roberta-tokenizer.pt')\n#     models_folder_path = Path(in_folder_path \/ 'models')\n    models_preds = []\n    n_models = 5\n    test_preds = np.zeros((test_df.shape[0]))\n    train_preds = np.zeros((train_df.shape[0]))\n\n    for model_num in range(n_models):\n        print(f'Inference#{model_num+1}\/{n_models}')\n        test_dataloader = make_dataloader(test_df, tokenizer, is_train=False)\n        test_pred = get_preds_k(test_dataloader,model_num,models_folder_path)\n        if model_num == 3:\n            test_pred = (test_pred * 0.7 + test_svm * 0.3)\n            test_preds += test_pred\n        else:\n            test_preds += test_pred\n\n        val_dl = make_dataloader(kfold_df[kfold_df.fold==model_num], tokenizer, is_train=False)\n        val_index = kfold_df[kfold_df.fold==model_num].index.tolist()\n        val_preds = get_preds_k(val_dl,model_num,models_folder_path)\n        train_preds[val_index] = val_preds\n    return train_preds,test_preds","438a8f87":"models_folder_path = Path(in_folder_path \/ 'models')\ntrain_preds_k_19,test_preds_k_19 = stacking_get_pred_k(models_folder_path)","d1413441":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\ndef create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'kfold'] = f\n    return data\ntrain = create_folds(train_df, num_splits=5)","cdac638d":"i = 3\ntrain_idx, val_idx = train.index[train['kfold'] != i].tolist(), train.index[train['kfold'] == i].tolist()","86eaaabc":"for_meta[val_idx] = (for_meta[val_idx] * 0.7 + train_svm[val_idx] * 0.3)\ntrain_preds_k_19[val_idx] = (train_preds_k_19[val_idx] * 0.7 + train_svm[val_idx] * 0.3)","e6856692":"oof_train = pd.DataFrame()\n# oof_train['model1'] = oof_roberta_base_i\n# oof_train['model2'] = oof_roberta_large_itpt\n# oof_train['model3'] = oof_roberta_large_ii\noof_train['model4'] = for_meta\n# oof_train['model5'] = clrp_train_preds\n# oof_train['model6'] = clrp_tpu_train_preds\n# oof_train['model7'] = k_train_pred\n# oof_train['model8'] = train_preds_8\n# oof_train['model9'] = distil_train\n# oof_train['model10'] = distil_aux_train\n# oof_train['model11'] = distil_no_aux_train\n# oof_train['model12'] = train_preds_k_12\n# oof_train['model13'] = train_all_predictions\n# oof_train['model15'] = train_preds_k\n# oof_train['model18'] = k_train_pred_18\noof_train['model19'] = train_preds_k_19\noof_train['target'] = train.target.values\noof_train = create_folds(oof_train, num_splits=5)\ndisplay(oof_train.shape)\noof_train.head()","72be2d71":"x_oof_train = oof_train[['model4','model19']]\nx_oof_train.head()","ceb16ec2":"oof_test = pd.DataFrame()\n# oof_test['model1'] = pred_df1.mean(axis=1)\n# oof_test['model2'] = pred_df2.mean(axis=1)\n# oof_test['model3'] = pred_df3.mean(axis=1)\noof_test['model4'] = all_predictions.mean(axis=0)\n# oof_test['model5'] = clrp_test_preds\/5\n# oof_test['model6'] = clrp_tpu_test_preds\/5\n# oof_test['model7'] = k_test_pred\/5\n# oof_test['model8'] = test_preds_8\/5\n# oof_test['model9'] = distil_test\/5\n# oof_test['model10'] = distil_aux_test\/5\n# oof_test['model11'] = distil_no_aux_test\/5\n# oof_test['model12'] = test_preds_k_12\/5\n# oof_test['model13'] = test_all_predictions\n# oof_test['model15'] = test_preds_k\/5\n# oof_test['model18'] = k_test_pred_18\/5\noof_test['model19'] = test_preds_k_19\/5\ndisplay(oof_test.shape)\noof_test.head()","f4a0d05c":"stacking_preds = []\noof_rmses = []\n\nFOLDS = 5\nfor fold in range(FOLDS):\n    print(f'\\nTraining Fold {fold + 1} \/ {FOLDS}')\n    \n    train_idx, val_idx = oof_train.index[oof_train['kfold']!=fold].tolist(), oof_train.index[oof_train['kfold']==fold].tolist()\n    x_train, y_train = x_oof_train.iloc[list(train_idx)], oof_train.target.iloc[train_idx]\n    x_val, y_val = x_oof_train.iloc[list(val_idx)], oof_train.target.iloc[val_idx]\n    from sklearn.linear_model import Ridge\n    reg = Ridge(alpha = 1)\n#     from sklearn.linear_model import BayesianRidge\n#     reg = BayesianRidge(n_iter=300, verbose=True)\n    reg.fit(x_train, y_train)\n    \n    val_pred = reg.predict(x_val)\n    oof_rmse = rmse(val_pred, oof_train.target[val_idx].values)\n    oof_rmses.append(oof_rmse)\n    print(f\"Fold {fold+1} train OOF RMSE: {oof_rmse}\")\n    \n    stacking_preds.append(reg.predict(oof_test))\n\nprint('Stacking Regressor: Mean OOF RMSE = {}'.format(np.mean(oof_rmses)))","ca3f9144":"submission = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmission.target = np.mean(stacking_preds,0)\nprint(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","7ed17dc2":"The goal of this project is to recognize the complexity of a piece of text by applying artificial intelligence skills. Submissions for this competition are scored on the root mean squared error (RMSE). Two datasets are used in this project: Competition Dataset and Augmented Dataset. Competition Dataset is provided by the Kaggle, which contains excerpts from different articles. As for the Augmented Dataset, it contains more content from the original article rather than Competition Dataset. In this project, we have tried and explored a variety of methods. \n\nFor the part of Deep Learning methods, we have tried to compare the performance between BERT and RoBERTa models. The result demonstrates that RoBERTa model could get a better score in this competition. We have trained a total amount of 19 models until now and named them from model 1 to model 19. Then we employed a scheme that implements a stacking ensemble of model 4 and model 19. Model 4 is a RoBERTa-base model, pre-trained with Competition Dataset, while model 19 is a RoBERTa-large model, pre-trained with Augmented Dataset. In the fine-tuning stage, 5-fold cross-validation is applied. To be specific, we will have 5 models after the fine-tuning, each one is trained by 4 folds and use the fold that does not participate in training to do evaluation. Next, two levels are designed for the stacking ensemble. At the base level, the RoBERTa-base and RoBERTa-large model with the highest RMSE score is chosen. At the meta-level, Ridge is used as the Regressor. \n\nIt is worth mentioning that all models we trained do not perform well in the 4th fold. To deal with this problem, we proposed a new model that fits well with the content of training data, which extracts the embeddings from RoBERTa-large and applies SVM for regression tasks. Then the prediction of this model is used to blend the 4th fold in all models at the base level. Our goal is to ensure that our final ensemble predictions are slightly better but not over-fitting. From the results, we can see that the method works and gets the highest score in many experiments.\n\nConsequently, the final RMSE score of our team is 0.457, which ranked 88th in the competition and won the silver medal.\n","a9725ab4":"# Stacking Ensemble","8bd6274c":"# SVM","26a659a3":"# Blending 4th fold on training dataset","8736fb0f":"# Model 4","4687c5b4":"# Model 19","995499af":"# Thanks for the ideas of @algernone,@Maunish dave,@tutty1992. Our team could never get this score without their help.\n\n"}}