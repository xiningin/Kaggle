{"cell_type":{"ed5354fb":"code","099ce36e":"code","24bae0ef":"code","45203f7a":"code","5034cb7d":"code","a4fdf07b":"code","9e20b4e6":"code","7a324e8e":"code","6a3bb5ca":"code","c9549681":"code","c799ee36":"code","12d1d3a3":"code","75d78b86":"code","eaf0e978":"code","a233940f":"code","dd838b0a":"code","2fb291fb":"code","fcb9a99b":"code","9d5e206d":"code","6df550ad":"code","e2f2ce62":"code","f11fc486":"code","3d7b6a38":"code","9524448f":"code","f8285961":"code","59fd4cae":"code","fcd20179":"code","a00da4b2":"code","98745097":"code","2a99c541":"code","a2f78e04":"code","5e6928cb":"code","05d3ffe7":"code","1ef5cdcb":"code","fd7656a9":"code","17d11e6d":"code","6de24da1":"code","095116fe":"code","56ef3224":"code","f464f040":"code","69129808":"code","e7a96a9e":"code","4d36e55e":"code","7a7549e2":"code","d04674c3":"code","d9252b67":"code","b23b1c95":"code","93f32d1e":"code","0a4dba7c":"code","b8493fc8":"code","30eb60ab":"code","3fb3c48f":"markdown","615738c0":"markdown","b57e8ca8":"markdown","860d29b3":"markdown","63621190":"markdown","42eb6fcf":"markdown","c3887ee8":"markdown","5df687d4":"markdown","d2474361":"markdown","38ef7b76":"markdown","122f7f5e":"markdown","4c9127f4":"markdown","bcd76590":"markdown","954ec59c":"markdown","3215b04b":"markdown","3c9095bd":"markdown","d0d338c5":"markdown","df24f513":"markdown","23a9abd6":"markdown","55a19887":"markdown","1c3c9d47":"markdown","ffcf3454":"markdown","2300d9c1":"markdown"},"source":{"ed5354fb":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report, confusion_matrix","099ce36e":"df=pd.read_csv('..\/input\/titanic\/train.csv')","24bae0ef":"df_test=pd.read_csv('..\/input\/titanic\/test.csv')","45203f7a":"df.describe(include='all')","5034cb7d":"df_test.describe(include='all')","a4fdf07b":"df.info()","9e20b4e6":"pd.isnull(df).sum()","7a324e8e":"df_test.info()","6a3bb5ca":"pd.isnull(df_test).sum()","c9549681":"def graficar(var):\n    plt.figure(figsize=(15,5))\n    \n    plt.subplot(1,2,1)\n    plt.title(var)\n    plt.hist(df[var]);\n    \n    plt.subplot(1,2,2)\n    plt.title(var)\n    plt.boxplot(df[var], vert=False);","c799ee36":"graficar('Pclass')","12d1d3a3":"graficar('Fare')","75d78b86":"df[df['Fare']>500]","eaf0e978":"graficar('SibSp')","a233940f":"cor=df.corr()","dd838b0a":"sns.heatmap(cor, annot=True)","2fb291fb":"df[['Sex','Survived']].groupby(['Sex']).mean()","fcb9a99b":"df[['Survived','Pclass']].groupby(['Pclass']).mean()","9d5e206d":"df[['Fare','Pclass']].groupby(['Pclass']).mean()","6df550ad":"df[['Pclass', 'Age']].groupby(['Pclass']).mean()","e2f2ce62":"df=df.drop(['Cabin', 'Ticket','Name'], axis=1)\n#testing data also\ndf_test=df_test.drop(['Cabin', 'Ticket','Name'], axis=1)","f11fc486":"sex=pd.get_dummies(df['Sex'],drop_first=True)\nembarked=pd.get_dummies(df['Embarked'], drop_first=True)\n#testing data also\nsex2=pd.get_dummies(df_test['Sex'],drop_first=True)\nembarked2=pd.get_dummies(df_test['Embarked'], drop_first=True)","3d7b6a38":"df.drop(['Sex','Embarked'],axis=1, inplace=True)\n#testing data also\ndf_test.drop(['Sex','Embarked'],axis=1, inplace=True)","9524448f":"df=pd.concat([df,sex,embarked], axis=1)\n#testing data also\ndf_test=pd.concat([df_test,sex2,embarked2], axis=1)","f8285961":"df.head()","59fd4cae":"df_test.head()","fcd20179":"cor=df.corr()\nsns.heatmap(cor, annot=True);","a00da4b2":"df.info()","98745097":"df_test.info()","2a99c541":"# Funci\u00f3n que recibe el dataframe y cambia los valores nulos de la columna 'Age' por los valores promedio de edad dependiendo de\n# la 'Pclass' del pasajero.\ndef fill_age(df):    \n    cont2=0\n    for i in pd.isna(df.loc[:,'Age']):\n        \n        if i:\n            \n            if df.loc[cont2,'Pclass']==1:\n                df.loc[cont2,'Age']=38\n            \n            elif df.loc[cont2,'Pclass']==2:\n                df.loc[cont2,'Age']=30\n                \n            else:\n                df.loc[cont2,'Age']=25\n            \n        cont2+=1   \n    return df","a2f78e04":"df=fill_age(df)\ndf_test=fill_age(df_test)","5e6928cb":"df_test[df_test['Fare'].isna()==True]","05d3ffe7":"df_test.loc[152,'Fare']=13.675550","1ef5cdcb":"df.info()","fd7656a9":"df_test.info()","17d11e6d":"X=df.drop(['Survived'],axis=1)\ny=df['Survived']","6de24da1":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=150)","095116fe":"modelo_Reg=LogisticRegression()","56ef3224":"modelo_Reg_E=modelo_Reg.fit(X_train,y_train)","f464f040":"y_pred=modelo_Reg_E.predict(X_test)","69129808":"print(confusion_matrix(y_pred,y_test))","e7a96a9e":"print(classification_report(y_pred,y_test))","4d36e55e":"modelo_NB=GaussianNB()\nmodelo_NB_E=modelo_NB.fit(X_train,y_train)\ny_pred_NB=modelo_NB_E.predict(X_test)","7a7549e2":"print(classification_report(y_test,y_pred_NB))","d04674c3":"print(confusion_matrix(y_test,y_pred_NB))","d9252b67":"y_pred2=modelo_Reg_E.predict(df_test)","b23b1c95":"column=pd.DataFrame(y_pred2,columns=['Survived'])","93f32d1e":"final=df_test[['PassengerId']]","0a4dba7c":"final=pd.concat([final,column],axis=1)    ","b8493fc8":"compression_opts = dict(method='zip',\n                     archive_name='out.csv')  \nfinal.to_csv('out.zip', index=False,\n          compression=compression_opts)  ","30eb60ab":"final","3fb3c48f":"Also from the Fare plots and the df.describe(), 75% of the passengers paid a fee less than 31 dolars.","615738c0":"From the Pclass histogram and boxplot we notice that the largest number of passengers traveled in Pclass 3 ","b57e8ca8":"At this point we are almost done with the cleaning process but notice that there are a lot of null data in the 'Age' column, so we will create a function that fill the null data with the average age of the passenger respective Pclass. The avarege age of the Pclass is:$$$$ 38 for Pclass 1 $$$$ 30 for Pclass 2 $$$$ 25 for Pclass 3 $$$$ We got that information in the .groupby().mean() analysis.$$$$ Also in df_test there is one null value in the 'Fare' column. To fix this we will get the Pclass of the passegner and fill the null value with the averege of the Fare corresponding to the passegners Pclass.","860d29b3":"This tells us that 74.2038% of women and 18.8908% of male in this dataset survived.","63621190":"Now that we fill the null values from the 'Age' column, we will fill the missing value in the 'Fare' column of the df_test dataframe. $$$$ First we detect the passenger with the mising value.","42eb6fcf":"# Naive Bayes","c3887ee8":"Now that we have done the data analysis we will proced with data cleaning; that is, give the best possible data to feed our model.$$$$\nWe sill start by droping the columns that don\u00b4t give us usefull information.\n","5df687d4":"From .info() we notice a lot of null data.","d2474361":"We will now use .groupby().mean() to determine the mean of the passengers in some groups.","38ef7b76":"# Titanic - Machine Learning from Disaster\n## Logistic Regression and Naive Bayes Analysis\n\nWe will start the notebook by importing our work enviroment and reading the traning,testing data.","122f7f5e":"Now that we have the information of 'Sex' in a new column 'male' and 'Embarqued' with their new respective columns, we drop 'Sex' and 'Embarqued' and append the new columns.","4c9127f4":"For visualization we will use matplotlib and seaborn libraries.","bcd76590":"# Logistic Regression\nNow we just have to train our modelo with logistic regression algorithm","954ec59c":"Model is making good predictions.$$$$ We will proceed to create a new model but with the Naive-Bayes algorithm.","3215b04b":"Also 62.9630% of the passengers in Pclass 1 , 47.2826% of the passengers in Pclass 2 and 24.2363% of the passengers in Pclass 3 survived.","3c9095bd":"Is the passenger 1044, corresponding to Pclass 3. We got the average fare by Pclass with the .groupby().mean() analysis, so for Pclass 3 the average fare is 13.675550. Using .loc[] we fill that missing value.","d0d338c5":"We use .get_dummies() to separate one column in diferent columns with the information and characteristics of the initial column. So we will apply .get_dummies() to the columns 'Sex' and 'Embarqued' so we can have that information in numbers.","df24f513":"Using Seaborn we can plot a heatmap. in this heatmap we notice that 'Survived' is somehow related with 'Fare' giving us to think that those who paid a more expensive fare\nare the ones who were more likely to survive.","23a9abd6":"We confirm there are no missing values.$$$$ Now we will split the data into training and testing.We will use train_test_split()","55a19887":"With a simple manipulation in pandas we can observe who are the ones paying more tha 500 dolars of fare.","1c3c9d47":"New heatmap to visualize the new columns and how they relate","ffcf3454":"Before we start with the cleaning of the data we will do some visualization and find paterns.","2300d9c1":"As we can see, Logistic Regression performs better than Naive-Bayes. So we are going to use Linear Regression predictions in df_test"}}