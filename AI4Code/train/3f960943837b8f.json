{"cell_type":{"5a736299":"code","031445d0":"code","60c73e49":"code","6892268b":"code","50cb4b45":"code","011746b0":"code","2a05c58c":"code","99c709e0":"code","f2435566":"code","bbcfdcfa":"code","5700f89b":"code","56c018ab":"markdown","2bea69cb":"markdown","e0270266":"markdown","393e3626":"markdown","5160ed4e":"markdown","b3f694a8":"markdown","53e28bbc":"markdown","9e0407fb":"markdown","509ad076":"markdown"},"source":{"5a736299":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import  cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nimport warnings","031445d0":"class drop_col_transformer(BaseEstimator, TransformerMixin):\n    \n    '''\n    BaseEstimator just gives it the get_params and set_params methods that all Scikit-learn estimators require\n    TransformerMixin gives it the fit_transform method\n    '''\n    \n    def __init__(self,columns):\n        self.columns=columns\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X,y=None): \n        print(self.columns)\n        return X.drop(self.columns,axis=1)\n    \n    \n","60c73e49":"class dummify(BaseEstimator, TransformerMixin):\n    '''\n    Wrapper for get dummies\n    '''\n    def __init__(self, drop_first=False, match_cols=True):\n        self.drop_first = drop_first\n        self.columns = []  # useful to well behave with FeatureUnion\n        self.match_cols = match_cols\n\n    def fit(self, X, y=None):\n        self.columns = []  # for safety, when we refit we want new columns\n        print('fit')\n        return self\n    \n    def match_columns(self, X):\n        miss_train = list(set(X.columns) - set(self.columns))\n        miss_test = list(set(self.columns) - set(X.columns))\n        print(self.columns)\n        \n        err = 0\n        \n        if len(miss_test) > 0:\n            for col in miss_test:\n                X[col] = 0  # insert a column for the missing dummy\n                err += 1\n        if len(miss_train) > 0:\n            for col in miss_train:\n                del X[col]  # delete the column of the extra dummy\n                err += 1\n                \n        if err > 0:\n            warnings.warn('The dummies in this set do not match the ones in the train set, we corrected the issue.',\n                         UserWarning)\n            \n        return X\n        \n    def transform(self, X, y=None):\n        X = pd.get_dummies(X, drop_first=self.drop_first)\n        \n        if (len(self.columns) > 0): \n            if self.match_cols:\n                X = self.match_columns(X)\n            self.columns = X.columns\n        else:\n            self.columns = X.columns\n        return X\n    \n    def get_features_name(self):\n        return self.columns\n\n    \n# https:\/\/www.kaggle.com\/lucabasa\/understand-and-use-a-pipeline\/notebook\n# https:\/\/githu.com\/lucabasa\/tubesML\/blob\/main\/tubesml\/dummy.py\n\n# tmp = train_set[['RoofMatl']].copy()\n# dummifier = dummify()\n# dummifier.fit_transform(tmp).sum() \n# >>>\n    # RoofMatl_ClyTile       1\n    # RoofMatl_CompShg    1145\n    # RoofMatl_Membran       1\n    # RoofMatl_Roll          1\n    # RoofMatl_Tar&Grv      11\n    # RoofMatl_WdShake       3\n    # RoofMatl_WdShngl       6\n\n# tmp = test_set[['RoofMatl']].copy()\n# dummifier.transform(tmp).sum()  # the same instance as before\n# >>>\n    # \/opt\/conda\/lib\/python3.6\/site-packages\/ipykernel_launcher.py:90: UserWarning: The dummies in this set do not match the ones in the train set, we corrected the issue.\n    # RoofMatl_CompShg    289\n    # RoofMatl_WdShake      2\n    # RoofMatl_Tar&Grv      0\n    # RoofMatl_WdShngl      0\n    # RoofMatl_Roll         0\n    # RoofMatl_ClyTile      0\n    # RoofMatl_Membran      0","6892268b":"raw_data = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\nraw_data.head()","50cb4b45":"# If \u2018coerce\u2019, then invalid parsing will be set as NaN.\nraw_data['TotalCharges'] = pd.to_numeric(raw_data.TotalCharges, errors='coerce')\n\n# drop NaN\nraw_data = raw_data.dropna(axis=0)\nraw_data.info()","011746b0":"X = raw_data.drop(['customerID', 'Churn'], axis=1)\n\nle = LabelEncoder()\ny = le.fit_transform(raw_data['Churn']) # raw_data['Churn']\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","2a05c58c":"num_attribs = [\"MonthlyCharges\", \"TotalCharges\", \"tenure\"]\n\ncat_attribs = [\"SeniorCitizen\", \"gender\", \"Partner\", \"Dependents\",\n               \"PhoneService\", \"MultipleLines\", \"InternetService\", 'OnlineSecurity', \n               'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', \n               'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()) # check Normalize - non-gaussian distribution.\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    #('dummify', dummify(drop_first=True)), #error 1-dimension. dummify return dataframe.\n    ('onehot', OneHotEncoder(drop='first'))\n    \n    # how to manage target \n])\n\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, num_attribs),\n    ('cat', categorical_transformer, cat_attribs)\n])\n\n\nrf = Pipeline(steps=[('preprocessor', preprocessor),\n#                      ('classifier', RandomForestClassifier(random_state=42))\n                     ('clfier_lg', LogisticRegression(random_state=42))\n                    ])\n\n\nrf.fit(X_train, y_train)\n\n\ny_pred = rf.predict(X_test)\n\n# accuracy.\nprint('Model score: %.4f' %rf.score(X_test, y_test))\n\nplot_confusion_matrix(rf, \n                      X_test, \n                      y_test, \n                     # display_labels=[\"Does not have HD\",\"Has HD\"]\n                     )\nplt.show()\n\n# Precision: Class 1: Churn yes => TP\nprint('precision: ', precision_score(y_test, y_pred, average='binary'))\n\n\n# reference https:\/\/medium.com\/vickdata\/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n\n# rf.get_params\n# rf.set_params(preprocessor__num__scaler__copy='True')","99c709e0":"clf_l = LogisticRegression(random_state=42)\nx_l = X_train[['SeniorCitizen', 'tenure','MonthlyCharges','TotalCharges']]\n\nclf_l.fit(x_l,y_train)\nclf_l.score(x_l,y_train)\n\n# it seems that sklearn classifiers such as logistic regression can handle categorical target variable without encoding.","f2435566":"# ****************************************************************************\n# Classifier\n\nclassifier = []\nnp.random.seed(42) # scikit-learn and keras make use of np. #not work for different sessions\n\nclassifier.append(\n    ('Random Forest', Pipeline(steps=[('preprocessor', preprocessor),\n                                      ('clfier_rf', RandomForestClassifier(random_state=42))]))\n                 )\n\nclassifier.append(\n    ('Logistic Regression', Pipeline(steps=[('preprocessor', preprocessor),\n                                            ('clfier_lg', LogisticRegression(solver='liblinear',\n                                                                             random_state=42))]))\n                 )\n\nclassifier.append(\n    ('SVM', Pipeline(steps=[('preprocessor', preprocessor),\n                            ('clfier_svm', svm.SVC(random_state=42))]))\n                 )\n\nclassifier.append(\n    ('Decision Tree', Pipeline(steps=[('preprocessor', preprocessor),\n                            ('clfier_DT', DecisionTreeClassifier(random_state=42))]))\n                 )\n\n# ****************************************************************************\n# PARAMS\n\nrf_params_grid = {\n    #'clfier_rf__n_estimators': [int(x) for x in np.linspace(start=10, stop=80, num=10)],\n    #'clfier_rf__max_features': ['auto', 'sqrt'],\n    'clfier_rf__max_depth': [None, 2, 5, 10, 15],\n    'clfier_rf__min_samples_split': [2,5],\n    'clfier_rf__min_samples_leaf': [1,2,4],\n    #'clfier_rf__bootstrap': [True, False]\n}\n\nlg_params_grid = {}\n\nsvm_params_grid = {}\n\nDT_params_grid = {\n   'clfier_DT__max_depth': [5, 10, 15, 20, 25, 30]\n}\n\nparams_grid = [rf_params_grid, lg_params_grid, svm_params_grid,DT_params_grid]\n\n\n# ****************************************************************************\n# Grid Search\ncv = StratifiedKFold(n_splits=5) #10\ntest_score = ['split0_test_score', 'split1_test_score','split2_test_score','split3_test_score','split4_test_score',]\n             #'split5_test_score', 'split6_test_score','split7_test_score','split8_test_score','split9_test_score'] #, 'mean_test_score', 'std_test_score'\n\ntrain_score = [x.replace('test','train') for x in test_score]\n\nnames = []\ntrain_results, test_results = [], []\nscoring = 'f1' #'precision' #'recall' #'precision' #['precision','accuracy' ]# #'accuracy' \ni=0\ngs_r = []\nfor name, model in classifier: \n    \n    grid_s = GridSearchCV(estimator=model, \n                           param_grid=params_grid[i], \n                           scoring=scoring,\n#                           refit = 'precision',\n                           cv=cv,\n                           return_train_score = True)\n    i += 1\n    \n    grid_s.fit(X_train, y_train)\n    gridsearch_cv_result_df = pd.DataFrame(grid_s.cv_results_).sort_values(by='rank_test_score')\n    \n    gs_r.append(gridsearch_cv_result_df)\n    \n    test_result = gridsearch_cv_result_df.iloc[0][test_score].values\n    train_result = gridsearch_cv_result_df.iloc[0][train_score].values\n\n    names.append(name)\n    test_results.append(list(test_result))\n    train_results.append(list(train_result))\n    \n\n    \n# ****************************************************************************\n# ****************************************************************************\n# Organization for plotting the scores.     \n\na = pd.DataFrame(train_results).T\nb = pd.DataFrame(test_results).T\na.columns = names\nb.columns = names\n\nprint('Train Score Description per ML algorithm: \\n', a.describe())\nprint('\\n\\nTest Score Description per ML algorithm: \\n', b.describe())\n\na['set'] = 'train'\nb['set'] = 'validation'\n\nc = a.append(b).reset_index().drop('index', axis=1)\n\n# ****************************************************************************\n# for two or more algorithm info.\n\ni=0\ne = pd.DataFrame()\nfor algorithm in names:\n    d = pd.DataFrame()\n    d = pd.DataFrame(c.iloc[:,[i,4]].values,\n                     columns=['score', 'set']\n                    )\n    d['algorithm'] = str(algorithm)\n    e = e.append(d)\n    i+=1\n\n# ****************************************************************************\n# Plot for two or more algorithm.\n\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\nplt.subplots(dpi=120)\ng = sns.boxplot(x=\"algorithm\", y=\"score\",\n                hue=\"set\", palette=[\"m\", \"g\"], showmeans=True,\n                meanprops={\"marker\":\"D\",\n                           \"markerfacecolor\":\"red\", \n                           \"markeredgecolor\":\"red\",\n                           \"markersize\":\"4\"},\n                data=e)\nplt.title('Classifier Algorithm Comparison')\ng.set_xticklabels([x.replace(\" \", \"\\n\") for x in names])\nplt.legend(bbox_to_anchor=(1.3, 1), loc='upper right', title='set')\nsns.despine(offset=10, trim=True)\nplt.show()\n\n\n# ****************************************************************************\n# OVERFITTING CHECK ? I used this to use rank 22 for random forest algorithm.\n# k =   gs_r[0]\n# k['over']=k['mean_train_score'] - k['mean_test_score']\n# k[['mean_train_score','mean_test_score','over','rank_test_score']].sort_values(by='over')","bbcfdcfa":"grid_s.best_params_\nprint(gs_r[0]['params'].iloc[0])\n\ngs_r[0]\nX_train\nX_train.shape\nlen(y_train)","5700f89b":"rf = Pipeline(steps=[('preprocessor', preprocessor),\n                     ('classifier', RandomForestClassifier(max_depth= 2, #2,#10, \n                                                           min_samples_leaf=1, #2,#1,\n                                                           min_samples_split=2, #5,#2,\n                                                           random_state=42))\n                    ])\n\n\nrf.fit(X_train, y_train)\n\ny_pred = rf.predict(X_test)\n\n# accuracy.\nprint('Accuracy: %.4f' %rf.score(X_test, y_test))\n\nplot_confusion_matrix(rf, \n                      X_test, \n                      y_test, \n                     # display_labels=[\"Does not have HD\",\"Has HD\"]\n                     )\nplt.show()\n\n# Precision: Class 1: Churn yes => TP\nprint('precision: ', precision_score(y_test, y_pred, average='binary'))\nprint('recall: ', recall_score(y_test, y_pred, average='binary'))","56c018ab":"# Perfomance evaluation with test set","2bea69cb":"## Question\/ Considerations.\n\nCross-Validation is a robust way to assest generalization perfomance.\n\n\n* Scaler changes with each fold of the CV? if each k-fold model has different scaler, is it fair to compare them? how I make each k-fold model similar?\n    * gridsearch uses StratifiedKFold as default strategy for spliting.\n    * I need to check the data of each fold. \n* Can we One hot encode the target variable within the pipeline? Even though, the sklearn model can handle categorical target, metrics such as precision need a numeric target\n* GridSearch rank best model based on the best mean_test_score (validation_score), but it does not considered overfitting between Validation and train set? how to deal with that?\n\n* Check the best model selection based on the difference between train_score and validation_score.\n\n* **Selecting the metric**\\\n    Accuracy: imbalance data. \\\n    Precision: it is only based on the predictive power of the model, it means that if the model only predict 1 churned customer and 0 false positive then precision = 100%. (I need sensitivity here to select the best model)\\\n        Since I am using gridsearch, after class stratification in each fold, it seems that the model wasn't able to detect any churn customer in random forest obtaining precision = 0%. This is imbalance data and I am using 10 fold, it could be a reason why some model are not able to predict churn customer.\n        \n    Recall - if I use recall, it not considered the false positive. That could be an issue if the final idea is related to generate a promotion (invest money) on those customer who are going to churned.\n        Focus only in recall afect precision. \n        \n","e0270266":"Pipeline steps are executed serially, where the output from the first step is passed to the second step, and so on. ColumnTransformers are different in that each step is executed separately, and the transformed features are concatenated at the end.","393e3626":"# This notebook is not finish\n\n## Question\/ Considerations.\n\n* Precision(Prediction Rate) in final evaluation (test set) >> The issue is that it only considers Positive prediction. If 352 customers churned, but the model only predicts correctly 10 customers, and 1 prediction is a false positive; considering that precision = TP\/(TP + FP) = (10)\/(10+1) = 91%, this is an issue because even though 90% of its positive prediction are correct, it is only able to detect around 2% of the churn (Sensitivity = Recall)\n\n\n\n\n\nTo-do. \n- Check a way to visualize data fold in gridsearchcv.\n- Check a way to considered overfitting to select the best model in gridsearch cv.","5160ed4e":"## drop_col_transformer()","b3f694a8":"# Import Libraries","53e28bbc":"From previous EDA - Part1\n    Making correction of Dtype\n    Drop NaN values. \n","9e0407fb":"## Dummify()","509ad076":"# Utility functions"}}