{"cell_type":{"83fc193f":"code","400e33a8":"code","2c379083":"code","1b203bb6":"code","b11605ba":"code","50ad4da5":"code","1618ea7c":"code","87f3fdd2":"code","3f5433c8":"code","f51f618f":"code","acff9117":"code","200d5618":"code","e5a70892":"code","53c8ede6":"code","698887b1":"code","6bade44a":"code","b68aa407":"code","0370e126":"code","4a361ace":"code","aafa0b3b":"code","4eacc001":"code","f8670ef4":"code","ea2b551c":"code","f428bfc4":"code","0c6cdd69":"code","43fb2bc8":"code","7048506f":"code","cafe78d4":"code","eefcbc9c":"code","75870967":"code","207f8b64":"code","27cec28a":"code","9c47b2aa":"code","d721e693":"code","a7cf72aa":"code","5d31fdad":"code","c933ab32":"code","b334ec8b":"code","64026a03":"markdown","a30c98d4":"markdown","37d9654a":"markdown","0f63a540":"markdown","29306973":"markdown","6ca663c9":"markdown","766cfd45":"markdown","cfef6160":"markdown","06cbb7e5":"markdown","413441b3":"markdown","02f9f7d2":"markdown","c4a3c37f":"markdown","7c0dfe45":"markdown","eaf6cf2c":"markdown","c512ea11":"markdown","0243326f":"markdown","d903cc22":"markdown","6c8f512e":"markdown"},"source":{"83fc193f":"import os\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd \nimport datetime\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport lightgbm as lgb\n\nimport seaborn as sns\nimport matplotlib.patches as patch\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('max_columns', None)\n\nplt.style.use('bmh')\nplt.rcParams['figure.figsize'] = (10, 10)\ntitle_config = {'fontsize': 20, 'y': 1.05}\n","400e33a8":"IS_LOCAL = False\nif(IS_LOCAL):\n    PATH=\"inputs\/\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","2c379083":"%%time\ntrain = pd.read_csv(PATH+\"train.csv\")\ntest = pd.read_csv(PATH+\"test.csv\")","1b203bb6":"train.head()","b11605ba":"train.describe()","50ad4da5":"train.info()","1618ea7c":"test.head()","87f3fdd2":"test.describe()","3f5433c8":"test.info()","f51f618f":"col=train.columns[2:]\nscaler_df = preprocessing.normalize(train.iloc[:,2:], copy=True)\nscaled_df = pd.DataFrame(scaler_df,columns=col)\ntrain2=train.iloc[:,:2]\ntrain2[train.columns[2:]]=scaled_df\ntrain=train2\ndel train2, scaler_df, scaled_df\ntrain.head()","acff9117":"col=test.columns[1:]\nscaler_df = preprocessing.normalize(test.iloc[:,1:], copy=True)\nscaled_df = pd.DataFrame(scaler_df,columns=col)\ntest2=test.iloc[:,:1]\ntest2[test.columns[1:]]=scaled_df\ntest=test2\ndel test2, scaler_df, scaled_df\ntest.head()","200d5618":"col_var = train.columns[2:]\ndf = pd.DataFrame(col_var, columns=['feature'])\ndf['n_train_unique'] = train[col_var].nunique(axis=0).values\ndf['n_test_unique'] = test[col_var].nunique(axis=0).values\n\nfor i in df.index:\n    col = df.loc[i, 'feature']\n    df.loc[i, 'n_overlap'] = int(np.isin(train[col].unique(), test[col]).sum())\n    \ndf['origin_train']=df.n_train_unique-df.n_overlap\n\ndf['origin_test']=df.n_test_unique-df.n_overlap\ndf.T","e5a70892":"%%time\nfor feature in train.columns[2:]:\n    train[feature] = np.round(train[feature], 5)\n    test[feature] = np.round(test[feature], 5)","53c8ede6":"col_var = train.columns[2:]\ndf = pd.DataFrame(col_var, columns=['feature'])\ndf['n_train_unique'] = train[col_var].nunique(axis=0).values\ndf['n_test_unique'] = test[col_var].nunique(axis=0).values\n\nfor i in df.index:\n    col = df.loc[i, 'feature']\n    df.loc[i, 'n_overlap'] = int(np.isin(train[col].unique(), test[col]).sum())\n    \ndf['origin_train']=df.n_train_unique-df.n_overlap\n\ndf['origin_test']=df.n_test_unique-df.n_overlap\ndf.T","698887b1":"df = df.sort_values(by='n_train_unique').reset_index(drop=True)\ndf[['n_train_unique', 'n_test_unique', 'n_overlap']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)\nplt.yticks(df.index, df['feature'].values)\nplt.xlabel('n_unique', fontsize=20)\nplt.ylabel('feature', fontsize=20)\nplt.legend(loc='center right', fontsize=20)","6bade44a":"df = df.sort_values(by='n_train_unique').reset_index(drop=True)\ndf[['origin_train', 'origin_test']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)\nplt.yticks(df.index, df['feature'].values)\nplt.xlabel('n_unique', fontsize=20)\nplt.ylabel('feature', fontsize=20)\nplt.legend(loc='center right', fontsize=20)","b68aa407":"def inform_overlap(train,test,ans,silent=True):\n    variable=[c for c in train.columns if c not in ['ID_code','target']]\n    vari=[]\n    res_train_out=[]\n    res_test_out=[]\n    res_reclose_train=[]\n    res_reclose_test=[]\n    for var in variable:\n        vari.append(var)\n        if silent==False:\n            print ('\\n Calulate {}'.format(var))\n        valu = train[var].isin(test[var].value_counts().index)\n        rows = valu[valu==False].index\n        \n        df_include = train.drop(index=rows)\n        df_not_include = train.drop(index=df_include.index)\n        \n        res_train_out.append(len(df_not_include))\n        res_reclose_train.append(len(df_include))\n        \n        \n        valu = test[var].isin(train[var].value_counts().index)\n        rows = valu[valu==False].index\n        \n        df_include = test.drop(index=rows)\n        df_not_include = test.drop(index=df_include.index)\n        \n        res_test_out.append(len(df_not_include))\n        res_reclose_test.append(len(df_include))\n        \n    ans['fetures']=vari\n    \n    ans['train_out']=res_train_out\n    ans['reclose_train']=res_reclose_train\n    ans['test_out']=res_test_out\n    ans['reclose_test']=res_reclose_test\n    \n    return ans","0370e126":"%%time\nansver_orig = inform_overlap(train,test,pd.DataFrame())","4a361ace":"ansver_orig['dif_train']=ansver_orig.train_out\/ansver_orig.reclose_train\nansver_orig['dif_test']=ansver_orig.test_out\/ansver_orig.reclose_test\nansver_orig.T","aafa0b3b":"def train_to_test_aug(train,test,silent=True):\n    variable=[c for c in train.columns if c not in ['ID_code','target']]\n    \n    for var in variable:\n        if silent==False:\n            print ('\\n Calulate {}'.format(var))\n        valu = train[var].isin(test[var].value_counts().index)\n        rows = valu[valu==False].index\n        df_include = train.drop(index=rows)\n        df_not_include = train.drop(index=df_include.index)\n\n        df_include_True = df_include[df_include.target==True]\n        df_include_False = df_include[df_include.target==False]\n        df_not_include_True = df_not_include[df_not_include.target==True]\n        df_not_include_False = df_not_include[df_not_include.target==False]\n        tmp=df_include_True.copy()\n        for x in range(len(df_not_include_True)\/\/len(df_include_True)):\n            tmp=pd.concat([tmp,df_include_True],ignore_index=True)\n        \n        if silent==False:\n            print ('Target == True:')\n            print (\"Count row's not include: {} . Count row's exemple: {} .\".format(len( df_not_include_True[var]),\n                                                                                    len(df_include_True[var])))\n     \n        df_not_include_True[var]=tmp[var].sample(n=len(df_not_include_True[var])).tolist()\n        \n        \n        tmp=df_include_False.copy()\n        for x in range(len(df_not_include_False)\/\/len(df_include_False)+1):\n            tmp=pd.concat([tmp,df_include_False],ignore_index=True)\n        \n        if silent==False:\n            print ('Target == False:')\n            print (\"Count row's not include: {} . Count row's exemple: {} .\".format(len( df_not_include_False[var]),\n                                                                                    len(df_include_False[var])))\n        \n        df_not_include_False[var]=tmp[var].sample(n=len(df_not_include_False[var])).tolist()\n        \n        train=pd.concat([df_include_True,df_include_False,df_not_include_True,df_not_include_False])\n        \n        train.sort_index(axis=0,inplace = True)\n    \n    return train","4eacc001":"%%time\nfake_train = train_to_test_aug(train, test)","f8670ef4":"col_var = fake_train.columns[2:]\ndf = pd.DataFrame(col_var, columns=['feature'])\ndf['n_fake_trane_unique'] = fake_train[col_var].nunique(axis=0).values\ndf['n_test_unique'] = test[col_var].nunique(axis=0).values\n\nfor i in df.index:\n    col = df.loc[i, 'feature']\n    df.loc[i, 'n_overlap'] = int(np.isin(fake_train[col].unique(), test[col]).sum())\ndf.T","ea2b551c":"df = df.sort_values(by='n_test_unique').reset_index(drop=True)\ndf[['n_fake_trane_unique', 'n_test_unique', 'n_overlap']].plot(kind='barh' ,figsize=(22, 100), fontsize=20, width=0.8)\nplt.yticks(df.index, df['feature'].values)\nplt.xlabel('n_unique', fontsize=20)\nplt.ylabel('feature', fontsize=20)\nplt.legend(loc='center right', fontsize=20)","f428bfc4":"%%time\nansver_fake = inform_overlap(fake_train,test,pd.DataFrame())","0c6cdd69":"ansver_fake['dif_train']=ansver_fake.train_out\/ansver_fake.reclose_train\nansver_fake['dif_test']=ansver_fake.test_out\/ansver_fake.reclose_test\nansver_fake.T","43fb2bc8":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([xs,xn])\n    y = np.concatenate([ys,yn])\n    return x,y","7048506f":"features= [c for c in fake_train.columns if c not in ['ID_code', 'target']]","cafe78d4":"%%time\n\nX_t, Y_t = augment(fake_train[features].values, fake_train['target'].values, 8)\n\ntr = pd.DataFrame(X_t)\n        \ntr = tr.add_prefix('var_')\n\ntr['target'] = Y_t","eefcbc9c":"param_pretrain = {\n    'bagging_freq': 1,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 100,\n    'min_sum_hessian_in_leaf': 40.0,\n    'num_leaves': 16,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1}","75870967":"%%time\nX_t, x_v = tr.iloc[:][features], fake_train.iloc[:][features]\nY_t, y_v = tr.iloc[:]['target'], fake_train.iloc[:]['target']","207f8b64":"%%time\nsn=datetime.datetime.now()\n\ntrain_res = np.zeros(len(X_t))\nvalid_res = np.zeros(len(x_v))\n\ntrn_data = lgb.Dataset(X_t, label=Y_t)\nval_data = lgb.Dataset(x_v, label=y_v)\nval_data2 = lgb.Dataset(train[features], label=train['target'])\n        \nnum_round = 60000\n        \nevals_result = {}\n        \nclf=lgb.train(param_pretrain, trn_data, num_round, valid_sets = [trn_data, val_data,val_data2], \n              verbose_eval=1000, early_stopping_rounds = 4000)","27cec28a":"%%time\nvalid_res = clf.predict(x_v, num_iteration=clf.best_iteration)\ndel1=datetime.datetime.now()-sn\nprint (\"AUG time :  {} . Valid score : {:<8.5f}\".format(str(del1), roc_auc_score(y_v,valid_res)))","9c47b2aa":"%%time\ntr_result=clf.predict(train[features], num_iteration=clf.best_iteration)\nprint (\"Train score : {:<8.5f}\".format(roc_auc_score(train['target'],tr_result)))","d721e693":"%%time\nfeat_importance_df = pd.DataFrame()\nfeat_importance_df[\"feature\"] = features\nfeat_importance_df[\"importance\"] = clf.feature_importance()","a7cf72aa":"%%time\n\nplt.figure(figsize=(14,36))\nsns.barplot(x=\"importance\", y=\"feature\", data=feat_importance_df.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance')\nplt.tight_layout()\nplt.savefig('fet_impotance.png')","5d31fdad":"%%time\nresult=clf.predict(test[features], num_iteration=clf.best_iteration)","c933ab32":"%%time\nsub_df = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub_df[\"target\"] = result\n\nsub_df.to_csv(\"lgb_fake_test_submission{}.csv\".format(422), index=False)","b334ec8b":"%%time\nsub_df = pd.DataFrame({\"ID_code\":train[\"ID_code\"].values})\nsub_df[\"target\"] = train[\"target\"].values\nsub_df[\"predict\"] = tr_result\nsub_df.to_csv(\"lgb_fake_train_submission{}.csv\".format(422), index=False)","64026a03":"Let's look on features importance:","a30c98d4":"Look at the data","37d9654a":"Hi all.\n\nI am a beginner in DS. I really try break 0.9 lvl, but it is over my skills. And my English is bad (\u0438\u0437 \u0431\u0435\u0434 \u0438 \u043e\u0433\u043e\u0440\u0447\u0435\u043d\u0438\u0439) =). Please, do not judge strictly.\n\nI hope you find this kernel is useful or at least naive.\n\nIn my kernel i use idea Jiwei Liu from [this kernel](https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment) about generation trane data by shuffle.\nAnd Spoiler Alert's idea that data in train and test data different. At [this kernel](https:\/\/www.kaggle.com\/triplex\/more-unique-values-in-train-set-than-test-set).","0f63a540":"look on overlap nunique train and test data","29306973":"Normalize data","6ca663c9":"It is a bad. Need to reduce the dimension. Let's round it.","766cfd45":"Let's look on overlap rows: ","cfef6160":"I will try to do more visualization later.\n\nThanks for attention!","06cbb7e5":"Can see original matter in trane and test. Good exam for model. ","413441b3":"I create 3 dataset: train - only augment data, valid - fake train data(not include in train) and valid2 - original train data (wich values not include in train and valid).","02f9f7d2":"Can see that fake train data overlap all rows. ","c4a3c37f":"Look on overlap fake train and test data.","7c0dfe45":"Let's create fake trane data without original matter. ","eaf6cf2c":"First, import the packages.","c512ea11":"Let's create a lot of data for train lgb. Augment data not include first data.","0243326f":"3 symbol doing all variables as categorical.\n5 just right. ","d903cc22":"Getting data","6c8f512e":"This score is most Interesting, because original data same have original value  as in test data."}}