{"cell_type":{"3f1f295c":"code","ffee37ec":"code","d2a338c2":"code","da2415ed":"code","e3d5fddc":"code","7d44dd1a":"code","3a65f5b2":"code","6d9fef4e":"code","fe202b44":"code","31ffa37a":"code","55e5db57":"code","69981a07":"code","42735298":"code","e58ae8e0":"code","c1326b18":"code","821cd3f7":"code","c81160ac":"code","b2f6965e":"code","9345befc":"code","dfe7623c":"markdown","331c92e4":"markdown","7d71809d":"markdown","15d485cf":"markdown","bc0d679b":"markdown"},"source":{"3f1f295c":"import os\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import probplot\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\nimport torch.nn as nn\nimport torch.optim as optim","ffee37ec":"df_train = pd.read_csv('..\/input\/widsdatathon2022\/train.csv')\ndf_test = pd.read_csv('..\/input\/widsdatathon2022\/test.csv')\n\nprint(f'Training Set Shape: {df_train.shape} - Memory Usage: {df_train.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Test Set Shape: {df_test.shape} - Memory Usage: {df_test.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","d2a338c2":"def visualize_feature_importance(df_feature_importance):\n\n    df_feature_importance.sort_values(by='Importance', inplace=True, ascending=False)\n\n    fig, ax = plt.subplots(figsize=(24, 48))       \n    sns.barplot(x='Importance', y=df_feature_importance.index, data=df_feature_importance, palette='Blues_d', ax=ax)\n    ax.set_xlabel('')\n    ax.tick_params(axis='x', labelsize=20)\n    ax.tick_params(axis='y', labelsize=20)\n    ax.set_title(f'Feature Importance', size=20, pad=20)\n    plt.show()\n\n    \ndef visualize_predictions(train_labels, train_predictions, test_predictions):\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6))                                            \n    sns.scatterplot(train_labels, train_predictions, ax=axes[0])\n    sns.distplot(train_predictions, label='Train Predictions', ax=axes[1])\n    sns.distplot(test_predictions, label='Test Predictions', ax=axes[1])\n\n    axes[0].set_xlabel(f'Train Labels', size=18)\n    axes[0].set_ylabel(f'Train Predictions', size=18)\n    axes[1].set_xlabel('')\n    axes[1].legend(prop={'size': 18})\n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=15)\n        axes[i].tick_params(axis='y', labelsize=15)\n    axes[0].set_title(f'Train Labels vs Train Predictions', size=20, pad=20)\n    axes[1].set_title(f'Train and Test Predictions\\' Distributions', size=20, pad=20)\n\n    plt.show()\n\n\ndef visualize_target(df, feature):\n    \n    print(f'{feature}\\n{\"-\" * len(feature)}')\n        \n    print(f'Mean: {df[feature].mean():.4f}  -  Median: {df[feature].median():.4f}  -  Std: {df[feature].std():.4f}')\n    print(f'Min: {df[feature].min():.4f}  -  25%: {df[feature].quantile(0.25):.4f}  -  50%: {df[feature].quantile(0.5):.4f}  -  75%: {df[feature].quantile(0.75):.4f}  -  Max: {df[feature].max():.4f}')\n    print(f'Skew: {df[feature].skew():.4f}  -  Kurtosis: {df[feature].kurtosis():.4f}')\n    missing_count = df[df[feature].isnull()].shape[0]\n    total_count = df.shape[0]\n    print(f'Missing Values: {missing_count}\/{total_count} ({missing_count * 100 \/ total_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100)\n\n    sns.distplot(df[feature], label=feature, ax=axes[0])\n    axes[0].axvline(df[feature].mean(), label='Mean', color='r', linewidth=2, linestyle='--')\n    axes[0].axvline(df[feature].median(), label='Median', color='b', linewidth=2, linestyle='--')\n    axes[0].legend(prop={'size': 15})\n    probplot(df[feature], plot=axes[1])\n    \n    for i in range(2):\n        axes[i].tick_params(axis='x', labelsize=12.5)\n        axes[i].tick_params(axis='y', labelsize=12.5)\n        axes[i].set_xlabel('')\n        axes[i].set_ylabel('')\n    axes[0].set_title(f'{feature} Distribution', fontsize=15, pad=12)\n    axes[1].set_title(f'{feature} Probability Plot', fontsize=15, pad=12)\n    \n    plt.show()\n\n    \ndef visualize_continuous_feature(feature):\n            \n    print(f'{feature}\\n{\"-\" * len(feature)}')\n\n    print(f'Training Mean: {df_train[feature].mean():.4f}  - Median: {df_train[feature].median():.4f} - Std: {df_train[feature].std():.4f}')\n    print(f'Test Mean: {df_test[feature].mean():.4f}  - Median: {df_test[feature].median():.4f} - Std: {df_test[feature].std():.4f}')\n    print(f'Training Min: {df_train[feature].min():.4f}  - Max: {df_train[feature].max():.4f}')\n    print(f'Test Min: {df_test[feature].min():.4f}  - Max: {df_test[feature].max():.4f}')\n    print(f'Training Skew: {df_train[feature].skew():.4f}  - Kurtosis: {df_train[feature].kurtosis():.4f}')\n    print(f'Test Skew: {df_test[feature].skew():.4f}  - Kurtosis: {df_test[feature].kurtosis():.4f}')\n    training_missing_count = df_train[df_train[feature].isnull()].shape[0]\n    test_missing_count = df_test[df_test[feature].isnull()].shape[0]\n    training_total_count = df_train.shape[0]\n    test_total_count = df_test.shape[0]\n    print(f'Training Missing Values: {training_missing_count}\/{training_total_count} ({training_missing_count * 100 \/ training_total_count:.4f}%)')\n    print(f'Test Missing Values: {test_missing_count}\/{test_total_count} ({test_missing_count * 100 \/ test_total_count:.4f}%)')\n\n    fig, axes = plt.subplots(ncols=2, figsize=(24, 6), dpi=100, constrained_layout=True)\n    \n    # Continuous Feature Training and Test Set Distribution\n    sns.distplot(df_train[feature], label='Training', ax=axes[0])\n    sns.distplot(df_test[feature], label='Test', ax=axes[0])\n    axes[0].set_xlabel('')\n    axes[0].tick_params(axis='x', labelsize=12.5)\n    axes[0].tick_params(axis='y', labelsize=12.5)\n    axes[0].legend(prop={'size': 15})\n    axes[0].set_title(f'{feature} Distribution in Training and Test Set', fontsize=15, pad=12)\n    \n    # Continuous Feature vs target\n    sns.scatterplot(df_train[feature], df_train['site_eui'], ax=axes[1])\n    axes[1].set_xlabel('')\n    axes[1].set_ylabel('')\n    axes[1].tick_params(axis='x', labelsize=12.5)\n    axes[1].tick_params(axis='y', labelsize=12.5)\n    axes[1].set_title(f'{feature} vs site_eui', fontsize=15, pad=12)\n    \n    plt.show()\n    \n    \ndef visualize_learning_curve(training_losses, validation_losses, title):\n\n    fig, ax = plt.subplots(figsize=(32, 8), dpi=100)\n    sns.lineplot(\n        x=np.arange(1, len(training_losses) + 1),\n        y=training_losses,\n        ax=ax,\n        label='train_loss'\n    )\n    sns.lineplot(\n        x=np.arange(1, len(validation_losses) + 1),\n        y=validation_losses,\n        ax=ax,\n        label='val_loss'\n    )\n    ax.set_xlabel('Epochs\/Steps', size=15, labelpad=12.5)\n    ax.set_ylabel('Loss', size=15, labelpad=12.5)\n    ax.tick_params(axis='x', labelsize=12.5, pad=10)\n    ax.tick_params(axis='y', labelsize=12.5, pad=10)\n    ax.legend(prop={'size': 18})\n    ax.set_title(title, size=20, pad=15)\n\n    plt.show()\n","da2415ed":"visualize_target(df_train, 'site_eui')","e3d5fddc":"continuous_features = [\n    'floor_area', 'year_built', 'energy_star_rating', 'ELEVATION',\n    'february_min_temp', 'january_avg_temp', 'january_max_temp',\n    'february_min_temp', 'february_avg_temp', 'february_max_temp',\n    'march_min_temp', 'march_avg_temp', 'march_max_temp',\n    'april_min_temp', 'april_avg_temp', 'april_max_temp',\n    'may_min_temp', 'may_avg_temp', 'may_max_temp',\n    'june_min_temp', 'june_avg_temp', 'june_max_temp',\n    'july_min_temp', 'july_avg_temp', 'july_max_temp',\n    'august_min_temp', 'august_avg_temp', 'august_max_temp',\n    'september_min_temp', 'september_avg_temp', 'september_max_temp',\n    'october_min_temp', 'october_avg_temp', 'october_max_temp',\n    'november_min_temp', 'november_avg_temp', 'november_max_temp',\n    'december_min_temp', 'december_avg_temp', 'december_max_temp',\n    'cooling_degree_days', 'heating_degree_days', 'precipitation_inches',\n    'snowfall_inches', 'snowdepth_inches', 'avg_temp',\n    'days_below_30F', 'days_below_20F', 'days_below_10F', 'days_below_0F',\n    'days_above_80F', 'days_above_90F', 'days_above_100F', 'days_above_110F'\n]\n\n#for feature in continuous_features:\n    #visualize_continuous_feature(feature)","7d44dd1a":"class PreprocessingPipeline:\n    \n    def __init__(self, train, test, n_splits, fill_missing, scale, verbose):\n        \n        self.train = train.copy(deep=True)\n        self.test = test.copy(deep=True)\n        self.n_splits = n_splits\n        self.fill_missing = fill_missing\n        self.scale = scale\n        self.verbose = verbose\n        \n    def get_folds(self):\n        \n        gkf = GroupKFold(n_splits=self.n_splits)\n        for fold, (_, val_idx) in enumerate(gkf.split(self.train, groups=self.train['Year_Factor']), 1):\n            self.train.loc[val_idx, 'fold'] = fold\n\n        self.train['fold'] = self.train['fold'].astype(np.uint8)\n        \n        if self.verbose:\n            print(f'\\nTraining set split into {self.n_splits} folds')\n            for fold in range(1, self.n_splits + 1):\n                df_fold = self.train[self.train['fold'] == fold]\n                print(f'Fold {fold} {df_fold.shape} - target mean: {df_fold[\"site_eui\"].mean():.4f} std: {df_fold[\"site_eui\"].std():.4f} min: {df_fold[\"site_eui\"].min():.4f} max: {df_fold[\"site_eui\"].max():.4f}')\n\n    def encode_categoricals(self):\n        \n        # Label encode raw categorical features\n        features_to_encode = ['State_Factor', 'building_class', 'facility_type']\n        for feature in features_to_encode:\n            le = LabelEncoder()\n            self.train[feature] = le.fit_transform(self.train[feature])\n            self.test[feature] = le.transform(self.test[feature])\n            \n    def clean_features(self):\n        \n        self.train.loc[self.train['year_built'] == 0, 'year_built'] = np.nan\n        self.test.loc[self.test['year_built'] == 0, 'year_built'] = np.nan\n        self.train.loc[self.train['energy_star_rating'] == 0, 'energy_star_rating'] = np.nan\n        self.test.loc[self.test['energy_star_rating'] == 0, 'energy_star_rating'] = np.nan\n        \n    def create_features(self):\n        \n        avg_temp_cols = [col for col in self.train.columns if col.endswith('_avg_temp')]\n        min_temp_cols = [col for col in self.train.columns if col.endswith('_min_temp')]\n        max_temp_cols = [col for col in self.train.columns if col.endswith('_max_temp')]\n        \n        for df in [self.train, self.test]:\n            \n            # Group facility_types\n            df['facility_type_grouped'] = df['facility_type'].apply(lambda x: str(x).split('_')[0])\n            df.loc[df['facility_type_grouped'] == 'Public', 'facility_type_grouped'] = df.loc[df['facility_type_grouped'] == 'Public', 'facility_type'].apply(lambda x: '_'.join(str(x).split('_')[:2]))\n            \n            # Yearly min and max temperatures\n            df['year_min_temp'] = df[min_temp_cols].min(axis=1)\n            df['year_max_temp'] = df[max_temp_cols].max(axis=1)\n            \n            # Unique building identifiers\n            df['building_id1.1'] = df['State_Factor'].astype(str) + '_' +\\\n                                   df['building_class'].astype(str)\n            df['building_id1.2'] = df['State_Factor'].astype(str) + '_' +\\\n                                   df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str)\n            df['building_id1.3'] = df['State_Factor'].astype(str) + '_' +\\\n                                   df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str)\n            df['building_id1.4'] = df['State_Factor'].astype(str) + '_' +\\\n                                   df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str)  + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str)\n            df['building_id1.5'] = df['State_Factor'].astype(str) + '_' +\\\n                                   df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str) + '_' +\\\n                                   df['energy_star_rating'].astype(str)\n            df['building_id1.6'] = df['State_Factor'].astype(str) + '_' +\\\n                                   df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str) + '_' +\\\n                                   df['energy_star_rating'].astype(str) + '_' +\\\n                                   df['ELEVATION'].astype(str)\n            \n            df['building_id2.1'] = df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str)\n            df['building_id2.2'] = df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str)\n            df['building_id2.3'] = df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str)\n            df['building_id2.4'] = df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str) + '_' +\\\n                                   df['energy_star_rating'].astype(str)\n            df['building_id2.5'] = df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str) + '_' +\\\n                                   df['energy_star_rating'].astype(str)\n            df['building_id2.6'] = df['building_class'].astype(str) + '_' +\\\n                                   df['facility_type'].astype(str) + '_' +\\\n                                   df['floor_area'].astype(str) + '_' +\\\n                                   df['year_built'].astype(str) + '_' +\\\n                                   df['energy_star_rating'].astype(str) + '_' +\\\n                                   df['ELEVATION'].astype(str)\n            \n        # Label encode unique building identifiers\n        df_all = pd.concat((self.train, self.test), axis=0, ignore_index=True)\n        features_to_encode = [\n            'facility_type_grouped',\n            'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4', 'building_id1.5', 'building_id1.6',\n            'building_id2.1', 'building_id2.2', 'building_id2.3', 'building_id2.4', 'building_id2.5', 'building_id2.6',\n        ]\n        for feature in features_to_encode:\n            le = LabelEncoder()\n            le.fit(df_all[feature])\n            self.train[feature] = le.transform(self.train[feature])\n            self.test[feature] = le.transform(self.test[feature])\n            \n    def fill_missing_values(self):\n        \n        df_all = pd.concat((self.train, self.test), axis=0, ignore_index=True)\n        df_all['energy_star_rating'] = df_all.groupby(['building_id1.4'], sort=False)['energy_star_rating'].apply(lambda x: x.fillna(x.mean()))\n        self.train['energy_star_rating'] = df_all.loc[:len(self.train) - 1, 'energy_star_rating'].values\n        self.test['energy_star_rating'] = df_all.loc[len(self.train):, 'energy_star_rating'].values\n        df_all['energy_star_rating'] = df_all.groupby(['building_id1.3'], sort=False)['energy_star_rating'].apply(lambda x: x.fillna(x.mean()))\n        self.train['energy_star_rating'] = df_all.loc[:len(self.train) - 1, 'energy_star_rating'].values\n        self.test['energy_star_rating'] = df_all.loc[len(self.train):, 'energy_star_rating'].values\n        df_all['energy_star_rating'] = df_all.groupby(['building_id1.2'], sort=False)['energy_star_rating'].apply(lambda x: x.fillna(x.mean()))\n        self.train['energy_star_rating'] = df_all.loc[:len(self.train) - 1, 'energy_star_rating'].values\n        self.test['energy_star_rating'] = df_all.loc[len(self.train):, 'energy_star_rating'].values\n        \n        df_all['year_built'] = df_all.groupby(['building_id1.3'], sort=False)['year_built'].apply(lambda x: x.fillna(x.mean()))\n        self.train['year_built'] = df_all.loc[:len(self.train) - 1, 'year_built'].values\n        self.test['year_built'] = df_all.loc[len(self.train):, 'year_built'].values\n        df_all['year_built'] = df_all.groupby(['building_id1.2'], sort=False)['year_built'].apply(lambda x: x.fillna(x.mean()))\n        self.train['year_built'] = df_all.loc[:len(self.train) - 1, 'year_built'].values\n        self.test['year_built'] = df_all.loc[len(self.train):, 'year_built'].values\n        df_all['year_built'] = df_all.groupby(['building_id1.1'], sort=False)['year_built'].apply(lambda x: x.fillna(x.mean()))\n        self.train['year_built'] = df_all.loc[:len(self.train) - 1, 'year_built'].values\n        self.test['year_built'] = df_all.loc[len(self.train):, 'year_built'].values\n        \n    def create_aggregations(self):\n        \n        df_all = pd.concat((self.train, self.test), axis=0, ignore_index=True)\n        for feature in [\n            'State_Factor', 'building_class', 'facility_type',\n            'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4', 'building_id1.5', 'building_id1.6',\n            'building_id2.1', 'building_id2.2', 'building_id2.3', 'building_id2.4', 'building_id2.5', 'building_id2.6',\n        ]:\n            feature_value_counts = df_all[feature].value_counts()\n            self.train[f'{feature}_value_counts'] = self.train[feature].map(feature_value_counts)\n            self.test[f'{feature}_value_counts'] = self.test[feature].map(feature_value_counts)\n            \n        for feature in [\n            'State_Factor', 'building_class', 'facility_type',\n            'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4',\n            'building_id2.1', 'building_id2.2', 'building_id2.3'\n        ]:\n            for aggregation in ['mean', 'std', 'min', 'max']:\n                energy_star_rating_aggregation = df_all.groupby(feature)['energy_star_rating'].agg(aggregation)\n                self.train[f'{feature}_energy_start_rating_{aggregation}'] = self.train[feature].map(energy_star_rating_aggregation).fillna(0)\n                self.test[f'{feature}_energy_start_rating_{aggregation}'] = self.test[feature].map(energy_star_rating_aggregation).fillna(0)\n                \n        for feature in ['State_Factor', 'building_class', 'facility_type']:\n            for aggregation in ['mean', 'std', 'min', 'max']:\n                floor_area_aggregation = df_all.groupby(feature)['floor_area'].agg(aggregation)\n                self.train[f'{feature}_floor_area_{aggregation}'] = self.train[feature].map(floor_area_aggregation).fillna(0)\n                self.test[f'{feature}_floor_area_{aggregation}'] = self.test[feature].map(floor_area_aggregation).fillna(0)\n                \n        for categorical_feature in [\n            'Year_Factor',\n            'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4', 'building_id1.5', 'building_id1.6',\n            'building_id2.1', 'building_id2.2', 'building_id2.3', 'building_id2.4', 'building_id2.5', 'building_id2.6',\n        ]:\n            for continuous_feature in ['cooling_degree_days', 'avg_temp', 'days_below_30F', 'days_above_80F']:\n                for aggregation in ['mean', 'std', 'min', 'max']:\n                    df_agg = df_all.groupby(categorical_feature)[continuous_feature].agg(aggregation)\n                    self.train[f'{categorical_feature}_{continuous_feature}_{aggregation}'] = self.train[categorical_feature].map(df_agg).fillna(0)\n                    self.test[f'{categorical_feature}_{continuous_feature}_{aggregation}'] = self.test[categorical_feature].map(df_agg).fillna(0)\n                    \n    def scale_features(self):\n        \n        df_all = pd.concat((self.train, self.test), axis=0, ignore_index=True)\n        categorical_features = [\n            'Year_Factor', 'State_Factor', 'building_class', 'facility_type', 'facility_type_grouped',\n            'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4', 'building_id1.5', 'building_id1.6',\n            'building_id2.1', 'building_id2.2', 'building_id2.3', 'building_id2.4', 'building_id2.5', 'building_id2.6'\n        ]\n        scale_features = [feature for feature in self.test.columns if feature not in categorical_features]\n        if self.scale == 'standard':\n            scaler = StandardScaler()\n        elif self.scale == 'min_max':\n            scaler = MinMaxScaler()\n        scaler.fit(df_all[scale_features].values)\n        self.train.loc[:, scale_features] = scaler.transform(self.train[scale_features].values)\n        self.test.loc[:, scale_features] = scaler.transform(self.test[scale_features].values)\n        \n    def one_hot_encode_categoricals(self):\n        \n        df_all = pd.concat((self.train, self.test), axis=0, ignore_index=True)\n        categorical_features = [\n            'State_Factor', 'building_class', 'facility_type',\n        ]\n        for feature in categorical_features:\n            one_hot_encoded = pd.get_dummies(df_all[feature], prefix=f'{feature}_Cat').astype(np.uint8).reset_index(drop=True)\n            self.train = pd.concat((self.train, one_hot_encoded.loc[:len(self.train) - 1, :].reset_index(drop=True)), axis=1)\n            self.test = pd.concat((self.test, one_hot_encoded.loc[len(self.train):, :].reset_index(drop=True)), axis=1)\n            \n    def transform(self):\n        \n        self.get_folds()\n        self.encode_categoricals()\n        self.clean_features()\n        self.create_features()\n        \n        if self.fill_missing:\n            self.fill_missing_values()\n        self.create_aggregations()\n        \n        if self.scale is not None:\n            self.scale_features()\n            \n        self.one_hot_encode_categoricals()\n        \n        return self.train, self.test\n","3a65f5b2":"preprocessor = PreprocessingPipeline(\n    train=df_train, test=df_test, n_splits=5,\n    fill_missing=True, scale='standard', verbose=True\n)\ndf_train_processed, df_test_processed = preprocessor.transform()\n\nprint(f'\\nProcessed Training Set Shape: {df_train_processed.shape} - Memory Usage: {df_train_processed.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\nprint(f'Processed Test Set Shape: {df_test_processed.shape} - Memory Usage: {df_test_processed.memory_usage().sum() \/ 1024 ** 2:.2f} MB')","6d9fef4e":"class LightGBMModel:\n    \n    def __init__(self, features, target, model_parameters, fit_parameters, categorical_features, seeds):\n        \n        self.features = features\n        self.target = target\n        self.model_parameters = model_parameters\n        self.fit_parameters = fit_parameters\n        self.categorical_features = categorical_features\n        self.seeds = seeds\n                \n    def train_and_predict(self, X_train, y_train, X_test):\n        \n        seed_avg_oof_predictions = np.zeros(X_train.shape[0])\n        seed_avg_test_predictions = np.zeros(X_test.shape[0])        \n        seed_avg_importance = pd.DataFrame(data=np.zeros(len(self.features)), index=self.features, columns=['Importance'])\n        \n        for seed in self.seeds:\n            print(f'{\"-\" * 30}\\nRunning LightGBM model with seed: {seed}\\n{\"-\" * 30}\\n')\n            self.model_parameters['seed'] = seed\n            self.model_parameters['feature_fraction_seed'] = seed\n            self.model_parameters['bagging_seed'] = seed\n            self.model_parameters['drop_seed'] = seed\n            self.model_parameters['data_random_seed'] = seed\n                \n            for fold in sorted(X_train['fold'].unique()):\n\n                trn_idx, val_idx = X_train.loc[X_train['fold'] != fold].index, X_train.loc[X_train['fold'] == fold].index\n                trn = lgb.Dataset(X_train.loc[trn_idx, self.features], label=y_train.loc[trn_idx], categorical_feature=self.categorical_features)\n                val = lgb.Dataset(X_train.loc[val_idx, self.features], label=y_train.loc[val_idx], categorical_feature=self.categorical_features)\n\n                model = lgb.train(\n                    params=self.model_parameters,\n                    train_set=trn,\n                    valid_sets=[trn, val],\n                    num_boost_round=self.fit_parameters['boosting_rounds'],\n                    early_stopping_rounds=self.fit_parameters['early_stopping_rounds'],\n                    verbose_eval=self.fit_parameters['verbose_eval']\n                )            \n\n                val_predictions = model.predict(X_train.loc[val_idx, self.features])\n                val_predictions = np.clip(val_predictions, a_min=1, a_max=1000)\n                seed_avg_oof_predictions[val_idx] += (val_predictions \/ len(self.seeds))\n                test_predictions = model.predict(X_test[self.features])\n                test_predictions = np.clip(test_predictions, a_min=1, a_max=1000)\n                seed_avg_test_predictions += (test_predictions \/ X_train['fold'].nunique() \/ len(self.seeds))\n                seed_avg_importance['Importance'] += (model.feature_importance(importance_type='gain') \/ X_train['fold'].nunique() \/ len(self.seeds))\n\n                fold_score = mean_squared_error(y_train.loc[val_idx], val_predictions, squared=False)\n                print(f'\\nLGB Fold {int(fold)} - X_trn: {X_train.loc[trn_idx, self.features].shape} X_val: {X_train.loc[val_idx, self.features].shape} - Score: {fold_score:.6f} - Seed: {seed}\\n')\n            \n        df_train_processed['lgb_predictions'] = seed_avg_oof_predictions\n        df_test_processed['lgb_predictions'] = seed_avg_test_predictions\n        oof_score = mean_squared_error(y_train, df_train_processed['lgb_predictions'], squared=False)\n        print(f'{\"-\" * 30}\\nLGB OOF RMSE: {oof_score:.6f} ({len(self.seeds)} Seed Average)\\n{\"-\" * 30}')\n        \n        visualize_feature_importance(seed_avg_importance)\n        visualize_predictions(df_train_processed[self.target], df_train_processed['lgb_predictions'], df_test_processed['lgb_predictions'])\n","fe202b44":"TRAIN_LGB = False\n\nif TRAIN_LGB:\n    \n    lgb_preprocessor = PreprocessingPipeline(train=df_train, test=df_test,n_splits=5, fill_missing=True, scale=None, verbose=False)\n    df_train_lgb, df_test_lgb = lgb_preprocessor.transform()\n\n    print(f'\\nLightGBM Training Set Shape: {df_train_lgb.shape} - Memory Usage: {df_train_lgb.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'LightGBM Test Set Shape: {df_test_lgb.shape} - Memory Usage: {df_test_lgb.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n\n    X_train_lgb = df_train_lgb.copy(deep=True)\n    y_train_lgb = df_train_lgb['site_eui'].copy(deep=True)\n    X_test_lgb = df_test_lgb.copy(deep=True)\n    \n    # LGB OOF RMSE: 29.762066 (1 Seed Average)\n    \n    features_lgb = [\n        'State_Factor', 'building_class', 'facility_type',\n        'floor_area', 'year_built', 'energy_star_rating',\n        'cooling_degree_days', 'avg_temp', 'days_below_30F', 'days_above_80F',\n        'State_Factor_value_counts', 'building_class_value_counts',\n        'facility_type_energy_start_rating_mean', 'facility_type_energy_start_rating_std',\n        'State_Factor_floor_area_min', 'State_Factor_floor_area_max',\n        'building_class_floor_area_mean', 'building_class_floor_area_std',\n        'building_class_floor_area_min', 'building_class_floor_area_max',\n        'Year_Factor_avg_temp_min', 'Year_Factor_avg_temp_max',\n        'facility_type_grouped',\n        'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4', 'building_id1.5', 'building_id1.6',\n        'building_id1.1_avg_temp_mean', 'building_id1.1_avg_temp_std',\n        'building_id1.2_avg_temp_mean', 'building_id1.2_avg_temp_std',\n        'building_id1.3_avg_temp_mean', 'building_id1.3_avg_temp_std',\n        'building_id1.4_avg_temp_mean', 'building_id1.4_avg_temp_std',\n        'building_id1.1_energy_start_rating_mean', 'building_id1.1_energy_start_rating_std',\n        'building_id1.1_energy_start_rating_min', 'building_id1.1_energy_start_rating_max',\n        'building_id1.2_energy_start_rating_mean', 'building_id1.2_energy_start_rating_std',\n        'building_id1.2_energy_start_rating_min', 'building_id1.2_energy_start_rating_max',\n        'building_id1.3_energy_start_rating_mean', 'building_id1.3_energy_start_rating_std',\n        'building_id1.3_energy_start_rating_min', 'building_id1.3_energy_start_rating_max',\n        'building_id1.4_energy_start_rating_mean', 'building_id1.4_energy_start_rating_std',\n        'building_id1.4_energy_start_rating_min', 'building_id1.4_energy_start_rating_max',\n    ]\n    categorical_features_lgb = [\n        'State_Factor', 'building_class', 'facility_type', 'facility_type_grouped',\n        'building_id1.1', 'building_id1.2', 'building_id1.3', 'building_id1.4', 'building_id1.5', 'building_id1.6'\n    ]\n\n    lgb_parameters = {\n        'features': features_lgb,\n        'target': 'site_eui',\n        'model_parameters': {\n            'num_leaves': 2 ** 7, \n            'learning_rate': 0.05,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 1,\n            'feature_fraction': 0.8,\n            'feature_fraction_bynode': 0.9,\n            'min_data_in_leaf': 10,\n            'min_gain_to_split': 0,\n            'min_data_per_group': 100,\n            'max_cat_threshold': 32,\n            'cat_l2': 10.0,\n            'cat_smooth': 10.0,\n            'max_cat_to_onehot': 4,\n            'lambda_l1': 0.1,\n            'lambda_l2': 0,\n            'max_bin': 255,\n            'max_depth': -1,\n            'objective': 'regression',\n            'seed': None,\n            'feature_fraction_seed': None,\n            'bagging_seed': None,\n            'drop_seed': None,\n            'data_random_seed': None,\n            'boosting_type': 'gbdt',\n            'verbose': 1,\n            'metric': 'rmse',\n            'n_jobs': -1,\n        },\n        'fit_parameters': {\n            'boosting_rounds': 20000,\n            'early_stopping_rounds': 500,\n            'verbose_eval': 500\n        },\n        'categorical_features': categorical_features_lgb,\n        'seeds': [42]\n    }\n\n    lgb_model = LightGBMModel(**lgb_parameters)\n    lgb_model.train_and_predict(X_train_lgb, y_train_lgb, X_test_lgb)\n\n    del df_train_lgb, df_test_lgb, X_train_lgb, y_train_lgb, X_test_lgb\n    del lgb_preprocessor, features_lgb, categorical_features_lgb, lgb_parameters, lgb_model\n    \n    print('Saving LightGBM Train and Test predictions to current working directory.')\n    df_train_processed[['id', 'lgb_predictions']].to_csv('lgb_oof_predictions.csv', index=False)\n    df_test_processed[['id', 'lgb_predictions']].to_csv('lgb_test_predictions.csv', index=False)\n    \nelse:\n    \n    print('Loading LightGBM Train and Test predictions from dataset.')\n    df_train_processed['lgb_predictions'] = pd.read_csv('..\/input\/wids-datathon-2022-dataset\/lgb_oof_predictions.csv')['lgb_predictions']\n    df_test_processed['lgb_predictions'] = pd.read_csv('..\/input\/wids-datathon-2022-dataset\/lgb_test_predictions.csv')['lgb_predictions']\n    oof_score = mean_squared_error(df_train_processed['site_eui'], df_train_processed['lgb_predictions'], squared=False)\n    print(f'{\"-\" * 30}\\nLGB OOF RMSE: {oof_score:.6f}\\n{\"-\" * 30}')\n","31ffa37a":"class TabularDataset(Dataset):\n\n    def __init__(self, features, targets=None):\n\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n\n        x = self.features[idx]\n        x = torch.as_tensor(x, dtype=torch.float)\n        \n        if self.targets is not None:\n            y = self.targets[idx]\n            y = torch.as_tensor(y, dtype=torch.float)\n            return x, y\n        else:\n            return x\n","55e5db57":"class DenseBlock(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim, output_dim):\n\n        super(DenseBlock, self).__init__()\n        self.dense_block = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim, bias=True),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim, bias=True),\n        )\n\n    def forward(self, x):\n\n        x = self.dense_block(x)\n        return x\n\n\nclass MultiLayerPerceptron(nn.Module):\n    \n    def __init__(self, input_dim):\n        \n        super(MultiLayerPerceptron, self).__init__()\n        \n        self.dense_block1 = DenseBlock(input_dim=input_dim, hidden_dim=128, output_dim=256)\n        self.dense_block2 = DenseBlock(input_dim=256, hidden_dim=384, output_dim=512)\n        self.dense_block3 = DenseBlock(input_dim=512, hidden_dim=768, output_dim=1024)\n        self.dense_block4 = DenseBlock(input_dim=1024, hidden_dim=768, output_dim=512)\n        self.dense_block5 = DenseBlock(input_dim=512, hidden_dim=384, output_dim=256)\n        self.dense_block6 = DenseBlock(input_dim=256, hidden_dim=128, output_dim=64)\n        self.linear = nn.Linear(64, 1)\n\n    def forward(self, x):\n        \n        x = self.dense_block1(x)\n        x = self.dense_block2(x)\n        x = self.dense_block3(x)\n        x = self.dense_block4(x)\n        x = self.dense_block5(x)\n        x = self.dense_block6(x)\n        output = self.linear(x)\n        \n        return output.view(-1)\n","69981a07":"def set_seed(seed, deterministic_cudnn=False):\n\n    if deterministic_cudnn:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n","42735298":"class NeuralNetworkTrainer:\n\n    def __init__(self, features, target, model_parameters, training_parameters):\n\n        self.features = features\n        self.target = target\n        self.model_parameters = model_parameters\n        self.training_parameters = training_parameters\n\n    def train_fn(self, train_loader, model, criterion, optimizer, device, scheduler=None):\n\n        print('\\n')\n        model.train()\n        progress_bar = tqdm(train_loader)\n        losses = []\n\n        for data, target in progress_bar:\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = torch.sqrt(criterion(target, output))\n            loss.backward()\n            optimizer.step()\n            if scheduler is not None:\n                scheduler.step()\n\n            losses.append(loss.item())\n            average_loss = np.mean(losses)\n            lr = scheduler.get_last_lr()[0] if scheduler is not None else optimizer.param_groups[0]['lr']\n            progress_bar.set_description(f'train_loss: {average_loss:.6f} - lr: {lr:.8f}')\n\n\n        train_loss = np.mean(losses)\n        return train_loss\n\n    def val_fn(self, val_loader, model, criterion, device):\n\n        model.eval()\n        progress_bar = tqdm(val_loader)\n        losses = []\n\n        with torch.no_grad():\n            for data, target in progress_bar:\n                data, target = data.to(device), target.to(device)\n                output = model(data)\n                loss = torch.sqrt(criterion(target, output))\n                losses.append(loss.item())\n                average_loss = np.mean(losses)\n                progress_bar.set_description(f'val_rmse: {average_loss:.6f}')\n\n        val_loss = np.mean(losses)\n        return val_loss\n    \n    def train_and_validate(self, df):\n\n        print(f'\\n{\"-\" * 30}\\nRunning Neural Network for Training\\n{\"-\" * 30}\\n')\n\n        for fold in sorted(df['fold'].unique()):\n\n            print(f'\\nFold {fold}\\n{\"-\" * 6}')\n\n            trn_idx, val_idx = df.loc[df['fold'] != fold].index, df.loc[df['fold'] == fold].index\n            train_dataset = TabularDataset(\n                features=df.loc[trn_idx, self.features].values,\n                targets=df.loc[trn_idx, self.target].values\n            )\n            train_loader = DataLoader(\n                train_dataset,\n                batch_size=self.training_parameters['batch_size'],\n                sampler=RandomSampler(train_dataset),\n                pin_memory=True,\n                drop_last=False,\n                num_workers=self.training_parameters['num_workers'],\n            )\n            val_dataset = TabularDataset(\n                features=df.loc[val_idx, self.features].values,\n                targets=df.loc[val_idx, self.target].values\n            )\n            val_loader = DataLoader(\n                val_dataset,\n                batch_size=self.training_parameters['batch_size'],\n                sampler=SequentialSampler(val_dataset),\n                pin_memory=True,\n                drop_last=False,\n                num_workers=self.training_parameters['num_workers'],\n            )\n\n            set_seed(self.training_parameters['random_state'], deterministic_cudnn=self.training_parameters['deterministic_cudnn'])\n            device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n            model = MultiLayerPerceptron(**self.model_parameters)\n            model = model.to(device)\n\n            criterion = nn.MSELoss()\n            optimizer = optim.Adam(\n                model.parameters(),\n                lr=self.training_parameters['learning_rate'],\n                betas=self.training_parameters['betas'],\n                weight_decay=self.training_parameters['weight_decay']\n            )\n            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer,\n                mode='min',\n                patience=self.training_parameters['reduce_lr_patience'],\n                factor=self.training_parameters['reduce_lr_factor'],\n                min_lr=self.training_parameters['reduce_lr_min'],\n                verbose=True\n            )\n\n            early_stopping = False\n            summary = {\n                'train_loss': [],\n                'val_loss': []\n            }\n\n            for epoch in range(1, self.training_parameters['epochs'] + 1):\n\n                if early_stopping:\n                    break\n\n                train_loss = self.train_fn(train_loader, model, criterion, optimizer, device)\n                val_loss = self.val_fn(val_loader, model, criterion, device)\n                print(f'Epoch {epoch} - Training Loss: {train_loss:.6f} - Validation Loss: {val_loss:.6f}')\n                scheduler.step(val_loss)\n\n                best_val_loss = np.min(summary['val_loss']) if len(summary['val_loss']) > 0 else np.inf\n                if val_loss < best_val_loss:\n                    model_path = f'neural_network_fold{fold}.pt'\n                    torch.save(model.state_dict(), model_path)\n                    print(f'Saving model to {model_path} (validation loss decreased from {best_val_loss:.6f} to {val_loss:.6f})')\n\n                summary['train_loss'].append(train_loss)\n                summary['val_loss'].append(val_loss)\n\n                best_iteration = np.argmin(summary['val_loss']) + 1\n                if len(summary['val_loss']) - best_iteration >= self.training_parameters['early_stopping_patience']:\n                    print(f'Early stopping (validation loss didn\\'t increase for {self.training_parameters[\"early_stopping_patience\"]} epochs\/steps)')\n                    print(f'Best validation loss is {np.min(summary[\"val_loss\"]):.6f}')\n                    visualize_learning_curve(\n                        training_losses=summary['train_loss'],\n                        validation_losses=summary['val_loss'],\n                        title=f'Neural Network - Fold {fold} Learning Curve'\n                    )\n                    early_stopping = True\n","e58ae8e0":"df_train_processed","c1326b18":"TRAIN_NN = False\n\nif TRAIN_NN:\n    \n    nn_preprocessor = PreprocessingPipeline(train=df_train, test=df_test,n_splits=5, fill_missing=True, scale='standard', verbose=False)\n    #df_train_nn, df_test_nn = nn_preprocessor.transform()\n\n    print(f'\\nNeural Network Training Set Shape: {df_train_nn.shape} - Memory Usage: {df_train_nn.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n    print(f'Neural Network Test Set Shape: {df_test_nn.shape} - Memory Usage: {df_test_nn.memory_usage().sum() \/ 1024 ** 2:.2f} MB')\n\n    # 38.599106\n    \n    features_nn = [\n        'floor_area', 'year_built', 'energy_star_rating',\n        'cooling_degree_days', 'avg_temp', 'days_below_30F', 'days_above_80F'\n    ] + [column for column in df_train_processed.columns if '_Cat' in column]\n\n    nn_parameters = {\n        'features': features_nn,\n        'target': 'site_eui',\n        'model_parameters': {\n            'input_dim': len(features_nn)\n        },\n        'training_parameters': {\n            'learning_rate': 0.0001,\n            'betas': (0.9, 0.999),\n            'weight_decay': 0,\n            'epochs': 200,\n            'batch_size': 256,\n            'reduce_lr_patience': 5,\n            'reduce_lr_factor': 0.5,\n            'reduce_lr_min': 1e-6,\n            'early_stopping_patience': 20,\n            'num_workers': 4,\n            'random_state': 42,\n            'deterministic_cudnn': False\n        },\n    }\n\n    nn_trainer = NeuralNetworkTrainer(**nn_parameters)\n    nn_trainer.train_and_validate(df_train_nn)\n\n    del df_train_nn, df_test_nn\n    del nn_preprocessor, features_nn, nn_parameters, nn_trainer\n    ","821cd3f7":"state_site_euis = {\n    0: (1.196721312, 910.5631977),\n    1: (46.68462283, 153.0936389),\n    2: (1.090174329, 903.6675747000004),\n    3: (1.597261263, 970.3885864),\n    4: (3.511354944, 854.6605229),\n    5: (1.002997979, 997.8661202),\n    6: (1.001169302, 944.8957064),\n}\n\nprediction_column = 'lgb_predictions'\n\nfor df in [df_train_processed, df_test_processed]:\n    for state_factor in df_train_processed['State_Factor'].unique():\n        df.loc[df['State_Factor'] == state_factor, 'clipped_predictions'] = np.clip(df.loc[df['State_Factor'] == state_factor, prediction_column], a_min=state_site_euis[state_factor][0], a_max=state_site_euis[state_factor][1])\n        \noof_score = mean_squared_error(df_train_processed['site_eui'], df_train_processed['clipped_predictions'], squared=False)\nprint(f'Post-processed OOF RMSE: {oof_score:.6f}')","c81160ac":"def match_and_overwrite_target(column, agg):\n    \n    # Find matching values in training and test set\n    matching_buildings = df_train_processed.loc[df_train_processed[column].isin(df_test_processed[column]), column].unique()\n    # Aggregate target of matching values and save it as a new column\n    matching_buildings_target_agg = df_train_processed[df_train_processed[column].isin(matching_buildings)].groupby(column)['site_eui'].agg(agg)\n    # Overwrite final predictions\n    df_train_processed[f'matching_{column}_target_{agg}'] = df_train_processed[column].map(matching_buildings_target_agg)\n    df_train_processed.loc[~df_train_processed[f'matching_{column}_target_{agg}'].isnull(), 'matched_predictions'] = df_train_processed.loc[~df_train_processed[f'matching_{column}_target_{agg}'].isnull(), f'matching_{column}_target_{agg}']\n    df_test_processed[f'matching_{column}_target_{agg}'] = df_test_processed[column].map(matching_buildings_target_agg)\n    df_test_processed.loc[~df_test_processed[f'matching_{column}_target_{agg}'].isnull(), 'matched_predictions'] = df_test_processed.loc[~df_test_processed[f'matching_{column}_target_{agg}'].isnull(), f'matching_{column}_target_{agg}']\n    \n    oof_score = mean_squared_error(df_train_processed['site_eui'], df_train_processed['matched_predictions'], squared=False)\n    print(f'{len(matching_buildings)} matching {column} values found - OOF Score: {oof_score:.4f}')\n    \n    \ndf_train_processed['matched_predictions'] = df_train_processed['clipped_predictions'].values\ndf_test_processed['matched_predictions'] = df_test_processed['clipped_predictions'].values\n\nmatch_and_overwrite_target('building_id1.3', 'min')\nmatch_and_overwrite_target('building_id1.4', 'min')\nmatch_and_overwrite_target('building_id1.5', 'min')\nmatch_and_overwrite_target('building_id1.6', 'min')","b2f6965e":"df_train_processed['final_predictions'] = (df_train_processed['matched_predictions'] * 0.5) + (df_train_processed['clipped_predictions'] * 0.5)\ndf_test_processed['final_predictions'] = (df_test_processed['matched_predictions'] * 0.5) + (df_test_processed['clipped_predictions'] * 0.5)\noof_score = mean_squared_error(df_train_processed['site_eui'], df_train_processed['final_predictions'], squared=False)\nprint(f'OOF Score: {oof_score:.4f}')","9345befc":"df_submission = pd.read_csv('..\/input\/widsdatathon2022\/sample_solution.csv')\ndf_submission['site_eui'] = df_test_processed['final_predictions'].values\ndf_submission.to_csv('submission.csv', index=False)","dfe7623c":"## Post-processing","331c92e4":"## Preprocessing","7d71809d":"## Neural Network","15d485cf":"## LightGBM","bc0d679b":"## Visualization"}}