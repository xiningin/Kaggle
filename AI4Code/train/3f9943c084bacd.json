{"cell_type":{"04f69d58":"code","88c1757a":"code","9eeba933":"code","83f5e265":"code","c4ca2b84":"code","a6b82718":"code","0d3d6a77":"code","74332b42":"code","f97e77e0":"code","1239656b":"code","531fb612":"code","d85b7c8e":"markdown","c9558641":"markdown","5b81a126":"markdown"},"source":{"04f69d58":"from __future__ import print_function\nimport numpy as np ## For numerical python\nnp.random.seed(42)\n\nfrom enum import Enum, unique, auto\n","88c1757a":"import keras\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef load_dataset(flatten=False):\n    (X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n\n    # normalize x\n    X_train = X_train.astype(float) \/ 255.\n    X_test = X_test.astype(float) \/ 255.\n\n    if flatten:\n        X_train = X_train.reshape([X_train.shape[0], -1])\n        X_test = X_test.reshape([X_test.shape[0], -1])\n\n    return X_train, y_train, X_test, y_test\n\nX_train, y_train, X_test, y_test = load_dataset(flatten=True)\n\n## Let's look at some example\nplt.figure(figsize=[6,6])\nfor i in range(4):\n    plt.subplot(2,2,i+1)\n    plt.title(\"Label: %i\"%y_train[i])\n    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');","9eeba933":"# X_train = X_train.T\n# X_test = X_test.T\n\n# m = X_train.shape[1]\n# m\/","83f5e265":"def ReLU(Z):\n    return np.maximum(Z, 0)\n\ndef Sigmoid(Z): \n    return 1\/(1 + np.exp(-Z))\n\ndef Sigmoid_deriv(Z):\n    return Sigmoid(Z)*(1-Sigmoid(Z))\n\ndef Tanh(Z):\n    return np.tanh(Z)\n\ndef Tanh_deriv(Z):\n    t = 1- np.tanh(Z)*np.tanh(Z)\n    return t\n\ndef ReLU_deriv(Z):\n    return Z > 0\n\n\ndef softmax(Z):\n    A = np.exp(Z) \/ sum(np.exp(Z))\n    return A\n    ","c4ca2b84":"@unique\nclass ActivationFunction(Enum):\n\t\n\t\"\"\"\n\tthis is the activation function docmentation\n\t\n\t\"\"\"\n\tSIGMOID = 1\n\tTANH = 2\n\tRELU = 3\n\tANOTHER = auto()","a6b82718":"class MLP:\n    \n    def __init__(self, max_iters=500, alpha=0.01, record_updates=False, activation_function=ActivationFunction.RELU, num_layers='2'):\n        self.max_iters = max_iters\n        self.alpha = alpha\n        self.record_updates = record_updates\n        if record_updates:\n            self.W1_hist = []\n#             self.n_hist = []\n            self.W2_hist = []\n            self.W3_hist = []\n        self.activation_function = activation_function\n        self.num_layers = num_layers\n\n    def fit(self, x, y):\n        x = x.T\n        m = x.shape[1]\n        \n        if self.num_layers == '2':\n            \n            def gradient_descent(X, Y, alpha, max_iters):\n                def init_params():\n                    W1 = np.random.rand(128, 784) - 0.5\n                    b1 = np.random.rand(128, 1) - 0.5\n                    W2 = np.random.rand(128, 128) - 0.5\n                    b2 = np.random.rand(128, 1) - 0.5\n                    W3 = np.random.rand(10, 128) - 0.5\n                    b3 = np.random.rand(10, 1) - 0.5\n\n                    return W1, b1, W2, b2, W3, b3\n\n                W1, b1, W2, b2, W3, b3 = init_params()\n\n\n                def forward_prop(W1, b1, W2, b2, W3, b3, X):\n                    Z1 = W1.dot(X) + b1\n                    \n                    if self.activation_function.name == 'RELU':\n                        A1 = ReLU(Z1)\n                    elif self.activation_function.name == 'TANH':\n                        A1 = Tanh(Z1)\n                    elif self.activation_function.name == 'SIGMOID':\n                        A1 = Sigmoid(Z1)\n\n                    Z2 = W2.dot(A1) + b2\n\n                    if self.activation_function.name == 'RELU':\n                        A2 = ReLU(Z2)\n                    elif self.activation_function.name == 'TANH':\n                        A2 = Tanh(Z2)\n                    elif self.activation_function.name == 'SIGMOID':\n                        A2 = Sigmoid(Z2)\n\n                    Z3 = W3.dot(A2) + b3\n                    A3 = softmax(Z3)\n                    return Z1, A1, Z2, A2, Z3, A3\n\n\n                def one_hot(Y):\n                    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n                    one_hot_Y[np.arange(Y.size), Y] = 1\n                    one_hot_Y = one_hot_Y.T\n                #     print(f'one_hot_Y shape is {one_hot_Y.shape}')\n                    return one_hot_Y\n\n\n\n                def backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y):\n                    one_hot_Y = one_hot(Y)\n                \n                    dZ3 = A3 - one_hot_Y\n                #     print(f'dz2 shape is {dZ3.shape}, A1 shape is {A1.shape}')\n                    dW3 = 1 \/ m * dZ3.dot(A2.T)\n                \n                    db3 = 1 \/ m * np.sum(dZ3)\n                #     print(f'db3 shape is {db3.shape}')\n                    if self.activation_function.name == 'RELU':\n                        dZ2 = W3.T.dot(dZ3) * ReLU_deriv(Z2)\n                    elif self.activation_function.name == 'TANH':\n                        dZ2 = W3.T.dot(dZ3) * Tanh_deriv(Z2)\n                    elif self.activation_function.name == 'SIGMOID':\n                        dZ2 = W3.T.dot(dZ3) * Sigmoid_deriv(Z2)\n\n                    dW2 = 1 \/ m * dZ2.dot(A1.T)\n                    db2 = 1 \/ m * np.sum(dZ2)\n\n                    if self.activation_function.name == 'RELU':\n                        dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n                    elif self.activation_function.name == 'TANH':\n                        dZ1 = W2.T.dot(dZ2) * Tanh_deriv(Z1)\n                    elif self.activation_function.name == 'SIGMOID':\n                        dZ1 = W2.T.dot(dZ2) * Sigmoid_deriv(Z1)\n\n                    dW1 = 1 \/ m * dZ1.dot(X.T)\n                    db1 = 1 \/ m * np.sum(dZ1)\n                    return dW1, db1, dW2, db2, dW3, db3\n\n\n                def update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha):\n                    W1 = W1 - alpha * dW1\n                    b1 = b1 - alpha * db1    \n                    W2 = W2 - alpha * dW2  \n                    b2 = b2 - alpha * db2    \n                    W3 = W3 - alpha * dW3\n                    b3 = b3 - alpha * db3\n                    return W1, b1, W2, b2, W3, b3\n\n\n                for i in range(self.max_iters):\n                    Z1, A1, Z2, A2, Z3, A3 = forward_prop(W1, b1, W2, b2, W3, b3, X)\n                    dW1, db1, dW2, db2, dW3, db3 = backward_prop(Z1, A1, Z2, A2, Z3, A3, W1, W2, W3, X, Y)\n                    W1, b1, W2, b2, W3, b3 = update_params(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, alpha)\n    #                 \n                return W1, b1, W2, b2, W3, b3\n\n            self.W1, self.b1, self.W2, self.b2, self.W3, self.b3 = gradient_descent(x, y, self.alpha, self.max_iters)\n            return self\n        \n        elif self.num_layers == '1':\n            \n            def gradient_descent(X, Y, alpha, max_iters):\n\n\n                def init_params():\n                    W1 = np.random.rand(128, 784) - 0.5\n                    b1 = np.random.rand(128, 1) - 0.5\n                    W2 = np.random.rand(10, 128) - 0.5\n                    b2 = np.random.rand(10, 1) - 0.5\n                    return W1, b1, W2, b2\n\n\n                W1, b1, W2, b2 = init_params()\n\n\n                def forward_prop(W1, b1, W2, b2, X):\n                    Z1 = W1.dot(X) + b1\n                    if self.activation_function.name == 'RELU':\n                        A1 = ReLU(Z1)\n                    elif self.activation_function.name == 'TANH':\n                        A1 = Tanh(Z1)\n                    elif self.activation_function.name == 'SIGMOID':\n                        A1 = Sigmoid(Z1)\n\n                    Z2 = W2.dot(A1) + b2\n                    A2 = softmax(Z2)\n                    return Z1, A1, Z2, A2\n                \n                \n                \n                def one_hot(Y):\n                    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n                    one_hot_Y[np.arange(Y.size), Y] = 1\n                    one_hot_Y = one_hot_Y.T\n                #     print(f'one_hot_Y shape is {one_hot_Y.shape}')\n                    return one_hot_Y\n\n\n\n                def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n                    one_hot_Y = one_hot(Y)\n                    dZ2 = A2 - one_hot_Y\n                    dW2 = 1 \/ m * dZ2.dot(A1.T)\n                \n                    db2 = 1 \/ m * np.sum(dZ2)\n                #     print(f'db3 shape is {db3.shape}')\n                    if self.activation_function.name == 'RELU':\n                        dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n                    elif self.activation_function.name == 'TANH':\n                        dZ1 = W2.T.dot(dZ2) * Tanh_deriv(Z1)\n                    elif self.activation_function.name == 'SIGMOID':\n                        dZ1 = W2.T.dot(dZ2) * Sigmoid_deriv(Z1)\n\n                    dW1 = 1 \/ m * dZ1.dot(X.T)\n                    db1 = 1 \/ m * np.sum(dZ1)\n                    return dW1, db1, dW2, db2\n\n\n                def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n                    W1 = W1 - alpha * dW1\n                    b1 = b1 - alpha * db1    \n                    W2 = W2 - alpha * dW2  \n                    b2 = b2 - alpha * db2\n                    return W1, b1, W2, b2\n\n\n                for i in range(self.max_iters):\n                    Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n                    dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n                    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n\n\n                return W1, b1, W2, b2\n\n            self.W1, self.b1, self.W2, self.b2 = gradient_descent(x, y, self.alpha, self.max_iters)\n            return self\n        \n        elif self.num_layers == '0':\n            \n            def gradient_descent(X, Y, alpha, max_iters):\n\n                def init_params():\n                    W1 = np.random.rand(10, 784) - 0.5\n                    b1 = np.random.rand(10, 1) - 0.5\n                    return W1, b1\n\n\n                W1, b1 = init_params()\n\n\n                def forward_prop(W1, b1, X):\n                    Z1 = W1.dot(X) + b1\n                    \n                    A1 = softmax(Z1)\n                    return Z1, A1\n\n\n                def one_hot(Y):\n                    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n                    one_hot_Y[np.arange(Y.size), Y] = 1\n                    one_hot_Y = one_hot_Y.T\n                #     print(f'one_hot_Y shape is {one_hot_Y.shape}')\n                    return one_hot_Y\n\n                def backward_prop(Z1, A1, W1, X, Y):\n                    one_hot_Y = one_hot(Y)\n                \n                    dZ1 = A1 - one_hot_Y\n                    dW1 = 1 \/ m * dZ1.dot(X.T)\n                \n                    db1 = 1 \/ m * np.sum(dZ1)\n                    return dW1, db1\n\n\n                def update_params(W1, b1, dW1, db1, alpha):\n                    W1 = W1 - alpha * dW1\n                    b1 = b1 - alpha * db1\n                    return W1, b1\n\n\n                for i in range(self.max_iters):\n                    Z1, A1 = forward_prop(W1, b1, X)\n                    dW1, db1 = backward_prop(Z1, A1, W1, X, Y)\n                    W1, b1 = update_params(W1, b1, dW1, db1, alpha)\n                return W1, b1\n\n            self.W1, self.b1 = gradient_descent(x, y, self.alpha, self.max_iters)\n            return self\n\n\n    \n    def predict(self, x):\n        x = x.T\n        \n        if self.num_layers == '0':\n            def _get_predictions(A1):\n                return np.argmax(A1, 0)\n        elif self.num_layers == '1':\n            def _get_predictions(A2):\n                return np.argmax(A2, 0)\n        elif self.num_layers == '2':\n            def _get_predictions(A3):\n                return np.argmax(A3, 0)\n            \n            \n            \n        if self.num_layers == '0':\n            \n            \n            def _make_predictions(x, W1, b1):\n                \n                def forward_prop(W1, b1, X):\n                    Z1 = W1.dot(X) + b1\n                    A1 = softmax(Z1)\n                    return Z1, A1\n                \n                _, self.A1 = forward_prop(self.W1, self.b1, x)\n                predictions = _get_predictions(self.A1)\n                return predictions\n            \n            predictions = _make_predictions(x, self.W1, self.b1)\n            return predictions\n\n        elif self.num_layers == '1':\n            \n            def forward_prop(W1, b1, W2, b2, X):\n                Z1 = W1.dot(X) + b1\n                if self.activation_function.name == 'RELU':\n                    A1 = ReLU(Z1)\n                elif self.activation_function.name == 'TANH':\n                    A1 = Tanh(Z1)\n                elif self.activation_function.name == 'SIGMOID':\n                    A1 = Sigmoid(Z1)\n\n                Z2 = W2.dot(A1) + b2\n                A2 = softmax(Z2)\n                return Z1, A1, Z2, A2\n            \n            def _make_predictions(x, W1, b1, W2, b2):\n                _, _, _, self.A2 = forward_prop(self.W1, self.b1, self.W2, self.b2, x)\n                predictions = _get_predictions(self.A2)\n                return predictions\n\n            predictions = _make_predictions(x, self.W1, self.b1, self.W2, self.b2)\n            return predictions\n            \n            \n        elif self.num_layers == '2':\n            \n            def forward_prop(W1, b1, W2, b2, W3, b3, X):\n                    Z1 = W1.dot(X) + b1\n                    if self.activation_function.name == 'RELU':\n                        A1 = ReLU(Z1)\n                    elif self.activation_function.name == 'TANH':\n                        A1 = Tanh(Z1)\n                    elif self.activation_function.name == 'SIGMOID':\n                        A1 = Sigmoid(Z1)\n\n                    Z2 = W2.dot(A1) + b2\n\n                    if self.activation_function.name == 'RELU':\n                        A2 = ReLU(Z2)\n                    elif self.activation_function.name == 'TANH':\n                        A2 = Tanh(Z2)\n                    elif self.activation_function.name == 'SIGMOID':\n                        A2 = Sigmoid(Z2)\n\n                    Z3 = W3.dot(A2) + b3\n                    A3 = softmax(Z3)\n                    return Z1, A1, Z2, A2, Z3, A3\n\n\n            def _make_predictions(x, W1, b1, W2, b2, W3, b3):\n                _, _, _, _, _, self.A3 = forward_prop(self.W1, self.b1, self.W2, self.b2, self.W3, self.b3, x)\n                predictions = _get_predictions(self.A3)\n                return predictions\n\n            predictions = _make_predictions(x, self.W1, self.b1, self.W2, self.b2, self.W3, self.b3)\n            return predictions\n    \n    \ndef get_accuracy(predictions, Y):\n    print(predictions, Y)\n    return np.sum(predictions == Y) \/ Y.size\n\n","0d3d6a77":"clf = MLP(max_iters=400, activation_function=ActivationFunction.SIGMOID, num_layers='0')\npredictions = clf.fit(X_train, y_train).predict(X_test)","74332b42":"print(get_accuracy(predictions, y_test))","f97e77e0":"\ndef test_prediction(index, W1, b1, W2, b2, W3, b3):\n    current_image = X_train[:, index, None]\n    prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2, W3, b3)\n    label = y_train[index]\n    print(\"Prediction: \", prediction)\n    print(\"Label: \", label)\n    \n    current_image = current_image.reshape((28, 28)) * 255\n    plt.gray()\n    plt.imshow(current_image, interpolation='nearest')\n    plt.show()","1239656b":"test_prediction(0, W1, b1, W2, b2, W3, b3)\ntest_prediction(1, W1, b1, W2, b2, W3, b3)\ntest_prediction(2, W1, b1, W2, b2, W3, b3)\ntest_prediction(3, W1, b1, W2, b2, W3, b3)","531fb612":"dev_predictions = make_predictions(X_test, W1, b1, W2, b2, W3, b3)\nget_accuracy(dev_predictions, y_test)","d85b7c8e":"Let's look at a couple of examples:","c9558641":"Finally, let's find the accuracy on the test set:","5b81a126":"# Simple MNIST NN from scratch\n\nIn this notebook, I implemented a simple two-layer neural network and trained it on the MNIST digit recognizer dataset. It's meant to be an instructional example, through which you can understand the underlying math of neural networks better.\n\nHere's a video I made explaining all the math and showing my progress as I coded the network: https:\/\/youtu.be\/w8yWXqWQYmU"}}