{"cell_type":{"7472dd63":"code","80c46088":"code","3497e957":"code","4b9a6413":"code","f2551014":"code","3e75bf01":"code","0eef4e11":"code","dd331740":"code","12615167":"code","e9db5af3":"code","efbd2272":"code","92e599c3":"code","ecb28bdd":"code","d931c716":"code","8d241d2f":"code","5161eb4a":"code","8157ebdc":"code","b4bdcb61":"code","3fa921ba":"code","8dff3055":"code","67e3e223":"code","5ba22ef5":"code","96e8a3be":"code","2125b250":"markdown","b09dfa76":"markdown","c58b18a7":"markdown","1a9cf94d":"markdown","301e1eae":"markdown","6dbd39ef":"markdown","a2ae8346":"markdown","c3396246":"markdown","b48a43e8":"markdown"},"source":{"7472dd63":"!pip install sklearn-crfsuite","80c46088":"import nltk\nimport sklearn_crfsuite\n\nfrom copy import deepcopy\nfrom collections import defaultdict\n\nfrom sklearn_crfsuite import metrics","3497e957":"from copy import deepcopy\nfrom collections import namedtuple\n\nEntity = namedtuple(\"Entity\", \"e_type start_offset end_offset\")\n\n\ndef collect_named_entities(tokens):\n    \"\"\"\n    Creates a list of Entity named-tuples, storing the entity type and the start and end\n    offsets of the entity.\n    :param tokens: a list of labels\n    :return: a list of Entity named-tuples\n    \"\"\"\n\n    named_entities = []\n    start_offset = None\n    end_offset = None\n    ent_type = None\n\n    for offset, token_tag in enumerate(tokens):\n\n        if token_tag == 'O':\n            if ent_type is not None and start_offset is not None:\n                end_offset = offset - 1\n                named_entities.append(Entity(ent_type, start_offset, end_offset))\n                start_offset = None\n                end_offset = None\n                ent_type = None\n\n        elif ent_type is None:\n            ent_type = token_tag[2:]\n            start_offset = offset\n\n        elif ent_type != token_tag[2:] or (ent_type == token_tag[2:] and token_tag[:1] == 'B'):\n\n            end_offset = offset - 1\n            named_entities.append(Entity(ent_type, start_offset, end_offset))\n\n            # start of a new entity\n            ent_type = token_tag[2:]\n            start_offset = offset\n            end_offset = None\n\n    # catches an entity that goes up until the last token\n    if ent_type and start_offset and end_offset is None:\n        named_entities.append(Entity(ent_type, start_offset, len(tokens)-1))\n\n    return named_entities\n\n\ndef compute_metrics(true_named_entities, pred_named_entities):\n    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0}\n    target_tags_no_schema = ['MISC', 'PER', 'LOC', 'ORG']\n\n    # overall results\n    evaluation = {'strict': deepcopy(eval_metrics),\n                  'ent_type': deepcopy(eval_metrics),\n                  'partial': deepcopy(eval_metrics),\n                  'exact': deepcopy(eval_metrics)}\n\n    # results by entity type\n    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in target_tags_no_schema}\n\n    true_which_overlapped_with_pred = []  # keep track of entities that overlapped\n\n    # go through each predicted named-entity\n    for pred in pred_named_entities:\n        found_overlap = False\n\n        # Check each of the potential scenarios in turn. See \n        # http:\/\/www.davidsbatista.net\/blog\/2018\/05\/09\/Named_Entity_Evaluation\/\n        # for scenario explanation. \n\n        # Scenario I: Exact match between true and pred\n\n        if pred in true_named_entities:\n            true_which_overlapped_with_pred.append(pred)\n            evaluation['strict']['correct'] += 1\n            evaluation['ent_type']['correct'] += 1\n            evaluation['exact']['correct'] += 1\n            evaluation['partial']['correct'] += 1\n\n            # for the agg. by e_type results\n            evaluation_agg_entities_type[pred.e_type]['strict']['correct'] += 1\n            evaluation_agg_entities_type[pred.e_type]['ent_type']['correct'] += 1\n            evaluation_agg_entities_type[pred.e_type]['exact']['correct'] += 1\n            evaluation_agg_entities_type[pred.e_type]['partial']['correct'] += 1\n\n        else:\n\n            # check for overlaps with any of the true entities\n\n            for true in true_named_entities:\n\n                pred_range = range(pred.start_offset, pred.end_offset)\n                true_range = range(true.start_offset, true.end_offset)\n\n                # Scenario IV: Offsets match, but entity type is wrong\n\n                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n                        and true.e_type != pred.e_type:\n\n                    # overall results\n                    evaluation['strict']['incorrect'] += 1\n                    evaluation['ent_type']['incorrect'] += 1\n                    evaluation['partial']['correct'] += 1\n                    evaluation['exact']['correct'] += 1\n\n                    # aggregated by entity type results\n                    evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n                    evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n                    evaluation_agg_entities_type[true.e_type]['partial']['correct'] += 1\n                    evaluation_agg_entities_type[true.e_type]['exact']['correct'] += 1\n\n                    true_which_overlapped_with_pred.append(true)\n                    found_overlap = True\n                    break\n\n                # check for an overlap i.e. not exact boundary match, with true entities\n\n                elif find_overlap(true_range, pred_range):\n\n                    true_which_overlapped_with_pred.append(true)\n\n                    # Scenario V: There is an overlap (but offsets do not match\n                    # exactly), and the entity type is the same.\n                    # 2.1 overlaps with the same entity type\n\n                    if pred.e_type == true.e_type:\n\n                        # overall results\n                        evaluation['strict']['incorrect'] += 1\n                        evaluation['ent_type']['correct'] += 1\n                        evaluation['partial']['partial'] += 1\n                        evaluation['exact']['incorrect'] += 1\n\n                        # aggregated by entity type results\n                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n                        evaluation_agg_entities_type[true.e_type]['ent_type']['correct'] += 1\n                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n\n                        found_overlap = True\n                        break\n\n                    # Scenario VI: Entities overlap, but the entity type is \n                    # different.\n\n                    else:\n                        # overall results\n                        evaluation['strict']['incorrect'] += 1\n                        evaluation['ent_type']['incorrect'] += 1\n                        evaluation['partial']['partial'] += 1\n                        evaluation['exact']['incorrect'] += 1\n\n                        # aggregated by entity type results\n                        # Results against the true entity\n\n                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n                        evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n\n                        # Results against the predicted entity\n\n                        # evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n\n                        found_overlap = True\n                        break\n\n            # Scenario II: Entities are spurious (i.e., over-generated).\n\n            if not found_overlap:\n                # overall results\n                evaluation['strict']['spurious'] += 1\n                evaluation['ent_type']['spurious'] += 1\n                evaluation['partial']['spurious'] += 1\n                evaluation['exact']['spurious'] += 1\n\n                # aggregated by entity type results\n                evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n                evaluation_agg_entities_type[pred.e_type]['ent_type']['spurious'] += 1\n                evaluation_agg_entities_type[pred.e_type]['partial']['spurious'] += 1\n                evaluation_agg_entities_type[pred.e_type]['exact']['spurious'] += 1\n\n    # Scenario III: Entity was missed entirely.\n\n    for true in true_named_entities:\n        if true in true_which_overlapped_with_pred:\n            continue\n        else:\n            # overall results\n            evaluation['strict']['missed'] += 1\n            evaluation['ent_type']['missed'] += 1\n            evaluation['partial']['missed'] += 1\n            evaluation['exact']['missed'] += 1\n\n            # for the agg. by e_type\n            evaluation_agg_entities_type[true.e_type]['strict']['missed'] += 1\n            evaluation_agg_entities_type[true.e_type]['ent_type']['missed'] += 1\n            evaluation_agg_entities_type[true.e_type]['partial']['missed'] += 1\n            evaluation_agg_entities_type[true.e_type]['exact']['missed'] += 1\n\n    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n    # overall results, and use these to calculate precision and recall.\n\n    for eval_type in evaluation:\n        evaluation[eval_type] = compute_actual_possible(evaluation[eval_type])\n\n    # Compute 'possible', 'actual', and precision and recall on entity level \n    # results. Start by cycling through the accumulated results.\n\n    for entity_type, entity_level in evaluation_agg_entities_type.items():\n\n        # Cycle through the evaluation types for each dict containing entity\n        # level results.\n\n        for eval_type in entity_level:\n\n            evaluation_agg_entities_type[entity_type][eval_type] = compute_actual_possible(\n                entity_level[eval_type]\n            )\n\n    return evaluation, evaluation_agg_entities_type\n\n\ndef find_overlap(true_range, pred_range):\n    \"\"\"Find the overlap between two ranges\n    Find the overlap between two ranges. Return the overlapping values if\n    present, else return an empty set().\n    Examples:\n    >>> find_overlap((1, 2), (2, 3))\n    2\n    >>> find_overlap((1, 2), (3, 4))\n    set()\n    \"\"\"\n\n    true_set = set(true_range)\n    pred_set = set(pred_range)\n\n    overlaps = true_set.intersection(pred_set)\n\n    return overlaps\n\n\ndef compute_actual_possible(results):\n    \"\"\"\n    Takes a result dict that has been output by compute metrics.\n    Returns the results dict with actual, possible populated.\n    When the results dicts is from partial or ent_type metrics, then\n    partial_or_type=True to ensure the right calculation is used for\n    calculating precision and recall.\n    \"\"\"\n\n    correct = results['correct']\n    incorrect = results['incorrect']\n    partial = results['partial']\n    missed = results['missed']\n    spurious = results['spurious']\n\n    # Possible: number annotations in the gold-standard which contribute to the\n    # final score\n\n    possible = correct + incorrect + partial + missed\n\n    # Actual: number of annotations produced by the NER system\n\n    actual = correct + incorrect + partial + spurious\n\n    results[\"actual\"] = actual\n    results[\"possible\"] = possible\n\n    return results\n\n\ndef compute_precision_recall(results, partial_or_type=False):\n    \"\"\"\n    Takes a result dict that has been output by compute metrics.\n    Returns the results dict with precison and recall populated.\n    When the results dicts is from partial or ent_type metrics, then\n    partial_or_type=True to ensure the right calculation is used for\n    calculating precision and recall.\n    \"\"\"\n\n    actual = results[\"actual\"]\n    possible = results[\"possible\"]\n    partial = results['partial']\n    correct = results['correct']\n\n    if partial_or_type:\n        precision = (correct + 0.5 * partial) \/ actual if actual > 0 else 0\n        recall = (correct + 0.5 * partial) \/ possible if possible > 0 else 0\n\n    else:\n        precision = correct \/ actual if actual > 0 else 0\n        recall = correct \/ possible if possible > 0 else 0\n\n    results[\"precision\"] = precision\n    results[\"recall\"] = recall\n    results[\"f1-score\"] = 2*precision*recall\/(precision + recall)\n\n    return results\n\n\ndef compute_precision_recall_wrapper(results):\n    \"\"\"\n    Wraps the compute_precision_recall function and runs on a dict of results\n    \"\"\"\n\n    results_a = {key: compute_precision_recall(value, True) for key, value in results.items() if\n                 key in ['partial', 'ent_type']}\n    results_b = {key: compute_precision_recall(value) for key, value in results.items() if\n                 key in ['strict', 'exact']}\n\n    results = {**results_a, **results_b}\n\n    return results","4b9a6413":"nltk.corpus.conll2002.fileids()\ntrain_sents = list(nltk.corpus.conll2002.iob_sents('esp.train'))\ntest_sents = list(nltk.corpus.conll2002.iob_sents('esp.testb'))","f2551014":"def word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n\n    features = {\n        'bias': 1.0,\n        'word.lower()': word.lower(),\n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit(),\n        'postag': postag,\n        'postag[:2]': postag[:2],\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper(),\n            '-1:postag': postag1,\n            '-1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['BOS'] = True\n\n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper(),\n            '+1:postag': postag1,\n            '+1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['EOS'] = True\n\n    return features\n\n\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]\n\ndef sent2tokens(sent):\n    return [token for token, postag, label in sent]","3e75bf01":"%%time\nX_train = [sent2features(s) for s in train_sents]\ny_train = [sent2labels(s) for s in train_sents]\n\nX_test = [sent2features(s) for s in test_sents]\ny_test = [sent2labels(s) for s in test_sents]","0eef4e11":"%%time\ncrf = sklearn_crfsuite.CRF(\n    algorithm='lbfgs',\n    c1=0.1,\n    c2=0.1,\n    max_iterations=100,\n    all_possible_transitions=True\n)\ncrf.fit(X_train, y_train)","dd331740":"y_pred = crf.predict(X_test)","12615167":"test_sents_labels = []\nfor sentence in test_sents:\n    sentence = [token[2] for token in sentence]\n    test_sents_labels.append(sentence)","e9db5af3":"index = 2\ntest_sents_labels[index]","efbd2272":"true = collect_named_entities(test_sents_labels[index])\npred = collect_named_entities(y_pred[index])","92e599c3":"true","ecb28bdd":"pred","d931c716":"compute_metrics(true, pred)","8d241d2f":"import spacy\nfrom spacy.util import compounding, minibatch\nimport random","5161eb4a":"def create_model(labels):\n    nlp = spacy.blank(\"it\")\n    ner = nlp.create_pipe(\"ner\")\n    nlp.add_pipe(ner)\n    for label in labels:\n        ner.add_label(str(label))\n    optimizer = nlp.begin_training()\n    return nlp, optimizer\n\ndef train_model(n_iter, nlp, optimizer, train_data):\n    # train_data = [(text, {entities:[(start_offset, end_offset, label), ]})]\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        for itn in range(n_iter):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=sizes)\n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts, annotations, sgd=optimizer,\n                           drop=0.35, losses=losses)\n            print(\"(%d\/%d)Losses\" % ((itn+1), n_iter), losses)\n            \ndef predict_model(nlp, pred_data):\n    results = []\n    for text, ent in pred_data:\n        doc = nlp(text)\n        results.append((\n            text,\n            {'entities':[\n                [len(str(doc[0:x.start])) + 1, len(str(doc[0:x.start])) + len(str(doc[x.start:x.end])) + 1,\n                     x.label_]\n                    for x in doc.ents\n            ]}\n        ))\n    return results","8157ebdc":"def prepare_dataset(sents):\n    dataset = []\n    labels = set()\n    for sent in sents:\n        text = ' '\n        entities = []\n        for w in sent:\n            text += w[0] + ' '\n            label = w[2]\n            if label != 'O':\n                labels.add(label)\n                entities.append([len(text)-len(w[0])-2,len(text)-2,label])\n        text = text.strip()\n        dataset.append( (text, {'entities': entities}) )\n    return dataset, labels","b4bdcb61":"train_dataset, labels = prepare_dataset(train_sents)\ntest_dataset, _ = prepare_dataset(test_sents)","3fa921ba":"ner, optimizer = create_model(labels)","8dff3055":"epochs = 1\ntrain_model(epochs ,ner, optimizer, train_dataset)","67e3e223":"from spacy.gold import GoldParse\nfrom spacy.scorer import Scorer\ndef evaluate_ner(ner_model, test_dataset):\n    scorer = Scorer()\n    for text, entities in test_dataset:\n        doc_gold_text = ner_model.make_doc(text)\n        gold = GoldParse(doc_gold_text, entities=entities['entities'])\n        pred_value = ner_model(text)\n        scorer.score(pred_value, gold)\n    return scorer.scores","5ba22ef5":"evaluate_ner(ner, test_dataset)","96e8a3be":"# compute_metrics(true, pred)","2125b250":"# Evaluate Model","b09dfa76":"# Train CRF","c58b18a7":"# Added code\nThis code is into a [git repo](https:\/\/github.com\/davidsbatista\/NER-Evaluation\/blob\/master\/ner_evaluation\/ner_eval.py)","1a9cf94d":"# Prepare dataset for Spacy","301e1eae":"# Train with spacy","6dbd39ef":"# Dataset","a2ae8346":"# Source\nThis example start from the article [Named-Entity evaluation metrics based on entity-level](http:\/\/www.davidsbatista.net\/blog\/2018\/05\/09\/Named_Entity_Evaluation\/)","c3396246":"# Evaluate CRF","b48a43e8":"# Prepare dataset for CRF"}}