{"cell_type":{"f76c57e1":"code","1fe25f09":"code","87f1cd17":"code","9b5dcb6d":"code","afa907ae":"code","5cc72e96":"code","366d6ad0":"code","6557ea64":"code","13f70bad":"code","2de23bef":"code","bc82139d":"code","2c9293ac":"code","ef517250":"code","1ced8c71":"code","aa5443c1":"code","64fc50c1":"code","9e688424":"code","a889d2b9":"code","55449d97":"code","fb8791fa":"code","3f2fa8fe":"code","a6a5de6f":"code","38c93552":"code","6ade9fd8":"code","ca62833f":"code","0e2792ce":"code","9aabad6d":"code","181f1448":"code","85ed6b46":"code","8a4776e5":"code","95bfc24a":"code","9d9a1614":"code","08a6d26f":"code","9475460d":"code","99b37d52":"code","564ae95f":"code","eb19248f":"code","eb1e6831":"code","6f533bf7":"code","e841cf71":"code","e0e163af":"code","1e112b43":"code","8ae3a47e":"code","e22a0505":"code","f0f05f90":"code","3ac52891":"code","0658fbd9":"code","e3e3d417":"code","afad68ed":"code","af814d17":"code","41070843":"markdown","efcbecd0":"markdown","5d57c100":"markdown","91afcf33":"markdown","a4301d1c":"markdown","a6e430e6":"markdown","c1f69f47":"markdown","08ab9312":"markdown","16101f32":"markdown","ae3d9cdf":"markdown","93c95183":"markdown","40806c67":"markdown","964e7538":"markdown","84161022":"markdown","7614a6b0":"markdown","37ee81ea":"markdown","ba79bcec":"markdown","cd01914b":"markdown","937842b6":"markdown","8d69238a":"markdown","efc996b2":"markdown","6e1d5237":"markdown","3c6900b6":"markdown","f668549b":"markdown","7c45e677":"markdown","ea12bce7":"markdown","97c3da46":"markdown","27314dfb":"markdown","b42bf58e":"markdown","17971c1c":"markdown","19f83b3c":"markdown","17e2e332":"markdown","3deca6d6":"markdown","9bfcf64d":"markdown","24a48f37":"markdown"},"source":{"f76c57e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fe25f09":"#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Preprocessing and helper libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\n#LOADING DIFFERENT ML MODELS\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n\n","87f1cd17":"train_data  = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ngender_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nId = test_data[\"PassengerId\"]","9b5dcb6d":"train_data.head()","afa907ae":"train_data.info()","5cc72e96":"test_data.info()","366d6ad0":"#printing only top 5 values\ntrain_data.head()","6557ea64":"#Storing both dataFrames as list dsp that they can be iterated over\ndata = [train_data, test_data]","13f70bad":"print(\"Training data with null values:\\n\",train_data.isnull().sum())\nprint(\"\\n\\n\\nTesting data with null values:\\n\",test_data.isnull().sum())","2de23bef":"for data_ in data:\n    # filling median of age at null values , and not mean because mean can get affected by outliers which are very few elderly people in this case\n    data_[\"Age\"].fillna(data_[\"Age\"].median(), inplace=True)\n    # Filling null cabin with mode  (which are most frequent ie cabins type in which most of people out of all passengers are staying are staying)\n    data_[\"Cabin\"].fillna(data_[\"Cabin\"].mode()[0], inplace=True)\n    # Filling fare with mean as fares are continous not discrete  \n    data_[\"Fare\"].fillna(data_[\"Fare\"].mean(), inplace=True)\n    # Filling embarked with mode ie same as  from where most people have embarked from.\n    data_[\"Embarked\"].fillna(data_[\"Embarked\"].mode()[0], inplace=True)\n\n    ","bc82139d":"print(\"Training data with null values:\\n\",train_data.isnull().sum())\nprint(\"\\n\\n\\nTesting data with null values:\\n\",test_data.isnull().sum())","2c9293ac":"train_data.describe()","ef517250":"\nfor data_ in data:\n    \n    #adding sibling, cousins, parents and the person to get the size of the family onboard\n    data_[\"familysize\"] = data_[\"SibSp\"] + data_[\"Parch\"] + 1;\n    \n    #Dividing age into bins of age-gap as it will be more categorical as compared to a continuos value like age\n    data_[\"agebin\"] = pd.cut(data_[\"Age\"].astype(int), 5)\n    \n    #Creating bins for fares also \n    data_[\"farebin\"] = pd.qcut(data_[\"Fare\"],5)\n    \n    #Assigning 1 to everyone ie everyone is alone and then cheking if its family size if >1 then it is not alone so the value is set to 0 \n    data_[\"alone\"] = 1\n    data_[\"alone\"].loc[data_[\"familysize\"] > 1] = 0\n\n    \n#Printing newly created features along with their values   \nprint(\"FamilySize :\\n\", data[0].familysize.value_counts())\nprint(\"\\n\\n\\nAlone :\\n\", data[0].alone.value_counts())\nprint(\"\\n\\n\\nAgeBin :\\n\", data[0].agebin.value_counts())\nprint(\"\\n\\n\\nFareBin :\\n\", data[1].farebin.value_counts())","1ced8c71":"data[0].head()","aa5443c1":"data[1].head()","64fc50c1":"label = LabelEncoder()\n\nfor data_ in data:\n    data_[\"Sex\"] = label.fit_transform(data_[\"Sex\"])\n    data_[\"Embarked\"] = label.fit_transform(data_[\"Embarked\"])\n    data_[\"Cabin\"] = label.fit_transform(data_[\"Cabin\"])\n    data_[\"agebin\"] = label.fit_transform(data_[\"agebin\"])\n    data_[\"farebin\"] = label.fit_transform(data_[\"farebin\"])","9e688424":"data[0].head()","a889d2b9":"print(\"Training data with null values:\\n\",train_data.info())\nprint(\"\\n\\n\\nTesting data with null values:\\n\",test_data.info())","55449d97":"#Plotting histograms for various features\n\nplt.figure(figsize=(10,10))\n\nplt.subplot(221)\nplt.hist(x = [train_data[train_data['Survived']==1]['familysize'], train_data[train_data['Survived']==0]['familysize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(222)\nplt.hist(x = [train_data[train_data['Survived']==1]['farebin'], train_data[train_data['Survived']==0]['farebin']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('FareBin Histogram by Survival')\nplt.xlabel('FareBin(#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\n\nplt.subplot(223)\nplt.hist(x = [train_data[train_data['Survived']==1]['Age'], train_data[train_data['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\n\nplt.subplot(224)\nplt.hist(x = [train_data[train_data['Survived']==1]['Sex'], train_data[train_data['Survived']==0]['Sex']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Sex Histogram by Survival')\nplt.xlabel('Sex (#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n","fb8791fa":"plt.figure(figsize = (16,10))\nsns.heatmap(train_data.corr(), annot=True, cmap=\"YlGnBu\")","3f2fa8fe":"train_data.columns","a6a5de6f":"clean_train = np.array( train_data.drop(columns=['PassengerId', 'Survived', 'Name','Ticket']))\nclean_target = np.array( train_data[\"Survived\"] )\nclean_test_data = np.array( test_data.drop(columns=['PassengerId','Name','Ticket']))","38c93552":"print(clean_train.shape, clean_test_data.shape)","6ade9fd8":"#SPLITTING DATA\nX_train, X_test, y_train, y_test = train_test_split(clean_train, clean_target, test_size=0.25, random_state=42)","ca62833f":"print(X_train.shape,y_train.shape, X_test.shape, y_test.shape)","0e2792ce":"random_forest_clf = RandomForestClassifier(n_estimators=10)\nextra_trees_clf = ExtraTreesClassifier(n_estimators=10)\nlg_reg_clf = LogisticRegression()\nbag_clf = BaggingClassifier(n_estimators=10)\nada_clf = AdaBoostClassifier(n_estimators=10)\ngrad_clf = GradientBoostingClassifier(n_estimators=10)\nxgb_clf = XGBClassifier(n_estimators=10)","9aabad6d":"# Helping_Function to show Cross Val Scores\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","181f1448":"# Estimators\nestimators = [random_forest_clf, extra_trees_clf, lg_reg_clf, bag_clf, ada_clf, grad_clf, xgb_clf]\n\n# STORING CROSS VAL-SCORE IN A LIST SCORES\nscores= []\nfor estimator in estimators:\n    scores.append(cross_val_score(estimator, X_train, y_train, cv=7))\n","85ed6b46":"# PRINTING ACCURACY AND MEAN OF EACH ESTIMATORS\ni = 0\nfor score in scores:\n    print(\"-\"*10 +estimators[i].__class__.__name__ + \"-\"*10 + \"\\n\")\n    display_scores(score)\n    print(\"\\n\\n\")\n    i = i+1","8a4776e5":"named_estimators = [\n    (\"random_forest_clf\", random_forest_clf),\n    (\"extra_trees_clf\", extra_trees_clf),\n    (\"lg_reg_clf\", lg_reg_clf),\n    (\"bag_clf\", bag_clf),\n    (\"ada_clf\", ada_clf),\n    (\"grad_clf\", grad_clf),\n    (\"xgb_clf\", xgb_clf),\n    \n]","95bfc24a":"# As we have seen above , the accuracy of some models are low as compared to others and they might be reducing the accuracy of your voting model so I have created a new list containing only top 4 classifiers.\nselected_named_estimators = [\n    (\"bag_clf\", bag_clf),\n    (\"ada_clf\", ada_clf),\n    (\"grad_clf\", grad_clf),\n    (\"xgb_clf\", xgb_clf),\n    \n]","9d9a1614":"# Both voting model , one with all estimators and one with selected estimators\n\nall_estimators_voting_clf = VotingClassifier(named_estimators, voting=\"soft\")\nselected_estimators_voting_clf = VotingClassifier(selected_named_estimators, voting = \"soft\")","08a6d26f":"all_estimators_voting_scores = cross_val_score(all_estimators_voting_clf, X_train ,y_train, cv=7)\nselected_estimators_voting_scores = cross_val_score(selected_estimators_voting_clf, X_train ,y_train, cv=7)","9475460d":"print(\"VOTING WITH ALL ESTIMATORS & WITHOUT TUNING :\\n\")\ndisplay_scores(all_estimators_voting_scores)\n\nprint(\"\\n\\n\"+\"%\"*100+\"\\n\\n\\nVOTING WITH SELECTED ESTIMATORS & WITHOUT TUNING :\\n\")\ndisplay_scores(selected_estimators_voting_scores)\n","99b37d52":"# Setting values for all the parameters\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n# You can use google and  references for getting all parameters \ngrid_param = [\n             [{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n            \n            \n            [{\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n            \n            [{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            \n      \n            [{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n            'random_state': grid_seed\n            }],\n    \n            [{\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n            [{\n            #XGBClassifier - http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n    \n            \n        ]\n","564ae95f":"# for clf, param in zip (named_estimators, grid_param):\n#     best_search = GridSearchCV(estimator = clf[1], param_grid = param, cv = 10, scoring = 'roc_auc')\n#     best_search.fit(X_train, y_train)\n#     best_param = best_search.best_params_\n#     print('The best parameter for {} is {} '.format(clf[1].__class__.__name__, best_param))\n","eb19248f":"best_random_forest_clf = RandomForestClassifier( criterion=\"entropy\", n_estimators=100, max_depth=8, oob_score=True, random_state=0)\nbest_extra_trees_clf = ExtraTreesClassifier( criterion=\"entropy\", n_estimators=300, max_depth=6, random_state=0)\nbest_lg_reg_clf = LogisticRegression( fit_intercept= True, random_state= 0, solver= 'newton-cg')\nbest_bag_clf = BaggingClassifier( max_samples = 0.25, n_estimators= 300,  random_state=0)\nbest_ada_clf = AdaBoostClassifier( learning_rate=0.1, n_estimators=100, random_state=0)\nbest_grad_clf = GradientBoostingClassifier( learning_rate = 0.05, max_depth= 4, n_estimators=300, random_state=0)\nbest_xgb_clf = XGBClassifier( learning_rate=0.25, max_depth=6, n_estimators=100, seed=0)","eb1e6831":"all_estimators_tuned = [\n    (\"random_forest_clf\", best_random_forest_clf),\n    (\"extra_trees_clf\", best_extra_trees_clf),\n    (\"lg_reg_clf\", best_lg_reg_clf),\n    (\"bag_clf\", best_bag_clf),\n    (\"ada_clf\", best_ada_clf),\n    (\"grad_clf\", best_grad_clf),\n    (\"xgb_clf\", best_xgb_clf),\n]","6f533bf7":"estimators = [best_random_forest_clf, best_extra_trees_clf, best_lg_reg_clf, best_bag_clf, best_ada_clf, best_grad_clf, best_xgb_clf]\n\nscores= []\nfor estimator in estimators:\n    scores.append(cross_val_score(estimator, X_train, y_train, cv=7))","e841cf71":"i = 0\nfor score in scores:\n    print(\"-\"*10 +estimators[i].__class__.__name__ + \"-\"*10 + \"\\n\")\n    display_scores(score)\n    print(\"\\n\\n\")\n    i = i+1","e0e163af":"selected_estimators_tuned = [\n    (\"random_forest_clf\", best_random_forest_clf),\n    (\"extra_trees_clf\", best_extra_trees_clf),\n    (\"bag_clf\", best_bag_clf),\n    (\"grad_clf\", best_grad_clf),\n\n]","1e112b43":"all_estimators_tuned_voting_clf = VotingClassifier(all_estimators_tuned, voting=\"soft\")\nselected_estimators_tuned_voting_clf = VotingClassifier(selected_estimators_tuned, voting=\"soft\")","8ae3a47e":"#Their cross-val scores calculation\nall_tuned_voting_clf_scores = cross_val_score(all_estimators_tuned_voting_clf,X_train, y_train,cv=7)\nselected_tuned_voting_clf_scores = cross_val_score(selected_estimators_tuned_voting_clf,X_train, y_train,cv=7)","e22a0505":"print(\"VOTING WITH ALL ESTIMATORS & WITHOUT TUNING :\\n\")\ndisplay_scores(all_estimators_voting_scores)\n\nprint(\"\\n\\n\"+\"%\"*100+\"\\n\\n\\nVOTING WITH SELECTED ESTIMATORS & WITHOUT TUNING :\\n\")\ndisplay_scores(selected_estimators_voting_scores)\n\nprint(\"\\n\\n\"+\"%\"*100+\"\\n\\n\\nVOTING WITH ALL ESTIMATORS & WITH TUNING :\\n\")\ndisplay_scores(all_tuned_voting_clf_scores)\n\nprint(\"\\n\\n\"+\"%\"*100+\"\\n\\n\\nVOTING WITH SELECTED ESTIMATORS & WITH TUNING :\\n\")\ndisplay_scores(selected_tuned_voting_clf_scores)\n\n","f0f05f90":"all_estimators_voting_clf.fit(X_train, y_train)\nselected_estimators_voting_clf.fit(X_train, y_train)\nall_estimators_tuned_voting_clf.fit(X_train, y_train)\nselected_estimators_tuned_voting_clf.fit(X_train, y_train)","3ac52891":"pred_allestimators_without_tuning = all_estimators_voting_clf.predict(clean_test_data)\npred_selectedestimators_without_tuning = selected_estimators_voting_clf.predict(clean_test_data)\npred_allestimators_with_tuning = all_estimators_tuned_voting_clf.predict(clean_test_data)\npred_selectedestimators_with_tuning = selected_estimators_tuned_voting_clf.predict(clean_test_data)\n","0658fbd9":"submission = pd.DataFrame({'PassengerId':Id, 'Survived':pred_allestimators_without_tuning})\nsubmission.to_csv('submission_pred_allestimators_without_tuning.csv', index = False) ","e3e3d417":"submission = pd.DataFrame({'PassengerId':Id, 'Survived':pred_selectedestimators_without_tuning})\nsubmission.to_csv('submission_pred_selectedestimators_without_tuning.csv', index = False)","afad68ed":"\nsubmission = pd.DataFrame({'PassengerId':Id, 'Survived':pred_allestimators_with_tuning})\nsubmission.to_csv('submission_pred_allestimators_with_tuning.csv', index = False)","af814d17":"\nsubmission = pd.DataFrame({'PassengerId':Id, 'Survived':pred_selectedestimators_with_tuning})\nsubmission.to_csv('submission_pred_selectedestimators_with_tuning.csv', index = False)","41070843":"## Loading Data to DataFrame","efcbecd0":"## CV SCORE:","5d57c100":"## From above analysis we can see that survival is very much correlated with:\n### 1. farebin\n### 2. Fare\n### 3. Sex\n### 4. Pclass\n### 5. Cabin\n### 6. Alone","91afcf33":"## Dropping unwanted columns","a4301d1c":"## Submission Score :0.78468\n","a6e430e6":"## Now After we get the best parameters , we build our final voting model:\n        The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 8, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} \n        The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 300, 'random_state': 0}\n        The best parameter for LogisticRegression is {'fit_intercept': True, 'random_state': 0, 'solver': 'newton-cg'} \n        The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0}  \n        The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 100, 'random_state': 0}  \n        The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300, 'random_state': 0} \n        The best parameter for XGBClassifier is {'learning_rate': 0.25, 'max_depth': 6, 'n_estimators': 100, 'seed': 0}\n        \n      \n      \n","c1f69f47":"## Double checking data","08ab9312":"# Some Visualization to gain some new insights:","16101f32":"## Handling Categorical Datatypes:","ae3d9cdf":"## Making final estimators which are tuned ie having right parameters and their right values.","93c95183":"## Initializing Estimators","40806c67":"## As we can see GradientBoosting_CLF, Bagging_CLF, ExtraTrees_CLF, RandomForest_CLF performed better than others, so we should remove other classifiers as they can have negative effect on our final voting model","964e7538":"## Data cleaned so now  we'll see some stats of the dataset\n","84161022":"## **This is for finding best params using gridsearchCV and its a bit time consuming so i have commented it**","7614a6b0":"## Checking for null, missing, categorical values on train & test data","37ee81ea":"## Submission Score :0.77033","ba79bcec":"# Feature Enginneering :\n###   -Creating new Features","cd01914b":"## ----------RandomForestClassifier----------\n\nScores: [0.82291667 0.80208333 0.89583333 0.75789474 0.83157895 0.8\n 0.89473684]\n### Mean: 0.8292919799498747\nStandard deviation: 0.046984121019082115\n\n\n\n## ----------ExtraTreesClassifier----------\n\nScores: [0.80208333 0.80208333 0.90625    0.78947368 0.8        0.77894737\n 0.90526316]\n### Mean: 0.8263001253132832\nStandard deviation: 0.050836173770773097\n\n\n\n## ----------LogisticRegression----------\n\nScores: [0.82291667 0.76041667 0.89583333 0.75789474 0.74736842 0.74736842\n 0.87368421]\n### Mean: 0.8007832080200501\nStandard deviation: 0.058571238717743335\n\n\n\n## ----------BaggingClassifier----------\n\nScores: [0.80208333 0.80208333 0.85416667 0.82105263 0.83157895 0.81052632\n 0.89473684]\n### Mean: 0.8308897243107769\nStandard deviation: 0.031144361211338737\n\n\n\n## ----------AdaBoostClassifier----------\n\nScores: [0.79166667 0.78125    0.88541667 0.76842105 0.77894737 0.74736842\n 0.89473684]\n### Mean: 0.806829573934837\nStandard deviation: 0.054216639375005905\n\n\n\n## ----------GradientBoostingClassifier----------\n\nScores: [0.82291667 0.73958333 0.88541667 0.78947368 0.82105263 0.82105263\n 0.89473684]\n### Mean: 0.8248903508771929\nStandard deviation: 0.049435306018664804\n\n\n\n## ----------XGBClassifier----------\n\nScores: [0.83333333 0.75       0.83333333 0.8        0.8        0.81052632\n 0.86315789]\n### Mean: 0.8129072681704261\nStandard deviation: 0.033070837849658676\n\n\n","937842b6":"# Tuning hyperparams\n","8d69238a":"## Models with all and selected estimators and with tuned parameters.","efc996b2":"## **The attributes have the following meaning:**\n1. **Survived:** that's the target, 0 means the passenger did not survive, while 1 means he\/she survived.\n2. **Pclass:** passenger class.\n3. **Name, Sex, Age:** self-explanatory\n4. **SibSp:** how many siblings & spouses of the passenger aboard the Titanic.\n5. **Parch:** how many children & parents of the passenger aboard the Titanic.\n6. **Ticket:** ticket id\n7. **Fare:** price paid (in pounds)\n8. **Cabin:** passenger's cabin number\n9. **Embarked:** where the passenger embarked the Titanic","6e1d5237":"## Some small inferences from above data:\n   ### - there are only 25% of people who are paying higher than mean fare (must be rich people or boarded much before)\n   ### - average survival chance is 38%.\n   ### - 75% of people are younger than 35 years\n   ### - less than 25% people are with children and parents as 75% of Parch and SibSp is 0 & 1 respectively. ","3c6900b6":"## LETS CHECK ONE BY ONE THE ACCURACY OF THE TUNED MODELS","f668549b":"## Double checking whether all missing values are handled","7c45e677":"# Final Cross-Validation Scores of all models","ea12bce7":"## Accuracy with all estimators and selected estimators respectively.","97c3da46":"# Select and Train Model","27314dfb":"## Submission Score :0.77511","b42bf58e":"## Handling missing data:","17971c1c":"# Now fitting all models to get our submission files","19f83b3c":"# Importing Libraries","17e2e332":"## Submission Score :0.78468","3deca6d6":"# Submissions with their scores:","9bfcf64d":"## Few features which would be better for models to learn :\n1. *If you have a family then you will be having more chance of surviving*\n2. *If instead of age we created a age bins then it would be easier to predict that if a person is belonging to this age bucket he is likely to survive or not and same with fare.*\n3. *Being alone or being with other family is surely affect whether he\/she will survive or not*\n4. *Paying high fare can mean you are belonging to high class or are a VIP then they will be rescued first.*","24a48f37":"## All estimators:"}}