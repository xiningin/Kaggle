{"cell_type":{"fd324bf2":"code","2f8073d6":"code","26f10723":"code","df4b3a58":"code","f568cab9":"code","ff7121c1":"code","8f5794e5":"code","a6e298b7":"code","06322e0f":"code","0400a7fc":"code","a2e47790":"code","bee7aefb":"code","2fafacb5":"code","ddb4a982":"code","4872c32c":"code","4082cc16":"code","60444697":"code","5795ccb4":"code","f0c8f20a":"code","018a58b8":"markdown","a8973087":"markdown","8cdd4627":"markdown","03ddbd31":"markdown","90680dc2":"markdown","12793556":"markdown","e9f191aa":"markdown","1a71780f":"markdown","83711fe7":"markdown","ba408b85":"markdown","d151abf6":"markdown","cf0df198":"markdown"},"source":{"fd324bf2":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix","2f8073d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","26f10723":"dataset = pd.read_csv('\/kaggle\/input\/wineuci\/Wine.csv')\ndataset.head()","df4b3a58":"dataset.columns = ['Name',\n                   'Alcohol',\n                   'Malic_Acid',\n                   'Ash',\n                   'Ash_Alcanity',\n                   'Magnesium',\n                   'Total_Phenols',\n                   'Flavanoids',\n                   'Nonflavanoid_Phenols',\n                   'Proanthocyanins',\n                   'Color_Intensity',\n                   'Hue',\n                   'OD280',\n                   'Proline']","f568cab9":"dataset.head()","ff7121c1":"dataset.shape","8f5794e5":"dataset.info()","a6e298b7":"dataset.describe()","06322e0f":"X = dataset.iloc[:, 1:].values\ny = dataset.iloc[:, 0].values","0400a7fc":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","a2e47790":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","bee7aefb":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nlda = LDA(n_components = 2)\nX_train = lda.fit_transform(X_train, y_train)\nX_test = lda.transform(X_test)","2fafacb5":"models = []\n\nmodels.append((\"Logistic Regression:\",LogisticRegression()))\nmodels.append((\"Naive Bayes:\",GaussianNB()))\nmodels.append((\"K-Nearest Neighbour:\",KNeighborsClassifier(n_neighbors=3)))\nmodels.append((\"Decision Tree:\",DecisionTreeClassifier()))\nmodels.append((\"Support Vector Machine-linear:\",SVC(kernel=\"linear\")))\nmodels.append((\"Support Vector Machine-rbf:\",SVC(kernel=\"rbf\")))\nmodels.append((\"Random Forest:\",RandomForestClassifier(n_estimators=7)))\nmodels.append((\"MLP:\",MLPClassifier(hidden_layer_sizes=(45,30,15), solver='sgd', learning_rate_init=0.01, max_iter=500)))\nmodels.append((\"AdaBoostClassifier:\",AdaBoostClassifier()))\nmodels.append((\"GradientBoostingClassifier:\",GradientBoostingClassifier()))\n\nprint('Models appended...')","ddb4a982":"results = []\nnames = []\nfor name,model in models:\n    kfold = KFold(n_splits=10)\n    cv_result = cross_val_score(model,X_train,y_train.ravel(), cv = kfold,scoring = \"accuracy\")\n    names.append(name)\n    results.append(cv_result)\nfor i in range(len(names)):\n    print(names[i],results[i].mean()*100)","4872c32c":"classifier = MLPClassifier(hidden_layer_sizes=(45,30,15), solver='sgd', learning_rate_init=0.01, max_iter=500)\nclassifier.fit(X_train, y_train)","4082cc16":"y_pred = classifier.predict(X_test)","60444697":"sns.heatmap(confusion_matrix(y_pred, y_test), annot = True, fmt = 'd', cmap = plt.cm.Blues, cbar = False)","5795ccb4":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('MLP (Training set)')\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.legend()\nplt.show()","f0c8f20a":"from matplotlib.colors import ListedColormap\nX_set, y_set = X_test, y_test\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\nplt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)\nplt.title('MLP (Test set)')\nplt.xlabel('LD1')\nplt.ylabel('LD2')\nplt.legend()\nplt.show()","018a58b8":"## Applying LDA","a8973087":"## Making the Confusion Matrix","8cdd4627":"## Importing the dataset","03ddbd31":"## Training the MLP model on the Training set","90680dc2":"## Visualising the Training set results","12793556":"## Visualising the Test set results","e9f191aa":"## Feature Scaling","1a71780f":"## Importing the libraries","83711fe7":"## Results ","ba408b85":"## Predicting the Test set results","d151abf6":"## Classifier Models ","cf0df198":"## Splitting the dataset into the Training set and Test set"}}