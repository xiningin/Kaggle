{"cell_type":{"c25336d6":"code","a4f030d1":"code","4bf3ef7d":"code","c6ff0913":"code","69d4ba12":"code","9e586555":"markdown","b2319e8a":"markdown","86d7c0a8":"markdown","6dc7b498":"markdown","cc909714":"markdown","9d3fb077":"markdown","9cda065a":"markdown","7f849013":"markdown","618aa023":"markdown"},"source":{"c25336d6":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nimport matplotlib.pyplot as plt\nfrom sklearn.svm import LinearSVC\nimport warnings\nfrom sklearn.feature_selection import RFE\nwarnings.filterwarnings(\"ignore\")","a4f030d1":"def draw_line(coef,intercept, mi, ma):\n\n    points=np.array([[((-coef[1]*mi - intercept)\/coef[0]), mi],[((-coef[1]*ma - intercept)\/coef[0]), ma]])\n    plt.plot(points[:,0], points[:,1])","4bf3ef7d":"# here we are creating 2d imbalanced data points \nratios = [(100,2), (100, 20), (100, 40), (100, 80)]\nplt.figure(figsize=(20,5))\nfor j,i in enumerate(ratios):\n    plt.subplot(1, 4, j+1)\n    X_p=np.random.normal(0,0.05,size=(i[0],2))\n    X_n=np.random.normal(0.13,0.02,size=(i[1],2))\n    y_p=np.array([1]*i[0]).reshape(-1,1)\n    y_n=np.array([0]*i[1]).reshape(-1,1)\n    X=np.vstack((X_p,X_n))\n    y=np.vstack((y_p,y_n))\n    plt.scatter(X_p[:,0],X_p[:,1])\n    plt.scatter(X_n[:,0],X_n[:,1],color='red')\nplt.show()","c6ff0913":"C = [1000, 1, 0.01]\nratios = [(100,2), (100, 20), (100, 40), (100, 80)]\n\nfor c in C:\n  plt.figure(figsize=(20,5))\n  for j,i in enumerate(ratios):\n      plt.subplot(1, 4, j+1)\n      X_p=np.random.normal(0,0.05,size=(i[0],2))\n      X_n=np.random.normal(0.13,0.02,size=(i[1],2))\n      y_p=np.array([1]*i[0]).reshape(-1,1)\n      y_n=np.array([0]*i[1]).reshape(-1,1)\n      X=np.vstack((X_p,X_n))\n      y=np.vstack((y_p,y_n))\n      y = y.ravel() \n\n      #Creating a Model\n      clf = LinearSVC(C=c)\n      clf.fit(X,y)\n\n      #plot_decision_regions(X=X, y=y,clf=clf, legend=2)\n      \n      plt.scatter(X_p[:,0],X_p[:,1])\n      plt.scatter(X_n[:,0],X_n[:,1],color='red')\n      draw_line(clf.coef_[0], clf.intercept_[0], -0.2,0.2)\n  plt.show()","69d4ba12":"C = [1000, 1, 0.01]\nratios = [(100,2), (100, 20), (100, 40), (100, 80)]\n\nfor c in C:\n  plt.figure(figsize=(20,5))\n  for j,i in enumerate(ratios):\n      plt.subplot(1, 4, j+1)\n      X_p=np.random.normal(0,0.05,size=(i[0],2))\n      X_n=np.random.normal(0.13,0.02,size=(i[1],2))\n      y_p=np.array([1]*i[0]).reshape(-1,1)\n      y_n=np.array([0]*i[1]).reshape(-1,1)\n      X=np.vstack((X_p,X_n))\n      y=np.vstack((y_p,y_n))\n      y = y.ravel() \n\n      #Creating a Model\n      clf = LogisticRegression(C=c)\n      clf.fit(X,y)\n\n      plt.scatter(X_p[:,0],X_p[:,1])\n      plt.scatter(X_n[:,0],X_n[:,1],color='red')\n      draw_line(clf.coef_[0], clf.intercept_[0], -0.2,0.2)\n  plt.show()","9e586555":"<h3>Observation:<\/h3>\n\n1. Here we can observe that 1000 is the best value of c. such that 0.001 is best lambda value. as C increases (lambda decreases)model is being underfitted.\n2. Model working better with balanced dataset than imbalaqnced dataset.1000 is the best value of c and 0.01 is the worst value of c . but consider c=1, as data get balanced the model works properly and line saparates the points properly that we can visulize.","b2319e8a":"> your task is to apply SVM (<a href='https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC'>sklearn.svm.SVC<\/a>) and LR (<a href='https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html'>sklearn.linear_model.LogisticRegression<\/a>) with different regularization strength [0.001, 1, 100]","86d7c0a8":"here draw_line() will draw the line to seprate datapoints for given classifier object","6dc7b498":"Here I am generating data from random distribution with different values of ratios and applying classifications.","cc909714":"# What if Data is imabalanced\n\n<pre>\n1. As a part of this task you will observe how linear models work in case of data imbalanced\n2. observe how hyper plane is changs according to change in your learning rate.\n3. below we have created 4 random datasets which are linearly separable and having class imbalance\n4. in the first dataset the ration between positive and negative is 100 : 2, in the 2nd data its 100:20, in the 3rd data its 100:40 and in 4th one its 100:80\n<\/pre>","9d3fb077":"* Here we can observe that SVC is better than LogisticRegression in this case because LogisticRegression has some missclassification. \n","9cda065a":"##  Applying SVM","7f849013":"##  Applying LR","618aa023":"<h2>Observation:<\/h2>\n1.Here we can observe that 1000 is the best value of c. such that 0.001 is best lambda value. as C increases (lambda decreases)model is being underfitted.\n2. 0.01 is worst value of c , so except that value we can observe that as data is getting balanced, model works properly\n\n"}}