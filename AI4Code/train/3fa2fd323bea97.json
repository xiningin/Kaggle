{"cell_type":{"bf0fb351":"code","798ceb93":"code","1b1d4ac2":"code","2387f443":"code","39630725":"code","9a3d42da":"code","17c7e841":"code","79ad6900":"code","e6b0839f":"code","12ac32d9":"code","6af395ae":"code","38ea85d6":"code","d6b373ba":"code","fc42f913":"code","7c0b929e":"code","d7cd4dbc":"code","a66466bb":"code","a8a1391d":"code","5838aea3":"code","29f44f65":"code","9aeb1bc3":"code","4aae87e9":"code","347ba955":"code","d236de4a":"code","2a5804d6":"code","6356e726":"code","ed4f667f":"code","1e4c7d0c":"code","77ccc58a":"code","9fc60626":"code","730ceadb":"code","e2c2d3ba":"code","2fbf6e77":"code","87b71513":"code","01f4b983":"code","44a0029f":"code","9673e83b":"code","95f47429":"code","d29d7411":"code","54ed7ed1":"code","04453141":"code","ae67ae10":"code","534c2ac3":"code","575fad0d":"code","2293e538":"code","6c3270a2":"code","498580ae":"code","8f82ce1e":"code","19c58029":"code","4a1b7ef6":"code","d8b87bdf":"code","7c13cf03":"code","b103adfe":"code","e6f3ab44":"code","f2ea2cb6":"code","e425cb8c":"code","390ea485":"code","3a91ede9":"code","419875ee":"code","65e775ad":"code","a1deb359":"code","bd557e80":"code","7526c8df":"code","975a7e83":"code","ce91f925":"code","22fc884b":"code","e1248cc4":"code","60bfa07b":"code","364a3285":"code","d9bb9b7c":"code","c41bc8da":"code","79c4e3b4":"code","d96a631a":"code","013be947":"code","7b543fd9":"code","5a174128":"code","54917dad":"code","9b7b1b9e":"code","d6022982":"code","a3d34c14":"code","6b7e9bd1":"code","ed015cd5":"code","6e6115c2":"code","5a723b6d":"code","685d79e6":"code","de535fe5":"code","2edbd3e0":"code","0650bede":"code","8aea47dc":"code","7d0b66b7":"code","9f615fed":"code","2fc0cc82":"code","716bbf8d":"code","bfdb0662":"code","a7d92ca3":"code","f15a614d":"code","299593e2":"code","430ad65e":"code","28ace1d4":"code","ec36119d":"code","d601f9fb":"code","3a6f08da":"code","14bc80e0":"code","c24c3edd":"code","d0d56356":"code","06763606":"code","c9ad77f2":"code","63bf7a9d":"code","4aae42a1":"code","d5495139":"code","d2ba602a":"code","f7b8bc9b":"code","ac624e2d":"code","97f8ec87":"code","45803385":"markdown","910e0b32":"markdown","3db7ec24":"markdown","867e4237":"markdown","45c0c735":"markdown","8fd7163f":"markdown","1241b85f":"markdown","d15aa5b6":"markdown","3278e988":"markdown","3d1deb29":"markdown","a80e3a95":"markdown","06adbf17":"markdown","a80052ff":"markdown","7aeb56fe":"markdown","6c253b0b":"markdown","9cd1261f":"markdown","c412e20c":"markdown","fa76317c":"markdown","2f2f139b":"markdown","b14c0e56":"markdown","73329368":"markdown","faa058d7":"markdown","020b5adf":"markdown","ec50ec82":"markdown","7c2e6e40":"markdown","18dabb00":"markdown","c8696fc6":"markdown","e905b596":"markdown","2ce28e4c":"markdown","543f20ba":"markdown","76eb30f7":"markdown","8c529a84":"markdown","ee474f72":"markdown","3c534612":"markdown","88e3a8fc":"markdown","e3fbd95f":"markdown","fcc05d19":"markdown","3d47d388":"markdown","9df492f9":"markdown","8c7b6e2b":"markdown","1603c52a":"markdown","c172cc3a":"markdown","fbdad0b7":"markdown","466ca072":"markdown","68db2b7e":"markdown"},"source":{"bf0fb351":"# Import Libraries\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\npd.set_option('chained_assignment',None)\n%matplotlib inline\n%load_ext autoreload","798ceb93":"# Load datasets and read dates properly \ntrain_data = pd.read_csv('\/kaggle\/input\/great-indian-hiring-hack-data\/Train.csv', parse_dates=[\"InvoiceDate\"])\ntest_data = pd.read_csv('\/kaggle\/input\/great-indian-hiring-hack-data\/Test.csv', parse_dates=[\"InvoiceDate\"])","1b1d4ac2":"# Inspect data\ntrain_data.head()","2387f443":"# Data types\ntrain_data.dtypes","39630725":"# Info\ntrain_data.info()","9a3d42da":"# Describe\ntrain_data.describe()","17c7e841":"# Let's see if there are duplicates in both train and test\ntrain_data.duplicated().sum(), test_data.duplicated().sum()","79ad6900":"train_data.drop_duplicates(inplace=True)","e6b0839f":"# Identifying the Unique Identifier- Let's see if Invoice No is the Unique Identifier\nif train_data['InvoiceNo'].nunique() == train_data.shape[0]:\n    print(\"Invoice No is the Unique Identifier\")\nelse:\n    print(\"Invoice No is not the Unique Identifier\")","12ac32d9":"most_common_Stock_code = train_data['StockCode'].mode()\ntrain_data[train_data[\"StockCode\"] == int(most_common_Stock_code)].head(10)","6af395ae":"train_data['StockCode'].nunique() == train_data['Description'].nunique()","38ea85d6":"# Check if there are any common Invoice No's between train and test\ninvoice_ids_train = set(train_data['InvoiceNo'])\ninvoice_ids_test = set(test_data['InvoiceNo'])\ncommon_invoices = invoice_ids_train.intersection(invoice_ids_test)\nprint(\"There are: \", len(common_invoices), \" common Invoices between train and test\")","d6b373ba":"common_invoices_train = train_data[train_data['InvoiceNo'].isin(list(common_invoices))]\ncommon_invoices_test = test_data[test_data['InvoiceNo'].isin(list(common_invoices))]\ncommon_invoices_train.sort_values('InvoiceNo', inplace=True)\ncommon_invoices_test.sort_values('InvoiceNo', inplace=True)","fc42f913":"# View common invoices train\ncommon_invoices_train.head(10)","7c0b929e":"# View common invoices test- Use same invoice id's\ncommon_invoices_test.head(10)","d7cd4dbc":"def get_datetime_features(dataset, column=\"InvoiceDate\"):\n    '''Extract Date time features and add to given dataset'''\n    dataset['Year'] = dataset[column].dt.year\n    dataset['Month'] = dataset[column].dt.month\n    dataset['DayOfMonth'] = dataset['InvoiceDate'].dt.day\n    dataset['DayOfWeek'] = dataset['InvoiceDate'].dt.dayofweek\n    dataset['HourOfDay'] = dataset['InvoiceDate'].dt.hour\n    dataset['DayOfYear'] = dataset['InvoiceDate'].dt.dayofyear\n    dataset['WeekOfYear'] = dataset['InvoiceDate'].dt.weekofyear\n    dataset['Quarter'] = dataset['InvoiceDate'].dt.quarter\n    dataset['IsWeekend'] = np.where(dataset['DayOfWeek'].isin([5,6]), 1, 0)\n    # Drop original Invoice Date\n    # dataset.drop(columns=[column], inplace=True)\n    return dataset","a66466bb":"train_data = get_datetime_features(train_data)\ntest_data = get_datetime_features(test_data)\ntrain_data.head()","a8a1391d":"# Create Line Plot\nsns.relplot(\n    data=train_data, x=\"Month\", y=\"UnitPrice\",\n    col=\"Year\", kind=\"line\",ci=None\n)\nplt.show()","5838aea3":"sns.relplot(\n    data=train_data, x=\"HourOfDay\", y=\"UnitPrice\",\n    col=\"IsWeekend\", kind=\"line\",ci=None\n)\nplt.show()","29f44f65":"# No use including features that are actually categorical. Included Description to try and figure out the encoding\ncorr = train_data.drop(columns = ['InvoiceNo', 'StockCode', 'CustomerID', 'Country']).corr()\n# Set up a mask\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\n# Generate a custom diverging colormap\n#cmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, annot=True, square=True)\nplt.show()","9aeb1bc3":"train_data.drop(columns = ['Quarter', 'WeekOfYear', 'DayOfYear'], inplace=True)\ntest_data.drop(columns = ['Quarter', 'WeekOfYear', 'DayOfYear'], inplace=True)\ncorr = train_data.drop(columns = ['InvoiceNo', 'StockCode', 'CustomerID', 'Country']).corr()\n# Set up a mask\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, annot=True, square=True)\nplt.show()","4aae87e9":"# Create a function to perform Univariate analysis \ndef plot_univariate_continuous(dataset, column_name):\n    '''Creates various Univariate plots for the given column and plots them'''\n    fig, axs = plt.subplots(nrows = 3, figsize=(15,15))\n    fig.tight_layout()\n    # Box Plot\n    axs[0].set_title(\"Boxplot of \"+ column_name)\n    plt.xlabel(column_name), plt.ylabel(\"FrequencyDist\")\n    sns.boxplot(y=dataset[column_name], ax=axs[0], palette=(\"limegreen\", (0.4,0,0.8)))\n    # ScatterPlot\n    axs[1].set_title(\"ScatterPlot of \" + column_name)\n    plt.xlabel(\"Index\"), plt.ylabel(column_name)\n    sns.scatterplot(dataset.index, dataset[column_name], ax=axs[1], color='red')\n    # Hist Plot\n    axs[2].set_title(\"Histogram of\" + column_name)\n    plt.xlabel(column_name), plt.ylabel(\"FrequencyDist\")\n    sns.distplot(dataset[column_name], ax=axs[2], kde_kws={\"color\": \"r\", \"lw\": 3},\n                  hist_kws={\"histtype\": \"bar\", \"linewidth\": 3, \"alpha\": 1, \"color\": \"g\", \"edgecolor\":\"black\", \"linewidth\":1.2})\n    plt.show()\nplot_univariate_continuous(train_data, \"UnitPrice\")","347ba955":"# As evident the Unit Price is very heavily skewed. Let's plot the distribution taking only values up to 3 times the 75th percentile\nplot_univariate_continuous(train_data[train_data['UnitPrice'] <= 3 * train_data['UnitPrice'].quantile(0.75)], \"UnitPrice\")","d236de4a":"# Let's also see this for Qty\nplot_univariate_continuous(train_data, \"Quantity\")","2a5804d6":"# Let's arrange Quantity in an ascending order and take 5 highest and lowest values\nQty_sorted = sorted(train_data['Quantity'].unique())\nhighest_5 = Qty_sorted[:5]\nlowest_5 = Qty_sorted[-5:]\nconcatenated_5 = highest_5 + lowest_5\n# Now filter and look at train data for these values\ntrain_data[train_data[\"Quantity\"].isin(concatenated_5)].sort_values(\"Quantity\")","6356e726":"# Though Description is actually not a numerical feature, let's plot the distributions for it and see if we can figure out anything\nplot_univariate_continuous(train_data, \"Description\")","ed4f667f":"# Create a function to draw plots betweeen categorical and numerical variable with an optional 3rd variable\ndef plot_bivariate_cat_cont(dataset, cont_variable_name, cat_variable_name, hue_variable=None):\n    '''Creates various Univariate plots for the given column and plots them'''\n    # Create a barplot\n    sns.barplot(x=dataset[cat_variable_name], y=dataset[cont_variable_name], hue=dataset[hue_variable], ci=None, dodge=True,\n            palette=['red','black'])\n    plt.show()\n    # Create a boxplot between the vars\n    sns.boxplot(x=dataset[cat_variable_name], y=dataset[cont_variable_name], hue=dataset[hue_variable])\n    plt.show()","1e4c7d0c":"plot_bivariate_cat_cont(train_data, \"UnitPrice\", \"Month\", \"IsWeekend\")","77ccc58a":"# Let's see if any pattern exists for Hour of Day\nplot_bivariate_cat_cont(train_data, \"UnitPrice\", \"HourOfDay\", \"IsWeekend\")","9fc60626":"# Stock Code- There are too many stock codes to visualize all so let's group by the stock code and visualize the\n# top 10 and bottom 10 Stocks along with counts\nstock_code_price_grouped_Top_10 = train_data.groupby('StockCode')['UnitPrice'].agg(['mean', 'count']).reset_index()\nstock_code_price_grouped_Top_10.rename(columns = {'mean':'MeanStockPrice', 'count':'CountStockSold'}, inplace=True)\nstock_code_price_grouped_Top_10.sort_values(\"MeanStockPrice\", ascending=False, inplace=True)\nstock_code_price_grouped_Top_10.reset_index(drop=True, inplace=True)\nstock_code_price_grouped_Top_10.head()","730ceadb":"# Filter Top 10 and bottom 10 values\nstock_code_price_grouped_Top_10_head = stock_code_price_grouped_Top_10.head(10)\nstock_code_price_grouped_Top_10_tail = stock_code_price_grouped_Top_10.tail(10)\nstock_code_price_grouped_Top_10 = pd.concat([stock_code_price_grouped_Top_10_head, stock_code_price_grouped_Top_10_tail])\nstock_code_price_grouped_Top_10.reset_index(drop=True, inplace=True)","e2c2d3ba":"# Visualize on a Column chart\nsns.barplot(x=stock_code_price_grouped_Top_10['StockCode'], y=stock_code_price_grouped_Top_10['MeanStockPrice'], ci=None, palette=\"deep\")\nplt.show()\nsns.barplot(x=stock_code_price_grouped_Top_10['StockCode'], y=stock_code_price_grouped_Top_10['CountStockSold'], ci=None, palette=\"deep\")\nplt.show()","2fbf6e77":"# Now let's group by Customer ID and visualize the results\ncustomer_price_grouped_Top_10 = train_data.groupby(['CustomerID'])['UnitPrice'].agg(['mean', 'count']).reset_index()\ncustomer_price_grouped_Top_10.rename(columns = {'mean':'MeanStockPriceCustomer', 'count':'StocksPurchasedCustomer'}, inplace=True)\ncustomer_price_grouped_Top_10.sort_values(['MeanStockPriceCustomer', 'StocksPurchasedCustomer'], ascending=False, inplace=True)\ncustomer_price_grouped_Top_10.reset_index(drop=True, inplace=True)\ncustomer_price_grouped_Top_10.head()","87b71513":"# Filter Top 10 and bottom 10 values\ncustomer_price_grouped_Top_10_head = customer_price_grouped_Top_10.head(10)\ncustomer_price_grouped_Top_10_tail = customer_price_grouped_Top_10.tail(10)\ncustomer_price_grouped_Top_10 = pd.concat([customer_price_grouped_Top_10_head, \n                                                         customer_price_grouped_Top_10_tail])\ncustomer_price_grouped_Top_10.reset_index(drop=True, inplace=True)\ncustomer_price_grouped_Top_10['CustomerID'] = customer_price_grouped_Top_10['CustomerID'].astype(int)","01f4b983":"# Visualize on a Column chart\nviz_1 = sns.barplot(x=customer_price_grouped_Top_10['CustomerID'], y=customer_price_grouped_Top_10['MeanStockPriceCustomer'], ci=None, palette=\"deep\")\nviz_1.set_xticklabels(viz_1.get_xticklabels(), rotation=45)\nplt.show()\nviz_2 = sns.barplot(x=customer_price_grouped_Top_10['CustomerID'], y=customer_price_grouped_Top_10['StocksPurchasedCustomer'], ci=None, palette=\"deep\")\nviz_2.set_xticklabels(viz_2.get_xticklabels(), rotation=45)\nplt.show()","44a0029f":"# Lastly let's see the data for Countries taking Top 10 countries in terms of Stocks Traded\ncountry_price_grouped_Top_10 = train_data.groupby(['Country'])['UnitPrice'].agg(['mean', 'count']).reset_index()\ncountry_price_grouped_Top_10.rename(columns = {'mean':'MeanStockPriceCountry', 'count':'StocksTradedCountry'}, inplace=True)\ncountry_price_grouped_Top_10.sort_values(['StocksTradedCountry'], ascending=False, inplace=True)\ncountry_price_grouped_Top_10.reset_index(drop=True, inplace=True)\ncountry_price_grouped_Top_10 = country_price_grouped_Top_10.head(10)\ncountry_price_grouped_Top_10.head()","9673e83b":"# Visualize on a Column chart\nviz_1 = sns.barplot(x=country_price_grouped_Top_10['Country'], y=country_price_grouped_Top_10['MeanStockPriceCountry'], ci=None, palette=\"deep\")\nviz_1.set_xticklabels(viz_1.get_xticklabels(), rotation=45)\nplt.show()\nviz_2 = sns.barplot(x=country_price_grouped_Top_10['Country'], y=country_price_grouped_Top_10['StocksTradedCountry'], ci=None, palette=\"deep\")\nviz_2.set_xticklabels(viz_2.get_xticklabels(), rotation=45)\nplt.show()","95f47429":"# Stock Code, Unit Price related Features- We will need to merge with test data to add these features to test data\ntrain_data['Mean_UnitPrice_per_Stock'] = train_data.groupby(['StockCode'])['UnitPrice'].transform(\"mean\")\ntrain_data['Median_UnitPrice_per_Stock'] = train_data.groupby(['StockCode'])['UnitPrice'].transform(\"median\")\ntrain_data['Min_UnitPrice_per_Stock'] = train_data.groupby(['StockCode'])['UnitPrice'].transform(\"min\")\ntrain_data['Max_UnitPrice_per_Stock'] = train_data.groupby(['StockCode'])['UnitPrice'].transform(\"max\")\ntrain_data['NumberOfDifferentPricesStockSold'] = train_data.groupby(['StockCode'])['UnitPrice'].transform('nunique').astype(int)\ntrain_data.head()","d29d7411":"# Merge with test data to get these features\ntest_data_additional_features = pd.merge(test_data, \n                                         train_data.loc[:, ['StockCode', 'Mean_UnitPrice_per_Stock','Median_UnitPrice_per_Stock',\n                                                            'Min_UnitPrice_per_Stock', 'Max_UnitPrice_per_Stock',\n                                                           'NumberOfDifferentPricesStockSold']].drop_duplicates(),\n                                         on=['StockCode'],\n                                         how='left')\ntest_data_additional_features.head()","54ed7ed1":"# If there are StockCodes in test not present in train it would have introduced NA's. Let's see if this is the case \ntest_data_additional_features.isnull().mean() * 100","04453141":"# Imputation\ntest_data_additional_features['Mean_UnitPrice_per_Stock'] = test_data_additional_features['Mean_UnitPrice_per_Stock'].fillna(train_data['UnitPrice'].mean())\ntest_data_additional_features['Median_UnitPrice_per_Stock'] = test_data_additional_features['Median_UnitPrice_per_Stock'].fillna(train_data['UnitPrice'].median())\ntest_data_additional_features['Min_UnitPrice_per_Stock'] = test_data_additional_features['Min_UnitPrice_per_Stock'].fillna(train_data['Min_UnitPrice_per_Stock'].mean())\ntest_data_additional_features['Max_UnitPrice_per_Stock'] = test_data_additional_features['Max_UnitPrice_per_Stock'].fillna(train_data['Max_UnitPrice_per_Stock'].mean())\ntest_data_additional_features['NumberOfDifferentPricesStockSold'] = test_data_additional_features['NumberOfDifferentPricesStockSold'].fillna(int(train_data['NumberOfDifferentPricesStockSold'].mean()))\n# Confirm NA's removed\ntest_data_additional_features.isnull().mean() * 100","ae67ae10":"# Customer, Unit Price related Features- We will need to merge with test data to add these features to test data\ntrain_data['AmountSpent'] = train_data['Quantity'] * train_data['UnitPrice']\ntrain_data['Average_Amt_Spent_Customer'] = train_data.groupby(['CustomerID'])['AmountSpent'].transform(\"mean\")\ntrain_data['Average_UnitPrice_Customer'] = train_data.groupby(['CustomerID'])['UnitPrice'].transform(\"mean\")\ntrain_data['Median_UnitPrice_Customer'] = train_data.groupby(['CustomerID'])['UnitPrice'].transform(\"median\")\ntrain_data['Min_UnitPrice_Customer'] = train_data.groupby(['CustomerID'])['UnitPrice'].transform(\"min\")\ntrain_data['Max_UnitPrice_Customer'] = train_data.groupby(['CustomerID'])['UnitPrice'].transform(\"max\")\ntrain_data['NumberOfDifferentStocksPurchasedCustomer'] = train_data.groupby(['CustomerID'])['StockCode'].transform('nunique').astype(int)\ntrain_data.drop(columns=['AmountSpent'], inplace=True) #This was a temp column\n# Let's also add a column to mark Returns\ntrain_data['Is_Return'] = np.where(train_data['Quantity'] < 0 , 1, 0)\ntrain_data.head()","534c2ac3":"# Merge with test data to get these features\ntest_data_additional_features = pd.merge(test_data_additional_features, \n                                         train_data.loc[:, ['CustomerID', 'Average_Amt_Spent_Customer','Average_UnitPrice_Customer',\n                                                            'Median_UnitPrice_Customer', 'Min_UnitPrice_Customer', 'Max_UnitPrice_Customer',\n                                                           'NumberOfDifferentStocksPurchasedCustomer']].drop_duplicates(),\n                                         on=['CustomerID'],\n                                         how='left')\ntest_data_additional_features.head()","575fad0d":"# If there are Customers in test not present in train it would have introduced NA's. Let's see if this is the case \ntest_data_additional_features.isnull().mean() * 100","2293e538":"# Imputation\ntest_data_additional_features['Average_Amt_Spent_Customer'] = test_data_additional_features['Average_Amt_Spent_Customer'].fillna(train_data['Average_Amt_Spent_Customer'].mean())\ntest_data_additional_features['Average_UnitPrice_Customer'] = test_data_additional_features['Average_UnitPrice_Customer'].fillna(train_data['Average_UnitPrice_Customer'].median())\ntest_data_additional_features['Median_UnitPrice_Customer'] = test_data_additional_features['Median_UnitPrice_Customer'].fillna(train_data['Median_UnitPrice_Customer'].mean())\ntest_data_additional_features['Min_UnitPrice_Customer'] = test_data_additional_features['Min_UnitPrice_Customer'].fillna(train_data['Min_UnitPrice_Customer'].mean())\ntest_data_additional_features['Max_UnitPrice_Customer'] = test_data_additional_features['Max_UnitPrice_Customer'].fillna(train_data['Max_UnitPrice_Customer'].mean())\ntest_data_additional_features['NumberOfDifferentStocksPurchasedCustomer'] = test_data_additional_features['NumberOfDifferentStocksPurchasedCustomer'].fillna(train_data['NumberOfDifferentStocksPurchasedCustomer'].mean())\n# Let's also add a column to mark Returns\ntest_data_additional_features['Is_Return'] = np.where(test_data_additional_features['Quantity'] < 0 , 1, 0)\n# Confirm NA's removed\ntest_data_additional_features.isnull().mean() * 100","6c3270a2":"# StockCode, Quantity and Unitprice- Let's create a feature which is ratio of Qty to Unit Price grouped by stock code\n# And let's do same for Customer ID group also\ntrain_data['RatioQtyUnitPrice'] = train_data['Quantity'] \/ train_data['UnitPrice']\ntrain_data['RatioQtyUnitPriceperStockCode'] = train_data.groupby('StockCode')['RatioQtyUnitPrice'].transform(\"mean\")\n# Ratio will be 0 if UnitPrice is 0. Impute such cases as 0\ntrain_data['RatioQtyUnitPriceperStockCode'] = np.where(train_data['RatioQtyUnitPriceperStockCode'] == np.inf, 0, train_data['RatioQtyUnitPriceperStockCode'])\n# Drop the ratio column\ntrain_data.drop(columns=['RatioQtyUnitPrice'], inplace=True)\ntrain_data.head()","498580ae":"# Add to test and impute null values\ntest_data_additional_features = pd.merge(test_data_additional_features, \n                                         train_data.loc[:, ['StockCode', 'RatioQtyUnitPriceperStockCode']].drop_duplicates(),\n                                         on=['StockCode'],\n                                         how='left')\ntest_data_additional_features['RatioQtyUnitPriceperStockCode'] = test_data_additional_features['RatioQtyUnitPriceperStockCode'].fillna(train_data['RatioQtyUnitPriceperStockCode'].mean())\ntest_data_additional_features.isnull().mean() * 100","8f82ce1e":"# MeanUnitPrice grouped by CustomerID and StockCode- may lead to lot of NA's in test\ntrain_data['MeanUnitPriceperCustomerandStock'] = train_data.groupby(['CustomerID', 'StockCode'])['UnitPrice'].transform(\"mean\")\ntrain_data.head()","19c58029":"# Add to test and check null values\ntest_data_additional_features = pd.merge(test_data_additional_features, \n                                         train_data.loc[:, ['StockCode', 'CustomerID', 'MeanUnitPriceperCustomerandStock']].drop_duplicates(),\n                                         on=['CustomerID', 'StockCode'],\n                                         how='left')\ntest_data_additional_features.isnull().mean() * 100","4a1b7ef6":"# Too many null values so drop\ntrain_data.drop(columns=['MeanUnitPriceperCustomerandStock'], inplace=True)\ntest_data_additional_features.drop(columns=['MeanUnitPriceperCustomerandStock'], inplace=True)","d8b87bdf":"train_data.columns","7c13cf03":"# For these features we will need to concat the train and test data\n# Create temp Index columns\ntrain_data['IsTrain'] = 1\ntest_data_additional_features['IsTrain'] = 0\ntrain_data['Index'] = train_data.index\ntest_data_additional_features['Index'] = test_data_additional_features.index\nUnitPrice = train_data['UnitPrice']\ntrain_data = train_data.loc[:, ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate',\n       'CustomerID', 'Country', 'Year', 'Month', 'DayOfMonth',\n       'DayOfWeek', 'HourOfDay', 'IsWeekend', 'Mean_UnitPrice_per_Stock',\n       'Median_UnitPrice_per_Stock', 'Min_UnitPrice_per_Stock',\n       'Max_UnitPrice_per_Stock', 'NumberOfDifferentPricesStockSold',\n       'Average_Amt_Spent_Customer', 'Average_UnitPrice_Customer',\n       'Median_UnitPrice_Customer', 'Min_UnitPrice_Customer',\n       'Max_UnitPrice_Customer', 'NumberOfDifferentStocksPurchasedCustomer',\n       'Is_Return', 'RatioQtyUnitPriceperStockCode','Index', 'IsTrain']]\ntest_data_additional_features = test_data_additional_features.loc[:, ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate',\n       'CustomerID', 'Country', 'Year', 'Month', 'DayOfMonth',\n       'DayOfWeek', 'HourOfDay', 'IsWeekend', 'Mean_UnitPrice_per_Stock',\n       'Median_UnitPrice_per_Stock', 'Min_UnitPrice_per_Stock',\n       'Max_UnitPrice_per_Stock', 'NumberOfDifferentPricesStockSold',\n       'Average_Amt_Spent_Customer', 'Average_UnitPrice_Customer',\n       'Median_UnitPrice_Customer', 'Min_UnitPrice_Customer',\n       'Max_UnitPrice_Customer', 'NumberOfDifferentStocksPurchasedCustomer',\n       'Is_Return', 'RatioQtyUnitPriceperStockCode','Index', 'IsTrain']]\ntrain_test_concat = pd.concat([train_data, test_data_additional_features], axis=0)\ntrain_test_concat.head()","b103adfe":"# Number of times a Stock is purchased and it's rank per Customer\ntrain_test_concat['Num_times_Stock_purchased_Customer'] = train_test_concat.groupby(['CustomerID', 'StockCode'])['StockCode'].transform('count')\ntrain_test_concat['Rank_Stock_Cust'] = \\\ntrain_test_concat.groupby(['CustomerID'])['Num_times_Stock_purchased_Customer'].rank(ascending=False, method='dense')\ntrain_test_concat.head()","e6f3ab44":"# Recency of each Stock purchased by Customer\ntrain_test_concat['Max_date_per_Customer'] = train_test_concat.groupby(['CustomerID'])['InvoiceDate'].transform('max')\ntrain_test_concat['Max_date_per_Stock_per_Customer'] = train_test_concat.groupby(['CustomerID', 'StockCode'])['InvoiceDate'].transform('max')\ntrain_test_concat['Stock_Purchased_Recency_per_Customer'] = train_test_concat['Max_date_per_Customer'] - train_test_concat['Max_date_per_Stock_per_Customer']\ntrain_test_concat['Stock_Purchased_Recency_per_Customer'] = train_test_concat['Stock_Purchased_Recency_per_Customer'].dt.days\ntrain_test_concat.head()","f2ea2cb6":"# Drop temp columns and get back train and test\ntrain_test_concat.columns\ntrain_test_concat.drop(columns=['Max_date_per_Customer', 'Max_date_per_Stock_per_Customer'], inplace=True)\ntrain_data = train_test_concat[train_test_concat['IsTrain'] == 1]\ntest_data_additional_features = train_test_concat[train_test_concat['IsTrain'] == 0]\ntrain_data.sort_values('Index', inplace=True)\ntest_data_additional_features.sort_values('Index', inplace=True)\n# Drop again\ntrain_data.drop(columns = ['IsTrain', 'Index'], inplace=True)\ntest_data_additional_features.drop(columns = ['IsTrain', 'Index'], inplace=True)\n# Add back UnitPrice\ntrain_data['UnitPrice'] = UnitPrice","e425cb8c":"# Final features for modelling- Let's keep only features we will be using in train and test\ntrain_data.drop(columns=['InvoiceNo', 'Description', 'InvoiceDate', 'CustomerID', 'StockCode'], inplace=True)\ntest_data_additional_features.drop(columns=['InvoiceNo', 'Description', 'InvoiceDate', 'CustomerID', 'StockCode'], inplace=True)","390ea485":"# Let's see correlations again\ncorr = train_data.corr()\n# Let's see only the significant correlations\ncorr = corr[(corr >= 0.4) | (corr <= -0.4)]\n# Set up a mask\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 10))\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, annot=True, square=True)\nplt.show()","3a91ede9":"# The largest value is simply too much of an Outlier\ntrain_data = train_data[train_data['UnitPrice'] < train_data['UnitPrice'].max()]","419875ee":"train_data['UnitPrice'].max()","65e775ad":"# Let's create a column of High, Low and Medium value purchases which we will use to stratify and create a Validation set- This will just be a temp variable\nIQR = train_data['UnitPrice'].quantile(0.75) - train_data['UnitPrice'].quantile(0.25)\nlow_ceiling = train_data['UnitPrice'].quantile(0.25) - 0.25*IQR\nhigh_ceiling = (1.5 * IQR) + train_data['UnitPrice'].quantile(0.75) \ntrain_data['Purchase_Category'] = np.where(train_data['UnitPrice'] >= high_ceiling, 'High_Value', \n                                          np.where(train_data['UnitPrice'] <= low_ceiling, 'Low_Value', 'Medium_Value'))\ntrain_data['Purchase_Category'] .value_counts()","a1deb359":"X_train, X_validation, Y_train, Y_validation = train_test_split(train_data.drop(columns=['UnitPrice']), \n                                                                train_data['UnitPrice'], \n                                                                test_size=0.20,\n                                                                stratify = train_data['Purchase_Category'],\n                                                                random_state=123)\nX_train.head()","bd557e80":"X_validation['Purchase_Category'].value_counts() \/ X_validation['Purchase_Category'].shape[0]","7526c8df":"X_train['Purchase_Category'].value_counts() \/ X_train['Purchase_Category'].shape[0]","975a7e83":"# Drop the Purchase category\nX_train.drop(columns=['Purchase_Category'], inplace=True)\nX_validation.drop(columns=['Purchase_Category'], inplace=True)","ce91f925":"X_train.shape, X_validation.shape","22fc884b":"# Scale both X_train and X_validation separately\ntransformer = RobustScaler().fit(X_train)\nX_train_scaled = transformer.transform(X_train)\ntransformer = RobustScaler().fit(X_validation)\nX_validation_scaled = transformer.transform(X_validation)","e1248cc4":"# View X_train\nX_train_scaled_df = pd.DataFrame(X_train_scaled)\nX_train_scaled_df.columns = X_train.columns\nX_train_scaled_df.head()","60bfa07b":"# Also scale test\ntransformer = RobustScaler().fit(test_data_additional_features)\ntest_data_additional_features_scaled = transformer.transform(test_data_additional_features)","364a3285":"# Get Model evaluation measures - rmse and r2_score\ndef get_model_evaluation_metrics(actual_value, predicted_value):\n    rmse = np.sqrt(mean_squared_error(actual_value, predicted_value))\n    rsquared = r2_score(actual_value, predicted_value)\n    return rmse, rsquared","d9bb9b7c":"# Initiate Model\nLR_model = LinearRegression()\n# Fit Model\nLR_model.fit(X_train_scaled, Y_train)\n# Get predictions on validation data\nY_pred = LR_model.predict(X_validation_scaled)\nrmse, rsquared = get_model_evaluation_metrics(Y_validation, Y_pred)\nprint(\"Model rmse on validation data: \", rmse, \"\\nModel rsquared on validation data: \", rsquared)","c41bc8da":"# Get predictions on Test and write for submission\nPredicted_Y = LR_model.predict(test_data_additional_features_scaled)\nPredicted_Y = pd.DataFrame(Predicted_Y)\nPredicted_Y.columns = ['UnitPrice']\n# Convert Prices < 0  to 0\nPredicted_Y['UnitPrice'] = np.where(Predicted_Y['UnitPrice'] < 0, 0, Predicted_Y['UnitPrice'])\nPredicted_Y.to_csv(\".\/Baseline Model Pred.csv\", index=False)","79c4e3b4":"# Initiate Model\nET_model = ExtraTreesRegressor(n_estimators=100, n_jobs=-1, random_state=10, verbose=2)\n                                      \n# Fit Model\nET_model.fit(X_train_scaled, Y_train)","d96a631a":"Y_pred = ET_model.predict(X_validation_scaled)\nY_pred = np.where(Y_pred <0, 0, Y_pred)","013be947":"# Get Model evaluation measures - rmse and r2_score\nrmse, rsquared = get_model_evaluation_metrics(Y_validation, Y_pred)\nprint(\"Model rmse on validation data: \", rmse, \"\\nModel rsquared on validation data: \", rsquared)","7b543fd9":"# Get predictions on Test and write for submission\nPredicted_Y = ET_model.predict(test_data_additional_features_scaled)\nPredicted_Y = pd.DataFrame(Predicted_Y)\nPredicted_Y.columns = ['UnitPrice']\nPredicted_Y['UnitPrice'] = np.where(Predicted_Y['UnitPrice'] <0, 0, Predicted_Y['UnitPrice'])\nPredicted_Y.to_csv(\".\/Extra trees model.csv\", index=False)","5a174128":"# Initiate Model\nGBM_model = GradientBoostingRegressor(learning_rate=0.1, n_estimators=1000,\n                                      random_state=42, verbose=1)\n                                      \n# Fit Model\nGBM_model.fit(X_train_scaled, Y_train)","54917dad":"Y_pred = GBM_model.predict(X_validation_scaled)\nY_pred = np.where(Y_pred <0, 0, Y_pred)","9b7b1b9e":"# Get Model evaluation measures - rmse and r2_score\nrmse, rsquared = get_model_evaluation_metrics(Y_validation, Y_pred)\nprint(\"Model rmse on validation data: \", rmse, \"\\nModel rsquared on validation data: \", rsquared)","d6022982":"# Get predictions on Test and write for submission\nPredicted_Y = GBM_model.predict(test_data_additional_features_scaled)\nPredicted_Y = pd.DataFrame(Predicted_Y)\nPredicted_Y.columns = ['UnitPrice']\nPredicted_Y['UnitPrice'] = np.where(Predicted_Y['UnitPrice'] <0, 0, Predicted_Y['UnitPrice'])\nPredicted_Y.to_csv(\".\/GBM_model_preds.csv\", index=False)","a3d34c14":"# Plot feature Importance\ndef plot_feature_importance(feature_importance_array, col_names):\n    '''Create a proper feature importance plot for sklearn models'''\n    feature_importance_data = pd.DataFrame(columns=['Feature', 'Feature_Importance_Score'])\n    feature_importance_data['Feature'] = pd.Series(col_names)\n    feature_importance_data['Feature_Importance_Score'] = pd.Series(feature_importance_array)\n    feature_importance_data.sort_values('Feature_Importance_Score', ascending=False, inplace=True)\n    sns.barplot(feature_importance_data[\"Feature_Importance_Score\"], feature_importance_data[\"Feature\"], orient=\"h\", palette='magma')\n    plt.show()\nplot_feature_importance(GBM_model.feature_importances_, X_train.columns)","6b7e9bd1":"from sklearn.metrics import make_scorer","ed015cd5":"#Define sklearn compatible eval metric\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\nrmse_sk = make_scorer(rmse, greater_is_better=False)","6e6115c2":"def run_lightgbm(X_tr, y_tr, X_val, y_val, lgb_params, feat_name=None, cat_feats=None):\n    model = lgb.LGBMRegressor(**lgb_params)\n    if cat_feats is None:\n        cat_feats = []\n    if feat_name is None:\n        feat_name = []\n    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric=[\"rmse\"], verbose=50, early_stopping_rounds=500)\n    val_preds = model.predict(X_val)\n    score = rmse(y_val, val_preds)\n    return score, model, val_preds","5a723b6d":"lgb_params = {\n                 \"n_estimators\": 20000,\n                 \"learning_rate\": 0.05,\n                 \"subsample\": 0.7,\n                 \"colsample_bytree\": 0.2,\n                 \"num_leaves\": 15,\n                 \"reg_lambda\": 1000,\n                 \"reg_alpha\": 10,\n                 \"max_depth\": 5,\n                 \"min_child_samples\": 100,\n                 \"min_child_weight\": 1,\n                 \"metrics\": None,\n                 \"seed\": 1,\n                 \"boosting_type\": 'gbdt',\n                 \"min_data_per_group\": 100\n               }","685d79e6":"base_score, model, val_preds = run_lightgbm(X_train_scaled, Y_train, X_validation_scaled, Y_validation, lgb_params)","de535fe5":"# Get Model evaluation measures - rmse and r2_score\nY_pred = model.predict(X_validation_scaled)\nY_pred = np.where(Y_pred <0, 0, Y_pred)\nrmse, rsquared = get_model_evaluation_metrics(Y_validation, Y_pred)\nprint(\"Model rmse on validation data: \", rmse, \"\\nModel rsquared on validation data: \", rsquared)","2edbd3e0":"# Get predictions on Test and write for submission\nPredicted_Y = model.predict(test_data_additional_features_scaled)\nPredicted_Y = pd.DataFrame(Predicted_Y)\nPredicted_Y.columns = ['UnitPrice']\nPredicted_Y['UnitPrice'] = np.where(Predicted_Y['UnitPrice'] < 0, 0, Predicted_Y['UnitPrice'])\nPredicted_Y.to_csv(\".\/Light GBM preds.csv\", index=False)","0650bede":"# Ensemble the preds from Extra trees, GBM and lgbm models\nET_preds = pd.read_csv('.\/Extra trees model.csv')\nGBM_preds = pd.read_csv('.\/GBM_model_preds.csv')\nLGB_preds = pd.read_csv('.\/Light GBM preds.csv')\nET_preds.rename(columns = {'UnitPrice':'UnitPriceETModel'}, inplace=True)\nGBM_preds.rename(columns = {'UnitPrice':'UnitPriceGBMModel'}, inplace=True)\nLGB_preds.rename(columns = {'UnitPrice':'UnitPriceLGBModel'}, inplace=True)\nEnsemble_preds = pd.concat([ET_preds, GBM_preds, LGB_preds] , axis=1)\nEnsemble_preds['UnitPrice'] = 0.30 * ET_preds['UnitPriceETModel'] + 0.20 * GBM_preds['UnitPriceGBMModel'] + 0.50 * LGB_preds['UnitPriceLGBModel']\nEnsemble_preds.head()","8aea47dc":"# Write and Submit\nEnsemble_preds = Ensemble_preds.loc[:, ['UnitPrice']]\nEnsemble_preds.to_csv('.\/Ensemble_final_sub.csv', index=False)","7d0b66b7":"# Load datasets and read dates properly \ntrain_data = pd.read_csv('\/kaggle\/input\/great-indian-hiring-hack-data\/Train.csv', parse_dates=[\"InvoiceDate\"])\ntest_data = pd.read_csv('\/kaggle\/input\/great-indian-hiring-hack-data\/Test.csv', parse_dates=[\"InvoiceDate\"])","9f615fed":"# We need only StockCode, Invoice Date and Unit Price for this. Drop other features\ntrain_data = train_data.loc[:, ['StockCode', 'InvoiceDate', 'UnitPrice']]\ntrain_data.drop_duplicates(inplace=True)\ntest_data = test_data.loc[:, ['StockCode', 'InvoiceDate']]","2fc0cc82":"# Add an Index column to test as we will need it later to preserve the data shape and order(for submission)\ntest_data['Index'] = test_data.index\ntest_data = test_data.loc[:, ['Index', 'StockCode', 'InvoiceDate']]\ntest_data.head()","716bbf8d":"# Try on Sample StockCode = 510\ntest_data_sample = test_data[test_data['StockCode'] == 510]","bfdb0662":"test_data_sample.shape","a7d92ca3":"# Join train and see shape\ntest_data_sample_merged_train = pd.merge(test_data_sample, train_data.rename(columns={'InvoiceDate':'InvoiceDateTrain'}),\n                                         on=['StockCode'], how='left')\ntest_data_sample_merged_train.shape","f15a614d":"test_data_sample_merged_train.head()","299593e2":"# Now get the closest date for each InvoiceDate in test\ntest_data_sample_merged_train['TimeDeltaTestTrainInvoiceDate'] = test_data_sample_merged_train['InvoiceDate'] - test_data_sample_merged_train['InvoiceDateTrain']\ntest_data_sample_merged_train['TimeDeltaTestTrainInvoiceDate'] = abs(test_data_sample_merged_train['TimeDeltaTestTrainInvoiceDate'])\ntest_data_sample_merged_train.head()","430ad65e":"# Group by Index and get MinTimeDelta for each\ntest_data_sample_merged_train['MinTimeDelta'] = test_data_sample_merged_train.groupby('Index')['TimeDeltaTestTrainInvoiceDate'].transform(min)\ntest_data_sample_merged_train.head()","28ace1d4":"# Keep only the rows where TimeDelta is minimum\ntest_data_sample_merged_train = \\\ntest_data_sample_merged_train[test_data_sample_merged_train['TimeDeltaTestTrainInvoiceDate'] == test_data_sample_merged_train['MinTimeDelta']]\ntest_data_sample_merged_train.head()","ec36119d":"# Let's see if the shape is still same\ntest_data_sample_merged_train.shape","d601f9fb":"# Now repeat this for the entire test dataset\n# Join train and see shape\ntest_data_merged_train = pd.merge(test_data, train_data.rename(columns={'InvoiceDate':'InvoiceDateTrain'}),\n                                         on=['StockCode'], how='left')\ntest_data_merged_train.shape","3a6f08da":"# There will be some cases where the StockCode is missing from Train. In such case InvoiceDateTrain, UnitPrice etc will be NULLS\n# Let's separate them out and we will use the GBM model preds to get predictions for them\ntest_data_merged_train_nas = test_data_merged_train[test_data_merged_train['UnitPrice'].isna()]\ntest_data_merged_train.dropna(inplace=True)\ntest_data_merged_train_nas.head()","14bc80e0":"# Now get the closest date for each InvoiceDate in test\ntest_data_merged_train['TimeDeltaTestTrainInvoiceDate'] = test_data_merged_train['InvoiceDate'] - test_data_merged_train['InvoiceDateTrain']\ntest_data_merged_train['TimeDeltaTestTrainInvoiceDate'] = abs(test_data_merged_train['TimeDeltaTestTrainInvoiceDate'])\ntest_data_merged_train.head()","c24c3edd":"# Group by Index and get MinTimeDelta for each\ntest_data_merged_train['MinTimeDelta'] = test_data_merged_train.groupby('Index')['TimeDeltaTestTrainInvoiceDate'].transform(min)\ntest_data_merged_train.head()","d0d56356":"# Keep only the rows where TimeDelta is minimum\ntest_data_merged_train = \\\ntest_data_merged_train[test_data_merged_train['TimeDeltaTestTrainInvoiceDate'] == test_data_merged_train['MinTimeDelta']]\ntest_data_merged_train.head()","06763606":"# Let's see if the shape is still same\nif (test_data_merged_train.shape[0] == test_data.shape[0]):\n    print('Successfull')\nelse:\n    print('Unsuccessful \/n', 'Original number of rows in test: ', test_data.shape[0], '\/n Number of rows post merge', test_data_merged_train.shape[0])","c9ad77f2":"# We will need to inspect this Manually. Take out the data along with a grouped column for numbers of each Index\ntest_data_merged_train['Rows_per_Index'] = test_data_merged_train.groupby('Index')['Index'].transform('count')\ntest_data_merged_train.sort_values('Index', inplace=True)\ntest_data_merged_train.to_csv('.\/Test_data_forchecking.csv', index=False)","63bf7a9d":"test_data_merged_train['MeanUnitPriceperIndex'] = test_data_merged_train.groupby('Index')['UnitPrice'].transform('mean')\n# Drop Original UnitPrice and duplicates\ntest_data_merged_train.drop(columns=['UnitPrice', 'InvoiceDateTrain'], inplace=True)\ntest_data_merged_train.drop_duplicates(inplace=True)","4aae42a1":"# Now get predictions for the portion of data with StockCodes not present in train\nEnsemble_preds = pd.read_csv('.\/Ensemble_final_sub.csv')\nEnsemble_preds['Index'] = Ensemble_preds.index\nEnsemble_preds.head()","d5495139":"# Merge with the data\ntest_data_merged_train_nas = pd.merge(test_data_merged_train_nas.drop(columns=['UnitPrice']), Ensemble_preds, how='left', on='Index')\ntest_data_merged_train_nas.head()","d2ba602a":"# Price cannot be below 0. Set to 0 if <0\ntest_data_merged_train_nas['UnitPrice'] = np.where(test_data_merged_train_nas['UnitPrice']<0, 0, test_data_merged_train_nas['UnitPrice'])\ntest_data_merged_train_nas.head()","f7b8bc9b":"# Keep only necessary columns in both test data and concat. Then sort by index and finally check if num rows is correct\ntest_data_merged_train.rename(columns={'MeanUnitPriceperIndex':'UnitPrice'}, inplace=True)\ntest_data_merged_train = test_data_merged_train.loc[:, ['Index', 'StockCode', 'InvoiceDate', 'UnitPrice']]\ntest_data_merged_train_nas = test_data_merged_train_nas.loc[:, ['Index', 'StockCode', 'InvoiceDate', 'UnitPrice']]\n# Concat\ntest_data_merged_train_final = pd.concat([test_data_merged_train, test_data_merged_train_nas], axis=0)\n# Sort\ntest_data_merged_train_final.sort_values('Index', inplace=True)","ac624e2d":"# Check shape\nif (test_data_merged_train_final.shape[0] == test_data.shape[0]):\n    print('Successfull')\nelse:\n    print('Unsuccessful \/n', 'Original number of rows in test: ', test_data.shape[0], '\/n Number of rows post merge', test_data_merged_train_final.shape[0])","97f8ec87":"# Write Final preds\nPredicted_Y = test_data_merged_train_final['UnitPrice']\nPredicted_Y = pd.DataFrame(Predicted_Y)\nPredicted_Y.columns = ['UnitPrice']\nPredicted_Y.to_csv(\".\/No_Model_Pred_using_Date_logic.csv\", index=False)","45803385":"Clearly there is no single Unique Identifier in this Data set. Each Invoice can contain Multiple items (Stock code) and it seems that Unit price varies for the same Stock No. too. Let's verify that with the most common Stock code","910e0b32":"#### Baseline- Multivariate Linear Regression","3db7ec24":"Seems StockCode and Description go together (1:1 relationship). Let's see if True","867e4237":"#### Feature Creation- Let's explore and create some features from the Invoice Date first","45c0c735":"Does seem like the Unit price is lower on weekends on average as compared to wekdays","8fd7163f":"Let's plot UnitPrice against some of the datetime features created to see if there are any clear patterns","1241b85f":"Doesn't really seem like Customer level Grouped features would be as important as Stock level ones but we will create them nevertheless.","d15aa5b6":"The 75th Quartile of unit price is just 3.75 and even the 99 quartile value is just 15 which is far from the max value of close to 40000. Either we should clip the Outliers or take a closer look to spot patterns if any. Will do this later","3278e988":"#### Light GBM model- No tuning","3d1deb29":"#### Creating Training and validation sets from the train data","a80e3a95":"Only 0.075% NULL values. We will simply impute with Mean, Median etc as per the feature","06adbf17":"Interesting that there are duplicates in train and test both. Being a hackathon we cannot drop the duplicates in test to preserve the shape but we will drop them in train.","a80052ff":"No it seems the same Stock Code may have multiple Descriptions (Probably at different points in time such as Sales?)","7aeb56fe":"#### Univariate Analysis- Boxplots, Histograms, Scatterplots etc.","6c253b0b":"**Observations**\n- Clearly it is redundant to have so many date time features some of which are perfectly correlated. Let us drop Quarter, WeekOfYear and DateOfYear which are all perfectly correlated to Month and then re check the correlation matrix\n- Significantly there is no real correlation between the dependent and any of Independent variables at all","9cd1261f":"The final submission I made was using the above UnitPrice not as a submission but as a feature in test with shifted Unit Prices (last days price) as the corresponding feature in Train. This portion was accidentally removed from the notebook but will add it back in future","c412e20c":"We had seen in the distribution of Train and Test that Stock Codes were randomly distributed between train and test for the same Invoice which indicates that the data was randomly sampled. We will use the same methodology here too and create a 80-20 split of train and validation","fa76317c":"Looking at the data there are some cases where the Time Delta is exactly equal or there are two different Prices for the same Item at the exact same time. So we will simply groupby and take a mean on the UnitPrice in such cases as the final value","2f2f139b":"**Work in Progress**  \nLeave an upvote if you found anything useful \ud83d\ude04","b14c0e56":"#### GBM Model- No Tuning","73329368":"#### Bivariate and Multivariate Analysis- Let's now explore in more detail the relation between Price and Independent Variables as well as between Variables themselves","faa058d7":"### EDA- Univariate, Bivariate Analysis and Feature creation","020b5adf":"Not really any insights from this","ec50ec82":"### Model Building, Prediction and Evaluation","7c2e6e40":"#### Customer and Stock Interaction Features","18dabb00":"Looking at this Quantity is not as highly skewed but almost seems to have a normal distribution with a very high peak around 0 and very long tails on both sides. What is strange however is the negative values in Quantity. Let's look into this","c8696fc6":"#### Scaling and Outlier Treatment","e905b596":"Looking at the first 2 values negative Quantities would seem to signify returns as the same StockCode and same Unit Price are used for the same Customer just with negative Qty.  \n**We can use this fact to directly predict Unit Prices in test for negative Quantities if the same Qunatities exist in train for same customer. Ditto for positive prices too if negative Quantities exist.** ","2ce28e4c":"We saw that the only column to contain Outliers was UnitPrice which is the dependent variable","543f20ba":"There isn't any clear relation with Month. However average Price on weekends does seem to be lower compared to weekdays","76eb30f7":"Country 35 is dominant in terms of transactions and there doesn't seem to be much of a difference in terms of Mean Stock Price between different Countries. All this tells us that Grouped features relating to StockID will probably be the most important in predicting UnitPrices for Unknown data","8c529a84":"#### Alternate Strategy using no model- Going through the data depicts that prices change periodically. We can predict the price in Train for a StockCode as the closest price for that Stock up to that time. For StockCodes in test that don't exist in train just use the predicted prices from the best Model","ee474f72":"### Read dataset and Initial exploration","3c534612":"#### Extra Trees Regressor","88e3a8fc":"## Great Indian Hiring Hackathon\n**`Overview`**    \nThis competition was organized by Machine Hack platform in collaboration with 12 companies as a screening round for potential canidates.The task was to predict UnitPrices for a number of Items sold over a 2 year period. Primary hurdles in the task were:-  \n- All features were encoded making it difficult to make proper sense of them. This was particularly problematic for ```Descriptions``` as it virtually rendered the feature useless.\n- Evaluation metric was RMSE which is particularly problematic due to presence of large outliers in the Dependent variable. Getting even 1 or 2 of these large Outlier values wrong leads to very high RMSE scores even if the validation set has been created properly and models are tuned carefully. Frankly the only way I find around this is to use ensembles (even this is only partially effective). \n- Large discrepancies between Pvt and Public LB due to point 2 above \n\n**`Problem Statement`**    \nThe task is to predict Unit Price of commodities given various encoded attributes such as Stock Code, Description, Qty, Invoice date etc. Due to the encoded nature of all attributes figuring out relations are difficult at a glance. So EDA, proper feature extraction and Data cleaning are probably going to be the major difference makers in the competition. Creating a representative validation set and Model tuning might also trun out to be important\n\n**My Approach**  \n- Intially concentrated on creating Grouped features using StockCode and CustomerID such as mean UnitPrice per StockCode, Maximum spend per customer etc.\n- Observed the presence of negative Quantities and corresponding positive ones. Theorized that negative Quantites mean cancellations and UnitPrice would be exactly equal in such cases. Used this to try and supplement model results but it didn't really help (Cancellation prices not always equal to buying prices). This part is not included in the Notebook as it was an early draft.\n- Observed that UnitPrices for a StockCode are same in most cases (atleast 90 percent). Used this and Invoice Dates to create a logic for Price Prediction in test without using any models. Later used these as a feature for Model building rather than direct predictions\n- Final predictions are an Ensemble of 3 Models using the most likely prices feature created in Step 3 plus other grouped features mentioned in Step 1. This has a score of ~22 on Pvt LB which would be Top 10 rank in the competition.","e3fbd95f":"Very few Null values so again we will impute with the earlier strategy","fcc05d19":"We will try out several Models from simple to complex. Let's create the baseline model as a Linear Regression Model and get predictions. We will improve from there","3d47d388":"So it's not that expensive stocks are sold rarely e.g. 3681 is sold more than 300 times and has a Mean price of nearly 300. It would be worthwhile to create grouped features using Stock Code even if we don't use StockCode itself directly.","9df492f9":"Don't seem to be any NULL values in the dataset. Also all attributes are encoded hence even categorical features are read as numeric","8c7b6e2b":"These results are not very good but not terrible","1603c52a":"#### Corrplot- Let's see the correlation between features","c172cc3a":"Interesting that there are 16985 common invoices between train and test. Let's see a bit of this data","fbdad0b7":"Invoices are split between train and test. One Invoice is unique to a single customer. There are some cases where the same StockCode appears multiple times with different Quantities in the same Invoice. Unit Price should be same in such cases.","466ca072":"#### Additional Feature Creation- Grouped Features","68db2b7e":"On Weekends trading hours seem between 9 to 16 only but this is not really important"}}