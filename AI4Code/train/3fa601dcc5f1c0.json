{"cell_type":{"ce3511e3":"code","27f79809":"code","f34b1907":"code","91feabe8":"code","4e011db4":"code","91565c8a":"markdown","81941a5f":"markdown","7df26a25":"markdown","f02cab8d":"markdown","8c44648f":"markdown","c98bf8d2":"markdown","98a66138":"markdown","c21d3b91":"markdown","36122994":"markdown","06e8b166":"markdown"},"source":{"ce3511e3":"# Forward pass\nx = [ 1.0 , - 2.0 , 3.0 ] # input values\nw = [ - 3.0 , - 1.0 , 2.0 ] # weights\nb = 1.0 # bias\n\n# Multiplying inputs by weights\nxw0 = x[ 0 ] * w[ 0 ]\nxw1 = x[ 1 ] * w[ 1 ]\nxw2 = x[ 2 ] * w[ 2 ]\n\nprint (xw0, xw1, xw2, b)\n\n# Adding weighted inputs and a bias\nz = xw0 + xw1 + xw2 + b\nprint (z)\n\n# ReLU activation function\ny = max (z, 0 )\nprint (y)","27f79809":"# Forward pass\nx = [ 1.0 , - 2.0 , 3.0 ] # input values\nw = [ - 3.0 , - 1.0 , 2.0 ] # weights\nb = 1.0 # bias\n\n# Multiplying inputs by weights\nxw0 = x[ 0 ] * w[ 0 ]\nxw1 = x[ 1 ] * w[ 1 ]\nxw2 = x[ 2 ] * w[ 2 ]\n\n# Adding weighted inputs and a bias\nz = xw0 + xw1 + xw2 + b\n\n# ReLU activation function\ny = max (z, 0 )\n\n# Backward pass\n# The derivative from the next layer\ndvalue = 1.0\n\n# Derivative of ReLU and the chain rule\ndrelu_dz = dvalue * ( 1. if z > 0 else 0. )\nprint (drelu_dz)\n\n# Partial derivatives of the multiplication, the chain rule\ndsum_dxw0 = 1\ndrelu_dxw0 = drelu_dz * dsum_dxw0\nprint (drelu_dxw0)\n\n#We can then perform the same operation with the next weighed input and so on","f34b1907":"#Let\u2019s add these partial derivatives, with the applied chain rule, to our code:\n\n# Forward pass\nx = [ 1.0 , - 2.0 , 3.0 ] # input values\nw = [ - 3.0 , - 1.0 , 2.0 ] # weights\nb = 1.0 # bias\n\n# Multiplying inputs by weights\nxw0 = x[ 0 ] * w[ 0 ]\nxw1 = x[ 1 ] * w[ 1 ]\nxw2 = x[ 2 ] * w[ 2 ]\n\n# Adding weighted inputs and a bias\nz = xw0 + xw1 + xw2 + b\n\n# ReLU activation function\ny = max (z, 0 )\n\n# Backward pass\n# The derivative from the next layer\ndvalue = 1.0\n\n# Derivative of ReLU and the chain rule\ndrelu_dz = dvalue * ( 1. if z > 0 else 0. )\nprint (drelu_dz)\n\n# Partial derivatives of the multiplication, the chain rule\ndsum_dxw0 = 1\ndsum_dxw1 = 1\ndsum_dxw2 = 1\ndsum_db = 1\ndrelu_dxw0 = drelu_dz * dsum_dxw0\ndrelu_dxw1 = drelu_dz * dsum_dxw1\ndrelu_dxw2 = drelu_dz * dsum_dxw2\ndrelu_db = drelu_dz * dsum_db\nprint (drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)","91feabe8":"# Forward pass\nx = [ 1.0 , - 2.0 , 3.0 ] # input values\nw = [ - 3.0 , - 1.0 , 2.0 ] # weights\nb = 1.0 # bias\n\n# Multiplying inputs by weights\nxw0 = x[ 0 ] * w[ 0 ]\nxw1 = x[ 1 ] * w[ 1 ]\nxw2 = x[ 2 ] * w[ 2 ]\n\n# Adding weighted inputs and a bias\nz = xw0 + xw1 + xw2 + b\n\n# ReLU activation function\ny = max (z, 0 )\n\n# Backward pass\n# The derivative from the next layer\ndvalue = 1.0\n\n# Derivative of ReLU and the chain rule\ndrelu_dz = dvalue * ( 1. if z > 0 else 0. )\nprint (drelu_dz)\n\n# Partial derivatives of the multiplication, the chain rule\ndsum_dxw0 = 1\ndsum_dxw1 = 1\ndsum_dxw2 = 1\ndsum_db = 1\ndrelu_dxw0 = drelu_dz * dsum_dxw0\ndrelu_dxw1 = drelu_dz * dsum_dxw1\ndrelu_dxw2 = drelu_dz * dsum_dxw2\ndrelu_db = drelu_dz * dsum_db\nprint (drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n\n# Partial derivatives of the multiplication, the chain rule\ndmul_dx0 = w[ 0 ]\ndmul_dx1 = w[ 1 ]\ndmul_dx2 = w[ 2 ]\ndmul_dw0 = x[ 0 ]\ndmul_dw1 = x[ 1 ]\ndmul_dw2 = x[ 2 ]\ndrelu_dx0 = drelu_dxw0 * dmul_dx0\ndrelu_dw0 = drelu_dxw0 * dmul_dw0\ndrelu_dx1 = drelu_dxw1 * dmul_dx1\ndrelu_dw1 = drelu_dxw1 * dmul_dw1\ndrelu_dx2 = drelu_dxw2 * dmul_dx2\ndrelu_dw2 = drelu_dxw2 * dmul_dw2\nprint (drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)","4e011db4":"import numpy as np \n\n# Suppose gradient is coming from the end. \ndvalues = np.array([[1., 1., 1.],\n                    [2., 2., 2.],\n                    3., 3., 3.])\n\n\n# set of inputs \ninputs = np.array([[1, 2, 3, 2.5],\n                   [2., 5., -1,  2],\n                   [-1.5, 2.7, 3.3, -0.8]])\n\n#weights \n\nweights = np.array( [[ 0.2, 0.8, -0.5, 1],\n                     [0.5, -0.91, 0.26, -0.5],\n                     [-0.26, -0.27, 0.17, 0.87]]).T\n\nbiases = np.array([[2, 3, 0.5]])\n\n#forward pass \nlayer_outputs = np.dot(inputs, weights) + biases \nrelu_outputs = np.maximum(0, layer_outputs)\n\n#backward pass \n#relu layer\ndrelu = relu_outputs.copy()\ndrelu[layer_outputs <= 0] = 0\n\n#dense layer\ndinputs = np.dot(drelu, weights.T)\ndweights = np.dot(inputs.T, drelu)\ndbiases = np.sum(drelu, axis = 0, keepdims = True)\n\nweights += -0.001 * dweights \nbiases += -0.001 * dbiases \n\nprint(weights)\nprint(biases )\n\n","91565c8a":"### Backpropagation using Numpy Tool and with gradients into a Matrix now to make code more easy .Here you will see my WEIGHTS are now updated .\n","81941a5f":"Let\u2019s add these partial derivatives, with the applied chain rule, to our code:\n\nThis is a good time to point out that, as we apply the chain rule in this way \u2014 working backward by taking the ReLU() \nderivative, taking the summing operation\u2019s derivative,\nmultiplying both, and so on, this is a process called backpropagation using the chain rule . \nAs the name implies, the resulting output function\u2019s gradients are passed back through the neural\nnetwork, using multiplication of the gradient of subsequent functions from later layers with the current one","7df26a25":"### Equation that will allow us to determine how to calculate the derivatives more easily:\ny = ReLU(sum(mul(x 0 , w 0 ), mul(x 1 , w 1 ), mul(x 2 , w 2 ), b))","f02cab8d":"# Code here just to show you how my weights get updated in Neural Networks without using any library.You will see how I just used python to show you the flow of gradients","8c44648f":"To calculate the impact of the example weight, w 0 , on\nthe output, the chain rule tells us to calculate the derivative of ReLU with respect to its parameter","c98bf8d2":"## oh YES ! I ACHIEVED IT","98a66138":"OK now to backpropagate let's find the derivaite of RELU FUNCTION wrt to previous parameters :\n\n- drelu_dxw0 \u2014 the partial derivative of the ReLU w.r.t. the first weighed input, w 0 x 0 ,\n- drelu_dxw1 \u2014 the partial derivative of the ReLU w.r.t. the second weighed input, w 1 x 1 ,\n- drelu_dxw2 \u2014 the partial derivative of the ReLU w.r.t. the third weighed input, w 2 x 2 ,\n- drelu_db \u2014 the partial d erivative of the ReLU with respect to the bias, b .","c21d3b91":"### Let's come to the last step where the ReLU activation function is applied on neuron output\n\nWill see how to Backpropagate the gradients using maths tool i.e Chain Rule and will get to know the concept of how this works then will find the how  gradients impact on my inputs and paramters by multiplying my old gradients with the news gradients gradients which I backpropagated from back","36122994":"### The first step is to backpropagate our gradients by calculating  partial derivatives with respect to each of our parameters and inputs.So will take Partial Derivative of the ReLU function and will apply chain rule to multiply with  wrt different functions accordingly ","06e8b166":"### We perform the same operation for other inputs and weights:"}}