{"cell_type":{"c4949040":"code","51a30796":"code","d1b0b348":"code","6d0e5f43":"code","2594492b":"code","b49ba377":"code","cfc98275":"code","b057b4f3":"code","90db04d2":"code","b3b7bc73":"code","089264b4":"code","8ff3eb8a":"code","2a48a870":"code","9a4c72e9":"code","0b60922c":"code","61cc1cc5":"code","d4ea2847":"code","fed64f1b":"code","faaf3b17":"code","9dcaad81":"code","61ca169b":"code","50e82423":"code","8d74cb0d":"code","9e35e785":"code","c849d23f":"code","10ad2b5a":"code","e6b2a99f":"code","d412a74c":"code","c24789ef":"code","2d906184":"code","14c6b597":"code","dd017c9c":"code","9d7558ee":"code","9e482d4d":"code","4de8b007":"code","21ddfbb0":"code","90a78b93":"code","ea1be049":"code","ef765477":"code","52e2b2f9":"code","c1240123":"code","19a26257":"code","6840a161":"code","ac73af0a":"code","042f2ad0":"code","280c79c7":"code","09d3f354":"code","c593661e":"code","ee9532fe":"code","b51210b3":"code","c724ac23":"code","9ef294fd":"code","1075f79f":"code","80d359fe":"code","75f0f5cc":"code","b91daabd":"code","1f457d39":"code","0301f841":"code","447bc87c":"code","4259b055":"code","27f9909f":"code","8cc3fee9":"code","c6981eb8":"code","15306de2":"code","9a2e2c00":"code","553b9089":"code","c3e0f660":"code","a916ad8d":"code","bb62e468":"code","cf26d0e1":"code","a729eb17":"code","a768e9e6":"code","cddcb95e":"code","dd4fe0d9":"code","92147f2d":"code","7a8472b2":"markdown","ebde4991":"markdown","018ed5b4":"markdown","1811c5db":"markdown","d126d0a2":"markdown","46a15682":"markdown","070e38ed":"markdown","9410c6e7":"markdown","9195b738":"markdown","bdfb4df4":"markdown","20fa1bb8":"markdown","2931e969":"markdown","453210cc":"markdown","9e4a5245":"markdown","c80cc0f0":"markdown","1fca6d53":"markdown","3c708737":"markdown","5d507ecd":"markdown","0f706234":"markdown"},"source":{"c4949040":"# \u30ed\u30b0\u306b\u51fa\u3059\u30ad\u30fc\u30ef\u30fc\u30c9\ndescription_of_this_commit='logistic regression stacking l2'\n\nseed = 71\nrounds = 100","51a30796":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport logging\nimport time\nimport scipy as sp\nimport itertools \nimport optuna","d1b0b348":"from sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nimport time\nfrom tqdm import tqdm_notebook as tqdm\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.dummy import DummyClassifier","6d0e5f43":"## \u74b0\u5883\u3054\u3068\u306e\u30d1\u30b9\u306e\u9055\u3044\u3092\u5438\u53ce\nimport os\nif 'KAGGLE_URL_BASE' in os.environ:\n    print('running in kaggle kernel')\n    data_dir = Path('..\/input')\n    log_dir = Path('.\/')\nelse:\n    print('running in other environment')\n    data_dir = Path('..\/data\/raw')\n    log_dir = Path('..\/log')\ndata_dir","2594492b":"logger = None","b49ba377":"def getLogger():\n    global logger\n    if logger is not None:\n        return logger\n    logfile = log_dir \/ 'all.log'\n    logger = logging.getLogger(description_of_this_commit)\n    formatter = logging.Formatter('%(asctime)s - [%(name)s] - %(levelname)s - %(message)s')\n    fh = logging.FileHandler(logfile, 'a+')\n    fh.setFormatter(formatter)\n    #fh.setLevel(logging.DEBUG)\n    logger.addHandler(fh)\n    logger.addHandler(logging.StreamHandler())\n    logger.setLevel(logging.DEBUG)\n    return logger","cfc98275":"logger = getLogger()","b057b4f3":"df_all_cache = None","90db04d2":"# \u4e00\u756a\u57fa\u672c\u7684\u306a\u30c7\u30fc\u30bf\u30ed\u30fc\u30c9\u304c\u7d42\u308f\u3063\u305f\u3068\u3053\u308d\u3067\u30ad\u30e3\u30c3\u30b7\u30e5\u3092\u4fdd\u5b58\u3057\u3066\u304a\u304d\u3001\u4e8c\u56de\u76ee\u4ee5\u964d\u306f\u30ad\u30e3\u30c3\u30b7\u30e5\u306e\u30b3\u30d4\u30fc\u3092\u8fd4\u3057\u3066\u9ad8\u901f\u5316\ndef load_data():\n    # load data from file or return cache\n    global df_all_cache\n    if df_all_cache is not None:\n        return df_all_cache.copy()\n    df_train = pd.read_csv(data_dir \/ 'train.csv', index_col=0)\n    df_test = pd.read_csv(data_dir \/ 'test.csv', index_col=0)\n    def lookup(s):    \n        #via: https:\/\/stackoverflow.com\/questions\/29882573\/pandas-slow-date-conversion\n        # \u884c\u3054\u3068\u306b\u6642\u523b\u3092parse\u3057\u306a\u3044\u3053\u3068\u3067\u9ad8\u901f\u5316\n        dates = {date:pd.to_datetime(date) for date in s.unique()}\n        return s.map(dates)\n    df_test['loan_condition'] = 0.0\n    df_train['is_train'] = True\n    df_test['is_train'] = False\n\n    df_all_cache = pd.concat([df_train, df_test], axis=0)\n    df_all_cache['issue_d'] = lookup(df_all_cache['issue_d'])\n    df_all_cache['earliest_cr_line'] = lookup(df_all_cache['earliest_cr_line'])\n    return df_all_cache.copy()","b3b7bc73":"def encode_missing_pattern(df_all):\n    # \u6b20\u640d\u5024\u306e\u5b58\u5728\u3059\u308b\u5217\u306e\u30d5\u30e9\u30b0\u3092\u4e26\u3079\u4e00\u3064\u306e2\u9032\u6570\u3068\u3057\u3066\u30a8\u30f3\u30b3\u30fc\u30c9\n    m = df_all.isnull().sum()\n    cols_with_missing = list(m[m != 0].index)\n\n    df_all['missing_pattern'] = 0\n    for col in cols_with_missing:\n        df_all['missing_pattern'] *= 2\n        df_all.loc[df_all[col].isnull(), 'missing_pattern'] += 1\n    \n    return df_all\n\ndef count_missing(df_all):\n    # \u6b20\u640d\u5024\u306e\u6570\u3092\u8fd4\u3059\n    df_all['missing_count'] = df_all.isnull().sum(axis=1)\n    return df_all","089264b4":"def missing_value_impute(df_all):\n    # \u30ab\u30e9\u30e0\u3054\u3068\u306b\u6c7a\u3081\u305f\u30eb\u30fc\u30eb\u306b\u5f93\u3044\u6b20\u640d\u5024\u88dc\u5b8c\n    numeric_cols = []\n    for col in df_all.columns:\n        if df_all[col].dtype in ['int64', 'float64']:\n            numeric_cols.append(col)\n    numeric_cols.remove('loan_condition')\n\n    #df_all[numeric_cols].isnull().sum()\n\n    imputation_rules = {\n     'acc_now_delinq': 0,\n     'annual_inc': 'median',\n     'collections_12_mths_ex_med': 0,\n     'delinq_2yrs': 0,\n     'dti': 'median',\n     'emp_length_num': 0,\n     'inq_last_6mths': 0,\n     'mths_since_last_delinq': 'median',\n     'mths_since_last_major_derog': 9999,\n     'mths_since_last_record': 'median',\n     'open_acc': 0,\n     'pub_rec': 0,\n     'revol_util': 'median',\n     'total_acc': 'median',\n     'tot_coll_amt': 0,\n     'tot_cur_bal': 'median',\n    }\n\n\n    import numbers\n    for col, v in df_all[numeric_cols].isnull().sum().iteritems():\n        if v == 0:\n            continue\n        if col not in imputation_rules:\n            print('rule not found!!')\n        col_missing = f'{col}_missing'\n        df_all[col_missing] = 0\n        df_all.loc[df_all[col].isnull(), col_missing] = 1\n        imputer = imputation_rules[col]\n        if isinstance(imputer, numbers.Number):\n            df_all.loc[df_all[col].isnull(), col] = imputer\n        elif imputer == 'median':\n            df_all.loc[df_all[col].isnull(), col] = df_all[col].median()\n\n    # \u30c6\u30b9\u30c8\n    col = 'annual_inc'\n    col_missing = f'{col}_missing'\n    if col_missing in df_all.columns:\n        print((df_all[df_all[col_missing] == 1][col] == df_all[col].median()).all())\n\n    col = 'acc_now_delinq'\n    col_missing = f'{col}_missing'\n    if col_missing in df_all.columns:\n        print((df_all[df_all[col_missing] == 1][col] == 0).all())\n\n    col = 'mths_since_last_major_derog'\n    col_missing = f'{col}_missing'\n    if col_missing in df_all.columns:\n        print((df_all[df_all[col_missing] == 1][col] == 9999).all())\n    return df_all","8ff3eb8a":"def add_ratios(df_all):\n    df_all = add_installment_ratio(df_all)\n    df_all = add_income_loan_ratio(df_all)\n    df_all = add_curbal_income_ratio(df_all)\n    df_all = add_install_dti_ratio(df_all)\n    return df_all","2a48a870":"def add_spi(df_all):\n    df_spi = load_spi_data()\n    df_all = df_all.reset_index().merge(df_spi, on='issue_d', how='left').set_index('ID')\n    return df_all\n","9a4c72e9":"def add_states(df_all):\n    ## state\n    df_state = load_state_data()\n    df_all = df_all.reset_index().merge(df_state, on='addr_state', how='left').set_index('ID')\n    return df_all","0b60922c":"def add_other_features(df_all):\n    df_all = add_ratios(df_all)\n    \n    df_all = add_spi(df_all)\n    df_all = add_states(df_all)\n    df_all = add_time_features(df_all)    \n    return df_all","61cc1cc5":"def add_annual_inc_is_clean(df_all):\n    # \u5e74\u53ce\u304c\u30ad\u30ea\u306e\u3044\u3044\u6570\u304b\u3069\u3046\u304b\n    df_all['annual_inc_is_clean100'] = df_all['annual_inc'].apply(lambda x: 1 if x%100 ==0 else 0)\n    df_all['annual_inc_is_clean1000'] = df_all['annual_inc'].apply(lambda x: 1 if x%1000 ==0 else 0)\n    df_all['annual_inc_is_clean10000'] = df_all['annual_inc'].apply(lambda x: 1 if x%10000 ==0 else 0)\n    \n    return df_all","d4ea2847":"def predict_annual_inc(df_all):\n    # \u81ea\u5df1\u7533\u544a\u5e74\u53ce\u304c\u30ad\u30ea\u306e\u3088\u304f\u306a\u3044\u4eba\u3092\u6559\u5e2b\u30c7\u30fc\u30bf\u3068\u3057\u3066\u4ed6\u306e\u4eba\u306e\u5e74\u53ce\u3092\u4e88\u6e2c\u3057\u3001\u4e88\u5b9f\u306e\u4e56\u96e2\u3092\u7279\u5fb4\u91cf\u5316\u3059\u308b\n    df_all_inc_pred = df_all.copy()\n    df_all_inc_pred.loc[df_all_inc_pred['annual_inc'] > 1000000, 'annual_inc'] = 1000000\n    X_train = df_all_inc_pred[df_all_inc_pred['is_train'] & ~df_all_inc_pred['annual_inc_is_clean1000']].drop(columns=['annual_inc'])\n    y_train = df_all_inc_pred[df_all_inc_pred['is_train'] & ~df_all_inc_pred['annual_inc_is_clean1000']]['annual_inc']\n\n    X_t, X_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.2, random_state=seed)\n    clf = LGBMRegressor(n_estimators=9999)\n    clf.fit(X_t, y_t, early_stopping_rounds=100,  eval_set=[(X_v, y_v)])\n\n    pred = clf.predict(df_all.drop(columns=['annual_inc']))\n\n    df_all['annual_inc_predicted'] = pred\n    df_all.loc[df_all['annual_inc_predicted'] <= 0, 'annual_inc_predicted'] = 100\n    df_all['annual_inc_diff_pred_actual'] = df_all['annual_inc'] - df_all['annual_inc_predicted']\n    df_all['annual_inc_ratio_pred_actual'] = df_all['annual_inc'] \/ df_all['annual_inc_predicted']\n    return df_all","fed64f1b":"def add_installment_ratio(df_all):\n    #\u6708\u53ce\u306b\u5bfe\u3059\u308b\u6708\u3005\u306e\u652f\u6255\u3044\u306e\u5272\u5408\u3002\u6708\u53ce(=\u5e74\u53ce\/12)\u304c0\u306e\u5834\u5408\u306f1.0\n    df_all.loc[df_all['annual_inc'] != 0, 'installment_ratio'] = df_all[df_all['annual_inc'] != 0].apply(lambda r: r['installment'] \/ (r['annual_inc'] \/ 12 ), axis=1)\n    df_all.loc[df_all['annual_inc'] == 0, 'installment_ratio'] = 1.0\n    return df_all","faaf3b17":"def add_income_loan_ratio(df_all):\n    df_all.loc[df_all['annual_inc'] != 0, 'income_loan_ratio'] = df_all[df_all['annual_inc'] != 0].apply(lambda r: r['loan_amnt'] \/ (r['annual_inc']), axis=1)\n    df_all.loc[df_all['annual_inc'] == 0, 'income_loan_ratio'] = 1.0\n    return df_all","9dcaad81":"def add_curbal_income_ratio(df_all):\n    # \u9810\u91d1\u304c\u5c11\u306a\u304f\u3066\u3082\u5e74\u53ce\u304c\u9ad8\u3044\u4eba\u306f\u8fd4\u6e08\u3067\u304d\u308b\u3001\u306a\u3069\u306e\u7d44\u5408\u305b\u7684\u6027\u8cea\u304c\u3042\u308b\u304b\u3082\n    df_all.loc[df_all['annual_inc'] != 0, 'curbal_income_ratio'] = df_all[df_all['annual_inc'] != 0].apply(lambda r: r['tot_cur_bal'] \/ (r['annual_inc']), axis=1)\n    df_all.loc[df_all['annual_inc'] == 0, 'curbal_income_ratio'] = 1.0\n    return df_all    ","61ca169b":"def add_install_dti_ratio(df_all):\n    # dti\u306e\u5b9a\u7fa9\u3088\u308a\u3053\u308c\u304c1\u306b\u306a\u308b\u306f\u305a\uff1f \u3068\u308a\u3042\u3048\u305a\u8a08\u7b97\n    df_all.loc[df_all['annual_inc']*df_all['dti'] != 0,'install_dti_ratio'] = df_all['installment'] \/ (df_all['dti'] * df_all['annual_inc'] \/ 12 )\n    df_all.loc[df_all['annual_inc']*df_all['dti'] == 0,'install_dti_ratio'] = 1.0\n    \n    return df_all","50e82423":"def load_spi_data():\n    df_spi = pd.read_csv(data_dir \/ 'spi.csv', parse_dates=['date'])\n    df_spi['year'] = df_spi['date'].dt.year\n    df_spi['month'] = df_spi['date'].dt.month\n    df_spi = df_spi.groupby(['year', 'month'])['close'].mean().reset_index()\n    for window_size in range(2,13):\n        col = f'close_roll_{window_size}'\n        df_spi[col] = df_spi['close'].rolling(window_size).mean()\n        df_spi.loc[df_spi[col].isnull(), col] = 208\n    df_spi['day'] = 1\n    df_spi['date'] = pd.to_datetime(df_spi[['year', 'month', 'day']])\n    df_spi = df_spi.drop(columns = ['year', 'month', 'day'])\n    df_spi = df_spi.rename(columns={'date': 'issue_d'})\n\n    return df_spi","8d74cb0d":"def load_state_data():\n    df_state = pd.read_csv(data_dir \/ 'statelatlong.csv')\n    df_state = df_state.rename(columns={'State': 'addr_state'})\n    df_gdp = pd.read_csv(data_dir \/ 'US_GDP_by_State.csv')\n    df_gdp = df_gdp.rename(columns={'State': 'City', 'State & Local Spending': 'state_spending', 'Gross State Product': 'state_product', 'Real State Growth %': 'state_growth', 'Population (million)': 'state_population'})\n    df_state = df_state.merge(df_gdp, on='City')\n    df_state = df_state.drop(columns = ['City'])\n    df_state = df_state.groupby(['addr_state']).mean().reset_index().drop(columns=['year'])\n    return df_state","9e35e785":"def feature_combinations(df_all, columns):\n    n = len(columns)\n    for i in range(n-1):\n        for j in range(i+1, n):\n            coli = columns[i]\n            colj = columns[j]\n            new_col = f'mul_{coli}_{colj}'\n            df_all[new_col] = df_all[coli] * df_all[colj]\n    return df_all","c849d23f":"def parse_emp_length(df_all):\n    emp_length_map = {'10+ years': 100, '7 years': 7, '5 years': 5, '2 years':2, '8 years':8, '3 years':3,\n       '< 1 year':0.5, '1 year':1, '6 years':6, '4 years':4, '9 years':9}\n    df_all['emp_length_num'] = df_all.emp_length.map(emp_length_map)\n    return df_all","10ad2b5a":"def segmentate(df_all):\n    ## annual_inct, loan_amnt\u3092\u30bb\u30b0\u30e1\u30f3\u30c8\u5316\u3059\u308b\n    segs = [0] + list(np.linspace(10000, 90000, 9)) + list(np.linspace(100000, 1000000, 10))\n    df_all['annual_inc_seg'] = f'over {int(segs[-1]) \/\/ 1000}k'\n    for i in range(1, len(segs)-1):\n        lower = segs[i-1]\n        upper = segs[i]\n        df_all.loc[(lower <= df_all['annual_inc']) & (df_all['annual_inc'] < upper), 'annual_inc_seg'] = f'{int(lower\/\/1000):03}k to {int(upper\/\/1000):03}k'\n    segs = np.linspace(0, df_all['loan_amnt'].max(), 21)\n\n    df_all['loan_amnt_seg'] = f'over {int(segs[-1]) \/\/ 1000}k'\n    for i in range(1, len(segs)-1):\n        lower = segs[i-1]\n        upper = segs[i]\n        df_all.loc[(lower <= df_all['loan_amnt']) & (df_all['loan_amnt'] < upper), 'loan_amnt_seg'] = f'{int(lower\/\/1000):03}k to {int(upper\/\/1000):03}k'\n    return df_all","e6b2a99f":"def _ordinal_encode_cat_pair(df_all, col1, col2):\n    map1 = {x: i for i, x in enumerate(sorted(df_all[col1].unique()))}\n    map2 = {x: i for i, x in enumerate(sorted(df_all[col2].unique()))}\n    return df_all[col1].map(map1) *1000 + df_all[col2].map(map2)\n\ndef ordinal_encode_cat_comb(df_all, col_combs=None):\n    if col_combs is None:\n        col_combs = []\n        cols = ['loan_amnt_seg', 'annual_inc_seg', 'grade', 'sub_grade']\n        for i in range(len(cols)):\n            for j in range(len(cols)):\n                if i == j:\n                    continue\n                col1, col2 = cols[i], cols[j]\n                if 'grade' in col1 and 'grade' in col2:\n                    continue\n                col_combs.append([col1, col2])\n    for col1, col2 in col_combs:\n        new_col = f'pair_enc_{col1}__{col2}'\n        df_all[new_col] = _ordinal_encode_cat_pair(df_all, col1, col2)\n    return df_all","d412a74c":"def remove_time_features(df_all):\n    cols_time_features = []\n    for col in df_all.columns:\n        if df_all[col].dtype == 'datetime64[ns]':\n            cols_time_features.append(col)\n    \n    if cols_time_features:\n        df_all = df_all.drop(columns=['issue_d', 'earliest_cr_line'])\n    return df_all","c24789ef":"def add_time_features(df_all):\n    meantime_till_issue = (df_all['issue_d'] -  df_all['earliest_cr_line']).mean()\n    \n    df_all['earliest_cr_line_missing'] = 0\n    df_all.loc[df_all['earliest_cr_line'].isnull(), 'earliest_cr_line_missing'] = 1\n    \n    # impute earliest_cr_line\n    df_all.loc[df_all['earliest_cr_line'].isnull(), 'earliest_cr_line'] = df_all['issue_d'] - meantime_till_issue\n\n    # add days since earliest_cr_line\n    df_all['days_since_earliest_cr_line'] = (df_all['issue_d'] - df_all['earliest_cr_line']).dt.days\n    \n    df_all['issue_month'] = df_all['issue_d'].dt.month\n    return df_all","2d906184":"def encode_categorical_features(df_all):\n    missing_marker = '__MISSING_VALUE__'\n    categorical_cols = []\n    for col in df_all.columns:\n        if df_all[col].dtype in ['object', 'datetime64[ns]']:\n            if col not in ['issue_d', 'grade', 'sub_grade']:\n                categorical_cols.append(col)\n\n    ## missing values\n    cols_with_none = ['emp_title', 'emp_length', 'title']\n    for col in cols_with_none:\n        col_missing = f'{col}_missing'\n        # add flag\n        df_all[col_missing] = 0\n        df_all.loc[df_all[col].isnull(), col_missing] = 1\n        # impute\n        df_all.loc[df_all[col].isnull(), col] = missing_marker\n\n        \n    for col in categorical_cols:\n        # replace cols with count encoded values\n        new_col = f'{col}_count'\n        df_all[new_col] = df_all[col].map(df_all[col].value_counts())\n        \n    # grade \/ sub_grade\u306f\u7279\u5225\u6271\u3044\u3057\u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u9806\u306b\u9023\u756a\u3092\u632f\u308b\n    for f in ['grade', 'sub_grade']:\n        gs = sorted(df_all[f].unique())\n        df_all[f] = df_all[f].map({g: i for (g, i) in zip(gs, range(len(gs)))})\n    return df_all","14c6b597":"from numpy.random import normal\n## via https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study\ndef mean_encode(train_data, test_data, columns, target_col, reg_method=None,\n                alpha=5, add_random='k_fold', rmean=0, rstd=0.1, folds=5):\n    '''Returns a DataFrame with encoded columns'''\n    encoded_cols = []\n    target_mean_global = train_data[target_col].mean()\n    for col in columns:\n        # Getting means for test data\n        nrows_cat = train_data.groupby(col)[target_col].count()\n        target_means_cats = train_data.groupby(col)[target_col].mean()\n        target_means_cats_adj = (target_means_cats*nrows_cat + \n                                 target_mean_global*alpha)\/(nrows_cat+alpha)\n        # Mapping means to test data\n        encoded_col_test = test_data[col].map(target_means_cats_adj)\n        # Getting a train encodings\n        if reg_method == 'expanding_mean':\n            train_data_shuffled = train_data.sample(frac=1, random_state=1)\n            cumsum = train_data_shuffled.groupby(col)[target_col].cumsum() - train_data_shuffled[target_col]\n            cumcnt = train_data_shuffled.groupby(col).cumcount()\n            encoded_col_train = cumsum\/(cumcnt)\n            encoded_col_train.fillna(target_mean_global, inplace=True)\n            if add_random:\n                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n                                                               size=(encoded_col_train.shape[0]))\n        elif (reg_method == 'k_fold') and (folds > 1):\n            kfold = StratifiedKFold(train_data[target_col].values, folds, shuffle=True, random_state=1)\n            parts = []\n            for tr_in, val_ind in kfold:\n                # divide data\n                df_for_estimation, df_estimated = train_data.iloc[tr_in], train_data.iloc[val_ind]\n                # getting means on data for estimation (all folds except estimated)\n                nrows_cat = df_for_estimation.groupby(col)[target_col].count()\n                target_means_cats = df_for_estimation.groupby(col)[target_col].mean()\n                target_means_cats_adj = (target_means_cats*nrows_cat + \n                                         target_mean_global*alpha)\/(nrows_cat+alpha)\n                # Mapping means to estimated fold\n                encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n                if add_random:\n                    encoded_col_train_part = encoded_col_train_part + normal(loc=rmean, scale=rstd, \n                                                                             size=(encoded_col_train_part.shape[0]))\n                # Saving estimated encodings for a fold\n                parts.append(encoded_col_train_part)\n            encoded_col_train = pd.concat(parts, axis=0)\n            encoded_col_train.fillna(target_mean_global, inplace=True)\n        else:\n            encoded_col_train = train_data[col].map(target_means_cats_adj)\n            if add_random:\n                encoded_col_train = encoded_col_train + normal(loc=rmean, scale=rstd, \n                                                               size=(encoded_col_train.shape[0]))\n\n        # Saving the column with means\n        encoded_col = pd.concat([encoded_col_train, encoded_col_test], axis=0)\n        encoded_col[encoded_col.isnull()] = target_mean_global\n        encoded_cols.append(pd.DataFrame({'mean_'+target_col+'_'+col:encoded_col}))\n    all_encoded = pd.concat(encoded_cols, axis=1)\n    return (all_encoded.loc[train_data.index,:], \n            all_encoded.loc[test_data.index,:])\n","dd017c9c":"def remove_categorical_features(df_all):\n    categorical_features = []\n    text_features = ['emp_title', 'title']\n    for col in df_all.columns:\n        if df_all[col].dtype == 'object' and col not in text_features:\n            categorical_features.append(col)\n    df_all = df_all.drop(columns=text_features)\n    df_all = df_all.drop(columns=categorical_features)\n    return df_all","9d7558ee":"def remove_old_records(df_all):\n    df_all = df_all[(df_all['issue_d'] > pd.to_datetime('2014-01-01 00:00:00'))]\n    # rows reduced (1321847, 33) => (1075503, 33)\n    return df_all","9e482d4d":"def check_score_time_split(df_al):\n    X_t, y_t, X_v, y_v = time_split(df_all)\n    clf = LGBMClassifier(n_estimators=9999)\n    clf.fit(X_t, y_t, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_v, y_v)])\n    pred = clf.predict_proba(X_v)\n    auc = roc_auc_score(y_v, pred[:,1])\n    return auc","4de8b007":"def get_score_on_text_feature(df_all, text_col, max_features=1000):\n    ## text_col\u306e\u30ab\u30e9\u30e0\u3060\u3051\u3092\u4f7f\u3063\u3066\u4e88\u6e2c\u30e2\u30c7\u30eb\u3092\u4f5c\u308a\u3001\u5f97\u3089\u308c\u305f\u30b9\u30b3\u30a2\u3092\u7279\u5fb4\u91cf\u3068\u3057\u3066\u8fd4\u3059\n    missing_marker = '__MISSING_VALUE__'\n    df_all.loc[df_all[text_col].isnull(), text_col] = missing_marker\n    vec = TfidfVectorizer(stop_words='english', max_features=max_features)\n    term_doc = vec.fit_transform(df_all[text_col])\n    X_train = term_doc[np.where(df_all['is_train'])]\n    y_train = df_all[df_all['is_train']]['loan_condition']\n    X_test = term_doc[np.where(~ df_all['is_train'])]\n    \n    stacking_scores, scores, pred = cv_score_lgb(X_train, y_train, n_splits=5, rounds=30, X_test=X_test)\n    \n    return np.append(stacking_scores, pred), scores","21ddfbb0":"def cv_score_lgb(X_train, y_train, params={}, n_splits=5, rounds=30, X_test=None, categorical_feature=None):\n    logger.info(f'cv_score_lgb rounds={rounds}')\n    # calc cv averaging when X_test is not None\n    \n    ## cv score (for stacking)\n    stacking_scores = pd.DataFrame({'score': np.zeros(X_train.shape[0])})\n    scores = []\n    predictions = []\n    skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n    for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n        # sparse\u306a\u5834\u5408\n        if type(X_train) == sp.sparse.csr.csr_matrix:\n            X_t = X_train[train_ix]\n            y_t = y_train.iloc[train_ix]\n            X_v = X_train[test_ix]\n            y_v = y_train.iloc[test_ix]\n        else:\n            X_t, y_t = X_train.iloc[train_ix], y_train.iloc[train_ix]\n            X_v, y_v = X_train.iloc[test_ix], y_train.iloc[test_ix]\n        clf = LGBMClassifier(n_estimators=9999, random_state=seed, **params)\n        logger.info(f'cv_score_lgb classifier={clf}')\n        if categorical_feature:\n            clf.fit(X_t, y_t, early_stopping_rounds=rounds, eval_metric='auc', eval_set=[(X_v, y_v)], categorical_feature=categorical_feature, verbose=50)\n        else:\n            clf.fit(X_t, y_t, early_stopping_rounds=rounds, eval_metric='auc', eval_set=[(X_v, y_v)], verbose=50)\n        #clf = DummyClassifier()\n        #clf.fit(X_t, y_t)\n        y_pred  = clf.predict_proba(X_v)[:,1]\n        # test_ix\u306f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3067\u306f\u306a\u304f\u884c\u756a\u53f7\u306e\u30ea\u30b9\u30c8\u306a\u306e\u3067iloc\u3067\u30a2\u30af\u30bb\u30b9\n        stacking_scores.iloc[test_ix, stacking_scores.columns.get_loc('score')]= y_pred\n        score = roc_auc_score(y_v, y_pred)\n        scores.append(score)\n        if X_test is not None:\n            y_pred_test  = clf.predict_proba(X_test)[:,1]\n            predictions.append(y_pred_test)\n    mean = sum(scores) \/ n_splits\n    logger.info(f'cv_score_lgb  scores={scores}, mean={mean}')\n    \n    stacking_scores = stacking_scores['score'].values\n    if X_test is not None:\n        pred = sum(predictions) \/ n_splits\n        return stacking_scores, scores, pred\n    else:\n        return stacking_scores, scores","90a78b93":"def create_nn_model(input_len):\n    inp = Input(shape=(input_len,))#, sparse=True) # \u758e\u884c\u5217\u3092\u5165\u308c\u308b\n    x = Dense(194, activation='relu')(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outp = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    return model\n\n\n","ea1be049":"def cv_score_nn(X_train, y_train, params={}, n_splits=5, epochs=99, X_test=None):\n    logger.info(f'cv_score_nn starts')\n    # calc cv averaging when X_test is not None\n    \n    ## cv score (for stacking)\n    stacking_scores = pd.DataFrame({'score': np.zeros(X_train.shape[0])})\n    scores = []\n    predictions = []\n    skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n    for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n        X_t, y_t = X_train.iloc[train_ix], y_train.iloc[train_ix]\n        X_v, y_v = X_train.iloc[test_ix], y_train.iloc[test_ix]\n        model = create_nn_model(X_t.shape[1])\n        logger.info(f'cv_score_nn classifier={model}')\n        es = EarlyStopping(monitor='val_loss', patience=0)\n        model.fit(X_t, y_t, batch_size=512, epochs=epochs, validation_data=(X_v, y_v), callbacks=[es])\n        y_pred = model.predict(X_v) # predict_proba[:,1]\u3067\u306a\u3044\u70b9\u306b\u6ce8\u610f\n        y_pred = (y_pred.T)[0]\n        stacking_scores.iloc[test_ix, stacking_scores.columns.get_loc('score')]= y_pred\n        score = roc_auc_score(y_v, y_pred)\n        scores.append(score)\n        if X_test is not None:\n            y_pred_test = model.predict(X_test) \n            y_pred_test = (y_pred_test.T)[0]\n            predictions.append(y_pred_test)\n    mean = sum(scores) \/ n_splits\n    logger.info(f'cv_score_lgb  scores={scores}, mean={mean}')\n    \n    stacking_scores = stacking_scores['score'].values\n    if X_test is not None:\n        pred = sum(predictions) \/ n_splits\n        return stacking_scores, scores, pred\n    else:\n        return stacking_scores, scores","ef765477":"def cv_score_logreg(X_train, y_train, params={}, n_splits=5, X_test=None):\n    logger.info(f'cv_score_logreg')\n    # calc cv averaging when X_test is not None\n    \n    ## cv score (for stacking)\n    stacking_scores = pd.DataFrame({'score': np.zeros(X_train.shape[0])})\n    scores = []\n    predictions = []\n    skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n    for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n        X_t, y_t = X_train.iloc[train_ix], y_train.iloc[train_ix]\n        X_v, y_v = X_train.iloc[test_ix], y_train.iloc[test_ix]\n        clf = LogisticRegression(**params)\n        logger.info(f'cv_score_logreg classifier={clf}')\n        clf.fit(X_t, y_t)\n        y_pred  = clf.predict_proba(X_v)[:,1]\n\n        stacking_scores.iloc[test_ix, stacking_scores.columns.get_loc('score')]= y_pred\n        score = roc_auc_score(y_v, y_pred)\n        scores.append(score)\n        if X_test is not None:\n            y_pred_test  = clf.predict_proba(X_test)[:,1]\n            predictions.append(y_pred_test)\n    mean = sum(scores) \/ n_splits\n    logger.info(f'cv_score_logreg  scores={scores}, mean={mean}')\n    \n    stacking_scores = stacking_scores['score'].values\n    if X_test is not None:\n        pred = sum(predictions) \/ n_splits\n        return stacking_scores, scores, pred\n    else:\n        return stacking_scores, scores","52e2b2f9":"def importance(gb, X_test):\n    feature_importance = gb.feature_importances_\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    plt.figure(figsize=(30, 16))\n    plt.subplot(1, 2, 2)\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, list(X_test.columns[sorted_idx]))\n    plt.xlabel('Relative Importance')\n    plt.title('Variable Importance')\n    plt.show()\n    return X_test.columns[sorted_idx]\n","c1240123":"def check_score_time_split_lgb(df_all, th='2014-05-01 00:00:00'):\n    X_train, y_train, X_val, y_val = time_split(df_all, th)\n    clf = LGBMClassifier(n_estimators=9999, random_state=seed)\n    clf.fit(X_train, y_train, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_val, y_val)])\n    y_pred  = clf.predict_proba(X_val)[:,1]\n    score = roc_auc_score(y_val, y_pred)\n    logger.info(f'check_score_time_split_lgb  score={score}')\n    return score","19a26257":"def add_timesplit_flag(df_all, th='2014-05-01 00:00:00'):\n    df_all['is_recent'] = True\n    df_all.loc[(df_all.issue_d <= pd.to_datetime(th)), 'is_recent'] = False\n    return df_all","6840a161":"def my_gridsearch_lgb(X_train, y_train, X_val, y_val, grid_params):\n    ## early_stopping\u3042\u308a\u3067GridSearchCV\u3092\u4f7f\u3046\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u518d\u5b9f\u88c5\n    func_name = 'my_gridsearch_lgb'\n    gridsearch_results = []\n    values = grid_params.values()\n    for values in itertools.product(*values):\n        param = {k:v for k,v in zip(grid_params.keys(), values)}\n\n        logger.info(f'{func_name} start validation for param={param}')\n        clf = LGBMClassifier(n_estimators=9999, random_state=seed, **param)\n        clf.fit(X_train, y_train, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_v, y_v)])\n        #clf = DummyClassifier()\n        #clf.fit(X_t, y_t)\n        y_pred  = clf.predict_proba(X_val)[:,1]\n        score = roc_auc_score(y_val, y_pred)\n        logger.info(f'{func_name}  validation done for param={param}, score={score}')\n        gridsearch_results.append({'param': param, 'score': score})\n    \n    best_score = max(gridsearch_results, key=lambda x: x['score'])['score']\n    best_param = max(gridsearch_results, key=lambda x: x['score'])['param']\n    return gridsearch_results, best_score, best_param    \n\ndef my_gridsearch_cv_lgb(X_train, y_train, grid_params, n_splits=5):\n    ## early_stopping\u3042\u308a\u3067GridSearchCV\u3092\u4f7f\u3046\u65b9\u6cd5\u304c\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u518d\u5b9f\u88c5\n    func_name = 'my_cv_lgb'\n    skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n    cv_results = []\n    values = grid_params.values()\n    for values in itertools.product(*values):\n        param = {k:v for k,v in zip(grid_params.keys(), values)}\n        scores = []\n\n        logger.info(f'{func_name} start cv for param={param}')\n        for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n            # sparse\u306a\u5834\u5408\n            if type(X_train) == sp.sparse.csr.csr_matrix:\n                X_t = X_train[train_ix]\n                y_t = y_train.iloc[train_ix]\n                X_v = X_train[test_ix]\n                y_v = y_train.iloc[test_ix]\n            else:\n                X_t, y_t = X_train.iloc[train_ix], y_train.iloc[train_ix]\n                X_v, y_v = X_train.iloc[test_ix], y_train.iloc[test_ix]\n\n            clf = LGBMClassifier(n_estimators=9999,  random_state=seed, **param)\n            clf.fit(X_t, y_t, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_v, y_v)])\n            #clf = DummyClassifier()\n            #clf.fit(X_t, y_t)\n            y_pred  = clf.predict_proba(X_v)[:,1]\n            score = roc_auc_score(y_v, y_pred)\n            scores.append(score)\n        mean = sum(scores) \/ n_splits\n        logger.info(f'{func_name}  cv done for param={param}, scores={scores}, mean={mean}')\n        cv_results.append({'param': param, 'scores': scores, 'mean_score': mean})\n    \n    best_score = max(cv_results, key=lambda x: x['mean_score'])['mean_score']\n    best_param = max(cv_results, key=lambda x: x['mean_score'])['param']\n    return cv_results, best_score, best_param","ac73af0a":"def tune_lgb_cv(X_train, y_train, params, n_splits=5, n_trials=5, rounds=30, categorical_feature=None, metric='mean'):\n    func_name = 'tune_lgb_cv'\n    logger.info(f'{func_name} start hyperparameter search')\n\n    def objective(trial):\n        param = {}\n        for p in params:\n            kv = params[p]\n            t = kv['type']\n            if t == 'fixed':\n                param[p] = kv['value']\n            elif t == 'int':\n                param[p] = trial.suggest_int(p, kv['lower'], kv['upper'])\n            elif t == 'uniform':\n                param[p] = trial.suggest_uniform(p, kv['lower'], kv['upper'])\n            elif t == 'loguniform':\n                param[p] = trial.suggest_loguniform(p, kv['lower'], kv['upper'])\n\n        aucs = []\n        skf = StratifiedKFold(n_splits=n_splits, random_state=seed, shuffle=True)\n\n        for i, (train_ix, test_ix) in enumerate(tqdm(skf.split(X_train, y_train))):\n            # sparse\u306a\u5834\u5408\n            if type(X_train) == sp.sparse.csr.csr_matrix:\n                X_t = X_train[train_ix]\n                y_t = y_train.iloc[train_ix]\n                X_v = X_train[test_ix]\n                y_v = y_train.iloc[test_ix]\n            else:\n                X_t, y_t = X_train.iloc[train_ix], y_train.iloc[train_ix]\n                X_v, y_v = X_train.iloc[test_ix], y_train.iloc[test_ix]\n\n            clf = LGBMClassifier(n_estimators=9999, random_state=seed,  **param)\n            if categorical_feature:\n                clf.fit(X_t, y_t, early_stopping_rounds=rounds, eval_metric='auc', eval_set=[(X_v, y_v)], categorical_feature=categorical_feature)\n            else:\n                clf.fit(X_t, y_t, early_stopping_rounds=rounds, eval_metric='auc', eval_set=[(X_v, y_v)])\n\n            y_pred  = clf.predict_proba(X_v)[:,1]\n            auc = roc_auc_score(y_v, y_pred)\n            aucs.append(auc)\n            logger.info(f'{func_name} a fold with param={param}, auc={auc}')\n\n        if metric == 'mean':\n            score = sum(aucs) \/ n_splits\n        elif metric == 'min':\n            score = min(aucs)\n        logger.info(f'{func_name}  cv done with param={param}, aucs={aucs}, score={score}, metric={metric}')\n  \n        return -score\n    study = optuna.create_study()\n    study.optimize(objective, n_trials=n_trials)\n    return study","042f2ad0":"def tune_lgb(X_train, y_train, X_val, y_val, params, n_trials=5):\n    func_name = 'tune_lgb'\n    logger.info(f'{func_name} start hyperparameter search')\n\n    def objective(trial):\n        param = {}\n        for p in params:\n            kv = params[p]\n            t = kv['type']\n            if t == 'fixed':\n                param[p] = kv['value']\n            elif t == 'int':\n                param[p] = trial.suggest_int(p, kv['lower'], kv['upper'])\n            elif t == 'uniform':\n                param[p] = trial.suggest_uniform(p, kv['lower'], kv['upper'])\n            elif t == 'loguniform':\n                param[p] = trial.suggest_loguniform(p, kv['lower'], kv['upper'])\n        clf = LGBMClassifier(n_estimators=9999,  random_state=seed, **param)\n        clf.fit(X_train, y_train, early_stopping_rounds=300, eval_metric='auc', eval_set=[(X_val, y_val)])\n        y_pred  = clf.predict_proba(X_val)[:,1]\n        score = roc_auc_score(y_val, y_pred)\n        logger.info(f'{func_name} validation done for param={param}, score={score}')\n        return -score\n    study = optuna.create_study()\n    study.optimize(objective, n_trials=n_trials)\n    logger.info(f'{func_name} finished. best_params={study.best_params}, best_values={study.best_value}')\n    return study","280c79c7":"df_all = load_data()","09d3f354":"## \u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306eTFIDF\u306e\u307f\u3067\u4e88\u6e2c\u3057\u305fmeta feature\u3092\u8ffd\u52a0\nfor col in ['emp_title', 'title']:\n    %time scores, auc = get_score_on_text_feature(df_all, col)\n    new_col = f'lgbscore_{col}'\n    df_all[new_col] = scores","c593661e":"df_all = add_timesplit_flag(df_all)\ndf_all = encode_missing_pattern(df_all)\ndf_all = count_missing(df_all)\n#df_all = remove_old_records(df_all)\ndf_all = parse_emp_length(df_all)\ndf_all = missing_value_impute(df_all)\n\n#df_all = segmentate(df_all)\n#df_all = ordinal_encode_cat_comb(df_all)\n\ndf_all = add_annual_inc_is_clean(df_all)\n\ndf_all = add_ratios(df_all)\ndf_all = add_states(df_all)\n#df_all = add_spi(df_all)\ndf_all = add_time_features(df_all)\n\n","ee9532fe":"df_all_back_for_lgbm_catencode = df_all.copy()","b51210b3":"df_all = encode_categorical_features(df_all)\ndf_all = remove_categorical_features(df_all)\ndf_all = remove_time_features(df_all)\n#df_all = predict_annual_inc(df_all)","c724ac23":"df_all_back_for_numerical_model = df_all.copy()\n","9ef294fd":"df_all = df_all_back_for_numerical_model.copy()","1075f79f":"X_train = df_all[df_all['is_train']].drop(columns=['loan_condition', 'is_train', 'is_recent'])\ny_train = df_all[df_all['is_train']]['loan_condition']\nX_test = df_all[~ (df_all['is_train'])].drop(columns=['is_train', 'loan_condition', 'is_recent'])\n","80d359fe":"for col in X_train:\n    scaler = StandardScaler()\n    scaler.fit(X_train[[col]])\n    X_train[[col]] = scaler.transform(X_train[[col]])\n    X_test[[col]] = scaler.transform(X_test[[col]])","75f0f5cc":"%time stacking_scores, scores, pred = cv_score_nn(X_train, y_train, n_splits=5, epochs=99, X_test=X_test)","b91daabd":"metafeature_nn = np.append(stacking_scores, pred)","1f457d39":"df_all = df_all_back_for_lgbm_catencode.copy()","0301f841":"df_all = segmentate(df_all)\ndf_all = ordinal_encode_cat_comb(df_all)","447bc87c":"categorical_features = [col for col in df_all.columns if df_all[col].dtypes in ['object', 'datetime64[ns]']]","4259b055":"df_all.loc[df_all['emp_length'].isnull(), 'emp_length'] = '__MISSING_VALUE__'","27f9909f":"for col in categorical_features:\n    le = LabelEncoder()\n    le.fit(df_all[col])\n    df_all[col] = le.transform(df_all[col])","8cc3fee9":"# \u7d44\u5408\u305b\u3067\u751f\u6210\u3057\u305f\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3082\u8ffd\u52a0\ncategorical_features += [col for col in df_all.columns if 'pair_enc' in col]","c6981eb8":"X_train = df_all[df_all['is_train']].drop(columns=['loan_condition', 'is_train', 'is_recent'])\ny_train = df_all[df_all['is_train']]['loan_condition']\nX_test = df_all[~ (df_all['is_train'])].drop(columns=['is_train', 'loan_condition', 'is_recent'])","15306de2":"current_best_params_for_lgb2 = {'min_child_samples': 60, 'num_leaves': 15, 'min_child_weight': 3, 'subsample': 0.5451137318009219, 'colsample_bytree': 0.05220322801009669}\n\n%time stacking_scores, scores, pred = cv_score_lgb(X_train, y_train,  params=current_best_params_for_lgb2, n_splits=5, rounds=rounds, X_test=X_test)\n","9a2e2c00":"metafeature_lgbm = np.append(stacking_scores, pred)","553b9089":"df_all = df_all_back_for_numerical_model.copy()","c3e0f660":"X_train = df_all[df_all['is_train']].drop(columns=['loan_condition', 'is_train', 'is_recent'])\ny_train = df_all[df_all['is_train']]['loan_condition']\nX_test = df_all[~ (df_all['is_train'])].drop(columns=['is_train', 'loan_condition', 'is_recent'])\n\n","a916ad8d":"#current_best_params_for_lgb = {'min_child_samples': 74, 'num_leaves': 12, 'min_child_weight': 11, 'subsample': 0.6130339453753051, 'colsample_bytree': 0.23506396376975347}\ncurrent_best_params_for_lgb = {'min_child_samples': 120, 'num_leaves': 21, 'min_child_weight': 31, 'subsample': 0.8168082691950477, 'colsample_bytree': 0.46720503612089026}\n\n#current_best_params_for_lgb = {'min_child_samples': 120, 'num_leaves': 78, 'min_child_weight': 2, 'subsample': 0.6175732431082002, 'colsample_bytree': 0.4890612005186778, 'learning_rate': 0.004327392867032571}\n%time stacking_scores, scores, pred = cv_score_lgb(X_train, y_train,  params=current_best_params_for_lgb, n_splits=5, rounds=rounds, X_test=X_test)\n","bb62e468":"metafeature_lgbm2 = np.append(stacking_scores, pred)","cf26d0e1":"df_all = load_data()\ndf_all['lgbm'] = metafeature_lgbm\ndf_all['lgbm2'] = metafeature_lgbm2\ndf_all['nn'] = metafeature_nn\n#df_all['logreg'] = metafeature_logreg\n\ndf_all = df_all[['lgbm', 'lgbm2', 'nn',  'loan_condition', 'is_train']]\n","a729eb17":"X_train = df_all[df_all['is_train']].drop(columns=['loan_condition', 'is_train'])\ny_train = df_all[df_all['is_train']]['loan_condition']\nX_test = df_all[~ (df_all['is_train'])].drop(columns=['is_train', 'loan_condition'])\n\n","a768e9e6":"logreg_param = {'C': 0.001, 'penalty': 'l1'}\n%time stacking_scores, scores, pred = cv_score_logreg(X_train, y_train,  params=logreg_param, n_splits=5, X_test=X_test)\n","cddcb95e":"import datetime\nts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","dd4fe0d9":"submission = pd.read_csv(data_dir \/ 'sample_submission.csv', index_col=0).drop(columns=['loan_condition'])\nsubmission['loan_condition'] = pred\nsubmission.to_csv(f'.\/submission_{description_of_this_commit}_{ts}.csv')","92147f2d":"# head and tail latest submission\n!ls -tr | grep submission | tail -n 1 | xargs head\n!echo\n!ls -tr | grep submission | tail -n 1 | xargs tail","7a8472b2":"### lightgbm\u305d\u306e2","ebde4991":"## Submit","018ed5b4":"## count encoding","1811c5db":"### \u30e2\u30c7\u30ea\u30f3\u30b0","d126d0a2":"## target encoding","46a15682":"### earliest_cr_line\u304b\u3089issue_d\u306e\u65e5\u6570\u3092\u8ffd\u52a0","070e38ed":"### lightgbm\n\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u6271\u3044\u3092lightgbm\u306b\u4efb\u305b\u308b\n","9410c6e7":"\n * stage 0 model\n     * lightgbm, \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf\u306eTF-IDF\u884c\u5217\u306e\u307f\u3092\u4f7f\u3063\u3066\u4e88\u6e2c\n         * 5CV AUC=0.5421\n * stage 1 model\n     * NN\n         * 5CV AUC=0.7053\n     * lightgbm, \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092cagegory_feature\u3068\u3057\u3066\u6307\u5b9a\u3057\u5b66\u7fd2\u5668\u306b\u4efb\u305b\u308b\n         * 5CV AUC=0.7120\n     * lightgbm, \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306fcount encoding\n         * 5CV AUC=0.7118\n * stage 2 model\n     * LogisticRegression\n         * 5CV AUC=0.7135\n \n\u306estacking\u3092\u3059\u308b","9195b738":"## \u57fa\u672c\u7684\u306a\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3059\u308b","bdfb4df4":"0.5%\u3060\u3051\u5dee\u7570\u304c\u3042\u308b\ndf_all[df_all['is_train']].groupby('annual_inc_is_clean')['loan_condition'].mean()","20fa1bb8":"\u51e6\u7406\u6982\u8981\n\n\n * stage 0 model\n     * lightgbm, \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf\u306eTF-IDF\u884c\u5217\u306e\u307f\u3092\u4f7f\u3063\u3066\u4e88\u6e2c\n         * 5CV AUC=0.5421\n * stage 1 model\n     * NN\n         * 5CV AUC=0.7053\n     * lightgbm, \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092cagegory_feature\u3068\u3057\u3066\u6307\u5b9a\u3057\u5b66\u7fd2\u5668\u306b\u4efb\u305b\u308b\n         * 5CV AUC=0.7120\n     * lightgbm, \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306fcount encoding\n         * 5CV AUC=0.7118\n * stacking\n     * rank averaging\n         * 5CV AUC=0.7124\n \n\u306estacking\u3092\u3059\u308b","2931e969":"### \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf","453210cc":"## \u6700\u7d42\u30e2\u30c7\u30eb","9e4a5245":"# \u4e8b\u524d\u6e96\u5099","c80cc0f0":"# Main","1fca6d53":"# \u6b20\u640d\u5024\u5bfe\u51e6","3c708737":"## \u305d\u306e\u4ed6\u306e\u7279\u5fb4\u91cf\u8ffd\u52a0","5d507ecd":"### NN","0f706234":"# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0"}}