{"cell_type":{"6349377e":"code","1efa2de7":"code","5723a073":"code","b2e6ad06":"code","fb3a3482":"code","14458a38":"code","9df49cc3":"code","9e9ffb5e":"code","31d5be78":"code","7c3ad1a2":"code","1159a8d4":"code","544f1f60":"code","bbf2cda2":"code","35085a74":"code","f2674d0c":"code","7dc1be7e":"code","62833f78":"code","cf3ad02f":"code","9ced8bd9":"code","43343168":"code","2ff8be5f":"code","81dc8fbc":"code","f01d8565":"code","46a9371a":"markdown","e2eee58e":"markdown","3d5ec758":"markdown","9a7540c4":"markdown","092d51ce":"markdown"},"source":{"6349377e":"!ls ..\/input\/planet-understanding-the-amazon-from-space -l","1efa2de7":"!ls ..\/input\/ -lh","5723a073":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport glob\n\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm_notebook","b2e6ad06":"!ls ..\/input\/ -lh","fb3a3482":"df_train = pd.read_csv('..\/input\/train_v2.csv')\ndf_train['image_name'] = '..\/input\/train-jpg\/' + df_train['image_name'] + '.jpg'\ndf_train.head()","14458a38":"df_test = pd.read_csv('..\/input\/sample_submission_v2.csv')\ndf_test['image_name'] = '..\/input\/test-jpg\/' + df_test['image_name'] + '.jpg'\ndf_test['tags'] = df_train['tags'].apply(lambda x: x.split(' '))\n","9df49cc3":"df_train['tags2'] = df_train['tags'].apply(lambda x: x.split(' '))\nlabels_list = sum(list(df_train['tags2'].values), [])\nlabels = set(labels_list)\n\nlabel_map = {l: i for i, l in enumerate(labels)}\ninv_label_map = {i: l for l, i in label_map.items()}","9e9ffb5e":"label_map","31d5be78":"%pylab inline\nimport seaborn as sns\n\nlabels_s = pd.Series(labels_list).value_counts() # To sort them by count\nfig, ax = plt.subplots(figsize=(16, 8))\nsns.barplot(x=labels_s, y=labels_s.index, orient='h')","7c3ad1a2":"images_title = [df_train[df_train['tags'].str.contains(label)].iloc[i]['image_name']\n                for i, label in enumerate(labels)]\n\nplt.rc('axes', grid=False)\n_, axs = plt.subplots(5, 4, sharex='col', sharey='row', figsize=(15, 20))\naxs = axs.ravel()\n\nfor i, (image_name, label) in enumerate(zip(images_title, labels)):\n    img = mpimg.imread(image_name)\n    axs[i].imshow(img)\n    axs[i].set_title('{}'.format(label))","1159a8d4":"# df_train = df_train.iloc[:3000]","544f1f60":"from sklearn.preprocessing import MultiLabelBinarizer\nlabel_encoder = MultiLabelBinarizer()\nY = label_encoder.fit_transform(df_train['tags2']).astype(float)","bbf2cda2":"# https:\/\/github.com\/trent-b\/iterative-stratification\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nmskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\nfor train_index, test_index in mskf.split(df_train['image_name'].values, Y):\n    # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = df_train['image_name'].iloc[train_index].values, df_train['image_name'].iloc[test_index].values\n    y_train, y_test = Y[train_index], Y[test_index]\n    \n    print(y_train.sum(0))\n    print(y_test.sum(0))\n    print('')","35085a74":"import torchvision.datasets as datasets\nfrom torch.utils.data.dataset import Dataset\nfrom PIL import Image\nimport torch\nfrom torch import nn\n\nfrom efficientnet_pytorch import EfficientNet\n\nclass PlantDataset(Dataset):\n    def __init__(self, path, label, transform=None):\n        self.path = path\n        self.label = label\n        if transform is not None:\n            self.transform = transform\n        else:\n            self.transform = None\n    \n    def __getitem__(self, index):\n        img = Image.open(self.path[index]).convert('RGB')\n\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, torch.from_numpy(np.array(self.label[index]))\n    \n    def __len__(self):\n        return len(self.path)\n    \nclass PlantNet(nn.Module):\n    def __init__(self):\n        super(PlantNet, self).__init__()\n                \n#         model = models.resnet50(True)\n#         model.avgpool = nn.AdaptiveAvgPool2d(1)\n#         model.fc = nn.Linear(2048, 100)\n#         self.resnet = model\n\n        model = EfficientNet.from_pretrained('efficientnet-b5') \n        model._fc = nn.Linear(2048, 17)\n        self.resnet = model\n        \n    def forward(self, img):        \n        out = self.resnet(img)\n        return out","f2674d0c":"# \u8bad\u7ec3\ndef train(model, device, train_loader, optimizer, criterion, epoch):\n    model.train() # \u8f6c\u4e3a\u8bad\u7ec3\uff0cdrop\u8d77\u4f5c\u7528\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad() # \u6e05\u7a7a\u68af\u5ea6\n        output = model(data) # \u6b63\u5411\u4f20\u64ad\n        loss = criterion(output, target) # \u8ba1\u7b97\u635f\u5931\n        loss.backward() # \u68af\u5ea6\u8ba1\u7b97\n        optimizer.step() # \u53c2\u6570\u66f4\u65b0\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\n# \u6d4b\u8bd5\ndef test(model, device, test_loader, criterion):\n    model.eval() \n    model_predict, model_target = [], []\n    test_loss = 0\n    with torch.no_grad():\n        for data, target in tqdm_notebook(test_loader):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            \n            model_predict.append(output.sigmoid().data.cpu().numpy())\n            model_target.append(target.data.cpu().numpy())\n            \n            test_loss += criterion(output, target).item()  # sum up batch loss\n\n    test_loss \/= len(test_loader.dataset)\n    \n    f2_socre = []\n    model_predict = np.vstack(model_predict)\n    model_target = np.vstack(model_target)\n    \n    for idx in range(len(model_predict)):\n        f2_socre.append(fbeta_score(model_predict[idx,:]>0.5, model_target[idx,:], beta=2))\n    f2_socre = np.mean(f2_socre)\n    \n    print('\\nTest set: Average loss: {:.4f} F2 Score: {:.4f}\\n'.format(test_loss, f2_socre))\n    \n    return test_loss, f2_socre, model_predict","7dc1be7e":"import torchvision.transforms as transforms\n\ntrain_dataset = PlantDataset(X_train, y_train,\n                            transforms.Compose([\n                            transforms.Resize((256, 256)),\n                            transforms.ColorJitter(hue=.05, saturation=.05),\n                            transforms.RandomHorizontalFlip(),\n                            transforms.RandomVerticalFlip(),\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]))\nvalid_dataset = PlantDataset(X_test, y_test,\n                            transforms.Compose([\n                            transforms.Resize((256, 256)),\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]))\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=20, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(valid_dataset,  batch_size=20)","62833f78":"device = torch.device(\"cuda:0\")\nmodel = PlantNet().to(device)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.Adam(model.parameters(), 0.001)","cf3ad02f":"from sklearn.metrics import fbeta_score\n\nbest_loss = 100\nfor epoch in range(1, 11):\n    train(model, device, train_loader, optimizer, criterion, epoch)\n    test_loss, f2_socre, _ = test(model, device, test_loader, criterion)\n    if test_loss < best_loss:\n        best_loss = test_loss\n        torch.save(model.state_dict(), 'plant_net.pt')","9ced8bd9":"test_dataset = PlantDataset(df_test['image_name'].values, np.zeros((df_test.shape[0], 17)),\n                            transforms.Compose([\n                            transforms.Resize((256, 256)),\n                            transforms.ToTensor(),\n                            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]))\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=40, shuffle=False)","43343168":"model.load_state_dict(torch.load('plant_net.pt'))\ntest_loss, f2_socre, test_predict = test(model, device, test_loader, criterion)","2ff8be5f":"pred_label = []\nfor pred in test_predict:\n    pred_idx = np.where(pred > 0.5)[0]\n    pred_label.append(' '.join(label_encoder.classes_[pred_idx]))","81dc8fbc":"df_test = pd.read_csv('..\/input\/sample_submission_v2.csv')\ndf_test['tags'] = pred_label","f01d8565":"df_test.to_csv('tmp.csv', index=None)","46a9371a":"# \u5b9a\u4e49dataset","e2eee58e":"# \u6570\u636e\u5212\u5206","3d5ec758":"# \u9605\u8bfb\u94fe\u63a5\n\n- [1st interview](https:\/\/medium.com\/kaggle-blog\/planet-understanding-the-amazon-from-space-1st-place-winners-interview-bf66fb444bc2)\n\nhttps:\/\/github.com\/Cadene\/pretrained-models.pytorch\n\nhttps:\/\/github.com\/lukemelas\/EfficientNet-PyTorch","9a7540c4":"# \u63d0\u4ea4\u7ed3\u679c","092d51ce":"# \u6570\u636e\u8bfb\u53d6"}}