{"cell_type":{"e693387d":"code","33b730a8":"code","80744968":"code","a770f0cf":"code","0e140f12":"code","c0d5e5dc":"code","2f8b1c39":"code","d271d89b":"code","da53dc73":"code","c035c160":"code","e882f84c":"code","1b2bafc3":"code","6f712fcf":"code","77f6640c":"code","3732d0c2":"code","025c316b":"code","a51aeb20":"code","b36fb770":"code","5078a953":"code","c7a323b1":"code","fff241af":"code","556154f8":"markdown","1bbda9b0":"markdown","9b1e6237":"markdown","2483382e":"markdown","9cef83fb":"markdown","2a8be12c":"markdown","693e9710":"markdown","2fc92142":"markdown","44b53468":"markdown","1ba5119e":"markdown","c24cd035":"markdown","797e814a":"markdown","73590394":"markdown"},"source":{"e693387d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","33b730a8":"clinical_data = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nclinical_data.head()","80744968":"clinical_data.describe()","a770f0cf":"clinical_data.isna().sum()","0e140f12":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","c0d5e5dc":"clinical_data.dtypes","2f8b1c39":"clinical_data.anaemia = clinical_data.anaemia.astype('category')     \nclinical_data.high_blood_pressure = clinical_data.high_blood_pressure.astype('category')     \nclinical_data.sex = clinical_data.sex.astype('category')     \nclinical_data.smoking = clinical_data.smoking.astype('category')   \nclinical_data.diabetes = clinical_data.diabetes.astype('category')     \nclinical_data.DEATH_EVENT = clinical_data.DEATH_EVENT.astype('category')     \n\nclinical_data.dtypes","d271d89b":"age_groups = pd.cut(clinical_data['age'],5,labels=['39-50','51-61','62-72','73-83','84-95'])\ncd = clinical_data.copy()\ncd = cd[cd['DEATH_EVENT'] == 1]\ncd['age_groups'] = age_groups\ncd = cd.groupby(by='age_groups').count()\nplt.figure(figsize=(11,6))\nax = sns.barplot(x=cd.index,y='DEATH_EVENT',data=cd)\nax.set(title=\"Number Of Deaths In A Particular Age Group\")\nplt.show()","da53dc73":"sns.set_style('whitegrid')\nplt.figure(figsize=(11,7))\nde = clinical_data[clinical_data['DEATH_EVENT']==1]\ncor = clinical_data.corr('pearson')\nsns.heatmap(cor,cmap='Blues',annot=True)","c035c160":"#first lets copy the original data so we dont need to reload it in case of a mistake\nw_data = clinical_data.copy()\nage_groups = pd.cut(clinical_data['age'],5,labels=['39-50','51-61','62-72','73-83','84-95'])\nw_data['age groups'] = age_groups\nw_data['platelets\/age'] = w_data['platelets']\/w_data['age']\nw_data['sodium\/creatinine'] = w_data['serum_sodium']\/w_data['serum_creatinine']\nw_data","e882f84c":"#target\ny = w_data.pop('DEATH_EVENT')\nw_data\nr_features = ['anaemia','diabetes','ejection_fraction','high_blood_pressure','sex','platelets\/age',\n     'sodium\/creatinine','smoking','time']\nX=w_data[r_features]","1b2bafc3":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import f1_score as f1","6f712fcf":"def optimal_n_leaf(train_x,train_y,test_x,test_y,leaf_list,rstate=0):\n    result = []\n    for n in leaf_list:\n        model = RandomForestRegressor(random_state=rstate,n_estimators=n,max_leaf_nodes=n)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        result.append(f1(np.round(pred),test_y))\n    return result\n        \ntrain_x,test_x,train_y,test_y = train_test_split(X,y)\nleaf_candidates = [2,3,5,7,12,19,31,50]\n","77f6640c":"leaf_check = optimal_n_leaf(train_x,train_y,test_x,test_y,leaf_candidates)\nplt.figure(figsize=(12,8))\nax = sns.lineplot(x=np.arange(8),y=leaf_check)\nax.set_xticklabels(labels=[-1,2,3,5,7,12,19,31,50])\nax.set_title('Best F1 Score Via K Variable')\nax.set_xlabel('K Value',fontsize=15)\nax.set_ylabel('F1 Score',fontsize=15)\nplt.show()\n","3732d0c2":"from sklearn.neighbors import KNeighborsClassifier","025c316b":"def optimal_k_value(train_x,train_y,test_x,test_y,k_list,rstate=0):\n    result = []\n    for k in k_list:\n        model = KNeighborsClassifier(n_neighbors=k)\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        result.append(f1(np.round(pred),test_y))\n    return result","a51aeb20":"k_result = optimal_k_value(train_x,train_y,test_x,test_y,[1,2,3,4,5,6,7,8,9,10,20])\nplt.figure(figsize=(12,8))\nax = sns.lineplot(x=np.arange(11),y=k_result)\nax.set_xticklabels(labels=[-1,1,2,3,4,5,6,7,8,9,10,20])\nax.set_title('Best F1 Score Via K Variable')\nax.set_xlabel('K Value',fontsize=15)\nax.set_ylabel('F1 Score',fontsize=15)\nplt.show()","b36fb770":"from sklearn.ensemble import AdaBoostClassifier","5078a953":"def optimal_n_value(train_x,train_y,test_x,test_y,n_list,rstate=0):\n    result = []\n    for n in n_list:\n        model = AdaBoostClassifier(n_estimators=n,learning_rate=0.01,algorithm='SAMME')\n        model.fit(train_x,train_y)\n        pred = model.predict(test_x)\n        result.append(f1(np.round(pred),test_y))\n    return result","c7a323b1":"best_n = optimal_n_value(train_x,train_y,test_x,test_y,[100,200,300,600,900])\nplt.figure(figsize=(12,8))\nax = sns.lineplot(x=np.arange(5),y=best_n)\nax.set_xticklabels(labels=[-1,100,200,300,600,900])\nax.set_title('Best F1 Score Via K Variable')\nax.set_xlabel('Number Of Estimators',fontsize=15)\nax.set_ylabel('F1 Score',fontsize=15)\nplt.show()","fff241af":"model = AdaBoostClassifier(n_estimators=100,learning_rate=0.05,algorithm='SAMME')\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nmodel.fit(X,y)\npredictions = model.predict(X)\nsubmission = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')\nsubmission['Prediction'] = predictions\nsubmission\nsubmission.to_csv(\"submission_AdaBoost.csv\", index=False)","556154f8":"# EDA ","1bbda9b0":"<h3>We can see that using small k values (which are usually prone to underfitting) give us an f1 score of less the 0.6 in average and as the value of k increases\u200b the f1 score drops <\/h3>","9b1e6237":"**lets try and see a more global point view of any correlations between our features using pearson correlation.**","2483382e":"__Lets try and see if a KNN model can do better!__","9cef83fb":"### We Can See there is no major correlation beteewn any of our features. ","2a8be12c":"<h2>We got great f1 score of 0.8 for most number of estimators up to 300 and its much more stable the the random forest which may give us better results somtimes but its less stable<\/h2>","693e9710":"<h3> Our target feature is DEATH_EVENT so lets check the correlation between the target feature and the rest of the features<\/h3>","2fc92142":"<h2>Considering the analysis we did i will prefer to use the the adaboost model as it is much more stable then the random forest model which was the only one from the ones tested which showed simillar f1 scores  <\/h2>","44b53468":"# **We can see that most heart failures occur between ages 51 and 72**","1ba5119e":"<h3>Lets try using an adaboost model and see if it can overscore our RandomForest model<\/h3>","c24cd035":"# Feature Engineering\nlets try to create some interesting features which will boost our chances of a successful model.","797e814a":"# Model Selection And Evaluation ","73590394":"<h3>We can see that on average even taking acount the NP properites of the random tree algorithm and uncontrolable states we can still on average get an f1_score of 0.80<\/h3>"}}