{"cell_type":{"f72e22fe":"code","a6bed3eb":"code","86f4b5aa":"code","16b828a1":"code","b144a77f":"code","ddcba54b":"code","a1b818be":"code","eca7460c":"code","cf21a723":"code","b5c907e5":"code","e81bfe1a":"code","5af056be":"code","9f75326d":"code","1e3cfaa3":"code","e9febd67":"code","a823a4e2":"code","352ec00c":"code","09b15c6d":"code","5224d3ed":"code","78382a98":"code","f119fcae":"code","cbf3b051":"code","64c794b8":"code","380f0393":"code","ccb6ceed":"code","ea1aa44e":"code","45cdcb98":"code","2977f464":"code","baaa1525":"code","1827967b":"code","161d7058":"code","1a0de881":"code","b2cb1ade":"code","80a9b881":"code","ecec0906":"code","cadaca33":"code","8753b54e":"code","debafa7d":"code","fe984fd9":"code","030fc3b4":"code","03ca0078":"code","e1523042":"code","742a5c4f":"code","5b0ed44c":"code","51dc64ff":"code","4f5ffd59":"code","afa3bbdb":"code","ac282e1a":"code","526bc0b5":"markdown","b91d8e72":"markdown","52d41126":"markdown","dc2189ea":"markdown","94526e58":"markdown","140f27be":"markdown","63486708":"markdown","2832bbe2":"markdown","1d54da7c":"markdown","cf16935f":"markdown","e0854cce":"markdown","423b0e3f":"markdown","4acd48f6":"markdown","0b28d8f7":"markdown","89a9c197":"markdown","b6290644":"markdown","f390eb6b":"markdown","34a92606":"markdown","fad96d08":"markdown","28dbe8fd":"markdown","a5bd94ed":"markdown","06cf0c43":"markdown","f39fb096":"markdown","e52c75fb":"markdown","1629cd81":"markdown","d3f0f182":"markdown","1a7b0cc1":"markdown","8c09cd4c":"markdown","62d59b48":"markdown","ae425582":"markdown","fd187d8c":"markdown","0008471d":"markdown","e7806b37":"markdown","93f9edbb":"markdown","972aafa9":"markdown","505e46c3":"markdown","dc471d1b":"markdown","66dd82a0":"markdown","9fc0b1cb":"markdown","33bd077f":"markdown","ad9eb06b":"markdown","a2483b79":"markdown"},"source":{"f72e22fe":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom imblearn import under_sampling, over_sampling\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve, auc\nfrom mlxtend.evaluate import bias_variance_decomp\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix\nimport shap\nfrom shap import summary_plot\n\npd.set_option(\"max_column\",100)\npd.set_option(\"max_colwidth\",1000)\npd.set_option(\"max_row\",1000)","a6bed3eb":"df = pd.read_csv('..\/input\/employee-attrition\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndf.head()","86f4b5aa":"#Change values of features Attrition and Overtime, where Yes=1 and No=0\ndf['Attrition'] = np.where(df['Attrition']=='Yes', 1, 0)\ndf['OverTime'] = np.where(df['OverTime']=='Yes', 1, 0)","16b828a1":"#Categorize numerical and categorical features\nnums_features1 = ['Age','DailyRate','DistanceFromHome','Education','EmployeeCount','EmployeeNumber',\n                 'EnvironmentSatisfaction', 'HourlyRate','JobInvolvement','JobLevel', 'JobSatisfaction','MonthlyIncome',\n                 'MonthlyRate','NumCompaniesWorked','OverTime']\n\nnums_features2 = ['PercentSalaryHike','PerformanceRating','RelationshipSatisfaction','StandardHours','StockOptionLevel',\n                 'TotalWorkingYears','TrainingTimesLastYear','WorkLifeBalance','YearsAtCompany','YearsInCurrentRole',\n                 'YearsSinceLastPromotion','YearsWithCurrManager']\n\ncats_features = ['BusinessTravel','Department','EducationField','Gender','JobRole','MaritalStatus','Over18']","b144a77f":"df[nums_features1].describe()","ddcba54b":"df[nums_features2].describe()","a1b818be":"df[cats_features].describe()","eca7460c":"plt.figure(figsize=(8,5))\nsns.countplot(x='Attrition', data=df, palette = 'RdGy')\nplt.title('Attrition Rate', fontsize=14, weight='bold')\nplt.xlabel('Attrition', fontsize = 12)\nplt.ylabel('Total Employee', fontsize = 12);","cf21a723":"plt.figure(figsize=(20, 20))\nsns.heatmap(df.corr(), cmap='Reds', annot=True, fmt='.2f');","b5c907e5":"numerical_features = []\nfor column in df.columns:\n    if df[column].dtype != object:\n        numerical_features.append(column)\n        \nnumerical_features.remove('Attrition')\n\nplt.figure(figsize=(20, 40))\n\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(9, 3, i)\n    df[df[\"Attrition\"] == 0][feature].hist(bins=35, color='blue', label='Not Attrition', alpha=0.6)\n    df[df[\"Attrition\"] == 1][feature].hist(bins=35, color='red', label='Attrition', alpha=0.6)\n    plt.legend()\n    plt.xlabel(feature)\n    plt.ylabel('count')","e81bfe1a":"categorical_features = []\nfor column in df.columns:\n    if df[column].dtype == object:\n        categorical_features.append(column)\n\nplt.figure(figsize=(20, 15))\n\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(3, 3, i)\n    df[df[\"Attrition\"] == 0][feature].hist(bins=35, color='blue', label='Not Attrition', alpha=0.6)\n    df[df[\"Attrition\"] == 1][feature].hist(bins=35, color='red', label='Attrition', alpha=0.6)\n    plt.legend()\n    plt.xlabel(feature)\n    plt.ylabel('count')","5af056be":"df_s1 = df.copy()","9f75326d":"#Check if there are missing values and whether the data type is appopriate\ndf_s1.info()","1e3cfaa3":"#Check if there is any duplicate data\ndf_s1.duplicated().sum()","e9febd67":"cats_onehot = ['BusinessTravel','Department', 'EducationField', 'Gender','JobRole','MaritalStatus']\n\n#Feature encoding for categorical data using onehots\nfor cat in cats_onehot:\n    onehots = pd.get_dummies(df_s1[cat], prefix=cat)\n    df_s1 = df_s1.join(onehots)\n\ndf_s1.head()","a823a4e2":"#Drop categorical data and unnecessary features\ndf_s1 = df_s1.drop(['BusinessTravel','Department', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'Gender', 'JobRole', 'MaritalStatus', 'Over18', 'StandardHours'], axis = 1)","352ec00c":"df_s1.info()","09b15c6d":"#Split features and target\nX = df_s1.drop(columns=['Attrition'])\ny = df_s1['Attrition']\nprint(X.shape)\nprint(y.shape)","5224d3ed":"#Create undersampling and oversampling datasets\nX_under, y_under = under_sampling.RandomUnderSampler(random_state=42).fit_resample(X, y)\nX_over, y_over = over_sampling.RandomOverSampler(random_state=42).fit_resample(X, y)\nX_over_smote, y_over_smote = over_sampling.SMOTE(random_state=42).fit_resample(X, y)","78382a98":"print(pd.Series(y).value_counts())\nprint(pd.Series(y_under).value_counts())\nprint(pd.Series(y_over).value_counts())\nprint(pd.Series(y_over_smote).value_counts())","f119fcae":"#Split data training and data test\n\n#Imbalance\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.3, random_state = 42)\n\n#Undersampling\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_under, y_under, test_size = 0.3, random_state = 42)\n\n#Oversampling random\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X_over, y_over, test_size = 0.3, random_state = 42)\n\n#Oversampling smote\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X_over_smote, y_over_smote, test_size = 0.3, random_state = 42)","cbf3b051":"def eval_classification(model, pred, proba, xtrain, ytrain, xtest, ytest):\n    print(\"Accuracy (Test Set): %.2f\" % accuracy_score(ytest, pred))\n    print(\"Precision (Test Set): %.2f\" % precision_score(ytest, pred))\n    print(\"Recall (Test Set): %.2f\" % recall_score(ytest, pred))\n    print(\"F1-Score (Test Set): %.2f\" % f1_score(ytest, pred))\n    \n    fpr, tpr, thresholds = roc_curve(ytest, proba, pos_label=1)\n    print(\"AUC: %.2f\" % auc(fpr, tpr))","64c794b8":"#Training\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train1,y_train1)\n\n#Predict\ny_pred = model.predict(X_test1)\ny_proba = model.predict_proba(X_test1)\ny_proba = y_proba[:,1]\n\n#Eval\neval_classification(model, y_pred, y_proba, X_train1, y_train1, X_test1, y_test1)","380f0393":"#Checking accuracy of data training and data test\nprint('Train score: ' + str(model.score(X_train1, y_train1))) \nprint('Test score:' + str(model.score(X_test1, y_test1)))","ccb6ceed":"#Training\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train2,y_train2)\n\n#Predict\ny_pred = model.predict(X_test2)\ny_proba = model.predict_proba(X_test2)\ny_proba = y_proba[:,1]\n\n#Eval\neval_classification(model, y_pred, y_proba, X_train2, y_train2, X_test2, y_test2)","ea1aa44e":"#Checking accuracy of data training and data test\nprint('Train score: ' + str(model.score(X_train2, y_train2))) \nprint('Test score:' + str(model.score(X_test2, y_test2)))","45cdcb98":"#Training\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train3,y_train3)\n\n#Predict\ny_pred = model.predict(X_test3)\ny_proba = model.predict_proba(X_test3)\ny_proba = y_proba[:,1]\n\n#Eval\neval_classification(model, y_pred, y_proba, X_train3, y_train3, X_test3, y_test3)","2977f464":"#Checking accuracy of data training and data test\nprint('Train score: ' + str(model.score(X_train3, y_train3))) \nprint('Test score:' + str(model.score(X_test3, y_test3)))","baaa1525":"#Training\nmodel = DecisionTreeClassifier(random_state=42)\nmodel.fit(X_train4,y_train4)\n\n#Predict\ny_pred = model.predict(X_test4)\ny_proba = model.predict_proba(X_test4)\ny_proba = y_proba[:,1]\n\n#Eval\neval_classification(model, y_pred, y_proba, X_train4, y_train4, X_test4, y_test4)","1827967b":"#Checking accuracy of data training and data test\nprint('Train score: ' + str(model.score(X_train4, y_train4))) \nprint('Test score:' + str(model.score(X_test4, y_test4)))","161d7058":"df_s2 = df.copy()","1a0de881":"list_roles = []\n\nfor index, kolom in df_s2.iterrows():\n    if kolom['JobRole'] == 'Sales Executive' or kolom['JobRole'] == 'Laboratory Technician' or kolom['JobRole'] == 'Human Resources':\n        result = 'Staff'\n    elif kolom['JobRole'] == 'Sales Representative' or kolom['JobRole'] == 'Healthcare Representative' or kolom['JobRole'] == 'Research Scientist':\n        result = 'Middle'\n    else:\n        result = 'Executive'\n        \n    list_roles.append(result)\n\ndf_s2['JobRole'] = list_roles\ndf_s2.head()","b2cb1ade":"list_gen = []\n\nfor index, kolom in df_s2.iterrows():\n    if kolom['Age'] >= 55:\n        result = 'Boomers'\n    elif kolom['Age'] >= 40 and kolom['Age'] <= 54:\n        result = 'Gen X'\n    elif kolom['Age'] >= 23 and kolom['Age'] <= 39:\n        result = 'Gen Y'\n    else:\n        result = 'Gen Z'\n    list_gen.append(result)\n\ndf_s2['Generation'] = list_gen\ndf_s2.head()","80a9b881":"#Drop unnecessary features\ndf_s2 = df_s2.drop(['DailyRate', 'EmployeeCount', 'EmployeeNumber', 'HourlyRate', 'Over18', 'MonthlyRate', 'StandardHours'], axis = 1)\ndf_s2.head()","ecec0906":"df_s2.info()","cadaca33":"#Define categorical features for modelling\ncat_features = ['BusinessTravel','Department','EducationField', 'Gender', 'Generation','JobRole','MaritalStatus']","8753b54e":"#Split features and target\nX = df_s2.drop(columns=['Attrition'])\ny = df_s2['Attrition']\nprint(X.shape)\nprint(y.shape)","debafa7d":"#Split data training and data eval (before oversampling)\nX1_train, X_eval, y1_train, y_eval = train_test_split(X, y, test_size = 0.1, random_state = 42)","fe984fd9":"#Oversampling data training\nX_over, y_over = over_sampling.RandomOverSampler(random_state=42).fit_resample(X1_train, y1_train)","030fc3b4":"#Split data training and data test (after oversampling)\nX2_train, X_test, y2_train, y_test = train_test_split(X_over, y_over, test_size = 0.3, random_state = 42)","03ca0078":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(learning_rate=0.05, random_state=42, iterations=300, eval_metric='AUC')\n\nclf.fit(X2_train, y2_train, cat_features= cat_features, plot=False, eval_set=(X_eval, y_eval), verbose=True)\n\ny_pred = clf.predict(X_eval)\ny_proba = clf.predict_proba(X_eval)\ny_proba = y_proba[:,1]\neval_classification(clf, y_pred, y_proba, X2_train, y2_train, X_eval, y_eval)","e1523042":"print('Train score: ' + str(clf.score(X2_train, y2_train))) \nprint('Test score:' + str(clf.score(X_eval, y_eval))) ","742a5c4f":"from catboost import CatBoostClassifier\nclf = CatBoostClassifier(learning_rate=0.05, random_state=42, iterations=300, eval_metric='Accuracy')\n\nclf.fit(X2_train, y2_train, cat_features= cat_features, plot=False, eval_set=(X_test, y_test), verbose=True)\n\ny_pred = clf.predict(X_test)\ny_proba = clf.predict_proba(X_test)\ny_proba = y_proba[:,1]\neval_classification(clf, y_pred, y_proba, X2_train, y2_train, X_test, y_test)","5b0ed44c":"#Checking accuracy of data training and data test\nprint('Train score: ' + str(clf.score(X2_train, y2_train))) \nprint('Test score:' + str(clf.score(X_test, y_test))) ","51dc64ff":"cf = confusion_matrix(y_test, y_pred)\ncf","4f5ffd59":"group_names = ['TN','FP','FN','TP']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                cf.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     cf.flatten()\/np.sum(cf)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf, annot=labels, fmt='', cmap='rocket');","afa3bbdb":"explainer = shap.Explainer(clf)\nshap_values = explainer(X)","ac282e1a":"shap.plots.beeswarm(shap_values)","526bc0b5":"### 3. Attrition x Categorical Features","b91d8e72":"Based on the results, random oversampling show the best performance from all. Therefore, I'll proceed the prediction using random oversampling dataset.","52d41126":"### Evaluate with data test","dc2189ea":"---\n# EDA","94526e58":"### **Imbalance Dataset**","140f27be":"---\n# Phase 1 \nI'll be using Decision Tree for 4 datasets (imbalance, undersampling, oversampling random, oversampling smote)","63486708":"# Mechanism","2832bbe2":"### 1. Heatmap","1d54da7c":"### Interpretation with SHAP","cf16935f":"## Data Pre-Processing","e0854cce":"### 2. Feature Selection\nIn addition of unnecessary features before (EmployeeCount, EmployeeNumber, Over18, StandardHours) I decided to drop Rate features (DailyRate, HourlyRate, MonthlyRate) because it's the rate that company must pay not the employee received.","423b0e3f":"### **Oversampling SMOTE Dataset**","4acd48f6":"Employee is one of the most important resource in company, where a high attrition rate indicates that the company is unable to maintain their employees. In a short term, with high attrition rate, company must pay a great money to cover the cost of turnover. While in a long term, this will affect the company's performance as employees come and go the company's performance will decline.","0b28d8f7":"The modeling will be implemented in 2 phases:\n1. Phase 1 <br>\nSince the target is imbalance, in this phase I would create 4 different datasets (imbalance, undersampling, oversampling random, and oversampling smote) to see which treatment is best for imbalance class.\n2. Phase 2 <br>\nIn this phase, I would focusing on improving model's performance through feature engineering and feature selection.","89a9c197":"The result of evaluation with oversampling dataset is excellent, with no signs of overfitting","b6290644":"### **Undersampling Dataset**","f390eb6b":"## Univariate Analysis","34a92606":"### **Oversampling Random Dataset**","fad96d08":"# Goals","28dbe8fd":"There are no missing values and all data types are appropriate","a5bd94ed":"## Data Pre-Processing","06cf0c43":"Most of numerical features with nominal data type has a high variation therefore it's positively skewed. And for categorical features, the unique value is only a few.","f39fb096":"There are no duplicate data","e52c75fb":"As we can see, the target is imbalanced","1629cd81":"There are some insights:\n1. Employees with low satisfaction (indicated by EnvironmentSatisfaction, JobSatisfaction, RelationshipSatisfaction) tend to resign\n2. Employees with low benefit (indicated by MonthlyIncome, StockOptionLevel tend to resign\n3. Young employees tend to resign\n4. Employees with high number of company worked tend to resign\n5. Overtime employees tend to resign","d3f0f182":"The result of evaluation with imbalance dataset is good enough, but the model shows the sign of overfitting","1a7b0cc1":"The relation between features and target is kinda weak, where the highest correlation is with OverTime ","8c09cd4c":"### 2. Attrition x Numerical Features","62d59b48":"### Evaluate with data eval","ae425582":"## Feature Selection","fd187d8c":"There are some insights:\n1. Employees who travel frequently tend to resign\n2. Sales employees tend to resign\n3. Single employees tend to resign\n4. Female employees tend to resign","0008471d":"## Modeling","e7806b37":"## Descriptive Statistic","93f9edbb":"# Business Problem","972aafa9":"## Multivariate Analysis","505e46c3":"## Modeling\nSince I'll be using oversampling method for training the model, I'll be using 2 kind of dataset for evaluation:\n1. Data test (oversampling)\n2. Data eval (imbalance)\n\nThis is to make sure the model is able to predict imbalance data as well, because in the production most likely the data will be imbalanced","dc471d1b":"## Feature Encoding","66dd82a0":"### 1. Feature Engineering\n- Grouping job role based on job level\n- Grouping age generation","9fc0b1cb":"To analyze the factors lead to employee attrition and make prediction of it, therefore company could give an appropriate treatment for the likely attrition employee.","33bd077f":"---\n# Phase 2\nUsing CatBoost for random oversampling dataset","ad9eb06b":"The plot above shows the 9 features that affecting employee's decision to resign or not. As we can see OverTime, StockOptionLevel, and MonthlyIncome are highly affecting employee's attrition. Therefore, with these insights I came up with some strategies:\n1. Evaluate the workload of employees, why do they get overtime? And even if they have to do overtime, the benefit needed to be re-evaluated\n2. Build an appropriate culture and create a good work environment in order to increase EnvironmentSatisfaction and JobSatisfaction\n3. Give or increase StockOptionLevel to high value employees who tend to attrition","a2483b79":"# Import Package and Dataset"}}