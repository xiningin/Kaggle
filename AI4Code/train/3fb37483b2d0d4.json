{"cell_type":{"6297858d":"code","5b6c33b1":"code","51a070df":"code","71b44fc3":"code","99671370":"code","0df8ed55":"code","86c8ca5f":"code","ca35537a":"code","e9cf01e3":"code","8abfedb4":"code","60860d1c":"code","8f4e5f16":"code","ee7c6ecf":"code","18893d35":"code","182247a4":"code","387f057f":"code","26ea5af7":"code","2f53f636":"code","3a5346a4":"code","7fcc9f31":"code","746981dd":"code","8de65716":"code","b2c86e4f":"code","98d09505":"code","df3f5be7":"code","48c79397":"code","7cd07e82":"code","ae34f3fa":"code","f7573cff":"code","706fb4e1":"code","64306e56":"code","5af1acc5":"code","c479fe35":"code","4a8d008f":"code","fbb6e38e":"code","d2332dbd":"code","5d1c1cf5":"code","e74950f0":"code","20a68489":"code","b5d9ad10":"code","4c97fde9":"code","562703dc":"code","09af189f":"code","708eb88d":"markdown","8ea5602b":"markdown","8cb3f829":"markdown","bf09b1d6":"markdown","fa3ee45e":"markdown","8c0faa18":"markdown","4b7eca8f":"markdown","0fccefc4":"markdown","2b678b82":"markdown","aa8c4406":"markdown","2774a782":"markdown","a5b25e68":"markdown","8dee8137":"markdown","ebae7710":"markdown","3b20a958":"markdown","86cae036":"markdown","a0a1c07e":"markdown","752b736c":"markdown","407dbf93":"markdown","a9001aa1":"markdown","67dc9160":"markdown","70264256":"markdown","4f1c40b0":"markdown","b938993f":"markdown","9f6f21b1":"markdown","61f5b7d4":"markdown","91e7aa95":"markdown","1b886d7d":"markdown","61850cdc":"markdown","c3286e1a":"markdown","9ed55a5b":"markdown","49753ac2":"markdown","95108f85":"markdown","f90e208c":"markdown","6f0ef8ed":"markdown","1909b32b":"markdown","4d03eab1":"markdown","9a964dc0":"markdown","7fb5f78c":"markdown","a292127d":"markdown","d5f4cf43":"markdown","60cdaaff":"markdown"},"source":{"6297858d":"!pip install minisom\n!pip install tslearn","5b6c33b1":"# Native libraries\nimport os\nimport math\n# Essential Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n# Algorithms\nfrom minisom import MiniSom\nfrom tslearn.barycenters import dtw_barycenter_averaging\nfrom tslearn.clustering import TimeSeriesKMeans\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.decomposition import PCA","51a070df":"directory = '\/kaggle\/input\/retail-and-retailers-sales-time-series-collection\/'\n\nmySeries = []\nnamesofMySeries = []\nfor filename in os.listdir(directory):\n    if filename.endswith(\".csv\"):\n        df = pd.read_csv(directory+filename)\n        df = df.loc[:,[\"date\",\"value\"]]\n        # While we are at it I just filtered the columns that we will be working on\n        df.set_index(\"date\",inplace=True)\n        # ,set the date columns as index\n        df.sort_index(inplace=True)\n        # and lastly, ordered the data according to our date index\n        mySeries.append(df)\n        namesofMySeries.append(filename[:-4])","71b44fc3":"print(len(mySeries))","99671370":"fig, axs = plt.subplots(6,4,figsize=(25,25))\nfig.suptitle('Series')\nfor i in range(6):\n    for j in range(4):\n        if i*4+j+1>len(mySeries): # pass the others that we can't fill\n            continue\n        axs[i, j].plot(mySeries[i*4+j].values)\n        axs[i, j].set_title(namesofMySeries[i*4+j])\nplt.show()","0df8ed55":"fig, axs = plt.subplots(6,4,figsize=(25,25))\nfig.suptitle('Series')\nfor i in range(6):\n    for j in range(4):\n        if i*4+j+1>len(mySeries): # pass the others that we can't fill\n            continue\n        axs[i, j].plot(mySeries[i*4+j].values)\n        axs[i, j].set_title(namesofMySeries[i*4+j])\nplt.show()","86c8ca5f":"series_lengths = {len(series) for series in mySeries}\nprint(series_lengths)","ca35537a":"ind = 0\nfor series in mySeries:\n    print(\"[\"+str(ind)+\"] \"+series.index[0]+\" \"+series.index[len(series)-1])\n    ind+=1","e9cf01e3":"max_len = max(series_lengths)\nlongest_series = None\nfor series in mySeries:\n    if len(series) == max_len:\n        longest_series = series","8abfedb4":"problems_index = []\n\nfor i in range(len(mySeries)):\n    if len(mySeries[i])!= max_len:\n        problems_index.append(i)\n        mySeries[i] = mySeries[i].reindex(longest_series.index)","60860d1c":"def nan_counter(list_of_series):\n    nan_polluted_series_counter = 0\n    for series in list_of_series:\n        if series.isnull().sum().sum() > 0:\n            nan_polluted_series_counter+=1\n    print(nan_polluted_series_counter)","8f4e5f16":"nan_counter(mySeries)","ee7c6ecf":"for i in problems_index:\n    mySeries[i].interpolate(limit_direction=\"both\",inplace=True)","18893d35":"nan_counter(mySeries)","182247a4":"a = [[2],[7],[11],[14],[19],[23],[26]]\nb = [[20000000],[40000000],[60000000],[80000000],[100000000],[120000000],[140000000]]\nfig, axs = plt.subplots(1,3,figsize=(25,5))\naxs[0].plot(a)\naxs[0].set_title(\"Series 1\")\naxs[1].plot(b)\naxs[1].set_title(\"Series 2\")\naxs[2].plot(a)\naxs[2].plot(b)\naxs[2].set_title(\"Series 1 & 2\")\nplt.figure(figsize=(25,5))\nplt.plot(MinMaxScaler().fit_transform(a))\nplt.plot(MinMaxScaler().fit_transform(b))\nplt.title(\"Normalized Series 1 & Series 2\")\nplt.show()","387f057f":"for i in range(len(mySeries)):\n    scaler = MinMaxScaler()\n    mySeries[i] = MinMaxScaler().fit_transform(mySeries[i])\n    mySeries[i]= mySeries[i].reshape(len(mySeries[i]))","26ea5af7":"print(\"max: \"+str(max(mySeries[0]))+\"\\tmin: \"+str(min(mySeries[0])))\nprint(mySeries[0][:5])","2f53f636":"a = [1,2]\nb = [3,7]\nc = [1,3]\nd = [3,8]\nimg = plt.imread(\"\/kaggle\/input\/notebook-material\/arrow.png\")\nfig, axs = plt.subplots(1,3,figsize=(25,5))\naxs[0].plot(a)\naxs[0].plot(b)\naxs[0].plot(c)\naxs[0].plot(d)\naxs[0].set_title(\"Time Series\")\naxs[1].imshow(img)\naxs[1].axis(\"off\")\naxs[2].set_title(\"Data Points\")\naxs[2].scatter(a[0],a[1], s=300)\naxs[2].scatter(b[0],b[1], s=300)\naxs[2].scatter(c[0],c[1], s=300)\naxs[2].scatter(d[0],d[1], s=300)\nplt.show()","3a5346a4":"som_x = som_y = math.ceil(math.sqrt(math.sqrt(len(mySeries))))\n# I didn't see its significance but to make the map square,\n# I calculated square root of map size which is \n# the square root of the number of series\n# for the row and column counts of som\n\nsom = MiniSom(som_x, som_y,len(mySeries[0]), sigma=0.3, learning_rate = 0.1)\n\nsom.random_weights_init(mySeries)\nsom.train(mySeries, 50000)\n","7fcc9f31":"# Little handy function to plot series\ndef plot_som_series_averaged_center(som_x, som_y, win_map):\n    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n    fig.suptitle('Clusters')\n    for x in range(som_x):\n        for y in range(som_y):\n            cluster = (x,y)\n            if cluster in win_map.keys():\n                for series in win_map[cluster]:\n                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n                axs[cluster].plot(np.average(np.vstack(win_map[cluster]),axis=0),c=\"red\")\n            cluster_number = x*som_y+y+1\n            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n\n    plt.show()","746981dd":"win_map = som.win_map(mySeries)\n# Returns the mapping of the winner nodes and inputs\n\nplot_som_series_averaged_center(som_x, som_y, win_map)","8de65716":"win_map = som.win_map(mySeries)\n# Returns the mapping of the winner nodes and inputs\n\nplot_som_series_averaged_center(som_x, som_y, win_map)","b2c86e4f":"def plot_som_series_dba_center(som_x, som_y, win_map):\n    fig, axs = plt.subplots(som_x,som_y,figsize=(25,25))\n    fig.suptitle('Clusters')\n    for x in range(som_x):\n        for y in range(som_y):\n            cluster = (x,y)\n            if cluster in win_map.keys():\n                for series in win_map[cluster]:\n                    axs[cluster].plot(series,c=\"gray\",alpha=0.5) \n                axs[cluster].plot(dtw_barycenter_averaging(np.vstack(win_map[cluster])),c=\"red\") # I changed this part\n            cluster_number = x*som_y+y+1\n            axs[cluster].set_title(f\"Cluster {cluster_number}\")\n\n    plt.show()","98d09505":"win_map = som.win_map(mySeries)\n\nplot_som_series_dba_center(som_x, som_y, win_map)","df3f5be7":"win_map = som.win_map(mySeries)\n\nplot_som_series_dba_center(som_x, som_y, win_map)","48c79397":"cluster_c = []\ncluster_n = []\nfor x in range(som_x):\n    for y in range(som_y):\n        cluster = (x,y)\n        if cluster in win_map.keys():\n            cluster_c.append(len(win_map[cluster]))\n        else:\n            cluster_c.append(0)\n        cluster_number = x*som_y+y+1\n        cluster_n.append(f\"Cluster {cluster_number}\")\n\nplt.figure(figsize=(25,5))\nplt.title(\"Cluster Distribution for SOM\")\nplt.bar(cluster_n,cluster_c)\nplt.show()","7cd07e82":"# Let's check first 5\nfor series in mySeries[:5]:\n    print(som.winner(series))","ae34f3fa":"cluster_map = []\nfor idx in range(len(mySeries)):\n    winner_node = som.winner(mySeries[idx])\n    cluster_map.append((namesofMySeries[idx],f\"Cluster {winner_node[0]*som_y+winner_node[1]+1}\"))\n\npd.DataFrame(cluster_map,columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")","f7573cff":"cluster_count = math.ceil(math.sqrt(len(mySeries))) \n# A good rule of thumb is choosing k as the square root of the number of points in the training data set in kNN\n\nkm = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n\nlabels = km.fit_predict(mySeries)","706fb4e1":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\n# For each label there is,\n# plots every series with that label\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","64306e56":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\n# For each label there is,\n# plots every series with that label\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","5af1acc5":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(dtw_barycenter_averaging(np.vstack(cluster)),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","c479fe35":"cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\ncluster_n = [\"Cluster \"+str(i) for i in range(cluster_count)]\nplt.figure(figsize=(15,5))\nplt.title(\"Cluster Distribution for KMeans\")\nplt.bar(cluster_n,cluster_c)\nplt.show()","4a8d008f":"labels","fbb6e38e":"fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\npd.DataFrame(zip(namesofMySeries,fancy_names_for_labels),columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")","d2332dbd":"pca = PCA(n_components=2)\n\nmySeries_transformed = pca.fit_transform(mySeries)","5d1c1cf5":"plt.figure(figsize=(25,10))\nplt.scatter(mySeries_transformed[:,0],mySeries_transformed[:,1], s=300)\nplt.show()","e74950f0":"print(mySeries_transformed[0:5])","20a68489":"kmeans = KMeans(n_clusters=cluster_count,max_iter=5000)\n\nlabels = kmeans.fit_predict(mySeries_transformed)","b5d9ad10":"plt.figure(figsize=(25,10))\nplt.scatter(mySeries_transformed[:, 0], mySeries_transformed[:, 1], c=labels, s=300)\nplt.show()","4c97fde9":"plot_count = math.ceil(math.sqrt(cluster_count))\n\nfig, axs = plt.subplots(plot_count,plot_count,figsize=(25,25))\nfig.suptitle('Clusters')\nrow_i=0\ncolumn_j=0\nfor label in set(labels):\n    cluster = []\n    for i in range(len(labels)):\n            if(labels[i]==label):\n                axs[row_i, column_j].plot(mySeries[i],c=\"gray\",alpha=0.4)\n                cluster.append(mySeries[i])\n    if len(cluster) > 0:\n        axs[row_i, column_j].plot(np.average(np.vstack(cluster),axis=0),c=\"red\")\n    axs[row_i, column_j].set_title(\"Cluster \"+str(row_i*som_y+column_j))\n    column_j+=1\n    if column_j%plot_count == 0:\n        row_i+=1\n        column_j=0\n        \nplt.show()","562703dc":"cluster_c = [len(labels[labels==i]) for i in range(cluster_count)]\ncluster_n = [\"cluster_\"+str(i) for i in range(cluster_count)]\nplt.figure(figsize=(15,5))\nplt.title(\"Cluster Distribution for KMeans\")\nplt.bar(cluster_n,cluster_c)\nplt.show()","09af189f":"fancy_names_for_labels = [f\"Cluster {label}\" for label in labels]\npd.DataFrame(zip(namesofMySeries,fancy_names_for_labels),columns=[\"Series\",\"Cluster\"]).sort_values(by=\"Cluster\").set_index(\"Series\")","708eb88d":"# 3. Libraries\n\nIn here, you can easily reach the libraries that I used in this notebook.\n- [Pandas](https:\/\/github.com\/pandas-dev\/pandas)\n- [NumPy](https:\/\/github.com\/numpy\/numpy)\n- [scikit-learn](https:\/\/github.com\/scikit-learn\/scikit-learn)\n- [MiniSom](https:\/\/github.com\/JustGlowing\/minisom)\n- [tslearn](https:\/\/github.com\/tslearn-team\/tslearn)\n- [matplotlib](https:\/\/matplotlib.org\/)","8ea5602b":"And we can see that now with the ```PCA``` algorithm, our series are much more equally distributed to clusters than before.","8cb3f829":"### 2. 3. 2. K-Means\n\nK-means clustering is a method that aims to cluster n input to k clusters in which each data point belongs to cluster with the nearest mean (cluster centroid). It can be visualized as Voronoi cells and it is one of the most popular clustering algorithms and the most basic one. For more info about k-means, you can check [this medium post](https:\/\/towardsdatascience.com\/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a).\n\n![Training process](https:\/\/i.imgur.com\/k4XcapI.gif)\n\nIn order to cluster our series with k-means, the essential thing to do is, as we do it with som, removing our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point. Another important thing to do is, selecting the distance metric. In the k-means algorithm, people usually use the euclidean distance but as we've seen in [DBA](https:\/\/github.com\/fpetitjean\/DBA), it is not effective in our case. So, we will be using Dynamic Time Warping (DTW) instead of euclidean distance and you can see why we are doing this in the following images.\n\n![Difference of dtw and euclidean distance](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/69\/Euclidean_vs_DTW.jpg)","bf09b1d6":"In order to make this piece of information more appealing to eye, we can map each node to a number <br>\n\n```e.g. for n*m grid (0,0)=1, (0,1)=2, ... (0,m)=m+1, (1,0)=(m+1)+1, (1,1)=(m+1)+2, ... , (n,m)=(n+1)*(m+1) ``` \n\nand print the name of the series with the cluster number.","fa3ee45e":"As you can see from the plot below, som perfectly clustered the 23 different series into 8 clusters.","8c0faa18":"#### 2. 3. 2. 1. Results\n\nAfter the training, I plotted the results as I did with the som. For each cluster, I plotted every series, a little bit transparent and in gray, and in order to see the movement or the shape of the cluster, I took the average of the cluster and plotted that averaged series in red.","4b7eca8f":"# 1. Introduction\n## 1. 1. What is clustering ?\n\n   Clustering is a type of unsupervised learning problem and the main idea is finding similarities between different data points and pair them under the same group in a way that those data points in the same group (cluster) are more like each other than to those in other groups. It is one of the main tasks of exploratory data mining and used in many fields such as bioinformatics, pattern recognition, image analysis, machine learning, etc.\n    \n![Clustering algorithms benchmark](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cluster_comparison_0011.png) \n<center>Source : scikit-learn Documentation: <a href=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cluster_comparison_0011.png\">sphx_glr_plot_cluster_comparison_0011.png<\/a><\/center>\n    \n## 1. 2. What are time series ?\n    \n   Time series are a stream of data that are created by making measures of something such as sales, temperature, stocks, etc. in fixed frequency. They have to be indexed in time order and usually used in weather forecasting, econometrics, earthquake prediction, signal processing, etc.\n    \n![Time series example](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/77\/Random-data-plus-trend-r2.png)\n<center>Source : Wiki Commons: <a href=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/77\/Random-data-plus-trend-r2.png\">Random-data-plus-trend-r2.png<\/a><\/center>\n\n# 2. Analysis\n\nIn this notebook, we will be using [Retail and Retailers Sales Time Series Collection](https:\/\/www.kaggle.com\/census\/retail-and-retailers-sales-time-series-collection) that is provided by [US Census Bureau](https:\/\/www.kaggle.com\/census).\n\n## 2. 1. Let's check the data\n\nFirst of all, let's read it from the input and put them in a list.","0fccefc4":"Thus, we don't have to use ```dtw``` anymore and instead of ```TimeSeriesKMeans``` from tslearn, we can use basic ```KMeans``` from ```sklearn```.","2b678b82":"We have 3 series that are polluted with nan and we used to have 3 series that are shorter than others, so math checks out.","aa8c4406":"## 2. 2. Preprocessing\n\nBefore we start analyzing let's check if our data is uniform in length.","2774a782":"The result of PCA is basically, representation of a 333-dimensional data point as a 2-dimensional data point. As a result of that instead of a time series, we have just 2 value for each series.","a5b25e68":"# **Contents**\n- [1. Introduction](#1.-Introduction)\n    - [1.1. What is clustering ?](#1.-1.-What-is-clustering-?)\n    - [1.2. What is time series ?](#1.-2.-What-are-time-series-?)\n- [2. Analysis](#2.-Analysis)\n    - [2.1. Let's check the data](#2.-1.-Let's-check-the-data)\n    - [2.2. Preprocessing](#2.-2.-Preprocessing)\n    - [2.3. Clustering](#2.-3.-Clustering)\n        - [2.3.1. SOM](#2.-3.-1.-SOM)\n           - [2.3.1.1. Results](#2.-3.-1.-1.-Results)\n           - [2.3.1.2. Cluster Distribution](#2.-3.-1.-2.-Cluster-Distribution)\n           - [2.3.1.3. Cluster Mapping](#2.-3.-1.-3.-Cluster-Mapping)\n        - [2.3.2. K-Means](#2.-3.-2.-K-Means)\n            - [2.3.2.1. Results](#2.-3.-2.-1.-Results)\n            - [2.3.2.2. Cluster Distribution](#2.-3.-2.-2.-Cluster-Distribution)\n            - [2.3.2.3. Cluster Mapping](#2.-3.-2.-3.-Cluster-Mapping)\n            - [2.3.2.4. Curse of Dimensionality](#2.-3.-2.-4.-Curse-of-Dimensionality)\n- [3. Libraries](#3.-Libraries)\n- [4. References](#4.-References)\n- [5. See Also](#5.-See-Also)","8dee8137":"We can check how many series are polluted with nan values with this function.","ebae7710":"#### 2. 3. 2. 2. Cluster Distribution\n\nWe can see the distribution of the time series in clusters in the following chart. And it seems like k-means clustered 15 of the time series as cluster 1, which is a bit skewed. The reason why this happens is the most probably ```The Curse of Dimentionality``` <p><small><small>You can check it out from the links that I provided at section 5 (See Also)<\/small><\/small><\/p>\n","3b20a958":"# 5. See Also\n\n* [K-means Clustering: Algorithm, Applications, Evaluation Methods, and Drawbacks](https:\/\/towardsdatascience.com\/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a)\n* [Self Organizing Maps](https:\/\/medium.com\/@abhinavr8\/self-organizing-maps-ff5853a118d4)\n* <p style=\"color:red\"><a href=\"https:\/\/towardsdatascience.com\/the-curse-of-dimensionality-50dc6e49aa1e\">The Curse of Dimensionality<\/a> <b>***<\/b><\/p>\n* <p style=\"color:red\"><a href=\"https:\/\/towardsdatascience.com\/k-nearest-neighbors-and-the-curse-of-dimensionality-e39d10a6105d\">k-Nearest Neighbors and the Curse of Dimensionality<\/a> <b>***<\/b><\/p>\n* <p style=\"color:red\"><a href=\"https:\/\/medium.com\/@aptrishu\/understanding-principle-component-analysis-e32be0253ef0\">Understanding Principal Component Analysisy<\/a> <b>***<\/b><\/p>\n* <p style=\"color:red\"><a href=\"https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ&ab_channel=StatQuestwithJoshStarmer\">StatQuest: Principal Component Analysis (PCA), Step-by-Step<\/a> <b>*** (An awesome video by StatQuest)<\/b><\/p>","86cae036":"We can't see much difference from this result but, I highly recommend that to use this method for this purpose. But, also, note that the operation of dba is not a light one. So, if you seek speed, this method might not be for you.","a0a1c07e":"## 2. 3. Clustering\n\nI will be using 2 different methods for clustering these series. The first of the methods is Self Organizing Maps(SOM) and the other method is K-Means.\n\n### 2. 3. 1. SOM\n \nSelf-organizing maps are a type of neural network that is trained using unsupervised learning to produce a low-dimensional representation of the input space of the training samples, called a map.\n\n![SOM](https:\/\/raw.githubusercontent.com\/izzettunc\/Kohonen-SOM\/master\/data\/screenshots\/landing.png)\n<center>Source : Github Repo: <a href=\"https:\/\/raw.githubusercontent.com\/izzettunc\/Kohonen-SOM\/master\/data\/screenshots\/landing.png\">landing.png<\/a><\/center>\n<br>    \nAlso, self-organizing maps  differ from other artificial neural networks as they apply competitive(or cooperative) learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n\n![Learning process of som](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/35\/TrainSOM.gif)\n<center>Source : Wiki Commons: <a href=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/35\/TrainSOM.gif\">TrainSOM.gif<\/a><\/center>\n<br>\nBecause of the ability to produce a map, som deemed as a method to do dimensionality reduction. But in our case, when each node of the som is accepted as medoids of the cluster, we can use it for clustering. To do so, we should remove our time indices from our time series, and instead of measured values of each date, we should accept them as different features and dimensions of a single data point.\n\nFor more info about some, you can check [this medium post](https:\/\/medium.com\/@abhinavr8\/self-organizing-maps-ff5853a118d4).","752b736c":"#### 2. 3. 1. 1. Results\n\nAfter the training, I plotted the results. For each cluster, I plotted every series, a little bit transparent and in gray, and in order to see the movement or the shape of the cluster, I took the average of the cluster and plotted that averaged series in red .","407dbf93":"For the implementation of the som algorithm I used [miniSom](https:\/\/github.com\/JustGlowing\/minisom) and set my parameters as follows:\n- sigma: 0.3\n- learning_rate: 0.5\n- random weight initialization\n- 50.000 iteration\n- Map size: square root of the number of series\n\nAs a side note, I didn't optimize these parameters due to the simplicity of the dataset.","a9001aa1":"As you can see from the plot below, k-means clustered the 23 different series into 5 clusters. 2 of the clusters contains only 1 time series which may be deemed as an outlier.","67dc9160":"And this is the result of the basic KMeans, pretty logical and straight forward.","70264256":"As we guessed, it is not uniform in length. So in this case, we should find which series contain missing data and fill them. Because, otherwise our indices will be shifted and i.th index -let's say it is 10th of May- of the x series won't be same as i.th index of the y series -let's say i.th index of the y series may be 11th of May-.","4f1c40b0":"It seems like there are pretty much similar time series such as ```MRTSSM44000USS``` and ```RETAILMSA``` or ```MRTSSM7221USN``` and ```MRTSSM44611USN```.","b938993f":"#### 2. 3. 2. 3. Cluster Mapping\n\nAs we did before, in this part we will be finding which series belonging to which cluster. Thanks to awesome scikit-learn library we actually already have that information. Order of the labels is the same order with our series.\n","9f6f21b1":"Another method to extract the movement\/shape of the cluster is instead of averaging each series in the cluster, using Dynamic Time Warping Barycenter Averaging ([DBA](https:\/\/github.com\/fpetitjean\/DBA)).\n\nDBA is another type of averaging method that used the Dynamic Time Warping method in it and might be very useful to extract the movement\/shape of the cluster as seen in the following images.\n\n![Arithmetic Averaging](https:\/\/raw.githubusercontent.com\/fpetitjean\/DBA\/master\/images\/arithmetic.png)\n![DBA](https:\/\/raw.githubusercontent.com\/fpetitjean\/DBA\/master\/images\/DBA.png)\n\n\nTo do so, I used ```dtw_barycenter_averaging``` method in the [tslearn](https:\/\/github.com\/tslearn-team\/tslearn) library and changed the ```np.average``` with it.","61f5b7d4":"Because these series lack only one point, I used linear interpolation to fill the gap but for series that have more missing value, you can use much more complex interpolation methods such as quadratic, cubic, spline, barycentric, etc.","91e7aa95":"So, for 23 series let's create a 6 by 4 grid which will be resulted in 24 slots and fill it with the plot of our series.","1b886d7d":"After handling missing values, the other issue is the scale of the series. Without, normalizing data the series that looks like each other will be seen so different from each other and will affect the accuracy of the clustering process. We can see the effect of the normalizing in the following images.\n\n","61850cdc":"Let's check how many series we have.","c3286e1a":"Now with less dimension than before, we can see how our series distributed in 2 dimensions.","9ed55a5b":"And again thanks to the clever implementation of ```KMeans``` algorithm by ```sklearn``` team, labels are returned in the same order. Thus, we can use the same code to visualize our cluster in series.","49753ac2":"#### 2. 3. 1. 2. Cluster Distribution\nWe can see the distribution of the time series in clusters in the following chart.","95108f85":"# 4. References\n\n- Petitjean F., Ketterlin A., Gan\u00e7arski P., A global averaging method for dynamic time warping, with applications to clustering, Pattern Recognition, 44(3), 678-693, 2011\n- Kohonen T., Self-organized formation of topologically correct feature maps, Biological Cybernetics, 43, 59\u201369, 1982\n- Bellman R., Kalaba R., On adaptive control processes, in IRE Transactions on Automatic Control, 4(2), 1-9, 1959","f90e208c":"In this code block, I reindexed the series that are not as long as the longest one and fill the empty dates with ```np.nan```.","6f0ef8ed":"Note that we normalized each time series by their own values, not the values of other time series.","1909b32b":"The result of the normalizing process seems fine.","4d03eab1":"As we can see, now all of our series are the same length and don't contain any missing value.","9a964dc0":"As you can see 6th, 11th and 12th series are not starting from the same date as others. To solve this problem, we should first find the longest series of the series and elongate others according to that. Usually, to do this we should check the oldest and newest date and elongate all series according to these dates. But in our case, nearly every series starts from 1992-01-01 and ends in 2019-09-01. Thus, finding the longest series will be enough for us.","7fb5f78c":"Hey, this is my first notebook and tutorial in Kaggle, so feel free to criticize and comment about, any error that you see, any idea of improvement for this notebook, or questions that you have.\n\nHave a nice day!","a292127d":"#### 2. 3. 1. 3. Cluster Mapping\n\n<p style=\"color:gray\">(Thank you for this wonderful question <a href=\"https:\/\/www.kaggle.com\/stephentseng\">Stephen Tseng<\/a>)<\/p>\nWell, we did cluster our series but how de we know which series belonging to which cluster? Ain't that the whole purpose of clustering? <br><br>\n\nAs we can see in [these illustrations](#2.-3.-1.-SOM) each node (or multiple of nodes in some cases) represents a cluster. Therefore we can find out which series is belonging to which cluster by checking the winner node of each series. ","d5f4cf43":"#### 2. 3. 2. 4. Curse of Dimensionality\n\nCurse of Dimensionality is a term, first invented by Richard E. Bellman when considering problems in dynamic programming. It basically means, when the dimensionality of the data increase so does the distance between data points. Thus, this change in measurement of distance affects the distance-based algorithms badly. To learn for more about it please check section [5. See Also](#5.-See-Also).\n\nTo solve this problem there are numerous algorithms that can be helpful such as PCA which is the most prominent of them, t-SNE, UMAP(map of the som), etc.","60cdaaff":"As I did before, I used [DBA](https:\/\/github.com\/fpetitjean\/DBA) to see much more time dilated series."}}