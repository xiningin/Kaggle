{"cell_type":{"1fbe4b88":"code","75cae73a":"code","33c96665":"code","a8abadb1":"code","8538b94c":"code","00cdc438":"code","3ce1a051":"code","f69a4ebe":"code","9819c410":"code","b39995b3":"code","c49feea9":"code","6e89542d":"code","fecae7c1":"code","194bc4e5":"code","d212d55a":"code","c3be66c1":"code","7d7905fb":"code","cc9f41b7":"code","fa915c9d":"code","f5d9a71e":"code","06525020":"code","8adcfb16":"code","46062e40":"code","0f896ae6":"code","00c0c211":"code","86a99789":"code","29f53782":"code","77d9d7b2":"code","61e21883":"code","0b5de840":"code","cab84472":"code","c53cbad3":"code","1c105a52":"code","a55166f1":"code","f404e92c":"code","ba426f65":"markdown","233b9d0b":"markdown","2a33d682":"markdown","8c8ebd27":"markdown","9a07617b":"markdown"},"source":{"1fbe4b88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75cae73a":"train=pd.read_csv('\/kaggle\/input\/ieee-ml-hack\/train.csv')\ntrain","33c96665":"test=pd.read_csv('\/kaggle\/input\/ieee-ml-hack\/test.csv')\ntest","a8abadb1":"train.info()","8538b94c":"import seaborn as sns\nimport matplotlib.pyplot as plt\ncor_mat=train.corr().round(2)\nplt.figure(figsize=(15,10))\nsns.heatmap(data=cor_mat,annot=True)","00cdc438":"#train['total']=train['50s']+train['6s']+train['Balls']","3ce1a051":"for i in train.columns:\n    a=train[i].isnull().sum()\n    if a>0:\n        print('column {} with null value'.format(i), a)","f69a4ebe":"train.drop(columns=['Name'],inplace=True)\ntest.drop(columns=['Name'],inplace=True)","9819c410":"#train[\"mean\"]  = train.groupby(['ID'])['6s'].transform('mean')\n","b39995b3":"from sklearn.model_selection import train_test_split\nX = train.drop(columns=['Ratings','Innings','Maidens','Age','Balls','Economy_Rate','100s'])\nY = train['Ratings']\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size=0.3, random_state=7)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","c49feea9":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.impute import SimpleImputer\n#from sklearn.model_selection import cross_val_score\nresults = list()\nstrategies = ['mean', 'median', 'most_frequent', 'constant']\nfor s in strategies:\n\n    pipeline = Pipeline(steps=[('i', SimpleImputer(strategy=s)), ('m', RandomForestRegressor())])\n   \n    pipeline.fit(X_train,Y_train)\n    scores = pipeline.score(X_test, Y_test)\n    \n    results.append(scores)\n    print('%s %.3f'% (s, np.mean(scores)))","6e89542d":"X_train = X_train.fillna(X_train.mean())\nX_test=X_test.fillna(X_test.mean())","fecae7c1":"pipeline = Pipeline(steps=[('m', RandomForestRegressor())])\npipeline.fit(X_train, Y_train)\nX_test.fillna(X_test.median())\nscores = pipeline.score(X_test, Y_test)\nprint((scores))","194bc4e5":"from sklearn.metrics import mean_squared_error,r2_score\nr_model = RandomForestRegressor(n_estimators=1800,max_features=\"auto\",n_jobs=-1,max_samples=0.7,max_depth=10, random_state=0)\nr_model.fit(X_train,Y_train)\ny_pred = r_model.predict(X_test)\n\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)","d212d55a":"feature_important = r_model.feature_importances_\nfeature_important\ntotal = sum(feature_important)\nnew = [value * 100. \/ total for value in feature_important]\nnew = np.round(new,2)\nkeys = list(X_train.columns)\nfeature_importances = pd.DataFrame()\nfeature_importances['Features'] = keys\nfeature_importances['Importance (%)'] = new\nfeature_importances = feature_importances.sort_values(['Importance (%)'],ascending=False).reset_index(drop=True)\nfeature_importances","c3be66c1":"import xgboost as xgb\ndtr =  xgb.XGBRegressor( max_depth=3,\n                        min_child_weight=1,\n                        gamma=5,\n                     eta = 0.04,\n            n_estimators = 1000 ,\n                  subsample=0.8,\n                        colsample_bytree=0.8,\n                        seed=300)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)   ","7d7905fb":"test_model = xgb.XGBRegressor(\n            max_depth=3,\n                        min_child_weight=1,\n                        gamma=5,\n                     eta = 0.04,\n            n_estimators = 1000 ,\n                  subsample=0.8,\n                        colsample_bytree=0.8,\n                        seed=300\n)\n#model.fit(X_train, y_train)\ntest_model.fit(X_train, Y_train, eval_metric='rmse', \n          eval_set=[(X_test, Y_test)], early_stopping_rounds=500, verbose=100)","cc9f41b7":"hyper_params = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': ['l2', 'auc'],\n    'learning_rate': 0.005,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 15,\n    'verbose': 0,\n    \"max_depth\": 10,\n    \"num_leaves\": 128,  \n    \"max_bin\": 512,\n    \"num_iterations\": 100000,\n    \"n_estimators\": 1000\n}","fa915c9d":"import lightgbm as lgb\ngbm = lgb.LGBMRegressor(**hyper_params)","f5d9a71e":"\ngbm.fit(X_train, Y_train,\n        eval_set=[(X_test, Y_test)],\n        eval_metric='l1',\n        early_stopping_rounds=1000)\n","06525020":"y_pred = gbm.predict(X_train, num_iteration=gbm.best_iteration_)\nprint('The r2_score of prediction is:', round(r2_score(y_pred, Y_train), 5))","8adcfb16":"from sklearn.neighbors import KNeighborsRegressor\ndtr =  KNeighborsRegressor( n_neighbors=15, weights='distance', algorithm='brute', leaf_size=1000, p=1, metric='minkowski', n_jobs=-1)\ndtr.fit(X_train,Y_train)\ny_pred = dtr.predict(X_test)\nmse = mean_squared_error(Y_test, y_pred)\nprint(\"RMSE Error:\", np.sqrt(mse))\nr2 = r2_score(Y_test, y_pred)\nprint(\"R2 Score:\", r2)  ","46062e40":"test","0f896ae6":"# g=test.iloc[:372]\n# g","00c0c211":"#test['Age']=test['Age'].map({'Un':42})","86a99789":"# test['Age']=test['Age'].map({'none':'42'})\n# test['100s']=test['100s'].map({'none':'42'})\ntest['Balls']=test['Balls'].map({'none':'42'})\n# test['Innings']=test['Innings'].map({'none':'42'})\n# test['Economy_Rate']=test['Economy_Rate'].map({'none':'42'})\n# test['Maidens']=test['Maidens'].map({'none':'42'})","29f53782":"test=test.fillna(test.mean())","77d9d7b2":"test","61e21883":"# for i in test.columns:\n#     a=test[i].isnull().sum()\n#     if a>0:\n#         print('column {} with null value'.format(i), a)","0b5de840":"test.drop(columns=['Innings','Maidens','Age','Balls','Economy_Rate','100s'],inplace=True)","cab84472":"test_pred=gbm.predict(test)\ntest_pred","c53cbad3":"result=pd.DataFrame(test_pred)","1c105a52":"result['Ratings']=pd.DataFrame(test_pred)\nresult['ID']=test['ID']","a55166f1":"result.to_csv('a3.csv',index=False)","f404e92c":"sub=pd.read_csv('\/kaggle\/input\/ieee-ml-hack\/sample_solution.csv')\nsub","ba426f65":"# working on test data","233b9d0b":"# NULL VALUES COUNT","2a33d682":"# XGB","8c8ebd27":"# LGBM","9a07617b":"# RANDOM FOREST"}}