{"cell_type":{"cb180484":"code","29dfdcfe":"code","e3ba118e":"code","1c54cef2":"code","5ca3a899":"code","e4cf0282":"code","6bc2c687":"code","64f06b7a":"code","03ccefc7":"code","ac5eacf4":"code","06c2ae9f":"code","54206676":"code","658355c1":"code","d22a7f9f":"code","182cd781":"code","c4849ddc":"code","20f9a94a":"code","45ac7ff1":"code","e8cacbec":"code","0ff48a7d":"code","f720ba4f":"code","1e865a65":"code","b7ae5608":"code","8ce0ccd0":"code","4e4c7385":"code","e3ed8950":"code","0a724a0c":"code","2801cd40":"code","2fc75e03":"code","b12a7873":"code","72f6eaab":"markdown","06e7322d":"markdown","d7e95384":"markdown","3a7076c5":"markdown","c0f4a167":"markdown","35d71262":"markdown","0370365d":"markdown","e0251b67":"markdown","a9456d27":"markdown","64cbd7c6":"markdown","0a415706":"markdown","2a1cba35":"markdown","312f13f2":"markdown","5305786a":"markdown"},"source":{"cb180484":"# Display images inline in pandas dataframe https:\/\/www.kaggle.com\/stassl\/displaying-inline-images-in-pandas-dataframe\nimport glob\nimport random\nimport base64\nimport pandas as pd\n\nfrom PIL import Image\nfrom io import BytesIO\nfrom IPython.display import HTML\npd.set_option('display.max_colwidth', -1)\n\ndef get_thumbnail(path):\n    i = Image.open(path)\n    i.thumbnail((150, 150), Image.LANCZOS)\n    return i\n\ndef image_base64(im):\n    if isinstance(im, str):\n        im = get_thumbnail(im)\n    with BytesIO() as buffer:\n        im.save(buffer, 'jpeg')\n        return base64.b64encode(buffer.getvalue()).decode()\n\ndef image_formatter(im):\n    im = Image.fromarray(im)\n    return f'<img src=\"data:image\/jpeg;base64,{image_base64(im)}\">'","29dfdcfe":"# Import TensorFlow >= 1.10 and enable eager execution\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nimport os\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport PIL\nfrom IPython.display import clear_output\n%matplotlib inline","e3ba118e":"tf.test.is_gpu_available()","1c54cef2":"DATA_DIR = '..\/input\/TerrainMapPairs'","5ca3a899":"filenames = os.listdir(DATA_DIR)\npaths = [os.path.join(DATA_DIR, filename) for filename in filenames]\n\ndf = pd.DataFrame({\n                    'filename': filenames,\n                    'path': paths\n                  })","e4cf0282":"# get images from file path, load as numpy arrays\ndf['img'] = df['path'].transform(lambda path: np.array(Image.open(path)))","6bc2c687":"# downsample images to 256x256 px\ndf['img_256'] = df['img'].transform(lambda img: img[::2,::2,:])","64f06b7a":"# pull out map from left of image and terrain from right of image\ndf['map'] = df['img_256'].transform(lambda img: img[:,:256,:])\ndf['terrain'] = df['img_256'].transform(lambda img: img[:,256:,:])\n\n# make tensors\ndf['tensor_256'] = df['img_256'].transform(lambda img: tf.convert_to_tensor(img))","03ccefc7":"# randomly choose data to be test, train and validation, give dataframe column\ndf['set'] = [np.random.choice(['test','train','val'], \n                              p=[2\/3, 1\/6, 1\/6])\n            for i in range(len(df))]","ac5eacf4":"# look at map, terrain and set columns from df\nHTML(df[['map','terrain','set']].head().to_html(formatters={'img': image_formatter,\n                                   'img_256': image_formatter,\n                                   'map': image_formatter,\n                                   'terrain': image_formatter},\n                       escape=False))","06c2ae9f":"PATH = '..\/input\/TerrainMapPairs\/'\nBUFFER_SIZE = 400\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\nDELTA = 30","54206676":"def load_image(image_file, is_train):\n    image = tf.read_file(image_file)\n    image = tf.image.decode_jpeg(image)\n\n    w = tf.shape(image)[1]\n\n    w = w \/\/ 2\n    # Stride of 2 to downsample to 256x256 px\n    # Original version by Thomas Pappas used full 512 px,\n    # but downsampling here allow use of original Pix2Pix\n    # architecture and reduce running time\n    real_image = image[::2, w::2, :]\n    input_image = image[::2, :w:2, :]\n\n    input_image = tf.cast(input_image, tf.float32)\n    real_image = tf.cast(real_image, tf.float32)\n\n    if is_train:\n        # random jittering\n\n        # resizing to larger than original size (512+delta x 512+delta x 3)\n        input_image = tf.image.resize_images(input_image, [IMG_WIDTH+DELTA, IMG_HEIGHT+DELTA], \n                                            align_corners=True, \n                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        real_image = tf.image.resize_images(real_image, [IMG_WIDTH+DELTA, IMG_HEIGHT+DELTA], \n                                            align_corners=True, \n                                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n        # randomly cropping to original size (512 x 512 x 3)\n        stacked_image = tf.stack([input_image, real_image], axis=0)\n        cropped_image = tf.random_crop(stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n        input_image, real_image = cropped_image[0], cropped_image[1]\n\n        if np.random.random() > 0.5:\n            # random mirroring\n            input_image = tf.image.flip_left_right(input_image)\n            real_image = tf.image.flip_left_right(real_image)\n    else:\n        input_image = tf.image.resize_images(input_image, size=[IMG_HEIGHT, IMG_WIDTH], \n                                             align_corners=True, method=2)\n        real_image = tf.image.resize_images(real_image, size=[IMG_HEIGHT, IMG_WIDTH], \n                                            align_corners=True, method=2)\n    \n    # normalizing the images to [-1, 1]\n    input_image = (input_image \/ 127.5) - 1\n    real_image = (real_image \/ 127.5) - 1\n\n    return input_image, real_image","658355c1":"train_files = df.loc[df['set']=='train','path']\ntest_files = df.loc[df['set']=='test','path']\nval_files = df.loc[df['set']=='val','path']","d22a7f9f":"train_dataset = tf.data.Dataset.list_files(train_files)\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.map(lambda x: load_image(x, True))\ntrain_dataset = train_dataset.batch(1)","182cd781":"test_dataset = tf.data.Dataset.list_files(test_files)\ntest_dataset = test_dataset.map(lambda x: load_image(x, False))\ntest_dataset = test_dataset.batch(1)","c4849ddc":"OUTPUT_CHANNELS = 3","20f9a94a":"class Downsample(tf.keras.Model):\n\n    def __init__(self, filters, size, apply_batchnorm=True):\n        super(Downsample, self).__init__()\n        self.apply_batchnorm = apply_batchnorm\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.conv1 = tf.keras.layers.Conv2D(filters, \n                                            (size, size), \n                                            strides=2, \n                                            padding='same',\n                                            kernel_initializer=initializer,\n                                            use_bias=False)\n        if self.apply_batchnorm:\n            self.batchnorm = tf.keras.layers.BatchNormalization()\n\n    def call(self, x, training):\n        x = self.conv1(x)\n        if self.apply_batchnorm:\n            x = self.batchnorm(x, training=training)\n        x = tf.nn.leaky_relu(x)\n        return x \n\n\nclass Upsample(tf.keras.Model):\n\n    def __init__(self, filters, size, apply_dropout=False):\n        super(Upsample, self).__init__()\n        self.apply_dropout = apply_dropout\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.up_conv = tf.keras.layers.Conv2DTranspose(filters, \n                                                       (size, size), \n                                                       strides=2, \n                                                       padding='same',\n                                                       kernel_initializer=initializer,\n                                                       use_bias=False)\n        self.batchnorm = tf.keras.layers.BatchNormalization()\n        if self.apply_dropout:\n            self.dropout = tf.keras.layers.Dropout(0.5)\n\n    def call(self, x1, x2, training):\n        x = self.up_conv(x1)\n        x = self.batchnorm(x, training=training)\n        if self.apply_dropout:\n            x = self.dropout(x, training=training)\n        x = tf.nn.relu(x)\n        x = tf.concat([x, x2], axis=-1)\n        return x\n\n\nclass Generator(tf.keras.Model):\n\n    def __init__(self):\n        super(Generator, self).__init__()\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.down1 = Downsample(64, 4, apply_batchnorm=False)\n        self.down2 = Downsample(128, 4)\n        self.down3 = Downsample(256, 4)\n        self.down4 = Downsample(512, 4)\n        self.down5 = Downsample(512, 4)\n        self.down6 = Downsample(512, 4)\n        self.down7 = Downsample(512, 4)\n        self.down8 = Downsample(512, 4)\n\n        self.up1 = Upsample(512, 4, apply_dropout=True)\n        self.up2 = Upsample(512, 4, apply_dropout=True)\n        self.up3 = Upsample(512, 4, apply_dropout=True)\n        self.up4 = Upsample(512, 4)\n        self.up5 = Upsample(256, 4)\n        self.up6 = Upsample(128, 4)\n        self.up7 = Upsample(64, 4)\n\n        self.last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, \n                                                    (4, 4), \n                                                    strides=2, \n                                                    padding='same',\n                                                    kernel_initializer=initializer)\n    @tf.contrib.eager.defun\n    def call(self, x, training):\n        # x shape == (bs, 256, 256, 3)    \n        x1 = self.down1(x, training=training) # (bs, 128, 128, 64)\n        x2 = self.down2(x1, training=training) # (bs, 64, 64, 128)\n        x3 = self.down3(x2, training=training) # (bs, 32, 32, 256)\n        x4 = self.down4(x3, training=training) # (bs, 16, 16, 512)\n        x5 = self.down5(x4, training=training) # (bs, 8, 8, 512)\n        x6 = self.down6(x5, training=training) # (bs, 4, 4, 512)\n        x7 = self.down7(x6, training=training) # (bs, 2, 2, 512)\n        x8 = self.down8(x7, training=training) # (bs, 1, 1, 512)\n\n        x9 = self.up1(x8, x7, training=training) # (bs, 2, 2, 1024)\n        x10 = self.up2(x9, x6, training=training) # (bs, 4, 4, 1024)\n        x11 = self.up3(x10, x5, training=training) # (bs, 8, 8, 1024)\n        x12 = self.up4(x11, x4, training=training) # (bs, 16, 16, 1024)\n        x13 = self.up5(x12, x3, training=training) # (bs, 32, 32, 512)\n        x14 = self.up6(x13, x2, training=training) # (bs, 64, 64, 256)\n        x15 = self.up7(x14, x1, training=training) # (bs, 128, 128, 128)\n\n        x16 = self.last(x15) # (bs, 256, 256, 3)\n        x16 = tf.nn.tanh(x16)\n\n        return x16","45ac7ff1":"class DiscDownsample(tf.keras.Model):\n    \n    def __init__(self, filters, size, apply_batchnorm=True):\n        super(DiscDownsample, self).__init__()\n        self.apply_batchnorm = apply_batchnorm\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.conv1 = tf.keras.layers.Conv2D(filters, \n                                            (size, size), \n                                            strides=2, \n                                            padding='same',\n                                            kernel_initializer=initializer,\n                                            use_bias=False)\n        if self.apply_batchnorm:\n            self.batchnorm = tf.keras.layers.BatchNormalization()\n\n    def call(self, x, training):\n        x = self.conv1(x)\n        if self.apply_batchnorm:\n            x = self.batchnorm(x, training=training)\n        x = tf.nn.leaky_relu(x)\n        return x \n\nclass Discriminator(tf.keras.Model):\n    \n    def __init__(self):\n        super(Discriminator, self).__init__()\n        initializer = tf.random_normal_initializer(0., 0.02)\n\n        self.down1 = DiscDownsample(64, 4, False)\n        self.down2 = DiscDownsample(128, 4)\n        self.down3 = DiscDownsample(256, 4)\n\n        # we are zero padding here with 1 because we need our shape to \n        # go from (batch_size, 32, 32, 256) to (batch_size, 31, 31, 512)\n        self.zero_pad1 = tf.keras.layers.ZeroPadding2D()\n        self.conv = tf.keras.layers.Conv2D(512, \n                                           (4, 4), \n                                           strides=1, \n                                           kernel_initializer=initializer, \n                                           use_bias=False)\n        self.batchnorm1 = tf.keras.layers.BatchNormalization()\n\n        # shape change from (batch_size, 31, 31, 512) to (batch_size, 30, 30, 1)\n        self.zero_pad2 = tf.keras.layers.ZeroPadding2D()\n        self.last = tf.keras.layers.Conv2D(1, \n                                           (4, 4), \n                                           strides=1,\n                                           kernel_initializer=initializer)\n    \n    @tf.contrib.eager.defun\n    def call(self, inp, tar, training):\n        # concatenating the input and the target\n        x = tf.concat([inp, tar], axis=-1) # (bs, 256, 256, channels*2)\n        x = self.down1(x, training=training) # (bs, 128, 128, 64)\n        x = self.down2(x, training=training) # (bs, 64, 64, 128)\n        x = self.down3(x, training=training) # (bs, 32, 32, 256)\n\n        x = self.zero_pad1(x) # (bs, 34, 34, 256)\n        x = self.conv(x)      # (bs, 31, 31, 512)\n        x = self.batchnorm1(x, training=training)\n        x = tf.nn.leaky_relu(x)\n\n        x = self.zero_pad2(x) # (bs, 33, 33, 512)\n        # don't add a sigmoid activation here since\n        # the loss function expects raw logits.\n        x = self.last(x)      # (bs, 30, 30, 1)\n\n        return x","e8cacbec":"# The call function of Generator and Discriminator have been decorated\n# with tf.contrib.eager.defun()\n# We get a performance speedup if defun is used (~25 seconds per epoch)\ngenerator = Generator()\ndiscriminator = Discriminator()","0ff48a7d":"LAMBDA = 100","f720ba4f":"def discriminator_loss(disc_real_output, disc_generated_output):\n    real_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_real_output), \n                                              logits = disc_real_output)\n    generated_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.zeros_like(disc_generated_output), \n                                                   logits = disc_generated_output)\n\n    total_disc_loss = real_loss + generated_loss\n\n    return total_disc_loss","1e865a65":"def generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = tf.losses.sigmoid_cross_entropy(multi_class_labels = tf.ones_like(disc_generated_output),\n                                             logits = disc_generated_output) \n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss","b7ae5608":"generator_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0.5)\ndiscriminator_optimizer = tf.train.AdamOptimizer(2e-4, beta1=0.5)","8ce0ccd0":"!mkdir ..\/ckpt\n!ls ..\ncheckpoint_dir = '..\/ckpt'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","4e4c7385":"# Original version by Thomas Pappas used 450 epochs,\n# but only using 200 here to allow it to run in the\n# time Kaggle allots for kernels\nEPOCHS = 200\nSAVE_EVERY = 20","e3ed8950":"def generate_images(model, test_input, tar):\n    # the training=True is intentional here since\n    # we want the batch statistics while running the model\n    # on the test dataset. If we use training=False, we will get \n    # the accumulated statistics learned from the training dataset\n    # (which we don't want)\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15,15))\n\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","0a724a0c":"def train(dataset, epochs, save_every, ckpt=None):\n    if ckpt:\n        checkpoint.restore(ckpt)\n    for epoch in range(epochs):\n        start = time.time()\n        for input_image, target in dataset:\n            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n                gen_output = generator(input_image, training=True)\n\n                disc_real_output = discriminator(input_image, target, training=True)\n                disc_generated_output = discriminator(input_image, gen_output, training=True)\n\n                gen_loss = generator_loss(disc_generated_output, gen_output, target)\n                disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n            generator_gradients = gen_tape.gradient(gen_loss,\n                                                    generator.variables)\n            discriminator_gradients = disc_tape.gradient(disc_loss,\n                                                         discriminator.variables)\n\n            generator_optimizer.apply_gradients(zip(generator_gradients,\n                                                    generator.variables))\n            discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                                    discriminator.variables))\n\n        # generate test output every save_every epochs\n        if epoch % save_every == 0:\n            clear_output(wait=True)\n            for inp, tar in test_dataset.take(1):\n                generate_images(generator, inp, tar)\n\n        # saving (checkpoint) the model every save_every epochs\n        if (epoch + 1) % save_every == 0:\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                            time.time()-start))","2801cd40":"train(train_dataset, EPOCHS, SAVE_EVERY, ckpt=tf.train.latest_checkpoint(checkpoint_dir))","2fc75e03":"# restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","b12a7873":"# Run the trained model on some of the test dataset\n# from tensorflow import random\nN_TEST_SAMPLES = 10\nfor inp, tar in test_dataset.take(N_TEST_SAMPLES):\n    generate_images(generator, inp, tar)","72f6eaab":"## Import TensorFlow and related","06e7322d":"## Write the generator and discriminator models\n    ","d7e95384":"## Have a look at the images from the training set","3a7076c5":"# Map2Terrain\nInspired by https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/7dwj1q\/p_fun_project_mspaint_to_terrain_map_with_gan\/ , and using the same dataset, trains Pix2Pix Model using code adapted from https:\/\/research.google.com\/seedbank\/seed\/5762637883244544\n\nGenerates plausible terrains from a 5-colour terrain map.","c0f4a167":"## Choose which images will be part of the test\/training\/validation sets","35d71262":"## Now we get to the Pix2Pix code!","0370365d":"## Define the loss functions and the optimizer","e0251b67":"## Training","a9456d27":"## Testing latest checkpoint on examples from the test dataset","64cbd7c6":"## Checkpoints (Object-based saving)","0a415706":"## Check that GPU is available in VM and that TensorFlow knows about it","2a1cba35":"## Display images inline in Pandas DataFrame\nfrom https:\/\/www.kaggle.com\/stassl\/displaying-inline-images-in-pandas-dataframe","312f13f2":"## Use tf.data to create batches, map(do preprocessing) and shuffle the dataset","5305786a":"## Set up Pandas DataFrame with filenames of data, images for convenience"}}