{"cell_type":{"49b09913":"code","269e134b":"code","db3776c7":"code","fd0abc6b":"code","74fe6679":"code","c84e9076":"code","7c9f2034":"code","fea16f8d":"code","1efae84d":"code","628190ef":"code","854512a1":"code","48e51f13":"code","e2276c31":"code","90b07d77":"code","491ace2f":"code","314f2ed8":"code","374e521d":"code","8af38685":"code","5cacc1f5":"markdown","d2892a00":"markdown","7981ad28":"markdown","dad258c5":"markdown","49322bfd":"markdown","feb75e24":"markdown","8a7c2f47":"markdown","3dbb38c1":"markdown","9e86d74b":"markdown","89b89497":"markdown","743793cb":"markdown","e0b7a3ac":"markdown","650fba69":"markdown","397151d5":"markdown","5f56f119":"markdown"},"source":{"49b09913":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nimport scipy.optimize as opt\nimport os","269e134b":"train = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-2\/test.csv\")\ntrain.head()","db3776c7":"train_ = train[train[\"ConfirmedCases\"] >= 0]\ntrain_.head()","fd0abc6b":"EMPTY_VAL = \"EMPTY_VAL\"\n\ndef fillState(state, country):\n    if state == EMPTY_VAL: return country\n    return state\n\ntrain_['Province_State'].fillna(EMPTY_VAL, inplace=True)\ntrain_['Province_State'] = train_.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntest['Province_State'].fillna(EMPTY_VAL, inplace=True)\ntest['Province_State'] = test.loc[:, ['Province_State', 'Country_Region']].apply(lambda x : fillState(x['Province_State'], x['Country_Region']), axis=1)\ntest.head()","74fe6679":"train_['row_number'] = train_.groupby(['Country_Region', 'Province_State']).cumcount()\nx = train_[train_[\"Country_Region\"] == 'China'][train_[\"Province_State\"] == 'Hubei']['row_number']\ny = train_[train_[\"Country_Region\"] == 'China'][train_[\"Province_State\"] == 'Hubei']['ConfirmedCases']\ny_ = train_[train_[\"Country_Region\"] == 'China'][train_[\"Province_State\"] == 'Hubei']['Fatalities']\n\ndef f(x, L, b, k, x_0):\n    return L \/ (1. + np.exp(-k * (x - x_0))) + b\n\n\ndef logistic(xs, L, k, x_0):\n    result = []\n    for x in xs:\n        xp = k*(x-x_0)\n        if xp >= 0:\n            result.append(L \/ ( 1. + np.exp(-xp) ) )\n        else:\n            result.append(L * np.exp(xp) \/ ( 1. + np.exp(xp) ) )\n    return result\n\np0 = [max(y), 0.0,max(x)]\np0_ = [max(y_), 0.0,max(x)]\nx_ = np.arange(0, 100, 1).tolist()\ntry:\n    popt, pcov = opt.curve_fit(logistic, x, y,p0)\n    yfit = logistic(x_, *popt)\n    popt_, pcov_ = opt.curve_fit(logistic, x, y_,p0_)\n    yfit_ = logistic(x_, *popt_)\nexcept:\n    popt, pcov = opt.curve_fit(f, x, y, method=\"lm\", maxfev=5000)\n    yfit = f(x_, *popt)\n    popt_, pcov_ = opt.curve_fit(f, x, y_, method=\"lm\", maxfev=5000)\n    yfit_ = f(x_, *popt_)\n    #print(\"problem\")\n\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nax.plot(x, y, 'o', label ='Actual Cases')\nax.plot(x_, yfit, '-', label ='Fitted Cases')\n\nax.plot(x, y_, 'o', label ='Actual Fatalities')\nax.plot(x_, yfit_, '-', label ='Fitted fatalities')\nax.title.set_text('China - Hubei province')\nplt.legend(loc=\"center right\")\nplt.show()","c84e9076":"unique = pd.DataFrame(train_.groupby(['Country_Region', 'Province_State'],as_index=False).count())\nunique.head()","7c9f2034":"import datetime as dt\n\ndef date_day_diff(d1, d2):\n    delta = dt.datetime.strptime(d1, \"%Y-%m-%d\") - dt.datetime.strptime(d2, \"%Y-%m-%d\")\n    return delta.days\n\nlog_regions = []\n\nfor index, region in unique.iterrows():\n    st = region['Province_State']\n    co = region['Country_Region']\n    \n    rdata = train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]\n\n    t = rdata['Date'].values\n    t = [float(date_day_diff(d, t[0])) for d in t]\n    y = rdata['ConfirmedCases'].values\n    y_ = rdata['Fatalities'].values\n\n    p0 = [max(y), 0.0, max(t)]\n    p0_ = [max(y_), 0.0, max(t)]\n    try:\n        popt, pcov = opt.curve_fit(logistic, t, y, p0, maxfev=10000)\n        try:\n            popt_, pcov_ = opt.curve_fit(logistic, t, y_, p0_, maxfev=10000)\n        except:\n            popt_, pcov_ = opt.curve_fit(f, t, y_,method=\"trf\", maxfev=10000)\n        log_regions.append((co,st,popt,popt_))\n    except:\n        popt, pcov = opt.curve_fit(f, t, y,method=\"trf\", maxfev=10000)\n        popt_, pcov_ = opt.curve_fit(f, t, y_,method=\"trf\", maxfev=10000)\n        log_regions.append((co,st,popt,popt_))\n\nprint(\"All done!\")","fea16f8d":"log_regions = pd.DataFrame(log_regions)\nlog_regions.head()","1efae84d":"log_regions.columns = ['Country_Region','Province_State','ConfirmedCases','Fatalities']\nlog_regions.head(1)","628190ef":"data = log_regions['ConfirmedCases'].str[1]\nbins = np.arange(0, 10, 0.01)\nplt.hist(data,bins=bins, alpha=0.5)\nplt.xlim([0,1])\nplt.ylabel('count')\nplt.show()\nlog_regions['ConfirmedCases'].str[1].quantile([.1, .25, .5, .75, .95, .99])","854512a1":"#log_regions.loc[log_regions['ConfirmedCases'].str[1] > 1.0, 'ConfirmedCases'] = 0.267766","48e51f13":"log_regions['ConfirmedCases'].str[1].quantile([.1, .25, .5, .75, .95, .99])","e2276c31":"T = np.arange(0, 100, 1).tolist()\npopt = list(log_regions[log_regions[\"Country_Region\"] == 'Italy'][log_regions[\"Province_State\"] == 'Italy']['ConfirmedCases'])[0]\npopt_ = list(log_regions[log_regions[\"Country_Region\"] == 'Italy'][log_regions[\"Province_State\"] == 'Italy']['Fatalities'])[0]\n\ntry:\n    yfit = logistic(T, *popt)\n    yfit_ = logistic(T, *popt_)\nexcept:\n    yfit = f(T, *popt)\n    yfit_ = f(T, *popt_)\n    \n\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\nax.plot(T, yfit, label=\"Fitted ConfirmedCases\")\nax.plot(T, yfit_, label=\"Fitted Fatalities\")\nax.title.set_text('Italy fitted params')\nplt.legend(loc=\"upper left\")\nplt.show()","90b07d77":"for index, rt in log_regions.iterrows():\n    st = rt['Province_State']\n    co = rt['Country_Region']\n    popt = list(['ConfirmedCases'])\n    popt_ = list(rt['Fatalities'])\n    print(co,st,popt,popt_)","491ace2f":"data0 = log_regions['Fatalities'].str[0]\/log_regions['ConfirmedCases'].str[0]\ndata1 = log_regions['Fatalities'].str[1]\/log_regions['ConfirmedCases'].str[1]\ndata2 = log_regions['Fatalities'].str[2]\/log_regions['ConfirmedCases'].str[2]\nbins = np.arange(0, 3, 0.01)\nplt.hist(data,bins=bins, alpha=0.5)\nplt.xlim([0,3])\nplt.ylabel('count')\nplt.ylim([0,5])\nplt.show()\nfp = np.array([data0.median(),data1.median(),data2.median()])\nfp","314f2ed8":"for index, rt in log_regions.iterrows():\n    st = rt['Province_State']\n    co = rt['Country_Region']\n    popt = list(rt['ConfirmedCases'])\n    popt_ = list(rt['Fatalities'])\n    \n    if popt_ == [0.0,0.0,69.0]:\n        popt_ = np.multiply(fp,popt)\n        print(co,st,popt,popt_)","374e521d":"submission = []\n\nfor index, rt in log_regions.iterrows():\n    st = rt['Province_State']\n    co = rt['Country_Region']\n    popt = list(rt['ConfirmedCases'])\n    popt_ = list(rt['Fatalities'])\n    if popt_ == [0.0,0.0,69.0]:\n        popt_ = np.multiply(fp,popt)\n    print(co,st,popt,popt_)\n    rtest = test[(test['Province_State']==st) & (test['Country_Region']==co)]\n    for index, rt in rtest.iterrows():\n        try:\n            tdate = rt['Date']\n            ca = logistic([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt)\n            try:\n                fa = logistic([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt_)\n            except:\n                fa = f([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt_)\n            submission.append((rt['ForecastId'], int(ca[0]), int(fa[0])))\n        except:\n            tdate = rt['Date']\n            ca = f([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt)\n            fa = f([date_day_diff(tdate, min(train_[(train_['Province_State']==st) & (train_['Country_Region']==co)]['Date'].values))], *popt_)\n            submission.append((rt['ForecastId'], int(ca[0]), int(fa[0])))\n\nprint(\"All done!\") ","8af38685":"submission = pd.DataFrame(submission)\nsubmission.columns = ['ForecastId','ConfirmedCases','Fatalities']\nsubmission.to_csv('.\/submission.csv', index = False)\nprint(\"submission ready!\")","5cacc1f5":"Lets try replace parameters for the locations where there havent been any fatalities so far, leaning on the notion that fatalities follow cases according to observation.\nwhat we will do is work out the median ratios of the parameters and apply them to the confirmed cases parameters.","d2892a00":"Filter out the 0's, countries are affected differently chronologically based on geography, holidaymakers, season etc... I could be wrong though, I mostly have online gambling experience, no epidemiological experience lol!\nedit: added them back because that produces a better score. Guess thats empirical testing for you.","7981ad28":"Convert it to a dataframe:","dad258c5":"**What is the Math behind COVID-19?**\nThe curve used to model this situation is called a logistic curve, an S-shaped curve that describes population growth of both viruses and people, as well as other phenomena in economics and science.\n![curve](https:\/\/www.nctm.org\/uploadedImages\/Content\/General\/pandemics\/Pandemic_curve.jpg)\nWe will fit this simple curve to our virus data (as provided by Kaggle) and see how well it performs on the leaderboard.","49322bfd":"Let us replace the empty Province_State cases with the Country_Region:","feb75e24":"Public Score\n0.19234\n\nRank 66\/220 as at 12:45 SAST Sunday, 29 March 2020.\n\nNot too bad for a simple approach. Let me know if you have any suggestions and how you fare!\n\nT","8a7c2f47":"see it in action first:","3dbb38c1":"Now we are ready to iterate through the trianing data and fit curves on the data:","9e86d74b":"Seems reasonable to me anyway. Go ahead and test a few. There are one or two that come back erroneous but for the most part its pretty good at it.\nNow lets create a dimensional lookup(LUP) of the composite (Country_Region,Province_State):","89b89497":"Wondering if i should curbe outliers here:","743793cb":"Now we iterate our fits through the test data for submission:","e0b7a3ac":"The logistic curve \ud835\udc53(\ud835\udc65)=\ud835\udc3f\/(1+\ud835\udc52\u2212\ud835\udc58(\ud835\udc65\u2212\ud835\udc650)) is what we will use.\n\nWe will also use another variation of the function: \ud835\udc53\ud835\udc3f,b,\ud835\udc58,\ud835\udc650(\ud835\udc65) = \ud835\udc3f\/(1+\ud835\udc52\u2212\ud835\udc58(\ud835\udc65\u2212\ud835\udc650))+b, depending on which works for the data.\n\nSome more commentary on scipy:\n\nIn SciPy, nonlinear least squares curve fitting works by minimizing the following cost function:\n\nS(\u03b2)=\u2211i=1n(yi\u2212f\u03b2(xi))2\n\nHere, \u03b2 is the vector of parameters (in our example, \u03b2=(L,b,k,x0).\n\nNonlinear least squares is really similar to linear least squares for linear regression. Whereas the function f is linear in the parameters with the linear least squares method, it is not linear here. Therefore, the minimization of S(\u03b2) cannot be done analytically by solving the derivative of S with respect to \u03b2. SciPy implements an iterative method called the Levenberg-Marquardt algorithm (an extension of the Gauss-Newton algorithm).\n\nLet us test the functions out:","650fba69":"Let us test graphing a curve from the fitted parameters:","397151d5":"Prepare results for submission:","5f56f119":"Rename columns:"}}