{"cell_type":{"fc1369c1":"code","f8dbfe59":"code","a3e67316":"code","bf73a582":"code","cd1cc7e6":"code","9d51c92f":"code","2d1cf364":"code","108c11c4":"code","7795d346":"code","8c8b1366":"code","dc8a5e50":"code","ab18ea3c":"code","455f098f":"code","7e58b906":"code","00349df4":"code","c6239ece":"code","083a84d9":"code","3af02a0b":"code","d5f8abd4":"code","e38c0158":"code","4bf86cd5":"code","31efb5d4":"code","44ee7fc0":"code","a374a5e7":"code","ab00c624":"code","56c0257f":"code","c1418e0e":"code","2c056831":"code","8d6a056a":"code","28ff5ad3":"code","86e86946":"code","0d0082a8":"code","a5345f43":"code","6a442467":"code","498148e0":"code","8f7f65be":"code","50868375":"code","d4b37bda":"code","5faa83c9":"code","12f44c68":"code","57bf4547":"code","57fff696":"code","da8e9f3e":"code","5deca77c":"code","291967b1":"code","cdb6d8e7":"code","9a114547":"code","9dd4b416":"code","abf2b1ad":"code","18d28317":"code","8577e361":"code","e5687267":"code","98c01575":"code","78588901":"code","37cc2812":"code","d8c909fb":"code","2aaf3a3d":"code","3b5205e7":"code","842422ab":"code","42e5bcbf":"code","a2e42a58":"code","7594ff6f":"code","91a79549":"code","ec9e7960":"code","94ab7351":"code","df13a511":"code","dec33ed1":"markdown","48bd802b":"markdown","201074e7":"markdown","e45f11d9":"markdown"},"source":{"fc1369c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('max_columns', None)\npd.set_option('max_rows', None)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8dbfe59":"import warnings\nwarnings.filterwarnings('ignore')\nfrom ast import literal_eval\nimport plotly.express as px\nimport nltk\nimport re","a3e67316":"meta_data = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/movies_metadata.csv\")\nlink_small = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/links_small.csv\")\nkeywords = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/keywords.csv\")\ncredits = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/credits.csv\")\nratings_small = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/ratings_small.csv\")\nmeta_data.head()","bf73a582":"meta_data['genres'] = meta_data['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n","cd1cc7e6":"languages = meta_data[\"original_language\"].value_counts()\nlangues_df = pd.DataFrame({'languages':languages.index, 'frequency':languages.values}).head(10)\n\nfig = px.bar(langues_df, x=\"frequency\", y=\"languages\",color='languages', orientation='h',\n             hover_data=[\"languages\", \"frequency\"],\n             height=1000,\n             title='Language which has more Movies')\nfig.show()\n","9d51c92f":"top_movies = meta_data[[\"title\",\"vote_count\"]]\ntop_movies = top_movies.sort_values(by=\"vote_count\",ascending=False)","2d1cf364":"fig = px.bar(data_frame=top_movies[:20],x=\"title\",y=\"vote_count\",color=\"title\",title=\"Most Voted Movies\")\nfig.show()","108c11c4":"s = meta_data.apply(lambda x: pd.Series(x['genres']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'genre'\ngen_md = meta_data.drop('genres', axis=1).join(s)\n","7795d346":"genre_counts = gen_md.genre.value_counts()\ngenre_df = pd.DataFrame({'genre':genre_counts.index,\"count\":genre_counts.values})\nfig = px.bar(data_frame=genre_df[:20],x=\"genre\",y=\"count\",color=\"genre\")\nfig.show()","8c8b1366":"adults_count = meta_data['adult'].value_counts()\nadults_df = pd.DataFrame({\"adults\":adults_count.index,\"count\":adults_count.values})\nfig = px.bar(data_frame=adults_df[:2],x=\"adults\",y=\"count\",color=\"adults\")\nfig.show()","dc8a5e50":"budget_analysis = meta_data.sort_values(by=\"budget\",ascending=False)\nbudget_analysis[[\"budget\",'title']].head(10)","ab18ea3c":"credits = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/credits.csv\")\ncredits.head()","455f098f":"links_small = pd.read_csv('\/kaggle\/input\/the-movies-dataset\/links_small.csv')\nlinks_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')","7e58b906":"ratings = pd.read_csv(\"\/kaggle\/input\/the-movies-dataset\/ratings.csv\")\nratings.head()","00349df4":"link_small = link_small[link_small['tmdbId'].notnull()]['tmdbId'].astype('int')\nmeta_data = meta_data.drop([19730, 29503, 35587])","c6239ece":"meta_data['id'] = meta_data['id'].astype('int')","083a84d9":"smd = meta_data[meta_data['id'].isin(links_small)]\nsmd.shape","3af02a0b":"smd['tagline'] = smd['tagline'].fillna('')\nsmd['description'] = smd['overview']\nsmd['description'] = smd['description'].fillna('')\n","d5f8abd4":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n# from surprise import Reader, Dataset, SVD, evaluate","e38c0158":"smd.overview.head()","4bf86cd5":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(smd['description'])\nprint(tfidf_matrix.shape)","31efb5d4":"cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","44ee7fc0":"smd = smd.reset_index()\ntitles = smd['title']\nindices = pd.Series(smd.index, index=smd['title'])","a374a5e7":"indices.head()","ab00c624":"def get_recommendations(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:11]\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]","56c0257f":"get_recommendations(\"The Godfather\").head(10)","c1418e0e":"meta_data.head()","2c056831":"title_df = meta_data[[\"overview\",\"genres\",\"title\"]]","8d6a056a":"title_df.head()","28ff5ad3":"punctuation = \"\"\"!()-[]{};:'\"\\, <>.\/?@#$%^&*_~\"\"\"\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nwords = stopwords.words(\"english\")\nlemma = nltk.stem.WordNetLemmatizer()\ndef pre_process(text):\n    text = str(text)\n    remove_hyperlink = re.sub('http:\/\/\\S+|https:\/\/\\S+', '', text)\n    for elements in remove_hyperlink:\n        if elements in punctuation:\n            remove_hyperlink = remove_hyperlink.replace(elements, \" \")\n    tokens = word_tokenize(remove_hyperlink)\n    remove_words = [word for word in tokens if not word in words]\n    text = [lemma.lemmatize(word) for word in remove_words]\n    joined_words = \" \".join(text)\n    return joined_words","86e86946":"title_df['title'] = title_df['title'].apply(pre_process)","0d0082a8":"title_df.head()","a5345f43":"from sklearn.preprocessing import MultiLabelBinarizer\n\nmultilabel_binarizer = MultiLabelBinarizer()\nmultilabel_binarizer.fit(title_df['genres'])\n\n# transform target variable\ny = multilabel_binarizer.transform(title_df['genres'])\n\n","6a442467":"tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=5000)","498148e0":"from sklearn import model_selection\nxtrain, xval, ytrain, yval = model_selection.train_test_split(title_df['title'], y, test_size=0.2, random_state=9)","8f7f65be":"xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\nxval_tfidf = tfidf_vectorizer.transform(xval)","50868375":"from sklearn.linear_model import LogisticRegression\n\n# Binary Relevance\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Performance metric\nfrom sklearn.metrics import f1_score","d4b37bda":"lr = LogisticRegression()\nclf = OneVsRestClassifier(lr)\n","5faa83c9":"clf.fit(xtrain_tfidf, ytrain)","12f44c68":"y_pred = clf.predict(xval_tfidf)","57bf4547":"y_pred[3]","57fff696":"multilabel_binarizer.inverse_transform(y_pred)[3]","da8e9f3e":"f1_score(yval, y_pred, average=\"micro\")","5deca77c":"y_pred_prob = clf.predict_proba(xval_tfidf)","291967b1":"t = 0.3 # threshold value\ny_pred_new = (y_pred_prob >= t).astype(int)","cdb6d8e7":"f1_score(yval, y_pred_new, average=\"micro\")","9a114547":"def inference(text):\n    processed_text = pre_process(text)\n    q_vec = tfidf_vectorizer.transform([processed_text])\n    q_pred = clf.predict(q_vec)\n    q_pred = clf.predict(q_vec)\n    return multilabel_binarizer.inverse_transform(q_pred)","9dd4b416":"for i in range(5):\n    k = xval.sample(1).index[0]\n    print(\"Movie: \", title_df['title'][k], \"\\nPredicted genre: \", inference(xval[k])), print(\"Actual genre: \",title_df['genres'][k], \"\\n\")","abf2b1ad":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport string\nimport unidecode\nimport random\nimport torch","18d28317":"# Check if GPU is available\ntrain_on_gpu = torch.cuda.is_available()\nif(train_on_gpu):\n    print('Training on GPU!')\nelse: \n    print('No GPU available, training on CPU; consider making n_epochs very small')","8577e361":"meta_data.head()","e5687267":"content = meta_data[\"overview\"]\ncontent[:5]","98c01575":"content.shape","78588901":"all_text = list(content)\ndef joinStrings(text):\n    text = str(text)\n    return ''.join(string for string in text)\ntext = joinStrings(all_text[:100])\n# text = [item for sublist in author[:5].values for item in sublist]\nlen(text.split())","37cc2812":"test_sentence = pre_process(text).split()","d8c909fb":"test_sentence","2aaf3a3d":"trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])\n            for i in range(len(test_sentence) - 2)]\nchunk_len=len(trigrams)\nprint(trigrams[:3])","3b5205e7":"\n\nvocab = set(test_sentence)\nvoc_len=len(vocab)\nword_to_ix = {word: i for i, word in enumerate(vocab)}\n\n","842422ab":"inp=[]\ntar=[]\nfor context, target in trigrams:\n        context_idxs = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n        inp.append(context_idxs)\n        targ = torch.tensor([word_to_ix[target]], dtype=torch.long)\n        tar.append(targ)","42e5bcbf":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        \n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size*2, hidden_size, n_layers,batch_first=True,\n                          bidirectional=False)\n        self.decoder = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, input, hidden):\n        input = self.encoder(input.view(1, -1))\n        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n        output = self.decoder(output.view(1, -1))\n        return output, hidden\n\n    def init_hidden(self):\n        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))","a2e42a58":"def train(inp, target):\n    hidden = decoder.init_hidden().cuda()\n    decoder.zero_grad()\n    loss = 0\n    \n    for c in range(chunk_len):\n        output, hidden = decoder(inp[c].cuda(), hidden)\n        loss += criterion(output, target[c].cuda())\n\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data.item() \/ chunk_len\n","7594ff6f":"\n\nimport time, math\n\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n","91a79549":"n_epochs = 300\nprint_every = 100\nplot_every = 10\nhidden_size = 100\nn_layers = 1\nlr = 0.015\n\ndecoder = RNN(voc_len, hidden_size, voc_len, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses = []\nloss_avg = 0\nif(train_on_gpu):\n    decoder.cuda()\nfor epoch in range(1, n_epochs + 1):\n    loss = train(inp,tar)       \n    loss_avg += loss\n\n    if epoch % print_every == 0:\n        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch \/ n_epochs * 50, loss))\n#         print(evaluate('ge', 200), '\\n')\n\n    if epoch % plot_every == 0:\n        all_losses.append(loss_avg \/ plot_every)\n        loss_avg = 0","ec9e7960":"import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nplt.figure()\nplt.plot(all_losses)","94ab7351":"\n\ndef evaluate(prime_str='this process', predict_len=100, temperature=0.8):\n    hidden = decoder.init_hidden().cuda()\n\n    for p in range(predict_len):\n        \n        prime_input = torch.tensor([word_to_ix[w] for w in prime_str.split()], dtype=torch.long).cuda()\n        inp = prime_input[-2:] #last two words as input\n        output, hidden = decoder(inp, hidden)\n        \n        # Sample from the network as a multinomial distribution\n        output_dist = output.data.view(-1).div(temperature).exp()\n        top_i = torch.multinomial(output_dist, 1)[0]\n        \n        # Add predicted word to string and use as next input\n        predicted_word = list(word_to_ix.keys())[list(word_to_ix.values()).index(top_i)]\n        prime_str += \" \" + predicted_word\n#         inp = torch.tensor(word_to_ix[predicted_word], dtype=torch.long)\n\n    return prime_str\n\n","df13a511":"print(evaluate('this process', 40, temperature=1))","dec33ed1":"# Genre Classification - MultiLabel Classification","48bd802b":"While our system has done a decent job of finding movies with similar plot descriptions, the quality of recommendations is not that great. \"The Dark Knight Rises\" returns all Batman movies while it is more likely that the people who liked that movie are more inclined to enjoy other Christopher Nolan movies. This is something that cannot be captured by the present system","201074e7":"Since we have used the TF-IDF vectorizer, calculating the dot product will directly give us the cosine similarity score. Therefore, we will use sklearn's linear_kernel() instead of cosine_similarities() since it is faster.","e45f11d9":"# Text Generation using Pytorch"}}