{"cell_type":{"df04cbfd":"code","8fcbd344":"code","477a11d5":"code","6772522d":"code","90174671":"code","0916cab0":"code","b74abef1":"code","a3884292":"code","a85ad664":"code","58fa8a25":"code","d783e7f3":"code","6090a61c":"code","56e7f4e1":"code","70579b15":"code","66867445":"code","5a2af574":"code","4d450b69":"code","43d0a612":"code","2f27b5f8":"code","7e5ba7bd":"code","1dc0232f":"code","2b3eb11f":"markdown","11246ffc":"markdown","9aa0e32e":"markdown","263395a1":"markdown"},"source":{"df04cbfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8fcbd344":"from sklearn import datasets\n\niris = datasets.load_iris()\nX = iris.data[:,[2,3]]\nY = iris.target","477a11d5":"plt.figure(2, figsize=(8, 6))\nplt.clf() #clear figure\n\n# Plot the training points\nplt.scatter(X[:, 0], X[:, 1], c=Y)\nplt.xlabel('Petal length')\nplt.ylabel('Petal width')","6772522d":"from sklearn.model_selection import train_test_split\n\n","90174671":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,random_state=0)\n\nlength_Train = len(X_train)\nlength_Test = len(X_test)\n\nprint(\"There are \",length_Train,\"samples in the trainig set and\",length_Test,\"samples in the test set\")\nprint(\"-----------------------------------------------------------------------------------------------\")\nprint(\"\")\n\n## 2. Feature scaling.\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_standard = sc.transform(X_train)\nX_test_standard = sc.transform(X_test)\n\nprint(\"X_train without standardising features\")\nprint(\"--------------------------------------\")\nprint(X_train[1:5,:])\nprint(\"\")\nprint(\"X_train standardising features\")\nprint(\"--------------------------------------\")\nprint(X_train_standard[1:5,:])","0916cab0":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(C = 1000.0, random_state = 0 )\nlr.fit(X_train_standard, Y_train)","b74abef1":"Y_pred_Logit = lr.predict(X_test_standard)\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy: %.2f\" % accuracy_score(Y_test,Y_pred_Logit))","a3884292":"from matplotlib.colors import ListedColormap\n\ndef plot_decision_regions(X,y,classifier,test_idx=None,resolution=0.02):\n    \n    # Initialise the marker types and colors\n    markers = ('s','x','o','^','v')\n    colors = ('red','blue','lightgreen','gray','cyan')\n    color_Map = ListedColormap(colors[:len(np.unique(y))]) #we take the color mapping correspoding to the \n                                                            #amount of classes in the target data\n    \n    # Parameters for the graph and decision surface\n    x1_min = X[:,0].min() - 1\n    x1_max = X[:,0].max() + 1\n    x2_min = X[:,1].min() - 1\n    x2_max = X[:,1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),\n                           np.arange(x2_min,x2_max,resolution))\n    \n    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    \n    plt.contour(xx1,xx2,Z,alpha=0.4,cmap = color_Map)\n    plt.xlim(xx1.min(),xx1.max())\n    plt.ylim(xx2.min(),xx2.max())\n    \n    # Plot samples\n    X_test, Y_test = X[test_idx,:], y[test_idx]\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x = X[y == cl, 0], y = X[y == cl, 1],\n                    alpha = 0.8, c = color_Map(idx),\n                    marker = markers[idx], label = cl\n                   )\nX_combined_standard = np.vstack((X_train_standard,X_test_standard))\nY_combined = np.hstack((Y_train, Y_test))\nplot_decision_regions(X = X_combined_standard\n                      , y = Y_combined\n                      , classifier = lr\n                      , test_idx = range(105,150))","a85ad664":"from sklearn.svm import SVC\n\nsvm = SVC(kernel = 'linear', C = 1.0, random_state = 0)\nsvm.fit(X_train_standard, Y_train)","58fa8a25":"Y_pred_SVM = svm.predict(X_test_standard)\n\nprint(\"Accuracy: %.2f\" % accuracy_score(Y_test,Y_pred_SVM))","d783e7f3":"plot_decision_regions(X = X_combined_standard\n                      , y = Y_combined\n                      , classifier = svm\n                      , test_idx = range(105,150))","6090a61c":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(criterion='entropy'\n                             , max_depth = 3\n                             , random_state = 0)\ntree.fit(X_train,Y_train)","56e7f4e1":"Y_pred_tree = tree.predict(X_test)\n\nprint(\"Accuracy: %.2f\" % accuracy_score(Y_test,Y_pred_tree))","70579b15":"X_combined = np.vstack((X_train, X_test))\nY_combined = np.hstack((Y_train, Y_test))\n\nplot_decision_regions(X = X_combined\n                      , y = Y_combined\n                      , classifier = tree\n                      , test_idx = range(105,150))","66867445":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(criterion = 'entropy'\n                                , n_estimators = 10\n                                , random_state = 1\n                                , n_jobs = 1)\n\nforest.fit(X_train, Y_train)","5a2af574":"Y_pred_RF = forest.predict(X_test)\n\nprint(\"Accuracy: %.2f\" % accuracy_score(Y_test,Y_pred_RF))","4d450b69":"plot_decision_regions(X = X_combined\n                      , y = Y_combined\n                      , classifier = forest\n                      , test_idx = range(105,150))","43d0a612":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5\n                          , p=2\n                          , metric = 'minkowski')\n\n\nknn.fit(X_train_standard,Y_train)","2f27b5f8":"Y_pred_KNN = knn.predict(X_test_standard)\n\nprint(\"Accuracy: %.2f\" % accuracy_score(Y_test,Y_pred_KNN))","7e5ba7bd":"plot_decision_regions(X = X_combined_standard\n                      , y = Y_combined\n                      , classifier = knn\n                      , test_idx = range(105,150))","1dc0232f":"plt.figure(figsize=(10, 10))\n\n\n\nplt.subplot(3,2,2)\nplot_decision_regions(X = X_combined_standard\n                      , y = Y_combined\n                      , classifier = lr\n                      , test_idx = range(105,150))\n\nplt.subplot(3,2,3)\nplot_decision_regions(X = X_combined_standard\n                      , y = Y_combined\n                      , classifier = svm\n                      , test_idx = range(105,150))\n\nplt.subplot(3,2,4)\nplot_decision_regions(X = X_combined\n                      , y = Y_combined\n                      , classifier = tree\n                      , test_idx = range(105,150))\n\nplt.subplot(3,2,5)\nplot_decision_regions(X = X_combined\n                      , y = Y_combined\n                      , classifier = forest\n                      , test_idx = range(105,150))\n\nplt.subplot(3,2,6)\nplot_decision_regions(X = X_combined_standard\n                      , y = Y_combined\n                      , classifier = knn\n                      , test_idx = range(105,150))","2b3eb11f":"From the above comparison K-NN (Nearest Neighbor (Last left Graph)) is simple to understand and very fast and efficient however, As we already know there is three number of neighbors as there are three different flowers. However, for some of the business problem number of neighbors are unknown and an assumption may lead to wrong result. \nSo, it is hard to know right at the start which algorithm will work best for the business problem. It is usually best to work iteratively amongst the available algorithms to get the performance of the algorithms and selecting the best one with regards to performance, accuracy and right balance of complexity. Each problem needs to have an awareness of demands, rules and regulations and stakeholders concern as well as considerable expertise. ","11246ffc":"Decision Tree Classification","9aa0e32e":"Random Forest","263395a1":"SVM"}}