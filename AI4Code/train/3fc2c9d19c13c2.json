{"cell_type":{"f7964628":"code","42e21abb":"code","58c93b9a":"code","239904e6":"code","83fa8e1b":"code","f6d413c3":"code","5fce506f":"code","ac07f2bf":"code","4c91e877":"code","43cf1626":"code","7f1a88af":"code","6e3703e3":"code","689300b3":"code","acfb0f53":"code","e567416f":"code","7c4b4528":"code","b1b52c44":"code","d6665ed4":"code","924340e4":"code","dcb8e82f":"code","fdf2f0e2":"code","037c3325":"code","53807ad1":"code","769e6763":"code","5e031706":"code","1a232f03":"code","96225031":"code","1d7d3e00":"code","68c1db88":"markdown","7717cde1":"markdown","e869e8dd":"markdown","24034ec3":"markdown","a7adeae2":"markdown","e116fb56":"markdown","87868b61":"markdown","dd380e6f":"markdown","b34f0018":"markdown","67c26624":"markdown","3271c460":"markdown","1ab5adb0":"markdown","e038b5a9":"markdown","94f79ed7":"markdown","2e34c701":"markdown","b294ba3b":"markdown","c37c5ecc":"markdown","74bf79f7":"markdown","edf4dcef":"markdown","c7912799":"markdown"},"source":{"f7964628":"import time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\nimport plotly.express as px\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate,\\\n    validation_curve, train_test_split\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\nfrom sklearn.metrics import make_scorer\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')","42e21abb":"train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\ndf = pd.concat([train, test], sort=False)","58c93b9a":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)","239904e6":"check_df(train)","83fa8e1b":"check_df(test)","f6d413c3":"df[[\"sales\"]].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99]).T","5fce506f":"# How many unique stores are there?\ndf[\"store\"].nunique()","ac07f2bf":"df[\"sales\"].sort_values(ascending=False).head()","4c91e877":"# Are there an equal number of unique items in each store?\ndf.groupby([\"store\"])[\"item\"].nunique()","43cf1626":"# Sales statistics in store breakdown\ndf.groupby(\"store\").agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","7f1a88af":"# 2D graphic to see seasonality\nsns.lineplot(x=\"date\",y=\"sales\", legend=\"full\",data=train)\nplt.show()","6e3703e3":"# Date Features\ndef create_date_features(dataframe):\n    dataframe['month'] = dataframe.date.dt.month\n    dataframe['day_of_month'] = dataframe.date.dt.day\n    dataframe['day_of_year'] = dataframe.date.dt.dayofyear\n    dataframe['week_of_year'] = dataframe.date.dt.weekofyear\n    dataframe['day_of_week'] = dataframe.date.dt.dayofweek\n    dataframe['year'] = dataframe.date.dt.year\n    dataframe[\"is_wknd\"] = dataframe.date.dt.weekday \/\/ 4\n    dataframe['is_month_start'] = dataframe.date.dt.is_month_start.astype(int)\n    dataframe['is_month_end'] = dataframe.date.dt.is_month_end.astype(int)\n    return df\ndf = create_date_features(df)\n\ncheck_df(df)","689300b3":"# Sales statistics in store-item-month \ndf.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","acfb0f53":"def random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))\n\na = np.random.normal(scale=1.6, size=(len(df)))\n\nsns.distplot(a)\nplt.show()","e567416f":"# The feature order here is important, first we set it as store, item, date.\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)","7c4b4528":"def lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n\ndf = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])\ndf.tail()","b1b52c44":"def roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n\ndf = roll_mean_features(df, [365, 546])\ndf.tail()","d6665ed4":"def ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\n\ndf.tail()","924340e4":"df = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","dcb8e82f":"# Converting sales to log(1+sales)\ndf['sales'] = np.log1p(df[\"sales\"].values)\ndf.head()","fdf2f0e2":"# MAE: mean absolute error\n# MAPE: mean absolute percentage error\n# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\n\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False","037c3325":"# Train set until the beginning of 2017 (until the end of 2016).\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# First three months of 2017 validation set.\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\n# Selecting the dependent and independent variable for the train set\nY_train = train['sales']\nX_train = train[cols]\n\n# Choosing the dependent and independent variable for the validation set\nY_val = val['sales']\nX_val = val[cols]","53807ad1":"Y_train.shape, Y_train.shape, Y_val.shape, X_val.shape","769e6763":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 1000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}\n\n\nlgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)\n\ny_pred_val = model.predict(X_val, num_iteration=model.best_iteration)","5e031706":"smape(np.expm1(y_pred_val), np.expm1(Y_val))","1a232f03":"def plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n\nplot_lgb_importances(model, num=30, plot=True)\nlgb.plot_importance(model, max_num_features=20, figsize=(10, 10), importance_type=\"gain\")\nplt.show()","96225031":"\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]\n\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}\n\n\n# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\n\nmodel = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)\n\ntest_preds = model.predict(X_test, num_iteration=model.best_iteration)","1d7d3e00":"submission_df = test.loc[:, ['id', 'sales']]\n\n# Undoing the standardization process.\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\n\nsubmission_df.to_csv('submission_demand.csv', index=False)\nsubmission_df.head(10)","68c1db88":"<a id = \"2\"><\/a><h1 id=\"Importing Libraries and Utilities\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Importing Libraries and Utilities<\/span><\/h1>","7717cde1":"## Custom Cost Function","e869e8dd":"## Rolling Mean Features ","24034ec3":"<a id = \"7\"><\/a><h1 id=\"Final Model\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Final Model<\/span><\/h1>","a7adeae2":"## Exponentially Weighted Mean Features","e116fb56":"<a id = \"3\"><\/a><h1 id=\"Load and Check Data\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Load and Check Data<\/span><\/h1>","87868b61":"## Time-Based Validation Sets","dd380e6f":"<a id = \"8\"><\/a><h1 id=\"Submission\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Submission<\/span><\/h1>","b34f0018":"## Lag\/Shifted Features","67c26624":"<p style=\"padding: 10px;color: Green;font-weight: bold;\n          text-align: center; font-size:250%;\">Store Item Demand Forecasting<\/p>\n","3271c460":"## Random Noise","1ab5adb0":"<a id = \"6\"><\/a><h1 id=\"Feature Importancel\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Feature Importance<\/span><\/h1>","e038b5a9":"## One-Hot Encoding","94f79ed7":"<a id = \"5\"><\/a><h1 id=\"LightGBM Model\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">LightGBM Model<\/span><\/h1>","2e34c701":"<a id = \"1\"><\/a><h1 id=\"Introduction\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Introduction<\/span><\/h1>","b294ba3b":"<a id = \"4\"><\/a><h1 id=\"Feature Engineering\"><span class=\"label label-default\" style=\"font-size:30px; color: Black; \">Feature Engineering<\/span><\/h1>","c37c5ecc":"![7807104_1578230927_time_analysis_tittle-banner (1).jpg](attachment:3d26a9aa-34ca-4fc5-b8c2-fe5a4e999225.jpg)","74bf79f7":"##  LightGBM Parameters","edf4dcef":"![83b0631f950fab03ac9854f0330baca2.jpg](attachment:6908ad60-471b-4900-9d2b-d64cb5b5b123.jpg)","c7912799":"This competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.\n\nYou are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores."}}