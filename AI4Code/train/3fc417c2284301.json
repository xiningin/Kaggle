{"cell_type":{"601c94ee":"code","f917a3cd":"code","daa03cac":"code","a5b45f4a":"code","197be4e5":"code","a555275b":"code","b97ae6a6":"code","42ab5ad5":"code","db4d57a3":"code","88ae02dc":"code","24ed669a":"code","daa07155":"code","c0bd61a0":"code","4a457119":"code","9937baab":"code","b1a397c4":"code","ac86fac8":"code","22762f5e":"code","6c0b5f2f":"code","f1a77ead":"markdown","0d3fae76":"markdown","b968e717":"markdown","9cd3a253":"markdown","85e22471":"markdown","ab6f58b8":"markdown","45941c49":"markdown","fa3051e4":"markdown","773e0fe4":"markdown","11e31791":"markdown","d347ee29":"markdown"},"source":{"601c94ee":"!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3","f917a3cd":"# add align_images.py\n!git clone https:\/\/github.com\/rkuo2000\/stylegan2-ada-pytorch\n%cd stylegan2-ada-pytorch","daa03cac":"#!python train.py --outdir=~\/training-runs --data=~\/mydataset.zip --gpus=1 --dry-run","a5b45f4a":"# Generate curated MetFaces image without truncation\n!python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/metfaces.pkl","197be4e5":"import os\nimport matplotlib.pyplot as plt\n\n# display foler's images in 2x2\ndef plot_images(img_dir, top=2):\n    all_img_dirs = os.listdir(img_dir)\n    img_files = [os.path.join(img_dir, file) for file in all_img_dirs]\n  \n    plt.figure(figsize=(10, 10))\n  \n    for idx, img_path in enumerate(img_files):\n        plt.subplot(2, 2, idx+1)\n    \n        img = plt.imread(img_path)\n        plt.tight_layout()         \n        plt.imshow(img, cmap='gray')\n        plt.axis('off')","a555275b":"# display images in output folder\nplot_images('.\/out')","b97ae6a6":"# Generate curated MetFaces image without truncation\n!python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/ffhq.pkl","42ab5ad5":"plot_images('.\/out')","db4d57a3":"!rm out\/*","88ae02dc":"# Style mixing example\n!python style_mixing.py --outdir=out --rows=85,100,458,1500 --cols=55,821,1789,293 --styles=0-6  \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/metfaces.pkl","24ed669a":"!ls out","daa07155":"# display images in a row\ndef plot_pics(img_files):  \n    plt.figure(figsize=(10, 10))\n  \n    for i in range(len(img_files)):\n        plt.subplot(1, len(img_files), i+1)\n    \n        img = plt.imread(img_files[i])\n        plt.tight_layout()         \n        plt.imshow(img, cmap='gray')\n        plt.axis('off')","c0bd61a0":"files = ['out\/100-100.png', 'out\/1789-1789.png', 'out\/100-1789.png']\nplot_pics(files)","4a457119":"!mkdir -p raw\n!wget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6d\/Shinz%C5%8D_Abe_Official.jpg -O raw\/example.jpg","9937baab":"# face alignment\n!python align_images.py raw aligned","b1a397c4":"!ls aligned","ac86fac8":"files = ['raw\/example.jpg', 'aligned\/example_01.png']\nplot_pics(files)","22762f5e":"# projector\n!python projector.py --outdir=out --target=aligned\/example_01.png \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/ffhq.pkl","6c0b5f2f":"from IPython.display import Video\nVideo('out\/proj.mp4')","f1a77ead":"## Generate\nPre-trained networks are stored as *.pkl files that can be referenced using local filenames or URLs:\n\n* Generate curated MetFaces images without truncation (Fig.10 left)\n\n    python generate.py --outdir=out --trunc=1 --seeds=85,265,297,849 \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/metfaces.pkl\n\n* Generate uncurated MetFaces images with truncation (Fig.12 upper left)\n\n    python generate.py --outdir=out --trunc=0.7 --seeds=600-605 \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/metfaces.pkl\n\n* Generate class conditional CIFAR-10 images (Fig.17 left, Car)\n\n    python generate.py --outdir=out --seeds=0-35 --class=1 \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/cifar10.pkl\n\n* Style mixing example\n\n    python style_mixing.py --outdir=out --rows=85,100,458,1500 --cols=55,821,1789,293 \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/metfaces.pkl","0d3fae76":"### download a picture","b968e717":"### face alignment","9cd3a253":"## Download Datasets\n### FFHQ\n* Step 1: Download the [Flickr-Faces-HQ dataset](https:\/\/github.com\/NVlabs\/ffhq-dataset) as TFRecords.\n\n* Step 2: Extract images from TFRecords using dataset_tool.py from the TensorFlow version of StyleGAN2-ADA:\n\n    Using dataset_tool.py from TensorFlow version at\n    https:\/\/github.com\/NVlabs\/stylegan2-ada\/\n    python ..\/stylegan2-ada\/dataset_tool.py unpack \\\n        --tfrecord_dir=~\/ffhq-dataset\/tfrecords\/ffhq --output_dir=\/tmp\/ffhq-unpacked\n        \n* Step 3: Create ZIP archive using dataset_tool.py from this repository\n\n* Original 1024x1024 resolution.\n    python dataset_tool.py --source=\/tmp\/ffhq-unpacked --dest=~\/datasets\/ffhq.zip\n\n    Scaled down 256x256 resolution.\n    python dataset_tool.py --source=\/tmp\/ffhq-unpacked --dest=~\/datasets\/ffhq256x256.zip \\\n    --width=256 --height=256","85e22471":"# StyleGAN2-ADA ","ab6f58b8":"## Projecting images to latent space\nTo find the matching latent vector for a given image file, run:\n\n    python projector.py --outdir=out --target=~\/mytargetimg.png \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/ffhq.pkl\n    \nFor optimal results, the target image should be cropped and aligned similar to the FFHQ dataset. The above command saves the projection target out\/target.png, result out\/proj.png, latent vector out\/projected_w.npz, and progression video out\/proj.mp4. You can render the resulting latent vector by specifying --projected_w for generate.py:\n    \n    python generate.py --outdir=out --projected_w=out\/projected_w.npz \\\n    --network=https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/ffhq.pkl    ","45941c49":"## Repro [Github](https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch)","fa3051e4":"![](https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch\/raw\/main\/docs\/stylegan2-ada-teaser-1024x252.png)","773e0fe4":"## Training new networks","11e31791":"## Paper: [Training Generative Adversarial Networks with Limited Data](https:\/\/arxiv.org\/abs\/2006.06676)","d347ee29":"## Projector"}}