{"cell_type":{"f23c0eb6":"code","df969c37":"code","62e1fd27":"code","505ab0bf":"markdown","24c84ba8":"markdown","b2d0c7a9":"markdown"},"source":{"f23c0eb6":"#!\/usr\/bin\/python3\n# coding=utf-8\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","df969c37":"#!\/usr\/bin\/python3\n# coding=utf-8\n#===========================================================================\n# load up the libraries\n#===========================================================================\nimport pandas  as pd\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#===========================================================================\n# select some features to rank. These are all 'integer' fields for today.\n#===========================================================================\nfeatures = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond', \n        'YearBuilt', 'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', \n        'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n        'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', \n        'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n        'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', \n        'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n        'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold']\n\n#===========================================================================\n#===========================================================================\nX_train       = train_data[features]\ny_train       = train_data[\"SalePrice\"]\nfinal_X_test  = test_data[features]\n\n#===========================================================================\n# simple preprocessing: imputation; substitute any 'NaN' with mean value\n#===========================================================================\nX_train      = X_train.fillna(X_train.mean())\nfinal_X_test = final_X_test.fillna(final_X_test.mean())\n\n#===========================================================================\n# set up our regressor + fit. \n# Today we shall be using the random forest regressor\n#===========================================================================\nfrom sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=100, max_depth=10)\nregressor.fit(X_train, y_train)\n\n#===========================================================================\n# perform the PermutationImportance\n#===========================================================================\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm_import = PermutationImportance(regressor, random_state=1).fit(X_train, y_train)\n\n# visualize the results\neli5.show_weights(perm_import, top=None, feature_names = X_train.columns.tolist())","62e1fd27":"#===========================================================================\n# perform a scikit-learn Recursive Feature Elimination (RFE)\n#===========================================================================\nfrom sklearn.feature_selection import RFE\n# here we want only one final feature, we do this to produce a ranking\nrfe = RFE(regressor, n_features_to_select=1)\nrfe.fit(X_train, y_train)\n\n#===========================================================================\n# now print out the features in order of ranking\n#===========================================================================\nfrom operator import itemgetter\nfor x, y in (sorted(zip(rfe.ranking_ , features), key=itemgetter(0))):\n    print(x, y)","505ab0bf":"## Permutation Importance: example\nThis is a simple example script to perfom permutation importance on the [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition data. The idea for this notebook comes directly from the magnificient introduction to [permutation importance](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance) by [Dan Becker](https:\/\/www.kaggle.com\/dansbecker), as part of the kaggle [Machine Learning Explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) micro-course. It makes use of the [ELI5 python library](https:\/\/eli5.readthedocs.io\/en\/latest\/) in conjunction with the [sklearn Permutation feature importance](https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html).\n\nFinally, we shall compare the results to those obtained from the scikit-learn Recursive Feature Elimination routine [sklearn.feature_selection.RFE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html). ","24c84ba8":"We can see that both techniques coincide in selecting the same four most important features:\n\n1. OverallQual\n2. GrLivArea\n3. TotalBsmtSF\n4. BsmtFinSF1\n\nafter which the order of the less important features change between both techniques and between runs.\n\n### **Note**: \n> Features that are deemed of **low importance for a bad model** (low cross-validation score) could be **very important for a good model**. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but **how important this feature is for a particular model**. ([source](https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html))","b2d0c7a9":"and now using Recursive Feature Elimination:"}}