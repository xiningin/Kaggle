{"cell_type":{"29602986":"code","cb5b5a18":"code","353fb4fb":"code","32343338":"code","a346d32f":"code","54d00aee":"code","ae244307":"code","05efacaa":"code","05e04235":"code","5a0e318d":"code","483818e8":"code","16489ca6":"code","22ba5e53":"code","f058bd35":"code","281a6b0b":"code","271c4c98":"code","9abbc59b":"code","0144dc60":"code","769c6663":"code","01345990":"code","ca9a2e2b":"code","d6f1b81b":"code","69e57767":"code","43f4e589":"code","e1be99da":"code","cf7700ae":"code","79c8e42a":"code","b693609c":"code","de6732b8":"code","aacdc114":"code","6fbf87fa":"code","01a856e5":"code","008c42f9":"code","12fb59e5":"code","a1102a1d":"code","0ece3c92":"code","be4ba587":"code","171c8c08":"code","a6d87449":"code","bc754022":"code","263a98d6":"code","ec747613":"code","e3eca725":"code","67ff5231":"code","14587787":"code","13a90c8d":"code","87776c0d":"code","8c1cd1f1":"code","cc7d0f55":"code","48a62873":"code","87769757":"code","7560f6c9":"code","372d4307":"code","0aa5b208":"code","2c8c501a":"code","31e96242":"code","bebcf404":"code","ec40dc55":"code","980f7ed4":"code","ae47b3fe":"code","8bde1dd0":"code","c7382462":"code","b7418c1f":"code","a55c4cbf":"code","6cd5edb5":"code","f86848df":"code","13dc7bdf":"code","7156f014":"code","c211aaf0":"code","8ff761ac":"code","80269363":"code","dd030521":"code","f373eb0c":"code","be0cfc55":"code","0460363b":"code","00d665b5":"markdown","c513ef21":"markdown","403923ac":"markdown","8551e70e":"markdown","4d94ca58":"markdown","932acb9d":"markdown","9a68cc9e":"markdown","ff2466f6":"markdown","0b883f4a":"markdown","d8be8e12":"markdown","11b5b9f7":"markdown","4fb7f3d3":"markdown","1df3dd11":"markdown","af45673a":"markdown","ccfb1fb1":"markdown","9a33cd36":"markdown","8ac186de":"markdown","aa90efce":"markdown","d990ca40":"markdown","52e64661":"markdown","869f2d81":"markdown","e8b50da6":"markdown","5521bd50":"markdown","f7a4f727":"markdown","e244ea81":"markdown","b0fe5771":"markdown","d355e9a1":"markdown","2f7dd30e":"markdown","dc47cfa3":"markdown","2bcc760a":"markdown","67f5ec2c":"markdown","e29e5bde":"markdown","9061143f":"markdown","f1b6c878":"markdown","69d85076":"markdown","42c73200":"markdown","b561a856":"markdown","1ed8636b":"markdown","25ee39b0":"markdown","fe0bfd62":"markdown"},"source":{"29602986":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cb5b5a18":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.gaussian_process.kernels import ExpSineSquared\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom scipy.stats import skew\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV,learning_curve\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n%matplotlib inline","353fb4fb":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","32343338":"train.shape","a346d32f":"test.shape","54d00aee":"train.info()","ae244307":"train.describe()","05efacaa":"train['SalePrice'].dropna().mean()","05e04235":"sns.distplot(train['SalePrice'])","5a0e318d":"sns.distplot(train['YearBuilt'])","483818e8":"sns.lmplot(x='GrLivArea', y='SalePrice', data=train)","16489ca6":"train[(train['GrLivArea'] > 4000) & (train['SalePrice'] < 200000)]","22ba5e53":"train.drop([523,1298], inplace=True)","f058bd35":"sns.lmplot(x='GrLivArea', y='SalePrice', data=train)","281a6b0b":"train_length = len(train)\ncombined = pd.concat([train, test])","271c4c98":"sns.heatmap(combined.isnull())","9abbc59b":"combined.isnull().sum().sort_values(ascending=False)[:40]","0144dc60":"combined['PoolQC'].fillna('No Pool', inplace=True)\ncombined['MiscFeature'].fillna('None', inplace=True)\ncombined['Alley'].fillna('No alley access', inplace=True)\ncombined['Fence'].fillna('No Fence', inplace=True)\ncombined['FireplaceQu'].fillna('No Fireplace', inplace=True)","769c6663":"combined[\"LotFrontage\"] = combined.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.dropna().median()))","01345990":"combined[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']] = combined[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']].fillna('No Garage')","ca9a2e2b":"combined[['GarageYrBlt', 'GarageArea', 'GarageCars']] = combined[['GarageYrBlt', 'GarageArea', 'GarageCars']].fillna(0)","d6f1b81b":"combined[['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtCond', 'BsmtQual']] = combined[['BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtCond', 'BsmtQual']].fillna('No Basement')","69e57767":"combined['MasVnrArea'] = combined['MasVnrArea'].fillna(0)\ncombined['MasVnrType'] = combined['MasVnrType'].fillna('None')","43f4e589":"combined['Electrical'] = combined['Electrical'].fillna(combined['Electrical'].mode()[0])\ncombined['MSZoning'] = combined['MSZoning'].fillna(combined['MSZoning'].mode()[0])\ncombined['Functional'] = combined['Functional'].fillna(combined['Functional'].mode()[0])\ncombined['Exterior1st'] = combined['Exterior1st'].fillna(combined['Exterior1st'].mode()[0])\ncombined['Exterior2nd'] = combined['Exterior2nd'].fillna(combined['Exterior2nd'].mode()[0])\ncombined['KitchenQual'] = combined['KitchenQual'].fillna(combined['KitchenQual'].mode()[0])\ncombined['SaleType'] = combined['SaleType'].fillna(combined['SaleType'].mode()[0])","e1be99da":"combined[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']] = combined[['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']].fillna(0) ","cf7700ae":"sns.countplot(x='Utilities', data=combined)","79c8e42a":"combined.drop('Utilities', axis=1, inplace=True)","b693609c":"combined['TotalBsmtSF'].fillna(0, inplace=True)","de6732b8":"combined.isnull().sum().sort_values(ascending=False)[:20]","aacdc114":"train[\"SalePrice\"] = train[\"SalePrice\"].map(lambda i: np.log1p(i))","6fbf87fa":"sns.distplot(train['SalePrice'])","01a856e5":"numeric_columns = []\ncategorical_columns = []\nfor column in combined.columns:\n    if(combined[column].dtype == np.object):\n        categorical_columns.append(column)\n    else :\n        numeric_columns.append(column)","008c42f9":"len(categorical_columns)","12fb59e5":"for column in categorical_columns:\n    combined[column] = LabelEncoder().fit_transform(combined[column])","a1102a1d":"len(numeric_columns)","0ece3c92":"skewed_columns = combined[numeric_columns].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)","be4ba587":"skewed_columns = skewed_columns.apply(abs)","171c8c08":"for column in skewed_columns[skewed_columns > 0.75].index:\n    combined[column] = combined[column].apply(lambda x : np.log1p(x))","a6d87449":"train = combined[:train_length]\ntest = combined[train_length:].drop('SalePrice', axis=1)","bc754022":"train.shape","263a98d6":"test.shape","ec747613":"X = train.drop('SalePrice', axis=1)\ny = train['SalePrice']","e3eca725":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","67ff5231":"KRR = KernelRidge()\nGBR = GradientBoostingRegressor()\nXGB = XGBRegressor()\nLGBM = LGBMRegressor()\nENET =  ElasticNet()\nLASS =  Lasso()","14587787":"models = [KRR, GBR, XGB, LGBM, ENET, LASS]","13a90c8d":"k_folds = KFold(5, shuffle=True, random_state=42)\n\ndef cross_val_rmse(model):\n    return np.sqrt(-1*cross_val_score(model, X, y,scoring=\"neg_mean_squared_error\",cv=k_folds))","87776c0d":"corss_val_score = []\nfor model in models:\n    model_name = model.__class__.__name__\n    corss_val_score.append((model_name,cross_val_rmse(model).mean()))","8c1cd1f1":"sorted(corss_val_score, key=lambda x : x[1], reverse=True)","cc7d0f55":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","48a62873":"for model in models:\n    plot_learning_curve(model,model.__class__.__name__ + \" mearning curves\",X,y,cv=5)","87769757":"# lass_param_grid = {\n#     'alpha' : [0.001, 0.0005],\n#     'random_state':[1,3,42]\n# }\n\n# enet_param_grid = {\n#     'alpha' : [0.001, 0.0005],\n#     'random_state':[1,3,42],\n#     'l1_ratio' : [.1,.9] \n# }\n\n# gboost_param_grid ={\n#     'n_estimators':[100,3000],\n#     'learning_rate': [0.1, 0.05],\n#     'max_depth':[4,6],\n#     'max_features':['sqrt'],\n#     'min_samples_leaf' :[3,9,15],\n#     'min_samples_split':[3,10],\n#     'loss':['huber'],\n#     'random_state':[5,42]\n# }\n\n# xgb_param_grid = {\n#     'colsample_bytree':[0.1,0.5],\n#     'gamma' :[0.01,0.04],\n#     'reg_alpha':[0.1,0.5],\n#     'reg_lambda':[0.1,0.9],\n#     'subsample':[0.1,0.5],\n#     'silent':[1],\n#     'random_state':[1,7],\n#     'nthread':[-1],\n#     'learning_rate': [0.1, 0.05],\n#     'max_depth': [3,6],\n#     'min_child_weight':[1.5,1.4,1.8],\n#     'n_estimators': [100,2000]}\n\n\n\n\n# krl_param_grid = {\"alpha\": [0.1, 0.6],\"degree\": [2,4], \"kernel\":['polynomial'], \"coef0\":[0.5,2.5]}\n\n\n\n# lgbm_param_grid = {\n#     'n_estimators':[100],\n#     'learning_rate': [0.1, 0.05, 0.01],\n#     'max_depth':[4,6],\n#     'max_leaves':[3,9,17],\n# }\n\n# models = [\n#     (KernelRidge,krl_param_grid),\n#     (XGBRegressor,xgb_param_grid),\n#     (GradientBoostingRegressor,gboost_param_grid),\n#     (Lasso,lass_param_grid),\n#     (ElasticNet,enet_param_grid)\n# ]","7560f6c9":"# best_models = []\n# for model, param in models:\n#     print(\"Fitting \", model.__class__.__name__)\n#     grid_search = GridSearchCV(model(),\n#                                scoring='neg_mean_squared_error',\n#                                param_grid=param,\n#                                cv=5,\n#                                verbose=2,\n#                                n_jobs=-1)\n#     grid_search.fit(X, y)\n#     print(grid_search.best_params_)","372d4307":"GBR =  GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\n\n\nXGB = XGBRegressor(gamma=0.04, learning_rate=0.05, colsample_bytree=0.5,  \n                              max_depth=3, \n                             min_child_weight=1.8, n_estimators=2000,\n                             reg_alpha=0.5, reg_lambda=0.9,\n                             subsample=0.5, silent=1,\n                             random_state =7, nthread = -1)\nKRR = KernelRidge(kernel='polynomial', alpha=0.6, coef0=2.5, degree=2)\n\nLASS = Lasso(alpha =0.0005, random_state=1)\nENET = ElasticNet( l1_ratio=.9,alpha=0.0005, random_state=3)\n","0aa5b208":"best_models = [GBR,XGB,KRR,LASS,ENET]","2c8c501a":"corss_val_score = []\nfor model in best_models:\n    model_name = model.__class__.__name__\n    print(\"Fitting \",model_name)\n    corss_val_score.append((model_name,cross_val_rmse(model).mean()))","31e96242":"sorted(corss_val_score, key=lambda x : x[1], reverse=True)","bebcf404":"corss_val_score","ec40dc55":"total_rmse = sum([x[1] for x in corss_val_score])","980f7ed4":"weighted_val_score = {}\nfor k,v in corss_val_score:\n    weighted_val_score[k] = round((v\/total_rmse)*100)","ae47b3fe":"weighted_val_score","8bde1dd0":"submission = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","c7382462":"for model in best_models:\n    model.fit(X,y)\n    y_pred = model.predict(test)\n    submission[model.__class__.__name__] = model.predict(test)\n    submission[model.__class__.__name__] = submission[model.__class__.__name__].apply(lambda x: np.expm1(x))","b7418c1f":"submission","a55c4cbf":"submission['AVG2'] = submission[['GradientBoostingRegressor','XGBRegressor', 'Lasso', 'ElasticNet']].mean(axis=1)\nweighted_average = submission[['Id', 'AVG2']]\nweighted_average.rename(columns={'AVG2':'SalePrice'}, inplace=True)\nweighted_average.to_csv(\"AVG2.csv\", index=False)","6cd5edb5":"submission['AVG3'] = submission[['GradientBoostingRegressor','XGBRegressor', 'ElasticNet']].mean(axis=1)\nweighted_average = submission[['Id', 'AVG3']]\nweighted_average.rename(columns={'AVG3':'SalePrice'}, inplace=True)\nweighted_average.to_csv(\"AVG3.csv\", index=False)","f86848df":"submission['AVG4'] = submission[['GradientBoostingRegressor', 'ElasticNet']].mean(axis=1)\nweighted_average = submission[['Id', 'AVG4']]\nweighted_average.rename(columns={'AVG4':'SalePrice'}, inplace=True)\nweighted_average.to_csv(\"AVG4.csv\", index=False)","13dc7bdf":"submission['AVG'] = submission[['GradientBoostingRegressor','XGBRegressor', 'KernelRidge', 'Lasso', 'ElasticNet']].mean(axis=1)\nweighted_average = submission[['Id', 'AVG']]\nweighted_average.rename(columns={'AVG':'SalePrice'}, inplace=True)\nweighted_average.to_csv(\"AVG.csv\", index=False)","7156f014":"submission['weighted_average'] = (submission['GradientBoostingRegressor']*(0.18))+(submission['XGBRegressor']*(0.18))+(submission['KernelRidge']*(0.28))+(submission['Lasso']*(0.18))+(submission['ElasticNet']*(0.18)) ","c211aaf0":"submission","8ff761ac":"weighted_average = submission[['Id', 'weighted_average']]\nweighted_average.rename(columns={'weighted_average':'SalePrice'}, inplace=True)\nweighted_average.to_csv(\"weighted_average.csv\", index=False)","80269363":"GRB = submission[['Id', 'GradientBoostingRegressor']]\nGRB.rename(columns={'GradientBoostingRegressor':'SalePrice'}, inplace=True)\nGRB.to_csv(\"GRB.csv\", index=False)","dd030521":"XGBR = submission[['Id', 'XGBRegressor']]\nXGBR.rename(columns={'XGBRegressor':'SalePrice'}, inplace=True)\nXGBR.to_csv(\"XGBR.csv\", index=False)","f373eb0c":"KRR = submission[['Id', 'KernelRidge']]\nKRR.rename(columns={'KernelRidge':'SalePrice'}, inplace=True)\nKRR.to_csv(\"KRR.csv\", index=False)","be0cfc55":"LASS = submission[['Id', 'Lasso']]\nLASS.rename(columns={'Lasso':'SalePrice'}, inplace=True)\nLASS.to_csv(\"LASS.csv\", index=False)","0460363b":"ENET = submission[['Id', 'ElasticNet']]\nENET.rename(columns={'ElasticNet':'SalePrice'}, inplace=True)\nENET.to_csv(\"ENET.csv\", index=False)","00d665b5":"No missing data we are good to go.","c513ef21":"> \"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this notebook proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\"\n\n","403923ac":"let us define our validation score method","8551e70e":"## 4. Gathering Data","4d94ca58":"now we can see that the data is well distributed. ","932acb9d":"since GarageYrBlt, GarageArea, GarageCars are numeric values, i will fill their missing values with 0","9a68cc9e":"now let us fix the skewed data of the ```SalePrice```","ff2466f6":"After tuning all above models, we can find the best parameters to give the more **negative** root mean squared error, which means the less **positive** root mean squared error.","0b883f4a":"### 7.4 Model Weighted Averaging Ensemble","d8be8e12":"let us do the same for year built","11b5b9f7":"let us check the skewness of the numeric columns","4fb7f3d3":"statistical summary","1df3dd11":"In general, i will fill NA value for columns of type object with the defined value for the NA values in the data description, missing values of numeric columns i will fill them with 0.","af45673a":"let us see the learning curve.","ccfb1fb1":"## 5. EDA - Exploratory Data Analysis","9a33cd36":"we have alot of missing data","8ac186de":"## 1. Background","aa90efce":"### 7.3 Hyperparameter Tuning","d990ca40":"## 3. Import Libraries","52e64661":"let see mean price","869f2d81":"LotFrontage - Linear feet of street connected to property\ni will fill missing value with the neighborhood mean value ","e8b50da6":"we can see here 2 prominent outliers, the two in the bottom with a high living (> 4,000) area but a low price (< 200,000).","5521bd50":"we can see that the ```SalePrice``` data is skewed - we will fix it later.","f7a4f727":"PoolQC - Pool quality, possible values are :\n*  Ex   : Excellent\n*  Gd   : Good\n*  TA   : Average\/Typical\n*  Fa   : Fair\n*  NA   : No Pool\n\nNA values defined as ```No Pool``` for the specific house, i will do the same for other features with missing values","e244ea81":"## 6. Feature Engineering","b0fe5771":"we can see that KernelRidge, GradientBoostingRegressor, XGBRegressor has the best cross validation result. \nlet us continue, plotting learning curve and  finding best estimators after the hyperparameter tuning process.","d355e9a1":"### 7.2 Cross Validation","2f7dd30e":"Sine there is no defined value for NA value for Electrical, i will fill missing value with the mode.","dc47cfa3":"# House Pricing Prediction","2bcc760a":"almost all of the Utilities values is ```AllPub```, so this will not effect the machine learning process, let us drop this column.","67f5ec2c":"now let us find categorical colmns and numeric columns","e29e5bde":"## 2. Problem","9061143f":"### 7.1 Model Defining","f1b6c878":"Now let us combine both data sets - train and test","69d85076":"First of all let us explore the outliers by checking the correlation between living area and  house price","42c73200":"## 7. Machine Learning Modeling","b561a856":"## 6. Data Cleaning","1ed8636b":"With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, USA. the goal is to predict the final price of each home.","25ee39b0":"i will transform categorical columns into numeric labels using ```LabelEncoder```","fe0bfd62":"we can see that more building were built afther the year 2000"}}