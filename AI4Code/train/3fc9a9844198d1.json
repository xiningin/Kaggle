{"cell_type":{"35a1dcdc":"code","1a8db2a7":"code","9b3d4279":"code","369644c9":"code","a7fbafb8":"code","85170e65":"code","4e6d52a5":"code","fcfa0c92":"code","bb8fd69b":"code","dd90358f":"code","34c04f5a":"code","59b1edd3":"code","518e43f5":"code","08969d32":"code","d341aa4a":"code","a391733d":"code","e8953054":"code","e5a12def":"code","0333e1ae":"code","70bc7c6e":"code","c3c31b52":"code","9e97ff08":"code","ed376c01":"code","cbed8c2a":"code","20771f50":"code","8eee0dc3":"code","b3c7a493":"code","44fd7d7d":"code","57b93b44":"code","8d415fd1":"code","abf148a9":"markdown","89d21a2f":"markdown","ef73aaa3":"markdown","55fd43ca":"markdown","b327a3de":"markdown","e28a8ff9":"markdown","f5a67448":"markdown","9115b3a5":"markdown","169465cb":"markdown","f516a7f8":"markdown","26916031":"markdown","58a5a905":"markdown","a36e0afe":"markdown","8a92e74f":"markdown","77d5cbbc":"markdown","576acbc9":"markdown","c2a6b327":"markdown","6843d991":"markdown","6b8b3b2f":"markdown","57ec1170":"markdown","e9765ce5":"markdown","82082d4a":"markdown","aedff3b8":"markdown","376501dd":"markdown","976b24ec":"markdown","366cd0c8":"markdown","804257c5":"markdown","889c8862":"markdown","ce3e422e":"markdown","4bca5be9":"markdown","eb80bc0e":"markdown","fb282f98":"markdown","1228cea4":"markdown"},"source":{"35a1dcdc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a8db2a7":"import matplotlib.pyplot as plt","9b3d4279":"# Data Loading\ntrain_set_path = '\/kaggle\/input\/nlp-getting-started\/train.csv'\ntrain_set = pd.read_csv(train_set_path) # returns pd.DataFrame","369644c9":"# Data Preview\nprint('---Input Analysis---')\nprint('Number of rows in Input:', train_set.shape[0]) \nprint('Number of columns in Input:', train_set.shape[1])# (7613,5)\nprint('First 5 rows: \\n')\nprint(train_set.head()) # id, keyword, location, text, target","a7fbafb8":"# ID\nprint('---ID Analysis---')\nprint('Unique IDs')\nprint(train_set.id.unique())\nprint('ID Counts')\nprint(train_set.id.value_counts())","85170e65":"# Keyword\nprint('---Keyword Analysis---')\nprint('Unique Keywords')\nprint(train_set.keyword.unique())\nprint('Keyword Counts')\nprint(train_set.keyword.value_counts())\ntrain_set.keyword.value_counts().plot.barh(figsize=(5,100), title='Keyword Counts', )","4e6d52a5":"# Text\nprint('---Text Analysis---')\nprint('Text Review')\nfor text in train_set.text:\n    print(text)\n\n# Character Counts\ntrain_set['char_counts'] = train_set['text'].str.len()\ntrain_set.hist(column='char_counts')\n\n# Word Counts\ntrain_set['word_counts'] = train_set['text'].str.split().map(lambda x: len(x))\ntrain_set.hist(column='word_counts')","fcfa0c92":"# Target Distribution\nprint('---Target Analysis---')\nprint('Unique Targets')\nprint(train_set.target.unique())\nprint('Target Counts')\nprint(train_set.target.value_counts())\nprint(train_set.target.value_counts(normalize=True))\ntrain_set.target.value_counts().plot.bar()\nplt.title('Class Distribution')","bb8fd69b":"# Keyword counts per class\nis_disaster = train_set['target'] == 1\ndisaster_keyword_counts = train_set[is_disaster].keyword.value_counts()\nnot_disaster_keyword_counts = train_set[~is_disaster].keyword.value_counts()\njoint_keyword_counts = pd.concat([disaster_keyword_counts, not_disaster_keyword_counts], axis=1, join='outer',\n                                 keys=['disaster', 'not_disaster'])\nprint(joint_keyword_counts)\njoint_keyword_counts.plot.barh(subplots=False, figsize=(5,100))\n\nbool_disaster_exclusive = pd.isnull(joint_keyword_counts.not_disaster)\ndisaster_exclusive_counts = joint_keyword_counts[bool_disaster_exclusive]\n\nprint('Disaster Exclusive Keywords:')\nprint(disaster_exclusive_counts)\n\nbool_not_disaster_exclusive = pd.isnull(joint_keyword_counts.disaster)\nnot_disaster_exclusive_counts = joint_keyword_counts[bool_not_disaster_exclusive]\n\nprint('Not Disaster Exclusive Keywords:')\nprint(not_disaster_exclusive_counts)","dd90358f":"# Character Counts per Class\ntrain_set.hist(column='char_counts', by='target')\n\n# Word Counts per Class\ntrain_set.hist(column='word_counts', by='target')","34c04f5a":"import re\nimport string","59b1edd3":"# remove_punctiations:\n# URLs\ndef remove_urls(text): \n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# Punctuations\ndef remove_punctuations(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ntrain_set['clean_text'] = train_set['text']\ntrain_set['clean_text'] = train_set['clean_text'].apply(lambda x : remove_urls(x))\ntrain_set['clean_text'] = train_set['clean_text'].apply(lambda x : remove_punctuations(x))\n\nfor text in train_set.clean_text:\n    print(text)","518e43f5":"train_set = train_set.sample(frac=1)\ntrain_set.head()","08969d32":"inputs = train_set.clean_text.tolist()\ntargets = train_set.target.tolist()\n    \nprint(inputs)\nprint(targets)","d341aa4a":"from nltk.corpus import stopwords\nstop_words = None\nstop_words = set(stopwords.words('english'))\nprint(stop_words)\n\ndef remove_stopwords(inputs, stop_words):\n    if not stop_words:\n        return corpus\n    \n    new_inputs = []\n    for sentence in inputs:\n        for word in stop_words:\n            word_with_space = \" \" + word + \" \" # Assuming words leads and trails with space. If not, subwords might be replaced.\n            sentence = sentence.replace(word_with_space, \" \")\n        new_inputs.append(sentence)\n            \n    return new_inputs\n\ninputs = remove_stopwords(inputs, stop_words)\nprint(inputs)","a391733d":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_length = 50\npadding_type = 'post'\ntrunc_type = 'post'\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(inputs)\n\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\n\nsequences = tokenizer.texts_to_sequences(inputs)\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\nprint('Vocabulary Size: ', len(word_index))","e8953054":"print('Number of Inputs:', len(inputs))\nprint('Number of Targets:', len(targets))\n\ntrain_size = 7000\ntrain_inputs = padded[:train_size]\ntrain_targets = targets[:train_size]\ntrain_targets = np.array(train_targets)\n\nprint(train_inputs)\nprint(train_targets)\n\ndev_inputs = padded[train_size:]\ndev_targets = targets[train_size:]\ndev_targets = np.array(dev_targets)\n\nprint(dev_inputs)\nprint(dev_targets)\n\nplt.hist(train_targets)\nplt.hist(dev_targets)","e5a12def":"embedding_path = '\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt'\n\nembeddings_index = {}\nwith open(embedding_path) as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint(len(embeddings_index))\n\nvocab_size = len(word_index)\nembedding_dim = 100\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector\n\nprint(len(embeddings_matrix))","0333e1ae":"from tensorflow.keras.optimizers import Adam\n\ncompile_opts = {}\ncompile_opts['loss'] = 'binary_crossentropy'\ncompile_opts['optimizer'] = Adam(learning_rate=1e-3)\ncompile_opts['metrics'] = ['accuracy']","70bc7c6e":"fit_options = {}\nfit_options['x'] = train_inputs\nfit_options['y'] = train_targets\nfit_options['validation_data'] = (dev_inputs, dev_targets)\nfit_options['batch_size'] = 128\nfit_options['epochs'] = 100\nfit_options['verbose'] = 2\nfit_options['callbacks'] = None","c3c31b52":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n    \nfrom tensorflow.keras.layers import Dense, Input\nimport tensorflow_hub as hub\nimport tokenization\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ntrain_input = bert_encode(train_set.text.values, tokenizer, max_len=50)\n#test_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train_set.target.values\n\nbert_model = build_model(bert_layer, max_len=50)\nbert_model.summary()\nbert_history = bert_model.fit(train_input, train_labels, validation_split=0.2, epochs=5, batch_size=16, verbose=2)","9e97ff08":"import tensorflow as tf\n\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.5)), # recurrent_dropout too slow\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nlstm_model.compile(**compile_opts)\nlstm_model.summary()\nlstm_history = lstm_model.fit(**fit_options)\n\nprint(\"Training Complete\")","ed376c01":"gru_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32, dropout=0.5)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\ngru_model.compile(**compile_opts)\ngru_model.summary()\ngru_history = gru_model.fit(**fit_options)\n\nprint(\"Training Complete\")","cbed8c2a":"conv_model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Conv1D(64, 5, activation='relu', padding='same'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nconv_model.compile(**compile_opts)\nconv_model.summary()\nconv_history = conv_model.fit(**fit_options)\n\nprint(\"Training Complete\")","20771f50":"def plot_graphs(history):\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epochs')\n    plt.ylabel('accuracy')\n    plt.legend(['accuracy', 'val_accuracy'])\n    plt.show()\n    \n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.xlabel('Epochs')\n    plt.ylabel('loss')\n    plt.legend(['loss', 'val_loss'])\n    plt.show()\n\nplot_graphs(lstm_history)\nplot_graphs(gru_history)\nplot_graphs(conv_history)\nplot_graphs(bert_history)","8eee0dc3":"import io\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\ne = lstm_model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)\n\n# Expected output\n# (1000, 16)\n\nout_v = io.open('\/kaggle\/working\/vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('\/kaggle\/working\/meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, vocab_size):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","b3c7a493":"lstm_pred = lstm_model.predict(dev_inputs)\nlstm_pred = [x[0] for x in lstm_pred]\ngru_pred = gru_model.predict(dev_inputs)\nconv_pred = conv_model.predict(dev_inputs)\n\ndev_sentences = inputs[train_size:]\ndev_df = pd.DataFrame(dev_sentences, columns=['Sentence'])\ndev_df['Truth'] = dev_targets == 1\ndev_df['LSTM'] = lstm_pred\ndev_df['LSTM'] = dev_df['LSTM'] >= 0.5\ndev_df['GRU'] = gru_pred\ndev_df['LSTM'] = dev_df['GRU'] >= 0.5\ndev_df['CONV'] = conv_pred\ndev_df['LSTM'] = dev_df['CONV'] >= 0.5\nprint(dev_df)\nFN = []\nfor truth, pred in zip(dev_targets, lstm_pred):\n    if truth == 1 and pred < 0.5:\n        FN.append(True)\n    else:\n        FN.append(False)\n        \nprint(dev_df[FN]['Sentence'])\n    ","44fd7d7d":"test_set = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_set['clean_text'] = test_set['text']\ntest_set['clean_text'] = test_set['clean_text'].apply(lambda x : remove_urls(x))\ntest_set['clean_text'] = test_set['clean_text'].apply(lambda x : remove_punctuations(x))\n\nfor text in test_set.clean_text:\n    print(text)\n    \ntest_inputs = test_set.clean_text.tolist()\ntest_inputs = remove_stopwords(test_inputs, stop_words)\ntest_sequences = tokenizer.texts_to_sequences(test_inputs)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","57b93b44":"lstm_pred = lstm_model.predict(test_padded)\nprint(lstm_pred)\nnum_test_samples = len(lstm_pred)","8d415fd1":"sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\npred = np.round(lstm_pred).astype(int).reshape(3263)\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':pred})\nsub.to_csv('\/kaggle\/working\/submission.csv', index=False)","abf148a9":"Submission","89d21a2f":"Visualize Accuracy and Loss","ef73aaa3":"GRU Model","55fd43ca":"Split Dataset","b327a3de":"BERT Model","e28a8ff9":"# Exploratory Data Analysis\nAnalyze column by column and figure out what to do for each column.","f5a67448":"Fit Options","9115b3a5":"# Embedding with GloVe (Global Vectors for Word Representation)\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/","169465cb":"* Data Preview\n\nInput data has 7613 rows and 5 columns.\n\nColumn headers are ['id', 'keyword, 'location', 'text', 'target']. \n\n","f516a7f8":"Text v.s. Targets (Before Processing)\n\n","26916031":"Prediction","58a5a905":"# Building the Inputs and Targets\n\nThe inputs is a list containing each tweet as a string.\n\nThe targets is a list with the provided labels (0\/1). ","a36e0afe":"* Column 2: Keyword\n\nThe 'keyword' column contains a tag for each tweet.\n\nTop five most frequent tags are:\n    1. fatalities               45\n    2. armageddon               42\n    3. deluge                   42\n    4. harm                     41\n    5. body%20bags              41\n\n    * Investigate correlation of tags and targets.\n    * Blank is filled by '%20', needs preprocessing.\n    * There are NaNs\/empty. ","8a92e74f":"* Column 1: ID\n\nThe 'id' column contains unique identifiers for each tweet.\n\nIn the training set, each tweet came from a distinct user.\n\nHowever, there might be a user who always post disaster tweets (target == 1).\n\n    * ID might be helpful for identifying targets.     ","77d5cbbc":"Stopwords","576acbc9":"Keyword v.s. Targets\n\nThere are some keywords that is exclusively used for disaster and vice versa.\n\nDisaster Exclusive Keywords: derailment, wreckage, debris.\n\nNot Disaster Exclusive Keywords: aftershock.\n\n","c2a6b327":"LSTM Model","6843d991":"Dataframe to List","6b8b3b2f":"Shuffle Dataframe","57ec1170":"Test Set","e9765ce5":"Visualize Embeddings","82082d4a":"Token and Padding","aedff3b8":"# Models","376501dd":"Compile Options","976b24ec":"Load Other Libraries","366cd0c8":"* Column 3: Text\nThe 'text' column contains the tweets for classification.\n\nSo this is definitely the ***INPUT\/X*** for the model.\n\nAll tweets are printed to check what preprocessing is needed. (Output is suppressed for now).\n\n    * Hashtags\n    * Punctuations\n    * Numbers\n    * Signs (=>)\n    * URLs","804257c5":"Misclassified Samples","889c8862":"* Column 4: Target\nThe 'target' column contains the ***TARGET\/Y*** for the task.\n\nThere are two targets 0 and 1, not disaster and disaster.\n\nThe classes are 57% to 43%, not too imbalanced.","ce3e422e":"Load Training Set","4bca5be9":"References:\n1. https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n2. https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n3. https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub","eb80bc0e":"1D Convolution Model","fb282f98":"Load Libraries","1228cea4":"# Data Cleaning\n\nIn text analysis section, every sample was reviewed to identify several special characters that should be removed to provide a cleaner input.\n\nCommon sources are punctuations, urls, and tags.\n\nIn this section, helper functions are defined for cleaning the inputs.\n"}}