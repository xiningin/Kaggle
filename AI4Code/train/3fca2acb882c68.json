{"cell_type":{"ee9be5d2":"code","d1cb827a":"code","cdbee621":"code","9e073f54":"code","10a4eb84":"code","69fd65b6":"code","328456d3":"code","6c499523":"code","274e9a66":"code","88866155":"code","fae2c8fc":"code","0869f972":"code","574636a8":"code","04f9cb7e":"code","37088769":"code","01a60f2d":"code","dae89ff0":"code","9f563cea":"code","41d16c50":"code","a1e52fbb":"code","2c28a800":"code","935c3a1f":"code","01b7238d":"code","79cf0221":"code","9ccf8f19":"code","5700a6f6":"code","57bf01d2":"code","d9c8964d":"code","cc26e7c5":"code","96ebf638":"code","ea9266bb":"code","aa6fa9a8":"code","3d296fb0":"code","52ffd5af":"code","bc0eb3fa":"code","3363ccf1":"code","05a6a614":"code","38400ce2":"code","88ddaabb":"code","274fe059":"code","0a4c6f94":"code","6c131c12":"code","9f2f3c6c":"code","6fb43e95":"code","fee3706b":"code","2d26230e":"code","9ae0b994":"code","87284902":"code","00429cb0":"code","85e3a22d":"code","c6db7918":"code","a6ff7bae":"code","25aebd03":"code","104a19a8":"code","45d9c4f7":"code","30f4e503":"code","189d6a15":"code","a8d41204":"code","07767b02":"code","b2fea587":"code","c5d3aff0":"code","537d7d8b":"code","a7e69079":"code","2e6d29c2":"code","831e10e7":"code","3eea7e48":"code","30495af3":"code","c98313a5":"code","c23a535f":"code","2391ad9c":"code","dfe7a9b5":"code","23ec6e8a":"code","9c9e0247":"markdown","a9de210e":"markdown","ed2c9398":"markdown","18b4e7a2":"markdown","6f195865":"markdown","0627ee26":"markdown","bb8d6502":"markdown","9ddb0ec3":"markdown","cc031d53":"markdown","e2ce3eff":"markdown","d5d87dad":"markdown","ae2c53d2":"markdown","bc7394fa":"markdown","6ae6c604":"markdown","7e98c8d7":"markdown","f2abaf7b":"markdown","75f7a520":"markdown","51b7defc":"markdown","ef41825b":"markdown","caf27494":"markdown","8fe7a2a9":"markdown","76f13f4d":"markdown","9f709952":"markdown","62d27396":"markdown","ed113f7d":"markdown","ee865681":"markdown","e9a44847":"markdown","cccf39dc":"markdown","3fd18fa2":"markdown","2e98f6b4":"markdown","2ffa3fd1":"markdown","a88996d4":"markdown","af248818":"markdown","9ef6c7b8":"markdown","64030ad6":"markdown","186679b8":"markdown","e974aad2":"markdown","14c5ca68":"markdown","b52a55ae":"markdown","50555307":"markdown","878a3cc1":"markdown","9a05e270":"markdown","f7ebf202":"markdown","072fa45d":"markdown","485edefb":"markdown","29e7a6c1":"markdown","078922ee":"markdown","be1e8778":"markdown","ad5ec0ca":"markdown","46d0cb96":"markdown","99d08a2f":"markdown","7773aa3d":"markdown","b470563b":"markdown","59167018":"markdown","7e8bb9a8":"markdown","ebb726d9":"markdown","a7fe53da":"markdown","8c2e8fac":"markdown"},"source":{"ee9be5d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d1cb827a":"from scipy import stats\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid') # just optional!\n%matplotlib inline\n\n#Setting display format to retina in matplotlib to see better quality images.\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')\n\n# Lines below are just to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","cdbee621":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","9e073f54":"df_train.head()","10a4eb84":"df_test.head()","69fd65b6":"df_train.describe()","328456d3":"df_test.describe()","6c499523":"sns.distplot(df_train['SalePrice'])","274e9a66":"df_train.info()","88866155":"df_test.info()","fae2c8fc":"df =pd.DataFrame()\ndf['null'] =  pd.Series(df_train.isnull().sum())\ndf = df[df.null != 0]\ndf.sort_values(by= 'null',ascending= False )","0869f972":"f, ax = plt.subplots(figsize=(8,5))\nplt.xticks(rotation='90')\nsns.barplot(x=df.index, y=df['null'])\nplt.xlabel('Features')\nplt.ylabel('Number of missing values')\nplt.title('Number missing data by feature')","574636a8":"# Replacing missing values with None \nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','MasVnrType','PoolQC','MiscFeature','FireplaceQu'\n            ,'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MSSubClass','Alley','Fence'):\n    df_train[col] = df_train[col].fillna('None')\n#\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','MasVnrType','PoolQC','MiscFeature','FireplaceQu'\n            ,'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MSSubClass','Alley','Fence'):\n    df_test[col] = df_test[col].fillna('None')","04f9cb7e":"#Replacing missing values with 0 \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'MasVnrArea',\n            'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_train[col] = df_train[col].fillna(0)\n#\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1','MasVnrArea', \n            'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_test[col] = df_test[col].fillna(0)","37088769":"#Replacing missing values with median \ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n#\ndf_test[\"LotFrontage\"] = df_test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","01a60f2d":"#Functional\ndf_train[\"Functional\"] = df_train[\"Functional\"].fillna(\"Typ\")\n#\ndf_test[\"Functional\"] = df_test[\"Functional\"].fillna(\"Typ\")","dae89ff0":"print (df_train['Utilities'].value_counts())\nprint (df_test['Utilities'].value_counts())","9f563cea":"#Drop utilities\ndf_train = df_train.drop(['Utilities'], axis=1)\n#\ndf_test = df_test.drop(['Utilities'], axis=1)","41d16c50":"# Replacing missing values with mode \nfor col in ('SaleType','Exterior2nd','Exterior1st','KitchenQual','Electrical','MSZoning'):\n    df_train[col] = df_train[col].fillna(df_train[col].mode()[0])\n    \nfor col in ('SaleType','Exterior2nd','Exterior1st','KitchenQual','Electrical','MSZoning'):\n    df_test[col] = df_test[col].fillna(df_test[col].mode()[0])","a1e52fbb":"#Recheck remaining missing values if any\ndf =pd.DataFrame()\ndf['null'] =  pd.Series(df_train.isnull().sum())\ndf = df[df.null != 0]\ndf","2c28a800":"print(\"The train data size before dropping Id feature is : {} \".format(df_train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(df_test.shape))\n\n#Save the 'Id' column\ntrain_ID = df_train['Id']\ntest_ID = df_test['Id']\n\n#dropping Id \ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))","935c3a1f":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True,cmap='GnBu')","01b7238d":"#saleprice correlation matrix\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap='GnBu')\nplt.show()","79cf0221":"categorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nli_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):\n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y='SalePrice', data=df_train, ax = axs[r][c])\nplt.tight_layout()\nplt.show()","9ccf8f19":"atg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual',\n                    'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType','FireplaceQu', 'GarageQual']\nnum_Strong_corr = [cols]","5700a6f6":"for df in [df_train, df_test]:\n    df['MSZ_num'] = 1\n    df.loc[(df['MSZoning']=='RH' ), 'MSZ_num'] = 2\n    df.loc[(df['MSZoning']== 'RM' ), 'MSZ_num'] = 3\n    df.loc[(df['MSZoning']== 'RL' ), 'MSZ_num'] = 4\n    df.loc[(df['MSZoning']=='FV' ), 'MSZ_num'] = 5\n\n    \nfor df in [df_train, df_test]:\n    df['NbHd_num'] = 1\n    df.loc[(df['Neighborhood']== 'Blmngtn'), 'NbHd_num'] = 2\n    df.loc[(df['Neighborhood']==  'ClearCr'), 'NbHd_num'] = 3\n    df.loc[(df['Neighborhood']==  'CollgCr'), 'NbHd_num'] = 4\n    df.loc[(df['Neighborhood']==  'Crawfor'), 'NbHd_num'] = 5\n    df.loc[(df['Neighborhood']==   'Gilbert'), 'NbHd_num'] = 6\n    df.loc[(df['Neighborhood']==  'NWAmes'), 'NbHd_num'] = 7\n    df.loc[(df['Neighborhood']==  'Somerst'), 'NbHd_num'] = 8\n    df.loc[(df['Neighborhood']==  'Timber'), 'NbHd_num'] = 9\n    df.loc[(df['Neighborhood']==  'Veenker'), 'NbHd_num'] = 10\n    df.loc[(df['Neighborhood']== 'NoRidge' ), 'NbHd_num'] = 11\n    df.loc[(df['Neighborhood']== 'NridgHt' ), 'NbHd_num'] = 12\n    df.loc[(df['Neighborhood']== 'StoneBr' ), 'NbHd_num'] = 13\n\n\nfor df in [df_train, df_test]:\n    df['Cond2_num'] = 1\n    df.loc[(df['Condition2']=='Norm' ), 'Cond2_num'] = 2\n    df.loc[(df['Condition2']=='RRAe' ), 'Cond2_num'] = 3\n    df.loc[(df['Condition2']=='PosA'), 'Cond2_num'] = 4\n    df.loc[(df['Condition2']=='PosN'), 'Cond2_num'] = 5\n\nfor df in [df_train, df_test]:\n    df['Mas_num'] = 1\n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2\n    \nfor df in [df_train, df_test]:\n    df['ExtQ_num'] = 1\n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2\n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3\n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4\n    \nfor df in [df_train, df_test]:\n    df['BsQ_num'] = 1\n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2\n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3\n    \nfor df in [df_train, df_test]:\n    df['CA_num'] = 0\n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1\n    \nfor df in [df_train, df_test]:\n    df['Elc_num'] = 1\n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2\n    \nfor df in [df_train, df_test]:\n    df['KiQ_num'] = 1\n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2\n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3\n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4\n    \nfor df in [df_train, df_test]:\n    df['SlTy_num'] = 1\n    df.loc[(df['SaleType']=='Oth'), 'SlTy_num'] = 2\n    df.loc[(df['SaleType']=='CWD'), 'SlTy_num'] = 3\n    df.loc[(df['SaleType']=='Con' ), 'SlTy_num'] = 4\n    df.loc[(df['SaleType']=='New' ), 'SlTy_num'] = 5\n    \n\nfor df in [df_train, df_test]:\n    df['FireplaceQu_num'] = 1\n    df.loc[(df['FireplaceQu']=='Gd'), 'FireplaceQu_num'] = 2\n    df.loc[(df['FireplaceQu']=='TA'), 'FireplaceQu_num'] = 3\n    df.loc[(df['FireplaceQu']=='Fa' ), 'FireplaceQu_num'] = 4\n    df.loc[(df['FireplaceQu']=='Ex' ), 'FireplaceQu_num'] = 5\n    df.loc[(df['FireplaceQu']=='Po' ), 'FireplaceQu_num'] = 6\n    \nfor df in [df_train, df_test]:\n    df['GarageQual_num'] = 1\n    df.loc[(df['GarageQual']=='None'), 'GarageQual_num'] = 2\n    df.loc[(df['GarageQual']=='Fa'), 'GarageQual_num'] = 3\n    df.loc[(df['GarageQual']=='Gd' ), 'GarageQual_num'] = 4\n    df.loc[(df['GarageQual']=='Po' ), 'GarageQual_num'] = 5\n    df.loc[(df['GarageQual']=='Ex' ), 'GarageQual_num'] = 6","57bf01d2":"isNumeric_train = df_train._get_numeric_data()\nisNumeric_test = df_test._get_numeric_data()","d9c8964d":"corrmat = isNumeric_train.corr()\nf, ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(corrmat, vmax=.8,square=True,cmap='GnBu')\n# cbar=True, annot=True, square=True","cc26e7c5":"\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(isNumeric_train[cols].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(15, 10))\nsns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap='GnBu')\nplt.show()","96ebf638":"#defind outliers\nfig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.title('')\nplt.show()","ea9266bb":"#Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","aa6fa9a8":"#function to create csv file\ndef create_csv_predictions(predictions, file_name):\n    \n    predictions=pd.DataFrame(predictions)\n    predictions['Id']= [i for i in range(1461, 2920)]\n    predictions.set_index('Id',inplace=True)\n    predictions.rename(columns={0:'SalePrice'},inplace = True)\n    predictions.to_csv(file_name)","3d296fb0":"from sklearn.preprocessing import StandardScaler","52ffd5af":"X_train= isNumeric_train[['OverallQual','GrLivArea','ExtQ_num','NbHd_num','KiQ_num','BsQ_num','TotalBsmtSF','GarageCars','1stFlrSF','GarageArea','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']]\ny_train= isNumeric_train['SalePrice']","bc0eb3fa":"X_test= isNumeric_test[['OverallQual','GrLivArea','ExtQ_num','NbHd_num','KiQ_num','BsQ_num','TotalBsmtSF','GarageCars','1stFlrSF','GarageArea','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']]\n","3363ccf1":"scaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","05a6a614":"Model= []\nR_s_training = []\nmean_cv_s_5 = []","38400ce2":"#find the score of beasline\n\n# create a DummyRegressor model instance\ndummy_constant = DummyRegressor(strategy='constant', constant=y_train.mean())\n\n# \"Train\" dummy regressor\ndummy_constant.fit(X_train, y_train)\ndummy_constant.score(X_train, y_train)","88ddaabb":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# create a linear regression model instance\nmodel = LinearRegression()\n\n# get cross validated scores\nscores = cross_val_score(model, X_train, y_train, cv=5)\nprint(\"Cross-validated training scores:\", scores)\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n#fit and evaluate the data on the whole training set\nmodel.fit(X_train, y_train)\n\nprint(\"Training Score:\", model.score(X_train, y_train))","274fe059":"predictions= model.predict(X_test)\ncreate_csv_predictions(predictions, 'predictions.csv')","0a4c6f94":"df_train = pd.read_csv('train.csv')\ndf_test = pd.read_csv('test.csv')","6c131c12":"\n# Replacing missing values with None \nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','MasVnrType','PoolQC','MiscFeature','FireplaceQu'\n            ,'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MSSubClass','Alley','Fence'):\n    df_train[col] = df_train[col].fillna('None')\n#\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','MasVnrType','PoolQC','MiscFeature','FireplaceQu'\n            ,'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MSSubClass','Alley','Fence'):\n    df_test[col] = df_test[col].fillna('None')\n\n#Replacing missing values with 0 \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'MasVnrArea',\n            'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_train[col] = df_train[col].fillna(0)\n#\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1','MasVnrArea', \n            'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    df_test[col] = df_test[col].fillna(0)\n\n#Replacing missing values with median \ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n#\ndf_test[\"LotFrontage\"] = df_test.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n\n#Functional\ndf_train[\"Functional\"] = df_train[\"Functional\"].fillna(\"Typ\")\n#\ndf_test[\"Functional\"] = df_test[\"Functional\"].fillna(\"Typ\")\n\n\n#Drop utilities\ndf_train = df_train.drop(['Utilities'], axis=1)\n#\ndf_test = df_test.drop(['Utilities'], axis=1)\n#\nfor col in ('SaleType','Exterior2nd','Exterior1st','KitchenQual','Electrical','MSZoning'):\n    df_train[col] = df_train[col].fillna(df_train[col].mode()[0])\n    \nfor col in ('SaleType','Exterior2nd','Exterior1st','KitchenQual','Electrical','MSZoning'):\n    df_test[col] = df_test[col].fillna(df_test[col].mode()[0])","9f2f3c6c":"isNumeric_train = df_train._get_numeric_data()\nisNumeric_test = df_test._get_numeric_data()","6fb43e95":"#creat dummies for categorical feats\ncategorical_feats = list(df_train.dtypes[df_train.dtypes == \"object\"].index)\ncategorical_train= df_train[categorical_feats]\ncategorical_test= df_test[categorical_feats]\ndummies_train = pd.get_dummies(categorical_train)\ndummies_test = pd.get_dummies(categorical_test)","fee3706b":"new_df_train= pd.concat([isNumeric_train,dummies_train], axis=1)\nnew_df_test= pd.concat([isNumeric_test,dummies_test], axis=1)","2d26230e":"X_train= new_df_train[[c for c in new_df_train if c != 'SalePrice']]\ny_train= new_df_train['SalePrice']\nX_test= new_df_test[[c for c in new_df_test]]\n","9ae0b994":"missing=[]\nfor mis in X_train.columns:\n    if mis not in X_test.columns:\n        missing.append(mis)\n\nmissing","87284902":"for col in missing:\n    X_test[col]= np.nan\n    \nX_test.fillna(0, inplace=True)\nX_test=X_test[X_train.columns]","00429cb0":"#models evaluation df\nevaluation = pd.DataFrame({'Model': [],\n                          'R-squared (training)':[],\n                          '5-Fold Cross Validation':[]})","85e3a22d":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\n\nscaler = MinMaxScaler(feature_range=(0, 1))\n\nx_train_scaled = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(x_train_scaled)\n\nx_test_scaled = scaler.fit_transform(X_test)\nX_test = pd.DataFrame(x_test_scaled)","c6db7918":"#gridsearch params\ndtc_params = {\n    'max_depth': range(1,30),\n    'max_features': [None, 'log2', 'sqrt'],\n    'min_samples_split': range(5,20),\n    'max_leaf_nodes': [None],\n    'min_samples_leaf': range(1,10)\n}\n\n\n# set the gridsearch\nmodel_dtc = DecisionTreeRegressor()\nmodel_dtc_gs = GridSearchCV(estimator=model_dtc,param_grid=dtc_params,cv=5, error_score='raise-deprecating', n_jobs=-1, verbose=1)\n\n","a6ff7bae":"# fit the model\nmodel_dtc_gs=model_dtc_gs.fit(X_train,y_train)","25aebd03":"predictions= model_dtc_gs.best_estimator_.predict(X_test)\ncreate_csv_predictions(predictions, 'predictions_dtc_gs.csv')","104a19a8":"# evaluate on the training set\nprint('Training score:', model_dtc_gs.best_score_)\n\nevaluation = evaluation.append({'Model' : 'dtc' ,\n                                'R-squared (training)' : model_dtc_gs.best_score_, \n                                '5-Fold Cross Validation':''} ,ignore_index=True)","45d9c4f7":"#import required packages\nfrom sklearn import neighbors\nfrom sklearn.metrics import mean_squared_error \nfrom math import sqrt\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# gridsearch params\ntuned_parameters = [{'weights': ['uniform', 'distance'],\n                    'n_neighbors': range(2,100)}]\n# create a GridSearchCV model instance\nmodel_KNN = GridSearchCV(neighbors.KNeighborsRegressor(), tuned_parameters, cv=5)","30f4e503":"# fit the model\nmodel_KNN=model_KNN.fit(X_train,y_train)","189d6a15":"# evaluate on the training set\nprint('Training score:', model_KNN.best_score_)\n\nevaluation = evaluation.append({'Model' : 'KNN' ,\n                                'R-squared (training)' : model_KNN.best_score_, \n                                '5-Fold Cross Validation':''} ,ignore_index=True)","a8d41204":"predictions= model_KNN.best_estimator_.predict(X_test)\ncreate_csv_predictions(predictions, 'predictions_KNN.csv')","07767b02":"# create a LassoCV model instance\nmodel_LassoCV = LassoCV(alphas=np.logspace(-50,100, 100), cv=5)\n\n# fit the model\nmodel_LassoCV.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model_LassoCV.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model_LassoCV.score(X_train, y_train))\n\nevaluation = evaluation.append({'Model' : 'LassoCV' ,\n                                'R-squared (training)' : model_LassoCV.score(X_train, y_train), \n                                '5-Fold Cross Validation':''} ,ignore_index=True)","b2fea587":"# create a Lasso model instance\nmodel_lasso = Lasso(alpha=80)\n\n# get cross validated scores\nscores = cross_val_score(model_lasso, X_train, y_train, cv=5)\nprint(\"Cross-validated training scores:\", scores)\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel_lasso.fit(X_train, y_train)\nprint(\"Training Score:\", model_lasso.score(X_train, y_train))\n\nevaluation = evaluation.append({'Model' : 'Lasso' ,\n                                'R-squared (training)' : model_lasso.score(X_train, y_train), \n                                '5-Fold Cross Validation': scores.mean()} ,ignore_index=True)","c5d3aff0":"# collect the model coefficients in a dataframe\ndf_coef = pd.DataFrame(model_lasso.coef_, index=X_train.columns,\n                       columns=['coefficients'])\n\n# calculate the absolute values of the coefficients\ndf_coef['coef_abs'] = df_coef.coefficients.abs()\ndf_coef.head()","537d7d8b":"predictions=model_lasso.predict(X_test)\ncreate_csv_predictions(predictions, 'predictions_lasso80.csv')","a7e69079":"# create a RidgeCV model instance\nmodel_RidgeCV = RidgeCV(alphas=np.logspace(-1, 2, 10), cv=5)\n\n# fit the model\nmodel_RidgeCV.fit(X_train, y_train)\n\n# get the best alpha\nprint('Best alpha:', model_RidgeCV.alpha_)\n\n# evaluate on the training set\nprint('Training score:', model_RidgeCV.score(X_train, y_train))\n\nevaluation = evaluation.append({'Model' : 'RidgeCV' ,\n                                'R-squared (training)' : model_RidgeCV.score(X_train, y_train), \n                                '5-Fold Cross Validation':''} ,ignore_index=True)","2e6d29c2":"# create a Ridge model instance\nmodel_Ridge = Ridge(alpha=1000)\n\n# get cross validated scores\nscores = cross_val_score(model_Ridge, X_train, y_train, cv=5)\nprint(\"Cross-validated training scores:\", scores)\nprint(\"Mean cross-validated training score:\", scores.mean())\n\n# fit and evaluate the data on the whole training set\nmodel_Ridge.fit(X_train, y_train)\nprint(\"Training Score:\", model_Ridge.score(X_train, y_train))\n\nevaluation = evaluation.append({'Model' : 'Ridge' ,\n                                'R-squared (training)' : model_Ridge.score(X_train, y_train), \n                                '5-Fold Cross Validation': scores.mean()} ,ignore_index=True)","831e10e7":"# collect the model coefficients in a dataframe\ndf_coef = pd.DataFrame(model_Ridge.coef_, index=X_train.columns,\n                       columns=['coefficients'])\n\n# calculate the absolute values of the coefficients\ndf_coef['coef_abs'] = df_coef.coefficients.abs()\ndf_coef.head()","3eea7e48":"predictions=model_Ridge.predict(X_test)\ncreate_csv_predictions(predictions, 'predictions_model_Ridge_1000.csv')","30495af3":"predictions=pd.DataFrame(predictions)\npredictions['Id']= [i for i in range(1461, 2920)]\npredictions.set_index('Id')\npredictions.rename(columns={'0':'SalePrice'},inplace = True)","c98313a5":"predictions.rename(columns={0:'SalePrice'},inplace = True)","c23a535f":"predictions.set_index('Id',inplace=True)\npredictions.head()","2391ad9c":"evaluation","dfe7a9b5":"corrmat = new_df_train.corr()\nk = 11\ncols = corrmat.nlargest(k,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(new_df_train[cols].values.T)\n\nl = list( cm[0])\nl\ncol = cols[1:11]\ncol\nl2 =l[1:11]\nl2\n\nf, ax = plt.subplots(figsize=(8,5))\nplt.xticks(rotation='90')\nsns.barplot(x= col, y=l2)\nplt.xlabel('Features')\nplt.title('Strongest Features')\n","23ec6e8a":"f, ax = plt.subplots(figsize=(8,5))\nplt.xticks(rotation='90')\nsns.barplot(x=evaluation['Model'], y=evaluation['R-squared (training)'])\nplt.xlabel('Models')\nplt.ylabel('R-squared')\nplt.title('R-squared for each model')\n","9c9e0247":"#### Correlation map for top 15 features correlated with SalePrice","a9de210e":"#### In this model, we created a Lasso CV model after calculating the best alpha. We applied this model on the entire features in X_train. This model scored the highest score for test data in Kaggle.","ed2c9398":"#### In this model, we created a K neighbors regressor after performing a grid search. We applied the model on the entire features in X_train.","18b4e7a2":"### Preprocessing and Modeling","6f195865":"#### We created a dataframe for the predictions and formatted it as required by Kaggle.","0627ee26":"Kaggle score \n![Screen%20Shot%202019-11-03%20at%208.41.55%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.41.55%20AM.png)","bb8d6502":"#### Load datasets again, in this part we will go with another approach here we will try dummies","9ddb0ec3":"#### Create new dataset contains dummies and dataset that contains numeric features that we created above","cc031d53":"Kaggle score \n![Screen%20Shot%202019-11-03%20at%208.44.57%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.44.57%20AM.png)","e2ce3eff":"#### Correlation map to see how features are correlated with SalePrice \"contains categorical features that we converted it before\"","d5d87dad":"#### We change each string in the categorical features that strongly correlated with SalePrice to numbers ","ae2c53d2":"For modeling, we made multiple models and evaluated the score of the train dataset, then submitted each model in Kaggle. The best test score was for Lasso CV model.","bc7394fa":"This is Kaggle score we got for the best model\n![Screen%20Shot%202019-11-03%20at%208.23.37%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.23.37%20AM.png)","6ae6c604":"#### In order to scale the data, we selected the MinMaxScaler method with the default range of (0,1).","7e98c8d7":"#### Display the data types of each feature in train and test datasets","f2abaf7b":"In this project, We were solving the problem of setting a house price based on the features of that house. We got the dataset of almost 3,000 houses in Iowa State along with 79 of their features. We splitted the data into train and test data. After that, we calculated the baseline by creating a model that predicts the house prices as a constant, which is the mean of all house prices in the train data. Then, we trained several models; Linear regresson after manually selecting the features, Lasso regression on the entire dataset, Ridge regression on the entire dataset, KNN regression, and decision tree regression. After that, we submitted the predictions of each model individually to Kaggle and got the score of each. The best score was for Lasso regression.<br>\nFor Lasso regression, which worked best for our dataset, here are the features that had the highest effects on the sale price of the houses:<br>\n- OverallQual\n- GrLivArea\n- GarageCars\n- GarageArea\n- TotalBsmtSF\n- 1stFlrSF\n- FullBath\n- BsmtQual_Ex\n- TotRmsAbvGrd\n- YearBuilt\n\nIn the end, we recommend that anyone considering getting a house to think about this list and priorities their options, because these are the features that have the strongest effects on the sale price <br>\n","75f7a520":"#### Replacing missing values with median of LotFrontage for each neighborhood (group by)","51b7defc":"#### In this model, we created a linear regression model on the features we manually selected.","ef41825b":"#### Display bar plot for number of missing data by feature","caf27494":"##### Replacing missing values with None in these columns the missing values means that it doesn't exist","8fe7a2a9":"Kaggle score\n![Screen%20Shot%202019-11-03%20at%208.27.49%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.27.49%20AM.png)","76f13f4d":"#### In this model, we created a decision tree after performing a grid search. We applied the model on the entire features in X_train.","9f709952":"#### In order to calculate the baseline, we used DummyRegressor. We set the predictions as a constant number, which is the mean of the target (y_train).","62d27396":"at the bottom right two with extremely large GrLivArea that are of a low price! These values are huge outliers. Therefore, we can safely delete them","ed113f7d":"#### Create dummies","ee865681":"#### Replacing missing values in the same way that we did above ","e9a44847":"#### Drop the 'Id' colum since it's unnecessary for  the prediction process","cccf39dc":"### Problem Statement:\n> In the real estate industry, it can be so complicated to determine a house price. That is because it depends on so many characterestics of the house itself. Therefore, in this project, we aquired a dataset containing 79 explanatory features describing almost every aspect of residential homes in Ames, Iowa, USA. After that, we built some regression models to predict each house price. Then, we selected the best model in terms of the test data score provided by Kaggle.","3fd18fa2":"#### Defind outliers","2e98f6b4":"As we see in the result below the Utilities column has two values: all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA, therefore the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. So we can then safely remove it","2ffa3fd1":"#### Correlation map to see how features are correlated with SalePrice","a88996d4":"#### Correlation map for top 15 features correlated with SalePrice \"contains categorical features that we converted it before\"","af248818":"#### Display the columns with null values and number of nulls","9ef6c7b8":"#### Display columns that is in X_train and not in X_test ","64030ad6":"#### Add the columns above to X_test and set it to null then replace null values with 0 , and sort the columns places ","186679b8":"#### In this model, we created a Ridge CV model after calculating the best alpha. We applied this model on the entire features in X_train.","e974aad2":"Kaggle score\n![Screen%20Shot%202019-11-03%20at%208.23.37%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.23.37%20AM.png)","14c5ca68":"### Preprocessing and Modeling","b52a55ae":"#### print summary statistics","50555307":"#### Deleting outliers","878a3cc1":"## Conclusion and Recommendations","9a05e270":"Kaggle score\n![Screen%20Shot%202019-11-03%20at%208.29.46%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.29.46%20AM.png)","f7ebf202":"#### Display list for categorical features which has strong correlation with SalePrice that we have extract it manually ","072fa45d":"#### Split data to train and test ","485edefb":"#### Recheck remaining missing values if any","29e7a6c1":"## Another approach: using dummies","078922ee":"#### SalePrice distribution","be1e8778":"#### Create two datasets that are contains only numeric values","ad5ec0ca":"Kaggle score\n![Screen%20Shot%202019-11-03%20at%208.35.26%20AM.png](attachment:Screen%20Shot%202019-11-03%20at%208.35.26%20AM.png)","46d0cb96":"#### Display box plots for all categorical features to see which ones has strong correlation with SalePrice","99d08a2f":"#### print the head of train and test data","7773aa3d":"#### Getting all numeric features in new dataset","b470563b":"### Data Cleaning and EDA","59167018":"#### Replacing missing values with most frequent string (mode)","7e8bb9a8":"The target variable is right skewed","ebb726d9":"#### We imported StandardScaler and applied it to both X_train and X_test.","a7fe53da":"#### Replacing missing values with 0 in these columns the missing values means that it does not exist, it is related to something such as garage or basement so if these are not exist the values related to it does not exist too ","8c2e8fac":"#### Replacing missing values with Typ because data description says NA means typical"}}