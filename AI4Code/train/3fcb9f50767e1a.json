{"cell_type":{"e6ec84c5":"code","fbbe9b9f":"code","25dca01e":"code","b812f1b3":"code","fd137ffe":"code","e85c6268":"code","a145b7d5":"code","b515b4f4":"code","f1952149":"code","d418cdb9":"code","3ef475a6":"code","7df361a8":"code","0719ed31":"code","3484f221":"code","a891cd92":"code","9f9471b1":"code","beaf1021":"code","7751da78":"code","8cf66039":"code","bebd17c3":"code","9132b4d6":"code","8e830a14":"code","a8afd8a1":"code","7c97767f":"code","9417a8ea":"markdown","00a95647":"markdown","195e2d36":"markdown","b9caec41":"markdown","355fd15e":"markdown","a28114c3":"markdown","d116f590":"markdown","80310870":"markdown","1d52b840":"markdown"},"source":{"e6ec84c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as pex\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbbe9b9f":"from sklearn.datasets import load_breast_cancer\n\ndata = load_breast_cancer()\ndf = pd.DataFrame(data['data'],columns=data['feature_names'])","25dca01e":"# Scale the data\ndf = (df - df.mean(axis=0)) \/ df.std(axis=0)\ndf.head()\ndf['status'] = data['target']","b812f1b3":"cov_matrix = np.cov(df.drop('status',axis=1), rowvar=False)\ncov_matrix.shape","fd137ffe":"values, vectors = np.linalg.eig(cov_matrix)","e85c6268":"print(\"Values shape: \" + str(values.shape))\nprint(\"Vectors shape: \" + str(vectors.shape))","a145b7d5":"\"\"\"\n- Each eigenvector has 30 entries, one for each feature.\n- You can see how much each feature \"contributes\" (feature importance) to each axis \/ eigenvector\n  by comparing the absolute values of entries in each eigenvector. For example,\n\"\"\"\n\nvectors[:,0]","b515b4f4":"plt.figure(figsize=(6,4),dpi=100)\nplt.plot(values,marker='s',color='red',lw=1)\nplt.xlabel('Number corresponding to each Eigenvalue')\nplt.ylabel('Variance')\nplt.title('Variances for Each Axis')\nplt.show()","f1952149":"# Let's take the top 3 eigenvectors and project data onto them.\n# From the plot we see that the eigenvalues are already sorted, so we can just take the first 3 indices.\ntop_3_vectors = vectors[:,np.array([0,1,2])]","d418cdb9":"# Project data down to 3 axes by computing dot product\nprincipal_comp = np.dot(df.drop('status',axis=1).values, top_3_vectors)","3ef475a6":"print(principal_comp.shape) #Correct shape!","7df361a8":"pex.scatter_3d(x=principal_comp[:,0],y=principal_comp[:,1],z=principal_comp[:,2],color=df.status, color_continuous_scale=pex.colors.sequential.Viridis)","0719ed31":"plt.figure(figsize=(10,6))\nsns.heatmap(np.abs(top_3_vectors),yticklabels=df.drop('status',axis=1).columns,cmap = 'coolwarm')","3484f221":"from sklearn.manifold import TSNE\nmodel = TSNE(n_components=2 ,init='pca')","a891cd92":"reduced = model.fit_transform(df.drop('status',axis=1))","9f9471b1":"model.kl_divergence_ # Kullback-Leibler Divergence - The difference between the random and original distributions after optimization","beaf1021":"pex.scatter(x=reduced[:,0],y=reduced[:,1],color=df['status'], color_continuous_scale=pex.colors.sequential.Viridis)","7751da78":"perplexity = np.arange(5,55,5) # Hyperparameter which is essentially an estimate for the number of nearest neighbors for east point.\nkl = []\n\nfor i in perplexity:\n    model = TSNE(n_components=2 ,init='pca', perplexity=i)\n    reduced = model.fit_transform(df.drop('status',axis=1))\n    kl.append(model.kl_divergence_)\n    \nplt.figure(figsize=(6,4),dpi=100)\nplt.plot(perplexity,kl,marker='s',color='red',lw=1)\nplt.title('KL Divergence After t-SNE Optimization for Varying Perplexity Values')\nplt.ylabel('Divergence')\nplt.xlabel('Perplexity Values')\nplt.show()","8cf66039":"# Let's try perplexity = 50, since it produced the lowest divergence\nmodel = TSNE(n_components=2 ,init='pca', perplexity=50)\nreduced = model.fit_transform(df.drop('status',axis=1))\npex.scatter(x=reduced[:,0],y=reduced[:,1],color=df['status'], color_continuous_scale=pex.colors.sequential.Viridis)","bebd17c3":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n\nmodel = lda(n_components=1) # n_components in this case is 1 less than the number of classes.","9132b4d6":"newData = model.fit_transform(df.drop('status', axis=1), df['status'])\nnewData = pd.DataFrame([newData[:,0],df['status']])\nnewData = newData.T\nnewData['y'] = [0 for _ in range(newData.shape[0])]","8e830a14":"model.explained_variance_ratio_ # Variance of data explained by the axis projected upon","a8afd8a1":"pex.scatter(newData, x=0, y='y', color=1, color_continuous_scale=pex.colors.sequential.Viridis)","7c97767f":"plt.figure(figsize=(10,6))\nsns.heatmap(model.coef_, xticklabels=df.drop('status',axis=1).columns,cmap = 'coolwarm')\nplt.show()","9417a8ea":"The plot above illustrates the separation of classes after the t-SNE approach. As you can see, there are some outliers, but overall there is relatively clear class separation. Let's try experimenting with the perplexity hyperparameter below.","00a95647":"## t-Distributed Stochastic Neighbor Embedding\nThis technique involves transforming the data to a lower-dimensional space using similarity scores, Normal, and t-distributions.\n- It will first create a similarity matrix incorporating similarity scores for all pairwise combinations of data points.\n- The similarity score for two points is calculated by converting the distance between the points to probability using the Normal Distribution.\n- Then all points will be randomly placed on the lower-dimensional space. A new similarity matrix will be calculated for this space, instead using the t-Distribution.\n- Using gradient descent, the points will be adjusted away from other points and closer to their own cluster by making the new similarity matrix as close as possible to the original similarity matrix.","195e2d36":"# Dimensionality Reduction Techniques: PCA, tSNE, LDA\nThis notebook will perform and visualize various dimensionality reduction techniques on Scikit-learn's Breast Cancer Dataset.\nTechniques covered include:\n- Principal Component Analysis\n- t-Distributed Stochastic Neighbor Embedding\n- Linear Discriminant Analysis","b9caec41":"Here, you can see the feature importances associated with this class; mean radius stands out the most.","355fd15e":"## Principal Component Analysis (PCA)\nThis technique involves projecting the high-dimensional data onto orthogonal axes that maximize the variance in the data.\n\n- We will perform this manually by taking the eigendecomposition of the covariance matrix of the data. \n- The eigenvectors represent the orthogonal axes which we will project the data onto. \n- Each eigenvalue represents the variance of the data when projected onto the axis represented by its corresponding eigenvector.\n- To visualize the data in reduced dimensions, we will choose 3 axes \/ eigenvectors that preserve the most variance in the data.\n- Therefore, these chosen eigenvectors will have the largest eigenvalues.\n","a28114c3":"With this plot, we can see which features contributed most to each principal component. For the first, second, and third axes, we can see that\nthe mean concave points, mean fractal dimension, and texture error features contributed the most in maximizing the variance respectively.","d116f590":"We can see the clear separation of classes from this plot, illustrating the benefits of dimensionality reduction. With a few more principal components to maximize the variance, we could train a SVM to take advantage of the relatively clear separation of classes to predict Breast Cancer Status.","80310870":"## Linear Discriminant Analysis\nLDA is another dimensionality reduction. Like PCA, it involves projecting the data onto axes; however, its goal is to choose axes that maximize class separability and minimize intra-class scatter simultaneously. With a binary-class dataset, the data will be projected onto a line.\nWith a n-class dataset, the data will be projected onto an (n-1)-dimensional-space.","1d52b840":"This plot shows the difference in class distributions for the data projected on a single axis. Overall, we have relatively clear separation. Following from this, LDA can also be used as a classifier in addition to a dimensionality reduction technique."}}