{"cell_type":{"e84d0176":"code","43af5928":"code","d453dbfe":"code","f3396557":"code","4f68b59c":"code","60aad6e6":"code","e31ba4b3":"code","97b3f047":"code","6dc1aa58":"code","253eebbf":"code","4ee5b8d6":"code","5f2d3e56":"code","51930304":"code","4309944d":"markdown","bd1ffde8":"markdown","3a82f689":"markdown","8632db10":"markdown","932018e7":"markdown","874e4a93":"markdown","d71080a2":"markdown","87f2120f":"markdown"},"source":{"e84d0176":"# libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport os\nimport pdb\nimport json\nfrom os.path import join as path_join\nimport cv2\nfrom skimage import measure","43af5928":"# load data\ndef load_data(path):\n    tasks = pd.Series()\n    for file_path in os.listdir(path):\n        task_file = path_join(path, file_path)\n        with open(task_file, 'r') as f:\n            task = json.load(f)\n        tasks[file_path[:-5]] = task\n    return tasks\n\n# only look as train tasks for now\ntrain_tasks = load_data('..\/input\/abstraction-and-reasoning-challenge\/training\/')\n#evaluation_tasks = load_data('abstraction-and-reasoning-challenge\/evaluation\/')\n#test_tasks = load_data('abstraction-and-reasoning-challenge\/test\/')","d453dbfe":"class ARC_solver:\n    def __init__(self, task_num):\n        self.task_num = task_num\n        # standardize plotting colors\n        self.cmap = colors.ListedColormap(['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n                                         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n        self.norm = colors.Normalize(vmin = 0, vmax = 9)\n        # initialize objects-related things\n        self.identified_objects = []\n        self.io_inx = [] # the original index of the identified objects (io)\n        self.io_height = [] # height of io\n        self.io_width = [] # width of io\n        self.io_pixel_count = [] # count of non-background pixels\n        self.io_size = [] # overall grid size\n        self.io_unique_colors = [] # number of unique colors\n        self.io_main_color = [] # the dominating color\n        \n    def reset(self):\n        self.identified_objects = []\n        self.io_inx = [] \n        self.io_height = [] \n        self.io_width = [] \n        self.io_pixel_count = [] \n        self.io_size = [] \n        self.io_unique_colors = [] \n        self.io_main_color = []\n    \n    def plot_task(self):\n        # plot examples of task input-output pairs \n        task = train_tasks[self.task_num]\n        cmap = self.cmap\n        norm = self.norm\n        fig, axs = plt.subplots(1, 5, figsize = (8,2))\n        axs[0].text(0.5, 0.5, 'Task', horizontalalignment = 'center', verticalalignment = 'center', fontsize = 15)\n        axs[0].get_xaxis().set_visible(False)\n        axs[0].get_yaxis().set_visible(False)\n        axs[0].axis('off')\n\n        axs[1].imshow(task['train'][0]['input'], cmap=cmap, norm=norm)\n        axs[1].axis('off')\n        axs[1].set_title('Train Input')\n        axs[2].imshow(task['train'][0]['output'], cmap=cmap, norm=norm)\n        axs[2].axis('off')\n        axs[2].set_title('Train Output')\n        axs[3].imshow(task['test'][0]['input'], cmap=cmap, norm=norm)\n        axs[3].axis('off')\n        axs[3].set_title('Test Input')\n        axs[4].imshow(task['test'][0]['output'], cmap=cmap, norm=norm)\n        axs[4].axis('off')\n        axs[4].set_title('Test Output')\n        plt.tight_layout()\n        plt.show()\n    \n    def plot_identified_objects(self, identified_objects, title = 'objects'):\n        # do not plot anything in the following situations\n        if len(identified_objects) == 0:\n            print('No objects were identified.')\n            return\n        if len(identified_objects) > 10:\n            print('Way too many objects (>10). Not gonna plot them.')\n            return\n        \n        fig, axs = plt.subplots(1, len(identified_objects) + 1, figsize = (8,2))\n        for i in range(len(identified_objects) + 1):\n            if i == 0:\n                axs[0].text(0.5, 0.5, title, horizontalalignment = 'center', verticalalignment = 'center', fontsize = 15)\n                axs[0].get_xaxis().set_visible(False)\n                axs[0].get_yaxis().set_visible(False)\n                axs[0].axis('off')\n            else:\n                obj = identified_objects[i-1]\n                axs[i].imshow(obj, cmap = self.cmap, norm = self.norm)\n                axs[i].axis('off')\n                axs[i].set_title('object{}'.format(i))\n        plt.tight_layout()\n        plt.show()\n    \n    def get_background(self, image):\n        # if image contains 0 \n        if 0 in image:\n          background = 0\n        # else use the most frequent pixel color\n        else: \n          unique_colors, counts = np.unique(image, return_counts = True)\n          background = unique_colors[np.argmax(counts)]\n        return background\n    \n    def check_pairs(self, inx_pairs, this_pair, return_inx = False):\n        # check if this_pair is in inx_pairs\n        match = []\n        for pair in inx_pairs:\n          if pair[0] == this_pair[0] and pair[1] == this_pair[1]:\n            match.append(True)\n          else:\n            match.append(False)\n        if return_inx:\n          return any(match), np.where(match)\n        else:\n          return any(match)\n    \n    def check_neighbors(self, all_pairs, this_pair, objectness, this_object):\n        # all_pairs: an array of index pairs for all nonzero\/colored pixels\n        # this_pair: the index pair whose neighbors will be checked\n        # objectness: an array with the shape of original image, storage for how much objectness has been identified\n        # this_object: the current object we are looking at\n        row_inx = this_pair[0]\n        col_inx = this_pair[1]\n        objectness[row_inx, col_inx] = this_object\n        # find if any neighboring pixels contain color\n        if self.check_pairs(all_pairs, [row_inx-1, col_inx-1]): # up-left\n          objectness[row_inx-1, col_inx-1] = this_object\n        if self.check_pairs(all_pairs, [row_inx-1, col_inx]): # up\n          objectness[row_inx-1, col_inx] = this_object \n        if self.check_pairs(all_pairs, [row_inx-1, col_inx+1]): # up-right\n          objectness[row_inx-1, col_inx+1] = this_object\n        if self.check_pairs(all_pairs, [row_inx, col_inx-1]): # left\n          objectness[row_inx, col_inx-1] = this_object\n        if self.check_pairs(all_pairs, [row_inx, col_inx+1]): # right\n          objectness[row_inx, col_inx+1] = this_object\n        if self.check_pairs(all_pairs, [row_inx+1, col_inx-1]): # down-left\n          objectness[row_inx+1, col_inx-1] = this_object\n        if self.check_pairs(all_pairs, [row_inx+1, col_inx]): # down\n          objectness[row_inx+1, col_inx] = this_object\n        if self.check_pairs(all_pairs, [row_inx+1, col_inx+1]): # down-right\n          objectness[row_inx+1, col_inx+1] = this_object\n        return objectness\n    \n    def identify_object_by_color(self, true_image, background = 0):\n        # identify obeject by the color only \n        unique_colors = np.unique(true_image)\n        for i, color in enumerate(unique_colors):\n          image = np.copy(true_image) # make a copy from original first\n          if color == background: \n            continue\n          image[image != color] = background\n          inx = np.where(image == color)\n          obj = image[np.min(inx[0]):np.max(inx[0])+1, np.min(inx[1]):np.max(inx[1])+1]\n          # append the object attributes\n          self.identified_objects.append(obj)\n          self.io_inx.append(inx)\n          self.io_height.append(obj.shape[0])\n          self.io_width.append(obj.shape[1])\n          self.io_pixel_count.append(obj[obj != background].shape[0])\n          self.io_size.append(obj.size)\n          nc, c = np.unique(obj, return_counts = True)\n          self.io_unique_colors.append(nc)\n          self.io_main_color.append(nc[np.argmax(c)])\n    \n    def identify_object_by_isolation(self, image, background = 0):\n        # identify all objects by physical isolation on the given image\n        all_pairs = np.array(np.where(image != background)).T\n        objectness = np.zeros(image.shape)\n        this_object = 1\n        while len(all_pairs) >= 1:\n          init_pair = all_pairs[0] # start with the first pair\n          objectness = self.check_neighbors(all_pairs, init_pair, objectness, this_object)\n          # get a list of index pairs whose neghbors haven't been checked\n          unchecked_pairs = np.array(np.where(objectness == this_object)).T\n          checked_pairs = np.zeros((0,2)) \n          # check all the index pairs in the expanding unchecked_pairs untill all have been checked\n          while len(unchecked_pairs) != 0:\n            this_pair = unchecked_pairs[0]\n            objectness = self.check_neighbors(all_pairs, this_pair, objectness, this_object)\n            # append the checked_pairs\n            checked_pairs = np.vstack((checked_pairs, this_pair))\n            # get all index pairs for the currently identified object\n            current_object_pairs = np.array(np.where(objectness == this_object)).T\n            # delete the checked pairs from current object pairs\n            checked_inx = []\n            for pair in checked_pairs:\n              _, inx = self.check_pairs(current_object_pairs, pair, return_inx = True)\n              checked_inx.append(inx[0][0])\n            unchecked_pairs = np.delete(current_object_pairs, checked_inx, axis = 0)\n\n          # store this object to identified_objects\n          current_object_pairs = np.array(np.where(objectness == this_object)).T\n          cop = current_object_pairs.T\n          obj = image[np.min(cop[0]):np.max(cop[0])+1, np.min(cop[1]):np.max(cop[1])+1]\n          # delete the current object pairs from all_pairs \n          cop_inx = []\n          for pair in current_object_pairs:\n            _, this_cop_inx = self.check_pairs(all_pairs, pair, return_inx = True)\n            cop_inx.append(this_cop_inx[0][0])\n          all_pairs = np.delete(all_pairs, cop_inx, axis = 0)\n          # append the object attributes\n          self.identified_objects.append(obj)\n          self.io_inx.append(inx)\n          self.io_height.append(obj.shape[0])\n          self.io_width.append(obj.shape[1])\n          self.io_pixel_count.append(obj[obj != background].shape[0])\n          self.io_size.append(obj.size)\n          nc, c = np.unique(obj, return_counts = True)\n          self.io_unique_colors.append(nc)\n          self.io_main_color.append(nc[np.argmax(c)])\n          # start identifying a new object\n          this_object += 1\n        return objectness\n    \n    def identify_object_by_color_isolation(self, true_image, background = 0):\n        # identify objects first by color then by physical isolation\n        unique_colors = np.unique(true_image)\n        for i, color in enumerate(unique_colors):\n          image = np.copy(true_image) # make a copy from the original first\n          if color == background:\n            continue\n          # identify objects by isolation in this color only \n          image[image != color] = background\n          self.identify_object_by_isolation(image, background = background)\n    \n    def identify_object(self, image, method):\n        # a wrapper of different methods\n        # in the future method can be a parameter to be learned\n        # 1 = by_color, 2 = by_isolation, 3 = by_color_isolation\n        background = self.get_background(image)\n        if method == 1:\n          self.identify_object_by_color(image, background)\n        elif method == 2:\n          self.identify_object_by_isolation(image, background)\n        elif method == 3:\n          self.identify_object_by_color_isolation(image, background)\n    \n    def extract_object_pairs(self, method):\n        # extract all objects from input and output\n        num_examples = len(train_tasks[self.task_num]['train'])\n        self.input_objects = []\n        self.input_objects_inx = []\n        self.input_objects_attr = {\"height\": [],\n                                   \"width\": [],\n                                   \"pixel_count\": [],\n                                   \"size\": [],\n                                   \"unique_colors\": [],\n                                   \"main_color\": []} \n        self.output_objects = []\n        self.output_objects_inx = []\n        self.output_objects_attr = {\"height\": [],\n                                   \"width\": [],\n                                   \"pixel_count\": [],\n                                   \"size\": [],\n                                   \"unique_colors\": [],\n                                   \"main_color\": []} \n\n        # iterate through training examples \n        for i in range(num_examples):\n          this_input = np.array(train_tasks[self.task_num]['train'][i]['input'])\n          this_output = np.array(train_tasks[self.task_num]['train'][i]['output'])\n          # identify all objects in inputs\n          arc.reset()\n          self.identify_object(this_input, method = method)\n          self.input_objects.append(self.identified_objects)\n          self.input_objects_inx.append(self.io_inx)\n          self.input_objects_attr['height'].append(self.io_height)\n          self.input_objects_attr['width'].append(self.io_width)\n          self.input_objects_attr['pixel_count'].append(self.io_pixel_count)\n          self.input_objects_attr['size'].append(self.io_size)\n          self.input_objects_attr['unique_colors'].append(self.io_unique_colors)\n          self.input_objects_attr['main_color'].append(self.io_main_color)\n          # identify all objects in outputs\n          arc.reset()\n          self.identify_object(this_output, method = method)\n          self.output_objects.append(self.identified_objects)\n          self.output_objects_inx.append(self.io_inx)\n          self.output_objects_attr['height'].append(self.io_height)\n          self.output_objects_attr['width'].append(self.io_width)\n          self.output_objects_attr['pixel_count'].append(self.io_pixel_count)\n          self.output_objects_attr['size'].append(self.io_size)\n          self.output_objects_attr['unique_colors'].append(self.io_unique_colors)\n          self.output_objects_attr['main_color'].append(self.io_main_color)","f3396557":"# randomly select 20 task numbers\nTASK_NUM = np.random.randint(1,400, size = 20)\nfor task_num in TASK_NUM:\n    arc = ARC_solver(task_num)\n    arc.plot_task()\n    # select an image for object identification\n    # WE USE TRAIN INPUT\n    image = np.array(train_tasks[task_num]['train'][0]['input'])\n\n    # identify objects only by color\n    arc.reset()\n    arc.identify_object(image, method = 1)\n    arc.plot_identified_objects(arc.identified_objects, title = 'by color')\n\n    # identify objects only by isolation\n    arc.reset()\n    arc.identify_object(image, method = 2)\n    arc.plot_identified_objects(arc.identified_objects, title = 'by isolation')\n\n    # identify objects by color and isolation\n    arc.reset()\n    arc.identify_object(image, method = 3)\n    arc.plot_identified_objects(arc.identified_objects, title = 'by both')","4f68b59c":"task_num = np.random.randint(1,400)\narc = ARC_solver(task_num)\narc.plot_task()\nimage = np.array(train_tasks[task_num]['train'][0]['input'])\narc.identify_object(image, method = 2) # by isolation\narc.plot_identified_objects(arc.identified_objects)","60aad6e6":"print(\"Identified object 1 has height {}, width {}, pixel count {}.\".format(arc.io_height[0], arc.io_width[0], arc.io_pixel_count[0]))\nprint(\"It has grid size {}, unique colors of {}, the dominating color is {}.\".format(arc.io_size[0], arc.io_unique_colors[0], arc.io_main_color[0]))","e31ba4b3":"task_num = np.random.randint(1,400)\narc = ARC_solver(task_num)\narc.plot_task()\nimage = np.array(train_tasks[task_num]['train'][0]['input'])\narc.identify_object(image, method = 2) # by isolation\narc.plot_identified_objects(arc.identified_objects)","97b3f047":"print(\"Identified object 1 has height {}, width {}, pixel count {}.\".format(arc.io_height[0], arc.io_width[0], arc.io_pixel_count[0]))\nprint(\"It has grid size {}, unique colors of {}, the dominating color is {}.\".format(arc.io_size[0], arc.io_unique_colors[0], arc.io_main_color[0]))","6dc1aa58":"task_num = np.random.randint(1,400)\narc = ARC_solver(task_num)\narc.plot_task()\narc.extract_object_pairs(method = 1) # by color\n# plot identified objects from train[0]input\narc.plot_identified_objects(arc.input_objects[0], title = 'input')\n# plot identified objects from train[0]output\narc.plot_identified_objects(arc.output_objects[0], title = 'output')","253eebbf":"arc.input_objects_attr['pixel_count'][0]","4ee5b8d6":"arc.output_objects_attr['pixel_count'][0]","5f2d3e56":"arc.input_objects_attr['main_color'][0]","51930304":"arc.output_objects_attr['main_color'][0]","4309944d":"## What's next?\n1. I think it's easiest to start with tasks that have clear input-output pairs with some simple transformation. \n2. The process goes as following:\n   1. Extract all input-output pairs\n   2. Pad them to the same size\n   3. Push them through a simple CNN\n3. The advantage of this method is that now we can increase the data size from #examples to #objects. \n4. If CNN method doesn't work (probably won't), then perhaps we can iterate through all the objects attributes and find the ones that are different (e.g. color, size). And figure out what's the rule (this is the hardest part).","bd1ffde8":"## Define a general class ARC_solver","3a82f689":"## Relevant attributes of identified objects are stored","8632db10":"## Let's have a look at the identified objects","932018e7":"## Extract input-output pairs if any","874e4a93":"## Input and output objects attributes are stored \n","d71080a2":"## Basic idea\n- A lot of (but not all) tasks rely on the concept of 'objectness'.\n- **Simple example**: use largest object from input as output \/ use the uniquely colored object from input as output\n- **Intermediate example**: identify all objects from input and change their colors based on some rule that is based on their attributes (e.g. height, size)\n- **Advanced examlpe**: identify all objects from input, and glue them together with a common fixation point (e.g. a gray pixel)\n- So I thought I'd begin small, just write some funtions that can correctly identify objects and their attributes given an image\n- Once we have a list of input-output object pairs, we can then begin to understand the transformation and try to extract the transformation rules\n- I believe this is 'hard-coding the prior of objectness'\n\n*This is my first kernel. It is far from complete and I'm sure there are better ways to achieve object identification. If you have any suggestions, please comment below! Many thanks in advance :)*","87f2120f":"## Limitations\n 1. The current three methods: `by_color`, `by_isolation` and `by_color_isolation` works ok with most tasks that require some sort of object identification. But it DOES NOT work when, say, two rectangles with same color are adjacent to each other. The `check_neighbors` function will consider these two a part of the same object. This needs further work.\n 2. Initially I want to write a `identify_objects` function that can in one-shot correctly identify the objects as a human would. Later I realized this is quite difficult, as different tasks require different grouping principles. It needs to be learned (trial-and-error, or something else)\n 3. `get_background` assumes the background is the most frequent pixel color when it does not contain black. Obviously this logic can be further refined.\n 4. All the object identifiers do not understand if an object disappears beyond edge, consequently some remaining part of it are identified as separate objects (if by isolation). Maybe group the identified objects together based on color and\/or location (inx)?"}}