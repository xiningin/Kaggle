{"cell_type":{"12aa52e5":"code","6707af39":"code","fd9f8542":"code","a2fadb13":"code","43370a19":"code","7d250108":"code","9467d9bc":"code","67fe89d2":"code","66d8ad5a":"code","4ad9a5a8":"code","bee97ab7":"code","c9a75ad3":"code","0e0bf403":"code","3a55a242":"code","b7a02f74":"code","875fbfc3":"code","ce5c5fae":"code","a9dc418d":"code","838617c2":"code","24fd3225":"code","acd104b4":"code","168450d7":"code","fd32d9bf":"code","2cb6939d":"code","64d37d1d":"code","0becb928":"code","9dd4d3df":"code","eb25e10f":"code","977c1ddc":"code","87ada19a":"code","a1037200":"code","26d71253":"code","3f580e00":"code","90efb1fd":"code","f057c980":"code","fd3cd160":"code","b44b434a":"code","bc3d34ea":"code","c643ac1a":"code","4b5651f5":"code","cdcbbe32":"code","c844951f":"code","aa4211f8":"code","24551195":"code","9048eb4f":"code","76295afc":"code","48554908":"code","7e26f538":"code","b6672d36":"code","27e51807":"code","ca378d3f":"code","5bff2840":"code","96651eb8":"code","5bd9c3b4":"code","624ce3ba":"code","b804e4e9":"code","eb1b5071":"code","be618d0c":"code","6f1ffddf":"code","79098a16":"code","1c6c53be":"code","8f8eb6dc":"code","337b783a":"code","9dce4408":"code","ade87e19":"code","43dcca18":"code","e6a1925f":"code","6a2e3ace":"code","392965b4":"code","43e9abd0":"code","fa9fbd6e":"code","036b9bf3":"code","cf0d9646":"code","2fe2abde":"code","31d487a0":"code","84580671":"code","d8d0fe35":"markdown","d8c1c7e3":"markdown","22ebc9d7":"markdown","547e70cd":"markdown","ca79bd8b":"markdown","7b3b5b9e":"markdown","94e1c7b9":"markdown","2e920751":"markdown","d3caa56e":"markdown","805d43b0":"markdown","610f2ebd":"markdown"},"source":{"12aa52e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6707af39":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb","fd9f8542":"train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\nsample_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')","a2fadb13":"train.head()","43370a19":"delete_columns = ['id']\ntrain.drop(delete_columns, axis=1, inplace=True)\ntest.drop(delete_columns, axis=1, inplace=True)","7d250108":"train.info()","9467d9bc":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nplt.figure(figsize=(10,5))\nsns.histplot(train['target'], color='slategray', stat='frequency');","67fe89d2":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['target'], plot=plt)\nplt.show()","66d8ad5a":"len(train[train['target'] < 4])","4ad9a5a8":"to_drop = train[train['target'] < 4].index\ntrain.drop(to_drop, inplace=True)\ntrain = train.reset_index()\ntrain.info()\n\nplt.figure(figsize=(10,5))\nsns.histplot(train['target'], color='slategray', stat='frequency');","bee97ab7":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['target'], plot=plt)\nplt.show()","c9a75ad3":"delete_columns = ['index']\ntrain.drop(delete_columns, axis=1, inplace=True)","0e0bf403":"train.info()","3a55a242":"from pandas_profiling import ProfileReport\n\nprofile = ProfileReport(train, title='Pandas Profiling Report')","b7a02f74":"profile","875fbfc3":"# drop non-infarmative features: cat0, cat4, cat6, cat7\nnon_infarmative_features = ['cat0', 'cat4', 'cat6', 'cat7']\ntrain.drop(non_infarmative_features, axis=1, inplace=True)\ntest.drop(non_infarmative_features, axis=1, inplace=True)","ce5c5fae":"for c in train.columns:\n    if train[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values)+list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","a9dc418d":"VAL_SIZE          = 0.33   # 33%\nN_FOLDS           = 5\n\n# RANDOM_SEED\nRANDOM_SEED       = 42","838617c2":"y_train = train['target']\nX_train = train.drop('target', axis = 1)\nX_test = test","24fd3225":"y_train.shape, X_train.shape, X_test.shape","acd104b4":"X_train.info()","168450d7":"categorical_features = ['cat1', 'cat2', 'cat3', 'cat5', 'cat8', 'cat9']","fd32d9bf":"y_preds = []\nmodels = []\noof_train = np.zeros(len(X_train))\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\n\nparams = {\n    'metric': 'rmse', \n    'random_state': 42,\n    'n_estimators': 20000,\n    'reg_alpha': 4.739177609135503,\n    'reg_lambda': 0.023314865020550128,\n    'colsample_bytree': 0.2885575936307841,\n    'subsample': 0.9541423485889546,\n    'learning_rate': 0.02211501295658431,\n    'max_depth': 18,\n    'num_leaves': 305,\n    'min_child_samples': 118,\n    'min_data_per_groups': 89\n}\n\ny_preds = 0\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n    \n    lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n    \n    model = lgb.train(params, lgb_train,\n                      valid_sets=[lgb_train, lgb_eval],\n                      verbose_eval = 200,\n                      num_boost_round = 20000,\n                      early_stopping_rounds=1000)\n    \n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    y_preds += y_pred\/5\n    models.append(model)","2cb6939d":"pd.DataFrame(oof_train).to_csv('oof_train_kfold.csv', index=False)","64d37d1d":"y_preds_lgb = pd.DataFrame(y_preds)","0becb928":"# VERSION = 8","9dd4d3df":"# sample_sub['target'] = y_preds_lgb\n# sample_sub.to_csv(f'submission_LightGBM_{VERSION}.csv', index=False)","eb25e10f":"from catboost import Pool, CatBoostRegressor, cv\nfrom catboost import CatBoostClassifier","977c1ddc":"# CATBOOST\nITERATIONS        = 3000\nLR                = 0.01\nMAX_DEPTH         = 10\nN_ESTIMATORS      = 3000\nL2_LEAF_REG       = 3.5","87ada19a":"X = train.drop(['target'], axis=1,)\ny = train.target.values\nX_sub = test.copy()","a1037200":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","26d71253":"cat_features_ids = np.where(X_train.dtypes == object)[0].tolist()\ncat_features_ids","3f580e00":"ctb = CatBoostRegressor(learning_rate=LR,\n                          max_depth=MAX_DEPTH,\n                          n_estimators=N_ESTIMATORS,\n                          l2_leaf_reg=L2_LEAF_REG,\n                          random_seed=RANDOM_SEED,\n                          eval_metric='RMSE',\n                          custom_metric=['R2', 'MAE']\n                         )\nctb.fit(X_train, y_train,\n          eval_set=(X_test, y_test),\n          verbose_eval=100,\n          cat_features=categorical_features,\n          use_best_model=True,\n          plot=True\n         )","90efb1fd":"ctb.save_model('catboost_single_model_baseline.ctb')","f057c980":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nidx = np.argsort(ctb.feature_importances_)\n\nplt.figure(figsize=(17,8))\n\nsns.barplot(x=ctb.feature_importances_[idx], y=np.array(ctb.feature_names_)[idx])","fd3cd160":"predict_submission_ctb = ctb.predict(X_sub)\npredict_submission_ctb","b44b434a":"# VERSION = 9","bc3d34ea":"# sample_sub['target'] = predict_submission_ctb\n# sample_sub.to_csv(f'submission_CatBoost_v{VERSION}.csv', index=False)\n# sample_sub.head(10)","c643ac1a":"X = train.drop(['target'], axis=1,)\ny = train.target.values\nX_sub = test.copy()","4b5651f5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","cdcbbe32":"from xgboost import XGBRegressor","c844951f":"xgb_params = {\n    'colsample_bytree':0.4603,\n    'gamma':0.0468,\n    'learning_rate':0.05,\n    'max_depth':3,\n    'min_child_weight':1.7817,\n    'n_estimators':2200,\n    'reg_alpha':0.4640,\n    'reg_lambda':0.8571,\n    'subsample':0.5213,\n    'nthread':-1,\n    'eval_metric':'rmse',\n    'seed':RANDOM_SEED,\n#     'tree_method':'gpu_hist',\n#     'gpu_id':0\n}","aa4211f8":"xgb = XGBRegressor(**xgb_params)\nxgb.fit(X_train, y_train)","24551195":"y_pred_xgb = xgb.predict(X_test)\n","9048eb4f":"from sklearn import metrics\n\ndef RMSE(y, y_pred):\n    \n    MSE = metrics.mean_squared_error(y, y_pred)\n    print(f'MSE = {MSE:.4f}')\n\n    RMSE = np.sqrt(MSE)\n    print(f'RMSE = {RMSE:.4f}')\n\n    print('-------')","76295afc":"rmse = RMSE(y_test, y_pred_xgb)\nrmse","48554908":"predict_submission_xgb = xgb.predict(X_sub)\npredict_submission_xgb","7e26f538":"# VERSION = 3","b6672d36":"# sample_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\n# sample_sub['target'] = predict_submission_xgb\n# sample_sub.to_csv(f'submission_XGBoost_v{VERSION}.csv', index=False)\n# sample_sub.head(10)","27e51807":"xgb_results = sorted(zip(map(lambda x: round(x, 4), xgb.feature_importances_), X), reverse=True)\nctb_results = sorted(zip(map(lambda x: round(x, 4), ctb.feature_importances_), X), reverse=True)\n\npd.DataFrame(data = [[(m, v) for v, m in xgb_results],\n                     [(m, v) for v, m in ctb_results]],\n             index=['xgb','ctb']).T.head(5)","ca378d3f":"fig, ax1 = plt.subplots(figsize=(8,8))\n\nax1.set_xlabel('\u041f\u043e\u0440\u044f\u0434\u043e\u043a \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430')\nax1.set_ylabel('\u0412\u0435\u0441\u0430 Forest&Tree')\n\n# \u043e\u0441\u044c 1\nax1.plot([v for v,m in xgb_results][:100], 'b')\n\nax2 = ax1.twinx() #\u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0441\u0438\n\nax2.set_ylabel('\u0412\u0435\u0441\u0430 CatBoost')\nax2.plot([v for v,m in ctb_results][:100], 'y')\nfig.tight_layout()\nplt.show()\n","5bff2840":"res_dict = dict()\n\nfor i in range(20):\n        for mod in ([m for v,m in xgb_results],\n                    [m for v,m in ctb_results]):\n            if mod[i] in res_dict:\n                res_dict[mod[i]] += 1\n            else:\n                res_dict[mod[i]] = 1\n\nresulted = sorted(zip(map(lambda x: x, res_dict.values()), res_dict.keys()), reverse=True)\nresulted[:10]","96651eb8":"for_use_vars = [k for v,k in resulted]\nprint('\u0412 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: ', len(for_use_vars))","5bd9c3b4":"zeros = []\nfor k in [k for v,k in resulted]:\n    for model in (xgb_results, ctb_results):\n        if k in [m for v,m in model[:100]]:\n            if [v for v,m in model][list([m for v,m in model][:100]).index(k)] == 0:\n                zeros.append(k)\n\nprint('\u0412 \u0432\u044b\u0431\u043e\u0440\u043a\u0435: ', len(zeros))\n\n\nmust_use = set(for_use_vars) - set(zeros)\nprint('\u0423\u0436\u0435 \u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0435\u0435 \u0447\u0438\u0441\u043b\u043e: ', len(must_use))","624ce3ba":"plt.figure(figsize=(20,20))\n\nsns.heatmap(X[X.columns[[list(X.columns).index(m) for m in must_use]]].corr(), annot = True, cmap = 'Blues')","b804e4e9":"X = train.drop(['target'], axis=1,)\ny = train.target.values\nX_sub = test.copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=VAL_SIZE, shuffle=True, random_state=RANDOM_SEED)","eb1b5071":"y_train.shape, X_train.shape, X_test.shape","be618d0c":"from sklearn import metrics\n\ndef RMSE(y, y_pred):\n    \n    MSE = metrics.mean_squared_error(y, y_pred)\n    print(f'MSE = {MSE:.4f}')\n\n    RMSE = np.sqrt(MSE)\n    print(f'RMSE = {RMSE:.4f}')\n\n    print('-------')","6f1ffddf":"from sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom catboost import Pool, CatBoostRegressor, cv\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nimport lightgbm as lgb\n\nxgb_params = {\n    'colsample_bytree':0.4603,\n    'gamma':0.0468,\n    'learning_rate':0.05,\n    'max_depth':3,\n    'min_child_weight':1.7817,\n    'n_estimators':2200,\n    'reg_alpha':0.4640,\n    'reg_lambda':0.8571,\n    'subsample':0.5213,\n    'nthread':-1,\n    'eval_metric':'rmse',\n    'seed':RANDOM_SEED,\n#     'tree_method':'gpu_hist',\n#     'gpu_id':0\n}","79098a16":"xgb = XGBRegressor(**xgb_params)\nrfr = RandomForestRegressor(n_estimators=100, max_depth=6, n_jobs=-1)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\n\nlr = LinearRegression()","1c6c53be":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone","8f8eb6dc":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","337b783a":"def rmsle(y, y_pred):\n    return np.sqrt(metrics.mean_squared_error(y, y_pred))","9dce4408":"stacked_averaged_models = StackingAveragedModels(base_models = (xgb, rfr),\n                                                 meta_model = lr)","ade87e19":"stacked_averaged_models.fit(X_train.values, y_train)","43dcca18":"stacked_train_pred = stacked_averaged_models.predict(X_train.values)\nprint(rmsle(y_train, stacked_train_pred))","e6a1925f":"stacked_pred = stacked_averaged_models.predict(X_sub.values)","6a2e3ace":"VERSION = 5","392965b4":"df_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\ndf_sub['target'] = stacked_pred\ndf_sub.to_csv(f'submission_stacking_v{VERSION}.csv', index=False)\ndf_sub.head(10)","43e9abd0":"xgb.fit(X_train, y_train)\nxgb_train_pred = xgb.predict(X_train)\nxgb_pred = np.expm1(xgb.predict(X_test))\nprint(rmsle(y_train, xgb_train_pred))","fa9fbd6e":"xgb_pred_stacked = xgb.predict(X_sub)\nxgb_pred_stacked","036b9bf3":"VERSION = 7","cf0d9646":"df_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\ndf_sub['target'] = xgb_pred_stacked\ndf_sub.to_csv(f'submission_stacking_v{VERSION}.csv', index=False)\ndf_sub.head(10)","2fe2abde":"ensemble = stacked_pred*0.2 + predict_submission_xgb*0.2 + predict_submission_ctb*0.3 + y_preds_lgb.values*0.3\n\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\ndf_sub['target'] = ensemble\ndf_sub.to_csv(f'submission_average.csv', index=False)\ndf_sub.head(10)","31d487a0":"print(len(predict_submission_ctb))\nprint(len(predict_submission_xgb))\nprint(len(y_preds_lgb))","84580671":"sub_predict_xgb = pd.read_csv('.\/submission_XGBoost_v3.csv')\nsub_predict_catboost = pd.read_csv('.\/submission_CatBoost_v3.csv')\nsub_predict_lgbm = pd.read_csv('.\/submission_LightGBM_8.csv')\n\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\nblend_sub_predict = (sub_predict_xgb['target'] + sub_predict_catboost['target'] + sub_predict_lgbm['target']) \/ 3\ndf_sub['target'] = blend_sub_predict\ndf_sub.to_csv(f'submission_average.csv', index=False)\ndf_sub.head(10)","d8d0fe35":"# EDA","d8c1c7e3":"https:\/\/www.youtube.com\/watch?v=5Al4pXdJmAY&ab_channel=DataStartConference","22ebc9d7":"# Averages","547e70cd":"I used LightGBM as a first attempt to submit.\nNot much EDA or Feature engineering, I only did encode the categoricals.\nI think there are so many things for improvment, as I am just new to Kaggle.\nHope you guys like it, and please upvote it!","ca79bd8b":"# CatBoost","7b3b5b9e":"# XGBoost","94e1c7b9":"# LightGBM\n","2e920751":"# Stacking","d3caa56e":"# \u0421\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u0435 features between CatBoost and XGBoost","805d43b0":"It's look better","610f2ebd":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed."}}