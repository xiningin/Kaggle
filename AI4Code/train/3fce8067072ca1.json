{"cell_type":{"2baa8d7f":"code","495bff56":"code","73350450":"code","5ea4d63a":"code","5c1358cb":"code","384de1b6":"code","2cf3bc72":"code","2c6e9e9f":"code","6d1331ad":"code","05edbe1f":"code","8127d929":"code","784d7a05":"code","6aa61877":"code","0ea0e35b":"code","25fe4849":"code","1ae750a4":"code","3d7d6e38":"code","377f16a3":"code","d0de0565":"code","aca1e5ac":"code","8ab6ff74":"code","acc90275":"code","06f15e87":"code","ac24cbfd":"code","8497d2ba":"code","dca19d73":"code","305eba44":"code","e6dd9e90":"code","300f9567":"code","32330ebe":"code","ed7203c9":"code","e5bc6756":"code","a9b9a267":"code","95b2e1f9":"code","8ab2f599":"code","239d2c57":"markdown","b12433fa":"markdown","7839f6ff":"markdown","041ddc88":"markdown","68dc427b":"markdown","e0a42a23":"markdown","4f1414be":"markdown","38457dbb":"markdown","d3d21e51":"markdown","47631049":"markdown","2cc06d24":"markdown","9b91dac4":"markdown","2e2a5ada":"markdown","85548de6":"markdown","1109b0ec":"markdown","d077bbd0":"markdown","4bbb9cd4":"markdown","969c140c":"markdown","7265d97b":"markdown","c6e26411":"markdown","43604a25":"markdown","6a0ff156":"markdown","60391fc4":"markdown"},"source":{"2baa8d7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","495bff56":"#!pip install tensorflow-gpu","73350450":"import random\nimport os\nimport numpy as np\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    #torch.manual_seed(seed)\n    #torch.cuda.manual_seed(seed)\n    #torch.backends.cudnn.deterministic = True\n    #from tensorflow import set_random_seed\n    #set_random_seed(2)\n\nseed_everything()","5ea4d63a":"#!pip install tensorflow-gpu","5c1358cb":"import tensorflow as tf\nprint(tf.test.gpu_device_name())\n# See https:\/\/www.tensorflow.org\/tutorials\/using_gpu#allowing_gpu_memory_growth\n#config = tf.ConfigProto()\n#config.gpu_options.allow_growth = True","384de1b6":"x = tf.random.uniform([3, 3])\n\nprint(\"Is there a GPU available: \"),\nprint(tf.test.is_gpu_available())\n\nprint(\"Is the Tensor on GPU #0:  \"),\nprint(x.device.endswith('GPU:0'))\n\nprint(\"Device name: {}\".format((x.device)))","2cf3bc72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Bidirectional, BatchNormalization, Conv1D, GlobalAveragePooling1D \nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re","2c6e9e9f":"train_dataset = pd.read_csv(\"\/kaggle\/input\/steam-reviews\/train.csv\", delimiter=\",\")\ntrain_dataset","6d1331ad":"test_dataset = pd.read_csv(\"\/kaggle\/input\/steam-reviews-test-dataset\/test.csv\", delimiter=\",\")\ntest_dataset['user_suggestion'] = None","05edbe1f":"dataset = pd.concat([train_dataset, test_dataset], axis = 0)\ndataset.reset_index(drop = True, inplace = True)\ndataset","8127d929":"import re\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    phrase = re.sub(r\"early access review\", \"early access review \", phrase)\n    phrase = re.sub(r\"\\+\", \" + \", phrase) \n    phrase = re.sub(r\"\\-\", \" - \", phrase)     \n    phrase = re.sub(r\"\/10\", \"\/10 \", phrase)     \n    phrase = re.sub(r\"10\/\", \" 10\/\", phrase)         \n    return phrase","784d7a05":"import re\ndef clean_reviews(lst):\n    # remove URL links (httpxxx)\n    lst = np.vectorize(remove_pattern)(lst, \"https?:\/\/[A-Za-z0-9.\/]*\")\n    # remove special characters, numbers, punctuations (except for #)\n    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z]\", \" \")\n    # remove amp with and\n    lst = np.vectorize(replace_pattern)(lst, \"amp\", \"and\")  \n    # remove hashtags\n    lst = np.vectorize(remove_pattern)(lst, \"#[A-Za-z0-9]+\")\n    lst = np.vectorize(remove_pattern)(lst, \"#[\\w]*\")    \n    return lst\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)        \n    return input_txt\ndef replace_pattern(input_txt, pattern, replace_text):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, replace_text, input_txt)        \n    return input_txt","6aa61877":"# Applying pre-processing to user reviews\ntext2 = clean_reviews(list(dataset['user_review'].astype('str')))\ntext3 = [ta.lower() for ta in text2]\ntext4 = [''.join([i if ord(i) < 128 else ' ' for i in t]) for t in text3]\ntext5 = [decontracted(u) for u in text4]\ntext5[1]","0ea0e35b":"dataset2 = dataset[['user_review', 'user_suggestion']]\ndataset2['user_review'] = text5\ndataset2","25fe4849":"dataset3 = dataset2.iloc[:17494,]\ndataset3","1ae750a4":"max_words = 15000\nmax_len = 400\ntokenizer = Tokenizer(num_words=max_words, split=' ')\ntokenizer.fit_on_texts(dataset3['user_review'].values)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","3d7d6e38":"X = tokenizer.texts_to_sequences(dataset3['user_review'].values)\nX = pad_sequences(X, max_len)\nX[1,], dataset3.loc[1,'user_review']","377f16a3":"Y = pd.get_dummies(dataset3['user_suggestion']).values\n#Y = dataset3['user_suggestion'].values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","d0de0565":"embed_dim = 100\nlstm_out = 128\n\nmodel = Sequential()","aca1e5ac":"model.add(Embedding(max_words, embed_dim, input_length = max_len))","8ab6ff74":"model.add(LSTM(lstm_out))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2,activation='softmax'))","acc90275":"import numpy as np\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n\nclass Metrics(Callback):\n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        val_predict = (np.asarray(self.model.predict(self.validation_data[0]))).round()\n        val_targ = self.validation_data[1]\n        _val_f1 = f1_score(val_targ, val_predict, average = \"weighted\")\n        _val_recall = recall_score(val_targ, val_predict)\n        _val_precision = precision_score(val_targ, val_predict)\n        self.val_f1s.append(_val_f1)\n        self.val_recalls.append(_val_recall)\n        self.val_precisions.append(_val_precision)\n        #print \u201c \u2014 val_f1: %f \u2014 val_precision: %f \u2014 val_recall %f\u201d %(_val_f1, _val_precision, _val_recall)\n        print(' \u2014 val_f1: %f \u2014 val_precision: %f \u2014 val_recall %f' % (_val_f1, _val_precision, _val_recall))\n        return\n\nmetrics = Metrics()","06f15e87":"#from sklearn.metrics import f1_score","ac24cbfd":"model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\nprint(model.summary())","8497d2ba":"batch_size = 256\n\nmodel.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 1, validation_split = 0.2)","dca19d73":"pred = np.argmax(model.predict(X_test), axis = 1)\nactual = np.argmax(Y_test, axis = 1)\n\npred, actual","305eba44":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(actual, pred)\nprint(\"Accuracy of LSTM  is {}\".format(acc))","e6dd9e90":"glove_dir = '..\/input\/glove-global-vectors-for-word-representation\/'\nembeddings_index = {}\nf = open(os.path.join(glove_dir, 'glove.6B.200d.txt'))\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\nprint('Found %s word vectors.' % len(embeddings_index))","300f9567":"embedding_dim = 200\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items():\n    if i < max_words:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","32330ebe":"tf.keras.backend.clear_session()","ed7203c9":"model = Sequential()\nmodel.add(Embedding(max_words, embedding_dim, input_length = max_len, weights = [embedding_matrix]))\n#model.add(Conv1D(128, 5, activation = 'relu'))\n#model.add(GlobalAveragePooling1D())\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\nmodel.add(BatchNormalization())\nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\nmodel.add(BatchNormalization())\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(BatchNormalization())\n#model.add(GlobalAveragePooling1D())\nmodel.add(Dense(64,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='rmsprop',metrics = ['accuracy'])\nprint(model.summary())","e5bc6756":"tf.random.set_seed(123)\nnp.random.seed(123)","a9b9a267":"batch_size = 256\n\nmodel.fit(X_train, Y_train, epochs = 20, batch_size=batch_size, verbose = 1, validation_split = 0.2)","95b2e1f9":"pred = np.argmax(model.predict(X_test), axis = 1)\nactual = np.argmax(Y_test, axis = 1)\n\npred, actual","8ab2f599":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(actual, pred)\nprint(\"Accuracy of LSTM  is {}\".format(acc))","239d2c57":"![Recurrent Neural Network](attachment:1_go8PHsPNbbV6qRiwpUQ5BQ.png)","b12433fa":"### Training RNNs and Issues with Long-Term Dependency\n\nTheir unique structure also makes it particularly interesting to understand how they learn the weights. RNNs use something called Backpropagation Through Time, where weights are updated sequentially over time. For a sentence with n words, RNN unfolds n times, i.e. there are n different steps at which an input is passed into the layer, which contributed to the overall error. As we update the weights based on gradients, we find that these gradients end up multiplying n times (courtesy of the Chain Rule for Differentiation) till we reach the first word. And here in lies the problem! \n\nFor complete understanding of the maths behind the weight updates and back-propagation, refer to this blog-post - \"\"","7839f6ff":"## Embeddings\n\nTo start off, we convert our one-hot encoded tokens into Embeddings.\n\n> A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n\n> It is an improvement over more the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.\n\n> Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.\n\nIntuitively, Embeddings are essentially multidimensional representations of words, which take into account their meanings as well! For example, words like \"king\" would be closer to \"man\", instead of \"woman\". Also, the gap between the words, would be consistent, like king - man = queen - woman. These are usually trained using huge corpuses of data, using ANNs, with help from linguistic rules. Some of the popular embeddings are glove, word2vec, etc.\n\nFor the first part of this exercise, I'm not chosing any of the pre-trained embeddings, instead I'll simply let my model learn them for this specific case.","041ddc88":"Previously, we explored TFIDF, Bag of Words alongside Ensemble Decision Tree Methods for Text Classification. This time we try and use a new approach - RNN.","68dc427b":"Dropout layer drops neurons randomly during the training, with the probability defined by us. The idea is to make the model robust to fluctuations\/noise in the data and forcing it to learn a more general pattern. This essentially helps in preventing overfitting of models, and along with BatchNorm, this the most commonly used regularization method in Deep Learning.\n\nThere are multiple kinds of Dropouts that can be used, varying from SpatialDropout to recurrent_dropout within LSTM cell. However, I'm using the normal Dropout layer with 0.5 chance of dropping any neuron.","e0a42a23":"All the preprocessing steps remain same here!","4f1414be":"# Enter LSTMs!","38457dbb":"### Intuition behind Vanishing Gradient & Long-Term Dependency\n\nThe problem of Vanishing Gradient, can be intuitively understood if we think of Long-Term Dependency.\n\n> Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in \u201cthe clouds are in the sky,\u201d we don\u2019t need any further context \u2013 it\u2019s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it\u2019s needed is small, RNNs can learn to use the past information.\n\n> But there are also cases where we need more context. Consider trying to predict the last word in the text \u201cI grew up in France\u2026 I speak fluent French.\u201d Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It\u2019s entirely possible for the gap between the relevant information and the point where it is needed to become very large.\n\n> Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.","d3d21e51":"**Validation Accuracy - 87%**\n\nThis is an improvement over our previous approach! Further fine-tuning the model with better dropout, and more epochs can actually improve this result!\n\nIn the next blog of this series, let's see how we can actually use Transfer Learning for NLP! Get ready, we are entering the big leagues now[](http:\/\/) ;)","47631049":"### Compiling and fitting the model","2cc06d24":"Enough talk, let's see how to implement a simple LSTM network in Keras.","9b91dac4":"> This chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They\u2019re the natural architecture of neural network to use for such data.\n\n> And they certainly are used! In the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning\u2026 The list goes on. I\u2019ll leave discussion of the amazing feats one can achieve with RNNs to Andrej Karpathy\u2019s excellent blog post, The Unreasonable Effectiveness of Recurrent Neural Networks. But they really are pretty amazing.","2e2a5ada":"# Tokenization & Padding","85548de6":"# Evolution of NLP - Part 2 - Recurrent Neural Networks (RNN)","1109b0ec":"## Dropout","d077bbd0":"### Recurrent Neural Networks\n\nRecurrent Neural Networks are a special kind of Neural Networks, that allow **learning over time**. Standard models, and even neural networks for example do not take into account the context of each word, i.e. the information from words that've come before the current one. This restricts the accuracy achievable as you are always guessing the context while analyzing each word independently. This is where RNN come in.\n\nRNN take input words sequentially - one at a time - and pass on the output generated for each word, back to the model, alongside the next word. This helps models to understand the context in which the word was used and predict accordingly.\n\nA simple classifier based on RNN would have a structure shown below. An input statement with words passed on one-by-one, and at the end the final result is the prediction.\n\nHere X0, X1, X2...Xt are words of a sentence, which are fed to the network one by one. RNN cells create the output in form of a hidden state h0, h1, h2....ht which is also passed ahead!","4bbb9cd4":"### Pre-trained Glove Embeddings","969c140c":"With this model, we manage to reach around **83%** in Epoch 3, after which model starts to over-fit. To improve this further, we use pre-trained word embeddings from Glove, and **Bidirectional LSTM**. These cells look at forward as well backward context of a word while generating output.","7265d97b":"In Tokenization step, the sentences are converted to individual words, or tokens, similar to previous models. In Padding Step, the length of inputs to the LSTM model is homogenized. Essentially, since the all the sentences would have different lengths, sentences which are shorter than the maximum get padded with 0s. This is done to make sure that all the inputs are of same length.","c6e26411":"# Model","43604a25":"To address this issue, LSTMs (Long Short Term Memory) and GRUs (Gated Recurring Unit) were introduced. Even though LSTMs has more weights to tune, both have a similar idea behind their structure - the goal is to preserve the context\/information present in the initial part of statement, by preventing the issue of Vanishing Gradient.\n\nIn this post, I'll walk you through implementing LSTMs. For a detailed and intuitive explanation, refer the \"Understanding LSTM\" post. \n\nIn short, LSTMs have 3 gates - in addition to the layers in normal RNN. \n*  The \u201cforget gate\u201d controls what part of previous information is retained in the cell state, and what is not \u2014 essentially a sigmoid with previous time step\u2019s hidden value and input data for current time step,\n* The \u201cinput gate\u201d controls what new information is to be updated in the cell state. This consists of 2 components \u2014 a sigmoid over new inputs and previous time step and a tanh over the same values to prepare candidate data for the Cell State, and finally\n* The \u201coutput gate\u201d which decides what to output from the new updated Cell State. This will generate output, i.e. our hidden state. And the cycle repeats in the next cell, which takes this hidden state, along with new input data and our updated cell state as inputs\n\nIntuitively, these gates ensure that the relevant context\/information necessary to make the right prediction is retained, and mathematically, the idea is to make sure the gradients don't approach 0 for words present in beginning of sentences. While calculating gradients, the individual partial differentials are not all small numbers (i.e. numbers<<1) getting multiplied, instead it's the sigmoids, which get multiplied. If a position's context is important, our model can learn to keep it 1, there by ensuring that the gradient on these position are significant, meaning the weights don't saturate. This simple mathematical trick greatly improved the effectiveness of RNNs in NLP tasks.\n\nTo get a better in-depth mathematical understanding of how the gradients for these two are different, refer this https:\/\/medium.com\/datadriveninvestor\/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577.","6a0ff156":"To find the blog for this tutorial on Medium for detailed explanation, follow [this link](https:\/\/medium.com\/@jainkanishk001\/evolution-of-nlp-part-1-bag-of-words-tf-idf-9518cb59d2d1), and for Part-1 of this series - [Evolution of NLP - Part 1 - Bag of Words, TF-IDF](https:\/\/medium.com\/@jainkanishk001\/evolution-of-nlp-part-2-recurrent-neural-networks-af483f708c3d)","60391fc4":"### Vanishing Gradient\n\nSince, all of these updates (read gradients) are either smaller than 1 or more than 1, taking exponential powers of this quickly lead to two critical issues with RNN - Vanishing Gradient and Exploding Gradient, respectively. As the names suggest, in first cases, gradients turn too small, and thus don't lead to any change in model weights, while in the later case, they tend to become too large that leads to egregeious shifts in model weights, again failing to improve learning. \n\nIn most of the cases, we use activation functions (tanh, sigmoid) whose gradients are lower than 1, hence Vanishing Gradient is the most prominent concern"}}