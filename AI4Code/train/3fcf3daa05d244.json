{"cell_type":{"681e1cf2":"code","66530492":"code","860ac4b4":"code","6393d0d8":"code","f044999a":"code","9bd41cf7":"code","b3079d43":"code","7ddec9e3":"code","03b0eaa8":"code","c0d35c92":"code","69021c02":"code","7ace4fd7":"code","be51c8d7":"code","021d0b64":"code","c7313344":"code","651de657":"code","80b3575b":"code","c76b1a2a":"code","3a3ea67a":"code","33675f59":"code","b0c76d1f":"code","c9016ff7":"code","a989dd46":"code","7d667152":"code","d28b263d":"code","43a308e0":"code","55dd7ce8":"code","1d7b250e":"code","7b5747b6":"code","840b1f2e":"code","5557b6fc":"code","4572d964":"code","cc0fbdf0":"code","9f063141":"code","6f9bf249":"code","377ba6bf":"code","be0146e6":"code","272efbe7":"code","6d2802cb":"code","81570798":"code","6e9cd672":"code","2c59bd7e":"code","90010f90":"code","411fd14c":"code","56c006a7":"code","1beeab8b":"code","91ae57d8":"code","e1283da3":"code","46605ef3":"code","e9867473":"code","d4368911":"code","8ea9128c":"code","0d449ba8":"code","12843690":"code","f4e131b0":"code","e1279a95":"code","b3e4f6fd":"code","75b5896b":"code","3f5cfa8c":"code","c9dd45bb":"code","40ca8435":"code","aa0deb7a":"code","aed41706":"code","83a7c8a7":"code","83c6b378":"code","773ba217":"code","6ffbdf8c":"code","fdca865f":"code","7165a87a":"code","9a2eb43d":"code","b9902fdd":"code","0cf9cc62":"code","ab4771cb":"code","09e6ab4e":"code","ec1464a0":"code","8fa061ea":"code","e4a1821e":"code","bccd6e0a":"code","6a0c38f1":"code","916e8de7":"code","b87b0c13":"code","cb007d5b":"code","445027a1":"code","d59a2ba0":"code","f9e02681":"code","65c7923f":"code","6f58d6ab":"code","e85c2e35":"code","38fd6483":"code","6ddf9a8e":"code","5d761e9b":"code","7350854b":"code","860cc229":"code","a7b16629":"code","3e693076":"code","bcc165fa":"code","ff78063d":"code","9aefe3a2":"code","6d37ee24":"code","c82892cf":"code","9241a8c4":"code","d3e2115a":"code","bc490640":"code","8fc4da10":"code","8228810c":"code","3060056b":"code","f7b6c409":"code","a886b3bc":"code","354a34cb":"code","279099b4":"code","b3f4aa6c":"code","a557138d":"code","b2b8ceff":"code","675ac664":"code","f2ba3ff4":"code","9d9836bf":"code","e85ad671":"code","16047bbe":"code","5dfccf20":"code","b1b4c07c":"code","c64ac85a":"code","b548d441":"code","fd842d3f":"code","6c5a224c":"code","2929bf48":"code","d551e7de":"code","a9aa46fb":"code","37878860":"code","747bda27":"code","13d91c77":"code","8dffba44":"code","1bb2f210":"code","85253ded":"code","d33b6bd5":"code","4c44dadf":"code","3e39b8bc":"code","f9f5ca33":"code","7658eb4e":"code","8652a1c8":"code","710f50d0":"code","e010c842":"code","01333c5d":"code","28935188":"code","5c2e63e3":"markdown","e3098b54":"markdown","db44d336":"markdown","fb922aec":"markdown","f41c0ba5":"markdown","89a64aab":"markdown","01832f86":"markdown","2ebbfcf7":"markdown","545eecb0":"markdown","c07aa7d2":"markdown","c937ebef":"markdown","6f968547":"markdown","e900939f":"markdown","24563ec2":"markdown","e8781a64":"markdown","b33c7aab":"markdown","2920fb42":"markdown","a1135c15":"markdown","e3365e09":"markdown","b6f285fb":"markdown","105c86d7":"markdown","3ae5f9ce":"markdown","7b993e9d":"markdown","74d93e56":"markdown","9f62599d":"markdown","1ed0e000":"markdown","208f1349":"markdown","20dfe358":"markdown","6db1fbc4":"markdown","cbe57a75":"markdown","0d4f0e9d":"markdown","2b0a003c":"markdown","2472ae25":"markdown","6b21c4b9":"markdown","5ef72dc3":"markdown","1d02f4e6":"markdown","6180bad8":"markdown","3c2d680d":"markdown","193766ff":"markdown","53505760":"markdown","66d38366":"markdown","ab75f246":"markdown","ccf9d05d":"markdown","5abe26eb":"markdown","a25af255":"markdown","bedc1e14":"markdown","21269a37":"markdown","0ac4bb92":"markdown","75ee45de":"markdown","d9ae5547":"markdown","8e954fe4":"markdown","2bc419a5":"markdown","d90fe342":"markdown","c6575d31":"markdown","8db7155d":"markdown","9f100ac6":"markdown","2426c189":"markdown","d3c92fda":"markdown","94e829bf":"markdown","99db0f6a":"markdown","71742b0b":"markdown","0f01b4d7":"markdown","d5880ce3":"markdown","115146e2":"markdown","7d7228b6":"markdown","ddb1ae0b":"markdown","fc47490b":"markdown","f467fe37":"markdown","3d36e5b2":"markdown","236fade0":"markdown","85543a06":"markdown","c6782628":"markdown","e0387048":"markdown","7f758aad":"markdown","e9e5ce92":"markdown","893c332d":"markdown"},"source":{"681e1cf2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","66530492":"import pandas as pd\nimport numpy as sns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n%matplotlib inline","860ac4b4":"plt.rcParams[\"figure.figsize\"] = (20, 10)","6393d0d8":"df_train= pd.read_csv('\/kaggle\/input\/supplement-sales-prediction\/TRAIN.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/supplement-sales-prediction\/TEST_FINAL.csv')","f044999a":"df_train.info()","9bd41cf7":"df_train.isnull().sum()","b3079d43":"df_test.info()","7ddec9e3":"df_test.isnull().sum()","03b0eaa8":"df_train.head()","c0d35c92":"df_test.head()","69021c02":"EDA_train = df_train.copy()\nEDA_test = df_test.copy()","7ace4fd7":"EDA_train[\"Date\"]=  pd.to_datetime(EDA_train[\"Date\"])","be51c8d7":"EDA_train.head()","021d0b64":"EDA_train.info()","c7313344":"EDA_test[\"Date\"]=  pd.to_datetime(EDA_test[\"Date\"])","651de657":"EDA_test.info()","80b3575b":"EDA_test.head()","c76b1a2a":"EDA_train['Sales'].describe()","3a3ea67a":"EDA_train.nunique()","33675f59":"EDA_test.nunique()","b0c76d1f":"correlation = EDA_train['#Order'].corr(EDA_train['Sales'])\nprint(\"Correlation between oder and sales:\",correlation)\nsns.scatterplot(x=EDA_train['#Order'],y=EDA_train['Sales'])","c9016ff7":"typecounts = EDA_train.Store_Type.value_counts().to_dict()\nEDA_train_store_types = pd.DataFrame(list(typecounts.items()), columns=['Store_Type', 'Counts'])\nEDA_train_store_types.sort_values(by='Counts',inplace=True,ascending=False)\nEDA_train_store_types","a989dd46":"#plotting figure of each store types as pie chart","7d667152":"fig = px.pie(EDA_train_store_types, values='Counts', names='Store_Type',\n             title='Popularity of Store Types',labels='Store_Type')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","d28b263d":"\nEDA_train.boxplot(by ='Store_Type', column =['Sales'])","43a308e0":"EDA_train.boxplot(by ='Store_Type', column =['#Order'])","55dd7ce8":"avg_sales = EDA_train.groupby('Store_Type')['Sales'].mean().to_dict()\nEDA_Train_avg_sales_storetype = pd.DataFrame(list(avg_sales.items()), columns=['Store_Type', 'AvgSales'])\n\nEDA_Train_avg_sales_storetype ","1d7b250e":"fig = px.bar(EDA_Train_avg_sales_storetype, \n             x=\"Store_Type\", \n             y=\"AvgSales\",  \n             title=\"Averge Sales - Per Store\",\n             color_discrete_sequence=[\"#DC143C\"], template='plotly_dark')\nfig.show()\n","7b5747b6":"location_counts = EDA_train.Location_Type.value_counts().to_dict()\nEDA_train_location_counts = pd.DataFrame(list(location_counts.items()), columns=['Location_Type', 'Counts'])\n","840b1f2e":"fig = px.pie(EDA_train_location_counts, values='Counts', names='Location_Type',\n             title='Popularity of location Types',labels='location_Type')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","5557b6fc":"EDA_train.boxplot(by ='Location_Type', column =['Sales'])","4572d964":"avg_sales_Location = EDA_train.groupby('Location_Type')['Sales'].mean().to_dict()\nEDA_Train_avg_sales_Locationtype = pd.DataFrame(list(avg_sales_Location.items()), columns=['Location_Type', 'AvgSales'])\n\nEDA_Train_avg_sales_Locationtype","cc0fbdf0":"fig = px.bar(EDA_Train_avg_sales_Locationtype, \n             x=\"Location_Type\", \n             y=\"AvgSales\",  \n             title=\"Averge Sales - Per Location\",\n             color_discrete_sequence=[\"#DC143C\"], template='plotly_dark')\nfig.show()","9f063141":"region_counts = EDA_train.Region_Code.value_counts().to_dict()\nEDA_train_region_counts = pd.DataFrame(list(region_counts.items()), columns=['Region_Type', 'Counts'])\n\nfig = px.pie(EDA_train_region_counts, values='Counts', names='Region_Type',\n             title='Popularity of Region Types',labels='Region_Type')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","6f9bf249":"EDA_train.boxplot(by ='Region_Code', column =['Sales'])","377ba6bf":"avg_sales_Region= EDA_train.groupby('Region_Code')['Sales'].mean().to_dict()\nEDA_Train_avg_sales_Region_code = pd.DataFrame(list(avg_sales_Region.items()), columns=['Region_Code', 'AvgSales'])\n\nEDA_Train_avg_sales_Region_code","be0146e6":"fig = px.bar(EDA_Train_avg_sales_Region_code, \n             x=\"Region_Code\", \n             y=\"AvgSales\",  \n             title=\"Averge Sales - Per Regions\",\n             color_discrete_sequence=[\"#DC143C\"], template='plotly_dark')\nfig.show()","272efbe7":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nDiscount_sales = EDA_train.groupby('Discount')['Sales'].mean()\nDiscount_counts = EDA_train.Holiday.value_counts()\n\nprint(Discount_sales)\nprint(Discount_counts)","6d2802cb":"fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Mean Discount Sales Amount\", \"Discount Given Days\"))\n\nfig.add_trace(go.Bar(x=Discount_sales.values, y=Discount_sales.index, orientation='h',),1, 1)\n\nfig.add_trace(go.Bar(x=Discount_sales.values, y=Discount_sales.index, orientation='h',),1, 2)\n\nfig.update_layout(coloraxis=dict(colorscale='Bluered_r'), template='plotly_dark', showlegend=False)\nfig.show()","81570798":"sns.lineplot(data=EDA_train, x=\"Date\", y=(EDA_train['Sales']),hue='Discount')","6e9cd672":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nholiday_sales = EDA_train.groupby('Holiday')['Sales'].mean()\nholiday_counts = EDA_train.Holiday.value_counts()\n\nprint(holiday_sales)\nprint(holiday_counts)","2c59bd7e":"holiday_sales.dtypes","90010f90":"EDA_Train_avg_sales_holidays = pd.DataFrame(list(holiday_sales.items()), columns=['Holidays', 'AvgSales'])\n\nEDA_Train_avg_sales_holidays","411fd14c":"EDA_Train_avg_sales_holidays['Holidays'] = EDA_Train_avg_sales_holidays['Holidays'].map(\n                   {True:'Yes',False:'No'})","56c006a7":"EDA_Train_avg_sales_holidays","1beeab8b":"EDA_Train_holidays_count = pd.DataFrame(list(holiday_counts.items()), columns=['Holidays', 'No of Hoildays'])\n\nEDA_Train_holidays_count","91ae57d8":"EDA_Train_holidays_count['Holidays'] =EDA_Train_holidays_count['Holidays'].map(\n                   {True:'Yes' ,False:'No'})\nEDA_Train_holidays_count","e1283da3":"fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Holidays\/Nonholidays Sales\", \"Holidays\/Nonholidays Counts\"))\n\nfig.add_trace(go.Bar(x=EDA_Train_avg_sales_holidays['AvgSales'], y=EDA_Train_avg_sales_holidays['Holidays'], orientation='h',),1, 1)\n\nfig.add_trace(go.Bar(x=EDA_Train_holidays_count['No of Hoildays'], y=EDA_Train_holidays_count['Holidays'], orientation='h',),1, 2)\n\nfig.update_layout(coloraxis=dict(colorscale='Bluered_r'), template='plotly_dark', showlegend=False)\nfig.show()","46605ef3":"sns.lineplot(data=EDA_train, x=\"Date\", y=(EDA_train['Sales']),hue='Holiday')","e9867473":"EDA_train[EDA_train['Sales'] <= 20000 ]","d4368911":"sns.lineplot(data=EDA_train[EDA_train['Sales'] < 20000 ], x=\"Date\", y=(EDA_train['Sales']),hue='Holiday')","8ea9128c":"sns.lineplot(data=EDA_train[EDA_train['Sales'] < 20000 ], x=\"Date\", y=(EDA_train['Sales']),hue='Discount')","0d449ba8":"EDA_train[EDA_train['Sales'] == 0]","12843690":"EDA_train.head()","f4e131b0":"EDA_test.head()","e1279a95":"FE_train = EDA_train.copy()","b3e4f6fd":"FE_test = EDA_test.copy()","75b5896b":"FE_train.head()","3f5cfa8c":"FE_test.head()","c9dd45bb":"FE_train['Year'] = FE_train['Date'].dt.year\nFE_train['Day'] = FE_train['Date'].dt.day\nFE_train['Month'] = FE_train['Date'].dt.month\n","40ca8435":"FE_train.head()","aa0deb7a":"FE_train_2 = FE_train.drop(['#Order','Date'],axis=1)","aed41706":"FE_train_2.head()","83a7c8a7":"FE_train_2.dtypes","83c6b378":"FE_test['Year'] = FE_test['Date'].dt.year\nFE_test['Day'] = FE_test['Date'].dt.day\nFE_test['Month'] = FE_test['Date'].dt.month","773ba217":"FE_test.head()","6ffbdf8c":"FE_test_2 = FE_test.drop(['Date'],axis=1)","fdca865f":"FE_test_2.head()","7165a87a":"FE_test_2.dtypes","9a2eb43d":"from xgboost import XGBRegressor\nfrom sklearn import ensemble\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_log_error\n\n","b9902fdd":"encoder = LabelEncoder()","0cf9cc62":"EC_train = FE_train_2.copy()","ab4771cb":"EC_train.columns","09e6ab4e":"EC_train.head()","ec1464a0":"EC_test = FE_test_2.copy()","8fa061ea":"EC_test.head()","e4a1821e":"for i in EC_train.columns: \n    if EC_train[i].dtype == 'object': \n        encoder.fit_transform(list(EC_train[i].values)) #To Fit transform\n        EC_train[i] = encoder.transform(EC_train[i].values) # TO fit Transform\n         \n        for j in EC_train.columns: \n            if EC_train[j].dtype == 'int':\n                EC_train[j] = EC_train[j].astype('float64') #To Change the type\n\nfor k in EC_test.columns: \n    if EC_test[k].dtype == 'object': \n        encoder.fit_transform(list(EC_test[k].values)) #To Fit transform\n        EC_test[k] = encoder.transform(EC_test[k].values) #FTO Transform\n         \n        for m in EC_test.columns: \n            if EC_test[m].dtype == 'int':\n                EC_test[m] = EC_test[m].astype('float64')","bccd6e0a":"EC_test.head()","6a0c38f1":"X = EC_train.drop(columns=['ID', 'Sales']) # Data X\ny = EC_train['Sales'] # Data y     \nX_test = EC_test.drop(columns=['ID'])","916e8de7":"X_train, X_cv, y_train, y_cv = train_test_split(\n    X, y,test_size=0.3, \n    random_state=42)","b87b0c13":"pipeline_XGB = Pipeline([ # Our Pipeline\n    ('scaler', MinMaxScaler()),\n    ('transformer', QuantileTransformer()),\n    ('model', XGBRegressor(\n        learning_rate=0.2,\n        n_estimators=10000,\n        random_state=42,\n        objective='reg:squarederror',\n        booster='gbtree'\n    ))\n])\n\npipeline_XGB.fit(X_train, y_train) # Train Data","cb007d5b":"from sklearn import metrics\nimport math","445027a1":"XGB_pred_train = pipeline_XGB.predict(X_train) # Predict Train Data\nXGB_pred_cv = pipeline_XGB.predict(X_cv) # Predict Valid Data","d59a2ba0":"Train_r2_score = metrics.r2_score(y_train, XGB_pred_train) # R2_score\nprint(f'Train R2_score: {Train_r2_score }')\n\nTrain_mse = metrics.mean_squared_error(y_train, XGB_pred_train) # MSE Score\nprint(f'Train MSE : {Train_mse}')\n\nTrain_RMSE = math.sqrt(metrics.mean_squared_error(y_train,XGB_pred_train)) # SQRT MSE Score\nprint(f'Train RMSE : {Train_RMSE}')\n","f9e02681":"train = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_train, 'Actual Sales':y_train}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntrain = train.reset_index()\ntrain = train.drop(\n    ['index'],axis=1\n)\n\nplt.plot(train[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","65c7923f":"Test_r2_score = metrics.r2_score(y_cv, XGB_pred_cv) # R2_score\nprint(f'Test R2_score: {Test_r2_score}')\n\nTest_mse = metrics.mean_squared_error(y_cv, XGB_pred_cv) # MSE Score\nprint(f'Test MSE : {Test_mse}')\n\nTest_RMSE = math.sqrt(metrics.mean_squared_error(y_cv, XGB_pred_cv)) # SQRT MSE Score\nprint(f'Test RMSE : {Test_RMSE}')\n","6f58d6ab":"test = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_cv, 'Actual Sales':y_cv}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntest = test.reset_index()\ntest = test.drop(\n    ['index'],axis=1\n)\n\nplt.plot(test[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","e85c2e35":"Test_prediction = pipeline_XGB.predict(X_test)","38fd6483":"Test_prediction","6ddf9a8e":"Test_data = pd.read_csv('SAMPLE.csv')","5d761e9b":"Test_3= FE_test_2.filter(['ID'],axis=1)\nTest_3['Sales'] = Test_prediction\nTest_3.to_csv('XGB.csv',index=False)","7350854b":"X_1 = EC_train.drop(columns=['ID', 'Sales','Year']) # Data X\ny_1 = EC_train['Sales'] # Data y     \nX_test_1 = EC_test.drop(columns=['ID','Year'])","860cc229":"X_test_1.head()\nX_1.head()","a7b16629":"X_train_1, X_cv_1, y_train_1, y_cv_1 = train_test_split(\n    X_1, y_1,test_size=0.3, \n    random_state=42)","3e693076":"pipeline_XGB = Pipeline([ # Our Pipeline\n    ('scaler', MinMaxScaler()),\n    ('transformer', QuantileTransformer()),\n    ('model', XGBRegressor(\n        learning_rate=0.2,\n        n_estimators=10000,\n        random_state=42,\n        objective='reg:squarederror',\n        booster='gbtree'\n    ))\n])\n\npipeline_XGB.fit(X_train_1, y_train_1) # Train Data","bcc165fa":"XGB_pred_train_1 = pipeline_XGB.predict(X_train_1) # Predict Train Data\nXGB_pred_cv_1 = pipeline_XGB.predict(X_cv_1) # Predict Valid Data","ff78063d":"Train_r2_score_1 = metrics.r2_score(y_train_1, XGB_pred_train_1) # R2_score\nprint(f'Train R2_score: {Train_r2_score_1 }')\n\nTrain_mse_1 = metrics.mean_squared_error(y_train_1, XGB_pred_train_1) # MSE Score\nprint(f'Train MSE : {Train_mse_1}')\n\nTrain_RMSE_1 = math.sqrt(metrics.mean_squared_error(y_train_1,XGB_pred_train_1)) # SQRT MSE Score\nprint(f'Train RMSE : {Train_RMSE_1}')","9aefe3a2":"train_1 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_train_1, 'Actual Sales':y_train_1}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntrain_1 = train_1.reset_index()\ntrain_1 = train_1.drop(\n    ['index'],axis=1\n)\n\nplt.plot(train_1[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","6d37ee24":"Test_r2_score_1 = metrics.r2_score(y_cv_1, XGB_pred_cv_1) # R2_score\nprint(f'Test R2_score: {Test_r2_score_1}')\n\nTest_mse_1 = metrics.mean_squared_error(y_cv_1, XGB_pred_cv_1) # MSE Score\nprint(f'Test MSE : {Test_mse_1}')\n\nTest_RMSE_1 = math.sqrt(metrics.mean_squared_error(y_cv_1, XGB_pred_cv_1)) # SQRT MSE Score\nprint(f'Test RMSE : {Test_RMSE_1}')\n","c82892cf":"test_1 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_cv_1, 'Actual Sales':y_cv_1}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntest_1 = test_1.reset_index()\ntest_1 = test_1.drop(\n    ['index'],axis=1\n)\n\nplt.plot(test_1[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","9241a8c4":"Test_prediction_1 = pipeline_XGB.predict(X_test_1)","d3e2115a":"Test_3= FE_test_2.filter(['ID'],axis=1)\nTest_3['Sales'] = Test_prediction_1\nTest_3.to_csv('XGB_1.csv',index=False)","bc490640":"#Making copy of Dataset\nMT_train = EC_train.copy() ","8fc4da10":"MT_test = EC_test.copy()","8228810c":"MT_train.head()","3060056b":"MT_test.head()","f7b6c409":"MT_train.info()","a886b3bc":"MT_train[MT_train['Sales'] <= 5000]","354a34cb":"indexNames = MT_train[ MT_train['Sales'] <= 5000 ].index\n","279099b4":"MT_train.drop(indexNames , inplace=True)","b3f4aa6c":"MT_train.info()","a557138d":"X_2 = MT_train.drop(columns=['ID', 'Sales','Year']) # Data X\ny_2 = MT_train['Sales'] # Data y     \nX_test_2 = MT_test.drop(columns=['ID','Year'])","b2b8ceff":"X_test_2.head()\ny_2.head()\nX_test_2.head()","675ac664":"X_train_2, X_cv_2, y_train_2, y_cv_2 = train_test_split(\n    X_2, y_2,test_size=0.2, \n    random_state=42)","f2ba3ff4":"pipeline_XGB.fit(X_train_2, y_train_2) # Train Data","9d9836bf":"XGB_pred_train_2 = pipeline_XGB.predict(X_train_2) # Predict Train Data\nXGB_pred_cv_2 = pipeline_XGB.predict(X_cv_2) # Predict Valid Data","e85ad671":"Train_r2_score_2 = metrics.r2_score(y_train_2, XGB_pred_train_2) # R2_score\nprint(f'Train R2_score: {Train_r2_score_2 }')\n\nTrain_mse_2 = metrics.mean_squared_error(y_train_2, XGB_pred_train_2) # MSE Score\nprint(f'Train MSE : {Train_mse_2}')\n\nTrain_RMSE_2 = math.sqrt(metrics.mean_squared_error(y_train_2,XGB_pred_train_2)) # SQRT MSE Score\nprint(f'Train RMSE : {Train_RMSE_2}')","16047bbe":"train_2 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_train_2, 'Actual Sales':y_train_2}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntrain_2 = train_2.reset_index()\ntrain_2 = train_2.drop(\n    ['index'],axis=1\n)\n\nplt.plot(train_2[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","5dfccf20":"- Evaluate Test Data","b1b4c07c":"Test_r2_score_2 = metrics.r2_score(y_cv_2, XGB_pred_cv_2) # R2_score\nprint(f'Test R2_score: {Test_r2_score_2}')\n\nTest_mse_2 = metrics.mean_squared_error(y_cv_2, XGB_pred_cv_2) # MSE Score\nprint(f'Test MSE : {Test_mse_2}')\n\nTest_RMSE_2 = math.sqrt(metrics.mean_squared_error(y_cv_2, XGB_pred_cv_2)) # SQRT MSE Score\nprint(f'Test RMSE : {Test_RMSE_2}')","c64ac85a":"test_2 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_cv_2, 'Actual Sales':y_cv_2}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntest_2 = test_2.reset_index()\ntest_2 = test_2.drop(\n    ['index'],axis=1\n)\n\nplt.plot(test_2[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","b548d441":"Test_prediction_2 = pipeline_XGB.predict(X_test_2)","fd842d3f":"Test_3= FE_test_2.filter(['ID'],axis=1)\nTest_3['Sales'] = Test_prediction_2\nTest_3.to_csv('XGB_2.csv',index=False)","6c5a224c":"#Considering X_train_1, y_train_1","2929bf48":"pipeline_XGB_2 = Pipeline([ # Our Pipeline\n    ('scaler', StandardScaler()),\n    ('transformer', QuantileTransformer()),\n    ('model', XGBRegressor(\n        learning_rate=0.04,\n        n_estimators=3000,\n        random_state=42,\n        objective='reg:squarederror',\n        booster='gbtree'\n    ))\n])\n\npipeline_XGB_2.fit(X_train_1, y_train_1) # Train Data","d551e7de":"XGB_pred_train_3 = pipeline_XGB_2.predict(X_train_1) # Predict Train Data\nXGB_pred_cv_3 = pipeline_XGB_2.predict(X_cv_1) # Predict Valid Data","a9aa46fb":"Train_r2_score_3 = metrics.r2_score(y_train_1, XGB_pred_train_3) # R2_score\nprint(f'Train R2_score: {Train_r2_score_3 }')\n\nTrain_mse_3 = metrics.mean_squared_error(y_train_1, XGB_pred_train_3) # MSE Score\nprint(f'Train MSE : {Train_mse_3}')\n\nTrain_RMSE_3 = math.sqrt(metrics.mean_squared_error(y_train_1,XGB_pred_train_3)) # SQRT MSE Score\nprint(f'Train RMSE : {Train_RMSE_3}')","37878860":"train_3 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_train_3, 'Actual Sales':y_train_1}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntrain_3 = train_3.reset_index()\ntrain_3 = train_3.drop(\n    ['index'],axis=1\n)\n\nplt.plot(train_3[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","747bda27":"Test_r2_score_3 = metrics.r2_score(y_cv_1, XGB_pred_cv_3) # R2_score\nprint(f'Test R2_score: {Test_r2_score_3}')\n\nTest_mse_3 = metrics.mean_squared_error(y_cv_1, XGB_pred_cv_3) # MSE Score\nprint(f'Test MSE : {Test_mse_3}')\n\nTest_RMSE_3 = math.sqrt(metrics.mean_squared_error(y_cv_1, XGB_pred_cv_3)) # SQRT MSE Score\nprint(f'Test RMSE : {Test_RMSE_3}')\n","13d91c77":"test_3 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_cv_3, 'Actual Sales':y_cv_1}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntest_3 = test_3.reset_index()\ntest_3 = test_3.drop(\n    ['index'],axis=1\n)\n\nplt.plot(test_3[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","8dffba44":"Test_prediction_3 = pipeline_XGB_2.predict(X_test_1)","1bb2f210":"Test_3= FE_test_2.filter(['ID'],axis=1)\nTest_3['Sales'] = Test_prediction_3\nTest_3.to_csv('XGB_a_80.csv',index=False)","85253ded":"X_4 = EC_train.drop(columns=['ID', 'Sales','Year','Location_Type','Region_Code']) # Data X\ny_4 = EC_train['Sales'] # Data y     \nX_test_4 = EC_test.drop(columns=['ID','Year','Location_Type','Region_Code'])","d33b6bd5":"X_test_4.head()\nX_4.head()","4c44dadf":"X_train_4, X_cv_4, y_train_4, y_cv_4 = train_test_split(\n    X_4, y_4,test_size=0.2, \n    random_state=42)","3e39b8bc":"pipeline_XGB_4 = Pipeline([ # Our Pipeline\n    ('scaler', MinMaxScaler()),\n    ('transformer', QuantileTransformer()),\n    ('model', XGBRegressor(\n        learning_rate=0.04,\n        n_estimators=3000,\n        random_state=42,\n        objective='reg:squarederror',\n        booster='gbtree'\n    ))\n])\n\npipeline_XGB_4.fit(X_train_4, y_train_4) # Train Data","f9f5ca33":"XGB_pred_train_4 = pipeline_XGB_4.predict(X_train_4) # Predict Train Data\nXGB_pred_cv_4 = pipeline_XGB_4.predict(X_cv_4) # Predict Valid Data","7658eb4e":"Train_r2_score_4 = metrics.r2_score(y_train_4, XGB_pred_train_4) # R2_score\nprint(f'Train R2_score: {Train_r2_score_4 }')\n\nTrain_mse_4 = metrics.mean_squared_error(y_train_4, XGB_pred_train_4) # MSE Score\nprint(f'Train MSE : {Train_mse_4}')\n\nTrain_RMSE_4 = math.sqrt(metrics.mean_squared_error(y_train_4,XGB_pred_train_4)) # SQRT MSE Score\nprint(f'Train RMSE : {Train_RMSE_4}')","8652a1c8":"train_4 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_train_4, 'Actual Sales':y_train_4}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntrain_4 = train_4.reset_index()\ntrain_4 = train_4.drop(\n    ['index'],axis=1\n)\n\nplt.plot(train_4[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","710f50d0":"Test_r2_score_4 = metrics.r2_score(y_cv_4, XGB_pred_cv_4) # R2_score\nprint(f'Test R2_score: {Test_r2_score_4}')\n\nTest_mse_4 = metrics.mean_squared_error(y_cv_4, XGB_pred_cv_4) # MSE Score\nprint(f'Test MSE : {Test_mse_4}')\n\nTest_RMSE_4 = math.sqrt(metrics.mean_squared_error(y_cv_4, XGB_pred_cv_4)) # SQRT MSE Score\nprint(f'Test RMSE : {Test_RMSE_4}')","e010c842":"test_4 = pd.DataFrame(\n    {'Predicted Sales':XGB_pred_cv_4, 'Actual Sales':y_cv_4}\n)\n\nfig= plt.figure(\n    figsize=(16, 9)\n)\n\ntest_4 = test_4.reset_index()\ntest_4 = test_4.drop(\n    ['index'],axis=1\n)\n\nplt.plot(test_4[:50])\nplt.legend(['Actual Sales','Predicted Sales'])\nplt.title('Actual & Predicted Sales')\nplt.show()","01333c5d":"Test_prediction_4 = pipeline_XGB_4.predict(X_test_4)","28935188":"Test_3= FE_test_2.filter(['ID'],axis=1)\nTest_3['Sales'] = Test_prediction_4\nTest_3.to_csv('XGB_final.csv',index=False)","5c2e63e3":"-Need to check any corelation between #orders and sales","e3098b54":" - creating new column Day,month and year","db44d336":"Repeat the same step for test set","fb922aec":"- Evaluate Test Data","f41c0ba5":"Finding how many stores for each category of store types","89a64aab":"\nYour Client WOMart is a leading nutrition and supplement retail chain that offers a comprehensive range of products for all your wellness and fitness needs. \n\nWOMart follows a multi-channel distribution strategy with 350+ retail stores spread across 100+ cities. \n\nEffective forecasting for store sales gives essential insight into upcoming cash flow, meaning WOMart can more accurately plan the cashflow at the store level.\n\nSales data for 18 months from 365 stores of WOMart is available along with information on Store Type, Location Type for each store, Region Code for every store, Discount provided by the store on every day, Number of Orders everyday etc.\n\nThe Task is to predict the store sales for each store in the test set for the next two months.","01832f86":"- imporved accurcy score to 282.996754794649","2ebbfcf7":"setting a threshold as 20000 as sales figure..exploring values below","545eecb0":"The Date object converted to Date time.Repeating the same step for test data set","c07aa7d2":"- Droping ID,Date for test set","c937ebef":"-We have 365 stores in test data set, 4 store types distributed among in 5 location types with 4 Region codes","6f968547":" checking data for zero sales","e900939f":"# 3.3 Evaluate Test Data","24563ec2":"Before we change anything in the original Dataframe, we create a copy","e8781a64":"- creating copy of test and train dataset for Encoding","b33c7aab":"plotting line plot to find the same","2920fb42":"# 3.5 Model tuning hyper parameters","a1135c15":"Question: How many shops and products do we have? ","e3365e09":"- Creating copy of both train and test set for Data prepration and model training","b6f285fb":"## EDA on Location types","105c86d7":"# EDA on Holiday","3ae5f9ce":"There a postive correlation between sales and orders .so only considering sales figure from onwards for Exploratorty Data Analysis","7b993e9d":"# 3.1 Encoding","74d93e56":"- S1 have large number of store counts,then S4\n- Lets find out the average sales of Each sales by box plot","9f62599d":"# Supplement Sales Prediction","1ed0e000":"\nThe dataset contain the two files in the csv format:\n\nTRAIN.csv : this file has 188340 rows and 10 columns.\n\nTEST_FINAL.csv : this file has 22265 rows and 8 columns.\n\nVariable : Definition\n\nID : Unique Identifier for a row\n\nStore_id : Unique id for each Store\n\nStore_Type : Type of the Store\n\nLocation_Type : Type of the location where Store is located\n\nRegion_Code :Code of the Region where Store is located\n\nDate : Information about the Date\n\nHoliday : If there is holiday on the given Date, 1 : Yes, 0 : No\n\nDiscount : If discount is offered by store on the given Date, Yes\/ No\n\nOrders : Number of Orders received by the Store on the given Day\n    \nSales : Total Sale for the Store on the given Day","208f1349":"\n- No missing values.Data set looks clean\n- There a postive correlation between sales and orders.so droping order doesn't need to consider while traning(Also order numbers are not in test set)\n- ID can be removed since there is no relation between sales \n- Location type,Region,Store type have different sales figure.considerd for training\n- Holiday and Discounthave effect in sales number.Considered for training\n- Removing outliers in sales number considerd after training the model if desired accuaracy is not achieved\n","20dfe358":"Need to tune model to get better score\n- Droping year column and training again to check if there is an increase in accuarcy","6db1fbc4":"ploting the sales figure hue holiday","cbe57a75":"Checking for missing values and data types in test data set","0d4f0e9d":"# 3.Data Preparation for Model Training","2b0a003c":"- Location Types also effect the average sales ","2472ae25":"Loading csv files to dataframe","6b21c4b9":"To find How Giving Discount Effect","5ef72dc3":"First look of the test data set","1d02f4e6":"- NO missing values found and data set look clean. order catergory is not in test data set\n-Date need to be converted into Date time instead of object","6180bad8":"- Discounted Days gives higher number of sales","3c2d680d":"ploting the sales figure hue Discount","193766ff":"- HOlidays have also having sales number.","53505760":"- Discount given days have higher sales figure","66d38366":"# EDA on Regions types","ab75f246":"- Regions also a dependent feature on sales numbers","ccf9d05d":"# Insights from EDA","5abe26eb":"-Not having much effect in accuracy","a25af255":"# EDA on Discount","bedc1e14":"# 3.5 Model tuning Droping days which have sales below 5000","21269a37":"What is our target?\n-We want to predict the number of items sold for test data .\nLet's go ahead and take a closer look at the items.\n","0ac4bb92":"Finding average sales of Each stores","75ee45de":"- No missing values.Data set looks clean\n- Date need to be converted into Date time instead of object","d9ae5547":"Before we starting to explority data analysis we need to convert Date column object as  Date time","8e954fe4":"First look of the train data set","2bc419a5":"- L1 and L2 location have High number of stores","d90fe342":"# Checking Outliers in sales figure","c6575d31":"- Checking data types of FE_train_2","8db7155d":"- s4 and s3 have high average sales","9f100ac6":"- As previsoly mentioned the orders and sales have a strong correlation the sales and orders show same type of plots","2426c189":"- Hoilday have higher effect in sales number\n- some hoildays have steep decline in sales numbers.need to find out how manys days have sales below a threshold value","d3c92fda":"Following steps will be performed for preparing the data for the subsequent model training\n\n- Based on the Exploratory Data Analysis and Coorelation study, the columns with weak relationship with the target column will be dropped\n- Input and Target dataframes will be created\n- Need to split datetime object into day,year,month for both test and train\n- Label encoding methods applied to categorical columns\n- Training and Validation datasets will be created\n- A function will be defined based on which the models performance will be measured","94e829bf":"plotting line plot of mean average sales with hue as Hoilday ","99db0f6a":"Importing Neccessary libraries","71742b0b":"# 3.2 PipelIne and training Data","0f01b4d7":"By checking both the figure it is found that there are some outliers in data which gives low sales in some days.","d5880ce3":"some days only giving zero sales.later decide whether data need to removed or not.","115146e2":"# 3.4 Model tuning Droping Year column from data","7d7228b6":"# 2. Exploratory Data Analysis","ddb1ae0b":"- it is shown s4 have more average sales than s1\n- having a lagre number og outliers.need to check these Outliers have any significance in data\n- The number of stores doesn't have much importance in sales\n- to verify this let's check it with order number by plotting box plot\n","fc47490b":"Judging by the mean and maximum values of sales, there are some outliers.\nWe need to keep this in mind and will exclude them for our first visualisations","f467fe37":"- Splitting X and y","3d36e5b2":"- Evaluate Test Data\n","236fade0":"# 3.5 Droping Location and store Type","85543a06":"Droping ID,Date,'#Order' for train test set","c6782628":"## Store Types","e0387048":"- Evaluate Test Data","7f758aad":"-We have 365 stores in train data set, 4 store types distributed among in 5 location types with 4 Region codes","e9e5ce92":"checking for missing values and data types in training set","893c332d":"# 1. Data Preparation"}}