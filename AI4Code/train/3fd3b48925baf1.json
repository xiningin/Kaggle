{"cell_type":{"cfa1f47c":"code","3b2c6c5c":"code","e0c7102a":"code","ba0f445b":"code","3c929e73":"code","8e5faf07":"code","c835c3d1":"code","37d1ef80":"code","db53d9f3":"code","782591ff":"code","f0dd5feb":"code","5115a937":"code","54fe3eb9":"code","e1ef518a":"code","564592c0":"code","523f78a5":"code","d3316850":"markdown","8ab5f281":"markdown","cb8a684d":"markdown","eabe0e51":"markdown","0a9b9ab8":"markdown","c53ce083":"markdown","6105f38c":"markdown","eec45a19":"markdown","dd9401a7":"markdown","1abf2bd9":"markdown","ce066c56":"markdown","7ac158f3":"markdown","792f99e5":"markdown","af67ec59":"markdown","cc705119":"markdown","179bd3e8":"markdown","143365d7":"markdown","6b4197f8":"markdown","81dba4e6":"markdown","199d5e7a":"markdown","237ae081":"markdown","39e9e171":"markdown","ebb08079":"markdown","c17a2474":"markdown","54c62135":"markdown"},"source":{"cfa1f47c":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\nDATE_COLUMN = 'date\/time'\nDATA_URL = ('https:\/\/s3-us-west-2.amazonaws.com\/'\n            'streamlit-demo-data\/uber-raw-data-sep14.csv.gz')\n","3b2c6c5c":"data = pd.read_csv(DATA_URL)\ndata","e0c7102a":"lowercase = lambda x: str(x).lower()\ndata.rename(lowercase, axis='columns', inplace=True)\ndata[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])","ba0f445b":"data.base.value_counts()","3c929e73":"new_df=data.copy()\nnew_df['hour']=data[DATE_COLUMN].dt.hour\nnew_df['date']=data[DATE_COLUMN].dt.date\nnew_df['lat']=new_df['lat'].round(2)\nnew_df['lon']=new_df['lon'].round(2)\n\ndatecount=new_df.date.value_counts().sort_index().rename_axis('date').rename_axis('date').reset_index(name='counts')\ndatecount.set_index('date')","8e5faf07":"from pandas import read_csv\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport numpy as np\ndatecount.counts.mean()\n","c835c3d1":"from statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nimport warnings","37d1ef80":"def evaluate_arima_model(X, arima_order):\n\t# prepare training dataset\n\tX = X.astype('float32')\n\ttrain_size = int(len(X) * 0.50)\n\ttrain, test = X[0:train_size], X[train_size:]\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\t# model_fit = model.fit(disp=0)\n\t\tmodel_fit = model.fit(trend='nc', disp=0)\n\t\tyhat = model_fit.forecast()[0] # [0] -> indicates the value of the forecast\n        # see syntax of forecast function here -> \n        # http:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.arima_model.ARIMAResults.forecast.html?highlight=forecast#statsmodels.tsa.arima_model.ARIMAResults.forecast\n\t\tpredictions.append(yhat)\n\t\thistory.append(test[t])\n\t# calculate out of sample error\n\tmse = mean_squared_error(test, predictions)\n\trmse = sqrt(mse)\n\treturn rmse\ndef evaluate_models(dataset, p_values, d_values, q_values):\n\tdataset = dataset.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\tmse = evaluate_arima_model(dataset, order)\n\t\t\t\t\tif mse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = mse, order\n\t\t\t\t\tprint('ARIMA%s RMSE=%.3f' % (order,mse))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\tprint('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n \n# load data\n\n# evaluate parameters\n# type your code here\n\n# evaluate parameters\np_values = range(0, 5)\nd_values = range(0, 3)\nq_values = range(0, 5)\n\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(new_df.date.value_counts().sort_index(), p_values, d_values, q_values)\n","db53d9f3":"from statsmodels.tsa.stattools import adfuller\ncal=new_df.date.value_counts().sort_index()\ncal=cal-cal.shift(7)\ncal=cal[7:]\ncal\n# from statsmodels.tsa.stattools import adfuller\nresult=adfuller(cal)\nprint(result[0])\nprint(result[4])\nnew_datecount=cal[7:]\nnew_datecount","782591ff":"def evaluate_arima_model(X, arima_order):\n\t# prepare training dataset\n\tX = X.astype('float32')\n\ttrain_size = int(len(X) * 0.50)\n\ttrain, test = X[0:train_size], X[train_size:]\n\thistory = [x for x in train]\n\t# make predictions\n\tpredictions = list()\n\tfor t in range(len(test)):\n\t\tmodel = ARIMA(history, order=arima_order)\n\t\t# model_fit = model.fit(disp=0)\n\t\tmodel_fit = model.fit(trend='nc', disp=0)\n\t\tyhat = model_fit.forecast()[0] # [0] -> indicates the value of the forecast\n        # see syntax of forecast function here -> \n        # http:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.arima_model.ARIMAResults.forecast.html?highlight=forecast#statsmodels.tsa.arima_model.ARIMAResults.forecast\n\t\tpredictions.append(yhat)\n\t\thistory.append(test[t])\n\t# calculate out of sample error\n\tmse = mean_squared_error(test, predictions)\n\trmse = sqrt(mse)\n\treturn rmse\ndef evaluate_models(dataset, p_values, d_values, q_values):\n\tdataset = dataset.astype('float32')\n\tbest_score, best_cfg = float(\"inf\"), None\n\tfor p in p_values:\n\t\tfor d in d_values:\n\t\t\tfor q in q_values:\n\t\t\t\torder = (p,d,q)\n\t\t\t\ttry:\n\t\t\t\t\tmse = evaluate_arima_model(dataset, order)\n\t\t\t\t\tif mse < best_score:\n\t\t\t\t\t\tbest_score, best_cfg = mse, order\n\t\t\t\t\tprint('ARIMA%s RMSE=%.3f' % (order,mse))\n\t\t\t\texcept:\n\t\t\t\t\tcontinue\n\tprint('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n \n# load data\n\n# evaluate parameters\n# type your code here\n\n# evaluate parameters\np_values = range(0, 5)\nd_values = range(0, 3)\nq_values = range(0, 5)\n\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(new_datecount.values, p_values, d_values, q_values)\n","f0dd5feb":"x=new_df.date.value_counts().sort_index().values\nx=x.astype('float32')\nmodel=ARIMA(x,order=(3,1,0))\nnew_model=model.fit(trend='nc',disp=0)\nyhat=new_model.forecast(steps=5,alpha=0.01)\nprint(yhat)\nimport pickle\npickle.dump(new_model,open(\"uberModel.pickle\",\"wb\"))","5115a937":"days_group=new_df.groupby('date')\n\nhoursperday=days_group['hour'].value_counts().sort_index()\nprint(hoursperday)\nhoursperday.column=['date','hour','counts']\nhourly=[]\nfor j in range(0,720,24):\n  hourly.append([hoursperday[j:j+25]])\nprint(hourly)  ","54fe3eb9":"each_hour=[]\nfor i in range(0,23):\n  each_hour.append([hourly[j][0][i] for j in range(0,30)])\n  \neach_hour=pd.DataFrame(each_hour)\neach_hour.loc['day']=[i for i in range (1,31)]\neach_hour","e1ef518a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\nfor i in range(0,23):\n  X=pd.DataFrame(each_hour.loc['day'])\n  y=each_hour.loc[i]\n  X=X.astype('float32')\n  y=y.astype('float32')\n  \n  d=DecisionTreeRegressor()\n  dr=RandomForestRegressor(n_estimators=20,random_state=0)\n  x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=5) \n#   # print(x_train,y_train)\n  dr.fit(x_train,y_train)\n  y_pred=dr.predict(x_test)\n  #ans=dr.predict([[1]])\n#   # print(np.sqrt(mean_squared_error(y_test,y_pred)),np.mean(x.values)*0.1)\n  #scores=np.sqrt(-cross_val_score(dr,X,y,cv=10, scoring='neg_mean_squared_error').mean())\n  #print(scores,np.mean(y)*0.1)\n  #print(ans)\n  from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n#  print(confusion_matrix(y_test,y_pred))  \n # print(classification_report(y_test,y_pred))  \n  #print(accuracy_score(y_test, y_pred)) \n  from sklearn import metrics\n\n  # print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n  # print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n  print(np.mean(y)*0.1)\n  \n  ","564592c0":"from statsmodels.tsa.arima_model import ARIMA\nimport pickle\npredictions=[]\nfor i in range(0,23):\n  X=pd.DataFrame(each_hour.loc['day'])\n  y=each_hour.loc[i]\n  X=X.astype('float32')\n  y=y.astype('float32')\n  \n  dr=RandomForestRegressor(n_estimators=20,random_state=0)\n  #x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=5) \n  # print(x_train,y_train)\n  \n  dr.fit(X,y)\n  pred=[[1],[2],[3],[4],[5]]\n  y_pred=dr.predict(pred)\n  predictions.append(y_pred)\n  print(y_pred)\n  filename='uber'+str(i)+'.pickle'\n  pickle.dump(dr,open(filename,'wb'))","523f78a5":"NOTE:\nThe below code must not be run in kaggle do a streamlit run of it from your computers terminal or command prompt.\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport streamlit as st\nimport pandas as pd\nimport numpy as np\n\nst.title('Uber pickups in NYC')\n\nDATE_COLUMN = 'date\/time'\nDATA_URL = ('https:\/\/s3-us-west-2.amazonaws.com\/'\n            'streamlit-demo-data\/uber-raw-data-sep14.csv.gz')\n\n@st.cache\ndef load_data(nrows):\n    data = pd.read_csv(DATA_URL, nrows=nrows)\n    lowercase = lambda x: str(x).lower()\n    data.rename(lowercase, axis='columns', inplace=True)\n    data[DATE_COLUMN] = pd.to_datetime(data[DATE_COLUMN])\n    return data\n\ndata_load_state = st.text('Loading data...')\ndata = load_data(10000)\ndata_load_state.text(\"Done! (using st.cache)\")\n\nif st.checkbox('Show raw data'):\n    st.subheader('Raw data')\n    st.write(data)\n\nst.subheader('Number of pickups by hour')\nhist_values = np.histogram(data[DATE_COLUMN].dt.hour, bins=24, range=(0,24))[0]\nst.bar_chart(hist_values)\n\n# Some number in the range 0-23\nhour_to_filter = st.slider('hour', 0, 23, 17)\nfiltered_data = data[data[DATE_COLUMN].dt.hour == hour_to_filter]\n\nst.subheader('Map of all pickups at %s:00' % hour_to_filter)\nst.map(filtered_data)\n\nimport pickle\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nmodel = pickle.load(open(\"uberModel.pickle\",\"rb\"))\n\nyhat=model.forecast(steps=5,alpha=0.01)\n\nst.text(yhat[0])\nst.subheader('Total Pickups For the Next 5 Days')\nst.bar_chart(yhat[0])\n\nst.subheader('Hourly No. of pickups in the next 5 Days')\n\ninde=[]\nans=[]\nfor i in range(0,23):\n\ttry:\n\t\tfilename='uber'+str(i)+'.pickle'\n\t\tmodel = pickle.load(open(filename,\"rb\"))\n\t\tyhat=model.predict([[1],[2],[3],[4],[5]])\n\t\tinde.append(i)\n\t\tans.append(yhat)\n\texcept:\n\t\tcontinue\ndf=pd.DataFrame(ans,index=inde)\nst.write(df)\nhour_to_filter = st.slider('Day', 0, 4, 0)\nfiltered_data = df[hour_to_filter]\nst.bar_chart(filtered_data)    ","d3316850":"# Random Forest \n\n![RandomForest](https:\/\/1.bp.blogspot.com\/-fZfQCD8OsqY\/X0tb1mU4vtI\/AAAAAAAAoqM\/Eby6pfAev0Qcqusk_ogdP19lY6jGrteAQCLcBGAsYHQ\/s353\/RandomForest.png)\n\n**Firstly we must know this model is a bagging technique(Bootstrap aggregating) where we have a bootstrap of multiple decision trees learning from packets of data from our dataset as we see in the image above.**\n\n**In a classification model random forest would output a prediction looking at the output of the majority of the multiple decision trees for instance 3 decision trees output '1' and 2 decison trees output '0' so the overall output would be '1'.**\n\n**Meanwhile in a regression model random forest would output data based on the Mean Value of all the decision Trees or a Median value from it.****","8ab5f281":"Here we create new dataframes from our main data and a datecount column.","cb8a684d":"Creating a random forest regressor to predict the daily ","eabe0e51":"We apply a to_datetime function to the DATE_COLUMN to convert all other data types of the date column to date time data type. ","0a9b9ab8":"Here we make a pickle file out of the model and this would be used in the streamlit presentation of our model. ","c53ce083":"# StreamLit Implementation\nIn this section of our document we will present our predictions data and graphs using streamlit.\nBelow is the code to complile our pickle files and use for streamlit presentation. ","6105f38c":"# Moving Average Model\n**Now let us see the Statistics and theory of the AR Model. The moving average model simply predicts data from the outcome of a previous time period.\nWe will be going across an example of this model where we have a crazy professor who hosts a party every month and tells us to being 10 cupcakes along but due to being crazy he always tells us we brought a bit more or less or just the correct amount. Here we will use an MA model to see how to predict the number of cupcakes to bring along in the further month looking at data of the previous month.**\n![graphMA](https:\/\/1.bp.blogspot.com\/-2DrIj1QfyrM\/X0tNmH50pvI\/AAAAAAAAopA\/jluvm12wM0oyr2FS2QNV6PZzZkHQlqw7wCLcBGAsYHQ\/s554\/MA_table.png)\n**Here in our table we are having f^t which is the actual number of cupcaes we bring and Et the error according to the professor and ft the actual number the professor wanted us to bring. Looking at this data we will see that our f^t is dependant on the data of the previous month and 50% of its error and is further done by the formula:**\n![MAformula](https:\/\/1.bp.blogspot.com\/-sc5uHhoAO0c\/X0tPSt0sYzI\/AAAAAAAAopg\/-dsHqwkbu-MR6jX31BhJPmUHF4hgySTCQCLcBGAsYHQ\/s432\/MA%2Bformula.png)\n**This is above is a MA(1) furthermore there could also be a MA(2) model for this case that would look something like this:**\n![MA(2)Formula](https:\/\/1.bp.blogspot.com\/-pMnJQNr83jE\/X0tP9TrPkWI\/AAAAAAAAopo\/bnDa1ItKP9ApW5IKxB6sF7VyD_1oeUZMwCLcBGAsYHQ\/s862\/MA%25282%2529%2Bformula.png)","eec45a19":"The creation of multiple pickle files are made here for the prediction of each days first hour in a seperate pickle file. ","dd9401a7":"# **The Arima Model**\nLearning the ARIMA model would done by firstly getting an introduction to this model being brokendown:\n*  **AR-Autoregressive Model**\n*  **I-Integrated**\n*  **MA-Moving Average**\n*  **ARMA - AutoRegressive Moving Average**\n*  **ARIMA - AutoRegressive Integrated Moving Average**\n","1abf2bd9":"# ARMA Model \n\n\n**Now coming on to the ARMA model it is the joint use of the Autoregressive and moving average model incorperating each other at the same time to make a prediction using past data.\nFrom the below formula we can see that our AR and MA models have been combined together and this below is an ARMA(1,1) model:**\n![ARMA(1,1)](https:\/\/1.bp.blogspot.com\/-ExBuzdZ2nyg\/X0tNkjvIDmI\/AAAAAAAAoo0\/iqyNzLxJki4Opm7PPSrVGYGljZKj8INTwCLcBGAsYHQ\/s826\/ARMA%2528formula.png)\nNote-->**Now there could also be different ARMA models say for example ARMA(3,4) so here in our formula in the AR part we will not only take data from t-1 but also t-2 and t-3 same going for the MA part we would take data of past 4 months being t-1,t-2,t-3 and t-4.**\n","ce066c56":"we get a value counts of the drives during the hours per day.","7ac158f3":"# The AR Model\n\n![ARinto.png](https:\/\/1.bp.blogspot.com\/-nAoxr0Zjpm4\/X0qqR4Tb4PI\/AAAAAAAAooU\/rQQk_KlkOqk2HqsqYV9YoB0AqqBulYt5gCLcBGAsYHQ\/s700\/ARinto.png)\n**Let us see firstly the Statistics and theory of the AR Model. By the words autoregressive we come to know that the this model is going to be a regression and used to predict something based on past data.\nFor example -> Stocks, Property prices or the price of a product.**\n\n**So basically there must be a pattern in our data and we must be able to capture that pattern with the use of this model.**\n\n**Sample Case\nWe are distributors of milk and want to predict the correct amount of milk that must be produced every month.**\n\n**Step 1\nDraw a plot of the quantity of milk against time.**\n![AR%20graph1.png](https:\/\/1.bp.blogspot.com\/-6Pn01Laz65k\/X0qqR2qCFVI\/AAAAAAAAooQ\/oxaR2opEEdsWPEmYGxcxC364rL4C5yH8ACLcBGAsYHQ\/s1196\/AR%2Bgraph1.png)\n**define some variables we will use those in our final formula\nMt = quantity of milk this month.\nMt-1 = quantity of milk last month.\nMt-12 = quantity of milk last year or 12 months ago.**\n\n**Note --> make sure to take sufficient data but not so much thinking it would only improve results because they can end up in statistical errors such as over-fitting of data.**\n\n**Make a PACF(Partial autocorrelation function) plot (basic is that this plot shows different time periods. We must choose periods \nhaving PACF above the red bands(these are then areas where PACF is near to zero) so negelect the red bands.**\n\n![PACF%20graph.png](https:\/\/1.bp.blogspot.com\/-oXnbQCbD5VE\/X0qqR6FTDVI\/AAAAAAAAooM\/Z5hJt1XODbI8atHPUl85qhvcoSVFh9vKACLcBGAsYHQ\/s1096\/PACF%2Bgraph.png)\n\n# Finally our model would look like this:\n![formula.png](https:\/\/1.bp.blogspot.com\/-5qq21Fyvgeo\/X0qqTNf8j9I\/AAAAAAAAooY\/OSPLF15MZQoS-2ldNRQPPar2B8HGqrF-wCLcBGAsYHQ\/s1233\/formula.png)","792f99e5":"Now we will begin doing the hourly predicitons for the next 5 days.","af67ec59":"Initally we had done an AD Fuller test and found out that the data was not stationary so below we have made a new dataframe 'cal' and 'new_datacount' to get stationarity in our data and then in the same cell here we do an AD Fuller test again and from the results we can see our data is now stationery and rejects the null hypothesis as it has a value lower than the critical values.","cc705119":"# A Practical Approach (CodingProject-Uber Pickups)\nBelow we have done a practical implementation using python3 code on a Uber dataset of our ML knowledge discussed above.","179bd3e8":"# **This would be the manner to streamlit run the above python code:**\n![streamlitrun](https:\/\/1.bp.blogspot.com\/-pos4gvyTsrI\/X0tWVDCRaCI\/AAAAAAAAop4\/LO6G8Q3EiWsIyGfWzZd0feZT1HHg_EmGgCLcBGAsYHQ\/s639\/streamlit%2Brun.png)\n# **The preview of the streamlit presentation on my browser:**\n![streamlit1](https:\/\/1.bp.blogspot.com\/-c1oTc3wQ4Iw\/X0tWVNkUoTI\/AAAAAAAAop0\/dWZTQqsYaEk7lNywUZ_lYfd8BQH1UnmgACLcBGAsYHQ\/s611\/streamlit1.png)\n![streamlit2](https:\/\/1.bp.blogspot.com\/-dzjEJrMA9p0\/X0tWVFHlP5I\/AAAAAAAAop8\/UxnoFPRpZR0dzmQue35otnfraffTHUP9QCLcBGAsYHQ\/s593\/streamlit2.png)","143365d7":"I would like to humbly and sincerely thank my mentor [Rocky Jagtiani](https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/). He is more of a friend to me than a mentor. The Python for Data Science taught by him and various assignments we did and are still doing is the best way to learn and skill in Data Science field. See https:\/\/datascience.suvenconsultants.com once for more.","6b4197f8":"Myself [Aden Gomes](https:\/\/www.linkedin.com\/in\/aden-gomes-2b2a091b1\/) and my team member [Aryan Parab](https:\/\/www.linkedin.com\/in\/aryan-parab-0b44991b2\/) created many such notebooks as part of the course work under \"Master in Data Science Programme\" at [Suven](https:\/\/datascience.suvenconsultants.com\/) , under mentor-ship of [Rocky Jagtiani](https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/) .\n\nIn this notebook we will learn about Machine Learning Models and how to implement them into a dataset.\nThis notebook will be divided into 5 sections:\n1. Learning the ML Model ARIMA from its base models AR, MA and ARMA.\n2. The Stationarity test done on data and how to make data stationary.\n3. Learning the ML Model Random Forest.\n4. The implementation of the theory using runnable Python3 code (mainly how one can create an ARIMA model to predict the total number of pickups for the next 5 days and a Random Forest Regressor to predict the hourly pickups in the next 5 days on the uber pickups dataset).\n5. Streamlit Implementation ","81dba4e6":"Here is the second Arima model with the stationary data and therefore we obeserve a much improved rmse values","199d5e7a":"In the above output we unfortunately do not get an RMSE within 10% of mean that could be due to the the less amount of data hence the model could not predict a stronger prediction value.","237ae081":"Below we are loading the dataset from the data url into data. ","39e9e171":"# Stationarity\n\n**For the use of these models one of the things to be done is to check for stationarity of data.\nThere are a few conditions for stationarity:**\n1. Constant mean\n2. Constant standard deviation\n3. No Seasonality in data \n\n**Now checking for stationarity involves:\n1 -> Doing visual tests like we can do from these graphs below:**\n![graphsstatinoary](https:\/\/1.bp.blogspot.com\/-XBYTm7ReNrk\/X0tNmCrtigI\/AAAAAAAAopE\/v4IlVL-NBTQf0KDlM8q1hGNG25iR-P4uQCLcBGAsYHQ\/s867\/Stationarity%2BGraphs.png)\n\n**Here we can see that the 1st graph is not having a constanst standard deviation, graph 2 does not have constant mean and the graph 3 has seasonality in data.**\n\n**2 -> We can do a Global vs Local Test\nFor a brief on this we will take for example the mean of our entire test data which would be our global and compare that with small chunks of mean at different time intervals which would be our local data and then do a global vs local and come up with conclusions.**\n\n**3-> Augmented Dickey Fuller Test (ADF)\nAn augmented Dickey\u2013Fuller test (ADF) tests the null hypothesis that a unit root(a unit root is a feature that can cause problems in statistical inference involving time series models) is present in a time series sample. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. It is an augmented version of the Dickey\u2013Fuller test for a larger and more complicated set of time series models.\nThe augmented Dickey\u2013Fuller (ADF) statistic, used in the test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.**\n\n**Our data is stationery if it rejects the null hypothesis by having a value lower than the critical values given else the data is not stationary.**","ebb08079":"Here we use a lambda function to convert the text in all columns to lowercase Making a pandas DataFrame string column lowercase returns the column with every string in lowercase while any non-alphabetical characters remain the same.\n","c17a2474":"Creating an arima model over the data","54c62135":"# ARIMA Model\n**Now coming back to the ARIMA model lets see how it functions using our knowledge from the AR and MA and ARMA models so the step up from the ARMA to ARIMA is the I in the model which stands for INTEGRATED so we do this because if we come across some non-stationary data we must know that the ARMA models cannot be directly incorperated as the need to function on stationary data. So as we see from the below graphs that the mean is not constant in our first graph so after doing a differencing of 1st order using the I of our model we can see our graph has achieved stationarity as we are taking shorter periods of data from the previous linear graph:**\n![Arimagraph](https:\/\/1.bp.blogspot.com\/-qGTCS69DCZ0\/X0tNlpto3PI\/AAAAAAAAoo8\/JZjWVFIP15IgWsfgmbc5BPL-oU6kFD2eACLcBGAsYHQ\/s611\/Graphs%2Bof%2BARIMA.png)\n**Now this was a first order graph where we have done only one differencing and we can have more differenicing in data if stationarity is not occured by changing its order of d. The ARIMA model has 3 variables p,d,q where p is the order of the AR model, d being the order of differencing and d is the order of the moving average (MA) model, the formula is such:**\n![Arimaformula](https:\/\/1.bp.blogspot.com\/-OFNfq2Nv11U\/X0tNkxRE4RI\/AAAAAAAAoo4\/4Rne-RjzLnsUtG0Ee8z2hWuT4euiXjSXQCLcBGAsYHQ\/s333\/ARIMA%2Bformula.png)\n**Now we can also see the further a formula for the 'ak' to predict a further value using the 'al' which is the last point on the graph and various other variables:**\n![Arimapredformula](https:\/\/1.bp.blogspot.com\/-SFvs27iN9lk\/X0tNkmb2OYI\/AAAAAAAAoow\/lYs9WEddh6oPZXJzz3NRGKQPjpBgtQdhQCLcBGAsYHQ\/s1117\/ARIMA%2528pred_formula%2529.png)"}}