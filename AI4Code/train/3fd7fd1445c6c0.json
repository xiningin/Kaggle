{"cell_type":{"2b6122ba":"code","ceb1fc35":"code","e7c73085":"code","42a348cb":"code","5f32d222":"code","22370145":"code","a11fb336":"code","bff0e999":"code","54e7e225":"code","43842e42":"code","9f13db1f":"code","ee68872c":"code","d574a25c":"code","d0eb377a":"code","3329aa84":"code","506ada3c":"code","ba7e5dff":"code","6c26bf31":"code","e932f5f8":"code","2427f946":"code","751b7961":"code","eb841dd7":"code","4a708e44":"code","909dd94b":"code","5b2991d0":"code","a3610b3c":"code","7736bed2":"code","b370a1f7":"code","ce2dadfa":"code","c193fcd4":"code","dafdd15b":"code","56ee08f7":"code","8d2cc919":"code","1a4a4309":"code","24741e93":"code","5646020f":"code","c794be94":"code","8b401f74":"code","aa945e99":"markdown","2970b0d4":"markdown","589d7aca":"markdown","a8377690":"markdown","ab418f4d":"markdown","7c330dfc":"markdown","b6323702":"markdown","ec45b3bc":"markdown","f9079f9e":"markdown","3dd4ace6":"markdown","06a29f84":"markdown","3bbf95b7":"markdown"},"source":{"2b6122ba":"import pandas as pd\nimport numpy as np\nfrom pandas import Series\nfrom math import sqrt\n\n# metrics\nfrom sklearn.metrics import mean_squared_error\n\nimport statsmodels.api as sm\n\n# forecasting model\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfrom statsmodels.tsa.arima_model import ARIMA\n\n# for analysis\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom shapely.geometry import LineString\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 7\n\nfrom IPython.display import display, HTML\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline","ceb1fc35":"train_original=pd.read_csv('..\/input\/jetrail-traffic-dataset\/Train.csv')\ntest_original=pd.read_csv('..\/input\/jetrail-traffic-dataset\/Test.csv')\n\ntrain_original.dropna(inplace=True)\ntest_original.dropna(inplace=True)\ntest_original.drop(test_original.tail(1).index, inplace=True)\n\ntrain_df=train_original.copy()\ntest_df=test_original.copy()","e7c73085":"train_original['Datetime']=pd.to_datetime(train_original.Datetime, format='%d-%m-%Y %H:%M')\ntest_original['Datetime']=pd.to_datetime(test_original.Datetime, format='%d-%m-%Y %H:%M')\ntrain_df['Datetime']=pd.to_datetime(train_df.Datetime, format='%d-%m-%Y %H:%M')\ntest_df['Datetime']=pd.to_datetime(test_df.Datetime, format='%d-%m-%Y %H:%M')\n\n# generate day, month, year feature\nfor i in (train_original, test_original, train_df, test_df):\n    i['year']=i.Datetime.dt.year\n    i['month']=i.Datetime.dt.month\n    i['day']=i.Datetime.dt.day\n    i['hour']=i.Datetime.dt.hour","42a348cb":"# sampling for daily basis\ntrain_df.index=train_df.Datetime\ntest_df.index=test_df.Datetime\n\ntrain_df=train_df.resample('D').mean()\ntest_df=test_df.resample('D').mean()","5f32d222":"# split data for training and validation\ntrain=train_df.loc['2012-08-25':'2014-06-24']\nvalid=train_df.loc['2014-06-25':'2014-09-25']\nplt.figure(figsize=(12,7))\ntrain.Count.plot(label='Train')\nvalid.Count.plot(label='valid')\nplt.legend(loc='best')","22370145":"# determine rolling stats\nrolmean=train.Count.rolling(window=7).mean() #for 7 days\nrolstd=train.Count.rolling(window=7).std()\nrolmean.dropna(inplace=True)\nrolstd.dropna(inplace=True)\n\nplt.figure(figsize=(12,7))\nrolmean.plot(label='Rolmean', color='green')\nrolstd.plot(label='rolstd')\ntrain.Count.plot(label='Train')\nplt.legend(loc='best')","a11fb336":"# check for stationary\ndftest=adfuller(train.Count, autolag='AIC')\ndfout=pd.Series(dftest[0:4], index=['Test statistics', 'p-value', '#Lags used', 'Number of observation used'])\nfor key, val in dftest[4].items():\n    dfout['Critical value (%s)'%key]=val\n\nprint(dfout)","bff0e999":"# Log scale tranformation\n# estimating trend\ntrain_count_log=np.log(train.Count)\n# train_count_log.plot()\n# make TS to be stationary\nmoving_avg=train_count_log.rolling(window=7).mean()\nmoving_std=train_count_log.rolling(window=7).std()\n\ntrain_count_log.plot(label='Log Scale')\nmoving_avg.plot(label='moving_avg')\nmoving_std.plot(label='moving_std')\nplt.legend(loc='best')","54e7e225":"dif_log=train_count_log-moving_avg\ndif_log.dropna(inplace=True)\ndif_log.plot()","43842e42":"def test_stationary(timeseries):\n    # determine roling stats\n    mov_avg=timeseries.rolling(window=7).mean()\n    mov_std=timeseries.rolling(window=7).std()\n    #plot rolling stats\n    plt.figure(figsize=(12,7))\n    timeseries.plot(label='Original')\n    mov_avg.plot(label='Mov avg')\n    mov_std.plot(label='Mov std')\n    plt.legend(loc='best')\n    plt.title('Rolling mean & standard deviation')\n    \n    # dickey-fuller test\n    print('Result of Dickey-fuller test')\n    dftest=adfuller(timeseries, autolag='AIC')\n    dfout=pd.Series(dftest[:4], index=['Test stats', 'p-value', '#Lag used', 'Number of observation used'])\n    for key, val in dftest[4].items():\n        dfout['Critical value (%s)'%key]=val\n    print(dfout)\n    ","9f13db1f":"test_stationary(dif_log)","ee68872c":"plt.figure(figsize=(12,7))\nedw_avg=train_count_log.ewm(halflife=7, min_periods=0, adjust=True).mean()\ntrain_count_log.plot(label='Log scale')\nedw_avg.plot(label='Exponential Decay Weight MA')","d574a25c":"# ADCF test\ndif_edw=train_count_log-edw_avg\ndif_edw.dropna(inplace=True)\ntest_stationary(dif_edw)","d0eb377a":"dif_shift=train_count_log-train_count_log.shift()\ndif_shift.dropna(inplace=True)\ntest_stationary(dif_shift)","3329aa84":"# decom=seasonal_decompose(train_count_log)\ndecom=seasonal_decompose(dif_edw)\n\ntrend=decom.trend\nseasonal=decom.seasonal\nresidual=decom.resid\n\nfig=plt.figure(figsize=(12,7))\nplt.subplot(411)\ntrain_count_log.plot(label='Original')\nplt.subplot(412)\ntrend.plot(label='Trend')\nplt.subplot(413)\nseasonal.plot(label='Seasonal')\nplt.subplot(414)\nresidual.plot(label='Residual')\nfig.tight_layout()\n\ndecom_log_data=residual\ndecom_log_data.dropna(inplace=True)\ntest_stationary(decom_log_data)","506ada3c":"def find_zero_intersection(y):\n    # find intersection\n    first_line = LineString(np.column_stack((np.arange(len(y)), y)))\n    second_line = LineString(np.column_stack((np.arange(len(y)), [0]*len(y))))\n    intersection = first_line.intersection(second_line)\n    point=list(LineString(intersection).xy[0])\n    return (point, [0]*len(point))","ba7e5dff":"# select Exponential Decay Weight Transformation\nlag_acf=acf(dif_edw, nlags=20)\nlag_pacf=pacf(dif_edw, nlags=20, method='ols')\n\n# plot ACF\nfig=plt.figure(figsize=(12,7))\nplt.subplot(211)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(dif_edw)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(dif_edw)), linestyle='--', color='gray')\n# find intersection\nx,y=find_zero_intersection(lag_acf)\nplt.plot(x,y,'o')\nplt.title('Autocorrelation Function') \nprint('Q (MA part): ', x[0])\n\n# plot PACF\nplt.subplot(212)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(dif_edw)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(dif_edw)), linestyle='--', color='gray')\n# find intersection\nx,y=find_zero_intersection(lag_pacf)\nplt.plot(x,y,'o')\nplt.title('Partial Autocorrelation Function') \nprint('P (AR part): ', x[0])\n\nfig.tight_layout()","6c26bf31":"# AR Model\nmodel=ARIMA(train_count_log, order=(2,1,0))\nresults_AR=model.fit(disp=0)\n\nplt.figure(figsize=(12,7))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_AR.fittedvalues.dropna(inplace=True)\nresults_AR.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_AR.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('RSS: %.4f'%sum((df[0]-df['Count'])**2))","e932f5f8":"# MA Model\nmodel=ARIMA(train_count_log, order=(0,1,2))\nresults_MA=model.fit(disp=0)\n\nplt.figure(figsize=(12,7))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_MA.fittedvalues.dropna(inplace=True)\nresults_MA.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_MA.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('RSS: %.4f'%sum((df[0]-df['Count'])**2))","2427f946":"# ARIMA Model\nmodel=ARIMA(train_count_log, order=(2,1,2))\nresults_ARIMA=model.fit(disp=0)\n\nplt.figure(figsize=(12,7))\ndif_edw.plot(label='Exponentian Decay Differentiation')\nresults_ARIMA.fittedvalues.dropna(inplace=True)\nresults_ARIMA.fittedvalues.plot(label='Results AR')\ndf=pd.concat([results_ARIMA.fittedvalues, dif_edw], axis=1).dropna()\nplt.title('RSS: %.4f'%sum((df[0]-df['Count'])**2))","751b7961":"# using AR model\npred_ar_dif=pd.Series(results_AR.fittedvalues, copy=True)\npred_ar_dif_cumsum=pred_ar_dif.cumsum()\n\npred_ar_log=pd.Series(train_count_log.iloc[0], index=train_count_log.index)\npred_ar_log=pred_ar_log.add(pred_ar_dif_cumsum, fill_value=0)\npred_ar_log.head()\n\n# inverse of log is exp\npred_ar=np.exp(pred_ar_log)\nplt.figure(figsize=(12,7))\ntrain.Count.plot(label='Train')\npred_ar.plot(label='Pred')","eb841dd7":"def validation(order):\n    # forecasting for validation\n    valid_count_log=list(np.log(valid.Count).values)\n    history = list(train_count_log.values)\n    model = ARIMA(history, order=order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(valid))\n    mse = mean_squared_error(valid_count_log, output[0])\n    rmse = np.sqrt(mse)\n    print('Test MSE: %.3f' % mse)\n    print('Test RMSE: %.3f' % rmse)\n    \n    fig=plt.figure(figsize=(12,7))\n    # reverse transform\n    pred=np.exp(output[0])\n    pred=pd.Series(pred, index=valid.index)\n    valid.Count.plot(label='Valid')\n    pred.plot(label='Pred')\n    plt.legend(loc='best')\n    \n    fig=plt.figure(figsize=(12,7))\n    train.Count.plot(label='Train')\n    valid.Count.plot(label='Valid')\n    pred.plot(label='Pred', color='black')\n","4a708e44":"validation((2,1,0))","909dd94b":"def arima_predict_hourly(data, arima_order):\n    # forecasting for testing (Hourly based forecasting)\n    history = data\n    model = ARIMA(history, order=arima_order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(test_original))\n\n    submit=test_original.copy()\n    submit.index=submit.ID\n    submit['Count']=np.exp(output[0])\n    submit.drop(['Unnamed: 0','ID','Datetime','year','month','day','hour'], axis=1, inplace=True)\n    \n    # plot result\n    plt.figure(figsize=(12,7))\n    train_original.index=train_original.Datetime\n    submit.index=test_original.Datetime\n\n    train_original.Count.plot(label='Train')\n    submit.Count.plot(label='Pred')\n    return submit","5b2991d0":"# forecasting for testing (Hourly based forecasting)\nhistory = list(np.log(train_original.Count).values)\nmodel = ARIMA(history, order=(2,1,0))\nmodel_fit = model.fit(disp=0)\noutput = model_fit.forecast(steps=len(test_original))\n\nsubmit=test_original.copy()\nsubmit.index=submit.ID\nsubmit['Count']=np.exp(output[0])\nsubmit.drop(['Unnamed: 0','ID','Datetime','year','month','day','hour'], axis=1, inplace=True)","a3610b3c":"# plot result\nplt.figure(figsize=(12,7))\ntrain_original.index=train_original.Datetime\nsubmit.index=test_original.Datetime\n\ntrain_original.Count.plot(label='Train')\nsubmit.Count.plot(label='Pred')","7736bed2":"# submission\n# submit.to_csv('submit2.csv')\n# score 250 (Best Score)","b370a1f7":"# forecasting for testing (Daily based forecasting)\nhistory = list(np.log(train.Count).values)\nmodel = ARIMA(history, order=(2,1,0))\nmodel_fit = model.fit(disp=0)\noutput = model_fit.forecast(steps=len(test_df))\n\ntest_df['pred']=np.exp(output[0])","ce2dadfa":"train_original['ratio']=train_original['Count']\/train_original['Count'].sum() \ntemp=train_original.groupby('hour')['ratio'].sum().reset_index()\n\nmerge=pd.merge(test_df, test_original, on=('day','month', 'year'), how='left')\nmerge['hour']=merge.hour_y\nmerge['ID']=merge['ID_y']\nmerge=merge.drop(['Unnamed: 0_x','ID_x','year', 'month','hour_x',\n                  'Unnamed: 0_y','Datetime','hour_y','ID_y'], axis=1) \npred=pd.merge(merge, temp, on='hour', how='left')\n\n# convert the ratio to the original scale\npred['Count']=pred['pred']*pred['ratio']*24","c193fcd4":"plt.figure(figsize=(12,7))\nsubmit=pd.DataFrame(pred.Count.values, columns=['Count'], index=pred.ID)\nsubmit.index=test_original.Datetime\ntrain_original.Count.plot(label='Train')\nsubmit.Count.plot(label='Pred')","dafdd15b":"submit.index=test_original.ID\n# submit.to_csv('submit6.csv')\n# score 280","56ee08f7":"# evaluate an ARIMA model for a given order (p,d,q)\ndef evaluate_arima_model(arima_order):\n    # forecasting for validation\n    valid_count_log=list(np.log(valid.Count).values)\n    history = list(train_count_log.values)\n    model = ARIMA(history, order=arima_order)\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast(steps=len(valid))\n    mse = mean_squared_error(valid_count_log, output[0])\n    rmse = np.sqrt(mse)\n#     print('Test MSE: %.3f' % mse)\n#     print('Test RMSE: %.3f' % rmse)\n    return mse","8d2cc919":"# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(p_values, d_values, q_values):\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    mse = evaluate_arima_model(order)\n                    if mse < best_score:\n                        best_score, best_cfg = mse, order\n                    print('ARIMA%s MSE=%.3f' % (order,mse))\n                except:\n                    continue\n    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))","1a4a4309":"# evaluate parameters\np_values = [0, 1, 2, 4, 6, 8]\nd_values = range(0, 3)\nq_values = range(0, 3)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(p_values, d_values, q_values)","24741e93":"validation((8,1,2))","5646020f":"# forecasting for testing (Hourly based forecasting)\nhistory = list(np.log(train_original.Count).values)\nmodel = ARIMA(history, order=(8,1,2))\nmodel_fit = model.fit(disp=0)\noutput = model_fit.forecast(steps=len(test_original))\n\nsubmit=test_original.copy()\nsubmit.index=submit.ID\nsubmit['Count']=np.exp(output[0])\nsubmit.drop(['Unnamed: 0','ID','Datetime','year','month','day','hour'], axis=1, inplace=True)","c794be94":"# plot result\nplt.figure(figsize=(12,7))\ntrain_original.index=train_original.Datetime\nsubmit.index=test_original.Datetime\n\ntrain_original.Count.plot(label='Train')\nsubmit.Count.plot(label='Pred')","8b401f74":"# submission\nsubmit.index=test_original.ID\n# submit.to_csv('submit5.csv')\n# score 260","aa945e99":"## Build Model","2970b0d4":"## Test forecasting","589d7aca":"## Time Shift Transformation\n\nGiven a set of observation on the time series:\n* x0,x1,x2,x3,....xn \nThe shifted values will be:\n* null,x0,x1,x2,....xn  <---- basically all xi's shifted by 1 pos to right\n\nThus, the time series with time shifted values are:\n* null,(x1\u2212x0),(x2\u2212x1),(x3\u2212x2),(x4\u2212x3),....(xn\u2212xn\u22121)","a8377690":"## Decomposition\nCheck the nature of residual","ab418f4d":"## ARIMA PDQ Param Tuning","7c330dfc":"## Convert to hourly basis manually","b6323702":"## Exponential Decay Transformation","ec45b3bc":"* logscaleL=stationarypart(L1)+trend(LT)\n* movingavgoflogscaleA=stationarypart(A1)+trend(AT)\n* resultseriesR=L\u2212A=(L1+LT)\u2212(A1+AT)=(L1\u2212A1)+(LT\u2212AT)\n\nSince, L & A are series & it moving avg, their trend will be more or less same, Hence\nLT-AT nearly equals to 0\n\nThus trend component will be almost removed. And we have,\n\nR=L1\u2212A1 , our final non-trend curve","f9079f9e":"## Prediction & Reverse Transformations","3dd4ace6":"## Plotting ACF & PACF","06a29f84":"## Log Scale Transformation","3bbf95b7":"## Validation"}}