{"cell_type":{"be9e2d8f":"code","1a7e1937":"code","834b0333":"code","54d6d6bf":"code","7ff78c51":"code","d4513f1c":"code","1f5d398a":"code","dd912f4d":"code","c4eea09c":"code","72b72b8a":"code","fdb003c3":"code","26b913eb":"code","219de4ab":"code","c16a0919":"code","53fca00c":"code","48fbddd3":"code","046c832e":"code","04eb18be":"code","495d0ba5":"code","7ab2b530":"code","f5789cef":"code","1b4d6998":"code","573b1e72":"code","7cc6781b":"code","0ddb08c1":"code","3394d35b":"code","68ceb40c":"code","075055f1":"code","1b54b36d":"code","6e945373":"code","cb9c1f41":"code","c2061f7d":"code","67d921e6":"code","35d6b95b":"code","2f4390b0":"code","d6e85a6f":"code","3e69007b":"code","f63aabf3":"code","31e7f485":"code","8925ea4a":"code","b0c50025":"code","ae0899b5":"code","b5bbf458":"code","2af48234":"code","ed7b35c8":"code","87d4504b":"code","089b827a":"code","9903ceb1":"code","8f305f50":"code","58f8e054":"code","916dc765":"code","e4d36368":"code","28c29a4b":"code","0ea02999":"code","9fb1f0ba":"code","c1c0a1f5":"code","92efa328":"code","4568b26a":"code","7eec4bb0":"code","7a43a9c8":"code","fba1da9a":"code","062cd3f7":"code","8a36c290":"code","eea5579b":"code","dde143ad":"code","784f33d3":"code","480c7965":"code","62b428dd":"code","8e86318e":"code","d7628af9":"code","b79bb1dd":"code","d18bbf95":"code","0a2bbbf3":"code","f20b0708":"code","1c415b9a":"code","02612604":"code","4f750f95":"code","eba34ce4":"code","d2e60d5b":"code","d386c7ef":"code","e899d99b":"code","1b0b20bd":"markdown","cd803849":"markdown","82b770c2":"markdown","942d5309":"markdown","d110aea7":"markdown","aed9132e":"markdown","30de7188":"markdown","9c8a49f9":"markdown","5ea86be0":"markdown","bacb90f0":"markdown","26635d0f":"markdown","59ba7312":"markdown","f6ae28ec":"markdown","aa2fbb04":"markdown","db9a2816":"markdown","59f42271":"markdown","46351c9b":"markdown","a5347677":"markdown","c1cac4d6":"markdown","b3b75915":"markdown","d8a76b65":"markdown","e09221dc":"markdown","8c31a7a5":"markdown","59477084":"markdown","78ecac90":"markdown","096a67d5":"markdown","bcbd28b3":"markdown","6e13ee9b":"markdown","dabc6a66":"markdown","1b8cbfab":"markdown","e20e998c":"markdown","12a2b7dd":"markdown","d997c1bb":"markdown","2c43fe07":"markdown","60e4caaf":"markdown","168fa147":"markdown","bac7a11c":"markdown","63b5e45f":"markdown","91069429":"markdown","b46e4e6d":"markdown","0b5f28bc":"markdown","5892bbc9":"markdown","f884b226":"markdown","3706d517":"markdown","b19b497e":"markdown","9ba6a93c":"markdown","bd5eac29":"markdown","57598784":"markdown","3a267aaa":"markdown","ed3b60f7":"markdown","1cd9e4e7":"markdown","ef1bf4fe":"markdown","3dbbb892":"markdown","7ec3d80e":"markdown","684390ab":"markdown","895a8db0":"markdown","c1d96c78":"markdown","c2df4a2b":"markdown","268ca65b":"markdown","e0045d0f":"markdown","3e5ee327":"markdown","b37cab0c":"markdown","8203d4cc":"markdown","abcef41c":"markdown","ab293816":"markdown","8e45ce87":"markdown","3f605c08":"markdown","e9d671f5":"markdown","e7126c7c":"markdown","c3b64877":"markdown","bfd38292":"markdown","e62f4970":"markdown","6afe72f3":"markdown","235279d0":"markdown","feae89d7":"markdown","1a4e082f":"markdown","ce2845eb":"markdown","48dd12b4":"markdown","7a04fa97":"markdown"},"source":{"be9e2d8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a7e1937":"import numpy as np \nimport pandas as pd\nimport pandas\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n%matplotlib inline\nimport seaborn as sns; sns.set()\n\nimport graphviz \n\nfrom pandas_profiling import ProfileReport\n\nfrom scipy.stats import norm, skew, boxcox\nfrom sklearn.model_selection import KFold\n\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn import metrics\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","834b0333":"dataset = pandas.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\ndataset.sample(10)","54d6d6bf":"dataset.isnull().sum()","7ff78c51":"dataset.drop('Unnamed: 0', axis = 1, inplace = True)","d4513f1c":"dataset.info()","1f5d398a":"dataset.describe()","dd912f4d":"def bar_plot(variable):\n    # get feature\n    var = dataset[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize = (9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}:\\n{}\".format(variable,varValue))","c4eea09c":"categorical = (dataset.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","72b72b8a":"sns.set_style('darkgrid')\nfor c in categorical_list:\n    bar_plot(c)","fdb003c3":"numerical_float64 = (dataset.dtypes == \"float64\")\nnumerical_float64_list = list(numerical_float64[numerical_float64].index)\n\nprint(\"Numerical variables:\")\nprint(numerical_float64_list)","26b913eb":"plt.figure(figsize=(30,20))\n\nplt.subplot(3,3,1)\nsns.histplot(dataset['carat'], color = 'red', kde = True).set_title('carat Interval and Counts')\n\nplt.subplot(3,3,2)\nsns.histplot(dataset['depth'], color = 'green', kde = True).set_title('depth Interval and Counts')\n\nplt.subplot(3,3,3)\nsns.histplot(dataset['table'], kde = True, color = 'blue').set_title('table Interval and Counts')\n\nplt.subplot(3,3,4)\nsns.histplot(dataset['x'], kde = True, color = 'black').set_title('length Interval and Counts')\n\nplt.subplot(3,3,5)\nsns.histplot(dataset['y'], kde = True, color = 'yellow').set_title('width Interval and Counts')\n\nplt.subplot(3,3,6)\nsns.histplot(dataset['z'], kde = True, color = 'purple').set_title('depth Interval and Counts')","219de4ab":"dataset[[\"cut\",\"price\"]].groupby([\"cut\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","c16a0919":"mean_price_by_cut = dataset[[\"cut\",\"price\"]].groupby([\"cut\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","53fca00c":"plt.figure(figsize=(10,7))\nsns.barplot(x = mean_price_by_cut['cut'], y = mean_price_by_cut['price'], palette=\"Set3\")\n\nplt.ylabel('Price')\nplt.xlabel('cut', style = 'normal', size = 24)\n\nplt.xticks(rotation = 0, size = 12)\nplt.yticks(rotation = 0, size = 12)\n\nplt.title('Average \"price\" Relative to \"cut\"',color = 'black',fontsize=15)\nplt.show()","48fbddd3":"labels = dataset['cut'].value_counts().index\nsizes = dataset['cut'].value_counts().values\nmyexplode = [0.05, 0, 0, 0, 0]\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fbdf70']\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, explode = myexplode, shadow = True, startangle=90, colors=colors, autopct='%1.1f%%')\nplt.title(\"Distribution of Diamonds by 'cut'\",color = 'black',fontsize = 15)","046c832e":"dataset[[\"color\",\"price\"]].groupby([\"color\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","04eb18be":"mean_price_by_color = dataset[[\"color\",\"price\"]].groupby([\"color\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","495d0ba5":"plt.figure(figsize=(10,7))\nsns.barplot(x = mean_price_by_color['color'], y = mean_price_by_color['price'], palette=\"Set3\")\n\nplt.ylabel('Price')\nplt.xlabel('color', style = 'normal', size = 24)\n\nplt.xticks(rotation = 0, size = 12)\nplt.yticks(rotation = 0, size = 12)\n\nplt.title('Average \"price\" Relative to \"color\"',color = 'black',fontsize=15)\nplt.show()","7ab2b530":"labels = dataset['color'].value_counts().index\nsizes = dataset['color'].value_counts().values\nmyexplode = [0.05, 0.05, 0.05, 0.05, 0.05,0.05,0.05]\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fbdf70','#ac9fd0','#8b7470']\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, explode = myexplode, shadow = True, startangle=90, colors=colors, autopct='%1.1f%%')\nplt.title(\"Distribution of Diamonds by 'color'\",color = 'black',fontsize = 15)","f5789cef":"mean_price_by_clarity = dataset[[\"clarity\",\"price\"]].groupby([\"clarity\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","1b4d6998":"dataset[[\"clarity\",\"price\"]].groupby([\"clarity\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","573b1e72":"plt.figure(figsize=(10,7))\nsns.barplot(x = mean_price_by_clarity['clarity'], y = mean_price_by_clarity['price'], palette=\"Set3\")\n\nplt.ylabel('Price')\nplt.xlabel('Clarity', style = 'normal', size = 24)\n\nplt.xticks(rotation = 0, size = 12)\nplt.yticks(rotation = 0, size = 12)\n\nplt.title('Average \"price\" Relative to \"clarity\"',color = 'black',fontsize=15)\nplt.show()","7cc6781b":"labels = dataset['clarity'].value_counts().index\nsizes = dataset['clarity'].value_counts().values\nmyexplode = [0.05, 0.05, 0.05, 0.05, 0.05,0.05,0.05,0.05]\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99','#fbdf70','#ac9fd0','#8b7470','#ebe478']\n\nplt.figure(figsize = (8,8))\nplt.pie(sizes, labels=labels, explode = myexplode, shadow = True, startangle=90, colors=colors, autopct='%1.1f%%')\nplt.title(\"Distribution of Diamonds by 'clarity'\",color = 'black',fontsize = 15)","0ddb08c1":"dataset[[\"carat\",\"price\"]].groupby([\"carat\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","3394d35b":"# Use JointGrid directly to draw a custom plot\ng = sns.JointGrid(data=dataset, x=\"carat\", y=\"price\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","68ceb40c":"dataset[[\"depth\",\"price\"]].groupby([\"depth\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","075055f1":"g = sns.JointGrid(data=dataset, x=\"depth\", y=\"price\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","1b54b36d":"dataset[[\"table\",\"price\"]].groupby([\"table\"], as_index = False).mean().sort_values(by=\"price\",ascending = False)","6e945373":"g = sns.JointGrid(data=dataset, x=\"table\", y=\"price\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","cb9c1f41":"g = sns.JointGrid(data=dataset, x=\"x\", y=\"price\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","c2061f7d":"g = sns.JointGrid(data=dataset, x=\"y\", y=\"price\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","67d921e6":"g = sns.JointGrid(data=dataset, x=\"z\", y=\"price\", space=0, ratio=17)\ng.plot_joint(sns.scatterplot, color=\"g\", alpha=.6, legend=False)\ng.plot_marginals(sns.rugplot, height=1, color=\"g\", alpha=.6)","35d6b95b":"plt.figure(figsize=(30,15))\nsns.set_theme(style=\"darkgrid\")\nplt.subplot(2,3,1)\nsns.violinplot(x = 'cut', y = 'price', data = dataset, palette=\"Set3\")\nplt.subplot(2,3,2)\nsns.violinplot(x = 'color', y = 'price', data = dataset, palette=\"Set3\")\nplt.subplot(2,3,3)\nsns.violinplot(x = 'clarity', y = 'price', data = dataset, palette=\"Set3\")","2f4390b0":"###\n# Source for this code:\n# https:\/\/seaborn.pydata.org\/examples\/kde_ridgeplot.html\n###\n\nsns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n\n# Create the data\n\nx = dataset['price']\ng = dataset['color']\ndf = dataset\n\n# Initialize the FacetGrid object\npal = sns.cubehelix_palette(10, rot=-.25, light=.7)\ng = sns.FacetGrid(dataset, row=\"color\", hue=\"color\", aspect=15, height=1, palette=pal)\n\n# Draw the densities in a few steps\ng.map(sns.kdeplot, \n      'price',\n      bw_adjust=.5, clip_on=False,\n      fill=True, alpha=1, \n      linewidth=1.5)\n\n#g.map(sns.kdeplot, \"overall\", clip_on=False, color=\"w\", lw=2, bw_adjust=.5)\ng.map(plt.axhline, y=0, lw=2, clip_on=False)\n\n\n# Define and use a simple function to label the plot in axes coordinates\ndef label(x, color, label):\n    ax = plt.gca()\n    ax.text(0, .22, label, fontweight=\"bold\", color=color,\n            ha=\"right\", va=\"center\", transform=ax.transAxes)\n\n\ng.map(label, 'price')\n\n# Set the subplots to overlap\ng.fig.subplots_adjust(hspace=-5)\n\n# Remove axes details that don't play well with overlap\ng.set_titles(\"\")\ng.set(yticks=[])\ng.despine(bottom=True, left=True)","d6e85a6f":"f, ax = plt.subplots(figsize=(6.5, 6.5))\nsns.despine(f, left=True, bottom=True)\nsns.set_theme(style=\"darkgrid\")\nsns.scatterplot(x=dataset['carat'], y=dataset['price'],\n                hue=dataset['clarity'], \n                #size=\"gender\",\n                palette=\"Set3\",\n                hue_order=dataset['clarity'],\n                sizes=(1, 8), \n                linewidth=0,\n                data=dataset, ax=ax)","3e69007b":"plt.figure(figsize=(30,15))\n\nplt.subplot(2,3,1)\nsns.boxenplot(x=dataset['cut'], y=dataset['price'],\n              palette=\"Set3\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,2)\nsns.boxenplot(x=dataset['color'], y=dataset['price'],\n              palette=\"Set3\", \n              scale=\"linear\", data=dataset)\n\nplt.subplot(2,3,3)\nsns.boxenplot(x=dataset['clarity'], y=dataset['price'],\n              palette=\"Set3\", \n              scale=\"linear\", data=dataset)","f63aabf3":"sns.displot(data=dataset, x='price', hue='cut', kind='kde', multiple=\"fill\", clip=(0,None), palette=\"Set3\")","31e7f485":"sns.displot(data=dataset, x='price', hue='clarity', kind='kde', multiple=\"fill\", clip=(0,None), palette=\"Set3\")","8925ea4a":"sns.displot(data=dataset, x='price', hue='color', kind='kde', multiple=\"fill\", clip=(0,None), palette=\"Set3\")","b0c50025":"import pandas_profiling as pp\npp.ProfileReport(dataset)","ae0899b5":"#This code is retrieved from here: https:\/\/www.kaggle.com\/kanncaa1\/dataiteam-titanic-eda#Introduction\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","b5bbf458":"dataset.loc[detect_outliers(dataset,['carat', 'depth', 'table', 'x', 'y', 'z', 'price'])]","2af48234":"# drop outliers\ndataset = dataset.drop(detect_outliers(dataset,['carat', 'depth', 'table', 'x', 'y', 'z', 'price']),axis = 0).reset_index(drop = True)","ed7b35c8":"dataset = dataset.dropna()","87d4504b":"dataset = dataset[dataset.x != 0]\ndataset = dataset[dataset.y != 0]\ndataset = dataset[dataset.z != 0]","089b827a":"(dataset['x'] == 0).sum().sum()","9903ceb1":"(dataset['y'] == 0).sum().sum()","8f305f50":"(dataset['z'] == 0).sum().sum()","58f8e054":"dataset.agg(['skew', 'kurtosis']).transpose()","916dc765":"sns.set_style('darkgrid')\nsns.distplot(dataset[\"carat\"], fit = norm)\nplt.title('Skeweed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"carat\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"carat\", mu, \"carat\", sigma))\nprint()","e4d36368":"dataset[\"carat\"], lam = boxcox(dataset[\"carat\"])\n\nsns.set_style('darkgrid')\nsns.distplot(dataset[\"carat\"], fit = norm)\nplt.title('Transformed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"carat\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"carat\", mu, \"carat\", sigma))\nprint()","28c29a4b":"sns.set_style('darkgrid')\nsns.distplot(dataset[\"price\"], fit = norm)\nplt.title('Skeweed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"price\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"price\", mu, \"price\", sigma))\nprint()","0ea02999":"dataset[\"price\"], lam = boxcox(dataset[\"price\"])\n\nsns.set_style('darkgrid')\nsns.distplot(dataset[\"price\"], fit = norm)\nplt.title('Transformed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"price\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"price\", mu, \"price\", sigma))\nprint()","9fb1f0ba":"sns.set_style('darkgrid')\nsns.distplot(dataset[\"y\"], fit = norm)\nplt.title('Skeweed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"y\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"y\", mu, \"y\", sigma))\nprint()","c1c0a1f5":"dataset[\"y\"], lam = boxcox(dataset[\"y\"])\n\nsns.set_style('darkgrid')\nsns.distplot(dataset[\"y\"], fit = norm)\nplt.title('Transformed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"y\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"y\", mu, \"y\", sigma))\nprint()","92efa328":"sns.set_style('darkgrid')\nsns.distplot(dataset[\"z\"], fit = norm)\nplt.title('Skeweed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"z\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"z\", mu, \"z\", sigma))\nprint()","4568b26a":"dataset[\"z\"], lam = boxcox(dataset[\"z\"])\n\nsns.set_style('darkgrid')\nsns.distplot(dataset[\"z\"], fit = norm)\nplt.title('Transformed')\nplt.show()\n(mu, sigma) = norm.fit(dataset[\"z\"])\nprint(\"mu {} : {}, sigma {} : {}\".format(\"z\", mu, \"z\", sigma))\nprint()","7eec4bb0":"dataset.agg(['skew','kurtosis']).transpose()","7a43a9c8":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=True, cmap='Dark2_r', linewidths = 2)\nplt.show()","fba1da9a":"cat_var =  ['cut', 'color', 'clarity']\n\nfor i in range (0, len(cat_var)):\n    print(f'Unique Values for {cat_var[i]}', dataset[f'{cat_var[i]}'].unique())","062cd3f7":"onehotencoder = OneHotEncoder()","8a36c290":"one_hot = ['cut', 'color', 'clarity']\n\nfor i in range(0, len(one_hot)):\n    dataset[f'{one_hot[i]}'] = pd.Categorical(dataset[f'{one_hot[i]}'])\n    dummies = pd.get_dummies(dataset[f'{one_hot[i]}'], prefix = f'{one_hot[i]}_encoded')\n    dataset.drop([f'{one_hot[i]}'], axis=1, inplace=True)\n    dataset = pd.concat([dataset, dummies], axis=1)","eea5579b":"dataset","dde143ad":"plt.figure(figsize=(12,8)) \nsns.heatmap(dataset.corr(), annot=False, cmap='Dark2_r', linewidths = 2)\nplt.show()","784f33d3":"X = dataset.drop([\"price\"],axis =1)\ny = dataset[\"price\"]","480c7965":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Total # of sample in test dataset: {len(X_test)}')","62b428dd":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","8e86318e":"LinearRegression().get_params().keys()","d7628af9":"parameters = {'fit_intercept': ['True', 'False'],\n              'normalize': ['True', 'False'],\n             'copy_X': ['True', 'False'],\n             'positive': ['True', 'False'],\n             }\n\ngcv = GridSearchCV(LinearRegression(), parameters, cv=15, verbose = 1, n_jobs = -1).fit(X_train, y_train)\nprint(f'GridSearchView Best Score: {gcv.best_score_*100}')\nprint(f'GridSearchView Best Estimator: {gcv.best_estimator_}')\nprint(f'GridSearchView Best Params: {gcv.best_params_}')","b79bb1dd":"model = LinearRegression(copy_X='True', fit_intercept='True', normalize='True', positive='True', n_jobs = -1)\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n \ntrain_score = model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","d18bbf95":"sns.set_theme(style=\"white\")\nsns.jointplot(x=y_test, y=pred, kind='reg', line_kws={\"color\": \"red\"})","0a2bbbf3":"print(\"Explained Variance:\",metrics.explained_variance_score(y_test, pred))","f20b0708":"print(\"Max Error:\",metrics.max_error(y_test, pred))","1c415b9a":"print(\"Mean Squared Error:\",metrics.mean_squared_error(y_test, pred))","02612604":"print(\"Mean Absolute Error:\",metrics.mean_absolute_error(y_test, pred))","4f750f95":"print(\"Mean Absolute Percentage Error:\",metrics.mean_absolute_percentage_error(y_test, pred))","eba34ce4":"print(\"Median Absolute Error:\",metrics.median_absolute_error(y_test, pred))","d2e60d5b":"print(\"R^2:\",metrics.r2_score(y_test, pred))","d386c7ef":"n=X_test.shape[0]\np=X_test.shape[1] - 1\nR2 = metrics.r2_score(y_test, pred)\n\nadj_rsquared = 1 - (1 - R2) * ((n - 1)\/(n-p-1))\nprint('Adjusted R Squared: {}'.format(adj_rsquared))","e899d99b":"print(\"Root Mean Squared Error:\",np.sqrt(metrics.mean_squared_error(y_test, pred)))","1b0b20bd":"R\u00b2 is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination or the multiple coefficient of determination for multiple regression. To put it in simpler language, R-square is a measure of fit for linear regression models.","cd803849":"<a id = \"25\"><\/a>\n# Correlation","82b770c2":"<a id = \"36\"><\/a>\n### Mean Absolute Percentage Error","942d5309":"It is a quadratic metric that measures the magnitude of the error, often used to find the distance between the predictor's predicted values and the actual values of a machine learning model. The standard deviation of the RMSE estimation errors (residues). That is, the residuals are a measure of how far the regression line is from the data points; RMSE is a measure of how far these residues spread. In other words, it tells you how dense that data is around the line that best fits the data. The RMSE value can range from 0 to infinity. Negative oriented scores, i.e. predictors with lower values, perform better. A zero RMSE value means the model made no mistakes. RMSE has the advantage of punishing large errors more so it may be better suited to some situations. RMSE prevents the unwanted use of absolute values in many mathematical calculations.","d110aea7":"![image.png](attachment:b8a17bce-08a0-4544-af2a-5a5b06bdc2f9.png)","aed9132e":"<a id = \"3\"><\/a>\n## Variable Descriptions","30de7188":"<a id = \"18\"><\/a>\n# Pandas Profiling","9c8a49f9":"**Skewness code snippets taken from this notebook:** https:\/\/www.kaggle.com\/rafetcan\/heart-failure-modeling-skewness-97-5-acc\n\n**Thank you to Rafet Can Kandar for his informative notebook.**","5ea86be0":"<a id = \"10\"><\/a>\n## Clarity - Price","bacb90f0":"<a id = \"27\"><\/a>\n# Train-Test Split","26635d0f":"<a id = \"26\"><\/a>\n# One-Hot Encoding","59ba7312":"The median_absolute_error is particularly interesting because it is robust to outliers. The loss is calculated by taking the median of all absolute differences between the target and the prediction.\n\n* Median absolute error regression loss.\n\n* Median absolute error output is non-negative floating point. The best value is 0.0. ","f6ae28ec":"In regression and time series models, mean absolute percent error is frequently used to measure the accuracy of predictions. If there are zero among the real values, the MAPE cannot be calculated as there will be division by zero. Percentage error cannot exceed 100% for very low predictive values, but there is no upper limit for the percentage error for very high predictive values. When MAPE is used to compare the accuracy of estimators, it is biased as it systematically selects a method that is too low for estimates. This small but serious problem can be overcome with an accuracy criterion that finds the ratio of predicted values to their true values. This approach leads to estimates that can be interpreted in terms of the geometric mean.","aa2fbb04":"<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"block\">\n  <mtext>Max Error<\/mtext>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>y<\/mi>\n  <mo>,<\/mo>\n  <mrow>\n    <mover>\n      <mi>y<\/mi>\n      <mo stretchy=\"false\">^<\/mo>\n    <\/mover>\n  <\/mrow>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo>=<\/mo>\n  <mi>m<\/mi>\n  <mi>a<\/mi>\n  <mi>x<\/mi>\n  <mo stretchy=\"false\">(<\/mo>\n  <mrow>\n    <mo stretchy=\"false\">|<\/mo>\n  <\/mrow>\n  <msub>\n    <mi>y<\/mi>\n    <mi>i<\/mi>\n  <\/msub>\n  <mo>&#x2212;<\/mo>\n  <msub>\n    <mrow>\n      <mover>\n        <mi>y<\/mi>\n        <mo stretchy=\"false\">^<\/mo>\n      <\/mover>\n    <\/mrow>\n    <mi>i<\/mi>\n  <\/msub>\n  <mrow>\n    <mo stretchy=\"false\">|<\/mo>\n  <\/mrow>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>","db9a2816":"**We have 7 colors. They are decreasing at close rates. The 'G' is the majority.**","59f42271":"<a id = \"40\"><\/a>\n### Root Mean Squared Error","46351c9b":"![image.png](attachment:90b27995-83d5-4f69-b5c1-035927e04072.png)","a5347677":"<a id = \"29\"><\/a>\n## Hyper-Parameter Optimization","c1cac4d6":"![image.png](attachment:b3e3f649-18e6-4d21-adf7-fbaacff685ca.png)","b3b75915":"<a id = \"12\"><\/a>\n## Depth - Price","d8a76b65":"<a id = \"33\"><\/a>\n### Max Error\n\nThe max_error function computes the maximum residual error, a metric that captures the worst case error between the predicted value and the true value. In a perfectly fitted single output regression model, max_error would be 0 on the training set and though this would be highly unlikely in the real world, this metric shows the extent of error that the model had when it was fitted.\n\nIf yi is the predicted value of the i-th sample, and yi is the corresponding true value, then the max error is defined as:","e09221dc":"I applied Linear Regression on Diamonds Dataset in this notebook. I made a detailed review of the features in the dataset. I explained the Skewness problem and showed its solution. After preparing the dataset, I determined the parameters by applying Hyper-Parameter Optimization on Linear Regression. I examined the results of the model under the title of evaluation metrics and gave information about these metrics. In total, I explained 9 evaluation metrics and showed their calculations.","8c31a7a5":"**Note: Some of explanations taken from Scikit-Learn library's documentation. You can visit this address to if you want to know more about evaluation metrics about classification, clustering and regression.**\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html","59477084":"## Content:\n\n1. [Importing the Necessary Libraries](#1)\n1. [Read Datas & Explanation of Features & Information About Datasets](#2)\n   1. [Variable Descriptions](#3)\n      1. [Univariate Variable Analysis](#4)\n         1. [Categorical Variables](#5)\n         1. [Numerical Variables](#6)\n1. [Basic Data Analysis](#7)    \n   1. [Cut - Price](#8)\n   1. [Color - Price](#9)\n   1. [Clarity - Price](#10)\n   1. [Carat - Price](#11)\n   1. [Depth - Price](#12)\n   1. [Table - Price](#13)\n   1. [x - Price](#14)\n   1. [y - Price](#15)\n   1. [z - Price](#16)\n1. [Data Visualization](#17)\n1. [Pandas Profiling](#18)\n1. [Anomaly Detection](#19)\n1. [Skewness](#20)\n   1. [Carat](#21)\n   1. [Price](#22)\n   1. [y](#23)\n   1. [z](#24)\n1. [Correlation](#25)\n1. [One-Hot Encoding](#26)\n1. [Train-Test Split](#27)\n1. [Linear Regression](#28)\n   1. [Hyper-Parameter Optimization](#29)\n   1. [Model](#30)\n   1. [Evaluation Metrics](#31)\n      1. [Explained Variance](#32)\n      1. [Max Error](#33)\n      1. [Mean Squared Error](#34)\n      1. [Mean Absolute Error](#35)\n      1. [Mean Absolute Percentage Error](#36)\n      1. [Median Absolute Error](#37)\n      1. [R Squared](#38)\n      1. [Adjusted R Squared](#39)\n      1. [Root Mean Squared Error](#40)\n1. [Conclusion](#41)   ","78ecac90":"**It can be said that the 'Price' variable generally increases depending on the 'carat'. However, there is no linear relationship between them. Variables such as 'color', 'cut', 'clarity' may be affecting this.**","096a67d5":"Each additional argument added to a model always increases the R\u00b2 value.\n\nAs the independent variable is added, the model becomes more complex, when the model becomes complex, \"overfitting\" occurs. Hence R-squared increases.\n\nAdjusted R-square comes into play to solve such problems. The adjusted R-square compensates for each independent variable and only increases when each given variable improves the model above what is possible.","bcbd28b3":"<a id = \"24\"><\/a>\n## z","6e13ee9b":"<a id = \"32\"><\/a>\n### Explained Variance\n\n* Explained variance regression score function.\n\n* Best possible score is 1.0, lower values are worse.\n\nIf \\hat{y} is the estimated target output, y the corresponding (correct) target output, and Var is Variance, the square of the standard deviation, then the explained variance is estimated as follow:","dabc6a66":"![image.png](attachment:b5bee226-1d09-4b8f-87e1-7cfb74a23688.png)","1b8cbfab":"Pandas profiling is a useful library that generates interactive reports about the data. With using this library, we can see types of data, distribution of data and various statistical information. This tool has many features for data preparing. Pandas Profiling includes graphics about specific feature and correlation maps too. You can see more details about this tool in the following url: https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/","e20e998c":"* In the formula, n is the number of samples, xm is the arithmetic mean of the array (sample mean), and 's' is its standard deviation.\n\n* As the value of skewness moves towards plus infinity, the force of negative skewness increases as it moves towards positive and minus infinity.","12a2b7dd":"<a id = \"35\"><\/a>\n### Mean Absolute Error","d997c1bb":"Mean Square Error (MSE) is the most commonly used regression loss function. MSE is the average loss of frames per sample across the entire dataset. To calculate the MSE, sum all frame losses for individual samples and then divide by the number of samples.","2c43fe07":"It can be said that skewness is the name given to the distortion of symmetry in data distribution in continuous or in other words, non-categorical data sets. In other words, it is the criterion of asymmetry. In summary, it is expected that the distribution of the data sets will show a normal distribution, but if the available data is contrary to this, it can be mentioned that the data is skewed. These distortions are among the reasons that prevent some machine learning models from learning from data, similar to the effect of imbalanced datasets used for categorical data.\n\nIn the image below, there is a graph showing the number of records belonging to three different data sets. In the data set with a symmetrical distribution as in the green graph, mode median and mean values are equal. In other words, the most frequently found number is both the median number and the average. The situation in the orange graph is expressed as positive skewness, and the situation in the blue graph as negative skewness.","60e4caaf":"<a id = \"7\"><\/a>\n# Basic Data Analysis","168fa147":"<a id = \"6\"><\/a>\n#### Numerical Variables","bac7a11c":"![image.png](attachment:8170b385-1cb2-40fc-b477-836310fa1602.png)","63b5e45f":"<a id = \"37\"><\/a>\n### Median Absolute Error","91069429":"<a id = \"2\"><\/a>\n# Read Datas & Explanation of Features & Information About Datasets","b46e4e6d":"<a id = \"28\"><\/a>\n# Linear Regression","0b5f28bc":"![image.png](attachment:96965e38-1a4a-43e4-b537-c4fc06704a85.png)","5892bbc9":"Anomaly is one that differs \/ deviates significantly from other observations in the same sample. An anomaly detection pattern produces two different results. The first is a categorical tag for whether the observation is abnormal or not; the second is a score or trust value. Score carries more information than the label. Because it also tells us how abnormal the observation is. The tag just tells you if it's abnormal. While labeling is more common in supervised methods, the score is more common in unsupervised and semisupervised methods.","f884b226":"* Categorical Variables: ['cut', 'color', 'clarity']\n\n* Numerical Variables: ['carat', 'depth', 'table', 'x', 'y', 'z', 'price']","3706d517":"Absolute error is the difference between estimated values and actual values. To be exact, it is the mean of the absolute value of each difference between the actual value and the predicted value for that sample across the entire sample of the data set.","b19b497e":"Source for this explanation: https:\/\/teachtomachines.com\/2020\/07\/07\/log-donusumu-ile-carpiklik-giderme\/","9ba6a93c":"![image.png](attachment:2e0c1e75-5ad0-44ef-9845-4e5935a9aa34.png)","bd5eac29":"<a id = \"38\"><\/a>\n### R square","57598784":"<a id = \"5\"><\/a>\n#### Categorical Variables","3a267aaa":"There are more than 2 unique values for each feature. So, One-Hot Encoding will be more useful than Label Encoding. Label Encoding can be cause of overfitting.","ed3b60f7":"<a id = \"16\"><\/a>\n## z - Price","1cd9e4e7":"<a id = \"23\"><\/a>\n## y","ef1bf4fe":"* x is the property the model uses to predict.\n* The prediction (x) meaning is the predicted value according to the x property.\n* y is the true value.\n* N is the number of samples.","3dbbb892":"<a id = \"19\"><\/a>\n# Anomaly Detection","7ec3d80e":"<a id = \"11\"><\/a>\n## Carat - Price","684390ab":"**Diamonds with 'Ideal' cut make up the majority. It cannot be said that the cuts are evenly distributed. There are only two values close to each other.**","895a8db0":"* y is the true value.\n* y^ is the predicted value.\n\n*A lower value indicates better accuracy.*","c1d96c78":"<a id = \"20\"><\/a>\n# Skewness","c2df4a2b":"<a id = \"8\"><\/a>\n## Cut - Price","268ca65b":"<a id = \"30\"><\/a>\n## Model","e0045d0f":"<a id = \"15\"><\/a>\n## y - Price","3e5ee327":"![image.png](attachment:d92c7dfd-fb7d-4b90-83d4-900cd99e51a8.png)","b37cab0c":"<a id = \"13\"><\/a>\n## Table - Price","8203d4cc":"<a id = \"34\"><\/a>\n### Mean Squared Error","abcef41c":"* Carat --> weight of the diamond (0.2--5.01)\n* Cut --> Describe cut quality of the diamond. Quality in increasing order Fair, Good, Very Good, Premium, Ideal\n* Color --> Color of the diamond, with D being the best and J the worst\n* Clarity --> How obvious inclusions are within the diamond:(in order from best to worst, FL = flawless, I3= level 3 inclusions) FL,IF, VVS1, VVS2, VS1, VS2, SI1, SI2, I1, I2, I3\n* Depth --> The height of a diamond, measured from the culet to the table, divided by its average girdle diameter\n* Table --> The width of the diamond's table expressed as a percentage of its average diameter\n* Price --> the price of the diamond\n* x --> length mm\n* y --> width mm\n* z --> depth mm\n","ab293816":"**There are 8 different 'Clarity' values. Uneven distribution is not observed.**","8e45ce87":"<a id = \"14\"><\/a>\n## x - Price","3f605c08":"<a id = \"41\"><\/a>\n# Conclusion\n\nI tried to explain a lot of things on this notebook.\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* If you liked this notebook, please let me know :)\n\nThank you for your time.","e9d671f5":"<a id = \"31\"><\/a>\n## Evaluation Metrics","e7126c7c":"<a id = \"22\"><\/a>\n## Price","c3b64877":"# EDA and Linear Regression with Evaluation Metrics on Diamonds Dataset","bfd38292":"<a id = \"17\"><\/a>\n# Data Visualization","e62f4970":"<a id = \"21\"><\/a>\n## Carat","6afe72f3":"<a id = \"9\"><\/a>\n## Color - Price","235279d0":"R-squared does not indicate whether a regression model fits your data adequately. A good model can have a low R-squared value. On the other hand, a biased model can have a high R-squared value!","feae89d7":"<a id = \"1\"><\/a>\n# Importing the Necessary Libraries","1a4e082f":"<a id = \"39\"><\/a>\n### Adjusted R Squared","ce2845eb":"<a id = \"4\"><\/a>\n### Univariate Variable Analysis","48dd12b4":"<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"block\">\n  <mi>e<\/mi>\n  <mi>x<\/mi>\n  <mi>p<\/mi>\n  <mi>l<\/mi>\n  <mi>a<\/mi>\n  <mi>i<\/mi>\n  <mi>n<\/mi>\n  <mi>e<\/mi>\n  <mi>d<\/mi>\n  <mi mathvariant=\"normal\">_<\/mi>\n  <mrow><\/mrow>\n  <mi>v<\/mi>\n  <mi>a<\/mi>\n  <mi>r<\/mi>\n  <mi>i<\/mi>\n  <mi>a<\/mi>\n  <mi>n<\/mi>\n  <mi>c<\/mi>\n  <mi>e<\/mi>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>y<\/mi>\n  <mo>,<\/mo>\n  <mrow>\n    <mover>\n      <mi>y<\/mi>\n      <mo stretchy=\"false\">^<\/mo>\n    <\/mover>\n  <\/mrow>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo>=<\/mo>\n  <mn>1<\/mn>\n  <mo>&#x2212;<\/mo>\n  <mfrac>\n    <mrow>\n      <mi>V<\/mi>\n      <mi>a<\/mi>\n      <mi>r<\/mi>\n      <mo fence=\"false\" stretchy=\"false\">{<\/mo>\n      <mi>y<\/mi>\n      <mo>&#x2212;<\/mo>\n      <mrow>\n        <mover>\n          <mi>y<\/mi>\n          <mo stretchy=\"false\">^<\/mo>\n        <\/mover>\n      <\/mrow>\n      <mo fence=\"false\" stretchy=\"false\">}<\/mo>\n    <\/mrow>\n    <mrow>\n      <mi>V<\/mi>\n      <mi>a<\/mi>\n      <mi>r<\/mi>\n      <mo fence=\"false\" stretchy=\"false\">{<\/mo>\n      <mi>y<\/mi>\n      <mo fence=\"false\" stretchy=\"false\">}<\/mo>\n    <\/mrow>\n  <\/mfrac>\n<\/math>","7a04fa97":"![image.png](attachment:ac8bfd6b-e689-408a-8235-dc49bc0942ef.png)"}}