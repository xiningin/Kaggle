{"cell_type":{"23013c96":"code","5d3eb9ed":"code","4e49ac22":"code","283565ad":"code","79748135":"code","1281b394":"code","f634558f":"code","175e9bf6":"code","015ad4c9":"code","63a7787f":"code","949da613":"code","95424947":"code","9c71f2d1":"code","799fbc5f":"code","a0c901c1":"code","453be7d0":"code","1ff3dc74":"code","59f6e28e":"code","80134fec":"markdown","82a4bfd1":"markdown","153a3c71":"markdown","3d0abf5a":"markdown","ab7bdf7a":"markdown","200064b2":"markdown","28dca48e":"markdown","e17f0af9":"markdown","8bca0fba":"markdown","2a755949":"markdown","ccd7636e":"markdown","c1989047":"markdown","ec32d2ef":"markdown","cb1e05bb":"markdown","5a4c7fff":"markdown","02295a4f":"markdown","ae5944d3":"markdown"},"source":{"23013c96":"#necessary import \nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sb\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, validation_curve,KFold\nfrom sklearn.preprocessing import MinMaxScaler , Normalizer\nfrom sklearn.metrics import  explained_variance_score, mean_squared_error,mean_absolute_error\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n","5d3eb9ed":"import requests, io, zipfile\n\nr = requests.get(\"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00464\/superconduct.zip\")\nz = zipfile.ZipFile(io.BytesIO(r.content))\ndata = pd.read_csv(z.open('train.csv'))","4e49ac22":"data.head()","283565ad":"data.info()","79748135":"# display the correlation matrix\nplt.figure(figsize=(15,10))\nsb.heatmap(data.iloc[:,:-1].corr(),cmap='twilight_shifted')\nplt.show()","1281b394":"data.describe()","f634558f":"# split  dataset into test and train\nx = data.iloc[:, :-1]\ny = data.iloc[:, -1] \nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.1) ","175e9bf6":"scalerx = MinMaxScaler()\nx_train = scalerx.fit_transform(x_train)\nx_test = scalerx.transform(x_test)\n#scalery = MinMaxScaler()\n#y_train = scalery.fit_transform(np.array(y_train).reshape(-1, 1))","015ad4c9":"pca = PCA(n_components = 30)\npca.fit(x_train)","63a7787f":"plt.rcParams[\"figure.figsize\"] = (12,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, 31, step=1)\ny = np.cumsum(pca.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, marker='o', linestyle='--', color='grey')\n\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(0, 31, step=1)) #change from 0-based array index to 1-based human-readable label\nplt.ylabel('Cumulative variance (%)')\nplt.title('The number of components needed to explain variance')\n\nplt.axhline(y=0.9, color='g', linestyle='-')\nplt.text(0.5, 0.92, '90% of total variance', color = 'g', fontsize=14)\n\nax.grid(axis='x')\nplt.show()","949da613":"# function returns MAE and MSE of the fitted model\n# for each number of components between 1 and 20 using cross validation\ndef cross_validation(model,x_train,y_train):\n    mae=[]\n    mse=[]\n    for n in range(1,31):\n        pca = PCA(n_components = n)\n        pca.fit(x_train)\n        x_train_pca = pca.transform(x_train)\n        scores = cross_validate(model, x_train_pca,y_train, scoring=(\"neg_mean_squared_error\", \"neg_mean_absolute_error\"), cv=10)\n        mse.append(-scores['test_neg_mean_squared_error'].mean())\n        mae.append(-scores['test_neg_mean_absolute_error'].mean())\n    d = {'number of components': list(range(1,31)),'mae':mae,'mse':mse}\n    # plot mae and mse\n    fig, (ax1, ax2) = plt.subplots(2, 1)\n    ax1.plot(range(1,31),mae, marker='o')\n    ax1.title.set_text('cv-MAE vs number of PCA components')\n    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n    ax2.plot(range(1,31), mse, marker='o' )\n    ax2.title.set_text('cv-MSE vs number of PCA components')\n    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n\n    return pd.DataFrame(d)\n    \n","95424947":"regressor = DecisionTreeRegressor()\ncross_validation(regressor,x_train,y_train)","9c71f2d1":"pca = PCA(n_components = 10)\npca.fit(x_train)\nx_train_pca = pca.transform(x_train)\nregressor = DecisionTreeRegressor(random_state=0)\npath = regressor.cost_complexity_pruning_path(x_train_pca, y_train)\nccp_alphas=path.ccp_alphas\npd.DataFrame(ccp_alphas).describe()","799fbc5f":"pca = PCA(n_components = 10)\npca.fit(x_train)\nx_train_pca = pca.transform(x_train)\nregressor = DecisionTreeRegressor()\nalpha = np.linspace(0, 40, 20)\nmae_train=[]\nmae_val=[]\nmse_train=[]\nmse_val=[]\nfor a in alpha:\n    regressor = DecisionTreeRegressor(ccp_alpha=a)\n    scores = cross_validate(regressor, x_train_pca,y_train, scoring=(\"neg_mean_squared_error\", \"neg_mean_absolute_error\"),return_train_score=True ,cv=10)\n    mse_train.append(-scores['train_neg_mean_squared_error'].mean())\n    mse_val.append(-scores['test_neg_mean_squared_error'].mean())\n    mae_train.append(-scores['train_neg_mean_absolute_error'].mean())\n    mae_val.append(-scores['test_neg_mean_absolute_error'].mean())\n   \n \n\n    \n    # plot mae and mse\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(alpha,mae_train , label='train',marker='o',drawstyle=\"steps-post\")\nax1.plot(alpha,mae_val, label='test',marker='o',drawstyle=\"steps-post\")\nax1.title.set_text('MAE vs alpha training and testing set')\nax1.legend(loc=\"upper right\", frameon=False)\nax2.plot(alpha, mse_train,label='train',marker='o',drawstyle=\"steps-post\")\nax2.plot(alpha, mse_val,label='test',marker='o',drawstyle=\"steps-post\")\nax2.title.set_text('MSE vs alpha training and testing set')\nax2.legend(frameon=False)\n    ","a0c901c1":"pca = PCA(n_components = 10)\npca.fit(x_train)\nx_train_pca = pca.transform(x_train)\nx_test_pca = pca.transform(x_test)\nregressor = DecisionTreeRegressor(ccp_alpha=0) \nregressor.fit(x_train_pca,y_train)\n#predict the test data\ny_pred = regressor.predict(x_test_pca)\n","453be7d0":"mae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"mae score = \",mae)\nprint(\"mse score = \",mse)","1ff3dc74":"model = RandomForestRegressor()\n\n#candidate values of hyperparameters\nparameters = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'n_estimators': [100, 300, 500,700]}\ngrid = GridSearchCV(model, parameters, cv=3, n_jobs=-1)\ngrid.fit(x_train_pca, y_train.ravel())\nprint('Best depth:',grid.best_estimator_.max_depth) \nprint('Best number of features:',grid.best_estimator_.max_features)\nprint('Best number of trees:',grid.best_estimator_.n_estimators)","59f6e28e":"#Random Forest regressor\nmodel=RandomForestRegressor(n_estimators=500,bootstrap=True,random_state=21,verbose=0,max_depth=100,max_features=3)\nmodel.fit(x_train_pca,y_train.ravel())\ny_pred = model.predict(x_test_pca)\n\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"mae score = \",mae)\nprint(\"mse score = \",mse)","80134fec":"The best lowest cross-validation (MAE,MSE) point occurs when n=10 components which explain almost 90% of the total variance.\nnow we use another cross validation model to choose the best complexity parameter $\\alpha$, but before let's take a look at the range of effective alpha finded by the regressor during the training time","82a4bfd1":"From the graph above , the best alpha value that minimizes both train and test error is 0","153a3c71":"as we see there is a huge number of values, this means that the tree is very deep and prone to overfitting. Obviously we can not gridsearch them all, but we will select some values( far enough from the extremities ) and look after the best that gives sufficient performance on the test set","3d0abf5a":"<br>\n<br>\n\n<br>\n\n<br>\n\n\n#    <center> \u2744\u2744 Prediction of Superconducting Critical Temperature \u2744\u2744 <center>\n<br>","ab7bdf7a":"### PCA application","200064b2":"This curve quantifies how much of the total 82-dimensional variance is contained within the first 30 components. As we see, the first 10 components contain 90% of the variance, \n#### one problem here is how to choose the optimal number of components ?\nthat's what we will try to answer using cross validation technique in the next part","28dca48e":"### features scaling\n","e17f0af9":"# Introduction","8bca0fba":"# 3.Training ","2a755949":"## random forest regressor \n\nfor the random forest regressor, there exist a considerable number of hyperparameters must be set by the user before training, this includes the number of decision trees in the forest and the number of features considered by each tree when splitting a node. to find out the probably best value candidates, we'll conduct the gridsearch technique, the results will then be used to fit the model on the whole trainnig set.\n\nthese are the parameters we're trying to adjust:\n* n_estimators = number of trees in the foreset\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* bootstrap = method for sampling data points (with or without replacement)","ccd7636e":"Principal Component Analysis (PCA) tends to reduce the dimensions of the data. It focus  on the maximum variance amount to convert correlated variables  into a set of orthogonal - mutually uncorrelated - variables called principal components such that the first principal component accounts for the maximum proportion of the variance of the original dataset, and subsequent orthogonal components account for the maximum  proportion  of  the  remaining  variance.  The process steps of PCA are as follows:    \n\nlet $Z$ be the standardized input matrix :\n \n1. Compute the mean vector of $Z$.  \n2. Compute the covariance matrix of data.  \n3. Compute the eigenvalue and eigenvector matrix of covariance matrix, let $E$ be the matrix of eigenvectors.\n4. Sort the eigenvalues \u03bb\u2081, \u03bb\u2082, \u2026, \u03bbp from largest to smallest, and then sort the eigenvectors in $E$ accordingly, let $E^*$ be the sorted matrix.\n5. Form the components using $E^*$ as weighting vector. $Z^* = ZE^*$\n\ntypically we take the first components explaining at least 90% of the common variation.  \n\nPCA technique is sensitive to the scaling of the features since it's based on the covariance matrix, a look at the summary table below shows that there are large differences between the ranges of variables in our dataset, those variables with larger ranges might dominate over those with small ranges , that is the reason why it is critical to perform standardization prior to PCA.","c1989047":"Except for number_of_elements and critical temperature; all of the variables are derived using highly correlated mathematical functions; mean, weighted mean, geometric mean, weighted geometric mean, standard deviation, weighted standard deviation, entropy, weighted entropy, range and weighted range calculations. This can be shown using the correlation matrix.\n","ec32d2ef":"the goal of this notebook is modeling critical temperature of a superconductor based on its chemical properties using two regression models, Decision tree and Random forest.\n\nThe first part will tackle the theory behind the algorithms and gives a short presentation of the relative parameters and metrics.  \nThe second part will be reserved to the dataset preprocessing, after introducing the necessary techniques to that purpose.  \nAnd in the last part, an evaluation of the models' performance will be presented as well as a comparative analysis to determine wich one gets to decode the pattern behind data.\n\n<br>\n<br>\n\n> \ud83d\udccd\ud83d\udccdNB: all you questions are welcome \ud83d\ude03\ud83d\ude03\n\n<br> \n<br> \n<br> \n<br> \n\n\n**What is superconductivity?**\n\nSuperconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic flux fields occurring in certain materials, called superconductors, when cooled below a characteristic critical temperature. Superconductors are widely used in many industries, e.g. the Magnetic Resonance Imaging (MRI) in health care, electricity transportation in energy industry and magnetic separation, etc.\nSuperconductivity Data Set used in this notebook is hosted in UCI machine learning [repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Superconductivty+Data)   \n\nthe repository contains two files:   \n* (1) train.csv contains 81 features extracted from 21263 superconductors along with the critical temperature in the 82nd column, and this will be our main dataset.\n* (2) unique_m.csv contains the chemical formula broken up for all the 21263 superconductors from the train.csv file. (which we will not use)\n","cb1e05bb":"<img src=\"https:\/\/static.spektrum.de\/fm\/912\/thumbnails\/Supraleiter-schwebender-Magnet-iStock_49768936_ktsimage.jpg.3112312.jpg\">","5a4c7fff":"# 1.Some mathematics\n<br>\n\n## Regression tree algorithm\n\nA regression tree is built through a process known as binary recursive partitioning, which is an iterative process that splits the data into partitions or branches, and then continues splitting each partition into smaller groups as the method moves up each branch.\n\nGiven a training set $X_n=x_1, x_2, ... x_n$ with responses $Y_n=y_1, y_2, ... y_n$ \n\nInitially, the algorithm begins allocating the data into the first two partitions considering every splitting variable $j$ and split point $s$    \n  \n  \n <center> $ R_1(j,s) = \\{ X| X_j \\le s \\} $ and $ R_2(j,s) = \\{ X| X_j \\gt s \\} $ <\/center>\n   \n   \nthen the algorithm selects the binary split that minimizes   \n  \n   <center> $ [ \\sum\\limits_{x_i \\in R_1(j,s)} (y_i-c_1)^2 + \\sum\\limits_{x_i \\in R_2(j,s)} (y_i-c_2)^2 ]$ <\/center>\n   \n with $c_k= avg(y_i | x_i\\in R_k(j,s))$ ,  $k = 1,2$  \n   \n <br>\n   \n   Having found the best split, This splitting rule is then applied to each of the resulting branches. This process continues until each node or partition reaches a user-prespecified minimum node size and becomes a terminal node. (If the quantity $\\sum\\limits_{x_i \\in node} (y_i-c_{node})^2$ in a node is zero, then that node is considered a terminal node even if it has not reached the minimum size.)  \n       \n\nA very large tree might overfit the data, while a small tree might not capture the important structure. this tradeoff is governed by the tree size, and to find the best tree size, we use the cost complexity pruning algorithm:\n> * start by growing a large tree $T_0$ and stop spplitting process only when minimum node size is reached.  \n* for a given $\\alpha$ find $T(\\alpha) \\subseteq T_0$ that minimize   \n  \n  \n <center> $ C_{\\alpha}(T) = C(T) + \\alpha|\\tilde T|$ <\/center>    \n   \n   \n>with $T$ define any subtree $\\subset T_0$, and $|\\tilde T|$  denote the number of terminal nodes in $T$,  and  $ C(T) =\\sum\\limits_{t \\in \\tilde T} N_t Q_t$ , $t$ indexes terminal nodes\n  \n  <center> $ Q_t= \\frac 1{N_t} \\sum\\limits_{x_i \\in R_t}(y_i-\\hat c_t)^2 $ <\/center>    \n   <center> $ N_t= cardinal\\{x_i \\in R_t\\} $ <\/center>  \n     \n     \n   <center> $ \\hat c_t= \\frac 1{N_t} \\sum\\limits_{x_i \\in R_t}y_i $ <\/center>\n     \n> * try the above for different values of $\\alpha$ and pick $\\hat \\alpha$ that minimizes the cross-validated error. It's clear that for $\\alpha = 0$ the solution is the full tree $T_0$ and with increasing $\\alpha$ the tree becomes smaller and smaller implying less complexity.\n\nOnce the final tree $T(\\hat \\alpha)$ is built, for every new observation that falls into a terminal node, the predicted response will be the mean value of responses in that node.\n\n<br>\n<br>\n\n## Random forest regression algorithm\n\nRandom forest is a kind of regression trees bagging, the idea behind is to construct multiple trees at training time and averaging the results to end up with a single prediction model highly accurate.  \nThe conception of the algorithm is simple but requires a short explanation of each step:  \n* For $b=1, ...,B$ :  \n\n * Sample from $X_n, Y_n$, with replacement, n-sized sample  $X_b, Y_b$ .  \n \n * for each bootsrapped training set $X_b, Y_b$, a random selection of $m$ predictors is chosen as split candidates from the full set of $p$ predictors, typically we choose $m= \\sqrt p$.  \n \n * Train a regression tree $\\hat T_b$ on $X_b, Y_b$.   \n   \n* for each new sample $x^* $, the predicted response is $ \\hat y^* = \\frac 1{B} \\sum\\limits_{b=1}^{B} \\hat y^b$  \n  \n  \n<br>\n<br>\n\n## Performance metrics\n  \nmany performance metrics exist to evaluate the quality of fit of a regression model, The most commonly used are:\n\n* MSE (Mean Squared Error), is defined as the average squared distance between the actual score and the predicted score, a good regression model is characterized by a small MSE and vice-versa\n\n<center> $MSE= \\frac { \\sum\\limits_{i=1}^n (y_i-\\hat y_i)^2 }n $ <\/center>\n<br>\n* MAE (Mean Absolute Error), is the average of absolute differences between our target and predicted values,\n\n<center> $MAE= \\frac { \\sum\\limits_{i=1}^n |y_i-\\hat y_i| }n $ <\/center>\n\n<br>\n\nthe main difference between MSE and MAE is that this latter is less sensitive to outliers, how ?? ===>\nImagine a target with a spread of values in $[3,12] $ and some outliers ranging from $70$ to $100$, the squared error between 100 and 3 is huge $(100-3)^2=9409$ compared to the absolute error  $|100-3|=97$, which gives impression that the MSE is huge as well. The algorithm by trying to minimize this MSE it will gives more weights to outliers, this scenario will certainly lead to overfitted and biased model. \n\nSo MAE loss is more preferable if we detect the existence of a outliers.","02295a4f":"# 2. Data preprocessing\n\none of the most important steps in data preprocessing is feature selection which aims at reducing the dimensionality of the problem, and eliminate redundant or irrelevant (noisy) variables. Benefits are twofold: it decreases complexity of the predictive model, and allows more accurate prediction since high-dimensionality makes predictive models more prone to overfitting, and estimates of parameters more variant.   \n\nTwo are the main approaches to feature selection:\n* Filter methods: They attempt to assess the merits of features from the data, ignoring the effects of the selected feature subset on the performance of the learning algorithm. Examples are methods that select variables by ranking them through compression techniques (like PCA), or by computing correlation or a more advanced similarity measure such as minimum redundancy maximum relevance (mRMR) with the output.   \n* Wrapper methods: these methods assess subsets of variables according to their usefulness to a given predictor. The method conducts a search for a good subset using the learning algorithm itself as part of the evaluation function. The problem boils down to a problem of stochastic state space search. Example are the stepwise methods proposed in linear regression analysis.\n\n## Correlation Analysis Between Predictors\n\nafter loading the training dataset we will try to investigate correlation between features, and if any features selection technique might be applied\n\n","ae5944d3":"As expected ,the randomforest shows better performance, than simple tree, the main cause is that the decision tree is more prone to overfit, however the randomforest aggregate many decision trees to limit overfitting as well as error due to bias and therefore yield great results."}}