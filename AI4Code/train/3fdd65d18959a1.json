{"cell_type":{"7f20880a":"code","811c1fcf":"code","6b5a17e1":"code","7e0ae24c":"code","6ad32fcc":"code","df4b4b44":"code","f69687e0":"code","dcfce274":"code","4ea14ae9":"code","1e93864b":"code","d6ab1397":"code","caf36e12":"code","a609df4c":"code","92144297":"code","45b43d5a":"code","5e285b4d":"code","c034196b":"code","3ea9b035":"code","a39c12e6":"markdown","8a6656bf":"markdown","8ef29be8":"markdown","9fa7543b":"markdown","7a7759b7":"markdown","c057e89f":"markdown","34e667ee":"markdown","eba86063":"markdown","7f5b0a36":"markdown","41bc090b":"markdown","e34a3d2e":"markdown","2f01ccb5":"markdown","f253cc92":"markdown","36adbd44":"markdown","80d240eb":"markdown","13c491ec":"markdown","8e4b7f8e":"markdown","a07eeb4e":"markdown","a9e420b8":"markdown","01ef3c5e":"markdown","78be12bb":"markdown","0eb2657b":"markdown"},"source":{"7f20880a":"import numpy as np\nimport pandas as pd\n\n# Pandas uses something called a dataframe. It is a \n# 2D data structure that can hold multiple data types.\n# Columns have labels.\n\n# Series are built on top of NumPy arrays. \n# Create a series by first creating a list\nlist_1 = ['a', 'b', 'c', 'd']\n# I can define that I want the series indexes to be the\n# provided labels\nlabels = [1, 2, 3, 4]\nser_1 = pd.Series(data=list_1, index=labels)\n\n# You can also add a NumPy array\narr_1 = np.array([1, 2, 3, 4])\nser_2 = pd.Series(arr_1)\n\n# You can quickly add labels and values with a dictionary\ndict_1 = {\"f_name\": \"Derek\", \n              \"l_name\": \"Banas\", \n              \"age\": 44}\nser_3 = pd.Series(dict_1)\n\n# Get data by label\nser_3[\"f_name\"]\n\n# You can get the datatype\nser_2.dtype\n\n# You can perform math operations on series\nser_2 + ser_2\nser_2 - ser_2\nser_2 * ser_2\nser_2 \/ ser_2\n\n# You can pass them into NumPy methods\n# See NumPy tutorial for more math methods\nnp.exp(ser_2)\n\n# The difference between Series and ndarray is that operations\n# align by labels\n# Create a series from a dictionary\nser_4 = pd.Series({4: 5, 5: 6, 6: 7, 7: 8})\n# If labels don't align you will get NaN\nser_2 + ser_4\n\n# You can assign names to series\nser_4 = pd.Series({8: 9, 9: 10}, name='rand_nums')\nser_4.name","811c1fcf":"from numpy import random\n\n# Create random matrix 2x3 with values between 10 and 50\narr_2 = np.random.randint(10, 50, size=(2, 3))\n\n# Create DF with data, row labels & column labels\ndf_1 = pd.DataFrame(arr_2, ['A', 'B'], ['C', 'D', 'E'])\n\n# Create a DF from multiple series in a dict\n# If series are of different lengthes extra spaces are NaN\ndict_3 = {'one': pd.Series([1., 2., 3.], index=['a', 'b', 'c']),\n         'two': pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}\ndf_2 = pd.DataFrame(dict_3)\ndf_2\n\n# from_dict accepts a column labels and lists\npd.DataFrame.from_dict(dict([('A', [1,2,3]), ('B', [4,5,6])]))\n\n# You can assign the keys as row labels and column labels separate\n# with orient='index'\npd.DataFrame.from_dict(dict([('A', [1,2,3]), ('B', [4,5,6])]),\n                      orient='index', columns=['one','two','three'])\n\n# Get number of rows and columns as tuple\nprint(df_1.shape)","6b5a17e1":"# Grab a column\ndf_1['C']\n# Get multiple columns\ndf_1[['C', 'E']]\n\n# Grabb a row as a series\ndf_1.loc['A']\n# Grab row by index position\ndf_1.iloc[1]\n\n# Grab cell with Row & Column\ndf_1.loc['A', 'C']\n# Grab multiple cells by defining rows wanted & the\n# columns from those rows\nprint(df_1.loc[['A', 'B'], ['D', 'E']])\n\n# Make new column\ndf_1['Total'] = df_1['C'] + df_1['D'] + df_1['E']\ndf_1\n\n# You can perform multiple calculations\ndf_2['mult'] = df_2['one'] * df_2['two']\ndf_2\n\n# Make a new row by appending\ndict_2 = {'C': 44, 'D': 45, 'E': 46}\nnew_row = pd.Series(dict_2, name='F')\ndf_1 = df_1.append(new_row)\n\n# Delete column and set inplace to True which is required\n# because Pandas tries to help you not delete data\n# by accident\ndf_1.drop('Total', axis=1, inplace=True)\ndf_1\n# Delete a row\ndf_1.drop('B', axis=0, inplace=True)\ndf_1\n\n# Create a new column and make it the index\ndf_1['Sex'] = ['Men', 'Women']\ndf_1.set_index('Sex', inplace=True)\n\n# You can reset index values to numbers\n#df_1.reset_index(inplace=True)\ndf_1\n\n# Assign can be used to create a column while leaving the\n# original DF untouched\ndf_2.assign(div=df_2['one'] \/ df_2['two'])\n\n# You can pass in a function as well\ndf_2.assign(div=lambda x: (x['one'] \/ x['two']))\n\n# Combine DataFrames while keeping df_3 data unless\n# there is a NaN value\ndf_3 = pd.DataFrame({'A': [1., np.nan, 3., np.nan]})\ndf_4 = pd.DataFrame({'A': [8., 9., 2., 4.]})\ndf_3.combine_first(df_4)","7e0ae24c":"arr_2 = np.random.randint(10, 50, size=(2, 3))\ndf_1 = pd.DataFrame(arr_2, ['A', 'B'], ['C', 'D', 'E'])\nprint(df_1)\n\n# You can use conditional operators to retrieve a table\n# based on the condition\nprint(\"Greater than 40\\n\", df_1 > 40.0)\n\n# You can use comparison operater functions as well like\n# gt, lt, ge, le, eq, ne\nprint(\"Greater than 45\\n\", df_1.gt(45.0))\n\n# You can place conditions in brackets as well\nbool_1 = df_1 >= 45.0\ndf_1[bool_1]\n\n# Get bools for a column\ndf_1['E'] > 40\n\n# Return a row if cell value in column matches a condition\ndf_1[df_1['E']>30]\n\n# You can focus on a column based on resulting dataframe\ndf_2 = df_1[df_1['E']>30]\ndf_2['C']\n\n# You can stack these commands\nprint(df_1[df_1['E']>20]['C'])\nprint()\n\n# You can also grab multiple columns\nprint(df_1[df_1['E']>20][['C', 'D']])\nprint()\n\n# You can use multiple conditions\narr_3 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\ndf_2 = pd.DataFrame(arr_3, ['A', 'B', 'C'], ['X', 'Y', 'Z'])\nprint(df_2, \"\\n\")\n# You can use or | to combine conditions as well\ndf_2[(df_2['X']>3) & (df_2['X']<7)]","6ad32fcc":"!pip install pymysql","df4b4b44":"import pymysql\n\n# Read a CSV file\n# Type pd.read_ [TAB] to see the file types you can read\ncs_df = pd.read_csv('..\/input\/icecreamsales\/ComputerSales.csv')\n\n# Save a CSV file, but don't save the index as a column\n# cs_df.to_csv('ComputerSalesBU.csv', index=False)\n\n# You can read data from Excel, but not formulas and macros\npd.read_excel('..\/input\/icecreamsales\/Financial Sample.xlsx',0)\n\n# Write to Excel\n# cs_df.to_excel('ComputerSales.xlsx')\n\n# Check if written\n# pd.read_excel('ComputerSales.xlsx',0)\n\n# Read from MySQL Database\n# try:\n#     db_connection = pymysql.connect(db='students', user='studentadmin', passwd='TurtleDove', host='localhost', port=3306)\n\n#     stud_df = pd.read_sql('SELECT * FROM students', con=db_connection)\n#     # print(stud_df)\n# except Exception as e:\n#     print(\"Exception : {}\".format(e))\n# finally:\n#     db_connection.close()\n    \n\n# Write to table \n# try:\n#     db_connection = pymysql.connect(db='students', user='studentadmin', passwd='TurtleDove', host='localhost', port=3306)\n#     # Used to issue queries\n#     cursor = db_connection.cursor()\n#     # Query to enter new student\n#     insert_stmt = \"INSERT INTO students VALUES(NULL, 'Frank', 'Silva', 'fsilva@aol.com', '666 Hell St', 'Yakima', 'WA', 98901, '792-223-8966', '1959-2-22', 'M', NOW(), 3.50)\"\n#     # Execute query\n#     cursor.execute(insert_stmt)\n#     # Commit changes to DB\n#     db_connection.commit()\n#     stud_df = pd.read_sql('SELECT * FROM students', con=db_connection)\n#     print(stud_df)\n# except Exception as e:\n#     print(\"Exception : {}\".format(e))\n# finally:\n#     db_connection.close()\n\n# Just get 1 column of data \ncs_df_st = pd.read_csv('..\/input\/icecreamsales\/ComputerSales.csv', usecols=[\"State\"], squeeze=True)\ncs_df_st","f69687e0":"# Display 1st 5 rows\ncs_df.head()\n# Display last 5 rows\ncs_df.tail()\n# Get 1st 2\ncs_df[:2]\n# Get 1st through 5 with a 2 step\ncs_df[:5:2]\n\n# Get indexes\ncs_df.index.array\n# Get NumPy array\ncs_df.to_numpy()\n# Get array from series\nser_1.array\n\ndict_3 = {'one': pd.Series([1., 2., 3.], index=['a', 'b', 'c']),\n         'two': pd.Series([1., 2., 3., 4.], index=['a', 'b', 'c', 'd'])}\ndf_2 = pd.DataFrame(dict_3)\n\n# You can replace NaN values with 0 or anything else\nprint(df_2.fillna(0))\n# Get values in row 2\nrow = df_2.iloc[1]\n# Add items in row 2 to all rows including row 2\n# You can do the same with sub, mul, and div\ndf_2.add(row, axis='columns')\n\n# Get column 2\ncol = df_2['two']\n# Subtract from other columns\ndf_2.sub(col, axis=0)\n\n# Check if empty\ndf_2.empty\n\n# Transform executes a function on a dataframe\ndf_5 = pd.DataFrame({'A': range(3), 'B': range(1, 4)})\ndf_5.transform(lambda x: x+1)\ndf_5.transform(lambda x: x**2)\ndf_5.transform(lambda x: np.sqrt(x))\n# You can transform using multiple functions\ndf_5.transform([lambda x: x**2, lambda x: x**3])\n# Passing a dictionary allows you to perform different calculations\n# on different columns\ndf_5.transform({'A': lambda x: x**2, 'B': lambda x: x**3})\n\n# map performs a function on a series\ndf_5['A'].map(lambda x: x**2)\n\n# applymap does the same on a dataframe\ndf_5.applymap(lambda x: x**2)\n\n# Get unique values in column 2 of DF\ndf_2['two'].unique()\n\n# Get number of uniques\ndf_2['two'].nunique()\n\n# Get the number of times each value showed in column 2\ndf_2['two'].value_counts()\n\n# Get column names\ndf_2.columns\n\n# Get index info\ndf_2.index\n\n# Return a DF that lists null values as True\ndf_2.isnull()","dcfce274":"# Groupby allows you to group rows based on a columnand perform a function\n# that combines those values (Aggregate Function)\ndict_5 = {'Store': [1,2,1,2], 'Flavor': ['Choc', 'Van', 'Straw', 'Choc'], \n         'Sales': [26, 12, 18, 22]}\n\ndf_11 = pd.DataFrame(dict_5)\n\n# Group data by the store number\nby_store = df_11.groupby('Store')\n# Get mean sales by store\nby_store.mean()\n\n# Get sales total just for store 1\nby_store.sum().loc[1]\n\n# You can use multiple functions of get a bunch\nby_store.describe()","4ea14ae9":"# You can concatenate DFs in the order DFs are provided\ndf_12 = pd.DataFrame({'A': [1,2,3],\n                     'B': [4,5,6]},\n                    index=[1,2,3])\ndf_13 = pd.DataFrame({'A': [7,8,9],\n                     'B': [10,11,12]},\n                    index=[4,5,6])\npd.concat([df_12, df_13])\n\n# Merge 2 DFs using their shared key column\ndf_12 = pd.DataFrame({'A': [1,2,3],\n                     'B': [4,5,6],\n                     'key': [1,2,3]})\ndf_13 = pd.DataFrame({'A': [7,8,9],\n                     'B': [10,11,12],\n                     'key': [1,2,3]})\n# inner merges at the intersection of keys\npd.merge(df_12, df_13, how='inner', on='key')\n# how='left' or 'right' : Use keys from left or right frame\n# how='outer' : Use union of keys\n\n# You can join DFs with different indexes and instead of using \n# keys use a column\ndf_12 = pd.DataFrame({'A': [1,2,3],\n                     'B': [4,5,6]},\n                    index=[1,2,3])\ndf_13 = pd.DataFrame({'C': [7,8,9],\n                     'D': [10,11,12]},\n                    index=[1,4,5])\ndf_12.join(df_13, how='outer')","1e93864b":"# Get ice cream sales data\nics_df = pd.read_csv('..\/input\/icecreamsales\/icecreamsales.csv')\nics_df\n\n# Get total count of both columns\nics_df.count()\n\n# skipna skips null \/ NaN values\nics_df.sum(skipna=True)\n# Get mean for named column\nics_df[\"Sales\"].mean()\nics_df[\"Sales\"].median()\nics_df[\"Sales\"].mode()\nics_df[\"Sales\"].min()\nics_df[\"Sales\"].max()\nics_df[\"Sales\"].prod() # Product of values\nics_df[\"Sales\"].std() # Standard deviation\nics_df[\"Sales\"].var() # Variance\nics_df[\"Sales\"].sem() # Standard error\n# Negative : Left long tail, Positive : Right long tail\nics_df[\"Sales\"].skew()\n# Kurtosis : < 3 less outliers, 3 Normal Distribution,\n# > 3 more outliers\nics_df[\"Sales\"].kurt()\nics_df[\"Sales\"].quantile(.5)\nics_df[\"Sales\"].cumsum()\nics_df[\"Sales\"].cumprod()\nics_df[\"Sales\"].cummax()\nics_df[\"Sales\"].cummin()\n\n# Multiple stats at once\nics_df.describe()\n\nser_dice = pd.Series(data=[2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, \n                           6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8,\n                          8, 8, 9, 9, 9, 9, 10, 10, 10, 11, 11, 12])\n# Count for each value in series\nser_dice.value_counts()\n\n# You can perform calculations on multiple columns using\n# aggregate\nprint(df_2)\ndf_2.agg(np.mean)\n\n# You can do this with multiple functions\ndf_2.agg(['mean', 'std'])","d6ab1397":"\n# Iterating over series\nser_7 = pd.Series(range(5), index=['a', 'b', 'c', 'd', 'e'])\nfor col in ser_7:\n    print(col)\n    \nprint()\n# Iterating over DFs\narr_4 = np.random.randint(10, 50, size=(2, 3))\ndf_8 = pd.DataFrame(arr_4, ['B', 'C'], ['C', 'D', 'E'])\nprint(df_8)\n\n# items allows you to iterate through key value pairs to make\n# calculations 1 column at a time\nfor label, ser in df_8.items():\n    print(label)\n    print(ser)\n    \nprint()\n# You can also iterate through rows\nfor index, row in df_8.iterrows():\n    print(f\"{index}\\n{row}\")\nprint()\n\n# Get a tuple that contains row data\nfor row in df_8.itertuples():\n    print(row)","caf36e12":"df_8\n\n# Sorting by index will return the same results if indexes\n# are in order, to reverse indexes mark ascending as False\ndf_8.sort_index(ascending=False)\n\n# Sort by value for column D (Use the same function for series)\ndf_8.sort_values(by='D')","a609df4c":"import sys\n\n# You can pass DataFrames and Series into functions\ndef get_profit_total(df):\n    prof_ser = df['Profit']\n    print(f\"Total Profit : {prof_ser.sum()}\")\n\nget_profit_total(cs_df)\n\n# Receives a DataFrame, splits the contact into new columns\n# being first and last name\ndef split_name(df):\n    def get_names(full_name):\n        # Split contact at space\n        f_name, l_name = full_name.split()\n        # Create a series with first & last names in columns\n        # with those labels\n        return pd.Series(\n        (f_name, l_name),\n        index=['First Name', 'Last Name']\n        )\n    # apply() executes the function on all names in Contact column\n    names = df['Contact'].apply(get_names)\n    df[names.columns] = names\n    return df\n\n# Run function and display top 5 results\nsplit_name(cs_df).head()\n\n# Will assign people to different age groups based on age\ndef create_age_groups(df):\n    # Must have 1 more bins than labels\n    bins = [0, 30, 50, sys.maxsize]\n    # Group labels\n    labels = ['<30', '30-50', '>50']\n    \n    # cut puts values into certain groups based on intervals\n    # The group assigned to <30 has an age between 0 and 30\n    # between 30 & 50 is assigned 30-50 and so on\n    age_group = pd.cut(df['Age'], bins=bins, labels=labels)\n    # Create new column and return new dataframe info\n    df['Age Group'] = age_group\n    return df\n\ncreate_age_groups(cs_df)\n\n# You can use a pipe to pass a dataframe to multiple functions\ncs_df.pipe(split_name).pipe(create_age_groups).head()","92144297":"ser_6 = pd.Series(range(5), index=['a', 'b', 'c', 'd', 'e'])\nsl_1 = ser_6[:4]\nsl_2 = ser_6[1:]\nprint(sl_1)\nprint(sl_2)\n# Align both series by the union of their indexes\nsl_1.align(sl_2)\n# Align by calling series\nsl_1.align(sl_2, join='left')\n# Use passed series indexes\nsl_1.align(sl_2, join='right')\n# Get where indexes intersect\nsl_1.align(sl_2, join='inner')\n\n# You can use align with DFs as well\narr_3 = np.random.randint(10, 50, size=(2, 3))\ndf_6 = pd.DataFrame(arr_3, ['A', 'B'], ['C', 'D', 'E'])\narr_3 = np.random.randint(10, 50, size=(2, 3))\ndf_7 = pd.DataFrame(arr_3, ['B', 'C'], ['C', 'D', 'E'])\ndf_6\ndf_6.align(df_7)\n\n# reindex allows you to align data by index\nser_6.reindex(['c','b','a'])\n\n# Do the same with DFs\ndf_6.reindex(['B','A'])\n\n# Drop is very similar to reindex except it receives labels\n# you don't want to include\ndf_6.drop(['A'], axis=0)\ndf_6.drop(['D'], axis=1)\n\n# You can rename labels\ndf_6.rename(columns={'C': 'Men', 'D': 'Women', 'E': 'Pets'},\n           index={'A': 1, 'B': 2})","45b43d5a":"# Multi-level indexing allows you to store data on multiple\n# dimensions\ndays = ['Day 1', 'Day 1', 'Day 1', 'Day 2', 'Day 2', 'Day 2']\nmeals = [1,2,3,1,2,3]\n# zip pairs the days and meals arrays \n# Then we create a list of those paired tuples\nhier_index = list(zip(days, meals))\nprint(hier_index)\n# Converts list of tuples into each row and column\nhier_index = pd.MultiIndex.from_tuples(hier_index)\n# Generate random array representing calories eaten per meal\narr_5 = np.random.randint(500, 700, size=(6, 2))\ndf_9 = pd.DataFrame(arr_5, hier_index, ['M', 'F'])\nprint(df_9)\n\n# Grab the day 1 DF\ndf_9.loc['Day 1']\n\n# Grab 1st row as a series\ndf_9.loc['Day 1'].loc[1]\n\n# Grab calories eaten by the female on day 2 for the 2nd meal\ndf_9.loc['Day 2'].loc[2]['F']\n\n# We can assign names to the Day and Meals Column\ndf_9.index.names = ['Day', 'Meal']\ndf_9\n\n# Get a cross section\n# This gets me the Day 2 DF\ndf_9.xs('Day 2')\n\n# Get calories for the 1st meal for both days by saying what\n# meal index you want and the Meal column name\ndf_9.xs(1, level='Meal')\n\n# Create a MultiIndex out of a DF using a pivot table\ndict_6 = {'A':['Day 1', 'Day 1', 'Day 1', 'Day 2', 'Day 2', 'Day 2'],\n         'B': [1,2,3,1,2,3],\n         'C': ['M', 'F', 'M', 'F', 'M', 'F'],\n         'D': [1,2,3,4,5,6]}\ndf_14 = pd.DataFrame(dict_6)\n# Designate the D column is the data\n# Make A & B a multilevel index\n# Define column names come from column C\n# You will have NaNs where data was missing\ndf_14.pivot_table(values='D', index=['A','B'], columns=['C'])","5e285b4d":"\ndict_4 = {'A': [1,2,np.nan], 'B': [4, np.nan, np.nan], 'C': [7.,8.,9.]}\ndf_10 = pd.DataFrame(dict_4)\nprint(df_10)\n\n# Drop missing data from DF (Drops any row with missing values)\ndf_10.dropna()\n\n# Drop all columns with any missing data\ndf_10.dropna(axis=1)\n\n# Drop row unless it has at least 2 non-NaN values\ndf_10.dropna(thresh=2)\n\n# Fill NaN values with 0\ndf_10.fillna(value=0.0)\n\n# Fill A column with the mean of column\ndf_10['A'].fillna(value=df_10['A'].mean())\n\n# Fill with previous value\ndf_10.fillna(method='ffill')\n\n# Fill with next value (Only works if there is a next value)\ndf_10.fillna(method='bfill')","c034196b":"cs_df.head() # Get 1st 5\nprint(cs_df.columns) # Get column names\ncs_df['Profit'].mean() # Average profit per item\n# Get the product with the highest profit\ncs_df[['Product ID', 'Profit']].max(axis=0).head()\n# Number of people who purchased from WV\ncs_df[cs_df['State']=='WV']['State'].count()\n# Number of purchases in 2019\nlen(cs_df[cs_df['Year']==2019].index)\n# Get number of sales for each product type\ncs_df['Product ID'].value_counts()\n# Get list of customers that bought a specific product\ncs_df[cs_df['Product ID']=='M01-F0024']['Contact']\n# How many made a website purchase for a profit over $200\ncs_df[(cs_df['Lead']=='Website') & (cs_df['Profit']>150)]['Lead'].count()\n# Find out how many product profit amounts include .89 in cents\ncs_df['Profit'].apply(lambda cents: str(cents).split('.')[1]=='89').value_counts()","3ea9b035":"# Library usef to create advanced static, animated and\n# interactive visualizations\nimport matplotlib.pyplot as plt\n\n# Displays matplotlib plots in the Notebook\n%matplotlib inline\n\n# Histograms provide an approximation of the distribution of\n# results. You create them by dividing the range of values into \n# bins or buckets. Then you count how many of the results fall\n# into each bin.\n# Rolls 2 dice 5000 times and charts the frequency and \n# a histogram\n\n# Even though the odds increase as you approach 7 and then\n# decrease again (1 way to roll a 2 \/ 6 ways to roll a 7)\n# over many rolls they are nearly equal.\ndf_dice = pd.DataFrame(\n    np.random.randint(1,7,5000),\n    columns = ['Hist'])\ndf_dice['Odds'] = df_dice['Hist'] + np.random.randint(1,7,5000)\n# Alpha decreases the opacity in the chart\nax = df_dice.plot.hist(bins=12, alpha=0.5)\n\n# Basic plot using 1000 random values that create cumulative sums\n# over an increasing date range\nser_5 = pd.Series(np.random.randn(1000),\n                 index=pd.date_range('11\/15\/2017', periods=1000))\nser_5 = ser_5.cumsum()\n# ser_5.plot()\n\n# Display 3 random plots\ndf_15 = pd.DataFrame(np.random.randn(1000, 3),\n                    index=pd.date_range('11\/15\/2017', periods=1000),\n                    columns=list('ABC'))\ndf_15 = df_15.cumsum()\n# df_15.plot()\n\n# Make bar chart from 5 random values\n# pd.DataFrame(np.random.randn(5)).plot.bar()\n\n# Make MultiBar Charts\nvals = ['A', 'B', 'C', 'D']\ndf_15 = pd.DataFrame(np.random.rand(10,4), columns=vals)\n# df_15.plot.bar()\n\n# Area plot \n# Define x range and y values\nx_rng = range(1,15)\ny_vals = [1,5,4,7,6,9,5,7,10,14,10,12,9,8]\n# Change fill color and opacity\n# plt.fill_between(x_rng, y_vals, color=\"skyblue\", alpha=0.5)\n# plt.show()\n\n# Area plot with multiple areas\n# pd.DataFrame(np.random.rand(10,3), columns=['A','B','C']).plot.area()\n\n# Create a scatterplot with 100 random values\n# pd.DataFrame(np.random.rand(100,2), \n#              columns=['A','B']).plot.scatter(x='A', y='B')\n\n# Multiple column scatter plots\ndf_15 = pd.DataFrame(np.random.rand(50,4), columns=['A','B','C','D'])\n# ax = df_15.plot.scatter(x='A', y='B', color='DarkBlue', label='Grp 1')\n# df_15.plot.scatter(x='C', y='D', color='Orange', label='Grp 2', ax=ax)\n\n# Pie Charts with 4 random values\n# pd.Series(np.random.rand(4),\n#          index=['a','b','c','d'], \n#           name='Pie').plot.pie(figsize=(6,6))","a39c12e6":"# <a id='FIO'>File Input \/ Output<\/a>\n[Go back to the main page](#main)\n\nPandas can work with the following types of data : Pickling, Flat file, Clipboard, Excel, JSON, HTML, HDFStore: PyTables (HDF5), Feather, Parquet, ORC, SAS, SPSS, SQL, Google BigQuery, STATA\n\n### [Here Check it out!](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/io.html#feather)","8a6656bf":"# <a id='B&M'>Basics & Math<\/a>\n[Go back to the main page](#main)","8ef29be8":"# <a id='S'>Series<\/a>\n[Go back to the main page](#main)\n\nOne-dimensional ndarray with axis labels (including time series).\n\nLabels need not be unique but must be a hashable type. The object supports both integer- and label-based indexing and provides a host of methods for performing operations involving the index. Statistical methods from ndarray have been overridden to automatically exclude missing data (currently represented as NaN).\n\nOperations between Series (+, -, \/, , *) align values based on their associated index values\u2013 they need not be the same length. The result index will be the sorted union of the two indexes.","9fa7543b":"# <a id='SO'>Sorting<\/a>\n[Go back to the main page](#main)","7a7759b7":"# <a id='EWD'>Experimenting with Data<\/a>\n[Go back to the main page](#main)","c057e89f":"# <a id='DF'>Dataframe<\/a>\n[Go back to the main page](#main)\n\nTwo-dimensional, size-mutable, potentially heterogeneous tabular data.\n\nData structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure.","34e667ee":"# About pandas\n## History of development\nIn 2008, pandas development began at [AQR Capital Management](https:\/\/www.aqr.com\/). By the end of 2009 it had been open sourced, and is actively supported today by a community of like-minded individuals around the world who contribute their valuable time and energy to help make open source pandas possible.\n\nSince 2015, pandas is a [NumFOCUS sponsored project](https:\/\/numfocus.org\/sponsored-projects). This will help ensure the success of development of pandas as a world-class open-source project.\n\n## Library Highlights\n- A fast and efficient **DataFrame** object for data manipulation with integrated indexing;\n- Tools for r**eading and writing data** between in-memory data structures and different formats: CSV and text files, Microsoft Excel, SQL databases, and the fast HDF5 format;\n- Intelligent **data alignment** and integrated handling of **missing data**: gain automatic label-based alignment in computations and easily manipulate messy data into an orderly form;\n- Flexible **reshaping** and pivoting of data sets;\n- Intelligent label-based **slicing**, **fancy indexing**, and **subsetting** of large data sets;\n- Columns can be inserted and deleted from data structures for **size mutability**;\n- Aggregating or transforming data with a powerful **group by** engine allowing split-apply-combine operations on data sets;\n- High performance **merging and joining** of data sets;\n- **Hierarchical axis indexing** provides an intuitive way of working with high-dimensional data in a lower-dimensional data structure;\n- **Time series**-functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging. Even create domain-specific time offsets and join time series without losing data;\n- Highly **optimized for performance**, with critical code paths written in Cython or C.\n- Python with pandas is in use in a wide variety of **academic and commercial** domains, including Finance, Neuroscience, Economics, Statistics, Advertising, Web Analytics, and more.","eba86063":"# <a id='I'>Iteration<\/a>\n[Go back to the main page](#main)","7f5b0a36":"# <a id='M'>MultiIndex<\/a>\n[Go back to the main page](#main)","41bc090b":"# <a id='CDF'>Creating Dataframe<\/a>\n[Go back to the main page](#main)","e34a3d2e":"# <a id='PDTF'>Passing Data to Functions<\/a>\n[Go back to the main page](#main)","2f01ccb5":"# <a id='GD'>Group Data<\/a>\n[Go back to the main page](#main)","f253cc92":"# <a id='ARARL'>Aligning, Reindexing and Renaming Labels<\/a>\n[Go back to the main page](#main)","36adbd44":"# <a id='CS'>Conditional Selection<\/a>\n[Go back to the main page](#main)","80d240eb":"# <a id='CM&JD'>Concatenate Merge & Join Data<\/a>\n[Go back to the main page](#main)","13c491ec":"In this notebook we are going to discuss a vast majority of the pandas Library using numerous Examples.","8e4b7f8e":"# <a id='E&RD'>Editing & Retrieving Data<\/a>\n[Go back to the main page](#main)","a07eeb4e":"# <a id='main'>Table of Contents<\/a>\n- [Series](#S)\n- [DataFrames](#DF)\n- [Creating DataFrames](#CDF)\n- [Editing & Retrieving Data](#E&RD)\n- [Conditional Selection](#CS)\n- [File Input \/ Output](#FIO)\n- [Basics & Math](#B&M)\n- [Group Data](#GD)\n- [Concatenate Merge & Join Data](#CM&JD)\n- [Statistics](#ST)\n- [Iteration](#I)\n- [Sorting](#SO)\n- [Passing Data to Functions](#PDTF)\n- [Aligning, Reindexing and Renaming Labels](#ARARL)\n- [MultiIndex](#M)\n- [Handling Missing Data](#HMD)\n- [Experimenting with Data](#EWD)\n- [Visualization](#V)","a9e420b8":"# <a id='ST'>Statistics<\/a>\n[Go back to the main page](#main)","01ef3c5e":"# <a id='V'>Visualization<\/a>\n[Go back to the main page](#main)","78be12bb":"# <a id='HMD'>Handling Missing Data<\/a>\n[Go back to the main page](#main)","0eb2657b":"<div>\n<img src=\"https:\/\/pandas.pydata.org\/static\/img\/pandas.svg\" width=\"500\"\/>\n<\/div>"}}