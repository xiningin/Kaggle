{"cell_type":{"b34681b2":"code","fa5da245":"code","dcb1da0e":"code","3e4e58c7":"code","3bcbf1a7":"code","441650a7":"code","1d12009e":"code","d29ef69d":"code","7ef8c081":"code","2cb2dd4e":"code","11481d28":"code","321febc3":"code","ed9f5109":"code","ce27a393":"code","f6c026ce":"code","bce6ca7a":"code","a769a385":"code","723cf33f":"code","bd9019eb":"code","b6a6b0c6":"code","daa2bcee":"code","2f5c46e2":"code","7d0a987b":"code","475a9b97":"code","888306b7":"markdown","f93dbb3c":"markdown","4b2c6c3a":"markdown","20d48fb8":"markdown","4f391d82":"markdown","8f961829":"markdown"},"source":{"b34681b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/initial-data-exploration-in-python\/\"))\nprint(os.listdir(\"..\/input\/house-prices-advanced-regression-techniques\/\"))\n\n# Any results you write to the current directory are saved as output.","fa5da245":"train = pd.read_csv('..\/input\/initial-data-exploration-in-python\/transformed_pca.csv')\ntrain.head()","dcb1da0e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\nX = train.iloc[:,:-1]\ny = train.SalePrice\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","3e4e58c7":"lm = LinearRegression()\nlm.fit(X_train,y_train)\npredictions = lm.predict(X_test)\nerror_df = pd.DataFrame({'actual':list(y_test),\n                        'predicted':predictions})\nerror_df['error'] = error_df.actual - error_df.predicted\nerror_df.head()","3bcbf1a7":"from sklearn.metrics import r2_score,mean_squared_error\n\nr_2 = r2_score(error_df.actual,error_df.predicted)\nprint('R-squared (coefficient of determination): '+str(r_2))\nprint('Root mean-squared error: ' + str(mean_squared_error(error_df.actual,error_df.predicted) ** (1\/2)))\nadj_r2= 1 - (1-r_2)*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1)\nprint('Adjusted R-squared: '+str(adj_r2))","441650a7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set(style=\"whitegrid\")\n\n# Plot the residuals after fitting a linear model\nfig,ax = plt.subplots(figsize=(10,7),sharex=False)\nsns.distplot(error_df.error, ax=ax)\nax.set_title('Error Distribution',fontdict=dict(size=18,weight='bold'))\nax.set_xlabel('')\nplt.show();","1d12009e":"# Use JointGrid directly to draw a custom plot\nfig,ax = plt.subplots(figsize=(10,7))\np1 = sns.relplot(x=\"actual\", y=\"error\",data=error_df,ax=ax)\nax.set_title('Actual vs. Error',fontdict=dict(size=18,weight='bold'))\nplt.close(p1.fig)\nplt.show();","d29ef69d":"# optimization code from https:\/\/www.kaggle.com\/dfitzgerald3\/optimizing-ridge-regression-parameterization\n\nfrom sklearn.linear_model import Ridge,RidgeCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nscorer = make_scorer(mean_squared_error, False)\n\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\nsolvers = ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']\n\ncv_score = []\nfor i in solvers:\n    for ii in alphas:\n        clf = Ridge(alpha = ii, solver = i)\n        cv_score.append([i,ii,np.sqrt(-cross_val_score(estimator=clf, \n                                            X=X_train, \n                                            y=y_train, \n                                            cv=15, \n                                            scoring = \"neg_mean_squared_error\")).mean()])\n","7ef8c081":"ridge_opt_df = pd.DataFrame(cv_score,columns=['solver','alpha','cv_score'])\nridge_opt_df.loc[ridge_opt_df.cv_score == ridge_opt_df.cv_score.min()]","2cb2dd4e":"r_reg = RidgeCV(alphas=[5],gcv_mode=['svd'],cv=15)\nr_reg.fit(X_train,y_train)\npredictions = r_reg.predict(X_test)\nerror_df = pd.DataFrame({'actual':list(y_test),\n                        'predicted':predictions})\nerror_df['error'] = error_df.actual - error_df.predicted\nerror_df.head()","11481d28":"r_2 = r2_score(error_df.actual,error_df.predicted)\nprint('R-squared (coefficient of determination): '+str(r_2))\nprint('Root mean-squared error: ' + str(mean_squared_error(error_df.actual,error_df.predicted) ** (1\/2)))\nadj_r2= 1 - (1-r_2)*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1)\nprint('Adjusted R-squared: '+str(adj_r2))","321febc3":"fig,ax = plt.subplots(figsize=(10,7),sharex=False)\nsns.distplot(error_df.error, ax=ax)\nax.set_title('Error Distribution',fontdict=dict(size=18,weight='bold'))\nax.set_xlabel('')\nplt.show();","ed9f5109":"test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain = train[[c for c in train.columns if c != 'SalePrice']]\ntest = pd.concat([train,test])","ce27a393":"test_cleaned = pd.get_dummies(test[[c for c in test if c != 'MiscVal']],dummy_na=False,drop_first=True)\ntest_cleaned_cols = test_cleaned.columns","f6c026ce":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nnew_data = pd.DataFrame(my_imputer.fit_transform(test_cleaned))\nnew_data.columns = test_cleaned.columns","bce6ca7a":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nscaler = MinMaxScaler(feature_range=[0, 1])\nnew_data = scaler.fit_transform(new_data[[c for c in new_data.columns if c != 'SalePrice']])","a769a385":"#Fitting the PCA algorithm with our Data\nnew_data = pd.DataFrame(new_data,columns=[c for c in test_cleaned_cols if c != 'SalePrice'])\npca = PCA().fit(new_data)\nexplained_var = pd.DataFrame({'n_components':list(np.arange(new_data.shape[1])),\n                              'cumsum':list(np.cumsum(pca.explained_variance_ratio_))})\nexplained_var.loc[explained_var['cumsum'] > .99].iloc[:5,:]","723cf33f":"pca = PCA(n_components=158)\ntransformed_features = pca.fit_transform(new_data)\ntransformed_data = pd.DataFrame(transformed_features)","bd9019eb":"transformed_data.shape","b6a6b0c6":"r_reg = RidgeCV(alphas=[5],gcv_mode=['svd'],cv=15)\nr_reg.fit(transformed_data.iloc[:1460,:] ,y)\npredictions = r_reg.predict(transformed_data.iloc[:1460,:])\nerror_df = pd.DataFrame({'actual':list(y),\n                        'predicted':predictions})\nerror_df['error'] = error_df.actual - error_df.predicted\nerror_df.head()","daa2bcee":"r_2 = r2_score(error_df.actual,error_df.predicted)\nprint('R-squared (coefficient of determination): '+str(r_2))\nprint('Root mean-squared error: ' + str(mean_squared_error(error_df.actual,error_df.predicted) ** (1\/2)))\nadj_r2= 1 - (1-r_2)*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1)\nprint('Adjusted R-squared: '+str(adj_r2))","2f5c46e2":"fig,ax = plt.subplots(figsize=(10,7),sharex=False)\nsns.distplot(error_df.error, ax=ax)\nax.set_title('Error Distribution',fontdict=dict(size=18,weight='bold'))\nax.set_xlabel('')\nplt.show();","7d0a987b":"predictions = r_reg.predict(transformed_data.iloc[1460:,:])\npredictions = [np.exp(pred) for pred in predictions]\nlen(predictions)","475a9b97":"pred_df = pd.DataFrame({'Id':np.arange(1461,transformed_data.shape[0]+1),\n                       'SalePrice':predictions})\npred_df.to_csv('predictions.csv',index=False)","888306b7":"In this exercise, we will experiment with different regression modeling approaches. We will start with a simple linear regression model and increase our sophistication in each iteration as the need arises.","f93dbb3c":"# Building the Model","4b2c6c3a":"## Simple Linear Regression ","20d48fb8":"## Fit on Entire Dataset","4f391d82":"## Ridge Regression\n\nWe will first identify the optimal _alpha_ value and the optimal solver. In a ridge regression model, _alpha_ refers to ","8f961829":"It appears that the model does not predict the upper outliers well, but we want this model to be sensitive to outliers. This makes a good case to try ridge regression, since ridge accounts for error and is also sensitive to outliers. Ridge regression utilizes the L2 (MSE) penalty. For all the following models, we will use cross-validation."}}