{"cell_type":{"5bc623a2":"code","b183c1c5":"code","1e8852f5":"code","96edeba7":"code","5a13574f":"code","6a549a52":"code","8715a1e3":"code","07a6c862":"code","0b9d791c":"code","d26499fb":"code","b33168f2":"code","7de3766d":"markdown","54385aa9":"markdown","2094eed5":"markdown","d4d710ad":"markdown","33ccc13b":"markdown","daf6574d":"markdown","9952d2d8":"markdown","866ea919":"markdown"},"source":{"5bc623a2":"import os\nimport numpy as np\nimport sklearn\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","b183c1c5":"PROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"decision_trees\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)","1e8852f5":"from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\niris = load_iris()\nX = iris.data[:, 2:] # petal length and width\ny = iris.target\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\ntree_clf.fit(X, y)","96edeba7":"tree_clf.predict_proba([[5, 1.5]]) # petal-length, petal-width","5a13574f":"tree_clf.predict([[5, 1.5]]) # Iris Virginica","6a549a52":"from graphviz import Source\nfrom sklearn.tree import export_graphviz","8715a1e3":"# export_graphviz(\n#         tree_clf,\n#         out_file=os.path.join(IMAGES_PATH, \"iris_tree.dot\"),\n#         feature_names=iris.feature_names[2:],\n#         class_names=iris.target_names,\n#         rounded=True,\n#         filled=True\n#     )\n\n# Source.from_file(os.path.join(IMAGES_PATH, \"iris_tree.dot\"))","07a6c862":"# Quadratic training set + noise\nnp.random.seed(42)\nm = 200\nX = np.random.rand(m, 1)\ny = 4 * (X - 0.5) ** 2\ny = y + np.random.randn(m, 1) \/ 10","0b9d791c":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree_reg.fit(X, y)","d26499fb":"## Regularized tree\ntree_reg1 = DecisionTreeRegressor(random_state=42)\ntree_reg2 = DecisionTreeRegressor(random_state=42, min_samples_leaf=10)\ntree_reg1.fit(X, y)\ntree_reg2.fit(X, y)\n\nx1 = np.linspace(0, 1, 500).reshape(-1, 1)\ny_pred1 = tree_reg1.predict(x1)\ny_pred2 = tree_reg2.predict(x1)","b33168f2":"## Plotting\n\nfig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(X, y, \"b.\")\nplt.plot(x1, y_pred1, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\nplt.axis([0, 1, -0.2, 1.1])\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", fontsize=18, rotation=0)\nplt.legend(loc=\"upper center\", fontsize=18)\nplt.title(\"No restrictions\", fontsize=14)\n\nplt.sca(axes[1])\nplt.plot(X, y, \"b.\")\nplt.plot(x1, y_pred2, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\nplt.axis([0, 1, -0.2, 1.1])\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.title(\"min_samples_leaf={}\".format(tree_reg2.min_samples_leaf), fontsize=14)\n\nplt.show()","7de3766d":"#### Limitations Of Decision Trees\nInstability - They are very sensitive to small variations in the training data.\n- If we change the orientation of the dataset, the model will not be able to fit the dataset. It will give a complex fit that will lead to overfitting. ","54385aa9":"#### The CART Training Algorithm\n- the algorithm first splits the training set in two subsets using a single feature k and a threshold tk (e.g., \u201cpetal length \u2264 2.45 cm\u201d).\n- It searches for the pair (k, tk) that produces the purest subsets (weighted by their size).","2094eed5":"#### Decision tree intuition\n- **Gini Impurity** -: Gi = 1 \u2212 \u2211 (pi, k)^2\n- **Entropy** -: Hi = \u2211 P log2(P)\n- **Information Gain** - Information gain is the reduction in entropy or surprise by transforming a dataset and is often used in training decision trees. \n- Information gain is calculated by comparing the entropy of the dataset before and after a transformation.\n- It also decides the best root node to split on.\n--------------------\n#### Gini Impurity or Entropy?\n- **Entropy** would be **1** when the probability is **0.5** and it will again reduce when the probability increases.\n- **Gini Impurity** would be **0.5** when the probability is **0.5**. \n- **Gini Impurity is prefered mostly as it is computationally fast**.","d4d710ad":"#### Estimating Class Probabilities\n- Estimate the probability that an instance belongs to a particular class k.\n- First it traverses the tree to find the leaf node for this instance, and then it returns the ratio of training instances of class k in this node.","33ccc13b":"#### Regularization Hyperparameters\n- By regularizing one can prevent the tree from **overfitting**.","daf6574d":"### Regression\n- Decision Trees are prone to overfitting when dealing with regression tasks.","9952d2d8":"### Decision Trees\n- Like SVMs, Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks, and even multioutput tasks.","866ea919":"#### Making Predictions\n- **Samples** attribute counts how many training instances it applies to. For example, 100 training instances have a petal length greater than 2.45 cm (depth 1,right)\n- **Value** attribute tells you how many training instances of each class this node applies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 IrisVersicolor, and 45 Iris-Virginica.\n- **Gini** attribute measures its impurity: a node is \u201cpure\u201d (gini=0) if all training instances it applies to belong to the same class. For example, since the depth-1 left node applies only to Iris-Setosa training instances, it is pure and its gini score is 0."}}