{"cell_type":{"33ae90e8":"code","d53f1fab":"code","374a05ad":"code","fa9402be":"code","b851a0f8":"code","44856860":"code","6854d2e9":"code","f53fddf0":"code","c056b3e6":"code","fdf83a3c":"code","b404e1ff":"code","859107d0":"code","0c886ffe":"code","4d3e3d1a":"code","d49c1b16":"code","7793c02e":"code","877f5a3d":"code","7c4cffaf":"code","22856ae9":"code","2064ceff":"code","1cde31fa":"code","adc00b6d":"code","a1fb059b":"code","2dfbf6df":"code","6926670a":"code","f5b6dff4":"code","31b0dc0e":"code","750ba06a":"code","7aee2f0d":"code","b3762a8d":"code","fb9fdda4":"code","fc7154fe":"code","33381b8f":"code","6a92125b":"code","402815e4":"code","f0ac1bca":"code","255eaab5":"code","c6d77829":"code","73cdfba7":"code","ccf6c1e0":"code","9c883936":"code","3d5c698b":"code","42197a6a":"code","417e44c7":"code","73fdecf4":"code","9fd06131":"code","72371b21":"code","46ee8917":"code","7cb75081":"code","a492e97f":"code","5741558e":"code","da230c09":"code","089cd320":"code","9b33c5ab":"code","233f012f":"code","15d8949e":"code","02494498":"code","6dc6b2f8":"code","d0d9adff":"code","370ce52e":"code","e3b21252":"code","70a08ab6":"code","393de8ca":"code","3960395a":"code","d4fa9042":"code","a20ee164":"code","0b9f2d3a":"code","602defda":"code","dafdab79":"code","bc4253d4":"code","3a9416c0":"code","2b5bd043":"code","c7c0d055":"code","c3a30f90":"code","8a3a983b":"markdown","11f2cc3e":"markdown","7d093c70":"markdown","9207eea9":"markdown","7b1b9c9f":"markdown","21b268c3":"markdown","fd677a6e":"markdown","01bc79d7":"markdown","751fb79f":"markdown","5bd975a4":"markdown","c0c22041":"markdown","b084a641":"markdown","bf7a9e4f":"markdown","099f6248":"markdown","14e00f7f":"markdown","da5219c8":"markdown","4b335356":"markdown","ea32cc60":"markdown","951e541f":"markdown"},"source":{"33ae90e8":"### Imports","d53f1fab":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","374a05ad":"#df =  pd.read_csv('healthcare-dataset-stroke-data.csv')\ndf =  pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')","fa9402be":"df.head()","b851a0f8":"df.columns","44856860":"df.shape","6854d2e9":"df.info()","f53fddf0":"df.isna().sum().plot(kind = 'bar')","c056b3e6":"# Replace null values with mean\ndf.bmi.replace(to_replace=np.nan, value=df.bmi.mean(), inplace=True)","fdf83a3c":"df.describe().T","b404e1ff":"categorical_columns = df.dtypes[df.dtypes == 'object'].index\nnumerical_columns = df.dtypes[df.dtypes != 'object'].index","859107d0":"categorical_columns","0c886ffe":"df['gender'].value_counts()","4d3e3d1a":"df['work_type'].value_counts().plot(kind = 'bar')","d49c1b16":"df['ever_married'].value_counts()","7793c02e":"df['Residence_type'].value_counts()","877f5a3d":"df['smoking_status'].value_counts().plot(kind = 'bar')","7c4cffaf":"df['hypertension'].value_counts()","22856ae9":"df['heart_disease'].value_counts()","2064ceff":"numerical_columns","1cde31fa":"#df['age'].hist()\nsns.distplot(df['age'])","adc00b6d":"#df['bmi'].hist()\nsns.distplot(df['bmi'])","a1fb059b":"sns.distplot(df['avg_glucose_level'])","2dfbf6df":"sns.scatterplot(x = 'avg_glucose_level',y = 'bmi',hue = 'stroke', size = 'heart_disease', data =df)","6926670a":"sns.lineplot(x = 'age',y = 'avg_glucose_level',hue = 'stroke',size='gender', data =df)","f5b6dff4":"sns.catplot(x='heart_disease',y='age', hue=\"work_type\", kind=\"bar\", data=df)","31b0dc0e":"sns.catplot(x='stroke', y=\"age\", hue = 'work_type', kind=\"box\", data=df)","750ba06a":"# Heat Map Correlation \n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","7aee2f0d":"plt.figure(figsize=(12,10))\n\nsns.distplot(df[df['stroke'] == 0][\"bmi\"], color='green') # No Stroke - green\nsns.distplot(df[df['stroke'] == 1][\"bmi\"], color='red') # Stroke - Red\n\nplt.title('No Stroke vs Stroke by BMI', fontsize=15)\nplt.xlim([10,100])\nplt.show()","b3762a8d":"plt.figure(figsize=(13,13))\nsns.set_theme(style=\"darkgrid\")\nplt.subplot(2,3,1)\nsns.violinplot(x = 'gender', y = 'stroke', data = df)\nplt.subplot(2,3,2)\nsns.violinplot(x = 'hypertension', y = 'stroke', data = df)\nplt.subplot(2,3,3)\nsns.violinplot(x = 'heart_disease', y = 'stroke', data = df)\nplt.subplot(2,3,4)\nsns.violinplot(x = 'ever_married', y = 'stroke', data = df)\nplt.subplot(2,3,5)\nsns.violinplot(x = 'work_type', y = 'stroke', data = df)\nplt.xticks(fontsize=9, rotation=45)\nplt.subplot(2,3,6)\nsns.violinplot(x = 'Residence_type', y = 'stroke', data = df)\nplt.show()","fb9fdda4":"df['stroke'].value_counts()","fc7154fe":"X = df.iloc[:, 1:-1].values\ny = df.iloc[:, -1].values","33381b8f":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder","6a92125b":"ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,5,9])], remainder= 'passthrough')\nX = np.array(ct.fit_transform(X))","402815e4":"from sklearn.preprocessing import LabelEncoder","f0ac1bca":"le = LabelEncoder()\nX[:, 15] = le.fit_transform(X[:, 15])\nX[:, 16] = le.fit_transform(X[:, 16])","255eaab5":"from imblearn.over_sampling import SMOTE","c6d77829":"smote = SMOTE()\nx_smote, y_smote = smote.fit_resample(X, y)","73cdfba7":"from sklearn.model_selection import train_test_split","ccf6c1e0":"X_train,X_test, y_train,y_test=train_test_split(x_smote,y_smote,test_size=0.33,random_state=42)\nX_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","9c883936":"print(\"Number transactions x_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions x_valid dataset: \", X_valid.shape)\nprint(\"Number transactions y_valid dataset: \", y_valid.shape)\nprint(\"Number transactions x_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","3d5c698b":"from sklearn.preprocessing import StandardScaler","42197a6a":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_valid = sc.transform(X_valid)","417e44c7":"from sklearn.linear_model import LogisticRegression","73fdecf4":"log_reg = LogisticRegression()\nlog_reg.fit(X_train,y_train)","9fd06131":"y_pred_log_reg_valid = log_reg.predict(X_valid)\n# y_pred_log_reg","72371b21":"from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score","46ee8917":"classification_report = classification_report(y_valid, y_pred_log_reg_valid)\nprint(classification_report)","7cb75081":"auc = roc_auc_score(y_valid, y_pred_log_reg_valid)\nauc","a492e97f":"cm = confusion_matrix(y_valid, y_pred_log_reg_valid)\ncm","5741558e":"predicted_probab_log = log_reg.predict_proba(X_valid)\npredicted_probab_log = predicted_probab_log[:, 1]\nfpr, tpr, _ = roc_curve(y_valid, predicted_probab_log)","da230c09":"from matplotlib import pyplot\npyplot.plot(fpr, tpr, marker='.', label='Logistic Regression')\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\npyplot.show()","089cd320":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","9b33c5ab":"models = []\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['GaussianNB', GaussianNB()])\nmodels.append(['BernoulliNB', BernoulliNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier(eval_metric= 'error')])","233f012f":"lst_1= []\nfor m in range(len(models)):\n    lst_2= []\n    model = models[m][1]\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    cm = confusion_matrix(y_valid, y_pred)  #Confusion Matrix\n    accuracies = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 10)   #K-Fold Validation\n    roc = roc_auc_score(y_valid, y_pred)  #ROC AUC Score\n    precision = precision_score(y_valid, y_pred)  #Precision Score\n    recall = recall_score(y_valid, y_pred)  #Recall Score\n    f1 = f1_score(y_valid, y_pred)  #F1 Score\n\n    lst_2.append(models[m][0])\n    lst_2.append((accuracy_score(y_valid, y_pred))*100) \n    lst_2.append(accuracies.mean()*100)\n    lst_2.append(accuracies.std()*100)\n    lst_2.append(roc)\n    lst_2.append(precision)\n    lst_2.append(recall)\n    lst_2.append(f1)\n    lst_1.append(lst_2)","15d8949e":"df = pd.DataFrame(lst_1, columns= ['Model', 'Accuracy', 'K-Fold Mean Accuracy', 'Std. Deviation', 'ROC AUC', 'Precision', 'Recall', 'F1'])","02494498":"df.sort_values(by= ['Accuracy', 'K-Fold Mean Accuracy'], inplace= True, ascending= False)\ndf","6dc6b2f8":"from sklearn.model_selection import GridSearchCV","d0d9adff":"grid_models = [(RandomForestClassifier(),[{'n_estimators':[100,150,200],'criterion':['gini','entropy'],'random_state':[0]}]), \n              (XGBClassifier(), [{'learning_rate': [0.01, 0.05, 0.1], 'eval_metric': ['error']}])]","370ce52e":"for i,j in grid_models:\n    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv = 10)\n    grid.fit(X_train, y_train)\n    best_accuracy = grid.best_score_\n    best_param = grid.best_params_\n    print('{}:\\nBest Accuracy : {:.2f}%'.format(i,best_accuracy*100))\n    print('Best Parameters : ',best_param)\n    print('')\n    print('----------------')\n    print('')","e3b21252":"classifier = RandomForestClassifier(criterion= 'entropy', n_estimators= 150, random_state= 0)\nclassifier.fit(X_train, y_train)","70a08ab6":"y_pred = classifier.predict(X_test)\ny_prob = classifier.predict_proba(X_test)[:,1]","393de8ca":"print(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))","3960395a":"# print(classification_report(y_test, y_pred))","d4fa9042":"# Visualizing Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","a20ee164":"from sklearn import metrics","0b9f2d3a":"# Roc AUC Curve\nfalse_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_prob)\nroc_auc = metrics.auc(false_positive_rate, true_positive_rate)","602defda":"sns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend()\nplt.show()","dafdab79":"classifier = XGBClassifier(eval_metric= 'error', learning_rate= 0.1)\nclassifier.fit(X_train, y_train)","bc4253d4":"y_pred = classifier.predict(X_test)\ny_prob = classifier.predict_proba(X_test)[:,1]","3a9416c0":"# print(classification_report(y_test, y_pred))\nprint(f'ROC AUC score: {roc_auc_score(y_test, y_prob)}')\nprint('Accuracy Score: ',accuracy_score(y_test, y_pred))","2b5bd043":"# Visualizing Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (8, 5))\nsns.heatmap(cm, cmap = 'Blues', annot = True, fmt = 'd', linewidths = 5, cbar = False, annot_kws = {'fontsize': 15}, \n            yticklabels = ['No stroke', 'Stroke'], xticklabels = ['Predicted no stroke', 'Predicted stroke'])\nplt.yticks(rotation = 0)\nplt.show()","c7c0d055":"# Roc Curve\nfalse_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(y_test, y_prob)\nroc_auc = metrics.auc(false_positive_rate, true_positive_rate)","c3a30f90":"sns.set_theme(style = 'white')\nplt.figure(figsize = (8, 8))\nplt.plot(false_positive_rate,true_positive_rate, color = '#b01717', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1], linestyle = '--', color = '#174ab0')\nplt.axis('tight')\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","8a3a983b":"### STANDARDIZATION ","11f2cc3e":"## Data Preprocessing ","7d093c70":"### Categorical Encoding ","9207eea9":"### Other models","7b1b9c9f":"#### Target value","21b268c3":"### Both models are probably overfitting the data. We need to add some Regularization here","fd677a6e":"### SPLITTING OF DATASET INTO TRAIN AND TEST","01bc79d7":"## EDA","751fb79f":"### Logistic Regression as a benchmark model","5bd975a4":"#### Hyperparameter Tuning using Grid Search","c0c22041":"# Some observations on the data below:\n* People with age above 50 and avg_gulucose_level above 160 and married have a high chance of stroke\n* Random Forest and XgBoost are the best classifiers","b084a641":"### RandomForest and evaluating on test set","bf7a9e4f":"### XgBoost Classifier and evaluating on test set","099f6248":"### TREATING IMBALANCE CLASS USING SMOTE","14e00f7f":"### Label Encoding \ncolumns: 'ever_married' and 'residence_type'","da5219c8":"## Model Building","4b335356":"### Pre Process Data","ea32cc60":"#### As we can see that Random Forest and XgBoost works best we will hyper tune their parameters","951e541f":"#### ROC CURVE"}}