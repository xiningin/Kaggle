{"cell_type":{"108bc40f":"code","90a0ee3c":"code","3a6da7fb":"code","41637493":"code","c489bfa9":"code","01f8ffbe":"code","9561a603":"code","fc93a5ee":"code","6addf197":"code","f03be3d5":"code","bfca364c":"code","e4ac8f1f":"code","5adae55f":"code","6bac1855":"code","00ae30e4":"code","db3d3a40":"code","df3e71f3":"code","34cf13b2":"code","ffd130cc":"code","2a0c8c56":"code","85c5d48c":"code","76717b25":"code","9ec74e96":"code","a9231bad":"code","8420eadc":"markdown","7e15169c":"markdown","aef9bf19":"markdown","83945c1c":"markdown","cdbfabeb":"markdown","7078129c":"markdown","25fda300":"markdown","43482590":"markdown","3c748e7a":"markdown","e3e5b9eb":"markdown","1bccc7d2":"markdown","528a3917":"markdown","21b842e7":"markdown","41c417dc":"markdown","0d6d9583":"markdown","22c67e2b":"markdown","03b670dc":"markdown","1aa9b71d":"markdown"},"source":{"108bc40f":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/pos-tagging\/pos.csv\")\ndata.head(10)","90a0ee3c":"sentences=[]\ni=0\nwhile i<len(data):\n    tmp=[]\n    while (data.iloc[i,0])!=\".\" and data.iloc[i,1] !='.':\n        tmp.append(data.iloc[i,0])\n        i=i+1\n    tmp.append('.')\n    sentences.append(tuple(tmp))\n    i=i+1\n","3a6da7fb":"tagged_sentences=[]\ni=0\nwhile i<len(data):\n    tmp=[]\n    while (data.iloc[i,0])!=\".\" and data.iloc[i,1] !='.':\n        tmp.append(data.iloc[i,1])\n        i=i+1\n    tmp.append('.')\n    tagged_sentences.append(tuple(tmp))\n    i=i+1","41637493":"print(len(tagged_sentences)),\n    \nprint(len(sentences))","c489bfa9":"from sklearn.model_selection import train_test_split\n \n \ntrain_sentences, test_sentences, train_tags, test_tags = train_test_split(sentences, tagged_sentences, test_size=0.2)","01f8ffbe":"words, tags = set([]), set([])\n \nfor s in train_sentences:\n    for w in s:\n        words.add(w)\n \nfor ts in train_tags:\n    for t in ts:\n        tags.add(t)\n# Chuy\u1ec3n v\u1ec1 s\u1ed1 \u0111\u1ec3 training, c\u00e1c gi\u00e1 tr\u1ecb ch\u01b0a c\u00f3 nh\u00e3n th\u00ec s\u1ebd thay b\u1eb1ng PAD c\u00f2n tr\u1ed1ng th\u00ec thay b\u1eb1ng \nword2index = {w: i + 2 for i, w in enumerate(list(words))}\nword2index['-PAD-'] = 0 \nword2index['-OOV-'] = 1\n \ntag2index = {t: i + 1 for i, t in enumerate(list(tags))}\ntag2index['-PAD-'] = 0 ","9561a603":"train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n \nfor s in train_sentences:\n    s_int = []\n    for w in s:\n        try:\n            s_int.append(word2index[w])\n        except KeyError:\n            s_int.append(word2index['-OOV-'])\n \n    train_sentences_X.append(s_int)\n \nfor s in test_sentences:\n    s_int = []\n    for w in s:\n        try:\n            s_int.append(word2index[w])\n        except KeyError:\n            s_int.append(word2index['-OOV-'])\n \n    test_sentences_X.append(s_int)\n \nfor s in train_tags:\n    train_tags_y.append([tag2index[t] for t in s])\n \nfor s in test_tags:\n    test_tags_y.append([tag2index[t] for t in s])\n \nprint(train_sentences_X[0])\nprint(test_sentences_X[0])\nprint(train_tags_y[0])\nprint(test_tags_y[0])","fc93a5ee":"MAX_LENGTH = len(max(train_sentences_X, key=len))\nprint(MAX_LENGTH)  # 271\n ","6addf197":"from keras.preprocessing.sequence import pad_sequences\n \ntrain_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\ntest_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\ntrain_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\ntest_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n \nprint(train_sentences_X[0])\nprint(test_sentences_X[0])\nprint(train_tags_y[0])\nprint(test_tags_y[0])\n ","f03be3d5":"!pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","bfca364c":"len(word2index)","e4ac8f1f":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\nfrom keras.optimizers import Adam\nfrom keras_contrib.layers import CRF\n \nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(MAX_LENGTH, )))\nmodel.add(Embedding(len(word2index), 128))\nmodel.add(LSTM(256, return_sequences=True))\nmodel.add(TimeDistributed(Dense(len(tag2index))))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(0.001),\n              metrics=['accuracy'])\n \nmodel.summary()","5adae55f":"def to_categorical(sequences, categories):\n    cat_sequences = []\n    for s in sequences:\n        cats = []\n        for item in s:\n            cats.append(np.zeros(categories))\n            cats[-1][item] = 1.0\n        cat_sequences.append(cats)\n    return np.array(cat_sequences)","6bac1855":"history  = model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=10, validation_split=0.2)","00ae30e4":"scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\nprint(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   ","db3d3a40":"scores = model.evaluate(train_sentences_X, to_categorical(train_tags_y, len(tag2index)))\nprint(f\"{model.metrics_names[1]}: {scores[1] * 100}\")   # acc: 99.09751977804825","df3e71f3":"import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","34cf13b2":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ffd130cc":"test_samples = [\"But very mystical too. \".split()]\nprint(test_samples)\n","2a0c8c56":"test_samples_X = []\nfor s in test_samples:\n    s_int = []\n    for w in s:\n        try:\n            s_int.append(word2index[w.lower()])\n        except KeyError:\n            s_int.append(word2index['-OOV-'])\n    test_samples_X.append(s_int)\n \ntest_samples_X = pad_sequences(test_samples_X, maxlen=MAX_LENGTH, padding='post')\nprint(test_samples_X)","85c5d48c":"predictions = model.predict(test_samples_X)\nprint(predictions, predictions.shape)","76717b25":"# Transform ng\u01b0\u1ee3c\ndef logits_to_tokens(sequences, index):\n    token_sequences = []\n    for categorical_sequence in sequences:\n        token_sequence = []\n        for categorical in categorical_sequence:\n            token_sequence.append(index[np.argmax(categorical)])\n \n        token_sequences.append(token_sequence)\n \n    return token_sequences","9ec74e96":"\nprint(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))","a9231bad":"import matplotlib.pyplot as plt\n#  Get all the sentences\nsentences = sentences\n# Plot sentence by lenght\nplt.hist([len(s) for s in sentences], bins=50)\nplt.title('Token per sentence')\nplt.xlabel('Len (number of token)')\nplt.ylabel('# samples')\nplt.show()","8420eadc":"\u0110\u1ec6M C\u00c1C GI\u00c1 TR\u1eca V\u00c0O C\u00c1C T\u1eea M\u00c0 CH\u01afA C\u00d3 NH\u00c3N T\u1eea LO\u1ea0I \u0110\u1ed2NG TH\u1edcI CHUY\u1ec2N T\u1ea4T C\u1ea2 V\u1ec0 S\u1ed0 \u0110\u1ec2 TRAINING","7e15169c":"GI\u1ed0NG NH\u01af H\u00c0M TRANSFORM H\u00c0M N\u00c0Y GI\u00daP CHUY\u1ec2N \u0110\u1ed4I \u0110\u1ea6U RA","aef9bf19":"PADDING GI\u00daP C\u00c1C CHU\u1ed6I C\u00d3 \u0110\u1ed8 D\u00c0I B\u1eb0NG NHAU","83945c1c":"---------------\nAccuracy on train","cdbfabeb":"-----------------\nAccuracy on test","7078129c":"CHIA TRAINING V\u00c0 TEST THEO T\u1ef6 L\u1ec6 80:20","25fda300":"M\u00c3 H\u00d3A 1 N\u00d3NG","43482590":"Processing **sentences**","3c748e7a":"------------\nPLOT","e3e5b9eb":"Gi\u1edd m\u1edbi chuy\u1ec3n, b\u01b0\u1edbc tr\u00ean l\u00e0m g\u00ec kh\u00f4ng bi\u1ebft ???","1bccc7d2":"T\u00c1CH NH\u00c3N V\u00c0 C\u00c2U T\u01af\u01a0NG \u1ee8NG","528a3917":"------- \n\nL\u00dd THUY\u1ebeT","21b842e7":"------\nTESTING\n","41c417dc":"Processing data into **sentences** and **tagged_sentences**","0d6d9583":"\n\n=> **Ta th\u1ea5y gi\u00e1 tr\u1ecb v\u1edbi Epoch ~6 th\u00ec val_accuracy t\u0103ng d\u1ea7n trong khi val_loss gi\u1ea3m d\u1ea7n nh\u01b0ng khi Epoch > 7 th\u00ec th\u1ec9nh tho\u1ea3ng ng\u01b0\u1ee3c l\u1ea1i t\u1ee9c m\u00f4 h\u00ecnh \u0111ang b\u1ecf c\u00e1c gi\u00e1 tr\u1ecb v\u00f4 \u00edch v\u00e0o model**","22c67e2b":"----------------------------------------------------------------------------------------------------------------\n\nLSTM \n","03b670dc":"Processing **tagged_sentences**","1aa9b71d":"--------------------------\nCODE"}}