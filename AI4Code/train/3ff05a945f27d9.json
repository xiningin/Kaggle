{"cell_type":{"2414025b":"code","c832cfbc":"code","3b1386be":"code","07352605":"code","e4e39942":"code","db84bfd3":"code","896f558a":"code","c2533ced":"code","8eb29d1a":"code","eb12d641":"code","0523e81b":"code","83989e2a":"code","8a79eda7":"code","48102f2a":"code","e2fcca13":"code","a34f6c20":"code","4e123f58":"code","62249212":"code","ba58f606":"code","5c38ddd8":"code","77a876ad":"code","405d8296":"code","86f3895c":"code","262f7978":"code","c0aaa590":"code","ee3ba51d":"code","f56128c9":"code","e8d0002e":"code","4cdb1a6b":"code","12ba20ad":"code","a9542490":"code","70762d77":"code","5a274429":"code","574288c9":"code","ff59399e":"code","87da2730":"code","f064450d":"code","2b1ecbb9":"code","ae7b78ce":"code","88d21ec7":"code","e0d23f48":"code","ce428739":"code","e2762db7":"code","47574836":"code","8f2740a4":"code","b87861bb":"code","9cbe8be5":"code","a721ce4c":"code","fbf0da5d":"code","17a35009":"code","b5d0620c":"code","2555ad0c":"code","f5367b80":"code","e3593ffe":"code","27ab0d05":"code","31e8c696":"code","1a282063":"code","a03e9b32":"code","8e49c4d9":"code","5599bb30":"code","9aa93f3f":"code","fa4b57c8":"code","4af061b0":"code","92024999":"code","38e643b9":"code","20eee90f":"code","cd59e34e":"code","ca2a3919":"code","74fd0c06":"code","af3f449e":"code","59728bf7":"code","0c41b640":"code","ad647124":"code","4a5f5f45":"code","c83acee9":"code","3da60b11":"code","4bce9069":"code","d8bfa697":"code","70f388c8":"code","c9ed9c46":"code","3f7d6c73":"code","d742d6ec":"code","e25785d6":"code","0bd32cf9":"code","1b01991d":"code","2c3f329f":"code","90d7e032":"code","8b749d94":"code","c45fee38":"code","17be4865":"code","c24b2cf4":"code","50fb7936":"code","e58844ab":"code","498f41ef":"code","b2c8283e":"code","e37c76cf":"code","a1c50a5a":"code","64ba3e5c":"code","d2baa881":"code","2c2603f7":"markdown","f4eddfb6":"markdown","bc2255cc":"markdown","66fe22f3":"markdown","aa93fce2":"markdown","34471c37":"markdown","c69365e8":"markdown","3a3c8cf1":"markdown","15254d20":"markdown","5b5efcd2":"markdown","fa67170c":"markdown","a585ac32":"markdown","a34182d2":"markdown","b1f65a71":"markdown","a293e5c2":"markdown","1b22aab5":"markdown","58f6d742":"markdown","168e545e":"markdown","ee8dae77":"markdown","989bf27c":"markdown","0520d434":"markdown","7057602d":"markdown","6a9cf6b9":"markdown","e44ee471":"markdown","00db89a8":"markdown","a57e221f":"markdown","b583a369":"markdown","5ae5a64d":"markdown","c6512843":"markdown","575e6b56":"markdown","07815269":"markdown","5400e67b":"markdown","d7a8bd1a":"markdown","ddb16c66":"markdown","947a2cd7":"markdown","0b0c95d5":"markdown","2a9afcb8":"markdown","fa158197":"markdown","3cb7d559":"markdown","9d412377":"markdown","853d0acb":"markdown","0dc85ef7":"markdown","d6ddb450":"markdown","cd1b61a7":"markdown","701954c6":"markdown"},"source":{"2414025b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c832cfbc":"#Importing librairies\n\nimport pandas as pd \nimport numpy as np\n\n# Scikit-learn library: For SVM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix,accuracy_score,matthews_corrcoef,f1_score\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport itertools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\n# Matplotlib library to plot the charts\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nfrom sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score,classification_report\n\n# Library for the statistic data vizualisation\nimport seaborn as sns\nimport joblib\n\n%matplotlib inline","3b1386be":"%pwd","07352605":"my_path = %pwd","e4e39942":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\nprint('Shape of data: ',data.shape)\ndata.head(3)","db84bfd3":"df = pd.DataFrame(data) # Converting data to Panda DataFrame","896f558a":"df.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)","c2533ced":"print(\"The num of months {}\".format(np.argmax(df.Time)\/30))\ndf['Month'] = pd.qcut(df['Time'], 9494)\ndf['Month']=df['Month'].cat.codes\ndf.head(5)","8eb29d1a":"plt.figure(figsize=(15,8))\n\nclass_0 = df.loc[df['Class'] == 0][\"Time\"]\nclass_1 = df.loc[df['Class'] == 1][\"Time\"]\n\nax=sns.distplot(class_0,hist=False,rug=False,label='Not Fraud').set(xlim=0)\nax=sns.distplot(class_1,hist=False,rug=False,label='Fraud').set_title('Credit Card Transactions Time Density Plot')","eb12d641":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))\ns = sns.boxplot(ax = ax1, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=True)\ns = sns.boxplot(ax = ax2, x=\"Class\", y=\"Amount\", hue=\"Class\",data=df, palette=\"PRGn\",showfliers=False)\nplt.show();","0523e81b":"fraud = df.loc[df['Class'] == 1]\n\ntrace = go.Scatter(\n    x = fraud['Time'],y = fraud['Amount'],\n    name=\"Amount\",\n     marker=dict(\n                color='rgb(238,23,11)',\n                line=dict(\n                    color='red',\n                    width=1),\n                opacity=0.5,\n            ),\n    text= fraud['Amount'],\n    mode = \"markers\"\n)\n\nlayout = dict(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest'\n         )\nfig=go.Figure()\nfig.update_layout(title = 'Amount of fraudulent transactions',\n          xaxis = dict(title = 'Time [s]', showticklabels=True), \n          yaxis = dict(title = 'Amount'),\n          hovermode='closest')\n\nfig.add_trace(trace)","83989e2a":"df_fraud = df[df['Class'] == 1] # Recovery of fraud data\nplt.figure(figsize=(15,10))\nplt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\nplt.title('Scratter plot amount fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.xlim([0,175000])\nplt.ylim([0,2500])\nplt.show()","8a79eda7":"nb_big_fraud = df_fraud[df_fraud['Amount'] > 1000].shape[0] # Recovery of frauds over 1000\nprint('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')","48102f2a":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n\nnumber_fraud = len(data[data.Class == 1])\nnumber_no_fraud = len(data[data.Class == 0])\nprint('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')","e2fcca13":"print(\"The accuracy of the classifier then would be : \"+ str((284315-492)\/284315)+ \" which is the number of good classification over the number of tuple to classify\")","a34f6c20":"df_corr = df.corr() # Calculation of the correlation coefficients in pairs, with the default method:\n                    # Pearson, Standard Correlation Coefficient","4e123f58":"plt.figure(figsize=(15,10))\nsns.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nsns.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()","62249212":"rank = df_corr['Class'] # Retrieving the correlation coefficients per feature in relation to the feature class\ndf_rank = pd.DataFrame(rank) \ndf_rank = np.abs(df_rank).sort_values(by='Class',ascending=False) # Ranking the absolute values of the coefficients\n                                                                  # in descending order\ndf_rank.dropna(inplace=True) # Removing Missing Data (not a number)\ndf_rank","ba58f606":"s = sns.lmplot(x='V20', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V7', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","5c38ddd8":"s = sns.lmplot(x='V2', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\ns = sns.lmplot(x='V5', y='Amount',data=df, hue='Class', fit_reg=True,scatter_kws={'s':2})\nplt.show()","77a876ad":"col_to_normalize=df.drop(['Class','Amount'],axis=1).columns.values.tolist()\n\nstd=StandardScaler()\ndf_to_nor=df[col_to_normalize]\ndf=df.drop(col_to_normalize,axis=1)\n\nstd.fit(df_to_nor)\ndf_to_nor=pd.DataFrame(std.transform(df_to_nor),columns=col_to_normalize)\n\ndf=pd.concat([df,df_to_nor],axis=1)\ndf.head(5)","405d8296":"#Drop the useless time feature\nX=df.drop(['Class','Time'],axis=1)\nY=df['Class']","86f3895c":"rus = RandomUnderSampler(sampling_strategy=1)\ncol=X.columns\nX, Y = rus.fit_resample(X, Y)\nX=pd.DataFrame(X,columns=col)\n\nCounter(Y)","262f7978":"my_df=pd.DataFrame(X.copy())\nmy_df['Class']=Y","c0aaa590":"f, axes = plt.subplots(ncols=4, figsize=(25,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V16\", data=my_df, ax=axes[0])\naxes[0].set_title('V16 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=my_df, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=my_df, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=my_df, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.tight_layout()\nplt.show()","ee3ba51d":"data = my_df['V14'].loc[my_df['Class'] == 1].values\nq1, q3 = np.percentile(data, 25), np.percentile(data, 75)\niqr = q3 - q1\n\nlower_bound=q1-(1.5*iqr)\nupper_bound=q3+(1.5*iqr)\nmy_df = my_df.drop(my_df[(my_df['V14'] > upper_bound) | (my_df['V14'] <lower_bound)].index)\n\ndata = my_df['V12'].loc[my_df['Class'] == 1].values\nq1, q3 = np.percentile(data, 25), np.percentile(data, 75)\niqr = q3 - q1\n\nlower_bound=q1-(1.5*iqr)\nupper_bound=q3+(1.5*iqr)\n\nmy_df = my_df.drop(my_df[(my_df['V12'] > upper_bound) | (my_df['V12'] <lower_bound)].index)\n\ndata = my_df['V10'].loc[my_df['Class'] == 1].values\nq1, q3 = np.percentile(data, 25), np.percentile(data, 75)\niqr = q3 - q1\n\nlower_bound=q1-(1.5*iqr)\nupper_bound=q3+(1.5*iqr)\n\nmy_df = my_df.drop(my_df[(my_df['V10'] > upper_bound) | (my_df['V10'] <lower_bound)].index)","f56128c9":"f, axes = plt.subplots(ncols=4, figsize=(25,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V16\", data=my_df, ax=axes[0])\naxes[0].set_title('V16 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=my_df, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=my_df, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=my_df, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.tight_layout()\nplt.show()","e8d0002e":"X=my_df.drop(['Class'],axis=1)\nY=my_df['Class']","4cdb1a6b":"#cost matrix\ncost_mat = np.zeros((my_df.shape[0], 4))\ncost_mat[:, 0] = 3\ncost_mat[:, 1] = my_df['Amount']\ncost_mat[:, 2] = 3\ncost_mat[:, 3] = 0","12ba20ad":"from sklearn.model_selection import GridSearchCV,train_test_split\n\n# However as we haven't infinite time nor the patience, we are going to run the classifier with the undersampled training data \n# (for those using the undersampling principle if results are really bad just rerun the training dataset definition)\n\n# We seperate ours data in two groups : a train dataset and a test dataset\n\nX_train,X_test,Y_train,Y_test,cost_mat_train, cost_mat_valid=train_test_split(X,Y,cost_mat,train_size=0.75,random_state=5,shuffle=True)","a9542490":"plt.figure(figsize=(15,10))\nsns.heatmap(my_df.corr(), cmap=\"YlGnBu\") # Displaying the Heatmap\nsns.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation After Resampling')\nplt.show()","70762d77":"X_train_rank = X_train[df_rank.index[1:11]] # We take the first ten ranked features\nX_train_rank = np.asarray(X_train_rank)","5a274429":"############################## with all the test dataset to see if the model learn correctly ##################\nX_test_all_rank = X_test[df_rank.index[1:11]]\nX_test_all_rank = np.asarray(X_test_all_rank)\ny_test_all = np.asarray(Y_test)","574288c9":"class_names=np.array(['0','1']) # Binary label, Class = 1 (fraud) and Class = 0 (no fraud)","ff59399e":"# Function to plot the confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","87da2730":"measures = {\"F1Score\": f1_score, \"Precision\": precision_score, \n            \"Recall\": recall_score, \"Accuracy\": accuracy_score}","f064450d":"# SVM\nclassifier = svm.SVC(kernel='linear',probability=True) # We set a SVM classifier, the default SVM Classifier (Kernel = Radial Basis Function)\nclassifier.fit(X_train, Y_train) # Then we train our model, with our balanced data train.","2b1ecbb9":"prediction_SVM_all = classifier.predict(X_test) #And finally, we predict our data test.","ae7b78ce":"cm = confusion_matrix(y_test_all, prediction_SVM_all)\nplot_confusion_matrix(cm,class_names)","88d21ec7":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","e0d23f48":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","ce428739":"classifier.fit(X_train_rank, Y_train) # Then we train our model, with our balanced data train.\nprediction_SVM = classifier.predict(X_test_all_rank) #And finally, we predict our data test.","e2762db7":"cm = confusion_matrix(y_test_all, prediction_SVM)\nplot_confusion_matrix(cm,class_names)","47574836":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","8f2740a4":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","b87861bb":"results_svm=[measures[measure](y_test_all,prediction_SVM) for measure in measures.keys()]\nresults_svm = pd.DataFrame(results_svm).T\nresults_svm.columns=measures.keys()\nresults_svm","9cbe8be5":"classifier_b = svm.SVC(kernel='linear',class_weight={0:0.60, 1:0.40},probability=True)","a721ce4c":"classifier_b.fit(X_train, Y_train) # Then we train our model, with our balanced data train.","fbf0da5d":"prediction_SVM_b_all = classifier_b.predict(X_test) #We predict all the data set.","17a35009":"cm = confusion_matrix(y_test_all, prediction_SVM_b_all)\nplot_confusion_matrix(cm,class_names)","b5d0620c":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","2555ad0c":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","f5367b80":"classifier_b.fit(X_train_rank, Y_train) # Then we train our model, with our balanced data train.\nprediction_SVM_b = classifier_b.predict(X_test_all_rank) #And finally, we predict our data test.","e3593ffe":"cm = confusion_matrix(y_test_all, prediction_SVM_b)\nplot_confusion_matrix(cm,class_names)","27ab0d05":"print('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]\/(cm[1][0]+cm[1][1])) \/ 5))","31e8c696":"print('We have detected ' + str(cm[1][1]) + ' frauds \/ ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]\/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) \/ (sum(cm[0]) + sum(cm[1]))))","1a282063":"results_svm_b=[measures[measure](y_test_all,prediction_SVM_b) for measure in measures.keys()]\nresults_svm_b = pd.DataFrame(results_svm_b).T\nresults_svm_b.columns=measures.keys()\nresults_svm_b","a03e9b32":"print(classification_report(y_test_all,prediction_SVM_b, target_names=['Not Fraud','Fraud']))","8e49c4d9":"#Using grid search:\n\n#Set up the searching area\ngrid_param = {  \n    'n_neighbors': range(1,20),\n    'weights': ['uniform', 'distance'],\n    'metric': ['minkowski', 'euclidean']\n}","5599bb30":"from sklearn.model_selection import GridSearchCV\n\ngd_sr = GridSearchCV(KNeighborsClassifier(),  \n                     param_grid = grid_param,\n                     scoring = 'accuracy',\n                     n_jobs=-1)","9aa93f3f":"print(gd_sr.fit(X_train_rank, Y_train).best_score_)","fa4b57c8":"print(gd_sr.fit(X_train_rank, Y_train).best_params_)","4af061b0":"# Store the Knn model\n\n#knn_model_fin = KNeighborsClassifier(n_neighbors = 2,weights = 'uniform', metric = 'minkowski')\n#knn_model_fin.fit(X_train_ran,Y)\n\n!mkdir knn_model\nknn_model_name = f'{my_path}\/knn_model\/knn_model_final.sav'\njoblib.dump(gd_sr.best_estimator_,knn_model_name)","92024999":"## Let's load the saved model first.\nkNN_loaded = joblib.load(f'{my_path}\/knn_model\/knn_model_final.sav')","38e643b9":"#Testing on the test set\ny_test_pred = kNN_loaded.predict(X_test_all_rank)","20eee90f":"# Print accuracy\n\nprint(accuracy_score(y_test_all,y_test_pred))","cd59e34e":"print(classification_report(y_test_all,y_test_pred, target_names=['Not Fraud','Fraud']))","ca2a3919":"results_knn=[measures[measure](y_test_all,y_test_pred) for measure in measures.keys()]\nresults_knn = pd.DataFrame(results_knn).T\nresults_knn.columns=measures.keys()\nresults_knn","74fd0c06":"from sklearn.ensemble import RandomForestClassifier\n\nparam={\n    'n_estimators':[10,12,14,16],\n    'max_depth':range(1,10)\n}\ngrid=GridSearchCV(estimator=RandomForestClassifier(random_state=41),param_grid=param,cv=3,scoring='f1')\ngrid.fit(X_train_rank,Y_train)\n\nprint(\"best_params\")\nprint(grid.best_params_)\nprint(\"best_score\")\nprint(grid.best_score_)","af3f449e":"test_pre = grid.predict(X_test_all_rank)\ntest_pre_pro = grid.predict_proba(X_test_all_rank)\ntrain_pre_pro = grid.predict_proba(X_train_rank)","59728bf7":"results_rf = pd.DataFrame([measures[measure](y_test_all,test_pre) for measure in measures.keys()]).T\nresults_rf.columns=measures.keys()\n\nresults_rf","0c41b640":"print(classification_report(y_test_all,test_pre, target_names=['Not Fraud','Fraud']))","ad647124":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nimport keras.backend as K\ndef dnn(indput_dim, dropout=0.2):\n    model = Sequential([\n    Dense(units=16, input_dim=indput_dim, activation='relu'),\n    Dropout(dropout),\n    Dense(units=16, activation='relu'),\n    Dropout(dropout),\n    Dense(1, activation='sigmoid')])\n    return model","4a5f5f45":"clf = dnn(indput_dim=X_train_rank.shape[1], dropout=0.2)\nclf.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\nclf.fit(X_train_rank, Y_train,batch_size=64,\n          epochs=50)","c83acee9":"ann_pred=clf.predict(X_test_all_rank)\n\nann_pred1=[]\nfor i in ann_pred:\n    ann_pred1.append(int(i>0.5))","3da60b11":"results_dnn = pd.DataFrame([measures[measure](y_test_all, ann_pred1) for measure in measures.keys()]).T\nresults_dnn.columns=measures.keys()\n\n\nresults_dnn","4bce9069":"print(classification_report(y_test_all,ann_pred1, target_names=['Not Fraud','Fraud']))","d8bfa697":"RANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\ntarget = 'Class'\npredictors=df_rank.index[1:11].tolist()\n#predictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n#       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n#       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n#       'Amount']","70f388c8":"import xgboost as xgb\n\n# Prepare the train and valid datasets\ndtrain = xgb.DMatrix(X_train_rank,Y_train)\ndtest = xgb.DMatrix(X_test_all_rank,y_test_all)\n\n#What to monitor (in this case, **train** )\nwatchlist = [(dtrain, 'train')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","c9ed9c46":"xgb_model = xgb.train(params, \n                dtrain, \n                MAX_ROUNDS, \n                watchlist, \n                early_stopping_rounds=EARLY_STOP, \n                maximize=True, \n                verbose_eval=VERBOSE_EVAL)","3f7d6c73":"preds = xgb_model.predict(dtest)","d742d6ec":"predictions = [round(value) for value in preds]","e25785d6":"results_gb = pd.DataFrame([measures[measure](y_test_all,predictions) for measure in measures.keys()]).T\nresults_gb.columns=measures.keys()\n\nresults_gb","0bd32cf9":"print(classification_report(y_test_all,predictions, target_names=['Not Fraud','Fraud']))","1b01991d":"from sklearn.linear_model import LogisticRegression\n\n#Using GridSearchCV to find the best parameters.\nparams = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_1 = GridSearchCV(LogisticRegression(), params)\ngrid_1.fit(X_train_rank,  Y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_1.best_estimator_","2c3f329f":"log_pre=grid_1.predict(X_test_all_rank)","90d7e032":"from sklearn.model_selection import cross_val_score\n# Overfitting Case\nscore = cross_val_score(log_reg, X_train_rank,  Y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(score.mean() * 100, 2).astype(str) + '%')","8b749d94":"results_log = pd.DataFrame([measures[measure](y_test_all,log_pre) for measure in measures.keys()]).T\nresults_log.columns=measures.keys()\n\nresults_log","c45fee38":"print(classification_report(y_test_all,log_pre, target_names=['Not Fraud','Fraud']))","17be4865":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve_for(estimator1, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ax1 = plt.subplots(1, 1, figsize=(20,10), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # Estimator\n    train_sizes, train_scores, test_scores = learning_curve(estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    return plt","c24b2cf4":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve_for(log_reg,X_train_rank, Y_train, (0.87, 1.01), cv=cv, n_jobs=4)\n","50fb7936":"from sklearn.metrics import roc_curve,roc_auc_score\n\n\n\nrf_fpr, rf_tpr, rf_thresold = roc_curve(y_test_all,grid.best_estimator_.predict_proba(X_test_all_rank)[:, 1])\ndnn_fpr, dnn_tpr, dnn_threshold = roc_curve(y_test_all, clf.predict_proba(X_test_all_rank))\nsvm_fpr, svm_tpr, svm_thresold = roc_curve(y_test_all,classifier.predict_proba(X_test_all_rank)[:, 1])\nsvmb_fpr, svmb_tpr, svmb_thresold = roc_curve(y_test_all,classifier_b.predict_proba(X_test_all_rank)[:, 1])\nknn_fpr, knn_tpr, knn_thresold = roc_curve(y_test_all,kNN_loaded.predict_proba(X_test_all_rank)[:, 1])\nxgb_fpr, xgb_tpr, xgb_thresold = roc_curve(y_test_all,preds)\nlog_fpr, log_tpr, log_thresold = roc_curve(y_test_all,grid_1.best_estimator_.predict_proba(X_test_all_rank)[:, 1])\n\n\ndef plot_roc_curve(rf_fpr, rf_tpr, csrf_fpr, csrf_tpr, dnn_fpr, dnn_tpr,svm_fpr, svm_tpr,svmb_fpr, svmb_tpr,knn_fpr, knn_tpr,xgb_fpr, xgb_tpr,log_fpr, log_tpr):\n    plt.figure(figsize=(20,15))\n    plt.title('ROC Curve', fontsize=18)\n    plt.plot(rf_fpr, rf_tpr, label='RandomForestClassifier Score: {:.4f}'.format(roc_auc_score(y_test_all,grid.best_estimator_.predict_proba(X_test_all_rank)[:, 1])))\n    #plt.plot(bmr_fpr, bmr_tpr, label='BMR Classifier Score: {:.4f}'.format(roc_auc_score(Y_train, bmr_pred_1)))\n    plt.plot(dnn_fpr, dnn_tpr, label='DNN Score: {:.4f}'.format(roc_auc_score(y_test_all, clf.predict_proba(X_test_all_rank))))\n    plt.plot(svm_fpr, svm_tpr, label='SVM Score: {:.4f}'.format(roc_auc_score(y_test_all,classifier.predict_proba(X_test_all_rank)[:, 1])))\n    plt.plot(svmb_fpr, svmb_tpr, label='SVM_b Score: {:.4f}'.format(roc_auc_score(y_test_all,classifier_b.predict_proba(X_test_all_rank)[:, 1])))\n    plt.plot(knn_fpr, knn_tpr, label='KNN Score: {:.4f}'.format(roc_auc_score(y_test_all,kNN_loaded.predict_proba(X_test_all_rank)[:, 1])))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Score: {:.4f}'.format(roc_auc_score(y_test_all,preds)))\n    plt.plot(log_fpr, log_tpr, label='LogisticsRegression Score: {:.4f}'.format(roc_auc_score(y_test_all,grid_1.best_estimator_.predict_proba(X_test_all_rank)[:, 1])))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()","e58844ab":"plot_roc_curve(rf_fpr, rf_tpr, dnn_fpr, dnn_tpr,svm_fpr, svm_tpr,svmb_fpr, svmb_tpr,knn_fpr, knn_tpr,xgb_fpr, xgb_tpr,log_fpr, log_tpr, log_fpr, log_tpr)\nplt.plot()","498f41ef":"res=pd.concat([results_svm,results_svm_b,results_knn,results_dnn,results_rf,results_gb,results_log],axis=0)","b2c8283e":"res.index=['SVM','SVM_b','KNN','DNN','RandomForest','XGBoost','LogisticsRegression']\nres","e37c76cf":"res.fillna(value=0)","a1c50a5a":"res_no_s=res","64ba3e5c":"res_no_s.plot(kind='bar',figsize=(18,10))\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n","d2baa881":"res_no_s.plot(kind='line',figsize=(22,10))","2c2603f7":"## Try to find the outliars","f4eddfb6":"## DNN","bc2255cc":"## The result of using randomforest is quite good","66fe22f3":"# Learning Curve Plot\n","aa93fce2":"## Scale the data except class and account","34471c37":"## Heatmap of correlation is informative","c69365e8":"### Transaction Amount","3a3c8cf1":"As we can notice, most of the features are not correlated with each other. This corroborates the fact that a PCA was previously performed on the data.\n\nWhat can generally be done on a massive dataset is a dimension reduction. By picking th emost important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.\n\nHowever in this case given the fact that a PCA was previously performed, if the dimension reduction is effective then the PCA wasn't computed in the most effective way. Another way to put it is that no dimension reduction should be computed on a dataset on which a PCA was computed correctly.","15254d20":"### Define predictors and target values\nLet's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature.","5b5efcd2":"In this previously used SVM model, the weigh of each class was the same, which means that missing a fraud is as bad as misjudging a non-fraud. The objective, for a bank, is to maximize the number of detected frauds! Even if it means considering more non-fraud tuple as fraudulent operation. So, we need to minimize the False positives : the number of no detected frauds.\n\nIndeed, by modifying the class_weight parameter, we can chose which class to give more importance during the training phase. In this case, the class_1 which describes the fraudulent operations will be considered more important than the class_0 (non-fraud operation). However, in this case we will give more importance to the class_0 due to the large number of misclassed non-fraud operation. Of course the goal is to lose as little effective fraud as possible in the process.","fa67170c":"## Randomforest and DNN are the best models if we look into the graph below\n","a585ac32":"## The amount of outliars decreases","a34182d2":"We can confirm that the two couples of features are inverse correlated (the regression lines for Class = 0 have a negative slope while the regression lines for Class = 1 have a very small negative slope).","b1f65a71":"To answer this problem we could use the oversampling principle or the undersampling principle The undersampling principle should be used only if we can be sure that the selected few tuples (in this case non-fraud) are representative of the whole non-fraud transactions of the dataset.","a293e5c2":"## Evaluation of different models' prediction\n","1b22aab5":"## Correlation of features","58f6d742":"We can see that the study using the reduced data is far from unrelevant, which means that the last step of the previously computed PCA could have been done in a more efficient way. Indeed one of the main question we have with the PCA once we calculated the principals components direction, is how many of this component are we gonna keep. This means that some of the 30 dimensions are do not discriminate classes that much.","168e545e":"Then we define training and testing set after applying a dimension reduction to illustrate the fact that nothing will be gained because a PCA was previously computed","ee8dae77":"By default, the predictions made by XGBoost are probabilities. Because this is a binary classification problem, each prediction is the probability of the input pattern belonging to the first class. We can easily convert them to binary class values by rounding them to 0 or 1.","989bf27c":"## Use IQR to delete those outliars which affect badly on our prediction","0520d434":"## RandomForestClassifier\n","7057602d":"## Model Selection","6a9cf6b9":"## Cost matrix used for cost sensitive learning\n- To evaluate a model\u2019s performance in terms of costs, I first calculated the sum of all costs resulting from the predictions based on whether the model predicted a False Positive, False Negative, True Positive or True Negative and the costs associated with each case.","e44ee471":"# Banks, merchants and credit card processors companies lose billions of dollars every year to credit card fraud. Credit card data can be stolen by criminals but sometimes the criminal is simply the clerk that processes your card when you buy things.\nThe latest Nilson report estimates that in 2016, worldwide credit card losses topped $24.71 billion. Barclays reports that 47% of all credit card fraud occurs in the United States. It is important that credit card companies should able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. This can be achieved with help of machine learning models. In December 2013, the Abington Police arrested two Post Office employees for stealing credit cards and using it to buy more than $50,000 worth of merchandise. It took police forces a few months before identifying the criminals.\nAnalyzing fraudulent transactions manually is unfeasible due to huge amounts of data and its complexity. However, given sufficiently informative features, one could expect it is possible to do using Machine Learning. This hypothesis will be explored in the project.\n\n## Dataset :\n\nThe datasets contain transactions made by credit cards in September 2013 by European cardholders. This dataset presents transactions that occurred in two days.\nIt contains only numerical input variables which are the result of a PCA transformation.\nDue to confidentiality issues, there are not provided the original features and more background information about the data.\n\u2022\tFeatures V1, V2, ... V28 are the principal components obtained with PCA;\n\u2022\tThe only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount, this feature can be used for example-dependent cost-sensitive learning.\n\u2022\tFeature Class is the response variable and it takes value 1 in case of fraud and 0 otherwise.","00db89a8":"## The BMR classifier is a decision model based on quantifying tradeoffs between various decisions using probabilities and the costs that accompany such decisions.\n## It's example dependent","a57e221f":"## Fraudulent transactions have a distribution more even than valid transactions - are equaly distributed in time, including the low real transaction times, during night in Europe timezone.","b583a369":"## Roc curve of different models are similar except the curve of CSRF","5ae5a64d":"## Data Selection","c6512843":"## Load Data","575e6b56":"This dataset is unbalanced which means using the data as it is might result in unwanted behaviour from a supervised classifier. To make it easy to understand if a classifier were to train with this data set trying to achieve the best accuracy possible it would most likely label every transaction as a non-fraud","07815269":"## Its dissappointing that we got bad prediction using BayesMinimumRiskClassifier\n## More importantly,If the recall score is worse, its meaningless to use this classifier","5400e67b":"## Binning the time","d7a8bd1a":"## SVM","ddb16c66":"We notive, first of all, the time doesn't impact the frequency of frauds. Moreover, the majority of frauds are small amounts.","947a2cd7":"The real transaction have a larger mean value, larger Q1, smaller Q3 and Q4 and larger outliers; fraudulent transactions have a smaller Q1 and mean, larger Q4 and smaller outliers.\n\nLet's plot the fraudulent transactions (amount) against time. The time is shown is seconds from the start of the time period (totaly 48h, over 2 days).","0b0c95d5":"### Transactions in time","2a9afcb8":"## KNN","fa158197":"We can confirm that the two couples of features are correlated (the regression lines for Class = 0 have a positive slope, whilst the regression line for Class = 1 have a smaller positive slope).\n\nLet's plot now the inverse correlated values.","3cb7d559":"# Gradient Boosting Model\n\n### XGBoost is a gradient boosting algorithm.\n\nLet's prepare the model.\n\n<b>Prepare the model<\/b>\n\nWe initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning.","9d412377":"## Logistics Regression Model\n","853d0acb":"## Random undersampling to resample the data","0dc85ef7":"## Re-balanced class weigh","d6ddb450":"## Saving score \n## The financial savings are defined as the cost of the algorithm versus the cost of using no algorithm at all.","cd1b61a7":"## Quite good prediction","701954c6":"## Roc curve"}}