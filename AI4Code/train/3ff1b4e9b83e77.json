{"cell_type":{"004a4ed2":"code","ce7856ad":"code","5cd6199f":"code","0679eddd":"code","1e723ddf":"code","5093127b":"code","5ba776f6":"code","9ef49476":"code","c15d3064":"code","1fded54e":"code","c4ac46bc":"code","8531df57":"code","d1ccf5eb":"code","5c4f534e":"code","f15406a1":"code","98d66b65":"code","298f22f1":"code","46d27d43":"code","39ecd818":"code","8ab00d0e":"code","b2602cdf":"code","26cead3d":"code","03f27fc6":"code","4c052b20":"code","b38fce13":"code","da317667":"code","c4becab1":"code","5f585eb4":"code","ec9d69a6":"code","37cea317":"code","a851687a":"code","7451ab8b":"code","50ca3d12":"code","e354e8af":"markdown","9c147c04":"markdown","2c0fd22a":"markdown","8bc51e1a":"markdown","8c5a6565":"markdown","79737a54":"markdown","c38b42c5":"markdown","00140477":"markdown","4d813925":"markdown","c412d251":"markdown","86944af4":"markdown","20f935fe":"markdown","ac1cf2a0":"markdown","727e145b":"markdown","7d58262c":"markdown","88af2992":"markdown","98881d5d":"markdown","681d062b":"markdown","114f5a0d":"markdown","6ac3475c":"markdown","9cde2c0e":"markdown","52eb9bf7":"markdown","bfdfefee":"markdown","eba89e8c":"markdown","f26e8509":"markdown","5b26ed64":"markdown","f3bd606d":"markdown","a9ae599d":"markdown","bf65223b":"markdown","de400b2a":"markdown","01b1f389":"markdown","774521fc":"markdown","d766a04f":"markdown","b1feec98":"markdown","ace13ff5":"markdown"},"source":{"004a4ed2":"import timeit\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom tempfile import mkdtemp\nfrom sklearn.base import clone\nimport matplotlib.pyplot as plt\nfrom scipy.stats import variation\nfrom sklearn.cluster import KMeans\nfrom imblearn import FunctionSampler\nfrom imblearn.combine import SMOTEENN\nfrom sklearn.decomposition import PCA\nfrom imblearn.pipeline import Pipeline\nfrom vecstack import StackingTransformer\nfrom scipy.stats import chi2_contingency\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE, SelectFromModel\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve, classification_report\nfrom sklearn.metrics import accuracy_score as metric_scorer, classification_report\nfrom imblearn.under_sampling import RandomUnderSampler, RepeatedEditedNearestNeighbours\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, TimeSeriesSplit, StratifiedKFold\nfrom sklearn.preprocessing import PolynomialFeatures, KBinsDiscretizer, PowerTransformer, OneHotEncoder, FunctionTransformer\n\nwarnings.filterwarnings('ignore')","ce7856ad":"MEMORY = mkdtemp()\n\nKEYS = {\n    'SEED': 1,\n    'DATA_PATH' : '..\/input\/turnover.csv',\n    'TARGET': 'left',\n    'METRIC': 'accuracy',\n    'TIMESERIES': False,\n    'SPLITS': 5,\n    'ESTIMATORS': 150,\n    'ITERATIONS': 500,\n    'MEMORY': MEMORY\n}","5cd6199f":"def read_data(input_path):\n    return pd.read_csv(input_path)\n\ndata = read_data(KEYS['DATA_PATH'])\n\ndata.head()","0679eddd":"data.describe()","1e723ddf":"data.dtypes","5093127b":"def missing_data(df):\n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \nmissing_data(data)","5ba776f6":" def convert_to_category(df, cols):\n    for i in cols:\n        df[i] = df[i].astype('category')\n    return df\n\ndata = convert_to_category(data, data.iloc[:,5:8])\n\ndata.dtypes","9ef49476":"train_data, holdout = train_test_split(data, test_size=0.2)","c15d3064":"pairplot = sns.pairplot(train_data, hue=KEYS['TARGET'], palette=\"husl\")","1fded54e":"def types(df, types, exclude = None):\n    types = df.select_dtypes(include=types)\n    excluded = [KEYS['TARGET']]\n    if exclude:\n        for i in exclude:\n            excluded.append(i)\n    cols = [col for col in types.columns if col not in excluded]\n    return df[cols]\n\ndef numericals(df, exclude = None):\n    return types(df, [np.number], exclude)\n\ndef categoricals(df, exclude = None):\n    return types(df, ['category', object], exclude)\n\ndef boxplot(df, exclude = []):\n    plt.figure(figsize=(12,10))\n    num = numericals(df, exclude)\n    num = (num - num.mean())\/num.std()\n    ax = sns.boxplot(data=num, orient='h')\n    \nboxplot(data)","c4ac46bc":"def coefficient_variation(df, threshold = 0.05, exclude=[]):\n        plt.figure(figsize=(8, 6))\n        cols = numericals(df, exclude)\n        variance = variation(cols)\n        ax = sns.barplot(\n            x=np.sort(variance)[::-1],\n            y=cols.columns,\n        )\n        \n        cols = [x for x in cols.columns[np.argwhere(variance < threshold)]]\n        if len(cols) > 0:\n            print(str(cols) + ' are invariant with a threshold of ' + str(threshold))\n        else:\n            print('No invariant columns')\n        return cols\n    \ninvariant = coefficient_variation(data, threshold = 0.05)","8531df57":"def correlated(df, threshold = 0.9):\n    categoric = categorical_correlated(df, threshold)\n    numeric = numerical_correlated(df, threshold)\n\n    plt.figure(figsize=(12,10))\n    sns.heatmap(categoric[1],cbar=True,fmt =' .2f', annot=True, cmap='viridis').set_title('Categorical Correlation', fontsize=30)\n\n    plt.figure(figsize=(12,10))\n    sns.heatmap(numeric[1],cbar=True,fmt =' .2f', annot=True, cmap='viridis').set_title('Numerical Correlation', fontsize=30)\n\n    correlated_cols = categoric[0] + numeric[0]\n\n    if(len(correlated_cols) > 0):\n        print('The following columns are correlated with a threshold of ' + str(threshold) + ': ' + str(correlated_cols))\n\n        if KEYS['TARGET'] in correlated_cols:\n            print('The target variable is correlated, consider removing its correlated counterpart')\n            correlated_cols.remove(KEYS['TARGET'])\n    else:\n        print('No correlated columns for the  ' + str(threshold) + ' threshold')\n\n    return correlated_cols\n\ndef numerical_correlated(df, threshold=0.9):\n    corr_matrix = np.absolute(df.select_dtypes(include=[np.number]).corr(method='spearman')).abs()\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    return [column for column in upper.columns if any(abs(upper[column]) > threshold)], corr_matrix\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x, y)\n    chi2 = chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r, k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1), (rcorr-1)))\n\ndef categorical_correlated(df, threshold=0.9):\n    columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    corr = pd.DataFrame(index=columns, columns=columns)\n    for i in range(0, len(columns)):\n        for j in range(i, len(columns)):\n            if i == j:\n                corr[columns[i]][columns[j]] = 1.0\n            else:\n                cell = cramers_v(df[columns[i]], df[columns[j]])\n                corr[columns[i]][columns[j]] = cell\n                corr[columns[j]][columns[i]] = cell\n    corr.fillna(value=np.nan, inplace=True)\n    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n    return [column for column in upper.columns if any(abs(upper[column]) > threshold)], corr\n\ncorrelated_cols = correlated(train_data, 0.7)","d1ccf5eb":"def under_represented(df, threshold = 0.99):\n    under_rep = []\n    for column in df:\n        counts = df[column].value_counts()\n        majority_freq = counts.iloc[0]\n        if (majority_freq \/ len(df)) > threshold:\n            under_rep.append(column)\n\n    if not under_rep:\n        print('No underrepresented features')\n    else:\n        if KEYS['TARGET'] in under_rep:\n            print('The target variable is underrepresented, consider rebalancing')\n            under_represented.remove(KEYS['TARGET'])\n        print(str(under_rep) + ' underrepresented')\n\n    return under_rep\n\nunder_rep = under_represented(train_data, 0.97)","5c4f534e":"def split_x_y(df):\n    return df.loc[:, df.columns != KEYS['TARGET']], df.loc[:, KEYS['TARGET']]\n\ndef one_hot_encode(df, cols):\n    for i in cols:\n        dummies = pd.get_dummies(df[i], prefix=i, drop_first = True)\n        df = pd.concat([df, dummies], axis = 1)\n        df = df.drop(i, axis = 1)\n\n    return df\n\ndef plot_pca_components(df, variance = 0.9, convert = False):\n    X, y = split_x_y(df)\n\n    if convert:\n        X = one_hot_encode(X, categoricals(X))\n\n    pca = PCA().fit(X)\n\n    sns.set_style(\"whitegrid\")\n    plt.figure(figsize=(9, 7))\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('Number of Components')\n    plt.ylabel('Variance (%)')\n    plt.show()\n    \nplot_pca_components(data, convert = True)","f15406a1":"def feature_importance(df, model, convert = False):\n    X, y = split_x_y(df)\n\n    if convert:\n        X = one_hot_encode(X, categoricals(X))\n    model.fit(X, y)\n    importances = model.feature_importances_\n    std = np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\n    indices = np.argsort(importances)\n\n    print(\"Feature ranking:\")\n    plt.figure(figsize=(16, 14))\n    plt.title(\"Feature importances\")\n    plt.barh(range(X.shape[1]), importances[indices],color=\"r\", xerr=std[indices], align=\"center\")\n    plt.yticks(range(X.shape[1]), [list(X)[i] for i in indices])\n    plt.ylim([-1, X.shape[1]])\n    plt.show()\n\nfeature_importance(data, RandomForestClassifier(n_estimators=KEYS['ESTIMATORS'], random_state = KEYS['SEED']), convert = True)","98d66b65":"def target_distribution(df):\n    plt.figure(figsize=(8,7))\n    target_count = (df[KEYS['TARGET']].value_counts()\/len(df))*100\n    target_count.plot(kind='bar', title='Target Distribution (%)')\n\ntarget_distribution(train_data)","298f22f1":"def avg_time_pp(df):\n    df = df.copy()\n    df['avg_time_per_project'] = (df['average_montly_hours'] * 12 * df['time_spend_company'])\/ df['number_project']\n    df['avg_time_per_project'] = df['avg_time_per_project'].replace([np.inf, -np.inf], np.nan)\n    df['avg_time_per_project'] = df['avg_time_per_project'].fillna(0)\n    \n    return df","46d27d43":"def drop_features(df, cols):\n    return df[df.columns.difference(cols)]","39ecd818":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True)),\n])","8ab00d0e":"categorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])","b2602cdf":"pipe = Pipeline([\n    ('avg_time_pp', FunctionTransformer(avg_time_pp, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, numericals(data, [KEYS['TARGET']]).columns),\n        ('categorical_pipeline', categorical_pipeline, ['sales', 'salary']),\n    ], remainder='passthrough')),\n])\n\nmodels = [\n    {'name':'logistic_regression', 'model': LogisticRegression(solver = 'lbfgs', max_iter = KEYS['ITERATIONS'], random_state = KEYS['SEED'])},\n    {'name':'random_forest', 'model': RandomForestClassifier(n_estimators = KEYS['ESTIMATORS'], random_state = KEYS['SEED'])},\n    {'name': 'extra_tree', 'model': ExtraTreesClassifier(random_state = KEYS['SEED'])}\n]","26cead3d":"def pipeline(df, models, pipe, all_scores = pd.DataFrame(), splits = None, note = ''):\n    if splits is None:\n        splits = KEYS['SPLITS']\n\n    for model in models:\n        if len(all_scores) == 0 or len(all_scores[(all_scores['Model'] == model['name']) & (all_scores['Steps'] == ', '.join(pipe_steps(pipe)))]) == 0:\n            try:\n                start = timeit.default_timer()\n\n                scores, cv_model = cross_val(df.copy(), model, pipe = pipe, splits = splits)\n\n            except Exception as error:\n                cv_model = pipe\n                note = 'Error: ' + str(error)\n                print(note)\n                scores = np.array([0])\n\n            all_scores = score(model['name'], scores, timeit.default_timer(), start, cv_model, note, all_scores)\n\n        else:\n            print(str(model['name']) + ' already trained on those parameters, ignoring')\n\n    show_scores(all_scores)\n\n    return all_scores\n\ndef cross_val(df, model, splits = None, pipe = None, grid = None):\n    if splits is None:\n        splits = KEYS['SPLITS']\n\n    X, y = split_x_y(df)\n\n    if KEYS['TIMESERIES']:\n        folds = TimeSeriesSplit(n_splits = splits)\n    else:\n        folds = StratifiedKFold(n_splits = splits, shuffle = True, random_state=KEYS['SEED'])\n\n    if pipe:\n        pipe_cv = clone(pipe)\n        pipe_cv.steps.append((model['name'], model['model']))\n        model = pipe_cv\n\n    if grid:\n        model = RandomizedSearchCV(model, grid, scoring = KEYS['METRIC'], cv = folds, n_iter = 10, refit=True, return_train_score = False, error_score=0.0, n_jobs = -1, random_state = KEYS['SEED'])\n        model.fit(X, y)\n        scores = model.cv_results_['mean_test_score']\n    else:\n        scores = cross_val_score(model, X, y, scoring = KEYS['METRIC'], cv = folds, n_jobs = -1)\n\n    return scores, model\n\ndef pipe_steps(pipe):\n    return flatten([x[0] if not isinstance(x[1], ColumnTransformer) else [list(i[1].named_steps.keys()) for ind,i in enumerate(x[1].transformers)] for x in pipe.steps])\n\ndef flatten(pipe):\n    flat = []\n    for i in pipe:\n        if isinstance(i,list): flat.extend(flatten(i))\n        else: flat.append(i)\n    return flat\n\ndef score(model, scores, stop, start, pipe, note = '', all_scores = pd.DataFrame()):\n    if len(all_scores) == 0:\n        all_scores  = pd.DataFrame(columns = ['Model', 'Mean', 'CV Score', 'Time', 'Cumulative', 'Pipe', 'Steps', 'Note'])\n\n    if len(scores[scores > 0]) == 0:\n        note = 'Warning: All scores negative'\n        mean = 0\n        std = 0\n    else:\n        mean = np.mean(scores[scores > 0])\n        std = np.std(scores[scores > 0])\n\n    cumulative = stop - start\n    if len(all_scores[all_scores['Model'] == model]) > 0:\n        cumulative += all_scores[all_scores['Model'] == model].tail(1)['Cumulative'].values[0]\n\n    return all_scores.append({'Model': model, 'Mean': mean, 'CV Score': '{:.3f} +\/- {:.3f}'.format(mean, std), 'Time': stop - start, 'Cumulative': cumulative, 'Pipe': pipe, 'Steps': ', '.join(pipe_steps(pipe)[:-1]), 'Note': note}, ignore_index=True)\n\ndef show_scores(all_scores, top = False):\n    pd.set_option('max_colwidth', -1)\n\n    if top:\n        a_s = all_scores.sort_values(['Mean'], ascending = False).groupby('Model').first()\n        display(a_s.loc[:, ~a_s.columns.isin(['Mean', 'Pipe', 'Cumulative'])])\n    else:\n        display(all_scores.loc[:, ~all_scores.columns.isin(['Mean', 'Pipe', 'Cumulative'])])\n            \nall_scores = pipeline(train_data, models, pipe)","03f27fc6":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True)),\n    ('binning', KBinsDiscretizer(n_bins = 5, encode = 'onehot-dense')),\n    ('polynomial', PolynomialFeatures(degree = 2, include_bias = False)),\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('avg_time_pp', FunctionTransformer(avg_time_pp, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, numericals(data, [KEYS['TARGET']]).columns),\n        ('categorical_pipeline', categorical_pipeline, ['sales', 'salary']),\n    ], remainder='passthrough'))\n])\n\nall_scores = pipeline(train_data, models, pipe, all_scores)","4c052b20":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True))\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('avg_time_pp', FunctionTransformer(avg_time_pp, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, numericals(data, [KEYS['TARGET']]).columns),\n        ('categorical_pipeline', categorical_pipeline, ['sales', 'salary']),\n    ], remainder='passthrough')),\n    ('combined_sampler', SMOTEENN(random_state = KEYS['SEED'])),\n])\n\nall_scores = pipeline(train_data, models, pipe, all_scores)","b38fce13":"num_pipeline = Pipeline([ \n    ('power_transformer', PowerTransformer(method='yeo-johnson', standardize = True))\n])\n\ncategorical_pipeline = Pipeline([\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n])\n\npipe = Pipeline([\n    ('avg_time_pp', FunctionTransformer(avg_time_pp, validate=False)),\n    ('drop_features', FunctionTransformer(drop_features, kw_args={'cols': correlated_cols + under_rep}, validate=False)),\n    ('column_transformer', ColumnTransformer([\n        ('numerical_pipeline', num_pipeline, numericals(data, [KEYS['TARGET']]).columns),\n        ('categorical_pipeline', categorical_pipeline, ['sales', 'salary']),\n    ], remainder='passthrough')),\n    ('pca', PCA(n_components = 6))\n])\n\nall_scores = pipeline(train_data, models, pipe, all_scores)","da317667":"def plot_models(all_scores):\n    sns.set_style(\"whitegrid\")\n    plt.figure(figsize=(16, 8))\n    ax = sns.lineplot(x=\"Cumulative\", y=\"Mean\", hue=\"Model\", style=\"Model\", markers=True, dashes=False, data=all_scores)\n    label = str(KEYS['METRIC']) + ' Score'\n    ax.set(ylabel=label, xlabel='Time')\n    \nplot_models(all_scores)","c4becab1":"show_scores(all_scores, top = True)","5f585eb4":"def top_pipeline(all_scores, index = 0):\n    return all_scores.sort_values(by=['Mean'], ascending = False).iloc[index]['Pipe']\n    \ngrid = {\n    'random_forest__criterion': ['gini', 'entropy'],\n    'random_forest__min_samples_leaf': [10, 20],\n    'random_forest__min_samples_split': [5, 8],\n    'random_forest__max_leaf_nodes': [30, 60],\n}\n\nfinal_scores, grid_pipe = cross_val(train_data, model = clone(top_pipeline(all_scores)), grid = grid)\nfinal_scores","ec9d69a6":"print(grid_pipe.best_params_)\nfinal_pipe = grid_pipe.best_estimator_","37cea317":"def predict(df, holdout, pipe):\n    X_train, y_train = split_x_y(df)\n    pipe.fit(X_train, y_train)\n\n    X, y = split_x_y(holdout)\n\n    return y, pipe.predict(X)\n\ny, predictions = predict(train_data, holdout, final_pipe)\nscore = metric_scorer(y, predictions)\nscore","a851687a":"def plot_roc(fpr, tpr, logit_roc_auc):\n    plt.figure(figsize=(12, 6))\n    plt.plot(fpr, tpr)\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.xlim([0.0, 1.05])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC curve')\n    plt.show()\n\ndef roc(df, model, predictions):\n    X, y = split_x_y(df)\n    logit_roc_auc = roc_auc_score(y, predictions)\n    fpr, tpr, thresholds = roc_curve(y, model.predict_proba(X)[:,1])\n    plot_roc(fpr, tpr, logit_roc_auc)\n    print(classification_report(y, predictions))\n        \nroc(holdout, final_pipe, predictions)","7451ab8b":"def stack_predict(df, holdout, pipes, amount = 2):\n    X, y = split_x_y(df)\n    X_test, y_test = split_x_y(holdout)\n\n    pipe = Pipeline(top_pipeline(pipes).steps[:-1])\n    X = pipe.fit_transform(X)\n    X_test = pipe.transform(X_test)\n\n    estimators = []\n\n    for i in range(amount):\n        estimators.append((str(i), top_pipeline(pipes, i).steps[-1][1]))\n\n    regression = False\n\n    if KEYS['METRIC'] in ['explained_variance', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_median_absolute_error', 'r2'] :\n        regression = True\n\n    stack = StackingTransformer(estimators, regression)\n    stack.fit(X, y)\n\n    S_train = stack.transform(X)\n    S_test = stack.transform(X_test)\n\n    final_estimator = estimators[0][1]\n    final_estimator.fit(S_train, y)\n\n    return final_estimator, y_test, final_estimator.predict(S_test)\n\nstacked, y_stacked, predictions_stacked = stack_predict(train_data, holdout, all_scores, amount = 2)\nscore_stacked = metric_scorer(y_stacked, predictions_stacked)\nscore_stacked","50ca3d12":"print(classification_report(y_stacked, predictions_stacked))","e354e8af":"### Pipeline Performance by Model\nHere we can see the performance of each model in the different pipelines we created.","9c147c04":"Here we will standardize features and fix its skewness so that the scale does not affect the modeling.","2c0fd22a":"## Feature Engineering \/ Pipeline \/ Modeling\n\nA number of different combinations of feature engineering steps and transformations will be performed in a pipeline with different models, each one will be cross validated to review the performance of the model.\n\nA feature called 'avg_time_per_project' is added to determine the average time each employee spends on a project.","8bc51e1a":"### Boxplot of Numerical Variables\n\nWe review the distribution of scaled numerical data through a boxplot for each variable. The first 3 functions are used to obtain automatically the numerical\/categorical columns of our data, they are used throught the notebook, therefore they are defined here.","8c5a6565":"### Underrepresented Features\n\nNow we determine underrepresented features, meaning those that in more than 97% of the records are composed of a single value.","79737a54":"## Receiver Operating Characteristic (ROC) \/ Area Under the Curve \nTo review the performance of the model, accuracy is not enough, therefore we plot the ROC of the model on the holdout data and print a classification report.","c38b42c5":"### Defining Holdout Set for Validation\n\n80% of the data will be used to train our model, while the remaining data will be used later on to validate the accuracy of our model.","00140477":"## Note:\nConsider that in reality, you will have a single pipeline cell with different steps that you comment\/uncomment to see the performance of the transformers in the modeling. Since Kaggle must run the entire notebook when commiting it, these changes are done separately in different cells.","4d813925":"As we can see, there are only a few outliers in the time spent in company, so outlier treatment does not seem necessary.","c412d251":"### Binning and Polynomials\nNow we try adding binning and polynomial features to our pipeline and see how it performs.","86944af4":"### SMOTEENN\nFor the class unbalance that is present in the data, we combine over and under-sampling techniques using SMOTE and Edited Nearest Neighbours (SMOTEENN) to our pipeline and see how it performs.","20f935fe":"### Data Correlation\n\nNow we analyze correlation in the data for both numerical and categorical columns and plot them, using a threshold of 70%.\n\nFor the numerical features we use Spearman correlation and for the categorical ones we use Cram\u00e9r's V.","ac1cf2a0":"### Principal Component Analysis (PCA)\n\nWe plot PCA component variance to define the number of components we wish to consider in the pipeline.","727e145b":"### Top Pipelines per Model\n\nHere we show the top pipelines per model.","7d58262c":"### Data types\n\nWe review the data types for each column.","88af2992":"# Employee Attrition: Basis to Create ML-Helper Lib\n\nAnalyzing a dataset from [HR Analytics](https:\/\/www.kaggle.com\/lnvardanyan\/hr-analytics) which contains employee information of a given company, we will hash out a library that can help us speed up and structure future machine learning projects.\n\n### Objective\n\nGiven the following variables:\n\n* satisfaction_level: The satisfaction of the employee\n* last_evaluation: How long ago the employee had his last evaluation\n* number_project: The amount of projects the employee has been involved in \n* average_montly_hours: The average amount of hours the employee works each month\n* time_spend_company: The amount of years the employee has worked there\n* Work_accident: Boolean representing if the employee has been involved in an accident\n* left: Our target variable, determines if the employee left the company or not\n* promotion_last_5years: Boolean on whether the employee was promoted in the last 5 years or not\n* sales: The name of the department the employee works in\n* salary: The salary of the employee (can be low, medium or high)\n\nWe want to build a classification model that can determine which employee will likely leave the company in order to make the necessary changes to reduce employee attrition. We will use 80% of the data for training and the remaining 20% for validation of our modeling.\n\n### Outline\n\nWe separate the project in 3 steps:\n\nData Loading and Exploratory Data Analysis: Load the data and analyze it to obtain an accurate picture of it, its features, its values (and whether they are incomplete or wrong), its data types among others. Also, the creation of different types of plots in order to help us understand the data and make the model creation easier.\n\nFeature Engineering \/ Modeling and Pipeline: Once we have the data, we create some features and then the modeling stage begins, making use of different models (and ensembles) and a strong pipeline with different transformers, we will hopefully produce a model that fits our expectations of performance. Once we have that model, a process of tuning it to the training data would be performed.\n\nResults and Conclusions: Finally, with our tuned model, we  predict against the test set we decided to separate initially, then we review those results against their actual values to determine the performance of the model, and finally, outlining our conclusions.\n\n### Helpers\n\nAs mentioned, this notebook contains many functions that help speed up the machine learning process and provide a formal structure to it. **These helpers are the basis for my package ML-Helper** and they can be used in your own projects by downloading the package at [Pypi](https:\/\/pypi.org\/project\/ml-helper\/) ```pip install ml-helper```.\n\nIf you wish to see a working example using these helpers through the package, please see my [kernel on time series regression](https:\/\/www.kaggle.com\/akoury\/bike-sharing-in-washington-d-c-using-ml-helper)","98881d5d":"# Results\nWe evaluate the final model with the holdout, obtaining the definitive score of the model.","681d062b":"## Scores\n\nHere you can see all of the scores for the different models throughout the entire cross validation process for each pipeline, in certain cases errors can happen (for example when a certain fold contains a sparse matrix), therefore you may see errors marked as such in the score.","114f5a0d":"## Exploratory Data Analysis\n\nHere we will perform all of the necessary data analysis, with different plots that will help us understand the data and therefore, create a better model.\n\nWe must specify that all of this analysis is performed only on the training data, so that we do not incur in any sort of bias when modeling.\n\nWe begin by plotting pairwise relationships between variables, as well as the distribution for each column in the diagonal.","6ac3475c":"We also drop some features, like the invariant, correlated and underrepresented ones.","9cde2c0e":"## Stacked Model\nFinally, we create a stacked model using the top 2 models obtained during the modeling phase and obtain the holdout results.","52eb9bf7":"### Missing Data\n\nWe check if there is any missing data.","bfdfefee":"### Setting Key Values\n\nThe following values are used throught the code, this cell gives a central source where they can be managed.","eba89e8c":"### Data Loading\n\nHere we load the necessary data, print its first rows and describe its contents.","f26e8509":"### Converting columns to their true categorical type\nNow we convert the data types of numerical columns that are actually categorical.","5b26ed64":"## Randomized Grid Search\n\nOnce we have a list of models, we perform a cross validated, randomized grid search on the best performing one to define the final model.","f3bd606d":"### Coefficient of Variation\n\nThe coefficient of variation is a dimensionless meassure of dispersion in data, the lower the value the less dispersion a feature has. We will select columns that have a variance of less than 0.05 since they would probably perform poorly.","a9ae599d":"### Feature Importance\n\nHere we plot feature importance using a random forest in order to get a sense of which features have the most importance.","bf65223b":"And finally we put them all together and we will try this pipeline with 3 different models, a simple logistic regression, a random forest and an extra tree classifier.","de400b2a":"### PCA\nWe try doing Principal Component Analysis and see how it performs.","01b1f389":"Since 0 is employees that stay and 1 is employees that leave, a rebalancing should be tried since there is a very big difference in the number of values for each option.","774521fc":"# Conclusions\nThe classification report obtained from our stacked model shows its precision (how often the predictions are correct) and the recall (how many of the total observations in the set are correctly classified), also f1-score (combination of both). The weighted average for all of them is near perfect, which means that it can classify which employees will leave the company with great efficacy.\n\nAs it was seen in the feature importance step, the most important features in determining employee attrition are their satisfaction level, the number of projects they had, the time spent in the company, their average monthly hours and the score on their last evaluation.\n\nThis information is extremely useful to the company and can be used to help them retain their talent and reduce financial losses, first by knowing that these are the factors that they must pay the most attention to, and second, because for each employee, they can obtain an accurate estimation on whether they will leave or not and take the necessary measures to prevent it.\n\nFinally, from this analysis we obtained a set of functions and steps that can greatly speed up and structure our future machine learning endeavors, we hope that you find them useful.","d766a04f":"### Check target variable balance\nWe review the distribution of values in the target variable.","b1feec98":"Now we one hot encode categorical features.","ace13ff5":"### Best Parameters for the Model"}}