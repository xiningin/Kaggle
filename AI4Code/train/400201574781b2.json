{"cell_type":{"13c8564c":"code","2d6c7df7":"code","a799c094":"code","69c5de5e":"code","e8256f42":"code","6fa3747b":"code","a4f33c08":"code","73c7c61e":"code","d26fa07d":"code","208b5b85":"code","b89d43ad":"code","42aae648":"code","8ae12864":"code","410d5742":"code","e21c649c":"code","e8928803":"code","5bc4fbb0":"code","7c053ed9":"code","d3c62b3f":"code","1a5d1ee9":"code","1d71e463":"code","3f12d686":"code","4bcd7344":"markdown","533c790b":"markdown","55376c46":"markdown","86426482":"markdown","3ebd6711":"markdown","ef4837b1":"markdown","315f1ba9":"markdown","e4d02fac":"markdown","2bdcba52":"markdown"},"source":{"13c8564c":"import numpy as np\nimport regex as re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics\nimport math\nimport os\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tokenizers\nfrom transformers import RobertaTokenizer, TFRobertaModel\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2d6c7df7":"# Detect hardware, return appropriate distribution strategy (you can see that it is pretty easy to set up).\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set (always set in Kaggle)\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)","a799c094":"MODEL_NAME = 'roberta-base'\nMAX_LEN = 256\nARTIFACTS_PATH = '..\/artifacts\/'\n\nBATCH_SIZE = 8 * strategy.num_replicas_in_sync\nEPOCHS = 3\n\nif not os.path.exists(ARTIFACTS_PATH):\n    os.makedirs(ARTIFACTS_PATH)","69c5de5e":"df = pd.read_csv('\/kaggle\/input\/bbc-articles-cleaned\/tfidf_dataset.csv')\ndf.head()","e8256f42":"X_data = df[['text']].to_numpy().reshape(-1)\ny_data = df[['category']].to_numpy().reshape(-1)","6fa3747b":"categories = df[['category']].values.reshape(-1)\n\ncounter_categories = Counter(categories)\ncategory_names = counter_categories.keys()\ncategory_values = counter_categories.values()\n\ny_pos = np.arange(len(category_names))\n\nplt.figure(1, figsize=(10, 5))\nplt.bar(y_pos, category_values, align='center', alpha=0.5)\nplt.xticks(y_pos, category_names)\nplt.ylabel('Number of texts')\nplt.title('Distribution of texts per category')\nplt.gca().yaxis.grid(True)\nplt.show()\n\nprint(counter_categories)","a4f33c08":"def calculate_stats(df, split_char=' '):\n    categories = df['category'].unique()\n    \n    all_lengths = []\n    per_category = {\n        'lengths': {c:[] for c in categories},\n        'mean': {c:0 for c in categories},\n        'stdev': {c:0 for c in categories}\n    }\n\n    for index, row in df.iterrows():\n        text = row['text']\n        text = re.sub(r\"\\s+\", ' ', text) # Normalize\n        text = text.split(split_char)\n        l = len(text)\n        \n        category = row['category']\n        \n        all_lengths.append(l)\n        per_category['lengths'][category].append(l)\n    \n    for c in categories:\n        per_category['mean'][c] = statistics.mean(per_category['lengths'][c])\n        per_category['stdev'][c] = statistics.stdev(per_category['lengths'][c])\n    \n    global_stats = {\n        'mean': statistics.mean(all_lengths),\n        'stdev': statistics.stdev(all_lengths),\n        'lengths': all_lengths\n    }\n    \n    return {\n        'global': global_stats,\n        'per_category': pd.DataFrame(per_category)\n    }\n\n\ndef display_lengths_histograms(df_stats, n_cols=3):\n    categories = df['category'].unique()\n    n_rows = math.ceil(len(categories) \/ n_cols)\n    \n    plt.figure(figsize=(15, 8))\n    plt.suptitle('Distribution of lengths')\n    \n    # Subplot of all lengths\n    plt.subplot(n_rows, n_cols, 1)\n    plt.title('All categories')\n    lengths = df_stats['global']['lengths']\n    plt.hist(lengths, color='r')\n\n    # Subplot of each category\n    index_subplot = 2\n    for c in categories:\n        plt.subplot(n_rows, n_cols, index_subplot)\n        plt.title('Category: %s' % c)\n        \n        lengths = df_stats['per_category']['lengths'][c]\n        plt.hist(lengths, color='b')\n\n        index_subplot += 1\n\n    plt.show()","73c7c61e":"df_stats = calculate_stats(df)\ndf_stats['per_category']","d26fa07d":"display_lengths_histograms(df_stats)","208b5b85":"n_texts = len(X_data)\nprint('Texts in dataset: %d' % n_texts)\n\ncategories = df['category'].unique()\nn_categories = len(categories)\nprint('Number of categories: %d' % n_categories)\n\nprint('Done!')","b89d43ad":"def roberta_encode(texts, tokenizer):\n    ct = len(texts)\n    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n\n    for k, text in enumerate(texts):\n        # Tokenize\n        tok_text = tokenizer.tokenize(text)\n        \n        # Truncate and convert tokens to numerical IDs\n        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n        \n        input_length = len(enc_text) + 2\n        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n        \n        # Add tokens [CLS] and [SEP] at the beginning and the end\n        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n        \n        # Set to 1s in the attention input\n        attention_mask[k,:input_length] = 1\n\n    return {\n        'input_word_ids': input_ids,\n        'input_mask': attention_mask,\n        'input_type_ids': token_type_ids\n    }","42aae648":"# Transform categories into numbers\ncategory_to_id = {}\ncategory_to_name = {}\n\nfor index, c in enumerate(y_data):\n    if c in category_to_id:\n        category_id = category_to_id[c]\n    else:\n        category_id = len(category_to_id)\n        category_to_id[c] = category_id\n        category_to_name[category_id] = c\n    \n    y_data[index] = category_id\n\n# Display dictionary\ncategory_to_name","8ae12864":"# Split into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=777) # random_state to reproduce results","410d5742":"# Import tokenizer from HuggingFace\ntokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)","e21c649c":"X_train = roberta_encode(X_train, tokenizer)\nX_test = roberta_encode(X_test, tokenizer)\n\ny_train = np.asarray(y_train, dtype='int32')\ny_test = np.asarray(y_test, dtype='int32')","e8928803":"def build_model(n_categories):\n    with strategy.scope():\n        input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n        input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n        input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n\n        # Import RoBERTa model from HuggingFace\n        roberta_model = TFRobertaModel.from_pretrained(MODEL_NAME)\n        x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n\n        # Huggingface transformers have multiple outputs, embeddings are the first one,\n        # so let's slice out the first position\n        x = x[0]\n\n        x = tf.keras.layers.Dropout(0.1)(x)\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(256, activation='relu')(x)\n        x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n\n        model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\n\n        return model","5bc4fbb0":"with strategy.scope():\n    model = build_model(n_categories)\n    model.summary()","7c053ed9":"with strategy.scope():\n    print('Training...')\n    history = model.fit(X_train,\n                        y_train,\n                        epochs=EPOCHS,\n                        batch_size=BATCH_SIZE,\n                        verbose=1,\n                        validation_data=(X_test, y_test))","d3c62b3f":"# This plot will look much better if we train models with more epochs, but anyway here is\nplt.figure(figsize=(10, 10))\nplt.title('Accuracy')\n\nxaxis = np.arange(len(history.history['accuracy']))\nplt.plot(xaxis, history.history['accuracy'], label='Train set')\nplt.plot(xaxis, history.history['val_accuracy'], label='Validation set')\nplt.legend()","1a5d1ee9":"def plot_confusion_matrix(X_test, y_test, model):\n    y_pred = model.predict(X_test)\n    y_pred = [np.argmax(i) for i in model.predict(X_test)]\n\n    con_mat = tf.math.confusion_matrix(labels=y_test, predictions=y_pred).numpy()\n\n    con_mat_norm = np.around(con_mat.astype('float') \/ con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n    label_names = list(range(len(con_mat_norm)))\n\n    con_mat_df = pd.DataFrame(con_mat_norm,\n                              index=label_names, \n                              columns=label_names)\n\n    figure = plt.figure(figsize=(10, 10))\n    sns.heatmap(con_mat_df, cmap=plt.cm.Blues, annot=True)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","1d71e463":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1] * 100))","3f12d686":"plot_confusion_matrix(X_test, y_test, model)","4bcd7344":"I am going to import the RoBERTa model from HuggingFace. Note that I must import not only the model but also the tokenizer (since I must use the same vocabulary as the model was trained with).\n\nWe should take into account that RoBERTa's input accepts up-to 512 tokens, thus **we must truncate the tokenized texts**. In my case, I truncate to 256 tokens, but you can put a higher value in the variable `MAX_LEN`.\n\nSee https:\/\/huggingface.co\/roberta-base","533c790b":"## Create RoBERTa model","55376c46":"## Prepare dataset\n\nNote that I have already cleaned the dataset, so the training should be better:\n- Remove punctuation symbols and double white spaces.\n- Lemmatization.\n- Remove stop words (see `spacy.lang.en.stop_words.STOP_WORDS`).\n- TD-IDF\n\nThe reason of this preprocess is that I have used this dataset with other models, such as LSTM. However, I believe (I didn't test it) that RoBERTa could deal with it.","86426482":"# Text Classification with RoBERTa\n\nThe RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu et al. It is based on Google\u2019s BERT model released in 2018: it modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\nSee:\n- RoBERTa's paper: https:\/\/arxiv.org\/pdf\/1907.11692.pdf\n- BERT's paper: https:\/\/arxiv.org\/pdf\/1810.04805.pdf","3ebd6711":"In these histograms and stats, we can see that almost all texts contain $500$ or less words. Also, we can see that the average length is very different depending on the category.\n\nPlease, note that the tokenization process may split words into several parts, so lengths could increase (or decrease too). This is only an orientative result.","ef4837b1":"# Tokenize & encode","315f1ba9":"## Evaluation\n\nIn a confusion matrix, we can see how many categories are classified c","e4d02fac":"## Train model\n\nThis is basic training of RoBERTa but, if your dataset is larger, you may use K-Folds in this section. In this notebook, I use K-Folds (use it as inspiration): https:\/\/www.kaggle.com\/dimasmunoz\/clean-english-data-roberta ;)","2bdcba52":"# Dataset analysis\n\nAs you can see in the following plot, the samples **are not balanced**. This could cause problems during the training but, since they are not highly unbalanced, I have left it as it is.\n\nIn other cases, such as fraud detecting where the positive classes are very few compared to the negative ones, we must apply techniques to balance it. For example, we could undersample the biggest category."}}