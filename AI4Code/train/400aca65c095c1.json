{"cell_type":{"186dc46c":"code","44b9f689":"code","95b542db":"code","6d3e117a":"code","a7e6d4d9":"code","02e540f6":"code","014846eb":"code","5e4888ba":"code","8948681f":"code","7484a9e1":"code","ae1cd028":"code","3db4a669":"code","5924aa90":"code","24c401f9":"code","d8c918dd":"code","b6dd9964":"code","3cec5dbb":"code","ec8ea384":"code","e607f917":"code","3afc8f12":"code","dc9138e6":"code","6c92e378":"code","d510b984":"code","6c3bf440":"code","7fe137a5":"code","fc7a8c46":"code","1d181184":"code","aad3c283":"code","0870e2c9":"code","385a9487":"code","7ff39416":"code","0adc7e08":"code","e8faca70":"code","258a1203":"code","758c18ec":"code","8b581b37":"markdown","7e4123ae":"markdown","49ea92f7":"markdown","77c7c61d":"markdown","c5cc6d7b":"markdown","a2579601":"markdown","db1364b4":"markdown","43c8dbf3":"markdown","03ff4f46":"markdown","7ad21fea":"markdown","76ef0ce3":"markdown","9c19baa6":"markdown","9349798e":"markdown","87b3928d":"markdown","d88e6f6d":"markdown","21bd8267":"markdown"},"source":{"186dc46c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","44b9f689":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.models import Sequential\nfrom keras.layers.embeddings import Embedding\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Embedding","95b542db":"\n# define documents\ndocs = ['Well done!',\n        'Good work',\n        'Great effort',\n        'nice work',\n        'Excellent!',\n        'Weak',\n        'Poor effort!',\n        'not good',\n        'poor work',\n        'could have done better.',\n        'the King is angry',\n        'eldest Son of the king is the right hier',\n        'the king of chola fell in love with the queen of chera',\n        'the queen looked very beautiful',\n        'king is a strong man',\n        'kings are men',\n        'queens are women',\n        'king and queen lived together happily',\n        'man are good King',\n        'women are good queen as well',\n        'queen are good women',\n       'the queen was sad waiting for the king to return',\n       'king father is dead',\n       'king married the queen']\n\n\n","6d3e117a":"\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0,0,1])\n\n# Count of unique words in the list of documents\nvocab_size = 70\n\n\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)","a7e6d4d9":"# pad documents to a max length of 12 words\n# since maximum length in your list of doc is 12\nmax_length = 12\n\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n# print(padded_docs)","02e540f6":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length,name='embedding'))\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","014846eb":"# model.compile(optimizer='adam', loss='categorical_crossentropy')\n# model.summary()\n# output_array = model.predict(padded_docs)\n# output_array","5e4888ba":"\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\n","8948681f":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","7484a9e1":"layer = model.get_layer('embedding' )\noutput_embeddings = layer.get_weights()","ae1cd028":"len(output_embeddings[0])","3db4a669":"king = output_embeddings[0][13]\nqueen = output_embeddings[0][30]\nwoman = output_embeddings[0][40]\nman = output_embeddings[0][54]\n\nweak = output_embeddings[0][51]\npoor = output_embeddings[0][53]\ngood = output_embeddings[0][23]","5924aa90":"\nq = (king - man) + woman\nq","24c401f9":"queen","d8c918dd":"from sklearn.metrics.pairwise import cosine_similarity","b6dd9964":"cosine_similarity(q.reshape(1,8),queen.reshape(1,8))","3cec5dbb":"cosine_similarity(weak.reshape(1,8),good.reshape(1,8))","ec8ea384":"cosine_similarity(weak.reshape(1,8),poor.reshape(1,8))","e607f917":"# # vectors\n# import numpy as np\n# a = q\n# b = queen\n \n# # manually compute cosine similarity\n# dot = np.dot(a, b)\n# norma = np.linalg.norm(a)\n# normb = np.linalg.norm(b)\n# cos = dot \/ (norma * normb)\n# cos","3afc8f12":"from keras.preprocessing.text import Tokenizer\n\n# define documents\ndocs = ['Well done!',\n        'Good work',\n        'Great effort',\n        'nice work',\n        'Excellent!',\n        'Weak',\n        'Poor effort!',\n        'not good',\n        'poor work',\n        'Could have done better.']\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n","dc9138e6":"# prepare tokenizer\nt = Tokenizer()\nt.fit_on_texts(docs)\nvocab_size = len(t.word_index) + 1","6c92e378":"# integer encode the documents\nencoded_docs = t.texts_to_sequences(docs)\nprint(encoded_docs)","d510b984":"\n# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)","6c3bf440":"\nimport csv\nimport os\nimport pandas as pd\n\nwords_vec = pd.read_csv('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)","7fe137a5":"words_vec.shape","fc7a8c46":"\n# create a weight matrix for words in training docs\nembedding_matrix = zeros((vocab_size, 100))\nfor word, i in t.word_index.items():\n    embedding_vector = words_vec[words_vec.index == word].iloc[:,:].values\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","1d181184":"king = words_vec[words_vec.index == 'king'].iloc[:,:].values\nqueen = words_vec[words_vec.index == 'queen'].iloc[:,:].values\nman = words_vec[words_vec.index == 'man'].iloc[:,:].values\nwoman = words_vec[words_vec.index == 'woman'].iloc[:,:].values\ngood = words_vec[words_vec.index == 'good'].iloc[:,:].values\nbad = words_vec[words_vec.index == 'bad'].iloc[:,:].values","aad3c283":"q = (king - man) + woman\n","0870e2c9":"cosine_similarity(queen.reshape(1,100),q.reshape(1,100))","385a9487":"# define model\nmodel = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\nmodel.add(e)\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","7ff39416":"\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n# summarize the model\nprint(model.summary())\n# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","0adc7e08":"from sklearn.decomposition import PCA\nwords = ['king','queen','man','woman','good','bad','cricket','ball','chess','water','rain','island','ocean','red','color','bright']\npca = PCA(n_components=2)\nresult = pca.fit_transform(words_vec.loc[words])","e8faca70":"from matplotlib import pyplot \n# create a scatter plot of the projection\npyplot.scatter(result[:, 0], result[:, 1])\n\nfor i, word in enumerate(words):\n    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\npyplot.show()","258a1203":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=3)\nresult = pca.fit_transform(words_vec.loc[words])\n","758c18ec":"from mpl_toolkits.mplot3d import Axes3D\nfig = pyplot.figure(figsize=(12, 8))\nax = fig.add_subplot(111, projection='3d')\nfor index in range(1,len(result)):\n    x = result[index][0]\n    y = result[index][1]\n    z = result[index][2]\n    ax.scatter(x, y, z, color='b')\n    ax.text(x, y, z, '%s' % (words[index]), size=12, zorder=1, color='k')\npyplot.draw()","8b581b37":"### Encoding the vocabulary","7e4123ae":"### Fitting the data to the model","49ea92f7":"### getting the vector space for selected  words from the glove vectors","77c7c61d":"### Getting the embeded vector for selected words","c5cc6d7b":"### Model network creation","a2579601":"### Using GLOVE vectors as seed at embedding layer","db1364b4":"### Creating tokens for each unique words","43c8dbf3":"### Every input vector has to be of same shape, hence take the maximum length","03ff4f46":"### Encoding the text using the above tokens for respective words","7ad21fea":"### Reading the glove embedded vectors of 6 Billion documents having 100 dimensions each","76ef0ce3":"### Testing semantic meaning on glove vectors","9c19baa6":"### creating model network","9349798e":"### Getting the trained weight from the embedding layer","87b3928d":"### Compile the model","d88e6f6d":"### Creating a list of documents.","21bd8267":"### Filtering the vectors for current documents and populating into a matrix"}}