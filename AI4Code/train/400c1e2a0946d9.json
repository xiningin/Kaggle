{"cell_type":{"29cc3266":"code","0bad1788":"code","c9dce81a":"code","f8c402cb":"code","b2780106":"code","65ed56c9":"code","c6038852":"code","e8b2ebe4":"code","94d28d81":"code","b192ad75":"code","844ffda9":"code","a3ac163a":"code","778623bd":"code","bdb4413d":"code","861522d4":"code","cbd432fc":"code","c1dd103d":"code","2957043b":"code","f0d2d8b8":"code","9b2b7551":"code","206d0b82":"code","5f77b8ed":"code","e620e301":"code","7751eb44":"code","95f20f7c":"code","85220934":"code","f1df35c8":"code","5a96f0cd":"code","ff0ef626":"code","27408a9d":"code","4ef088b8":"code","5ffc635a":"code","b5b97a50":"code","1f861e6d":"code","4452b60f":"code","a5da46ff":"code","5fc422af":"code","ec5415aa":"code","839a0444":"code","dd6a09fc":"code","d813ade3":"code","b0f0899c":"code","83cc66d5":"code","f4211a89":"code","b5f1cb87":"code","35724fba":"code","c60232e8":"code","d5d2cbd0":"code","7b19af5f":"code","19321e67":"code","38177982":"code","9d2985ad":"code","a979f635":"code","93c54966":"code","065f5cac":"code","8a7ea178":"code","b4ce6893":"code","352548ab":"code","9e6fd503":"code","c82d6d22":"code","e4ba3989":"code","a7e43ea5":"code","e6a49f8a":"code","640bd192":"code","19bd7018":"code","f2c13d56":"markdown","146e1544":"markdown","ebbbd176":"markdown","90ce7a49":"markdown","43518c53":"markdown","9bebed6b":"markdown","4b7f20c5":"markdown","39068565":"markdown","5c312125":"markdown","166ee24f":"markdown","515657a3":"markdown","d3a60157":"markdown","21cc5773":"markdown","277d88c9":"markdown","ac6eb0fd":"markdown","7fa60b81":"markdown","40bdd363":"markdown","c7a50575":"markdown","6387254d":"markdown","8619877d":"markdown","012e5365":"markdown","dd8c41c2":"markdown","81792a28":"markdown","f7c0a64a":"markdown"},"source":{"29cc3266":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0bad1788":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom xgboost import XGBClassifier, plot_importance as plot_importance_xgb\n\n# Deep Learning\n\nimport torch \nimport torchvision as tv ","c9dce81a":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\ntrain_df","f8c402cb":"train_df.columns","b2780106":"train_df.info()\nprint('_'*40)\ntest_df.info()","65ed56c9":"train_df.describe(include=['O'])","c6038852":"train_df.Sex.value_counts()","e8b2ebe4":"test_df.Sex.value_counts()","94d28d81":"cat_cols = ['Pclass','Parch','Sex','SibSp','Cabin','Embarked']\nfor i in cat_cols:\n    print(train_df[[i, 'Survived']].groupby([i], as_index=False).mean().sort_values(by='Survived', ascending=False))\n    print('-'*50)\n    print('\\n')\n    \n    \n# Pclass seems like an important feature to predict survival as PClass 1 means more survived!!\n# Female survived more!!\n# SibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).\n","b192ad75":"# check Reltn btw cont and Class\n\nfor i in ['Age','Fare']:\n    g = sns.FacetGrid(train_df, col='Survived')\n    g.map(plt.hist, i , bins=20)","844ffda9":"sns.pairplot(train_df)","a3ac163a":"fig, ax = plt.subplots(figsize=(10, 10))\ncorr = train_df.corr()\nsns.heatmap(corr, linewidths=.5, cbar_kws={\"shrink\": .5},annot_kws={'fontsize':12 },annot=True,)","778623bd":"fig, ax = plt.subplots(figsize=(12, 7))\n\nsns.barplot(x = train_df['Pclass'],y = train_df['Fare'])\nplt.title('# P1 vs Fare', size=20)\nplt.xlabel('Pclass', size=15)\nplt.ylabel('Fare', size=15)\nplt.xticks(size=15)\nplt.yticks(size=15)\nplt.show()\n\n# a Clear -ve correlation... but does this correlation matters? because Pclass is a category not numeric!!","bdb4413d":"fig, ax = plt.subplots(figsize=(17, 7))\nsns.distplot(train_df['Age'].dropna())","861522d4":"fig, ax = plt.subplots(figsize=(17, 7))\nsns.distplot(train_df['Fare'].dropna())","cbd432fc":"import pandas_profiling # library for automatic EDA\nreport = pandas_profiling.ProfileReport(train_df)","c1dd103d":"display(report)","2957043b":"!pip install autoviz \n\nfrom autoviz.AutoViz_Class import AutoViz_Class\nfrom IPython.display import display # display from IPython.display\n\nAV = AutoViz_Class()","f0d2d8b8":"# Let's now visualize the plots generated by AutoViz.\nreport_2 = AV.AutoViz(\"\/kaggle\/input\/titanic\/train.csv\")","9b2b7551":"print(\"{} \\nNan values found\".format(train_df.isna().sum()))\nprint(\"{} \\nNan values found\".format(test_df.isna().sum()))\n\n# train_df.dropna(inplace=True) # drop na","206d0b82":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\ntrain_df","5f77b8ed":"train_df = train_df.drop(['PassengerId','Cabin','Ticket'], axis=1)\ntest_df = test_df.drop(['PassengerId','Cabin','Ticket'], axis=1)\n\n#complete missing age with median\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)\ntest_df['Age'].fillna(test_df['Age'].median(), inplace = True)\n\n#complete embarked with mode\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)\n\ntest_df['Fare'].fillna(test_df['Fare'].mode()[0], inplace = True)\n\ntrain_df","e620e301":"print(\"{} \\nNan values found\".format(train_df.isna().sum()))\nprint(\"{} \\nNan values found\".format(test_df.isna().sum()))\n\n# train_df.dropna(inplace=True) # drop na","7751eb44":"from scipy.stats import pearsonr\n    \n\n# get if a variable is continous, \n# do some sanity check so onehotencoders are not affected\ndef get_continous(df):\n    cont_cols = []\n    for col in df.columns:\n        try:\n            df[col] = list(map(int,df[col])) # convert to int, eg '123' --> 123 and 'abc'--> 'ValueError'. Win win!!\n            if set(df[col].unique())!={0,1}: # skip onehot values\n                cont_cols.append(col)\n        except ValueError:\n            pass  \n    return cont_cols\n\n# we can use df.corr(method='pearson') to get pearson relation but this compromise the flexibility\n# and in order to remove we had to pass though data again, \n# rather we calculate corr column by column and remove if feels necessary, flexible and faster\n\ndef find_pearson_cor(df):\n    cont_cols = get_continous(df) # get cont values to reduce number of col search\n    if cont_cols is not None:\n        cnt = 0\n        should_drop=[]\n        for idx,i in enumerate(cont_cols):\n            for j in cont_cols[idx+1:]: # go through columns and find corr\n                corr, _ = pearsonr(df[i],df[j]) # use scipy\n                print(i,\"has corr value =\",corr,\"with\",j)\n                if corr > 0.85: \n                    cnt+=1\n                    random_drop = random.randint(0,1) # randomly select 1 column with high corr value \n                    if random_drop==1:\n                        should_drop.append(j)\n#                         df.drop(j,axis=1,inplace = True) # drop it\n                        print(j,\"Should be dropped with corr = \",corr)\n                    else:\n                        should_drop.append(i)\n#                         df.drop(i,axis=1,inplace = True) # drop it\n                        print(i,\"Should be dropped with corr = \",corr)\n        if cnt==0:\n            print(\"No columns are highly Correlated!!\")\n    else:\n        print(\"no continous columns\")\n\n    return should_drop # return df\n","95f20f7c":"find_pearson_cor(train_df)","85220934":"# get if a variable is continous, \n# do some sanity check so onehotencoders are not affected\ndef get_continous(df):\n    cont_cols = []\n    for col in df.columns:\n        try:\n            df[col] = list(map(int,df[col])) # convert to int, eg '123' --> 123 and 'abc'--> 'ValueError'. Win win!!\n            if set(df[col].unique())!={0,1}: # skip onehot values\n                cont_cols.append(col)\n        except ValueError:\n            pass\n  \n    return cont_cols\n\n# 2 common ways to detect outliers, we will use IQR method cz I feel it works better in practical scenerios and is more flexible.\ndef detect_outliers(df,OUT_THRES=7):\n    df_len = len(df)\n    cont_cols = get_continous(df) # get cont values\n\n    print(\"Variable with continues values\",cont_cols)\n  \n    if cont_cols is not None:\n        for i in cont_cols:\n            # find outliers for every column thatis continous, this is why IQR is better than z as ir gives more flexibility\n            Q1 = df[i].quantile(0.20)\n            Q3 = df[i].quantile(0.80)\n            IQR = Q3 - Q1\n            # get bounds\n            Lower_Bound = Q1-1.5*IQR\n            Upper_Bound = Q3+1.5*IQR\n\n            # get actual outlier values\n            df_u = df[i]<Upper_Bound\n            df_l = df[i]>Lower_Bound\n\n            # Sanity check\n            # If number of outliers is less than the given threshold, only remove then, otherwise let it be, it may be occuring naturally\n            if df_len-sum(df_u)<=OUT_THRES:\n                df = df[df_u]\n\n            if df_len-sum(df_l)<=OUT_THRES:\n                df = df[df_l]\n\n            print(\"For {} removed {} outliers\".format(i,df_len-len(df)))\n            df_len = len(df)\n    else:\n        print('No Continous variables found')\n\n    return df","f1df35c8":"print(\"Final df\\n\",detect_outliers(train_df))","5a96f0cd":"# Use SimpleImputer?\n\n# from sklearn.impute import SimpleImputer\n\n# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# imputer.fit(X[:, 3:4])\n# X[:, 3:4] = imputer.transform(X[:, 3:4])","ff0ef626":"train_df['Sex'] = train_df['Sex'].map({'male':0,'female':1})\ntest_df['Sex'] = test_df['Sex'].map({'male':0,'female':1})\n\ntrain_df","27408a9d":"# Encoding the Dependent Variable if needed\n\n# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# y = le.fit_transform(y)\n# print(y)","4ef088b8":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntest_df['AgeBand'] = pd.cut(test_df['Age'], 5)\n\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","5ffc635a":"train_df['FareBin'] = pd.qcut(train_df['Fare'], 4)\ntest_df['FareBin'] = pd.qcut(test_df['Fare'], 4)\n\ntrain_df[['FareBin', 'Survived']].groupby(['FareBin'], as_index=False).mean().sort_values(by='FareBin', ascending=True)","b5b97a50":"for dataset in [train_df,test_df]:\n\n    \"\"\"\n    you could divide Age into bins like this based on the above observed formula, \n    but  using LabelEncoder is easier and works better thus, I'll follow that\n    \"\"\"\n#     dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n#     dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n#     dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n#     dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n#     dataset.loc[ dataset['Age'] > 64, 'Age']\n    \n    \n    # Creating a categorical variable for Family Sizes\n    dataset['FamilySize'] = ''\n    dataset['FamilySize'].loc[(dataset['SibSp'] <= 2)] = 0\n    dataset['FamilySize'].loc[(dataset['SibSp'] > 2) & (dataset['SibSp'] <= 5 )] = 1\n    dataset['FamilySize'].loc[(dataset['SibSp'] > 5)] = 2 \n\n\n    # Creating a categorical variable to tell if the passenger is alone\n    dataset['IsAlone'] = ''\n    dataset['IsAlone'].loc[((dataset['SibSp'] + dataset['Parch']) > 0)] = 1\n    dataset['IsAlone'].loc[((dataset['SibSp'] + dataset['Parch']) == 0)] = 0\n    \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    # take only top 10 titles\n    title_names = (dataset['Title'].value_counts() < 10) #this will create a true false series with title name as index\n\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    print(dataset['Title'].value_counts())\n    print(\"-\"*10)\n    \n\ntrain_df.head()","1f861e6d":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nlabel = LabelEncoder()\n\nfor dataset in [train_df,test_df]:\n    dataset['AgeBand_Code'] = label.fit_transform(dataset['AgeBand'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \n    #Drop Columns\n    dataset.drop(['Name','AgeBand','FareBin'], axis=1,inplace=True)\n\n    \ntrain_df","4452b60f":"#define x and y variables for dummy features original\ntrain_dummy = pd.get_dummies(train_df)\ntest_dummy = pd.get_dummies(test_df)\n\ntrain_dummy","a5da46ff":"y = train_dummy['Survived']\ntrain_dummy.drop(['IsAlone_1','FamilySize_2','Survived'],axis=1,inplace=True)\ntest_dummy.drop(['IsAlone_1','FamilySize_2'],axis=1,inplace=True)","5fc422af":"\"\"\"\nYou could do OneHotEncoding with Column Transfer as Well!!\n\"\"\"\n\n# OneHotEncode\n\n# # Encoding categorical data\n# # Encoding the Independent Variable\n# from sklearn.compose import ColumnTransformer\n# from sklearn.preprocessing import OneHotEncoder\n\n# ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0,6])], remainder='passthrough')\n# X_encoded = np.array(ct.fit_transform(X_train))\n# X_test_encoded =  np.array(ct.transform(X_test))\n# X_encoded","ec5415aa":"train_dummy","839a0444":"from sklearn.feature_selection import SelectKBest, chi2\n\nbest_feature = SelectKBest(score_func= chi2, k = 'all')\nbest_feature = best_feature.fit(train_dummy.values , y)\n\ncol_scores = pd.DataFrame(best_feature.scores_)\ncol_names = pd.DataFrame(train_dummy.columns)\n\nfeature_score = pd.concat([col_names, col_scores], axis=1)\nfeature_score.columns = ['attribute', 'score']\nfeature_score","dd6a09fc":"# Did you Notice Something?? ","d813ade3":"fig, ax = plt.subplots(figsize=(17, 15))\ncorr = train_dummy.corr()\nsns.heatmap(corr, linewidths=.5, cbar_kws={\"shrink\": .5},annot_kws={'fontsize':12 },annot=True,)","b0f0899c":"find_pearson_cor(train_dummy)","83cc66d5":"clean_data = pd.concat([train_dummy,pd.DataFrame({'Survived':y})],axis=1)\nclean_data","f4211a89":"# In here we keep only Original Values encoded\ntrain_calc = ['Pclass','Sex','SibSp', 'Parch', 'Age', 'Fare', 'Embarked_C', 'Embarked_Q','Embarked_S', 'Title_Master','Title_Misc','Title_Miss', 'Title_Mr','Title_Mrs'] \n\n\n# In this We keep all the featured values and remove correlated values as seen above in pearson corr heatmap\ntrain_feat = ['Sex','Pclass', 'Embarked_C', 'Embarked_Q','Embarked_S','FamilySize_0','FamilySize_1','IsAlone_0', 'Title_Master','Title_Misc','Title_Miss', 'Title_Mr','Title_Mrs', 'FamilySize_0','FamilySize_1', 'AgeBand_Code', 'FareBin_Code']\n#define x variables for original w\/bin features to remove continuous variables\n\ntrain_calc_df = clean_data[train_calc+['Survived']]\ntest_calc_df = test_dummy[train_calc]\ntrain_calc_df","b5f1cb87":"clean_feat_df = clean_data[train_feat+['Survived']]\nclean_feat_df","35724fba":"clean_feat_df.info()","c60232e8":"X_final = train_calc_df.drop(['Survived'],axis=1).values # for original features\n\nX_final_feat = clean_feat_df.drop(['Survived'],axis=1).values # for new features\n\ntarget = train_calc_df['Survived'].values","d5d2cbd0":"X_final.shape","7b19af5f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_final = sc.fit_transform(X_final)\nprint(X_final.shape,X_final)\n\nsc_feat = StandardScaler()\nX_final_feat = sc_feat.fit_transform(X_final_feat)\nprint(X_final_feat.shape,X_final_feat)","19321e67":"X_train, X_test, y_train, y_test = train_test_split(X_final, target, test_size=0.10)\nprint(X_train.shape, X_test.shape)","38177982":"X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X_final_feat, target, test_size=0.10)\nprint(X_train_feat.shape, X_test_feat.shape)","9d2985ad":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_final, target)\nacc_log = round(logreg.score(X_final, target) * 100, 2)\nacc_log","a979f635":"coeff_df = pd.DataFrame(train_calc_df.columns.delete(-1))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","93c54966":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_final, target)\n\nacc_decision_tree = round(decision_tree.score(X_final, target) * 100, 2)\nacc_decision_tree","065f5cac":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_final_feat, target)\nacc_log = round(logreg.score(X_final_feat, target) * 100, 2)\nacc_log","8a7ea178":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_final_feat, target)\n\nacc_decision_tree = round(decision_tree.score(X_final_feat, target) * 100, 2)\nacc_decision_tree","b4ce6893":"# Let's create a clean predict method \ndef predict_custom(classifier,x_test,y_test):\n    \n    predictions = classifier.predict(x_test)\n    \n    # get f1score,prec,recall,roc_auc, and accuracy\n    f1 = f1_score(y_test, predictions)\n    precision = precision_score(y_test, predictions)\n    recall = recall_score(y_test, predictions)\n    roc_auc = roc_auc_score(y_test, predictions)\n    accuracy = accuracy_score(y_test, predictions)\n    \n    result_df = pd.DataFrame({'f1':[f1],'Precision':[precision],'Recall':[recall],'roc_auc_score':[roc_auc],'Accuracy':[accuracy]})\n    \n    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n    print(\"F1 score, Precision, Recall, ROC_AUC score, Accuracy:\")\n    return result_df","352548ab":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Gradient Boosting\": GradientBoostingClassifier(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"XGB\":XGBClassifier()   \n}","9e6fd503":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    \n    # scoring on final\n    classifier.fit(X_final, target)\n    \n    training_score = cross_val_score(classifier, X_final, target, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n    print('-'*50)\n    print('\\n')\n    \nfor key, classifier in classifiers.items(): \n    # scoring on splitted data!!1\n    print(\"Scoring on\",key)\n    classifier.fit(X_train, y_train)\n    print(predict_custom(classifier,X_test,y_test))\n    print('-'*50)\n    print('\\n')\n","c82d6d22":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    \n    # scoring on final\n    classifier.fit(X_final_feat, target)\n    \n    training_score = cross_val_score(classifier, X_final_feat, target, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n    print('-'*50)\n    print('\\n')\n    \nfor key, classifier in classifiers.items(): \n    # scoring on splitted data!!1\n    print(\"Scoring on\",key)\n    classifier.fit(X_train_feat, y_train_feat)\n    print(predict_custom(classifier,X_test_feat,y_test_feat))\n    print('-'*50)\n    print('\\n')\n","e4ba3989":"# Best models are XGBoost, Logistic,RandomForest,GradientBoosting\n\n# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'],\n                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params,cv=5)\ngrid_log_reg.fit(X_final, target)\nlog_reg = grid_log_reg.best_estimator_\nprint(log_reg)\n\n#XGBoost\nxgboost_params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\ngrid_xgboost = GridSearchCV(XGBClassifier(), xgboost_params,cv=5)\ngrid_xgboost.fit(X_final, target)\nxgboost_clf = grid_xgboost.best_estimator_\nprint(xgboost_clf)\n\n\n# # Gradient Boosting Classifier\n\n# grad_boost_parameters = {\n#     \"loss\":[\"deviance\"],\n#     \"learning_rate\": [0.05, 0.1,  0.5],\n#     \"min_samples_split\": np.linspace(0.5, 12),\n#     \"min_samples_leaf\": np.linspace(0.5, 12),\n#     \"max_depth\":[3,5,8],\n#     \"max_features\":[\"log2\",\"sqrt\"],\n#     \"subsample\":[0.5, 0.8, 1.0],\n#     \"n_estimators\":[10,50,100]\n#     }\n\n# grid_gradBoost = GridSearchCV(GradientBoostingClassifier(), grad_boost_parameters,cv=5)\n# grid_gradBoost.fit(X_final, target)\n# grad_boost = grid_gradBoost.best_estimator_\n# print(grad_boost)\n\n# Random Forest Classifier\nrandF_params = { \n    'n_estimators': [50,100,200],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,6,8],\n    'criterion' :['gini', 'entropy']\n}\n\ngrid_tree = GridSearchCV(RandomForestClassifier(), randF_params,cv=5)\ngrid_tree.fit(X_final, target)\n\n# tree best estimator\nRandF_clf = grid_tree.best_estimator_\nprint(RandF_clf)","a7e43ea5":"log_reg_score = cross_val_score(log_reg, X_final, target, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nxgboost_score = cross_val_score(xgboost_clf, X_final, target, cv=5)\nprint('XGBoost Cross Validation Score', round(xgboost_score.mean() * 100, 2).astype(str) + '%')\n\n# grad_boost_score = cross_val_score(grad_boost, X_final, target, cv=5)\n# print('GradientBoostingClassifier Cross Validation Score', round(grad_boost_score.mean() * 100, 2).astype(str) + '%')\n\nrandF_score = cross_val_score(RandF_clf,X_final, target, cv=5)\nprint('Random Forest Classifier Cross Validation Score', round(randF_score.mean() * 100, 2).astype(str) + '%')","e6a49f8a":"Y_pred = log_reg.predict(test_calc_df.values)\nY_pred","640bd192":"submission = pd.DataFrame({\n        \"PassengerId\": pd.read_csv('..\/input\/titanic\/test.csv')[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n\nsubmission","19bd7018":"submission.to_csv('..\/working\/submission.csv', index=False)","f2c13d56":"## 5 Step Brief RoadMap\n\n1. Understand Columns.\n    Know what a column\/feature is doing in the data. What type it is, how much is null, what is intuitive role of feature? Is it Significant?\n\n\n2. Visualize.\n\n    1. We may want to complete Age feature as it is definitely correlated to survival.\n    2. We may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\n\n3. Complete incomplete Variables or drop or Create new features using Old. Basically Feature Engineering.\n\n    1. Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n    2. Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n    3. PassengerId may be dropped from training dataset as it does not contribute to survival.\n    4. Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n    \n    5. We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n    6. We may want to engineer the Name feature to extract Title as a new feature.\n    7. We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n    8. We may also want to create a Fare range feature if it helps our analysis.\n    \n\n4. Look at correlations btw variables and fix problems like MultiCollinearity.\n\n     We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n\n5. Classifying.\n\n    1. We may also add to our assumptions based on the problem description noted earlier.\n    2. Women (Sex=female) were more likely to have survived.\n    3. Children (Age<?) were more likely to have survived.\n    4. The upper-class passengers (Pclass=1) were more likely to have survived.\n\n\nCross-Validation, Gridsearch CV are really good to select good model in ML, and we'll do that only!!\n","146e1544":"Let's Understand what each feature is doing..\n\n\n**survival**: Survival (0 = No, 1 = Yes)\n\n**pclass**:\tTicket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n\n**sibsp**: # of siblings \/ spouses aboard the Titanic \t\n\n**parch**: \t# of parents \/ children aboard the Titanic \t\n\n**ticket**: Ticket number \t\n\n**cabin**: \tCabin number \t\n\n**embarked**: \tPort of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","ebbbd176":"> Pretty Clear that Decision Tree Shined Like a Charm!! \n**[Tho this is training Accuracy, so it could be overfitting!!]**\n\nAlso, Original Features give better performance on training set as opposed to Engineered ones in this case!!","90ce7a49":"### It is time we make final clean data!!\n### I will use 2 dfs, one with only Original Values and one with Coded Values in Bins ( Age,Fare )","43518c53":"\n\nWhich features are categorical?\n\n    Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n\nWhich features are numerical?\n\n    Continous: Age, Fare. Discrete: SibSp, Parch.\n\n","9bebed6b":"## WAIT!!\n\n\n# Looking for 100 Data Science Interview Questions?\n## I've got you: *https:\/\/www.linkedin.com\/posts\/alaapdhall_day-7-of-100-data-science-interview-questions-activity-6712629560569069568-yQbn*\n\n## If you're Interested in Deep Learning with PyTorch, visit https:\/\/www.aiunquote.com for 100 project in Deep Learning Series!!\n\n---","4b7f20c5":"Let's do Chi Sq test!! \nLet's do a chi sq test and find sifnificance if we have any, I am not yet normalizing the data tho.\n\n[We can use p-value test as well to get significance, if any]","39068565":"## LEt's Check for Outliers!\nWe can us z-test and other techniques but I'll use Box Plot to check for outliers!!","5c312125":"**A very large chi square test statistic means that the data does not fit very well. In other words, there isn't a relationship.**\n\nIn feature selection, we aim to select the features which are highly dependent on the response.\n\nWhen two features are independent, the observed count is close to the expected count, thus we will have smaller Chi-Square value. So high Chi-Square value indicates that the hypothesis of independence is incorrect. In simple words, higher the Chi-Square value the feature is more dependent on the response and it can be selected for model training.\n\nHere we have very high values, which doesn't seem like any variable is good to predict Confirmed case. Let's first look at time series data and see what we can dig out","166ee24f":"# Feature Engineering","515657a3":"# Machine Learning","d3a60157":"**Let's OneHotEncode**","21cc5773":"**We have got hell lot of Information from Pandas Profiling and AutoViz, Let's make some Hypothesis and do Feature Engineering**","277d88c9":"**Most Important thing to do in a model is to get to know your data!!**\n\n*     Understand the Question or problem definition properly.\n    \n*     Acquire the knowledge of training and testing data, make sure you know what a particular column is doing here.\n    \n*     Work to Improve data.. Do Data Wrangling, prepare, and cleanse the data.\n    \n*     Do EDA, or Analyze, identify patterns, and explore the data. [ This can give you some insights!! ]\n    \n*     Model, predict and solve the problem.\n    \n*     Do MEtric Evaluations and Visualize, report, and present the final solution.","ac6eb0fd":"## Let's check pearson Correlation!\nWe remove any value with corr value above 0.85","7fa60b81":"## Lets' do Cross Validation and GridSearchCV","40bdd363":"# Let's Understand more about Columns and also check the correlation to Independent Variable","c7a50575":"# We can use Log_reg or XGBoost for our Submission","6387254d":"\nWhat is the distribution of categorical features?\n\n*     Names are unique across the dataset (count=unique=891)\n*     Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n*     Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n*     Embarked takes three possible values. S port used by most passengers (top=S)\n*     Ticket feature has high ratio (22%) of duplicate values (unique=681).\n","8619877d":"### We will do Visualization and Correlation tests to answer questions like:\n\n    1. Which feature is responsible for the target\n\n    2. What is the distribution of numerical feature values across the samples?\n    \n    3. What anomaly do we observe in the data, any imbalance? Outliers? Skewness? and so on...\n    \n    4. Is data Linear or Is there multi collinearity?\n    \n    5. Are the Features Significant?\n    \nThere are bunch of more questions and we will see all of that over time!!\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.","012e5365":"## This was a basic yet Informative DS Framework to work with any ML data.\n\n\n#### Please visit my website wherein I am doing 100 Projects in Deep Learning.\n---\n### [AI Unquote](https:\/\/www.aiunquote.com)\n### Follow me on LinkedIn where I post DS related Stuff everyday!!  [Alaap Dhall](https:\/\/www.linkedin.com\/in\/alaapdhall\/)\n***\n\n<br>\n\n**Thank you**","dd8c41c2":"**It is always a good Idea to Scale the Data**","81792a28":"# Let's do some Visualization","f7c0a64a":"## We make new variable AgeBand and cut it to see how it relates to Survival"}}