{"cell_type":{"3b89ea2d":"code","fcd5ec4d":"code","0b066781":"code","8640d46e":"code","79617b3e":"code","cc2645f4":"code","8b5a62a8":"code","ef568198":"code","d868ef2f":"code","4dd4a564":"code","28d39609":"code","4fac2fc2":"code","ab7cf09b":"code","dd1f11c4":"code","44969927":"code","b1563a41":"code","25cfe151":"code","6c67fe91":"code","f29c928e":"code","6fca4d1d":"code","f0872aa8":"code","85009426":"code","61b0de55":"code","27828d54":"code","0792cf74":"code","836809b1":"code","fd60b883":"code","1337370e":"code","aa74a4f2":"code","946c66cb":"markdown","61a9d8f2":"markdown","0e3c3fea":"markdown","37d36696":"markdown","9808a853":"markdown","6c231107":"markdown","5d7423dd":"markdown","7217811e":"markdown","b56ff411":"markdown","efdeb4fc":"markdown","76daef91":"markdown","70bc3375":"markdown","4655e13d":"markdown","5ed57a09":"markdown","204b0df7":"markdown","4eb06a4b":"markdown","d5626e55":"markdown","9fbaf7fc":"markdown","89dc1f27":"markdown","76bf5376":"markdown","89db729b":"markdown","c082eaa1":"markdown","93746010":"markdown","ef15b6f6":"markdown","459146c8":"markdown","e81b9b3f":"markdown","4dd07f37":"markdown","d246b11d":"markdown","77a4722a":"markdown"},"source":{"3b89ea2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fcd5ec4d":"import pandas as pd","0b066781":"df= pd.read_csv(\"\/kaggle\/input\/us-accidents\/US_Accidents_Dec20_Updated.csv\")","8640d46e":"df","79617b3e":"df.columns","cc2645f4":"df.info()","8b5a62a8":"df.describe()","ef568198":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnumeric_df = df.select_dtypes(include=numerics)\nlen(numeric_df.columns)","d868ef2f":"total = df.isnull().sum().sort_values(ascending=False)","4dd4a564":"missing_percentage=total\/len(df)\nmissing_percentage","28d39609":"type(missing_percentage)","4fac2fc2":"missing_percentage[missing_percentage!=0].plot(kind='barh')","ab7cf09b":"df.info()","dd1f11c4":"import seaborn as sns\nimport matplotlib as plt","44969927":"# Map of accidents, color code by county\n\nsns.scatterplot(x='Start_Lng', y='Start_Lat', data=df, hue='County', legend=False, s=20)","b1563a41":"cities=df.City.unique()\nlen(cities)","25cfe151":"# use value counts to find the unique occurences\ncities_by_accidents=df.City.value_counts()\ncities_by_accidents","6c67fe91":"cities_by_accidents[:10]","f29c928e":"'NY' in df.State","6fca4d1d":"cities_by_accidents[:20].plot(kind='barh')","f0872aa8":"sns.distplot(cities_by_accidents)","85009426":"high_accident_cities=cities_by_accidents[cities_by_accidents>1000]\nlen(high_accident_cities)\/len(cities)","61b0de55":"sns.distplot(high_accident_cities)","27828d54":"low_accident_cities=cities_by_accidents[cities_by_accidents<=1000]\nlen(low_accident_cities)","0792cf74":"sns.distplot(low_accident_cities)","836809b1":"sns.histplot(cities_by_accidents,log_scale=True)","fd60b883":"df.Start_Time","1337370e":"df.Start_Time[0]","aa74a4f2":"pd.to_datetime(df.Start_Time)","946c66cb":"It's a good idea to drop the columns which are missing more than half of the times like number, precipitation etc.For now i'll keep them around,drop them later if not needed","61a9d8f2":"**New york is missing from the dataset.**\n\n# Note from my side:\n**It's important to find these kind of anomolies.For ex:imgine if you are given the task of data analysis in your company.And miss this and made assumption like \"population is not responsible for number of accidents\",lot of this might go wrong.Since the decision is made based on analysis.It's better to mention the data doesn't contain new york in the beginning of analysis.**","0e3c3fea":"The above is the columns that we are working with.","37d36696":"We have 14 numerical columns","9808a853":"**Los Angles has highest number of accidents followed by houston etc.let's look at population,but it's not in the dataset.let's search in wiki**","6c231107":"Hence the proof that it is a series.[Series is one column from a dataframe]","5d7423dd":"# **Exploratory data analysis**","7217811e":"**There are around 11790 uniqie cities.it's impossible to look at all the cities,so i will try to look the cities with highest number accidents**","b56ff411":"**Let's to understand the distribution of accidents on cities.**","efdeb4fc":"**Let's try to get info regarding each numerical column.**","76daef91":"**The power of pandas..! It took just 30 to 40 secs to load all 3 million records.If it was on excel we would definately have hard time on reading it.**","70bc3375":"**Let's plot the missing percentage.For better view.since we have one column of data in missing percentage it is a series not a dataframe**","4655e13d":"# Start time","5ed57a09":"**11276 has less than 1000 accidents**","204b0df7":"**Note from my side:**\n\nLet's imagine your manager\/supervisor has given you this dataset.it's a good practice to know the columns and it's information.(The information is given to us since this is a kaggle dataset but it won't always be the case.)","4eb06a4b":"![image.png](attachment:ed65e928-3009-4447-90ed-de72634d4ba8.png).\n**This is intersting since new york is at top.let's investigate**","d5626e55":"# We learn:\n**Major chunk of the cities falls between 0 to 100 accidents.But we can see that large number of cities(Around 1200+) falls around 1 or 2.**\n\n**It could be a outlier or data could be simply missing.**","9fbaf7fc":"# Data preparation and cleaning\n**1)Load the file using Pandas.**\n\n**2)Look at some information about the data & the columns**\n\n**3)Fix any missing or incorrect values**","89dc1f27":"**Less than 5% of cities has more than 1000 accidents**","76bf5376":"**This is a string.let's convert it into date time.**","89db729b":"**The distribution of accidents on cities(higher end) is exponentially decreasing.**","c082eaa1":"**Seems like the cities are following exponential distribution**","93746010":"**We learn:**\n\nThere are 2906610 rows and 47 columns","ef15b6f6":"**We learn:**\n\n**There are following types:**\n\n1)float\n\n2)int\n\n3)bool \n\n4)object\n\nMemory usage is around 790 MB","459146c8":"# We learn:\n**New york is in the most populated city,but it doesn't even crack top 10 in accidents.let's see the number of accidents in new york**","e81b9b3f":"**Most of the cities has very few accidents less than 2000**","4dd07f37":"**Converted to date time.**","d246b11d":"# Exploring cities\n**Let's try to find out the number of unique cities in the dataset.**","77a4722a":"# My view:\n\n**Look at temp column.The accident occur on Max temperature of 61 fahrenheit and min is -8 fahrenheit.Let's try to understand in which area (warmer or colder) accidents occur the most.**\n\n**But first we need to know how many numerical columns are there.**"}}