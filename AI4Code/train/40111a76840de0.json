{"cell_type":{"98dab7d9":"code","81ff6023":"code","855cc802":"code","670c2c0a":"code","6711c156":"code","01df3a7d":"code","f27ffbbb":"code","cd369ccd":"code","2d508d1f":"code","fd6dbf7c":"code","d4408a69":"code","cd8ceabc":"code","2bedecd6":"code","9f99d7ca":"code","4c41ca04":"code","c783a7bd":"code","9b06361f":"code","fa9fd4df":"code","02118f7c":"code","0bfdcdc3":"code","98af645c":"code","954fa9f6":"code","f13996e4":"code","d7d5e881":"code","d72b5c86":"code","8eb6e4b4":"code","d734a64f":"code","2ad0a4a9":"code","08d0949a":"code","12426b72":"code","8c53ceee":"markdown","3983dac8":"markdown","b6e2a859":"markdown","39be9cf1":"markdown","579664ba":"markdown","1a9ae338":"markdown","7acd8ee6":"markdown","9c5d1c95":"markdown","33010097":"markdown","dcfef6cd":"markdown","3cf5827b":"markdown","59522919":"markdown","21b0308f":"markdown","787d9cd3":"markdown","8a5ef532":"markdown","5217b03e":"markdown","a06e841a":"markdown","4ba885b4":"markdown","8f1c0b29":"markdown","ef3e9a66":"markdown","b5dca3a9":"markdown"},"source":{"98dab7d9":"!pip install torchsummary\n# !pip install cloud-tpu-client==0.10 https:\/\/storage.googleapis.com\/tpu-pytorch\/wheels\/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl","81ff6023":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport os\nimport torchsummary\nfrom time import sleep\nimport scipy","855cc802":"image_names = os.listdir('..\/input\/animefacedataset\/images\/')\nprint(len(image_names))\n\ntraining_terminating_idx = int(0.9 * len(image_names))\ntrain_img_list = image_names[:training_terminating_idx]\nval_img_list = image_names[training_terminating_idx:]\nprint('training: ', len(train_img_list))\nprint('validation: ', len(val_img_list))","670c2c0a":"class DatasetAnime(Dataset):\n    \n  def __init__(self,img_list,image_dir, transform=None):\n    self.transform = transform\n    self.image_dir = image_dir\n    self.img_list = img_list\n          \n  def __len__(self):\n    return len(self.img_list)\n\n  def __getitem__(self, index):\n    img_path = self.image_dir + self.img_list[index]\n    image = Image.open(img_path).convert('RGB')\n    label = Image.open(img_path).convert('RGB')\n    image = image.resize((64,64))\n    label = label.resize((64,64))\n    image = np.asarray(image, dtype=np.float32)\/255\n    label = np.asarray(label, dtype=np.float32)\/255\n    image = torch.from_numpy(image)\n    label = torch.from_numpy(label)\n    image = image.permute(2,0,1) # channel * width * height\n    label = label.permute(2,0,1)\n    return image, label\n","6711c156":"train_dataset=DatasetAnime(train_img_list, '..\/input\/animefacedataset\/images\/')\nval_dataset = DatasetAnime(val_img_list, '..\/input\/animefacedataset\/images\/')","01df3a7d":"def show_samples(batch_tensor,n_row):\n  grid = torchvision.utils.make_grid(batch_tensor, nrow=n_row)\n  plt.imshow(grid.permute(1, 2, 0).detach().cpu())","f27ffbbb":"sample_torch_img, _ = train_dataset[121]\nsample_single_batch = torch.reshape(sample_torch_img, (1, *sample_torch_img.shape))\nprint(sample_torch_img.shape)\nshow_samples(sample_single_batch, 1)","cd369ccd":"class ProgressBar:\n    def __init__(self, total_dt, tabs):\n        self.total_data = total_dt\n        self.done_data = 0\n        self.printed = 0\n        \n        self.tabs = tabs\n        self.to_backspace = 0\n    \n    def print_init(self):\n        print('-' * self.tabs)\n    \n    # the extra info thing works on my local machine but not here, \n    #  maybe have something to do with how kaggle console deals with flush and backspace\n    def update(self, num, extra_info=None):\n        if self.to_backspace > 0:\n            back_str = '\\b' * self.to_backspace\n            print(back_str, end='', flush=True)\n            self.to_backspace = 0\n        self.done_data += num\n        while self.printed < int((self.done_data \/ self.total_data) * self.tabs):\n            print('>', end='', flush=True)\n            self.printed += 1\n        \n        if extra_info is not None:\n            extra_info = str(extra_info)  # nothing should go multi-line though\n            print(extra_info, end='', flush=True)\n            self.to_backspace = len(extra_info)","2d508d1f":"# a helper layer that takes a 2-D tensor of <channels> channels, stack a convolution layer and a downsample layer to shrink the\n#  width and height by half. Output has <out_channels> channels.\n# also have batchnorm, dropout and activation function added\n# a key feature is that it not only returns the output, but also return a residual tensor that stacks the channels of the input and\n#  output tensor together\nclass DownsampleResidualConv(torch.nn.Module):\n  # downsample ratio set to two just for simplicity, and residue is done through summation\n  def __init__(self, channels, out_channels, dropout_rate=0.1):\n    super().__init__()\n    self.conv = torch.nn.utils.spectral_norm(\n                  torch.nn.Conv2d(\n                    channels, \n                    out_channels=out_channels,\n                    kernel_size= 3, \n                    stride= 2, \n                    padding = 1\n                ))\n    self.batch_norm = torch.nn.BatchNorm2d(out_channels)\n    self.dropout = torch.nn.Dropout(p=dropout_rate)\n    self.activation = torch.nn.LeakyReLU()\n    self.downsample = torch.nn.MaxPool2d((4, 4), stride=2, padding=1, dilation=1)\n\n  def forward(self, x):\n    result = self.conv(x)\n    result = self.batch_norm(result)\n    result = self.dropout(result)\n    result = self.activation(result)\n    \n    down_x = self.downsample(x)\n    residual_result = torch.cat((result, down_x), dim=1)\n    return result, residual_result\n\n\n# similar to DownsampleResidualConv except it upsamples by a factor of 2\n# residual strategy can be both concatenation or addition\nclass UpsampleResidualConv(torch.nn.Module):\n  res_strategies = ['concat', 'add']\n\n  def __init__(self, channels, out_channels, res_strategy='concat', dropout_rate=0.1):\n    super().__init__()\n    self.conv = torch.nn.utils.spectral_norm(\n                  torch.nn.Conv2d(\n                    channels, \n                    out_channels=out_channels,\n                    kernel_size= 3, \n                    stride= 1, \n                    padding = 1\n                ))\n    self.batch_norm = torch.nn.BatchNorm2d(out_channels)\n    self.dropout = torch.nn.Dropout(p=dropout_rate)\n    self.activation = torch.nn.LeakyReLU()\n    self.upsample = torch.nn.Upsample(scale_factor=2)\n    \n    if res_strategy not in self.res_strategies:\n      raise ValueError('residual strategy can only be among {0}'.format(','.join(self.res_strategies)))\n    self.res_strategy = res_strategy\n\n  def forward(self, x):\n    result = self.conv(x)\n    result = self.batch_norm(result)\n    result = self.dropout(result)\n    result = self.activation(result)\n    result = self.upsample(result)\n\n    x_up = self.upsample(x)\n\n    if self.res_strategy == 'concat':\n      residual_result = torch.cat((result, x_up), dim=1)\n    elif self.res_strategy == 'add':\n      residual_result = result + x_up\n    else:\n      raise ValueError('residual strategy can only be among {0}'.format(','.join(self.res_strategies)))\n    return residual_result\n\n\n# a layer to sample a random Gaussian vector based on the input and provided relative mean and var\n# first based on the input x (a 1-D vector) and output a sample delta mean and variance (log taken for computational purpose) \n#  then the sample mean = delta mean + mean relative to, sample variance = delta variance * variance relative to\n# then sample a random Gaussian vector based on sample mean and variance\n# return the sample, the sample distribution, and delta to the previous distribution, for convenience of calculation\n# note the linear layers to predict delta means and variances are spectral normalized, based on paper's suggestions\nclass SampleLayer(torch.nn.Module):\n  def __init__(self, input_dim, output_dim, internal_layers=2, dropout_rate=0.1):\n    super().__init__()\n\n    def construct_sequential_list():\n      inpt = input_dim\n\n      seq_lst = []\n      for internal_lyr in range(0, internal_layers - 1):\n        otpt = min([(internal_layers - internal_lyr) * output_dim, (inpt + output_dim) \/\/ 2])\n        seq_lst.extend([\n          torch.nn.utils.spectral_norm(\n            torch.nn.Linear(\n              inpt,\n              otpt\n            )\n          ),\n          torch.nn.BatchNorm1d(otpt),\n          torch.nn.Dropout(p=dropout_rate),\n          torch.nn.LeakyReLU()\n        ])\n        inpt = otpt\n\n      seq_lst.append(torch.nn.utils.spectral_norm(torch.nn.Linear(inpt, output_dim)))\n      seq_lst.append(torch.nn.Dropout(p=dropout_rate))\n      return seq_lst\n    self.mean_ln = torch.nn.Sequential(*construct_sequential_list()) \n    self.logvar_ln = torch.nn.Sequential(*construct_sequential_list()) \n    \n  def forward(self, x, rel_mu, rel_logvar):\n    sample_delta_center, sample_delta_logvar = self.mean_ln(x), self.logvar_ln(x)\n    sample_center, sample_logvar = rel_mu + sample_delta_center, rel_logvar + sample_delta_logvar\n    sample_base = torch.normal(torch.zeros(sample_center.shape), torch.ones(sample_logvar.shape)).to(x.device)\n\n    sampled_result = sample_center + sample_base * torch.exp(sample_logvar * 0.5)\n    return sampled_result, sample_center, sample_logvar, sample_delta_center, sample_delta_logvar\n    \n\n# encoder part, takes in an image, and returns z_1, ..., z_n (n is configurable by hidden_layers), it also returns the\n#  underlying distributions of z_1, ..., z_n to use in calculation of loss during the training\n# simulates P[z_n, ..., z_1 | I] in the paper\nclass BiEncoder(torch.nn.Module):\n  def __init__(self, img_size, hidden_size, hidden_layers, encoding_size, encoding_layers=None, img_channels=3, dropout_rate=0.01):\n    super().__init__()\n    if encoding_layers is None:\n      encoding_layers = hidden_layers\n    encoding_layers = min((encoding_layers, hidden_layers))\n\n    self.width, self.height = img_size\n    self.img_channels = img_channels\n\n    self.hidden_size = hidden_size\n\n    self.encoding_size = encoding_size\n    self.encoding_layers = encoding_layers\n\n    w, h = self.width, self.height\n    residual_down_lst = []\n    backward_sample_net_lst = []\n    forward_sample_net_lst = []\n    in_channels = self.img_channels\n    for lyr in range(0, hidden_layers):\n      w, h = w\/\/2, h\/\/2\n      \n      # image features to use to predict hidden variables, see forward function for its usuage\n      residual_down_lst.append(DownsampleResidualConv(in_channels, self.hidden_size, dropout_rate=dropout_rate))\n      backward_sample_net_lst.append(SampleLayer(self.hidden_size * w * h, self.encoding_size, internal_layers=2, dropout_rate=dropout_rate))\n\n      if lyr > 0:\n        forward_sample_net_lst.append(SampleLayer(self.encoding_size, self.encoding_size, internal_layers=2, dropout_rate=dropout_rate))\n\n      in_channels += self.hidden_size\n\n    # residual models to, based on image, predict z_i backwards from z_n to z_1 (that is, P[z_{i-1} | z_i])\n    # both in the order of z_n, z_{n-1}, ..., z_1\n    self.backward_modules = torch.nn.ModuleList(residual_down_lst)\n    self.backward_sample_modules = torch.nn.ModuleList(backward_sample_net_lst)\n\n    # P[z_i | z_{i-1}]\n    # in order of z_2, ..., z_n\n    self.forward_sample_modules = torch.nn.ModuleList(forward_sample_net_lst)\n\n    self.z1_concat_vec = torch.nn.Parameter(torch.zeros(self.encoding_size))\n\n  # takes in the image as the input.\n  def forward(self, x, hint=0.0):\n    residual_conv_input = x\n    backward_results = []\n\n    # go through the backward model, first form I_n, ..., I_1 in the order of I_{n+1} => I_n => ... => I_1 where I_{n+1} is input image\n    for lyr in range(0, len(self.backward_modules)):\n      output_i, res_output_i = self.backward_modules[lyr](residual_conv_input)\n\n      backward_results.append(output_i)\n      residual_conv_input = res_output_i\n      # print(output_i.shape)\n\n    prev_z = torch.stack(tuple([self.z1_concat_vec for _ in range(0, x.shape[0])]), dim=0)\n    base_mu = torch.zeros(self.encoding_size).to(x.device)\n    base_logvar = torch.zeros(self.encoding_size).to(x.device)\n\n    # this time backward\n    hidden_vars_backward = []\n    mu_lst = []\n    logvar_lst = []\n    dmu_lst = []\n    dlogvar_lst = []\n\n    # now based on I_n, ...., I_1, sample z_n, ..., z_1\n    for lyr in range(0, self.encoding_layers):\n      mu_lst.append(base_mu)\n      logvar_lst.append(base_logvar)\n\n      gen_input_i = torch.flatten(backward_results[-lyr - 1], start_dim=1)\n      # gen_input_i = torch.cat((gen_input_i, prev_z), dim=1)\n      z_i, mu, logvar, dmu, dlogvar = self.backward_sample_modules[-lyr - 1](gen_input_i, base_mu, base_logvar)\n\n      hidden_vars_backward.append(z_i)\n\n      base_mu = mu\n      base_logvar = logvar\n\n      dmu_lst.append(dmu)\n      dlogvar_lst.append(dlogvar)\n      \n      prev_z = z_i\n\n    # using the generated z_1, form forward encoding using z_2', ..., z_n'\n    hidden_vars_forward, mu_lst_foward, logvar_lst_forward, dmu_lst_forward, dlogvar_lst_forward = self.get_forward_encodings(\n        hidden_vars_backward[0],\n        mu_lst[1] if self.encoding_layers >= 2 else None,\n        logvar_lst[1] if self.encoding_layers >= 2 else None\n    )\n\n    mu_lst.extend(mu_lst_foward)\n    logvar_lst.extend(logvar_lst_forward)\n    dmu_lst.extend(dmu_lst_forward)\n    dlogvar_lst.extend(dlogvar_lst_forward)\n\n    # if training mode, take a random weighted mean for the z and z', and if evaluating, mix it in the hint coefficient given\n    #  for the weight of backward z (idea is that z used the full info from the image while z' only knows the bottom level\n    #  info)\n    hidden_vars_coefficient = [\n      torch.rand(h_b.shape).to(h_b.device) if self.training else hint for h_b in hidden_vars_backward\n    ]\n\n    hidden_vars = [\n      h_b * c + (1-c) * h_f for h_b, h_f, c in zip(\n        hidden_vars_backward, \n        hidden_vars_forward, \n        hidden_vars_coefficient\n      )\n    ]\n    \n    forward_backward_diffs = torch.tensor(torch.cat(hidden_vars_backward).detach().cpu().numpy()).to(x.device) - torch.cat(hidden_vars_forward) \n\n    # return the final hidden variables, and also their distributions for loss computations. we also want z' to approximate z\n    #  so also return difference between z and z', but we treat z as ground truth so the diff is not differentiable in respect\n    #  to z\n    return hidden_vars, mu_lst, logvar_lst, dmu_lst, dlogvar_lst, forward_backward_diffs\n\n  # apply the forward models to form the encodings based on z_1\n  def get_forward_encodings(self, z_1, base_mu=None, base_logvar=None):\n    if base_mu is None:\n      base_mu = torch.zeros(self.encoding_size).to(z_1.device)\n    if base_logvar is None:\n      base_logvar = torch.zeros(self.encoding_size).to(z_1.device)\n\n    hidden_vars_forward = [z_1]\n    mu_lst_foward = []\n    logvar_lst_forward = []\n    dmu_lst_forward = []\n    dlogvar_lst_forward = []\n\n    for lyr in range(1, self.encoding_layers):\n      mu_lst_foward.append(base_mu)\n      logvar_lst_forward.append(base_logvar)\n\n      prev_z = hidden_vars_forward[lyr - 1]\n      z_i, mu, logvar, dmu, dlogvar = self.forward_sample_modules[lyr - 1](prev_z, base_mu, base_logvar)\n\n      hidden_vars_forward.append(z_i)\n\n      base_mu = mu\n      base_logvar = logvar\n\n      dmu_lst_forward.append(dmu)\n      dlogvar_lst_forward.append(dlogvar)\n\n    return hidden_vars_forward, mu_lst_foward, logvar_lst_forward, dmu_lst_forward, dlogvar_lst_forward\n\n# decoder of image, take in a list of the hidden variables, and generate an image\n# simulates P[I | z_n, ..., z_1]\nclass HDecoder(torch.nn.Module):\n  def __init__(self, image_size, encoding_dim, encoding_layers, hidden_size, img_channels=3, dropout_rate=0.1):\n    super().__init__()\n    w, h = image_size\n    self.encoding_dim = encoding_dim\n    self.encoding_layers = encoding_layers\n    self.hidden_size = hidden_size\n    self.img_channels = img_channels\n\n    # construct in the order of z_n, ..., z_1 and reverse afterwards\n    # to predict I_i based on z_i, see forward for usage\n    encoding_to_feature_models_ln = []\n    self.encoding_to_feature_shapes = []\n    encoding_to_feature_models_conv = []\n    for encoding_sub_levels in range(0, encoding_layers):\n      w, h = w\/\/2, h\/\/2\n      encoding_to_feature_models_ln.append(\n          torch.nn.Sequential(\n              torch.nn.Linear(self.encoding_dim, w * h * self.hidden_size),\n              torch.nn.BatchNorm1d(w * h * self.hidden_size),\n              torch.nn.Dropout(p=dropout_rate)\n          )\n      )\n      self.encoding_to_feature_shapes.append((self.hidden_size, w, h))\n      encoding_to_feature_models_conv.append(\n          torch.nn.Sequential(\n              *[\n                  UpsampleResidualConv(\n                    self.hidden_size, \n                    self.hidden_size, \n                    'add', \n                    dropout_rate=dropout_rate\n                  ) for _ in range(0, encoding_sub_levels + 1)\n              ],\n              torch.nn.Conv2d(\n                self.hidden_size, \n                out_channels=(self.hidden_size + self.img_channels) \/\/ 2,\n                kernel_size= 5, \n                stride= 1, \n                padding = 2\n              ),\n              torch.nn.LeakyReLU(),\n              torch.nn.Conv2d(\n                (self.hidden_size + self.img_channels) \/\/ 2,\n                out_channels=self.img_channels,\n                kernel_size= 5, \n                stride= 1, \n                padding = 2\n              ),\n              torch.nn.Dropout(p=dropout_rate)\n          )\n      )\n    encoding_to_feature_models_ln.reverse()\n    self.encoding_to_feature_shapes.reverse()\n    encoding_to_feature_models_conv.reverse()\n\n    self.encoding_to_features_ln = torch.nn.ModuleList(encoding_to_feature_models_ln)\n    self.encoding_to_features_conv = torch.nn.ModuleList(encoding_to_feature_models_conv)\n\n  def forward(self, hidden_vars):\n    hidden_compute_results = []\n    # based on z_1, ...,  z_n generate image features of each layer respectively, I_1, ..., I_n\n    for zi, m_ln_i, m_conv_i, m_shape_i in zip(\n        hidden_vars, \n        self.encoding_to_features_ln, \n        self.encoding_to_features_conv,\n        self.encoding_to_feature_shapes\n    ):\n      ln_result_i = m_ln_i(zi)\n      ln_reshaped_i = ln_result_i.reshape((-1, *m_shape_i))\n      conv_result_i = m_conv_i(ln_reshaped_i)\n      hidden_compute_results.append(conv_result_i)\n    \n    # to predict the image based on the features, each z_i will result in a set of features I_i and then a 3-channel image\n    #  O_i. Use the gradient boosting approach that result_j = O_1 + ... + O_j, and the final result is result_n\n    history = []\n    result = 0.0\n    for hidden_result_i in hidden_compute_results:\n        result = hidden_result_i + result\n        history.append(result)\n    return result, history\n\n","fd6dbf7c":"# if os.environ.get('COLAB_GPU', '0') == '1':\n#   os.environ['GPU_NUM_DEVICES'] = '1'\n#   os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=\/usr\/local\/cuda\/'\n# import torch_xla.core.xla_model as xm\n\nTRAINING_PARAMS = {\n    'batch_size': 100,\n    'lr': 0.002,\n#     'device': xm.xla_device(), # used for TPU, but no necessity so far\n    'device': 'cuda',\n    'elbo_coe': 0.05, # the weight of ELBO loss, see later definition for its details\n    # coefficients for the loss on each level of the generated image in hierachy, starting from z_1, ..., z_n\n    # a list for the coefficient in image itself, image gradient, image Hessian, ...\n    # see loss function definition for details, this is to enforce the hierachy is actually trained in model\n    #   the first layer only cares about the image being accurate, while the second layer also try to make the image gradient\n    #     accurate (details), ...\n    'reconstruct_coes': [\n        [0.4],\n        [0.3, 0.2],\n        [0.3, 0.2, 0.01]\n    ],\n    'forward_backward_error_coe': 0.1, # the weight of the difference of z and z' in the encoder model in the loss\n    'epochs': 1 # put this to 1 if pre-trained\n}\nMODEL_PARAMS= {\n    'encoder_hidden_size': 30, # the hidden I_1, ..., I_n have this many channels\n    'encoder_hidden_layers': 3, # the number n\n    'encoder_dropout_rate': 0.01,\n\n    'decoder_hidden_size': 30, # similar to the encoder one\n    'decoder_dropout_rate': 0.01,\n\n    'encoding_size': 200, # the dimension of the vector z's\n    \n    # the number of z's to actually return, for some layers we might just apply conv but not predict its z, less than or equal to n\n    'encoding_layers': 3, \n    \n}","d4408a69":"# ELBO loss in the paper of one single distribution\ndef ELBO_KL_term(mu, logvar, dmu, dlogvar):\n  return 0.5 * torch.mean(dmu * dmu \/ torch.exp(logvar) + torch.exp(dlogvar) - dlogvar - 1)\n\n# sum of all the ELBO losses for each hidden var\ndef ELBO_KL(mus, stds, dmus, dlogvars):\n  elbo = torch.tensor(0.0)\n  for mu_i, std_i, dmu_i, dlogvar_i in zip(mus, stds, dmus, dlogvars):\n    elbo = elbo + ELBO_KL_term(mu_i, std_i, dmu_i, dlogvar_i)\n  return elbo\n\n# for an image I, if you apply sobel filter, it will find [I_x, I_y] where I_x = dI \/ dx (channelwise, I_y similar)\n# if you apply it recursively, you will be able to find higher order derivatives\ndef apply_sobel(img):\n  _, img_channels, width, height = img.shape\n  sobel_filter = torch.zeros((img_channels * 2, img_channels, 3, 3))\n  x_sobel = torch.tensor([[1.0, 0, -1], [2, 0, -2], [1, 0, -1]])\n  y_sobel = torch.tensor([[1.0, 2, 1], [0, 0, 0], [-1, -2, -1]])\n  for ci in range(0, img_channels):\n    sobel_filter[ci, ci] = x_sobel\n    sobel_filter[img_channels + ci, ci] = y_sobel\n  filtered_img = torch.nn.functional.conv2d(img, sobel_filter.to(img.device))\n  return filtered_img\n\n# the error term for recontruction for two images on order <order>\n#  that is, mean square of D^{order}(img1) - D^{order}(img1), where D^{order} means taking derivatives on images <order> times\ndef reconstruct_error_term(img1, img2, order=0):\n  if order <= 0:\n    return torch.mean(torch.sum(torch.pow(img1 - img2, 2), axis=1))\n  else:\n    dimg1 = apply_sobel(img1)\n    dimg2 = apply_sobel(img2)\n    return reconstruct_error_term(dimg1, dimg2, order-1)\n\n# sum up all terms, with weights specified in the coefficient list provided, ranging from 0th, 1st, ... n'th derivative\ndef reconstruct_error(img1, img2, order_coefficients=[1.0]):\n  result=0.0\n  for order_num, order_c in enumerate(order_coefficients):\n    result = result + reconstruct_error_term(img1, img2, order_num) * order_c\n  return result\n\n# sum up reconstruction error for all histories in the decoder\ndef reconstruct_error_historywise(history, base_img, order_coefficient_list):\n  result = 0.0\n  for hist_img, order_coe_list in zip(history, order_coefficient_list):\n    result += reconstruct_error(hist_img, base_img, order_coe_list)\n  return result\n\n\ndef forward_backward_error_term(fb_diffs):\n  return torch.mean(torch.pow(fb_diffs, 2))","cd8ceabc":"sample_sobeled = apply_sobel(sample_single_batch)\nsample_sobel_x = sample_sobeled[:,:3]\nsample_sobel_y = sample_sobeled[:,3:]\nshow_samples(torch.cat((sample_sobel_x, sample_sobel_y, torch.pow(sample_sobel_x, 2) + torch.pow(sample_sobel_y, 2)), dim=0), 3)","2bedecd6":"training_loader = DataLoader(train_dataset, batch_size=TRAINING_PARAMS['batch_size'])\nval_loader = DataLoader(val_dataset, batch_size=TRAINING_PARAMS['batch_size'])","9f99d7ca":"# define encoder based on parameters\nencoder = BiEncoder(\n    # img_size, hidden_size, hidden_layers, encoding_size, encoding_layers=None, img_channels=3, dropout_rate=0.01\n    (64, 64), \n    MODEL_PARAMS['encoder_hidden_size'], \n    MODEL_PARAMS['encoder_hidden_layers'],\n    MODEL_PARAMS['encoding_size'],\n    MODEL_PARAMS['encoding_layers'],\n    dropout_rate=MODEL_PARAMS['encoder_dropout_rate']\n).to(TRAINING_PARAMS['device'])\ndecoder = HDecoder(\n    # image_size, encoding_dim, encoding_layers, hidden_size, img_channels=3, dropout_rate=0.1\n    (64, 64), \n    MODEL_PARAMS['encoding_size'],\n    MODEL_PARAMS['encoding_layers'],\n    MODEL_PARAMS['decoder_hidden_size'],\n    dropout_rate=MODEL_PARAMS['decoder_dropout_rate']\n).to(TRAINING_PARAMS['device'])\n\ntorchsummary.summary(encoder, (3, 64, 64))\n# not summarizing decoder because \n#  (1) the functions signature of decoder's forward is not compatible with this function\n#  (2) decoder is much smaller size\n","4c41ca04":"# load if pretrained model uploaded\nif os.path.isfile('encoder.pt'):\n  encoder.load_state_dict(torch.load('encoder.pt'))\n  print('loading pre-trained encoder')\nif os.path.isfile('decoder.pt'):\n  decoder.load_state_dict(torch.load('decoder.pt'))\n  print('loading pre-trained decoder')","c783a7bd":"optimizer = torch.optim.Adam(\n              [\n                {'params': encoder.parameters()},\n                {'params': decoder.parameters()}\n              ], \n              lr = TRAINING_PARAMS['lr']\n            )","9b06361f":"!rm -rf model_checkpoints*\n!mkdir -p model_checkpoints","fa9fd4df":"# training\nprint('using {0}'.format(TRAINING_PARAMS['device']))\nfor epoch in range(0, TRAINING_PARAMS['epochs']):\n  print('epoch {0} \/ {1}'.format(epoch + 1, TRAINING_PARAMS['epochs']))\n\n  encoder.train()\n  decoder.train()\n  train_reconstruct_errs = []\n  train_elbo_errs = []\n  train_fb_errs = []\n  pg_bar = ProgressBar(len(training_loader), 100)\n  pg_bar.print_init()\n  for image_batch, label_batch in training_loader:\n    optimizer.zero_grad()\n\n    # predict the z's, and the reconstructed images\n    image_batch, label_batch = image_batch.to(TRAINING_PARAMS['device']), label_batch.to(TRAINING_PARAMS['device'])\n    encodings, mus, logvars, dmus, dlogvars, fb_diffs = encoder(image_batch)\n\n    reconstructed_img, reconstruct_hist = decoder(encodings)\n\n    # calculate individual losses\n    recon_err = reconstruct_error_historywise(reconstruct_hist, label_batch, TRAINING_PARAMS['reconstruct_coes'])\n    elbo_err = ELBO_KL(\n      mus, \n      logvars, \n      dmus, \n      dlogvars \n    )\n    fb_err = forward_backward_error_term(fb_diffs)\n    \n    # the training loss is a weighted sum of all defined losses\n    loss = recon_err + elbo_err * TRAINING_PARAMS['elbo_coe'] + fb_err * TRAINING_PARAMS['forward_backward_error_coe']\n    loss.backward()\n\n    if TRAINING_PARAMS['device'] in ['cuda', 'cpu']:\n      optimizer.step()\n    else:\n      xm.optimizer_step(optimizer)\n\n    train_reconstruct_errs.append(recon_err.detach().cpu().numpy())\n    train_elbo_errs.append(elbo_err.detach().cpu().numpy())\n    train_fb_errs.append(fb_err.detach().cpu().numpy())\n    # print(train_reconstruct_errs[-1], train_elbo_errs[-1])\n    pg_bar.update(1)\n\n  # calculate losses on validation set\n  encoder.eval()\n  decoder.eval()\n  val_reconstrct_errs = []\n  val_elbo_errs = []\n  for val_img, val_label in val_loader:\n    val_img, val_label = val_img.to(TRAINING_PARAMS['device']), val_label.to(TRAINING_PARAMS['device'])\n    val_encodings, val_mus, val_logvars, val_dmus, val_dlogvars, _ = encoder(val_img)\n    val_reconstructed_img, val_reconstruct_hist = decoder(val_encodings)\n    \n    val_recon_err = reconstruct_error_historywise(val_reconstruct_hist, val_label, TRAINING_PARAMS['reconstruct_coes'])\n    val_elbo_err = ELBO_KL(\n      val_mus, \n      val_logvars, \n      val_dmus, \n      val_dlogvars\n    )\n    val_reconstrct_errs.append(val_recon_err.detach().cpu().numpy())\n    val_elbo_errs.append(val_elbo_err.detach().cpu().numpy())\n\n    \n  train_recon_mean_loss = np.mean(train_reconstruct_errs)\n  train_elbo_mean_loss = np.mean(train_elbo_errs)\n  train_fb_mean_err = np.mean(train_fb_errs)\n\n  val_recon_mean_loss = np.mean(val_reconstrct_errs)\n  val_elbo_mean_loss = np.mean(val_elbo_errs)\n  # print training stats in this epoch and save checkpoints\n  print()\n  print('training reconstruct err', train_recon_mean_loss)\n  print('training elbo err mean', train_elbo_mean_loss)\n  print('training forward backward diff', train_fb_mean_err)\n    \n  print('validation reconstruct err', val_recon_mean_loss)\n  print('validation elbo err mean', val_elbo_mean_loss)\n  print()\n\n  checkpoint_suffix = 'E{0}_recon_{1}_elbo_{2}'.format(     \n      epoch+1,\n      str(train_recon_mean_loss).replace('.', '_'),\n      str(train_elbo_mean_loss).replace('.', '_')\n  )\n  torch.save(encoder.state_dict(), 'model_checkpoints\/encoder_{0}.pt'.format(checkpoint_suffix))\n  torch.save(decoder.state_dict(), 'model_checkpoints\/decoder_{0}.pt'.format(checkpoint_suffix))","02118f7c":"encoder.eval()\ndecoder.eval()\nsample_batch = torch.cat([torch.reshape(train_dataset[idx][0], (1, *(train_dataset[idx][0].shape))) for idx in range(117, 127)], dim=0)\nsample_encoding_no_hint, *_ = encoder(sample_batch.to(TRAINING_PARAMS['device']), 0.0)\nsample_encoding_hint, *_ = encoder(sample_batch.to(TRAINING_PARAMS['device']), 1.0)\nsample_prediction_no_hint, sample_pred_history_no_hint = decoder(sample_encoding_no_hint)\nsample_prediction_hint, sample_pred_history_hint = decoder(sample_encoding_hint)\nshow_samples(torch.cat((sample_batch.to(TRAINING_PARAMS['device']), sample_prediction_no_hint, sample_prediction_hint), dim=0), 10)\nencoder.train()\ndecoder.train()\nprint('')\n","0bfdcdc3":"show_samples(torch.cat(sample_pred_history_no_hint, dim=0), 10)","98af645c":"show_samples(torch.cat(sample_pred_history_hint, dim=0), 10)","954fa9f6":"encoder.eval()\ndecoder.eval()\n\nencoding_results = [[] for _ in range(0, MODEL_PARAMS['encoding_size'])]\npg_bar = ProgressBar(len(training_loader), 100)\npg_bar.print_init()\nfor image_batch, label_batch in training_loader:\n    image_batch, label_batch = image_batch.to(TRAINING_PARAMS['device']), label_batch.to(TRAINING_PARAMS['device'])\n    encodings, mus, logvars, dmus, dlogvars, fb_diffs = encoder(image_batch)\n    encoding0 = encodings[0].detach().cpu().numpy()\n    for encoding_idx in range(0, len(encoding_results)):\n        encoding_results[encoding_idx].extend(encoding0[:, encoding_idx])\n    pg_bar.update(1)\n","f13996e4":"encoding_results = np.array(encoding_results)\nencoding_means = []\nencoding_vars = []\nencoding_dis_stats = []\n\nfor encoding_idx in range(0, len(encoding_results)):\n    encoding_data_vec = encoding_results[encoding_idx]\n    encoding_mean = np.mean(encoding_data_vec)\n    encoding_var = np.var(encoding_data_vec)\n    # encoding_data_nrm = (encoding_data_vec - encoding_mean) \/ np.sqrt(encoding_var)\n    encoding_ds = scipy.stats.kstest(encoding_data_vec, np.random.randn(*encoding_data_vec.shape) * np.sqrt(encoding_var) + encoding_mean).pvalue\n    \n    encoding_means.append(encoding_mean)\n    encoding_vars.append(encoding_var)\n    encoding_dis_stats.append(encoding_ds)\n    ","d7d5e881":"plt.hist(encoding_means)\nplt.show()","d72b5c86":"plt.hist(encoding_vars)\nplt.show()","8eb6e4b4":"plt.hist(-np.log(encoding_dis_stats), density=True)\nplt.show()","d734a64f":"for p_threshold in [0.1, 0.05, 0.01]:\n    print('{1}% of the distributions have p-value greater than or equal to {0}'.format(p_threshold, 100 * float(np.sum(np.array(encoding_dis_stats) > p_threshold)) \/ len(encoding_dis_stats)))\n","2ad0a4a9":"TOP_N = 3\n\nmin_idx_lst = np.argsort(encoding_dis_stats)[:TOP_N]\nmx_idx_lst = np.argsort(encoding_dis_stats)[-TOP_N:]\n\nprint(np.array(encoding_dis_stats)[min_idx_lst])\nprint(np.array(encoding_dis_stats)[mx_idx_lst])\n\nfg, axs  = plt.subplots(2, TOP_N, sharey=True)\nfor plt_idx in range(0, TOP_N):\n    min_idx= min_idx_lst[plt_idx]\n    mx_idx = mx_idx_lst[plt_idx]\n    min_results = encoding_results[min_idx]\n    min_ideal = np.random.randn(*min_results.shape) * encoding_vars[min_idx] + encoding_means[min_idx]\n    mx_results = encoding_results[mx_idx]\n    mx_ideal = np.random.randn(*mx_results.shape) * encoding_vars[mx_idx] + encoding_means[mx_idx]\n    axs[0, plt_idx].hist([min_results, min_ideal], bins=200)\n    axs[1, plt_idx].hist([mx_results, mx_ideal], bins=200)","08d0949a":"encoding_cov = np.cov(encoding_results)\nplt.matshow(encoding_cov)\nplt.show()","12426b72":"decoder.eval()\nencoder.eval()  \n# match the empirical distributions in trainings for encodings\nrandom_encodings, *_ = encoder.get_forward_encodings(\n    (1.0 * torch.tensor(np.sqrt(encoding_vars)) * torch.randn((36, MODEL_PARAMS['encoding_size'])) + torch.tensor(encoding_means)).to(TRAINING_PARAMS['device'])\n)\nrandom_images, _ = decoder(random_encodings)\nshow_samples(random_images, 6)\nencoder.train()\ndecoder.train()\nprint('')","8c53ceee":"Let's take a look at the histogram of the worst and best of the p-value in terms of p-value","3983dac8":"**Histgram of p-value**","b6e2a859":"The attributes are mostly independent.","39be9cf1":"# Model Definition\nPaper found here: https:\/\/arxiv.org\/pdf\/2007.03898.pdf\nA VAE model that takes hierachies. Unlike the conventional VAE that finds the underlying variable representation $z$ for an image $I$, it finds a series of underlying varibles $z_1, z_2, ..., z_n$ where each $z_i$ depends on $z_1, ..., z_{i-1}$, and finally $I$ forms a distribution based on $z_1, z_2, ..., z_n$. The idea is that the $z_1$ is supposed to decide the most high level attributes of an image, for instance, the general shape of the face, and the following variables add some details in each layer.","579664ba":"**Just out of curiosity, calculating the co-variance between each attribute**","1a9ae338":"# Parameters and training\nconfigure parameters and train the results, the loss functions will also be defined in this section","7acd8ee6":"**Demo on reconstructing the image**","9c5d1c95":"**The mean, variance of the actual generated first-level encodings, as well as p-value of them following normal distributions**","33010097":"just a small demo on sobel's result\nfirst image is $\\frac{dI}{dx}$, second is $\\frac{dI}{dy}$, third is $norm([\\frac{dI}{dx}, \\frac{dI}{dy}])$","dcfef6cd":"**Generate random images**$$$$\nrandomly generate 36 $z_1$'s that is from normal distribution with mean, variance of each attribute , and based on those random encodings, give 36 images using the decoder","3cf5827b":"# Some Utils\n- a function to show images visually\n- a progress bar to show the training progress (tqdm not working very well on kaggle)","59522919":"The reconstruction sometimes does not fully match some details(collar, glasses, hairclip, color in certain regions like hair or eyes), but other than that it seems to be quite good quality","21b0308f":"# Demo and Analysis","787d9cd3":"Most of the means are near 0, which is ideal","8a5ef532":"Most of the variance are around 1.15, which is slightly higher than ideal, which is one, but we can adjust that","5217b03e":"**Histgram of Variance**","a06e841a":"The model is able to fix the details when we progress to the upper layers.","4ba885b4":"# Read data\n\nRead the data into a torch Dataset for future iteration.\ncredit to https:\/\/www.kaggle.com\/sanketgandhi\/anime-image-generation-with-vae-hvae-and-gan for the dataset, dataloader code as well as some implementation related inspirations in terms of the model structure.","8f1c0b29":"We can see that while even for those with quite low p-value, it is likely to result from the large data size and the result is pretty normally distributed","ef3e9a66":"**Histgram of Mean**","b5dca3a9":"Split the dataset into training dataset and validation. (10% for validation)"}}