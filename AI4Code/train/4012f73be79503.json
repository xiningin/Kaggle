{"cell_type":{"4a8e366f":"code","315ca1f5":"code","72b2c10d":"code","5a97512b":"code","b4852510":"code","62573612":"code","7db7f6e3":"code","7d59d688":"code","d17030a2":"code","6d157bf3":"markdown","5ed5d959":"markdown"},"source":{"4a8e366f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","315ca1f5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","72b2c10d":"SEED = 19\nNFOLDS = 5\nDATA_DIR = '\/kaggle\/input\/lish-moa\/'\nnp.random.seed(SEED)","5a97512b":"train = pd.read_csv(DATA_DIR + 'train_features.csv')\ntargets = pd.read_csv(DATA_DIR + 'train_targets_scored.csv')\n\ntest = pd.read_csv(DATA_DIR + 'test_features.csv')\nsub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n\n# drop where cp_type==ctl_vehicle (baseline)\nctl_mask = train.cp_type=='ctl_vehicle'\ntrain = train[~ctl_mask]\ntargets = targets[~ctl_mask]\n\n# drop id col\nX = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].to_numpy() ","b4852510":"clf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', MultiOutputClassifier(\n                                 XGBClassifier(tree_method='gpu_hist')))\n               ])","62573612":"params = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\nclf.set_params(**params)","7db7f6e3":"oof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\nkf = KFold(n_splits=NFOLDS)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds \/ NFOLDS","7d59d688":"print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))","d17030a2":"# setting control test preds to 0\ncontrol_mask = [test['cp_type']=='ctl_vehicle']\n\ntest_preds[control_mask] = 0\n\n# createing submission file\nsub.iloc[:,1:] = test_preds\nsub.to_csv('submission.csv', index=False)","6d157bf3":"## Analysis","5ed5d959":"## Training"}}