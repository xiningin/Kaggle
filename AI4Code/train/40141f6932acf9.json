{"cell_type":{"478e21bd":"code","87346a12":"code","fcbc30f1":"code","0def4a42":"code","272cb6d9":"code","0a8cebb9":"code","ceb60afd":"code","31b908df":"code","1b578ee7":"code","7c7434f6":"code","460f3e8c":"code","60873339":"markdown","245c9d8c":"markdown","e5dbcf66":"markdown","ce8e372a":"markdown"},"source":{"478e21bd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D\nfrom keras.layers import Input, Add, Dense, BatchNormalization, Flatten, Conv2D, MaxPooling2D\nimport tensorflow as tf\nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import Callback\nfrom keras.models import Model\nfrom keras.preprocessing.image import array_to_img\nfrom PIL import Image\nfrom numpy import asarray\nimport os\n%matplotlib inline","87346a12":"# Setting path variables\npath = \"..\/input\/covid19-image-dataset\/Covid19-dataset\/train\"\npath_test = \"..\/input\/covid19-image-dataset\/Covid19-dataset\/test\"","fcbc30f1":"def min_size(path_of_image):\n    \"\"\"\n    Checking min size available in all folder.\n    This will help you to give idea, which size you have to select for resizing.\n    \n    \"\"\"\n    # wlking across directory to open all available images\n    size_images = {}\n    for dirpath, _, filenames in os.walk(path_of_image):\n        for path_image in filenames:\n            image = os.path.abspath(os.path.join(dirpath, path_image))\n            \n            # checking images size and storing in a dict to compare\n            with Image.open(image) as img:\n                width, heigth = img.size\n                size_images[path_image] = {'width': width, 'heigth': heigth}\n                \n    # Creating a small DF to check min & max size of images\n    df_image_desc = pd.DataFrame(size_images).T\n    min_width = df_image_desc['width'].min()\n    min_height = df_image_desc['heigth'].min()\n    \n    return min_height, min_width\n\nmin_size(path)","0def4a42":"######################################## Data prep started #########################################################\n\ndef load_image_from_folder(path, basewidth, hsize):\n    \n    \"\"\"\n    Loading all images in a numpy array with labels\n    \n    \"\"\"\n    # creating temp array\n    image_array = []\n    labels = []\n    # directory walking started\n    for subdir, dirs, files in os.walk(path):\n        for file in files:\n            if file != []:\n                # trying to get path of each images\n                path_updated = os.path.join(subdir, file)\n                # fetching lables from directory names\n                label = subdir.split(\"\/\")[-1]\n                labels.append(label)\n                # Converting image & resizing it\n                img = Image.open(path_updated).convert('L')\n                img = img.resize((basewidth, hsize), Image.ANTIALIAS)\n                frame = asarray(img)\n                # appending array of image in temp array\n                image_array.append(frame)\n                \n    # Now i have to convert this images to array channel format which can be done using zero matrix\n    # creating a dummy zero matrix of same shape with single channel\n    \n    image_array1 = np.zeros(shape=(np.array(image_array).shape[0], hsize, basewidth,  1))\n    for i in range(np.array(image_array).shape[0]):\n        # finally each sub matrix will be replaced with respective images array\n        image_array1[i, :, :, 0] = image_array[i]\n    \n    return image_array1, np.array(labels)\n\nimage_array, labels = load_image_from_folder(path, 48, 48)\nimage_array_test, labels_test = load_image_from_folder(path_test, 48, 48)","272cb6d9":"\ndef vis_training(hlist, start=1):\n    \n    \"\"\"\n    This function will help to visualize the loss, val_loss, accuracy etc.\n    \n    \"\"\"\n    # getting history of all kpi for each epochs\n    loss = np.concatenate([hlist.history['loss']])\n    val_loss = np.concatenate([hlist.history['val_loss']])\n    acc = np.concatenate([hlist.history['accuracy']])\n    val_acc = np.concatenate([hlist.history['val_accuracy']])\n    epoch_range = range(1,len(loss)+1)\n    \n    # Block for training vs validation loss\n    plt.figure(figsize=[12,6])\n    plt.subplot(1,2,1)\n    plt.plot(epoch_range[start-1:], loss[start-1:], label='Training Loss')\n    plt.plot(epoch_range[start-1:], val_loss[start-1:], label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.legend()\n    # Block for training vs validation accuracy\n    plt.subplot(1,2,2)\n    plt.plot(epoch_range[start-1:], acc[start-1:], label='Training Accuracy')\n    plt.plot(epoch_range[start-1:], val_acc[start-1:], label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend()\n    plt.show()","0a8cebb9":"# Since this is a small dataset so i need to generate some data increase accuracy\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n","ceb60afd":"# Converting labels in int format as TF accepts only int in targets class\nlabels[labels == 'Covid'] = 0\nlabels[labels == 'Normal'] = 1\nlabels[labels == 'Viral Pneumonia'] = 2\n\nlabels_test[labels_test == 'Covid'] = 0\nlabels_test[labels_test == 'Normal'] = 1\nlabels_test[labels_test == 'Viral Pneumonia'] = 2\n# Converting to int format\nlabels = labels.astype('int')\nlabels_test = labels_test.astype('int')\n\n# Adding extra data from datagen\ntrain_gen = datagen.flow(image_array, labels)\ntest_gen = datagen.flow(image_array_test, labels_test)\n\n################################ Data Prep ended ###############################################################","31b908df":"##################################### model building started ########################################################\ndef model_seq():\n    \"\"\"\n    This decorator will help to create a CNN model.\n    \n    \"\"\"\n    # just clearing the session\n    tf.keras.backend.clear_session()\n    model=Sequential()\n\n    # Adding Conv layer  \n    model.add(Conv2D(filters=32, kernel_size = (3,3), activation=\"relu\", input_shape=(48, 48, 1)))\n    # Added max pooling layer\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    # Added normalization layer\n    model.add(BatchNormalization())\n    model.add(Conv2D(filters=64, kernel_size = (3,3), activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(BatchNormalization())\n    \n    # Added flatten option\n    model.add(Flatten())\n    model.add(BatchNormalization())\n    model.add(Dense(128,activation=\"relu\"))\n    # Adding dropout of 20%\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(Dense(3, activation=\"softmax\"))\n\n    optimiser = tf.keras.optimizers.Adam()\n    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), optimizer=optimiser, metrics=[\"accuracy\"])\n    # Printing summary of layers\n    print(model.summary())\n\n    return model","1b578ee7":"\nclass TerminateOnBaseline(Callback):\n\n    \"\"\"Callback that terminates training when either acc or val_acc reaches a specified baseline\n    \"\"\"\n    def __init__(self, monitor='val_accuracy', baseline=0.9):\n        super(TerminateOnBaseline, self).__init__()\n        self.monitor = monitor\n        self.baseline = baseline\n\n    def on_epoch_end(self, epoch, logs=None):\n        logs = logs or {}\n        acc = logs.get(self.monitor)\n        if acc is not None:\n            if acc >= self.baseline:\n                print('Epoch %d: Reached baseline, terminating training' % (epoch))\n                self.model.stop_training = True\n\n#  Setting callbacks i) Whenever it sees convergence reduce the learning rate ii) Once it reaches to desired val_accuracy\n# terminates the training\ncallbacks = [\ntf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',min_lr=0.00001,factor=0.2, patience=1, verbose=1),\nTerminateOnBaseline(monitor='val_accuracy', baseline=0.95)]","7c7434f6":"# Model training\nmodel_covid = model_seq()\n\n# Running model training\nbatch_size = 8\ntrain_log = model_covid.fit(\n    train_gen,\n    validation_data = test_gen,\n    epochs=200,\n    batch_size = batch_size,\n    callbacks = callbacks\n    )","460f3e8c":"# Visuals of loss and accuracy\nvis_training(train_log, start=1)","60873339":"# **Importing Libraries**","245c9d8c":"# **Model building block**","e5dbcf66":"# **Model training block**","ce8e372a":"# **Data Prep Blocks**"}}