{"cell_type":{"0ed361bb":"code","10f78281":"code","ac0c2ccc":"code","62a8e5ba":"code","89c9f0d9":"code","ae948804":"code","8560c876":"code","e7e59a11":"code","05f61044":"markdown","41c55abb":"markdown","e504cef6":"markdown","d5e0ba83":"markdown","4c213599":"markdown"},"source":{"0ed361bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport re\nimport nltk\nimport time\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport gensim.parsing.preprocessing as gpp\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","10f78281":"import gensim\nprint(\"List of stopwords:\\n\")\nprint(gensim.parsing.preprocessing.STOPWORDS)","ac0c2ccc":"# Custom function for counting words in a list of text data\ndef count_words(mylist):\n    w = 0\n    for items in mylist.iteritems():\n        w += len(items[1].split())\n    return w\n\n# Map POS tag to first character lemmatize() accepts\ndef get_wordnet_pos(word):\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)","62a8e5ba":"tweets = pd.read_csv(\"\/kaggle\/input\/tweets-with-keyword-lockdown-in-apriljuly-2020\/tweets_lockdown.csv\",\n                     index_col=0)\n\nwords_start = count_words(tweets[\"Text\"])\n\nprint(\"Number of rows: {}\".format(tweets.shape[0]))\nprint(\"Number of words: {}\".format(words_start))\ntweets.head(5)","89c9f0d9":"# Removing links and ampersand attached text from the tweets\ntext_list = [re.sub(r\"(?:\\@|\\&|http)\\S+\", \"\", item) for item in tweets[\"Text\"]]\n\n# Removing numeric data from tweets\ntext_list = [gpp.strip_numeric(item) for item in text_list]\n\n# Removing non alphanumeric characters from tweets\ntext_list = [gpp.strip_non_alphanum(item) for item in text_list]\n\n# Removing punctuation symbols from tweets\ntext_list = [gpp.strip_punctuation(item) for item in text_list]\n\n# Stripping short words with length less than minsize\ntext_list = [gpp.strip_short(item, minsize=2) for item in text_list]\n\n# Convert everything to lowercase\ntext_list = [item.lower() for item in text_list]\n\n# Removing stopwords from the text based on gensim stopwords list\ntext_list = [gpp.remove_stopwords(item) for item in text_list]\n\n# Alternate unverified method to remove everything except text and numbers\n# text_list[\"text\"] = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', item) for item in text_list[\"text\"]]\n\ntweets_text = pd.DataFrame(text_list, columns=[\"Text\"])\ntweets_text.dropna(inplace=True)\nprint(\"Number of rows: {}\".format(tweets_text.shape[0]))\nwords_cleaning = count_words(tweets_text[\"Text\"])\nprint(\"Number of words: {}\".format(words_cleaning))\nprint(\"Number of words removed in text cleaning: {}\".format(words_start-words_cleaning))\n\ntweets_text.head(5)","ae948804":"# custom_words are words to not remove while checking in english dictionary\n# For example, the word \"lockdown\" is not in the dictionary\ncustom_words = [\"lockdown\"]\n\nprint(\"\\nRunning lemmatization ...\\n\")\n\nstart = time.time()\nlemma = nltk.wordnet.WordNetLemmatizer()\nwords = set(nltk.corpus.words.words())\n\n# Adding custom words in the dictinary so they are not removed\nfor i in custom_words:\n    words.add(i)\n\ntweets_ll = []\nfor item in tweets_text[\"Text\"]:\n    word_list = item.split()\n\n    # Lemmatization of each word \n    word_list = [lemma.lemmatize(x, get_wordnet_pos(x)) for x in word_list]\n\n    # Checking for words in nltk english dictionary\n    word_list = [x for x in word_list if x in words]\n    tweets_ll.append(\" \".join(word_list))\n    word_list = None\n\nend = time.time()\ntweets_final = pd.DataFrame(tweets_ll, columns=[\"Text\"])\ntweets_final.dropna(inplace=True)\nprint(\"Total time taken in lemmatization: {:.2f} seconds\".format(end-start))","8560c876":"print(\"Number of rows: {}\".format(tweets_final.shape[0]))\nwords_lemmatized = count_words(tweets_final[\"Text\"])\nprint(\"Number of words: {}\".format(words_lemmatized))\nprint(\"Number of words removed in lemmatization: {}\".format(words_cleaning-words_lemmatized))\ntweets_final.head(5)","e7e59a11":"tweets_final.to_csv(\"tweets_cleaned_text.csv\")\ntweets_final.head(5)","05f61044":"## Text lemmatization and removing non-dictionary words","41c55abb":"## Getting data from the csv file","e504cef6":"## Saving to output file (if needed)","d5e0ba83":"# Introduction\n\nThis notebook illustrates how I went through cleaning the twitter text data. Used gensim ([https:\/\/pypi.org\/project\/gensim\/](https:\/\/pypi.org\/project\/gensim\/)) and nltk ([https:\/\/pypi.org\/project\/nltk\/](https:\/\/pypi.org\/project\/nltk\/)) libraries.","4c213599":"## Basic text cleaning\n\nUsing the gensim library ([https:\/\/pypi.org\/project\/gensim\/](https:\/\/pypi.org\/project\/gensim\/)) and its functions to clean the data as much as possible."}}