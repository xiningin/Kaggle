{"cell_type":{"717bd261":"code","def83870":"code","16c73913":"code","c31c4e0f":"code","f06035be":"code","fc04cad1":"code","7b25e75f":"code","d279f901":"code","a4f17078":"code","6aecefcd":"code","45c4f6e3":"code","9553cc26":"code","327f51b2":"code","d6837f6f":"code","7f35587c":"code","ff1e1e4c":"code","a2a19607":"code","51af1005":"code","49dddf22":"code","98e4c579":"code","ed0a21d1":"code","38713c2a":"code","8f1eac91":"code","fc83a4c4":"code","a349ee72":"code","7b92d9dc":"code","36240e20":"code","1df7abe6":"code","1de98878":"code","af21dddf":"markdown","480bdc77":"markdown","20cf1053":"markdown","4a804d3c":"markdown","47cad600":"markdown","7724849a":"markdown","8aa88344":"markdown","e178d5ef":"markdown","46c222fa":"markdown","6a4df64e":"markdown","549670b9":"markdown","0bfd786f":"markdown","2c21e6fb":"markdown","df607a40":"markdown","56acff71":"markdown","d8cf9824":"markdown"},"source":{"717bd261":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom xgboost import XGBRegressor\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","def83870":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","16c73913":"train.head()","c31c4e0f":"train.shape","f06035be":"# Public test data: 200,000 rows, private test data: 600,000 rows.\ntest.shape # no target column.\n","fc04cad1":"# plot colors\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","7b25e75f":"# Comparing dataset lengths\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","d279f901":"# Statistical overview of the train dataset\ntrain.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T","a4f17078":"# Checking if there are missing values in the datasets\ntrain.isna().sum().sum(), test.isna().sum().sum()","6aecefcd":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"palevioletred\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","45c4f6e3":"print(f\"{(train['target'] < 5).sum() \/ len(train) * 100:.3f}% of the target values are less than 5\")","9553cc26":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(train[\"target\"],\n               bins=3500,\n               range=(6.9,10.4),\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","327f51b2":"pd.DataFrame(train[\"target\"]).plot(kind='density', figsize=(20,14), xlim=(6.7, 10.5))\n","d6837f6f":"# Lists of categorical and numerical feature columns\ncat_features = [\"cat\" + str(i) for i in range(10)]\nnum_features = [\"cont\" + str(i) for i in range(14)]","7f35587c":"# plot continuous features values distribution\n\n# Combined dataframe containing numerical features only\ndf = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n        \nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.suptitle(\"Numerical feature values distribution in both datasets\")\nplt.show();","ff1e1e4c":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(train[\"cont2\"],\n               bins=3500,\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Cont2 distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Cont2 value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","a2a19607":"fig, ax = plt.subplots(figsize=(24, 12))\n\nbars = ax.hist(train[\"cont1\"],\n               bins=3500,\n               color=\"orange\",\n               edgecolor=\"orange\")\nax.set_title(\"Cont1 distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Cont1 value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","51af1005":"# plot categorical feature values distribution\n\n# Combined dataframe containing categorical features only\ndf = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n\n            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n            bars_pos = np.arange(0, len(values))\n            if len(values)<4:\n                height=0.1\n            else:\n                height=0.3\n\n            bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"teal\",\n                                   edgecolor=\"black\",\n                                   label=\"Train Dataset\")\n            bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"salmon\",\n                                   edgecolor=\"black\",\n                                   label=\"Test Dataset\")\n            y_labels = [str(x) for x in values]\n\n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_xlim(0, len(train[\"id\"])+50)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.suptitle(\"Categorical feature values distribution in both datasets\")\nplt.show();","49dddf22":"# Bars position should be numerical because there will be arithmetical operations with them\nbars_pos = np.arange(len(cat_features))\n\nwidth=0.3\nfig, ax = plt.subplots(figsize=(14, 6))\n# Making two bar objects. One is on the left from bar position and the other one is on the right\nbars1 = ax.bar(bars_pos-width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"darkorange\", edgecolor=\"black\")\nbars2 = ax.bar(bars_pos+width\/2,\n               test[cat_features].nunique().values,\n               width=width,\n               color=\"steelblue\", edgecolor=\"black\")\nax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\nax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\nax.set_xticks(bars_pos)\nax.set_xticklabels(cat_features, fontsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","98e4c579":"# Checking if test data doesn't contain categories that are not present in the train dataset\nfor col in cat_features:\n    print(set(train[col].value_counts().index) == set(test[col].value_counts().index))","ed0a21d1":"# Plot dataframe\ndf = train.drop(\"id\", axis=1)\n\n# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","38713c2a":"columns = train.drop([\"id\", \"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3) # distance between plots\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            train = train.sort_values(columns[i], ascending = True)\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"target\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n        \nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.suptitle(\"Features vs target\")\nplt.show();","8f1eac91":"columns = [col for col in train.columns if col.startswith(\"cat\")]\n\ncols = 2\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(18,36), sharex=False)\nfig.tight_layout()\nfig.subplots_adjust(top=0.96)\nplt.suptitle(\"Boxplots of Categorical Features vs Target\")\nplt.subplots_adjust(hspace = 0.3) # distance between plots\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            var = columns[i]\n            data = pd.concat([train['target'], train[var]], axis=1)\n            fig = sns.boxplot(x=var, y=\"target\", data=data.sort_values(var), ax=axs[r,c])\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n\nplt.show();\n\n# box plot for cat8\/target alone:\n\n# var = 'cat8'\n# data = pd.concat([train['target'], train[var]], axis=1)\n# f, ax = plt.subplots(figsize=(8, 6))\n# fig = sns.boxplot(x=var, y=\"target\", data=data.sort_values(var))\n# fig.axis();","fc83a4c4":"# helper functions:\n\nfrom math import ceil\n\n# So as not to have so much repetitive code between plots . . .\ndef plot(data, cols, features_type, nrows, ncols, bins='auto', target=None, figsize=None,\n         hspace=None, wspace=None, color = None):\n    '''plot all features vs target or the distribution of features'''\n    if figsize != None:\n        plt.figure(figsize = figsize)\n    for col, plot_num in zip(cols, list(range(1, len(cols)+1))):\n        plt.subplot(nrows, ncols, plot_num)\n        if hspace != None or wspace != None:\n            plt.subplots_adjust(hspace = hspace, wspace = wspace)\n            \n        if features_type == 'numerical':\n            if target != None:\n                plt.scatter(data[col], data[target])\n                plt.title(col)\n            else:\n                sns.histplot(data[col], bins=bins)\n                \n        if features_type == 'categorical':\n            if target != None:\n                sns.violinplot(data=data, y=col, x=target, color=color, inner='quartile');\n            else:\n                countplot_ratio(x = col, data = data, color = color)\n                \n# function to plot the distribution of categorical variable \n# since the countplot function show the counts of observations in each categorical bin using bars.\ndef countplot_ratio(x = None, data = None, hue = None, ax = None, color = None):\n    # plot the variable\n    ax = sns.countplot(x, data = data, hue = hue, ax = ax, color = color)\n    # names of x labels\n    ax.set_xticklabels(ax.get_xticklabels())\n    # plot title\n    ax.set_title(x + \" Distribution\")\n    # total number of data which used to get the proportion\n    total = float(len(data))\n    # for loop to iterate on the patches\n    for patch in ax.patches:\n        # get the height of the patch which represents the number of observations.\n        height = patch.get_height()\n        # Put text on each patch with the proportion of the observations\n        ax.text(patch.get_x()+patch.get_width()\/2,height+4,'{:.1f}%'.format((height\/total)*100),weight = 'bold',\n                fontsize = 10,ha = 'center', va='bottom')","a349ee72":"n_cols = 4\nn_rows = ceil(len(num_features)\/n_cols)\nbins = np.arange(0, 1.3, 0.02)\nplot(data=train, cols=num_features, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3, wspace=0.5, bins=bins,\n    figsize = (15, 15))","7b92d9dc":"import warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\nn_cols = 2\nn_rows = ceil(len(cat_features)\/n_cols)\nbase_color = sns.color_palette(n_colors=2)[1]\nplot(data=train, cols=cat_features, features_type='categorical', nrows=n_rows, ncols=n_cols,\n     hspace=0.3, figsize = (15, 25), color=base_color)","36240e20":"n_cols = 3\nn_rows = ceil(len(cat_features)\/n_cols)\nplot(data=train, target='target', cols=cat_features, features_type='categorical',\n     nrows=n_rows, ncols=n_cols, hspace=0.5, figsize = (15, 20), color=base_color)","1df7abe6":"def make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","1de98878":"from sklearn.feature_selection import mutual_info_regression\n\nX = df.copy()\ny = X.pop(\"target\")\n\nmi_scores = make_mi_scores(X, y)\nmi_scores","af21dddf":"As you can see, target column is very weakly correlated with all features.\n\nLet's visualize each feature vs target.","480bdc77":"Here are a few more graphs adapted from [Mohammed Al Sayed's notebook](https:\/\/www.kaggle.com\/mohammadghanaym\/30-ml-eda-xgboost-kfold) that are different ways of visualing (also note the utility functions that reduce code repetition):","20cf1053":"Nearly all values fall between 6.9 to 10.4. Let's take a closer look, by restricting range to where most of the data is at, and increasing the number of bins:","4a804d3c":"# **Conclusions**\nFrom exploratory data analysis, we've so far learned that:\n\n* The data set is perfectly clean\n* The data is well-balanced between the training and test sets\n* Histograms reveal many data spikes in continuous variables and the target\n* The target has very weak correlations with any individual feature - worth formally exploring if any features can be dropped\n* Cat4 has no usuful information according to mutual information analysis. Can probably drop this column.\n\nAfter this EDA Part 1 notebook are 5 more:\n\nIn [30dToML: EDA Part 2 - target analysis](https:\/\/www.kaggle.com\/filterjoe\/fork-of-30dtoml-eda-part-2-target-analysis) we look much more closely at the 7 spikes in the target and see if there's anything we can do with this to build better models.\n\nIn [30dToML: Boruta-SHAP Feature Selection](https:\/\/www.kaggle.com\/filterjoe\/30dtoml-boruta-shap-feature-selection) we use Boruta-SHAP to decide which features to drop.\n\nIn [30dML: hyperparameter tuning with optuna](https:\/\/www.kaggle.com\/filterjoe\/30dml-hyperparameter-tuning-with-optuna) we create\/select features and then use optuna to tune the hyperpameters for XGBoost.\n\nIn [30dML: Predict with Optimized XGBoost](https:\/\/www.kaggle.com\/filterjoe\/30dml-predict-with-optimized-xgboost) we again create\/select the same features, then use the best set of hyperparameters generated from optuna tuning to plug in the final predictions, which we can then submit when done.\n\nIn [blending blending blending](https:\/\/www.kaggle.com\/filterjoe\/blending-blending-blending) we use Abhishek Thakur's notebook to blend together two XGBoost models for a slightly better score. Same set of features were created for first model. No features created on the 2nd model.\n\nOptionally, we can post process the submission using the method discussed in EDA Part 2 for a slight improvement in results.","47cad600":"# **Data import**","7724849a":"Cont2 has many spikes but seems almost random. Cont1 on the other hand has some distinctive gaps and patterns to it, with a similar number of massive spikes as the target. There might or might not be a relationship between Cont1 spikes and target spikes. Perhaps there's a way to explore possible relationships between the spikes of Cont1 and target. But lacking ideas for how to do that exploration, let's move on to a look at the categorical features:","8aa88344":"Let's check if the datasets have different amount of categories in categorical features.","e178d5ef":"No missing values in either datase. No data cleaning needed (but note that real life data typically requires extensive, time consuming cleaning).\n\nLet's check target distribution.","46c222fa":"Continuing with overview EDA . . .\n\nThe dataset contains categorical and numerical values. Let's see values distribution for these categories.","6a4df64e":"According to the mutual information measure, Cat4 has no usual information and can probably be dropped. Later we'll select features in a more sophisticated manner using Boruta SHAP analysis. We'll again find that there Cat4 is not worth keeping. I have run some experiments dropping cat_4 while keeping everything else the same, and did not notice significant change in model accuracy outcomes.","549670b9":"So the datasets are pretty well balanced. Let's look at feature correlation.","0bfd786f":"# EDA part 1 - General Analysis (fork from Maxim Kazantsev, overfitting league)\n\nThis fork uses the EDA portion of Maxim Kazantsev's EDA + Xboost notebook as base for a general overview EDA, with enhancements:\n\n* Additional comments\n* Mutual importance scores\n* Extra frequency (higher bin count) histograms of target and some continuous features\n* Box Plots for categorical features\n* Fixed minor formatting issues on a few plots (ordering the category values on each subplot is what mattered most)\n\nThe frequency histogram of target with 3500 bins merits additional analysis. So . . .\n\n[30dToML: EDA Part 2 - target analysis](https:\/\/www.kaggle.com\/filterjoe\/fork-of-30dtoml-eda-part-2-target-analysis) explores target anomalies, and explores ways to deal with the anomolies to improve model results.","2c21e6fb":"The frequency distribution of target values (chopped into .001-wide bins) shows huge spikes, followed by cliff-like drops, with frequency immediately after the cliffs being quite a bit lower than before the cliffs.\n\nWhat is going on here?\n\nThis is explored in depth in [30dToML: EDA Part 2 - target analysis](https:\/\/www.kaggle.com\/filterjoe\/fork-of-30dtoml-eda-part-2-target-analysis).\n\nNote that the 7 cliff-like structures in the above histogram are not at all obvious in a density plot:","df607a40":"# **EDA**","56acff71":"The histogram of target values for some of the continuous variables looks a bit odd, so let's look at a couple of the continuous features in finer detail like we did with the target (especially cont1 which looks like it has gaps):","d8cf9824":"# **Mutual Information**"}}