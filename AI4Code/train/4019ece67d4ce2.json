{"cell_type":{"e260e1e6":"code","00e69df9":"code","9eb93c2b":"code","0372a48c":"code","ff3813a2":"code","21bdfae0":"code","7ec07708":"code","1003b725":"code","815cad41":"code","d8d4fb2c":"code","ca378a2c":"code","5d9ccb08":"code","01ed2117":"code","05f1cbc1":"code","f1ccc04c":"code","300294a7":"code","bbdfce53":"code","1c14b42c":"code","4fa791e7":"code","c8fb5b9d":"code","6f39263b":"code","f7e93b82":"code","10a4ceee":"code","263e5c87":"code","fb6ec138":"markdown","2fd5428f":"markdown","bc2ab25f":"markdown","fc5efb9c":"markdown","c573d4f0":"markdown","26c8a055":"markdown","b37d5e6b":"markdown","0bfde8fc":"markdown","7b6aa290":"markdown","2989db5c":"markdown","b05ab7bc":"markdown","93c4e191":"markdown","dba4024b":"markdown","64fd9449":"markdown","60a4e342":"markdown","e4ee0bfd":"markdown","10566e87":"markdown","3c93d649":"markdown","32216668":"markdown","f7db5665":"markdown","d46f34ed":"markdown","f32ca728":"markdown","f23635fd":"markdown"},"source":{"e260e1e6":"import pandas as pd\nimport pickle\nimport os\nimport numpy as np\nimport scipy\nimport math\nimport random\nimport sklearn\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity, linear_kernel\nfrom scipy.sparse.linalg import svds\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom spacy.lang.en import English\nimport spacy\nimport re\nimport en_core_web_sm\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom nltk.corpus import stopwords","00e69df9":"stop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])\ntoken_pos = ['NOUN', 'VERB', 'PROPN', 'ADJ', 'INTJ', 'X']","9eb93c2b":"data_folder = \"..\/input\/\"\nprofessionals = pd.read_csv(os.path.join(data_folder, 'professionals.csv'))\ntag_users = pd.read_csv(os.path.join(data_folder, 'tag_users.csv'))\nstudents = pd.read_csv(os.path.join(data_folder, 'students.csv'))\ntag_questions = pd.read_csv(os.path.join(data_folder, 'tag_questions.csv'))\ngroups = pd.read_csv(os.path.join(data_folder, 'groups.csv'))\nemails = pd.read_csv(os.path.join(data_folder, 'emails.csv'))\ngroup_memberships = pd.read_csv(os.path.join(data_folder, 'group_memberships.csv'))\nanswers = pd.read_csv(os.path.join(data_folder, 'answers.csv'))\nanswer_scores = pd.read_csv(os.path.join(data_folder, 'answer_scores.csv'))\ncomments = pd.read_csv(os.path.join(data_folder, 'comments.csv'))\nmatches = pd.read_csv(os.path.join(data_folder, 'matches.csv'))\ntags = pd.read_csv(os.path.join(data_folder, 'tags.csv'))\nquestions = pd.read_csv(os.path.join(data_folder, 'questions.csv'))\nquestion_scores = pd.read_csv(os.path.join(data_folder, 'question_scores.csv'))\nschool_memberships = pd.read_csv(os.path.join(data_folder, 'school_memberships.csv'))","0372a48c":"#get questions dataframe\n#looking at the data, we realize that some questions are asked by professionals, \n#so we also need to get data from the professionals data frame in this context\nprof_author = professionals.rename( columns = {\n    'professionals_id':'author_id',\n    'professionals_location': 'author_location',\n    'professionals_date_joined': 'author_date_joined'\n})\nstudents_author = students.rename( columns = {\n    'students_id':'author_id',\n    'students_location': 'author_location',\n    'students_date_joined': 'author_date_joined'\n})\nauthors = students_author.append(prof_author, sort=True)\nauthors.drop(['professionals_headline', 'professionals_industry'], axis=1, inplace=True)\n\n#join questions with students\ndf = pd.merge(questions, authors, how='left',\n                   left_on=['questions_author_id'],\n                   right_on=['author_id'])\n#merge with question tags\ntag_merge = pd.merge (tag_questions, tags, how='left',\n                     left_on=['tag_questions_tag_id'], right_on=['tags_tag_id'])\nq_tags = tag_merge.groupby('tag_questions_question_id')['tags_tag_name'].apply(lambda x: pd.unique(x.values)).rename(\"question_tags\").reset_index()\ndf_2 = pd.merge(df, q_tags, how='left', left_on=['questions_id'], right_on=['tag_questions_question_id'])\n\n","ff3813a2":"#get professionals dataframe\n#looking at the data, we realize that some questions are actually answered by Students, so we need to get them here as well\nprof_answerers = professionals.rename( columns = {\n    'professionals_id':'answerers_id',\n    'professionals_location': 'answerers_location',\n    'professionals_date_joined': 'answerers_date_joined',\n    'professionals_headline' : 'answerers_headline',\n    'professionals_industry' : 'answerers_industry'\n})\nstudents_answerers = students.rename( columns = {\n    'students_id':'answerers_id',\n    'students_location': 'answerers_location',\n    'students_date_joined': 'answerers_date_joined'\n})\nanswerers = students_answerers.append(prof_answerers, sort=True)\nprof1 = pd.merge (answerers, answers, how='outer',\n                     left_on=['answerers_id'], right_on=['answers_author_id'])\n#merge prof tags\ntag_merge = pd.merge (tag_users, tags, how='left',\n                     left_on=['tag_users_tag_id'], right_on=['tags_tag_id'])\np_tags = tag_merge.groupby('tag_users_user_id')['tags_tag_name'].apply(lambda x: pd.unique(x.values)).rename(\"prof_tags\").reset_index()\n\nprof2 = pd.merge(prof1, p_tags, how='left', left_on=['answerers_id'], right_on=['tag_users_user_id'])\n\n#merge group types\ngroup_cats = pd.merge (group_memberships, groups, how='left',\n                     left_on=['group_memberships_group_id'], right_on=['groups_id'])\ngroup_cats = group_cats.groupby('group_memberships_user_id')['groups_group_type'].apply(lambda x: pd.unique(x.values)).rename(\"group_type\").reset_index()\n\nprof3 = pd.merge(prof2, group_cats, how='left', left_on=['answerers_id'], right_on=['group_memberships_user_id'])\n\n#merge school counts\nschool_counts = school_memberships.groupby(['school_memberships_user_id']).size().reset_index(name='school_count')\nprof4 = pd.merge(prof3, school_counts, how='left', left_on=['answerers_id'], right_on=['school_memberships_user_id'])","21bdfae0":"#join question and answers\nquestion_answer = pd.merge(df_2, prof4, how='left', left_on=['questions_id'], right_on=['answers_question_id'])\n\n#join answer scores\nans_scores = answer_scores.rename(columns={'id': 'answer_scores_id', 'score':'answer_score'})\nquestion_answer_2 = pd.merge(question_answer, ans_scores, how='left', left_on=['answers_id'], right_on=['answer_scores_id'])\n\n#join question scores\nqtn_scores = question_scores.rename(columns={'id': 'question_scores_id', 'score':'question_score'})\nquestion_answer_3 = pd.merge(question_answer_2, qtn_scores, how='left', left_on=['questions_id'], right_on=['question_scores_id'])\n","7ec07708":"#get profs who haven't answered any questions\nprof_with_no_answers_1 = pd.merge (prof4, answers, how='left',\n                     left_on=['answerers_id'], right_on=['answers_author_id'], indicator=True)\nprof_with_no_answers = prof_with_no_answers_1[prof_with_no_answers_1.answerers_headline.notnull()]\nprof_with_no_answers = prof_with_no_answers[['answerers_id', 'answerers_headline', 'answerers_industry', 'group_type', 'school_count', 'prof_tags']]\nprof_with_no_answers.drop_duplicates(subset='answerers_id', inplace=True)\nprof_with_no_answers.reset_index(inplace=True, drop=True)\nprof_with_no_answers.head(5)","1003b725":"features = question_answer_3.copy()\n#combine question title and body into one text\nfeatures['questions_full_text'] = features['questions_title'] +  \" \" + features['questions_body']\n#Replace Nan with an empty string\nfeatures['questions_full_text'] = features['questions_full_text'].fillna('')\n","815cad41":"def pre_process(data):\n    # Function to pre_proces the data and convert to data works. \n    \n    # Remove Emails\n    data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n    # Remove new line characters\n    data = [re.sub('\\s+', ' ', sent) for sent in data]\n    # Remove distracting single quotes\n    data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n    data = [re.compile(r'<[^>]+>').sub('', x) for x in data] #Remove HTML-tags\n    \n    def sent_to_words(sentences):\n        for sentence in sentences:\n            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\n    data_words = list(sent_to_words(data))\n    return data_words\n    ","d8d4fb2c":"def build_models(data_words, min_count, threshold):\n    # Function to build bigram and trigram models. \n    # Parameters: \n    # data_words - data_words returned by the pre_processing function used above\n    # min_count - minimum setting threshold to create the phrase. \n    # threshold - Represent a score threshold for forming the phrases (higher means fewer phrases). \n    #             A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold\n    \n    # Build the bigram and trigram models\n    bigram = gensim.models.Phrases(data_words, min_count=min_count, threshold=threshold) # higher threshold fewer phrases.\n    trigram = gensim.models.Phrases(bigram[data_words], threshold=threshold)\n\n    # Faster way to get a sentence clubbed as a trigram\/bigram\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n    return bigram_mod, trigram_mod","ca378a2c":"def nlp_tokenization (data_words, bigram_mod, trigram_mod):\n    #Function to create tokens based on the data words and models created so far\n    \n   # Define functions for stopwords, bigrams, trigrams and lemmatization\n    def remove_stopwords(texts):\n        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\n    def make_bigrams(texts):\n        return [bigram_mod[doc] for doc in texts]\n\n    def make_trigrams(texts):\n        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\n    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n        #https:\/\/spacy.io\/api\/annotation\"\"\"\n        def token_filter(token):\n            #Keep tokens who are alphapetic, in the pos (part-of-speech) list and not in stop list\n            return not token.is_stop and token.is_alpha and token.pos_ in token_pos\n        texts_out = []\n        for sent in texts:\n            doc = nlp(\" \".join(sent))\n            filtered_tokens = [token.lemma_.lower() for token in doc if token_filter(token)]\n            texts_out.append([token.lemma_.lower() for token in doc if token.pos_ in allowed_postags])\n        return texts_out\n    # Remove Stop Words\n    data_words_nostops = remove_stopwords(data_words)\n\n    # Form Bigrams\n    data_words_bigrams = make_bigrams(data_words_nostops)\n\n    # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n    # python3 -m spacy download en\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n\n    # Do lemmatization keeping only noun, adj, vb, adv and remove stop words\n    processed_tokens = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n    return processed_tokens","5d9ccb08":"#get data_words for all questions texts\ndata_words = pre_process(features.questions_full_text.values.tolist())\n#create models using the data_words. Params can be adjusted if needed\nbigram_mod_questions, trigram_mod_questions = build_models(data_words, 2, 20)\n#create tokens\nfeatures['nlp_questions_tokens'] = nlp_tokenization(data_words, bigram_mod_questions, trigram_mod_questions)","01ed2117":"def get_similar_items(features, token_list, new_text, bigram_mod, trigram_mod):\n    #function to get similar items based on new_text \n    #features - dataframe to merge the final recommended list\n    #token_list - list of tokens to compare the new_text\n    #new_text - new item to compare\n    #bigram_mod - name of the bigram model used\n    #trigram_mod - name of the trigram model used\n    \n    #get tokens into a list\n    nlp_items_corpus = [' '.join(x) for x in token_list]\n    new_data_words = pre_process(new_text)\n    nlp_new_item_corpus = [' '.join(x) for x in nlp_tokenization(new_data_words, bigram_mod, trigram_mod)]\n    \n    #vectorize the tokens\n    vectorizer = TfidfVectorizer()\n    vectorizer.fit(nlp_items_corpus)\n    items_tfidf = vectorizer.transform(nlp_items_corpus)\n    new_item_tfidf = vectorizer.transform(nlp_new_item_corpus)\n    \n    #find a cosine similarity between new item and the items list\n    sim = cosine_similarity(items_tfidf, new_item_tfidf)\n    sim_df = pd.DataFrame({'similarity': sim[:,0]})\n    \n    #merge with features dataset and return\n    item_sim = features.join(sim_df)\n    return item_sim","05f1cbc1":"new_question_text = ['How do I learn clinical data science?']\nsimilar_questions = get_similar_items(features, features['nlp_questions_tokens'], new_question_text, bigram_mod_questions, trigram_mod_questions)\n#sort for items with highest similarity\nitem_sim = similar_questions.sort_values('similarity', ascending=False)\nitem_sim['similarity'].head(5)","f1ccc04c":"#Append all text attributes into one field\nfeatures_prof = prof_with_no_answers.copy()\nfeatures_prof['prof_full_text'] = features_prof['answerers_headline'].astype(str) + \" \" + \\\n    features_prof['answerers_industry'].astype(str) + \" \" + \\\n    features_prof['group_type'].astype(str) +  \" \" + \\\n    features_prof ['school_count'].astype(str) +  \" \" + \\\n    features_prof['prof_tags'].astype(str)\nfeatures_prof['prof_full_text'] = features_prof['prof_full_text'].fillna('')","300294a7":"data_words = pre_process(features_prof.prof_full_text.values.tolist())\nbigram_mod_prof, trigram_mod_prof = build_models(data_words, 2, 20)\nfeatures_prof['nlp_prof_tokens'] = nlp_tokenization(data_words, bigram_mod_prof, trigram_mod_prof)","bbdfce53":"new_prof_text = ['Enterprise Risk Management - Technology Risk Information Technology and Services nan nan information-technology-and-services leadership team-leadership career-development risk risk-management cybersecurity']\nsimilar_profs = get_similar_items(features_prof, features_prof['nlp_prof_tokens'], new_prof_text, bigram_mod_prof, trigram_mod_prof)\nitem_sim = similar_profs.sort_values('similarity', ascending=False)\nitem_sim['similarity'].head(5)","1c14b42c":"#create a new feature - time taken to answer a question\nsimilar_questions['questions_added_datetime'] = pd.to_datetime(similar_questions['questions_date_added'], infer_datetime_format=True)\nsimilar_questions['answers_added_datetime'] = pd.to_datetime(similar_questions['answers_date_added'], infer_datetime_format=True)\nsimilar_questions['time_to_answer'] = similar_questions['answers_added_datetime'] - similar_questions['questions_added_datetime']\nsimilar_questions['time_to_answer'].head(5)","4fa791e7":"similar_questions = similar_questions.sort_values(['similarity', 'answer_score', 'time_to_answer'], ascending=[False, False, True])\n#Get Top 5 professionals. Please update this number as you see fit. \nRecommended_profs = similar_questions.head(5)","c8fb5b9d":"Recommended_profs['prof_full_text'] = Recommended_profs['answerers_headline'].astype(str) + \" \" + \\\n    Recommended_profs['answerers_industry'].astype(str) + \" \" + \\\n    Recommended_profs['group_type'].astype(str) +  \" \" + \\\n    Recommended_profs ['school_count'].astype(str) +  \" \" + \\\n    Recommended_profs['prof_tags'].astype(str)\nRecommended_profs['prof_full_text'] = Recommended_profs['prof_full_text'].fillna('')","6f39263b":"def get_similar_profs(new_prof_text, return_count):\n    #function to get similar profs for model 2. \n    #new_prof_text - new prof text to find similarities\n    #return_count - number of new professionals to return\n    new_prof_list = new_prof_text.split()\n    similar_profs = get_similar_items(features_prof, features_prof['nlp_prof_tokens'], new_prof_list, bigram_mod_prof, trigram_mod_prof)\n    item_sim = similar_profs.sort_values('similarity', ascending=False).head(return_count)\n    return item_sim","f7e93b82":"Recommended_not_answered_profs = pd.DataFrame()\nfor new_prof_text in (Recommended_profs['prof_full_text']):\n    temp = get_similar_profs(new_prof_text, 5)\n    Recommended_not_answered_profs = Recommended_not_answered_profs.append(temp)","10a4ceee":"#Append both dataframes to get final recommendations\nFinal_recs = Recommended_profs.append(Recommended_not_answered_profs, sort=False)","263e5c87":"Final_recs[['answerers_id', 'answerers_headline', 'answerers_industry', 'group_type', 'school_count', 'prof_tags']]","fb6ec138":"Standard tokenization typically breaks words into single word token and interprets meaning thereafer. \n\nFor example - a question such as 'How do I learn Clinical Data Science' - gets split into individual words as 'Clinical', 'Data' and 'Science'. Therefore it tries and assigns this question to a Clinical Physician or a Scientist. In this instance the model should combine the relevant words into one single token such as 'Clinical-Data-Science' to assign to the appropriate professional. \n\nUsing gensim models it combines relevant phrases comprised of two or three words pairs into single tokens, two or three word pairs to avoid the issue above.  \n\nParameters in the below function can be adjusted as needed. ","2fd5428f":"**Create models based on the available questions data**","bc2ab25f":"Sort the similar questions in this order to get the recommended professionals list - \n\n1. Similarity - Descending\n2. answer_score - Descending\n3. time_to_answer - Ascending (quickest to answer)","fc5efb9c":"The system not only identifies the best professionals based on their answers so far, it also identifies professionals who have never answered any questions. The goal here is to encourage new professionals to be engaged in the system much more thereby contributing to the environment. \n\nThe steps are described as follows - \n\n1. When a new question comes in from a user, it is sent to the the Rec Sys 1. \n2. Rec Sys 1 - This system identifies all questions similar to the new question. It considers the question title, body and tags to identify similar questions already answered in the database. \n3. It then identifies professionals who have best answered these similar questions. The professionals are identified using the similarity score, answer score and time to answer. \n3. The identified professionals are then passed to another recommendation system - Rec Sys 2. This system searches in a subset of professionals who have never answered a question, and identifies the most similar in that group. \n4. Combine the output from both systems - a. best professionals from the group of answers and b. best professionals who have never answered a question so far\n\nBy providing a recommendation of professionals who have already answered similar question, this system gives a better chance at the question being answered quickly. However, by also providing a recommendation of professionals who haven't answered questions so far, this system provides a way to engage with those users and get them to answer more questions by leveraging instances where past professionals have answered a question. ","c573d4f0":"<a id='2. Data Munging'><\/a>\n> **Data Munging**\n\nWe thought it works best to combine all datasets and relevant fields into one big dataframe. There are two major data objects - Questions and Professionals. We will combine the relevant dataframes into one big **question** dataframe, and similarly do the same for the **professionals** dataframe and then merge them together for our main **features** dataframe. ","26c8a055":"<a><\/a>","b37d5e6b":"<a id='4. Recommendation System #2'><\/a>\n> **Recommendation System #2** \n\nUse a similar workflow as above to build models for recommendation system #2. Here we will try to find similar professional given the attributes of a professional in the system.\n\nPlease note that we are only using a subset of the data with professionals who havent answered any questions so far","0bfde8fc":"**Table of Contents**\n\n<a href='#1. Introduction'>1. Introduction<\/a> <br\/>\n<a href='#2. Data Munging'>2. Data Munging<\/a> <br\/>\n<a href='#3. Recommendation System #1'>3. Recommendation System #1<\/a> <br\/>\n<a href='#4. Recommendation System #2'>4. Recommendation System #2<\/a> <br\/>\n<a href='#5. Combine both Recommendation Systems'>5. Combine both Recommendation Systems<\/a> <br\/>\n","7b6aa290":"**Example inference using a new professional text**","2989db5c":"<a id='3. Recommendation System #1'><\/a>\n> Recommendation System #1 \n\nBuild a model and recommendation system to get a new question and identify similar questions, and thereby professionals who best answered those similar questions. ","b05ab7bc":"**Imports and set parameters**","93c4e191":"**Join questions and answers**","dba4024b":"**Get similar questions**\n\nFunction to get similar questions based on a new question","64fd9449":"**Lets also get a list of professionals who haven't answered any questions to use in <a href='#4. Recommendation System #2'> Recommendation System #2.<\/a> <br\/>**","60a4e342":"<a id='1. Introduction'><\/a>\n\n> **Introduction**\n\nBelow is our approach to solve the recommendation problem using two recommendation systems. The workflow is in the diagram below. \n\n![Image](https:\/\/i.imgur.com\/vqGHNfX.jpg)","e4ee0bfd":"**Build models using this dataset**","10566e87":"**Get Professionals dataframe**","3c93d649":"**Merge for questions dataframe**","32216668":"**Run a loop on all Recommended Professionals to identify similar professionals in the not answered pool**","f7db5665":"**Process Steps**\n\n1. Pre Process data\n2. Build Models\n3. NLP Tokenization and identify similarity","d46f34ed":"**Summary**\n\nWith the following approach we hope to accommplish two goals:\n    1. Leverage historical data to match new questions to professionals based on question similarity.\n    2. Increase the likelihood for new users to start contributing within the community by providing them questions that have a high frequency of answers based on similar professional profiles.\n    \nLastly, we hope this approach will both allow for an efficient implementation into production quickly as well as provide a way to greatly reduce the number of false positive matches between questions and professionals.","f32ca728":"**Example inference with a new question**","f23635fd":"<a id='5. Combine both Recommendation Systems'><\/a>\n> **Combine both recommendation systems**\n\nGet a list of recommended professionals from RecSys #1, and use that to identify a list of professionals using Rec sys #2"}}