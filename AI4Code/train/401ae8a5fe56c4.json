{"cell_type":{"2472dfe5":"code","d4efc176":"code","8370b132":"code","3afdfe81":"code","a9be75ce":"code","d22a3c97":"code","6f50a47f":"code","ddaf5142":"code","af177c45":"code","f507d035":"code","87d0ec16":"code","ebc50d84":"code","bb386ec3":"code","9a775b09":"code","c41ed41b":"code","74b81e7e":"code","eae7f432":"code","7d5783b1":"code","e0dd1c58":"code","5c51dcab":"code","ad353b94":"code","180245a9":"code","6bfc5f71":"code","d725f99c":"code","987695c6":"code","04994a02":"code","0061aac0":"code","2f949dab":"code","3b800a3f":"code","0b234c2a":"code","9f470043":"code","2f4b3b96":"code","be8917f5":"code","1629d4ee":"code","8ff1f970":"code","0f5f9192":"markdown","f03b5a18":"markdown","eeceb8c1":"markdown","7517ac4d":"markdown","23c6dced":"markdown","f7eb523e":"markdown","318e617b":"markdown","53c1ee2f":"markdown","dcc92a1c":"markdown","f9516dcf":"markdown","9c5e104a":"markdown","51f68374":"markdown","fc9e1e44":"markdown","11bdc124":"markdown","f163e648":"markdown","8a17ad03":"markdown","1accda31":"markdown","77396ce8":"markdown","b98ebe8f":"markdown","c4906aab":"markdown","81971406":"markdown","c6fd1cb5":"markdown","339d7226":"markdown","0f9e376d":"markdown","4ca88b75":"markdown","bbac786a":"markdown","821f8f48":"markdown","a1b23fa6":"markdown","bdaf7610":"markdown","a018a37b":"markdown","0d146028":"markdown","fafb74e8":"markdown","6293efc4":"markdown","c649812d":"markdown","28621b43":"markdown"},"source":{"2472dfe5":"!pip install xgboost --upgrade","d4efc176":"# A continuaci\u00f3n importaremos varias bibliotecas que se utilizar\u00e1n:\n\n# Biblioteca para trabajar con JSON\nimport json\n\n# Biblioteca para realizar solicitudes HTTP\nimport requests\n\n# Biblioteca para exploraci\u00f3n y an\u00e1lisis de datos\nimport pandas as pd\n\n# Biblioteca con m\u00e9todos num\u00e9ricos y representaciones matriciales\nimport numpy as np\n\n# Biblioteca para construir un modelo basado en la t\u00e9cnica Gradient Boosting\nimport xgboost as xgb\n\n# Paquetes scikit-learn para preprocesamiento de datos\n# \"SimpleImputer\" es una transformaci\u00f3n para completar los valores faltantes en conjuntos de datos\nfrom sklearn.impute import SimpleImputer\n\n# Paquetes de scikit-learn para entrenamiento de modelos y construcci\u00f3n de pipelines\n# M\u00e9todo para separar el conjunto de datos en muestras de testes y entrenamiento\nfrom sklearn.model_selection import train_test_split\n# M\u00e9todo para crear modelos basados en \u00e1rboles de decisi\u00f3n\nfrom sklearn.tree import DecisionTreeClassifier\n# Clase para crear una pipeline de machine-learning\nfrom sklearn.pipeline import Pipeline\n\n# Paquetes scikit-learn para evaluaci\u00f3n de modelos\n# M\u00e9todos para la validaci\u00f3n cruzada del modelo creado\nfrom sklearn.model_selection import KFold, cross_validate","8370b132":"   \nimport itertools\n%matplotlib inline\ndef plot_confusion_matrix(y_true, y_pred, class_names,title=\"Confusion matrix\",normalize=False,onehot = False, size=4):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n\n    Args:\n    cm (array, shape = [n, n]): a confusion matrix of integer classes\n    class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    if onehot :\n        cm = confusion_matrix([y_i.argmax() for y_i in y_true], [y_ip.argmax() for y_ip in y_pred])\n    else:\n        cm = confusion_matrix(y_true, y_pred)\n    figure = plt.figure(figsize=(size, size))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n\n    # Normalize the confusion matrix.\n    cm = np.around(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], decimals=2) if normalize else cm\n\n    # Use white text if squares are dark; otherwise black.\n    threshold = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = \"red\" if cm[i, j] > threshold else \"black\"\n        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    #return figure","3afdfe81":"df_to_be_scored = pd.read_csv(r'..\/input\/tech-students-profile-prediction\/to_be_scored_tortuga.csv')\ndf_to_be_scored.head()","a9be75ce":"# Primero, importaremos el conjunto de datos proporcionado para el desaf\u00edo, que ya est\u00e1 incluido en este proyecto.\n\n#!wget --no-check-certificate --content-disposition https:\/\/raw.githubusercontent.com\/maratonadev-la\/desafio-2-2020\/master\/Assets\/Data\/dataset-tortuga-desafio-2.csv\ndf_training_dataset = pd.read_csv('..\/input\/tech-students-profile-prediction\/dataset-tortuga.csv')\ndf_training_dataset.tail(10)","d22a3c97":"df_training_dataset.isnull().sum()","6f50a47f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ddaf5142":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_training_dataset['HOURS_DATASCIENCE'].dropna(), ax=axes[0])\nsns.distplot(df_training_dataset['HOURS_BACKEND'].dropna(), ax=axes[1])\nsns.distplot(df_training_dataset['HOURS_FRONTEND'].dropna(), ax=axes[2])","af177c45":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(28, 8))\n\nsns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_DATASCIENCE'].dropna(), ax=axes[0][0] )\nsns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_BACKEND'].dropna(), ax=axes[0][1] )\nsns.distplot(df_training_dataset['NUM_COURSES_BEGINNER_FRONTEND'].dropna(), ax=axes[0][2])\nsns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_DATASCIENCE'].dropna(), ax=axes[1][0])\nsns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_BACKEND'].dropna(), ax=axes[1][1])\nsns.distplot(df_training_dataset['NUM_COURSES_ADVANCED_FRONTEND'].dropna(), ax=axes[1][2])","f507d035":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(28, 4))\n\nsns.distplot(df_training_dataset['AVG_SCORE_DATASCIENCE'].dropna(), ax=axes[0])\nsns.distplot(df_training_dataset['AVG_SCORE_BACKEND'].dropna(), ax=axes[1])\nsns.distplot(df_training_dataset['AVG_SCORE_FRONTEND'].dropna(), ax=axes[2])","87d0ec16":"fig, axes = plt.subplots(figsize=(28, 4))\n\nsns.countplot(ax=axes, x='PROFILE', data=df_training_dataset)\ndf_training_dataset['PROFILE'].value_counts()","ebc50d84":"df_train = df_training_dataset.drop(columns = [ 'Unnamed: 0', 'USER_ID','NAME' ])","bb386ec3":"from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\ndef fill_categorical_valuesUsingNumerical_rfc(data, aux_table, columns2fill, columnsBase_num,columnsBase_cat=None,max_depth=8):\n    \"\"\"\n    columnsBase: es una lista de columnas num\u00e9ricas\n    columns2fill: es una lista de columnas categ\u00f3ricas\n    aux_table: tabla sin nulos\n    data: data a completar\n    \"\"\"\n    aux_table = aux_table.reset_index(drop=True).copy()\n    data = data.copy()\n#     for ind , table in enumerate(data):\n#         table['Genero'] = table['Genero'].replace('0', np.nan)\n#         data[ind] = table\n    models= defaultdict(list)\n    \n    encoder = OneHotEncoder(handle_unknown='ignore')\n    def fill_nan_categorical(x, model,column,columnsBase_num,mean_values,columnsBase_cat,mode_values):\n        if x[column] == x[column]: ## solo deben entrar los valores nulos\n            return x[column] \n        valores2look = [x[col_name] if x[col_name] == x[col_name] else mean_values[col_name]  for col_name in columnsBase_num]\n        #valores2look = valores2look\n        valores2look = np.stack(valores2look).reshape(1, -1)\n        #print(valores2look)\n        return model.predict(valores2look )[0] \n    \n    def fill_nan_categorical_catBase(x, model,column,columnsBase_num,mean_values,columnsBase_categ,mode_values,encoder):\n        if x[column] == x[column]: ## solo deben entrar los valores nulos\n            return x[column] \n        ## Tomando los valores n\u00famericos para la predicci\u00f3n\n        #print(columnsBase_num)\n        valores2look = []\n        if len(columnsBase_num)>0:\n            valores2look = [x[col_name] if x[col_name] == x[col_name] else mean_values[col_name]  for col_name in columnsBase_num]\n        if len(columnsBase_categ)> 0:\n            ## Tomando los valores categ\u00f3ricos para la predicci\u00f3n\n            valores2look_cat = [x[col_name] if x[col_name] == x[col_name] else mode_values[col_name]  for col_name in columnsBase_categ]\n            ##     Transformando con el encoder\n            valores2look_cat = onehot_transfor(encoder, valores2look_cat)\n            ############ SUMANDO LAS DOS LISTAS DE VALORES\n            valores2look = valores2look + valores2look_cat\n        #ADAPTANDO AL FORMATO DE ENTRADA DEL CLASIFICADOR\n        valores2look = np.stack(valores2look).reshape(1, -1)\n        #print(valores2look)\n        value = model.predict(valores2look)[0]\n        return value if type(value)== str else round(value) \n    \n    def upsampling_classes(X, Y , column):\n        df_train_umb = X.join(Y).copy()\n        count4bal = df_train_umb[column].value_counts().sort_values(ascending=True)\n        class_sorter = count4bal.index.tolist()[:-1]\n        class_mayor = count4bal.index[-1]  ##  clase mayoritaria\n        mayority_sample = count4bal[-1]  ## cantidad de muestras en la clase mayoritaria\n        df_train_balanced = df_train_umb.loc[df_train_umb[column]==class_mayor]\n        for class_i in class_sorter:\n            df_minor_upsampled = resample(df_train_umb.loc[df_train_umb[column]==class_i], \n                                     replace=True,     # sample with replacement\n                                     n_samples=mayority_sample,    # to match majority class\n                                     random_state=17) # reproducible results\n            df_train_balanced = pd.concat([df_train_balanced,df_minor_upsampled ],ignore_index=True)\n\n        return df_train_balanced.drop(columns=[column]), df_train_balanced[column]\n    \n    def onehot_transfor(encoder, lista):\n        output = encoder.transform(np.stack([lista[0]]).reshape(-1, 1)).toarray()\n        if len(lista)>1:\n            for cat in lista[1:]:\n                output = output +  encoder.transform(np.stack([cat]).reshape(-1, 1)).toarray()\n            return output[0].tolist()\n        else:\n            return output[0].tolist()\n        \n    def best_model(X, Y,column):\n        #display(X.columns)\n        if type(Y[0]) == object or type(Y[0]) == str:\n            #X, Y = upsampling_classes(X, Y , column)\n            clf_rfr = GridSearchCV(estimator=RandomForestClassifier(random_state=25, n_jobs=-1), \n                        param_grid=[{'n_estimators':[10,20,100,120],'criterion':['entropy','gini']}], # 'max_depth':[None,20], \n                        scoring='f1_macro', n_jobs=-1, cv=5)\n            clf_rfr.fit(X,Y)\n            print(clf_rfr.best_params_)########\n            return clf_rfr\n        else: \n            clf_rfr = GridSearchCV(estimator=RandomForestRegressor(random_state=25, n_jobs=-1), \n                        param_grid=[{ 'n_estimators':[100,120],'criterion':['mse','mae']}], #'max_depth':[None,],\n                        scoring='r2', n_jobs=-1, cv=KFold(n_splits=5) )\n            clf_rfr.fit(X,Y)\n            print(clf_rfr.best_params_)########\n            return clf_rfr\n            \n    ### MODO SOLO NUM\u00c9RICO COMO BASE ####################################################\n    columnsBase = columnsBase_num.copy() + columnsBase_cat.copy()\n     ### LLENANDO VALORES USANDO EL CLASSIFICADOR ADD DOC\n    mean_values = aux_table[columnsBase_num].mean()\n    mode_values = aux_table[columnsBase_cat].mode() if columnsBase_cat else None\n    for column in tqdm(columns2fill):\n        if column in columnsBase:\n            columnsBase_cat_ad = columnsBase_cat.copy()\n            columnsBase_num_ad = columnsBase_num.copy()\n            if column in columnsBase_cat:\n                columnsBase_cat_ad.remove(column)\n            if column in columnsBase_num:\n                columnsBase_num_ad.remove(column)\n\n            if len(columnsBase_cat_ad)>0:\n                base_cat_list = np.stack([str(item)+str(ind)   for ind , column in enumerate(columnsBase_cat_ad) for item in np.unique(aux_table[column].to_numpy())]).reshape(-1, 1)\n                ##print(base_cat_list)\n                encoder.fit(base_cat_list)\n\n            aux_column_set = columnsBase.copy()\n            aux_column_set.remove(column)\n            model = best_model(pd.get_dummies(aux_table[aux_column_set]),aux_table[column],column)\n            ##print(pd.get_dummies(aux_table[aux_column_set]).columns.tolist())\n            models[column] = model.best_score_\n            ### RELLENANDO DATOS\n            for ind, table in enumerate(data):\n                table[column] = table.apply(fill_nan_categorical_catBase, args=(model,column,columnsBase_num_ad,mean_values,columnsBase_cat_ad,mode_values,encoder ), axis=1)\n                data[ind] = table\n        else:\n            base_cat_list = np.stack([str(item)+str(ind)   for ind , column in enumerate(columnsBase_cat)    for item in np.unique(aux_table[column].to_numpy())]).reshape(-1, 1)\n            #print(base_cat_list)\n            encoder.fit(base_cat_list)\n            model = best_model(pd.get_dummies(aux_table[columnsBase]),aux_table[column],column)\n            #print(pd.get_dummies(aux_table[columnsBase]).columns.tolist())\n            models[column] = model.best_score_\n            #models[column] = model\n            ### RELLENANDO DATOS\n            for ind, table in enumerate(data):\n                table[column] = table.apply(fill_nan_categorical_catBase, args=(model,column,columnsBase_num,mean_values,columnsBase_cat,mode_values,encoder ), axis=1)\n                data[ind] = table\n    display(models)\n    return data[0], data[1]","9a775b09":"df_temp_aux = df_train.dropna().reset_index(drop=True).copy()","c41ed41b":"df_train.isnull().sum()","74b81e7e":"columnsBase_num = ['HOURS_DATASCIENCE', 'HOURS_BACKEND', 'HOURS_FRONTEND',\n       'NUM_COURSES_BEGINNER_DATASCIENCE', 'NUM_COURSES_BEGINNER_BACKEND',\n       'NUM_COURSES_BEGINNER_FRONTEND', 'NUM_COURSES_ADVANCED_DATASCIENCE',\n       'NUM_COURSES_ADVANCED_BACKEND', 'NUM_COURSES_ADVANCED_FRONTEND',\n       'AVG_SCORE_DATASCIENCE', 'AVG_SCORE_BACKEND', 'AVG_SCORE_FRONTEND' ]\ncolumnsBase_cat = []\n#columnsBase_num = []\ncolumns2bfilled = df_train.columns.tolist()\n# columns2bfilled.remove('Banca_movil_userfriendly')\n# columns2bfilled.remove('Frecuencia_tarjeta_virtual_mes')\ncolumns2bfilled.remove('PROFILE')\ndf_train_fill,df_test_fill = fill_categorical_valuesUsingNumerical_rfc(data = [df_train,df_to_be_scored], aux_table = df_temp_aux.copy(), \n                                                              columns2fill=columns2bfilled, columnsBase_num=columnsBase_num,\n                                                              columnsBase_cat=columnsBase_cat )\ndf_train_fill.isnull().sum()","eae7f432":"#df_train_fill,df_test_fill \ncolumns_int = ['NUM_COURSES_BEGINNER_DATASCIENCE',\n 'NUM_COURSES_BEGINNER_BACKEND',\n 'NUM_COURSES_BEGINNER_FRONTEND',\n 'NUM_COURSES_ADVANCED_DATASCIENCE',\n 'NUM_COURSES_ADVANCED_BACKEND',\n 'NUM_COURSES_ADVANCED_FRONTEND',]\ndf_train_fill[columns_int] = df_train_fill[columns_int].apply(lambda x: round(x,0)).astype(int)","7d5783b1":"from sklearn.utils import resample\ncount4bal = df_train_fill['PROFILE'].value_counts().sort_values(ascending=True)\nclass_sorter = count4bal.index.tolist()[:-1]\nclass_mayor = count4bal.index[-1]\nmayority_sample = count4bal[-1]\ndf_balanced = df_train_fill.loc[df_train_fill['PROFILE']==class_mayor]\nfor class_i in class_sorter:\n    df_minor_upsampled = resample(df_train_fill.loc[df_train_fill['PROFILE']==class_i], \n                             replace=True,     # sample with replacement\n                             n_samples=mayority_sample,    # to match majority class\n                             random_state=17) # reproducible results\n\n    df_balanced = pd.concat([df_balanced,df_minor_upsampled ],ignore_index=True)\ndf_balanced['PROFILE'].value_counts()","e0dd1c58":"def add_new_features(data):\n    def horasxarea(x):\n        total_backend = x['NUM_COURSES_BEGINNER_BACKEND'] + x['NUM_COURSES_ADVANCED_BACKEND'] \n        total_ds      = x['NUM_COURSES_BEGINNER_DATASCIENCE'] + x['NUM_COURSES_ADVANCED_DATASCIENCE']\n        total_frontend= x['NUM_COURSES_ADVANCED_FRONTEND'] + x['NUM_COURSES_BEGINNER_FRONTEND']\n\n        HR_A_DS = round(x['HOURS_DATASCIENCE']\/total_ds) if total_ds != 0 else 0\n        HR_A_BE = round(x['HOURS_BACKEND']\/total_backend) if total_backend != 0 else 0\n        HR_A_FE = round(x['HOURS_FRONTEND']\/total_frontend)if total_frontend != 0 else 0\n\n        SCORE_HR_A_DS =round( x['AVG_SCORE_DATASCIENCE']\/HR_A_DS )if HR_A_DS !=0 else 0\n        SCORE_HR_A_BE = round(x['AVG_SCORE_BACKEND']\/HR_A_BE )    if HR_A_BE !=0 else 0\n        SCORE_HR_A_FE = round(x['AVG_SCORE_FRONTEND']\/HR_A_FE )   if HR_A_FE !=0 else 0\n\n        return pd.Series([total_backend,total_ds,total_frontend,HR_A_DS, HR_A_BE, HR_A_FE,SCORE_HR_A_DS,SCORE_HR_A_BE,SCORE_HR_A_FE], index = ['NUM_CURS_BE','NUM_CURS_DS', 'NUM_CURS_FE', 'HR_A_DS', 'HR_A_BE', 'HR_A_FE','SCORE_HR_A_DS','SCORE_HR_A_BE','SCORE_HR_A_FE'])\n\n    return data.join(data.apply(horasxarea, axis=1))\n    \ndf_balanced_improve =  add_new_features(df_balanced.copy())\ndf_balanced_improve.describe()","5c51dcab":"df_balanced_improve.groupby(['PROFILE']).mean()","ad353b94":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\nn_components = 12 #n_components\nscaler = StandardScaler()#MinMaxScaler()\n# scaler.fit(df_nca[[i for i in range(n_components)]])\n# df_nca[[i for i in range(n_components)]] = scaler.transform(df_nca[[i for i in range(n_components)]])\n# df_nca.head()\ncolumns = df_balanced_improve.columns.tolist()\ncolumns.remove('PROFILE')\n\nscaler.fit(df_balanced_improve.drop(columns=['PROFILE']))\ndf_balanced_improve[columns] = scaler.transform(df_balanced_improve[columns])\ndf_balanced_improve.head()","180245a9":"class raw2test():\n    def __init__(self, columns_int, scaler, auggfunc):\n        self.columns_int = columns_int\n        self.scaler = scaler\n        self.auggfunc = auggfunc\n        \n    def transform(self, X):\n        X = X.drop(columns = [ 'Unnamed: 0', 'USER_ID','NAME' ])\n        columns = X.columns.tolist()\n        #X[columns] = self.knn_imputer.transform(X) \n        X[self.columns_int] = X[self.columns_int].apply(lambda x: round(x,0)).astype(int)\n        X = self.auggfunc(X)\n        X = self.scaler.transform(X)\n        return X       #test_knn_imputer\npretest = raw2test(columns_int =columns_int, scaler =  scaler,auggfunc=add_new_features)","6bfc5f71":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV , RandomizedSearchCV,StratifiedKFold\n#from imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier,ExtraTreesClassifier, StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n#from sklearn import svm\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.decomposition import PCA\n#from sklearn.neighbors import NeighborhoodComponentsAnalysis as NCA\nfrom sklearn.neighbors import KNeighborsClassifier as KNC\nfrom collections import defaultdict\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nbest_parameters = defaultdict(list) # 'xgb': XGBClassifier(),\nclassifiers = defaultdict(list) # 'xgb': XGBClassifier(), X_train_whole,y_train_whole\nX_train_whole       = df_balanced_improve.drop(columns=['PROFILE']).to_numpy()\ny_train_whole       = df_balanced_improve['PROFILE']\n\nX_train, X_test, y_train, y_test = train_test_split(X_train_whole,y_train_whole, stratify = y_train_whole, random_state= 17,test_size = 0.2 )\n","d725f99c":"##NOTA:\n# El metodo df_catdumm (cat+PCA+scalar) no sirve, entrega un pesimo rendimiento        F1 = 0.64\n# EL metodo df_cat (usar bandas -10 para agrupar valores) entreg\u00f3 un rendimiento medio F1=0.81\n# EL m\u00e9todo df_balanced entreg\u00f3 un rendimiento de F1 = 0.93\n# X_train       = df_balanced.drop(columns=['PROFILE'])\n# y_train       = df_balanced['PROFILE']\n\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n# Parameters to tune\n# SVC\ntuned_parameters_svc = [{'penalty': ['l2','l1'], 'loss': ['squared_hinge','hinge'],'C': [0.1,1.0,10,100], 'max_iter': [4000], 'random_state':[15], 'dual':[False]}]\n# XGB\ntuned_parameters_xgb = [{'learning_rate':[0.2,0.3,0.4,0.5],'n_estimators':[140,160,180,220,250,300],'min_child_weight':[.01],'subsample':[.4,.5,.8,1.0],'colsample_bytree':[.5,.8,1.0],\n                    'objective':['multi:softmax','binary:logistic'],'n_jobs':[-1],'random_state':[15] }]\n\n#KNC\ntuned_parameters_knc = [{'n_neighbors':[2,4,6,8,10,12,14,16],'n_jobs':[-1],'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],'p':[1,2]}]\n\n#RFC\ntuned_parameters_rfc = [{'n_estimators':[100,120,140,180],'criterion':['gini','entropy'],'min_samples_split':[2,3,4,5],'n_jobs':[-1],'random_state':[15]}]\n\n#GBC\ntuned_parameters_gbc = [{'loss':['deviance','exponential'],'learning_rate':[0.1,0.2,0.3,0.4],'n_estimators':[160,190,220,250,270],'subsample':[1.0],'criterion':[ 'mse', 'friedman_mse'],'max_depth':[3,8,10],'random_state':[15]}]\n\n#ETC\ntuned_parameters_etc = [{ 'min_samples_split':[.2,.4,.8], 'n_estimators':[80,120,150,180,200,250],'warm_start':[True],'bootstrap':[True],\n                         'n_jobs':[-1], 'random_state':[15], 'min_samples_leaf':[3,4,5,6] ,'criterion':['gini', 'entropy'],'max_features':[ 'sqrt']   }]\n\n#LR\ntuned_parameters_lr = [{'C':[0.1,1.0,10.0], 'dual':[False], 'solver':['newton-cg', 'saga','sag'],'multi_class':['ovr', 'multinomial']}]\n\n# Parameter tunning\nscores = ['f1']\n\n# Best parameters\n#  'lr': LR(random_state=17), 'svc':LinearSVC(), 'knc': KNC(),\nmodels = { 'xgb': XGBClassifier(),  'rfc':RandomForestClassifier(), 'etc':ExtraTreesClassifier() ,'gbc':GradientBoostingClassifier() }\nparameters = {'lr':tuned_parameters_lr, 'xgb': tuned_parameters_xgb, 'knc': tuned_parameters_knc,'rfc':tuned_parameters_rfc,'svc':tuned_parameters_svc, 'gbc':tuned_parameters_gbc, 'etc':tuned_parameters_etc }\n\n#;ista = ['etc']\nfor model_name in models.keys():\n    print(\"######### MODEL tunning hyper-parameters for %s\" % model_name)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s ###############################################################\" % (model_name, score))\n        clf_i = GridSearchCV(models[model_name], parameters[model_name], scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters[model_name] = clf_i.best_params_\n        classifiers[model_name] = clf_i\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4 ))","987695c6":"best_parameters = defaultdict(list,\n            {'xgb': {'colsample_bytree': 1.0,\n              'learning_rate': 0.4,\n              'min_child_weight': 0.01,\n              'n_estimators': 300,\n              'n_jobs': -1,\n              'objective': 'multi:softmax',\n              'random_state': 15,\n              'subsample': 0.8},\n             'rfc': {'criterion': 'gini',\n              'min_samples_split': 2,\n              'n_estimators': 140,\n              'n_jobs': -1,\n              'random_state': 15},\n             'etc': {'bootstrap': True,\n              'criterion': 'gini',\n              'max_features': 'sqrt',\n              'min_samples_leaf': 3,\n              'min_samples_split': 0.2,\n              'n_estimators': 250,\n              'n_jobs': -1,\n              'random_state': 15,\n              'warm_start': True},\n             'gbc': {'criterion': 'friedman_mse',\n              'learning_rate': 0.4,\n              'loss': 'deviance',\n              'max_depth': 8,\n              'n_estimators': 160,\n              'random_state': 15,\n              'subsample': 1.0},\n             'knc': {'algorithm': 'auto',\n              'n_jobs': -1,\n              'n_neighbors': 12,\n              'p': 2},\n             'svc': {'C': 0.1,\n              'dual': False,\n              'loss': 'squared_hinge',\n              'max_iter': 4000,\n              'penalty': 'l2',\n              'random_state': 15},\n             'lr': {'C': 1.0,\n              'dual': False,\n              'multi_class': 'multinomial',\n              'solver': 'newton-cg'}})","04994a02":"clf_gbc = GradientBoostingClassifier(**best_parameters['gbc'] )#n_estimators=80, n_jobs=-1)# 20 - 80\nclf_RFC = RandomForestClassifier(**best_parameters['rfc'] )#\nclf_KNC = KNC(**best_parameters['knc'] )\nxgb_model = XGBClassifier(**best_parameters['xgb'])\nsvc = LinearSVC(**best_parameters['svc'])\n#etc = ExtraTreesClassifier(**best_parameters['etc'])\n#lr  = LR(**best_parameters['lr'])\n\nmodels = {'knc': clf_KNC,'svc': svc}\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_bagging = defaultdict(list)\n#lista = ['etc']\nfor model_name in models.keys():\n    params_bagging = [{'n_estimators': [50,100], 'max_samples':[1.0],'base_estimator': [models[model_name]],'n_jobs':[-1] }]\n    print(\"######### Bagging MODEL tunning hyper-parameters for %s\" % model_name)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s #########################################################\" % (model_name, score))\n        clf_i = GridSearchCV(BaggingClassifier(), params_bagging, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_bagging[model_name] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Bagging classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4))","0061aac0":"best_parameters_bagging = defaultdict(list,\n            {'knc': {'base_estimator': KNC(**best_parameters['knc']),\n              'max_samples': 1.0,\n              'n_estimators': 50,\n              'n_jobs': -1},})","2f949dab":"xgb_f = XGBClassifier(**best_parameters['xgb'])\nKNC_f = BaggingClassifier( **best_parameters_bagging['knc'] )\nRFC_f = RandomForestClassifier(**best_parameters['rfc'] )\nsvc_f = LinearSVC( **best_parameters['svc'] )\ngbc_f = GradientBoostingClassifier(**best_parameters['gbc'] )\netc_f = ExtraTreesClassifier(**best_parameters['etc'])\nlr_f  = LR( **best_parameters['lr'] )","3b800a3f":"from sklearn.model_selection import StratifiedKFold, KFold,cross_val_score #( X_train_PCA , label)\nfrom sklearn.metrics import f1_score\n# Ir retirnando los modelos m\u00e1s deviles progresivamente\nestimators0= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('svc',svc_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]\nestimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]\nestimators2= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f)]\nestimators3= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f)] # **** ganador \nestimators4= [ ('xgb', xgb_f),('rfc',RFC_f),('gbc',gbc_f)] \nestimators5= [ ('rfc',RFC_f),('gbc',gbc_f)]\n\nestimators = [ estimators0,estimators1,estimators2,estimators3,estimators4,estimators5]\nvoting = ['hard','soft']\n\nparams_voting = [{'voting':voting,'n_jobs':[-1] }] # 'estimators': estimators, \n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_voting = defaultdict(list)\n# Scores\nscore_f1 = []\n\nprint(\"######### Voting MODEL tunning hyper-parameters for %s\" % model_name)\nfor ind , set_estimator in enumerate(estimators):\n    model_vc_i = VotingClassifier(estimators = set_estimator)\n    for score in scores:\n        print(\"# %s - Tuning hyper-parameters for %s #############################################################\" % ('Voting', score))\n        clf_i = GridSearchCV(model_vc_i, params_voting, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_voting['set'+str(ind)] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n    #         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n    #             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Bagging classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        score_f1.append(np.nan_to_num( means).max())\n        best_parameters_voting['scores'] = score_f1\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4))","0b234c2a":"estimators3= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f)] # **** ganador \n## X_train, X_test, y_train, y_test\nmodel_VC = VotingClassifier (estimators = estimators3, voting='hard',n_jobs=-1) ## editar\nmodel_VC.fit(X_train,y_train)\ny_true , y_pred =y_test,  model_VC.predict(X_test)\n\nprint(\"Detailed Voting classification report:\")\nprint(classification_report(y_true, y_pred, digits=4))\n#plot_confusion_matrix(y_true=y_train_whole, y_pred=y_pred, class_names=np.unique(y_pred),title=\"VotingClassifier\",normalize=True,size=5)","9f470043":"from sklearn.tree import DecisionTreeClassifier as DTC\ndtc = DTC()\n# Stratify Kfold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\nskf1 = StratifiedKFold(n_splits=4, shuffle=True, random_state=15)\nskf2 = StratifiedKFold(n_splits=8, shuffle=True, random_state=15)\nskf3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)\n\n# Ir retirnando los modelos m\u00e1s deviles progresivamente\nestimators0= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('svc',svc_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]\nestimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]  # **** Winner\nestimators2= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f)]\nestimators3= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f)]\nestimators4= [ ('xgb', xgb_f),('rfc',RFC_f),('gbc',gbc_f)]  \nestimators5= [ ('rfc',RFC_f),('gbc',gbc_f)]\n\n## Par\u00e1metro ganador\n##params_winer = {'cv': StratifiedKFold(n_splits=8, random_state=15, shuffle=True), 'final_estimator': SVC(), 'n_jobs': -1}\n\nestimators = [ estimators0,estimators1,estimators2,estimators3,estimators4,estimators5]\nskf_method = [skf,skf1,skf2,skf3]\n\nparams_sc = [{'cv':skf_method,'n_jobs':[-1], 'final_estimator':[ LR(random_state=17),LinearSVC(random_state=17)] }] # 'estimators': estimators, \n\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_sc = defaultdict(list)\n# Scores\nscore_sc_f1 = []\n\nprint(\"######### Stacking MODEL tunning hyper-parameters\" )\nfor ind , set_estimator in enumerate(estimators):\n    model_sc_i = StackingClassifier(estimators = set_estimator)\n    for score in scores:\n        print(\"# %s - %s Tuning hyper-parameters for %s #############################################################\" % ('Stacking',ind, score))\n        clf_i = GridSearchCV(model_sc_i, params_sc, scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n        clf_i.fit(X_train,y_train)\n        print(\"Best parameters set found on development set:\")\n        print()\n        print(clf_i.best_params_)\n        best_parameters_sc['set'+str(ind)] = clf_i.best_params_\n        print(\"Grid scores on development set:\")\n        print()\n        means = clf_i.cv_results_['mean_test_score']\n        stds = clf_i.cv_results_['std_test_score']\n    #         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n    #             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n        print(\"Detailed Staking classification report:\")\n        print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n        score_sc_f1.append(np.nan_to_num( means).max())\n        best_parameters_sc['scores'] = score_sc_f1\n        print()\n        y_true, y_pred = y_test, clf_i.predict(X_test)\n        print(classification_report(y_true, y_pred, digits=4))","2f4b3b96":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score #( X_train_PCA , label)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score\n# Stratify Kfold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)\nskf1 = StratifiedKFold(n_splits=4, shuffle=True, random_state=15)\nskf2 = StratifiedKFold(n_splits=8, shuffle=True, random_state=15)\nskf3 = StratifiedKFold(n_splits=3, shuffle=True, random_state=15)\nskf_method = [skf,skf2]\nestimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]  # **** Winner\nmodel_SC = StackingClassifier (estimators=estimators1 , final_estimator = LogisticRegression(random_state=17) )\n\nparams_SC = {'final_estimator__C': [0.1,1.0, 10.0], 'final_estimator__solver': [ 'sag', 'saga','newton-cg' ], 'final_estimator__max_iter':[200], 'cv': skf_method,'n_jobs':[-1], }\n\n# Parameter tunning\nscores = ['f1']\n# Best parameters\nbest_parameters_SC_final_estim = defaultdict(list)\n#lista = ['etc']\nfor score in scores:\n    print(\"# %s - Tuning hyper-parameters for %s\" % ('Stacking Classifier', score))\n    clf_i = GridSearchCV(estimator=model_SC, \n                    param_grid=params_SC, \n                    scoring='%s_macro' % score, n_jobs=-1, cv=skf)\n\n    clf_i.fit(X_train,y_train)\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf_i.best_params_)\n    best_parameters_SC_final_estim['SC'] = clf_i.best_params_\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf_i.cv_results_['mean_test_score']\n    stds = clf_i.cv_results_['std_test_score']\n#         for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#             print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n    print(\"Detailed Staking classification report:\")\n    print(\"CV - Results max score: {}\".format(np.nan_to_num( means).max()))\n    score_sc_f1.append(np.nan_to_num( means).max())\n    best_parameters_sc['scores'] = score_sc_f1\n    print()\n    y_true, y_pred = y_test, clf_i.predict(X_test)\n    print(classification_report(y_true, y_pred, digits=4))","be8917f5":"estimators1= [ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)]  # **** XGB optimizado + GBC optimizado\n\nmodel_sc = StackingClassifier(estimators = estimators1, final_estimator = LR(C=10.0, max_iter=200,solver = 'sag', random_state=17), cv = StratifiedKFold(n_splits=5, random_state=15, shuffle=True), n_jobs=-1)\nmodel_sc.fit(X_train,y_train)\ny_true, y_pred = y_test, model_sc.predict(X_test) ## improve 0.9723 from 0.9698 from 0.9693\nprint(\"Detailed Staking classification report:\") \nprint(classification_report(y_true, y_pred, digits=4))","1629d4ee":"model_scf = StackingClassifier(estimators = estimators1, final_estimator = LR(C=10.0, max_iter=200,solver = 'sag', random_state=17), cv = StratifiedKFold(n_splits=5, random_state=15, shuffle=True), n_jobs=-1)\nmodel_scf.fit(X_train_whole,y_train_whole)\ny_true, y_pred = y_train_whole, model_scf.predict(X_train_whole)\nprint(\"Detailed Staking classification report whole data:\") \nprint(classification_report(y_true, y_pred, digits=4))","8ff1f970":"from sklearn.tree import DecisionTreeClassifier as DTC\ndtc = DTC()\n\nskf = StratifiedKFold(n_splits=10 )\n\n\nmodel_sc_l0 = StackingClassifier (estimators=[ ('tree', dtc),('svc',svc_f)], final_estimator = LR(random_state=17, C=1.0, solver='sag'), cv = skf,n_jobs=-1)\n\nmodel_SC = StackingClassifier (estimators=[ ('xgb', xgb_f), ('knc', KNC_f),('rfc',RFC_f),('svc',svc_f),('gbc',gbc_f),('etc',etc_f),('lr', lr_f)], final_estimator = model_sc_l0, cv = skf,n_jobs=-1)\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X_train_whole,y_train_whole, stratify = y_train_whole, random_state= 25,test_size = 0.2 )\n\nmodel_SC.fit(X_train1,y_train1)\ny_pred = model_SC.predict(X_test1)\n\nprint('F1 Score for Stacking model = {}'.format(f1_score(y_test1, y_pred, average='macro')))\nprint(\"Detailed Staking classification report whole data:\") \nprint(classification_report(y_test1, y_pred, digits=4))\n#plot_confusion_matrix(y_true=y_true, y_pred=y_pred, class_names=np.unique(y_pred),title=\"Stacking Classifier\",normalize=True,size=5)","0f5f9192":"# UPsampling Data\nLa diferencia entre clases no es muy alta pero por buenas pr\u00e1cticas, vamos a nivelar las clases para que los modelos no tengan preferencias por desbalance.","f03b5a18":"### Explorando los datos proporcionados\n\nPodemos continuar la exploraci\u00f3n de los datos proporcionados con la funci\u00f3n ``info()``:","eeceb8c1":"* Stacking classifier Testing - Entrenando con el 80% de los datos","7517ac4d":"# Improved Models\n\nDependiendo de los resultados del bagging debemos elegir si quedarnos o no el modelo en bagging","23c6dced":"* Testing - Entrenando con el 80% de los datos","f7eb523e":"# Filling Nan Values\n\nVamos a llenar nos valores perdidos, agrupando los datos seg\u00fan el profile y llenando los datos con la tecnica de SKITLEARN KNN, dentro de cada clase.","318e617b":"## Bagging Classifier MODEL - GRIDSEARCH + CV","53c1ee2f":"## STACKING CLASSIFIER + GRIDSEARCH ESTIMADORES","dcc92a1c":"# STACKING CLASSIFIER | Two Layers","f9516dcf":"### Trabajando scikit-learn","9c5e104a":"columns = df_balanced_improve.columns.tolist()\ncolumns.remove('PROFILE')\n\nfig, axes = plt.subplots(nrows=round(len(columns)\/3), ncols=3, figsize=(28, 24))\nfor ax, column in zip(axes.flatten(),columns):\n    sns.distplot(df_balanced_improve[column], ax=ax )","51f68374":"# FEATURE ENGINEERING","fc9e1e44":"### Visualizaci\u00f3n (visualizations)\n\nPara ver el conjunto de datos suministrado, podemos usar las bibliotecas ``matplotlib`` y ``seaborn``:","11bdc124":"* Creando la caracteristica | CANTIDAD DE CURSOS X3 | Horas\/Especialidad X 3 | Eficiencia_en_especialidad X3","f163e648":"En proyectos de ciencia de datos destinados a construir modelos de *aprendizaje autom\u00e1tico*, o aprendizaje estad\u00edstico, es muy inusual que los datos iniciales ya est\u00e9n en el formato ideal para la construcci\u00f3n de modelos. Se requieren varios pasos intermedios de preprocesamiento de datos, como la codificaci\u00f3n de variables categ\u00f3ricas, normalizaci\u00f3n de variables num\u00e9ricas, tratamiento de datos faltantes, etc. La biblioteca **scikit-learn**, una de las bibliotecas de c\u00f3digo abierto m\u00e1s populares para *aprendizaje autom\u00e1tico* en el mundo, ya tiene varias funciones integradas para realizar las transformaciones de datos m\u00e1s utilizadas. Sin embargo, en un flujo com\u00fan de un modelo de aprendizaje autom\u00e1tico, es necesario aplicar estas transformaciones al menos dos veces: la primera vez para \"entrenar\" el modelo, y luego nuevamente cuando se env\u00edan nuevos datos como entrada para ser clasificados por este modelo.\n","8a17ad03":"from sklearn.neighbors import NeighborhoodComponentsAnalysis as NCA\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.metrics import classification_report\nfrom collections import defaultdict\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV \n# columns_improve = df_balanced_improve.columns.tolist()\n# n_components=12\n# nca = NCA(n_components=n_components,random_state=17)\n# nca.fit(df_balanced_improve.drop(columns=['PROFILE']),df_balanced_improve['PROFILE']) \n# df_nca = pd.DataFrame(data = nca.transform(df_balanced_improve.drop(columns=['PROFILE']))).join( df_balanced_improve['PROFILE'] )\n# df_nca.head()\n\nscaler_grid = MinMaxScaler()\nbest_parameters = defaultdict(list) # 'xgb': XGBClassifier(),\nX_train       = df_balanced_improve.drop(columns=['PROFILE']).to_numpy()\ny_train       = df_balanced_improve['PROFILE']\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=15)\nN_FEATURES_OPTIONS = [5,10, 12, 16,18 ]\npipe_nca = Pipeline([\n    # the reduce_dim stage is populated by the param_grid\n    ('reduce_dim', 'passthrough'),\n    ('scaler',scaler_grid ),\n    ('classify', LR(C= 10.0, dual=False, multi_class= 'multinomial', solver= 'newton-cg'))\n])\n\nparam_grid = [\n    {\n        'reduce_dim': [NCA(random_state=17)],\n        'reduce_dim__n_components': N_FEATURES_OPTIONS\n    }]\n\n# scores = ['f1']\n# for score in scores:\n#     print(\"# %s - Tuning hyper-parameters for %s\" % ('NCA n_features', score))\n# #     clf_i = GridSearchCV(pipe_nca, param_grid, scoring='%s_macro' % score, n_jobs=8, cv=skf)\n# #     clf_i.fit(X_train,y_train)\n#     print(\"Best parameters set found on development set:\")\n#     print()\n#     print(clf_i.best_params_)\n#     #best_parameters[model_name] = clf_i.best_params_\n#     print(\"Grid scores on development set:\")\n#     print()\n#     means = clf_i.cv_results_['mean_test_score']\n#     stds = clf_i.cv_results_['std_test_score']\n#     for mean, std, params in zip(means, stds, clf_i.cv_results_['params']):\n#         print(\"%s_macro - %0.3f (+\/-%0.03f) for %r\"% (score, mean, std * 2, params))\n#     print(\"Detailed classification report:\")\n#     print()\n#     y_true, y_pred = y_train, clf_i.predict(X_train)\n#     print(classification_report(y_true, y_pred, digits=4 ))","1accda31":"## Corrigiendo los datos de las columnas de valores enteros","77396ce8":"## Scaling data\nUtilizaremo sel minMax Scaler para colocar algunas caracteristicas dentro del rango de 0-1","b98ebe8f":"* Stacking Classifier - Entrenamiento final con todos los datos","c4906aab":"import seaborn as sns\n# compute correlation matrix using pandas corr() function\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots(figsize=(15,15))         # Sample figsize in inches\n#sns.heatmap(df1.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\ncorr_df = df_balanced_improve.corr(method='pearson')  # .loc[df_balanced_improve['PROFILE']=='advanced_backend']\nmatrix = np.triu(corr_df)\nhmap=sns.heatmap(corr_df,annot=True, ax=ax, mask=matrix, vmin=-.8,vmax=.8, center=0)","81971406":"# Primero, realizamos la instalaci\u00f3n de scikit-learn\n!pip install scikit-learn --upgrade","c6fd1cb5":"# Graficando las variables saneadas","339d7226":"Tenemos 16 columnas presentes en el set de datos proporcionado, 15 de las cuales son variables features (datos de entrada) y una de ellas es una variable target (que queremos que nuestro modelo va a predecir).\n\nLas variables features son:\n\n    Unnamed: 0                          - Esta columna no tiene nombre y debe ser eliminada del dataset\n    NAME                                - Nombre del estudiante\n    USER_ID                             - N\u00famero de identificaci\u00f3n del estudiante\n    HOURS_DATASCIENCE                   - N\u00famero de horas de estudio en Data Science\n    HOURS_BACKEND                       - N\u00famero de horas de estudio en Web (Back-End)\n    HOURS_FRONTEND                      - N\u00famero de horas de estudio en Web (Front-End)\n    NUM_COURSES_BEGINNER_DATASCIENCE    - N\u00famero de cursos de nivel principiante en Data Science completados por el estudiante\n    NUM_COURSES_BEGINNER_BACKEND        - N\u00famero de cursos de nivel principiante en Web (Back-End) completados por el estudiante\n    NUM_COURSES_BEGINNER_FRONTEND       - N\u00famero de cursos de nivel principiante en Web (Front-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_DATASCIENCE    - N\u00famero de cursos de nivel avanzado en Data Science completados por el estudiante\n    NUM_COURSES_ADVANCED_BACKEND        - N\u00famero de cursos de nivel avanzado en Web (Back-End) completados por el estudiante\n    NUM_COURSES_ADVANCED_FRONTEND       - N\u00famero de cursos de nivel avanzado en Web (Front-End) completados por el estudiante\n    AVG_SCORE_DATASCIENCE               - Promedio acumulado en cursos de Data Science completados por el estudiante\n    AVG_SCORE_BACKEND                   - Promedio acumulado en cursos de Web (Back-End) completados por el estudiante\n    AVG_SCORE_FRONTEND                  - Promedio acumulado en cursos de Web (Front-End) completados por el estudiante\n    \nLa variable target es:\n\n    PROFILE                             - Perfil de carrera del estudiante (puede ser uno de 6)\n    \n        - beginner_front_end\n        - advanced_front_end\n        - beginner_back_end\n        - advanced_back_end\n        - beginner_data_science\n        - advanced_data_science\n        \nCon un modelo capaz de clasificar a un alumno en una de estas categor\u00edas, podemos recomendar contenidos a los alumnos de forma personalizada seg\u00fan las necesidades de cada alumno.","0f9e376d":"## Creando nuevas variables\n\nPuedo extraer caracteristicas de los individuos:\n* En d\u00f3nde tiene su nota m\u00e1s alta\n* Donde tiene la mayor cantidad de cursos\n* D\u00f3nde tiene la mayor cantidad de horas <br>\n===> Horas\/curso = Cantidad de horas_mayor \/ Candidad de cursos_mayor <br>\n===> \u00c1rea de mayor nota         = MAX avg note","4ca88b75":"# Stacking Classifier - GridSearch final_estimator","bbac786a":"### Introducci\u00f3n","821f8f48":"# MODEL TIME","a1b23fa6":"# Voting Classifier Model + GridSerch","bdaf7610":"# Class to Transform DATA","a018a37b":"# CORR MATRIX - de una sola clase [aleatoria]\n","0d146028":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import resample\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.impute import KNNImputer\n\nclasses = df_train['PROFILE'].unique()\nclasses = classes.tolist()\ncolumns = df_train.columns.tolist()\ncolumns.remove('PROFILE')\ndf_healthy = pd.DataFrame(columns=columns)\n\nwhole_profile = df_train[['PROFILE']]\n\nfor class_i in  classes:\n    knnimputer = KNNImputer(n_neighbors=6, weights=\"uniform\")\n    feature_class = df_train.loc[df_train['PROFILE']==class_i ].drop(columns=['PROFILE'])\n    knnimputer.fit(feature_class)\n    feature_class[columns] = knnimputer.transform(feature_class)\n    df_healthy = pd.concat([df_healthy,feature_class])\ndf_healthy = df_healthy.join(whole_profile )\ndf_healthy.sort_index(inplace=True)\n\n\n##################################### Creando KNN_imputer para las muestras de Testeo ####################\ntest_knn_imputer = KNNImputer(n_neighbors=6, weights=\"uniform\")\ntest_knn_imputer.fit(df_healthy.drop(columns=['PROFILE']))\n########\ndf_healthy.head()","fafb74e8":"# MARAT\u00d3N BEHIND THE CODE 2020\n\n## DESAF\u00cdO 2: TORTUGA CODE","6293efc4":"## Drop columns\n\nPodemos borrar datos que no vinculantes con las caracteristicas del alumno\n* Unnamed: 0\n* USER_ID\n* NAME","c649812d":"# Reducci\u00f3n de dimensionalidad [n_components selection]\n\nImplementaremos el KNN para reducci\u00f3n de dimensionalidad, dada la naturaleza multiclase del problema es m\u00e1s conveniente esta alternativa, manteniendo los par\u00e1metros por default\n\n*Resultados*\nLa reducci\u00f3n de dimensionalidad no resulta util para este studio del caso, el modelo lineal no logra mejorar la puntuaci\u00f3n ","28621b43":"### Importar  un .csv a tu proyecto en IBM Cloud Pak for Data al Kernel de este notebook"}}