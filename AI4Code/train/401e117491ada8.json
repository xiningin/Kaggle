{"cell_type":{"9ab3c06f":"code","70aa2e2d":"code","9c20a693":"code","5d44e83b":"code","b4ad3f3b":"code","9eb558ed":"code","cc1d7223":"code","3742507b":"code","239ac4ba":"code","ac0cfc3e":"code","5bd061db":"code","8ceb5237":"code","6e989c51":"code","097bd1b8":"code","19a80e92":"code","9230a902":"code","128f4d93":"code","76612118":"code","32ddce24":"code","dbca70b1":"code","9bd17db5":"code","ee60e7a7":"code","dad0e07b":"code","285e1e3f":"code","62e3635d":"code","a8d9a66d":"code","b8fa891f":"code","9983bc8b":"code","1018e93d":"code","0a1375f8":"code","5c93f416":"markdown","305b8c70":"markdown","b1741dfe":"markdown","9e845bba":"markdown","307a37a7":"markdown","36327a77":"markdown","e1b428c3":"markdown","3bb2bf06":"markdown","598b38c7":"markdown","d068c8e7":"markdown","7a96d0ea":"markdown","fe897444":"markdown","b4dd5f73":"markdown","73599dcf":"markdown","bef7cefd":"markdown","fa1c8927":"markdown","5417f611":"markdown","e2b6e7d0":"markdown","50e5c463":"markdown","1eb7316c":"markdown","e576e9a8":"markdown","d486ed23":"markdown","de62e84b":"markdown","c6921b49":"markdown","9f50e9a3":"markdown","fce7ad44":"markdown","ab74fdf2":"markdown","efb9bdc7":"markdown","abbac665":"markdown","2408bba3":"markdown","a5c73c7c":"markdown","9d20608c":"markdown","d19980ab":"markdown","d3e14496":"markdown","95cbbc68":"markdown"},"source":{"9ab3c06f":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\nimport re\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport sys\nif not sys.warnoptions:\n    import os, warnings\n    warnings.simplefilter(\"ignore\") \n    os.environ[\"PYTHONWARNINGS\"] = \"ignore\" \n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\n#timer\nimport time\nfrom contextlib import contextmanager\n\n# Importing modelling libraries\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,BaggingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\npd.options.display.float_format = \"{:,.2f}\".format\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} done in {:.0f}s\".format(title, time.time() - t0))","70aa2e2d":"# Read train and test data with pd.read_csv():\ndf = pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\")","9c20a693":"df.head()","5d44e83b":"df.info()","b4ad3f3b":"df.iloc[:,2:len(df)].describe([0.01,0.1,0.25,0.5,0.75,0.99]).T","9eb558ed":"    for var in df:\n        if var != 'Exited':\n            if len(list(df[var].unique())) <= 11:\n                    print(pd.DataFrame({'Mean_Exited': df.groupby(var)['Exited'].mean()}), end = \"\\n\\n\\n\")","cc1d7223":"# Let's visualize the correlations between numerical features of the train set.\nfig, ax = plt.subplots(figsize=(12,6)) \nsns.heatmap(df.loc[:,'Surname':'Exited'].corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","3742507b":"g= sns.factorplot(x = \"Tenure\", y = \"Exited\", data = df, kind = \"bar\", size = 4)\ng.set_ylabels(\"Churn Probability\")\nplt.show()","239ac4ba":"g= sns.factorplot(x = \"Gender\", y = \"Exited\", data = df, kind = \"bar\", size = 5)\ng.set_ylabels(\"Churn Probability\")\nplt.show()","ac0cfc3e":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","5bd061db":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"Balance\", bins = 25)\nplt.show()","8ceb5237":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"EstimatedSalary\", bins = 25)\nplt.show()","6e989c51":"g= sns.FacetGrid(df, col = \"Exited\")\ng.map(sns.distplot, \"CreditScore\", bins = 25)\nplt.show()","097bd1b8":"##\nxs = df.drop(['RowNumber',\"Exited\"], axis=1)\ntarget = df[\"Exited\"]\nx_train, x_val, y_train, y_val = train_test_split(xs, target, test_size = 0.20, random_state = 0)\n\nval_ids = x_val['CustomerId']\ntrain_ids=x_train['CustomerId']\n\nx_train = x_train.drop(['CustomerId'], axis=1)\nx_val= x_val.drop(['CustomerId'], axis=1)\n\ndf_train=df[df['CustomerId'].isin(train_ids)]\ndf_val=df[df['CustomerId'].isin(val_ids)]","19a80e92":"x_train.shape","9230a902":"for df in [x_train,x_val]:\n    df[\"Gender\"]=df[\"Gender\"].map(lambda x: 0 if x=='Female' else 1)\n    df.drop(['Surname'], axis = 1, inplace=True)","128f4d93":"x_train,x_val= [ pd.get_dummies(data, columns = ['Geography']) for data in [x_train,x_val]]","76612118":"x_train.shape","32ddce24":"x_train.info()","dbca70b1":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","9bd17db5":"for df in [x_train,x_val]:\n    reduce_mem_usage(df)    ","ee60e7a7":"r=1309\nmodels = [LogisticRegression(random_state=r),GaussianNB(), KNeighborsClassifier(),\n          SVC(random_state=r,probability=True),BaggingClassifier(random_state=r),DecisionTreeClassifier(random_state=r),\n          RandomForestClassifier(random_state=r), GradientBoostingClassifier(random_state=r),\n          XGBClassifier(random_state=r), MLPClassifier(random_state=r),\n          CatBoostClassifier(random_state=r,verbose = False)]\nnames = [\"LogisticRegression\",\"GaussianNB\",\"KNN\",\"SVC\",\"Bagging\",\n             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"Art.Neural_Network\",\"CatBoost\"]","dad0e07b":"print('Default model validation accuracies for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    model.fit(x_train, y_train)\n    y_pred = model.predict(x_val) \n    print(name,':',\"%.3f\" % accuracy_score(y_pred, y_val))","285e1e3f":"predictors=pd.concat([x_train,x_val])","62e3635d":"results = []\nprint('10 fold Cross validation accuracy and std of the default models for the train data:', end = \"\\n\\n\")\nfor name, model in zip(names, models):\n    kfold = KFold(n_splits=10, random_state=1001)\n    cv_results = cross_val_score(model, predictors, target, cv = kfold, scoring = \"accuracy\")\n    results.append(cv_results)\n    print(\"{}: {} ({})\".format(name, \"%.3f\" % cv_results.mean() ,\"%.3f\" %  cv_results.std()))","a8d9a66d":"# Possible hyper parameters\nlogreg_params= {\"C\":np.logspace(-1, 1, 10),\n                    \"penalty\": [\"l1\",\"l2\"], \"solver\":['lbfgs', 'liblinear', 'sag', 'saga'], \"max_iter\":[1000]}\n\nNB_params = {'var_smoothing': np.logspace(0,-9, num=100)}\nknn_params= {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nsvc_params= {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1, 5, 10 ,50 ,100],\n                 \"C\": [1,10,50,100,200,300,1000]}\nbag_params={\"n_estimators\":[50,120,300]}\ndtree_params = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\nrf_params = {\"max_features\": [\"log2\",\"auto\",\"sqrt\"],\n                \"min_samples_split\":[2,3,5],\n                \"min_samples_leaf\":[1,3,5],\n                \"bootstrap\":[True,False],\n                \"n_estimators\":[50,100,150],\n                \"criterion\":[\"gini\",\"entropy\"]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\ngbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\nxgb_params ={\n        'n_estimators': [50, 100, 200],\n        'subsample': [ 0.6, 0.8, 1.0],\n        'max_depth': [1,2,3,4],\n        'learning_rate': [0.1,0.2, 0.3, 0.4, 0.5],\n        \"min_samples_split\": [1,2,4,6]}\n\nmlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n              \"hidden_layer_sizes\": [(10,10,10),\n                                     (100,100,100),\n                                     (100,100),\n                                     (3,5), \n                                     (5, 3)],\n              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\"max_iter\":[1000]}\ncatb_params =  {'depth':[2, 3, 4],\n              'loss_function': ['Logloss', 'CrossEntropy'],\n              'l2_leaf_reg':np.arange(2,31)}\nclassifier_params = [logreg_params,NB_params,knn_params,svc_params,bag_params,dtree_params,rf_params,\n                     gbm_params, xgb_params,mlpc_params,catb_params]               \n                  ","b8fa891f":"# Tuning by Cross Validation  \ncv_result = {}\nbest_estimators = {}\nfor name, model,classifier_param in zip(names, models,classifier_params):\n    with timer(\">Model tuning\"):\n        clf = GridSearchCV(model, param_grid=classifier_param, cv =10, scoring = \"accuracy\", n_jobs = -1,verbose = False)\n        clf.fit(x_train,y_train)\n        cv_result[name]=clf.best_score_\n        best_estimators[name]=clf.best_estimator_\n        print(name,'cross validation accuracy : %.3f'%cv_result[name])","9983bc8b":"accuracies={}\nprint('Validation accuracies of the tuned models for the train data:', end = \"\\n\\n\")\nfor name, model_tuned in zip(best_estimators.keys(),best_estimators.values()):\n    y_pred =  model_tuned.fit(x_train,y_train).predict(x_val)\n    accuracy=accuracy_score(y_pred, y_val)\n    print(name,':', \"%.3f\" %accuracy)\n    accuracies[name]=accuracy","1018e93d":"n=3\naccu=sorted(accuracies, reverse=True, key= lambda k:accuracies[k])[:n]\nfirstn=[[k,v] for k,v in best_estimators.items() if k in accu]","0a1375f8":"# Ensembling First n Score\n\nvotingC = VotingClassifier(estimators = firstn, voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(x_train, y_train)\nprint(accuracy_score(votingC.predict(x_val),y_val))","5c93f416":"## 3.3 Handling Categorical Variables <a id = '3.6'><\/a><br>","305b8c70":"## 4.2 Cross validation accuracy and std of the default models for all the train data <a id = '6.3'><\/a><br>","b1741dfe":" ### 2.3.3 Gender and Exited <a id = '4.3'><\/a><br>       ","9e845bba":"Females exits more.","307a37a7":"## 4.1 Validation Set Accuracy for the default models <a id = '6.2'><\/a><br>","36327a77":"### 2.1 Importing Libraries and Loading Data <a id = '2.1'><\/a><br>","e1b428c3":"### 2.3.1 Correlation matrix <a id = '4.1'><\/a><br>","3bb2bf06":"df['Tenure'].head()","598b38c7":"## 3.4 Memory Reduction <a id = '3.4'><\/a><br>","d068c8e7":"## 4.3 Model tuning using crossvalidation <a id = '6.4'><\/a><br>","7a96d0ea":"## 2.3 Visualizations <a id = '2.3'><\/a><br> ","fe897444":"The given data is splitted into train and validation sets to test the accuracy of training with the untrained 20% of the sample.","b4dd5f73":" ## 2.3.7 Creeditscore versus Exited <a id = '4.7'><\/a><br>   ","73599dcf":"## 1.1 Summary Information about the variables and their types in the data <a id = '1.1'><\/a><br>\n\n","bef7cefd":"<a id = '6'><\/a><br> \n# 4. Modeling, Evaluation and Model Tuning  ","fa1c8927":" ### 2.6 EstimatedSalary versus Exited <a id = '4.4'><\/a><br>   ","5417f611":"Although the they are similar, there seems to be slightly higher salaries for the exited customers.","e2b6e7d0":"### 3.3.1 Label encoding of gender variable and removing surname <a id = '3.6.1'><\/a><br>","50e5c463":"In this section we are going to illustrate the relationship between variables by using visualization tools.","1eb7316c":"## 4.4 Ensembling first n (e.g. 3) models <a id = '6.6'><\/a><br>","e576e9a8":"## 3.1 Splitting the data as train and validation data <a id = '3.1'><\/a><br>","d486ed23":"In the mid tenure level there is less exit.","de62e84b":"# 1. Introduction ","c6921b49":"## 2.2 Basic summary statistics about the data <a id = '2.2'><\/a><br>","9f50e9a3":"### 3.3.2 One hot encoding of Geography (Country) <a id = '3.6.2'><\/a><br>","fce7ad44":"##### Descriptive statistics excluding CustomerId and row number which do not carry any meaningful information for Survival.","ab74fdf2":" ### 2.3.5 Balance versus Exited <a id = '4.4'><\/a><br>   ","efb9bdc7":"##### This study predicts which bank customers will churn by means of machine learning modelling techniques. It presents a full machine learning work flow, use 11 Machine Learning algorithms, tune their parameters and ensemble the best n (e.g. 3) of them using their accuracy scores for the validation set. ","abbac665":"### 2.3.2 Tenure and Exited <a id = '4.2'><\/a><br>       ","2408bba3":"There is no missing value in the data as seen in section 2.2. In addition, from decriptive statistics we can see that  median and mean values are very similar for most of the numerical variables.","a5c73c7c":" ### 2.3.4 Age versus Exited <a id = '4.4'><\/a><br>   ","9d20608c":"<font color = 'blue'>\n CONTENTS:  \n    \n   1. [Introduction](#1)\n       * 1.1 [Summary Information about the variables and their types in the data](#1.1)\n   2. [Exploratory Data Analysis](#2)\n       * 2.1 [Importing Libraries and Loading Data](#2.1)\n       * 2.2 [Basic summary statistics about the data](#2.2)            \n       * 2.3 [Visualizations](#2.3)\n           * 2.3.1 [Correlation matrix as heatmap](#4.1)\n           * 2.3.2 [Tenure and Exited](#4.2)\n           * 2.3.3 [Gender versus Exited](#4.3)\n           * 2.3.4 [Age versus Exited](#4.4)\n           * 2.3.5 [Balance versus Survived](#4.5)\n           * 2.3.6 [EstimatedSalary versus Exited](#4.6)\n           * 2.3.7 [Creeditscore versus Exited](#4.7)\n   3. [Data Preprocessing](#3)\n       * 3.1 [Splitting the data as train and validation data](#3.1)  \n       * 3.2 [Handling Categorical Variables](#3.6)\n           * 3.2.1 [Label encoding of gender variable and removing surname](#3.6.1)            \n           * 3.2.3 [One hot encoding of Geography (Country)](#3.6.2)   \n       * 3.3 [Memory Reduction](#3.3)\n   4. [Modeling, Model Evaluation and Model Tuning](#6)\n       * 4.1 [Validation Set Test Accuracy for the default models](#6.2) \n       * 4.2 [Cross validation accuracy and std of the default models for all the train data](#6.3)    \n       * 4.3 [Model tuning using crossvalidation](#6.4)   \n       * 4.4 [Ensembling first n (e.g. 5) models](#6.6) \n\n ","d19980ab":"# 3. Data Preprocessing <a id = '3'><\/a><br> ","d3e14496":"* Surname          : The surname of the customer\n* CreditScore      : The credit score of the customer\n* Geography        : The country of the customer(Germany\/France\/Spain)\n* Gender           : The gender of the customer (Female\/Male)\n* Age              : The age of the customer  \n* Tenure           : The customer's number of years in the in the bank \n* Balance          : The customer's account balance\n* NumOfProducts    : The number of bank products that the customer uses \n* HasCrCard        : Does the customer has a card? (0=No,1=Yes) \n* IsActiveMember   : Does the customer has an active mebership (0=No,1=Yes) \n* EstimatedSalary  : The estimated salary of the customer\n* Exited           : Churned or not? (0=No,1=Yes)","95cbbc68":"# 2. Exploratory Data Analysis <a id = '2'><\/a><br> "}}