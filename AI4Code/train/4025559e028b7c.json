{"cell_type":{"3751b198":"code","6d5ea632":"code","8fef36f3":"code","c73ede23":"code","4fae8208":"code","0641eecf":"code","7e590f85":"code","e72d9e9c":"code","781a4d9b":"code","beb03d07":"code","e9f73db0":"code","ab1b438f":"code","ad4fdaaf":"code","d3c7cf25":"code","9fa93fa7":"code","3393eaf7":"code","1f623f90":"code","3f14ba4f":"code","a87bbf11":"code","8bde3603":"code","2a870f9b":"code","d4ed66e8":"code","993414a4":"code","d2adfebc":"code","7140615c":"code","e8a69ece":"code","2010dfba":"code","a0cd34f0":"code","8593a507":"code","369f978f":"code","b394f6b5":"code","6e144838":"code","ef77d965":"code","12fc247d":"code","bee34b99":"code","4fb7f4d5":"code","c89ae716":"code","2ef1842d":"code","4b92d63b":"code","e9f918e8":"code","e1a18aa2":"code","a876b2f7":"markdown","9e995e86":"markdown","bc0d120f":"markdown","695eb3c5":"markdown","4235d28b":"markdown","e2fff917":"markdown","9411b2f8":"markdown","f027bb78":"markdown","b9bd81c4":"markdown","431abde9":"markdown","4745e63a":"markdown","e6738396":"markdown","0213aacb":"markdown","3fc12f81":"markdown","928fdc9a":"markdown","1a84d7d2":"markdown","2c4ab9ee":"markdown","cb423922":"markdown","6ac95b3c":"markdown","e3f3386a":"markdown"},"source":{"3751b198":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6d5ea632":"# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n# warnings\nimport warnings\nwarnings.filterwarnings('ignore')","8fef36f3":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","c73ede23":"# some information about the dataframe\ntrain.info()","4fae8208":"# Data types of columns\ntrain.dtypes","0641eecf":"# Summary\ntrain.describe() # by default we get the summary of numerical datatype columns","7e590f85":"# Sex feature\nsns.barplot(x=train.Sex, y=train.Survived)\nprint(f'females survival percent: ',train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint(f'males survival percent: ',train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","e72d9e9c":"# Pclass feature\nsns.barplot(x=train.Pclass, y=train.Survived)\nprint(f'Pclass 1 survival percent: ',train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(f'Pclass 2 survival percent: ',train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(f'Pclass 3 survival percent: ',train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","781a4d9b":"# checking missing values in train dataset\ntrain.isnull().sum()","beb03d07":"# checking missing values in test dataset\ntest.isnull().sum()","e9f73db0":"# Along with cabin columns we also need to remove a few more columns which will be meaningless in future like PassengerId, Name, Ticket, Cabin\ntrain.drop(columns=['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)\ntest.drop(columns=['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)","ab1b438f":"# Replace the Sex column male with 1 and female with 0 in train and test datasets\ntrain['Sex'] = [1 if gender=='male' else 0 for gender in train['Sex']]\ntest['Sex'] = [1 if gender=='male' else 0 for gender in test['Sex']]","ad4fdaaf":"train.head()","d3c7cf25":"train.info()","9fa93fa7":"# filling the missing age values in train and test\ntrain.Age.fillna(train.Age.median(), inplace=True)\ntest.Age.fillna(test.Age.median(), inplace=True)","3393eaf7":"# grouping the age in train and test\nbins = [0,10,20,40,60,80,100]\ngroups = [0,2,3,4,5,6]\ntrain['AgeGroup'] = pd.cut(train['Age'], bins, labels=groups, include_lowest=True)\ntest['AgeGroup'] = pd.cut(test['Age'], bins, labels=groups, include_lowest=True)","1f623f90":"# so remove the age column from both train and test\ntrain.drop(columns=['Age'], inplace=True)\ntest.drop(columns=['Age'], inplace=True)","3f14ba4f":"# there is a missing value in the Fare column of test dataset so let's fill that first\ntest.Fare.fillna(test.Fare.median(), inplace=True)","a87bbf11":"# Now lets group the fare column for both train and test dataset\nbins = [0,100,200,300,400,500,600]\ngroups = [0,1,2,3,4,5]\ntrain['FareGroup'] = pd.cut(train['Fare'], bins, labels=groups, include_lowest=True)\ntest['FareGroup'] = pd.cut(test['Fare'], bins, labels=groups, include_lowest=True)","8bde3603":"# Now remove the Fare column from both datasets\ntrain.drop(columns=['Fare'], axis=1, inplace=True)\ntest.drop(columns=['Fare'], axis=1, inplace=True)","2a870f9b":"# we have missing values in Embarked column in train dataset so lets fill that first\ntrain.Embarked.fillna(train.Embarked.mode(), inplace=True)","d4ed66e8":"train = pd.get_dummies(train, columns=['Pclass','AgeGroup','Embarked'])\ntest = pd.get_dummies(test, columns=['Pclass','AgeGroup','Embarked'])","993414a4":"train.head()","d2adfebc":"predictors = train.drop(columns=['Survived'], axis=1)","7140615c":"target = train[['Survived']]","e8a69ece":"from sklearn.model_selection import train_test_split","2010dfba":"x_train,x_val,y_train,y_val = train_test_split(predictors,target,test_size=0.2,random_state=123)","a0cd34f0":"# lets see the shapes\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","8593a507":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC","369f978f":"# importing accuracy \nfrom sklearn.metrics import accuracy_score","b394f6b5":"lr = LogisticRegression()\nlr.fit(x_train,y_train)\npreds = lr.predict(x_val)\nlr_accuracy = accuracy_score(y_val,preds)\nprint(f'Logistic Regression accuracy: {lr_accuracy*100}')","6e144838":"knn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\npreds = knn.predict(x_val)\nknn_accuracy = accuracy_score(y_val, preds)\nprint(f'KNN accuracy: {knn_accuracy*100}')","ef77d965":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\npreds = dt.predict(x_val)\ndt_accuracy = accuracy_score(y_val, preds)\nprint(f'Decission Tree accuracy: {dt_accuracy*100}')","12fc247d":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\npreds = rf.predict(x_val)\nrf_accuracy = accuracy_score(y_val, preds)\nprint(f'RandomForest accuracy: {rf_accuracy*100}')","bee34b99":"gbc = GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\npreds = gbc.predict(x_val)\ngbc_accuracy = accuracy_score(y_val, preds)\nprint(f'GradientBoostClassifier accuracy: {gbc_accuracy*100}')","4fb7f4d5":"svc = SVC()\nsvc.fit(x_train,y_train)\npreds = svc.predict(x_val)\nsvc_accuracy = accuracy_score(y_val, preds)\nprint(f'SVC accuracy: {svc_accuracy*100}')","c89ae716":"models = pd.DataFrame({'Model':['LogisticRegression','KNN','DecissionTree','RandomForest','GradientBoostClassifier','SVM'],\n         'Accuracy':[lr_accuracy*100,knn_accuracy*100,dt_accuracy*100,rf_accuracy*100,gbc_accuracy*100,svc_accuracy*100]})\nmodels","2ef1842d":"data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nids = data['PassengerId']","4b92d63b":"preds = dt.predict(test)","e9f918e8":"output = pd.DataFrame({'PassengerId':ids, 'Survived':preds})","e1a18aa2":"output.to_csv('submission.csv', index=False)","a876b2f7":"# RandomForest","9e995e86":"## One-hot encoding","bc0d120f":"### lets make dataframe of train and test","695eb3c5":"# Decission Tree","4235d28b":"# GradientBoostClassifier","e2fff917":"## Importing models","9411b2f8":"# Logistic Regression","f027bb78":"### Importing rest of the required libraries","b9bd81c4":"## Some facts","431abde9":"## Thanks for reading out. If you find it helpful please do appreciate and share with others and recommendations are very welcome.","4745e63a":"# SVM","e6738396":"**We see that there are total 891 records and 12 columns, most of them are numerical. We also have a few columns with highest number of missing values like Survived, Age and Cabin**","0213aacb":"# <div style='text-align: center'> Titanic Survival Predictions <\/div>","3fc12f81":"# Modeling","928fdc9a":"## Missing values","1a84d7d2":"**We see that there are missing values in both train and test datasets. There's a great number of missing values in Cabin column so better we remove the column and fill the rest of missing values**","2c4ab9ee":"# KNN","cb423922":"### lets create the predictors and target variables from trian dataset","6ac95b3c":"# Exploratory data analysis","e3f3386a":"## Create x_train,x_val,y_train,y_val "}}