{"cell_type":{"71218b02":"code","5d325b49":"code","17982384":"code","85ec6bc4":"code","074c88e3":"code","d3f9c206":"code","e61d7c20":"code","20dec873":"code","df634376":"code","0a4013bc":"code","b29240fb":"code","32c81274":"code","65b661bc":"code","fba6f684":"code","ea185ee1":"code","51ea49e7":"code","d5f1a44d":"code","25834b02":"code","a2dc8737":"code","e4c91ac7":"code","a357c1a7":"code","7dace35f":"code","33479199":"code","0ac9fcd1":"code","6ce6e753":"code","95439ccc":"code","c08ffa7d":"code","1e760651":"code","6da2ebd7":"code","dae53f3d":"code","55632cb9":"code","38f7f336":"code","1ec24a61":"code","70a0e7f6":"code","182a170b":"code","e615cda1":"code","69212dbc":"code","a429a6f1":"code","7d1a4a20":"code","448fc5c4":"code","ef1c463b":"code","6d929687":"code","503f67c0":"code","cd74c3eb":"code","978ec767":"code","2517bcef":"code","5fb3b48c":"code","e9c1577c":"code","736df1eb":"code","ef3a8aa5":"code","b54e64a0":"code","36045b47":"code","c63f2ffa":"code","1de42c42":"code","7d1ec6f2":"code","79a8a8ec":"code","671ab338":"code","9a264da4":"code","233cb49d":"code","0697a883":"code","f6735ab1":"code","2865a5b0":"code","70dd36c7":"code","e14bf757":"code","1b0ed07b":"code","9d1c96f1":"code","30a11c2f":"code","892b596f":"code","7fe19440":"code","fdc70e52":"code","cb2f22f9":"code","5a5706ca":"code","9d82f47f":"code","3ebd6c49":"markdown","b2530e0f":"markdown","f4619a7e":"markdown","80d82871":"markdown","3afa9516":"markdown","c3f9e5f2":"markdown","a1053d0d":"markdown"},"source":{"71218b02":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport json\nimport math\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, Ridge\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nfrom sklearn.decomposition import PCA\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor, HistGradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\n\nimport os\n\n%matplotlib inline","5d325b49":"import os\nprint(os.listdir(\"..\/input\/tabular-playground-series-mar-2021\/\"))","17982384":"#Loading Train and Test Data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\") ","85ec6bc4":"train.head()","074c88e3":"test.head()","d3f9c206":"train.info()","e61d7c20":"test.info()","20dec873":"train.describe()","df634376":"test.describe()","0a4013bc":"train['target'].value_counts()","b29240fb":"np.mean(train.target)","32c81274":"train['cat0'].value_counts()","65b661bc":"train['cat1'].value_counts()","fba6f684":"plt.figure(figsize=(12, 5))\nplt.hist(train['cat1'].values, bins=200)\nplt.title('Histogram cat1 counts in train')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","ea185ee1":"train['cat2'].value_counts()","51ea49e7":"plt.figure(figsize=(12, 5))\nplt.hist(train['cat2'].values, bins=200)\nplt.title('Histogram cat2 counts in train')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","d5f1a44d":"train['cat3'].value_counts()","25834b02":"plt.figure(figsize=(12, 5))\nplt.hist(train['cat3'].values, bins=200)\nplt.title('Histogram cat3 counts in train')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","a2dc8737":"train['cat4'].value_counts()","e4c91ac7":"plt.figure(figsize=(12, 5))\nplt.hist(train['cat4'].values, bins=200)\nplt.title('Histogram cat4 counts in train')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","a357c1a7":"train['cat5'].value_counts()","7dace35f":"plt.figure(figsize=(12, 5))\nplt.hist(train['cat5'].values, bins=200)\nplt.title('Histogram cat5 counts in train')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","33479199":"train['cat6'].value_counts()","0ac9fcd1":"plt.figure(figsize=(12, 5))\nplt.hist(train['cat6'].values, bins=200)\nplt.title('Histogram cat5 counts in train')\nplt.xlabel('Value')\nplt.ylabel('Count')\nplt.show()","6ce6e753":"profile_train = ProfileReport(train, title='Pandas Train Profiling Report', html={'style':{'full_width':True}})","95439ccc":"profile_train","c08ffa7d":"profile_test = ProfileReport(test, title='Pandas Train Profiling Report', html={'style':{'full_width':True}})","1e760651":"profile_test","6da2ebd7":"y = train.target\nX = train.drop([\"id\", \"target\"], axis=1)\n\nX_test = test.drop([\"id\"], axis=1)","dae53f3d":"\n#List of categorical col\nlist_cat = [col for col in X.columns if col.startswith(\"cat\")]\n\n\nX_all = pd.concat([X, X_test], axis=0)\n\nle = LabelEncoder()\n\nfor col in list_cat:\n    X_all[col] = le.fit_transform(X_all[col])\n    \nX_all.head()","55632cb9":"X = X_all.iloc[:len(train), :]\nX_test = X_all.iloc[len(train):, :]\n","38f7f336":"train_oof = np.zeros((X.shape[0],))\ntest_preds = 0\ntrain_oof.shape","1ec24a61":"%%time\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\nmax_iter = 350\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(X, y))):\n        #print(f'Fold {f}')\n        train_df, val_df = X.iloc[train_ind], X.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        \n        model = HistGradientBoostingClassifier(max_iter=max_iter, validation_fraction=None, learning_rate=0.05, \n                                               max_depth=9, min_samples_leaf=23, max_leaf_nodes=100)\n        \n\n        model =  model.fit(train_df, train_target)\n        temp_oof = model.predict_proba(val_df)[:,1]\n        temp_test = model.predict_proba(X_test)[:,1]\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))","70a0e7f6":"roc_auc_score(y, train_oof)\n","182a170b":"0.8912443537006325","e615cda1":"np.save('train_oof_hgb_0', train_oof)\nnp.save('test_preds_hgb_0', test_preds)","69212dbc":"%%time\n\ntrain_oof_lgbm_0 = np.zeros((X.shape[0],))\ntest_preds_lgbm_0 = 0\ntrain_oof_lgbm_0.shape\n\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(X, y))):\n        #print(f'Fold {f}')\n        train_df, val_df = X.iloc[train_ind], X.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        \n        model = LGBMClassifier(\n                    cat_feature=[x for x in range(19)],\n                    random_state=777,\n                    cat_l2=25.999876242730252,\n                    cat_smooth=89.2699690675538,\n                    colsample_bytree=0.2557260109926193,\n                    learning_rate=0.004,\n                    max_bin=788,\n                    max_depth=81,\n                    metric=\"auc\",\n                    min_child_samples=292,\n                    min_data_per_group=177,\n                    n_estimators=4000000,\n                    n_jobs=-1,\n                    num_leaves=171,\n                    reg_alpha=0.7115353581785044,\n                    reg_lambda=5.658115293998945,\n                    subsample=0.9262904583735796,\n                    subsample_freq=1,\n                    verbose=-1,\n                )\n        \n\n        model =  model.fit(train_df, train_target, eval_set=[(val_df,val_target)],early_stopping_rounds=450,verbose=False)\n        temp_oof = model.predict_proba(val_df)[:,1]\n        temp_test = model.predict_proba(X_test)[:,1]\n\n        train_oof_lgbm_0[val_ind] = temp_oof\n        test_preds_lgbm_0 += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))\n        \nprint(roc_auc_score(y, train_oof_lgbm_0))\nnp.save('train_oof_lgbm_0', train_oof_lgbm_0)\nnp.save('test_preds_lgbm_0', test_preds_lgbm_0)\n","a429a6f1":"y = train.target\nX = train.drop([\"id\", \"target\"], axis=1)\n\nX_test = test.drop([\"id\"], axis=1)","7d1a4a20":"X.head()","448fc5c4":"categorical_cols = ['cat'+str(i) for i in range(19)]\ncontinous_cols = ['cont'+str(i) for i in range(11)]","ef1c463b":"cols=categorical_cols+continous_cols\ntrain_objs_num = len(train)\ndataset = pd.concat(objs=[X[cols], X_test[cols]], axis=0)\ndataset_preprocessed = pd.get_dummies(dataset,columns=categorical_cols)\ntrain_preprocessed = dataset_preprocessed[:train_objs_num]\ntest_preprocessed = dataset_preprocessed[train_objs_num:]","6d929687":"train_preprocessed.head()","503f67c0":"train_oof_lr_0 = np.zeros((X.shape[0],))\ntest_preds_lr_0 = 0\ntrain_oof_lr_0.shape","cd74c3eb":"%%time\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train_preprocessed, y))):\n        #print(f'Fold {f}')\n        train_df, val_df = train_preprocessed.iloc[train_ind], train_preprocessed.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        \n        model = LogisticRegression(max_iter=200)\n        \n\n        model =  model.fit(train_df, train_target)\n        temp_oof = model.predict_proba(val_df)[:,1]\n        temp_test = model.predict_proba(test_preprocessed)[:,1]\n\n        train_oof_lr_0[val_ind] = temp_oof\n        test_preds_lr_0 += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))","978ec767":"np.save('train_oof_lr_0', train_oof_lr_0)\nnp.save('test_preds_lr_0', test_preds_lr_0)","2517bcef":"print(roc_auc_score(y, train_oof_lr_0))","5fb3b48c":"print(roc_auc_score(y, 0.85*train_oof+0.15*train_oof_lr_0))","e9c1577c":"0.8925740557816217","736df1eb":"train_oof_lgbm_1 = np.zeros((X.shape[0],))\ntest_preds_lgbm_1 = 0\ntrain_oof_lgbm_1.shape","ef3a8aa5":"lgbm_params={'metric': 'auc', \n             'reg_alpha': 6.010538011450937, \n             'reg_lambda': 0.031702113663443346, \n             'colsample_bytree': 0.27,\n             'subsample': 0.6, \n             'learning_rate': 0.005, \n             'max_depth': 100, \n             'num_leaves': 100, \n             'min_child_samples': 216,\n             'cat_smooth': 87, \n             'random_state': 77,\n             'n_estimators': 200000}","b54e64a0":"%%time\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train_preprocessed, y))):\n        #print(f'Fold {f}')\n        train_df, val_df = train_preprocessed.iloc[train_ind], train_preprocessed.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        \n        model = LGBMClassifier(**lgbm_params) \n        \n\n        model =  model.fit(train_df, train_target, eval_set=[(val_df,val_target)],early_stopping_rounds=1100,verbose=False)\n        temp_oof = model.predict_proba(val_df)[:,1]\n        temp_test = model.predict_proba(test_preprocessed)[:,1]\n\n        train_oof_lgbm_1[val_ind] = temp_oof\n        test_preds_lgbm_1 += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))\n        \nprint(roc_auc_score(y, train_oof_lgbm_1))\n","36045b47":"%%time\ntrain_oof_lgbm_2 = np.zeros((X.shape[0],))\ntest_preds_lgbm_2 = 0\ntrain_oof_lgbm_2.shape\n\nlgbm_parameters = {\n    'cat_feature': categorical_cols,\n    'metric': 'auc', \n    'n_estimators': 20000,\n    'reg_alpha': 0.000721024661208569,\n    'reg_lambda': 47.79748127808107,\n    'colsample_bytree': 0.24493010466517195,\n    'subsample': 0.12246675404710294,\n    'learning_rate': 0.013933182980403087,\n    'max_depth': 21,\n    'num_leaves': 90,\n    'min_child_samples': 144,\n    'cat_smooth': 63\n}\n\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\n\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train_preprocessed, y))):\n        #print(f'Fold {f}')\n        train_df, val_df = train_preprocessed.iloc[train_ind], train_preprocessed.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        \n        model = LGBMClassifier(**lgbm_params) \n        \n\n        model =  model.fit(train_df, train_target, eval_set=[(val_df,val_target)],early_stopping_rounds=1100,verbose=False)\n        temp_oof = model.predict_proba(val_df)[:,1]\n        temp_test = model.predict_proba(test_preprocessed)[:,1]\n\n        train_oof_lgbm_2[val_ind] = temp_oof\n        test_preds_lgbm_2 += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))\n        \nprint(roc_auc_score(y, train_oof_lgbm_2))","c63f2ffa":"print(roc_auc_score(y, 0.95*train_oof_lgbm_1+0.05*train_oof))","1de42c42":"print(roc_auc_score(y, 0.5*train_oof_lgbm_0+0.5*train_oof_lgbm_1))","7d1ec6f2":"print(roc_auc_score(y, 0.25*train_oof_lgbm_0+0.25*train_oof_lgbm_1+0.5*train_oof_lgbm_2))","79a8a8ec":"np.save('train_oof_lgbm_1', train_oof_lgbm_1)\nnp.save('test_preds_lgbm_1', test_preds_lgbm_1)","671ab338":"np.save('train_oof_lgbm_2', train_oof_lgbm_2)\nnp.save('test_preds_lgbm_2', test_preds_lgbm_2)","9a264da4":"train_oof_hgb_1 = np.zeros((X.shape[0],))\ntest_preds_hgb_1 = 0\ntrain_oof_hgb_1.shape","233cb49d":"%%time\nNUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=137)\nmax_iter = 350\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train_preprocessed, y))):\n        #print(f'Fold {f}')\n        train_df, val_df = train_preprocessed.iloc[train_ind], train_preprocessed.iloc[val_ind]\n        train_target, val_target = y.iloc[train_ind], y.iloc[val_ind]\n        \n        \n        model = HistGradientBoostingClassifier(max_iter=max_iter, validation_fraction=None, learning_rate=0.05, \n                                               max_depth=9, min_samples_leaf=23, max_leaf_nodes=100)\n        \n\n        model =  model.fit(train_df, train_target)\n        temp_oof = model.predict_proba(val_df)[:,1]\n        temp_test = model.predict_proba(test_preprocessed)[:,1]\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(roc_auc_score(val_target, temp_oof))\n","0697a883":"np.save('train_oof_hgb_1', train_oof_hgb_1)\nnp.save('test_preds_hgb_1', test_preds_hgb_1)","f6735ab1":"'''sample_submission['target'] = 0.85*test_preds+0.15*test_preds_lr_0\nsample_submission.to_csv('submission.csv', index=False)'''","2865a5b0":"sample_submission['target'] = test_preds\nsample_submission.to_csv('submission_hgb_0.csv', index=False)","70dd36c7":"sample_submission['target'] = test_preds_hgb_1\nsample_submission.to_csv('submission_hgb_1.csv', index=False)","e14bf757":"sample_submission['target'] = test_preds_lr_0\nsample_submission.to_csv('submission_lr_0.csv', index=False)","1b0ed07b":"sample_submission['target'] = test_preds_lgbm_0\nsample_submission.to_csv('submission_lgbm_0.csv', index=False)","9d1c96f1":"sample_submission['target'] = test_preds_lgbm_1\nsample_submission.to_csv('submission_lgbm_1.csv', index=False)","30a11c2f":"sample_submission['target'] = 1.1*test_preds_lgbm_1-0.1*test_preds\nsample_submission.to_csv('submission_blend_0.csv', index=False)","892b596f":"sample_submission['target'] = 0.5*test_preds_lgbm_0+0.5*test_preds_lgbm_1\nsample_submission.to_csv('submission_blend_0.csv', index=False)","7fe19440":"sample_submission['target'] = 0.55*test_preds_lgbm_0+0.45*test_preds_lgbm_1\nsample_submission.to_csv('submission_blend_1.csv', index=False)","fdc70e52":"sample_submission['target'] = 1.05*(0.55*test_preds_lgbm_0+0.45*test_preds_lgbm_1)-0.05*test_preds\nsample_submission.to_csv('submission_blend_2.csv', index=False)","cb2f22f9":"sample_submission['target'] = 1.05*(0.5*test_preds_lgbm_0+0.5*test_preds_lgbm_1)-0.05*test_preds\nsample_submission.to_csv('submission_blend_3.csv', index=False)","5a5706ca":"sample_submission['target'] = 0.25*test_preds_lgbm_0+0.25*test_preds_lgbm_1+0.5*test_preds_lgbm_2\nsample_submission.to_csv('submission_blend_4.csv', index=False)","9d82f47f":"sample_submission['target'] = 1.05*(0.25*test_preds_lgbm_0+0.25*test_preds_lgbm_1+0.5*test_preds_lgbm_2)-0.05*test_preds\nsample_submission.to_csv('submission_blend_5.csv', index=False)","3ebd6c49":"So this is a binary classification problem with imbalanced data.","b2530e0f":"# Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still very raw. I will work on it as my very limited time permits, and hope to expend it in the upcoming days and weeks.\n\n# Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","f4619a7e":"Let's see what files we have in the input directory:","80d82871":"Instead of label encoding, we could also do one hot encoding","3afa9516":"Let's look at the distribution of the target:\n\n","c3f9e5f2":"Now let's do some simple modeling. First, we'll have to encode all teh categorical variales so that we can use them with numerical algorithms. ","a1053d0d":"Now let's take a look at LightGBM instead."}}