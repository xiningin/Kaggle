{"cell_type":{"42c3ed6e":"code","522968dc":"code","62c00bbf":"code","f2f80f55":"code","16e37622":"code","cb8059cd":"code","6c036906":"code","00a95f8b":"code","3b8d2acd":"code","d66a480a":"code","5bb18d72":"code","a3f21882":"code","13fe768c":"code","8ef5d496":"code","f5f68de4":"code","daca59d8":"code","0d2ffed6":"code","8635d920":"code","4cbdf318":"code","bb237618":"code","da89ca3b":"code","a066a3a9":"code","f709e2b0":"code","a789c717":"code","f3049dbf":"code","04df0e0c":"code","f3a4e8a6":"code","9786a2d8":"code","e29b2159":"code","188f9455":"code","ffc0bea8":"code","e91baec9":"code","d3883094":"code","3f0242f1":"code","a749e084":"code","3c5ac7b7":"code","f26af377":"code","86e74191":"code","daf4fead":"code","eb13e8a3":"code","e893f8e4":"code","0e5a6ad8":"code","96ee5272":"code","0e7a6962":"code","5712d620":"code","7eed646a":"code","0dff4236":"code","b42e3e65":"code","66cd7003":"code","2e2538d5":"code","ca25c537":"code","70735f01":"code","2faf5762":"code","5b87d715":"code","5a95acb4":"code","d0a47769":"code","79abed85":"code","b229a835":"code","644806a7":"code","41c7995b":"code","7943169c":"code","5eee3421":"code","c3ce7cb5":"code","1fbfce3f":"code","3b404f17":"code","03c3a52c":"code","cc496225":"code","f1412fd3":"code","852f160f":"code","7bac1381":"code","cd436b30":"code","34585971":"code","314eb27f":"code","e4a5dec6":"code","d1f46522":"code","9d99bcbc":"code","83db47bd":"code","6edd055b":"code","b94eecf2":"code","53e5d077":"code","efe3cbd6":"code","922fbbc9":"code","c3ea7160":"code","a578afa9":"code","7c25fc3f":"code","bf453701":"code","98f4a851":"code","e495f036":"code","de2144b8":"code","f1a87877":"code","05feb7b5":"code","f919cbf1":"code","31a82cd8":"code","6bbabe01":"code","e7c92f21":"code","51520a04":"code","a92f1222":"code","23b596b0":"code","c4b458c2":"code","ead833df":"code","a6128986":"code","716408da":"code","a96601b6":"code","9270eee0":"code","86dfe7e2":"code","cc21ce91":"code","df635f4e":"code","7088a5b4":"code","5aed7f03":"code","dde7ba14":"code","171fc5a1":"code","35ca5a44":"code","6d347ce6":"code","c8395eff":"code","6dd08c3d":"code","7978082d":"code","b1b2112b":"code","9749e327":"code","712c4152":"code","76c8d7b1":"code","659c07b3":"code","94f2a015":"code","6fadaf19":"markdown","464110b9":"markdown","4b3870b8":"markdown","d437e951":"markdown","4d6d98ab":"markdown","2f39b0bc":"markdown","cf10a772":"markdown","286d4687":"markdown","aa26ca42":"markdown","b6806550":"markdown","f9492f93":"markdown","8e3f1316":"markdown","74d95a42":"markdown","5e8654d1":"markdown","adfe9083":"markdown","4b6175b0":"markdown","c9a3ea5d":"markdown","f62113ce":"markdown","1a5d99c0":"markdown","8ccf3892":"markdown","921dfd9a":"markdown","624784a4":"markdown","175db1fe":"markdown","2c52fb5c":"markdown","977073d4":"markdown","94edfc7c":"markdown","f8f0a95b":"markdown","7e4f7a99":"markdown","fcdf4645":"markdown","8add8df6":"markdown","5a0d507c":"markdown","e83f83e6":"markdown","a5f33bf5":"markdown","170ba275":"markdown","24629a47":"markdown","fa5e58e6":"markdown","f972ace6":"markdown","5239cb27":"markdown","ba55da6c":"markdown","ab4f668f":"markdown","c58799bf":"markdown","9d25242c":"markdown","73d0f74f":"markdown","59f5b373":"markdown","4064dd6f":"markdown","a56f0bf7":"markdown","8fc73e7b":"markdown","e4d1d1a0":"markdown","bececc49":"markdown","56a0a042":"markdown","ec830c54":"markdown","0e3bdfa1":"markdown","f66abafe":"markdown","44bf3440":"markdown","7ed0513f":"markdown","314da4be":"markdown","a7e4eead":"markdown","6df7fd44":"markdown","ebe9dd2d":"markdown","309ab408":"markdown","70710976":"markdown","cb12726f":"markdown","7596744d":"markdown","922ec7b9":"markdown","be4ba129":"markdown","31a3e08a":"markdown","46b4320d":"markdown","7465b855":"markdown","7d7cea4a":"markdown","2cc94569":"markdown","19f46428":"markdown"},"source":{"42c3ed6e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","522968dc":"cust_data = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\ngeo_data =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv')\norder_items =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\norder_payments =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv')\norder_reviews =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv')\norder_data =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\nproducts_data = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\nsellers_data = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv')\nproduct_category = pd.read_csv('..\/input\/brazilian-ecommerce\/product_category_name_translation.csv')","62c00bbf":"cust_data.head()","f2f80f55":"cust_data.isnull().sum()","16e37622":"cust_data.customer_state.value_counts().plot(kind='pie',figsize=(10,8),autopct='%.1f%%',radius=2)\nplt.legend()\nplt.show()","cb8059cd":"#Top 10 cities with their value counts\ncust_data.customer_city.value_counts().sort_values(ascending=False)[:10]","6c036906":"order_items.head()","00a95f8b":"#checking if any null\/NA present in any entry\norder_items.isnull().any().any()","3b8d2acd":"print('Number of sellers :',order_items.seller_id.unique().shape[0])\nprint('Number of unique products are : ',order_items.product_id.unique().shape[0])","d66a480a":"order_items.price.describe()","5bb18d72":"#temporararily removing few outliers that can reduce the smoothness of the pdf and cdf\nsata = order_items[order_items.price <= 2000]\ncounts, bin_edges = np.histogram(sata.price, bins=1000, \n                                 density = True)\npdf = counts\/(sum(counts))\n\n#compute CDF\ncdf = np.cumsum(pdf)\nplt.figure(figsize=(10,8))\nplt.plot(bin_edges[1:],pdf,color='red',label='PDF')\nplt.plot(bin_edges[1:], cdf,color='blue',label='CDF')\nplt.legend()\n\n\nplt.show();","a3f21882":"order_payments.head()","13fe768c":"#Most used Payment method for orders\nexplode = [0.2]*5\norder_payments['payment_type'].groupby(order_payments['payment_type']).count().plot(kind='pie',radius=3,labels=order_payments.payment_type.unique(),autopct='%.1f%%',explode=explode)\nplt.legend()\nplt.show()","8ef5d496":"#number of installments feature\norder_payments.payment_installments.value_counts().plot(kind = 'bar',figsize=(10,8))\nplt.legend()\nplt.xlabel('No. of Installments')\nplt.show()","f5f68de4":"#checking if any null\/NA present in any entry\norder_payments.isnull().any().any()","daca59d8":"order_reviews.head()","0d2ffed6":"#pie chart for review_score\norder_reviews.review_score.value_counts().plot(kind = 'pie',radius=3,autopct='%.1f%%')\nplt.legend()\nplt.show()","8635d920":"#how many null\/missing entries are present\norder_reviews.isnull().sum()","4cbdf318":"order_data.head()","bb237618":"#order_status\norder_data.order_status.value_counts().plot(kind='pie',radius=3,autopct='%.1f%%')\nplt.legend()\nplt.show()","da89ca3b":"#checking if any null\/NA present in any entry\norder_data.isnull().sum()","a066a3a9":"products_data.head()","f709e2b0":"products_data.rename(columns = {'product_name_lenght':'product_name_length',\n                       'product_description_lenght':'product_description_length'},inplace=True)","a789c717":"#Density plot of product_description_length and \nproducts_data.product_description_length.plot(kind='density',figsize=(10,8),color= 'purple')\nplt.xlabel('product_description_length')","f3049dbf":"#Density plot of product_description_length\nd = products_data.product_name_length\n\nd.plot(kind='kde',figsize=(12,8),color= 'y')\nplt.xlabel('product_name_length')\nplt.show()","04df0e0c":"counts, bin_edges = np.histogram(products_data.product_description_length.dropna(), bins=10, \n                                 density = True)\nfig,ax = plt.subplots(2,figsize=(16,16))\npdf = counts\/(sum(counts))\ncdf = np.cumsum(pdf)\nax[0].plot(bin_edges[1:],pdf,color='red',label='PDF')\nax[1].plot(bin_edges[1:], cdf,color= 'b',label=\"CDF\")\nplt.xlabel('Product_description_length')\nplt.legend()\nplt.show()","f3a4e8a6":"sellers_data.head()","9786a2d8":"#top 10  seller state\nsellers_data.seller_state.value_counts().sort_values(ascending=False)[:10].plot(kind='barh',color='pink',figsize=(12,8))\nplt.legend()\nplt.show()","e29b2159":"#unique sellers\nprint('Number of unique sellers are: ',sellers_data.seller_id.unique().shape[0])","188f9455":"#renaming all the zip_code_prefix so as to make the name common in all tables inorder to perform join\n\ngeo_data.rename(columns={'geolocation_zip_code_prefix':'zip_code_prefix'},inplace=True)\ncust_data.rename(columns={'customer_zip_code_prefix':'zip_code_prefix'},inplace=True)\nsellers_data.rename(columns={'seller_zip_code_prefix':'zip_code_prefix'},inplace=True)","ffc0bea8":"#keeping only the unique zip code prefix so that it can act as key to join tables\ngeo_data.drop_duplicates(subset='zip_code_prefix',inplace=True)\ngeo_data.shape","e91baec9":"#merging all customer related data\n\nA = pd.merge(order_data,order_reviews,on='order_id')\nA = pd.merge(A,order_payments,on='order_id')\nA = pd.merge(A,cust_data,on='customer_id')\n#peforming left outer join as we need every geo based address related to customer\nA = pd.merge(A,geo_data,how='left',on='zip_code_prefix')\nA.shape","d3883094":"#merging all seller related data\n\nB = pd.merge(order_items,products_data,on='product_id')\nB = pd.merge(B,sellers_data,on='seller_id')\nB = pd.merge(B,product_category,on='product_category_name')\n#peforming left outer join as we need every geo based address related to seller\nB = pd.merge(B,geo_data,how='left',on='zip_code_prefix')\nB.shape","3f0242f1":"#merging customer based data to the seller based data\n\ndata = pd.merge(A,B,on='order_id')\ndata.shape","a749e084":"#final data columns\ndata.columns","3c5ac7b7":"data.isnull().sum()","f26af377":"print(data.shape[0])\ndata[data['order_status'] != 'delivered'].shape[0]","86e74191":"100 - round((2514\/112863)*100,3)","daf4fead":"#removing some mis filled data\ndata = data[data['geolocation_state_y'] == data['seller_state']]\n\n#list of useless feature\nuseless_features = ['review_comment_title','review_comment_message','product_category_name','product_weight_g','review_creation_date',\n                    'product_length_cm','product_height_cm','product_width_cm','seller_city','review_answer_timestamp',\n                   'geolocation_lat_y','geolocation_lng_y','geolocation_city_y','geolocation_state_y','review_id','order_approved_at','order_status',\n                   'order_id','customer_id','order_item_id','geolocation_lat_x',\n                   'geolocation_lng_x','geolocation_city_x','geolocation_state_x']\nprint('Number of useless features as of now are : ',len(useless_features))\n\ndata.drop(useless_features,axis=1,inplace=True)\n\ndata.rename(columns = {'product_category_name_english':'product_category_name','zip_code_prefix_x':'zipCode_prefix_cust',\n                      'zip_code_prefix_y':'zipCode_prefix_seller'},inplace=True)\n\n\n","eb13e8a3":"prev_size = data.shape[0]\ndata.dropna(how='any',inplace=True)\n\ncurrent_size = data.shape[0]\n#no Null values now\ndata.isnull().values.any()\nprint(data.shape)\nprint('Only {}% of data got removed'.format(round(((prev_size - current_size)\/prev_size)*100,2)))","e893f8e4":"data.columns","0e5a6ad8":"#understanding the growth of Olist as an ecommerce company from the range of data given to us\nimport matplotlib.patches as mpatches\ndata['order_purchase_timestamp'] = pd.to_datetime(data['order_purchase_timestamp'])\nmonthly_sales_data = data.groupby(data['order_purchase_timestamp'].dt.strftime('%m,%y'))['price'].sum().plot(kind='bar',figsize = (20,8),color=['r','g'],width=0.8)\nplt.ylabel('sales')\n\nred_patch = mpatches.Patch(color='red', label='2017')\ngreen_patch = mpatches.Patch(color='green', label='2018')\nplt.legend(handles=[red_patch,green_patch])\nplt.xlabel('Month,year')\nplt.show()","96ee5272":"#top 10 states as per number of orders\ndata.groupby(data['customer_state'])['order_purchase_timestamp'].count().sort_values()[:10].plot(kind='barh',figsize=(20,8),color='green')\nplt.legend()\nplt.show()","0e7a6962":"print('Total unique category are ',data.product_category_name.unique().shape[0])","5712d620":"#category wise sales with their rating(top 20)\ndata.groupby(data['product_category_name'])['price'].sum().sort_values(ascending=False)[:20].plot(kind='bar',figsize=(20,8))\nplt.legend()\nplt.show()","7eed646a":"#Box plot for price with review score\nplt.figure(figsize= (16,8))\nsns.boxplot(x='review_score',y='price', data=data)\nplt.show()","0dff4236":"d = data[['product_description_length','product_name_length','product_photos_qty','review_score']]\nsns.set_style(\"whitegrid\");\nsns.pairplot(d, hue=\"review_score\", height=4);\nplt.show()","b42e3e65":"#plot showing relationship among number of photos and the count of number of orders\nd = data.groupby(data['product_photos_qty'].astype(int))['order_purchase_timestamp'].count()\nd.plot(kind='bar',figsize=(14,8),color='c')\nd.plot(kind='line',figsize=(14,8),color='b')\nplt.legend()\nplt.show()","66cd7003":"#taking only those points which was shown in the scatter plot\nd = data[(data['product_name_length'] > 40) & (data['product_name_length'] < 60) & (data['product_description_length'] < 1500)]\n\nplt.figure(figsize=(10,8))\n#box plot\nsns.violinplot(x='review_score',y='product_description_length', data=d)\nplt.show()","2e2538d5":"plt.figure(figsize=(10,8))\nsns.boxplot(x='review_score',y='product_description_length', data=d)\nplt.show()","ca25c537":"#payments installments\n#data.payment_installments.value_counts().plot(kind = 'bar',figsize=(10,8))\nplt.figure(figsize=(10,8))\nsns.countplot(data.payment_installments)\nplt.show()","70735f01":"#converting the timestamp format data to date data as we need just the date and not the exact time\ndata['order_purchase_timestamp'] = data['order_purchase_timestamp'].dt.date\ndata['order_estimated_delivery_date'] = pd.to_datetime(data['order_estimated_delivery_date']).dt.date\ndata['order_delivered_customer_date'] = pd.to_datetime(data['order_delivered_customer_date']).dt.date\ndata['shipping_limit_date'] = pd.to_datetime(data['shipping_limit_date']).dt.date\n\ntype(data['order_delivered_customer_date'][0])","2faf5762":"#subtracting the order_purchase_time to rest time based feature and converting date time into string to remove the timestamp notation \ndata['delivery_days'] = data['order_delivered_customer_date'].sub(data['order_purchase_timestamp'],axis=0).astype(str)\ndata['estimated_days'] = data['order_estimated_delivery_date'].sub(data['order_purchase_timestamp'],axis=0).astype(str)\ndata['ships_in'] = data['shipping_limit_date'].sub(data['order_purchase_timestamp'],axis=0).astype(str)","5b87d715":"#replacing the time stamp notation and converting type to int\ndata['delivery_days'] = data['delivery_days'].str.replace(\" days\",\"\").astype(int)\ndata['estimated_days'] = data['estimated_days'].str.replace(\" days\",\"\").astype(int)\ndata['ships_in'] = data['ships_in'].str.replace(\" days\",\"\").astype(int)\ndata['arrival_time'] = (data['estimated_days'] - data['delivery_days']).apply(lambda x: 'Early\/OnTime' if x > 0 else 'Late')","5a95acb4":"sns.FacetGrid(data, hue=\"arrival_time\", height=8) \\\n   .map(sns.distplot, 'review_score') \\\n   .add_legend();\nplt.show();","d0a47769":"data.arrival_time.value_counts()","79abed85":"#dropping exceptional delivery or possible outliers\nix = data[(data['delivery_days'] > 60) | (data['estimated_days'] > 60) | (data['ships_in'] > 60)].index\nprint(ix.shape)\n\nprint('Percentage of extermely late delivered packages in the dataset is: {}%'.format(round((((ix.shape[0])\/(data.shape[0])))*100,3)))\n#we can remove those outliers\ndata.drop(ix,inplace=True)","b229a835":"#binning and grouping delivery times into groups or classes\n\ndelivery_feedbacks = []\nestimated_del_feedbacks = []\nshipping_feedback = []\nd_days = data.delivery_days.values.tolist()\nest_days = data.estimated_days.values.tolist()\nship_days = data.ships_in.values.tolist()\n\n#actural delivery days\nfor i in d_days:\n    if i in range(0,8):\n        delivery_feedbacks.append('Very_Fast')\n    elif i in range(8,16):\n        delivery_feedbacks.append('Fast')\n    elif i in range(16,25):\n        delivery_feedbacks.append('Neutral')\n    elif i in range(25,40):\n        delivery_feedbacks.append('Slow')\n    elif i in range(40,61):\n        delivery_feedbacks.append('Worst')\n\n#estimated delivery days\nfor i in est_days:\n    if i in range(0,8):\n        estimated_del_feedbacks.append('Very_Fast')\n    elif i in range(8,16):\n        estimated_del_feedbacks.append('Fast')\n    elif i in range(16,25):\n        estimated_del_feedbacks.append('Neutral')\n    elif i in range(25,40):\n        estimated_del_feedbacks.append('Slow')\n    elif i in range(40,61):\n        estimated_del_feedbacks.append('Worst')\n\n#estimated shipping days\nfor i in ship_days:\n    if i in range(0,4):\n        shipping_feedback.append('Very_Fast')\n    elif i in range(4,8):\n        shipping_feedback.append('Fast')\n    elif i in range(8,16):\n        shipping_feedback.append('Neutral')\n    elif i in range(16,28):\n        shipping_feedback.append('Slow')\n    elif i in range(28,61):\n        shipping_feedback.append('Worst')\n\n#putting list values into the dataframe as feature\ndata['delivery_impression'] = delivery_feedbacks\ndata['estimated_del_impression'] = estimated_del_feedbacks\ndata['ship_impression'] = shipping_feedback","644806a7":"#Delivery Days histogram with review rating as hue\ndf_plot = data.groupby(['delivery_impression', 'review_score']).size().reset_index().pivot(columns='delivery_impression', index='review_score', values=0)\ndf_plot.plot(kind='barh',stacked=True,figsize=(20,8))","41c7995b":"#Estimated Days histogram with review rating as hue\nest_days = data.ship_impression.values.tolist()\nreview_score = data.review_score.values\nfig = plt.figure(figsize=(24,12))\nsns.countplot(est_days,hue=review_score,palette='rocket')\nplt.show()","7943169c":"from collections import Counter\n#finding all those customers who have made multiple product purchases\norder_counts = [k for k,v in Counter(data.customer_unique_id).items() if v > 1]\nexisting_cust = []\nfor i in data.customer_unique_id.values:\n    if i in order_counts:\n        existing_cust.append(1)\n    else:\n        existing_cust.append(0)\nlen(existing_cust)","5eee3421":"print('Number of orders that are made using exisiting customer\/Multiple orders are ',existing_cust.count(1))","c3ce7cb5":"#seller popularity based on number of orders for each seller\nmax_value_count = data.seller_id.value_counts().max()\nseller = data.seller_id.value_counts().to_dict()\nseller_popularity = []\nfor _id in data.seller_id.values:\n    seller_popularity.append(seller[_id])\ndata['seller_popularity'] = seller_popularity\ndata.shape","1fbfce3f":"#dropping all id based features\ndata.drop(['customer_unique_id','seller_id','product_id','zipCode_prefix_seller','zipCode_prefix_cust'],axis=1,inplace=True)\n\n#adding exisitng customer and seller_ID feature\n\ndata['existing_cust'] = existing_cust\ndata.shape","3b404f17":"#if score> 3, set score = 1\n#if score<=3, set score = 0 \ndata.loc[data['review_score'] < 3 ,'Score'] = 0\ndata.loc[data['review_score'] > 3,'Score'] = 1\n#removing neutral reviews\ndata.drop(data[data['review_score'] == 3].index,inplace=True)\ndata.drop('review_score',axis=1,inplace=True)\nprint(data.shape)","03c3a52c":"#removing the datetime features as we dont need them now\ndata.drop(['order_purchase_timestamp', 'order_delivered_carrier_date',\n       'order_delivered_customer_date', 'order_estimated_delivery_date','shipping_limit_date'],axis=1,inplace=True)\ndata.to_csv('preprocessed_data.csv',index=False)","cc496225":"print(data.shape)\nprint('Final set of features are : ')\ndata.columns","f1412fd3":"import pandas as pd\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler,LabelEncoder,StandardScaler,Normalizer,OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy.sparse import hstack\nfrom sklearn.metrics import f1_score,accuracy_score,roc_auc_score,precision_recall_fscore_support\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve,auc\nimport itertools\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix","852f160f":"data = pd.read_csv('preprocessed_data.csv')\ndata.columns","7bac1381":"data.shape","cd436b30":"#spliting data to train and test data\nX = data.drop('Score',axis=1)\nY = data.Score.values\n\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=.33,stratify=Y,random_state=42)\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","34585971":"std_scaler = Normalizer()\nmin_max = MinMaxScaler()\n\n#payment_sequential feature\npayment_sequential_train = std_scaler.fit_transform(X_train.payment_sequential.values.reshape(-1,1))\npayment_sequential_test = std_scaler.transform(X_test.payment_sequential.values.reshape(-1,1))\n\n#payment_installments feature\npayment_installments_train = std_scaler.fit_transform(X_train.payment_installments.values.reshape(-1,1))\npayment_installments_test = std_scaler.transform(X_test.payment_installments.values.reshape(-1,1))\n\n#Payment value feature\npayment_value_train = std_scaler.fit_transform(X_train.payment_value.values.reshape(-1,1))\npayment_value_test = std_scaler.transform(X_test.payment_value.values.reshape(-1,1))\n\n\n#price\nprice_train = std_scaler.fit_transform(X_train.price.values.reshape(-1,1))\nprice_test = std_scaler.transform(X_test.price.values.reshape(-1,1))\n\n#freight_value\nfreight_value_train = std_scaler.fit_transform(X_train.freight_value.values.reshape(-1,1))\nfreight_value_test = std_scaler.transform(X_test.freight_value.values.reshape(-1,1))\n\n#product_name_length\nproduct_name_length_train = std_scaler.fit_transform(X_train.product_name_length.values.reshape(-1,1))\nproduct_name_length_test = std_scaler.transform(X_test.product_name_length.values.reshape(-1,1))\n\n\n#product_description_length\nproduct_description_length_train = std_scaler.fit_transform(X_train.product_description_length.values.reshape(-1,1))\nproduct_description_length_test = std_scaler.transform(X_test.product_description_length.values.reshape(-1,1))\n\n\n#product_photos_qty\nproduct_photos_qty_train = std_scaler.fit_transform(X_train.product_photos_qty.values.reshape(-1,1))\nproduct_photos_qty_test = std_scaler.transform(X_test.product_photos_qty.values.reshape(-1,1))\n\n\n#delivery_days\ndelivery_days_train = std_scaler.fit_transform(X_train.delivery_days.values.reshape(-1,1))\ndelivery_days_test = std_scaler.transform(X_test.delivery_days.values.reshape(-1,1))\n\n\n#estimated_days\nestimated_days_train = std_scaler.fit_transform(X_train.estimated_days.values.reshape(-1,1))\nestimated_days_test = std_scaler.transform(X_test.estimated_days.values.reshape(-1,1))\n\n\n#ships_in\nships_in_train = std_scaler.fit_transform(X_train.ships_in.values.reshape(-1,1))\nships_in_test = std_scaler.transform(X_test.ships_in.values.reshape(-1,1))\n\n#seller_popularity\nseller_popularity_train = min_max.fit_transform(X_train.seller_popularity.values.reshape(-1,1))\nseller_popularity_test = min_max.transform(X_test.seller_popularity.values.reshape(-1,1))","314eb27f":"#initialising oneHotEncoder\n\nonehot = CountVectorizer()\ncat = OneHotEncoder()\n#payment_type\npayment_type_train = onehot.fit_transform(X_train.payment_type.values)\npayment_type_test = onehot.transform(X_test.payment_type.values)\n\n\n#customer_state\ncustomer_state_train = onehot.fit_transform(X_train.customer_state.values)\ncustomer_state_test = onehot.transform(X_test.customer_state.values)\n\n#seller_state\nseller_state_train = onehot.fit_transform(X_train.seller_state.values)\nseller_state_test = onehot.transform(X_test.seller_state.values)\n\n\n#product_category_name\nproduct_category_name_train = onehot.fit_transform(X_train.product_category_name.values)\nproduct_category_name_test = onehot.transform(X_test.product_category_name.values)\n\n\n#arrival_time\narrival_time_train = onehot.fit_transform(X_train.arrival_time.values)\narrival_time_test = onehot.transform(X_test.arrival_time.values)\n\n#delivery_impression\ndelivery_impression_train = onehot.fit_transform(X_train.delivery_impression.values)\ndelivery_impression_test = onehot.transform(X_test.delivery_impression.values)\n\n\n#estimated_del_impression\nestimated_del_impression_train = onehot.fit_transform(X_train.estimated_del_impression.values)\nestimated_del_impression_test = onehot.transform(X_test.estimated_del_impression.values)\n\n#ship_impression\nship_impression_train = onehot.fit_transform(X_train.ship_impression.values)\nship_impression_test = onehot.transform(X_test.ship_impression.values)\n\n\n#existing_cust\nexisting_cust_train = cat.fit_transform(X_train.existing_cust.values.reshape(-1,1))\nexisting_cust_test = cat.transform(X_test.existing_cust.values.reshape(-1,1))","e4a5dec6":"#stacking up all the encoded features\nX_train_vec = hstack((payment_sequential_train,payment_installments_train,payment_value_train,price_train,\n                      freight_value_train,product_name_length_train,product_description_length_train,\n                      product_photos_qty_train,delivery_days_train,estimated_days_train,ships_in_train,\n                      payment_type_train,customer_state_train,seller_state_train,product_category_name_train,\n                      arrival_time_train,delivery_impression_train,estimated_del_impression_train,\n                     ship_impression_train,seller_popularity_train))\n\nX_test_vec = hstack((payment_sequential_test,payment_installments_test,payment_value_test,price_test,\n                      freight_value_test,product_name_length_test,product_description_length_test,\n                      product_photos_qty_test,delivery_days_test,estimated_days_test,ships_in_test,\n                      payment_type_test,customer_state_test,seller_state_test,product_category_name_test,\n                      arrival_time_test,delivery_impression_test,estimated_del_impression_test,\n                     ship_impression_test,seller_popularity_test))\n\nprint(X_train_vec.shape,X_test_vec.shape)","d1f46522":"naive = MultinomialNB(class_prior=[0.5,0.5])\n\nparam = {'alpha': [0.0001,0.001,0.01,0.1,1,10,100,1000]}\n\n#for the bow based model\nNB = GridSearchCV(naive,param,cv=3,refit=False,return_train_score=True,scoring='roc_auc')\nNB.fit(X_train_vec,y_train)","9d99bcbc":"NB.best_params_","83db47bd":"clf = MultinomialNB(alpha=0.0001,class_prior=[0.5,0.5])\nclf.fit(X_train_vec,y_train)\n\n#predicted value of y probabilities\ny_pred_train = clf.predict_proba(X_train_vec)\ny_pred_test = clf.predict_proba(X_test_vec)\n\n#predicted values of Y labels\npred_label_train = clf.predict(X_train_vec)\npred_label_test = clf.predict(X_test_vec)\n\n#Confusion Matrix\ncf_matrix_train = confusion_matrix(y_train,pred_label_train)\ncf_matrix_test = confusion_matrix(y_test,pred_label_test)\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train,y_pred_train[:,1])\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_pred_test[:,1])\n\ntrain_auc = round(auc(fpr_train,tpr_train),3)\ntest_auc = round(auc(fpr_test,tpr_test),3)\n\nplt.plot(fpr_train,tpr_train,color='red',label='train-auc = '+str(train_auc))\nplt.plot(fpr_test,tpr_test,color='blue',label='test-auc = '+str(test_auc))\nplt.plot(np.array([0,1]),np.array([0,1]),color='black',label='random model auc = '+str(0.5))\nplt.xlabel('False Positive Rate(FPR)')\nplt.ylabel('True Positive Rate(TPR)')\nplt.title('ROC curve')\nplt.legend()\nplt.show()\nprint('Best AUC for the model is {} '.format(test_auc))","6edd055b":"#plot confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_matrix_test\/np.sum(cf_matrix_test), annot=True,fmt='.2%', cmap='Greens')\nplt.show()","b94eecf2":"#f1 score\nprint('Train F1_score for this model is : ',round(f1_score(y_train,pred_label_train),4))\nprint('Test F1_score for this model is : ',round(f1_score(y_test,pred_label_test),4))","53e5d077":"print('Train Accuracy score for this model : ',round(accuracy_score(y_train,pred_label_train),4))\nprint('Test Accuracy score for this model : ',round(accuracy_score(y_test,pred_label_test),4))","efe3cbd6":"#we have used max_iter 1000 as it was causing exception while fitting\nLogi = LogisticRegression(max_iter=1000,solver='lbfgs')\n\nparam = {'C': [0.0001,0.001,0.01,0.1,1,10,20,30]}\n\n#for the bow based model\nLR = GridSearchCV(Logi,param,cv=3,refit=False,return_train_score=True,scoring='roc_auc')\nLR.fit(X_train_vec,y_train)","922fbbc9":"LR.best_params_","c3ea7160":"#model\nclf = LogisticRegression(C=0.1,max_iter=1000,solver='lbfgs')\nclf.fit(X_train_vec,y_train)","a578afa9":"#predicted value of y probabilities\ny_pred_train = clf.predict_proba(X_train_vec)\ny_pred_test = clf.predict_proba(X_test_vec)\n\n#predicted values of Y labels\npred_label_train = clf.predict(X_train_vec)\npred_label_test = clf.predict(X_test_vec)\n\n#Confusion Matrix\ncf_matrix_train = confusion_matrix(y_train,pred_label_train)\ncf_matrix_test = confusion_matrix(y_test,pred_label_test)\n\nfpr_train,tpr_train,threshold_train = roc_curve(y_train,y_pred_train[:,1])\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_pred_test[:,1])\n\ntrain_auc = round(auc(fpr_train,tpr_train),3)\ntest_auc = round(auc(fpr_test,tpr_test),3)\n\nplt.plot(fpr_train,tpr_train,color='red',label='train-auc = '+str(train_auc))\nplt.plot(fpr_test,tpr_test,color='blue',label='test-auc = '+str(test_auc))\nplt.plot(np.array([0,1]),np.array([0,1]),color='black',label='random model auc = '+str(0.5))\nplt.xlabel('False Positive Rate(FPR)')\nplt.ylabel('True Positive Rate(TPR)')\nplt.title('ROC curve')\nplt.legend()\nplt.show()\nprint('Best AUC for the model is {} '.format(test_auc))","7c25fc3f":"#plot confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_matrix_test\/np.sum(cf_matrix_test), annot=True,fmt='.2%', cmap='Greens')\nplt.show()","bf453701":"#f1 score\nprint('Train F1_score for this model is : ',round(f1_score(y_train,pred_label_train),4))\nprint('Test F1_score for this model is : ',round(f1_score(y_test,pred_label_test),4))","98f4a851":"print('Train Accuracy score for this model : ',round(accuracy_score(y_train,pred_label_train),4))\nprint('Test Accuracy score for this model : ',round(accuracy_score(y_test,pred_label_test),4))","e495f036":"#model initialize\nDT = DecisionTreeClassifier(class_weight='balanced')\n\n#hyper parameters\nparam = {'max_depth':  [1, 5,10,15,20], 'min_samples_split': [5, 10, 100, 300,500,1000]}\n\n#Grid search CV\nDT = GridSearchCV(DT,param,cv=3,refit=False,return_train_score=True,scoring='roc_auc')\nDT.fit(X_train_vec,y_train)","de2144b8":"#best params\nDT.best_params_","f1a87877":"#model\nclf = DecisionTreeClassifier(class_weight='balanced',max_depth=20,min_samples_split=300)\nclf.fit(X_train_vec,y_train)\n\n\n#predicted value of y probabilities\ny_pred_train = clf.predict_proba(X_train_vec)\ny_pred_test = clf.predict_proba(X_test_vec)\n\n#predicted values of Y labels\npred_label_train = clf.predict(X_train_vec)\npred_label_test = clf.predict(X_test_vec)\n\n#Confusion Matrix\ncf_matrix_train = confusion_matrix(y_train,pred_label_train)\ncf_matrix_test = confusion_matrix(y_test,pred_label_test)\n\n#taking the probabilit scores instead of the predicted label\n#predict_proba returns probabilty scores which is in the 2nd column thus taking the second column\nfpr_train,tpr_train,threshold_train = roc_curve(y_train,y_pred_train[:,1])\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_pred_test[:,1])\n\ntrain_auc = round(auc(fpr_train,tpr_train),3)\ntest_auc = round(auc(fpr_test,tpr_test),3)\n\nplt.plot(fpr_train,tpr_train,color='red',label='train-auc = '+str(train_auc))\nplt.plot(fpr_test,tpr_test,color='blue',label='test-auc = '+str(test_auc))\nplt.plot(np.array([0,1]),np.array([0,1]),color='black',label='random model auc = '+str(0.5))\nplt.xlabel('False Positive Rate(FPR)')\nplt.ylabel('True Positive Rate(TPR)')\nplt.title('ROC curve')\nplt.legend()\nplt.show()\nprint('Best AUC for the model is {} '.format(test_auc))","05feb7b5":"#plot confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_matrix_test\/np.sum(cf_matrix_test), annot=True,fmt='.2%', cmap='Greens')\nplt.show()","f919cbf1":"#f1 score\nprint('Train F1_score for this model is : ',round(f1_score(y_train,pred_label_train),4))\nprint('Test F1_score for this model is : ',round(f1_score(y_test,pred_label_test),4))","31a82cd8":"print('Train Accuracy score for this model : ',round(accuracy_score(y_train,pred_label_train),4))\nprint('Test Accuracy score for this model : ',round(accuracy_score(y_test,pred_label_test),4))","6bbabe01":"#param grid\n#we have limit max_depth to 10 so that the model doesn't overfit\nparam = { 'min_samples_split':[5,10,30,50,100],'max_depth':[5,7,10]}\n\n#Random forest classifier\nRFclf = RandomForestClassifier(class_weight='balanced')\n\n#using grid search cv to tune parameters\nRF = GridSearchCV(RFclf,param,cv=5,refit=False,n_jobs=-1,verbose=1,return_train_score=True,scoring='roc_auc')\nRF.fit(X_train_vec,y_train)","e7c92f21":"RF.best_params_","51520a04":"#model\nclf = RandomForestClassifier(class_weight='balanced',max_depth=10,min_samples_split=5)\nclf.fit(X_train_vec,y_train)\n\n\n#predicted value of y probabilities\ny_pred_train = clf.predict_proba(X_train_vec)\ny_pred_test = clf.predict_proba(X_test_vec)\n\n#predicted values of Y labels\npred_label_train = clf.predict(X_train_vec)\npred_label_test = clf.predict(X_test_vec)\n\n#Confusion Matrix\ncf_matrix_train = confusion_matrix(y_train,pred_label_train)\ncf_matrix_test = confusion_matrix(y_test,pred_label_test)\n\n#taking the probabilit scores instead of the predicted label\n#predict_proba returns probabilty scores which is in the 2nd column thus taking the second column\nfpr_train,tpr_train,threshold_train = roc_curve(y_train,y_pred_train[:,1])\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_pred_test[:,1])\n\ntrain_auc = round(auc(fpr_train,tpr_train),3)\ntest_auc = round(auc(fpr_test,tpr_test),3)\n\nplt.plot(fpr_train,tpr_train,color='red',label='train-auc = '+str(train_auc))\nplt.plot(fpr_test,tpr_test,color='blue',label='test-auc = '+str(test_auc))\nplt.plot(np.array([0,1]),np.array([0,1]),color='black',label='random model auc = '+str(0.5))\nplt.xlabel('False Positive Rate(FPR)')\nplt.ylabel('True Positive Rate(TPR)')\nplt.title('ROC curve')\nplt.legend()\nplt.show()\nprint('Best AUC for the model is {} '.format(test_auc))","a92f1222":"#plot confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_matrix_test\/np.sum(cf_matrix_test), annot=True,fmt='.2%', cmap='Greens')\nplt.show()","23b596b0":"#f1 score\nprint('Train F1_score for this model is : ',round(f1_score(y_train,pred_label_train),4))\nprint('Test F1_score for this model is : ',round(f1_score(y_test,pred_label_test),4))","c4b458c2":"print('Train Accuracy score for this model : ',round(accuracy_score(y_train,pred_label_train),4))\nprint('Test Accuracy score for this model : ',round(accuracy_score(y_test,pred_label_test),4))","ead833df":"#param grid\n#we have limit max_depth to 8 so that the model doesn't overfit\nparam = { 'min_samples_split' : [5,10,30,50],'max_depth' : [3,5,7,8]}\n\nGBDTclf = GradientBoostingClassifier()\n\nclf = GridSearchCV(RFclf,param,cv=5,refit=False,return_train_score=True,scoring='roc_auc')\nclf.fit(X_train_vec,y_train)","a6128986":"#best parameters\nclf.best_params_","716408da":"import pickle","a96601b6":"#Model\nclf = GradientBoostingClassifier(max_depth=8,min_samples_split=5)\nclf.fit(X_train_vec,y_train)\n\n# save the model to disk\nPkl_Filename = \"final_model.pkl\"  \nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(clf, file)\n\n#predicted value of y probabilities\ny_pred_train = clf.predict_proba(X_train_vec)\ny_pred_test = clf.predict_proba(X_test_vec)\n\n#predicted values of Y labels\npred_label_train = clf.predict(X_train_vec)\npred_label_test = clf.predict(X_test_vec)\n\n#Confusion Matrix\ncf_matrix_train = confusion_matrix(y_train,pred_label_train)\ncf_matrix_test = confusion_matrix(y_test,pred_label_test)\n\n#taking the probabilit scores instead of the predicted label\n#predict_proba returns probabilty scores which is in the 2nd column thus taking the second column\nfpr_train,tpr_train,threshold_train = roc_curve(y_train,y_pred_train[:,1])\nfpr_test,tpr_test,threshold_test = roc_curve(y_test,y_pred_test[:,1])\n\ntrain_auc = round(auc(fpr_train,tpr_train),3)\ntest_auc = round(auc(fpr_test,tpr_test),3)\n\nplt.plot(fpr_train,tpr_train,color='red',label='train-auc = '+str(train_auc))\nplt.plot(fpr_test,tpr_test,color='blue',label='test-auc = '+str(test_auc))\nplt.plot(np.array([0,1]),np.array([0,1]),color='black',label='random model auc = '+str(0.5))\nplt.xlabel('False Positive Rate(FPR)')\nplt.ylabel('True Positive Rate(TPR)')\nplt.title('ROC curve')\nplt.legend()\nplt.show()\nprint('Best AUC for the model is {} '.format(test_auc))","9270eee0":"#plot confusion matrix\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_matrix_test\/np.sum(cf_matrix_test), annot=True,fmt='.2%', cmap='Greens')\nplt.show()","86dfe7e2":"#f1 score\nprint('Train F1_score for this model is : ',round(f1_score(y_train,pred_label_train),4))\nprint('Test F1_score for this model is : ',round(f1_score(y_test,pred_label_test),4))","cc21ce91":"print('Train Accuracy score for this model : ',round(accuracy_score(y_train,pred_label_train),4))\nprint('Test Accuracy score for this model : ',round(accuracy_score(y_test,pred_label_test),4))","df635f4e":"#setting input length to number of features\ninput_length = X_train_vec.shape[1]","7088a5b4":"##imports\nimport os\nimport sys\nimport os\nimport random as rn\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport tensorflow as tf\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import roc_auc_score\nfrom keras.utils import np_utils\nfrom tensorflow.keras.regularizers import l2\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPool1D,Conv1D, MaxPool1D, Embedding,concatenate,Flatten,Dropout,LSTM,Concatenate,LeakyReLU\nfrom tensorflow.keras.models import Model","5aed7f03":"##create an NN and train\nos.environ['PYTHONHASHSEED'] = '0'\n## Set the random seed values to regenerate the model.\ntf.keras.backend.clear_session()\nnp.random.seed(0)\nrn.seed(0)\n\n#input layer\ninput_layer = Input(shape=(input_length,1))\n\n#Conv layer\nConv1 = Conv1D(filters=10,kernel_size=3,strides=1,padding='valid',activation='relu',kernel_initializer='he_normal',name='Conv1')(input_layer)\n\n#Conv layer\nConv2 = Conv1D(filters=10,kernel_size=3,strides=1,padding='valid',activation='relu',kernel_initializer='he_normal',name='Conv2')(Conv1)\n\n#Conv layer\nConv3 = Conv1D(filters=10,kernel_size=3,strides=1,padding='valid',activation='relu',kernel_initializer='he_normal',name='Conv3')(Conv2)\n\n\n#MaxPool Layer\npool1 = MaxPool1D(5)(Conv3)\n\n#Flatten Layer\nflatten_1 = Flatten()(pool1)\n\n#Batch Normalisation to ensure the training is fast\nBM = BatchNormalization()(flatten_1)\n\n#FC layer\nFC1 = Dense(units=64,activation='relu',kernel_initializer='he_normal',kernel_regularizer=l2(0.001),name='FC1')(BM)\n\n#dropout\ndrop1 = Dropout(rate = 0.5)(FC1)\n\n#Batch Noramlisation\nBM2 = BatchNormalization()(drop1)\n\n#FC layer\nFC3 = Dense(units=32,activation='relu',kernel_initializer='he_normal',kernel_regularizer=l2(0.001),name='FC3')(BM2)\n# FC3= LeakyReLU(alpha = 0.3)(FC3)\n\n#output layer\nOut = Dense(units=2,activation='softmax',name='Output')(FC3)\n\nmodel = Model(inputs=input_layer,outputs=Out)\n\n\nmodel.summary()\n# tf.keras.utils.plot_model(model, \"model_3.png\", show_shapes=True)\n","dde7ba14":"X_train_vec = np.expand_dims(X_train_vec.toarray(),2)\nX_test_vec = np.expand_dims(X_test_vec.toarray(),2)","171fc5a1":"y_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)","35ca5a44":"#auc roc score can be undefined if no data from one class is present in the batch\n#so handling such case with custom auc_roc function which returns [0.5,0.5] if there are no points for one class\nfrom sklearn.metrics import roc_auc_score\ndef auroc(y_true, y_pred):\n    return tf.numpy_function(roc_auc_score, (y_true, y_pred), tf.double)","6d347ce6":"#callbacks\nfrom tensorflow.keras.callbacks import *\n\n#TensorBoard Callback\nlog_dir=\"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\nreduce_lr_1 = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=1, min_lr=0.002,verbose = 1)\ntensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)","c8395eff":"#compiling and fitting the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001),loss='categorical_crossentropy',metrics=[auroc])\nmodel.fit(X_train_vec,y_train,batch_size=512,workers=16,epochs=20,callbacks=[tensorboard_callback,reduce_lr_1],validation_data=(X_test_vec,y_test))","6dd08c3d":"pred_label_train = model.predict(X_train_vec)\npred_label_train = [1 if x[1] > x[0] else 0 for x in pred_label_train]\npred_label_test = model.predict(X_test_vec)\npred_label_test = [1 if x[1] > x[0] else 0 for x in pred_label_test]","7978082d":"#Please run train_test split cell to represent y_train and y_test in 1D vector instead of onehot encoded vector.\n#f1 score\nprint('Train F1_score for this model is : ',round(f1_score(y_train,pred_label_train),4))\nprint('Test F1_score for this model is : ',round(f1_score(y_test,pred_label_test),4))","b1b2112b":"print('Train Accuracy score for this model : ',round(accuracy_score(y_train,pred_label_train),4))\nprint('Test Accuracy score for this model : ',round(accuracy_score(y_test,pred_label_test),4))","9749e327":"from prettytable import PrettyTable\n\ntable = PrettyTable()\ntable.field_names = [\"Model\", \"F1_score\", \" AUC_score \",\" Accuracy \"]\n\ntable.add_row([ \"Naive Bayes\",'0.8575','0.694','0.7689'])\ntable.add_row([\"Logistic Regression\",'0.9217','0.699','0.8605'])\ntable.add_row([\"Decision Tree\",'0.8031','0.713','0.7021'])\ntable.add_row([\"Random Forest\",'0.9013','0.718','0.8315',])\ntable.add_row([\"GBDT**(BEST)\",'0.9243','0.745','0.8651'])\ntable.add_row([\"Deep NN\",'0.9233','0.710','0.8629'])\n\nprint(table)","712c4152":"#imports\nimport pandas as pd\nfrom sklearn.metrics import f1_score,accuracy_score,roc_auc_score,confusion_matrix,roc_curve,auc\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport seaborn as sns\nimport scipy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle","76c8d7b1":"#Takes test data as input and results Test labels as ouput\ndef function1(X_test):\n    #loads best model\n    # Load the Model back from file\n    with open('final_model.pkl', 'rb') as file:  \n        clf = pickle.load(file)\n    \n    #predict labels\n    pred_label = clf.predict(X_test)\n    \n    #predicted value of y probabilities\n    pred_proba = clf.predict_proba(X_test)\n    \n    return [pred_label,pred_proba]","659c07b3":"#takes true label and predicted label and shows all necessary performance plots and metrics\ndef function2(y_true,y_pred,pred_proba):\n    #Confusion Matrix\n    cf_matrix_test = confusion_matrix(y_true,y_pred)\n\n    #taking the probabilit scores instead of the predicted label\n    #predict_proba returns probabilty scores which is in the 2nd column thus taking the second column\n    fpr_test,tpr_test,threshold_test = roc_curve(y_true,pred_proba[:,1])\n    test_auc = round(auc(fpr_test,tpr_test),3)\n    \n    #ROC_AUC plot and score\n    print('ROC_AUC curve plot :\\n\\n')\n    plt.figure(figsize=(10,8))\n    plt.plot(fpr_test,tpr_test,color='blue',label='test-auc = '+str(test_auc))\n    plt.plot(np.array([0,1]),np.array([0,1]),color='black',label='random model auc = '+str(0.5))\n    plt.xlabel('False Positive Rate(FPR)')\n    plt.ylabel('True Positive Rate(TPR)')\n    plt.title('ROC curve')\n    plt.legend()\n    plt.show()\n    print('\\n\\t*********Best AUC for the model is {}  **********'.format(test_auc))\n    \n    #plot confusion matrix\n    print('\\n\\nConfusion Matrix : \\n\\n')\n    plt.figure(figsize=(10,8))\n    sns.heatmap(cf_matrix_test\/np.sum(cf_matrix_test), annot=True,fmt='.2%', cmap='Greens')\n    plt.show()\n    \n    #f1 score\n    print('\\n\\nTest F1_score for this model is : ----->',round(f1_score(y_true,y_pred),4))\n    \n    #accuracy score\n    print('\\n\\nTest Accuracy score for this model : ----->',round(accuracy_score(y_true,y_pred),4))","94f2a015":"# Function Call\n#Function 1 to get the predicted values\nX_test = sp.sparse.load_npz('X_test_vec.npz')\npred_label,pred_proba = function1(X_test)\n\n#Function 2 to get the performance metrics and plots\ny_test = pd.read_csv('test_labels.csv').values.ravel()\nfunction2(y_test,pred_label,pred_proba)","6fadaf19":"# Summary","464110b9":"**Customer_ID **","4b3870b8":"# Function 1","d437e951":"# Summary\n\n1. GBDT performs better in comparision to rest of the model in terms of all the performance metric.\n2. Logistic regression performs fairly similar to GBDT, but GBDT is more robust to outliers.\n3. Rating prediction is not fairly dependent directly on most of the features, so the performance is not at its peak.\n4. We have used f1 score as our primary performance metric.\n5. We have taken care of overfitting in each model.\n6. Each model performed better after we removed neutral review score from the data.","4d6d98ab":"**Payment method Feature**","2f39b0bc":"# Results","cf10a772":"# Observations\n\n1. Logistic regression performs considerably better than Naive bayes in terms of f1 score, however AUC score being almost the same.\n2. Misclassification of False positives reduced which resulted in the increase of f1 score of 92%.\n3. Accuracy was 86% for both train and test which shows the model doesn't overfit at all.","286d4687":"# Hyper parameter tuning","aa26ca42":"# GBDT","b6806550":"1. Price is centered around 40-120 Brazilian real, so the box plot is not able separate any of the review score based on the price as hue. \n\n2. User rating is infact not depended soley on the price of the product.","f9492f93":"**Observations**\n\n* As in most of the cases customer and seller zip_code are not matching so we are dropping the featrure\n because it will increase dimenssion by 34k.","8e3f1316":"**Importing Libraries**","74d95a42":"**Price feature**","5e8654d1":"# Data cleaning and preprocessing","adfe9083":"**Observations**\n\n1. Data is severly immbalaced, so scatter plot is able to separate the classes based on the selected feature\n\n2. Point in the range (0-1500) product_description_length and product_name_length in range(30-60) have high frequency\nof 5 rated review.\n","4b6175b0":"**Observations**\n\n1. Clearly from the above plot the cutomers are more likely to give an 4-5 rating if the product either arrives early or arrive on time.\n\n2. As delivery time impacts a lot to the customer rating we will do further analysis on the delivery time.\n","c9a3ea5d":"**Sellers Data**","f62113ce":"# Decision Tree","1a5d99c0":"**NOTE**\n\n* For performance measurement we will not use accuracy as a metric as the data set is highly imbalanced.\n* We will use AUC score and f1 score as performance metric.","8ccf3892":"**Order_Payments**","921dfd9a":"# *Pipelining*","624784a4":"# Observations\n\n1. Decision Tree does nothing better interms of both f1 score , auc score and accuracy comes out to be 0.708 and 70%.\n2. It misclassfied False Positives to a lot.\n3. Model doesn't overfit but doesn't perform better either.","175db1fe":"**Customer data - contains information about the customer**","2c52fb5c":"**Observations**\n\n* There are 3095 unique sellers and most of seller are in State SP.","977073d4":"# Observations\n\n1. Random forest performs better than logistic regression in terms of f1 score and accuracy.\n2. It gives an f1 score of 90.13% and doesn't seem to overfit.\n3. Misclassification rate is still not that great.\n4. AUC is score is 0.718\n5. Accuracy score is 83%.","94edfc7c":"# Logistic Regression","f8f0a95b":"# Observations\n\n1. Gradient Boosted classifier results the best f1 score of 0.9243 and auc score of 0.745.\n2. Misclassification of False Positives and True negetives is also reduced to 11% also true positive rate is 83%.\n3. Accuracy score is 86% for test and 87% for train data.\n4. Model does overfit a slight comapred to rest of the models.","7e4f7a99":"# Feature Engineering","fcdf4645":"**Observations**\n\n* Density plots for product_name_length is dense around (20-60) and product_description length around (0-2000) \n\n* Above CDF shows that about 92% of the product have product_description_length to be less than or equal to 2000.\n\n* PDF shows that probability of getting product_description_length 500-100 have the maximum probablity.","8add8df6":"**Stacking the data**","5a0d507c":"# Exploratory Data Analysis","e83f83e6":"**Observations**\n\n* Review_score by maximum customers is 5 star(57%) and 4star(19.2%)\n\n*  review_comment_title and review_comment_message have lots of entires as blank or null, which is a problem. This is however xpected because most customers don't prefer to write reviews.\n    \n*  As the percentage of null\/blank value is over 30% (here it is about 80%) , so we drop these two features.","a5f33bf5":"**Loading Data**","170ba275":"**Removing all NULL values**","24629a47":"High Accuracy \n\nLow latency (Rating should be known within the completion of the order)\n\nProne to outliers\n","fa5e58e6":"# Feature Engineering for ID based features","f972ace6":"# Observations\n\n1. Naive bayes performed pretty decent in terms of minimal overfitting in train and test performances.\n2. Both train and test f1 score was 0.86 and accuracy 77%.\n3. But the confusion matrix says it has misclassified many points as False Positives.\n4. AUC score for test data was 0.694.","5239cb27":"**Observations**\n\n1. There are many features having null\/blank entires too , but their proportion is less than 30%, so we can simply remove all such rows.\n\n2. Moreover, we have the product category name in a different language, so we simply rename the english version of the category names to category_name and delete the other.\n\n3. To add on to this , we can clearly remove very detailed information about the product i.e product_description_lenght, product_name_lenght, product_weight_g, product_length_cm, product_height_cm, product_width_cm as they dont decide wheather a customer will like it or not.\n\n4. Only feature in product section that can be useful is number of photos the product has, if it has many photos customer customer is well aware of how the product looks from all possible ways and is more certainly sure about going for it , thus increasing the chance of getting a positive rating.\n\n5. On a similar note , product_description lenght can also act somewhat useful, because if the description of the product is detailed and long , customer is most likely to know evrey crux of the product thus increasing the high rating probability.\n\n6. We dont clearly need latitudinal and longtuditnal info, seller_city(seller state is enough)\n\n7. Order_approved_at is a datetime based feature which is useless, because customer is least intersted when the order was approved, because in almost all case it gets approved instantly or within some minutes or hours. It is a trivial feature.\n\n8. As we have already removed product reviews and review titles , there is no use of review id now.\n\n9. As we have merged all the tables into a single one, so the there is no need some primary key or id. So all id can dropped except the customer unique id which may be useful to study the customer as not all the customers are same and seller_id and product_id","ba55da6c":"**Observations**\n\n1. Density plots for the product name length shows that density around is very high around (40-60) range.\n2. Number of orders w.r.t number of photos qty starts to decrease with increasing in number of photos which is not\nvery obvious. This may be because of other feature like product_description length or name length.","ab4f668f":"**Box plot**","c58799bf":"**Observation**","9d25242c":"*Seller_ID*","73d0f74f":"# Hyperparameter Tuning","59f5b373":"**Observation**\n\n1. Most customers have made the payment through vouchers i.e 73.8% and then credit card about 19.5%.\n2. Number of installments majority of the time is 1, that is full payment was done. Otherwise it is not more than\n    4 in most of the cases.","4064dd6f":"**Order_reviews data**","a56f0bf7":"# Function 2","8fc73e7b":"# Standard Neural Network based model\n","e4d1d1a0":"# Fitting the Model","bececc49":"**Products Data**","56a0a042":"**Order_item Data ( each item in a particular order )**","ec830c54":"\n* Data is heavily imbalanced having mostly positive review_score, this can impact the model so we have to perform necessiry data modelling to deal with this. \n\n* From above analysis it is quite conclusive that the review_score highly depends on the delivery time, however some of the rest features are also useful but we need actual model to determine feature importances.\n\n* Other features like price , product name length, description length can be very useful feature, but as the data is immbalace it can not be simple EDA.\n\n* We have added few more features to the dataset that coresponds to the delivery and shipment.\n\n* We will be strict about sentiments in this analysis and consider even neutral or (3 rating) to be negetive because for product recommendation the stricter the benchmark is the better.\n\n* Feature engineering done here might not be suffiicient to obtain the best accuracy so we might need to perform some more feature engineering.","0e3bdfa1":"**Month wise sales**","f66abafe":"# *Modelling*","44bf3440":"Most customers do not post a review rating or any comment after purchasing a product which is a challenge for any ecommerce platform to perform If a company predicts whether a customer liked\/disliked a product so that they can recommend more similar and related products as well as they can decide whether or not a product should be sold at their end.\nThis is crucial for ecommerce based company because they need to keep track of each product of each seller , so that none of products discourage their customers to come shop with them again. Moreover, if a specific product has very few rating and that too negetive, a company must not drop the product straight away, may be many customers who found the product to be useful haven't actually rated it.\n\nSome reasons could possibly be comparing your product review with those of your competitors beforehand,gaining lots of insight about the product and saving a lot of manual data pre-processin,maintain good customer relationship with company,lend gifts, offers and deals if the company feels the customer is going to break the relation.\n\nObjective of this case study is centered around predicting customer satisfaction with a product which can be deduced after predicting the product rating a user would rate after he makes a purchase.","7ed0513f":"**Observations**\n\n1. Almost 97.77% of orders are marked as delivered, some values are canceled, approved e.t.c. Note that such circumstances are very rare which shows in the data too, so this feature is of no use. We can drop it.\n\n2. There are some orders which have missing order_delivered_carrier_date, order_delivered_customer_date and very few have order_approved_at missing.\n","314da4be":"**Observations**\n\n\n1. Majority of orders are delivered in the Fast range i.e (8-15) within 2 weeks .\n\n2. However, next to it Very Fast range of delivery days which is (0-7) days.\n\n3. Important observation here is customers are more likely to get things delivered in top 3 categories which are Fast,Very Fast and Neutral. From which majority of elment have rated the product to be 5 , some 4 while very few have rated less than in each of the 3 cases.\n\n4. Delivery days is thus an important feature which can be added.\n\n\n5. Moreover we have another feature which is estmated delivery date, so we also add a new feature to the dataset which is estimated delivery days. This can be some what useful, because this date is shown in advance to the user.\n\n6. In the estimated delivery days feature though, company says out the maximum days it can take to reach. \n\n7. Upon observing the histograms for the estimated_days with rating as hue, similar trend of maximum rating for all the Fast Neutral and slow category. However, very fast category becomes one of the least. \n","a7e4eead":"# Observations\n\n1. We created a standard deep Neural network model and trained it for 20 epochs this resulted f1 score very similar to our best ML model yet which is GBDT.\n2. Kindly note that this neural network was very little hyper-parameter tuning done,and still results in a very decent performance.\n3. However the auc score of GBDT is still better than the NN model.\n4. Important thing to note that NN based models can be much better than conventional ML models for such problems.","6df7fd44":"**State wise sales(Top 10 states with maximum orders)**","ebe9dd2d":"# Problem statement","309ab408":"**Observations**\n\n* Even after taking the most favourable data to clearly separate them out, there is still no separation.","70710976":"# Constraints","cb12726f":"# Hyper parameter Tuning","7596744d":"# Hyper parameter Tuning","922ec7b9":"**Orders_data**","be4ba129":"# Understanding the data","31a3e08a":"# Normalising Categorical features","46b4320d":"**PDF and CDF for product description length**","7465b855":"# Naive Bayes","7d7cea4a":"# Random Forest","2cc94569":"# Normalising all the numerical features","19f46428":"# HyperParmater tuning"}}