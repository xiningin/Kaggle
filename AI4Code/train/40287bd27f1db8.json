{"cell_type":{"b985ea6e":"code","05715e40":"code","0ff78f0e":"code","9b6b9ed2":"code","4b4d15d4":"code","809ea670":"code","71ce922d":"code","009abf6d":"code","1a4a1a46":"code","eae3a323":"code","51d26b16":"code","133a070c":"code","ccafd8c7":"code","0c4b1d35":"code","1e43e3f6":"code","fd6dbd7d":"code","20a8971f":"code","42144e9b":"markdown","1b139103":"markdown","b790a9a8":"markdown","dbc65993":"markdown","11eaa2cf":"markdown","96cccc47":"markdown","59238b4a":"markdown","40727e67":"markdown","adee3578":"markdown","99252636":"markdown","1fe3c28d":"markdown","99f8d14d":"markdown","0dba27c9":"markdown","a6868e18":"markdown","82f07f93":"markdown","a4d29bd6":"markdown","e1f798d8":"markdown","10be35a5":"markdown"},"source":{"b985ea6e":"!apt-get -y install ffmpeg freeglut3-dev xvfb  # For visualization\n!pip install stable-baselines3[extra]","05715e40":"import stable_baselines3\nstable_baselines3.__version__","0ff78f0e":"import gym\nimport numpy as np","9b6b9ed2":"from stable_baselines3 import PPO","4b4d15d4":"from stable_baselines3.ppo import MlpPolicy","809ea670":"env = gym.make('CartPole-v1')\n\nmodel = PPO(MlpPolicy, env, verbose=0)","71ce922d":"def evaluate(model, num_episodes=100):\n    \"\"\"\n    Evaluate a RL agent\n    :param model: (BaseRLModel object) the RL Agent\n    :param num_episodes: (int) number of episodes to evaluate it\n    :return: (float) Mean reward for the last num_episodes\n    \"\"\"\n    # This function will only work for a single Environment\n    env = model.get_env()\n    all_episode_rewards = []\n    for i in range(num_episodes):\n        episode_rewards = []\n        done = False\n        obs = env.reset()\n        while not done:\n            # _states are only useful when using LSTM policies\n            action, _states = model.predict(obs)\n            # here, action, rewards and dones are arrays\n            # because we are using vectorized env\n            obs, reward, done, info = env.step(action)\n            episode_rewards.append(reward)\n\n        all_episode_rewards.append(sum(episode_rewards))\n\n    mean_episode_reward = np.mean(all_episode_rewards)\n    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n\n    return mean_episode_reward","009abf6d":"from stable_baselines3.common.evaluation import evaluate_policy","1a4a1a46":"# Use a separate environement for evaluation\neval_env = gym.make('CartPole-v1')\n\n# Random Agent, before training\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n\nprint(f\"mean_reward:{mean_reward:.2f} +\/- {std_reward:.2f}\")","eae3a323":"# Train the agent for 10000 steps\nmodel.learn(total_timesteps=10000)","51d26b16":"# Evaluate the trained agent\nmean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n\nprint(f\"mean_reward:{mean_reward:.2f} +\/- {std_reward:.2f}\")","133a070c":"# Set up fake display; otherwise rendering will fail\nimport os\nos.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\nos.environ['DISPLAY'] = ':1'","ccafd8c7":"import base64\nfrom pathlib import Path\n\nfrom IPython import display as ipythondisplay\n\ndef show_videos(video_path='', prefix=''):\n  \"\"\"\n  Taken from https:\/\/github.com\/eleurent\/highway-env\n\n  :param video_path: (str) Path to the folder containing videos\n  :param prefix: (str) Filter the video, showing only the only starting with this prefix\n  \"\"\"\n  html = []\n  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n      video_b64 = base64.b64encode(mp4.read_bytes())\n      html.append('''<video alt=\"{}\" autoplay \n                    loop controls style=\"height: 400px;\">\n                    <source src=\"data:video\/mp4;base64,{}\" type=\"video\/mp4\" \/>\n                <\/video>'''.format(mp4, video_b64.decode('ascii')))\n  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))","0c4b1d35":"from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n\ndef record_video(env_id, model, video_length=500, prefix='', video_folder='videos\/'):\n  \"\"\"\n  :param env_id: (str)\n  :param model: (RL model)\n  :param video_length: (int)\n  :param prefix: (str)\n  :param video_folder: (str)\n  \"\"\"\n  eval_env = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n  # Start the video at step=0 and record 500 steps\n  eval_env = VecVideoRecorder(eval_env, video_folder=video_folder,\n                              record_video_trigger=lambda step: step == 0, video_length=video_length,\n                              name_prefix=prefix)\n\n  obs = eval_env.reset()\n  for _ in range(video_length):\n    action, _ = model.predict(obs)\n    obs, _, _, _ = eval_env.step(action)\n\n  # Close the video recorder\n  eval_env.close()","1e43e3f6":"record_video('CartPole-v1', model, video_length=500, prefix='ppo-cartpole')","fd6dbd7d":"show_videos('videos', prefix='ppo')","20a8971f":"model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)","42144e9b":"We will record a video using the [VecVideoRecorder](https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook.","1b139103":"In fact, Stable-Baselines3 already provides you with that helper:","b790a9a8":"Let's evaluate the un-trained agent, this should be a random agent.","dbc65993":"### Prepare video recording","11eaa2cf":"## Bonus: Train a RL Model in One Line\n\nThe policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/quickstart.html).","96cccc47":"The first thing you need to import is the RL model, check the documentation to know what you can use on which problem","59238b4a":"The next thing you need to import is the policy class that will be used to create the networks (for the policy\/value functions).\nThis step is optional as you can directly use strings in the constructor: \n\n```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n\nNote that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option.","40727e67":"### Visualize trained agent\n\n","adee3578":"## Introduction to Reinforcement Learning (RL)\n\nReinforcement learning is the training of machine learning models to make a sequence of decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. In reinforcement learning, an artificial intelligence faces a game-like situation. The computer employs trial and error to come up with a solution to the problem. To get the machine to do what the programmer wants, the artificial intelligence gets either rewards or penalties for the actions it performs. Its goal is to maximize the total reward.\n\nAlthough the designer sets the reward policy\u2013that is, the rules of the game\u2013he gives the model no hints or suggestions for how to solve the game. It\u2019s up to the model to figure out how to perform the task to maximize the reward, starting from totally random trials and finishing with sophisticated tactics and superhuman skills. By leveraging the power of search and many trials, reinforcement learning is currently the most effective way to hint machine\u2019s creativity. In contrast to human beings, artificial intelligence can gather experience from thousands of parallel gameplays if a reinforcement learning algorithm is run on a sufficiently powerful computer infrastructure.\n\nFor example, in usual circumstances we would require an autonomous vehicle to put safety first, minimize ride time, reduce pollution, offer passengers comfort and obey the rules of law. With an autonomous race car, on the other hand, we would emphasize speed much more than the driver\u2019s comfort. The programmer cannot predict everything that could happen on the road. Instead of building lengthy \u201cif-then\u201d instructions, the programmer prepares the reinforcement learning agent to be capable of learning from the system of rewards and penalties. The agent (another name for reinforcement learning algorithms performing the task) gets rewards for reaching specific goals.\n\n## Introduction to Stable Baselines3 (SB3)\n\nStable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines.\n\nYou can read a detailed presentation of Stable Baselines3 in the v1.0 blog post.\n\nThese algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n\n\n-------\nIn this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n\n<span style='color:green'>Please support the notebook through your valuable comments and votes.<\/spam>\n\nLet's start\n","99252636":"## Conclusion\n\nIn this notebook we have seen:\n- how to define and train a RL model using stable baselines3, it takes only one line of code ;)\n","1fe3c28d":"## Imports","99f8d14d":"## Create the Gym env and instantiate the agent\n\nFor this example, we will use CartPole environment, a classic control problem.\n\n\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n\nCartpole environment: [https:\/\/gym.openai.com\/envs\/CartPole-v1\/](https:\/\/gym.openai.com\/envs\/CartPole-v1\/)\n\n![Cartpole](https:\/\/cdn-images-1.medium.com\/max\/1143\/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n\nNote: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the DummyVecEnv.\n\nWe chose the MlpPolicy because input of CartPole is a feature vector, not images.\n\nThe type of action to use (discrete\/continuous) will be automatically deduced from the environment action space\n\n\nHere we are using the [Proximal Policy Optimization](https:\/\/stable-baselines.readthedocs.io\/en\/master\/modules\/ppo2.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n\nIt combines ideas from [A2C](https:\/\/stable-baselines.readthedocs.io\/en\/master\/modules\/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https:\/\/stable-baselines.readthedocs.io\/en\/master\/modules\/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n\nPPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\nIt is usually less sample efficient than off-policy alorithms like [DQN](https:\/\/stable-baselines.readthedocs.io\/en\/master\/modules\/dqn.html), [SAC](https:\/\/stable-baselines.readthedocs.io\/en\/master\/modules\/sac.html) or [TD3](https:\/\/stable-baselines.readthedocs.io\/en\/master\/modules\/td3.html), but is much faster regarding wall-clock time.\n","0dba27c9":"## Install Dependencies and Stable Baselines3 Using Pip\n\n```\npip install stable-baselines3[extra]\n```","a6868e18":"We create a helper function to evaluate the agent:","82f07f93":"Apparently the training went well, the mean reward increased a lot ! ","a4d29bd6":"## Continue learning: Stable Baselines3\n\nGithub repo: https:\/\/github.com\/araffin\/rl-tutorial-jnrr19\n\nStable-Baselines3: https:\/\/github.com\/DLR-RM\/stable-baselines3\n\nDocumentation: https:\/\/stable-baselines.readthedocs.io\/en\/master\/\n\nRL Baselines3 zoo: https:\/\/github.com\/DLR-RM\/rl-baselines3-zoo\n\n\n[RL Baselines3 Zoo](https:\/\/github.com\/DLR-RM\/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n\nIt also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.","e1f798d8":"## Train the agent and evaluate it","10be35a5":"Stable-Baselines works on environments that follow the [gym interface](https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/custom_env.html).\nYou can find a list of available environment [here](https:\/\/gym.openai.com\/envs\/#classic_control).\n\nIt is also recommended to check the [source code](https:\/\/github.com\/openai\/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\nNot all algorithms can work with all action spaces, you can find more in this [recap table](https:\/\/stable-baselines.readthedocs.io\/en\/master\/guide\/algos.html)"}}