{"cell_type":{"1b7fc717":"code","2fbe9a3e":"code","1784142e":"code","597c0ec1":"code","6c04cafc":"code","001e165a":"code","524971ea":"code","70e91384":"code","a0f13f67":"code","e1f7c510":"code","b7f38d8a":"code","58c3b351":"code","c1e9cd34":"code","a1e21732":"code","4c229de7":"code","0888602e":"code","af34d80f":"code","8454874f":"code","d25e99a4":"code","687e1723":"code","6100be3e":"code","cd826430":"code","1c5d0540":"code","7db0c6fd":"code","2e8d3d30":"code","d0d123a2":"markdown","67d9d999":"markdown","db4d07e1":"markdown","0eb7f578":"markdown","68bd7323":"markdown","c0dff124":"markdown","03e6c402":"markdown"},"source":{"1b7fc717":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport warnings \nwarnings.filterwarnings('ignore')\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2fbe9a3e":"train = pd.read_csv('\/kaggle\/input\/60k-stack-overflow-questions-with-quality-rate\/train.csv')","1784142e":"train.head()","597c0ec1":"train.columns","6c04cafc":"train.info()","001e165a":"train = train[['Body','Y']]\ntrain = train[:10000]","524971ea":"train[\"Body\"] = train['Body'].str.replace('<p>','')","70e91384":"import string\ndef remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.lower().replace(punctuation, '')\n    return text\ntrain[\"Body\"] = train['Body'].apply(remove_punctuations)\ntrain.head()","a0f13f67":"\ntrain.loc[train['Y'] == 'LQ_CLOSE', 'Y'] = 0\ntrain.loc[train['Y'] == 'LQ_EDIT', 'Y'] = 1\ntrain.loc[train['Y'] == 'HQ', 'Y'] = 2\n","e1f7c510":"train.head(3)","b7f38d8a":"train.Y = train.Y.astype('int')","58c3b351":"train.Y.value_counts()","c1e9cd34":"sns.countplot(x = train.Y).set_title('Category')\nplt.show()","a1e21732":"import nltk\nfrom nltk.corpus import stopwords\nimport re\nimport nltk as nlp\nnltk.download('stopwords') \nnltk.download('punkt')\nnltk.download('wordnet') \n\nlemma = nlp.WordNetLemmatizer()","4c229de7":"\ntext_list = []\nfor description in train.Body:\n    \n    description = re.sub('[^a-zA-Z]',' ',description)\n    #description = description.lower()\n    description = nltk.word_tokenize(description)\n    #description = [ word for word in description if not word in set(stopwords.words('english'))]\n    #lemma = nlp.WordNetLemmatizer()\n    #description = [lemma.lemmatize(word)for word in description]\n    description = ' '.join(description)\n    text_list.append(description)","0888602e":"len(text_list)","af34d80f":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features = 300\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n\nspace_matrix = count_vectorizer.fit_transform(text_list).toarray()  # x\n","8454874f":"space_matrix.shape","d25e99a4":"y = train.iloc[:,1].values   \nx = space_matrix\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.1, random_state = 42)\nprint('X_train shape', X_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_train shape', y_train.shape)\nprint('y_test shape', y_test.shape)","687e1723":"#Libraries\n\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor , RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n","6100be3e":"\nrandom_state = 42\nclassifier = [LogisticRegression(random_state = random_state),\n                  RandomForestClassifier(random_state = random_state),\n                  DecisionTreeClassifier(),\n                  KNeighborsClassifier()]\n\nlogreg_param_grid = {'C': np.logspace(-3,3,7),\n                    'penalty': ['l1','l2']}\n\n\nrf_param_grid = rf_param_grid = {'max_features':[1,5],\n                'min_samples_split': [2,3],\n                'min_samples_leaf':[1,3],\n                'bootstrap':[False],\n                'n_estimators':[100],\n                'criterion': ['gini']}\n\ndt_param_grid = {'min_samples_split': range(10,50,2),\n                'max_depth': range(1,10,2)}\n\nknn_param_grid = {'n_neighbors': np.linspace(1,6,2, dtype = int).tolist()}\n\nclassifier_param = [logreg_param_grid ,rf_param_grid,dt_param_grid, knn_param_grid]","cd826430":"from sklearn.model_selection import StratifiedKFold,GridSearchCV\nfrom sklearn.metrics import accuracy_score \ncv_results = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i],param_grid= classifier_param[i],cv = StratifiedKFold(n_splits = 10),scoring = 'accuracy',n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_results.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_results[i])","1c5d0540":"cv_result = pd.DataFrame({'Cross Validation Means': cv_results, 'ML Models': ['LogisticRegression',\n              'RandomForestClassifier',\n              'DecisionTreeClassifier',\n              'KNeighborsClassifier'\n                ] })\n\ng = sns.barplot('Cross Validation Means','ML Models',data = cv_result)\ng.set_xlabel('Mean Accuracy')\ng.set_title('Cross Validation Scores')\nplt.show()","7db0c6fd":"cv_result","2e8d3d30":"best_estimators","d0d123a2":"<a id = \"5\"><\/a>\n## Cross-Validation  Hyperparameter Tuning","67d9d999":"<a id = \"1\"><\/a>\n## Load and Check Data","db4d07e1":"<a id = '6'><\/a>\n## CONCLUSION","0eb7f578":"<a id = \"3\"><\/a>\n## Data Preprocessing","68bd7323":"<a id = '4'><\/a>\n## MODELLING","c0dff124":" ## Introduction \n\n\n<font color = \"blue\">\nContent:\n\n1. [Load and check Data](#1)\n2. [Variable Description](#2)\n3. [Data Preprocessing](#3)\n\n1. [MODELLING](#4)\n    * [Cross-Validation Hyperparameter Tuning](#5)\n    * [CONCLUSION](#6)\n","03e6c402":"<a id=\"2\"> <\/a> \n## Variable Description\n\n* Id: Unique number of each questions\n* Title: The title of questions\n* Body: The questions\n* CreationDate: Which time question created\n* Y: Quality class \n    * HQ: High-quality posts with a total of 30+ score and without a single edit. (2)\n    * LQ_EDIT: Low-quality posts with a negative score, and multiple community edits. However, they still remain open after those changes. (1)\n    * LQ_CLOSE: Low-quality posts that were closed by the community without a single edit. (0)\n"}}