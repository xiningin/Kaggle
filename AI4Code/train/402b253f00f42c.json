{"cell_type":{"371b9367":"code","f0766d77":"code","90815cd1":"code","d8b8c224":"code","0feba3d5":"code","49ac0d72":"code","2a82c692":"code","9f2374ac":"code","a6f0e952":"code","10ed78a8":"code","93d5e0dd":"code","65f1266f":"code","d623e94f":"code","123d88b3":"code","2ea1154f":"code","e5c7ce7c":"code","25c5fcb3":"code","d54926ad":"code","9e69c541":"markdown","915715fe":"markdown"},"source":{"371b9367":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\npy.offline.init_notebook_mode(connected = True)\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0766d77":"df = pd.read_csv('..\/input\/design-thinking-arxiv\/arxiv_papers.csv', delimiter=',', encoding = \"utf8\")\n\ndf.head()","90815cd1":"df_abstract = df['abstract'].str.lower()\ndf_abstract.head()","d8b8c224":"x= len(df_abstract)\nprint(x)","0feba3d5":"df.abstract","49ac0d72":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\nval_df = df.sample(frac=0.1, random_state=1007)\ntrain_df = df.drop(val_df.index)\ntest_df = train_df.sample(frac=0.1, random_state=1007)\ntrain_df.drop(test_df.index, inplace=True)","2a82c692":"val_df","9f2374ac":"text = \"\"\nfor i,r in df.iterrows():\n    text += r.abstract + '\\n'","a6f0e952":"text[:1000]","10ed78a8":"df['title'].loc[:100]","93d5e0dd":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\nfor line in df.abstract.loc[:200]:\n    input_text = line\n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n    input_texts.append(input_text)    \nfor line in df['title'].loc[:200]: \n    target_text = line\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = \"\\t\" + target_text + \"\\n\"\n   \n    target_texts.append(target_text)\n    \n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","65f1266f":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\n'''input_characters = set()\ntarget_characters = set()\nfor char in input_texts:\n    if char not in input_characters:\n        input_characters.add(char)\nfor char in target_texts:\n    if char not in target_characters:\n         target_characters.add(char)'''\n\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\n'''max_encoder_seq_length = max(abstract_count)\nmax_decoder_seq_length = max(title_count)\n'''\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint(\"Number of samples:\", len(input_texts))\nprint(\"Number of unique input tokens:\", num_encoder_tokens)\nprint(\"Number of unique output tokens:\", num_decoder_tokens)\nprint(\"Max sequence length for inputs:\", max_encoder_seq_length)\nprint(\"Max sequence length for outputs:\", max_decoder_seq_length)\n\ninput_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n)\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n)\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n)\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.0\n    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.0\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0","d623e94f":"import tensorflow as tf\nfrom tensorflow import keras","123d88b3":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\nbatch_size = 64  # Batch size for training.\nepochs = 10 # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nnum_samples = 1000  # Number of samples to train on.","2ea1154f":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\n# Define an input sequence and process it.\nencoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\nencoder = keras.layers.LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","e5c7ce7c":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\nmodel.compile(\n    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n)\nmodel.fit(\n    [encoder_input_data, decoder_input_data],\n    decoder_target_data,\n    batch_size=batch_size,\n    epochs=100,\n    validation_split=0.2,\n)\n# Save model\nmodel.save(\"s2s\")","25c5fcb3":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\nmodel = keras.models.load_model(\"s2s\")\n\nencoder_inputs = model.input[0]  # input_1\nencoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\nencoder_states = [state_h_enc, state_c_enc]\nencoder_model = keras.Model(encoder_inputs, encoder_states)\n\ndecoder_inputs = model.input[1]  # input_2\ndecoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\ndecoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4\")\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_lstm = model.layers[3]\ndecoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs\n)\ndecoder_states = [state_h_dec, state_c_dec]\ndecoder_dense = model.layers[4]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = keras.Model(\n    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index['\\t']] = 1.0\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = \"\"\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.0\n\n        # Update states\n        states_value = [h, c]\n    return decoded_sentence","d54926ad":"#Code by Doaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\nfor seq_index in range(3):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index : seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print(\"-\")\n    print(\"Input sentence:\", input_texts[seq_index])\n    print(\"Decoded sentence:\", decoded_sentence)","9e69c541":"#Train the model","915715fe":"#Acknowledgement:\n\nDoaajaber https:\/\/www.kaggle.com\/doaajaber\/titles-prediction-read-text\n\n"}}