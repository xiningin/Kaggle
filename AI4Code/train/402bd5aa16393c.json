{"cell_type":{"643e64a0":"code","734aeb98":"code","ab44673d":"code","f103867c":"code","a6b45cde":"code","7e7eec2f":"code","795186cd":"code","8fbbf592":"code","9c559383":"code","65c00c01":"code","92264108":"code","4365f283":"code","bfae4a8b":"code","3e96705c":"code","6359cd5e":"code","8c5a794e":"code","5f1a513f":"code","74f50cf1":"code","e3a48333":"code","b9966ecc":"code","d0f3f148":"code","3f334a7b":"code","c287293b":"code","ebe07418":"code","2ea823bd":"code","5669c841":"code","b68f1a60":"code","e3723703":"code","d75399e7":"code","a48aef5a":"code","c4b540cb":"code","f21a7956":"code","eccbadc0":"code","a5d294f8":"code","37f0643c":"code","a326b2f5":"code","3c4f9283":"code","5d3aad1e":"markdown","6cc00cfd":"markdown"},"source":{"643e64a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.set_printoptions(precision=4)\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score , confusion_matrix, f1_score, roc_auc_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom IPython.display import display,Markdown,HTML\nimport warnings\nwarnings.filterwarnings('ignore')","734aeb98":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head(5)","ab44673d":"df.columns","f103867c":"df.info()","a6b45cde":"df['Churn'] = df['Churn'].map({'No':0, 'Yes':1})\ndf['TotalCharges'] = pd.to_numeric(df.TotalCharges,errors='coerce')","7e7eec2f":"df.isna().sum()","795186cd":"df.dropna(inplace=True)","8fbbf592":"df.isna().sum()","9c559383":"df.duplicated().sum()","65c00c01":"df = df.drop(['customerID'],axis=1)\ndf.columns","92264108":"text_negative = \"Negative\"\ntext_positive = \"Positive\"\ntarget_column = \"Churn\"\n\ndf_all = df.copy()\n\ndf_positive = df[df[target_column]==1]\n\ndf_negative = df[df[target_column]==0]","4365f283":"def plot_pie(column, title=\"All Group\/Class\"):\n    fig,axs = plt.subplots(1,1)\n    data = df_all[column].value_counts()\n    plt.pie(data,autopct='%1.2f%%',labels=data.index)\n    plt.title(title)\n    plt.show()\n    \ndef plot_hist(column, title=\"All Group\/Class\"):\n    plt.hist(df_all[column],density=True)\n    plt.title(title)\n    plt.show()\n\ndef plot_bar(column, sort=False, title=\"All Group\/Class\"):\n    if sort:\n        data_all = df_all[column].value_counts().sort_index()\n    else:\n        data_all = df_all[column].value_counts()\n    plt.bar(data_all.index.astype(str),data_all)\n    plt.title(title)\n    plt.show()\n    \ndef plot_bar_compare(column, sort=False):\n    if sort:\n        data_positive = df_positive[column].value_counts().sort_index()\n        data_negative = df_negative[column].value_counts().sort_index()\n    else:\n        data_positive = df_positive[column].value_counts()\n        data_negative = df_negative[column].value_counts()\n    \n    fig,axs = plt.subplots(2,1)\n    plt.subplots_adjust(left=0, bottom=0, right=1, top=2, wspace=0, hspace=0.2)\n    axs[0].bar(data_negative.index.astype(str),data_negative)\n    axs[0].title.set_text(text_negative)\n    axs[1].bar(data_positive.index.astype(str),data_positive)\n    axs[1].title.set_text(text_positive)\n    plt.show()\n\ndef plot_hist_compare(column, bins=5):\n    plt.hist([df_negative[column], df_positive[column]] , color=['c','r'])\n    plt.legend((text_negative, text_positive))\n    plt.show()\n    \ndef plot_pie_compare(column):\n    data_positive = df_positive[column].value_counts()\n    data_negative = df_negative[column].value_counts()\n    \n    fig,axs = plt.subplots(2,1)\n    plt.subplots_adjust(left=0, bottom=0, right=1, top=2, wspace=0, hspace=0.2)\n    axs[0].pie(data_negative,autopct='%1.2f%%',labels=data_negative.index)\n    axs[0].title.set_text(text_negative)\n    axs[1].pie(data_positive,autopct='%1.2f%%',labels=data_positive.index)\n    axs[1].title.set_text(text_positive)\n    plt.show()\n\ndef plot_boxplot(column, title=\"\"):\n    ax = sns.boxplot(x=target_column, y=column, palette=[\"c\", \"r\"],\n            hue=target_column,  data=df_all).set_title(title, fontsize=15)\n    plt.show()\n\ndef check_median(column):\n    data_negative = df_negative[column].describe()\n    data_positive = df_positive[column].describe()\n    print(\"Median:\")\n    print('{}: {}'.format(text_negative,data_negative['50%']))\n    print('{}: {}'.format(text_positive,data_positive['50%']))\n\ndef check_most(column):\n    data_negative = df_negative[column].value_counts()\n    data_positive = df_positive[column].value_counts()\n    print(\"Most:\")\n    print('{}: {}'.format(text_negative,data_negative.index[0]))\n    print('{}: {}'.format(text_positive,data_positive.index[0]))","bfae4a8b":"def eda(df_all):\n    display(HTML('<h1>Exploratory Data Analysis<h1>'))\n    \n    for column in df_all.columns:\n        if column == target_column:\n            continue\n        display(HTML('<h2>{}<h2>'.format(column)))\n        if df[column].dtype == 'int64' or df[column].dtype == 'float64':\n            if len(df[column].unique())>10 :\n                plot_boxplot(column)\n                check_median(column)\n            else:\n                plot_bar(column)\n                plot_pie(column)\n                plot_pie_compare(column)\n                check_most(column)\n        elif df[column].dtype == 'object':\n            if len(df[column].unique())>10 :\n                df[column].value_counts().head(5)\n                df_negative[column].value_counts().head(5)\n                df_positive[column].value_counts().head(5)\n            else:\n                plot_bar(column)\n                plot_pie(column)\n                plot_pie_compare(column)\n                check_most(column)\n        else:\n            None","3e96705c":"df['Churn'].value_counts()","6359cd5e":"plot_pie('Churn')","8c5a794e":"eda(df_all)","5f1a513f":"plot_hist('TotalCharges')","74f50cf1":"plot_hist_compare('TotalCharges')","e3a48333":"df['gender'] = df['gender'].map({'Female':0, 'Male':1})\ndf['Partner'] = df['Partner'].map({'No':0, 'Yes':1})\ndf['Dependents'] = df['Dependents'].map({'No':0, 'Yes':1})\ndf['PhoneService'] = df['PhoneService'].map({'No':0, 'Yes':1})\ndf['PaperlessBilling'] = df['PaperlessBilling'].map({'No':0, 'Yes':1})","b9966ecc":"df.head(5)","d0f3f148":"X = df.copy()\n\ny = X['Churn']\n\nX = X.drop(['Churn'], axis=1)","3f334a7b":"#transform categorical data\nX = pd.get_dummies(X, columns=['MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV','StreamingMovies','Contract','PaymentMethod'], drop_first=True)","c287293b":"X.columns","ebe07418":"X.info()","2ea823bd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)","5669c841":"from imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=1234)\n\nX_sm, y_sm = sm.fit_resample(X_train, y_train)\n\nprint(f'''Shape of X before SMOTE: {X.shape}\nShape of X after SMOTE: {X_sm.shape}''')\n\nprint('\\nBalance of positive and negative classes (%):')\ny_sm.value_counts(normalize=True) * 100","b68f1a60":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_sm = sc.fit_transform(X_sm)\nX_test = sc.transform(X_test)","e3723703":"# Import ML Libraries\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [[CatBoostClassifier(verbose=0),'CatBoost Classifier'],[XGBClassifier(eval_metric='error'),'XGB Classifier'], [RandomForestClassifier(),'Random Forest'], \n    [KNeighborsClassifier(), 'K-Nearest Neighbours'], [SGDClassifier(),'SGD Classifier'], [SVC(),'SVC'],[LGBMClassifier(),'LGBM Classifier'],\n              [GaussianNB(),'GaussianNB'],[DecisionTreeClassifier(),'Decision Tree Classifier'],[LogisticRegression(),'Logistic Regression'],[AdaBoostClassifier(),\"AdaBoostClassifier\"]]","d75399e7":"for cls in classifiers:\n    model = cls[0]\n    model.fit(X_sm, y_sm)\n    \n    y_pred = model.predict(X_test)\n    print(cls[1])\n    print ('Confusion Matrix:')\n    print(confusion_matrix(y_test, y_pred))\n    print(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\n    print(\"Recall : \", recall_score(y_test, y_pred) *  100)\n    print(\"Precision : \", precision_score(y_test, y_pred) *  100)\n    print(\"F1 : \", f1_score(y_test, y_pred) *  100)\n    print(\"ROC AUC : \", roc_auc_score(y_test, y_pred) *  100)\n    print('\\n')","a48aef5a":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.losses import BinaryCrossentropy","c4b540cb":"X_train.shape","f21a7956":"#train the model\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(32, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","eccbadc0":"opt = Adam(learning_rate=0.0001)\nearlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=15, verbose=1,restore_best_weights=True)\nmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nhistory = model.fit(X_sm, y_sm, batch_size=32, epochs=200,validation_split = 0.15, callbacks = [earlystopper],verbose = 1)\nhistory_dict = history.history","a5d294f8":"loss_values = history_dict['loss']\nval_loss_values=history_dict['val_loss']\nplt.plot(loss_values,'b',label='training loss')\nplt.plot(val_loss_values,'r',label='val training loss')\nplt.legend()\nplt.xlabel(\"Epochs\")","37f0643c":"accuracy_values = history_dict['accuracy']\nval_accuracy_values=history_dict['val_accuracy']\nplt.plot(val_accuracy_values,'-r',label='val_accuracy')\nplt.plot(accuracy_values,'-b',label='accuracy')\nplt.legend()\nplt.xlabel(\"Epochs\")","a326b2f5":"y_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)\ny_pred = [1 if x == True else 0 for x in y_pred]","3c4f9283":"print(confusion_matrix(y_test, y_pred))\nprint(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\nprint(\"Recall : \", recall_score(y_test, y_pred) *  100)\nprint(\"Precision : \", precision_score(y_test, y_pred) *  100)\nprint(\"F1 : \", f1_score(y_test, y_pred) *  100)\nprint(\"ROC AUC : \", roc_auc_score(y_test, y_pred) *  100)","5d3aad1e":"# Data Preprocessing","6cc00cfd":"The best algorithm is **ANN**\n\n* Accuracy :  78.60696517412936\n* Recall :  57.938718662952645\n* Precision :  58.10055865921788\n* F1 :  58.0195258019526\n* ROC AUC :  71.81287078185801"}}