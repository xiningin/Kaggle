{"cell_type":{"6accfaa2":"code","fd788923":"code","9abfd1d0":"code","af00fdb4":"code","2b074db7":"code","e7baa38e":"code","5647c33c":"code","312d103c":"code","acd15b5d":"code","b1dbe66a":"code","87860fcd":"code","ae0ea3f5":"code","92188840":"code","afc64218":"code","c747dcc5":"code","5fef5798":"code","c5b61d16":"code","59247d2a":"code","a0587795":"code","7335f8fc":"code","58e040db":"code","e0b872ac":"code","a1f4ce12":"code","36cf36b5":"code","6483ab4a":"code","ac756425":"code","1288a664":"code","f41d65ae":"code","84394f68":"code","cffe1c6e":"code","9857edc0":"code","c5a22708":"code","e158caa5":"code","e7083cf1":"code","71d72ec3":"code","873b548b":"code","91b05840":"code","31021d55":"code","0ebb5d4c":"code","1158b084":"code","b007fa4a":"code","f4688499":"code","d89cf325":"code","6ac3a5a5":"code","04858df1":"code","c9614aa8":"code","446a91f0":"code","4bee2993":"code","6560b2f2":"code","552b3dd5":"code","ee13a1e7":"code","0c62d95b":"code","59285422":"markdown","bda25416":"markdown","9599bd07":"markdown","99de43ee":"markdown","e39614ec":"markdown","702b6e5b":"markdown","800fc6dd":"markdown","a70e7736":"markdown","708b1edf":"markdown","995ac0a0":"markdown","2c749dd8":"markdown","d4c97fc1":"markdown","24179b4a":"markdown","95144fc3":"markdown","389cabaf":"markdown","ef03069b":"markdown","0bf39677":"markdown","785c0e39":"markdown","4ecf2fc7":"markdown","eae8f8ca":"markdown","efd0e068":"markdown","08ef6895":"markdown","eb257125":"markdown","7e5116c1":"markdown","857eb0fa":"markdown","91a2e1f2":"markdown","5819646c":"markdown","b109758d":"markdown","93c1f4c6":"markdown","1c3ffadc":"markdown","b7a71a97":"markdown","456ff5db":"markdown","f4a83064":"markdown","771b14e7":"markdown","ff90345b":"markdown","78db0fec":"markdown","fb1bdb40":"markdown","2d0c3f19":"markdown","e4f2b575":"markdown"},"source":{"6accfaa2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport datetime\nimport seaborn as sns\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom numpy import genfromtxt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fd788923":"calendar_data = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsell_price_data = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","9abfd1d0":"sales_train_validation.shape","af00fdb4":"sales_train_validation.head()","2b074db7":"date_col = [col for col in sales_train_validation if col.startswith('d_')]","e7baa38e":"len(date_col)","5647c33c":"sales_train_validation.state_id.value_counts()","312d103c":"sales_train_validation['total_sales'] = sales_train_validation[date_col].sum(axis=1)","acd15b5d":"sales_train_validation['total_sales'].head()","b1dbe66a":"sales_train_validation.groupby('state_id').agg({\"total_sales\":\"sum\"}).reset_index()","87860fcd":"print(sales_train_validation.state_id.value_counts())\n#Tilf\u00f8j ny kolonne til dataset med total salg (summen af alle dato kolonner)\nsales_train_validation['total_sales'] = sales_train_validation[date_col].sum(axis=1)\n#Calculating the sales ratio\ntotal_salg_per_stat = sales_train_validation.groupby('state_id').agg({\"total_sales\":\"sum\"})\/sales_train_validation.total_sales.sum() * 100\ntotal_salg_per_stat = total_salg_per_stat.reset_index()\n#Plotting the sales ratio\nfig1, ax1 = plt.subplots()\n#Opret et nyt pie chart vha. matplotlib\nax1.pie(total_salg_per_stat['total_sales'],labels= total_salg_per_stat['state_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nplt.tight_layout()\nplt.title(\"Total salg per stat\",fontweight = \"bold\")\nplt.show()","ae0ea3f5":"print(sales_train_validation.groupby('store_id').agg({\"total_sales\":\"sum\"}).reset_index())\n\n#Finder den totale salgsrate fordelt p\u00e5 de enkelte butikker\ntotal_salg_per_butik=sales_train_validation.groupby('store_id').agg({\"total_sales\":\"sum\"})\/sales_train_validation.total_sales.sum() * 100\n#Lav et piechart som viser fordelingen i procent\ntotal_salg_per_butik = total_salg_per_butik.reset_index()\nfig1, ax1 = plt.subplots()\nax1.pie(total_salg_per_butik['total_sales'],labels= total_salg_per_butik['store_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Totale solgte varer i procent per butik\",fontweight = \"bold\")\nplt.show()","92188840":"print(sales_train_validation.groupby('cat_id').agg({\"total_sales\":\"sum\"}).reset_index())\n\n#Finder den totale salgsrate fordelt p\u00e5 de enkelte kategorier\ntotal_salg_per_kategori = sales_train_validation.groupby('cat_id').agg({\"total_sales\":\"sum\"})\/sales_train_validation.total_sales.sum() * 100\ntotal_salg_per_kategori = total_salg_per_kategori.reset_index()\n#Lav et piechart som viser fordelingen i procent\nfig1, ax1 = plt.subplots()\nax1.pie(total_salg_per_kategori['total_sales'],labels= total_salg_per_kategori['cat_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Totale solgte varer i procent per kategori\",fontweight = \"bold\")\nplt.show()","afc64218":"sales_train_validation.groupby(['state_id','cat_id']).agg({\"total_sales\":\"sum\"}).reset_index()","c747dcc5":"dept_sales = sales_train_validation.groupby('dept_id').agg({\"total_sales\":\"sum\"}).reset_index()\nprint(dept_sales)\n#Finder den totale salgsrate fordelt p\u00e5 de enkelte afdelinger\ntotal_salg_per_afdeling = sales_train_validation.groupby('dept_id').agg({\"total_sales\":\"sum\"})\/sales_train_validation.total_sales.sum() * 100\n#Lav et piechart som viser fordelingen i procent\ntotal_salg_per_afdeling = total_salg_per_afdeling.reset_index()\nfig1, ax1 = plt.subplots()\nax1.pie(total_salg_per_afdeling['total_sales'],labels= total_salg_per_afdeling['dept_id'] , autopct='%1.1f%%',\n        shadow=True, startangle=90)# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\nplt.title(\"Totale solgte varer i procent per afdeling\",fontweight = \"bold\")\nplt.show()","5fef5798":"sell_price_data.head()","c5b61d16":"sell_price_data.groupby(['store_id', 'item_id']).agg({\"sell_price\": [\"max\", \"min\"]}).reset_index()","59247d2a":"#Unders\u00f8gelse af pris\u00e6ndringer for de enkelte items over tid, med min og max v\u00e6rdier\n#Aggregerer min og max prisv\u00e6rdier for de enkelte items i hver butik\nitem_store_prices = sell_price_data.groupby([\"item_id\",\"store_id\"]).agg({\"sell_price\":[\"max\",\"min\"]})\n#print(item_store_prices.head())\n#\u00c6ndre navn p\u00e5 kolonner for min og max v\u00e6rdier\nitem_store_prices.columns = [f'{i}_{j}' if j != '' else f'{i}' for i,j in item_store_prices.columns]\n#Tilf\u00f8jer en ny 'price change' kolonne med pris\u00e6ndring for hver item per butik\nitem_store_prices[\"price_change\"] = item_store_prices[\"sell_price_max\"] - item_store_prices[\"sell_price_min\"]\n#print(item_store_prices.head())\n#Laver en ny data frame med v\u00e6rdierne sorteret efter pris\u00e6ndring\nitem_store_prices_sorted = item_store_prices.sort_values([\"price_change\",\"item_id\"],ascending=False).reset_index()\n#Tilf\u00f8jer en ny kolonne 'category' med navnet for kategorien per item\nitem_store_prices_sorted[\"category\"] = item_store_prices_sorted[\"item_id\"].str.split(\"_\",expand = True)[0]\n#print(item_store_prices_sorted.head())\n#Boxplot af pris\u00e6ndringer ved brug af seaborn\nsns.boxplot(x=\"price_change\", y=\"category\", data=item_store_prices_sorted)\ntitle = plt.title(\"Boxplot over pris\u00e6ndringer for alle items opdelt efter kategori\")","a0587795":"calendar_data.head()","7335f8fc":"calendar_data.shape","58e040db":"snap_days = calendar_data.groupby(['year','month'])['snap_CA','snap_TX','snap_WI'].sum().reset_index()\nsnap_days.pivot(index=\"month\",columns = \"year\",values = [\"snap_CA\",\"snap_TX\",\"snap_WI\"])","e0b872ac":"start_date = datetime.datetime(2011,1,29)","a1f4ce12":"sales_sum = pd.DataFrame(sales_train_validation[date_col].sum(axis =0),columns = [\"sales\"])\n","36cf36b5":"#tilf\u00f8jer dato kolonne vha. for in range loop\nsales_sum['date'] = [start_date + datetime.timedelta(days=x) for x in range(1913)]\nsales_sum.set_index('date', drop=True, inplace=True)\nprint(sales_sum)","6483ab4a":"sales_sum.plot()","ac756425":"clndr = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nclndr['date'] = pd.to_datetime(clndr.date)\nclndr['days'] = clndr['date'].dt.day\n\ndf = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n\ndata = pd.DataFrame(df.groupby(by= ['cat_id','dept_id','item_id','store_id']).sum())\n\n# Daily Sales data for each category\n\nfood = pd.DataFrame(data.xs('FOODS').sum(axis = 0))\nhobbies = pd.DataFrame(data.xs('HOBBIES').sum(axis = 0))\nhouse = pd.DataFrame(data.xs('HOUSEHOLD').sum(axis = 0))\n\nclndr = pd.merge(clndr,food,how = 'left',left_on=clndr['d'],right_on = food.index)\ndel clndr['key_0']\nclndr.rename(columns = {0:'food'},inplace = True)\nclndr = pd.merge(clndr,hobbies,how = 'left',left_on=clndr['d'],right_on = hobbies.index)\ndel clndr['key_0']\nclndr.rename(columns = {0:'hobby'},inplace = True)\nclndr = pd.merge(clndr,house,how = 'left',left_on=clndr['d'],right_on = house.index)\ndel clndr['key_0']\nclndr.rename(columns = {0:'house'},inplace = True)\n\ncln = clndr[0:1913]\n\nl1 = ['FOODS','HOBBIES','HOUSEHOLD']\nl2 = list(df['store_id'].unique())\nfor cat in l1:\n    for store in l2:\n        tmp = pd.DataFrame(data.xs(cat).xs(store,level = 2 ).sum(axis = 0))\n        tmp.reset_index(inplace = True)\n        tmp.rename(columns = {0:(cat+'_'+store).lower()},inplace = True)\n        cln = pd.concat([cln,tmp[(cat+'_'+store).lower()]],axis = 1)\n\ngrps = cln.groupby(by=['year','month'])\ndef plot_trend(factor,subplots):\n    if subplots == True:\n        f, a = plt.subplots(3,2,figsize = (14,10))\n        if type(factor) == list:\n            for i,fact in enumerate(factor):\n                check = grps.agg(fact=(fact,'sum'))\n                check.rename(columns = {'fact':factor[i]},inplace=True)\n                check.xs(2011).plot(ax=a[0,0])\n                check.xs(2012).plot(ax=a[0,1])\n                check.xs(2013).plot(ax=a[1,0])\n                check.xs(2014).plot(ax=a[1,1])\n                check.xs(2015).plot(ax=a[2,0])\n                check.xs(2016).plot(ax=a[2,1])\n                \n        else:\n            check = grps.agg({factor:'sum'})\n            check.xs(2011).plot(ax=a[0,0])\n            check.xs(2012).plot(ax=a[0,1])\n            check.xs(2013).plot(ax=a[1,0])\n            check.xs(2014).plot(ax=a[1,1])\n            check.xs(2015).plot(ax=a[2,0])\n            check.xs(2016).plot(ax=a[2,1])\n        a[0,0].title.set_text('2011')\n        a[0,1].title.set_text('2012')\n        a[1,0].title.set_text('2013')\n        a[1,1].title.set_text('2014')\n        a[2,0].title.set_text('2015')\n        a[2,1].title.set_text('2016')\n        f.tight_layout()\n        f.suptitle('Monthly Sales Trends')\n    else:\n        fig,ax = plt.subplots(figsize = (20,5))\n        for fact in factor:\n            cln.set_index('date')[fact].rolling(30).mean().plot(label = fact)\n            plt.legend()\n            fig.suptitle('30 Day Moving Average')","1288a664":"plot_trend(['food','hobby','house'],subplots = True)","f41d65ae":"plot_trend(['food'],subplots = False)","84394f68":"plot_trend(['hobby'],subplots = False)","cffe1c6e":"plot_trend(['house'],subplots = False)","9857edc0":"result = seasonal_decompose(sales_sum, model='additive')\nresult.plot()\nplt.show()","c5a22708":"calendar_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/calendar.csv', parse_dates=['date'], usecols=['date','d'])\ncalendar_stv = calendar_df[:1913] \n\nsales_train_df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv', index_col='id')\n\nstore_dept = sales_train_df.groupby(by= ['cat_id'], axis=0).mean()\nstore_dept.columns = calendar_stv['date']\nstore_trans = store_dept.transpose()","e158caa5":"weekends = ['01-03-2015','01-04-2015','01-10-2015','01-11-2015','01-17-2015', '01-18-2015','01-24-2015', '01-25-2015', '01-31-2015', \n            '02-01-2015', '02-07-2015', '02-08-2015', '02-14-2015', '02-15-2015', '02-21-2015', '02-22-2015', '02-28-2015', \n            '03-01-2015', '03-07-2015', '03-08-2015', '03-14-2015', '03-15-2015', '03-21-2015', '03-22-2015', '03-28-2015',  '03-29-2015']","e7083cf1":"plt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = [25, 5]\nax = store_trans['01-01-2015':'04-02-2015'].plot(title=\"Gns. salg 3 m\u00e5neder jan-mar 2015\")\nax.set_ylabel('# enheder')\nax.vlines(weekends, 0, 2.5, colors=['y','c'])\nplt.show()","71d72ec3":"sales_train_validation.dtypes","873b548b":"train_dataset = sales_train_validation[date_col[-100:-30]]\nval_dataset = sales_train_validation[date_col[-30:]]","91b05840":"predictions = []\nsummary = []\n##An ARMA model is an ARIMA model where the d parameter in the order is 0\nfor row in (train_dataset[train_dataset.columns[-100:]].values[:3]):\n    print(row)\n    fit = SARIMAX(row, seasonal_order=(1, 1, 1, 7)).fit()\n    predictions.append(fit.forecast(30))\n    summary.append(fit.summary())\n    fit.plot_diagnostics()\npredictions = np.array(predictions).reshape((-1, 30))\nerror_arima = np.linalg.norm(predictions[:3] - val_dataset.values[:3])\/len(predictions[0])","31021d55":"print(predictions)\nprint(summary)","0ebb5d4c":"pred_1 = predictions[0]\npred_2 = predictions[1]\npred_3 = predictions[2]\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[0].values, mode='lines', marker=dict(color=\"darkorange\"),\n               name=\"Val\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_1, mode='lines', marker=dict(color=\"seagreen\"),\n               name=\"Pred\"),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[1].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_2, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=val_dataset.loc[2].values, mode='lines', marker=dict(color=\"darkorange\"), showlegend=False),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=np.arange(70, 100), y=pred_3, mode='lines', marker=dict(color=\"seagreen\"), showlegend=False,\n               name=\"Denoised signal\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=1200, width=800, title_text=\"ARIMA\")\nfig.show()","1158b084":"predict_dataset = sales_train_validation[sales_train_validation.columns[-500:]].values[:5]\n\nprint(predict_dataset)\n","b007fa4a":"#results = []\n#for row in (sales_train_validation[sales_train_validation.columns[-100:]].values):\n#    fit = SARIMAX(row, seasonal_order=(1, 1, 1, 7)).fit()\n#    results.append(fit.forecast(28))\n#print(results)","f4688499":"results = genfromtxt(\"sarimax-predictions.csv\", delimiter=',')","d89cf325":"column_index = [1,2,3,4,5]\nfor i in range(6 , len(sales_train_validation.columns)):\n    column_index.append(i)\n\nclus_hobbies = sales_train_validation.iloc[:,column_index].query(\"cat_id == 'HOBBIES'\")\nclus_household = sales_train_validation.iloc[:,column_index].query(\"cat_id == 'HOUSEHOLD'\")\nclus_foods = sales_train_validation.iloc[:,column_index].query(\"cat_id == 'FOODS'\")\nclus_ca = sales_train_validation.iloc[:,column_index].query(\"state_id == 'CA'\")\nclus_tx = sales_train_validation.iloc[:,column_index].query(\"state_id == 'TX'\")\nclus_wi = sales_train_validation.iloc[:,column_index].query(\"state_id == 'WI'\")\nclus = sales_train_validation.iloc[:,column_index]","6ac3a5a5":"calendar_data[\"event_type_1_snap\"] = pd.notna(calendar_data[\"event_type_1\"]) \ncalendar_data[\"event_type_2_snap\"] = pd.notna(calendar_data[\"event_type_2\"]) \ncalendar_data[\"date\"] =  pd.to_datetime(calendar_data[\"date\"])\ncalendar_data[\"d_month\"] = calendar_data[\"date\"].dt.day\ncalendar_data[\"year\"] = pd.to_numeric(calendar_data[\"year\"])\ncalendar_data[\"wday\"] = pd.to_numeric(calendar_data[\"wday\"])\nprint(calendar_data.shape)\ncalendar_data.head()","04858df1":"#Bucket columns by calander days of month\ncolumnsets = []\nfor i in range(1,32):      \n    d = calendar_data[:1913].query(\"d_month == \"+ str(i))[\"d\"]\n    columnsets.append([d.values])\n","c9614aa8":"# Label encoding for catagorical data\ndef label_encoding(data_preap,cat_features):\n    categorical_names = {}\n    data = []\n    encoders = []\n    \n    data = data_preap[:]\n    for feature in cat_features:\n        le = LabelEncoder()\n        le.fit(data.iloc[:,feature])\n        data.iloc[:, feature] = le.transform(data.iloc[:, feature])\n        categorical_names[feature] = le.classes_\n        encoders.append(le)\n    X_data = data.astype(float)\n    return X_data, encoders","446a91f0":"# Training random forest model\ndef train_model(X_train, X_test, Y_train, Y_test):\n    # Random forest regressor model with Training dataset\n    start_time = datetime.today()\n    regressor = RandomForestRegressor(n_estimators = 350, random_state = 50)\n    regressor.fit(X_train,Y_train)\n\n    print(\"Time taken to Train Model: \" + str(datetime.today() - start_time))\n\n    # Running Regession model score check\n    Y_score = regressor.score(X_test,Y_test)\n    return regressor,Y_score","4bee2993":"# Predict function from model\ndef model_predict(regressor,X_data):\n    # Predicting model model result\n    Y_pred = regressor.predict(X_data)\n    return Y_pred","6560b2f2":"# Validating model with last year data & generating rmse value for the model predection\ndef validate_model(regressor,X_validation, Y_validation):\n   \n    Y_validation_pred = model_predict(regressor, X_validation)\n    mse = mean_squared_error(Y_validation, Y_validation_pred)\n    rmse = np.sqrt(mse)\n    return rmse, Y_validation_pred","552b3dd5":"# Basic function for geting data from pandas based on range\ndef get_data_range(Inital_Range,start_index,end_index):\n    result = []\n    [result.append(a) for a in Inital_Range]\n    for i in range(max(Inital_Range) +1 + start_index, end_index):\n        result.append(i)\n    return result","ee13a1e7":"# main function to run predictions\ndef run_predictions(orig_data):\n    process_data = orig_data[:]\n    results = pd.DataFrame()\n    for s in range(1,29):\n        categorical_features = [0,1]\n        data = []\n        data_range = []\n        for i in range(0,s):\n            [data_range.append(a) for a in columnsets[i]]\n        data_list = [process_data[a] for a in data_range]\n        data  = pd.concat(data_list,axis = 1)\n\n\n        data.insert(loc=0, column='item_id', value=process_data[\"item_id\"])\n        data.insert(loc=1, column='store_id', value=process_data[\"store_id\"])\n        X_data_preap = data[:]\n\n        d = get_data_range(categorical_features,0,len(X_data_preap.columns)-1)   \n        X,label_encoders = label_encoding(X_data_preap.iloc[:,d],categorical_features)\n        Y = X.iloc[:,-1]\n\n        d_validation = get_data_range(categorical_features,1,len(X_data_preap.columns))   \n        X_validation,label_encoders_validation = label_encoding(X_data_preap.iloc[:,d_validation],categorical_features)\n        Y_validation = X_validation.iloc[:,-1]\n\n        print(\"Running Model for Day \" + str(s))\n        # Sampling data for train & split\n        X_train, X_test, Y_train, Y_test = train_test_split(X.iloc[:,0:len(X.columns)-1],Y,test_size = 0.2, random_state = 0)\n        model, score = train_model(X_train, X_test, Y_train, Y_test)\n        print(\"Model Score: \" + str(score))\n        \n       # Uncomment for inital model\n        rmse,validation_predictions = validate_model(model,X_validation.iloc[:,0:len(X_validation.columns)-1], Y_validation)\n        print(\"RMSE Result: \" + str(rmse))\n        \n        if (len(results.columns) == 0):\n            for feature in categorical_features:\n                results[feature] = label_encoders_validation[feature].inverse_transform(X_validation.iloc[:,feature].astype(int))\n\n        results[\"d_\" + str(s)] = validation_predictions.astype(int)\n        print(results)\n        results.to_csv('pd_predictions_' + str(s) +'.csv')\n    return results","0c62d95b":"#pd_predictions = run_predictions(clus)","59285422":"Her 'cluster' vi data fra de 3 kategorier og de 3 stater i hver sit dataset, som vi s\u00e5 samler i 'clus'. ","bda25416":"Metode til at validere mod sidste \u00e5rs data","9599bd07":"# Import libraries","99de43ee":"Totalt salg af varer pr butik","e39614ec":"Tilf\u00f8jer nye kolonner til calendar_data, med datoer opdelt. ","702b6e5b":"Nedenst\u00e5ende laver predictions med en RandomForest model og gemmer resultatet i en variabel. Det tog i omegnen af 18 timer at eksekvere og er derfor udkommenteret. ","800fc6dd":"Metode til at lave predictions p\u00e5 test data.","a70e7736":"Vi v\u00e6lger at lave en sarimax model med en seasonality p\u00e5 7, da vi ud fra vores tidligere anylyse har kunnet se en seasonality p\u00e5 7 dage.\n","708b1edf":"Pivot er en omdannelse af vores dataframe snap_days s\u00e5 vores \u00e5r bliver til kolonner, og index'et bliver til m\u00e5neden. ","995ac0a0":"Metode til at foretage predictions ud fra det originale data","2c749dd8":"Vi vil starte med at fors\u00f8ge at predicte p\u00e5 dataen med en ARIMA model","d4c97fc1":"Det er tydeligt at der er seasonality i datasettet, samt at salget dropper til 0 omkring nyt\u00e5r og juleaften, m\u00e5ske fordi Wallmart har lukket de dage.","24179b4a":"## Sell Price Data","95144fc3":"Totalt salg pr stat, i antal af varer.","389cabaf":"Metode til at encode de enkelte features til den rette datatype.","ef03069b":"Salg af varer efter dept_id","0bf39677":"## Analyse af time series","785c0e39":"Looper igennem alle produkter og foretager predictions med en sarimax '1 1 1 7' model for de n\u00e6ste 28 dage og gemmer dem i en liste kaldet results.","4ecf2fc7":"# Random Forest Model","eae8f8ca":"Vi kan se at p\u00e5 2 ud af de 3 fittede modeler er det ikke s\u00e6rligt godt.\n","efd0e068":"Metode til tr\u00e6ning af RandomForestRegressor model med 350 tr\u00e6er. ","08ef6895":"## Calendar Data","eb257125":"Totalt salg fordelt i kategorier","7e5116c1":"# ARIMA Model","857eb0fa":"## Sales Train Validation","91a2e1f2":"Der kan ses en stiging i salget af hobbyartikler omkring starten af 2013, da det var \u00e5ret hvor den amerikanske \u00f8konomi begyndte at komme sig efter finanskrisen i 2009.","5819646c":"Egentlig forecast til aflevering:","b109758d":"Det tog 2 timer at k\u00f8re ovenst\u00e5ende (udkommenteret) vi har derfor gemt resultatet i en CSV til senere brug.","93c1f4c6":"Finder total m\u00e6ngde af varer solgt pr vare","1c3ffadc":"Sammenligning af predictions mod de oprindelige v\u00e6rdier","b7a71a97":"Samler kolonner efter m\u00e5nedens dage i et 'columnsets' data set.","456ff5db":"Fordi vi kan se det er et seasonal timeseries, en timeseries best\u00e5r af 3 systematiske komponenter, Trend, level, Seasonality og en ikke-systematisk komponent residuals, benytter vi seasonal_decompose for at unders\u00f8ge disse.\n\nSystematiske komponenter er dele af timeseries' der har konsistens eller optr\u00e6der gentagne gange og kan beskrives og modelleres.\n\nIkke-systematiske komponenter er dele af timeseries der ikke kan modeleres direkte.\n\n* Y[t] = T[t] + S[t] + e[t]\n\nTrend er den voksende eller faldende v\u00e6rdi i datas\u00e6ttet.\n\nLevel er den gennemsnittelige v\u00e6rdi i datas\u00e6ttet.\n\nSeasonality er gentagende short-term cycluser i datas\u00e6ttet.\n\nResiduals er st\u00f8jen der er i datas\u00e6ttet\n\nVi benytter additiv fordi det ligner at datas\u00e6ttet er line\u00e6rt og ikke expotientielt.\n\n","f4a83064":"Metode til at hente data fra en pandas Dataframe ud fra start og slut index","771b14e7":"## Yearly overall sales","ff90345b":"## Snap Days","78db0fec":"# Loading datasets","fb1bdb40":"Vi kan se at der er en for\u00f8gning i salget omkring weekender, hvorfor man kan argumentere for at der er en seasonality p\u00e5 7 dage, hvilket vi kan anvende i vores ARIMA model.","2d0c3f19":"Sepererer Data om hvor meget der er solgt en dag i en liste, v\u00e6k fra de andre kolonner i sales_train_validation, ved at sortere p\u00e5 string segmentet 'd_'","e4f2b575":"# Exploratory Data Analysis"}}