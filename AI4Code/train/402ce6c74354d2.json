{"cell_type":{"9a0eb835":"code","59363443":"code","8ee0cb30":"code","b9d9a39c":"code","f82ba61a":"code","005b4843":"code","3d5ace89":"markdown","2e142e84":"markdown","bf029ee5":"markdown"},"source":{"9a0eb835":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers","59363443":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(train.shape, test.shape)","8ee0cb30":"from sklearn.preprocessing import scale\n\ny = train['target']\ntrain = train.drop(['target', 'ID_code'], axis=1)\nid_test = test['ID_code']\ntest = test.drop(['ID_code'], axis=1)\n\n# Scaling the data:\ntrain = scale(train)\ntest = scale(test)\n\nx_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.25,\n                                                    random_state=42)","b9d9a39c":"from keras.models import Sequential\nfrom keras import layers\n\ninput_dim = train.shape[1]\nprint('Input dimension =', input_dim)\n\nmodel = Sequential()\nmodel.add(layers.Dense(16, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\nmodel.add(layers.Dense(16, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l2(0.005)))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nmodel.summary()","f82ba61a":"history = model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))","005b4843":"# save our prediction\nprediction = model.predict(test)\npd.DataFrame({\"ID_code\":id_test,\"target\":prediction[:,0]}).to_csv('result_keras.csv',index=False,header=True)","3d5ace89":"We will have 3 layers in our model. We add $l^2$ regularization, and take relatively small layers.","2e142e84":"> # NN Approach\nWe will discuss how to use a NN approach without overfitting too much. There are several best practices one should keep in mind to avoid overfitting when using this approach:\n* Having as much as data as possible. Here, we will have 200000 data points in the train set.\n* Having as few parameters as possible in the model. It is easy to build a model with millions of parameters, which often leads to overfitting.\n* Using regularization. Basically, this means adding a penalization to having large weights in the layers. This is a similiar concept to lasso\/ridge regression. We add the $l^2$, or $l^1$ norm of the weights to our loss function. This will prevent the network from having complex, \"large\" weights which will be an overfit.\n* Using dropout. In this approach, we randomly set a proportion of the weights of the network to $0$.\n\nThe book \"Deep Learning with Python\" by Fran\u00e7ois Chollet gives a wonderful introduction to these concept.","bf029ee5":"We first split the data into a train\/test set and scale it (taking z-scores which is easily done by using sklearn's scale function)."}}