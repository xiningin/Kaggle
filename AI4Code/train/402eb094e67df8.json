{"cell_type":{"5131983f":"code","974b8cef":"code","a000781d":"code","0a226d63":"code","5f3ad30e":"code","4c99075f":"code","b813744b":"code","618b178b":"code","4e85b374":"code","0c37b81a":"code","0c5616ff":"code","877b5927":"code","6eb186d6":"code","3caac8b9":"code","810b4ce8":"code","84cf84b9":"code","c13b8b1e":"code","8660e988":"code","bc99b7a5":"code","b2c93144":"code","f538103e":"code","c527d0eb":"code","8c2acc50":"code","a62b6233":"code","f75514d1":"code","402fd1c2":"code","a0ec395e":"code","0dd50462":"code","7b122d47":"code","97f9db0d":"code","d009b442":"code","a82d5d88":"code","d5426846":"code","04e120c6":"code","e9b4825f":"code","2d02a235":"code","4327abc2":"code","76e9bf67":"code","d960eea9":"code","14b58156":"code","2baeb049":"code","93740ea7":"code","f89d5cb2":"markdown","3c2a9240":"markdown","47d17c65":"markdown","fe7b6c24":"markdown","6684dcda":"markdown","f30ebe4d":"markdown","9aa0dc99":"markdown","bb123e64":"markdown","58aaae42":"markdown","62f7fee4":"markdown","6baf1179":"markdown","148d0b26":"markdown","3b57f93c":"markdown","6f470f75":"markdown","11336e69":"markdown","fbdaecc0":"markdown","84ed3f3c":"markdown","10fef216":"markdown"},"source":{"5131983f":"# GPU memory wizardry to avoid out of memory when using tensorflow-GPU as Keras backend\n# remove this part of using CPU only version of tensorflow. \nimport tensorflow as tf\nfrom keras import backend as k\nconfig = tf.ConfigProto()                                   # Set GPU options for tensorflow GPU \nconfig.gpu_options.allow_growth = True                      # Don't pre-allocate memory; allocate as-needed\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.8    # Only allow a total of half the GPU memory to be allocated\nk.tensorflow_backend.set_session(tf.Session(config=config)) # Create a session with the above options specified.","974b8cef":"# If librosa report \"no backend error\", install audio codec\n!apt-get -y install libav-tools\n# or ffmpeg\n#!apt-get -y install software-properties-common\n#!add-apt-repository ppa:mc3man\/trusty-media  \n#!apt-get -y install ffmpeg  \n#!apt-get -y install frei0r-plugins  \n#!ffmpeg -version","a000781d":"import keras\nfrom keras.layers import Activation, Dense, Dropout, Conv2D, \\\n                         Flatten, MaxPooling2D\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,TensorBoard,ProgbarLogger\nfrom sklearn.model_selection import train_test_split\nimport librosa\nimport librosa.display\nimport numpy as np\nimport pandas as pd\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#object serialization\nimport _pickle as cPickle  #python 3 change\nimport os  \n\n%matplotlib inline","0a226d63":"#enable memory profiler for memory management usage %%memit \nfrom memory_profiler import memory_usage\n%load_ext memory_profiler\n\n#enable garbage collection control\nimport gc\ngc.enable()","5f3ad30e":"#progress tracker\nfrom tqdm import tqdm, tqdm_notebook","4c99075f":"# when set to TRUE, training data get loaded from a saved serialized data object file \n# All audio files data get saved to a serialized object file to save reloading time on training runs \n#\n# Note: \n# On first time run, if serialized file doesn't exist, this flag will get overrident \n#\nSKIP_AUDIO_RELOAD = False","b813744b":"#location of the sound files\nINPUT_PATH='..\/input'\n\nTRAIN_INPUT=INPUT_PATH+'\/train'\nTRAIN_AUDIO_DIR=TRAIN_INPUT+'\/Train'\n\nTEST_INPUT=INPUT_PATH+'\/test'\nTEST_AUDIO_DIR=TEST_INPUT+'\/Test'","618b178b":"def load_input_data(pd, filepath):\n    # Read Data\n    data = pd.read_csv(filepath)\n    return data","4e85b374":"# training file\nTRAIN_FILE=TRAIN_INPUT+'\/train.csv'\n\n#show info\ntrain_input=load_input_data(pd,TRAIN_FILE)\ntrain_input.head()","0c37b81a":"# training file\nTEST_FILE=TEST_INPUT+'\/test.csv'\n\n#show info\ntest_input=load_input_data(pd,TEST_FILE)\ntest_input.head()","0c5616ff":"#labels\nvalid_train_label = train_input[['Class']]\n#x=data['label'].unique()\nvalid_train_label.count()\n\n#unique classes\nx = train_input.groupby('Class')['Class'].count()\nx","877b5927":"# train data size\nvalid_train_data = train_input[['ID', 'Class']] \nvalid_train_data.count()","6eb186d6":"# test data size\nvalid_test_data = test_input[['ID']] \nvalid_test_data.count()","3caac8b9":"# sample-1 load\nsample1=TRAIN_AUDIO_DIR+'\/943.wav'\nduration=2.97 \nsr=22050\n\ny, sr = librosa.load(sample1, duration=duration,  sr=sr)\nps = librosa.feature.melspectrogram(y=y, sr=sr)\n\ninput_length=sr*duration\noffset = len(y) - round(input_length)\nprint (\"input:\", round(input_length), \" load:\", len(y) , \" offset:\", offset)\nprint (\"y shape:\", y.shape, \" melspec shape:\", ps.shape)","810b4ce8":"# sample-1 waveplot\nlibrosa.display.waveplot(y,sr)","84cf84b9":"# sample-1: audio\nimport IPython.display as ipd\nipd.Audio(sample1) ","c13b8b1e":"# sample-1: spectrogram\nlibrosa.display.specshow(ps, y_axis='mel', x_axis='time')","8660e988":"# sample-2 load\nsample2=TRAIN_AUDIO_DIR+'\/1.wav'\nduration=2.97 \nsr=22050\n\ny2, sr2 = librosa.load(sample2, duration=duration,  sr=sr)\nps2 = librosa.feature.melspectrogram(y=y2, sr=sr2)\n\ninput_length=sr*duration\noffset = len(y) - round(input_length)\nprint (\"input:\", round(input_length), \" load:\", len(y) , \" offset:\", offset)\nprint (\"y shape:\", y.shape, \" melspec shape:\", ps2.shape)","bc99b7a5":"# sample-2: audio\nipd.Audio(sample2) ","b2c93144":"# sample-2: spectrogram\nlibrosa.display.specshow(ps2, y_axis='mel', x_axis='time')\nps.shape","f538103e":"#training audio files\nvalid_train_data['path'] = TRAIN_AUDIO_DIR+'\/' + train_input['ID'].astype('str')+\".wav\"\nprint (\"sample\",valid_train_data.path[1])\nvalid_train_data.head(5)","c527d0eb":"#test audio files\nvalid_test_data['path'] = TEST_AUDIO_DIR+'\/' + test_input['ID'].astype('str') +\".wav\"\nprint (\"sample\",valid_test_data.path[1])\n\nvalid_test_data.head(5)","8c2acc50":"#\n# set duration on audio loading to make audio content to ensure each training data have same size\n# \n# for instance, 3 seconds audio will have 128*128 which will be use on this notebook\n#\ndef audio_norm(data):\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data+0.0001)\n    return data-0.5\n\n#fix the load audio file size\naudio_play_duration=2.97\n\ndef load_audio_file(file_path, duration=2.97, sr=22050):\n    #load 5 seconds audio file, default 22 KHz default sr=22050\n    # sr=resample to 16 KHz = 11025\n    # sr=resample to 11 KHz = 16000\n    # To preserve the native sampling rate of the file, use sr=None\n    input_length=sr*duration\n    # Load an audio file as a floating point time series.\n    # y : np.ndarray [shape=(n,) or (2, n)] - audio time series\n    # sr : number > 0 [scalar] - sampling rate of y\n    y, sr = librosa.load(file_path,sr=sr, duration=duration)\n    dur = librosa.get_duration(y=y)\n    #pad output if audio file less than duration\n    # Use edge-padding instead of zeros\n    #librosa.util.fix_length(y, 10, mode='edge')\n    if (round(dur) < duration):\n        offset = len(y) - round(input_length)\n        print (\"fixing audio length :\", file_path)\n        print (\"input:\", round(input_length), \" load:\", len(y) , \" offset:\", offset)\n        y = librosa.util.fix_length(y, round(input_length))      \n    # y = audio_norm(y)\n    # using a pre-computed power spectrogram\n    # Short-time Fourier transform (STFT)\n    #D = np.abs(librosa.stft(y))**2\n    #ps = librosa.feature.melspectrogram(S=D)    \n    ps = librosa.feature.melspectrogram(y=y, sr=sr)\n    return ps","a62b6233":"%%time\n%%memit \n# Dataset\ntrain_audio_data = [] \ntrain_object_file='saved_train_audio_data.p'\n\n#override the reload flag if serized file doesn't exist\nif not os.path.isfile(train_object_file):\n    SKIP_AUDIO_RELOAD = False\n\n#load training data\nif SKIP_AUDIO_RELOAD is True:\n    print (\"skip re-loading TRAINING data from audio files\")\nelse:\n    print (\"loading train audio data, may take more than 15 minutes. please wait!\")\n    for row in tqdm(valid_train_data.itertuples()):\n        ps = load_audio_file(file_path=row.path, duration=2.97)\n        if ps.shape != (128, 128): continue\n        train_audio_data.append( (ps, row.Class) ) \n    print(\"Number of train samples: \", len(train_audio_data))\n# this step took sometime to finish    5382\n#peak memory: 1141.30 MiB, increment: 642.16 MiB\n#CPU times: user 15min 41s, sys: 14min 57s, total: 30min 39s","f75514d1":"# load saved audio object\nif SKIP_AUDIO_RELOAD is True:\n    train_audio_data = cPickle.load(open(train_object_file, 'rb'))\n    print (\"loaded train data [%s] records from object file\" % len(train_audio_data))  \nelse:\n    cPickle.dump(train_audio_data, open(train_object_file, 'wb')) \n    print (\"saved loaded train data :\",len(train_audio_data))","402fd1c2":"%%time\n%%memit \n#load test data\ntest_audio_data = []\ntest_object_file='saved_test_audio_data.p'\n\n#override the reload flag if serized file doesn't exist\nif not os.path.isfile(test_object_file):\n    SKIP_AUDIO_RELOAD = False\n\nif SKIP_AUDIO_RELOAD is True:\n    print (\"skip re-loading TEST data from audio files\")\nelse:\n    print (\"loading test audio data, may take more than 15 minutes. please wait!\")\n    for row in tqdm(valid_test_data.itertuples()):\n        ps = load_audio_file(file_path=row.path, duration=2.97)\n        if ps.shape != (128, 128):\n            print (\"***data shape is wrong, replace it with zeros \", ps.shape, row.path)\n            ps = np.zeros([128, 128])\n            #continue\n        test_audio_data.append( (ps, row.ID) ) \n    print(\"Number of train samples: \", len(train_audio_data))\n    \n# this step took sometime to finish    3251\n#peak memory: 1586.96 MiB, increment: 445.65 MiB\n#CPU times: user 9min 32s, sys: 9min 37s, total: 19min 10s ","a0ec395e":"# load saved data\nif SKIP_AUDIO_RELOAD is True:\n    test_audio_data = cPickle.load(open(test_object_file, 'rb'))\n    print (\"loaded test data [%s] records from object file\" % len(test_audio_data))      \nelse:\n    cPickle.dump(test_audio_data, open(test_object_file, 'wb')) \n    print (\"save loaded test data :\", len(test_audio_data))","0dd50462":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom numpy import argmax\n\n# get a set of unique text labels\nlist_labels = sorted(list(set(valid_train_data.Class.values)))\nprint (\"unique text labels count: \",len(list_labels))\nprint (\"labels: \",list_labels)\n\n# integer encode\nlabel_encoder = LabelEncoder()\nlabel_integer_encoded = label_encoder.fit_transform(list_labels)\nprint(\"encoded labelint values\", label_integer_encoded)\n\n# one hot encode\nencoded_test = to_categorical(label_integer_encoded)\ninverted_test = argmax(encoded_test[0])\n#print(encoded_test, inverted_test)\n\n#map filename to label\nfile_to_label = {k:v for k,v in zip(valid_train_data.path.values, valid_train_data.ID.values)}\n\n# Map integer value to text labels\nlabel_to_int = {k:v for v,k in enumerate(list_labels)}\n#print (\"test label to int \",label_to_int[\"Applause\"])\n\n# map integer to text labels\nint_to_label = {v:k for k,v in label_to_int.items()}\n","7b122d47":"#full dataset\ndataset = train_audio_data\nrandom.shuffle(dataset)\n\nRATIO=0.9\ntrain_cutoff= round(len(dataset) * RATIO)\ntrain = dataset[:train_cutoff]\ntest = dataset[train_cutoff:]\n\nX_train, y_train = zip(*train)\nX_test, y_test = zip(*test)\n\n# Reshape for CNN input\nX_train = np.array([x.reshape( (128, 128, 1) ) for x in X_train])\nX_test = np.array([x.reshape( (128, 128, 1) ) for x in X_test])\n\nprint (\"train \",X_train.shape, len(y_train))\nprint (\"test \", X_test.shape, len(y_test))","97f9db0d":"# Apply sck-learn label text encoding to integer\nlabel_encoder = LabelEncoder()\ny_train_integer_encoded = label_encoder.fit_transform(y_train)\ny_test_integer_encoded = label_encoder.fit_transform(y_test)","d009b442":"# Apply Keras One-Hot encoding for classes\ny_train = np.array(keras.utils.to_categorical(y_train_integer_encoded, len(list_labels)))\ny_test = np.array(keras.utils.to_categorical(y_test_integer_encoded, len(list_labels)))","a82d5d88":"#split up test into test and validation \nX_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.30, random_state=42)\n\nprint (\"test \",X_test.shape, len(y_test))\nprint (\"valid \", X_val.shape, len(y_val))","d5426846":"# build convolution model\n# input shape = (128, 128, 1)\nmodel = Sequential()\ninput_shape= X_train.shape[1:] \n\nmodel.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\nmodel.add(MaxPooling2D((4, 2), strides=(4, 2)))\nmodel.add(Activation('relu'))\n\nmodel.add(Conv2D(48, (5, 5), padding=\"valid\"))\nmodel.add(MaxPooling2D((4, 2), strides=(4, 2)))\nmodel.add(Activation('relu'))\n\nmodel.add(Conv2D(48, (5, 5), padding=\"valid\"))\nmodel.add(Activation('relu'))\n\nmodel.add(Flatten())\nmodel.add(Dropout(rate=0.5))\n\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(rate=0.5))\n\nmodel.add(Dense(len(list_labels)))\nmodel.add(Activation('softmax'))\nmodel.summary()","04e120c6":"%%time\n%%memit\n\n# NOTE:\n# Increase number if epochs from  1 to 60 or 100 for higher prediction accuracy\n# default is set to 1 for faster commit \nMAX_EPOCHS=3\nMAX_BATCH_SIZE=23            \n# learning rate reduction rate \nMAX_PATIENT=2  \n\n# saved model checkpoint file\nbest_model_file=\".\/best_model_trained.hdf5\"\n\n# callbacks\n# removed EarlyStopping(patience=MAX_PATIENT)\ncallback=[ReduceLROnPlateau(patience=MAX_PATIENT, verbose=1),\n          ModelCheckpoint(filepath=best_model_file, monitor='loss', verbose=1, save_best_only=True)]\n\n#compile\nmodel.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n\n#train\nprint('training started.... please wait!')\nhistory = model.fit(x=X_train, y=y_train,\n                    epochs=MAX_EPOCHS,\n                    batch_size=MAX_BATCH_SIZE, \n                    verbose=0,\n                    validation_data= (X_val, y_val), \n                    callbacks=callback)\nprint('training finished')\n\n# quick evaludate model\nprint('Evaluate model with test data')\nscore = model.evaluate(x=X_test,y=y_test)\n\nprint('test loss:', score[0])\nprint('test accuracy:', score[1])","e9b4825f":"%%time\n%%memit\n\nimport matplotlib.pyplot as plt\n#Plot loss and accuracy for the training and validation set.\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    plt.figure(figsize=(22,10))\n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    ## Accuracy\n    plt.figure(221, figsize=(20,10))\n    ## Accuracy\n    # plt.figure(2,figsize=(14,5))\n    plt.subplot(221, title='Accuracy')\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(222, title='Loss')\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n# plot history\nplot_history(history)","2d02a235":"#Evaludate model use Keras reported accuracy:\nscore = model.evaluate(X_train, y_train, verbose=0) \nprint (\"model train data score       : \",round(score[1]*100) , \"%\")\n\nscore = model.evaluate(X_test, y_test, verbose=0) \nprint (\"model test data score        : \",round(score[1]*100) , \"%\")\n\nscore = model.evaluate(X_val, y_val, verbose=0) \nprint (\"model validation data score  : \", round(score[1]*100), \"%\")","4327abc2":"print (\"Prediction with [train] data\")\ny_pred = model.predict_classes(X_train)\nmissed=[]\nmatched=[]\nfor i in range(len(y_pred)):\n    y_val_label_int = argmax(y_train[i])\n    if (y_pred[i]!=y_val_label_int):\n        missed.append( (y_pred[i], \"-\", int_to_label[y_pred[i]], \" - \", int_to_label[y_val_label_int] ))\n    else:\n        matched.append((y_pred[i], \"-\", int_to_label[y_pred[i]], \" - \", int_to_label[y_val_label_int]))\n\nprint (\"  |__match    :\", len(matched))\nprint (\"  |__miss     :\", len(missed))\nprint (\"  |__accuracy :\", round((len(matched)-len(missed))\/len(matched)*100,2), \"%\")\nprint (\"\")\n#print (\"Value missed : \\n\",missed)\n\n# show sample results\nprint (\"---samples---\")\nfor i in range(5):\n    print (i,\"predict =\", int_to_label[y_pred[i]])\n    print (i,\"original=\", int_to_label[argmax(y_train[i])])\n    print (\"\")","76e9bf67":"# prediction class \nprint (\"Prediction with [test] data\")\ny_pred = model.predict_classes(X_test)\nmissed=[]\nmatched=[]\nfor i in range(len(y_pred)):\n    y_val_label_int = argmax(y_test[i])\n    if (y_pred[i]!=y_val_label_int):\n        missed.append( (y_pred[i], \"-\", int_to_label[y_pred[i]], \" - \", int_to_label[y_val_label_int] ))\n    else:\n        matched.append((y_pred[i], \"-\", int_to_label[y_pred[i]], \" - \", int_to_label[y_val_label_int]))\n\nprint (\"  |__match    :\", len(matched))\nprint (\"  |__miss     :\", len(missed))\nprint (\"  |__accuracy :\", round((len(matched)-len(missed))\/len(matched)*100,2), \"%\")\nprint (\"\")\n#print (\"Value missed : \\n\",missed)\n\n# show sample results\nprint (\"---samples---\")\nfor i in range(8):\n    print (i,\"predict =\", int_to_label[y_pred[i]])\n    print (i,\"original=\", int_to_label[argmax(y_test[i])])\n    print (\"\")","d960eea9":"# prediction class \nprint (\"Prediction with [validation] data\")\ny_pred = model.predict_classes(X_val)\nmissed=[]\nmatched=[]\nfor i in range(len(y_pred)):\n    y_val_label_int = argmax(y_val[i])\n    if (y_pred[i]!=y_val_label_int):\n        missed.append( (y_pred[i], \"-\", int_to_label[y_pred[i]], \" - \", int_to_label[y_val_label_int] ))\n    else:\n        matched.append((y_pred[i], \"-\", int_to_label[y_pred[i]], \" - \", int_to_label[y_val_label_int]))\n\nprint (\"  |__match    :\", len(matched))\nprint (\"  |__miss     :\", len(missed))\nprint (\"  |__accuracy :\", round((len(matched)-len(missed))\/len(matched)*100,2), \"%\")\nprint (\"\")\n#print (\"Value missed : \\n\",missed)\n\n# show sample results\nprint (\"---samples---\")\nfor i in range(8):\n    print (i,\"predict =\", int_to_label[y_pred[i]])\n    print (i,\"original=\", int_to_label[argmax(y_val[i])])\n    print (\"\")","14b58156":"print (\"test data size \",len(test_audio_data))\nsub_test = test_audio_data[1:22]\ntx_test, ty_test = zip(*test_audio_data)\n\n# make prediction \ntx_test2 = np.array([x.reshape((128, 128, 1)) for x in tx_test])\nprint (\"test data shape \", tx_test2.shape)","2baeb049":"# run prediction data\ny_pred = model.predict_classes(tx_test2, batch_size=1)\nprint ( len(y_pred), len(tx_test2))","93740ea7":"# save result for submission\nprediction_output_file='prediction_result_1.csv'\nwith open(prediction_output_file,\"w\") as file:\n    file.write(\"ID,Prediction\\n\") \n    i=0\n    for i in range( (len(valid_test_data)-1)) :\n        #print(i, y_pred[i])\n        file.write(str(valid_test_data['ID'][i])+','+ int_to_label[y_pred[i]])\n        file.write('\\n')\n        i=i+1\n        \nprint (len(y_pred))\noutput = pd.read_csv(prediction_output_file)\noutput.head(20)","f89d5cb2":"**Loading audio file and features**","3c2a9240":"**Pre-requisite**\n- librosa audio codec which required internet connected turn on under settings","47d17c65":"**Check input audio file samples**","fe7b6c24":"Audio file loading control flag","6684dcda":"Optional GPU configuration initialization","f30ebe4d":"With data augmentation, model prediction accurancy has increased. however due performance issue when commit notebook in Kaggle. I decied to temporary removed it for now. I will added back in next version.","9aa0dc99":"####  Dataset exploration","bb123e64":"#### Prediction test ","58aaae42":"That is it for now, drop any comment or suggestion.  Vote up if you find it useful. \n\nthanks  - Min yang","62f7fee4":"#### split up data into train,  test and validation","6baf1179":"**Prepare data for training**","148d0b26":"## Automatic Urban Sound Classification with CNN","3b57f93c":"**Background**\n\nThe automatic classification of environmental sound is a growing research field with multiple applications to largescale, content-based multimedia indexing and retrieval. In particular, the sonic analysis of urban environments is the subject of increased interest, partly enabled by multimedia sensor networks, as well as by large quantities of online multimedia content depicting urban scenes.\n\n**Challenges**\n\nThere are primarily two major challenges with urban sound research namely\n\nLack of labeled audio data. Previous work has focused on audio from carefully produced movies or television tracks from specific environments such as elevators or office spaces and on commercial or proprietary datasets .\n\nLack of common vocabulary when working on urban sounds.This means the classification of sounds into semantic groups may vary from study to study, making it hard to compare results so the objective of this notebook is to address the above two mentioned challenges.\n\n**Dataset**\n\nThe dataset is called UrbanSound and contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: - The dataset contains 8732 sound excerpts (<=4s) of urban sounds from 10 classes, namely: Air Conditioner Car Horn Children Playing Dog bark Drilling Engine Idling Gun Shot Jackhammer Siren Street Music The attributes of data are as follows: ID \u2013 Unique ID of sound excerpt Class \u2013 type of sound\n\n\n**Objective**\n\nThe objective of this notebook is train a CNN model that will automate classification Urban sounds. \n\n\n**Note:** Loading audio files and pre-processsing takes some times to complete with large dataset. To avoid reload everytime reset the kernel or resume works on next day, all loaded audio data will be serialized into a object file. so next round only need to load the seriazed object file.   \n","6f470f75":"#### Prepcare  Submission","11336e69":"**Encode labels**","fbdaecc0":"Import libraries","84ed3f3c":"**Prepare data file loading**","10fef216":"**Model Evaluation**"}}