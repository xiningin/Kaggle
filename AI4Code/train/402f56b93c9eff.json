{"cell_type":{"239ef919":"code","4a540e31":"code","9b440b1e":"code","2a42e314":"code","c75e31ef":"code","96514a0e":"code","6ac7157b":"code","86b7637b":"code","79dce165":"code","a21231ef":"code","c083ee10":"code","fcfa575d":"code","41c3bc00":"code","d4fb13f3":"code","4ed0bf2c":"code","3487056b":"code","d6e3d2d3":"code","0d9a562d":"code","2b807b0c":"code","463e532f":"code","5f78e60b":"code","f16acac2":"code","cddfc5c3":"code","0a7014aa":"code","1048b8eb":"code","109113d6":"code","4a6183bb":"code","ca8b7676":"code","b28dff93":"code","e9f6c73d":"code","3ff97651":"code","8d39568d":"code","d48226a3":"code","0b4645ef":"code","3f71fe69":"code","87b65b98":"code","b68ec1d0":"code","b5e4aa3d":"code","f7b53e9c":"code","ac5d15e3":"code","a4c46956":"code","07665277":"code","8556e5b1":"code","b5018796":"code","94152b99":"code","70c60efa":"code","f55d4b37":"code","d0e957e6":"code","998e0aac":"code","dd03e555":"code","8047ed4e":"code","fa50ab07":"code","ea025369":"code","ef8706ed":"code","41f27f8c":"code","ba352f9f":"code","b799e262":"code","fd8b4144":"code","19d98ebe":"code","633af01c":"code","01608169":"code","d7890c30":"code","7cae5013":"code","a471f139":"code","34bbc96c":"code","482eeb38":"code","50126d2f":"code","141053b7":"code","9a9db6f5":"code","f37586ae":"code","62ebaa8c":"code","4f99ffe3":"markdown","dd2ecc0c":"markdown","bb8eb55f":"markdown","ddc4dfa8":"markdown","1afa0a2f":"markdown","25eff1ff":"markdown","0ed09352":"markdown","12ff0c31":"markdown","25432a59":"markdown","0ab849a0":"markdown","6f82f621":"markdown","3aa1885e":"markdown","405bd56d":"markdown","daa76a6e":"markdown","c9d9d84d":"markdown","f58b2be8":"markdown","adf6156f":"markdown","b410fe9e":"markdown","f1be5d0f":"markdown","a252c282":"markdown","e9552e41":"markdown","3f30b7ba":"markdown","f2ff914b":"markdown","5efa972a":"markdown"},"source":{"239ef919":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4a540e31":"# read in the dataset and take a quick pick at it\ndf = pd.read_csv(\"\/kaggle\/input\/us-accidents-may19\/US_Accidents_May19.csv\")\ndf.sample(10)","9b440b1e":"print('The DataFrame has {} rows and {} columns'.format(df.shape[0],df.shape[1]))\nprint('\\n')\nmissing = df.isnull().sum().sort_values(ascending=False)\npercent_missing = ((missing\/df.isnull().count())*100).sort_values(ascending=False)\nmissing_df = pd.concat([missing,percent_missing], axis=1, keys=['Total', 'Percent'],sort=False)\nmissing_df[missing_df['Total']>=1]","2a42e314":"lst = ['Humidity(%)','Precipitation(in)','Wind_Chill(F)','Wind_Speed(mph)','Visibility(mi)']\nfor l in lst:\n    df[l] = df[l].fillna(0)","c75e31ef":"lst = ['Temperature(F)','Pressure(in)']\nfor l in lst:\n    df[l]=df[l].fillna(df[l].mean())","96514a0e":"'''\nThis is a good time to take a look at our missing values again. I have added a third column showing the respective data types\n'''\nmissing = df.isnull().sum().sort_values(ascending=False)\npercent_missing = ((missing\/df.isnull().count())*100).sort_values(ascending=False)\nmissing_df = pd.concat([missing,percent_missing,df[missing.index].dtypes], axis=1, keys=['Total', 'Percent','Data Types'],sort=False)\nmissing_df[missing_df['Total']>=1]","6ac7157b":"missing_copy = missing_df[missing_df['Total']>=1].copy()","86b7637b":"object_columns = missing_copy[missing_copy['Data Types']=='object'].index\ndf[object_columns].head()","79dce165":"df['City'] = df.groupby('State')['City'].transform(lambda grp: grp.fillna(grp.value_counts().index[0]))","a21231ef":"df['Start_Time'] = pd.to_datetime(df['Start_Time']) # convert Start_Time to datetime\ndf['End_Time'] = pd.to_datetime(df['End_Time']) # convert End_Time to datetime\ndf['Weather_Timestamp'] = pd.to_datetime(df['Weather_Timestamp']) # convert Weather_Timestamp to datetime","c083ee10":"# fill the Nautical_Twilight column with Day\/Night by inferring the Start_Time column\n\ndef filler(df,columns):\n    # get list comprising column missing data\n    lst = df[df[columns].isna()].index\n    for i in lst:\n        if 6<= df.loc[i,'Start_Time'].hour and df.loc[i,'Start_Time'].hour <18:\n            df[columns] = df[columns].fillna('Day')\n        else:\n            df[columns] = df[columns].fillna('Night')\n\nfiller(df,'Nautical_Twilight')","fcfa575d":"# Another easier option is to just impute the Day\/Night values wth the mode as ['Sunrise_Sunset','Civil_Twilight','Astronomical_Twilight'] \n# vary depending on time of year and might be difficult to infer based on hour of day.\n\ndef median_imputer(x):\n    df[x].fillna(df[x].mode()[0],inplace=True)\n\nmedian_impute = ['Sunrise_Sunset','Civil_Twilight','Astronomical_Twilight','Wind_Direction','Weather_Condition']\nfor col in median_impute:\n    median_imputer(col)","41c3bc00":"# impute the timezone based on the State column\n\ndf['Timezone'] = df.groupby('State')['Timezone'].transform(lambda tz: tz.fillna(tz.value_counts().index[0]))","d4fb13f3":"# impute the Weather_Timestamp with the value at Start_Time. This column records the time the weather was taken (we won't really need it)\n\ndf.loc[(pd.isnull(df.Weather_Timestamp)), 'Weather_Timestamp'] = df.Start_Time","4ed0bf2c":"'''\nThis is a good time to take a look at our missing values again.\n'''\nmissing = df.isnull().sum().sort_values(ascending=False)\npercent_missing = ((missing\/df.isnull().count())*100).sort_values(ascending=False)\nmissing_df = pd.concat([missing,percent_missing,df[missing.index].dtypes], axis=1, keys=['Total', 'Percent','Data Types'],sort=False)\nmissing_df[missing_df['Total']>=1]","3487056b":"# we do for Zipcode and Airport_Code what we did for columns like Timezone\ndf['Zipcode'] = df.groupby('State')['Zipcode'].transform(lambda zc: zc.fillna(zc.value_counts().index[0]))\ndf['Airport_Code'] = df.groupby('State')['Airport_Code'].transform(lambda ac: ac.fillna(ac.value_counts().index[0]))","d6e3d2d3":"# we will fill the one record in Description with 'Accident'\n\ndf.Description = df.Description.fillna('Accident')","0d9a562d":"df.drop(labels=['End_Lat', 'End_Lng'],axis=1,inplace=True)","2b807b0c":"df['Number'] = df.groupby('State')['Number'].transform(lambda n: n.fillna(n.value_counts().index[0]))\ndf.TMC = df.TMC.fillna(201.0)","463e532f":"'''\nThis is a good time to take a look at our missing values again.\n'''\nmissing = df.isnull().sum().sort_values(ascending=False)\npercent_missing = ((missing\/df.isnull().count())*100).sort_values(ascending=False)\nmissing_df = pd.concat([missing,percent_missing,df[missing.index].dtypes], axis=1, keys=['Total', 'Percent','Data Types'],sort=False)\nmissing_df[missing_df['Total']>=1]","5f78e60b":"df.sample(10)","f16acac2":"# write and store the cleaned file to a pickle file\ndf.to_pickle('US_Accidents_Cleaned.pkl')","cddfc5c3":"# import libraries for Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')","0a7014aa":"df = pd.read_pickle('US_Accidents_Cleaned.pkl')","1048b8eb":"# create new features for timeseries analysis.\ndf['Hour'] = df['Start_Time'].dt.hour\ndf['Day'] = df['Start_Time'].dt.day\ndf['Day_Name'] = df['Start_Time'].dt.day_name()\ndf['Week'] = df['Start_Time'].dt.week\ndf['Month'] = df['Start_Time'].dt.month\ndf['Count'] = 1","109113d6":"df.groupby('Month')['Count'].value_counts()","4a6183bb":"import calendar\ndf.groupby('Month')['Count'].value_counts().plot(kind='bar')\ndf.groupby('Month')['Count'].value_counts().plot(color='k',linestyle='-',marker='.',linewidth=0.4)\nplt.xticks(np.arange(12),calendar.month_name[1:13],rotation=45)\nplt.xlabel('Month')\nplt.title('Monthly Accident Count')","ca8b7676":"plt.figure(figsize=(10,6))\ndf.groupby('Week')['Count'].value_counts().plot(linewidth=1,marker='.')\nplt.xticks(np.arange(52),np.arange(1,53),rotation = 90)\nplt.xlabel('Week of Year')\nplt.title('Accident Count by Week of Year')\nplt.show()","b28dff93":"plt.figure(figsize=(10,6))\ndf.groupby('State')['Count'].value_counts().plot(kind='bar')\nplt.xticks(np.arange(50),sorted(df['State'].unique()),rotation = 90)\nplt.xlabel('State')\nplt.title('Accident Count by State')\nplt.show()","e9f6c73d":"by_severity = df.groupby('Severity')['Count'].sum()","3ff97651":"sns.countplot(x='Severity',data=df)","8d39568d":"# Bivariate visualization of categorical variables\n\n#create a frequency table of state against severity\ncat_var = pd.crosstab(columns=df['Severity'],\n    index=df['State'])\n\n#plot a stacked plot\ncat_var.plot(kind='bar',stacked=True,figsize=(16,8),color=['purple','orange','blue','red','green'])\nplt.title('Stacked plot of Accident Severity in respective State')\nplt.ylabel('Frequency')\nplt.show()","d48226a3":"plt.figure(figsize=(12,6))\nsns.boxplot(x='Severity',y='Wind_Speed(mph)',data=df,hue='Severity')\nplt.ylim(0,100)","0b4645ef":"# I used median here because there are so many outliers in the boxplot that i felt using mean would skew the data\n\ndf.groupby('Severity')['Wind_Speed(mph)'].median().plot(kind='bar')\nplt.ylabel('Wind_Speed(mph)')\nplt.title(\"Median 'Wind_Speed(mph)' by Severity\")\nplt.show()","3f71fe69":"plt.figure(figsize=(12,6))\nsns.boxplot(x='Severity',y='Wind_Chill(F)',data=df,hue='Severity')\nplt.legend(loc='best')\nplt.show()","87b65b98":"df.groupby('Severity')['Wind_Chill(F)'].mean().plot(kind='bar')\nplt.ylabel('Wind_Chill(F)')\nplt.title(\"Average 'Wind_Chill(F)' by Severity\")\nplt.show()","b68ec1d0":"def catplotter(col):\n    x = df.groupby([col, 'Severity'])['Count'].sum().reset_index()\n    sns.catplot(\"Severity\", \"Count\", col=col, data=x, kind=\"bar\")\n    plt.show()","b5e4aa3d":"catplotter('Roundabout')","f7b53e9c":"catplotter('Bump')","ac5d15e3":"catplotter('Amenity')","a4c46956":"catplotter('Crossing')","07665277":"catplotter('Give_Way')","8556e5b1":"catplotter('Junction')","b5018796":"catplotter('No_Exit')","94152b99":"catplotter('Railway')","70c60efa":"catplotter('Station')","f55d4b37":"catplotter('Stop')","d0e957e6":"catplotter('Traffic_Signal')","998e0aac":"catplotter('Turning_Loop')","dd03e555":"catplotter('Side')","8047ed4e":"catplotter('Sunrise_Sunset')","fa50ab07":"catplotter('Civil_Twilight')","ea025369":"catplotter('Nautical_Twilight')","ef8706ed":"catplotter('Astronomical_Twilight')","41f27f8c":"df.sample(10)","ba352f9f":"# Severity Impact by Temperature\nplt.figure(figsize = (16, 6))\nsns.violinplot(y=\"Temperature(F)\", x=\"Severity\", data=df,width=0.6,linewidth=0.5)\nplt.show()","b799e262":"# Severity Impact by Humidity \nplt.figure(figsize = (16, 6))\nsns.violinplot(y=\"Humidity(%)\", x=\"Severity\", data=df,width=0.6,linewidth=0.5)\nplt.show()","fd8b4144":"# Severity Impact by Precipitation(in) \nplt.figure(figsize = (16, 6))\nsns.violinplot(y='Precipitation(in)', x=\"Severity\", data=df,width=0.6,linewidth=0.5)\nplt.show()","19d98ebe":"# Severity Impact by Pressure(in)\nplt.figure(figsize = (16, 6))\nsns.violinplot(y='Pressure(in)', x=\"Severity\", data=df,width=0.6,linewidth=0.5)\nplt.show()","633af01c":"# Top 10 weather condition\nplt.figure(figsize = (15, 6))\ndf[df['Weather_Condition'] != 0]['Weather_Condition'].value_counts().iloc[:10].plot(\n    kind='bar',color=['b','k','g','r','c','violet','lime','y','m','purple'])\nplt.show()","01608169":"plt.figure(figsize=(12,6))\nsns.countplot(x='Hour',data=df)\ndf.groupby('Hour')['Count'].value_counts().plot(color='k',linestyle='-',marker='.',linewidth=0.6)\nplt.title('Count of Accidents by Hour')\nplt.xticks(np.arange(0,24),np.arange(0,24),rotation=90)\nplt.xlabel('Hour')\nplt.plot()","d7890c30":"x = pd.crosstab(index=df['Hour'],columns=df['Severity'])\nx.plot(kind='bar',stacked=True, color=['b','k','g','r','c'],figsize=(12,6))\nplt.show()","7cae5013":"from PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","a471f139":"severity_2 = df[df['Severity']==2]['Description']\nseverity_3 = df[df['Severity']==3]['Description']\nseverity_4 = df[df['Severity']==4]['Description']","34bbc96c":"desc_2 = severity_2.str.split(\"(\").str[0].value_counts().keys()\nwc_desc_2 = WordCloud(scale=5,max_words=100,colormap=\"rainbow\",background_color=\"white\").generate(\" \".join(desc_2))\n\ndesc_3 = severity_3.str.split(\"!\").str[0].value_counts().keys()\nwc_desc_3 = WordCloud(scale=5,max_words=100,colormap=\"rainbow\",background_color=\"white\").generate(\" \".join(desc_3))\n\ndesc_4 = severity_4.str.split(\"!\").str[0].value_counts().keys()\nwc_desc_4 = WordCloud(scale=5,max_words=100,colormap=\"rainbow\",background_color=\"white\").generate(\" \".join(desc_4))","482eeb38":"fig, axs = plt.subplots(1,3,sharey=True,figsize=(17,14))\n\naxs[0].imshow(wc_desc_2,interpolation=\"bilinear\")\naxs[1].imshow(wc_desc_3,interpolation=\"bilinear\")\naxs[2].imshow(wc_desc_4,interpolation=\"bilinear\")\n\naxs[0].axis(\"off\")\naxs[1].axis(\"off\")\naxs[2].axis(\"off\")\n\naxs[0].set_title('Severity 2 Accidents')\naxs[1].set_title('Severity 3 Accidents')\naxs[2].set_title('Severity 4 Accidents')\n\nplt.show()","50126d2f":"import folium","141053b7":"df.sample(3)","9a9db6f5":"w = df.groupby(['State'])['Count'].sum().reset_index()","f37586ae":"state_geo = '\/kaggle\/input\/usa-states\/usa-states.json'","62ebaa8c":"n = folium.Map(location=[39.381266, -97.922211],zoom_start=5)\nfolium.Choropleth(\n geo_data=state_geo,\n data=w,\n columns=['State', 'Count'],\n key_on='feature.id',\n fill_color='YlOrRd',\n fill_opacity=0.7,\n line_opacity=0.2,\n legend_name='Accidents'\n).add_to(n)\nn","4f99ffe3":"<b>Is Wind_Speed a factor that influences accident Severity?<\/b> <br\/>Honestly from the plot it's inconclusive. It seems the distribution amongst the respective severities is somewhat similar. There seems to be more accidents in Severity 2 and 3 that in Severity 4 actually","dd2ecc0c":"# Exploratory Data Analysis","bb8eb55f":"The two remaining columns, i will fill the Number (which records street number) the the common street accidents happen by State, and will just fill TMC with the 201 Code since all records represent accidents.\n<b> Another possible way to deal with the Number column would be to fill in NaN's with (0) considering that all accidents may not occur in a street. But, what\/how does that affect our model? <\/b>","ddc4dfa8":"Choropleth Map of the US showing what we've seen with Barplots earlier - US states according to accident count.\n- California has the highest count\n- Texas is second.\n\nTwo things I have learned from all this:\n- It seems that folium.CircleMarker runs into problems when trying to plot a lot of data points. It seems it's different for different people. On my Maching, I couldn't plot more than 40k data points, and that's when I was trying to plot California State data only.\n- I am a bit torn with the Choropleth map, i feel the State view is too high level but at the same time I don't see how I can plot a more granulated map as I feel that would be appropriate when analyzing at State level and not country level. Unless someone has any ideas.\n","1afa0a2f":"For one thing, this plot demonstrates the impact on road usage the accident severity has. Severity 2 and 3 accidents have somewhat of a similar impact on the road, blocked lanes or shoulder, while severity 4 accidents lead to the closure of the road entirely","25eff1ff":"Similar to Wind_Speed, the distributions of Wind_Chill are inconclusive. Except that most accidents happen when the Wind Chill is over 0\u02da","0ed09352":"May registers low accident count in a year. August has the highest accident count. Probably because it's summer and people travel a lot.\n<br> Why are there relatively low accidents in April, May, June, and July compared to other months in the year? <\/br>","12ff0c31":"Analysis of Boolean Columns","25432a59":"Most accidents happen on the right had side. Makes sense since the U.S drives on the right side of the road","0ab849a0":"If you have any siggestions or you like this, please let me know.. would really appreciate it!!","6f82f621":"### fill NaN's with mean\nI will fill the following columns with the average value in the column as filling them with zero doesn't make much sense.","3aa1885e":"This is a walkthrough through the US_Accidents datasets. It is an attempt of an end to end project.\nA data dictionary for the dataset can be found at: https:\/\/smoosavi.org\/datasets\/us_accidents","405bd56d":"### Missing Value treatment by dropping values\nI will drop the End_Lat and End_Lng columns. The record the lat and long where the accident ended, if the accident affected a huge aread of road. It'll be difficult to impute them. Although, one way of imputing would be to set their values the same as Start_lat and Start_lng, but that'll be the same as removing them since about 77% of data in the columns would be the same. Deleting\/Droping is the logical choice.","daa76a6e":"Most accidents happen during the day time. ","c9d9d84d":"CA, FL, NC, NY, TX are the States that register the highest count of accidents in the Country.\n<b\/> Why are these states so high when it comes to accidents? Could do an analysis of accident count of state v Size. I suspect the bigger the state the more cars -> accidents?","f58b2be8":"Week 22 registered the lowest accident. <br> Questions is why? Why is there such a relatively low accident count in May compared to other months? <\/br>","adf6156f":"### fill NaN's with zero (0).\nI will fill these columns with 0 because it is possible to have no recorded value for these. For example, it's possible to have zero rain if rain didn't fall that day. ","b410fe9e":"2. for the next missing value imputation. I want to impute the Day\/Night columns. To do that, I will reference the 'Start_Time' column to get the hour, and impute Day or Night based on the value. We need to convert the 'Start_Time' column to a datetime and while we're at it, we'll do the same for the 'End_Time' column.","f1be5d0f":"Our dataset has 49 columns and over 2.2 million rows. We also have a lot of missing data within our dataset. We will now determing the method with which we will treat our missing values using the data dictionary to understand the data that each column has.","a252c282":"### Plotting Long and Lat using Folium","e9552e41":"1. Filling the 'City' column. Since we have a 'State' column. I'll fill the city column with the most occuring city of the state it belongs to","3f30b7ba":"### Fill NaN's with the most occuring entry\nThe data in these columns is categorical in nature. So i will fill the missing values with the most occuring value for these columns.","f2ff914b":"barplot of ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit',\n       'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming',\n       'Traffic_Signal', 'Turning_Loop'] accidents by Severity","5efa972a":"1. Quick peek through the data shows we have all sorts of data types (string, datetime, float, boolian, and integers) but we also have some NaN values. \n2. Next I will look at the shape of the df and how many NaN values each column contains."}}