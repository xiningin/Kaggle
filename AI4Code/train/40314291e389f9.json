{"cell_type":{"0d3dffb4":"code","fc84c5db":"code","3c040adf":"code","ff499aac":"code","e43a76ee":"code","7352ff83":"code","56fd41bd":"code","7b7974e8":"code","506e234b":"code","f0c695eb":"code","06b72518":"code","64a6d685":"code","88fc8128":"code","dd49bfca":"code","1db78840":"code","621dc9a8":"code","aeb76b73":"code","0e6eedba":"code","a02459f6":"code","6ad8f43d":"code","1244bacb":"code","164a88a1":"code","9ed08b34":"markdown","612eaf28":"markdown","9688a763":"markdown","9cdc51a7":"markdown","cfac675b":"markdown","22dbd70e":"markdown","7e73b398":"markdown","9c9808fa":"markdown","f646bcf1":"markdown","885c79ac":"markdown","42dcd2fd":"markdown","9efae988":"markdown","caefe69a":"markdown","7ce3ce6b":"markdown","52dc2dd1":"markdown","6b30c369":"markdown","4503dab3":"markdown","cc900e3e":"markdown","cee076a7":"markdown","613ace68":"markdown","f545986e":"markdown","269d2e12":"markdown","c037120c":"markdown","1254bae0":"markdown","c0ae944f":"markdown","14549b1a":"markdown","92a305e7":"markdown","9cee994d":"markdown","57fc2dd7":"markdown","9a599824":"markdown","96950584":"markdown","91639dd5":"markdown","484aa29e":"markdown","90a1d514":"markdown","7be17d94":"markdown","49607075":"markdown","f96f4536":"markdown","90a38be7":"markdown","5c2bad4b":"markdown"},"source":{"0d3dffb4":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()","fc84c5db":"# Now loading in data with Pandas\ndf = pd.read_csv('..\/input\/Admission_Predict.csv')\n\nprint('dataframe shape: {}'.format(df.shape))\ndf.head(3)","3c040adf":"# Now getting a better look at what each column represents\ndf.info()","ff499aac":"# Dropping 'Serial No.'\ndf = df.drop(columns=['Serial No.'])","e43a76ee":"# Standardizing column names\ndf.columns = ['GRE_score', 'TOEFL_score', 'university_rating', 'statement_of_purpose', 'letter_of_recommendation', 'GPA', 'research', 'chance_of_admit']","7352ff83":"df.head(3)","56fd41bd":"df['chance_of_admit'].hist(figsize=(10,4))","7b7974e8":"df['chance_of_admit'].plot(kind='density', subplots=True, figsize=(10, 4));","506e234b":"# Creating a correlation matrix:\ncorr_matrix = df.corr()\n# Plotting heatmap\nsns.heatmap(corr_matrix);","f0c695eb":"sns.jointplot(x='chance_of_admit', y='GPA', data=df, kind='scatter');","06b72518":"sns.lmplot('chance_of_admit', 'GPA', data=df, hue='research', fit_reg=False);","64a6d685":"# taking care of our ML imports\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","88fc8128":"# Seperating our features and our target.\ntrain_features = list(set(df.columns) - set(['chance_of_admit']))\n\ntrain_X = df[train_features]\ntrain_y = df['chance_of_admit']","dd49bfca":"# Create LinearRegression instance\nlinear_regression = LinearRegression()\n\n# Begin training! This is also knows as 'fitting' the model to our data\nlinear_regression.fit(train_X, train_y)","1db78840":"pd.DataFrame(linear_regression.coef_, train_X.columns, columns=['Coefficient'])","621dc9a8":"# Loading in Test set\ntest_df = pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')\n\ntest_df.head(3)","aeb76b73":"test_df = test_df.drop(columns=['Serial No.'])\ntest_df.columns = ['GRE_score', 'TOEFL_score', 'university_rating', 'statement_of_purpose', 'letter_of_recommendation', 'GPA', 'research', 'chance_of_admit']","0e6eedba":"test_df.head(3)","a02459f6":"test_X = test_df[train_features]\ntest_y = test_df['chance_of_admit']","6ad8f43d":"y_pred = linear_regression.predict(test_X)","1244bacb":"pd.DataFrame({'Prediction': y_pred, 'Actual': test_y}).head(10)","164a88a1":"print(\"Mean Absolute Error: {}\".format(metrics.mean_absolute_error(test_y, y_pred)))","9ed08b34":"Awesome! Lets move on to some further exploration and visualization","612eaf28":"Again, lets take a look at our first three entries. We should now only see the _Serial No._ column gone and the columns renamed:","9688a763":"First things first, lets take a quick look at the distribution of acceptance probabilites across our dataset. For this, we will ceate a __histogram__","9cdc51a7":"### Quantitative Relationships w\/ Scatter Plots\n\nWe can spend all day looking deeper into certain data relationships, but we'll keep our current analysis only down to a few.\n\nThe strongest relationship we noted in the correlation matrix for *chance_of_admit* was with *GPA*. Lets start there.\n\nWe will use a _jointplot_, which will mainly display a _scatterplot_ for our correlation, but also contain a distribution of each feature above its respected axis:","cfac675b":"Some might scoff at picking to use such a simple model to predict acceptance rates.\n\nWhile linear regression is one of the simplest models available for implementation, it is also quite effective and has a fast iteration cycle. Part of our job as machine learning engineers and data scientists is to work with _iteration_.\n\nYou could begin with a more complicated model, such as a _Support Vector Machine_, but configuration is not as straight forward, and you won't have a universal baseline of performance.\n\nLinear Regression is so widely used, that it serves as a stronger baseline of how effectively you can make predictions. With time, we can iterate and implement more complex models and analyze the improvements that may come with them.","22dbd70e":"As noted before,we see the correlation that as *GPA* increases, so does *chance_of_admit*. We also can see the distribution for columns. *GPA* seems to follow a more standard Gaussian distribution than *chance_of_admit*, and it is not as skewed.\n\nBut this one seems __quite obvious__. What about that tricker relationship, regarding *research*? Lets take a look at it now","7e73b398":"For the sake of consistency, we will convert our column names for the test dataframe and drop *Serial No.*.","9c9808fa":"## 1) Standard Imports and Loading the Dataset","f646bcf1":"### Test Set Loading & Formatting","885c79ac":"### Test Formatting, Evaluation Score\n\nWe set up our test and run the model","42dcd2fd":"### Correlations\n\nSince we're dealing with only a few number of features, lets implement a _correlation matrix_. A correlation matrix is great to get a high level view of our features and any possible relationships between them:","9efae988":"## 3) Data Exploration \/ Visualization","caefe69a":"### Quantitative-Cateogrical Analysis with Research and Acceptance","7ce3ce6b":"### A Note on Picking ML Models","52dc2dd1":"### Acceptance Distributions","6b30c369":"Here we can see that it _roughly_ follows a normal (gaussian ditribution), skewed to the left.\n\nWe can confirm our observation by using a _density plot_","4503dab3":"# Graduate Admissions Data:\n\n## An Exploration & Endeavor To Predict Acceptance Probabilities\n\nWhat do the chances for getting accepted into grad school look like for a prospective student?\n\nIn this notebook, we will attempt to aswer this question by:\n\n- Implementing data visualization techniques\n- Utilizing Data Cleaning \/ Features Engineering\n- Predicting outcomes with a machine learning model\n\nLets get to it!","cc900e3e":"Obvious start, we'll begin with some standard imports for working with data and load the dataset itself:","cee076a7":"What does this mean? It means that for most of our data entries, our distribution leans to most of them having higher probabilities of grad school acceptance.\n\nThis could pose a problem in our analysis and any future predictions because of a possible __bias__ in a test set. Our bias could potentially be that we give candidates a higher probability score than what it actually should be.\n\nHowever, given the current dataset, we cannot extrapolate to what a healthy and realistic distribution would look like. We will continue working with the probabilites at hand, but also keep this potential bias problem on our minds.","613ace68":"_Interesting!_ Based on our new findings, we can see that __almost all candidates with higher chances of admission have participated in research work__.\n\nThis is a great example of needing to use various visualization tools to get a better understanding of our data. If we only had relied on the correlation matrix, we would have concluded that research would not have mattered for grad school acceptance.","f545986e":"As we can see, GPA was the most influential factor for our linear regression model, with research coming in second.\n\nIts also interesting to see that GPA outweighs all the other features, by almost as much as 5 times compared to research.\n\nNow that the model has been fitted, lets evaluate it with our test set","269d2e12":"With this notebook, we created a full machine learning pipeline, from mapping out an initial dataframe to making reasonable predictions about a target variable.\n\nWe dealt with mostly straighforward data, with not many dimensions regarding our features. This allowed us to fit an incredibly effective but simple linear regression model that brought with it strong prediction results. To recap on some highlights\/ lessons learned:\n\n- Predictions for small datasets are not viable for deep learning techniques. Instead, we choose to go with traditional ML algorithms.\n- Data Cleaning\/ Feature Engineering helps us in standardizing our dataset, and makes exploration\/visuazization a much easier task\n- Never settle for one particular visualization. At first glance, research was not too important when looking at grad admission rates. After looking at a colored scatterplot, we discovered otherwise.\n- Machine Learning modelling should be an iterative process; start simple, and as needed work with more advanced algorithms and configurations.","c037120c":"## 5) Evaluating our Model","1254bae0":"We've achieved our basic goal of data exploration and visualization. Its always a good idea to look further and do more visualizations, but we will stop for now and move on to seeing how we can predict acceptance probabilities for candidates","c0ae944f":"It is common to denote our features by **X**, and our target variable as __y__. We will follow this common notation here when creating our training set","14549b1a":"We haven't looked too closely at the data yet to get involved in faeatures engineering, but we can already start with some items that will make our life easier. Right off the bat, this is what we can do:\n\n- Get rid of the 'Serial No.' Column, as it only serves the purpose of identifying entries and would not contribute to data exploration\/visualization\/predicitons\n- Standardize the column names, for ease of access. We will go ahead and use *snake_case*. Also, uncommon acronyms will be replaced for their meaning (like 'LOR' for 'Letter of Recommendation'), but keep common ones, like GPA","92a305e7":"### Analyzing Coefficients\n\nNow that it has been trained, we can see how heavily each feature influenced our linear regression model. This will be good insight to se understand whats happening under the hood:","9cee994d":"### Importing sklearn ||  Seperating our Feature Inputs and Target\n\nTo implement linear regression, we will import the existing function from scikit learn, one of the landmark libraries for data science programming in Python. We will also go ahead and import *metrics*, which will allow us to quantify our predictions based on their actual results.","57fc2dd7":"### Initializing and Training our Regression Model\n\nNow that our training set has been properly allocated, we can actually create an instance of our model and begin training!","9a599824":"## 4) Building a Machine Learning Model for Acceptance Predictions\n\nTime for everyone's favorite part, machine learning! \n\nBefore we go crazy throwing all kinds of ml models at the problem, let's break down what exactly we are trying to do here:\n\n__Given all features, given *chance_of_admit*, predict the *percentage* of *chance_of_admit*__\n\nFrom our problem statement, we can identidy the following:\n\n- Predicting based on an already given label is a _supervised learning_ problem\n- Predicting based on a percentage can be modeled as a _regression_ problem\n\nThis makes our initial endeavor into ML for this dataset straightforward: _We will begin with a simple linear regression model to predict chance of admission._ ","96950584":"So we're off to a good and clear start. Here is what we can tell so far:\n\n- __Data Shape:__ By looking at the first few entries and dataframe shape, we can see that we are working with 400 entries, each one with 9 features.\n- __Feature Data Types:__ All of our features are _numerical_, meaning they are quantified measurements which we can perform mathematical operations on without doing any kind of special conversion.\n- __Amount of Data:__ As we just mentioned, we have only 400 entries, and this is important to take note. It means that when it comes time to implement some machine learning model, _we should avoid neural networks_. Why? Neural networks only start to work well with large datasets (at least a few thousand entries), and our 400 entries now won't cut it. We will have to work with more traditional techniques to see good results\n\nGreat! Before we can go into some further data exploration and begin visualization techniques, lets do a little housekeeping:","91639dd5":"We can see that our model is making reasonable assumptions about admission rates! While not perfect, this is a good first step with a linear regression model.\n\nLet's finally take an accurate score with the __Mean Absolute Error__","484aa29e":"Before using our actual loss function, lets take a manual look of how our predictions compare to the actual results","90a1d514":"## 2) Basic Data Cleaning","7be17d94":"Now we've followed the same structure as our training data","49607075":"The heatmap allows us to note the stronger correlations by lighter-colored tiles. \n\nWe can see that *chance_of_admit_*, *GRE_score*, *TOEFL_score*, and *GPA*, are closely related. If one of these move in a certain direction (increase or decrease in value), it is very likely that the other features will follow in that direction.\n\nHigher University scores also seem to influence *chance_of_admit*, but not as much as GPAs and test scores.\n\nInterestingly, we note that *research* seems to be weakly correlated with our *chance_of_admit*. Its important to remember that this is binary value, with 1 representing experience in research, while 0 represents no experience. Soon, we will take a closer look and see how they add up.","f96f4536":"We just used a scatter plot to visualize the relationship between *GPA* and *chance_of_admit*. But how about research? Here we implement the same scatterplot visualization, but we discriminate them by research status","90a38be7":"Evaluation is crucial to measure the performance of our regression model. For this we need two things:\n\n- A test set\n- A criterion, also known as a loss function\n\nFor our test set, we will load that in as it has already been prepared. For the criterion, we will use the __MAE (Mean Absolute Error) Loss Function__.","5c2bad4b":"## Conclusion"}}