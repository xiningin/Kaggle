{"cell_type":{"013e8589":"code","69dee1cb":"code","1a455f12":"code","928d3eac":"code","f398c0ba":"code","04ecca05":"code","5ed48567":"code","874ae5d3":"code","8a8fb66d":"code","6ad370bc":"code","51073a22":"code","1ec178d0":"code","03461fd5":"code","9c016440":"code","c8d08a51":"code","23eff606":"code","b52cc4c9":"code","8110172c":"code","d52b47bb":"code","57afbdc1":"code","ffaa2f94":"code","8aa7e463":"code","f036fe81":"code","49a631cb":"code","334d4022":"code","1c79653a":"code","49c61055":"code","ff9ad122":"code","74e64030":"code","009c215e":"code","5d192fb1":"code","eea6d284":"code","83e8dd1d":"code","93474e43":"code","cd82f8c1":"code","2e82d0c8":"code","003d4c56":"code","3ef577e2":"code","6519198a":"code","17b44616":"code","bc1349eb":"code","e81dff92":"code","0662e7c2":"code","4a962469":"code","90ea1b2e":"code","e11da813":"code","2edb59f5":"code","08cfb96b":"code","0e5fb6e3":"code","8ebb9c0e":"code","6ebe407b":"code","00a7c031":"code","6f8f7b1d":"markdown","aa7b9b7e":"markdown","21cfc5e3":"markdown","1ae18ce8":"markdown","fe1e41d4":"markdown","949a3e3f":"markdown","32714c15":"markdown","405a2bfa":"markdown","771a3532":"markdown","515c1901":"markdown","e93d4a94":"markdown","143d2a64":"markdown","1ada138f":"markdown","c6ce7efe":"markdown","a16ddf6a":"markdown","cafd28c7":"markdown","45b7f5d4":"markdown","d850c337":"markdown","69a28d3e":"markdown","a0a4f6f1":"markdown","562fbcd4":"markdown","f49deea8":"markdown","5b802738":"markdown","47e33cf3":"markdown","36fcbd3e":"markdown","552ffea9":"markdown","8824a084":"markdown","9fc8cfd4":"markdown","992fc4ef":"markdown","2c22055a":"markdown","72916723":"markdown","201f67ec":"markdown","034c7209":"markdown","f7d98cf8":"markdown","cfb7366a":"markdown","1b67b95a":"markdown","6e4944cd":"markdown","624419d1":"markdown","30cc647a":"markdown","460064f8":"markdown","757fd75c":"markdown","93470cac":"markdown"},"source":{"013e8589":"# import packages\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","69dee1cb":"# load the titanic data\n\ntest_data = pd.read_csv('..\/input\/test-data\/testdata_with_groundTruth.csv')\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\n\n# combine test and train data sets\n\ntitanic_data = train_data.append(test_data,ignore_index=True)","1a455f12":"# inspect the titanic dataset\n\ntitanic_data.info()","928d3eac":"titanic_data.head()","f398c0ba":"# fill the NANs in 'Age' column with the median values\n\ntitanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].dropna().median())\ntitanic_data['Age'] = titanic_data['Age'].astype(int)\n\n# fill the NANs in 'Embarked' column with the most common value\n\ntitanic_data['Embarked'].value_counts()\ntitanic_data['Embarked'] = titanic_data['Embarked'].fillna('S')","04ecca05":"# convert the 'Cabin' values into categorical values(numbers) and fill the NANs\n\ntitanic_data['Cabin'] = titanic_data['Cabin'].apply(lambda x: x[0] if x == x else x)\n\ncabins = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'U': 8}\ntitanic_data['Cabin'] = titanic_data['Cabin'].map(cabins)\ntitanic_data['Cabin'] = titanic_data['Cabin'].fillna(0)\ntitanic_data['Cabin'] = titanic_data['Cabin'].astype(int)","5ed48567":"# fill the NANs in 'Fare' column with the median values\n\ntitanic_data['Fare'] = titanic_data['Fare'].fillna(titanic_data['Fare'].dropna().median())\ntitanic_data['Fare'] = titanic_data['Fare'].astype(int)","874ae5d3":"# verify that we handled the missing values\n\ntitanic_data.isnull().sum()","8a8fb66d":"# add the values of 'SibSp' and 'Parch' column to create a new column 'Family size'\n\ntitanic_data['Family size'] = titanic_data['SibSp'] + titanic_data['Parch']","6ad370bc":"dt_train = titanic_data[0:891]","51073a22":"# how many males and females survived and not survived\n\nsex_survived = dt_train[dt_train['Survived'] == 1]['Sex'].value_counts()\nsex_not_survived = dt_train[dt_train['Survived'] == 0]['Sex'].value_counts()\n\ndf = pd.DataFrame([sex_survived, sex_not_survived])\ndf.index = ['Survived','Not Survived']\n\ndf.plot(kind ='bar',color = ['limegreen','dodgerblue'],figsize=(8,4))\n\nplt.ylabel('Number of Passengers',fontsize=14)\nplt.title('Survival by Gender',fontsize=16)","1ec178d0":"# how many males and females survived and not survived by age\n\nplt.figure(figsize=(14, 5))\ngender = [dt_train[dt_train['Sex']=='female'],dt_train[dt_train['Sex']=='male']]\n\nfor i,v in enumerate(gender):\n    \n    plt.subplot(1, len(gender) , i+1)\n    plt.hist([v[v['Survived']==1]['Age'], v[v['Survived']==0]['Age']], bins=12, label=['Survived','Not Survived'],color = ['turquoise','plum'])\n    \n    plt.xlabel('Age',fontsize=14)\n    plt.ylabel('Number of Passengers',fontsize=14)\n    plt.legend()   \n    plt.title('Females',fontsize=16) if i==0  else plt.title('Males',fontsize=16)","03461fd5":"# passengers who survived and not survived by Pclass\n\nplt.figure(figsize=(8, 4))\ndt_train[dt_train['Survived']==1].groupby('Pclass').mean()['Fare'].plot(kind='bar',color='turquoise',label='Survived')\ndt_train[dt_train['Survived']==0].groupby('Pclass').mean()['Fare'].plot(kind='bar',color='plum',label='Not Survived')\n\nplt.ylabel('Number of Passengers',fontsize=14)\nplt.xlabel('Pclass',fontsize=14)\nplt.legend()\nplt.show()","9c016440":"# passengers who survived and not survived by fare\n\nplt.figure(figsize=(8, 4))\nplt.hist([dt_train[dt_train['Survived']==1]['Fare'],dt_train[dt_train['Survived']==0]['Fare']],bins=12,label=['Survived','Not Survived'],color=['turquoise','plum'])\n\nplt.xlabel('Fare',fontsize=14)\nplt.ylabel('Number of Passengers',fontsize=14)\nplt.legend()","c8d08a51":"# passengers who survived and not survived by family size\n\nplt.figure(figsize=(8, 4))\nplt.hist([dt_train[dt_train['Survived']==1]['Family size'],dt_train[dt_train['Survived']==0]['Family size']],bins=12,label=['Survived','Not Survived'],color=['turquoise','plum'])\n\nplt.xlabel('Family size',fontsize=14)\nplt.ylabel('Number of Passengers',fontsize=14)\nplt.legend()","23eff606":"# passengers who survived and not survived by port of embarkation\n\nemb_survived = dt_train[dt_train['Survived'] == 1]['Embarked'].value_counts()\nemb_not_survived = dt_train[dt_train['Survived'] == 0]['Embarked'].value_counts()\n\ndf=pd.DataFrame([emb_survived, emb_not_survived])\ndf.index = ['Survived','Not survived']\n\ndf.plot(kind='bar',color=['limegreen','dodgerblue','slateblue'],figsize=(8,4))\n\nplt.ylabel('Number of Passengers',fontsize=14)\nplt.title('Survival by Port of Embarkation',fontsize=16)","b52cc4c9":"# convert 'Embarked' values into categorical values(numbers)\n\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ntitanic_data['Embarked'] = titanic_data['Embarked'].map(ports)","8110172c":"# convert 'Sex' values into categorical values(numbers)\n\ntitanic_data['Sex'] = titanic_data['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","d52b47bb":"# extract the titles from the 'Name' values and convert them into categorical values(numbers) \n\ntitanic_data['Title'] = titanic_data['Name'].apply(lambda x: x.split(', ')[1].split('.')[0])\ntitles ={'Mr': 1,\n'Mrs': 2,\n'Miss': 3,\n'Master': 5,\n'Don': 6,\n'Rev': 4,\n'Dr': 4,\n'Mme': 2,\n'Ms': 3,\n'Major': 4,\n'Lady': 6,\n'Sir': 6,\n'Mlle': 3,\n'Col': 4,\n'Capt': 4,\n'the Countess': 6,\n'Jonkheer': 6,\n'Dona': 6\n}\n\n# create the new column 'Title' \n\ntitanic_data['Title'] = titanic_data.Title.map(titles)","57afbdc1":"# delete the 'Name' column\n\ndel titanic_data['Name']","ffaa2f94":"# inspect the 'Ticket' values\n\ntitanic_data['Ticket'].count()","8aa7e463":"# count the unique values of 'Ticket' column\n\ntitanic_data['Ticket'].nunique()","f036fe81":"# delete the 'Ticket' and 'PassengerId' columns\n\ndel titanic_data['Ticket']\n\ndel titanic_data['PassengerId']","49a631cb":"# verify that all features are of type 'int' \n\ntitanic_data.info()","334d4022":"# plot the correlation heatmap\n\ncorrelation_matr = titanic_data[0:891].corr().round(2)\n\nplt.figure(figsize=(9, 5))\nmp = sns.heatmap(data=correlation_matr, annot=True, cmap=sns.diverging_palette(220, 10, as_cmap=True))\nbottom,top = mp.get_ylim()\nmp.set_ylim(bottom + 0.5, top - 0.5)","1c79653a":"titanic_data.head()","49c61055":"pd.qcut(titanic_data['Age'], q=6,duplicates='drop')","ff9ad122":"# convert 'Age' values into categorical values(numbers) \n\ntitanic_data.loc[ titanic_data['Age'] <= 18, 'Age'] = 0\ntitanic_data.loc[(titanic_data['Age'] > 18) & (titanic_data['Age'] <= 22), 'Age'] = 1\ntitanic_data.loc[(titanic_data['Age'] > 22) & (titanic_data['Age'] <= 27), 'Age'] = 2\ntitanic_data.loc[(titanic_data['Age'] > 27) & (titanic_data['Age'] <= 29), 'Age'] = 3\ntitanic_data.loc[(titanic_data['Age'] > 29) & (titanic_data['Age'] <= 40), 'Age'] = 4\ntitanic_data.loc[ titanic_data['Age'] > 40, 'Age'] = 5","74e64030":"pd.qcut(titanic_data['Fare'], q=6,duplicates='drop')","009c215e":"# convert 'Fare' values into categorical values(numbers) \n\ntitanic_data.loc[ titanic_data['Fare'] <= 7, 'Fare'] = 0\ntitanic_data.loc[(titanic_data['Fare'] > 7) & (titanic_data['Fare'] <= 10), 'Fare'] = 1\ntitanic_data.loc[(titanic_data['Fare'] > 10) & (titanic_data['Fare'] <= 15), 'Fare'] = 2\ntitanic_data.loc[(titanic_data['Fare'] > 15) & (titanic_data['Fare'] <= 26), 'Fare'] = 3\ntitanic_data.loc[(titanic_data['Fare'] > 26) & (titanic_data['Fare'] <= 53), 'Fare'] = 4\ntitanic_data.loc[ titanic_data['Fare'] > 53, 'Fare'] = 5","5d192fb1":"titanic_data.head()","eea6d284":"# train and test data\n\ntrain_dt = titanic_data[0:891]\ntest_dt = titanic_data[891:]\n\nx_train = train_dt[['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked','Family size','Title']]\ny_train = train_dt['Survived']\n\nx_test = test_dt[['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Embarked','Family size','Title']]\ny_test = test_dt['Survived']","83e8dd1d":"# import packages\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom numpy.random import seed\nimport tensorflow \n","93474e43":"# Logistic regression\n\nprint('Logistic Regression:')\nlr = LogisticRegression(solver='lbfgs')\n\n%time lr.fit(x_train,y_train)\n\nscore_train  = lr.score(x_train, y_train)\nprint('Train score:',score_train)\nscore_test  = lr.score(x_test, y_test)\nprint('Test score:',score_test)\nprint('------------------------')\n\n# Random Forest\n\nprint('Random Forest Classifier:')\nrfc = RandomForestClassifier(n_estimators=100)\n\n%time rfc.fit(x_train,y_train)\n\nscore_train  = rfc.score(x_train, y_train)\nprint('Train score:',score_train)\nscore_test  = rfc.score(x_test, y_test)\nprint('Test score:',score_test)\nprint('------------------------')\n\n# Neural Network (MLP)\n\nprint('Neural Network:')\n\ndef neural_network(lyrs=[8], act='linear', opt='Adam', dr=0.0):\n    \n    seed(42)\n    tensorflow.random.set_seed(42)\n    \n    model = Sequential()\n    model.add(Dense(lyrs[0], input_dim=x_train.shape[1], activation=act))\n    \n    for i in range(1,len(lyrs)):\n        model.add(Dense(lyrs[i], activation=act))\n    \n    model.add(Dropout(dr))\n    model.add(Dense(1, activation='sigmoid')) \n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    return model\n\nnn = neural_network()\n%time nn.fit(x_train, y_train, epochs=100, batch_size=32, verbose=0)\n\naccuracy = np.mean(nn.fit(x_train, y_train, epochs=100, batch_size=32, verbose=0).history['accuracy'])\nprint('Training Accuracy:', accuracy)\nscores = nn.evaluate(x_test, y_test, verbose=0)\nprint('Testing accuracy:',scores[1])","cd82f8c1":"# Logistic regression: cross-validation for selecting the optimal C\n\nprint('logistic regression-cross validation:tuning hyper-parameter')\n\nparameters = [{'C': [1, 10, 100, 1000]}]\nclf = GridSearchCV(LogisticRegression(solver='lbfgs'), parameters, cv=5)\nclf.fit(x_train, y_train)\n\nprint('best parameters:')\nprint(clf.best_params_)","2e82d0c8":"# print the corresponding testing accuracy\n\noptimal_C = 10\nprint('Optimal C:', optimal_C)\n\nlr = LogisticRegression(solver='lbfgs',C = optimal_C).fit(x_train, y_train)\ny_pred = lr.predict(x_test)\n\nprint('logistic regression classifier accuracy:',accuracy_score(y_test, y_pred))","003d4c56":"# function for feature importances:logistic regression\n\ndef plot_coefficients(classifier, feature_names, clf_id, top_features=5):\n    coef = classifier.coef_.ravel()\n    top_positive_coefficients = np.argsort(coef)[-top_features:]\n    top_negative_coefficients = np.argsort(coef)[:top_features]\n    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n    \n    plt.figure(figsize=(10, 5))\n    plt.title(clf_id)\n    colors = ['coral' if c < 0 else 'cornflowerblue' for c in coef[top_coefficients]]\n    plt.bar(np.arange(2*top_features), coef[top_coefficients], color=colors)\n    feature_names = np.array(feature_names)\n    plt.xticks(np.arange(1+2*top_features), feature_names[top_coefficients], rotation=40, ha='right')\n    plt.show()","3ef577e2":"# plot the feature importances\n\nplot_coefficients(lr,x_train.columns,'Logistic Regression')","6519198a":"# Random forest: cross-validation for selecting the optimal parameters\n\nprint('random forest cross validation:tuning hyper-parameter')\nrf=RandomForestClassifier(random_state=42)\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nclf = GridSearchCV(estimator=rf,param_grid=param_grid, cv= 5)\nclf.fit(x_train, y_train)\n\nprint('best parameters:')\nprint(clf.best_params_)   ","17b44616":"# print the corresponding testing accuracy\n\nbest_params = {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 200}\nprint('Best parameters:',best_params)\n\nrfc = RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=8, criterion='gini')\nrfc.fit(x_train, y_train)\ny_pred = rfc.predict(x_test)\n\nprint('Random forest classifier accuracy:',accuracy_score(y_test, y_pred))","bc1349eb":"# function for feature importances:random forest\n\ndef plot_keyFeat_trees(classifier, feature_names, clf_id, top_features=10):\n    importances = classifier.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    indices_vis = indices[:top_features]\n    \n    plt.figure()\n    plt.figure(figsize=(10, 5))\n    plt.title(clf_id)\n    plt.bar(range(top_features), importances[indices_vis],\n       color=\"cornflowerblue\", align=\"center\")\n    plt.xticks(range(top_features), indices)\n    plt.xlim([-1, top_features])\n    feature_names = np.array(feature_names)\n    plt.xticks(np.arange(0, top_features), feature_names[indices_vis], rotation=40, ha='right')\n    plt.show()","e81dff92":"# plot the feauture importances\n\nplot_keyFeat_trees(rfc,x_train.columns,'Random Forest')","0662e7c2":"# drop the 'Parch' column\n\nx_train  = x_train.drop(\"Parch\", axis=1)\nx_test  = x_test.drop(\"Parch\", axis=1)","4a962469":"# Logistic regression: cross-validation for selecting the optimal C\n\nprint('logistic regression-cross validation:tuning hyper-parameter')\n\nparameters = [{'C': [0.1,1, 10, 100, 1000]}]\nclf = GridSearchCV(LogisticRegression(solver='lbfgs'), parameters, cv=5)\nclf.fit(x_train, y_train)\n\nprint('best parameters:')\nprint(clf.best_params_)","90ea1b2e":"# print the corresponding testing accuracy\n\noptimal_C = 10\nprint('Optimal C:', optimal_C)\n\nlr = LogisticRegression(solver='lbfgs',C = optimal_C).fit(x_train, y_train)\ny_pred = lr.predict(x_test)\n\nprint('logistic regression classifier accuracy:',accuracy_score(y_test, y_pred))","e11da813":"# Random forest: cross-validation for selecting the optimal parameters\n\nprint('random forest-cross validation:tuning hyper-parameter')\nrf=RandomForestClassifier(random_state=42)\n\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nclf = GridSearchCV(estimator=rf,param_grid=param_grid, cv= 5)\nclf.fit(x_train, y_train)\n\nprint('best parameters:')\nprint(clf.best_params_) ","2edb59f5":"# print the corresponding testing accuracy\n\nbest_params = {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 500}\nprint('Best parameters:',best_params)\n\nrfc = RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 500, max_depth=8, criterion='gini')\nrfc.fit(x_train, y_train)\ny_pred = rfc.predict(x_test)\n\nprint('Random forest classifier accuracy:',accuracy_score(y_test, y_pred))","08cfb96b":"# Neural Network: cross-validation for selecting the optimal parameters\n\nprint('neural network-cross validation:tuning hyper-parameter')\nnn = KerasClassifier(build_fn=neural_network,batch_size=32,epochs=50,verbose=0)\n\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam']\nlayers = [(8),(10),(10,5)]\ndrops = [0.0, 0.05, 0.1, 0.2]\n\nparam_grid = dict(opt=optimizer,lyrs=layers,dr=drops)\n\nclf = GridSearchCV(estimator=nn,param_grid=param_grid,cv=3,verbose=2,n_jobs=-1)  \nclf.fit(x_train, y_train)\n\nprint('best parameters:')\nprint(clf.best_params_) ","0e5fb6e3":"# print the corresponding testing accuracy\n\nbest_params = {'dr': 0.2, 'lyrs': (10,5), 'opt': 'RMSprop'} \nprint('Best parameters:',best_params)\n\nnn = neural_network(lyrs=(10,5), dr=0.2,opt='RMSprop') \nnn.fit(x_train, y_train, epochs=50, batch_size=32, verbose=0) \nscores = nn.evaluate(x_test, y_test,verbose=0) \nprint('Testing accuracy:',scores[1])","8ebb9c0e":"results_dt = pd.DataFrame({'Model': [ 'Logistic Regression','Random Forest','Neural Network'],'Score': [ 0.7703,0.7632,0.7632]})\nresults_dt","6ebe407b":"# create a  DataFrame with the passengers ids and the predictions of Logistic Regression for survival \n\ntest_data['Survived'] = y_pred\n\nsolution = test_data[['PassengerId', 'Survived']]\nsolution.head()","00a7c031":"# convert the DataFrame into a csv file that can be uploaded\n\nfilename = 'Titanic Predictions.csv'\n\nsolution.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","6f8f7b1d":"- The dataset was split into a training set consisting of 891 rows, and a test set consisting of 418 rows\n- The Machine learning models of Logistic Regression, Random Forest and Neural Network were trained\n- The models were applied on the training set, the test set was used for prediction\n- Predictive metrics were estimated for each model ","aa7b9b7e":"In order to reduce overfitting and improve prediction metrics, the methods of hyper-parameter tuning and feature importances were applied on the Logistic Regression and Random Forest models:","21cfc5e3":"- The port of embarkation might be correlated with survival \n- Those who embarked at Southampton had a significantly higher probability of not surviving","1ae18ce8":"### Train the models again ","fe1e41d4":"The attributes that contained missing values (NANs) were the following: \n\n- 'Age': 263 Missing Values\n- 'Embarked': 2 Missing Values\n- 'Cabin': 1014 Missing Values\n- 'Fare': 1 Missing Value","949a3e3f":"The final overview of the titanic dataset:","32714c15":" - Logistic's Regression test score increased","405a2bfa":"Logistic Regression was the model with the highest accuracy, so its predictions will be uploaded:","771a3532":"### Logistic Regession, Random Forest,Neural Network ","515c1901":"- Also 'Ticket' and 'PassengerId' columns were deleted, as they didn't seem to contribute to survival ('Ticket' had unique values)","e93d4a94":"### Cross-validation for tuning hyper-parameters and Feature Importances","143d2a64":"## Inspect data","1ada138f":" - \u201cOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew\u201d\n - The Kaggle\u2019s competition Titanic dataset was analyzed so as to predict what categories of passengers were likely to survive the sinking of the ocean liner \n - In this project, machine learning methods were used in order to predict as accurate as possible from general information about passengers which passengers survived the tragedy","c6ce7efe":"- Those with 1-3 relatives were more likely to survive","a16ddf6a":"- 'Age' and 'Fare' values were on a different scale compared to the rest of the attributes\n- We divided those values into 6 categories for each attribute","cafd28c7":"## Submission","45b7f5d4":"Before proceeding with EDA, the missing values were imputed:","d850c337":"## Exploratory Data Analysis","69a28d3e":"EDA was performed on the training dataset:","a0a4f6f1":"- Many features needed to be converted from objects into numbers","562fbcd4":"For the reduced data set, the best hyper-parameters were tuned for all the models and we calculated the corresponding accuracies:","f49deea8":"## Data Munging","5b802738":"Let's inspect the titanic dataset to see the types of attributes and whether missing values exist:","47e33cf3":"## Introduction","36fcbd3e":"Then we performed some EDA:","552ffea9":"- Women had higher survival chances between 15 and 35 years old \n- Men were more likely to survive when they were between 25 and 35 years old \n- Infants had a higher probability of survival ","8824a084":"- Hyper-parameter tuning on the reduced data set decreased the performance of Neural Network but improved the performance of Random Forest \n- The performance of Logistic Regression didn't improve \n- All accuracies were close to each other, with Logistic Regression's being the highest\n","9fc8cfd4":"## Machine Learning Algorithms","992fc4ef":"- Sex seemed to correlate well with survival\n- Women were more likely to survive","2c22055a":"- The upper-class had more survival chances \n- This is verified by the fact that as the fare increased, those who survived were always more compared to those who didn\u2019t survive","72916723":"- Some groups of people were more likely to survive the Titanic sinking: women and the upper-class\n- Also there was a higher probability of survival for infants \n- Although the performance of Random Forest increased considerably, the accuracy for Logistic Regression didn't improve and the accuracy for the Neural Network decreased after reducing the dimensionality of the dataset\n- The models performed on unknown testing data, with an accuracy of around  76 \u2013 77% \n","201f67ec":"The observations from earlier are verified from these two graphs: \n- Sex, Title and Pclass features have higher importances, confirming that they correlate well with survival \n- 'Parch' is suggested to have the lowest importance from both models, therefore the dimensionality of the data set could be reduced by omitting that feature:","034c7209":"Also, a 'Family size' column with the sum of 'SibSp' and 'Parch' was created:","f7d98cf8":"The dataset consisted of 1.309 records of Titanic passengers.There are the following 11 attributes:\n\n- 'Survived': survival (1=Yes, 0=No)\n- 'Pclass': the passenger class\n- 'Name' (object)\n- 'Sex' (object)\n- 'Age'\n- 'Sibsp': number of siblings\/spouses aboard\n- 'Parch': number of parents\/children aboard\n- 'Ticket' (object)\n- 'Fare'\n- 'Cabin' (object)\n- 'Embarked': Port of embarkation(C = Cherbourg, Q = Queenstown, S = Southampton) (object)","cfb7366a":"The transformed dataset:","1b67b95a":"# Titanic:My first Kaggle competition\ud83d\udea2\ud83d\udcca- EDA and Machine Learning","6e4944cd":"## Feature Engineering ","624419d1":"- Neural Network performed best, although the computational time for it was slower\n- Random Forest had lower test score, and we see that the gap between the training and testing  accuracy for it is larger ->   sign of overfitting \n- Although the performance of the models on the test set is good, it could be improved, especially for Random Forest\n","30cc647a":"## Conclusions","460064f8":"Before training the machine learning models, we performed some feature engineering:\n","757fd75c":" - Random's Forest test score increased as well","93470cac":"- Sex, Title and Pclass attributes have higher correlations with the survival\n- 'Parch' and 'Embarked' appear to correlate less with survival \n- 'Title' correlates well with 'Sex' -> stronger relationship between sex feature and survival"}}