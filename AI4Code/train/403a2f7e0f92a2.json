{"cell_type":{"47d56824":"code","45be70e2":"code","31e58ef4":"code","195125e7":"code","19510aa9":"code","a4acd8cb":"code","2ef4908a":"code","0eb5baf6":"code","9ca932c0":"code","c3dfe471":"code","32d83236":"code","536c9180":"code","455b33d5":"code","2143df4a":"code","8bdedabf":"code","5d06ae4f":"code","b3a22a0a":"code","a2f558ab":"code","1bfb8f31":"code","fb156a7e":"code","416840d0":"code","41ff7baf":"code","854f9148":"code","c788e135":"code","9f7d55f0":"code","a0c6c32d":"code","2dde8284":"code","efd158c8":"code","d9cc901b":"code","8905b652":"code","da1bb87f":"code","be2e3a50":"code","b2ea15af":"code","aa818a16":"code","5b8d4b8f":"code","dc356eb5":"code","e6bd313c":"code","38b64bd4":"code","bc228548":"code","2c072646":"code","dc88dde9":"code","b35dfd01":"code","7fd39d15":"code","5636dd09":"code","8d19d50a":"code","64645c5c":"code","f8b35cf7":"code","2cc98114":"code","fdc00b60":"code","ab0dffe8":"code","d0c5293e":"code","fab2391b":"code","c5a88a57":"markdown","0322b53b":"markdown","371ec8f9":"markdown","61c2ebb5":"markdown","ccf236be":"markdown","713b68e0":"markdown","e7304416":"markdown","2f08ed68":"markdown","36ee927e":"markdown","a1dd6fcb":"markdown","9f73f056":"markdown","24297740":"markdown","26ca5cb8":"markdown","80a6b958":"markdown","fe57b74b":"markdown","4479357b":"markdown","17758976":"markdown","98c8272e":"markdown","ccf8ebc3":"markdown","db199188":"markdown","137ca105":"markdown","c056078e":"markdown","42322160":"markdown","bc89e37e":"markdown","7b4f4923":"markdown","04559440":"markdown","80cc4fcd":"markdown","d86d9fef":"markdown","3480c615":"markdown","38b4a671":"markdown","11464934":"markdown","e6701adc":"markdown","b5447eec":"markdown","7e046935":"markdown","f5c12908":"markdown","953df29b":"markdown","d444a885":"markdown","da0decb7":"markdown","3a120183":"markdown","912308f6":"markdown","2853ce67":"markdown","54893d8a":"markdown","ba94dc24":"markdown","0614ef80":"markdown","f420a36b":"markdown","1e39864a":"markdown","86c97200":"markdown","4b1fb077":"markdown","243822b3":"markdown","1e256a28":"markdown","6d58ea52":"markdown","9ef792cc":"markdown","8b4ca6aa":"markdown","f65061f8":"markdown","1fd01cc0":"markdown","3f4b7429":"markdown","8c661737":"markdown","f54a67aa":"markdown","d5a50a4d":"markdown","e289ce27":"markdown"},"source":{"47d56824":"import torch\nimport  torch.nn as nn\nfrom torch.functional import F\nimport optuna\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split,KFold,cross_validate\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import r2_score,mean_squared_error\n\nimport lightgbm\nimport xgboost\nfrom tqdm.notebook import tqdm\n\nimport os\nimport random","45be70e2":"seed = 100\n\ndef seed_everything(seed=1):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed)","31e58ef4":"audi_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/audi.csv\"\nbmw_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv\"\nford_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/ford.csv\"\nhyundi_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv\"\nmerc_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/merc.csv\"\nskoda_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv\"\ntoyota_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv\"\nvauxhall_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv\"\nvw_path=\"..\/input\/used-car-dataset-ford-and-mercedes\/vw.csv\"","195125e7":"fig_count = (i for i in range(1,101))\ntable_count = (j for j in range(1,101))","19510aa9":"class Data_loading():\n    def __init__(self,path):\n        try:\n            self.df = pd.read_csv(path,delimiter=\",\",header=0)\n            self.basename = os.path.basename(path)\n        except:\n                print(f\"This path name not exist \u2192  {path}\")\n\n    def data_create(self):\n        print(\"Data name : {0} \\nFull batch size : {1} \\nDimension : {2}\".format(self.basename,self.df.shape[0],self.df.shape[1]))\n        print(f\"\\n Table{next(table_count)}. Dataframe first 5\")\n        display(self.df.head())\n        print(f\"\\n Table{next(table_count)}. Statistics Table\")\n        display(self.df.describe())\n        dummy_a = self.df.dtypes\n        dummy_b = np.sum(pd.isnull(self.df),axis=0)\n        check_df = pd.DataFrame((dummy_a,dummy_b),index=[\"dtype\",\"isnull\"]).T\n        print(f\"\\n Table{next(table_count)}. Dtype and Null-count\")\n        display(check_df)\n        return self.df,os.path.splitext(self.basename)[0]","a4acd8cb":"dl_audi = Data_loading(audi_path)\ndf_audi,audi = dl_audi.data_create()\n\ndl_bmw = Data_loading(bmw_path)\ndf_bmw,bmw = dl_bmw.data_create()\n\ndl_ford = Data_loading(ford_path)\ndf_ford,ford = dl_ford.data_create()\n\ndl_hyundi = Data_loading(hyundi_path)\ndf_hyundi,hyundi = dl_hyundi.data_create()\n\ndl_merc = Data_loading(merc_path)\ndf_merc,merc = dl_merc.data_create()\n\ndl_skoda = Data_loading(skoda_path)\ndf_skoda,skoda = dl_skoda.data_create()\n\ndl_toyota = Data_loading(toyota_path)\ndf_toyota,toyota = dl_toyota.data_create()\n\ndl_vauxhall = Data_loading(vauxhall_path)\ndf_vauxhall,vauxhall = dl_vauxhall.data_create()\n\ndl_vw = Data_loading(vw_path)\ndf_vw,vw = dl_vw.data_create()","2ef4908a":"df_hyundi = df_hyundi.rename(columns={\"tax(\u00a3)\":\"tax\"})\n\ndf_audi[\"company\"] = audi\ndf_bmw[\"company\"] = bmw\ndf_ford[\"company\"] = ford\ndf_hyundi[\"company\"] = hyundi\ndf_merc[\"company\"] = merc\ndf_skoda[\"company\"] = skoda\ndf_toyota[\"company\"] = toyota\ndf_vauxhall[\"company\"] = vauxhall\ndf_vw[\"company\"] = vw\n\ndf = pd.concat([df_audi, df_bmw,df_ford,\n                df_hyundi,df_merc, df_skoda,\n                df_toyota,df_vauxhall,df_vw],                \n               ignore_index=True,\n               axis=0,\n              )","0eb5baf6":"corr = df.corr(method=\"pearson\")\nfig,ax = plt.subplots(figsize=(12,12),dpi=80)\nplt.title(f\"Fig{next(fig_count)}. Correlation All data\",fontsize=12)\nheatmap = sns.heatmap(corr,vmin=-1,vmax=1,annot=True,linewidth=0.25,\n                    linecolor=\"gray\",square=True,cmap=\"bwr\",ax=ax)","9ca932c0":"import warnings\n\nwarnings.simplefilter('ignore')\ndf_sample = df.sample(frac=0.05) #compression\npair = sns.pairplot(df_sample,corner=False,hue=\"fuelType\",palette=\"RdPu_r\")\npair.fig.suptitle(f\"Fig{next(fig_count)}. PairPlot\",y=1.,fontsize=12)\npair.fig.set_size_inches(12,12)\nplt.show()","c3dfe471":"df_z = df.copy(deep=True)\ndf_z[\"price\"] = np.log(df[\"price\"])","32d83236":"sturges = lambda n: np.ceil(np.log2(n*2))\n\ngraph1 = sns.displot(\n            df['price'],\n            label='price (yen)',\n            kde=True,\n            rug=True,\n            bins=int(sturges(len(df['price']))),\n            color=\"#ba0f7b\",\n            alpha=0.3,\n        )\ngraph1.fig.set_figwidth(12)\nplt.title(f\"Fig.{next(table_count)} Price distribution\")\nplt.grid(which=\"major\",axis=\"y\",color=\"#ba0f7b\",linestyle=\"--\",alpha=0.2)\nplt.legend()\n\ngraph2 = sns.displot(\n            df_z['price'],\n            label='Log price (yen)',\n            kde=True,\n            rug=True,\n            bins=int(sturges(len(df['price']))),\n            color=\"#ba0f7b\",\n            alpha=0.3,\n        )\ngraph2.fig.set_figwidth(12)\nplt.title(f\"Fig.{next(table_count)} Log Price distribution\")\nplt.grid(which=\"major\",axis=\"y\",color=\"#ba0f7b\",linestyle=\"--\",alpha=0.2)\nplt.legend()\nplt.show()","536c9180":"fig, ax = plt.subplots(1,1,figsize=(20,4))\nsns.boxplot(x=\"year\", y=\"price\", data=df_z, showfliers=False, ax=ax)\nsns.stripplot(x=\"year\",y=\"price\",data=df_z,jitter=True,color='black',ax=ax,alpha=0.5,size=2)\nplt.xticks(rotation=90)\nplt.show()","455b33d5":"df_f = df_z[(df_z[\"year\"] <= 2021)] #goodbye\ndf_f = df_f.sort_values(by=\"year\",ascending=False)\ndf_f.head()","2143df4a":"df_f[df_f[\"year\"]==1970]","8bdedabf":"df_f = df_f[(df_f[\"year\"] > 1970)] #goodbye\ndf_f = df_f.sort_values(by=\"year\",ascending=True)\ndf_f.head()","5d06ae4f":"fig= plt.figure(figsize=(12,5))\nsns.countplot(x=\"year\",data=df_f,palette=\"RdPu\")\nplt.grid(which=\"major\",axis=\"y\",color=\"#ba0f7b\",linestyle=\"--\",alpha=0.2)\nplt.xticks(rotation=40)\nplt.show()","b3a22a0a":"mpg_to_kpl = lambda mpg : 0.425144*mpg\ndf_ = df_f.copy(deep=True)\ndf_[\"kpl\"] = df_f[\"mpg\"].apply(mpg_to_kpl)\ndf_s = df_.drop(\"mpg\",axis=1)","a2f558ab":"fuel_data = pd.DataFrame(df_s.groupby(by=\"fuelType\").size())\nfuel_data.columns = [\"Count\"]\ndisplay(fuel_data.sort_values(by=\"Count\",ascending=False).T)\n\ndf_t = df_s.copy(deep=True)\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"fuelType\",data=df_t,palette=\"RdPu_r\")\nplt.show()","1bfb8f31":"emit_data = pd.DataFrame(df_s.groupby(by=\"engineSize\").size())\nemit_data.columns = [\"count\"]\ndisplay(emit_data.T)\n\ndf_f = df_t.copy(deep=True)\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"engineSize\",data=df_f,palette=\"RdPu_r\")\nplt.xticks(rotation=90)\nplt.show()","fb156a7e":"company = pd.DataFrame(df_f.groupby(by=\"company\").size())\ncompany.columns = [\"count\"]\ndisplay(company.sort_values(by=\"count\",ascending=False).T)\n\ndf_ff = df_f.copy(deep=True)\n\nfig = plt.figure(figsize=(12,5))\nsns.countplot(x=\"company\",data=df_ff,order=df_f[\"company\"].value_counts().index,palette=\"RdPu_r\")\nplt.show()","416840d0":"df_si = pd.get_dummies(df_ff,drop_first=True)\ndf_si.head(3)","41ff7baf":"numerical_data = df_si.select_dtypes(include=[int,float]).drop(\"price\",axis=1)\nnum_col_name = numerical_data.columns\n\nsc = StandardScaler()\n_ = sc.fit_transform(numerical_data)\n\ndf_e = df_si.copy(deep=True)\ndf_e[num_col_name] = _","854f9148":"class ParamTuning():     \n    \"\"\"\n    This parameter optimization class uses optune developed by PFN.\n    Please See  https:\/\/www.preferred.jp\/ja\/projects\/optuna\/\n                https:\/\/github.com\/optuna\/optuna\n\n    *-----------------------------------------------------------------*\n    Note1. How to use\n    *-----------------------------------------------------------------*\n    tuner = HyperTuning(X_train,y_train,X_test,y_test)\n    lasso = tuner.tuning_lasso()\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(lasso,n_trials=n_trials)\n    *-----------------------------------------------------------------*\n\n    *-----------------------------------------------------------------*\n    Note2. Regressor params optimizer function name\n    *-----------------------------------------------------------------*\n    LightGBM Regressor          : tuning_lgm\n    *-----------------------------------------------------------------*\n    \"\"\"\n\n    def  __init__(self,X,y,X_target,y_target):\n        self.X = X\n        self.y = y.values.ravel()\n        self._ = y #Dummy\n        self.X_target = X_target # Validation Data\n        self.y_target = y_target # Validation Data\n\n    def tuning_lasso(self):\n        def objective(trial):\n            params = {\n            \"alpha\" : trial.suggest_float(\"alpha\",0.001,0.099),\n            \"tol\" : 0.01 \n            }\n            las = Lasso(**params,)\n            las.fit(self.X,self.y)\n            y_pred = las.predict(self.X_target)\n            error = mean_squared_error(self.y_target,y_pred)\n            return np.sqrt(error)\n        return objective\n\n    def tuning_xgb(self):\n        def objective(trial):\n            params = {\n            \"max_depth\" : trial.suggest_int(\"max_depth\",1,20),\n            \"eta\" : trial.suggest_float(\"eta\",0.001,0.1),\n            \"min_child_weight\" : trial.suggest_int(\"min_child_weight\",1,20),\n            \"subsample\" : trial.suggest_float(\"subsample\",0.1,0.9),\n            \"gamma\" : trial.suggest_float(\"gamma\",0.01,0.9),\n            \"gpu_id\": 0,\n            \"tree_method\" : \"gpu_hist\",\n            \"objective\" : \"reg:squarederror\",\n            }\n            xgb = xgboost.XGBRegressor(**params)\n            xgb.fit(self.X,self.y)\n            y_pred = xgb.predict(self.X_target)\n            error = mean_squared_error(self.y_target,y_pred)\n            return np.sqrt(error)\n        return objective\n    \n    def tuning_lgm(self):\n        \"\"\"\n        Minimize validation loss \u2192 Cross validation score\n        \"\"\"\n        def objective(trial):\n            params = {\n            'boosting_type':'gbdt',\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n            'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n            'colsample_bytree': trial.suggest_loguniform('colsample_bytree', .5, 1.),\n            'subsample': trial.suggest_loguniform('subsample', .5, 1.),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            \"device\":\"gpu\",\n            \"verbose\":-1,\n            }\n            lgm = lightgbm.LGBMRegressor(**params,random_state=seed)\n            kf = KFold(n_splits=3,shuffle=True,random_state=seed)\n            \n            X = pd.concat([self.X,self.X_target],axis=0)\n            y = pd.concat([self._,self.y_target],axis=0)\n            \n            score = cross_validate(lgm,X=X,y=y,scoring=\"neg_root_mean_squared_error\",cv=kf) #negative value!\n            return -1.*np.mean(score[\"test_score\"]) #negative val \u2192 positive val\n        return objective","c788e135":"X = df_e.drop(\"price\",axis=1)\ny = df_e.loc[:,[\"price\"]]\n\nX_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=0.25, random_state=seed)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train, test_size=0.1, random_state=seed)\n\nprint(\"------shapes------\")\nprint(f\"training  {X_train.shape}\")\nprint(f\"validatioin {X_valid.shape}\")\nprint(f\"test {X_test.shape}\" )","9f7d55f0":"PARAMETER_TUNING = True","a0c6c32d":"if PARAMETER_TUNING:\n    tuner = ParamTuning(X_train,y_train,X_valid,y_valid)\n\n    func0 = tuner.tuning_lasso()\n    func1 = tuner.tuning_xgb()\n    func2 = tuner.tuning_lgm()\n\n    n_trials = 20\n    study_lasso = optuna.create_study(direction=\"minimize\",study_name=\"LassoRegressor\")\n    study_lasso.optimize(func0,n_trials=n_trials)\n    study_xgb = optuna.create_study(direction=\"minimize\",study_name=\"XgBoostRegressor\")\n    study_xgb.optimize(func1,n_trials=n_trials)\n    study_lgm = optuna.create_study(direction=\"minimize\",study_name=\"LightGBMRegressor\")\n    study_lgm.optimize(func2,n_trials=n_trials)","2dde8284":"PARAMETER_SAVING = False","efd158c8":"if PARAMETER_SAVING:\n    import csv\n\n    params = [study_lasso.best_params,\n              study_svr.best_params,\n              study_lgm.best_params]\n\n    with open(\".\/best_params.csv\",\"w\",encoding=\"Shift_jis\") as f:\n        writer = csv.writer(f)\n        writer.writerow(params)","d9cc901b":"PARAMETER_LOADING = False","8905b652":"if PARAMETER_LOADING:\n    import ast\n    import csv\n\n    with open(\".\/best_params.csv\",\"r\") as f:\n        reader = csv.reader(f)\n        params = [param for param in reader]\n\n    params_list = []\n\n    for i in range(len(params[0])):\n        params_list.append(ast.literal_eval(params[0][i]))","da1bb87f":"lasso = Lasso(**study_lasso.best_params) # If PARAMETER_LOADING=True, **study_lasso.best_params\u21d2params_list[0]\n_ = lasso.fit(X_train,y_train)\nlasso_pred = lasso.predict(X_test)\nrmse_lasso = np.sqrt(mean_squared_error(y_test,lasso_pred))\nprint(\"RMSE = \",rmse_lasso)\nr2_lasso = (r2_score(y_test,lasso_pred))\nprint(\"r2_score = \",r2_lasso)","be2e3a50":"xgb = xgboost.XGBRegressor(gpu_id = 0,tree_method=\"gpu_hist\",**study_xgb.best_params) # If PARAMETER_LOADING=True, **study_lasso.best_params\u21d2params_list[0]\n_ = xgb.fit(X_train,y_train.values.ravel())\nxgb_pred = xgb.predict(X_test)\nrmse_xgb = np.sqrt(mean_squared_error(y_test,xgb_pred))\nprint(\"RMSE = \",rmse_xgb)\nr2_xgb = (r2_score(y_test,xgb_pred))\nprint(\"r2_score = \",r2_xgb)","b2ea15af":"lgm = lightgbm.LGBMRegressor(device=\"gpu\",**study_lgm.best_params)\n_ = lgm.fit(X_train,y_train.values.ravel())\nlgm_pred = lgm.predict(X_test)\nrmse_lgm = np.sqrt(mean_squared_error(y_test,lgm_pred))\nprint(\"RMSE = \",rmse_lgm)\nr2_lgm = (r2_score(y_test,lgm_pred))\nprint(\"r2_score = \",r2_lgm)","aa818a16":"feature_name = np.sort(X.columns)[::-1][:20]\nsc_importance = StandardScaler()\nxgb_importance = xgb.feature_importances_[::-1][:20]\nlgm_importance = np.sort(lgm.feature_importances_)[::-1][:20]\n\nimportance_df = pd.DataFrame({\"FeatureName\":np.array(feature_name),\n                         \"XgbImportance\":xgb_importance,\n                         \"LgmImportance\":lgm_importance,\n                        })\n\nimportance_df_sc = importance_df.copy(deep=True)\n\nimportance_df_sc[[\"XgbImportance\",\"LgmImportance\"]] = sc_importance.fit_transform(importance_df[[\"XgbImportance\",\"LgmImportance\"]])\nimportance_df_sc = importance_df_sc.melt(id_vars=[\"FeatureName\"],\n                                         value_name=\"Importance\",\n                                         var_name=\"Category\")\n\nfig,ax = plt.subplots(1,1,figsize=(12, 4))\n\nax.set_title(\"ex) Feature Importance 20\")\nsns.barplot(x=\"FeatureName\",y=\"Importance\",data=importance_df_sc,hue=\"Category\",palette=\"RdPu_r\",ax=ax)\nax.set_xlabel(\"Importance\")\nax.tick_params(axis=\"x\",rotation=90)\nplt.show()","5b8d4b8f":"plt_params={\"marker\":\"x\",\n            \"s\":1,\n            \"alpha\":0.8,\n            \"cmap\":\"RdPu\",\n           }\n\nfig,ax = plt.subplots(1,3,figsize=(12,4),constrained_layout=True)\n\nax[0].scatter(y_test,lasso_pred,c=lasso_pred,label=\"Lasso Regressor\",**plt_params)\nax[0].plot([0,13],[0,13],linestyle=\"--\",color=\"#4f4d4f\",lw=1)\nax[0].set_ylim(6,13);ax[0].set_xlim(6,13)\nax[0].set_title(\"Lasso Fitting\")\nax[0].set_ylabel(\"Prediction\")\nax[0].set_xlabel(\"True\")\nax[0].text(10,6.5,f\"R2 : {r2_lasso:.3f}\",color=\"gray\",fontsize=\"x-large\")\n\nax[1].scatter(y_test,xgb_pred,c=xgb_pred,label=\"XgBoost Regressor\",**plt_params)\nax[1].plot([0,13],[0,13],linestyle=\"--\",color=\"#4f4d4f\",lw=1)\nax[1].set_ylim(6,13);ax[1].set_xlim(6,13)\nax[1].set_title(\"XgBoost Fitting\")\nax[1].set_ylabel(\"Prediction\")\nax[1].set_xlabel(\"True\")\nax[1].text(10,6.5,f\"R2 : {r2_xgb:.3f}\",color=\"gray\",fontsize=\"x-large\")\n\nax[2].scatter(y_test,lgm_pred,c=lgm_pred,label=\"LightGBM Regressor\",**plt_params)\nax[2].plot([0,13],[0,13],linestyle=\"--\",color=\"#4f4d4f\",lw=1)\nax[2].set_ylim(6,13);ax[2].set_xlim(6,13)\nax[2].set_title(\"LightGBM Fitting\")\nax[2].set_ylabel(\"Prediction\")\nax[2].set_xlabel(\"True\")\nax[2].text(10,6.5,f\"R2 : {r2_lgm:.3f}\",color=\"gray\",fontsize=\"x-large\")\n\nplt.show()","dc356eb5":"from IPython.display import Image\nImage(\"..\/input\/images\/NN_MLP.png\")","e6bd313c":"from torch.utils.data import DataLoader,TensorDataset\nfrom torch.autograd import Variable","38b64bd4":"Image(\"..\/input\/images\/EarlyStopping.png\")","bc228548":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            #self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","2c072646":"import math\nfrom torch.optim import Optimizer\n\nclass AdaBound(Optimizer):\n    \"\"\"Implements AdaBound algorithm.\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https:\/\/openreview.net\/forum?id=Bkg3g2R9FX\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(\"Invalid final learning rate: {}\".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(\"Invalid gamma parameter: {}\".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBound, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBound, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsbound', False)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsbound = group['amsbound']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsbound:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(grad,alpha = 1 - beta1)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) \/ bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group['final_lr'] * group['lr'] \/ base_lr\n                lower_bound = final_lr * (1 - 1 \/ (group['gamma'] * state['step'] + 1))\n                upper_bound = final_lr * (1 + 1 \/ (group['gamma'] * state['step']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss\n\nclass AdaBoundW(Optimizer):\n    \"\"\"Implements AdaBound algorithm with Decoupled Weight Decay (arxiv.org\/abs\/1711.05101)\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https:\/\/openreview.net\/forum?id=Bkg3g2R9FX\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(\"Invalid final learning rate: {}\".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(\"Invalid gamma parameter: {}\".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBoundW, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBoundW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsbound', False)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsbound = group['amsbound']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsbound:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) \/ bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group['final_lr'] * group['lr'] \/ base_lr\n                lower_bound = final_lr * (1 - 1 \/ (group['gamma'] * state['step'] + 1))\n                upper_bound = final_lr * (1 + 1 \/ (group['gamma'] * state['step']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                if group['weight_decay'] != 0:\n                    decayed_weights = torch.mul(p.data, group['weight_decay'])\n                    p.data.add_(-step_size)\n                    p.data.sub_(decayed_weights)\n                else:\n                    p.data.add_(-step_size)\n\n        return loss","dc88dde9":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","b35dfd01":"TRAINING_MODELS = True\nbatch_size = 2**11\nvalid_batch_size = 2**6\nmodel_path=\".\/models\/\"\n\n\"\"\"\nIncreasing the batch size leads to a reduction in learning time, \nbut on the other hand it causes a decrease in the versatility of the model. \n(Gradient steps become rough)\n\"\"\"\n\n#--datasetting--\nX_train_tensor= torch.from_numpy(X_train.values).float()\ny_train_tensor = torch.from_numpy(y_train.values).float()\nX_valid_tensor= torch.from_numpy(X_valid.values).float()\ny_valid_tensor = torch.from_numpy(y_valid.values).float()\nX_test_tensor = torch.from_numpy(X_test.values).float()\ny_test_tensor = torch.from_numpy(y_test.values).float()\ntest_data, test_label = Variable(X_test_tensor).to(device), Variable(y_test_tensor).to(device)\nN = X_train_tensor.size()[0]\nD = X_train_tensor.size()[1]\n\nif not TRAINING_MODELS:\n    print(\"NON TRAINING MODE.\")\n\n    MODEL_SAVE = False\n    \n    train = TensorDataset(X_train_tensor,y_train_tensor)\n    valid = TensorDataset(X_valid_tensor,y_valid_tensor)\n    minibatch = DataLoader(train,batch_size,shuffle=True)\n    valid_batch = DataLoader(valid,valid_batch_size,shuffle=True)\n    \nelse:\n    print(\"TRAINING MODE.\")\n    \n    MODEL_SAVE = False\n    \n    if MODEL_SAVE:\n        print(\"With the current settings, the training model is saved.\")\n    else:\n        print(\"With the current settings, the training model is NOT saved.\")\n\n    train = TensorDataset(X_train_tensor,y_train_tensor)\n    valid = TensorDataset(X_valid_tensor,y_valid_tensor)\n    minibatch = DataLoader(train,batch_size,shuffle=True)\n    valid_batch = DataLoader(valid,2**4,shuffle=True)","7fd39d15":"print(f\"X training shape : {X_train_tensor.shape}\")\nprint(f\"X valid shape : {X_valid_tensor.shape}\")\nprint(f\"X test shape : {X_test_tensor.shape}\")\nprint(f\"y training shape : {y_train_tensor.shape}\")\nprint(f\"y valid shape : {y_valid_tensor.shape}\")\nprint(f\"y test shape : {y_test_tensor.shape}\")","5636dd09":"def RMSELoss(yhat,y):\n    return torch.sqrt(torch.mean((yhat-y)**2))","8d19d50a":"class Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x *( torch.tanh(F.softplus(x)))\n\n    def backward(self, x):\n        omega = (4*(x + 1) + 4*torch.exp(2*x)\n              + torch.exp(3*x) + (4*x + 6)*torch.exp(x))\n        delta = 2*torch.exp(x) + torch.exp(2*x) + 2\n        return torch.exp(x)*omega\/delta**2","64645c5c":"def score_report(model):\n    model.eval()\n    predict = model(test_data)\n    rmse_ = np.sqrt(mean_squared_error(y_test,predict.to(\"cpu\").detach().numpy()))\n    r2 = r2_score(y_test,predict.to(\"cpu\").detach().numpy())\n    print(f\"RMSE = {rmse_:.4f}, R2 = {r2:.4f}\")\n\n    return rmse_,r2","f8b35cf7":"def training_mlp(model,minibatch,valid_batch,optimizer,lr=0.001,epochs=200,\n                 TRAINING_MODELS=True,MODEL_SAVE=False,model_name=\"temp\",\n                 patience=100):\n\n    if TRAINING_MODELS:\n        model_ = model.to(device)\n        optim = optimizer(model_.parameters(),lr=lr)\n\n        train_losses = []\n        val_losses = []\n        avg_val_losses = []\n\n        early_stopping = EarlyStopping(patience=patience, verbose=True)\n\n        for epoch in tqdm(range(epochs)):\n            total_val_loss = 0\n            total_loss = 0\n            __ = 0\n            model_.train()\n\n            for train_data,train_label in minibatch:\n                train_data,train_label = Variable(train_data).to(device),Variable(train_label).to(device).view(len(train_label),1)\n\n                def closure():\n                    nonlocal total_loss\n                    optim.zero_grad()\n                    output = model_(train_data)\n                    criterion = RMSELoss\n                    loss = criterion(output,train_label)\n                    loss.backward()\n                    total_loss += loss.item()\n                    return loss\n\n                optim.step(closure)\n\n            train_losses.append(total_loss)\n\n            model_.eval()\n\n            with torch.no_grad():\n                for valid_data,valid_label in valid_batch:\n                    valid_data,valid_label = Variable(valid_data).to(device),Variable(valid_label).to(device).view(len(valid_label),1)\n                    output = model_(valid_data)\n                    criterion = RMSELoss\n                    val_loss = criterion(output,valid_label)\n                    total_val_loss += val_loss.item()\n                val_losses.append(total_val_loss) \n\n            if (epoch+1) % 100 == 0:\n                print(f\"{epoch+1} epoch | total_loss is {total_loss:.4f} | val_loss is {total_val_loss:.4f}\")\n\n            early_stopping(total_val_loss, model)\n\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n\n        if MODEL_SAVE:\n            torch.save(model_.state_dict(), model_path+model_name+\".pth\")\n            print(f\"Model seccessfully saved !  Path : {model_path}\")\n        else:\n            print(\"The model is NOT saved.\")\n\n        return model_, train_losses, val_losses\n\n    else:\n        model_ = model.to(device)\n        try:\n            print(\"It is currently set to non-training mode. \\nIf you want to train, switch TRAINING_MODELS = True.\")\n            model_.load_state_dict(torch.load(model_path+model_name+\".pth\"))\n            print(f\"Models seccessfully loaded !  Path : {model_path}\")\n            return model_, 0, 0\n        \n        except:\n            print(f\"ERROR : Failed to load the model. Path : {model_path+model_name+'.pth'}\")\n            return _, _, _","2cc98114":"def plot_history(train_losses,val_losses):\n    plt.figure(figsize=(10,8),dpi=80)\n    plt.title(f\"Fig.{next(fig_count)} training Loss\")\n    plt.plot(np.arange(1,len(train_losses)+1,1),train_losses,label=\"training\",lw=0.8,c=\"#152dcf\")\n    plt.plot(np.arange(1,len(val_losses)+1,1),val_losses,label=\"Validation\",lw=0.8,c=\"#ff3d3d\")\n    plt.ylim(0,150)\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Epoch\")\n\n    plt.legend()\n    plt.show()","fdc00b60":"class MLP(nn.Module):\n    \"\"\"\n    H[N,node] = X[N,D]W[D,node]+B[N,node] \u2192 BN \u2192 Activator \u25cb\n    \"\"\"\n    def __init__(self,N,D,node):\n        super(MLP,self).__init__()\n        self.fc1 = nn.Linear(D,node) # input layer (D is Data Dimension)\n        self.bn1 = nn.BatchNorm1d(node)\n        \n        self.fc2 = nn.Linear(node,node) # Hidden layer1\n        self.bn2 = nn.BatchNorm1d(node)\n        \n        self.fc3 = nn.Linear(node,node) # Hidden layer2\n        self.bn3 = nn.BatchNorm1d(node)\n\n        self.fc4 = nn.Linear(node,node) # Hidden layer3\n        self.bn4 = nn.BatchNorm1d(node)\n        \n        self.fc5 = nn.Linear(node,1) #output layer\n\n    def forward(self,x):\n        activator = Mish()\n\n        x1 = self.fc1(x)# H = X[BatchSize,D]*W[D,HiddenSize]+B[BatchSize,HiddenSize]\n        x1 = self.bn1(x1)\n        x1 = activator(x1)\n        \n        x2 = self.fc2(x1)\n        x2 = self.bn2(x2)\n        x2 = activator(x2)\n        \n        x2 += x1\n        \n        x3 = self.fc3(x2)\n        x3 = self.bn3(x3)\n        x3 = activator(x3)\n        \n        x3 += x1\n    \n        x4 = self.fc4(x3)\n        x4 = self.bn4(x4)\n        x4 = activator(x4)\n        \n        x4 += x1 \n        \n        out = self.fc5(x4) #[BatchSize,1]\n\n        return out","ab0dffe8":"TRAINING_MODELS = True\nMODEL_SAVE=False\n\nepochs = 1500\nnode = 30\nlr = 0.01\nmodel_name = \"MLP\"\npatience = 100\n\nmlp,train_losses, val_losses = training_mlp(MLP(N,D,node=node),\n                                            minibatch,\n                                            valid_batch,\n                                            AdaBound,\n                                            lr=lr,\n                                            epochs=epochs,\n                                            TRAINING_MODELS=TRAINING_MODELS,\n                                            MODEL_SAVE=MODEL_SAVE,\n                                            model_name=model_name,\n                                            patience=patience,\n                                           )","d0c5293e":"rmse_mlp,r2_mlp = score_report(mlp)\nif TRAINING_MODELS:\n    plot_history(train_losses,val_losses)","fab2391b":"r2_summary = [r2_lasso,r2_xgb,r2_lgm,r2_mlp]\ndf_summary = pd.DataFrame({\"R2 score\":r2_summary},\n                           index=(\"Lasso\",\"XgBoost\",\"LightGBM\",\"3-layer MLP\"))\ndf_summary","c5a88a57":"It's not good to put too much subjectivity in the analysis anyway, so I eliminate 2060. ","0322b53b":"Grid search and random search are often used for hyperparameter adjustment, but the disadvantage is that it takes time. Therefore, we will use Optuna developed by PFN. I will omit the detailed algorithm of Optuna, but a kind of Bayesian optimization called TPE (Tree-structured Parzen Estimator) is used.","371ec8f9":"<u><h3><font color=\"#563ac7\">Table and Figure Number<\/font><\/h3><\/u>","61c2ebb5":"$$\n    R^2 = 1-\\frac{MSE}{VAR[y]} = 1-\\frac{\\sum_{n=1}^{N}(y_i-\\hat{y_i})^2}{\\sum_{n=1}^{N}(y_i-\\bar{y_i})^2}\n$$","ccf236be":"<h4><font color=\"#563ac7\">Import Module<font color=\"#563ac7\"><\/h4>","713b68e0":"Next, let's take a look at the 1970 data","e7304416":"<u><h3><font color=\"#563ac7\">Engine Size<\/font><\/h3><\/u>","2f08ed68":"<h3><font color=\"#563ac7\">Import modules<\/font><\/h3>","36ee927e":"<u><h3><font color=\"#563ac7\">Fixed Random Seed<\/font><\/h3><\/u>","a1dd6fcb":"<h4><font color=\"#563ac7\">Data Settings<\/font><\/h4>","9f73f056":"<h2><font color=\"#563ac7\">Prediction Price<\/font><\/h2>\nI tried to predict the selling price of used cars.<br>\nThe techniques I used are LassoRegressor, LightGBM and Multiple Layer Perceptron(MLP).<br>\nThis analysis uses Bayesian optimization with optuna for optimal parameter tuning.","24297740":"<u><h3><font color=\"#563ac7\">Companys count<\/font><\/h3><\/u>","26ca5cb8":"<u><h3><font color=\"#563ac7\">Price Distribution<\/font><\/h3><\/u>","80a6b958":"<u><h3><font color=\"#563ac7\">MPG to KPL<\/font><\/h3><\/u>\nI'm familiar with KPL than MPG, so I'll convert it. The work here is not always necessary.","fe57b74b":"<h4><font color=\"#563ac7\">Learning Curve<\/font><\/h4>","4479357b":"<u><h3><font color=\"#563ac7\">Integration Data<\/font><\/h3><\/u>","17758976":"**<u><h3><font color=\"#563ac7\">Multi Layer Perceptron<\/font><\/h3><\/u>","98c8272e":"First, let's investigate Mercedes. According to wikipedia (https:\/\/en.wikipedia.org\/wiki\/Mercedes-Benz_GLE-Class), the Mercedes Mclass is believed to have been on the market around 1997. From this, 1970 may be a mistake. Also, since it is a diesel vehicle, the displacement of 0L is clearly wrong. <br>\nNext, let's investigate the zafira vauxhall. This vehicle has also been sold by Opel and Vauxhall since 1999, according to https:\/\/en.wikipedia.org\/wiki\/Opel_Zafira. Therefore, 1970 is wrong. Also, it seems that the first Zafira is also called Zafira A, and the displacement lineup is from 1.6L to 2.2L, so I think that the engine Size is also wrong.<br>\nDue to these facts, the 1970 data contains errors and I will eliminated these data.","ccf8ebc3":"<h4><font color=\"#563ac7\">Optimizer<\/font><\/h4>\nAdaBound<br>\nPlease See : <a>https:\/\/github.com\/Luolc\/AdaBound<\/a>\n<pre>\n@inproceedings{Luo2019AdaBound,\n               author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},\n               title = {Adaptive Gradient Methods with Dynamic Bound of Learning Rate},\n               booktitle = {Proceedings of the 7th International Conference on Learning Representations},\n               month = {May},\n               year = {2019},\n               address = {New Orleans, Louisiana}\n}\n<\/pre>","db199188":"$$\n     Mish(x) = xtanh(ln(1+e^x))\n$$","137ca105":"<u><h3><font color=\"#563ac7\">Year count<\/font><\/h3><\/u>\nLooking at the year distribution on the count plot after removing the outliers, most of the data is from 2013 to 2020.","c056078e":"<u><h3><font color=\"#563ac7\">Data Settings<\/font><\/h3><\/u>","42322160":"MLP is based on a simple and powerful theory. After dividing the input data into batches, it is affine-transformed using weight W and bias B. However, instead of using the converted result as it is, Batch Normalization is performed to avoid covariate shift, and Activator is used to calculate to obtain more non-linear expressive power. The resulting matrix is the hidden layer H. After performing this series of operations multiple times, calculate the error from the vector y that you want to finally predict. After calculating the error, adjust the parameters W and B slightly based on the backpropagation method. This is repeated many times, and when the error becomes small, the learning is completed.","bc89e37e":"MLP's R2 score is about the same as LightGBM. Given the training costs, it seems better to use boosting for such regression models. <br>To summarize the results so far, <br>1. LightGBM has the highest R2 score, which is 0.967. <br>2. 3-layer MLP has a higher R2 score than XgBoost. <br>3. The calculation cost of MLP is the highest, and it takes about 30 minutes to learn this data.<br>GoodBye \ud83e\udd9d","7b4f4923":"<u><h3><font color=\"#563ac7\">XgBoost Regression<\/font><\/h3><\/u>","04559440":"<h4><font color=\"#563ac7\">Traning MLP<\/font><\/h4>","80cc4fcd":"<u><h3><font color=\"#563ac7\">Parameter Tuning class by optuna<\/font><\/h3><\/u>","d86d9fef":"In recent years, compact high-power engines using turbochargers have been the mainstream of conventional engines. Looking at the distribution of engine size, most of them are 1.0L to 2.0L.","3480c615":"<h4><font color=\"#563ac7\">Early Stopping<\/font><\/h4>\n\nI quoted Mr.Bjarten's code. (I am forever grateful.)<br>\nPlease See : <a>https:\/\/github.com\/Bjarten\/early-stopping-pytorch<\/a>","38b4a671":"LightGBM attaches great importance to the year and transmission of the vehicle. I think it is a feature evaluation that seems to be convincing. However, XgBoost emphasizes the vehicle model and ignores Year and transmission. I think this is one of the causes of the non-uniformity of the label.","11464934":"<h4><font color=\"#563ac7\">Divice Setting<\/font><\/h4>","e6701adc":"<h4><font color=\"#563ac7\">History plot function<\/font><\/h4>","b5447eec":"Looking at fuelType, it seems that there are a lot of gasoline and diesel in the used car market, and there are few hybrid and electric cars. There may be several reasons, but now hybrids and electric cars are the mainstream, and everyone may be letting go of conventional engines.","7e046935":"<h4><font color=\"#563ac7\">Activator<\/font><\/h4>","f5c12908":"This is a break. LightGBM achieved an R2 score of 0.96 thanks to parameter adjustments. This is not a bad number for me and I think the modeling was a success. Next, let's make a prediction using a 3-layer MLP.","953df29b":"<u><h3><font color=\"#563ac7\">Saving Params.<\/font><\/h3><\/u>","d444a885":"<u><h3><font color=\"#563ac7\">YY-Plot<\/font><\/h3><\/u>","da0decb7":"This is the end of the discussion about the contents of the data. From now on, I will adjust the data for price forecasting and create a model.","3a120183":"<u><h3><font color=\"#563ac7\">LightGBM Regression<\/font><\/h3><\/u>","912308f6":"<h4><font color=\"#563ac7\">Score Report<\/font><\/h4>","2853ce67":"<h4><font color=\"#563ac7\">Loss Function<\/font><\/h4>","54893d8a":"<u><h3><font color=\"#563ac7\">Eliminate year outlier<\/font><\/h3><\/u>\nThere is some interesting data from the boxplot below. It's 1970 and 2060. For 2060, it's probably due to a typo when entering data. For 2060, the workers probably wanted to enter 2016 or 2006. (Some may be curious, but the most expensive 2011 data are Mercedes SL class cars. This car is a luxury car that is classified as a supercar, so it is really expensive.)","ba94dc24":"<h4><font color=\"#563ac7\">Data Shape Confirmation<\/font><\/h4>","0614ef80":"In the UK market, it seems that there are many European and North American manufacturers. On the contrary, are Japanese cars and Korean cars not so popular?","f420a36b":"<u><h3><font color=\"#563ac7\">Parameter tuning<\/font><\/h3><\/u>","1e39864a":"<u><h3><font color=\"#563ac7\">Feature Importance (TOP 10)<\/font><\/h3><\/u>","86c97200":"<h3><font color=\"#563ac7\">Summary<\/font><\/h3>","4b1fb077":"<u><h3><font color=\"#563ac7\">Data loading class<\/font><\/h3><\/u>","243822b3":"<u><h3><font color=\"#563ac7\">Visualization<\/font><\/h3><\/u>","1e256a28":"I have a lot of categorical data and I need to be able to analyze them. There are LabelEncoding and OneHotEncoding, but I choose OneHotEncoding. If you do not want to increase the features, you may proceed with Label Encoding. (Countermeasures against curse of dimensionality)","6d58ea52":"<u><h3><font color=\"#563ac7\">One-Hot-Encoding<\/font><\/h3><\/u>","9ef792cc":"<u><h3><font color=\"#563ac7\">Logarithmic transformation<\/font><\/h3><\/u>\nYou can see that the target(price) distribution is skewed right.\nOne of the effective means to make a good prediction in such a case is to perform logarithmic conversion. Assume a pseudo normal distribution by logarithmic transformation.","8b4ca6aa":"$$\n     RMSE = \\sqrt{\\frac{1}{N}\\sum_{n=1}^{N}(y_i-\\hat{y_i})^2}\n$$","f65061f8":"<u><h3><font color=\"#563ac7\">Fuel Type<\/font><\/h3><\/u>","1fd01cc0":"Numerical data are very different in scale, so it is recommended to match them.","3f4b7429":"<u><h3><font color=\"#563ac7\">Feature Scaling for numerial columns<\/font><\/h3><\/u>","8c661737":"<u><h3><font color=\"#563ac7\">Data Path<\/font><\/h3><\/u>","f54a67aa":"<u><h3><font color=\"#563ac7\">Lasso Regression<\/font><\/h3><\/u>","d5a50a4d":"<u><h3><font color=\"#563ac7\">Loading Params.<\/font><\/h3><\/u>","e289ce27":"<h4><font color=\"#563ac7\">NN TRAINING Function<\/font><\/h4>"}}