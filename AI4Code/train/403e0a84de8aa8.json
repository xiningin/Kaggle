{"cell_type":{"a1edd1ce":"code","35280932":"code","0e0ca20b":"code","a5910b4d":"code","7b670bf5":"code","e4f2ff17":"code","490f647d":"code","ded57537":"code","e07d72ef":"code","b1b95b01":"code","90828474":"code","e47c5e6c":"code","cb528ad9":"code","1bceb089":"code","afa5e9b7":"code","f2cb4957":"code","ab47610e":"code","0fadd388":"code","6626c829":"code","7d46dea9":"code","7b28b03e":"code","5b05526c":"code","036bca69":"code","71fcf03e":"markdown","edab4632":"markdown","fda29999":"markdown","d3eda910":"markdown"},"source":{"a1edd1ce":"import copy\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom skimage import io, transform\nfrom sklearn.metrics import confusion_matrix\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torchvision\nfrom torchvision import transforms, utils, models","35280932":"print(os.listdir(\"..\/input\/honey-bee-annotated-images\/\"))","0e0ca20b":"PATH = '..\/input\/honey-bee-annotated-images\/bee_data.csv'\n# df = dataframe\ndf = pd.read_csv(PATH)\ndf.head()","a5910b4d":"df.subspecies.value_counts()","7b670bf5":"# Return the count of unique values\ndf['health'].value_counts()","e4f2ff17":"df['health'] = df['health'].map({'healthy': 0,\n                                 'few varrao, hive beetles': 1,\n                                 'Varroa, Small Hive Beetles': 2,\n                                 'ant problems': 3,\n                                 'hive being robbed': 4,\n                                 'missing queen': 5})\n\ndf[\"health\"].value_counts()","490f647d":"df.head()","ded57537":"# Plot the graphs wrt different columns\nf, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,10))\n\ndf.subspecies.value_counts().plot(kind='bar',ax=ax[0, 0])\nax[0,0].set_title('Subspecies')\n\ndf.location.value_counts().plot(kind='bar', ax=ax[0, 1])\nax[0,1].set_title('Location')\n\ndf.caste.value_counts().plot(kind='bar', ax=ax[1, 0])\nax[1,0].set_title('Caste')\nax[1,0].set_ylabel('Count')\n\ndf.health.value_counts().plot(kind='bar', ax=ax[1,1])\nax[1,1].set_title('Health')\nax[1,1].set_ylabel('Count')\n\nf.tight_layout()\nplt.show()","e07d72ef":"transform = {'train': transforms.Compose([transforms.Resize(256),\n                                          transforms.CenterCrop(224),\n                                          #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n                                          #transforms.RandomHorizontalFlip(),\n                                          transforms.ToTensor(),\n                                          transforms.Normalize([0.485, 0.456, 0.406],\n                                                               [0.229, 0.224, 0.225])]),\n             \n             'val': transforms.Compose([transforms.Resize(256),\n                                        transforms.CenterCrop(224),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.485, 0.456, 0.406], \n                                                             [0.229, 0.224, 0.225])]),\n            \n             'test': transforms.Compose([transforms.Resize(256),\n                                         transforms.CenterCrop(224),\n                                         transforms.ToTensor(),\n                                         transforms.Normalize([0.485, 0.456, 0.406], \n                                                              [0.229, 0.224, 0.225])])}\n\n# Check for cuda\ndevice = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","b1b95b01":"class HoneyBeeDataset(Dataset):\n    # instance attributes\n    def __init__(self, df, csv_file, root_dir, transform=None):\n        self.data = df\n        self.root_dir = root_dir\n        self.labels = np.asarray(self.data.iloc[:, 6])\n        self.transform = transform\n        \n    # length of the dataset passed to this class method    \n    def __len__(self):\n        return len(self.data)\n    \n    # get the specific image and labels given the index\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 0])\n        image = Image.open(img_name)\n        image = image.convert('RGB')\n        image_label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, image_label","90828474":"dataset = HoneyBeeDataset(df=df,\n                          csv_file=PATH,\n                          root_dir='..\/input\/honey-bee-annotated-images\/bee_imgs\/bee_imgs')","e47c5e6c":"validation_split = 0.2\nte_split = 0.5\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices)\nval_split = int(np.floor(validation_split * dataset_size))\ntest_split = int(np.floor(te_split * val_split))\ntrain_indices = indices[val_split:]\nrest_indices = indices[:val_split]\nval_indices, test_indices = rest_indices[test_split:], rest_indices[:test_split]","cb528ad9":"# Sanity Check\n# Expected outcome: 4138, 517, 517\nlen(train_indices), len(val_indices), len(test_indices)","1bceb089":"dataset_sizes = {'train': len(train_indices), 'val': len(val_indices), 'test': len(test_indices)}\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nvalid_sampler = SubsetRandomSampler(val_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrain_dataset = HoneyBeeDataset(df=df,\n                                csv_file=PATH,\n                                root_dir='..\/input\/honey-bee-annotated-images\/bee_imgs\/bee_imgs',\n                                transform=transform['train'])\n\nval_dataset = HoneyBeeDataset(df=df,\n                              csv_file=PATH,\n                              root_dir='..\/input\/honey-bee-annotated-images\/bee_imgs\/bee_imgs',\n                              transform=transform['val'])\n\ntest_dataset = HoneyBeeDataset(df=df,\n                               csv_file=PATH,\n                               root_dir='..\/input\/honey-bee-annotated-images\/bee_imgs\/bee_imgs',\n                               transform=transform['test'])","afa5e9b7":"dataloaders = {'train' : torch.utils.data.DataLoader(train_dataset, batch_size=4, sampler=train_sampler),\n               'val': torch.utils.data.DataLoader(val_dataset, batch_size=4, sampler=valid_sampler),\n               'test': torch.utils.data.DataLoader(test_dataset, batch_size=1, sampler=test_sampler)}","f2cb4957":"def train_model(model, criterion, optimizer, scheduler, num_epochs):\n    #copy the best model weights\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(\"Epoch: {}\/{}\".format(epoch, num_epochs-1))\n        print(\"=\"*10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0\n            \n            for data in dataloaders[phase]:\n                inputs, labels = data\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase=='train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if phase=='train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item()*inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            \n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n        \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n    \n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n        ","ab47610e":"# Load VGG16 Model\nmodel_pre = models.vgg16()\nmodel_pre.load_state_dict(torch.load(\"..\/input\/vgg16\/vgg16-397923af.pth\"))","0fadd388":"model_pre","6626c829":"# don't calculate gradient since we will use the weights of pretrained model\nfor param in model_pre.features.parameters():\n    param.required_grad = False\n\nnum_features = model_pre.classifier[6].in_features\n# Remove last layer\nfeatures = list(model_pre.classifier.children())[:-1] \n# Add new layer with out_features = len(health_classes)\nfeatures.extend([nn.Linear(num_features, 6)])\n# Replace the model classifier with new classifier\nmodel_pre.classifier = nn.Sequential(*features) \nprint(model_pre.classifier)","7d46dea9":"# load gpu\nmodel_pre = model_pre.to(device)\n# loss function\ncriterion = nn.CrossEntropyLoss()\n# Observe that all parameters are being optimized\noptimizer = torch.optim.SGD(model_pre.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 10 epochs\nexp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","7b28b03e":"EPOCHS = 35\n# train\nmodel_pre = train_model(model_pre, criterion, optimizer, exp_lr_scheduler, num_epochs=EPOCHS)","5b05526c":"def test_model():\n    running_correct = 0\n    running_total = 0\n    true_labels = []\n    pred_labels = []\n    # no gradient calculation\n    with torch.no_grad():\n        for data in dataloaders['test']:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.append(labels.item())\n            outputs = model_pre(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            pred_labels.append(preds.item())\n            running_total += labels.size(0)\n            running_correct += (preds == labels).sum().item()\n    return (true_labels, pred_labels, running_correct, running_total)","036bca69":"true_labels, pred_labels, running_correct, running_total= test_model()\nprint('Correct: {}, Total: {}'.format(running_correct, running_total))\nprint('Test Accuracy: ', (running_correct\/running_total))","71fcf03e":"## Training","edab4632":"## Custom Dataset\n\nCreate a dataset class that is the subclass of *torch.utils.data.Dataset* class.\nOverride __init__, __len__ and __getitem__ methods. ","fda29999":"## Test Model","d3eda910":"## VGG16-Model\n\n\nVgg16 Model has two parts: \n\n1. Features\n2. Classifier\n\nWe are going to use the same weights of the pretrained model so we won't be computing gradients on that part of the model.\nThe last layer of the classifier part of the model has *out_features = 1000* we are going to replace that layer by another layer with *out_features = 4*."}}