{"cell_type":{"4f7c9545":"code","05c55cf1":"code","db43e8a2":"code","f3a7e95a":"code","f106ad7c":"code","eeec746f":"code","1886dcf3":"code","809b42e0":"code","585e9ae9":"code","b4e2334c":"code","2e86acdd":"code","82dfa1da":"code","1203ab44":"code","c8aeaacd":"code","7b989c87":"code","cb86c41b":"code","73a758ef":"code","e058d4f3":"code","db7bf52d":"code","6775fb87":"code","0647af05":"code","1e4ab27f":"code","a6fbb64b":"code","d3f7ca52":"code","748c0fca":"code","a22bb027":"code","0393f122":"code","e891201c":"markdown","ff55071f":"markdown","c621b53f":"markdown"},"source":{"4f7c9545":"from keras.layers import Conv2D, BatchNormalization, LeakyReLU, Dropout, Dense\nfrom keras.layers import MaxPool2D, Input, GlobalAveragePooling2D,Flatten\nfrom keras.models import load_model, Model\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\n","05c55cf1":"import pandas as pd\ndata = pd.read_csv('..\/input\/digit-recognizer\/train.csv')  \n","db43e8a2":"data.head()","f3a7e95a":"labels = data[data.columns[0]]\npixels = data.columns[1:]","f106ad7c":"allImages = np.array(data.loc[:,pixels])\n               ","eeec746f":"print(allImages.shape)","1886dcf3":"allImages = np.reshape(allImages,(allImages.shape[0],28,28,1))","809b42e0":"print(allImages.shape)\nallImages = allImages\/256","585e9ae9":"\nplt.imshow(allImages[1]),print('label: ',labels[1])","b4e2334c":"y=list(labels)\nlen_y = len(y)\nlen_y\ny_onehot = np.zeros((len_y,10))\ny_onehot.shape\nfor i in range(len_y):\n    y_onehot[i,y[i]]=1","2e86acdd":"y[0:5]","82dfa1da":"y_onehot[0:5]","1203ab44":"x = allImages\ny = y_onehot","c8aeaacd":"x.shape","7b989c87":"model = tf.keras.Sequential()\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same', input_shape = (28,28,1)))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.05))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.05))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.05))\n\n\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3),padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha=0.05))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256))\nmodel.add(Dropout(0.5))\n\nmodel.add(LeakyReLU(alpha=0.05))\n\nmodel.add(Dense(10, activation = \"softmax\"))","cb86c41b":"model.summary()","73a758ef":"\nopt = tf.keras.optimizers.Adam(learning_rate=0.008)\nmodel.compile(optimizer = opt , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","e058d4f3":"epoch = 10\nbatch_size = 128\nhistory = model.fit(x, y, validation_split=0.1,batch_size = batch_size, epochs=epoch,shuffle=True)","db7bf52d":"\n\ntrain_loss = history.history['accuracy']\nval_loss   = history.history['val_accuracy']\n\n\nxc = range(10)\n\nplt.figure()\nplt.plot(xc, train_loss)\nplt.plot(xc, val_loss)","6775fb87":"import pandas as pd\ndata_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')  \n","0647af05":"data_test","1e4ab27f":"\npixels_test = data_test.columns[0:]","a6fbb64b":"allImages_test = np.array(data_test.loc[:,pixels_test])\nprint(allImages_test.shape)","d3f7ca52":"allImages_test = np.reshape(allImages_test,(allImages_test.shape[0],28,28,1))\nprint(allImages_test.shape)\nallImages_test = allImages_test\/256","748c0fca":"plt.imshow(allImages_test[0])\nprint(allImages_test[0].shape)","a22bb027":"len(allImages_test), allImages_test.shape","0393f122":"#allImages_test\nfile1 = open(\".\/myResult.txt\",\"w\") \n\n\nfor i in range(len(allImages_test)):\n        image_test = np.expand_dims(allImages_test[i],axis=0)\n        pred = model.predict(image_test)[0]\n        index = np.where(pred == np.max(pred))\n        file1.write(str(i)+\",\"+str(index[0][0]))\n        file1.write(\"\\n\")\n\nfile1.close() \n  ","e891201c":"# Read csv file","ff55071f":"# Add library","c621b53f":"# convert labels to one-hot encoding"}}