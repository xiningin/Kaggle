{"cell_type":{"c1b6a0d6":"code","a6ddfbe9":"code","658c8207":"code","57972379":"code","2dfff02a":"code","fbfb772d":"code","f8c8af5b":"code","97a67fb9":"code","229a8d7a":"code","5a8dbf36":"code","0c373568":"code","225e13fe":"code","31cc4d2e":"code","21ffb279":"code","63b4b642":"code","72ba2d1d":"code","09edaa78":"code","fb54ae36":"code","e49c52fd":"code","677685a4":"code","e5f7b586":"code","e1283ff1":"code","69b7f79e":"code","3c773f1c":"code","aa6874e1":"code","8dd9e477":"code","63d49553":"code","05cfa502":"code","e6d98f53":"code","9fffaed7":"code","20f6a43d":"code","5182373a":"code","dd2f012c":"code","d6bf6fc8":"code","f9e14278":"code","20b510b9":"code","ff5fae3a":"code","f2e77879":"code","46d3bd04":"code","a7437fa6":"code","4eed1172":"code","91361b6f":"code","5f335131":"code","7658e3c8":"code","6aec2bfd":"code","a0f08c13":"code","46067641":"code","74e94e36":"code","9605fc59":"code","0d1e72e3":"code","077bf19d":"code","8d8de2d6":"code","b51785e0":"code","fda85d15":"code","52ed30cf":"code","bce5474d":"code","846c0be3":"code","b9d1ae9a":"code","b63fd06e":"code","41a25378":"code","e7ae9b0a":"code","0d5b3afe":"code","49b8ebc8":"code","b6b33f6d":"code","19f718f8":"code","c4c7bb23":"code","9e1952f9":"code","fff7d0d2":"code","740499c3":"markdown","8ede33e0":"markdown","2d2ec62f":"markdown","7c9681ef":"markdown","d70480fd":"markdown","ee1b189a":"markdown","58a47e55":"markdown","93721eb3":"markdown","2fe2b17b":"markdown","c3d52e67":"markdown","06dd5fe8":"markdown","9ddcf2a2":"markdown","2e6e3ac1":"markdown","80ff2b22":"markdown","bbe8abba":"markdown","97f8db96":"markdown","6c448884":"markdown","39750ecb":"markdown","5adce724":"markdown","624b0b76":"markdown","b15010bf":"markdown","90b482c2":"markdown","088dff7d":"markdown"},"source":{"c1b6a0d6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom time import clock, time\nfrom datetime import datetime\n\nfrom sklearn import metrics  \nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras.models import Sequential, load_model, model_from_json\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\n\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import scale, label_binarize\nfrom sklearn.svm import SVC","a6ddfbe9":"plt.rcParams[\"font.size\"] = 30\nplt.rcParams['figure.figsize'] = [12, 8]\nplt.rcParams['figure.titlesize'] = 20\nplt.rcParams[\"axes.titlesize\"] = 20\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams[\"xtick.labelsize\"] = 20\nplt.rcParams[\"ytick.labelsize\"] = 20\nplt.rcParams[\"legend.fontsize\"] = 10","658c8207":"BASE_DIR = \".\"\nDEBUG = False\n\nnp.random.seed(42)","57972379":"df=pd.read_csv('..\/input\/train.csv')\nX_test=pd.read_csv('..\/input\/test.csv')","2dfff02a":"df.head()","fbfb772d":"df.target.value_counts()","f8c8af5b":"df.shape","97a67fb9":"df['var_139'].hist()","229a8d7a":"df.head(2)","5a8dbf36":"X = df.loc[:,'var_0':]\ny = df[\"target\"]","0c373568":"#X, X_hideout, y, y_hideout = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)","225e13fe":"#df=X.join(y)","31cc4d2e":"df.head(2)","21ffb279":"# Class count\ncount_class_0, count_class_1 = df.target.value_counts()\n\n# Divide by class\ndf_class_0 = df[df['target'] == 0]\ndf_class_1 = df[df['target'] == 1]","63b4b642":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_sampled = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_sampled.target.value_counts())\n\ndf_sampled.target.value_counts().plot(kind='bar', title='Count (target)');","72ba2d1d":"df['var_81'].hist()","09edaa78":"df.describe()","fb54ae36":"df.info()","e49c52fd":"df_sampled.target.value_counts()","677685a4":"df_sampled.corr()","e5f7b586":"sns.heatmap(df_sampled.corr())","e1283ff1":"df_sampled1=df_sampled.drop('target', axis=1)","69b7f79e":"df_sampled.shape","3c773f1c":"df_sampled1.head(2)","aa6874e1":"from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import StratifiedShuffleSplit","8dd9e477":"X = df_sampled.loc[:,'var_0':]\ny = df_sampled[\"target\"]","63d49553":"X.shape, y.shape","05cfa502":"df_sampled[\"target\"].value_counts()","e6d98f53":"import skopt\n\nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\nfrom skopt.callbacks import VerboseCallback # Callback to control the verbosity\nfrom skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\n\nimport pprint\n","9fffaed7":"import datetime as dt\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit","20f6a43d":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title, callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    if callbacks:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n    best_score = optimizer.best_score_\n    best_score_std = optimizer.cv_results_['std_test_score'][optimizer.best_index_]\n    best_params = optimizer.best_params_\n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           +u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                  len(optimizer.cv_results_['params']),\n                                  best_score,\n                                  best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","5182373a":"# Setting a 5-fold stratified cross-validation (note: shuffle=True)\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)","dd2f012c":"# Converting average precision score into a scorer suitable for model selection\navg_prec = make_scorer(average_precision_score, greater_is_better=True, needs_proba=True)","d6bf6fc8":"df_sampled1.head(2)","f9e14278":"clf = lgb.LGBMClassifier(boosting_type='gbdt',\n                         class_weight='balanced',\n                         objective='binary',\n                         n_jobs=1, \n                         verbose=0)\n\nsearch_spaces = {\n        'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n        'num_leaves': Integer(2, 50),\n        'max_depth': Integer(0, 20),\n        'min_child_samples': Integer(0, 200),\n        'max_bin': Integer(100, 100000),\n        'subsample': Real(0.01, 1.0, 'uniform'),\n        'subsample_freq': Integer(0, 10),\n        'colsample_bytree': Real(0.01, 1.0, 'uniform'),\n        'min_child_weight': Integer(0, 10),\n        'subsample_for_bin': Integer(100000, 500000),\n        'reg_lambda': Real(1e-9, 1000, 'log-uniform'),\n        'reg_alpha': Real(1e-9, 1.0, 'log-uniform'),\n        'scale_pos_weight': Real(1e-6, 500, 'log-uniform'),\n        'n_estimators': Integer(500, 2000)        \n        }\n\nopt = BayesSearchCV(clf,\n                    search_spaces,\n                    scoring=avg_prec,\n                    cv=skf,\n                    n_iter=40,\n                    n_jobs=-1,\n                    return_train_score=False,\n                    refit=True,\n                    optimizer_kwargs={'base_estimator': 'GP'},\n                    random_state=22)\n    \nbest_params = report_perf(opt, X, y,'LightGBM', \n                          callbacks=[DeltaXStopper(0.001), \n                                     DeadlineStopper(60*5)])\n#DeadlineStopper and DeltaXStopper are skopt callbacks that control the total time spent and \n#the improvement of a BayesSearchCV (in our implementation to be called with report_perf, using the parameter callbacks=[]).","20b510b9":"# this is the first method using the bayesian optimizer fit classifier, we will try also with seperate classifier\n#which will give a LB %89.7 at 10K fold\n\nIndex=X_test['ID_code']\nX_test=X_test.drop('ID_code', axis=1)","ff5fae3a":"sub_preds=opt.predict_proba(X_test)[:,1]","f2e77879":"submissions=pd.DataFrame({'ID_code':Index, 'target': sub_preds})","46d3bd04":"submissions.to_csv('submissions_lgb_samplingFEtuningbest1.csv', index=False)","a7437fa6":"val_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(X_test.shape[0])\nkf = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\n\nfor i, (train_index, test_index) in enumerate(kf.split(X, y)):\n    # Create stratified 5\/10 CV data\n    X_train, y_train = X.iloc[train_index].copy(), y.iloc[train_index]\n    X_valid, y_valid = X.iloc[test_index].copy(), y.iloc[test_index].copy()\n    print(\"\\nFold \", i)\n#Parameters defined by Bayesian Optimizer https:\/\/www.kaggle.com\/lucamassaron\/kaggle-days-paris-gbdt-workshop\n    clf = LGBMClassifier(\n            n_estimators=1704,\n            learning_rate=0.16624226726409647, \n            num_leaves=4,\n            colsample_bytree=.8,\n            subsample=.7,\n            subsample_freq= 7,\n            subsample_for_bin= 375140,\n            max_depth=5,\n            reg_alpha=1.081049236893711e-05,\n            reg_lambda=1.043686239159047,\n            min_split_gain=.01,\n            min_child_weight=4,\n            min_child_samples=22,\n            silent=-1,\n            verbose=-1,\n        )\n        \n    clf.fit(X_train, y_train, \n                eval_set= [(X_train, y_train), (X_valid, y_valid)], \n                eval_metric='auc', verbose=100, early_stopping_rounds=100\n               )\n            \n    val_preds[test_index] = clf.predict_proba(X_valid, num_iteration=clf.best_iteration_)[:, 1]\n    sub_preds += clf.predict_proba(X_test.loc[:,'var_0':], num_iteration=clf.best_iteration_)[:, 1] \/ kf.n_splits","4eed1172":"val_preds[test_index] = clf.predict_proba(X_valid, num_iteration=clf.best_iteration_)[:,1]","91361b6f":"predictions_lgbm_01 = np.where(val_preds[test_index] > 0.5, 1, 0)","5f335131":"from sklearn.metrics import f1_score\n\nlgb_F1 = f1_score(y_valid, predictions_lgbm_01, average = 'weighted')\nprint(\"The Light GBM F1 for Validation Data is\", lgb_F1)","7658e3c8":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_valid,predictions_lgbm_01)","6aec2bfd":"predictions_lgbm_01.shape, y_valid.shape","a0f08c13":"clf.best_score_","46067641":"#Print Confusion Matrix\nplt.figure()\ncm = confusion_matrix(y_valid, predictions_lgbm_01)\nlabels = ['0', '1']\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nplt.show()","74e94e36":"#hide_out_preds= clf.predict_proba(X_hideout, num_iteration=clf.best_iteration_)[:, 1]","9605fc59":"#predictions_lgbm_02 = np.where(hide_out_preds > 0.5, 1, 0)","0d1e72e3":"#lgb_F1 = f1_score(y_hideout, predictions_lgbm_02, average = 'weighted')\n#print(\"The Light GBM F1 on Hideout Data is\", lgb_F1)","077bf19d":"#from sklearn.metrics import roc_auc_score\n#roc_auc_score(y_hideout,sub_preds)","8d8de2d6":"#Plot Variable Importances\n#lgb.plot_importance(clf, max_num_features=21, importance_type='gain')","b51785e0":"#X_test=pd.read_csv('test.csv')","fda85d15":"X_test.head(2)","52ed30cf":"Index=X_test['ID_code']\nX_test=X_test.drop('ID_code', axis=1)","bce5474d":"#test_data_preds= clf.predict_proba(X_test, num_iteration=clf.best_iteration_)[:, 1]","846c0be3":"#predictions_lgbm_03 = np.where(test_data_preds > 0.5, 1, 0)","b9d1ae9a":"submissions=pd.DataFrame({'ID_code':Index, 'target': sub_preds})\n# Subpreds from the k-fold iteration above","b63fd06e":"submissions.to_csv('submissions_lgb_samplingFEtuningbest2.csv', index=False)","41a25378":"#train_df=df_sampled1.copy()\n#test_df=X_test.copy()","e7ae9b0a":"df_sampled2=df_sampled1.drop('ID_code', axis=1)\ndf_sampled2.head(2)","0d5b3afe":"# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","49b8ebc8":"boosting = [\"goss\",\"dart\"]\ndef kfold_lightgbm(train_df, test_df, num_folds, stratified = False, boosting = boosting[0]):\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=2045)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n    \n    # k-fold\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, df_sampled[\"target\"])):\n        train_x, train_y = train_df.iloc[train_idx], df_sampled[\"target\"].iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], df_sampled[\"target\"].iloc[valid_idx]\n\n        # set data structure\n        lgb_train = lgb.Dataset(train_x,label=train_y,free_raw_data=False)\n        lgb_test = lgb.Dataset(valid_x,label=valid_y,free_raw_data=False)\n\n        # params optimized by optuna\n        params ={\n                        'task': 'train',\n                        'boosting': 'goss',\n                        'objective': 'binary',\n                        'metric': 'auc',\n                        'learning_rate': 0.01,\n                        'subsample': 0.8,\n                        'max_depth': -1,\n                        'top_rate': 0.9064148448434349,\n                        'num_leaves': 32,\n                        'min_child_weight': 41.9612869171337,\n                        'other_rate': 0.0721768246018207,\n                        'reg_alpha': 9.677537745007898,\n                        'colsample_bytree': 0.5665320670155495,\n                        'min_split_gain': 9.820197773625843,\n                        'reg_lambda': 8.2532317400459,\n                        'min_data_in_leaf': 21,\n                        'verbose': -1,\n                        'seed':int(2**n_fold),\n                        'bagging_seed':int(2**n_fold),\n                        'drop_seed':int(2**n_fold)\n                        }\n\n        reg = lgb.train(\n                        params,\n                        lgb_train,\n                        valid_sets=[lgb_train, lgb_test],\n                        valid_names=['train', 'test'],\n                        num_boost_round=7000,early_stopping_rounds= 200,\n                        verbose_eval=100,\n                        )\n\n        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n        sub_preds += reg.predict(test_df, num_iteration=reg.best_iteration) \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = train_x.columns\n        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d roc_auc_score : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del reg, train_x, train_y, valid_x, valid_y\n        #gc.collect()\n\n    # display importances\n    display_importances(feature_importance_df)\n    \n        # save submission file\n    submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\n    submission['target'] = sub_preds\n    submission.to_csv(boosting+\".csv\", index=False)\n    display(submission.head())\n    return (submission)","b6b33f6d":"kfold_lightgbm(df_sampled2, X_test, num_folds=5, stratified = True, boosting = boosting[0])","19f718f8":"#folds = StratifiedKFold(n_splits= 5, shuffle=True, random_state=326)\n","c4c7bb23":"#folds.n_splits","9e1952f9":"# Save the model\nimport joblib\n#save model\njoblib.dump(lgb, 'lgb_train model')","fff7d0d2":"submissions_stacked","740499c3":"lets seperate target and features","8ede33e0":"### TEST DATA PREDICTIONS","2d2ec62f":"### BAGGING","7c9681ef":"## Regularized Random Forest Classifier","d70480fd":"## 3. feature selection, removing unnecessary features","ee1b189a":"### STACKING","58a47e55":"### CV 5 fold, then model apply","93721eb3":"### Hideout Data with Light GBM","2fe2b17b":"# KNN","c3d52e67":"### LIGHT GBM DIFFERENT APPROACH (% 89.7 Leaderboard Score)\n#### https:\/\/www.kaggle.com\/ashishpatel26\/imbalance-class-problem-solved-lightgbm","06dd5fe8":"### Bayesian Optimizing XGBOOST","9ddcf2a2":"## CATBOOST IMPLEMENTATION","2e6e3ac1":"## Lets do some exploration and feature engineering","80ff2b22":"### We will use these parameters for our Classifier (% 89.8 LB Score)","bbe8abba":"### BAYESIAN OPTIMIZATION https:\/\/www.kaggle.com\/lucamassaron\/kaggle-days-paris-gbdt-workshop","97f8db96":"### TEST DATA PREDICTIONS","6c448884":"#### Save the Model","39750ecb":"##  Bayesian Optimization, Model Implementation","5adce724":"### Hideout data","624b0b76":"### Stochastic Gradient Descent","b15010bf":"### XGBOOST (% 84.5 LB SCORE)","90b482c2":"### binary_logloss for Validation Data': 0.314","088dff7d":"### FEATURE ENGINEERING"}}