{"cell_type":{"280c2c10":"code","09f5249a":"code","923f37fd":"code","5520fadf":"code","ede6deff":"code","91d80078":"code","2ec11af9":"code","79f981dc":"code","05569752":"code","c64591b7":"code","10888f18":"code","3c6603bc":"code","04a8e34c":"code","677098d6":"code","db5f3adb":"code","f6b2b27d":"code","cc5cd1df":"code","f574b96e":"code","023a178e":"code","c52b4118":"code","1862f6cd":"code","5e792dbb":"code","4f2a5509":"code","a694f598":"code","f0f193f3":"code","9ad4f6d5":"code","c046ae64":"code","18c3223a":"code","f604af21":"markdown","353721eb":"markdown","65aa798f":"markdown","dea4f442":"markdown","9e92f868":"markdown","04fb171b":"markdown","103e407e":"markdown","a1714f8d":"markdown","113054ce":"markdown","31121a13":"markdown","f7447653":"markdown","17fac483":"markdown","7668f817":"markdown","c3dfde76":"markdown"},"source":{"280c2c10":"import tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd \nimport numpy as np \n\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, LabelEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.model_selection import train_test_split  \n","09f5249a":"'''\n\n# I had to get rid of single quote marks in the csv\n# 'breast' had a space (so I removed all spaces)\n\ntext = open('\/content\/drive\/MyDrive\/Colab Notebooks\/breast_cancer_data\/breast-cancer-1.csv', \"r\")\n\ntext = ''.join([i for i in text]).replace(\"'\", \"\").replace(\" \",\"\")\n\nx = open(\"\/content\/drive\/MyDrive\/Colab Notebooks\/breast_cancer_data\/breast-cancer-3.csv\",\"w\")\nx.writelines(text)\n\n\n'''","923f37fd":"df = pd.read_csv('..\/input\/breastcancer3\/breast-cancer-3.csv')\n\ndf.head(10)","5520fadf":"###\n###\n###\n# I'm going to see if I can help this subpar model (despite that has imbalanced classes) by reducing class imbalance.\n###\n###\n###\n\n","ede6deff":"# delete rows with NaN in them\n\ndf = df.dropna()\n\n'''\nSee also:\nDataFrame.isna: Indicate missing values.\nDataFrame.notna : Indicate existing (non-missing) values.\nDataFrame.fillna : Replace missing values.\nSeries.dropna : Drop missing values.\nIndex.dropna : Drop missing indices.'''\n\n# NaN was in row 20, now row 20 is gone, see...\nprint(df.iloc[19:22])\n\n\nprint(df)\n\n'''\n# the other thing we could do is replace NaNs with mean nums... \n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\ndf = imputer.fit_transform(df.values)\n'''\n","91d80078":"def remove_rows(dataframe_name, col, col_var_to_reduce, target_num_to_remain=0):\n  '''\n  col and col_var_to_reduce are strings\n  '''\n  print(col, col_var_to_reduce)\n\n  get_idxs_of_col_var_to_reduce = dataframe_name.index[ dataframe_name[col]==col_var_to_reduce ].tolist()\n\n  get_idxs_of_col_var_to_reduce = np.asarray(get_idxs_of_col_var_to_reduce) # convert list to np array \n\n  print(len(get_idxs_of_col_var_to_reduce)) \n\n  # get a random sample of unique idxs from get_idxs_of_col_var_to_reduce array\n\n  np.random.seed(42) # set random seed for reproducible results\n\n  idxs_to_drop_from_df = np.random.choice(get_idxs_of_col_var_to_reduce, size=len(get_idxs_of_col_var_to_reduce)-target_num_to_remain, replace=False)\n\n  print(idxs_to_drop_from_df)\n\n  # drop these idxs from df\n\n  dataframe_name = dataframe_name.drop(idxs_to_drop_from_df, axis=0, inplace=True) # inplace=True mutates ori df\n","2ec11af9":"remove_rows(dataframe_name=df, col='breast-quad', col_var_to_reduce='left_low', target_num_to_remain=20)\n\nremove_rows(dataframe_name=df, col='breast-quad', col_var_to_reduce='left_up', target_num_to_remain=20)\n\nremove_rows(dataframe_name=df, col='breast-quad', col_var_to_reduce='right_up', target_num_to_remain=20)\n\nremove_rows(dataframe_name=df, col='breast-quad', col_var_to_reduce='right_low', target_num_to_remain=20)\n\nremove_rows(dataframe_name=df, col='breast-quad', col_var_to_reduce='central', target_num_to_remain=20)\n","79f981dc":"df\n\n# note that now we have a smaller dataset BUT with equal num of class samples","05569752":"# prep column_transformer\n# in this instance, we will encode all X variables with OHE (using OE throws an error)\n\nct = make_column_transformer(\n    (OneHotEncoder(handle_unknown='ignore', sparse=False), ['age', 'tumor-size', 'inv-nodes', 'menopause', 'node-caps','deg-malig', 'breast', 'irradiat', 'class'])\n)\n\n\n","c64591b7":"# encode the dependent variable, y\n\n'''\nle = LabelEncoder()\ny = df['breast-quad']\ny = le.fit_transform(y)\n!!! Got better val_loss result when I used OHE on the y variable instead of LE\n'''\n\nohe = OneHotEncoder(sparse=False) # sparse=False or we will get a non-tensor shape\n\ny = df[['breast-quad']] # add double square brackets or we'll get a shape error\n\ny = ohe.fit_transform(y)\n\ny[:10], y.shape","10888f18":"# define X \n\nX = df.drop('breast-quad', axis='columns')\n\nX, y, X.shape, y.shape","3c6603bc":"# get training, validation and test datasets (90:5:5 ratio)\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n \nprint(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n# (193, 9) (193,) (84, 9) (84,)\n \nX_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n \nprint(X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n# (42, 9) (42,) (42, 9) (42,)\n\nprint('X_train: ' + str(X_train))\n \nprint('y_train: ' + str(y_train))\n \nprint('X_val: ' + str(X_val))\n \nprint('y_val: ' + str(y_val))\n \nprint('X_test: ' + str(X_test))\n \nprint('y_test: ' + str(y_test))\n\n","04a8e34c":"X_train","677098d6":"# fit and transform the training data\n\nct.fit(X_train)\n\nX_train_normal = ct.transform(X_train)\nX_val_normal = ct.transform(X_val)\nX_test_normal = ct.transform(X_test)\n\nprint(X_train_normal.shape, X_val_normal.shape, X_test_normal.shape)\nprint(X_train_normal[0])\nprint(type(X_train_normal))","db5f3adb":"# data preprocessing is done, all training data have been encoded to a 0 or 1\n# now we can build the model","f6b2b27d":"tf.random.set_seed(42)\n\nmodel_1 = keras.Sequential([\n                            keras.layers.Dense(50, activation='relu'),  \n                            keras.layers.Dropout(0.2),  \n                            keras.layers.Dense(30, activation='relu'),\n                            keras.layers.Dense(5, activation='softmax'),\n\n])\n","cc5cd1df":"model_1.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics='accuracy')\n\n'''\nBoth, categorical cross entropy and sparse categorical cross entropy have the same loss function which you have mentioned above. The only difference is the format in which you mention \ud835\udc4c\ud835\udc56 (i,e true labels).\nIf your \ud835\udc4c\ud835\udc56's are one-hot encoded, use categorical_crossentropy. Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\nBut if your \ud835\udc4c\ud835\udc56's are integers, use sparse_categorical_crossentropy. Examples for above 3-class classification problem: [1] , [2], [3]\nThe usage entirely depends on how you load your dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n'''","f574b96e":"history_1 = model_1.fit(\n    X_train_normal,\n    y_train,\n    epochs=6,\n    validation_data=(X_val_normal, y_val),\n    verbose=2,\n)","023a178e":"# IMPROVEMENT LOG # val_loss: 1.5041 - val_accuracy: 0.4286 , when train-val-test ratio was (70:15:15)\n# HIGHEST SCORE TO DATE: val_loss: 1.3646 - val_accuracy: 0.5357, train-val-test ratio revised to (80:10:10)","c52b4118":"# loss curve\n\nimport matplotlib.pyplot as plt\n\npd.DataFrame(history_1.history).plot(figsize=(10,7))\nplt.xlabel('epochs')","1862f6cd":"\n# make a prediction with X_test_normal\n\ny_pred = model_1.predict(X_test_normal)\n\nprint(y_pred)\n\n# now we clean up y_pred: create one list with the index of largest num in each y_pred result\/row \n\ny_pred = y_pred.argmax(axis=-1)\n\nprint(y_pred)\n\n\n# get the actual results of y_test (pre-encoded)\n\nactual_y_test = ohe.inverse_transform(y_test)\n\nprint(y_test)\n\n# reshaped to a 2D tensor for easier comparison\n\nactual_y_test = actual_y_test.ravel()\n\nprint(actual_y_test)\n\n\n# convert y_pred to string labels to compare with actual_y_test. First we manually enter classes by index\n\nclasses_by_index = ['central', 'left-low', 'left-up', 'right-low', 'right-up']\n\n# then we append each prediction into a list\n\ny_pred_breast_quad = []\n\nfor i in y_pred:\n  y_pred_breast_quad.append(classes_by_index[i])\n\n# we convert the list into a numpy array and then into a 2D tensor so we can compare with actual_y_test\n\n#y_pred_breast_quad = np.asarray(y_pred_breast_quad).reshape(4,7)\n\nprint(y_pred_breast_quad)\n","5e792dbb":"# evaluate val and test outcomes\n\nmodel_1.evaluate(X_val_normal, y_val), model_1.evaluate(X_test_normal, y_test)\n","4f2a5509":"# Check for \"class imbalance\" (our class is breast-quad)\n\nc = df.iloc[:, 7:8]\n\nc.value_counts()\n'''\nbreast-quad\nright_up       20\nright_low      20\nleft_up        20\nleft_low       20\ncentral        20\n'''\n# NO CLASS IMBALANCE bc we dropped rows using our remove_rows function","a694f598":"'''le = LabelEncoder()\n\ninput_labels = ['red', 'black', 'red', 'green', 'black', 'yellow', 'white']\n\nle.fit(input_labels)\n\n\n\ntest_labels = ['green', 'yellow', 'black']\n\nencoded_values = le.transform(test_labels)\n\nprint(encoded_values)\n\n\n\ndecoded_list = le.inverse_transform(encoded_values)\n\nprint(decoded_list)'''","f0f193f3":"'''# THE AVERAGE TUMOR SIZE FROM ENCODED DATA IS... \n\nprint(X_train['tumor-size'].mean())\n# NB we don't use the normalised X_train \n\nprint()\n\n\n# GET THE PRE-ENCODED DATA SO WE CAN FIND OUT WHAT AVERAGE TUMOR SIZE FROM ENCODED DATA IS\n\n# pre_encoded = OrdinalEncoder().fit(THE ORIGINAL PRE-ENCODED COLUMN IN 2D ARRAY).inverse_transform(THE POST-ENCODED COLUMN ALSO IN 2D ARRAY)\n\n\n# get pre_encoded col\n\npre_encoded = df['tumor-size']\n\nprint(pre_encoded.shape) # (277,) shape is not usable, so... \n\nreshaped_pre_encoded = pre_encoded.to_numpy().reshape(-1,1)\n\nprint(reshaped_pre_encoded.shape) # shape is now (277, 1)\n\n\n# now get post_encoded col\n\npost_encoded = X.iloc[:, 2:3]\n\nprint(post_encoded.head(15), '\\n', type(post_encoded), '\\n', post_encoded.shape) # shape is (277, 1)\n\n\n# now we can inverse transform\n\npre_encoded_result = OrdinalEncoder().fit(reshaped_pre_encoded).inverse_transform(post_encoded)\n\nprint(pre_encoded_result[:15], '\\n', type(pre_encoded_result))\n\n# print(len(df['tumor-size']), len(pre_encoded_result)) # both are 277\n\n\n# FROM THE ABOVE WE CAN CONCLUDE THAT...\n\n# from the mean of 4 (3.98) for tumor-size,\n# and from comparing pre- and post-encoded nums,\n# mean of 4 (3.98) for tumor-size == ['25-29']\n# which means the average tumor size is 25 to 29 (mm?)\n\n\n# OK so the funny thing is that if we just print reshaped_pre_encoded we will get the pre-encoded data from idx 0\n# without having to go thru all the rest of the code!!!\nprint(reshaped_pre_encoded[:10])\n# so we can do this next time :-)\n'''","9ad4f6d5":"'''\n# TURN SERIES WITH VALUE COUNTS INTO NEW, USABLE DATAFRAME\n\nseries = df.value_counts().to_frame()\n# note that a is a pandas series, which is what value_counts() creates, \n# while to_frame() turns it into a dataframe with shape of (n, 1)\n\nprint(series)\n\nprint(series.shape) # (263, 1)\n\nprint()\n\ncounts_only = tf.constant(series)\n\nprint(counts_only[:3]) # see first 3\n\nprint()\n\ncounts_only = np.ravel(counts_only) # convert counts_only shape to 1D array (n, )\n\nprint(counts_only, counts_only.shape)\n\nprint()\n\nprint(series.index[0], series.index[1], series.index[2]) # we get tuples of each row!\n\nprint()\n\ntemp_list = []\n\nfor i in range(len(series)):\n  each_list = list(series.index[i]) # convert each tuple into a list\n  each_count = counts_only[i] # get each count\n  each_list.append(each_count) # add each_count to each_list list\n  temp_list.append(each_list) # add each_list to temp_list\n\nstacked_arr = np.row_stack( ( temp_list ) )\n\nprint(stacked_arr, stacked_arr.shape)\n'''","c046ae64":"'''# we onehotencode these rows with pandas get_dummies(): breast, breast-quad\n\ndf = pd.get_dummies(data=df, columns=['breast', 'breast-quad'])\n\ndf'''","18c3223a":"def remove_rows(dataframe_name, col, col_var_to_reduce, target_num_to_remain=0):\n  '''\n  col and col_var_to_reduce are strings\n  '''\n  print(col, col_var_to_reduce)\n\n  get_idxs_of_col_var_to_reduce = dataframe_name.index[ dataframe_name[col]==col_var_to_reduce ].tolist()\n\n  get_idxs_of_col_var_to_reduce = np.asarray(get_idxs_of_col_var_to_reduce) # convert list to np array \n\n  print(len(get_idxs_of_col_var_to_reduce)) \n\n  # get a random sample of unique idxs from get_idxs_of_col_var_to_reduce array\n\n  np.random.seed(42) # set random seed for reproducible results\n\n  idxs_to_drop_from_df = np.random.choice(get_idxs_of_col_var_to_reduce, size=len(get_idxs_of_col_var_to_reduce)-target_num_to_remain, replace=False)\n\n  print(idxs_to_drop_from_df)\n\n  # drop these idxs from df\n\n  dataframe_name = dataframe_name.drop(idxs_to_drop_from_df, axis=0, inplace=True) # inplace=True mutates ori df\n\n\n###\n\n# run function...\n\n#remove_rows(dataframe_name=df, col='breast-quad', col_var_to_reduce='left_low', target_num_to_remain=20)\n\n\n\n\n\n\n'''\n# SOURCE CODE FOR FUNCTION\n\nleft_low_idx = df.index[ df['breast-quad']=='left_low'].tolist()\n\nleft_low_idx = np.asarray(left_low_idx) # convert list to np array \n\nprint(len(left_low_idx)) # 106, same as val count output\n\nprint(left_low_idx)\n\n# let's reduce all target vars count to N\n# so let's pick N idx numbers to delete from the dataframe, based on the idx nums of left_low rows\n\ntarget_num_to_remain = 20\n\nnp.random.seed(42)\n\n\n# get a random sample of unique idxs only from left_low_idx\n\nidxs_to_drop_from_df = np.random.choice(left_low_idx, size=len(left_low_idx)-target_num_to_remain, replace=False)\n\nprint(idxs_to_drop_from_df, len(idxs_to_drop_from_df))\n\n\n# drop these idxs from df\n\ndf = df.drop(idxs_to_drop_from_df, axis=0, inplace=True)\n\nprint(df)\n'''\n\n\n###\n\n\n\n\n","f604af21":"# DEFINE X \n","353721eb":"## remove_rows function","65aa798f":" # DEFINE AND ENCODE Y","dea4f442":"# SCRIPT DUMP","9e92f868":"# EVALUATE MODEL","04fb171b":"# IMPORT LIBRARIES AND PREPROCESS DATA","103e407e":"# FINDINGS \n\n- I get 2 out of 5 correct, testing with unseen data, with this model. This is not a small victory for me as prev models would get 8\/28 right ie 28.6% whereas now I can achieve 40% accuracy.\n- Remember that this is a very small sample of only 100! \n- Best of all, IMHO, this model is more 'creative' with preds ie [3 4 1 0 2] rather than [1 1 2 1 1 1 1 1 1 1 2 2 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1] previously!\n- By **(1) radically fixing class imbalance** (row reduction from 277 to 100) and\n- **(2) adding a lot more units per hidden layer** and \n- **(3) adding a Dropout layer**, I was able to improve the model\n","a1714f8d":"# PREP COLUMN_TRANSFORMER","113054ce":"Excellent article on class imbalance https:\/\/journalofbigdata.springeropen.com\/articles\/10.1186\/s40537-019-0192-5 ","31121a13":"# MAKE A PREDICTION","f7447653":"# DEFINE TRAINING, VALIDATION AND TEST DATASETS\n- I increased the X_train y_train set to 90% to get better results when I fit the model, ie 60% val accuracy ","17fac483":"# BUILD MODEL\n- By adding a lot more hidden units (neurons) eg from 10 to 50, and by adding a Dropout layer to help avoid overfitting, I was able to increase the val accuracy from 50% to 60%","7668f817":"## FUNCTION TO REMOVE ROWS IN COLUMNS THAT MEET CRITERIUM","c3dfde76":"LET'S BUILD A MULTICLASS CLASSIFICATION MODEL \n\nWe use breast cancer data to predict which position of the breast will cancer likely to be found, with a range of independent variables which you can inspect in the dataset."}}