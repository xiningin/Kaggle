{"cell_type":{"d0284921":"code","732b7b30":"code","75801b9f":"code","a2919920":"code","3bd11215":"code","27c4c990":"code","75498d28":"code","863606bc":"code","89928aaa":"code","0c04d436":"code","f64d8547":"code","d6c88df7":"code","7e9fd79d":"code","9f8ca7e3":"code","43cefad1":"code","50c287c1":"code","f71d7300":"code","6b7e6cd7":"code","3309dde2":"code","0fb9df25":"code","834869d2":"code","bd5280f6":"code","23df95f6":"code","a0503a90":"code","b9f2f407":"code","5c4fefc0":"code","dc693bc7":"code","f77a3fb6":"code","509e6bf3":"code","c3acc03b":"code","72bb946f":"code","77d8500c":"code","dd2cb7b8":"markdown","1eb76192":"markdown","478fc00d":"markdown","4681e2a5":"markdown","7d632557":"markdown","87882cb7":"markdown","f53cab41":"markdown","8028accd":"markdown","d42e8e2b":"markdown","0d9e6533":"markdown","fcc65135":"markdown","64522e6d":"markdown","7c7c9268":"markdown","346f5a0a":"markdown","4f159155":"markdown","b983ee76":"markdown","8f1aa1e6":"markdown","f7e8bc71":"markdown","ba1ba2df":"markdown","d35c390d":"markdown","d9bbbf82":"markdown","5aa8e2da":"markdown","29437e0b":"markdown","7c0779c8":"markdown","4dc69b0d":"markdown","50160677":"markdown"},"source":{"d0284921":"# Import Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom xgboost import XGBClassifier,XGBRegressor,plot_importance,XGBRFRegressor\n\nfrom scipy.stats import mode,boxcox,skew\n\nimport matplotlib.pyplot as plt\nimport optuna\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,PowerTransformer\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer,KNNImputer,IterativeImputer\nfrom sklearn.feature_selection import SelectFromModel\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom category_encoders import target_encoder\nimport seaborn as sns\nimport gc\nimport sys,os\n\nfrom scipy.spatial import distance\ngc.enable()\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","732b7b30":"# Read Train, Test and Sample Submission Files\ndef read_data():\n    df_train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\n    df_test = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\n    df_submission = pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")\n    return df_train,df_test,df_submission","75801b9f":"# Read datasets\ndf_train,df_test,df_submission = read_data()","a2919920":"df_train.isna().sum()","3bd11215":"df_train.info()","27c4c990":"df_test.info()","75498d28":"df_train.describe().T","863606bc":"df_test.describe().T","89928aaa":"cont_cols = [col for col in df_train.columns if col not in ['id','song_popularity','audio_mode','time_signature','key']]\ncat_cols = ['audio_mode','time_signature','key']","0c04d436":"print(f'% Distribution of Song Popularity:\\n{(df_train.song_popularity.value_counts())\/len(df_train.song_popularity)*100}')","f64d8547":"df_train.key.value_counts()","d6c88df7":"df_train.audio_mode.value_counts()","7e9fd79d":"df_train.time_signature.value_counts()","9f8ca7e3":"for col in cat_cols:\n    df_train.hist(col,grid=False,bins = 12)","43cefad1":"for col in cont_cols:\n    sns.displot(data=df_train, x=col,hue=\"song_popularity\",kind = \"kde\", fill=True)","50c287c1":"for col in cont_cols:\n    plt.figure()\n    sns.boxplot(data = df_train,x='song_popularity',y = col)","f71d7300":"# Check correlation between the columns\ncorr = df_train.drop(['id','song_popularity'],axis=1).corr()\ncm = sns.light_palette(\"green\", as_cmap=True)\ncm = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\ncorr.style.background_gradient(cmap=cm)","6b7e6cd7":"# Function to Scale and transform dataset\ndef data_scaler_fit(option,df):\n    if option == 1:\n        transformer = StandardScaler().fit(df)\n    if option == 2 :\n        transformer = RobustScaler().fit(df)\n    if option == 3 :\n        transformer = MinMaxScaler().fit(df)\n    if option == 4 :\n        transformer = PowerTransformer(method = 'yeo-johnson').fit(df)\n    return transformer","3309dde2":"# Function to Remove outliers\ndef remove_outliers(x,method):\n    if method == 'mean':\n        upper_limit = x.mean() + (3*x.std())\n        lower_limit = x.mean() - (3*x.std())\n        return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))\n    elif method == 'median':\n        upper_limit = x.median() + (1.5*x.quantile(0.75))\n        lower_limit = x.median() - (1.5*x.quantile(0.25))\n        return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))\n    else:\n        return x","0fb9df25":"def imputations(impute_method,method=None):\n    if impute_method == 'knn':\n        imputer = KNNImputer(n_neighbors = 10,weights = 'distance')\n    if impute_method == 'iter':\n        imputer = IterativeImputer(max_iter=20)\n    if impute_method == 'simple':\n        imputer = SimpleImputer(strategy=method)\n    if impute_method == 'lgbm':\n        if not os.path.exists(\"kuma_utils\/\"):\n            !git clone https:\/\/github.com\/analokmaus\/kuma_utils.git\n        sys.path.append(\"kuma_utils\/\")\n        from kuma_utils.preprocessing.imputer import LGBMImputer\n        imputer = LGBMImputer(n_iter=300, verbose=False)\n    return imputer","834869d2":"def feature_transform(df,option,method):\n    ids = df.id.values.tolist()\n    \n    # Replace missing values for continuous columns\n    impute = imputations(impute_method)\n    \n    df_temp = pd.DataFrame(impute.fit_transform(pd.concat([df[cat_cols],df[cont_cols]],axis = 1)))\n    \n    df_temp.columns = cat_cols + cont_cols\n    \n    df_cat = df_temp[cat_cols].copy()\n    df_cont = df_temp[cont_cols].copy()\n    \n    # Decreasing data skewness for continuos variables using BoxCox\n    df_cont['golden_ratio'] = (df_cont['song_duration_ms'] \/ 1000) * 0.618033\n    df_cont['song_duration_ms'] = boxcox(df_cont.song_duration_ms\/60000)[0]\n    \n    df_cont['instrumentalness_main'] = np.where(df_cont['instrumentalness']<=0.01,df_cont['instrumentalness'],0)\n    df_cont['instrumentalness_side'] = np.where(df_cont['instrumentalness']>0.2,df_cont['instrumentalness'],0)\n    df_cont.drop('instrumentalness',axis =1 ,inplace=True)\n    #df_cont['instrumentalness_side'] = boxcox((df_cont.instrumentalness_side)+0.005)[0]\n    #df_cont['instrumentalness_main'] = boxcox((df_cont.instrumentalness_main)+0.2)[0]\n    \n    \n    #df_cont['acousticness'] = boxcox(df_cont.acousticness + 0.2)[0]\n    \n    df_cont['liveness_main'] = np.where(df_cont['liveness']<0.8,df_cont['liveness'],0)\n    df_cont['liveness_side'] = np.where(df_cont['liveness']>=0.8,df_cont['liveness'],0)\n    df_cont.drop('liveness',axis =1 ,inplace=True)\n    #df_cont['liveness']  = boxcox(df_cont.liveness + 0.04)[0]    \n    \n    #df_cont['loudness'] = boxcox(df_cont.loudness + 100)[0] \n    #df_cont['energy'] = boxcox(df_cont.energy+0.2)[0]\n    #df_cont['danceability'] = boxcox(df_cont.danceability)[0]\n  \n    \n    df_cont['speechiness_sw'] = np.where(df_cont['speechiness']>0.66,df_cont['speechiness'],0)\n    df_cont['speechiness_mw'] = np.where((df_cont['speechiness']<=0.66) & (df_cont['speechiness']>=0.33),df_cont['speechiness'],0)\n    df_cont['speechiness_m'] = np.where(df_cont['speechiness']<0.33,df_cont['speechiness'],0)\n    #df_cont['speechiness'] = boxcox(df_cont.speechiness)[0]\n    df_cont.drop('speechiness',axis =1 ,inplace=True)\n    \n    \n    #df_cont['tempo'] = boxcox(df_cont.tempo)[0]\n    \n    # Normalise Continuous columns\n    transformer = data_scaler_fit(option,df_cont)\n    df_cont = pd.DataFrame(transformer.transform(df_cont.apply(lambda x: remove_outliers(x,method))))\n    \n    df_cont.columns = [x for x in cont_cols if (x not in ['instrumentalness','liveness','speechiness'])] + ['golden_ratio','instrumentalness_main','instrumentalness_side','liveness_main','liveness_side','speechiness_sw','speechiness_mw','speechiness_m']\n    \n    df = pd.concat([df_cont,df_cat.reset_index(drop=True)],axis = 1)\n    \n    df['id'] = ids\n    \n    return df","bd5280f6":"# Initialize Static values\nrandom_state = 151\nearly_stopping_rounds=1500\nverbose = 2000\nn_estimators = 15000\nn_splits = 5\nmethod = 'mean'\nscaling_option = 4\nimpute_method = 'lgbm'","23df95f6":"df_train,df_test,df_submission = read_data()\n\ny = df_train.song_popularity.copy()\nX = df_train.drop(['song_popularity'],axis = 1)\n\n\ndef objective(trial):\n    # XGBoost parameters\n    \n    gc.collect()\n    \n    final_valid_predictions = {}\n    \n    scores = []\n    auc_score = []\n    \n    params = {\n        \"objective\": \"reg:squaredlogerror\",\n        \"n_estimators\": n_estimators,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 15),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.5),\n        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.9),\n        \"subsample\": trial.suggest_loguniform(\"subsample\", 0.2, 0.9),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 100.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.01, 100.0),\n        \"gamma\": trial.suggest_loguniform(\"lambda\", 0.01, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 10, 100),\n        \"n_jobs\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"predictor\" : \"gpu_predictor\",\n    }\n    \n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n        xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n        xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n\n        # Store IDs of validation Dataset\n        valid_ids = xvalid.id.values.tolist()\n\n        #Save a copy of yvalid\n        true_valid = yvalid\n\n        n_class = len(np.unique(ytrain))\n\n        xtrain = feature_transform(xtrain,scaling_option,method)\n        xvalid = feature_transform(xvalid,scaling_option,method)\n        \n        xtrain = xtrain.drop('id',axis = 1)\n        xvalid = xvalid.drop('id',axis = 1)\n        \n        \n        model = XGBRegressor(\n            random_state = random_state,\n            sampling_method = 'gradient_based',\n            use_label_encoder=False,\n            eval_metric = ['auc'],\n            **params\n        )\n        model.fit(xtrain, ytrain,early_stopping_rounds=early_stopping_rounds, eval_set=[(xvalid, yvalid)], verbose=verbose)\n\n        preds_valid = model.predict(xvalid)\n        \n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n        auc_scr = roc_auc_score(true_valid, preds_valid)\n\n        auc_score.append(auc_scr)\n\n        print(f\"Fold {fold+1} || AUC : {auc_scr} || Mean AUC : {np.mean(auc_score)}\")\n    \n    return roc_auc_score(y.to_numpy(), np.array(sorted(final_valid_predictions.items()))[:,1])","a0503a90":"df_train,df_test,df_submission = read_data()\n\ny = df_train.song_popularity.copy()\nX = df_train.drop(['song_popularity'],axis = 1)\n\n\ndef objective(trial):\n    # XGBoost parameters\n    \n    gc.collect()\n    \n    final_valid_predictions = {}\n    \n    scores = []\n    auc_score = []\n    \n    \n    \"allow_writing_files\": False,\n    \"bagging_temperature\": 1,\n    \"boosting_type\": \"Plain\",\n    \"border_count\": 231,\n    \"depth\": 3,\n    \"eval_metric\": \"AUC\",\n    \"l2_leaf_reg\": 12,\n    \"learning_rate\": 0.05271928250913923,\n    #\"loss_function\": \"Logloss\",\n    \"od_pval\": 0.01,\n    \"od_type\": \"IncToDec\",\n    \"rsm\": 0.22545533368402262,\n    \"random_seed\": 53703,\n    \"iterations\": n_estimators,\n    'early_stopping_rounds': early_stopping_rounds ,\n    \"cat_features\" : ['audio_mode','time_signature'],\n    \n    params = {\n        \"objective\": \"reg:squaredlogerror\",\n        \"n_estimators\": n_estimators,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 15),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.5),\n        \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.2, 0.9),\n        \"subsample\": trial.suggest_loguniform(\"subsample\", 0.2, 0.9),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 100.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.01, 100.0),\n        \"gamma\": trial.suggest_loguniform(\"lambda\", 0.01, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 10, 100),\n        \"n_jobs\": -1,\n        \"tree_method\": \"gpu_hist\",\n        \"predictor\" : \"gpu_predictor\",\n    }\n    \n    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n        xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n        xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n\n        # Store IDs of validation Dataset\n        valid_ids = xvalid.id.values.tolist()\n\n        #Save a copy of yvalid\n        true_valid = yvalid\n\n        n_class = len(np.unique(ytrain))\n\n        xtrain = feature_transform(xtrain,scaling_option,method)\n        xvalid = feature_transform(xvalid,scaling_option,method)\n        \n        xtrain = xtrain.drop('id',axis = 1)\n        xvalid = xvalid.drop('id',axis = 1)\n        \n        \n        model = XGBRegressor(\n            random_state = random_state,\n            sampling_method = 'gradient_based',\n            use_label_encoder=False,\n            eval_metric = ['auc'],\n            **params\n        )\n        model.fit(xtrain, ytrain,early_stopping_rounds=early_stopping_rounds, eval_set=[(xvalid, yvalid)], verbose=verbose)\n\n        preds_valid = model.predict(xvalid)\n        \n        final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n        auc_scr = roc_auc_score(true_valid, preds_valid)\n\n        auc_score.append(auc_scr)\n\n        print(f\"Fold {fold+1} || AUC : {auc_scr} || Mean AUC : {np.mean(auc_score)}\")\n    \n    return roc_auc_score(y.to_numpy(), np.array(sorted(final_valid_predictions.items()))[:,1])","b9f2f407":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)","5c4fefc0":"hp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")","dc693bc7":"# Read DATA Again\ndf_train,df_test,df_submission = read_data()\n\n# Seperate X and y\ny = df_train.song_popularity.copy()\nX = df_train.drop(['song_popularity'],axis = 1)\n\n\n# Transform Test Dataset\ndf_test = feature_transform(df_test,scaling_option,method)\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\n\nscores = []\nauc_score = []\n\ncv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n    xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n    \n    xtest = df_test.copy().drop('id',axis = 1)\n\n    # Store IDs of validation Dataset\n    valid_ids = xvalid.id.values.tolist()\n    \n    #Save a copy of yvalid\n    true_valid = yvalid\n    \n    n_class = len(np.unique(ytrain))\n    \n    xtrain = feature_transform(xtrain,scaling_option,method)\n    xvalid = feature_transform(xvalid,scaling_option,method)\n    \n    xtrain = xtrain.drop('id',axis = 1)\n    xvalid = xvalid.drop('id',axis = 1)\n    \n    \n    static = {\n        \"n_estimators\": n_estimators,\n        \"objective\": \"reg:squaredlogerror\",\n        \"random_state\": random_state,\n        \"n_jobs\": -1,\n        #\"tree_method\": \"gpu_hist\",\n        #\"predictor\" : \"gpu_predictor\",\n        \"eval_metric\" : ['aucpr','auc'],\n        #\"sampling_method\" : 'gradient_based',\n        \"use_label_encoder\" : False,\n    }\n    \n    params = dict(static)\n\n    params.update(study.best_params)\n    \n\n    model = XGBRegressor(\n        **params\n    )\n    model.fit(xtrain, ytrain,early_stopping_rounds=early_stopping_rounds, eval_set=[(xtrain, ytrain),(xvalid, yvalid)], verbose=verbose)\n\n    preds_valid = model.predict(xvalid)\n\n    test_preds = model.predict(xtest)\n\n    final_test_predictions.append(test_preds)\n\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n    auc_scr = roc_auc_score(true_valid, preds_valid)\n    \n    auc_score.append(auc_scr)\n    \n    print('_'*65)\n    \n    print(f\"Fold {fold+1}  || AUC : {auc_scr} || Mean AUC : {np.mean(auc_score)}\")\n    \n    print('_'*65)\n    \n    print('\\n')\n    \n    gc.collect()","f77a3fb6":"#Plot Feature Importance\nplt.figure(figsize=(20, 15),dpi=80)\nplot_importance(model)\nplt.show()","509e6bf3":"# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['aucpr'])\nx_axis = range(0, epochs)\n# plot AUCPR\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['aucpr'], label='Train')\nax.plot(x_axis, results['validation_1']['aucpr'], label='Valid')\nax.legend()\nplt.ylabel('AUCPR Score')\nplt.title('XGBoost AUCPR')\nplt.show()\n# plot AUC\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['auc'], label='Train')\nax.plot(x_axis, results['validation_1']['auc'], label='Valid')\nax.legend()\nplt.ylabel('AUC Score')\nplt.title('XGBoost AUC')\nplt.show()","c3acc03b":"hyper_para = {\n    \"allow_writing_files\": False,\n    \"bagging_temperature\": 1,\n    \"boosting_type\": \"Plain\",\n    \"border_count\": 231,\n    \"depth\": 3,\n    \"eval_metric\": \"AUC\",\n    \"l2_leaf_reg\": 12,\n    \"learning_rate\": 0.05271928250913923,\n    #\"loss_function\": \"Logloss\",\n    \"od_pval\": 0.01,\n    \"od_type\": \"IncToDec\",\n    \"rsm\": 0.22545533368402262,\n    \"random_seed\": 53703,\n    \"iterations\": n_estimators,\n    'early_stopping_rounds': early_stopping_rounds ,\n    \"cat_features\" : ['audio_mode','time_signature'],\n}","72bb946f":"# Read DATA Again\ndf_train,df_test,df_submission = read_data()\n\n# Seperate X and y\ny = df_train.song_popularity.copy()\nX = df_train.drop(['song_popularity'],axis = 1)\n\n\n# Transform Test Dataset\ndf_test = feature_transform(df_test,scaling_option,method)\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\n\nscores = []\nauc_score = []\n\ncv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n    xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n    \n    xtest = df_test.copy().drop('id',axis = 1)\n\n    # Store IDs of validation Dataset\n    valid_ids = xvalid.id.values.tolist()\n    \n    #Save a copy of yvalid\n    true_valid = yvalid\n    \n    n_class = len(np.unique(ytrain))\n    \n    xtrain = feature_transform(xtrain,scaling_option,method)\n    xvalid = feature_transform(xvalid,scaling_option,method)\n    \n    xtrain = xtrain.drop('id',axis = 1)\n    xvalid = xvalid.drop('id',axis = 1)\n    \n    \n    \n    params = hyper_para\n    \n\n    model = CatBoostRegressor(**params)\n    model.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain),(xvalid, yvalid)], verbose=verbose)\n\n    preds_valid = model.predict(xvalid)\n\n    test_preds = model.predict(xtest)\n\n    final_test_predictions.append(test_preds)\n\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n    auc_scr = roc_auc_score(true_valid, preds_valid)\n    \n    auc_score.append(auc_scr)\n    \n    print('_'*65)\n    \n    print(f\"Fold {fold+1}  || AUC : {auc_scr} || Mean AUC : {np.mean(auc_score)}\")\n    \n    print('_'*65)\n    \n    print('\\n')\n    \n    gc.collect()","77d8500c":"df_submission.song_popularity = np.mean(np.column_stack(final_test_predictions), axis=1)\ndf_submission.columns = [\"id\", \"song_popularity\"]\ndf_submission.to_csv(\"submission.csv\", index=False)","dd2cb7b8":"> **Let's Create a function below to Scale our data**","1eb76192":"#### *Yeah, you guessed it right, my favourite musician*","478fc00d":"### 4. Using Optuna for Hyperparameter Tuning","4681e2a5":"![alt text](https:\/\/miro.medium.com\/max\/900\/1*80wf6AeqTLD9ntyFxYMuLw.jpeg)","7d632557":"#### Let's see how you look.. any missing values.. ANY?","87882cb7":"### Keep Calm .. Optuna is Studying....","f53cab41":"<h1><p style=\"text-align: center;\"> <u> Song Popularity Prediction <\/u>  <\/p> <\/h1>\n<h3><p style=\"text-align: center;\"> <i> Tabular The Data Is <\/i>  <\/p> <\/h3>","8028accd":"> **A Function to remove outlier, the standard way**","d42e8e2b":"**Insight**: Both Test and Train have same columns which have NULL values .. Nice","0d9e6533":"### 6. Using CAT Boost","fcc65135":"### 2. Plot the distribution of data for each column","64522e6d":"#### Print best parameters ..","7c7c9268":"***Do upvote if you found this useful*** \n\nPS: I am still working on improving this notebook and Model, so stay tuned!!!","346f5a0a":"> Let's check box plots for those.. outliers","4f159155":"### 1. Read Train, Test and Submission files","b983ee76":"> **I NEED SOME CHARTS.. NOW**","8f1aa1e6":"### I know my visualization skills need a serious upgrade\n\n#### *Working..... On it........*","f7e8bc71":"### 3. Let's Transform those features, one at a time","ba1ba2df":"![Alt text](https:\/\/media.makeameme.org\/created\/if-you-didnt-58c83d63de.jpg)","d35c390d":"<img src=\"https:\/\/i.gifer.com\/9mhx.gif\" width=\"500\" \/>","d9bbbf82":"### What does each variable mean?\n> - **Instrumentalness:** This value represents the amount of vocals in the song. The closer it is to 1.0, the more instrumental the song is.\n> - **Acousticness:** This value describes how acoustic a song is. A score of 1.0 means the song is most likely to be an acoustic one.\n> - **Liveness:** This value describes the probability that the song was recorded with a live audience. According to the official documentation \u201ca value above 0.8 provides strong likelihood that the track is live\u201d.\n> - **Speechiness:** \u201cSpeechiness detects the presence of spoken words in a track\u201d. If the speechiness of a song is above 0.66, it is probably made of spoken words, a score between 0.33 and 0.66 is a song that may contain both music and words, and a score below 0.33 means the song does not have any speech.\n> - **Energy:** \u201c(energy) represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy\u201d.\n> - **Danceability:** \u201cDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable\u201d.\n> - **Valence:** \u201cA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry)\u201d\n> - **Song Duration ms :** Duration of song in milliseconds\n> - **Audio Mode :** No specific description\n> - **Tempo :** Tempo (Italian for \"time\"; plural tempos, or tempi from the Italian plural) is the speed or pace of a given piece.For example, a tempo of 60 beats per minute signifies one beat per second, while a tempo of 120 beats per minute is twice as rapid, signifying one beat every 0.5 seconds\n> - **Time Signature :** The time signature indicates how many counts are in each measure and which type of note will receive one count. The top number is commonly 2, 3, 4, or 6.The bottom number is either 4 or 8. Simple time signatures divide music into groups of 2 and compound divide music into groups of 3.\n> - **Loudness:** Loudness measures the decibel level of a song. Decibels are relative to a reference value, so songs with lower loudness values are quieter relative to the reference value of 0.\n> - **Danceability:** Danceability quantifies how suitable a track is for dancing based on a combination of musical elements, like tempo, rhythm, and beat. Songs with higher danceability have stronger and more regular beats.Like acousticness, danceability is measured on a scale of 0 (low danceability) to 100 (high danceability).","5aa8e2da":"### 5. Training XGB Model based on best Parameters","29437e0b":"<p style=\"text-align: center\" >\n<img src= \"https:\/\/media.giphy.com\/media\/DMHEccCwpNxCQBZlvQ\/giphy.gif\"\n<\/p>","7c0779c8":"> 1. Column **Key**,**audio_mode**,**time_signature**  seems to be categorical, hence replacing null values using KNN\n> 2. Convert **time_signature** from milliseconds to minutes\n> 3. Fill Rest of the columns which has null values using mean of the columns (Might replace with something better)\n> 4. Use Boxcox transform to reduce skewness in continuous columns\n> 5. Finally using StandarScaler on all continuous columns : *It transforms the data in such a manner that it has mean as 0 and standard deviation as 1*","4dc69b0d":"### 7. Submit Score","50160677":"Hmm... Below plots will show the distribution of data against our target variables.. Please be nice"}}