{"cell_type":{"4701ec32":"code","57bf0166":"code","1af74a33":"code","98fbeb1d":"code","69f18642":"code","72c34e4e":"code","ffb7f643":"code","4bb21ed0":"code","066cf4b2":"code","03a7ac5e":"code","89290330":"code","b623e534":"code","67c6bd36":"code","37aacc00":"code","99d63bc1":"code","e614d1cd":"code","17fc4229":"code","27a6b031":"code","3f1a2650":"code","691dc11b":"code","bb63e8ac":"code","740133e3":"code","2f8fc136":"markdown","154f1816":"markdown","153704ad":"markdown","ed89956c":"markdown","289ecb48":"markdown","b6fffb83":"markdown","d0b99b09":"markdown","5228fb3a":"markdown","6d5f01e4":"markdown","b8ab6d75":"markdown","3810f3ad":"markdown","1aaa02d5":"markdown"},"source":{"4701ec32":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom scipy import stats\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom xgboost import XGBRegressor\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn import preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","57bf0166":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest  = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsubmit = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","1af74a33":"train.head()","98fbeb1d":"train.tail()","69f18642":"test.head()","72c34e4e":"test.tail()","ffb7f643":"train.info()","4bb21ed0":"test.info()","066cf4b2":"train.isnull().sum()","03a7ac5e":"test.isnull().sum()","89290330":"train.describe()","b623e534":"categorical_cols=['cat'+str(i) for i in range(10)]\ncontinous_cols=['cont'+str(i) for i in range(14)]","67c6bd36":"print(categorical_cols)\nprint(32*\"----\") \nprint(continous_cols)","37aacc00":"i = 1\nplt.figure()\nfig, ax = plt.subplots(7, 2,figsize=(20, 24))\nfor feature in continous_cols:\n    plt.subplot(7, 2,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1","99d63bc1":"i = 1\nplt.figure()\nfig, ax = plt.subplots(5, 2,figsize=(28, 44))\nfor feature in categorical_cols:\n    plt.subplot(5, 2,i)\n    sns.histplot(train[feature],color=\"blue\", label='train')\n    sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","e614d1cd":"corr = train[continous_cols+['target']].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","17fc4229":"train.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","27a6b031":"x = train.drop(['target'], axis=1)\ny = train['target']\nX_test = test.copy()","3f1a2650":"ordinal_encoder = OrdinalEncoder()\nx[categorical_cols] = ordinal_encoder.fit_transform(x[categorical_cols])\nX_test[categorical_cols] = ordinal_encoder.transform(X_test[categorical_cols])\nx.head()","691dc11b":"# train test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(x, y,test_size=0.2, random_state=50)","bb63e8ac":"xgb_params = {'objective': 'reg:squarederror',\n              'n_estimators': 10000,\n              'learning_rate': 0.036,\n              'subsample': 0.926,\n              'colsample_bytree': 0.118,\n              'grow_policy':'lossguide',\n              'max_depth': 3,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'reg_lambda': 0.00087,\n              'reg_alpha': 23.132}\n\nmodel_XGB = XGBRegressor(**xgb_params)\nmodel_XGB.fit(X_train, y_train) \npred_XGB = model_XGB.predict(X_valid)\nprint(mean_squared_error(y_valid, pred_XGB, squared=False))","740133e3":"pred = model_XGB.predict(X_test)\n# Save the predictions to a CSV file\nsubmit['target']=pred\nsubmit.to_csv('submission.csv', index=False)","2f8fc136":"Parameters were taken from https:\/\/www.kaggle.com\/miladagdam\/30-days-of-ml-xgboost","154f1816":"# **Reading dataset as train, test and submit**","153704ad":"# **Firstly, let's import all necessary libraries**","ed89956c":"# Data Encoding","289ecb48":"# Data Visualization","b6fffb83":"# **Let's observe our data**","d0b99b09":"As you see code below (*train.isnull().sum()*), we don't have any null value.","5228fb3a":"## Distribution of Continuous Features  ","6d5f01e4":"## Heat Map","b8ab6d75":"## Distribution of Categorical Features  ","3810f3ad":"# Application of Xgboost ","1aaa02d5":"# Let's submit to the competition"}}