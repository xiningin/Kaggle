{"cell_type":{"4c9576a9":"code","fa577d8e":"code","f2b6838e":"code","1419431b":"code","ae8fb1bb":"code","034dc514":"code","f0de0175":"code","4fc4f941":"code","5e382e39":"code","6efda3f6":"code","c6d2e652":"code","e1f44ba9":"code","5740238a":"code","74025a57":"code","036271d1":"code","59d29104":"code","d1d881d2":"code","34137a06":"code","c0b59be2":"code","cef0fc8d":"code","6f7fe3cb":"code","652bb09b":"code","57229fc1":"code","8cce2003":"code","5e1b77af":"code","a55e3cbd":"code","d30df21f":"code","11f44743":"code","bdfe45fe":"code","d3f0f645":"code","d5e81a4e":"code","ea5717c9":"code","56a0b630":"code","564ea821":"code","3f73154c":"code","e4d83949":"code","911a62c8":"code","cf6b82c3":"code","b8b28c8b":"markdown","8a9a3633":"markdown","35695a30":"markdown","d8d5cb5f":"markdown","105b7e6a":"markdown","f04c0112":"markdown","122559ae":"markdown"},"source":{"4c9576a9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler","fa577d8e":"# \/kaggle\/input\/titanic\/train.csv\n# \/kaggle\/input\/titanic\/gender_submission.csv\n# \/kaggle\/input\/titanic\/test.csv\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain.head()","f2b6838e":"corrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, annot=True, fmt='.2f', vmax=1, square=True)","1419431b":"sns.set()\nsns.pairplot(train, size = 2.5)\nplt.show()","ae8fb1bb":"sns.barplot(train['Survived'], train['Sex'])","034dc514":"sns.swarmplot(train['Survived'], train['Fare'])","f0de0175":"sns.barplot(train['Survived'], train['Pclass'])","4fc4f941":"sns.swarmplot(x=\"Survived\", y=\"Fare\", data=train)","5e382e39":"sns.swarmplot(train['Survived'], train['Age'])\n# sns.barplot(train['Survived'], train['Age'])","6efda3f6":"sns.barplot(train['Survived'], (train['Age'] < 15).astype(int))\n","c6d2e652":"train_survived = train['Survived']\ntrain_len = len(train)\nall_data = pd.concat((train, test), ignore_index=True)\n\ntrain_len","e1f44ba9":"# FamilyName, Title, Famliy\nall_data['FamilyName'] = all_data['Name'].apply(lambda st: st[0:st.find(\",\")])\nall_data['Title'] = all_data['Name'].apply(lambda st: st[st.find(\",\") + 1:st.find(\".\")])\nall_data['Family'] = all_data['SibSp'] + all_data['Parch'] > 0\nall_data['Family'] = all_data['Family'].astype(int)","5740238a":"# IsChild\n# all_data['IsChild'] = (all_data['Age'] < 20).astype(int)\n\n# Sex (trans to int)\n# all_data['Sex'] = (all_data['Sex'] == 'female').astype(int)\n\n# all_data.tail()","74025a57":"# missing_train\nmissing = all_data.isnull().sum().sort_values(ascending=False)\nmissing","036271d1":"# Cabin => None\nall_data['Cabin'] = all_data['Cabin'].fillna('None')\n\n# Age => Drop\nall_data.drop(['Age'], axis=1, inplace=True)","59d29104":"# Fare => mean (Pclass, Embarked, Famliy)\nfare_nan_data = all_data[all_data['Fare'].isnull()]\nfare_nan_data_id = fare_nan_data['PassengerId']\nfare_nan_data","d1d881d2":"\nfare_nan_data_input = all_data[(all_data['Embarked'] == fare_nan_data['Embarked'].values[0]) & \n                               (all_data['Family'] == fare_nan_data['Family'].values[0]) & \n                            (all_data['Pclass'] == fare_nan_data['Pclass'].values[0])].mean()\nall_data.loc[fare_nan_data.index, 'Fare'] = fare_nan_data_input['Fare']\nall_data.loc[fare_nan_data.index, 'Fare']","34137a06":"# Embarked => Drop        \nall_data[all_data['Embarked'].isnull()]","c0b59be2":"all_data = all_data.drop(index=[61, 829])\nall_data.head()","cef0fc8d":"# Scaler(Age, Fare)\nscaler = MinMaxScaler()\nall_data['Fare'] = scaler.fit_transform(all_data['Fare'].to_numpy().reshape(-1, 1))\nall_data.head()","6f7fe3cb":"all_data['Sex'] = all_data['Sex'] == 'female'\nall_data['Sex'] = all_data['Sex'].astype(int)\nall_data.head()","652bb09b":"all_data.drop(['Name', 'SibSp', 'Parch', 'Cabin', 'FamilyName', 'Embarked', 'Title'], axis=1, inplace=True)\nall_data.head()","57229fc1":"# all_data['Pclass'] = all_data['Pclass'].astype(str)\n\nall_data = pd.get_dummies(all_data)\nall_data.head()","8cce2003":"# Split Train and Test\ntrain = all_data[:train_len - 2] # remove 2 rows (by Embarked)\ntest = all_data[train_len - 2:]\n\n# Test Id\ntest_id = test['PassengerId']\ny_train = train.Survived.values\n\n# Drop Id\ntrain.drop(['PassengerId'], axis=1, inplace=True)\ntest.drop(['PassengerId'], axis=1, inplace=True)\n\n# Drop Survived\ntrain.drop(['Survived'], axis=1, inplace=True)\ntest.drop(['Survived'], axis=1, inplace=True)\n\nprint(len(all_data), len(train), len(test))","5e1b77af":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, Ridge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","a55e3cbd":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","d30df21f":"ridge = make_pipeline(RobustScaler(), Ridge(alpha =0.0005, random_state=1))\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost1 = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n# GBoost with different option\nGBoost2 = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=5)\nRF = make_pipeline(RobustScaler(), RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=5))\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","11f44743":"ridge_score = rmsle_cv(ridge)\nlasso_score = rmsle_cv(lasso)\nenet_score = rmsle_cv(ENet)\nkrr_score = rmsle_cv(KRR)\ngboost1_score = rmsle_cv(GBoost1)\ngboost2_score = rmsle_cv(GBoost2)\nrf_score = rmsle_cv(RF)\nxgb_score = rmsle_cv(model_xgb)\nlgb_score = rmsle_cv(model_lgb)\nprint(\"Ridge score: {:.4f} ({:.4f})\\n\".format(ridge_score.mean(), ridge_score.std()))\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(lasso_score.mean(), lasso_score.std()))\nprint(\"ENet score: {:.4f} ({:.4f})\\n\".format(enet_score.mean(), enet_score.std()))\nprint(\"KRR score: {:.4f} ({:.4f})\\n\".format(krr_score.mean(), krr_score.std()))\nprint(\"Gradient Boosting1 score: {:.4f} ({:.4f})\\n\".format(gboost1_score.mean(), gboost1_score.std()))\nprint(\"Gradient Boosting2 score: {:.4f} ({:.4f})\\n\".format(gboost2_score.mean(), gboost2_score.std()))\nprint(\"Random Forest score: {:.4f} ({:.4f})\\n\".format(rf_score.mean(), rf_score.std()))\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(xgb_score.mean(), xgb_score.std()))\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(lgb_score.mean(), lgb_score.std()))\nprint(\"totalAVG: {:.4f}\\n\".format((ridge_score.mean() +\n                                   lasso_score.mean() +\n                                   enet_score.mean()+\n                                   krr_score.mean() +\n                                   gboost1_score.mean() +\n                                   gboost2_score.mean() +\n                                   rf_score.mean() +\n                                   xgb_score.mean() +\n                                   lgb_score.mean()) \/ 9))","bdfe45fe":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=200)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","d3f0f645":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, KRR, GBoost1, lasso),\n                                                 meta_model = RF)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","d5e81a4e":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","ea5717c9":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = stacked_averaged_models.predict(test.values)\nprint(rmsle(y_train, stacked_train_pred))","56a0b630":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = model_xgb.predict(test)\nprint(rmsle(y_train, xgb_train_pred))","564ea821":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = model_lgb.predict(test.values)\nprint(rmsle(y_train, lgb_train_pred))","3f73154c":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n# print(ensemble)","e4d83949":"for n, i in enumerate(ensemble):\n    if i >= 0.5:\n        ensemble[n] = 1\n    else:\n        ensemble[n] = 0\nprint(ensemble)","911a62c8":"result = pd.DataFrame()\nresult['PassengerId'] = test_id\nresult['Survived'] = np.asarray(ensemble, dtype=int)\n\nresult.head()","cf6b82c3":"result.to_csv('submission_sex_to_num.csv', index=False)","b8b28c8b":"## 3.1 Append & Drop Columns ","8a9a3633":"# 2. Anaylsis","35695a30":"## 3.3 Drop Data (Un Use Data)","d8d5cb5f":"# 1. Read Data","105b7e6a":"# 4. Modeling","f04c0112":"## 3.2 Missing Data","122559ae":"# 3. Preprocessing"}}