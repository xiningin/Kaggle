{"cell_type":{"d4b9e6e6":"code","94758ddd":"code","dea61b4b":"code","6be1e9a2":"code","ae70e1de":"code","cc2113f9":"code","3f8206e9":"code","c16f592d":"code","97b1ba55":"code","25e14400":"code","aff0b58b":"code","ec8681a3":"code","b031d7c0":"code","1a9d7904":"code","8f1a6ce8":"code","a3994253":"code","9a45f593":"code","f36ce3df":"code","37389461":"markdown","1112a6bb":"markdown","4c52803c":"markdown","8079e007":"markdown","b6184fd3":"markdown","0eeeeb8f":"markdown","096223a5":"markdown","c1f08d14":"markdown","bbf82f30":"markdown","d37d5c74":"markdown","be137bd6":"markdown","ce2b9303":"markdown","ffbfb885":"markdown","f983721a":"markdown","34cc89ac":"markdown","f44b8799":"markdown","a472beb3":"markdown","27d84108":"markdown","fcb514f3":"markdown","87921d2b":"markdown","08d2eba6":"markdown"},"source":{"d4b9e6e6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.svm import  SVC\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder\nfrom sklearn.pipeline import make_pipeline\npd.set_option('display.max_columns', 10)\npd.set_option('display.max_rows', 1000)\n","94758ddd":"training = pd.read_csv('..\/input\/summeranalytics2020\/train.csv')\ntrain_data = training.copy()\ntest_data = pd.read_csv('..\/input\/summeranalytics2020\/test.csv')","dea61b4b":"train_data.head()","6be1e9a2":"train_data.info()","ae70e1de":"print(train_data.describe())","cc2113f9":"train_id = train_data.Id\ntrain_data = train_data.drop(['Behaviour','Id'],axis = 1)\n\ntest_id = test_data.Id\ntest_data = test_data.drop(['Behaviour','Id'],axis = 1)","3f8206e9":"train_data['PerformanceRating'] = train_data['PerformanceRating'].apply(lambda x: 0 if x == 3 else 1)\ntest_data['PerformanceRating'] = test_data['PerformanceRating'].apply(lambda x: 0 if x == 3 else 1)","c16f592d":"train_data['Attrition'].value_counts().plot(kind = 'bar')","97b1ba55":"print('Number of duplicates: ',train_data.duplicated().sum())","25e14400":"train_data[train_data.duplicated()]['Attrition'].value_counts().plot(kind = 'bar')","aff0b58b":"# drop them\n\ntrain_unq = train_data.drop_duplicates()\nprint('New train set: ',train_unq.shape)\nX = train_unq.drop('Attrition',axis = 1)\ny = train_unq['Attrition']\ny.value_counts().plot(kind = 'bar')\nplt.show()","ec8681a3":"# Standard Scaling\nskf = StratifiedKFold(n_splits = 10,random_state=42,shuffle=True)\n\ncategorical = [f for f in training.columns if training[f].dtype == object]\nnumeric = [f for f in X.columns if f not in categorical+['Id','Attrition','Behaviour','PerformanceRating']]\n\npre_pipe = make_column_transformer((OneHotEncoder(),categorical),(StandardScaler(),numeric))","b031d7c0":"pipe_rf = make_pipeline(pre_pipe,RandomForestClassifier())\npipe_xgb = make_pipeline(pre_pipe,XGBClassifier())\npipe_svc = make_pipeline(pre_pipe,SVC(probability=True))\n\n\nprint('RF: ',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_rf,scoring='roc_auc')))\nprint('XGB: ',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_xgb,scoring='roc_auc')))\nprint('SVC:',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_svc,scoring='roc_auc')))","1a9d7904":"n = 46\npipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=True,C = 1,kernel='rbf'))\nprint('SVC: ',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_svc,scoring='roc_auc')))\n\nplt.figure(figsize=(10,8))\npipe_svc.fit(X,y)\nplt.plot(range(1,n+1),pipe_svc.named_steps['pca'].explained_variance_ratio_.cumsum())\nplt.xticks(range(1,n+1,2))\nplt.title('Explained Variance')\nplt.grid()\nplt.show()","8f1a6ce8":"n = 34\npre_pipe = make_column_transformer((OneHotEncoder(),categorical),(StandardScaler(),numeric),remainder = 'passthrough')\npipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=True,C = 1,kernel='rbf'))\nprint('SVC: ',np.mean(cross_val_score(X=X,y=y,cv=skf,estimator=pipe_svc,scoring='roc_auc')))","a3994253":"n = 34\npre_pipe = make_column_transformer((OneHotEncoder(),categorical),(StandardScaler(),numeric),remainder = 'passthrough')\npipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=True,C = 1,kernel = 'rbf'))\n\nparam_grid = {\n    \n    'svc__C':[0.001,0.01,0.1,1,10,100,1000],\n    'svc__gamma': ['auto','scale'],\n    'svc__class_weight': ['balanced',None]\n}    \n\ngrid_search = GridSearchCV(pipe_svc,param_grid=param_grid,cv = skf, verbose=2, n_jobs = -1,scoring='roc_auc')\ngrid_search.fit(X,y)\nprint('Best score ',grid_search.best_score_)\nprint('Best parameters ',grid_search.best_params_)\nbest_svc = grid_search.best_estimator_","9a45f593":"pipe_svc = make_pipeline(pre_pipe,PCA(n_components=n),SVC(probability=True,C = 1,kernel='rbf',class_weight=None,gamma='auto'))\nparam_grid={\n    'svc__C':[0.01,0.03,0.05,0.07,0.1,0.3,0.5,0.7,1]  \n}\ngrid_search = GridSearchCV(pipe_svc,param_grid=param_grid,cv = skf, verbose=2, n_jobs = -1,scoring = 'roc_auc')\ngrid_search.fit(X,y)\nprint('Best score ',grid_search.best_score_)\nprint('Best parameters ',grid_search.best_params_)\nbest_svc = grid_search.best_estimator_ # final model - 0.808 private LB*","f36ce3df":"best_svc.fit(X,y)\nprediction = best_svc.predict_proba(test_data)[:,1]\nsubmission = pd.DataFrame(prediction,columns=['Attrition'])\nsubmission['Id'] = test_id\nsubmission = submission[['Id','Attrition']]\nsubmission.to_csv('submissionfile_postcomp.csv',index = None)","37389461":"Submission","1112a6bb":"# Importing Packages","4c52803c":"We can also use a cross validation strategy such as stratified k-fold which keeps the distribution of our target variable (here Attrition) similar across the folds. The training and validation data is split using stratified sampling instead of random sampling. The stratas here are the two values of our target variable. If you don't understand what this means then don't worry about it just remember that it is an effective method to tackle imbalanced datasets while we train our model. You can learn more about it in the scikit-learn user guide for cross validation.","8079e007":"SVC performs best here - trying out PCA","b6184fd3":"**If you liked this notebook and learnt something new from it do give an upvote. You can also checkout my [blog on medium](http:\/\/https:\/\/medium.com\/@mishraarpan6) where I have given an in-depth explanation about Support Vector Machines and have provided a deeper dive into the code here with links to all the documentaion.**","0eeeeb8f":"Lets check the distribution of out target variable Attrition.","096223a5":"34 components are explaining 100% of the variance","c1f08d14":"Testing on 3 candidate models: Random Forest, XGBoost, Support Vector Machines","bbf82f30":"Now our training data has 1000 data points and the target variable is imbalanced. There are many ways to tackle imbalanced data sets like upsampling or downsampling using SMOTE.","d37d5c74":"We have 1628 observations and 29 features, out of which 22 are integers and 7 are objects. Some of the integer data type features might also be categorical. We have to predict Attrition which can either be 0 or 1 (1 if the employee left the company).","be137bd6":"On exploring a little further I found that the feature PerformanceRating has only two values, 3 or 4 so I have mapped them to 0 and 1 respectively.","ce2b9303":"# Basic EDA","ffbfb885":"# Pre-Processing, Training and Validation","f983721a":"# Loading Datasets","34cc89ac":"Tuning SVC - using Grid Search","f44b8799":"\nWe will follow the following steps now:\n\n- Drop performance rating (on exploration I found that 85% values were of a single class this might lead to overfitting)\n- One Hot Encode all the 'object' data type features\n- Use standard scaling on all the integer data type features.\n- Use the pre-processed data and split it using Stratified K-Fold.\n- Fit and validate with 3 candidate models: Random Forest, XGBoost and Support Vector Classifier","a472beb3":"We can tune it further","27d84108":"We have more or less a balanced distribution. Lets check for duplciates","fcb514f3":"To preserve the original training data I have made a copy of it.","87921d2b":"Alright, so the data has 628 duplicates and all the duplicates correspond to Attrition 1. So that means that data was oversampled to make it balanced. We will drop the duplicates now and check the distribution again.","08d2eba6":"The weird thing to note here is that Behaviour has 0 standard deviation and mean=min=max = 1. This means that this columns has a value of 1 for all the observations, so we will drop it. We will also drop Id because it will have unique values for all the employees."}}