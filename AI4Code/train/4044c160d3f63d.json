{"cell_type":{"078c5a73":"code","b3bec5e8":"code","2f601ffb":"code","a71e322b":"code","bd92cd4a":"code","07feb0da":"code","d8dfd33d":"code","e6579073":"code","d7792c60":"code","7d2afcc3":"code","533d8a5c":"code","f72dc968":"code","ed87f118":"code","b5865c43":"code","e6384dfe":"code","220f0841":"code","075d03a9":"code","8918c1f9":"code","6459b551":"code","4caacdf3":"code","6140e17a":"code","02d04741":"code","d26bc3eb":"code","b08bd700":"code","902ab4e8":"code","dd2942c4":"code","2194ddda":"code","68a15739":"code","9625aa43":"markdown","08506014":"markdown","5acfca52":"markdown","109c8990":"markdown","31d52a2c":"markdown","61ff9570":"markdown","b95f262c":"markdown","a3ee2289":"markdown","cdc1bdd1":"markdown","db187cb1":"markdown","767a22a7":"markdown","1a474744":"markdown","b09c4f40":"markdown","b967ef14":"markdown","6c128728":"markdown","dff56eb8":"markdown","2af5db10":"markdown","2c19c477":"markdown","7bb802f4":"markdown","e94813d7":"markdown","debb8218":"markdown","0357c4e8":"markdown","6137187e":"markdown","82d1de15":"markdown","38eb50ef":"markdown","44c54329":"markdown","0baa3602":"markdown","956540af":"markdown"},"source":{"078c5a73":"#import the packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import preprocessing \n%matplotlib inline","b3bec5e8":"#load the data\ndata_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', sep=',')\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', sep=',')\n\nprint (f\"The train data has the empty values {data_train.isnull().values.sum()} of total {len(data_train.index)} rows\")\nprint (f\"The test data has the empty values {data_test.isnull().values.sum()} of total {len(data_test.index)} rows\")","2f601ffb":"def check_dublicates (df):\n    unic = len(set(df.Id))\n    total = df.shape[0]\n    is_dublicates = unic - total\n    if is_dublicates:\n        return 'There are {is_dublicates} dublicate records'","a71e322b":"check_dublicates(data_train)\ncheck_dublicates(data_test)\ndata_test['SalePrice'] = np.zeros((len(data_test.index),1))\njoined_data = pd.concat((data_train, data_test)).reset_index(drop=True)\nprint (f\"The joined data has the empty values {joined_data.isnull().values.sum()} of total {len(joined_data.index)} rows\")","bd92cd4a":"joined_data.head(5)","07feb0da":"missing_report = joined_data[joined_data.columns[joined_data.isnull().sum()!=0]]\n(missing_report.isnull().sum() \/ missing_report.shape[0]).sort_values(ascending=False)","d8dfd33d":"joined_data = joined_data.drop(['PoolQC', 'MiscFeature', 'Alley'], axis=1)","e6579073":"#[val for val in categorical if val in checking_categorical_data (joined_data )]\nimport operator\nres ={col: joined_data[ joined_data[col]==0 ][col].count() \/ joined_data[col].shape[0] \n     for col in joined_data if joined_data[ joined_data[col]==0 ][col].count() \/ joined_data[col].shape[0] > 0.9}\ndict(sorted(res.items(), key=operator.itemgetter(1),reverse=True))","d7792c60":"joined_data = joined_data.drop([ 'PoolArea', '3SsnPorch', 'LowQualFinSF', 'MiscVal', 'BsmtHalfBath', 'ScreenPorch' ], axis=1)","7d2afcc3":"nominal_var = [\n    'MSSubClass','MSZoning','Street',\n    'LotShape','LandContour','Utilities',\n    'LotConfig','LandSlope','Neighborhood',\n    'Condition1','Condition2','BldgType',\n    'HouseStyle','RoofStyle','RoofMatl',\n    'Exterior1st','Exterior2nd','MasVnrType',\n    'Foundation','Heating','CentralAir',\n    'BsmtFullBath','FullBath','HalfBath',\n    'TotRmsAbvGrd','Functional','Fireplaces',\n    'PavedDrive','MoSold','SaleType',\n    'SaleCondition','GarageCars',\n]\nordinal_var = [\n    'ExterQual','ExterCond','BsmtQual','BsmtCond',\n    'BsmtExposure','BsmtFinType2','HeatingQC','Electrical',\n    'FireplaceQu','GarageType','GarageFinish','GarageQual',\n    'GarageCond','Fence','YrSold','YearBuilt','YearRemodAdd',\n    'GarageYrBlt','BsmtFinType1', 'BedroomAbvGr', 'KitchenQual',\n    'KitchenAbvGr'\n]\nnumeric_var=[\n    'LotFrontage','LotArea','OverallQual','OverallCond',\n    'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n    'TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea',\n    'GarageArea','WoodDeckSF','OpenPorchSF',\n    'EnclosedPorch'\n]","533d8a5c":"def checking_categorical_data ( dataframe ):\n    list_ord = []\n    for feature in dataframe.columns:\n        if 1.*dataframe[feature].nunique()\/dataframe[feature].count() < 0.05:\n            list_ord.append( feature )\n    return list_ord\n\ncategorical = nominal_var + ordinal_var\nres = checking_categorical_data (joined_data )\n#list(set(categorical) - set(res)), list(set(joined_data.columns)-set(categorical+numeric_var))","f72dc968":"## Replaced all missing values in LotFrontage by imputing the median value of each neighborhood. \n#joined_data['LotFrontage'] = joined_data.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))    \n#joined_data['MSSubClass'] = joined_data['MSSubClass'].astype(str)\n#joined_data['MSZoning'] = joined_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))    \n#joined_data['YrSold'] = joined_data['YrSold'].astype(str)\n#joined_data['MoSold'] = joined_data['MoSold'].astype(str) \n#joined_data['Functional'] = joined_data['Functional'].fillna('Typ') \n#joined_data['Utilities'] = joined_data['Utilities'].fillna('AllPub') \n#joined_data['Exterior1st'] = joined_data['Exterior1st'].fillna(joined_data['Exterior1st'].mode()[0]) \n#joined_data['Exterior2nd'] = joined_data['Exterior2nd'].fillna(joined_data['Exterior2nd'].mode()[0])\n#joined_data['KitchenQual'] = joined_data['KitchenQual'].fillna(\"TA\") \n#joined_data['SaleType'] = joined_data['SaleType'].fillna(joined_data['SaleType'].mode()[0])\n#joined_data['Electrical'] = joined_data['Electrical'].fillna(\"SBrkr\")     \n\nfor var in ordinal_var:\n    joined_data[var] = joined_data[var].fillna('NA')\nfor var in numeric_var:\n    joined_data[var] = joined_data[var].fillna(0)\n#for var in numeric_var:\n#    joined_data[var]=joined_data[var].apply(lambda x: x if x else joined_data[var].mean())\nfor var in nominal_var:\n    joined_data[var] = joined_data[var].fillna('NA')\n\nbefore_encoding_joined_data = joined_data.copy()\n#Check remaining missing values if any \nprint(joined_data.isnull().values.sum())","ed87f118":"def one_hot_encoding ( dataframe, feature_to_encode ):\n    for cat in feature_to_encode:\n        dummies = pd.get_dummies( dataframe[ [ cat ] ] )\n        dataframe = pd.concat( [ dataframe, dummies ], axis=1 )\n        dataframe.drop( cat, axis=1, inplace=True )\n    return dataframe \n\ndef labeled_encoding ( dataframe, feature_to_encode ):\n    for col in feature_to_encode:\n        labelencoder = LabelEncoder()\n        dataframe[col+'_Cat'] = labelencoder.fit_transform( dataframe[col].astype(str) )\n        dataframe.drop( col, axis=1, inplace=True )\n    return dataframe\n\ndef normalization_columns ( dataframe, feature_to_encode ):\n    for col in feature_to_encode:\n        min_max_scaler = preprocessing.MinMaxScaler()\n        x = dataframe[[col]].values.astype(float)\n        x_scaled = min_max_scaler.fit_transform(x)\n        dataframe[col] = pd.DataFrame(x_scaled)\n    return dataframe    \n\njoined_data = one_hot_encoding( joined_data, nominal_var ) #one_hot_encoding\njoined_data = one_hot_encoding( joined_data, ordinal_var )\n#joined_data = normalization_columns ( joined_data, numeric_var )\njoined_data.head()","b5865c43":"print (f\"After handling the joined_data has {len(joined_data.columns)} column of total {len(joined_data.index)} rows\")","e6384dfe":"def show_scatter( otput_col, input_var, df ):\n    for col in input_var:\n        plt.subplots(figsize = (12,8))\n        res = df[df[otput_col] > 0]\n        sns.scatterplot(res[otput_col], res[col])\n        \ndef show_resid( otput_col, input_var, df ):\n    for col in input_var:\n        plt.subplots(figsize = (12,8))\n        res = df[df[otput_col] > 0]\n        sns.residplot(res[col], res[otput_col])  \n        \nshow_scatter('SalePrice', numeric_var, joined_data)","220f0841":"outlines = {\n    'LotFrontage':0.9, \n    'LotArea':0.7, \n    'BsmtFinSF1':0.7, \n    'TotalBsmtSF':0.7, \n    '1stFlrSF':0.7, \n    'GrLivArea':1, \n    'WoodDeckSF':0.9, \n    'OpenPorchSF':0.9, \n    'EnclosedPorch' : 0.9\n}\n\njoined_data.reset_index(drop = True, inplace = True)\n    \n#for key, value in outlines.items():\n#    joined_data[key] = joined_data[key].apply( lambda x: x if x < value else joined_data[key].median() )\nshow_scatter('SalePrice', numeric_var, joined_data)","075d03a9":"show_resid('SalePrice', numeric_var, joined_data)","8918c1f9":"plt.figure(figsize = (24,12))\nplt.subplot(121)\nsns.distplot(joined_data[joined_data['SalePrice']>0]['SalePrice'])\nplt.subplot(122)\nstats.probplot(joined_data[joined_data['SalePrice']>0]['SalePrice'],plot=plt)","6459b551":"plt.figure(figsize = (24,12))\nplt.subplot(121)\nsns.distplot( np.log(joined_data[joined_data['SalePrice']>0]['SalePrice']) )\nplt.subplot(122)\nstats.probplot( np.log(joined_data[joined_data['SalePrice']>0]['SalePrice']), plot=plt )","4caacdf3":"plt.figure(figsize = (24,12))\nplt.subplot(121)\nsns.distplot( (joined_data[joined_data['SalePrice']>0]['SalePrice'])**0.5 )\nplt.subplot(122)\nstats.probplot( (joined_data[joined_data['SalePrice']>0]['SalePrice'])**0.5, plot=plt )","6140e17a":"joined_data['SalePrice'] = np.log(joined_data[joined_data['SalePrice']>0]['SalePrice']) \nshow_resid('SalePrice', numeric_var, joined_data)","02d04741":"skewed_feats = joined_data[numeric_var].apply(lambda x: skew(x)).sort_values(ascending=False)\nprint(\"\\nSkew in numerical parameters: \\n\")\nskewness = pd.DataFrame({'Skew' : skewed_feats})\nskewness.head(10)","d26bc3eb":"skewness = skewness[abs(skewness) > 0.75]\nskewed_features = skewness.index\nlam = 0.15\nbefore_skewed = joined_data.copy()\nfor col in numeric_var:\n    joined_data[col] = boxcox1p(joined_data[col], boxcox_normmax(joined_data[col] + 1))","b08bd700":"numeric_var=[\n    'LotFrontage','LotArea','OverallQual','OverallCond',\n    'MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n    'TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea',\n    'GarageArea','WoodDeckSF','OpenPorchSF',\n    'EnclosedPorch'\n]\n#joined_data['OverallQual'] = np.log1p(joined_data['OverallQual'])\n#joined_data['OverallCond'] = np.log1p(joined_data['OverallCond'])\n#joined_data['BsmtUnfSF'] = np.log1p(joined_data['BsmtUnfSF'])\n#joined_data['GrLivArea'] = np.log1p(joined_data['GrLivArea'])\n#joined_data['GarageArea'] = np.log1p(joined_data['GarageArea'])\n#joined_data['WoodDeckSF'] = np.log1p(joined_data['WoodDeckSF'])\n#joined_data['OpenPorchSF'] = np.log1p(joined_data['OpenPorchSF'])\n#joined_data['EnclosedPorch'] = np.log1p(joined_data['EnclosedPorch'])\n#joined_data['KitchenAbvGr'] = np.log1p(joined_data['KitchenAbvGr'])\n#joined_data = joined_data.drop([ 'KitchenAbvGr' ], axis=1)\n#joined_data[var].apply(lambda x: x if x else joined_data[var].mean())\n#joined_data['MasVnrArea'] = joined_data['MasVnrArea'].apply(lambda x: x if x else joined_data['MasVnrArea'].mean())\n#print(joined_data[joined_data['MasVnrArea'] == 0]['MasVnrArea'].sum())\n#sns.distplot(joined_data['KitchenAbvGr'], kde_kws={\"label\":\"before transformation\"})\n#sns.distplot(np.log1p(joined_data['KitchenAbvGr']), kde_kws={\"label\":\"after transformation\"})\n#sns.distplot(joined_data['BsmtFinSF2'])","902ab4e8":"#Best heatmap, the code below - https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing\n## Plot fig sizing. \nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(before_encoding_joined_data.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(before_encoding_joined_data.corr(), \n            cmap=sns.diverging_palette(20, 220, n=200), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","dd2942c4":"joined_data['is_garage'] = joined_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\njoined_data['is_bsmt'] = joined_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\njoined_data['is_second_floor'] = joined_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n","2194ddda":"from sklearn import ensemble\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression, RidgeCV, Ridge, Lasso, LassoCV, ElasticNet\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\ntrain = joined_data[joined_data['SalePrice']>0]\ny = train['SalePrice'].values.tolist()\ny_tr = train['SalePrice']\ntrain = train.drop('SalePrice', axis=1 )\nX = train.values.tolist()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n\nreg_rgl = np.linspace(-0.1, 3, num=100) \nreg_lso = np.linspace(-5, 5, num=1000) \nreg_els = np.linspace(-5, 5, num=1000)\nreg_lin = np.linspace(-5, 5, num=100)\n\nscore_ridge   = {}\nscore_lasso   = {}\nscore_elastic = {}\nscore_linear  = {}\n\ndef get_key(d, value):\n    for k, v in d.items():\n        if v == value:\n            return k\n\n\nfor i in reg_lin:\n    linear = LinearRegression()\n    linear.fit(X_train, y_train)\n    score_linear[i] = mean_squared_error(y_test, linear.predict(X_test))\n\nfor i in reg_rgl:\n    ridge = Ridge(alpha= i, normalize=True)\n    ridge.fit(X_train, y_train)\n    score_ridge[i] = mean_squared_error(y_test, ridge.predict(X_test))\n\nfor i in reg_lso:\n    lasso_reg = Lasso(alpha= i, normalize=True)\n    lasso_reg.fit(X_train, y_train)\n    score_lasso[i] = mean_squared_error(y_test, lasso_reg.predict(X_test))\n\nfor i in reg_els:\n    elastic_net = ElasticNet(alpha= i, normalize=True)\n    elastic_net.fit(X_train, y_train)\n    score_elastic[i] = mean_squared_error(y_test, elastic_net.predict(X_test))    \n    \nBayRidge = linear_model.BayesianRidge(normalize=True)\nBayRidge.fit(X_train, y_train)\nBayRidge_res = mean_squared_error(y_test, BayRidge.predict(X_test))\n    \nsvr = SVR(C = 20, epsilon= 0.008, gamma=0.0003,)\nsvr.fit(X_train, y_train)\nSVR_res = mean_squared_error(y_test, y_pred = svr.predict(X_test)) \n\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=1.5)\nKRR.fit(X_train, y_train)\nKRR_res = mean_squared_error(y_test, y_pred = KRR.predict(X_test)) \n\nprint(\"Best results for:\")\nprint( f\"KernelRidge - loss {KRR_res}\")\nprint( f\"Ridge - loss {min(score_ridge.values())}, koeff -{  get_key(score_ridge, min(score_ridge.values())) } \")\nprint( f\"Lasso - loss {min(score_lasso.values())}, koeff -{  get_key(score_lasso, min(score_lasso.values())) } \")\nprint( f\"Elastic - loss {min(score_elastic.values())}, koeff -{  get_key(score_elastic, min(score_elastic.values())) } \")\nprint( f\"svr - loss {SVR_res}\")\nprint( f\"BayRidge - loss {KRR_res}\")\n\n#print( f\"Best results for linear - {min(score_lasso.values())}\" )\n#print( f\"Best results for KernelRidge - {KRR_res} linear - {min(score_linear.values())}, \\n ridge - {min(score_ridge.values())}, \\n lasso - {min(score_lasso.values())}, \\n elastic -  {min(score_elastic.values())} \\n bayes_reg -  {BayRidge_res}\\n SVR - {SVR_res}\")","68a15739":"sub = pd.DataFrame()\ntest = joined_data[joined_data['SalePrice'].isnull()]\ntest = test.drop( 'SalePrice', axis=1 )\nsub['Id'] = test['Id']\nsub[\"SalePrice\"] =  np.exp(lasso_reg.predict(test.values.tolist()))\nsub.to_csv('submission.csv',index=False)\n","9625aa43":"Obviously, the first transformation is better, so we get normality of output parameter - SalePrice. Redisplay residplot for numeric parameters.We may observe same plots with homoscedastic, it's very nice result! ","08506014":"Indeed, the most cases display the linearity with big noises, we could delete some points are in out range many values. For LotFrontage >0.9, LotArea > 0.7, BsmtUnfSF > 0.7, TotalBsmtSF > 0.7, 1stFLrSF > 0.7, GrLivArea > 1, WoodDeckSF > 0.9, OpenPorchSF > 0.9, EnclosedPorch > 0.9.      ","5acfca52":"In this case, we don't see strong correlations between parameters excepting some property such as GarageArea vs GarageCars and etc. Anyway, we will modeling the predictor by linear regression with lasso and ridge regularization.   ","109c8990":"Checking the categorial parameters by frequency of same values in colunms","31d52a2c":"Let's fill in each missing value by an appropriate value. We could be used the different techniques of defining the types of parameters. For example, the categorical variables are defined by frequency of same values in colunms (if dataframe[feature].nunique()\/dataframe[feature].count() < 0.05), but not at this time. In this case, it need to consider each parameters, because we face to different type of categorical variable such as ordinal and nominal.\nThere are three types of paramerets:\n* nominal variables with one-hot encoding ( MSSubClass (moda), MSZoning (moda), Street (moda), LotShape (moda), LandContour (moda), Utilities (moda), LotConfig (moda), LandSlope (moda), Neighborhood (moda), Condition1 (moda), Condition2 (moda), BldgType (moda), HouseStyle ( moda ), RoofStyle (moda), RoofMatl (moda), Exterior1st (moda), Exterior2nd (moda), MasVnrType (None), Foundation (moda), Heating (moda), CentralAir (moda), BsmtFullBath (moda), FullBath (moda), HalfBath (moda), TotRmsAbvGrd (moda), Functional (moda), Fireplaces (moda), PavedDrive (moda), MoSold (moda), SaleType (moda), SaleCondition (moda), GarageCars (moda) );\n* ordinal variables with labeled encoding ( ExterQual (NA), ExterCond (NA), BsmtQual (NA), BsmtCond (NA), BsmtExposure (NA), BsmtFinType2 (NA), HeatingQC (NA), Electrical (NA), FireplaceQu (NA), GarageType (NA), GarageFinish (NA), GarageQual (NA), GarageCond (NA), Fence (NA), YrSold (NA), YearBuilt (NA), YearRemodAdd (NA), GarageYrBlt (NA), BedroomAbvGr (NA), KitchenQual (NA) );\n* numeric variables ( LotFrontage (mean), LotArea(mean), OverallQual(mean), OverallCond(mean), MasVnrArea (mean), BsmtFinSF1 (mean), BsmtFinSF2 (mean), BsmtUnfSF (mean), TotalBsmtSF (mean), 1stFlrSF (mean), 2ndFlrSF (mean), GrLivArea (mean), GarageArea (mean), WoodDeckSF (mean), OpenPorchSF (mean), EnclosedPorch (mean), KitchenAbvGr(mean), BsmtFinType1(mean) ); \n","61ff9570":"Therefore, we need to research the ouput variable (SalePrice) in context of normality distributions. Let's display the gistrogram for SalePrice to undertand type of distribution. ","b95f262c":"Badly result! SalePrice haven't normal distribution, therefore qq-polt not like as \n![image.png](attachment:image.png)","a3ee2289":"Before the joining, we need check for the dublicates.","cdc1bdd1":"To time encode these parameters: nominal variables with one-hot encoding,ordinal variables with labeled encoding. ","db187cb1":"Now, we have PoolQC, MiscFeature and Alley with 90 and more percentages of missing values. Recall about those parameters: PoolQC ( Pool quality ), MiscFeature( Miscellaneous feature not covered in other categories ). Therefore, it could be hendled all empty values, but the parameters with 90 and more percentages of missing values will be deleted. ","767a22a7":"In most cases variance increases with growing input parameter, therefore we see other pictire - scedasticity (http:\/\/www.statistics4u.com\/fundstat_eng\/cc_scedasticity.html).\nWhen developing models of measured data it is generally important to analyse the residuals. One important aspect is whether the residuals vary with the signal level, i.e. if the random part of the signal grows with increasing signal level. We therefore distinguish two cases:\n* **the homoscedastic case: the random part is constant and exhibits a normal distribution**\n* **the heteroscedastic case: the random part depends on the signal amplitude**\n\nThe following figure shows how one can determine whether a signal contains heteroscedastic noise. The noisy signal is compared to a noise-free model function by caclulating the difference of the two signals. If this difference is plotted against the amplitude of the noise-free signal the resulting diagram will reveal the type of the noise.\n![image](http:\/\/www.statistics4u.com\/fundstat_eng\/img\/heteroscedastic_sig.gif)","1a474744":"Anyway, we should be get normal distribution, there are many way to do it. Let's looked at (https:\/\/www.statisticssolutions.com\/transforming-data-for-normality\/)\n![image.png](attachment:image.png)","b09c4f40":"After then, we should transform input parameters in same manner, but how can we do it? On the one hand, all parameters should be analyzed in context skew is property normal distribution shows deviation of mean from median. Look at this.    \n![image](https:\/\/i.pinimg.com\/originals\/c1\/01\/b0\/c101b0da6ea1a0dab31f80d9963b0368.png)\nOn the other hand, I guess we need to observe all parameter to pick up transrormation types based on skew and form distribution is realted to mean or expectation. This is dirty job! At the same time, we have the super tool for this target. This is Box Cox Transformation is a way to transform non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn\u2019t normal, applying a Box-Cox means that you are able to run a broader number of tests (https:\/\/www.statisticshowto.com\/box-cox-transformation\/).    ","b967ef14":"Let's look at results before transformation and after.","6c128728":"The data analysis for empty values","dff56eb8":"Homoscedasticity - describes a situation in which the error term (that is, the \u201cnoise\u201d or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables (https:\/\/www.statisticssolutions.com\/homoscedasticity\/). To understand this effect is residplot which shows the variance under line, let's display resideplot for some numeric parameters.   ","2af5db10":"We have positive skew such as first two pictires with log and square root transrormations. Let's try both ways and comparises results. ","2c19c477":"It should be converted nominal_var to string as well as ordinal_var and numeric_var converted float type. At the same time, it will be filled missing parameters appropriate values (nominal variables with median, ordinal variables with NA and numetric mean).   ","7bb802f4":"Linearity(Correct functional form): Linear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present.","e94813d7":"# Introduction\n\nThe dataset has different variables such as categorical or numeric parameters, so our first target is to understand how th\u0435se variables are linked each other and how much the missing variable in the dataset and how to split the dataset into categorial and numeric types. \n# Plan\n* The data preparation\n* The data analysis for empty values\n* Designing the prediction model\n\n# 1. The data preparation\n\n* The joining of the train and test data \n* The data analysis for empty values\n* Encoding data with appropriate variable type ","debb8218":"## Creating New Features","0357c4e8":"Let's check the missing parameters with zero value ","6137187e":"It's difficult to handle and to analyz the train dataset because we need take account any categoryes that may not be inside the test dataset. In this case, we concates two dataset for handling all type variables.      ","82d1de15":"# 2. Study Design for Prediction Modeling\n* **It could be suggested linearity relation between input parameters and output parameter** \n* **If input parameter increases, error variance should be constant, it's called Homoscedasticity**\n* **Input parameters are independent between each other, so parameters errors are independent too**\n* **Multivariate Normality with normality of errors**\n* **Dataset have not multicollinearity.** ","38eb50ef":"Now, it's important to consider the multicollinearity for each parameters. For this reason, let's show the correlation map to see how parameters are correlated each other.\nIn statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy (https:\/\/en.wikipedia.org\/wiki\/Multicollinearity)., At the same time, Linear regression or multilinear regression requires independent variables to have little or no similar features. Let's look at heatmap is an excellent way to identify whether there is multicollinearity or not. The best way to solve multicollinearity is to use regularization methods like Ridge or Lasso.","44c54329":"# 3. Designing the prediction model\n* **Tuning hyperparameters for simple model predictions** \n* **If input parameter increases, error variance should be constant, it's called Homoscedasticity**\n* **Input parameters are independent between each other, so parameters errors are independent too**\n* **Multivariate Normality with normality of errors**\n* **Dataset have not multicollinearity.** ","0baa3602":"Now, we have PoolArea, 3SsnPorch, LowQualFinSF, MiscVal, BsmtHalfBath, ScreenPorch with 90 and more percentages of zero values, so we may delete these uninformative properties.    ","956540af":"Compute the Box-Cox transformation of 1 + x.\n\nThe Box-Cox transformation computed by boxcox1p is:\n\ny = ((1+x)**lmbda - 1) \/ lmbda  if lmbda != 0\n    log(1+x)                    if lmbda == 0\n(https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.boxcox1p.html)    "}}