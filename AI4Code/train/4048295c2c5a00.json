{"cell_type":{"f44daee5":"code","bc78ad01":"code","d02d9650":"code","ecf5d22e":"code","b20344fe":"code","15eb78ef":"code","632f9950":"code","fa12cdf3":"code","8566b789":"code","60f98971":"code","4ed49f8b":"code","58eec087":"code","d099cddf":"code","e14d5587":"code","e1315985":"markdown","19e7fec1":"markdown"},"source":{"f44daee5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bc78ad01":"# Kullan\u0131lacak k\u00fct\u00fcphaneler.\nimport pandas as pd\nimport re\nimport nltk \nfrom nltk.corpus import stopwords\nimport nltk as nlp\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nimport nltk as nlp\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier","d02d9650":"# csv format\u0131ndaki datam\u0131z\u0131 df de\u011fi\u015fkenine at\u0131yoruz\ndf = pd.read_csv(\"\/kaggle\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\",encoding = \"latin1\")\ndf.head()","ecf5d22e":"df.describe()\nprint(df.columns)","b20344fe":"del df[\"salary_range\"]\ndel df[\"job_id\"]\n","15eb78ef":"# datadaki metinleri tek bir text verisi haline getiriyoruz.\ndf.fillna(\" \",inplace = True)\ndf['text']=df['title']+\" \"+df['location']+\" \"+df['department']+\" \"+df['company_profile']+\" \"+df['description']+\" \"+df['requirements']+\" \"+df['benefits']+\" \"+df['employment_type']+\" \" +df['required_education']+\" \"+df['industry']+\" \"+df['function'] \ndf.head()","632f9950":"# datada gereksiz bilgileri siliyoruz.\ndel df['title']\ndel df['location']\ndel df['department']\ndel df['company_profile']\ndel df['description']\ndel df['requirements']\ndel df['benefits']\ndel df['employment_type']\ndel df['required_experience']\ndel df['required_education']\ndel df['industry']\ndel df['function']\n\ndf.head()","fa12cdf3":"# Cleaning Data:\ntext_list = []\nfor text in df.text:\n    text= re.sub(\"[^a-zA-Z]\",\" \",text) #a-z A-Z aral\u0131\u011f\u0131 d\u0131\u015f\u0131ndaki t\u00fcm ifadeleri bo\u015fluk ile de\u011fi\u015ftiriyoruz\n    text = text.lower()   # buyuk harftan kucuk harfe \u00e7eviriyoruz\n    text = nltk.word_tokenize(text) # textdeki her bir kelimeyi ay\u0131r\u0131yoruz\n    lemma = nlp.WordNetLemmatizer() \n    text = [lemma.lemmatize(word) for word in text] # for d\u00f6ng\u00fcs\u00fc ile textin i\u00e7indeki her kelimeyi k\u00f6klerine ay\u0131r\u0131yoruz\n    text = \" \".join(text)  # tek tek ay\u0131rd\u0131\u011f\u0131m\u0131z kelimeleri aralar\u0131na bo\u015fluk koyarak tekrar birle\u015ftiriyoruz\n    text_list.append(text) # olu\u015fturdu\u011fumuz t\u00fcm textleri bir listenin i\u00e7ine topluyoruz  ","8566b789":"# Bag of Words:\n \nmax_features = 150    # textde en \u00e7ok kulland\u0131lan 150 kelime.\n# stop_words ile ingilizce harici kelimeleri siliyoruz.\ncount_vectorizer = CountVectorizer(max_features=max_features, stop_words = \"english\") \n\n# metodumuzu textler \u00fczerinde fit ediyoruz ve sonucu bir liste haline getiriyoruz.   \nsparce_matrix = count_vectorizer.fit_transform(text_list).toarray() \n\nprint(\"en s\u0131k kullan\u0131lan {} kelimeler :{}\".format(max_features,count_vectorizer.get_feature_names()))\nx = sparce_matrix\ny = df.iloc[:,3].values ","60f98971":"# datam\u0131z\u0131 train ve test olarak 0.1 oran\u0131yla ay\u0131rd\u0131k.\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.1,random_state = 42)","4ed49f8b":"# Logistic Regression:\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"logreg accuracy {}\".format(lr.score(x_test,y_test)))\n\n","58eec087":"# Naive Bayes\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n# prediction \ny_pred = nb.predict(x_test)\n\nprint(\"nb accuracy:\",nb.score(y_pred.reshape(-1,1),y_test))\n","d099cddf":"# KNN \nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} knn score: {} \".format(3,knn.score(x_test,y_test)))\n","e14d5587":"score_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()\n\n\n","e1315985":"Bu \u00e7al\u0131\u015fmada i\u015f ilanlar\u0131n\u0131n ger\u00e7ek mi sahte mi oldu\u011funu 3 farkl\u0131 algoritma \u00fczerinden test etmeye \u00e7al\u0131\u015f\u0131caz. Kullan\u0131ca\u011f\u0131m\u0131z ML algoritmalar\u0131 s\u0131ras\u0131yla \u015fu \u015fekildedir. \n\n1) Logistic Regression \n2) Naive Bayes\n3) KNN ","19e7fec1":"Datam\u0131z\u0131 d\u00fczenli hale getirdik, \u015fimdi onu bir ML algoritmas\u0131na uygun hale getirmemiz gerekiyor. S\u0131ras\u0131yla uygulayaca\u011f\u0131m\u0131z ad\u0131mlar:\n\n1) Cleaning Data : Bu ad\u0131mda textde bulunan ifadeleri algoritmam\u0131z\u0131n anlaya\u011f\u0131 bir hale getirece\u011fiz. Mesela, \"!!merHAba\" gibi bir ifadeyi \"merhaba\" \u015fekline \u00e7evirece\u011fiz.\n\n2) Bag of Words : Texti ML uygulayacak hale getirece\u011fiz.\n\n3) Text Classification: Bu ad\u0131mda ise ML algoritmalar\u0131 ile varmak istedi\u011fimiz sonu\u00e7lar\u0131 ediniyoruz. "}}