{"cell_type":{"d9fa6892":"code","1be05459":"code","156758cb":"code","505270f2":"code","3961896a":"code","14516fe8":"code","a56886c2":"code","88394591":"code","cd0b972f":"code","15381908":"code","a80fd78b":"code","8899094f":"code","79e42aed":"code","3aaea584":"code","55bcc576":"code","4ff00e04":"code","6d245b31":"code","3296a6ab":"code","3386520c":"code","efbd3c4c":"code","e975bdb9":"code","f892e0a7":"code","9eb55df0":"code","b0d42735":"code","1b737bf3":"code","e75ba36a":"code","e42ff7c8":"code","2b2ac1ac":"code","6946fc9f":"code","a41b8579":"code","471a0180":"code","fd099af4":"code","0c1f975b":"code","a497a19e":"code","a6028d0e":"code","e4145408":"code","59bb8654":"code","1f8b9100":"code","7e193731":"code","3583223a":"code","824c31f6":"code","9788725c":"code","09c08bcd":"code","fb0a19d5":"code","56bbb771":"code","0045be63":"code","d4ee53b6":"code","161e082b":"code","fa5760da":"markdown","32f5bcf4":"markdown","7506f9cb":"markdown","60cefc78":"markdown","6b6b7bb5":"markdown","649dbd2c":"markdown","f57c1912":"markdown","1d09c2f6":"markdown","d24bf50d":"markdown","427f473b":"markdown","b95f56cb":"markdown","376bdad7":"markdown","1ab9d63d":"markdown","eb67048a":"markdown","843058cd":"markdown","05d0600f":"markdown","0ec26f3a":"markdown"},"source":{"d9fa6892":"#In this notebook,we  are going use many time Series Techniques,ARIMA Model(Auto regression),ARIMAX Model\n#Auto Arima and analysing trends","1be05459":"#importing Liabraries\nimport numpy as np \nimport pandas as pd \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly as py\nfrom plotly import tools\nfrom plotly.offline import iplot\nimport matplotlib.pyplot as plt\nfrom matplotlib import dates\nimport seaborn as sns\nfrom math import sqrt\nimport statsmodels.api as sm\n#from pmdarima.arima import auto_arima\nfrom statsmodels.tsa.stattools  import adfuller\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import acovf, acf\nfrom statsmodels.graphics.tsaplots import plot_acf\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n#Graph Settings\nplt.rcParams['figure.figsize'] = (15,4)\nplt.rcParams['axes.grid'] = True\nplt.rcParams['font.size'] = 14","156758cb":"#Importing Packages\n!pip install pmdarima\nfrom pmdarima import auto_arima","505270f2":"#Loading Data\ndt = pd.read_csv(\"..\/input\/pakistans-largest-ecommerce-dataset\/Pakistan Largest Ecommerce Dataset.csv\", parse_dates=[\"created_at\", \"Working Date\"], low_memory=False)","3961896a":"#Basic EDA\ndt.info()","14516fe8":"print(\"Data Dimensions are: \", dt.shape)\nprint(\"Columns: \", dt.columns)","a56886c2":"dt.rename(columns = {' MV ':'MV'}, inplace = True)\n#Droping Null Columns\ndt = dt.iloc[:, :-5]\ndt = dt.dropna(how = 'all') \ndt.columns","88394591":"#Casting on Data types\ndt['Customer ID'] = dt['Customer ID'].astype(str)\ndt['item_id'] = dt['item_id'].astype(str)\ndt['qty_ordered'] = dt['qty_ordered'].astype(int)  \ndt['Year'] = dt['Year'].astype(int)  \ndt['Month'] = dt['Month'].astype(int) ","cd0b972f":"#Data Summary\ndt.describe()","15381908":"dt = dt.sort_values('created_at')\n#Adding New Features\ndtg = dt.groupby('created_at')['grand_total'].sum().reset_index()\ndtq = dt.groupby('created_at')['qty_ordered'].sum().reset_index()\ndtd = dt.groupby('created_at')['discount_amount'].sum().reset_index()\n#Comput count for non numeric values\ndts = dt.groupby('created_at')['sku'].count().reset_index() \ndtst = dt.groupby('created_at')['status'].count().reset_index()\n# new data set\np = pd.DataFrame(dtg) \np['qty_ordered'] = dtq['qty_ordered']\np['discount_amount'] = dtd['discount_amount']\np['sku'] = dts['sku']\np['status'] = dtst['status']\n#Cumulative Sum\np['cum_grand_total'] = p['grand_total'].cumsum()\np['cum_qty_ordered'] = p['qty_ordered'].cumsum()\np['cum_discount_amount'] = p['discount_amount'].cumsum()\np['cum_sku_cnt'] = p['sku'].cumsum()\np['cum_status_cnt'] = p['status'].cumsum()\n# Date features\np['Dateofmonth'] = p['created_at'].dt.day\np['Month'] = p['created_at'].dt.month\np['Week'] = p['created_at'].dt.week\np['Dayofweek'] = p['created_at'].dt.dayofweek # 0 = monday.\np['Weekdayflg'] = (p['Dayofweek'] \/\/ 5 != 1).astype(float)\np['Month'] = p['created_at'].dt.month\np['Quarter'] = p['created_at'].dt.quarter\np['Dayofyear'] = p['created_at'].dt.dayofyear","a80fd78b":"p.head()","8899094f":"#selecting first two column\np = p.iloc[:, 0:2]  \np.head()","79e42aed":"#Converting this column to datetime\np['created_at'] = pd.to_datetime(p['created_at'],format='%Y-%m-%d')","3aaea584":"#Accessing any specific date\np.loc[1, 'created_at'].day_name()\n#We can also convert dates into days\np['created_at'].dt.day_name()","55bcc576":"p.head()","4ff00e04":"#For Time Series,Date column must be in indexes\np = p.set_index('created_at')\nfrom datetime import datetime\np.head()","6d245b31":"#Plotting graph between year and Total sales\nsns.lineplot(x = \"created_at\" , y = \"grand_total\",data= p )\nplt.show()","3296a6ab":"#Checking Dataset is stationary or not\nresults = adfuller(p['grand_total'])\nprint(results)","3386520c":"#We can also print p-value,Test statistics and Critical values as follows\nresults = adfuller(p['grand_total'])\nprint('Test Statistics: %f' % results[0])\nprint('p-value: %f' % results[1])\nprint('Critical Values: ')\nfor key, value in results[4].items():\n    print('\\t%s: %.3f' % (key,value))","efbd3c4c":"#1.Difference with diff()\ndf_stationary = p.diff().dropna()\nresult = adfuller(df_stationary)\nprint(result)","e975bdb9":"indexeddataset_logscale = np.log(p['grand_total'])\nresult = adfuller(indexeddataset_logscale)\nprint(result)","f892e0a7":"datasetLogDiffShifting = indexeddataset_logscale - indexeddataset_logscale.shift()\ndatasetLogDiffShifting.dropna(inplace=True)\nresult = adfuller(datasetLogDiffShifting)\nprint(result)","9eb55df0":"movingaverage = indexeddataset_logscale.rolling(window=12).mean()\nmovingstd = indexeddataset_logscale.rolling(window=12).std()\ndatasetlogscaleminusmovingaverage = indexeddataset_logscale - movingaverage\ndatasetlogscaleminusmovingaverage.dropna(inplace=True)\nresult = adfuller(datasetlogscaleminusmovingaverage)\nprint(result)","b0d42735":"plot = plt.plot(datasetlogscaleminusmovingaverage,color='blue',label='Original')\nmean = plt.plot(movingaverage, color='red',label='Rolling Mean')\nstd = plt.plot(movingstd, color='Black',label='Rolling STD')\nplt.legend(loc='best')\n#plt.title()\nplt.show(block=True)","1b737bf3":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(datasetlogscaleminusmovingaverage)\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\nplt.figure(figsize=(12,7))\nplt.subplot(411)\nplt.plot(datasetlogscaleminusmovingaverage, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend,label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual,label='Residuals')\nplt.legend(loc='best')","e75ba36a":"datasetlogscaleminusmovingaverage.head()","e42ff7c8":"type(datasetlogscaleminusmovingaverage)","2b2ac1ac":"#Converting above from Series to dataframe\ndatasetlogscaleminusmovingaverage = pd.Series(datasetlogscaleminusmovingaverage)\nnew_dataset = datasetlogscaleminusmovingaverage.to_frame()\nnew_dataset.head()","6946fc9f":"type(new_dataset)","a41b8579":"#Autocorrelation\necom_df = new_dataset\nacf(ecom_df, fft=False)\nsm.graphics.tsa.plot_acf(ecom_df.values.squeeze(), lags=60);","471a0180":"sm.graphics.tsa.plot_pacf(ecom_df.values.squeeze(), lags=60,\n                          method='ywunbiased');\n\nsm.graphics.tsa.plot_pacf(ecom_df.values.squeeze(), lags=60,\n                         method='ols');","fd099af4":"# Change frequency to day\necom_df = new_dataset.asfreq('d')\n\n# Set style for seaborn plot\nsns.set_style('darkgrid')\n\n# Add automatic datetime converters\npd.plotting.register_matplotlib_converters()\n\n# Default figure size\nsns.mpl.rc('figure',figsize=(12, 6))\n\n# Plot daily max temps\nfig, ax = plt.subplots()\nax = ecom_df.plot(ax=ax)\n\n# Creating model \nmodel = AutoReg(ecom_df['grand_total'],30)\nmodel_fit = model.fit()\n\n# Define training and testing area\nlen(ecom_df) # 778 observations\ntrain_df = ecom_df.iloc[:620] \ntest_df = ecom_df.iloc[620:] \n\n# Define training model for 159 days \ntrain_model = AutoReg(ecom_df['grand_total'], 159).fit(cov_type=\"HC0\")\n\n# Define start and end for prediction \nstart = len(train_df)\nend = len(train_df) + len(test_df) - 1\nprediction = train_model.predict(start=start, end=end, dynamic=True)\n# Plot testing data with prediction\nax = test_df.plot(ax=ax) # Orange\nax = prediction.plot(ax=ax) # Green","0c1f975b":"sns.mpl.rc('figure',figsize=(12, 6))\nfig, ax = plt.subplots()\nax = ecom_df.plot(ax=ax)\nmodel = AutoReg(ecom_df['grand_total'], 30)\nmodel_fit = model.fit()\nlen(ecom_df) # 778 observations\ntrain_df = ecom_df.iloc[:620] # 80%\ntest_df = ecom_df.iloc[620:] # Last 20%\n# Define training model for 159 days \ntrain_model = AutoReg(ecom_df['grand_total'], 159).fit(cov_type=\"HC0\")\nstart = len(train_df)\nend = len(train_df) + len(test_df) - 1\nprediction = train_model.predict(start=start, end=end, dynamic=True)\nax = test_df.plot(ax=ax) \nax = prediction.plot(ax=ax) \n\n# Predict 100 days into the future\nforecast = train_model.predict(start=end, end=end+100, dynamic=True)\nforecast.plot(ax=ax) ","a497a19e":"mean_absolute_error(test_df, prediction)","a6028d0e":"ecom_df = new_dataset.asfreq('m')\necom_df.head()","e4145408":"# Set style for seaborn plot\nsns.set_style('darkgrid')\n\n# Add automatic datetime converters\npd.plotting.register_matplotlib_converters()\n\n# Default figure size\nsns.mpl.rc('figure',figsize=(12, 6))\n\n# Plot monthly max temps\nfig, ax = plt.subplots()\nax = ecom_df.plot(ax=ax)\n\n# Creating model \nmodel = AutoReg(ecom_df['grand_total'],10)\nmodel_fit = model.fit()\n\n# Define training and testing area\nlen(ecom_df) # 25 observations\ntrain_df = ecom_df.iloc[:16] \ntest_df = ecom_df.iloc[16:] \n\n# Define training model for 5 days \ntrain_model = AutoReg(ecom_df['grand_total'], 5).fit(cov_type=\"HC0\")\n\n# Define start and end for prediction \nstart = len(train_df)\nend = len(train_df) + len(test_df) - 1\nprediction = train_model.predict(start=start, end=end, dynamic=True)\n\n# Predict 4 months into the future\nforecast = train_model.predict(start=end, end=end+4, dynamic=True)\nforecast.plot(ax=ax)","59bb8654":"mean_absolute_error(test_df, prediction)","1f8b9100":"new_dataset = datasetlogscaleminusmovingaverage.to_frame()\necom_df = new_dataset.asfreq('d')\necom_df.head()","7e193731":"ecom_df.plot(figsize=(16,8))\n# We verify that there is a seasonal component\nres = seasonal_decompose(ecom_df, model='add')\nres.plot()","3583223a":"#Auto Arima\nauto_arima(ecom_df, seasonal=True, m=12, trace=True).summary()","824c31f6":"train_df = ecom_df.iloc[:583] # 80%\ntest_df = ecom_df.iloc[583:] # 20%\nmodel = SARIMAX(train_df, order=(0, 1, 3),\n               seasonal_order=(1, 0, 1, 12))\nres = model.fit()\nstart = len(train_df)\nend = len(train_df) + len(test_df) - 1\nforecast = res.predict(start, end )\nforecast = pd.DataFrame(forecast,index = test_df.index,columns=['Prediction'])\nplt.plot(train_df, label='Train')\nplt.plot(test_df, label='Test')\nplt.plot(forecast, label='Prediction')\nplt.show()","9788725c":"# Predict the future\nmodel = SARIMAX(ecom_df, order=(0, 1, 3),\n               seasonal_order=(1, 0, 1, 12))\nres = model.fit()\nfuture = res.predict(len(ecom_df), len(ecom_df)+12, \n                    typ='levels').rename('Future')\ntest_df.plot(legend=True, figsize=(16,8))\nfuture.plot(legend=True)","09c08bcd":"dt = dt.sort_values('created_at')\n#Adding New Features\ndtg = dt.groupby('created_at')['grand_total'].sum().reset_index()\ndtq = dt.groupby('created_at')['qty_ordered'].sum().reset_index()\ndtd = dt.groupby('created_at')['discount_amount'].sum().reset_index()\n#Comput count for non numeric values\ndts = dt.groupby('created_at')['sku'].count().reset_index() \ndtst = dt.groupby('created_at')['status'].count().reset_index()\n# new data set\np = pd.DataFrame(dtg) \np['qty_ordered'] = dtq['qty_ordered']\np['discount_amount'] = dtd['discount_amount']\np['sku'] = dts['sku']\np['status'] = dtst['status']\n#Cumulative Sum\np['cum_grand_total'] = p['grand_total'].cumsum()\np['cum_qty_ordered'] = p['qty_ordered'].cumsum()\np['cum_discount_amount'] = p['discount_amount'].cumsum()\np['cum_sku_cnt'] = p['sku'].cumsum()\np['cum_status_cnt'] = p['status'].cumsum()\n# Date features\np['Dateofmonth'] = p['created_at'].dt.day\np['Month'] = p['created_at'].dt.month\np['Week'] = p['created_at'].dt.week\np['Dayofweek'] = p['created_at'].dt.dayofweek # 0 = monday.\np['Weekdayflg'] = (p['Dayofweek'] \/\/ 5 != 1).astype(float)\np['Month'] = p['created_at'].dt.month\np['Quarter'] = p['created_at'].dt.quarter\np['Dayofyear'] = p['created_at'].dt.dayofyear\n\np.head()","fb0a19d5":"trend_df = p.iloc[:, 0:2]  \ntrend_df.head()","56bbb771":"#Creating another column\ntrend_df['Year_month'] = trend_df['created_at'].apply(lambda x: x.strftime('%Y-%m'))\ntrend_df.head()","0045be63":"#Grouping Month Year\ntrend_df = trend_df.groupby('Year_month').sum()['grand_total'].reset_index()","d4ee53b6":"plt.figure(figsize=(15,6))\nplt.plot(trend_df['Year_month'],trend_df['grand_total'],color='#b80045')\nplt.xticks(rotation='vertical',size=8)\nplt.show()","161e082b":"# If you like my notebook,don,t forget to upvote.","fa5760da":"# TRENDS\n\n### Lets find out overall trend","32f5bcf4":"### We observe overall trend from july-2016 to aug-2018 and concluded following points\n* #### Purchases increases in last Three months ( October,November and December ) of 2016 and 2017\n* #### Most e-commerce buyers purchases products in November\n* #### November is most suitable month for sellers to market their products\n* #### Overall volume of ecommerce sales is increasing year by year \n* #### For example,you can compare first six months of 2017 with 2018","7506f9cb":"### This model works slightly better than Above model","60cefc78":"**In Above Graph, Different line Colours are used**\n1. Red line shows predictions\n2. Green Line shows training part of Dataset\n3. Orange Line shows Test part of Dataset","6b6b7bb5":"## **Creating a Sub-Dataset for Sales Predictions**","649dbd2c":"## As dataset is Stationary but we can improve its stationarity by using following techniques\n1. Difference with diff()\n2. Log\n3. Substracting previous value with Shift()\n4. Differencing Simple Moving Average\n5. Seasonal Decomposition","f57c1912":"## 5.Seasonal Decomposition\n##### By using this,we can find Seasonality,trends and residuals.","1d09c2f6":"## Differencing Simple Moving Average Provides us most suitable P-value and Test statistics","d24bf50d":"# Applying ARIMA Model(Auto Regression)","427f473b":"## 3.Substracting previous value with Shift()","b95f56cb":"# Predicting Sales on Daily Time frame","376bdad7":"## 2.Log","1ab9d63d":"#### Here 0th value is (-6.75) which shows test Statistic (More negative it is more stationary dataset will be)\n#### Here 1th value is (2.82 e-09) which shows P-value (More negative p-value in data is considered as more stationary)\n## Following conditions must be fulfilled for making dataset stationary\n1. Test Statistics needed to be below -2.91 \n2. Test Statistics has less value than critical value at 5%\n3.  P-value is less than 0.5","eb67048a":"## 4.Differencing Simple Moving Average","843058cd":"# Predicting Sales on Monthly Time frame","05d0600f":"# Applying SARIMAX Model","0ec26f3a":"# Contents\n \n*  Importing Liabraries\n*  Exploring and Modifying data\n*  Sales Predictions\n*  Models Implimentation\n*  Analysing Trends"}}