{"cell_type":{"48183b53":"code","e297aac4":"code","82961d50":"code","6eb15192":"code","8d148565":"code","bf544a9f":"code","5c745539":"code","7c6e1536":"code","31943942":"code","d05982fd":"code","e6509334":"code","3ef2d3cb":"code","f8a661b2":"code","6f5e52fb":"code","35f3a894":"code","3a5e162a":"code","17a6ad1f":"code","54d9ca78":"markdown","66a4f547":"markdown","2c56a2bb":"markdown","c8d77b0f":"markdown","df5d4310":"markdown","3f246365":"markdown","51945a5f":"markdown","33dbb16c":"markdown","90552a9f":"markdown"},"source":{"48183b53":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.optimize import fmin_l_bfgs_b\n\n# imsave is not working now, possibly due to a Kaggle package update.\n# Use PIL instead.\n#from scipy.misc import imsave\n\nfrom PIL import Image\nimport time\n\n# We won't use cv2 to read or save images. \n# It processes images in BGR format by default. We need RGB.\n# It gives images a blue colour which we don't want.\nimport cv2\n\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras import backend as K\nfrom keras.applications import vgg19\n\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')","e297aac4":"os.listdir('..\/input')","82961d50":"# Set the start and finish rows.\n# To create a new batch add 170 to START_ROW.\n\nSTART_ROW = 0\nEND_ROW = START_ROW + 170\n\nprint(START_ROW)\nprint(END_ROW)","6eb15192":"# Read a dataframe from another kernel.\n# This contains the id's of all images in the AISegment dataset.\n\npath = '..\/input\/selfie-segmenter-keras-and-u-net\/df_data.csv.gz'\ndf_portraits = pd.read_csv(path)\n\ndf_portraits.head()","8d148565":"# We will create each batch in a separate kaggle kernel version.\n# To speed up the process of data creation, 4 versions \n# of the same kernel can be comitted at the same time.\n# Commit a kernel, refresh the page, change START_ROW, then commit again.\n\ndf = df_portraits[START_ROW:END_ROW]\n\n\nprint(df.shape)","bf544a9f":"# create lists of image_id's\n\nPORTRAIT_LIST = list(df['image_id'])\n\nprint(len(PORTRAIT_LIST))","5c745539":"# ==================================== #\n\ntotal_variation_weight = 1e-4\nstyle_weight = 1.0\ncontent_weight = 0.025\n\niterations = 10\n\n# ==================================== #","7c6e1536":"# Auxiliary functions for loading, preprocessing, \n# and postprocessing the images that go in and out of VGG19.\n\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(img_height, img_width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    \n    # This step pre-processes the images in the exact same way as the ImageNet images \n    # were processed when VGG19 was pre-trained.\n    img = vgg19.preprocess_input(img)\n    return img\n\ndef deprocess_image(x):\n    # Zero-centering by removing the mean pixel value from ImageNet. \n    # This reverses a transformation done by vgg19.preprocess_input.\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    \n    # Converts images from 'BGR' to 'RGB'.\n    # This is also part of the reversal of vgg19.preprocess_input.\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n\n","31943942":"# Content loss\n\ndef content_loss(base, combination):\n    return K.sum(K.square(combination - base))\n\n\n\n# Style loss\n\ndef gram_matrix(x):\n    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\ndef style_loss(style, combination):\n        S = gram_matrix(style)\n        C = gram_matrix(combination)\n        channels = 3\n        size = img_height * img_width\n        return K.sum(K.square(S - C)) \/ (4. * (channels ** 2) * (size ** 2))\n\n\n    \n# Total variation loss\n\ndef total_variation_loss(x):\n    a = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - \\\n        x[:, 1:, :img_width - 1, :])\n    b = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - \\\n        x[:, :img_height - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))\n","d05982fd":"\n# This class wraps fetch_loss_and_grads in a way that lets you retrieve the \n# losses and gradients via two separate method calls, \n# which is required by the SciPy optimizer you'll use.\n\nclass Evaluator(object):\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n    def loss(self, x):\n        assert self.loss_value is None\n        x = x.reshape((1, img_height, img_width, 3))\n        outs = fetch_loss_and_grads([x])\n        loss_value = outs[0]\n        grad_values = outs[1].flatten().astype('float64')\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n    \nevaluator = Evaluator()\n","e6509334":"# Create a new directory to store the generated images\n\nimg_dir = 'img_dir'\nos.mkdir(img_dir)","3ef2d3cb":"# for each portrait image i.e. each content image\nfor j in range(0, len(PORTRAIT_LIST)):\n    \n    # Path to the image you want to transform\n    image_id = PORTRAIT_LIST[j]\n    folder_id = image_id.split('-')[0]\n    target_image_path = '..\/input\/aisegmentcom-matting-human-datasets\/matting_human_half\/clip_img\/' + str(folder_id) + \\\n    '\/clip_00000000\/' + image_id\n\n    # Path to the style image\n    style_reference_image_path = '..\/input\/art-by-ai-neural-style-transfer\/style_image.jpg'\n\n    # Dimensions of the generated picture\n    width, height = load_img(target_image_path).size\n    img_height = 400\n    img_width = int(width * img_height \/ height)\n    \n    \n    \n    # Set up VGG19\n    # =============\n    \n    target_image = K.constant(preprocess_image(target_image_path)) \n    style_reference_image = K.constant(preprocess_image(style_reference_image_path)) \n\n    # placeholder that will contain the generated image\n    combination_image = K.placeholder((1, img_height, img_width, 3))\n\n    # Combines the three images in a single batch\n    input_tensor = K.concatenate([target_image, style_reference_image,\n                                  combination_image], axis=0)\n\n    # Builds the VGG19 network with the batch of three images as input. \n    # The model will be loaded with pretrained ImageNet weights.\n    model = vgg19.VGG19(input_tensor=input_tensor, \n                        weights='imagenet',include_top=False) \n    \n    \n    \n    \n    # Set up the losses\n    # =================\n    \n    # Final loss\n\n    # Dictionary that maps layer names to activation tensors\n    outputs_dict = dict([(layer.name, layer.output) for layer in model.layers]) \n\n    # layer used for content loss\n    content_layer = 'block5_conv2'\n\n    # layers used for style loss\n    style_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1',\n                'block4_conv1',\n                'block5_conv1']\n\n\n    # Weights and weighted average of the loss components.\n    # I've moved these lines of code to the top of this notebook.\n    #total_variation_weight = 1e-4\n    #style_weight = 1.\n    #content_weight = 0.025\n\n\n    # You\u2019ll define the loss by adding all components to this scalar variable.\n    loss = K.variable(0.)\n\n    # adds the content loss\n    layer_features = outputs_dict[content_layer]\n    target_image_features = layer_features[0, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    loss += content_weight * content_loss(target_image_features,\n                                          combination_features)\n\n    # adds the style loss component for each target layer\n    for layer_name in style_layers:\n        layer_features = outputs_dict[layer_name]\n        style_reference_features = layer_features[1, :, :, :]\n        combination_features = layer_features[2, :, :, :]\n        sl = style_loss(style_reference_features, combination_features)\n        loss += (style_weight \/ len(style_layers)) * sl\n\n    # adds the total variation loss\n    loss += total_variation_weight * total_variation_loss(combination_image)\n    \n    \n    \n    # Run gradient descent.\n    # For each portrait image this code will generate a \n    # corresponding art image.\n    # The art image will be saved in a folder called img_dir.\n    # =====================================================\n    \n    # Gets the gradients of the generated image with regard to the loss\n    grads = K.gradients(loss, combination_image)[0]\n\n    # Function to fetch the values of the current loss and the current gradients\n    fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n  \n\n    # This is the initial state: the target image.\n    x = preprocess_image(target_image_path) \n\n    # You flatten the image because scipy.optimize.fmin_l_bfgs_b \n    # can only process flat vectors.\n    x = x.flatten()\n\n    for i in range(iterations):\n\n        #print('Start of iteration', i)\n        start_time = time.time()\n\n        # Runs L-BFGS optimization over the pixels of the generated image to \n        # minimize the neural style loss. Note that you have to pass the function \n        # that computes the loss and the function that computes the gradients \n        # as two separate arguments.\n        x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x, fprime=evaluator.grads, maxfun=20)\n\n        #print('Current loss value:', min_val)\n\n        # only save the last image generated\n        if i == (iterations-1):\n            # Saves the current generated image.\n            img = x.copy().reshape((img_height, img_width, 3))\n            img = deprocess_image(img)\n\n            fname = image_id\n            path = 'img_dir\/' + image_id\n            \n            # cv2 saves as BGR which gives the image a blue colour. Don't use cv2 to save.\n            #cv2.imwrite(path, img)\n            \n            # save the image using PIL\n            result = Image.fromarray(img.astype(np.uint8))\n            result.save(path)\n            print('Image saved as', fname)\n\n        end_time = time.time()\n        #print('Iteration %d completed in %ds' % (i, end_time - start_time))","f8a661b2":"# check if img_dir exists\n\n!ls","6f5e52fb":"# check how many files have been created\n\nlen(os.listdir('img_dir'))","35f3a894":"# Save this so that there is a non image file in the kernel output.\n# If we don't have a non image in the output it may not be possible to download img_dir.\n# Click 'Download All' to download img_dir even though it may not be listed\n# in the kernel output.\n\ndf_portraits.to_csv('df_portraits.csv.gz', compression='gzip', index=False)","3a5e162a":"# zip the folder\n\nimport shutil\nshutil.make_archive(img_dir, 'zip', img_dir)","17a6ad1f":"# Check that the zipped folder exists\n!ls","54d9ca78":"<hr>","66a4f547":"[Art by Ai](https:\/\/www.kaggle.com\/vbookshelf\/art-by-ai-neural-style-transfer) is a Kaggle dataset of machine textured human portrait images, also called art images. \n\nThis notebook sets out the workflow that I used to create the dataset.\n\n\n**Data Sources for this Notebook**\n\n- [Art by Ai Dataset](https:\/\/www.kaggle.com\/vbookshelf\/art-by-ai-neural-style-transfer) - style_image\n- [AISegment Dataset](https:\/\/www.kaggle.com\/laurentmih\/aisegmentcom-matting-human-datasets) - Human half-length portrait images\n- [Selfie Segmenter kaggle kernel](https:\/\/www.kaggle.com\/vbookshelf\/selfie-segmenter-keras-and-u-net) - df_data csv output file containing a list of all AISegment portrait images\n\n**Approach**\n\n- Select 170 portrait images from the AISegment dataset.\n- For each portrait use neural style transfer to create one art image.\n- Create data in batches of 170 images. (A batch size >186 will exceed the RAM limit and cause the kernel to crash.)\n- Dowload the images from the kernel output.\n- Put images from all batches into the same folder.\n- Upload the folder to Kaggle.\n\n**Citation**<br>\n\nThe neural style transfer code used in this kernel is taken from the book, [Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python) by Francois Chollet.\n\nThe same style transfer code is also part of the Keras docs:<br>\nhttps:\/\/keras.io\/examples\/neural_style_transfer\/","2c56a2bb":"## Conclusion","c8d77b0f":"## Generate the Portrait Images","df5d4310":"<hr>","3f246365":"That's how the dataset was created. One thing I've learned in my machine learning journey so far is that although Ai tools may be powerful, they mean nothing without data. Anyone who controls both the data and Ai tech will run the show. This is something strategic to keep in mind going forward.\n\nThank you for reading.","51945a5f":"## Introduction","33dbb16c":"## Set Up the Style Transfer Code","90552a9f":"**Before running the code**:\n\n> - Switch on the kernel GPU.\n> - Switch on kernel internet access"}}