{"cell_type":{"cefb9098":"code","8757f61d":"code","8fbd9e19":"code","06463405":"code","916854d4":"code","5482ee1e":"code","855e7840":"code","3602ca18":"code","88f50b75":"code","c994e0ea":"code","65d365b3":"code","68bd3a63":"code","82eab46a":"code","7d1965ae":"markdown","ad855f52":"markdown","c17d716b":"markdown","11a4401d":"markdown","0b3e3791":"markdown","30d35939":"markdown","f1031bab":"markdown","06860535":"markdown","4678bdf8":"markdown","b0e9b3dc":"markdown"},"source":{"cefb9098":"#Install Dask\n!python -m pip install \"dask[complete]\" ","8757f61d":"import xgboost\nxgboost.__version__","8fbd9e19":"#upgrade xgboost\n!pip install xgboost==1.5","06463405":"# Step 1. import all necessary packages","916854d4":"#import all necessary packages\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport xgboost as xgb\nimport dask.array as da\nimport dask.distributed\nfrom multiprocessing import freeze_support","5482ee1e":"    np.random.seed(42)\n    \n    #read file\n    train_path = r\"..\/input\/tabular-playground-series-dec-2021\/train.csv\"\n    df = pd.read_csv(train_path)\n\n    #set id column as index\n    df = df.set_index('Id')\n    \n    #save y target as numpy array\n    y = df['Cover_Type'].values\n    \n    #drop Cover_Type column and convert pandas to numpy array\n    df.drop('Cover_Type', axis=1, inplace=True)\n    x = df.to_numpy()","855e7840":"#split data\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.3, random_state=42)","3602ca18":"#count how many samples in each group\ntrain_label_count = np.bincount(y_train)","88f50b75":"#remove class 5 with single value\nX_train = np.delete(X_train, (np.where(y_train==5)), axis=0)\ny_train =np.delete(y_train, (np.where(y_train==5)), axis=0)\n    ","c994e0ea":"cluster = dask.distributed.LocalCluster()\nclient = dask.distributed.Client(cluster)\n\n#scale the cluster to n workers\ncluster.scale(1) ","65d365b3":"X = da.from_array(X_train, chunks=(1000, X_train.shape[1]))\ndtrain = xgb.DMatrix(client, X, y)\n\nparams =  {\"verbosity\": 1, \"tree_method\": \"hist\", \"objective\": 'multi:softprob', 'num_class':8, 'eta':0.8}\n       \nmodel = xgb.train(\n    client,params,\n    dtrain,num_boost_round=100,\n    evals=[(dtrain, \"train\")]\n)\n    \n#make prediction\nxgb_train = xgb.predict(client, model, da.from_array(X_train))\nxgb_val = xgb.predict(client, model, da.from_array(X_val))\n    \n#convert results to numpy array\nxgb_train = xgb_train.compute()\nxgb_val = xgb_val.compute()\n","68bd3a63":"#convert softmax to predicted labels\ndef soft_predicted_labels(a):\n    max_values = [np.argmax(r) for i, r in enumerate(a)]\n    return max_values\n\ny_train_pred = soft_predicted_labels(xgb_train) \ny_val_pred = soft_predicted_labels(xgb_val) \n    \nxgb_train_acc = accuracy_score(y_train, y_train_pred)\nxgb_val_acc = accuracy_score(y_test, y_val_pred)\n    \nprint(\"train acc: {} test acc : {}\".format(xgb_train_acc, xgb_val_acc))","82eab46a":"#predict on test \ntest_path =  r\"..\/input\/tabular-playground-series-dec-2021\/test.csv\"\ndf_test = pd.read_csv(test_path)\n    \ndf_test.set_index('Id', inplace = True)\narray_test = df_test.to_numpy()\narray_test = dask.array.from_array(df_test.to_numpy(), chunks=(1000,array_test.shape[1]))\n    \npred_test = xgb.dask.predict(client, model, array_test)\npred_test = pred_test.compute()\npred_test = soft_predicted_labels(pred_test) \n    \npred_test = pd.DataFrame(pred_test, columns=['Cover_Type'])\npred_test['Id'] = df_test.index\npred_test.set_index('Id', inplace=True)\n    \npred_test.to_csv(\".\/submission.csv\")","7d1965ae":"As you can see data is imbalanced. we will remove the sample that belongs to class 5 since it is only one sample. At this point I will mention that I tried to deal with the imbalance by:\n1. oversampling (SMOTE) \n2. weight_samples\n\nHowever, none of these methods worked. So we will remove the sample from the training data","ad855f52":"Build the model with specified params (Here you can fin tune XGBoost parameters).\nDask takes only Dask.array which are numpy=like objects. This is better for dealing with large-scale datasets that may consume a lot of memory.","c17d716b":"![image.png](attachment:8d9a7a40-7a09-407a-8aae-4539d1dbaad1.png)","11a4401d":"# Step 2. Data - loading and preprocessing","0b3e3791":"# 3. prediction","30d35939":"Training XGBoost may be a time and memory consuming process. Therefore, training on GPU accelerates the process. Of course, you would have a GPU installed in your machine. XGBoost supports fully distributed GPU training using Dask. For more information you can go to: https:\/\/xgboost.readthedocs.io\/en\/stable\/gpu\/index.html\n\nIn order to run XGBoost with GPU you will have to:\n1. set up a distributed cluster using Dask-cuda\n2. Connect to distributed cluster\n3. scale num_workers to 1 to specify number of processes","f1031bab":"![image.png](attachment:36f51b58-5f04-46db-9f99-43abc373609d.png)","06860535":"# Train Distributed XGBoost with Dask (local)","4678bdf8":"XGBoost is a powerful tool for multiclass prediction with multiple features. In the *Forest Cover Type* dataset we have a mix of numerical features and categorical features that were already converted to dummy features. \n\nIn this tutorial I will show you how to train XGBoost model with Dask on your local GPU.\n","b0e9b3dc":"We're getting softmax output: vectors with probability per each class. \nWe need to convert the predictions to actual labels (max probability for each sample)"}}