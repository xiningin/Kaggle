{"cell_type":{"5f8c5ed7":"code","48ca98a4":"code","8e408d9f":"code","48d6b474":"code","1a0e3147":"code","9bd4c291":"code","6b10af1c":"code","0f3cd5dd":"code","2298c42e":"code","31978a8b":"code","05a49a2a":"code","2a4e781c":"code","050cb187":"code","2054b7b6":"code","15759f37":"code","8341a769":"code","5a94f38b":"code","890f29e3":"code","fb325bb7":"code","e0670609":"code","de5a3e9c":"code","7049f89f":"code","8c5d4f8f":"code","49613dd9":"code","4c317918":"code","239b4977":"code","7ac93b73":"code","b4004592":"code","3958347d":"code","801725aa":"code","2ddddb7e":"code","0163bb00":"code","8e286918":"code","902489fa":"code","f540a672":"code","8fe375b5":"code","50da3a1a":"code","9457d84d":"code","6c001b15":"code","d98a1522":"code","7b83a0b7":"code","8cd2e907":"code","7e67bbc8":"code","473f4ddd":"code","b5adf7a9":"code","858e2231":"markdown","509fd999":"markdown","d0e0cc7f":"markdown","9c374e5e":"markdown","e9525b5a":"markdown","1c0d83d0":"markdown","87db3d54":"markdown","0f46fd18":"markdown","4266d3e3":"markdown","cd754b50":"markdown","656f3af0":"markdown","b5691d09":"markdown","9197a057":"markdown"},"source":{"5f8c5ed7":"#Any tips\/advice appreciated :)\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_ids = test['Id']\n","48ca98a4":"train.head()","8e408d9f":"train.info()","48d6b474":"combined_sets = pd.concat([train, test], axis = 0, sort = False)\ncombined_sets = combined_sets.drop(['SalePrice', 'Id'], axis = 1)","1a0e3147":"def clean_features(data, weight_perc, missing_perc):\n    \n    for col in data.columns:\n\n        nulls = data[col].isna().sum()\n        uniques = data[col].nunique()\n        values = data[col].value_counts()\n        feature_len = len(data[col])\n        highest = 0\n\n        for value_count in values:\n            if value_count > highest:\n                highest = value_count\n        \n        value_weight = round(highest\/feature_len, 4)\n        missing_values = round(nulls\/feature_len, 4)\n        print(f'{col}: one value weight: {value_weight}, missing values: {missing_values} ')\n        \n        if value_weight > weight_perc or missing_values > missing_perc:\n            data = data.drop([col], axis = 1)\n        \n    return data","9bd4c291":"combined_sets = clean_features(data = combined_sets, weight_perc = 0.8, missing_perc = 0.4)","6b10af1c":"combined_sets.columns","0f3cd5dd":"train = train[['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'LotShape',\n       'LotConfig', 'Neighborhood', 'HouseStyle', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'Exterior1st', 'Exterior2nd',\n       'MasVnrType', 'MasVnrArea', 'ExterQual', 'Foundation', 'BsmtQual',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF',\n       'TotalBsmtSF', 'HeatingQC', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n       'BsmtFullBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Fireplaces', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'MoSold', 'YrSold', 'SalePrice']]\n\n\ntrain_numerical = train.select_dtypes(include = np. number)\ntrain_numerical","2298c42e":"#1. Heatmap\n#We will only look at the numerical values with a correlation above 40%\n\ncorr = train_numerical.corr()\ncorr_over = corr.index[abs(corr['SalePrice']) > 0.4]\nplt.figure(figsize=(15,15))\ng = sns.heatmap(train_numerical[corr_over].corr(), annot = True, cmap = 'coolwarm')","31978a8b":"train_numerical_heatmap = train[['OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea','TotalBsmtSF', '1stFlrSF', 'GrLivArea',\n                       'FullBath', 'Fireplaces', 'GarageCars', 'GarageArea']]","05a49a2a":"#2. ExtraTreeClassifier\n#To use this we have to fill all the missing values.\ntrain_numerical.info()","2a4e781c":"#LotFrontage, MsVnrArea and GarageYrBlt have missing values. Let's take a closer look at them.\n\nplt.figure(figsize= (20,8))\nsns.histplot(train_numerical['LotFrontage'])\nplt.show()","050cb187":"#filling the missing values with the mean seems fine here\ntrain_numerical['LotFrontage'].fillna((train_numerical['LotFrontage'].mean()), inplace = True)","2054b7b6":"plt.figure(figsize= (20,8))\nsns.histplot(train_numerical['MasVnrArea'])\nplt.show()","15759f37":"#filling the missing values with 0 seems fine here\ntrain_numerical['MasVnrArea'].fillna(0, inplace = True)\n","8341a769":"plt.figure(figsize= (20,8))\nsns.distplot(train_numerical['GarageYrBlt'])\nplt.show()","5a94f38b":"#Filling the missing values with the mean seems fine here, but maybe change later or delete.\ntrain_numerical['GarageYrBlt'].fillna((train_numerical['GarageYrBlt'].mean()), inplace = True)","890f29e3":"#2. ExtraTreesClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\ny_train = train_numerical['SalePrice']\ntrain_numerical_etc = train_numerical.drop(['SalePrice'], axis = 1)\n\netc = ExtraTreesClassifier()\netc.fit(train_numerical_etc, y_train)\nimportant_features = pd.Series(etc.feature_importances_, index = train_numerical_etc.columns)\n\nprint(etc.feature_importances_)\nplt.figure(figsize = (20,10))\nimportant_features.nlargest(10).plot(kind='barh')\nplt.show()","fb325bb7":"#3. RFECV with linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFECV\n\ntrain_numerical_rfecv = train_numerical.drop(['SalePrice'], axis = 1)\n\nols =LinearRegression()\nrfecv = RFECV(estimator=ols, step=1, scoring=\"neg_mean_squared_error\", cv=4, verbose=0, n_jobs=4)\nrfecv.fit(train_numerical_rfecv, y_train)\nrfecv.transform(train_numerical_rfecv)\nprint(\"Optimum number of features: %d\" % rfecv.n_features_)\ntrain_numerical_rfecv.columns[rfecv.support_]","e0670609":"train_numerical = train_numerical_heatmap[['OverallQual', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea','TotalBsmtSF', '1stFlrSF', 'GrLivArea',\n                       'FullBath', 'Fireplaces', 'GarageCars', 'GarageArea']]","de5a3e9c":"train_categorical = train.select_dtypes(include = object)\ntrain_categorical.info()","7049f89f":"for col in train_categorical.columns:\n     train_categorical[col] = train_categorical[col].fillna('U')\n","8c5d4f8f":"sns.catplot(x = 'LotShape', y = 'SalePrice', data = train)","49613dd9":"sns.catplot(x = 'LotShape', y = 'SalePrice', kind = 'point', data = train)","4c317918":"#Let's make it binary, as there seem to be a signifcant difference between regular and irregular\ntrain_categorical['LotShape'] = train_categorical['LotShape'].map(lambda x: 1 if (x == 'Reg') else 0)","239b4977":"plt.figure(figsize= (20,10))\nsns.histplot(train_categorical['Exterior1st'])\nplt.show()","7ac93b73":"plt.figure(figsize= (20,10))\nsns.histplot(train_categorical['Exterior2nd'])\nplt.show()","b4004592":"#Seems similar enough, let's delete Exterior2nd\ntrain_categorical = train_categorical.drop(['Exterior2nd'], axis = 1)","3958347d":"sns.catplot(x = 'HouseStyle', y = 'SalePrice', data = train)","801725aa":"plt.figure(figsize= (20,8))\nsns.pointplot(x = 'HouseStyle', y = 'SalePrice', data = train, )\nplt.show()","2ddddb7e":"#Housestyle does not seem that predicative of the sale price, even if grouped. Let's delete this feature.\ntrain_categorical = train_categorical.drop(['HouseStyle'], axis = 1)\n","0163bb00":"test_categorical = test[train_categorical.columns]\ntest_categorical['LotShape'] = test_categorical['LotShape'].map(lambda x: 1 if (x == 'Reg') else 0)\nfor col in test_categorical.columns:\n     test_categorical[col] = test_categorical[col].fillna('U')\n\nfor col in train_categorical.columns:\n    unique_train = train_categorical[col].unique()\n    unique_test = test_categorical[col].unique()\n    \n    for value in unique_train:\n        if value not in unique_test:\n            print(f\"train in col {col} has {value}, but test does not.\")\n   \n    for value in unique_test:\n        if value not in unique_train:\n            print(f\"test in col {col} has {value}, but train does not.\")","8e286918":"test_categorical['MSZoning'].value_counts()","902489fa":"#MSZoning, 4 outliers,\ntrain_categorical['Exterior1st'].value_counts()","f540a672":"#2 are Stone, 1 is ImStucc\ntest_categorical['Exterior1st'].value_counts()","8fe375b5":"#2 are Stone, 1 is ImStucc\ntest_categorical['Exterior1st'].value_counts()","50da3a1a":"#One missing value\ntest_categorical['KitchenQual'].value_counts()","9457d84d":"test_categorical = test_categorical[test_categorical['MSZoning'] != 'U']\ntrain_categorical = train_categorical[train_categorical['Exterior1st'] != 'Stone']\ntrain_categorical = train_categorical[train_categorical['Exterior1st'] != 'ImStucc']\ntrain_categorical = train_categorical[train_categorical['Exterior1st'] != 'CBlock']\n\ntest_categorical = test_categorical[test_categorical['Exterior1st'] != 'U']\ntest_categorical = test_categorical[test_categorical['KitchenQual'] != 'U']\n","6c001b15":"categorical_grouped = {\"ExterQual\":      {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5 },\n                     \"BsmtFinType1\":   {\"NA\": 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6},\n                     \"HeatingQc\":      {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                     \"KitchenQual\":    {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                     \"FireplaceQu\":    {\"NA\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n                     \"GarageFinish\":   {\"NA\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}}\n","d98a1522":"train_categorical_grouped = train_categorical.replace(categorical_grouped)\ntrain_categorical = pd.get_dummies(train_categorical_grouped)\ntrain = pd.concat([train_numerical, train_categorical], axis = 1)","7b83a0b7":"#Now do test too:\ntest_numerical = test[train_numerical.columns]\n\ntest_categorical = test_categorical.replace(categorical_grouped)\ntest_categorical = pd.get_dummies(test_categorical)\ntest = pd.concat([test_numerical, test_categorical], axis = 1)","8cd2e907":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train, y_train, test_size = 0.2, random_state = 0)\n","7e67bbc8":"import xgboost\nfrom sklearn.model_selection import GridSearchCV\n'''\nxgb1 = xgboost.XGBRegressor()\nparameters = {\n              'learning_rate': [0.01, 0.02, 0.03],\n              'max_depth': [3, 4, 5],\n              'min_child_weight': [1, 2, 3],\n              'subsample': [0.7, 0.8],\n              'n_estimators': [500, 600, 700],\n              'random_state': [0],\n}\n\nxgb_grid = GridSearchCV(xgb1,\n                        parameters,\n                        cv = 2,\n                        n_jobs = 5,\n                        verbose=True,\n                       )\n\nxgb_grid.fit(X_train,\n         y_train)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)\n'''\n","473f4ddd":"\nxgb_model = xgboost.XGBRegressor(subsample=0.7, \n                             learning_rate=0.02,\n                             max_depth=3,\n                             min_child_weight = 2,\n                             random_state=np.random.randint(1000),\n                             n_estimators=500).fit(X_train, y_train)\nprint(\"Performance on train data:\", xgb_model.score(X_train, y_train))\nprint(\"Performance on train data:\", xgb_model.score(X_test, y_test))\n","b5adf7a9":"\nprediction_submission = xgb_model.predict(test)\n\n\nsubmission_df = pd.DataFrame({'Id': test_ids.values,\n                             \"SalePrice\": prediction_submission,\n                             })\nsubmission_df.to_csv('submission.csv', index = False)","858e2231":"Now, the thing is that some features in the test data have values the train data do not, and the other way around. That will be a problem as we will use dummies to make new features. Let's find the features where this is the case.","509fd999":"**2.2 Categorical Featurs**\n\nWe cannot direct copy the same correlation techniques we did on the numerical data on the categorical data.\nThe reason is that we first need to encode the features. As there is different kinds of categorical features, we have to treat them as such when we encode them. \n\nMoving on, let's fill the missing values from the categorical data.","d0e0cc7f":"**2.1 Numerical Features**\n\nNow, let's look at the numerical data and see if we can drop more features based on its correlation with the sale price.","9c374e5e":"GarageYrBlt and YearBilt, TotRmsAbvGrd and GrLivArea, GarageCars and GarageArea, are values that highly correlate. Thus, let's drop GarageYrBlt, TotRmsAbvGrd and GarageArea as they correlate the least with SalePrice.","e9525b5a":"**4. Modeling**","1c0d83d0":"We can see that MSZoning, Exterior1st and KitchenQual have does not have the same values.\n- MSZoning in train has no missing values, but that is the case with the test data.\n- Exterior1st in train has Stone and ImStucc, the test does not. Test has a missing value, train does not.\n- KitchenQual has a missing value in test, train does not.\n\nWe suppose all these values are outliers and delete the examples if they are few","87db3d54":"Now, let's find out if LotShape can be binary, if Exterior1st and Exterior2nd can be reduced to one feature, and what kind of data HouseStyle and BsmtQual are.","0f46fd18":"There are a lot of different ways to select features. Here, we will be using\n1. Heatmap\n2. ExtraTreeClassifier\n3. RFECV (rfe with cross-validation)","4266d3e3":"Taking a look at the description of the values of each of the features, we can gather several cues:\n\n- LotShape: Maybe we can turn it into a binary feature (Regular and irregular), find out.\n- Neighborhood: Seems nominal, given that the data tells us about the physical location, which seems to have no inherent order -> dummies.\n- HouseStyle: Seems too detailed compared to the information each value gives, maybe group (or delete). Nominal or ordinal, not quite sure yet.\n- Exterior1st and Exterior2nd: Exterior1st is the exterior covering the house, Exterior2nd the exterior covering the house, given that more than 1 material is used. Exterior2nd seems deprecative. Nominal -> dummies\n- MasVnrType: Masonry veneer type. Seems nominal. -> dummies\n- ExterQual: Definitly ordinal. Values ranging from poor to excellent. -> group\n- Foundation: Type of foundation. Seems nominal.-> dummies\n- BsmtQual: Heigh of the basement. Not sure if nominal or ordinal. find out?\n- BsmtExposure: Walkout\/garden level walls exposure. Assume nominal -> dummies\n- BsmtFinType1: Rating of basement finished area. Ordinal -> group\n- HeatingQc: Heating quality and condition. Ordinal -> group\n- Kitchen Qual: Ordinal -> group.\n- FireplaceQu: Ordinal -> group\n- GarageType: Garage location. Nominal -> onehot\n- GarageFinish: Interior finish of the garage. Ordinal -> group","cd754b50":"As we can see, ExtraTreesClassifier and rfecv with linear regression gives us a lot of the same features. I'm gonna keep the ones from the heatmap.","656f3af0":"**1. Importing**","b5691d09":"**2. Deleting Features**\n\nWe see a lot of the features are missing values in the training data. There might also be a lot of features with the same value. Let's get rid of all these features. Note that such features are also in the test data, so let's combine the two datasets and do the cleaning","9197a057":"**3. Encode**"}}