{"cell_type":{"a6387c7d":"code","f13638d3":"code","ec4b06fa":"code","460364a6":"code","cd00f66e":"code","b4ee9d9e":"markdown","c7d9db64":"markdown","2df2ce96":"markdown","55f7ca98":"markdown","1855f54c":"markdown"},"source":{"a6387c7d":"from covid19_data_processing_module import *","f13638d3":"# recreate the schema from \"json_schema.txt\"\nclass author():\n    \n    def __init__(self, input_dict=None):\n        \n        self.first = \"\"\n        self.middle = []\n        self.last = \"\"\n        self.suffix = \"\"\n        self.affiliation = {}\n        self.email = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"first\" in key:\n                    self.first = input_dict[key]\n                if \"middle\" in key:\n                    self.middle = input_dict[key]\n                if \"last\" in key:\n                    self.last = input_dict[key]\n                if \"suffix\" in key:\n                    self.suffix = input_dict[key]\n                if \"affiliation\" in key:\n                    self.affiliation = input_dict[key]\n                if \"email\" in key:\n                    self.email = input_dict[key]    \n    \n    def print_items(self):\n        \n        print(\"first: \" + str(self.first) +  \n              \", middle: \" + str(self.middle) + \n              \", last: \" + str(self.last) + \n              \", suffix: \" + str(self.suffix) +\n              \", email: \" + str(self.email) + \n              \", affiliation: \" + json.dumps(self.affiliation, indent=4, sort_keys=True)\n             )\n\n\nclass inline_ref_span():\n    \n    def __init__(self, input_dict=None):\n        \n        self.start = 0\n        self.end = 0\n        self.text = \"\"\n        self.ref_id = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"start\" in key:\n                    self.start = input_dict[key]\n                if \"end\" in key:\n                    self.end = input_dict[key]\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"ref_id\" in key:\n                    self.ref_id = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"Text: \" + str(self.text) + \", Start: \" + \n              str(self.start) + \", End: \" + str(self.end) + \n              \", Ref_id: \" + str(self.ref_id))\n\n    def step_index(self, n):\n        \n        self.start += n\n        self.end += n\n        \n        \nclass text_block():\n    \n    def __init__(self, input_dict=None):\n        \n        self.text = \"\"\n        self.cite_spans = []\n        self.ref_spans = []\n        self.eq_spans = []\n        self.section = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"cite_spans\" in key:\n                    self.cite_spans = [inline_ref_span(c) for c in input_dict[key]]                \n                if \"ref_spans\" in key:\n                    self.ref_spans = [inline_ref_span(r) for r in input_dict[key]] \n                if \"eq_spans\" in key:\n                    self.eq_spans = [inline_ref_span(e) for e in input_dict[key]]\n                if \"section\" in key:\n                    self.section = input_dict[key]\n        \n    def clean(self, swap_dict=None):\n            \n        self.text = clean(self.text, swap_dict)\n    \n    def print_items(self):\n        \n        print(\"\\ntext: \" + str(self.text))\n        print(\"\\nsection: \" + str(self.section))\n        print(\"\\ncite_spans: \")\n        [c.print_items() for c in self.cite_spans]\n        print(\"\\nref_spans: \")\n        [r.print_items() for r in self.ref_spans]\n        print(\"\\neq_spans: \")\n        [e.print_items() for e in self.eq_spans]\n\n\ndef combine_text_block(text_block_list):\n    \n    if text_block_list:\n        \n        combined_block = text_block_list[0]\n        block_length = len(combined_block.text)\n        \n        for i in range(1,len(text_block_list)):\n            combined_block.text += \" \" + text_block_list[i].text\n            block_length += 1\n            \n            # update spans start & stop index\n            [ref.step_index(block_length) for ref in text_block_list[i].cite_spans]\n            [ref.step_index(block_length) for ref in text_block_list[i].ref_spans]\n            [ref.step_index(block_length) for ref in text_block_list[i].eq_spans]\n            \n            # combine spans\n            combined_block.cite_spans += text_block_list[i].cite_spans\n            combined_block.ref_spans += text_block_list[i].ref_spans\n            combined_block.eq_spans += text_block_list[i].eq_spans           \n            combined_block.section += \", \" + str(text_block_list[i].section)           \n            \n            block_length += len(text_block_list[i].text)\n                       \n        return [combined_block]\n    else:\n        return [text_block()]\n      \n\nclass bib_item():\n    \n    def __init__(self, ref_id=None, input_dict=None):\n        \n        self.ref_id: \"\"\n        self.title: \"\"\n        self.authors = []\n        self.year = 0\n        self.venue = \"\"\n        self.volume = \"\"\n        self.issn = \"\"\n        self.pages = \"\"\n        self.other_ids = {}\n        \n        if ref_id:\n            self.ref_id = ref_id\n            \n            if input_dict:\n                for key in input_dict.keys():\n                    if \"title\" in key:\n                        self.title = input_dict[key]\n                    if \"authors\" in key:\n                        self.authors = [author(a) for a in input_dict[key]]\n                    if \"year\" in key:\n                        self.year = input_dict[key]\n                    if \"venue\" in key:\n                        self.venue = input_dict[key]\n                    if \"volume\" in key:\n                        self.volume = input_dict[key]\n                    if \"issn\" in key:\n                        self.issn = input_dict[key]\n                    if \"pages\" in key:\n                        self.pages = input_dict[key]\n                    if \"other_ids\" in key:\n                        self.other_ids = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"\\nBib Item:\")\n        print(\"ref_id: \" + str(self.ref_id))\n        print(\"title: \" + str(self.title))\n        print(\"Authors:\")\n        [a.print_items() for a in self.authors]\n        print(\"year: \" + str(self.year))\n        print(\"venue: \" + str(self.venue))\n        print(\"issn: \" + str(self.issn))\n        print(\"pages: \" + str(self.pages))\n        print(\"other_ids: \" + json.dumps(self.other_ids, indent=4, sort_keys=True))\n        \n        \nclass ref_entries():\n    \n    def __init__(self, ref_id=None, input_dict=None):\n        \n        self.ref_id = \"\"\n        self.text = \"\"\n        self.latex = None\n        self.type = \"\"\n        \n        if ref_id:\n            self.ref_id = ref_id\n            \n            if input_dict:\n                for key in input_dict.keys():\n                    if \"text\" in key:\n                        self.text = input_dict[key]\n                    if \"latex\" in key:\n                        self.latex = input_dict[key]\n                    if \"type\" in key:\n                        self.type = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"ref_id: \" + str(self.ref_id))\n        print(\"text: \" + str(self.text))\n        print(\"latex: \" + str(self.latex))\n        print(\"type: \" + str(self.type))\n        \n                    \nclass back_matter():\n    \n    def __init__(self, input_dict=None):\n        \n        self.text = \"\"\n        self.cite_spans = []\n        self.ref_spans = []\n        self.section = \"\"\n        \n        if input_dict:\n            for key in input_dict.keys():\n                if \"text\" in key:\n                    self.text = input_dict[key]\n                if \"cite_spans\" in key:\n                    self.cite_spans = [inline_ref_span(c) for c in input_dict[key]]                \n                if \"ref_spans\" in key:\n                    self.ref_spans = [inline_ref_span(r) for r in input_dict[key]] \n                if \"section\" in key:\n                    self.section = input_dict[key]\n    \n    def print_items(self):\n        \n        print(\"text: \" + str(self.text))\n        print(\"cite_spans: \")\n        [c.print_items() for c in self.cite_spans]\n        print(\"ref_spans: \")\n        [r.print_items() for r in self.ref_spans]        \n        print(\"section:\" + str(self.section))\n\n        \n# The following Class Definition is a useful helper object to store various \n# different covid-19 data types.\nclass document():\n    \n    def __init__(self, file_path=None):\n        \n        self.doc_filename = \"\"\n        self.doc_language = {}\n        self.paper_id = \"\"\n        self.title = \"\"\n        self.authors = []\n        self.abstract = []\n        self.text = []\n        self.bib = []\n        self.ref_entries = []\n        self.back_matter = []\n        self.tripples = {}\n        self.key_phrases = {}\n        self.entities = {}\n        \n        # load content from file on obj creation\n        self.load_file(file_path)\n     \n    def _load_paper_id(self, data):\n        \n        if \"paper_id\" in data.keys():\n            self.paper_id = data['paper_id']\n        \n    def _load_title(self, data):\n        \n        if \"metadata\" in data.keys():\n            if \"title\" in data['metadata'].keys():\n                self.title = data['metadata'][\"title\"]\n    \n    def _load_authors(self, data):\n        \n        if \"metadata\" in data.keys():\n            if \"authors\" in data['metadata'].keys():\n                self.authors = [author(a) for a in data['metadata'][\"authors\"]]\n                \n    def _load_abstract(self, data):\n        \n        if \"abstract\" in data.keys():\n            self.abstract = [text_block(a) for a in data[\"abstract\"]]\n    \n    def _load_body_text(self, data):\n        \n        if \"body_text\" in data.keys():\n            self.text = [text_block(t) for t in data[\"body_text\"]]\n    \n    def _load_bib(self, data):\n        \n        if \"bib_entries\" in data.keys():\n            self.bib = [bib_item(b, data[\"bib_entries\"][b]) for b in data[\"bib_entries\"].keys()]\n    \n    def _load_ref_entries(self, data):\n        \n        if \"ref_entries\" in data.keys():\n            self.ref_entries = [ref_entries(r, data[\"ref_entries\"][r]) for r in data[\"ref_entries\"].keys()]\n            \n    def _load_back_matter(self, data):\n        \n        if \"back_matter\" in data.keys():\n            self.back_matter = [back_matter(b) for b in data[\"back_matter\"]]\n        \n    def load_file(self, file_path):\n        \n        if file_path:\n            \n            with open(file_path) as file:\n                data = json.load(file)\n                \n                # call inbuilt data loading functions\n                self.doc_filename = file_path\n                self._load_paper_id(data)\n                self._load_title(data)\n                self._load_authors(data)\n                self._load_abstract(data)\n                #self._load_body_text(data) - temp removal to save memory for later scripts\n                self._load_bib(data)\n                self._load_ref_entries(data)\n                self._load_back_matter(data)\n    \n    def combine_data(self):\n        \n        self.data = {'doc_filename': self.doc_filename,\n                     'doc_language': self.doc_language,\n                     'paper_id': self.paper_id,\n                     'title': self.title,\n                     'authors':self.authors,\n                     'abstract': self.abstract,\n                     'text': self.text,\n                     'bib_entries':self.bib,\n                     'ref_entries': self.ref_entries,\n                     'back_matter': self.back_matter,\n                     'tripples': self.tripples,\n                     'key_phrases': self.key_phrases,\n                     'entities': self.entities}\n\n    def extract_data(self):\n        \n        self.doc_filename = self.data['doc_filename']\n        self.doc_language = self.data['doc_language']\n        self.paper_id = self.data['paper_id']\n        self.title = self.data['title']\n        self.authors = self.data['authors']\n        self.abstract = self.data['abstract']\n        self.text = self.data['text']        \n        self.bib = self.data['bib_entries']\n        self.ref_entries = self.data['ref_entries']\n        self.back_matter = self.data['back_matter']\n        self.tripples = self.data['tripples']\n        self.key_phrases = self.data['key_phrases']\n        self.entities = self.data['entities']\n\n    def save(self, dir):\n        \n        self.combine_data()\n\n        if not os.path.exists(os.path.dirname(dir)):\n            try:\n                os.makedirs(os.path.dirname(dir))\n            except OSError as exc:  # Guard against race condition\n                if exc.errno != errno.EEXIST:\n                    raise\n\n        with open(dir, 'w') as json_file:\n            json_file.write(json.dumps(self.data))\n\n    def load_saved_data(self, dir):\n        \n        with open(dir) as json_file:\n            self.data = json.load(json_file)\n        self.extract_data()\n    \n    def print_items(self):\n         \n        print(\"---- Document Content ----\") \n        print(\"doc_filename: \" + str(self.doc_filename))\n        print(\"doc_language: \" + str(self.doc_language))\n        print(\"paper_id: \" + str(self.paper_id))\n        print(\"title: \" + str(self.title))\n        print(\"\\nAuthors: \")\n        [a.print_items() for a in self.authors]\n        print(\"\\nAbstract: \")\n        [a.print_items() for a in self.abstract]\n        print(\"\\nText: \")\n        [t.print_items() for t in self.text]\n        print(\"\\nBib_entries: \")\n        [b.print_items() for b in self.bib]\n        print(\"\\nRef_entries: \")\n        [r.print_items() for r in self.ref_entries]\n        print(\"\\nBack_matter: \")\n        [b.print_items() for b in self.back_matter]\n        \n        print(\"\\nTripples: \")\n        print(json.dumps(self.tripples, indent=4, sort_keys=True))\n        print(\"\\nKey Phrases: \")\n        print(json.dumps(self.key_phrases, indent=4, sort_keys=True))        \n        print(\"\\nEntities: \")\n        print(json.dumps(self.entities, indent=4, sort_keys=True))\n\n    def clean_text(self, swap_dict=None):\n        \n        # clean all blocks of text\n        [t.clean(swap_dict) for t in self.text]\n    \n    def clean_abstract(self, swap_dict=None):\n        \n        [t.clean(swap_dict) for t in self.abstract]\n    \n    def combine_text(self):\n        \n        # this function takes all text blocks within document.text and combines them into a single text_block object\n        self.text = combine_text_block(self.text)\n    \n    def combine_abstract(self):\n        \n        self.abstract = combine_text_block(self.abstract)   \n    \n    \n    def set_title_tripples(self):\n        \n        title_tripples = {}\n        pairs, entities = get_entity_pairs(self.title)\n            \n        #if any tripples found\n        if pairs.shape[0]>0:\n            title_tripples[\"title\"] = pairs.to_json()     \n            self.tripples.update(title_tripples)\n        \n    def set_abstract_tripples(self):\n                \n        abstract_tripples = {}\n        for i in range(0, len(self.abstract)):\n            #for every block in the abstract, extract entity tripples\n            self.abstract[i].clean()                       \n            pairs, entities = get_entity_pairs(self.abstract[i].text)\n            \n            #if any tripples found\n            if pairs.shape[0]>0:\n                abstract_tripples[\"abstract_\" + str(i)] = pairs.to_json()\n        \n        if abstract_tripples:\n            self.tripples.update(abstract_tripples)\n        \n    def set_text_tripples(self):\n        \n        text_tripples = {}\n        for i in range(0, len(self.text)):\n            \n            self.text[i].clean()                       \n            pairs, entities = get_entity_pairs(self.text[i].text)\n            if pairs.shape[0]>0:\n                text_tripples[\"text_\" + str(i)] = pairs.to_json()\n        \n        if text_tripples:\n            self.tripples.update(text_tripples)\n        \n    def set_ref_tripples(self):\n        \n        ref_tripples = {}\n        for r in self.ref_entries:\n            pairs, entities = get_entity_pairs(r.text)\n            if pairs.shape[0]>0:\n                ref_tripples[\"ref_\" + r.ref_id] = pairs.to_json()\n        \n        if ref_tripples:\n            self.tripples.update(ref_tripples)\n        \n    def set_doc_language(self):\n        # set the doc language based on the analysis of the first block within the abstract\n        if self.text:\n            self.doc_language = get_text_language(self.text[0].text)\n        elif self.abstract:\n            self.doc_language = get_text_language(self.abstract[0].text)\n        else:\n            self.doc_language = get_text_language(self.title)","ec4b06fa":"def process_document(file_path):\n    \n    doc = None\n    \n    try:\n        # create doc obj\n        doc = document(file_path)\n    except:\n        print(\"Error Processing: \" + file_path)\n        return None\n    \n    # identify doc language\n    try:\n        doc.set_doc_language()\n    except:\n        print(\"Error set_doc_language: \" + file_path)\n        \n    # process abstract and text\n    try:\n        doc.set_abstract_tripples()\n    except:\n        print(\"Error set_abstract_tripples: \" + file_path)\n    \n    try: \n        doc.set_ref_tripples()\n    except:\n        print(\"Error set_ref_tripples: \" + file_path)\n    \n    \n    return doc","460364a6":"# dir_input_data = '\/kaggle\/input\/CORD-19-research-challenge\/'\n#\n# file_list = []\n# for dirname, _, filenames in os.walk(dir_input_data):\n#     \n#     files = [names for names in filenames if '.json' in names]    \n# \n#     files = files[0:10]\n#     for file in files:\n#         input_file = os.path.join(dirname, file)\n#     \n#         #process abstracts in file\n#         print(input_file)\n#         doc = process_document(input_file)\n#         if doc:\n#             print(\"    Title: \" + doc.title)\n#             print(\"    Language: \" +  str(doc.doc_language[\"language\"]))","cd00f66e":"dir_input_data = '\/kaggle\/input\/CORD-19-research-challenge'\n\nfile_list = []\nfor dirname, _, filenames in os.walk(dir_input_data):\n    \n    data = []\n    files = [names for names in filenames if '.json' in names]    \n    \n    for file in files:\n        \n        input_file = os.path.join(dirname, file)\n        \n        #process abstracts in file\n        doc = process_document(input_file)\n        if doc:\n            data.append(doc)\n    \n    if data:\n        \n        # format input and output file paths\n        dirname_split = dirname.split(\"\/\")\n        filename = Path(dirname_split[-3] + \"_\" + dirname_split[-2] + \"_\" + dirname_split[-1]).with_suffix('.pickle')\n\n        # Save File to output folder\n        with open(filename,\"wb\") as f:\n            pickle.dump(data, f)","b4ee9d9e":"Process all files","c7d9db64":"### Data Extraction and Enrichment\nThis notebook loads and processes the publications to extract, clean and load the contents into a class *document* and enrich with entity pairs for further analysis.\nEntity extraction is enabled by the [SpaCy](https:\/\/spacy.io\/) library and pre-trained NLP modules and text standardization using the [NLTK](https:\/\/www.nltk.org\/) library.\nThe medical text in the publications contains several abbreviations which can be difficult to analyse with standard NLP techniques and difficult to read and compare. We try to decode these with the [Sci-SpaCy](https:\/\/allenai.github.io\/scispacy\/) library.  \n\nThe document object contains the origional text data and select meta data and enriched features to aid in the generation of knowledge graphs for furture analyis. \n\nThe processed data is exported as a list documents objects and saved as a compressed *.pickle* file, one for each directory of json files. This processing is time comsuming and is decoupled form analysis to speed up experimentation. ","2df2ce96":"Run a silgle file to test code","55f7ca98":"Apply extraction, cleaning and entity pair identifycation function to each of the publications in the corpus. ","1855f54c":"## Run Code to Load and Process Data Abstracts"}}