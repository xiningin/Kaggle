{"cell_type":{"4abefba1":"code","c4d264d0":"code","9d0ff22c":"code","b6aac8c1":"code","6348b1b2":"code","574fc756":"code","f0be347e":"code","d31d8594":"code","259368bc":"code","e46c3fd5":"code","0f97efdd":"code","dfcf6d99":"code","721fc527":"code","4f4e790d":"code","71250cc2":"code","0409ae8d":"code","26a3f12b":"code","3eb74f8c":"code","ab302c93":"code","188584c2":"code","d295b575":"code","769659e7":"code","66d426c6":"code","557ae8e8":"code","937545ef":"code","e1cf571e":"code","4285f4ad":"code","709be700":"code","b18099a0":"code","ba661526":"code","abcd3fb9":"markdown","a64a2fad":"markdown","e812c2ea":"markdown","58b32d34":"markdown","6e87296f":"markdown","bd76ff70":"markdown","36b402a5":"markdown","a05dad63":"markdown","f5b72619":"markdown","5f462e96":"markdown","31e0c774":"markdown","5c2c8661":"markdown","c1b01535":"markdown","cc083ee6":"markdown","7c204a8e":"markdown","c0f04e67":"markdown"},"source":{"4abefba1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm, skew, boxcox_normmax \n\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor\n\nfrom xgboost import XGBRegressor","c4d264d0":"sample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","9d0ff22c":"train.head()","b6aac8c1":"print(train.shape)","6348b1b2":"test.head()","574fc756":"print(test.shape)","f0be347e":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","d31d8594":"# Plot histogram and probability\nfrom scipy import stats\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\nnorm.fit(train['SalePrice'])\n\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('Before transformation')","259368bc":"#We use the numpy fuction log1p \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\nfig = plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nsns.distplot(train['SalePrice'] , fit=norm);\nnorm.fit(train['SalePrice'])\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nplt.subplot(1,2,2)\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.suptitle('After transformation')","e46c3fd5":"plt.figure(figsize=[12,4])\n\nplt.subplot(1,2,1)\nplt.hist(train.SalePrice, bins=20, color='plum', edgecolor='k') #create a histogram for saleprice it is right skewed.\n\nplt.subplot(1,2,2)\nplt.hist(np.log(train.SalePrice), bins=20, color='plum', edgecolor='k') #histogram of logsaleprice it is normally distributed.\n\nplt.show()\n\n#So insteaed of using saleprice we will use log of salesprice in our model training.","0f97efdd":"X_train = train.drop(['Id','SalePrice'], axis=1)\ny_train = train.SalePrice\nX_test = test.drop(['Id'], axis=1)\n\nprint('X_train shape:', X_train.shape)\nprint('y_train shape:', y_train.shape)\nprint('X_test shape: ', X_test.shape)","dfcf6d99":"X_train.isna().sum().sort_values(ascending=False)[:20]\n\n#Count the missing value per column and sort the result by no of missing values.\n#we can see there are so many feature who has more missing value.we can drop it but we are not dropping it here to avoid the overfitting.\n","721fc527":"print(X_train.dtypes)\n#the variable that have object data type are going under catagorical values.variable with object data type could be numeric or \n#string .he we will take it as string.","4f4e790d":"print(np.unique(X_train.dtypes.values))\n#it will show the unique data type in this data set.","71250cc2":"#create a sel_num vector and will do a comparision of which entries are int and which are float.\nsel_num=(X_train.dtypes.values==\"int64\") | (X_train.dtypes.values==\"float64\") #give a boolean array of either float or int\nnum_idx=np.arange(0, len(X_train.columns))[sel_num] #series of numeric column index.\nX_train_num = X_train.iloc[:, num_idx] #use loc to select out those columns.\n\nprint(\"Number of numeric columns :\" , np.sum(sel_num), '\\n')\nprint(\"Indices of numeric columns :\" , num_idx ,'\\n')\nprint(\"Name of numeric columns :\" , X_train_num.columns.values)\n","0409ae8d":"sel_cat=(X_train.dtypes.values=='O')\ncat_idx=np.arange(0,len(X_train.columns))[sel_cat]\nX_train_cat=X_train.iloc[:,cat_idx]\n\nprint(\"Number of catagorical columns :\" , np.sum(sel_cat), '\\n')\nprint(\"Indices of catagorical columns :\" , cat_idx ,'\\n')\nprint(\"Name of catagorical columns :\" , X_train_cat.columns.values)\n\n\n","26a3f12b":"num_transformer=Pipeline(\n    steps=[\n           ('Imputer' , SimpleImputer(strategy='mean'))# pipeline for numeric\n           #Use imputer to store the missing value with mean.\n       ]\n)\n\ncat_transformer=Pipeline(\n    steps=[\n            ('Imputer' ,SimpleImputer(strategy='constant',fill_value='missing')), #pipeline for cat\n            ('Onehot' , OneHotEncoder(handle_unknown='ignore', sparse= False))\n            \n            #'ignore deal with after the encoder has trained.tell the encoder how to deal with values havenot seen before after it alreadybeen trained.'\n        ]\n)\n\n#Columntransformer split the cat and num col and then it take the cat col and fit it into cat_transformer,then take num col and \n#fit it into num_transformer and whn done it will stitch them back together.\n\npreprocessor=ColumnTransformer(\n    transformers=[\n        ('num',num_transformer,num_idx),\n        ('cat',cat_transformer,cat_idx)\n        \n    ]\n)\npreprocessor.fit(X_train)\ntrain_processor=preprocessor.transform(X_train)#transform the traning set according to the preprocessor.\nprint(train_processor.shape, '\\n')\n#print(train_processor[1,:])\n\n#col are expanded to 304 from 80\n#can see 1st 36 entries are numerical features.others are 0 and 1 these col are created by hard coding.\n","3eb74f8c":"lr_pipe=Pipeline(\nsteps=[\n    ('preprocessor' , preprocessor),#perform preprocessor\n    ('regressor' , LinearRegression())#create lr model\n]\n)\nlr_pipe.fit(X_train,y_train) #traing data divided into num feature and cat feature each one those will pas through its own  transformer\n#so it can perform imputation missing value and onehot coding then it stitch together that is the result of preprocessor and that \n#result will go through the LR()\nlr_pipe.score(X_train,y_train)","ab302c93":"cv_results = cross_val_score(lr_pipe, X_train, y_train, cv=10, scoring='r2')#traing data split in to 10 flod then we train this pipeline 10 time each\n#time it train on 90% of data leaving 10% aside then it validate on that 10%.each time cal the r2 then we print the avg of it.\n\nprint('Results by fold:\\n', cv_results, '\\n')\nprint('Mean CV Score:', np.mean(cv_results))","188584c2":"cv_results = cross_val_score(lr_pipe, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error')\n\nprint('Results by fold:\\n', cv_results, '\\n')\nprint('Mean CV Score:', np.mean(cv_results))","d295b575":"\n#Reguralized version opf LR\nen_pipe=Pipeline(\nsteps=[\n    ('preprocessor' , preprocessor),\n    ('regressor', ElasticNet(max_iter=1000))#verson of LR with some amount of regulization as in LR we got a overfitting observation\n    #so use some regulation.\n]\n)\n#elasticnet have 2 type of hypoparameter\nparam_grid = {\n    'regressor__alpha': [0.0001, 0.001, 0.01, 0.1],# regulize parameter if set small num for alpha we will get a unregularize model if set too high then will get a model that too inflexiable to learn. a \n    'regressor__l1_ratio': [0, 0.25, 0.5, 0.75, 1.0],# 2 regulization l1 and l2 we perfom blened of l1 and l2 \n    \n}\nnp.random.seed(1)\nen_grid_search = GridSearchCV(en_pipe, param_grid, cv=10, scoring='neg_root_mean_squared_error',\n                              refit='True', verbose = 10, n_jobs=-1)\n#refit =true means it will fit again and verbose=10 higher the no more \n#output will get.n_jobs=-1 it will shw all the course avl.if =2 then will shw 2 course.\n\nen_grid_search.fit(X_train, y_train)\n\nprint(en_grid_search.best_score_)\nprint(en_grid_search.best_params_)\n    \n","769659e7":"%%time \n\nrf_pipe = Pipeline(\n    steps = [\n        ('preprocessor', preprocessor),\n        ('regressor', RandomForestRegressor(n_estimators=100))#more tree in forest will give gd result we use 100 tree here.\n    ]\n)\n#120 random forest model tree each rf contain 100 tree so we are fitting 12000 diff random forest model here.\nparam_grid = {\n    'regressor__min_samples_leaf': [8, 16, 32],\n    'regressor__max_depth': [4, 8, 16, 32],\n}\n\nnp.random.seed(1)\nrf_grid_search = GridSearchCV(rf_pipe, param_grid, cv=10, scoring='neg_root_mean_squared_error',\n                              refit='True', verbose = 10, n_jobs=-1)#refit='True' means will take best tree model\nrf_grid_search.fit(X_train, y_train)\n\nprint(rf_grid_search.best_score_)\nprint(rf_grid_search.best_params_)\n","66d426c6":"rf_model = rf_grid_search.best_estimator_.steps[1][1]","557ae8e8":"%%time \n#similar to RF,its aunassemble of decission tree.with rf each tree is dependent on the tree.with GB we training tree sequentially.\n#generate one tree follow that with a 2nd tree which will a improvment version of previous tree.\nxgd_pipe = Pipeline(\n    steps = [\n        ('preprocessor', preprocessor),\n        ('regressor', XGBRegressor(n_estimators=50, subsample=0.5))# take 50 tree,if taking 2 tree then if we  combine them to get a better result so it is a add on not a replacement of 1st one.\n    ]\n)\n\nparam_grid = {\n    'regressor__learning_rate' : [0.1, 0.5, 0.9],#small lr means not focusing on future tree large means focusing on future tree.\n    'regressor__alpha' : [0, 1, 10],\n    'regressor__max_depth': [4, 8, 16]\n    \n}\n\nnp.random.seed(1)\nxgd_grid_search = GridSearchCV(xgd_pipe, param_grid, cv=10, scoring='neg_root_mean_squared_error',\n                              refit='True', verbose = 10, n_jobs=-1)\nxgd_grid_search.fit(X_train, y_train)\n\nprint(xgd_grid_search.best_score_)\nprint(xgd_grid_search.best_params_)","937545ef":"print(type(en_grid_search.best_estimator_))","e1cf571e":"ensemble = VotingRegressor(  #use VotingRegressor class from siket learn,parameter of this is estimator.we choose 3 estimator.\n    estimators = [\n        ('en', en_grid_search.best_estimator_),#this is not a EN model ,this is a pipeline obj consist of preprocssing steps.\n        ('rf', rf_grid_search.best_estimator_),\n        ('xgb', xgd_grid_search.best_estimator_),\n    ]#This is not actual fit in model this is a pipeline that hasbeen set to obtain the hypoparameter.\n)\n#we ensemble all the 3 model.\ncv_results = cross_val_score(ensemble, X_train, y_train, cv=10, scoring='neg_root_mean_squared_error')\n\nprint('Results by fold:\\n', cv_results, '\\n')\nprint('Mean CV Score:', np.mean(cv_results))\n#score is better than the indivisual score of models\n","4285f4ad":"cv_results = cross_val_score(ensemble, X_train, y_train, cv=10, scoring='r2')\n\nprint('Results by fold:\\n', cv_results, '\\n')\nprint('Mean CV Score:', np.mean(cv_results))","709be700":"ensemble.fit(X_train, y_train)\nensemble.score(X_train, y_train)","b18099a0":"sample_submission.head()","ba661526":"submission = sample_submission.copy()\nsubmission.SalePrice = np.exp(ensemble.predict(X_test))#take the cpy of sample submission snd overwrite the sale price col.\n\nsubmission.to_csv('my_submission.csv', index=False)\nsubmission.head()","abcd3fb9":"The graph shows that our data now looks more normal","a64a2fad":"separate numeric and catagorical data so that we can do different process on these like in numeric data set we will fill the missing value and use siket learning and in catagorical value fill missing value then do hard coding.","e812c2ea":"# Column Data Types","58b32d34":"# Load Data","6e87296f":"# Load Packages","bd76ff70":"# Preprocessing Pipelines\n\n","36b402a5":"# Random Forest Regressor","a05dad63":"# Test Predictions","f5b72619":"# **Determine the Skew & kurtosis of our Target Varible, SalePrice ***","5f462e96":"# Linear Regression \n\n","31e0c774":"# Missing Values","5c2c8661":"# Ensemble","c1b01535":"# ElasticNet","cc083ee6":"# Exploratory Analysis","7c204a8e":"# Gradient Boosting Trees","c0f04e67":"This distribution is Right\/positively skewed. Notice that the black curve is more deviated towards the right. If you encounter that your predictive (response) variable is skewed, it is recommended to fix the skewness to make good decisions by the model.This is to make sure that SalesPrice values are distributed normaly using function log1p which applies log(1+x) to all elements of the column which fixes the skeweness of the distribution.\n\n\n\n\n\n\n"}}