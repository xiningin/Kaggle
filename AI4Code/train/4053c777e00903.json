{"cell_type":{"45ece259":"code","86a30b7a":"code","b88a20fd":"code","f5179f3d":"code","1e84ee78":"code","69d8ec64":"code","e2df1938":"code","94897d0c":"code","285f6c9b":"code","314947f9":"code","dac9192f":"code","1073be4e":"code","bba43f71":"code","b98d965e":"code","1806665e":"code","d0caa019":"code","9f77363f":"code","d1f44633":"code","880e6955":"code","9873c005":"code","5961f25e":"code","6fb67805":"code","88d482aa":"code","36a246e6":"code","50b7b8de":"code","e3047e5f":"code","998eb5f8":"code","6d5e4f35":"code","03145dd7":"code","4ba07018":"code","39ce1068":"markdown","5b378735":"markdown","54aa267c":"markdown","f1c621c3":"markdown","311c3a0f":"markdown","b27ba1ee":"markdown","6e6cff02":"markdown","65919c16":"markdown"},"source":{"45ece259":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86a30b7a":"import matplotlib \nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set(color_codes = True)\n%matplotlib inline\npd.pandas.set_option('display.max_columns', None)","b88a20fd":"df = pd.read_csv('\/kaggle\/input\/fraud-detection-bank-dataset-20k-records-binary\/fraud_detection_bank_dataset.csv')\ndf.head()","f5179f3d":"# Check Nan Values.\nfor i in df.columns:\n    print (i+\": \"+str(df[i].isna().sum()))","1e84ee78":"# Drop Index\ndf = df.drop('Unnamed: 0',axis = 1)\ndf.describe()","69d8ec64":"# Check correlation between all variable.\ncorr= df.corr()\nplt.figure(figsize = (25,10))\nsns.heatmap(corr,annot = True,cmap = 'rocket')","e2df1938":"# Assigning dependent and independent variable.\nx = df.drop('targets',axis = 1)\ny = df.iloc[:,-1]","94897d0c":"# Normalizing data.\nfrom sklearn.preprocessing import StandardScaler\n\nstd = StandardScaler()\n\nx_std = std.fit_transform(x)","285f6c9b":"#Reduce diamensionality of the data.\nfrom sklearn.decomposition import PCA\n\npca=PCA(n_components=2)\nx_pca = pca.fit_transform(x)\nx_pca.shape\n","314947f9":"plt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=df['targets'])\nplt.xlabel('First principle component')\nplt.ylabel('Second principle component')","dac9192f":"#Split data into Train and test format\n\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x_pca,y,test_size = 0.20,random_state =42)\n\nprint('Shape of Training Xs:{}'.format(x_train.shape))\nprint('shape of Test:{}'.format(x_test.shape))","1073be4e":"from sklearn.linear_model import LogisticRegression\n\n# apply algorithm on data and find out wether model is suitable or not.\nclf = LogisticRegression();\nclf.fit(x_train,y_train)\ny_predicted = clf.predict(x_test)\nscore = clf.score(x_test,y_test)","bba43f71":"print(score)","b98d965e":"from sklearn.metrics import confusion_matrix\n\ncnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","1806665e":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","d0caa019":"classes = df[\"targets\"].value_counts()\nclasses.index = [str(x) for x in classes.index]","9f77363f":"plt.figure()\nplot_confusion_matrix(cnf_matrix, classes=classes.index,\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= classes.index, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","d1f44633":"from sklearn.model_selection import RandomizedSearchCV\n#Randomized Search CV\n\n\n#Criteron for tree\ncriterion = ['gini','entropy']\n#splitter for tree\nsplitter = [\"best\", \"random\"]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 40, num = 8)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15,25,50,75, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5,6,10,15]","880e6955":"# Create the random grid\nrandom_grid = {'criterion': criterion,\n               'splitter': splitter,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","9873c005":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier();\nrf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = 1)\nrf_random.fit(x_train,y_train)","5961f25e":"rf_random.best_params_","6fb67805":"rf = DecisionTreeClassifier(splitter='best',min_samples_split =25,min_samples_leaf= 1,max_features= 'sqrt',max_depth= 5,criterion= 'entropy')\n\nrf.fit(x_train,y_train)\nscore = rf.score(x_test,y_test)\ny_predicted = rf.predict(x_test)\nprint(score)","88d482aa":"cnf_matrix = confusion_matrix(y_test, y_predicted)\nnp.set_printoptions(precision=2)\ncnf_matrix","36a246e6":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","50b7b8de":"plt.figure()\nplot_confusion_matrix(cnf_matrix, classes=classes.index,\n                      title='Confusion matrix, without normalization')\n# With normalization\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes= classes.index, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()","e3047e5f":"from sklearn.model_selection import RandomizedSearchCV\n#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1500, num = 15)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 50, num = 10)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15,25,50,75 ,100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 7, 10, 14]","998eb5f8":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","6d5e4f35":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 50, cv = 3, verbose=2, random_state=42)\n\nrf_random.fit(x_train,y_train)","03145dd7":"rf_random.best_params_","4ba07018":"rf_best_params = RandomForestClassifier(n_estimators= 1000,min_samples_split= 25,min_samples_leaf= 2, max_features= 'auto',max_depth= 5)\n\nrf_best_params.fit(x_train,y_train)\nscore = rf_best_params.score(x_train,y_train)\nprint(score)","39ce1068":"<a id=\"4\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>DATA_PREPROCESSING<\/center><\/h1> ","5b378735":"<a id=\"7\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>HYPERPARAMETER_TUNING<\/center><\/h1> ","54aa267c":"<a id=\"1\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>LIBRARIES<\/center><\/h1> ","f1c621c3":"<a id=\"6\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>CONFUSION_MATRIX<\/center><\/h1> ","311c3a0f":"<a id=\"6\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>BUILD_MODEL<\/center><\/h1> ","b27ba1ee":"<a id=\"3\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>DATA_VISUALIZATION<\/center><\/h1> ","6e6cff02":"<a id=\"5\"><\/a>\n\n<h1 style='background:#a9a799; border:0; color:black'><center>VALIDATION_METHOD<\/center><\/h1> ","65919c16":"<a id=\"2\"><\/a>\n<h1 style='background:#a9a799; border:0; color:black'><center>DATA_IMPORTING<\/center><\/h1> "}}