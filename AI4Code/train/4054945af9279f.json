{"cell_type":{"e5f5ab4d":"code","0f11143b":"code","e2f99e91":"code","30aed0b1":"code","7c0e69e0":"code","a35e139c":"code","abafc4a6":"code","bbe2cf3c":"code","db7d60f7":"code","8244e178":"code","455ea93e":"code","8c1452a6":"code","4f977fa8":"code","7568424f":"code","ce2432d2":"code","042312f3":"code","ec70653e":"code","5d80c9f8":"code","137f007b":"code","505313fb":"code","dee288a4":"code","40f8c704":"code","4ce45b13":"code","ac83814c":"code","3afb9fdb":"code","93017830":"code","57d8ce28":"code","67233496":"code","89aa8101":"code","58cb7f4d":"code","334182da":"code","4598c4ae":"code","66270013":"code","fe120e40":"code","7621a0b2":"code","2bd8c3e8":"code","e4dedbd8":"code","921f91f3":"markdown","63f33502":"markdown","f31df623":"markdown","4b85664f":"markdown","03f5a038":"markdown","dc3b574e":"markdown","9118190e":"markdown","7a3b3321":"markdown","8a099908":"markdown","186b082c":"markdown","66cde7d5":"markdown","8e7f4720":"markdown","4ea58f5a":"markdown","03ac8346":"markdown","e94dc099":"markdown","58c6757e":"markdown","69e1031d":"markdown","2fc36024":"markdown","845b1537":"markdown","867f9d94":"markdown","2218440c":"markdown","917e2b33":"markdown"},"source":{"e5f5ab4d":"import torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))","0f11143b":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nprint('Train data : {}'.format(train.shape))\nprint('Test data : {}'.format(test.shape))","e2f99e91":"train.head()","30aed0b1":"test.head()","7c0e69e0":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","a35e139c":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","abafc4a6":"train_size = train.shape[0]\n\ntrain_Id = train['Id']\ntest_Id = test['Id']\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n\nsales_price = train['SalePrice'].values\n\ntrain = train.drop(columns=['SalePrice'])","bbe2cf3c":"all_data = pd.concat((train, test)).reset_index(drop=True)\n\nprint(all_data.shape)\nall_data.head()","db7d60f7":"# Perform the sales price to make it normally distributed\nn_sales_price = np.log1p(sales_price)\n\nf, axes = plt.subplots(1, 2, figsize=(10,3))\nsns.distplot(pd.Series(sales_price, name=\"Sales Price (Before Transformation)\") , fit=norm, ax=axes[0]);\nsns.distplot(pd.Series(n_sales_price, name=\"Sales Price (After Transformation)\")  , fit=norm, ax=axes[1]);","8244e178":"def get_missing_data(data_df):\n    null_columns = data_df.columns[data_df.isnull().any()]\n    missing_data = ((data_df[null_columns].isnull().sum() \/ len(data_df[null_columns])) *100).sort_values(ascending=False)\n    missing_data = pd.DataFrame({'Missing Ratio' :missing_data})\n    return missing_data\n\nmissing_data = get_missing_data(all_data)\nmissing_data.head()","455ea93e":"def replace_missing_data(data_df):\n    \n    # These are the fields for with the missing values that we will be replaced with...\n    \n    # 'None'\n    cols_none   = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual', 'MasVnrType']\n    for col in cols_none:\n        data_df[col] = data_df[col].fillna('None')\n    \n    \n    # 0\n    cols_0      = ['GarageYrBlt', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'GarageArea']\n    for col in cols_0:\n        data_df[col] = data_df[col].fillna(0)\n    \n    # Median (Group by Neighboorhood)\n    cols_median = ['LotFrontage']\n    for col in cols_median:\n        data_df[col] = data_df.groupby('Neighborhood')[col].transform(lambda x: x.fillna(x.median()))\n    \n    # Mode\n    cols_mode   = ['Electrical', 'MSZoning', 'Utilities', 'Functional', 'Exterior1st', 'Exterior2nd', 'SaleType', 'KitchenQual']\n    for col in cols_mode:\n        data_df[col] = data_df[col].fillna(data_df[col].mode()[0])\n        \n    return data_df\n\nall_data = replace_missing_data(all_data)\nall_data.head()","8c1452a6":"# Check to confirm that there is no missing value anymore\nmissing_data = get_missing_data(all_data)\nprint(len(missing_data))","4f977fa8":"all_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YearBuilt'] = all_data['YearBuilt'].astype(str)\nall_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str)","7568424f":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","ce2432d2":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n\nskewness = pd.DataFrame({'Skew' :skewed_feats})\n\nprint(skewness.shape)\nskewness.head()","042312f3":"def plot_visual_skewness():\n    col = 6\n    row = 5\n    f, axes = plt.subplots(row, col, figsize=(20,18))\n    \n    n = 0\n    for i in range(row):\n        for j in range(col):\n            field = skewness.index[n]\n            sns.distplot(all_data[field] , fit=norm, ax=axes[i][j]);\n            n += 1\n            \nplot_visual_skewness()","ec70653e":"from scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lam)\n    \nplot_visual_skewness()","5d80c9f8":"all_data = pd.get_dummies(all_data)\nall_data.shape","137f007b":"x_train = all_data[:train_size]\nx_test = all_data[train_size:]\n\ny_train = n_sales_price\n\n((x_train.shape, y_train.shape), x_test.shape)","505313fb":"x_train.head()","dee288a4":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","40f8c704":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    rmse= np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","4ce45b13":"# LASSO Regression\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n# Elastic Net Regression\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n\n# Kernel Ridge Regression\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n\n# Gradient Boost Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', \n                                   min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =5)\n\n# XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1, random_state =7, nthread = -1)\n\n# LightGBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nmodels = {'LASSO': lasso, \n          'Elastic Net': ENet, \n          'Kernel Ridge Regression': KRR, \n          'Gradient Boost': GBoost, \n          'XGBoost': model_xgb, \n          'LightGBM': model_lgb}","ac83814c":"# for key, value in models.items():\n#     score = rmsle_cv(value)\n#     print( key + \" score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","3afb9fdb":"# class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n#     def __init__(self, models):\n#         self.models = models\n        \n#     # we define clones of the original models to fit the data in\n#     def fit(self, X, y):\n#         self.models_ = [clone(x) for x in self.models]\n        \n#         # Train cloned base models\n#         for model in self.models_:\n#             model.fit(X, y)\n\n#         return self\n    \n#     #Now we do the predictions for cloned models and average them\n#     def predict(self, X):\n#         predictions = np.column_stack([\n#             model.predict(X) for model in self.models_\n#         ])\n#         return np.mean(predictions, axis=1)   ","93017830":"# averaged_models = AveragingModels(models = (ENet, GBoost, model_lgb, lasso))\n\n# score = rmsle_cv(averaged_models)\n# print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","57d8ce28":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","67233496":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\n# stacked_averaged_models = StackingAveragedModels(base_models = (model_xgb, model_lgb, KRR),\n#                                                  meta_model = GBoost)\n\n# score = rmsle_cv(stacked_averaged_models)\n# print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","89aa8101":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","58cb7f4d":"models = {'LASSO': lasso, \n          'Elastic Net': ENet, \n          'Kernel Ridge Regression': KRR, \n          'Gradient Boost': GBoost, \n          'XGBoost': model_xgb, \n          'LightGBM': model_lgb,\n          'StackedRegressor' : stacked_averaged_models}","334182da":"# for key, model in models.items():\n    \n#     model.fit(x_train.values, y_train)\n#     train_pred = model.predict(x_train.values)\n#     print( key + ': {:.4f}'.format(rmsle(y_train, train_pred)))\n    \n# #     pred = np.expm1(model.predict(x_test.values))\n\nGBoost.fit(x_train.values, y_train)\ntrain_pred = GBoost.predict(x_train.values)\nprint('Gradient Boost : {:.4f}'.format(rmsle(y_train, train_pred)))","4598c4ae":"Id = test_Id\nSalePrice = np.expm1(GBoost.predict(x_test.values))\n\nmy_submission = pd.DataFrame({'Id': Id, 'SalePrice': SalePrice})\nmy_submission.to_csv('submission.csv', index=False)\n\nmy_submission.head()","66270013":"# def get_ensemble(data):\n#     ensemble =  (0.7 * stacked_averaged_models.predict(data)) + (0.1 * GBoost.predict(data)) + (0.1 * model_xgb.predict(data)) + (0.1 * model_lgb.predict(data))\n# #     ensemble =  (0.7 * np.expm1(stacked_averaged_models.predict(data))) + (0.1 * np.expm1(GBoost.predict(data))) + (0.1 * np.expm1(model_xgb.predict(data))) + (0.1 * np.expm1(model_lgb.predict(data)))\n    \n#     return ensemble\n\n# train_ensemble = get_ensemble(x_train.values)\n# print('RMSLE score on train data:')\n# print(rmsle(y_train, train_ensemble))","fe120e40":"# ensemble = np.expm1(get_ensemble(x_test.values))","7621a0b2":"# Id = test_Id\n# SalePrice = ensemble\n\n# my_submission = pd.DataFrame({'Id': Id, 'SalePrice': SalePrice})\n# my_submission.to_csv('submission.csv', index=False)\n\n# my_submission.head()","2bd8c3e8":"# from sklearn.linear_model import LinearRegression\n\n# reg = LinearRegression().fit(x_train, y_train)\n# reg.score(x_train, y_train)\n\n\n\n# reg.coef_\n# reg.intercept_","e4dedbd8":"# y_test = reg.predict(x_test)\n\n# Id = test_Id\n# SalePrice = y_test\n\n# my_submission = pd.DataFrame({'Id': Id, 'SalePrice': SalePrice})\n# my_submission.to_csv('submission.csv', index=False)\n\n# my_submission.head()","921f91f3":"##### Some numeric fields such as 'YearBuilt' are actually meant to be categorical as such, convert them to strings","63f33502":"#### Root Mean Squared Logarithmic Error evaluation function\n","f31df623":"The features look better now (normally distrubuted). Not all though, but better than where we used to be.\n\nLets move on.","4b85664f":"#### Check the distibution of  numeric fields","03f5a038":"Thanks to [Serigne](https:\/\/www.kaggle.com\/serigne) for his kernel on [Stacked Regression](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook)","dc3b574e":"### Remove outliers from the training set","9118190e":"# Data Acquisition","7a3b3321":"### Check the distribution of the sales price","8a099908":"#### Now, lets transform the skewed data","186b082c":"Split the data to get the train and test datasets","66cde7d5":"##### Add total area of the building as a feature","8e7f4720":"### Lets input missing values","4ea58f5a":"# Model Stacking","03ac8346":"### train and test datasets should now have the same number of columns (79)\n* Merge the two datasets","e94dc099":"Now lets visualize the skewness of the distribution","58c6757e":"Train and fit models","69e1031d":"# Features Engineering\n\n### Let us identify missing values","2fc36024":"#### Ensemble\n70% - StackedRegressor\n\n10% - Gradient Boost\n\n10% - XG Boost\n\n10% - Light GBM","845b1537":"# Data Preparation","867f9d94":"#### Categorize the data","2218440c":"### Load the data","917e2b33":"* Extract and drop the IDs from both the train and test datasets\n* Extract and drop sales price from the train dataset"}}