{"cell_type":{"b6b406a2":"code","47a2141b":"code","290cbc95":"code","aee2d482":"code","45c1f7a9":"code","5130a87b":"code","77949c6b":"code","c24effd4":"code","03b12a29":"code","48f699a7":"code","9cad7fdc":"code","4adff54e":"code","80e8dc7c":"code","1281b22e":"code","4d20aed8":"code","7107e432":"code","07fea0cf":"code","7cde2e74":"code","75329429":"code","0e0cb150":"code","ec192823":"code","9f808b97":"code","bdd55afb":"code","96f6dc97":"code","ac508fdf":"code","4dbc9c3a":"code","89eb263c":"code","cccd54a6":"code","17f299de":"markdown","40a57c7b":"markdown","1be6a975":"markdown","70407199":"markdown","45751368":"markdown","ab7371f6":"markdown","21e72fff":"markdown","64d14b7c":"markdown","8b6da537":"markdown","eb28efb0":"markdown","f87c7f67":"markdown","7ae3af1d":"markdown","7c252d47":"markdown","754d47cf":"markdown","e65e222c":"markdown","b036494f":"markdown","e001c2f9":"markdown","8230611b":"markdown","ba174c7a":"markdown","ff8c4cfb":"markdown"},"source":{"b6b406a2":"!pip install mlxtend","47a2141b":"import numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom mlxtend.preprocessing import TransactionEncoder\n\nfrom surprise import SVD\nfrom surprise import Reader, Dataset\nfrom surprise import accuracy\nfrom surprise.prediction_algorithms import KNNBasic\nfrom surprise.model_selection import train_test_split as surprise_train_test_split\n\nfrom lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k\nfrom lightfm.evaluation import auc_score\n\nfrom sklearn.model_selection import train_test_split as sklearn_train_test_split\nfrom scipy.sparse import csr_matrix","290cbc95":"df = pd.read_csv('..\/input\/sbermarket-internship-competition\/train.csv').drop(columns=['order_completed_at'])\nsubmit_df = pd.read_csv('..\/input\/sbermarket-internship-competition\/sample_submission.csv')","aee2d482":"users = []\nitems = []\n\nfor row in submit_df.to_numpy():\n    user, item = map(int, row[0].split(';'))\n    users.append(user)\n    items.append(item)\n\nusers = list(set(users))\nitems = list(set(items))\n\ndf_actual = df.loc[df['cart'].isin(items)]\ndf_actual = df.loc[df['user_id'].isin(users)]","45c1f7a9":"users_items_dict = {}\n\nfor row in df_actual.to_numpy():\n    if row[0] not in users_items_dict:\n        users_items_dict[row[0]] = []\n    users_items_dict[row[0]].append(row[1])\n    \nusers_items_dict = dict(sorted(users_items_dict.items()))","5130a87b":"data_arr = []\ndata_dubles_arr = []\ntrain_arr = []\ntrain_dubles_arr = []\ntest_arr = []\n\nfor wait, user_basket in enumerate(list(users_items_dict.values())):\n    print(f'{wait+1}\/{len(users_items_dict)}', end='   \\r')\n    basket_len = len(np.unique(user_basket))\n    if basket_len >= 20:\n        train_arr.append([])\n        train_dubles_arr.append([])\n        test_arr.append([])\n        \n        to_train = int(basket_len * 0.8)\n        for item in user_basket:\n            if to_train > 0:\n                if item not in train_arr[-1]:\n                    to_train -= 1\n                    train_arr[-1].append(item)\n                else:\n                    if item not in train_dubles_arr[-1]:\n                        train_dubles_arr[-1].append(item)\n                    train_dubles_arr[-1].append(item)\n            else:\n                if item not in test_arr[-1]:\n                    test_arr[-1].append(item)\n    \n    data_arr.append([])\n    data_dubles_arr.append([])\n    for item in user_basket:\n        if item not in data_arr[-1]:\n            data_arr[-1].append(item)\n        else:\n            if item not in data_dubles_arr[-1]:\n                data_dubles_arr[-1].append(item)\n            data_dubles_arr[-1].append(item)","77949c6b":"def matrix_df_proced(matrix_df):\n    to_add = []\n    for i in items:\n        if i not in matrix_df:\n            to_add.append(i)\n    matrix_df[to_add] = 0.0\n    matrix_df = matrix_df.reindex(sorted(matrix_df.columns), axis=1)\n    return matrix_df","c24effd4":"one_hot_encoding = TransactionEncoder()\n\none_hot_train = one_hot_encoding.fit(train_arr).transform(train_arr).astype('float')\none_hot_train_df = pd.DataFrame(one_hot_train, columns=one_hot_encoding.columns_)\none_hot_train_df = matrix_df_proced(one_hot_train_df)\nsparse_train = csr_matrix(one_hot_train_df.to_numpy(), dtype=np.float32)\n\none_hot_test = one_hot_encoding.fit(test_arr).transform(test_arr).astype('float')\none_hot_test_df = pd.DataFrame(one_hot_test, columns=one_hot_encoding.columns_)\none_hot_test_df = matrix_df_proced(one_hot_test_df)\nsparse_test = csr_matrix(one_hot_test_df.to_numpy(), dtype=np.float32)\n\none_hot_data = one_hot_encoding.fit(data_arr).transform(data_arr).astype('float')\none_hot_data_df = pd.DataFrame(one_hot_data, columns=one_hot_encoding.columns_)\none_hot_data_df = matrix_df_proced(one_hot_data_df)\nsparse_data = csr_matrix(one_hot_data_df.to_numpy(), dtype=np.float32)","03b12a29":"def check_advice(advice, test_data):\n    prec = []\n    dense = []\n    for i in range(len(test_data)):\n        print(f'{i+1}\/{len(test_data)}', end='     \\r')\n        dense_counter = 0\n        for item in advice[i]:\n            if item in test_data[i]:\n                prec.append(1)\n                dense.append(1)\n                dense_counter += 1\n            else:\n                prec.append(0)\n        for _ in range(len(test_data[i])-dense_counter):\n            dense.append(0)\n    prec = np.round(np.mean(prec)*100, 2)\n    dense = np.round(np.mean(dense)*100, 2)\n    print(f'precision => {prec}%')\n    print(f'dense     => {dense}%')\n    return prec, dense","48f699a7":"def get_repeated_advice(data_arr, point=2):\n    repeated_advice = []\n    for wait, basket in enumerate(data_arr):\n        print(f'{wait+1}\/{len(data_arr)}', end='     \\r')\n        repeated_advice.append([])\n        cart, count = np.unique(basket, return_counts=True)\n        for i in range(len(cart)):\n            if count[i] >= point:\n                repeated_advice[-1].append(cart[i])\n    return repeated_advice","9cad7fdc":"repeated_train_advice = {}\n\nfor i in range(10)[2:6]:\n    print(f'point     ==> {i}')\n    advice = get_repeated_advice(train_dubles_arr, point=i)\n    check_advice(advice, test_arr)\n    repeated_train_advice[i] = advice\n    print('\\n')","4adff54e":"def get_popularity_advice(data_arr, point=0.2):\n    data_arr_flat = [item for arr in data_arr for item in arr]\n    arr, count = np.unique(data_arr_flat, return_counts=True)\n    advice = arr[np.where(count\/len(data_arr) >= point)[0]]\n    popularity_advice = [advice for _ in range(len(data_arr))]\n    return popularity_advice","80e8dc7c":"popularity_train_advice = {}\n\nfor i in range(10)[2:6]:\n    print(f'point     ==> {i\/10}')\n    advice = get_popularity_advice(train_arr, point=i\/10)\n    check_advice(advice, test_arr)\n    popularity_train_advice[i] = advice\n    print('\\n')","1281b22e":"def get_model_df(arr, user_from, user_till):\n    data = []\n    for user, basket in enumerate(arr[user_from:user_till]):\n        print(f'{user+1}\/{user_till-user_from}', end='   \\r')\n        data_row = {}\n        for item in items:\n            data_row[item] = 0.0\n        for item in basket:\n            data_row[item] = 1.0\n        for item, val in data_row.items():\n            data.append({'user_id': user+user_from, 'cart': item, 'purchase': val})\n    return pd.DataFrame(data)\n\ndef get_model_advice(model_preds, point=0.5):\n    model_advice = []\n    true_val = []\n    cur_user = -1\n    for wait, pred in enumerate(model_preds):\n        print(f'{wait+1}\/{len(model_preds)}', end='        \\r')\n        user, item = map(int, pred[0:2])\n        if cur_user != user:\n            model_advice.append([])\n            true_val.append([])\n            cur_user = user\n        if pred[3] >= point:\n            model_advice[-1].append(item)\n        if pred[2] == 1.0:\n            true_val[-1].append(item)\n    return model_advice, true_val\n\n    \n\nsvd_model = SVD()\nreader = Reader(rating_scale=(0.0, 1.0))","4d20aed8":"train_df = get_model_df(train_arr, 0, 1000)\ntest_df = get_model_df(test_arr, 0, 1000)\n\ndata = Dataset.load_from_df(train_df[['user_id', 'cart', 'purchase']], reader)\ntrainset = data.build_full_trainset()\n\nprint('svd train')\nsvd_model.fit(trainset)\nprint('svd pred')\nsvd_preds = svd_model.test(test_df.to_numpy())","7107e432":"svd_train_advice = {}\n\nfor i in range(10)[3:8]:\n    print(f'point     ==> {(i+0.5)\/10}')\n    svd_preds_by_user, svd_real_val = get_model_advice(svd_preds, point=(i+0.5)\/10)\n    check_advice(svd_preds_by_user, svd_real_val)\n    svd_train_advice[(i+0.5)] = svd_preds_by_user\n    print('\\n')","07fea0cf":"alpha = 0.0001\n\nmodel = LightFM(no_components=30, loss='warp', user_alpha=alpha, item_alpha=alpha)\nmodel.fit_partial(sparse_train, epochs=10)","7cde2e74":"light_train_advice = {}\n\nfor i in range(10)[5:]:\n    print(f'point     ==> {i\/10}')\n    \n    advice = []\n    true_val = []\n\n    for j in range(len(train_arr)):\n        print(f'{j+1}\/{len(train_arr)}', end='    \\r')\n        point = int(np.ceil( len(train_arr[j]) * (i\/10) ))\n        preds = model.predict(j, np.arange(len(items)))\n        preds = np.argpartition(preds, -point)[-point:]\n        advice.append(preds)\n        true_val.append(np.where(one_hot_test_df.iloc[j].to_numpy() == 1)[0])\n\n    check_advice(advice, true_val)\n    light_train_advice[i] = advice\n    print('\\n')","75329429":"def get_overal_advice(advices:list):\n    overal_advice = []\n    for i in range(len(advices[0])):\n        print(f'{i+1}\/{len(advices[0])}', end='        \\r')\n        preds = []\n        for advice in advices:\n            preds.extend(advice[i])\n        preds = np.unique(preds)\n        overal_advice.append(preds)\n    return overal_advice\n\ndef get_overal_advice_majority(advices:list):\n    overal_advice = []\n    for i in range(len(advices[0])):\n        print(f'{i+1}\/{len(advices[0])}', end='        \\r')\n        preds = []\n        for advice in advices:\n            preds.extend(advice[i])\n        preds, count = np.unique(preds, return_counts=True)\n        preds = np.array(preds)[np.where(count > len(advices)\/2)[0]]\n        overal_advice.append(preds)\n    return overal_advice","0e0cb150":"results = []\nlimit = 1000\n\nfor key_1 in repeated_train_advice.keys():\n    for key_2 in popularity_train_advice.keys():\n        for key_3 in svd_train_advice.keys():\n            for key_4 in light_train_advice.keys():\n                print(f'== {key_1} | {key_2} | {key_3} | {key_4} ==')\n                overal_advice = get_overal_advice([repeated_train_advice[key_1][:limit], \n                                                            popularity_train_advice[key_2][:limit],\n                                                            svd_train_advice[key_3][:limit],\n                                                            light_train_advice[key_4][:limit]])\n                prec, dense = check_advice(overal_advice, test_arr[:limit])\n                results.append({'keys': [key_1, key_2, key_3, key_4], 'prec': prec, 'dense': dense, 'score': 0})","ec192823":"for i in range(len(results)):\n    results[i]['score'] = 0\n\nresults = sorted(results, key=lambda k: k['prec'], reverse=True)\nfor i in range(20)[:len(results)]: results[i]['score'] += 20-i\n\nresults = sorted(results, key=lambda k: k['dense'], reverse=True)\nfor i in range(20)[:len(results)]: results[i]['score'] += 20-i\n\nresults = sorted(results, key=lambda k: k['score'], reverse=True)\nresults[:50]","9f808b97":"repeated_real_advice = get_repeated_advice(data_dubles_arr, point=3)","bdd55afb":"popularity_real_advice = get_popularity_advice(data_arr, point=5\/10)","96f6dc97":"pack_size = 1003\nsvd_real_advice = []\n\nfor init in range(len(users))[::pack_size]:\n    data_df = get_model_df(data_arr, init, init + pack_size)\n\n    data = Dataset.load_from_df(data_df[['user_id', 'cart', 'purchase']], reader)\n    trainset = data.build_full_trainset()\n    \n    print('svd train', end='     \\r')\n    svd_model.fit(trainset)\n    print('svd pred', end='     \\r')\n    svd_preds = svd_model.test(data_df.to_numpy())\n    \n    svd_preds_by_user, _ = get_model_advice(svd_preds, point=5.5\/10)\n    svd_real_advice.extend(svd_preds_by_user)\n    print(f'progress ==> {init+pack_size}\/{len(users)}')","ac508fdf":"alpha = 0.0001\n\nmodel = LightFM(no_components=30, loss='warp', user_alpha=alpha, item_alpha=alpha)\nmodel.fit_partial(sparse_data, epochs=10)\n\nlightfm_real_advice = []\n\nfor i in range(len(data_arr)):\n    print(f'{i+1}\/{len(data_arr)}', end='    \\r')\n    point = int(np.ceil( len(data_arr[i]) * (5\/10) ))\n    preds = model.predict(i, np.arange(len(items)))\n    preds = np.argpartition(preds, -point)[-point:]\n    lightfm_real_advice.append(preds)","4dbc9c3a":"overal_real_advice = get_overal_advice_majority([repeated_real_advice, \n                                                 popularity_real_advice, \n                                                 svd_real_advice, \n                                                 lightfm_real_advice])","89eb263c":"def prepare_submit(advice, name='submit'):\n    submit_df['target'].values[:] = 0\n    for i, row in enumerate(submit_df.to_numpy()):\n        print(f'{i+1}\/{len(submit_df)}', end='    \\r')\n        user, item = map(int, row[0].split(';'))\n        if item in advice[users.index(user)]:\n            submit_df.at[i,'target'] = 1\n    submit_df.to_csv(f'{name}.csv', index=False)","cccd54a6":"prepare_submit(overal_real_advice, name='repeated + popular + svd + lightfm majority (calibrated)')","17f299de":"# MODELS","40a57c7b":"### lightfm","1be6a975":"### svd","70407199":"## Load Data","45751368":"## Repeated\n\u0415\u0441\u043b\u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \u043a\u0443\u043f\u0438\u043b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u043e\u0434\u0438\u043d \u0438 \u0442\u043e\u0442 \u0436\u0435 \u0442\u043e\u0432\u0430\u0440, \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u044d\u0442\u043e\u0442 \u0436\u0435 \u0442\u043e\u0432\u0430\u0440 \u044e\u0437\u0435\u0440 \u0437\u0430\u0445\u043e\u0447\u0435\u0442 \u043a\u0443\u043f\u0438\u0442\u044c \u0435\u0449\u0435 \u0440\u0430\u0437","ab7371f6":"## Set Data Vars\n\u0438\u043d\u0438\u0446\u0438\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439\/\u0442\u043e\u0432\u0430\u0440\u043e\u0432\n\n\u0442\u0430\u043a\u0436\u0435 \u0444\u0438\u043b\u044c\u0442\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u043c \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b (\u0442\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432 \u0441\u0430\u0431\u043c\u0438\u0442\u0435)","21e72fff":"## Data & Train & Test Split\n\u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0432 \u0441\u043e\u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0438 80\/20, \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438","64d14b7c":"### popularity","8b6da537":"# DATA","eb28efb0":"## Sparse Matrix\n\u0441\u043e\u0437\u0434\u0430\u0435\u043c \u0440\u0430\u0437\u0440\u044f\u0436\u0435\u043d\u043d\u044b\u0435 \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0435\u0439 lightFM","f87c7f67":"# SUMARIZE","7ae3af1d":"# IMPORTS","7c252d47":"### repeated","754d47cf":"## SVD Model","e65e222c":"### overal","b036494f":"## Test Advices\n\u0442\u0435\u0441\u0442\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0432\u0441\u0435\u0445 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0432\u044b\u0431\u0438\u0440\u0430\u044f \u0438\u0445 \u0441\u0432\u044f\u0437\u043a\u0443 \u0441 \u043b\u0443\u0447\u0448\u0438\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438","e001c2f9":"## Get Real Deal Advices\n\u043d\u0430\u0445\u043e\u0434\u0438\u043c \u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","8230611b":"## Prepare Submit","ba174c7a":"## Popularity\n\u0415\u0441\u043b\u0438 \u0442\u043e\u0432\u0430\u0440\u044b \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b \u0441\u0440\u0435\u0434\u0438 \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0430 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439, \u0442\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0438\u043c \u0437\u0430\u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043e\u0432\u0430\u043b\u0441\u044f \u0431\u044b \u0438 \u0442\u043e\u0442 \u043a\u0442\u043e \u0435\u0449\u0435 \u0435\u0433\u043e \u043d\u0435 \u043f\u0440\u0438\u043e\u0431\u0440\u0435\u043b","ff8c4cfb":"## LightFM"}}