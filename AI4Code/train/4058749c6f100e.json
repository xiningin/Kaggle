{"cell_type":{"989a0e1b":"code","d71b70e3":"code","4e3d71b2":"code","940d9e6f":"code","63842896":"code","17212d92":"code","a5aa45ee":"code","8d102174":"code","4110ea33":"code","ba96efeb":"code","516f973c":"code","f24c2838":"code","4927fa22":"code","732b24bf":"code","e8ad4330":"code","7e5667ac":"code","78508e9a":"code","7992964b":"code","0003848d":"code","6d42bf4e":"code","81d3c224":"code","e5ce105f":"code","b0e9a142":"code","fed5bae3":"code","f46de881":"code","e2a8c149":"code","1a845620":"code","51367719":"markdown","eaf76b18":"markdown","e8b26faa":"markdown","b763edd4":"markdown","e005454f":"markdown","58a318ea":"markdown","572f0457":"markdown","3443394f":"markdown","29128cb2":"markdown","5d5893f1":"markdown","378f4764":"markdown","8fe609e9":"markdown","591f1f6b":"markdown","90f3e81b":"markdown","6ec4a8fe":"markdown","708fabb2":"markdown","fa3dbd5c":"markdown","d7b953cb":"markdown","4f7f837a":"markdown","070a7757":"markdown","4a5e9e5f":"markdown","1f09ef7e":"markdown","24f5f018":"markdown","525ad363":"markdown","61a80cca":"markdown","cab7114c":"markdown","d709f1ff":"markdown","44127f67":"markdown","a10d1572":"markdown","4e38f25e":"markdown"},"source":{"989a0e1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d71b70e3":"df = pd.read_csv(\"..\/input\/sample-blog-corpus\/blogtext.csv\")","4e3d71b2":"df.columns","940d9e6f":"df.shape","63842896":"df.head()","17212d92":"df.isnull().sum()","a5aa45ee":"df2 = df.head(7000)\n#df2 = df","8d102174":"# Select only alphabets\nimport re\ndf2.text = df2.text.apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))\n\n# Convert text to lowercase\ndf2.text = df2.text.apply(lambda x: x.lower())\n\n# Strip unwanted spaces\ndf2.text = df2.text.apply(lambda x: x.strip())\n\n# Remove stopwords\nfrom nltk.corpus import stopwords\nstopwords = set(stopwords.words('english'))\ndf2.text = df2.text.apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords]))","4110ea33":"df2.text[5]","ba96efeb":"df2['labels'] = df2.apply(lambda row: [row['gender'], str(row['age']), \n                                       row['topic'], row['sign']], axis=1)","516f973c":"df3 = df2[['text','labels']]","f24c2838":"df3.head()","4927fa22":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df3.text.values, df3.labels.values, \n                                                    test_size=0.20, random_state=42)","732b24bf":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\nX_train_bow = vectorizer.fit_transform(X_train)\nX_test_bow = vectorizer.transform(X_test)","e8ad4330":"vectorizer.get_feature_names()[:6]","7e5667ac":"X_train_bow.toarray()","78508e9a":"label_counts = dict()\n\nfor labels in df3.labels.values:\n    for label in labels:\n        if label in label_counts:\n            label_counts[label] += 1\n        else:\n            label_counts[label] = 1","7992964b":"label_counts","0003848d":"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer(classes=sorted(label_counts.keys()))\ny_train = mlb.fit_transform(y_train)\ny_test = mlb.transform(y_test)","6d42bf4e":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(solver='lbfgs')\nclf = OneVsRestClassifier(clf)","81d3c224":"clf.fit(X_train_bow, y_train)","e5ce105f":"predicted_labels = clf.predict(X_test_bow)\npredicted_scores = clf.decision_function(X_test_bow)","b0e9a142":"pred_inversed = mlb.inverse_transform(predicted_labels)\ny_test_inversed = mlb.inverse_transform(y_test)","fed5bae3":"for i in range(5):\n    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n        X_test[i],\n        ','.join(y_test_inversed[i]),\n        ','.join(pred_inversed[i])\n    ))","f46de881":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import recall_score\n\ndef print_evaluation_scores(y_val, predicted):\n    print('Accuracy score: %.2f' %(accuracy_score(y_val, predicted)*100))\n    print('F1 score (micro): %.2f' %(f1_score(y_val, predicted, average='micro')*100))\n    print('Average precision score (micro): %.2f' %(average_precision_score(y_val, predicted, average='micro')*100))\n    print('Average recall score (micro): %.2f' %(recall_score(y_val, predicted, average='micro')*100))\n    print('F1 score (macro): %.2f' %(f1_score(y_val, predicted, average='macro')*100))\n    print('Average precision score (macro): %.2f' %(average_precision_score(y_val, predicted, average='macro')*100))\n    print('Average recall score (macro): %.2f' %(recall_score(y_val, predicted, average='macro')*100))","e2a8c149":"print('Bag-of-words')\nprint_evaluation_scores(y_test, predicted_labels)","1a845620":"import matplotlib.pyplot as plt\nRecall = np.array([41.4,68.1,50.4,59.1])\nlabel = np.array(['Accuracy score','F1 score (micro)',\n                  'Avg precision score (micro)','Avg recall score(micro)'])\nindices = np.argsort(Recall)\ncolor = plt.cm.rainbow(np.linspace(0, 1, 9))\n\nplt.rcParams['figure.figsize'] = (18, 7)\nplt.bar(range(len(indices)), Recall[indices], color = color)\nplt.xticks(range(len(indices)), label[indices])\nplt.title('Model metrics', fontsize = 30)\nplt.grid()\nplt.tight_layout()\nplt.show()","51367719":"### 12. Vectorize the data\n#### 12.1 Create Bag of Words\n* Use CountVectorizer\n* Transform the traing and testing data","eaf76b18":"### 9. Select only required columns from your dataframe","e8b26faa":"### 3. Get the dataframe size","b763edd4":"### 15. Create a dictionary to get label counts","e005454f":"### 6. Subset the data \n\nMake our data short during development. So that overall process takes less time to execute and we are able to rectify all the errors fast, and check if our code is running smoothly.\nWhen everything is sorted at last, we will load about 1% of the data (~7K) as 681K rows is causing the notebook to crash","58a318ea":"#### We get a F1 score (micro method) of about 70% with the simplistic model","572f0457":"![image.png](attachment:image.png)","3443394f":"### 20. Make predictions\n* Get predicted labels and scores","29128cb2":"### 13. Let us have a look at some feature names","5d5893f1":"### 5. Check if we have missing value for any columns","378f4764":"### 11. Create training and testing data","8fe609e9":"### 14. View term-document matrix","591f1f6b":"### 17. Multi label binarizer\n#### 17.1 Load a multilabel binarizer and fit it on the labels.","90f3e81b":"### 22. Calculate accuracy\n- Accuracy\n- F1-score\n- Precision\n- Recall","6ec4a8fe":"### 8. Merge the label coulmns\n\nMerge all the label columns together, so that we have all the tags together for a particular sentence","708fabb2":"### 2. Get the column names","fa3dbd5c":"### 10. Print the final dataframe","d7b953cb":"### 23. Plot the model metrics","4f7f837a":"### 19. Fit the classifier","070a7757":"### 7. Preprocess text\n#### 7.1 Preprocess values of text column\n\n* Remove unwanted characters\n* Convert text to lowercase\n* Remove unwanted spaces\n* Remove stopwords","4a5e9e5f":"### Micro- and macro-averages (for whatever metric) will compute slightly different things, and thus their interpretation differs. A macro-average will compute the metric independently for each class and then take the average (hence treating all classes equally), whereas a micro-average will aggregate the contributions of all classes to compute the average metric. In a multi-class classification setup, micro-average is preferable if we suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).","1f09ef7e":"### 1. Read the csv file","24f5f018":"### 4. See some of the sample rows","525ad363":"### 18. Classifier\n#### 18.1 Use a linear classifier of your choice, wrap it up in OneVsRestClassifier to train it on every label. (Code provided in project description)","61a80cca":"### 7.2 Verify the preprocessing steps by looking over some values","cab7114c":"### 0. Setup the environment","d709f1ff":"### 16. Print the dictionary","44127f67":"### 21. Print 5 samples","a10d1572":"### We see that the model is able to predict either exactly or a portion of the true label fairly accurately. Example 34,Sagittarius,female,indUnk is predicted correctly while 35,Aries,Technology,male is able to predict sometime correctly and other time till Aries,male only","4e38f25e":"### Micro average makes sense here due to class imbalance"}}