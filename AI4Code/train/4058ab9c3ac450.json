{"cell_type":{"37d3a366":"code","f7c3e031":"code","746accc3":"code","c347cc10":"code","5474072e":"code","25b9d871":"code","615b340f":"code","03f620f1":"code","e66b4080":"code","484e33a2":"code","0b91478b":"code","267c146e":"code","66e70f76":"code","1540aabf":"code","05f6098b":"code","695ed334":"code","7a061838":"code","c5f98799":"code","fd640992":"code","b5506e28":"code","74477641":"code","023c657a":"markdown","a789cb6e":"markdown","502e64ac":"markdown","fcbc2493":"markdown","4dfa9f73":"markdown","404f0405":"markdown","0279b563":"markdown","7583ebd2":"markdown","f3ba63c0":"markdown","6593ab9a":"markdown"},"source":{"37d3a366":"import numpy as np \nimport pandas as pd \nimport os, gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport lightgbm as lgb\nimport xgboost as xgb\nimport optuna\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore')","f7c3e031":"PATH = '..\/input\/tabular-playground-series-feb-2021\/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')\nsample = pd.read_csv(PATH + 'sample_submission.csv')\n\nprint(train.shape, test.shape)","746accc3":"train.head(10)","c347cc10":"test.head(10)","5474072e":"train.info()","25b9d871":"test.info()","615b340f":"fig, ax = plt.subplots(1, 2, figsize=(16, 6))\nsns.distplot(train['target'], ax=ax[0])\nsns.boxplot(train['target'], ax=ax[1])","03f620f1":"train.describe()","e66b4080":"FEATURES = train.drop(['id', 'target'], 1).columns\nFEATURES","484e33a2":"fig, ax = plt.subplots(7, 2, figsize=(16, 40))\n\nax = ax.flatten()\n\ncont_features = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\n\nfor k, i in enumerate(cont_features):\n    sns.distplot(train[i], ax=ax[k], hist=False, label='train')\n    sns.distplot(test[i], ax=ax[k], hist=False, label='test')","0b91478b":"fig, ax = plt.subplots(10, 2, figsize=(16, 50))\nax = ax.flatten()\n\ncat_features = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\n\nfor k, i in enumerate(cat_features):\n    sns.countplot(train[i], ax=ax[2*k], label='train')\n    sns.countplot(test[i], ax=ax[(2*k)+1], label='test')\n    ","267c146e":"for i in cat_features:\n    le = LabelEncoder()\n    le.fit(train[i])\n    train[i] = le.transform(train[i])\n    test[i] = le.transform(test[i])\n\ntrain.head()","66e70f76":"x = train.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(x, annot=True)","1540aabf":"cv = KFold(n_splits=5, shuffle=True)\ncv","05f6098b":"X = train[FEATURES]\ny = train.target\nprint(X.shape, y.shape)","695ed334":"model = lgb.LGBMRegressor()\nmodel\n\nNUM_BOOST_ROUNDS = 20000\nEARLY_STOPPING_ROUNDS = 500\nVERBOSE_EVAL = 0\n\noof_df = train[['id', 'target']].copy()\nfold_ = 1\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.3, shuffle=True)","7a061838":"def objective(trial):\n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val)\n\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": 1,\n        \"boosting_type\": \"gbdt\",\n\n\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 0, 256),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 31),\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.4, 10),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 0.9),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 0.9),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 5, 15),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    model = lgb.train(param,\n                      train_set, \n                         num_boost_round=NUM_BOOST_ROUNDS,\n                         early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                         verbose_eval=VERBOSE_EVAL,\n                         valid_sets=[train_set, val_set])\n    \n    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n    scc = math.sqrt(mean_squared_error(val_preds, y_val))\n    return -1*scc","c5f98799":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\ntrial = study.best_trial\ntrial.params['metric'] = 'rmse'","fd640992":"print(trial.params)","b5506e28":"for train_idx, val_idx in cv.split(X, y):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y[train_idx], y[val_idx]\n\n    train_set = lgb.Dataset(X_train, y_train)\n    val_set = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(trial.params,\n                      train_set,\n                      num_boost_round=NUM_BOOST_ROUNDS,\n                      early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n                      verbose_eval=-10,\n                      valid_sets=[train_set, val_set]\n                      )\n\n    val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n    test_preds = model.predict(\n        test[FEATURES], num_iteration=model.best_iteration)\n\n    oof_df.loc[oof_df.iloc[val_idx].index, 'oof'] = val_preds\n    sample[f'fold{fold_}'] = test_preds\n\n    score = mean_squared_error(\n        oof_df.loc[oof_df.iloc[val_idx].index]['target'], oof_df.loc[oof_df.iloc[val_idx].index]['oof'])\n    print(math.sqrt(score))\n    fold_ += 1","74477641":"print(math.sqrt(mean_squared_error(oof_df.target, oof_df.oof)))\nsample['target'] = sample.drop(['id', 'target'], 1).mean(axis=1)\nsample[['id', 'target']].to_csv('submission.csv', index=False)","023c657a":"## <a>Loading Packages and Data<\/a>","a789cb6e":"## <a>Model<\/a>","502e64ac":"The countplots look same for cat0, cat1, cat2, cat4, cat5, cat6 and different for the rest. Let's look at the correlations now. We can LabelEncode the categorial variables before plotting correlation matrix.","fcbc2493":"All the features are multimodal with varying number of peaks. The feature distributions from train and test set are almost same.  Let's check the categorical features now.","4dfa9f73":"Just like the target variable distribution of TPS Jan 2021, the target variable has a bimodel distribution. Outliers are present. We'll be using LightGBM so no need for transformations for now.","404f0405":"## <a>Introduction<\/a>\n\nWelcome to this new competition series by Kaggle. This is somewhat in between basic playground competitions and competitive featured ones. \n\nIn this competition, we are given a regression task. We will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.\n\nLet's get started.","0279b563":"## <a>EDA<\/a>\n\nLet's first check the distribution of target variable.\n","7583ebd2":"1. There is a correlation cluster from cont5 to cont12, but the highest value of correlation coefficient is 0.63, so no need to drop any features.\n2. Features are not correlated to the target.\n3. This is very similar to the datasets in TPS Jan2021.","f3ba63c0":"We've no missing values in the train and test sets. Let's move on to EDA.","6593ab9a":"Both train and test are medium sized datasets. Let's take a look at the train set.\n"}}