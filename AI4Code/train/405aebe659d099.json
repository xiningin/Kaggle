{"cell_type":{"5adf214d":"code","14c1f926":"code","a9f6273a":"code","71b6aad4":"code","655c3d62":"code","b7772ac0":"code","56f9e427":"code","6ded0bac":"code","98956a35":"code","589814b4":"code","44ed8b53":"code","25a2d172":"code","65efbbf3":"code","e7f2fcc7":"code","cdc7c86e":"code","643460e2":"code","2431b57a":"code","b7be5667":"code","7cbba761":"code","f884124a":"code","0732edc6":"code","bd616896":"code","b1981323":"code","36e0d431":"code","fa8764c3":"code","8536cdf1":"code","d4d332d0":"code","7efd1d81":"code","fb030c04":"code","a90a8150":"code","0bcf240d":"code","1afd3cd5":"code","b1c30c5c":"code","42f6d157":"code","0c102d00":"code","32351a5f":"code","7c043c86":"code","cfc3cc43":"code","d3a0016b":"code","7f9bb7f8":"code","f0199bae":"code","238bd43e":"code","071ccfe5":"markdown","3d49059c":"markdown","bd66771c":"markdown","f552d86a":"markdown","2fcdf75c":"markdown","c1e968b1":"markdown"},"source":{"5adf214d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\n%matplotlib inline \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV , LassoCV, Ridge , Lasso\nimport scipy.stats as st","14c1f926":"df = pd.read_csv('..\/input\/bike-sharing-demand\/train.csv')\ndf1 = pd.read_csv('..\/input\/bike-sharing-demand\/test.csv')\nsample_submission_df = pd.read_csv('..\/input\/bike-sharing-demand\/sampleSubmission.csv')\nprint(df.shape)\ndf","a9f6273a":"df1.head()","71b6aad4":"df.dtypes","655c3d62":"df['datetime'].value_counts()","b7772ac0":"df['dayofweek'] = pd.DatetimeIndex(df['datetime']).dayofweek\ndf.tail(2)","56f9e427":"df['month'] = pd.DatetimeIndex(df['datetime']).month\ndf.tail(2)","6ded0bac":"df = df.drop(['datetime','casual','registered'],axis = 1)","98956a35":"sns.distplot((df['count']**2))\nplt.show()","589814b4":"sns.distplot(df['count'])\nplt.show()","44ed8b53":"(np.log(df['count']**2)).isnull().sum()","25a2d172":"#sns.pairplot(df)","65efbbf3":"df.corr()","e7f2fcc7":"df['season'].value_counts()","cdc7c86e":"c = df.groupby(by='season')\nc1 = c.get_group(4)\nc2 = c.get_group(3)\nc3 = c.get_group(2)\nc4 = c.get_group(1)","643460e2":"from scipy.stats import f_oneway, ttest_ind\nf_oneway(c1['count'],c2['count'],c3['count'],c4['count'])","2431b57a":"df.boxplot(column = 'count', by = 'holiday')\nplt.show()","b7be5667":"h = df.groupby(by='holiday')\nh1 = h.get_group(1)\nh0 = h.get_group(0)","7cbba761":"ttest_ind(h1['count'], h0['count'])","f884124a":"df.boxplot(column = 'count', by = 'workingday')","0732edc6":"w = df.groupby(by='workingday')\nw1 = w.get_group(1)\nw0 = w.get_group(0)","bd616896":"ttest_ind(w1['count'], w0['count'])","b1981323":"df.boxplot(column = 'count', by = 'weather')\nplt.show()","36e0d431":"wh = df.groupby(by='weather')\nwh1 = wh.get_group(4)\nwh2 = wh.get_group(3)\nwh3 = wh.get_group(2)\nwh4 = wh.get_group(1)","fa8764c3":"f_oneway(wh1['count'],wh2['count'],wh3['count'],wh4['count'])","8536cdf1":"df.columns","d4d332d0":"df = df.drop(['holiday','workingday', 'month'], axis = 1)","7efd1d81":"df.columns","fb030c04":"#sns.pairplot(df)","a90a8150":"df.corr()","0bcf240d":"from sklearn.preprocessing import StandardScaler\nX = df.drop('count', axis = 1)\ny = df['count']\nX1 = sm.add_constant(X)","1afd3cd5":"import warnings\nwarnings.filterwarnings('ignore')\nimport statsmodels.api as sm\nLR = sm.OLS(y, X1).fit()\nLR.summary()","b1c30c5c":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor,BaggingRegressor\nimport warnings \nfrom sklearn.exceptions import DataConversionWarning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression","42f6d157":"knn = KNeighborsRegressor()\nknn_params = {'n_neighbors': np.arange(3,20), 'weights': ['uniform','distance']}\nGS = GridSearchCV(knn, knn_params,cv = 5, scoring = 'neg_mean_squared_error' )\nGS.fit(X, y)\nGS.best_params_","0c102d00":"from sklearn.preprocessing import PolynomialFeatures, StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\nGS.fit(X_scaled, y)\nGS.best_params_","32351a5f":"dt = DecisionTreeRegressor(random_state = 0)\ndt_params = {'max_depth': np.arange(3,55), 'min_samples_leaf' : np.arange(2,20) }\nGS_dt = GridSearchCV(dt, dt_params,cv = 10, scoring = 'neg_mean_squared_error' )\nGS_dt.fit(X_scaled, y)\nGS_dt.best_params_","7c043c86":"KNN = KNeighborsRegressor(n_neighbors = 19, weights = 'uniform')\nDT = DecisionTreeRegressor(max_depth = 6, min_samples_leaf = 11, random_state = 0)\nRF = RandomForestRegressor(n_estimators = 27, random_state = 0)\nAB_RF = AdaBoostRegressor(base_estimator = RF, n_estimators = 100, random_state = 0)\nGBoost = GradientBoostingRegressor(n_estimators = 100) ","cfc3cc43":"models1 = []\nmodels1.append(('KNNRegressor', KNN ))\nmodels1.append(('DTRegressor', DT))\nmodels1.append(('RFRegressor', RF))\nmodels1.append(('ADABoostRegressor',AB_RF))\nmodels1.append(('GradientBoostRegressor',GBoost))\n","d3a0016b":"from sklearn import model_selection\nresults = []\nnames = []\nfor name,model in models1:\n    kfold = model_selection.KFold(shuffle = True, n_splits = 10,random_state = 0)\n    cv_results = model_selection.cross_val_score(model, X_scaled, y, cv = kfold, scoring = 'neg_mean_squared_log_error')\n    #print(cv_results)\n    results.append(np.sqrt(np.abs(cv_results))) # every fold, RMSE scores... only for plotting purposes\n    names.append(name)\n    print(\"%s:%f(%f)\" %(name, np.mean(np.sqrt(np.abs(cv_results))), \n                        np.std(np.sqrt(np.abs(cv_results)), ddof = 1)))","7f9bb7f8":"#from sklearn.ensemble import VotingRegressor\n#stacked = VotingRegressor(estimators = [('GradientBoostRegressor',GBoost), ('KNNRegressor', KNN ), ('ADABoostRegressor',AB_RF)])","f0199bae":"models = []\n#models.append(('?VotingRegressor', stacked))","238bd43e":"for name,model in models:\n    kfold = model_selection.KFold(shuffle = True, n_splits = 17,random_state = 0)\n    cv_results = model_selection.cross_val_score(model, X_scaled, y, cv = kfold, scoring = 'neg_mean_squared_error')\n    #print(cv_results)\n    results.append(np.sqrt(np.abs(cv_results))) # every fold, RMSE scores... only for plotting purposes\n    names.append(name)\n    print(\"%s:%f(%f)\" %(name, np.mean(np.sqrt(np.abs(cv_results))), \n                        np.std(np.sqrt(np.abs(cv_results)), ddof = 1)))","071ccfe5":"- DT Regressor","3d49059c":"- KNN regressor","bd66771c":"- Reject the null hypothesis : => can use this as Potential Variable !!!","f552d86a":"NOT a Potential Variable to fit a linear Model !","2fcdf75c":"- It is a potential variable","c1e968b1":"- NOT a Potential Variable to fit a linear Model !"}}