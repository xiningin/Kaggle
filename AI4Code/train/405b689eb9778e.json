{"cell_type":{"0ddca12d":"code","584444da":"code","7a07bbe0":"code","6f83cf13":"code","bf19863e":"code","a7645a83":"code","b33faa65":"code","7488cc95":"code","ca0c6a59":"code","72300902":"code","b95a9429":"code","82bca836":"code","fa2a47dd":"code","76602b99":"code","918db05d":"code","945df51d":"code","9e210de6":"code","a69e5d50":"code","ed449f60":"code","1cb4d43d":"code","c2652fbb":"code","099ed160":"code","17f07dbb":"markdown","777521f1":"markdown","45b7bdad":"markdown","8b1e21ce":"markdown","989e2b4e":"markdown","1a1e8ace":"markdown","e96e6a1b":"markdown","7de851e8":"markdown","14cff81e":"markdown","5af0f568":"markdown"},"source":{"0ddca12d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip install pyspellchecker\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nimport spacy\nimport string\npd.options.mode.chained_assignment = None\n\n\nimport seaborn as sns#Understanding my variables\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))    \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom spellchecker import SpellChecker\n\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom nltk import pos_tag\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import pairwise_distances","584444da":"full_data = pd.read_csv(\"..\/input\/customer-support-on-twitter\/twcs\/twcs.csv\")","7a07bbe0":"pd.options.display.float_format = '{:,.0f}'.format\nanswers = full_data.loc[full_data['author_id'] == \"sainsburys\"]","6f83cf13":"full_data.shape","bf19863e":"full_data.info()","a7645a83":"answers.info()","b33faa65":"answers.head()","7488cc95":"answers.columns[answers.isnull().any()]","ca0c6a59":"questionTweetId = []\nanswers[\"in_response_to_tweet_id\"] = answers[\"in_response_to_tweet_id\"].fillna(0.0).astype(int)\n\nquestionTweetId = answers[\"in_response_to_tweet_id\"].to_list()\n","72300902":"#Below was code used for various testing\/troubleshooting\n\n\n#tweetId = questionTweetId[3819]\n#tweetId = 758160\n#print(tweetId)\n\n#index = full_data.index\n#condition = full_data[\"tweet_id\"] == tweetId\n#indices = index[condition]\n#indices_list = indices.tolist()\n\n#condition1 = full_data[\"in_response_to_tweet_id\"] == tweetId\n#indices1 = index[condition1]\n#indices_list1 = indices1.tolist()\n\n#print(indices_list)\n\n#print(indices_list1)\n#if indices_list:\n   # print(full_data.at[indices_list[0], \"text\"])\n#if indices_list1:\n    #print(full_data.at[indices_list1[0], \"text\"])\n\n\n#full_data.iloc[[677615]]","b95a9429":"QandA = []\ni = len(questionTweetId)\nfor j in range(i):\n    tweetId = questionTweetId[j]\n    if tweetId != 0:\n        index = full_data.index\n        condition = full_data[\"tweet_id\"] == tweetId\n        indices = index[condition]\n        indices_list = indices.tolist()\n        condition1 = full_data[\"in_response_to_tweet_id\"] == tweetId\n        indices1 = index[condition1]\n        indices_list1 = indices1.tolist()\n        \n        #Below was used for testing\n        #print(\"TweetId:{}\".format(tweetId))\n        #print(indices_list[0])\n        \n        #The use of these if statements take advantage of the fact that empty sequences are false. This was implemented because of a missing tweetId, 758160\n        if indices_list:\n            Q = full_data.at[indices_list[0], \"text\"]   \n        if indices_list1:\n            A = full_data.at[indices_list1[0], \"text\"]\n        QandA.append([Q, A])","82bca836":"data_clean = pd.DataFrame(QandA, columns =['Question', 'Answer'])\ndata_clean","fa2a47dd":"data_clean.shape","76602b99":"data_clean = data_clean.sample(n=2000)\ndata_clean","918db05d":"data_clean[\"Original_Question\"] = data_clean[\"Question\"].astype(str)","945df51d":"data_clean[\"Original_Answer\"] = data_clean[\"Answer\"].astype(str)\ndata_clean[\"Answer\"] = data_clean[\"Answer\"].astype(str)\ndata_clean[\"Answer\"] = data_clean.apply(lambda row: re.sub(\"\\B#\\w+\", \"\", row['Answer']), axis=1)\ndata_clean[\"Answer\"] = data_clean.apply(lambda row: re.sub(\"\\B@\\w+\", \"\", row['Answer']), axis=1)","9e210de6":"lemmatizer = WordNetLemmatizer()\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\nstopwords = set(stopwords.words('english'))\nspell = SpellChecker()\ndef cleanText(text):\n    text = str(text).lower()\n    text = re.sub(\"\\B#\\w+\", \"\", text)\n    text = re.sub(\"\\B@\\w+\", \"\", text)\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n        text = \" \".join(corrected_text)\n        text = \" \".join([word for word in str(text).split() if word not in stopwords])\n    text.translate(str.maketrans('', '', string.punctuation))\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n","a69e5d50":"data_clean[\"Question\"] = data_clean[\"Question\"].apply(cleanText)","ed449f60":"data_clean","1cb4d43d":"def userAsk(text):\n    tfidf=TfidfVectorizer()\n    x_tfidf=tfidf.fit_transform(data_clean[\"Question\"]).toarray()\n\n    df_tfidf = pd.DataFrame(x_tfidf,columns=tfidf.get_feature_names())\n\n    Question_tfidf = tfidf.transform([text]).toarray()\n\n    cos=1-pairwise_distances(df_tfidf,Question_tfidf,metric='cosine')#Find cosine similarity\n\n    index_value = cos.argmax()\n    \n    #Resets the index of data_clean as otherwise it starts at like 6000 which means it won't work with index-value\n    data_clean.reset_index(inplace = True, drop = True)\n\n    return data_clean[\"Answer\"].loc[index_value]","c2652fbb":"UserQuestion = input(\"Enter your Question: \")","099ed160":"UserQuestion = cleanText(UserQuestion)\nUserAnswer = userAsk(UserQuestion)\nprint(UserAnswer)","17f07dbb":"gets the new question and runs it through 'cleanText' & 'userAsk'","777521f1":"Created a new dataset called data_clean that consists of 'Questions' and 'Answers'","45b7bdad":"We will get the customer tweets by using in_response_to_tweet to find what tweet the customer service of sainsburys was answering","8b1e21ce":"'cleanText' function created.\n* Removes all @'s and hashtags\n* Runs a spellchecker\n* Removes stop words\n* Removes puncutation\n* Lemmatizes words","989e2b4e":"This dataset will be very large, so I will be using a single companies customer service data.","1a1e8ace":"applies 'cleanText' to 'Question'","e96e6a1b":"function 'userAsk' created\n* uses tfidf to get coseine similarity to all values in 'question'","7de851e8":"We reduced the dataset from 2811774 entries to 19466 entries, but we still need to include the sutomers tweets","14cff81e":"This gets all the tweets from sainsburys and what they were responding to","5af0f568":"Cleaning\/pre-proccessing of the data\n\n'Answer' has all @'s and Hashtags removed"}}