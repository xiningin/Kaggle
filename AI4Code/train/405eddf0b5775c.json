{"cell_type":{"b1f81dad":"code","aae27e83":"code","30eb2c9c":"code","a8de299e":"code","e7486695":"code","84f88a2d":"code","1d2f66d7":"code","1b847dec":"code","7d89580b":"code","54f3d534":"code","68ae75d1":"code","27fd1513":"code","8a2666c3":"code","495ee19b":"code","57b9ff6d":"code","990e272a":"code","5e4ae7ef":"code","25dc24ca":"code","94e9d126":"code","349e4b09":"code","da42f1a2":"code","ae39beee":"code","50a0f296":"code","a35e3132":"code","84d0e332":"code","667bf113":"code","1a7e2e76":"code","34e397e3":"code","4274019a":"code","3ac4e07d":"code","3a304b4a":"code","bb301286":"code","4bb4ea0c":"code","4bd1c796":"code","225b54a4":"code","3cbacf8f":"code","5ded2345":"code","356093e3":"code","cd9dace2":"code","620c3c78":"code","35166e1d":"code","9233ca1c":"code","09d4421f":"code","bfa868a8":"code","9c3e7c58":"code","1d14ee7f":"code","8e504034":"code","8c1c7d5e":"code","1db25076":"code","748cc884":"markdown","7c615a9f":"markdown","609f02d8":"markdown","ba2c788c":"markdown","e3097a2a":"markdown","61e10520":"markdown","7c4a9dff":"markdown","8984ee6a":"markdown","dd73de6f":"markdown","bcd404dc":"markdown","b80c505a":"markdown","6358c5dc":"markdown","fb9729bf":"markdown","9a59dfa5":"markdown"},"source":{"b1f81dad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","aae27e83":"import pandas as pd,numpy as np\nimport matplotlib.pyplot as plt    # For plotting\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split","30eb2c9c":"df = pd.read_excel('..\/input\/Data_Train.xlsx')\ndf = df.drop(['Arrival_Time','Additional_Info'],axis = 1)\n\ntf = pd.read_excel('..\/input\/Test_set.xlsx')\ntf = tf.drop(['Arrival_Time','Additional_Info'],axis = 1)\ndf.head()\n#tf.head()","a8de299e":"df = df[df['Price']<= 20000]\n#tf = tf[tf['Price']<= 20000]\ndf.head()\ndf.shape","e7486695":"\nClass = {'IndiGo': 'Economy',\n         'GoAir': 'Economy',\n         'Vistara': 'Economy',\n         'Vistara Premium economy': 'Premium Economy',\n         'Air Asia': 'Economy',\n         'Trujet': 'Economy',\n         'Jet Airways': 'Economy',\n         'SpiceJet': 'Economy',\n         'Jet Airways Business': 'Business',\n         'Air India': 'Economy',\n         'Multiple carriers': 'Economy',\n         'Multiple carriers Premium economy': 'Premium Economy'}\ndf['Booking_Class'] = df['Airline'].map(Class)\ntf['Booking_Class'] = tf['Airline'].map(Class)","84f88a2d":"market = {'IndiGo': 41.3,\n         'GoAir': 8.4,\n         'Vistara': 3.3,\n         'Vistara Premium economy': 3.3,\n         'Air Asia': 3.3,\n         'Trujet': 0.1,\n         'Jet Airways': 17.8,\n         'SpiceJet': 13.3,\n         'Jet Airways Business': 17.8,\n         'Air India': 13.5,\n         'Multiple carriers': 1,\n         'Multiple carriers Premium economy': 1}\ndf['Market_Share'] = df['Airline'].map(market)\ntf['Market_Share'] = tf['Airline'].map(market)","1d2f66d7":"\ndf1 = df.copy() \ndf1['Day_of_Booking'] = '1\/3\/2019'\ndf1['Day_of_Booking'] = pd.to_datetime(df1['Day_of_Booking'],format='%d\/%m\/%Y')\ndf1['Date_of_Journey'] = pd.to_datetime(df1['Date_of_Journey'],format='%d\/%m\/%Y')\ndf1['Days_to_Departure'] = (df1['Date_of_Journey'] - df1['Day_of_Booking']).dt.days\ndf['Days_to_Departure'] = df1['Days_to_Departure']\n\ndf2 = tf.copy() \ndf2['Day_of_Booking'] = '1\/3\/2019'\ndf2['Day_of_Booking'] = pd.to_datetime(df2['Day_of_Booking'],format='%d\/%m\/%Y')\ndf2['Date_of_Journey'] = pd.to_datetime(df2['Date_of_Journey'],format='%d\/%m\/%Y')\ndf2['Days_to_Departure'] = (df2['Date_of_Journey'] - df2['Day_of_Booking']).dt.days\ntf['Days_to_Departure'] = df2['Days_to_Departure']\n\ndel df1, df2","1b847dec":"df.head()","7d89580b":"import pandas_profiling as pp\npp.ProfileReport(df)","54f3d534":"df.iloc[:,1] = pd.to_datetime(df.iloc[:,1])\ndf.iloc[:,6] = df.iloc[:,6].replace(\"h\",'', regex=True)\ndf.iloc[:,6] = df.iloc[:,6].replace(\"m\",'', regex=True)\n\ntf.iloc[:,1] = pd.to_datetime(tf.iloc[:,1])\ntf.iloc[:,6] = tf.iloc[:,6].replace(\"h\",'', regex=True)\ntf.iloc[:,6] = tf.iloc[:,6].replace(\"m\",'', regex=True)\n# df.iloc[:,6] = pd.to_datetime(df.iloc[:,6],format='%H:%M')\ndf.dtypes","68ae75d1":"new = df['Duration'].str.split(\" \", n = 2, expand = True) \ndf1 = df.copy()\ndf1[['Duration_hour','Duration_min']] = new\ndf1[['Duration_hour','Duration_min']] = df1[['Duration_hour','Duration_min']].fillna(0)\ndf1 = df1.drop(['Duration'],axis = 1)\ndf1['Dep_Time']= df1['Dep_Time'].str.split(\":\", n = 1, expand = True)[0]\ndf1.head()\n\nnewt = tf['Duration'].str.split(\" \", n = 2, expand = True) \ntf1 = tf.copy()\ntf1[['Duration_hour','Duration_min']] = newt\ntf1[['Duration_hour','Duration_min']] = tf1[['Duration_hour','Duration_min']].fillna(0)\ntf1 = tf1.drop(['Duration'],axis = 1)\ntf1['Dep_Time']= tf1['Dep_Time'].str.split(\":\", n = 1, expand = True)[0]\ntf1.head()","27fd1513":"df1['DOJ_Day'] = pd.to_datetime(df1.iloc[:,1]).dt.day\ndf1['DOJ_Month'] = pd.to_datetime(df1.iloc[:,1]).dt.month\ndf1 = df1.drop(['Date_of_Journey'],axis = 1)\ndf1.head()\n\ntf1['DOJ_Day'] = pd.to_datetime(tf1.iloc[:,1]).dt.day\ntf1['DOJ_Month'] = pd.to_datetime(tf1.iloc[:,1]).dt.month\ntf1 = tf1.drop(['Date_of_Journey'],axis = 1)\ntf1.head()","8a2666c3":"df1.iloc[:,[8,9,10,11]] = df1.iloc[:,[10,8,9,11]].astype('int')\ndf1.iloc[:,[13,11]] = df1.iloc[:,[13,11]].astype('object')\ndf1.dtypes\n\ntf1.iloc[:,[8,9,7,10]] = tf1.iloc[:,[8,9,7,10]].astype('int')\ntf1.iloc[:,[12,11]] = tf1.iloc[:,[12,11]].astype('object')\ntf1.dtypes","495ee19b":"df1 = pd.get_dummies(df1)\ndf1.head()\n\ntf1 = pd.get_dummies(tf1)\ntf1.head()","57b9ff6d":"X = df1.copy().drop(\"Price\",axis=1) \ny = df[\"Price\"]\n\n## Split the data into trainx, testx, trainy, testy with test_size = 0.20 using sklearn\ntrainx, testx, trainy, testy = train_test_split(X, y, test_size=0.20)\n\n## Print the shape of X_train, X_test, y_train, y_test\nprint(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","990e272a":"testx.head()","5e4ae7ef":"from sklearn.preprocessing import StandardScaler\n\n## Scale the numeric attributes\nscaler = StandardScaler()\nscaler.fit(trainx.iloc[:,:4])\n\ntrainx.iloc[:,:4] = scaler.transform(trainx.iloc[:,:4])\ntestx.iloc[:,:4] = scaler.transform(testx.iloc[:,:4])\ntf1.iloc[:,:4] = scaler.transform(tf1.iloc[:,:4])\ntrainx.head()","25dc24ca":"from sklearn.linear_model import LinearRegression\nlin_model = LinearRegression()\n\nlin_model.fit(trainx,trainy)","94e9d126":"lin_train_pred = lin_model.predict(trainx)\nlin_test_pred = lin_model.predict(testx)","349e4b09":"from sklearn.metrics import mean_absolute_error,mean_squared_error\nimport numpy as np\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\nprint(mean_absolute_error(lin_train_pred,trainy))\nprint(mean_absolute_percentage_error(lin_train_pred,trainy))\nprint(mean_absolute_error(lin_test_pred,testy))\nprint(mean_absolute_percentage_error(lin_test_pred,testy))","da42f1a2":"from sklearn.linear_model import Ridge\nridge_model = Ridge(alpha = 0.9)\nridge_model.fit(trainx,trainy)","ae39beee":"ridge_train_pred = ridge_model.predict(trainx)\nprint(mean_absolute_percentage_error(ridge_train_pred,trainy))\n\nridge_test_pred = ridge_model.predict(testx)\nprint(mean_absolute_percentage_error(ridge_test_pred,testy))","50a0f296":"from yellowbrick.regressor import ResidualsPlot\n\n# Instantiate the linear model and visualizer\nridge = Ridge()\nvisualizer = ResidualsPlot(ridge)\n\nvisualizer.fit(trainx, trainy)  # Fit the training data to the model\nvisualizer.score(testx , testy)  # Evaluate the model on the test data\nvisualizer.poof()  # Draw\/show\/poof the data","a35e3132":"visualizer.poof()","84d0e332":"from yellowbrick.model_selection import LearningCurve\n\nfrom sklearn.linear_model import RidgeCV\nsizes = np.linspace(0.3, 1.0, 10)\n\n# Create the learning curve visualizer, fit and poof\nviz = LearningCurve(RidgeCV(cv = 3), train_sizes=sizes, scoring='r2')\nviz.fit(trainx,trainy)\nviz.poof()","667bf113":"from sklearn.linear_model import Lasso\nLasso_model = Lasso(alpha = 0.5,max_iter=5000)\n%time Lasso_model.fit(trainx,trainy)","1a7e2e76":"Lasso_train_pred = Lasso_model.predict(trainx)\nprint(mean_absolute_percentage_error(Lasso_train_pred,trainy))\n\nLasso_test_pred = Lasso_model.predict(testx)\nprint(mean_absolute_percentage_error(Lasso_test_pred,testy))","34e397e3":"from sklearn.neighbors import KNeighborsRegressor\nKNN = KNeighborsRegressor(n_neighbors=6,metric='euclidean')\n%time KNN.fit(trainx,trainy) ","4274019a":"import math\n%time KNN_train_pred = KNN.predict(trainx)\nprint(mean_absolute_percentage_error(KNN_train_pred,trainy))\n\n%time KNN_test_pred = KNN.predict(testx)\nprint(mean_absolute_percentage_error(KNN_test_pred,testy))\n\nprint(math.sqrt(mean_squared_error(KNN_test_pred,testy)))","3ac4e07d":"from sklearn.svm import SVR\nSVR = SVR(gamma='scale', C=2.0, epsilon=0.1,kernel='poly')\n%time SVR.fit(trainx,trainy)","3a304b4a":"SVR_train_pred = SVR.predict(trainx)\nprint(mean_absolute_percentage_error(SVR_train_pred,trainy))\n\nSVR_test_pred = SVR.predict(testx)\nprint(mean_absolute_percentage_error(SVR_test_pred,testy))\n\nprint(math.sqrt(mean_squared_error(SVR_test_pred,testy)))","bb301286":"from sklearn.ensemble import RandomForestRegressor\n\nregr = RandomForestRegressor(n_jobs=-1,min_samples_leaf=3, min_samples_split=2,max_depth=51,n_estimators=400)\n%time regr.fit(trainx, trainy)  ","4bb4ea0c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nrfr_train_pred = regr.predict(trainx)\nprint(mean_squared_error(trainy, rfr_train_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(trainy, rfr_train_pred)))\nprint(mean_absolute_error(trainy, rfr_train_pred))\n\nrfr_test_pred = regr.predict(testx)\nprint(mean_squared_error(testy, rfr_test_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(testy, rfr_test_pred)))\nprint(mean_absolute_error(testy, rfr_test_pred))\n\nimport numpy as np\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\nprint(mean_absolute_percentage_error(trainy,rfr_train_pred))\nprint(mean_absolute_percentage_error(testy,rfr_test_pred))\n\ndef rmsle(y_pred, y_test) : \n    assert len(y_test) == len(y_pred)\n    return np.sqrt(np.mean((np.log(1+y_pred) - np.log(1+y_test))**2))\n\nprint(rmsle(rfr_train_pred,trainy))\nprint(rmsle(rfr_test_pred,testy))","4bd1c796":"from yellowbrick.model_selection import ValidationCurve\n\nRFviz = ValidationCurve(\n    RandomForestRegressor(), param_name=\"max_depth\",\n    param_range=np.arange(2, 20), cv=3, scoring='neg_mean_absolute_error'\n)\n\n# Fit and poof the visualizer\nRFviz.fit(trainx, trainy)\nRFviz.poof()","225b54a4":"from sklearn.model_selection import RandomizedSearchCV\n\n\nrfr_grid2 = RandomForestRegressor(n_jobs=-1, max_features='auto',oob_score=True)\n \n# Set parameters for the grid search\n\n \nparam_grid2 = {\"n_estimators\" : [250,500],\n           \"max_depth\" : [12,15],\n           \"min_samples_leaf\" : [1,3],\n           'min_samples_split' : [2,3],\n           'max_features' : ['auto','sqrt']}\n \nrfr_cv_grid2 = RandomizedSearchCV(estimator = rfr_grid2, param_distributions = param_grid2, cv = 5, n_iter=10)\n%time rfr_cv_grid2.fit(trainx, trainy)\nrfr_cv_grid2.best_estimator_","3cbacf8f":"rfr_cv_grid2.best_score_","5ded2345":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nrfr2_train_pred = rfr_cv_grid2.predict(trainx)\nprint(mean_squared_error(trainy, rfr2_train_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(trainy, rfr2_train_pred)))\nprint(mean_absolute_error(trainy, rfr2_train_pred))\n\nrfr2_test_pred = rfr_cv_grid2.predict(testx)\nprint(mean_squared_error(testy, rfr2_test_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(testy, rfr2_test_pred)))\nprint(mean_absolute_error(testy, rfr2_test_pred))\n\nprint(rmsle(rfr2_train_pred,trainy))\nprint(rmsle(rfr2_test_pred,testy))\n\nprint(mean_absolute_percentage_error(trainy,rfr2_train_pred))\nprint(mean_absolute_percentage_error(testy,rfr2_test_pred))","356093e3":"from sklearn.ensemble import AdaBoostRegressor\nABR = AdaBoostRegressor(base_estimator=None, learning_rate=0.1, loss='exponential',\n        n_estimators= 100, random_state=1)\n%time ABR.fit(trainx,trainy)","cd9dace2":"ABR_train_pred = ABR.predict(trainx)\nprint(mean_absolute_percentage_error(ABR_train_pred,trainy))\n\nABR_test_pred = ABR.predict(testx)\nprint(mean_absolute_percentage_error(ABR_test_pred,testy))\n\nprint(math.sqrt(mean_squared_error(ABR_test_pred,testy)))","620c3c78":"from sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(max_depth=6, n_estimators=2000,learning_rate=0.05)\n# gbrt.fit(trainx, trainy)\n# errors = [mean_squared_error(testy, y_pred)\n#  for y_pred in gbrt.staged_predict(testx)]\n# bst_n_estimators = np.argmin(errors)\n# gbrt_best = GradientBoostingRegressor(max_depth=12,n_estimators=bst_n_estimators)\n%time gbrt.fit(trainx, trainy)","35166e1d":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\ngbrt_train_pred = gbrt.predict(trainx)\nprint(mean_squared_error(trainy, gbrt_train_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(trainy, gbrt_train_pred)))\nprint(mean_absolute_error(trainy, gbrt_train_pred))\n\ngbrt_test_pred = gbrt.predict(testx)\nprint(mean_squared_error(testy, gbrt_test_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(testy, gbrt_test_pred)))\nprint(mean_absolute_error(testy, gbrt_test_pred))\n\nprint(rmsle(gbrt_train_pred,trainy))\nprint(rmsle(gbrt_test_pred,testy))\n\nprint(mean_absolute_percentage_error(trainy,gbrt_train_pred))\nprint(mean_absolute_percentage_error(testy,gbrt_test_pred))","9233ca1c":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\n\nxg_reg = xgb.XGBRegressor(max_depth=8, \n                   learning_rate=0.15, \n                   n_estimators=210, \n                   silent=False, \n                   objective='reg:linear', \n                   booster='gbtree', \n                   n_jobs=1, \n                   nthread=None, \n                   gamma=0, \n                   min_child_weight=0, \n                   max_delta_step=0, \n                   subsample=1, \n                   colsample_bytree=1, \n                   colsample_bylevel=1, \n                   reg_alpha=0.1, \n                   reg_lambda=0.1, \n                   scale_pos_weight=1, \n                   base_score=0.5, \n                   random_state=0, )\n%time xg_reg.fit(trainx,trainy)","09d4421f":"xg_reg_train_pred = xg_reg.predict(trainx)\nprint(mean_squared_error(trainy, xg_reg_train_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(trainy, xg_reg_train_pred)))\n#print(mean_absolute_error(trainy, xg_reg_train_pred))\n\nxg_reg_test_pred = xg_reg.predict(testx)\nprint(mean_squared_error(testy, xg_reg_test_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(testy, xg_reg_test_pred)))\n#print(mean_absolute_error(testy, xg_reg_test_pred))\n\nprint(rmsle(xg_reg_train_pred,trainy))\nprint(rmsle(xg_reg_test_pred,testy))\n\nprint(mean_absolute_percentage_error(trainy,xg_reg_train_pred))\nprint(mean_absolute_percentage_error(testy,xg_reg_test_pred))","bfa868a8":"import xgboost as xgb\nfrom sklearn.model_selection import validation_curve\ndefault_params = {\n    'objective': 'reg:linear',\n    'max_depth': 8,\n    'learning_rate': 0.1\n    }\n\nn_estimators_range = np.linspace(1, 240, 30).astype('int')\n\ntrain_scores, test_scores = validation_curve(\n    xgb.XGBRegressor(**default_params),\n    trainx, trainy,\n    param_name = 'n_estimators',\n    param_range = n_estimators_range,\n    cv=2,\n    scoring='neg_mean_squared_log_error'\n)","9c3e7c58":"\n%matplotlib inline\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nfig = plt.figure(figsize=(10, 6), dpi=100)\n\nplt.title(\"Validation Curve with XGBoost (eta = 0.3)\")\nplt.xlabel(\"number of trees\")\nplt.ylabel(\"neg_mean_absolute_error\")\nplt.ylim(-0.4,0.25)\n\nplt.plot(n_estimators_range,\n             train_scores_mean,\n             label=\"Training score\",\n             color=\"r\")\n\nplt.plot(n_estimators_range,\n             test_scores_mean, \n             label=\"Cross-validation score\",\n             color=\"g\")\n\nplt.fill_between(n_estimators_range, \n                 train_scores_mean - train_scores_std,\n                 train_scores_mean + train_scores_std, \n                 alpha=0.2, color=\"r\")\n\nplt.fill_between(n_estimators_range,\n                 test_scores_mean - test_scores_std,\n                 test_scores_mean + test_scores_std,\n                 alpha=0.2, color=\"g\")\n\nplt.axhline(y=1, color='k', ls='dashed')\n\nplt.legend(loc=\"best\")\nplt.show()\n\ni = np.argmax(test_scores_mean)\nprint(\"Best cross-validation result ({0:.2f}) obtained for {1} trees\".format(test_scores_mean[i], n_estimators_range[i]))","1d14ee7f":"import lightgbm as lgb\ntrain_data=lgb.Dataset(trainx,label=trainy)\nparams = {'objective': 'regression',\n         'boosting': 'gbdt',\n         'num_iterations': 8000,   \n         'learning_rate': 0.01,  \n         'num_leaves': 40,  \n         'max_depth': 24,   \n         'min_data_in_leaf':6,  \n         'max_bin': 4, \n         'metric': 'mape'\n         }\n%time lgbmodel= lgb.train(params, train_data)","8e504034":"lgb_reg_train_pred = lgbmodel.predict(trainx)\nprint(mean_squared_error(trainy, lgb_reg_train_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(trainy, lgb_reg_train_pred)))\n#print(mean_absolute_error(trainy, lgb_reg_train_pred))\n\nlgb_reg_test_pred = lgbmodel.predict(testx)\nprint(mean_squared_error(testy, lgb_reg_test_pred))\nfrom math import sqrt\nprint(sqrt(mean_squared_error(testy, lgb_reg_test_pred)))\n#print(mean_absolute_error(testy, lgb_reg_test_pred))\n\nprint(rmsle(lgb_reg_train_pred,trainy))\nprint(rmsle(lgb_reg_test_pred,testy))\n\nprint(mean_absolute_percentage_error(trainy,lgb_reg_train_pred))\nprint(mean_absolute_percentage_error(testy,lgb_reg_test_pred))\n\n#print('RMSLE:', sqrt(mean_squared_log_error(np.exp(testy), np.exp(lgb_reg_test_pred))))","8c1c7d5e":"sub_pred = 0.4*(xg_reg_test_pred)+0.6*(rfr_test_pred)","1db25076":"print(mean_absolute_percentage_error(testy,sub_pred))","748cc884":"# EDA","7c615a9f":"## Convert Data Types","609f02d8":"# Pre Processing","ba2c788c":"## Random Forest","e3097a2a":"## Gradient Boost","61e10520":"#### Grid Search","7c4a9dff":"## Light GBM","8984ee6a":"# Content\n#### [Import Libraries](#Import Libraries)\n#### [Read Files](#Read Files)\n### [Exploatory Data Analysis](#EDA)\n### [Pre Processing](#Pre Processing)\n#### [Convert Data Types](#Convert Data Types)\n#### [Encoding Categorical Features](#Encoding Categorical Features)\n### [Train Test Split](#Train Test Split)\n### [Standardizing Numeric features](#Standardizing Numeric features)\n# [Model Buildling](#Model Building)\n#### [Random Forest](#Random Forest)\n#### [Gradient Boost](#Gradient Boost)\n#### [XGBoost](#XGBoost)\n#### [Light GBM](#Ligh GBM)","dd73de6f":"## Read Files","bcd404dc":"## Standardizing Numeric features","b80c505a":"## Encoding Categorical features","6358c5dc":"## Import Libraries","fb9729bf":"## XGBoost","9a59dfa5":"## Train Test Split"}}