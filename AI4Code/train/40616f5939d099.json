{"cell_type":{"87dbe83b":"code","30cac7b7":"code","e1ffde34":"code","1f50fd08":"code","1b2f12b0":"code","a42f320d":"code","70ad5d41":"code","ca54d58e":"code","94dc2753":"code","cef152ed":"code","8180a8f5":"code","909bebbd":"code","92482d6f":"code","191bf1d9":"code","6ebf0888":"code","4b5f8fe5":"code","8ded904d":"code","57fa4680":"code","fc59bb2d":"code","df023b3b":"code","bcdecae5":"markdown","ac167087":"markdown","35bf24a5":"markdown","e7abd7de":"markdown","7ea889a9":"markdown","f1ef9da3":"markdown","c3ee3405":"markdown","3cd57470":"markdown","7686732b":"markdown"},"source":{"87dbe83b":"# Import Time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom tqdm import tqdm_notebook as tqdm\n\n%matplotlib inline\n\nBATCH_SIZE = 256\n\nprint(os.listdir('..\/input\/Kannada-MNIST\/'))\n\n# Gets the GPU if there is one, otherwise the cpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","30cac7b7":"df_train = pd.read_csv('..\/input\/Kannada-MNIST\/train.csv')\ntarget = df_train['label']\ndf_train.drop('label', axis=1, inplace=True)\n\nX_test = pd.read_csv('..\/input\/Kannada-MNIST\/test.csv')\nX_test.drop('id', axis=1, inplace=True)\n\nX_test.head()","e1ffde34":"X_train, X_dev, y_train, y_dev = train_test_split(df_train, target, stratify=target, random_state=42, test_size=0.01)\nprint('X_train', len(X_train))\nprint('X_dev', len(X_dev))\nprint('X_test', len(X_test))","1f50fd08":"# Visualization Reference Kernel https:\/\/www.kaggle.com\/josephvm\/kannada-with-pytorch\n# Some quick data visualization \n# First 10 images of each class in the training set\n\nfig, ax = plt.subplots(nrows=10, ncols=10, figsize=(10,10))\n\n# I know these for loops look weird, but this way num_i is only computed once for each class\nfor i in range(10): # Column by column\n    num_i = X_train[y_train == i]\n    ax[0][i].set_title(i)\n    for j in range(10): # Row by row\n        ax[j][i].axis('off')\n        ax[j][i].imshow(num_i.iloc[j, :].to_numpy().astype(np.uint8).reshape(28, 28), cmap='gray')","1b2f12b0":"class CharData(Dataset):\n    \"\"\"\n    Infer from standard PyTorch Dataset class\n    Such datasets are often very useful\n    \"\"\"\n    \n    def __init__(self,\n                 images,\n                 labels=None,\n                 transform=None,\n                ):\n        self.X = images\n        self.y = labels\n        \n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        img = np.array(self.X.iloc[idx, :], dtype='uint8').reshape([28, 28, 1])\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        if self.y is not None:\n            y = np.zeros(10, dtype='float32')\n            y[self.y.iloc[idx]] = 1\n            return img, y\n        else:\n            return img","a42f320d":"# Put some augmentation on training data\ntrain_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomAffine(degrees=5, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),\n    transforms.ToTensor()\n])\n\n# Test data without augmentation\ntest_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor()\n])","70ad5d41":"# Create dataset objects\ntrain_dataset = CharData(X_train, y_train, train_transform)\ndev_dataset = CharData(X_dev, y_dev, test_transform)\ntest_dataset = CharData(X_test, transform=test_transform)","ca54d58e":"# Create data generators - they will produce batches\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\ndev_loader = DataLoader(dataset=dev_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","94dc2753":"fig, ax = plt.subplots(nrows=1, ncols=16, figsize=(30,4))\n\nfor batch in train_loader:\n    for i in range(16):\n        ax[i].set_title(batch[1][i].data.numpy().argmax())\n        ax[i].imshow(batch[0][i, 0], cmap='gray')\n    break","cef152ed":"DEPTH_MULT = 2\n\nclass ConvLayer(nn.Module):\n    def __init__(self, input_size, output_size, kernel_size=3):\n        super(ConvLayer, self).__init__()\n        self.ops = nn.Sequential(\n            nn.Conv2d(input_size, output_size, kernel_size=kernel_size, stride=1, padding=kernel_size\/\/2),\n            nn.BatchNorm2d(output_size),\n            nn.ReLU(inplace=True)\n        )\n    \n    def forward(self, x):\n        return self.ops(x)\n\n\nclass FCLayer(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(FCLayer, self).__init__()\n        self.ops = nn.Sequential(\n            nn.Linear(input_size, output_size),\n            nn.BatchNorm1d(output_size),\n            nn.ReLU(inplace=True)\n        )\n        self.residual = input_size == output_size\n    \n    def forward(self, x):\n        if self.residual:\n            return (self.ops(x) + x) \/ np.sqrt(2)\n        return self.ops(x)\n\n\ndef mixup(x, shuffle, lam, i, j):\n    if shuffle is not None and lam is not None and i == j:\n        x = lam * x + (1 - lam) * x[shuffle]\n    return x\n\n\nclass Net(nn.Module):\n    def __init__(self, num_classes):\n        super(Net, self).__init__()\n        \n        self.conv1 = ConvLayer(1, DEPTH_MULT * 32)\n        self.conv2 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 32)\n        self.conv3 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 32)\n        self.conv4 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 32)\n        \n        self.conv5 = ConvLayer(DEPTH_MULT * 32, DEPTH_MULT * 64)\n        self.conv6 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv7 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv8 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv9 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        self.conv10 = ConvLayer(DEPTH_MULT * 64, DEPTH_MULT * 64)\n        \n        self.mp = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.fc1 = FCLayer(DEPTH_MULT * 64 * 7 * 7, DEPTH_MULT * 512)\n        self.fc2 = FCLayer(DEPTH_MULT * 512, DEPTH_MULT * 512)\n        self.fc3 = FCLayer(DEPTH_MULT * 512, DEPTH_MULT * 512)\n        self.fc4 = FCLayer(DEPTH_MULT * 512, DEPTH_MULT * 512)\n        self.projection = nn.Linear(DEPTH_MULT * 512, 10)\n    \n    def forward(self, x):\n        if isinstance(x, list):\n            x, shuffle, lam = x\n        else:\n            shuffle = None\n            lam = None\n        \n        # Decide which layer to mixup\n        j = np.random.randint(15)\n        \n        x = mixup(x, shuffle, lam, 0, j)\n        x = self.conv1(x)\n        x = mixup(x, shuffle, lam, 1, j)\n        x = self.conv2(x)\n        x = mixup(x, shuffle, lam, 2, j)\n        x = self.conv3(x)\n        x = mixup(x, shuffle, lam, 3, j)\n        x = self.conv4(x)\n        x = self.mp(x)\n        \n        x = mixup(x, shuffle, lam, 4, j)\n        x = self.conv5(x)\n        x = mixup(x, shuffle, lam, 5, j)\n        x = self.conv6(x)\n        x = mixup(x, shuffle, lam, 6, j)\n        x = self.conv7(x)\n        x = mixup(x, shuffle, lam, 7, j)\n        x = self.conv8(x)\n        x = mixup(x, shuffle, lam, 8, j)\n        x = self.conv9(x)\n        x = mixup(x, shuffle, lam, 9, j)\n        x = self.conv10(x)\n        x = self.mp(x)\n        \n        x = x.view(x.size(0), -1)\n        x = mixup(x, shuffle, lam, 10, j)\n        x = self.fc1(x)\n        x = mixup(x, shuffle, lam, 11, j)\n        x = self.fc2(x)\n        x = mixup(x, shuffle, lam, 12, j)\n        x = self.fc3(x)\n        x = mixup(x, shuffle, lam, 13, j)\n        x = self.fc4(x)\n        x = mixup(x, shuffle, lam, 14, j)\n        x = self.projection(x)\n        \n        return x","8180a8f5":"def criterion(input, target, size_average=True):\n    \"\"\"Categorical cross-entropy with logits input and one-hot target\"\"\"\n    l = -(target * torch.log(F.softmax(input, dim=1) + 1e-10)).sum(1)\n    if size_average:\n        l = l.mean()\n    else:\n        l = l.sum()\n    return l","909bebbd":"model = Net(10)\nmodel = model.to(device)\n\nn_epochs = 160\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=n_epochs \/\/ 4, gamma=0.1)","92482d6f":"def train(epoch, history=None):\n    model.train()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data = data.to(device)\n        target = target.to(device)\n        \n        # mixup\n        alpha = 2\n        lam = np.random.beta(alpha, alpha)\n        shuffle = torch.randperm(data.shape[0])\n        target = lam * target + (1 - lam) * target[shuffle]\n        \n        optimizer.zero_grad()\n        output = model([data, shuffle, lam])\n        loss = criterion(output, target)\n        if history is not None:\n            history.loc[epoch + batch_idx \/ len(train_loader), 'train_loss'] = loss.data.cpu().numpy()\n        \n        loss.backward()\n        optimizer.step()\n        \n        if (batch_idx + 1) % 100 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLR: {:.6f}\\tLoss: {:.6f}'.format(\n                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n                100. * (batch_idx + 1) \/ len(train_loader),\n                optimizer.state_dict()['param_groups'][0]['lr'],\n                loss.data))\n    exp_lr_scheduler.step()\n\ndef evaluate(epoch, history=None):\n    model.eval()\n    loss = 0\n    correct = 0\n    \n    with torch.no_grad():\n        for data, target in dev_loader:\n            data = data.to(device)\n            target = target.to(device)\n\n            output = model(data)\n\n            loss += criterion(output, target, size_average=False).data\n\n            pred = output.data.max(1, keepdim=True)[1]\n            correct += pred.eq(target.max(1, keepdim=True)[1].data.view_as(pred)).cpu().sum().numpy()\n    \n    loss \/= len(dev_loader.dataset)\n    accuracy = correct \/ len(dev_loader.dataset)\n    \n    if history is not None:\n        history.loc[epoch, 'dev_loss'] = loss.cpu().numpy()\n        history.loc[epoch, 'dev_accuracy'] = accuracy\n    \n    print('Dev loss: {:.4f}, Dev accuracy: {}\/{} ({:.3f}%)\\n'.format(\n        loss, correct, len(dev_loader.dataset),\n        100. * accuracy))","191bf1d9":"%%time\nimport gc\n\nhistory = pd.DataFrame()\n\nfor epoch in range(n_epochs):\n    torch.cuda.empty_cache()\n    gc.collect()\n    train(epoch, history)\n    evaluate(epoch, history)","6ebf0888":"history['train_loss'].plot();","4b5f8fe5":"history.dropna()['dev_loss'].plot();","8ded904d":"history.dropna()['dev_accuracy'].plot();","57fa4680":"print('max', history.dropna()['dev_accuracy'].max())\nprint('max in last 5', history.dropna()['dev_accuracy'].iloc[-5:].max())\nprint('avg in last 5', history.dropna()['dev_accuracy'].iloc[-5:].mean())","fc59bb2d":"model.eval()\npredictions = []\n\nfor data in tqdm(test_loader):\n    data = data.to(device)\n    output = model(data).max(dim=1)[1] # argmax\n    predictions += list(output.data.cpu().numpy())","df023b3b":"submission = pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')\nsubmission['label'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","bcdecae5":"# Load data","ac167087":"# Train model","35bf24a5":"# Define model","e7abd7de":"# Manifold mixup\n\nHere I implement the idea from the paper [Manifold Mixup: Better Representations by Interpolating Hidden States](https:\/\/arxiv.org\/pdf\/1806.05236.pdf).  \nIt is a new regularization method which allows for training very deep and wide neural networks with much less overfitting.  \n\nThis notebook is a simple PyTorch implementation of CNN with manifold mixup.  \nThis is not precise implementation of the paper.  \n\n### Input mixup\nIt's easier to first understand what is input mixup: [mixup: BEYOND EMPIRICAL RISK MINIMIZATION](https:\/\/arxiv.org\/pdf\/1710.09412.pdf).  \nInput mixup is a regularization done during training procedure.  \nHere is the piece of PyTorch code from the paper:  \n```\n# y1, y2 should be one-hot vectors\nfor(x1, y1), (x2, y2)in zip(loader1, loader2):\n    lam = numpy.random.beta(alpha, alpha)\n    x = Variable(lam*x1 + (1. - lam)*x2)\n    y = Variable(lam*y1 + (1. - lam)*y2)\n    optimizer.zero_grad()\n    loss(net(x), y).backward()\n    optimizer.step()\n```\nIn short: model is given a *linear combination of inputs* and is asked to return *linear combination of outputs*.  \nIt forces the network to **interpolate between samples**.  \n\n### Manifold mixup\nManifold mixup is a similar idea, but the interpolation is done at a random layer inside neural network.  \nSometimes it is the 0'th layer, which means input mixup.  \nIt forces the network to **interpolate between hidden representations of samples**.  ","7ea889a9":"# Visualize","f1ef9da3":"# PyTorch data generator","c3ee3405":"# Predict","3cd57470":"# Plot losses","7686732b":"## Show some generated samples"}}