{"cell_type":{"95e44598":"code","d50fd71b":"code","58a77c28":"code","2b7fe00b":"code","1929a63f":"code","0df8d7f7":"code","d13e27ac":"code","b937a94c":"code","1d1814a9":"code","0bac0ca9":"code","cfad1120":"code","0d6cbb8a":"code","eb507f24":"code","9385c42c":"code","0b1e8f57":"code","1a51215d":"code","7834ed16":"code","7c6601ea":"code","1c8b3ea3":"code","a27063d2":"code","66b0c0fb":"code","0334da62":"code","0082ed64":"code","11ab1583":"code","3a889be2":"code","724332a7":"code","112afada":"code","c922067e":"code","14173765":"code","172e6d53":"code","331459aa":"code","d0eaa7b9":"code","56000266":"code","ccebd117":"code","fcb4d211":"code","c1d43797":"code","04beaca2":"code","07bb2184":"code","6f7ed5c8":"code","f50fc279":"code","d303ef14":"code","32bd965c":"code","f02c8bff":"code","951dafbb":"code","82dd332d":"code","dc2b2925":"code","0ef80fa4":"code","b03ebd2c":"code","26eb222f":"code","2349e23b":"code","57593997":"code","f7a9f8c0":"code","2e8c4abf":"code","16db8093":"code","ea6b7655":"code","98af2dec":"code","61242e30":"code","41fc2699":"code","49e49777":"code","7a7b9d48":"code","62a43aac":"code","c12de413":"code","2d800444":"code","2941f1d2":"code","41d7816d":"code","17b6de03":"code","65108ab1":"code","500a2de7":"code","58c2b6d2":"code","d491fded":"code","4b50a602":"code","79c2409f":"code","e4287a6e":"code","39b58d8c":"code","3cd343e7":"code","4054e21e":"code","949f59e7":"code","20939ddf":"code","096e5cb2":"code","b365777c":"code","2bfd702e":"code","4b5436f6":"code","54069a55":"code","ee3f3856":"code","822b4606":"code","9e7d723e":"code","91db2e7a":"code","ce057afa":"code","489f626c":"code","f41f2a9e":"code","48fcfb38":"code","03936483":"code","cc932419":"code","d63e0709":"code","a0f159d1":"code","557b27cc":"code","176a96c4":"code","ec6f8fce":"code","d7f5d8da":"code","323948d5":"code","06ffeb74":"code","4473948b":"code","d05411f5":"code","b5d37494":"code","c60aeacc":"code","23012221":"code","e6133191":"code","32dd18f0":"code","20e106db":"code","034854d4":"code","9de4a1c6":"code","860478e7":"code","12b68d49":"code","ee1543ca":"code","3c99867c":"code","9843a3ad":"code","ec4e4274":"code","478d270d":"code","763408ec":"code","d6ff12fc":"code","9ea40043":"code","542d23ff":"code","4c36d68d":"code","3a79fc63":"code","615400f4":"code","1fc312b7":"code","87710534":"code","2b959a5a":"markdown","9674c026":"markdown","75f164ab":"markdown","22eae539":"markdown","d185becd":"markdown","4f585626":"markdown","ada694ad":"markdown","9e872e06":"markdown","ba8d9686":"markdown","52d49d69":"markdown","895ec24c":"markdown","da371100":"markdown","32de5f57":"markdown","eeb457ca":"markdown","7ebbd545":"markdown","05912bf1":"markdown","fb993bd8":"markdown","29d28a59":"markdown","8494488e":"markdown","7e740534":"markdown","cbb436bb":"markdown","186406ff":"markdown","23003ab9":"markdown","7a8e70c5":"markdown","3daa92a6":"markdown","3b27026b":"markdown","fce20eba":"markdown","ed279f61":"markdown","e57baf99":"markdown","82896afe":"markdown","e4ce3680":"markdown","757c1d9a":"markdown","0e1ad7b5":"markdown","64118204":"markdown","50367d0d":"markdown","5be318fe":"markdown","0c1b3f30":"markdown","a7d446b1":"markdown","d069919d":"markdown","a894e56d":"markdown","ef3d682f":"markdown","ffa04062":"markdown","f6a6d7e1":"markdown","04e40111":"markdown","49830439":"markdown","3d2b4026":"markdown","32722411":"markdown","dd63c057":"markdown","5ed83bc0":"markdown","02f2686b":"markdown","997740e8":"markdown","3f8103c2":"markdown","fa6b8f65":"markdown","07728b14":"markdown","16ce0766":"markdown","7744892c":"markdown","42a2d72c":"markdown","d36e8d99":"markdown","d22ddd6b":"markdown","3e47a5b6":"markdown","b154bf2d":"markdown","df72fa89":"markdown","3ffccf45":"markdown","354cfdd8":"markdown"},"source":{"95e44598":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = \"retina\"\n# this sets the backend of matplotlib to the 'inline backend' \n#   that is graphs will be included in the notebool, next to code. \n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport cufflinks as cf\n\n#In order to display the plot inside the notebook,  initiate plotly\u2019s notebook\ninit_notebook_mode(connected=True)\ncf.go_offline()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d50fd71b":"#getting all the datasets into variables \nglobal_temp = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv')\nglobal_temp.head()\nprint(global_temp.dtypes)","58a77c28":"print(global_temp.dtypes)","2b7fe00b":"global_temp['dt'] = pd.to_datetime(global_temp['dt'])\nglobal_temp.info()","1929a63f":"\n#global_temp.set_index('dt', inplace=True)\n#tried to set the date as the index but error, SOLVE IT LATER!\n#SOLVED IT!!!","0df8d7f7":"global_temp.index\n","d13e27ac":"global_temp = pd.read_csv(\"\/kaggle\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv\")\nglobal_temp_by_country = pd.read_csv(\"\/kaggle\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv\")","b937a94c":"#lets now create new months and years cols\n\n\nglobal_temp[\"dt\"] = pd.to_datetime(global_temp[\"dt\"])\nglobal_temp[\"Month\"] = global_temp[\"dt\"].dt.month\nglobal_temp[\"Year\"] = global_temp[\"dt\"].dt.year\nglobal_temp = global_temp.drop(\"dt\", axis = 1)\nglobal_temp = global_temp[global_temp.Year >= 1900]","1d1814a9":"global_temp_by_country[\"dt\"] = pd.to_datetime(global_temp_by_country[\"dt\"])\nglobal_temp_by_country[\"Month\"] = global_temp_by_country[\"dt\"].dt.month\nglobal_temp_by_country[\"Year\"] = global_temp_by_country[\"dt\"].dt.year\nglobal_temp_by_country = global_temp_by_country.drop(\"dt\", axis = 1)\nglobal_temp_by_country = global_temp_by_country[global_temp_by_country.Year >= 1900]","0bac0ca9":"global_temp.isnull().sum().sort_values(ascending = False).head(3)\nglobal_temp_by_country.isnull().sum().sort_values(ascending = False).head(3)\nglobal_temp_by_country= global_temp_by_country.dropna()\nprint(global_temp_by_country.isnull().sum().sort_values(ascending = False).head(3))\nprint(global_temp_by_country.shape)","cfad1120":"global_temp.describe()","0d6cbb8a":"global_temp_by_state = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByState.csv')\nglobal_temp_by_state.head()\nglobal_temp_by_country = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv')\nglobal_temp_by_country.head()\nglobal_temp_by_city = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCity.csv')\nglobal_temp_by_city.head()\nglobal_temp_by_major_city = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByMajorCity.csv')\nglobal_temp_by_major_city.head()\n","eb507f24":"global_temp_by_state['dt'] = pd.to_datetime(global_temp_by_state['dt'])\nglobal_temp_by_state.info()\n#global_temp_by_country['dt'] = pd.to_datetime(global_temp_by_country['dt'])\n#global_temp_by_country.info()\nglobal_temp_by_city['dt'] = pd.to_datetime(global_temp_by_city['dt'])\nglobal_temp_by_city.info()\nglobal_temp_by_major_city['dt'] = pd.to_datetime(global_temp_by_major_city['dt'])\nglobal_temp_by_major_city.info()","9385c42c":"global_temp_by_state.set_index('dt', inplace=True)\nglobal_temp_by_country.set_index('dt', inplace=True)\nglobal_temp_by_city.set_index('dt', inplace=True)\n","0b1e8f57":"global_temp_by_major_city.set_index('dt', inplace=True)\nglobal_temp_by_major_city.index","1a51215d":"global_temp.shape,global_temp_by_state.shape, global_temp_by_country.shape, global_temp_by_city.shape, global_temp_by_major_city.shape","7834ed16":"global_temp.isna().sum()","7c6601ea":"india_temp = global_temp_by_country[global_temp_by_country['Country']=='India']\nindia_temp.head()\nindia_temp.reset_index()","1c8b3ea3":"temp = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv')\nnew_df = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv')\nnew_df['year'] = pd.to_datetime( new_df['dt']).dt.year # Converting date into year and making new column.\n\nby_new = new_df.groupby(['year'] )['LandAverageTemperature'].mean().reset_index()\nnew_pivot = by_new.pivot_table(values='LandAverageTemperature',index='year')\nnew_pivot.iplot(kind='scatter')\n\n#COMMENT IT","a27063d2":"india = temp[temp['Country']=='India']\nindia['year'] = pd.to_datetime(india['dt']).dt.year\n\nnew_india = india.groupby('year')['AverageTemperature'].mean().reset_index()\nnew_india.iplot(kind='scatter', x='year', y='AverageTemperature', title='Temperature trend in India',\n               xTitle='Year', yTitle='Temperature')","66b0c0fb":"#dont have to do it again, but if you are starting from here, uncomment and run it forom here.\ndf_state = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByState.csv')","0334da62":"state = df_state[df_state['Country']=='India']\nstate = state.groupby('State')['AverageTemperature'].mean().reset_index()\nstate.sort_values('AverageTemperature',inplace=True, )\nstate = state[:10]\nstate.iplot(kind='bar', x='State', y='AverageTemperature', title='Top 10 Coolest States',\n           xTitle='State', yTitle='Temperature', color='Red')","0082ed64":"state = df_state[df_state['Country']=='India']\nstate = state.groupby('State')['AverageTemperature'].mean().reset_index()\nstate.sort_values('AverageTemperature',inplace=True, ascending=False)\nstate = state[:10]\nstate.iplot(kind='bar', x='State', y='AverageTemperature', title='Top 10 Hotest States',\n           xTitle='State', yTitle='Temperature')","11ab1583":"#df_city = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByMajorCity.csv')\n#dont have to do it if u started from up top. else execute","3a889be2":"temp_df = df_city[df_city['Country']== 'India']\ntemp_df.head()","724332a7":"print(temp_df.City.unique())","112afada":"temp_df = df_city[df_city['City']== 'Madras']\ntemp_df['year'] = pd.to_datetime(temp_df['dt']).dt.year\n\nby_year = temp_df.groupby('year')['AverageTemperature'].mean().reset_index()\nby_year.iplot(kind='scatter', x='year', y='AverageTemperature', title='Temperature trend of Chennai City (formerly known as Madras)',\n             xTitle='Year', yTitle='Temperature', legend=True)","c922067e":"#global_temp = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv')\n#already done above, not necessary to load again, but load if starting from here. ","14173765":"# cleanning out all the unecessary fields and cols.\n#going global as in india we just have two seasons: summer and monsoon.\nglobal_temp = global_temp[['dt', 'LandAverageTemperature']]\n\nglobal_temp['dt'] = pd.to_datetime(global_temp['dt'])\nglobal_temp['year'] = global_temp['dt'].map(lambda x: x.year)\nglobal_temp['month'] = global_temp['dt'].map(lambda x: x.month)\n\n#getting the season month wise\ndef get_season(month):\n    if month >= 3 and month <= 5:\n        return 'spring'\n    elif month >= 6 and month <= 8:\n        return 'summer'\n    elif month >= 9 and month <= 11:\n        return 'autumn'\n    else:\n        return 'winter'\n    \nmin_year = global_temp['year'].min()\nmax_year = global_temp['year'].max()\nyears = range(min_year, max_year + 1)\n\nglobal_temp['season'] = global_temp['month'].apply(get_season)\n\nspring_temps = []\nsummer_temps = []\nautumn_temps = []\nwinter_temps = []\n\n# adding into new series on the basis of their season\nfor year in years:\n    curr_years_data = global_temp[global_temp['year'] == year]\n    spring_temps.append(curr_years_data[curr_years_data['season'] == 'spring']['LandAverageTemperature'].mean())\n    summer_temps.append(curr_years_data[curr_years_data['season'] == 'summer']['LandAverageTemperature'].mean())\n    autumn_temps.append(curr_years_data[curr_years_data['season'] == 'autumn']['LandAverageTemperature'].mean())\n    winter_temps.append(curr_years_data[curr_years_data['season'] == 'winter']['LandAverageTemperature'].mean())\nsns.set(style=\"whitegrid\")\nsns.set_color_codes(\"pastel\")\nf, ax = plt.subplots(figsize=(10, 6))\n\n#plotting with different colors to easily differentiate.\nplt.plot(years, summer_temps, label='Summers average temperature', color='orange')\nplt.plot(years, autumn_temps, label='Autumns average temperature', color='r')\nplt.plot(years, spring_temps, label='Springs average temperature', color='g')\nplt.plot(years, winter_temps, label='Winters average temperature', color='b')\n\nplt.xlim(min_year, max_year)\n\nax.set_ylabel('Average temperature')\nax.set_xlabel('Year')\nax.set_title('Average temperature in each season')\nlegend = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), frameon=True, borderpad=1, borderaxespad=1)","172e6d53":"global_temp_by_country = global_temp_by_country.sort_values('AverageTemperature')\nbins =  np.arange(-38, 38, 5)\nind = np.digitize(global_temp_by_country['AverageTemperature'],bins)\n    \nglobal_temp_by_country.groupby(ind).head() \n\n\n#global_temp_by_country.groupby('AverageTemperature').Country.count()","331459aa":"co2_ppm = pd.read_csv(\"..\/input\/carbon-dioxide\/archive.csv\")\nco2_ppm.head()","d0eaa7b9":"global_temp_by_country = pd.read_csv(\"..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv\")\ndata_india = global_temp_by_country[global_temp_by_country[\"Country\"] == \"India\"].copy()\ndata_india[\"dt\"] = pd.to_datetime(data_india[\"dt\"])\ndata_india.head()","56000266":"global_temp = pd.read_csv(\"..\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv\")\nglobal_temp[\"dt\"] = pd.to_datetime(global_temp[\"dt\"])","ccebd117":"annual_mean_global = global_temp.groupby(global_temp[\"dt\"].dt.year).mean()\nreference_temperature_global = annual_mean_global.loc[1951:1980].mean()[\"LandAndOceanAverageTemperature\"]\nannual_mean_global[\"Anomaly\"] = annual_mean_global[\"LandAndOceanAverageTemperature\"] - reference_temperature_global\n\nannual_mean_india = data_india.groupby(data_india[\"dt\"].dt.year).mean()\nreference_temperature_india = annual_mean_india.loc[1951:1980].mean()[\"AverageTemperature\"]\nannual_mean_india[\"Anomaly\"] = annual_mean_india[\"AverageTemperature\"] - reference_temperature_india","fcb4d211":"plt.figure()\nplt.style.use(\"fivethirtyeight\")\nannual_mean_global.loc[1960:2015][\"Anomaly\"].plot(figsize = (10,5), grid=True, legend=True)\nplt.title(\"Annual anomaly from base mean temperature (Global)\")\nplt.xlabel('')\nplt.ylabel('Temperature Anomaly')\nplt.show()","c1d43797":"plt.figure()\nplt.style.use(\"fivethirtyeight\")\nannual_mean_india.loc[1960:2012][\"Anomaly\"].plot(figsize = (10,5), grid=True, legend=True)\nplt.title(\"Annual anomaly from base mean temperature (India)\")\nplt.xlabel('')\nplt.ylabel('Temperature Anomaly')\nplt.show()","04beaca2":"plt.figure()\nplt.style.use(\"fivethirtyeight\")\nannual_co2_ppm = co2_ppm.groupby(co2_ppm[\"Year\"]).mean()\nannual_co2_ppm.loc[1960:2015][\"Carbon Dioxide (ppm)\"].plot(figsize = (10,5), grid=True, legend=True)\nplt.title(\"Global annual CO2 levels in atmosphere\")\nplt.ylabel(\"CO2 parts per million\")\nplt.show()","07bb2184":"annual_co2_temp = pd.merge(annual_mean_global.loc[1960:2015], annual_co2_ppm.loc[1960:2015], left_index=True, right_index=True)\nannual_co2_temp = annual_co2_temp[[\"LandAndOceanAverageTemperature\", \"Anomaly\", \"Carbon Dioxide (ppm)\"]].copy()\nannual_co2_temp.corr()\n#annual_co2_temp.head()\n","6f7ed5c8":"plt.figure(figsize=(10,8))\nplt.title(\"Change CO2 levels in atmosphere\")\nplt.scatter(x=\"Anomaly\",y=\"Carbon Dioxide (ppm)\", data=annual_co2_temp)\nplt.xlabel(\"Change in Anomaly\")\nplt.ylabel(\"Co2 content in admosphere\")","f50fc279":"# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected = True)\nimport plotly.figure_factory as ff","d303ef14":"global_temp.head()","32bd965c":"#dont run this, takes forever to run\n#dat = ff.create_table(global_temp)\ndat = ff.create_table(global_temp.head())\npy.iplot(dat)","f02c8bff":"sns.pairplot(global_temp)\nplt.title('Pairplot for the Data', fontsize = 4)\nplt.show()\n#Pair Plots are a really simple (one-line-of-code simple!) way to visualize relationships between each variable. \n#It produces a matrix of relationships between each variable in your data for an instant examination of our data. \n#It can also be a great jumping off point for determining types of regression analysis to use.\n","951dafbb":"plt.rcParams['figure.figsize'] = (15, 8)\nsns.heatmap(global_temp.corr(), cmap = 'Wistia', annot = True)\nplt.title('Heatmap for the Data', fontsize = 20)\nplt.show()\n\n# they show in a glance which variables are correlated, to what degree, in which direction, and alerts us to potential multicollinearity problems.","82dd332d":"\nglobal_temp.head()\nx = global_temp.iloc[:, [0, 1]].values\n\n#print(x.shape)\n#print(x)\n\n","dc2b2925":"\n\nfrom sklearn.cluster import KMeans\n\nwcss = []\nfor i in range(1, 11):\n    km = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    km.fit(x)\n    wcss.append(km.inertia_)\n    \nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method', fontsize = 20)\nplt.xlabel('No. of Clusters')\nplt.ylabel('wcss')\nplt.show()\n\n#In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set.\n#The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow \n#of the curve as the number of clusters to use.\n","0ef80fa4":"km = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(x)\n\nplt.scatter(x[y_means == 0, 0], x[y_means == 0, 1], s = 100, c = 'pink', label = 'hot')\nplt.scatter(x[y_means == 1, 0], x[y_means == 1, 1], s = 100, c = 'cyan', label = 'cold')\nplt.scatter(x[y_means == 2, 0], x[y_means == 2, 1], s = 100, c = 'yellow', label = 'Moderate')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'blue' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Fblobal Average Temperature')\nplt.ylabel('Uncertainity')\nplt.legend()\nplt.grid()\nplt.show()","b03ebd2c":"global_temp.describe()","26eb222f":"import warnings\nwarnings.filterwarnings('ignore')","2349e23b":"# Open the file and saving th data into a dataframe df, then later seleect the rows with country india on them.\ndf = pd.read_csv(\"..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv\", index_col=0, parse_dates=True)\ndf = df.loc[(df.Country==\"India\")|(df.Country==\"China\")]\n# Cleaning process - Drop the rows in which the average temperature is not available\ndf = df.dropna(subset = [\"AverageTemperature\"])","57593997":"df.head()\ndf.tail()","f7a9f8c0":"df.shape","2e8c4abf":"col_names = df.columns\ncol_names","16db8093":"#getting the season month wise\ndef get_target(Country):\n    if Country==\"India\":\n        return 1\n    else:\n        return 0\n    \ndf['target'] = df['Country'].apply(get_target)\n","ea6b7655":"col_names = df.columns\ncol_names","98af2dec":"df.head()\n#df.tail()","61242e30":"df.describe()","41fc2699":"plt.subplot(2, 1, 1)\nfig = df.boxplot(column='AverageTemperature')\nfig.set_title('')\nfig.set_ylabel('AT')\n\n\nplt.subplot(2, 1, 2)\nfig = df.boxplot(column='AverageTemperatureUncertainty')\nfig.set_title('')\nfig.set_ylabel('AT Uncrtnty')","49e49777":"\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(2, 1, 1)\nfig = df['AverageTemperature'].hist(bins=20)\nfig.set_xlabel('AT')\n#fig.set_ylabel('')\n\n\nplt.subplot(2, 1, 2)\nfig = df['AverageTemperatureUncertainty'].hist(bins=20)\nfig.set_xlabel('AT - U')\n#fig.set_ylabel('')","7a7b9d48":"X = df.drop(['target'], axis=1)\nX = df.drop(['Country'],axis=1)\n\ny = df['target']","62a43aac":"# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\nX_train.shape, X_test.shape","c12de413":"cols = X_train.columns\nfrom sklearn.preprocessing import StandardScaler\n# import SVC classifier\nfrom sklearn.svm import SVC\n\n\n# import metrics to compute accuracy\nfrom sklearn.metrics import accuracy_score\n\n","2d800444":"\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","2941f1d2":"X_train = pd.DataFrame(X_train, columns=[cols])","41d7816d":"X_test = pd.DataFrame(X_test, columns=[cols])","17b6de03":"X_train.describe()","65108ab1":"# instantiate classifier with default hyperparameters\nsvc=SVC() \n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n# compute and print accuracy score\nprint('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","500a2de7":"# instantiate classifier with rbf kernel and C=100\nsvc=SVC(C=100.0) \n\n\n# fit classifier to training set\n\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with rbf kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","58c2b6d2":"# instantiate classifier with rbf kernel and C=1000\nsvc=SVC(C=1000.0) \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with rbf kernel and C=1000.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","d491fded":"# instantiate classifier with linear kernel and C=1.0\nlinear_svc=SVC(kernel='linear', C=1.0) \n\n\n# fit classifier to training set\nlinear_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred_test=linear_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))","4b50a602":"\n\n# instantiate classifier with linear kernel and C=100.0\nlinear_svc100=SVC(kernel='linear', C=100.0) \n\n\n# fit classifier to training set\nlinear_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=linear_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\n","79c2409f":"\n\n# instantiate classifier with linear kernel and C=100.0\nlinear_svc100=SVC(kernel='linear', C=100.0) \n\n\n# fit classifier to training set\nlinear_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=linear_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n","e4287a6e":"y_pred_train = linear_svc.predict(X_train)\n\ny_pred_train","39b58d8c":"\n\nprint('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))\n\n","3cd343e7":"print('Training set score: {:.4f}'.format(linear_svc.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(linear_svc.score(X_test, y_test)))","4054e21e":"# check class distribution in test set\n\ny_test.value_counts()","949f59e7":"\n\n# check null accuracy score\n\nnull_accuracy = (493\/(493+449))\n\nprint('Null accuracy score: {0:0.4f}'. format(null_accuracy))\n\n","20939ddf":"# instantiate classifier with polynomial kernel and C=1.0\npoly_svc=SVC(kernel='poly', C=1.0) \n\n\n# fit classifier to training set\npoly_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","096e5cb2":"# instantiate classifier with polynomial kernel and C=100.0\npoly_svc100=SVC(kernel='poly', C=100.0) \n\n\n# fit classifier to training set\npoly_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","b365777c":" #instantiate classifier with sigmoid kernel and C=1.0\nsigmoid_svc=SVC(kernel='sigmoid', C=1.0) \n\n\n# fit classifier to training set\nsigmoid_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","2bfd702e":"# instantiate classifier with sigmoid kernel and C=100.0\nsigmoid_svc100=SVC(kernel='sigmoid', C=100.0) \n\n\n# fit classifier to training set\nsigmoid_svc100.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","4b5436f6":" #print(X_train)","54069a55":"#     from matplotlib.colors import ListedColormap  \n#     x_set, y_set = X_train, y_train  \n#     x1, x2 = np.meshgrid(np.arange(start = x_set[:, 0].min() - 1, stop = x_set[:, 0].max() + 1, step  =0.01),np.arange(start = x_set[:, 1].min() - 1, stop = x_set[:, 1].max() + 1, step = 0.01))  \n#     mtp.contourf(x1, x2, classifier.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),alpha = 0.75, cmap = ListedColormap(('red', 'green')))  \n#     mtp.xlim(x1.min(), x1.max())  \n#     mtp.ylim(x2.min(), x2.max())  \n#     for i, j in enumerate(np.unique(y_set)):  \n#         mtp.scatter(x_set[y_set == j, 0], x_set[y_set == j, 1],c = ListedColormap(('red', 'green'))(i), label = j)  \n#     mtp.title('SVM classifier (Training set)')  \n#     mtp.xlabel('Age')  \n#     mtp.ylabel('Estimated Salary')  \n#     mtp.legend()  \n#     mtp.show()  ","ee3f3856":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_test)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","822b4606":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","9e7d723e":"\n\n# plot ROC Curve\n\nfrom sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_test)\n\nplt.figure(figsize=(6,4))\n\nplt.plot(fpr, tpr, linewidth=2)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\n\nplt.title('ROC curve for the classifier')\n\nplt.xlabel('False Positive Rate (1 - Specificity)')\n\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()\n\n","91db2e7a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#ignore all the above if you are starting from the beginning. Else if starting from here, run!\n\nfrom sklearn import linear_model\nfrom sklearn.utils import shuffle\nfrom subprocess import check_output\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures","ce057afa":"# Open the file and saving th data into a dataframe df, then later seleect the rows with country india on them.\ndf = pd.read_csv(\"..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv\", index_col=0, parse_dates=True)\ndf = df.loc[df[\"Country\"]==\"India\"]\n# Cleaning process - Drop the rows in which the average temperature is not available\ndf = df.dropna(subset = [\"AverageTemperature\"])","489f626c":"# Shuffle the dataset after selecting the column average temperature\ntemp = shuffle(df[\"AverageTemperature\"])\n# Calculating the number of days since day 1 in the dataset i.e 1796-01-01\ndays_since = pd.Series((temp.keys().year * 365 + temp.keys().month * 30 + temp.keys().day) - (1796*365 + 1*30 + 1),index=temp.keys(),name=\"DaysSince\")\n# Concatenating the series' temp and day_since to create a dataframe 'data' with all the necessary data for basic linear regression\ndata = pd.concat([temp, days_since],axis=1)\n# Deleting the initial dataframe as it is useless now\ndel df","f41f2a9e":"# Dividing the shuffled data into a training set and test set\ntrain = data.iloc[:-int(len(data)*.2)]\ntest = data.iloc[-int(len(data)*.2):]","48fcfb38":"# Using the linear model from Sci-kit Learn to train a linear regression\nregr = linear_model.LinearRegression()\nregr.fit(train[\"DaysSince\"].to_frame(), train[\"AverageTemperature\"].to_frame())","03936483":"\n# Printing the coefficients after training the linear regression model on the training data\nprint('Coefficients: \\n', regr.coef_)\n# Printing the mean squared error calculated from the data in the test set\nprint(\"Mean squared error: %.2f\"\n      % pd.DataFrame.mean((regr.predict(test[\"DaysSince\"].to_frame()) - test[\"AverageTemperature\"].to_frame()) ** 2))\n# Printing the variance score on the test set\nprint('Variance score: %.2f' % regr.score(test[\"DaysSince\"].to_frame(), test[\"AverageTemperature\"].to_frame()))\n\n# Plotting the test data as points\nplt.scatter(test[\"DaysSince\"], test[\"AverageTemperature\"],  color='black')","cc932419":"\n# Plotting the test data as points\nplt.scatter(test[\"DaysSince\"], test[\"AverageTemperature\"],  color='black')","d63e0709":"\ndf_ml = global_temp.groupby(\"Year\")[\"LandAndOceanAverageTemperature\"].mean()\ndf_ml.head()","a0f159d1":"df_ml = df_ml.to_frame().reset_index()\nprint(df_ml.head(3))\nprint(df_ml.shape)","557b27cc":"X = df_ml[[\"Year\"]].values\nY = df_ml[[\"LandAndOceanAverageTemperature\"]].values","176a96c4":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y,random_state = 0,test_size = 0.25)","ec6f8fce":"#from sklearn.linear_model import Perceptron\n#from sklearn.preprocessing import PolynomialFeatures\n#clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,shuffle=False).fit(X, Y)","d7f5d8da":"lm = LinearRegression()\nlm.fit(X_train, Y_train)\nprint(\"Test accuracy: \" + str(lm.score(X_test, Y_test)))\nprint(\"Train accuracy: \" + str(lm.score(X_train, Y_train)))\n\nY_predicted_lm = lm.predict(X_test)","323948d5":"print(lm.intercept_)\n#this is the intercept","06ffeb74":"print(lm.coef_)\n#this is the slope","4473948b":"y_pred = lm.predict(X_train)\n#print(y_pred)","d05411f5":"plt.figure(figsize = (12,8))\nplt.scatter(X_train, Y_train, label = \"Train\")\nplt.scatter(X_test, Y_test, label = \"Test\")\nplt.plot(X_test, Y_predicted_lm, color = \"green\", linewidth = 2, label = \"Regression\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature\")\nplt.title(\"Linear Regression\", fontsize = 16)\nplt.legend()\nplt.show()","b5d37494":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_predicted_lm))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_predicted_lm))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_predicted_lm)))","c60aeacc":"poly = PolynomialFeatures(degree = 8, include_bias = False)\n\nX_train_transformed = poly.fit_transform(X_train)\nX_test_transformed = poly.fit_transform(X_test)","23012221":"model_poly = LinearRegression()\nmodel_poly.fit(X_train_transformed, Y_train)\nprint(\"Test accuracy: \" + str(model_poly.score(X_test_transformed, Y_test)))\nprint(\"Train accuracy: \" + str(model_poly.score(X_train_transformed, Y_train)))\n","e6133191":"plt.figure(figsize = (12,8))\nplt.scatter(X_train, Y_train, label = \"Train\")\nplt.scatter(X_test, Y_test, label = \"Test\")\nplt.plot(X, model_poly.predict(poly.fit_transform(X)), color = \"green\", linewidth = 2, label = \"Regression\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Temperature\")\nplt.title(\"Polynomial Regression\", fontsize = 16)\nplt.legend()\nplt.show()","32dd18f0":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","20e106db":"import fbprophet","034854d4":"print('Prophet %s' % fbprophet.__version__)","9de4a1c6":"# load the temperature dataset, if starting from here. Else skip it.\nfrom pandas import read_csv\ndf = read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv', header=0)\ndf=df.iloc[:,:-1]\n# getting the number of rows and cols \nprint(df.shape)\n\n#display a few values frm the last.\nprint(df.tail())","860478e7":"# fit prophet model on Temperature dataset\nfrom pandas import read_csv\nfrom pandas import to_datetime\nfrom fbprophet import Prophet\n# load data\npath = '..\/input\/climate-change-earth-surface-temperature-data\/GlobalLandTemperaturesByCountry.csv'\ndf = read_csv(path, header=0)\ndf=df.iloc[:,:-2]\n# prepare expected column names\ndf.columns = ['ds', 'y']\ndf['ds']= to_datetime(df['ds'])\ndf.head()\ndf.dropna(inplace=True)\ndf.head()\nprint(df.shape)\ndf=df.iloc[:50000,:]","12b68d49":"# define the model\nmodel = Prophet()\n# fit the model\nmodel.fit(df)","ee1543ca":"future = list()\nfor i in range(1, 13):\n\tdate = '2025-%02d' % i\n\tfuture.append([date])\nfuture = pd.DataFrame(future)\nfuture.columns = ['ds']\nfuture['ds']= to_datetime(future['ds'])","3c99867c":"# summarize the forecast\nforecast = model.predict(future)\nprint(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head())","9843a3ad":"# calculate MAE between expected and predicted values for december\nfrom sklearn.metrics import mean_absolute_error\ny_true = df['y'][-12:].values\ny_pred = forecast['yhat'].values\nmae = mean_absolute_error(y_true, y_pred)\nprint('MAE: %.3f' % mae)","ec4e4274":"global_temp_by_major_city.index","478d270d":"global_temp_by_major_city = global_temp_by_major_city.reset_index()","763408ec":"global_temp_by_major_city.index","d6ff12fc":"plt.figure()\nx = pd.to_datetime(global_temp_by_major_city.dt[:], format='%Y-%m-%d', errors='ignore').dt.year\nplt.subplot(221) \nplt.title('Before 1900')\nplt.boxplot([global_temp_by_major_city.AverageTemperature[x<1900][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Madras\"].dropna().values,\n             global_temp_by_major_city.AverageTemperature[x<1900][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Delhi\"].dropna().values,\n            global_temp_by_major_city.AverageTemperature[x<1900][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Bangalore\"].dropna().values]\n            ,0,'kd',1,1)\nplt.axis([0,4,-25, 35])\nplt.xticks([1, 2, 3], ['Chn', 'Dlh', 'Bnglr'])\nplt.subplot(222) \nplt.title('1900-2000')\nplt.boxplot([global_temp_by_major_city.AverageTemperature[x>=1900][x<2000][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Madras\"].dropna().values,\n             global_temp_by_major_city.AverageTemperature[x>=1900][x<2000][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Delhi\"].dropna().values,\n            global_temp_by_major_city.AverageTemperature[x>=1900][x<2000][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Bangalore\"].dropna().values]\n            ,0,'kd',1,1)\nplt.axis([0,4,-25, 35])\nplt.xticks([1, 2, 3], ['Chn', 'Dlh', 'Bnglr'])\nplt.subplot(223) \nplt.title('2000-2008')\nplt.boxplot([global_temp_by_major_city.AverageTemperature[x>=2000][x<2008][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Madras\"].dropna().values,\n             global_temp_by_major_city.AverageTemperature[x>=2000][x<2008][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Delhi\"].dropna().values,\n            global_temp_by_major_city.AverageTemperature[x>=2000][x<2008][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Bangalore\"].dropna().values]\n            ,0,'kd',1,1)\nplt.axis([0,4,-25, 35])\nplt.xticks([1, 2, 3], ['Chn', 'Dlh', 'Bnglr'])\nplt.subplot(224) \nplt.title('After 2008')\nplt.boxplot([global_temp_by_major_city.AverageTemperature[x>=2008][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Madras\"].dropna().values,\n             global_temp_by_major_city.AverageTemperature[x>=2008][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Delhi\"].dropna().values,\n            global_temp_by_major_city.AverageTemperature[x>=2008][global_temp_by_major_city.Country==\"India\"][global_temp_by_major_city.City==\"Bangalore\"].dropna().values]\n            ,0,'kd',1,1)\nplt.axis([0,4,-25, 35])\nplt.xticks([1, 2, 3], ['Chn', 'Dlh', 'Bnglr'])\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.4,\n                    wspace=0.3)","9ea40043":"plt.figure(figsize = (12,8))\nsns.lineplot(data = global_temp, x = \"Year\", y = \"LandAndOceanAverageTemperature\")\nplt.title(\"Global Average Temperature per Year\")\nplt.ylabel(\"Temperature\")\nplt.show()","542d23ff":"plt.figure(figsize = (12,8))\nsns.lineplot(data = global_temp, x = \"Year\", y = \"LandMaxTemperature\", label = \"Max\")\nsns.lineplot(data = global_temp, x = \"Year\", y = \"LandMinTemperature\", label = \"Min\")\nplt.title(\"Global Maximum and Minimum Temperatures\")\nplt.ylabel(\"Temperature\")\nplt.legend()\nplt.show()\n","4c36d68d":"plt.figure(figsize = (12,8))\nglobal_temp_by_country.groupby(\"Country\")[\"AverageTemperature\"].mean().sort_values(ascending = False).head(30).plot.bar()\nplt.ylabel(\"Temperature\")\nplt.title(\"Hottest Countries\")\nplt.show()","3a79fc63":"plt.figure(figsize = (12,8))\nglobal_temp_by_country.groupby(\"Country\")[\"AverageTemperature\"].mean().sort_values(ascending = True).head(30).plot.bar()\nplt.ylabel(\"Temperature\")\nplt.title(\"Coldest Countries\")\nplt.show()","615400f4":"plt.rcParams['figure.figsize'] = (18, 7)\nsns.boxenplot(global_temp['Year'], global_temp['LandAverageTemperature'], palette = 'Blues')\nplt.title('Year vs Temperature', fontsize = 20)\nplt.show()\n\n#similar to a box plot in plotting a nonparametric representation of a distribution in which all features correspond to actual observations. \n#By plotting more quantiles, it provides more information about the shape of the distribution, particularly in the tails. ","1fc312b7":"#global_temp_by_country[col].plot(subplots=True, figsize=(20, 10))\n#plt.savefig('Linesubplots.png', bbox_inches = 'tight')\n#produces error , troubleshoot it later. ","87710534":"\n\nx = global_temp['Year']\ny = global_temp['LandAverageTemperature']\nz = global_temp['LandAndOceanAverageTemperature']\n\nsns.lineplot(x, y, color = 'blue')\nsns.lineplot(x, z, color = 'pink')\nplt.title('Year vs Land Temperature and Ocean Temperature', fontsize = 20)\nplt.show()\n\n","2b959a5a":"So now we've taken a dataset and converted a rougue object data type to datatime datatype and made it the index of the dataset.  Additionally we got the summary of the dataset as well, now the same procedure is repeated for all other datasets in this directory as well, so it would be useful to just directly use them in the futhur rather than cleaning the data whenever necessary.","9674c026":"Sigmoid kernal has slightly lass accuracy when compared to modal accuracy but it still is high, and does a good job in predictng.","75f164ab":"this means that every one unit (day) the temo changes by an increase of 0.007%.","22eae539":"Target class created. It has to be discrete. ","d185becd":"Now we can furthur create new cols with the year alone ","4f585626":"Calculated the mean temperature of the 1951 - 1980 period to establish the global base mean temperature. \nThe deviation from this temperature is added in the Anomaly column. \n\/+ created a new dataframe for our country, and repeated this process.","ada694ad":"____________________________________________________________________________\n____________________________________________________________________________\n","9e872e06":"We have the maximum accuracy with rfd and linear kernal htat is 1.0. based on the analysis we can conclude that our model does a good job in predicting class labels. But this is not true because the dataset is imbalanced\/skewed.\nThe problem is that accuracy is an inadequate measure for quantifying predictive performance in the imbalanced dataset problem.","ba8d9686":"the global mean temperature has grown steadily the past decades, leading to a temperature anomaly of about 0.75 celsius in 2015. As expected, this result is consistent with the scientific consensus on climate change.","52d49d69":"Polynomial Regression","895ec24c":"Algorithm - 3 : Kmeans","da371100":"There are a lot of ouliers in the uncertainities col. Now let's handle these outliers with SVM","32de5f57":"Now that we've seen various states, let'dig deeper and have a glance at out city. ","eeb457ca":"____________________________________________________________________________\n____________________________________________________________________________\n","7ebbd545":"Apriori Algorithm - ALgorithm implementation no - 1","05912bf1":"____________________________________________________________________________\n____________________________________________________________________________\n","fb993bd8":"____________________________________________________________________________\n____________________________________________________________________________\n","29d28a59":"\nRun SVM with rbf kernel and C=100.0\n\nWe have seen that there are outliers in our dataset. So, we should increase the value of C as higher C means fewer outliers. So, I will run SVM with kernel=rbf and C=100.0.\n","8494488e":"We just predicted the temperature of the earth in this year, with the data collected 10 years before today. Similarly we can also calculate the temperature of many years to come. ","7e740534":"This scatter plot visualizes the linear relation between CO2 levels and temperature anomaly.","cbb436bb":"____________________________________________________________________________\n____________________________________________________________________________\n","186406ff":"We see a lot of NaN values in the dataset, let us try and compute the number of NaN in the dataset.","23003ab9":"Algorithm No - 4 \nSVM support vector machines : \nThis is a supervised machine learning algorithm that is used for classification and regression and finding outlier purposes. \nAn SVM classifier builds a model that assigns new data points to one of the given categories","7a8e70c5":"____________________________________________________________________________\n____________________________________________________________________________\n","3daa92a6":"Previously the datatype of date is mentioned as object. This data type can be worked with, but would be rather tiresome. datetime is the data time that is more suitable for initialising dates. ","3b27026b":"This is intended for leaning and for swe2009 - Data mining techniquies project. \nAuthors : Harshitha Devineni, Daiyaan Ahmed. \n\nIn this notebook we are going to start of with data cleaning, then followed up working up a few algorithms on the dataset and then finish it off by visualizations of the data and the analysis that we've come upon. \nThank you :)\n","fce20eba":"____________________________________________________________________________\n____________________________________________________________________________\n","ed279f61":"Box Plot\n","e57baf99":"We can dig in deeper and find the temperatures per state and per city as well. This is basically refining the database to our requirements. For states, we use the 'GlobalLandTemperaturesByState.csv' dataset and for cities we use 'GlobalLandTemperaturesByMajorCity.csv'\nglobal_temp = pd.read_csv('..\/input\/climate-change-earth-surface-temperature-data\/GlobalTemperatures.csv')","82896afe":"____________________________________________________________________________\n____________________________________________________________________________","e4ce3680":"Evenly spead across y axis, this means that, the uncertainities are similar for temperature clusters of various types.","757c1d9a":"Well, Chennai is named as Madras, let's have a look at our temperature over the years.","0e1ad7b5":"____________________________________________________________________________\n____________________________________________________________________________\n","64118204":"Feature Scaling","50367d0d":"Now let's implement our last algorithm - ** Phophet algorithm. **\nThis is a predictive algorithm.\n\nLet's start with importing 'prophet' - last Algorithm (No6)","5be318fe":"____________________________________________________________________________\n____________________________________________________________________________\n","0c1b3f30":"1 is the highest accuracy possible, test set and train set are very much comparable.","a7d446b1":"End of implementation of algorithms. ","d069919d":"The results are skewed.","a894e56d":"____________________________________________________________________________\n____________________________________________________________________________\n","ef3d682f":"Default hyperparameter means C=1.0, kernel=rbf and gamma=auto among other parameters.","ffa04062":"Alternamte solution for solving such imbalanced probelms is confusion matrix","f6a6d7e1":"MAE is margin of Error.","04e40111":"**HOTTEST\/COLDEST COUNTRIES**","49830439":"plotting the histograms to check distributions to find out if they are normal or skewed.","3d2b4026":"____________________________________________________________________________\n____________________________________________________________________________\n","32722411":"Now let's dig deap into our country's data and see what we can infer from it and if we can clean the dataset furthur. For this, we use the 'global_temp_by_country.csv' dataset","dd63c057":"____________________________________________________________________________\n____________________________________________________________________________\n","5ed83bc0":"Algorithm 2 - Correlation \n\nCorrelation is widely used for finsing the strength of relationship between two variables. How dependent is one on the other. Here we are going to use another dataset which has the CO2 emmisions rates and then see it's correlation with the temperature raise. This will let us determine if CO2 does indeed effect temperature raise or not.\n","02f2686b":"despite modal accuracy being high, null score accuracy is low, therefore we connot prediect that with certainity that the predicting by SVM is high likey.","997740e8":"Declaring feature vector and then the target variable.\nSplitting the traiing set into training and test sets.","3f8103c2":"** GLOBAL TEMPERATURES**","fa6b8f65":"**GLOBAL MIN AND MAX TEMPERATURES**","07728b14":"This describes the number of (rows, cols) in respective datasets.","16ce0766":"Now let's dive into Visualizations. The least attractive and appealing thing about data is there is are a lot of numbers or other datatypes. In a glance it is highly impossible to guess or judge its nature. But visualization makes this possible. Now gaint heaps of data can be seen in ways we connot even begin to imagine in the first place. Now lets explore a few of these methods. ","7744892c":"The temperature has also steadily increased","42a2d72c":"Other related measures...","d36e8d99":"Algorithm - 5 ! \nApplying Linear regression to this model. ","d22ddd6b":"Linear Regression","3e47a5b6":"X_train dataset ready to be fed into the Logistic Regression classifier. I will do it as follows.","b154bf2d":"Predicting global average temperatures by using simple Linear Regression and Polynomial Regression","df72fa89":"The CO2 levels in atmosphere have steadily risen in the 1950-2010 period, indicating a linear relation between greenhouse gases and global temperature.\n\nThe correlation coefficient of CO2 and temperature anomaly is 0.92 , confirming the linear relation between the two variables.\n","3ffccf45":"There are literally zero errors - both type 1 and type 2. Therefore with guarantee the classifies works the best.","354cfdd8":"____________________________________________________________________________\n____________________________________________________________________________\n"}}