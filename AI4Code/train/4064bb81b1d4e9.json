{"cell_type":{"6d1f3c71":"code","31a62b2e":"code","ecaf4483":"code","1031032d":"code","c1366f4e":"code","da1fd91b":"code","0582a23c":"code","6f95aef9":"code","7b786f3d":"code","1098261e":"code","20f917ae":"code","08903655":"code","ba6313b4":"code","753df9cc":"code","4a8cbf95":"code","ea418388":"code","bc24a482":"code","40d189df":"code","578764ec":"code","fd4a1bbc":"code","d602bc3c":"code","1989eca8":"code","47fb5134":"code","05385fbe":"code","4b3d483b":"code","f5d73b16":"code","15300903":"code","02c7632d":"code","d5b44941":"code","75bdafe5":"code","da4d1d4a":"code","78508c70":"code","20f02546":"code","6dc9cdce":"code","e161a04b":"code","d0613e85":"code","79c62f74":"code","1272d037":"code","17d2d1f8":"code","5fad4438":"code","149f02a8":"code","850beac9":"code","c3d0ebd0":"markdown","7bd36c24":"markdown","d27919cb":"markdown","434baf8a":"markdown","2ab8ab11":"markdown","e173a48e":"markdown","496c9cb4":"markdown","31df9aeb":"markdown","588f761c":"markdown","892b14cd":"markdown","1335bd1c":"markdown","474ceb7f":"markdown","211d2059":"markdown","243dd801":"markdown","2d61bea8":"markdown","59412afc":"markdown","4b8baa72":"markdown","f4dfc100":"markdown","c7ed9262":"markdown","1a3dd3da":"markdown"},"source":{"6d1f3c71":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31a62b2e":"import os\ncwd = os.getcwd()\nos.chdir(cwd)\nprint(os.listdir(\"..\/input\"))","ecaf4483":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader","1031032d":"# Hyperparameters etc.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nLEARNING_RATE = 2e-4  # could also use two lrs, one for gen and one for disc\nBATCH_SIZE = 128\nIMAGE_SIZE = 64\nCHANNELS_IMG = 3\nNOISE_DIM = 100\nNUM_EPOCHS = 10\nFEATURES_DISC = 64\nFEATURES_GEN = 64","c1366f4e":"device","da1fd91b":"transforms = transforms.Compose([\n    transforms.Resize([64,64]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5 for i in range(CHANNELS_IMG)], [0.5 for i in range(CHANNELS_IMG)])\n])","0582a23c":"IMAGE_PATH = '..\/input\/celeba-dataset\/img_align_celeba'","6f95aef9":"IMAGE_PATH","7b786f3d":"dataset = datasets.ImageFolder(IMAGE_PATH, transform = transforms)","1098261e":"def split_indices(n, val_per, seed = 0):\n    n_val = int(n * val_per)\n    np.random.seed(seed)\n    idx = np.random.permutation(n)\n    return idx[n_val : ], idx[: n_val]","20f917ae":"val_per = 0.5\nrand_seed = 42\n\ntrain_indices, val_indices = split_indices(len(dataset), val_per, rand_seed)\n\nprint(len(train_indices), len(val_indices))","08903655":"print(\"Validation Indices: \", val_indices[:20])\nprint(\"Training Indices: \", train_indices[:20])","ba6313b4":"from torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data.dataloader import DataLoader","753df9cc":"# training data loader\ntrain_sampler = SubsetRandomSampler(train_indices)\nloader = DataLoader(dataset, BATCH_SIZE, sampler = train_sampler)","4a8cbf95":"len(dataset)","ea418388":"len(loader)","bc24a482":"class Discriminator(nn.Module):\n    \n    def __init__(self, channels_img, features_d):\n        super(Discriminator, self).__init__()\n        \n        self.disc = nn.Sequential(\n            #size = 3*64*64\n            nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), # Size : 32*32\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d, features_d*2, kernel_size = 4, stride = 2, padding = 1), # size = 16*16\n            nn.BatchNorm2d(features_d*2),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d*2, features_d*4, kernel_size = 4, stride = 2, padding = 1), # size = 8*8\n            nn.BatchNorm2d(features_d*4),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d*4, features_d*8, kernel_size = 4, stride = 2, padding = 1), # size = 4*4\n            nn.BatchNorm2d(features_d*8),\n            nn.LeakyReLU(0.2),\n            \n            nn.Conv2d(features_d*8, 1, kernel_size = 4, stride = 2, padding = 0), #1*1\n            nn.Sigmoid()\n        )\n        \n        \n    def forward(self, x):\n        return self.disc(x)","40d189df":"class Generator(nn.Module):\n    \n    def __init__(self, z_dim, channels_img, features_g):\n        super(Generator, self).__init__()\n        \n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, features_g*16, kernel_size = 4, stride = 1, padding = 0), # size = 4*4\n            nn.BatchNorm2d(features_g*16),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*16, features_g*8, kernel_size = 4, stride = 2, padding = 1), # size = 8*8\n            nn.BatchNorm2d(features_g*8),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*8, features_g*4, kernel_size = 4, stride = 2, padding = 1), # size = 16*16\n            nn.BatchNorm2d(features_g*4),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*4, features_g*2, kernel_size = 4, stride = 2, padding = 1), # size = 32*32\n            nn.BatchNorm2d(features_g*2),\n            nn.ReLU(),\n            \n            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size = 4, stride = 2, padding = 1),\n            nn.Tanh()  # [-1, 1]\n        )\n        \n    \n    def forward(self, x):\n        return self.net(x)","578764ec":"def initialize_weights(model):\n    # Initializes weights according to the DCGAN paper\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n            nn.init.normal_(m.weight.data, 0.0, 0.02)","fd4a1bbc":"def test():\n    N, in_channels, H, W = 8, 3, 64, 64\n    noise_dim = 100\n    x = torch.randn((N, in_channels, H, W))\n    disc = Discriminator(in_channels, 8)\n    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n    gen = Generator(noise_dim, in_channels, 8)\n    z = torch.randn((N, noise_dim, 1, 1))\n    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n    print(\"Success\")","d602bc3c":"test()","1989eca8":"gen = Generator(z_dim = NOISE_DIM, channels_img = CHANNELS_IMG, features_g = FEATURES_GEN).to(device)\ndisc = Discriminator(CHANNELS_IMG, FEATURES_DISC).to(device)\ninitialize_weights(gen)\ninitialize_weights(disc)","47fb5134":"opt_gen = optim.Adam(gen.parameters(), lr = LEARNING_RATE, betas = (0.5, 0.999))\nopt_disc = optim.Adam(disc.parameters(), lr = LEARNING_RATE, betas = (0.5, 0.999))\ncriterion = nn.BCELoss()","05385fbe":"def reset_grad():\n    opt_disc.zero_grad()\n    opt_gen.zero_grad()","4b3d483b":"def train_discriminator(images):\n    # create labels, for real image label is 1, for fac=ke 0\n    \n    real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n    fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n    \n    # loss for real images\n    \n    outputs = disc(images).reshape(-1)\n    d_loss_real = criterion(outputs, torch.ones_like(outputs))\n    real_score = outputs\n    \n    z = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n    fake_images = gen(z)\n    outputs = disc(fake_images).reshape(-1)\n    d_loss_fake = criterion(outputs, torch.zeros_like(outputs))\n    fake_score = outputs\n    \n    d_loss = d_loss_real + d_loss_fake\n    reset_grad()\n    # Compute gradients\n    \n    d_loss.backward()\n    #Adjust parameters using backpropagation\n    \n    opt_disc.step()\n    \n    return d_loss, real_score, fake_score","f5d73b16":"def train_generator():\n    # Generate fake images and calculate loss\n    z = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n    fake_images = gen(z)\n    labels = torch.ones(BATCH_SIZE, 1).to(device)\n    output = disc(fake_images).reshape(-1)\n    g_loss = criterion(output, torch.ones_like(output))\n\n    # Backprop and optimize\n    reset_grad()\n    g_loss.backward()\n    opt_gen.step()\n    return g_loss, fake_images","15300903":"import os\n\nsample_dir = 'samples'\nif not os.path.exists(sample_dir):\n    os.makedirs(sample_dir)","02c7632d":"def show_img(img, label):\n    print('Label: ', label)\n    plt.imshow(img.permute(1,2,0), cmap = 'gray')","d5b44941":"def denorm(x):\n  out = (x + 1) \/ 2\n  return out.clamp(0, 1)","75bdafe5":"img = dataset[0]","da4d1d4a":"img[0].shape","78508c70":"from IPython.display import Image\nfrom torchvision.utils import save_image\n\n# Save some real images\nfor images, _ in loader:\n    images = images.reshape(images.size(0), 3, 64, 64)\n    save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'), nrow=16)\n    break\n   \nImage(os.path.join(sample_dir, 'real_images.png'))","20f02546":"sample_vectors = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n\ndef save_fake_images(index):\n    fake_images = gen(sample_vectors)\n    fake_images = fake_images.reshape(fake_images.size(0), 3, 64, 64)\n    fake_fname = 'fake_images-{0:0=4d}.png'.format(index)\n    print('Saving', fake_fname)\n    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=16)\n    \n# Before training\nsave_fake_images(0)\nImage(os.path.join(sample_dir, 'fake_images-0000.png'))","6dc9cdce":"%%time\n\n\ntotal_step = len(loader)\nd_losses, g_losses, real_scores, fake_scores = [], [], [], []\n\nfor epoch in range(NUM_EPOCHS):\n    for i, (images, _) in enumerate(loader):\n        # Load a batch & transform to vectors\n        images = images.to(device)\n        \n        # Train the discriminator and generator\n        d_loss, real_score, fake_score = train_discriminator(images)\n        g_loss, fake_images = train_generator()\n        \n        \n        # Inspect the losses\n        if (i+1) % 100 == 0:\n            d_losses.append(d_loss.item())\n            g_losses.append(g_loss.item())\n            real_scores.append(real_score.mean().item())\n            fake_scores.append(fake_score.mean().item())\n            print('Epoch [{}\/{}], Step [{}\/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n                  .format(epoch, NUM_EPOCHS, i+1, total_step, d_loss.item(), g_loss.item(), \n                          real_score.mean().item(), fake_score.mean().item()))\n        \n       \n        \n    # Sample and save images\n    save_fake_images(epoch+1)","e161a04b":"# Save the model checkpoints \ntorch.save(gen.state_dict(), 'G.ckpt')\ntorch.save(disc.state_dict(), 'D.ckpt')","d0613e85":"import cv2\nimport os\nfrom IPython.display import FileLink\n\nvid_fname = 'gans_training.avi'\n\nfiles = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'fake_images' in f]\nfiles.sort()\n\nout = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (650,650))\n[out.write(cv2.imread(fname)) for fname in files]\nout.release()\nFileLink('gans_training.avi')","79c62f74":"import matplotlib.pyplot as plt","1272d037":"plt.plot(d_losses, '-')\nplt.plot(g_losses, '-')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Discriminator', 'Generator'])\nplt.title('Losses');","17d2d1f8":"plt.plot(real_scores, '-')\nplt.plot(fake_scores, '-')\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.legend(['Real Score', 'Fake score'])\nplt.title('Scores');","5fad4438":"Image('.\/samples\/fake_images-0005.png')","149f02a8":"Image('.\/samples\/fake_images-0008.png')","850beac9":"Image('.\/samples\/fake_images-0010.png')","c3d0ebd0":"# Plot Losses","7bd36c24":"# Save Model","d27919cb":"# Plot Score","434baf8a":"# training Generator","2ab8ab11":"This is really breathtaking how an algorithm generates images from nothing to such meaningful output within 10 round of Epoch!!\n\nThanks for your time, kindly consider Upvoting!!","e173a48e":"# Generator Modeling","496c9cb4":"\n**How does GANs work?**\n\nGANs learn a probability distribution of a dataset by pitting two neural networks against each other.\n\nOne neural network, called the Generator, generates new data instances, while the other, the Discriminator, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data that it reviews belongs to the actual training dataset or not.\n\nMeanwhile, the generator is creating new, synthetic\/fake images that it passes to the discriminator. It does so in the hopes that they, too, will be deemed authentic, even though they are fake. The fake image is generated from a 100-dimensional noise (uniform distribution between -1.0 to 1.0) using the inverse of convolution, called transposed convolution.\n\nThe goal of the generator is to generate passable images: to lie without being caught. The goal of the discriminator is to identify images coming from the generator as fake.\n\n**Here are the steps a GAN takes:**\n\nThe generator takes in random numbers and returns an image.\n\nThis generated image is fed into the discriminator alongside a stream of images taken from the actual, ground-truth dataset.\n\nThe discriminator takes in both real and fake images and returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.\n\nSo you have a double feedback loop:\n\nThe discriminator is in a feedback loop with the ground truth of the images, which we know.\n\nThe generator is in a feedback loop with the discriminator.","31df9aeb":"# Check Images","588f761c":"# Image Saving Code","892b14cd":"# Generator and Discriminator Initiation","1335bd1c":"# Import Required Libraries & Load Data","474ceb7f":"# Training Discriminator","211d2059":"**Define a GAN Model**\n\nNext, a GAN model can be defined that combines both the generator model and the discriminator model into one larger model. This larger model will be used to train the model weights in the generator, using the output and error calculated by the discriminator model. The discriminator model is trained separately, and as such, the model weights are marked as not trainable in this larger GAN model to ensure that only the weights of the generator model are updated. This change to the trainability of the discriminator weights only affects when training the combined GAN model, not when training the discriminator standalone.\n\nThis larger GAN model takes as input a point in the latent space, uses the generator model to generate an image, which is fed as input to the discriminator model, then output or classified as real or fake.\n\nSince the output of the Discriminator is sigmoid, we use binary cross-entropy for the loss. RMSProp as an optimizer generates more realistic fake images compared to Adam for this case. The learning rate is 0.0001. Weight decay and clip value stabilize learning during the latter part of the training. You have to adjust the decay if you want to adjust the learning rate.\n\nGANs try to replicate a probability distribution. Therefore, we should use loss functions that reflect the distance between the distribution of the data generated by the GAN and the distribution of the real data.","243dd801":"# Discriminator Model Building","2d61bea8":"**Training the GAN model:**\n\nTraining is the hardest part and since a GAN contains two separately trained networks, its training algorithm must address two complications:\n\nGANs must juggle two different kinds of training (generator and discriminator).\n\nGAN convergence is hard to identify.\n\nAs the generator improves with training, the discriminator performance gets worse because the discriminator can\u2019t easily tell the difference between real and fake. If the generator succeeds perfectly, then the discriminator has a 50% accuracy. In effect, the discriminator flips a coin to make its prediction.\n\nThis progression poses a problem for convergence of the GAN as a whole: the discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its quality may collapse.","59412afc":"# Training of Model","4b8baa72":"# Image View","f4dfc100":"Thanks for your time, kindly consider Upvoting!!","c7ed9262":"**Why were GANs developed in the first place?**\n\nIt has been noticed most of the mainstream neural nets can be easily fooled into misclassifying things by adding only a small amount of noise into the original data. Surprisingly, the model after adding noise has higher confidence in the wrong prediction than when it predicted correctly. The reason for such an adversary is that most machine learning models learn from a limited amount of data, which is a huge drawback, as it is prone to overfitting. Also, the mapping between the input and the output is almost linear. Although it may seem that the boundaries of separation between the various classes are linear, in reality, they are composed of linearities and even a small change in a point in the feature space might lead to misclassification of data.","1a3dd3da":"# Hyper parameter Set"}}