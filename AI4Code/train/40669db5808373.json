{"cell_type":{"982694f5":"code","7ff13883":"code","d3b77254":"code","cab098cc":"code","f3a2f3ee":"code","5d42d290":"code","59a2cd58":"code","56a93fb9":"code","040bc011":"code","d4b566a1":"code","e7d81bda":"code","4de8e10f":"code","183a4693":"code","f5200509":"code","ab7a72eb":"code","1bb8b855":"code","f3773323":"code","298ce7ae":"code","ec2035a9":"code","daa6a1b2":"code","bf6480b6":"code","e8fc0f64":"code","f9f322ad":"code","20d206dc":"code","aab78957":"code","05525dce":"code","dbd4ef30":"code","4c94634b":"code","e3f606bb":"code","570385b6":"code","6d70886b":"code","fbd2ae5b":"code","993e0127":"code","059ba5aa":"code","25942831":"markdown"},"source":{"982694f5":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.metrics import classification_report, f1_score\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\nfrom keras.layers.embeddings import Embedding","7ff13883":"VOCAB_SIZE = 20000\nMAX_LEN = 100\nNUM_CLASSES = 6","d3b77254":"train_df = pd.read_csv('..\/input\/train.csv', index_col='id').fillna(' ')\nval_df = pd.read_csv('..\/input\/valid.csv', index_col='id').fillna(' ')\ntest_df = pd.read_csv('..\/input\/test.csv', index_col='id').fillna(' ')","cab098cc":"label_binarizer = LabelBinarizer().fit(pd.concat([train_df['label'], val_df['label']]))","f3a2f3ee":"y_train = label_binarizer.transform(train_df['label'])\ny_valid = label_binarizer.transform(val_df['label'])","5d42d290":"X_train, X_valid = train_df['text'].values, val_df['text'].values","59a2cd58":"vocabulary_size = VOCAB_SIZE\ntokenizer = Tokenizer(num_words=vocabulary_size)\ntokenizer.fit_on_texts(X_train)\nsequences = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(sequences, maxlen=MAX_LEN)","56a93fb9":"sequences = tokenizer.texts_to_sequences(X_valid)\nX_valid = pad_sequences(sequences, maxlen=MAX_LEN)","040bc011":"model = Sequential()\nmodel.add(Embedding(VOCAB_SIZE, 100, input_length=MAX_LEN))\nmodel.add(Dropout(rate=0.5))\nmodel.add(Conv1D(64, 5, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(100))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))","d4b566a1":"model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","e7d81bda":"model.fit(X_train, y_train,\n                    batch_size=1024,\n                    epochs=10,\n                    verbose=1,\n                    validation_split=0.1)","4de8e10f":"score = model.evaluate(X_valid, y_valid, batch_size=256, verbose=1)\nprint('Validation accuracy:', score[1])","183a4693":"val_preds = model.predict(X_valid)","f5200509":"print(classification_report(np.argmax(y_valid,axis=1), np.argmax(val_preds, axis=1)))","ab7a72eb":"f1_score(np.argmax(y_valid,axis=1), np.argmax(val_preds, axis=1), average='micro')","1bb8b855":"!git clone https:\/\/github.com\/jasonwei20\/eda_nlp.git\n!rm -rf eda_nlp\/.git","f3773323":"train_df['sent_len'] = train_df['text'].apply(lambda s: len(s.split()))\n\ntrain_df = train_df[train_df['sent_len'] > 1]","298ce7ae":"train_df.head()","ec2035a9":"train_df[['label', 'text']].to_csv('train.tsv', sep='\\t', index=None, header=None)","daa6a1b2":"%%time\n!python eda_nlp\/code\/augment.py --input=train.tsv --output=train_aug.tsv","bf6480b6":"!wc -l train_aug.tsv","e8fc0f64":"train_df_aug = pd.read_csv('train_aug.tsv', sep='\\t', header=None,\n                          names=['label', 'text'])","f9f322ad":"train_df_aug.head()","20d206dc":"X_train_aug = train_df_aug['text'].values","aab78957":"y_train_aug = label_binarizer.transform(train_df_aug['label'])","05525dce":"%%time\nvocabulary_size = VOCAB_SIZE\ntokenizer = Tokenizer(num_words=vocabulary_size)\ntokenizer.fit_on_texts(X_train_aug)\nsequences = tokenizer.texts_to_sequences(X_train_aug)\nX_train_aug = pad_sequences(sequences, maxlen=MAX_LEN)","dbd4ef30":"sequences = tokenizer.texts_to_sequences(val_df['text'])\nX_valid = pad_sequences(sequences, maxlen=MAX_LEN)","4c94634b":"model_aug = Sequential()\nmodel_aug.add(Embedding(VOCAB_SIZE, 100, input_length=MAX_LEN))\nmodel_aug.add(Dropout(rate=0.5))\nmodel_aug.add(Conv1D(64, 5, activation='relu'))\nmodel_aug.add(MaxPooling1D(pool_size=2))\nmodel_aug.add(LSTM(100))\nmodel_aug.add(Dense(NUM_CLASSES, activation='softmax'))","e3f606bb":"model_aug.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","570385b6":"model_aug.fit(X_train_aug, y_train_aug,\n          batch_size=1024, epochs=3,\n          verbose=1, validation_split=0.1)","6d70886b":"score = model_aug.evaluate(X_valid, y_valid, batch_size=256, verbose=1)\nprint('Validation accuracy:', score[1])","fbd2ae5b":"val_preds = model_aug.predict(X_valid)","993e0127":"print(classification_report(np.argmax(y_valid,axis=1), np.argmax(val_preds, axis=1)))","059ba5aa":"f1_score(np.argmax(y_valid,axis=1), np.argmax(val_preds, axis=1), average='micro')","25942831":"## Augmentation"}}