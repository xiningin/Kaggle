{"cell_type":{"34923573":"code","8a2dc3d5":"code","b78eed71":"code","b619ecb5":"code","8b693f75":"code","205ea3cb":"code","da5324fa":"code","c344c17f":"markdown","c7216b2b":"markdown","b4f813ad":"markdown","980b2513":"markdown","18d1a0cb":"markdown","c3b5269f":"markdown"},"source":{"34923573":"\n{\n   \"schemaVersion\": 2,\n   \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\n   \"config\": {\n      \"mediaType\": \"application\/vnd.docker.container.image.v1+json\",\n      \"size\": 19290,\n      \"digest\": \"sha256:80df1c851fa3bf3afead5d3c6dad3cc2ec8626b7dcdced8dd305a159f80ffe8d\"\n   },\n   \"layers\": [\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 26692096,\n         \"digest\": \"sha256:423ae2b273f4c17ceee9e8482fa8d071d90c7d052ae208e1fe4963fceb3d6954\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 35365,\n         \"digest\": \"sha256:de83a2304fa1f7c4a13708a0d15b9704f5945c2be5cbb2b3ed9b2ccb718d0b3d\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 852,\n         \"digest\": \"sha256:f9a83bce3af0648efaa60b9bb28225b09136d2d35d0bed25ac764297076dec1b\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 163,\n         \"digest\": \"sha256:b6b53be908de2c0c78070fff0a9f04835211b3156c4e73785747af365e71a0d7\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 403170736,\n         \"digest\": \"sha256:5650063cfbfb957d6cfca383efa7ad6618337abcd6d99b247d546f94e2ffb7a9\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 81117097,\n         \"digest\": \"sha256:89142850430d0d812f21f8bfef65dcfb42efe2cd2f265b46b73f41fa65bef2fe\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 6868,\n         \"digest\": \"sha256:498b10157bcd37c3d4d641c370263e7cf0face8df82130ac1185ef6b2f532470\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 144376365,\n         \"digest\": \"sha256:a77a3b1caf74cc7c9fb700cab353313f1b95db5299642f82e56597accb419d7c\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1551901872,\n         \"digest\": \"sha256:0603289dda032b5119a43618c40948658a13e954f7fd7839c42f78fd0a2b9e44\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 467065,\n         \"digest\": \"sha256:c3ae245b40c1493b89caa2f5e444da5c0b6f225753c09ddc092252bf58e84264\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 324,\n         \"digest\": \"sha256:67e85692af8b802b6110c0a039f582f07db8ac6efc23227e54481f690f1afaae\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 450,\n         \"digest\": \"sha256:ea72ab3b716788097885d2d537d1d17c9dc6d9911e01699389fa8c9aa6cac861\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 197,\n         \"digest\": \"sha256:b02850f0d90ca01b50bbfb779bcf368507c266fc10cc1feeac87c926e9dda2c1\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 198,\n         \"digest\": \"sha256:4295de6959cedecdd0ba31406e15c19e38c13c0ebc38f3d6385725501063ef46\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 213,\n         \"digest\": \"sha256:d651a7c122d62d2869af2a5330c756f2f4b35a8e44902174be5c8ce1ad105edd\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 213,\n         \"digest\": \"sha256:69e0b993e5f56695ee76b3776275dac236d38d32ba1f380fd78b900232e006ec\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 21081464,\n         \"digest\": \"sha256:76eb6a01fbbe8e12713bc5504ad1f0f52391955c553591b059f27b2f6e79024c\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 253,\n         \"digest\": \"sha256:6cfb0579f73a0c86edd04c296ec05bb34073a4d85cba73324a5f1b9f60136776\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 267,\n         \"digest\": \"sha256:bb07f87b0d491b8aeb4147b88fd88148e7844072c48218ca6cacb2b938f16b9f\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1384,\n         \"digest\": \"sha256:19a3313fd31b32db4db167d9d279ff8e7a55b265cc7c31e1fc0acb03a3ba4128\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 384,\n         \"digest\": \"sha256:179f31a0b43127df65cc699581db2c578dfd3abc6b323521908ceade6ee9bc5e\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1926051283,\n         \"digest\": \"sha256:88c3f77a753ce1980969cf78cc79e130349e9be8b2bfabc5bdc76ddeae7e53a7\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 28959,\n         \"digest\": \"sha256:fb34744a245ce33bf571e7028c90c96a82c6ce082636a887bae5170de24a9c5a\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 111211777,\n         \"digest\": \"sha256:f123d9cc3bd9624cc12081283686bad71c5ee5eedce5cf8e484ae139ddbe6ef5\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 593961338,\n         \"digest\": \"sha256:3e847c367f901109e9a1913fc1c5426100526d56185d8608d31d157cdafa9d5c\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 623855132,\n         \"digest\": \"sha256:bbd6ae0789b6fb5e2783ca0474b547f111b1ad2c3a9ad72abdf3834f8f66716f\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 235210915,\n         \"digest\": \"sha256:5d5081cd74410bfe2ab5044dff743985d9da90632bdfe332edb3c99cae68ad63\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 10216,\n         \"digest\": \"sha256:68a17dee703fb005341aa2d50352d5be134e7b37a964e67f48905cc90a2fb243\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 145100843,\n         \"digest\": \"sha256:2c82d12950456bb0b9d46463b99f0f22f109ec83af2d38c1f80139123c67b01c\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 14110,\n         \"digest\": \"sha256:ad94a4adbd4de9849d7a716e7c46664ee5b8bd439cdfa5d831660b7ae3af3503\"\n      }\n   ]\n}\n","8a2dc3d5":"\n{\n   \"schemaVersion\": 2,\n   \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\n   \"config\": {\n      \"mediaType\": \"application\/vnd.docker.container.image.v1+json\",\n      \"size\": 31231,\n      \"digest\": \"sha256:79f52292b1d0c079b84bc79d002947be409a09b15a1320355a4de834f57b2ee8\"\n   },\n   \"layers\": [\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 45339314,\n         \"digest\": \"sha256:c5e155d5a1d130a7f8a3e24cee0d9e1349bff13f90ec6a941478e558fde53c14\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 95104141,\n         \"digest\": \"sha256:86534c0d13b7196a49d52a65548f524b744d48ccaf89454659637bee4811d312\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1571501372,\n         \"digest\": \"sha256:5764e90b1fae3f6050c1b56958da5e94c0d0c2a5211955f579958fcbe6a679fd\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1083072,\n         \"digest\": \"sha256:ba67f7304613606a1d577e2fc5b1e6bb14b764bcc8d07021779173bcc6a8d4b6\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 526,\n         \"digest\": \"sha256:19abed793cf0a9952e1a08188dbe2627ed25836757d0e0e3150d5c8328562b4e\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 458,\n         \"digest\": \"sha256:df204f1f292ae58e4c4141a950fad3aa190d87ed9cc3d364ca6aa1e7e0b73e45\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 13119161,\n         \"digest\": \"sha256:1f7809135d9076fb9ed8ee186e93e3352c861489e0e80804f79b2b5634b456dd\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 555884253,\n         \"digest\": \"sha256:03a365d6218dbe33f5b17d305f5e25e412f7b83b38394c5818bde053a542f11b\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 102870915,\n         \"digest\": \"sha256:00e3d0b7af78551716541d2076836df5594948d5d98f04f382158ef26eb7c907\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 95925388,\n         \"digest\": \"sha256:59782fefadba835c1e83cecdd73dc8e81121eae05ba58d3628a44a1c607feb6e\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 142481172,\n         \"digest\": \"sha256:f81b01cf2c3f02e153a71704cc5ffe6102757fb7c2fcafc107a64581b0f6dc10\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1128076783,\n         \"digest\": \"sha256:f08bbb5c2bce948f0d12eea15c88aad45cdd5b804b71bee5a2cfdbf53c7ec254\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 444800302,\n         \"digest\": \"sha256:b831800c60a36c21033cb6e85f0bd3a5f5c9d96b2fa2797d0e8d4c50598180b8\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 157365696,\n         \"digest\": \"sha256:6d354ec67fa4ccf30460efadef27d48edf9599348cbab789c388f1d3a7fee232\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 63237273,\n         \"digest\": \"sha256:464f9b4eca5cdf36756cf0bef8c76b23344d0e975667becb743e8d6b9019c3cd\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 427820542,\n         \"digest\": \"sha256:6c1f6bcbc63b982a86dc94301c2996505cec571938c60a434b3de196498c7b89\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 44581800,\n         \"digest\": \"sha256:c0a8110c6fede3cf54fa00a1b4e2fcb136d00b3cf490f658ec6d596e313c986e\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 127637178,\n         \"digest\": \"sha256:c25df885c8dea40780f5b479bb6c7be924763399a21fa46c204d5bfac45056bd\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 956429221,\n         \"digest\": \"sha256:7c1d98590e22f78a1b820f89b6ce245321437639957264e628b4abf4862e1223\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 586276809,\n         \"digest\": \"sha256:aab720d802b7d006b679ac10df4a259b3556812dea4dfc52d6111db47fc41e62\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 21717560,\n         \"digest\": \"sha256:5ee4a4cda8613a3fb172a827143aadacb98128479a22a280495604f989bf4483\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 93512644,\n         \"digest\": \"sha256:c4699852e987bc3fe9adde2544ffa690ad52ebec229c20f7e4153b015ac238ff\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 19141,\n         \"digest\": \"sha256:8d93692c8dcecacb8aca746a868f53d0b0cf1207e08ced8ffb2134bb01c1f871\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 84125618,\n         \"digest\": \"sha256:57c74d175611802a57531be97d19f586dc9cd810a5490eab04fd40b648312ead\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 3261,\n         \"digest\": \"sha256:1ac7a265bf03308e06e9cad7e77d12b22ca8bc6b7791d46398d95977e0042574\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 2162,\n         \"digest\": \"sha256:1b4a5be69a4439f3de72877b7d408400e3aa0b4c37e9c70c4490b480bce682c0\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1270,\n         \"digest\": \"sha256:648046d6f6c28a42a39c9e68a9da90ccdabbd1ecfd0be77941114df4eb2406a4\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 644,\n         \"digest\": \"sha256:19a794f6956d460edfe74d5562d44366a7cf8bd46d83f408f1bf3c46e7282464\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 2052,\n         \"digest\": \"sha256:880f92e310c2e03c28c5db85b342069b1a56cd13de7998ae52f699829774f075\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 875,\n         \"digest\": \"sha256:cad389727d6cd1696ed7e91b70eedd4c86fd30babb648e7be6cc1639582b0928\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 214,\n         \"digest\": \"sha256:c873da9a657a590abeae80bd3c0d0d87a6bfdfaf1d3873a0f210760a4050d6db\"\n      }\n   ]\n}\n","b78eed71":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","b619ecb5":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8b693f75":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","205ea3cb":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","da5324fa":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n\n     \nRUN BASE_TAG=scatter\nARG TENSORFLOW_VERSION=2.1.0\n\nFROM gcr.io\/kaggle-images\/python-tensorflow-whl:{TENSORFLOW_VERSION}-py37 as tensorflow_whl\nFROM gcr.io\/deeplearning-platform-release\/base-cpu:{BASE_TAG}\n\nADD clean-layer.sh  \/tmp\/clean-layer.sh\nADD patches\/nbconvert-extensions.tpl \/opt\/kaggle\/nbconvert-extensions.tpl\n\n# This is necessary for apt to access HTTPS sources\nRUN apt-get update && \\\n    apt-get install apt-transport-https && \\\n    \/tmp\/clean-layer.sh\n\n    # Use a fixed apt-get repo to stop intermittent failures due to flaky httpredir connections,\n    # as described by Lionel Chan at http:\/\/stackoverflow.com\/a\/37426929\/5881346\nRUN sed -i \"s\/httpredir.debian.org\/debian.uchicago.edu\/\" \/etc\/apt\/sources.list && \\\n    apt-get update && \\\n    # Needed by vowpalwabbit & lightGBM (GPU build).\n    # https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki\/Python#installing\n    # https:\/\/lightgbm.readthedocs.io\/en\/latest\/GPU-Tutorial.html#build-lightgbm\n    apt-get install -y build-essential unzip cmake && \\\n    apt-get install -y libboost-dev libboost-program-options-dev libboost-system-dev libboost-thread-dev libboost-math-dev libboost-test-dev libboost-python-dev libboost-filesystem-dev zlib1g-dev && \\\n    pip install --upgrade pip && \\\n    # enum34 is a backport of the Python 3.4 enum class to Python < 3.4.\n    # No need since we are using Python 3.7. This is causing errors for packages\n    # expecting the 3.7 version of enum. e.g. AttributeError: module 'enum' has no attribute 'IntFlag'\n    pip uninstall -y enum34 && \\\n    \/tmp\/clean-layer.sh\n\n# Make sure the dynamic linker finds the right libstdc++\nENV LD_LIBRARY_PATH=\/opt\/conda\/lib\n# b\/128333086: Set PROJ_LIB to points to the proj4 cartographic library.\nENV PROJ_LIB=\/opt\/conda\/share\/proj\n\n# Install conda packages not available on pip.\n# When using pip in a conda environment, conda commands should be ran first and then\n# the remaining pip commands: https:\/\/www.anaconda.com\/using-pip-in-a-conda-environment\/\nRUN conda install -c conda-forge matplotlib basemap cartopy python-igraph imagemagick pysal && \\\n    # b\/142337634#comment22 pin required to avoid torchaudio downgrade.\n    conda install -c pytorch pytorch torchvision \"torchaudio>=0.4.0\" cpuonly && \\\n    \/tmp\/clean-layer.sh\n\n# The anaconda base image includes outdated versions of these packages. Update them to include the latest version.\n# b\/150498764 distributed 2.11.0 fails at import while trying to reach out to 8.8.8.8 since the network is disabled in our hermetic tests.\nRUN pip install distributed==2.10.0 && \\\n    pip install seaborn python-dateutil dask && \\\n    pip install pyyaml joblib pytagcloud husl geopy ml_metrics mne pyshp && \\\n    pip install pandas && \\\n    # Install h2o from source.\n    # Use `conda install -c h2oai h2o` once Python 3.7 version is released to conda.\n    apt-get install -y default-jre-headless && \\\n    pip install -f https:\/\/h2o-release.s3.amazonaws.com\/h2o\/latest_stable_Py.html h2o && \\\n    \/tmp\/clean-layer.sh\n\n# Install tensorflow from a pre-built wheel\nCOPY --from=tensorflow_whl \/tmp\/tensorflow_cpu\/*.whl \/tmp\/tensorflow_cpu\/\nRUN pip install \/tmp\/tensorflow_cpu\/tensorflow*.whl && \\\n    rm -rf \/tmp\/tensorflow_cpu && \\\n    \/tmp\/clean-layer.sh\n\nRUN apt-get install -y libfreetype6-dev && \\\n    apt-get install -y libglib2.0-0 libxext6 libsm6 libxrender1 libfontconfig1 --fix-missing && \\\n    pip install gensim && \\\n    pip install textblob && \\\n    pip install wordcloud && \\\n    pip install xgboost && \\\n    # Pinned to match GPU version. Update version together.\n    pip install lightgbm==2.3.1 && \\\n    pip install git+git:\/\/github.com\/Lasagne\/Lasagne.git && \\\n    pip install keras && \\\n    pip install flake8 && \\\n    #neon\n    cd \/usr\/local\/src && \\\n    git clone --depth 1 https:\/\/github.com\/NervanaSystems\/neon.git && \\\n    cd neon && pip install . && \\\n    #nolearn\n    pip install nolearn && \\\n    pip install Theano && \\\n    pip install pybrain && \\\n    pip install python-Levenshtein && \\\n    pip install hep_ml && \\\n    # chainer\n    pip install chainer && \\\n    # NLTK Project datasets\n    mkdir -p \/usr\/share\/nltk_data && \\\n    # NLTK Downloader no longer continues smoothly after an error, so we explicitly list\n    # the corpuses that work\n    # \"yes | ...\" answers yes to the retry prompt in case of an error. See b\/133762095.\n    yes | python -m nltk.downloader -d \/usr\/share\/nltk_data abc alpino averaged_perceptron_tagger \\\n    basque_grammars biocreative_ppi bllip_wsj_no_aux \\\n    book_grammars brown brown_tei cess_cat cess_esp chat80 city_database cmudict \\\n    comtrans conll2000 conll2002 conll2007 crubadan dependency_treebank \\\n    europarl_raw floresta gazetteers genesis gutenberg \\\n    ieer inaugural indian jeita kimmo knbc large_grammars lin_thesaurus mac_morpho match \\\n    masc_tagged maxent_ne_chunker maxent_treebank_pos_tagger moses_sample movie_reviews \\\n    mte_teip5 names nps_chat omw opinion_lexicon paradigms \\\n    pil pl196x porter_test ppattach problem_reports product_reviews_1 product_reviews_2 propbank \\\n    pros_cons ptb punkt qc reuters rslp rte sample_grammars semcor senseval sentence_polarity \\\n    sentiwordnet shakespeare sinica_treebank smultron snowball_data spanish_grammars \\\n    state_union stopwords subjectivity swadesh switchboard tagsets timit toolbox treebank \\\n    twitter_samples udhr2 udhr unicode_samples universal_tagset universal_treebanks_v20 \\\n    vader_lexicon verbnet webtext word2vec_sample wordnet wordnet_ic words ycoe && \\\n    # Stop-words\n    pip install stop-words && \\\n    pip install scikit-image && \\\n    \/tmp\/clean-layer.sh\n\nRUN pip install ibis-framework && \\\n    pip install mxnet && \\\n    pip install gluonnlp && \\\n    pip install gluoncv && \\    \n    \/tmp\/clean-layer.sh\n\n# scikit-learn dependencies\nRUN pip install scipy && \\\n    pip install scikit-learn && \\\n    # HDF5 support\n    pip install h5py && \\\n    pip install biopython && \\\n    # PUDB, for local debugging convenience\n    pip install pudb && \\\n    pip install imbalanced-learn && \\\n    # Convex Optimization library\n    # Latest version fails to install, see https:\/\/github.com\/cvxopt\/cvxopt\/issues\/77\n    #    and https:\/\/github.com\/cvxopt\/cvxopt\/issues\/80\n    # pip install cvxopt && \\\n    # Profiling and other utilities\n    pip install line_profiler && \\\n    pip install orderedmultidict && \\\n    pip install smhasher && \\\n    pip install bokeh && \\\n    pip install numba && \\\n    pip install datashader && \\\n    # Boruta (python implementation)\n    pip install Boruta && \\\n    apt-get install -y graphviz && pip install graphviz && \\\n    # Pandoc is a dependency of deap\n    apt-get install -y pandoc && \\\n    pip install git+git:\/\/github.com\/scikit-learn-contrib\/py-earth.git@issue191 && \\\n    pip install essentia && \\\n    \/tmp\/clean-layer.sh\n\n# vtk with dependencies\nRUN apt-get install -y libgl1-mesa-glx && \\\n    pip install vtk && \\\n    # xvfbwrapper with dependencies\n    apt-get install -y xvfb && \\\n    pip install xvfbwrapper && \\\n    \/tmp\/clean-layer.sh\n\nRUN pip install mpld3 && \\\n    pip install mplleaflet && \\\n    pip install gpxpy && \\\n    pip install arrow && \\\n    pip install nilearn && \\\n    pip install nibabel && \\\n    pip install pronouncing && \\\n    pip install markovify && \\\n    pip install imgaug && \\\n    pip install preprocessing && \\\n    pip install Baker && \\\n    pip install path.py && \\\n    pip install Geohash && \\\n    # https:\/\/github.com\/vinsci\/geohash\/issues\/4\n    sed -i -- 's\/geohash\/.geohash\/g' \/opt\/conda\/lib\/python3.7\/site-packages\/Geohash\/__init__.py && \\\n    pip install deap && \\\n    pip install tpot && \\\n    pip install scikit-optimize && \\\n    pip install haversine && \\\n    pip install toolz cytoolz && \\\n    pip install sacred && \\\n    pip install plotly && \\\n    pip install hyperopt && \\\n    pip install fitter && \\\n    pip install langid && \\\n    # Delorean. Useful for dealing with datetime\n    pip install delorean && \\\n    pip install trueskill && \\\n    pip install heamy && \\\n    # Useful data exploration libraries (for missing data and generating reports)\n    pip install missingno && \\\n    pip install pandas-profiling && \\\n    pip install s2sphere && \\\n    pip install git+https:\/\/github.com\/fmfn\/BayesianOptimization.git && \\\n    pip install matplotlib-venn && \\\n    pip install pyldavis && \\\n    pip install mlxtend && \\\n    pip install altair && \\\n    pip install pystan && \\\n    pip install ImageHash && \\\n    pip install ecos && \\\n    pip install CVXcanon && \\\n    pip install fancyimpute && \\\n    pip install pymc3 && \\\n    pip install tifffile && \\\n    pip install spectral && \\\n    pip install descartes && \\\n    pip install geojson && \\\n    pip install terminalplot && \\\n    pip install pydicom && \\\n    pip install wavio && \\\n    pip install SimpleITK && \\\n    pip install hmmlearn && \\\n    pip install bayespy && \\\n    pip install gplearn && \\\n    pip install PyAstronomy && \\\n    pip install squarify && \\\n    pip install fuzzywuzzy && \\\n    pip install python-louvain && \\\n    pip install pyexcel-ods && \\\n    pip install sklearn-pandas && \\\n    pip install stemming && \\\n    # b\/148383434 remove pip install for holidays once fbprophet is compatible with latest version of holidays.\n    pip install holidays==0.9.12 && \\\n    pip install fbprophet && \\\n    pip install holoviews && \\\n    # 1.6.2 is not currently supported by the version of matplotlib we are using.\n    # See other comments about why matplotlib is pinned.\n    pip install geoviews==1.6.1 && \\\n    pip install hypertools && \\\n    pip install py_stringsimjoin && \\\n    pip install nibabel && \\\n    pip install mlens && \\\n    pip install scikit-multilearn && \\\n    pip install cleverhans && \\\n    pip install leven && \\\n    pip install catboost && \\\n    # fastFM doesn't support Python 3.7 yet: https:\/\/github.com\/ibayer\/fastFM\/issues\/151\n    # pip install fastFM && \\\n    pip install lightfm && \\\n    pip install folium && \\\n    pip install scikit-plot && \\\n    # dipy requires the optional fury dependency for visualizations.\n    pip install fury dipy && \\\n    # plotnine 0.5 is depending on matplotlib >= 3.0 which is not compatible with basemap.\n    # once basemap support matplotlib, we can unpin this package.\n    pip install plotnine==0.4.0 && \\\n    pip install scikit-surprise && \\\n    pip install pymongo && \\\n    pip install geoplot && \\\n    pip install eli5 && \\\n    pip install implicit && \\\n    pip install dask-ml[xgboost] && \\\n    \/tmp\/clean-layer.sh\n\nRUN pip install kmeans-smote --no-dependencies && \\\n    # Add google PAIR-code Facets\n    cd \/opt\/ && git clone https:\/\/github.com\/PAIR-code\/facets && cd facets\/ && jupyter nbextension install facets-dist\/ --user && \\\n    export PYTHONPATH=PYTHONPATH:\/opt\/facets\/facets_overview\/python\/ && \\\n    pip install tensorpack && \\\n    pip install pycountry && pip install iso3166 && \\\n    pip install pydash && \\\n    pip install kmodes --no-dependencies && \\\n    pip install librosa && \\\n    pip install polyglot && \\\n    pip install mmh3 && \\\n    pip install fbpca && \\\n    pip install sentencepiece && \\\n    pip install cufflinks && \\\n    pip install lime && \\\n    pip install memory_profiler && \\\n    \/tmp\/clean-layer.sh\n\n# install cython & cysignals before pyfasttext\nRUN pip install --upgrade cython && \\\n    pip install --upgrade cysignals && \\\n    pip install pyfasttext && \\\n    # ktext has an explicit dependency on Keras 2.2.4 which is not\n    # compatible with TensorFlow 2.0 (support was added in Keras 2.3.0).\n    # Add the package back once it is fixed upstream.\n    # pip install ktext && \\\n    pip install fasttext && \\\n    apt-get install -y libhunspell-dev && pip install hunspell && \\\n    pip install annoy && \\\n    # Need to use CountEncoder from category_encoders before it's officially released\n    pip install git+https:\/\/github.com\/scikit-learn-contrib\/categorical-encoding.git && \\\n    pip install google-cloud-automl && \\\n    # Newer version crashes (latest = 1.14.0) when running tensorflow.\n    # python -c \"from google.cloud import bigquery; import tensorflow\". This flow is common because bigquery is imported in kaggle_gcp.py\n    # which is loaded at startup.\n    pip install google-cloud-bigquery==1.12.1 && \\\n    pip install google-cloud-storage && \\\n    pip install ortools && \\\n    pip install scattertext && \\\n    # Pandas data reader\n    pip install pandas-datareader && \\\n    pip install wordsegment && \\\n    pip install pyahocorasick && \\\n    pip install wordbatch && \\\n    pip install emoji && \\\n    # Add Japanese morphological analysis engine\n    pip install janome && \\\n    pip install wfdb && \\\n    pip install vecstack && \\\n    # Doesn't support Python 3.7 yet. Last release on pypi is from 2017.\n    # Add back once this PR is released: https:\/\/github.com\/scikit-learn-contrib\/lightning\/pull\/133\n    # pip install sklearn-contrib-lightning && \\\n    # yellowbrick machine learning visualization library\n    pip install yellowbrick && \\\n    pip install mlcrate && \\\n    \/tmp\/clean-layer.sh\n\nRUN pip install bcolz && \\\n    pip install bleach && \\\n    pip install certifi && \\\n    pip install cycler && \\\n    pip install decorator && \\\n    pip install entrypoints && \\\n    pip install html5lib && \\\n    # Latest version breaks nbconvert: https:\/\/github.com\/ipython\/ipykernel\/issues\/422\n    pip install ipykernel==5.1.1 && \\\n    pip install ipython && \\\n    pip install ipython-genutils && \\\n    pip install ipywidgets && \\\n    pip install isoweek && \\\n    pip install jedi && \\\n    pip install Jinja2 && \\\n    pip install jsonschema && \\\n    pip install jupyter && \\\n    pip install jupyter-client && \\\n    pip install jupyter-console && \\\n    pip install jupyter-core && \\\n    pip install MarkupSafe && \\\n    pip install mistune && \\\n    pip install nbconvert && \\\n    pip install nbformat && \\\n    pip install notebook==5.5.0 && \\\n    pip install olefile && \\\n    pip install opencv-python && \\\n    pip install pandas_summary && \\\n    pip install pandocfilters && \\\n    pip install pexpect && \\\n    pip install pickleshare && \\\n    pip install Pillow && \\\n    # Install openslide and its python binding\n    apt-get install -y openslide-tools && \\\n    # b\/152402322 install latest from pip once is in: https:\/\/github.com\/openslide\/openslide-python\/pull\/76\n    pip install git+git:\/\/github.com\/rosbo\/openslide-python.git@fix-setup && \\\n    pip install ptyprocess && \\\n    pip install Pygments && \\\n    pip install pyparsing && \\\n    pip install pytz && \\\n    pip install PyYAML && \\\n    pip install pyzmq && \\\n    pip install qtconsole && \\\n    pip install six && \\\n    pip install terminado && \\\n    # Latest version (6.0) of tornado breaks Jupyter notebook:\n    # https:\/\/github.com\/jupyter\/notebook\/issues\/4439\n    pip install tornado==5.0.2 && \\\n    pip install tqdm && \\\n    pip install traitlets && \\\n    pip install wcwidth && \\\n    pip install webencodings && \\\n    pip install widgetsnbextension && \\\n    pip install pyarrow && \\\n    pip install feather-format && \\\n    pip install fastai && \\\n    pip install torchtext && \\\n    pip install allennlp && \\\n    # b\/149359379 remove once allennlp 1.0 is released which won't cause a spacy downgrade.\n    pip install spacy==2.2.3 && python -m spacy download en && python -m spacy download en_core_web_lg && \\\n    \/tmp\/clean-layer.sh\n\n    ###########\n    #\n    #      NEW CONTRIBUTORS:\n    # Please add new pip\/apt installs in this block. Don't forget a \"&& \\\" at the end\n    # of all non-final lines. Thanks!\n    #\n    ###########\n\nRUN pip install flashtext && \\\n    pip install wandb && \\\n    pip install marisa-trie && \\\n    pip install pyemd && \\\n    pip install pyupset && \\\n    pip install pympler && \\\n    pip install s3fs && \\\n    pip install featuretools && \\\n    pip install -e git+https:\/\/github.com\/SohierDane\/BigQuery_Helper#egg=bq_helper && \\\n    pip install hpsklearn && \\\n    pip install git+https:\/\/github.com\/Kaggle\/learntools && \\\n    pip install kmapper && \\\n    pip install shap && \\\n    pip install ray && \\\n    pip install gym && \\\n    pip install tensorforce && \\\n    pip install pyarabic && \\\n    pip install conx && \\\n    pip install pandasql && \\\n    pip install tensorflow_hub && \\\n    pip install jieba  && \\\n    pip install git+https:\/\/github.com\/SauceCat\/PDPbox && \\\n    pip install ggplot && \\\n    pip install cesium && \\\n    pip install rgf_python && \\\n    # b\/145404107: latest version force specific version of numpy and torch.\n    pip install pytext-nlp==0.1.2 && \\\n    pip install tsfresh && \\\n    pip install pykalman && \\\n    pip install optuna && \\\n    pip install chainercv && \\\n    pip install chainer-chemistry && \\\n    pip install plotly_express && \\\n    pip install albumentations && \\\n    pip install catalyst && \\\n    # b\/145133331: latest version is causing issue with gcloud.\n    pip install rtree==0.8.3 && \\\n    # b\/145133331 osmnx 0.11 requires rtree >= 0.9 which is causing issue with gcloud.\n    pip install osmnx==0.10 && \\\n    apt-get -y install libspatialindex-dev && \\\n    pip install pytorch-ignite && \\\n    pip install qgrid && \\\n    pip install bqplot && \\\n    pip install earthengine-api && \\\n    pip install transformers && \\\n    pip install dlib && \\\n    pip install kaggle-environments && \\\n    # b\/149905611 The geopandas tests are broken with the version 0.7.0\n    pip install geopandas==0.6.3 && \\\n    pip install nnabla && \\\n    pip install vowpalwabbit && \\\n    \/tmp\/clean-layer.sh\n\n# Tesseract and some associated utility packages\nRUN apt-get install tesseract-ocr -y && \\\n    pip install pytesseract && \\\n    pip install wand==0.5.3 && \\\n    pip install pdf2image && \\\n    pip install PyPDF && \\\n    pip install pyocr && \\\n    \/tmp\/clean-layer.sh\nENV TESSERACT_PATH=\/usr\/bin\/tesseract\n\n# For Facets\nENV PYTHONPATH=PYTHONPATH:\/opt\/facets\/facets_overview\/python\/\n# For Theano with MKL\nENV MKL_THREADING_LAYER=GNU\n\n# Temporary fixes and patches\n    # Temporary patch for Dask getting downgraded, which breaks Keras\nRUN pip install --upgrade dask && \\\n    # Stop jupyter nbconvert trying to rewrite its folder hierarchy\n    mkdir -p \/root\/.jupyter && touch \/root\/.jupyter\/jupyter_nbconvert_config.py && touch \/root\/.jupyter\/migrated && \\\n    mkdir -p \/.jupyter && touch \/.jupyter\/jupyter_nbconvert_config.py && touch \/.jupyter\/migrated && \\\n    # Stop Matplotlib printing junk to the console on first load\n    sed -i \"s\/^.*Matplotlib is building the font cache using fc-list.*$\/# Warning removed by Kaggle\/g\" \/opt\/conda\/lib\/python3.7\/site-packages\/matplotlib\/font_manager.py && \\\n    # Make matplotlib output in Jupyter notebooks display correctly\n    mkdir -p \/etc\/ipython\/ && echo \"c = get_config(); c.IPKernelApp.matplotlib = 'inline'\" > \/etc\/ipython\/ipython_config.py && \\\n    \/tmp\/clean-layer.sh\n\n# gcloud SDK https:\/\/cloud.google.com\/sdk\/docs\/quickstart-debian-ubuntu\nRUN echo \"deb [signed-by=\/usr\/share\/keyrings\/cloud.google.gpg] http:\/\/packages.cloud.google.com\/apt cloud-sdk main\" \\\n    | tee -a \/etc\/apt\/sources.list.d\/google-cloud-sdk.list && \\\n    curl https:\/\/packages.cloud.google.com\/apt\/doc\/apt-key.gpg | \\\n    apt-key --keyring \/usr\/share\/keyrings\/cloud.google.gpg add - && \\\n    apt-get update -y && apt-get install google-cloud-sdk -y && \\\n    \/tmp\/clean-layer.sh\n\n# Add BigQuery client proxy settings\nENV PYTHONUSERBASE \"\/root\/.local\"\nADD patches\/kaggle_gcp.py \/root\/.local\/lib\/python3.7\/site-packages\/kaggle_gcp.py\nADD patches\/kaggle_secrets.py \/root\/.local\/lib\/python3.7\/site-packages\/kaggle_secrets.py\nADD patches\/kaggle_web_client.py \/root\/.local\/lib\/python3.7\/site-packages\/kaggle_web_client.py\nADD patches\/kaggle_datasets.py \/root\/.local\/lib\/python3.7\/site-packages\/kaggle_datasets.py\nADD patches\/log.py \/root\/.local\/lib\/python3.7\/site-packages\/log.py\nADD patches\/sitecustomize.py \/root\/.local\/lib\/python3.7\/site-packages\/sitecustomize.py\n# Override default imagemagick policies\nADD patches\/imagemagick-policy.xml \/etc\/ImageMagick-6\/policy.xml\n\n# TensorBoard Jupyter extension. Should be replaced with TensorBoard's provided magic once we have\n# worker tunneling support in place.\n# b\/139212522 re-enable TensorBoard once solution for slowdown is implemented.\n# ENV JUPYTER_CONFIG_DIR \"\/root\/.jupyter\/\"\n# RUN pip install jupyter_tensorboard && \\\n#     jupyter serverextension enable jupyter_tensorboard && \\\n#     jupyter tensorboard enable\n# ADD patches\/tensorboard\/notebook.py \/opt\/conda\/lib\/python3.7\/site-packages\/tensorboard\/notebook.py\n\n# Set backend for matplotlib\nENV MPLBACKEND \"agg\"\n\n# We need to redefine TENSORFLOW_VERSION here to get the default ARG value defined above the FROM instruction.\n# See: https:\/\/docs.docker.com\/engine\/reference\/builder\/#understand-how-arg-and-from-interact\nARG TENSORFLOW_VERSION\nARG GIT_COMMIT=unknown\nARG BUILD_DATE=unknown\n\nLABEL git-commit=GIT_COMMIT\nLABEL build-date=BUILD_DATE\nLABEL tensorflow-version=TENSORFLOW_VERSION\n\n# Correlate current release with the git hash inside the kernel editor by running `!cat \/etc\/git_commit`.\nRUN echo \"$GIT_COMMIT\" > \/etc\/git_commit && echo \"$BUILD_DATE\" > \/etc\/build_date","c344c17f":"There is 0 csv file in the current version of the dataset:\n","c7216b2b":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","b4f813ad":"## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)","980b2513":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing.","18d1a0cb":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","c3b5269f":"Oh, no! There are no automatic insights available for the file types used in this dataset. As your Kaggle kerneler bot, I'll keep working to fine-tune my hyper-parameters. In the meantime, please feel free to try a different dataset."}}