{"cell_type":{"4197f050":"code","64d85c8e":"code","c41722b1":"code","691d4a9d":"code","07d78c20":"code","d44e851a":"code","63d70c25":"code","a9eb1bde":"code","064828d9":"code","90e68d8c":"code","9923ef40":"code","c9bd32e4":"code","8eea8497":"code","9f6ce484":"code","3c3ac1f3":"code","59f3bac3":"code","586e1592":"code","eeea2a0e":"code","61fbe380":"code","8ee9fa79":"code","e96cdbb2":"code","43ba2586":"code","f7f8b134":"code","73b5c230":"code","24efdc65":"code","2e3c3173":"code","9df8dc6c":"markdown","ef7fa2b9":"markdown","162107b6":"markdown","cf64c196":"markdown","f2ce50df":"markdown","caac4c59":"markdown","6476bba1":"markdown","b1a6fba7":"markdown","074ff375":"markdown"},"source":{"4197f050":"!ls ..\/input\/sarcasm\/","64d85c8e":"# some necessary imports\nimport os\nimport string\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, naive_bayes,linear_model, metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\ncolor = sns.color_palette()\n\nfrom matplotlib import pyplot as plt\nfrom plotly import tools\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)","c41722b1":"train_df = pd.read_csv('..\/input\/sarcasm\/train-balanced-sarcasm.csv')","691d4a9d":"train_df.head()","07d78c20":"train_df.info()","d44e851a":"train_df.dropna(subset=['comment'], inplace=True)","63d70c25":"train_df.info()","a9eb1bde":"train_df['label'].value_counts()","064828d9":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17, train_size=.7)","90e68d8c":"#cm = plt.cm.get_cmap('RdYlBu_r')\n\n#n, bins, patches = plt.hist(train_df['label'], density = True)\n#    then normalize\n#col = (n - n.min())\/(n.max() - n.min())\n#print(col)\n#for c, p in zip(col, patches):\n #   plt.setp(p, 'facecolor', cm(c))\ntrgt_count = train_df['label'].value_counts()\n\nlabels = '0', '1'\nsizes = np.array(trgt_count \/ trgt_count.sum() * 100)\n#_, axes = plt.subplots(1, 2, sharey = True, figsize=(12, 8))    \nsns.countplot(x ='label', data = train_df)\n\n   ## target distribution ##\nplt.pie(sizes, labels = labels)","9923ef40":"#target count#\ntrgt_counts = train_df['label'].value_counts()\ntrace = go.Bar(\n    x=trgt_counts.index, \n    y = trgt_counts.values,\n    marker=dict(\n        color=trgt_counts.values,\n        colorscale='Picnic',\n        reversescale=True\n    ),\n)\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18),\n    width = 400, \n    height =500,\n)\ndata=[trace]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig,filename='TargetCount')\n\n#target distribution#\n\nlabels = np.array(trgt_counts.index)\nsizes = np.array(trgt_counts \/trgt_counts.sum() * 100)\n\ntrace = go.Pie(labels=labels,values=sizes)\nlayout = go.Layout(\n    title='Target Distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata=[trace]\nfig=go.Figure(data=data,layout=layout)\npy.iplot(fig,filename='shit')","c9bd32e4":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width = 800, \n                    height = 400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize = figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func = image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_texts, title='Word Cloud of Comments')","8eea8497":"from collections import defaultdict\n\ntrain1_df = train_texts[y_train == 1]\ntrain0_df = train_texts[y_train == 0]\n\n## let's generate some ngrams ##\ndef generate_ngrams(text, n_gram = 1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\n# custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y = df['word'].values[::-1],\n        x = df['wordcount'].values[::-1],\n        showlegend=False,\n        orientation='h',\n        marker=dict(color=color),\n    )\n    return trace\n\n# Get the bar chart for non-sarcasm comments #\nfreq_dict = defaultdict(int)\nfor sent in train0_df:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns = ['word','wordcount']\ntrace0 = horizontal_bar_chart(fd_sorted.head(50),'blue')\n\n# Get the bar chart for sarcasm comments #\nfreq_dict = defaultdict(int)\nfor sent in train1_df:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns = ['word','wordcount']\ntrace1 = horizontal_bar_chart(fd_sorted.head(50),'red')\n\n#create two subplots\nfig = tools.make_subplots(rows=1,cols=2,vertical_spacing=0.04,\n                         subplot_titles=['Frequent words of non-sarcasm comments',\n                                        'Frequent words of sarcasm comments'])\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\nfig['layout'].update(height=1200, width=900,paper_bgcolor='rgb(233,233,233)',title='Word count sarcasm plots')\npy.iplot(fig, filename='Word_count_plots')","9f6ce484":"freq_dict = defaultdict(int)\nfor sent in train0_df:\n    for word in generate_ngrams(sent, 2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns=['word','wordcount']\ntrace0=horizontal_bar_chart(fd_sorted.head(50),'yellow')\n\n#Get the bar chart from sarcasm comments ##\nfreq_dict =defaultdict(int)\nfor sent in train1_df:\n    for word in generate_ngrams(sent, 2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns=['word','wordcount']\ntrace1=horizontal_bar_chart(fd_sorted.head(50),'orange')\n\n#create two subplots\nfig = tools.make_subplots(rows=1,cols=2,vertical_spacing=0.04,\n                         subplot_titles=['Frequent bigrams of non-sarcasm comments',\n                                        'Frequent bigrams of sarcasm comments'])\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\nfig['layout'].update(height=1200, width=900,paper_bgcolor='rgb(233,233,233)',title='Bigrams sarcasm plots')\npy.iplot(fig, filename='word_count_plots')","3c3ac1f3":"freq_dict = defaultdict(int)\nfor sent in train0_df:\n    for word in generate_ngrams(sent, 3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns=['word','wordcount']\ntrace0=horizontal_bar_chart(fd_sorted.head(50),'green')\n \n# Get the bar chart from sarcasm comments #\nfreq_dict =defaultdict(int)\nfor sent in train1_df:\n    for word in generate_ngrams(sent, 3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(),key=lambda x:x[1])[::-1])\nfd_sorted.columns = ['word','wordcount']\ntrace1=horizontal_bar_chart(fd_sorted.head(50),'blue')\n\n#create two subplots\nfig = tools.make_subplots(rows=1,cols=2,vertical_spacing=0.04,\n                         subplot_titles=['Frequent trigrams of non-sarcasm comments',\n                                        'Frequent trigrams of sarcasm comments'])\nfig.append_trace(trace0,1,1)\nfig.append_trace(trace1,1,2)\nfig['layout'].update(height=1200, width=1500,paper_bgcolor='rgb(233,233,233)',title='Trigram sarcasm plots')\npy.iplot(fig, filename='word_count_plots')","59f3bac3":"type(train_texts)","586e1592":"train_texts = train_texts.to_frame('comment')","eeea2a0e":"valid_texts=valid_texts.to_frame('comment')","61fbe380":"train_texts['label'] = y_train\nvalid_texts['label'] = y_valid","8ee9fa79":"# creating some extra features for better prediction accuracy\n\ntrain_texts['num_words'] = train_texts['comment'].apply(lambda x: len(str(x).split()))\nvalid_texts['num_words'] = valid_texts['comment'].apply(lambda x: len(str(x).split()))\n\ntrain_texts['num_unique_words'] = train_texts['comment'].apply(lambda x: len(set(str(x).split())))  # for each comment\nvalid_texts['num_unique_words'] = valid_texts['comment'].apply(lambda x: len(set(str(x).split())))\n\ntrain_texts['num_chars'] = train_texts['comment'].apply(lambda x: len(str(x))) # for each comment\nvalid_texts['num_chars'] = valid_texts['comment'].apply(lambda x: len(str(x)))\n\ntrain_texts['num_stopwords'] = train_texts['comment'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\nvalid_texts['num_stopwords'] = train_texts.apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\ntrain_texts['num_punctuations'] = train_texts['comment'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\nvalid_texts['num_punctuations'] = valid_texts['comment'].apply(lambda x: len([p for p in str(x) if p in string.punctuation]))\n\ntrain_texts['num_words_upper'] = train_texts['comment'].apply(lambda x: len([u for u in str(x) if u.isupper()]))\nvalid_texts['num_words_upper'] = valid_texts['comment'].apply(lambda x: len([u for u in str(x) if u.isupper()]))\n\ntrain_texts['num_words_title'] = train_texts['comment'].apply(lambda x: len([t for t in str(x) if t.istitle()]))\nvalid_texts['num_words_title'] = valid_texts['comment'].apply(lambda x: len([t for t in str(x) if t.istitle()]))\n\ntrain_texts['mean_word_len'] = train_texts['comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\nvalid_texts['mean_word_len'] = valid_texts['comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","e96cdbb2":"# Truncate some extreme values for better visuals ##\ntrain_texts['num_words'].loc[train_texts['num_words']>60] = 60\ntrain_texts['num_punctuations'].loc[train_texts['num_punctuations']>10] = 10\ntrain_texts['num_chars'].loc[train_texts['num_chars']>350] = 350\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='label', y='num_words', data=train_texts,ax=axes[0])\naxes[0].set_xlabel('Label', fontsize=12)\naxes[0].set_title('Number of words in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_chars', data=train_texts,ax=axes[1])\naxes[1].set_xlabel('Label', fontsize=12)\naxes[1].set_title('Number of characters in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_punctuations', data=train_texts,ax=axes[2])\naxes[2].set_xlabel('Label', fontsize=12)\naxes[2].set_title('Number of punctuations in each class', fontsize=15)","43ba2586":"# get the tfidf vectors #\ntfidf_vec = TfidfVectorizer(stop_words ='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(train_texts['comment'].values.tolist() + valid_texts['comment'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(train_texts['comment'].values.tolist())\ntest_tfidf = tfidf_vec.transform(valid_texts['comment'].values.tolist())","f7f8b134":"train_tfidf","73b5c230":"train_y = train_texts['label'].values\n\ndef runModel(train_X, train_y, test_X, test_y, test_X2):\n    logreg = linear_model.LogisticRegression(C=5, solver = 'sag')\n    logreg.fit(train_X, train_y)\n    pred_test_y=logreg.predict_proba(test_X)[:,1]\n    pred_test_y2=logreg.predict_proba(test_X2)[:,1]\n    return pred_test_y, pred_test_y2, logreg\n\ncv_scores =[]\npred_full_test=0\npred_train=np.zeros([train_df.shape[0]])\n\nkf=KFold(n_splits=5,shuffle=True,random_state=2020)\nfor dev_index,val_index in kf.split(train_texts):\n    dev_X, val_X = train_tfidf[dev_index],train_tfidf[val_index]\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\n    pred_val_y, pred_test_y, model = runModel(dev_X, dev_y, val_X, val_y, test_tfidf)\n    pred_full_test=pred_full_test+pred_test_y\n    pred_train[val_index]=pred_val_y\n    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n    break","24efdc65":"for thresh in np.arange(0.3, 0.401, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_val_y>thresh).astype(int))))","2e3c3173":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","9df8dc6c":"## Word Frequency plot of sarcasm & non-sarcasm comments:","ef7fa2b9":"## Let's build a model for that:","162107b6":"**Init code**","cf64c196":"## Then get some bigrams:","f2ce50df":"## Now look at the trigrams:","caac4c59":"We notice that the dataset is indeed balanced","6476bba1":"We split data into training and validation parts.","b1a6fba7":"Some comments are missing, so we drop the corresponding rows.","074ff375":"## <center>Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https:\/\/arxiv.org\/abs\/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https:\/\/www.kaggle.com\/danofer\/sarcasm)."}}