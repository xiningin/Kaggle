{"cell_type":{"7bbef32b":"code","654e4466":"code","af6b7a51":"code","ec0197c5":"code","be5195ee":"code","5b081acc":"code","51e2d03a":"code","07883f73":"code","30982a29":"code","bcdea6d1":"code","5f932472":"code","3dac4488":"code","c66c47c2":"code","082fc591":"code","06aef238":"code","e5b22c11":"code","f56f5cd6":"code","f35763e4":"code","7bee7ce1":"code","b5f74a03":"code","9fc3b763":"code","daec21c5":"code","4d8b1a8c":"code","77cd4676":"code","726927bb":"code","917bdc04":"code","2fda1e62":"markdown","be9e9177":"markdown","0a2e8539":"markdown","1d1f51b6":"markdown","b064502e":"markdown","39cfb642":"markdown"},"source":{"7bbef32b":"import pandas as pd\nimport numpy as np\nimport re","654e4466":"# \u0417\u0430\u0432\u0441\u0440\u044b\u043d \u04af\u0433\u043d\u04af\u04af\u0434\nstopwords = ['\u0430\u0430', '\u0430\u0430\u043d\u0445\u0430\u0430', '\u0430\u043b\u0438\u0432', '\u0431\u0430', '\u0431\u0430\u0439\u0433\u0430\u0430', '\u0431\u0430\u0439\u0433\u0430\u0430\u0434', '\u0431\u0430\u0439\u0434\u0430\u0433', '\u0431\u0430\u0439\u0436\u044d\u044d', '\u0431\u0430\u0439\u043d\u0430', '\u0431\u0430\u0439\u0441\u0430\u043d', '\u0431\u0430\u0439\u0445\u0430\u0430', '\u0431\u0430\u0441', '\u0431\u043e\u043b\u043e\u043d', '\u0431\u0438\u0448\u04af\u04af', '\u0431\u0443\u044e\u0443', '\u0431\u0443\u0439', '\u0431\u043e\u043b', '\u0431\u043e\u043b\u0436\u044d\u044d', '\u0431\u043e\u043b\u043d\u043e', '\u0431\u043e\u043b\u043e\u043e', '\u0431\u043e\u043b\u0441\u043e\u043d', '\u0431\u04e9\u0433\u04e9\u04e9\u0434', '\u0431\u04af\u0440', '\u0431\u044d', '\u0432\u044d', '\u0433\u044d\u0433\u0447', '\u0433\u044d\u0433\u0447\u044d\u044d\u0440', '\u0433\u044d\u0436', '\u0433\u044d\u0436\u044d\u044d', '\u0433\u044d\u043b\u0442\u0433\u04af\u0439', '\u0433\u044d\u0432\u044d\u043b', '\u0433\u044d\u0441\u044d\u043d', '\u0433\u044d\u0434\u044d\u0433', '\u0433\u044d\u0442\u044d\u043b', '\u0433\u044d\u0445', '\u0433\u044d\u0445\u044d\u0434', '\u0433\u044d\u044d\u0434', '\u0434\u0430\u0440\u0430\u0430', '\u0434\u044d\u044d', '\u0434\u044d\u044d\u0433\u04af\u04af\u0440', '\u0434\u043e\u043e\u0433\u0443\u0443\u0440', '\u0435\u0440', '\u0437\u0430', '\u0438\u0439\u043d\u0445\u04af\u04af', '\u0438\u0439\u043c', '\u043b', '\u043c\u0430\u0430\u043d\u044c', '\u043c\u04e9\u043d', '\u043c\u0438\u043d\u044c', '\u043c\u044d\u0442\u0438\u0439\u043d', '\u043c\u044d\u0442', '\u043d\u044c', '\u043d\u04e9\u0433\u04e9\u04e9', '\u04e9\u043d\u04e9\u04e9', '\u0442\u0438\u0439\u043c', '\u0442\u044d\u0440', '\u0442\u0443\u043b', '\u0442\u0443\u0445\u0430\u0439\u043d', '\u0443\u0443', '\u0443\u0447\u0438\u0440', '\u0445\u0430\u0440\u0438\u043d', '\u0445\u044d\u043d', '\u0447', '\u0447\u0438\u043d\u044c', '\u0448\u0438\u0433', '\u044d\u043d\u044d', '\u044d\u044d', '\u044e\u043c', '\u044e\u043c\u0430\u0430', '\u04af\u04af', '\u04af\u0435\u0434', '\u04af\u043b', '\u04e9\u04e9', '\u044f\u0430\u0433\u0430\u0430\u0434', '\u044f\u0433']","af6b7a51":"#\u04e8\u0433\u04e9\u0433\u0434\u04e9\u043b \u043e\u0440\u0443\u0443\u043b\u0436 \u0438\u0440\u044d\u0445\ndf = pd.read_csv('..\/input\/muis-challenge\/train.csv')\ndf_t = pd.read_csv('..\/input\/muis-challenge\/test.csv')","ec0197c5":"#\u0413\u043e\u043b \u04af\u0433 \u043c\u0430\u0430\u043d\u044c \u0434\u0430\u0440\u0430\u0430\u0433\u0438\u0439\u043d \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u0440\u0442 \u0445\u043e\u043e\u0440\u043e\u043d\u0434\u044b\u043d \u0437\u0430\u0439\u0433\u04af\u0439 \u0431\u0430\u0439\u0433\u0430\u0430\u0433 \u044f\u043d\u0437\u043b\u0430\u0445 \u0434\u0430\u0432\u0442\u0430\u043b\u0442\nnew_df = pd.DataFrame(columns= ['text_id','text','synset_id'])\npre_word=''\nmain_word='wwwyyy'\nfor index, row in df.iterrows():\n    txt=\"\"\n    if row['text'].find(main_word) == -1:\n        txt = row['text']\n    else:\n        txt = row['text'].rsplit(main_word)[0]+main_word+\" \"+\"\".join(row['text'].split(main_word,1)[1:])\n        \n    text_id = row['text_id']\n    synset_id = row['synset_id']\n    txt = re.sub('[\\\\\\\\<>?\\';:\/!@$_,.\"{}*()]',\"\",txt) \n    txt = re.sub(' +',\" \",txt)\n    \n    new_df=new_df.append({'text_id':text_id,'text':txt,'synset_id':synset_id},ignore_index=True)\n    main_word = row['text'].split('#')[0]","be5195ee":"\ndf_meaning = pd.read_csv('..\/input\/muis-challenge\/synset_meaning.csv')\ndf_joined = pd.merge(left=new_df, right=df_meaning, left_on='synset_id', right_on='synset_id', how='left')\nnew_df2 = pd.DataFrame(columns= ['text_id','text','synset_id','meaning'])\nfor index, row in df_joined.iterrows():\n    txt = row['text']\n    txt=txt.lower()\n    txt = re.sub(' #',\"#\",txt)\n\n    #\u0417\u0430\u0432\u0441\u0440\u044b\u043d \u04af\u0433\u043d\u04af\u04af\u0434\u0438\u0439\u0433 \u0445\u0430\u0441\u0430\u0445\n    txt_words = txt.split()\n    txt_result = [wrd for wrd in txt_words if wrd.lower() not in stopwords]\n    result = ' '.join(txt_result)\n    \n\n    #5 \u04af\u0433 \u0430\u0432\u0430\u0445 \u0445\u044d\u0441\u044d\u0433\n    word = re.findall(r'\\w+#\\w+', result)[0]\n    \n    list_of_words = result.split()\n\n    try:\n        word1 =list_of_words[list_of_words.index(word) -2]\n    except:\n        word1=\"\"    \n    try:    \n        word2 =list_of_words[list_of_words.index(word) -1]\n    except:\n        word2=\"\"\n   \n    try:\n        word4 =list_of_words[list_of_words.index(word) +1]\n    except:\n        word4=\"\"    \n    try:    \n        word5 =list_of_words[list_of_words.index(word) +2]\n    except:\n        word5=\"\"    \n   \n    # \u0447\u0430\u0433\u0442\u0438\u0439\u0433 \u0430\u0440\u0438\u043b\u0433\u0430\u0445\n    t_id = row['text_id']\n    t_id = t_id.replace(\"t\",\"#\")\n\n\n    txt = word1+\" \"+word2+\" xxfld \"+word+\" \"+word4+\" \"+word5\n    \n    txt = re.sub(' +',\" \",txt)\n    \n    txt = txt.rsplit(t_id)[0]+txt.split(t_id)[1]\n    synset_id = row['synset_id']\n    new_df2=new_df2.append({'text_id':row['text_id'],'text':txt,'synset_id':synset_id,'meaning':row['word']+\" \"+str(row['synset_id'])},ignore_index=True)","5b081acc":"#Fastai library is_valid \u0431\u0430\u0433\u0430\u043d\u0430\u0442\u0430\u0439 \u0431\u0430\u0439\u0434\u0430\u0433 \u0442\u0443\u043b \u043d\u044d\u043c\u0436 \u043e\u0440\u0443\u0443\u043b\u0436 \u04e9\u0433\u04e9\u0432\nnew_df2['is_valid']=0\nnew_df2=new_df2.append({'text_id':'t0000025749','text':'\u044a\u044f \u044f\u044f \u044e\u044e','meaning':'\u044d\u0440\u04af\u04af\u043b 69','is_valid':1},ignore_index=True)\nnew_df2.drop(columns=['synset_id'], inplace=True)","51e2d03a":"# \u041c\u04e9\u043d \u0430\u0434\u0438\u043b \u0433\u043e\u043b \u04af\u0433 \u043c\u0430\u0430\u043d\u044c \u0434\u0430\u0440\u0430\u0430\u0433\u0438\u0439\u043d \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u0440\u0442 \u0445\u043e\u043e\u0440\u043e\u043d\u0434\u044b\u043d \u0437\u0430\u0439\u0433\u04af\u0439 \u0431\u0430\u0439\u0433\u0430\u0430\u0433 \u044f\u043d\u0437\u043b\u0430\u0445 \u0434\u0430\u0432\u0442\u0430\u043b\u0442\nnew_df = pd.DataFrame(columns= ['text_id','text'])\npre_word=''\nmain_word='wwwyyy'\nfor index, row in df_t.iterrows():\n    txt=\"\"\n    if row['text'].find(main_word) == -1:\n        txt = row['text']\n    else:\n        txt = row['text'].rsplit(main_word)[0]+main_word+\" \"+\"\".join(row['text'].split(main_word,1)[1:])\n        \n    text_id = row['text_id']\n    \n    txt = re.sub('[\\\\\\\\<>?\\';:\/!@$_,.\"{}*()]',\"\",txt) \n    txt = re.sub(' +',\" \",txt)\n    \n    new_df=new_df.append({'text_id':text_id,'text':txt},ignore_index=True)\n    main_word = row['text'].split('#')[0]","07883f73":"# \u041c\u04e9\u043d \u0430\u0434\u0438\u043b \u04e9\u0433\u04af\u04af\u043b\u0431\u044d\u0440\u044d\u044d\u0441 \u0433\u043e\u043b \u04af\u0433\u0438\u0439\u0433 \u0442\u043e\u0439\u0440\u043e\u043d 4 \u04af\u0433 \u0430\u0432\u0447 \u0433\u043e\u043b \u04af\u0433\u0438\u0439\u043d \u04e9\u043c\u043d\u04e9 TOKEN \u043e\u0440\u0443\u0443\u043b\u0436 \u04e9\u0433\u04e9\u0432\nnew_df_t = pd.DataFrame(columns= ['text_id','text'])\nfor index, row in new_df.iterrows():\n    \n    txt = row['text']\n    txt=txt.lower()\n    txt = re.sub(' #',\"#\",txt)\n\n    #\u0417\u0430\u0432\u0441\u0440\u044b\u043d \u04af\u0433\u043d\u04af\u04af\u0434\u0438\u0439\u0433 \u0445\u0430\u0441\u0430\u0445\n    txt_words = txt.split()\n    txt_result = [wrd for wrd in txt_words if wrd.lower() not in stopwords]\n    result = ' '.join(txt_result)\n\n\n    #5 \u04af\u0433 \u0430\u0432\u0430\u0445 \u0445\u044d\u0441\u044d\u0433\n    word = re.findall(r'\\w+#\\w+', result)[0]\n    list_of_words = result.split()\n\n    try:\n        word1 =list_of_words[list_of_words.index(word) -2]\n    except:\n        word1=\"\"    \n    try:    \n        word2 =list_of_words[list_of_words.index(word) -1]\n    except:\n        word2=\"\"\n   \n    try:\n        word4 =list_of_words[list_of_words.index(word) +1]\n    except:\n        word4=\"\"    \n    try:    \n        word5 =list_of_words[list_of_words.index(word) +2]\n    except:\n        word5=\"\"    \n        \n    \n    txt = word1+\" \"+word2+\" xxfld \"+word+\" \"+word4+\" \"+word5\n    \n    txt = re.sub(' +',\" \",txt)\n\n    # \u0447\u0430\u0433\u0442\u0438\u0439\u0433 \u0430\u0440\u0438\u043b\u0433\u0430\u0445\n    tid = row['text_id']\n    tid = tid.replace(\"t\",\"#\")\n    txt = txt.rsplit(tid)[0]+txt.split(tid)[1]\n\n    \n    new_df_t=new_df_t.append({'text_id':row['text_id'],'text':txt},ignore_index=True)\n","30982a29":"!pip install -qq cupy-cuda110\n!pip install -qq torchtext \n!pip install -qq ohmeow-blurr --upgrade","bcdea6d1":"#!pip install -qq fastai --upgrade\n#!pip install -qq fastcore --upgrade","5f932472":"from fastai.text.all import * \nfrom blurr.utils import *\nfrom blurr.data.core import *\nfrom blurr.modeling.core import *","3dac4488":"#! pip install -qq datasets transformers[sentencepiece]","c66c47c2":"from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig","082fc591":"checkpoint = \"tugstugi\/bert-base-mongolian-cased\"\nconfig = AutoConfig.from_pretrained(checkpoint)\nconfig.num_labels = 69","06aef238":"hf_arch, hf_config, hf_tokenizer, hf_model = BLURR.get_hf_objects(checkpoint, model_cls=AutoModelForSequenceClassification, config=config)\n\nprint(hf_arch)\nprint(type(hf_config))\nprint(type(hf_tokenizer))\nprint(type(hf_model))","e5b22c11":"hf_tokenizer.pad_token = hf_tokenizer.eos_token","f56f5cd6":"blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model, max_length=128, padding=True, truncation=True), CategoryBlock)\ndblock = DataBlock(blocks=blocks, get_x=ColReader('text'), get_y=ColReader('meaning'), splitter=ColSplitter())","f35763e4":"dls = dblock.dataloaders(new_df2, bs=1)","7bee7ce1":"model = HF_BaseModelWrapper(hf_model)\n\nlearn = Learner(dls, \n                model,\n                opt_func=partial(OptimWrapper, opt=torch.optim.Adam),\n                loss_func=CrossEntropyLossFlat(),\n                metrics=[accuracy],\n                cbs=[HF_BaseModelCallback],\n                splitter=hf_splitter)\n\nlearn.freeze()","b5f74a03":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","9fc3b763":"learn.fit_one_cycle(3, lr_max=0.0005)","daec21c5":"#\u0411\u044d\u043b\u0434\u0441\u044d\u043d \u0442\u043e\u0434\u0443\u043b-\u0430\u0430 \u0442\u0435\u0441\u0442 \u0434\u0430\u0442\u0430 \u0434\u044d\u044d\u0440 \u0430\u0448\u0438\u0433\u043b\u0430\u0445\ndl = learn.dls.test_dl(new_df_t['text'])","4d8b1a8c":"probs, name, decoded = learn.get_preds(dl=dl, with_decoded=True)","77cd4676":"vocab = learn.dls.vocab\nmapper = lambda t: vocab[t]\nvfunc = np.vectorize(mapper)\ncats = vfunc(decoded)","726927bb":"new_df_t['meaning']=cats","917bdc04":"new_df_t['synset_id'] = new_df_t['meaning'].str.split(expand = True)[1]\nnew_df_t.to_csv('submission.csv', index=False, columns=['text_id','synset_id'])","2fda1e62":"### Submission \u0444\u0430\u0439\u043b \u04af\u04af\u0441\u0433\u044d\u0445","be9e9177":"## \u0414\u0430\u0442\u0430 \u0441\u0443\u0440\u0433\u0430\u043b\u0442","0a2e8539":"## \u04e8\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u0446\u044d\u0432\u044d\u0440\u043b\u044d\u0433\u044d\u044d","1d1f51b6":"## \u0422\u0430\u0430\u043c\u0430\u0433\u043b\u0430\u043b\u0442\n","b064502e":"### \u04e8\u0433\u04af\u04af\u043b\u0431\u044d\u0440\u044d\u044d\u0441 \u0433\u043e\u043b \u04af\u0433\u0438\u0439\u0433 \u0442\u043e\u0439\u0440\u043e\u043d 4 \u04af\u0433 \u0430\u0432\u0447 \u0433\u043e\u043b \u04af\u0433\u0438\u0439\u043d \u04e9\u043c\u043d\u04e9 TOKEN \u043e\u0440\u0443\u0443\u043b\u0436 \u04e9\u0433\u04e9\u0432","39cfb642":"### \u0422\u0435\u0441\u0442 \u04e9\u0433\u04e9\u0433\u0434\u043b\u0438\u0439\u043d \u0446\u044d\u0432\u044d\u0440\u043b\u044d\u0433\u044d\u044d"}}