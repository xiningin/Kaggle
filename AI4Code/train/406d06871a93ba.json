{"cell_type":{"4bc38b7d":"code","03ed79f8":"code","843c54ce":"code","e3bfaa6f":"code","673b93c0":"code","fde33ca1":"code","07984e33":"code","ea68c033":"code","4da8f877":"code","ebd000ee":"code","62d06745":"code","5964bada":"code","4936bc28":"code","dce2ada1":"code","21ed5615":"code","a0537f36":"code","b1f8c66d":"code","47c00258":"code","2452b9d2":"code","3cdbaea7":"code","9c1fa656":"code","1b1a5efd":"code","c911b6e8":"code","a5d6071b":"code","d537fbdb":"code","8dd35609":"code","585ab3f3":"code","cfa83271":"code","00c526ad":"code","b5cff1da":"code","67c21c0f":"code","691ac43c":"code","dc66d09a":"code","b9765571":"code","70d65d9f":"code","60cfed4b":"code","e83aa3e1":"code","46c141cc":"code","4214b6de":"code","f25c98ee":"code","81ec6ef9":"code","0d4ebe61":"code","3b4f049e":"code","41a540df":"code","6de74475":"code","fd3a3314":"code","b2de5030":"code","eec29a5c":"code","1dda3ab8":"code","604200f6":"code","7d61375e":"markdown","ba30c590":"markdown","0049d676":"markdown","f11f3d9c":"markdown","f2f88db4":"markdown","8d5b279c":"markdown","02c0c202":"markdown","d885996a":"markdown","8d19debd":"markdown","acf2ecfd":"markdown"},"source":{"4bc38b7d":"# Libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.gridspec as gridspec\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nimport plotly\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode(connected=True)\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# color palette\ncustom_colors=[\"#B69BC5\",\"#BB1C8B\",\"#05A4C0\",'#CCEBC5',\"#D2A7D8\",'#FDDAEC',  \"#85CEDA\",]\ncustomPalette=sns.set_palette(sns.color_palette(custom_colors))\n\n#set size\nsns.palplot(sns.color_palette(custom_colors),size=1)\nplt.tick_params(axis='both',labelsize=0,length=0)\n","03ed79f8":"# train data\ntrain_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_df.head()","843c54ce":"#test data\ntest_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_df.head()","e3bfaa6f":"train_df.info()","673b93c0":"# test_df.info()","fde33ca1":"# Visualize the distribution of target variable\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nplt.figure(figsize = (12,6))\nsns.distplot(train_df['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","07984e33":"# Correlation matrix - numerical correlation between features\nf, ax = plt.subplots(figsize=(30, 25))\nmat = train_df.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","ea68c033":"# MasVnrArea vs SalePrice\nfig = px.scatter(train_df, x='MasVnrArea' , y='SalePrice')\nfig.show()","4da8f877":"#TotRmsAbvGrd vs SalePrice\nfig = px.scatter(train_df, x='TotRmsAbvGrd' , y='SalePrice')\nfig.show()","ebd000ee":"#LotFrontage vs SalePrice\nfig = px.scatter(train_df, x=\"LotFrontage\" , y=\"SalePrice\")\nfig.show()","62d06745":"#TotalBsmtSF vs SalePrice\nfig = px.scatter(train_df, x=\"TotalBsmtSF\" , y=\"SalePrice\")\nfig.show()","5964bada":"# YearBuilt vs SalePrice\nPearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=train_df, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","4936bc28":"#YearRemodAdd vs SalePrice\nfig = px.bar(train_df, x=\"YearRemodAdd\", y=\"SalePrice\",\n              barmode='group',\n             height=600, color_discrete_sequence=px.colors.qualitative.Set1)\nfig.show()","dce2ada1":"#Sale Price vs 2nd floor in sq feet\nN = 2000\ntrain_x = train_df[\"2ndFlrSF\"]\ntrain_y = train_df[\"SalePrice\"]\ncolors = np.random.rand(2938)\nsz = np.random.rand(N)*30\n\nfig = go.Figure()\nfig.add_scatter(x = train_x,\n                y = train_y,\n                mode = 'markers',\n                marker = {'size': sz,\n                         'color': colors,\n                         'opacity': 0.6,\n                         'colorscale': 'Portland',\n                          \n                       })\n\nplotly.offline.iplot(fig)","21ed5615":"#Quality of basement finished area\nfig = px.pie(train_df, names = \"BsmtFinType1\", title = \"Quality of basement finished area\", color_discrete_sequence=px.colors.qualitative.Set3)\nfig.show()","a0537f36":"#Garage location\nfig = px.pie(train_df, names = \"GarageType\", title = \"Garage location\", color_discrete_sequence=px.colors.qualitative.Set2)\nfig.show()","b1f8c66d":"#Type of sale\nfig = px.pie(train_df, names = \"SaleType\", title = \"Type of sale\", color_discrete_sequence=px.colors.qualitative.Set1)\nfig.show()","47c00258":"#Condition of sale\nfig = px.pie(train_df, names = \"SaleCondition\", title = \"Condition of sale\")\nfig.show()","2452b9d2":"#MSSubClass vs SalePrice\nfig =px.bar(train_df,x='ExterQual', y='SalePrice',barmode='group',\n             height=600)\nfig.show()","3cdbaea7":"#MSZoning vs SalePrice\nfig =px.bar(train_df,x='MSZoning', y='SalePrice',barmode='group',\n             height=800, color_discrete_sequence=[\"fuchsia\"])\n\nfig.show()","9c1fa656":"#Scatterplot matrices\nfig = px.scatter_matrix(train_df, dimensions=['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF'])\nfig.show()","1b1a5efd":"#Histogram for each numerical attribute\ntrain_df.hist(bins=50, figsize=(20,15), color='orchid')\nplt.show()","c911b6e8":"# Data preparation\ntarget = train_df['SalePrice']\ntest_id = test_df['Id']\ntest_df = test_df.drop(['Id'],axis = 1)\ntrain_df = train_df.drop(['SalePrice','Id'], axis = 1)\n\n\n# Concatenating train & test set\n\ntrain_test = pd.concat([train_df,test_df], axis=0, sort=False)","a5d6071b":"# Looking at NaN % within the data\n# if NaN % in under 20% keep it else discard it\nnan = pd.DataFrame(train_test.isna().sum(), columns = ['Total Missng Values'])\nnan['Feature'] = nan.index\nnan['Perc(%)'] = (nan['Total Missng Values']\/1460)*100\nnan = nan[nan['Total Missng Values'] > 0]\nnan = nan.sort_values(by = ['Total Missng Values'])\nnan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')\nnan","d537fbdb":"# Plotting Nan\n\nplt.figure(figsize = (15,5))\nsns.barplot(x = nan['Feature'], y = nan['Perc(%)'])\nplt.xticks(rotation=45)\nplt.title('Features containing Nan')\nplt.xlabel('Features')\nplt.ylabel('% of Missing Data')\nplt.show()","8dd35609":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)\n","585ab3f3":"# Data Imputation\ntrain_test['Functional'] = train_test['Functional'].fillna('Typ')\ntrain_test['Electrical'] = train_test['Electrical'].fillna(\"SBrkr\")\ntrain_test['KitchenQual'] = train_test['KitchenQual'].fillna(\"TA\")\ntrain_test['Exterior1st'] = train_test['Exterior1st'].fillna(train_test['Exterior1st'].mode()[0])\ntrain_test['Exterior2nd'] = train_test['Exterior2nd'].fillna(train_test['Exterior2nd'].mode()[0])\ntrain_test['SaleType'] = train_test['SaleType'].fillna(train_test['SaleType'].mode()[0])\ntrain_test[\"PoolQC\"] = train_test[\"PoolQC\"].fillna(\"None\")\ntrain_test[\"Alley\"] = train_test[\"Alley\"].fillna(\"None\")\ntrain_test['FireplaceQu'] = train_test['FireplaceQu'].fillna(\"None\")\ntrain_test['Fence'] = train_test['Fence'].fillna(\"None\")\ntrain_test['MiscFeature'] = train_test['MiscFeature'].fillna(\"None\")\n\nfor col in ('GarageArea', 'GarageCars'):\n    train_test[col] = train_test[col].fillna(0)\n        \nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    train_test[col] = train_test[col].fillna('None')\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train_test[col] = train_test[col].fillna('None')\n    \n    # Checking the features with NaN remained out\n\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","cfa83271":"# Removing the useless variables\nuseless = ['GarageYrBlt','YearRemodAdd'] \ntrain_test = train_test.drop(useless, axis = 1)","00c526ad":"# Imputing with KnnRegressor \ndef impute_knn(df):\n    ttn = train_test.select_dtypes(include=[np.number])\n    ttc = train_test.select_dtypes(exclude=[np.number])\n    cols_nan = ttn.columns[ttn.isna().any()].tolist()         # columns w\/ nan \n    cols_no_nan = ttn.columns.difference(cols_nan).values     # columns w\/n nan\n    for col in cols_nan:\n        imp_test = ttn[ttn[col].isna()]   # indicies which have missing data will become our test set\n        imp_train = ttn.dropna()          # all indicies which which have no missing data \n        model = KNeighborsRegressor(n_neighbors=5)  # KNR Unsupervised Approach\n        knr = model.fit(imp_train[cols_no_nan], imp_train[col])\n        ttn.loc[ttn[col].isna(), col] = knr.predict(imp_test[cols_no_nan])  \n    return pd.concat([ttn,ttc],axis=1)\ntrain_test = impute_knn(train_test)\nobjects = []\nfor i in train_test.columns:\n    if train_test[i].dtype == object:\n        objects.append(i)\ntrain_test.update(train_test[objects].fillna('None'))\n## Checking NaN presence\nfor col in train_test:\n    if train_test[col].isna().sum() > 0:\n        print(train_test[col][0])","b5cff1da":"#Let's create some new features combining the ones that we already have. \n#These could help us to increase the performance of the model!\n\ntrain_test[\"SqFtPerRoom\"] = train_test[\"GrLivArea\"] \/ (train_test[\"TotRmsAbvGrd\"] +\n                                                       train_test[\"FullBath\"] +\n                                                       train_test[\"HalfBath\"] +\n                                                       train_test[\"KitchenAbvGr\"])\n\ntrain_test['Total_Home_Quality'] = train_test['OverallQual'] + train_test['OverallCond']\n\ntrain_test['Total_Bathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) +\n                               train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test[\"HighQualSF\"] = train_test[\"1stFlrSF\"] + train_test[\"2ndFlrSF\"]","67c21c0f":"# Converting non-numeric predictors stored as numbers into string\n\ntrain_test['MSSubClass'] = train_test['MSSubClass'].apply(str)\ntrain_test['YrSold'] = train_test['YrSold'].apply(str)\ntrain_test['MoSold'] = train_test['MoSold'].apply(str)","691ac43c":"# Creating dummy variables from categorical features\n\ntrain_test_dummy = pd.get_dummies(train_test)","dc66d09a":"# Fetch all numeric features\n\nnumeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index\nskewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_features[skewed_features > 0.5]\nskew_index = high_skew.index","b9765571":"# Normalize skewed features using log_transformation\nfor i in skew_index:\n    train_test_dummy[i] = np.log1p(train_test_dummy[i])","70d65d9f":"# SalePrice before transformation\n\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\" qq-plot & distribution SalePrice \", fontsize= 15)\n\nsm.qqplot(target, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\n\nsns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()\n","60cfed4b":"# SalePrice after transformation\ntarget_log = np.log1p(target)\nfig, ax = plt.subplots(1,2, figsize= (15,5))\nfig.suptitle(\"qq-plot & distribution SalePrice \", fontsize= 15)\nsm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=\"45\", ax = ax[0])\nsns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1])\nplt.show()","e83aa3e1":"import shap\nimport xgboost as xgb\nfrom catboost import Pool\nfrom sklearn.svm import SVR\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor\nfrom mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n","46c141cc":"# Train-Test separation\n\ntrain = train_test_dummy[0:1460]\ntest = train_test_dummy[1460:]\ntest['Id'] = test_id\n\n# Creation of the RMSE metric:\n    \ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, train, target_log, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","4214b6de":"# 10 Fold Cross validation\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\ncv_scores = []\ncv_std = []\n\nbaseline_models = ['Linear_Reg.','Bayesian_Ridge_Reg.','LGBM_Reg.','SVR',\n                   'Dec_Tree_Reg.','Random_Forest_Reg.', 'XGB_Reg.',\n                   'Grad_Boost_Reg.','Cat_Boost_Reg.','Stacked_Reg.']\n\n# Linear Regression\n\nlreg = LinearRegression()\nscore_lreg = cv_rmse(lreg)\ncv_scores.append(score_lreg.mean())\ncv_std.append(score_lreg.std())\n\n# Bayesian Ridge Regression\n\nbrr = BayesianRidge(compute_score=True)\nscore_brr = cv_rmse(brr)\ncv_scores.append(score_brr.mean())\ncv_std.append(score_brr.std())\n\n# Light Gradient Boost Regressor\n\nl_gbm = LGBMRegressor(objective='regression')\nscore_l_gbm = cv_rmse(l_gbm)\ncv_scores.append(score_l_gbm.mean())\ncv_std.append(score_l_gbm.std())\n\n# Support Vector Regression\n\nsvr = SVR()\nscore_svr = cv_rmse(svr)\ncv_scores.append(score_svr.mean())\ncv_std.append(score_svr.std())\n\n# Decision Tree Regressor\n\ndtr = DecisionTreeRegressor()\nscore_dtr = cv_rmse(dtr)\ncv_scores.append(score_dtr.mean())\ncv_std.append(score_dtr.std())\n\n# Random Forest Regressor\n\nrfr = RandomForestRegressor()\nscore_rfr = cv_rmse(rfr)\ncv_scores.append(score_rfr.mean())\ncv_std.append(score_rfr.std())\n\n# XGB Regressor\n\nxgb = xgb.XGBRegressor()\nscore_xgb = cv_rmse(xgb)\ncv_scores.append(score_xgb.mean())\ncv_std.append(score_xgb.std())\n\n# Gradient Boost Regressor\n\ngbr = GradientBoostingRegressor()\nscore_gbr = cv_rmse(gbr)\ncv_scores.append(score_gbr.mean())\ncv_std.append(score_gbr.std())\n\n# Cat Boost Regressor\n\ncatb = CatBoostRegressor()\nscore_catb = cv_rmse(catb)\ncv_scores.append(score_catb.mean())\ncv_std.append(score_catb.std())\n\n# Stacked Regressor\n\nstack_gen = StackingRegressor(regressors=(CatBoostRegressor(),\n                                          LinearRegression(),\n                                          BayesianRidge(),\n                                          GradientBoostingRegressor()),\n                              meta_regressor = CatBoostRegressor(),\n                              use_features_in_secondary = True)\n\nscore_stack_gen = cv_rmse(stack_gen)\ncv_scores.append(score_stack_gen.mean())\ncv_std.append(score_stack_gen.std())\n\nfinal_cv_score = pd.DataFrame(baseline_models, columns = ['Regressors'])\nfinal_cv_score['RMSE_mean'] = cv_scores\nfinal_cv_score['RMSE_std'] = cv_std","f25c98ee":"final_cv_score","81ec6ef9":"plt.figure(figsize = (12,8))\nsns.barplot(final_cv_score['Regressors'],final_cv_score['RMSE_mean'])\nplt.xlabel('Regressors', fontsize = 12)\nplt.ylabel('CV_Mean_RMSE', fontsize = 12)\nplt.xticks(rotation=45)\nplt.show()","0d4ebe61":"X_train,X_val,y_train,y_val = train_test_split(train,target_log,test_size = 0.1,random_state=42)\n\n# Cat Boost Regressor\n\ncat = CatBoostRegressor()\ncat_model = cat.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = 0)","3b4f049e":"cat_pred = cat_model.predict(X_val)\ncat_score = rmse(y_val, cat_pred)\ncat_score","41a540df":"# Features' importance of our model\n\nfeat_imp = cat_model.get_feature_importance(prettified=True)\nfeat_imp","6de74475":"# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h')\nplt.show()","fd3a3314":"# Preforming a Random Grid Search to find the best combination of parameters\n\ngrid = {'iterations': [1000,6000],\n        'learning_rate': [0.05, 0.005, 0.0005],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5, 9]}\n\nfinal_model = CatBoostRegressor()\nrandomized_search_result = final_model.randomized_search(grid,\n                                                   X = X_train,\n                                                   y= y_train,\n                                                   verbose = False,\n                                                   plot=True)\n                                                   ","b2de5030":"# Final Cat-Boost Regressor\n\nparams = {'iterations': 6000,\n          'learning_rate': 0.005,\n          'depth': 4,\n          'l2_leaf_reg': 1,\n          'eval_metric':'RMSE',\n          'early_stopping_rounds': 200,\n          'verbose': 200,\n          'random_seed': 42}\n         \ncat_f = CatBoostRegressor(**params)\ncat_model_f = cat_f.fit(X_train,y_train,\n                     eval_set = (X_val,y_val),\n                     plot=True,\n                     verbose = False)\n\ncatf_pred = cat_model_f.predict(X_val)\ncatf_score = rmse(y_val, catf_pred)","eec29a5c":"catf_score","1dda3ab8":"# Test CSV Submission\n\ntest_pred = cat_f.predict(test)\nsubmission = pd.DataFrame(test_id, columns = ['Id'])\ntest_pred = np.expm1(test_pred)\nsubmission['SalePrice'] = test_pred \nsubmission.head()","604200f6":"# Saving the results in a csv file\n\nsubmission.to_csv(\"result.csv\", index = False, header = True)","7d61375e":"##### From above we can see that data is not normally distributed. Instead it is right skewed. We will handle this issue later in this notebook","ba30c590":"<h3><center>Data Preparation<\/center><\/h3>\nIn Data Processing we have to take care of three things-\n<ol>\n    <li>Missing Values<\/li>\n    <li>Dealing with categorical features<\/li>\n    <li>Data Normalization<\/li>\n<\/ol>","0049d676":"##### Finally transform target column ('Saleprice') distribution","f11f3d9c":"<h1><center>House Prices - Advanced Regression Techniques<\/center><\/h1>\n<h3><center>Predict sales prices and practice feature engineering, RFs, and gradient boosting<\/center><\/h3>","f2f88db4":"![Screen-Shot-2019-07-19-at-2.16.43-PM-1.png](attachment:2b4eb3da-1af0-4f57-a8a6-7f43b4794f1c.png)","8d5b279c":"Finally...\n<h3><center>Modeling<\/center><\/h3>","02c0c202":"<h3><center>Feature Engineering<\/center><\/h3>\n","d885996a":"<h3><center> 2. EDA & Data Description <\/center><\/h3>","8d19debd":"##### As we can see there are 79 features in the training dataset. Let's have a quick understanding of them.....\n<br>\n\n* **SalePrice** - the property's sale price in dollars. This is the target variable that you're trying to predict.\n\n* **MSSubClass**: The building class\n\n* **MSZoning**: The general zoning classification\n\n* **LotFrontage**: Linear feet of street connected to property\n\n* **LotArea**: Lot size in square feet\n\n* **Street**: Type of road access\n\n* **Alley**: Type of alley access\n\n* **LotShape**: General shape of property\n\n* **LandContour**: Flatness of the property\n\n* **Utilities**: Type of utilities available\n\n* **LotConfig**: Lot configuration\n\n* **LandSlope**: Slope of property\n\n* **Neighborhood**: Physical locations within Ames city limits\n\n* **Condition1**: Proximity to main road or railroad\n\n* **Condition2**: Proximity to main road or railroad (if a second is present)\n\n* **BldgType**: Type of dwelling\n\n* **HouseStyle**: Style of dwelling\n\n* **OverallQual**: Overall material and finish quality\n\n* **OverallCond**: Overall condition rating\n\n* **YearBuilt**: Original construction date\n\n* **YearRemodAdd**: Remodel date\n\n* **RoofStyle**: Type of roof\n\n* **RoofMatl**: Roof material\n\n* **Exterior1st**: Exterior covering on house\n\n* **Exterior2nd**: Exterior covering on house (if more than one material)\n\n* **MasVnrType**: Masonry veneer type\n\n* **MasVnrArea**: Masonry veneer area in square feet\n\n* **ExterQual**: Exterior material quality\n\n* **ExterCond**: Present condition of the material on the exterior\n\n* **Foundation**: Type of foundation\n\n* |**BsmtQual**: Height of the basement\n\n* **BsmtCond**: General condition of the basement\n\n* **BsmtExposure**: Walkout or garden level basement walls\n\n* **BsmtFinType1**: Quality of basement finished area\n\n* **BsmtFinSF1**: Type 1 finished square feet\n\n* **BsmtFinType2**: Quality of second finished area (if present)\n\n* **BsmtFinSF2**: Type 2 finished square feet\n\n* **BsmtUnfSF**: Unfinished square feet of basement area\n\n* **TotalBsmtSF**: Total square feet of basement area\n\n* **Heating**: Type of heating\n\n* **HeatingQC**: Heating quality and condition\n\n* **CentralAir**: Central air conditioning\n\n* **Electrical**: Electrical system\n\n* **1stFlrSF**: First Floor square feet\n\n* **2ndFlrSF**: Second floor square feet\n\n* **LowQualFinSF**: Low quality finished square feet (all floors)\n\n* **GrLivArea**: Above grade (ground) living area square feet\n\n* **BsmtFullBath**: Basement full bathrooms\n\n* **BsmtHalfBath**: Basement half bathrooms\n\n* **FullBath**: Full bathrooms above grade\n\n* **HalfBath**: Half baths above grade\n\n* **Bedroom**: Number of bedrooms above basement level\n\n* **Kitchen**: Number of kitchens\n\n* **KitchenQual**: Kitchen quality\n\n* **TotRmsAbvGrd**: Total rooms above grade (does not include bathrooms)\n\n* **Functional**: Home functionality rating\n\n* **Fireplaces**: Number of fireplaces\n\n* **FireplaceQu**: Fireplace quality\n\n* **GarageType**: Garage location\n\n* **GarageYrBlt**: Year garage was built\n\n* **GarageFinish**: Interior finish of the garage\n\n* **GarageCars**: Size of garage in car capacity\n\n* **GarageArea**: Size of garage in square feet\n\n* **GarageQual**: Garage quality\n\n* **GarageCond**: Garage condition\n\n* **PavedDrive**: Paved driveway\n\n* **WoodDeckSF**: Wood deck area in square feet\n\n* **OpenPorchSF**: Open porch area in square feet\n\n* **EnclosedPorch**: Enclosed porch area in square feet\n\n* **3SsnPorch**: Three season porch area in square feet\n\n* **ScreenPorch**: Screen porch area in square feet\n\n* **PoolArea**: Pool area in square feet\n\n* **PoolQC**: Pool quality\n\n* **Fence**: Fence quality\n\n* **MiscFeature**: Miscellaneous feature not covered in other categories\n\n* **MiscVal**: $Value of miscellaneous feature\n\n* **MoSold**: Month Sold\n\n* **YrSold**: Year Sold\n\n* **SaleType**: Type of sale\n\n* **SaleCondition**: Condition of sale","acf2ecfd":"<h3><center>1. Understanding <\/center><\/h3>\nThis competition challenges you to <b>predict the final price<\/b> of each home, with 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa."}}