{"cell_type":{"79c9eb8a":"code","9246b55c":"code","6f864338":"code","2a1dd819":"code","f69b40cf":"code","f7cdc367":"code","3ab65ab0":"code","2d392846":"code","af8a0b6d":"code","3de37f5c":"code","77e7e471":"code","e31432f0":"code","73fd042a":"code","907a5ef1":"code","77fc7a63":"code","7b330f60":"code","8e82ec8c":"code","f19af765":"code","23faa81f":"code","65a77a5c":"code","4cf8cb37":"code","7e249c42":"code","22180eb8":"code","ae025c4e":"code","88be48b6":"code","c7938427":"code","de0dd377":"code","ec3ea4c7":"code","1968b3ee":"code","d5d0346a":"code","332c1dca":"code","93932358":"code","7fc412ae":"code","bbc7b22d":"code","607a21db":"code","0a21c8da":"code","80a5cbd6":"code","a97e7430":"code","67430a98":"code","e1e6becb":"code","46985b88":"code","44f2901e":"code","ccea5990":"code","2b7e90df":"code","74ba6ff8":"code","0cba5d6f":"code","6ac2f74b":"code","bfafbac3":"code","fee8b7b7":"code","b656e889":"code","70744c91":"code","b149dc6d":"code","1c4cd97d":"code","4ae69351":"code","ddf2a6a3":"code","28124a99":"markdown","772c9601":"markdown","5d121247":"markdown","7ac989df":"markdown","13b35ae5":"markdown","52d25718":"markdown","6e5dd460":"markdown","2cb4bc25":"markdown","549f07b1":"markdown","89c77e02":"markdown","04a8d547":"markdown","2e56e2e0":"markdown","d4aedebf":"markdown","168d3171":"markdown","fded282c":"markdown","642669bb":"markdown","e242876d":"markdown","03f93212":"markdown","af70c8f0":"markdown","d275f940":"markdown","236b0982":"markdown","1d0d996d":"markdown"},"source":{"79c9eb8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9246b55c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import StackingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA","6f864338":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","2a1dd819":"train.head()","f69b40cf":"test.head()","f7cdc367":"sample.head()","3ab65ab0":"print(\"Training data shape :\",train.shape)\nprint(\"Test data shape :\",test.shape)","2d392846":"sns.set_style(\"darkgrid\")\nf, ax = plt.subplots(figsize=(8, 7))\n#Check the new distribution \nsns.distplot(train['SalePrice'], color=\"r\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","af8a0b6d":"# Skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","3de37f5c":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","77e7e471":"# log(1+x) transform\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","e31432f0":"#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","73fd042a":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","907a5ef1":"fig, ax = plt.subplots()\nax.scatter(x = train['OverallQual'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('OverallQual', fontsize=13)\nplt.show()","77fc7a63":"# Remove outliers\ntrain.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<12.5)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","7b330f60":"# Remove the Ids from train and test, as they are unique for each row and hence not useful for the model\ntrain_ID = train['Id']\ntest_ID = test['Id']\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","8e82ec8c":"# Split features and labels\ntrain_labels = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test\n\n# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape","f19af765":"numeric = [col for col in train.columns if train[col].dtype in ['float64','int64','int32','float32','int16','float16']]\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nfor i, feature in enumerate(list(train[numeric]), 1):\n    if(feature=='MiscVal'):\n        break\n    plt.subplot(len(numeric), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', palette='Blues', data=train)\n        \n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 10})","23faa81f":"all_data_na = (all_features.isnull().sum() \/ len(all_features)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","65a77a5c":"# Visualize missing values\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nax.xaxis.grid(False)\nsns.despine(trim=True, left=True)","4cf8cb37":"all_features['MSSubClass'].value_counts()","7e249c42":"all_features.OverallCond.value_counts()","22180eb8":"# convert them into strings \n\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)\nall_features['OverallCond'] = all_features['OverallCond'].astype(str)","ae025c4e":"### Lets start imputing them ###\nall_features[\"PoolQC\"] = all_features[\"PoolQC\"].fillna(\"None\")\nall_features[\"MiscFeature\"] = all_features[\"MiscFeature\"].fillna(\"None\")\nall_features[\"Alley\"] = all_features[\"Alley\"].fillna(\"None\")\nall_features[\"Fence\"] = all_features[\"Fence\"].fillna(\"None\")\nall_features[\"FireplaceQu\"] = all_features[\"FireplaceQu\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_features[\"LotFrontage\"] = all_features.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_features[col] = all_features[col].fillna('None')\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_features[col] = all_features[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_features[col] = all_features[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_features[col] = all_features[col].fillna('None')\nall_features[\"MasVnrType\"] = all_features[\"MasVnrType\"].fillna(\"None\")\nall_features[\"MasVnrArea\"] = all_features[\"MasVnrArea\"].fillna(0)\nall_features['MSZoning'] = all_features['MSZoning'].fillna(all_features['MSZoning'].mode()[0])\nall_features = all_features.drop(['Utilities'], axis=1)\nall_features[\"Functional\"] = all_features[\"Functional\"].fillna(\"Typ\")\nall_features['Electrical'] = all_features['Electrical'].fillna(all_features['Electrical'].mode()[0])\nall_features['KitchenQual'] = all_features['KitchenQual'].fillna(all_features['KitchenQual'].mode()[0])\nall_features['Exterior1st'] = all_features['Exterior1st'].fillna(all_features['Exterior1st'].mode()[0])\nall_features['Exterior2nd'] = all_features['Exterior2nd'].fillna(all_features['Exterior2nd'].mode()[0])\nall_features['SaleType'] = all_features['SaleType'].fillna(all_features['SaleType'].mode()[0])\nall_features['MSSubClass'] = all_features['MSSubClass'].fillna(\"None\")\n","88be48b6":"#Check remaining missing values if any \nall_features_na = (all_features.isnull().sum() \/ len(all_features)) * 100\nall_features_na = all_features_na.drop(all_features_na[all_features_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_features_na})\nmissing_data.head()","c7938427":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_features[c].values)) \n    all_features[c] = lbl.transform(list(all_features[c].values))\n\n# shape        \nprint('Shape all_features: {}'.format(all_features.shape))","de0dd377":"all_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']","ec3ea4c7":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in all_features.columns:\n    if all_features[i].dtype in numeric_dtypes:\n        numeric.append(i)","1968b3ee":"# Find skewed numerical features\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","d5d0346a":"# Normalize skewed features\nlam = 0.15\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i],lam)","332c1dca":"all_features = pd.get_dummies(all_features)\nprint(all_features.shape)","93932358":"# Remove any duplicated column names\nall_features = all_features.loc[:,~all_features.columns.duplicated()]","7fc412ae":"xtrain = all_features.iloc[:len(train_labels), :]\nxtest = all_features.iloc[len(train_labels):, :]\nprint(xtrain.shape); print(train_labels.shape); print(xtest.shape)","bbc7b22d":"pca = PCA(n_components=180)\nxtrain =pca.fit_transform(xtrain)\nxtest = pca.transform(xtest)","607a21db":"print(xtrain.shape); print(xtest.shape)","0a21c8da":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef rmse_cross_val(model, X=xtrain):\n    rmse = np.sqrt(-cross_val_score(model, xtrain, train_labels, scoring=\"neg_mean_squared_error\", cv=KFold(n_splits=12, random_state=42, shuffle=True)))\n    return rmse","80a5cbd6":"# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=6000,max_depth=4,min_child_weight=0,gamma=0.6,\n                       subsample=0.7,colsample_bytree=0.7,objective='reg:squarederror',nthread=-1,scale_pos_weight=1,\n                       seed=27,reg_alpha=0.00006,random_state=42)\n\n\n# Light Gradient Boosting Regressor\n#lightgbm = LGBMRegressor(objective='regression', num_leaves=6,learning_rate=0.01, n_estimators=7000,max_bin=200, \n #                      bagging_fraction=0.8,bagging_freq=4, bagging_seed=8,feature_fraction=0.2,\n  #                     feature_fraction_seed=8,min_sum_hessian_in_leaf = 11,verbose=-1,random_state=42)\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,learning_rate=0.01,max_depth=4,max_features='sqrt',\n                                min_samples_leaf=15,min_samples_split=10,loss='huber',random_state=42) \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,max_depth=15,min_samples_split=5,min_samples_leaf=5,\n                          max_features=None,oob_score=True,random_state=42)\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Ridge Regressor\nridge_alphas = [1e-10, 1e-8, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=KFold(n_splits=12, random_state=42, shuffle=True)))\n\n# Stack up all the models above, optimized using xgboost\n#stack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),meta_regressor=xgboost,random_state=42)\n\nstack_reg = StackingRegressor(estimators=[('xgboost',xgboost),('svr',svr),('ridge',ridge),('gbr',gbr),('rf',rf)], final_estimator=xgboost)    ","a97e7430":"print('stack_reg')\nstack_reg_model = stack_reg.fit(np.array(xtrain), np.array(train_labels))","67430a98":"#print('lightgbm')\n#lgb_data = lightgbm.fit(xtrain, train_labels)","e1e6becb":"print('xgboost')\nxgb_data = xgboost.fit(xtrain, train_labels)","46985b88":"print('Svr')\nsvr_data = svr.fit(xtrain, train_labels)","44f2901e":"print('Ridge')\nridge_data = ridge.fit(xtrain, train_labels)","ccea5990":"print('RandomForest')\nrf_data = rf.fit(xtrain, train_labels)","2b7e90df":"print('GradientBoosting')\ngbr_data = gbr.fit(xtrain, train_labels)","74ba6ff8":"scores = {}\n\n#score = rmse_cross_val(lightgbm)\n#print(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n#scores['lgb'] = (score.mean(), score.std())","0cba5d6f":"score = rmse_cross_val(svr)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","6ac2f74b":"score = rmse_cross_val(ridge)\nprint(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['ridge'] = (score.mean(), score.std())","bfafbac3":"score = rmse_cross_val(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","fee8b7b7":"score = rmse_cross_val(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","b656e889":"# Blend models in order to make the final predictions more robust to overfitting\ndef blended_predictions(xtrain):\n    return ((0.2 * ridge_data.predict(xtrain)) + \\\n            (0.15 * svr_data.predict(xtrain)) + \\\n            (0.17 * gbr_data.predict(xtrain)) + \\\n            (0.1 * xgb_data.predict(xtrain)) + \\\n            (0.03 * rf_data.predict(xtrain)) + \\\n            (0.35 * stack_reg_model.predict(np.array(xtrain))))","70744c91":"# Get final precitions from the blended model\nblended_score = rmsle(train_labels, blended_predictions(xtrain))\nscores['blended'] = (blended_score, 0)\nprint('RMSE score on train data:')\nprint(blended_score)","b149dc6d":"# Plot the predictions for each model\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n","1c4cd97d":"sample.head()","4ae69351":"# Append predictions from blended models\nsample.iloc[:,1] = np.floor(np.expm1(blended_predictions(xtest)))","ddf2a6a3":"# Fix outlier predictions\n#q1 = sample['SalePrice'].quantile(0.0045)\n#q2 = sample['SalePrice'].quantile(0.99)\n#submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\n#submission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsample.to_csv(\"my_submission.csv\", index=False)","28124a99":"So its clear that our target variable is skewed towards right. So we have to apply some transformations to it to make it normally distributed otherwise it's going to be wrong","772c9601":"**Lets first define the evaluation metrics : RMSLE , RMSE**","5d121247":"# Feature Engineering","7ac989df":"Gettting dummy categorical features","13b35ae5":" Lets experiment with the **Logarithmic transformation** [log(1+x)] to SalePrice","52d25718":"![](http:\/\/)Hence the above features although seem numerical, it's not so. Similar is the case with 'YrSold','MoSold'","6e5dd460":"# Time to separate again the Training & Test sets","2cb4bc25":"Adding an extra feature : basement area + 1st floor area + 2nd floor area","549f07b1":"# SalePrice : our target variable\nLets have a look into its distribution","89c77e02":"**Label Encoding some categorical variables**","04a8d547":"**Fit the models**","2e56e2e0":"Lets apply blending of all models","d4aedebf":"**Box-Cox Transformation of highly-skewed features**","168d3171":"**Handling Missing Entries**","fded282c":"So there are no missing   values left","642669bb":"**Handling Skewed Features**","e242876d":"**Lets visualize the cv-scores for each model**","03f93212":"# Lets have a visualisation of the relationship between target & other numerical features","af70c8f0":"Lets detect & explore few outliers, to facilitate analysis","d275f940":"**Well now the target variable seems to be more or less Normally Distributed**","236b0982":"# Modelling","1d0d996d":"Lets concatenate the train & test data features , so that all faeture transformation tasks can be delivered through a pipeline"}}