{"cell_type":{"6a770ec0":"code","9eaf33da":"code","acfc7995":"code","7f40fccc":"code","93895bc6":"code","48c5a996":"code","61b500db":"code","725015b5":"code","c740927c":"code","4b3c970d":"code","25f0ef12":"code","6f5165cf":"code","b800d5de":"code","0d1ee547":"code","c20153e1":"code","ada04cf7":"code","2add3029":"code","298f505d":"code","fef3bd14":"code","1b9aa476":"code","4bc68440":"code","adf92736":"code","9bb7a317":"code","430dc687":"code","7f22c464":"code","8a1b4767":"code","13180027":"markdown","5431bc3d":"markdown","7afaf563":"markdown","88fe8f8a":"markdown","836896aa":"markdown","948b9bbc":"markdown","b9d24481":"markdown","db224498":"markdown","2df0862a":"markdown","afa269be":"markdown","9245076b":"markdown","17078324":"markdown","cc434d1d":"markdown","4567bc76":"markdown","49e115f6":"markdown","9b39906b":"markdown","206219fe":"markdown","c58be3ea":"markdown","80b362f9":"markdown","c667051f":"markdown"},"source":{"6a770ec0":"%%sh\n\npip install timm\npip install wandb --upgrade\npip install -q nnAudio","9eaf33da":"import os\nimport platform\nimport wandb\nfrom dataclasses import dataclass, field, asdict\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom nnAudio.Spectrogram import CQT1992v2\n\nimport timm\nimport albumentations as A\nimport albumentations.pytorch as AP\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","acfc7995":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb_key\")\n\nwandb.login(key=wandb_key)","7f40fccc":"@dataclass\nclass Config:\n    lr: float = 1e-5\n    autocast: bool = False\n    resize: tuple = (224, 224)\n    model_name: str = \"tf_efficientnet_b3\"\n    pretrained: bool = True\n    epochs: int = 5\n    scheduler: str = \"CosineAnnealingLR\"\n    n_splits: int = 5\n    split: float = 0.1\n    folds: list = field(default_factory=lambda: [1, 2, 3, 4, 5])\n    workers: int = 4\n    train_bs: int = 64\n    valid_bs: int = 64\n    seed: int = 0\n    num_labels: int = 1\n    grad_acc_step: int = 1\n    max_gnorm: int = 1000\n    wandb: bool = True\n    architecture: str = \"CNN\"\n    competition: str = \"G2Net\"\n    group: str = \"effnet\"","93895bc6":"cfg = Config()\nrun = wandb.init(project=\"g2net\",\n                 config=asdict(cfg),\n                 group=cfg.group,\n                 job_type=\"train\"\n                )","48c5a996":"TRAIN_PATH = \"..\/input\/g2net-gravitational-wave-detection\/train\"\nTEST_PATH = \"..\/input\/g2net-gravitational-wave-detection\/test\"\nTRAIN_LABELS = \"..\/input\/g2net-gravitational-wave-detection\/training_labels.csv\"\nSAMPLE_PATH = \"..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv\"","61b500db":"TRAIN_FILE = \"..\/input\/g2net-gravitational-wave-detection-file-paths\/training_labels_with_paths.csv\"","725015b5":"def wandb_log(**kwargs):\n    \"\"\"\n    Logs key value pair to WandB\n    \"\"\"\n    step = None\n    if \"epoch\" in kwargs:\n        step = kwargs[\"epoch\"]\n        del kwargs[\"epoch\"]\n    \n    for k, v in kwargs.items():\n        wandb.log({k: v}, step=step)\n        \ndef get_train_file_path(image_id):\n    \"\"\"\n    Taken from Y.Nakama's notebook\n    \"\"\"\n    return \"..\/input\/g2net-gravitational-wave-detection\/train\/{}\/{}\/{}\/{}.npy\".format(\n        image_id[0], image_id[1], image_id[2], image_id\n    )\n\ndef get_test_file_path(image_id):\n    \"\"\"\n    Taken from Y.Nakama's notebook\n    \"\"\"\n    return \"..\/input\/g2net-gravitational-wave-detection\/test\/{}\/{}\/{}\/{}.npy\".format(\n        image_id[0], image_id[1], image_id[2], image_id\n    )\n\ndef convert_to_list(tensor):\n    \"\"\"\n    Converts a tensor to list\n    \"\"\"\n    return tensor.cpu().detach().numpy().tolist()\n\ndef quick_visual(dataset, n=5, is_test=False):\n    \"\"\"\n    Quickly visualize dataset\n    \"\"\"\n    for i in range(n):\n        image = dataset[i]\n        if not is_test:\n            plt.title(f\"Label: {image[1]}\")\n            image = image[0]\n        plt.imshow(image[0])\n        plt.show() ","c740927c":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(cfg.model_name, pretrained=cfg.pretrained, in_chans=1)\n        self.n_f = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Linear(self.n_f, cfg.num_labels)\n        \n    def forward(self, inputs):\n        return self.backbone(inputs)","4b3c970d":"class G2NetDataset(torch.utils.data.Dataset):\n    def __init__(self, data, is_test=False, transform=None):\n        self.data = data\n        self.is_test = is_test\n        self.file_names = self.data[\"file_path\"].values\n        self.labels = self.data[\"target\"].values\n        self.wave_transform = CQT1992v2(sr=2048, fmin=20, fmax=1024, hop_length=64)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def apply_qtransform(self, waves, transform):\n        waves = np.hstack(waves)\n        waves = waves \/ np.max(waves)\n        waves = torch.from_numpy(waves).float()\n        image = transform(waves)\n        return image\n    \n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        waves = np.load(file_path)\n        image = self.apply_qtransform(waves, self.wave_transform)\n        \n        if self.transform:\n            image = image.squeeze().numpy()\n            image = self.transform(image=image)['image']\n\n        if self.is_test:\n            return image\n\n        label = torch.tensor(self.labels[idx]).float()        \n        return image, label","25f0ef12":"def get_augementations(a_type=\"train\"):\n    \"\"\"\n    Train and Validation Augmentations\n    \"\"\"\n    if a_type == \"train\":\n        return A.Compose([\n            AP.ToTensorV2(p=1.0),\n        ], p=1.0)\n    if a_type == \"valid\":\n        return A.Compose([\n            AP.ToTensorV2(p=1.0),\n        ], p=1.0)","6f5165cf":"class Trainer:\n    def __init__(self, model, optimizer, scheduler, train_dataloader, valid_dataloader, device):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.train_dl = train_dataloader\n        self.valid_dl = valid_dataloader\n        self.loss_fn = self.yield_loss\n        self.valid_loss_fn = self.yield_loss\n        self.device = device\n        if cfg.autocast:\n            self.scaler = torch.cuda.amp.GradScaler()\n        \n    \n    def yield_loss(self, outputs, targets):\n        \"\"\"\n        Returns the loss function\n        \"\"\"\n        return nn.BCEWithLogitsLoss()(outputs, targets)\n    \n    def train_one_epoch(self):\n        \"\"\"\n        Trains the model for one epoch\n        \"\"\"\n        pbar = tqdm(enumerate(self.train_dl), total=len(self.train_dl))\n        self.model.train()\n        avg_loss = 0\n        for idx, (inputs, targets) in pbar:\n            image = inputs.to(self.device, dtype=torch.float)\n            targets = targets.to(self.device, dtype=torch.float)\n            \n            if cfg.autocast:\n                with torch.cuda.amp.autocast():\n                    outputs = self.model(image).view(-1)\n                    loss = self.loss_fn(outputs, targets)\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optimizer)\n                self.scaler.update()\n            else:\n                outputs = self.model(image).view(-1)\n                loss = self.loss_fn(outputs, targets)\n                loss.backward()\n                self.optimizer.step()\n                \n            self.optimizer.zero_grad()\n            pbar.set_description(f\"Loss: {loss.item():.2f}\")\n            \n            avg_loss += loss.item()\n        \n        return avg_loss \/ len(self.train_dl)\n    \n    def valid_one_epoch(self):\n        \"\"\"\n        Runs a validation epoch on the model\n        \"\"\"\n        pbar = tqdm(enumerate(self.valid_dl), total=len(self.valid_dl))\n        self.model.eval()\n        \n        all_targets = []\n        all_preds = []\n        avg_loss = 0\n\n        with torch.no_grad():\n            for idx, (inputs, targets) in pbar:\n                image = inputs.to(self.device, dtype=torch.float)\n                targets = targets.to(self.device, dtype=torch.float)\n                \n                outputs = self.model(image).view(-1)\n                \n                val_loss = self.valid_loss_fn(outputs, targets)\n                pbar.set_description(f\"Val Loss: {val_loss.item():.2f}\")\n                \n                all_targets.extend(convert_to_list(targets))\n                all_preds.extend(convert_to_list(torch.sigmoid(outputs)))\n                \n                avg_loss += val_loss.item()\n            \n            val_roc_auc = roc_auc_score(all_targets, all_preds)\n            return val_roc_auc, avg_loss \/ len(self.valid_dl)\n        \n    def get_model(self):\n        \"\"\"\n        Return model\n        \"\"\"\n        return self.model","b800d5de":"if torch.cuda.is_available():\n    print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    DEVICE = torch.device('cuda')\nelse:\n    print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n    DEVICE = torch.device('cpu')","0d1ee547":"data = pd.read_csv(TRAIN_FILE)\ndata[\"file_path\"] = data[\"id\"].apply(get_train_file_path)\n\ntrain_data, valid_data = train_test_split(data, test_size=cfg.split, random_state=cfg.seed)\n\nprint(f\"Shape of Training Samples: {train_data.shape}\")\nprint(f\"Shape of Validation Samples: {valid_data.shape}\")","c20153e1":"test_data = pd.read_csv(SAMPLE_PATH)\ntest_data[\"file_path\"] = test_data[\"id\"].apply(get_test_file_path)\n\nprint(f\"Shape of Test Samples: {test_data.shape}\")","ada04cf7":"training_set = G2NetDataset(data=train_data, transform=get_augementations())\nvalidation_set = G2NetDataset(data=valid_data)\ntest_set = G2NetDataset(data=test_data, is_test=True)","2add3029":"quick_visual(training_set)","298f505d":"train_dl = torch.utils.data.DataLoader(\n    training_set,\n    batch_size=cfg.train_bs,\n    shuffle=True,\n    num_workers=cfg.workers,\n    pin_memory=True\n)","fef3bd14":"valid_dl = torch.utils.data.DataLoader(\n    validation_set,\n    batch_size=cfg.valid_bs,\n    shuffle=False,\n    num_workers=cfg.workers,\n)","1b9aa476":"os.mkdir(\"models\")","4bc68440":"model = Model().to(DEVICE)\nprint(f\"Training Model: {cfg.model_name}\")\n\ntrain_steps = int(len(train_data) \/ cfg.train_bs) * cfg.epochs\n\noptimizer = optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=1e-6)\n\ntrainer = Trainer(model, optimizer, None, train_dl, valid_dl, DEVICE)\n\nfor epoch in tqdm(range(1, cfg.epochs + 1)):\n    print(f\"Epoch: {epoch} \/ {cfg.epochs}\")\n    \n    train_loss = trainer.train_one_epoch()\n    \n    # Validate\n    current_roc, valid_loss = trainer.valid_one_epoch()\n    \n    if cfg.wandb:\n        wandb_log(\n            training_loss=train_loss,\n            validation_loss=valid_loss,\n            roc_auc_score=current_roc,\n            epoch=epoch\n        )\n        \n    print(f\"Validation ROC-AUC: {current_roc:.4f}\")\n    \n    torch.save(trainer.get_model().state_dict(), f\"models\/{cfg.model_name}_{current_roc:.2f}.pt\")","adf92736":"test_dl = torch.utils.data.DataLoader(\n    test_set,\n    batch_size=cfg.valid_bs,\n    shuffle=False,\n    num_workers=cfg.workers,\n)","9bb7a317":"models = os.listdir(\"models\")\nsorted_list = sorted(models, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[1]), reverse=True)","430dc687":"model = trainer.get_model()\nmodel.load_state_dict(torch.load(f\"models\/{sorted_list[0]}\"))","7f22c464":"model.eval()\npbar = tqdm(enumerate(test_dl), total=len(test_dl))\nprobs = []\nfor i, (images) in pbar:\n    images = images.to(DEVICE)    \n    with torch.no_grad():\n        outputs = model(images).view(-1)\n    probs.append(convert_to_list(torch.sigmoid(outputs)))\npredictions = np.concatenate(probs)","8a1b4767":"test_data['target'] = predictions\ntest_data[['id', 'target']].to_csv('submission.csv', index=False)\ntest_data.head()","13180027":"## Helper Functions","5431bc3d":"## Load and Split Data","7afaf563":"## Convert to DataLoader","88fe8f8a":"## Wandb Login","836896aa":"## Inference","948b9bbc":"# Define Models","b9d24481":"# G2Net\n\nThis is my implementation of G2Net. A lot of inspiration is borrowed, I will be listing my references below. Please upvote these notebooks, you will learn a lot more from there than here!\n\nReferences:\n1. [[Training] G2Net Multi-Model PyTorch \ud83d\udcbb + W&B \ud83d\ude80](https:\/\/www.kaggle.com\/heyytanay\/training-g2net-multi-model-pytorch-w-b)\n2. [G2Net \/ efficientnet_b7 \/ baseline [inference]](https:\/\/www.kaggle.com\/yasufuminakama\/g2net-efficientnet-b7-baseline-inference)","db224498":"## File Paths","2df0862a":"# Get best ROC-AUC performing model","afa269be":"## Check for GPUs","9245076b":"Create models folder for saving our trained models.\n\nThis will be used later to pick the model with the best score.","17078324":"## Prepare Datasets","cc434d1d":"# Augmentations","4567bc76":"# Creating Submission","49e115f6":"# Create Dataset Class","9b39906b":"## Configuration","206219fe":"# Create Trainer","c58be3ea":"## Loop","80b362f9":"# Training","c667051f":"# Install and Import Dependencies"}}