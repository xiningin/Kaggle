{"cell_type":{"3b2900d9":"code","ce9e1db3":"code","902ae419":"code","a58ef1d6":"code","e81e09cf":"code","f5e15511":"code","00c356ad":"code","4f3dedd7":"code","e84acdb2":"code","47ad4e8d":"code","1067e36b":"code","90e93d6c":"code","6da6a422":"code","3e4ef692":"code","14736dc2":"code","0b42a57a":"code","0a33632c":"code","b850b425":"code","f384eda3":"code","1c355fa6":"code","69ce5374":"code","fdef9ac0":"code","19badce4":"code","17f5150a":"code","8cc18db4":"code","300b06ec":"code","de88b155":"code","63c9a4f8":"code","2cd80a49":"code","499fa18c":"code","cc3b755b":"code","b26dafa1":"code","321a73b7":"code","77aca2b7":"code","1cad5e2c":"code","32b2e898":"code","6a74b948":"code","5af59ba4":"code","a3ac1914":"code","1873028a":"code","67845636":"code","fea59ed6":"code","3f882240":"code","e25ac090":"code","4f82a1f5":"code","c021d32a":"code","b64d0e4f":"code","519f98da":"code","fced2dff":"code","995b3a1b":"code","f81d1ce6":"code","3dbb79de":"code","bdaf71c6":"code","59e42beb":"markdown","ad70eed7":"markdown","a0c045dd":"markdown","fd49c79c":"markdown","0efcf1a7":"markdown","3b40fbba":"markdown","fd112d25":"markdown","182e64a0":"markdown","1ceae2e0":"markdown","5fcfff1b":"markdown","52676db7":"markdown","e2e7ced8":"markdown","7cc275c9":"markdown","5d9f2e40":"markdown","ea50d741":"markdown","423783e3":"markdown","d2f181a3":"markdown","6b8ee05c":"markdown","95d84043":"markdown","891edaa2":"markdown","c80d9b55":"markdown","65d63fd2":"markdown","4abc38dd":"markdown","08730bda":"markdown","32921802":"markdown","c93c8da8":"markdown","3815477c":"markdown","daefb64b":"markdown","6d96e3d4":"markdown"},"source":{"3b2900d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce9e1db3":"# import library\nimport pandas as pd\nimport numpy as np\n\n#matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","902ae419":"data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndata = pd.DataFrame(data)\ndata.head()","a58ef1d6":"#Deleting the last 2 columns\ndata.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                   'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2',\n                   'CLIENTNUM'], \n          inplace=True, axis=1)\ndata","e81e09cf":"data.dtypes","f5e15511":"numerical_columns = [\"Customer_Age\", \n                     \"Months_on_book\", \n                     \"Credit_Limit\", \n                     \"Total_Revolving_Bal\", \n                     \"Avg_Open_To_Buy\", \n                     \"Total_Amt_Chng_Q4_Q1\", \n                     \"Total_Trans_Amt\", \n                     \"Total_Trans_Ct\", \n                     \"Total_Ct_Chng_Q4_Q1\", \n                     \"Avg_Utilization_Ratio\"]\n\ncategorical_columns = ['Attrition_Flag',\n                       'Gender',\n                       'Education_Level',\n                       'Marital_Status',\n                       'Income_Category',\n                       'Card_Category']\n\ndiscrete_columns = ['CLIENTNUM',\n                    'Dependent_count',\n                    'Total_Relationship_Count',\n                    'Months_Inactive_12_mon',\n                    'Contacts_Count_12_mon']","00c356ad":"data.info()","4f3dedd7":"print(data.duplicated().sum())\nprint(data.isnull().sum())","e84acdb2":"# Numerical Value \nfig, axes = plt.subplots(5,2, figsize=(15,16))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, col in enumerate(numerical_columns):\n  plot = sns.histplot(data=data, x=col, ax=axes[i], fill=True)\nplt.tight_layout()","47ad4e8d":"# Categorical Value\nfig, axes = plt.subplots(3,2, figsize=(15,11))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, col in enumerate(categorical_columns):\n  data[col].value_counts()[::-1].plot(kind='barh',ax=axes[i], title=col, fontsize=14)\n  axes[i].set_ylabel('')\nplt.tight_layout()","1067e36b":"fig, axes = plt.subplots(5,2, figsize=(10,20))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, col in enumerate(numerical_columns):\n    plot = sns.boxplot(data=data, y=col, x='Attrition_Flag', ax=axes[i])\nplt.tight_layout()","90e93d6c":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ndata['Attrition_Flag'] = label.fit_transform(data['Attrition_Flag'])\ndata.head()","6da6a422":"plt.figure(figsize=(12,7))\nax = sns.heatmap(data.corr(), annot=True,cmap=\"YlGnBu\")\ncbar = ax.collections[0].colorbar\ncbar.set_ticks([0, .25, .50, .75, 1])\ncbar.set_ticklabels(['0%', '25%','50%', '75%', '100%'])\nplt.show()","3e4ef692":"#performing one-hot encoding\nprint('Before doing one-hot encoding',data.shape)\ndata_dumm = pd.get_dummies(data, prefix_sep='_')\nprint('After doing one-hot encoding',data_dumm.shape)\ndata_dumm.head()","14736dc2":"# Before Removing Outliers\ndata_dumm['Attrition_Flag'].value_counts()","0b42a57a":"#Removing Outliers Function (Standard Deviation Method)\ndef remove_outliers(data_dumm, columns, tresh=3):\n    dataset = data_dumm.copy()\n    index_to_remove = []\n    value_to_remove = []\n    for col in columns:\n        array_to_remove = dataset[col].values\n        mean, std = np.mean(array_to_remove), np.std(array_to_remove)\n        z_score = np.abs((array_to_remove - mean) \/ std)\n        threshold = tresh\n        good = z_score < threshold\n        for i in range(len(good)):\n            if good[i] == False:\n                index_to_remove.append(i)\n                value_to_remove.append(array_to_remove[i])\n    index_to_remove = np.unique(np.array(index_to_remove))\n    print(\"Removed rows: \", len(index_to_remove))\n    dataset = dataset.drop(dataset.index[index_to_remove])\n    return dataset","0a33632c":"data_after_removing_outliers = remove_outliers(data_dumm, numerical_columns)","b850b425":"data_after_removing_outliers['Attrition_Flag'].value_counts()","f384eda3":"fig, axes = plt.subplots(5,2, figsize=(10,20))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, col in enumerate(numerical_columns):\n    plot = sns.boxplot(data=data_after_removing_outliers, y=col, x='Attrition_Flag', ax=axes[i])\nplt.tight_layout()","1c355fa6":"X = data_after_removing_outliers.drop(['Attrition_Flag'],1)\ny = data_after_removing_outliers['Attrition_Flag']","69ce5374":"# Train test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 123, stratify=y)","fdef9ac0":"from imblearn.combine import SMOTEENN\n\nsmoteen = SMOTEENN()\nX_train, y_train = smoteen.fit_resample(X_train, y_train)","19badce4":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","17f5150a":"X_train","8cc18db4":"import warnings\nwarnings.filterwarnings('ignore')","300b06ec":"# Cross K Validation\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score, recall_score\n\ndef evaluate_model(model, X_test, y_test):\n  y_pred = cross_val_predict(model, X_test, y_test, cv=5)\n  cm = confusion_matrix(y_test, y_pred)\n  print(cm)\n  print(\"ROC AUC Score: \", roc_auc_score(y_test, y_pred))\n  print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n  print(\"Precision Score: \", precision_score(y_test, y_pred))\n  print(\"Recall Score: \", recall_score(y_test, y_pred))","de88b155":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)","63c9a4f8":"evaluate_model(knn, X_test, y_test)","2cd80a49":"from sklearn.svm import SVC\n\nsvm = SVC()\nsvm.fit(X_train, y_train)","499fa18c":"evaluate_model(svm, X_test, y_test)","cc3b755b":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","b26dafa1":"evaluate_model(lr, X_test, y_test)","321a73b7":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)","77aca2b7":"evaluate_model(dt, X_test, y_test)","1cad5e2c":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)","32b2e898":"evaluate_model(rf, X_test, y_test)","6a74b948":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)","5af59ba4":"evaluate_model(xgb, X_test, y_test)","a3ac1914":"import lightgbm as lgb\n\ngbm = lgb.LGBMClassifier()\ngbm.fit(X_train, y_train)","1873028a":"evaluate_model(gbm, X_test, y_test)","67845636":"params = {\n    'max_depth':[3,7,9,'max'],\n    'n_estimators': [150, 200, 250],\n    'num_leaves': [25, 35, 40],\n    'learning_rate': [0.01, 0.22, 0.3]\n    }","fea59ed6":"gbm_grid = lgb.LGBMClassifier()\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(\n             estimator=gbm_grid,\n             param_grid=params,\n             scoring = 'roc_auc',\n             n_jobs = -1,\n             cv = 3,\n             verbose=2\n            )","3f882240":"grid.fit(X_train,y_train)","e25ac090":"grid.best_params_","4f82a1f5":"import lightgbm as lgb\n\ngbm_after_tuning = lgb.LGBMClassifier(learning_rate=0.22, max_depth=7, n_estimators=250, num_leaves=25)\ngbm_after_tuning.fit(X_train, y_train)","c021d32a":"evaluate_model(gbm_after_tuning, X_test, y_test)","b64d0e4f":"params_XGB = {\n    'eta':[0.1, 0.3, 0.7],\n    'max_depth': [3, 5, 6, 7],\n    'gamma': [0, 10, 50]\n    }","519f98da":"xgb_grid = XGBClassifier()\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(\n             estimator=xgb_grid,\n             param_grid=params_XGB,\n             scoring = 'roc_auc',\n             n_jobs = -1,\n             cv = 3,\n             verbose=2\n            )","fced2dff":"grid.fit(X_train,y_train)","995b3a1b":"grid.best_params_","f81d1ce6":"from xgboost import XGBClassifier\n\nxgb_after_tuning = XGBClassifier(eta= 0.3, gamma= 0, max_depth= 5)\nxgb_after_tuning.fit(X_train, y_train)","3dbb79de":"evaluate_model(xgb_after_tuning, X_test, y_test)","bdaf71c6":"# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\nfeature_imp = pd.DataFrame(sorted(zip(gbm.feature_importances_,X.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","59e42beb":"# Oversampling (SMOTEENN)","ad70eed7":"> Then the analysis continues by seeing how the level of correlation between all columns, but the first do Label Encoder to change \"Existing Customer\" = 1, and \"Attrited Customer\" = 0","a0c045dd":"## **Support Vector Machine**","fd49c79c":"## **K-Nearest Neighbor**","0efcf1a7":"# **Evaluation**","3b40fbba":"# **5. Develop Model**","fd112d25":"## **XG Boost**","182e64a0":"# **4. Data Pre-Processing**","1ceae2e0":"# Removing Ouliers (STD Method)","5fcfff1b":"# **Hyperparameter Tuning For XG Boost**","52676db7":"## **Decision Tree**","e2e7ced8":"Now we can know what column that has good corelation with \"attrition flag\" column","7cc275c9":"# One Hot Encoding for Categorical Value","5d9f2e40":"Problem :\n\nA manager at the bank is disturbed with more and more customers leaving their credit card services. They would really appreciate if one could predict for them who is gonna get churned so they can proactively go to the customer to provide them better services and turn customers decisions in the opposite direction. We have only 16.07% of customers who have churned. Thus, it's a bit difficult to train our model to predict churning customers.","ea50d741":"> **Analyze the data better focus on attrition_Flag column to dig more information about churning customers**\n\n> The first analysis is to look for outliers values, because these values must be determined to see the pattern of distribution so that we can study the data better.\n\n> To find outliers, it is necessary to pay attention to using a boxplot chart","423783e3":"# **Hyperparameter Tuning for Light GBM**","d2f181a3":"# **3. Analyze the Data**","6b8ee05c":"# Split Train and Test ","95d84043":"Because there are no null values and duplicated values found, it can be said that this data is clean, and can be continued for data analysis by looking at the distribution of the data.","891edaa2":"# Standard Scaling","c80d9b55":"# Label Eencoding for Target Value","65d63fd2":"## **Light GBM**","4abc38dd":"> Based on the note from the data source, I decided to ignore the last 2 columns of the data, so the first thing I did was I had to delete it so it wouldn't interfere when analyzing the overall data","08730bda":"## **Random Forest**","32921802":"## **Logistic Regression**","c93c8da8":"# **2. Identify the Data**","3815477c":"> Grouping columns by data type","daefb64b":"# 1. Understanding the Data","6d96e3d4":"# **Credit Card customers**\n\n**Predict Churning customers**"}}