{"cell_type":{"885efd9a":"code","d7449902":"code","ed9ca9f4":"code","adc09998":"code","66ad6006":"code","b6496481":"code","32820fd3":"code","e4e5c903":"code","0b31aeb5":"code","59f11025":"code","566ba158":"code","c532e617":"code","705e7c95":"code","67c46ece":"code","649b3893":"code","9f919617":"code","87837a26":"code","1909d5ad":"code","2d50b49b":"code","3032ca20":"code","93fea189":"code","f5bbb20f":"code","4cb56aa9":"code","ea9cde4e":"code","44d75ff0":"code","a228eea5":"code","0f3640f3":"code","40a1eab2":"code","2594eb18":"code","c33456d7":"code","a2637202":"code","daa35970":"code","d1306222":"code","a000ab0c":"code","ebcc8c2e":"code","bef0483e":"code","d7f97919":"code","6cbfe1df":"code","7a535fc7":"code","fc0ca338":"code","78c4dd98":"code","e3a519bc":"code","143f6dce":"code","0ec28eb4":"code","aeac3dd7":"markdown","ac8d7253":"markdown","6425531b":"markdown","9ce60f25":"markdown","31cb5b01":"markdown","32706670":"markdown","04df9c1b":"markdown","8f113284":"markdown","433ad704":"markdown","81f074e1":"markdown","9983e7d3":"markdown","caed934b":"markdown","2b973169":"markdown","82ad3254":"markdown","e384b38e":"markdown","ffed2896":"markdown","7360c587":"markdown","8d9f6e54":"markdown","226ea8b8":"markdown","76cbfbcb":"markdown","2b9169ee":"markdown"},"source":{"885efd9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7449902":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom pyearth import Earth\nfrom sklearn.tree import DecisionTreeRegressor\nimport xgboost as xgb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nimport warnings\nwarnings.simplefilter('ignore')","ed9ca9f4":"# Load Data\n\ndf = pd.read_csv(\"\/kaggle\/input\/cars-dataset-audi-bmw-ford-hyundai-skoda-vw\/cars_dataset.csv\")\nprint(\"Shape of Dataset:\", df.shape)\ndf.head()","adc09998":"# Data Information\ndf.info()","66ad6006":"# Descriptive statistics\ndf.describe()","b6496481":"# Show the distribution of car price \nsns.distplot(df['price'],color=\"blue\")","32820fd3":"# Find IQR\nQ1 = df['price'].quantile(0.25)\nQ3 = df['price'].quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","e4e5c903":"# Remove outliers with a criteria: 1.5 x IOR\ndf = df[~((df['price'] < (Q1 - 1.5 * IQR)) |(df['price'] > (Q3 + 1.5 * IQR)))]\ndf.shape","0b31aeb5":"# Show the distribution of price: outliers removed\nsns.distplot(df['price'], color=\"blue\")","59f11025":"# Show the list of car models\nprint(df['model'].unique().tolist())","566ba158":"# Show the frequency of each car model\nfor index, value in df['model'].value_counts().iteritems():\n    print(index, ': ', value)","c532e617":"# Show the value counts of transmission\nfor index, value in df['transmission'].value_counts().iteritems():\n    print(index, ': ', value)","705e7c95":"# Show the value counts of fuelType\nfor index, value in df['fuelType'].value_counts().iteritems():\n    print(index, ': ', value)","67c46ece":"# Show the value counts of Make\nfor index, value in df['Make'].value_counts().iteritems():\n    print(index, ': ', value)","649b3893":"# Representing categorical data using violin plots\nfig = plt.figure(figsize=(16,4))\nplt.subplot(1,3,1)\nsns.violinplot(x = 'transmission', y = 'price', data = df, palette=\"winter\")\nplt.subplot(1,3,2)\nsns.violinplot(x = 'fuelType', y = 'price', data = df, palette=\"winter\")\nplt.subplot(1,3,3)\nsns.violinplot(x = 'Make', y = 'price', data = df, palette=\"winter\")\nplt.tight_layout()\nplt.show()","9f919617":"# Create a list of continuous variables\ncont = [\"year\", \"price\", \"mileage\", \"tax\", \"mpg\", \"engineSize\"]\n\n# Create a dataframe of continuous variables\ndf_cont = df[cont]","87837a26":"# Visualize correlation between continuous variables\n\n# Compute the correlation matrix\ncorr = df_cont.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(8, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)","1909d5ad":"# Visualize regression between each feature and output variable\n\n# The number of variables\nk = 6\nfig = plt.figure(figsize=(12,12))\n# Correlations between each variable\ncorrmat = df_cont.corr()\n# Take k elements in descending order of coefficient \ncols = corrmat.nlargest(6, \"price\")[\"price\"].index\n# Calculate correlation\nfor i in np.arange(1,6):\n    regline = df_cont[cols[i]]\n    ax = fig.add_subplot(3,2,i)\n    sns.regplot(x=regline, y=df['price'], scatter_kws={\"color\": \"royalblue\", \"s\": 3},\n                line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","2d50b49b":"# Split X and y\nX = df.drop(['model', 'price'], axis=1)\ny = df['price']","3032ca20":"# Create dummies for categorical variables\n\n# subset all categorical variables\ncars_categorical = X.select_dtypes(include=['object'])\n# convert into dummies\ncars_dummies = pd.get_dummies(cars_categorical, drop_first=True)\n# drop categorical variables \nX = X.drop(list(cars_categorical.columns), axis=1)\n# concat dummy variables with X\nX = pd.concat([X, cars_dummies], axis=1)","93fea189":"# Scale the features\n\n# Store column names since the column names will be lost after scaling\ncols = X.columns\n\n# Scale the features and convert it back to a dataframe\nX = pd.DataFrame(scale(X))\n\n# Write in the column names again\nX.columns = cols\nX.columns","f5bbb20f":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    train_size=0.7,\n                                                    test_size = 0.3, random_state=100)","4cb56aa9":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\ny_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, y_pred)\nlr_mae = mean_absolute_error(y_test, y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","ea9cde4e":"# Evaluate the model based on the assumption of linear regression:\n\n# Assumption 1. The error terms are normally distributed with mean approximately 0.\n\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=50, color=\"blue\")\nfig.suptitle('Error Terms', fontsize=14)                  \nplt.xlabel('y_test-y_pred', fontsize=12)                  \nplt.ylabel('Index', fontsize=12)                          \nplt.show()","44d75ff0":"# Assumption 2: Homoscedasticity, i.e. the variance of the error term (y_true-y_pred) is constant.\n\nc = [i for i in range(len(y_pred))]\nfig = plt.figure()\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\", alpha=0.4)\nfig.suptitle('Error Terms', fontsize=14)               \nplt.xlabel('Index', fontsize=12)                      \nplt.ylabel('ytest-ypred', fontsize=12)                \nplt.show()","a228eea5":"# Assumption 3: There is little correlation between the predictors. i.e., Multicollinearity:\n\npredictors = ['year', 'mileage', 'tax', 'mpg', 'engineSize','transmission_Manual','transmission_Other', \n              'transmission_Semi-Auto', 'fuelType_Electric','fuelType_Hybrid', 'fuelType_Other', 'fuelType_Petrol', \n              'Make_Ford','Make_Hyundai', 'Make_audi', 'Make_skoda', 'Make_toyota', 'Make_vw']\n\n# Compute the correlation matrix\ncors = X.loc[:, list(predictors)].corr()\n\n# Generate a mask for the upper triangle\nmask_2 = np.triu(np.ones_like(cors, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cors, mask=mask_2, cmap=\"winter\", vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\nplt.show()","0f3640f3":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","40a1eab2":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","2594eb18":"# Initiate the model\nmars_model = Earth()\n\n# By default, we do not need to set any of the algorithm hyperparameters.\n# The algorithm automatically discovers the number and type of basis functions to use.\n\n# Fit the model\nmars_model.fit(X_train, y_train)\n\n# Making predictions\nmars_y_pred = mars_model.predict(X_test)\n\n# Performance Metrics\nmars_r2 = r2_score(y_test, mars_y_pred)\nmars_mae = mean_absolute_error(y_test, mars_y_pred)\n\n# Show the model performance\nprint(\"MARS R2: \", mars_r2)\nprint(\"MARS MAE: \", mars_mae)","c33456d7":"# Initiate the model\ndt_model = DecisionTreeRegressor()\n\n# Grid search\ndt_gs = GridSearchCV(dt_model,\n                     param_grid = {'max_depth': range(1, 11),\n                                   'min_samples_split': range(10, 60, 10)},\n                     cv=5,\n                     n_jobs=1,\n                     scoring='neg_mean_squared_error')\n\ndt_gs.fit(X_train, y_train)\n\nprint(dt_gs.best_params_)\nprint(-dt_gs.best_score_)","a2637202":"# Initiate the best model\ndt_model_best = DecisionTreeRegressor(max_depth=10, min_samples_split=20)\n\n# Fit the best model\ndt_model_best.fit(X_train, y_train)","daa35970":"# Make predictions\ndt_y_pred = dt_model_best.predict(X_test)\n\n# Performance metrics\ndt_r2 = r2_score(y_test, dt_y_pred)\ndt_mae = mean_absolute_error(y_test, dt_y_pred)\n\n# Show the model performance\nprint(\"DT R2: \", dt_r2)\nprint(\"DT MAE: \", dt_mae)","d1306222":"# Initiate the model\nxgb_model = xgb.XGBRegressor()\n\n# Grid search\n#xgb_gs = GridSearchCV(xgb_model,\n#                      param_grid = {'max_depth': range(8, 15),\n#                                   'min_samples_split': range(1, 11, 3)},\n#                      cv=5,\n#                      n_jobs=1,\n#                      scoring='neg_mean_squared_error')\n                      \n#xgb_gs.fit(X_train, y_train)\n\n#print(xgb_gs.best_params_)\n#print(-xgb_gs.best_score_)","a000ab0c":"# Initiate the best model\nxgb_model_best = xgb.XGBRegressor(max_depth=10, min_samples_split=10)\n\n# Fit the best model\nxgb_bst = xgb_model_best.fit(X_train, y_train)","ebcc8c2e":"# Make predictions\nxgb_y_pred = xgb_bst.predict(X_test)\n\n# Performance metrics\nxgb_r2 = r2_score(y_test, xgb_y_pred)\nxgb_mae = mean_absolute_error(y_test, xgb_y_pred)\n\n# Show the model performance\nprint(\"XGB R2: \", xgb_r2)\nprint(\"XGB MAE: \", xgb_mae)","bef0483e":"# Define a DNN\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model","d7f97919":"# Initiate DNN\ndnn = KerasRegressor(build_fn=create_model, epochs=300, batch_size=20, verbose=1)\n\n# Fit DNN\ndnn_history = dnn.fit(X_train, y_train)","6cbfe1df":"# Visualize the DNN learning\nloss_train = dnn_history.history['loss']\nepochs = range(1,301)\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_train, 'royalblue', label='Training loss', linewidth=3)\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","7a535fc7":"# Make predictions\ndnn_y_pred = dnn.predict(X_test)\n\n# Performance metrics\ndnn_r2 = r2_score(y_test, dnn_y_pred)\ndnn_mae = mean_absolute_error(y_test, dnn_y_pred)\n\n# Show the model performance\nprint(\"DNN R2: \", dnn_r2)\nprint(\"DNN MAE: \", dnn_mae)","fc0ca338":"results_table = pd.DataFrame([[np.mean(lr_r2), np.mean(lr_mae)],\n                             [np.mean(mars_r2), np.mean(mars_mae)],\n                             [np.mean(dt_r2), np.mean(dt_mae)],\n                             [np.mean(xgb_r2), np.mean(xgb_mae)],\n                             [np.mean(dnn_r2), np.mean(dnn_mae)]],\n                            columns=['R2', 'MAE'],\n                            index=[\"Linear Regression\",\"MARS\",\"Decision Tree\",\"XGBoost\",\"DNN\"])\npd.options.display.precision = 3\nresults_table","78c4dd98":"pred_table = pd.DataFrame({\"Linear Regression: Predicted Price\": y_pred,\n                           \"MARS: Predicted Price\": mars_y_pred,\n                           \"Decision Tree: Predicted Price\": dt_y_pred,\n                           \"XGBoost: Predicted Price\": xgb_y_pred,\n                           \"DNN: Predicted Price\": dnn_y_pred,\n                          \"Actual Price\": y_test})","e3a519bc":"# Visualize the predicted price and actual price\nfig = plt.figure(figsize=(10,10))\nplt.subplot(3,2,1)\nsns.regplot(x = 'Linear Regression: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,2)\nsns.regplot(x = 'MARS: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,3)\nsns.regplot(x = 'Decision Tree: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,4)\nsns.regplot(x = 'XGBoost: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,2,5)\nsns.regplot(x = 'DNN: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","143f6dce":"xgb_bst.feature_importances_","0ec28eb4":"fig = plt.figure(figsize=(6,6))\nsorted_idx = xgb_bst.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb_bst.feature_importances_[sorted_idx], color=\"royalblue\", alpha=0.9)\nplt.xlabel(\"Xgboost Feature Importance\")","aeac3dd7":"- Predict used car price by various regression models\n\n- Regression Models:\n  - Linear Regression\n  - Multivariate Adaptive Regression Splines\n  - Decision Tree Regressor\n  - XGBoost Regressor\n  - Deep Neural Network\n\n- Performace Metrics:\n  - R-Squared\n  - Mean Absolute Error","ac8d7253":"All VIF values are lower than 5. So, there is no possibility of multicollinearity.","6425531b":"## 2.1. Remove Outliers in Target Variable","9ce60f25":"## 3.5. Deep Neural Network","31cb5b01":"Since great many car models are contained, let's ignore car models in prediction.","32706670":"# 3.4. XGB Regression","04df9c1b":"## 3.1. Linear Regression","8f113284":"The second assumption seems to be met.","433ad704":"# Car Price Prediction","81f074e1":"## 2.4. Data Preparation for Modeling","9983e7d3":"Some features are highly correlated. So let's check the multicolliearity by VIF.","caed934b":"# 2. Data Exploration and Preprocessing","2b973169":"The first assumption seems to be met.","82ad3254":"# 1. Import Libraries and Load Dataset","e384b38e":"The distribution is right-skewed. Let's remove outliers using the IQR as the criteria.","ffed2896":"## 2.2. Exploration of Categorical Variables","7360c587":"## 2.3. Exploration of Continuous Variables","8d9f6e54":"There are no missing values in the dataset.","226ea8b8":"## 3.2. Multivariate Adaptive Regression Splines (MARS)","76cbfbcb":"# 3. Regression","2b9169ee":"## 3.3. Decision Tree Regression"}}