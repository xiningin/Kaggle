{"cell_type":{"2e0d120f":"code","499c7670":"code","e508e42a":"code","5ad6fd80":"code","e01d3711":"code","f6b5c429":"code","adf23fbb":"code","3f408a73":"markdown","2f5bfc98":"markdown","1d23df0c":"markdown","1f45c821":"markdown","7bb90810":"markdown","7b2c3fa6":"markdown","cfbf4f50":"markdown","7558e6c5":"markdown"},"source":{"2e0d120f":"import platform\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.utils.data import Dataset, DataLoader\n\nimport warnings\nwarnings.simplefilter('ignore')","499c7670":"class Config:\n    NB_EPOCHS = 10\n    LR = 1e-6\n    MAX_LEN = 185\n    N_SPLITS = 5\n    TRAIN_BS = 16\n    VALID_BS = 32\n    BERT_MODEL = '..\/input\/huggingface-bert\/bert-large-uncased'\n    MODEL_NAME = 'bert-large-uncased'\n    FILE_NAME = '..\/input\/commonlitreadabilityprize\/train.csv'\n    TOKENIZER = transformers.BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n    scaler = GradScaler()","e508e42a":"class BERTDataset(Dataset):\n    def __init__(self, review, target=None, is_test=False):\n        self.review = review\n        self.target = target\n        self.is_test = is_test\n        self.tokenizer = Config.TOKENIZER\n        self.max_len = Config.MAX_LEN\n    \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, idx):\n        review = str(self.review[idx])\n        review = ' '.join(review.split())\n        global inputs\n        \n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True\n        )        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        \n        if self.is_test:\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n            }\n        else:    \n            targets = torch.tensor(self.target[idx], dtype=torch.float)\n            return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': targets\n            }","5ad6fd80":"class Trainer:\n    def __init__(\n        self, \n        model, \n        optimizer, \n        scheduler, \n        train_dataloader, \n        valid_dataloader,\n        device\n    ):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.train_data = train_dataloader\n        self.valid_data = valid_dataloader\n        self.loss_fn = self.yield_loss\n        self.device = device\n        \n    def yield_loss(self, outputs, targets):\n        \"\"\"\n        This is the loss function for this task\n        \"\"\"\n        return torch.sqrt(nn.MSELoss()(outputs, targets))\n    \n    def train_one_epoch(self):\n        \"\"\"\n        This function trains the model for 1 epoch through all batches\n        \"\"\"\n        prog_bar = tqdm(enumerate(self.train_data), total=len(self.train_data))\n        self.model.train()\n        with autocast():\n            for idx, inputs in prog_bar:\n                ids = inputs['ids'].to(self.device, dtype=torch.long)\n                mask = inputs['mask'].to(self.device, dtype=torch.long)\n                ttis = inputs['token_type_ids'].to(self.device, dtype=torch.long)\n                targets = inputs['targets'].to(self.device, dtype=torch.float)\n\n                outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis).view(-1)\n\n                loss = self.loss_fn(outputs, targets)\n                prog_bar.set_description('loss: {:.2f}'.format(loss.item()))\n\n                Config.scaler.scale(loss).backward()\n                Config.scaler.step(self.optimizer)\n                Config.scaler.update()\n                self.optimizer.zero_grad()\n                self.scheduler.step()\n    \n    def valid_one_epoch(self):\n        \"\"\"\n        This function validates the model for one epoch through all batches of the valid dataset\n        It also returns the validation Root mean squared error for assesing model performance.\n        \"\"\"\n        prog_bar = tqdm(enumerate(self.valid_data), total=len(self.valid_data))\n        self.model.eval()\n        all_targets = []\n        all_predictions = []\n        with torch.no_grad():\n            for idx, inputs in prog_bar:\n                ids = inputs['ids'].to(self.device, dtype=torch.long)\n                mask = inputs['mask'].to(self.device, dtype=torch.long)\n                ttis = inputs['token_type_ids'].to(self.device, dtype=torch.long)\n                targets = inputs['targets'].to(self.device, dtype=torch.float)\n\n                outputs = self.model(ids=ids, mask=mask, token_type_ids=ttis).view(-1)\n                all_targets.extend(targets.cpu().detach().numpy().tolist())\n                all_predictions.extend(outputs.cpu().detach().numpy().tolist())\n\n        val_rmse_loss = np.sqrt(mean_squared_error(all_targets, all_predictions))\n        print('Validation RMSE: {:.2f}'.format(val_rmse_loss))\n        \n        return val_rmse_loss\n    \n    def get_model(self):\n        return self.model","e01d3711":"# Model\nclass BERT_BASE_UNCASED(nn.Module):\n    def __init__(self):\n        super(BERT_BASE_UNCASED, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(Config.MODEL_NAME)\n        self.drop = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.drop(output)\n        output = self.fc(output)\n        return output\n\nclass BERT_LARGE_UNCASED(nn.Module):\n    def __init__(self):\n        super(BERT_LARGE_UNCASED, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(Config.MODEL_NAME)\n        self.drop = nn.Dropout(0.3)\n        self.fc = nn.Linear(1024, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.drop(output)\n        output = self.fc(output)\n        return output\n    \nclass DBERT_BASE_UNCASED(nn.Module):\n    def __init__(self):\n        super(DBERT_BASE_UNCASED, self).__init__()\n        self.dbert = transformers.DistilBertModel.from_pretrained(Config.MODEL_NAME)\n        self.drop = nn.Dropout(0.2)\n        self.out = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask):\n        output = self.dbert(ids, attention_mask=mask, return_dict=False)\n        output = self.drop(output)\n        output = self.out(output)\n        return output","f6b5c429":"def yield_optimizer(model):\n    \"\"\"\n    Returns optimizer for specific parameters\n    \"\"\"\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {\n            \"params\": [\n                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.003,\n        },\n        {\n            \"params\": [\n                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    return transformers.AdamW(optimizer_parameters, lr=Config.LR)","adf23fbb":"# Training Code\nif __name__ == '__main__':\n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n        DEVICE = torch.device('cuda:0')\n    else:\n        print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n        DEVICE = torch.device('cpu')\n    \n    data = pd.read_csv(Config.FILE_NAME)\n    data = data.sample(frac=1).reset_index(drop=True)\n    data = data[['excerpt', 'target']]\n    \n    # Do Kfolds training and cross validation\n    kf = StratifiedKFold(n_splits=Config.N_SPLITS)\n    nb_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, 'bins'] = pd.cut(data['target'], bins=nb_bins, labels=False)\n    \n    for fold, (train_idx, valid_idx) in enumerate(kf.split(X=data, y=data['bins'].values)):\n        # Train for only 1 fold, you can train it for more.\n        if fold != 0:\n            continue\n        print(f\"\\nFold: {fold}\")\n        print(f\"{'-'*20}\\n\")\n        \n        train_data = data.loc[train_idx]\n        valid_data = data.loc[valid_idx]\n        \n        train_set = BERTDataset(\n            review = train_data['excerpt'].values,\n            target = train_data['target'].values\n        )\n\n        valid_set = BERTDataset(\n            review = valid_data['excerpt'].values,\n            target = valid_data['target'].values\n        )\n\n        train = DataLoader(\n            train_set,\n            batch_size = Config.TRAIN_BS,\n            shuffle = True,\n            num_workers=8\n        )\n\n        valid = DataLoader(\n            valid_set,\n            batch_size = Config.VALID_BS,\n            shuffle = False,\n            num_workers=8\n        )\n\n        model = BERT_LARGE_UNCASED().to(DEVICE)\n        nb_train_steps = int(len(train_data) \/ Config.TRAIN_BS * Config.NB_EPOCHS)\n        optimizer = yield_optimizer(model)\n        scheduler = transformers.get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=nb_train_steps\n        )\n\n        trainer = Trainer(model, optimizer, scheduler, train, valid, DEVICE)\n\n        best_loss = 100\n        for epoch in range(1, Config.NB_EPOCHS+1):\n            print(f\"\\n{'--'*5} EPOCH: {epoch} {'--'*5}\\n\")\n\n            # Train for 1 epoch\n            trainer.train_one_epoch()\n\n            # Validate for 1 epoch\n            current_loss = trainer.valid_one_epoch()\n\n            if current_loss < best_loss:\n                print(f\"Saving best model in this fold: {current_loss:.4f}\")\n                torch.save(trainer.get_model().state_dict(), f\"{Config.MODEL_NAME}_fold_{fold}.pt\")\n                best_loss = current_loss\n        \n        print(f\"Best RMSE in fold: {fold} was: {best_loss:.4f}\")\n        print(f\"Final RMSE in fold: {fold} was: {current_loss:.4f}\")","3f408a73":"Below is the main training code.\n\nI am also printing valuable information at multiple steps to make the code execution see more lively and not just absolute silence. It also used for future debugging purposes.\n\nThe below code for stratified kfolds is inspired from Abhishek Thakur's [Notebook](https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds) on creating Kfolds.","2f5bfc98":"Below is a custom `Trainer` class that I wrote from scratch to facilitate my training and validation sub-routines.\n\nThis class hence provides a very \"fastai\" type interface for doing training.","1d23df0c":"Below is a Custom dataset we are making. This dataset consists of 3 classes:\n* `__init__()`: Constructor function. Deals with class instance initalization, variable definition, etc\n* `__getitem__()`: This function deals with getting the elements when the dataset is called in iteration\n* `__len__()`: Length function overload. This function just returns the length of the dataset.","1f45c821":"Below is the function to get the appropriate optimizer.","7bb90810":"**That's it folks!**\n\n**If you like my work, don't forget to leave an upvote!**","7b2c3fa6":"Below are multiple model classes we can use for this task.\n\nIn this notebook, I am only training the model on `bert-large-uncased` but you can train it on whatever model you want.","cfbf4f50":"# PyTorch BERT Multi-Model Trainer + KFolds\ud83c\udfaf\n\nThis is training script written in Vanilla PyTorch with a Custom Trainer Class. I hope it can help you in developing more sophisticated models.\n\nAlong with BERT Base Uncased and Cased, I have also included DistilBert Base Model code. If you want to use that, you'll have to make some small changes in the code. Refer to official HuggingFace documentation for more.\n\nThink of this notebook has a skeleton for all BERT based Models (in-fact any PyTorch Hugginface model in reality). You can change chunks of code to suit your needs and it will work efficiently in most cases.\n\n\ud83d\udccc KFolds Inference (Submission) Notebook : https:\/\/www.kaggle.com\/heyytanay\/inference-0-6-lb-vanilla-pytorch-bert-starter\n\n\ud83d\udccc My EDA and Multi Linear Models Notebook: https:\/\/www.kaggle.com\/heyytanay\/commonlit-readability-eda-multi-models\n\n\n**Feel free to fork and change the models and do some preprocessing, but if you do please leave an upvote : )**","7558e6c5":"We define a Config class to store variables and functions that are to be used globally inside our training script.\nThis makes the code more modular and easy to approach at the same time."}}