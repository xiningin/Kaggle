{"cell_type":{"20c38844":"code","7604b274":"code","21fffc1f":"code","9a8beb54":"code","a6306dff":"code","c5a35ba6":"code","c9f5c49b":"code","0f4befdf":"code","c8cecc67":"code","dba2086d":"code","7fc8504d":"code","f7ec2a64":"code","91f4a137":"code","e895cf22":"code","b06815b7":"code","a7152fb1":"code","c463b0c5":"code","c07df177":"code","dbc8ea3c":"code","9ea242d6":"code","cc847d0b":"code","b7a9beaf":"code","c7147a9e":"code","02c89c66":"code","3119bcb3":"code","1e87a94c":"code","4300dc9e":"code","0a81eb7c":"code","1391e546":"code","b91ba421":"code","b24a4fb2":"code","14c89225":"code","7ed7eb20":"code","8b30109c":"markdown","0104ed97":"markdown","4c662343":"markdown","854cd67e":"markdown","0cabf6ac":"markdown","56f46243":"markdown","2349e2d4":"markdown","9300540f":"markdown","65553812":"markdown","6e2e5707":"markdown","c4055f9d":"markdown","6d4063a7":"markdown"},"source":{"20c38844":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nfrom sklearn import decomposition\nfrom sklearn.metrics import log_loss\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import GridSearchCV  \nimport imblearn\nfrom imblearn.over_sampling import RandomOverSampler\nfrom matplotlib import pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7604b274":"# Read training data\ndata = pd.read_csv('..\/input\/train.csv')\nfrom sklearn.model_selection import train_test_split\ntrain_X = data.drop([\"target\",\"id\"], axis=1)","21fffc1f":"le = LabelEncoder()\nle.fit(data[\"target\"])\ntrain_y = le.transform(data[\"target\"])","9a8beb54":"# split train set into 2 parts with same distribution: 80% train, 20% validation\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\nfor train_index, test_index in sss.split(train_X.values, train_y):\n    X_train = train_X.values[train_index]\n    X_val = train_X.values[test_index]\n\n    y_train = train_y[train_index]\n    y_val = train_y[test_index]","a6306dff":"missing_val_count_by_column = (data.isnull().sum())\nprint(missing_val_count_by_column.sum())","c5a35ba6":"data.describe()","c9f5c49b":"data[\"target\"].value_counts().plot.bar()","0f4befdf":"data[\"target\"].value_counts()","c8cecc67":"ros = RandomOverSampler()\nX_ros, y_ros = ros.fit_sample(X_train, y_train)\n\nunique, counts = np.unique(y_ros, return_counts=True)\n\nprint(np.asarray((unique, counts)).T)","dba2086d":"pd.Series(y_ros).value_counts().plot.bar()","7fc8504d":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_val_scaled = scaler.transform(X_val)","f7ec2a64":"test_data = pd.read_csv('..\/input\/test.csv')\ntest_X = test_data.drop([\"id\"], axis=1)\nscaler_all = StandardScaler()\ntrain_X_scaled = scaler_all.fit_transform(train_X)\ntest_X_scaled = scaler.transform(test_X)","91f4a137":"pca = decomposition.PCA(n_components=20)\npca.fit(X_train_scaled)\n\nX_train_pca = pca.transform(X_train_scaled)\nprint(pca.explained_variance_ratio_)\nprint(pca.explained_variance_)","e895cf22":"pca = decomposition.PCA()\npca.fit(X_train_scaled)\n\nX_train_pca = pca.transform(X_train_scaled)\n#print(np.cumsum(pca.explained_variance_ratio_))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","b06815b7":"xgb = XGBClassifier()\nxgb.fit(X_train_scaled, y_train)\npreds = xgb.predict_proba(X_val_scaled)\nscore = log_loss(y_val, preds)\nprint(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","a7152fb1":"xgb.get_params","c463b0c5":"from sklearn.model_selection import GridSearchCV\n\n\"\"\"\nparam_test = {\n    'n_estimators': [300],\n    'n_jobs': [4], #Number of jobs to run in parallel. -1 means using all processors\n}\ngsearch = GridSearchCV(estimator = XGBClassifier(), param_grid = param_test, scoring='neg_log_loss', n_jobs=-1,iid=False, cv=3,verbose=1, return_train_score=True)\ngsearch.fit(X_train_scaled,y_train)\npd.DataFrame(gsearch.cv_results_)\n\"\"\"","c07df177":"scores = []\nn_estimators = [100,200,400,450,500,525,550,600,700]\n\nfor nes in n_estimators:\n    xgb = XGBClassifier(learning_rate =0.1, n_estimators=nes, max_depth=7, min_child_weight=3, subsample=0.8, \n                             colsample_bytree=0.8, nthread=4, seed=42, objective='multi:softprob')\n    xgb.fit(X_train_scaled, y_train)\n    preds = xgb.predict_proba(X_val_scaled)\n    score = log_loss(y_val, preds)\n    scores.append(score)\n    print(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","dbc8ea3c":"plt.plot(n_estimators,scores,'o-')\nplt.ylabel(log_loss)\nplt.xlabel(\"n_estimator\")\nprint(\"best n_estimator {}\".format(n_estimators[np.argmin(scores)]))","9ea242d6":"scores_md = []\nmax_depths = [1,3,5,6,7,8,10]\n\nfor md in max_depths:\n    xgb = XGBClassifier(learning_rate =0.1, n_estimators=n_estimators[np.argmin(scores)], \n                        max_depth=md, min_child_weight=3, subsample=0.8, \n                        colsample_bytree=0.8, nthread=4, seed=42, objective='multi:softprob')\n    xgb.fit(X_train_scaled, y_train)\n    preds = xgb.predict_proba(X_val_scaled)\n    score = log_loss(y_val, preds)\n    scores_md.append(score)\n    print(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","cc847d0b":"plt.plot(max_depths,scores_md,'o-')\nplt.ylabel(log_loss)\nplt.xlabel(\"max_depth\")\nprint(\"best max_depth {}\".format(max_depths[np.argmin(scores_md)]))","b7a9beaf":"scores_mcw = []\nmin_child_weights = [1,2,3,4,5]\n\nfor mcw in min_child_weights:\n    xgb = XGBClassifier(learning_rate =0.1, n_estimators=n_estimators[np.argmin(scores)],\n                        max_depth=max_depths[np.argmin(scores_md)], \n                        min_child_weight=mcw, subsample=0.8, \n                        colsample_bytree=0.8, nthread=4, seed=42, objective='multi:softprob')\n    xgb.fit(X_train_scaled, y_train)\n    preds = xgb.predict_proba(X_val_scaled)\n    score = log_loss(y_val, preds)\n    scores_mcw.append(score)\n    print(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","c7147a9e":"plt.plot(min_child_weights,scores_mcw,\"o-\")\nplt.ylabel(log_loss)\nplt.xlabel(\"min_child_weight\")\nprint(\"best min_child_weight {}\".format(min_child_weights[np.argmin(scores_mcw)]))","02c89c66":"scores_ss = []\nsubsamples = [0.5,0.6,0.7,0.8,0.9,1]\n\nfor ss in subsamples:\n    xgb = XGBClassifier(learning_rate =0.1, n_estimators=n_estimators[np.argmin(scores)], \n                        max_depth=max_depths[np.argmin(scores_md)],\n                        min_child_weight=min_child_weights[np.argmin(scores_mcw)], subsample=ss, \n                        colsample_bytree=0.8, nthread=4, seed=42, objective='multi:softprob')\n    xgb.fit(X_train_scaled, y_train)\n    preds = xgb.predict_proba(X_val_scaled)\n    score = log_loss(y_val, preds)\n    scores_ss.append(score)\n    print(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","3119bcb3":"plt.plot(subsamples,scores_ss,\"o-\")\nplt.ylabel(log_loss)\nplt.xlabel(\"subsample\")\nprint(\"best subsample {}\".format(subsamples[np.argmin(scores_ss)]))","1e87a94c":"scores_cb = []\ncolsample_bytrees = [0.5,0.6,0.7,0.8,0.9,1]\n\nfor cb in colsample_bytrees:\n    xgb = XGBClassifier(learning_rate =0.1, n_estimators=n_estimators[np.argmin(scores)], \n                        max_depth=max_depths[np.argmin(scores_md)], \n                        min_child_weight=min_child_weights[np.argmin(scores_mcw)], \n                        subsample=subsamples[np.argmin(scores_ss)], \n                        colsample_bytree=cb, nthread=4, seed=42, objective='multi:softprob')\n    xgb.fit(X_train_scaled, y_train)\n    preds = xgb.predict_proba(X_val_scaled)\n    score = log_loss(y_val, preds)\n    scores_cb.append(score)\n    print(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","4300dc9e":"plt.plot(colsample_bytrees,scores_cb,\"o-\")\nplt.ylabel(log_loss)\nplt.xlabel(\"colsample_bytree\")\nprint(\"best colsample_bytree {}\".format(colsample_bytrees[np.argmin(scores_cb)]))","0a81eb7c":"scores_eta = []\netas = [0.001,0.01,0.1,0.2,0.3,0.5,1]\n\nfor eta in etas:\n    xgb = XGBClassifier(learning_rate =eta, n_estimators=n_estimators[np.argmin(scores)], \n                        max_depth=max_depths[np.argmin(scores_md)], \n                        min_child_weight=min_child_weights[np.argmin(scores_mcw)], \n                        subsample=subsamples[np.argmin(scores_ss)], \n                        colsample_bytree=colsample_bytrees[np.argmin(scores_cb)], \n                        nthread=4, seed=42, objective='multi:softprob')\n    xgb.fit(X_train_scaled, y_train)\n    preds = xgb.predict_proba(X_val_scaled)\n    score = log_loss(y_val, preds)\n    scores_eta.append(score)\n    print(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","1391e546":"plt.plot(etas,scores_eta,\"o-\")\nplt.ylabel(log_loss)\nplt.xlabel(\"eta\")\nprint(\"best eta {}\".format(etas[np.argmin(scores_eta)]))","b91ba421":"xgb = XGBClassifier(learning_rate =eta, n_estimators=n_estimators[np.argmin(scores)], \n                        max_depth=max_depths[np.argmin(scores_md)], \n                        min_child_weight=min_child_weights[np.argmin(scores_mcw)], \n                        subsample=subsamples[np.argmin(scores_ss)], \n                        colsample_bytree=colsample_bytrees[np.argmin(scores_cb)], \n                        nthread=4, seed=42, objective='multi:softprob')\ncalibrated_xgb = CalibratedClassifierCV(xgb, cv=5, method='isotonic')\ncalibrated_xgb.fit(X_train_scaled, y_train)\npreds = calibrated_xgb.predict_proba(X_val_scaled)\nscore = log_loss(y_val, preds)\nscores_eta.append(score)\nprint(\"test data log loss eval : {}\".format(log_loss(y_val,preds)))","b24a4fb2":"xgb = XGBClassifier(learning_rate =0.1, n_estimators=525, max_depth=8, min_child_weight=3, subsample=0.7, \n                       colsample_bytree=0.7, nthread=4, seed=42, objective='multi:softprob')\nmy_model = CalibratedClassifierCV(xgb, cv=5, method='isotonic')\nmy_model.fit(train_X_scaled,train_y)\ntest_preds = my_model.predict_proba(test_X_scaled)\noutput = pd.DataFrame(test_preds,columns=[\"Class_\"+str(i) for i in range(1,10)])\noutput.insert(loc=0, column='id', value=test_data.id)\noutput.to_csv('submission.csv', index=False)","14c89225":"test_data.head()","7ed7eb20":"output.head()","8b30109c":"# submission","0104ed97":"## PCA ?","4c662343":"# Representation of the target with numerical values ","854cd67e":"# XGBOOST","0cabf6ac":"At least 95% of the variance in the data can be explained by 77 components.","56f46243":"## Scaling","2349e2d4":"# Splitting the data (train.csv)","9300540f":"# Fitting and Tuning an Algorithm","65553812":"## Null values ?","6e2e5707":"# Preprocessing","c4055f9d":"## Determine number of components","6d4063a7":"## Balance in the class ?"}}