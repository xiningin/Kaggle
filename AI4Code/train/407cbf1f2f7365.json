{"cell_type":{"f345fe9a":"code","a0e440b8":"code","7610b0d3":"code","0ca2b3c5":"code","6b975fb5":"code","9bf1b659":"code","31399d8f":"code","08194e76":"code","01cdc150":"code","7168f4a1":"code","2f8bfde5":"code","c53d3ff9":"code","4b93f73b":"code","e9aac32c":"code","9e60d5cd":"code","100467c5":"code","b511086e":"code","b89a7237":"code","77a6c128":"code","f9896ffb":"code","a7dc8ca9":"code","37a8885c":"code","9e08b342":"code","2d641b9e":"code","3a03a77a":"code","a15813d6":"code","27cd510c":"code","4c422afb":"code","c5b9df76":"code","fa682d9e":"code","9067b8a2":"code","15bc66ee":"code","a5dd8456":"code","a394732f":"code","7a7fd16e":"code","5bb09a4f":"code","3279a584":"code","14de87c6":"code","10684f02":"code","9f2b0207":"code","541503cb":"code","7702256d":"code","142a1632":"code","0d30779b":"code","9143d93e":"code","f014256d":"code","4baafa7f":"code","3379f565":"code","1430212b":"code","032ea483":"code","9d676d07":"code","d174918a":"code","286eb685":"code","ed080978":"code","8320e3df":"code","318d76e8":"code","e8a35f19":"code","99530b5b":"code","ba33e9dd":"code","585ab19b":"code","fe6b82bf":"code","ae1e1ace":"code","a21ef87c":"code","f21e4b70":"markdown","f466be12":"markdown","42f5d424":"markdown","d847180a":"markdown","3b4605cc":"markdown","5652ccfa":"markdown","7e876e6e":"markdown","8c51ceda":"markdown","945b1b06":"markdown","68e6d868":"markdown","4bb6ec57":"markdown","3e4bb181":"markdown","2c8d5ae3":"markdown","8cac4f0b":"markdown","ac82a082":"markdown","dfa27f79":"markdown","45c31294":"markdown","5350123c":"markdown","2a77b5c1":"markdown","ce7096dc":"markdown","162ff748":"markdown","51ee3636":"markdown","417512af":"markdown","ed6c9ca5":"markdown","f9c32032":"markdown","24dfed39":"markdown","5d46225d":"markdown","bf5e6807":"markdown","1e10f493":"markdown","983549e3":"markdown","02aa68cb":"markdown","f04bcdc1":"markdown","c5ec35a6":"markdown"},"source":{"f345fe9a":"try:\n    from shaphypetune import BoostSearch, BoostBoruta, BoostRFE, BoostRFA\nexcept:\n    !pip install --upgrade shap-hypetune\n    from shaphypetune import BoostSearch, BoostBoruta, BoostRFE, BoostRFA\n","a0e440b8":"from shaphypetune import BoostSearch, BoostRFE, BoostRFA, BoostBoruta","7610b0d3":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_classification, make_regression\n\nfrom hyperopt import hp\nfrom hyperopt import Trials\n\nfrom lightgbm import *\n\n\nimport warnings\nwarnings.simplefilter('ignore')","0ca2b3c5":"#Loading the dataset\ntrain=pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest=pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")\nsample=pd.read_csv(\"..\/input\/song-popularity-prediction\/sample_submission.csv\")","6b975fb5":"train.head()","9bf1b659":"#Dividing the datasetin training and target variables\nX_clf=train.drop(columns=['id','song_popularity'])\ny_clf=train[['song_popularity']]","31399d8f":"X_clf","08194e76":"y_clf","01cdc150":"#Splitting the dataset into training and validation dataset\nX_clf_train, X_clf_valid, y_clf_train, y_clf_valid = train_test_split(X_clf, y_clf, test_size=0.3, shuffle=True,random_state=42)","7168f4a1":"#This parameters are for GridSearchCV\nparam_grid = {\n    'learning_rate': [0.2, 0.1],\n    'num_leaves': [25, 35],\n    'max_depth': [10, 12]\n}\n#This parameters are for RandomSearchCV\nparam_dist = {\n    'learning_rate': stats.uniform(0.09, 0.25),\n    'num_leaves': stats.randint(20,40),\n    'max_depth': [10, 12]\n}\n#This parameters are for Hyperopt\nparam_dist_hyperopt = {\n    'max_depth': 15 + hp.randint('num_leaves', 5), \n    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0)\n}\n","2f8bfde5":"#For classification problems\nclf_lgbm = LGBMClassifier(n_estimators=150, random_state=0, n_jobs=-1)\n\n#For regression problems\n\n#regr_lgbm = LGBMRegressor(n_estimators=150, random_state=0, n_jobs=-1) #just replace, \"clf_lgbm\"  with regr_lgbm\" in the whole code.","c53d3ff9":"### HYPERPARAM TUNING WITH GRID-SEARCH ###\n\nmodel = BoostSearch(clf_lgbm, param_grid=param_grid)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0)","4b93f73b":"model.estimator_, model.best_params_, model.best_score_","e9aac32c":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.predict_proba(X_clf_valid).shape)","9e60d5cd":"from sklearn.metrics import roc_auc_score\n\ndef AUC(y_true, y_hat):\n    return 'auc', roc_auc_score(y_true, y_hat), True","100467c5":"clf_lgbm = LGBMClassifier(n_estimators=150, random_state=0, n_jobs=-1, metric=\"custom\")","b511086e":"### HYPERPARAM TUNING WITH RANDOM-SEARCH ###\n\nmodel = BoostSearch(\n    clf_lgbm, param_grid=param_dist,\n    n_iter=8, sampling_seed=0\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)\n\n#Here you are getting the results in terms of AUC","b89a7237":"model.estimator_, model.best_params_, model.best_score_","77a6c128":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \nmodel.predict_proba(X_clf_valid).shape) ","f9896ffb":" model.predict_proba(X_clf_valid)","a7dc8ca9":"### HYPERPARAM TUNING WITH HYPEROPT ###\n\nmodel = BoostSearch(\n    clf_lgbm, param_grid=param_dist_hyperopt,\n    n_iter=8, sampling_seed=0\n)\nmodel.fit(\n    X_clf_train, y_clf_train, trials=Trials(), \n    eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","37a8885c":"model.estimator_, model.best_params_, model.best_score_","9e08b342":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.predict(X_clf_valid, pred_contrib=True).shape)","2d641b9e":"### BORUTA ###\n\nmodel = BoostBoruta(clf_lgbm, max_iter=200, perc=100)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","3a03a77a":"model.estimator_, model.n_features_","a15813d6":"model.ranking_","27cd510c":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict_proba(X_clf_valid).shape)","4c422afb":"### RECURSIVE FEATURE ELIMINATION (RFE) ###\n\nmodel = BoostRFE(clf_lgbm, min_features_to_select=1, step=1)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)\n","c5b9df76":"model.estimator_, model.n_features_","fa682d9e":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","9067b8a2":"### RECURSIVE FEATURE ADDITION (RFA) ###\n\nmodel = BoostRFA(clf_lgbm, min_features_to_select=1, step=1)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","15bc66ee":"model.estimator_, model.n_features_","a5dd8456":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","a394732f":"### BORUTA SHAP ###\n\nmodel = BoostBoruta(\n    clf_lgbm, max_iter=200, perc=100,\n    importance_type='shap_importances', train_importance=False\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","7a7fd16e":"model.estimator_, model.n_features_","5bb09a4f":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict_proba(X_clf_valid).shape)","3279a584":"### RECURSIVE FEATURE ELIMINATION (RFE) SHAP ###\n\nmodel = BoostRFE(\n    clf_lgbm, min_features_to_select=1, step=1,\n    importance_type='shap_importances', train_importance=False\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","14de87c6":"model.estimator_, model.n_features_","10684f02":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","9f2b0207":"### RECURSIVE FEATURE ADDITION (RFA) SHAP ###\n\nmodel = BoostRFA(\n    clf_lgbm, min_features_to_select=1, step=1,\n    importance_type='shap_importances', train_importance=False\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","541503cb":"model.estimator_, model.n_features_","7702256d":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","142a1632":"### HYPERPARAM TUNING WITH GRID-SEARCH + BORUTA ###\n\nmodel = BoostBoruta(clf_lgbm, param_grid=param_grid, max_iter=200, perc=100)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","0d30779b":"model.estimator_, model.best_params_, model.best_score_, model.n_features_","9143d93e":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict_proba(X_clf_valid).shape)","f014256d":"### HYPERPARAM TUNING WITH RANDOM-SEARCH + RECURSIVE FEATURE ELIMINATION (RFE) ###\n\nmodel = BoostRFE(\n    clf_lgbm, param_grid=param_dist, min_features_to_select=1, step=1,\n    n_iter=8, sampling_seed=0\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","4baafa7f":"model.estimator_, model.best_params_, model.best_score_, model.n_features_","3379f565":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","1430212b":"### HYPERPARAM TUNING WITH HYPEROPT + RECURSIVE FEATURE ADDITION (RFA) ###\n\nmodel = BoostRFA(\n    clf_lgbm, param_grid=param_dist_hyperopt, min_features_to_select=1, step=1,\n    n_iter=8, sampling_seed=0\n)\nmodel.fit(\n    X_clf_train, y_clf_train, trials=Trials(), \n    eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC\n)","032ea483":"model.estimator_, model.best_params_, model.best_score_, model.n_features_","9d676d07":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","d174918a":"### HYPERPARAM TUNING WITH GRID-SEARCH + BORUTA SHAP ###\n\nmodel = BoostBoruta(\n    clf_lgbm, param_grid=param_grid, max_iter=200, perc=100,\n    importance_type='shap_importances', train_importance=False\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","286eb685":"model.estimator_, model.best_params_, model.best_score_, model.n_features_","ed080978":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict_proba(X_clf_valid).shape)","8320e3df":"### HYPERPARAM TUNING WITH RANDOM-SEARCH + RECURSIVE FEATURE ELIMINATION (RFE) SHAP ###\n\nmodel = BoostRFE(\n    clf_lgbm, param_grid=param_dist, min_features_to_select=1, step=1,\n    n_iter=8, sampling_seed=0,\n    importance_type='shap_importances', train_importance=False\n)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)","318d76e8":"model.estimator_, model.best_params_, model.best_score_, model.n_features_","e8a35f19":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","99530b5b":"### HYPERPARAM TUNING WITH HYPEROPT + RECURSIVE FEATURE ADDITION (RFA) SHAP ###\n\nmodel = BoostRFA(\n    clf_lgbm, param_grid=param_dist_hyperopt, min_features_to_select=1, step=1,\n    n_iter=8, sampling_seed=0,\n    importance_type='shap_importances', train_importance=False\n)\nmodel.fit(\n    X_clf_train, y_clf_train, trials=Trials(), \n    eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC\n)","ba33e9dd":"model.estimator_, model.best_params_, model.best_score_, model.n_features_","585ab19b":"(model.score(X_clf_valid, y_clf_valid), \n model.predict(X_clf_valid).shape, \n model.transform(X_clf_valid).shape,\n model.predict(X_clf_valid, pred_contrib=True).shape)","fe6b82bf":"X_clf, y_clf = make_classification(n_samples=6000, n_features=20, n_classes=2, \n                                   n_informative=4, n_redundant=6, random_state=0)\n\nX_clf_train, X_clf_valid, y_clf_train, y_clf_valid = train_test_split(\n    X_clf, y_clf, test_size=0.3, shuffle=False)","ae1e1ace":"categorical_feature = [0,1,2]\n\nX_clf_train[:,categorical_feature] = (X_clf_train[:,categorical_feature]+100).clip(0).astype(int)\nX_clf_valid[:,categorical_feature] = (X_clf_valid[:,categorical_feature]+100).clip(0).astype(int)","a21ef87c":"### PASS category COLUMNS IN PANDAS DF ###\n\nmodel = BoostRFE(clf_lgbm, param_grid=param_grid, min_features_to_select=1, step=1)\nmodel.fit(X_clf_train, y_clf_train, eval_set=[(X_clf_valid, y_clf_valid)], early_stopping_rounds=6, verbose=0,eval_metric=AUC)\n\n","f21e4b70":"# **Importing necessary libraries**","f466be12":"# **Checking the best parameter & the best values**","42f5d424":"# **LGBM_usage**","d847180a":"# **Shap-hypetune main features**:\n\n*     designed for gradient boosting models, as LGBModel or XGBModel;\n*     developed to be integrable with the scikit-learn ecosystem;\n*     effective in both classification or regression tasks;\n*     customizable training process, supporting early-stopping and all the other fitting options available in the standard algorithms api;\n*      ranking feature selection algorithms: Recursive Feature Elimination (RFE); Recursive Feature Addition (RFA); or Boruta;\n*     classical boosting based feature importances or SHAP feature importances (the later can be computed also on the eval_set);\n*     apply grid-search, random-search, or bayesian-search (from hyperopt);\n*     parallelized computations with joblib.\n","3b4605cc":"# **Installation**","5652ccfa":"# **Features Selection**","7e876e6e":"# **Introduction:**\n\n\n## Shap-hypertune is a python package for simultaneous Hyperparameters Tuning and Features Selection for Gradient Boosting Models.\n\n## Thanks to the awesome github resporatory of [Marco Cerliani.](https:\/\/medium.com\/@cerlymarco)\n\n## Github link: [shap-hypetune](https:\/\/github.com\/cerlymarco\/shap-hypetune)\n\n## I came to know about this package from [Philip Vollet](https:\/\/www.linkedin.com\/posts\/philipvollet_datascience-deeplearning-machinelearning-activity-6893842562860609536-MxnB)'s linkedin post.\n","8c51ceda":"# **XGBoost_usage**","945b1b06":"For working with xgboost, we just need to define the XGBclassifier or XGBRegressor. Just replace, \"**clf_lgbm**\" with \"**clf_xgb**\" in the whole code.\n\n# **Example**,\n\n## For regression problem\n\n**regr_xgb = XGBRegressor(n_estimators=150, random_state=0, verbosity=0, n_jobs=-1)**\n\n## For classification problem\n**clf_xgb = XGBClassifier(n_estimators=150, random_state=0, verbosity=0, n_jobs=-1)**\n","68e6d868":"# > If you need to have a different metric, then you have to define it properly.\n> \n> Here I am giving an example for **roc_auc_score**","4bb6ec57":"> # Here you need to be careful about one thing. The \"**score**\" parameter.\n> The default metric is **Accuracy for classification, R2 for regression**\n> Signature: model.score(X, y, sample_weight=None)\n> \n# > Parameters\n> ----------\n> **X** : array-like of shape (n_samples, n_features)\n>     Test samples.\n> \n> **y** : array-like of shape (n_samples,)\n>     True values for X.\n> \n> **sample_weight** : array-like of shape (n_samples,), default=None\n>     Sample weights.\n> \n# > Returns\n> -------\n> **score** : float\n>     Accuracy for classification, R2 for regression.","3e4bb181":"# To predict on new values with probabilities, we need to use the **model.predict_proba** command","2c8d5ae3":"# **Checking the best parameter & the best values**","8cac4f0b":"# **Hyperparameter tuning with GridSearchCV**","ac82a082":"**We will be using three types of hyperparameter optimization techniques**\n\n* [GridSearchCV](https:\/\/towardsdatascience.com\/gridsearchcv-for-beginners-db48a90114ee)\n\n* [RandomSearchCV](https:\/\/thinkingneuron.com\/how-to-tune-hyperparameters-using-random-search-cv-in-python\/)\n\n* [Hyperopt](https:\/\/machinelearningmastery.com\/hyperopt-for-automated-machine-learning-with-scikit-learn\/)\n\nI have added the reference links for all of these techniques so that you can understand all the techniques easily.","dfa27f79":"# **Hyperparameters Tuning + Features Selection**","45c31294":"# **HYPERPARAM TUNING WITH HYPEROPT**","5350123c":"# **CUSTOM EVAL METRIC SUPPORT**","2a77b5c1":"# Listing the hyperparameters, to find its best values","ce7096dc":"# **Thank you**","162ff748":"# **Hyperparameters Tuning + Features Selection with SHAP**","51ee3636":"# **We will be using the dataset of song popularity prediction competition to get you familiarize with Shap Hypertune**","417512af":"# **Features Selection with SHAP**","ed6c9ca5":"# >  **Here are all the parameters that you can pass in Recursive Feature Elimination (RFE)**\n> \n> BoostRFE(\n>     estimator,\n>     *,\n>     min_features_to_select=None,\n>     step=1,\n>     param_grid=None,\n>     greater_is_better=False,\n>     importance_type='feature_importances',\n>     train_importance=True,\n>     n_iter=None,\n>     sampling_seed=None,\n>     verbose=1,\n>     n_jobs=None,\n> )\n> \n# > Parameters\n> ----------\n> **estimator** : object\n>     A supervised learning estimator of LGBModel or XGBModel type.\n> \n> **step** : int or float, default=1\n>     If greater than or equal to 1, then `step` corresponds to the\n>     (integer) number of features to remove at each iteration.\n>     If within (0.0, 1.0), then `step` corresponds to the percentage\n>     (rounded down) of features to remove at each iteration.\n>     Note that the last iteration may remove fewer than `step` features in\n>     order to reach `min_features_to_select`.\n> \n> **min_features_to_select** : int, default=None\n>     The minimum number of features to be selected. This number of features\n>     will always be scored, even if the difference between the original\n>     feature count and `min_features_to_select` isn't divisible by\n>     `step`. The default value for min_features_to_select is set to 1 when a\n>     eval_set is provided, otherwise it always corresponds to n_features \/\/ 2.\n> \n> **importance_type** : str, default='feature_importances'\n>      Which importance measure to use. It can be 'feature_importances'\n>      (the default feature importance of the gradient boosting estimator)\n>      or 'shap_importances'.\n> \n> **train_importance** : bool, default=True\n>     Effective only when importance_type='shap_importances'.\n>     Where to compute the shap feature importance: on train (True)\n>     or on eval_set (False).\n> \n> **param_grid** : dict, default=None\n>     Dictionary with parameters names (`str`) as keys and distributions\n>     or lists of parameters to try.\n>     None means no hyperparameters search.\n> \n> **greater_is_better** : bool, default=False\n>     Effective only when hyperparameters searching.\n>     Whether the quantity to monitor is a score function,\n>     meaning high is good, or a loss function, meaning low is good.\n> \n> **n_iter** : int, default=None\n>     Effective only when hyperparameters searching.\n>     Effective only for random or hyperopt serach.\n>     Number of parameter settings that are sampled.\n>     n_iter trades off runtime vs quality of the solution.\n> \n> **sampling_seed** : int, default=None\n>     Effective only when hyperparameters searching.\n>     Effective only for random or hyperopt serach.\n>     The seed used to sample from the hyperparameter distributions.\n> \n> **n_jobs** : int, default=None\n>     Effective only when hyperparameters searching without hyperopt.\n>     The number of jobs to run in parallel for model fitting.\n>     ``None`` means 1 using one processor. ``-1`` means using all\n>     processors.\n> \n> **verbose** : int, default=1\n>     Verbosity mode. <=0 silent all; ==1 print trial logs (when\n>     hyperparameters searching); >1 print feature selection logs plus\n>     trial logs (when hyperparameters searching).\n> \n# > Attributes\n> ----------\n> **estimator_** : estimator\n>     The fitted estimator with the select features and the optimal\n>     parameter combination (when hyperparameters searching).\n> \n> **n_features_** : int\n>     The number of selected features (from the best param config\n>     when hyperparameters searching).\n> \n> **ranking_** : ndarray of shape (n_features,)\n>     The feature ranking, such that ``ranking_[i]`` corresponds to the\n>     ranking position of the i-th feature (from the best param config\n>     when hyperparameters searching). Selected  features are assigned\n>     rank 1.\n> \n> **support_** : ndarray of shape (n_features,)\n>     The mask of selected features (from the best param config\n>     when hyperparameters searching).\n> \n> **score_history_** : list\n>     Available only when a eval_set is provided.\n>     Scores obtained reducing the features (from the best param config\n>     when hyperparameters searching).\n> \n> **best_params_** : dict\n>     Available only when hyperparameters searching.\n>     Parameter setting that gave the best results on the eval_set.\n> \n> **trials_** : list\n>     Available only when hyperparameters searching.\n>     A list of dicts. The dicts are all the parameter combinations tried\n>     and derived from the param_grid.\n> \n> **best_score_** : float\n>     Available only when hyperparameters searching.\n>     The best score achieved by all the possible combination created.\n> \n> **scores_** : list\n>     Available only when hyperparameters searching.\n>     The scores achieved on the eval_set by all the models tried.\n> \n> **best_iter_** : int\n>     Available only when hyperparameters searching.\n>     The boosting iterations achieved by the best parameters combination.\n> \n> **iterations_** : list\n>     Available only when hyperparameters searching.\n>     The boosting iterations of all the models tried.\n> \n> **boost_type_** : str\n>     The type of the boosting estimator (LGB or XGB).\n> ","f9c32032":"# **Hyperparameter tuning with RandomSearchCV**","24dfed39":"# > **Here are all the parameters that you can pass in BoostSearch.**\n> \n> BoostSearch(\n>     estimator,\n>     *,\n>     param_grid,\n>     greater_is_better=False,\n>     n_iter=None,\n>     sampling_seed=None,\n>     verbose=1,\n>     n_jobs=None,\n> )\n> \n> \n# > Parameters\n> ----------\n> **estimator** : object\n>     A supervised learning estimator of LGBModel or XGBModel type.\n> \n> **param_grid** : dict\n>     Dictionary with parameters names (`str`) as keys and distributions\n>     or lists of parameters to try. \n> \n> **greater_is_better** : bool, default=False\n>     Whether the quantity to monitor is a score function, \n>     meaning high is good, or a loss function, meaning low is good.\n> \n> **n_iter** : int, default=None\n>     Effective only for random or hyperopt search.\n>     Number of parameter settings that are sampled. \n>     n_iter trades off runtime vs quality of the solution.\n> \n> **sampling_seed** : int, default=None\n>     Effective only for random or hyperopt search.\n>     The seed used to sample from the hyperparameter distributions.\n> \n> **n_jobs** : int, default=None\n>     Effective only with grid and random search.\n>     The number of jobs to run in parallel for model fitting.\n>     ``None`` means 1 using one processor. ``-1`` means using all\n>     processors.\n> \n> **verbose** : int, default=1\n>     Verbosity mode. <=0 silent all; >0 print trial logs with the \n>     connected score.\n> \n# > Attributes\n> ----------\n> **estimator_** : estimator\n>     Estimator that was chosen by the search, i.e. estimator\n>     which gave the best score on the eval_set.\n> \n> **best_params_** : dict\n>     Parameter setting that gave the best results on the eval_set.\n> \n> **trials_** : list\n>     A list of dicts. The dicts are all the parameter combinations tried \n>     and derived from the param_grid.\n> \n> **best_score_** : float\n>     The best score achieved by all the possible combination created.\n> \n> **scores_** : list\n>     The scores achieved on the eval_set by all the models tried.\n> \n> **best_iter_** : int\n>     The boosting iterations achieved by the best parameters combination.\n> \n> **iterations_** : list\n>     The boosting iterations of all the models tried.\n> \n> **boost_type_** : str\n>     The type of the boosting estimator (LGB or XGB).\n> ","5d46225d":"# **Checking the best parameter & the best values**","bf5e6807":"# >  **Here are all the parameters that you can pass in Recursive Feature Addition (RFA)**\n> BoostRFA(\n>     estimator,\n>     *,\n>     min_features_to_select=None,\n>     step=1,\n>     param_grid=None,\n>     greater_is_better=False,\n>     importance_type='feature_importances',\n>     train_importance=True,\n>     n_iter=None,\n>     sampling_seed=None,\n>     verbose=1,\n>     n_jobs=None,\n> )\n> \n# > Parameters\n> ----------\n> **estimator** : object\n>     A supervised learning estimator of LGBModel or XGBModel type.\n> \n> **step** : int or float, default=1\n>     If greater than or equal to 1, then `step` corresponds to the\n>     (integer) number of features to remove at each iteration.\n>     If within (0.0, 1.0), then `step` corresponds to the percentage\n>     (rounded down) of features to remove at each iteration.\n>     Note that the last iteration may remove fewer than `step` features in\n>     order to reach `min_features_to_select`.\n> \n> **min_features_to_select** : int, default=None\n>     The minimum number of features to be selected. This number of features\n>     will always be scored, even if the difference between the original\n>     feature count and `min_features_to_select` isn't divisible by\n>     `step`. The default value for min_features_to_select is set to 1 when a\n>     eval_set is provided, otherwise it always corresponds to n_features \/\/ 2.\n> \n> **importance_type** : str, default='feature_importances'\n>      Which importance measure to use. It can be 'feature_importances'\n>      (the default feature importance of the gradient boosting estimator)\n>      or 'shap_importances'.\n> \n> **train_importance** : bool, default=True\n>     Effective only when importance_type='shap_importances'.\n>     Where to compute the shap feature importance: on train (True)\n>     or on eval_set (False).\n> \n> **param_grid** : dict, default=None\n>     Dictionary with parameters names (`str`) as keys and distributions\n>     or lists of parameters to try.\n>     None means no hyperparameters search.\n> \n> **greater_is_better** : bool, default=False\n>     Effective only when hyperparameters searching.\n>     Whether the quantity to monitor is a score function,\n>     meaning high is good, or a loss function, meaning low is good.\n> \n> **n_iter** : int, default=None\n>     Effective only when hyperparameters searching.\n>     Effective only for random or hyperopt serach.\n>     Number of parameter settings that are sampled.\n>     n_iter trades off runtime vs quality of the solution.\n> \n> **sampling_seed** : int, default=None\n>     Effective only when hyperparameters searching.\n>     Effective only for random or hyperopt serach.\n>     The seed used to sample from the hyperparameter distributions.\n> \n> **n_jobs** : int, default=None\n>     Effective only when hyperparameters searching without hyperopt.\n>     The number of jobs to run in parallel for model fitting.\n>     ``None`` means 1 using one processor. ``-1`` means using all\n>     processors.\n> \n> **verbose** : int, default=1\n>     Verbosity mode. <=0 silent all; ==1 print trial logs (when\n>     hyperparameters searching); >1 print feature selection logs plus\n>     trial logs (when hyperparameters searching).\n> \n# > Attributes\n> ----------\n> **estimator_** : estimator\n>     The fitted estimator with the select features and the optimal\n>     parameter combination (when hyperparameters searching).\n> \n> **n_features_** : int\n>     The number of selected features (from the best param config\n>     when hyperparameters searching).\n> \n> **ranking_** : ndarray of shape (n_features,)\n>     The feature ranking, such that ``ranking_[i]`` corresponds to the\n>     ranking position of the i-th feature (from the best param config\n>     when hyperparameters searching). Selected  features are assigned\n>     rank 1.\n> \n> **support_** : ndarray of shape (n_features,)\n>     The mask of selected features (from the best param config\n>     when hyperparameters searching).\n> \n> **score_history_** : list\n>     Available only when a eval_set is provided.\n>     Scores obtained reducing the features (from the best param config\n>     when hyperparameters searching).\n> \n> **best_params_** : dict\n>     Available only when hyperparameters searching.\n>     Parameter setting that gave the best results on the eval_set.\n> \n> **trials_** : list\n>     Available only when hyperparameters searching.\n>     A list of dicts. The dicts are all the parameter combinations tried\n>     and derived from the param_grid.\n> \n> **best_score_** : float\n>     Available only when hyperparameters searching.\n>     The best score achieved by all the possible combination created.\n> \n> **scores_** : list\n>     Available only when hyperparameters searching.\n>     The scores achieved on the eval_set by all the models tried.\n> \n> **best_iter_** : int\n>     Available only when hyperparameters searching.\n>     The boosting iterations achieved by the best parameters combination.\n> \n> **iterations_** : list\n>     Available only when hyperparameters searching.\n>     The boosting iterations of all the models tried.\n> \n> **boost_type_** : str\n>     The type of the boosting estimator (LGB or XGB).\n> ","1e10f493":"**We will first check for LGBM_usage of shap-hypertune.**","983549e3":"**We will be using three types of features selection techniques**\n\n* [SHAP](https:\/\/towardsdatascience.com\/shap-for-feature-selection-and-hyperparameter-tuning-a330ec0ea104)\n\n* [Boruta](https:\/\/towardsdatascience.com\/boruta-and-shap-for-better-feature-selection-20ea97595f4a)\n\n* [Recursive Feature Selection](https:\/\/towardsdatascience.com\/recursive-feature-selection-addition-or-elimination-755e5d86a791)\n\nI have added the reference links for all of these techniques so that you can understand all the techniques easily.","02aa68cb":"# **CATEGORICAL FEATURE SUPPORT**\n\n> You can also work with categorical features.\n> \n> Here is a demo of including categorical features","f04bcdc1":"# > **Here are all the parameters that you can pass in BoostBoruta.**\n> BoostBoruta(\n>     estimator,\n>     *,\n>     perc=100,\n>     alpha=0.05,\n>     max_iter=100,\n>     early_stopping_boruta_rounds=None,\n>     param_grid=None,\n>     greater_is_better=False,\n>     importance_type='feature_importances',\n>     train_importance=True,\n>     n_iter=None,\n>     sampling_seed=None,\n>     verbose=1,\n>     n_jobs=None,\n> )\n> \n> \n# > Parameters\n> ----------\n> **estimator** : object\n>     A supervised learning estimator of LGBModel or XGBModel type.\n> \n> **perc** : int, default=100\n>     Threshold for comparison between shadow and real features.\n>     The lower perc is the more false positives will be picked as relevant\n>     but also the less relevant features will be left out.\n>     100 correspond to the max.\n> \n> **alpha** : float, default=0.05\n>     Level at which the corrected p-values will get rejected in the\n>     correction steps.\n> \n> **max_iter** : int, default=100\n>     The number of maximum Boruta iterations to perform.\n> \n> **early_stopping_boruta_rounds** : int, default=None\n>     The maximum amount of iterations without confirming a tentative\n>     feature. Use early stopping to terminate the selection process\n>     before reaching `max_iter` iterations if the algorithm cannot\n>     confirm a tentative feature after N iterations.\n>     None means no early stopping search.\n> \n> **importance_type** : str, default='feature_importances'\n>      Which importance measure to use. It can be 'feature_importances'\n>      (the default feature importance of the gradient boosting estimator)\n>      or 'shap_importances'.\n> \n> **train_importance** : bool, default=True\n>     Effective only when importance_type='shap_importances'.\n>     Where to compute the shap feature importance: on train (True)\n>     or on eval_set (False).\n> \n> **param_grid** : dict, default=None\n>     Dictionary with parameters names (`str`) as keys and distributions\n>     or lists of parameters to try.\n>     None means no hyperparameters search.\n> \n> **greater_is_better** : bool, default=False\n>     Effective only when hyperparameters searching.\n>     Whether the quantity to monitor is a score function,\n>     meaning high is good, or a loss function, meaning low is good.\n> \n> **n_iter** : int, default=None\n>     Effective only when hyperparameters searching.\n>     Effective only for random or hyperopt seraches.\n>     Number of parameter settings that are sampled.\n>     n_iter trades off runtime vs quality of the solution.\n> \n> **sampling_seed** : int, default=None\n>     Effective only when hyperparameters searching.\n>     Effective only for random or hyperopt serach.\n>     The seed used to sample from the hyperparameter distributions.\n> \n> **n_jobs** : int, default=None\n>     Effective only when hyperparameters searching without hyperopt.\n>     The number of jobs to run in parallel for model fitting.\n>     ``None`` means 1 using one processor. ``-1`` means using all\n>     processors.\n> \n> **verbose** : int, default=1\n>     Verbosity mode. <=0 silent all; ==1 print trial logs (when\n>     hyperparameters searching); >1 print feature selection logs plus\n>     trial logs (when hyperparameters searching).\n> \n# > Attributes\n> ----------\n> **estimator_** : estimator\n>     The fitted estimator with the select features and the optimal\n>     parameter combination (when hyperparameters searching).\n> \n> **n_features_** : int\n>     The number of selected features (from the best param config\n>     when hyperparameters searching).\n> \n> **ranking_** : ndarray of shape (n_features,)\n>     The feature ranking, such that ``ranking_[i]`` corresponds to the\n>     ranking position of the i-th feature (from the best param config\n>     when hyperparameters searching). Selected features are assigned\n>     rank 1 (2: tentative upper bound, 3: tentative lower bound, 4:\n>     rejected).\n> \n> **support_** : ndarray of shape (n_features,)\n>     The mask of selected features (from the best param config\n>     when hyperparameters searching).\n> \n> **importance_history_** : ndarray of shape (n_features, n_iters)\n>     The importance values for each feature across all iterations.\n> \n> **best_params_** : dict\n>     Available only when hyperparameters searching.\n>     Parameter setting that gave the best results on the eval_set.\n> \n> **trials_** : list\n>     Available only when hyperparameters searching.\n>     A list of dicts. The dicts are all the parameter combinations tried\n>     and derived from the param_grid.\n> \n> **best_score_** : float\n>     Available only when hyperparameters searching.\n>     The best score achieved by all the possible combination created.\n> \n> **scores_** : list\n>     Available only when hyperparameters searching.\n>     The scores achived on the eval_set by all the models tried.\n> \n> **best_iter_** : int\n>     Available only when hyperparameters searching.\n>     The boosting iterations achieved by the best parameters combination.\n> \n> **iterations_** : list\n>     Available only when hyperparameters searching.\n>     The boosting iterations of all the models tried.\n> \n> **boost_type_** : str\n>     The type of the boosting estimator (LGB or XGB).","c5ec35a6":"# **Now you just have to define the metric type as \"custom\" in your classifier and add the metric funtion in model.fit**"}}