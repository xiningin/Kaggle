{"cell_type":{"aaf5d3de":"code","43d2b08a":"code","d54b3ac4":"code","e4c36e7c":"code","133b85e2":"code","0f68d55a":"code","7dc4c07a":"code","e8e190ce":"code","92ed2d96":"code","8cc006dc":"code","29112a1b":"code","8c2275ad":"markdown","2ad4522b":"markdown","f563d76a":"markdown","f43b8c9c":"markdown","6339377f":"markdown","08a42ea9":"markdown","82f4fdbb":"markdown","05b26325":"markdown","a4c8e4b5":"markdown"},"source":{"aaf5d3de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nfrom scipy.spatial import distance\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43d2b08a":"DataSet = pd.DataFrame()\nDataSet = pd.read_csv(\"\/kaggle\/input\/diabetes\/diabetes.csv\")\nDataSet.head(10)","d54b3ac4":"for column in DataSet.columns[1:-3]:\n    DataSet[column].replace(0,np.NaN, inplace=True)\n    DataSet[column].fillna(round(DataSet[column].mean(skipna=True)), inplace=True)\nDataSet.head(10)","e4c36e7c":"X = DataSet.iloc[:, :8]\ny = DataSet.iloc[:, 8:]","133b85e2":"# Split data into 20% for testing and 80% for training\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=0)","0f68d55a":"# Self explainatory: This function will find distance of data points in testing dataset with all points in training DataSet\ndef distance_with_all_train_Points(X_train, X_test):\n    # find the distance with all the Training points\n    eculidean_distance = {}\n    for data_point in X_test.itertuples():\n        for point in X_train.itertuples():\n            eculidean_distance[tuple([list(data_point)[0],list(point)[0]])] = distance.euclidean(list(data_point)[1:], list(point)[1:])\n    return eculidean_distance","7dc4c07a":"def k_nearest_neighbors(eculidean_distance, X_test, y, k):\n        \n        # Here i is just a counter. While, k is to indicate how muc neighbors we gonna consider. Here k_nearest neighbors are taken as k=5\n        # yes would be to check iff label is 1 (i.e diabaties=1), and if there isn't diabaties no would be incremented. We'll check it on\n        # nearest neighbors of a data point.\n        # Output_labels is to store the labels predicted based on the nearest neighbors.\n        all_neighbours = {}\n        Output_labels = []\n        for data_point in X_test.itertuples():\n            for key, value in eculidean_distance.items():\n                if key[0] == list(data_point)[0]:\n                    all_neighbours[key] = value\n                else:\n                    continue\n            i, yes, no = 0, 0, 0\n            for item in sorted(all_neighbours.items(), key = lambda x: x[1]):\n                if i < k:\n                    if(y.iloc[item[0][1]][\"Outcome\"] == 1):\n                        yes += 1\n                    else:\n                        no += 1\n                    i += 1\n        # Till this point, we know that how much nearest neighbours have label of one and label of 0 for having diabaties or not.\n        # On the basis of this we'll assign label to new data Point.\n            if(yes > no):\n                Output_labels.append(1)\n            else:\n                Output_labels.append(0)\n            all_neighbours.clear()\n            \n        return Output_labels","e8e190ce":"eculidean_distance = distance_with_all_train_Points(X_train, X_test)\nOutput_labels = k_nearest_neighbors(eculidean_distance, X_test, y, 5)","92ed2d96":"neighbor = KNeighborsClassifier(n_neighbors=5)\nneighbor.fit(X_train, np.array(y_train).ravel())","8cc006dc":"y_pred = neighbor.predict(X_test)\ny_pred","29112a1b":"neighbor.score(X_test, Output_labels)","8c2275ad":"# As we have labels, which we have predicted by checking the k nearest neighbors above (Output_labels) which are real labels, and now we have got predictions by scikit learn. We'll compare both predicted and actual labels to get accuracy score.","2ad4522b":"# Find distance of data points in testing dataset with all points in training DataSet","f563d76a":"# Here we are just calling that functions to get labels of testing dataset","f43b8c9c":"# Split DataSet into Training and Testing","6339377f":"# Replace zeros with null values and then with mean of respective columns for these columns\n# [Glucose, BloodPressure, SkinThickness, Insulin, BMI]","08a42ea9":"# Prediction using scikit learn model","82f4fdbb":"# Import DataSet named diabetes.csv available on kaggle","05b26325":"# This function will take the eculidean_distances and then sort them, for checking which data points are nearest neighbor to the point.  And then it will check the labels of k nearest neighbors, here we are using k=5. Depending on that lebels, this function will give labesls to all testing dataSet and return Output_labels.","a4c8e4b5":"# Now, we are just using sckit learn library to use it's KNeighborsClassifier(). We'll train and then predict by using this model."}}