{"cell_type":{"d393fac0":"code","38666b54":"code","28ed7e73":"code","5942d78d":"code","fcdc780e":"code","0e61ec1f":"code","8c1b7e98":"code","a2e83d2c":"code","da82b61f":"code","4e70ef9c":"code","cb232be9":"code","547ee695":"code","c0d28306":"code","ebecd8db":"code","13e19e77":"code","ae7ffff6":"code","6aa06457":"code","ccd18e45":"code","451579e4":"code","8bbb9bb2":"code","d3c978d2":"code","3111bffa":"code","4b98be4d":"code","b56a44f4":"code","ef34e178":"code","a9e3ddce":"code","360378ae":"code","251a3a53":"code","7b78a76f":"code","68a5d925":"code","e1ecd304":"code","ea39780f":"markdown","f389ebf8":"markdown","13ff2dd6":"markdown","f7db7acd":"markdown","85b39cf9":"markdown","d960f49a":"markdown","f2e5612f":"markdown","e88924c0":"markdown","20442357":"markdown","95455764":"markdown","d4e1d387":"markdown","ecf617b3":"markdown","944a9138":"markdown","477c7ec7":"markdown","d516b28d":"markdown","5939010d":"markdown","a1dbdd5d":"markdown","331a25f3":"markdown"},"source":{"d393fac0":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom hyperopt import tpe,hp,Trials\nfrom hyperopt.fmin import fmin\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","38666b54":"adm_pred1=pd.read_csv('..\/input\/Admission_Predict.csv',index_col=0)\nadm_pred1_1=pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv',index_col=0)","28ed7e73":"data=pd.concat([adm_pred1,adm_pred1_1])\ndata.sample(5)","5942d78d":"data.shape","fcdc780e":"data.info()","0e61ec1f":"data.describe()","8c1b7e98":"sns.distplot(data['GRE Score'],kde=True)","a2e83d2c":"sns.distplot(data['TOEFL Score'],kde=True)","da82b61f":"sns.distplot(data['CGPA'],kde=True)","4e70ef9c":"data.rename(columns={'LOR ':'LOR','Chance of Admit ':'CoA'},inplace=True)","cb232be9":"plt.subplots(2,2,figsize=(10,8))\nplt.subplot(2,2,1)\nsns.countplot(data['SOP'])\nplt.subplot(2,2,2)\nsns.countplot(data['LOR'])\nplt.subplot(2,2,3)\nsns.countplot(data['University Rating'])\nplt.subplot(2,2,4)\nsns.countplot(data['Research'])\n\nplt.tight_layout()","547ee695":"data['CoA'].plot.hist()","c0d28306":"\nsns.lmplot(x='GRE Score',y='CoA',data=data)\n","ebecd8db":"sns.jointplot(x='TOEFL Score',y='CoA',data=data)","13e19e77":"data.plot.scatter(x='CGPA',y='CoA')","ae7ffff6":"plt.subplots(2,2,figsize=(10,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='SOP',y='CoA',data=data)\nplt.subplot(2,2,2)\nsns.boxplot(x='LOR',y='CoA',data=data)\nplt.subplot(2,2,3)\nsns.stripplot(x='University Rating',y='CoA',data=data)\nplt.subplot(2,2,4)\nsns.boxenplot(x='Research',y='CoA',data=data)","6aa06457":"data.isnull().sum()","ccd18e45":"\n\nX=data.drop('CoA',axis=1)\nY=data['CoA']\ntrain_X,val_X,train_y,val_y=train_test_split(X,Y,test_size=0.2,random_state=1)\n\n","451579e4":"lr=LinearRegression()\nlr.fit(train_X,train_y)\npred=lr.predict(val_X)\nscore=mean_squared_error(val_y,pred)\nprint(score)","8bbb9bb2":"\n\nrfr=RandomForestRegressor(random_state=1)\nrfr.fit(train_X,train_y)\npred_rfr=rfr.predict(val_X)\nscore_rfr=mean_squared_error(pred_rfr,val_y)\nprint(score_rfr)","d3c978d2":"\n\nXgb=xgb.XGBRegressor(random_state=1)\nXgb.fit(train_X,train_y)\npred_xgb=Xgb.predict(val_X)\nscore_xgb=mean_squared_error(val_y,pred_xgb)\nprint(score_xgb)","3111bffa":"\n\nseed=2\ndef objective(params):\n    est=int(params['n_estimators'])\n    md=int(params['max_depth'])\n    msl=int(params['min_samples_leaf'])\n    mss=int(params['min_samples_split'])\n    model=RandomForestRegressor(n_estimators=est,max_depth=md,min_samples_leaf=msl,min_samples_split=mss)\n    model.fit(train_X,train_y)\n    pred=model.predict(val_X)\n    score=mean_squared_error(val_y,pred)\n    return score\n\ndef optimize(trial):\n    params={'n_estimators':hp.uniform('n_estimators',100,500),\n           'max_depth':hp.uniform('max_depth',5,20),\n           'min_samples_leaf':hp.uniform('min_samples_leaf',1,5),\n           'min_samples_split':hp.uniform('min_samples_split',2,6)}\n    best=fmin(fn=objective,space=params,algo=tpe.suggest,trials=trial,max_evals=500,rstate=np.random.RandomState(seed))\n    return best\n\ntrial=Trials()\nbest=optimize(trial)\n\n\n        \n    ","4b98be4d":"print(best)","b56a44f4":"for t in trial.trials[:2]:\n    print (t)","ef34e178":"TID=[t['tid'] for t in trial.trials]\nLoss=[t['result']['loss'] for t in trial.trials]\nmaxd=[t['misc']['vals']['max_depth'][0] for t in trial.trials]\nnest=[t['misc']['vals']['n_estimators'][0] for t in trial.trials]\nmin_ss=[t['misc']['vals']['min_samples_split'][0] for t in trial.trials]\nmin_sl=[t['misc']['vals']['min_samples_leaf'][0] for t in trial.trials]\n\nhyperopt_rfr=pd.DataFrame({'tid':TID,'loss':Loss,\n                          'max_depth':maxd,'n_estimators':nest,\n                          'min_samples_split':min_ss, 'min_samples_leaf':min_sl})\n\n","a9e3ddce":"plt.subplots(3,2,figsize=(10,10))\nplt.subplot(3,2,1)\nsns.scatterplot(x='tid',y='max_depth',data=hyperopt_rfr)\nplt.subplot(3,2,2)\nsns.scatterplot(x='tid',y='loss',data=hyperopt_rfr)\nplt.subplot(3,2,3)\nsns.scatterplot(x='tid',y='n_estimators',data=hyperopt_rfr)\nplt.subplot(3,2,4)\nsns.scatterplot(x='tid',y='min_samples_leaf',data=hyperopt_rfr)\nplt.subplot(3,2,5)\nsns.scatterplot(x='tid',y='min_samples_split',data=hyperopt_rfr)\n\nplt.tight_layout()","360378ae":"\nseed=5\ndef objective2(params):\n    est=int(params['n_estimators'])\n    md=int(params['max_depth'])\n    learning=params['learning_rate']\n    \n    \n    model=xgb.XGBRegressor(n_estimators=est,max_depth=md,learning_rate=learning)\n    model.fit(train_X,train_y)\n    pred=model.predict(val_X)\n    score=mean_squared_error(val_y,pred)\n    return score\n\ndef optimize2(trial):\n    params={'n_estimators':hp.uniform('n_estimators',100,500),\n           'max_depth':hp.uniform('max_depth',5,20),\n           'learning_rate':hp.uniform('learning_rate',0.01,0.1)}\n    best2=fmin(fn=objective2,space=params,algo=tpe.suggest,trials=trial,max_evals=500,rstate=np.random.RandomState(seed))\n    return best2\n\ntrial2=Trials()\nbest2=optimize2(trial2)","251a3a53":"print(best2)","7b78a76f":"TID2=[t['tid'] for t in trial2.trials]\nLoss2=[t['result']['loss'] for t in trial2.trials]\nmaxd2=[t['misc']['vals']['max_depth'][0] for t in trial2.trials]\nnest2=[t['misc']['vals']['n_estimators'][0] for t in trial2.trials]\nlrt=[t['misc']['vals']['learning_rate'][0] for t in trial2.trials]\n\n\nhyperopt_xgb=pd.DataFrame({'tid':TID2,'loss':Loss2,\n                          'max_depth':maxd2,'n_estimators':nest2,\n                          'learning_rate':lrt})","68a5d925":"plt.subplots(2,2,figsize=(10,10))\nplt.subplot(2,2,1)\nsns.scatterplot(x='tid',y='max_depth',data=hyperopt_xgb)\nplt.subplot(2,2,2)\nsns.scatterplot(x='tid',y='loss',data=hyperopt_xgb)\nplt.subplot(2,2,3)\nsns.scatterplot(x='tid',y='n_estimators',data=hyperopt_xgb)\nplt.subplot(2,2,4)\nsns.scatterplot(x='tid',y='learning_rate',data=hyperopt_xgb)\n\n\nplt.tight_layout()","e1ecd304":"\nrfr_opt=RandomForestRegressor(n_estimators=151,max_depth=17,min_samples_split=2,min_samples_leaf=1)\nrfr_opt.fit(train_X,train_y)\npred_rfr_opt=rfr_opt.predict(val_X)\nscore_rfr_opt=mean_squared_error(val_y,pred_rfr_opt)\nprint(score_rfr_opt)\n\nxgb_opt=xgb.XGBRegressor(n_estimators=427,max_depth=9,learning_rate=0.06446)\nxgb_opt.fit(train_X,train_y)\npred_xgb_opt=xgb_opt.predict(val_X)\nscore_xgb_opt=mean_squared_error(val_y,pred_xgb_opt)\nprint(score_xgb_opt)","ea39780f":" tid is the time id.  ","f389ebf8":"We can see how hyperopt makes the parameters approach some value as tid increases. This is done to minimize the loss.\n","13ff2dd6":"Now, let us explore how Chance of Admit depends on various factors.","f7db7acd":"We shall combine version 1 and 1.1 of admission predict.","85b39cf9":"**Using Hyperopt for Fine tuning with informed method**","d960f49a":"From this, we can see how Hyperopt is really working.","f2e5612f":"**Random Forest Without Optimization**","e88924c0":"GridSearch and Random Search are some other methods to find optimal solution within provided range of parameters. But these two approach the problem in an **uninformed manner**, that is, they don't use the score of previous parameters to search for new set of parameters while on the other hand, Hyperopt follows **informed method**. It uses the score of previous parameters to approach to new set of parameters for hopefully better score. ","20442357":"**Basic EDA**","95455764":"**Most Basic model : Linear Regression**","d4e1d387":"We can see that upto now, Random Forest has got the least mse,followed by XGB and then Linear Regression.\nNow let us try to fine tune some hyprparameters of Random Forest and XGB.","ecf617b3":"As expected, Chance of Admit increases as GRE, TOEFL Score and CGPA increases","944a9138":"**Importing basic libraries**","477c7ec7":"On an average, as SOP,LOR, University Rating increases in value, the chance of admit increases aswell. If you practice research aswell then too, chance of admit increases. This matches intuition. ","d516b28d":"Here again, the mediocres prevail !","5939010d":"GRE Scores, TOEFL Scores and CGPA are somewhat normally distributed.","a1dbdd5d":"**XBGRegressor Without Optimization**","331a25f3":"Finally, after some hyperparameter tuning, we got much lower mean squared error values. Now, XGB has given lower mse than Random Forest. "}}