{"cell_type":{"0a9ad938":"code","1dc17a7b":"code","19ea6781":"code","93946bed":"code","6f3cd962":"code","0f4bbeca":"code","f4b11317":"code","c28bc9e5":"code","ca553ed8":"code","31a1bbb6":"code","23fd2890":"code","583b76d8":"code","74aad292":"code","2256d561":"code","8cffe708":"code","daf27560":"code","a7bfcfc5":"code","331b8f9d":"code","e4de3bef":"code","3cf16650":"code","fa7e98e8":"code","cbbe65eb":"code","c60f90d3":"code","cfc4941d":"code","b0a35577":"code","82053be8":"code","e9c21fb7":"code","39f6c401":"code","65756383":"code","266743c6":"code","6b2e4cb0":"code","ef963b19":"markdown","66dd4fab":"markdown","dacc5552":"markdown","c647b6b3":"markdown","feb366a6":"markdown","07a143d3":"markdown","1d8a8638":"markdown","2a2346b1":"markdown","90e473b8":"markdown","b67e189d":"markdown","ba62c176":"markdown","58f09162":"markdown","7359b530":"markdown","4e7a5c19":"markdown","0ff6c474":"markdown"},"source":{"0a9ad938":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport IPython.display as ipd\nimport librosa","1dc17a7b":"# Input data files are available in the \"..\/input\/\" directory.\nimport os\nprint(os.listdir(\"..\/input\"))\n# !ls ..\/input\/","19ea6781":"train = pd.read_csv(\"..\/input\/train_curated.csv\")\ntrain['is_curated'] = True\ntrain_noisy = pd.read_csv(\"..\/input\/train_noisy.csv\")\ntrain_noisy['is_curated'] = False\ntrain = pd.concat([train, train_noisy], axis=0)\ndel train_noisy\n# print(train.head())\n# print(train.tail())","93946bed":"train.head()","6f3cd962":"train.tail()","0f4bbeca":"train['n_label'] = train.labels.str.split(',').apply(lambda x: len(x))\nprint(train.head(10))\nprint(train.tail(6))\nprint(\"Number of train examples: \", train.shape[0])\nprint(\"In curated subset: \", train[train.is_curated == True].shape[0])\nprint(\"In noisy subset: \", train[train.is_curated == False].shape[0])\n# print(\"Number of classes:\", len(set(train.labels)))","f4b11317":"test = pd.read_csv(\"..\/input\/sample_submission.csv\")\n# print(test.head())\ntest.head()","c28bc9e5":"print(\"Number of test examples: \", test.shape[0],\n      \"\\nNumber of classes: \", len(set(test.columns[1:])))\nprint(set(test.columns[1:]))","ca553ed8":"import wave\nSAMPLE_RATE = 44100\n\ntrain_1 = train[train.is_curated == True].sort_values('labels').reset_index()\ntrain_1['nframes'] = train_1['fname'].apply(lambda f: \n    wave.open('..\/input\/train_curated\/' + f).getnframes()\/SAMPLE_RATE)\ntrain_1.head()\n","31a1bbb6":"train_2 = train[train.is_curated == False].sort_values('labels').reset_index()\ntrain_2['nframes'] = train_2['fname'].apply(lambda f: \n    wave.open('..\/input\/train_noisy\/' + f).getnframes()\/SAMPLE_RATE)\ntrain_2.head()","23fd2890":"test_new = test\ntest_new['nframes'] = test_new['fname'].apply(lambda f: \n    wave.open('..\/input\/test\/' + f).getnframes()\/SAMPLE_RATE)\ntest_new.head()","583b76d8":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 1, figsize=(16,5))\ntrain_1.nframes.hist(bins=100, grid=True, rwidth=0.5, color='blue', ax=axes[0])\ntrain_2.nframes.hist(bins=100, grid=True, rwidth=0.5, color='black', ax=axes[1])\ntest_new.nframes.hist(bins=100, grid=True, rwidth=0.5, color='red', ax=axes[2])\nplt.suptitle('Duration Distribution in curated, noisy and test set', ha='center', fontsize='large');","74aad292":"train_1.query(\"nframes > 30\")","2256d561":"ipd.Audio( '..\/input\/train_curated\/' + '77b925c2.wav')","8cffe708":"del train_1\ndel train_2\ndel test_new","daf27560":"train.query('is_curated == True').n_label.value_counts()","a7bfcfc5":"train.query('is_curated == False').n_label.value_counts()","331b8f9d":"cat_gp = train[train.n_label == 1].groupby(\n    ['labels', 'is_curated']).agg({'fname':'count'}).reset_index()\ncat_gpp = cat_gp.pivot(index='labels', columns='is_curated', values='fname').reset_index().set_index('labels')\n\nplot = cat_gpp.plot(\n    kind='barh',\n    title=\"Number of samples per category\",\n    stacked=True,\n    color=['deeppink', 'darkslateblue'],\n    figsize=(15,20))\nplot.set_xlabel(\"Number of Samples\", fontsize=20)\nplot.set_ylabel(\"Label\", fontsize=20);","e4de3bef":"from wordcloud import WordCloud\nwordcloud = WordCloud(max_font_size=50, width=600, height=300).generate(' '.join(train.labels))\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Labels\", fontsize=35)\nplt.axis(\"off\")\nplt.show() ","3cf16650":"import librosa, librosa.display\nimport matplotlib.pyplot as plt\nimport os\nimport IPython\nSAMPLE_RATE = 44100\n\ndef load_and_show(path, fname):\n    plt.figure(figsize=(10,3))\n    wav, sr = librosa.core.load(os.path.join(path, fname))\n#     melspec = librosa.feature.melspectrogram(\n#         librosa.resample(wav, sr, SAMPLE_RATE),\n#         sr=SAMPLE_RATE\/2, n_fft = 1024,\n#         hop_length=512, n_mels= 128\n#     )\n#     logmel = librosa.core.power_to_db(melspec)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\n    # https:\/\/www.kaggle.com\/c\/freesound-audio-tagging-2019\/discussion\/91827#latest-529419\n#     D = librosa.pcen(np.abs(librosa.stft(wav)))\n    # CQT = librosa.amplitude_to_db(np.abs(librosa.cqt(y, sr=sr)), ref=np.max)\n    plt.subplot(1,2,1)\n    librosa.display.waveplot(wav, sr=SAMPLE_RATE)\n    plt.subplot(1,2,2)\n    librosa.display.specshow(D, sr=SAMPLE_RATE, x_axis='time', y_axis='linear')\n    plt.title(os.path.join(path, fname))","fa7e98e8":"# sampling an audio in train_curated\n# choose a label randomly\nsamp = train[(train.n_label == 1) & (train.is_curated == True)].sample(1)\nspecific_label = samp.labels.values[0]\n\n# for the specific label, choose several samples\nsamples = train[(train.n_label == 1) \n               & (train.is_curated == True) \n               & (train.labels == specific_label)].sample(5)\n\n# Listen and check the wave, spectrogram of the samples.\nprint('Label:', specific_label)\nfor fname in list(samples.fname):\n#     print('..\/input\/train_curated\/{}'.format(fname))\n    load_and_show('..\/input\/train_curated', fname)\n    IPython.display.display(ipd.Audio('..\/input\/train_curated\/{}'.format(fname)))\n","cbbe65eb":"# sampling an audio in train_noisy\n# for the specific label, choose several samples\nsamples = train[(train.n_label == 1) \n               & (train.is_curated == False) \n               & (train.labels == specific_label)].sample(5)\n\n# Listen and check the wave, spectrogram of the samples.\nprint('Label:', specific_label)\nfor fname in list(samples.fname):\n    print('..\/input\/train_noisy\/{}'.format(fname))\n    load_and_show('..\/input\/train_noisy', fname)\n    IPython.display.display(ipd.Audio('..\/input\/train_noisy\/{}'.format(fname)))","c60f90d3":"cat_gp = train[(train.n_label > 1) & (train.is_curated == True)].groupby('labels').agg({'fname':'count'})\ncat_gp.columns = ['counts']\n\nplot = cat_gp.sort_values(ascending=True, by='counts').plot(\n    kind='barh',\n    title=\"Number of Audio Samples per Category\",\n    color='deeppink',\n    figsize=(15,30))\nplot.set_xlabel(\"Number of Samples\", fontsize=20)\nplot.set_ylabel(\"Label\", fontsize=20);","cfc4941d":"label_set = set(train.loc[(train.n_label == 2) & (train.is_curated == True), 'labels']) & set(\n    train.loc[(train.n_label == 2) & (train.is_curated == False), 'labels'])\n\nlabel_samp = np.random.choice(list(label_set), 1)[0]\nsamp = train[(train.labels == label_samp) & (train.is_curated == True)].sample(1)\nprint(label_samp)\nIPython.display.display(ipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0])))\nload_and_show('..\/input\/train_curated', samp.fname.values[0])","b0a35577":"# sampling an audio in train_noisy\nsamp_n = train[(train.labels == label_samp) & (train.is_curated == False)].sample(1)\nprint(samp_n.labels.values[0])\nIPython.display.display(ipd.Audio('..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0])))\nload_and_show('..\/input\/train_noisy', samp_n.fname.values[0])","82053be8":"label_set = set(train.loc[(train.n_label == 3) & (train.is_curated == True), 'labels']) & set(\n    train.loc[(train.n_label == 3) & (train.is_curated == False), 'labels'])\n\nlabel_samp = np.random.choice(list(label_set), 1)[0]\nsamp = train[(train.labels == label_samp) & (train.is_curated == True)].sample(1)\nprint('File name:', '..\/input\/train_curated\/{}'.format(samp.fname.values[0]), '\\nLabel:', label_samp)\nIPython.display.display(ipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0])))\nload_and_show('..\/input\/train_curated', samp.fname.values[0])","e9c21fb7":"# sampling an audio in train_noisy\nsamp_n = train[(train.labels == label_samp) & (train.is_curated == False)].sample(1)\nprint('File name:', '..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0]), '\\nLabel:', samp_n.labels.values[0])\nIPython.display.display(ipd.Audio('..\/input\/train_noisy\/{}'.format(samp_n.fname.values[0])))\nload_and_show('..\/input\/train_noisy', samp_n.fname.values[0])","39f6c401":"samp = train[(train.n_label == 4) & (train.is_curated == True)].sample(1)\nprint('File name:', '..\/input\/train_curated\/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('..\/input\/train_curated\/{}'.format(samp.fname.values[0])))\nload_and_show('..\/input\/train_curated', samp.fname.values[0])","65756383":"samp = train[(train.n_label == 4) & (train.is_curated == False)].sample(1)\nprint('File name:', '..\/input\/train_noisy\/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('..\/input\/train_noisy\/{}'.format(samp.fname.values[0])))\nload_and_show('..\/input\/train_noisy', samp.fname.values[0])","266743c6":"samp = train[(train.n_label == 5) & (train.is_curated == False)].sample(1)\nprint('File name:', '..\/input\/train_noisy\/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('..\/input\/train_noisy\/{}'.format(samp.fname.values[0])))\nload_and_show('..\/input\/train_noisy', samp.fname.values[0])","6b2e4cb0":"samp = train[(train.n_label == 6) & (train.is_curated == False)].sample(1)\nprint('File name:', '..\/input\/train_noisy\/{}'.format(samp.fname.values[0]), '\\nLabel:', samp.labels.values[0])\nIPython.display.display(ipd.Audio('..\/input\/train_noisy\/{}'.format(samp.fname.values[0])))\nload_and_show('..\/input\/train_noisy', samp.fname.values[0])","ef963b19":"### Intro\nThe data set for audio tagging 2019 is designed with label noise. This notebook is to explore the basic information, the audio features of three subsets.\n\nThanks to below kagglers, some functions are re-orgnized here. Wish this notebook could be helpful.\n\nref:\n* https:\/\/www.kaggle.com\/maxwell110\/explore-multi-labeled-data\n* https:\/\/www.kaggle.com\/dude431\/beginner-s-visualization-and-removing-uniformative","66dd4fab":"### Audio duration\n\nIt's introduced that durations of samples in curated subset are from 0.3s to 30s, while those in noisy subset are from 1s to 15s, with the vast majority lasting 15s.","dacc5552":"#### Wordcloud for labels","c647b6b3":"### 1. Basic information\nBasice information about the data set, such as head of data, numbers, label distributions and so on.","feb366a6":"### Train set labels distributions:\nSamples in curated subset have 1, 2, 3, 4, 6 labels, while samples in noisy subset have 1, 2, 3, 4, 5, 6, 7 labels. Most samples have a single label.","07a143d3":"Summary\n\n* Samples with multi labels sound really difficult even for people to label all events correctly. Spectrograms of those samples maybe confused with all events happen the same time.\n\neg. \"File name: ..\/input\/train_noisy\/5fde4352.wav Label: Fill_(with_liquid),Water_tap_and_faucet,Hiss,Toilet_flush,Sink_(filling_or_washing)\"\n\n* **A lot of noisy data seem to have wrong labels**. In the previous competition, using the robust loss function to suppress the effect of mislabeled data was one of the important points to get high score. We must care of that again.","1d8a8638":"### Audio features\nTry to listen samples of specific label, check waveform and spectrogram together.","2a2346b1":"### 4 and more labels\n\nThere are no common record with same multi labels in curated and noisy. Will check audios in different subsets.","90e473b8":"There are only about 10 kinds of multi labels whose samples are more than 10.","b67e189d":"### Co-occurrence\n\nHow to get labels for the trainning process.\n\nref:\n* https:\/\/www.kaggle.com\/maxwell110\/explore-multi-labeled-data","ba62c176":"#### 2 label conditions","58f09162":"We can try some different labels, it seems a lot of noisy data have wrong labels.\n\nTo let the noisy data make positive effect, designed loss function or semi-supervised learning should be used.","7359b530":"### Multi labels\n\nAbout 1\/5 of samples in curated and noisy subset are with multi labels, so let's check the samples with multi labels.","4e7a5c19":"Majority of the audio files are short than 10s, when we crop audio as cnn train data, 4-8 seconds should be OK.\n\nThere are an abnormal length in the train histogram.","0ff6c474":"#### 3 labels conditions"}}