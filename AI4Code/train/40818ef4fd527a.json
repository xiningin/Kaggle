{"cell_type":{"1f8f97fc":"code","b4ba95fb":"code","59986a0c":"code","4e6b19d7":"code","591d4e7b":"code","a5fa5263":"code","93d3c9fb":"code","87dbaae7":"code","c2239e31":"code","c3bda1d1":"code","3885b32e":"code","0757a186":"code","2e2cb4c0":"code","0afa6ced":"code","f01964e4":"code","2e876158":"code","6e056d2a":"code","407a383c":"code","8156ac6f":"code","b90e84dc":"code","480e9672":"code","c8c73a06":"code","93c68b0f":"code","ceb10342":"code","1239580a":"code","8f86e16b":"code","d4a39920":"code","a9878bbe":"code","3dccc93d":"code","d245b4ab":"code","8df0ecae":"code","8dabc5be":"code","1c4e5781":"code","a3d69d04":"code","a667693d":"code","2e5d2a82":"code","e7c26911":"code","a12a0e85":"code","f673385c":"code","b5fd47ec":"code","cd3a873a":"code","e77476af":"code","07efb033":"code","bd6949fe":"code","1866c038":"code","663029ca":"code","62bbf451":"code","929dcf8e":"code","0bcdb935":"code","c2aacfc9":"code","08dc0242":"code","aaff18ec":"code","bb00718f":"code","33dcce82":"code","7f7d5e44":"code","ba6c477a":"code","89cbfcd6":"code","bdfd491d":"code","b88f96da":"code","56b91fa3":"code","1f6cd9cd":"code","8432ed77":"code","0ca08844":"code","2c165b28":"code","6b04a57e":"code","4b26294f":"code","1a93ae4c":"code","a177e9c5":"code","5c1deb5f":"code","5d050ef6":"code","ef26f19e":"code","f9ba1dac":"code","16f84247":"code","725a3fa5":"code","067b97ef":"code","995e50d6":"code","3a23063b":"code","7ec86555":"code","ba3c1da6":"code","2993fecb":"code","ef64c690":"code","1c7f9e79":"code","7e572eb7":"code","9684d48e":"code","7b1a346f":"code","76155b1b":"code","996f76fa":"code","570dd742":"code","c7b6276d":"code","b3425c3b":"code","59e3d3b5":"code","8d61b792":"code","7e9bb7cd":"code","38485d98":"code","b4a942d9":"code","0bb0bee4":"code","cd5e78b3":"code","2e85cb6c":"code","30f6da92":"code","d822228d":"code","5a2dfe11":"code","e5d67b39":"code","0fe2775b":"code","5ba91de2":"code","ecd65d18":"code","df9f6ef9":"code","a4e97b17":"code","23d324ac":"code","61a48c31":"code","1bbf6620":"code","4c757a32":"code","62951494":"code","3cf49eb8":"code","cf8cb90c":"markdown","1dfc9a08":"markdown","f7b1788b":"markdown","e2f06362":"markdown","ccee8441":"markdown","01fd634a":"markdown","c5437c26":"markdown","a8a2626c":"markdown","3e860d03":"markdown","e6777dfc":"markdown","6bc0fa7b":"markdown","d3231bef":"markdown","b1f1166a":"markdown","84be0391":"markdown","a0c2ac5e":"markdown","18b35862":"markdown","1c590ebb":"markdown","737a5df4":"markdown","bb1caad4":"markdown","9c8f1fc7":"markdown","eaf0e987":"markdown","15df29bc":"markdown","e04b0ed9":"markdown","922dd524":"markdown","36823b66":"markdown","6f3d38b6":"markdown","8d135329":"markdown","bb831eba":"markdown","c60b5dc7":"markdown","e11af0ca":"markdown","7e5a2c81":"markdown","5f5aa388":"markdown","127cfe94":"markdown","57396187":"markdown","5bf77c98":"markdown","b58f2c69":"markdown","7fc22576":"markdown","00158660":"markdown","894b5bd4":"markdown","d7dc4643":"markdown","0306eb40":"markdown","a367d046":"markdown","d261538b":"markdown","c81eadc2":"markdown","63dcd9d7":"markdown","78dbdaf8":"markdown","632b45a1":"markdown","93aad09f":"markdown","38bbe9c6":"markdown","6b8380a1":"markdown","106384f1":"markdown","50e6a3ce":"markdown","4acbed13":"markdown","8f5f0ef2":"markdown","4b1d0b3d":"markdown","2669db6a":"markdown","3e059e19":"markdown","8252d4c6":"markdown","f885c3ec":"markdown","a3f85a6b":"markdown","55f153e5":"markdown","8caa527b":"markdown","4b32db1e":"markdown","090b2df2":"markdown","b2a26b85":"markdown","b71eff66":"markdown","4f8414c4":"markdown","f7c04993":"markdown","66907530":"markdown","8f6299c8":"markdown","fb33375c":"markdown","21ad3adb":"markdown","60983883":"markdown","2e5f0fc0":"markdown","5487e1c4":"markdown","094e0cb7":"markdown","916c27de":"markdown","f63dd47e":"markdown","89dad56e":"markdown","a7ce6581":"markdown","b5976903":"markdown","ede8f338":"markdown","a7279528":"markdown","de4368fc":"markdown","76eecdb4":"markdown","3cfa2301":"markdown","752c8a5a":"markdown","5188d890":"markdown","5e081421":"markdown","b4b59c4f":"markdown","5bbd1c6e":"markdown","d7f524f9":"markdown","f4783a6e":"markdown","375e2603":"markdown","1b2adc4c":"markdown"},"source":{"1f8f97fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4ba95fb":"import numpy as np\nfrom datetime import datetime\nfrom IPython.display import Image","59986a0c":"dates=np.arange('2020-01-01', '2020-01-31',  dtype='datetime64[D]')\ndates","4e6b19d7":"dates=np.arange('2020-01-01', '2020-01-31', 10, dtype='datetime64[D]')\ndates","591d4e7b":"import pandas as pd\ndates=pd.date_range('2020-01-01', periods=7, freq='D')\ndates","a5fa5263":"data=pd.read_csv('..\/input\/starbucks\/starbucks.csv')\ndata.head()","93d3c9fb":"data.info()","87dbaae7":"data['Date']=pd.to_datetime(data['Date'])","c2239e31":"data.info()","c3bda1d1":"data=data.set_index('Date')\ndata.head()","3885b32e":"data.index","0757a186":"# daily to yearly\ndata.resample('A').mean()","2e2cb4c0":"data.head()","0afa6ced":"# Let's resample only a series column and plot it\n\ndata['Close'].resample('A').mean().plot(kind='bar')","f01964e4":"# shifting means time shifting the whole dataframe or a particular series by 1 up or down\n\ndata.head()","2e876158":"data.shift(1)","6e056d2a":"data.rolling(window=3).mean().head()\n# Notice that first 2 values are NAN hence sampling mean of 3 samples will start only from the 3rd sample","407a383c":"data['Close'].plot(figsize=(12,5))\ndata['Close'].rolling(window=30).mean().plot()","8156ac6f":"# here we only focus on the plot containing the data from 2016-2017. we can also limit the data in the plot itself using xlim and ylim but i suggest to limit the data before plotting\ndata['Close']['2016-01-01':'2017-12-31'].plot(figsize=(12,5), ls='--', color='red')\n","b90e84dc":"# Dates are separated by a comma:\ndata['Close'].plot(figsize=(12,4),xlim=['2017-01-01','2017-03-01']);","480e9672":"df = pd.read_csv('..\/input\/macrodata\/macrodata.csv',index_col=0,parse_dates=True)\ndf.head()","c8c73a06":"ax = df['realgdp'].plot()\nax.autoscale(axis='x',tight=True)\nax.set(ylabel='REAL GDP');","93c68b0f":"from statsmodels.tsa.filters.hp_filter import hpfilter\n\n# Tuple unpacking\ngdp_cycle, gdp_trend = hpfilter(df['realgdp'], lamb=1600)","ceb10342":"df['trend'] = gdp_trend\ndf[['trend','realgdp']].plot().autoscale(axis='x',tight=True);","1239580a":"df[['trend','realgdp']]['2000-03-31':].plot(figsize=(12,8)).autoscale(axis='x',tight=True);","8f86e16b":"airline = pd.read_csv('..\/input\/airline\/airline_passengers.csv',index_col='Month',parse_dates=True)\nairline.dropna(inplace=True)\nairline.head()","d4a39920":"airline.plot();","a9878bbe":"from statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(airline['Thousands of Passengers'], model='multiplicative')  # model='mul' also works\nresult.plot();","3dccc93d":"airline['6-month-SMA'] = airline['Thousands of Passengers'].rolling(window=6).mean()\nairline['12-month-SMA'] = airline['Thousands of Passengers'].rolling(window=12).mean()\nairline.head(15)","d245b4ab":"airline.plot();","8df0ecae":"airline['EWMA12'] = airline['Thousands of Passengers'].ewm(span=12,adjust=False).mean()","8dabc5be":"airline[['Thousands of Passengers','EWMA12']].plot();","1c4e5781":"airline[['Thousands of Passengers','EWMA12','12-month-SMA']].plot(figsize=(12,8)).autoscale(axis='x',tight=True);","a3d69d04":"df = pd.read_csv('..\/input\/airline\/airline_passengers.csv',index_col='Month',parse_dates=True)\ndf.dropna(inplace=True)\ndf.index","a667693d":"df.index.freq = 'MS'\ndf.index","2e5d2a82":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n\nspan = 12\nalpha = 2\/(span+1)\n\ndf['EWMA12'] = df['Thousands of Passengers'].ewm(alpha=alpha,adjust=False).mean()\ndf['SES12']=SimpleExpSmoothing(df['Thousands of Passengers']).fit(smoothing_level=alpha,optimized=False).fittedvalues.shift(-1)\ndf.head()","e7c26911":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\ndf['DESadd12'] = ExponentialSmoothing(df['Thousands of Passengers'], trend='add').fit().fittedvalues.shift(-1)\ndf.head()","a12a0e85":"df[['Thousands of Passengers','EWMA12','DESadd12']].iloc[:24].plot(figsize=(12,6)).autoscale(axis='x',tight=True);","f673385c":"df['DESmul12'] = ExponentialSmoothing(df['Thousands of Passengers'], trend='mul').fit().fittedvalues.shift(-1)\ndf.head()","b5fd47ec":"df[['Thousands of Passengers','DESadd12','DESmul12']].iloc[:24].plot(figsize=(12,6)).autoscale(axis='x',tight=True);","cd3a873a":"df = pd.read_csv('..\/input\/airline\/airline_passengers.csv',index_col='Month',parse_dates=True)\ndf.index.freq = 'MS'\ndf.head()","e77476af":"train_data = df.iloc[:109] # Goes up to but not including 109\ntest_data = df.iloc[108:]","07efb033":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nfitted_model=ExponentialSmoothing(train_data['Thousands of Passengers'], trend='mul', seasonal='mul', seasonal_periods=12).fit()","bd6949fe":"# Let's do the forecast. Ignore the warings as it is related to statsmodels\npredictions=fitted_model.forecast(36).rename(\"Forecast\")\npredictions","1866c038":"train_data['Thousands of Passengers'].plot(legend=True, label=\"TRAIN DATA\")\ntest_data['Thousands of Passengers'].plot(legend=True, label=\"TEST DATA\", figsize=(12,8))\npredictions.plot(legend=True, label='PREDICTIONS')","663029ca":"train_data['Thousands of Passengers'].plot(legend=True,label='TRAIN')\ntest_data['Thousands of Passengers'].plot(legend=True,label='TEST',figsize=(12,8))\npredictions.plot(legend=True,label='PREDICTION',xlim=['1958-01-01','1961-01-01'])","62bbf451":"from sklearn.metrics import mean_squared_error,mean_absolute_error","929dcf8e":"mean_absolute_error(test_data,predictions)","0bcdb935":"np.sqrt(mean_squared_error(test_data,predictions))","c2aacfc9":"df2 = pd.read_csv('..\/input\/samples\/samples.csv',index_col=0,parse_dates=True)\ndf2.head()","08dc0242":"df2['a'].plot(ylim=[0,100],title=\"STATIONARY DATA\").autoscale(axis='x',tight=True);","aaff18ec":"df2['b'].plot(ylim=[0,100],title=\"NON-STATIONARY DATA\").autoscale(axis='x',tight=True);","bb00718f":"from statsmodels.tsa.statespace.tools import diff\ndf2['d1'] = diff(df2['b'],k_diff=1)\n\ndf2['d1'].plot(title=\"FIRST DIFFERENCE DATA\").autoscale(axis='x',tight=True);","33dcce82":"\n# Load a non-stationary dataset\ndf1 = pd.read_csv('..\/input\/airline\/airline_passengers.csv',index_col='Month',parse_dates=True)\ndf1.index.freq = 'MS'\n\n# Load a stationary dataset\ndf2 = pd.read_csv('..\/input\/dailytotalfemalebirths\/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\ndf2.index.freq = 'D'","7f7d5e44":"# Import the models\nfrom statsmodels.tsa.stattools import acovf,acf,pacf,pacf_yw,pacf_ols","ba6c477a":"#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","89cbfcd6":"df = pd.DataFrame({'a':[13, 5, 11, 12, 9]})\narr = acovf(df['a'])\narr","bdfd491d":"arr2 = acovf(df['a'],unbiased=True)\narr2","b88f96da":"arr3 = acf(df['a'])\narr3","56b91fa3":"arr4 = pacf_yw(df['a'],nlags=4,method='mle')\narr4","1f6cd9cd":"arr5 = pacf_yw(df['a'],nlags=4,method='unbiased')\narr5","8432ed77":"from pandas.plotting import lag_plot\n\nlag_plot(df1['Thousands of Passengers']);","0ca08844":"lag_plot(df2['Births']);","2c165b28":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf","6b04a57e":"# Let's look first at the ACF array. By default acf() returns 40 lags\nacf(df2['Births'])","4b26294f":"# Now let's plot the autocorrelation at different lags\ntitle = 'Autocorrelation: Daily Female Births'\nlags = 40\nplot_acf(df2,title=title,lags=lags);","1a93ae4c":"acf(df1['Thousands of Passengers'])","a177e9c5":"title = 'Autocorrelation: Airline Passengers'\nlags = 40\nplot_acf(df1,title=title,lags=lags);","5c1deb5f":"# Load a seasonal dataset\ndf1 = pd.read_csv('..\/input\/airline\/airline_passengers.csv',index_col='Month',parse_dates=True)\ndf1.index.freq = 'MS'\n\n# Load a nonseasonal dataset\ndf2 = pd.read_csv('..\/input\/dailytotalfemalebirths\/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\ndf2.index.freq = 'D'","5d050ef6":"df1['12-month-SMA'] = df1['Thousands of Passengers'].rolling(window=12).mean()\ndf1['12-month-Std'] = df1['Thousands of Passengers'].rolling(window=12).std()\n\ndf1[['Thousands of Passengers','12-month-SMA','12-month-Std']].plot();","ef26f19e":"from statsmodels.tsa.stattools import adfuller\nprint('Augmented Dickey-Fuller Test on Airline Data')\ndftest = adfuller(df1['Thousands of Passengers'],autolag='AIC')\ndftest","f9ba1dac":"help(adfuller)","16f84247":"print('Augmented Dickey-Fuller Test on Airline Data')\n\ndfout = pd.Series(dftest[0:4],index=['ADF test statistic','p-value','# lags used','# observations'])\n\nfor key,val in dftest[4].items():\n    dfout[f'critical value ({key})']=val\nprint(dfout)","725a3fa5":"from statsmodels.graphics.tsaplots import month_plot,quarter_plot\n\n# Note: add a semicolon to prevent two plots being displayed in jupyter\nmonth_plot(df1['Thousands of Passengers']);","067b97ef":"dfq = df1['Thousands of Passengers'].resample(rule='Q').mean()\n\nquarter_plot(dfq);","995e50d6":"# Load a non-stationary dataset\ndf1 = pd.read_csv('..\/input\/airline\/airline_passengers.csv',index_col='Month',parse_dates=True)\ndf1.index.freq = 'MS'\n\n# Load a stationary dataset\ndf2 = pd.read_csv('..\/input\/dailytotalfemalebirths\/DailyTotalFemaleBirths.csv',index_col='Date',parse_dates=True)\ndf2.index.freq = 'D'","3a23063b":"!pip install pmdarima","7ec86555":"from pmdarima import auto_arima\n\n# Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ba3c1da6":"help(auto_arima)","2993fecb":"from statsmodels.tsa.seasonal import seasonal_decompose","ef64c690":"seasonal_decompose(df2['Births']).plot();","1c7f9e79":"auto_arima(df2['Births'],error_action='ignore').summary()","7e572eb7":"arima_model=auto_arima(df2['Births'], start_p=0, start_q=0, d=None, seasonal=False, max_p=5, max_q=5, error_action='ignore', stepwise=True, suppress_warnings=True)","9684d48e":"arima_model.summary()","7b1a346f":"# Load specific forecasting tools\nfrom statsmodels.tsa.arima_model import ARMA,ARMAResults,ARIMA,ARIMAResults\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf # for determining (p,q) orders\nfrom pmdarima import auto_arima # for determining ARIMA orders\n\n\ndf2 = pd.read_csv('..\/input\/tradeinvestories\/TradeInventories.csv',index_col='Date',parse_dates=True)\ndf2.index.freq='MS'","76155b1b":"title = 'Real Manufacturing and Trade Inventories'\nylabel='Chained 2012 Dollars'\nxlabel='' # we don't really need a label here\n\nax = df2['Inventories'].plot(figsize=(12,5),title=title)\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel, ylabel=ylabel)\n","996f76fa":"result=seasonal_decompose(df2['Inventories'], model='additive').plot()","570dd742":"auto_arima(df2['Inventories'], seasonal=False).summary()","c7b6276d":"(df2['Inventories']-df2['Inventories'].shift(1)).plot()","b3425c3b":"model_arima_1 = auto_arima(df2['Inventories'], start_p=0, start_q=0,\n                          max_p=2, max_q=2, m=12,\n                          seasonal=False,\n                          d=None, trace=True,\n                          error_action='ignore',   # we don't want to know if an order does not work\n                          suppress_warnings=True,  # we don't want convergence warnings\n                          stepwise=True)           # set to stepwise\n\nmodel_arima_1.summary()","59e3d3b5":"len(df2)","8d61b792":"# Set one year for testing\ntrain = df2.iloc[:252]\ntest = df2.iloc[252:]","7e9bb7cd":"# Fit the ARIMA model . Let's use 1,1,1 order as auto arima selected 0,1,0 due to early stopping but it seems 1,1,1 is much better\nmodel = ARIMA(train['Inventories'],order=(1,1,1))\nresults = model.fit()\nresults.summary()","38485d98":"# Obtain predicted values\nstart=len(train)\nend=len(train)+len(test)-1\n","b4a942d9":"predictions = results.predict(start=start, end=end, dynamic=False, typ='levels').rename('ARIMA(1,1,1) Predictions')","0bb0bee4":"ax = test['Inventories'].plot(legend=True,figsize=(12,6),title=title)\npredictions.plot(legend=True)\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel, ylabel=ylabel)\n","cd5e78b3":"model = ARIMA(df2['Inventories'],order=(1,1,1))\nresults = model.fit()\nfcast = results.predict(len(df2),len(df2)+11,typ='levels').rename('ARIMA(1,1,1) Forecast')","2e85cb6c":"ax = df2['Inventories'].plot(legend=True,figsize=(12,6),title=title)\nfcast.plot(legend=True)\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel, ylabel=ylabel)","30f6da92":"from statsmodels.tsa.statespace.sarimax import SARIMAX","d822228d":"df=pd.read_csv('..\/input\/co2-mm\/co2_mm_mlo.csv')\ndf.head()","5a2dfe11":"# let's create a datetime column\ndf['Date']=pd.to_datetime({'year': df['year'], 'month':df['month'], 'day':1})","e5d67b39":"df=df.set_index('Date')","0fe2775b":"df.info()","5ba91de2":"seasonal_check=seasonal_decompose(df['interpolated'], model='add').plot()","ecd65d18":"auto_arima(df['interpolated'], seasonal=True, m=12).summary()","df9f6ef9":"len(df)","a4e97b17":"train=df.iloc[:717]\ntest=df.iloc[717:]","23d324ac":"model=SARIMAX(train['interpolated'], order=(2,1,1), seasonal_order=(1,0,1,12) )","61a48c31":"results=model.fit()","1bbf6620":"results.summary()","4c757a32":"start=len(train)\nend=len(train)+len(test)-1","62951494":"predictions=results.predict(start, end, typ='levels').rename(\"SARIMA predictions\")","3cf49eb8":"test['interpolated'].plot(legend=True, figsize=(12,8))\npredictions.plot(legend=True)","cf8cb90c":"### 1.2 how to get a series of dates with numpy","1dfc9a08":"### Exposing Seasonality with Month and Quarter Plots\nStatsmodels has two plotting functions that group data by month and by quarter. Note that if the data appears as months, you should employ <em>resampling<\/em> with an aggregate function before running a quarter plot. These plots return a <tt>matplotlib.Figure<\/tt> object.\n<div class=\"alert alert-info\"><h3>Related Plot Methods:<\/h3>\n<tt><strong>\n<a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.graphics.tsaplots.month_plot.html'>tsaplots.month_plot<\/a><\/strong><font color=black>(x)<\/font>&nbsp;&nbsp;&nbsp;&nbsp;Seasonal plot of monthly data<br>\n<strong>\n<a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.graphics.tsaplots.quarter_plot.html'>tsaplots.quarter_plot<\/a><\/strong><font color=black>(x)<\/font>&nbsp;&nbsp;Seasonal plot of quarterly data<\/tt>\n<\/div>","f7b1788b":"EWMA will allow us to reduce the lag effect from SMA and it will put more weight on values that occured more recently (by applying more weight to the more recent values, thus the name). The amount of weight applied to the most recent values will depend on the actual parameters used in the EWMA and the number of periods given a window size.\n[Full details on Mathematics behind this can be found here](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/computation.html#exponentially-weighted-windows).\nHere is the shorter version of the explanation behind EWMA.\n\nThe formula for EWMA is:\n### $y_t =   \\frac{\\sum\\limits_{i=0}^t w_i x_{t-i}}{\\sum\\limits_{i=0}^t w_i}$","e2f06362":"### Choosing X limits by Slice","ccee8441":"See, now 'Date' column has been converted to datetime object. Now let's set this column as index","01fd634a":"****Stationary Vs Non Stationary****","c5437c26":"___\n### Double Exponential Smoothing\nWhere Simple Exponential Smoothing employs just one smoothing factor $\\alpha$ (alpha), Double Exponential Smoothing adds a second smoothing factor $\\beta$ (beta) that addresses trends in the data. Like the alpha factor, values for the beta factor fall between zero and one ($0<\\beta\u22641$). The benefit here is that the model can anticipate future increases or decreases where the level model would only work from recent calculations.\n\nWe can also address different types of change (growth\/decay) in the trend. If a time series displays a straight-line sloped trend, you would use an <strong>additive<\/strong> adjustment. If the time series displays an exponential (curved) trend, you would use a <strong>multiplicative<\/strong> adjustment.\n\nAs we move toward forecasting, it's worth noting that both additive and multiplicative adjustments may become exaggerated over time, and require <em>damping<\/em> that reduces the size of the trend over future periods until it reaches a flat line.","a8a2626c":"As seen above, arange gave us all the days between start and end dates","3e860d03":"As it can be seen that seasonal component range is very low as compared to trend, hence we will consider this as non seasonal data","e6777dfc":"### Augmented Dickey-Fuller Test\nTo determine whether a series is stationary we can use the <a href='https:\/\/en.wikipedia.org\/wiki\/Augmented_Dickey-Fuller_test'>augmented Dickey-Fuller Test<\/a>. In this test the null hypothesis states that $\\phi = 1$ (this is also called a unit test). The test returns several statistics we'll see in a moment. Our focus is on the p-value. A small p-value ($p<0.05$) indicates strong evidence against the null hypothesis.\n\nTo demonstrate, we'll use a dataset we know is <em>not<\/em> stationary, the airline_passenger dataset. First, let's plot the data along with a 12-month rolling mean and standard deviation:","6bc0fa7b":"## ACF plots","d3231bef":"### Run the model on the full dataset and forecast the future","b1f1166a":"### Choosing X limits by Slice","84be0391":"### we can see above that 'Date' column is not a datetime object. we can use 2 methods to make it a date column.\n1. Either during importing, we set parse_dates=True and index_col='Date'\n2. set index of the dataframe to 'Date' and convert 'Date' to datetime object using pandas to_datetime","a0c2ac5e":"### ETS","18b35862":"There is little evidence of autocorrelation here.","1c590ebb":"### Comparing SMA to EWMA","737a5df4":"#### Checking non stationary data","bb1caad4":"# 2.Time Series analysis with Statsmodels","9c8f1fc7":"### Simple Moving Average","eaf0e987":"## Rolling","15df29bc":"#### Introduction to Statsmodels\nStatsmodels is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. An extensive list of result statistics are available for each estimator. The results are tested against existing statistical packages to ensure that they are correct. The package is released under the open source Modified BSD (3-clause) license. The online documentation is hosted at statsmodels.org. The statsmodels version used in the development of this course is 0.9.0.","e04b0ed9":"## Stationarity\n\nTime series data is said to be <em>stationary<\/em> if it does <em>not<\/em> exhibit trends or seasonality. That is, fluctuations in the data are entirely due to outside forces and noise. The file <tt>samples.csv<\/tt> contains made-up datasets that illustrate stationary and non-stationary data.","922dd524":"The Hodrick-Prescott filter separates a time-series  \ud835\udc66\ud835\udc61  into a trend component  \ud835\udf0f\ud835\udc61  and a cyclical component  \ud835\udc50\ud835\udc61 \n\ud835\udc66\ud835\udc61=\ud835\udf0f\ud835\udc61+\ud835\udc50\ud835\udc61 \nThe components are determined by minimizing the following quadratic loss function, where  \ud835\udf06  is a smoothing parameter:\n\nmin\ud835\udf0f\ud835\udc61\u2211\ud835\udc47\ud835\udc61=1\ud835\udc502\ud835\udc61+\ud835\udf06\u2211\ud835\udc47\ud835\udc61=1[(\ud835\udf0f\ud835\udc61\u2212\ud835\udf0f\ud835\udc61\u22121)\u2212(\ud835\udf0f\ud835\udc61\u22121\u2212\ud835\udf0f\ud835\udc61\u22122)]2 \nThe  \ud835\udf06  value above handles variations in the growth rate of the trend component.\nWhen analyzing quarterly data, the default lambda value of 1600 is recommended. Use 6.25 for annual data, and 129,600 for monthly data.","36823b66":"Although minor, it does appear that a multiplicative adjustment gives better results. Note that the green line almost completely overlaps the original data","6f3d38b6":"## Autocorrelation for 1D\nThe correlation $\\rho$ (rho) between two variables $y_1,y_2$ is given as:\n\n### $\\rho = \\frac {\\operatorname E[(y_1\u2212\\mu_1)(y_2\u2212\\mu_2)]} {\\sigma_{1}\\sigma_{2}} = \\frac {\\operatorname {Cov} (y_1,y_2)} {\\sigma_{1}\\sigma_{2}}$,\n\nwhere $E$ is the expectation operator, $\\mu_{1},\\sigma_{1}$ and $\\mu_{2},\\sigma_{2}$ are the means and standard deviations of $y_1$ and $y_2$.\n\nWhen working with a single variable (i.e. <em>autocorrelation<\/em>) we would consider $y_1$ to be the original series and $y_2$ a lagged version of it. Note that with autocorrelation we work with $\\bar y$, that is, the full population mean, and <em>not<\/em> the means of the reduced set of lagged factors (see note below).\n\nThus, the formula for $\\rho_k$ for a time series at lag $k$ is:\n\n${\\displaystyle \\rho_k = \\frac {\\sum\\limits_{t=1}^{n-k} (y_t - \\bar{y})(y_{t+k}-\\bar{y})} {\\sum\\limits_{t=1}^{n} (y_t - \\bar{y})^2}}$\n\nThis can be written in terms of the covariance constant $\\gamma_k$ as:\n\n${\\displaystyle \\rho_k = \\frac {\\gamma_k n} {\\gamma_0 n} = \\frac {\\gamma_k} {\\sigma^2}}$\n\nFor example,<br>\n$\\rho_4 = \\frac {\\gamma_4} {\\sigma^2} = \\frac{-0.6} {8} = -0.075$\n\nNote that ACF values are bound by -1 and 1. That is, ${\\displaystyle -1 \\leq \\rho_k \\leq 1}$","8d135329":"### Unbiased Autocovariance\nNote that the number of terms in the calculations above are decreasing.<br>Statsmodels can return an \"unbiased\" autocovariance where instead of dividing by $n$ we divide by $n-k$.\n\n$\\gamma_0 = \\frac {(13-10)(13-10)+(5-10)(5-10)+(11-10)(11-10)+(12-10)(12-10)+(9-10)(9-10)} {5-0} = \\frac {40} 5 = 8.0 \\\\\n\\gamma_1 = \\frac {(13-10)(5-10)+(5-10)(11-10)+(11-10)(12-10)+(12-10)(9-10)} {5-1} = \\frac {-20} 4 = -5.0 \\\\\n\\gamma_2 = \\frac {(13-10)(11-10)+(5-10)(12-10)+(11-10)(9-10)} {5-2} = \\frac {-8} 3 = -2.67 \\\\\n\\gamma_3 = \\frac {(13-10)(12-10)+(5-10)(9-10)} {5-3} = \\frac {11} 2 = 5.5 \\\\\n\\gamma_4 = \\frac {(13-10)(9-10)} {5-4} = \\frac {-3} 1 = -3.0$","bb831eba":"### Statsmodels ","c60b5dc7":"## ACF and PACF\n## Autocorrelation Function \/ Partial Autocorrelation Function\nBefore we can investigate <em>autoregression<\/em> as a modeling tool, we need to look at <em>covariance<\/em> and <em>correlation<\/em> as they relate to lagged (shifted) samples of a time series.\n\n\n#### Goals\n * Be able to create ACF and PACF charts\n * Create these charts for multiple times series, one with seasonality and another without\n * Be able to calculate Orders PQD terms for ARIMA off these charts (highlight where they cross the x axis)\n \n<div class=\"alert alert-info\"><h3>Related Functions:<\/h3>\n<tt><strong>\n<a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.acovf.html'>stattools.acovf<\/a><\/strong><font color=black>(x[, unbiased, demean, fft, \u2026])<\/font>&nbsp;Autocovariance for 1D<br>\n<strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.acf.html'>stattools.acf<\/a><\/strong><font color=black>(x[, unbiased, nlags, qstat, \u2026])<\/font>&nbsp;&nbsp;Autocorrelation function for 1d arrays<br>\n<strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.pacf.html'>stattools.pacf<\/a><\/strong><font color=black>(x[, nlags, method, alpha])<\/font>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Partial autocorrelation estimated<br>\n<strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.pacf_yw.html'>stattools.pacf_yw<\/a><\/strong><font color=black>(x[, nlags, method])<\/font>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Partial autocorrelation estimated with non-recursive yule_walker<br>\n<strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.pacf_ols.html'>stattools.pacf_ols<\/a><\/strong><font color=black>(x[, nlags])<\/font>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Calculate partial autocorrelations<\/tt>\n   \n<h3>Related Plot Methods:<\/h3>\n<tt><strong>\n<a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.graphics.tsaplots.plot_acf.html'>tsaplots.plot_acf<\/a><\/strong><font color=black>(x)<\/font>&nbsp;&nbsp;&nbsp;Plot the autocorrelation function<br>\n<strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.graphics.tsaplots.plot_pacf.html'>tsaplots.plot_pacf<\/a><\/strong><font color=black>(x)<\/font>&nbsp;&nbsp;Plot the partial autocorrelation function<\/tt>\n\n<h3>For Further Reading:<\/h3>\n<strong>\n<a href='https:\/\/en.wikipedia.org\/wiki\/Autocovariance'>Wikipedia:<\/a><\/strong>&nbsp;&nbsp;<font color=black>Autocovariance<\/font><br>\n<strong>\n<a href='https:\/\/otexts.com\/fpp2\/autocorrelation.html'>Forecasting: Principles and Practice<\/a><\/strong>&nbsp;&nbsp;<font color=black>Autocorrelation<\/font><br>\n<strong>\n<a href='https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section4\/pmc4463.htm'>NIST Statistics Handbook<\/a><\/strong>&nbsp;&nbsp;<font color=black>Partial Autocorrelation Plot<\/font><\/div>\n","e11af0ca":"#### Fitting the model\n    ","7e5a2c81":"Where $x_t$ is the input value, $w_i$ is the applied weight (Note how it can change from $i=0$ to $t$), and $y_t$ is the output.\n\nNow the question is, how to we define the weight term $w_i$?\n\nThis depends on the <tt>adjust<\/tt> parameter you provide to the <tt>.ewm()<\/tt> method.\n\nWhen <tt>adjust=True<\/tt> (default) is used, weighted averages are calculated using weights equal to $w_i = (1 - \\alpha)^i$\n\nwhich gives\n\n### $y_t = \\frac{x_t + (1 - \\alpha)x_{t-1} + (1 - \\alpha)^2 x_{t-2} + ...\n+ (1 - \\alpha)^t x_{0}}{1 + (1 - \\alpha) + (1 - \\alpha)^2 + ...\n+ (1 - \\alpha)^t}$","5f5aa388":"Very clear seasonality observed as per above graph","127cfe94":"* <strong>Span<\/strong> corresponds to what is commonly called an \u201cN-day EW moving average\u201d.\n* <strong>Center of mass<\/strong> has a more physical interpretation and can be thought of in terms of span: $c=(s\u22121)\/2$\n* <strong>Half-life<\/strong> is the period of time for the exponential weight to reduce to one half.\n* <strong>Alpha<\/strong> specifies the smoothing factor directly.\n\nWe have to pass precisely one of the above into the <tt>.ewm()<\/tt> function. For our data we'll use <tt>span=12<\/tt>.","57396187":"This shows a recommended (p,d,q) ARIMA Order of (1,1,1), with no seasonal_order component","5bf77c98":"# ARIMA","b58f2c69":"### 2.1 Using statsmodels to get the trend","7fc22576":"As seen above, seasonal graph has very small range suggesting no or very low seasonality","00158660":"### Split the test into train and test set","894b5bd4":"Non stationary data as there are no. of lags before ACF values drop off","d7dc4643":"### 2.2 ETS Models ( Error, Trend, Seasonality)\n* Exponential smooting\n* Trend methods models\n* ETS Decomposition\n\nAs we begin working with endogenous data (\"endog\" for short) and start to develop forecasting models, it helps to identify and isolate factors working within the system that influence behavior. Here the name \"endogenous\" considers internal factors, while \"exogenous\" would relate to external forces. These fall under the category of state space models, and include decomposition (described below), and exponential smoothing (described in an upcoming section).\n\nThe decomposition of a time series attempts to isolate individual components such as error, trend, and seasonality (ETS). We've already seen a simplistic example of this in the Introduction to Statsmodels section with the Hodrick-Prescott filter. There we separated data into a trendline and a cyclical feature that mapped observed data back to the trend.","0306eb40":"When <tt>adjust=False<\/tt> is specified, moving averages are calculated as:\n\n### $\\begin{split}y_0 &= x_0 \\\\\ny_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t,\\end{split}$\n\nwhich is equivalent to using weights:\n\n \\begin{split}w_i = \\begin{cases}\n    \\alpha (1 - \\alpha)^i & \\text{if } i < t \\\\\n    (1 - \\alpha)^i        & \\text{if } i = t.\n\\end{cases}\\end{split}","a367d046":"As $k$ increases, we can solve for $\\phi_k$ using matrix algebra and the <a href='https:\/\/en.wikipedia.org\/wiki\/Levinson_recursion'>Levinson\u2013Durbin recursion<\/a> algorithm which maps the sample autocorrelations $\\rho$ to a <a href='https:\/\/en.wikipedia.org\/wiki\/Toeplitz_matrix'>Toeplitz<\/a> diagonal-constant matrix. The full solution is beyond the scope of this course, but the setup is as follows:\n\n\n$\\displaystyle \\begin{pmatrix}\\rho_0&\\rho_1&\\cdots &\\rho_{k-1}\\\\\n\\rho_1&\\rho_0&\\cdots &\\rho_{k-2}\\\\\n\\vdots &\\vdots &\\ddots &\\vdots \\\\\n\\rho_{k-1}&\\rho_{k-2}&\\cdots &\\rho_0\\\\\n\\end{pmatrix}\\quad \\begin{pmatrix}\\phi_{k1}\\\\\\phi_{k2}\\\\\\vdots\\\\\\phi_{kk}\\end{pmatrix}\n\\mathbf = \\begin{pmatrix}\\rho_1\\\\\\rho_2\\\\\\vdots\\\\\\rho_k\\end{pmatrix}$","d261538b":"As seen above, result is stationary","c81eadc2":":\n\n\\begin{split}\\alpha =\n \\begin{cases}\n     \\frac{2}{s + 1},               & \\text{for span}\\ s \\geq 1\\\\\n     \\frac{1}{1 + c},               & \\text{for center of mass}\\ c \\geq 0\\\\\n     1 - \\exp^{\\frac{\\log 0.5}{h}}, & \\text{for half-life}\\ h > 0\n \\end{cases}\\end{split}\n","63dcd9d7":"## Plotting","78dbdaf8":"### pmdarima","632b45a1":"#### As per auto arima, d=1 so let's check if difference with first order shift is stationary or not.","93aad09f":"### Simple Exponential Smoothing\nThe above example employed <em>Simple Exponential Smoothing<\/em> with one smoothing factor <strong>\u03b1<\/strong>. Unfortunately, this technique does a poor job of forecasting when there is a trend in the data as seen above","38bbe9c6":"### 1.4 Shifting and rolling","6b8380a1":"### Below are the aliases which you can use while resampling\nfor example \"A\" means resample to yearly","106384f1":"### Tests for Stationarity\nA time series is <em>stationary<\/em> if the mean and variance are fixed between any two equidistant points. That is, no matter where you take your observations, the results should be the same. A times series that shows seasonality is <em>not<\/em> stationary.\n\nA test for stationarity usually involves a <a href='https:\/\/en.wikipedia.org\/wiki\/Unit_root_test'>unit root<\/a> hypothesis test, where the null hypothesis $H_0$ is that the series is <em>nonstationary<\/em>, and contains a unit root. The alternate hypothesis $H_1$ supports stationarity. The <a href='https:\/\/en.wikipedia.org\/wiki\/Augmented_Dickey-Fuller_test'>augmented Dickey-Fuller<\/a> and <a href='https:\/\/en.wikipedia.org\/wiki\/KPSS_test'>Kwiatkowski-Phillips-Schmidt-Shin<\/a> tests are stationarity tests. ","50e6a3ce":"It also gave similar results and we only had to specify start date, with periods and frequency.","4acbed13":"### Let's run an ETS decomposition","8f5f0ef2":"![alias.JPG](attachment:alias.JPG)","4b1d0b3d":"Here we can see that Double Exponential Smoothing is a much better representation of the time series data.\nLet's see if using a multiplicative seasonal adjustment helps.","2669db6a":"![stationary_1.JPG](attachment:stationary_1.JPG)![stationary_2.JPG](attachment:stationary_2.JPG)","3e059e19":"Based on this chart, it looks like the trend in the earlier days is increasing at a higher rate than just linear (although it is a  bit hard to tell from this one plot).","8252d4c6":"we can see the trend clearly going upwards. Also huge seasonality component involved. ","f885c3ec":"### 1.4 Time Resampling","a3f85a6b":"### Autocovariance Example:\nSay we have a time series with five observations: {13, 5, 11, 12, 9}.<br>\nWe can quickly see that $n = 5$, the mean $\\bar{y} = 10$, and we'll see that the variance $\\sigma^2 = 8$.<br>\nThe following calculations give us our covariance values:\n<br><br>\n$\\gamma_0 = \\frac {(13-10)(13-10)+(5-10)(5-10)+(11-10)(11-10)+(12-10)(12-10)+(9-10)(9-10)} 5 = \\frac {40} 5 = 8.0 \\\\\n\\gamma_1 = \\frac {(13-10)(5-10)+(5-10)(11-10)+(11-10)(12-10)+(12-10)(9-10)} 5 = \\frac {-20} 5 = -4.0 \\\\\n\\gamma_2 = \\frac {(13-10)(11-10)+(5-10)(12-10)+(11-10)(9-10)} 5 = \\frac {-8} 5 = -1.6 \\\\\n\\gamma_3 = \\frac {(13-10)(12-10)+(5-10)(9-10)} 5 = \\frac {11} 5 = 2.2 \\\\\n\\gamma_4 = \\frac {(13-10)(9-10)} 5 = \\frac {-3} 5 = -0.6$\n<br><br>\nNote that $\\gamma_0$ is just the population variance $\\sigma^2$\n\nLet's see if statsmodels gives us the same results! For this we'll create a <strong>fake<\/strong> dataset:","55f153e5":"### Exponentially moving Average\n\nWe just showed how to calculate the SMA based on some window. However, basic SMA has some weaknesses:\n* Smaller windows will lead to more noise, rather than signal\n* It will always lag by the size of the window\n* It will never reach to full peak or valley of the data due to the averaging.\n* Does not really inform you about possible future behavior, all it really does is describe trends in your data.\n* Extreme historical values can skew your SMA significantly\n\nTo help fix some of these issues, we can use an <a href='https:\/\/en.wikipedia.org\/wiki\/Exponential_smoothing'>EWMA (Exponentially weighted moving average)<\/a>.","8caa527b":"# 4. Introduction to ARIMA Models\nLet's study variety of models\n\n<strong>ARIMA<\/strong>, or <em>Autoregressive Integrated Moving Average<\/em> is actually a combination of 3 models:\n* <strong>AR(p)<\/strong> Autoregression - a regression model that utilizes the dependent relationship between a current observation and observations over a previous period\n* <strong>I(d)<\/strong> Integration - uses differencing of observations (subtracting an observation from an observation at the previous time step) in order to make the time series stationary\n* <strong>MA(q)<\/strong> Moving Average - a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\n<strong>Moving Averages<\/strong> we've already seen with EWMA and the Holt-Winters Method.<br>\n<strong>Integration<\/strong> will apply differencing to make a time series stationary, which ARIMA requires.<br>\n<strong>Autoregression<\/strong> is explained in detail in the next section. Here we're going to correlate a current time series with a lagged version of the same series.<br>\nOnce we understand the components, we'll investigate how to best choose the $p$, $d$ and $q$ values required by the model.","4b32db1e":"# 3. General Forecasting models ","090b2df2":"#### Train Test split","b2a26b85":"### 1.1 Datetime Index","b71eff66":"## Partial Autocorrelation\nPartial autocorrelations measure the linear dependence of one variable after removing the effect of other variable(s) that affect both variables. That is, the partial autocorrelation at lag $k$ is the autocorrelation between $y_t$ and $y_{t+k}$ that is not accounted for by lags $1$ through $k\u22121$.\n\nA common method employs the non-recursive <a href='https:\/\/en.wikipedia.org\/wiki\/Autoregressive_model#Calculation_of_the_AR_parameters'>Yule-Walker Equations<\/a>:\n\n$\\phi_0 = 1\\\\\n\\phi_1 = \\rho_1 = -0.50\\\\\n\\phi_2 = \\frac {\\rho_2 - {\\rho_1}^2} {1-{\\rho_1}^2} = \\frac {(-0.20) - {(-0.50)}^2} {1-{(-0.50)}^2}= \\frac {-0.45} {0.75} = -0.60$","4f8414c4":"# 1.Time Series with Pandas","f7c04993":"### Evaluating predictions","66907530":"we can also limit the data before plotting if we want to focus only towards a particular interval. Let's see below","8f6299c8":"Clearly there is a strong correlation between y and it's lagged values. hence it is not stationary","fb33375c":"we can also add \"step size\" to give us only the desired dates after each step size. 10 is number of days as \"D\" is mentioned in datetime type","21ad3adb":"Let's create some predictions","60983883":"### Plot the data","2e5f0fc0":"### Holt-Winters Methods\nwe discussed  on <strong>Exponentially Weighted Moving Averages<\/strong> (EWMA) we applied <em>Simple Exponential Smoothing<\/em> using just one smoothing factor $\\alpha$ (alpha). This failed to account for other contributing factors like trend and seasonality.\n\nNow, we'll look at <em>Double<\/em> and <em>Triple Exponential Smoothing<\/em> with the <a href='https:\/\/otexts.com\/fpp2\/holt-winters.html'>Holt-Winters Methods<\/a>. \n\nIn <strong>Double Exponential Smoothing<\/strong> (aka Holt's Method) we introduce a new smoothing factor $\\beta$ (beta) that addresses trend:\n\n\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\nb_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\ny_t &= l_t + b_t & \\text{    fitted model}\\\\\n\\hat y_{t+h} &= l_t + hb_t & \\text{    forecasting model (} h = \\text{# periods into the future)}\\end{split}\n\nBecause we haven't yet considered seasonal fluctuations, the forecasting model is simply a straight sloped line extending from the most recent data point. We'll see an example of this in upcoming lectures.\n\nWith <strong>Triple Exponential Smoothing<\/strong> (aka the Holt-Winters Method) we introduce a smoothing factor $\\gamma$ (gamma) that addresses seasonality:\n\n\\begin{split}l_t &= (1 - \\alpha) l_{t-1} + \\alpha x_t, & \\text{    level}\\\\\nb_t &= (1-\\beta)b_{t-1} + \\beta(l_t-l_{t-1}) & \\text{    trend}\\\\\nc_t &= (1-\\gamma)c_{t-L} + \\gamma(x_t-l_{t-1}-b_{t-1}) & \\text{    seasonal}\\\\\ny_t &= (l_t + b_t) c_t & \\text{    fitted model}\\\\\n\\hat y_{t+m} &= (l_t + mb_t)c_{t-L+1+(m-1)modL} & \\text{    forecasting model (} m = \\text{# periods into the future)}\\end{split}\n\nHere $L$ represents the number of divisions per cycle. In our case looking at monthly data that displays a repeating pattern each year, we would use $L=12$.\n\nIn general, higher values for $\\alpha$, $\\beta$ and $\\gamma$ (values closer to 1), place more emphasis on recent data.\n\n<div class=\"alert alert-info\"><h3>Related Functions:<\/h3>\n<tt><strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.holtwinters.SimpleExpSmoothing.html'>statsmodels.tsa.holtwinters.SimpleExpSmoothing<\/a><\/strong><font color=black>(endog)<\/font>&nbsp;&nbsp;&nbsp;&nbsp;\nSimple Exponential Smoothing<br>\n<strong><a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.holtwinters.ExponentialSmoothing.html'>statsmodels.tsa.holtwinters.ExponentialSmoothing<\/a><\/strong><font color=black>(endog)<\/font>&nbsp;&nbsp;\n    Holt-Winters Exponential Smoothing<\/tt>\n    \n<h3>For Further Reading:<\/h3>\n<tt>\n<strong>\n<a href='https:\/\/www.itl.nist.gov\/div898\/handbook\/pmc\/section4\/pmc43.htm'>NIST\/SEMATECH e-Handbook of Statistical Methods<\/a><\/strong>&nbsp;&nbsp;<font color=black>What is Exponential Smoothing?<\/font><\/tt><\/div>","5487e1c4":"### Shifting","094e0cb7":"Orders are (0,1,0)","916c27de":"We can create a rolling mean dataframe or series using rolling method. We need to specify the window size and mean will be created according to the window size. For ex: window size of 6 means, it will take the first 6 samples and create a mean value and them move from second sample to next 6 and so on","f63dd47e":"When <tt>adjust=True<\/tt> we have $y_0=x_0$ and from the last representation above we have \n$y_t=\\alpha x_t+(1\u2212\u03b1)y_{t\u22121}$, therefore there is an assumption that $x_0$ is not an ordinary value but rather an exponentially weighted moment of the infinite series up to that point.\n\nFor the smoothing factor $\\alpha$ one must have $0<\\alpha\u22641$, and while it is possible to pass <em>alpha<\/em> directly, it\u2019s often easier to think about either the <em>span<\/em>, <em>center of mass<\/em> (com) or <em>half-life<\/em> of an EW moment:","89dad56e":"Our Null hypotheses in Adfuller test is that Data is NON STATIONARY p-value is greater than 0.05 hence we fail to reject the null hypothesis. Hence data is non stationary","a7ce6581":"## Autocovariance for 1D\nIn a <em>deterministic<\/em> process, like $y=sin(x)$, we always know the value of $y$ for a given value of $x$. However, in a <em>stochastic<\/em> process there is always some randomness that prevents us from knowing the value of $y$. Instead, we analyze the past (or <em>lagged<\/em>) behavior of the system to derive a probabilistic estimate for $\\hat{y}$.\n\nOne useful descriptor is <em>covariance<\/em>. When talking about dependent and independent $x$ and $y$ variables, covariance describes how the variance in $x$ relates to the variance in $y$. Here the size of the covariance isn't really important, as $x$ and $y$ may have very different scales. However, if the covariance is positive it means that $x$ and $y$ are changing in the same direction, and may be related.\n\nWith a time series, $x$ is a fixed interval. Here we want to look at the variance of $y_t$ against lagged or shifted values of $y_{t+k}$\n\nFor a stationary time series, the autocovariance function for $\\gamma$ (gamma) is given as:\n\n${\\displaystyle {\\gamma}_{XX}(t_{1},t_{2})=\\operatorname {Cov} \\left[X_{t_{1}},X_{t_{2}}\\right]=\\operatorname {E} [(X_{t_{1}}-\\mu _{t_{1}})(X_{t_{2}}-\\mu _{t_{2}})]}$\n\nWe can calculate a specific $\\gamma_k$ with:\n\n${\\displaystyle \\gamma_k = \\frac 1 n \\sum\\limits_{t=1}^{n-k} (y_t - \\bar{y})(y_{t+k}-\\bar{y})}$","b5976903":"### 1.3 pandas date_range ","ede8f338":"### 2.3 Seasonal Decomposition\nStatsmodels provides a <em>seasonal decomposition<\/em> tool we can use to separate out the different components. This lets us see quickly and visually what each component contributes to the overall behavior.\n\n\nWe apply an <strong>additive<\/strong> model when it seems that the trend is more linear and the seasonality and trend components seem to be constant over time (e.g. every year we add 10,000 passengers).<br>\nA <strong>multiplicative<\/strong> model is more appropriate when we are increasing (or decreasing) at a non-linear rate (e.g. each year we double the amount of passengers).\n\nFor these examples we'll use the International Airline Passengers dataset, which gives monthly totals in thousands from January 1949 to December 1960.","a7279528":"## Differencing\nNon-stationary data can be made to look stationary through <em>differencing<\/em>. A simple differencing method calculates the difference between consecutive points.\n\n<div class=\"alert alert-info\"><h3>Related Functions:<\/h3>\n<tt><strong>\n<a href='https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.statespace.tools.diff.html'>statespace.tools.diff<\/a><\/strong><font color=black>(series[, k_diff, \u2026])<\/font>&nbsp;&nbsp;Difference a series simply and\/or seasonally along the zero-th axis.<\/tt><\/div>","de4368fc":"### Manually checking ACF and PACF plots to find best p, d, q can be confusing and difficult, hence it is advisable to use pyramid arima library (pmdarima) which will perform gridsearch to find the best combination of p, d and q.","76eecdb4":"# Time Series Forecasting (End to End with codes)\n\n## we will work on almost all available techniques including:\n\n**1.** **Time Series analysis with pandas**\n\n   * Working with datetime index\n   * date_range to create range of dates\n   * Resampling the data\n   * Shifting and Rolling\n   \n**2.** **Time Series analysis with Statsmodels**\n\n   * Using statsmodels to get the trend, plot the data\n   * ETS Models ( Error, Trend, Seasonality)\n   * Seasonal Decomposition\n   * Moving Averages- Simple Moving Averages, Holten winters method, Exponential moving averages,    Simple exponential smoothing, Double exponential smooting.\n   \n**3.** **General forecasting models**\n\n   * Forecasting with holt winters method\n   * Stationarity\n   * Differencing\n   * Autocorrelation ACF, PACF\n   * Augmented Dickey-Fuller Test\n   \n**4. Introduction to ARIMA models**\n\n   * Choosing ARIMA orders, auto_arima (pmdarima)\n   * ARIMA\n   * SARIMA\n   * Model building and predictions\n   * plotting\n","3cfa2301":"### Now, use auto_arima to determine ARIMA orders","752c8a5a":"### 2.4 Moving Averages","5188d890":"# SARIMAX","5e081421":"data has been shifted downwards. Hence, first row becomes blank and last row gets pruned. Similarly shift(-1) will shift the data upwards","b4b59c4f":"### Forecasting with Holt-Winters method","5bbd1c6e":"This is a typical ACF plot for stationary data, with lags on the horizontal axis and correlations on the vertical axis. The first value $y_0$ is always 1. A sharp dropoff indicates that there is no AR component in the ARIMA model.\n\nNext we'll look at non-stationary data with the <strong>Airline Passengers<\/strong> dataset:","d7f524f9":"This is a daily data as you can see from dates. However, we can use resampling to convert into weekly, monthly or yearly as per our requirement","f4783a6e":"### Choosing ARIMA orders (p, d, q) !!","375e2603":"### Setting a DatetimeIndex Frequency\nNote that our DatetimeIndex does not have a frequency. In order to build a Holt-Winters smoothing model, statsmodels needs to know the frequency of the data (whether it's daily, monthly etc.). Since observations occur at the start of each month, we'll use MS.<br>A full list of time series offset aliases can be found <a href='http:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#offset-aliases'>here<\/a>.","1b2adc4c":"___\n### Simple Exponential Smoothing\n\nA variation of the statmodels Holt-Winters function provides Simple Exponential Smoothing. It will be shown that it performs the same calculation of the weighted moving average as the pandas <tt>.ewm()<\/tt> method:<br>\n$\\begin{split}y_0 &= x_0 \\\\\ny_t &= (1 - \\alpha) y_{t-1} + \\alpha x_t,\\end{split}$"}}