{"cell_type":{"3d301361":"code","be54599c":"code","fa17f770":"code","9ac8f02b":"code","068042df":"code","b610de0e":"code","6e1113af":"code","78f44044":"code","57d49838":"code","145a5aaf":"code","b068df6d":"code","8f8767a7":"code","9d951f30":"code","e7e0af66":"markdown"},"source":{"3d301361":"import torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import models\n\nimport pandas as pd\nimport torch.utils.data as data\n\ntorch.__version__,torchvision.__version__","be54599c":"root_directory = '..\/input\/mrnet-dataset\/MRNet-v1.0\/'","fa17f770":"class MRNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.pretrained_model = models.alexnet(pretrained=True)\n        self.pooling_layer = nn.AdaptiveAvgPool2d(1)\n        self.classifer = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x = torch.squeeze(x, dim=0)\n#         this below line is will try to avoid \n# RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB \n# (GPU 0; 15.90 GiB total capacity; 14.91 GiB already allocated; 25.75 MiB free; \n# 15.00 GiB reserved in total by PyTorch)\n\n        torch.cuda.empty_cache()\n        features = self.pretrained_model.features(x)\n        pooled_features = self.pooling_layer(features)\n        pooled_features = pooled_features.view(pooled_features.size(0), -1)\n        flattened_features = torch.max(pooled_features, 0, keepdim=True)[0]\n        output = self.classifer(flattened_features)\n        \n        return output","9ac8f02b":"class MRDataset(data.Dataset):\n    def __init__(self, root_dir, task, plane, train=True, transform=None, weights=None):\n        super().__init__()\n        self.task = task\n        self.plane = plane\n        self.root_dir = root_dir\n        self.train = train\n        if self.train:\n            self.folder_path = self.root_dir + 'train\/{0}\/'.format(plane)\n            self.records = pd.read_csv(\n                self.root_dir + 'train-{0}.csv'.format(task), header=None, names=['id', 'label'])\n        else:\n            transform = None\n            self.folder_path = self.root_dir + 'valid\/{0}\/'.format(plane)\n            self.records = pd.read_csv(\n                self.root_dir + 'valid-{0}.csv'.format(task), header=None, names=['id', 'label'])\n\n        self.records['id'] = self.records['id'].map(\n            lambda i: '0' * (4 - len(str(i))) + str(i))\n        self.paths = [self.folder_path + filename +\n                      '.npy' for filename in self.records['id'].tolist()]\n        self.labels = self.records['label'].tolist()\n\n        self.transform = transform\n        if weights is None:\n            pos = np.sum(self.labels)\n            neg = len(self.labels) - pos\n            self.weights = [1, neg \/ pos]\n        else:\n            self.weights = weights\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        array = np.load(self.paths[index])\n        label = self.labels[index]\n        label = torch.FloatTensor([label])\n        \n        if self.transform:\n            array = self.transform(array)\n        else:\n            array = np.stack((array,)*3, axis=1)\n            array = torch.FloatTensor(array)\n\n        if label.item() == 1:\n            weight = np.array([self.weights[1]])\n            weight = torch.FloatTensor(weight)\n        else:\n            weight = np.array([self.weights[0]])\n            weight = torch.FloatTensor(weight)\n\n        return array, label, weight","068042df":"!pip install git+https:\/\/github.com\/ncullen93\/torchsample","b610de0e":"import shutil\nimport os\nimport time\nfrom datetime import datetime\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.optim as optim\nfrom torchsample.transforms import RandomRotate, RandomTranslate, RandomFlip, Compose\nfrom torchvision import transforms\n\nfrom tensorboardX import SummaryWriter\n\nfrom sklearn import metrics","6e1113af":"def train_model(model, train_loader, epoch, num_epochs, optimizer, current_lr):\n    _ = model.train()\n\n    if torch.cuda.is_available():\n        model.cuda()\n\n    y_preds = []\n    y_trues = []\n    losses = []\n\n    for i, (image, label, weight) in enumerate(train_loader):\n        optimizer.zero_grad()\n\n        if torch.cuda.is_available():\n            \n            image = image.cuda()\n            label = label.cuda()\n            weight = weight.cuda()\n#         print(label,label[0])\n        label = label[0]\n        weight = weight[0]\n\n        prediction = model.forward(image.float())[0]\n        \n#         print(weight,prediction,prediction.shape,label,sep=' ^^^ ')\n#         loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n        criterion = torch.nn.BCEWithLogitsLoss(weight=weight)\n        loss = criterion(prediction,label)\n        loss.backward()\n        optimizer.step()\n\n        loss_value = loss.item()\n        losses.append(loss_value)\n\n        probas = torch.sigmoid(prediction)\n        \n#         print(label[0],probas[0])\n        \n        y_trues.append(int(label[0]))\n        y_preds.append(probas[0])\n\n        try:\n            auc = metrics.roc_auc_score(y_trues, y_preds)\n        except:\n            auc = 0.5\n\n        \n\n\n    train_loss_epoch = np.round(np.mean(losses), 2)\n    train_auc_epoch = np.round(auc, 2)\n    return train_loss_epoch, train_auc_epoch","78f44044":"import gc\n\n\n# print(torch.cuda.memory_summary(device=None, abbreviated=False))","57d49838":"def evaluate_model(model, val_loader, epoch, num_epochs, current_lr):\n    _ = model.eval()\n\n    if torch.cuda.is_available():\n        model.cuda()\n\n    y_trues = []\n    y_preds = []\n    losses = []\n\n    for i, (image, label, weight) in enumerate(val_loader):\n\n        if torch.cuda.is_available():\n            torch.cuda.memory_summary(device=None, abbreviated=False)\n            image = image.cuda()\n            label = label.cuda()\n            weight = weight.cuda()\n        \n        \n\n        label = label[0]\n        weight = weight[0]\n\n        prediction = model.forward(image.float())[0]\n\n#         loss = torch.nn.BCEWithLogitsLoss(weight=weight)(prediction, label)\n        criterion = torch.nn.BCEWithLogitsLoss(weight=weight)\n        loss = criterion(prediction,label)\n        \n        loss_value = loss.item()\n        losses.append(loss_value)\n\n        probas = torch.sigmoid(prediction)\n        \n        y_trues.append(int(label[0]))\n        y_preds.append(probas[0])\n\n        try:\n            auc = metrics.roc_auc_score(y_trues, y_preds)\n        except:\n            auc = 0.5\n\n    val_loss_epoch = np.round(np.mean(losses), 2)\n    val_auc_epoch = np.round(auc, 2)\n    return val_loss_epoch, val_auc_epoch","145a5aaf":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","b068df6d":"def customized_run(task,plane,lr,lr_scheduler,gamma,epochs,patience):\n    augmentor = Compose([\n        transforms.Lambda(lambda x: torch.Tensor(x)),\n        RandomRotate(25),\n        RandomTranslate([0.11, 0.11]),\n        RandomFlip(),\n        transforms.Lambda(lambda x: x.repeat(3, 1, 1, 1).permute(1, 0, 2, 3)),\n    ])\n\n    train_dataset = MRDataset(root_directory, task,\n                              plane, transform=augmentor, train=True)\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=1, shuffle=True, num_workers=4, drop_last=False)\n\n    validation_dataset = MRDataset(\n        root_directory, task, plane, train=False)\n    validation_loader = torch.utils.data.DataLoader(\n        validation_dataset, batch_size=1, shuffle=-True, num_workers=4, drop_last=False)\n\n    mrnet = MRNet()\n\n    if torch.cuda.is_available():\n        mrnet = mrnet.cuda()\n\n    optimizer = optim.Adam(mrnet.parameters(), lr=lr, weight_decay=0.1)\n\n    if lr_scheduler == \"plateau\":\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer, patience=3, factor=.3, threshold=1e-4, verbose=True)\n    elif lr_scheduler == \"step\":\n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, step_size=3, gamma=args.gamma)\n\n    best_val_loss = float('inf')\n    best_val_auc = float(0)\n\n    num_epochs = epochs\n    iteration_change_loss = 0\n    \n    t_start_training = time.time()\n\n    for epoch in range(num_epochs):\n        current_lr = get_lr(optimizer)\n\n        t_start = time.time()\n        \n        train_loss, train_auc = train_model(\n            mrnet, train_loader, epoch, num_epochs, optimizer, current_lr)\n        \n#         let's clean this\n        \n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        val_loss, val_auc = evaluate_model(\n            mrnet, validation_loader, epoch, num_epochs, current_lr)\n\n        if lr_scheduler == 'plateau':\n            scheduler.step(val_loss)\n        elif lr_scheduler == 'step':\n            scheduler.step()\n\n        t_end = time.time()\n        delta = t_end - t_start\n\n        print(\"train loss : {0} | train auc {1} | val loss {2} | val auc {3} | elapsed time {4} s\".format(\n            train_loss, train_auc, val_loss, val_auc, delta))\n\n        iteration_change_loss += 1\n        print('-' * 30)\n\n        if val_auc > best_val_auc:\n            best_val_auc = val_auc\n            \n            file_name = f'model_abirs_{task}_{plane}_val_auc_{val_auc:0.2f}_train_auc_{train_auc:0.2f}_epoch_{epoch+1}.pth'\n            for f in os.listdir('.\/models\/'):\n                if (args.task in f) and (args.plane in f) and (args.prefix_name in f):\n                    os.remove(f'.\/models\/{f}')\n            torch.save(mrnet, f'.\/models\/{file_name}')\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            iteration_change_loss = 0\n\n        if iteration_change_loss == patience:\n            print('Early stopping after {0} iterations without the decrease of the val loss'.\n                  format(iteration_change_loss))\n            break\n    t_end_training = time.time()\n    print(f'training took {t_end_training - t_start_training} s')","8f8767a7":"# recommended\nlr = 0.00001\nlr_scheduler = 'plateau'\ngamma=0.5\nepochs=50\npatience = 5\n\ncustomized_run('acl','coronal',lr,lr_scheduler,gamma,epochs,patience)","9d951f30":"print('Done')","e7e0af66":"# **Start's from Here**"}}