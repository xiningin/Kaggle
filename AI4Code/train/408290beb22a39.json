{"cell_type":{"dabcab93":"code","5cdc491f":"code","e6891baa":"code","62c7b324":"code","1ad931ba":"code","8e3e3e92":"code","c84584e5":"code","80312341":"code","3b78f070":"code","ad8694a5":"code","8d7cf6b7":"code","add12622":"code","6871a548":"code","e0da8e0d":"code","f9307412":"code","fbd71949":"code","cbacfc17":"code","b0c26bed":"code","212e2626":"code","c83b00b7":"code","79c84d9f":"code","b1cdf3d6":"code","f323aa25":"code","3beb4fb1":"code","a9f5789c":"markdown","6148e6c8":"markdown","e9e6e53d":"markdown","ca55068f":"markdown","ce85f7db":"markdown"},"source":{"dabcab93":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nimport keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n","5cdc491f":"train = pd.read_csv(\"..\/input\/teleplay\/Teleplay.csv\", delimiter=\",\")\ntrain[\"members\"].fillna(0, inplace=True)\ntrain[\"type\"].fillna(\"medium\", inplace=True)","e6891baa":"train.head()","62c7b324":"train.describe()\n","1ad931ba":"\n\ndef tokenize(input_str: str = None) -> list:\n    try:\n        assert isinstance(input_str, str)\n    except AssertionError as e:\n        return list()\n\n    return [word.strip() for word in input_str.split(\",\")]\n\n\ndef get_all_genre() -> list:\n    genre = train[\"genre\"]\n    all_genre = set()\n    for line in genre:\n        for item in tokenize(line):\n            all_genre.add(item)\n    # Python set does not guarantee order, so convert to list\n    return list(all_genre)\n\n\nall_genre = get_all_genre()\nall_type = np.unique(train[\"type\"]).tolist()\n","8e3e3e92":"\n\ndef encode_genre(arr: list, all_genre: list) -> list:\n    \"\"\"\n        encode genre to a vector, like \"multi-hot encoder\"\n    \"\"\"\n    encoded = np.zeros_like(all_genre, dtype=np.int64)\n    for genre in arr:\n        encoded[all_genre.index(genre)] = 1\n    return encoded\n\n\ndef encode_type(arr: list, all_type: list) -> list:\n    encoded = [all_type.index(t) for t in arr]\n    return encoded\n\n","c84584e5":"encoded_genre = pd.DataFrame(\n    np.array([encode_genre(tokenize(genre), all_genre) for genre in train[\"genre\"]], dtype=np.int64), columns=all_genre, dtype=np.int64)\nencoded_type = pd.DataFrame(\n    np.array(encode_type(train[\"type\"], all_type), dtype=np.int64), columns=[\"type\"])\n\ntrain = train.drop([\"genre\", \"type\", \"name\"], axis=1)\ntrain.loc[train[\"episodes\"] == \"Unknown\", \"episodes\"] = 0.0\n\ntrain = train.astype(\"float\")\n\ntrain = pd.concat([train, encoded_type], axis=1)\ntrain = pd.concat([train, encoded_genre], axis=1)\ntrain.dropna(axis=0, how=\"any\", inplace=True)\n\n# boxplot before capping\nplt.rc(\"figure\", figsize=(10, 6))\nfig, axes = plt.subplots(2, 1)\nsns.boxplot(data=train, x=\"episodes\", ax=axes[0])\nsns.boxplot(data=train, x=\"members\", ax=axes[1])\naxes[0].set_title(\n    \"Box Plot illustrating long-tailed distribution of some features\")\nplt.show()\n\n#!! baselines","80312341":"model_result = []  # RMSE of models\n\ntrain = train.astype(\"float\")\ntrain_Y = train[\"rating\"]\ntrain_X = train.drop([\"rating\", \"teleplay_id\"], axis=1)","3b78f070":"rf = RandomForestRegressor(random_state=4434)\ntrain_X, val_X, train_Y, val_Y = train_test_split(\n    train_X, train_Y, test_size=0.2)\nrf.fit(train_X, train_Y)\nmean_squared_error(val_Y, rf.predict(val_X))\nresult = np.sqrt(cross_val_score(rf, train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE: %.4f\" % result.mean())\nmodel_result.append(result.mean())","ad8694a5":"plt.rc(\"figure\", figsize=(6, 5))\nimportance = rf.feature_importances_\nsns.barplot(train_X.columns[:3], importance[:3], palette=\"Blues_d\")","8d7cf6b7":"# #!data visualization","add12622":"plt.rc(\"figure\", figsize=(15, 10))\nfig, axes = plt.subplots(2, 2)\nsns.distplot(train[\"rating\"], ax=axes[0][0])\naxes[0][0].set_title(\"Distribution of Rating\")\nsns.distplot(train[train[\"episodes\"] < 100][\"episodes\"], ax=axes[0][1])\naxes[0][1].set_title(\"Distribution of episodes (trimmed)\")\nsns.distplot(train[train[\"members\"] < 1e4][\"members\"], ax=axes[1][0])\naxes[1][0].set_title(\"Distribution of members (trimmed)\")\nsns.histplot(train[\"type\"], ax=axes[1][1],)\naxes[1][1].set_title(\"Distribution of types\")\naxes[1][1].set_xticklabels([\"\"]+all_type)\nplt.show()","6871a548":"pair_fig = sns.pairplot(train[[\"episodes\", \"rating\", \"members\", \"type\"]])\npair_fig.savefig(\"pair_plot.png\")\nplt.rc(\"figure\", figsize=(6, 4))","e0da8e0d":"plt.rc(\"figure\", figsize=(6, 4))\ncorr = sns.heatmap(train[[\"episodes\", \"rating\", \"members\", \"type\"]\n                         ].corr(), vmin=0, vmax=1, center=0, cmap=\"YlGnBu\", annot=True)\ncorr.get_figure().savefig(\"corr.png\")\n\n# capping\ntrain.loc[train[\"episodes\"] > 1000, \"episodes\"] = 1000\ntrain.loc[train[\"members\"] > 10000, \"members\"] = 10000","f9307412":"lr = LinearRegression()\nresult = np.sqrt(cross_val_score(lr, train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE of LR: %.4f\" % result.mean())\nmodel_result.append(result.mean())","fbd71949":"expansion = PolynomialFeatures()\npoly_train_X = expansion.fit_transform(train_X)\nPLR = LinearRegression()\nresult = np.sqrt(cross_val_score(PLR, poly_train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE of Polynomial LR: %.4f\" % result.mean())\nmodel_result.append(result.mean())\n\n","cbacfc17":"mlp = keras.models.Sequential()\nmlp.add(keras.layers.Input([46, ]))\nmlp.add(keras.layers.BatchNormalization())\nmlp.add(keras.layers.Dense(300, activation=\"relu\"))\nmlp.add(keras.layers.BatchNormalization())\nmlp.add(keras.layers.Dense(200, activation=\"relu\"))\nmlp.add(keras.layers.BatchNormalization())\nmlp.add(keras.layers.Dense(1))\nmlp.compile(loss=['mse'], metrics=['mse'])\nmlp.summary()\nes = keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\nhistory = mlp.fit(train_X, train_Y, epochs=40,\n                  validation_data=(val_X, val_Y), callbacks=[es],)\nplt.rc(\"figure\", figsize=(6, 4))\nplt.xlabel(\"epoches\")\nplt.ylabel(\"loss\")\nplt.plot(history.history['mse'])\nplt.plot(history.history['val_mse'])\nresult = np.sqrt(mean_squared_error(train_Y, mlp.predict(train_X)))\nmodel_result.append(result.mean())\n# result = cross_val_score(mlp, train_X, train_Y,\n#                          scoring=make_scorer(mean_squared_error))","b0c26bed":"# a magic number 27, I will show the reason in final report.\nknr = KNeighborsRegressor(27)\nknr.fit(train_X, train_Y)\nresult = np.sqrt(cross_val_score(knr, train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE of knn: %.4f\" % result.mean())\nmodel_result.append(result.mean())\n","212e2626":"gbr = GradientBoostingRegressor(random_state=4434)\ngbr.fit(train_X, train_Y)\nresult = np.sqrt(cross_val_score(gbr, train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE of gbr: %.4f\" % result.mean())\nmodel_result.append(result.mean())","c83b00b7":"svr = SVR()\nsvr.fit(train_X, train_Y)\nresult = np.sqrt(cross_val_score(svr, train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE of svr: %.4f\" % result.mean())\nmodel_result.append(result.mean())","79c84d9f":"plt.rc(\"figure\", figsize=(8, 6))\nsns.barplot([\"RF\", \"LR\", \"Polynomial LR\", \"MLP\", \"KNN\", \"GBDT\", \"SVM\"],\n            model_result, palette=\"Blues_d\")\nplt.ylabel(\"5-fold CV RMSE\")","b1cdf3d6":"print(\"training Stacking model may take long time, please wait...\")\nestimators = [('lr', lr), ('rf', rf), ('knn', knr),\n              ('gbdt', gbr), ('svm', svr)]\nstacking = StackingRegressor(\n    estimators=estimators, final_estimator=LinearRegression())\nstacking.fit(train_X, train_Y)\nresult = np.sqrt(cross_val_score(stacking, train_X, train_Y,\n                                 scoring=make_scorer(mean_squared_error), cv=5))\nprint(\"avg RMSE of stacking: %.4f\" % result.mean())\nmodel_result.append(result.mean())","f323aa25":"new_tele = pd.read_csv(\"..\/input\/teleplay\/New_Teleplay.csv\", delimiter=\",\")\nnew_tele[\"members\"].fillna(0, inplace=True)\nnew_tele[\"type\"].fillna(\"medium\", inplace=True)\nencoded_genre = pd.DataFrame(\n    np.array([encode_genre(tokenize(genre), all_genre) for genre in new_tele[\"genre\"]], dtype=np.int64), columns=all_genre, dtype=np.int64)\nencoded_type = pd.DataFrame(\n    np.array(encode_type(new_tele[\"type\"], all_type), dtype=np.int64), columns=[\"type\"])\n\nnew_tele = new_tele.drop(\n    [\"genre\", \"type\", \"name\", \"rating\", \"teleplay_id\"], axis=1)\nnew_tele.loc[new_tele[\"episodes\"] == \"Unknown\", \"episodes\"] = 0.0\nnew_tele = new_tele.astype(\"float\")\nnew_tele = pd.concat([new_tele, encoded_type], axis=1)\nnew_tele = pd.concat([new_tele, encoded_genre], axis=1)\nnew_tele.loc[new_tele[\"episodes\"] > 1000, \"episodes\"] = 1000\nnew_tele.loc[new_tele[\"members\"] > 10000, \"members\"] = 10000","3beb4fb1":"prediction = rf.predict(new_tele)\nnew_tele = pd.read_csv(\"..\/input\/teleplay\/New_Teleplay.csv\", delimiter=\",\")\nnew_tele[\"rating\"] = prediction\nnew_tele.to_csv(\"19079662D_ task1.csv\", index=False)\n","a9f5789c":"# Prediction","6148e6c8":"# \"This is for marking purpose only, please leave this notebook if you are not the intended viewer\"","e9e6e53d":"# preparation","ca55068f":"# Visulization","ce85f7db":"# Baseline"}}