{"cell_type":{"ac9f5324":"code","d257240b":"code","623e0e74":"code","67a1c3d6":"code","49b752b4":"code","cd62eb79":"code","7df15e58":"code","212a387e":"code","3267e0d3":"code","8cb70dff":"code","a72d41bc":"code","cc6817d9":"code","c0c5d9ad":"code","37b06657":"code","bc0dcab3":"code","03a1cce9":"code","58548a6f":"code","af5bf7c5":"code","24b81738":"code","124ac02b":"code","7ede503b":"code","1d536124":"code","1adf5725":"code","e15deea1":"code","850bf1f6":"code","68ecfb2e":"code","18bb3b80":"code","46f807e2":"code","0badd9d8":"code","c318c0d9":"code","250e3e20":"code","41f68c6f":"code","836b3ff8":"code","c72ba418":"code","c0e88fad":"code","6b1ed008":"code","7f99f32f":"code","a08b9ad7":"code","5fde4d5e":"code","ddb2591f":"code","67aeb91b":"code","a33ac44e":"code","56bcd927":"markdown","aeb7d6e3":"markdown","a3817491":"markdown","4848ae72":"markdown","cb9ea65c":"markdown","f585746a":"markdown","368a5ddf":"markdown","aa0c898b":"markdown","d40899d7":"markdown","58d1d30d":"markdown","c9a483bf":"markdown","700c9cae":"markdown","36965af1":"markdown","b20387c1":"markdown","c63bf86c":"markdown","a7120c75":"markdown","d73eb1d4":"markdown","460c4ea7":"markdown","002e4de4":"markdown","858ba59c":"markdown","a151b6a4":"markdown"},"source":{"ac9f5324":"import pandas as pd\nimport numpy as np\nimport os\n\nimport missingno as msno\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image","d257240b":"#COMMENT THIS SECTION INCASE RUNNING THIS NOTEBOOK LOCALLY\n\n#Checking the kaggle paths for the uploaded datasets\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","623e0e74":"#INCASE RUNNING THIS LOCALLY, PASS THE RELATIVE PATH OF THE CSV FILES BELOW\n#(e.g. if files are in same folder as notebook, simple write \"train.csv\" as path)\n\ndata = pd.read_csv(\"..\/input\/ml-lab-II-c28\/train.csv\")\nunseen = pd.read_csv(\"..\/input\/ml-lab-II-c28\/test.csv\")\nsample = pd.read_csv(\"..\/input\/ml-lab-II-c28\/sample.csv\")\ndata_dict = pd.read_csv(\"..\/input\/ml-lab-II-c28\/data_dictionary.csv\")\n\nprint(data.shape)\nprint(unseen.shape)\nprint(sample.shape)\nprint(data_dict.shape)","67a1c3d6":"for i in data_dict.iterrows():\n    print(i[1]['column'],':',i[1]['description'])","49b752b4":"Image(filename=\"..\/input\/ml-lab-II-c28\/features.png\")","cd62eb79":"word_features = ['n_tokens_title', \n                 'n_tokens_content', \n                 'n_unique_tokens', \n                 'n_non_stop_words', \n                 'n_non_stop_unique_tokens', \n                 'average_token_length']\n\nmedia_features = ['num_imgs', 'num_videos']\n\ntemporal_features = ['weekday_is_monday', \n                     'weekday_is_tuesday', \n                     'weekday_is_wednesday',\n                     'weekday_is_thursday',\n                     'weekday_is_friday',\n                     'weekday_is_saturday',\n                     'weekday_is_sunday',\n                     'is_weekend']\n\nchannel_features = ['data_channel_is_lifestyle', 'data_channel_is_entertainment', \n                    'data_channel_is_bus', 'data_channel_is_socmed', \n                    'data_channel_is_tech', 'data_channel_is_world']\n\nkeyword_features = ['kw_min_min', 'kw_max_min', 'kw_avg_min',\n                    'kw_min_max', 'kw_max_max', 'kw_avg_max', \n                    'kw_min_avg', 'kw_max_avg', 'kw_avg_avg',\n                    'num_keywords']\n\n\nreference_features = ['num_hrefs',\n                      'num_self_hrefs',\n                      'self_reference_min_shares',\n                      'self_reference_max_shares',\n                      'self_reference_avg_sharess']\n\ntopic_features = ['LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04']\n\nsubjectivity_features = ['global_subjectivity', 'title_subjectivity', 'abs_title_subjectivity']\n\nsentiment_features = ['global_sentiment_polarity', 'global_rate_positive_words',\n                      'global_rate_negative_words', 'rate_positive_words',\n                      'rate_negative_words', 'avg_positive_polarity',\n                      'min_positive_polarity', 'max_positive_polarity',\n                      'avg_negative_polarity', 'min_negative_polarity',\n                      'max_negative_polarity', 'title_sentiment_polarity',\n                      'abs_title_sentiment_polarity']\n\nids = ['id']\n\nothers = ['url']\n\ntarget = ['shares']","7df15e58":"features = [*ids, *word_features, *media_features, \n            *temporal_features, *channel_features, \n            *keyword_features, *reference_features, \n            *topic_features, *subjectivity_features, \n            *sentiment_features, *others, *target]\n\nlen(features), len(data.columns)","212a387e":"######################################\n#Only for purpose of starter notebook#\n######################################\n\nselected_features = [*ids, *word_features, *channel_features, *temporal_features, *subjectivity_features]\nprint(len(selected_features))\n\n######################################","3267e0d3":"X = data[selected_features].set_index('id')\ny = data[target]","8cb70dff":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n[i.shape for i in [X_train, X_test, y_train, y_test]]","a72d41bc":"X_train.head()","cc6817d9":"X_train.info()","c0c5d9ad":"X_train.describe()","37b06657":"msno.bar(X_train)","bc0dcab3":"msno.matrix(X_train)","03a1cce9":"missing_data_percent = 100*X_train.isnull().sum()\/len(y_train)\nmissing_data_percent","58548a6f":"impute_features = missing_data_percent[missing_data_percent.gt(0)].index\nimpute_features","af5bf7c5":"imp = SimpleImputer(strategy='constant', fill_value=0)\nX_train[impute_features] = imp.fit_transform(X_train[impute_features])","24b81738":"msno.matrix(X_train)","124ac02b":"X_train.describe()","7ede503b":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train)","1d536124":"scale = StandardScaler()\nX_train[:] = scale.fit_transform(X_train)","1adf5725":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train)","e15deea1":"def cap_outliers(array, k=3):\n    upper_limit = array.mean() + k*array.std()\n    lower_limit = array.mean() - k*array.std()\n    array[array<lower_limit] = lower_limit\n    array[array>upper_limit] = upper_limit\n    return array","850bf1f6":"X_train = X_train.apply(cap_outliers, axis=0)","68ecfb2e":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=45)\nsns.boxplot(data = X_train)","18bb3b80":"plt.figure(figsize=(10,8))\nsns.heatmap(pd.DataFrame(X_train, columns=X_train.columns).corr())","46f807e2":"#Distribution for the target variable\nsns.histplot(y_train)","0badd9d8":"sns.histplot(np.log(y_train))","c318c0d9":"rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nrf.fit(X_train, y_train.to_numpy().reshape(-1,))","250e3e20":"feature_importances = pd.DataFrame({'col':X_train.columns, 'importance':rf.feature_importances_})","41f68c6f":"plt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\nplt.bar(feature_importances['col'], feature_importances['importance'])","836b3ff8":"pca = PCA(n_components=14)\ncomponents = pca.fit_transform(X_train)\n\n#Plotting first 2\nsns.scatterplot(x=components[:,0], y=components[:,1])","c72ba418":"rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\nrf.fit(components, y_train.to_numpy().reshape(-1,))\n\nfeature_importances = pd.DataFrame({'col':['component_'+str(i) for i in range(components.shape[1])], 'importance':rf.feature_importances_})\n\nplt.figure(figsize=(15,8))\nplt.xticks(rotation=90)\nplt.bar(feature_importances['col'], feature_importances['importance'])","c0e88fad":"poly = PolynomialFeatures(degree=2)\npoly_features = poly.fit_transform(components)\npoly_features.shape","6b1ed008":"#Instantiate pipeline\nimp = SimpleImputer(strategy='constant', fill_value=0)\nscale = StandardScaler()\npca = PCA(n_components=14)\npoly = PolynomialFeatures(degree=2)\n#model = LinearRegression(n_jobs=-1)\nmodel = RandomForestRegressor(n_estimators=100, max_depth=20, n_jobs=-1)\n\npipe_lr = Pipeline(steps = [('imputation',imp), \n                            ('scaling',scale), \n                            ('pca',pca), \n                            ('poly',poly), \n                            ('model',model)])","7f99f32f":"#Metric report function\n#Code reference - https:\/\/stackoverflow.com\/a\/57239611\/4755954\n\ndef regression_results(y_true, y_pred):\n\n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n\n    print('explained_variance: ', round(explained_variance,4))    \n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n    print('RMSE: ', round(np.sqrt(mse),4))","a08b9ad7":"pipe_lr.fit(X_train, y_train.to_numpy().reshape(-1,))\n\ntest_pred = pipe_lr.predict(X_test)\n\nprint(\"\")\nprint(\"Test metrics\")\nregression_results(y_test.to_numpy().reshape(-1,), test_pred)","5fde4d5e":"sample.head()","ddb2591f":"submission_data = unseen[selected_features].set_index('id')\nsubmission_data.shape","67aeb91b":"unseen['shares'] = pipe_lr.predict(submission_data)\noutput = unseen[['id','shares']]\noutput.head()","a33ac44e":"output.to_csv('submission_pca_poly_rf_sept.csv',index=False)","56bcd927":"## 0. Problem statement\n\nIn this competition, you will build Regression models which accurately predict the popularity of new articles (the number of times it w will be shared online) based on ~60 features provided to you.\n\n**Importance of online news**\n\nThe consumption of online news is expediting day by day due to the extensive adoption of smartphones and the rise of social networks. Note that online news content comprises various key properties. For example, it is easily produced and small in size; its lifespan is short and the cost is low. Such Qualities make news content more effective to be consumed on social sharing platforms. More interestingly, this sort of content can capture the eye of a signi\ufb01cant amount of Internet users within a brief period of your time. As a result, the main target on the analysis of online news content like predicting the recognition of stories articles, demonstrating the decay of interest over time to know the world of online news has greatly increased since it has so many practical meanings.\n\n**Why news popularity prediction?**\n\nThere are multiple areas of applications for online news popularity prediction. Some of these benefits include gaining better insights into the audience consuming online news content. Consequently, it increases the ability of news organizations to deliver more relevant and appealing content in a proactive manner as well as the company can allocate resources more wisely to prepare stories over their life cycle. Moreover, prediction of news content is also bene\ufb01cial for trend forecasting, understanding the collective human behavior, advertisers to propose more pro\ufb01table monetization techniques, and readers to \ufb01lter the huge amount of information quickly and ef\ufb01ciently.","aeb7d6e3":"Lets analyse the features that are given to us. As detailed in the Data section of the competition page, the features provided come from different sources and have been categorized into various types. Some of these are coming directly from other NLP (Natural language processing) models. Let's print and try to categorize the variable names for better analysis.","a3817491":"## 2. Create X, y and then Train test split\n\nLets create X and y datasets and then split them into train and test, before we do any form of transformations on the data or cleaning.\n\n> Important: For the purpose of this **starter notebook**, we I will restrict the dataset to only a small set of variables. However, for building your models and submission files, please ensure to select from all the features given so as to ensure good ranking on the leaderboard.","4848ae72":"### 4.2 Handling outliers\n\nThe box plots of these features show there a lot of outliers. These can be capped with k-sigma method.","cb9ea65c":"# News Popularity Prediction - Starter Notebook\n\n**Author:** Akshay Sehgal (www.akshaysehgal.com)","f585746a":"Finally, lets create a csv file out of this dataset, ensuring to set index=False to avoid an addition column in the csv.","368a5ddf":"## 3. Handling Missing data\n\nFirst lets analyse the missing data. We can use missingno library for quick visualizations.","aa0c898b":"### 6. Model Building\n\nLet's explore a few baseline models on these features. The types of regression we may want to explore further - \n\n**Linear regression:** Linear regression is one of the most basic types of regression in machine learning. The linear regression model consists of a predictor variable and a dependent variable related linearly to each other. In case the data involves more than one independent variable, then linear regression is called multiple linear regression models.\n\n```python\n#Algebra notation\nY = W.X + b\n```\n\n**Ridge regression:** This is type of regression in machine learning which is usually used when there is a high correlation between the independent variables. This is because, in the case of multi collinear data, the least square estimates give unbiased values. But, in case the collinearity is very high, there can be some bias value. Therefore, a bias matrix is introduced in the equation of Ridge Regression. This is a powerful regression method where the model is less susceptible to overfitting. \n\n**Lasso regression:** Lasso Regression is one of the types of regression in machine learning that performs regularization along with feature selection. It prohibits the absolute size of the regression coefficient. As a result, the coefficient value gets nearer to zero, which does not happen in the case of Ridge Regression. Due to this, feature selection gets used in Lasso Regression, which allows selecting a set of features from the dataset to build the model. In the case of Lasso Regression, only the required features are used, and the other ones are made zero. This helps in avoiding the overfitting in the model. In case the independent variables are highly collinear, then Lasso regression picks only one variable and makes other variables to shrink to zero.\n\n**Polynomial regression:** Polynomial Regression is another one of the types of regression analysis techniques in machine learning, which is the same as Multiple Linear Regression with a little modification. In Polynomial Regression, the relationship between independent and dependent variables, that is X and Y, is denoted by the n-th degree.\n\nIt is a linear model as an estimator. Least Mean Squared Method is used in Polynomial Regression also. The best fit line in Polynomial Regression that passes through all the data points is not a straight line, but a curved line, which depends upon the power of X or value of n.\n\nReference for this information is from [this wonderful article](https:\/\/www.upgrad.com\/blog\/types-of-regression-models-in-machine-learning\/).","d40899d7":"### 7. Creating submission file\nFor submission, we need to make sure that the format is exactly the same as the sample.csv file. It contains 2 columns, id and shares","58d1d30d":"### 4.1 Feature scaling\n\nClearly there is a need for scaling the features so that the distributions can be analysed further. We can use StandardScaler for this purpose (few other alternates are min-max scaling and Z-scaling).","c9a483bf":"Since we plan on trying out a variety of regression models, including polynomial regression, lets also generate a bunch of polynomial features. \n\n>NOTE!: Generating polynomial features increases the number of features exponential! So, use the degree parameter carefully.","700c9cae":"You can perform feature transformations at this stage. \n\n1. **Positively skewed:** Common transformations of this data include square root, cube root, and log.\n2. **Negatively skewed:** Common transformations include square, cube root and logarithmic.\n\nPlease read the following link to understand how to perform feature scaling and preprocessing : https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n \nLets also plot the correlations for each feature for bivariate analysis.","36965af1":"## 1. Loading dependencies & datasets\n\nLets start by loading our dependencies. We can keep adding any imports to this cell block, as we write mode and mode code.","b20387c1":"The goal of this notebook is to provide an overview of how write a notebook and create a submission file that successfully solves the News popularity prediction problem statement. Please download the datasets, unzip and place them in the same folder as this notebook.\n\nWe are going to follow the process called CRISP-DM.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/639px-CRISP-DM_Process_Diagram.png\" style=\"height: 400px; width:400px;\"\/>\n\nAfter Business and Data Understanding via EDA, we want to prepare data for modelling. Then evaluate and submit our predictions.","c63bf86c":"Next, we try imputation on variables with any amount of missing data still left. There are multiple ways of imputing data, and each will require a good business understanding of what the missing data is and how you may handle it.\n\nSome tips while working with missing data - \n\n1. Can simply replace missing values directly with a constant value such as 0\n2. In certain cases you may want to replace it with the average value for each column respectively\n3. For timeseries data, you may consider using linear or spline interplolation between a set of points, if you have data available for some of the months, and missing for the others.\n4. You can consider more advance methods for imputation such as MICE.\n\nIn our case, I will just demostrate a simple imputation with constant values as zeros.","a7120c75":"You can perform feature transformations at this stage.\n\nPositively skewed: Common transformations of this data include square root, cube root, and log.\nNegatively skewed: Common transformations include square, cube root and logarithmic.\nPlease read the following link to understand how to perform feature scaling and preprocessing : https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n\nLet's use PCA for dimensionality reduction as well.","d73eb1d4":"### 5. Feature engineering and selection\n\nLet's understand feature importances for raw features as well as components to decide top features for modelling.","460c4ea7":"You can now take this file and upload it as a submission on Kaggle.","002e4de4":"Next, we load our datasets and the data dictionary file.\n\nThe **train.csv** file contains both dependent and independent features, while the **test.csv** contains only the independent variables. \n\nSo, for model selection, I will create our own train\/test dataset from the **train.csv** and use the model to predict the solution using the features in unseen test.csv data for submission.","858ba59c":"## 4. Exploratory Data Analysis & Preprocessing\n\nLets start by analysing the univariate distributions of each feature.","a151b6a4":"There seem to be some missing values in the count as shown by describe. Lets analyze and fix the same."}}