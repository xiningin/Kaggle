{"cell_type":{"21204880":"code","f115c0c4":"code","92d61f8d":"code","3b4c932c":"code","d31f3a87":"code","74f26daf":"code","c608834d":"code","cb137abc":"code","10d4aa6e":"code","ff6d086e":"code","7178b3c5":"code","f0d69475":"code","dad369a7":"code","181dea15":"code","7a07aa5c":"code","c1549b6e":"code","7c99831f":"code","f7a66dc2":"code","d1ff05b9":"code","01a9fb5d":"code","9ff0c175":"code","61980390":"code","ddb01a23":"code","059cf0f5":"code","0a81650b":"code","58947220":"code","c30477af":"code","43cd3672":"code","7c47050a":"code","2349c83d":"code","e91ae84f":"code","02409cbd":"code","f763d462":"code","5cb447f6":"code","966caebe":"code","bf2396e8":"code","e026ff89":"code","e6a2cca4":"code","cbd4ea3b":"code","2ab65b1e":"code","5df10a4c":"code","9006611a":"code","59928fb8":"code","6bfbe23b":"code","6343b2a4":"markdown","495860be":"markdown","aa90c1e6":"markdown","a997e5ee":"markdown","c80742b0":"markdown","97b973a9":"markdown","8e9a723c":"markdown","bc81b0e8":"markdown","2d4b3498":"markdown","e6228b1f":"markdown","7478d092":"markdown","301fcba3":"markdown","cd3123ff":"markdown","4dde24bb":"markdown","bbd9b95e":"markdown","16351c53":"markdown","d175c2f6":"markdown","110b1bf0":"markdown"},"source":{"21204880":"import tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","f115c0c4":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path = os.path.join(dirname, filename)\n\n# df = pd.read_csv('Datasets\/Customer Chrum Prediction\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf = pd.read_csv(path)\ndf.head()","92d61f8d":"df.info()","3b4c932c":"# First thing we notice that is our \"TotalCharges\" Column is of type object and not \"Numeric\" so let's see why's that\ndf['TotalCharges'].values\n# We notice that it's actually Strings so we need to fix that","d31f3a87":"pd.to_numeric(df['TotalCharges'])","74f26daf":"df.TotalCharges.replace(\" \", 0, inplace=True)","c608834d":"df.TotalCharges = pd.to_numeric(df.TotalCharges)\ndf.TotalCharges.unique()","cb137abc":"def print_unique_col_values(df):\n    for col in df:\n        print(f'{col}: {df[col].unique()}')","10d4aa6e":"print_unique_col_values(df)","ff6d086e":"# Now we actually notice that CusomerID is not really helpful so we're just gonna drop that\ndf.drop(\"customerID\", axis='columns', inplace=True)","7178b3c5":"df.columns","f0d69475":"#Now we notice that some columns has \"No Internet Sevice\" which is actually equivilant to \"No\" so we're gonna replace that with no\ndf.replace(\"No internet service\", \"No\", inplace=True)\ndf.replace(\"No phone service\", \"No\", inplace=True)","dad369a7":"print_unique_col_values(df)","181dea15":"CATEGORICAL_COLS = [\"Partner\", \"Dependents\", \"PhoneService\", \"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\", \"StreamingTV\", \"StreamingMovies\", \"PaperlessBilling\", \"Churn\"]\n\nfor category in CATEGORICAL_COLS:\n    df[category].replace({\"Yes\": 1, \"No\": 0}, inplace=True)","7a07aa5c":"df['gender'].replace({\"Female\": 1, \"Male\": 0}, inplace=True)","c1549b6e":"print_unique_col_values(df)","7c99831f":"# Now let's do One-Hot-Encoding for the categorical columns with multiple categories\ndf1 = pd.get_dummies(df, columns=['InternetService','Contract','PaymentMethod'])\ndf1.columns","f7a66dc2":"df1.dtypes","d1ff05b9":"import seaborn as sns\nsns.heatmap(df1.isnull(), yticklabels=False, cbar=True, cmap='viridis', )","01a9fb5d":"cols_to_scale = [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf1[cols_to_scale] = scaler.fit_transform(df1[cols_to_scale])","9ff0c175":"df1.head()","61980390":"X = df1.drop(\"Churn\", axis='columns')\ny = df1['Churn']","ddb01a23":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size=.2, random_state=103)","059cf0f5":"print(f'X Train Shape: {X_train.shape}')\nprint(f'X Test Shape: {X_test.shape}')","0a81650b":"model = keras.models.Sequential([\n    keras.layers.Dense(20, input_shape=(26,), activation='relu'), # this is the first hidden layer\n    keras.layers.Dense(100, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.fit(X_train, y_train, epochs=50)","58947220":"model_test = keras.models.Sequential([\n    keras.layers.Dense(120, input_shape=(26,), activation='relu'), # this is the first hidden layer\n    keras.layers.Dense(1, activation='sigmoid'),\n])\n\nmodel.compile(\n    optimizer='SGD' ,\n    loss=keras.losses.binary_crossentropy,\n    metrics=['accuracy']\n)\n\nmodel.fit(X_train, y_train, epochs=10)","c30477af":"model.evaluate(X_test, y_test)","43cd3672":"y_preds = model.predict(X_test)","7c47050a":"y_preds[0]","2349c83d":"y_values = []\n\nfor i in y_preds:\n    if i < .5:\n        y_values.append(0)\n    else:\n        y_values.append(1)","e91ae84f":"print(\"Predicted Values: \", y_values[:10])\nprint(\"Actual    Values: \", y_test[:10])","02409cbd":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, y_values))","f763d462":"import seaborn as sns\n\nplt.figure(figsize=(9,6))\nsns.heatmap(confusion_matrix(y_test, y_values), annot=True, fmt='d')\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Actual Values\")\nplt.title(\"Confusion MAtrix\")","5cb447f6":"round((921+169)\/(921+169+125+194),2) # Note that this value is already in the classifcation report (3th row)\n# Total of accurate preds over the whole pred data","966caebe":"round(921\/(921+194),2) # Note that this value is already in the classifcation report (1st col)\n# Total of accurate preds over the COLUMN values ","bf2396e8":"round(169\/(169+125), 2) # Note that this value is already in the classifcation report (1st col)\n# Total of accurate preds over the COLUMN values ","e026ff89":"round(921\/(921+125),2) # Note that this value is already in the classifcation report (2nd col)\n# Total of accurate preds over the ROW values ","e6a2cca4":"round(169\/(194+169),2) # Note that this value is already in the classifcation report (2nd col)\n# Total of accurate preds over the ROW values ","cbd4ea3b":"#So We're just gonna make a quick function to do all the training and classification and everything \ndef ANN(X_train, y_train, X_test, y_test, loss, weights):\n    model = keras.Sequential([\n        keras.layers.Dense(26, input_dim=26, activation='relu'),\n        keras.layers.Dense(15, activation='relu'),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n    \n    if weights == -1:\n        model.fit(X_train, y_train, epochs=50)\n    else:\n        model.fit(X_train, y_train, epochs=50, class_weight = weights)\n    \n    print(model.evaluate(X_test, y_test))\n    \n    y_preds = model.predict(X_test)\n    y_preds = np.round(y_preds)\n    \n    print(\"Classification Report: \\n\", classification_report(y_test, y_preds))\n    \n    return y_preds","2ab65b1e":"import imblearn","5df10a4c":"X = df1.drop('Churn', axis='columns')\ny = df1['Churn']\nsmote = imblearn.over_sampling.SMOTE(sampling_strategy=\"minority\")\nX_sampled, y_sampled = smote.fit_resample(X, y)\n\ny_sampled.value_counts()","9006611a":"X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, random_state=15, stratify=y_sampled)\ny_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)","59928fb8":"X = df1.drop('Churn', axis='columns')\ny = df1['Churn']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=15)\n\nensamble = imblearn.under_sampling.EditedNearestNeighbours()\nX_sem, y_sem = ensamble.fit_resample(X_train, y_train)\n\ny_sem.value_counts()","6bfbe23b":"X_train, X_test, y_train, y_test = train_test_split(X_sem, y_sem, test_size=0.2, random_state=15, stratify=y_sem)\ny_preds = ANN(X_train, y_train, X_test, y_test, 'binary_crossentropy', -1)","6343b2a4":"So here I tried to use different amount of layers and different optimizers and loss functions but nothing actually got to the first model accuracy so we're gonna stick to the first model","495860be":"#### Now that we cleared our data there's still a little thing that we can do and that is normalizing our data cause if we noticed that our columns values have a very high range and so we need to normalize these values so our model can work more efficiently","aa90c1e6":"So here We noticed that the accuracy is reahing maximum to 83% after 100 epoches and then it'll take a loooong time to improve just a little so I'm gona try to do some other models to figure out if we can get better accuracy","a997e5ee":"### Under-Sampling","c80742b0":"### Precision for 1 class. i.e. Precision for customers who actually churned","97b973a9":"### Accuracy","8e9a723c":"<i>We Got an error because some values have an empty string so our function doesn't know how to do with that and for that what we can do is either drop these rows or just replace these values with 0<i>","bc81b0e8":"### Recall for 0 class","2d4b3498":"The Next Cell will basically draw if there's any null value in your dataset so you can tak care of that\n","e6228b1f":"## Customers Chrum Predictor\nUsing Dataset from Kaggle https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn","7478d092":"### Over-sampliing (SMOTE)","301fcba3":"### Precision for 0 class. i.e. Precision for customers who did not churn","cd3123ff":"## Regarding Our Classification Report we find that the F1-Score of the 0 Class is okay but for the 1 Class it's not good\n\nThis Condition is called In-Balanced Dataset and we have 3 approaches to fix it\n    1- Under-Sampling <br>\n    2- Over-Sampling (Blind Copy) <br>\n    3- Over-Sampling (SMOTE) <br>\n    4- Ensemble <br>\n    5- Focal loss <br>","4dde24bb":"##### Now it's time for some data cleaning and to basically see what our data is and get familier with it ","bbd9b95e":"So we notice that the number of features is `26` so this is gonna be our `input_shape` so let's start building our neural network","16351c53":"We notice that Under Sampling is actually working good and getting good results sooo we can choose to work with Under Sampling for this data set<br><br>\n<b>But that's not a rule that Under Sampling is always better.. maybe in a different dataset over-sampling will work better for us<b>","d175c2f6":"### Now by that we can say that we can say that our data is good to go and actually start Deep Learning ","110b1bf0":"### Recall for 1 class"}}