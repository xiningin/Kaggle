{"cell_type":{"4f863f89":"code","9e96e786":"code","710b06b5":"code","711992d2":"code","cf76e353":"code","7934edca":"code","bd9c86d4":"code","5da3b1af":"code","0c9e143d":"code","6395816a":"code","18d9080e":"code","52fdf3ad":"code","d37c77d4":"code","fffa9d3e":"code","75c5392e":"code","0b578a08":"code","15bde98d":"code","5103c82a":"code","57bdd610":"code","870c7de7":"code","a4f15171":"code","11f3a353":"code","2dd689ac":"code","08c693fc":"code","8ac68b0f":"code","c3982e9c":"code","5fa73a0d":"code","601709fa":"code","00c3f7eb":"code","bf1a54f3":"code","c14e3416":"code","ebdd2e45":"code","b1f51767":"code","6b7367cc":"code","c2fbb827":"code","bb702956":"code","3717a69f":"code","5f68ca48":"code","dc37897e":"code","a141e291":"code","85a08b25":"code","787a4c26":"code","9397c092":"code","79b0c11e":"code","163e0d88":"code","2b71b7ad":"code","3c4712a5":"code","9ab2c442":"code","96b6f904":"code","7b80c4b3":"code","a37090a2":"code","ffb9e8ee":"code","a45024f4":"code","7025f3f5":"code","e3e0fb78":"code","1b6d348d":"code","dfb325d1":"code","59b3bda7":"code","53450a3e":"code","b49e22ed":"code","e2e96724":"code","d2226187":"code","8a0cdff4":"code","335cb841":"code","ed9b521e":"code","83251831":"code","4dd3c7a9":"code","722f1a06":"code","0052a8d1":"code","c667ffcb":"code","5a7c968e":"code","ec75d846":"code","81584f88":"code","8af8a4b9":"code","2711ca8e":"code","8714acbc":"code","cf8294e3":"code","4c6861b6":"code","3710854f":"code","68d56bb0":"code","adeaae59":"code","ccf54e8a":"code","eeb91b45":"code","0dc53bf1":"code","4dc3db04":"code","9a2d3841":"code","35fc434c":"code","e175d4f0":"code","c9fb63a8":"code","58c8c61d":"code","044a3b45":"code","d7782b6e":"code","afa0f13c":"code","f8492c93":"code","584e1597":"code","1ef97e76":"code","46209fb2":"code","5d149e4f":"code","b044279a":"code","62fc23a3":"code","4689344b":"code","5863658d":"code","da62b97f":"code","e3f07659":"code","08ad8614":"code","3c455e08":"code","a8761659":"code","44646e77":"code","32d81337":"code","918312ba":"code","c5033258":"code","3416669f":"code","a4229099":"code","3308516b":"code","31b0197b":"code","d698ad71":"code","dcd8f770":"code","58e38a4d":"code","e9e9e243":"code","fb77914e":"code","31d347f9":"code","d7bfbd24":"code","fdf70f66":"code","a973a26f":"code","bec694b2":"code","5deb40a8":"code","6fd388b1":"code","7345fc99":"code","7627f44b":"code","235fa3bf":"code","eb309e65":"code","333db372":"code","d1b8f843":"code","7882ecc3":"code","a1bfa576":"code","640b7a5b":"code","7bc3985e":"code","e728d069":"code","6d24bf26":"code","22d1203a":"code","5354c028":"markdown","948fa018":"markdown","56cd0ae7":"markdown","53d060c1":"markdown","5697f484":"markdown","23a2ecef":"markdown","38f16778":"markdown","872e0efd":"markdown","2b5ade38":"markdown","f3aa5691":"markdown","a93cbec0":"markdown","1670d9e7":"markdown","9ecf2fae":"markdown","c0e24a90":"markdown","a6ab9cce":"markdown","f1fe23bb":"markdown","685653e9":"markdown","0348be17":"markdown","59f8aed6":"markdown","ec03329f":"markdown","5d4ef0d2":"markdown","9925b75e":"markdown","ac0595a8":"markdown","0b425c95":"markdown"},"source":{"4f863f89":"from matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nfrom random import random, seed\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.utils import resample\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import  train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9e96e786":"def FrankeFunction(x,y):\n    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n    term2 = 0.75*np.exp(-((9*x+1)**2)\/49.0 - 0.1*(9*y+1))\n    term3 = 0.5*np.exp(-(9*x-7)**2\/4.0 - 0.25*((9*y-3)**2))\n    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n    return term1 + term2 + term3 + term4","710b06b5":"def FrankeFunction_noisy(x,y):\n    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n    term2 = 0.75*np.exp(-((9*x+1)**2)\/49.0 - 0.1*(9*y+1))\n    term3 = 0.5*np.exp(-(9*x-7)**2\/4.0 - 0.25*((9*y-3)**2))\n    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n    noise = np.random.normal(0, 0.1, len(x)*len(x)) \n    noise = noise.reshape(len(x),len(x))\n    return term1 + term2 + term3 + term4 + noise","711992d2":"def xy_data_2(n):\n    x = np.linspace(0,1,n)\n    y = np.linspace(0,1,n)\n    x,y = np.meshgrid(x,y)\n    return x,y","cf76e353":"def xy_data(n):\n    x = np.sort(np.random.uniform(0, 1, n))\n    y = np.sort(np.random.uniform(0, 1, n))\n    x,y = np.meshgrid(x,y)\n    return x,y","7934edca":"x,y = xy_data(40)\nz = FrankeFunction(x,y)","bd9c86d4":"fig = plt.figure(figsize=(20,8))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,z,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\nax.set_zlim(-0.10,1.40)\nax.set_xlabel('X-axis', fontsize=30)\nax.set_ylabel('Y-axis', fontsize=30)\nax.set_zlabel('Z-axis', fontsize=30)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=20)\nax.tick_params(axis='y', labelsize=20)\nax.tick_params(axis='z', labelsize=20)\nfig.savefig(\"Franke_func.png\", dpi=300)\n","5da3b1af":"x,y = xy_data_2(40)\nz_noise = FrankeFunction_noisy(x,y)","0c9e143d":"fig = plt.figure(figsize=(20,8))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,z_noise,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n#ax.set_zlim(-0.10,1.40)\nax.set_xlabel('X-axis', fontsize=30)\nax.set_ylabel('Y-axis', fontsize=30)\nax.set_zlabel('Z-axis', fontsize=30)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=20)\nax.tick_params(axis='y', labelsize=20)\nax.tick_params(axis='z', labelsize=20)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"Franke function\",fontsize=\"20\", color = \"black\")\nfig.savefig(\"Franke_func_noise.png\", dpi=300)\n","6395816a":"# This function makes the design matrix for polynomials up to 10\ndef make_X_matrix(x,y,n):\n    x = x.ravel()\n    y = y.ravel()\n    length = len(x)\n    if n == 1:\n        X =np.stack((np.ones(length), x , y), axis=-1)\n    elif n == 2:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y), axis=-1)\n    elif n == 3:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2)), axis=-1)\n    elif n == 4:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3)), axis=-1)\n    elif n == 5:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3),x**5, y**5,(x**4)*(y**4)), axis=-1)\n    elif n == 6:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3),x**5, y**5,(x**4)*(y**4),x**6, y**6,(x**5)*(y**5)), axis=-1)\n    elif n == 7:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3),x**5, y**5,(x**4)*(y**4),x**6, y**6,(x**5)*(y**5),x**7, y**7,(x**6)*(y**6)), axis=-1)\n    elif n == 8:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3),x**5, y**5,(x**4)*(y**4),x**6, y**6,(x**5)*(y**5),x**7, y**7,(x**6)*(y**6),x**8, y**8,(x**7)*(y**7)), axis=-1)\n    elif n == 9:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3),x**5, y**5,(x**4)*(y**4),x**6, y**6,(x**5)*(y**5),x**7, y**7,(x**6)*(y**6),x**8, y**8,(x**7)*(y**7),x**9, y**9,(x**8)*(y**8)), axis=-1)\n    elif n == 10:\n        X =np.stack((np.ones(length), x , y , x**2 , y**2 , x*y , x**3 , y**3 ,(x**2)*(y**2),x**4, y**4,(x**3)*(y**3),x**5, y**5,(x**4)*(y**4),x**6, y**6,(x**5)*(y**5),x**7, y**7,(x**6)*(y**6),x**8, y**8,(x**7)*(y**7),x**9, y**9,(x**8)*(y**8),x**10, y**10,(x**9)*(y**9)), axis=-1)\n    return X\n\n    ","18d9080e":"# This function calculates beta for OLS\ndef calc_beta(X,y):\n    beta=np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n    return beta","52fdf3ad":"# OLS prediction function\ndef predict(x,y,n,beta):\n    if n == 1:\n        pred =beta[0] + beta[1]* x + beta[2]* y\n    elif n == 2:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y\n    elif n == 3:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)\n    elif n == 4:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)\n    elif n == 5:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)+ beta[12]* x**5 + beta[13]* y**5+ beta[14] * (x**4)*(y**4)\n    elif n == 6:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)+ beta[12]* x**5 + beta[13]* y**5+ beta[14] * (x**4)*(y**4) + beta[15]* x**6 + beta[16]* y**6 + beta[17] * (x**5)*(y**5)\n    elif n == 7:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)+ beta[12]* x**5 + beta[13]* y**5+ beta[14] * (x**4)*(y**4) + beta[15]* x**6 + beta[16]* y**6 + beta[17] * (x**5)*(y**5) + beta[18]* x**7 + beta[19]* y**7 + beta[20] * (x**6)*(y**6)\n    elif n == 8:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)+ beta[12]* x**5 + beta[13]* y**5+ beta[14] * (x**4)*(y**4) + beta[15]* x**6 + beta[16]* y**6 + beta[17] * (x**5)*(y**5) + beta[18]* x**7 + beta[19]* y**7 + beta[20] * (x**6)*(y**6) + beta[21]* x**8 + beta[22]* y**8 + beta[23] * (x**7)*(y**7)\n    elif n == 9:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)+ beta[12]* x**5 + beta[13]* y**5+ beta[14] * (x**4)*(y**4) + beta[15]* x**6 + beta[16]* y**6 + beta[17] * (x**5)*(y**5) + beta[18]* x**7 + beta[19]* y**7 + beta[20] * (x**6)*(y**6) + beta[21]* x**8 + beta[22]* y**8 + beta[23] * (x**7)*(y**7) + beta[24]* x**9 + beta[25]* y**9 + beta[26] * (x**8)*(y**8)\n    elif n == 10:\n        pred =beta[0] + beta[1]* x + beta[2]* y + beta[3]* x**2 + beta[4]* y**2 + beta[5] * x*y + beta[6]* x**3 + beta[7]* y**3 + beta[8] * (x**2)*(y**2)+ beta[9]* x**4 + beta[10]* y**4 + beta[11] * (x**3)*(y**3)+ beta[12]* x**5 + beta[13]* y**5+ beta[14] * (x**4)*(y**4) + beta[15]* x**6 + beta[16]* y**6 + beta[17] * (x**5)*(y**5) + beta[18]* x**7 + beta[19]* y**7 + beta[20] * (x**6)*(y**6) + beta[21]* x**8 + beta[22]* y**8 + beta[23] * (x**7)*(y**7) + beta[24]* x**9 + beta[25]* y**9 + beta[26] * (x**8)*(y**8) + beta[27]* x**10 + beta[28]* y**10 + beta[29] * (x**9)*(y**9)\n    return pred","d37c77d4":"n = 40\nn_bootstraps=50\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\npolynomial = 10\n\n# Stacking x and y and split data into test and train\nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\nx_and_y_train, x_and_y_test, z_train, z_test = train_test_split(x_and_y,z.ravel(),test_size=0.2, random_state=42, shuffle=True)\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(x_and_y)\n\n# Make list and arrays to store results\nmean_test_error_ols=[]\nmean_train_error_ols=[]\nall_test_error_ols=[]\nall_train_error_ols=[]\nall_r2_ols=[]\nmean_r2_ols=[]\nerror = np.zeros(polynomial)\nbias = np.zeros(polynomial)\nvariance = np.zeros(polynomial)\npolydegree = np.zeros(polynomial)\ntrain_error = np.zeros(polynomial)\n\nfor poly in range(polynomial):\n    # Make list and arrays to store results\n    r2_ = []\n    testing_error=[]\n    training_error =[]\n    pred_test = np.empty((z_test.shape[0], n_bootstraps))\n    pred_train = np.empty((int(z_train.shape[0]*0.9), n_bootstraps))\n    for i in range(n_bootstraps):\n        # Bootstrap resampling - leaving out 10% of the data\n        x_and_y_resampled, z_ = resample(x_and_y_train, z_train,n_samples=int(z_train.shape[0]*0.9))\n        \n        # Scale training and test data\n        X_train_scaled = scaler.transform(x_and_y_resampled)\n        X_test_scaled = scaler.transform(x_and_y_test)\n        \n        # Fit training data\n        X = make_X_matrix(X_train_scaled.T[0],X_train_scaled.T[1],poly+1)\n        beta = calc_beta(X, z_)\n        \n        # Do prediction on test and train data\n        pred_train[:,i] = predict(X_train_scaled.T[0],X_train_scaled.T[1],poly+1,beta).ravel()\n        pred_test[:,i]  = predict(X_test_scaled.T[0],X_test_scaled.T[1],poly+1,beta).ravel()\n        z_pred_train = predict(X_train_scaled.T[0],X_train_scaled.T[1],poly+1,beta).ravel()\n        z_pred_test  = predict(X_test_scaled.T[0],X_test_scaled.T[1],poly+1,beta).ravel()\n        \n        testing_error.append(mean_squared_error(z_test, z_pred_test))\n        training_error.append(mean_squared_error(z_, z_pred_train))\n        r2_.append(r2_score(z_test,z_pred_test))\n\n        \n    bias[poly] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(pred_test, axis=1, keepdims=True))**2 )\n    variance[poly] = np.mean( np.var(pred_test, axis=1, keepdims=True) )\n    \n    print('Polynomial degree:', poly+1)\n    print('Error:', testing_error[poly])\n    print('Bias^2:', bias[poly])\n    print('Var:', variance[poly])\n    print('{} >= {} + {} = {}'.format(testing_error[poly], bias[poly], variance[poly], bias[poly]+variance[poly]))\n    \n    #plotting prediction based on all data \n    X_plot_scaled = scaler.transform(x_and_y)\n    z_plot = z.reshape(n,n)\n    z_pred_for_plot = predict(X_plot_scaled.T[0],X_plot_scaled.T[1],poly+1,beta)\n    z_plot = z_pred_for_plot.reshape(n,n)\n    fig = plt.figure(figsize=(32,12))\n    ax = fig.gca(projection ='3d')\n    surf = ax.plot_surface(x,y,z_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n    ax.set_zlim(-0.10,1.40)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    fig.colorbar(surf,shrink=0.5, aspect=5)\n    fig.suptitle(\"A {} degree polynomial fit of Franke function using OLS\".format(poly+1) ,fontsize=\"40\", color = \"black\")\n    fig.savefig(\"Franke_function_OLS_bootstrap{}deg_reg.png\".format(poly+1))\n    fig.show()\n        \n        \n    all_r2_ols.append(r2_)\n    mean_r2_ols.append(np.mean(r2_, axis=0))\n    mean_test_error_ols.append(np.mean(testing_error))\n    mean_train_error_ols.append(np.mean(training_error))\n    all_test_error_ols.append(testing_error)\n    all_train_error_ols.append(training_error)","fffa9d3e":"plt.figure(figsize=(10,8))\nplt.plot(mean_train_error_ols, label=\"MSE train\")\nplt.plot(mean_test_error_ols, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"test_train_mse_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","75c5392e":"plt.figure(figsize=(10,8))\nplt.plot(bias, label=\"Bias\")\nplt.plot(variance, label=\"Variance\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"bias_variance_mse_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","0b578a08":"plt.figure(figsize=(10,8))\nplt.plot(bias+variance, label=\"Bias + Variance\")\nplt.plot(mean_test_error_ols, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"bias_variance_test_error_mse_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","15bde98d":"plt.figure(figsize=(10,8))\nplt.plot(variance, label=\"Variance\",color=\"orange\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"variance_mse_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","5103c82a":"plt.figure(figsize=(10,8))\nplt.plot(bias+variance, label=\"bias+variance\")\nplt.plot(mean_test_error_ols, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"bias_variance_error_mse_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","57bdd610":"print(\"best R2-score:\",np.amax(mean_r2_ols))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}\".format(int(np.where(mean_r2_ols == np.amax(mean_r2_ols))[0])+1))\n\nplt.figure(figsize=(10,8))\nplt.plot(mean_r2_ols, label=\"R2-score\")\nplt.legend()\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\nplt.xlabel(\"polynomial degree\")\nplt.savefig(\"R2_score_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","870c7de7":"plt.figure(figsize=(10,8))\nplt.boxplot(all_r2_ols)\nplt.ylabel(\"R2_score\", fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=20)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.savefig(\"boxplot_r2_score_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","a4f15171":"all_test_error_ols = np.asarray(all_test_error_ols)\nall_train_error_ols = np.asarray(all_train_error_ols)\npos_1=np.ones(all_test_error_ols.shape[0])\npos_2=np.ones(all_test_error_ols.shape[0])\nfor i in range(all_test_error_ols.shape[0]):\n    pos_1[i]=0.5 + i * 2\n    pos_2[i]= i *2\n\nplt.figure(figsize=(10,10))\nbox1 = plt.boxplot(all_test_error_ols.tolist(), positions =pos_1, boxprops=dict(color='red'), medianprops=dict(color='black'))\nbox2 = plt.boxplot(all_train_error_ols.tolist(), positions = pos_2, boxprops=dict(color='blue'),medianprops=dict(color='black'))\nplt.ylabel(\"Mean Squared Error\", fontsize=18)\nplt.xlabel(\"Polynomial degree\", fontsize=20)\nplt.xticks(ticks = [0,2,4,6,8,10,12,14,16,18], labels = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.yticks(fontsize=18)\nplt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], ['MSE test', 'MSE train'], loc='upper right', fontsize=18)\nplt.savefig(\"boxplot_mse_train_test_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","11f3a353":"# Creating data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# number of bootstraps\nn_bootstraps=50\n\n# Polynomial fit\npoly = 17\n\n# Stacking x and y \nX=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\n# Split data into test and train\nX_train, X_test, z_train, z_test = train_test_split(X_scaled,z.ravel(),test_size=0.2, random_state=42)\n\n\n# Make list and arrays to store results\nerror = np.zeros(poly)\nbias = np.zeros(poly)\nvariance = np.zeros(poly)\npolydegree = np.zeros(poly)\ntrain_error = np.zeros(poly)\nmean_test_sk_error_ols=[]\nmean_train_sk_error_ols=[]\nmean_bias_sk_ols = []\nmean_variance_sk_ols = []\nmean_r2_sk_ols = []\nall_test_sk_error_ols=[]\nall_train_sk_error_ols=[]\nall_r2_sk_ols=[]\n\nfor i in range(poly):\n    # Make list and arrays to store results\n    degree=i + 1\n    r2_ = []\n    testing_error =[]\n    training_error =[]\n    pred_test = np.empty((z_test.shape[0], n_bootstraps))\n    pred_train = np.empty((int(z_train.shape[0] * 0.9), n_bootstraps)) \n    for j in range(n_bootstraps):\n        # Bootstrap resampling - leaving out 10% of the data\n        X_, z_ = resample(X_train, z_train,n_samples=int(z_train.shape[0]*0.9))\n        # Fit training data\n        polyreg=make_pipeline(PolynomialFeatures(degree),LinearRegression())\n        polyreg.fit(X_,z_)\n        # Do prediction on test and train data\n        pred_train[:,j] = polyreg.predict(X_)\n        pred_test[:,j]  = polyreg.predict(X_test)\n        z_pred_train =  polyreg.predict(X_)\n        z_pred_test  = polyreg.predict(X_test)\n        \n        # Append results to arrays and lists\n        testing_error.append(mean_squared_error(z_test, z_pred_test))\n        training_error.append(mean_squared_error(z_, z_pred_train))\n        r2_.append(r2_score(z_test,z_pred_test))\n    train_error[i] = np.mean( np.mean((z_.reshape(z_.shape[0],1) - pred_train)**2, axis=1, keepdims=True) )   \n    error[i] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - pred_test)**2, axis=1, keepdims=True) )\n    bias[i] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(pred_test, axis=1, keepdims=True))**2 )\n    variance[i] = np.mean( np.var(pred_test, axis=1, keepdims=True) )\n\n    print('Polynomial degree:', i+1)\n    print('Error:', error[i])\n    print('Bias^2:', bias[i])\n    print('Var:', variance[i])\n    print('{} >= {} + {} = {}'.format(error[i], bias[i], variance[i], bias[i]+variance[i]))\n\n    #plotting prediction based on all data   \n    z_pred=polyreg.predict(X_scaled)\n    z_pred_plot = z_pred.reshape(n,n)\n    fig = plt.figure(figsize=(32,10))\n    ax = fig.gca(projection ='3d')\n    surf = ax.plot_surface(x,y,z_pred_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n    ax.set_zlim(-0.10,1.40)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    fig.colorbar(surf,shrink=0.5, aspect=5)\n    fig.suptitle(\"A {} degree polynomial fit of Franke function using Sklearn\".format(degree) ,fontsize=\"40\", color = \"black\")\n    fig.show()\n    \n    # Append results to arrays and lists\n    mean_r2_sk_ols.append(np.mean(r2_))\n    mean_bias_sk_ols.append(np.mean(bias))\n    mean_variance_sk_ols.append(np.mean(variance))\n    all_r2_sk_ols.append(r2_)\n    mean_test_sk_error_ols.append(np.mean(testing_error))\n    mean_train_sk_error_ols.append(np.mean(training_error))\n    all_test_sk_error_ols.append(testing_error)\n    all_train_sk_error_ols.append(training_error)","2dd689ac":"plt.figure(figsize=(10,8))\nplt.plot(train_error, label=\"MSE train\")\nplt.plot(error, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"mse_train_test_error_SK_17deg.png\", dpi=300)\nplt.show()","08c693fc":"plt.figure(figsize=(10,8))\nplt.plot(bias, label=\"Bias\")\nplt.plot(variance, label=\"Variance\")\nplt.plot(error, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.axvline(x=7,linestyle='--', color='black')\nplt.axvline(x=9,linestyle='--',color='black')\nplt.savefig(\"bias_variance_SK_17deg.png\", dpi=300)\nplt.show()","8ac68b0f":"plt.figure(figsize=(10,8))\nplt.plot(error, label=\"MSE test\")\nplt.plot(bias+variance, label=\"bias+variance\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"bias_variance_and_test_mse_17deg.png\", dpi=300)\nplt.show()","c3982e9c":"print(\"best R2-score:\",np.amax(mean_r2_sk_ols))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}\".format(int(np.where(mean_r2_sk_ols == np.amax(mean_r2_sk_ols))[0])+1))\nplt.figure(figsize=(10,8))\nplt.plot(mean_r2_sk_ols, label=\"R2-score\")\nplt.legend()\nplt.ylabel(\"R2-score\", fontsize = 18)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\"],fontsize=18)\nplt.xlabel(\"polynomial degree\",fontsize=18)\nplt.xticks(fontsize=18)\nplt.axvline(x=7,linestyle='--', color='black')\nplt.axvline(x=9,linestyle='--',color='black')\nplt.savefig(\"R2_score_SK_17deg_ols.png\", dpi=300)\nplt.show()","5fa73a0d":"plt.figure(figsize=(10,8))\nplt.boxplot(all_r2_sk_ols)\nplt.ylim(-1,1)\nplt.ylabel(\"R2_score\", fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=20)\nplt.ylim(0,1)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.legend()\nplt.show()","601709fa":"all_test_sk_error_ols = np.asarray(all_test_sk_error_ols)\nall_train_sk_error_ols = np.asarray(all_train_sk_error_ols)\npos_1=np.ones(all_test_sk_error_ols.shape[0])\npos_2=np.ones(all_test_sk_error_ols.shape[0])\nfor i in range(all_test_sk_error_ols.shape[0]):\n    pos_1[i]=0.5 + i * 2\n    pos_2[i]= i *2\n\nplt.figure(figsize=(30,10))\nbox1 = plt.boxplot(all_test_sk_error_ols.tolist(), positions =pos_1, boxprops=dict(color='red'), medianprops=dict(color='black'))\nbox2 = plt.boxplot(all_train_sk_error_ols.tolist(), positions = pos_2, boxprops=dict(color='blue'),medianprops=dict(color='black'))\n#box3 = plt.boxplot(all_error_sk_ols_cv_sc,  positions = [-0.5, 1.5, 3.5, 5.5, 7.5], boxprops=dict(color='green'),medianprops=dict(color='black'))\nplt.ylim(0,0.1)\nplt.ylabel(\"Mean Squared Error\", fontsize=18)\nplt.xlabel(\"Polynomial degree\", fontsize=20)\nplt.xticks(ticks = [0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32], labels = ['1 order', '2 order', '3 order','4 order', '5 order','6 order', '7 order', '8 order','9 order', '10 order','11 order', '12 order', '13 order','14 order', '15 order','16 order', '17 order'], fontsize=18)\nplt.yticks(fontsize=18)\nplt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], ['MSE test', 'MSE train'], loc='upper right', fontsize=18)\nplt.savefig(\"mse_boxplot_SK_17deg_ols.png\", dpi=300)\nplt.show()","00c3f7eb":"# K fold algorithm inspired by:\n# https:\/\/machinelearningmastery.com\/implement-resampling-methods-scratch-python\/\n\nfrom random import seed\nfrom random import randrange\n \n# Split a dataset into k folds\ndef cross_validation_split(dataset, folds=10):\n    dataset_split = list()\n    dataset_copy = list(dataset)\n    fold_size = int(len(dataset) \/ folds)\n    for i in range(folds):\n        fold = list()\n        while len(fold) < fold_size:\n            index = randrange(len(dataset_copy))\n            fold.append(dataset_copy.pop(index))\n        dataset_split.append(fold)\n    return dataset_split","bf1a54f3":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# number of k-folds\nk_folds = 10\n\n# Polynomial fit\npolynomial = 10\n\n# Stacking x and y \nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nx_and_y_scaled = scaler.transform(x_and_y)\n\n# Make list and arrays to store results\nall_r2_ols_cv=[]\nmean_r2_ols_cv=[]\nerror = np.zeros(polynomial)\nbias = np.zeros(polynomial)\nvariance = np.zeros(polynomial)\npolydegree = np.zeros(polynomial)\ntrain_error = np.zeros(polynomial)\n\n\nfor poly in range(polynomial):\n    # Make list and arrays to store results\n    r2_ = []\n    \n    #Make array to store predictions\n    pred_test = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n    pred_train = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n    \n    # Stacking x , y (X) and z \n    data = np.hstack((x_and_y_scaled,z.ravel().reshape(n**2,1)))\n    \n    #Make folds \n    folds = cross_validation_split(data, k_folds)\n    for i in range(k_folds):\n        #Make train and test data using the i'th fold\n        n_fold = folds.copy()\n        test_data = n_fold.pop(i)\n        test_data= np.asarray(test_data)\n        train_data = np.vstack(n_fold)\n        \n        #split z and X\n        z_train = train_data[:,-1]\n        xy_train = train_data[:,0:-1]\n        z_test = test_data[:,-1]\n        xy_test = test_data[:,0:-1]\n        \n        # Fit training data\n        X_train = make_X_matrix(xy_train.T[0],xy_train.T[1],poly+1)\n        beta = calc_beta(X_train, z_train)\n        \n        # Do prediction on test and train data\n        z_pred_test=predict(xy_test.T[0],xy_test.T[1],poly+1,beta)\n        z_pred_train=predict(xy_train.T[0],xy_train.T[1],poly+1,beta)\n        pred_test[:,i]=predict(xy_test.T[0],xy_test.T[1],poly+1,beta)\n        pred_train[:,i]=predict(xy_train.T[0],xy_train.T[1],poly+1,beta)\n        \n        # Append results to arrays and lists\n        r2_.append(r2_score(z_test,z_pred_test))\n        \n    train_error[poly] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - pred_train)**2, axis=1, keepdims=True) )   \n    error[poly] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - pred_test)**2, axis=1, keepdims=True) )\n    bias[poly] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(pred_test, axis=1, keepdims=True))**2 )\n    variance[poly] = np.mean( np.var(pred_test, axis=1, keepdims=True) )\n\n    print('Polynomial degree:', poly+1)\n    print('Error:', error[poly])\n    print('Bias^2:', bias[poly])\n    print('Var:', variance[poly])\n    print('{} >= {} + {} = {}'.format(error[poly], bias[poly], variance[poly], bias[poly]+variance[poly]))\n        \n    #plotting prediction based on all data     \n    z_pred_for_plot = predict(x_and_y_scaled.T[0],x_and_y_scaled.T[1],poly+1,beta)\n    fig = plt.figure(figsize=(32,12))\n    ax = fig.gca(projection ='3d')\n    surf = ax.plot_surface(x,y,z_pred_for_plot.reshape(n,n),cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n    ax.set_zlim(-0.10,1.40)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    fig.colorbar(surf,shrink=0.5, aspect=5)\n    fig.suptitle(\"A {} degree polynomial fit of Franke function using OLS and K-fold crossval\".format(poly+1) ,fontsize=\"40\", color = \"black\")\n    fig.show()\n        \n    all_r2_ols_cv.append(r2_)\n    mean_r2_ols_cv.append(np.mean(r2_))","c14e3416":"plt.figure(figsize=(10,8))\nplt.plot(train_error, label=\"MSE train\")\nplt.plot(error, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"Kfold_10_mse_test_train.png\", dpi=300)\nplt.show()","ebdd2e45":"plt.figure(figsize=(10,8))\nplt.plot(bias, label=\"Bias\")\nplt.plot(bias+variance, label=\"Bias+Variance\")\nplt.plot(variance, label=\"Variance\")\nplt.plot(error, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.axvline(x=4,linestyle='--', color='black')\n#plt.axvline(x=13,linestyle='--',color='black')\nplt.savefig(\"bias_variance_kfold_10_myOLS_10deg.png\", dpi=300)\nplt.show()","b1f51767":"print(\"best R2-score:\",np.amax(mean_r2_ols_cv))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}\".format(int(np.where(mean_r2_ols_cv == np.amax(mean_r2_ols_cv))[0])+1))\nplt.figure(figsize=(10,8))\nplt.plot(mean_r2_ols_cv, label=\"R2-score\")\nplt.legend()\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.ylabel(\"R2_score\", fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=20)\nplt.savefig(\"R2_score_kfold_10pol_myOLS.png\", dpi=300)\nplt.show()","6b7367cc":"plt.figure(figsize=(10,8))\nplt.boxplot(all_r2_ols_cv)\nplt.ylabel(\"R2_score\", fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=20)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.savefig(\"boxplot_r2_score_kfold_10pol_myOLS.png\", dpi=300)\nplt.show()","c2fbb827":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# number of k-folds\nk_folds = 10\n\n# Polynomial fit\npolynomial =25\n\n# Stacking x and y \nX=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\n# Make list and arrays to store results\nmean_test_error_sk_ols_cv = []\nmean_train_error_sk_ols_cv = []\nmean_bias_sk_ols_cv =[]\nmean_r2_sk_ols_cv = []\nmean_variance_sk_ols_cv =[]\n\nall_test_error_sk_ols_cv = []\nall_train_error_sk_ols_cv = []\nall_bias_sk_ols_cv =[]\nall_r2_sk_ols_cv = []\nall_variance_sk_ols_cv =[]\n\nerror = np.zeros(polynomial)\nbias = np.zeros(polynomial)\nvariance = np.zeros(polynomial)\npolydegree = np.zeros(polynomial)\ntrain_error = np.zeros(polynomial)\n\nfor poly in range(polynomial):\n    # Make list and arrays to store results\n    r2_ = []\n    error_train =[]\n    error_test=[]\n    bias_pol = []\n    variance_pol = []\n    \n    #Make folds using SKlearn\n    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n    kf.get_n_splits(X_scaled)\n    \n    #Make array to store predictions\n    pred_test = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n    pred_train = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n    # Make counter to keep track of k-folds\n    i = 0\n    for train_index, test_index in kf.split(X_scaled):\n        # #Make train and test data using the folds made by SKlearn\n        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n        z_train, z_test = z.ravel()[train_index], z.ravel()[test_index]\n        \n        # Fit training data usink SKlearn\n        polyreg=make_pipeline(PolynomialFeatures(poly+1),LinearRegression())\n        polyreg.fit(X_train,z_train)\n        \n        # Do prediction on test and train data\n        z_pred_test=polyreg.predict(X_test)\n        z_pred_train=polyreg.predict(X_train)\n        pred_test[:,i]=polyreg.predict(X_test)\n        pred_train[:,i]=polyreg.predict(X_train)\n        \n        # Increment counter i\n        i = i + 1\n        \n        # Append results to arrays and lists\n        error_test.append(mean_squared_error(z_test, z_pred_test))\n        error_train.append(mean_squared_error(z_train, z_pred_train))\n        r2_.append(r2_score(z_test,z_pred_test))\n        bias_pol.append(np.mean((z_test - np.mean(z_pred_test))**2))\n        variance_pol.append(np.mean( np.var(z_pred_test)))\n        \n    train_error[poly] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - pred_train)**2, axis=1, keepdims=True) )   \n    error[poly] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - pred_test)**2, axis=1, keepdims=True) )\n    bias[poly] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(pred_test, axis=1, keepdims=True))**2 )\n    variance[poly] = np.mean( np.var(pred_test, axis=1, keepdims=True) )\n\n    print('Polynomial degree:', poly+1)\n    print('Error:', error[poly])\n    print('Bias^2:', bias[poly])\n    print('Var:', variance[poly])\n    print('{} >= {} + {} = {}'.format(error[poly], bias[poly], variance[poly], bias[poly]+variance[poly]))\n    \n    #plotting prediction based on all data  \n    z_pred=polyreg.predict(X_scaled)\n    z_pred_plot = z_pred.reshape(n,n)\n    fig = plt.figure(figsize=(32,10))\n    ax = fig.gca(projection ='3d')\n    surf = ax.plot_surface(x,y,z_pred_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n    #surf2 = ax.plot_surface(x,y,z,cmap=cm.plasma, linewidth = 0, antialiased=False)\n    ax.set_zlim(-0.10,1.40)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    fig.colorbar(surf,shrink=0.5, aspect=5)\n    fig.suptitle(\"A {} degree polynomial fit of Franke function using Sklearn\".format(poly+1) ,fontsize=\"40\", color = \"black\")\n    fig.show()\n    \n    # Append results to arrays and lists\n    mean_test_error_sk_ols_cv.append(np.mean(error_test))\n    mean_train_error_sk_ols_cv.append(np.mean(error_train))\n    mean_r2_sk_ols_cv.append(np.mean(r2_))\n    mean_bias_sk_ols_cv.append(np.mean(bias_pol))\n    mean_variance_sk_ols_cv.append(np.mean(variance_pol))\n    all_test_error_sk_ols_cv.append(error_test)\n    all_train_error_sk_ols_cv.append(error_train)\n    all_r2_sk_ols_cv.append(r2_)\n    all_bias_sk_ols_cv.append(bias_pol)\n    all_variance_sk_ols_cv.append(variance_pol)","bb702956":"plt.figure(figsize=(14,12))\nplt.plot(error, label=\"MSE_test\")\nplt.plot(train_error, label=\"MSE_train\")\nplt.plot(variance, label=\"Variance\")\nplt.plot(bias, label=\"Bias\")\nplt.legend(fontsize = 12)\nplt.axvline(x=6,linestyle='--', color='black')\nplt.axvline(x=15,linestyle='--',color='black')\nplt.ylim(0,1)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error \/ R2-score\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"Bias_variance_analysis_ols_25deg_sklearn_cv.png\", dpi=300)\nplt.show()","3717a69f":"print(\"best R2-score:\",np.amax(mean_r2_sk_ols_cv))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}\".format(int(np.where(mean_r2_sk_ols_cv == np.amax(mean_r2_sk_ols_cv))[0])+1))\nplt.figure(figsize=(14,12))\nplt.plot(mean_r2_sk_ols_cv, label=\"R2\")\nplt.legend(fontsize = 12)\nplt.axvline(x=6,linestyle='--', color='black')\nplt.axvline(x=15,linestyle='--',color='black')\nplt.ylim(0,1)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error \/ R2-score\", fontsize = 18)\nplt.yticks(fontsize=18)\nplt.savefig(\"Bias_variance_analysis_ols_25deg_sklearn_cv.png\", dpi=300)\nplt.show()\n","5f68ca48":"def calc_beta_ridge(X,y, alpha):\n    beta=np.linalg.inv(X.T.dot(X)+alpha * np.identity(X.shape[1])).dot(X.T).dot(y)\n    return beta","dc37897e":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n# Polynomials and lambdas to test\npoly = 10\nLAMBDA = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001 , 0.000001]\n\n# number of k-folds\nk_folds = 10\n\n# Stacking x and y \nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nx_and_y_scaled = scaler.transform(x_and_y)\n\n# Make list and arrays to store results\nmean_error_test=[]\nmean_error_train = []\nmean_bias = []\nmean_variance = []\nmean_r2=[]\nall_error_test=[]\nall_error_train=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\nfor i in range(poly):\n    # Make list and arrays to store results\n    r2_poly = []\n    mean_r2_lambda = []\n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n    for l,j in enumerate(LAMBDA):\n        # Make list and arrays to store results\n        r2_lambda = []\n        \n        # Stacking x , y (X) and z in one matrix\n        data = np.hstack((x_and_y_scaled,z.ravel().reshape(n**2,1)))\n        \n        # Make an array to use for k-fold validation\n        folds = cross_validation_split(data, k_folds)\n        # Make array to store predictions\n        test_pred = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n        train_pred = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n        for k in range(k_folds):\n            #Make train and test data using the k'th fold\n            n_fold = folds.copy()\n            test_data = n_fold.pop(k)\n            test_data= np.asarray(test_data)\n            train_data = np.vstack(n_fold)\n\n            #split z and X\n            z_train = train_data[:,-1]\n            xy_train = train_data[:,0:-1]\n            z_test = test_data[:,-1]\n            xy_test = test_data[:,0:-1]\n            \n            # Fit training data\n            X_train = make_X_matrix(xy_train.T[0],xy_train.T[1],i+1)\n            beta = calc_beta_ridge(X_train,z_train, j)\n            \n            # Do prediction on test and train data\n            z_pred_test=predict(xy_test.T[0],xy_test.T[1],i+1,beta)\n            z_pred_train=predict(X_train.T[1],X_train.T[2],i+1,beta)\n            test_pred[:,k] = predict(xy_test.T[0],xy_test.T[1],i+1,beta)\n            train_pred[:,k] = predict(X_train.T[1],X_train.T[2],i+1,beta)\n            \n            # Append results to arrays and lists\n            r2_lambda.append(r2_score(z_test,z_pred_test))\n\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        \n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n        \n        #plotting prediction based on all data\n        z_pred_for_plot = predict(x_and_y_scaled.T[0],x_and_y_scaled.T[1],i+1,beta)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot.reshape(n,n),cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n        ax.set_zlim(-0.10,1.40)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n        \n        # Append results to arrays and lists\n        r2_poly.append(np.mean(r2_lambda))\n        mean_r2_lambda.append(np.mean(r2_lambda))\n        \n    mean_error_test.append(np.mean(error))\n    mean_error_train.append(np.mean(train_error))\n    mean_r2.append(mean_r2_lambda)\n    mean_bias.append(bias)\n    mean_variance.append(variance)\n    all_error_test.append(error)\n    all_error_train.append(train_error)\n    all_r2.append(r2_poly)\n    all_bias.append(bias)\n    all_variance.append(variance)","a141e291":"all_r2 = np.asarray(all_r2)\nprint(\"best R2-score:\",np.amax(all_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(all_r2 == np.amax(all_r2))[0])+1,LAMBDA[int(np.where(all_r2 == np.amax(all_r2))[1])]))\nr2_x = np.arange(len(LAMBDA))\nr2_y = np.arange(poly) \nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,all_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nfig.savefig(\"ridge_my_alg_and_my_kfold_r2score.png\", dpi=500)\nfig.show()","85a08b25":"all_error_test = np.asarray(all_error_test)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly) \nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_test,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nfig.savefig(\"frankes_ridge_my_alg_and_my_kfold_MSEscore_test.png\", dpi=500)\nfig.show()","787a4c26":"all_error_train = np.asarray(all_error_train)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly) \nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_train,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nfig.savefig(\"frankes_ridge_my_alg_and_my_kfold_MSEscore_test.png\", dpi=500)\nfig.show()","9397c092":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","79b0c11e":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","163e0d88":"all_error_train = np.asarray(all_error_train)\nfor i in range(all_error_train.shape[0]):\n    plt.figure(figsize=(10,8))\n    #plt.plot(mean_r2_ols, label=\"R2-score\")\n    plt.plot(all_error_test[i], label=\"MSE test\")\n    plt.plot(mean_bias[i]+mean_variance[i], label=\"bias+variance\")\n    plt.plot(all_error_train[i], label=\"MSE train\")\n    plt.plot(mean_bias[i], label=\"Bias\")\n    plt.plot(mean_variance[i], label=\"Variance\")\n    plt.legend(fontsize = 12)\n    plt.xticks(ticks=np.arange(len(LAMBDA)),labels=LAMBDA, fontsize=18)\n    plt.xlabel(\"Lambda\", fontsize=18)\n    plt.ylabel(\"Error\", fontsize = 18)\n    plt.yticks(fontsize=18)\n    #plt.savefig(\"test_train_mse_ols_bootstrap.png\", dpi=300)\n    plt.show()","2b71b7ad":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# Polynomials and lambdas to test\npoly = 10\nLAMBDA = [1000, 100,10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n\n# number of bootstraps\nn_bootstraps=50\n\n# Stacking x and y \nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nX_train_scaled = scaler.transform(x_and_y)\n\n# Train\/test split (80%\/20%)\nx_and_y_train, x_and_y_test, z_train, z_test = train_test_split(X_train_scaled,z.ravel(),test_size=0.2,shuffle = True, random_state=42)\n\n# Make list and arrays to store results\nmean_error_test=[]\nmean_error_train=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\nall_error_test=[]\nall_error_train=[]\nall_bias = []\nall_variance = []\nall_r2=[]\nfor i in range(poly):\n    \n    # Make list and arrays to store results\n    error_poly_test = []\n    error_poly_train = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    mean_r2_lambda = []\n    mean_error_lambda_test = []\n    mean_error_lambda_train = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    for l,j in enumerate(LAMBDA):\n        \n        # Make list and arrays to store results\n        error_lambda_train = []\n        error_lambda_test = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        # Make array to store predictions\n        z_pred = np.empty((z_test.shape[0], n_bootstraps))\n        train_pred = np.empty((int(z_train.shape[0] * 0.9), n_bootstraps))\n        \n        for k in range(n_bootstraps):\n            # resample using bootstrap method\n            x_and_y_resampled, z_ = resample(x_and_y_train, z_train,n_samples=int(z_train.shape[0]*0.9))\n            \n            # Fit training data\n            X = make_X_matrix(x_and_y_resampled.T[0],x_and_y_resampled.T[1],i+1)\n            beta = calc_beta_ridge(X, z_, j)\n            \n            # Do prediction on test and train data\n            z_predict = predict(x_and_y_test.T[0],x_and_y_test.T[1],i+1,beta)\n            z_pred[:,k] = predict(x_and_y_test.T[0],x_and_y_test.T[1],i+1,beta)\n            train_pred[:,k] = predict(x_and_y_resampled.T[0],x_and_y_resampled.T[1],i+1,beta)\n            \n            # Append results to arrays and lists\n            r2_lambda.append(r2_score(z_test,z_predict))\n\n        mean_r2_lambda.append(np.mean(r2_lambda))\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - z_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_.reshape(z_.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(z_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(z_pred, axis=1, keepdims=True) )\n\n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n\n\n        \n        z_pred_for_plot = predict(X_train_scaled.T[0],X_train_scaled.T[1],i+1,beta)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot.reshape(n,n),cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n        ax.set_zlim(-0.10,1.40)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n        \n        \n        \n        \n    mean_error_test.append(np.mean(error))\n    mean_error_train.append(np.mean(train_error))\n    mean_r2.append(mean_r2_lambda)\n    mean_bias.append(bias)\n    mean_variance.append(variance)\n    \n    all_error_test.append(error)\n    all_error_train.append(train_error)\n    #all_r2.append(r2_poly)\n    all_bias.append(bias)\n    all_variance.append(variance)","3c4712a5":"all_error_test = np.asarray(all_error_test)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_test,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","9ab2c442":"all_error_train = np.asarray(all_error_train)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_train,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 20\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=10)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE train when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","96b6f904":"mean_bias = np.asarray(mean_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,mean_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","7b80c4b3":"mean_variance = np.asarray(mean_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,mean_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.04f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","a37090a2":"bias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,mean_bias+mean_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias + Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","ffb9e8ee":"mean_r2 = np.asarray(mean_r2)\nprint(\"best R2-score:\",np.amax(mean_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]))\nr2_x = np.arange(len(LAMBDA))\nr2_y = np.arange(poly) \nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nfig.savefig(\"frankes_ridge_my_alg_and_my_bootstrap_r2score.png\", dpi=300)\nfig.show()","a45024f4":"all_error_train = np.asarray(all_error_train)\nfor i in range(all_error_train.shape[0]):\n    plt.figure(figsize=(10,8))\n    plt.plot(all_error_test[i], label=\"MSE test\")\n    plt.plot(all_error_train[i], label=\"MSE train\")\n    plt.plot(mean_variance[i], label=\"Variance\")\n    plt.legend(fontsize = 12)\n    plt.xticks(ticks=np.arange(len(LAMBDA)),labels=LAMBDA, fontsize=18)\n    plt.xlabel(\"Lambda\", fontsize=18)\n    plt.ylabel(\"Error\", fontsize = 18)\n    plt.yticks(fontsize=18)\n    #plt.savefig(\"test_train_mse_ols_bootstrap.png\", dpi=300)\n    plt.show()","7025f3f5":"for i in range(all_error_train.shape[0]):\n    plt.figure(figsize=(10,8))\n    plt.plot(mean_bias[i], label=\"Bias\")\n    plt.plot(mean_variance[i], label=\"Variance\")\n    plt.legend(fontsize = 12)\n    plt.xticks(ticks=np.arange(len(LAMBDA)),labels=LAMBDA, fontsize=18)\n    plt.xlabel(\"Lambda\", fontsize=18)\n    plt.ylabel(\"Error\", fontsize = 18)\n    plt.yticks(fontsize=18)\n    #plt.savefig(\"test_train_mse_ols_bootstrap.png\", dpi=300)\n    plt.show()","e3e0fb78":"from sklearn.linear_model import Ridge","1b6d348d":"def PolynomialRidge(pol,alphafactor, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree=pol),\n                         Ridge(**kwargs, alpha=alphafactor))","dfb325d1":"def fit_ridge_model(pol,X,y,alphafactor):\n    ridgemodel = PolynomialRidge(pol,alphafactor)\n    ridgemodel.fit(X,y)\n    return ridgemodel","59b3bda7":"def ridge_pred(ridgemodel,X_test):\n    y_test = ridgemodel.predict(X_test)\n    return X_test,y_test","53450a3e":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# Polynomials and lambdas to test\npoly = 27\nLAMBDA = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001 , 0.000001]\n\n# number of k-folds\nk_folds = 10\n\n# Make list and arrays to store results\nmean_error=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\nall_test_error=[]\nall_train_error=[]\nall_bias = []\nall_variance = []\nall_r2=[]\nfor i in range(poly):\n    \n    # Make list and arrays to store results\n    error_poly = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    mean_r2_lambda = []\n    mean_error_lambda = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    for l,j in enumerate(LAMBDA):\n        # Make list and arrays to store results\n        error_lambda = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        # Stacking x and y \n        X=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n        \n        # Scaling data\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X_scaled = scaler.transform(X)\n        \n        # Make folds using SKlearn K-fold algorithm\n        kf = KFold(n_splits=10, random_state=42, shuffle=True)\n        kf.get_n_splits(X_scaled)\n\n        # Make array to store predictions\n        test_pred = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n        train_pred = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n        \n        # Make a counter to increment for each fold\n        k=0\n        for train_index, test_index in kf.split(X):\n            \n            # Split into Training and test\/validaion data\n            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n            z_train, z_test = z.ravel()[train_index], z.ravel()[test_index]\n            \n            # Fit training data using SKlearn\n            ridge=fit_ridge_model(i+1,X_train,z_train,j)\n            \n            # Do prediction on test and train data\n            _, z_pred =ridge_pred(ridge, X_test)\n            _, z_pred_train =ridge_pred(ridge, X_train)    \n            _,test_pred[:,k] = ridge_pred(ridge, X_test)\n            _,train_pred[:,k] = ridge_pred(ridge, X_train)\n            \n            # Add results to arrays and lists\n            error_lambda.append(np.mean(np.mean((z_test - z_pred)**2)))\n            bias_lambda.append(np.mean((z_test - np.mean(z_pred))**2))\n            variance_lambda.append(np.mean( np.var(z_pred)))\n            r2_lambda.append(r2_score(z_test,z_pred))\n            \n            # Increment k with 1\n            k= k+1\n        \n        # Add results to arrays and lists\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n        r2_poly.append(np.mean(r2_lambda))\n\n        #plotting prediction based on all data\n        ridge=fit_ridge_model(i+1,X_scaled,z.ravel(),j)\n        _, z_pred_for_plot =ridge_pred(ridge, X)\n        z_pred_for_plot = z_pred_for_plot.reshape(n,n)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n        ax.set_zlim(-0.10,1.40)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n\n    mean_r2.append(mean_r2_lambda)\n    mean_error.append(mean_error_lambda)\n    mean_bias.append(mean_bias_lambda)\n    mean_variance.append(mean_variance_lambda)\n    all_r2.append(r2_poly)\n    all_test_error.append(error)\n    all_train_error.append(train_error)\n    all_bias.append(bias)\n    all_variance.append(variance)","b49e22ed":"all_r2 = np.asarray(all_r2)\nprint(\"best R2-score:\",np.amax(all_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(all_r2 == np.amax(all_r2))[0])+1,LAMBDA[int(np.where(all_r2 == np.amax(all_r2))[1])]))\nr2_x = np.arange(len(LAMBDA)) \nr2_y = np.arange(poly)\nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,all_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"R2-score_ridge_cv_27pol.png\", dpi=300)\nfig.show()","e2e96724":"all_test_error = np.asarray(all_test_error)\nerror_x = np.arange(len(LAMBDA)) \nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_test_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","d2226187":"all_train_error = np.asarray(all_train_error)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_train_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Train error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Train error when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","8a0cdff4":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","335cb841":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","ed9b521e":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# Polynomials and lambdas to test\npoly = 17\nLAMBDA = [1000,100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001 , 0.000001]\n\n# number of bootstraps\nn_bootstraps=50\n\n# Stacking x and y \nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nx_and_y_scaled = scaler.transform(x_and_y)\n\n# Split data into train and test\nx_and_y_train, x_and_y_test, z_train, z_test = train_test_split(x_and_y_scaled,z.ravel(),test_size=0.2)\n\n# Make list and arrays to store results\nmean_error=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\nall_error_test=[]\nall_error_train=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\nfor i in range(poly):\n    # Make list and arrays to store results\n    error_poly = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    mean_r2_lambda = []\n    mean_error_lambda = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n    for l,j in enumerate(LAMBDA):\n        error_lambda = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        # Make array to store predictions\n        test_pred = np.empty((z_test.shape[0], n_bootstraps))\n        train_pred = np.empty((int(z_train.shape[0] * 0.9), n_bootstraps))\n        for k in range(n_bootstraps):\n            # Resample using bootstrap method\n            x_and_y_resampled, z_ = resample(x_and_y_train, z_train,n_samples=int(z_train.shape[0]*0.9))\n            \n            # Fit training data SKlearn\n            ridge=fit_ridge_model(i+1,x_and_y_resampled,z_,j)\n            \n            # Do prediction on test and train data\n            _, z_pred =ridge_pred(ridge, x_and_y_test)\n            _,test_pred[:,k] = ridge_pred(ridge, x_and_y_test)\n            _,train_pred[:,k] = ridge_pred(ridge, x_and_y_resampled)\n            # Add results to arrays and lists\n            r2_lambda.append(r2_score(z_test,z_pred))\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_.reshape(z_.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n        mean_r2_lambda.append(np.mean(r2_lambda))\n\n        #plotting prediction based on all data\n        ridge=fit_ridge_model(i+1,x_and_y_scaled,z.ravel(),j)\n        _, z_pred_for_plot =ridge_pred(ridge, x_and_y_scaled)\n        z_pred_for_plot = z_pred_for_plot.reshape(n,n)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n        ax.set_zlim(-0.10,1.40)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n\n    # Add results to arrays and lists\n    mean_error_test.append(np.mean(error))\n    mean_error_train.append(np.mean(train_error))\n    mean_r2.append(mean_r2_lambda)\n    mean_bias.append(bias)\n    mean_variance.append(variance)\n    \n    all_error_test.append(error)\n    all_error_train.append(train_error)\n    all_bias.append(bias)\n    all_variance.append(variance)","83251831":"mean_r2 = np.asarray(mean_r2)\nprint(\"best R2-score:\",np.amax(mean_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]))\nr2_x = np.arange(len(LAMBDA)) \nr2_y = np.arange(poly)\nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"R2-score_ridge_bootstrap_17pol.png\", dpi=300)\nfig.show()","4dd3c7a9":"all_error_test = np.asarray(all_error_test)\nerror_x = np.arange(len(LAMBDA)) \nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_test,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"Error_test_ridge_bootstrap_17pol.png\", dpi=300)\nfig.show()","722f1a06":"all_error_train = np.asarray(all_error_train)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_train,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Train error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Train error when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","0052a8d1":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","c667ffcb":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","5a7c968e":"from sklearn.linear_model import Lasso\n\ndef PolynomialLasso(pol,alphafactor, **kwargs):\n    return make_pipeline(PolynomialFeatures(degree=pol),\n                         Lasso(**kwargs, alpha=alphafactor))\n\ndef fit_lasso_model(pol,X,y,alphafactor):\n    lassomodel = PolynomialLasso(pol,alphafactor)\n    lassomodel.fit(X,y)\n    return lassomodel\n\ndef lasso_pred(lassomodel, X_test):\n    y_test = lassomodel.predict(X_test)\n    return X_test,y_test","ec75d846":"# Make data set\nn= 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# Polynomials and lambdas to test\npoly = 25\nLAMBDA = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001 , 0.000001]\n\n# number of k-folds\nk_folds = 10\n\n# Make list and arrays to store results\nmean_error=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\nall_test_error=[]\nall_train_error=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\nfor i in range(poly):\n    # Make list and arrays to store results\n    error_poly = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    mean_r2_lambda = []\n    mean_error_lambda = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n    for l,j in enumerate(LAMBDA):\n        # Make list and arrays to store results\n        error_lambda = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        # Stacking x and y into one matrix\n        X=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n        \n        # Scaling data\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X_scaled = scaler.transform(X)\n        \n        # Fit SKlearns K-fold algorithm \n        kf = KFold(n_splits=10, random_state=42, shuffle=True)\n        kf.get_n_splits(X_scaled)\n\n        # Make array to store predictions\n        test_pred = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n        train_pred = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n        \n        # MAke a counter to increment for every k-fold\n        k=0\n        for train_index, test_index in kf.split(X):\n            #Make train and test data using the k'th fold\n            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n            z_train, z_test = z.ravel()[train_index], z.ravel()[test_index]\n            \n            # Fit training data using SKlearns Lasso\n            lasso=fit_lasso_model(i+1,X_train,z_train,j)\n            \n            # Do prediction on test and train data\n            _, z_pred =lasso_pred(lasso, X_test)\n            _, z_pred_train =lasso_pred(lasso, X_train)\n            _,test_pred[:,k] = lasso_pred(lasso, X_test)\n            _,train_pred[:,k] = lasso_pred(lasso, X_train)\n            \n            # Add results to arrays and lists\n            error_lambda.append(np.mean(np.mean((z_test - z_pred)**2)))\n            bias_lambda.append(np.mean((z_test - np.mean(z_pred))**2))\n            variance_lambda.append(np.mean( np.var(z_pred)))\n            r2_lambda.append(r2_score(z_test,z_pred))\n            # Increment counter by 1\n            k= k+1\n        # Append results to arrays and lists\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n\n\n        #plotting prediction based on all data\n        lasso=fit_lasso_model(i+1,X,z.ravel(),j)\n        _, z_pred_for_plot =lasso_pred(lasso, X)\n        z_pred_for_plot = z_pred_for_plot.reshape(n,n)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n        ax.set_zlim(-0.10,1.40)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Lasso with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n        \n        # Add results to arrays and lists\n        error_poly.append(error_lambda)\n        bias_poly.append(bias_lambda)\n        variance_poly.append(variance_lambda)\n        r2_poly.append(r2_lambda)\n        mean_r2_lambda.append(np.mean(r2_lambda))\n        mean_error_lambda.append(np.mean(error_lambda))\n        mean_bias_lambda.append(np.mean(bias_lambda))\n        mean_variance_lambda.append(np.mean(variance_lambda))\n\n    mean_r2.append(mean_r2_lambda)\n    mean_error.append(mean_error_lambda)\n    mean_bias.append(mean_bias_lambda)\n    mean_variance.append(mean_variance_lambda)\n    all_r2.append(r2_poly)\n    all_test_error.append(error)\n    all_train_error.append(train_error)\n    all_bias.append(bias)\n    all_variance.append(variance)","81584f88":"mean_r2 = np.asarray(mean_r2)\nprint(\"best R2-score:\",np.amax(mean_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]))\nr2_x = np.arange(len(LAMBDA)) \nr2_y = np.arange(poly)\nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"R2-score_lasso_kfold_25poly.png\", dpi=300)\nfig.show()","8af8a4b9":"all_test_error = np.asarray(all_test_error)\nerror_x = np.arange(len(LAMBDA)) \nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_test_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"MSE_test_lasso_kfold_25pol.png\", dpi=300)\nfig.show()","2711ca8e":"all_train_error = np.asarray(all_train_error)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_train_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Train error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Train error when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","8714acbc":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.tick_params(axis='y', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"bias_test_lasso_kfold_25pol.png\", dpi=300)\nfig.show()","cf8294e3":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"variance_test_lasso_kfold_25pol.png\", dpi=300)\nfig.show()","4c6861b6":"# Make data set\nn = 40\nx,y = xy_data_2(n)\nz = FrankeFunction_noisy(x,y)\n\n# Polynomials and lambdas to test\npoly = 25\nLAMBDA = [1000,100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001 , 0.000001]\n\n# number of bootstraps\nn_bootstraps=50\n\n# Stacking x and y\nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\n# Scaling data\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nx_and_y_scaled = scaler.transform(x_and_y)\n\n# Spliting data into test and train (20\/80)\nx_and_y_train, x_and_y_test, z_train, z_test = train_test_split(x_and_y_scaled,z.ravel(),test_size=0.2)\n\n# Make list and arrays to store results\nmean_error_test=[]\nmean_error_train=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\nall_error_test=[]\nall_error_train=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\nfor i in range(poly):\n    # Make list and arrays to store results\n    error_poly = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    mean_r2_lambda = []\n    mean_error_lambda = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n    for l,j in enumerate(LAMBDA):\n        # Make list and arrays to store results\n        error_lambda = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        # Make array to store predictions\n        test_pred = np.empty((z_test.shape[0], n_bootstraps))\n        train_pred = np.empty((int(z_train.shape[0] * 0.9), n_bootstraps))\n        for k in range(n_bootstraps):\n            # Resamble using bootstrap method. Leave out 10% of the training data\n            x_and_y_resampled, z_ = resample(x_and_y_train, z_train,n_samples=int(z_train.shape[0]*0.9))\n            \n            # Fit training data\n            lasso=fit_lasso_model(i+1,x_and_y_resampled,z_,j)\n            \n            # Do prediction on test and train data\n            _, z_pred =lasso_pred(lasso, x_and_y_test)\n            _,test_pred[:,k] = lasso_pred(lasso, x_and_y_test)\n            _,train_pred[:,k] = lasso_pred(lasso, x_and_y_resampled)\n            \n            # Add results to arrays and lists\n            r2_lambda.append(r2_score(z_test,z_pred))\n        \n        # Add results to arrays and lists\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_.reshape(z_.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n        mean_r2_lambda.append(np.mean(r2_lambda))\n\n        #plotting prediction based on all data\n        lasso=fit_lasso_model(i+1,x_and_y_scaled,z.ravel(),j)\n        _, z_pred_for_plot =lasso_pred(lasso, x_and_y_scaled)\n        z_pred_for_plot = z_pred_for_plot.reshape(n,n)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.coolwarm, linewidth = 0, antialiased=False)\n        ax.set_zlim(-0.10,1.40)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n\n    # Append results to arrays and lists\n    mean_error_test.append(np.mean(error))\n    mean_error_train.append(np.mean(train_error))\n    mean_r2.append(mean_r2_lambda)\n    mean_bias.append(bias)\n    mean_variance.append(variance)\n    all_error_test.append(error)\n    all_error_train.append(train_error)\n    #all_r2.append(r2_poly)\n    all_bias.append(bias)\n    all_variance.append(variance)","3710854f":"mean_r2 = np.asarray(mean_r2)\nprint(\"best R2-score:\",np.amax(mean_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]))\nr2_x = np.arange(len(LAMBDA)) \nr2_y = np.arange(poly)\nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nplt.savefig(\"R2-score_lasso_bootstrap_25poly.png\", dpi=300)\nfig.show()","68d56bb0":"all_error_test = np.asarray(all_error_test)\nerror_x = np.arange(len(LAMBDA)) \nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_test,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","adeaae59":"all_error_train = np.asarray(all_error_train)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_train,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Train error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Train error when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","ccf54e8a":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","eeb91b45":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","0dc53bf1":"!pip install neurokit2","4dc3db04":"!pip install ecg_plot","9a2d3841":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer","35fc434c":"import neurokit2 as nk\nfrom scipy.io import loadmat\nimport wfdb\n\n# This function loads ECG recordings and meta data given a specific recording number\ndef load_challenge_data(filename):\n    x = loadmat(filename)\n    data = np.asarray(x['val'], dtype=np.float64)\n    new_file = filename.replace('.mat','.hea')\n    input_header_file = os.path.join(new_file)\n    with open(input_header_file,'r') as f:\n        header_data=f.readlines()\n    return data, header_data","e175d4f0":"import timeit","c9fb63a8":"# This piece of code searches through all data, in 1 out of the 2 ECG databases, after patients with LVH diagnose. \n#Then the second heartbeat in every record is added to a new list and interpolated to a 12 x 500 matrix \n\nfrom scipy import interpolate\nlvh_data = []\nstarttime = timeit.default_timer()\nprint(\"The start time is :\",starttime)\nfor i in sorted(os.listdir(\"\/kaggle\/input\/china-12lead-ecg-challenge-database\/Training_2\/\")):\n    if i.endswith(\".mat\"):\n        data, header_data = load_challenge_data(\"\/kaggle\/input\/china-12lead-ecg-challenge-database\/Training_2\/\"+i)\n        diagnose = header_data[15][5:-1]\n        diagnose = diagnose.split(\",\")\n        diagnose = np.asarray(diagnose)\n        if pd.Series('164873001').isin(diagnose).any():\n            _, rpeaks = nk.ecg_peaks(data[1], sampling_rate=int(header_data[0].split(\" \")[2]))\n            split_num = int(((np.diff(rpeaks['ECG_R_Peaks'])[0]+np.diff(rpeaks['ECG_R_Peaks'])[1])\/2)\/2)\n            data = data\/int(header_data[1].split(\" \")[2].split(\"\/\")[0])\n            ecg3d_lvh=[]\n            for i in range(data.shape[0]):\n                ecg3d_lvh.append(data[i][rpeaks['ECG_R_Peaks'][1]-split_num:rpeaks['ECG_R_Peaks'][1]+split_num])\n            ecg3d_lvh = np.asarray(ecg3d_lvh)\n            x=np.arange(ecg3d_lvh.shape[1])\n            y=np.arange(ecg3d_lvh.shape[0])\n            #x,y = np.meshgrid(x, y)\n            f = interpolate.interp2d(x, y, ecg3d_lvh, kind='cubic')\n            xnew = np.linspace(0,len(x),500)\n            ynew = np.arange(len(y))\n            #xnew,ynew = np.meshgrid(xnew, ynew)\n            znew = f(xnew, ynew)\n            lvh_data.append(znew)\n            print(\"Time after adding first patient data is :\", timeit.default_timer() - starttime)\n            print(len(lvh_data))\n        else:\n            pass\nlvh_data = np.asarray(lvh_data)","58c8c61d":"# This piece of code searches through all data, in 1 out of the 2 ECG databases, after patients with normal sinus rythm. \n#Then the second heartbeat in every record is added to a new list and interpolated to a 12 x 500 matrix \n\nnorm_data = []\nstarttime = timeit.default_timer()\nfor i in sorted(os.listdir(\"\/kaggle\/input\/china-physiological-signal-challenge-in-2018\/Training_WFDB\/\")):\n    if i.endswith(\".mat\"):\n        data, header_data = load_challenge_data(\"\/kaggle\/input\/china-physiological-signal-challenge-in-2018\/Training_WFDB\/\"+i)\n        diagnose = header_data[15][5:-1]\n        diagnose = diagnose.split(\",\")\n        diagnose = np.asarray(diagnose)\n        if pd.Series('426783006').isin(diagnose).any():\n            _, rpeaks = nk.ecg_peaks(data[1], sampling_rate=int(header_data[0].split(\" \")[2]))\n            split_num = int(((np.diff(rpeaks['ECG_R_Peaks'])[0]+np.diff(rpeaks['ECG_R_Peaks'])[1])\/2)\/2)\n            data = data\/int(header_data[1].split(\" \")[2].split(\"\/\")[0])\n            ecg3d_norm=[]\n            for i in range(data.shape[0]):\n                ecg3d_norm.append(data[i][rpeaks['ECG_R_Peaks'][1]-split_num:rpeaks['ECG_R_Peaks'][1]+split_num])\n            ecg3d_norm = np.asarray(ecg3d_norm)\n            x=np.arange(ecg3d_norm.shape[1])\n            y=np.arange(ecg3d_norm.shape[0])\n            #x,y = np.meshgrid(x, y)\n            f = interpolate.interp2d(x, y, ecg3d_norm, kind='cubic')\n            xnew = np.linspace(0,len(x),500)\n            ynew = np.arange(len(y))\n            #xnew,ynew = np.meshgrid(xnew, ynew)\n            znew = f(xnew, ynew)\n            norm_data.append(znew)\n            print(\"Time after adding first patient data is :\", timeit.default_timer() - starttime)\n            print(len(norm_data))\n        else:\n            pass\nnorm_data = np.asarray(norm_data)","044a3b45":"plt.figure(figsize=(10,8))\nfor i in range(len(norm_data[0])):\n    plt.plot(norm_data[1][i])\n\nplt.ylabel(\"mV\", fontsize=18)\nplt.xlabel(\"Samples\",fontsize=18)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.savefig(\"ecg12lead_onedim.png\", dpi=300)\nplt.show()","d7782b6e":"x=np.arange(500)\ny=np.arange(12)\nx,y = np.meshgrid(x, y)","afa0f13c":"fig = plt.figure(figsize=(20,8))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,lvh_data[0],cmap=cm.bone, linewidth = 0, antialiased=False)\nax.set_xlabel('Samples', fontsize=20)\nax.set_ylabel('ECG leads', fontsize=20)\nax.set_zlabel('mV', fontsize=20)\nax.xaxis.labelpad = 20\n#ax.yaxis.set_ticks(['I','II','III','aVL','aVR','aVF','V1','V2','V3','V4','V5','V6'])\nax.set_yticklabels(['I','II','III','aVL','aVR','aVF','V1','V2','V3','V4','V5','V6'], fontsize=16)\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(length=12)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.labelpad = 12\n#ax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\n#ax.set_zlim(-0.10,1.40)\nax.view_init(20, 75)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"ECG plot\",fontsize=\"40\", color = \"black\")\nplt.savefig(\"ecg12lead_threedim.png\", dpi=300)","f8492c93":"# Plot the avverage of the first 100 normal sinus rythm heartbeats\nfig = plt.figure(figsize=(20,8))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,np.mean(norm_data[:100],axis=0),cmap=cm.bone, linewidth = 0, antialiased=False)\nax.set_xlabel('Samples', fontsize=20)\nax.set_ylabel('ECG leads', fontsize=20)\nax.set_zlabel('mV', fontsize=20)\nax.xaxis.labelpad = 20\n#ax.yaxis.set_ticks(['I','II','III','aVL','aVR','aVF','V1','V2','V3','V4','V5','V6'])\nax.set_yticklabels(['I','II','III','aVL','aVR','aVF','V1','V2','V3','V4','V5','V6'], fontsize=16)\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(length=12)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.labelpad = 12\n#ax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\n#ax.set_zlim(-0.10,1.40)\nax.view_init(20, 75)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n#fig.colorbar(surf,shrink=0.5, aspect=5)\n#fig.suptitle(\"ECG plot\",fontsize=\"40\", color = \"black\")\nplt.savefig(\"ecg12lead_mean_of_100.png\", dpi=300)","584e1597":"\nz = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\nk_folds = 10\npolynomial = 10\n\nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nx_and_y_scaled = scaler.transform(x_and_y)\n\n\nall_r2_ols_cv=[]\nmean_r2_ols_cv=[]\nerror = np.zeros(polynomial)\nbias = np.zeros(polynomial)\nvariance = np.zeros(polynomial)\npolydegree = np.zeros(polynomial)\ntrain_error = np.zeros(polynomial)\n\n\n\n\n\nfor poly in range(polynomial):\n\n    r2_ = []\n\n\n    pred_test = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n    pred_train = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds)) \n    data = np.hstack((x_and_y_scaled,z.ravel().reshape(x.shape[1]*x.shape[0],1)))\n    # Make an array to use for k-fold validation\n    folds = cross_validation_split(data, k_folds)\n    for i in range(k_folds):\n        n_fold = folds.copy()\n        test_data = n_fold.pop(i)\n        test_data= np.asarray(test_data)\n        \n        train_data = np.vstack(n_fold)\n        \n        z_train = train_data[:,-1]\n        xy_train = train_data[:,0:-1]\n        \n        z_test = test_data[:,-1]\n        xy_test = test_data[:,0:-1]\n        \n\n        \n        X_train = make_X_matrix(xy_train.T[0],xy_train.T[1],poly+1)\n        beta = calc_beta(X_train, z_train)\n        \n        z_pred_test=predict(xy_test.T[0],xy_test.T[1],poly+1,beta)\n        z_pred_train=predict(xy_train.T[0],xy_train.T[1],poly+1,beta)\n        pred_test[:,i]=predict(xy_test.T[0],xy_test.T[1],poly+1,beta)\n        pred_train[:,i]=predict(xy_train.T[0],xy_train.T[1],poly+1,beta)\n\n\n        \n        r2_.append(r2_score(z_test,z_pred_test))\n        \n    train_error[poly] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - pred_train)**2, axis=1, keepdims=True) )   \n    error[poly] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - pred_test)**2, axis=1, keepdims=True) )\n    bias[poly] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(pred_test, axis=1, keepdims=True))**2 )\n    variance[poly] = np.mean( np.var(pred_test, axis=1, keepdims=True) )\n    \n    print('Polynomial degree:', poly+1)\n    print('Error:', error[poly])\n    print('Bias^2:', bias[poly])\n    print('Var:', variance[poly])\n    print('{} >= {} + {} = {}'.format(error[poly], bias[poly], variance[poly], bias[poly]+variance[poly]))\n    print(\"R2-scores:\",r2_)\n        \n    #X_plot_scaled = scaler.transform(X)    \n    z_pred_for_plot = predict(x_and_y_scaled.T[0],x_and_y_scaled.T[1],poly+1,beta)\n    fig = plt.figure(figsize=(32,12))\n    ax = fig.gca(projection ='3d')\n    surf = ax.plot_surface(x,y,z_pred_for_plot.reshape(x.shape[0],x.shape[1]),cmap=cm.bone, linewidth = 0, antialiased=False)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    fig.colorbar(surf,shrink=0.5, aspect=5)\n    ax.view_init(20, 75)\n    fig.suptitle(\"A {} degree polynomial fit of Franke function using OLS and K-fold crossval\".format(poly+1) ,fontsize=\"40\", color = \"black\")\n    #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n    fig.show()\n        \n        \n    all_r2_ols_cv.append(r2_)\n    mean_r2_ols_cv.append(np.mean(r2_))","1ef97e76":"plt.figure(figsize=(10,8))\n#plt.plot(mean_r2_ols, label=\"R2-score\")\nplt.plot(train_error, label=\"MSE train\")\nplt.plot(error, label=\"MSE test\")\n#plt.plot(bias, label=\"Bias\")\n#plt.plot(variance, label=\"Variance\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.savefig(\"Kfold_mse_test_train.png\", dpi=300)\nplt.show()","46209fb2":"plt.figure(figsize=(10,8))\n#plt.plot(mean_r2_ols, label=\"R2-score\")\n#plt.plot(train_error, label=\"MSE train\")\n#plt.plot(bias+variance, label=\"bias+variance\")\n#plt.plot(error, label=\"MSE test\")\nplt.plot(bias, label=\"Bias\")\nplt.plot(bias+variance, label=\"bias+variance\")\nplt.plot(variance, label=\"Variance\")\nplt.plot(error, label=\"MSE test\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.axvline(x=4,linestyle='--', color='black')\n#plt.axvline(x=13,linestyle='--',color='black')\n#plt.savefig(\"bias_variance_SK_17deg.png\", dpi=300)\nplt.show()","5d149e4f":"plt.figure(figsize=(10,8))\nplt.plot(mean_r2_ols_cv, label=\"R2-score\")\nplt.legend()\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"])\nplt.xlabel(\"polynomial degree\")\nplt.savefig(\"R2_score_ols_bootstrap.png\", dpi=300)\nplt.show()","b044279a":"plt.figure(figsize=(10,8))\nplt.boxplot(all_r2_ols_cv)\nplt.ylabel(\"R2_score\", fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=20)\nplt.xticks(fontsize=18)\nplt.yticks(fontsize=18)\nplt.savefig(\"boxplot_r2_score_ols_bootstrap.png\", dpi=300)\nplt.show()","62fc23a3":"\npoly = 10\nLAMBDA = [1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001 , 0.000001]\n\nz = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\nk_folds = 10\n\nx_and_y=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\nscaler = StandardScaler()\nscaler.fit(x_and_y)\nx_and_y_scaled = scaler.transform(x_and_y)\n\n\nmean_error_test=[]\nmean_error_train = []\nmean_bias = []\nmean_variance = []\nmean_r2=[]\n\nall_error_test=[]\nall_error_train=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\nfor i in range(poly):\n\n    r2_poly = []\n    mean_r2_lambda = []\n\n    \n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n\n    for l,j in enumerate(LAMBDA):\n        r2_lambda = []\n        \n\n        \n\n        data = np.hstack((x_and_y_scaled,z.ravel().reshape(z.shape[0]*z.shape[1],1)))\n        # Make an array to use for k-fold validation\n        folds = cross_validation_split(data, k_folds)\n        test_pred = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n        train_pred = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n        for k in range(k_folds):\n            n_fold = folds.copy()\n            test_data = n_fold.pop(k)\n            test_data= np.asarray(test_data)\n\n            train_data = np.vstack(n_fold)\n\n            z_train = train_data[:,-1]\n            xy_train = train_data[:,0:-1]\n\n            z_test = test_data[:,-1]\n            xy_test = test_data[:,0:-1]\n            X_train = make_X_matrix(xy_train.T[0],xy_train.T[1],i+1)\n            #X_test = make_X_matrix(xy_test.T[0],xy_test.T[1],i+1)\n\n            beta = calc_beta_ridge(X_train,z_train, j)\n            z_pred_test=predict(xy_test.T[0],xy_test.T[1],i+1,beta)\n            z_pred_train=predict(X_train.T[1],X_train.T[2],i+1,beta)\n            test_pred[:,k] = predict(xy_test.T[0],xy_test.T[1],i+1,beta)\n            train_pred[:,k] = predict(X_train.T[1],X_train.T[2],i+1,beta)\n            r2_lambda.append(r2_score(z_test,z_pred_test))\n\n\n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        \n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n        \n        \n        z_pred_for_plot = predict(x_and_y_scaled.T[0],x_and_y_scaled.T[1],i+1,beta)\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot.reshape(z.shape[0],z.shape[1]),cmap=cm.bone, linewidth = 0, antialiased=False)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.view_init(20, 75)\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n        \n        \n\n        r2_poly.append(np.mean(r2_lambda))\n        mean_r2_lambda.append(np.mean(r2_lambda))\n\n        \n        \n\n    mean_error_test.append(np.mean(error))\n    mean_error_train.append(np.mean(train_error))\n    mean_r2.append(mean_r2_lambda)\n    mean_bias.append(bias)\n    mean_variance.append(variance)\n    \n    all_error_test.append(error)\n    all_error_train.append(train_error)\n    all_r2.append(r2_poly)\n    all_bias.append(bias)\n    all_variance.append(variance)","4689344b":"all_error_test = np.asarray(all_error_test)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_test,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","5863658d":"all_error_train = np.asarray(all_error_train)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_error_train,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 20\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=10)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE train when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","da62b97f":"mean_bias = np.asarray(mean_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,mean_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","e3f07659":"mean_variance = np.asarray(mean_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,mean_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.04f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","08ad8614":"bias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,mean_bias+mean_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias + Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","3c455e08":"mean_r2 = np.asarray(mean_r2)\nr2_x = np.arange(len(LAMBDA))\nr2_y = np.arange(poly) \nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='y', labelsize=16)\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\nfig.savefig(\"frankes_ridge_my_alg_and_my_kfold_r2score.png\", dpi=500)\nfig.show()","a8761659":"for i in range(all_error_test.shape[0]):\n    plt.figure(figsize=(10,8))\n    #plt.plot(mean_r2_ols, label=\"R2-score\")\n    plt.plot(all_error_test[i], label=\"MSE test\")\n    #plt.plot(mean_bias[4]+mean_variance[4], label=\"bias+variance\")\n    plt.plot(all_error_train[i], label=\"MSE train\")\n    #plt.plot(mean_bias[i], label=\"Bias\")\n    plt.plot(mean_variance[i], label=\"Variance\")\n    plt.legend(fontsize = 12)\n    plt.xticks(ticks=np.arange(len(LAMBDA)),labels=LAMBDA, fontsize=18)\n    plt.xlabel(\"Lambda\", fontsize=18)\n    plt.ylabel(\"Error\", fontsize = 18)\n    plt.yticks(fontsize=18)\n    #plt.savefig(\"test_train_mse_ols_bootstrap.png\", dpi=300)\n    plt.show()","44646e77":"for i in range(all_error_test.shape[0]):\n    plt.figure(figsize=(10,8))\n    #plt.plot(mean_r2_ols, label=\"R2-score\")\n    #plt.plot(mean_error_test[i], label=\"MSE test\")\n    #plt.plot(mean_bias[4]+mean_variance[4], label=\"bias+variance\")\n    #plt.plot(mean_error_train[i], label=\"MSE train\")\n    plt.plot(mean_bias[i], label=\"Bias\")\n    plt.plot(mean_variance[i], label=\"Variance\")\n    plt.legend(fontsize = 12)\n    plt.xticks(ticks=np.arange(len(LAMBDA)),labels=LAMBDA, fontsize=18)\n    plt.xlabel(\"Lambda\", fontsize=18)\n    plt.ylabel(\"Error\", fontsize = 18)\n    plt.yticks(fontsize=18)\n    #plt.savefig(\"test_train_mse_ols_bootstrap.png\", dpi=300)\n    plt.show()","32d81337":"k_folds = 10\npolynomial =75\n\nz = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\nX=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\nmean_test_error_sk_ols_cv = []\nmean_train_error_sk_ols_cv = []\nmean_bias_sk_ols_cv =[]\nmean_r2_sk_ols_cv = []\nmean_variance_sk_ols_cv =[]\n\nall_test_error_sk_ols_cv = []\nall_train_error_sk_ols_cv = []\nall_bias_sk_ols_cv =[]\nall_r2_sk_ols_cv = []\nall_variance_sk_ols_cv =[]\n\nerror = np.zeros(polynomial)\nbias = np.zeros(polynomial)\nvariance = np.zeros(polynomial)\npolydegree = np.zeros(polynomial)\ntrain_error = np.zeros(polynomial)\n\nfor poly in range(polynomial):\n\n    r2_ = []\n    error_train =[]\n    error_test=[]\n    bias_pol = []\n    variance_pol = []  \n\n    kf = KFold(n_splits=10, random_state=42, shuffle=True)\n    kf.get_n_splits(X_scaled)\n    pred_test = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n    pred_train = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds)) \n    i = 0\n    for train_index, test_index in kf.split(X_scaled):\n        \n        X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n        z_train, z_test = z.ravel()[train_index], z.ravel()[test_index]\n        \n        polyreg=make_pipeline(PolynomialFeatures(poly+1),LinearRegression())\n        polyreg.fit(X_train,z_train)\n\n        z_pred_test=polyreg.predict(X_test)\n        z_pred_train=polyreg.predict(X_train)\n        pred_test[:,i]=polyreg.predict(X_test)\n        pred_train[:,i]=polyreg.predict(X_train)\n\n        i = i + 1\n        error_test.append(mean_squared_error(z_test, z_pred_test))\n        error_train.append(mean_squared_error(z_train, z_pred_train))\n\n        r2_.append(r2_score(z_test,z_pred_test))\n        bias_pol.append(np.mean((z_test - np.mean(z_pred_test))**2))\n        variance_pol.append(np.mean( np.var(z_pred_test)))\n        \n        \n    train_error[poly] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - pred_train)**2, axis=1, keepdims=True) )   \n    error[poly] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - pred_test)**2, axis=1, keepdims=True) )\n    bias[poly] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(pred_test, axis=1, keepdims=True))**2 )\n    variance[poly] = np.mean( np.var(pred_test, axis=1, keepdims=True) )\n\n    print('Polynomial degree:', poly+1)\n    print('Error:', error[poly])\n    print('Bias^2:', bias[poly])\n    print('Var:', variance[poly])\n    print('{} >= {} + {} = {}'.format(error[poly], bias[poly], variance[poly], bias[poly]+variance[poly]))\n    '''\n    z_pred=polyreg.predict(X_scaled)\n    z_pred_plot = z_pred.reshape(z.shape[0],z.shape[1])\n    fig = plt.figure(figsize=(32,10))\n    ax = fig.gca(projection ='3d')\n    surf = ax.plot_surface(x,y,z_pred_plot,cmap=cm.bone, linewidth = 0, antialiased=False)\n    #surf2 = ax.plot_surface(x,y,z,cmap=cm.plasma, linewidth = 0, antialiased=False)\n    ax.zaxis.set_major_locator(LinearLocator(10))\n    ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n    ax.view_init(20, 75)\n    fig.colorbar(surf,shrink=0.5, aspect=5)\n    fig.suptitle(\"A {} degree polynomial fit of Franke function using Sklearn\".format(poly+1) ,fontsize=\"40\", color = \"black\")\n    fig.show()\n    '''\n    mean_test_error_sk_ols_cv.append(np.mean(error_test))\n    mean_train_error_sk_ols_cv.append(np.mean(error_train))\n    mean_r2_sk_ols_cv.append(np.mean(r2_))\n    mean_bias_sk_ols_cv.append(np.mean(bias_pol))\n    mean_variance_sk_ols_cv.append(np.mean(variance_pol))\n    all_test_error_sk_ols_cv.append(error_test)\n    all_train_error_sk_ols_cv.append(error_train)\n    all_r2_sk_ols_cv.append(r2_)\n    all_bias_sk_ols_cv.append(bias_pol)\n    all_variance_sk_ols_cv.append(variance_pol)","918312ba":"plt.figure(figsize=(10,8))\n\nplt.plot(error, label=\"MSE_test\")\nplt.plot(train_error, label=\"MSE_train\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.savefig(\"Bias_variance_analysis_ols_10deg_sklearn_cv_and_ols.png\", dpi=300)\nplt.show()","c5033258":"plt.figure(figsize=(10,8))\n#plt.plot(mean_variance_sk_ols_cv, label=\"Var2\")\nplt.plot(variance, label=\"Var\")\nplt.plot(bias, label=\"Bias\")\n#plt.plot(mean_bias_sk_ols_cv, label=\"Bias2\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.savefig(\"Bias_variance_analysis_ols_10deg_sklearn_cv_and_ols.png\", dpi=300)\nplt.show()","3416669f":"plt.figure(figsize=(10,8))\n#plt.plot(np.asarray(mean_train_error_sk_ols_cv), label=\"MSE train\")\n#plt.plot(mean_test_error_sk_ols_cv, label=\"MSE test\")\nplt.plot(error, label=\"MSE test\")\n#plt.plot(train_error, label=\"MSE_train\")\nplt.plot(variance, label=\"Var\")\nplt.plot(bias, label=\"Bias\")\nplt.plot(bias+variance, label=\"Bias+Variance\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"Error\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.savefig(\"Bias_variance_analysis_ols_10deg_sklearn_cv_and_ols.png\", dpi=300)\nplt.show()","a4229099":"plt.figure(figsize=(10,8))\nprint(\"best R2-score:\",np.amax(mean_r2_sk_ols_cv))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}\".format(int(np.where(mean_r2_sk_ols_cv == np.amax(mean_r2_sk_ols_cv))[0])+1))\n#plt.plot(np.asarray(mean_train_error_sk_ols_cv), label=\"MSE train\")\n#plt.plot(mean_test_error_sk_ols_cv, label=\"MSE test\")\nplt.plot(mean_r2_sk_ols_cv, label=\"R2-score\")\nplt.legend(fontsize = 12)\nplt.xticks([0,1,2,3,4,5,6,7,8,9],[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\nplt.xlabel(\"polynomial degree\", fontsize=18)\nplt.ylabel(\"R2-scorer\", fontsize = 18)\nplt.yticks(fontsize=18)\n#plt.savefig(\"Bias_variance_analysis_ols_10deg_sklearn_cv_and_ols.png\", dpi=300)\nplt.show()","3308516b":"z = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\n\n        \nX=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\n\npolyreg=make_pipeline(PolynomialFeatures((int(np.where(mean_r2_sk_ols_cv == np.amax(mean_r2_sk_ols_cv))[0])+1)),LinearRegression())\npolyreg.fit(X_scaled,z.ravel())\nz_pred_test=polyreg.predict(X_scaled)\n\n\n\n\nprint(\"R2-score:\",r2_score(z.ravel(),z_pred_test))\n\n\n        \nz_pred_for_plot = z_pred.reshape(z.shape[0],z.shape[1])\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.bone, linewidth = 0, antialiased=False)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.view_init(20, 75)\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"A {} degree polynomial fit of the ECG data using OLS\".format((int(np.where(mean_r2_sk_ols_cv == np.amax(mean_r2_sk_ols_cv))[0])+1)) ,fontsize=\"40\", color = \"black\")\n#fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\nfig.show()","31b0197b":"ols_r2_scores_norm = []\nfor i in range(norm_data[100:].shape[0]):\n    ols_r2_scores_norm.append(r2_score(norm_data[100:][i].ravel(),z_pred_test))\nols_r2_scores_norm = np.asarray(ols_r2_scores_norm)\nprint(np.mean(ols_r2_scores_norm))","d698ad71":"ols_r2_scores_lvh = []\nfor i in range(lvh_data.shape[0]):\n    ols_r2_scores_lvh.append(r2_score(lvh_data[i].ravel(),z_pred_test))\nols_r2_scores_lvh = np.asarray(ols_r2_scores_lvh)\nprint(np.mean(ols_r2_scores_lvh))","dcd8f770":"plt.figure(figsize=(10,10))\nplt.boxplot((ols_r2_scores_norm,ols_r2_scores_lvh))\n#plt.boxplot(all_r2_scores_lvh,  boxprops=dict(color='blue'),medianprops=dict(color='black'))\n#plt.ylabel(\"Mean Squared Error\", fontsize=18)\n#plt.xlabel(\"Polynomial degree\", fontsize=20)\n#plt.xticks(ticks = [0,2,4,6,8,10,12,14,16,18], labels = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\n#plt.yticks(fontsize=18)\n#plt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], ['MSE test', 'MSE train'], loc='upper right', fontsize=18)\n#plt.savefig(\"boxplot_mse_train_test_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","58e38a4d":"\npoly = 50\nLAMBDA = [10000, 100, 1, 0.01, 0.0001, 0.000001, 0.00000001,0.0000000001,0.000000000001]\nk_folds = 10\n\nz = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\n\nmean_error=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\n\nall_test_error=[]\nall_train_error=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\n\n\n\nfor i in range(poly):\n    error_poly = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    \n    mean_r2_lambda = []\n    mean_error_lambda = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    \n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n    for l,j in enumerate(LAMBDA):\n        error_lambda = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        X=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X_scaled = scaler.transform(X)\n        kf = KFold(n_splits=10, random_state=42, shuffle=True)\n        kf.get_n_splits(X_scaled)\n\n\n        test_pred = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n        train_pred = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n        k=0\n        for train_index, test_index in kf.split(X):\n\n            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n            z_train, z_test = z.ravel()[train_index], z.ravel()[test_index]\n            \n            ridge=fit_ridge_model(i+1,X_train,z_train,j)\n            _, z_pred =ridge_pred(ridge, X_test)\n            _, z_pred_train =ridge_pred(ridge, X_train)\n            \n            _,test_pred[:,k] = ridge_pred(ridge, X_test)\n            _,train_pred[:,k] = ridge_pred(ridge, X_train)\n        \n            error_lambda.append(np.mean(np.mean((z_test - z_pred)**2)))\n            bias_lambda.append(np.mean((z_test - np.mean(z_pred))**2))\n            variance_lambda.append(np.mean( np.var(z_pred)))\n            r2_lambda.append(r2_score(z_test,z_pred))\n            k= k+1\n            \n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        \n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n        \n        '''\n        ridge=fit_ridge_model(i+1,X_scaled,z.ravel(),j)\n        _, z_pred_for_plot =ridge_pred(ridge, X_scaled)\n        z_pred_for_plot = z_pred_for_plot.reshape(z.shape[0],z.shape[1])\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.bone, linewidth = 0, antialiased=False)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.view_init(20, 75)\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n        '''\n        error_poly.append(error_lambda)\n        bias_poly.append(bias_lambda)\n        variance_poly.append(variance_lambda)\n        r2_poly.append(r2_lambda)\n        \n        mean_r2_lambda.append(np.mean(r2_lambda))\n        mean_error_lambda.append(np.mean(error_lambda))\n        mean_bias_lambda.append(np.mean(bias_lambda))\n        mean_variance_lambda.append(np.mean(variance_lambda))\n\n\n\n    mean_r2.append(mean_r2_lambda)\n    mean_error.append(mean_error_lambda)\n    mean_bias.append(mean_bias_lambda)\n    mean_variance.append(mean_variance_lambda)\n    \n    all_r2.append(r2_poly)\n    all_test_error.append(error)\n    all_train_error.append(train_error)\n    all_bias.append(bias)\n    all_variance.append(variance)","e9e9e243":"print(\"best R2-score:\",np.amax(mean_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]))\nmean_r2 = np.asarray(mean_r2)\nr2_x = np.arange(len(LAMBDA)) \nr2_y = np.arange(poly)\nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(70, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","fb77914e":"all_test_error = np.asarray(all_test_error)\nerror_x = np.arange(len(LAMBDA)) \nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_test_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(70, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","31d347f9":"all_train_error = np.asarray(all_train_error)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_train_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Train error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Train error when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","d7bfbd24":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","fdf70f66":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","a973a26f":"z = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\n\n        \nX=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\nLAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]\n\n            \nridge=fit_ridge_model(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,X_scaled,z.ravel(),LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])])\n_, z_pred =ridge_pred(ridge, X_scaled)\n\n\n\nprint(\"R2-score:\",r2_score(z.ravel(),z_pred))\n\n\n        \nz_pred_for_plot = z_pred.reshape(z.shape[0],z.shape[1])\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.bone, linewidth = 0, antialiased=False)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.view_init(20, 75)\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"A {} degree polynomial fit of ECG data using Ridge with lambda {}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]) ,fontsize=\"40\", color = \"black\")\n#fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\nfig.show()","bec694b2":"ridge_r2_scores_norm = []\nfor i in range(norm_data[100:].shape[0]):\n    ridge_r2_scores_norm.append(r2_score(norm_data[100:][i].ravel(),z_pred))\nridge_r2_scores_norm = np.asarray(ridge_r2_scores_norm)\nprint(np.mean(ridge_r2_scores_norm))","5deb40a8":"ridge_r2_scores_lvh = []\nfor i in range(lvh_data.shape[0]):\n    ridge_r2_scores_lvh.append(r2_score(lvh_data[i].ravel(),z_pred))\nridge_r2_scores_lvh = np.asarray(ridge_r2_scores_lvh)\nprint(np.mean(ridge_r2_scores_lvh))","6fd388b1":"plt.figure(figsize=(10,10))\nplt.boxplot((ridge_r2_scores_norm,ridge_r2_scores_lvh))\n#plt.boxplot(all_r2_scores_lvh,  boxprops=dict(color='blue'),medianprops=dict(color='black'))\n#plt.ylabel(\"Mean Squared Error\", fontsize=18)\n#plt.xlabel(\"Polynomial degree\", fontsize=20)\n#plt.xticks(ticks = [0,2,4,6,8,10,12,14,16,18], labels = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\n#plt.yticks(fontsize=18)\n#plt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], ['MSE test', 'MSE train'], loc='upper right', fontsize=18)\n#plt.savefig(\"boxplot_mse_train_test_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","7345fc99":"\npoly = 50\nLAMBDA = [10000, 100, 1, 0.01, 0.0001, 0.000001, 0.00000001,0.0000000001,0.000000000001]\nk_folds = 10\n\n\nz = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\nmean_error=[]\nmean_bias = []\nmean_variance = []\nmean_r2=[]\n\nall_test_error=[]\nall_train_error=[]\nall_bias = []\nall_variance = []\nall_r2=[]\n\n\n\n\nfor i in range(poly):\n    error_poly = []\n    bias_poly = []\n    variance_poly = []\n    r2_poly = []\n    \n    mean_r2_lambda = []\n    mean_error_lambda = []\n    mean_bias_lambda = []\n    mean_variance_lambda = []\n    \n    error = np.zeros(len(LAMBDA))\n    bias = np.zeros(len(LAMBDA))\n    variance = np.zeros(len(LAMBDA))\n    polydegree = np.zeros(len(LAMBDA))\n    train_error = np.zeros(len(LAMBDA))\n    \n    for l,j in enumerate(LAMBDA):\n        error_lambda = []\n        bias_lambda = []\n        variance_lambda = []\n        r2_lambda = []\n        \n        X=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X_scaled = scaler.transform(X)\n        kf = KFold(n_splits=10, random_state=42, shuffle=True)\n        kf.get_n_splits(X_scaled)\n\n\n        test_pred = np.empty((int(z.ravel().shape[0]*(1\/k_folds)), k_folds))\n        train_pred = np.empty((int(z.ravel().shape[0]*(1-(1\/k_folds))), k_folds))\n        k=0\n        for train_index, test_index in kf.split(X):\n\n            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n            z_train, z_test = z.ravel()[train_index], z.ravel()[test_index]\n            \n            \n            lasso=fit_lasso_model(i+1,X_train,z_train,j)\n            _, z_pred =lasso_pred(lasso, X_test)\n            _, z_pred_train =lasso_pred(lasso, X_train)\n            \n            _,test_pred[:,k] = lasso_pred(lasso, X_test)\n            _,train_pred[:,k] = lasso_pred(lasso, X_train)\n        \n            error_lambda.append(np.mean(np.mean((z_test - z_pred)**2)))\n            bias_lambda.append(np.mean((z_test - np.mean(z_pred))**2))\n            variance_lambda.append(np.mean( np.var(z_pred)))\n            r2_lambda.append(r2_score(z_test,z_pred))\n            k= k+1\n            \n        error[l] = np.mean( np.mean((z_test.reshape(z_test.shape[0],1) - test_pred)**2, axis=1, keepdims=True) )\n        train_error[l] = np.mean( np.mean((z_train.reshape(z_train.shape[0],1) - train_pred)**2, axis=1, keepdims=True) )\n        bias[l] = np.mean( (z_test.reshape(z_test.shape[0],1) - np.mean(test_pred, axis=1, keepdims=True))**2 )\n        variance[l] = np.mean( np.var(test_pred, axis=1, keepdims=True) )\n        \n        print('Polynomial degree:', i+1)\n        print('lambda:', j)\n        print('Error:', error[l])\n        print('Bias^2:', bias[l])\n        print('Var:', variance[l])\n        print('{} >= {} + {} = {}'.format(error[l], bias[l], variance[l], bias[l]+variance[l]))\n\n\n        '''\n        lasso=fit_lasso_model(i+1,X,z.ravel(),j)\n        _, z_pred_for_plot =lasso_pred(lasso, X)\n        z_pred_for_plot = z_pred_for_plot.reshape(z.shape[0],z.shape[1])\n        fig = plt.figure(figsize=(32,12))\n        ax = fig.gca(projection ='3d')\n        surf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.bone, linewidth = 0, antialiased=False)\n        ax.view_init(20, 75)\n        ax.zaxis.set_major_locator(LinearLocator(10))\n        ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n        fig.colorbar(surf,shrink=0.5, aspect=5)\n        fig.suptitle(\"A {} degree polynomial fit of Franke function using Ridge with lambda {}\".format(i+1,j) ,fontsize=\"40\", color = \"black\")\n        #fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\n        fig.show()\n        '''\n        error_poly.append(error_lambda)\n        bias_poly.append(bias_lambda)\n        variance_poly.append(variance_lambda)\n        r2_poly.append(r2_lambda)\n        \n        mean_r2_lambda.append(np.mean(r2_lambda))\n        mean_error_lambda.append(np.mean(error_lambda))\n        mean_bias_lambda.append(np.mean(bias_lambda))\n        mean_variance_lambda.append(np.mean(variance_lambda))\n\n\n\n    mean_r2.append(mean_r2_lambda)\n    mean_error.append(mean_error_lambda)\n    mean_bias.append(mean_bias_lambda)\n    mean_variance.append(mean_variance_lambda)\n    \n    all_r2.append(r2_poly)\n    all_test_error.append(error)\n    all_train_error.append(train_error)\n    all_bias.append(bias)\n    all_variance.append(variance)","7627f44b":"print(\"best R2-score:\",np.amax(mean_r2))\nprint(\"Parameters for best R2-score:\")\nprint(\"Polynom = {}, Lambda ={}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]))\nmean_r2 = np.asarray(mean_r2)\nr2_x = np.arange(len(LAMBDA)) \nr2_y = np.arange(poly)\nr2_x, r2_y = np.meshgrid(r2_x, r2_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(r2_x,r2_y,mean_r2,cmap=cm.plasma, linewidth = 0, antialiased=True)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('R2 score', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"R2-scores for fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","235fa3bf":"all_test_error = np.asarray(all_test_error)\nerror_x = np.arange(len(LAMBDA)) \nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_test_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"MSE when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","eb309e65":"all_train_error = np.asarray(all_train_error)\nerror_x = np.arange(len(LAMBDA))\nerror_y = np.arange(poly)\nerror_x, error_y = np.meshgrid(error_x, error_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(error_x,error_y,all_train_error,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Train error', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Train error when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","333db372":"all_bias = np.asarray(all_bias)\nbias_x = np.arange(len(LAMBDA))\nbias_y = np.arange(poly)\nbias_x, bias_y = np.meshgrid(bias_x, bias_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(bias_x,bias_y,all_bias,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.xaxis.set_ticks(np.arange(len(LAMBDA)))\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.yaxis.set_ticks(np.arange(poly))\nax.set_yticklabels(np.arange(1,poly+1), fontsize=16)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Bias', fontsize=20)\nax.zaxis.labelpad = 35\nax.xaxis.labelpad = 25\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16, pad=15)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Bias when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","d1b8f843":"all_variance = np.asarray(all_variance)\nvar_x = np.arange(len(LAMBDA))\nvar_y = np.arange(poly)\nvar_x, var_y = np.meshgrid(var_x, var_y)\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(var_x,var_y,all_variance,cmap=cm.plasma, linewidth = 0, antialiased=False)\nax.set_xticklabels(LAMBDA, fontsize=16)\nax.set_xlabel('Lambda values', fontsize=20)\nax.set_ylabel('Polynomial features', fontsize=20)\nax.set_zlabel('Variance', fontsize=20)\nax.zaxis.labelpad = 12\nax.xaxis.labelpad = 15\nax.yaxis.labelpad = 20\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='z', labelsize=16)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.zaxis.set_major_formatter(FormatStrFormatter('%.03f'))\nax.view_init(30, 45)\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"Variance when fitting Franke's function using Ridge regression with a grid search \\n on polynomial and lambda parameters\".format(poly) ,fontsize=\"40\", color = \"black\")\n#plt.savefig(\"ecg12lead_threedim.png\", dpi=300)\nfig.show()","7882ecc3":"z = np.mean(norm_data[:100],axis=0)\nx = np.arange(500)\ny = np.arange(12)\nx,y=np.meshgrid(x,y)\n\n\n\n        \nX=np.hstack((x.ravel().reshape(x.ravel().shape[0],1),y.ravel().reshape(y.ravel().shape[0],1)))\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\n\n\nlasso=fit_lasso_model(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,X_scaled,z.ravel(),LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])])\n_, z_pred =lasso_pred(lasso, X_scaled)\n\n        \n\nprint(\"R2-score:\",r2_score(z.ravel(),z_pred))\n\n\n        \nz_pred_for_plot = z_pred.reshape(z.shape[0],z.shape[1])\nfig = plt.figure(figsize=(32,12))\nax = fig.gca(projection ='3d')\nsurf = ax.plot_surface(x,y,z_pred_for_plot,cmap=cm.bone, linewidth = 0, antialiased=False)\nax.zaxis.set_major_locator(LinearLocator(10))\nax.view_init(20, 75)\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\nfig.colorbar(surf,shrink=0.5, aspect=5)\nfig.suptitle(\"A {} degree polynomial fit of ECG data using Lasso with lambda {}\".format(int(np.where(mean_r2 == np.amax(mean_r2))[0])+1,LAMBDA[int(np.where(mean_r2 == np.amax(mean_r2))[1])]) ,fontsize=\"40\", color = \"black\")\n#fig.savefig(\"Franke_function_{}deg_reg.png\".format(degree))\nfig.show()","a1bfa576":"lasso_r2_scores_norm = []\nfor i in range(norm_data[100:].shape[0]):\n    lasso_r2_scores_norm.append(r2_score(norm_data[100:][i].ravel(),z_pred))\nlasso_r2_scores_norm = np.asarray(lasso_r2_scores_norm)\nprint(np.mean(lasso_r2_scores_norm))","640b7a5b":"lasso_r2_scores_lvh = []\nfor i in range(lvh_data.shape[0]):\n    lasso_r2_scores_lvh.append(r2_score(lvh_data[i].ravel(),z_pred))\nlasso_r2_scores_lvh = np.asarray(lasso_r2_scores_lvh)\nprint(np.mean(lasso_r2_scores_lvh))","7bc3985e":"plt.figure(figsize=(10,10))\nplt.boxplot((lasso_r2_scores_norm,lasso_r2_scores_lvh))\n#plt.boxplot(all_r2_scores_lvh,  boxprops=dict(color='blue'),medianprops=dict(color='black'))\n#plt.ylabel(\"Mean Squared Error\", fontsize=18)\n#plt.xlabel(\"Polynomial degree\", fontsize=20)\n#plt.xticks(ticks = [0,2,4,6,8,10,12,14,16,18], labels = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"], fontsize=18)\n#plt.yticks(fontsize=18)\n#plt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], ['MSE test', 'MSE train'], loc='upper right', fontsize=18)\n#plt.savefig(\"boxplot_mse_train_test_ols_bootstrap_10deg.png\", dpi=300)\nplt.show()","e728d069":"all_r2_scores_norm = np.vstack((ols_r2_scores_norm,ridge_r2_scores_norm,lasso_r2_scores_norm))","6d24bf26":"all_r2_scores_lvh = np.vstack((ols_r2_scores_lvh,ridge_r2_scores_lvh,lasso_r2_scores_lvh))","22d1203a":"pos_1=np.ones(all_r2_scores_norm.shape[0])\npos_2=np.ones(all_r2_scores_lvh.shape[0])\nfor i in range(all_r2_scores_lvh.shape[0]):\n    pos_1[i]=0.5 + i * 2\n    pos_2[i]= i * 2\nplt.figure(figsize=(10,10))\nbox1 = plt.boxplot(all_r2_scores_norm.tolist(), positions =pos_1, boxprops=dict(color='red'), medianprops=dict(color='black'))\nbox2 = plt.boxplot(all_r2_scores_lvh.tolist(), positions = pos_2, boxprops=dict(color='blue'),medianprops=dict(color='black'))\nplt.ylabel(\"R2-score\", fontsize=18)\nplt.xticks(ticks = [0,2,4], labels = [\"OLS\",\"Ridge\",\"Lasso\"], fontsize=20)\nplt.yticks(fontsize=18)\nplt.legend([box1[\"boxes\"][0], box2[\"boxes\"][0]], ['Normal', 'LVH'], loc='upper right', fontsize=18)\nplt.savefig(\"normal_lvh_fit.png\", dpi=300)\nplt.show()","5354c028":"## Ridge SKlearn  Bootstrap","948fa018":"# <center> Linear fitting of Franke's function <\/center>","56cd0ae7":"### ECG Ridge Sklearn","53d060c1":"## Scikit-learn OLS with Bootstrap resampling:\nsource: https:\/\/towardsdatascience.com\/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2","5697f484":"SNOMED-CT code 426783006 represents normal sinus rythm = normal heart beat","23a2ecef":"# Ridge Bootstrap","38f16778":"## Ridge Pred","872e0efd":"## My ridge","2b5ade38":"# <center> Fitting Franke's function using Ordinary Least Square and cross validation<\/center>","f3aa5691":"## <center> Lasso bootstrap <\/center>","a93cbec0":"# <center>Lasso Regression<\/center>","1670d9e7":"## <center>Ridge using own algorithm<\/center>","9ecf2fae":"# SKlearn OLS and Cross validation","c0e24a90":"### ECG Lasso Sklearn","a6ab9cce":"# <center>Fit a 12-lead ECG surface plot<\/center>","f1fe23bb":"## <center> Lasso using sklearn <center>","685653e9":"# Test Lasso model on unseen ECG's","0348be17":"## My algorithm - Ordinary Least Square and cross validation","59f8aed6":"# <center>Ridge Regression<\/center>","ec03329f":"## My OLS kfold","5d4ef0d2":"## OLS pred","9925b75e":"## OLS SKlearn","ac0595a8":"## <center> Fitting Franke's function using Ordinary Least Square and Bootstrap resampling <\/center>","0b425c95":"## <center> Ridge using Sklearn <\/center>"}}