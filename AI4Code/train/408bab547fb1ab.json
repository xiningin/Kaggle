{"cell_type":{"1dd86516":"code","ded145a7":"code","441ca23b":"code","5cbc771a":"code","ef48163b":"code","9e7daaf0":"code","6315ce4c":"code","6c0b74da":"code","04de5375":"code","20390499":"code","de94d5ff":"code","c166f3f0":"code","96b7152e":"code","a9352a0b":"code","b3f0e971":"code","8682f6b7":"code","76b7f237":"code","1f44ea8c":"code","b422fca1":"code","8b764e4e":"code","bb645538":"code","4659d425":"code","dfb7ad13":"code","eb65f859":"code","d58ac808":"code","8c66ea8e":"markdown","39dc1c13":"markdown","77c1aa57":"markdown","a40e4af1":"markdown","283846d6":"markdown","51ee4f06":"markdown","17334457":"markdown","20c96339":"markdown","0c596018":"markdown","25ade8ea":"markdown","8ade2f75":"markdown","9c7ae30d":"markdown","f9d82b01":"markdown"},"source":{"1dd86516":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ded145a7":"import bz2\nfrom collections import Counter\nimport re\nimport nltk\nimport numpy as np\nnltk.download('punkt')","441ca23b":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')","5cbc771a":"train_file = train_file.readlines()\ntest_file = test_file.readlines()","ef48163b":"#pct of the data used for the models\npct = 0.3\nnum_train = int(len(train_file)*pct) #max 3600000\nnum_test = int(len(test_file)*pct) #max 400000","9e7daaf0":"train_file = [x.decode('utf-8') for x in train_file[:num_train]]\ntest_file = [x.decode('utf-8') for x in test_file[:num_train]]","6315ce4c":"train_file[1:5]","6c0b74da":"# Extracting labels from sentences\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file]\ntrain_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file]\n\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file]\ntest_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file]\n","04de5375":"for i in range(len(train_sentences)):\n    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n\nfor i in range(len(test_sentences)):\n    test_sentences[i] = re.sub('\\d','0',test_sentences[i])","20390499":"for i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n        \nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","de94d5ff":"words = Counter()  # Dictionary that will map a word to the number of times it appeared in all the training sentences\nfor i, sentence in enumerate(train_sentences):\n    # The sentences will be stored as a list of words\/tokens\n    train_sentences[i] = []\n    for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n        words.update([word.lower()])  # Converting all the words to lowercase\n        train_sentences[i].append(word)\n    if i%20000 == 0:\n        print(str((i*100)\/num_train) + \"% done\")\nprint(\"100% done\")","c166f3f0":"# Removing the words that only appear once\nwords = {k:v for k,v in words.items() if v>1}\n# Sorting the words according to the number of appearances, with the most common word being first\nwords = sorted(words, key=words.get, reverse=True)\n# Adding padding and unknown to our vocabulary so that they will be assigned an index\nwords = ['_PAD','_UNK'] + words\n# Dictionaries to store the word to index mappings and vice versa\nword2idx = {o:i for i,o in enumerate(words)}\nidx2word = {i:o for i,o in enumerate(words)}","96b7152e":"for i, sentence in enumerate(train_sentences):\n    # Looking up the mapping dictionary and assigning the index to the respective words\n    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n\nfor i, sentence in enumerate(test_sentences):\n    # For test sentences, we have to tokenize the sentences as well\n    test_sentences[i] = [word2idx[word.lower()] if word.lower() in word2idx else 0 for word in nltk.word_tokenize(sentence)]","a9352a0b":"# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\ndef pad_input(sentences, seq_len):\n    features = np.zeros((len(sentences), seq_len),dtype=int)\n    for ii, review in enumerate(sentences):\n        if len(review) != 0:\n            features[ii, -len(review):] = np.array(review)[:seq_len]\n    return features\n\nseq_len = 200  # The length that the sentences will be padded\/shortened to\n\ntrain_sentences = pad_input(train_sentences, seq_len)\ntest_sentences = pad_input(test_sentences, seq_len)\n\n# Converting our labels into numpy arrays\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)","b3f0e971":"split_frac = 0.5 # 50% validation, 50% test\nsplit_id = int(split_frac * len(test_sentences))\nval_sentences, test_sentences = test_sentences[:split_id], test_sentences[split_id:]\nval_labels, test_labels = test_labels[:split_id], test_labels[split_id:]","8682f6b7":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\ntrain_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_labels))\nval_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_labels))\ntest_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n\nbatch_size = 400\n\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\nval_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)","76b7f237":"# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\nis_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print('GPU available')\nelse:\n    device = torch.device(\"cpu\")\n    print('Only CPU')","1f44ea8c":"class SentimentNet(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        super(SentimentNet, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding(x)\n        lstm_out, hidden = self.lstm(embeds, hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n        out = self.sigmoid(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n        return hidden","b422fca1":"vocab_size = len(word2idx) + 1\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 512\nn_layers = 2\n\nmodel = SentimentNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nmodel.to(device)\n\nlr=0.005\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","8b764e4e":"epochs = 2\ncounter = 0\nprint_every = 500\nclip = 5\nvalid_loss_min = np.Inf\n\nmodel.train()\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        counter += 1\n        h = tuple([e.data for e in h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if counter%print_every == 0:\n            val_h = model.init_hidden(batch_size)\n            val_losses = []\n            model.eval()\n            for inp, lab in val_loader:\n                val_h = tuple([each.data for each in val_h])\n                inp, lab = inp.to(device), lab.to(device)\n                out, val_h = model(inp, val_h)\n                val_loss = criterion(out.squeeze(), lab.float())\n                val_losses.append(val_loss.item())\n                \n            model.train()\n            print(\"Epoch: {}\/{}...\".format(i+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n            if np.mean(val_losses) <= valid_loss_min:\n                torch.save(model.state_dict(), '.\/state_dict.pt')\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                valid_loss_min = np.mean(val_losses)","bb645538":"# Loading the best model\nmodel.load_state_dict(torch.load('.\/state_dict.pt'))\n\ntest_losses = []\nnum_correct = 0\nh = model.init_hidden(batch_size)\n\nmodel.eval()\nfor inputs, labels in test_loader:\n    h = tuple([each.data for each in h])\n    inputs, labels = inputs.to(device), labels.to(device)\n    output, h = model(inputs, h)\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(output.squeeze())  # Rounds the output to 0\/1\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}%\".format(test_acc*100))","4659d425":"class GRUNet(nn.Module):\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n        super(GRUNet, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_size)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        batch_size = x.size(0)\n        x = x.long()\n        embeds = self.embedding(x)\n        GRU_out, hidden = self.gru(embeds, hidden)\n        GRU_out = GRU_out.contiguous().view(-1,self.hidden_dim)\n        \n        out = self.dropout(GRU_out)\n        out = self.fc(out)\n        out = self.sigmoid(out)\n        \n        out = out.view(batch_size, -1)\n        out = out[:,-1]\n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n        return hidden","dfb7ad13":"vocab_size = len(word2idx) + 1\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 512\nn_layers = 2\n\nmodel = GRUNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nmodel.to(device)\n\nlr=0.001\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)","eb65f859":"epochs = 2\ncounter = 0\nprint_every = 500\nclip = 5\nvalid_loss_min = np.Inf\n\nmodel.train()\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n    \n    for inputs, labels in train_loader:\n        counter += 1\n        h = h.data\n        inputs, labels = inputs.to(device), labels.to(device)\n        model.zero_grad()\n        output, h = model(inputs, h)\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        if counter%print_every == 0:\n            val_h = model.init_hidden(batch_size)\n            val_losses = []\n            model.eval()\n            for inp, lab in val_loader:\n                val_h = val_h.data\n                inp, lab = inp.to(device), lab.to(device)\n                out, val_h = model(inp, val_h)\n                val_loss = criterion(out.squeeze(), lab.float())\n                val_losses.append(val_loss.item())\n                \n            model.train()\n            print(\"Epoch: {}\/{}...\".format(i+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n            if np.mean(val_losses) <= valid_loss_min:\n                torch.save(model.state_dict(), '.\/state_dict.pt')\n                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n                valid_loss_min = np.mean(val_losses)","d58ac808":"# Loading the best model GRU\nmodel.load_state_dict(torch.load('.\/state_dict.pt'))\n\ntest_losses = []\nnum_correct = 0\nh = model.init_hidden(batch_size)\n\nmodel.eval()\nfor inputs, labels in test_loader:\n    h = h.data\n    inputs, labels = inputs.to(device), labels.to(device)\n    output, h = model(inputs, h)\n    test_loss = criterion(output.squeeze(), labels.float())\n    test_losses.append(test_loss.item())\n    pred = torch.round(output.squeeze())  # Rounds the output to 0\/1\n    correct_tensor = pred.eq(labels.float().view_as(pred))\n    correct = np.squeeze(correct_tensor.cpu().numpy())\n    num_correct += np.sum(correct)\n\nprint(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\ntest_acc = num_correct\/len(test_loader.dataset)\nprint(\"Test accuracy: {:.3f}%\".format(test_acc*100))","8c66ea8e":"Split into train\/Val\/test datasets","39dc1c13":"to remove typos and words that likely don't exist, we'll remove all words from the vocab that only appear once throughout\n\nTo account for unknown words and padding, we'll have to add them to our vocabulary as well. Each word in the vocabulary will then be assigned an integer index and after that mapped to this integer.","77c1aa57":"Check for GPU","a40e4af1":"Tokenization of the sentenses with NLTK library","283846d6":"GRU model","51ee4f06":"Convert bytes object into utf-8","17334457":"Create model with Pytorch","20c96339":"padding the sentences with 0s and shortening the lengthy sentences so that the data can be trained in batches to speed things up","0c596018":"We'll have to extract out the labels from the sentences.","25ade8ea":"Data cleaning","8ade2f75":"convert the words in the sentences to their corresponding indexes","9c7ae30d":"![Simple LSTM model](https:\/\/blog.floydhub.com\/content\/images\/2019\/06\/Slide24.JPG)\nsource : https:\/\/blog.floydhub.com\/long-short-term-memory-from-zero-to-hero-with-pytorch\/","f9d82b01":"> Modify URLs to ``<url>``"}}