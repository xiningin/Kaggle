{"cell_type":{"df2734bb":"code","c403c467":"code","45d03ed9":"code","a47d6295":"code","5665ce11":"code","ad1c9d78":"code","92032bd3":"code","a2659cba":"code","6674f77a":"code","9054b25e":"code","83917125":"code","3e94f30d":"code","3c51a78a":"code","d86874c6":"code","432d1232":"code","d587126e":"code","483a591b":"code","9dfa0982":"code","bc985603":"code","f3d49d34":"code","8e872c6b":"code","5e6e17ab":"code","049057a2":"code","c2a37d6f":"markdown"},"source":{"df2734bb":"!ls '\/kaggle\/input\/'","c403c467":"import pandas as pd\nimport random\nimport numpy as np\nimport re\nfrom transformers import BertTokenizer\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom string import punctuation","45d03ed9":"tokenizer = BertTokenizer.from_pretrained('\/kaggle\/input\/bert-base-uncased', do_lower_case=True)","a47d6295":"df = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv',engine='python')\ndf.head()","5665ce11":"df.columns","ad1c9d78":"df['p_review'] = df['review']\ndf['p_review'].replace(to_replace = '.<br\\s+\\\/'.format(punctuation),inplace=True,value='',regex=True)\ndf['p_review'].replace(to_replace = '[{}]'.format(punctuation),inplace=True,value='',regex=True)\ndf['p_review'].replace(to_replace = '\\s+',inplace=True,value=' ',regex=True)\n\n","92032bd3":"#converting polarity to numnber\ndf['p_polarity']=df['sentiment']\ndf['p_polarity'] = df['p_polarity'].map({'positive':0,'negative':1})\ndf.head()","a2659cba":"data_all=[]\nfor i,row in df.iterrows():\n    data_all.append((row['p_review'],row['p_polarity']))","6674f77a":"id_list = list(range(0,len(data_all)))\nnp.random.seed(3)\nnp.random.shuffle(id_list)\ntrain_data = [data_all[i] for i in id_list[:int(len(data_all)*.90)]]\ntest_data = [data_all[i] for i in id_list[int(len(data_all)*.90):]]\nlen(train_data),len(test_data)\ntrain_data=train_data[:2000]\ntest_data=test_data[:1000]","9054b25e":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","83917125":"#Data Loader\nclass MyLoader(Dataset):\n    def __init__(self,data_set):\n        \n        self.data=data_set\n        \n    def __getitem__(self,index):\n        return self.data[index][0],self.data[index][1]\n        \n        \n    def __len__(self):\n        return len(self.data)\ndef my_collate_fn(batch):\n    sent_list = [i for i,j in batch]\n    lbl_list = [j for i,j in batch]\n    t_encode = tokenizer.batch_encode_plus(sent_list,pad_to_max_length=True,max_length=512)\n    return torch.tensor(t_encode['input_ids']),torch.tensor(t_encode['token_type_ids']),torch.tensor(t_encode['attention_mask']),torch.tensor(lbl_list)","3e94f30d":"batch_sz=4\ntrain_obj = MyLoader(train_data)\ntrain_loader=torch.utils.data.DataLoader(train_obj,batch_size=batch_sz,collate_fn=my_collate_fn)\n\nval_obj = MyLoader(test_data)\nval_loader=torch.utils.data.DataLoader(val_obj,batch_size=batch_sz,collate_fn=my_collate_fn)","3c51a78a":"for i,(input_ids,token_type_ids,attention_mask,lbl_list) in enumerate(train_loader): \n    print(type(input_ids[0]))\n    break","d86874c6":"#Load and Fine tune PreTrained Bert Model\n\nmodel = BertForSequenceClassification.from_pretrained(\\\n                                                      \n    \"\/kaggle\/input\/bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\\\n                                                      \n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\\\n                                                      \n    output_hidden_states = False, # Whether the model returns all hidden-states.\n                                                      \n)\nmodel.to(device)","432d1232":"# The epsilon parameter eps = 1e-8 is \u201ca very small number to prevent any division by zero in the implementation\u201d\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","d587126e":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","483a591b":"epochs=2","9dfa0982":"loss_values = []\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n    # Reset the total loss for this epoch.\n    total_loss = 0\n    # Put the model into training mode. Don't be mislead--the call to \n    model.train()\n    # For each batch of training data...\n    for step, batch in enumerate(train_loader):\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[2].to(device)\n        b_labels = batch[3].to(device)\n        # Always clear any previously calculated gradients before performing a\n        \n        model.zero_grad()        \n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the loss value out of the tuple.\n        loss = outputs[0]\n        # Accumulate the training loss over all of the batches so that we can\n        total_loss += loss.item()\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n        \n        # Update the learning rate.\n#         scheduler.step()\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_loader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    #print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n    print(\"\")\n    print(\"Running Validation...\")\n    \n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n    # Tracking variables \n    eval_loss, eval_accuracy = 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n    # Evaluate data for one epoch\n    for batch in val_loader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids,abc, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n        # Track the number of batches\n        nb_eval_steps += 1\n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n   # print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\nprint(\"\")\nprint(\"Training complete!\")","bc985603":"# torch.save(model,'checkpoint.pth')","f3d49d34":"loaded_model = torch.load('checkpoint.pth')\nloaded_model.eval()\nloaded_model.to(device)","8e872c6b":"!ls ","5e6e17ab":"val_obj1 = MyLoader([('I found this movie which is good.',0),('one of the rubbish movie ever watched',1)])\nval_loader1=torch.utils.data.DataLoader(val_obj1,batch_size=batch_sz,collate_fn=my_collate_fn)","049057a2":"for (b_input_ids,abc,b_input_mask,lbl) in val_loader1:\n    outputs = loaded_model(b_input_ids.to(device), \n                            token_type_ids=None, \n                            attention_mask=b_input_mask.to(device))\n    \n    print(outputs[0])","c2a37d6f":"### Training"}}