{"cell_type":{"fce4f8bb":"code","4d58b309":"code","c4ed3875":"code","0b557e06":"code","f3604b8e":"code","d5482935":"code","87a35f9a":"code","b55aea5b":"code","4454470c":"code","36c4b552":"code","2ffffe57":"code","c949b29f":"code","b805d8a5":"code","556a68ac":"code","bd9b00f3":"code","16634a53":"code","3ec254ee":"code","e0b9108f":"code","21a351b0":"markdown","26e3dab5":"markdown","72ab195b":"markdown","3b3cea62":"markdown","2921c3b6":"markdown","35f83e55":"markdown","abf81874":"markdown","d7936229":"markdown","94954f9a":"markdown"},"source":{"fce4f8bb":"!wget 'https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2019\/03\/zebra.jpg'","4d58b309":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport struct\n\nimport matplotlib.pyplot as plt\n\nfrom numpy import expand_dims\nfrom keras.models import load_model\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\n\nfrom keras.layers import Conv2D\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import LeakyReLU\nfrom keras.layers import ZeroPadding2D\nfrom keras.layers import UpSampling2D\nfrom keras.layers.merge import add, concatenate\nfrom keras.models import Model\n\nfrom matplotlib.patches import Rectangle","c4ed3875":"def _conv_block(inp, convs, skip=True):\n    x = inp\n    count = 0\n    \n    for conv in convs:\n        if count == (len(convs) - 2) and skip:\n            skip_connection = x\n        count += 1\n        \n        if conv['stride'] > 1: x = ZeroPadding2D(((1,0),(1,0)))(x) # peculiar padding as darknet prefer left and top\n        x = Conv2D(conv['filter'], \n                   conv['kernel'], \n                   strides=conv['stride'], \n                   padding='valid' if conv['stride'] > 1 else 'same', # peculiar padding as darknet prefer left and top\n                   name='conv_' + str(conv['layer_idx']), \n                   use_bias=False if conv['bnorm'] else True)(x)\n        if conv['bnorm']: x = BatchNormalization(epsilon=0.001, name='bnorm_' + str(conv['layer_idx']))(x)\n        if conv['leaky']: x = LeakyReLU(alpha=0.1, name='leaky_' + str(conv['layer_idx']))(x)\n\n    return add([skip_connection, x]) if skip else x\n\n\ndef make_yolov3_model():\n    input_image = Input(shape=(None, None, 3))\n\n    # Layer  0 => 4\n    x = _conv_block(input_image, [{'filter': 32, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 0},\n                                  {'filter': 64, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 1},\n                                  {'filter': 32, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 2},\n                                  {'filter': 64, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 3}])\n\n    # Layer  5 => 8\n    x = _conv_block(x, [{'filter': 128, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 5},\n                        {'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 6},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 7}])\n\n    # Layer  9 => 11\n    x = _conv_block(x, [{'filter':  64, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 9},\n                        {'filter': 128, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 10}])\n\n    # Layer 12 => 15\n    x = _conv_block(x, [{'filter': 256, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 12},\n                        {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 13},\n                        {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 14}])\n\n    # Layer 16 => 36\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 16+i*3},\n                            {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 17+i*3}])\n        \n    skip_36 = x\n        \n    # Layer 37 => 40\n    x = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 37},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 38},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 39}])\n\n    # Layer 41 => 61\n    for i in range(7):\n        x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 41+i*3},\n                            {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 42+i*3}])\n        \n    skip_61 = x\n        \n    # Layer 62 => 65\n    x = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 2, 'bnorm': True, 'leaky': True, 'layer_idx': 62},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 63},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 64}])\n\n    # Layer 66 => 74\n    for i in range(3):\n        x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 66+i*3},\n                            {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 67+i*3}])\n        \n    # Layer 75 => 79\n    x = _conv_block(x, [{'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 75},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 76},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 77},\n                        {'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 78},\n                        {'filter':  512, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 79}], skip=False)\n\n    # Layer 80 => 82\n    yolo_82 = _conv_block(x, [{'filter': 1024, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 80},\n                              {'filter':  255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 81}], skip=False)\n\n    # Layer 83 => 86\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 84}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_61])\n\n    # Layer 87 => 91\n    x = _conv_block(x, [{'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 87},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 88},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 89},\n                        {'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 90},\n                        {'filter': 256, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True, 'layer_idx': 91}], skip=False)\n\n    # Layer 92 => 94\n    yolo_94 = _conv_block(x, [{'filter': 512, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 92},\n                              {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 93}], skip=False)\n\n    # Layer 95 => 98\n    x = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True, 'leaky': True,   'layer_idx': 96}], skip=False)\n    x = UpSampling2D(2)(x)\n    x = concatenate([x, skip_36])\n\n    # Layer 99 => 106\n    yolo_106 = _conv_block(x, [{'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 99},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 100},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 101},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 102},\n                               {'filter': 128, 'kernel': 1, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 103},\n                               {'filter': 256, 'kernel': 3, 'stride': 1, 'bnorm': True,  'leaky': True,  'layer_idx': 104},\n                               {'filter': 255, 'kernel': 1, 'stride': 1, 'bnorm': False, 'leaky': False, 'layer_idx': 105}], skip=False)\n\n    model = Model(input_image, [yolo_82, yolo_94, yolo_106])    \n    return model\n\n\n\n\nclass WeightReader:\n    def __init__(self, weight_file):\n        with open(weight_file, 'rb') as w_f:\n            major,    = struct.unpack('i', w_f.read(4))\n            minor,    = struct.unpack('i', w_f.read(4))\n            revision, = struct.unpack('i', w_f.read(4))\n\n            if (major*10 + minor) >= 2 and major < 1000 and minor < 1000:\n                w_f.read(8)\n            else:\n                w_f.read(4)\n\n            transpose = (major > 1000) or (minor > 1000)\n            \n            binary = w_f.read()\n\n        self.offset = 0\n        self.all_weights = np.frombuffer(binary, dtype='float32')\n        \n    def read_bytes(self, size):\n        self.offset = self.offset + size\n        return self.all_weights[self.offset-size:self.offset]\n\n    def load_weights(self, model):\n        for i in range(106):\n            try:\n                conv_layer = model.get_layer('conv_' + str(i))\n                print(\"loading weights of convolution #\" + str(i))\n\n                if i not in [81, 93, 105]:\n                    norm_layer = model.get_layer('bnorm_' + str(i))\n\n                    size = np.prod(norm_layer.get_weights()[0].shape)\n\n                    beta  = self.read_bytes(size) # bias\n                    gamma = self.read_bytes(size) # scale\n                    mean  = self.read_bytes(size) # mean\n                    var   = self.read_bytes(size) # variance            \n\n                    weights = norm_layer.set_weights([gamma, beta, mean, var])  \n\n                if len(conv_layer.get_weights()) > 1:\n                    bias   = self.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    \n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel, bias])\n                else:\n                    kernel = self.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n                    kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n                    kernel = kernel.transpose([2,3,1,0])\n                    conv_layer.set_weights([kernel])\n            except ValueError:\n                print(\"no convolution #\" + str(i))     \n    \n    def reset(self):\n        self.offset = 0","0b557e06":"model = make_yolov3_model()\n\n# load the model weights\nweight_reader = WeightReader('..\/input\/yolo3weights\/yolov3.weights') \n\n# set the model weights into the model \nweight_reader.load_weights(model)","f3604b8e":"def load_image_pixels(filename, shape):\n    image = load_img(filename)\n    width, height = image.size\n    \n    image = load_img(filename, target_size=shape)\n    image = img_to_array(image)\n    \n    image = image.astype('float32')\n    \n    image = image\/255.0\n    \n    image = np.expand_dims(image, 0)\n    return image, width, height\n\ninput_w, input_h = 416, 416\n\nimage_path = '.\/zebra.jpg'\n\nimage, image_w, image_h = load_image_pixels(image_path, (input_w, input_h))\n\ny_pred = model.predict(image)\n\nprint([a.shape for a in y_pred])","d5482935":"class BoundBox:\n    def __init__(self, xmin, ymin, xmax, ymax, objness = None, classes = None):\n        self.xmin = xmin\n        self.ymin = ymin\n        self.xmax = xmax\n        self.ymax = ymax\n        \n        self.objness = objness\n        self.classes = classes\n\n        self.label = -1\n        self.score = -1\n\n    def get_label(self):\n        if self.label == -1:\n            self.label = np.argmax(self.classes)\n        \n        return self.label\n    \n    def get_score(self):\n        if self.score == -1:\n            self.score = self.classes[self.get_label()]\n            \n        return self.score\n\n\ndef _sigmoid(x):\n    return 1. \/ (1. + np.exp(-x))\n\n\ndef decode_netout(netout, anchors, obj_thresh, net_h, net_w):\n    grid_h, grid_w = netout.shape[:2]\n    nb_box = 3\n    netout = netout.reshape((grid_h, grid_w, nb_box, -1))\n    nb_class = netout.shape[-1] - 5\n\n    boxes = []\n\n    netout[..., :2]  = _sigmoid(netout[..., :2])\n    netout[..., 4:]  = _sigmoid(netout[..., 4:])\n    netout[..., 5:]  = netout[..., 4][..., np.newaxis] * netout[..., 5:]\n    netout[..., 5:] *= netout[..., 5:] > obj_thresh\n\n    for i in range(grid_h*grid_w):\n        row = i \/ grid_w\n        col = i % grid_w\n        \n        for b in range(nb_box):\n            # 4th element is objectness score\n            objectness = netout[int(row)][int(col)][b][4]\n            #objectness = netout[..., :4]\n            \n            if(objectness.all() <= obj_thresh): continue\n            \n            # first 4 elements are x, y, w, and h\n            x, y, w, h = netout[int(row)][int(col)][b][:4]\n\n            x = (col + x) \/ grid_w # center position, unit: image width\n            y = (row + y) \/ grid_h # center position, unit: image height\n            w = anchors[2 * b + 0] * np.exp(w) \/ net_w # unit: image width\n            h = anchors[2 * b + 1] * np.exp(h) \/ net_h # unit: image height  \n            \n            # last elements are class probabilities\n            classes = netout[int(row)][col][b][5:]\n            \n            box = BoundBox(x-w\/2, y-h\/2, x+w\/2, y+h\/2, objectness, classes)\n            #box = BoundBox(x-w\/2, y-h\/2, x+w\/2, y+h\/2, None, classes)\n\n            boxes.append(box)\n\n    return boxes\n\n\n\n\ndef correct_yolo_boxes(boxes, image_h, image_w, net_h, net_w):\n    if (float(net_w)\/image_w) < (float(net_h)\/image_h):\n        new_w = net_w\n        new_h = (image_h*net_w)\/image_w\n    else:\n        new_h = net_w\n        new_w = (image_w*net_h)\/image_h\n        \n    for i in range(len(boxes)):\n        x_offset, x_scale = (net_w - new_w)\/2.\/net_w, float(new_w)\/net_w\n        y_offset, y_scale = (net_h - new_h)\/2.\/net_h, float(new_h)\/net_h\n        \n        boxes[i].xmin = int((boxes[i].xmin - x_offset) \/ x_scale * image_w)\n        boxes[i].xmax = int((boxes[i].xmax - x_offset) \/ x_scale * image_w)\n        boxes[i].ymin = int((boxes[i].ymin - y_offset) \/ y_scale * image_h)\n        boxes[i].ymax = int((boxes[i].ymax - y_offset) \/ y_scale * image_h)\n        \n        \ndef _interval_overlap(interval_a, interval_b):\n    x1, x2 = interval_a\n    x3, x4 = interval_b\n\n    if x3 < x1:\n        if x4 < x1:\n            return 0\n        else:\n            return min(x2,x4) - x1\n    else:\n        if x2 < x3:\n             return 0\n        else:\n            return min(x2,x4) - x3   \n        \n\ndef bbox_iou(box1, box2):\n    intersect_w = _interval_overlap([box1.xmin, box1.xmax], [box2.xmin, box2.xmax])\n    intersect_h = _interval_overlap([box1.ymin, box1.ymax], [box2.ymin, box2.ymax])\n    \n    intersect = intersect_w * intersect_h\n\n    w1, h1 = box1.xmax-box1.xmin, box1.ymax-box1.ymin\n    w2, h2 = box2.xmax-box2.xmin, box2.ymax-box2.ymin\n    \n    union = w1*h1 + w2*h2 - intersect\n    \n    return float(intersect) \/ union\n\n\ndef do_nms(boxes, nms_thresh):\n    if len(boxes) > 0:\n        nb_class = len(boxes[0].classes)\n    else:\n        return\n        \n    for c in range(nb_class):\n        sorted_indices = np.argsort([-box.classes[c] for box in boxes])\n\n        for i in range(len(sorted_indices)):\n            index_i = sorted_indices[i]\n\n            if boxes[index_i].classes[c] == 0: continue\n\n            for j in range(i+1, len(sorted_indices)):\n                index_j = sorted_indices[j]\n\n                if bbox_iou(boxes[index_i], boxes[index_j]) >= nms_thresh:\n                    boxes[index_j].classes[c] = 0\n                    \n","87a35f9a":"# get all of the results above a threshold\ndef get_boxes(boxes, labels, thresh):\n    v_boxes, v_labels, v_scores = list(), list(), list()\n    # enumerate all boxes\n    for box in boxes:\n        # enumerate all possible labels\n        for i in range(len(labels)):\n        # check if the threshold for this label is high enough\n            if box.classes[i] > thresh:\n                v_boxes.append(box)\n                v_labels.append(labels[i]) \n                v_scores.append(box.classes[i]*100)\n        # don't break, many labels may trigger for one box\n    return v_boxes, v_labels, v_scores\n\n# draw all results\ndef draw_boxes(filename, v_boxes, v_labels, v_scores):\n    # load the image\n    data = plt.imread(filename)\n    # plot the image\n    plt.imshow(data)\n    # get the context for drawing boxes\n    ax = plt.gca()\n    # plot each box\n    for i in range(len(v_boxes)):\n        box = v_boxes[i]\n        # get coordinates\n        y1, x1, y2, x2 = box.ymin, box.xmin, box.ymax, box.xmax\n        # calculate width and height of the box\n        width, height = x2 - x1, y2 - y1\n        # create the shape\n        rect = Rectangle((x1, y1), width, height, fill=False, color='red') # draw the box\n        ax.add_patch(rect)\n        # draw text and score in top left corner\n        label = \"%s (%.3f)\" % (v_labels[i], v_scores[i])\n        plt.text(x1, y1, label, color='red') # show the plot\n    plt.show()","b55aea5b":"anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]] \n# define the probability threshold for detected objects\nclass_threshold = 0.6\n\nboxes = list()\n\nfor i in range(len(y_pred)):\n  # decode the output of the network\n  boxes += decode_netout(y_pred[i][0], anchors[i], class_threshold, input_h, input_w)","4454470c":"correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n\n# suppress non-maximal boxes\ndo_nms(boxes, 0.5)\n\nlabels = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \\\n              \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \\\n              \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \\\n              \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \\\n              \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \\\n              \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \\\n              \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \\\n              \"chair\", \"sofa\", \"pottedplant\", \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \\\n              \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \\\n              \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"]\n\n# get the details of the detected objects\nv_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)","36c4b552":"# summarize what we found\nfor i in range(len(v_boxes)):\n    print(v_labels[i], v_scores[i])\n\ndraw_boxes(image_path, v_boxes, v_labels, v_scores)","2ffffe57":"\n\nimage_path = '..\/input\/object-detection\/Training_set\/Training_set\/electric car\/electric-cars-107.jpeg'\n\ndef detect_objects(image_path):\n    input_w, input_h = 416, 416\n\n    image, image_w, image_h = load_image_pixels(image_path, (input_w, input_h))\n\n    y_pred = model.predict(image)\n\n\n    anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30, 33,23]] \n    # define the probability threshold for detected objects\n    class_threshold = 0.6\n\n    boxes = list()\n\n    for i in range(len(y_pred)):\n      # decode the output of the network\n      boxes += decode_netout(y_pred[i][0], anchors[i], class_threshold, input_h, input_w)\n\n    correct_yolo_boxes(boxes, image_h, image_w, input_h, input_w)\n\n    # suppress non-maximal boxes\n    do_nms(boxes, 0.5)\n\n    # get the details of the detected objects\n    v_boxes, v_labels, v_scores = get_boxes(boxes, labels, class_threshold)\n\n\n    # summarize what we found\n    for i in range(len(v_boxes)):\n        print(v_labels[i], v_scores[i])\n\n    draw_boxes(image_path, v_boxes, v_labels, v_scores)\n    \ndetect_objects(image_path)","c949b29f":"\n\nimage_path = '..\/input\/object-detection\/Training_set\/Training_set\/electric bus\/electric_bus-102.jpeg'\n\ndetect_objects(image_path)","b805d8a5":"\nimage_path = '..\/input\/fruit-images-for-object-detection\/train_zip\/train\/apple_17.jpg'\n\ndetect_objects(image_path)","556a68ac":"image_path = '..\/input\/object-detection\/object_det_beach.jpeg'\ndetect_objects(image_path)","bd9b00f3":"image_path = '..\/input\/object-detection\/object_det_mult.jpg'\ndetect_objects(image_path)","16634a53":"image_path = '..\/input\/object-detection\/object_det_traffic1.jpg'\ndetect_objects(image_path)","3ec254ee":"image_path = '..\/input\/object-detection\/object_det_traffic2.jpg'\ndetect_objects(image_path)","e0b9108f":"image_path = '..\/input\/object-detection\/object_det_wineglass.jpg'\ndetect_objects(image_path)","21a351b0":"<h3><center>1. Keras Model for YOLOv3<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nNext, we need to define a Keras model that has the right number and type of layers to match the downloaded model weights. The model architecture is called DarkNet and was originally loosely based on the VGG-16 model. make_yolov3_model() function is used to create the model for us, and the helper function _conv_block() that is used to create blocks of layers. We can now define the Keras model for YOLOv3.\n    <\/div>","26e3dab5":"<h3><center>3. Predict Image<\/center><\/h3>","72ab195b":"<h3>4.2. Correct Size of Bounding Boxes<\/h3>","3b3cea62":"<h3>4.3. Draw Image<\/h3>","2921c3b6":"<h3>4.1. Generate Bounding Boxes<\/h3>","35f83e55":"<h3><center>Object Detection<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n  Object detection is a task in computer vision that involves identifying the presence, location, and type of one or more objects in a given photograph. It is a challenging problem that involves building upon methods for object recognition (e.g. where are they), object localization (e.g. what are their extent), and object classification (e.g. what are they).  <br><br>\n<\/div>\n\n![image.png](attachment:image.png)\n\n<h3><center>YOLO for Object Detection<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    The You Only Look Once, or YOLO, family of models are a series of end-to-end deep learning models designed for fast object detection.<br>\n    The approach involves a single deep convolutional neural network (originally a version of GoogLeNet, later updated and called DarkNet based on VGG) that splits the input into a grid of cells and each cell directly predicts a bounding box and object classification. The result is a large number of candidate bounding boxes that are consolidated into a final prediction by a post-processing step.<br><br>\n    There are three main variations of the approach; they are YOLOv1, YOLOv2, and YOLOv3. The first version proposed the general architecture, whereas the second version refined the design and made use of predefined anchor boxes to improve bounding box proposal, and version three further refined the model architecture and training process. Although the accuracy of the models is close but not as good as Region-Based Convolutional Neural Networks (R-CNNs), they are popular for object detection because of their detection speed, often demonstrated in real-time on video or with camera feed input.\n    <br><br>\n    <\/div>\n<h3><center>Object Detection with YOLOv3<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nThe keras-yolo3 project provides a lot of capability for using YOLOv3 models, including object detection, transfer learning, and training new models from scratch. In this kernel, we will use a pre-trained model to perform object detection on an unseen photograph. \n    <\/div>","abf81874":"<h3><center>4. Interpret the Results<\/center><\/h3>","d7936229":"<div style=\"font-family:verdana; word-spacing:1.9px;\">\nRunning the example returns a list of three NumPy arrays, the shape of which is displayed as output. These arrays predict both the bounding boxes and class labels but are encoded. They must be interpreted.\n    <\/div>","94954f9a":"<h3><center>2. Define Model & load Weights<\/center><\/h3>"}}