{"cell_type":{"9b35451a":"code","145dfbe7":"code","819dfd9b":"code","9eec024b":"code","842063ea":"code","02401451":"code","e6eab378":"code","0eab0089":"code","9eb04385":"code","f801008b":"code","d6cd6fe8":"code","b8460911":"code","809d25fd":"code","2561f5b2":"code","9be985ca":"code","c27e4e14":"code","a527989d":"code","7924c739":"code","e3c2c0f8":"code","47bb894c":"code","b24c5c0f":"code","4ae3bfc3":"code","14cd3d64":"code","c714d5f4":"code","4921bdf6":"code","1225b8bc":"code","044c5d66":"code","7470e3eb":"code","9af4c34d":"code","10706bb5":"code","6ca427ca":"code","1ae2e9eb":"code","170771d0":"code","bb2d0266":"code","28541eb4":"code","492208dc":"code","e60d5142":"code","65324095":"code","b0b525d6":"code","7744caeb":"code","afc418ea":"code","b9c360ec":"code","2269cb1c":"code","0950e175":"code","2ec3d520":"code","069a1356":"code","6567e6bf":"code","4247beea":"code","c4021bd9":"code","5bf20275":"markdown","b26c2ac5":"markdown","78d5be02":"markdown","caf68918":"markdown","83845285":"markdown","16bf3c5a":"markdown"},"source":{"9b35451a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D\nfrom tensorflow.keras.optimizers import Adam\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler    # in order to scale data\nfrom sklearn.metrics import classification_report,accuracy_score\n\n\nimport warnings as wr\nwr.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","145dfbe7":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndata.head(5)","819dfd9b":"data.shape","9eec024b":"data.isnull().sum()","842063ea":"data.info()","02401451":"data.Class.values","e6eab378":"data.Class.value_counts()","0eab0089":"plt.figure(figsize = (6,5))\nsns.countplot(data.Class, color = \"orange\")\nplt.show()","9eb04385":"data.hist(figsize=(30,30))\nplt.show()","f801008b":"fraud = data[data.Class == 1]","d6cd6fe8":"fraud             # Each row with class = 1","b8460911":"non_fraud = data[data.Class == 0]","809d25fd":"non_fraud           # Each row with class = 0","2561f5b2":"print(\"Shape of fraud data:\", fraud.shape)\nprint(\"Shape of non-fraus data:\", non_fraud.shape)","9be985ca":"nan_fraud_balanced = non_fraud.sample(4000)","c27e4e14":"nan_fraud_balanced","a527989d":"balanced_data = fraud.append(nan_fraud_balanced, ignore_index = True)","7924c739":"balanced_data     # 492 of them Class = 1 (fraud), 492 of them Class = 0 (nan_fraud)","e3c2c0f8":"balanced_data.Class.value_counts()","47bb894c":"x = balanced_data.drop(\"Class\", axis = 1)\nx                                           # dataset without Class column","b24c5c0f":"y = balanced_data.Class\ny","4ae3bfc3":"plt.figure(figsize = (6,5))\nsns.countplot(y, palette=\"Set2\")\nplt.show()","14cd3d64":"xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state = 42)","c714d5f4":"xtrain.shape","4921bdf6":"xtest.shape","1225b8bc":"scaler = StandardScaler()","044c5d66":"scaled_xtrain = scaler.fit_transform(xtrain)\nscaled_xtest = scaler.fit_transform(xtest)","7470e3eb":"scaled_xtrain","9af4c34d":"type(scaled_xtrain)","10706bb5":"scaled_xtest","6ca427ca":"type(scaled_xtest)","1ae2e9eb":"print(scaled_xtrain.shape)\nprint(scaled_xtest.shape)","170771d0":"print(ytrain.shape)\nprint(ytest.shape)","bb2d0266":"190820+93987 # Total dataset rows","28541eb4":"scaled_xtrain3d = scaled_xtrain.reshape(scaled_xtrain.shape[0],scaled_xtrain.shape[1],1)\nscaled_xtest3d = scaled_xtest.reshape(scaled_xtest.shape[0],scaled_xtest.shape[1],1)\n\nscaled_xtrain3d.shape, scaled_xtest3d.shape","492208dc":"# First Layer:\n\ncnn = Sequential()\ncnn.add(Conv1D(32, 2, activation = \"relu\", input_shape = (30,1)))\ncnn.add(Dropout(0.1))","e60d5142":"# Second Layer:\n\ncnn.add(BatchNormalization()) # Batch normalization is a technique for training very deep neural networks \n                               # that standardizes the inputs to a layer for each mini-batch. This \n                               # has the effect of stabilizing the learning process and dramatically\n                               # reducing the number of training epochs required to train deep networks\n\ncnn.add(Conv1D(64, 2, activation = \"relu\"))\ncnn.add(Dropout(0.2))          # prevents over-fitting (randomly remove some neurons)","65324095":"# Flattening Layer:\n\ncnn.add(Flatten())\ncnn.add(Dropout(0.4))\ncnn.add(Dense(64, activation = \"relu\"))\ncnn.add(Dropout(0.5))","b0b525d6":"# Last Layer:\n\ncnn.add(Dense(1, activation = \"sigmoid\"))","7744caeb":"cnn.summary()","afc418ea":"from keras.utils import plot_model\nplot_model(cnn)","b9c360ec":"cnn.compile(optimizer = Adam(lr=0.0001), loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","2269cb1c":"history = cnn.fit(scaled_xtrain3d, ytrain, epochs = 20, validation_data=(scaled_xtest3d, ytest), verbose=1)","0950e175":"fig, ax1 = plt.subplots(figsize= (10, 5))\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","2ec3d520":"fig, ax1 = plt.subplots(figsize= (10, 5))\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Model loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Train\", \"Test\"], loc = \"upper left\")\nplt.show()","069a1356":"from sklearn.metrics import confusion_matrix\ncnn_predictions = cnn.predict_classes(scaled_xtest3d)\nconfusion_matrix = confusion_matrix(ytest, cnn_predictions)\nsns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cbar = False)\nplt.title(\"CNN Confusion Matrix\")\nplt.show()","6567e6bf":"accuracy_score(ytest, cnn_predictions)","4247beea":"from sklearn.metrics import precision_recall_fscore_support as score","c4021bd9":"precision, recall, fscore, support = score(ytest, cnn_predictions)\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","5bf20275":"# Balancing the Dataset","b26c2ac5":"# Standardation","78d5be02":"# Training and Testing Part","caf68918":"* 492 frauds out of 284,807 transactions\n* features V1 - V28 are a result of the PCA transformation and are simply numerical representations\n* \"Amount\" is the value in dollars of the transaction\n* \"Time\" variable is the amount of time that passed from the time when the first transaction took place.\n* Fraud = 1 , Not Fraud = 0\n","83845285":"# 3D Format","16bf3c5a":"# CNN Implementation"}}