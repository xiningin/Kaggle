{"cell_type":{"2ed9ae26":"code","b89bbf34":"code","f2f7c512":"code","a735974e":"code","b1a8a0c8":"code","fa80cc0f":"code","bbea4c05":"code","7e202d9b":"code","34023584":"code","772f1f47":"code","d3e0c377":"code","1a47291b":"code","81f2521f":"code","4334452f":"code","04ea1b43":"code","62b511ed":"code","353ae0ab":"code","ced79c06":"code","610dc6be":"code","7d5ce2a6":"code","d0aa3c89":"code","19505c7e":"code","572e4ab5":"code","57b66f80":"code","fc4192b0":"code","1da47c4f":"code","bb8eba7d":"code","9248a88d":"code","a9a71253":"code","99ad2de3":"code","d825ec9a":"code","c5d4f20f":"code","463f17b6":"code","5864d254":"code","b5e02e51":"code","3b4cc090":"code","998fc2b1":"code","2337baf7":"code","6438923a":"code","9d48f6f1":"code","0d6a8244":"code","699bf066":"code","5f0d4997":"code","3463ba22":"code","feb972b1":"code","53b3d933":"code","1fe5156a":"code","88eeabb1":"code","e5147169":"code","05f439ad":"code","8ac7cad1":"code","481c431a":"code","8be3d2de":"code","f43d4f41":"code","cd06343f":"code","c0a914a4":"code","5d33ac8c":"code","930df155":"code","7837b62e":"code","e444da6a":"code","aefff02a":"code","1dfce0d6":"code","e852a6e0":"code","e0483340":"code","06f6e0c1":"code","6c4b50e5":"code","7df59918":"code","6ed0532c":"code","28bd4f5b":"code","43c47e9f":"code","9fdb26c8":"code","c4d65c2b":"code","5ced5c91":"code","b8e47203":"code","7119c9db":"code","aa836d90":"code","9dcbff0e":"code","53e227e3":"code","be4727b5":"code","01f4dc1d":"code","096c10d4":"code","e040f2ef":"code","4d859007":"code","71a26581":"code","7ec84803":"code","a7dc2e9c":"code","69aebb1b":"code","21d98234":"code","b5c96123":"code","16b8f5b9":"code","bf3edf5d":"code","670e804a":"code","823efd51":"code","806380f3":"code","682bb2a0":"code","b55a1958":"code","cc948183":"code","f51f2ac2":"code","806f9d7e":"code","0d1ab1da":"code","edfb9ea6":"code","2c3457c6":"code","62413d04":"code","b0113c75":"code","b35b9a0e":"code","3694b8f5":"code","a24aaeab":"code","a5120fbc":"code","cd0a3c75":"code","592572d6":"code","bbf91f80":"code","2ada74bb":"code","92707a31":"code","1ab69de4":"code","00dda698":"code","b60f457f":"code","a4bae35b":"code","40d67699":"code","440ddc75":"code","e2aa8ef0":"code","c90cf1dc":"code","ed89269e":"code","8886c092":"code","8f875751":"code","c0ba48d1":"code","6a043fce":"code","aaf576e4":"markdown","3a807d22":"markdown","6f383c5a":"markdown","f4245187":"markdown","04dbac3c":"markdown","3b9d38df":"markdown","a37958d8":"markdown","38e55f06":"markdown","3e8b91de":"markdown","6979f048":"markdown","7f3ec387":"markdown","1e5b736c":"markdown","22a6c418":"markdown","069acb41":"markdown","bb8fb214":"markdown","00ec2985":"markdown","e2de1f18":"markdown","68899bae":"markdown","274a11eb":"markdown","73cc3874":"markdown","083ed4ab":"markdown","50516090":"markdown","9bd60ba2":"markdown","d1b8f754":"markdown","996685d5":"markdown","05c1f935":"markdown","e034b449":"markdown","70e19139":"markdown","99b56318":"markdown","97f18d46":"markdown"},"source":{"2ed9ae26":"#1.1;- Importing Liabraries\nimport pandas as pd\nimport pylab as pl\nimport numpy as np\nfrom math import sqrt\nfrom mpl_toolkits.mplot3d import Axes3D \nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#ML Liabraries\nimport itertools\nimport random \nimport scipy\nimport pylab\nimport scipy.cluster.hierarchy\nimport scipy.optimize as opt\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn import svm \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom scipy.optimize import curve_fit\nfrom scipy import ndimage \nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom scipy.cluster.hierarchy import fcluster","b89bbf34":"# Data Loading preprocessing\ndf = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/CO2_emission.csv\")\npd.set_option('display.max_columns', None)\ndf.head()","f2f7c512":"# Summarize the data\ndf.describe()","a735974e":"# Exploring few features in detail\ncdf = df[['Engine_Size','Cylinders','Fuel_Consumption_comb(L\/100km)','CO2_Emissions']]\ncdf.head(9)","b1a8a0c8":"# plotting each of these features\nviz = cdf[['Cylinders','Engine_Size','CO2_Emissions','Fuel_Consumption_comb(L\/100km)']]\nviz.hist()\nplt.show()","fa80cc0f":"# Fuel Consumption comb(L\/100km) vs CO2_Emissions\nplt.scatter(cdf.Cylinders , cdf.CO2_Emissions,  color='blue')\nplt.xlabel(\"Fuel Consumption comb(L\/100km)\")\nplt.ylabel(\"CO2_Emissions\")\nplt.show()","bbea4c05":"# Engine size vs Emission\nplt.scatter(cdf.Engine_Size, cdf.CO2_Emissions,  color='blue')\nplt.xlabel(\"Engine size\")\nplt.ylabel(\"Emission\")\nplt.show()","7e202d9b":"# Splitting our dataset into train and test sets.(80% , 20%)\nmsk = np.random.rand(len(df)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]","34023584":"# Modeling\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['Engine_Size']])\ntrain_y = np.asanyarray(train[['CO2_Emissions']])\nregr.fit (train_x, train_y)","772f1f47":"# Plotting the fit line over the data\nplt.scatter(train.Engine_Size, train.CO2_Emissions,  color='blue')\nplt.plot(train_x, regr.coef_[0][0]*train_x + regr.intercept_[0], '-r')\nplt.xlabel(\"Engine size\")\nplt.ylabel(\"Emission\")","d3e0c377":"# Evaluation\n\ntest_x = np.asanyarray(test[['Engine_Size']])\ntest_y = np.asanyarray(test[['CO2_Emissions']])\ntest_y_ = regr.predict(test_x)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y , test_y_) )\n# variance score 1 is perfect prediction\nprint('Variance score: %.2f' % regr.score(train_x, train_y))","1a47291b":"# Modeling\nregr1 = linear_model.LinearRegression()\nx = np.asanyarray(train[['Engine_Size','Cylinders','Fuel_Consumption_comb(L\/100km)']])\ny = np.asanyarray(train[['CO2_Emissions']])\nregr1.fit (x, y)","81f2521f":"# Predictions\ny_hat= regr1.predict(test[['Engine_Size','Cylinders','Fuel_Consumption_comb(L\/100km)']])\nx = np.asanyarray(test[['Engine_Size','Cylinders','Fuel_Consumption_comb(L\/100km)']])\ny = np.asanyarray(test[['CO2_Emissions']])\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((y_hat - y) ** 2))\n\nprint('Variance score: %.2f' % regr1.score(x, y))","4334452f":"# Exploring another dataset \"china_gdp\"\nchina_df = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/china_gdp.csv\")\nchina_df.head()","04ea1b43":"# Plotting the dataset\nplt.figure(figsize=(8,5))\nx_data, y_data = (china_df[\"Year\"].values, china_df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()","62b511ed":"#Building the model\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 \/ (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n\n# Lets look at a sample sigmoid line that might fit with the data\nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')","353ae0ab":"# Lets normalize our data\nxdata =x_data\/max(x_data)\nydata =y_data\/max(y_data)","ced79c06":"#Finding the best parameters for our fit line\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\n#print the final parameters\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))","610dc6be":"#Plotting our resulting regression model\nx = np.linspace(1960, 2015, 55)\nx = x\/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()","7d5ce2a6":"#Evaluating the accuracy of our model\n\n# split data into train\/test\nmsk = np.random.rand(len(china_df)) < 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )","d0aa3c89":"# Appling Ploynomial regression on co2_emmision dataset\ndf = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/CO2_emission.csv\")\n# take a look at the dataset\npoly_cdf = df[['Engine_Size','Cylinders','Fuel_Consumption_comb(L\/100km)','CO2_Emissions']]\npoly_cdf.head()","19505c7e":"# Creating train and test dataset\nmsk = np.random.rand(len(poly_cdf)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]\n\ntrain_x = np.asanyarray(train[['Engine_Size']])\ntrain_y = np.asanyarray(train[['CO2_Emissions']])\n\ntest_x = np.asanyarray(test[['Engine_Size']])\ntest_y = np.asanyarray(test[['CO2_Emissions']])\n\n\npoly_regr = PolynomialFeatures(degree=2)\ntrain_x_poly = poly_regr.fit_transform(train_x)\ntrain_x_poly","572e4ab5":"clf = linear_model.LinearRegression()\ntrain_y_ = clf.fit(train_x_poly, train_y)\n\nplt.scatter(train.Engine_Size, train.CO2_Emissions,  color='blue')\nXX = np.arange(0.0, 10.0, 0.1)\nyy = clf.intercept_[0]+ clf.coef_[0][1]*XX+ clf.coef_[0][2]*np.power(XX, 2)\nplt.plot(XX, yy, '-r' )\nplt.xlabel(\"Engine size\")\nplt.ylabel(\"Emission\")","57b66f80":"# Applying polynomial regression with this dataset but this time with degree three (cubic).\npoly3 = PolynomialFeatures(degree=3)\ntrain_x_poly3 = poly3.fit_transform(train_x)\nclf3 = linear_model.LinearRegression()\ntrain_y3_ = clf3.fit(train_x_poly3, train_y)\n\n# The coefficients\nprint ('Coefficients: ', clf3.coef_)\nprint ('Intercept: ',clf3.intercept_)\nplt.scatter(train.Engine_Size, train.CO2_Emissions,  color='blue')\nXX = np.arange(0.0, 10.0, 0.1)\nyy = clf3.intercept_[0]+ clf3.coef_[0][1]*XX + clf3.coef_[0][2]*np.power(XX, 2) + clf3.coef_[0][3]*np.power(XX, 3)\nplt.plot(XX, yy, '-r' )\nplt.xlabel(\"Engine size\")\nplt.ylabel(\"Emission\")\ntest_x_poly3 = poly3.fit_transform(test_x)\ntest_y3_ = clf3.predict(test_x_poly3)\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y3_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y3_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y,test_y3_ ) )","fc4192b0":"#Evaluation\ntest_x_poly = poly_regr.fit_transform(test_x)\ntest_y_ = clf.predict(test_x_poly)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y,test_y_ ) )","1da47c4f":"#Data Loading & preprocessing \n\n# Importing another dataset\ntel_df = pd.read_csv('..\/input\/top-10-machine-learning-datasets\/telecom_cus.csv')\ntel_df.head()","bb8eba7d":"# Let\u2019s see how many of each class is in our data set\ntel_df['custcat'].value_counts()","9248a88d":"# You can easily explore your data using visualization techniques\ntel_df.hist(column='income', bins=50)","a9a71253":"tel_df.columns","99ad2de3":"#To use scikit-learn library, we have to convert the Pandas data frame to a Numpy array:\nX = tel_df[['region', 'tenure','age', 'marital', 'address', 'income', 'ed', 'employ','retire', 'gender', 'reside']] .values  #.astype(float)\nX[0:5]\n\ny = tel_df['custcat'].values\ny[0:5]\n\n# Data Normalization\nX = preprocessing.StandardScaler().fit(X).transform(X.astype(float))\nX[0:5]\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","d825ec9a":"# Let's start the algorithm with k=4 for now:\nk = 4\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","c5d4f20f":"#Prediction\nyhat = neigh.predict(X_test)\nyhat[0:5]","463f17b6":"# Accuracy Evaluation\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat))","5864d254":"# Let's start the algorithm with k=6 for now:\nk = 6\nneigh6 = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nyhat6 = neigh6.predict(X_test)\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh6.predict(X_train)))\nprint(\"Test set Accuracy: \", metrics.accuracy_score(y_test, yhat6))","b5e02e51":"# We can calculate the accuracy of KNN for different values of k.\nKs = 10\nmean_acc = np.zeros((Ks-1))\nstd_acc = np.zeros((Ks-1))\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\n    \n    std_acc[n-1]=np.std(yhat==y_test)\/np.sqrt(yhat.shape[0])\n\nmean_acc\n\n\n# Plot the model accuracy for a different number of neighbors and best value for K\nplt.plot(range(1,10),mean_acc,'g')\nplt.fill_between(range(1,10),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\nplt.fill_between(range(1,10),mean_acc - 3 * std_acc,mean_acc + 3 * std_acc, alpha=0.10,color=\"green\")\nplt.legend(('Accuracy ', '+\/- 1xstd','+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","3b4cc090":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","998fc2b1":"# Exploring another dataset\ndrug_df = pd.read_csv('..\/input\/top-10-machine-learning-datasets\/drug.csv')\ndrug_df.head()","2337baf7":"drug_df.shape","6438923a":"X = drug_df[['age', 'sex', 'bp', 'cholesterol', 'Na_to_K']].values\nX[0:5]","9d48f6f1":"# Converting categorical features to numerical values using pandas.get_dummies()\n#from sklearn import preprocessing\nle_sex = preprocessing.LabelEncoder()\nle_sex.fit(['F','M'])\nX[:,1] = le_sex.transform(X[:,1]) \n\n\nle_BP = preprocessing.LabelEncoder()\nle_BP.fit([ 'LOW', 'NORMAL', 'HIGH'])\nX[:,2] = le_BP.transform(X[:,2])\n\n\nle_Chol = preprocessing.LabelEncoder()\nle_Chol.fit([ 'NORMAL', 'HIGH'])\nX[:,3] = le_Chol.transform(X[:,3]) \n\nX[0:5]","0d6a8244":"# Filling target variable\ny = drug_df[\"drug\"]\ny[0:5]","699bf066":"# Train_test_split\nX_trainset, X_testset, y_trainset, y_testset = train_test_split(X, y, test_size=0.3, random_state=3)","5f0d4997":"# Printing the shape of X_trainset and y_trainset to Ensure that the dimensions match.\nprint('Shape of X training set {}'.format(X_trainset.shape),'&',' Size of Y training set {}'.format(y_trainset.shape))","3463ba22":"# Modeling\ndrugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\n# it shows the default parameters\ndrugTree \n\ndrugTree.fit(X_trainset,y_trainset)","feb972b1":"# Prediction\npredTree = drugTree.predict(X_testset)","53b3d933":"# Visually comparing the predictions to the actual values.\nprint (predTree [0:5])\nprint (y_testset [0:5])","1fe5156a":"# Evaluation\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_testset, predTree))","88eeabb1":"# Importing another dataset\nchurn_df = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/churn_Data.csv\")\nchurn_df.head()","e5147169":"#Data Pre-processing\nchurn_df.shape","05f439ad":"# Defining X and Y for our dataset\nX = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n\ny = np.asarray(churn_df['churn'])","8ac7cad1":"# Normalize the dataset\nX = preprocessing.StandardScaler().fit(X).transform(X)\n\n# Spliting our dataset into train and test set:\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","481c431a":"# Modeling\n\n#Fitting our model\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nLR","8be3d2de":"yhat_prob = LR.predict_proba(X_test)\n\nyhat = LR.predict(X_test)\nyhat","f43d4f41":"# 1.jaccard index\njaccard_score(y_test, yhat,pos_label=0)","cd06343f":"# 2.Confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nprint(confusion_matrix(y_test, yhat, labels=[1,0]))","c0a914a4":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\nnp.set_printoptions(precision=2)\n\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')","5d33ac8c":"# Based on the count of each section, we can calculate precision and recall of each label\nprint (classification_report(y_test, yhat))","930df155":"# 3.Log Loss\nlog_loss(y_test, yhat_prob)","7837b62e":"cell_df = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/cell_samples.csv\")\ncell_df.head()","e444da6a":"ax = cell_df[cell_df['Class'] == 4][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='DarkBlue', label='malignant');\ncell_df[cell_df['Class'] == 2][0:50].plot(kind='scatter', x='Clump', y='UnifSize', color='Yellow', label='benign', ax=ax);\nplt.show()","aefff02a":"# Data pre-processing\ncell_df.dtypes","1dfce0d6":"cell_df = cell_df[pd.to_numeric(cell_df['BareNuc'], errors='coerce').notnull()]\ncell_df['BareNuc'] = cell_df['BareNuc'].astype('int')","e852a6e0":"feature_df = cell_df[['Clump', 'UnifSize', 'UnifShape', 'MargAdh', 'SingEpiSize', 'BareNuc', 'BlandChrom', 'NormNucl', 'Mit']]\nX = np.asarray(feature_df)\nX[0:5]","e0483340":"cell_df['Class'] = cell_df['Class'].astype('int')\ny = np.asarray(cell_df['Class'])\ny [0:5]","06f6e0c1":"# Split our dataset into train and test set\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","6c4b50e5":"# Modeling\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train)","7df59918":"# After being fitted, the model can then be used to predict new values\nyhat = clf.predict(X_test)\nyhat [0:5]","6ed0532c":"# Evaluation\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","28bd4f5b":"# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, yhat, labels=[2,4])\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, yhat))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Benign(2)','Malignant(4)'],normalize= False,  title='Confusion matrix')","43c47e9f":"f1_score(y_test, yhat, average='weighted')","9fdb26c8":"jaccard_score(y_test, yhat,pos_label=2)","c4d65c2b":"# Importing another dataset\ncust_df = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/cust_segmentation_Data.csv\")\ncust_df.head()","5ced5c91":"# Normalizing over the standard deviation\nX = cust_df.values[:,1:]\nX = np.nan_to_num(X)\nClus_dataSet = StandardScaler().fit_transform(X)\nClus_dataSet","b8e47203":"# Modeling\nclusterNum = 3\nk_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\nk_means.fit(X)\nlabels = k_means.labels_","7119c9db":"# let's look at the distribution of customers based on their age and income:\narea = np.pi * ( X[:, 1])**2  \nplt.scatter(X[:, 0], X[:, 3], s=area, c=labels.astype(np.float), alpha=0.5)\nplt.xlabel('Age', fontsize=18)\nplt.ylabel('Income', fontsize=16)\n\nplt.show()","aa836d90":"# Exploring in 3 Dimentials \nfig = plt.figure(1, figsize=(8, 6))\nplt.clf()\nax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\nplt.cla()\nax.set_xlabel('Education')\nax.set_ylabel('Age')\nax.set_zlabel('Income')\n\nax.scatter(X[:, 1], X[:, 0], X[:, 3], c= labels.astype(np.float))\n","9dcbff0e":"# Importing another dataset\ndf1 = pd.read_csv(\"..\/input\/top-10-machine-learning-datasets\/cars_clus.csv\")\npd.set_option('display.max_columns', None)\ndf1","53e227e3":"# Data Preprocessing\nprint (\"Shape of dataset before cleaning: \", df1.size)\ndf1[[ 'sales', 'resale', 'type', 'price', 'engine_s',\n       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n       'mpg', 'lnsales']] = df1[['sales', 'resale', 'type', 'price', 'engine_s',\n       'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap',\n       'mpg', 'lnsales']].apply(pd.to_numeric, errors='coerce')\ndf1 = df1.dropna()\ndf1 = df1.reset_index(drop=True)\nprint (\"Shape of dataset after cleaning: \", df1.size)\ndf1.head(5)","be4727b5":"# Feature selection\nfeatureset = df1[['engine_s',  'horsepow', 'wheelbas', 'width', 'length', 'curb_wgt', 'fuel_cap', 'mpg']]","01f4dc1d":"# Normalization\nx = featureset.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nfeature_mtx = min_max_scaler.fit_transform(x)\nfeature_mtx [0:5]","096c10d4":"# Clustering using Scipy\nleng = feature_mtx.shape[0]\nD = scipy.zeros([leng,leng])\nfor i in range(leng):\n    for j in range(leng):\n        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])\nD","e040f2ef":"# Normalization\nZ = hierarchy.linkage(D, 'complete')\nmax_d = 3\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters","4d859007":"# You can determine the number of clusters directly\nk = 5\nclusters = fcluster(Z, k, criterion='maxclust')\nclusters","71a26581":"# plotting the dendrogram:\nfig = pylab.figure(figsize=(18,50))\ndef llf(id):\n    return '[%s %s %s]' % (df1['manufact'][id], df1['model'][id], int(float(df1['type'][id])) )\n    \ndendro = hierarchy.dendrogram(Z,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')","7ec84803":"dist_matrix = euclidean_distances(feature_mtx,feature_mtx) \nprint(dist_matrix)","a7dc2e9c":"Z_using_dist_matrix = hierarchy.linkage(dist_matrix, 'complete')","69aebb1b":"#we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset.\nagglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\nagglom.fit(dist_matrix)\n\nagglom.labels_","21d98234":"# We can add a new field to our dataframe to show the cluster of each row:\ndf1['cluster_'] = agglom.labels_\ndf1.head()","b5c96123":"n_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\n\n# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(16,14))\n\nfor color, label in zip(colors, cluster_labels):\n    subset = df1[df1.cluster_ == label]\n    for i in subset.index:\n            plt.text(subset.horsepow[i], subset.mpg[i],str(subset['model'][i]), rotation=25) \n    plt.scatter(subset.horsepow, subset.mpg, s= subset.price*10, c=color, label='cluster'+str(label),alpha=0.5)\n#    plt.scatter(subset.horsepow, subset.mpg)\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","16b8f5b9":"df1.groupby(['cluster_','type'])['cluster_'].count()","bf3edf5d":"# Now we can look at the characteristics of each cluster\nagg_cars = df1.groupby(['cluster_','type'])['horsepow','engine_s','mpg','price'].mean()\nagg_cars","670e804a":"plt.figure(figsize=(16,10))\nfor color, label in zip(colors, cluster_labels):\n    subset = agg_cars.loc[(label,),]\n    for i in subset.index:\n        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'type='+str(int(i)) + ', price='+str(int(subset.loc[i][3]))+'k')\n    plt.scatter(subset.horsepow, subset.mpg, s=subset.price*20, c=color, label='cluster'+str(label))\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('horsepow')\nplt.ylabel('mpg')","823efd51":"# Importing another dataset\nmovies_df = pd.read_csv('..\/input\/top-10-machine-learning-datasets\/movies.csv')\npd.set_option('display.max_columns', None)\nmovies_df","806380f3":"# Importing another dataset\nrating_df = pd.read_csv('..\/input\/top-10-machine-learning-datasets\/ratings.csv')\npd.set_option('display.max_columns', None)\nrating_df","682bb2a0":"movies_df.rename(columns = {'Title':'title'}, inplace = True)\nmovies_df.rename(columns = {'Genres':'genres'}, inplace = True)\nmovies_df","b55a1958":"#Using regular expressions to find a year stored between parentheses\n#We specify the parantheses so we don't conflict with movies that have years in their titles\nmovies_df['year'] = movies_df.title.str.extract('(\\(\\d\\d\\d\\d\\))',expand=False)\n#Removing the parentheses\nmovies_df['year'] = movies_df.year.str.extract('(\\d\\d\\d\\d)',expand=False)\n#Removing the years from the 'title' column\nmovies_df['title'] = movies_df.title.str.replace('(\\(\\d\\d\\d\\d\\))', '')\n#Applying the strip function to get rid of any ending whitespace characters that may have appeared\nmovies_df['title'] = movies_df['title'].apply(lambda x: x.strip())\nmovies_df.head()","cc948183":"#Every genre is separated by a | so we simply have to call the split function on |\nmovies_df['genres'] = movies_df.genres.str.split('|')\nmovies_df.head()","f51f2ac2":"#Copying the movie dataframe into a new one since we won't need to use the genre information in our first case.\nmoviesWithGenres_df = movies_df.copy()\n\n#For every row in the dataframe, iterate through the list of genres and place a 1 into the corresponding column\nfor index, row in movies_df.iterrows():\n    for genre in row['genres']:\n        moviesWithGenres_df.at[index, genre] = 1\n#Filling in the NaN values with 0 to show that a movie doesn't have that column's genre\nmoviesWithGenres_df = moviesWithGenres_df.fillna(0)\nmoviesWithGenres_df.head()","806f9d7e":"#Drop removes a specified row or column from a dataframe\nrating_df = rating_df.drop('Timestamp', 1)\nrating_df.head()","0d1ab1da":"userInput = [\n            {'title':'Breakfast Club, The', 'rating':5},\n            {'title':'Toy Story', 'rating':3.5},\n            {'title':'Jumanji', 'rating':2},\n            {'title':\"Pulp Fiction\", 'rating':5},\n            {'title':'Akira', 'rating':4.5}\n         ] \ninputMovies = pd.DataFrame(userInput)\ninputMovies","edfb9ea6":"#Filtering out the movies by title\ninputId = movies_df[movies_df['title'].isin(inputMovies['title'].tolist())]\n#Then merging it so we can get the movieId. It's implicitly merging it by title.\ninputMovies = pd.merge(inputId, inputMovies)\n#Dropping information we won't use from the input dataframe\ninputMovies = inputMovies.drop('genres', 1).drop('year', 1)\n#Final input dataframe\n#If a movie you added in above isn't here, then it might not be in the original \n#dataframe or it might spelled differently, please check capitalisation.\ninputMovies","2c3457c6":"#Filtering out the movies from the input\nuserMovies = moviesWithGenres_df[moviesWithGenres_df['Id'].isin(inputMovies['Id'].tolist())]\nuserMovies","62413d04":"#Resetting the index to avoid future issues\nuserMovies = userMovies.reset_index(drop=True)\n#Dropping unnecessary issues due to save memory and to avoid issues\nuserGenreTable = userMovies.drop('Id', 1).drop('title', 1).drop('genres', 1).drop('year', 1)\nuserGenreTable","b0113c75":"inputMovies['rating']","b35b9a0e":"#Dot produt to get weights\nuserProfile = userGenreTable.transpose().dot(inputMovies['rating'])\n#The user profile\nuserProfile","3694b8f5":"#Now let's get the genres of every movie in our original dataframe\ngenreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['Id'])\n#And drop the unnecessary information\ngenreTable = genreTable.drop('Id', 1).drop('title', 1).drop('genres', 1).drop('year', 1)\ngenreTable.head()","a24aaeab":"genreTable.shape","a5120fbc":"#Multiply the genres by the weights and then take the weighted average\nrecommendationTable_df = ((genreTable*userProfile).sum(axis=1))\/(userProfile.sum())\nrecommendationTable_df.head()","cd0a3c75":"#Sort our recommendations in descending order\nrecommendationTable_df = recommendationTable_df.sort_values(ascending=False)\n#Just a peek at the values\nrecommendationTable_df.head()","592572d6":"#The final recommendation table\nmovies_df.loc[movies_df['Id'].isin(recommendationTable_df.head(20).keys())]","bbf91f80":"#Filtering out users that have watched movies that the input has watched and storing it\nuserSubset = rating_df[rating_df['MovieId'].isin(inputMovies['Id'].tolist())]\nuserSubset.head()","2ada74bb":"#Groupby creates several sub dataframes where they all have the same value in the column specified as the parameter\nuserSubsetGroup = userSubset.groupby(['Id'])","92707a31":"#Let's look at one of the users, e.g. the one with userID=5.\nuserSubsetGroup.get_group(5)","1ab69de4":"#Sorting it so users with movie most in common with the input will have priority\nuserSubsetGroup = sorted(userSubsetGroup,  key=lambda x: len(x[1]), reverse=True)\nuserSubsetGroup[0:3]","00dda698":"# This limit is imposed because we don't want to waste too much time going through every single user.\nuserSubsetGroup = userSubsetGroup[0:100]","b60f457f":"#Store the Pearson Correlation in a dictionary, where the key is the user Id and the value is the coefficient\npearsonCorrelationDict = {}\n\n#For every user group in our subset\nfor name, group in userSubsetGroup:\n    #Let's start by sorting the input and current user group so the values aren't mixed up later on\n    group = group.sort_values(by='Id')\n    inputMovies = inputMovies.sort_values(by='Id')\n    #Get the N for the formula\n    nRatings = len(group)\n    #Get the review scores for the movies that they both have in common\n    temp_df = inputMovies[inputMovies['Id'].isin(group['Id'].tolist())]\n    #And then store them in a temporary buffer variable in a list format to facilitate future calculations\n    tempRatingList = temp_df['rating'].tolist()\n    #Let's also put the current user group reviews in a list format\n    tempGroupList = group['Rating'].tolist()\n    #Now let's calculate the pearson correlation between two users, so called, x and y\n    Sxx = sum([i**2 for i in tempRatingList]) - pow(sum(tempRatingList),2)\/float(nRatings)\n    Syy = sum([i**2 for i in tempGroupList]) - pow(sum(tempGroupList),2)\/float(nRatings)\n    Sxy = sum( i*j for i, j in zip(tempRatingList, tempGroupList)) - sum(tempRatingList)*sum(tempGroupList)\/float(nRatings)\n    \n    #If the denominator is different than zero, then divide, else, 0 correlation.\n    if Sxx != 0 and Syy != 0:\n        pearsonCorrelationDict[name] = Sxy\/sqrt(Sxx*Syy)\n    else:\n        pearsonCorrelationDict[name] = 0","a4bae35b":"pearsonCorrelationDict.items()","40d67699":"pearsonDF = pd.DataFrame.from_dict(pearsonCorrelationDict, orient='index')\npearsonDF.columns = ['similarityIndex']\npearsonDF['userId'] = pearsonDF.index\npearsonDF.index = range(len(pearsonDF))\npearsonDF.head()","440ddc75":"topUsers=pearsonDF.sort_values(by='similarityIndex', ascending=False)[0:50]\ntopUsers.head()","e2aa8ef0":"# Now, let's start recommending movies to the input user.\n\n# Rating of selected users to all movies\ntopUsersRating=topUsers.merge(rating_df, left_on='userId', right_on='Id', how='inner')\ntopUsersRating.head()","c90cf1dc":"#Multiplies the similarity by the user's ratings\ntopUsersRating['weightedRating'] = topUsersRating['similarityIndex']*topUsersRating['Rating']\ntopUsersRating.head()","ed89269e":"#Applies a sum to the topUsers after grouping it up by userId\ntempTopUsersRating = topUsersRating.groupby('MovieId').sum()[['similarityIndex','weightedRating']]\ntempTopUsersRating.columns = ['sum_similarityIndex','sum_weightedRating']\ntempTopUsersRating.head()","8886c092":"#Creates an empty dataframe\nrecommendation_df = pd.DataFrame()\n#Now we take the weighted average\nrecommendation_df['weighted average recommendation score'] = tempTopUsersRating['sum_weightedRating']\/tempTopUsersRating['sum_similarityIndex']\nrecommendation_df['movieId'] = tempTopUsersRating.index\nrecommendation_df.head()","8f875751":"recommendation_df = recommendation_df.sort_values(by='weighted average recommendation score', ascending=False)\nrecommendation_df.head(10)","c0ba48d1":"movies_df.loc[movies_df['Id'].isin(recommendation_df.head(10)['movieId'].tolist())]","6a043fce":"# If you like this notebook, don't forget to upvote.","aaf576e4":"### It is obvious that we have 3 main clusters with the majority of vehicles in those.\n# Cars:\n\n### Cluster 1: with almost high mpg, and low in horsepower.\n\n### Cluster 2: with good mpg and horsepower, but higher price than average.\n\n### Cluster 3: with low mpg, high horsepower, highest price.\n\n# Trucks:\n\n### Cluster 1: with almost highest mpg among trucks, and lowest in horsepower and price.\n### Cluster 2: with almost low mpg and medium horsepower, but higher price than average.\n### Cluster 3: with good mpg and horsepower, low price.","3a807d22":"# 2.3:- Non-Linear Regression ","6f383c5a":"# 5.Recommender Systems","f4245187":"### k-means will partition your customers into mutually exclusive groups, for example, into 3 clusters.The customers in each cluster are similar to each other demographically.Now we can create a profile for each group, considering the common characteristics of each cluster. For example, the 3 clusters can be:\n## AFFLUENT, EDUCATED AND OLD AGED\n## MIDDLE AGED AND MIDDLE INCOME\n## YOUNG AND LOW INCOME","04dbac3c":"# 3.2:- Decision Trees","3b9d38df":"# 2.1:- Simple Regression Model","a37958d8":"#### As 0.0 Mean absolute error is good and we have 0.04 which is also very satisfied.\n#### MSE 0.0 means that your model is perfectly fit.","38e55f06":"## We can use any of the following Kerneling Type\n## 1.Linear\n## 2.Polynomial\n## 3.Radial basis function (RBF)\n## 4.Sigmoid","3e8b91de":"# 1.2:- About datasets\n## In this notebook,we are going to use following datsets in algorithms\n### 1.Dataset on CO2_emission (CO2_emission.csv)                      \n### 2.Dataset on china_gdp   (china_gdp.csv)  \n### 3.Dataset on Telecom_customer_segmentation  (telecom_cus.csv)    \n### 4.Dataset on set of patients suffered from the same illness (drug.csv) \n### 5.Dataset on  telecom_customer_churn  (churn_Data.csv)  \n### 6.Dataset on Cancer data (cell_samples.csv)       \n### 7.Dataset on customer segmentation (cust_segmentation_Data.csv) \n### 8. Dataset on  Vehicle data  (cars_clus.csv)     \n### 9. Dataset on  Movies data  (movies.csv)     \n### 10. Dataset on Ratings data  (ratings.csv)    ","6979f048":"#  **Table of Contents**\n\n# 1.Setup \n### 1.1:- Importing Liabraries\n### 1.2:- About datasets\n\n# 2.Regression\n### 2.1:- Simple Regression Model\n### 2.2:- Multiple Regression Model\n### 2.3:- Non-Linear Regression \n### 2.4:- Polynomial Regression\n\n# 3.Classification\n### 3.1:- K-Nearest Neighbors (K-NN)\n### 3.2:- Decision Trees\n### 3.3:- Logistic Regression\n### 3.4:- Support Vector Machines (SVM)\n\n# 4.Clustering\n### 4.1:- K-Means Clustering\n### 4.2:- Hierarchical Clustering \n\n# 5.Recommender Systems\n### 5.1:- Content based Recommendation Systems\n### 5.2:- Collaborative Filtering","7f3ec387":"### Advantages and Disadvantages of Collaborative Filtering\n## Advantages\n### Takes other user's ratings into consideration\n### Doesn't need to study or extract information from the recommended item\n### Adapts to the user's interests which might change over time\n## Disadvantages\n### Approximation function can be slow\n### There might be a low of amount of users to approximate\n### Privacy issues when trying to learn the user's preferences","1e5b736c":"# 3. Classification","22a6c418":"# 4. Clustering","069acb41":"# 3.3:- Logistic Regression","bb8fb214":"## Advantages and Disadvantages of Content-Based Filtering\n\n# Advantages\n### Learns user's preferences\n### Highly personalized for the user\n\n# Disadvantages\n### Doesn't take into account what others think of the item, so low quality item recommendations might happen\n### Extracting data is not always intuitive\n### Determining what characteristics of the item the user dislikes or likes is not always obvious","00ec2985":"# 3.1:- K-NN(Nearest Neighbors)","e2de1f18":"# 4.2:- Hierarchical Clustering - Agglomerative","68899bae":"# 5.2:- Collaborative Filtering","274a11eb":"## Data Preprocessing","73cc3874":"### Variance score 1 is perfect prediction and we got 0.94","083ed4ab":"## We can evaluate our model by using \n### 1.jaccard index\n### 2.Confusion matrix \n### 3.log loss","50516090":"### All three evaluation techniques shows that our model works very well","9bd60ba2":"# 5.1:- Content-Based recommendation system","d1b8f754":"# 2. Regression","996685d5":"# 3.4:- SVM (Support Vector Machines)","05c1f935":"# 4.1:- K-Means Clustering","e034b449":"#### As polynomial regression is considered to be a special case of traditional multiple linear regression.So, you can use the same mechanism as linear regression to solve such a problems.","70e19139":"# 2.4:- Polynomial Regression","99b56318":"# 2.2:- Multiple Regression Model","97f18d46":"### There are 2 types of vehicles in our dataset, \"truck\" (value of 1 in the type column) and \"car\" (value of 0 in the type column).  So, we use them to distinguish the classes, and summarize the cluster. First we count the number of cases in each group."}}