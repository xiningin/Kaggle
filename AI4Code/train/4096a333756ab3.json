{"cell_type":{"58cf8273":"code","4918a402":"code","349cc96d":"code","93d3cd7e":"code","1e7ea110":"code","a77bd4d7":"code","5b7a4b65":"code","39aaa051":"code","fe3407e2":"code","2f18ebcc":"code","5d2dd9b9":"code","3d07b004":"code","5770065c":"code","c0db2a3f":"code","56cfcb4b":"code","2d4e56c7":"code","2bee3a27":"code","e03eac3e":"code","37b108b4":"code","15e35d5b":"code","e20bf162":"code","cfe234bb":"code","f0cbf0dc":"code","47aaee54":"code","e4fb7f7a":"code","6f9d690b":"code","38f4caac":"code","9b876d7e":"code","c009fece":"code","574d23e8":"code","554cee5e":"code","3c43bdd8":"code","9deff3e3":"code","d1f8c93a":"code","cd3bfbd3":"code","ec68a7dd":"markdown","9ea387e5":"markdown","71f180fd":"markdown","96e8b7b5":"markdown","c58562db":"markdown","958c3bff":"markdown","8c45f117":"markdown","79ea155d":"markdown","6ba5ca31":"markdown","f1abefc9":"markdown","91616385":"markdown","76181e5a":"markdown","447710e1":"markdown","98511124":"markdown","44e37d3e":"markdown","1fbaa27e":"markdown","3cf9dedd":"markdown"},"source":{"58cf8273":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom tensorflow.keras import layers, callbacks\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.models import load_model\nfrom numpy import linalg","4918a402":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntrain.head()","349cc96d":"y = train['label']\nX = train.drop(['label'], axis = 1)\n\ny_shape = y.shape\nX_shape = X.shape\n\nprint('Tama\u00f1o de X: ', X_shape, '\\nTama\u00f1o de y: ', y_shape)","93d3cd7e":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2\n                                                    , random_state = 0)","1e7ea110":"#plt.imshow(X_train[0], cmap = plt.cm.binary)\n\n#np.set_printoptions(precision=2,sppress=True,linewidth=120)\n#print(np.matrix(X_train[0]))\n\n#for i in range(9):\n    # define subplot\n    #plt.subplot(330 + 1 + i)\n    # plot raw pixel data\n    #plt.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n# show the figure\n#plt.show()","a77bd4d7":"X_train = X_train.astype('float32')\nX_val = X_val.astype('float32')\n\nX_train\/=255\nX_val\/=255","5b7a4b65":"y_train = to_categorical(y_train, num_classes = 10)\ny_val = to_categorical(y_val, num_classes = 10)\n\nprint(y_train[0])","39aaa051":"model1 = Sequential([\n    tf.keras.layers.Dense(512, activation = 'relu', input_shape = (784,)),   \n    tf.keras.layers.Dense(10, activation = 'softmax')\n])\n\nmodel1.summary()","fe3407e2":"early_stopping = callbacks.EarlyStopping(\n    min_delta = 0.001,\n    patience = 50,\n    restore_best_weights = True\n    )\n\nmodel_checkpoint = ModelCheckpoint('.\/best_model.hdf5',\n                                   monitor = 'val_loss', \n                                   mode = \"min\", verbose = 1, \n                                   save_best_model = True)","2f18ebcc":"model1.compile(loss = 'categorical_crossentropy',\n              optimizer = 'sgd',\n              metrics = ['accuracy']\n)","5d2dd9b9":"history = model1.fit(\n    X_train,\n    y_train,\n    validation_data = (X_val, y_val),\n    batch_size = 256,\n    epochs = 100,\n    callbacks = [early_stopping, model_checkpoint],\n    verbose = 1\n    )","3d07b004":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = len(accuracy)\n\nplt.figure()\n\nplt.plot(range(epochs), accuracy, 'b', label='Training accuracy')\nplt.plot(range(epochs), val_accuracy, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(range(epochs), loss, 'b', label='Training Loss')\nplt.plot(range(epochs), val_loss, 'r', label='Training Loss')\nplt.title('Training and validation loss')\n\nplt.show()","5770065c":"model1 = load_model(\".\/best_model.hdf5\")","c0db2a3f":"model1.evaluate(X_val, y_val, verbose = 1)","56cfcb4b":"model2 = Sequential([\n    tf.keras.layers.Dense(512, activation = 'relu', input_shape = (784,)),\n    tf.keras.layers.Dense(512, activation = 'relu'),\n    tf.keras.layers.Dense(10, activation = 'softmax')\n])\n\nmodel2.summary()","2d4e56c7":"model2.compile(loss = 'categorical_crossentropy',\n              optimizer = 'sgd',\n              metrics = ['accuracy']\n)","2bee3a27":"history = model2.fit(\n    X_train,\n    y_train,\n    validation_data = (X_val, y_val),\n    batch_size = 256,\n    epochs = 100,\n    callbacks = [early_stopping, model_checkpoint],\n    verbose = 1\n    )","e03eac3e":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = len(accuracy)\n\nplt.figure()\n\nplt.plot(range(epochs), accuracy, 'b', label='Training accuracy')\nplt.plot(range(epochs), val_accuracy, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(range(epochs), loss, 'b', label='Training Loss')\nplt.plot(range(epochs), val_loss, 'r', label='Training Loss')\nplt.title('Training and validation loss')\n\nplt.show()","37b108b4":"model2 = load_model(\".\/best_model.hdf5\")","15e35d5b":"model2.evaluate(X_val, y_val, verbose = 1)","e20bf162":"model3 = Sequential([\n    tf.keras.layers.Dense(512, activation = 'relu', input_shape = (784,)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(512, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(10, activation = 'softmax')\n])\n\nmodel3.summary()","cfe234bb":"model3.compile(loss = 'categorical_crossentropy',\n              optimizer = 'sgd',\n              metrics = ['accuracy']\n)","f0cbf0dc":"history = model3.fit(\n    X_train,\n    y_train,\n    validation_data = (X_val, y_val),\n    batch_size = 256,\n    epochs = 100,\n    callbacks = [early_stopping, model_checkpoint],\n    verbose = 1\n    )","47aaee54":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = len(accuracy)\n\nplt.figure()\n\nplt.plot(range(epochs), accuracy, 'b', label='Training accuracy')\nplt.plot(range(epochs), val_accuracy, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(range(epochs), loss, 'b', label='Training Loss')\nplt.plot(range(epochs), val_loss, 'r', label='Training Loss')\nplt.title('Training and validation loss')\n\nplt.show()","e4fb7f7a":"model3 = load_model(\".\/best_model.hdf5\")","6f9d690b":"model3.evaluate(X_val, y_val, verbose = 1)","38f4caac":"model4 = Sequential([\n    tf.keras.layers.Dense(1024, activation = 'relu', input_shape = (784,)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(1024, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(512, activation = 'relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(10, activation = 'softmax')\n])\n\nmodel4.summary()","9b876d7e":"model4.compile(loss = 'categorical_crossentropy',\n              optimizer = 'sgd',\n              metrics = ['accuracy']\n)","c009fece":"history = model4.fit(\n    X_train,\n    y_train,\n    validation_data = (X_val, y_val),\n    batch_size = 256,\n    epochs = 100,\n    callbacks = [early_stopping, model_checkpoint],\n    verbose = 1\n    )","574d23e8":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = len(accuracy)\n\nplt.figure()\n\nplt.plot(range(epochs), accuracy, 'b', label='Training accuracy')\nplt.plot(range(epochs), val_accuracy, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\n\nplt.figure()\n\nplt.plot(range(epochs), loss, 'b', label='Training Loss')\nplt.plot(range(epochs), val_loss, 'r', label='Training Loss')\nplt.title('Training and validation loss')\n\nplt.show()","554cee5e":"model4 = load_model(\".\/best_model.hdf5\")","3c43bdd8":"model4.evaluate(X_val, y_val, verbose = 1)","9deff3e3":"X_test = test.astype('float32')\/255\ny_test = model4.predict(X_test).argmax(axis=1)\ny_test.shape","d1f8c93a":"submission = pd.DataFrame({'ImageId': np.arange(1, 28001), 'Label': y_test})\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission","cd3bfbd3":"!kaggle competitions submit -c digit-recognizer -f submission.csv -m \"Comparacion de redes neuronales simples\"","ec68a7dd":"Para la segunda red neuronal seguimos los mismos pasos que para la primera","9ea387e5":"# Introducci\u00f3n\n\nEn este proyecto de deep learning se va a crear una red neuronal convolucional para identificar distintos tipos de digitos escritos a mano, para ello se va a usar TensorFlow y su librer\u00eda Keras adem\u00e1s de Scikit-learn. Este ejercicio en concreto, el reconocimiento de d\u00edgitos de la librer\u00eda MNIST, es considerado el Hello World! de la programaci\u00f3n de inteligencias artificiales con redes neuronales, es decir, una primera toma de contacto con para conocer los distintos conceptos te\u00f3ricos globales aplicados a un caso en concreto.\n\nAl igual que los modelos de machine learning, los modelos de deep learning siguen una serie de pasos.\n\n1. Extraer librer\u00edas e importar datos\n2. Explorar datos\n3. Crear un modelo y entrenarlo\n4. Comprobar los datos","71f180fd":"Creamos las graficas que nos indicas como evoluciona nuestro modelo con el paso del tiempo","96e8b7b5":"Aqui convertimos los numeros en categorircos para verlos como en la salida de abajo","c58562db":"# 1. Importamos las librer\u00edas\n\nEmpezamos importando las librer\u00edas que vamos a utilizar, numpy, pandas, tensorflow y keras son las pricipales, tras esto, cargamos los datos en una variable. Como es costumbre, mostramos las cinco primera lineas de la variable que vamos a utilizar","958c3bff":"Entrenamos el primer model con 100 epocas y los parametros que hemos establecido antes","8c45f117":"Para poder tratar los datos se transforman en punto flotante y se dividen entre 255 para normalizarlos, ya que los datos van desde 0 hasta 255","79ea155d":"# 3. Creamos los modelos\n\nEn esta seccion crearemos tres modelos para compararlos, el primer modelo consta solo de tres capas, una de entrada, una capa densa oculta (una capa densa es aquella que tiene todas sus neuronas conectadas a las otras capas) y otra de salida, en la primera capa, la entrada, comun a las tres redes neuronales, tiene 784 entrada, una por cada pixel de las imagenes (28x28), la segunda red neuronal tiene una capa densa adicional que mejora levemente los resultados del conjunto de la red, y, la ultima red, consta de las mismas capas que la anterior, salvo que tiene dos normalizaciones y dos dropauts (la normalizacion, como su propio nombre indica, normaliza los datos antes de pasarlo a la siguiente capa y el dropout sirve para eliminar parte de los datos para que la red no se sobreajuste y aprenda los parametros caracteristicos del conjunto de entrenamiento, haciendo inutil todo el trabajo anterior)","6ba5ca31":"# 2. Exploraci\u00f3n de datos\n\nPara poder tratar los datos y hacer predicciones, tenemos que separar los datos de entrenamiento en dos variables, una 'X', con las variables independientes, y otra 'y', con la variable dependiente, tras hacer esto, separamos los datos de entrenamiento en dos subconjuntos, uno de entrenamiento propiamente dicho, del que nuestro modelo aprender\u00e1, y otro de validaci\u00f3n o validaci\u00f3n cruzada, que usaremos como test","f1abefc9":"# 4. Comprobar los datos","91616385":"Compilamos el entrenamiento estableciendo una funcion de perdida (loss), un optimizador y las metricas","76181e5a":"Para la tercera red neuronal seguimos los mismos pasos que para las dos anteriores","447710e1":"Aqui configuramos el early_stopping (o parada temprana en castellano), esto sirve para evitar el sobreajuste y mejorar el aprendizaje\n\nModelCheckpoint nos sirve para guardar el mejor resultado obtenido en el entrenamiento","98511124":"Y, para acabar, evaluamos el modelo con el mejor que hemos obtenido de esta red neuronal","44e37d3e":"Con la siguiente linea mostrariamos los el valor 0 de la simulacion, a mi me ha dado error, con las dos lineas siguientes veriamos el numero en forma matricial","1fbaa27e":"La mejor red neuronal es la tercera, que tiene un 97.65% de acierto","3cf9dedd":"Seleccionamos el mejor model de los que hemos obtenido"}}