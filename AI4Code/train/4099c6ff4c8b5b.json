{"cell_type":{"bac6ab29":"code","e9222afe":"code","caaf23d0":"code","f3902adf":"code","150c68b8":"code","372db72c":"code","bc2c711b":"code","9655107f":"code","57aebf9b":"code","3c372a65":"code","bc0bfb22":"code","480db0db":"code","86ca3dcb":"code","86a8b2f0":"code","dfdeebbc":"code","4c4dd770":"markdown","193766f1":"markdown","76de3e1a":"markdown","2eb994bf":"markdown","b3e1722d":"markdown","05b2b2cd":"markdown","221d9696":"markdown","4c4f471d":"markdown","b2065fc5":"markdown"},"source":{"bac6ab29":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import GroupKFold\nimport xgboost as xgb","e9222afe":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","caaf23d0":"# COLUMNS WITH STRINGS\nstr_type = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain','M1', 'M2', 'M3', 'M4','M5',\n            'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_30', \n            'id_31', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']\n\n# FIRST 53 COLUMNS\ncols = ['TransactionID', 'TransactionDT', 'TransactionAmt',\n       'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n       'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain',\n       'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11',\n       'C12', 'C13', 'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8',\n       'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M1', 'M2', 'M3', 'M4',\n       'M5', 'M6', 'M7', 'M8', 'M9']\n\n# V COLUMNS TO LOAD DECIDED BY CORRELATION EDA\n# https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\nv =  [1, 3, 4, 6, 8, 11]\nv += [13, 14, 17, 20, 23, 26, 27, 30]\nv += [36, 37, 40, 41, 44, 47, 48]\nv += [54, 56, 59, 62, 65, 67, 68, 70]\nv += [76, 78, 80, 82, 86, 88, 89, 91]\n\n#v += [96, 98, 99, 104] #relates to groups, no NAN \nv += [107, 108, 111, 115, 117, 120, 121, 123] # maybe group, no NAN\nv += [124, 127, 129, 130, 136] # relates to groups, no NAN\n\n# LOTS OF NAN BELOW\nv += [138, 139, 142, 147, 156, 162] #b1\nv += [165, 160, 166] #b1\nv += [178, 176, 173, 182] #b2\nv += [187, 203, 205, 207, 215] #b2\nv += [169, 171, 175, 180, 185, 188, 198, 210, 209] #b2\nv += [218, 223, 224, 226, 228, 229, 235] #b3\nv += [240, 258, 257, 253, 252, 260, 261] #b3\nv += [264, 266, 267, 274, 277] #b3\nv += [220, 221, 234, 238, 250, 271] #b3\n\nv += [294, 284, 285, 286, 291, 297] # relates to grous, no NAN\nv += [303, 305, 307, 309, 310, 320] # relates to groups, no NAN\nv += [281, 283, 289, 296, 301, 314] # relates to groups, no NAN\n#v += [332, 325, 335, 338] # b4 lots NAN\n\ncols += ['V'+str(x) for x in v]\ndtypes = {}\nfor c in cols+['id_0'+str(x) for x in range(1,10)]+['id_'+str(x) for x in range(10,34)]: \n    dtypes[c] = 'float32'\nfor c in str_type: dtypes[c] = 'object'","f3902adf":"%%time\n\nfolder_path = '..\/input\/ieee-fraud-detection\/'\nprint('Loading data...')\n\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv', index_col='TransactionID',  dtype=dtypes)\nprint('\\tSuccessfully loaded train_identity!')\n\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv', index_col='TransactionID', dtype=dtypes, usecols=cols+['isFraud'])\nprint('\\tSuccessfully loaded train_transaction!')\n\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv', index_col='TransactionID', dtype=dtypes)\nprint('\\tSuccessfully loaded test_identity!')\n\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv', index_col='TransactionID', dtype=dtypes, usecols=cols)\nprint('\\tSuccessfully loaded test_transaction!')\n\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\nprint('\\tSuccessfully loaded sample_submission!')\n\nprint('Data was successfully loaded!\\n')","150c68b8":"print('Merging data...')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint('Data was successfully merged!\\n')\n\ndel train_identity, train_transaction, test_identity, test_transaction\n\nprint(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\ngc.collect()","372db72c":"train.head()","bc2c711b":"test.head()","9655107f":"%%time\n\nfor col in train.columns:\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))","57aebf9b":"import datetime\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')\n\ntrain['DT_M'] = train['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntrain['DT_M'] = (train['DT_M'].dt.year-2017)*12 + train['DT_M'].dt.month \n\ntest['DT_M'] = test['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\ntest['DT_M'] = (test['DT_M'].dt.year-2017)*12 + test['DT_M'].dt.month","3c372a65":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","bc0bfb22":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n\nX_test = test.drop(['TransactionDT'], axis=1)\n\ndel train, test\ngc.collect()","480db0db":"X = X.fillna(-1)\nX_test = X_test.fillna(-1)\n\ncols = X.columns","86ca3dcb":"xgb_params = {'n_estimators':5000,\n              'max_depth':12,\n              'learning_rate':0.02,\n              'subsample':0.8,\n              'colsample_bytree':0.4,\n              'missing':-1,\n              'eval_metric':'auc',\n              'tree_method':'gpu_hist'\n             }","86a8b2f0":"%%time\n\noof = np.zeros(len(X))\npreds = np.zeros(len(X_test))\n\nskf = GroupKFold(n_splits=6)\nfor i, (idxT, idxV) in enumerate(skf.split(X, y, groups=X['DT_M'])):\n    month = X.iloc[idxV]['DT_M'].iloc[0]\n    print('Fold',i,'withholding month',month)\n    print('rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    \n    clf = xgb.XGBClassifier(**xgb_params)\n\n    h = clf.fit(X[cols].iloc[idxT], y.iloc[idxT], \n            eval_set=[(X[cols].iloc[idxV],y.iloc[idxV])],\n            verbose=100, early_stopping_rounds=200)\n\n    oof[idxV] += clf.predict_proba(X[cols].iloc[idxV])[:,1]\n    preds += clf.predict_proba(X_test[cols])[:,1]\/skf.n_splits\n    \n    del h, clf\n    x = gc.collect()\n\nprint('#'*20)\nprint ('XGB95 OOF CV=',roc_auc_score(y,oof))","dfdeebbc":"sub['isFraud'] = preds\nsub.to_csv(\"submission_xgb.csv\", index=False)","4c4dd770":"## Carregando os dados","193766f1":"## Preparando dados para o modelo","76de3e1a":"## Primeiro modelo, sem features novas","2eb994bf":"## Fun\u00e7\u00f5es \u00fateis","b3e1722d":"## Juntando os dados","05b2b2cd":"## Encoding","221d9696":"## Feature selection (tema n\u00e3o abordado na aula)","4c4f471d":"## DATA 1.1 | The power of Feature Engineering\n\nThis notebook was made for a lecture on feature engineering at the University of S\u00e3o Paulo for the members of the Data Science and Machine Learning group. It is written in Portuguese","b2065fc5":"## Importando bibliotecas"}}