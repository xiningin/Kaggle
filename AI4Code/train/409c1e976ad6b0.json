{"cell_type":{"c5bc1bdf":"code","7395db37":"code","c19f3f84":"code","489b9b87":"code","7b4668a7":"code","b66a1b8c":"code","d4c72f63":"code","0f32acbd":"code","e6400785":"code","05699156":"code","df08521d":"code","402a0550":"code","d39040d6":"code","42eb6a35":"code","4f54d6ef":"markdown","6bca10fb":"markdown","64dd23e3":"markdown","a66b63ce":"markdown","5b672724":"markdown","2dacb3c2":"markdown","95c38392":"markdown"},"source":{"c5bc1bdf":"!nvidia-smi -L","7395db37":"import numpy as np\nimport pandas as pd\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f'Using {device.type.upper()}')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c19f3f84":"import re\nimport string\n\ndef remove_html_tag(text): \n    return re.sub(\n        '<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', \n        '', text)","489b9b87":"df = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf['clean_text'] = df.review.apply(remove_html_tag)\ndf","7b4668a7":"df.clean_text.apply(lambda x: len(x.split(' '))).describe()","b66a1b8c":"from sklearn.model_selection import train_test_split\n\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train = df_train.reset_index()\ndf_test = df_test.reset_index()","d4c72f63":"class BertDataset(torch.utils.data.Dataset):\n    def __init__(self, df, bert_model_name='bert-base-cased'):\n        self.review = df.clean_text\n        self.sentiment = df.sentiment.apply(lambda x: 1 if x == 'positive' else 0)\n        self.tokenizer = BertTokenizerFast.from_pretrained(bert_model_name)\n        \n    def __len__(self):\n        return len(self.sentiment)\n    \n    def __getitem__(self, idx):\n        x = self.tokenize(self.review[idx])\n        for key in x:\n            x[key] = x[key].view(-1).to(device)\n        y = self.sentiment[idx]\n        return x, y\n    \n    def tokenize(self, x):\n        x = self.tokenizer(x, max_length=256, padding='max_length', \n                              truncation=True, return_tensors='pt')\n        return x","0f32acbd":"from transformers import BertTokenizerFast\n\ntrainloader = torch.utils.data.DataLoader(BertDataset(df_train), batch_size=32)\ntestloader = torch.utils.data.DataLoader(BertDataset(df_test), batch_size=32)","e6400785":"from transformers import BertModel\n\nclass BertClassifier(torch.nn.Module):\n    def __init__(self, bert_model_name='bert-base-cased', bert_freeze=False):\n        super(BertClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        for param in self.bert.parameters():\n            param.requires_grad = False if bert_freeze else True\n        self.dropout = torch.nn.Dropout(0.1)\n        self.classifier = torch.nn.Linear(768, 1)\n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        out = self.bert(input_ids, attention_mask, token_type_ids).pooler_output\n        out = self.dropout(out)\n        out = torch.sigmoid(self.classifier(out)).view(-1)\n        return out","05699156":"model = BertClassifier().to(device)\noptimizer = torch.optim.Adam(model.parameters(), 2e-5)\nloss = torch.nn.BCELoss()","df08521d":"from tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score\n\ndef train(epoch, model, optimizer, loss_fn, dataloader):\n    # Enable training\n    model.train()\n    with tqdm(dataloader, total=len(dataloader)) as pbar:\n        accuracy = []\n        losses = []\n        pbar.set_description('Epoch %d - Training\\t' %(epoch + 1))\n        for x, y in pbar:\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            \n            # Convert label for specified device and to float type\n            y = y.to(device).float()\n                \n            # Update weights and parameters by, forward + backward + optimize\n            y_pred = model(**x)\n            loss = loss_fn(y_pred, y)\n            loss.backward()\n            optimizer.step()\n            \n            # Convert tensors to CPU mode and integer type\n            y = y.cpu().int()\n            y_pred = (y_pred > 0.5).cpu().int()\n            \n            # Add metrics record to lists\n            accuracy.append(accuracy_score(y, y_pred))\n            losses.append(loss.item())\n            \n            # Store average of metrics\n            history = {'training_loss': np.mean(losses), \n                       'training_accuracy': np.mean(accuracy)}\n            \n            # Update progress bar\n            pbar.set_postfix(history)\n        return history\n    \ndef evaluate(epoch, model, loss_fn, dataloader):\n    # Disable training\n    model.eval()\n    with torch.no_grad(), tqdm(dataloader, total=len(dataloader)) as pbar:\n        accuracy = []\n        losses = []\n        pbar.set_description('Epoch %d - Validation\\t' %(epoch + 1))\n        for x, y in pbar:\n            # Generate prediction\n            y_pred = model(**x)\n            \n            # Convert label for specified device and to float type\n            y = y.to(device).float()\n            \n            # Compute loss\n            loss = loss_fn(y_pred, y)\n            \n            # Convert tensors to CPU mode and integer type\n            y_pred = (y_pred > 0.5).cpu().int()\n            y = y.cpu().int()\n            \n            # Add metrics record to lists\n            accuracy.append(accuracy_score(y, y_pred))\n            losses.append(loss.item())\n            \n            # Store average of metrics\n            history = {'validation_loss': np.mean(losses), \n                       'validation_accuracy': np.mean(accuracy)}\n            \n            # Update progress bar\n            pbar.set_postfix(history)\n        return history","402a0550":"history = [{**train(epoch, model, optimizer, loss, trainloader),\n            **evaluate(epoch, model, loss, testloader)}\n           for epoch in range(5)]","d39040d6":"training_loss = [h['training_loss'] for h in history]\nvalidation_loss = [h['validation_loss'] for h in history]\ntraining_accuracy = [h['training_accuracy'] for h in history]\nvalidation_accuracy = [h['validation_accuracy'] for h in history]","42eb6a35":"import matplotlib.pyplot as plt\n\n# Styling\nplt.style.use('seaborn-darkgrid')\n\n# Initialize plot\nfig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 5))\n# Left plot\nax1.set_title('Loss')\nax1.plot(training_loss, 'o-', label='Training Loss')\nax1.plot(validation_loss, 'o-', label='Validation Loss')\nax1.legend()\n# Right plot\nax2.set_title('Accuracy')\nax2.plot(training_accuracy, 'o-', label='Training Accuracy')\nax2.plot(validation_accuracy, 'o-', label='Validation Accuracy')\nax2.legend()\nfig.show()","4f54d6ef":"# Model Definition and Training","6bca10fb":"## Token Length Exploration","64dd23e3":"## Text Cleaning","a66b63ce":"# Check for GPU","5b672724":"# Initial Imports","2dacb3c2":"## Data Preparation","95c38392":"## Load Dataset and Clean Text"}}