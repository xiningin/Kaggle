{"cell_type":{"cd54a1bd":"code","fdf208bd":"code","c19e1337":"code","315974a2":"code","a5f0b5c7":"code","2d11976f":"code","da24d9e0":"code","491e13ba":"code","ee1facec":"code","19d588e5":"code","f36f829e":"code","e7573397":"code","95cc0701":"code","8aa0fdb7":"code","eb848a26":"code","f8e4b770":"code","4c562559":"code","a7e5a438":"code","15bf2e88":"markdown","6cc2f96f":"markdown","34420e64":"markdown","34e9ffb9":"markdown","a5eb02dc":"markdown","08b43b30":"markdown","3316af76":"markdown","5e42c877":"markdown","e0ab1574":"markdown","63a2a8ed":"markdown","adb6370d":"markdown","118692b9":"markdown"},"source":{"cd54a1bd":"import warnings\nwarnings.filterwarnings(\"ignore\")","fdf208bd":"from transformers import *  # this is HuggingFace library\nimport tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel, BertConfig","c19e1337":"# this will download the BERT Tokenizer\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")  \n# this will download the BERT Trained Model\n# output_hidden_states=False, as we are training & not interested in output state.\nconfig = BertConfig.from_pretrained(\"bert-base-uncased\",output_hidden_states=False) # dropout=0.2, attention_dropout=0.2\nbert_model = TFBertModel.from_pretrained('bert-base-uncased', config=config)","315974a2":"sent= 'This is a example Text that we are using for Understanding Purpose, another word that we gonna use is Kaggle'\ntokens=bert_tokenizer.tokenize(sent)\nprint(tokens)","a5f0b5c7":"ids = bert_tokenizer.convert_tokens_to_ids(tokens)\nprint(\"convert_tokens_to_ids:\",ids)\n\nids_encode = bert_tokenizer.encode(sent,add_special_tokens = True,max_length =30,pad_to_max_length = True,\nreturn_attention_mask = True)\nprint(\"\\n\\nencode:\",ids_encode)\n\nids_encode_plus = bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =30,pad_to_max_length = True,\nreturn_attention_mask = True)\nprint(\"\\n\\nencode_plus:\",ids_encode_plus)","2d11976f":"print(\"convert_ids_to_tokens:\",bert_tokenizer.convert_ids_to_tokens(ids))\n\nprint(\"\\ndecode:\",bert_tokenizer.decode(ids_encode_plus['input_ids']))","da24d9e0":"import pandas as pd\nimport re\nimport numpy as np\nfrom tqdm import tqdm\ntrain = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv',encoding = \"ISO-8859-1\")","491e13ba":"train['sentiment'].value_counts()","ee1facec":"def clean_text(temp):\n    temp=re.sub(\"@\\S+\", \" \", temp)\n    temp=re.sub(\"https*\\S+\", \" \", temp)\n    temp=re.sub(\"#\\S+\", \" \", temp)\n    temp=re.sub(\"\\'\\w+\", '', temp)\n    temp=re.sub(r'\\w*\\d+\\w*', '', temp)\n    temp=re.sub('\\s{2,}', \" \", temp)\n    return temp.strip()","19d588e5":"train['review_clean'] = train['review'].apply(clean_text)\nsentences = train['review_clean']\n\n# train['OriginalTweetC'] = train['OriginalTweet'].apply(clean_text)\n# sentences = train['OriginalTweetC']","f36f829e":"input_ids=[]\nattention_masks=[]\n\nfor sent in tqdm(sentences):\n    bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =128,pad_to_max_length = True,return_attention_mask = True)\n    input_ids.append(bert_inp['input_ids'])\n    attention_masks.append(bert_inp['attention_mask'])\n\ninput_ids=np.asarray(input_ids)\nattention_masks=np.array(attention_masks)\ntarget = np.array(pd.get_dummies(train['sentiment']))","e7573397":"bert_tokenizer.convert_ids_to_tokens(101)","95cc0701":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test,train_mask,test_mask=train_test_split(input_ids,target,attention_masks,test_size=0.2)","8aa0fdb7":"bert_model.summary()","eb848a26":"def create_model(model_):\n    input_ids = tf.keras.Input(shape=(128,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(128,),dtype='int32')\n    \n    output = model_(input_ids,attention_masks)\n    output = output[0]      # this is inline in config.output_hidden_states as we want only the top head\n    \n    output = output[:,0,:]  #  We are only interested in <cls> or classification token of the model which can be extracted\n                            #  using the slice operation. Now we have 2D data and build the network as one desired.\n                            #  While converting 3D data to 2D we may miss on valuable info.\n    \n    output = tf.keras.layers.Dense(32,activation='relu')(output)\n    output = tf.keras.layers.Dropout(0.2)(output)\n    output = tf.keras.layers.Dense(2,activation='softmax')(output)\n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    \n    \n    for layer in model.layers[:3]:\n        layer.trainable = False\n    return model\n\nmodel = create_model(bert_model)\nmodel.summary()  ","f8e4b770":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory=model.fit([X_train,train_mask],y_train,batch_size=32,epochs=4,validation_data=([X_test,test_mask],y_test))","4c562559":"def create_model(model_):\n    \n    input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n    input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n\n    embedding_layer = model_(input_ids_in, attention_mask=input_masks_in)[0] \n    # as 3D data is generated earlier embedding layer, we can use LSTM to extract great details\n    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\n    # Next thing is to transform the 3D data into 2D so that we can use a FC layer. You can use any Pooling layer to perform this.\n    X = tf.keras.layers.GlobalMaxPool1D()(X)\n    X = tf.keras.layers.Dense(50, activation='relu')(X)\n    X = tf.keras.layers.Dropout(0.2)(X)\n    X = tf.keras.layers.Dense(2, activation='sigmoid')(X)\n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\n    for layer in model.layers[:3]:\n        layer.trainable = False\n        \n    return model  \n\nmodel = create_model(bert_model)\nmodel.summary()","a7e5a438":"model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nhistory=model.fit([X_train,train_mask],y_train,batch_size=32,epochs=4,validation_data=([X_test,test_mask],y_test))","15bf2e88":"Source\/Credit for above two BERT codes:https:\/\/towardsdatascience.com\/working-with-hugging-face-transformers-and-tf-2-0-89bf35e3555a","6cc2f96f":"---\n### convert_tokens_to_ids:\nConverts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the vocabulary.\n### encode:\nConverts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n###  encode_plus:\nReturns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified.\n\n\n## Difference b\/ encode vs encode_plus:\n\n> The main difference is stemming from the additional information that encode_plus is providing. If you read the documentation on the respective functions, then there is a slight difference forencode():\n> \n> > Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary. Same as doing self.convert_tokens_to_ids(self.tokenize(text)).\n> \n> and the description of encode_plus():\n> \n> > Returns a dictionary containing the encoded sequence or sequence pair and additional information: the mask for sequence classification and the overflowing elements if a max_length is specified.\n> \n> Depending on your specified model and input sentence, the difference lies in the additionally encoded information, specifically the input mask. Since you are feeding in two sentences at a time, BERT (and likely other model variants), expect some form of masking, which allows the model to discern between the two sequences, see here(https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertTokenizer.create_token_type_ids_from_sequences). Since encode_plus is providing this information, but encode isn't, you get different output results.\n\nCredit\/Source: https:\/\/stackoverflow.com\/a\/61732210","34420e64":"### Below is example how BERT Uncased Tokenizer works:\n\n\n> * BERT-Base, uncased uses a vocabulary of 30,522 words. The processes of tokenization involve splitting the input text into a list of tokens that are available in the vocabulary. In order to deal with the words not available in the vocabulary, BERT uses a technique called BPE based WordPiece tokenization. In this approach, an out of vocabulary word is progressively split into subwords and the word is then represented by a group of subwords. Since the subwords are part of the vocabulary, we have learned representations a context for these subwords and the context of the word is simply the combination of the context of the subwords.\n> \n> * The tokens are either words or subwords. Here for instance, \u201cKaggle\u201d wasn\u2019t in the model vocabulary, so it\u2019s been split into \u201cka\u201d and \u201c##ggle\u201d. To indicate those tokens are not separate words but parts of the same word, a double-hash prefix is added for \u201cggle\u201d.\n\nCredit\/Source: https:\/\/swatimeena989.medium.com\/bert-text-classification-using-keras-903671e0207d#b60a\n\nYou can see that Uppercase words are also converted into smallcase.","34e9ffb9":"## BERT: Bidirectional Encoder Representations from Transformers\n\n> BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT\u2019s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:\n> - Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.\n> - Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.\n> - Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence.\n \n\n![image.png](attachment:4536426a-24b3-456e-8db3-15f06bc53aa9.png)\n\nAbove Text and Image is from(Credit\/Source): https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/simple-text-multi-classification-task-using-keras-bert\/","a5eb02dc":"https:\/\/jaketae.github.io\/category\/common-sense\/","08b43b30":"## More about token_type_ids and attentation_mask:\n### token_type_ids -\n> This is enough for some models to understand where one sequence ends and where another begins.\n\nsequence_a = \"HuggingFace is based in NYC\"\n\nsequence_b = \"Where is HuggingFace based?\"\n\nencoded_dict = tokenizer(sequence_a, sequence_b)\n\ndecoded = tokenizer.decode(encoded_dict[\"input_ids\"])\n\n\n\n> encoded_dict['token_type_ids']\n\n>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n\n> The first sequence, the \u201ccontext\u201d used for the question, has all its tokens represented by a 0, whereas the second sequence, corresponding to the \u201cquestion\u201d, has all its tokens represented by a 1.\n\n### attention_mask \u2013\n> *  Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]: 1 for tokens that are not masked, 0 for tokens that are marked (0 if the token is added by padding).\n> * This argument indicates to the model which tokens should be attended to, and which should not.\n> * If we have 2 sentences and the sequence length of one sentence is 8 and another one is 10, then we need to make them of equal length and for that, padding is required. To distinguish between the padded and nonpadded input attention mask is used.\n\nCredit\/Source for attention_mask part: https:\/\/swatimeena989.medium.com\/bert-text-classification-using-keras-903671e0207d#b60a\n\n\n","3316af76":"---\n### convert_ids_to_tokens:\nConverts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and added tokens.\n\n### decode:\nConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special tokens and clean up tokenization spaces","5e42c877":"### tokenize:\nConverts a string in a sequence of tokens, using the tokenizer","e0ab1574":"# BERT Keras Coding with HuggingFace Library","63a2a8ed":"## Lets start implementing BERT on some real life Dataset, here we are gonna use IMDB Movie Review dataset","adb6370d":"### More about sparse_categorical_crossentropy vs categorical_crossentropy:\n> \n> * For sparse_categorical_crossentropy, For class 1 and class 2 targets, in a 5-class classification problem, the list should be [1,2]. Basically, the targets should be in integer form in order to call sparse_categorical_crossentropy. This is called sparse since the target representation requires much less space than one-hot encoding. For example, a batch with b targets and k classes needs b * k space to be represented in one-hot, whereas a batch with b targets and k classes needs b space to be represented in integer form.\n> \n> * For categorical_crossentropy, for class 1 and class 2 targets, in a 5-class classification problem, the list should be [[0,1,0,0,0], [0,0,1,0,0]]. Basically, the targets should be in one-hot form in order to call categorical_crossentropy.\n\nSource\/Credit: https:\/\/stackoverflow.com\/a\/58574260","118692b9":"## More about BERT base model:\n> Pretrained model on English language using a masked language modeling (MLM) objective. It was introduced in this paper and first released in this repository. This model is uncased: it does not make a difference between english and English.\n\nCredit\/Source: https:\/\/huggingface.co\/bert-base-uncased\n\n### Other types of BERT Trained Models:\n\n1. BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters\n2. BERT-Large, Uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n3. BERT-Base, Cased: 12-layer, 768-hidden, 12-heads , 110M parameters\n4. BERT-Large, Cased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n5. BERT-Base, Multilingual Case: 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters\n6. BERT-Base, Chinese: Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters"}}