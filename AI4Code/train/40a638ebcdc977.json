{"cell_type":{"60bcbf00":"code","dfac15c5":"code","f1aabbe6":"code","d2c9eaf5":"code","273b7562":"code","16c501f6":"code","7e4ae2ec":"code","e2ec54b7":"code","f38ebcaa":"code","7b1afac3":"code","a6280b33":"code","a8eaa37c":"markdown","3a5ca1c3":"markdown","671882ef":"markdown","73a2dc5b":"markdown","c6de7c19":"markdown","1812609b":"markdown"},"source":{"60bcbf00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","dfac15c5":"train_data =pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","f1aabbe6":"test_data =pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","d2c9eaf5":"women =train_data.loc[train_data.Sex == 'female'] [\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint (\"% of women who survived:\", rate_women)","273b7562":"men = train_data.loc[train_data.Sex == 'male'] [\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint (\"% of the men who survived:\", rate_men)","16c501f6":"#Aside from 'Sex', the 'Age' feature is second in importance.\ndef simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n\n#Each Cabin starts with a letter.\ndef simplify_cabins(df):\n    df.Cabin = df.Cabin.fillna('N')\n    df.Cabin = df.Cabin.apply(lambda x: x[0])\n    return df\n\n#Fare is another continuous value that should be simplified.\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df\n\n#Extract information from the 'Name' feature. Rather than use the full name\ndef format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n    return df    \n\n#drop useless features. (Ticket and Name).\ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = format_name(df)\n    df = drop_features(df)\n    return df\n\ntrain_data = transform_features(train_data)\ntest_data = transform_features(test_data)\ntrain_data.head()","7e4ae2ec":"from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n    \ntrain_data, test_data = encode_features(train_data, test_data)\ntrain_data.head()","e2ec54b7":"from sklearn.model_selection import train_test_split\n\nX_all = train_data.drop(['Survived', 'PassengerId'], axis=1)\ny_all = train_data['Survived']\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=23)","f38ebcaa":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, y_train)","7b1afac3":"predictions = clf.predict(X_test)\nprint(accuracy_score(y_test, predictions))","a6280b33":"ids = test_data['PassengerId']\npredictions = clf.predict(test_data.drop('PassengerId', axis=1))\n\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('microwave.csv', index = False)\noutput.head()\n","a8eaa37c":"The code cell below looks for patterns in four different columns (\"Pclass\", \"Sex\", \"SibSp\", and \"Parch\") of the data.\nThe code also saves these new predictions in a CSV file","3a5ca1c3":"**RANDOM FOREST MODEL**","671882ef":"## Predict the Actual Test Data","73a2dc5b":"## Splitting up the Training Data\n\nFirst, separate the features(X) from the labels(y). \n\n**X_all:** All features minus the value we want to predict (Survived).\n\n**y_all:** Only the value we want to predict. \n\nSecond, use Scikit-learn to randomly shuffle this data into four variables. In this case, I'm training 80% of the data, then testing against the other 20%.","c6de7c19":"## Transforming Features","1812609b":"75% of the women on board survived, whereas only 19% of the men lived\ngender-based submission bases its predictions on only a single column. by considering multiple columns, we can discover more complex patterns that can potentially yield better-informed predictions."}}