{"cell_type":{"7fe8e5ee":"code","01eb4b0b":"code","b95aae17":"code","ce037e6c":"code","3d70548a":"code","a9b1aefd":"code","6b0106e2":"code","99d53ade":"code","8a08a981":"code","ac33d6a5":"code","6e2bd626":"code","84f36e9f":"code","ce01ab87":"code","c42f0f7c":"code","2b73f0ea":"code","ca522af7":"code","19f1484d":"code","733d9634":"code","7848b494":"code","50bc2849":"code","51c9e376":"code","590c486d":"code","1148c331":"code","0aae4d1f":"code","14c9ae38":"code","0243f16f":"code","0a2cbcca":"code","7c8eac64":"code","39460d9c":"code","5185e5ae":"code","4a2e6a2f":"code","12fafc9a":"code","33ded7b7":"code","c1f141ae":"code","7fef8177":"code","eca71c86":"code","27d93475":"code","ba0fbd03":"code","f5a9a8e4":"code","60c176dc":"code","86f84fb2":"code","7f06ac2a":"code","d6fb318e":"code","055e8562":"code","147c89ae":"code","db694941":"code","d7995bd4":"code","1c9f439f":"code","4c22c740":"code","58fe3cc6":"code","7a78ee74":"code","05518df3":"code","5c2b701d":"code","d6b52e25":"code","7e948537":"code","feb59e6d":"code","cbc7fcc2":"code","7b67e64f":"code","b6cc1c9b":"code","b64df443":"code","55617fac":"code","88e488d4":"markdown","b6bc8716":"markdown","d87f2a90":"markdown","47e87609":"markdown","2a4ae758":"markdown","6fe17d9e":"markdown","898a6f5e":"markdown","208d234a":"markdown","38eb0063":"markdown","ed248103":"markdown","1e036585":"markdown","91e8f76b":"markdown","d1338d27":"markdown","98d9543e":"markdown","e01382c2":"markdown","1c0fcaf0":"markdown","cab641c1":"markdown","cec7b1ef":"markdown","6ff76b28":"markdown"},"source":{"7fe8e5ee":"import warnings\nimport inspect\nwarnings.filterwarnings('ignore')\n\n# Imports\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom functools import reduce\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nfrom scipy.stats import loguniform\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import RadiusNeighborsClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n# Dimension Reduction\n# https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html\n# https:\/\/scikit-learn.org\/stable\/modules\/unsupervised_reduction.html\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_feature_agglomeration_vs_univariate_selection.html#sphx-glr-auto-examples-cluster-plot-feature-agglomeration-vs-univariate-selection-py\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_digits_agglomeration.html#sphx-glr-auto-examples-cluster-plot-digits-agglomeration-py\n\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py","01eb4b0b":"cmap = sns.diverging_palette(h_neg=10, h_pos=240, as_cmap=True)","b95aae17":"test_df  = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-02\/eval.csv')\ntrain_df  = pd.read_csv('..\/input\/cap-4611-2021-fall-assignment-02\/train.csv')\n\n# For exploritory analysis and training\nesrb_rating_le = LabelEncoder()\ntrain_df = train_df.drop(['title', 'id'], axis=1).copy()\ntrain_df['esrb_rating'] = esrb_rating_le.fit_transform(train_df[['esrb_rating']])","ce037e6c":"print(f\"Missing values in train: {train_df.isnull().sum().sum()}\")\nprint(f\"Missing values in test: {test_df.isnull().sum().sum()}\")\nmissingno.matrix(train_df)\nmissingno.matrix(test_df)","3d70548a":"# Note that -1 for label column\nprint(f\"Total Features in train:\\t{len(train_df.columns) - 1}\")\nprint(f\"Numerical Features in train:\\t{len(train_df.select_dtypes(include = np.int).columns) - 1}\")\nprint(f\"Other Features in train:\\t{len(train_df.columns) - 1 - (len(train_df.select_dtypes(include = np.int).columns) - 1)}\")\n\nprint()\n\n# Note that -1 for id column\nprint(f\"Total Features in train:\\t{len(test_df.columns) - 1}\")\nprint(f\"Numerical Features in train:\\t{len(test_df.select_dtypes(include = np.int).columns) - 1}\")\nprint(f\"Other Features in train:\\t{len(test_df.columns) - 1 - (len(test_df.select_dtypes(include = np.int).columns) - 1)}\")\n\nprint()\n\nprint(f\"Train set data range:\\t\\t[{train_df.iloc[:, :-1].min().min()}, {train_df.iloc[:, :-1].max().max()}]\")\nprint(f\"Test set data range:\\t\\t[{test_df.iloc[:, 1:].min().min()}, {test_df.iloc[:, 1:].max().max()}]\")","a9b1aefd":"# Labels\ncatagories = [\n    'E',\n    'ET',\n    'M',\n    'T',\n]\nmixed_catagories = [(x, y) for x in catagories for y in catagories if x is not y and catagories.index(x) < catagories.index(y)]","6b0106e2":"not_e_or_et_features = [\n    'blood',\n    'blood_and_gore',\n    'sexual_content',\n    'sexual_themes',\n]\n\nnot_e_features = [\n    'animated_blood',\n    'blood',\n    'blood_and_gore',\n    'cartoon_violence',\n    'drug_reference',\n    'fantasy_violence',\n    'intense_violence',\n    'language',\n    'lyrics',\n    'mature_humor',\n    'mild_blood',\n    'mild_suggestive_themes',\n    'nudity',\n    'partial_nudity',\n    'sexual_content',\n    'sexual_themes',\n    'simulated_gambling',\n    'strong_janguage',\n    'strong_sexual_content',\n    'suggestive_themes',\n    'use_of_alcohol',\n    'violence',\n]\n\nnot_m_features = [\n    'animated_blood',\n    'cartoon_violence',\n    'mild_cartoon_violence',\n    'mild_language',\n    'mild_lyrics',\n    'mild_suggestive_themes',\n    'mild_violence',\n]","99d53ade":"def mask_by_labels(labels=catagories, inplace=True):\n    mask = reduce(lambda a, b: a | b, [esrb_rating_le.inverse_transform(train_df['esrb_rating']) == label for label in labels])\n    \n    if inplace:\n        return train_df[mask].copy()\n    return mask\n\ndef mask_by_features(df=train_df, features=not_e_or_et_features, value=0, inplace=True, encode=False):\n    df = df.copy()\n    mask = reduce(lambda a, b: a & b, [df[feature] == value for feature in features])\n    \n    if inplace:\n        if encode and 'esrb_rating' in df.columns: \n            df['esrb_rating'] = esrb_rating_le.inverse_transform(df['esrb_rating'])\n        return df[mask]\n    return mask\n\ndef has_any_feature(df=train_df, features=not_e_or_et_features, value=1, inplace=True, encode=False):\n    df = df.copy()\n    mask = ~mask_by_features(df=df, features=features, value=int(value == 0), inplace=False)\n    \n    if inplace:\n        if encode and 'esrb_rating' in df.columns: \n            df['esrb_rating'] = esrb_rating_le.inverse_transform(df['esrb_rating'])\n        return df[mask]\n    return mask","8a08a981":"rotation=75\n# Bar plot occurences of features for each label\nfor cat in catagories:\n    cats = [cat]\n    # Make figure readable lol\n    plt.figure(figsize=(20, 14)) # default 6.4, 4.8\n\n    # Keep only rows with matching categories\n    cat_df = mask_by_labels(labels=[cat])\n    \n    # Plot\n    cat_df.iloc[:, :-1].sum().plot.bar()\n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    plt.title(f\"Feature Occurence for label {title_cats}\", fontsize=20)\n    plt.xticks(rotation = rotation)\n    plt.show()","ac33d6a5":"train_not_m = mask_by_features(features=not_e_features, inplace=False) # is likely E\ntrain_is_m_or_t = has_any_feature(features=not_e_or_et_features, inplace=False) # can train without worrying about E or ET\ntmp_df = train_df.copy()\ntmp_df['esrb_rating'] = esrb_rating_le.inverse_transform(tmp_df['esrb_rating'])\n\nrotation = 0\nfor title, value_counts in (\n        (\"Not M by features\", has_any_feature(features=not_m_features, encode=True)['esrb_rating'].value_counts()), # Can train without worrying about M\n        (\"Not E by features\", has_any_feature(features=not_e_features, encode=True)['esrb_rating'].value_counts()), # Can train without worrying about E\n        (\"Either M or T by features (not E or ET)\", has_any_feature(features=not_e_or_et_features, encode=True)['esrb_rating'].value_counts()), # Either M or T by features\n        (\"Unlikely M by features\", mask_by_features(features=not_e_or_et_features, encode=True)['esrb_rating'].value_counts()), # Small likelyhood of being M by features\n        (\"Not M, likely E by features\", mask_by_features(features=not_e_features, encode=True)['esrb_rating'].value_counts()), # Not M, likely E by features\n        ('\"Not Not\" M by features\\n(seems evenly distributed)', mask_by_features(features=not_m_features, encode=True)['esrb_rating'].value_counts()), # \"Not Not\" M by features\n        ('\"Not Not\" M intersect Not M or T', tmp_df[~train_not_m & ~train_is_m_or_t]['esrb_rating'].value_counts()),\n    ):\n    value_counts.plot.bar()\n    plt.title(f\"Class distribution when\\n{title}\", fontsize=20)\n    plt.xticks(rotation = rotation)\n    plt.show()","6e2bd626":"test_not_m = mask_by_features(df=test_df, features=not_e_features, inplace=False) # is likely E\ntest_is_m_or_t = has_any_feature(df=test_df, features=not_e_or_et_features, inplace=False) # can train without worrying about E or ET\n\nprint(\"# rows likely E, 100% not M:         \\t\", len(mask_by_features(df=test_df, features=not_e_features)))\nprint(\"# rows that are M or T:              \\t\", len(has_any_feature(df=test_df, features=not_e_or_et_features, encode=True)))\nprint(\"# rows not M intersect either M or T:\\t\", len(test_df[test_not_m & test_is_m_or_t]))\nprint(\"# Not E:                             \\t\", len(test_df[~test_not_m & ~test_is_m_or_t]))\nprint('# of rows above combined:            \\t', len(mask_by_features(df=test_df, features=not_e_features)) + len(has_any_feature(df=test_df, features=not_e_or_et_features, encode=True)) + len(test_df[test_not_m & test_is_m_or_t]) + len(test_df[~test_not_m & ~test_is_m_or_t]))\nprint('Overall number of test entries:      \\t', len(test_df))\nprint('\\nWe can separate the data and train more specific models')","84f36e9f":"# Subsets to train our models\nSUBSET_NOT_E = ~train_not_m & ~train_is_m_or_t\nSUBSET_NOT_M = mask_by_features(features=not_e_features, inplace=False) # Very likely to be E\nSUBSET_EITHER_M_OR_T = has_any_feature(features=not_e_or_et_features, inplace=False)\n\nTRAIN_SUBSETS = [\n    {\n        'title': \"Not E\",\n        'mask': SUBSET_NOT_E,\n        'to_drop': []\n    },\n    {\n        'title': \"Not M\",\n        'mask': SUBSET_NOT_M,\n        'to_drop': []\n    },\n    {\n        'title': \"Not Not M\",\n        'mask': ~SUBSET_NOT_M,\n        'to_drop': []\n    },\n    {\n        'title': \"Either M or T\",\n        'mask': SUBSET_NOT_E,\n        'to_drop': []\n    },\n]\n","ce01ab87":"# Just to make sure no intersections\ntemp_dict = {'a': SUBSET_NOT_E, 'b': SUBSET_NOT_M, 'c': SUBSET_EITHER_M_OR_T}\nfor x,y in [(temp_dict[a], temp_dict[b]) for a in temp_dict for b in temp_dict if a is not b and list(temp_dict.keys()).index(a) < list(temp_dict.keys()).index(b)]:\n    print(len(train_df[x & y]))","c42f0f7c":"rotation = 75\nfor subset in TRAIN_SUBSETS:\n    mask = subset['mask']\n    title = subset['title']\n    \n    # Make figure readable lol\n    plt.figure(figsize=(12, 8)) # default 6.4, 4.8\n    \n    # Occurrences Dataframe\n    occ_df = train_df[mask].iloc[:, :-1].sum()\n    \n    # Plot\n    occ_df.plot.bar()\n    plt.title(f\"Subset '{title}'\\nFeature Occurrences\", fontsize=20)\n    plt.xticks(rotation = rotation)\n    plt.show()\n    \n    # Drop empty columns\n    subset['to_drop'] = occ_df.index[train_df[mask].iloc[:, :-1].sum() == 0].values    \n    print(\"Useless features:\", subset['to_drop'])\n    print()","2b73f0ea":"# Make figure readable lol\nplt.figure(figsize=(20, 14)) # default 6.4, 4.8\nrotation=75\n\n# Absolute value of correlation\ntrain_corr = train_df.corr().abs() * 100 # Easier to read\n\n# Keep only lower triangle\ntrain_corr_mask = np.triu(np.ones_like(train_corr, dtype=bool))\n\n# Graph correlation lower triangle\nsns.heatmap(train_corr, mask=train_corr_mask,\n            cmap=cmap, center=0, linewidths=1,\n            annot=True, fmt='.1f')\nplt.title(\"General Correlation Heatmap\", fontsize=20)\nplt.xticks(rotation = rotation)\nplt.show()\n\nfor cats in mixed_catagories:\n    print(cats)\n    # Make figure readable lol\n    plt.figure(figsize=(20, 14)) # default 6.4, 4.8\n\n    # Keep only rows with matching categories\n    row_mask = reduce(lambda a, b: a | b, [esrb_rating_le.inverse_transform(train_df['esrb_rating']) == cat for cat in cats])\n    \n    # Multiple by 100 for readability\n    corr_df = train_df[row_mask].corr().abs() * 100\n\n    # Keep only lower triangle\n    corr_df_mask = np.triu(np.ones_like(corr_df, dtype=bool))\n\n    # Plot\n    sns.heatmap(corr_df, mask=train_corr_mask,\n            cmap=cmap, center=0, linewidths=1,\n            annot=True, fmt='.1f')\n    \n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    plt.title(f\"Correlation Heatmap specific for {title_cats}\", fontsize=20)\n    plt.xticks(rotation = rotation)\n    plt.show()","ca522af7":"m = TSNE(learning_rate=50, random_state=111)\ntsne_features = m.fit_transform(train_df.iloc[:, :-1])\n\ntemp_df = train_df.copy()\ntemp_df['x'] = tsne_features[:, 0]\ntemp_df['y'] = tsne_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n\n\ntemp_df = mask_by_labels(labels=['E', 'ET'])\n\nfor subset in TRAIN_SUBSETS:\n    mask = subset['mask']\n    title = subset['title']\n    to_drop = subset['to_drop']\n    \n    # Keep only subset and meaningful columns\n    temp_df = train_df[mask].drop([*to_drop, 'console'], axis=1).copy()\n    \n    # t-SNE\n    tsne_features = m.fit_transform(StandardScaler().fit_transform(temp_df.iloc[:, :-1]))\n\n    temp_df['x'] = tsne_features[:, 0]\n    temp_df['y'] = tsne_features[:, 1]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    sns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\n    plt.title(f\"t-SNE for '{title}' subset\", fontsize=20)\n    plt.show()","19f1484d":"m = TSNE(learning_rate=50, random_state=111)\ntsne_features = m.fit_transform(train_df.iloc[:, :-1])\n\ntemp_df = train_df.copy()\ntemp_df['x'] = tsne_features[:, 0]\ntemp_df['y'] = tsne_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n\n\ntemp_df = mask_by_labels(labels=['E', 'ET'])\n\nfor cats in mixed_catagories:\n    # Keep only rows with matching categories\n    temp_df = mask_by_labels(labels=cats)\n    # T\n    m = TSNE(learning_rate=50, random_state=111)\n    tsne_features = m.fit_transform(temp_df.iloc[:, :-1])\n\n    temp_df['x'] = tsne_features[:, 0]\n    temp_df['y'] = tsne_features[:, 1]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    sns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\n    plt.title(f\"t-SNE for {title_cats}\", fontsize=20)\n    plt.show()\n\n","733d9634":"pca = PCA(n_components=2, random_state=69420)\npca_features = pca.fit_transform(train_df.iloc[:, :-1])\n\ntemp_df = train_df.copy()\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n\nfor subset in TRAIN_SUBSETS:\n    mask = subset['mask']\n    title = subset['title']\n    to_drop = subset['to_drop']\n    \n    # Keep only subset and meaningful columns\n    temp_df = train_df[mask].drop([*to_drop, 'console'], axis=1).copy()\n    \n    # PCA\n    pca_features = pca.fit_transform(StandardScaler().fit_transform(temp_df.iloc[:, :-1]))\n    \n    temp_df['x'] = pca_features[:, 0]\n    temp_df['y'] = pca_features[:, 1]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    sns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\n    plt.title(f\"2d PCA for '{title}' subset\", fontsize=20)\n    plt.show()","7848b494":"pca = PCA(n_components=2, random_state=69420)\npca_features = pca.fit_transform(train_df.iloc[:, :-1])\n\ntemp_df = train_df.copy()\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n\nfor subset in TRAIN_SUBSETS:\n    mask = subset['mask']\n    title = subset['title']\n    to_drop = subset['to_drop']\n    \n    # Keep only subset and meaningful columns\n    temp_df = train_df[mask].drop([*to_drop, 'console'], axis=1).copy()\n    \n    # PCA\n    pca_features = pca.fit_transform((temp_df.iloc[:, :-1]))\n    \n    temp_df['x'] = pca_features[:, 0]\n    temp_df['y'] = pca_features[:, 1]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    sns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\n    plt.title(f\"2d PCA for '{title}' subset\", fontsize=20)\n    plt.show()","50bc2849":"pca = PCA(n_components=2, random_state=69420)\npca_features = pca.fit_transform(train_df.iloc[:, :-1])\n\ntemp_df = train_df.copy()\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n\nfor cats in mixed_catagories:\n    # Keep only rows with matching categories\n    temp_df = mask_by_labels(labels=cats)\n    \n    # PCA\n    pca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\n    temp_df['x'] = pca_features[:, 0]\n    temp_df['y'] = pca_features[:, 1]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    sns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\n    plt.title(f\"2d PCA for {title_cats}\", fontsize=20)\n    plt.show()","51c9e376":"pca = PCA(n_components=3, random_state=69420)\ntemp_df = train_df.copy()\npca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['z'] = pca_features[:, 2]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\ntraces = []\nfor s in temp_df.esrb_rating.unique():\n    traces.append(\n        go.Scatter3d(\n            x=temp_df.x[temp_df.esrb_rating == s],\n            y=temp_df.y[temp_df.esrb_rating == s],\n            z=temp_df.z[temp_df.esrb_rating == s],\n                mode='markers',\n            marker=dict(\n                size=6,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5\n                ),\n                opacity=1\n            ),\n            name = s,\n            customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n        )\n    )    \n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=traces, layout=layout)\nfig.update_layout(\n    title={\n        'text': \"3d PCA with all labels\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n)\npy.iplot(fig)\n    \nfor subset in TRAIN_SUBSETS:\n    mask = subset['mask']\n    title = subset['title']\n    to_drop = subset['to_drop']\n\n    # Keep only subset and meaningful columns\n    temp_df = train_df[mask].drop([*to_drop, 'console'], axis=1).copy()\n    \n    # PCA\n    pca_features = pca.fit_transform(StandardScaler().fit_transform(temp_df.iloc[:, :-1]))\n\n    # Prepare axis and labels\n    temp_df['x'] = pca_features[:, 0]\n    temp_df['y'] = pca_features[:, 1]\n    temp_df['z'] = pca_features[:, 2]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    traces = []\n    for s in cats:\n        traces.append(\n            go.Scatter3d(\n                x=temp_df.x[temp_df.esrb_rating == s],\n                y=temp_df.y[temp_df.esrb_rating == s],\n                z=temp_df.z[temp_df.esrb_rating == s],\n                    mode='markers',\n                marker=dict(\n                    size=6,\n                    line=dict(\n                        color='rgba(217, 217, 217, 0.14)',\n                        width=0.5\n                    ),\n                    opacity=1\n                ),\n                name = s,\n                customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n            )\n        )    \n\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    title_cats = f\"3d PCA for '{title}' subset\"\n    fig = go.Figure(data=traces, layout=layout)\n    fig.update_layout(\n        autosize=False,\n        width=800,\n        height=800,\n        title = { 'text': title_cats, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top' }\n    )\n    \n    py.iplot(fig)\n","590c486d":"pca = PCA(n_components=3, random_state=69420)\ntemp_df = train_df.copy()\npca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['z'] = pca_features[:, 2]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\ntraces = []\nfor s in temp_df.esrb_rating.unique():\n    traces.append(\n        go.Scatter3d(\n            x=temp_df.x[temp_df.esrb_rating == s],\n            y=temp_df.y[temp_df.esrb_rating == s],\n            z=temp_df.z[temp_df.esrb_rating == s],\n                mode='markers',\n            marker=dict(\n                size=6,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5\n                ),\n                opacity=1\n            ),\n            name = s,\n            customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n        )\n    )    \n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=traces, layout=layout)\nfig.update_layout(\n    title={\n        'text': \"3d PCA with all labels\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n)\npy.iplot(fig)\n    \nfor subset in TRAIN_SUBSETS:\n    mask = subset['mask']\n    title = subset['title']\n    to_drop = subset['to_drop']\n\n    # Keep only subset and meaningful columns\n    temp_df = train_df[mask].drop([*to_drop, 'console'], axis=1).copy()\n    \n    # PCA\n    pca_features = pca.fit_transform((temp_df.iloc[:, :-1]))\n\n    # Prepare axis and labels\n    temp_df['x'] = pca_features[:, 0]\n    temp_df['y'] = pca_features[:, 1]\n    temp_df['z'] = pca_features[:, 2]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    traces = []\n    for s in cats:\n        traces.append(\n            go.Scatter3d(\n                x=temp_df.x[temp_df.esrb_rating == s],\n                y=temp_df.y[temp_df.esrb_rating == s],\n                z=temp_df.z[temp_df.esrb_rating == s],\n                    mode='markers',\n                marker=dict(\n                    size=6,\n                    line=dict(\n                        color='rgba(217, 217, 217, 0.14)',\n                        width=0.5\n                    ),\n                    opacity=1\n                ),\n                name = s,\n                customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n            )\n        )    \n\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    title_cats = f\"3d PCA for '{title}' subset\"\n    fig = go.Figure(data=traces, layout=layout)\n    fig.update_layout(\n        autosize=False,\n        width=800,\n        height=800,\n        title = { 'text': title_cats, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top' }\n    )\n    \n    py.iplot(fig)\n","1148c331":"pca = PCA(n_components=3, random_state=69420)\ntemp_df = train_df.copy()\npca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['z'] = pca_features[:, 2]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\ntraces = []\nfor s in temp_df.esrb_rating.unique():\n    traces.append(\n        go.Scatter3d(\n            x=temp_df.x[temp_df.esrb_rating == s],\n            y=temp_df.y[temp_df.esrb_rating == s],\n            z=temp_df.z[temp_df.esrb_rating == s],\n                mode='markers',\n            marker=dict(\n                size=6,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5\n                ),\n                opacity=1\n            ),\n            name = s,\n            customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n        )\n    )    \n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=traces, layout=layout)\nfig.update_layout(\n    title={\n        'text': \"3d PCA with all labels\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n)\npy.iplot(fig)\n\n\nfor cats in mixed_catagories:\n    # Keep only rows with matching categories\n    temp_df = mask_by_labels(labels=cats)\n    \n    # PCA\n    pca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\n    # Prepare axis and labels\n    temp_df['x'] = pca_features[:, 0]\n    temp_df['y'] = pca_features[:, 1]\n    temp_df['z'] = pca_features[:, 2]\n    temp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\n    traces = []\n    for s in cats:\n        traces.append(\n            go.Scatter3d(\n                x=temp_df.x[temp_df.esrb_rating == s],\n                y=temp_df.y[temp_df.esrb_rating == s],\n                z=temp_df.z[temp_df.esrb_rating == s],\n                    mode='markers',\n                marker=dict(\n                    size=6,\n                    line=dict(\n                        color='rgba(217, 217, 217, 0.14)',\n                        width=0.5\n                    ),\n                    opacity=1\n                ),\n                name = s,\n                customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n            )\n        )    \n\n    layout = go.Layout(\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0\n        )\n    )\n    title_cats = f\"'{cats[0]}'\" if len(cats) is 1 else \", \".join(f\"'{s}'\" for s in cats[:-1]) + f\" and '{cats[-1]}'\" if len(cats) > 1 else \"\"\n    title_cats = f\"2d PCA for {title_cats}\"\n    fig = go.Figure(data=traces, layout=layout)\n    fig.update_layout(\n        autosize=False,\n        width=800,\n        height=800,\n        title = { 'text': title_cats, 'y':0.9, 'x':0.5, 'xanchor': 'center', 'yanchor': 'top' }\n    )\n    \n    py.iplot(fig)\n","0aae4d1f":"vt = VarianceThreshold(threshold=.016)\n\ntemp_df = train_df.copy()\nX = temp_df.iloc[:, :-1]\nvt_mask = np.append(vt.fit(X).get_support(), [True])","14c9ae38":"vt = VarianceThreshold(threshold=.016)\n\ntemp_df = train_df.copy()\nvt_mask = np.append(vt.fit(temp_df.iloc[:, :-1]).get_support(), [True]) # True for label\ncorr_mask = np.array(train_df.corr()['esrb_rating'].abs() > 0.03)\n\ntemp_df.loc[:, vt_mask & corr_mask]\n\n\npca_features = pca.fit_transform(train_df.iloc[:, :-1])","0243f16f":"temp_df.loc[:, vt_mask & corr_mask]","0a2cbcca":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nvt = VarianceThreshold(threshold=.016)\n\ntemp_df = train_df.copy()\nvt_mask = np.append(vt.fit(temp_df.iloc[:, :-1]).get_support(), [True]) # True for label\ncorr_mask = np.array(train_df.corr()['esrb_rating'].abs() > 0.03)","7c8eac64":"pca = PCA(n_components=2, random_state=69420)\ntemp_df = train_df.copy().loc[:, vt_mask & corr_mask]\npca_features = pca.fit_transform(StandardScaler().fit_transform(temp_df.iloc[:, :-1]))\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n\npca = PCA(n_components=2, random_state=69420)\ntemp_df = train_df.copy().loc[:, vt_mask & corr_mask]\npca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)\nplt.show()\n","39460d9c":"pca = PCA(n_components=2, random_state=69420)\ntemp_df = train_df.copy().loc[:, vt_mask & corr_mask]\npca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nsns.scatterplot(x='x', y='y', hue='esrb_rating', data=temp_df)","5185e5ae":"pca = PCA(n_components=3, random_state=69420)\ntemp_df = train_df.copy().loc[:, vt_mask & corr_mask]\npca_features = pca.fit_transform(StandardScaler().fit_transform(temp_df.iloc[:, :-1]))\n\ntemp_df = train_df.copy()\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['z'] = pca_features[:, 2]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\nfig = plt.figure(figsize=(20, 14))\nax = fig.add_subplot(111, projection='3d')\n\nfor s in temp_df.esrb_rating.unique():\n    ax.scatter(temp_df.x[temp_df.esrb_rating == s], temp_df.y[temp_df.esrb_rating == s], temp_df.z[temp_df.esrb_rating == s],label=s)\n    \nax.legend()\nplt.show()","4a2e6a2f":"pca = PCA(n_components=3, random_state=69420)\ntemp_df = train_df.copy().loc[:, vt_mask & corr_mask]\npca_features = pca.fit_transform(StandardScaler().fit_transform(temp_df.iloc[:, :-1]))\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['z'] = pca_features[:, 2]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\ntraces = []\nfor s in temp_df.esrb_rating.unique():\n    traces.append(\n        go.Scatter3d(\n            x=temp_df.x[temp_df.esrb_rating == s],\n            y=temp_df.y[temp_df.esrb_rating == s],\n            z=temp_df.z[temp_df.esrb_rating == s],\n                mode='markers',\n            marker=dict(\n                size=6,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5\n                ),\n                opacity=1\n            ),\n            name = s,\n            customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n        )\n    )    \n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig)\n\npca = PCA(n_components=3, random_state=69420)\ntemp_df = train_df.copy().loc[:, vt_mask & corr_mask]\npca_features = pca.fit_transform(temp_df.iloc[:, :-1])\n\ntemp_df['x'] = pca_features[:, 0]\ntemp_df['y'] = pca_features[:, 1]\ntemp_df['z'] = pca_features[:, 2]\ntemp_df['esrb_rating'] = esrb_rating_le.inverse_transform(temp_df['esrb_rating'])\n\ntraces = []\nfor s in temp_df.esrb_rating.unique():\n    traces.append(\n        go.Scatter3d(\n            x=temp_df.x[temp_df.esrb_rating == s],\n            y=temp_df.y[temp_df.esrb_rating == s],\n            z=temp_df.z[temp_df.esrb_rating == s],\n                mode='markers',\n            marker=dict(\n                size=6,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5\n                ),\n                opacity=1\n            ),\n            name = s,\n            customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n        )\n    )    \n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig)","12fafc9a":"traces = []\nfor s in ['E', 'ET']:\n    traces.append(\n        go.Scatter3d(\n            x=temp_df.x[temp_df.esrb_rating == s],\n            y=temp_df.y[temp_df.esrb_rating == s],\n            z=temp_df.z[temp_df.esrb_rating == s],\n                mode='markers',\n            marker=dict(\n                size=6,\n                line=dict(\n                    color='rgba(217, 217, 217, 0.14)',\n                    width=0.5\n                ),\n                opacity=1\n            ),\n            name = s,\n            customdata=temp_df.drop(['x','y','z'], axis=1)[temp_df.esrb_rating == s]\n        )\n    )    \n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=traces, layout=layout)\npy.iplot(fig)","33ded7b7":"\n(esrb_rating_le.inverse_transform(train_df['esrb_rating']) == 'E').sum()","c1f141ae":"\n(esrb_rating_le.inverse_transform(train_df['esrb_rating']) == 'ET').sum()","7fef8177":"# Make figure readable lol\nplt.figure(figsize=(20, 14)) # default 6.4, 4.8\n\nX = train_df.iloc[:, :-1]\nX.boxplot()\n\n# Make plot labels readable\nplt.xticks(rotation = 45)\nplt.show()","eca71c86":"# Make figure readable lol\nplt.figure(figsize=(20, 14)) # default 6.4, 4.8\n\n# Display Normalized boxplot\nnormalized_df = X \/ X.mean()\nnormalized_df.boxplot()\n\n# Make plot labels readable\nplt.xticks(rotation = 45)\nplt.show()","27d93475":"X.var()","ba0fbd03":"vt = VarianceThreshold(threshold=.15)\nvt.fit(X)","f5a9a8e4":"vt.get_support()","60c176dc":"train_df.animated_blood.sum()","86f84fb2":"# TODO : CV describe data\ndef searchCV(classifier, params, method=\"grid\", n_iter=100, cv=4, random_state=69420, n_jobs=-1, print_scores=False, verbose=0, scoring='accuracy'):\n    METHODS = {'grid': GridSearchCV, 'random': RandomizedSearchCV}\n    if type (method) is not str or method.lower() not in METHODS.keys():\n        method_keys = list(METHODS.keys())\n        methods_str = \", \".join(f\"'{s}'\" for s in method_keys[:-1]) + f\" and '{method_keys[-1]}'\" if len(method_keys) > 1 else \"\"\n        raise ValueError(f\"Unkown method '{method}'. Allowed values are: {methods_str}.\")\n    \n    model = METHODS[method](classifier, params, scoring=scoring, cv=cv, n_jobs=n_jobs, **({'random_state': random_state, 'n_iter': n_iter} if method is 'random' else {}), verbose=verbose)\n    \n    if print_scores:\n        cv_score = cross_val_score(model, X, y, cv=15)\n        print(cv_score)\n        print(cv_score.mean())\n\n    return model","7f06ac2a":"TRAIN_SUBSETS__1 = TRAIN_SUBSETS[1:3]\nTRAIN_SUBSETS__1.append({\n    'mask': pd.Series([True] * len(train_df.index)),\n    'title': 'All',\n    'to_drop': []\n})","d6fb318e":"def train_eval_model(name, classifier, params=None, random_state=69420, models=None):\n    if models is not None and type(models) is dict:\n        models[name] = {}\n    \n    for subset in TRAIN_SUBSETS__1:\n        mask = subset['mask']\n        title = subset['title']\n        to_drop = subset['to_drop']\n        subset[f\"cls__{name}\"] = {}\n        \n        sub_df = train_df[mask].copy()\n\n        X = sub_df.drop([*to_drop, 'console'], axis=1).iloc[:, :-1]\n        y = sub_df.drop([*to_drop, 'console'], axis=1).iloc[:, -1]\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=random_state, stratify=y, shuffle=True)\n\n        param_space = {\n#             'pca__n_components': range(2,21)\n#             'model__probability': [True]\n        } if params is None else params\n        if 'random_state' in inspect.signature(classifier.__init__).parameters.keys():\n            param_space['model__random_state'] = [random_state]\n        \n        pipe = Pipeline([\n#             ('scaler', StandardScaler()),\n#             ('pca', PCA()),\n            ('model', classifier())\n        ])\n\n        pipe = searchCV(pipe, param_space, method='grid')\n        pipe.fit(X_train, y_train)\n\n        # Save results\n        subset[f\"cls__{name}\"]['score'] = pipe.score(X_test, y_test)\n        subset[f\"cls__{name}\"]['best_params'] = pipe.best_params_\n        subset[f\"cls__{name}\"]['cls_report'] = classification_report(esrb_rating_le.inverse_transform(y_test), esrb_rating_le.inverse_transform(pipe.predict(X_test)))\n        subset[f\"cls__{name}\"]['estimator'] = pipe\n        subset[f\"cls__{name}\"]['best_estimator'] = pipe.best_estimator_\n        \n        # Preserve model\n        if models is not None and type(models) is dict:\n            models[name][title] = pipe\n\n        print(); print();\n        print(f\"{name} finished training '{title}' subset\")\n        print(subset[f\"cls__{name}\"]['cls_report'])\n        print('\\t\\tparams:', subset[f\"cls__{name}\"]['best_params'])\n        print(\"Score:\", subset[f\"cls__{name}\"]['score'].round(6))\n        print(); print();","055e8562":"saved_models = {}","147c89ae":"train_eval_model('Random Forest', RandomForestClassifier, models=saved_models)","db694941":"train_eval_model('Random Forest', RandomForestClassifier, params={'model__n_estimators': [300, 600, 800]}, models=saved_models)","d7995bd4":"train_eval_model('DecisionTreeClassifier', DecisionTreeClassifier, models=saved_models)","1c9f439f":"train_eval_model('ExtraTreesClassifier', ExtraTreesClassifier, models=saved_models)","4c22c740":"train_eval_model('ExtraTreesClassifier', ExtraTreesClassifier, params={'model__criterion': ['entropy']}, models=saved_models)","58fe3cc6":"train_eval_model('LogisticRegression', LogisticRegression, models=saved_models)","7a78ee74":"train_eval_model('SVC', SVC, params={'model__probability': [True]}, models=saved_models)","05518df3":"train_eval_model('SVC', SVC, params={'model__probability': [True], 'model__gamma': ['scale', 'auto'], 'model__kernel': ['linear', 'poly', 'rbf', 'sigmoid']}, models=saved_models)","5c2b701d":"train_eval_model('KNeighborsClassifier', KNeighborsClassifier, params={'model__n_neighbors': [3,4,5]}, models=saved_models)","d6b52e25":"train_eval_model('GradientBoostingClassifier', GradientBoostingClassifier, models=saved_models)","7e948537":"train_eval_model('XGBClassifier', XGBClassifier, params={'model__verbosity': [0], 'model__silent': [True]}, models=saved_models)","feb59e6d":"# train_eval_model('CatBoostClassifier', CatBoostClassifier, params={'model__logging_level': ['Silent']}, models=saved_models)","cbc7fcc2":"# train_eval_model('LGBMClassifier', LGBMClassifier)","7b67e64f":"# Re-arrange models according to subsets\nvoting_models_subsets = {}\nfor model_name, subsets in saved_models.items():\n    for subset_name, model in subsets.items():\n        print(model_name, subset_name)\n        if subset_name not in voting_models_subsets:\n            voting_models_subsets[subset_name] = {}\n        \n        voting_models_subsets[subset_name][model_name] = model","b6cc1c9b":"required_estimators = [(name, model) for name, model in voting_models_subsets['All'].items() if name in ['KNeighborsClassifier', 'SVC', 'LogisticRegression', 'Random Forest', 'DecisionTreeClassifier']]\n\n# Use all data\nsubset = TRAIN_SUBSETS__1[2]\nmask = subset['mask']\ntitle = subset['title']\nto_drop = subset['to_drop']\n\nsub_df = train_df[mask].copy()\n\n# Split Data\nX = sub_df.drop([*to_drop, 'console'], axis=1).iloc[:, :-1]\ny = sub_df.drop([*to_drop, 'console'], axis=1).iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=69420, stratify=y, shuffle=True)\n\n# Evaluate Models\nfor idx, model_info in enumerate(required_estimators):\n    name, model = model_info\n\n    print(f\"   ({idx+1}) {name}\")\n    print(f\"Score: {model.score(X_test, y_test)}\")\n    print('CV Describe:')\n    print(f\"{pd.DataFrame(cross_val_score(model, X, y, cv=8)).describe().to_string()}\")\n    \n    if (idx + 1 < len(required_estimators)):\n        print(\"\\n\\n\\n\")","b64df443":"to_include = []\nto_exclude = ['XGBClassifier', 'DecisionTreeClassifier', 'Random Forest']\n\n# Split estimator groups according to subset\nestimators_0 = [(name, model) for name, model in voting_models_subsets['Not M'].items() if name not in to_exclude]\nestimators_1 = [(name, model) for name, model in voting_models_subsets['Not Not M'].items() if name not in to_exclude]\nestimators_2 = [(name, model) for name, model in voting_models_subsets['All'].items() if name not in to_exclude]","55617fac":"# Submission\nsubset = TRAIN_SUBSETS__1[2]\nmask = subset['mask']\ntitle = subset['title']\nto_drop = subset['to_drop']\n\nsub_df = train_df[mask].copy()\nestimator_names = f\"{estimators_2[0][0]}\" if len(estimators_2) is 1 else \", \".join(f\"{s[0]}\" for s in estimators_2[:-1]) + f\" and {estimators_2[-1][0]}\" if len(estimators_2) > 1 else \"\"\n\nX = sub_df.drop([*to_drop, 'console'], axis=1).iloc[:, :-1]\ny = sub_df.drop([*to_drop, 'console'], axis=1).iloc[:, -1]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=69420, stratify=y, shuffle=True)\n\neslf2 = VotingClassifier(estimators=estimators_2, voting='hard')\neslf2.fit(X_train, y_train)\n# print(eslf2.score(X_test, y_test))\n# print(classification_report(esrb_rating_le.inverse_transform(y_test), esrb_rating_le.inverse_transform(eslf2.predict(X_test))))\n\nprint(f\"(1) Hard Voting Classifier\\n({estimator_names})\\n\")\nprint(f\"Score: {eslf2.score(X_test, y_test)}\")\nprint(classification_report(esrb_rating_le.inverse_transform(y_test), esrb_rating_le.inverse_transform(eslf2.predict(X_test))))\nprint('CV Describe:')\nprint(f\"{pd.DataFrame(cross_val_score(eslf2, X, y, cv=8)).describe().to_string()}\")\nprint('\\n\\n\\n')\n\neslf2 = VotingClassifier(estimators=estimators_2, voting='soft')\neslf2.fit(X_train, y_train)\n# print(eslf2.score(X_test, y_test))\n# print(classification_report(esrb_rating_le.inverse_transform(y_test), esrb_rating_le.inverse_transform(eslf2.predict(X_test))))\n\nprint(f\"(2) Soft Voting Classifier\\n({estimator_names})\\n\")\nprint(f\"Score: {eslf2.score(X_test, y_test)}\")\nprint(classification_report(esrb_rating_le.inverse_transform(y_test), esrb_rating_le.inverse_transform(eslf2.predict(X_test))))\nprint('CV Describe:')\nprint(f\"{pd.DataFrame(cross_val_score(eslf2, X, y, cv=8)).describe().to_string()}\")\nprint('\\n\\n\\n')\n\n\nsubmission_df = test_df[['id']].copy()\nsubmission_df['esrb_rating'] = -1\n\nsubmission_df['esrb_rating'] = esrb_rating_le.inverse_transform(eslf2.predict(test_df.drop(['id', 'console'], axis=1)))\nsubmission_df.to_csv(f'submssion_voting__4.csv',index=False)\n\nprint(\"Submission Predictions:\")\nprint(submission_df.to_string())","88e488d4":"# Required Models","b6bc8716":"# Models","d87f2a90":"### Check for missing values","47e87609":"#### t-SNE conclusion\nData is beautiful, and so is the plot above.\\\nFeature extraction demostrates that we have two clustered ranking groups with similarities: \n1) **E** *(Everyone)* **ET** *(Everyone 10+)* are similar\\\n2) **T** *(Teen)* and **M** *(Mature)* are similar\n\nThat is obvious, but our features can be reduced to reach this conclusion.\\\nFurthermore, we can see that apparently **M** (Mature) is more similar to **E** and **ET** than **T** *(Teen)*","2a4ae758":"### Add a \"subset\" for all entries and features\nThis model would use all original features, excluding `console`.\\\nTherefore the mask is `True` for all entries and `to_drop` is an empty list.","6fe17d9e":"#### Missing values conclusion\nWe can see that both **train** and **test** sets do not have any missing data. :)","898a6f5e":"### Looking at Variance","208d234a":"### Correlations","38eb0063":"## EDA conclusion\nThere's no missing data for us to deal with, and our entire dataset features consist of zeros (0s) and ones (1s).\\\nTraining with PCA did not yield any better results, so did splitting our data into subcategories for sub-models.\n\nOur best model, as seen bellow, was an ensemble of multiple base models.","ed248103":"## Train base models","1e036585":"#### \\* With a Little Help from My Friends _(methods*)_","91e8f76b":"### Test data analysis\nWe can separate the data and train more specific models","d1338d27":"# EDA","98d9543e":"## Preprocess Data","e01382c2":"### t-SNE visualization\nOur dataset is high-dimensional dataset so we will use t-Stochastic Neighbor Embedding (t-SNE) to visualize our data.","1c0fcaf0":"#### Correlation conclusion\nWe have so many features that's it's to early to conclude anything specific.\\\nA lot of features have a very low correlation to the `esrb_rating`, and some have a more substantial correlation.\\\nWe should consider feature reduction of some sorts.","cab641c1":"### Check data types\nInspect the type of data present in our datasets","cec7b1ef":"#### Values and type exploration conclusion\nHooray! Our features are all binary!","6ff76b28":"# Voting Classifier Submission\nWe will use voting classifier for our submission"}}