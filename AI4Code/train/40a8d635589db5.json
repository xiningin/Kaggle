{"cell_type":{"0df011a5":"code","39607011":"code","82d4ce23":"code","ccd129ee":"code","50ce07da":"code","668bde45":"code","76c3cb9d":"code","8f7c3917":"code","71d8453b":"code","e148c3ac":"code","322c45c5":"code","c3ad8e16":"code","166a6297":"code","6d995a8e":"code","371ff79d":"code","75b1505b":"code","d5ce6eda":"code","a34ffda9":"code","7e204058":"code","7f49bbb5":"code","3daeb405":"code","3772f0d3":"code","c0c58a16":"code","c6634f46":"code","fb69d3b4":"code","db44d998":"code","ce05d664":"code","be9d2790":"code","eca077cf":"code","e679c0c5":"code","66b6d01b":"code","00025520":"code","7a734249":"code","4540a3c5":"code","6c1e0e37":"code","f4f2d15b":"code","2b77b245":"code","75ea59d1":"code","47edb075":"code","d43f2949":"code","20fabb39":"code","2b05925b":"code","8537c3e7":"code","9dd0df36":"code","de3c33fd":"code","e8f60b1b":"code","a8d3d71f":"code","e276a9b6":"code","cd4bf406":"code","2e9444d9":"code","fe5e57ed":"code","6c02910d":"code","91e80e54":"code","d7bff437":"code","319dd0d2":"code","1f90e77c":"code","16ed78b3":"code","96c81ee0":"code","54b5eedb":"code","594809b7":"code","c38da757":"code","6c2caca5":"code","03eeb07c":"code","ef8a2a08":"code","e2b57aa0":"code","ff1d9b65":"code","f8266f14":"code","f45fc41e":"code","1140bb7e":"code","afbcd233":"code","04f8ae24":"code","ae276f73":"code","cd804504":"code","a67ba0c4":"code","7614695a":"code","ae80f82d":"code","7b571d91":"code","02706b96":"code","18c1d2ec":"code","ad0d20db":"code","823ab067":"code","6ed3a28c":"code","b61f72af":"code","4b3c1554":"code","35e32a77":"code","441c1c00":"code","66a79c18":"code","b665caed":"code","c7629927":"code","01bf531d":"code","ee0e4d16":"code","b16f78b2":"code","962f5cbd":"code","4777c051":"code","13e8fcc4":"code","b3662772":"code","6fddb4ed":"code","f9d5458b":"code","cb075608":"code","8f34ce81":"code","bae82d09":"code","ae649cf8":"code","5f624bb6":"code","c407a548":"code","cba87825":"code","937171b1":"code","ce7dedf4":"code","d18ecc46":"code","47b1f56e":"code","a47e748f":"code","a46c9090":"code","1a3c6886":"code","e3e582c3":"code","83eb92c0":"code","d176840e":"code","a789c834":"code","76c181f5":"markdown","fda72e29":"markdown"},"source":{"0df011a5":"#!pip install --upgrade pip\n!pip install gdown","39607011":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","82d4ce23":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","ccd129ee":"!pip install gdown","50ce07da":"\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#To parellize the task\nimport dask.dataframe as dd\ndata = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/app_events.csv')\napp_events = pd.DataFrame(data)\napp_events.head()\napp_events=reduce_mem_usage(app_events)","668bde45":"data = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/app_labels.csv')\napp_labels = pd.DataFrame(data)\napp_labels.head()\napp_labels=reduce_mem_usage(app_labels)","76c3cb9d":"data = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/events.csv')\nevents = pd.DataFrame(data)\nevents.head()\nevents=reduce_mem_usage(events)","8f7c3917":"#data = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/gender_age_test.csv')\n#gender_age_test = pd.DataFrame(data)\n#gender_age_test.head()\ndata = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/gender_age_train.csv')\ngender_age = pd.DataFrame(data)\ngender_age.head()\ngender_age=reduce_mem_usage(gender_age)\n#gender_age = pd.concat([gender_age_test,gender_age_train],axis=0)\n#gender_age.head()","71d8453b":"data = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/label_categories.csv')\nlabel_categories = pd.DataFrame(data)\nlabel_categories.head()\nlabel_categories=reduce_mem_usage(label_categories)","e148c3ac":"data = pd.read_csv('\/kaggle\/input\/china-mobile-user-gemographics\/phone_brand_device_model.csv')\nphone_device = pd.DataFrame(data)\nphone_device.head()\nphone_device=reduce_mem_usage(phone_device)","322c45c5":"#Check for duplicates in phone_device dataframe\n#There are several duplicates in the dataframe which we would need to drop\npd.concat(g for _, g in phone_device.groupby(\"device_id\") if len(g) > 1)","c3ad8e16":"#Drop the duplicates and check if they have actually dropped\nphone_device2 = phone_device.drop_duplicates(subset=['device_id'])\nprint(phone_device2[phone_device2.device_id==-9194249084574705214])\nprint(phone_device2[phone_device2.device_id==9196371203514832504])","166a6297":"app_event_label = pd.merge(app_events, app_labels, on='app_id',how='inner')\n#del [[app_events,app_labels]]\n#import gc as gc\n#gc.collect()\ndel app_events\ndel app_labels\n#app_events=pd.DataFrame()\n#app_labels=pd.DataFrame()\napp_event_label.head()\n#\n#app_event_label.head()\napp_event_label=reduce_mem_usage(app_event_label)\napp_event_label.drop(['app_id'],axis=1,inplace=True)","6d995a8e":"app_event_label2 = pd.merge(app_event_label,events, on='event_id',how='inner')\ndel app_event_label\ndel events\n#app_event_label=pd.DataFrame()\n#events=pd.DataFrame()\napp_event_label2.head()\napp_event_label2=reduce_mem_usage(app_event_label2)\napp_event_label2.drop(['event_id'],axis=1,inplace=True)","371ff79d":"#app_event_label2.drop(['event_id'],axis=1,inplace=True)","75b1505b":"app_event_label_gender = pd.merge(app_event_label2,gender_age, on='device_id')\ndel app_event_label2\ndel gender_age\napp_event_label_gender.head()\napp_event_label_gender=reduce_mem_usage(app_event_label_gender)\n#app_event_label2.drop(['event_id'],axis=1,inplace=True)\n","d5ce6eda":"app_event_label_gender2 = pd.merge(app_event_label_gender,label_categories, on='label_id')\ndel app_event_label_gender\ndel label_categories\napp_event_label_gender2.head()\napp_event_label_gender2=reduce_mem_usage(app_event_label_gender2)\n","a34ffda9":"app_event_label_gender_phone = pd.merge(app_event_label_gender2,phone_device, on='device_id')\ndel app_event_label_gender2\ndel phone_device\napp_event_label_gender_phone.head()\napp_event_label_gender_phone=reduce_mem_usage(app_event_label_gender_phone)","7e204058":"app_event_label_gender_phone.head()","7f49bbb5":"#The data frame has 79,061,565 (~80 MM) rows and 13 columns\n#In total there were 15 columns, we dropped 2 columns due to datasize issue during merge step \n#namely app_id and event_id therefore current dataset has 13 columns\napp_event_label_gender_phone.shape","3daeb405":"#Different datatypes \n#is_installed, is_active, label_id, device_id and age are integers\n#longitude, latitude are float\n#gender, group,category, phone_brand and device_model are objects (categorical variables)\napp_event_label_gender_phone.dtypes","3772f0d3":"#Taking a 0.1% sample (5% sample is causing lot of memory issues)\ndf = app_event_label_gender_phone.sample(frac=0.001,  random_state=1111)","c0c58a16":"del app_event_label_gender_phone","c6634f46":"df.head()","fb69d3b4":"#Keep only necessary columns and drop the columns used for joining like label_id, device_id etc\n#We can see that phone_brand and device_model have part English and part Chinese characters\ndf2 = df.drop(['label_id','device_id'],axis=1)\ndf2.head()","db44d998":"#Predominantly the apps are inactive and male people have majority in both active \n#and non active segment\nsns.countplot(x ='is_active', hue = \"gender\", data = df2); ","ce05d664":"#Very interesting plot few age groups are exclusively female while few are only male\na4_dims = (20, 10)\nfig, ax = pyplot.subplots(figsize=a4_dims)\nsns.countplot(x ='group', hue = \"gender\", data = df2); ","be9d2790":"#conda install googletrans","eca077cf":"#In a 5% sample we have 3,953,078 (~4MM) rows and 11 columns\n#In a 0.01% sample we have 79,062 (~80K) rows and 11 columns\ndf2.shape","e679c0c5":"#import googletrans\n#from googletrans import Translator\n#translator = Translator()\n#result = translator.translate('Mit\u00e4 sin\u00e4 teet')\n#print(result.text)\n#df2['phone_brand_eng'] = str(df2.phone_brand)\n#print(df2.head())\n#df2['device_model_eng'] = str(df2.device_model)\n#print(df2.head())\n#print(df2['phone_brand_eng'].unique)\n#rint(str(df.iloc[0, df.columns.get_loc('phone_brand')]))\n#import re\n#for x in range(0, df.shape[0]):\n#    result = translator.translate(str(df2.iloc[x, df2.columns.get_loc('phone_brand')]))\n#    s = result.text\n#    df2.iloc[x, df2.columns.get_loc('phone_brand_eng')] = re.sub('[^a-zA-Z]+', '', s)    \n#print(df.phone_brand_eng)","66b6d01b":"#a4_dims = (20, 10)\n#fig, ax = pyplot.subplots(figsize=a4_dims)\n#sns.countplot(x ='phone_brand_eng',  data = df2); ","00025520":"#The 5 number summary of the numerical data\n#We see that longitude and latitude has mean as NaN (Not a number) and standard deviation as zero\n#This means we will not be able to normalize latitude and longitude as mean and std deviation\n#are not having valid values\n#We would need to check if we have missing values there and in that case we would need to impute or remove \ndf2.describe().transpose()","7a734249":"#All mobiles have app installed - no zero values in the random sample\n#So this column is practically useless for us and can be removed later\n#app_event_label_gender_phone\ndf2[\"is_installed\"].value_counts()","4540a3c5":"#About 66% users are not actively using the app and 34% are actively using\ndf2[\"is_active\"].value_counts(normalize=True)","6c1e0e37":"#Timestamp lot of detailed value in terms of both date and time\n#What we can do here is create few derived variables out of this like date and then time\n#Time can then further be classified as morning, afternoon, evening, night etc\n#However due to lot of memory challenges was unable to explore this part\ndf2[\"timestamp\"].value_counts(normalize=True)","f4f2d15b":"#from datetime import datetime\n\n#df2['date'] = df2['timestamp'].map(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").date)\n#df2['time'] = df2['timestamp'].map(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").time)\n#df2[\"date\"] = datetime.strptime(str(df2[\"timestamp\"]),\"%d\/%m\/%Y %H:%M\")\n#print(df2[\"date\"]);","2b77b245":"#Most of the users have longitude of 0 and 1 - probably means data is missing \n#China is not near the equator but much above it which is having latitude of 0\ndf2[\"longitude\"].value_counts()","75ea59d1":"#Most of the users have longitude of 0 and 1 - probably means data is missing or invalid\n#China is not at prime meridian which runs through Greenwich in England giving us GMT\ndf2[\"latitude\"].value_counts()","47edb075":"#74% male population and 26% female population - Data skewed towards male group\ndf2[\"gender\"].value_counts(normalize=\"True\")","d43f2949":"#Data is fairly well distributed among age group for male population - prefix M\n#Data is more skewed towards older women in the population (Age 29 and above) - prefix F\ndf2[\"group\"].value_counts(normalize=\"True\")","20fabb39":"#Most of the apps belong to Industry tag, Property industry and services\n#A large number is unknown as well\ndf2[\"category\"].value_counts()","2b05925b":"#With help of Google translate the top few Chinese phone brands are Huawei, Xiaomi, Samsung, Meizu and Oppo below\ndf2[\"phone_brand\"].value_counts()","8537c3e7":"#Top few device models are Glory 6, Mate 7, MI 4, Honor 6 Plus, Galaxy Note 3 etc\ndf2[\"device_model\"].value_counts()","9dd0df36":"#A lot of categorical variables are there which we need to convert to numerical for clustering\n#Convert gender into dummy\/indicator variables using One Hot Encoding\ndf3 = pd.get_dummies(df2, columns=[\"gender\"])","de3c33fd":"df3.head()","e8f60b1b":"df4 = pd.get_dummies(df3, columns=[\"group\"])","a8d3d71f":"df4.head()\ndel df3\n#del df2\ndel df","e276a9b6":"df5 = pd.get_dummies(df4, columns=[\"category\"])\ndel df4","cd4bf406":"df5.head()","2e9444d9":"#df6 = pd.get_dummies(df5, columns=[\"phone_brand\"])\ndf6 = df5","fe5e57ed":"del df5","6c02910d":"\ndf6.head()","91e80e54":"#Majority of the values for latitude and longitude are clustered at values 0 and 1 as noted earlier\n#We would not be able to normalize latitude and longitude as std deviation is zero and mean is NaN\n#So we will drop it from further analysis\n#Also is_installed is not useful as it has value of 1 throughout and will be dropped later\ndf6.describe().transpose()","d7bff437":"# Around 1.5MM rows and 535 columns in the dataset for 5%\n# Around 80K rows and 384 columns in the dataset for 0.01%\ndf6.shape","319dd0d2":"#Check for null values - No null values found\ndf6.isnull().sum()","1f90e77c":"#No NA values found\ndf6.isna().sum()","16ed78b3":"#Datetime is an object - let us change that into datetime\ndf6.dtypes","96c81ee0":"df6[\"timestamp\"] = pd.to_datetime(df6[\"timestamp\"])","54b5eedb":"df6.dtypes","594809b7":"import datetime\ndf6[\"day\"] = pd.to_datetime(df6[\"timestamp\"].dt.strftime('%Y-%m-%d'))\ndf6[\"day\"].value_counts()","c38da757":"#Timestamp and day variable could not be used due to memory issues\n#latitude and longitude values could not be standardized as std dev is zero and mean is NaN\n#Device model and phone brand are having Chinese values so could not gather much information\n#Is_installed is 1 throughout so practically useless for clustering\ndf7 = df6.drop(['timestamp','device_model','day','latitude','longitude','phone_brand','is_installed'],axis=1)\n#del df6","6c2caca5":"# For running the clustering algorithm, we would require the data to be scaled\ndf7.dtypes","03eeb07c":"#Scaled the data using zscore\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import zscore\nnp.seterr(divide='ignore', invalid='ignore')\n# 4 samples\/observations and 2 variables\/features\n#X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])# the scaler object (model)\nscaler = StandardScaler()# fit and transform the data\n#scaled_data = scaler.fit_transform(df7)\nscaled_data = df7.apply(zscore)","ef8a2a08":"scaled_data.head()","e2b57aa0":"#Hopkins Statistic is a way of measuring the cluster tendency of a data set.\nfrom sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) # heuristic from article [1]\n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), \n                                    2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H\n","ff1d9b65":"#Checking whether data can be clustered through Hopkins Coefficient\n#A very high coefficient value (~0.99) shows that the data can be clustered well\nNum_features = scaled_data.select_dtypes(include=[np.number]).columns\nhopkins(scaled_data[Num_features])","f8266f14":"#sns.pairplot(scaled_data,diag_kind='kde')","f45fc41e":"#Finding optimal no. of clusters\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\nclusters=range(1,10)\nmeanDistortions=[]\n\nfor k in clusters:\n    model=KMeans(n_clusters=k)\n    model.fit(scaled_data)\n    prediction=model.predict(scaled_data)\n    meanDistortions.append(sum(np.min(cdist(scaled_data, model.cluster_centers_, 'euclidean'), axis=1)) \n                           \/ scaled_data.shape[0])\n\n\nplt.plot(clusters, meanDistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')\n","1140bb7e":"# Let us first start with K = 2 as elbow appears to be at 3\nfinal_model=KMeans(3)\nfinal_model.fit(scaled_data)\nprediction=final_model.predict(scaled_data)\n\n#Append the prediction \ndf7[\"GROUP\"] = prediction\n#scaled_data[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ndf7.head()","afbcd233":"#The two clusters are made on the basis of gender - one is male another is female\nDataClust = df7.groupby(['GROUP'])\nDataClust.mean()","04f8ae24":"# Let us try with K = 6 (slight elbow is there)\nfinal_model=KMeans(6)\nfinal_model.fit(scaled_data)\nprediction=final_model.predict(scaled_data)\n\n#Append the prediction \n#tech_supp_df[\"GROUP\"] = prediction\ndf7[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ndf7.head()","ae276f73":"#The five clusters are made on the basis of gender - one is male another is female, other three clusters are a \n#mix of male and female group \nDataClust = df7.groupby(['GROUP'])\nDataClust.mean()","cd804504":"# Let us try with K = 8 as there is an elbow there as well\nfinal_model=KMeans(8)\nfinal_model.fit(scaled_data)\nprediction=final_model.predict(scaled_data)\n\n#Append the prediction \ndf7[\"GROUP\"] = prediction\n#scaled_data[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ndf7.head()","a67ba0c4":"DataClust = df7.groupby(['GROUP'])\nDataClust.mean()","7614695a":"DataClust = df7.groupby(['GROUP'])\nDataClust.mean()","ae80f82d":"#from scipy.cluster.hierarchy import cophenet, dendrogram, linkage\n#from scipy.spatial.distance import pdist\n#Z = linkage(scaled_data, method = 'centroid', metric = 'euclidean')\n#c, coph_dists = cophenet(Z,pdist(scaled_data))","7b571d91":"# PCA\n# Step 1 - Create covariance matrix\n\ncov_matrix = np.cov(scaled_data.T)\nprint('Covariance Matrix \\n%s', cov_matrix)\n","02706b96":"# Step 2- Get eigen values and eigen vector\neig_vals, eig_vecs = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eig_vecs)\nprint('\\n Eigen Values \\n%s', eig_vals)","18c1d2ec":"tot = sum(eig_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","ad0d20db":"plt.plot(var_exp)","823ab067":"# Ploting \nplt.figure(figsize=(10 , 5))\nplt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","6ed3a28c":"#We need atleast 350 out of the 378 to get proper coverage of variance\nscaled_data.shape","b61f72af":"from sklearn.decomposition import PCA\n\n# NOTE - we are generating only 350 PCA dimensions (dimensionality reduction from 379 to 350)\n\npca = PCA(n_components=350)\ndata_reduced = pca.fit_transform(scaled_data)\ndata_reduced.transpose()","4b3c1554":"pca.components_","35e32a77":"#df_comp = pd.DataFrame(pca.components_,columns=list(scaled_data))\n#df_comp.head()","441c1c00":"#Finding optimal no. of clusters\nfrom scipy.spatial.distance import cdist\nfrom sklearn.cluster import KMeans\nclusters=range(1,10)\nmeanDistortions=[]\n\nfor k in clusters:\n    model=KMeans(n_clusters=k)\n    model.fit(data_reduced)\n    prediction=model.predict(data_reduced)\n    meanDistortions.append(sum(np.min(cdist(data_reduced, model.cluster_centers_, 'euclidean'), axis=1)) \n                           \/ data_reduced.shape[0])\n\n\nplt.plot(clusters, meanDistortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Average distortion')\nplt.title('Selecting k with the Elbow Method')","66a79c18":"#We see a very clear cluster at k = 3\n#Let us use k-means to do the clustering\n# Let us try with K = 5 as there is an elbow there as well\nfinal_model=KMeans(3)\nfinal_model.fit(data_reduced)\nprediction=final_model.predict(data_reduced)\n\n#Append the prediction \ndf7[\"GROUP\"] = prediction\n#scaled_data[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ndf7.head()","b665caed":"#Very clear clusters formed 1 for female and 3 for male, another group is 84% male and 16% female\n#The last group has overlapping interests\nDataClust = df7.groupby(['GROUP'])\nDataClust.mean()","c7629927":"final_model=KMeans(5)\nfinal_model.fit(data_reduced)\nprediction=final_model.predict(data_reduced)\n\n#Append the prediction \ndf7[\"GROUP\"] = prediction\n#scaled_data[\"GROUP\"] = prediction\nprint(\"Groups Assigned : \\n\")\ndf7.head()","01bf531d":"#Very clear clusters formed 1 for female and 3 for male, another group is 84% male and 16% female\n#The last group has overlapping interests\nDataClust = df7.groupby(['GROUP'])\nDataClust.mean()","ee0e4d16":"#Optional part try k prototype clustering\n#conda install kmodes","b16f78b2":"import numpy as np\nfrom kmodes.kprototypes import KPrototypes\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom kmodes.kmodes import KModes\nstyle.use(\"ggplot\")","962f5cbd":"#Running data with KModes type\n\nkm = KModes(n_clusters=5, init='Huang', n_init=6, verbose=1)\n\n#clusters = KPrototypes().fit_predict(X, categorical=[1, 2])\n\nclusters = km.fit_predict(data_reduced)\n\n# Print the cluster centroids\nprint(km.cluster_centroids_)","4777c051":"from sklearn.preprocessing import StandardScaler\nnp.seterr(divide='ignore', invalid='ignore')\nNum_features =df2.select_dtypes(include=[np.number]).columns\n# 4 samples\/observations and 2 variables\/features\n#X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])# the scaler object (model)\n#scaler = StandardScaler()# fit and transform the data\n#scaled_data2 = scaler.fit_transform(df2) \ndf2scaled = df2[Num_features].apply(zscore)","13e8fcc4":"df2scaled.head()","b3662772":"df2scaled.drop(['is_installed'],axis=1,inplace=True)","6fddb4ed":"#Principal Component\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3, whiten=True)\nNum_features=df2scaled.select_dtypes(include=[np.number]).columns\nx=df2scaled[Num_features]\nprincipalComponents = pca.fit_transform(x)\n\n# Cumulative Explained Variance\ncum_explained_var = []\nfor i in range(0, len(pca.explained_variance_ratio_)):\n    if i == 0:\n        cum_explained_var.append(pca.explained_variance_ratio_[i])\n    else:\n        cum_explained_var.append(pca.explained_variance_ratio_[i] + \n                                 cum_explained_var[i-1])\n\nprint(cum_explained_var)","f9d5458b":"#Principal Components converted to a Data frame\nprincipalDf  = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\nprincipalDf.shape","cb075608":"df2.head()","8f34ce81":"df2.reset_index(drop=True,inplace=True)","bae82d09":"principalDf.reset_index(drop=True, inplace=True)\n","ae649cf8":"#Concatenating the PCAs with the categorical variable\nfinalDf_Cat = pd.concat([principalDf, df2[\"gender\"],df2[\"group\"],df2[\"category\"]], axis = 1)\nfinalDf_Cat.head(2)","5f624bb6":"##### Choosing optimal K value\ncost = []\nX = finalDf_Cat\nfor num_clusters in list(range(2,7)):\n    kproto = KPrototypes(n_clusters=num_clusters, init='Huang', random_state=1111,n_jobs=-2,\n                         max_iter=15,n_init=6,verbose=2) \n    kproto.fit_predict(X, categorical=[3,4,5])\n    cost.append(kproto.cost_)\n\nplt.plot(cost)\nplt.xlabel('K')\nplt.ylabel('cost')\nplt.show","c407a548":"# Converting the dataset into matrix\nX = finalDf_Cat.to_numpy()","cba87825":"# Running K-Prototype clustering K =2 gives a good elbow point as above\nkproto = KPrototypes(n_clusters=2, init='Huang', verbose=2, \n                     random_state=1111,max_iter=20, n_init=6,n_jobs=-2,gamma=.25) \nclusters = kproto.fit_predict(X, categorical=[3,4,5])","937171b1":"#Visualize K-Prototype clustering on the PCA projected Data\ndf=pd.DataFrame(finalDf_Cat)\ndf['Cluster_id']=clusters\nprint(df['Cluster_id'].value_counts())\nsns.pairplot(df,hue='Cluster_id',palette='Dark2',diag_kind='kde')","ce7dedf4":"del finalDf_Cat\ndel df","d18ecc46":"#df7.boxplot(by='GROUP', layout = (200,40),figsize=(15,10))\n#from sklearn.cluster import AgglomerativeClustering\n#model = AgglomerativeClustering(n_clusters=3,affinity='euclidean',linkage='average')\n#model.fit(scaled_data)\n#df7['GROUP'] = model.labels_\n#df7.head()","47b1f56e":"#Very clear clusters formed 1 for female and 3 for male, another group is 84% male and 16% female\n#The last group has overlapping interests\n#DataClust = df7.groupby(['GROUP'])\n#DataClust.mean()","a47e748f":"#Convert age group into indicator\/dummy variables using One hot encoding\n#pd.get_dummies(df2, columns=[\"group\"]).head()","a46c9090":"#Convert category into indicator\/dummy variables using One hot encoding\n#pd.get_dummies(df2, columns=[\"category\"]).head()","1a3c6886":"#Convert Phone brand group into indicator\/dummy variables using One hot encoding\n#pd.get_dummies(df2, columns=[\"phone_brand\"]).head()","e3e582c3":"#Convert device model into indicator\/dummy variables using One hot encoding\n#We will not do this as it is making too many columns \n#We might have two issue 1) curse of dimensionality 2) memory issues\n#pd.get_dummies(df2, columns=[\"device_model\"]).head()","83eb92c0":"#df2.head()","d176840e":"#df2.shape","a789c834":"#df2.describe().transpose()","76c181f5":"Parameters for KMode run\n    -----------\n    n_clusters : int, optional, default: 8\n        The number of clusters to form as well as the number of\n        centroids to generate.\n    max_iter : int, default: 300\n        Maximum number of iterations of the k-modes algorithm for a\n        single run.\n    cat_dissim : func, default: matching_dissim\n        Dissimilarity function used by the algorithm for categorical variables.\n        Defaults to the matching dissimilarity function.\n    init : {'Huang', 'Cao', 'random' or an ndarray}, default: 'Cao'\n        Method for initialization:\n        'Huang': Method in Huang [1997, 1998]\n        'Cao': Method in Cao et al. [2009]\n        'random': choose 'n_clusters' observations (rows) at random from\n        data for the initial centroids.\n        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n        and gives the initial centroids.\n    n_init : int, default: 10\n        Number of time the k-modes algorithm will be run with different\n        centroid seeds. The final results will be the best output of\n        n_init consecutive runs in terms of cost.\n    verbose : int, optional\n        Verbosity mode.","fda72e29":"# Mobile User Segmentation\n## Unsupervised Learning\n### Domain\n#### Mobile, Apps\n\nBusiness Context\nA key challenge for Mobile App businesses is to analyze the trend in the market to increase their sales\/usage.\nThe trend can be easily observed if the companies can group the customers based on their activity on the\nnetwork.\nWe have access to the user's demographic characteristics, their app usage, geolocation, and mobile device\nproperties. This grouping can be done by applying different criteria like App installed status, Actively using or\nnot, phone brand compatibility and so on.\nThe machine learning clustering algorithms can provide an analytical method to cluster user segments with\nsimilar interests\/habits. This will help App\/mobile providers better understand and interact with their\nsubscribers.\nObjective\nWe will be clustering the users into groups who show similar interest in their app usage and understand which\nfactors are responsible for making the clusters\n\nDataset description\n* App_event.csv - Details on App is installed and used actively or not\n* App_labels.csv - Apps and their labels, the label_id can be used to join with label categories\n* events.csv - Event data has an event id, location detail (lat\/long), and timestamp when the user isusing an app on his device\n* gender_age.csv - details of users age & gender\n* label_categories.csv - Apps' labels and their categories\n* phone_device.csv - Device ids, brand, and models name. here the brand names are in Chinese, you can convert it in English using google for better understanding but we will not do it here. We will use this as a profiling variable, not as a clustering variable.\n\n1. Preprocessing the data (10 points)\na. Import required libraries and read all the CSVs.\nb. Check for duplicate device id (phones) and remove them\nc. Merge the provided files into a dataframe.\nd. Drop unnecessary columns\n\n2. Exploratory Data Analysis (10 points)\na. Check dimensions of the dataframe in terms of rows and columns and study few of the\nvariables\nb. Check the data type\nc. Sample only 5 % of the total dataset for our clustering problem since it\u2019s a very large dataset\nand can lead to memory issues. Use random_state = 1111 [ Note - take a smaller sample if you\nface memory issues ]\nd. Check the frequency and distribution of the relevant features\ne. Convert string features into categories and make them numerical\nf. Study summary statistics and mention your findings\ng. Check for missing values and impute missing values if any\nh. Standardize the data\n\n3. Build a clustering algorithm for clustering mobile users. Kindly follow the below steps: (10 points) [ Hint\n- you can try both k-means and hierarchical clustering]\na. Evaluate the clustering algorithm you\u2019ve used\nb. Mention the hyperparameters that perform the best\n\n4. Cluster Profiling: (10 points)\na. Comment on the optimal cluster size\nb. Compute the statistical summary for observations in the cluster\nc. Check mean, sd, freq, modes, min, max, range..all basic central tendency numbers\n\n5. Do dimensionality reduction using PCA (10 points)\n\n6. Apply k means clustering on the PCA transformed data (10 points)\n\n7. Mention your comments and findings (10 points)\n\nOptional:\n\n1. Try KPrototypes algorithm to cluster the data\na. https:\/\/medium.com\/@guruprasad0o_o0\/notes-on-k-prototype-for-clustering-mixed-typeddata-e80eb526b226\nb. https:\/\/medium.com\/datadriveninvestor\/k-prototype-in-clustering-mixed-attributese6907db91914\n2. Try kmodes library to cluster the data\na. https:\/\/pypi.org\/project\/kmodes\/\nProprietary content. \u00a9 Great Learning. All Rights Reserved. Unauthorized use or distribution prohibited. 3\nb. https:\/\/medium.com\/@davidmasse8\/unsupervised-learning-for-categorical-datadd7e497033ae\n\nFood for thought\n\n* Does applying PCA gives a better result in comparison to earlier?\n* Can you apply any other algorithms to create clusters of data?\n* How clustering can be helpful for your analysis?\n* What can you infer about the properties of users of different clusters formed in this project?\n\nLearning Outcomes\n\n* PCA\n* k-means clustering\n* Scaling\n* Silhouette Coefficient"}}