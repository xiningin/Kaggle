{"cell_type":{"05c6a5b5":"code","0c7068c4":"code","125033c1":"code","01304827":"code","176b9df2":"code","58850319":"code","687b23d6":"code","73d38adf":"code","5c092a21":"code","94961f98":"code","4fb35586":"code","4c19017a":"code","5abd21d6":"code","ac11cb7b":"code","3e5651e6":"code","48403092":"code","861ee3f4":"code","eb1eafec":"code","7e21105f":"code","26b62205":"code","a8525545":"code","dd3f0829":"code","fc8cfcb6":"code","feec1a06":"code","d2759c8a":"code","2b762e10":"code","69c2845b":"code","0aec81a3":"code","28bb6ef3":"code","7752843e":"code","47e3b152":"code","f2aa3b78":"code","b878a596":"code","39911900":"markdown","c285d8ea":"markdown","5eaef011":"markdown","30ae32ce":"markdown","12be9873":"markdown","ea78a3b9":"markdown"},"source":{"05c6a5b5":"# # For Kernal Mode\n!pip install -q ..\/input\/tensorflow-determinism\n!pip install -q ..\/input\/huggingfacetokenizers\/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl\n# !pip install -q ..\/input\/sacremoses\n!pip uninstall --yes pytorch-transformers\n!pip install -q ..\/input\/huggingface-transformers-master","0c7068c4":"import pandas as pd\nimport numpy as np\nimport random\nimport random, math, time\nimport os, sys\nfrom pathlib import Path\nimport pickle\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nimport bisect\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n# from tqdm.notebook import tqdm\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n# https:\/\/github.com\/NVIDIA\/tensorflow-determinism\nos.environ['TF_DETERMINISTIC_OPS'] = '1' # TF 2.1\n# from tfdeterminism import patch\n# patch()\n\nimport transformers\nfrom transformers import *\n\nimport torch\n\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nfrom bs4 import BeautifulSoup\n\nimport gc\ngc.enable()\n\nnp.set_printoptions(suppress=True)\nprint('Tensorflow version', tf.__version__)\n\nprint('PyTorch version', torch.__version__)\n\nprint('Transformers version',\n      transformers.__version__)  # Current version: 2.3.0","125033c1":"# https:\/\/www.tensorflow.org\/guide\/gpu#limiting_gpu_memory_growth\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    # Restrict TensorFlow to only use the first GPU\n    try:\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n    except RuntimeError as e:\n        # Visible devices must be set before GPUs have been initialized\n        print(e)\n\n# tf.config.gpu.set_per_process_memory_fraction(0.85)\n# tf.config.gpu.set_per_process_memory_growth(True)\n\n# from tensorflow.compat.v1 import ConfigProto\n# from tensorflow.compat.v1 import InteractiveSession\n\n# config = ConfigProto()\n# config.gpu_options.per_process_gpu_memory_fraction = 0.2\n# config.gpu_options.allow_growth = True\n# session = InteractiveSession(config=config)","01304827":"import os\nos.listdir(\"..\/input\")","176b9df2":"rand_seed = 20201120\nn_splits = 5\n\n# BERT_PATH = \"\/workspace\/Kaggle\/QA\/pretrained_models\/\"\n# dataset_folder = Path(\"\/workspace\/Kaggle\/QA\/\")\n# MODEL_PATH_list = [\n#     \"\/workspace\/Kaggle\/QA\/completed\/tf-roberta-base-exp-v7\/\",\n#     \"\/workspace\/Kaggle\/QA\/completed\/bert-base-uncased-exp-v4\/\",\n#     \"\/workspace\/Kaggle\/QA\/completed\/tf-bert-base-cased-exp-v4\/\",\n#     \"\/workspace\/Kaggle\/QA\/completed\/tf-roberta-base-exp-v4\/\",\n#     \"\/workspace\/Kaggle\/QA\/completed\/xlnet-base-cased-exp-v7\/\",\n# ]\n\nBERT_PATH = \"..\/input\/\"\ndataset_folder = Path(\"..\/input\/google-quest-challenge\")\nMODEL_PATH_list = [\n    \"..\/input\/tf-roberta-base-exp-v7\/\",\n    \"..\/input\/bert-base-uncased-exp-v4\/\",\n    \"..\/input\/tf-bert-base-cased-exp-v4\/\",\n    \"..\/input\/tf-roberta-base-exp-v4\/\",\n    \"..\/input\/xlnet-base-cased-exp-v7\/\",\n]\n\n# pretrained_model_metadata = [\n#     # (pretrained_model_name, is_tf, infer_batch_size, cate_embed_mode)\n#     (\"tf-roberta-base\", True, 32, True),\n#     (\"bert-base-uncased\", True, 32, False),\n#     (\"tf-bert-base-cased\", True, 32, False),\n#     (\"tf-roberta-base\", True, 32, False),\n#     (\"xlnet-base-cased\", True, 32, True),\n# ]\n\npretrained_model_metadata = [\n    # (pretrained_model_name, is_tf, infer_batch_size, cate_embed_mode)\n    (\"tf-roberta-base\", True, 56, True),\n    (\"bert-base-uncased\", True, 56, False),\n    (\"tf-bert-base-cased\", True, 56, False),\n    (\"tf-roberta-base\", True, 56, False),\n    (\"xlnet-base-cased\", True, 48, True),\n]\n\nmodel_filename_prefix_list = [\n    \"tf-roberta-base_exp_cate_embed\",\n    \"bert-base-uncased_exp_split_dense\",\n    \"tf-bert-base-cased_exp_split_dense\",\n    \"tf-roberta-base_exp_split_dense\",\n    \"xlnet-base-cased_exp_cate_embed\",\n]\n\nMAX_SEQUENCE_LENGTH = 512\nmax_title_length = 100\n\n# learning_rate = 2e-5\n# embeddings_dropout = 0.2\n# dense_dropout = 0.1","58850319":"for i, p in enumerate(MODEL_PATH_list):\n    prefix = model_filename_prefix_list[i]\n    for f in os.listdir(p):\n        if f != \"dataset-metadata.json\":\n            print(p+f)\n            assert prefix in f","687b23d6":"df_train = pd.read_csv(dataset_folder \/ 'train.csv')\ndf_test = pd.read_csv(dataset_folder \/ 'test.csv')\ndf_sub = pd.read_csv(dataset_folder \/ 'sample_submission.csv')\nprint('Train shape:', df_train.shape)\nprint('Test shape:', df_test.shape)","73d38adf":"output_categories = list(df_train.columns[11:])\n# Select only question title, body and answer\ninput_categories = list(df_train.columns[[1, 2, 5]])\n\nprint('\\nOutput categories:\\n', output_categories)\nprint('\\nInput categories:\\n', input_categories)","5c092a21":"# Extract domain\ndef extract_netloc(x):\n    tokens = x.split(\".\")\n    if len(tokens) > 3:\n        print(x)\n        return \".\".join(tokens[:2])\n        # looks like meta is a special site, we should keep it\n        # https:\/\/stackoverflow.com\/help\/whats-meta\n        # the part of the site where users discuss the workings and policies of Stack Overflow rather than discussing programming itself.\n        # return tokens[1]\n    else:\n        return tokens[0]\n\n\n# TODO: test it\n# df_train['netloc'] = df_train['host'].apply(\n#     lambda x: extract_netloc(x))\n# df_test['netloc'] = df_test['host'].apply(\n#     lambda x: extract_netloc(x))\n\ndf_train['netloc'] = df_train['host'].apply(lambda x: x.split(\".\")[0])\ndf_test['netloc'] = df_test['host'].apply(lambda x: x.split(\".\")[0])","94961f98":"def set_all_seeds(rand_seed):\n    np.random.seed(rand_seed)\n    random.seed(rand_seed)\n    os.environ['PYTHONHASHSEED'] = str(rand_seed)\n    \n    # TF 2.0\n    tf.random.set_seed(rand_seed)\n    \n    # PyTorch\n    torch.manual_seed(rand_seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","4fb35586":"set_all_seeds(rand_seed)","4c19017a":"# Redirect outputs to console\n# import sys\n# jupyter_console = sys.stdout\n# sys.stdout = open('\/dev\/stdout', 'w')\n\n# Append to log file\n# sys.stdout = open(f\"stdout.log\", 'a')\n# sys.stdout = jupyter_console","5abd21d6":"def _convert_to_transformer_inputs(title, question, answer, tokenizer,\n                                   max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n    def return_id(str1, str2, truncation_strategy, length):\n\n        inputs = tokenizer.encode_plus(str1,\n                                       str2,\n                                       add_special_tokens=True,\n                                       max_length=length,\n                                       truncation_strategy=truncation_strategy)\n\n        input_ids = inputs[\"input_ids\"]\n        input_masks = [1] * len(input_ids)\n        input_segments = inputs[\"token_type_ids\"]\n        padding_length = length - len(input_ids)\n        padding_id = tokenizer.pad_token_id\n        input_ids = input_ids + ([padding_id] * padding_length)\n        input_masks = input_masks + ([0] * padding_length)\n        input_segments = input_segments + ([0] * padding_length)\n\n        return [input_ids, input_masks, input_segments]\n    \n    def remove_html_special_symbols(x):\n        html_entities = [\n            (\"&quot;\", \"\\\"\"),\n            (\"&num;\", \"#\"),\n            (\"&dollar;\", \"$\"),\n            (\"&percnt;\", \"%\"),\n            (\"&amp;\", \"&\"),\n            (\"&apos;\", \"'\"),\n            (\"&lpar;\", \"(\"),\n            (\"&rpar;\", \")\"),\n            (\"&ast;\", \"*\"),\n            (\"&plus;\", \"+\"),\n            (\"&comma;\", \",\"),\n            (\"&minus;\", \"-\"),\n            (\"&period;\", \".\"),\n            (\"&sol;\", \"\/\"),\n            (\"&colon;\", \":\"),\n            (\"&semi;\", \";\"),\n            (\"&lt;\", \"<\"),\n            (\"&equals;\", \"=\"),\n            (\"&gt;\", \">\"),\n            (\"&quest;\", \"?\"),\n            (\"&commat;\", \"@\"),\n            (\"&lsqb;\", \"[\"),\n            (\"&bsol;\", \"\\\\\"),\n            (\"&rsqb;\", \"]\"),\n            (\"&Hat;\", \"^\"),\n            (\"&lowbar;\", \"_\"),\n            (\"&grave;\", \"`\"),\n            (\"&lcub;\", \"{\"),\n            (\"&verbar;\", \"|\"),\n            (\"&rcub;\", \"}\"),\n            # (\"\", \"\"),\n        ]\n        for (k, v) in html_entities:\n            x = str(x.replace(k, v))\n        return x\n\n    def remove_latex_and_code_tokens(tokens):\n        return [\n            x for x in tokens if not (x.startswith(\"$\") or x.startswith(\"\\\\\"))\n        ]\n\n    # Remove extra spaces\n    title = remove_html_special_symbols(\" \".join(\n        remove_latex_and_code_tokens(str(title).split()))).strip()\n    question = remove_html_special_symbols(\" \".join(\n        remove_latex_and_code_tokens(str(question).split()))).strip()\n    answer = remove_html_special_symbols(\" \".join(\n        remove_latex_and_code_tokens(str(answer).split()))).strip()\n\n    # Extract plain text from html\n    try:\n        soup_q = BeautifulSoup(question)\n        question = soup_q.get_text()\n    except Exception as e:\n        print(e)\n        pass\n\n    try:\n        soup_a = BeautifulSoup(answer)\n        answer = soup_a.get_text()\n    except Exception as e:\n        print(e)\n        pass\n\n    input_ids_q, input_masks_q, input_segments_q = return_id(\n        \"[CLS] \" + title[:max_title_length] + \" [SEP] \" + question + \" [SEP]\", None,\n        'longest_first', max_sequence_length)\n\n    input_ids_a, input_masks_a, input_segments_a = return_id(\n        \"[CLS] \" + answer + \" [SEP]\", None, 'longest_first', max_sequence_length)\n\n    return [\n        input_ids_q, input_masks_q, input_segments_q, input_ids_a,\n        input_masks_a, input_segments_a\n    ]","ac11cb7b":"def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n    input_ids_q, input_masks_q, input_segments_q = [], [], []\n    input_ids_a, input_masks_a, input_segments_a = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n\n        input_ids_q.append(ids_q)\n        input_masks_q.append(masks_q)\n        input_segments_q.append(segments_q)\n\n        input_ids_a.append(ids_a)\n        input_masks_a.append(masks_a)\n        input_segments_a.append(segments_a)\n\n    return [\n        np.asarray(input_ids_q, dtype=np.int32),\n        np.asarray(input_masks_q, dtype=np.int32),\n        np.asarray(input_segments_q, dtype=np.int32),\n        np.asarray(input_ids_a, dtype=np.int32),\n        np.asarray(input_masks_a, dtype=np.int32),\n        np.asarray(input_segments_a, dtype=np.int32)\n    ]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","3e5651e6":"def compute_spearmanr_ignore_nan(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.nanmean(rhos)\n\ndef compute_spearmanr(trues, preds):\n    rhos = []\n    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n        rhos.append(spearmanr(tcol, pcol).correlation)\n    return np.mean(rhos)","48403092":"class SpearmanMonitorCallback(tf.keras.callbacks.Callback):\n    def __init__(self, valid_data, batch_size=16, fold=None):\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n\n        self.batch_size = batch_size\n        self.fold = fold\n\n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n\n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n\n        print(f\" Fold {self.fold+1} Validation Score: {rho_val:.6f}\")","861ee3f4":"def create_model(pretrained_model_name):\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n\n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n\n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n\n    pretrained_model = model_class.from_pretrained(BERT_PATH +\n                                                   f\"{pretrained_model_name}\")\n\n    q_embedding = pretrained_model(q_id,\n                                   attention_mask=q_mask,\n                                   token_type_ids=q_atn)[0]\n    a_embedding = pretrained_model(a_id,\n                                   attention_mask=a_mask,\n                                   token_type_ids=a_atn)[0]\n\n#     q_embedding = tf.keras.layers.SpatialDropout1D(embeddings_dropout)(\n#         q_embedding)\n#     a_embedding = tf.keras.layers.SpatialDropout1D(embeddings_dropout)(\n#         a_embedding)\n\n\n    # Get CLS token output\n    q = q_embedding[:, 0, :]\n    a = a_embedding[:, 0, :]\n\n    q = tf.keras.layers.Dense(256, activation='relu')(q)\n    a = tf.keras.layers.Dense(256, activation='relu')(a)\n    \n    # TODO: Test dense dropout\n    # q = tf.keras.layers.Dropout(dense_dropout)(q)\n    # a = tf.keras.layers.Dropout(dense_dropout)(a)\n\n    q = tf.keras.layers.Dense(21, activation='sigmoid')(q)\n    a = tf.keras.layers.Dense(9, activation='sigmoid')(a)\n\n    x = tf.keras.layers.Concatenate()([q, a])\n\n    model = tf.keras.models.Model(inputs=[\n        q_id,\n        q_mask,\n        q_atn,\n        a_id,\n        a_mask,\n        a_atn,\n    ],\n                                  outputs=x)\n\n    return model, pretrained_model","eb1eafec":"def create_model_cate_embed(pretrained_model_name, embed_info):\n    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n\n    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n\n    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH, ), dtype=tf.int32)\n\n    pretrained_model = model_class.from_pretrained(BERT_PATH +\n                                                   f\"{pretrained_model_name}\")\n\n    # Get last hidden-state from 1st element of output\n    if \"xlnet\" in pretrained_model_name:\n        q_embedding = pretrained_model(q_id,\n                                       attention_mask=q_mask,\n                                       token_type_ids=q_atn)[0]\n        a_embedding = pretrained_model(a_id,\n                                       attention_mask=a_mask,\n                                       token_type_ids=a_atn)[0]\n    else:\n        q_embedding, q_pooler_output = pretrained_model(q_id,\n                                                        attention_mask=q_mask,\n                                                        token_type_ids=q_atn)\n        a_embedding, a_pooler_output = pretrained_model(a_id,\n                                                        attention_mask=a_mask,\n                                                        token_type_ids=a_atn)\n\n    # Get CLS token output\n    q = q_embedding[:, 0, :]\n    a = a_embedding[:, 0, :]\n\n    host_input = tf.keras.Input(shape=(1, ), name=\"host_input\")\n    netloc_input = tf.keras.Input(shape=(1, ), name=\"netloc_input\")\n    cate_input = tf.keras.Input(shape=(1, ), name=\"category_input\")\n\n    host_embed_info = embed_info[\"host\"]\n    host_embed = tf.keras.layers.Embedding(input_dim=host_embed_info[0],\n                                           output_dim=host_embed_info[1],\n                                           input_length=(1, ))(host_input)\n\n    netloc_embed_info = embed_info[\"netloc\"]\n    netloc_embed = tf.keras.layers.Embedding(input_dim=netloc_embed_info[0],\n                                             output_dim=netloc_embed_info[1],\n                                             input_length=(1, ))(netloc_input)\n\n    cate_embed_info = embed_info[\"category\"]\n    cate_embed = tf.keras.layers.Embedding(input_dim=cate_embed_info[0],\n                                           output_dim=cate_embed_info[1],\n                                           input_length=(1, ))(cate_input)\n\n    host_embed = tf.keras.layers.Reshape(\n        target_shape=(host_embed_info[1], ))(host_embed)\n    netloc_embed = tf.keras.layers.Reshape(\n        target_shape=(netloc_embed_info[1], ))(netloc_embed)\n    cate_embed = tf.keras.layers.Reshape(\n        target_shape=(cate_embed_info[1], ))(cate_embed)\n\n    embed_concat = tf.keras.layers.Concatenate()(\n        [host_embed, netloc_embed, cate_embed])\n    embed_concat = tf.keras.layers.Dense(128, activation='relu')(embed_concat)\n\n    # Concatenation\n    q_concat = tf.keras.layers.Concatenate()([q, embed_concat])\n    # q_concat = tf.keras.layers.Concatenate()([q, host_embed, cate_embed, q_pooler_output])\n    q_concat = tf.keras.layers.Dense(256, activation='relu')(q_concat)\n\n    a_concat = tf.keras.layers.Concatenate()([a, embed_concat])\n    # a_concat = tf.keras.layers.Concatenate()([a, host_embed, cate_embed, a_pooler_output])\n    a_concat = tf.keras.layers.Dense(256, activation='relu')(a_concat)\n\n    # Dense dropout\n    # q_concat = tf.keras.layers.Dropout(dense_dropout)(q_concat)\n    # a_concat = tf.keras.layers.Dropout(dense_dropout)(a_concat)\n\n    # Use sigmoid for multi-label predictions\n    q_concat = tf.keras.layers.Dense(21, activation='sigmoid')(q_concat)\n    a_concat = tf.keras.layers.Dense(9, activation='sigmoid')(a_concat)\n\n    x = tf.keras.layers.Concatenate()([q_concat, a_concat])\n\n    model = tf.keras.models.Model(inputs=[\n        q_id, q_mask, q_atn, a_id, a_mask, a_atn, host_input, netloc_input,\n        cate_input\n    ],\n                                  outputs=x)\n\n    return model, pretrained_model","7e21105f":"set_all_seeds(rand_seed)\ngkf = GroupKFold(n_splits=n_splits).split(X=df_train.question_body,\n                                          groups=df_train.question_body)\ngkf = list(gkf)\nlen(gkf)","26b62205":"outputs = compute_output_arrays(df_train, output_categories)","a8525545":"def optimize_ranks(preds, unique_labels):\n    new_preds = np.zeros(preds.shape)\n    for i in range(preds.shape[1]):\n        interpolate_bins = np.digitize(preds[:, i],\n                                       bins=unique_labels,\n                                       right=False)\n        \n        if len(np.unique(interpolate_bins)) == 1:\n            # Use original preds\n            new_preds[:, i] = preds[:, i]\n        else:\n            new_preds[:, i] = unique_labels[interpolate_bins]\n\n    return new_preds","dd3f0829":"y_labels = df_train[output_categories].copy()\ny_labels = y_labels.values.flatten()\nunique_labels = np.array(sorted(np.unique(y_labels)))\nunique_labels","fc8cfcb6":"denominator = 60\nq = np.arange(0, 101, 100 \/ denominator)\nexp_labels = np.percentile(unique_labels, q)\nexp_labels","feec1a06":"infer_start_time = time.time()\n\nall_test_preds = []\nall_val_preds = []\nall_val_scores = []\nall_magic_val_scores = []\n\ngc.collect()\n\nfor k, MODEL_PATH in enumerate(MODEL_PATH_list):\n#     if k < 2:\n#         continue\n    \n    pretrained_model_name, is_tf, infer_batch_size, cate_embed_mode = pretrained_model_metadata[k]\n    model_filename_prefix = model_filename_prefix_list[k]\n\n    print(f\"Generating prediction from fine-tuned {pretrained_model_name} ......\")\n    \n    preprocessing_start = time.time()\n    if is_tf:\n        model_class = TFAutoModel\n        tokenizer_class = AutoTokenizer\n    else:\n        model_class = AutoModel\n        tokenizer_class = AutoTokenizer\n    tokenizer = tokenizer_class.from_pretrained(BERT_PATH +\n                                                f\"{pretrained_model_name}\")\n    inputs = compute_input_arrays(df_train, input_categories, tokenizer,\n                                  MAX_SEQUENCE_LENGTH)\n    test_inputs = compute_input_arrays(df_test, input_categories,\n                                       tokenizer, MAX_SEQUENCE_LENGTH)\n    print(f\"Time spent on preprocessing: {(time.time()-preprocessing_start)\/60:,.2f} minutes\")\n\n    test_preds = []\n    val_preds = []\n    val_scores = []\n    magic_val_scores = []\n\n    for i, (train_idx, valid_idx) in enumerate(gkf):\n        set_all_seeds(rand_seed)\n\n        print(f\"Using {pretrained_model_name} Fold {i+1} {pretrained_model_name} for inference ......\")\n\n        fold_start = time.time()\n\n        # Generate validation score\n        # TODO: remove this part later\n        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n        valid_outputs = outputs[valid_idx]\n        \n        if cate_embed_mode:\n            # Extra categorical embeddings\n            embed_info = {}\n            category_features = {}\n            def extract_category_ids(train, val, test, c, info):\n                le = LabelEncoder()\n                le.fit(train[c])\n                # Set unknonwn category\n                val[c] = val[c].map(lambda s: '<unknown>'\n                                    if s not in le.classes_ else s)\n                test[c] = test[c].map(lambda s: '<unknown>'\n                                      if s not in le.classes_ else s)\n\n                le_classes = le.classes_.tolist()\n                bisect.insort_left(le_classes, '<unknown>')\n                le.classes_ = le_classes\n\n                train[c + \"_label\"] = le.transform(train[c])\n                val[c + \"_label\"] = le.transform(val[c])\n                test[c + \"_label\"] = le.transform(test[c])\n\n                no_of_unique_cat = train[c + \"_label\"].nunique()\n                embedding_size = min(np.ceil((no_of_unique_cat) \/ 2), 50)\n                embedding_size = int(embedding_size)\n                vocab_size = no_of_unique_cat + 1\n                info[c] = (vocab_size, embedding_size)\n\n                print(f\"Extracted (vocab_size, embedding_size) for {c}: ({vocab_size}, {embedding_size})\")\n\n                return val[c + \"_label\"], test[c + \"_label\"]\n\n            host_val, host_test = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n                                                       df_train.iloc[valid_idx, :].copy(),\n                                                       df_test.copy(),\n                                                       \"host\",\n                                                       embed_info)\n            netloc_val, netloc_test = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n                                                           df_train.iloc[valid_idx, :].copy(),\n                                                           df_test.copy(),\n                                                           \"netloc\",\n                                                           embed_info)\n            cate_val, cate_test = extract_category_ids(df_train.iloc[train_idx, :].copy(),\n                                                       df_train.iloc[valid_idx, :].copy(),\n                                                       df_test.copy(),\n                                                       \"category\",\n                                                       embed_info)\n\n            valid_inputs.append(host_val)\n            valid_inputs.append(netloc_val)\n            valid_inputs.append(cate_val)\n\n            # Copy test_inputs\n            submit_inputs = [np.copy(x) for x in test_inputs]\n            submit_inputs.append(host_test)\n            submit_inputs.append(netloc_test)\n            submit_inputs.append(cate_test)\n        \n        print(\"Cleaning session ...\")\n        K.clear_session()    \n\n        print(\"Loading pretrained model and weights ...\")\n        if cate_embed_mode:\n            model, pretrained_model = create_model_cate_embed(pretrained_model_name, embed_info)\n        else:\n            model, pretrained_model = create_model(pretrained_model_name)\n            \n        model_filename = f\"{model_filename_prefix}_fold{i+1}.h5\"\n        # Load fine-tuned weights\n        model.load_weights(MODEL_PATH + model_filename)\n        \n        fold_val_preds = model.predict(valid_inputs,\n                                       batch_size=infer_batch_size)\n        rho_val = compute_spearmanr(valid_outputs, fold_val_preds)\n        print(f\"Fold {i+1} Validation Score: {rho_val:.6f}\")\n        val_preds.append(fold_val_preds)\n        val_scores.append(rho_val)\n        \n        val_magic_preds = optimize_ranks(fold_val_preds, exp_labels)\n        magic_rho_val = compute_spearmanr(valid_outputs, val_magic_preds)\n        print(f\"Fold {i+1} Magic Validation Score: {magic_rho_val:.6f}\")\n        magic_val_scores.append(magic_rho_val)\n\n        # Generate test predictions\n        if cate_embed_mode:\n            test_preds.append(model.predict(submit_inputs,\n                                            batch_size=infer_batch_size))\n        else:\n            test_preds.append(model.predict(test_inputs,\n                                            batch_size=infer_batch_size))\n\n        print(f\"Time spent on fold {i+1} inference: {(time.time()-fold_start)\/60:,.2f} minutes\")\n\n        del model, pretrained_model, valid_inputs, valid_outputs, fold_val_preds, rho_val\n        gc.collect()\n\n    all_test_preds.append(test_preds)\n    all_val_preds.append(val_preds)\n    all_val_scores.append(val_scores)\n    all_magic_val_scores.append(magic_val_scores)\n    \n    del tokenizer, inputs, test_inputs\n    gc.collect()\n    \nprint(f\"Time spent on ensemble inference: {(time.time()-infer_start_time)\/60:,.2f} minutes\")","d2759c8a":"print(f\"Mean Validation Score: {np.mean(all_val_scores):.6f}\")\nprint(f\"Mean Magic Validation Score: {np.mean(all_magic_val_scores):.6f}\")","2b762e10":"# Mean Validation Score: 0.396150\n# Mean Magic Validation Score: 0.417236","69c2845b":"def val_ensemble_preds(all_val_preds, weights):\n    oof_preds = np.zeros(outputs.shape)\n    for i, model_preds in enumerate(all_val_preds):\n        for j, (train_idx, valid_idx) in enumerate(gkf):\n            tmp = np.vstack(model_preds[j])\n            oof_preds[valid_idx] += tmp * weights[i]\n\n    oof_preds \/= np.sum(weights)\n\n    return oof_preds","0aec81a3":"with open('ensemble-models-v4-v7.pickle', 'wb') as handle:\n    pickle.dump(all_val_preds, handle, protocol=pickle.HIGHEST_PROTOCOL)","28bb6ef3":"weights = [1.0, 1.0, 1.0, 1.0, 1.0]\noof_preds = val_ensemble_preds(all_val_preds, weights)\nmagic_preds = optimize_ranks(oof_preds, exp_labels)\nblend_score = compute_spearmanr(outputs, magic_preds)\nprint(weights, blend_score)\n\nweights = [2.0, 1.0, 1.0, 1.0, 2.0]\noof_preds = val_ensemble_preds(all_val_preds, weights)\nmagic_preds = optimize_ranks(oof_preds, exp_labels)\nblend_score = compute_spearmanr(outputs, magic_preds)\nprint(weights, blend_score)\n\nweights = [2.0, 1.0, 1.0, 1.0, 1.5]\noof_preds = val_ensemble_preds(all_val_preds, weights)\nmagic_preds = optimize_ranks(oof_preds, exp_labels)\nblend_score = compute_spearmanr(outputs, magic_preds)\nprint(weights, blend_score)\n\nweights = [1.5, 1.0, 1.0, 1.0, 1.5]\noof_preds = val_ensemble_preds(all_val_preds, weights)\nmagic_preds = optimize_ranks(oof_preds, exp_labels)\nblend_score = compute_spearmanr(outputs, magic_preds)\nprint(weights, blend_score)","7752843e":"submit_preds = [np.average(x, axis=0) for x in all_test_preds]","47e3b152":"submit_preds = np.average(submit_preds, weights = [2.0, 1.0, 1.0, 1.0, 1.5],\n                          axis=0)  # for weighted average set weights=[...]","f2aa3b78":"# Optimize ranks\nsubmit_preds = optimize_ranks(submit_preds, exp_labels)","b878a596":"df_sub.iloc[:, 1:] = submit_preds\ndf_sub.to_csv('submission.csv', index=False)","39911900":"## Do Inference","c285d8ea":"## Generate Submission File","5eaef011":"## Check Ensemble Validation Score","30ae32ce":"## Preprocessing Utilities","12be9873":"### Split K-Folds by Unique Group","ea78a3b9":"## Create Custom Model"}}