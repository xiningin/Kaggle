{"cell_type":{"30f931ef":"code","45640d5e":"code","d15a4355":"code","d81a5cfc":"code","fc961116":"code","f9d7c8d9":"code","fff841bb":"code","906eb854":"code","af130640":"code","462876e9":"code","e6fda0d8":"code","4b04ffcc":"code","44e3cdb2":"code","4e944033":"code","e4acc80c":"code","46c2ce89":"code","e69c1e75":"code","2dedbecc":"code","0d17260d":"code","a274e646":"code","abe9e366":"code","d639484b":"code","3f00ec66":"code","bdc4efd5":"code","98fe5051":"code","c9f872dc":"code","63369435":"code","02c701a8":"code","36ba9078":"code","6a8b23d3":"code","0ddc400a":"code","7d5054cd":"code","6520ad37":"code","6987e5fe":"code","7ddc8b51":"markdown","644e1226":"markdown","070e7960":"markdown","905bddc7":"markdown","5d6612e5":"markdown","662603c6":"markdown","12ee96b5":"markdown","d26ab878":"markdown","01715e07":"markdown","2a85b3ca":"markdown","d3960b17":"markdown","7243cbeb":"markdown","d2a62675":"markdown","2c9358af":"markdown","4cb12f6b":"markdown","232553ae":"markdown","7b4abf86":"markdown","bef24776":"markdown","6b6da8fe":"markdown","32152354":"markdown","14d7de35":"markdown","65a021be":"markdown","51e22ad9":"markdown","a6ebda19":"markdown","429b8476":"markdown","a41ff8a3":"markdown"},"source":{"30f931ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Data Visualization\nimport seaborn as sns # Data Visualization\nplt.rcParams['figure.figsize'] = [12,7] # setting figsize permanantly\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler # preprocessing and normalizing techniques\n\nfrom sklearn.model_selection import train_test_split # Splitting data\nfrom sklearn.neighbors import KNeighborsClassifier # K nearest neighbours classifier\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree Classifier\n\nfrom sklearn.metrics import confusion_matrix,classification_report # metrics\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","45640d5e":"# Reading data into dataframe\ndf=pd.read_csv('\/kaggle\/input\/iris\/Iris.csv') ","d15a4355":"# Printing first five records in dataframe with dataset columns\/features.\ndf.head()","d81a5cfc":"df.columns","fc961116":"df.shape","f9d7c8d9":"# Dropping Id column\ndf.drop(columns=['Id'],inplace=True)","fff841bb":"# Printing unique Species\ndf['Species'].unique()","906eb854":"# Frequency plot for each species\nsns.countplot(df['Species'])","af130640":"# Five number Summary\ndf.describe().transpose()","462876e9":"# Columns info\ndf.info()","e6fda0d8":"# Checking Missing values\ndf.isna().sum()","4b04ffcc":"for col in ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']:\n    sns.boxplot(df[col])\n    plt.show()","44e3cdb2":"# Scatterplot of features with hue as Species.\nsns.pairplot(df,hue='Species')","4e944033":"df.columns","e4acc80c":"sns.violinplot(x='Species',y='SepalLengthCm',data=df)","46c2ce89":"sns.violinplot(x='Species',y='SepalWidthCm',data=df)","e69c1e75":"sns.violinplot(x='Species',y='PetalLengthCm',data=df)","2dedbecc":"sns.violinplot(x='Species',y='PetalWidthCm',data=df)","0d17260d":"labelencoder=LabelEncoder()","a274e646":"df['Species']=labelencoder.fit_transform(df['Species'])","abe9e366":"df.head()","d639484b":"minmax=MinMaxScaler()","3f00ec66":"col=df.drop(columns=['Species']).columns","bdc4efd5":"df_encoded=minmax.fit_transform(df[col])","98fe5051":"df_encoded=pd.DataFrame(df_encoded,columns=df.drop(columns=['Species']).columns)\ndf_encoded.head()","c9f872dc":"df_encoded['Species']=df['Species']","63369435":"df_encoded.head()","02c701a8":"sns.heatmap(df_encoded.corr(),annot=True,cmap='Blues')","36ba9078":"df_encoded[['SepalLengthCm','PetalWidthCm']].var()","6a8b23d3":"X=df_encoded.drop(columns=['Species','SepalWidthCm','PetalLengthCm'])\ny=df_encoded['Species']","0ddc400a":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=42)","7d5054cd":"model_knn=KNeighborsClassifier(n_neighbors=5)\nmodel_knn.fit(X_train,y_train)\nprint(\"KNN R2 score:\",model_knn.score(X_test,y_test))\ny_pred_knn=model_knn.predict(X_test)\nprint(\"\\n\\nConfusion Matrix:\")\nprint(pd.DataFrame(confusion_matrix(y_pred_knn,y_test)))\nprint(\"\\n\\nClassification Report:\")\nprint(classification_report(y_pred_knn,y_test))","6520ad37":"model_dt=DecisionTreeClassifier(max_depth=3,min_samples_split=5,min_impurity_decrease=0.05)\nmodel_dt.fit(X_train,y_train)\nprint(\"Decision Tree R2 score:\",model_dt.score(X_test,y_test))\ny_pred_dt=model_knn.predict(X_test)\nprint(\"\\n\\nConfusion Matrix:\")\nprint(pd.DataFrame(confusion_matrix(y_pred_dt,y_test)))\nprint(\"\\n\\nClassification Report:\")\nprint(classification_report(y_pred_dt,y_test))","6987e5fe":"model_svc=SVC(kernel='rbf')\nmodel_svc.fit(X_train,y_train)\nprint(\"SVC model R2 score:\",model_svc.score(X_test,y_test))\ny_pred_svc=model_svc.predict(X_test)\nprint(\"\\n\\nConfusion Matrix:\")\nprint(pd.DataFrame(confusion_matrix(y_pred_svc,y_test)))\nprint(\"\\n\\nClassification Report:\")\nprint(classification_report(y_pred_svc,y_test))","7ddc8b51":"No missing values.","644e1226":"We can see that PetalWidthCm have high variance.","070e7960":"#### b) Decision Tree","905bddc7":"# 5) Feature Engineering and Feature Selection","5d6612e5":"#### c) Support Vector Classifier","662603c6":"# 6) Model Training","12ee96b5":"#### a) K nearest neighbours","d26ab878":"Normalizing data for correlation.\n(This step is not neccesary for this dataset as it does not have a large range between values)","01715e07":"# 3) Exploratory Data Analysis","2a85b3ca":"Creating a dataframe from the .csv file","d3960b17":"# 4) Data Preprocessing","7243cbeb":"Splitting our dataset into training and testing dataset","d2a62675":"We can detect outliers in this dataset, but we cannot remove or waste any datapoints as data collection is a very crucial process.","2c9358af":"From above plots, we can infer that petal length and petal width shows variance in the data. ","4cb12f6b":"# 1) Importing Neccesary Libraries\n","232553ae":"![](https:\/\/thegoodpython.com\/assets\/images\/iris-species.png)\n\n\n# **Hello Kagglers** # \nHere in this notebook, this notebook is a basic tutorail for machine learning. We will try to do a detailed exploratory data analysis on the iris dataset and based on our learnings and insights we will train our machine learning model for classification. We will perform the data science process over the iris dataset.\n\nThis is a very basic approach.\nHope you find it useful.** Please Upvote.**\n\nThe Data Science Cycle consist of several steps:-\n1. Understanding the Problem Statement\n2. Collecting Data (Data Mining)\n3. Importing Data\n4. Data Cleaning and Data Manipulation\n5. Exploratory Data Analysis (For self consumption)\n6. Data Preprocessing and Feature Selection\n7. Model Building\n8. Model Evaluation\n9. Data Visualization (For client consumption)\n10. Model Deployment","7b4abf86":"Here we can that there are columns which shows variance in between different species in the dataset.","bef24776":"According to our problem statement, our target variable is species for classification.","6b6da8fe":"We can see that Petal features are higly correlated with each other. Hence, we can drop one of them.\nSepal Width is negative correlated with our target variable. Hence, we can drop it.","32152354":"# 7) Conclusion","14d7de35":"# 2) Importing Data","65a021be":"Dropping Id column as Id column does not give any information about the dataset","51e22ad9":"Label Encoding our target variable such that we can check the correlation between our features.","a6ebda19":"Thus, we have implemented all of our machine learning techniques on this dataset.\nAll of our models are performing very well. These are the basic data science steps which is followed to solve a problem statement.\n\nHope you find this notebook useful.","429b8476":"Each species have 50 records.\nThe dataset is not biased for any of the species.","a41ff8a3":"As a Data Scientist, it is important to understand the problem statement. You have to decide that the problem statement requires your data science skills or not. You have to analyse that the problem statement can be solved using machine learning and data science or not.\n**Communication plays an important role in data science.**\nAfter deciding that the problem requires your technical expertise, you have to understand what type of problem you are dealing with.\nThis step is necessary in order to know which type of Machine Learning algorithms can be effectively applied.\n\nThere are generally two types of machine learning problems:-\n1. **Supervised Machine Learning** -  predicts future outputs based on labeled input & output data\n2. **Unsupervised Machine Learning** - finds hidden patterns or groupings in unlabeled input data\n\n![](https:\/\/www.mathworks.com\/help\/stats\/machinelearning_supervisedunsupervised.png)\n\nFrom the problem statement, we can understand that our problem is of supervised machine learning type.\n\nSupervised Learning can be divided into two major categories:-\n1. Classification \u2014 predicts discrete categorical response (Tumor is cancerous or benign)\n2. Regression \u2014 predicts continuous numerical response (House Price Prediction)\n\nOur problem statement is of Classification type as our dataset requires us to classify specie of a flower.\n\nThe process is as follows:-\n![](https:\/\/www.mathworks.com\/help\/stats\/machinelearningoverviewworkflow.jpg)"}}