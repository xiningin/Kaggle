{"cell_type":{"29bd8f17":"code","55a6dfe5":"code","579f3da1":"code","bc82100d":"code","8af29b14":"code","204a2861":"code","fb7cb34e":"code","fe15383e":"code","48a0e17d":"code","c068599b":"code","fccb2675":"code","b2f8fbc0":"code","5f1dadda":"code","37c60710":"code","591b9ce6":"code","2c0c5bf1":"code","8b8589dd":"code","4ba31c28":"code","6e90d720":"code","00e9b42a":"code","6aab973f":"code","f00dc780":"code","b3e12605":"code","17a7cf21":"code","71acd0d4":"code","6f613893":"code","68e4185b":"code","46debbdc":"code","627e3e07":"code","20f28048":"code","62ed0265":"code","20e12dbf":"code","0420b106":"code","ee4b17a9":"code","220f6626":"code","09d8f761":"code","6ba671a4":"code","e6e29315":"code","75a03732":"code","63e979ed":"code","840292a3":"code","119b19c5":"code","1f8e6bae":"code","2c4c44aa":"code","8c02c649":"code","55594eec":"code","0d434918":"code","b6bcf5bd":"code","0911b8d0":"code","1fefa7e8":"code","e082b68b":"code","109556d0":"code","50f6245f":"code","3a9591b4":"code","6c6e6094":"code","b78356b6":"code","ec29ecc4":"code","760b6536":"code","eba49241":"code","90e715d1":"code","b122209a":"code","c831af8d":"code","3e3252e8":"code","a998b3c7":"code","0473e1d6":"code","c138cc1b":"code","db31070b":"code","13e19bf9":"code","0c8a6b73":"markdown","3a7f834e":"markdown","9742d836":"markdown","397129a8":"markdown","c6842f23":"markdown","8be39fe9":"markdown","051bc02e":"markdown","05ca29c2":"markdown","5f7effec":"markdown","cee9828c":"markdown","4eb40232":"markdown","ae102e59":"markdown","e86873b3":"markdown","74724fb0":"markdown","ba780f77":"markdown","a88dba67":"markdown","37b2e77a":"markdown","b9a4d246":"markdown","ba34e773":"markdown","c0370b2f":"markdown","7e7c742e":"markdown","ce52dad3":"markdown","23d689f1":"markdown","4f398932":"markdown","8de8bca8":"markdown","d8b2ec07":"markdown","01f51a2f":"markdown","a043e41a":"markdown","598815e4":"markdown","612f4db3":"markdown","e86dc829":"markdown","e1893666":"markdown","85ba920a":"markdown","3a84874e":"markdown","bd160929":"markdown","efc6c95b":"markdown","cf157af6":"markdown","b03f11d3":"markdown","89c33671":"markdown","d1efc4a0":"markdown","44a928da":"markdown","6d033c01":"markdown","f6240655":"markdown","b5836ecd":"markdown","7ccc18a4":"markdown","3a556cb6":"markdown","e666b2d9":"markdown","ebba0001":"markdown","b63e6fba":"markdown","7c1e4135":"markdown","c782ecc0":"markdown","42918e67":"markdown","dfd080f3":"markdown","19401f19":"markdown","4e041201":"markdown","c68b25d9":"markdown","47e45290":"markdown","90c584ac":"markdown","49222f1b":"markdown","fd3693c4":"markdown","509bcc07":"markdown","ebad898f":"markdown","8d12b099":"markdown","5dffbd18":"markdown","98e283ff":"markdown","c319b042":"markdown","d4c32f34":"markdown","81c52ad4":"markdown","2915ab2a":"markdown","e35e747b":"markdown","07263bd8":"markdown","690ad157":"markdown","a225d703":"markdown","f9bfa381":"markdown","b4e70aae":"markdown","cd378926":"markdown","79dbfcc6":"markdown","662868d2":"markdown","b3c59a4c":"markdown","ecf87adc":"markdown"},"source":{"29bd8f17":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport string\nfrom datetime import datetime, date\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n%matplotlib inline\npd.pandas.set_option('display.max_columns', None)","55a6dfe5":"df_train = pd.read_csv('..\/input\/consumer-complaint-resolution-data-set\/Consumer_Complaints_train.csv')\ndf_test = pd.read_csv('..\/input\/consumer-complaint-resolution-data-set\/Consumer_Complaints_test.csv')","579f3da1":"df_train.head()","bc82100d":"df_test.head()","8af29b14":"df_train.shape, df_test.shape","204a2861":"df_train.columns","fb7cb34e":"df_test.columns","fe15383e":"data_types_train = pd.DataFrame(df_train.dtypes, columns = ['Train'])\ndata_types_test = pd.DataFrame(df_test.dtypes, columns = ['Test'])\ndata_types = pd.concat([data_types_train, data_types_test], axis = 1)\ndata_types","48a0e17d":"missing_values_train = pd.DataFrame(df_train.isna().sum(), columns = ['Train'])\nmissing_values_test = pd.DataFrame(df_test.isna().sum(), columns = ['Test'])\nmissing_values = pd.concat([missing_values_train, missing_values_test], axis = 1)\nmissing_values","c068599b":"columns_with_missing_values = ['Sub-product', 'Sub-issue', 'Consumer complaint narrative', 'Company public response', 'Tags', 'Consumer consent provided?']\ndf_train = df_train.drop(columns_with_missing_values, axis = 1)\ndf_test = df_test.drop(columns_with_missing_values, axis = 1)","fccb2675":"df_train['Year_Received'] = df_train['Date received'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').year)\ndf_test['Year_Received'] = df_test['Date received'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').year)\ndf_train['Month_Received'] = df_train['Date received'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').month)\ndf_test['Month_Received'] = df_test['Date received'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').month)\ndf_train['Day_Received'] = df_train['Date received'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').day)\ndf_test['Day_Received'] = df_test['Date received'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').day)","b2f8fbc0":"df_train['Year_Sent'] = df_train['Date sent to company'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').year)\ndf_test['Year_Sent'] = df_test['Date sent to company'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').year)\ndf_train['Month_Sent'] = df_train['Date sent to company'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').month)\ndf_test['Month_Sent'] = df_test['Date sent to company'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').month)\ndf_train['Day_Sent'] = df_train['Date sent to company'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').day)\ndf_test['Day_Sent'] = df_test['Date sent to company'].apply(lambda dateString : datetime.strptime(dateString,'%Y-%m-%d').day)","5f1dadda":"df_train['Date received'] = pd.to_datetime(df_train['Date received'])\ndf_test['Date received'] = pd.to_datetime(df_test['Date received'])\ndf_train['Date sent to company'] = pd.to_datetime(df_train['Date sent to company'])\ndf_test['Date sent to company'] = pd.to_datetime(df_test['Date sent to company'])","37c60710":"df_train['Days held'] = df_train['Date sent to company'] - df_train['Date received']\ndf_test['Days held'] = df_test['Date sent to company'] - df_test['Date received']","591b9ce6":"df_train['Days held'] = df_train['Days held'].astype('timedelta64[D]').astype(int)\ndf_test['Days held'] = df_test['Days held'].astype('timedelta64[D]').astype(int)","2c0c5bf1":"df_train = df_train.drop(['Date received', 'Date sent to company'], axis = 1)\ndf_test = df_test.drop(['Date received', 'Date sent to company'], axis = 1)","8b8589dd":"df_train = df_train.drop(['ZIP code', 'Complaint ID'], axis = 1)\ndf_test = df_test.drop(['ZIP code', 'Complaint ID'], axis = 1)","4ba31c28":"df_train['State'].mode(), df_test['State'].mode()","6e90d720":"df_train['State'] = df_train['State'].replace(np.nan, 'CA')\ndf_test['State'] = df_test['State'].replace(np.nan, 'CA')","00e9b42a":"missing_values_train = pd.DataFrame(df_train.isna().sum(), columns = ['Train'])\nmissing_values_test = pd.DataFrame(df_test.isna().sum(), columns = ['Test'])\nmissing_values = pd.concat([missing_values_train, missing_values_test], axis = 1)\nmissing_values","6aab973f":"df_train = df_train.drop(['Year_Sent', 'Month_Sent', 'Day_Sent'], axis = 1)\ndf_test = df_test.drop(['Year_Sent', 'Month_Sent', 'Day_Sent'], axis = 1)","f00dc780":"week_train = []\nfor i in df_train['Day_Received']:\n    if i < 8:\n        week_train.append(1)\n    elif i >= 8 and i < 16:\n        week_train.append(2)\n    elif i >=16 and i < 22:\n        week_train.append(3)\n    else:\n        week_train.append(4)\ndf_train['Week_Received'] = week_train\nweek_test = []\nfor i in df_test['Day_Received']:\n    if i < 8:\n        week_test.append(1)\n    elif i >= 8 and i < 16:\n        week_test.append(2)\n    elif i >=16 and i < 22:\n        week_test.append(3)\n    else:\n        week_test.append(4)\ndf_test['Week_Received'] = week_test","b3e12605":"df_train = df_train.drop(['Day_Received'], axis = 1)\ndf_test = df_test.drop(['Day_Received'], axis = 1)","17a7cf21":"df_train.head()","71acd0d4":"df_test.head()","6f613893":"disputed_cons = df_train[df_train['Consumer disputed?'] == 'Yes']","68e4185b":"sns.countplot(x = 'Consumer disputed?', data = df_train)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Consumer disputed', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","46debbdc":"sns.countplot(x = 'Product', hue = 'Consumer disputed?', data = df_train)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Product', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","627e3e07":"top_issues_disputes = disputed_cons['Issue'].value_counts().sort_values(ascending = False).head(10)\nsns.barplot(x = top_issues_disputes.index, y = top_issues_disputes.values)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Issues', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","20f28048":"fig, ax = plt.subplots(figsize=(20, 10))\nsns.countplot(x = df_train['State'])\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('State', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","62ed0265":"sns.countplot(x = 'Submitted via', data = disputed_cons)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Submitted Via', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","20e12dbf":"sns.countplot(x = 'Company response to consumer', data = df_train)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Company Response', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","0420b106":"sns.countplot(x = 'Company response to consumer', data = disputed_cons)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Company Response', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","ee4b17a9":"sns.countplot(x = 'Timely response?', data = disputed_cons)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Timely Response', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","220f6626":"sns.countplot(x = 'Year_Received', data = df_train)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Year', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","09d8f761":"sns.countplot(x = 'Year_Received', data = disputed_cons)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Year', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","6ba671a4":"sns.countplot(x = 'Month_Received', data = df_train)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Month', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","e6e29315":"sns.countplot(x = 'Month_Received', data = disputed_cons)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Month', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","75a03732":"sns.countplot(x = 'Week_Received', data = df_train)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Week', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","63e979ed":"sns.countplot(x = 'Week_Received', data = disputed_cons)\nplt.xticks(fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Week', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","840292a3":"worst_company_complaints = df_train['Company'].value_counts().sort_values(ascending = False).head(10)\nsns.barplot(x = worst_company_complaints.index, y = worst_company_complaints.values)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Company', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","119b19c5":"worst_company_disputes = disputed_cons['Company'].value_counts().sort_values(ascending = False).head(10)\nsns.barplot(x = worst_company_complaints.index, y = worst_company_complaints.values)\nplt.xticks(rotation = 90, fontsize = 10, ha = \"right\")\nplt.yticks(fontsize = 10)\nplt.xlabel('Company', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)","1f8e6bae":"df_train['Days held'].describe()","2c4c44aa":"df_test['Days held'].describe()","8c02c649":"Days_held_train = []\nfor i in df_train['Days held']:\n    if i < 0:\n        Days_held_train.append(0)\n    else:\n        Days_held_train.append(i)\ndf_train['Days_held'] = Days_held_train\nDays_held_test = []\nfor i in df_test['Days held']:\n    if i < 0:\n        Days_held_test.append(0)\n    else:\n        Days_held_test.append(i)\ndf_test['Days_held'] = Days_held_test","55594eec":"df_train = df_train.drop('Days held', axis = 1)\ndf_test = df_test.drop('Days held', axis = 1)","0d434918":"relevant_text_train = df_train['Issue']\nrelevant_text_test = df_test['Issue']\ntokenized_data_train = relevant_text_train.apply(lambda x: wordpunct_tokenize(x.lower()))\ntokenized_data_test = relevant_text_test.apply(lambda x: wordpunct_tokenize(x.lower()))\ndef remove_punctuation(text):\n    no_punctuation = []\n    for w in text:\n        if w not in string.punctuation:\n            no_punctuation.append(w)\n    return no_punctuation\nno_punctuation_data_train = tokenized_data_train.apply(lambda x: remove_punctuation(x))\nno_punctuation_data_test = tokenized_data_test.apply(lambda x: remove_punctuation(x))\nstop_words = stopwords.words('english')\nfiltered_sentence_train = [w for w in no_punctuation_data_train if not w in stop_words]\nfiltered_sentence_train = pd.Series(filtered_sentence_train)\nfiltered_sentence_test = [w for w in no_punctuation_data_test if not w in stop_words]\nfiltered_sentence_test = pd.Series(filtered_sentence_test)\ndef lemmatize_text(text):\n    lem_text = [WordNetLemmatizer().lemmatize(w,pos = 'v') for w in text]\n    return lem_text\nlemmatized_data_train = filtered_sentence_train.apply(lambda x:lemmatize_text(x))\nlemmatized_data_test = filtered_sentence_test.apply(lambda x:lemmatize_text(x))\ndef stem_text(text):\n    stem_text = [PorterStemmer().stem(w) for w in text]\n    return stem_text\nstemmed_data_train = lemmatized_data_train.apply(lambda x:stem_text(x))\nstemmed_data_test = lemmatized_data_test.apply(lambda x:stem_text(x))\ndef word_to_sentence(text):\n    text_sentence = \" \".join(text)\n    return text_sentence\nclean_data_train = stemmed_data_train.apply(lambda x:word_to_sentence(x))\nclean_data_test = stemmed_data_test.apply(lambda x:word_to_sentence(x))","b6bcf5bd":"df_train['Issues_cleaned'] = clean_data_train\ndf_test['Issues_cleaned'] = clean_data_test\ndf_train = df_train.drop('Issue', axis = 1)\ndf_test = df_test.drop('Issue', axis = 1)","0911b8d0":"drop_cols = ['Company', 'State', 'Year_Received', 'Days_held']\ndf_train = df_train.drop(drop_cols, axis = 1)\ndf_test = df_test.drop(drop_cols, axis = 1)","1fefa7e8":"df_train['Consumer disputed?'] = np.where(df_train['Consumer disputed?'] == \"Yes\", 1, 0)","e082b68b":"dum_cols = ['Product', 'Submitted via', 'Company response to consumer', 'Timely response?']\ndf_train_dummies = pd.get_dummies(df_train[dum_cols], prefix_sep = '_', drop_first = True)\ndf_test_dummies = pd.get_dummies(df_test[dum_cols], prefix_sep = '_', drop_first = True)","109556d0":"df_train = df_train.drop(dum_cols, axis = 1)\ndf_test = df_test.drop(dum_cols, axis = 1)\ndf_train = pd.concat([df_train, df_train_dummies], axis = 1)\ndf_test = pd.concat([df_test, df_test_dummies], axis = 1)","50f6245f":"tf = TfidfVectorizer()\nissues_cleaned_train = tf.fit_transform(df_train['Issues_cleaned']).toarray()\nissues_cleaned_test = tf.fit_transform(df_test['Issues_cleaned']).toarray()\ntf_columns_train = []\ntf_columns_test = []\nfor i in range(issues_cleaned_train.shape[1]):\n    tf_columns_train.append('Feature' + str(i+1))\nfor i in range(issues_cleaned_test.shape[1]):\n    tf_columns_test.append('Feature' + str(i+1))\nissues_train = pd.DataFrame(issues_cleaned_train, columns = tf_columns_train)\nissues_test = pd.DataFrame(issues_cleaned_test, columns = tf_columns_test)\nweights = pd.DataFrame(tf.idf_, index = tf.get_feature_names(), columns = ['Idf_weights']).sort_values(by = 'Idf_weights', ascending = False)\nweights.head()","3a9591b4":"df_train = df_train.drop('Issues_cleaned', axis = 1)\ndf_test = df_test.drop('Issues_cleaned', axis = 1)\ndf_train = pd.concat([df_train, issues_train], axis = 1)\ndf_test = pd.concat([df_test, issues_test], axis = 1)\nFeature168 = [0] * 119606\ndf_test['Feature168'] = Feature168","6c6e6094":"df_train.head()","b78356b6":"df_test.head()","ec29ecc4":"df_train.shape, df_test.shape","760b6536":"df_train_scaled = pd.DataFrame(StandardScaler().fit_transform(df_train.drop('Consumer disputed?', axis = 1)), columns = df_test.columns)\ndf_test_scaled = pd.DataFrame(StandardScaler().fit_transform(df_test), columns = df_test.columns)","eba49241":"pca_columns = []\nfor i in range(df_train_scaled.shape[1]):\n    pca_columns.append('PC' + str(i+1))\npca_model = PCA()\npca_model.fit(df_train_scaled)\ndf_pca_train = pd.DataFrame(pca_model.transform(df_train_scaled), columns = pca_columns)\nexplained_info_train = pd.DataFrame(pca_model.explained_variance_ratio_, columns=['Explained Info']).sort_values(by = 'Explained Info', ascending = False)\nimp = []\nfor i in range(explained_info_train.shape[0]):\n    imp.append(explained_info_train.head(i).sum())\nexplained_info_train_sum = pd.DataFrame()\nexplained_info_train_sum['Variable'] = pca_columns\nexplained_info_train_sum['Importance'] = imp\nexplained_info_train_sum.head(60)","90e715d1":"pca_columns = []\nfor i in range(53):\n    pca_columns.append('PC' + str(i+1))\npca_model = PCA(n_components = 53)\npca_model.fit(df_train_scaled)\ndf_pca_train = pd.DataFrame(pca_model.transform(df_train_scaled), columns = pca_columns)","b122209a":"df_pca_train.head()","c831af8d":"pca_model = PCA(n_components = 53)\npca_model.fit(df_test_scaled)\ndf_pca_test = pd.DataFrame(pca_model.transform(df_test_scaled), columns = pca_columns)","3e3252e8":"X = df_pca_train\ny = df_train['Consumer disputed?']","a998b3c7":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state = 17)\nX_test = df_pca_test","0473e1d6":"X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape","c138cc1b":"models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier(), KNeighborsClassifier(), XGBClassifier()]\nmodel_names = ['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier', 'KNeighborsClassifier', 'XGBClassifier']\naccuracy_train = []\naccuracy_val = []\nfor model in models:\n    mod = model\n    mod.fit(X_train, y_train)\n    y_pred_train = mod.predict(X_train)\n    y_pred_val = mod.predict(X_val)\n    accuracy_train.append(accuracy_score(y_train, y_pred_train))\n    accuracy_val.append(accuracy_score(y_val, y_pred_val))\ndata = {'Modelling Algorithm' : model_names, 'Train Accuracy' : accuracy_train, 'Validation Accuracy' : accuracy_val}\ndata = pd.DataFrame(data)\ndata['Difference'] = ((np.abs(data['Train Accuracy'] - data['Validation Accuracy'])) * 100)\/(data['Train Accuracy'])\ndata.sort_values(by = 'Validation Accuracy', ascending = False)","db31070b":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred_test = lr.predict(X_test)\ny_pred_test = pd.DataFrame(y_pred_test, columns = ['Prediction'])\ny_pred_test.head()","13e19bf9":"y_pred_test.to_csv('Prediction.csv')","0c8a6b73":"        98% disputes were timely repsonded at the intial stages.","3a7f834e":"        30% disputes are in the 4th week of the month.\n        50% disputes are in both the half of the month.","9742d836":"### Spliting the Data Sets Into X and Y","397129a8":"### Shape of Train and Test Data","c6842f23":"        LogisticRegression is the best model to build the model.","8be39fe9":"### Concating Clean Data to Train and Test Data and Dropping Issue","051bc02e":"### Calculating the Number of Days the Complaint was with the Company","05ca29c2":"        So 53 variables are making upto 80% of the information.","5f7effec":"### Feature Selection","cee9828c":"### Whether there are Disputes Instead of Timely Response","4eb40232":"### Replacing Issues_cleaned by Vectorized Issues","ae102e59":"### Disputed Consumers Data","e86873b3":"### Year Wise Disputes","74724fb0":"### Exporting Predictions to CSV","ba780f77":"### Maximum Disputes Submitted Via","a88dba67":"### Company's Response Leading to Disputes","37b2e77a":"### Train Data","b9a4d246":"        15% disputes from CA.\n        25% disputes from CA and FL\n        32% disputes from CA, FL and TX.\n        38% disputes from CA, FL, TX and NY.\n        43% disputes from CA, FL, TX, NY and GA.\n        47% disputes from CA, FL, TX, NY, GA and NJ.\n        50% disputes from CA, FL, TX, NY, GA, NJ and IL.\n        54% disputes from CA, FL, TX, NY, GA, NJ, IL and VA.\n        57% disputes from CA, FL, TX, NY, GA, NJ, IL, VA and PA.\n        61% disputes from CA, FL, TX, NY, GA, NJ, IL, VA, PA and MD.","ba34e773":"### Test Data","c0370b2f":"### Converting Negative Days Held to Zero","7e7c742e":"### Creating Dummy Variables","ce52dad3":"### Dropping Year Sent, Month Sent and Day Sent","23d689f1":"### Importing Libraries","4f398932":"### Changing Consumer Disputed Column to 0 and 1","8de8bca8":"### Shapes of the Data Sets","d8b2ec07":"### Dropping Missing Values","01f51a2f":"### Month Wise Complaints","a043e41a":"        66% disputes are from these 10 issues.","598815e4":"### Top Issues with Highest Disputes","612f4db3":"### Train Data","e86dc829":"### Extracting Date, Month and Year From Date Sent to the Company Column","e1893666":"### Top Companies with Highest Disputes","85ba920a":"        82% disputes are closed with explanation at the initial stage.\n        89% disputes are either closed with explanation or non-monetary relief in the earlier stage.","3a84874e":"### Checking Missing Values  Data","bd160929":"### Calculating TF-IDF","efc6c95b":"### Train Data","cf157af6":"### Extracting Date, Month and Year from Date Received Column","b03f11d3":"        28% complaints are raised in 2015.\n        53% complaints are raised in 2014 and 2015.\n        71% complaints are raised in 2013 to 2015.\n        88% complaints are raised in 2013 to 2016.","89c33671":"### Week Wise Complaints","d1efc4a0":"### Products-wise Disputes","44a928da":"        Roughly 37% consumer with mortgage have disputed.\n        Approx. 54% consumer who have disputed are from mortgage or debt collection.\n        68% consumer having disputes are from mortgage, debt collection and credit reporting.\n        Adding credit card consumers and bank account or services consumer will make it 80% and 91% respectively.","6d033c01":"### Days Held Column Analysis","f6240655":"### Dropping Day_Received","b5836ecd":"### Top Companies with Highest Complaints","7ccc18a4":"        72% disputes are submitted via web.\n        88% disputes are submitted via web and referral.","3a556cb6":"        27% disputes are raised in 2015.\n        50% disputes are raised in 2014 and 2015.\n        69% disputes are raised in 2014 to 2016.\n        87% disputes are raised in 2013 to 2016.","e666b2d9":"### Test Data","ebba0001":"        56% disputes are raised in March to August.\n        73% disputes are raised in January to August.","b63e6fba":"### Shape of Train and Test Data","7c1e4135":"### Categorizing Days into Weeks","c782ecc0":"### Month Wise Disputes","42918e67":"### Final Model and Prediction","dfd080f3":"        74% complaints are closed with explanation.","19401f19":"### Data Types","4e041201":"        53% disputes are for these 10 companies.","c68b25d9":"        Roughly 21% of the consumer have disputed.","47e45290":"        30% complaints are in the 4th week of the month.\n        50% complaints are in both the half of the month.","90c584ac":"### Converting Dates from Object Type to Datetime Type","49222f1b":"### Columns of Train and Test Data","fd3693c4":"### Test Data","509bcc07":"### Dropping ZIP Code, Complaint ID","ebad898f":"### Imputing Nulls in State by Mode","8d12b099":"### Missing Values","5dffbd18":"### State with Maximum Disputes","98e283ff":"### Dropping Unneccesary Columns for the Model Building","c319b042":"### Scaling the Data Sets","d4c32f34":"### Year Wise Complaints","81c52ad4":"### Dropping Days Held with Negative Values\n","2915ab2a":"### Reading Data Sets","e35e747b":"### Dropping Date Received and Date Sent to Company","07263bd8":"### Company's Response to the Complaints","690ad157":"### Performance of Different Algorithms","a225d703":"        64% complaints are raised in January to July.\n        72% complaints are raised in January to August.","f9bfa381":"### Train, Validation and Test Data Sets","b4e70aae":"### Week Wise Disputes ","cd378926":"        53% complaints are for these 10 companies.","79dbfcc6":"### Concating Dummy Variables and Dropping the Original Columns","662868d2":"### Converting Days Held to Int","b3c59a4c":"### Number of Disputes","ecf87adc":"### NLP Pre-Processing"}}