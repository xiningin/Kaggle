{"cell_type":{"a0a6e88a":"code","6b85f6a5":"code","6850458b":"code","dcf83be1":"code","8eb1be8b":"code","4e852700":"code","2e4dbd38":"code","586380f1":"code","19ff10c1":"code","2e61b217":"code","9906b15f":"code","274edd5d":"code","81fd40b2":"code","fb2818df":"code","d49a8a78":"code","efd32a24":"code","33bf6acf":"code","c4d98719":"code","2f0b2389":"code","5618d9ec":"code","fd025381":"code","f20d0f3a":"code","d3c3d7e6":"code","fc89025b":"code","412633e3":"code","6f6c5b56":"code","4d67d21d":"code","1205dcd9":"markdown","a5f6a0ef":"markdown","49a4ee35":"markdown","7cbd5f6c":"markdown","ee2d7c32":"markdown","b58f86da":"markdown","29f09bee":"markdown","546dc4c8":"markdown","5565a2a2":"markdown","b21990a4":"markdown","d1f16227":"markdown","7f1e6a42":"markdown","6cf52e41":"markdown","dfbde39d":"markdown","38c82588":"markdown","a710b3cb":"markdown","2577911e":"markdown","c93c0296":"markdown","c6f1f130":"markdown","d7af206b":"markdown","2d9a9884":"markdown","57257a58":"markdown","26289370":"markdown","d9ce8ac6":"markdown"},"source":{"a0a6e88a":"import nltk\n#nltk.download()\nfrom nltk.corpus import stopwords\n","6b85f6a5":"import pandas as pd\nadd = \"..\/input\/spam.csv\"\ndata = pd.read_csv(add, encoding='latin-1')\ndata.head(5)\ndata = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\npd.set_option('display.max_colwidth', 0)\ndata.columns = ['label','text']\n\ndata.head(5)\n\n#print(data.head(5))","6850458b":"import string\nimport re\nstopword = nltk.corpus.stopwords.words('english')","dcf83be1":"ps = nltk.PorterStemmer()","8eb1be8b":"def clean_text(text):\n    remove_punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+',remove_punct)\n    noStop = ([ps.stem(word) for word in tokens if word not in stopword])\n    return noStop","4e852700":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_Vector = TfidfVectorizer(analyzer= clean_text)\nXtfidf_Vector = tfidf_Vector.fit_transform(data['text'])","2e4dbd38":"import string","586380f1":"def punct_percent(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/ (len(text) - text.count(\" \")),3)*100\ndata['punct_%'] = data['text'].apply(lambda x: punct_percent(x))\ndata['length'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\n","19ff10c1":"pd.set_option('display.max_colwidth', 0)\n\nprint(data.head(5))","2e61b217":"import re\n\n#def find_num(text):\n#    return re.findall('\\d{7,}',text)\n\ndata['number'] = pd.DataFrame(data['text'].apply(lambda x: len(re.findall('\\d{5,}',x))))","9906b15f":"data.head(5)","274edd5d":"\n#def get_currency_symbol(text):\n#    pattern = r'(\\D*)\\d*\\.?\\d*(\\D*)'\n#    result = re.match(pattern,text).group()\n#    return result\n#data['currency']= pd.DataFrame(data['text'].apply(lambda x: len(get_currency_symbol(x))))\n","81fd40b2":"#print(data.head(5))","fb2818df":"def web_address(t):\n    if(len(re.findall('www|http|https|.co',t)) > 0):\n        return 1\n    else:\n        return 0\n    \ndata['url'] = pd.DataFrame(data['text'].apply(lambda x: web_address(x)))\nprint(data.head(5))   ","d49a8a78":"import numpy as np\nfrom matplotlib import pyplot\n%matplotlib inline","efd32a24":"bins = np.linspace(0,200,40)\npyplot.hist(data[data['label'] == 'spam']['length'],bins,alpha = 0.5,normed = True,label = 'spam')\npyplot.hist(data[data['label'] == 'ham']['length'],bins,alpha = 0.5,normed = True, label = 'ham')\npyplot.legend(loc = 'upper right')\npyplot.figure(figsize = (1000,400), dpi = 1000)\npyplot.show()","33bf6acf":"bins = np.linspace(0,50,40)\npyplot.hist(data[data['label'] == 'spam']['punct_%'], bins, alpha = 0.5,normed = True, label = 'spam')\npyplot.hist(data[data['label'] == 'ham']['punct_%'], bins, alpha = 0.5,normed = True, label = 'ham')\npyplot.legend(loc = 'upper right')\npyplot.show()","c4d98719":"bins = np.linspace(0,5,10)\npyplot.hist(data[data['label'] == 'spam']['number'], bins,alpha = 0.5, label = 'spam')\npyplot.hist(data[data['label'] == 'ham']['number'], bins, alpha = 0.5, label = 'ham')\npyplot.legend(loc = 'upper right')\npyplot.show()","2f0b2389":"bins = np.linspace(0,5,100)\npyplot.hist(data[data['label'] == 'spam']['url'], bins,alpha = 0.5, label = 'spam')\npyplot.hist(data[data['label'] == 'ham']['url'], bins, alpha = 0.5, label = 'ham')\npyplot.legend(loc = 'upper right')\npyplot.show()","5618d9ec":"data['url'].value_counts()","fd025381":"Xfeatures_data = pd.concat([data['length'],data['punct_%'],data['number'],data['url'], pd.DataFrame(Xtfidf_Vector.toarray())], axis = 1)\nXfeatures_data.head(5)","f20d0f3a":"from sklearn.ensemble import RandomForestClassifier\nprint(dir(RandomForestClassifier))\nprint(RandomForestClassifier())","d3c3d7e6":"from sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(Xfeatures_data, data['label'], test_size = 0.2)\n\n","fc89025b":"rf = RandomForestClassifier(n_estimators= 50, max_depth= 20, n_jobs = -1)\nrf_model = rf.fit(X_train,y_train)\nsorted(zip(rf.feature_importances_,X_train.columns), reverse= True)[0:10]","412633e3":"y_pred = rf_model.predict(X_test)\nprecision,recall,fscore,support = score(y_test,y_pred,pos_label = 'spam', average = 'binary')\nprint('Precision: {}\/ Recall: {}\/ Accuracy: {}'.format(round(precision,3), round(recall,3), (y_pred == y_test).sum()\/len(y_pred)))","6f6c5b56":"def train_rf(n_est, depth):\n    rf = RandomForestClassifier(n_estimators= n_est, max_depth= depth, n_jobs = -1)\n    rf_model = rf.fit(X_train,y_train)\n    y_pred = rf_model.predict(X_test)\n    precision,recall,fscore,support = score(y_test,y_pred,pos_label= 'spam',average= 'binary')\n    print('Est: {}\/ Depth: {}\/ Precision: {}\/ Recall: {}\/ Accuracy : {}'.format(n_est,depth, round(precision,3), round(recall,3), (y_pred == y_test).sum()\/len(y_pred)))","4d67d21d":"for n_est in [10,30,50,70]:\n    for depth in [20,40,60,80, None]:\n        train_rf(n_est,depth)\n","1205dcd9":"### Applying the Data Cleaning Pipeline  - followed by TF_IDF","a5f6a0ef":"#### Feature for Length of Text messages & Punctuation Percentage","49a4ee35":"# Importing NLP's nltk library\n# Downloading Stopwords to remove from the dataset later","7cbd5f6c":"# 4. Understand the Feartures","ee2d7c32":"# 3. Feature Engineering","b58f86da":"####  0 values : 78.4%\n#### 1 valyes  : 21.6%","29f09bee":"# Spam Detection using NLP and Random Forest","546dc4c8":"# 10. Tweaking hyperparameters using Grid Search","5565a2a2":"We'll apply Stemming so as to remove any redundancy among words, i.e words which have the same root eg. running and run etc would be converted into run using stemming. \n\nNOTE :  For the same task,another technique named lemmatizing can also be used, which also considers the grammatical sense of the word in the context in which it has been used.\n\nAs we're using stemming here, we'll use the portstemmer as below","b21990a4":"# 9. Applying the model on Test set","d1f16227":"We'll Remove  all the Punctuation, StopWords from the dataset and finally apply stemming on the resulting dataset to make clean, tidy and ready for processing","7f1e6a42":"# 7. Building the model on Train Data\n# 8. Understanding the relative importance of features added","6cf52e41":"The stopwords can be in many languages, for this particular dataset we'll use the English Language words","dfbde39d":"# 1. Data Cleaning Pipeline","38c82588":"# 2. Applying NLYKs - TF-IDF on the Cleaned Dataset","a710b3cb":"#### Feature for web address","2577911e":"#### Feature for Currency","c93c0296":"# 6. Splitting the Dataset into Test and Train to build and apply the model\n","c6f1f130":"This shows that our original assumption about < numbers, length, url > in a row being good predictors of ham\/spam class was correct","d7af206b":"#### Drawing pyplot for Datawith url as feature for data","2d9a9884":"# Conclusion :\n\n\n\nSo our best results are :  Est: 70\/ Depth: None\/ Precision: 1.0\/ Recall: 0.908\/ Accuracy : 0.9883303411131059","57257a58":"#### Features for Rows having numbers with number.length > =5","26289370":"# 5. Making new Dataframe with all features for prediction and TF-IDF data","d9ce8ac6":"This Project focusses on the following areas : \n\n1. Data Cleaning Pipeline : \n    - Removing Punctuation\n    - Tokenizing\n    - Removing Stop Words\n2. Applying NLYKs - TF-IDF on the Cleaned Dataset\n3. Feature Engineering\n    - Length of Text Messages\n    - Percent of punctuation used in each corpus\n    - Number finding in each corpus\n    - Web links in each corpus\n4. Understanding the Features\n5. Making a new Dataframe having all the required features for prediction\n   and have the TF-IDF data\n6. Splitting the Dataset into Test and Train to build and apply the model\n7. Building model on Train set using Random Forest\n8. Understanding the relative importance of features used\n9. Applying the model on Test set\n10. Tweaking hyperparameters using Grid Search\n"}}