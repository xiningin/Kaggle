{"cell_type":{"ee61d941":"code","06d397ce":"code","502a4d18":"code","bd574f22":"code","7c50319c":"code","d662ec5b":"code","54830ee8":"code","25a1c52f":"code","2d0b4579":"code","1b361760":"code","075e6f3b":"code","5d34349b":"code","c5d55c6c":"code","4ea96419":"code","9b0b413d":"code","c63c1100":"code","e88e4e9d":"code","80016f6e":"code","fc31870b":"code","80471be1":"code","98c74e0b":"code","405a1299":"code","1adf8bde":"code","8fdd24ca":"code","f5f64a6b":"code","c460dde5":"code","55b95856":"code","4a171b5a":"code","eb1b10c0":"code","461205ff":"code","67308889":"markdown","bcb0c7ca":"markdown","2fb2fd75":"markdown","83bb73a6":"markdown","e82e6164":"markdown","f2bac53d":"markdown","0ce57aea":"markdown","c814c263":"markdown","5550b56d":"markdown","89b98b43":"markdown","babf8d78":"markdown","4b512673":"markdown","0fa41295":"markdown","e7906221":"markdown","37c9fd00":"markdown","b5324371":"markdown","e1e6a27e":"markdown","d5b76edb":"markdown","4420cc3d":"markdown"},"source":{"ee61d941":"!pip install fast-bert","06d397ce":"!git clone https:\/\/github.com\/NVIDIA\/apex","502a4d18":"%cd apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n%cd ..","bd574f22":"import pandas as pd\nimport os","7c50319c":"!mkdir data\nDATA_FOLDER = 'data'","d662ec5b":"train_df = pd.read_csv('..\/input\/train.csv', index_col='id')\nval_df = pd.read_csv('..\/input\/valid.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/test.csv', index_col='id') ","54830ee8":"label_cols = list(pd.get_dummies(train_df['label']).columns)\nwith open(os.path.join(DATA_FOLDER, 'labels.csv'), 'w') as f:\n    f.write('\\n'.join(label_cols))\n","25a1c52f":"def to_fastbert(df:pd.DataFrame, name:str):\n  d = pd.get_dummies(df['label'])\n  d['text'] = df['text']\n  d.to_csv(os.path.join(DATA_FOLDER, f'{name}.csv'), index=True, index_label=['id'])\n  ","2d0b4579":"for x,n in zip([train_df, val_df], ['train', 'val']):\n  to_fastbert(x, n)","1b361760":"#!wget https:\/\/storage.googleapis.com\/bert_models\/2018_11_23\/multi_cased_L-12_H-768_A-12.zip","075e6f3b":"#!unzip multi_cased_L-12_H-768_A-12.zip","5d34349b":"#!git clone https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT.git","c5d55c6c":"#!python pytorch-pretrained-BERT\/pytorch_pretrained_bert\/convert_tf_checkpoint_to_pytorch.py --tf_checkpoint_path multi_cased_L-12_H-768_A-12\/bert_model.ckpt --bert_config_file multi_cased_L-12_H-768_A-12\/bert_config.json --pytorch_dump_path multi_cased_L-12_H-768_A-12\/pytorch_model.bin","4ea96419":"from pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertForPreTraining, BertConfig, BertForMaskedLM, BertForSequenceClassification\nfrom pathlib import Path\nimport torch\n\nfrom fastai.text import Tokenizer, Vocab\nimport pandas as pd\nimport collections\nimport os\nfrom tqdm import tqdm, trange\nimport sys\nimport random\nimport numpy as np\nimport apex\nfrom sklearn.model_selection import train_test_split\n\nimport datetime\n    \nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom pytorch_pretrained_bert.optimization import BertAdam\n\nfrom fast_bert.modeling import BertForMultiLabelSequenceClassification\nfrom fast_bert.data import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features\nfrom fast_bert.learner import BertLearner\nfrom fast_bert.metrics import accuracy_multilabel, accuracy_thresh, fbeta, roc_auc","9b0b413d":"torch.cuda.empty_cache()\npd.set_option('display.max_colwidth', -1)\nrun_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')","c63c1100":"DATA_PATH = Path('data\/')\nLABEL_PATH = Path('data\/')\n\nMODEL_PATH=Path('models\/')\nLOG_PATH=Path('logs\/')\nMODEL_PATH.mkdir(exist_ok=True)\n\nmodel_state_dict = None\n\nFINETUNED_PATH = None\n# model_state_dict = torch.load(FINETUNED_PATH)\n\nLOG_PATH.mkdir(exist_ok=True)","e88e4e9d":"# If using a local version, download and convert by uncommenting cells 10,11,12,13\n#BERT_PRETRAINED_PATH = Path('multi_cased_L-12_H-768_A-12\/')\n\n# We'll let the library get a plus-n-play model \nBERT_PRETRAINED_PATH = 'bert-base-multilingual-cased'\n\n","80016f6e":"args = {\n    \"run_text\": \"amazon pet review\",\n    \"train_size\": -1,\n    \"val_size\": -1,\n    \"log_path\": LOG_PATH,\n    \"full_data_dir\": DATA_PATH,\n    \"data_dir\": DATA_PATH,\n    \"task_name\": \"Amazon Pet Review\",\n    \"no_cuda\": False,\n    \"bert_model\": BERT_PRETRAINED_PATH,\n    \"output_dir\": MODEL_PATH\/'output',\n    \"max_seq_length\": 512,\n    \"do_train\": True,\n    \"do_eval\": True,\n    \"do_lower_case\": False,\n    \"train_batch_size\": 16,\n    \"eval_batch_size\": 16,\n    \"learning_rate\": 5e-6,\n    \"num_train_epochs\": 4.0,\n    \"warmup_proportion\": 0.1,\n    \"no_cuda\": False,\n    \"local_rank\": -1,\n    \"seed\": 42,\n    \"gradient_accumulation_steps\": 1,\n    \"optimize_on_cpu\": False,\n    \"fp16\": True,\n    \"loss_scale\": 128\n}","fc31870b":"import logging\n\nlogfile = str(LOG_PATH\/'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n    datefmt='%m\/%d\/%Y %H:%M:%S',\n    handlers=[\n        logging.FileHandler(logfile),\n        logging.StreamHandler(sys.stdout)\n    ])\n\nlogger = logging.getLogger()","80471be1":"logger.info(args)","98c74e0b":"tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_PATH, do_lower_case=args['do_lower_case'])","405a1299":"device = torch.device('cuda')\nif torch.cuda.device_count() > 1:\n    multi_gpu = True\nelse:\n    multi_gpu = False","1adf8bde":"databunch = BertDataBunch(args['data_dir'], LABEL_PATH, tokenizer, train_file='train.csv', val_file='val.csv',\n                          test_data=list(test_df['text'].values),\n                          text_col=\"text\", label_col=label_cols,\n                          bs=args['train_batch_size'], maxlen=args['max_seq_length'], \n                          multi_gpu=multi_gpu, multi_label=True)\ndatabunch.save()","8fdd24ca":"num_labels = len(databunch.labels)","f5f64a6b":"from functools import partial\n\nmetrics = []\nmetrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})\nmetrics.append({'name': 'roc_auc', 'function': roc_auc})\nmetrics.append({'name': 'F1', 'function': partial(fbeta, beta=1)})\nmetrics.append({'name': 'accuracy_single', 'function': accuracy_multilabel})","c460dde5":"learner = BertLearner.from_pretrained_model(databunch, BERT_PRETRAINED_PATH, metrics, device, logger, \n                                            finetuned_wgts_path=FINETUNED_PATH, \n                                            is_fp16=args['fp16'], loss_scale=args['loss_scale'], \n                                            multi_gpu=multi_gpu,  multi_label=True)","55b95856":"learner.fit(4, lr=args['learning_rate'], schedule_type=\"warmup_cosine_hard_restarts\")","4a171b5a":"preds = learner.predict_batch()","eb1b10c0":"test_df['label'] = [max(x, key=lambda z: z[1])[0] for x in preds]\ntest_df['label'].to_csv('fast_bert_submission.csv', index=True, index_label=['id'], header=True)","461205ff":"!rm -Rf apex","67308889":"## Set up path for data","bcb0c7ca":"## Create an estimator and train it\nThis is now a complete model, inluding a BERT model for feature-generation, on top of which a classifier is added.\nIt is 1 fully connected layer, that takes the embedding for the `<CLS>` token as input, and outputs as many logits as labels.\n\nThere is no way in current code to force another kind of classifier (more layers, etc...). The way to do that would be to inheritate the `BertForMultiLabelSequenceClassification` class and add parameters that describe the Classifier Neural Network, or to suggest a PR to the author with a modified Base class.\nSee https:\/\/github.com\/kaushaltrivedi\/fast-bert\/blob\/master\/fast_bert\/modeling.py \n\nWe use all the created objects...\n- the databunch, to feed data into the model\n- the path to the BERT pre-trained, for weight initialization\n- the metrics, training device, logger\n- the path where the fine-tuned model will be saved\n\nOnce the `BertLearner` is created, the `fit()` method is called for the training.","2fb2fd75":"# Get a Pre-trained BERT","83bb73a6":"# Clean","e82e6164":"## Convert BERT from Tensorflow to PyTorch (if using a local version)","f2bac53d":"**(45mins per epoch, with GPU activated)**","0ce57aea":"## Labels\nfast-bert needs the list of labels in a flat file","c814c263":"## Instantiate a Tokenizer from a pre-trained BERT\n**Careful with lower_case**","5550b56d":"# Demonstration Kernel with fast-bert\n\nThis notebook is an annotated copy of https:\/\/github.com\/kaushaltrivedi\/fast-bert\/blob\/master\/sample_notebooks\/toxic-multilabel-lib.ipynb\n\n","89b98b43":"## Imports and setup","babf8d78":"## Logging","4b512673":"## Load the data from prepared dataset\n\nThe `BertDataBunch` organizes the delivery of data to the model during training.\n\nSee https:\/\/github.com\/kaushaltrivedi\/fast-bert\/blob\/master\/fast_bert\/data.py\n\nArgument per argument:\n\n- `data_dir`: folder where to look for data files\n- `label_dir`: folder with the labels file\n- `tokenizer`: the tokenizer that will be used to pre-process the text before submitting to BERT embedding layer\n- `train_file`, `val_file`. `test_data`: csv files with the data. in multi-class situation, the labels must be 1-hot encoded with dummy columns\n- `label_file`: the file with the list of labels. Omitted in our case, the default is `labels.csv`\n- `text_col`: in the dataframes made from the data csv files, what is the name of the column with the text to be classified\n- `label_col`: in the dataframes made from the data csv files, what are the names of the columns with the 1-hot encoded classification labels\n- `bs`: batch size for training\n- `max_len`: maximum sequence length for BERT input\n- `multi_gpu`: True if training will happen over multiple gpu\n- `multi_label`: True is the classification task is multi-label, False if binary classification\n\nThe resulting object will be used when instantiating the BertLearner that will actually do the training.\n\n**Can take some time, as it is pre-processing all text, using the provided Tokenizer**","0fa41295":"# Install Packages\nfast-bert (will install bert)\n\napex (must be compiled from source) **(can take ~5mins)**","e7906221":"# Code for the Classifier based on BERT","37c9fd00":"## All arguments for training the model","b5324371":"## Metrics to report at evaluation time","e1e6a27e":"# Data Handling\nfast-bert can deal directly with text, pre-processing will be handled. CSV file needs to be adapted, as it expects the multi-class labels to be given as dummy variables, while in original format we have a single column with the label","d5b76edb":"# Predict the TEST set","4420cc3d":"For the pre-trained BERT model, 2 choices:\n* Provide the path to a saved \/ downloaded model (like we did above, in folder `multi_cased_L-12_H-768_A-12`)\n* Give the name of a standard model, in the list `[bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese]`\n\nThese two options are valid when using `BertTokenizer.from_pretrained()` and `BertLearner.from_pretrained_model()` factory methods"}}