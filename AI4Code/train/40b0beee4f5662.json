{"cell_type":{"d4fdefec":"code","873934cf":"code","6dae919c":"code","9f846078":"code","04348f0c":"code","4f0af65b":"code","223391f3":"code","3476cabe":"code","35d9a33f":"code","29f90686":"code","2df35836":"code","edb8618a":"code","cbb738ea":"code","3daf7ab9":"code","cddd75bf":"code","e1567fcd":"code","f7f35b85":"code","ace6aef9":"code","10d4db5e":"code","c4aeac42":"code","80f20a84":"code","62b72f99":"code","4c51ea1c":"code","590211f5":"code","85b4f98b":"code","d63cb6bd":"code","db21a138":"code","aac14606":"code","56d15680":"code","778e69e0":"code","0a478a82":"code","6812130b":"code","c0b7d690":"code","d03d6691":"code","d9a573f8":"code","2b51823b":"code","281c4c6b":"code","39e594f9":"code","a1bd8df0":"code","e40bb00b":"code","5835601b":"code","14af0855":"code","b4d279bb":"code","8e3a00ac":"code","f3bb8337":"code","4fe39f8b":"code","9ddd6f14":"code","017197d3":"code","262f35e1":"code","0d27170e":"code","b182a015":"code","88649eac":"code","cf201806":"code","1c80fba4":"code","7ee44c7d":"code","e14d17e3":"code","a0d15871":"code","b635e17d":"code","08d66e7d":"code","36ae38bd":"code","1bec5b6c":"code","06e0ddfc":"code","b62ddf8c":"code","6708d21e":"code","4f637101":"code","fe193e58":"code","e6d7ce22":"code","6bffb894":"code","e4111121":"code","9a122af3":"code","6dcd1b29":"code","107bc9e6":"code","51cb97d3":"code","5224f2f4":"code","de7a59ff":"code","32acb693":"code","43420f46":"code","c5a17407":"code","19286e1a":"code","646c4cce":"code","8884b852":"code","d406e9cf":"code","6d7b06f8":"code","9702fd59":"code","7871e088":"code","3d00fd67":"code","05e9140d":"code","dbd476fc":"code","db71b590":"code","96748abb":"markdown","1bacbf70":"markdown","f6de4fe2":"markdown","cccf95da":"markdown","cacafa8f":"markdown","67561ff5":"markdown","5cf73db6":"markdown","87a5c053":"markdown","5dc65249":"markdown","983a430f":"markdown","8dbd8e47":"markdown","7495dc4c":"markdown","62e3dd78":"markdown","d16fbe52":"markdown","f56646e3":"markdown","1225b3d3":"markdown","0765d302":"markdown","bb034376":"markdown","1afb9cd9":"markdown","bacb6062":"markdown","8a4380e8":"markdown","52bb825d":"markdown","68045000":"markdown","18416b07":"markdown","a72a4a4a":"markdown","1dcd9e18":"markdown","81563d33":"markdown","ec594b04":"markdown","9d77026e":"markdown","eaf40013":"markdown","d1485383":"markdown","0b4f8bfe":"markdown","ea42e458":"markdown","b7c301fc":"markdown","8ae5ba14":"markdown","90aaf159":"markdown","05795a79":"markdown","4cae2868":"markdown","bf33106c":"markdown","b1cc096b":"markdown","d76affee":"markdown","c7e8ad37":"markdown","14a46a50":"markdown","ce6d2844":"markdown","257b07ee":"markdown","cb822c76":"markdown","bd2a4882":"markdown","fdf6413c":"markdown","0ee69d0c":"markdown","419e8327":"markdown","ebc6cafe":"markdown","55501c5d":"markdown","3efdf7f6":"markdown","b391e766":"markdown","342982a0":"markdown","6caea156":"markdown","fb659193":"markdown","16966319":"markdown","b204bb17":"markdown","4a924959":"markdown","0ae12aa4":"markdown","a2ec0a3f":"markdown"},"source":{"d4fdefec":"# Useful functions and constants from\n# https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions\/notebook\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport json\nimport os\nfrom IPython.core.display import display, HTML\n\n# Fix DOI links\ndef doi_url(d):\n    if d.startswith('http'):\n        return d\n    elif d.startswith('doi.org'):\n        return f'http:\/\/{d}'\n    else:\n        return f'http:\/\/doi.org\/{d}'\n    \n# Turn authors list into '<surname>' or '<surname> et al'\ndef shorten_authors(authors):\n    if isinstance(authors, str):\n        authors = authors.split(';')\n        if len(authors) == 1:\n            return authors[0].split(',')[0]\n        else:\n            return f'{authors[0].split(\",\")[0]} et al'\n    else:\n        return authors\n\ndef load_metadata(metadata_file):\n    df = pd.read_csv(metadata_file,\n                 dtype={'Microsoft Academic Paper ID': str,\n                        'pubmed_id': str},\n                 low_memory=False)\n    df.doi = df.doi.fillna('').apply(doi_url)\n    df['authors_short'] = df.authors.apply(shorten_authors)\n    df['sorting_date'] = pd.to_datetime(df.publish_time)\n    print(f'loaded DataFrame with {len(df)} records')\n    return df.sort_values('sorting_date', ascending=False)\n\ndef abstract_title_filter(df, search_string):\n    return (df.abstract.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False) |\n            df.title.str.lower().str.replace('-', ' ')\n            .str.contains(search_string, na=False))\n\n# Helper function which counts synonyms and adds tag column to DF\ndef count_and_tag(df: pd.DataFrame,\n                  synonym_list: list,\n                  tag_suffix: str) -> (pd.DataFrame, pd.Series):\n    counts = {}\n    df[f'tag_{tag_suffix}'] = False\n    for s in synonym_list:\n        synonym_filter = abstract_title_filter(df, s)\n        counts[s] = sum(synonym_filter)\n        df.loc[synonym_filter, f'tag_{tag_suffix}'] = True\n    print(f'Added tag_{tag_suffix} to DataFrame')\n    return df\n\ndef dotplot(input_series, title, x_label='Count', y_label='Regex'):\n    subtitle = '<br><i>Hover over dots for exact values<\/i>'\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(\n    x=input_series.sort_values(),\n    y=input_series.sort_values().index.values,\n    marker=dict(color=\"crimson\", size=12),\n    mode=\"markers\",\n    name=\"Count\",\n    ))\n    fig.update_layout(title=f'{title}{subtitle}',\n                  xaxis_title=x_label,\n                  yaxis_title=y_label)\n    fig.show()\n\n# Function for printing out key passage of abstract based on key terms\ndef print_key_phrases(df, key_terms, n=5, chars=300):\n    for ind, item in enumerate(df[:n].itertuples()):\n        print(f'{ind+1} of {len(df)}')\n        print(item.title)\n        print('[ ' + item.doi + ' ]')\n        try:\n            i = len(item.abstract)\n            for kt in key_terms:\n                kt = kt.replace(r'\\b', '')\n                term_loc = item.abstract.lower().find(kt)\n                if term_loc != -1:\n                    i = min(i, term_loc)\n            if i < len(item.abstract):\n                print('    \"' + item.abstract[i-30:i+chars-30] + '\"')\n            else:\n                print('    \"' + item.abstract[:chars] + '\"')\n        except:\n            print('NO ABSTRACT')\n        print('---')\n        \ndef add_tag_covid19(df):\n    # Customised approach to include more complicated logic\n    #df, covid19_counts = count_and_tag(df, COVID19_SYNONYMS, 'disease_covid19')\n    df = count_and_tag(df, COVID19_SYNONYMS, 'disease_covid19')\n    novel_corona_filter = (abstract_title_filter(df, 'novel corona') &\n                           df.publish_time.str.startswith('2020', na=False))\n    df.loc[novel_corona_filter, 'tag_disease_covid19'] = True\n    #covid19_counts = covid19_counts.append(pd.Series(index=['novel corona'],\n    #                                                 data=[novel_corona_filter.sum()]))\n    #return df, covid19_counts\n    return df\n\ndef load_full_text(df, data_folder):\n    json_list = []\n    # Adding code to handle PDF vs. XML parse\n    for row in df[df.has_pdf_parse].itertuples():\n        filename = f'{row.sha}.json'\n        sources = ['biorxiv_medrxiv', 'comm_use_subset',\n                   'custom_license', 'noncomm_use_subset']\n        for source in sources:\n            if filename in os.listdir(os.path.join(data_folder, source, source, 'pdf_json')):\n                with open(os.path.join(data_folder, source, source, 'pdf_json', filename), 'rb') as f:\n                    json_list.append(json.load(f))\n    for row in df[df.has_pmc_xml_parse].itertuples():\n        filename = f'{row.sha}.json'\n        sources = ['comm_use_subset',\n                   'custom_license', 'noncomm_use_subset']\n        for source in sources:\n            if filename in os.listdir(os.path.join(data_folder, source, source, 'pmc_json')):\n                with open(os.path.join(data_folder, source, source, 'pmc_json', filename), 'rb') as f:\n                    json_list.append(json.load(f))\n    print(f'Found {len(json_list)} full texts for {len(df)} records')\n    return json_list\n\n\ndef display_dataframe(df, title):\n    text = f'<h2>{title}<\/h2><table><tr>'\n    text += ''.join([f'<td><b>{col}<\/b><\/td>' for col in df.columns.values]) + '<\/tr>'\n    for row in df.itertuples():\n        text +=  '<tr>' + ''.join([f'<td valign=\"top\">{v}<\/td>' for v in row[1:]]) + '<\/tr>'\n    text += '<\/table>'\n    display(HTML(text))\n    \n\n# CONSTANTS\n\nCOVID19_SYNONYMS = [\n                    'covid',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b'\n]\nSARS_SYNONYMS = [r'\\bsars\\b',\n                 'severe acute respiratory syndrome']\n\nMERS_SYNONYMS = [r'\\bmers\\b',\n                 'middle east respiratory syndrome']\nARDS_SYNONYMS = ['acute respiratory distress syndrome',\n                 r'\\bards\\b']\nRISKFAC_SYNONYMS = [\n    'risk factor analysis',\n    'cross sectional case control',\n    'prospective case control',\n    'matched case control',\n    'medical records review',\n    'seroprevalence survey',\n    'syndromic surveillance'\n]\nRISK_FACTOR_SYNONYMS = ['risk factor',\n                        'risk model',\n                        'risk by',\n                        'comorbidity',\n                        'comorbidities',\n                        'coexisting condition',\n                        'co existing condition',\n                        'clinical characteristics',\n                        'clinical features',\n                        'demographic characteristics',\n                        'demographic features',\n                        'behavioural characteristics',\n                        'behavioural features',\n                        'behavioral characteristics',\n                        'behavioral features',\n                        'predictive model',\n                        'prediction model',\n                        'univariate', # implies analysis of risk factors\n                        'multivariate', # implies analysis of risk factors\n                        'multivariable',\n                        'univariable',\n                        'odds ratio', # typically mentioned in model report\n                        'confidence interval', # typically mentioned in model report\n                        'logistic regression',\n                        'regression model',\n                        'factors predict',\n                        'factors which predict',\n                        'factors that predict',\n                        'factors associated with',\n                        'underlying disease',\n                        'underlying condition']\nAGE_SYNONYMS = ['median age',\n                'mean age',\n                'average age',\n                'elderly',\n                r'\\baged\\b',\n                r'\\bold',\n                'young',\n                'teenager',\n                'adult',\n                'child'\n               ]\n\nSEX_SYNONYMS = ['sex',\n                'gender',\n                r'\\bmale\\b',\n                r'\\bfemale\\b',\n                r'\\bmales\\b',\n                r'\\bfemales\\b',\n                r'\\bmen\\b',\n                r'\\bwomen\\b'\n               ]\n\nBODYWEIGHT_SYNONYMS = [\n    'overweight',\n    'over weight',\n    'obese',\n    'obesity',\n    'bodyweight',\n    'body weight',\n    r'\\bbmi\\b',\n    'body mass',\n    'body fat',\n    'bodyfat',\n    'kilograms',\n    r'\\bkg\\b', # e.g. 70 kg\n    r'\\dkg\\b'  # e.g. 70kg\n]\n\nSMOKING_SYNONYMS = ['smoking',\n                    'smoke',\n                    'cigar', # this picks up cigar, cigarette, e-cigarette, etc.\n                    'nicotine',\n                    'cannabis',\n                    'marijuana']\n\nDIABETES_SYNONYMS = [\n    'diabet', # picks up diabetes, diabetic, etc.\n    'insulin', # any paper mentioning insulin likely to be relevant\n    'blood sugar',\n    'blood glucose',\n    'ketoacidosis',\n    'hyperglycemi', # picks up hyperglycemia and hyperglycemic\n]\n\nHYPERTENSION_SYNONYMS = [\n    'hypertension',\n    'blood pressure',\n    r'\\bhbp\\b', # HBP = high blood pressure\n    r'\\bhtn\\b' # HTN = hypertension\n]\n\nIMMUNODEFICIENCY_SYNONYMS = [\n    'immune deficiency',\n    'immunodeficiency',\n    r'\\bhiv\\b',\n    r'\\baids\\b'\n    'granulocyte deficiency',\n    'hypogammaglobulinemia',\n    'asplenia',\n    'dysfunction of the spleen',\n    'spleen dysfunction',\n    'complement deficiency',\n    'neutropenia',\n    'neutropaenia', # alternate spelling\n    'cell deficiency' # e.g. T cell deficiency, B cell deficiency\n]\n\nCANCER_SYNONYMS = [\n    'cancer',\n    'malignant tumour',\n    'malignant tumor',\n    'melanoma',\n    'leukemia',\n    'leukaemia',\n    'chemotherapy',\n    'radiotherapy',\n    'radiation therapy',\n    'lymphoma',\n    'sarcoma',\n    'carcinoma',\n    'blastoma',\n    'oncolog'\n]\n\nCHRONICRESP_SYNONYMS = [\n    'chronic respiratory disease',\n    'asthma',\n    'chronic obstructive pulmonary disease',\n    r'\\bcopd',\n    'chronic bronchitis',\n    'emphysema'\n]\nASTHMA_SYNONYMS = [\n    'asthma'\n]\n\n\n\nIMMUNITY_SYNONYMS = [\n    'immunity',\n    r'\\bvaccin',\n    'innoculat'\n]\n\nCLIMATE_SYNONYMS = [\n    'climate',\n    'weather',\n    'humid',\n    'sunlight',\n    'air temperature',\n    'meteorolog', # picks up meteorology, meteorological, meteorologist\n    'climatolog', # as above\n    'dry environment',\n    'damp environment',\n    'moist environment',\n    'wet environment',\n    'hot environment',\n    'cold environment',\n    'cool environment'\n]\n\nTRANSMISSION_SYNONYMS = [\n    'transmiss', # Picks up 'transmission' and 'transmissibility'\n    'transmitted',\n    'incubation',\n    'environmental stability',\n    'airborne',\n    'via contact',\n    'human to human',\n    'through droplets',\n    'through secretions',\n    r'\\broute',\n    'exportation'\n]\n\nREPR_SYNONYMS = [\n    r'reproduction \\(r\\)',\n    'reproduction rate',\n    'reproductive rate',\n    '{r}_0',\n    r'\\br0\\b',\n    r'\\br_0',\n    '{r_0}',\n    r'\\b{r}',\n    r'\\br naught',\n    r'\\br zero'\n]\n\nINCUBATION_SYNONYMS = [\n    'incubation period',\n    'period of incubation',\n    'latent period',\n    'latency period',\n    'period of latency',\n    'window period'\n]\n\nPERSISTENCE_SYNONYMS = ['persistence',\n                        # r'(?<!viral )surface[s]?\\b', # THIS DOESN'T WORK\n                        'survival surface',\n                        'persistence surface',\n                        'survival on a surface',\n                        'persistence on a surface',\n                        'carrier test',\n                        'suspension test',\n                        'fomite',\n                        # 'survival time',\n                        'environmental surface',\n                        'environmental stability',\n                        'environmental reservoirs',\n                        'environmental survival',\n                        'pathogens in the environment',\n                        'environmental pathogen',\n                        'contaminated',\n                        'contamination',\n                        'surface stability',\n                        'surface swab',\n                        'inanimate surface',\n                        'surface disinfection'\n                       ]","873934cf":"# Load data\nmetadata_file = '..\/input\/CORD-19-research-challenge\/metadata.csv'\ndf = pd.read_csv(metadata_file,\n                 dtype={'Microsoft Academic Paper ID': str,\n                        'pubmed_id': str})\ndf.doi = df.doi.fillna('').apply(doi_url)\n\nprint(f'loaded DataFrame with {len(df)} records')","6dae919c":"df = count_and_tag(df, COVID19_SYNONYMS, 'disease_covid19')\nnovel_corona_filter = (abstract_title_filter(df,'novel corona') &\n                       df.publish_time.str.startswith('2020', na=False))\ndf.loc[novel_corona_filter, 'tag_disease_covid19'] = True\n\n# Fix the earlier papers that are about something else\ndf.loc[df.tag_disease_covid19 & ~df.publish_time.str.startswith('2020', na=True),\n       'tag_disease_covid19'] = False","9f846078":"df = count_and_tag(df, SARS_SYNONYMS, 'disease_sars')","04348f0c":"df = count_and_tag(df, MERS_SYNONYMS, 'disease_mers')","4f0af65b":"df = count_and_tag(df, ARDS_SYNONYMS, 'disease_ards')","223391f3":"df = count_and_tag(df, RISKFAC_SYNONYMS, 'design_riskfac')\n","3476cabe":"df = count_and_tag(df, RISK_FACTOR_SYNONYMS, 'risk_generic')\n","35d9a33f":"df = count_and_tag(df, AGE_SYNONYMS, 'risk_age')\n","29f90686":"df = count_and_tag(df, SEX_SYNONYMS, 'risk_sex')\n","2df35836":"df = count_and_tag(df, BODYWEIGHT_SYNONYMS, 'risk_bodyweight')\n","edb8618a":"df = count_and_tag(df, SMOKING_SYNONYMS, 'risk_smoking')\n","cbb738ea":"df = count_and_tag(df, DIABETES_SYNONYMS, 'risk_diabetes')\n","3daf7ab9":"df = count_and_tag(df, HYPERTENSION_SYNONYMS, 'risk_hypertension')\n","cddd75bf":"df = count_and_tag(df, IMMUNODEFICIENCY_SYNONYMS, 'risk_immunodeficiency')\n","e1567fcd":"df = count_and_tag(df, CANCER_SYNONYMS, 'risk_cancer')\n","f7f35b85":"df = count_and_tag(df, CHRONICRESP_SYNONYMS, 'risk_chronicresp')\n","ace6aef9":"df = count_and_tag(df, ['asthma'], 'risk_asthma')","10d4db5e":"df = count_and_tag(df, IMMUNITY_SYNONYMS, 'immunity_generic')\n","c4aeac42":"df = count_and_tag(df, CLIMATE_SYNONYMS, 'climate_generic')\n","80f20a84":"df = count_and_tag(df, TRANSMISSION_SYNONYMS, 'transmission_generic')\n","62b72f99":"df = count_and_tag(df, REPR_SYNONYMS, 'transmission_repr')\n\n","4c51ea1c":"df = count_and_tag(df, INCUBATION_SYNONYMS, 'incubation_per')\ndf = count_and_tag(df, PERSISTENCE_SYNONYMS, 'persistence')\n\n","590211f5":"#import sys\n#!{sys.executable} -m pip install langdetect \nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install langdetect","85b4f98b":"from langdetect import detect\nfrom langdetect import DetectorFactory\n\n# set seed\nDetectorFactory.seed = 0\n\ndef detectLanguage(x):\n    lan = 'en'\n    try:\n        if x[\"abstract\"]:\n            lan = detect(x['abstract'][0:70])\n        else:\n            lan = detect(x['title'])\n    except:\n        #problem with the text structure\n        lan = 'not_defined'\n    return lan","d63cb6bd":"from tqdm import tqdm \ntqdm.pandas()\nlanguages = list(df.progress_apply(lambda x: detectLanguage(x),axis = 1))\n","db21a138":"import matplotlib.pyplot as plt\nplt.style.use('seaborn-pastel')\n\nlanguage_dict = {}\nfor language in languages:\n    if language not in language_dict:\n        language_dict[language] = 1\n    else:\n        language_dict[language] += 1\nlanguage_dict.pop(\"not_defined\")\nplt.bar(range(0,len(language_dict)),list(language_dict.values()), align='center')\nplt.xticks(range(0,len(language_dict)), list(language_dict.keys()))\nplt.title(\"Language Distribution\")\naxes = plt.gca()\naxes.set_ylim([0,10000])\nplt.show()","aac14606":"df[\"language\"] = languages\ndf = df[df[\"language\"] == \"en\"]\ndf.reset_index(inplace=True)","56d15680":"df=df[df[\"tag_disease_covid19\"]]\ndf.reset_index(inplace=True)","778e69e0":"#Download the spacy bio parser\n#internet settings should be on\n#import sys\n#!{sys.executable} -m pip install scispacy\n#!{sys.executable} -m pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install scispacy\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","0a478a82":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport scispacy\nimport spacy\nimport en_core_sci_lg\n\nfrom scipy.spatial.distance import jensenshannon\n\nimport joblib\n\nfrom IPython.display import HTML, display\n\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nimport ipywidgets as widgets\nfrom IPython.display import clear_output\n","6812130b":"parser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 6000000\n\n\n# New stop words list \ncustomize_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-'\n]\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    parser.vocab[w].is_stop = True","c0b7d690":"def spacy_tokenizer(sentence):\n    tokens = parser(sentence)\n    #tokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens ]\n    tokens = [ word.lemma_.lower().strip() for word in tokens if not(word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1) ]\n    tokens = \" \".join([i for i in tokens])\n    return tokens","d03d6691":"tqdm.pandas()\ndf[\"processed_abstract\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)","d9a573f8":"cvec = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nvectorized_data = cvec.fit_transform(tqdm(df['processed_abstract']))","2b51823b":"vectorized_data.shape","281c4c6b":"# most frequent words\nword_count = pd.DataFrame({'word': cvec.get_feature_names(), 'count': np.asarray(vectorized_data.sum(axis=0))[0]})\n\nword_count.sort_values('count', ascending=False).set_index('word')[:20].sort_values('count', ascending=True).plot(kind='barh')","39e594f9":"#joblib.dump(vectorized_data, 'vectorized_data.csv')","a1bd8df0":"lda = LatentDirichletAllocation(n_components=20, random_state=0)\nlda.fit(vectorized_data)\n#joblib.dump(lda, 'lda.csv')","e40bb00b":"def print_top_words(model, vectorizer, n_top_words):\n    feature_names = vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        message = \"\\nTopic #%d: \" % topic_idx\n        message += \" \".join([feature_names[i]\n                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n        print(message)\n    print()","5835601b":"print_top_words(lda, cvec, n_top_words=10)","14af0855":"doc_topic_dist = pd.DataFrame(lda.transform(vectorized_data))\ndoc_topic_dist.columns = [\"topic_\"+str(column) for column in doc_topic_dist.columns.values]\ndf = df.join(doc_topic_dist)\n","b4d279bb":"df.head(5).apply(lambda row: sum([row[column] for column in df.columns.values if 'topic' in column]), axis=1)","8e3a00ac":"df.to_csv('papers.csv', index=False)","f3bb8337":"df['authors_list'] = df.authors.str.split(\";\")\nauthors = pd.Series(df.explode('authors_list')[\"authors_list\"].str.replace(\" \", \"\").unique()).to_frame()\nauthors.columns = ['author_name']\nauthors[\"author_id\"] = authors.index + 1\n","4fe39f8b":"author_papers = df.explode('authors_list')[['authors_list','sha']]\nauthor_papers.reset_index(inplace=True)\nauthor_papers.drop(['index'],inplace=True,axis=1)\nauthor_papers.columns = ['author_name','sha']\nauthor_papers['author_name'] = author_papers['author_name'].str.replace(\" \", \"\")\nauthor_papers = pd.merge(author_papers,authors,on=['author_name'],how='left')\nauthor_papers.drop('author_name',inplace=True,axis=1)","9ddd6f14":"author_papers.to_csv('author_papers.csv', index=False)\nauthors.to_csv('authors.csv', index=False)","017197d3":"\"\"\"import pandas as pd\nauthor_papers = pd.read_csv(\"author_papers.csv\")\nauthors = pd.read_csv(\"authors.csv\")\ndf = pd.read_csv(\"papers.csv\")\"\"\"","262f35e1":"authorsDroppedNan = authors.dropna()\n[idx for idx,value in enumerate(list((authorsDroppedNan['author_name'].str.strip().duplicated() == True).values)) if value == True]","0d27170e":"\"\"\"from tqdm import tqdm\nimport sys\ndroppedNanAuthors = authors[['author_name']].dropna().reset_index()\nauthor_list = droppedNanAuthors['author_name'].str.split(\",\")\nfor i in tqdm(range(len(author_list))):\n    for j in range(i,len(author_list)):\n        if author_list[i] != author_list[j]:\n            notFound = False\n            for author in author_list[i]:\n                if author not in author_list[j]:\n                    notFound = True\n            if not notFound:\n                print(author_list[i],\"i author list\",author_list[j],\"j author list\")\n                sys.exit()\n\"\"\"","b182a015":"df = df[~pd.isna(df['sha'])]\n#df.reset_index(inplace=True)","88649eac":"author_papers  = author_papers[author_papers['sha'].isin(df['sha'])]\nauthor_papers.reset_index(inplace=True)","cf201806":"authors  = authors[authors['author_id'].isin(author_papers['author_id'])]\nauthors.reset_index(inplace=True)","1c80fba4":"authors_df = pd.merge(author_papers,df[[column for column in df.columns.values if 'topic' in column or 'tag' in column or column == 'sha']],on=['sha'],\n        how='inner')\nauthors_df = authors_df[[column for column in authors_df if 'tag' in column or 'topic' in column or column == 'author_id']]\nfor column in authors_df.columns.values:\n    if 'tag' in column:\n        authors_df[column] = authors_df[column].astype(int)\n","7ee44c7d":"authors_df = authors_df.groupby('author_id').sum().reset_index()\ntotal_count = author_papers.groupby('author_id').agg({'sha':['count']})\ntotal_count.columns = ['total_count']\ntotal_count.reset_index(inplace=True)\nauthors_df = pd.merge(authors_df,total_count,on=['author_id'],how='left')\nfor column in authors_df.columns.values:\n    if column != 'author_id' and column != 'total_count':\n        authors_df[column] = authors_df[column] \/ authors_df['total_count']","e14d17e3":"import matplotlib.pyplot as plt\nplt.style.use('seaborn-pastel')\nauthor_paper_counts = dict(zip(authors_df['author_id'], authors_df['total_count']))\nxmin = min(author_paper_counts.values())\nxmax = max(author_paper_counts.values())\nbins = (xmax-xmin)\n#plt.xlim(xmin-1, xmax+1)\n\nax = plt.hist(list(author_paper_counts.values()), bins=bins, log=2, width=0.95, alpha=0.8 );\n\naxis_font = { 'size':'14'}\nplt.xlabel('Papers per author', **axis_font)\nplt.ylabel('Authors ', **axis_font)\n\nplt.grid(0)\nplt.show()","a0d15871":"from sklearn.decomposition import PCA\nplt.style.use('ggplot')\n\npca = PCA().fit(authors_df[[column for column in authors_df.columns.values if column not in ['author_id','total_count']]].values)\n\nax = plt.plot(pca.explained_variance_ratio_.cumsum(), '--o')\naxis_font = { 'size':'14'}\n\nplt.xlabel('Component', **axis_font)\nplt.ylabel('Variance', **axis_font)\nplt.show()","b635e17d":"authors_df.shape","08d66e7d":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=30, random_state=42)\nauthors_df_reduced= pca.fit_transform(authors_df[[column for column in authors_df.columns.values if column not in ['author_id','total_count']]].values)\nauthors_df_reduced.shape","36ae38bd":"#find optimal number of clusters to be used in clustering algorithm and apply k-means clustering\nfrom sklearn.cluster import KMeans\nplt.style.use('ggplot')\n\nSum_of_squared_distances = []\nK = range(1,20)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(authors_df_reduced)\n    Sum_of_squared_distances.append(km.inertia_)\n    \nplt.plot(K, Sum_of_squared_distances, marker='o', markersize=10)\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","1bec5b6c":"n_cluster = 10\nkmeans = KMeans(n_clusters=n_cluster, random_state=42)\ny_pred = kmeans.fit_predict(authors_df_reduced)\nauthors_df['label'] = y_pred","06e0ddfc":"cls_means = authors_df[[column for column in authors_df.columns.values if 'tag' in column or column == 'label']].groupby('label').mean()\ntransposedMeans = cls_means.T\ntransposedMeans.columns = [str(int(column)+1) for column in transposedMeans.columns.values]\nax = transposedMeans.plot(kind='bar', label='index', colormap='Paired',figsize=(15,10))\n#cls_means.plot.bar(rot=0,figsize=(15,10))\nplt.show()","b62ddf8c":"cls_means = authors_df[[column for column in authors_df.columns.values if 'topic' in column or column == 'label']].groupby('label').mean()\ncls_means","6708d21e":"int_p_id = dict(enumerate(list(author_papers['sha'].unique())))\nint_a_id = dict(enumerate(list(author_papers['author_id'].unique())))\n\na_int_id = {authorId:intVal for intVal,authorId in int_a_id.items()}\np_int_id = {paperId:intVal for intVal,paperId in int_p_id.items()}","4f637101":"from scipy import sparse as sp\nimport numpy as np\n\nauthor_paper_tuples = list(zip(author_papers['author_id'],author_papers['sha']))\nauthor_paper_tuples = [(a_int_id[t[0]],p_int_id[t[1]]) for t in author_paper_tuples]\n\n#AP[i,j] = 1 indicates that author i published paper j\nAP = sp.csc_matrix((np.ones(len(author_paper_tuples)), zip(*author_paper_tuples)))\n\n##AA[i,j] = 1 indicates that author i published a paper with author j\n\nAA = AP.dot(AP.T)","fe193e58":"import networkx as nx\n\n#remove diagonal\nAA = np.array(AA - np.diag(AA.diagonal()))\n\n#construct graph\nG = nx.from_numpy_matrix(AA, parallel_edges=True)","e6d7ce22":"deg_measure = nx.degree(G) \ncent_measure = nx.degree_centrality(G) \nbet_measure = nx.betweenness_centrality(G)","6bffb894":"AP.shape","e4111121":"#from IPython.display import Image\n#Image(filename='..\/pics\/bobNetwork.png', width=800, height=800)","9a122af3":"#from IPython.display import Image\n#Image(filename='..\/pics\/betwennessBob.png', width=800, height=800)","6dcd1b29":"authors['degree'] = authors['author_id'].apply(lambda l: deg_measure[a_int_id.get(l)])\nauthors['degree_cent'] = authors['author_id'].apply(lambda l: cent_measure[a_int_id.get(l)])\nauthors['degree_bet'] = authors['author_id'].apply(lambda l: bet_measure.get(a_int_id.get(l)))\nauthors_df = pd.merge(authors_df,authors[['author_id','degree','degree_cent','degree_bet']],on=['author_id'],how='left')","107bc9e6":"plt.style.use('seaborn-pastel')\ndegree_sort = authors.sort_values(by=['degree'],ascending=False)[0:10][['author_name','degree']]\ndegree_sort = degree_sort.set_index('author_name')\nax = degree_sort.plot.barh(rot=0,figsize=(18,5))\nax.invert_yaxis()  # labels read top-to-bottom\nplt.ylabel(\"degree\")\n#plt.style.use('fivethirtyeight')\n\n","51cb97d3":"degree_sort = authors.sort_values(by=['degree_cent'],ascending=False)[0:10][['author_name','degree_cent']]\ndegree_sort = degree_sort.set_index('author_name')\nax = degree_sort.plot.barh(rot=0,figsize=(18,5))\nax.invert_yaxis()  # labels read top-to-bottom\nplt.ylabel(\"degree_centrality\")\n#plt.style.use('bmh')\n","5224f2f4":"degree_sort = authors.sort_values(by=['degree_bet'],ascending=False)[0:10][['author_name','degree_bet']]\ndegree_sort = degree_sort.set_index('author_name')\nax = degree_sort.plot.barh(rot=0,figsize=(18,5))\nax.invert_yaxis()  # labels read top-to-bottom\nplt.ylabel(\"degree_betwenness\")","de7a59ff":"#Image(filename='..\/pics\/networkCentralization.png', width=400, height=400)","32acb693":"sum(max(authors['degree'])-authors['degree'])\/((len(authors)-1)*(len(authors)-2))","43420f46":"print(G.number_of_edges(), G.number_of_nodes(), nx.density(G)) \n","c5a17407":"author_classes = dict(zip(authors_df.author_id, authors_df.label))","19286e1a":"cls_names ={0: 'label1', 1: 'label2', 2: 'label3', 3: 'label4', 4: 'label5', 5:'label6',6:'label7',7:'label8',8:'label9',9:'label10'}\n\nfilterAuthors = dict(zip(author_papers['author_id'].value_counts().keys(),author_papers['author_id'].value_counts().values))\nfilterAuthors = [authorId for authorId,paperCount in filterAuthors.items() if paperCount>2]\n\n#subnetwork\nG1 = G.subgraph([a_int_id[i] for i in filterAuthors])\nprint(G1.number_of_edges(), G1.number_of_nodes())","646c4cce":"#import sys\n#!{sys.executable} -m pip install pygraphviz \n\"\"\"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install pygraphviz\n\nfrom networkx.drawing.nx_agraph import graphviz_layout\npos_nodes = graphviz_layout(G)\n\n\"\"\"\nnode_pos= nx.spring_layout(G1, iterations=50)\n","8884b852":"from tqdm import tqdm \n\ngroups=[]\nfor node in tqdm(G1.nodes()):\n    groups.append(author_classes[int_a_id[node]])\n    \ngroup_colors = {0:'grey', 1:'green', 2:'red', 3:'blue',4:'mediumpurple',5:'lightsalmon',6:'orange',7:'darkmagenta',8:'maroon',9:'aqua'}\n##match authornames with authorids\na_id_name = dict(zip(authors.author_id, authors.author_name))","d406e9cf":"#import sys\n#!{sys.executable} -m pip install plotly \nfrom IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install plotly","6d7b06f8":"import plotly.offline as plotly # Interactive plotting library\nfrom plotly.graph_objs import *\nfrom plotly import tools # Plotly's tools, such as for making figures\/subplots\n\n# Initialize plotly in offline mode\nplotly.init_notebook_mode(connected=True)\n\nedge_trace = Scatter(\n    x=[],\n    y=[],\n    line=Line(width=1.5,color='#888'),\n    hoverinfo='none',\n    mode='lines')\nx_list = []\ny_list = []\nfor edge in tqdm(G1.edges(data=True)):\n    \n    x0, y0 = node_pos[edge[0]]\n    x1, y1 = node_pos[edge[1]]\n    x_list = list(edge_trace['x'])\n    x_list += [x0, x1]\n    edge_trace['x'] = tuple(x_list)\n    y_list = list(edge_trace['y'])\n    y_list += [y0, y1]\n    edge_trace['y'] = tuple(y_list)\n\nprint(\"done with edges\")\n","9702fd59":"node_trace = Scatter(\n    x=[],\n    y=[],\n    text=[],\n    mode='markers',\n    hoverinfo='text',\n    marker=Marker(\n        showscale=True,\n        colorscale='Jet',\n        color=groups,\n        size=[],\n        colorbar=dict(\n            title='Class',\n            tickvals = [i for i in range(10)],\n            ticktext = [cls_names[i] for i in range(10)],\n            ticks = 'outside'\n        ),\n        line=dict(width=1)))\nx_list = []\ny_list = []\nfor node in tqdm(G1.nodes()):\n    x, y = node_pos[node]#G.node[node]['pos']\n    x_list = list(node_trace['x'])\n    x_list += [x]\n    y_list = list(node_trace['y'])\n    y_list += [y]\n    node_trace['x'] = tuple(x_list)\n    node_trace['y'] = tuple(y_list)","7871e088":"node_deg = G1.degree()\nnode_trace['marker']['size'] =[]\nnode_trace['marker']['color'] =[]\nnode_trace['text'] =[]\nmax_degree = max([node[1] for node in node_deg])\nmarker_size_list = []\nmarker_text_list = []\nmarker_color_list = []\nfor node in tqdm(G1.nodes()):\n    marker_size_list = list(node_trace['marker']['size'])\n    marker_size_list += [50*(node_deg[node])\/max_degree]\n    node_trace['marker']['size'] = tuple(marker_size_list)\n    name = a_id_name.get(int_a_id[node])\n    node_info = '# of connections: %s <br \/>AuthorName: %s <br \/>AuthorID: %s <br \/>Label: %s'%(str(node_deg[node]), str(name),str(int_a_id[node]),\n                                                                                               str(author_classes[int_a_id[node]]))\n    marker_text_list = list(node_trace['text'])\n    marker_text_list += [node_info]\n    node_trace['text'] = tuple(marker_text_list)\n    marker_color_list = list(node_trace['marker']['color'])\n    marker_color_list += [author_classes[int_a_id[node]]]\n    node_trace['marker']['color'] = tuple(marker_color_list)","3d00fd67":"fig = Figure(data=Data([edge_trace, node_trace]),\n             layout=Layout(\n                title='<br>Collaboration Network',\n                titlefont=dict(size=16),\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=10,l=15,r=5,t=40),\n                xaxis=XAxis(autorange=True, showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=YAxis(autorange=True, showgrid=False, zeroline=False, showticklabels=False)))\n\nplotly.iplot(fig, filename='collaboration_network_vis')","05e9140d":"df[df['sha'] == \"e0f76974abe7c6d61b7ddea1c46fff36831d34a4\"][[column for column in df.columns.values if 'topic' in column]]","dbd476fc":"df[(df['sha'] == \"e0f76974abe7c6d61b7ddea1c46fff36831d34a4\") | (df['sha'] == \"d548934676e1a7e52a001348244b52760e5a2ad4\") |\n  (df['sha'] == \"d4514afa97f2fcbcecac303c4975854d92dda228\")][[column for column in df.columns.values if 'topic' in column]].describe()","db71b590":"import matplotlib.pyplot as plt\nplt.figure(3,figsize=(12,12)) \nnx.draw(G1, with_labels=False, node_size = 2, node_color = 'lightblue') ","96748abb":"### Immunodeficiency\n\nImmunodeficiency (e.g. HIV \/ AIDS, side effect of chemotherapy, etc.) may be important.","1bacbf70":"### Cancer","f6de4fe2":"Example shows one of the papers research group colored dark blue with label 1 has written. The papers of this particular research group have high values in columns topic 13 topic 14. These topics are about antivirus and vaccines. This shows that our topic modelling is able to summarize the interests of the research groups and authors in particular. But this analysis needs more evaluation. The point was that reserch groups are indeed visible in the network with colors indicating their research areas and interests.","cccf95da":"### Language Distribution","cacafa8f":"# Load the Data\n<a id='load-data'><\/a>\n\nLoad data and tag thematic expressions following the notebook by Ivan Ega Pratama, from Kaggle.\n#### Cite: Covid-19-thematic-tagging-with-regular-expressions\/notebook]( https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions\/notebook)","67561ff5":"#### Degree centrality\n- Degree centrality is the simplest centrality measure to compute. A node's degree is simply the number of connections the node has. A node with 5 connections would have a degree centrality of 1 whereas a node with 1 connection would have a degree centrality of 1. Degree centrality can be normalized in some applications to find relative degree centrality of nodes.\n\n- For degree centrality, **higher values** mean that the node is more central. There are however exceptions to this rule of thumb. Although a node may be connected to many other nodes, it may also be far off on the edge of the network.","5cf73db6":"#### Sex\n\ne.g. _Sex difference and smoking predisposition in patients with COVID-19_, https:\/\/doi.org\/10.1016\/S2213-2600(20)30117-X","87a5c053":"### Chronic respiratory disease","5dc65249":"### Centrality Metrics\n[source](https:\/\/cs.brynmawr.edu\/Courses\/cs380\/spring2013\/section02\/slides\/05_Centrality.pdf)\n","983a430f":"> # Co-authorship Network Analysis","8dbd8e47":"- Above graph lists the highest 10 rankings of scholars in terms of centrality measures in overall network. The reason of different rankings in betweeness degree is possibly an explanation to existence of different research groups, but a scholar hierarchy between authors.\n- Another insight is that authors with both high betweennness and degree centrality should be at the very heart of the fight against covid19. Not only these authors contributed to their research areas but also they connect dispatched groups. ","7495dc4c":"### Discovered Topics","62e3dd78":"![bobNetwork.png](attachment:bobNetwork.png)","d16fbe52":"###\u00a0Bodyweight\n\nObesity and related problems (e.g. diabetes, hypertension) have been widely speculated as risk factors, e.g. _The confluence of the COVID19 pandemic with the obesity epidemic_, https:\/\/doi.org\/10.1136\/bmj.m810","f56646e3":"# LatentDirichletAllocation\n<a id='lda'><\/a>\nReduce # of topics in the model as only abstract is being analyzed here, evaluate performance (#TO-DO)\n","1225b3d3":"It is important to normalize the number of times a particular topic or a tag appears in an author's work, as authors have different number of papers in the dataset\n","0765d302":"## Transmission\n\n### Transmission \/ incubation generic","bb034376":"### Reproduction rates ($R$ \/ $R_0$)\n\n- Basic reproduction rate ($R_0$)\n- Effective reproduction rate ($R$)","1afb9cd9":"Analysis on authors and papers","bacb6062":"## Remove stopwords, punctions, spaces, numbers\n","8a4380e8":"# Identify authors\n<a id='identify-scientists'><\/a>\n","52bb825d":"### Acute Respiratory Distress Syndrome (ARDS)\n\nARDS is a possible consequence of Covid-19 infection.\n\nSee: https:\/\/en.wikipedia.org\/wiki\/Acute_respiratory_distress_syndrome","68045000":"Network metrics to identify people at the heart of the studies on covid19","18416b07":"**Future work include**:\n<li>Filtering options should be added in order to filter out co-network and focus on particular groups<\/li>\n<li><a href=\"https:\/\/ieeexplore.ieee.org\/stamp\/stamp.jsp?arnumber=6297585\">Evaluate different layouts using different network visualization techniques<\/a><\/li>\n<li>Better evaluate topic modelling and tagging procedures<\/li>\n<li>Augment dataset with citation counts, etc<\/li>\n<li>Include more network metrics and investigate their relation with co-author network analysis in greater depth<\/li>\n<li>Answer task related questions considering co-authorship network and their research interests(labels from clustering) so that more refined answers could be found<\/li>\n<li>Make the analysis live so that it is easy to detect recent changes in the network<\/li>","a72a4a4a":"### Demographic risk factors\n\n#### Age","1dcd9e18":"Centralization is distributed across nodes, which is good ","81563d33":"\n\n### Goal\nGiven a large amount of literature and rapidly spreading COVID-19, it is difficult for a scientist to keep up with the research community promptly. It became equally hard for policy makers to follow the recent advances in fight against COVID-19. As days pass, multiple publications lead to new findings, but there seems to be a lost connection between policy makers and the research community.\n\nThe fight against COVID-19 is not one dimensional. Researchers have been working immensely in diverse set of fields to know more about potential risk factors, immunity and vaccinations, transmission rates, public policies... The goal of this research is to find out the intristic relation between diverse set of research groups. This will not only allow decision makers to consult to right people, but also will present insights into the research community about diverse set of research goals. This analysis helps to find answers to these questions: How many research groups there are? Do these research groups study the same set of questions? Can policy makers consult to right set of people? Who will be head of a research group studying a specific problem? The current analysis will allow identification of undiscovered connections within authorship networks that have not been properly addressed as a network analysis problem.\n\n**Approach**:\n<ol>\n    <li><a href=\"#load-data\">Load data<\/a><\/li>\n    <li><a href=\"#tag-papers\">Tag papers with themes (e.g. tag_disease_covid19 or tag_risk_smoking) using handcrafted rules based on synonyms and related term<\/a><\/li>\n    <li><a href=\"#preprocess\">Preprocess data (removing non-english studies, removing duplicates if any, removing stop words, vectorization)<\/a><\/li>\n    <li><a href=\"#lda\">Tag papers with topics using topic modelling technique LDA<\/a><\/li>\n    <li><a href=\"#identify-scientists\">Idetify scientists and identify relations between scientist<\/a><\/li>\n    <li><a href=\"#clustering\">PCA and Clustering<\/a><\/li>\n    <li><a href=\"#network-analysis\">Network Analysis<\/a><\/li>\n<\/ol>\n\n### Dataset Description\n\n>*In response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 29,000 scholarly articles, including over 13,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up.*\n#### Cite: [COVID-19 Open Research Dataset Challenge (CORD-19) | Kaggle](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) <br>\n\n**Clustering section of the project (cite):** *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*, 2nd Edition, by Aurelien Geron (O'Reilly). Copyright 2019 Kiwisoft S.A.S, 978-1-492-03264-9. Machine Learning Practice. Implimenting this section following the Chapter-9 project on O'REILLY's Hands-On Machine Learning. <br>","ec594b04":"Above operation is too costly, leaving like this for now.","9d77026e":"Since the network is very sparse, we filter authors contributing to a threshold number of papers in the network. This allows network analysis to be more informative. ","eaf40013":"As you can see, Bob although having less number of connections, Bob is connecting two dispatched group of nodes","d1485383":"## Vectorization","0b4f8bfe":"### Middle East Respiratory Syndrome (MERS)\n\nSee: https:\/\/en.wikipedia.org\/wiki\/Middle_East_respiratory_syndrome","ea42e458":"As you can see, Bob although having the same number of connections in both of the networks. Bob is located to more center of right network.","b7c301fc":"Analyze groups of scientists across tags and topics","8ae5ba14":"## Diseases\n\n- Covid-19\n- Severe Acute Respiratory Syndrome (SARS)\n- Middle East Respiratory Syndrome (MERS)\n- Coronaviruses\n- Acute Respiratory Distress Syndrome (ARDS)\n\n### Covid-19\n\nWe are looking for papers that specifically refer to the recent outbreak, known variously as Covid-19, SARS-CoV-2, 2019-nCoV, Wuhan Pneumonia, novel coronavirus.\n\nSee: https:\/\/en.wikipedia.org\/wiki\/Coronavirus_disease_2019","90aaf159":"### Diabetes\n\n- Type I Diabetes\n- Type II Diabetes","05795a79":"#### Freeman\u2019s general formula for centralization\n","4cae2868":"# PCA & Clustering\n<a id='clustering'><\/a>\n\nApply clustering to dimension reduced data. Classifying authors could be of great help later in network analysis","bf33106c":"![networkCentralization.PNG](attachment:networkCentralization.PNG)","b1cc096b":"## Immunity\n\nLooking for terms which indicate factors relating to vaccination and immunity.\n\n### Generic immunity \/ vaccination\n\nPapers which mention generic themes relating to immunity \/ vaccination. (As the research develops, we may extend this section to include specific lines of research relating to immunity \/ vaccination.","d76affee":"### Severe Acute Respiratory Syndrome (SARS)\n\nSARS typically means the related coronavirus that caused an outbreak in 2003, although Covid-19 is sometimes referred to with a SARS name.\n\nSee: https:\/\/en.wikipedia.org\/wiki\/Severe_acute_respiratory_syndrome_coronavirus","c7e8ad37":"Filter dataset such that it only consists of covid19 tagged papers. This will make sure that the analysis is more focused and the computation and memory resources are better used. However, you can remove this filter and analyze the results. Remember that any filtering options applied or removed affects the parameters used in LDA, adjust parameters accordingly","14a46a50":"## Incubation and Persistance","ce6d2844":"## Climate\n\nClimate has been hypothesised as a factor in the spread of Covid-19","257b07ee":"### Asthma","cb822c76":"### Hypertension","bd2a4882":"There exists diverse research groups of different size and interest. These research groups can be seen as clusters of same color in the co-network analysis. Authors having same color indicates that their research interests are similar to each other. Let's observe this on a particular example.","fdf6413c":"# Tag Papers\n<a id='tag-papers'><\/a>\n","0ee69d0c":"### Smoking\n\ne.g. _Sex difference and smoking predisposition in patients with COVID-19_,  https:\/\/doi.org\/10.1016\/S2213-2600(20)30117-X","419e8327":"## Risks\n\nPotential risk factors:\n\n- Generic risk factors\n- _Demographic_:\n    - Age\n    - Sex\n    - Bodyweight\n    - Blood type\n    - Ethnicity (TODO)\n- _Behavioural:\n    - Smoking\n    - Occupation (TODO)\n    - Animal contact (TODO)\n    - Social activity (TODO)\n- _Pre-existing conditions_:\n    - Diabetes\n    - Hypertension\n    - Immunodeficiency (general)\n    - Cancer (general)\n    - Chronic respiratory disease (general - inc. asthma, bronchitis)\n    - Asthma\n    - Cardiovascular disease (TODO)\n    - Chronic respiratory disease \/ bronchitis (TODO)\n    - Cerebral infarction (TODO)\n\nSee _Estimation of risk factors for COVID-19 mortality - preliminary results_, https:\/\/doi.org\/10.1101\/2020.02.24.20027268","ebc6cafe":"Check any inconsistency across author names ","55501c5d":"#### Network Visualization\nCo-author network, where nodes represent authors and edges represents if the connection between authors. \n- For now, the size of the node is proportional to the co-authorships of the author. \n- Colors represent the class identified with the author (the research interests, topics) in the network. \n- Hovering over the nodes displays information about the author. ","3efdf7f6":"# Network-Analysis\n<a id='network-analysis'><\/a>\n\nConstruct author-author relationship from bipartite graph of author-paper relationship\n","b391e766":"#### Betweennes centrality\n- Betweenness centrality is important in many cases in network analysis. As above exmaple tells, a higher degree centrality will not always yield to higher betweenness centrality. Betweenness centrality is an important metric, as it discovers nodes connecting many dispatched group of nodes to one another.\n\n- For betweenness centrality, **higher values** mean that the node is connecting higher number of nodes to one another. In co-authorship networks this could be the fact that authors having higher betweenness centrality have diverse set of interests and possibly interacts and experiments with diverse set of fields. These people are very important in fight against covid19.","342982a0":"## Detect language\nLimit the size of the string to be sent to api, since the length affects the time it takes to detect the language of the text","6caea156":"An interesting property of the co-authorship network is that the node's sizes are directly proportional to the respective research group size. This although makes research groups more visible in the network, metrics other than degree can be used to rescale nodes. Betweenness, closeness, custom degree metrics are some of the choices here.","fb659193":"# Preprocess data\n<a id='preprocess'><\/a>\n- Remove non-English languages, as topic modelling works better when the context is well defined and concise. Translation of words from other languages to English may result in loss of semantic meaning.\n- Remove stop words to generate a good subset of words(features) to be used in topic modelling. Stop words do not have any semantic meanning, remove punctions as well\n- Apply vectorization while removing most used frequent words from set of words(features) to be used in topic modelling ","16966319":"This network looks promosing. As it definitely identifies small groups of networks as well as the big actors. Authors that could be responding with right answers and should in my naive opinion be much more in front of the fight against disease include: \n<li><a href=\"http:\/\/med.sustc.edu.cn\/staff\/view\/id-73.html?locale=en_US\">Professor Zheng Zhang<\/a><\/li>\n<li><a href=\"http:\/\/med.sustech.edu.cn\/staff\/view\/id-72.html?locale=en_US\">Professor Liu Lei<\/a><\/li>\n<li><a href=\"http:\/\/www.csu.edu.cn\/organization\/academy\/yanjiusuo\/mirc\/name_ww.htm\">Professor Wei Wang<\/a><\/li>\n","b204bb17":"From the plot above, 30 and above seems enough to explain variation.","4a924959":"![betwennessBob.png](attachment:betwennessBob.png)","0ae12aa4":"## Design\n\nResearch design (thanks to Savanna Reid for input on these):\n\n- risk factor analysis\n    - retrospective cohort\n    - cross-sectional case-control\n    - prospective case-control\n    - matched case-control\n    - medical records review\n    - seroprevalence survey\n    - syndromic surveillance\n- time series analysis\n    - survival analysis","a2ec0a3f":"Get unique authors"}}