{"cell_type":{"dbc2ae10":"code","28c35978":"code","c72fe373":"code","e89b0ede":"code","2e28753b":"code","fa33d5ea":"code","a86fefeb":"code","e2041c63":"code","4948f6ed":"code","02f201b2":"code","9aa44d83":"code","47027c68":"code","45a2dc2d":"code","efbe38e6":"code","543377ab":"code","d2e4ea74":"code","e3597217":"code","edfba351":"code","6492f4f0":"code","db6736a5":"code","dc13e305":"code","2dcc23b0":"code","ac9f150a":"markdown","e0845e6b":"markdown","67ea1d54":"markdown","4ac7b0bf":"markdown","7b9fcc9f":"markdown","e386692b":"markdown","d5ca4901":"markdown","756d47fa":"markdown","da8a1515":"markdown","71c077d8":"markdown","b651b023":"markdown","55011f5c":"markdown","d63c8d2f":"markdown","f9d883ff":"markdown","4e8bd1c2":"markdown","54edcfe5":"markdown","fbd2ed9c":"markdown","57e11f38":"markdown"},"source":{"dbc2ae10":"# Import TensorFlow into collab\nimport tensorflow as tf\nprint(f\"Tensorflow version: {tf.__version__}\")","28c35978":"import tensorflow as tf\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n  raise SystemError('GPU device not found')\nprint('Found GPU at: {}'.format(device_name))","c72fe373":"# Import required packages\nimport os \nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n%matplotlib inline","e89b0ede":"# Get Path to Train and Valid folders\ntrain_path = '..\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)\/train'\nvalid_path = '..\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)\/valid'\n\n# Get list of all subfolders for each Subset\ntrain_dir = os.listdir(train_path)\nvalid_dir = os.listdir(valid_path)\n# Check length of subfolders\nlen(train_dir), len(valid_dir)","2e28753b":"# Create Dataframe\n\ndef create_info_df(path):\n    \"\"\"\n    input: `path` - folder path\n    From folder path, create a Dataframe with columns: \n    Plant | Category | Path | Plant___Category | Disease\n    return DataFrame\n    \"\"\"\n\n    list_plants = []\n    list_dir = os.listdir(path) # Get list direcotry\n    # Go through each folder to create url and get required information\n    for plant in list_dir:\n        url = path +'\/'+plant\n        for img in os.listdir(url):\n            list_plants.append([*plant.split('___'), url+'\/'+img, plant])\n\n    # Create DataFrame\n    df = pd.DataFrame(list_plants, columns=['Plant', 'Category', 'Path','Plant___Category'])\n    # Add `Disease` column - if folder name is not Healthy then plant is diseased\n    df['Disease'] = df.Category.apply(lambda x: 0 if x=='healthy' else 1)\n\n    return df\n\n# Get Validation and Training DF\ntrain_info = create_info_df(train_path)\nvalid_info = create_info_df(valid_path)\n\nprint(train_info.shape, valid_info.shape)\n\n#Unique label list:\nunique_plant_cat = np.unique(train_info['Plant___Category'].to_numpy())\nprint(\"Number of Categories to predict: \", len(unique_plant_cat))\n","fa33d5ea":"# Creation of constants\nIMG_SIZE = 64\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE)\nbatch_size = 32\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nOUTPUT_SHAPE = 38\nNUM_EPOCHS = 20","a86fefeb":"## FUNCTION UTILS - Prepare Data and Dataset ##\n\ndef create_img_df(df_info, frac=0.1, random_state=42):\n    return df_info.sample(frac=frac, random_state=random_state).reset_index()\n\ndef create_train_val_df(valid_info, train_info, frac=0.1, random_state=42):\n    \"\"\"\n    Create Train and validation dataframe\n    Return:\n      - train dataframe\n      - validation dataframe\n    \"\"\"\n    valid_df = create_img_df(valid_info, frac, random_state)\n    train_df = create_img_df(train_info, frac, random_state)\n    \n    # Get information shape\n    valid_img_cnt,train_img_count = valid_df.shape[0], train_df.shape[0]\n    total = valid_img_cnt + train_img_count\n    # Print information\n    print(f'Total images (frac={frac}): ', total)\n    print(f\"Training ({train_img_count}): {train_img_count\/total*100:.2f}% - Validation ({valid_img_cnt}): {valid_img_cnt\/total*100:.2f}%\")\n     \n    return train_df, valid_df\n\n\ndef get_bool_label(labels):\n    # Create a variable of all Labels\n    plant_cat_labels = labels.to_numpy()\n    # Create Boolean label list\n    bool_plant_cat = [unique_plant_cat == plant_cat for plant_cat in plant_cat_labels]\n    # return array\n    return bool_plant_cat\n    \n# Prepare Data\ndef prepare_data(train_df, valid_df):\n    \"\"\"\n    Get Train and Validation Data Frame and return X_train, X_val, y_train, y_val\n    \"\"\"\n    # create images (X) arrays\n    X_train = train_df['Path']\n    X_val = valid_df['Path']\n    \n    # create labels (y) arrays\n    y_train = get_bool_label(train_df['Plant___Category'])\n    y_val = get_bool_label(valid_df['Plant___Category'])\n    \n    print('Shape: ',X_train.shape, X_val.shape, len(y_train), len(y_val))\n    \n    return X_train, X_val, y_train, y_val\n    \n\n\n# Dataset function utils\n\n# Decode and load image\ndef decode_img(path, img_shape=IMG_SHAPE):\n    \"\"\"\n    Read image from `path`, and convert the image to a 3D tensor\n    return resized image.\n    input: `path`: Path to an image\n    return: resized tensor image\n    \"\"\"\n    print('Image size: ({})'.format(img_shape))\n    # Read the image file\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.cast(img, tf.float32)\/255\n    # Resize image to our desired size\n    img = tf.image.resize(img, img_shape)\n    return img\n\n# Configure dataset for performance\ndef configure_for_performance(ds):\n    #ds = ds.cache()\n    ds = ds.batch(batch_size)\n    #ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\n# Create a function to get Dataset\ndef create_dataset(X, y=None, valid_data=False, test_data=False, img_shape=IMG_SHAPE):\n    \"\"\"\n    Create Dataset from Images (X) and Labels (y)\n    Shuffles the data if it's training data but doesn't shuffle if it validation data.\n    Also accepts test data as input (no labels).\n    Return Dataset \n    \"\"\"\n    print(\"Creating data set...\")\n    # If test data, there is no labels\n    if test_data:       \n        print(\"Creating test data batches...\")\n        dataset = tf.data.Dataset.from_tensor_slices((X))\n        dataset = dataset.map(lambda x: decode_img(x, img_shape), num_parallel_calls=AUTOTUNE)\n        dataset = configure_for_performance(dataset)\n    # If Valid_data - we don't need to shuffle\n    elif valid_data:\n        print(\"Creating Valid data batches...\")\n        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n        dataset = dataset.map(lambda x, y: [decode_img(x, img_shape), y], num_parallel_calls=AUTOTUNE)\n        dataset = configure_for_performance(dataset)\n    else:\n        print(\"Creating Training data batches...\")\n        dataset = tf.data.Dataset.from_tensor_slices((X, y))\n        dataset = dataset.map(lambda x, y: [decode_img(x, img_shape), y], num_parallel_calls=AUTOTUNE)\n        dataset = dataset.shuffle(buffer_size=len(X))\n        dataset = configure_for_performance(dataset)\n           \n    print(dataset.element_spec)\n\n    return dataset\n\n\n# Create Models function utils #\n################################\n\n# Callbacks\n# Early stopping Callbacks\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5)\n\n# Reduce Learning rate Callbacks\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                                                   patience=3,\n                                                   factor=0.2,\n                                                   verbose=2,\n                                                   mode='min')\n\n\n# Useful Functions for Model training, saving and loading\n\ndef train_model(transfer_model, epochs = NUM_EPOCHS):\n    \"\"\"  \n    Trains a given model and returns the trained version.\n     Input: model, number of Epochs (default = NUM_EPOCHS)\n     Output: model\n    \"\"\"\n    # create model\n    model = create_model(transfer_model)\n    # Create TensorBoard session\n    tensorboard = create_tensorboard_callback()\n\n    model.summary()\n    print(f\"Information: epochs = {epochs} and number of images = {NUM_IMAGES}\")\n\n    # Fit model\n    model.fit(x=dataset_train,\n              epochs=epochs,\n              validation_data=dataset_val,\n              callbacks=[early_stopping, lr_callback])\n    return model\n\nimport datetime\n# Save and load model\n# Create a function to save a model\ndef save_model(model, suffix=None):\n    \"\"\"\n    Saves a given model in ad models directory and appends a suffix (string).\n    \"\"\"\n    # Create a model directory pathname with current time\n    modeldir = os.path.join(\"..\/output\/kaggle\/working\/saved_models\",\n                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n    model_path = modeldir + \"-\" + suffix + \".h5\" #save format of model\n    print(f\"Save model to: {model_path}...\")\n    model.save(model_path)\n    return model_path\n\n# Create a function to load a model\ndef load_model(model_path):\n    \"\"\"\n    Load a saved model from a specify path\n    \"\"\"\n    print(f\"Loading saved model from: {model_path}...\")\n    model = tf.keras.models.load_model(model_path)\n    return model","e2041c63":"\n#Create and get dataset\nFRAC = 1\nIMG_SIZE = 64\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE)\nOUTPUT_SHAPE = len(unique_plant_cat)\n\ntrain_df, valid_df = create_train_val_df(valid_info, train_info, frac=FRAC)\n# Get data ready\nX_train, X_val, y_train, y_val = prepare_data(train_df, valid_df)\n\n# Create Dataset #\n##################\n# Train dataset - shuffle \ndataset_train = create_dataset(X_train, y_train, img_shape=IMG_SHAPE)\n# Validation Dataset - not shuffle\ndataset_val = create_dataset(X_val, y_val, valid_data=True, img_shape=IMG_SHAPE)\n# Verify length of both datasets\nlen(dataset_train), len(dataset_val)\n\nNUM_IMAGES = len(y_train) + len(y_val)","4948f6ed":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Activation\n\nNUM_EPOCHS = 20\nINPUT_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\n# Create model\n# 4 conv2D layers\n# Batch Normalisation and MaxPooling\ndef get_model():\n    \"\"\"\n    Create a 4 Conv2D layers with\n    - Batch Normalisation\n    - MaxPooling\n    - ReLU activation\n    And 2 Dense layers reLU activation (and Dropout)\n\n    Return 38 probabilities (= number of plants we want to predict) - activation Softmax\n    \"\"\"\n\n    model_v2 = Sequential([\n        # First CNN                     \n        Conv2D(128, kernel_size=3, input_shape=INPUT_SHAPE, activation='relu'),\n        MaxPooling2D(),\n        BatchNormalization(),\n        # Second CNN\n        Conv2D(256, kernel_size=3, activation='relu'),   \n        MaxPooling2D(),\n        BatchNormalization(),\n        # Third CNN\n        Conv2D(512, kernel_size=3, activation='relu'), \n        MaxPooling2D(),\n        BatchNormalization(),\n        # Flatten last CNN output for Dense layers\n        Flatten(),\n        Dense(512, activation='relu'),\n        Dropout(0.2),\n        Dense(256, activation='relu'),\n        Dropout(0.2),\n        # Return 38 probabilities (= number of plants we want to predict)\n        Dense(OUTPUT_SHAPE, activation= 'softmax')\n    ])\n\n    return model_v2\n\n# To Do: modify Adam optimizer and add a specific Learning rate\nmodel = get_model()\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.0005),\n            loss='categorical_crossentropy',\n            metrics=['accuracy']\n            )\n# Show Summary \nmodel.summary()\n","02f201b2":"# Train Model\nhistory = model.fit(x=dataset_train,\n                  epochs=NUM_EPOCHS,\n                  validation_data=dataset_val,\n                  callbacks=[early_stopping, lr_callback])\n\n# Get Validation Loss and Accuracy\nval_loss, val_acc = model.evaluate(dataset_val)\nval_acc = round(val_acc, 3)\n\n# Save model\nsuffix = 'tf_ep-'+str(NUM_EPOCHS)+'_img-'+str(NUM_IMAGES)+'_acc_'+str(val_acc)+'-model_cnn_'+str(FRAC)\nsave_model(model, suffix=suffix)\n","9aa44d83":"# convert the history.history dict to a pandas DataFrame:     \nhist_df = pd.DataFrame(history.history) \n\ndate = datetime.datetime.now().strftime(\"%Y%m%d\")\nhist_csv_file = '..\/output\/kaggle\/working\/history_full_'+str(val_acc)+'_'+str(date)+'.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","47027c68":"# Useful functions to evaluate model\n\n# Turn prediction probabilities into their respective label (easier to understand)\ndef get_pred_label(prediction_probabilities):\n    \"\"\"\n    Turns an array of prediction probabilities into a label.\n    \"\"\"\n    return unique_plant_cat[np.argmax(prediction_probabilities)]\n\n# Create a function to unbatch a batch dataset\ndef unbatchify(batch_data):\n    \"\"\"\n    Take batch data and return unbatch data (separate arrays of images and labels) in a form of a tuple of lists\n    \"\"\"\n    img = []\n    lbl = []\n    for image, label in batch_data.unbatch().as_numpy_iterator():\n        img.append(image*255)\n        lbl.append(get_pred_label(label))\n\n    return img,lbl\n\n# Show images and prediction rate\ndef show_img_and_prediction(model, nb_img=9):\n    # Get predictions\n    predictions = model.predict(dataset_val)\n    # Get Validation datset images and true labels\n    imgs, labels = unbatchify(dataset_val)\n    # Get 10 random images in the validation dataset\n    img_rdm = np.random.randint(0, len(imgs), nb_img)\n\n    plt.figure(figsize=(20,12))\n    for idx, i in enumerate(img_rdm):\n        color = 'red'\n\n        plt.subplot(3,3,idx+1)\n        plt.imshow(imgs[i].astype('uint8'))\n        plt.xticks([])\n        plt.yticks([])\n\n        if get_pred_label(predictions[i]) == labels[i]:\n            color = 'green'\n\n        plt.title('Pred({}) : {} - {:2.0f}%'.format(i, get_pred_label(predictions[i]), np.max(predictions[i])*100), color=color)\n        plt.xlabel('Real: {}'.format(labels[i]));    \n\n\ndef plot_acc_and_loss(history):\n    \"\"\"\n    From Model History, plot two Graphs: \n    - Accuracy Train + Validation\n    - Loss Train + Validation\n\n    Input: model history\n    \"\"\"\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(loss)+1)\n\n    plt.figure(figsize=(16,10))\n\n    plt.subplot(121)\n    plt.plot(epochs, acc, color='red', label='Training Accuracy')\n    plt.plot(epochs, val_acc, color='blue', label='Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.xticks(epochs)\n    plt.yticks(np.arange(0,1.1,0.1))\n    plt.legend()\n\n    plt.subplot(122)\n    plt.plot(epochs, loss, color='orange', label='Training Loss')\n    plt.plot(epochs, val_loss, color='navy', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.xticks(epochs)\n    plt.legend()\n\n\ndef plot_pred_prob(predictions, labels, n=1):\n    \"\"\"\n    Show the top 3 highest prediction confidences along with the truth label for sample n.\n    \n    Inputs: \n      - predictions array \n      - labels array\n      - n id of sample to check \n    \"\"\"\n    pred_prob, true_label = predictions[n], labels[n]\n\n    # Get predicted label\n    pred_label = get_pred_label(pred_prob)\n\n    # Get top 3 prediction confidence indexes\n    top_3_pred_indexes = pred_prob.argsort()[-3:][::-1]\n    # Find the top 3 prediction confidence values\n    top_3_pred_values = pred_prob[top_3_pred_indexes]\n    # Find the top 3 prediction labels\n    top_3_pred_labels = unique_plant_cat[top_3_pred_indexes]\n\n    # Setup plot\n    \n    top_plot = plt.barh(np.arange(len(top_3_pred_labels)),\n                     top_3_pred_values,\n                     color=\"grey\")\n    plt.yticks(np.arange(len(top_3_pred_labels)),\n             labels=top_3_pred_labels,\n             rotation=\"horizontal\")\n    plt.xlabel('Probability')\n  \n    # Change color of true label\n    if np.isin(true_label, top_3_pred_labels):\n        top_plot[np.argmax(top_3_pred_labels == true_label)].set_color(\"green\")\n        if top_3_pred_labels[0] != true_label:\n            top_plot[0].set_color(\"red\")\n        else:\n            pass\n\ndef get_wrong_preds(predictions, labels, n=9):\n\n    pred_idx = []\n\n    for i, pred_prob in enumerate(predictions):\n        pred_label = get_pred_label(pred_prob)\n        if pred_label != labels[i]:\n            pred_idx.append(i)\n\n        if len(pred_idx) >= n:\n            return pred_idx\n\n    return pred_idx","45a2dc2d":"# Get predictions\npredictions = model.predict(dataset_val)\n# Get Validation datset images and true labels\nimgs, labels = unbatchify(dataset_val)\n","efbe38e6":"# Plot Accuracy & Loss\nplot_acc_and_loss(history)","543377ab":"show_img_and_prediction(model)","d2e4ea74":"#wrong_pred_idx\nwrong_pred_idx = get_wrong_preds(predictions, labels, n=6)\n\nplt.figure(figsize=(20,18))\nplt.subplots_adjust(wspace = 0.6)\nfor i, pred_idx in enumerate(wrong_pred_idx):\n    plt.subplot(3,3,i+1)\n    plot_pred_prob(predictions, labels, n=pred_idx)\n    plt.title('Real: {}'.format(labels[pred_idx]))\n","e3597217":"# Get test data\nimport os\nimport pandas as pd\n\n\ntest_path = '..\/input\/new-plant-diseases-dataset\/test\/test'\n\ntest_imgs = [os.path.join(test_path,img) for img in os.listdir(test_path)]\ndf_test = pd.DataFrame(test_imgs, columns=['Path'])\n\ndf_test.head()","edfba351":"#Create Test Dataset\nIMG_SIZE = 64\nIMG_SHAPE = (IMG_SIZE, IMG_SIZE)\n\ntest_dataset = create_dataset(df_test['Path'], test_data=True, img_shape=IMG_SHAPE)","6492f4f0":"# Create a DF with Predictions\npreds_df = pd.DataFrame(columns=[\"id\"] + list(unique_plant_cat))\n# Append test image ID's to prediction DataFrame\ntest_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]\npreds_df[\"id\"] = test_ids","db6736a5":"test_preds = model.predict(test_dataset)","dc13e305":"# Add the prediction probabilities to each plants category columns\npreds_df[list(unique_plant_cat)] = test_preds\npreds_df.head()","2dcc23b0":"# Show Test Images\nimport matplotlib.image as mpimg\n\nimages_test = []\nfor img_path in df_test['Path']:\n    images_test.append(mpimg.imread(img_path))\n\nplt.figure(figsize=(20,40))\nfor i, image in enumerate(images_test):\n    plt.subplot(11,3,i+1)\n    plt.imshow(image)\n    plt.title('Pred: {} - {:2.0f}%'.format( get_pred_label(test_preds[i]), np.max(test_preds[i])*100))\n    plt.xlabel(f'Real: {test_ids[i]}', fontsize=13, color='blue')\n    plt.xticks([])\n    plt.yticks([])","ac9f150a":"### Use GPU","e0845e6b":"# New Plant Disease - Classification using Tensorflow 2.x","67ea1d54":"## 1. Environment preparation\n1. Make sure TensorFlow is installed \n2. Check if we are connected to GPU\n\n","4ac7b0bf":"### 3. Plot wrong predictions","7b9fcc9f":"For the purpose of Image Classification, we will use GPU to allow us to use Kaggle resources. This will help us to considerably improve the capacity of the laptop to execute ML programs.","e386692b":"### 2. Plot Predictions","d5ca4901":"We retreived our data from the Dataset folder.\n\nWe created multiple methods and functions that would help us to prepare our data into batches and create our model.\n\nWe can now start the Modeling phase.","756d47fa":"## What Next?","da8a1515":"## 2. Evaluation of model","71c077d8":"## 2. Initialisation code ","b651b023":"## Evaluate model with Test dataset","55011f5c":"The model have been trained, we can now evaluate it to conclude about the good performance.","d63c8d2f":"We will create DataFrame to stor required information for our program.\nDataFrames are the same used during **Data Vizualisation** report.\n\nTrain and Validation Dataframe composed of:\n- List of Plants name (Plant)\n- List of Plants Categories (Category)\n- List of images path (Path) > Features\n- List of Plants name + Plants Categories (Plant___Category) > Labels\n- If Plant is diseased or not (Disease)","f9d883ff":"# MODELS","4e8bd1c2":"Now that we get our predictions from the Test data we can:\n- Create a Dataframe with the Test images and the prediciton probabilities for each categories\n- Show each Test images with Prediction label and Real label","54edcfe5":"We can see that most of the predictions are correct. So our model has been well trained.\n\nNow that we validate our model, we should design a better application where the user can upload a picture of a plant, and the application should provide below information:\n- What kind of plant?\n- Is the plant healthy or diseased?\n- If the plant is diseased, provide information about the disease itself.","fbd2ed9c":"## 1. Create own CNN","57e11f38":"### 1. Plot Accuracy & Loss"}}