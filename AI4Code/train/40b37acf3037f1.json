{"cell_type":{"42148f3e":"code","f8010d60":"code","b7490259":"code","56c90e46":"code","449a3ac2":"code","a4cdfc37":"code","f2c4f0fe":"code","ed7a4633":"code","2b9fe53b":"code","ea306d31":"code","8cd613c6":"code","2c053375":"code","69b2d5a6":"code","95003bbf":"code","56d5744b":"code","66d48d85":"code","59d4a45e":"code","5fe59e26":"code","09481ead":"code","e595a401":"code","656c254f":"code","f73cb125":"code","84befc1d":"code","38f16c7a":"code","c3f0f12c":"code","fc802cf5":"code","30903479":"code","ce7602bd":"code","ac2e68c9":"code","4e8a38fb":"code","b97cf30e":"code","92f55167":"code","f4eae5f5":"code","4a2b787a":"code","ca1effb5":"code","94feeefd":"code","3a4183db":"markdown","2d79dbeb":"markdown","f6f65438":"markdown","3007293a":"markdown","048ab390":"markdown","e72de7fc":"markdown","f0e350e2":"markdown","1e45606a":"markdown","55b49b7b":"markdown","34202093":"markdown","0badabdb":"markdown","790a560f":"markdown","8e53b627":"markdown","d22efe10":"markdown","9713fceb":"markdown","0158e7c3":"markdown","b0569826":"markdown","45ae5ac3":"markdown","1980a6e8":"markdown","2e7e778f":"markdown","23b5e69b":"markdown","748a9639":"markdown","8e7dab78":"markdown"},"source":{"42148f3e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\npyo.init_notebook_mode()\nsns.set_style('darkgrid')\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nplt.rc('figure',figsize=(18,9))\n%pip install imbalanced-learn\nfrom imblearn.over_sampling import SMOTE","f8010d60":"c_data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\nc_data = c_data[c_data.columns[:-2]]\nc_data.head(3)","b7490259":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Customer_Age'],name='Age Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Customer_Age'],name='Age Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Customer Ages\")\nfig.show()","56c90e46":"fig = make_subplots(\n    rows=2, cols=2,subplot_titles=('','<b>Platinum Card Holders','<b>Blue Card Holders<b>','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"pie\",\"rowspan\": 2}       ,{\"type\": \"pie\"}] ,\n           [None                               ,{\"type\": \"pie\"}]            ,                                      \n          ]\n)\n\nfig.add_trace(\n    go.Pie(values=c_data.Gender.value_counts().values,labels=['<b>Female<b>','<b>Male<b>'],hole=0.3,pull=[0,0.3]),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Pie(\n        labels=['Female Platinum Card Holders','Male Platinum Card Holders'],\n        values=c_data.query('Card_Category==\"Platinum\"').Gender.value_counts().values,\n        pull=[0,0.05,0.5],\n        hole=0.3\n        \n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Pie(\n        labels=['Female Blue Card Holders','Male Blue Card Holders'],\n        values=c_data.query('Card_Category==\"Blue\"').Gender.value_counts().values,\n        pull=[0,0.2,0.5],\n        hole=0.3\n    ),\n    row=2, col=2\n)\n\n\n\nfig.update_layout(\n    height=800,\n    showlegend=True,\n    title_text=\"<b>Distribution Of Gender And Different Card Statuses<b>\",\n)\n\nfig.show()","449a3ac2":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Dependent_count'],name='Dependent count Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Dependent_count'],name='Dependent count Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Dependent counts (close family size)\")\nfig.show()","a4cdfc37":"ex.pie(c_data,names='Education_Level',title='Propotion Of Education Levels',hole=0.33)","f2c4f0fe":"ex.pie(c_data,names='Marital_Status',title='Propotion Of Different Marriage Statuses',hole=0.33)\n","ed7a4633":"ex.pie(c_data,names='Income_Category',title='Propotion Of Different Income Levels',hole=0.33)","2b9fe53b":"ex.pie(c_data,names='Card_Category',title='Propotion Of Different Card Categories',hole=0.33)","ea306d31":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Months_on_book'],name='Months on book Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Months_on_book'],name='Months on book Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of months the customer is part of the bank\")\nfig.show()","8cd613c6":"print('Kurtosis of Months on book features is : {}'.format(c_data['Months_on_book'].kurt()))","2c053375":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Total_Relationship_Count'],name='Total no. of products Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Total_Relationship_Count'],name='Total no. of products Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of Total no. of products held by the customer\")\nfig.show()","69b2d5a6":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Months_Inactive_12_mon'],name='number of months inactive Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Months_Inactive_12_mon'],name='number of months inactive Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the number of months inactive in the last 12 months\")\nfig.show()","95003bbf":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Credit_Limit'],name='Credit_Limit Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Credit_Limit'],name='Credit_Limit Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the Credit Limit\")\nfig.show()","56d5744b":"fig = make_subplots(rows=2, cols=1)\n\ntr1=go.Box(x=c_data['Total_Trans_Amt'],name='Total_Trans_Amt Box Plot',boxmean=True)\ntr2=go.Histogram(x=c_data['Total_Trans_Amt'],name='Total_Trans_Amt Histogram')\n\nfig.add_trace(tr1,row=1,col=1)\nfig.add_trace(tr2,row=2,col=1)\n\nfig.update_layout(height=700, width=1200, title_text=\"Distribution of the Total Transaction Amount (Last 12 months)\")\nfig.show()","66d48d85":"ex.pie(c_data,names='Attrition_Flag',title='Proportion of churn vs not churn customers',hole=0.33)","59d4a45e":"c_data","5fe59e26":"c_data.Attrition_Flag = c_data.Attrition_Flag.replace({'Attrited Customer':1,'Existing Customer':0})\nc_data.Gender = c_data.Gender.replace({'F':1,'M':0})\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Education_Level']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Income_Category']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Marital_Status']).drop(columns=['Unknown'])],axis=1)\nc_data = pd.concat([c_data,pd.get_dummies(c_data['Card_Category']).drop(columns=['Platinum'])],axis=1)\nc_data.drop(columns = ['Education_Level','Income_Category','Marital_Status','Card_Category','CLIENTNUM'],inplace=True)","09481ead":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =c_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=0.7,ygap=0.7,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =c_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=0.7,ygap=0.7,colorscale=colorscale),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Numeric Correaltions\")\nfig.show()","e595a401":"oversample = SMOTE()\nX, y = oversample.fit_resample(c_data[c_data.columns[1:]], c_data[c_data.columns[0]])\nusampled_df = X.assign(Churn = y)","656c254f":"ohe_data =usampled_df[usampled_df.columns[15:-1]].copy()\n\nusampled_df = usampled_df.drop(columns=usampled_df.columns[15:-1])","f73cb125":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =usampled_df.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =usampled_df.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Upsmapled Correlations\")\nfig.show()\n","84befc1d":"\nN_COMPONENTS = 4\n\npca_model = PCA(n_components = N_COMPONENTS )\n\npc_matrix = pca_model.fit_transform(ohe_data)\n\nevr = pca_model.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='Explained Variance Using {} Dimensions'.format(N_COMPONENTS))\nfig.show()","38f16c7a":"usampled_df_with_pcs = pd.concat([usampled_df,pd.DataFrame(pc_matrix,columns=['PC-{}'.format(i) for i in range(0,N_COMPONENTS)])],axis=1)","c3f0f12c":"fig = ex.scatter_matrix(\n    usampled_df_with_pcs[['PC-{}'.format(i) for i in range(0,N_COMPONENTS)]].values,\n    color=usampled_df_with_pcs.Credit_Limit,\n    dimensions=range(N_COMPONENTS),\n    labels={str(i):'PC-{}'.format(i) for i in range(0,N_COMPONENTS)},\n    title=f'Total Explained Variance: {total_var:.2f}%')\n\nfig.update_traces(diagonal_visible=False)\nfig.update_layout(\n    coloraxis_colorbar=dict(\n        title=\"Credit_Limit\",\n    ),\n)\nfig.show()","fc802cf5":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\n\n\ns_val =usampled_df_with_pcs.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =usampled_df_with_pcs.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\nfig.update_layout(\n    hoverlabel=dict(\n        bgcolor=\"white\",\n        font_size=16,\n        font_family=\"Rockwell\"\n    )\n)\nfig.update_layout(height=700, width=900, title_text=\"Upsmapled Correlations With PC\\'s\")\nfig.show()\n\n","30903479":"X_features = ['Total_Trans_Ct','PC-3','PC-1','PC-0','PC-2','Total_Ct_Chng_Q4_Q1','Total_Relationship_Count']\n\nX = usampled_df_with_pcs[X_features]\ny = usampled_df_with_pcs['Churn']","ce7602bd":"train_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)","ac2e68c9":"rf_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",RandomForestClassifier(random_state=42)) ])\nada_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",AdaBoostClassifier(random_state=42,learning_rate=0.7)) ])\nsvm_pipe = Pipeline(steps =[ ('scale',StandardScaler()), (\"RF\",SVC(random_state=42,kernel='rbf')) ])\n\n\nf1_cross_val_scores = cross_val_score(rf_pipe,train_x,train_y,cv=5,scoring='f1')\nada_f1_cross_val_scores=cross_val_score(ada_pipe,train_x,train_y,cv=5,scoring='f1')\nsvm_f1_cross_val_scores=cross_val_score(svm_pipe,train_x,train_y,cv=5,scoring='f1')","4e8a38fb":"fig = make_subplots(rows=3, cols=1,shared_xaxes=True,subplot_titles=('Random Forest Cross Val Scores',\n                                                                     'Adaboost Cross Val Scores',\n                                                                    'SVM Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(f1_cross_val_scores))),y=f1_cross_val_scores,name='Random Forest'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(ada_f1_cross_val_scores))),y=ada_f1_cross_val_scores,name='Adaboost'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(svm_f1_cross_val_scores))),y=svm_f1_cross_val_scores,name='SVM'),\n    row=3, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"F1 Score\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","b97cf30e":"rf_pipe.fit(train_x,train_y)\nrf_prediction = rf_pipe.predict(test_x)\n\nada_pipe.fit(train_x,train_y)\nada_prediction = ada_pipe.predict(test_x)\n\nsvm_pipe.fit(train_x,train_y)\nsvm_prediction = svm_pipe.predict(test_x)","92f55167":"fig = go.Figure(data=[go.Table(header=dict(values=['<b>Model<b>', '<b>F1 Score On Test Data<b>'],\n                                           line_color='darkslategray',\n    fill_color='whitesmoke',\n    align=['center','center'],\n    font=dict(color='black', size=18),\n    height=40),\n                               \n                 cells=dict(values=[['<b>Random Forest<b>', '<b>AdaBoost<b>','<b>SVM<b>'], [np.round(f1(rf_prediction,test_y),2), \n                                                                          np.round(f1(ada_prediction,test_y),2),\n                                                                          np.round(f1(svm_prediction,test_y),2)]]))\n                     ])\n\nfig.update_layout(title='Model Results On Test Data')\nfig.show()","f4eae5f5":"ohe_data =c_data[c_data.columns[16:]].copy()\npc_matrix = pca_model.fit_transform(ohe_data)\noriginal_df_with_pcs = pd.concat([c_data,pd.DataFrame(pc_matrix,columns=['PC-{}'.format(i) for i in range(0,N_COMPONENTS)])],axis=1)\n\nunsampled_data_prediction_RF = rf_pipe.predict(original_df_with_pcs[X_features])\nunsampled_data_prediction_ADA = ada_pipe.predict(original_df_with_pcs[X_features])\nunsampled_data_prediction_SVM = svm_pipe.predict(original_df_with_pcs[X_features])","4a2b787a":"fig = go.Figure(data=[go.Table(header=dict(values=['<b>Model<b>', '<b>F1 Score On Original Data (Before Upsampling)<b>'],\n                                           line_color='darkslategray',\n    fill_color='whitesmoke',\n    align=['center','center'],\n    font=dict(color='black', size=18),\n    height=40),\n                               \n                 cells=dict(values=[['<b>Random Forest<b>', '<b>AdaBoost<b>','<b>SVM<b>'], [np.round(f1(unsampled_data_prediction_RF,original_df_with_pcs['Attrition_Flag']),2), \n                                                                          np.round(f1(unsampled_data_prediction_ADA,original_df_with_pcs['Attrition_Flag']),2),\n                                                                          np.round(f1(unsampled_data_prediction_SVM,original_df_with_pcs['Attrition_Flag']),2)]]))\n                     ])\n\nfig.update_layout(title='Model Result On Original Data (Without Upsampling)')\nfig.show()","ca1effb5":"z=confusion_matrix(unsampled_data_prediction_RF,original_df_with_pcs['Attrition_Flag'])\nfig = ff.create_annotated_heatmap(z, x=['Not Churn','Churn'], y=['Predicted Not Churn','Predicted Churn'], colorscale='Fall',xgap=3,ygap=3)\nfig['data'][0]['showscale'] = True\nfig.update_layout(title='Prediction On Original Data With Random Forest Model Confusion Matrix')\nfig.show()","94feeefd":"unsampled_data_prediction_RF = rf_pipe.predict_proba(original_df_with_pcs[X_features])\nskplt.metrics.plot_precision_recall(original_df_with_pcs['Attrition_Flag'], unsampled_data_prediction_RF)\nplt.legend(prop={'size': 20})","3a4183db":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table Of Content<\/h1>\n\n\n* [1. Introduction](#1)\n    * [1.1 Libraries And Utilities](#1.1)\n    * [1.2 Data Loading](#1.2)\n* [2. Exploratory Data Analysis(EDA)](#2)\n* [3. Data Preprocessing](#3)\n    * [3.1 Data Upsampling Using SMOTE](#3.1)\n    * [3.2 Principal Component Analysis Of One Hot Encoded Data](#3.2)  \n* [4. Model Selection And Evaluation](#4) \n    * [4.1 Cross Validation](#4.1)\n    * [4.2 Model Evaluation](#4.2)\n    * [4.3 Model Evaluation On Original Data (Before Upsampling)](#4.3)\n* [5. Results](#5) \n","2d79dbeb":"<a id=\"4.2\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation<\/h3>\n","f6f65438":"<a id=\"3\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h1>\n","3007293a":"<a id=\"3.2\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Principal Component Analysis Of One Hot Encoded Data <\/h3>\n","048ab390":"<a id=\"4\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Model Selection And Evaluation<\/h1>\n","e72de7fc":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>If most of the customers with unknown education status lack any education, we can state that more than 70% of the customers have a formal education level. About 35% have a higher level of education.<\/span><\/p>","f0e350e2":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We will use principal component analysis to reduce the dimensionality of the one-hot encoded categorical variables losing some of the variances, but simultaneously, using a couple of principal components instead of tens of one-hot encoded features will help me construct a better model.<\/span><\/p>","1e45606a":"<a id=\"3.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Upsampling Using SMOTE<\/h3>\n","55b49b7b":"<a id=\"4.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Cross Validation<\/h3>\n","34202093":"<a id=\"5\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Results<\/h1>\n","0badabdb":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The distribution of Dependent counts is fairly normally distributed with a slight right skew.<\/span><\/p>","790a560f":"<a id=\"1.2\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading<\/h3>\n","8e53b627":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We see that the distribution of the total transactions (Last 12 months) displays a multimodal distribution, meaning we have some underlying groups in our data; it can be an interesting experiment to try and cluster the different groups and view the similarities between them and what describes best the different groups which create the different modes in our distribution.<\/span><\/p>","d22efe10":"<a id=\"2\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:300%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","9713fceb":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>More samples of females in our dataset are compared to males, but the percentage of difference is not that significant, so we can say that genders are uniformly distributed.<\/span><\/p>","0158e7c3":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can see that the distribution of customer ages in our dataset follows a fairly normal distribution; thus, further use of the age feature can be done with the normality assumption.<\/span><\/p>","b0569826":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>We have a low kurtosis value pointing to a very flat shaped distribution (as shown in the plots above as well), meaning we cannot assume normality of the feature.<\/span><\/p>","45ae5ac3":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>As we can see, only 16% of the data samples represent churn customers; in the following steps, I will use SMOTE to upsample the churn samples to match them with the regular customer sample size to give the later selected models a better chance of catching on small details which will almost definitely be missed out with such a size difference.<\/span><\/p>","1980a6e8":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The distribution of the total number of products held by the customer seems closer to a uniform distribution and may appear useless as a predictor for churn status.<\/span><\/p>","2e7e778f":"<a id=\"4.3\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Model Evaluation On Original Data (Before Upsampling)<\/h3>\n","23b5e69b":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Here we one hot encode all the categorical features describing different statuses of a customer.<\/span><\/p>","748a9639":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>","8e7dab78":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Almost half of the bank customers are married, and interestingly enough, almost the entire other half are single customers. only about 7% of the customers are divorced, which is surprising considering the worldwide divorce rate statistics! (let me know where the bank is located and sign me up!)<\/span><\/p>"}}