{"cell_type":{"ec8dd798":"code","6ec46aa8":"code","8dce6974":"code","e2423819":"code","10625c85":"code","6e23f8b7":"code","fd7d2fcb":"code","d382ea0b":"code","252eb29f":"code","65ebc053":"code","b99427a7":"code","9e336e99":"code","35a9f2f8":"code","c7cffc2e":"code","e06dee25":"code","0d9e48ae":"code","46699ba0":"code","676a10be":"code","053b5577":"code","d53b02e4":"code","116e2a9d":"code","d6e762b8":"code","be0f410a":"code","7858a347":"code","fa6d020e":"code","3da9f890":"code","1eb3280c":"code","6b52d72d":"code","aac35151":"code","cea74c45":"code","b3433c46":"code","afcf191c":"code","5af6d2ac":"code","2fc6ad1f":"code","73c1f12b":"code","bdbf4b61":"code","d27388ba":"code","d8ecbf71":"code","484bbbec":"markdown"},"source":{"ec8dd798":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ec46aa8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time\nimport datetime as dt","8dce6974":"temp=pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv')\ndf= pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv')","e2423819":"df.head()","10625c85":"df.describe()","6e23f8b7":"(df.isnull().sum())\n","fd7d2fcb":"df.dropna(inplace=True)","d382ea0b":"df.nunique()","252eb29f":"df.info()","65ebc053":"df= df[df['Quantity']>=0]\ndf = df[df['UnitPrice']>=0]","b99427a7":"df.describe()","9e336e99":"df['totalcost']=df['Quantity']*df['UnitPrice']\ndf","35a9f2f8":"pi=df.groupby('Country').count().sort_values(\"InvoiceNo\",ascending = False)\npi.head()","c7cffc2e":"\nf=df['Country'].unique()\n\nfig = plt.figure(figsize =(10, 7)) \nplt.pie(pi['InvoiceNo'].head(6), labels= f[:6],autopct='%1.2f%%',explode = [0.2, 0, 0, 0,0,0])\nplt.title(\"Country wise Distribution\")\nplt.show() \n","e06dee25":"p=df.groupby('Description').count().sort_values(\"InvoiceNo\",ascending = False)\np=p.head(15)\n\nsns.barplot(p.index,p['Quantity'],palette = \"rocket\")\nplt.xlabel('Top Selling Items')\nplt.xticks(rotation=90)\n","0d9e48ae":"\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\ndf['Date'] = df['InvoiceDate'].apply(lambda x: x.date())\ndf.head()\n\n\n#df['Date'] = df['InvoiceDate'].dt.date\n#df.head()","46699ba0":"recency_df = df.groupby(by='CustomerID', as_index=False)['Date'].max()\nrecency_df.columns = ['CustomerID','LastPurchaseDate']\nrecency_df","676a10be":"now = dt.date(2021,12,1)\nprint(now)","053b5577":"recency_df['Recency'] = recency_df['LastPurchaseDate'].apply(lambda x: (now - x).days)\n\n\nrecency_df.drop('LastPurchaseDate',axis = 1,inplace=True)\nrecency_df.head(5)","d53b02e4":"temp = df.copy()\ntemp.drop_duplicates(['InvoiceNo','CustomerID'],inplace=True)\nfrequency_df = temp.groupby(by=['CustomerID'], as_index=False)['InvoiceNo'].count()\nfrequency_df.columns = ['CustomerID','Frequency']\nfrequency_df.head()","116e2a9d":"monetary_df = df.groupby(by = 'CustomerID',as_index=False).agg({'totalcost':'sum'})\nmonetary_df.columns = ['CustomerID','TotalCost']\nmonetary_df.head(5)","d6e762b8":"rfm = recency_df.merge(frequency_df,on='CustomerID').merge(monetary_df,on='CustomerID')\nrfm.set_index('CustomerID',inplace=True)\na=rfm.columns\n","be0f410a":"print(rfm.corr())\nsns.heatmap(rfm.corr(),cmap=\"RdYlGn\",annot=True)","7858a347":"sns.pairplot(rfm, diag_kind=\"hist\")","fa6d020e":"from sklearn.preprocessing import PowerTransformer\npt = PowerTransformer()\nrfm = pd.DataFrame(pt.fit_transform(rfm))\nrfm.columns = a\nrfm.head()","3da9f890":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nrfm_scaled = sc.fit_transform(rfm)\nrfm_scaled[:5]","1eb3280c":"from sklearn.decomposition import PCA\npca = PCA()\npca_tranformed_data = pca.fit_transform(rfm_scaled)","6b52d72d":"X = rfm.copy()\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(X)\n\ndf_pca = pd.DataFrame(df_pca)\ndf_pca.head(5)","aac35151":"X=df_pca.copy()","cea74c45":"from sklearn.cluster import KMeans \n\ncluster_range = range(1, 8)\ncluster_errors = []\ncluster_sil_scores = []\n\nfor num in cluster_range: \n    clusters = KMeans(num, n_init = 100,init='k-means++',random_state=0)\n    clusters.fit(X)\n    labels = clusters.labels_                     # capture the cluster lables\n    centroids = clusters.cluster_centers_         # capture the centroids\n    cluster_errors.append( clusters.inertia_ )    # capture the intertia\nclusters_df = pd.DataFrame({ \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors} )\nclusters_df[0:10]","b3433c46":"plt.figure(figsize=(15,6))\nplt.plot(clusters_df[\"num_clusters\"],clusters_df[\"cluster_errors\"],marker = 'o')\nplt.xlabel('count of clusters')\nplt.ylabel('error')","afcf191c":"from  sklearn.metrics import silhouette_score\n\nfor num in range(2,16):\n    clusters = KMeans(n_clusters=num,random_state=0)\n    labels = clusters.fit_predict(df_pca)\n    \n    sil_avg = silhouette_score(df_pca, labels)\n    print('For',num,'The Silhouette Score is =',sil_avg)","5af6d2ac":"kmeans = KMeans(n_clusters = 4,random_state=0)\nkmeans = kmeans.fit(df_pca)\nlabels = kmeans.predict(df_pca)\ncentroids = kmeans.cluster_centers_","2fc6ad1f":"df_pca['Clusters'] = labels\ndf_pca.head()","73c1f12b":"df_pca['Clusters'].value_counts()","bdbf4b61":"sns.pairplot(df_pca,diag_kind='hist',hue='Clusters',palette='rocket')","d27388ba":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import cross_val_score\n","d8ecbf71":"X = df_pca[[0,1]]\nY = df_pca['Clusters']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\nlr = LogisticRegression(max_iter=1000,random_state=0)\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint('Test accuracy = ', accuracy_score(y_test, y_pred))","484bbbec":"> ***Since CustomerID is missing ,there is no known way to find it .It is best to drop all those rows* .**"}}