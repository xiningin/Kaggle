{"cell_type":{"da242d7c":"code","7e2b902a":"code","80bc0e93":"code","9f8c2bb3":"code","3dc53efa":"code","db306542":"code","d608a070":"code","90e7006d":"code","213dda03":"markdown","c916f71c":"markdown","8cdd3cd2":"markdown","506f44d2":"markdown","bd5875b8":"markdown","1b8fc132":"markdown","d757257f":"markdown","917419a4":"markdown","1613d728":"markdown","ee04c240":"markdown"},"source":{"da242d7c":"import importlib\nimport nltk\nnltk.download('brown')\n","7e2b902a":"# Get a dataset\n\nfrom nltk.corpus import brown\n\n# Import APIs for generating trigrams\n\nfrom nltk import trigrams\n\n# Import Counter API to store frequencies\n\nfrom collections import Counter, defaultdict\n","80bc0e93":"gptFree = defaultdict(lambda: defaultdict(lambda: 0))\n\n# We don't want a KeyError, hence using defaultdict. \n# It will assign zero probability if a trigram probobality turns out zero\n\nprint(gptFree)","9f8c2bb3":"for sentence in brown.sents():\n    for word_1, word_2, word_3 in trigrams(sentence, pad_right=True, pad_left=True):\n        gptFree[(word_1, word_2)][word_3] += 1 # Storing frequencies as and updating +1 as we see them\n        \n        ","3dc53efa":"for word1_word2 in gptFree:\n    freq = float(sum(gptFree[word1_word2].values())) # Fetch frequencies of two words coming together\n    for word_3 in gptFree[word1_word2]:\n        gptFree[word1_word2][word_3] \/= freq","db306542":"dict(gptFree['I','am'])","d608a070":"dict(gptFree['What','is'])","90e7006d":"dict(gptFree['The','bank'])","213dda03":"## Exercise | Building your own language model","c916f71c":"> Provide two starter words to predict the next word","8cdd3cd2":"### Predict next word\n\n","506f44d2":"## Python code\n\n---","bd5875b8":"---\n","1b8fc132":"### Let's count and store frequency of trigrams in the reuters data set.","d757257f":"### Convert frequencies to probabilites","917419a4":"### Let's name our State-Of-The-Art model as gptFree\n\n","1613d728":"### A quick recap to language models\n\n#### To deep dive and to go Zero-to-Hero in NLP, check out this awesome github repo. Link below\n[Zero-to-Hero in NLP](https:\/\/github.com\/samacker77\/Zero-to-Hero-in-NLP)\n\n> Language models assign probability to predict the next word based on the previous words.\n\n1. n-gram language model - Predicts next word by looking at (n-1) words in the past.\n    - bi-gram\/2-gram language model - Predicts next word by looking at (2-1) = 1 word in the past.\n    - tri-gram\/3-gram language model - Predicts next word by looking at (3-1) = 2 words in the past.\n    \n    \n2. How to assign probabilites?\n\nSuppose the past words for a tri-gram language model are $w_1$, $w_2$ and we need to assign probability for the next word $w$.\n\nCount the number of times $w_1$ and $w_2$ come together[C1] , and count the number of times $w_1$, $w_2$, and $w$ come together [C2].\n\nDivide C2 by C1.\n\n$P(w | w_1,w_2) = \\frac{C2}{C1}$\n\n\nFor example,\n\nImagine you have a corpus, and you want to assign probability to the word 'Sam' after seeing two words 'I am'.\n\nCount the number of times 'I am Sam' come together, and count the number of times 'I am' come together. You will get the probability of getting 'Sam' after seeing 'I am'. Easy maths right?\n\nTo generate more coherent sentence, we continue this simple math with chain rule of probability.","ee04c240":"---"}}