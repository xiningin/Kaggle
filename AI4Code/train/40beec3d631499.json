{"cell_type":{"303c326b":"code","d88d5132":"code","a5ef3b37":"code","c77eb8e9":"code","130bb648":"code","d77b65f3":"code","fe73ae50":"code","056ae283":"code","f66a40cb":"code","e24991e5":"code","406f2275":"code","13ce1ad3":"code","1896927c":"code","fd0fbaed":"code","35b0ea59":"code","15980c33":"code","a701cc89":"markdown","e20292df":"markdown","274169be":"markdown","a0120bd1":"markdown","0930257a":"markdown","5f60395b":"markdown","094edce0":"markdown","ec9da6cb":"markdown","a77c6fd7":"markdown","bc0d5c80":"markdown","36e63cd7":"markdown","6bec0c14":"markdown","5461ed44":"markdown","234ba612":"markdown","e25eb7fd":"markdown","4f513007":"markdown","60f456ec":"markdown","d5fb9e6f":"markdown","c5aa58d0":"markdown","0fcd3766":"markdown","7996e331":"markdown","426a1131":"markdown","deb5dcaf":"markdown"},"source":{"303c326b":"import numpy as np  # for matrix operations\nimport pandas as pd  # for loading CSV Files\nimport matplotlib.pyplot as plt # for Data Visualization","d88d5132":"data = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/heart_disease.csv\")","a5ef3b37":"data.head() #head function displays the first 5 rows by default","c77eb8e9":"data.shape # This gives the shape of the DataFrame i.e the (no. of rows, no. of columns)","130bb648":"X = data.drop('target', axis=1) #Input variables\n# axis=1 indicates that a column will be dropped\ny = data['target']  # Target variable\n\nprint(type(X))\nprint(type(y))","d77b65f3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","fe73ae50":"# find the number of input features\nX_train.shape[1]","056ae283":"# Imports\nimport tensorflow as tf  # Importing the TensorFlow Library\nfrom tensorflow import keras  # Import Keras from TensorFlow\n\nfrom tensorflow.keras import Sequential \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense","f66a40cb":"# Building the model\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))   \nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))                                  ","e24991e5":"# Compiling the model\nfrom tensorflow.keras.optimizers import RMSprop\noptimizer = RMSprop(0.001)  # Here, we have set our learning rate as 0.001\nmodel.compile(loss='binary_crossentropy', optimizer= optimizer , metrics=['accuracy'])","406f2275":"# printing the summary of the model\nmodel.summary()","13ce1ad3":"# plotting the model\nfrom tensorflow.keras.utils import plot_model\nplot_model(model)","1896927c":"history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=10, verbose=1)","fd0fbaed":"model.evaluate(X_test, y_test)# batch_size=32","35b0ea59":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","15980c33":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'])\nplt.show()","a701cc89":"* The above code creates a Neural Network that has 4 layers. \n\n* The **last node** uses the **sigmoid activation function** that will squeeze all the values between 0 and 1.\n\n* The other layers use **ReLU (Rectified Linear Units)** as the activation function. ReLU is a half rectified function; that is, for all the inputs less than 0 (e.g. -120,-6.7, -0.0344, 0) the value is 0 while for anything positive (e.g. 10,15, 34) the value is retained. \n\n* One output unit is used since for each record values in X, a probability will be predicted. If it is high, then the person has a heart disease. If it is less, then the person does not have a heart disease.","e20292df":"The validation accuracy predicted by the model is around 80%. It can further be increased by trying to optimize the epochs, the number of layers or the number of nodes per layer.","274169be":"# Splitting into Train and Test Sets\nThe next step will be to divide the data into test and train sets. We want to check the performance of the model that we built. For this purpose, we always split (both input and output data) the given data into **train set** which will be used to train the model, and **test set** which will be used to check how accurately the model is predicting outcomes.\n\nThis is achieved using `train_test_split` function provided in the `model_selection class of sklearn` module.\n\nBy passing our X and y variables into the train_test_split method, we are able to capture the splits in data by assigning 4 variables to the result.\n\n* **X_train:** independent\/input feature data for training the model\n* **y_train:** dependent\/output feature data for training the model\n* **X_test:** independent\/input feature data for testing the model; will be used to predict the output values\n* **y_test:** original dependent\/output values of X_test; We will compare this values with our predicted values to check the performance of our built model.\n* **test_size = 0.20:** 20% of the data will go for test set and 80% of the data will go for train set\n* **random_state = 42:** this is just for code reproducability. It will fix the split i.e. there will be the same data in train and test sets each time you run the code","a0120bd1":"### Model Accuracy\nNow, we'll use the history object created above to plot the Accuracy and Loss throughout the training process.\n\nYou can think of history.history as a Python dictionary from which the values can be obtained by specifying a key within square brackets.\n\nFor eg. `history.history['accuracy']` will give the train accuracy throughout the training process.","0930257a":"We have 303 examples (rows) in our dataset. ","5f60395b":"## Model Creation\/ Definition\nWe create a Sequential model and add layers one at a time until we are happy with our network architecture.\n\nThe first thing to get right is to ensure the input layer has the right number of input features.\n\nSince this is a binary classification problem, we will use a sigmoid activation function in the final layer of our network.\n\n**Sigmoid is commonly used in the output layer. This is because it helps in giving a probability(value between 0 and 1) which is useful in Binary Classification.**","094edce0":"# Loading the basic Data Science Libraries\nAll Python capabilities are not loaded to our working environment by default (even if they are already installed in your system). So, we import each and every library that we want to use.\n\nIn data science, numpy, pandas and matplotlib are most commonly used libraries. We chose alias names for our libraries for the sake of our convenience (numpy --> np and pandas --> pd, matplotlib.pyplot as plt).","ec9da6cb":"## Model Compilation","a77c6fd7":"# You might ask \"why we are using relu first and sigmoid at the last?\" or \"How are we choosing the no. of neurons in dense layer?\" etc. Most of it is covered in [this article](https:\/\/machinelearningknowledge.ai\/activation-functions-neural-network\/) and [this article](https:\/\/towardsdatascience.com\/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e). \n\nIn short, the activation functions have some properties that make one favourable to use at one place and another activation function to be suitable at another place.","bc0d5c80":"# Loading Data\nPandas module is used for reading files. Since we have our data in '.csv' format, we will use 'read_csv()' function for loading the data.","36e63cd7":"Look how the accuracy is slowly increasing and the loss slowly decreasing. ","6bec0c14":"# Agenda\n1. Introduction\n2. Data Description\n3. Loading the basic Data Science Libraries\n4. Loading Data\n5. Separating Input and Target Variable\n6. Splitting into Train and Test Sets\n7. Model Building\n  \n  7.1 Model Creation\/Definition\n    - Our model architecture\n\n  7.2 Model Compilation\n    - History\n\n  7.3 Model Training\n\n  7.4 Model Evaluation\n    - Model Accuracy\n    - Model Loss\n8. Conclusion","5461ed44":"# Model Building\nNow that we have our data fully processed and split into training and testing datasets, we can begin building a neural network to solve this classification problem. \n\n\n","234ba612":"## Model Evaluation\nEvaluating the model requires that you first choose a separate dataset used to evaluate the model. This should be data not used in the training process i.e. the X_test.\n\nNow, let us use the trained model to predict the probability values for the new data set - The test set we had initially created. The below code passes the X_test and y_test to the trained model and gives out the probability.","e25eb7fd":"# Introduction\nClassification is a process of dividing the given data points into two or more classes. When the number of classes = 2, it is known as Binary Classification.\n\nIn this notebook, we'll be performing Binary Classification to **predict whether a person has a heart disease or not.**\n\nThere are 2 classes:\n1. Heart disease present (target=1)\n2. Heart disease not present (target=0)","4f513007":"## Model Training\nThe model is initially trained for **200 epochs** with a **batch size of 10**. \nBoth epochs and batch size are hyperparameters that can be modified to optimise the model.\n\n**validation_split:** Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. \nThe val_loss and val_accuracy that you can see below are calculated with this validation data.\n\n\n**verbose:** Verbose is just for printing purposes, for making the output more readable.\n\n","60f456ec":"### History\nNotice that we're saving the trained model to a variable **history**. \n\nWhen running a model, Tensorflow Keras maintains a so-called History object in the background. This object keeps all loss values and other metric values in memory so that they can be used for visualizations.\n\nThe history object is the output of the fit operation.","d5fb9e6f":"# Conclusion\nIn this example, we developed a working Neural Network for the binary classification problem.\n\nThe model can be optimised further using hyperparameter tuning and other techniques.","c5aa58d0":"# Separating Input and Target Variable\n**Target Variable\/ Independent Variable(y):** Our objective is to detect the presence of heart disease. Thus, our target variable will be the column that indicates whether heart disease is present or not. \n\nWe'll store that column in a variable y.\n\n**Input Variables\/ Dependent Variables(X):** The remaining columns, without the Target variable will be stored in variable X.","0fcd3766":"### Model Loss","7996e331":"# Data Description\n* **age:** Age in years\n* **sex:** 1 = male, 0 = female\n* **cp:** Chest pain type\n* **trestbps:** Resting blood pressure (in mm Hg on admission to the hospital)\n* **chol:** serum cholesterol in mg\/dl\n* **fbs:** fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n* **restecg:**Resting electrocardiographic results\n* **thalach:** Maximum heart rate achieved\n* **exang:** Exercise induced angina (1 = yes; 0 = no)\n* **oldpeak:** ST depression induced by exercise relative to rest\n* **slope:** The slope of the peak exercise ST segment\n* **ca:** Number of major vessels (0-3) colored by fluoroscopy\n* **thal:** 3 = normal; 6 = fixed defect; 7 = reversible defect\n* **target:** 1 = Heart disease present, 0 = Heart disease not present","426a1131":"### Our model architecture\nFor our model, we'll be considering the following:\n\n* Input = the no. of features in X_train = 13\n* No. of neurons\/units in first Dense layer = 32\n* No. of neurons\/units in second Dense layer = 16\n* No. of neurons\/units in third Dense layer = 8\n* No. of neurons\/units in output layer = 1\n\nIf you closely look at it, we're slowly decreasing the number of neurons in each layer. Deciding the no. of hidden layers and no. of neurons is a process of trial and error. \n\nIf you're interested in understanding the intuition behind it better, you can go through the following article: [Beginners Ask \u201cHow Many Hidden Layers\/Neurons to Use in Artificial Neural Networks?\u201d](https:\/\/towardsdatascience.com\/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e)","deb5dcaf":"* The above code compiles the network.\n\n* The **loss function** used is **binary_crossentropy**. For binary classification problems that give output in the form of probability, binary_crossentropy is usually the optimizer of choice.\n\n*  It uses **rmsprop** as an **optimizer**. \n\n* The **learning rate** is taken to be **0.001**. You can even try different values to see which works the best.It is important to find a good value for the learning rate for your model on your training dataset. \n\n  We cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a good (or good enough) learning rate must be discovered via trial and error.\n\n  The range of values to consider for the learning rate is less than 1.0 and greater than $10^{-6}$.\n\n  A traditional default value for the learning rate is 0.1, 0.01 or 0.001, and this may represent a good starting point on your problem.\n\n* **Metrics** used to evaluate the model is **accuracy**. Accuracy calculates how often the predictions calculated by the model are correct."}}