{"cell_type":{"f85b88a3":"code","3cb735b6":"code","ee61516c":"code","e7ac90d3":"code","7ddb549c":"code","548e43c9":"code","5ef98af6":"code","6f16af45":"code","55ced756":"code","58e9500c":"code","6f8a6aa3":"code","198838ed":"code","97fe4dfe":"code","f1decda3":"code","125f1362":"code","58475244":"code","cea9aa76":"code","2424e236":"code","2e945f7d":"code","64fb405a":"code","91f96932":"code","881dd634":"code","162aa718":"code","58771fb4":"code","ca8ced8b":"markdown","4207389a":"markdown","12b07946":"markdown","a177b99c":"markdown","313519e8":"markdown","5fc2732b":"markdown","16c0c3c7":"markdown","c9c92f7c":"markdown","609e6e8e":"markdown","2ea14f92":"markdown","1c29a347":"markdown","a5b11b95":"markdown","0cd7b95c":"markdown","c11041c6":"markdown","b9e55646":"markdown","e900ca56":"markdown","64252da3":"markdown","702651ab":"markdown","00736fa6":"markdown","5b65963c":"markdown","addd83a9":"markdown","2a18a159":"markdown","0163d4fc":"markdown","cdeefe88":"markdown","25d0e091":"markdown","12e2813c":"markdown","d8875491":"markdown","78e748a3":"markdown","919660b9":"markdown","3946d708":"markdown","63d636cb":"markdown","836ac0cd":"markdown","1f963926":"markdown","13af2c54":"markdown","8a287a9c":"markdown","f64d2fcb":"markdown","4e007e54":"markdown","3e31128a":"markdown","da7b4da4":"markdown","22603387":"markdown","b9240e6f":"markdown","b3b2c291":"markdown","46ba9dfa":"markdown","077ce96b":"markdown","b15ddf0c":"markdown","f968860b":"markdown","8258005e":"markdown","24843f61":"markdown","73d1d21f":"markdown","4763d71c":"markdown","041bb01c":"markdown","15fd0f22":"markdown","bd8c1441":"markdown","3edf26b5":"markdown","803010ce":"markdown","1b4ec172":"markdown","ffa3d41f":"markdown","5db09006":"markdown","aa449f9d":"markdown","28886c6d":"markdown","3f7398a4":"markdown","d4374635":"markdown","ed16dd2e":"markdown","b7a2480b":"markdown","02d620e0":"markdown","357a1030":"markdown","b965bd5c":"markdown"},"source":{"f85b88a3":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\npd.options.display.max_columns = None\npd.set_option('display.float_format', lambda x: '%.6f' % x)\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nplt.style.use('ggplot')\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom scipy.stats import skew","3cb735b6":"#\"kaggle\" prefix to avoid confusion with train-test splits we'll generate later for cross-validation\nkaggle_train = pd.read_csv('..\/input\/train.csv') \nkaggle_test = pd.read_csv('..\/input\/test.csv')\n\n#merging the two dataframes in one\ndf = pd.concat([kaggle_train, kaggle_test]).reset_index()","ee61516c":"noSalePrice = [x for x in df.columns if x!='SalePrice']\ndf[noSalePrice] = df[noSalePrice].fillna(df[noSalePrice].mean())","e7ac90d3":"fig, ax = plt.subplots(1, 3, figsize=(17,5))\nsns.scatterplot('GrLivArea', 'SalePrice', data=df[:1460], ax=ax[0]); #after 1460 no more SalePrice values anymore\nsns.scatterplot('OverallQual', 'SalePrice', data=df[:1460], ax=ax[1]);\nsns.scatterplot('TotalBsmtSF', 'SalePrice', data=df[:1460], ax=ax[2]);\nax[1].set_title('Before Removing Outliers');\n\nfor a in ax:\n    a.yaxis.label.set_visible(False)\n    a.get_yaxis().set_visible(False)","7ddb549c":"df = df[~df['Id'].isin([524, 1299])] #getting rid of outliers, situated at Ids 524 and 1299\n\nfig, ax = plt.subplots(1, 3, figsize=(17,5))\nsns.scatterplot('GrLivArea', 'SalePrice', data=df[:1458], ax=ax[0]);\nsns.scatterplot('OverallQual', 'SalePrice', data=df[:1458], ax=ax[1]);\nsns.scatterplot('TotalBsmtSF', 'SalePrice', data=df[:1458], ax=ax[2]);\nax[1].set_title('After Removing Outliers');\n\nfor a in ax:\n    a.yaxis.label.set_visible(False)\n    a.get_yaxis().set_visible(False)","548e43c9":"fig, ax = plt.subplots(1, 2, figsize=(17,5))\nsns.distplot(df[:1458]['SalePrice'], ax=ax[0]);\nax[0].set_title('Before Log Transformation');\n\n#log transformation\ndf.loc[df.SalePrice.notnull(), 'SalePrice_LOG'] = np.log1p(df.loc[df.SalePrice.notnull(), 'SalePrice']) \n\nsns.distplot(df[:1458]['SalePrice_LOG'], ax=ax[1])\nax[1].set_title('After Log Transformation');","5ef98af6":"#TRANSFORMING SALEPRICE AND GETTING RID OF SALEPRICE_LOG, JUST NEEDED FOR THE GRAPHS\ndf['SalePrice'] = np.log1p(df['SalePrice'])\nif('SalePrice_LOG' in df.columns): df.drop('SalePrice_LOG', axis=1, inplace=True)","6f16af45":"fig, ax = plt.subplots(1, 3, figsize=(17,5))\nsns.scatterplot('GrLivArea', 'SalePrice', data=df, ax=ax[0]);\nsns.scatterplot('OverallQual', 'SalePrice', data=df, ax=ax[1]);\nsns.scatterplot('TotalBsmtSF', 'SalePrice', data=df, ax=ax[2]);\nax[1].yaxis.label.set_visible(False)\nax[2].yaxis.label.set_visible(False)","55ced756":"dfc = df[['SalePrice', 'GrLivArea', 'OverallQual', 'TotalBsmtSF']].corr()\ndfc[['SalePrice']][1:]","58e9500c":"from patsy import dmatrices\nY, X = dmatrices('SalePrice ~ OverallQual+GrLivArea+TotalBsmtSF', df, return_type='dataframe')\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif","6f8a6aa3":"#training dataset\nX = [[1, 2], [3, 4]]\nY = [5, 6]\n\n#defining and training the machine in two lines of code\nmodel = linear_model.LinearRegression()\nmodel.fit(X, Y);\n\n#from this point, the machine can compute a Y value every time it is given some X values\n#here, with 5 and 6 as inputs, the machine predicts the output will be 7 \nmodel.predict([[5,6]])[0]","198838ed":"X = df[:1458][['GrLivArea']] #independant variables\nY = df[:1458]['SalePrice'] #dependant variable\nmodel = linear_model.LinearRegression() #LinearRegression() = Least Square Algorithm\nmodel.fit(X, Y); #fitting, or \"training\" the model","97fe4dfe":"plt.rcParams['figure.figsize'] = (12.0, 6.0)\nsns.scatterplot('GrLivArea', 'SalePrice', data=df); \n\nYpredicted = model.predict(X)\nsns.lineplot(X.iloc[:, 0].tolist(), Ypredicted, color='black', label='Least Squares Line'); \n\nYinvented = X.iloc[:, 0].as_matrix() * 0.00145 + 10\nsns.lineplot(X.iloc[:, 0].tolist(), Yinvented, color='blue', label='My Custom Line');","f1decda3":"ind_vars = ['GrLivArea', 'OverallQual', 'TotalBsmtSF']\nXtrain, Xtest, Ytrain, Ytest = train_test_split(df[:1458][ind_vars], df[:1458]['SalePrice'], test_size=0.33)","125f1362":"model = LinearRegression()\nmodel.fit(Xtrain, Ytrain);\nYpredicted = model.predict(Xtest)\ndff = pd.DataFrame({\"The values our model predicted\":np.expm1(Ypredicted), \n              \"The values it should have predicted, assuming it was perfect\":np.expm1(Ytest.tolist())})\ndff.astype(int).head()","58475244":"def adjusted_r2(Xtest, r2):\n\n    p = len(Xtest.columns) #number of independant values\n    n = len(Xtest) #length of test dataset\n    adj_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\n    return adj_r2","cea9aa76":"r2 = r2_score(Ytest, Ypredicted)\nadj_r2 = adjusted_r2(Xtest, r2)\nmse = mean_squared_error(Ytest, Ypredicted)\npd.DataFrame(data=[r2, adj_r2, mse], index=['r2', 'r2_adj', 'mse'])","2424e236":"X = df[:1458][ind_vars]\nY = df[:1458]['SalePrice']\nmse = -model_selection.cross_val_score(model, X, Y, cv=10, scoring='neg_mean_squared_error').mean()\nround(mse, 4)","2e945f7d":"#extracting numeric independant variables\ndfc = df[:1458].corr()[['SalePrice']].sort_values('SalePrice', ascending=False) \nnum_cols = dfc.drop(['SalePrice', 'Id']).index \n\n# df_scaled = pd.DataFrame(scaler.fit_transform(df[num_cols].astype(float)), columns=num_cols) #scaling the numeric columns\nY = df[:1458]['SalePrice']\nX = df[:1458][num_cols]\nXtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.33, random_state=42)","64fb405a":"#TRAINING MODELS\nlinear_model = LinearRegression().fit(Xtrain, Ytrain)\nridge_model = RidgeCV(alphas=[0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 200], normalize=True).fit(Xtrain, Ytrain)\nlasso_model = LassoCV(alphas=[0.00001, 0.0001, 0.001, 0.01], normalize=True).fit(Xtrain, Ytrain)\nelastic_model = ElasticNetCV(alphas= [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.02], normalize=True).fit(Xtrain, Ytrain)\n\n#TESTING PERFORMANCES\nprint('Linear  | mse :', round(mean_squared_error(Ytest, linear_model.predict(Xtest)), 5))\nprint('Ridge   | mse :', round(mean_squared_error(Ytest, ridge_model.predict(Xtest)), 5), '| alpha :', ridge_model.alpha_)\nprint('Lasso   | mse :', round(mean_squared_error(Ytest, lasso_model.predict(Xtest)), 5), '| alpha :', lasso_model.alpha_)\nprint('Elastic | mse :', round(mean_squared_error(Ytest, elastic_model.predict(Xtest)), 5), '| alpha :', elastic_model.alpha_)","91f96932":"# FUNCTION TO RETURN THE ACCURACY OF THE REGRESSION, THROUGH MEAN SQUARED ERROR\ndef test_regression(model, X, Y): \n    return round(np.sqrt(-model_selection.cross_val_score(model, X, Y, cv=5, scoring='neg_mean_squared_error')).mean(), 5)","881dd634":"# FUNCTION TO CREATE ALL COMBINATIONS OF CORRELATION, SKEWNESS AND MODELS\n # AND STOCKING RESULTS IN LIST OF DICTS\ndef tests(X, Y, test_dicts, include_dummies):\n\n    #LOOPING THROUGH CORRELATIONS THRESHOLDS\n    for i in corr_list:\n\n        #getting only ind. variables that have over the requested correlation with dep. variable\n        not_correlated_vars = correlations[correlations.iloc[:,0]<i].index\n        cols_to_keep = [x for x in X.columns if x not in not_correlated_vars]\n        X2 = X[cols_to_keep]\n\n        #LOOPING THROUGH SKEWNESS THRESHOLDS\n        for i2 in skew_list:\n\n            X3 = X2.copy()\n            #getting only ind. variables that have over the requested skewness\n            skewed_feats = skewness[skewness.iloc[:,0]>i2].index\n            skewed_feats = [x for x in skewed_feats if x in X3.columns]\n            X3[skewed_feats] = np.log1p(X3[skewed_feats])\n            \n            #stocking relevant information into dict\n            param_dict = {'correlation_threshold':str(i), 'skewness_thresold':str(i2), 'number_of_variables':X3.shape[1], 'include_dummies':include_dummies}\n\n            #TESTING THE FOUR MODELS FOR EACH COMBINATION OF CORRELATION\/SKEWNESS\n                #AND APPENDING THEIR SCORE TO LIST OF DICTS, WITH INFORMATION ASSOCIATED\n            model = LinearRegression()\n            #z in zscore only to have column at end of dataframe\n            test_dicts.append({**{'zscore':test_regression(model, X3, Y), 'model':'linear'}, **param_dict})\n\n            model = RidgeCV(alphas=[0.01, 0.1, 0.5, 1, 5, 10, 50, 100, 200], normalize=True)\n            test_dicts.append({**{'zscore':test_regression(model, X3, Y), 'model':'ridge'}, **param_dict})\n\n\n            model = LassoCV(alphas=[0.00001, 0.0001, 0.001, 0.01], tol=0.1, normalize=True)\n            test_dicts.append({**{'zscore':test_regression(model, X3, Y), 'model':'lasso'}, **param_dict})\n\n\n            model = ElasticNetCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1], tol=0.1, normalize=True)\n            test_dicts.append({**{'zscore':test_regression(model, X3, Y), 'model':'elastic'}, **param_dict})","162aa718":"test_dicts = []\n\n#defining correlation and skewness threshold\ncorr_list = [0, 0.25, 0.5]\nskew_list = [0, 0.5, 0.75, 0.9, 999] #999 threshold is same as skewing no variable\n\n#computing skewness and correlation of ind.variables\nskewness = df[num_cols].apply(lambda x: skew(x)).abs().to_frame() #skewness can be evaluated on both train and test\ncorrelations = df[:1458].corr().abs().sort_values('SalePrice', ascending=False)['SalePrice'].to_frame()[1:].drop('Id')\n\n#RUNNING THE FIRST TESTS, WITHOUT CATEGORICAL VARIABLES\ntests(X, Y, test_dicts, include_dummies='no')\n\n\n#RUNNING THE SECOND SET OF TESTS, INCLUDING CATEGORICAL VARIABLES\ndf_dummies = pd.get_dummies(df)\nX = df_dummies[:1458].drop(['SalePrice', 'Id'], axis=1)\ntests(X, Y, test_dicts, include_dummies='yes')\n\n#computing the results dataframe\nresults = pd.DataFrame(test_dicts)\nresults.sort_values('zscore').head()","58771fb4":"#RECREATING BEST SCENARIO\nXtrain = df_dummies[:1458].drop(['SalePrice'], axis=1) \nYtrain = df[:1458]['SalePrice']\nXtest = df_dummies[1458:].drop(['SalePrice'], axis=1) \n\nskewed_feats = skewness[skewness.iloc[:,0]>0.9].index\nskewed_feats = [x for x in skewed_feats if x in Xtest.columns]\nXtest[skewed_feats] = np.log1p(Xtest[skewed_feats])\nXtrain[skewed_feats] = np.log1p(Xtrain[skewed_feats])\n\nmodel = LassoCV(alphas=[0.00001, 0.0001, 0.001, 0.01], tol=0.1, normalize=True)\n\n#FITTING AND PREDICTING\nmodel.fit(Xtrain, Ytrain)\npredicted_prices = np.expm1(model.predict(Xtest))\n\n#SUBMITTING\nmy_submission = pd.DataFrame({'Id': Xtest.Id, 'SalePrice': predicted_prices}) \nmy_submission.to_csv('submission.csv', index=False)","ca8ced8b":"Another important stat is the <b> Mean Squared Error <\/b>. It is a representation of the distance between the points and the regression line. So the lower the value, the better. It has an absolute value so it's better used when comparing models in-between themselves. ","4207389a":"## Linear relationship","12b07946":"## Cross-validation","a177b99c":"Regularized models are meant to avoid Overfitting. But push them too far and you'll end up with Underfitting. So before we dive into each model indivdually, we must understand both concepts.<br>\n\nOverfitting means that your regression will probably fit well to the data you are working on, but as soon as you bring in yet unseen values, your prediction will lose accuracy. Indeed, if the dataset contains too many \"weird\" values or outliers for instance, an overfitted regression line will try to be the closest possible to these points though they are a not a good representation of the dataset.The regression will then not be able to predict new points that <i>do<\/i> represent the dataset. <br>\n<i> If there is noise in the training data, then the estimated coefficients won\u2019t generalize well to the future data. <\/i><br>\n<br>\nBelow a schema that personnaly made me quickly understand the concepts.","313519e8":"- <b>Numerical variables<\/b>\n- <b>Linear relationship<\/b> between independent and dependent variables\n- <b>No outliers<\/b>\n- <b>Normal distribution<\/b>\n- <b>No multicollinearity<\/b>\n- <b>No autocorrelation<\/b> \n- <b>Homoskedasticity <\/b><br>\n\nLet's go through all the conditions one by one","5fc2732b":"Let's have a shot at testing these main statistics on our predictions.","16c0c3c7":"First things first! In this section we intent to resume all the conditions needed to run a Linear Regression. <br>\nFor this section - and to be as clear as possible - we'll stick with the independant variables Overall Qual, GrLivArea and TotalBsmtSF.","c9c92f7c":"## Homoskedasticity","609e6e8e":"## Mini-preprocessing","2ea14f92":"Now that we have our variables, let's go back to fitting & predicting","1c29a347":"First of all, note that we are studying Linear Relationship <i> after <\/i> having log-transformed SalePrice and <i> after <\/i> having removed outliers. That's important to know as both those methods can increase the linearity of some relationships.<br>","a5b11b95":"## Train & Test","0cd7b95c":"Remember the prediction machine we trained earlier?\nWell, we did something quite stupid doing that : we gave it all the SalePrice values we had at disposal. So now, there's no way of testing it on a new set of data and see if it performs well.<br>\n\nIndeed, if we want to test the accuracy of the model, we need to set a part of the dataframe aside before training it. So we'll have two sets :\n - Train : the data that will train the machine\n - Test : the \"untouched\" data that will be used to test the machine's accuracy. (Note that we can't use Kaggle's \"test\" dataset, because we don't know its SalePrice values)\n\nIn other words, once you have trained a machine with dataset \"Train\", you'll use dataset \"Test\" to put it to the test.<br>\nYou'll pretend to the machine you know nothing about the Ys from \"Test\" it is supposed to predict. So once it has predicted them, you bring in the real YTest values and compare them with the predicted values : you'll then know if it's a good machine or not.<br><br>\nLet's create our own Train and Test, thanks to the train_test_split method, which comes in quite handy.<br>\nIt will create, in one line of code, both datasets, and extract independant and dependant values from them.\n","c11041c6":"From the graphs we saw above, we can say heteroskedasticity was not too important : we don't have to remove any of the independant variables.","b9e55646":"## Importing data","e900ca56":"## Numerical variables","64252da3":"\"Outliers\" can be considered as the weird values, the observations that don't represent the general trend, the datapoints that stand alone vs. the mass of all others. We need to get rid of them because they represent an exception rather an a rule. <br>\nTo spot them, scatterplot the independant values vs. the dependant value.","702651ab":"### VIF to check multicolinearity","00736fa6":"It seems obvious... and it is. \nSo let's quickly jump to the next one.","5b65963c":"Here we go! Our best result turns out to be : using Lasso on all variables, including dummies, and log-transforming only independant variables that show more than 90% of skweness.<br>","addd83a9":"Graphically, the chosen variables do show some linear relations.<br>\nA good rule of thumb is to chose only variables that have a correlation score (r2) of over 0.5 with the independant variable.","2a18a159":"<img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2017\/06\/05210948\/overunder1.png\"\/>","0163d4fc":"Concept of multicollinearity is pretty intuitive : if your independant variables show some correlations in-between themselves, then you are facing multicollinearity. <br>","cdeefe88":"# Introduction","25d0e091":"Homoskedasticity is the fact of having equal residuals across the regression line. <br>\nHomoskedasticity and heteroskedasticity can be understood in one graphic.\n<a href=\"https:\/\/ibb.co\/DWq4R9v\"><img src=\"https:\/\/i.ibb.co\/3dJ1Cpj\/homoskedasticity.png\" alt=\"homoskedasticity\" border=\"0\" style=\"height: 200px;\"><\/a>","12e2813c":"### Overfitting & Underfitting","d8875491":"# Assumptions","78e748a3":"Now that we know all our assumptions are respected, we can start to predict.","919660b9":"Let's predict SalePrice with these conditions and submit our results.<br>\nHope you liked the kernel!","3946d708":"## No multicollinearity","63d636cb":"### Regularized Models","836ac0cd":"## Importing modules","1f963926":"Here, no variables should be dropped as all features have small Variance Inflation Factors.","13af2c54":"One last thing to know before we define the models : each of them take alphas as inputs. These values define how \"intense\" are the regularizations going to be. If the value is not relevant, we can end up either with overfitting or underfitting.<br>\nUsing SkLearn's RidgeCV() instead of Ridge() allows to input a range of alphas instead of a single one; the method will then select itself the best one and can tell you which one it chose with the alpha_ attribute.<br>\nI came up with the below alphas \"manually\" : I tried a random range, ran the methods, got the alpha_ attribute. If the alpha_ was a maximum or a minimum of my range, I added values to my range (higher in case of maximum, lower in case of minimum).<br>\nThe best method though is probably the one exposed in this great [kernel](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) : you loop through a range of alphas and plot a line, and pick the alpha where the line is the lowest.","8a287a9c":"How is the machine designed? Many methods exist and we'll review some of them later in the Kernel, but for now we'll only cover the most commonly used method : Least Squares.<br>\nLet's apply it to the House Prices dataset, by predicting SalePrice from GrLivArea.","f64d2fcb":"## Statistics","4e007e54":"What the OLS does is it computes the regression line so that there is no other \npossible line out there that can result in a lower sum of squared residuals than the OLS line. <br>\nHere you can instictively see that the total distance \"points -> blue line\" is superior to \"points -> black line\". You can modify the blue line's equation with any intercept and coefficient you want, you'll never get a smaller distance than to the \"points -> black line\" distance. That's what OLS does.<br>","3e31128a":"## Scenario testing","da7b4da4":"From the five rows above, some predicted values don't seem too far away from the actual values. But how can we quantify the overall proximity?","22603387":"## No outliers","b9240e6f":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Importing-modules\" data-toc-modified-id=\"Importing-modules-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Importing modules<\/a><\/span><\/li><li><span><a href=\"#Importing-data\" data-toc-modified-id=\"Importing-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Importing data<\/a><\/span><\/li><li><span><a href=\"#Mini-preprocessing\" data-toc-modified-id=\"Mini-preprocessing-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Mini-preprocessing<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Assumptions\" data-toc-modified-id=\"Assumptions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Assumptions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-variables\" data-toc-modified-id=\"Numerical-variables-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Numerical variables<\/a><\/span><\/li><li><span><a href=\"#No-outliers\" data-toc-modified-id=\"No-outliers-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>No outliers<\/a><\/span><\/li><li><span><a href=\"#Normal-distribution\" data-toc-modified-id=\"Normal-distribution-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Normal distribution<\/a><\/span><\/li><li><span><a href=\"#Linear-relationship\" data-toc-modified-id=\"Linear-relationship-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Linear relationship<\/a><\/span><\/li><li><span><a href=\"#No-multicollinearity\" data-toc-modified-id=\"No-multicollinearity-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>No multicollinearity<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#VIF-to-check-multicolinearity\" data-toc-modified-id=\"VIF-to-check-multicolinearity-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;<\/span>VIF to check multicolinearity<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#No-autocorrelation\" data-toc-modified-id=\"No-autocorrelation-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;<\/span>No autocorrelation<\/a><\/span><\/li><li><span><a href=\"#Homoskedasticity\" data-toc-modified-id=\"Homoskedasticity-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;<\/span>Homoskedasticity<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Fitting-&amp;-Predicting\" data-toc-modified-id=\"Fitting-&amp;-Predicting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Fitting &amp; Predicting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#The-concept\" data-toc-modified-id=\"The-concept-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>The concept<\/a><\/span><\/li><li><span><a href=\"#A-basic-algorithm-:-Least-Squares\" data-toc-modified-id=\"A-basic-algorithm-:-Least-Squares-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>A basic algorithm : Least Squares<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Testing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Train-&amp;-Test\" data-toc-modified-id=\"Train-&amp;-Test-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Train &amp; Test<\/a><\/span><\/li><li><span><a href=\"#Statistics\" data-toc-modified-id=\"Statistics-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Statistics<\/a><\/span><\/li><li><span><a href=\"#Cross-validation\" data-toc-modified-id=\"Cross-validation-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Cross-validation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Seeking-Performance\" data-toc-modified-id=\"Seeking-Performance-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Seeking Performance<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Regularizing\" data-toc-modified-id=\"Regularizing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Regularizing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Overfitting-&amp;-Underfitting\" data-toc-modified-id=\"Overfitting-&amp;-Underfitting-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;<\/span>Overfitting &amp; Underfitting<\/a><\/span><\/li><li><span><a href=\"#Regularized-Models\" data-toc-modified-id=\"Regularized-Models-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;<\/span>Regularized Models<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Scenario-testing\" data-toc-modified-id=\"Scenario-testing-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Scenario testing<\/a><\/span><\/li><li><span><a href=\"#Submitting\" data-toc-modified-id=\"Submitting-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Submitting<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","b3b2c291":"## Regularizing","46ba9dfa":"We have two datasets at our disposal : \n- train.csv. In this dataset, we know the values of SalePrice, and we'll use it to build our predicition models.\n- test.csv. Here the values of SalePrice are hidden, only the guys at Kaggle know them - we'll probably never get to know their exact values. However we'll use what we have learned from the train dataset to try to predict them. We'll then submit our results and Kaggle will tell us if we did good or not.<br><br>\nWe could only exercice on the train dataset, however it will be easier to merge the train and the test dataset, for two reasons :\n- The test dataset still has some precious information on independant variables (all the variables except SalePrice) that could help us improve our prediction models. The more data the better.\n- We'll need to make some transformations on the independant variables : doing them on both datasets at the same time will save us some time.","077ce96b":"Lasso is performing slightly better than the classic Linear Regression.<br>","b15ddf0c":"## No autocorrelation","f968860b":"From the graphs above, we see 'GrLivArea' and 'OverallQual' graphs each show two outliers on their lower right side. They actually correspond to the same observations, let's get rid of them.","8258005e":"As we said, we'll spend minimum time on data cleaning. There are plenty of great Kernels out there that are really specific on this subject.<br> We still need to get rid of nan values though, we'll do this the easy way by replacing them with the mean of their column. Except for SalePrice of course.","24843f61":"There are a lot of factors influencing a regression's quality, and depending on the dataset you are working on, these factors will behave differently. One way to improve the prediction's accuracy is to test the regression by \"playing\" with these factors' parameters. Let's test a few scenarios here : \n<br>\n- Log transformation. \n    - log-transforming all ind. variables\n    - log-transforming only ind. variables showing over 50% of skewness\n    - log-transforming only ind. variables showing over 75% of skewness\n    - log-transforming none of the ind. variables\n- Correlation\n    - considering all numeric variables\n    - considering variables correlated with dependant variable at 0.3 threshold\n    - considering variables correlated with dependant variable at 0.5 threshold\n- Dummies\n    - considering continuous variables only\n    - considering continuous & categorical variables\n- Regression model\n    - Linear\n    - Ridge\n    - Lasso\n    - ElasticNet\n    \nWe'll test every possible combination. The code will be a bit harsh, but at the end we'll have a nice table presenting which combinations performed the best.","73d1d21f":"However the best way to test you model is using sklearn's method, cross_val_score. <br>\nIndeed, this method needs one line of code to :\n- Consider the model we want to use (in this case LinearRegression())\n- Make itself a train dataset with a 20% sample of X and Y\n- Fit the model with the train dataset\n- Test the model on the other 80%\n- Give us an accuracy score (based on a given statistic)\n\nMoreover, it will do that 10 times (assuming you define cv=10) and compute the mean.<br>\nSo for calculating mean squared error, it would look like this.","4763d71c":"## The concept","041bb01c":"## Submitting","15fd0f22":"So far so good, all variables show linear relationships with 'SalePrice'","bd8c1441":"The Least Squares algorithm intents to find the \"Line of Best Fit\" by minimizing the sum of the distances between each of the points and the regression line. Best way to understand this is with the help of a plot.<br>\nSo let's plot the Least Squares regression line and compare it with our own custom line that comes (almost) straight out of our imagination.","3edf26b5":"Now imagine the lines below, but applied to all the variables of your model. Some variables will tend to overfit more than others.<br>\nThis is where Regularization comes in and helps improve accuracy in two ways : \n- It considers each variable and is able to lower the coefficients of variables that overfit\n- It prevents from multicollinearity by lowering influence of variables already \"represented\" by others\n\n<br>\nOn the other hand, if regularization is too severe, we'll be in a situation of Underfitting, meaning that the Regression Line will be too \"generalized\" : by trying to satisfy any possible situation, it ends up satisfying none. It takes so little risk at predicting that it becomes useless.","803010ce":"In order to evaluate a regression model's accuracy, there are <a href = \"https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\"> many statistics <\/a> that can be tested. Let's have an overview of the main ones.<br>\n\nThe most common stat is <b>R-square<\/b>.\nRsquare \"is the percentage of the response variable variation that is explained by a linear model\". So if you get 0.8, you'll be able to brag claiming \"80% of SalePrice can be explained by my prediction machine\" (doesn't mean 80% of your predictions are exactly right though). It is easy to interpret as its values ranges from 0 to 1 : the closer to 1, the better the model. <br><br>\nHowever the problem with R-square is whenever you add more independant variable into the model, R-square always increases. So you can be tricked into thinking your model is getting better as its gets more complex, but it's not the case. To avoid that, you'll be better off using <b>Ajdusted R-square<\/b>, which takes into account the number of independant values. Sklearn does not have a built-in function to compute that, you'll find one below.","1b4ec172":"Best method to check for multicolinearity is Variance Inflation Factor  (VIF). The analysis gives a factor for each of the dependant variables : variables with values over 5 should be dropped.","ffa3d41f":"The independant variable must show a normal distribution, which is quickly checked with a histogram.\nHere on the left-hand side, we see the original SalePrice is not normally distributed as it is skewed on the left. So we fix this by running a log transformation to it, which gives us the right-hand side histogram.","5db09006":"# Seeking Performance","aa449f9d":"This tutorial intents to summarize the most essential things to know about linear regressions, how to apply them, what are the particular cases to watch out for and what to respond to them.\n\nIt is written in a didactic purpose rather than to seek performance in prediction. <br>\nTherefore we will spend minimum time on data cleaning in order to get straight to the point.\n<br><br>\n","28886c6d":"## A basic algorithm : Least Squares ","3f7398a4":"# Testing","d4374635":"Autocorrelation is a particular case that mainly appears with time series, which is not the case in this dataset.\nA pretty intuituive way to understand autocorrelation is with temperature data over days : a temperature on one day will likely be similar to the temperature on the day before (due to seasons). Therefore this data will be autocorelated as a value on day i can help you predict the value on the day i+1.","ed16dd2e":"Linear relationship implies you could imagine some kind of straight line going through the points when you plot them one against the other. <br>\nOn the other hand, if you see datapoints scattered all around the place, that's not good. <br>","b7a2480b":"## Normal distribution","02d620e0":"This is where the magic happens.<br>\n\"Fitting a model\" is like designing a machine that has one goal : make some predictions. It will transform some X values into Y values<br> \nHowever the machine cant' be designed from scratch, it first needs known X and Y values to train with. So from the moment you have some known X and Y values, you can design your very own predicting machine in two lines of codes : ","357a1030":"# Fitting & Predicting","b965bd5c":"There are different regularized models out there, main ones are : Ridge, Lasso and Elastic-Net.<br>\nMain differences are :\n - Ridge performs well on small datasets\n - Lasso and Elastic-Net have the capacity to bring coefficients down to 0. Meaning completely canceling the influence of some variables in the model.\n - Elastic-Net is good when you have a lot lot lot of independant variables\n \n<br>\nBecause regularized models have the ability to lower the coefficients of useless variables, you can throw all the independant variables in there, and let the models decide which ones are valuable. <br>\nMeaning that somehow, you can forget about the assumptions on multi-collinearity and linear relationships.\n<br>\nThen you can test them all the models using cross-validation and pick the one that gets the best results.\n"}}