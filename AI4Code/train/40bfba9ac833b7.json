{"cell_type":{"0e62840b":"code","5026dd48":"code","87361607":"code","2e51f8c0":"code","1f6a59e9":"code","33b697e2":"code","9314098f":"code","1ad4a050":"code","fb013fde":"code","a431b8f9":"code","992816fa":"code","170e7c28":"code","ae5c7f65":"code","057d94dd":"code","75b239fe":"code","73e3f984":"code","90e9cf2f":"code","3050877a":"code","2fdad891":"code","09e9e9cc":"code","ff02853d":"code","4490fc04":"code","0ec7da83":"code","52ef54d8":"code","a3fd1a02":"code","26eb9248":"code","0ce7e7f8":"code","315b3c78":"code","f2c2775c":"code","50bd8089":"code","7d531234":"code","a4f65df0":"code","d7751c41":"code","463a6895":"code","c62c006b":"code","723d9187":"code","2141678a":"code","0090ac4a":"code","6661d9d5":"code","fb8eef24":"code","d5a1a613":"code","2dab3c74":"code","8f5df0d0":"code","5eba592e":"code","5494f6a7":"code","dea58afa":"code","c3dc5620":"code","739f505d":"code","ea02de47":"code","739ce727":"markdown","4d5621eb":"markdown","f3ae9eb3":"markdown","30b7be8a":"markdown","6c8ef3fb":"markdown","aaf3a9a2":"markdown","6f423ffc":"markdown","356a36b3":"markdown","bfa13178":"markdown","59380897":"markdown"},"source":{"0e62840b":"import pandas as pd\nimport numpy as np \nimport json\nimport os\nimport re\nimport string \nimport unicodedata\nimport warnings \nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom wordcloud import WordCloud","5026dd48":"pd.set_option(\"display.max_colwidth\", 200)\n\n%matplotlib inline","87361607":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","2e51f8c0":"df.shape","1f6a59e9":"df.head()","33b697e2":"df.info()","9314098f":"df[df['target'] == 0].head(10)","1ad4a050":"df[df['target'] == 1].head(10)","fb013fde":"df[\"target\"].value_counts()","a431b8f9":"df['hashtags'] = df['text'].str.findall(r'#.*?(?=\\s|$)')\ndf['hashtags'] = df['hashtags'].apply(lambda x: [item for item in x if item != '#'])\ndf.head()","992816fa":"df['mentions'] = df['text'].str.findall(r'@.*?(?=\\s|$)')\ndf['mentions'] = df['mentions'].apply(lambda x: [item for item in x if item != '@'])\ndf.head()","170e7c28":"def remove_diacritics(text):\n    \"\"\"\n    Returns a string with all diacritics (aka non-spacing marks) removed.\n    For example \"H\u00e9ll\u00f4\" will become \"Hello\".\n    Useful for comparing strings in an accent-insensitive fashion.\n    \"\"\"\n    normalized = unicodedata.normalize(\"NFKD\", text)\n    return \"\".join(c for c in normalized if unicodedata.category(c) != \"Mn\")","ae5c7f65":"def preprocess(text):\n    space_pattern = '\\s+'\n    new_line = '\\n+'\n    giant_url_regex = ('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    non_word_char = '[^\\w ]|_'\n    html_and= '\\bamp\\b'\n    \n    parsed_text = re.sub(space_pattern, ' ', text)\n    parsed_text = re.sub(new_line, ' ', parsed_text)\n    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n    parsed_text = re.sub(non_word_char, ' ', parsed_text)\n    parsed_text = re.sub(r\"\\bamp\\b\", '', parsed_text)\n    \n    return parsed_text","057d94dd":"def deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","75b239fe":"df['tidy_tweet'] = np.vectorize(remove_diacritics)(df['text'])\ndf.head()","73e3f984":"df['tidy_tweet'] = np.vectorize(preprocess)(df['tidy_tweet'])","90e9cf2f":"df['tidy_tweet'] = np.vectorize(deEmojify)(df['tidy_tweet'])\ndf.head()","3050877a":"df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\ndf['tidy_tweet'] = df['tidy_tweet'].str.strip()","2fdad891":"df['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\ndf.head()","09e9e9cc":"from nltk.corpus import stopwords\nsw_nltk = stopwords.words('english')\n\ndf['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: [item for item in x if item not in sw_nltk])\ndf.head()","ff02853d":"df.drop(['text'], axis = 1, inplace = True)\ndf['tidy_tweet'] = df['tidy_tweet'].apply(lambda x: ' '.join([w for w in x]))\ndf.head()","4490fc04":"all_words = ' '.join([text for text in df['tidy_tweet']])\n\n\nwordcloud = WordCloud(font_path='..\/input\/arial-font\/arial.ttf', width=800, height=500, random_state=21,\n                      max_font_size=110, background_color = 'white', colormap = 'winter',\n                     max_words = 100).generate(all_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show();","0ec7da83":"normal_words =' '.join([text for text in df['tidy_tweet'][df['target'] == 0]])\n\n\nwordcloud = WordCloud(font_path='..\/input\/arial-font\/arial.ttf', width=800, height=500, random_state=21,\n                      max_font_size=110, background_color = 'white', colormap = 'winter',\n                     max_words = 100).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show();","52ef54d8":"disaster_words =' '.join([text for text in df['tidy_tweet'][df['target'] == 1]])\n\n\nwordcloud = WordCloud(font_path='..\/input\/arial-font\/arial.ttf', width=800, height=500, random_state=21,\n                      max_font_size=110, background_color = 'white', colormap = 'winter',\n                     max_words = 100).generate(disaster_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show();","a3fd1a02":"HT_normal = df['hashtags'][df['target'] == 0]\nHT_disaster = df['hashtags'][df['target'] == 1]\n\nHT_normal = sum(HT_normal,[])\nHT_disaster = sum(HT_disaster,[])","26eb9248":"a = nltk.FreqDist(HT_normal)\nd = pd.DataFrame({'Hashtag': list(a.keys()), 'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags\nd = d.nlargest(columns=\"Count\", n = 20)\nticks = d.Hashtag.tolist()\n\n    \nfig = px.bar(x=ticks, y=d['Count'], title='Normal hashtags', template='plotly_white')\nfig.update_layout(\n    xaxis = dict(title = 'Hashtags'),\n    yaxis = dict(title='Count'),)\nfig.show()","0ce7e7f8":"a = nltk.FreqDist(HT_disaster)\nd = pd.DataFrame({'Hashtag': list(a.keys()), 'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags\nd = d.nlargest(columns=\"Count\", n = 20)\nticks = d.Hashtag.tolist()\n\n    \nfig = px.bar(x=ticks, y=d['Count'], title='Disaster hashtags', template='plotly_white')\nfig.update_layout(\n    xaxis = dict(title = 'Hashtags'),\n    xaxis_tickangle=30,\n    yaxis = dict(title='Count'),)\nfig.show()","315b3c78":"M_normal = df['mentions'][df['target'] == 0]\nM_disaster = df['mentions'][df['target'] == 1]\n\nM_normal = sum(M_normal,[])\nM_disaster = sum(M_disaster,[])","f2c2775c":"a = nltk.FreqDist(M_normal)\nd = pd.DataFrame({'mentions': list(a.keys()), 'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags\nd = d.nlargest(columns=\"Count\", n = 20)\nticks = d.mentions.tolist()\n\n    \nfig = px.bar(x=ticks, y=d['Count'], title='Normal Mentions', template='plotly_white')\nfig.update_layout(\n    xaxis = dict(title = 'Mentions'),\n    yaxis = dict(title='Count'),)\nfig.show()","50bd8089":"a = nltk.FreqDist(M_disaster)\nd = pd.DataFrame({'mention': list(a.keys()), 'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags\nd = d.nlargest(columns=\"Count\", n = 20)\nticks = d.mention.tolist()\n\n    \nfig = px.bar(x=ticks, y=d['Count'], title='Disaster Mentions', template='plotly_white')\nfig.update_layout(\n    xaxis = dict(title = 'Mentions'),\n    yaxis = dict(title='Count'),)\nfig.show()","7d531234":"a = nltk.FreqDist(df['keyword'][df['target']==0])\nd = pd.DataFrame({'keyword': list(a.keys()), 'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags\nd = d.nlargest(columns=\"Count\", n = 20)\nticks = d.keyword.tolist()\n\n    \nfig = px.bar(x=ticks, y=d['Count'], title='Normal keywords', template='plotly_white')\nfig.update_layout(\n    xaxis = dict(title = 'Keywords'),\n    yaxis = dict(title='Count'),)\nfig.show()","a4f65df0":"a = nltk.FreqDist(df['keyword'][df['target']==1])\nd = pd.DataFrame({'keyword': list(a.keys()), 'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags\nd = d.nlargest(columns=\"Count\", n = 20)\nticks = d.keyword.tolist()\n\n    \nfig = px.bar(x=ticks, y=d['Count'], title='Normal keywords', template='plotly_white')\nfig.update_layout(\n    xaxis = dict(title = 'keywords'),\n    yaxis = dict(title='Count'),)\nfig.show()","d7751c41":"df['keyword'].fillna(' ', inplace=True)","463a6895":"def keyword_sentiment(khra):\n    count = df[df['keyword'] == khra]['target'].sum()\n    total = df[df['keyword'] == khra].shape[0]\n    return count\/total","c62c006b":"keyword_sentiment(\"accident\")","723d9187":"df['keyword_sentiment'] = np.vectorize(keyword_sentiment)(df['keyword'])\n\ndf.head()","2141678a":"df['keyword_sentiment'].hist();","0090ac4a":"df.drop(['keyword'], axis= 1, inplace = True)\ndf","6661d9d5":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.feature_selection import SelectPercentile, f_classif, chi2\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn import metrics\n\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nfrom sklearn.model_selection import train_test_split","fb8eef24":"tfidf_vectorizer = TfidfVectorizer(\n    use_idf=True,\n    decode_error='replace',\n    min_df=2,\n    max_df=0.9,\n    sublinear_tf = True\n    )\n\ntfidf = tfidf_vectorizer.fit_transform(df['tidy_tweet']).toarray()\ntfidf.shape","d5a1a613":"X_new_tfidf = SelectPercentile(chi2, percentile=30).fit_transform(tfidf, df['target'])\nX_new_tfidf.shape","2dab3c74":"transformer = MaxAbsScaler()","8f5df0d0":"X_new_tfidf = transformer.fit_transform(X_new_tfidf)","5eba592e":"# splitting data into training and validation set\nxtrain_tfidf, xtest_tfidf, ytrain, ytest = train_test_split(X_new_tfidf, df['target'], random_state=42, test_size=0.3)","5494f6a7":"print('training input :', xtrain_tfidf.shape[0])\nprint('training output :', ytrain.shape[0])\nprint('testing input :', xtest_tfidf.shape[0])\nprint('testing output :', ytest.shape[0])","dea58afa":"model = svm.SVC().fit(xtrain_tfidf, ytrain)\npred = model.predict(xtest_tfidf)","c3dc5620":"print(classification_report(ytest, pred))\nprint(confusion_matrix(ytest, pred))\nplot_confusion_matrix(model, xtest_tfidf, ytest, cmap = plt.cm.Blues);","739f505d":"model = LogisticRegression(max_iter = 4000).fit(xtrain_tfidf, ytrain)\npred = model.predict(xtest_tfidf)","ea02de47":"print(classification_report(ytest, pred))\nplot_confusion_matrix(model, xtest_tfidf, ytest, cmap = plt.cm.Blues);","739ce727":"## Keywords","4d5621eb":"# Modelling","f3ae9eb3":"## hashtags","30b7be8a":"# Imene KOLLI","6c8ef3fb":"# Data Visualization","aaf3a9a2":"# Data Pre-processing","6f423ffc":"## WordClouds","356a36b3":"## Mentions","bfa13178":"# Data Inspection","59380897":"## Location"}}