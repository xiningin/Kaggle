{"cell_type":{"6cb20c1d":"code","0196595e":"code","06f6b19f":"code","3fcff6f4":"code","cd1d3027":"code","9a0f8196":"markdown","08bcd5ff":"markdown","ce5fb46e":"markdown","76bbf6f5":"markdown","6397e14c":"markdown"},"source":{"6cb20c1d":"!pip install transformers","0196595e":"from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\ntokenizer = BertTokenizer.from_pretrained(\"uer\/gpt2-chinese-lyric\")\nmodel = GPT2LMHeadModel.from_pretrained(\"uer\/gpt2-chinese-lyric\")\ntext_generator = TextGenerationPipeline(model, tokenizer)   ","06f6b19f":"text_generator(\"\u6700\u7f8e\u7684\u4e0d\u662f\u4e0b\u96e8\u5929\uff0c\u662f\u66fe\u4e0e\u4f60\u8eb2\u8fc7\u96e8\u7684\u5c4b\u6a90\", max_length=100, do_sample=True)","3fcff6f4":"text_generator(\"\u6211\u6eab\u4e86\u4e00\u58fa\u9109\u6101\uff0c\u5c07\u5f80\u4e8b\u559d\u500b\u5920\", max_length=100, do_sample=True)","cd1d3027":"text_generator(\"\u90a3\u8584\u5982\u87ec\u7ffc\u7684\u672a\u4f86\uff0c\u7d93\u4e0d\u8d77\u8ab0\u4f86\u731c\", max_length=100, do_sample=True)","9a0f8196":"```\npython3 preprocess.py --corpus_path corpora\/lyric.txt \\\n                      --vocab_path models\/google_zh_vocab.txt \\\n                      --dataset_path lyric_dataset.pt --processes_num 32 \\\n                      --seq_length 512 --target lm\n```\n```\npython3 pretrain.py --dataset_path lyric_dataset.pt \\\n                    --pretrained_model_path models\/cluecorpussmall_gpt2_seq1024_model.bin-250000 \\\n                    --vocab_path models\/google_zh_vocab.txt \\\n                    --config_path models\/gpt2\/config.json \\\n                    --output_model_path models\/lyric_gpt2_model.bin \\\n                    --world_size 8 --gpu_ranks 0 1 2 3 4 5 6 7 \\\n                    --total_steps 100000 --save_checkpoint_steps 10000 --report_steps 5000 \\\n                    --learning_rate 5e-5 --batch_size 64 \\\n                    --embedding word_pos --remove_embedding_layernorm \\\n                    --encoder transformer --mask causal --layernorm_positioning pre \\\n                    --target lm --tie_weights\n```","08bcd5ff":"# GPT-2 Chinese Lyrics","ce5fb46e":"## Training data\n#### Training data contains 150,000 Chinese lyrics which are collected by [Chinese-Lyric-Corpus](https:\/\/github.com\/gaussic\/Chinese-Lyric-Corpus) and [MusicLyricChatbot](https:\/\/github.com\/liuhuanyong\/MusicLyricChatbot).","76bbf6f5":"## Training Procedure\n#### The model is pre-trained by [UER-py](https:\/\/github.com\/dbiir\/UER-py\/) on Tencent Cloud. We pre-train 100,000 steps with a sequence length of 512 on the basis of the pre-trained model gpt2-base-chinese-cluecorpussmall","6397e14c":"```\n\n```"}}