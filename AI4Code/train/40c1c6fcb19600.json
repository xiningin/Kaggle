{"cell_type":{"969a8957":"code","33241e11":"code","36d8dd15":"code","e9c26f11":"code","74bef32b":"code","c0722496":"code","d078c52c":"code","83bdfc7e":"code","69b7a2c8":"code","1992541a":"code","92412a5a":"code","837802d9":"code","056eb964":"code","965972da":"code","01e52f5a":"code","3bc2d362":"code","e8c94ffb":"code","a58fbc10":"code","55011d79":"code","9fea953b":"code","14286cc3":"code","ecc9426b":"code","db85c306":"code","37d199be":"code","2616f933":"code","713ab74d":"code","a36ded2b":"code","8fff0c84":"code","ef95ca60":"code","865d441f":"code","b9495330":"code","f2347598":"code","290bbe6f":"code","ae199688":"code","f535f437":"code","dd4c10f7":"code","93a06ac5":"code","3cbec6d3":"code","1987c3ac":"code","a0fb5778":"code","b25b2fc8":"code","0da5acd1":"code","4411a99d":"code","962f9d70":"code","f49b8a42":"code","ee5694cc":"code","70b97b95":"code","c52050a4":"code","8c236201":"code","3260faa3":"code","c0487e6a":"code","4f9b8aef":"code","3f89d142":"code","1a3d1238":"code","d4dea10c":"code","1305b471":"code","fe062a6f":"code","1aeb757b":"code","d169db77":"code","226fe68c":"code","a59e5978":"code","4622a3db":"code","fd82648e":"code","0fd21526":"code","60d089a3":"code","fc099bd8":"code","6e832fdc":"code","8594b7fc":"code","21ea5b31":"code","489ec32f":"code","d6e33cf8":"code","c738333d":"code","c940c8f3":"markdown","2a63e3de":"markdown","62fca372":"markdown","8a81b479":"markdown","7aaae983":"markdown","7366186a":"markdown","de7969bc":"markdown","68c44263":"markdown","e1706ebc":"markdown","ade236c6":"markdown","d0e53cc1":"markdown","ff2908be":"markdown","be7f8f63":"markdown","21d9f2b8":"markdown","ea0ce7af":"markdown","a3321e18":"markdown","6be85ad8":"markdown","ea4fde02":"markdown","bce50020":"markdown","273aba12":"markdown","44e912e4":"markdown","1c79c844":"markdown","90b66229":"markdown"},"source":{"969a8957":"\nimport numpy as np \nimport pandas as pd \n\n","33241e11":"path = \"..\/input\/eda-for-biginner-updated-to-english-ver\"","36d8dd15":"traindf = pd.read_csv(path+\"\/traindf.csv\")\ntraindf","e9c26f11":"tmp = traindf[traindf[\"landmark_id\"]==7]\ntmp","74bef32b":"import cv2\nimport matplotlib.pyplot as plt","c0722496":"for a in tmp[\"path\"]:\n    img = cv2.imread(a)\n    plt.figure()\n    plt.imshow(img)","d078c52c":"dfcnt = pd.read_csv(path+\"\/dfcnt.csv\")\ndfcnt","83bdfc7e":"plt.scatter(dfcnt[\"id\"],dfcnt[\"count\"])","69b7a2c8":"traindf","1992541a":"dfcnt","92412a5a":"tmp1 = dfcnt[\"id\"].iloc[0]\ntmp1","837802d9":"tmpdf1 = traindf[traindf[\"landmark_id\"]==tmp1]\ntmpdf1","056eb964":"tlist = []\nvlist = []","965972da":"tlist.append(tmpdf1.iloc[0].values)\ntlist","01e52f5a":"vlist.append(tmpdf1.iloc[1].values)","3bc2d362":"# \u3053\u308c\u3092\u7e70\u308a\u8fd4\u3059","e8c94ffb":"from tqdm import tqdm","a58fbc10":"import os\nos.path.exists(\".\/tdf.csv\")","55011d79":"tlist = []\nvlist = []","9fea953b":"if os.path.exists(\".\/tdf.csv\")==False:\n    \n\n    \n\n    tmp1 = dfcnt[\"id\"].values #.values\u3067numpy. for\u6587\u306fnumpy\u306e\u307b\u3046\u304c\u65e9\u3044\u3068\u304d\u304c\u3042\u308b\u3002\n\n    for a in tqdm(range(len(dfcnt))):\n\n        tmpdf1 = traindf[traindf.landmark_id.values==tmp1[a]]\n        tlist.append(tmpdf1.iloc[0].values)\n        vlist.append(tmpdf1.iloc[1].values)","14286cc3":"tdf = pd.DataFrame(tlist,columns=tmpdf1.columns)\ntdf[\"repair_id\"]=np.arange(0,len(tdf),1)\ntdf","ecc9426b":"vdf = pd.DataFrame(vlist,columns=tmpdf1.columns)\nvdf[\"repair_id\"]=np.arange(0,len(vdf),1)\nvdf","db85c306":"if os.path.exists(\".\/tdf.csv\"):\n    tdf = pd.read_csv(\".\/tdf.csv\")\n    vdf = pd.read_csv(\".\/vdf.csv\")","37d199be":"tdf.to_csv(\"tdf.csv\",index=False)\nvdf.to_csv(\"vdf.csv\",index=False)","2616f933":"tdf2 = tdf.iloc[:10,:]\nvdf2 = vdf.iloc[:10,:]","713ab74d":"tdf2","a36ded2b":"vdf2","8fff0c84":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision.models import resnet18\nfrom albumentations import Normalize, Compose\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\nimport os\nimport glob\nimport multiprocessing as mp\n\n\n\nif torch.cuda.is_available():\n    device = 'cuda:0'\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\nelse:\n    device = 'cpu'\nprint(f'Running on device: {device}')","ef95ca60":"preprocess = Compose([\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], p=1)\n])\n\n# resnext\u306a\u3069\u306epre-train\u30e2\u30c7\u30eb\u306f\u5168\u3066\u3001\u540c\u3058\u65b9\u6cd5\u3067\u6b63\u898f\u5316\u3055\u308c\u305f\u5165\u529b\u753b\u50cf\u3092\u4f7f\u7528\u3057\u306a\u3051\u308c\u3070\u306a\u3089\u306a\u3044\u3002\u305d\u308c\u306e\u5909\u63db\u3092\u3053\u306e\u95a2\u6570\u3067\u884c\u3046\u3002\u5024\u306fdefault\u3002\n# Compose\u306f\u4eca\u56de\u3042\u307e\u308a\u3001\u610f\u5473\u3092\u306a\u3055\u306a\u3044\n# https:\/\/betashort-lab.com\/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9\/albumentations%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81\/ \u306b\u8a73\u7d30\u306f\u66f8\u3044\u3066\u3042\u308b","865d441f":"# \u753b\u50cf\u3092\u3069\u308c\u3060\u3051\u5c0f\u3055\u304f\u3059\u308b\u304b\u306e\u51e6\u7406\nROWS = 32\nCOLS = 32","b9495330":"class GLDataset(Dataset):\n    \n    def __init__(self,img_pass,labels,preprocess=None):\n        self.img_pass = img_pass\n        self.labels = labels\n        self.preprocess = preprocess\n        \n    def __len__(self):\n        return len(self.img_pass)\n    \n    def __getitem__(self,idx):\n        \n        # \u3053\u3053\u304b\u3089dataset\u306b\u98df\u308f\u305b\u308b\u524d\u306e\u524d\u51e6\u7406\u306e\u8a18\u8ff0\u3002\n        \n        img_pass = self.img_pass[idx]\n        label = self.labels[idx]\n        \n        land = cv2.imread(img_pass)\n        land = cv2.resize(land,(ROWS,COLS),interpolation = cv2.INTER_CUBIC)\n        land = cv2.cvtColor(land,cv2.COLOR_BGR2RGB) # augment\u3092\u4f7f\u3046\u3068\u304d\u306bBGR\u304b\u3089RGB\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002\n        \n        if self.preprocess is not None: # \u3053\u3053\u3067\u3001\u524d\u51e6\u7406\u3092\u5165\u308c\u3066normalization\u3057\u3066\u3044\u308b\u3002\n                augmented = self.preprocess(image=land) # preprocess\u306eimage\u3092face\u3067\u8aad\u3080\n                land = augmented['image'] # https:\/\/betashort-lab.com\/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9\/albumentations%E3%81%AE%E3%81%BE%E3%81%A8%E3%82%81\/\u3000\u306b\u66f8\u3044\u3066\u3042\u308b\n                \n        return {'landmarks': land.transpose(2, 0, 1), 'label': np.array(label, dtype=int)}  # pytorch\u306fchannnl, x, y\u306e\u5f62\u3002\u3053\u308c\u306f\u8f9e\u66f8\u578b\u3067\u8fd4\u3057\u3066\u3044\u308b\u3002(\u6271\u3044\u3084\u3059\u3044\u3068\u3044\u3046\u3060\u3051\u304b\u3082\u3002)\n        \n        \n        \n        \n        \n        \n        \n        ","f2347598":"land = cv2.imread(tdf2[\"path\"].iloc[0])","290bbe6f":"plt.imshow(land)","ae199688":"land = cv2.resize(land,(ROWS,COLS),interpolation = cv2.INTER_CUBIC)","f535f437":"plt.imshow(land)","dd4c10f7":"land = cv2.cvtColor(land,cv2.COLOR_BGR2RGB) # augment\u3092\u4f7f\u3046\u3068\u304d\u306bBGR\u304b\u3089RGB\u306b\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u304b\u3082\u3057\u308c\u306a\u3044\u3002","93a06ac5":"plt.imshow(land)","3cbec6d3":"augmented = preprocess(image=land) # preprocess\u306eimage\u3092face\u3067\u8aad\u3080\nland = augmented['image']","1987c3ac":"plt.imshow(land)","a0fb5778":"land.shape","b25b2fc8":"land=land.transpose(2, 0, 1)","0da5acd1":"land.shape","4411a99d":"# instance\u5316\ntrain_dataset = GLDataset(\n    img_pass=tdf2[\"path\"],\n    labels=tdf2[\"repair_id\"].to_numpy(),\n    preprocess=preprocess\n)\n#val_dataset = FaceValDataset(\n\nval_dataset = GLDataset(\n    img_pass=vdf2[\"path\"],\n    labels=vdf2[\"repair_id\"].to_numpy(),\n    preprocess=preprocess\n)","962f9d70":"print(train_dataset[0])","f49b8a42":"BATCH_SIZE = 2\n\n#NUM_WORKERS = mp.cpu_count()\n#NUM_WORKERS = 0 # \u3053\u3053\u30920\u306b\u3057\u306a\u3044\u3068\u52d5\u304b\u306a\u3044\u3002cpu\u306e\u4ed5\u69d8\u500b\u6570\u3002\u2190\u5b9f\u306f\u52d5\u304f\u3053\u3068\u304c\u5224\u660e\u3002class\u306e\u4e2d\u8eab\u6b21\u7b2c\uff01","ee5694cc":"NUM_WORKERS = mp.cpu_count()\nNUM_WORKERS","70b97b95":"## DataLoader\u306fimport torch.utils.data.Dataset\u3067import\u6e08\u307f\u306e\u3082\u306e\ntrain_dataloader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False, #https:\/\/schemer1341.hatenablog.com\/entry\/2019\/01\/06\/024605 \u3092\u53c2\u8003. id\u304c\u308f\u304b\u3089\u306a\u304f\u306a\u308b\n    num_workers=NUM_WORKERS\n)\nval_dataloader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS\n)","c52050a4":"encoder = resnet18(pretrained = True)","8c236201":"class LandmarkClassifier(nn.Module):\n    \n    def __init__(self,encoder,in_channels=3,num_classes = len(dfcnt)):\n        \n        super(LandmarkClassifier,self).__init__()\n        # super().init__()\n        \n        self.encoder = encoder\n        \n        self.encoder.conv1 = nn.Conv2d(\n        \n        in_channels,\n            64,\n            kernel_size=7,\n            stride=2,\n            padding = 3,\n            bias = False      \n        \n        \n        )\n        \n        self.encoder.fc = nn.Linear(512*1,num_classes)\n        \n    def forward(self,x):\n        \n        return self.encoder(x)       \n        \n        \n    \n    \n    \n    \n    \n    \n    \n    ","3260faa3":"classifier = LandmarkClassifier(encoder = encoder,in_channels = 3, num_classes = len(dfcnt))","c0487e6a":"# classifier = classifier.to(device)\n\nclassifier.train()","4f9b8aef":"criterion = nn.CrossEntropyLoss()","3f89d142":"optimizer = optim.Adam(classifier.parameters(),lr=1e-6)","1a3d1238":"classifier.train()","d4dea10c":"train_dataloader[0]","1305b471":"for a in train_dataloader:\n    print(a)","fe062a6f":"for a in train_dataloader:\n    y_pred = classifier(a[\"landmarks\"])\n    print(y_pred)","1aeb757b":"for a in train_dataloader:\n    y_pred = classifier(a[\"landmarks\"])\n    \n    label = a[\"label\"]\n    \n    loss = criterion(y_pred,label)\n    print(loss)","d169db77":"for a in train_dataloader:\n    y_pred = classifier(a[\"landmarks\"])\n    \n    label = a[\"label\"]\n    \n    loss = criterion(y_pred,label)\n    print(loss.item())","226fe68c":"for a in train_dataloader:\n    y_pred = classifier(a[\"landmarks\"])\n    \n    label = a[\"label\"]\n    \n    loss = criterion(y_pred,label)\n    print(loss.item())\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    \n    ","a59e5978":"trainloss = []\n\nfor a in train_dataloader:\n    y_pred = classifier(a[\"landmarks\"])\n    \n    label = a[\"label\"]\n    \n    loss = criterion(y_pred,label)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \ntrainloss.append(loss.item())\n    \n    ","4622a3db":"trainloss","fd82648e":"def trainmodel(train_dataloader):\n    \n    classifier.train()\n \n    for a in train_dataloader:\n        y_pred = classifier(a[\"landmarks\"])\n\n        label = a[\"label\"]\n\n        loss = criterion(y_pred,label)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return(loss.item())\n    \n    ","0fd21526":"def valmodel(val_dataloader):\n    \n    classifier.eval()\n \n    for a in val_dataloader:\n        y_pred = classifier(a[\"landmarks\"])\n\n        label = a[\"label\"]\n\n        loss = criterion(y_pred,label)\n    \n    return(loss.item())\n    \n    ","60d089a3":"epochs = 10","fc099bd8":"trainloss = []\nvalloss= []\n\nfor a in tqdm(range(epochs)):\n   \n    trainloss.append(trainmodel(train_dataloader))\n    valloss.append(valmodel(val_dataloader))","6e832fdc":"x = np.arange(epochs)\nplt.scatter(x,trainloss)\nplt.scatter(x,valloss)","8594b7fc":"trainloss = []\nvalloss= []\n\nbestloss = None\nsavename = \"resnet18.pth\"\n\nfor a in tqdm(range(epochs)):\n   \n    trainloss.append(trainmodel(train_dataloader))\n    valloss.append(valmodel(val_dataloader))\n    \n    if bestloss is None:\n        bestloss = valloss[-1]\n        state = {\n            \"state_dict\":classifier.state_dict(),\n            \"optimizer_dict\":optimizer.state_dict(),\n            \"bestloss\" : bestloss\n        }\n        \n        torch.save(state,savename)\n        \n        print(\"save the first model\")\n        \n    elif valloss[-1] < bestloss:\n        bestloss = valloss[-1]\n        state = {\n            \"state_dict\":classifier.state_dict(),\n            \"optimizer_dict\":optimizer.state_dict(),\n            \"bestloss\" : bestloss\n        }\n        \n        torch.save(state,savename)\n        \n        print(\"found the better point\")\n    \n    else:\n        pass\n        \n    \n    \n    ","21ea5b31":"x = np.arange(epochs)\nplt.scatter(x,trainloss)\nplt.scatter(x,valloss)","489ec32f":"def savemodel(bestloss,valloss):\n    if bestloss is None:\n        bestloss = valloss[-1]\n        state = {\n            \"state_dict\":classifier.state_dict(),\n            \"optimizer_dict\":optimizer.state_dict(),\n            \"bestloss\" : bestloss\n        }\n        \n        torch.save(state,savename)\n        \n        print(\"save the first model\")\n        \n    elif valloss[-1] < bestloss:\n        bestloss = valloss[-1]\n        state = {\n            \"state_dict\":classifier.state_dict(),\n            \"optimizer_dict\":optimizer.state_dict(),\n            \"bestloss\" : bestloss\n        }\n        \n        torch.save(state,savename)\n        \n        print(\"found the better point\")\n    \n    else:\n        pass\n    \n    return bestloss","d6e33cf8":"trainloss = []\nvalloss= []\n\nbestloss = None\nsavename = \"resnet18.pth\"\n\nfor a in tqdm(range(epochs)):\n   \n    trainloss.append(trainmodel(train_dataloader))\n    valloss.append(valmodel(val_dataloader))\n    \n    bestloss = savemodel(bestloss,valloss)\n        \n    \n    \n    \n","c738333d":"x = np.arange(epochs)\nplt.scatter(x,trainloss)\nplt.scatter(x,valloss)","c940c8f3":"# \u5168\u90e8\u3084\u3063\u3066\u3082\u826f\u3044\u304c\u3001\u81ea\u5206\u3067\u4f5c\u6210\u3059\u308b\u3068\u304d\u306f10\u679a\u304f\u3089\u3044\u3067\u30c6\u30b9\u30c8\u3059\u308b\u307b\u3046\u304c\u52b9\u7387\u7684","2a63e3de":"# 1.\u524d\u56de\u30c7\u30fc\u30bf\u306e\u78ba\u8a8d","62fca372":"## 1\u30641\u3064\u8ffd\u3063\u3066\u3001\u4f55\u3084\u3063\u3066\u3044\u308b\u304b\u3092\u898b\u3066\u3044\u304f\u3002","8a81b479":"## dfcnt\u306eid\u3067filtering\u3057\u3066\u3001\u4e00\u756a\u4e0a\u306b\u304d\u305f\u3084\u3064\u3092train data, \u4e0a\u304b\u30892\u3064\u76ee\u3092validation\u3068\u3059\u308b\n## \u308f\u304b\u308a\u3084\u3059\u304f\u3059\u308b\u305f\u3081\u3001\uff11\u500b\u3067\u8aac\u660e","7aaae983":"# epoch\u3092\u56de\u3059\u3053\u3068\u3092\u8003\u3048\u308b\u3068\u3001loss\u3092\u4fdd\u5b58","7366186a":"## \u52b9\u7387\u7684\u306a\u3084\u308a\u65b9\n## Process 1 : Simple\u306b\u3057\u3066\u3067\u304d\u308b\u3068\u3053\u308d\u304b\u3089\u3084\u308b\u3002\u4f8b\u3048\u30701\u3064\u3060\u3051\u3084\u308b\n## Process 2 : \u3044\u3063\u305f\u3093\u307e\u3068\u3081\u3066\u307f\u308b\n## Process 3 : for\u6587\u306b\u3057\u3066\u56de\u3057\u3066\u307f\u308b\n## Process 4 : \u6c4e\u7528\u6027\u3092\u6301\u305f\u305b\u308b (\u6570\u5b57 \u2192 \u6587\u5b57\u5316)\n## Process 5 : \u3055\u3089\u306b\u6c4e\u7528\u6027\u3092\u6301\u305f\u305b\u308b (\u95a2\u6570\u5316)\n## Process 6 : class\u5316\u3059\u308b","de7969bc":"# \u304a\u3055\u3089\u3044","68c44263":"# \u3053\u306e\u30b5\u30a4\u30c8\u304c\u3068\u3066\u3082\u5206\u304b\u308a\u3084\u3059\u304f\u66f8\u3044\u3066\u304f\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u8ff7\u3063\u305f\u3089\u3053\u3053\u3092\u898b\u308b\nhttps:\/\/qiita.com\/takurooo\/items\/e4c91c5d78059f92e76d","e1706ebc":"# 1. transform\u306e\u5b9a\u7fa9","ade236c6":"# \u304a\u3055\u3089\u30442\u3000import collection\u3092\u4f7f\u3063\u3066\u3001\u5404id\u306e\u500b\u6570\u3092\u6570\u3048\u305f\u3002\u305d\u308c\u3092count\u6570\u3054\u3068\u306b\u4e26\u3079\u305f\u306e\u304c\u3001dfcnt","d0e53cc1":"# 2. Dataset","ff2908be":"# \u6b63\u89e3\u3068\u4e88\u6e2c\u306e loss\u3092\u6c42\u3081\u308b","be7f8f63":"# \u3053\u306e\u6642\u70b9\u3067\u3001landmark id\u306e\u7a2e\u985e\u306f\u5168\u90e8\u306781313\u500b\u3001\u6700\u5c0f\u679a\u6570\u306f2\u679a\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308b(\u524d\u56de\u306f138982\u304c6272\u500b\u306b\u6ce8\u76ee\u3057\u3066\u305f)\u306e\u3067\u3001\n# \u5404landmark id\u3054\u3068\u306b1\u679a\u8a13\u7df4\u30c7\u30fc\u30bf(traindata)\u3001\uff11\u679a\u691c\u8a3c\u30c7\u30fc\u30bf(validation)\u306b\u3057\u3066pytorch\u3067\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u3053\u3068\u3092\u8003\u3048\u308b\u3002","21d9f2b8":"# \u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3057\u3066\u3044\u304f","ea0ce7af":"# validation\u3092\u540c\u69d8\u306b\u4f5c\u308b","a3321e18":"# Dataset\u306einstance\u5316","6be85ad8":"# 1 epoch\u306e\u6d41\u308c\u3092\u898b\u3066\u3044\u304f \u307e\u305a\u306ftrain\u304b\u3089","ea4fde02":"# model\u3092save\u3059\u308b","bce50020":"# \u304f\u3063\u3064\u3051\u3066epoch\u3067\u56de\u3059","273aba12":"## This notebook is just for test.\n## This cannot make a complete model because of memory error.\n## Thank you.","44e912e4":"# 3. DataLoader","1c79c844":"# \u3053\u3053\u304b\u3089pytorch","90b66229":"# \u521d\u65e5\u306f\u3053\u3053\u307e\u3067\u304b\u306a\u30fb\u30fb\u30fb"}}