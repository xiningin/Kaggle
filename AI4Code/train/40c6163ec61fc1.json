{"cell_type":{"56fd4053":"code","c0ac3b3f":"code","fbb4928c":"code","7bfb18a0":"code","21feda8c":"code","140eff14":"code","dcdfec52":"code","2f9df07f":"code","8b64d59f":"code","244686b6":"code","66bebce4":"code","0f9bf996":"code","fa11000b":"code","f7778422":"code","6ca3cdfb":"code","e59e98d7":"code","0e892658":"code","4305f53b":"code","3d70827a":"markdown","82c0f352":"markdown","df979c2e":"markdown","6edd8f51":"markdown","3b9c0754":"markdown","4c845cd2":"markdown","0755a066":"markdown"},"source":{"56fd4053":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0ac3b3f":"df=pd.read_csv(\"\/kaggle\/input\/allenunger-global-commodity-prices\/all_commodities.csv\")","fbb4928c":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows',None)\n#col width option is not necessary, but can be usefull for looking at large amounts of text in columns such as sources.\n#pd.set_option('max_colwidth', -1)","7bfb18a0":"#I want to analyze Wheat in the 100 years from 1801-1900  \n#Using loc indexing in combination with boolean expressions returns that snapshot\nwheat_df=df.loc[(df.Commodity == \"Wheat\")& (df['Item Year'] >1800)& (df['Item Year'] <1901)]\nwheat_df.head()","21feda8c":"#I want to check which locations have the most data for wheat\nwheat_counts=wheat_df.groupby([\"Location\",\"Commodity\"]).size().reset_index(name=\"Wheat_Counts\").sort_values(by=\"Wheat_Counts\",ascending=False)\nwheat_counts.head()","140eff14":"#I have identified locations that have 85 or more Data points for wheat in this time period\n#I want to take a look at if there will be potential issues with having different sources or varieties before proceeding.\n#I also want to make sure the data is continuous if possible. If not I will have to fill in with appropriate method\n\n# Porto, Portugal 1801-1854 two sets of data 2 sources, variety NA, too many missing data points,45, to continue with\n#wheat_df[wheat_df.Location==\"Porto\"]\n\n# Low, Ukraine 1801-1900 variety NA\n#wheat_df[wheat_df.Location==\"Lwow\"]\n\n# Tours France variety NA 1801-1900 variety NA\n#wheat_df[wheat_df.Location==\"Tours\"]\n\n# england\/southengland dataset 1800-1900 variety NA for both and I will investigate before combining\n#wheat_df[wheat_df.Location==\"England\"]\n#wheat_df[wheat_df.Location==\"Southern England\"]\n\n# Arnhem, Netherlands 1800-1900 variety NA\n#wheat_df[wheat_df.Location==\"Arnhem\"]\n\n#Ghent, Belgium is NA variety and 1816-1900\n#wheat_df[wheat_df.Location==\"Ghent\"]\n\n#Krakow,Poland variety NA 1800-1900 2 different sources, so I will have to investigate before combining\n#wheat_df[wheat_df.Location==\"Krakow\"]","dcdfec52":"#Making dataframes for cities. At this point both England locations and both Krakow sources are seperate\n#England\nengland=wheat_df.loc[(wheat_df.Location==\"England\")].reset_index()\nsouthern_england=wheat_df.loc[(wheat_df.Location==\"Southern England\")].reset_index()\n#Poland\nkrakow_unger=wheat_df.loc[(wheat_df.Location==\"Krakow\") & (wheat_df.Sources==\"(Richard Unger)\")].reset_index()\nkrakow_gorkiewicz=wheat_df.loc[(wheat_df.Location==\"Krakow\") & (wheat_df.Sources!=\"(Richard Unger)\")].reset_index()\n#France\ntours=wheat_df.loc[(wheat_df.Location==\"Tours\")].reset_index()\n#Belgium\nghent=wheat_df.loc[(wheat_df.Location == \"Ghent\")].reset_index()\n#Netherlands\narnhem=wheat_df.loc[(wheat_df.Location == \"Arnhem\")].reset_index()\n#Ukraine\nlwow=wheat_df.loc[wheat_df.Location==\"Lwow\"].reset_index()","2f9df07f":"krakow_combined=krakow_gorkiewicz.copy()\nkrakow_combined[\"Standard Value\"]=(krakow_gorkiewicz[\"Standard Value\"] + krakow_unger[\"Standard Value\"])\/2\nkrakow_combined.head()","8b64d59f":"england_combined=england.copy()\nengland_combined[\"Standard Value\"]=(england[\"Standard Value\"] + southern_england[\"Standard Value\"])\/2\nengland_combined.head()","244686b6":"#I want to calculate the Wheat price % change per year relative to the first year of data that all locations have, 1816\n#This could prove usefull both for filling in missing data points for Ghent which is missing values before 1816\n#It can be more resistant to violatility than a standard mean calculation in some cases.\n\n#Add a column for % change by year relative to the FIRST year by accessing the zero index value in \"Standard Value\" column\nengland_combined[\"england_pct_change\"] = england_combined[\"Standard Value\"] \/ england_combined[england_combined['Item Year'] == 1816]['Standard Value'].iat[0]-1\ntours[\"tours_pct_change\"] = tours[\"Standard Value\"] \/ tours[tours['Item Year'] == 1816]['Standard Value'].iat[0]-1\nkrakow_combined[\"krakow_pct_change\"] = krakow_combined[\"Standard Value\"] \/ krakow_combined[krakow_combined['Item Year'] == 1816]['Standard Value'].iat[0]-1\nlwow[\"lwow_pct_change\"] = lwow[\"Standard Value\"] \/ lwow[lwow['Item Year'] == 1816]['Standard Value'].iat[0]-1\narnhem[\"arnhem_pct_change\"] = arnhem[\"Standard Value\"] \/ arnhem[arnhem['Item Year'] == 1816]['Standard Value'].iat[0]-1\nghent[\"ghent_pct_change\"] = ghent[\"Standard Value\"] \/ ghent[ghent['Item Year'] == 1816]['Standard Value'].iat[0]-1\nengland_combined.head()","66bebce4":"#Creating this new dataframe will streamline the aggregation process\nengland_combined2=england_combined[[\"england_pct_change\",\"Item Year\"]].copy() \ntours2=tours[[\"tours_pct_change\",\"Item Year\"]].copy() \nkrakow_combined2=krakow_combined[[\"krakow_pct_change\",\"Item Year\"]].copy() \nlwow2=lwow[[\"lwow_pct_change\",\"Item Year\"]].copy()  \narnhem2=arnhem[[\"arnhem_pct_change\",\"Item Year\"]].copy()  \nghent2=ghent[[\"ghent_pct_change\",\"Item Year\"]].copy()\n","0f9bf996":"#Combine all the relevant data\ndflist=[england_combined2,krakow_combined2,lwow2,tours2,arnhem2,ghent2]\ndfs = [df.set_index('Item Year') for df in dflist]\nEurope= pd.concat(dfs, axis=1)\n#Europe= pd.concat(dflist, axis=1)\nEurope[\"avg_pct_change\"]=Europe.agg(\"mean\",axis=1)\nEurope=Europe.reset_index()\nEurope.head()","fa11000b":"#I now have reasonable values for the missing values of Ghent\n\nghent3 = pd.merge(Europe[['Item Year', 'avg_pct_change']], ghent, how='outer', on=['Item Year'])\nghent3.loc[ghent3['Standard Value'].isnull(), 'Standard Value'] = (1+ghent3['avg_pct_change']) * ghent3[ghent3['Item Year'] == 1816]['Standard Value'].iat[0]\n#Need to fill in percent change for ghent as well\n\nghent3[\"ghent_pct_change\"] = ghent3[\"Standard Value\"] \/ ghent3[ghent3['Item Year'] == 1801]['Standard Value'].iat[0]-1\n#ghent3=ghent3.drop([\"level_0\",\"index\"],axis=1)\nghent3[\"Commodity\"].fillna(\"Wheat\",inplace=True)\nghent3[\"Location\"].fillna(\"Ghent\",inplace=True)\nghent3[\"Original Currency\"].fillna(\"Belgian Franc\",inplace=True)\nghent3[\"Standard Currency\"].fillna(\"Silver\",inplace=True)\nghent3[\"Orignal Measure\"].fillna(\"Kilogram\",inplace=True)\nghent3[\"Standard Measure\"].fillna(\"Kilogram\",inplace=True)\nghent3[\"Sources\"].fillna(\"(G. Avondts-P. Scholliers) (De Gentse Textielarbeiders in de 19e en 20e Eeuw dossier 5) (Brussels: Centrum voor Hedendaagse Sociale Geschiedenis-1977)\",inplace=True)\nghent3[\"Notes\"].fillna(\"(gpih.ucdavis.edu)-(D.S.Jacks_2001-P.H. Lindert_2008-J.W.Ambrosin_2007)\",inplace=True)\n\nghent3.head()","f7778422":"#england_combined=england_combined.reset_index()\nengland_combined['20_yr_rolling_avg_price'] = england_combined[\"Standard Value\"].rolling(window=20).mean()\nengland_combined['20_yr_rolling_std_price'] = england_combined['Standard Value'].rolling(window=20).std()\nengland_combined['2_std_price_decline'] = england_combined['20_yr_rolling_avg_price'] - 2 * england_combined['20_yr_rolling_std_price']\nengland_combined['2_std_price_rise'] = england_combined['20_yr_rolling_avg_price'] + 2 * england_combined['20_yr_rolling_std_price']\nengland_combined.loc[england_combined['Standard Value'] >= england_combined['2_std_price_rise'], '95_pct_sig_move'] = 'sig_price_increase'\nengland_combined.loc[england_combined['Standard Value'] <= england_combined['2_std_price_decline'], '95_pct_sig_move'] = 'sig_price_decrease'\n\nengland_combined.head()","6ca3cdfb":"england_combined3=england_combined.copy() \ntours3=tours.copy() \nkrakow_combined3=krakow_combined.copy() \nlwow3=lwow.copy()  \narnhem3=arnhem.copy()  \nghent4=ghent3.copy()","e59e98d7":"#Same idea as above, but I need this to be with respect to 1801 this time.\nengland_combined3[\"england_pct_change\"] = england_combined[\"Standard Value\"] \/ england_combined[england_combined['Item Year'] == 1801]['Standard Value'].iat[0]-1\ntours3[\"tours_pct_change\"] = tours[\"Standard Value\"] \/ tours[tours['Item Year'] == 1801]['Standard Value'].iat[0]-1\nkrakow_combined3[\"krakow_pct_change\"] = krakow_combined[\"Standard Value\"] \/ krakow_combined[krakow_combined['Item Year'] == 1801]['Standard Value'].iat[0]-1\nlwow3[\"lwow_pct_change\"] = lwow[\"Standard Value\"] \/ lwow[lwow['Item Year'] == 1801]['Standard Value'].iat[0]-1\narnhem3[\"arnhem_pct_change\"] = arnhem[\"Standard Value\"] \/ arnhem[arnhem['Item Year'] == 1801]['Standard Value'].iat[0]-1\n#ghent2[\"ghent_pct_change\"] = ghent3[\"Standard Value\"] \/ ghent3[ghent3['Item Year'] == 1801]['Standard Value'].iat[0]-1\n\n\n\n","0e892658":"england_combined3=england_combined3[[\"england_pct_change\",\"Item Year\"]] \ntours3=tours3[[\"tours_pct_change\",\"Item Year\"]] \nkrakow_combined3=krakow_combined3[[\"krakow_pct_change\",\"Item Year\"]] \nlwow3=lwow3[[\"lwow_pct_change\",\"Item Year\"]]  \narnhem3=arnhem3[[\"arnhem_pct_change\",\"Item Year\"]]  \nghent3=ghent3[[\"ghent_pct_change\",\"Item Year\"]]\n","4305f53b":"#With respect to the year 1801 I now have the percent differences and average percent difference for Europe.\ndflist2=[england_combined3,krakow_combined3,lwow3,tours3,arnhem3,ghent3]\ndfs2 = [df.set_index('Item Year') for df in dflist2]\nEurope_1801= pd.concat(dfs2, axis=1)\n\nEurope_1801[\"avg_pct_dif\"]=Europe_1801.agg(\"mean\",axis=1)\n\nEurope_1801=Europe.reset_index()\nEurope_1801.head()","3d70827a":"# Read CSV and Display Options","82c0f352":"# Impute Missing Values for Ghent","df979c2e":"# Combine Data for England and Krakow","6edd8f51":"# Build 95% Confidence Intervals","3b9c0754":"# Establish DataFrames for Wheat Prices\n","4c845cd2":"# Calculate the Percent Change Relative to 1816","0755a066":"# Recalculate Avg Percent Change for Europe 1801-1900\n"}}