{"cell_type":{"929ed793":"code","c02318fc":"code","a70749d2":"code","3ebbb076":"code","31ebc32a":"code","e18153e5":"code","ca31bc69":"code","780754ce":"code","3cfb0b56":"code","3d955ddd":"code","f4edf6f6":"code","1eee79e0":"code","27341fda":"code","62f0e0e0":"code","a168aadf":"code","a11ff90f":"code","79efb38e":"code","88c855de":"code","ec9a8958":"code","d974dba4":"code","648f7ad7":"code","425278d3":"code","0e9d69da":"code","baffec5c":"code","6e4688c2":"code","c31e8457":"code","26d5fc27":"code","8882fe85":"code","f2459278":"code","1c887687":"code","8f7d8a30":"code","cfd4482f":"code","635947ae":"code","e3f303df":"markdown","7dc908eb":"markdown","fb54b9b7":"markdown","f3550543":"markdown","884ed6b1":"markdown","e3febaaa":"markdown","8ce378e9":"markdown","27507fee":"markdown","89b53a9c":"markdown","a0b2a3f9":"markdown","aa386a4a":"markdown","a1bc2541":"markdown","868c869e":"markdown","614fd455":"markdown","0846d382":"markdown","7fa76d48":"markdown","0823b22b":"markdown","cfb73575":"markdown","bfd07bd6":"markdown","fbb85b78":"markdown","bc1c7d64":"markdown","49218bb9":"markdown","eeeb0983":"markdown","10b7b7d9":"markdown","6591f0c5":"markdown","c34a69e2":"markdown","7eae05ca":"markdown","fd6770e8":"markdown","83a38ef5":"markdown","9ac78324":"markdown","5b9e1bfe":"markdown","88c5781b":"markdown","2675e56b":"markdown","56814209":"markdown","41aa6d29":"markdown","caa7342a":"markdown","d1e98c43":"markdown","68722026":"markdown","d2398cbb":"markdown","d7335ab0":"markdown","d2dcf47f":"markdown","23926f3a":"markdown","0b6b60f4":"markdown","6ad7f591":"markdown","e76260b0":"markdown","5eafc9c3":"markdown","0d43b218":"markdown","aad868e8":"markdown"},"source":{"929ed793":"import nltk\nimport re\nimport pandas as pd","c02318fc":"doc = '..\/input\/pg2680.txt'\nwith open(doc, encoding='utf-8-sig') as file:\n    burgess = file.read()\nprint (burgess[:400])","a70749d2":"# convert all letters to lowercase in order to standardize the text.\nburgess = burgess.lower()\n\n# here, we just split the whole text by spaces \ntokens = [word for word in burgess.split()]\nprint(tokens[:100])","3ebbb076":"tokens = nltk.word_tokenize(burgess)\nprint(tokens[:100])","31ebc32a":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(burgess)\nprint(tokens[:100])","e18153e5":"token_freq = nltk.FreqDist(tokens)\ntoken_freq[\"marcus\"]","ca31bc69":"%matplotlib inline \n\ntoken_freq.plot(20, cumulative=False)","780754ce":"stop_words = set(nltk.corpus.stopwords.words('english'))\nprint(stop_words)","3cfb0b56":"# filter each sentence by removing the stop words\ndef remove_stopwords(tokens):\n    return [word for word in tokens if word not in stop_words]\n        \nprint(remove_stopwords(tokens)[:100])","3d955ddd":"token_freq = nltk.FreqDist(remove_stopwords(tokens))\ntoken_freq.plot(20, cumulative=False)","f4edf6f6":"sents = nltk.sent_tokenize(burgess)\nprint(sents[:10])","1eee79e0":"import string\n\ndef remove_punctuation(text):\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    tokens = nltk.word_tokenize(text)\n    return tokens\n\nprint(remove_punctuation(burgess)[:100])","27341fda":"#-----PorterStemmer------\nstemmer = nltk.stem.PorterStemmer()\nprint('##### Porter Stemmer #####')\nprint (stemmer.stem('running'))\nprint (stemmer.stem('runner'))\nprint (stemmer.stem('decreases'))\nprint (stemmer.stem('multiplying\\n'))\n\n#-----LancasterStemmer------\nstemmer = nltk.stem.LancasterStemmer()\nprint('##### Lancaster Stemmer #####')\nprint (stemmer.stem('running'))\nprint (stemmer.stem('runner'))\nprint (stemmer.stem('decreases'))\nprint (stemmer.stem('multiplying\\n'))\n\n#-----SnowballStemmer------\n# we need to specify language to initiate this stemmer\nstemmer = nltk.stem.SnowballStemmer(\"english\") \nprint('##### Snowball Stemmer #####')\nprint (stemmer.stem(\"running\"))\nprint (stemmer.stem('runner'))\nprint (stemmer.stem('decreases'))\nprint (stemmer.stem('multiplying\\n'))\n\n#-----WordNetLemmatizer------\nlemmatizer = nltk.stem.WordNetLemmatizer()\nprint('##### WordNet Lemmatizer #####')\nprint(lemmatizer.lemmatize('running'))\nprint (lemmatizer.lemmatize('runner'))\nprint(lemmatizer.lemmatize('decreases'))\nprint(lemmatizer.lemmatize('multiplying\\n'))","62f0e0e0":"print(lemmatizer.lemmatize('playing', pos=\"v\"))\nprint(lemmatizer.lemmatize('playing', pos=\"n\"))\nprint(lemmatizer.lemmatize('playing', pos=\"a\"))\nprint(lemmatizer.lemmatize('playing', pos=\"r\"))","a168aadf":"sents = nltk.sent_tokenize(burgess)\nprint (sents[10])\ntokens = nltk.word_tokenize(sents[10])\nnltk.pos_tag(tokens)","a11ff90f":"sentence = \"Bill Gates, CEO of Microsoft Inc. is living in California.\"\n\ntokens = nltk.word_tokenize(sentence)\n# chunks = nltk.ne_chunk(nltk.pos_tag(tokens))\n# chunks    ","79efb38e":"tokens = remove_punctuation(burgess)\nbigrams = nltk.bigrams(tokens)\nprint(list(bigrams)[:50])","88c855de":"trigrams = nltk.trigrams(tokens)\nprint(list(trigrams)[:50])","ec9a8958":"fourgrams = nltk.ngrams(tokens, 4)\nprint(list(fourgrams)[:50])","d974dba4":"sentence_1 = \"I love studying AI\"\nsentence_2 = \"I love studying statistics\"\nsentence_3 = \"I love working as Data Scientist\"\n\n# so, lets respresent this in a numerical way\n# We'll at first combine all these sentences and then tokenize and get distinct tokens.\n# we basically combine the sentences with spaces between them.\ntext = \" \".join([sentence_1, sentence_2, sentence_3])\n\n# now we tokenize and get the distinct tokens as a list by using set() function.\ntokens = list(set(nltk.word_tokenize(text)))\nprint(tokens)","648f7ad7":"from sklearn.feature_extraction.text import CountVectorizer\n\nvect = CountVectorizer()\n\npattern = re.compile('(\\r\\n)+')  # Windows new line \nclean_text = re.sub(pattern, ' ', burgess)\nclean_sent = nltk.sent_tokenize(clean_text)\n\n# Now remove the punctuation and stopwords from each sentence\nclean_corpus = []\nfor sent in clean_sent:\n    words = nltk.word_tokenize(sent)\n    remove_punc = [word for word in words if word not in string.punctuation]\n    clean_corpus.append(' '.join([word for word in remove_punc if not word in stop_words]))\n\nclean_corpus[:5]","425278d3":"vect.fit(clean_corpus)\nmatrix = vect.transform(clean_corpus)\nprint(vect.get_feature_names()[:50])","0e9d69da":"cv_df = pd.DataFrame(matrix.toarray(), columns=vect.get_feature_names())\ncv_df.head()","baffec5c":"cv_df.shape","6e4688c2":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvect = TfidfVectorizer()\nvect.fit(clean_corpus)\nmatrix = vect.transform(clean_corpus)\ntf_df = pd.DataFrame(matrix.toarray(), columns=vect.get_feature_names())\ntf_df.head()","c31e8457":"# Now look at the first sentence by sorting the tfidf scores in a decreasing order\ntf_df.iloc[10].sort_values(ascending=False).head(10)","26d5fc27":"import spacy\n\nen = spacy.load('en')\ndoc = en('galaxies recede from us at speeds proportional to their distances, going faster the farther away they are.')\n\n# now text is parsed by nearly all the basic NLP techniques and we are ready to extract what we need.\n# tokenization \nfor token in doc:\n    print(token.text)","8882fe85":"for token in doc:\n    print(token.text, token.lemma_, token.pos_, token.tag_,\n         token.dep_, token.shape_, token.is_alpha, token.is_stop, sep=\" ==> \")","f2459278":"#lets view this parsing in Pandas dataframe\n\ntokens = [token.text for token in doc]\nlemmas = [token.lemma_ for token in doc]\npos = [token.pos_ for token in doc]\ntag = [token.tag_ for token in doc]\ndep = [token.dep_ for token in doc]\nshape = [token.shape_ for token in doc]\nis_alpha = [token.is_alpha for token in doc]\nis_stop = [token.is_stop for token in doc]\n\npd.DataFrame({\"tokens\":tokens, \"lemmas\":lemmas, \"pos\":pos, \"tag\":tag, \n             \"dep\": dep, \"shape\":shape, \"is_alpha\":is_alpha, \"is_stop\":is_stop}, \n             columns=[\"tokens\", \"lemmas\", \"pos\", \"tag\", \"dep\", \"shape\", \"is_alpha\", \"is_stop\"])","1c887687":"en = spacy.load('en')\ndoc = en(\"Steve Jobs, the CEO of Apple Inc. is living in San Francisco.\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.label_)","8f7d8a30":"# lets see how it works with the following sentences.\n\ndoc1 = \"I have big exam tomorrow and I need to study hard to get a good grade.\"\ndoc2 = \"My wife likes to go out with me but I prefer staying at home and studying.\"\ndoc3 = \"Kids are playing football in the field and they seem to have fun\"\ndoc4 = \"Sometimes I feel depressed while driving and it's hard to focus on the road.\"\ndoc5 = \"I usually prefer reading at home but my wife prefers watching a TV.\"\n\n# array of documents aka corpus\ncorpus = [doc1, doc2, doc3, doc4, doc5]\n\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntoken_list = [tokenizer.tokenize(sentence.lower()) for sentence in corpus]\n\ndef remove_stopwords(words):\n    return [word for word in words if word not in stop_words]\n\ntokenized_data = [remove_stopwords(token) for token in token_list]\ntokenized_data","cfd4482f":"from gensim import corpora, models\n\n# Build a Dictionary - association word to numeric id\ndictionary = corpora.Dictionary(tokenized_data)\n \n# Transform the collection of texts to a numerical form\ncorpus = [dictionary.doc2bow(text) for text in tokenized_data]\n\n# We are asking LDA to find 10 topics in the data\nlda_model = models.LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)\n\nfor idx in range(10):\n    # Print the first 10 most representative topics\n    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 5))\n","635947ae":"new_sentence = \"My wife plans to go out tonight\"\nlda_model.get_document_topics(dictionary.doc2bow(new_sentence.split()))","e3f303df":"We see that, without applying any other procedure, with just a few lines of code, Steve and Jobs are recognized as a whole person.","7dc908eb":"For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. \n\nAdditionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n\n***Stemming*** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. (removing affixes from words and return the root word. Ex: The stem of the word working => work.)\n\n***Lemmatization*** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . \n\nIf confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. \n\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. As we can understand, both techniques could remove important information but also help us to normalize our corpus (although lemmatization is the one that is usually applied)\n\nThere are many algorithms for stemming and most of them produce similar results. Here are the embedded stemming algorithms in NLTK:\n\n- nltk.stem.ISRIStemmer\n\n- nltk.stem.LancasterStemmer\n\n- nltk.stem.PorterStemmer\n\n- nltk.stem.RegexpStemmer\n\n- nltk.stem.RSLPStemmer\n\n- nltk.stem.SnowballStemmer","fb54b9b7":"A problem with scoring word frequency is that highly frequent words start to dominate in the document (larger score), but may not contain as much \u201cinformational content\u201d to the model as rarer but perhaps domain specific words. One approach is to rescale the frequency of words by how often they appear in all documents, so that the scores for frequent words like \u201cthe\u201d that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency \u2013 Inverse Document Frequency, or TF-IDF for short, where:\n\nTerm Frequency: is a scoring of the frequency of the word in the current document.\nInverse Document Frequency: is a scoring of how rare the word is across documents.\n\nThe scores are a weighting where not all words are equally as important or interesting. The scores have the effect of highlighting words that are distinct (contain useful information) in a given document. Just like CountVectorizer, TfidfVectorizer also creates a document term matrix (DTM) from our messages. However, instead of filling the DTM with token counts it calculates term frequency-inverse document frequency value for each word(TF-IDF). The TF-IDF is the product of two weights, the term frequency and the inverse document frequency.\n\nTo generalize: TF-IDF = term frequency * (1 \/ document frequency)\n\nOr: TF-IDF = term frequency * inverse document frequency\n\nTerm frequency is a weight representing how often a word occurs in a document. If we have several occurences of the same word in one document we can expect the TF-IDF to increase. Inverse document frequency is another weight representing how common a word is across documents. If a word is used in many documents then the TF-IDF will decrease. With the definition out of the way we\u2019ll go through a few examples to see how it works. Since the usage is pretty much identical to CountVectorizer and we\u2019ll be going through a few examples we\u2019ll make a function to create a DTM from our messages to make things a bit easier and clearer.","f3550543":"### Tokenization","884ed6b1":"### Topic Modeling","e3febaaa":"Applying all these processes with Spacy package is quite easy and straightforward. We should decide which package to use by checking the accuracy or any other metrics that we want to deal with. We at first import Spacy package and load \"English\" parser. Then we dump any text into this parser and Spacy will apply various preprocssing mthods as default. All we need to do is to usee relevant functions to extract what we wamt such as tokens, lemmas, pos etc.","8ce378e9":"# NLP Essentials\n\n## Libraries and Packages","27507fee":"As you can see from the plot, the words like he, the, and, etc. are highly populated in the text but don\u2019t add any extra information in a sentence. Such words can often create noise while modelling. Such words are known as ***Stop Words***. \n\nLet's see the stopwords in English language as in NLTK package","89b53a9c":"We will get error because of the virtual environment. It provides us with an entity tree picture. You can run it locally to see it. Expected results: Bill is recognizd as name but CEO is recognized as organization. Since we used the simplest way of NER eembedded in NLTK, it's not strong enough to catch these kind of consecutive entities as a one. In practice, we generally use much stronger and efficient NER packages such as StandfordNERTagger, Spacy etc. ","a0b2a3f9":"### Stemming and Lemmatization","aa386a4a":"Since we trained and built our LDA model over the five simple sentences, whenever we want to detect the topic of a new sentence or text, we'll at first prepare the text and then push that into our model to get a topic. Let's try to predict a topic for a new sentence.","a1bc2541":"as you can see, topic-5 (listed above) is the most relevant topic for this sentence. \n\nTopic #5: 0.103*\"studying\" + 0.101*\"likes\" + 0.101*\"go\" + 0.096*\"home\" + 0.094*\"wife\"","868c869e":"Stemming works on words without knowing its context and that\u2019s why stemming has lower accuracy and faster than lemmatization. \n\nWe can also specify the part-of-speech in lemmatization","614fd455":"N-gram is a continuous sequence of n items from a given sequence of text or speech and N-grams of texts are extensively used in text mining and natural language processing tasks. In other words, n-grams are simply all combinations of adjacent words or letters of length n that you can find in your source text. If it's a one word gram, it's called unigram, bigram for two-word grams, trigram for three-word grams and so on. They are basically a set of co-occuring words within a given window and when computing the n-grams you typically move one word forward.\n\nN-grams are used for a variety of different task. For example, when developing a language model, n-grams are used to develop not just unigram models but also bigram and trigram models. Another simple purpose would be building a keyword sequesnces made of multiple words. One another use of n-grams, may be the most important one, is for developing features for supervised Machine Learning models such as SVMs, Naive Bayes, etc. The idea is to use tokens such as bigrams in the feature space instead of just unigrams.","0846d382":"#### Count Vectorizer \n\nWith count vectorizer, we count the appearance of the words in each text. Using the fit method, our Count Vectorizer will \u201clearn\u201d what tokens are being used in our sentences inside the corpus. That is, we at first train our Count Vectorizer model with the training set and then let the model learn the tokens. Then, whenever we throw a new sentence to this model, our Count Vectorizer model will show use the matrix representation of that sentence.","7fa76d48":"This return a dictionary like key-value pair and in order to find a frequency of a token, we simply use that token as key within brackets just as we do in dictionary types. And we can easily plot this with an embedded plot function...","0823b22b":"**Lemmatization and POS**\n\n- **text**: The original word text.\n- **lemma**: The base form of the word.\n- **pos**: The simple part-of-speech tag.\n- **tag**: The detailed part-of-speech tag.\n- **dep**: Syntactic dependency, i.e. the relation between tokens.\n- **shape**: The word shape \u2013 capitalisation, punctuation, digits.\n- **is_alpha**: Is the token an alpha character?\n- **is_stop**: Is the token part of a stop list, i.e. the most common words of the language?","cfb73575":"Here are the meaning of POS tags:\n\n\n|Symbol| Explanation|Symbol| Explanation|\n|---|---|---|---|\n|CC | Coordinating conjunction | `PRP$` | Possessive pronoun |\n|CD | Cardinal number | RB | Adverb\n|DT | Determiner | RBR | Adverb, comparative |\n|EX | Existential there | RBS | Adverb, superlative |\n|FW | Foreign word | RP | Particle |\n|IN | Preposition or subordinating conjunction | SYM | Symbol |\n|JJ | Adjective |TO | to |\n|JJR | Adjective, comparative |UH | Interjection |\n|JJS | Adjective, superlative |VB | Verb, base form |\n|LS | List item marker |VBD | Verb, past tense |\n|MD | Modal |VBG | Verb, gerund or present participle |\n|NN | Noun, singular or mass |VBN | Verb, past participle |\n|NNS | Noun, plural |VBP | Verb, non-3rd person singular present |\n|NNP | Proper noun, singular |VBZ | Verb, 3rd person singular present |\n|NNPS | Proper noun, plural |WDT | Wh-determiner |\n|PDT | Predeterminer |WP | Wh-pronoun |\n|POS | Possessive ending |`WP$` | Possessive wh-pronoun |\n|PRP | Personal pronoun |WRB | Wh-adverb |","bfd07bd6":"For the ngrams larger than three, we can use another nltk function with the number of grams as a parameter.","fbb85b78":"***Commonly Used Types of Named Entity***\n\nORGANIZATION\tGeorgia-Pacific Corp., WHO\n\nPERSON\tEddy Bonte, President Obama\n\nLOCATION\tMurray River, Mount Everest\n\nDATE\tJune, 2008-06-29\n\nTIME\ttwo fifty a m, 1:30 p.m.\n\nMONEY\t175 million Canadian Dollars, GBP 10.40\n\nPERCENT\ttwenty pct, 18.75 %\n\nFACILITY\tWashington Monument, Stonehenge\n\nGPE\t(Geo-Political Entity) South East Asia, Midlothian","bc1c7d64":"Knowing when to use which package all depends on the use case:\n\n- ***NLTK*** is recommended only as an education and research tool. Its modularized structure makes it excellent for learning and exploring NLP concepts, but it\u2019s not meant for production. It\u2019s the most famous Python NLP library, and it\u2019s led to incredible breakthroughs in the field.  NLTK is also popular for education and research. On its own website, NLTK claims to be an \u201can amazing library to play with natural language.\u201d The major drawback of NLTK is that it\u2019s heavy and slippery, and it has a steep learning curve. The second major weakness is that it\u2019s slow and not production-ready.\n\n- ***TextBlob*** is built on top of NLTK, and it\u2019s more easily-accessible. This is our favorite library for fast prototyping or building applications that don\u2019t require highly optimized performance. Beginners should start here. TextBlob makes text processing simple by providing an intuitive interface to NLTK. It\u2019s a welcome addition to an already solid lineup of Python NLP libraries because it has a gentle learning curve while boasting a surprising amount of functionality.\n\n- ***Stanford\u2019s CoreNLP*** is a Java library with Python wrappers. It\u2019s in many existing production systems due to its speed. Stanford CoreNLP is a suite of production-ready natural analysis tools. It includes part-of-speech (POS) tagging, entity recognition, pattern learning, parsing, and much more. Many organizations use CoreNLP for production implementations. It\u2019s fast, accurate, and able to support several major languages.\n\n- ***SpaCy*** is a new NLP library that\u2019s designed to be fast, streamlined, and production-ready. It\u2019s not as widely adopted, but if you\u2019re building a new application, you should give it a try. SpaCy is minimal and opinionated, and it doesn\u2019t flood you with options like NLTK does. Its philosophy is to only present one algorithm (the best one) for each purpose. You don\u2019t have to make choices, and you can focus on being productive. Because it\u2019s built on Cython, it\u2019s also lightning-fast. \n\n- ***Gensim*** is most commonly used for topic modeling and similarity detection. It\u2019s not a general-purpose NLP library, but for the tasks it does handle, it does them well. Gensim is a well-optimized library for topic modeling and document similarity analysis. Among the Python NLP libraries listed here, it\u2019s the most specialized. Its topic modeling algorithms, such as its Latent Dirichlet Allocation (LDA) implementation, are best-in-class. In addition, it\u2019s robust, efficient, and scalable.\n\n- ***Fasttext*** is mainly used for applying deep learning techniques to NLP problems. FastText combines some of the most successful concepts introduced by the natural language processing and machine learning communities in the last few decades. These include representing sentences with bag of words and bag of n-grams, as well as using subword information, and sharing information across classes through a hidden representation.  It also employs a hierachical softmax that takes advantage of the unbalanced distribution of the classes to speed up computation. These different concepts are being used for two different tasks: efficient text classification and learning word vector representations.","49218bb9":"As you can see, now the plot makes much moree sense and we can easily understand that the text is talking about ***bear and farmers***.\n\nNow let's tokenize this txt by sentences","eeeb0983":"The length of the numerical representation of sentences (numerical vector) will be the length of this list. Since we have six distinct words, we'll represent each sentence with 6 digits. To numerically represent each sentence, let's assign 1 to each occurence of words above. So, if a word form th tokenized list is inside the sentence, it's 1; if it's not, it's 0. Beware that we are not dealing with the order of words inside the sentence.\n\n['working', 'Data', 'statistics', 'as', 'studying', 'I', 'AI', 'love', 'Scientist']\n\nBy looking at the reference list, we can represent sentence_1 (\"I love studying AI\") as below:  \nSo, the numerical representation of sentence_1 is: `[0, 0, 0, 0, 1, 1, 1, 1, 0]`\n\nSimply said, a bag-of-words is a representation of text that describes the occurrence of words within a document. That is, each document or data entry is represented as numerical vectors based on the vocabulary built from the corpora. The intuition is that documents are similar if they have similar content. \n\nIt is called a \u201cbag\u201d of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n\nAs you can see, while working with large texts, there will be multiple occurences for some words. And we need to repreesent this as well. Here comes term frequency. ","10b7b7d9":"Now we loaded a whole text file into our project. Since the size of this text is quite large, we can split that by paragraph and select a few of them or just select as many words as we want by splitting into words.","6591f0c5":"Now let's find the frequencies and plot again","c34a69e2":"The LDA model discovers the different topics that the documents represent and how much of each topic is present in a document. \n\nPython provides many great libraries for text mining practices, \u201cgensim\u201d is one such clean and beautiful library to handle text data. It is scalable, robust and efficient. Following code shows how to convert a corpus into a document-term matrix.","7eae05ca":"#### TfidfVectorizer (Term Frequency \u2013 Inverse Document Frequency)","fd6770e8":"### Named Entity Recognition (NER)","83a38ef5":"The architecture for a simple information extraction system begins by processing a document using several of the procedures: \n- Raw text of the document is split into sentences using a sentence segmenter\n- Each sentence is further subdivided into words using a tokenizer. \n- Each sentence is tagged with part-of-speech tags\n- Named entity detection. \n- Finally, we use relation detection to search for likely relations between different entities in the text.\n\nWe should keep in mind that because models are statistical and strongly depend on the examples they were trained on, this doesn't always work perfectly and might need some tuning later, depending on your use case.","9ac78324":"In the real world, in our daily conversations we don\u2019t work directly with the categories of words. For example, if we want to build a Netflix chatbot we want it to recognize both \u2018Joker\u2019 and \u2018Batman\u2019 as instances of the same group which we call \u2018movies\u2019 , but \u2018Christopher Nolen\u2019 as a \u2018director\u2019. This concept of semantic field dependent of a context is what we define as entity. The role of a named entity recognizer is to detect relevant entities in our corpus.\nThe process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :\n\nSentence: Bill, the CEO of Microsoft Inc. is living in USA.\n\nNamed Entities \u2013  ( \u201cperson\u201d : \u201cBill\u201d ), (\u201corg\u201d : \u201cMicrosoft Inc.\u201d), (\u201clocation\u201d : \u201cUSA\u201d)","5b9e1bfe":"Installing NLTK package is a little bit different than installing other Python packages. NLTK package has many other sub modules and data that should be installed after we import NLTK.\n\nOnce that we have confirmed that nltk is installed, we will have to download and install NLTK data. NLTK Data consists of the corpora and all the words in a language along with various grammar syntaxes, toy grammars, trained models, etc. They help the users to easily process languages by applying the various functions. Please note that installing nltk data is a long process it involves downloading over 1 GB of data. \n\nTo download the nltk data, import NLTK and then run `nltk.download_shell()` or `nltk.download()` to see the download options.","88c5781b":"POS Tagging is used when we need to analysis the words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words.\n\nApart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. \n\nPart of Speech tagging is highly useful in terms of word sense disambiguation. The meaning of certain words would change based on the context. ","2675e56b":"## NLP Techniques and Algorithms\n\n### N-grams","56814209":"We can also tokenizee the text with custom regex patterns which is called RegexpTokenizer. Using this function, we can apply highly customized and detailed tokenization that could help us extract much more information without further processing.","41aa6d29":"## 3. Basic Text Processing and NLP Techniques\n\nThe written text or spoken language is the most unstructured form of all the available data. Before we try to extract meaningful insights from text data, we need to sanitize the data by applying some text preprocessing techniques such as tokenization, lemmatization and stopword removal. In NLP, our first goal is to prepare the text data into a format that could be handled by machine learning algorithms. For this purpose, we're going to use the NLP packages mentioned above but let's start with NLTK package at first due to its simplicity.","caa7342a":"Tokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc. This immediately turns an unstructured string (text document) into a more usable data, which can be further structured, and made more suitable for machine learning. Further processing is generally performed after a piece of text has been appropriately tokenized. There are three different ways i.e Split, RegexpTokenizer and Word Tokenize.\n\nLet's tokenize this text by using pure python at first.","d1e98c43":"Here are the most powerful and useful NLP libraries in Python.\n\n\u2022\t**Scikit-learn:** the most powerful and famous Machine Learning library in Python\n\n\u2022\t**Natural Language Toolkit (NLTK):** The complete toolkit for all NLP techniques.\n\n\u2022\t**TextBlob:** Easy to use NLP tools API, built on top of NLTK and Pattern.\n\n\u2022\t**SpaCy:** Industrial strength NLP with Python and Cython.\n\n\u2022\t**Gensim:** Topic Modelling for Humans\n\n\u2022\t**Stanford Core NLP:** NLP services and packages by Stanford NLP Group.\n\n\u2022\t**Fasttext:** NLP library for learning of\u00a0word embeddings\u00a0and sentence classification created by\u00a0Facebook's AI Research (FAIR) lab","68722026":"### Part-of-Speech (POS) Tagging ","d2398cbb":"A problem with modeling text is that it is messy, and techniques like machine learning algorithms prefer well defined fixed-length inputs and outputs. Machine learning algorithms cannot work with raw text directly; so the text must be converted into numbers. Specifically, vectors of numbers. If we want to use text in machine learning algorithms, we\u2019ll have to convert them to a numerical representation. One of the methods is called bag-of-words approach.\n\nThe bag of words model , or BoW for short, is a way of extracting features from text for use in modeling and it ignores grammar and order of words. Once we have a corpus (text data) then first, a list of vocabulary is created based on the entire corpus. Then each document or data entry is represented as numerical vectors based on the vocabulary built from the corpora.","d7335ab0":"Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as \u201ca repeating pattern of co-occurring terms in a corpus\u201d. Topic modelling can be described as a method for finding a group of words (i.e topic) from a collection of documents that best represents the information in the collection.\n\nAs the name suggests, it is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Thus, assisting better decision making. \n\nTopic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called \u201ctopics\u201d) in large clusters of texts.\n\nA good topic model results in \u2013 \u201chealth\u201d, \u201cdoctor\u201d, \u201cpatient\u201d, \u201chospital\u201d for a topic \u2013 Healthcare, and \u201cfarm\u201d, \u201ccrops\u201d, \u201cwheat\u201d for a topic \u2013 \u201cFarming\u201d.\n\nTopic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example \u2013 New York Times are using topic models to boost their user \u2013 article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.\n\nThere are many approaches for obtaining topics from a text such as \u2013 Term Frequency and Inverse Document Frequency (TfIdf). NonNegative Matrix Factorization techniques. Latent Dirichlet Allocation(LDA) is the most popular topic modeling technique.\n\nLDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.","d2dcf47f":"### Bag of Words - Term Frequency","23926f3a":"This is not the best way as we can see there are some word with symbols. Now let's do the same with NLTK tokenizer","0b6b60f4":"As you can see, the sentence tokenization basically works with ending punctuations such as periods and question marks. \n\nWhat if we want to remove the punctuations as well? In order to accomplish this, we can either come up with a list  unwanted punctuations or use a ready-to-use list in python.","6ad7f591":"As you see, wee don't see any punctuations since we tokenized by words in regex. \n\nLet's see the frequeencies of each token. This is quite simple and straightforward with NLTK","e76260b0":"## Conclusions\n\nI hope you liked my kernel. Don't forget to upvote if you do :)","5eafc9c3":"As you can see, we now have all the frequencies of each token inside a sentence. \n\nWe have 6428 different features (number of columns in dataframe) created from all of our sentences. This means each row will mostly be filled with zeros. In order to save space\/computational power a sparse matrix is created. This means that only the location and value of non-zero values is saved. Since we're transforming the same sentences into a model fitted from those sentences, this is not an issue at the moment. But when we try to find a matrix representation of another sentence other than we already have, it's quite possible that many of thee features (tokens) will not be in the next sentence and all column would be zero. So the idea behind the sparse matrix is to save some space because of that issue.\n\nHowever, if we'd like to feed a new sentence into this model and try to get matrix representation, we may have an issue though. Since our model is fitted on the sentences we already have, if a new sentence doesn't have a token that could be recognized by our model, that token wouldn't be represented. In this situation we simply neeed to include append our new message to our original collection and then refit and transform to make sure we don\u2019t lose this information. ","0d43b218":"- **NER**: Named entities are available as the ***ents*** property of a doc:","aad868e8":"### Let's apply these NLP procedures with Spacy package"}}