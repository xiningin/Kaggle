{"cell_type":{"bf32801e":"code","cc04bc18":"code","c7d85a00":"code","0264b35c":"code","977e302c":"code","d604810d":"code","9b43b48e":"code","3fb69112":"code","7e5775a9":"code","035bce6c":"code","f5003aec":"code","463cb240":"code","bc95c82e":"code","89d3188e":"code","6eae2daf":"code","2e9cedb3":"code","0cd02bfe":"code","81de5e15":"code","9a2b95b1":"code","d29aef41":"code","d67a9875":"code","3356c176":"code","ba9dce4a":"code","fd908097":"code","52b38454":"code","32b32565":"code","ce12bef7":"code","6dfc301d":"code","0faa0b57":"code","48f32996":"code","b0b9b79e":"code","daa880e2":"code","f9f5184e":"code","b632b264":"code","974ae365":"code","56061645":"code","57049a49":"code","8e5c53cc":"code","326be4b5":"code","33280ad5":"code","a02b473b":"code","0f3bf20b":"code","06f5d38e":"code","58efcdb6":"code","1189983e":"code","79453b14":"code","ebe135f3":"code","c7d6f2fd":"code","fdc7d31f":"code","3af30f8e":"code","3c9a87e8":"code","b5457f5e":"code","fdc01385":"code","4d14cd07":"code","14295f87":"code","cb5c4f80":"code","4e4debfc":"code","57104d4d":"code","3a43567f":"code","43ad66f8":"code","a6303f84":"code","fada7a2c":"code","a5aca5e5":"code","22654ca9":"code","5a7c2ebe":"code","a303deea":"code","ea1be40b":"code","ac02a68c":"code","e9399b02":"code","66ff37ea":"code","a8c90352":"markdown","3b41a86a":"markdown","7eb03042":"markdown","bf6b64be":"markdown","d0069970":"markdown","74e59b49":"markdown","0f5e811e":"markdown","fc65c260":"markdown","33695e15":"markdown","2be2d5df":"markdown","5e51afaf":"markdown","d3770728":"markdown","bc411a7f":"markdown","dbc92a35":"markdown","0443c581":"markdown","cf329046":"markdown","f1798984":"markdown","52ef9409":"markdown","10da6e01":"markdown","a54d9ad2":"markdown","d0a38567":"markdown","4a018db0":"markdown","1be2017b":"markdown"},"source":{"bf32801e":"import pandas as pd\nimport numpy as np","cc04bc18":"# Importing Housing.csv\nhousing = pd.read_csv('..\/input\/housing\/Housing.csv')","c7d85a00":"# Looking at the first five rows\nhousing.head()","0264b35c":"# What type of values are stored in the columns?\nhousing.info()","977e302c":"# Converting Yes to 1 and No to 0\nhousing['mainroad'] = housing['mainroad'].map({'yes': 1, 'no': 0})\nhousing['guestroom'] = housing['guestroom'].map({'yes': 1, 'no': 0})\nhousing['basement'] = housing['basement'].map({'yes': 1, 'no': 0})\nhousing['hotwaterheating'] = housing['hotwaterheating'].map({'yes': 1, 'no': 0})\nhousing['airconditioning'] = housing['airconditioning'].map({'yes': 1, 'no': 0})\nhousing['prefarea'] = housing['prefarea'].map({'yes': 1, 'no': 0})","d604810d":"# Now let's see the head\nhousing.head()","9b43b48e":"# Creating a dummy variable for 'furnishingstatus'\nstatus = pd.get_dummies(housing['furnishingstatus'])","3fb69112":"# The result has created three variables that are not needed.\nstatus.head()","7e5775a9":"# we don't need 3 columns.\n# we can use drop_first = True to drop the first column from status df.\nstatus = pd.get_dummies(housing['furnishingstatus'],drop_first=True)\n","035bce6c":"status.head()","f5003aec":"#Adding the results to the master dataframe\nhousing = pd.concat([housing,status],axis=1)","463cb240":"# Now let's see the head of our dataframe.\nhousing.head()","bc95c82e":"# Dropping furnishingstatus as we have created the dummies for it\nhousing.drop(['furnishingstatus'],axis=1,inplace=True)","89d3188e":"# Now let's see the head of our dataframe.\nhousing.head()","6eae2daf":"# Let us create the new metric and assign it to \"areaperbedroom\"\nhousing['areaperbedroom'] = housing['area']\/housing['bedrooms']","2e9cedb3":"# Metric:bathrooms per bedroom\nhousing['bbratio'] = housing['bathrooms']\/housing['bedrooms']","0cd02bfe":"housing.head()","81de5e15":"#defining a normalisation function \ndef normalize (x): \n    return ( (x-np.min(x))\/ (max(x) - min(x)))\n                                            \n                                              \n# applying normalize ( ) to all columns \nhousing = housing.apply(normalize) ","9a2b95b1":"housing.columns","d29aef41":"# Putting feature variable to X\nX = housing[['area', 'bedrooms', 'bathrooms', 'stories', 'mainroad',\n       'guestroom', 'basement', 'hotwaterheating', 'airconditioning',\n       'parking', 'prefarea', 'semi-furnished', 'unfurnished',\n       'areaperbedroom', 'bbratio']]\n\n# Putting response variable to y\ny = housing['price']","d67a9875":"#random_state is the seed used by the random number generator, it can be any integer.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7 ,test_size = 0.3, random_state=100)","3356c176":"import statsmodels.api as sm          # Importing statsmodels\nX_train = sm.add_constant(X_train)    # Adding a constant column to our dataframe\n# create a first fitted model\nlm_1 = sm.OLS(y_train,X_train).fit()","ba9dce4a":"#Let's see the summary of our first linear model\nprint(lm_1.summary())","fd908097":"# UDF for calculating vif value\ndef vif_cal(input_data, dependent_col):\n    vif_df = pd.DataFrame( columns = ['Var', 'Vif'])\n    x_vars=input_data.drop([dependent_col], axis=1)\n    xvar_names=x_vars.columns\n    for i in range(0,xvar_names.shape[0]):\n        y=x_vars[xvar_names[i]] \n        x=x_vars[xvar_names.drop(xvar_names[i])]\n        rsq=sm.OLS(y,x).fit().rsquared  \n        vif=round(1\/(1-rsq),2)\n        vif_df.loc[i] = [xvar_names[i], vif]\n    return vif_df.sort_values(by = 'Vif', axis=0, ascending=False, inplace=False)","52b38454":"# Calculating Vif value|\nvif_cal(input_data=housing, dependent_col=\"price\")","32b32565":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ce12bef7":"# Let's see the correlation matrix \nplt.figure(figsize = (16,10))     # Size of the figure\nsns.heatmap(housing.corr(),annot = True)","6dfc301d":"# Dropping highly correlated variables and insignificant variables\nX_train = X_train.drop('bbratio', 1)","0faa0b57":"# Create a second fitted model\nlm_2 = sm.OLS(y_train,X_train).fit()","48f32996":"#Let's see the summary of our second linear model\nprint(lm_2.summary())","b0b9b79e":"# Calculating Vif value\nvif_cal(input_data=housing.drop([\"bbratio\"], axis=1), dependent_col=\"price\")","daa880e2":"# Dropping highly correlated variables and insignificant variables\nX_train = X_train.drop('bedrooms', 1)","f9f5184e":"# Create a third fitted model\nlm_3 = sm.OLS(y_train,X_train).fit()","b632b264":"#Let's see the summary of our third linear model\nprint(lm_3.summary())","974ae365":"# Calculating Vif value\nvif_cal(input_data=housing.drop([\"bedrooms\",\"bbratio\"], axis=1), dependent_col=\"price\")","56061645":"# # Dropping highly correlated variables and insignificant variables\nX_train = X_train.drop('areaperbedroom', 1)","57049a49":"# Create a fourth fitted model\nlm_4 = sm.OLS(y_train,X_train).fit()","8e5c53cc":"#Let's see the summary of our fourth linear model\nprint(lm_4.summary())","326be4b5":"# Calculating Vif value\nvif_cal(input_data=housing.drop([\"bedrooms\",\"bbratio\",\"areaperbedroom\"], axis=1), dependent_col=\"price\")","33280ad5":"# # Dropping highly correlated variables and insignificant variables\nX_train = X_train.drop('semi-furnished', 1)","a02b473b":"# Create a fifth fitted model\nlm_5 = sm.OLS(y_train,X_train).fit()","0f3bf20b":"#Let's see the summary of our fifth linear model\nprint(lm_5.summary())","06f5d38e":"# Calculating Vif value\nvif_cal(input_data=housing.drop([\"bedrooms\",\"bbratio\",\"areaperbedroom\",\"semi-furnished\"], axis=1), dependent_col=\"price\")","58efcdb6":"# # Dropping highly correlated variables and insignificant variables\nX_train = X_train.drop('basement', 1)","1189983e":"# Create a sixth fitted model\nlm_6 = sm.OLS(y_train,X_train).fit()","79453b14":"#Let's see the summary of our sixth linear model\nprint(lm_6.summary())","ebe135f3":"# Calculating Vif value\nvif_cal(input_data=housing.drop([\"bedrooms\",\"bbratio\",\"areaperbedroom\",\"semi-furnished\",\"basement\"], axis=1), dependent_col=\"price\")","c7d6f2fd":"# Adding  constant variable to test dataframe\nX_test_m6 = sm.add_constant(X_test)","fdc7d31f":"# Creating X_test_m6 dataframe by dropping variables from X_test_m6\nX_test_m6 = X_test_m6.drop([\"bedrooms\",\"bbratio\",\"areaperbedroom\",\"semi-furnished\",\"basement\"], axis=1)","3af30f8e":"# Making predictions\ny_pred_m6 = lm_6.predict(X_test_m6)","3c9a87e8":"# Actual vs Predicted\nc = [i for i in range(1,165,1)]\nfig = plt.figure()\nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\")     #Plotting Actual\nplt.plot(c,y_pred_m6, color=\"red\",  linewidth=2.5, linestyle=\"-\")  #Plotting predicted\nfig.suptitle('Actual and Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('Housing Price', fontsize=16)                       # Y-label","b5457f5e":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred_m6)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","fdc01385":"# Error terms\nfig = plt.figure()\nc = [i for i in range(1,165,1)]\nplt.plot(c,y_test-y_pred_m6, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('ytest-ypred', fontsize=16)                # Y-label","4d14cd07":"# Plotting the error terms to understand the distribution.\nfig = plt.figure()\nsns.distplot((y_test-y_pred_m6),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test-y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label","14295f87":"import numpy as np\nfrom sklearn import metrics\nprint('RMSE :', np.sqrt(metrics.mean_squared_error(y_test, y_pred_m6)))","cb5c4f80":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","4e4debfc":"# Running RFE with the output number of the variable equal to 9\nlm = LinearRegression()\nrfe = RFE(lm, 9)             # running RFE\nrfe = rfe.fit(X_train, y_train)\nprint(rfe.support_)           # Printing the boolean results\nprint(rfe.ranking_)  ","57104d4d":"col = X_train.columns[rfe.support_]","3a43567f":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","43ad66f8":"import statsmodels.api as sm  \nX_train_rfe = sm.add_constant(X_train_rfe)","a6303f84":"lm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model","fada7a2c":"#Let's see the summary of our linear model\nprint(lm.summary())","a5aca5e5":"vif_cal(input_data=housing.drop(['area','bedrooms','stories','basement','semi-furnished','areaperbedroom'], axis=1), dependent_col=\"price\")","22654ca9":"# Now let's use our model to make predictions.\n\n# Creating X_test_6 dataframe by dropping variables from X_test\nX_test_rfe = X_test[col]\n\n# Adding a constant variable \nX_test_rfe = sm.add_constant(X_test_rfe)\n\n# Making predictions\ny_pred = lm.predict(X_test_rfe)","5a7c2ebe":"# Now let's check how well our model is able to make predictions.\n\n# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a303deea":"# Actual and Predicted\nimport matplotlib.pyplot as plt\nc = [i for i in range(1,165,1)] # generating index \nfig = plt.figure() \nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\") #Plotting Actual\nplt.plot(c,y_pred, color=\"red\",  linewidth=2.5, linestyle=\"-\") #Plotting predicted\nfig.suptitle('Actual and Predicted', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('Housing Price', fontsize=16)                       # Y-label","ea1be40b":"# Error terms\nc = [i for i in range(1,165,1)]\nfig = plt.figure()\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Error Terms', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                      # X-label\nplt.ylabel('ytest-ypred', fontsize=16)                # Y-label","ac02a68c":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label","e9399b02":"# Plotting the error terms to understand the distribution.\nfig = plt.figure()\nsns.distplot((y_test-y_pred),bins=50)\nfig.suptitle('Error Terms', fontsize=20)                  # Plot heading \nplt.xlabel('y_test-y_pred', fontsize=18)                  # X-label\nplt.ylabel('Index', fontsize=16)                          # Y-label","66ff37ea":"# Now let's check the Root Mean Square Error of our model.\nimport numpy as np\nfrom sklearn import metrics\nprint('RMSE :', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","a8c90352":"## Making Predictions Using the Final Model","3b41a86a":"#### Creating a new variable","7eb03042":"## Housing Case Study","bf6b64be":"**Building model using Sklearn**","d0069970":"### Dropping the Variable and Updating the Model","74e59b49":"### Rescaling the Features \nIt is extremely important to rescale the variables so that they have a comparable scale. \nThere are twocoon ways of rescaling \n1. Normalisation (min-max scaling) and \n2. standardisation (mean-o, sigma-1) \nLet's try normalisation","0f5e811e":"### Checking VIF","fc65c260":"### Dropping the Variable and Updating the Model","33695e15":"### Dropping the Variable and Updating the Model","2be2d5df":"Problem Statement:\n\nConsider a real estate company that has a dataset containing the prices of properties. It wishes to use the data to optimise the sale prices of the properties based on important factors such as area, bedrooms, parking, etc.\n\nEssentially, the company wants \u2014\n\n\n- To identify the variables affecting house prices, e.g. area, number of rooms, bathrooms, etc.\n\n- To create a linear model that quantitatively relates house prices with variables such as number of rooms, area, number of bathrooms, etc.\n\n- To know the accuracy of the model, i.e. how well these variables can predict house prices.","5e51afaf":"### Prediction with Model 6","d3770728":"## Building a linear model","bc411a7f":"## Splitting Data into Training and Testing Sets","dbc92a35":"### Dropping the Variable and Updating the Model","0443c581":"## Correlation matrix","cf329046":"#### Assessment  question\n#### Design four models by dropping all the variables one by one with high vif (>5). Then, compare the results.","f1798984":"- You can see that your dataset has many columns with values as 'Yes' or 'No'.\n\n- We need to convert them to 1s and 0s, where 1 is a 'Yes' and 0 is a 'No'.","52ef9409":"### Dropping the Variable and Updating the Model","10da6e01":"## Model Evaluation","a54d9ad2":"** Model refinement using RFE**<br\/>\nIf we have more number of columns,the above process takes more time as we dropping one column at a time and checking the OLS table. By RFF, this can be done automatically. <br\/>\nThe goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a featureimportances attribute. Then, the less important features are pruned from the the current set of features. This procedure is recursively repeated on the pruned dataset until the desired number of features to select is reached.","d0a38567":"The variable 'furnishingstatus' had three levels. We need to convert it to integer.","4a018db0":"### Data Preparation","1be2017b":"### Importing and Understanding Data"}}