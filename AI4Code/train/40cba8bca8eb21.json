{"cell_type":{"4604cd59":"code","670a2f3d":"code","7a181199":"code","c15ebe0f":"code","20e4201d":"code","656e5048":"code","951d4a63":"code","f9c33467":"code","6c15e1f3":"code","ebbaf630":"code","e3b991d9":"code","c943ed6b":"code","f3a055b7":"code","d3cc2881":"code","e898d910":"code","3e655a00":"code","bf3fbdcb":"code","751061bf":"code","80c067d0":"code","89fadd37":"code","e1ace030":"code","f53dca57":"code","4648de61":"code","5fb88ab4":"code","612ad0b6":"code","b22b5e1b":"code","162fcc3e":"code","4efbe193":"code","bcb3424d":"code","7412e5ad":"code","144ba526":"code","15701289":"code","d464015d":"code","96572a46":"code","35831a34":"code","57491f5b":"code","06ad98b0":"code","0130b754":"code","fb199326":"code","15a058e8":"code","607e4eb0":"code","aa7565d0":"code","878b49ba":"code","84cea279":"markdown","d879e417":"markdown","cdb51d79":"markdown","b63bb989":"markdown","a4009f9d":"markdown","247a138f":"markdown","a0ab921d":"markdown","c740ab59":"markdown","14e1e97a":"markdown","27bcc1c3":"markdown","bcca8ed8":"markdown"},"source":{"4604cd59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","670a2f3d":"#import required packages\n#basics\nimport pandas as pd \nimport numpy as np\n\n#misc\nimport gc\nimport time\nimport warnings\n\n#stats\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\n\n#viz\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec \nimport seaborn as sns\nfrom wordcloud import WordCloud ,STOPWORDS\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \n\n\n#FeatureEngineering\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import f1_score\n\n# model\nimport lightgbm as lgb\n\neng_stopwords = set(stopwords.words(\"english\"))","7a181199":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","c15ebe0f":"print(\"The shape of train data frame: %s\" % str(train_df.shape))\nprint(\"The shape of test data frame: %s\" % str(test_df.shape))","20e4201d":"print(\"The main columns: %s\" % str(train_df.columns.values))","656e5048":"train_df.head()","951d4a63":"pos_num = train_df[train_df.target == 1].shape[0]\nneg_num = train_df[train_df.target == 0].shape[0]\nprint(\"The number of positive %d, and the number of negative %d\" % (pos_num, neg_num))\nprint(\"The rate of postive %.5f\" % (pos_num \/ train_df.shape[0]))","f9c33467":"train_df[train_df.target == 1].question_text.values[:10].tolist()","6c15e1f3":"def generate_indirect_features(df):\n    df['count_word'] = df.question_text.apply(lambda x: len(str(x).split()))\n    df['count_unique_word']=df.question_text.apply(lambda x: len(set(str(x).split())))\n    df['count_letters']=df.question_text.apply(lambda x: len(str(x)))\n    df[\"count_punctuations\"] =df.question_text.apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    df[\"count_words_upper\"] = df.question_text.apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n    df[\"count_words_title\"] = df.question_text.apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n    df[\"count_stopwords\"] = df.question_text.apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n    df[\"mean_word_len\"] = df.question_text.apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    df['word_unique_percent']=df['count_unique_word']*100\/df['count_word']\n    df['punct_percent']=df['count_punctuations']*100\/df['count_word']\n    return df\n\ntrain_df = generate_indirect_features(train_df)\ntest_df = generate_indirect_features(test_df)","ebbaf630":"def violin_chart(df, column_name, min_clip=None, max_clip=None, title=None):\n    title = column_name if title is None else title\n    plt.figure(figsize=(12, 6))\n    plt.title(title, fontsize=15)\n    sub_df = df[[column_name, 'target']]\n    min_clip = np.min(sub_df[column_name]) if min_clip is None else min_clip\n    max_clip = np.max(sub_df[column_name]) if max_clip is None else max_clip\n    sub_df[column_name] = np.clip(df[column_name].values, min_clip, max_clip)\n    sns.violinplot(y=column_name, x='target', data=sub_df, split=True, innert=\"quart\")\n    plt.xlabel(\"Is Isincere?\", fontsize=12)\n    plt.ylabel(column_name, fontsize=12)\n    plt.show()","e3b991d9":"for column_name in train_df.columns.values:\n    if column_name not in ['qid', 'question_text', 'target']:\n        print(column_name, end=\", \")","c943ed6b":"violin_chart(train_df, \"count_word\")\nviolin_chart(train_df, \"count_unique_word\")\nviolin_chart(train_df, \"count_letters\")\nviolin_chart(train_df, \"count_punctuations\", max_clip=20)\nviolin_chart(train_df, \"count_words_upper\", max_clip=25)\nviolin_chart(train_df, \"count_words_title\")\nviolin_chart(train_df, \"count_stopwords\")\nviolin_chart(train_df, \"mean_word_len\")\nviolin_chart(train_df, \"word_unique_percent\")\nviolin_chart(train_df, \"punct_percent\", max_clip=70)","f3a055b7":"tf_idf_vector = TfidfVectorizer(strip_accents='unicode', analyzer='word',ngram_range=(1,1),\n                                use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english')\ntrain_vect = tf_idf_vector.fit_transform(train_df.question_text.values)\nprint(\"The shape of TF-IDF train matrix: %s\" % str(train_vect.shape))","d3cc2881":"def top_tfidf_words(tfidf_, words, top_n=25):\n    topn_ids = np.argsort(tfidf_)[::-1][:top_n]\n    top_words = [(words[i], tfidf_[i]) for i in topn_ids]\n    df = pd.DataFrame(top_words)\n    df.columns = ['word', 'tfidf']\n    return df\n\ndef top_mean_words(tf_idf_matrix, words, grp_ids, min_tfidf=0.1, top_n=25):\n    _matrix = tf_idf_matrix[grp_ids]\n#     _matrix[_matrix < min_tfidf] = 0\n    tfidf_means = _matrix.mean(axis=0)\n    tfidf_means = np.asarray(tfidf_means).reshape(-1)\n    return top_tfidf_words(tfidf_means, words, top_n)\n\ndef top_words_by_target(tf_idf_matrix, words, min_tfidf=0.1, top_n=20):\n    pos_idx = train_df.index[train_df.target == 1].values\n    neg_idx = train_df.index[train_df.target == 0].values\n    return top_mean_words(tf_idf_matrix, words, pos_idx, min_tfidf, top_n), top_mean_words(tf_idf_matrix, words, neg_idx, min_tfidf, top_n)","e898d910":"pos_top_tfidf, neg_top_tfidf = top_words_by_target(train_vect, tf_idf_vector.get_feature_names())","3e655a00":"pos_top_tfidf.head()","bf3fbdcb":"trace = go.Bar(\n    x = pos_top_tfidf.word,\n    y = pos_top_tfidf.tfidf\n)\nlayout = dict(\n    title = \"Mean TF-IDF of word in positive\",\n    xaxis = dict(title = 'Word'),\n    yaxis = dict(title = 'Mean TF-IDF')\n)\ndata = [trace]\n\npy.iplot(dict(data = data, layout = layout), filename = 'basic-line')","751061bf":"neg_top_tfidf.head()","80c067d0":"trace = go.Bar(\n    x = neg_top_tfidf.word,\n    y = neg_top_tfidf.tfidf\n)\n\nlayout = dict(\n    title = \"Mean TF-IDF of word in negative\",\n    xaxis = dict(title = 'Word'),\n    yaxis = dict(title = 'Mean TF-IDF')\n)\n\ndata = [trace]\n\npy.iplot(dict(data = data, layout = layout), filename = 'basic-line')","89fadd37":"tf_idf_vector = TfidfVectorizer(strip_accents='unicode', analyzer='word',ngram_range=(2,2),\n                                use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english')\ntrain_vect = tf_idf_vector.fit_transform(train_df.question_text.values)\nprint(\"The shape of bi-gram TF-IDF train matrix: %s\" % str(train_vect.shape))","e1ace030":"pos_top_tfidf, neg_top_tfidf = top_words_by_target(train_vect, tf_idf_vector.get_feature_names())","f53dca57":"pos_top_tfidf.head()","4648de61":"neg_top_tfidf.head()","5fb88ab4":"trace = go.Bar(\n    x = pos_top_tfidf.word,\n    y = pos_top_tfidf.tfidf\n)\n\nlayout = dict(\n    title = \"Mean TF-IDF of word in positive (Bigram)\",\n    xaxis = dict(title = 'Word'),\n    yaxis = dict(title = 'Mean TF-IDF')\n)\n\ndata = [trace]\n\npy.iplot(dict(data = data, layout = layout), filename = 'basic-line')","612ad0b6":"trace = go.Bar(\n    x = neg_top_tfidf.word,\n    y = neg_top_tfidf.tfidf\n)\n\nlayout = dict(\n    title = \"Mean TF-IDF of word in negative (Bigram)\",\n    xaxis = dict(title = 'Word'),\n    yaxis = dict(title = 'Mean TF-IDF')\n)\n\ndata = [trace]\n\npy.iplot(dict(data = data, layout = layout), filename = 'basic-line')","b22b5e1b":"count_vector = CountVectorizer(strip_accents='unicode', analyzer='word',ngram_range=(1,1),  stop_words = 'english')\ntrain_vect = count_vector.fit_transform(train_df.question_text.values)\nprint(\"The shape of count train matrix: %s\" % str(train_vect.shape))","162fcc3e":"pos_top_count, neg_top_count = top_words_by_target(train_vect, count_vector.get_feature_names())","4efbe193":"pos_top_count.head()","bcb3424d":"trace = go.Bar(\n    x = pos_top_count.word,\n    y = pos_top_count.tfidf\n)\n\nlayout = dict(\n    title = \"Mean Count Rate of word in positive (Bigram)\",\n    xaxis = dict(title = 'Word'),\n    yaxis = dict(title = 'Mean Count Rate')\n)\n\ndata = [trace]\n\npy.iplot(dict(data = data, layout = layout), filename = 'basic-line')","7412e5ad":"neg_top_count.head()","144ba526":"trace = go.Bar(\n    x = neg_top_count.word,\n    y = neg_top_count.tfidf\n)\n\nlayout = dict(\n    title = \"Mean Count Rate of word in negative (Bigram)\",\n    xaxis = dict(title = 'Word'),\n    yaxis = dict(title = 'Mean Count Rate')\n)\n\ndata = [trace]\n\npy.iplot(dict(data = data, layout = layout), filename = 'basic-line')","15701289":"def count_keywords(df, word):\n    df[\"count_%s\" % word] = df.question_text.apply(lambda x: x.lower().count(word))","d464015d":"train_df['count_muslim'] = train_df.question_text.apply(lambda x: x.lower().count(\"muslim\"))\ntest_df['count_muslim'] = test_df.question_text.apply(lambda x: x.lower().count(\"muslim\"))","96572a46":"violin_chart(train_df, \"count_muslim\")","35831a34":"keywords = [\"trump\", \"chinese people\", \"black\", \"white people\", \"indians\", \"muslims\", \"sex\", \"india\"]\nfor keyword in keywords:\n    count_keywords(train_df, keyword)\n    count_keywords(test_df, keyword)","57491f5b":"tf_idf_word_vector = TfidfVectorizer(strip_accents='unicode', analyzer='word',ngram_range=(1, 2), \n                                     use_idf=1, smooth_idf=1, sublinear_tf=1, stop_words = 'english', \n                                     max_features=200000)\n\n# tf_idf_char_vector = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', \n#                                      analyzer='char', token_pattern=r'\\w{1,}',stop_words='english',\n#                                      ngram_range=(2, 5), max_features=50000)\n# tf_idf_char_vector = TfidfVectorizer(sublinear_tf=True, strip_accents='unicode', analyzer='char_wb', token_pattern=r'\\w{1,}',\n#                                      stop_words='english', ngram_range=(2, 5), max_features=50000)\n\ntrain_word_tfidf = tf_idf_word_vector.fit_transform(train_df.question_text)\n# train_char_tfidf = tf_idf_char_vector.fit_transform(train_df.question_text)\n# train_tfidf = sparse.hstack([train_word_tfidf, train_char_tfidf]).tocsr()\ntrain_tfidf = train_word_tfidf\ndel train_word_tfidf\n# del train_char_tfidf\ngc.collect()\nprint(\"The train tf-idf shape %s\" % str(train_tfidf.shape))\n\ntest_word_tfidf = tf_idf_word_vector.transform(test_df.question_text)\n# test_char_tfidf = tf_idf_char_vector.transform(test_df.question_text)\n# test_tfidf = sparse.hstack([test_word_tfidf, test_char_tfidf]).tocsr()\ntest_tfidf = test_word_tfidf\ndel test_word_tfidf\n# del test_char_tfidf\ngc.collect()\n\nprint(\"The test tf-idf shape %s\" % str(test_tfidf.shape))","06ad98b0":"indirect_features_name = [feat_name for feat_name in train_df.columns.values if feat_name not in [\"qid\", \"question_text\", \"target\"]]","0130b754":"# indirect_features_name\ntrain_indirect_features = train_df[indirect_features_name].values\ntest_indirect_features = test_df[indirect_features_name].values\ntarget = train_df.target.values\n\n# prepare to delete the train dataframe and test dataframe\nnum_train = train_df.shape[0]\nsubmission_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nvalidation_df = pd.DataFrame({\"qid\":train_df[\"qid\"].values})\ndel train_df\ndel test_df\ngc.collect()\n\nX_train = sparse.hstack([train_tfidf, train_indirect_features]).tocsr()\nX_test = sparse.hstack([test_tfidf, test_indirect_features]).tocsr()\ndel train_tfidf\ndel test_tfidf\ngc.collect()","fb199326":"def lgb_f1_score(y_pre, data):\n    y_true = data.get_label()\n    best_f1 = f1_score(y_true, (y_pre>0.5).astype(int))\n#     best_f1 = 0\n#     for thresh in np.arange(0.1, 0.501, 0.01):\n#         thresh = np.round(thresh, 2)\n#         _f1 = f1_score(y_true, (y_pre>thresh).astype(int))\n#         if _f1 > best_f1:\n#             best_f1 = _f1\n    return 'f1', best_f1, True","15a058e8":"# Set LGBM parameters\nparams = {\n    \"objective\": \"binary\",\n    'metric': {'auc'},\n    \"boosting_type\": \"gbdt\",\n    \"verbosity\": -1,\n    \"num_threads\": 4,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 0.8,\n    \"learning_rate\": 0.1,\n    \"num_leaves\": 31,\n    \"verbose\": -1,\n    \"min_split_gain\": .1,\n    \"reg_alpha\": .1,\n    \"device_type\": \"gpu\",\n    \"seed\": 2018\n}\n\nscores = []\nfolds = KFold(n_splits=5)\nindices = np.arange(num_train)\ntrn_lgbset = lgb.Dataset(data=X_train, label=target, free_raw_data=False)\nvalid_predict = np.zeros(num_train, dtype=np.float32)\nmean_best_iter = 0\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(indices)):\n    print(\"valid in the %d fold\" % (n_fold + 1))\n    model = lgb.train(\n        params=params,\n        train_set=trn_lgbset.subset(trn_idx),\n        num_boost_round=1000,\n        valid_sets=[trn_lgbset.subset(val_idx)],\n        early_stopping_rounds=50,\n#         feval=lgb_f1_score,\n        verbose_eval=200\n    )\n    mean_best_iter += model.best_iteration \/ 5\n    valid_predict[val_idx] = model.predict(trn_lgbset.data[val_idx], num_iteration=model.best_iteration)","607e4eb0":"best_thresh = 0\nbest_f1 = 0\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    _f1 = f1_score(target, (valid_predict>thresh).astype(int))\n    if _f1 > best_f1:\n        best_f1 = _f1\n        best_thresh = thresh\n    print(\"\\tF1 score at threshold {0} is {1}\".format(thresh, _f1))\n\nprint(\"Best F1 score {0}, Best thresh {1}\".format(best_f1, best_thresh))","aa7565d0":"model = lgb.train(\n    params=params,\n    train_set=trn_lgbset,\n    feval=lgb_f1_score,\n    num_boost_round=int(mean_best_iter)\n)\n\npredict = model.predict(X_test, num_iteration=model.best_iteration)","878b49ba":"validation_df[\"prediction\"] = valid_predict\nvalidation_df.to_csv(\"validation.csv\", index=False)\n\nsubmission_df[\"prediction\"] = (predict > best_thresh).astype(int)\nsubmission_df.to_csv(\"submission.csv\", index=False)","84cea279":"we can see the rate of postive sample is 6.1%, just little postive samples. So if we use deep learning to predict the test samples, we should pay attention to overfit to negative sample.","d879e417":"**This is the first time I wrote the Pulibc kernel. I hope everyone can help me upvote. Thx**","cdb51d79":"emmm..... It is possible that the keywords not captured by countvector and tf-idf vector are basically the same.\n\n### keywords analysis\n\nAs we have guessed some of the keywords above, we try to draw a distribution containing the number of these keywords.\n\n#### Muslim\n\nThroughout the years, due to terrorist attacks and Trump\u2019s appointment, the word Muslim has become racist.","b63bb989":"we can find that \n1. The words used in most texts are unique.\n2. The number of uppercase words in a positive sample is relatively small\n\n### 2. TF-IDF analysis","a4009f9d":"We found that the negative sample basically does not have the word Muslim, and the positive sample appears relatively more.\n\n### Build Models","247a138f":"## check the number of positive and negative","a0ab921d":"### 1. violin chart","c740ab59":"Here in the positive sample n-gram keywords: Donald Trump, chinese people, black people, white people. So we can know why \u201cpeople\u201d become the keyword in the word. Maybe bi-gram can catch best keywords for model as features.\n\n### Count words analysis","14e1e97a":"we can find the topic of positive samples are sensitive","27bcc1c3":"## analysis features","bcca8ed8":"We found that in the positive sample, there are some keywords: Muslim, white, black, sex. Some of the key factors in the negative sample may be: best, good. Maybe we can catch those in our features.\n\n**Bi-gram TF-IDF**"}}