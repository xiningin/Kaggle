{"cell_type":{"85d02a0a":"code","6e61dde7":"code","f96a3fa9":"code","6502583a":"code","0ddac9e3":"code","6b2fd94a":"code","4b8ded54":"code","49dca9c7":"code","9f3d2163":"code","ffb3eded":"code","36614534":"code","da406b53":"code","99c3efe1":"code","2382c2eb":"code","26696263":"code","18ccbbc6":"code","cdbbd3ba":"code","3cf2f667":"code","463f4d36":"code","85aad085":"code","c96c48d9":"code","83a510be":"code","f9cac16f":"code","d4af99ba":"code","2ccfc5f6":"code","c465c7e5":"code","3a8f0df7":"code","883a883e":"code","593b28f0":"markdown","46f3e35e":"markdown","6ea7b44f":"markdown","6b31c04a":"markdown","73c7e57d":"markdown","0807eecb":"markdown","c0d38232":"markdown"},"source":{"85d02a0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e61dde7":"#Label extraction on training data\nimport os\nlabels = []\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/0'):\n    labels.append(0)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/1'):\n    labels.append(1)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/2'):\n    labels.append(2)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/3'):\n    labels.append(3)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/4'):\n    labels.append(4)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/5'):\n    labels.append(5)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/6'):\n    labels.append(6)\n   ","f96a3fa9":"#feature extraction on training data\nimport cv2\nloc1 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/0'\nloc2 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/1'\nloc3 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/2'\nloc4 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/3'\nloc5 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/4'\nloc6 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/5'\nloc7 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/train\/6'\nfeatures = []\nfrom tqdm import tqdm\nfor i in tqdm(os.listdir(loc1)):\n    features.append(cv2.imread(os.path.join(loc1,i),0))\n    \nfor i in tqdm(os.listdir(loc2)):\n    features.append(cv2.imread(os.path.join(loc2,i),0))\n    \nfor i in tqdm(os.listdir(loc3)):\n    features.append(cv2.imread(os.path.join(loc3,i),0))\n    \nfor i in tqdm(os.listdir(loc4)):\n    features.append(cv2.imread(os.path.join(loc4,i),0))\n    \nfor i in tqdm(os.listdir(loc5)):\n    features.append(cv2.imread(os.path.join(loc5,i),0))\n    \nfor i in tqdm(os.listdir(loc6)):\n    features.append(cv2.imread(os.path.join(loc6,i),0))\n    \nfor i in tqdm(os.listdir(loc7)):\n    features.append(cv2.imread(os.path.join(loc7,i),0))","6502583a":"#Label extraction on testing data\nimport os\nlabels_test = []\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/0'):\n    labels_test.append(0)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/1'):\n    labels_test.append(1)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/2'):\n    labels_test.append(2)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/3'):\n    labels_test.append(3)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/4'):\n    labels_test.append(4)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/5'):\n    labels_test.append(5)\nfor i in os.listdir('..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/6'):\n    labels_test.append(6)","0ddac9e3":"#feature extraction on testing data\nimport cv2\nloc1 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/0'\nloc2 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/1'\nloc3 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/2'\nloc4 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/3'\nloc5 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/4'\nloc6 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/5'\nloc7 = '..\/input\/facial-expression-dataset-image-folders-fer2013\/data\/test\/6'\ntest_features = []\nfrom tqdm import tqdm\nfor i in tqdm(os.listdir(loc1)):\n    test_features.append(cv2.imread(os.path.join(loc1,i),0))\n    \nfor i in tqdm(os.listdir(loc2)):\n    test_features.append(cv2.imread(os.path.join(loc2,i),0))\n    \nfor i in tqdm(os.listdir(loc3)):\n    test_features.append(cv2.imread(os.path.join(loc3,i),0))\n    \nfor i in tqdm(os.listdir(loc4)):\n    test_features.append(cv2.imread(os.path.join(loc4,i),0))\n    \nfor i in tqdm(os.listdir(loc5)):\n    test_features.append(cv2.imread(os.path.join(loc5,i),0))\n    \nfor i in tqdm(os.listdir(loc6)):\n    test_features.append(cv2.imread(os.path.join(loc6,i),0))\n    \nfor i in tqdm(os.listdir(loc7)):\n    test_features.append(cv2.imread(os.path.join(loc7,i),0))","6b2fd94a":"import pandas as pd\ntrain_data = pd.DataFrame()\ntest_data = pd.DataFrame()","4b8ded54":"train_data['emotion'] = labels\ntrain_data['pixel_values'] = features\ntest_data['emotion'] = labels_test\ntest_data['pixel_values'] = test_features","49dca9c7":"train_data.head()","9f3d2163":"test_data.head()","ffb3eded":"emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n\ndef setup_axe(axe,df,title):\n    df['emotion'].value_counts(sort=False).plot(ax=axe, kind='bar', rot=0)\n    axe.set_xticklabels(emotion_labels)\n    axe.set_xlabel(\"Emotions\")\n    axe.set_ylabel(\"Count\")\n    axe.set_title(title)\n    \n    # set individual bar lables using above list\n    for i in axe.patches:\n         axe.text(i.get_x()-.05, i.get_height()+120, \\\n                str(round((i.get_height()), 2)), fontsize=14, color='red',\n                    rotation=0)\n\nimport matplotlib.pyplot as plt   \nfig, axes = plt.subplots(1,2, figsize=(20,8), sharey=True)\nsetup_axe(axes[0],train_data,'train')\nsetup_axe(axes[1],test_data,'test')\nplt.show()","36614534":"import numpy as np\nfrom keras.utils import np_utils \nfeatures = np.array(features).reshape(-1,48,48,1)\ntest_features = np.array(test_features).reshape(-1,48,48,1)\n\nfeatures = features\/255\ntest_features = test_features\/255\n\nlabels = np_utils.to_categorical(labels)\nlabels_test =np_utils.to_categorical(labels_test)","da406b53":"print('Training features shape ',features.shape)\nprint('Training labels shape',labels.shape)\nprint('Testing features shape ',test_features.shape)\nprint('Testing labels shape',labels_test.shape)","99c3efe1":"from keras.models import Sequential\nfrom keras.layers import Dense , Activation , Dropout ,Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.metrics import categorical_accuracy\nfrom keras.models import model_from_json\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.optimizers import *\nfrom keras.layers.normalization import BatchNormalization\nfrom sklearn.metrics import accuracy_score\n\n\n","2382c2eb":"model = Sequential()\ninput_shape = (48,48,1)\nmodel.add(Conv2D(64, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\nmodel.add(Conv2D(64, (5, 5), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\nmodel.add(Conv2D(128, (5, 5),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\nmodel.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(7))\nmodel.add(Activation('softmax'))\n  \nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n","26696263":"model.summary()","18ccbbc6":"from keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', patience = 10, mode = 'min', restore_best_weights=True)","cdbbd3ba":"history = model.fit(x=features, \n            y=labels, \n            batch_size=64,\n            steps_per_epoch=len(features) \/ 64,\n            epochs=30, \n            verbose=1, \n            callbacks = [es],\n            validation_data=(test_features,labels_test),\n            shuffle=True)","3cf2f667":"import matplotlib.pyplot as plt\n# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n","463f4d36":"# Plot training & validation loss values\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n","85aad085":"test_true = np.argmax(labels_test, axis=1)\ntest_pred = np.argmax(model.predict(test_features), axis=1)\nprint(\"CNN Model Accuracy on testing data: {:.4f}\".format(accuracy_score(test_true, test_pred)))","c96c48d9":"from sklearn import metrics\n# Predicted values\ny_pred = test_pred\n# Actual values\ny_act = test_true \n# Printing the confusion matrix\n# The columns will show the instances predicted for each label,\n# and the rows will show the actual number of instances for each label.\nprint(metrics.confusion_matrix(y_act, y_pred, labels=[0,1,2,3,4,5,6]))\n# Printing the precision and recall, among other metrics\nprint(metrics.classification_report(y_act, y_pred, labels=[0,1,2,3,4,5,6]))","83a510be":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ncm = confusion_matrix(y_act, y_pred)\n# Normalise\ncmn = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=emotion_labels, yticklabels=emotion_labels)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show(block=False)","f9cac16f":"objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\ny_pos = np.arange(len(objects))\nprint(y_pos)","d4af99ba":"import matplotlib.pyplot as plt\ndef emotion_analysis(emotions):\n    objects = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n    y_pos = np.arange(len(objects))\n    plt.bar(y_pos, emotions, align='center', alpha=0.9)\n    plt.tick_params(axis='x', which='both', pad=10,width=4,length=10)\n    plt.xticks(y_pos, objects)\n    plt.ylabel('percentage')\n    plt.title('emotion')\n    plt.show()","2ccfc5f6":"from keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom skimage import io\ndef predict_image(pic):\n    img = image.load_img(pic, grayscale=True, target_size=(48, 48))\n    show_img=image.load_img(pic, grayscale=False, target_size=(200, 200))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis = 0)\n\n    x \/= 255\n\n    custom = model.predict(x)\n    \n    emotion_analysis(custom[0])\n\n    x = np.array(x, 'float32')\n    x = x.reshape([48, 48]);\n\n    plt.gray()\n    plt.imshow(show_img)\n    plt.show()\n\n    m=0.000000000000000000001\n    a=custom[0]\n    for i in range(0,len(a)):\n        if a[i]>m:\n            m=a[i]\n            ind=i\n        \n    print('Expression Prediction:',objects[ind])","c465c7e5":"predict_image('..\/input\/angryman\/angry.jpg')","3a8f0df7":"predict_image('..\/input\/hchildimg\/happychild.jpg')","883a883e":"predict_image('..\/input\/fearful\/fear.jpg')","593b28f0":"Standardization of the data****","46f3e35e":"Data Extraction on testing set****","6ea7b44f":"Confusion Matrix****","6b31c04a":"Data Extraction on training set****","73c7e57d":"CNN Model****","0807eecb":"Real- time prediction****","c0d38232":"Data Visualization****"}}