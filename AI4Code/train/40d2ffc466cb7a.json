{"cell_type":{"2b0890e7":"code","ee490081":"code","8b6f8627":"code","46cf73de":"code","0c78e54a":"code","bc7e8d96":"code","c3b57475":"code","62209e8e":"code","1f89881a":"code","9665957b":"code","cf428210":"code","3d63e2a9":"code","ebf0338e":"code","091a6c4e":"code","64abcae1":"code","412159b0":"code","85954bae":"code","c3335bbc":"code","76fe6a2f":"code","0021d6bb":"code","a047752d":"code","47558da2":"code","e630db7e":"code","7ba0fb36":"code","ce845676":"code","571d04ab":"code","f44cfdbb":"code","9da61025":"code","e42086a5":"code","f0449e1d":"code","c8444d93":"code","baf1780f":"code","41466348":"code","f88afbf6":"code","4f864f98":"code","d974918e":"code","da19a2fd":"code","951f740e":"code","f71645b8":"code","f8b9325b":"code","3c363c15":"code","f06e4e33":"code","b969e9a4":"code","c1b95c7f":"code","7481ce20":"code","b7b15086":"code","221a5960":"code","6c5e0844":"code","f5e96942":"code","8f7ce683":"code","558e622d":"code","2084c7c7":"code","b43b2fea":"code","36f351ed":"code","5394a009":"code","19fa6336":"code","ba842c58":"code","2100b903":"code","10f9b11d":"code","15513b89":"code","cd6150ce":"code","86affb21":"code","d4064c34":"code","8b9a3ec0":"code","f151ee4e":"markdown","22c6c867":"markdown","ea03c564":"markdown","7fcef3a2":"markdown","92e66de0":"markdown","bc2930f8":"markdown","430920b1":"markdown","64970362":"markdown","ee92bff1":"markdown","235a1a9d":"markdown","56fb9d94":"markdown","a39732b4":"markdown","03e17713":"markdown","a18ed55b":"markdown","d85cdd2f":"markdown","ea2f5e6c":"markdown","39ec54b8":"markdown","2e4e387b":"markdown","d822decb":"markdown","55d7f07f":"markdown","56027efe":"markdown","9e797a5b":"markdown","ccda0f32":"markdown","0132a46c":"markdown","755bbdb8":"markdown","2c56a3d9":"markdown","1497c2ee":"markdown","3a68cd29":"markdown","738f1844":"markdown","8ae5258a":"markdown","ac84ca74":"markdown","50a2334d":"markdown","5b5b1843":"markdown","fd1cc39b":"markdown","5e667de4":"markdown","9a0c45f3":"markdown","14e012db":"markdown"},"source":{"2b0890e7":"# `BertTokenizer.detokenize` is not in `tf-text` stable yet (currently 2.4.3).\n!pip install -q tf-nightly\n!pip install -q tensorflow-text-nightly","ee490081":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport os\nimport re\nimport random\nimport time\nimport collections, json","8b6f8627":"# GPU Check\ntf.config.list_physical_devices(\"GPU\")","46cf73de":"# @title Load objects detections model names\nALL_MODELS = {\n'CenterNet HourGlass104 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_512x512\/1',\n'CenterNet HourGlass104 Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_512x512_kpts\/1',\n'CenterNet HourGlass104 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_1024x1024\/1',\n'CenterNet HourGlass104 Keypoints 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_1024x1024_kpts\/1',\n'CenterNet Resnet50 V1 FPN 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v1_fpn_512x512\/1',\n'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v1_fpn_512x512_kpts\/1',\n'CenterNet Resnet101 V1 FPN 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet101v1_fpn_512x512\/1',\n'CenterNet Resnet50 V2 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v2_512x512\/1',\n'CenterNet Resnet50 V2 Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v2_512x512_kpts\/1',\n'EfficientDet D0 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d0\/1',\n'EfficientDet D1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d1\/1',\n'EfficientDet D2 768x768' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d2\/1',\n'EfficientDet D3 896x896' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d3\/1',\n'EfficientDet D4 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d4\/1',\n'EfficientDet D5 1280x1280' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d5\/1',\n'EfficientDet D6 1280x1280' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d6\/1',\n'EfficientDet D7 1536x1536' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d7\/1',\n'SSD MobileNet v2 320x320' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/2',\n'SSD MobileNet V1 FPN 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v1\/fpn_640x640\/1',\n'SSD MobileNet V2 FPNLite 320x320' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/fpnlite_320x320\/1',\n'SSD MobileNet V2 FPNLite 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/fpnlite_640x640\/1',\n'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet50_v1_fpn_640x640\/1',\n'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet50_v1_fpn_1024x1024\/1',\n'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet101_v1_fpn_640x640\/1',\n'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet101_v1_fpn_1024x1024\/1',\n'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet152_v1_fpn_640x640\/1',\n'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet152_v1_fpn_1024x1024\/1',\n'Faster R-CNN ResNet50 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_640x640\/1',\n'Faster R-CNN ResNet50 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_1024x1024\/1',\n'Faster R-CNN ResNet50 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_800x1333\/1',\n'Faster R-CNN ResNet101 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_640x640\/1',\n'Faster R-CNN ResNet101 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_1024x1024\/1',\n'Faster R-CNN ResNet101 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_800x1333\/1',\n'Faster R-CNN ResNet152 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_640x640\/1',\n'Faster R-CNN ResNet152 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_1024x1024\/1',\n'Faster R-CNN ResNet152 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_800x1333\/1',\n'Faster R-CNN Inception ResNet V2 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/inception_resnet_v2_640x640\/1',\n'Faster R-CNN Inception ResNet V2 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/inception_resnet_v2_1024x1024\/1',\n'Mask R-CNN Inception ResNet V2 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/mask_rcnn\/inception_resnet_v2_1024x1024\/1'\n}","0c78e54a":"#@title Model Selection { display-mode: \"form\", run: \"auto\" }\nmodel_display_name = 'SSD MobileNet V2 FPNLite 640x640' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\nmodel_handle = ALL_MODELS[model_display_name]\n\nprint('Selected model:'+ model_display_name)\nprint('Model Handle at TensorFlow Hub: {}'.format(model_handle))","bc7e8d96":"print('loading model...')\nhub_model = hub.load(model_handle)\nprint('model loaded!')","c3b57475":"def load_img(path):\n    im = tf.io.read_file(path)\n    im = tf.io.decode_jpeg(im, channels=3)\n    return im\n\nsample_image_file = \"..\/input\/coco-2017-dataset\/coco2017\/train2017\/000000000009.jpg\"\nsample_im = load_img(sample_image_file)\n\nplt.imshow(sample_im)\nplt.axis(\"off\")\nplt.show()\n\n# Reshape to (1, height, width, channel) for detections\nsample_im = tf.expand_dims(sample_im, axis=0)","62209e8e":"%%time\n\nresult = hub_model(sample_im)","1f89881a":"def get_image_detections(result, max_detections=20):\n    detections = result[\"detection_boxes\"]\n    best_ind = tf.image.non_max_suppression(boxes=result[\"detection_boxes\"][0],\n                                          scores=result[\"detection_scores\"][0],\n                                          max_output_size=max_detections)\n\n    detections = tf.gather(detections, best_ind, axis=1)\n    return detections  # (1, num_detections, bbox)\n\ndetections = get_image_detections(result) ","9665957b":"colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\ntest_im = tf.image.draw_bounding_boxes(images=sample_im.numpy(), \n                                       boxes=detections, \n                                       colors=colors)\n\nplt.figure(figsize=(8, 8))\nplt.imshow( (test_im \/ 255.)[0] )\nplt.axis(\"off\")\nplt.show()","cf428210":"def extract_image_regions(image, detections):\n  \n    h, w = image.shape[1:3]\n    patches = detections * [h, w, h, w]\n    regions = []\n    coors = tf.cast(patches, tf.int32) # (1, num_detections, 4)\n\n    # Remove the first axis before iterate\n    for coor in tf.squeeze(coors):\n        y_min, x_min, y_max, x_max = coor\n        regions.append(image[:, y_min:y_max, x_min:x_max, : ])\n\n    return regions\n\nim_regions = extract_image_regions(sample_im, detections)\nplt.imshow(im_regions[0][0])\nplt.axis(\"off\")\nplt.show()","3d63e2a9":"image_features_extractor = tf.keras.applications.ResNet101(include_top=False, \n                                                           weights=\"imagenet\", \n                                                           pooling=\"avg\")\nimage_features_extractor.trainable = False\nimage_features_extractor(tf.random.uniform((1, 200, 100, 3)))","ebf0338e":"def detect_and_encode_image(image, max_detections=20):\n    \"\"\"\n    Perform an object detection model from a specific image to produce bbox on \n    detected objects. The bbox result then become the coordinates for image\n    regions that'll be extracted from the images. The results of the image regions \n    before will be encoded with pretrained-CNN model.\n\n    args:\n    image: A 4D tensor with shape [1, height, width, channel].\n    max_detections: Total detections needed from object detection model, cap to 300.\n\n    return:\n    image_regions_features \n    \"\"\"\n    assert max_detections <= 300, \"max detections limits is 300\"\n\n    # Feed image into object detection model\n    object_detection_result = hub_model(image)\n\n    # Get 'max_detections' bounding boxes\n    detections = get_image_detections(object_detection_result, max_detections=max_detections)\n\n    # Extract the region of image based on the bounding boxes values\n    regions = extract_image_regions(image, detections)\n\n    features = np.zeros( (len(regions), 2048) )\n\n    for index, r in enumerate(regions):\n        feature = image_features_extractor(r)\n        features[index] = feature\n\n    return features # (max_detections, 2048)","091a6c4e":"%%time\n\nsample_feature = detect_and_encode_image(sample_im, max_detections=20)\nsample_feature.shape","64abcae1":"VOCAB_SIZE = 8000\n\nbert_tokenizer_params = dict(lower_case=True)\nreserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n\nbert_vocab_args = dict(\n    # The target vocabulary size\n    vocab_size=VOCAB_SIZE,\n    # Reserved tokens that must be included in the vocabulary\n    reserved_tokens=reserved_tokens,\n    # Arguments for `text.BertTokenizer`\n    bert_tokenizer_params=bert_tokenizer_params,\n    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n    learn_params={},\n)","412159b0":"tfr_path = \"..\/input\/m2transformertfr\"","85954bae":"tokenizer = text.BertTokenizer(tfr_path + \"\/cap_vocab.txt\", **bert_tokenizer_params)","c3335bbc":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n    return pos * angle_rates","76fe6a2f":"def positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)","0021d6bb":"n, d = 2048, 512\npos_encoding = positional_encoding(n, d)\nprint(pos_encoding.shape)\npos_encoding = pos_encoding[0]\n\n# Juggle the dimensions for the plot\npos_encoding = tf.reshape(pos_encoding, (n, d\/\/2, 2))\npos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\npos_encoding = tf.reshape(pos_encoding, (d, n))\n\nplt.pcolormesh(pos_encoding, cmap='RdBu')\nplt.ylabel('Depth')\nplt.xlabel('Position')\nplt.colorbar()\nplt.show()","a047752d":"def create_padding_mask(seq, m_slots=None):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n    # Memory slots pads\n    if m_slots:\n        m_pad = tf.zeros((tf.shape(seq)[0], m_slots))  # (batch_size, m_slots)\n        seq = tf.concat([seq, m_pad], axis=1)\n\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)","47558da2":"x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]], dtype=tf.float32)\ncreate_padding_mask(x, None)","e630db7e":"def create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)","7ba0fb36":"x = tf.random.uniform((1, 3))\ntemp = create_look_ahead_mask(x.shape[1])\ntemp","ce845676":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead)\n    but it must be broadcastable for addition.\n\n    Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n    output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights","571d04ab":"def print_out(q, k, v):\n    temp_out, temp_attn = scaled_dot_product_attention(\n      q, k, v, None)\n    print('Attention weights are:')\n    print(temp_attn)\n    print('Output is:')\n    print(temp_out)","f44cfdbb":"np.set_printoptions(suppress=True)\n\ntemp_k = tf.constant([[10, 0, 0],\n                      [0, 10, 0],\n                      [0, 0, 10],\n                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n\ntemp_v = tf.constant([[1, 0],\n                      [10, 0],\n                      [100, 5],\n                      [1000, 6]], dtype=tf.float32)  # (4, 2)","9da61025":"# This `query` aligns with the second `key`,\n# so the second `value` is returned.\ntemp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\nprint_out(temp_q, temp_k, temp_v)","e42086a5":"class MultiHeadAttentionMemory(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, m_slots):\n        super(MultiHeadAttentionMemory, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model \/\/ self.num_heads\n        self.m_slots = m_slots\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        if self.m_slots:\n            self.mk, self.mv = self.get_memories(m_slots)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n  \n\n    def get_memories(self, m_slots):\n        mk = self.add_weight(name=\"memory_k\", \n                            shape=(1, m_slots, self.d_model), \n                            initializer=tf.keras.initializers.RandomNormal(stddev=1\/self.d_model))\n        mv = self.add_weight(name=\"memory_v\", \n                            shape=(1, m_slots, self.d_model), \n                            initializer=tf.keras.initializers.RandomNormal(stddev=1\/self.m_slots))\n        return mk, mv\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # ...\n        v = self.wv(v)  # ...\n\n        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len, depth)\n        k = self.split_heads(k, batch_size)  \n        v = self.split_heads(v, batch_size)\n    \n        if self.m_slots:\n            m_k = self.split_heads(self.mk, batch_size=1) # (batch_size, num_heads, memory_slots, depth)\n            m_v = self.split_heads(self.mv, batch_size=1) # ...\n            k = tf.concat(\n              [k, tf.tile(m_k, [batch_size, 1, 1, 1])], axis=2) # (batch_size, num_heads, memory_slots + seq_len_k, depth)\n            v = tf.concat(\n              [v, tf.tile(m_v, [batch_size, 1, 1, 1])], axis=2) # ...\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k + memory_slots)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n    \n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n    \n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights","f0449e1d":"temp_mham = MultiHeadAttentionMemory(d_model=512, num_heads=8, m_slots=40)\n\ny = tf.random.uniform((64, 60, 2048))\nsample_mask = create_padding_mask(tf.reduce_sum(y, axis=-1), m_slots=40)\n\noutput, attn = temp_mham(y, k=y, q=y, mask=sample_mask)\noutput.shape, attn.shape","c8444d93":"sample_mask.shape","baf1780f":"def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","41466348":"sample_ffn = point_wise_feed_forward_network(512, 2048)\nsample_ffn(tf.random.uniform((64, 50, 512))).shape","f88afbf6":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, m_slots, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttentionMemory(d_model, num_heads, m_slots)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        out1 = self.dropout1(attn_output, training=training) # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","4f864f98":"sample_encoder_layer = EncoderLayer(512, 8, 40, 2048)\n\nsample_encoder_layer_output = sample_encoder_layer(\n    tf.random.uniform((64, 20, 2048)), False, None)\n\nsample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)","d974918e":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, m_slots, dff, rate)\n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.dropout(x, training=training)\n\n        enc_outputs = []\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n            enc_outputs.append(x)\n\n        return x, enc_outputs  # (batch_size, input_seq_len, d_model)","da19a2fd":"sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, m_slots=40, dff=2048)\nsample_encoder_output, sample_all_encoder_output = sample_encoder(\n    tf.random.uniform((64, 20, 2048)), False, None\n)\nsample_encoder_output.shape, sample_all_encoder_output[0].shape","951f740e":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.self_attn = MultiHeadAttentionMemory(d_model, num_heads, m_slots)\n\n        self.enc_attn = MultiHeadAttentionMemory(d_model, num_heads, m_slots)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.fc_alphas = [tf.keras.layers.Dense(d_model) for i in range(num_layers)]\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, enc_outputs, training, look_ahead_mask, padding_mask):\n    \n        assert len(enc_outputs) == len(self.fc_alphas)\n\n        N = len(enc_outputs)\n\n        # Self-Attention\n        self_out, self_attn_weights = self.self_attn(x, x, x, look_ahead_mask)\n        self_out = self.dropout1(self_out, training=training)\n\n        mesh_outs = []\n        for i in range(N):\n            # Cross-Attention between self_out and enc_outputs\n            cross_out, cross_attn_weights = self.enc_attn(enc_outputs[i], \n                                                        enc_outputs[i], \n                                                        self_out, \n                                                        padding_mask)\n      \n            alpha_out = tf.nn.sigmoid(\n                self.fc_alphas[i](tf.concat([self_out, cross_out], axis=-1))\n            )\n            mesh_outs.append(alpha_out * cross_out)\n    \n        mesh_out = tf.add_n(mesh_outs) \/ tf.math.sqrt(float(N))\n        mesh_out = self.dropout2(mesh_out, training=training)\n        mesh_out = self.layernorm1(mesh_out + self_out)\n    \n        ffn_out = self.ffn(mesh_out)\n        ffn_out = self.dropout3(ffn_out, training=training)\n        ffn_out = self.layernorm2(ffn_out + mesh_out) # (batch_size, seq_len_q, d_model)\n\n        return ffn_out, self_attn_weights, cross_attn_weights","f71645b8":"# Decoder layer don't issue memory slots\nsample_decoder_layer = DecoderLayer(num_layers=2, d_model=512, num_heads=8, m_slots=None, dff=2048)\nsample_decoder_output, _, _ = sample_decoder_layer(\n    tf.random.uniform((64, 30, 2048)), sample_all_encoder_output, False, None, None\n)\nsample_decoder_output.shape","f8b9325b":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, target_vocab_size,\n                   maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.decoder_layers = [\n          DecoderLayer(num_layers, d_model, num_heads, m_slots, dff, rate) \n          for _ in range(num_layers) \n        ]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n  \n    def call(self, x, enc_outputs, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(float(self.d_model))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, self_attn_weights, cross_attn_weights = self.decoder_layers[i](x, enc_outputs, training, \n                                                                              look_ahead_mask, padding_mask)\n\n        return x, self_attn_weights, cross_attn_weights","3c363c15":"sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, m_slots=None, dff=2048, \n                         target_vocab_size=8000, maximum_position_encoding=5000)\n\nsample_decoder_output, self_attn, cross_attn = sample_decoder(\n    tf.random.uniform((64, 30)), sample_all_encoder_output, False, None, None\n)\nsample_decoder_output.shape, self_attn.shape, cross_attn.shape","f06e4e33":"tf.TensorSpec([], tf.int64)\ntf.constant(False)","b969e9a4":"input_signatures = [tf.TensorSpec([None, 20, 2048], tf.float32),\n                    tf.TensorSpec([None, None], tf.int64),\n                    tf.TensorSpec([], tf.bool),\n                    tf.TensorSpec([None, None], tf.float32)\n                   ]","c1b95c7f":"class M2_Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, target_vocab_size, \n               pe_target, rate=0.1):\n        super(M2_Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, m_slots, dff)\n        # Decoder didn't use memory slots\n        self.decoder = Decoder(num_layers, d_model, num_heads, None, dff, \n                                 target_vocab_size, pe_target)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n    @tf.function\n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output, all_enc_outputs = self.encoder(inp, training, mask=enc_padding_mask)\n        dec_output, self_attn, cross_attn = self.decoder(\n            tar, all_enc_outputs, training, look_ahead_mask, dec_padding_mask\n        )\n\n        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output","7481ce20":"sample_m2_transformer = M2_Transformer(num_layers=2, d_model=512, num_heads=8, m_slots=40, dff=2048,\n                                       target_vocab_size=8000, pe_target=5000)\n\nx = tf.random.uniform((64, 20, 2048))\ny = tf.random.uniform((64, 25))\n\nsample_enc_mask = create_padding_mask(tf.reduce_sum(x, axis=-1), m_slots=40)\nsample_dec_mask = create_padding_mask(tf.reduce_sum(x, axis=-1), m_slots=None)\n\nsample_final_output = sample_m2_transformer(\n    inp=x, \n    tar=y, \n    training=False,\n    enc_padding_mask=sample_enc_mask,\n    look_ahead_mask=None, \n    dec_padding_mask=sample_dec_mask\n)\n\nsample_final_output.shape","b7b15086":"num_layers = 6\nd_model = 512\nm_slots = 40\ndff = 2048\nnum_heads = 8\ndropout_rate = 0.1","221a5960":"optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)","6c5e0844":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","f5e96942":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_) \/ tf.reduce_sum(mask)\n\nsample_real = tf.constant([1, 0, 1])\nsample_pred = tf.constant([[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]])\nloss_function(sample_real, sample_pred)","8f7ce683":"def accuracy_function(real, pred):\n    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    accuracies = tf.math.logical_and(mask, accuracies)\n\n    accuracies = tf.cast(accuracies, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n\n    return tf.reduce_sum(accuracies) \/ tf.reduce_sum(mask)","558e622d":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')","2084c7c7":"init_params = dict(num_layers=num_layers, \n                    d_model=d_model, \n                    num_heads=num_heads, \n                    m_slots=m_slots, \n                    dff=dff,\n                    target_vocab_size=VOCAB_SIZE, \n                    pe_target=1000,\n                    rate=dropout_rate)","b43b2fea":"m2_transformer = M2_Transformer(**init_params)","36f351ed":"def create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp, m_slots=m_slots)\n\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by\n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n    return enc_padding_mask, combined_mask, dec_padding_mask","5394a009":"ckpt = tf.train.Checkpoint(transformer=m2_transformer,\n                           optimizer=optimizer)\n\n# Restore from previous training ckpt\nprev_ckpt = tf.train.latest_checkpoint(\"..\/input\/m2checkpoint\/checkpoints\/train\")\nckpt.restore(prev_ckpt)","19fa6336":"val_cap_path = \"..\/input\/coco-2017-dataset\/coco2017\/annotations\/captions_val2017.json\"\nwith open(val_cap_path, \"r\") as file:\n    val_annot = json.load(file)","ba842c58":"val_dir = \"..\/input\/coco-2017-dataset\/coco2017\/val2017\"\nval_len = 10\nval_img_dict = val_annot[\"images\"][:val_len]\nval_img_paths = []\n\nfor info in val_img_dict:\n    image_path = os.path.join(val_dir, info[\"file_name\"])\n    val_img_paths.append(image_path)\n    \nval_img_paths","2100b903":"# Start and End Token\nSTART = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\nEND = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")","10f9b11d":"def preprocess_image(im_path):\n    image = tf.expand_dims(load_img(im_path), 0) # (1, height, width, channel)\n    encoded_detections = detect_and_encode_image(image)\n    encoded_detections = tf.expand_dims(encoded_detections, axis=0)\n    return encoded_detections\n\n\ndef tokenize_and_plot(output, image):\n    # Tokenize decoder outputs\n    words = tokenizer.detokenize(output)\n    text = tf.strings.reduce_join(words, separator=' ', axis=-1)\n    print(text.numpy())\n    \n    # Plot image\n    plt.imshow(image[0])\n    plt.axis(\"off\")\n    plt.show()\n    \n    return text","15513b89":"class BeamSearch:\n    \"\"\"\n    Perform beam search on M2-Transformer model. The search is divided by 3 phase:\n    \n        1. First-phase : Predict next word, sort the result by logits and get the first \n                         'n-width' tokens.\n                         \n        2. Second-phase: Predict next word using every previous 'n-width' tokens, \n                         sort the result by logits and get the first 'n-width' tokens.\n                         You should have 'n-width^2' pair of tokens right now.\n                         \n        3. Third-phase : Now you have n-width^2 sentence candidates. They'll be scored based\n                         on their sum of logs and the top n-width candidates will be choosen.\n        \n        \n    Args:\n        model      : Trained M2-Transformer model instance\n        enc_input  : 2D-shape tensor to be fed into model\n        start      : Token which mark the start of sentence (0D-shape tensor)\n        end        : Token which marks the end of sentence  (0D-shape tensor)\n        beam_width : Beam search width (default=1)\n    \"\"\"\n    def __init__(self, model, enc_input, start, end, beam_width=1):\n        # Model and input\n        self.model = model\n        self.enc_input = enc_input\n        \n        # Start and End tokens\n        self.start = start\n        self.end = end\n\n        # Search parameter\n        self.width = beam_width\n        \n        # Prediction Output\n        self.output = tf.reshape(self.start, [1, 1])\n        \n        # Predictions logits\n        self.logits = 0\n        \n        # Model parameter\n        self.model_params = {\"inp\": self.enc_input, \n                             \"tar\": self.output, \n                             \"training\": False,\n                             \"enc_padding_mask\": None, \n                             \"look_ahead_mask\": None,\n                             \"dec_padding_mask\": None,}\n    \n    \n    def get_output_shape(self):\n        return tf.shape(self.output)\n    \n    def predict_words(self):\n        \n        # Match enc_input with output batch size\n        out_shape = self.get_output_shape()\n        enc_input = tf.repeat(self.enc_input, out_shape[0], axis=0)\n        \n        # Reduce one dim to match create_masks function (create_masks take 2D tensor)\n        self.enc_input_reduced = tf.reduce_sum(\n            self.enc_input, axis=-1) \n        _, tar_look_ahead_mask, _ = create_masks(self.enc_input_reduced, self.output)\n        \n        # Refresh model parameter\n        self.model_params[\"inp\"] = enc_input\n        self.model_params[\"tar\"] = self.output\n        self.model_params[\"look_ahead_mask\"] = tar_look_ahead_mask\n        \n        predictions = self.model(**self.model_params)\n        return predictions\n    \n    \n    def __first_sl(self):\n        \"\"\"First phase of search.\"\"\"\n        \n        predictions = self.predict_words()\n        predictions = predictions[:, -1:, :] # Choose the last word prediction\n        \n        pred_sorted = tf.sort(predictions, axis=-1, direction=\"DESCENDING\")[:, :, :self.width]\n        pred_sorted = tf.reshape(pred_sorted, [self.width, 1])\n        \n        indices = tf.argsort(predictions, axis=-1, direction=\"DESCENDING\")[:, :, :self.width]\n        indices = tf.reshape(indices, [self.width, 1])\n        indices = tf.cast(indices, dtype=tf.int64)\n        \n        self.logits = pred_sorted\n        \n        # Match output shape with indices shape\n        self.output = tf.repeat(self.output, self.width, axis=0)\n        self.output = tf.concat([self.output, indices], axis=1)\n    \n    \n    def __second_sl(self):\n        \"\"\"Second phase of search.\"\"\"\n        predictions = self.predict_words()\n        predictions = predictions[:, -1:, :] # Choose the last word prediction\n        \n        pred_sorted = tf.sort(predictions, axis=-1, direction=\"DESCENDING\")[:, :, :self.width]\n        pred_sorted = tf.reshape(pred_sorted, [self.width**2, 1]) # n^2\n        \n        indices = tf.argsort(predictions, axis=-1, direction=\"DESCENDING\")[:, :, :self.width]\n        indices = tf.reshape(indices, [self.width**2, 1]) # n^2\n        indices = tf.cast(indices, dtype=tf.int64)\n        \n        # Match output and logit shape\n        self.output = tf.repeat(self.output, self.width, axis=0)\n        self.logits = tf.repeat(self.logits, self.width, axis=0)\n        \n        # Concat with current predictions\n        self.output = tf.concat([self.output, indices], axis=1)\n        self.logits = tf.concat([self.logits, pred_sorted], axis=1)\n        \n    \n    def __third_sl(self):\n        # Sum all of the logits of axis=1\n        scores = tf.reduce_sum(self.logits, axis=1)\n        # Sort by high to low\n        ranks = tf.argsort(scores, axis=0, direction=\"DESCENDING\")\n        \n        # Pick top n-width words\n        self.logits = tf.gather(self.logits, ranks[:self.width])\n        self.output = tf.gather(self.output, ranks[:self.width])\n        \n        return self.output, self.logits\n    \n    \n    def check_eos(self):\n        # Look for end token in output tensor.\n        self.eos_founds = tf.where(self.output == self.end) # (row_idx, col_idx)\n        \n        # Get all row index\n        eos_candidate = self.eos_founds[:, 0]\n        \n        # Strip off duplicate row index\n        self.eos_unique = tf.unique_with_counts(eos_candidate)\n        eos_unique_candidate = self.eos_unique.y\n        \n        # Total number of eos in each candidate\n        return len(eos_unique_candidate)\n    \n    \n    def final_evaluation(self):\n        # Nearest eos found on each row\n        eos_unique_count = self.eos_unique.count\n        eos_near_index = tf.math.cumsum(eos_unique_count)\n        eos_near_index = tf.concat([[0], eos_near_index], axis=0)[:-1]\n        eos_near_index = tf.gather(self.eos_founds, eos_near_index)\n        \n        # Get each candidate from start until nearest end token\n        candidate_length = eos_near_index[:, 1]\n        \n        all_candidate = [\n            output[:length + 1].numpy() for output, length in zip(self.output, candidate_length)]\n        \n        all_candidate = tf.ragged.constant(all_candidate)\n        \n        best_index = tf.reduce_mean(all_candidate, axis=1)\n        best_index = tf.argsort(best_index, axis=0, direction=\"DESCENDING\")[:1]\n        \n        return all_candidate[int(best_index)]\n        \n    \n    def predicts(self):\n        self.__first_sl()\n        for i in range(25):\n            self.__second_sl()\n            self.__third_sl()\n            # Stop searching when all candidates reached eos.\n            if self.check_eos() == self.width:\n                break\n        \n        best_output = self.final_evaluation()\n        \n        return best_output\n        ","cd6150ce":"sample_path = \"..\/input\/coco-2017-dataset\/coco2017\/train2017\/000000000025.jpg\"\nsample_val = load_img(sample_path)\nplt.imshow(sample_val)\nplt.axis(\"off\");","86affb21":"sample_encoded = preprocess_image(sample_path)","d4064c34":"bs = BeamSearch(m2_transformer, sample_encoded, START, END, 5)\nbest_output = bs.predicts()\nprint(bs.output)","8b9a3ec0":"desc = tokenizer.detokenize(tf.expand_dims(best_output, 0))\nfor sen in desc:\n    print(tf.strings.reduce_join(sen, separator=\" \").numpy())\nplt.imshow(sample_val)\nplt.axis(\"off\")\nplt.show()","f151ee4e":"Code Test","22c6c867":"Code Test","ea03c564":"## Coco Validation Data","7fcef3a2":"# Loss and Metric","92e66de0":"## Extract image regions","bc2930f8":"## Decoder","430920b1":"# Create M2-Transformer","64970362":"## Encoder Layer","ee92bff1":"## Masking","235a1a9d":"## Point wise feed forward network","56fb9d94":"# Model Checkpoints","a39732b4":"## Setup bert tokenizer","03e17713":"## Scaled dot product attention","a18ed55b":"## Get Sample Image","d85cdd2f":"# Optimizer","ea2f5e6c":"## Encoder","39ec54b8":"## Multi Head Attention Memory\n","2e4e387b":"## Visualize the Bounding Box","d822decb":"## Decoder Layer","55d7f07f":"Code Test","56027efe":"Code Test","9e797a5b":"# Image Preprocessing\nSince we already have the preprocessed image. I will show you how the image processing process.","ccda0f32":"## Image features from ResNet101","0132a46c":"# Evaluate","755bbdb8":"## Positional Encoding","2c56a3d9":"# Setup","1497c2ee":"Code Test","3a68cd29":"## Bert tokenizer dependencies","738f1844":"Code Test","8ae5258a":"## Text tokenizer","ac84ca74":"Code Test","50a2334d":"# Set Hyperparameters","5b5b1843":"# Text Preprocessing","fd1cc39b":"# Meshed-Memory-Transformer\nPaper can be found [here](https:\/\/arxiv.org\/pdf\/1912.08226v2.pdf)","5e667de4":"## Libraries","9a0c45f3":"Code Test","14e012db":"## Loading Selected Model from Tensorflow Hub"}}