{"cell_type":{"0d13481b":"code","2c94f3a8":"code","1db489ed":"code","e489b347":"code","93aee86c":"code","3d3a9d1d":"code","610d758a":"code","e5061d25":"code","24bdbe52":"code","be8078cf":"code","96b8aea1":"code","e4fd8f01":"code","c160112a":"code","5fe179e1":"code","952a9dd5":"code","0031609f":"code","9dd72a33":"code","02055e4c":"code","68a30186":"code","9e774ee7":"code","502d1e7b":"code","8b2f02ff":"code","229147c2":"code","598830c3":"code","6370094f":"code","dd73a5fb":"code","d3568918":"code","dac17920":"code","c5198f44":"code","73297b18":"code","e2cb0299":"code","1fac5bcf":"code","a3c3144e":"code","2ff3c2e9":"code","f74a96bd":"code","15b12299":"code","8bd1b42e":"code","fdcaae3f":"code","4374e34e":"code","59af8bb2":"code","3d849966":"code","5045840e":"code","e3c209a7":"code","a3e2e784":"code","e146d95d":"code","71690802":"markdown","18e08e0d":"markdown","0a3037e6":"markdown","3d4f0481":"markdown","d96c18a7":"markdown","adb041a5":"markdown","b6934857":"markdown","3e6610a9":"markdown","346441f5":"markdown","dc12587d":"markdown","620c186b":"markdown","663af3b1":"markdown","a9228382":"markdown","36a3fc02":"markdown","a8fd135d":"markdown","2b5eb021":"markdown","f140a24a":"markdown","43308774":"markdown","2b5fdfaa":"markdown","e3e7dee4":"markdown","04ec26f1":"markdown","4fd6cd91":"markdown","64ba8ae7":"markdown","7f3935ff":"markdown","b9c8ad9c":"markdown","4fa9ebb1":"markdown","1de39967":"markdown","d37cd8d1":"markdown","cce9d695":"markdown","5a5eb228":"markdown","4eb87128":"markdown","888e4de3":"markdown","8981ea66":"markdown","1db4eeea":"markdown","dfee08a5":"markdown","eafc474e":"markdown","c69cf419":"markdown","298baf37":"markdown","182dcf67":"markdown","01dde2a3":"markdown","9138d013":"markdown","0ea0f8ef":"markdown","8bb412a1":"markdown","051d66df":"markdown","2dc37fae":"markdown","bb2be6a2":"markdown","dc2c5cae":"markdown","740bcd25":"markdown","6ff3da40":"markdown","b2663d31":"markdown","4c358442":"markdown","f3f194de":"markdown","0e04f23d":"markdown","b6fa2c01":"markdown","ff7d6f13":"markdown","57b7eb0a":"markdown","6db39222":"markdown","602eae65":"markdown","14eb3546":"markdown","b33d7425":"markdown","08bfac9e":"markdown","b0caf058":"markdown","73696b1e":"markdown","60557f2c":"markdown","8afeab26":"markdown","dc9bb504":"markdown"},"source":{"0d13481b":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn.model_selection as ms\nfrom tensorflow.keras.preprocessing import sequence\nimport tensorflow.keras.preprocessing.text as text\nimport sklearn.metrics as m\nimport sklearn.preprocessing as pre\nimport datetime\nimport sklearn.utils as u\nimport seaborn as sns\nimport fbprophet as fbp","2c94f3a8":"np.random.seed(0)\n\n#for keras.mape:\ntf.keras.backend.set_epsilon(1)\nmape = tf.keras.losses.MeanAbsolutePercentageError()","1db489ed":"df = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv')","e489b347":"df.head(10)","93aee86c":"df.describe()","3d3a9d1d":"df.date.min(), df.date.max()","610d758a":"def get_sales_for_store_and_item(df, store, item):\n    return df[(df.item == item) & (df.store == store)]","e5061d25":"for i in range(1, 5):\n    tmp = get_sales_for_store_and_item(df, 1, i)        \n    sns.lineplot(tmp.date, tmp.sales)\nplt.show()","24bdbe52":"df_prep_fbp = df\ndf_prep_fbp = df.rename(columns={\"date\": \"ds\", \"sales\": \"y\"})","be8078cf":"threshold = '2017-10-03'\ndf_train_fbp = df_prep_fbp[df_prep_fbp.ds < threshold]\ndf_valid_fbp = df_prep_fbp[df_prep_fbp.ds >= threshold]","96b8aea1":"df_train_fbp.shape, df_valid_fbp.shape","e4fd8f01":"(df_valid_fbp.item.unique() == df_train_fbp.item.unique()).all()","c160112a":"(df_valid_fbp.store.unique() == df_train_fbp.store.unique()).all()","5fe179e1":"# params_grid = {'seasonality_mode': ('multiplicative', 'additive'),\n#                'changepoint_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5],\n#               'holidays_prior_scale': [0.1, 0.2, 0.3, 0.4, 0.5],\n#               'n_changepoints' : [100, 150, 200]}\n# grid = ms.ParameterGrid(params_grid)","952a9dd5":"# df_train_one_store_fbp = df_train_fbp[(df_train_fbp.store == 1)]\n# df_valid_one_store_fbp = df_valid_fbp[(df_valid_fbp.store == 1)]","0031609f":"# model_parameters = pd.DataFrame(columns = ['MAPE','Parameters'])\n# for p in grid:\n#     print(p)    \n#     param_runs = np.zeros(0)\n\n#     for i in range(1, 11):\n#         print(f'Run: {i}')\n        \n#         df_current_train = df_train_one_store_fbp[(df_train_one_store_fbp.item == i)]\n#         df_current_valid = df_valid_one_store_fbp[(df_valid_one_store_fbp.item == i)]\n\n\n#         train_model = fbp.Prophet(changepoint_prior_scale = p['changepoint_prior_scale'],\n#                              holidays_prior_scale = p['holidays_prior_scale'],\n#                              n_changepoints = p['n_changepoints'],\n#                              seasonality_mode = p['seasonality_mode'],\n#                              weekly_seasonality=True,\n#                              daily_seasonality = True,\n#                              yearly_seasonality = True,\n#                              #holidays=holiday, \n#                              interval_width=0.95)\n\n\n#         train_model.add_country_holidays(country_name='US')\n#         train_model.fit(df_current_train[['ds', 'y']])\n#         train_forecast = train_model.predict(df_current_valid[['ds']])\n\n#         param_runs = np.append(param_runs, mape(df_current_valid.y, train_forecast.yhat))\n        \n#     MAPE = param_runs.mean()\n#     print(f'Mean Absolute Percentage Error: {MAPE}')\n#     model_parameters = model_parameters.append({'MAPE':MAPE,'Parameters':p},ignore_index=True)","9dd72a33":"def features_target_split(dataset, history_size, target_size):\n    features = []\n    target = []\n\n    start_index = history_size\n    end_index = len(dataset) - target_size\n\n    for i in range(start_index, end_index):\n        indices = range(i-history_size, i)\n        \n        features_to_append = dataset[indices]\n        features.append(np.reshape(features_to_append, (len(features_to_append), 1)))\n        target.append(dataset[i:i+target_size])\n\n    return np.array(features), np.array(target)","02055e4c":"def features_split(dataset, history_size):\n    features = []\n    features_to_append = dataset[-history_size:]\n    features.append(np.reshape(features_to_append, (len(features_to_append), 1)))\n    return np.array(features)","68a30186":"history_size = 360\ntarget_size = 90\nsecond_features_size = 2\n\nX_train_dnn = np.zeros((0, history_size, 1))\nX2_train_dnn = np.zeros((0, second_features_size))\ny_train_dnn = np.zeros((0, target_size))\n\nX_valid_dnn = np.zeros((0, history_size, 1))\nX2_valid_dnn = np.zeros((0, second_features_size))\ny_valid_dnn = np.zeros((0, target_size))\n\nfor s in range(1, df.store.max()+1):\n    for i in range(1, df.item.max()+1):\n\n        t = df[(df.store == s) & (df.item == i)]\n\n        train_temp = t[t.date < threshold]\n        valid_temp = t[t.date >= threshold]\n\n\n        x_temp_train, y_temp_train = features_target_split(train_temp.sales.values, history_size, target_size)\n\n\n        x_temp_valid = train_temp.sales.tail(history_size).values\n        x_temp_valid = np.reshape(x_temp_valid, (1, history_size, 1))\n\n        y_temp_valid = np.reshape(valid_temp.sales.values, (1, target_size))\n\n\n        x2_temp_train = np.repeat(np.array([[s, i]]), repeats=x_temp_train.shape[0], axis=0)\n        x2_temp_valid = np.repeat(np.array([[s, i]]), repeats=x_temp_valid.shape[0], axis=0)\n\n\n        X_train_dnn = np.concatenate((X_train_dnn, x_temp_train))\n        X_valid_dnn = np.concatenate((X_valid_dnn, x_temp_valid))\n\n        X2_train_dnn = np.concatenate((X2_train_dnn, x2_temp_train), axis=0)\n        X2_valid_dnn = np.concatenate((X2_valid_dnn, x2_temp_valid), axis=0)\n\n        y_train_dnn = np.concatenate((y_train_dnn, y_temp_train))\n        y_valid_dnn = np.concatenate((y_valid_dnn, y_temp_valid))\n        ","9e774ee7":"plt.plot(X_valid_dnn[0])\nplt.plot(np.concatenate((np.zeros((history_size)), y_valid_dnn[1])))\nplt.show()","502d1e7b":"for i in range(5):\n    plt.plot(X_valid_dnn[i])\n    plt.plot(np.concatenate((np.zeros((history_size)), y_valid_dnn[i])))\n    plt.show()","8b2f02ff":"n_features = 1\nn_seq = 4\nn_steps = 90\n\nX_train_reshaped_dnn = X_train_dnn.reshape((X_train_dnn.shape[0], n_seq, 1, n_steps, n_features))\nX_valid_reshaped_dnn = X_valid_dnn.reshape((X_valid_dnn.shape[0], n_seq, 1, n_steps, n_features))\n\nX_train_reshaped_dnn.shape, X_valid_reshaped_dnn.shape","229147c2":"one_hot_encoder = pre.OneHotEncoder(sparse=False, handle_unknown='ignore')\nX2_train_dnn = pd.DataFrame(one_hot_encoder.fit_transform(X2_train_dnn))\nX2_valid_dnn = pd.DataFrame(one_hot_encoder.transform(X2_valid_dnn))","598830c3":"X_train_dnn_final, X2_train_dnn_final, y_train_dnn_final = u.shuffle(X_train_reshaped_dnn, X2_train_dnn, y_train_dnn)","6370094f":"X_valid_dnn_final, X2_valid_dnn_final, y_valid_dnn_final = u.shuffle(X_valid_reshaped_dnn, X2_valid_dnn, y_valid_dnn)","dd73a5fb":"BATCH_SIZE = 500\n\nx_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_dnn_final, X2_train_dnn_final))\ny_train_dataset = tf.data.Dataset.from_tensor_slices(y_train_dnn_final)\ntrain_dataset = tf.data.Dataset.zip((x_train_dataset, y_train_dataset))\ntrain_dataset = train_dataset.cache().shuffle(X_train_dnn.shape[0]).batch(BATCH_SIZE).repeat()\n\nx_val_dataset = tf.data.Dataset.from_tensor_slices((X_valid_dnn_final, X2_valid_dnn_final))\ny_val_dataset = tf.data.Dataset.from_tensor_slices(y_valid_dnn_final)\n\nval_dataset = tf.data.Dataset.zip((x_val_dataset, y_val_dataset))\nval_dataset = val_dataset.batch(BATCH_SIZE).repeat()","d3568918":"# i = tf.keras.layers.Input(shape=(3,))\n# h = tf.keras.layers.Dense(units=3, activation='relu')(i)\n# o = tf.keras.layers.Dense(units=1)(h)\n# model = tf.keras.models.Model(inputs=i, outputs=o)","dac17920":"# DROPOUT = hp.HParam('dropout', hp.Discrete([0.10, 0.15, 0.20]))\n# KERNEL2_SIZE = hp.HParam('kernel2_size', hp.Discrete([2, 3]))\n# LAYER_WITH_512 = hp.HParam('layer_with_512', hp.Discrete([0, 1, 2]))\n# THIRD_LAYER = hp.HParam('third_layer', hp.Discrete([False, True]))","c5198f44":"# METRIC_MAPE = 'mape'","73297b18":"# with tf.summary.create_file_writer('logs\/hparam_tuning').as_default():\n#     hp.hparams_config(\n#         hparams=[DROPOUT, KERNEL2_SIZE, LAYER_WITH_512, THIRD_LAYER],\n#         metrics=[hp.Metric(METRIC_MAPE, display_name=METRIC_MAPE)],\n#     )","e2cb0299":"# def train_val_model(hparams):\n    \n#     filters_1 = 512 if hparams[LAYER_WITH_512] == 1 else 256\n#     filters_2 = 512 if hparams[LAYER_WITH_512] == 2 else 256\n\n#     i1 = tf.keras.layers.Input(shape=(n_seq, 1, n_steps, n_features))\n#     tsl = tf.keras.layers.ConvLSTM2D(filters=filters_1, recurrent_dropout=hparams[DROPOUT], padding='same', return_sequences=True,  kernel_size=(1,hparams[KERNEL2_SIZE]), activation='relu')(i1)\n#     tsl = tf.keras.layers.BatchNormalization()(tsl)\n#     tsl = tf.keras.layers.Dropout(hparams[DROPOUT])(tsl)\n\n#     tsl = tf.keras.layers.ConvLSTM2D(filters=filters_2, recurrent_dropout=hparams[DROPOUT], padding='same', return_sequences=True, kernel_size=(1,hparams[KERNEL2_SIZE]), activation='relu')(tsl)\n#     tsl = tf.keras.layers.BatchNormalization()(tsl)\n#     tsl = tf.keras.layers.Dropout(hparams[DROPOUT])(tsl)\n    \n#     if hparams[THIRD_LAYER]:\n#         tsl = tf.keras.layers.ConvLSTM2D(filters=filters_2, recurrent_dropout=hparams[DROPOUT], kernel_size=(1,hparams[KERNEL2_SIZE]), activation='relu')(tsl)\n#         tsl = tf.keras.layers.BatchNormalization()(tsl)\n#         tsl = tf.keras.layers.Dropout(hparams[DROPOUT])(tsl)\n    \n#     fl1 = tf.keras.layers.Flatten()(tsl)\n\n#     i2 = tf.keras.layers.Input(shape=(X2_train_dnn.shape[1]))\n#     ohl = tf.keras.layers.Dense(units=10, activation='relu')(i2)\n\n#     merge = tf.keras.layers.concatenate([fl1, ohl])\n\n#     d = tf.keras.layers.Dense(units=150, activation='relu')(merge)\n#     o = tf.keras.layers.Dense(90)(d)\n\n#     model = tf.keras.models.Model(inputs=[i1, i2], outputs=o)\n\n#     adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n#     reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n#                               patience=5, min_lr=0.00001)\n\n#     model.compile(optimizer=adam, loss='mape')\n\n#     EVALUATION_INTERVAL = 200\n#     EPOCHS = 40\n\n#     logdir = (\"logs\/cv\/\" +  \n#     'DR-' + str(hparams[DROPOUT]) + '.' + \n#     'K2S-' + str(hparams[KERNEL2_SIZE]) + '.' +\n#     'LW512-' + str(hparams[LAYER_WITH_512]) + '.' +\n#     '3L-' + str(hparams[THIRD_LAYER])\n#     )\n\n#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n    \n#     model.fit(train_dataset, epochs=EPOCHS, callbacks=[reduce_lr, tensorboard_callback],\n#                       steps_per_epoch=EVALUATION_INTERVAL,\n#                       verbose=2,\n#                       validation_data=val_dataset, validation_steps=75)\n\n#     l = model.evaluate(val_univariate, steps=75)\n\n#     return l","1fac5bcf":"# def run(run_dir, hparams):\n#     with tf.summary.create_file_writer(run_dir).as_default():\n#         hp.hparams(hparams)\n#         res = train_val_model(hparams)\n#         tf.summary.scalar(METRIC_MAPE, res, step=1)","a3c3144e":"# session_num = 0\n# start = time.perf_counter()\n\n# total_models = (\n# len(DROPOUT.domain.values) \n# * len(LAYER_WITH_512.domain.values)\n# * len(KERNEL2_SIZE.domain.values)\n# * len(THIRD_LAYER.domain.values)\n# )\n\n# for dropout in DROPOUT.domain.values:\n#     for layer_with_512 in LAYER_WITH_512.domain.values:\n#         for kernel2_size in KERNEL2_SIZE.domain.values:\n#             for third_layer in THIRD_LAYER.domain.values:\n\n#                 hparams = {\n#                     DROPOUT: dropout,\n#                     LAYER_WITH_512: layer_with_512,\n#                     KERNEL2_SIZE: kernel2_size,\n#                     THIRD_LAYER: third_layer,\n#                 }\n#                 run_name = \"run-%d\" % session_num\n#                 print('--- Starting trial: %s' % run_name)\n#                 print({h.name: hparams[h] for h in hparams})\n#                 run('logs\/hparam_tuning\/' + run_name, hparams)\n\n#                 elapsed = (time.perf_counter() - start) \/ 60.0\n#                 print('Elapsed from start %.3f minutes.' % elapsed)\n\n#                 session_num += 1\n\n# pbar.close()","2ff3c2e9":"model_name = 'model.h5'\nmodel_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(model_name, save_best_only=True, monitor='val_loss', mode='min')","f74a96bd":"reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)","15b12299":"adam = tf.keras.optimizers.Adam(learning_rate=0.001)","8bd1b42e":"DROPOUT = 0.35","fdcaae3f":"i1 = tf.keras.layers.Input(shape=(n_seq, 1, n_steps, n_features))\ntsl = tf.keras.layers.ConvLSTM2D(filters=256, recurrent_dropout=DROPOUT, padding='same', return_sequences=True,  kernel_size=(1,3), activation='relu')(i1)\ntsl = tf.keras.layers.BatchNormalization()(tsl)\ntsl = tf.keras.layers.Dropout(DROPOUT)(tsl)\n\ntsl = tf.keras.layers.ConvLSTM2D(filters=256, recurrent_dropout=DROPOUT, padding='same', return_sequences=True,  kernel_size=(1,3), activation='relu')(i1)\ntsl = tf.keras.layers.BatchNormalization()(tsl)\ntsl = tf.keras.layers.Dropout(DROPOUT)(tsl)\n\ntsl = tf.keras.layers.ConvLSTM2D(filters=256, recurrent_dropout=DROPOUT, kernel_size=(1,3), activation='relu')(tsl)\ntsl = tf.keras.layers.BatchNormalization()(tsl)\ntsl = tf.keras.layers.Dropout(DROPOUT)(tsl)\nfl1 = tf.keras.layers.Flatten()(tsl)\n\ni2 = tf.keras.layers.Input(shape=(X2_train_dnn.shape[1]))\nohl = tf.keras.layers.Dense(units=10, activation='relu')(i2)\n\nmerge = tf.keras.layers.concatenate([fl1, ohl])\n\nd = tf.keras.layers.Dense(units=150, activation='relu')(merge)\no = tf.keras.layers.Dense(90)(d)\n\nfinal_model = tf.keras.models.Model(inputs=[i1, i2], outputs=o)\n\nfinal_model.compile(optimizer=adam, loss='mape')","4374e34e":"final_model.summary()","59af8bb2":"EVALUATION_INTERVAL = 600\nEPOCHS = 50\n\nhistory = final_model.fit(train_dataset, \n                epochs=EPOCHS, \n                callbacks=[reduce_lr_cb, model_checkpoint_cb],\n                steps_per_epoch=EVALUATION_INTERVAL, \n                verbose=1,\n                validation_data=val_dataset, validation_steps=100)","3d849966":"def plot_history(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()","5045840e":"plot_history(history)","e3c209a7":"model = tf.keras.models.load_model(model_name)\nmodel.evaluate(val_dataset, steps=100)","a3e2e784":"y_pred = pd.DataFrame()\nfor i in range(1, df.item.max()+1):\n    for s in range(1, df.store.max()+1):\n        t = df[(df.store == s) & (df.item == i)]\n\n        x_t = features_split(t.sales.values, history_size)\n        x_t = x_t.reshape((x_t.shape[0], n_seq, 1, n_steps, n_features))\n\n        x2_t = np.array([[s, i]])\n        x2_t = pd.DataFrame(one_hot_encoder.transform(x2_t))\n\n        y_t = pd.DataFrame()\n        y_t['sales'] = model.predict([x_t, x2_t]).reshape((-1))\n        y_pred = y_pred.append(y_t, ignore_index=True)\n\ny_pred['Id'] = y_pred.index\ny_pred","e146d95d":"y_pred.to_csv(\"submission.csv\", index=False)","71690802":"# Predicting","18e08e0d":"## Mape","0a3037e6":"Now I will perform grid search of hyperparameters. df contains 10 items and 50 stores but for cross-validation I will use 1 store and 10 items.","3d4f0481":"So, using this we can simply create two inputs, separately process the data from them, and merge outputs of the layers into the one layer.","d96c18a7":"![Screenshot from 2020-07-23 20-21-16.png](attachment:19e7f19d-8661-485d-bd0c-9f85a35816cd.png)","adb041a5":"![Screenshot from 2020-07-23 00-21-36.png](attachment:b1b9297f-3415-4c12-868c-0396928b1f7c.png)","b6934857":"![Screenshot from 2020-07-23 20-20-04.png](attachment:98a991ae-3220-48aa-9ae9-1430ab413cb6.png)","3e6610a9":"I also want to show some interesting thing. Here is the another run:","346441f5":"# Comparing Prophet and DNN","dc12587d":"Now let's look what LSTM can do.","620c186b":"Also, I want to note that Prophet isn't like other ML algorithms - we can't just feed all this data in the one estimator, we need to train one estimator per one store and one item, so in total we need to train 500 estimators. This isn't very nice - one estimator will receive only 1\/500 of the full data","663af3b1":"We have best Prophet MAPE loss = 14.32, and best DNN MAPE loss = 12.82. So, we will use the DNN model as the final model.","a9228382":"## Keras Functional API","36a3fc02":"I think it's unfair to compare only losses, so here are the pros of these two algorithms:","a8fd135d":"Prophet - is a Facebook library which contains the eponymous algorithm for the training  additive regression models. It has three main model components: trend, seasonality, and holidays:","2b5eb021":"I also want to use information about store and item in the our DNN, so I will add X2 features which will contain this info.","f140a24a":"Here is the example of inconsistent features and target - please look at the indexes:","43308774":"![Screenshot from 2020-07-23 20-11-13.png](attachment:92387889-cadf-4e93-a733-80af5bacac52.png)","2b5fdfaa":"Now, let's use the defined above function and split the data.","e3e7dee4":"Looks right, now we can go further.","04ec26f1":"We have successfully compared Prophet algorithm and DNN with CNN-LSTM cells. I chose LSTM because it gives smaller loss. Unfortunately this model still experiences validation loss fluctuations, but it gives fine test results and this is the main thing :)","4fd6cd91":"Check are validation and train parts have the same stores and items.","64ba8ae7":"Now we are ready to shuffle the data","7f3935ff":"But after that and a few other hyperparameters test runs I manually tested some other net versions which worked slightly better, so I will use a little bit different architecture for the final model.","b9c8ad9c":"It's also imporant to visually check that features consistent to the target.","4fa9ebb1":"## CNN-LSTM","1de39967":"![Screenshot from 2020-07-23 20-12-13.png](attachment:cbe0be06-3882-4e37-8f0e-f3e886f6daa7.png)","d37cd8d1":"Now we need to check is our splitting code works well:","cce9d695":"Now we come to the hardest and longest part :) For the hyperparameters tuning I used auto and manual random search on the multiple machines. There have been many attempts and a lot of code for them, so I'll only leave one part that shows what part of this process looked like. And I also will comment out it, but it is valid.","5a5eb228":"I will also define a function for the test data without target part:","4eb87128":"Here are top 3 result:","888e4de3":"DNN with LSTM cells: <br> - Very flexible <br> - Any input, not only time series <br> - Many hyperparameters to tune <br> - Smaller loss","8981ea66":"# Settings","1db4eeea":"Prophet: <br> - Very easy and intuitive to use <br> - Need small amount of data for the good results <br> - Fast training <br> - Hyperparameters tuning is also fast and straightforward","dfee08a5":"First of all, we need to prepare data for our DNN. And this is not as easy as with the data for Prophet - now we need to split the data on the features(X) and target(y). So, how can we do this with time series? Well, we can use sales values from one year as features and next 90 days as target. Let's do this!","eafc474e":"Before the DNN hyperparameters search I want to explain how we can feed mixed data(X which are time series and X2 which are one-hot features) in the our model. For this I will use Keras Functional API which is the way how we can build non-linear graphs of layers in Keras. So, instead of stacking layers one on other as we can do with the Sequential API, with Functional API we can build almost any non-cyclic graph. In this API one layer can receive previous layer as function can receive its parameter, here is an example:","c69cf419":"![Screenshot from 2020-07-22 21-11-34.png](attachment:58eb58da-0ff7-4826-8498-041b829de680.png) <br> Source: https:\/\/peerj.com\/preprints\/3190\/","298baf37":"# Conclusion","182dcf67":"## Pros and cons","01dde2a3":"# Prophet","9138d013":"Here are the best hyperparameters and its MAPE:","0ea0f8ef":"![image.png](attachment:aac0560f-d6fc-4666-bcc0-15cd48f65649.png) <br> Source: https:\/\/www.quora.com\/How-does-the-CNN-LSTM-model-work","8bb412a1":"## Hyperparameters tuning","051d66df":"## Original data vizualization","2dc37fae":"![image.png](attachment:d9a57aca-9126-45be-8954-76f4e5ccdc94.png) <br> Source: http:\/\/neerc.ifmo.ru\/wiki\/index.php?title=%D0%94%D0%BE%D0%BB%D0%B3%D0%B0%D1%8F_%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%BE%D1%81%D1%80%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C","bb2be6a2":"I will rename columns how that Prophet requires, set the train\/validation date threshold and split the data.","dc2c5cae":"Also, it's better to convert X2 data to the one-hot representation","740bcd25":"Note! It's very important to split the data on train\/validation parts using data threshold, not sklearn.model_selection.train_test_split(), because train_test_split will shuffle the data, thus the same dates can be in the train and validation parts.","6ff3da40":"Long short-term memory - is a type of RNN which besides short-term memory(lower from cell-to-cell arrow on the picture) has long-term memory(upper arrow):","b2663d31":"Now let's train the final model.","4c358442":"# Load and check the data","f3f194de":"Let's import the required libs first","0e04f23d":"In this notebook I want to test and compare two types of algorithms: Prophet from Facebook and DNN with LSTM cells.","b6fa2c01":"# Model finalization","ff7d6f13":"# Introduction","57b7eb0a":"## Data preparation","6db39222":"## Data preparation","602eae65":"I also want to use not just simple LSTM, but CNN-LSTM. As you can guess, this is hybrid of the CNN and LSTM and it initially uses kernels as CNN does to get the features and then pass them to the plain LSTM.","14eb3546":"And now I need to additionally prepare the X data for the CNN-LSTM","b33d7425":"So, we have sales information about 50 items in 10 stores from 2013-01-01 to 2017-12-31","08bfac9e":"I want to use it because I think that DNN can get some improvements using this - maybe it will find some patterns in the time series.","b0caf058":"# LSTM ","73696b1e":"I have already run hyperparameters tuning related code on my local machine, so I comment out it, but it is valid code.","60557f2c":"## Hyperparameters tuning","8afeab26":"So, hyperparameters 'changepoint_prior_scale': 0.3, 'holidays_prior_scale': 0.1, 'n_changepoints': 200, 'seasonality_mode': 'multiplicative' can give MAPE = 14.3208.","dc9bb504":"And as you can see, the bigger dropout, the bigger validation loss fluctuations, but the smaller loss in the end. Interesting, notice this."}}